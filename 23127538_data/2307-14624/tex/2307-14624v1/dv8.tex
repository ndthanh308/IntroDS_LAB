%%



%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.

% \documentclass[sigconf,authordraft]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{ragged2e} 
\usepackage{booktabs,makecell, multirow, tabularx}

% \documentclass[sigconf,review,anonymous]{acmart}


%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

% \pagestyle{plain}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[MM '23]{Proceedings of the 31th ACM International Conference on Multimedia}{October 29--November 2, 2023}{Ottawa, Canada}

\acmConference[]{}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{3579}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\settopmatter{printacmref=false} %remove ACM reference format
\renewcommand\footnotetextcopyrightpermission[1]{}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{FS-Depth: Focal-and-Scale Depth Estimation from a Single Image in Unseen Indoor Scene}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author[author1]{Chengrui Wei, Meng Yang, Lei He, Nanning Zheng}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }


% \author[author2]{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}
\renewcommand{\shortauthors}{ }

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
It has long been an ill-posed problem to predict absolute depth maps from single images in real (unseen) indoor scenes. We observe that it is essentially due to not only the scale-ambiguous problem but also the focal-ambiguous problem that decreases the generalization ability of monocular depth estimation. That is, images may be captured by cameras of different focal lengths in scenes of different scales. In this paper, we develop a focal-and-scale depth estimation model to well learn absolute depth maps from single images in unseen indoor scenes. First, a relative depth estimation network is adopted to learn relative depths from single images with diverse scales/semantics. Second, multi-scale features are generated by mapping a single focal length value to focal length features and concatenating them with intermediate features of different scales in relative depth estimation. Finally, relative depths and multi-scale features are jointly fed into an absolute depth estimation network. In addition, a new pipeline is developed to augment the diversity of focal lengths of public datasets, which are often captured with cameras of the same or similar focal lengths. Our model is trained on augmented NYUDv2 and tested on three unseen datasets. Our model considerably improves the generalization ability of depth estimation by 41\%/13\% (RMSE) with/without data augmentation compared with five recent SOTAs and well alleviates the deformation problem in 3D reconstruction. Notably, our model well maintains the accuracy of depth estimation on original NYUDv2.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010224.10010226.10010239</concept_id>
       <concept_desc>Computing methodologies~3D imaging</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224.10010225.10010227</concept_id>
       <concept_desc>Computing methodologies~Scene understanding</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224.10010225.10010233</concept_id>
       <concept_desc>Computing methodologies~Vision for robotics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Vision for robotics}
\ccsdesc[500]{Computing methodologies~Scene understanding}
\ccsdesc[500]{Computing methodologies~3D imaging}




% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{monocular depth estimation, focal length, scene scale, indoor scene}
% \keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   % Figure removed
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

\begin{teaserfigure}
  \centerline{% Figure removed}
  \caption{ Depth map prediction from an image with different focal lengths and scene scales/scemantics. Left: Predicted depth maps, Right: depth maps in reconstructed 3D view. Our model accurately predicts absolute depth values in unseen scenes and well alleviates the serious deformation problem in 3D view (red boxes) compared with a recent SOTA model ZoeDepth \cite{2}.}
  \Description{Enjoying the baseball game from the third-base
  seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}



% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Monocular depth estimation is a fundamental task in the field of multimedia systems and robotics in indoor scenes. This task predicts pixel-by-pixel depth values as well as 3D geometry of the scene from a single 2D image. It well facilitates subsequent tasks of 3D scene perception such as human-robot interaction \cite{49,56}, augmented reality \cite{3,57}, 3D reconstruction \cite{50,55}, autonomous robots \cite{4,58}, and CAD model generation \cite{5}.

Monocular depth estimation is essentially ill-posed due to the scale-ambiguous problem of single images in real (unseen) scene. That is, a 2D image may correspond to infinite depth maps of different scales in 3D scene. In recent years, this task has made great advances thanks to the development of deep neural networks, which learns prior knowledge from RGB-Depth datasets and then regresses depth maps from single images. Recent advances can be classified into two branches including absolute depth estimation \cite{6,7,8,9,10} and relative depth estimation \cite{9,11,48,21}. Absolute depth estimation directly regresses the pixel-level distances (in meters) between the scene and the camera. Absolute depth estimation models are usually trained and tested on the dataset of a single scene to avoid the scale-ambiguous problem. Mixing multiple datasets with different scene scales may disturb the training of the network, and thereby significantly reduce the accuracy of depth estimation. As a result, most existing models tend to be overfitting in a single dataset and cannot be well generalized to unseen scenes. To address this problem, relative depth estimation \cite{11,21,48,28} learns the relationships (i.e. far or near) of points in the scene regardless of the scale-ambiguous problem. Therefore, relative depth estimation models can be well trained on multiple datasets with different scene scales and have excellent generalization ability in unseen scenes. However, relative depth maps are often limited in practical applications due to the lack of object distances compared with absolute depth maps. Recently, researchers attempted to combine relative/absolute depth estimation models \cite{2,12,13,48}, which are realized by first predicting relative depth maps from mixed datasets of different scene scales and then regressing absolute depth maps with a scale network. Therefore, these models could well predict absolute depth maps from single images and can be well generalized to unseen scene with different scales.

In this paper, we observe that it is essentially due to not only the scale-ambiguous problem of the scenes but also the focal-ambiguous of cameras to disturb the training of monocular depth estimation in unseen indoor scenes. That is, high-quality images may be captured by cameras of different focal lengths in scenes of different scales/semantics. It is acknowledged that cameras generally focus 3D scenes on CCD/CMOS \cite{53,54} planes by intrinsic optical imaging system \cite{45}. As a result, the focal length of cameras may vary in three cases in unseen scenes. First, the focal length of cameras may be changed smaller or larger in the scenes with different scales. This case corresponds to the common scale-ambiguous problem, which has been well studied in the literature \cite{46}. Second, the focal length of cameras may be changed smaller or larger in the scenes with different semantics such as tiny and huge objects. Third, the focal lengths of different camera configurations naturally differ with each other. The diversity of focal lengths may disturb the training of monocular depth estimation together with the diversity of scene scales, thereby reduce the accuracy of predicted depth maps in unseen scenes. Unfortunately, such focal-ambiguous problem of monocular depth estimation was not investigated sufficiently in the literature \cite{1,2}. Though the effect of scene scales on focal lengths has been partly avoided in joint relative/absolute depth prediction \cite{2,12}, focal lengths of cameras may still be changed in the other two cases in unseen scenes. Existing absolute depth estimation models are often trained/tested on the same dataset such as NYUDv2 \cite{20} and these models often achieve desire results in similar scenes. One reason lies that all images in these datasets are captured with cameras of the same or similar focal lengths. Therefore, these models cannot be well generalized to unseen indoor scenes due to not only the scale-ambiguous problem but also the focal-ambiguous problem \cite{1}.

In this paper, we further investigate the focal-ambiguous problem of monocular depth estimation in unseen indoor scenes. To this end, we develop a focal-and-scale depth estimation model, namely \textbf{FS-Depth}, to well learn absolute depth maps from single images with diverse focal lengths and scales in unseen scenes. First, we adopt a relative depth estimation network \cite{11} to learn relative depth maps from single images with diverse scales/semantics. This model is well pre-trained on multiple datasets of different scene scales/semantics. Second, we generate multi-scale features by mapping a single focal length to focal length features and concatenating them with intermediate features of different scales in relative depth estimation. Third, relative depth maps and multi-scale features are jointly fed into an absolute depth estimation network to predict accurate depth maps from single images. The recent \cite{2} is adopted as our base model, which well combined relative/absolute depth estimation and achieved state-of-the-art (SOTA) results. Finally, we well fine-tune our FS-Depth model by exploring a critical issue of learning rates that may be significantly disrupted by diverse focal lengths. In addition, we develop a new data augmentation pipeline to generate images with diverse focal lengths for training and testing, considering that most public datasets are captured with cameras of the same or similar focal lengths.

Our model is trained on NYUDv2, evaluated on three unseen datasets including iBims-1 \cite{14}, DIODE-Indoor \cite{15}, DIML-Indoor \cite{16}, and compared with five recent SOTAs \cite{2,6,7,10,38}. The results demonstrate that our model considerably improves the generalization ability of monocular depth estimation by 41\%/13\% (RMSE) on the three unseen datasets with/without data augmentation. 
Visual results demonstrate that our model consistently produces accurate depth values and alleviates the serious deformation problem in 3D reconstruction compared with SOTAs such as the examples in Figure  \ref{fig:teaser}.
One reason lies that our model well incorporates focal lengths in monocular depth estimation while focal length cannot be directly fed into existing models. Notably, our model well maintains the accuracy of monocular depth estimation on the test dataset of NYUDv2, which has the same focal length and scene scales with the training dataset. 

The main contributions are summarized as follows.
\begin{itemize}
\item  	We observe that it is essentially due to not only the scale-ambiguous problem but also the focal-ambiguous problem that decreases the generalization ability of monocular depth estimation in unseen indoor scenes.
\item 	We develop a focal-and-scale depth estimation model to well learn absolute depth maps from single images with diverse focal lengths and scene scales/semantics. Our model considerably improves the generalization ability on unseen datasets compared with recent SOTAs.
\item We develop a new data augmentation pipeline to collect RGB-Depth datasets with diverse focal lengths, considering that most public datasets are captured with cameras of the same or similar focal lengths.
\end{itemize}

%%%%%%%%%%
% The mainstream approach to solving this problem nowadays is to use neural networks that contain both an encoder and a decoder. There are primarily two branches of monocular depth estimation: metric depth estimation \cite{6,7,8,9,10} and relative depth estimation \cite{9,11}. metric depth estimation predicts the absolute distance between each point in the scene and the camera, and is a pixel-level regression problem. It typically outputs distances in physical units (meters). Metric depth estimation is usually trained and tested on a single dataset. However, since each dataset has a different scale range, training on multiple datasets of different scales can lead to scale ambiguity in the network, reducing performance. As a result, most existing metric depth estimation models tend to be overfitting on a single dataset and struggle to generalize to other datasets.

% Relative depth estimation is a technique used to predict the relative depth order of each point in a given scene. It does not require measuring the physical distance value of each point. Therefore, relative depth estimation networks can be trained on multiple datasets. By normalizing the scale, relative depth estimation does not need to worry about scale ambiguity. Since it is trained on a large amount of data, it has excellent generalization performance and can understand the relative distance relationships of a vast number of scenes.

% Combining relative depth estimation with metric depth estimation \cite{2,12,13}, utilizes a scale network to regress metric depth for each pixel based on the estimated relative depth map. This approach not only ensures the network has certain generalization ability, but also retains its ability to predict absolute distance. However, these models that aim to improve the network's generalization ability overlook an important factor: camera focal length. Images taken with different focal lengths have significant differences. When the camera is in the same position shooting the same scene, objects appear larger and the image has a smaller field of view with a larger focal length. However, corresponding points in images taken with different focal lengths have the same depth value. This difference results in the depth estimation network providing significantly different results for the same scene captured by the same camera in the same position with different focal lengths. This paper reveals through experiments that focal length changes have a significant impact on metric monocular depth estimation networks.

% The problems faced by generalization of monocular depth estimation include:
% \begin{itemize}
% \item {\verb|train|}: The focal length of different metric depth estimation datasets is often different. If we directly use a network trained on dataset A and test on dataset B, without inputting the focal length, the network will have no idea that the focal length has changed, which will result in poor generalization performance. Therefore, the focal length input is essential to obtain a widely applicable absolute depth estimation model.
% \item{\verb|applications|}: The network model obtained from the training dataset is often unable to be directly applied to practical applications because the focal length of the camera in the training dataset is often different from that in practical applications. Therefore, to obtain a widely applicable metric depth estimation model, the focal length input is necessary.
% \end{itemize}

% Due to the fact that most of the existing single metric depth estimation datasets are obtained using single focal length, there is no single dataset available that can enable the network to learn how to handle focal length inputs. Therefore, based on the fundamental principles of camera imaging, we propose a simple and effective data augmentation method for simulating focal length changes, which can generate multiple pairs of RGB and depth data under different focal lengths. In order to incorporate focal length input well into network training, we map the single focal length number to a learnable matrix and add it to the network's training. Since focal length input cannot be easily integrated into the same position as RGB input (mainstream backbone network pre-trained weights \cite{27} are designed for 3-channel, standard RGB input, and if focal length is added simultaneously with RGB, the pre-trained weights will become ineffective), we choose to add focal length input to the decoder part of the network. We investigate a series of detailed issues related to network training after adding focal length input, such as learning rate, dataset proportion, and dataset resize method.

% Our final model improved its performance on multiple generalization datasets (iBims \cite{14}, DIODE Indoor \cite{15}, DIML-Indoor \cite{16}) by an average of 13\%, and achieved the current state-of-the-art generalization metric, while maintaining the performance of the baseline method on the NYU \cite{20} dataset. In our simulated focal length variation test, the performance was improved by 43\%. The main contributions of this paper can be summarized as follows:
% \begin{itemize}
% \item  Introducing the focal length feature in the study of generalization in monocular depth estimation networks, revealing its impact on this task
% \item 	Proposing a method for generating multi-focal length RGB and depth and incorporating the focal length input into network training in a reasonable way;
% \item Investigating several important details of using focal length as network input and training.
% \end{itemize}


\section{Related Work}
% \subsection{Depth estimation}
\subsection{Absolute depth estimation}
Absolute depth estimation regresses pixel-level distances between the scene and the camera. Absolute depth estimation has been widely investigated in the past decade \cite{51,6,7,8,9,10,39}. For example,  Eigen \textit{et al.} \cite{39} first proposed a coarse-fine network to obtain the depth with the constraint of a scale-invariant loss.
In addition, some researchers improved the accuracy of absolute depth estimation by incorporating prior knowledges. For example, He \textit{et al.} \cite{1} first learned absolute depth maps from single images with deep neural network embedding focal length. Saxena \textit{et al.} \cite{52} transformed 3D coordinates into planar parameters based on geometric prior, and utilized mean pane loss to improve the prediction of depth values. Most existing models are trained and tested on a single dataset such as NYUDv2. Training on multiple datasets may degrade the accuracy of these models due to the scale-ambiguous problem. As a result, most existing models tend to be overfitting on a single dataset and the generalization ability was not well investigated. Therefore, most existing models cannot be well generalized to unseen scenes. The reason lies that images may be captured by cameras of different focal lengths in scenes of different scales/semantics in unseen scenes. In this paper, we well investigate the generalization ability of absolute depth estimation in unseen scenes.

\subsection{Relative depth estimation}
Relative depth estimation \cite{9,11,13,21} learns the relationships of points in the scene instead of absolute distances. This task was first  investigated in \cite{48}, which also introduced a new dataset consisting of images annotated with relative depth values. Recently, Ranftl \textit{et al.} \cite{11} proposed a robust training objective that is invariant to changes in depth range and scale. Relative depth estimation well handles the scale-ambiguous problem by ignoring the scales of different scenes. Therefore, it can be well trained on multiple datasets and can be easily generalized to unseen scenes. Relative depth estimation could be well used in a few applications such as human-robot interaction. However, these models are often limited in other scenarios due to the lack of object distances. For example, it may require absolute depth values in 3D reconstruction and autonomous robots. In this paper, we aim to directly estimate absolute depth values in unseen scenes.

\subsection{Joint relative/absolute depth estimation}
Recently, researchers combined relative/absolute depth estimation to well learn absolute depth values from single images \cite{48, 2, 12}. For example, Jun \textit{et al.} \cite{12} decomposed the relative depth into normalized depth and scale features, and proposed a multi-code network in which the absolute depth decoder utilized relative depth features from the gradient and normalized depth decoders. Benefiting from relative depth estimation, these models can be well trained on multiple datasets and thereby are well generalized to unseen scenes. In this paper, we adopt a recent model of joint relative/absolute depth estimation \cite{2} as base model to handle the scale-ambiguous problem. However, we observe that it is essentially due to not only the scale-ambiguous problem of the scenes but also the focal-ambiguous of cameras that degreases the generalization ability of monocular depth estimation. Therefore, we additionally take focal length as input of the base model to well handle the focal-ambiguous problem.


% Figure environment removed

% tutu
% Network Architecture. Our FS-Depth model comprises four main components. First, the relative depth estimation network learns relative depths from single images with diverse scales/semantics. Second, the focal length feature generation maps a single focal length to multi-scale focal length features by multiplying the focal length value with the learnable focal encoding matrix and up-sampling. Third, the multi-scale features are generated by concatenating the up-sampled focal length features with intermediate features of different resolutions in relative depth estimation. Finally, the absolute depth estimation network predicts accurate depth maps with the inputs of multi-scale features and relative depth maps. The recent \cite{2} is adopted as our base model.



\section{Method}
\subsection{Framework}
Figure \ref{fig:net} shows the network architecture of our FS-Depth model. It takes a single RGB image and the focal length of camera as the input and output an absolute depth map. First, a relative depth estimation network is used to learn relative depths from single images with diverse scales/semantics (in \textbf{Section 3.2}). Second, focal length features are generated by multiplying a single focal length to a learnable focal encoding matrix and up-sampling it to multiple resolutions (in \textbf{Section 3.3}). Third, multi-scale features are generated by concatenating focal length features of different resolutions with intermediate features of different scales in relative depth estimation (in \textbf{Section 3.4}). Finally, relative depth maps and multi-scale features are jointly fed into an absolute depth estimation network to predict accurate depth maps (in \textbf{Section 3.5}). Our model is well fine-tuned by exploring a critical issue of learning rates (in \textbf{Section 3.6}). The recent \cite{2} is adopted as our base model, which well combined relative/absolute depth estimation networks and utilized a scale-invariant logarithmic loss \cite{39}. In addition, we develop a new data augmentation method to collect datasets with diverse focal lengths for training and testing (in \textbf{Section 5}). Our FS-Depth model is introduced in detail in the following.

\subsection{Relative depth estimation}
% Relative depth estimation is used to well learn relative depths, i.e. the relationships (far or near) of pixels, from single images. It ignores the scales of different scenes, therefore, can be well trained on mixed datasets and can be easily generalized to unseen scenes. 
Relative depth estimation is used to learn relative depths, \textit{i.e.} the relationships (far or near) of pixels, from single images. It ignores the scales of different scenes, therefore, can be well trained on mixed datasets. 
There are many relative depth estimation models such as \cite{48,11,21}, among which the recent MiDaS \cite{11} is the most famous one. This model was realized based on the backbone ResNeXt-101-WSL \cite{resnext} and a normalized scale loss function. It was pretrained on ImageNet \cite{imagenet} and then further trained on mixing multiple datasets with a pareto-optimal strategy to well learn relative depth maps from single images. This model was adopted in our base model ZoeDepth \cite{2}. Notably, it used an updated version with the DPT encoder-decoder architecture \cite{21} and a transformer-based backbone BEiT384-L \cite{47} instead to further improve the accuracy of relative depth estimation. In our solution, this well-trained model in \cite{2} is directly adopted to predict high-quality relative depth maps from single images of diverse scene scales/semantics.

\subsection{Focal length features generation}
Our FS-Depth model jointly takes a single RGB image $\mathbf{x}$ and reference focal length of the camera $f$ as inputs during training and testing. Unfortunately, a single value of focal length $f$ cannot be directly fed into the network together with the vector $\mathbf{x}$ of RGB image. One reason lies that typical backbone networks with pre-training weights are often designed for single RGB images, which are only applicable to inputs with three channels. Therefore, no pre-training weights are available to handle additional input of focal lengths. We attempt to input the focal length in the middle stage of the network, rather than at the beginning along with the RGB image. Specifically, we directly embed the focal length into the absolute depth estimation network. To this end, we introduce a learnable focal encoding matrix $\mathbf{M}$ inspired by the position encoding design in ViT \cite{26}. In order to align with the minimal intermediate feature maps of input RGB image, we set the size of the matrix to $12\times16$. We then generate a focal length feature for the network by multiplying the focal length value with the matrix as $\mathbf{F}=f\times\mathbf{M}$. The focal length feature is up-sampled to obtain focal length features of different scales by a linear interpolation operation. In our model, five focal length features $\mathbf{F}_{j}  \ (1\leq j \leq 5)$ of the scales 1/2, 1/4, 1/8, 1/16, and 1/32 are generated, where $j$ denotes the index of the five scales.

\subsection{Multi-scale features fusion}
The generated focal length features of different scales $\mathbf{F}_{j} \  (1\leq j \leq 5)$ are then embedded into the absolute depth estimation network. To this end, we similarly extract five intermediate features $\mathbf{N}_{j}$ of different scales 1/2, 1/4, 1/8, 1/16, and 1/32 from the relative depth estimation network, where $j$ denotes the index of the intermediate layers of the image $\mathbf{x}$. Specifically, the five intermediate features are from the bottleneck of the encoder-decoder architecture and the four subsequent levels of the decoder in MiDaS. The focal length features $\mathbf{F}_{j}$ are then concatenated with the intermediate features $\mathbf{N}_{j}$ at different scales, resulting in our final multi-scale features $\mathbf{feature}_{j}=[\mathbf{N}_{j},\mathbf{F}_{j}]$.  Our multi-scale features add the focal length value to intermediate features of different scales in relative depth estimation, which can well consider the influence of focal length on depth estimation more comprehensively. Once the focal length is added to a single layer of the network, the influence of focal length on subsequent layers may fade gradually. Our final multi-scale features are then fed into the subsequent absolute depth estimation network. The effect of the multi-scale features will be verified in the ablation study (in \textbf{Section 5.6}).

\subsection{Absolute depth estimation}
The absolute depth estimation network jointly takes our multi-scale features and output of relative depth estimation network to predict accurate depth maps. In our base model, a tensor is first obtained by concatenating the relative depth map with the last intermediate feature in relative depth estimation \cite{2}. Then the model predicts discrete depth bins \cite{22, 23, 24, 25} from intermediate features of relative depth estimation and the probability of each pixel corresponding to different bins from the tensor. Finally, the model obtains absolute depth value of each point by weighted summation of discrete depth bins based on the probability distribution. In our solution, we only adjust the number of channels in depth bin prediction to well accommodate our multi-scale features with focal length.

\subsection{Learning rates determination}
Our FS-Depth model comprises three networks including the relative depth estimation network , the focal length features generation network and the absolute depth estimation network in Figure \ref{fig:net}. However, only  the focal length features generation network and the absolute depth estimation network have access to the focal length and the relative depth estimation network is well pre-trained in advance. Therefore, learning rates of the three networks play an important role in our solution. If the learning rates of the three networks are not properly constrained, errors induced by focal length may disturb the well-trained relative depth estimation network and thereby reduce the performance of our model. In our solution, we determine the learning rate of the relative depth estimation network to be $1/50$ of  the focal length features generation network and the absolute depth estimation network. The effect of the learning rates will be verified in the ablation study (in \textbf{Section 5.6}).
% Various simple strategies can be adopted for this problem such as freezing the learning rate of the relative depth estimation network or setting their learning rates the same. However, a higher learning rate may introduce errors caused by the focal length to the relative depth estimation network. Simply freezing the weights of the relative depth estimation network would lead to excessive learning burden, thereby hindering the performance of the absolute depth estimation network. In our solution, we determine the learning rate of the relative depth estimation network to be 1/50 of the absolute depth estimation network. The effect of the learning rates will be verified in the ablation study (in Section 5.6).

% Figure environment removed

\section{RGB-Depth Data Augmentation}
Our FS-Depth model requires RGB-Depth datasets with diverse focal lengths and scene scales for training and testing. The scene scales generally vary across public datasets. However, most public datasets are collected with cameras of the same or similar focal lengths such as NYUDv2. In this paper, we propose a new data augmentation pipeline to generate datasets with diverse focal lengths from public datasets above.
Figure \ref{chengxiang} illustrates the basic principle of the camera to capture an image, where $o_{c}$ denotes the position of the camera, $p=(x_w, y_w, z_w)$ denotes the coordinates of a point in the 3D scene, $s$ denotes the image plane of the camera, $p_s=(x_s, y_s)$ denotes the projected point of the point $p$ in the image plane, and $f$ denotes the focal length of the camera. It follows that:
\begin{equation}
\frac{x_{w}}{x_{s}}=\frac{y_{w}}{y_{s}}=\frac{z_{w}}{f}.\label{bili} \tag{1}
\end{equation}

When the coordinates $p_s=(x_s, y_s)$ and the focal length $f$ are simultaneously scaled by a factor $k$, the equation (\ref{bili}) still holds. The coordinates of point $p$ in the real scene remains unchanged. Therefore, by scaling the coordinates of each pixel in the RGB image by a factor $k$, the resulting RGB-Depth pair is equivalent to the one captured by a camera with the focal length  $kf$.

Similarly, when the coordinates $p_s=(x_s, y_s)$ are scaled by a factor $k$ while the focal length  $f$ is kept unchanged, the depth value $z_w$ needs to be scaled by $1/k$ to maintain the equation (\ref{bili}). Therefore, by scaling the coordinates of all pixels in the RGB image by a factor $k$ and scaling the values of all pixels in the depth map by a factor $1/k$, the resulting RGB-Depth pair is equivalent to the one captured by a camera with unchanged focal length $f$.

Based on the above analysis, the data augmentation pipeline is described as follows.

$\mathbf{Step \ 1}.$ Given a random factor $k \ (0.7\leq k\leq 1)$ and an RGB-Depth pair of size $m\times n$ that is captured with the focal length $f$, we crop a new RGB-Depth pair of small size $km\times kn$ at the center of the image.

$\mathbf{Step \ 2}.$ Up-sample the new RGB-Depth pair of size $km\times kn$ to the size of $m\times n$ by nearest-neighbor interpolation. The generated RGB-Depth pair is equivalent to the one captured by a camera with the focal length $f/k$. Notably, pixel values in the depth map are not changed in this step.

$\mathbf{Step \ 3}.$ Up-sample the new RGB-Depth pair of the size $km\times kn$ to the size of $m\times n$ by nearest-neighbor interpolation and rescale the depth values $z_w$ by a factor $k$ to obtain a depth map with the values $kz_w$. The generated RGB-Depth pair is equivalent to the one captured by a camera with the focal length $f$. Notably, pixel values in the depth map are changed in this step.

Figure \ref{step} shows an example of our data augmentation pipeline. Notably both Step 2 and Step 3 in our data augmentation pipeline are used to generate RGB-Depth datasets with diverse focal lengths. When only Step 2 is activated, multiple RGB images with different focal lengths correspond to the same depth range. The network may be overfitting to a single depth range and fail to well learn the meaning of input focal lengths. Thereby, it may significantly reduce the generalization ability of the network. When only Step 3 is activated, some prior knowledge of the real scene such as the size of objects may be destroyed due to the change of depth values. The monocular depth estimation network could not well learn such prior knowledge in the real scene and thereby its generalization ability may be decreased. In our solution, the RGB-Depth pairs in Step 2 and Step 3 are mixed in a ratio of $60\%:40\%$ to generate an augmented training dataset with diverse focal lengths. The effectiveness of our data augmentation pipeline will be verified in the ablation study (in \textbf{Section 5.6})



% We choose ZoeDepth \cite{2} as our base network for predicting metric depth by incorporating scale to relative depth. The MiDaS \cite{11} training strategy is employed to train the network for relative depth estimation. The loss function used in MiDaS is invariant to scaling and offset. If multiple datasets are available, a multi-task loss that ensures pareto-optimality across the datasets is used. The MiDaS training strategy can be applied to various network architectures. We use the DPT encoder-decoder architecture \cite{21} as our basic model and add a focal length embedding module on top of the original ZoeDepth \cite{2} network. The focal length and RGB are jointly fed as inputs to the network during both training and testing. As the focal length input is a single value and differs greatly in form from an RGB image, it requires certain design considerations to embed it to features. Drawing inspiration from the positional encoding design in ViT \cite{26}, we embed the focal length input to a learnable matrix, concatenate it with the intermediate layer features of the network, and feed it to the subsequent layers for processing.

% Assuming that an RGB image $x_{i}$ corresponds to a focal length of $f_{i}$, let $N\left(x_{i}\right)_{j}$ denote the j-th intermediate layer feature of $x_{i}$. We introduce a learnable focal length matrix $F$, and use  $F_{f_{i}}=f_{i} \times F$ as the focal length feature matrix for the network, which is concatenated with  $N\left(x_{i}\right)_{j}$. The resulting feature,  $\text { feature }_{j}=\left[\mathrm{N}\left(x_{i}\right)_{j}, F_{f_{i}}\right]$, is used as input for subsequent network processing. Our original focal length matrix has a size of 16Ã—18, and we upsample it to match features of images at different scales. We use scale-invariant logarithmic loss \cite{39} for pixel-level supervision.
% Figure environment removed

\section{Experiments and Analysis}
\subsection{Experiment setting}
$\mathbf{Datasets}.$ Our model is trained on the training dataset of NYUDv2. The generalization ability of our FS-Depth model is tested on three unseen datasets iBims-1 \cite{14}, DIODE-Indoor \cite{15}, and DIML-Indoor \cite{16} with different scene scales/semantics. The dataset NYUDv2 consists 464 indoor scenes. We use the official split which consists 24231 examples from 249 scenes provided by previous methods \cite{38} for training and the test set consists 654 examples from 215 scenes based on \cite{39}. The dataset iBims-1 contains 100 test examples. The DIODE-Indoor dataset contains 325 test examples. The DIML-Indoor dataset contains 503 test examples.

\noindent $\mathbf{Train}.$ We conduct the experiments on two NVIDIA GeForce RTX 3090 graphics cards with a batch size of 4. We employ the AdamW \cite{adamw} optimizer with a weight decay of 0.01. The learning rates of the focal length features generation network and  the absolute depth estimation network is set to $1.6^{-4} $, while the learning rate of the relative depth estimation network is set to $1/50 \times 1.6^{-4} $. We train our model for 5 epochs on the training dataset of NYUDv2. Notably the relative depth estimation network in our model has been pre-trained on twelve datasets following \cite{11,2}.

\subsection{Results on augmented datasets (unseen)}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table*}[]
  \caption{Generalization ability on augmented datasets (unseen). Best results are in bold, second best are underlined. Our model consistently achieves a considerable improvement on these unseen datasets compared with the second-best model.}
  \label{fanhua_a}
  \setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Method}                              & \multicolumn{3}{c|}{iBims-1}                                 & \multicolumn{3}{c|}{DIODE-Indoor}                                                                                         & \multicolumn{3}{c}{DIML-Indoor}                                                                                          \\
                                                     & $\delta_{1} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l|}{$RMSE \downarrow$} & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l}{$RMSE \downarrow$} \\ \hline
BTS \cite{38} & 0.252                 & 0.288            & 1.450             & 0.149                                     & 0.468                                & 2.232                                  & 0.183                                     & 0.375                                & 1.289                                 \\
AdaBins \cite{6}                     & 0.159                 & 0.374            & 1.509             & 0.101                                     & 0.541                                & 2.341                                  & 0.167                                     & 0.366                                & 1.240                                 \\
LocalBins \cite{7}                   & 0.237                 & 0.330            & 1.235             & 0.132                                     & 0.497                                & 2.230                                  & 0.259                                     & 0.314                                & 1.124                                 \\
NeWCRFs \cite{10}                    & 0.219                 & 0.323            & 1.277             & 0.121                                     & 0.526                                & 2.150                                  & 0.282                                     & 0.296                                & 1.007                                 \\
ZoeDepth \cite{2}                    & \underline{0.280}           & \underline{0.274}      & \underline{1.069}       & \underline{0.232}                               & \underline{0.393}                          & \underline{1.836}                            & \underline{0.297}                               & \underline{0.265}                          & \underline{0.833}                           \\ \hline
\textbf{Ours}                                        & \textbf{0.741}        & \textbf{0.169}   & \textbf{0.574}    & \textbf{0.508}                            & \textbf{0.310}                       & \textbf{1.327}                         & \textbf{0.797}                            & \textbf{0.151}                       & \textbf{0.430}                        \\
Improvement                                & 164.6\%               & 38.3\%           & 46.3\%            & 118.9\%                                   & 21.1\%                               & 27.7\%                                 & 168.4\%                                   & 43.0\%                               & 48.4\%                               \\ \hline
\end{tabular}}
\end{table*}

We first test the generalization ability of our model on the augmented datasets with diverse focal lengths and scene scales. Table \ref{fanhua_a} concludes the results of our model using three common metrics. Five recent monocular depth estimation models are used as the baselines, among which the two models NewCRFs \cite{10} and ZoeDepth \cite{2} achieve the SOTA performance. Notably both our model and the baseline models are not fine-tuned on these test datasets. Our model consistently achieves a significant improvement on all these datasets using different metrics. For example, RMSE of predicted depth maps are reduced by 46.3\%, 27.7\%, and 48.4\%, respectively, on the three datasets compared with the second-best model. It indicates that our model is well generalized to the datasets that are collected by cameras of different focal lengths in unseen scenes. The reason lies that incorporating focal length can effectively improve the adaptability of our model to the cameras with different focal lengths. By comparison, though some of the baselines are well generalized to unseen scenes of different scales, focal length cannot be directly incorporated in all these models.
Figure \ref{diml_a} shows the visual results of predicted depth maps on DIML-Indoor. Our model not only well predicts the structures of depth maps, but more importantly, estimate accurate absolute depth values. It can be clearly seen in Figure \ref{diml_a_3d} that our model well alleviates the deformation problem in 3D reconstruction (in red boxes) compared with the second-best model.

% Figure environment removed

% Figure environment removed




% % Figure environment removed

\subsection{Results on original datasets (unseen)}
We then evaluate the generalization ability of our model on the original datasets of iBims-1, DIODE-Indoor, and DIML-Indoor. Notably the focal lengths across the test datasets and the training dataset NYUDv2 are still different, though each dataset is collected by cameras with the same or similar focal lengths.  Similarly, the five recent models are used as baselines. Table \ref{fanhua_o} concludes the results of our model and the baselines. Our model consistently outperforms all the baselines with different metrics. For example, RMSE of predicted depth maps are reduced by 17.3\%, 14.2, and 18.8\%, respectively, on the three datasets compared with the second-best model. In indicates that our model still works effectively on original public datasets. Figure \ref{diml_o} and \ref{diml_o_3d} show the visual results of predicted depth maps and reconstructed 3D scenes on a dataset. The results are similar to Figure \ref{diml_a} and \ref{diml_a_3d}.







\begin{table*}[]
  \caption{Generalization ability on original datasets (unseen).}
  \label{fanhua_o}
  \setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Method}                              & \multicolumn{3}{c|}{iBims-1}                                 & \multicolumn{3}{c|}{DIODE-Indoor}                                                                                         & \multicolumn{3}{c}{DIML-Indoor}                                                                                          \\
                                                     & $\delta_{1} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l|}{$RMSE \downarrow$} & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l}{$RMSE \downarrow$} \\ \hline
BTS \cite{38} & 0.538                 & 0.231            & 0.919             & 0.210                                     & 0.418                                & 1.905                                  & 0.652                                     & 0.223                                & 0.640                                 \\
AdaBins \cite{6}                     & 0.555                 & 0.212            & 0.901             & 0.174                                     & 0.443                                & 1.963                                  & 0.657                                     & 0.216                                & 0.623                                 \\
LocalBins \cite{7}                   & 0.558                 & 0.211            & 0.880             & 0.229                                     & 0.412                                & 1.853                                  & 0.689                                     & 0.208                                & 0.587                                 \\
NeWCRFs \cite{10}                    & 0.548                 & 0.206            & 0.861             & 0.187                                     & 0.404                                & 1.867                                  & 0.708                                     & 0.204                                & 0.553                                 \\
ZoeDepth \cite{2}                    & \underline{0.658}           & \underline{0.169}      & \underline{0.711}       & \underline{0.376}                               & \underline{0.327}                          & \underline{1.588}                            & \underline{0.646}                               & \underline{0.177}                          & \underline{0.536}                           \\ \hline
\textbf{Ours}                                        & \textbf{0.787}        & \textbf{0.145}   & \textbf{0.588}    & \textbf{0.522}                            & \textbf{0.283}                       & \textbf{1.363}                         & \textbf{0.796}                            & \textbf{0.147}                       & \textbf{0.414}                        \\
Improvement                                & 19.6\%               & 14.2\%           & 17.3\%            & 38.8\%                                   & 13.5\%                               & 14.2\%                                 & 23.2\%                                   & 16.9\%                               & 22.8\%                               \\ \hline
\end{tabular}}
\end{table*}


\begin{table*}[]
  \caption{Ablation study of focal length incorporation on three original datasets (unseen).}
  \label{cmp}
    \setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Method}                              & \multicolumn{3}{c|}{iBims-1 \cite{14}}                                 & \multicolumn{3}{c|}{DIODE-Indoor \cite{15}}                                                                                         & \multicolumn{3}{c}{DIML-Indoor \cite{16}}                                                                                          \\
                                                     & $\delta_{1} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l|}{$RMSE \downarrow$} & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l}{$RMSE \downarrow$} \\ \hline
base model & 0.658                 & 0.169            & 0.711             & 0.376                                     & 0.327                                & 1.588                                 & 0.646                                     & 0.177                                & 0.536                                 \\
single-scale & 0.738                 & 0.151            & 0.637             & 0.471                                     & 0.294                                & 1.428                                  & 0.767                                     & 0.150                                & 0.437                                 \\


\textbf{mutil-scale}                                        & \textbf{0.787}        & \textbf{0.145}   & \textbf{0.588}    & \textbf{0.522}                            & \textbf{0.283}                       & \textbf{1.363}                         & \textbf{0.796}                            & \textbf{0.147}                       & \textbf{0.414}                        \\
\hline
\end{tabular}}
\end{table*}


\begin{table*}[]
  \caption{Ablation study of the data augmentation ratio on three original datasets (unseen).}
  \label{ratio}
  \setlength{\tabcolsep}{1.75mm}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Ratio}                              & \multicolumn{3}{c|}{iBims-1}                                 & \multicolumn{3}{c|}{DIODE-Indoor}                                                                                         & \multicolumn{3}{c}{DIML-Indoor}                                                                                          \\
                                                     & $\delta_{1} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l|}{$RMSE \downarrow$} & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l}{$RMSE \downarrow$} \\ \hline
100\% : 0\% & 0.422                 & 0.229            & 0.939             & 0.226                                     & 0.368                                & 1.709                                  & 0.665                                     & 0.171                                & 0.516                                 \\


\textbf{60\% : 40\%}                                        & \textbf{0.787}        & \textbf{0.145}   & \textbf{0.588}    & \textbf{0.522}                            & \textbf{0.283}                       & \textbf{1.363}                         & \textbf{0.796}                            & \textbf{0.147}                       & \textbf{0.414}                        \\
0\% : 100\% & 0.608                 & 0.182            & 0.764             & 0.312                                     & 0.339                                & 1.614                                  & 0.498                                     & 0.205                                & 0.619                                 \\
\hline
\end{tabular}}
\end{table*}





% % Figure environment removed

% % Figure environment removed




% \subsection{Accuracy on augmented NYUDv2}
% Our work mainly investigates the effect of focal-ambiguous problem in monocular depth estimation. Therefore, we further test our model on augmented NYUDv2 with only diver focal lengths. Table \ref{nyu_a} concludes the accuracy of our model on augmented NYUDv2 using six common metrics. The same baselines are used in this test, among which the two models NewCRFs and ZoeDepth still achieve the SOTA performance. Our model considerably outperforms these baselines with all the metrics. For example, RMSE of predicted depth maps are reduced by 46.1\% on average compared with the second-best model. It indicates that our model well adapts to single images captured by cameras of diverse focal lengths. Figure \ref{nyua_ap} shows the visual results of predicted depth maps comparing with the second-best model. 

% \begin{table}[]
%   \caption{Accuracy on augmented NYUDv2 with only diverse focal lengths.}
%   \label{nyu_a}
% \setlength{\tabcolsep}{1.5mm}{
% \begin{tabular}{c|cccccc}
% \hline
% Method        & $\delta_{1} \uparrow$ & $\delta_{2} \uparrow$ & $\delta_{3} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & $log_{10} \downarrow$ \\ \hline
% BTS \cite{38}           & 0.531                 & 0.945                 & 0.992                 & 0.197            & 0.651             & 0.097                 \\
% AdaBins \cite{6}       & 0.628                 & 0.957                 & 0.993                 & 0.178            & 0.617             & 0.086                 \\
% LocalBins \cite{7}     & 0.698                 & 0.966                 & 0.995                 & 0.158            & 0.547             & 0.078                 \\
% NeWCRFs \cite{10}       & 0.719                 & 0.974                 & 0.997                 & \underline{0.153}            & 0.518             & \underline{0.074}                 \\
% ZoeDepth \cite{2}      & \underline{0.733}                 & \underline{0.986}                 & \underline{0.998}                 & 0.155            & \underline{0.490}             & \underline{0.074}                 \\ \hline
% \textbf{Ours} & \textbf{0.951}                 & \textbf{0.995}                 & \textbf{0.999}                 & \textbf{0.080}            & \textbf{0.264}             & \textbf{0.034}                 \\
%               & 29.7\%                & 0.9\%                 & 0.1\%                 & 48.4\%           & 46.1\%            & 54.1\%                \\ \hline
% \end{tabular}}
% \end{table}

% % Figure environment removed

% % Figure environment removed


\subsection{Accuracy on original NYUDv2 (seen)}
Our model aims to improve the generalization ability of monocular depth estimation in unseen datasets without reducing its accuracy in seen datasets. Therefore, we test our model on the test examples of original NYUDv2. Notably the training and testing examples on original NYUDv2 are captured by the camera of the same focal length in similar scenes. Table \ref{nyu_o} concludes the results compared with not only the five recent baselines but also more other ones. Our model achieves the same or better results compared with all the baselines. For example, our model slightly outperforms the second-best model \cite{2} by 0.7\% in RMSE. It indicates that incorporating focal length does not reduce the performance of our model on the original NYUDv2.



\subsection{Examples in real scenes}
We additionally test our model in real scenes. Figure \ref{real} gives three test images captured by a mobile phone with different focal lengths at different locations in our lab. We use our FS-Depth model and the second-best model \cite{2} to predict absolute depth maps from these images. The results show that the baseline model wrongly estimates the absolute depth values (\textit{i.e.} distances in meters), though structure of the scene is well predicted. By comparison, our model predicts the absolute depth values of the scene more accurately. It ensures that our model may be well implemented in practical scenarios.





\subsection{Ablation studies}

% \subsection{Compare with other focal length embedding method}
\noindent $\mathbf{Focal \ length \ incorporation.}$
In \textbf{Section 3.4}, we propose multi-scale features to incorporate focal length in multiple layers of the network, denoted â€œmulti-scaleâ€. We verify its effectiveness by comparing with two other ways: (1) our base model ZoeDepth \cite{2} that does not incorporate focal length in the network, denoted â€œbase modelâ€; (2) incorporating focal length in a single layer of our base model following the work \cite{1}, denoted â€œsingle-scaleâ€. Notably the number of nodes in the last fully connected layer is adjusted to 768 to better fuse with the features of the base model. Table \ref{cmp} concludes the results of the ablation study. Our final model consistently achieves the best results. It indicates that focal length is considered more comprehensively in our final model.


\noindent $\mathbf{Learning \ rates.}$
In\textbf{ Section 3.6}, we determine the learning rate of the relative depth estimation network to be $1/50$ of the focal length features generation network and the absolute depth estimation network, denoted â€œ$1/50 \times 1.6^{-4} $â€. We verify its effectiveness by comparing with two general strategies: (1) freezing the learning rate of the relative depth estimation network, denoted â€œfreezeâ€; (2) setting the three learning rates the same, denoted â€œ$1.6^{-4}$â€. Table \ref{lr} concludes the results of the ablation study. We observe that the learning rate has a significant impact on our model. The learning rate of our final model â€œ$1/50 \times 1.6^{-4}$â€ achieves the best results with all metrics. Because the relative depth estimation network does not involve the input of focal length, a higher learning rate may introduce errors caused by the focal length to the relative depth estimation network. Simply freezing the weights of the relative depth estimation network would lead to excessive learning burden, hindering the performance of the absolute depth estimation network. The learning rate in our final model makes a balance between the two cases above.



% \begin{table*}[]
%   \caption{Ablation study of the data augmentation ratio on three unseen datasets. Ratio of Step 3.}
%   \label{ratio}
% \begin{tabular}{c|ccc|ccc|ccc}
% \hline
% \multirow{Ratio}                              & \multicolumn{3}{c|}{iBims-1}                                 & \multicolumn{3}{c|}{DIODE Indoor}                                                                                         & \multicolumn{3}{c}{DIML Indoor}                                                                                          \\
%                                                      & $\delta_{1} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l|}{$RMSE \downarrow$} & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l}{$RMSE \downarrow$} \\ \hline
% 100\% : 0\% & 0.422                 & 0.229            & 0.939             & 0.226                                     & 0.368                                & 1.709                                  & 0.665                                     & 0.171                                & 0.516                                 \\


% \textbf{60\% : 40\%}                                        & \textbf{0.787}        & \textbf{0.145}   & \textbf{0.588}    & \textbf{0.522}                            & \textbf{0.283}                       & \textbf{1.363}                         & \textbf{0.761}                            & \textbf{0.152}                       & \textbf{0.435}                        \\
% 0\% : 100\% & 0.608                 & 0.182            & 0.764             & 0.312                                     & 0.339                                & 1.614                                  & 0.498                                     & 0.205                                & 0.619                                 \\
% \hline
% \end{tabular}
% \end{table*}






\noindent $\mathbf{Data \ augmentation \ ratio.}$
In\textbf{ Section 4}, the RGB-Depth pairs in Step 2 and Step 3 are mixed in a ratio of 60\%:40\% to generate an augmented training dataset with diverse focal lengths. We verify its effectiveness by comparing with two other cases: (1) only Step 2 is activated, denoted â€œ100\%:0\%â€, (2) only Step 3 is activated, denoted â€œ0\%:100\%â€. Table \ref{ratio} concludes the ablation studies on three original unseen datasets. 
% We observe that the ratio has little effect on the accuracy of our model, however, has a significant impact on the generalization ability. 
We observe that the ratio has a significant impact on the generalization ability of monocular depth estimation. 
The ratio of our final model â€œ60\%:40\%â€ consistently achieves the best results with the metrics in all test datasets. In the case of â€œ100\%:0\%â€, the network may be overfitting to a single depth range and fail to well learn the meaning of input focal lengths. In the case of â€œ0\%:100\%â€, some prior knowledge such as the size of objects may be destroyed and monocular depth estimation could not well learn such prior knowledge. The data augmentation ratio in our final model makes a balance between the two cases above.



% He \textit{et al.} \cite{1} first discovered that monocular depth estimation models are affected by the focal length factor, and proposed a method of embedding the focal length using fully connected layers and inputting it into the network. We implemented their focal length input method on ZoeDepth \cite{2} (to better fuse with the features of the base model, we adjusted the number of nodes in the last fully connected layer to 768), and tested it on the original three generalization datasets. The results are shown in Table \ref{cmp}. Our method performed best on all three datasets. ZoeDepth represents the basic method without focal length input.



% \clearpage



% Figure environment removed

% Figure environment removed

\begin{table}[]
  \caption{Accuracy on original NYUDv2 (seen). Incorporating focal length does not reduce the performance of our model on the original NYUDv2.}
  \label{nyu_o}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|cccccc}
\hline
Method                & $\delta_{1} \uparrow$ & $\delta_{2} \uparrow$ & $\delta_{3} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & $log_{10} \downarrow$ \\ \hline
Eigen \textit{et al.} \cite{39} & 0.769                 & 0.950                 & 0.988                 & 0.158            & 0.641             & \_\_\_\_              \\
Laina \textit{et al.} \cite{40} & 0.811                 & 0.953                 & 0.988                 & 0.127            & 0.573             & 0.055                 \\
Hao \textit{et al.} \cite{41}  & 0.841                 & 0.966                 & 0.911                 & 0.127            & 0.555             & 0.053                 \\
DORN \cite{22}                 & 0.828                 & 0.965                 & 0.992                 & 0.115            & 0.509             & 0.051                 \\
SharpNet \cite{42}             & 0.836                 & 0.966                 & 0.993                 & 0.139            & 0.502             & 0.047                 \\
BTS \cite{38}                  & 0.885                 & 0.978                 & 0.994                 & 0.110            & 0.392             & 0.047                 \\
Yin \textit{et al.}   & 0.875                 & 0.976                 & 0.994                 & 0.108            & 0.416             & 0.048                 \\
AdaBins \cite{6}              & 0.903                 & 0.984                 & 0.997                 & 0.103            & 0.364             & 0.044                 \\
LocalBins \cite{7}            & 0.907                 & 0.987                 & \underline{0.998}                 & 0.099            & 0.357             & 0.042                 \\
NeWCRFs  \cite{10}             & 0.922                 & \underline{0.992}                 & \underline{0.998}                 & \underline{0.095}            & 0.334             & \underline{0.041}                 \\
ZoeDepth \cite{2}             & \underline{0.955}                 & \textbf{0.995}        & \textbf{0.999}        & \textbf{0.075}   & \underline{0.270}             & 0.032                 \\ \hline
\textbf{Ours}         & \textbf{0.956}        & \textbf{0.995}        & \textbf{0.999}        & \textbf{0.075}   & \textbf{0.268}    & \textbf{0.032}       \\ \hline
\end{tabular}}
\end{table}


% Figure environment removed



\begin{table}[]
  \caption{Ablation study of the learning rate in relative depth estimation (original NYUDv2).}
  \label{lr}
  \setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{c|cccccc}
\hline
Method     & $\delta_{1} \uparrow$ & $\delta_{2} \uparrow$ & $\delta_{3} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & $log_{10} \downarrow$ \\ \hline
$1.6^{-4}$          & 0.948                 & 0.995                 & 0.998                 & 0.080            & 0.284             & 0.034                 \\
\textbf{$1/50 \times 1.6^{-4}$} & \textbf{0.956}        & \textbf{0.995}        & \textbf{0.999}        & \textbf{0.075}   & \textbf{0.268}    & \textbf{0.033}        \\
freeze          & 0.861                 & 0.984                 & 0.997                 & 0.121            & 0.437             & 0.053                 \\ \hline
\end{tabular}}
\end{table}

\section{Conclusion}
In this paper, we investigated the ill-posed problem of monocular depth estimation in unseen indoor scenes. We developed a focal-and-scale depth estimation model to well learn absolute depth from single images with diverse focal lengths and scene scales. Our model considerably improved the generalization ability of monocular depth estimation on unseen datasets. Notably, our model additionally incorporated focal lengths in monocular depth estimation while focal lengths cannot be directly fed into existing models. Our model provided a possible way to well predict absolute depth values in practical scenarios. In the future, we aim to well improve its generalization ability across indoor and outdoor scenes.


% \section{Experiments and Analysis}

% The title of your work should use capital letters appropriately -
% \url{https://capitalizemytitle.com/} has useful rules for
% capitalization. Use the {\verb|title|} command to define the title of
% your work. If your work has a subtitle, define it with the
% {\verb|subtitle|} command.  Do not insert line breaks in your title.

% If your title is lengthy, you must define a short version to be used
% in the page headers, to prevent overlapping text. The \verb|title|
% command has a ``short title'' parameter:
% \begin{verbatim}
%   \title[short title]{full title}
% \end{verbatim}

% \section{Authors and Affiliations}

% Each author must be defined separately for accurate metadata
% identification. Multiple authors may share one affiliation. Authors'
% names should not be abbreviated; use full first names wherever
% possible. Include authors' e-mail addresses whenever possible.

% Grouping authors' names or e-mail addresses, or providing an ``e-mail
% alias,'' as shown below, is not acceptable:
% \begin{verbatim}
%   \author{Brooke Aster, David Mehldau}
%   \email{dave,judy,steve@university.edu}
%   \email{firstname.lastname@phillips.org}
% \end{verbatim}

% The \verb|authornote| and \verb|authornotemark| commands allow a note
% to apply to multiple authors --- for example, if the first two authors
% of an article contributed equally to the work.

% If your author list is lengthy, you must define a shortened version of
% the list of authors to be used in the page headers, to prevent
% overlapping text. The following command should be placed just after
% the last \verb|\author{}| definition:
% \begin{verbatim}
%   \renewcommand{\shortauthors}{McCartney, et al.}
% \end{verbatim}
% Omitting this command will force the use of a concatenated list of all
% of the authors' names, which may result in overlapping text in the
% page headers.

% The article template's documentation, available at
% \url{https://www.acm.org/publications/proceedings-template}, has a
% complete explanation of these commands and tips for their effective
% use.

% Note that authors' addresses are mandatory for journal articles.

% \section{Rights Information}

% Authors of any work published by ACM will need to complete a rights
% form. Depending on the kind of work, and the rights management choice
% made by the author, this may be copyright transfer, permission,
% license, or an OA (open access) agreement.

% Regardless of the rights management choice, the author will receive a
% copy of the completed rights form once it has been submitted. This
% form contains \LaTeX\ commands that must be copied into the source
% document. When the document source is compiled, these commands and
% their parameters add formatted text to several areas of the final
% document:
% \begin{itemize}
% \item the ``ACM Reference Format'' text on the first page.
% \item the ``rights management'' text on the first page.
% \item the conference information in the page header(s).
% \end{itemize}

% Rights information is unique to the work; if you are preparing several
% works for an event, make sure to use the correct set of commands with
% each of the works.

% The ACM Reference Format text is required for all articles over one
% page in length, and is optional for one-page articles (abstracts).

% \section{CCS Concepts and User-Defined Keywords}

% Two elements of the ``acmart'' document class provide powerful
% taxonomic tools for you to help readers find your work in an online
% search.

% The ACM Computing Classification System ---
% \url{https://www.acm.org/publications/class-2012} --- is a set of
% classifiers and concepts that describe the computing
% discipline. Authors can select entries from this classification
% system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
% commands to be included in the \LaTeX\ source.

% User-defined keywords are a comma-separated list of words and phrases
% of the authors' choosing, providing a more flexible way of describing
% the research being presented.

% CCS concepts and user-defined keywords are required for for all
% articles over two pages in length, and are optional for one- and
% two-page articles (or abstracts).

% \section{Sectioning Commands}

% Your work should use standard \LaTeX\ sectioning commands:
% \verb|section|, \verb|subsection|, \verb|subsubsection|, and
% \verb|paragraph|. They should be numbered; do not remove the numbering
% from the commands.

% Simulating a sectioning command by setting the first word or words of
% a paragraph in boldface or italicized text is {\bfseries not allowed.}

% \section{Tables}

% The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
% package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
% high-quality tables.

% Table captions are placed {\itshape above} the table.

% Because tables cannot be split across pages, the best placement for
% them is typically the top of the page nearest their initial cite.  To
% ensure this proper ``floating'' placement of tables, use the
% environment \textbf{table} to enclose the table's contents and the
% table caption.  The contents of the table itself must go in the
% \textbf{tabular} environment, to be aligned properly in rows and
% columns, with the desired horizontal and vertical rules.  Again,
% detailed instructions on \textbf{tabular} material are found in the
% \textit{\LaTeX\ User's Guide}.

% Immediately following this sentence is the point at which
% Table~\ref{tab:freq} is included in the input file; compare the
% placement of the table here with the table in the printed output of
% this document.

% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}

% To set a wider table, which takes up the whole width of the page's
% live area, use the environment \textbf{table*} to enclose the table's
% contents and the table caption.  As with a single-column table, this
% wide table will ``float'' to a location deemed more
% desirable. Immediately following this sentence is the point at which
% Table~\ref{tab:commands} is included in the input file; again, it is
% instructive to compare the placement of the table here with the table
% in the printed output of this document.

% \begin{table*}
%   \caption{Some Typical Commands}
%   \label{tab:commands}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% Always use midrule to separate table header rows from data rows, and
% use it only for this purpose. This enables assistive technologies to
% recognise table headers and support their users in navigating tables
% more easily.

% \section{Math Equations}
% You may want to display math equations in three distinct styles:
% inline, numbered or non-numbered display.  Each of the three are
% discussed in the next sections.

% \subsection{Inline (In-text) Equations}
% A formula that appears in the running text is called an inline or
% in-text formula.  It is produced by the \textbf{math} environment,
% which can be invoked with the usual
% \texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
% the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
% and structures, from $\alpha$ to $\omega$, available in
% \LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
% examples of in-text equations in context. Notice how this equation:
% \begin{math}
%   \lim_{n\rightarrow \infty}x=0
% \end{math},
% set here in in-line math style, looks slightly different when
% set in display style.  (See next section).

% \subsection{Display Equations}
% A numbered display equation---one set off by vertical space from the
% text and centered horizontally---is produced by the \textbf{equation}
% environment. An unnumbered display equation is produced by the
% \textbf{displaymath} environment.

% Again, in either environment, you can use any of the symbols and
% structures available in \LaTeX\@; this section will just give a couple
% of examples of display equations in context.  First, consider the
% equation, shown as an inline equation above:
% \begin{equation}
%   \lim_{n\rightarrow \infty}x=0
% \end{equation}
% Notice how it is formatted somewhat differently in
% the \textbf{displaymath}
% environment.  Now, we'll enter an unnumbered equation:
% \begin{displaymath}
%   \sum_{i=0}^{\infty} x + 1
% \end{displaymath}
% and follow it with another numbered equation:
% \begin{equation}
%   \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
% \end{equation}
% just to demonstrate \LaTeX's able handling of numbering.

% \section{Figures}

% The ``\verb|figure|'' environment should be used for figures. One or
% more images can be placed within a figure. If your figure contains
% third-party material, you must clearly identify it as such, as shown
% in the example below.
% % Figure environment removed

% Your figures should contain a caption which describes the figure to
% the reader.

% Figure captions are placed {\itshape below} the figure.

% Every figure should also have a figure description unless it is purely
% decorative. These descriptions convey whatâ€™s in the image to someone
% who cannot see it. They are also used by search engine crawlers for
% indexing images, and when images cannot be loaded.

% A figure description must be unformatted plain text less than 2000
% characters long (including spaces).  {\bfseries Figure descriptions
%   should not repeat the figure caption â€“ their purpose is to capture
%   important information that is not already provided in the caption or
%   the main text of the paper.} For figures that convey important and
% complex new information, a short text description may not be
% adequate. More complex alternative descriptions can be placed in an
% appendix and referenced in a short figure description. For example,
% provide a data table capturing the information in a bar chart, or a
% structured list representing a graph.  For additional information
% regarding how best to write figure descriptions and why doing this is
% so important, please see
% \url{https://www.acm.org/publications/taps/describing-figures/}.

% \subsection{The ``Teaser Figure''}

% A ``teaser figure'' is an image, or set of images in one figure, that
% are placed after all author and affiliation information, and before
% the body of the article, spanning the page. If you wish to have such a
% figure in your article, place the command immediately before the
% \verb|\maketitle| command:
% \begin{verbatim}
%   \begin{teaserfigure}
%     % Figure removed
%     \caption{figure caption}
%     \Description{figure description}
%   \end{teaserfigure}
% \end{verbatim}

% \section{Citations and Bibliographies}

% The use of \BibTeX\ for the preparation and formatting of one's
% references is strongly recommended. Authors' names should be complete
% --- use full first names (``Donald E. Knuth'') not initials
% (``D. E. Knuth'') --- and the salient identifying features of a
% reference should be included: title, year, volume, number, pages,
% article DOI, etc.

% The bibliography is included in your source document with these two
% commands, placed just before the \verb|\end{document}| command:
% \begin{verbatim}
%   \bibliographystyle{ACM-Reference-Format}
%   \bibliography{bibfile}
% \end{verbatim}
% where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
% suffix, of the \BibTeX\ file.

% Citations and references are numbered by default. A small number of
% ACM publications have citations and references formatted in the
% ``author year'' style; for these exceptions, please include this
% command in the {\bfseries preamble} (before the command
% ``\verb|\begin{document}|'') of your \LaTeX\ source:
% \begin{verbatim}
%   \citestyle{acmauthoryear}
% \end{verbatim}

%   % Some examples.  A paginated journal article \cite{Abril07}, an
%   % enumerated journal article \cite{Cohen07}, a reference to an entire
%   % issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
%   % monograph/whole book in a series (see 2a in spec. document)
%   % \cite{Harel79}, a divisible-book such as an anthology or compilation
%   % \cite{Editor00} followed by the same example, however we only output
%   % the series if the volume number is given \cite{Editor00a} (so
%   % Editor00a's series should NOT be present since it has no vol. no.),
%   % a chapter in a divisible book \cite{Spector90}, a chapter in a
%   % divisible book in a series \cite{Douglass98}, a multi-volume work as
%   % book \cite{Knuth97}, a couple of articles in a proceedings (of a
%   % conference, symposium, workshop for example) (paginated proceedings
%   % article) \cite{Andler79, Hagerup1993}, a proceedings article with
%   % all possible elements \cite{Smith10}, an example of an enumerated
%   % proceedings article \cite{VanGundy07}, an informally published work
%   % \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
%   %   AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
%   % master's thesis: \cite{anisi03}, an online document / world wide web
%   % resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
%   % (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
%   % and (Case 3) a patent \cite{JoeScientist001}, work accepted for
%   % publication \cite{rous08}, 'YYYYb'-test for prolific author
%   % \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
%   % contain 'duplicate' DOI and URLs (some SIAM articles)
%   % \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
%   % multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
%   % couple of citations with DOIs:
%   % \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
%   % citations: \cite{TUGInstmem, Thornburg01, CTANacmart}. Artifacts:
%   % \cite{R} and \cite{UMassCitations, 1, 2, 3, 4, 5, 6, 7, 8, 9}.

% \section{Acknowledgments}

% Identification of funding sources and other support, and thanks to
% individuals and groups that assisted in the research and the
% preparation of the work should be included in an acknowledgment
% section, which is placed just before the reference section in your
% document.

% This section has a special environment:
% \begin{verbatim}
%   \begin{acks}
%   ...
%   \end{acks}
% \end{verbatim}
% so that the information contained therein can be more easily collected
% during the article metadata extraction phase, and to ensure
% consistency in the spelling of the section heading.

% Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

% \section{Appendices}

% If your work needs an appendix, add it before the
% ``\verb|\end{document}|'' command at the conclusion of your source
% document.

% Start the appendix with the ``\verb|appendix|'' command:
% \begin{verbatim}
%   \appendix
% \end{verbatim}
% and note that in the appendix, sections are lettered, not
% numbered. This document has two appendices, demonstrating the section
% and subsection identification method.

% \section{Multi-language papers}

% Papers may be written in languages other than English or include
% titles, subtitles, keywords and abstracts in different languages (as a
% rule, a paper in a language other than English should include an
% English title and an English abstract).  Use \verb|language=...| for
% every language used in the paper.  The last language indicated is the
% main language of the paper.  For example, a French paper with
% additional titles and abstracts in English and German may start with
% the following command
% \begin{verbatim}
% \documentclass[sigconf, language=english, language=german,
%                language=french]{acmart}
% \end{verbatim}

% The title, subtitle, keywords and abstract will be typeset in the main
% language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
% begin title, subtitle and keywords, can be used to set these elements
% in the other languages.  The environment \verb|translatedabstract| is
% used to set the translation of the abstract.  These commands and
% environment have a mandatory first argument: the language of the
% second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
% of their usage.

% \section{SIGCHI Extended Abstracts}

% The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
% not in Word) produces a landscape-orientation formatted article, with
% a wide left margin. Three environments are available for use with the
% ``\verb|sigchi-a|'' template style, and produce formatted output in
% the margin:
% \begin{itemize}
% \item {\verb|sidebar|}:  Place formatted text in the margin.
% \item {\verb|marginfigure|}: Place a figure in the margin.
% \item {\verb|margintable|}: Place a table in the margin.
% \end{itemize}

% %%
% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
% \clearpage
% \appendix



% \section{Appendix}


% \subsection{Accuracy on augmented NYUDv2}
% Our work mainly investigates the effect of focal-ambiguous problem in monocular depth estimation. Therefore, we further test our model on augmented NYUDv2 with only diver focal lengths. Table \ref{nyu_a} concludes the accuracy of our model on augmented NYUDv2 using six common metrics. The same baselines are used in this test, among which the two models NewCRFs and ZoeDepth still achieve the SOTA performance. Our model considerably outperforms these baselines with all the metrics. For example, RMSE of predicted depth maps are reduced by 46.1\% on average compared with the second-best model. It indicates that our model well adapts to single images captured by cameras of diverse focal lengths. Figure \ref{nyua} and \ref{3dnyu} shows the visual results of predicted depth maps and 3D reconstruction comparing with the second-best model.


% % \subsection{Compare with other focal length embedding method}
% % He \textit{et al.} \cite{1} first discovered that monocular depth estimation models are affected by the focal length factor, and proposed a method of embedding the focal length using fully connected layers and inputting it into the network. We implemented their focal length input method on ZoeDepth \cite{2} (to better fuse with the features of the base model, we adjusted the number of nodes in the last fully connected layer to 768), and tested it on the original three generalization datasets. The results are shown in Table \ref{cmp}. Our method performed best on all three datasets. ZoeDepth represents the basic method without focal length input.



% % \subsection{Examples in real scenes}
% % We additionally test our model in real scenes. Figure \ref{real} gives three test images captured by a mobile phone with different focal lengths or at different locations in our lab. We use our FS-Depth model and the second-best model \cite{2} to predict absolute depth maps from these images. The results show that the baseline model wrongly estimates the absolute depth values (i.e. distances in meters), though structure of the scene is well predicted. By comparison, our model predicts the absolute depth values of the scene more accurately. It ensures that our model may be well implemented in practical scenarios.


% % % Figure environment removed

% \newpage
% \begin{table}[]
%   \caption{Accuracy on augmented NYUDv2 (seen) with only diverse focal lengths.}
%   \label{nyu_a}
% \setlength{\tabcolsep}{1.5mm}{
% \begin{tabular}{c|cccccc}
% \hline
% Method        & $\delta_{1} \uparrow$ & $\delta_{2} \uparrow$ & $\delta_{3} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & $log_{10} \downarrow$ \\ \hline
% BTS \cite{38}           & 0.531                 & 0.945                 & 0.992                 & 0.197            & 0.651             & 0.097                 \\
% AdaBins \cite{6}       & 0.628                 & 0.957                 & 0.993                 & 0.178            & 0.617             & 0.086                 \\
% LocalBins \cite{7}     & 0.698                 & 0.966                 & 0.995                 & 0.158            & 0.547             & 0.078                 \\
% NeWCRFs \cite{10}       & 0.719                 & 0.974                 & 0.997                 & \underline{0.153}            & 0.518             & \underline{0.074}                 \\
% ZoeDepth \cite{2}      & \underline{0.733}                 & \underline{0.986}                 & \underline{0.998}                 & 0.155            & \underline{0.490}             & \underline{0.074}                 \\ \hline
% \textbf{Ours} & \textbf{0.951}                 & \textbf{0.995}                 & \textbf{0.999}                 & \textbf{0.080}            & \textbf{0.264}             & \textbf{0.034}                 \\
% Improvement              & 29.7\%                & 0.9\%                 & 0.1\%                 & 48.4\%           & 46.1\%            & 54.1\%                \\ \hline
% \end{tabular}}
% \end{table}

% % Figure environment removed

% % Figure environment removed


% \subsection{More results}
% We demonstrate more results of our model.
% It can be clearly seen in the reconstructed 3D scenes in Figure \ref{3da_ap}. Our model well alleviates the deformation problem in 3D reconstruction of three augmented unseen datasets such as the regions in red boxes compared with the second-best model. Figure \ref{fanhuao_ap} and \ref{3do_ap} show the visual results of predicted depth maps and reconstructed 3D scenes on three original unseen datasets. 




% % \begin{table*}[]
% %   \caption{Compare with other focal length embedding method on three original  datasets (unseen). ZoeDepth represents the base model without focal length input.}
% %   \label{cmp}
% % \begin{tabular}{c|ccc|ccc|ccc}
% % \hline
% % \multirow{Method}                              & \multicolumn{3}{c|}{iBims-1}                                 & \multicolumn{3}{c|}{DIODE-Indoor}                                                                                         & \multicolumn{3}{c}{DIML-Indoor}                                                                                          \\
% %                                                      & $\delta_{1} \uparrow$ & $REL \downarrow$ & $RMSE \downarrow$ & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l|}{$RMSE \downarrow$} & \multicolumn{1}{l}{$\delta_{1} \uparrow$} & \multicolumn{1}{l}{$REL \downarrow$} & \multicolumn{1}{l}{$RMSE \downarrow$} \\ \hline
% % ZoeDepth & 0.658                 & 0.169            & 0.711             & 0.376                                     & 0.327                                & 1.588                                 & 0.646                                     & 0.177                                & 0.536                                 \\
% % He \textit{et al.} \cite{1} & 0.738                 & 0.151            & 0.637             & 0.471                                     & 0.294                                & 1.428                                  & 0.767                                     & 0.150                                & 0.437                                 \\


% % \textbf{Ours}                                        & \textbf{0.787}        & \textbf{0.145}   & \textbf{0.588}    & \textbf{0.522}                            & \textbf{0.283}                       & \textbf{1.363}                         & \textbf{0.796}                            & \textbf{0.147}                       & \textbf{0.414}                        \\
% % \hline
% % \end{tabular}
% % \end{table*}


% % Figure environment removed


% % Figure environment removed

% % Figure environment removed

% % Figure environment removed










% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.