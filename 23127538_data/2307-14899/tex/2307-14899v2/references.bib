
@inproceedings{reimers2019sentence,
  title={{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3982--3992},
  year={2019}
}

@article{bast2016semantic,
  title={Semantic search on text and knowledge bases},
  author={Bast, Hannah and Buchhold, Bj{\"o}rn and Haussmann, Elmar and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={10},
  number={2-3},
  pages={119--271},
  year={2016},
  publisher={Now Publishers, Inc.}
}

@Book{pustejovsky2012natural,
  title={Natural Language Annotation for Machine Learning: A guide to corpus-building for applications},
  author={Pustejovsky, James and Stubbs, Amber},
  year={2012},
  publisher={O'Reilly Media, Inc.}
}


@inproceedings{kenton2019bert,
  title={{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and  Kristina Toutanova},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}


@inproceedings{thakur-etal-2021-augmented,
    title = "Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
    author = "Thakur, Nandan  and
      Reimers, Nils  and
      Daxenberger, Johannes  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.28",
    doi = "10.18653/v1/2021.naacl-main.28",
    pages = "296--310",
    
}
@article{song2020mpnet,
  title={{MPNet: Masked and Permuted Pre-training for Language Understanding}},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16857--16867},
  year={2020}
}
@article{liu2019roberta,
  title={{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv [cs.CL] preprint arXiv:1907.11692},
  year={2019}
}
@inproceedings{reimers2020making,
  title={Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4512--4525},
  year={2020}
}

@misc{elasticsearch,
  author = {Elasticsearch},
  title = {Elasticsearch},
  howpublished = {\url{https://www.elastic.co/}},
  year = {2010},
  url = {https://www.elastic.co/},
  note = {Accessed: April 6, 2023}
}

@inproceedings{dong2022table,
  title={Table Enrichment System for Machine Learning},
  author={Dong, Yuyang and Oyamada, Masafumi},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={3267--3271},
  year={2022}
}

@misc{mturk_website,
  author = {Amazon},
  title = {Amazon Mechanical Turk},
  howpublished = {\url{https://www.mturk.com/}},
  year = {2023},
  note = {Accessed April 24, 2023}
}

@article{liu2021document,
  title={Document Retrieval for Precision Medicine Using a Deep Learning Ensemble Method},
  author={Liu, Zhiqiang and Feng, Jingkun and Yang, Zhihao and Wang, Lei},
  journal={JMIR Medical Informatics},
  volume={9},
  number={6},
  pages={e28272},
  year={2021},
  publisher={JMIR Publications Toronto, Canada}
}


@inproceedings{bruch2022reneuir,
  title={ReNeuIR: Reaching Efficiency in Neural Information Retrieval},
  author={Bruch, Sebastian and Lucchese, Claudio and Nardini, Franco Maria},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={3462--3465},
  year={2022}
}
@article{yang2019xlnet,
  title={{XLNet}: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{atance2001episodic,
  title={Episodic future thinking},
  author={Atance, Cristina M and O'Neill, Daniela K},
  journal={Trends in cognitive sciences},
  volume={5},
  number={12},
  pages={533--539},
  year={2001},
  publisher={Elsevier}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
@inproceedings{zamfir2019systems,
  title={{Systems Monitoring and Big Data Analysis using the Elasticsearch System}},
  author={Zamfir, Vlad-Andrei and Carabas, Mihai and Carabas, Costin and Tapus, Nicolae},
  booktitle={2019 22nd International Conference on Control Systems and Computer Science (CSCS)},
  pages={188--193},
  year={2019},
  organization={IEEE}
}

@article{johnson2019billion,
  title={{Billion-scale similarity search with GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@misc{Jones1,
author = {Nils Reimers},
year = {2022},
howpublished = {\url{https://www.sbert.net/docs/pretrained_models.html}},
note = {Accessed April 24, 2023}
}



@article{stein2018episodic,
  title={Episodic future thinking reduces delay discounting and cigarette demand: an investigation of the good-subject effect},
  author={Stein, Jeffrey S and Tegge, Allison N and Turner, Jamie K and Bickel, Warren K},
  journal={Journal of behavioral medicine},
  volume={41},
  pages={269--276},
  year={2018},
  publisher={Springer}
}

@article{stein2016unstuck,
  title={Unstuck in time: episodic future thinking reduces delay discounting and cigarette smoking},
  author={Stein, Jeffrey S and Wilson, A George and Koffarnus, Mikhail N and Daniel, Tinuke Oluyomi and Epstein, Leonard H and Bickel, Warren K},
  journal={Psychopharmacology},
  volume={233},
  pages={3771--3778},
  year={2016},
  publisher={Springer}
}

@inproceedings{karpukhin2020dense,
  title={Dense Passage Retrieval for Open-Domain Question Answering},
  author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6769--6781},
  year={2020}
}
@inproceedings{yang2020multilingual,
  title={Multilingual Universal Sentence Encoder for Semantic Retrieval},
  author={Yang, Yinfei and Cer, Daniel and Ahmad, Amin and Guo, Mandy and Law, Jax and Constant, Noah and Abrego, Gustavo Hernandez and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and others},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={87--94},
  year={2020}
}

@inproceedings{10.5555/3295222.3295230,
author = {Lundberg, Scott M. and Lee, Su-In},
title = {A Unified Approach to Interpreting Model Predictions},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4768–4777},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@misc{huggingface,
  author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
  title = {Hugging Face: State-of-the-Art Natural Language Processing},
  howpublished = {[software]},
  year = {2019},
  url = {https://huggingface.co/},
  note = {Accessed: April 6, 2023}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{prasad2020enhancement,
  title={Enhancement of Natural Language to SQL Query Conversion using Machine Learning Techniques},
  author={Prasad, Akshar and Badhya, Sourabh S and Yashwanth, YS and Shetty, Rohan and Shobha, G and Deepamala, N},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={11},
  number={12},
  year={2020},
  publisher={Science and Information (SAI) Organization Limited}
}

@inproceedings{lu2020auto,
  title={{On the Auto-Tuning of Elastic-Search based on Machine Learning}},
  author={Lu, Zhenyan and Chen, Chao and Xin, Jinhan and Yu, Zhibin},
  booktitle={Proceedings of the 2020 1st International Conference on Control, Robotics and Intelligent System},
  pages={150--156},
  year={2020}
}

@article{kim2022suggestion,
  title={A Suggestion on the LDA-Based Topic Modeling Technique Based on ElasticSearch for Indexing Academic Research Results},
  author={Kim, Mi and Kim, Dosung},
  journal={Applied Sciences},
  volume={12},
  number={6},
  pages={3118},
  year={2022},
  publisher={MDPI}
}

@inproceedings{japkowicz2000class,
  title={The class imbalance problem: Significance and strategies},
  author={Japkowicz, Nathalie},
  booktitle={Proc. of the Int’l Conf. on artificial intelligence},
  volume={56},
  pages={111--117},
  year={2000}
}


@article{japkowicz2002class,
  title={The class imbalance problem: A systematic study},
  author={Japkowicz, Nathalie and Stephen, Shaju},
  journal={Intelligent data analysis},
  volume={6},
  number={5},
  pages={429--449},
  year={2002},
  publisher={IOS Press}
}

@article{liu2008exploratory,
  title={Exploratory undersampling for class-imbalance learning},
  author={Liu, Xu-Ying and Wu, Jianxin and Zhou, Zhi-Hua},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={39},
  number={2},
  pages={539--550},
  year={2008},
  publisher={IEEE}
}
@article{johnson2019survey,
  title={Survey on deep learning with class imbalance},
  author={Johnson, Justin M and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={1--54},
  year={2019},
  publisher={Springer}
}

@inproceedings{chidambaram2019learning,
  title={Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model},
  author={Chidambaram, Muthu and Yang, Yinfei and Cer, Daniel and Yuan, Steve and Sung, Yunhsuan and Strope, Brian and Kurzweil, Ray},
  booktitle={Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)},
  pages={250--259},
  year={2019}
}

@inproceedings{thakurbeir,
  title={{BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models}},
  author={Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
   year={2021}
}





























