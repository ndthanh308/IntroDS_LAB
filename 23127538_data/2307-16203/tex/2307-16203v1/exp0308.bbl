% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{cipolla2013machine}
R.~Cipolla, S.~Battiato, and G.~M. Farinella, \emph{Machine Learning for
  Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2013,
  vol.~5.

\bibitem{pallathadka2021impact}
H.~Pallathadka, M.~Mustafa, D.~T. Sanchez, G.~S. Sajja, S.~Gour, and M.~Naved,
  ``Impact of machine learning on management, healthcare and agriculture,''
  \emph{Materials Today: Proceedings}, 2021.

\bibitem{dixon2020machine}
M.~F. Dixon, I.~Halperin, and P.~Bilokon, \emph{Machine Learning in
  Finance}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, vol. 1406.

\bibitem{athey2018impact}
S.~Athey, ``The impact of machine learning on economics,'' in \emph{The
  economics of artificial intelligence: An agenda}.\hskip 1em plus 0.5em minus
  0.4em\relax University of Chicago Press, 2018, pp. 507--547.

\bibitem{bowling2006machine}
M.~Bowling, J.~F{\"u}rnkranz, T.~Graepel, and R.~Musick, ``Machine learning and
  games,'' \emph{Machine learning}, vol.~63, no.~3, pp. 211--215, 2006.

\bibitem{shawe2004kernel}
J.~Shawe-Taylor, N.~Cristianini \emph{et~al.}, \emph{Kernel methods for pattern
  analysis}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge University Press,
  2004.

\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville, \emph{Deep Learning}.\hskip 1em
  plus 0.5em minus 0.4em\relax MIT Press, 2016.

\bibitem{deng2014deep}
L.~Deng, D.~Yu \emph{et~al.}, ``Deep learning: methods and applications,''
  \emph{Foundations and trends{\textregistered} in signal processing}, vol.~7,
  no. 3--4, pp. 197--387, 2014.

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton, ``Deep learning,'' \emph{nature}, vol. 521,
  no. 7553, pp. 436--444, 2015.

\bibitem{kayhan2020translation}
O.~S. Kayhan and J.~C.~v. Gemert, ``On translation invariance in cnns:
  Convolutional layers can exploit absolute spatial location,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2020, pp. 14\,274--14\,285.

\bibitem{zhang2019rotation}
Z.~Zhang, B.-S. Hua, D.~W. Rosen, and S.-K. Yeung, ``Rotation invariant
  convolutions for 3d point clouds deep learning,'' in \emph{2019 International
  conference on 3d vision (3DV)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2019, pp. 204--213.

\bibitem{chatzidakis2019towards}
M.~Chatzidakis and G.~Botton, ``Towards calibration-invariant spectroscopy
  using deep learning,'' \emph{Scientific reports}, vol.~9, no.~1, pp. 1--10,
  2019.

\bibitem{bengio2009learning}
Y.~Bengio, ``Learning deep architectures for ai,'' \emph{Foundations and
  Trends{\textregistered} in Machine Learning}, vol.~2, no.~1, pp. 1--127,
  2009.

\bibitem{brahma2015deep}
P.~P. Brahma, D.~Wu, and Y.~She, ``Why deep learning works: A manifold
  disentanglement perspective,'' \emph{IEEE transactions on neural networks and
  learning systems}, vol.~27, no.~10, pp. 1997--2008, 2015.

\bibitem{bengio2013representation}
Y.~Bengio, A.~Courville, and P.~Vincent, ``Representation learning: A review
  and new perspectives,'' \emph{IEEE Transactions on Pattern Analysis and
  Machine Intelligence}, vol.~35, no.~8, pp. 1798--1828, 2013.

\bibitem{yarotsky2017error}
D.~Yarotsky, ``Error bounds for approximations with deep relu networks,''
  \emph{Neural Networks}, vol.~94, pp. 103--114, 2017.

\bibitem{chui2020realization}
C.~K. Chui, S.-B. Lin, B.~Zhang, and D.-X. Zhou, ``Realization of spatial
  sparseness by deep relu nets with massive data,'' \emph{IEEE Transactions on
  Neural Networks and Learning Systems}, 2020.

\bibitem{schwab2019deep}
C.~Schwab and J.~Zech, ``Deep learning in high dimension: Neural network
  expression rates for generalized polynomial chaos expansions in uq,''
  \emph{Analysis and Applications}, vol.~17, no.~01, pp. 19--55, 2019.

\bibitem{lin2018generalization}
S.-B. Lin, ``Generalization and expressivity for deep nets,'' \emph{IEEE
  transactions on neural networks and learning systems}, vol.~30, no.~5, pp.
  1392--1406, 2018.

\bibitem{chui2019deep}
C.~K. Chui, S.-B. Lin, and D.-X. Zhou, ``Deep neural networks for
  rotation-invariance approximation and learning,'' \emph{Analysis and
  Applications}, vol.~17, no.~05, pp. 737--772, 2019.

\bibitem{mhaskar2016deep}
H.~N. Mhaskar and T.~Poggio, ``Deep vs. shallow networks: An approximation
  theory perspective,'' \emph{Analysis and Applications}, vol.~14, no.~06, pp.
  829--848, 2016.

\bibitem{han2020depth}
Z.~Han, S.~Yu, S.-B. Lin, and D.-X. Zhou, ``Depth selection for deep relu nets
  in feature extraction and generalization,'' \emph{IEEE Transactions on
  Pattern Analysis and Machine Intelligence}, 2020.

\bibitem{shaham2018provable}
U.~Shaham, A.~Cloninger, and R.~R. Coifman, ``Provable approximation properties
  for deep neural networks,'' \emph{Applied and Computational Harmonic
  Analysis}, vol.~44, no.~3, pp. 537--557, 2018.

\bibitem{kohler2016nonparametric}
M.~Kohler and A.~Krzy{\.z}ak, ``Nonparametric regression based on hierarchical
  interaction models,'' \emph{IEEE Transactions on Information Theory},
  vol.~63, no.~3, pp. 1620--1630, 2016.

\bibitem{petersen2020equivalence}
P.~Petersen and F.~Voigtlaender, ``Equivalence of approximation by
  convolutional neural networks and fully-connected networks,''
  \emph{Proceedings of the American Mathematical Society}, vol. 148, no.~4, pp.
  1567--1581, 2020.

\bibitem{oono2019approximation}
K.~Oono and T.~Suzuki, ``Approximation and non-parametric estimation of
  resnet-type convolutional neural networks,'' in \emph{International
  conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2019, pp. 4922--4931.

\bibitem{zhou2018deep}
D.-X. Zhou, ``Deep distributed convolutional neural networks: Universality,''
  \emph{Analysis and Applications}, vol.~16, no.~06, pp. 895--919, 2018.

\bibitem{zhou2020universality}
------, ``Universality of deep convolutional neural networks,'' \emph{Applied
  and Computational Harmonic Analysis}, vol.~48, no.~2, pp. 787--794, 2020.

\bibitem{hanin2017approximating}
B.~Hanin and M.~Sellke, ``Approximating continuous functions by relu nets of
  minimal width,'' \emph{arXiv preprint arXiv:1710.11278}, 2017.

\bibitem{lin2022universal}
S.-B. Lin, K.~Wang, Y.~Wang, and D.-X. Zhou, ``Universal consistency of deep
  convolutional neural networks,'' \emph{IEEE Transactions on Information
  Theory}, 2022.

\bibitem{gyorfi2002distribution}
L.~Gy{\"o}rfi, M.~Kohler, A.~Krzy{\.z}ak, and H.~Walk, \emph{A
  Distribution-free Theory of Nonparametric Regression}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2002, vol.~1.

\bibitem{cucker2007learning}
F.~Cucker and D.~X. Zhou, \emph{Learning Theory: an Approximation Theory
  Viewpoint}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge University
  Press, 2007, vol.~24.

\bibitem{petersen2018optimal}
P.~Petersen and F.~Voigtlaender, ``Optimal approximation of piecewise smooth
  functions using deep relu neural networks,'' \emph{Neural Networks}, vol.
  108, pp. 296--330, 2018.

\bibitem{schmidt2020nonparametric}
J.~Schmidt-Hieber, ``Nonparametric regression using deep neural networks with
  relu activation function,'' \emph{The Annals of Statistics}, vol.~48, no.~4,
  pp. 1875--1897, 2020.

\bibitem{lin2017does}
H.~W. Lin, M.~Tegmark, and D.~Rolnick, ``Why does deep and cheap learning work
  so well?'' \emph{Journal of Statistical Physics}, vol. 168, no.~6, pp.
  1223--1247, 2017.

\bibitem{safran2017depth}
I.~Safran and O.~Shamir, ``Depth-width tradeoffs in approximating natural
  functions with neural networks,'' in \emph{International conference on
  machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp.
  2979--2987.

\bibitem{gu2018recent}
J.~Gu, Z.~Wang, J.~Kuen, L.~Ma, A.~Shahroudy, B.~Shuai, T.~Liu, X.~Wang,
  G.~Wang, J.~Cai \emph{et~al.}, ``Recent advances in convolutional neural
  networks,'' \emph{Pattern recognition}, vol.~77, pp. 354--377, 2018.

\bibitem{zhou2020theory}
D.-X. Zhou, ``Theory of deep convolutional neural networks: Downsampling,''
  \emph{Neural Networks}, vol. 124, pp. 319--327, 2020.

\bibitem{daubechies1992ten}
I.~Daubechies, \emph{Ten Lectures on Wavelets}.\hskip 1em plus 0.5em minus
  0.4em\relax SIAM, 1992.

\end{thebibliography}
