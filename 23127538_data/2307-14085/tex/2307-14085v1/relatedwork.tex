\vspace{5pt}
{\noindent \bf Online Single-Agent Reinforcement Learning with Function Approximation.} 
Our works is related to the line of work that designs provably sample-efficient RL algorithms for MDPs in the context of function approximation.
Various works propose  RL algorithms that focus on the linear case, \citep{wang2019optimism, yang2019sample, cai2020provably, jin2020provably, 
zanette2020frequentist, ayoub2020model, modi2020sample, yang2020provably, zhou2021nearly}. 
Among these works, our work is most related to \citet{jin2020provably}. In particular, the linear Markov game model we study for the linear case is a direct extension of the linear MDP model in \citet{jin2020provably}. 
Moreover, the bonus function $\Gamma_h^{(1)}$ 
%in \eqref{eq:linear ridge-neurips} 
and the $U$-function (which plays the same role as the Q-function in single-agent RL) constructed in \eqref{eq:linear ridge} are the  same as the UCB bonus function and the optimistic value function in \citet{jin2020provably}. 
Furthermore, more recently, there is a line of research that study  sample-efficient RL in the context of general function approximation. See, e.g., \citet{jiang@2017, sun2019model, wang2020reinforcement, jin2021bellman, du2021bilinear, dann2021provably, zhong2022posterior, foster2021statistical, foster2023tight} and the references therein.
Among these works, our work is particularly related to the work of \citet{jin2021bellman}, which proposes a model-free algorithm named GOLF that is an optimistic variant of least-squares value iteration. 
For the case of online RL with  general function approximation,
our algorithm for updating the leader's value functions is based on a modified version of GOLF, where we additionally take into account the estimation uncertainty of the follower's  quantal response model. 
Moreover, \citet{jin2021bellman} introduce the Bellman eluder dimension that characterizes the exploration 
difficulty of the MDP problem. 
In the regret analysis, we introduce 
similar notions of eluder dimension for learning the leader's optimal policy. 
In particular, we introduce two versions of  eluder dimensions that captures the complexity of leader's Bellman error and the follower's quantal response error.  

\vspace{5pt}
{\noindent \bf Online Multi-Agent Reinforcement Learning.} 
Our work is also related to the literature on online multi-agent RL. 
Most of the existing research focus on learning Markov perfect equilibria in two-agent zero-sum Markov games or correlated or coarse correlated equilibria in general-sum Markov games. 
These works can be divided into two strands depending whether the proposed algorithm is centralized or decentralized. 
When there is a central controller that learns the policies of all agents, 
a few recent works propose extensions of single-agent RL algorithms to Markov games based on the principle of   optimism in the face of uncertainty \citep{bai2020near,bai2020provable,liu2020sharp, jin2021power, huang2021towards, xiong22b, xie2020learning, chen2021almost}. 
The second strand of research develops decentralized online RL algorithm for a single-agent of in a Markov games. 
See, e.g., \citet{jin2021v,liu2022learning, zhan2022decentralized, songcan,tian2021online,  mao2023provably, erez2022regret,  wang2023breaking,cui2023breaking} and the references therein. 
The algorithms proposed in most of these works handle the nonstationary due to other agents by leveraging techniques from adversarial bandit literature. 

Our work is more related to works that learns Stackelberg equilibria in (Markov) games via online RL \citep{bai2021sample, zhong2021can, kao2022decentralized,  zhao2023online}.
All of these works assume the follower is myopic and perfectly rational. 
In specific, \citet{bai2021sample, zhao2023online} focus on the static setting.  
\citet{bai2021sample} consider a centralized setting where central controller can determines the actions taken by both the leader and the follower. \citet{zhao2023online} assume the follower is omniscient in the sense that the follower always plays the best response policy, which is similar to our setting. They show that when the  follower is perfectly rational, the regret of the leader  exhibits different scenarios depending on the relationship between the leader's and follower's rewards. Besides, \citet{kao2022decentralized} assume that the leader and follower are cooperative and design a decentralized algorithm for both the leader and follower, under the tabular setting.  \citet{zhong2021can} study online and offline RL for the leader, assuming the follower's reward function is known, and thus the best response of the follower is known to the leader. 




 \vspace{5pt}
{\noindent \bf Offline Reinforcement Learning with Pessimism.} 
Our work is also related to the recent line of research on the efficacy of pessimism in offline RL. See, e.g., \citep{yu2020mopo, kidambi2020morel, kumar2020conservative, buckman2020importance, jin2021pessimism, rashidinejad2021bridging, zanette2021provable, uehara2021pessimistic,  xie2021bellman, lyu2022pessimism, shi2022pessimistic, yan2022efficacy, zhong2022pessimistic, cui2022offline, cui2022provably, yu2022strategic, zhang2023offline} and the references therein for algorithms for MDP or Markov games based on  the pessimism principle. 
Among these works, our work is particularly related to \citet{jin2021pessimism, xie2021bellman, uehara2021pessimistic, yu2022strategic}. 
In particular, in the case of myopic follower and linear function approximation, our offline RL is based on the LSVI-LCB algorithm introduced in \citep{jin2021pessimism}, which uses a penalty function to perform pessimism. 
Furthermore, in the case of myopic follower and   general function approximation, our algorithm is an extension of the value-based pessimistic algorithm proposed in \citet{xie2021bellman} to leader-follower game, and when the follower is farsighted, our algorithm is an extension of the model-based pessimistic algorithm proposed in \citet{uehara2021pessimistic}. 
Finally, \citet{yu2022strategic} studies a different leader-follower game with myopic followers and they propose a model-based and pessimistic algorithm that leverages nonparametric instrumental variable regression. 

 \vspace{5pt}
{\noindent \bf Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL).} 
Our approach of learning the quantal response mapping via maximum likelihood estimation is related to the existing works on MaxEnt IRL, where the goal is to recover the reward function from expert trajectories based on an energy-based model \citep{ziebart2008maximum, neu2009training, ziebart2010modeling, choi2012nonparametric, gleave2022primer, zhu2023principled}. 
Among these works, our work is more relevant to  \citet{zhu2023principled}, which  establishes the sample complexity of MaxEnt IRL, and learning reward functions from comparison data in both bandits and MDPs. 
The analysis in \citet{zhu2023principled} builds upon the body of literature on learning from comparisons \citep{bradley1952rank, plackett1975analysis, luce2012individual, hajek2014minimax,shah2015estimation, negahban2012iterative}. 
\citet{zhu2023principled} considers maximum likelihood estimation with linear rewards, whereas we also consider general function approximation. More importantly, learning quantal response model from follower's actions is only the lower level problem, and our eventual goal is to learn the optimal policy of the leader. Such a bilevel structure is not considered in  \citet{zhu2023principled}. 
  

 