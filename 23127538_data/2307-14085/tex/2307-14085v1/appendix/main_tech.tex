\section{More Details of Technical Ingredients}\label{sec:app-major-tech}

In this section, we summarize and provide  proofs of the  important techniques used for analyzing the QSE. The following is a table of addition constants used in this section. 
\begin{table}[h!]
    \centering
    {\setlength\doublerulesep{1pt}
    \begin{tabular}{p{2cm}|p{10.5cm}}
        \toprule[2pt]\midrule[0.5pt]
        Notations & Interpretations \\ \toprule[1.5pt]
        $C^{(0)}$   & $C^{(0)}=2\eta H$ \\\midrule
        $C^{(1)}$ & $C^{(1)}=\eta^2 H   \bigrbr{1+ 4 \eff_H(\gamma)}\cdot \exp(2\eta B_A)$ \\\midrule
        $C^{(2)}$ & $
           C^{(2)}   =  2  \eta^2 H^2\cdot \exp\orbr{6\eta B_A} \cdot(1+4 \eff_H(\gamma)) \cdot \rbr{\eff_H(\exp(2\eta B_A)\gamma)}^2$ 
        \\\midrule
        $C^{(3)}$ & $C^{(3)}={\eta^2 \exp(2\eta B_A)}\bigrbr{2+\eta B_A \cdot  \exp\rbr{2\eta B_A}}/2$
        \\\midrule
        $L^{(1)}$ & $L^{(1)} = 6(\eta^{-1}+2 B_A)\cdot {\eff_H\rbr{\gamma}}$
        \\\midrule 
        $L^{(2)}$ & $L^{(2)} = c H^2 \eff_H(c_2)^2 \kappa^2 \exp\rbr{8\eta B_A} (\eta^{-1}+B_A)^2$, where 
        $c_2 = \gamma(2\exp(2\eta B_A)+\kappa\exp(4\eta B_A))$, and $c$ is a universal constant.\\
        \bottomrule[2pt]
    \end{tabular}
    }
    \caption{Constants used for \Cref{sec:app-major-tech}}\label{tab:}
\end{table}

\subsection{Performance Difference Lemma for for QSE}\label{sec:app-subopt-decompose}

In this subsection, we further elaborate on the performance difference lemma introduced in \S\ref{sec:subopt decomposition}. 
For generality, we consider the farsighted case.  
In the following,  
we consider a fixed policy $\pi$ and let its quantal response under the true model be $\nu^{\pi}$. 
Let $\tilde \nu$ be an estimate of $\nu^{\pi}$ and let $\tilde U$ and $\tilde W$ be any estimates of  $U^{\pi} $ and $  W^{\pi}$, which are defined respectively in    \eqref{eq:U_function} and \eqref{eq:W_function}.
We note that $\tilde W$ and $\tilde U$ not necessarily satisfy $
\tilde W_h(s ) = \la \tilde U_h (s, \cdot , \cdot ) , \pi _h \otimes \tilde \nu_h^{\pi} (\cdot , \cdot \given s) \ra . 
$.
We present a slightly more general version of   the performance difference lemma, which directly implies \eqref{eq:performance diff-1}.

% {\main Lemma \ref{lem:performance diff informal}\fi} {\neurips \eqref{eq:performance diff-1}\fi}. 


\begin{lemma}[Performance Difference]\label{lem:subopt-decomposition} 
For any fixed policy $\pi$, 
let $\tilde \nu$ be an estimate of the quantal response $\nu^{\pi}$ and let $\tilde U$ and $\tilde W$ be estimates of $U^\pi$ and $W^{\pi}$ respectively. 
Based on $\tilde U$ and $\tilde W$, we can estimate $J(\pi)$ defined in \eqref{eq:J} by 
\$
\EE_{s_1\sim\rho_0}\bigl [ \orbr{T_1^{\pi,\tilde\nu}\tilde U_1} (s_1 )\bigr ]  \qquad \textrm{and} \qquad \EE_{s_1\sim\rho_0}\bigl [ \tilde W_1(s_1)\bigr]  , 
\$
where we operator $T_h^{\pi,\tilde\nu}$ is defined in \eqref{eq:operator_T_pi_nu}.
The error or these estimators can be bounded as follows: 
\begin{align}
    &\EE_{s_1\sim\rho_0}\bigsbr{ \orbr{T_1^{\pi,\tilde\nu}\tilde U_1}(s_1 )} - J(\pi ) \nend
    %%%%%%
    % &\quad = {\sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}} + \sum_{h=1}^H \EE\sbr{\tilde W_h(s_h)-\tilde U_h(s_h, a_h, b_h)}\nend
    %%%%%%
    &\quad\le\underbrace{{\sum_{h=1}^H \EE\Bigsbr{\orbr{\tilde U_h - u_h}(s_h, a_h, b_h)-  \orbr{T_{h+1}^{\pi,\tilde\nu} \tilde U_{h+1}}(s_{h+1})}}}_{\dr \text{Leader's Bellman error}}+ \underbrace{\sum_{h=1}^H   H  \cdot \EE \bigsbr{ \nbr{\rbr{\tilde \nu_h-\nu_h^{\pi }}(\cdot\given s_h)}_1} }_{\dr \text{Quantal response error}}\label{eq:perform-diff-general}, \\
    & \EE_{s_1\sim\rho_0}\bigsbr{\tilde W_1(s_1)}  - J(\pi)\nend
    %%%%%%%%
    &\quad=\underbrace{{\sum_{h=1}^H \EE\bigsbr{\orbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}}}_{\dr \text{Leader's Bellman error}} + \underbrace{\sum_{h=1}^H \EE\bigsbr{\tilde W_h(s_h) -  \orbr{T_{h}^{\pi,\tilde\nu} \tilde U_{h}} (s_{h})}}_{\ds\text{Value mismatch error}}\nend
    &\qqquad+\underbrace{\sum_{h=1}^H  H  \cdot  \EE  \bigsbr{ \nbr{\rbr{\tilde \nu_h-\nu_h^{\pi }}(\cdot\given s_h)}_1} }_{\dr \text{Quantal response error}}, 
    \label{eq:perform-diff-linear} 
\end{align}
where  the expectation is taken with respect to the randomness of the trajectory generated by $(\pi, \nu^{\pi})$ on the true model $M^*$. 

 

\end{lemma}



Here, we provide two ways to estimate $J(\pi)$.
The first inequality \eqref{eq:perform-diff-general} is useful for  setting with general function approximation, and the second inequality \eqref{eq:perform-diff-linear} becomes handy particularly in the   setting with linear function approximation, where we  additionally introduced an algorithm with penalty/bonus terms. 

This lemma shows that the estimation error of $\EE_{s_1\sim\rho_0}\bigl [ \orbr{T_1^{\pi,\tilde\nu}\tilde U_1} (s_1 )\bigr ] $ can be decomposed into a sum of three terms --- leader's Bellman error, the error of the estimated quantal response, and an additional term that measures the mismatch between $\tilde W$ and $\tilde U$, which is included here for generality and will be used in the analysis of both \Cref{alg:PMLE} and \Cref{alg:MLE-OVI} where some penalties/bonuses are included in the estimation of $\hat W$.

Furthermore,  by the definition of 
$T_h^{\pi,\tilde\nu}$ in \eqref{eq:operator_T_pi_nu},
%  {\main\eqref{eq:relation_tileWU}\fi} {\neurips $
%  \tilde W_h(s ) = \la \tilde U_h (s, \cdot , \cdot ) , \pi _h \otimes \tilde \nu_h^{\pi} (\cdot , \cdot \given s) \ra . 
%  $\fi}  
 $
 \tilde W_h(s ) = \la \tilde U_h (s, \cdot , \cdot ) , \pi _h \otimes \tilde \nu_h^{\pi} (\cdot , \cdot \given s) \ra
 $
 is equivalent to $\tilde W_ h = T_h^{\pi, \tilde \nu} \tilde U_h$. 
Thus, 
\eqref{eq:perform-diff-linear} directly implies \eqref{eq:performance diff-1}
% {\main Lemma \ref{lem:performance diff informal}\fi} {\neurips \eqref{eq:performance diff-1}\fi} 
as a special case.

% \begin{align}
%     &\EE_{s_1\sim\rho_0}\sbr{ T_1^{\pi,\tilde\nu}\tilde U_1(s_1, a_1, b_1)} - J(\pi ) \nend
%     %%%%%%
%     % &\quad = {\sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}} + \sum_{h=1}^H \EE\sbr{\tilde W_h(s_h)-\tilde U_h(s_h, a_h, b_h)}\nend
%     %%%%%%
%     &\quad\le\underbrace{{\sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)-  T_{h+1}^{\pi,\tilde\nu} \tilde U_{h+1}(s_{h+1})}}}_{\dr \text{Leader's Bellman error}}+ \underbrace{\sum_{h=1}^H  H \EE \nbr{\rbr{\tilde \nu_h-\nu_h}(\cdot\given s_h)}_1}_{\dr \text{Quantal response error}}\label{eq:perform-diff-general}\\
%     %%%%%%%%
%     &\quad=\underbrace{{\sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}}}_{\dr \text{Leader's Bellman error}} + \underbrace{\sum_{h=1}^H \EE\sbr{\tilde W_h(s_h) -  T_{h}^{\pi,\tilde\nu} \tilde U_{h}(s_{h})}}_{\ds\text{Value mismatch error}}\nend
%     &\qqquad+\underbrace{\sum_{h=1}^H  H \EE \nbr{\rbr{\tilde \nu_h-\nu_h}(\cdot\given s_h)}_1}_{\dr \text{Quantal response error}}, 
%     \label{eq:perform-diff-linear}
% \end{align}
 




\begin{proof}
By the definitions of $U^{\pi}$ and $W^{\pi}$ in \eqref{eq:U_function} and \eqref{eq:W_function}, $J(\pi)$ can be written as 
\$
J(\pi) = \EE_{s_1 \sim \rho_0 } [ W_1 ^{\pi} (s_1) ] = \EE_{s_1 \sim \rho_0 } [  \orbr{T_1 ^{\pi, \nu}U_1 ^{\pi} } (s_1) ],
\$ 
where we write $\nu = \nu^{\pi}$ to simplify the notation.
Recall that we define the quantal Bellman operator $\TT_h^{\pi}$ in \eqref{eq:bellman_operator_leader}, whose fixed point is $U^{\pi}$.
Then, by direct calculation, we have 
\#\label{eq:subopt-decompose-equality-0}
&\EE_{s_1\sim\rho_0}\bigsbr{ \orbr{T_1^{\pi,\tilde\nu}\tilde U_1}(s_1 )} - J(\pi )    \nend 
& \quad  =    \EE_{s_1\sim\rho_0}\bigsbr{\orbr{T_1^{\pi,\tilde\nu} - T_1^{\pi,\nu}} \tilde U_1(s_1)}  + \EE_{s_1\sim\rho_0}\bigsbr{\orbr{  T_1^{\pi,\nu}  \tilde U_1 - T_1^{\pi,\nu}  U_1^{\pi} } (s_1)}. 
\#
Furthermore, using the Bellman equation of $U^{\pi}$, we have 
\#\label{eq:subopt-decompose-equality-01}
& \EE_{s_1\sim\rho_0}\bigsbr{\orbr{  T_1^{\pi,\nu}  \tilde U_1 - T_1^{\pi,\nu}  U_1^{\pi} } (s_1)} 
= \EE \bigsbr{  \tilde U_1(s_1, s_1, a_1) - U_1 ^{\pi} (s_1, a_1, b_1)}\nend 
& \quad  = \EE \bigsbr{  \orbr{\tilde U_1 -u_1} (s_1, s_1, a_1) - T_{2}^{\pi,\nu} \tilde U_{2}(s_2 ) }  +  \EE\bigsbr{ T_{2}^{\pi,\nu} \tilde U_{2} (s_2) - T_{2}^{\pi,\nu}U_{2}^{\pi}  (s_2) }. 
\# 
Here the second equality follows from the Bellman equation, and the expectation is taken with respect to the randomness of the trajectory generated by $(\pi, \nu^{\pi})$ on the true model $M^*$. . 
Furthermore, by replacing $T_{2}^{\pi,\nu} \tilde U_{2}$ by $T_{2}^{\pi, \tilde \nu} \tilde U_{2}$ in \eqref{eq:subopt-decompose-equality-01}, and combining \eqref{eq:subopt-decompose-equality-0}, we obtain that 
\# 
&\EE_{s_1\sim\rho_0}\bigsbr{ \orbr{T_1^{\pi,\tilde\nu}\tilde U_1}(s_1 )} - J(\pi )    \nend 
     & \quad 
     =  {\sum_{h=1}^H \EE\bigsbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)-  \orbr{T_{h+1}^{\pi,\tilde\nu} \tilde U_{h+1}}(s_{h+1})}} + \sum_{h=1}^H \EE\bigsbr{\orbr{ T_h^{\pi,\tilde\nu}-  T_h^{\pi,\nu}} \tilde U_h(s_h)} ,\label{eq:subopt-decompose-equality-1}
\# 
where we apply recursion over all $h\in [H]$. 
Finally, note that $\tilde U_h $ is bounded by $H$ in the $\ell_{\infty}$-norm. Using H\"older's inequality, we have 
\#
\EE\bigsbr{\orbr{ T_h^{\pi,\tilde\nu}-  T_h^{\pi,\nu}} \tilde U_h(s_h)} \leq H \cdot \EE \bigsbr{ \nbr{\rbr{\tilde \nu_h-\nu_h}(\cdot\given s_h)}_1}.\label{eq:subopt-decompose-equality-12}
\#
Combining \eqref{eq:subopt-decompose-equality-1} and \eqref{eq:subopt-decompose-equality-12}, we establish \eqref{eq:perform-diff-general}.

 
It remains to prove  \eqref{eq:perform-diff-linear}. 
To this end, it suffices to incorporate the value mismatch error in \eqref{eq:perform-diff-linear} into \eqref{eq:perform-diff-general}.
Specifically,  for any $h \in [H]$, we have 
\#\label{eq:subopt-decompose-equality-2}
& \EE\bigsbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)-  T_{h+1}^{\pi,\tilde\nu} \tilde U_{h+1}(s_{h+1})} \notag \\
& \quad    ={\sum_{h=1}^H \EE\bigsbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}} + \sum_{h=2}^H \EE\bigsbr{\tilde W_h(s_h) - T_h^{\pi,\tilde\nu}\tilde U_h(s_h)}. 
\#
Combining \eqref{eq:perform-diff-general} and \eqref{eq:subopt-decompose-equality-2}
we have 
\$
& \EE_{s_1\sim\rho_0}\bigsbr{ \tilde W_1(s_1)} - J(\pi) \notag \\
& \quad = \EE\bigsbr{\tilde W_1(s_1) - \orbr{T_1^{\pi,\tilde\nu}\tilde U_1}(s_1)} +  \EE\bigsbr{ \orbr{T_1^{\pi,\tilde\nu}\tilde U_1} (s_1)} - J(\pi) 
\notag \\
&  \quad\le {\sum_{h=1}^H \EE\bigsbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}} + \sum_{h=1}^H \EE\bigsbr{\tilde W_h(s_h) - \orbr{T_h^{\pi,\tilde\nu}\tilde U_h }(s_h)}\nend
&\qqquad+ \sum_{h=1}^H  H \cdot  \EE \bigsbr{ \nbr{\rbr{\tilde \nu_h-\nu_h}(\cdot\given s_h)}_1} ,
\$ 
which gives us \eqref{eq:perform-diff-linear}. 
    Hence, we complete the proof.
% where $H$ upper bound $\tilde U$. The first equality holds by a switch between $T_1^{\pi, \tilde\nu}$ and $T_1^{\pi, \nu}$, the second equality holds by doing this switching recursively and noting that $\nu_{H+1}=\tilde\nu_{H+1}$ since it is out of the horizon, and the last inequality holds by the difinition of $\nbr{\cdot}_1$-norm. Hence, we obtain \eqref{eq:perform-diff-general}. If we include $\tilde W$, we just decompose all the $T_{h+1}^{\pi,\tilde\nu}\tilde U_{h+1}(s_{h+1})$ terms for $h=1,\dots, H$ on the righthand side of \eqref{eq:perform-diff-general} and also the $T_1^{\pi,\tilde\nu}\tilde U_1(s_1, a_1, b_1)$ term on the left hand side and obtain 
%     \begin{align}
%         &\EE_{s_1\sim\rho_0}\sbr{ \tilde W_1(s_1)} - J(\pi) \nend
%     %%%%%%
%     &\quad =  {\sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)-  T_{h+1}^{\pi,\tilde\nu} \tilde U_{h+1}(s_{h+1})}} + \sum_{h=1}^H \EE\sbr{\rbr{ T_h^{\pi,\tilde\nu}-  T_h^{\pi,\nu}} \tilde U_h(s_h)}\nend
%     &\qqquad + \EE\sbr{\tilde W_1(s_1) - T_1^{\pi,\tilde\nu}\tilde U_1(s_1)}\nend
%     %%%%%%%%
%     &\quad={\sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}} + \sum_{h=1}^H \EE\sbr{\tilde W_h(s_h) - T_h^{\pi,\tilde\nu}\tilde U_h(s_h)}\nend
%     &\qqquad+  \sum_{h=1}^H \EE\sbr{\rbr{ T_h^{\pi,\tilde\nu}-  T_h^{\pi,\nu}} \tilde U_h(s_h)}\label{eq:subopt-decompose-equality-2}\\
%     %%%%%%%%%%
%     &\quad\le {\sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}} + \sum_{h=1}^H \EE\sbr{\tilde W_h(s_h) - T_h^{\pi,\tilde\nu}\tilde U_h(s_h)}\nend
%     &\qqquad+ \sum_{h=1}^H  H \EE \nbr{\rbr{\tilde \nu_h-\nu_h}(\cdot\given s_h)}_1, \notag
%     \end{align}
%     which gives us \eqref{eq:perform-diff-linear}. 
%     Hence, we complete the proof.
\end{proof}



Recall that we estimate the quantal response mapping via   model-based maximum likelihood  estimation. 
In particular, we approximate the true quantal response policy $\nu^{\pi}$ within class $\{ \nu^{\pi, \theta}\}_{\theta \in \Theta}$, where 
$\nu^{\pi, \theta}$ can be written as 
\$
\nu_h^{\pi, \theta} (b_h\given s_h) &= \exp\bigl ( \eta \cdot A_h^{\pi, \theta} (s_h, b_h) \bigr), \quad\text{where}\quad A_h^{\pi, \theta} (s_h, b_h) = Q_h^{\pi, \theta} (s_h, b_h) - V_{   h}^{\pi, \theta} (s_h) ,
\$
Here $A^{\pi, \theta}$, $Q^{\pi, \theta}$, and $V^{\pi, \theta}$ are the advantage function, and value functions corresponding to policy $\pi$, under the model $\{ r^{\theta} , P^{\theta}\}$. 
In the following, we present a lemma which relates the error of quantal response mapping in \eqref{eq:perform-diff-general} and \eqref{eq:perform-diff-linear} to  estimation errors of the follower's value functions.
To simplify the notation, 
we consider a fixed policy $\pi$, and define  
$r_h^\pi(s_h,b_h) =\inp{r_h(s_h, \cdot, b_h)}{\pi(\cdot\given s_h, b_h)}_{\cA}$ and $P_h^\pi(s_{h+1}\given s_h,b_h) =\inp{P_h(s_{h+1}\given s_h, \cdot, b_h)}{\pi(\cdot\given s_h, b_h)}_{\cA} $.
To simplify the notation, 
in the rest of subsection, 
we let $\EE=\EE^{\pi,M^*}$ and $\EE_{z} [\cdot]=\EE^{\pi,M^*}[\cdot\given z]$ for any variable $z$.
 

%Our first group of techniques concern the suboptimality decomposition of the QSE with strategic follower. 
%   , and denote by $(\tilde U, \tilde W, \tilde Q, \tilde V, \tilde A)$ an alternative satisfying $\tilde V_h = \eta^{-1}\log \int \exp(\eta \tilde Q_h)$, $\tilde A_h = \tilde Q_h -\tilde V_h$. 
% Moreover, we let $\tilde \nu_h = \exp(\eta \tilde A_h)$ as the response under $\tilde A$. In the sequel, we let $\EE=\EE^{\pi,M^*}$ and $\EE_{z} [\cdot]=\EE^{\pi,M^*}[\cdot\given z]$ if without special reminder. We ignore the superscript $\pi, M^*$ for simplicity. We also let $r_h^\pi(s_h,b_h) =\inp{r_h(s_h, \cdot, b_h)}{\pi(\cdot\given s_h, b_h)}$ and $P_h^\pi(s_{h+1}\given s_h,b_h) =\inp{P_h(s_{h+1}\given s_h, \cdot, b_h)}{\pi(\cdot\given s_h, b_h)}$.  
% Based on \eqref{eq:performance diff-1},






% We consider a fixed policy $\pi$,  denote by $(U, W, Q, V, A, \nu)$ the ground truth under $\pi$ and the true model $M^*$, and denote by $(\tilde U, \tilde W, \tilde Q, \tilde V, \tilde A, \tilde \nu)$ an alternative satisfying $\tilde V_h = \eta^{-1}\log \int \exp(\eta \tilde Q_h)$, $\tilde A_h = \tilde Q_h -\tilde V_h$, and $\tilde \nu_h = \exp(\eta \tilde A_h)$. We then have for the U function that



% We next characterize the quantal response error in a Taylor expansion flavor.
\begin{lemma}[Response Model Error]
    \label{lem:performance diff}
    We consider a fixed policy $\pi$
and   let $\tilde Q$    be an estimate  of $Q^{\pi}$. 
We define a V-function  $\tilde V$ and an advantage function $\tilde A$ by letting 
\#\label{eq:tilde_functions}
\tilde V_h (s) = \frac{1}{\eta} \cdot \log \bigg(  \sum_{b \in \cB} \exp( \eta \cdot \tilde Q_h(s, b) ) \biggr), \qquad \textrm{and}\qquad \tilde A_h(s,a) = \tilde Q_h (s,b) - \tilde V_h (s). 
\# 
Furthermore, we define a follower's policy $\tilde \nu$ by letting $\tilde \nu_h(b \given s) = \exp( \eta\cdot \tilde A_h(s,b))$. 
Then the difference between $\tilde \nu$ and $\nu^{\pi}$ can be bounded by 
\begin{align}
    &\sum_{h=1}^H  H \cdot  \EE \bigsbr{\nbr{\rbr{\tilde \nu_h-\nu_h^{\pi} }(\cdot\given s_h)}_1} \nend
    &\quad\le C^{(0)} \cdot \sum_{h=1}^H \underbrace{\EE\bigsbr{\bigabr{\tilde \Delta^{(1)}_h(s_h, b_h)}}}_{\ds\text{1st-order error}}  + C^{(1)} \cdot 
    \sum_{h=1}^H \underbrace{\EE\bigsbr{ \bigabr{(\tilde A_h  - A_h^{\pi})(s_h, b_h)}^2}}_{\ds\text{2nd-order error}} \label{eq:taylor-myopic}\\
    &\quad\le 
    C^{(0)} \cdot 
    \sum_{h=1}^H \underbrace{\EE\bigsbr{\bigabr{\tilde \Delta^{(1)}_h(s_h, b_h)}}}_{\ds\text{1st-order error}}  + C^{(2)} \cdot 
    \max_{h\in [H]} \underbrace{\EE\bigsbr{ \bigabr{\orbr{\tilde Q_h - r_h^\pi - \gamma P_h^\pi \tilde V_{h+1}}(s_h, b_h)}^2}}_{\ds\text{2nd-order error}},\label{eq:taylor-farsighted} 
\end{align}
where $\tilde \Delta^{(1)}_h(s_h, b_h)$ is defined as
\begin{align*}
    \tilde \Delta^{(1)}_h(s_h, b_h) &=  \rbr{\EE_{s_h, b_h} -\EE_{s_h}}\Biggsbr{\sum_{l=h}^H \gamma^{l-h}\underbrace{\rbr{\tilde Q_l - r_l^\pi - \gamma P_l^\pi \tilde V_{l+1}}(s_l, b_l)}_{\ds\text{Follower's Bellman error}}}. 
    % \label{eq:def Delta^1}
\end{align*}
Furthermore, the constants $C^{(0)}$, $C^{(1)}$, and $C^{(2)}$ are given by
\#\label{eq:define_constants}
\begin{split}
    C^{(0)}&=2\eta H , \qquad   C^{(1)}=\eta^2 H   \bigrbr{1+ 4 \eff_H(\gamma)}\cdot \exp(2\eta B_A), \\
C^{(2)}  & =  2  \eta^2 H^2\cdot \exp\orbr{6\eta B_A} \cdot(1+4 \eff_H(\gamma)) \cdot \rbr{\eff_H(\exp(2\eta B_A)\gamma)}^2,
\end{split}
\#
where $B_A$ defined in \eqref{eq:define_BA} is an upper bound on the magnitude of the advantage function, and we define $\eff_H(x) = (1-x^H)/(1-x)$ as the \say{effective}  horizon with respect to $x$.
% \begin{proof}
%     See \Cref{lem:performance diff} for a detailed proof.
% \end{proof}



    % denote by $(U, W, Q, V, A)$ the ground truth under $\pi$, and denote by $(\tilde U, \tilde W, \tilde Q, \tilde V, \tilde A)$ an alternative satisfying $\tilde V_h = \eta^{-1}\log \int \exp(\eta \tilde Q_h)$, $\tilde A_h = \tilde Q_h -\tilde V_h$. Suppose that $\tilde\nu_h=\exp\orbr{\eta \tilde A_h}$ and $\nu$ is the real quantal response under policy $\pi$. The behavior model error in \eqref{eq:perform-diff-linear} can be upper bounded by
\end{lemma}
\begin{proof}
    See \Cref{sec:proof-performance diff} for a detailed proof.
\end{proof}

If we view the quantal response as a functional of the advantage function (as shown in \eqref{eq:quantal_response_policy}),  this lemma plays the role of Taylor expansion of the quantal response into the first- and second-order errors. 
In particular, the first-order error corresponds to the Bellman error of the follower's problem, and the second-order term is mean-squared error of the advantage function (as in \eqref{eq:taylor-myopic}) or the sum of squares of the Bellman error (as in \eqref{eq:taylor-farsighted}). 
Here we establish two versions of upper bounds because \eqref{eq:taylor-myopic} is handy for the myopic case while the second \eqref{eq:taylor-farsighted} is more useful for the farsighted case.
 
In the case with a  myopic follower,
the $Q$-function is reduced to the reward function $r$ of the follower. 
We have the following corollary. 
\begin{corollary}[Response Model Error for Myopic Case]\label{cor:response-diff-myopic}
    Let $r$ be the true reward function and let $\tilde r$ be an estimated reward. 
Let $\pi \colon \cS \times \cB \rightarrow \Delta(\cA )$ be a fixed policy. 
Let $\nu$ and $\tilde \nu$ be the quantal response function based on $r$ and $\tilde r$, respectively, i.e.,
\$
\nu(b\given s) \propto \exp \big (  \eta\cdot  r^{\pi} (s,b)\big ) ,\qquad \nu(b\given s) \propto \exp\big ( \eta \cdot \tilde r^{\pi} (s,b)\big ).
\$
Here we define $r^{\pi} $ by letting $r^{\pi} (s,b) = \la r(s,\cdot, b), \pi(\cdot \given s, b)\ra$, and define $\tilde r^{\pi}$ similarly. 
Then for any state $s\in \cS$, we have 
    \begin{align}
        D_\TV\rbr{\nu(\cdot\given s), \tilde\nu(\cdot\given s)}
        &\le  \eta  \EE_s\sbr{\abr{(\tilde r^\pi(s, b) - r^\pi(s, b)) - \EE_s\bigsbr{\tilde r^\pi(s, b) - r^\pi(s, b)}}} \nend
        &\qquad + C^{(3)}\EE_s\bigsbr{\rbr{\rbr{\tilde r^\pi(s, b) -r^\pi(s, b)} - \EE_s\sbr{\tilde r^\pi(s, b) -r^\pi(s, b)}}^2}, \label{eq:TV-for-myopic}  
        % \notag 
    \end{align}
    where $C^{(3)}={\eta^2 \exp(2\eta B_A)}\bigrbr{2+\eta B_A \cdot  \exp\rbr{2\eta B_A}}/2$ and $B_A$  defined in \eqref{eq:define_BA} is $2 + 2 \log |\cB| / \eta  $. 
    Here the expectation $\EE_s$ is only taken with respect to $b \sim \nu(\cdot \given s)$ when conditioned on this $s$.
    \begin{proof}
        See \Cref{sec:proof-response-diff-myopic} for a detailed proof.
    \end{proof}
\end{corollary}
In the following, we present the result for a special case where 
the follower is myopic with a linear reward function. 
In this case, we write 
  $Q^{\pi, \theta}(s, b) = r^{\pi, \theta}(s, b) = \inp[]{\phi^\pi(s, b)}{\theta}$ for some $\RR^d$ kernel $\phi:\cS\times\cA\times\cB\rightarrow\RR^d$ with $\phi^\pi(s, b)=\inp{\phi(s, \cdot, b)}{\pi(\cdot\given s, b)}_\cA$ and parameter $\theta\in\RR^d$.
  The quantal response policy is given by $\nu^{\pi, \theta} (b \given s) \propto \exp(\eta \cdot \la \phi^{\pi}(s,b), \theta \ra)$. 
In particular, we show that the 1st- and the 2nd-order QRE decomposition in \eqref{eq:QRE-decompose} of \Cref{sec:learning QSE} is just a direct result of \Cref{cor:response-diff-myopic}. Recall by definition of the $\QRE$ in \eqref{eq:QRE}, 
\begin{align*}
    \QRE(s_h, b_h;\tilde\theta,\pi) &=  (\Upsilon_h^{\pi}(\tilde r_h - r_h))\orbr{s_h, b_h}\nend
    &= \dotp{\pi_h(\cdot\given s_h, b_h)}{(\tilde r_h - r_h)(s_h, \cdot, b_h)} - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{(\tilde r_h - r_h)(s_h,\cdot,\cdot)}\nend
    & = \sbr{{(\tilde r_h^\pi(s_h, b_h) - r_h^\pi(s_h, b_h)) - \EE_{s_h}\bigsbr{\tilde r_h^\pi(s_h, b_h) - r_h^\pi(s_h, b_h)}}}. 
\end{align*}
We plug in the linear representation of the follower's reward $r_h^\pi(s_h, b_h) = \la\phi_h^\pi(s_h, b_h), \theta_h^*\ra$, which implies that 
\begin{align*}
    \EE_{s_h}\QRE(s_h, b_h;\tilde\theta, \pi)^2 = \Cov_{s_h}^{\pi,\theta^*} \sbr{(\tilde r_h^\pi - r_h^\pi)(s_h, b_h)} = \onbr{\tilde \theta_h -\theta_h}_{\Sigma_{s_h}^{\pi,\theta^*}}^2,
\end{align*}
where $\Sigma_{s_h}^{\pi, \theta^*} = \Cov_{s_h}^{\pi, \theta^*}[\phi_h^\pi(s_h, b_h)]$. Moreover, For the first term on the right hand side of \eqref{eq:TV-for-myopic}, 
\begin{align*}
    &\EE_{s_h}\sbr{\abr{(\tilde r^\pi(s_h, b_h) - r^\pi(s_h, b_h)) - \EE_s\bigsbr{\tilde r^\pi(s_h, b_h) - r^\pi(s_h, b_h)}}} \nend
    &\quad \le \sqrt{\EE_{s_h}\bigsbr{\rbr{\rbr{\tilde r^\pi(s_h, b_h) -r^\pi(s_h, b_h)} - \EE_{s_h}\sbr{\tilde r^\pi(s_h, b_h) -r^\pi(s_h, b_h)}}^2}} \nend
    &\quad = \EE_{s_h}\QRE(s_h, b_h;\tilde\theta, \pi)^2,
\end{align*}
where the inequality holds by the Cauchy-Schwarz inequality. The second term follows similarly.
We further generalize the above argument to the following corollary. 

\begin{corollary}[Response Model Error for Linear and  Myopic Case]\label{lem:response diff-myopic-linear}
    Under the setting of \Cref{cor:response-diff-myopic}, we assume the reward function of the myopic follower is a linear function of $\phi$. 
    Let $\theta^*$ be the parameter of the true reward function and let 
    $\tilde \theta \in \Theta$ be another parameter. 
    Let $\pi \in \Pi$ be any policy and let $s \in \cS$ be any state. 
   We define a matrix  $\Sigma_s^{\pi,\theta}$  as $ \Cov_s^{\pi,\theta}[\phi^\pi(s, b)], $ i.e.,
   \#\label{eq:define_sigma_s}
   \Sigma_s^{\pi,\theta} = \EE_s \bigl [ (\phi^\pi(s, b) - \EE_s [\phi^\pi(s, b)] \bigr )(\phi^\pi(s, b) - \EE_s [\phi^\pi(s, b)] \bigr )^\top \bigr ],
   \#
   where the expectation is taken with respect to $\nu^{\pi,\theta}(\cdot \given s)$.
    Then we have 
    \begin{align}
        &D_\TV\rbr{\nu^{\pi,\theta^*}(\cdot\given s) , \nu^{\pi,\tilde\theta}(\cdot\given s)} \nend
        &\quad \le \min\Bigcbr{f\Bigrbr{\sqrt{\trace\bigrbr{{\Psi}^{\dagger} \Sigma_{ s}^{\pi, \tilde\theta}}}\cdot \bignbr{\theta^*- \tilde\theta}_{{\Psi}}}, f\Bigrbr{\sqrt{\trace\bigrbr{{\Psi}^{\dagger} \Sigma_{ s}^{\pi, \theta^*}}}\cdot \bignbr{\theta^*- \tilde\theta}_{{\Psi}}}},
        % \label{eq:TV-for-myopic-linear}
    \end{align}
    where $\Psi\in \SSS_+^{d}$ can be any fixed nonnegative definite matrix, $\Psi^{\dagger}$ is the pseudo-inverse of $\Psi$,  and the univariate function $f$ is defined as
    $
        f(x) = \eta x + C^{(3)} x^2 
    $ with 
    $$C^{(3)}={\eta^2 \exp(2\eta B_A)}\bigrbr{2+\eta B_A \cdot  \exp\rbr{2\eta B_A}}/2.$$
\end{corollary}

\begin{proof}
    See \Cref{sec:proof-response-diff-myopic_linear} for a detailed proof. 
\end{proof}
    % $\Gamma^{(2)}_h$ is defined as
    % \begin{align*}
    %     \Gamma^{(2)}_h(s_h;\alpha,\theta_h) = \sqrt{\EE_{s_h}^{{\alpha,\theta_h}}\sbr{\phi_h^{\alpha}(s_h,b_h)^\top {\Psi}^{\dagger}\phi_h^{\alpha}(s_h,b_h)} -\bignbr{\EE_{s_h}^{{\alpha,\theta_h}}\phi_h^{\alpha}(s_h,b_h)}_{{\Psi}^{\dagger}}^2}\cdot \bignbr{\theta_h^*- \theta_h}_{{\Psi}}, 
    % \end{align*}
    % and $\Gamma^{(3)}$ is defined as 
    % \begin{align*}
    %     \Gamma^{(3)}_h(s_h;\alpha,\theta_h) = \sqrt{\EE_{s_h}^{{\alpha,\theta_h^*}}\sbr{\phi_h^{\alpha}(s_h,b_h)^\top {\Psi}^{\dagger}\phi_h^{\alpha}(s_h,b_h)} -\bignbr{\EE_{s_h}^{{\alpha,\theta_h^*}}\phi_h^{\alpha}(s_h,b_h)}_{{\Psi}^{\dagger}}^2}\cdot \bignbr{\theta_h^*- \theta_h}_{{\Psi}}
    % \end{align*}
    
We first show that the uncertainty quantifier $\Gamma^{(2)}_h(s_h;\pi, \theta_h)$ used in \eqref{eq:Gamma^2} of \S\ref{sec:offline-ML} {\neurips and \eqref{eq:Gamma^2-neurips}} can be derived directly from \Cref{lem:response diff-myopic-linear}. 
Recall by definition, 
\begin{align*}
    \Gamma^{(2)}_h(s_h;\pi_h , \theta_h) = 2 H(\eta  \xi(s_h;\pi,\theta_h)  + C^{(3)}  \xi(s_h;\pi,\theta_h)^2 ), 
\end{align*}
where $\xi(s_h;\pi,\theta_h)^2$ is defined as
\begin{equation*}
    \xi(s_h;\pi, \theta_h)^ 2 =  \trace\bigrbr{\bigrbr{T\Sigma_{h,\cD}^{\theta} + I_d}^\dagger \Sigma_{s_h}^{\pi , \theta}}  \cdot  \bigl( 2 (\eta^{-1}+B_A)^2 \beta + 4B_\Theta^2 \bigr).
\end{equation*}
One can easily check that for any $\theta_h\in\CI_{h,\Theta}(\beta)$ where $\CI_{h,\Theta}(\beta)$ is the offline confidence set, 
\begin{align*}
    \Gamma^{(2)}_h(s_h;\pi_h,\theta_h) 
    &= 2 H \cdot f\Bigrbr{\sqrt{\trace\bigrbr{\bigrbr{T\Sigma_{h,\cD}^{\theta}+I_d}^\dagger \Sigma_{s_h}^{\pi,\theta}}} (2C_\eta^2 \beta + 4 B_\Theta^2)}\nend
    &\ge 2 H \cdot f\Bigrbr{\sqrt{\trace\bigrbr{\bigrbr{T\Sigma_{h,\cD}^{\theta}+I_d}^\dagger \Sigma_{s_h}^{\pi,\theta}}} (T\nbr{\theta_h -\theta_h^*}_{\Sigma_{h,\cD}^{\theta}}^2 + \nbr{\theta_h -\theta_h^*}_{I_d}^2)}\nend
    & \ge 2 H D_\TV\bigrbr{\nu_h^{\pi,\theta^*}(\cdot\given s_h), \nu_h^{\pi,\theta}(\cdot\given s_h)}, 
\end{align*}
where we use definition $C_\eta^2 = \eta^{-1}+B_A$ in the first equality, and in the first inequality, $2C_\eta^2\beta T^{-1} \ge \nbr{\theta_h-\theta_h^*}_{\Sigma_{h,\cD}^\theta}^2$ holds by \Cref{cor:formal-MLE confset-linear myopic} if $\CI_{h,\Theta}(\beta)$ is a valid confidence set, and $4B_\Theta^2\ge \nbr{\theta_h-\theta_h^*}_{I_d}^2$ holds by noting that $\nbr{\theta_h}^2\le B_\Theta$ for any $\theta_h\in\Theta_h$. 
The last inequality holds just by \Cref{lem:response diff-myopic-linear} where we plug in $\Psi = T\Sigma_{h,\cD}^{\theta}+I_d$.
% Note that $\bignbr{\theta^*-\tilde\theta}_{\Sigma_{s_h}^{\pi,\theta^*}}$



\subsection{Learning Quantal Response via MLE}\label{sec:app-MLE}
In \S\ref{sec:MLE for behavior model}, we introduce how to learn the follower's  quantal response model from the follower's feedbacks via maximum likelihood estimation. 
In the following, we provide a slightly stronger lemma that implies  \Cref{sec:proof-MLE-general} in \S\ref{sec:MLE for behavior model}. 
In the following, we follow the notation used in \S\ref{sec:MLE for behavior model}, where 
we let $\theta = \{ \theta_h \}_{h \in [H]} \in \Theta $ denote the parameters of the follower's response model,
where $\Theta = \Theta_1 \times \ldots \times \Theta_H$ is the set of all parameters. 
Here we assume $\theta_h \in \Theta_h$ for all $h\in [H]$.
In specific, each $\theta$ is associated with a model, denoted by  ${ r^{\theta}, P^{\theta}}$, 
which is   shorthand notation for 
\$
\{ r_1^{\theta_1}, P_1^{\theta_1},  \ldots, r_H ^{\theta_H}, P_H^{\theta_H}\}. 
\$ 
Moreover, when the follower is myopic, $\theta$ only parameterize a reward function $r^{\theta}$. 
We assume that the parametric model of the follower captures the true model. That is, there exists $\theta^* \in \Theta $ such that 
$\{r^{\theta}, P^{\theta^*}\}$ is the true environment. 


%we assume that  
%$\Theta$ is the set of the  parameters that determines the follower's response model. 


For any $\theta \in \Theta$ and any policy $\pi$ of the leader, we let $\nu^{\pi, \theta}$, $A^{\pi, \theta}$, $Q^{\pi, \theta } $ and $V^{\pi, \theta }$ denote the quantal response of $\pi$, advantage function, and Q- and V-functions under model $\{ r^{\theta}, P^{\theta} \}$, which are defined according to  \eqref{eq:quantal_response_policy}--{\main\eqref{eq:v_pi_qr}\fi}{\neurips \eqref{eq:qv_pi_qr}\fi}.
 The quantal response   under the true model is $  \nu^{\pi, \theta^*}$.
Thus, given a  (possibly adaptive) dataset $\cD = \{(s_h ^i, a_h ^i, b_h ^i, \pi_h ^i)\}_{i\in[t-1], h\in [H]}$,
the negative log-likelihood at step $h$ is given by
\#\label{eq:loglikelihood-1}
\cL_h^t  (\theta  )  & = -  \sum_{i = 1}^{t-1}  
\log \nu^{\pi^i, \theta} (b_h^i \given s_h^i) 
  = -  \sum_{i=1}^{t-1}   \eta  \cdot A_h^{\pi^i , \theta}(s^i,  b^i), 
\# 
 where   $\nu^{\pi^i, \theta} (b_h^i \given s_h^i) $ is the probability of observing the follower's action $b_h^i$ when the model parameter is $\theta$, state is $s_h^i$, and the leader announces $\pi^i$, and the second equality in \eqref{eq:loglikelihood-1} is due to \eqref{eq:quantal_response_policy}.   
 Here we assume the data $\cD$ satisfies the compliance property, i.e., $\PP^{\pi^t}_\cD(b_h^t\given s_h^t, (s_j^t, a_j^t, b_j^t, u_j^t)_{j\in[h-1]}, \tau^{1:t-1})  = \nu_h^{\pi^t}(b_h^t\given s_h^t), \forall h\in[H], t\in[T]$. 
 Such a property is satisfied 
by the online setting and the offline setting where behavior policies are adaptive. 
 Therefore, the MLE estimator of $\theta^*$ can be obtained by minimizing $\cL_h^t(\theta)$ over $\Theta$. 
Moreover, based on $\cL_h$, we can construct a confidence set for $\theta^*$: 
\begin{align}
    \confset_{h,\Theta}^t(\beta)=\cbr{\theta\in\Theta: \cL_h^t(\theta)\le \inf_{\theta'\in\Theta}\cL_h^t(\theta') + \beta}, \label{eq:behavior_model_confset-1}
\end{align}
where  $\cL_h^t(\theta) = \sum_{i=1}^{t-1} \eta A_h^{\pi^i, \theta}(s_h^i, b_h^i)$.


We remark that $\cL_h^t(\theta)$ \eqref{eq:loglikelihood-1} is a function of $\theta = \{ \theta_h \}_{h\in [H]}$. 
The reason is that the follower's quantal response is obtained by solving the optimal policy of an entropy-regularized MDP, whose optimal policy depends on the reward and transitions across all $H$ steps. 
But if the follower is myopic, then $\cL_h^t(\theta)$ only depends on $\theta_h$. In this case, we can regard $\cL_h^t$ as a function on $\Theta_h$, and replace $\Theta$ in \eqref{eq:behavior_model_confset-1} by $\Theta_h$.  We will leverage such observation in \Cref{sec:learning QSE}.


\begin{lemma}[Confidence Set]\label{lem:MLE-formal}
We define a distance $\rho  $ on $\Theta$ by letting 
\begin{align}\label{eq:rho for MLE}
 \rho(\theta, \tilde \theta) \defeq \max_{\pi\in\Pi, s_h\in\cS, h\in[H]} \cbr{D_\H\orbr{\nu_h^{\pi, \theta}(\cdot\given s_h), \nu_h^{\pi, \tilde \theta}(\cdot\given s_h)}, (1+\eta) \cdot\bignbr{Q_h^{\pi, \theta} - Q_h^{\pi, \tilde\theta}}_\infty}, 
\end{align}
where $B_A$ upper bounds the follower's A-, Q-, and V-functions and is specified in \eqref{eq:define_BA}. 
Let $\cN_\rho(\Theta,\epsilon)$ be the $\epsilon$-covering number of $\Theta$ with respect to the distance $\rho$.
That is, $\cN_\rho(\Theta,\epsilon)$ is the smallest $N \geq 1$ with the following property: there exists $\{ \theta^i\}_{i\in [N]} \subseteq \Theta$ such that, for any $\theta \in \Theta$, there exists $\theta^i$ such that $\rho(\theta, \theta^i) \leq \epsilon$.
For any $\delta \in (0,1)$, we set $\beta \ge  2\log(e^3 H\cdot \cN_\rho(\Theta, T^{-1})/\delta)$.  
Then  with probability at least $1-\delta$,   the following properties hold for $\confset_{h, \Theta}^t (\beta)$ defined in \eqref{eq:behavior_model_confset-1}:
    \begin{itemize}
    \item [(i)]  (Validity) $\theta^*\in\confset_{h, \Theta}^t(\beta)$; 
    \item [(ii)] (Accuracy) For any $\theta\in\Theta, t\in[T], h\in[H]$, it holds that 
    \#
        &
        \sum_{i=1}^{t-1} D_\H^2\bigrbr{\nu_h^{\pi^i, \theta}(\cdot\given s_h^i), \nu_h^{\pi^i, \theta^*}(\cdot\given s_h^i)} \le  \frac {1}{2}\rbr{\cL_h^t(\theta) - \cL_h^t(\theta^*)+ \beta}, \label{eq:MLE-guarantee-hellinger-1}\\
        &
        \sum_{i=1}^{t-1} \EE^{\pi^i }D_\H^2\bigrbr{\nu_h^{\pi^i, \theta}, \nu_h^{\pi^i, \theta^*}} \le  \frac {1}{2}\rbr{\cL_h^t(\theta) - \cL_h^t(\theta^*)+ \beta}\label{eq:MLE-guarantee-hellinger-2}.
 \#
 \end{itemize} 
 Furthermore, the above two inequalities ensure respectively that for 
   $\forall \theta'\in\bigcbr{\theta^*, \theta}, \theta\in\Theta, \forall h\in[H]$,
    \begin{align}
        &\sum_{i=1}^{t-1} {\Var_{s_h^i}^{\pi^i, \theta'} \bigsbr{Q_h^{\pi^i, \theta}(s_h, b_h) - Q_h^{\pi^i, \theta^*}(s_h, b_h)}} \le 4 C_\eta^2 \rbr{\cL_h^t(\theta) - \cL_h^t(\theta^*)+ \beta}, \label{eq:MLE_guarantee_Q-1} \\
        & \sum_{i=1}^{t-1} \EE^{\pi^i }{\Var_{s_h}^{\pi^i, \theta'} \bigsbr{Q_h^{\pi^i, \theta}(s_h, b_h) - Q_h^{\pi^i, \theta^*}(s_h, b_h)}} \le  4 C_\eta^2 \rbr{\cL_h^t(\theta) - \cL_h^t(\theta^*)+ \beta},\label{eq:MLE_guarantee_Q}
    \end{align}
    where $\Var_s^{\pi, \theta}[Z] = \Var^{\pi, \theta}[Z\given s] = \EE^{\pi,\theta}[(Z - \EE^{\pi, \theta}[Z\given s])^2\given s]$, $C_\eta =\eta^{-1}+B_A$. Moreover, for any $\theta\in\CI_\Theta(\beta), h\in[H]$, and a given  $t\in[T]$, we have with probability at least $1-\delta$ that
\begin{align}
&\sum_{i=1}^{t-1} 
    \rbr{\rbr{Q_h^{\pi^i, \theta} - Q_h^{\pi^i, \theta^*}}(s_h^i, b_h^i)
    -\EE_{s_h^i}^{\pi^i,\theta^*}\sbr{ \bigrbr{Q_h^{\pi^i, \theta} - Q_h^{\pi^i, \theta^*}}(s_h, b_h)}}^2 
    \le  \cO(C_\eta^2 \beta) .  \label{eq:MLE-guarantee-Q-3}
\end{align}
 
% and $\cN_\varrho (\Theta, \epsilon)$ is the covering number of the smallest $\epsilon$-covering numebr of $\Theta$ with respect to distance 
% \begin{align*}
%     \varrho(\theta, \tilde\theta) \defeq \max_{\pi\in\Pi, h\in[H]}\nbr{Q_h^{\pi, \theta} - Q_h^{\pi, \tilde\theta}}_\infty.
% \end{align*}
    \begin{proof}
        See \Cref{sec:proof-MLE-general} for a detailed proof.
    \end{proof}
\end{lemma}
We remark that if $\theta\in\CI_\Theta^t(\beta)$, the guarantees in the accuracy results \eqref{eq:MLE_guarantee_Q} and \eqref{eq:MLE_guarantee_Q-1} are just $8C_\eta^2 \beta$ since $\cL_h^t(\theta)-\cL_h^t(\theta^*)\le \cL_h^t(\theta) - \inf_{\theta'\in\Theta}\cL_h^t(\theta') \le \beta$ for all $h\in[H]$.
Moreover, the covering number defined here is a special case of a more general version given by \eqref{eq:rho-Theta} and also in the myopic case it is given by \eqref{eq:rho-Theta_h}. 
Next, we comment on the myopic case, where it suffices to consider a covering for $\Theta_h$ (which only contains the parameters for the follower's reward function at step $h$) instead of the whole $H$-step class $\Theta$.
\begin{remark}[Myopic case for \Cref{lem:MLE-formal}]\label{rmk:MLE-formal-myopic}
    If the follower is myopic, it suffices to use the distance $\rho$ for class $\Theta_h$ defined in \eqref{eq:rho-Theta_h}, and let $\cN_\rho(\Theta, \epsilon) = \max_{h\in[H]} \cN_\rho(\Theta_h, \epsilon)$. The conclusion in \Cref{lem:MLE-formal} still applies.
\end{remark}

% {\main
% Here, the first expectation-type guarantee is useful for the online setting while the non-expectation-type guarantee is used for the offline setting.
% For the latter,  we allow the data to be dependent across samples or there might be no stationary distribution for the data collection process.
% To bridge the result in \Cref{lem:MLE-formal} to \eqref{eq:bandit-ub-1}, we define $\hat\theta_\MLE=\argmin_{\theta\in\Theta}\cL^t(\theta)$, where $\cL^t(\theta)=\sum_{h=1}^H \cL_h^t(\theta)$. We also let $\beta'=H\beta$, and it holds that  $$\cL^t(\theta^*) - \inf_{\theta'\in\Theta}\cL^t(\theta')= \sum_{h=1}^H \cL_h^t(\theta^*) - \cL_h^t(\hat\theta_\MLE) \le \sum_{h=1}^H \cL_h^t(\theta^*) - \inf_{\theta'\in\Theta}\cL_h^t(\theta') \le H \beta =\beta', $$ 
% which shows that the confidence set in \eqref{eq:behavior_model_confset} is valid with significance level $\beta'$.
% For the accuracy result, we note that \eqref{eq:MLE-guarantee-hellinger-2} holds for any $\theta\in\Theta$ and we just sum them up for all $h\in[H]$ and use the fact \begin{align*}
%     \sum_{i=1}^{t-1} \EE^{\pi^i }D_\H^2\bigrbr{\nu_h^{\pi^i, \theta}, \nu_h^{\pi^i, \theta^*}} 
%     &\le  \sum_{h=1}^H \sum_{i=1}^{t-1} \EE^{\pi^i }D_\H^2\bigrbr{\nu_h^{\pi^i, \theta}, \nu_h^{\pi^i, \theta^*}} \nend
%     & \le \frac {1}{2}\rbr{\cL_h^t(\theta) - \cL_h^t(\theta^*)} + H \log\rbr{\frac{d H\cN_\rho(\Theta, T^{-1})}{\delta}}\nend
%     &\le \frac 1 2 \rbr{\cL^t(\theta)-\cL^t(\hat\theta_\MLE)} + H \beta \le 2\beta', 
% \end{align*}
% for $\theta\in \{\theta:\cL^t(\theta)-\cL^t(\hat\theta_\MLE)\le \beta'$\}. 
% Therefore, the conclusion in \Cref{lem:bandit} just follows from \eqref{eq:MLE-guarantee-hellinger-2} of \Cref{lem:MLE-formal}.
% \fi}

Next, we give a proof on \Cref{eq:bandit-ub-1} 
% {\main\Cref{cor:MLE confset-linear myopic}\fi}{\neurips \eqref{eq:bandit-ub-1}\fi}
, which borrows \Cref{rmk:MLE-formal-myopic} and the covering number of $\Theta_h$ given in \eqref{eq:cN-Theta_h}.
\begin{remark}[Formal statement of \eqref{eq:bandit-ub-1}
    % {\ifmain \Cref{cor:MLE confset-linear myopic}\fi}{\ifneurips \eqref{eq:bandit-ub-1}\fi}
    ]\label{cor:formal-MLE confset-linear myopic}
    Consider the myopic and linear case, Suppose that $\beta\ge C d\log(H(1+\eta T^2 + (1+\eta)T)\delta^{-1})$ for some universal constant $C>0$,  \eqref{eq:MLE_guarantee_Q-1} further implies that for all $h\in[H]$
        \begin{align}
            \max\cbr{\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\theta^*}}^2, \bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\hat\theta}}^2, 
            \EE^{\pi^i, M^*}\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\theta^*}}^2, 
            \EE^{\pi^i, M^*}\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\hat\theta}}^2 } \le 8 C_{\eta}^2 \beta,
            \label{eq:app-bandit-ub-1}
        \end{align}
        where $C_\eta =\eta^{-1}+B_A$, $B_A$ is specified in \eqref{eq:define_BA}, and $\Sigma_{h, t}^{\theta}$ is a data-dependent covariance matrix defined as 
        \begin{align}\label{eq:app-cov matrix}
            \Sigma_{h, t}^{\theta}= \sum_{i=1}^{t-1}
            {\Cov_{s_h^i}^{\pi^i, \theta}\bigsbr{\phi^{\pi^i}(s_h, b_h)}}, 
        \end{align}
    where $\Cov_{s_h}^{\pi, \theta}[\phi^{\pi}(s_h, b_h)]$ represents the covariance matrix of the feature $\phi^\pi$ with respect to $\nu_h^{\pi,\theta}(\cdot\given s_h)$.
    % \begin{proof}
    %     We use \eqref{eq:MLE_guarantee_Q-1} and obtain by noting that $(Q^{\pi^i, \theta}-Q^{\pi^i, \theta^*})(s, b) = \phi^{\pi^i}(s, b)^\top(\theta_h-\theta_h^*)$, which gives the result.
    % \end{proof}
\end{remark}

We next consider a special case where each trajectory in the offline data is independently collected. 
We have the following lemma for the MLE guarantee with independent dataset.


\begin{lemma}[{Confidence in Hellinger Distance with Independent Data}]
    \label{lem:MLE-indep-data}
    Suppose the conditions in \Cref{lem:MLE-formal} hold.
    Suppose each trajectory in the offline dataset $\cD=\{\tau^t\}_{t\in[T]}$ is independently collected. 
    For the confidence set $\CI_\Theta(\beta)$ defined in \Cref{eq:behavior_model_confset-1} with $\beta$ properly chosen according to \Cref{lem:MLE-formal},
    with probability at least $1-2\delta$, it holds that (i) $\theta^*\in\CI_\Theta(\beta)$; (ii) for any $\theta\in\CI_\Theta(\beta)$ , $h\in[H]$, 
    \begin{align*}
        \sum_{i=1}^T\EE_\cD\sbr{D_\H^2\rbr{\nu_h^{\pi^i, \theta}(\cdot\given s_h^i), \nu_h^{\pi^i, \theta^*}(\cdot\given s_h^i)}}  \le \cO(\beta),
    \end{align*}
    where the expectation is taken for  the randomness in both the trajectories and in the leader's policy choices.
    \begin{proof}
        See \ref{sec:proof-MLE-indep-data} for a detailed proof.
    \end{proof}
\end{lemma}
Up to now, we have obtain all the guarantees we  need from the MLE of the follower's quantal response.

\subsection{Learning Leader's Value Function}\label{sec:app-value function}
In this section, we study the problem of learning the leader's value function for both the offline and the online setting. 

\paragraph{Learning Leader's Value Function in Offline Setting.}
for each $\pi$ and estimated follower's response model $\theta$. 
%The results in this section mainly follows \citet{xie2021bellman,lyu2022pessimism}.
We only focus on myopic follower in this section. Recall the Bellman loss we defined in \Cref{sec:offline-myopic}, which is defined as
\begin{align*}
    &\ell_h(U_h', U_{h+1}, \theta, \pi) = \sum_{i=1}^T \rbr{U_h'(s_h^i, a_h^i, b_h^i) - u_h^i -   T^{\pi, \theta}_h U_{h+1}(s_{h+1}^i)}^2.
\end{align*}
where we define $ T_h^{\pi, \theta}U_{h+1}(s_{h+1}) = \inp[]{U_{h+1}(s_{h+1}, \cdot, \cdot)}{\pi_{h+1}\otimes \nu_{h+1}^{\pi, \theta}(\cdot, \cdot\given s_{h+1})}$. 
We aim to characterize the confidence set 
\begin{align*}
    \CI_{\cU}^{\pi,\theta}(\beta) = \cbr{U\in\cU: \ell_h(U_h, U_{h+1},\theta, \pi) - \inf_{U_h'\in\cU} \ell_h(U_h', U_{h+1}, \theta,\pi)\le \beta, \forall h\in[H]}.
\end{align*}
Recall the definition of the Bellman operator for the leader in \eqref{eq:bellman_operator_leader}. Similar to this definition, we define $\TT_h^{\pi,\theta}:\sF(\cS\times\cA\times\cB)\rightarrow \sF(\cS\times\cA\times\cB)$ as 
\begin{align*}
    \rbr{\TT_h^{\pi,\theta} f} (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE_{s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)} \sbr{\rbr{ T_{h+1}^{\pi,\theta}f}(s_{h+1})},
\end{align*}
and we add $\theta$ to the superscription to remind ourselves that the expectation within $\TT_h^{\pi,\theta}$ is taken with respect to the follower's quantal behavior guided by both $\pi$ and $\theta$.
In the sequel, we denote by $U^{\pi,\theta}$ the leader's U function defined similar to \eqref{eq:U_function} but with respect to policy $\pi$ and the response model $\theta$, 
\begin{align*}
    U_h^{\pi,\theta}(s_h, a_h, b_h) & = u_h(s_h, a_h, b_h) + \rbr{ P_h \circ T_{h+1}^{\pi,\theta} \circ U_{h+1}^{\pi,\theta}}(s_h, a_h, b_h) = \TT_h^{\pi,\theta} U_{h+1}^{\pi,\theta}(s_h, a_h, b_h).
\end{align*}
We clarify that $\theta$  only contains the estimated reward for myopic follower. 
Now, we present the following corollary on the validity and accuracy of the confidence set $\CI_\cU^{\pi,\theta}(\beta)$. 
\begin{lemma}[Confidence Set $\CI_{\cU}^{\pi,\theta}(\beta)$]\label{lem:leader-bellman-loss}
    Suppose that each trajectory in the data is independently collected and the function class $\cU$ satisfies the realizability and the completeness assumption given by \Cref{thm:Offline-MG}. Suppose we have 
    % \todo{
    $\beta \ge 
    {110 H^2\cdot\log(H \cN_\rho(\cY, T^{-1})\delta^{-1}) }  + (45 H^2 + 60 H )
  $, where the covering number is defined by \eqref{eq:cN-cY}.
%   }  
  Here, we have a joint class $\cY_h = \Theta_{h+1}\times \Pi_{h+1}\times \cU^2$ and the $\epsilon$-covering number $\cN_\rho(\cY_h,\epsilon)$ is with respect to the distance $\rho$ specified in \eqref{eq:rho-cY}.
%   following distance:
%   \begin{align*}
%     &\nbr{y-\tilde y}_\infty \nend
%     &\quad = \max_{h\in[H]}\cbr{\bignbr{U_h-\tilde U_h}_\infty,  \bignbr{U_{h+1}-\tilde U_{h+1}}_\infty, \sup_{s_{h+1}\in\cS}\bignbr{(\pi_{h+1}\otimes \nu_{h+1}^{\pi, \theta}-\tilde \pi_{h+1}\otimes \nu_{h+1}^{\tilde\pi, \tilde\theta})(\cdot, \cdot\given s_{h+1})}_\TV}.
% \end{align*}
Then for any $h\in[H], \theta\in\Theta, \pi\in\Pi$, we have with probability at least $1-\delta$: (i) $U^{\pi, \theta}\in \CI_\cU^{\pi,\theta}(\beta)$; (ii) for any $\tilde U\in\CI_\cU^{\pi,\theta}(\beta)$, $\EE_\cD[\|\tilde U_{h} - \TT_{h}^{\pi,\theta}\tilde U_{h + 1}\|^2]\le 4\beta T^{-1}$, where $\EE_\cD$ is the expectation taken with respect to the data generating distribution.

% \Zhuoran{Comment on linear case.} 


% $ \cN_{\mathrm{cov}} =  \cN  ( 1/T)$ 
    \begin{proof}
        See \Cref{sec:proof-leader-bellman-loss} for a detailed proof.
    \end{proof}
\end{lemma}


% \begin{corollary}[\textit{Offline guarantee for the confidence set of the leader's value function}]\label{cor:CI-U}
%     Suppose that each trajectory in the data is independently collected and the function class $\cU$ satisfies the realizability and the completeness assumption given by \Cref{thm:Offline-MG}.  By selecting $\beta = \epsilon_S$ where $\epsilon_S$ in given in \Cref{lem:leader-bellman-loss}, 
%     \begin{proof}
       
%     \end{proof}
% \end{corollary}

\paragraph{Learning Leader's Value Function in Online Setting.}
In this subsection, we provide gurantee for online learning the leader's value function. The analysis in this subsection maily follows \citet{jin2021bellman}.
Recall the online Bellman loss given by \eqref{eq:online-MG-bellman loss} in \Cref{sec:myopic-online}, 
\begin{align*}
    &\ell_h^t(U_h', U_{h+1}, \theta_{h+1}) \nend
    &\quad = \sum_{i=1}^{t-1}  \rbr{U_h'(s_h^i, a_h^i, b_h^i) - u_h^i -  \max_{\pi_{h+1}\in\sA}\inp[\Big]{U_{h+1}(s_{h+1}^i, \cdot, \cdot)}{\pi_{h+1}\otimes \nu_{h+1}^{\pi, \theta}(\cdot, \cdot\given s_{h+1}^i)}}^2.
\end{align*}
We define confidence set for each $\theta\in\Theta, t\in[T]$ as 
\begin{align*}
    \CI_\cU^{t, \theta}(\beta) = \cbr{U\in\cU: \ell_h^t(U_h, U_{h+1}, \theta_{h+1}, \pi) - \inf_{U'\in\cU_h} \ell_h^t(U', U_{h+1}, \theta_{h+1}, \pi)\le \beta, \forall h\in[H]}.
\end{align*}
In the sequel, we define $U^{*, \theta}$ as the optimal value function if the follower's true response model is $\theta$. 
Specifically, $U^{*, \theta}$ satisfies
\begin{align*}
    U_h^{*, \theta}(s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \bigrbr{\bigrbr{P_h\circ  T_{h+1}^{*, \theta}} U_{h+1}^{*, \theta}} (s_h, a_h, b_h), 
\end{align*}
where $ T_h^{*,\theta}:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS)$ is the policy optimization operator defined as
\begin{align*}
     T_{h}^{*,\theta} f(s_h) = \max_{\pi_h\in\sA} \dotp{f(s_h,\cdot,\cdot)}{\pi_h\otimes \nu^{\pi, \theta}(\cdot, \cdot\given s_h)}.
\end{align*}
With respect to $ T^{*,\theta}$, we define the optimistic Bellman operator for the leader 
$\TT_h^{*,\theta}:\sF(\cS\times\cA\times\cB)\rightarrow \sF(\cS\times\cA\times\cB)$ as 
\begin{align*}
    \big(\TT_h^{*,\theta} f \bigr) 
    (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE_{s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)} \bigsbr{\bigrbr{ T_{h+1}^{*,\theta}f}(s_{h+1})}.
\end{align*}
We have the following guarantee on the confidence set.
\begin{lemma}[\textrm{Online guarantee for the confidence set of the leader's value function}]\label{lem:CI-U-online}
    For the online setting and the function class $\cU$ that satisfies the realizability and the completeness assumption given by \Cref{thm:Online-MG}. We consider a joint function class $\cZ_h=\cU^2\times\Theta_{h+1}$ and denote by $\cN_\rho(\cZ_{h}, \epsilon)$ the covering number of the smallest $\epsilon$-covering net for $\cZ_h$ with respect to this distance $\rho$ defined in \eqref{eq:rho-cZ}.
    By selecting $\beta \ge \epsilon_S=c H^2 \allowbreak \log(HT\cN_\rho(\cZ, T^{-1})\delta^{-1}) + (45 H^2 + 60 B_u)$ for some universal constant $c$ where the covering number is defined by \eqref{eq:cN-cZ}, for any $t\in[T], h\in[H], \theta\in\Theta$, we have with probability at least $1-\delta$: (i) $U^{*, \theta}\in \CI_\cU^{t,\theta}(\beta)$; (ii) for any $\tilde U\in\CI_\cU^{t,\theta}(\beta)$, $\sum_{i=1}^{t-1}\EE^{\pi^i}[\orbr{\orbr{\tilde U_{h} - \TT_{h}^{*,\theta}\tilde U_{h + 1}}(s_h, a_h, b_h)}^2]\le 4\beta$ and $\sum_{i=1}^{t-1} \orbr{\orbr{\tilde U_{h} - \TT_{h}^{*, \theta}\tilde U_{h + 1}}(s_h^i, a_h^i, b_h^i)}^2 \le 4 \beta$.
    \begin{proof}
        See \Cref{sec:proof-CI-U-online} for a detailed proof.
    \end{proof}
\end{lemma}




\subsection{Putting Everything Together:  Bounding Leader's Suboptimality}\label{sec:app-connection}

Here, we give a summary of the results presented in this section and show how the results in the previous parts are connected with each other for obtaining a guarantee of the suboptimality.

\vspace{5pt} 
{\noindent \bf Controlling Leader's Bellman Error.}
In \Cref{sec:app-subopt-decompose}, we study the suboptimality decomposition for the leader. From \Cref{lem:subopt-decomposition}, we learn that the suboptimality comprises two major terms, namely the leader's Bellman error and the follower's response error. The leader's Bellman error is simply given by 
\begin{align*}
    \text{Leader's Bellman error} = \sum_{h=1}^H \EE\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)-  T_{h+1}^{\pi,\tilde\nu} \tilde U_{h+1}(s_{h+1})} = \sum_{h=1}^H \EE\sbr{\tilde U_h - \TT_h^{\pi,\tilde\theta} \tilde U_{h+1}}, 
\end{align*}
where a list of definitions for $\TT_h$ and $ T_h$ can be found in \Cref{sec:app-notations}. For the offline setting with independent collected data, we use the guarantee from \Cref{lem:leader-bellman-loss} that $\EE_\cD[\|\tilde U_{h} - \TT_{h}^{\pi,\theta} \tilde U_{h + 1}\|^2]\le 4\beta T^{-1}$ if $\tilde U$ if properly chosen from the confidence set $\CI_\cU^{\pi,\theta}(\beta)$. 
For the online setting, we employ  \Cref{lem:CI-U-online} to show that $\sum_{i=1}^{t-1}\EE^{\pi^i}[\|\tilde U_{h} - \TT_{h}^{\theta}\tilde U_{h + 1}\|^2]\le 4\beta$ if $\tilde U$ is properly chosen such that  $\tilde U\in\CI_\cU^{t,\theta}(\beta)$. 
Moreover, in the online setting, $\pi$ is just the optimistic policy and $\TT_h^{\pi,\theta} = \TT_h^{\theta}$. Hence, the leader's Bellman error and the value function guarantee matches and we can control the leader's Bellman error by a distribution shift argument leveraging concentrability coefficients in the offline setting or via the eluder dimension of a proper function class that captures the complexity of this Bellman error in the online setting.
% \Zhuoran{How to control? via analysis of prediction eror and  distributional shift or offlien data (offline), eluder dimension of a proper function class that captures the complexity of the Bellman error (online)}
%how to control the leader's Bellman error is straightforward.

\vspace{5pt} 
{\noindent \bf Controlling Myopic Follower's Quantal Response Error.}
We first look at the easier setting where we aim to control a myopic follower's quantal response error, which is given by the TV distance betweem $\nu$ and $\tilde \nu$.
Recall from \Cref{cor:response-diff-myopic} that for a given state $s\in\cS$,
\begin{align*}
    \EE D_\TV\rbr{\nu(\cdot\given s), \tilde\nu(\cdot\given s)}
        &\le  \eta  \EE\sbr{\abr{(\tilde r^\pi(s, b) - r^\pi(s, b)) - \EE\bigsbr{\tilde r^\pi(s, b) - r^\pi(s, b)}}} \nend
        &\qquad + C^{(3)}\EE\sbr{\rbr{\rbr{\tilde r^\pi(s, b) -r^\pi(s, b)} - \EE\sbr{\tilde r^\pi(s, b) -r^\pi(s, b)}}^2}.
\end{align*}
If we look at the guarantee of MLE in \eqref{eq:MLE_guarantee_Q} of \Cref{lem:MLE-formal} for the myopic case, we directly have 
\begin{align*}
    \sum_{i=1}^{t-1} \EE^{\pi^i, M^*}{\Var_{s_h}^{\pi^i, \theta^*} \bigsbr{r^{\pi^i, \theta}(s, b) - r^{\pi^i, \theta^*}(s, b)}} \le  2 C_\eta^2 \beta, 
\end{align*}
for both the online and the offline cases, which gives control to both the first order and the second order terms in the TV distance upper bound.


\vspace{5pt} 
{\noindent \bf Controlling Farsighted Follower's Quantal Response Error.}
The last part is a more challenging case for a farsighted follower. Using the result in \Cref{lem:performance diff}, 
\begin{align*}
    \text{Quantal response error} \le C^{(0)}
    \sum_{h=1}^H \underbrace{\EE\sbr{\abr{\tilde \Delta^{(1)}_h(s_h, b_h)}}}_{\ds\text{1st-order error}}  + C^{(2)}
    \max_{h\in [H]} \underbrace{\EE\sbr{ \rbr{\tilde Q_h - r_h^\pi - \gamma P_h^\pi \tilde V_{h+1}}^2}}_{\ds\text{2nd-order error}}, 
\end{align*}
with $\tilde\Delta^{(1)}$ given by 
\begin{align*}
    \tilde \Delta^{(1)}_h(s_h, b_h) &=  \rbr{\EE_{s_h, b_h} -\EE_{s_h}}\Biggsbr{\sum_{l=h}^H \gamma^{l-h}\underbrace{\rbr{\tilde Q_l - r_l^\pi - \gamma P_l^\pi \tilde V_{l+1}}(s_l, b_l)}_{\ds\text{Follower's Bellman error}}}, 
\end{align*}
it is not straightforward to see how to bound these two terms by guarantee of the MLE in \Cref{lem:MLE-formal}. 
Fortunately,  we have the following two lemmas that  bound  the first-order error and the second-order error  separately.
\begin{lemma}[Bounding First-Order Error]\label{lem:1st-ub}
    For any $\pi\in\Pi$ and $(\tilde U, \tilde W, \tilde Q, \tilde V, \tilde A, \tilde \nu)$ satisfying the conditions in \Cref{lem:subopt-decomposition}, we have for all $h\in[H]$ that
    \begin{align*}
        \EE\sbr{\abr{\tilde \Delta^{(1)}_h(s_h, b_h)}} &\le   L^{(1)} \cdot
        \max_{h\in[H]} \EE \bigl [  D_\H(\nu_h(\cdot\given s_h),\tilde\nu_h(\cdot\given s_h)) \bigr ] , 
    \end{align*}
    where $L^{(1)} = 6(\eta^{-1}+2 B_A)\cdot {\eff_H\rbr{\gamma}}$ and  $\eff_H(\gamma) = \orbr{1-\gamma^H}/\orbr{1-\gamma}$ is the effective foresight of the follower. For the second order, we have
    \begin{align}\label{eq:1st-ub-2}
        &\bigrbr{\tilde \Delta_h^{(1)}(s_h, b_h)}^2  \nend
        &\quad \le 2 \rbr{\rbr{\EE_{s_h, b_h}-\EE_{s_h}} \bigsbr{\orbr{Q_h - \tilde Q_h}(s_h, b_h)}}^2 \\
        &\qqquad + 16 \gamma^2  \rbr{\eta^{-1} +2 B_A}^2\eff_H(\gamma) \sum_{l=h+1}^H \gamma^{l-h-1} {\rbr{\EE_{s_h}+\EE_{s_h, b_h}}\sbr{D_\H^2(\nu_l(\cdot\given s_l), \tilde\nu_l(\cdot\given s_l))}}. \notag 
    \end{align}
        \begin{proof}
            See \Cref{sec:proof-1st-ub} for a detailed proof.
        \end{proof}
    \end{lemma}
An important observation is that the first order term is bounded only by the follower's quantal response distance without invoking any transition model error, even for farsighted follower. This is because that the first order term only captures parts of the follower's response error. We next bound the second order term.
\begin{lemma}[Bounding Second-Order Error] \label{lem:2nd-ub}
    For any $\pi\in\Pi$ and $(\tilde U, \tilde W, \tilde Q, \tilde V, \tilde A, \tilde \nu)$ satisfying the conditions in \Cref{lem:subopt-decomposition}, we additionally assume $\tilde Q_h(s_h, b_h) = \tilde r_h^\pi(s_h, b_h) + (\tilde P_h^{\pi} \tilde V_{h+1})(s_h, b_h)$ for estimated reward $\tilde r$ and transition kernel $\tilde P$. Suppose that the follower's reward at each state satisfies a linear constraint $\dotp{x}{r_h(s_h, a_h, \cdot)} = \varsigma$ at every $(s_h,a_h)\in\cS\times\cA$ for some $x:\cB\rightarrow \RR$ such that $\inp{\ind}{x}\neq 0$ and $\varsigma\in\RR$. Define ratio $\kappa = \nbr{x}_\infty/|\inp{x}{\ind}|$. Then we have that
    \begin{align*}
        &\max_{h\in[H]}\EE\sbr{ \rbr{\rbr{\tilde Q_h - r_h^\pi - \gamma P_h^\pi \tilde V_{h+1}}(s_h, b_h)}^2} \nend
        &\quad \le L^{(2)} \max_{h\in[H]}\cbr{\EE D_\H^2(\nu_h(\cdot\given s_h),\tilde\nu_h(\cdot\given s_h))+\EE D_\TV^2(P_h^\pi(\cdot\given s_h, b_h),\tilde P_h^\pi(\cdot\given s_h, b_h))}, 
    \end{align*}
    where $L^{(2)} = c H^2 \eff_H(c_2)^2 \kappa^2 \exp\rbr{8\eta B_A} (\eta^{-1}+B_A)^2$ for some absolute constant $c>0$, and 
    $$c_2 = \gamma \rbr{2\exp\rbr{2\eta B_A}+\kappa\exp\rbr{4\eta B_A} }.$$
    \begin{proof}
        See \Cref{sec:proof-2nd-ub} for a detailed proof.
    \end{proof}
\end{lemma}

In the farsighted follower case, we will intensively turn to these two lemmas to control the first- and second-order terms both online and offline. Specifically, the first term in \eqref{eq:1st-ub-2} can be controlled by \eqref{eq:MLE-guarantee-Q-3} in \Cref{lem:MLE-formal}, while the second term is just the Hellinger distance, which can be controlled by our guarantee in \eqref{eq:MLE-guarantee-hellinger-1}. The argument for \Cref{lem:2nd-ub} is quite the same, while both the Hellinger distance of the quantal response and the TV distance of the transition kernel is controllable, as we will see in \Cref{lem:MLE}.

% \subsection{Proof for \Cref{sec:app-major-tech}}

% \subsubsection{Proof of \Cref{lem:performance diff}}\label{sec:proof-performance diff}
% \input{appendix/app_B/prf_perform_diff.tex}

% \subsubsection{Proof of \Cref{cor:response-diff-myopic}}
% \label{sec:proof-response-diff-myopic}
% \input{appendix/app_B/response_diff_myopic.tex}

% % \subsubsection{Proof of \Cref{lem:response diff-myopic}}
% % \label{sec:proof-formal-response diff-linear}
% % \input{appendix/app_B/response_diff_linear.tex}

% \subsubsection{Proof of \Cref{lem:MLE-formal}}
% \label{sec:proof-MLE-general}
% \input{appendix/app_B/MLE_guarantee.tex}

% \subsubsection{Proof of \Cref{lem:MLE-indep-data}}\label{sec:proof-MLE-indep-data}
% \input{appendix/app_B/MLE_indepdata.tex}

% \subsubsection{Proof of \Cref{lem:leader-bellman-loss}}\label{sec:proof-leader-bellman-loss}
% \input{appendix/app_B/value_func.tex}

% \subsubsection{Proof of \Cref{lem:CI-U-online}}\label{sec:proof-CI-U-online}
% \input{appendix/app_B/value_func_online.tex}

% \subsubsection{Proof of \Cref{lem:1st-ub}}
% \label{sec:proof-1st-ub}
% \input{appendix/app_B/1st_order_term_ub.tex}

% \subsubsection{Proof of \Cref{lem:2nd-ub}}
% \label{sec:proof-2nd-ub}
% \input{appendix/app_B/2nd_order_term_ub.tex}