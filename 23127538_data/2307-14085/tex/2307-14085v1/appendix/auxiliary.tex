\subsection{Other Auxiliary Lemmas}
\begin{lemma}[Lemma A.4 in \citet{foster2021statistical}]\label{lem:freeman-variation}
  For any sequence of real-valued random variables $(X_t)_{t\in[T]}$ adapted to a filtration $(\cF_t)_{t\in[T]}$, it holds with probability at least $1-\delta$ for all $t\in[T]$ that 
  \begin{align*}
      \sum_{i=1}^{t}X_i\le \sum_{i=1}^t \log\rbr{\EE\sbr{e^{X_i}\biggiven \sF_{i-1}}} + \log(\delta^{-1}).
  \end{align*}
\end{lemma}

\begin{lemma}[Bernstein inequality]\label{lem:bernstein}
  For independent random variables $Z_1,\dots, Z_i,\dots, Z_T$ such that $|Z_i-\EE Z_i|\le B$ and $\Var[Z_i]\le L \EE[Z_i]$, we have with probability at least $1-\delta$ that
  \begin{align*}
    \abr{\frac 1 T \sum_{i=1}^T Z_i - \EE\sbr{Z_i}} &\le \frac{1}{2T}\sum_{i=1}^T \EE [Z_i] + \frac{(2L +4B/3)\log(2\delta^{-1})}{T}.
  \end{align*}
\end{lemma}
  \begin{proof}
    By a standard Bernstein inequality, 
\begin{align*}
  \abr{\frac 1 T \sum_{i=1}^T Z_i - \EE\sbr{Z_i}} &\le \sqrt{\frac{4\sum_{i=1}^T\Var[Z_i]\log(2\delta^{-1})}{T^2}} + \frac{4 B \log(2\delta^{-1})}{3T} \nend
    &\le \sqrt{\frac{4 L\sum_{i=1}^T \EE[Z_i]\log(2\delta^{-1})}{T^2}} + \frac{4 B\log(2\delta^{-1})}{3T}\nend
    &\le \frac{1}{2T}\sum_{i=1}^T \EE [Z_i] + \frac{(2L +4B/3)\log(2\delta^{-1})}{T}.
  \end{align*}
  Therefore, we conclude the proof.
\end{proof}


\begin{lemma}[Freedman's inequality, Lemma 9 in \citet{agarwal2014taming}]\label{lem:freedman}
  Let $X_1, X_2, \dotsc, X_T$ be a sequence of real-valued random variables adapted to a filtration $\{\sF_t\}_{t\in[T]}$.
  Assume for all $t \in \{1,2,\dotsc,T\}$, $X_t \leq R$ and
  $\EE[X_t|\sF_{t-1}] = 0$.
  Define $S := \sum_{t=1}^T X_t$ and $V := \sum_{t=1}^T
  \EE[X_t^2|\sF_{t-1}]$.
  For any $\delta \in (0,1)$
  and $\lambda \in [0,1/R]$,
  with probability at least $1-\delta$,
  \begin{equation}
    S \leq (e-2)\lambda V + \frac{\log(1/\delta)}{\lambda}\le \lambda V + \frac{\log(1/\delta)}{\lambda}.\label{eq:freedman ineq}
  \end{equation}
  \end{lemma}

  \begin{corollary}[Martingale concentration derived from Freedman's inequality] \label{cor:martigale concentration}
    Let $X_1, X_2, \dotsc, X_T$ be a sequence of real-valued random variables adapted to a filtration $\{\cF_t\}_{t\in[T]}$.
    Suppose that $|X_t - \EE\sbr{X_t\given \sF_{t-1}}| \le R$ for all $t\in[T]$.
    For any $\delta \in (0,1)$,
  with probability at least $1-\delta$,
  \begin{align}
    \abr{\sum_{t=1}^T X_t - \EE\sbr{X_t\given \sF_{t-1}}} \le  R\sqrt{T} \log(2e\delta^{-1}). \label{eq:martingale-1}
  \end{align}
  Moreover, suppose that $X_t\in[0, R]$ for any $t\in[T]$, with probability at least $1-\delta$, 
  \begin{align}
    \frac 1 2\sum_{t=1}^T \EE\sbr{X_t\given \sF_{t-1}} - 2R\log(2\delta^{-1}) \le \sum_{t=1}^T X_t \le \frac 3 2 \sum_{t=1}^T \EE\sbr{X_t\given \sF_{t-1}} + 2R\log(2\delta^{-1}). \label{eq:martingale-2} 
  \end{align}
  \end{corollary}

\begin{proof}
  We let $Z_t = X_t - \EE[X_t\given \sF_{t-1}]$ and it is obvious that $\EE[Z_t\given \sF_{t-1}] = 0$ and $|Z_t|\le R$.
  For \eqref{eq:martingale-1}, we use \eqref{eq:freedman ineq} for sequence $\{Z_t\}_{t\in[T]}$, and it holds with probability at least $1-\delta/2$
  \begin{align}
    \sum_{t=1}^T X_t - \EE\sbr{X_t\given \sF_{t-1}} 
    &\le  \lambda \sum_{t=1}^T \EE[Z_t^2\given \sF_{t-1}] + \lambda^{-1} \log(2\delta^{-1})\label{eq:martingale-proof-1}\\
    &\le  \lambda T R^2 + \lambda^{-1} \log(2\delta^{-1}). \notag
  \end{align}
  We let $\lambda = (R\sqrt T)^{-1}$ an have with probability $1-\delta/2$ that
  \begin{align*}
    \sum_{t=1}^T X_t - \EE\sbr{X_t\given \sF_{t-1}} \le  R\sqrt{T} \log(2e\delta^{-1}), 
  \end{align*}
  Using \eqref{eq:freedman ineq} again for $\{-Z_t\}_{t\in[T]}$, we obtain the opposite side, which implies that \eqref{eq:martingale-1} holds with probability at least $1-\delta$. 

  For \eqref{eq:martingale-2}, we notice from \eqref{eq:martingale-proof-1} that with probability at least $1-\delta/2$,
  \begin{align*}
    \sum_{t=1}^T X_t - \EE\sbr{X_t\given \sF_{t-1}} 
    &\le  \lambda \sum_{t=1}^T \EE[Z_t^2\given \sF_{t-1}] + \lambda^{-1} \log(2\delta^{-1})\nend
    &\le  \lambda \sum_{t=1}^T \EE[X_t^2\given \sF_{t-1}] + \lambda^{-1} \log(2\delta^{-1})\nend
    & \le \lambda R \sum_{t=1}^T \EE[X_t\given \sF_{t-1}] + \lambda^{-1} \log(2\delta^{-1}), 
  \end{align*}
  where the last inequality holds by the nonnegativity of $X_t$. Applying this inequality to $\{-Z_t\}_{t\in[T]}$ as well, we conclude with probability at least $1-\delta$ that
  \begin{align*}
    \abr{\sum_{t=1}^T X_t - \EE\sbr{X_t\given \sF_{t-1}}} \le \lambda R \sum_{t=1}^T \EE[X_t\given \sF_{t-1}] + \lambda^{-1} \log(2\delta^{-1}).
  \end{align*}
  We take $\lambda = 1/(2R)$, which gives the result in \eqref{eq:martingale-2}.
\end{proof}
% \begin{theorem}[Adaptation of Theorem 11.6 from~\citet{gyorfi2002distribution}]
%     \label{thm:11_6_gyorfi}
%     Let $B \geq 1$ and let $\cG$ be a class of functions $g: \RR^{d} \to [0, B]$. Let $Z_{1}, Z_{2}, \ldots, Z_{K}$ be i.i.d.~$\RR^{d}$-valued random variables. Assume $\alpha > 0$, $0 < \epsilon < 1$, and $K \geq 1$. Then
%     \[
%       \Pr\rbr{\sup_{g \in \cG}\frac{\frac{1}{K}\sum_{j = 1}^{K}g(Z_{j}) - \EE[Z_{j}]}{\alpha + \frac{1}{K}\sum_{j = 1}^{K}g(Z_{j}) + \EE[Z_{j}]} > \epsilon} \leq 4\cN_{\infty}\rbr{\frac{\alpha\epsilon}{5}, \cG}\exp\rbr{-\frac{3\epsilon^{2}\alpha K}{40B}}.
%     \]
%     \end{theorem}

% \begin{lemma}[\textit{Concentration of Self-Normalized Processes \citep{abbasi2011improved}}]
%   Let $\{\cF_t \}^\infty_{t=0}$ be a filtration and $\{\epsilon_t\}^\infty_{t=1}$ be an $\RR$-valued stochastic process such that $\epsilon_t$ is $\sF_{t} $-measurable for all $t\geq 1$.
%   Moreover, suppose that conditioning on $\sF_{t-1}$, 
%    $\epsilon_t $ is a  zero-mean and $\sigma$-sub-Gaussian random variable for all $t\geq 1$, that is,  
%    \$%\label{eq:def_subgaussian} 
%     \EE[\epsilon_t\given \sF_{t-1}]=0,\qquad \EE\bigl[ \exp(\lambda \epsilon_t) \biggiven \sF_{t-1}\bigr]\leq \exp(\lambda^2\sigma^2/2) , \qquad \forall \lambda \in \RR. 
%     \$
%    Meanwhile, let $\{\phi_t\}_{t=1}^\infty$ be an $\RR^d$-valued stochastic process such that  $\phi_t $  is $\sF_{t -1}$-measurable for all $ t\geq 1$. 
%   Also, let  $M_0 \in \RR^{d\times d}$ be a  deterministic positive-definite matrix and 
%   \$
%   M_t = M_0 + \sum_{s=1}^t \phi_s\phi_s^\top
%   \$ for all $t\geq 1$. For all $\delta>0$, it holds that
%   \begin{equation*}
%   \Big\| \sum_{s=1}^t \phi_s \epsilon_s \Big\|_{ M_t ^{-1}}^2 \leq 2\sigma^2\cdot  \log \Bigl( \frac{\det(M_t)^{1/2}\cdot \det(M_0)^{- 1/2}}{\delta} \Bigr)
%   \end{equation*}
%   for all $t\ge1$ with probability at least $1-\delta$.
%   \label{lem:concen_self_normalized}
%   \end{lemma}

\begin{lemma}[Elliptical potential  lemma for vectors, Proposition 1 in \citet{carpentier2020elliptical}]\label{lem:elliptical potential}
Let $u_1,\ldots,u_T$ be a sequence of arbitrary vectors in $\mathbb{R}^d$ such that $\|u_i\|_2 \leq 1$. For any $1\leq t\leq T$, we define
\[
V_t = \sum_{s=1}^{t-1} u_s u_s^\top +\lambda I\, .
\]
It then holds that
\[
  \sum_{t=1}^T \|u_t\|_{V_{t}^{-1}} \le \sum_{t=1}^T \|u_t\|_{V_{t+1}^{-1}} \leq \sqrt{Td\log\left(\frac{T+d\lambda}{d\lambda}\right)}, 
\]
and also that
\[
  \sum_{t=1}^T \|u_t\|_{V_{t}^{-1}}^2 \le 2 \log\rbr{{\det(V_T)}}
\]
\end{lemma}

\begin{lemma}[Elliptical Potential Lemma for matrices]
  \label{lem:potential-matrix}
  Suppose $U_0 = \lambda I_d$, $U_t = U_{t-1} + X_t$ for $X_t\in\SSS_+^d$, and $\trace\rbr{X_t} \leq L$, then
  \begin{equation*}
    \sum_{t=1}^T \sqrt{\trace\rbr{U_{t-1}^{-1} X_t}} \leq \sqrt{\frac{LT/\lambda}{\log\rbr{1+L/\lambda}}\cdot d \log\rbr{1+\frac{LT}{\lambda d}}}, 
  \end{equation*}
  and also
  \begin{align*}
    \sum_{t=1}^T {\trace\rbr{U_{t-1}^{-1} X_t}} \le \frac{L/\lambda}{\log\rbr{1+L/\lambda}}\cdot d \log\rbr{1+\frac{LT}{\lambda d}}
  \end{align*}
\end{lemma}

\begin{proof}
  First, we have the following decomposition,
  \[
      U_t = U_{t-1} + X_tX_t^{\top} = U_{t-1}^{\frac{1}{2}}(I + U_{t-1}^{-1/2}X_tU_{t-1}^{-1/2})U_{t-1}^{\frac{1}{2}}.
  \]

  Taking the determinant on both sides, we get
  \[
  \det(U_t) = \det(U_{t-1}) \det(I + U_{t-1}^{-1/2}X_tU_{t-1}^{-1/2}),
  \]
  which follows from $\det(I_d + X)\ge 1+\trace(X) $ for $X\in\SSS_+^d$ that
  \[
     \det(U_t) = \det(U_{t-1}) \rbr{1 + \trace\rbr{U_{t-1}^{-1} X_t}}.
\]
  By taking advantage of the telescope structure, we have
  \[
    \begin{split}
           & \sum_{t=1}^T \log\rbr{1 + \trace\rbr{U_{t-1}^{-1} X_t}} \leq \log \frac{\det(U_T)}{\det(U_0)} \leq d\log \left(1 + \frac{LT}{\lambda d}\right),
    \end{split}
  \]
  where the last inequality follows from the fact that $\mbox{Tr}(U_T) \leq \mbox{Tr}(U_0) + LT = \lambda d + LT$, and thus $\det(U_T) \leq (\lambda + LT/d)^d$. Moreover, by noting that $\log(1+x)\ge \log(1+B)\cdot x/B$, we have 
  \begin{align*}
    \sum_{t=1}^T \trace\rbr{U_{t-1}^{-1} X_t} &\le \frac{L/\lambda}{\log\rbr{1+L/\lambda}} \cdot \sum_{t=1}^T \log\rbr{1+\trace\rbr{U_{t-1}^{-1} X_t}} \nend
    &\le \frac{L/\lambda}{\log\rbr{1+L/\lambda}}\cdot d \log\rbr{1+\frac{LT}{\lambda d}}.
  \end{align*} 
  Therefore, Cauchy-Schwarz inequality implies,
  \[
    \sum_{t=1}^T \sqrt{\trace\rbr{U_{t-1}^{-1} X_t}} \leq \sqrt{T\sum_{t=1}^T \trace\rbr{U_{t-1}^{-1} X_t}} \leq \sqrt{\frac{LT/\lambda}{\log\rbr{1+L/\lambda}}\cdot d \log\rbr{1+\frac{LT}{\lambda d}}}.
  \]
  Therefore, we conclude the proof.
\end{proof}


\begin{lemma}[Bounding Cumulative Error with Eluder Dimension, adapted from Lemma 41 in \citet{jin2021bellman}]
  \label{lem:de-regret}
  Given a function class $\cG$ defined on $\cX$ with $|g(x)|\le B$ for all $(g,x)\in\cG\times\cX$, and a family of finite signed measures $\sP$ over $\cX$. 
  Suppose sequence $\{g_t\}_{t=1}^{T}\subset \cG$ and $\{\rho_t\}_{t=1}^{T}\subset\sP$ satisfy that for all $t\in[T]$,
  $\sum_{i=1}^{t-1} (\EE_{\rho_i} [g_t])^2 \le \beta$. Then for all $t\in[T]$ and $\omega>0$ we have for the first order cumulative error that
  \begin{align}\label{eq:DE error-1st order}
      \sum_{i=1}^{t} \abr{\int_{\cX} g_i \rd \rho_i} \le 2\sqrt{\dim_\DE (\cG,\sP,\omega)\beta t}+\min\{t,\dim_\DE (\cG,\sP,\omega)\}B +t\omega, 
  \end{align} 
  and for the second order cumulative error that
  \begin{align}\label{eq:DE error-2nd order}
      \sum_{i=1}^t \rbr{\int_{\cX} g_i \rd \rho_i}^2 \le \dim_\DE(\cG,\sP,\omega)\beta \log(eT) + \min\{t,\dim_\DE (\cG,\sP,\omega)\}B^2 + t\omega^2.
  \end{align}
  \begin{proof}
      We first invoke the following proposition (extended to signed measures) from \citet{jin2021bellman} whose proof can be found in the proof of Proposition 43 in \citet{jin2021bellman}.
      \begin{proposition}\label{prop:ed-sequence length}
          Given a function class $\cG$ defined on $\cX$, and a family of signed measures $\sP$ over $\cX$. 
          Suppose sequences $\{\phi_i\}_{i=1}^{T}\subset \cG$ and $\{\rho_i\}_{i=1}^{T}\subset\sP$ satisfy that for all $t\in[T]$,
          $\sum_{i=1}^{t-1} (\int_\cX \phi_t \rd \rho_i)^2 \le \beta$. Then for all $t\in[T]$,
      $$
          \sum_{i=1}^{t} \ind \cbr{\abr{\int_\cX \phi_i \rd \rho_i} > \epsilon } \leq \rbr{\frac{\beta}{\epsilon^2}+1}\dim_\DE (\cG,\sP,\epsilon).
      $$
      \label{prop:de-regret-prop}
      \end{proposition}
  The following proof is largely the same as those in \citet{jin2021bellman}. For completeness, we still present them here.
  Fix $t \in [T]$ and let $d = \dim_\DE (\cG,\sP,\omega)$. Sort the sequence $\{|\int_{\cX}\phi_1 \rd \rho_1|,\dots,|\int_{\cX}\phi_t \rd \rho_t|\}$ in a decreasing order and denote it by $\{e_1,\dots,e_t\}$ such that $e_1 \geq e_2 \geq \dots \geq e_t$. We have for the cumulative reward of order $k\in\{1, 2\}$ that
$$
  \sum_{i=1}^t |\EE_{\rho_i} [\phi_i]|^k = \sum_{i=1}^t e_i^k = \sum_{i=1}^t e_i^k \ind\big \{e_i \leq \omega \big \} + \sum_{i=1}^t e_i^k \ind\big \{e_i > \omega\big\} \leq t\omega^k + \sum_{i=1}^t e_i^k \ind\big \{e_i > \omega\big\}.
$$
For $i \in [t]$, we want to prove that if $e_i > \omega$, then we have $e_i \leq \min\{\sqrt{{d\beta}/\rbr{i-d}},B\}$. Assume $i\in[t]$ satisfies $e_i >  \omega$. 
Then there exists $\alpha$ such that $e_i >\alpha\ge  \omega$.
By Proposition \ref{prop:de-regret-prop}, we have
$$
i \le 
\sum_{i=1}^t \ind\big \{e_i > \alpha\big\} 
\le \rbr{ \frac{\beta}{\alpha^2} + 1 } \dim_\DE (\cG,\sP,\alpha)
\le 
\rbr{ \frac{\beta}{\alpha^2} + 1 } \dim_\DE (\cG,\sP,\omega),
$$
  where the last inequality holds by definition of the distributional eluder dimension.
The inequality directly implies that $\alpha\le \sqrt{{d\beta}/\rbr{i-d}}$.
Besides, recall $e_i \leq B$, so we have   $e_i \leq \min\{\sqrt{{d\beta}/\rbr{i-d}},B\}$.
Finally, we have
\begin{equation*}
\begin{aligned}
    & \sum_{i=1}^t e_i^k \ind\big \{e_i > \omega\big\}  \nend 
          &\quad \leq \min\{d,t\}B^k+\sum_{i=d+1}^t \rbr{\frac{d\beta}{i-d}}^{k/2} \nend
          &\quad \leq \min\{d,t\}B^k + \ind\cbr{k=1}\cdot \sqrt{d\beta}\int_{0}^t \frac{1}{\sqrt{x}} dx +\ind\cbr{k=2} \cdot  \rbr{d\beta \int_{1}^t x^{-1} \rd x + d\beta}\\
    &\quad \leq \min\{d,t\}B^k + \ind\cbr{k=1}\cdot 2\sqrt{d\beta t} + \ind\cbr{k=2} \cdot d\beta \rbr{\log t +1},
\end{aligned}
\end{equation*}
which completes the proof.
  \end{proof}
\end{lemma}
Note that the above proof as well as \Cref{prop:ed-sequence length} should hold for both the eluder dimension and the distributional eluder dimension.