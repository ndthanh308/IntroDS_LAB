We give a proof for online learning with myopic follower and linear function approximation.

\paragraph{Step 1. Uncertainty Quantification.}
For the leader's side, we are able to invoke the same guarantee in \Cref{lem:Gamma_1-pessi} for all $t\in[T]$ with a union bound, which requires a $\beta$ that additionally has a $\log(T)$ dependency. The reason that \Cref{lem:Gamma_1-pessi} can still be used here is that the data compliance condition is still satisfied in the online learning process. Therefore, with probability at least $1-\delta$, for any $t\in[T], h\in[H]$, 
\begin{align}
    \abr{\rbr{P_h\hat W_{h+1}^t  + u_h - \phi_h^\top \hat\omega_h^t}(s_h, a_h, b_h)} \le \Gamma^{(1, t)}_h(\cdot,\cdot,\cdot), \quad\forall (s_h,a_h,b_h)\in\cS\times\cA\times\cB.\label{eq:OnML-Bellman-error-guarantee}
\end{align}
%  which is even not needed since the core of the proof in \Cref{lem:Gamma_1-pessi}
For the follower's side, we invoke \eqref{eq:app-bandit-ub-1} in \Cref{cor:formal-MLE confset-linear myopic}, and obtain 
\begin{align}
    \max\cbr{\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\theta_h^*}}^2, \bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\hat\theta_h}}^2, 
    \EE^{\pi^i}\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\theta_h^*}}^2, 
    \EE^{\pi^i}\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, t}^{\hat\theta_h}}^2 } \le 8 C_{\eta}^2 \beta, \label{eq:OnML-MLE-guarantee}
\end{align}
for any $\hat\theta_h\in \confset_{h,\Theta}^t(\beta)=\{\theta_h\in\Theta:\cL_{h}^t(\theta_h)\le \min_{\theta_h'\in\Theta_h}\cL_{h}^t(\theta_h') + \beta\}$.
Here, we require $\beta \ge C d\log(HT(1+\eta T^2 +(1+\eta)T)\delta^{-1})$, which takes a union bound over $\Theta_h$, $h\in[H]$, and $t\in[T]$. 
It holds straightforward that the confidence set is valid and accurate. In the sequel, for ease of presentation, we take $\CI_{\Theta}^t(\beta)=\cbr{\{\theta_h\}_{h\in[H]}: \theta_h\in\cC_{h,\Theta}^t(\beta), \forall h\in[H]}$, which is just a combination of these $H$ independent confidence sets, and all the results still hold for $\CI_\Theta^t(\beta)$. In the sequel, if we say $\hat\theta_h\in\CI_\Theta^t(\beta)$, we actually means $\hat\theta_h\in\CI_{h,\Theta}^t(\beta)$. The following part is based on the success of \eqref{eq:OnML-Bellman-error-guarantee} and \eqref{eq:OnML-MLE-guarantee}.

\paragraph{Step 2. Suboptimality Decomposition: Optimism.}
Recall the two schemes in \eqref{eq:scheme-4} and \eqref{eq:scheme-5}, 
\begin{align}
    \textbf{S4:}\quad  \pi_h^t(s_h)  &= \argmax_{\pi_h(s_h) \in\sA
    \atop \theta_h\in\confset_{h, \Theta}^t(\beta)} \inp[\big]{\hat U_h^t(s_h, \cdot,\cdot)}{\pi_h \otimes\nu_h^{\pi_h , \theta_h}(\cdot,\cdot\given s_h)}_{\cA_h\times \cB_h}, \quad \forall s_h\in\cS_h,\label{eq:scheme-4-new}\\
    \textbf{S5:}\quad  \pi_h^t(s_h)  &= \argmax_{\pi_h(s_h) \in\sA}  \inp[\big]{\hat U_h^t(s_h, \cdot,\cdot)}{\pi_h \otimes\nu^{\pi_h , \hat\theta_{h,\MLE}^t}(\cdot,\cdot\given s_h)}_{\cA_h\times \cB_h} + \Gamma^{(2,t)}_h(s_h;\pi_h , \hat\theta_{h, \MLE}^t). \label{eq:scheme-5-new}
\end{align}
Note that $\hat W_h(s_h)$ is simply the optimal value. 
We have for the suboptimality that
\begin{align*}
    \subopt(\hat\pi^t) = J(\pi^*) - J(\hat\pi^t) = \underbrace{J(\pi^*) - \EE\sbr{\hat W_1^t(s_1)}}_{\dr (i)} + {\EE\sbr{\hat W_1^t(s_1)} - J(\hat\pi^t)}, 
\end{align*}
By the performance decomposition of (i) given  by \eqref{eq:subopt-decompose-equality-2} of \Cref{lem:subopt-decomposition} applied with $\pi^*, \hat W^t,\hat U^t, \hat \nu^{*, t}$, where $\hat\nu^{*, t}$ is the optimizer to \eqref{eq:scheme-4-new} or \eqref{eq:scheme-5-new} when fixing $\pi_h=\pi_h^*$. We have that
\begin{align}
    \dr{(i)} &= J(\pi^*) - \EE[\hat W_1^t(s_1)] \nend
    & ={{\sum_{h=1}^H \EE\sbr{-\bigrbr{\hat U_h^t - u_h}(s_h, a_h, b_h) + \hat W_{h+1}^t(s_{h+1})}}} + {\sum_{h=1}^H \EE\sbr{-\hat W_h^t(s_h) + T_{h}^{\pi^*,
    \hat\nu^{*,t}} \hat U_{h}^t(s_{h})}} \nend
    &\qquad + \sum_{h=1}^H \rbr{T_{h}^{\pi^*,
    \nu} - T_{h}^{\pi^*,
    \hat\nu^{*,t}}} \hat U_{h}^t(s_{h})\label{eq:OnML-subopt-1} \\
    &\le {{\sum_{h=1}^H \EE\sbr{-\bigrbr{\hat U_h^t - u_h}(s_h, a_h, b_h) + \hat W_{h+1}^t(s_{h+1})}}} + {\sum_{h=1}^H \EE\sbr{-\hat W_h^t(s_h) + T_{h}^{\pi^*,\hat\nu^{*,t}} \hat U_{h}(s_{h})}}\nend
    &\qqquad + {\sum_{h=1}^H  \EE D_\TV\rbr{\hat\nu_h^{*, t}(\cdot\given s_h), \nu_h(\cdot\given s_h)}}.\label{eq:OnML-subopt-2}
\end{align}
where we define $\nu= \nu^{\pi^*}$ and the expectation is taken with respect to the trajectory induced by $\pi^*$ and the true model. In both Scheme 4 and Scheme 5, we directly have the first term of \eqref{eq:OnML-subopt-1} and \eqref{eq:OnML-subopt-2} nonpositive since $ \hat U_h^t(s_h, a_h, b_h) = \phi_h(s_h,a_h,b_h)^\top \hat\omega_h^t + \Gamma^{(1,t)}_h(s_h, a_h, b_h)\ge P_h \hat W_{h+1}^t + u_h$ by \eqref{eq:OnML-Bellman-error-guarantee}. 
For Scheme 4, we have that the second term in \eqref{eq:OnML-subopt-1} is also nonpositive since $\theta^*\in\CI_\Theta^t(\beta)$, and $\hat W_h^t(s_h)= \max_{\pi_h(s_h)\in\sA, \theta\in\CI_\Theta(\beta)} T_h^{\theta, \pi} \hat U_h^t(s_h) \ge \max_{\theta\in\CI_\Theta^t(\beta)}T_h^{\theta, \pi^*} \hat U_h^t(s_h) = T_h^{\pi^*, \hat\nu^{*,t}}$. Moreover, for the third term, we have it also nonpositive since $\hat\nu^{*,t}$ is the maximizer under $\pi^*$ and the fact that $\theta^*\in\CI_\Theta(\beta)$.
Therefore, we have \eqref{eq:OnML-subopt-1} nonpositive for Scheme 4. 

For Scheme 5, we just check the second term in \eqref{eq:OnML-subopt-2},
\begin{align*}
    &{-\hat W_h^t(s_h) + T_{h}^{\pi^*,\hat\nu^{*,t}} \hat U_{h}(s_{h})}\nend
    &\quad \le  { -
    T_h^{\pi^*, \hat\nu_h^{*, t}} \hat U_h^t(s_h)
     - \Gamma^{(2,t)}_h(s_h;\pi_h^*(s_h) , \hat\theta_{h, \MLE}^t) + T_h^{\pi^*, \hat\nu_h^{*, t}} \hat U_h^t(s_h) }\nend
     &\quad = - \Gamma^{(2,t)}_h(s_h;\pi_h^*(s_h) , \hat\theta_{h, \MLE}^t),
\end{align*}
where the inequality holds by noting that $\hat W_h^t(s_h)$ is maximized over the leader's policy.
Therefore, we have \eqref{eq:OnML-subopt-2} upper bounded by for Scheme 5 that
\begin{align}
    {\dr (i)} 
    &\le \sum_{h=1}^H \EE \rbr{D_\TV\bigrbr{\hat\nu_h^{*, t}(\cdot\given s_h), \nu_h(\cdot\given s_h)} - \Gamma^{(2,t)}_h(s_h;\pi_h^*(s_h) , \hat\theta_{h, \MLE}^t)}. \label{eq:OnML-subopt-3}
\end{align}
where the last inequality holds by \Cref{lem:response diff-myopic-linear} that for both Scheme 4 and Scheme 5,
\begin{align}
    &2 H D_\TV\rbr{\nu_h^{*,t}(\cdot\given s_h), \nu_h(\cdot\given s_h)}\nend
    &\quad \le 2 H\min_{\Psi\in \SSS^d_+}\cbr{f\rbr{\sqrt{\trace\rbr{\Psi^\dagger \Sigma_{s_h}^{\pi^*, \tilde \theta}}} \nbr{\theta_h^* - \tilde \theta}_{\Psi}}, f\rbr{\sqrt{\trace\rbr{\Psi^\dagger \Sigma_{s_h}^{\pi^*, \theta_h^*}}} \nbr{\theta_h^* - \tilde \theta}_{\Psi}}}\nend
    &\quad \le 2 H f\rbr{\sqrt{\trace\rbr{\rbr{\Sigma_{h,t}^{\tilde \theta} + I_d}^\dagger \Sigma_{s_h}^{\pi^*, \tilde \theta}}} \nbr{\theta_h^* - \tilde \theta}_{\Sigma_{h,t}^{\tilde \theta} + I_d}} \nend
    &\quad \le \Gamma^{(2, t)}_h(s_h;\pi_h^*(s_h), \tilde \theta), \label{eq:OnML-TV bound-1}
\end{align}
where we define $\tilde\theta$ as the optimizer to \eqref{eq:scheme-4-new} or \eqref{eq:scheme-5-new} with $\pi^*$ plugged in.
Here, the second inequality holds by plugging $\Psi = \Sigma_{h,t}^{\hat\theta_{s_h}^{*, t}} + I_d$ and only keeping the first.
Recall that 
$\Sigma_{h,t}^{\theta_h} = \sum_{i=1}^{t-1} \Cov_{s_h^i}^{\pi^i, \theta_h} \allowbreak [\phi_h^{\pi^i}(s_h^i, b_h)]$ and  $\Gamma^{(2,t)}_h$ that $\Gamma_h^{(2,t)}(s_h;\pi_h(s_h),\theta_h)=2 H(\eta \xi  + C^{(3)} \xi^2 )
$ with 
\begin{align*}
    \xi = \sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}.
\end{align*}
Moreover, the last inequality of \eqref{eq:OnML-TV bound-1} holds by further noting that $\bignbr{\theta_h^* - \tilde\theta}_{\Sigma_{h, t}^{\tilde\theta}+I_d}^2 \le 8 C_\eta^2 \beta + 4 B_\Theta^2$ by guarantee of \eqref{eq:OnML-MLE-guarantee} and noting that $\tilde\theta\in\CI_\Theta^t(\beta)$ by definition for both Scheme 4 (explicitly in the optimization problem) and Scheme 5 (the MLE estimator is within the confidence set). 
Plugging \eqref{eq:OnML-TV bound-1} into \eqref{eq:OnML-subopt-3}, we conclude that ${\dr (i)}\le 0$
for Scheme 5 as well.

% \paragraph{Controlling Online Regret by Eluder Dimension.}
Summarizing previous results, we have by \eqref{eq:perform-diff-linear} in \Cref{lem:subopt-decomposition} that for $(\hat\pi^t, \hat W^t, \hat U^t, \hat \nu^t)$, where $\hat\nu^t$ is the optimizer to Scheme 4 or Scheme 5 under $\hat\pi^t$,
\begin{align}
    &\sum_{t=1}^T \subopt(\hat\pi^t) \nend
    &\quad\le 
    \sum_{t=1}^T {\EE\sbr{\hat W_1^t(s_1)} - J(\hat\pi^t)}\nend
    &\quad\le \sum_{t=1}^T {\sum_{h=1}^H \EE^t\sbr{\bigrbr{\hat U_h^t - u_h}(s_h, a_h, b_h)- \hat W_{h+1}^t(s_{h+1})}} 
    %%%%%%%%%%%%
    + \sum_{t=1}^T {\sum_{h=1}^H \EE^t\sbr{\hat W_h^t(s_h) -  T_{h}^{\hat\pi^t,\hat \nu^t} \hat U_h^t(s_{h})}}\nend
    %%%%%%%%%%%
    &\qqquad+\sum_{t=1}^T {\sum_{h=1}^H  H \EE^t \nbr{\rbr{\hat \nu_h^t-\nu_h^t}(\cdot\given s_h)}_1}\nend
    &\quad\le \sum_{t=1}^T \sum_{h=1}^H 2 \EE^t \Gamma_h^{(1, t)}(s_h, a_h, b_h) 
    +  \underbrace{\sum_{t=1}^T {\sum_{h=1}^H \EE^t\sbr{\hat W_h^t(s_h) -  T_{h}^{\hat\pi^t,\hat \nu^t} \hat U_h^t(s_{h})}}}_{\dr (ii)}\nend
    %%%%%%%%%%%
    &\qqquad+\sum_{t=1}^T {\sum_{h=1}^H  \Gamma_h^{(2, t)} (s_h;\hat\pi_h^t(s_h), \theta^*)} \label{eq:OnML-subopt-4}
\end{align}
where we define $\nu_h^t$ as the real quantal response under $\hat\pi^t$ and $\EE^t$ is taken with respect to policy $\hat\pi^t$ under the true model. Here, the first inequality holds by optimism, the last inequality holds by the guarantee in \eqref{eq:OnML-Bellman-error-guarantee} in the first term, and the last term comes from \eqref{eq:OnML-TV bound-1}, which says
\begin{align}
    &D_\TV\rbr{\nu_h^t(\cdot\given s_h), \hat\nu_h^t(\cdot\given s_h)}\nend
    &\quad \le \min_{\Psi\in \SSS^d_+}\cbr{f\rbr{\sqrt{\trace\rbr{\Psi^\dagger \Sigma_{s_h}^{\hat\pi^t, \hat \theta^t}}} \nbr{\theta_h^* - \hat \theta^t}_{\Psi}}, f\rbr{\sqrt{\trace\rbr{\Psi^\dagger \Sigma_{s_h}^{\hat\pi^t, \theta_h^*}}} \nbr{\theta_h^* - \hat \theta^t}_{\Psi}}}\nend
    &\quad \le f\rbr{\sqrt{\trace\rbr{\rbr{\Sigma_{h,t}^{\theta^*} + I_d}^\dagger \Sigma_{s_h}^{\hat\pi^t, \theta^*}}} \nbr{\theta_h^* - \hat \theta^t}_{\Sigma_{h,t}^{\theta^*} + I_d}} \nend
    &\quad \le \Gamma^{(2, t)}_h(s_h;\hat\pi_h^t(s_h), \theta^*), \label{eq:OnML-TV bound-2}
\end{align}
where we define $\hat\theta^t$ as the maximizer to \eqref{eq:scheme-4-new} or \eqref{eq:scheme-5-new} at state $s_h$, which also corresponds to $\hat\nu^t$ and $\hat\pi^t$ in the optimization problem. The first inequality follows from \Cref{lem:response diff-myopic-linear}, the second inequality holds by using the second term in the minimization and plug in $\Psi=\Sigma_{h, t}^{\theta^*} + I_d$, and the last inequality just follows from the definition of $\Gamma^{(2, t)}$ and using the fact that 
$\onbr{\theta_h^* - \hat \theta^t}_{\Sigma_{h,t}^{\theta^*} + I_d}^2 \le 8 C_\eta^2 \beta + 4 B_\Theta^2$ since $\hat\theta^t\in\CI_\Theta^2(\beta)$ and using the bound in \eqref{eq:OnML-MLE-guarantee}.
Furthermore, for Scheme 4, we have for term (ii) that ${\dr (ii)}=0$ by definition in \eqref{eq:scheme-4-new}. For Scheme 5, we have
\begin{align*}
    {\dr (ii)} 
    &= \sum_{t=1}^T {\sum_{h=1}^H \EE^t\sbr{\hat W_h^t(s_h) -  T_{h}^{\hat\pi^t,\hat \nu^t} \hat U_h^t(s_{h})}} \nend
    &= \sum_{t=1}^T \sum_{h=1}^H \EE^t \Gamma^{(2, t)}_h \rbr{s_h; \hat\pi_h^t(s_h), \hat\theta_{h,\MLE}} \nend
    &\le \exp\rbr{2\eta B_A}\sum_{t=1}^T \sum_{h=1}^H \EE^t \Gamma_h^{(2, t)}\rbr{s_h; \hat\pi_h^t(s_h), \theta_h^*}, 
\end{align*}
where the last inequaltiy holds by the following proposition. 

\begin{proposition}\label{eq:Gamma_2-ub}
For any $\theta,\tilde\theta\in\Theta$, $\pi\in\Pi, s_h\in\cS$, suppose $\onbr{A_h^{\pi, \theta}(s_h,\cdot)} \le B_A$ and $\onbr{\tilde A_h^{\pi, \theta}(s_h,\cdot)} \le B_A$. We then have that
\begin{align*}
    \Sigma_{s_h}^{\pi, \theta} \le \exp\rbr{2\eta B_A} \Sigma_{s_h}^{\pi, \tilde \theta}, 
\end{align*}
and also
\begin{align*}
    \Gamma_h^{(2,t)}(s_h;\pi_h(s_h),\theta_h) \le \exp\rbr{4\eta B_A} \Gamma_h^{(2,t)}(s_h;\pi_h(s_h),\tilde \theta_h).
\end{align*}
\begin{proof}
To see how we derive the upper bound, we note that by definition $\Gamma_h^{(2,t)}(s_h;\pi_h(s_h),\theta_h)=2 H(\eta \xi  + C^{(3)} \xi^2 )
$ with 
\begin{align*}
    \xi = \sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}.
\end{align*}
Furthermore, by definition of $\Sigma_{h, t}^{\theta_h}$, we have 
\begin{align*}
    \Sigma_{h,t}^{\theta_h} 
    &= \sum_{i=1}^{t-1} \Cov_{s_h^i}^{\pi^i, \theta_h} \sbr{\phi_h^{\pi^i}(s_h^i, b_h)} \nend
    &= \sum_{i=1}^{t-1}  \sum_{b_h', b_h''} \phi_h^{\pi^i}(s_h,b_h') \cdot \H_{s_h}^{\pi^i, \theta_h}(b_h', b_h'') \cdot \phi_h^{\pi^i}(s_h,b_h'')\nend
    &\le \exp\rbr{2\eta B_A } \sum_{i=1}^{t-1}  \sum_{b_h', b_h''} \phi_h^{\pi^i}(s_h,b_h') \cdot \H_{s_h}^{\pi^i, \tilde \theta_h}(b_h', b_h'') \cdot \phi_h^{\pi^i}(s_h,b_h'')\nend
    & = \exp\rbr{2\eta B_A} \Sigma_{h, t}^{\tilde \theta_h}, 
\end{align*}
where we define $\H_{s_h}^{\pi, \theta}(b_h', b_h'') = \diag(\nu_h^{\pi, \theta}(\cdot\given s_h)) - \nu_h^{\pi, \theta}(\cdot\given s_h) \nu_h^{\pi, \theta}(\cdot\given s_h)^\top$ as the Hessian matrix corresponding to $\nu_h^{\pi, \theta}$ at state $s_h$.
Here, the inequality holds by invoking \Cref{prop:Hessian-ulb} and 
a lower bound follows similarly by invoking the lower bound in \Cref{prop:Hessian-ulb}.
Also, the argument for $\Sigma_{s_h}^{\pi,\theta_h} = \Cov_{s_h}^{\pi,\theta_h}[\phi_h^\pi(s_h, b_h)]$ follows the same way. 
Therefore, we have for $\Gamma^{(2, t)}$ that 
\begin{align*}
    \Gamma_h^{(2,t)}(s_h;\pi_h(s_h),\theta_h)
    &=2 H\eta \sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}  \nend
    &\qquad + 2 H C^{(3)} \rbr{\sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}}^2\nend
    &\le 2 H\eta \exp\rbr{2\eta B_A}\sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\tilde\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \tilde\theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}  \nend
    &\qquad + 2 H C^{(3)} \exp\rbr{4\eta B_A} \rbr{\sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\tilde\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \tilde\theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}}^2\nend
    & \le \exp\rbr{4\eta B_A} \Gamma_h^{(2,t)}(s_h;\pi_h(s_h),\tilde\theta_h), 
\end{align*}
which completes the proof.
\end{proof}
\end{proposition}
Therefore, for Scheme 4, 
\begin{align}
    \sum_{t=1}^T \subopt(\hat\pi^t) 
    \le \sum_{t=1}^T \sum_{h=1}^H 2 \EE^t \Gamma_h^{(1, t)}(s_h, a_h, b_h) 
    +  \sum_{t=1}^T {\sum_{h=1}^H  \Gamma_h^{(2, t)} (s_h;\hat\pi_h^t(s_h), \theta^*)}, \label{eq:OnML-scheme-4-regret}
\end{align}
and for Scheme 5, 
\begin{align}
    \sum_{t=1}^T \subopt(\hat\pi^t) 
    \le \sum_{t=1}^T \sum_{h=1}^H 2 \EE^t \Gamma_h^{(1, t)}(s_h, a_h, b_h) 
    +  2\exp(4\eta B_A)\sum_{t=1}^T {\sum_{h=1}^H  \Gamma_h^{(2, t)} (s_h;\hat\pi_h^t(s_h), \theta^*)}. \label{eq:OnML-scheme-5-regret}
\end{align}

By \eqref{eq:OnML-scheme-4-regret} and \eqref{eq:OnML-scheme-5-regret}, we can see that it suffices to bound $\sum_{t=1}^T \EE^t \Gamma_h^{(1, t)}(s_h, a_h, b_h)$ and $\sum_{t=1}^T \EE^t \Gamma_h^{(2,t)}(s_h;\hat\pi_h^t(s_h),\theta_h^*)$.
% \begin{align*}
%     \Gamma^{(2, t)}_h \rbr{s_h;\pi(s_h), \theta_h} = 
% \end{align*}

\paragraph{Controlling the Suboptimality by Eluder Dimension.}
We first bound the leader's Bellman error $\sum_{t=1}^T \EE^t \Gamma_h^{(1, t)}(s_h, a_h, b_h)$.
Note that by linear approximation, 
\begin{align*}
    &\sum_{t=1}^T \EE^t \Gamma_h^{(1, t)}(s_h, a_h, b_h) \nend
    & \quad = \sum_{t=1}^T \EE^t {C_1 d H \allowbreak \sqrt{\log(2d H T^t/\delta)}\cdot \sqrt{\phi_h(s_h, a_h, b_h)^\top (\Lambda_h^t)^{-1}\phi_h(s_h, a_h, b_h)}}\nend
    &\quad \le \sum_{t=1}^T {C_1 d H \allowbreak \sqrt{\log(2d H T^2/\delta)} \cdot \sqrt{\phi_h(s_h^t, a_h^t, b_h^t)^\top (\Lambda_h^t)^{-1}\phi_h(s_h^t, a_h^t, b_h^t)}} \nend
    &\qqquad + C_1 d H \allowbreak \sqrt{\log(2d H T^2/\delta)} B_\phi \sqrt T \log\rbr{He\delta^{-1}} \nend
    &\quad \le {C_1 d H \allowbreak \sqrt{\log(2d H T^2/\delta)} \cdot B_\phi \sqrt{T d \log(T/d)}}  + C_1 d H \allowbreak \sqrt{\log(2d H T^2/\delta)} B_\phi \sqrt T \log\rbr{He\delta^{-1}},
\end{align*}
where the first inequality holds with probability at least $1-\delta$ for all $h\in[H]$ by a standard martingale concentration in \Cref{cor:martigale concentration} with $B_\phi = \nbr{\phi_h}_\infty$ and noting that $\Lambda_h^t\succeq I_d$, and the last inequality holds by definition $\Lambda_h^t = \sum_{i=1}^{t-1} \phi_h(s_h^i, a_h^i, b_h^i)\phi_h(s_h^i, a_h^i, b_h^i)^\top + I_d$, and using the elliptical potential lemma in \Cref{lem:elliptical potential}.

For the follower's response error, we just need to bound 
\begin{align*}
    &\sum_{t=1}^T \EE^t \Gamma_h^{(2,t)}(s_h;\hat\pi_h^t(s_h),\theta_h^*) \nend
    &\quad \lesssim \sum_{t=1}^T 4 H\eta \sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h^*} + I_d}^\dagger  \Sigma_{s_h^t}^{\pi^t,\theta^*} }} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}  \nend
    &\qqquad + \sum_{t=1}^T 4 H C^{(3)} \rbr{\sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h^*} + I_d}^\dagger \Sigma_{s_h^t}^{\pi^t,\theta^*} }} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}}^2 \nend
    &\qqquad + 2 H B_\phi \rbr{\eta \sqrt{8 C_\eta^2 \beta + 4 B_\Theta^2}}\sqrt T \log\rbr{2H\delta^{-1}}+ 8 H^2 B_\phi^2 \rbr{C^{(3)} \sqrt{8 C_\eta^2 \beta + 4 B_\Theta^2}}^2 \log(2H\delta^{-1}),
\end{align*}
where the inequality uses the martingale concentration for nonnegative processes, i.e., \eqref{eq:martingale-1} for the first order term in $\Gamma^{(2, t)}$ and \eqref{eq:martingale-2} in \Cref{cor:martigale concentration} for the second order term.
Next, we justify the use of the elliptical potential lemma in order to bound these terms. 
We remind the readers of the following definitions
\begin{gather*}
    \bigrbr{\Upsilon_h^{\pi,\theta} \phi_h}(s_h, b_h) = \phi_h^\pi(s_h, b_h) - \bigdotp{\phi_h^\pi(s_h, \cdot)}{\nu_h^{\pi, \theta}(\cdot\given s_h)}, \nend
    \Sigma_{s_h}^{\pi,\theta}  = \Cov_{s_h}^{\pi, \theta}\sbr{\phi_h^\pi(s_h, b_h)} = \Cov^{\pi,\theta}\bigsbr{\bigrbr{\Upsilon_h^{\pi,\theta}\phi_h}(s_h, b_h)\biggiven s_h}, \nend
    \Sigma_{h,t}^{\theta} = \sum_{i=1}^{t-1} \Cov_{s_h^i}^{\pi^i,\theta} \bigsbr{\bigrbr{\Upsilon_h^{\pi^i,\theta}\phi_h}(s_h, b_h)\biggiven s_h = s_h^i} = \sum_{i=1}^{t-1} \Sigma_{s_h^i}^{\pi^i,\theta}.
\end{gather*}
Critically, we see that $\Sigma_{h,t}^\theta$ is just a summation of $\Sigma_{s_h^i}^{\pi^i, \theta}$, which is a nonnegative definite $d$-dimensional matrix, and this gives us a self-normalized process. Therefore, we have from \Cref{lem:potential-matrix} the elliptical potential lemma for matrices that
\begin{align*}
    \sum_{t=1}^T \sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h^*} + I_d}^\dagger  \Sigma_{s_h^t}^{\pi^t,\theta^*} }}  &\le \sqrt{C_0 d T\log\rbr{1+4 B_\phi^2 T/d}}, \nend
    \sum_{t=1}^T {\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h^*} + I_d}^\dagger  \Sigma_{s_h^t}^{\pi^t,\theta^*} }} & \le C_0 d \log\rbr{1+ 4 B_\phi^2T/d}.
\end{align*}
where $C_0=4 B_\phi^2 /(\log(1+B_\phi^2))$ and $4B_\phi^2$ upper bounds $\trace\bigrbr{\Sigma_{s_h}^{\pi,\theta^*}}$. 
To see why $B_\phi$ is a valid upper bound, we have by definition of $\Sigma_{s_h}^{\pi, \theta^*}$ that 
\begin{align*}
    \trace\rbr{\Sigma_{s_h}^{\pi, \theta^*}} &= \trace\rbr{\Cov^{\pi,\theta^*}\bigsbr{\bigrbr{\Upsilon_h^{\pi,\theta^*}\phi_h}(s_h, b_h)\biggiven s_h}}\nend
    &\le \max_{s_h, b_h, \pi}{\bignbr{\bigrbr{\Upsilon_h^{\pi,\theta^*}\phi_h}(s_h, b_h)}^2}\nend
    &\le 4B_\phi^2.
\end{align*}

We summarize the result here. For the leader's Bellman error in Scheme 4, we can bound it by 
\begin{align*}
    {\text{LBE}} &\lesssim {C_1 d H^2 \allowbreak \sqrt{\log(2d H T^2/\delta)} \cdot B_\phi \sqrt{T d \log(T/d)}}  + C_1 d H^2  \sqrt{\log(2d H T^2/\delta)} B_\phi \sqrt T \log\rbr{He\delta^{-1}}\nend
    &\lesssim d H^2 \sqrt{T d}.
\end{align*}
For the first-order term in the follower's quantal response, we can bound it by 
\begin{align*}
    \text{1st-QRE}&\lesssim 4 H^2\eta \sqrt{C_0 d T} \sqrt{8C_\eta^2 \beta + 4 B_\Theta^2} + 2 H^2 B_\phi\eta \sqrt{2C_\eta^2 \beta + 4 B_\Theta^2} \sqrt{T} \nend
    &\le \eta C_\eta H^{2} d \sqrt T, 
\end{align*}
where we use $\beta\lesssim d$. In Scheme 5, we just multiply an $\exp(4\eta B_A)$ factor. 
For the second-order term in Scheme 4, 
\begin{align*}
    \text{2nd-QRE}\lesssim H^2 C^{(3)} (C_\eta^2\beta + 4 B_\Theta^2) C_0 d^2 \log(T) \lesssim \exp(4\eta B_A) (1+\eta B_A)^3 H^{2} d^2 \log(T)
\end{align*}
In Scheme 5, we just multiply an $\exp(4\eta B_A)$ factor. 
Combining everything together, 
for Scheme 4, 
\begin{align*}
    \Reg(T)\lesssim d H^2 \sqrt{d T} + \eta C_\eta H^2 d \sqrt{T} + \exp(4\eta B_A) (1+\eta B_A)^3 H^2 d^2 \log T, 
\end{align*}
and for Scheme 5, 
\begin{align*}
    \Reg(T)\lesssim d H^2 \sqrt{d T} + \exp(4\eta B_A)\eta C_\eta H^2 d \sqrt{T} + \exp(8\eta B_A) (1+\eta B_A)^3 H^2 d^2 \log T. 
\end{align*}
We thus complete the proof of \Cref{thm:Online-ML}.

% We next study the first order term in $\Gamma^{(2)}$, which is given by
% \begin{align*}
%     &\sum_{t=1}^T \Gamma_h^{(2,t)}(s_h^t;\pi_h^t(s_h^t),\theta_h^*)\nend
%     &\quad =\sum_{t=1}^T 2 H\eta \sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h^*} + I_d}^\dagger \Sigma_{s_h^t}^{\pi^t , \theta_h^*}}} \cdot \sqrt{2 C_{\eta}^2 \beta + 4B_\Theta^2}  \nend
%     &\qqquad + \sum_{t=1}^T 2 H C^{(3)} \rbr{\sqrt{\trace\rbr{\rbr{\Sigma_{h, t}^{\theta_h^*} + I_d}^\dagger \Sigma_{s_h^t}^{\pi^t , \theta_h^*}}} \cdot \sqrt{2 C_{\eta}^2 \beta + 4B_\Theta^2}}^2 \nend
%     &\quad \le 
% \end{align*}