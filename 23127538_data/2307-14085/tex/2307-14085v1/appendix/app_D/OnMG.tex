We give a proof on \Cref{thm:Online-MG} in this subsection.

\paragraph{Step 1. Validity and Accuracy for the Confidence Set.}
Recall the confidence set we constructed in \eqref{eq:myopic-online-general-confset}
\begin{align}
    &\CI_{\cU, \Theta}^t(\beta) \nend
    &\quad= \cbr{
    (U,\theta)\in\cU\times\Theta:
    \rbr{ \ds
        \cL_h^t(\theta_h)-\inf_{\theta_h'\in\Theta_h}\cL_h^t(\theta_h') \le \beta 
    \atop \ds
        \ell_h^t(U_h, U_{h+1}, \theta_{h+1}) - \inf_{U'\in\cU_h} \ell_h^t(U', U_{h+1}, \theta_{h+1})\le H^2\beta}, 
    \forall h\in[H]}. \label{eq:OnMG-confset}
\end{align}
For the follower's side, note that the data compliance condition is automatically satisfied by the online interaction process. 
Since $\beta\ge C\log(H T\cN_\rho(\Theta, T^{-1})\delta^{-1})$, we have by \eqref{eq:MLE-guarantee-Q-3} in \Cref{lem:MLE-formal} that with probability at least $1-\delta$, for all $\theta\in \CI_{\cU,\Theta}^t(\beta), h\in[H], t\in[T]$, 
\begin{align}
    &\sum_{i=1}^{t-1} \rbr{ \rbr{r_h^{\hat\pi^i, \theta} - r_h^{\hat\pi^i, \theta^*}}(s_h^i, b_h^i) - \EE_{s_h^i}^{\hat\pi^i, \theta'}\sbr{\rbr{r_h^{\hat\pi^i, \theta} - r_h^{\hat\pi^i, \theta^*}}(s_h, b_h)}}^2 
    % \nend
    % %%%%%%%%%
    % &\quad\le  4 C_\eta^2 \beta + 32 B_r^2 \log\rbr{H \cN_\varrho(\Theta, T^{-1})\delta^{-1} }\nend
    % &\quad 
    \lesssim C_\eta^2 \beta
    \label{eq:OnMG-MLE-guarantee}
\end{align}
where we recall the definition of $\cN_\rho(\Theta, \epsilon)$ in \eqref{eq:cN-Theta-myopic}. 
Recall that $\hat\pi^i$ is the policy used in episode $i$. On the leader's side, noticing that $H^2\beta \gtrsim H^2\log(HT\cN_\rho(\cZ, T^{-1})\delta^{-1})$, we have by \Cref{lem:CI-U-online} that with probability at least $1-\delta$ for all $t\in[T]$: 
\begin{itemize}
    \item[(i)] (Validity) $U^{*, \theta}\in \CI_{\cU, \Theta}^t(\beta)$ for any $\theta\in\CI_{\cU,\Theta}^t(\beta)$; 
    \item[(ii)] (Accuracy) for any $(\theta, U)\in\CI_{\cU, \Theta}^t(\beta)$, $\sum_{i=1}^{t-1}\EE^{\hat\pi^i}[(U_{h} - \TT_{h}^{*, \theta} U_{h + 1})^2]\lesssim H^2\beta$ and $\sum_{i=1}^{t-1}|(U_{h} - \TT_{h}^{*,\theta} U_{h + 1})(s_h^i, a_h^i, b_h^i)|^2\lesssim H^2\beta$ for all $h\in[H]$,
\end{itemize}
where we remind the readers that $U^{*, \theta}$ is defined as
\begin{align*}
    U_h^{*, \theta}(s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \bigrbr{\bigrbr{P_h\circ  T_{h+1}^{*, \theta}} U_{h+1}^{*, \theta}} (s_h, a_h, b_h), 
\end{align*}
where $ T_h^{*,\theta}:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS)$ is the one-step optimistic integral operator defined as
\begin{align*}
     T_{h}^{*, \theta} f(s_h) = \max_{\alpha_h\in\cA} \dotp{f(s_h,\cdot,\cdot)}{\alpha_h\otimes \nu^{\alpha_h, \theta}(\cdot, \cdot\given s_h)}.
\end{align*}
With respect to $ T^{*, \theta}$, we define the optimistic Bellman operator for the leader 
$\TT_h^{*,\theta}:\sF(\cS\times\cA\times\cB)\rightarrow \sF(\cS\times\cA\times\cB)$ as 
\begin{align*}
    \bigrbr{\TT_h^{*, \theta} f} (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE_{s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)} \bigsbr{\bigrbr{ T_{h+1}^{*, \theta}f}(s_{h+1})}.
\end{align*}
Combining these results, we have for $\CI_{\cU,\Theta}^t(\beta)$ that with probability at least $1-2\delta $ and for all $t\in[T]$ that,
\begin{itemize}
    \item[(i)] (Validity) $(\theta^*, U^{*})\in \CI_{\cU, \Theta}^t(\beta)$; 
    \item[(ii)] (Accuracy) for any $(\theta, U)\in\CI_{\cU, \Theta}^t(\beta)$, we have \eqref{eq:OnMG-MLE-guarantee} and  $\sum_{i=1}^{t-1}\EE^{\hat\pi^i}[\|U_{h} - \TT_{h}^{*,\theta} U_{h + 1}\|^2]\lesssim H^2\beta$, $\sum_{i=1}^{t-1}\|(U_{h} - \TT_{h}^{*, \theta} U_{h + 1})(s_h^i, a_h^i, b_h^i)\|^2\lesssim H^2\beta$ hold for all $h\in[H]$.
\end{itemize}
The following proof is based on the success of $\CI_{\cU, \Theta}^t(\beta)$.

\paragraph{Step 2. Suboptimality Decomposition via Optimism.}
Recall the optimisitic policy optimization in \eqref{eq:online-MG-opt-parameter} that 
\begin{align}
    (\hat U^t, \hat\theta^t)=\argmax_{(U, \theta)\in\CI_{\cU,\Theta}^t(\beta)} \underbrace{\max_{\pi_1\in\Pi}\EE_{s_1\sim\rho_0} \sbr{{\inp[\big]{U_1(s_1, \cdot, \cdot)}{\pi_1\otimes\nu_1^{\pi, \theta}(\cdot,\cdot\given s_1)}}}_{\cA\times \cB } }_{\ds J(U, \theta)}, \label{eq:OnMG-U theta hat}
\end{align}
and also the optimistic policy in \eqref{eq:online-MG-hat pi} that 
\begin{align}
    \hat\pi^t(s_h)=\argmax_{\pi(s_h)\in\sA}\inp[]{U_{h}(s_{h}, \cdot, \cdot)}{\pi_{h}\otimes \nu_{h}^{\pi, \theta}(\cdot, \cdot\given s_{h})}. \label{eq:OnMG-pi hat}
\end{align}
We have for the suboptimality that
\begin{align*}
    \subopt(\hat\pi^t) = J(\pi^*) - J(\hat U^t, \hat\theta^t) + J(\hat U^t, \hat\theta^t) - J(\hat\pi^t) \le J(\hat U^t, \hat\theta^t) - J(\hat\pi^t)
\end{align*}
For the first inequality, we notice by the validity of $\CI_{\cU,\Theta}^t(\beta)$ that $(U^{\theta^*}, \theta^*)\in \CI_{\cU,\Theta}^t(\beta)$ and and by optimism $J(\hat U^t, \hat \theta^t) \ge J(U^{\theta^*}, \theta^*) = J(\pi^*)$. 
we then invoke \eqref{eq:subopt-decompose-equality-1} of \Cref{lem:subopt-decomposition} with respect to $\hat\pi^t, \hat U^t, \hat\nu^t$ where $\hat\nu^t$ is the quantal response with respect to $\hat\pi^t$ and $\hat\theta^t$,
\begin{align*}
    \subopt(\hat\pi^t) &\le J(\hat U^t, \hat\theta^t) - J(\hat\pi^t) \nend
    %%%%%%%%%
    &\le {\sum_{h=1}^H \EE^t\Bigsbr{ {\bigrbr{\hat U_h^t - u_h}(s_h, a_h, b_h)-  T_{h+1}^{\hat\pi^t,\hat\nu^t} \hat U_{h+1}^t(s_{h+1})}}} 
    + \sum_{h=1}^H 2 H \EE^t D_\TV\rbr{\hat\nu_h^t(\cdot\given s_h), \nu_h^t(\cdot\given s_h)}\nend
    %%%%%%%%%
    &\le  {\sum_{h=1}^H \EE^t\Bigsbr{\underbrace{\bigrbr{\hat U_h^t - u_h}(s_h, a_h, b_h)-  T_{h+1}^{*, \hat\theta^t} \hat U_{h+1}^t(s_{h+1})}_{\ds \hat U_h^t - \TT_h^{*, \hat\theta^t}\hat U_{h+1}^t }}} \nend
    &\qquad + \sum_{h=1}^H 2 H \eta \EE^t\Bigsbr{\bigabr{ \underbrace{(\hat r_h^t(s_h, b_h) - r_h^t(s_h, b_h)) - \EE^t\bigsbr{\hat r_h^t(s_h, b_h) - r_h^t(s_h, b_h)}}_{\ds \Upsilon_h^{\hat\pi^t} (r_h^{\hat\theta^t} - r_h) }} } \nend
    &\qquad + \sum_{h=1}^H 2 H C^{(3)}\EE^t\sbr{\rbr{\rbr{\hat r_h^t(s_h, b_h) -r_h^t(s_h, b_h)} - \EE^t \sbr{\hat r_h^t(s_h, b_h) -r_h^t(s_h, b_h)}}^2},
\end{align*}
where $\nu^t$ is the actual quantal response under $\hat\pi^t$ and the true model, $r^t(s_h, b_h) = \inp[]{\hat\pi^t(\cdot\given s_h, b_h)}{r_h(s_h, \cdot, \cdot)}$ is the integral reward for the follower under $\hat\pi^t$ and the true model, and $\hat r^t$ is the alternative of $r^t$ under the same policy $\hat\pi^t$ but the estimated model $\hat\theta^t$. We also denote by $\EE^t$ the expectation taken with respect to $\hat\pi^t$ and the true model.
Here, for the last inequality, we replace $T_{h+1}^{\hat\pi^t, \hat\nu^t}$ by the optimistic integral operator $T_{h+1}^{*, \hat\theta^t}$ by definition of $\hat\pi^t$ in the first term, and the remaining hold by using \Cref{cor:response-diff-myopic} for the TV distance.


\paragraph{Controlling Regret by eluder Dimension.}
For the leader's side, we have the following configurations:
\begin{itemize}[leftmargin=20pt]
    \item Define function class $\cG_{h,L} = \bigcbr{g:\cS\times\cA\times\cB\rightarrow \RR\given  g = U_h-\TT_h^{*, \theta} U_{h+1}, \exists U\in\cU, \theta\in\Theta}$. 
    Moreover, consider sequence $\{g_h^i = \hat U_h^i - \TT_h^{*, \hat\theta^i} \hat U_{h+1}^i\}_{i\in[T]}$, it is obvious that $g_h^i\in\cG_{h,L}$;
    \item Define the class of probability measures as 
    $$\sP_{h,L} = \cbr{\rho\in\Delta(\cS\times\cA\times\cB)\given\rho(\cdot)=\PP^\pi((s_h, a_h, b_h)=\cdot), \pi\in\Pi}.$$
    Moreover, consider sequence $\{\rho^i(\cdot) = \PP^{\hat\pi^i}((s_h, a_h, b_h)=\cdot)\}_{i\in[T]}$;
    \item For the chosen sequences, we have
    \begin{align*}
        \EE_{\rho_h^i}[g_h^t] = \EE^{i} \sbr{(\hat U_h^t - \TT_h^{*, \hat\theta^t} \hat U_{h+1}^t)(s_h,a_h,b_h)}.
    \end{align*}
    If $i=t$, this corresponds to the leader's Bellman error we aim to bound.
    Moreover, the online guarantee is 
    $$\sum_{i=1}^{t-1} \rbr{\EE_{\rho^i}\sbr{g_h^t}}^2 = \sum_{i=1}^{t-1}\EE^{i}[(\hat U_{h}^{t} - \TT_{h}^{*, \hat\theta^t} \hat U_{h + 1}^t)^2]\lesssim H^2\beta$$ for any $t\in[T]$ by the guarantee of $\CI_{\cU, \Theta}^t(\beta)$ since $(\hat U^t, \hat\theta^t)\in\CI_{\cU,\Theta}^t(\beta)$. Moreover, $|\EE_{\rho^i}[g_h^t]|\le 2H$ globally.
\end{itemize}
Under these conditions, we let $\dim(\cG_L) = \max_{h\in[H]}\dim_\DE\rbr{\cG_{h, L}, \sP_{h, L}, T^{-1/2}}$ and we have by the first order cumulative error in \Cref{lem:de-regret} that
\begin{align*}
    &\sum_{i=1}^T \abr{\EE^t\sbr{\bigrbr{\hat U_h^t - u_h}(s_h, a_h, b_h)-  T_{h+1}^{\hat\pi^t,\hat\nu^t} \hat U_{h+1}^t(s_{h+1})}} \nend
    & \quad \lesssim \sqrt{\dim(\cG_L) H^2\beta T} + \min\cbr{T, \dim(\cG_L)} H + \sqrt T.
\end{align*}
For the follower's side, recall the linear operator $\Upsilon_h^\pi:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cB)$ defined as 
\begin{align*}
    \rbr{\Upsilon_h^\pi f}(s_h, b_h) = \dotp{\pi_h(\cdot\given s_h, b_h)}{f(s_h, \cdot, b_h)} - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{f(s_h,\cdot,\cdot)}. 
\end{align*}
Here, $(\Upsilon_h ^{\pi} f ) (s_h,b_h) $ quantifies  how far $b_h$ is from being the quantal response of $\pi$ at state $s_h$, measured in terms of  $f$. One can also think of $(\Upsilon_h^\pi f)(s_h, b_h)$ as the \say{advantage} of the reward induced by action $b_h$ compared to the reward induced by the quantal response. 
% It suffices to use $\Upsilon_h^\pi $ instead for a myopic follower where $\alpha\in\sA$ is a state-wise prescription.
We have the following configurations for the follower's quantal response error:
\begin{itemize}[leftmargin=20pt]
    \item We define function class on $\cS\times\cA\times\cB$ as,
    \begin{align*}
    \cG_{h, F} = \cbr{g:\cS\times\cA\times\cB\rightarrow \RR: \exists \theta\in\Theta, g= {r_h^\theta - r_h}}.
    \end{align*}
    In addition, we consider a sequence $\{g_h^i = r_h^{\hat\theta^i} - r_h\}_{i\in[T]}$. 
    \item We define a class of signed measures on $\cS\times\cA\times\cB$ as
    \begin{align*}
        \sP_{h, F} &= \{\rho(\cdot) = \PP^\pi(a_h=\cdot\given s_h, b_h)\delta_{(s_h, b_h)}(\cdot) 
        - \PP^\pi((a_h, b_h)=\cdot\given s_h)\delta_{(s_h)}(\cdot)\nend
        &\qqquad \biggiven \pi\in\Pi, (s_h, b_h)\in\cS\times\cB\}, 
    \end{align*}
    where $\delta_{(s_h, b_h)}(\cdot)$ is the measure that assigns measure $1$ to state-action pair $(s_h, b_h)$. 
    In addition, we consider a sequence 
    $$\cbr{\rho_h^i(\cdot) = \PP^{\hat\pi^i}(a_h=\cdot\given s_h, b_h)\delta_{(s_h, b_h)}(\cdot) 
    - \PP^{\hat\pi^i}((a_h, b_h)=\cdot\given s_h)\delta_{(s_h)}(\cdot)}_{i\in[T]}$$
    \item 
    For simplicity, we denote by $g_h^t(s_h^i, b_h^i, \pi^i)$ the integral of $g_h^t$ with respect to the signed measure $\rho_h^i$ since $\rho_h^i$ is uniquely determined by $(s_h^i, b_h^i, \pi^i)$. 
    It is easy to check that $$g_h^t(s_h^i, b_h^i, \pi^i) = \bigrbr{\Upsilon_h^{\pi^i}(r_h^{\hat\theta^t}-r_h)}(s_h^i, b_h^i) = \QRE(s_h^i, b_h^i; \hat\theta^t, \pi^i).$$
    We have $g_h^t(s_h^i, b_h^i, \pi^i)f$ globally bounded by $4$ and the
    online guarantee for these two sequences are 
    \begin{align*}
        \sum_{i=1}^{t-1} (g_h^t(s_h^i, b_h^i, \pi^i))^2 \lesssim C_\eta^2\beta, 
    \end{align*}
    which holds because $\hat\theta^t\in\CI_{\cU, \Theta}^t(\beta)$ and by \eqref{eq:OnMG-MLE-guarantee}.
\end{itemize}
We let $\dim(\cG_F) = \max_{h\in[H]}\dim_\DE(\cG_{h,F},\sP_{h, F}, T^{-1/2})$ and by using \eqref{eq:DE error-1st order}, we have
\begin{align*}
    &\sum_{t=1}^T \EE^t\sbr{\abr{(\hat r_h^t(s_h, b_h) - r_h^t(s_h, b_h)) - \EE^t\bigsbr{\hat r_h^t(s_h, b_h) - r_h^t(s_h, b_h)}}}\nend
    &\quad \lesssim  2\sum_{t=1}^T \abr{\Upsilon_h^{\hat\pi^t} \rbr{r_h^{\hat\theta^t} - r_h}(s_h^t, b_h^t) } + \log\rbr{\delta^{-1} H\cN_\rho(\Theta, T^{-1})} + 1\nend
    &\quad \lesssim  \sqrt{\dim(\cG_F) (C_\eta^2 + 32)\beta T} + \min\cbr{T, \dim(\cG_F)} +  \beta, 
\end{align*}
for all $h\in[H]$. Here, the first inequality uses a standard martingale concentration in \Cref{cor:martigale concentration} and note that the approximation error of a $T^{-1}$- covering net $\Theta_{T^{-1}}$ is $\cO(1)$. Moreover, we have for the second order term that 
\begin{align*}
    &\sum_{t=1}^T \EE^t\sbr{\rbr{\rbr{\hat r_h^t(s_h, b_h) -r_h^t(s_h, b_h)} - \EE^t \sbr{\hat r_h^t(s_h, b_h) -r_h^t(s_h, b_h)}}^2}\nend
    &\quad \lesssim 2\sum_{t=1}^T \rbr{\Upsilon_h^{\hat\pi^t} \rbr{r_h^{\hat\theta^t} - r_h}(s_h^t, b_h^t)}^2 + \log\rbr{\delta^{-1} H\cN_\rho(\Theta, T^{-1})} +  1\nend
    & \quad \lesssim \dim(\cG_F) C_\eta^2 \beta \log(T) + \min\cbr{T, \dim(\cG_F)} + \beta.
\end{align*}
In summary, for the Leader's Bellman error
\begin{align*}
    \text{LBE} \lesssim H^2 \sqrt{\dim(\cG_L) \beta T},
\end{align*}
for the follower's first-order QRE, 
\begin{align*}
    \text{1st-QRE}  \lesssim H^2 \eta \sqrt{\dim(\cG_F)C_\eta^2\beta T} \lesssim H^2 \eta C_\eta \sqrt{\dim(\cG_F) \beta T},
\end{align*}
and for the follower's second-order QRE,
\begin{align*}
    \text{2nd-QRE}
    \lesssim H^2 C^{(3)} \dim(\cG_F) C_\eta^2 \beta \log(T) \lesssim H^2 (\eta C_\eta)^3  \exp(4\eta B_A) \beta \log T
\end{align*}
Summing them up gives us the result to \Cref{thm:Online-MG}.

% \Cref{thm:MLE-GOLF-neurips}