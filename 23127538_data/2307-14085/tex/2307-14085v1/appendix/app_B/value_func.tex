In the following proof, we always consider the expectation to be taken with respect to the data generating distribution.
We first prove the following concentration result: for any $h\in[H]$ and any $y=(\theta_{h+1}, \pi_{h+1}, U_{h+1}, U_{h})\in \cY_h = \Theta_{h+1}\times \Pi_{h+1}\times \cU^2$, 
it holds with probability at least $1-\delta$ that 
      \begin{align}
        &\abr{T\EE[(U_{h} - \TT_{h}^{\pi,\theta}U_{h + 1})^2] - \ell_{h}(U_{h}, U_{h + 1}, \theta, \pi) + \ell_{h}(\TT_{h}^{\pi,\theta}U_{h + 1}, U_{h + 1}, \theta, \pi)} \notag\\
        &\qquad \le \epsilon_S + \frac{T}{2} \EE[(U_{h} - \TT_{h}^{\pi,\theta}U_{h + 1})^2].\label{eq:cY-confset-1}
      \end{align}
     where
    $
    {110 B_U^2\cdot\log(H \max_{h\in[H]}\cN_\rho(\cY_h, T^{-1})\delta^{-1}) } \cdot {T^{-1}} + (45 B_U^2 + 60 B_U )T^{-1}
    $ and $B_U=H$ is the upper bound for the function class $U$.

\paragraph{Concentration. }
Our proof is an adaptation of Lemma D.2 in \citep{lyu2022pessimism}, although we simplify a little bit by directly using the covering number for a joint class $\cY_h = \Pi_{h+1}\times\Theta_{h+1}\times\cU^2$. We take an $\epsilon$-covering net $\cY_\epsilon$ for $\cY$ with respect to distance $\rho$ specified by \eqref{eq:rho-cY}. 
We first use \Cref{lem:bernstein},  
where we take 
\begin{align*}
    {Z_h^i} &= \ell_{h}(U'_{h}, U_{h + 1}, \theta, \pi) - \ell_{h}(\TT_{h}^{\pi,\theta}U_{h + 1}, U_{h + 1}, \theta, \pi)\nend
    & = \rbr{U_h(s_h^i, a_h^i, b_h^i) - u_h^i -  T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i)}^2  - \rbr{\TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i) - u_h^i - T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i)}^2.
\end{align*}
Here, we recall the definition of $\TT_h^{\pi}$ given by \eqref{eq:bellman_operator_leader}.
One can verify that $|{Z_h^i}|\le 9B_U^2$ where $B_U$ bounds both the leader's reward and the value function class $\cU$.
We first calculate the expectation of $Z_h^i$, 
\begin{align*}
    \EE[{Z_h^i}] 
    &= \EE \bigg[\EE_{s_h^i, a_h^i, b_h^i} \Big[\rbr{U_h(s_h^i, a_h^i, b_h^i) - u_h^i -  T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i)}^2  \nend
    &\qquad - \rbr{\TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i) - u_h^i - T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i)}^2\Big]\bigg]\nend
    & = \EE\bigg[\Bigrbr{U_h(s_h^i, a_h^i, b_h^i)- \TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i)}\nend
    &\qquad \cdot \EE_{s_h^i, a_h^i, b_h^i}\sbr{U_h(s_h^i, a_h^i, b_h^i) + \TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i)- 2u_h^i - 2T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i) }\bigg]\nend
    & = \EE\sbr{\Bigrbr{U_h(s_h^i, a_h^i, b_h^i)- \TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i)}^2}, 
\end{align*}
where $\EE_{x}[\cdot]$ is a short hand of $\EE[\cdot\given x]$ and the expectation is taken with respect to the data generating distribution. Here, the second equality holds by the law of total expectation, and the third equality holds by noting that $\EE_{s_h^i, a_h^i, b_h^i}\osbr{u_h^i  + T_h^{\pi,\theta} U_{h+1}(s_{h+1}^i)} = \TT_h^{\pi, \theta} U_{h+1} (s_h^i, a_h^i, b_h^i)$.
Next, we calculate the variance, 
\begin{align*}
    \Var[{Z_h^i}] &\le \EE[{Z_h^i}^2] \nend
    &\le \EE\bigg[\Bigrbr{U_h(s_h^i, a_h^i, b_h^i)- \TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i)}^2\nend
    &\qquad \cdot \EE_{s_h^i, a_h^i, b_h^i}\sbr{\rbr{U_h(s_h^i, a_h^i, b_h^i) + \TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i)- 2u_h^i - 2T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i)}^2 } \bigg]\nend
    &\le 49 B_U^2 \EE\sbr{\Bigrbr{U_h(s_h^i, a_h^i, b_h^i)- \TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i)}^2} = 49 B_U^2 \EE[{Z_h^i}].
\end{align*}
Now, by \Cref{lem:bernstein}, we have for each $y\in\cY_\epsilon$ that
\begin{align*}
    \abr{\frac 1 T \sum_{i=1}^T {Z_h^i} - \EE\sbr{{Z_h^i}}} &\le \frac{1}{2T}\sum_{i=1}^T \EE [{Z_h^i}] + \frac{110 B_U^2\cdot\log(2\delta^{-1})}{T}.
\end{align*}
Now, we extend the result to $\cY$, where we notice that for any two $y, \tilde y\in\cY$ such that $\rho(y, \tilde y) \le \epsilon$, 
\begin{align*}
    &\bigrbr{U_h(s_h^i, a_h^i, b_h^i) - u_h^i -  T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i)}^2  - \bigrbr{\TT_h^{\pi, \theta} U_{h+1}(s_h^i, a_h^i, b_h^i) - u_h^i - T_{h+1}^{\pi,\theta} U_{h+1}(s_{h+1}^i)}^2 \nend
    &\qquad - \rbr{\bigrbr{\tilde U_h(s_h^i, a_h^i, b_h^i) - u_h^i -  T_{h+1}^{\tilde\pi,\tilde\theta} \tilde U_{h+1}(s_{h+1}^i)}^2  - \bigrbr{\TT_h^{\tilde\pi, \tilde\theta} \tilde U_{h+1}(s_h^i, a_h^i, b_h^i) - u_h^i - T_{h+1}^{\tilde\pi,\tilde\theta} \tilde U_{h+1}(s_{h+1}^i)}^2}\nend
    %%%%%%%%%%%%%
    &\quad \le 6 B_U \rbr{\onbr{U-\tilde U}_\infty + \bignbr{(T_{h+1}^{\pi,\theta}-T_{h+1}^{\tilde\pi,\tilde\theta} )\tilde U_{h+1}}_\infty + \bignbr{T_{h+1}^{\pi,\theta}(U_{h+1} - \tilde U_{h+1})}_\infty}\nend
    &\qquad + 6 B_U \cdot 2\rbr{\bignbr{(T_{h+1}^{\pi,\theta}-T_{h+1}^{\tilde\pi,\tilde\theta} )\tilde U_{h+1}}_\infty + \bignbr{T_{h+1}^{\pi,\theta}(U_{h+1} - \tilde U_{h+1})}}\nend
    &\quad\le 6 B_U(2\epsilon + B_U \epsilon ) + 12 B_U(B_U\epsilon + \epsilon)\nend
    &\quad\le (18 B_U^2 + 24 B_U)\epsilon, 
\end{align*}
where the second inequality follows from the definition of the covering net with respect to distance $\rho$ defined in \eqref{eq:rho-cY}. 
We obtain with probability at least $1-\delta$ that for any $y\in\cY_h$, $h\in[H]$, 
\begin{align*}
    \abr{\frac 1 T \sum_{i=1}^T {Z_h^i} - \EE\sbr{{Z_h^i}}} &\le \inf_{\epsilon>0} \frac{1}{2T}\sum_{i=1}^T \EE [{Z_h^i}] + \frac{110 B_U^2\cdot\log(H \cN_\rho(\cY_h, \epsilon)\delta^{-1})}{T} +2.5\cdot (18B_U^2 + 24 B_U)\epsilon\nend
    &\le \frac{1}{2T}\sum_{i=1}^T \EE [{Z_h^i}] + {110 B_U^2\cdot\log( H \cN_\rho(\cY_h, T^{-1})\delta^{-1}) } \cdot {T^{-1}} + (45 B_U^2 + 60 B_U )T^{-1}\nend
    & = \frac 1 T \rbr{\epsilon_S + \frac{T}{2} \EE[(U_{h} - \TT_{h}^{\pi,\theta}U_{h + 1})^2]}, 
\end{align*}
where $\epsilon_S = {110 B_U^2\cdot\log(H \max_{h\in[H]}\cN_\rho(\cY_h, T^{-1})\delta^{-1}) }  + (45 B_U^2 + 60 B_U )$, 
which proves our claim in \eqref{eq:cY-confset-1}.

\paragraph{Guarantee of the Confidence Set $\CI_{\cU}^{\pi,\theta}(\beta)$.}
We give a brief proof for the validity and the accuracy of the confidence set. For any $U_{h+1}\in\cU, \theta\in\Theta, \pi\in\Pi, h\in[H]$, on the one hand,
\begin{align}
    \ell_h(U_h, U_{h+1},\theta, \pi) - \inf_{U_h'\in\cU} \ell_h(U_h', U_{h+1}, \theta,\pi) \le \epsilon_S - \frac{T}{2} \EE[\orbr{U_{h} - \TT_{h}^{\pi,\theta}U_{h + 1}}^2], \label{eq:Bellman-loss-guarantee-1}
\end{align}
on the other hand,
\begin{align}
    \ell_h(U_h, U_{h+1},\theta, \pi) - \inf_{U_h'\in\cU} \ell_h(U_h', U_{h+1}, \theta,\pi) 
    &\ge \ell_h(U_h, U_{h+1},\theta, \pi) - \ell_h(\TT_h^{\pi, \theta} U_{h+1},  U_{h+1}, \theta,\pi) \nend
    &\ge - \epsilon_S  + \frac{T}{2} \EE[\orbr{U_{h} - \TT_{h}^{\pi,\theta}U_{h + 1}}^2], \label{eq:Bellman-loss-guarantee-2}
\end{align}
where the first inequality holds by the completeness assumption. 
For \eqref{eq:Bellman-loss-guarantee-1}, we plug in $U=U^{\pi,\theta}$ (realizability) and obtain 
$$\ell_h(U^{\pi,\theta}_h, U^{\pi,\theta}_{h+1},\theta, \pi) - \inf_{U_h'\in\cU} \ell_h(U_h', U^{\pi,\theta}_{h+1}, \theta,\pi) \le \epsilon_S.$$
Therefore, by having $\beta \ge \epsilon_S$, we have $U^{\pi,\theta}\in\CI_\cU^{\pi,\theta}(\beta)$. For the second one in \eqref{eq:Bellman-loss-guarantee-2}, we plug in any $U\in\CI_{\cU}^{\pi,\theta}(\beta)$ and obtain $\EE[\|U_{h} - \TT_{h}^{\pi,\theta}U_{h + 1}\|^2]\le T^{-1}\cdot (2\beta +2\epsilon_S)\le 4\beta T^{-1}$. Hence, we complete our proof of \Cref{lem:leader-bellman-loss}.