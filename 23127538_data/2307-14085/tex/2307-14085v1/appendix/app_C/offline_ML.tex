In this section, we give finite sample guarantee for \Cref{alg:PMLE}. 
During this proof, we let $\hat M = \{\hat U_h, \hat W_h\}_{h\in[H]}$ be the estimated value functions given by \Cref{alg:PMLE} and $M^{\pi} = \{U_h^{\pi}, W_h^{\pi}\}_{h\in[H]}$ be the value functions under policy $\pi$ and the true model. We also let $\hat\pi$ be the estimated policy.
We first summarize the three schemes used in \Cref{alg:PMLE}.
\begin{align}
    &\textbf{S1:}\quad  \hat\pi_h(s_h)  = \argmax_{\pi_h \in\sA} \min_{\theta_h\in\confset_{h, \Theta}(\beta)} \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi_h \otimes\nu_h^{\pi_h , \theta_h}(\cdot,\cdot\given s_h)}_{\cA\times\cB},\label{eq:scheme-1-new}\\
    %%%%%%%%%%
    &\textbf{S2:}\quad  \hat\pi_h(s_h) = \argmax_{\pi_h \in\sA, \atop \theta_h\in\confset_{h, \Theta}(\beta)} \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi_h \otimes\nu^{\pi_h , \theta_h}(\cdot,\cdot\given s_h)}_{\cA\times\cB} - \Gamma^{(2)}_h(s_h;\pi_h , \theta_h),\label{eq:scheme-2-new}\\
    %%%%%%%%%%
    & \textbf{S3:}\quad  \hat\pi_h(s_h) = \argmax_{\pi_h \in\sA}  \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi_h \otimes\nu^{\pi_h , \hat\theta_{h,\MLE}}(\cdot,\cdot\given s_h)}_{\cA\times\cB} - \Gamma^{(2)}_h(s_h;\pi_h , \hat\theta_{h, \MLE}) .\label{eq:scheme-3-new}
\end{align}
Moreover, $\hat W_h(s_h)$ is the optimal value in these three schemes.
\paragraph{Step 1. Uncertainty Quantifications. }
We first invoke the following lemma that characterizes the uncertainty on the leader's side. 
\begin{lemma}[\textit{$\Gamma^{(1)}_h$ as a  $\delta$-uncertainty quantifier, Lemma 5.2 in \citet{jin2021pessimism}}] \label{lem:Gamma_1-pessi}
     Under the data compliance condition $\PP_\cD(u_h^i=u, s_{h+1}^i=s\given \tau^{i-1}, \{s_{h'}^i, \alpha_{h'}^i, a_{h'}^i, b_{h'}^i\}_{h'\in[h]}) =\PP(u_h=u, s_{h+1}=s\given s_h=s_h^i, a_h=a_h^i, b_h=b_h^i)$, in \Cref{alg:PMLE}, by setting $\Gamma^{(1)}_h(\cdot,\cdot,\cdot) \ge C_1 d H \sqrt{\log(2d H T/\delta)}\cdot \sqrt{\phi_h(\cdot,\cdot,\cdot)^\top \Lambda_h^{-1}\phi_h(\cdot,\cdot,\cdot)}$ for some universal constant $C_1>0$, we have with probability at least $1-\delta$ that the following holds,
\begin{align}
    \abr{\rbr{P_h\hat W_{h+1}  + u_h - \phi_h^\top \hat\omega_h}(s_h, a_h, b_h)} \le \Gamma^{(1)}_h(\cdot,\cdot,\cdot), \quad\forall (s_h,a_h,b_h)\in\cS\times\cA\times\cB, h\in[H].\label{eq:OffML-Bellman-error-guarantee}
\end{align}
\begin{proof}
    See \citet[Lemma 5.2]{jin2021pessimism} for a detailed proof.
\end{proof}
\end{lemma}
Here, \Cref{lem:Gamma_1-pessi} is only slightly different from Lemma 5.2 in \citet{jin2021pessimism} in terms of the feature map $\phi$ since we additionally have a follower's action. However, nothing much is changed, and we omit the proof here.
For the follower, we invoke \Cref{eq:bandit-ub-1} in \Cref{cor:formal-MLE confset-linear myopic}, and obtain 
\begin{align}
     \max\cbr{\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, \cD}^{\theta_h^*}}^2, \bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, \cD}^{\hat\theta_h}}^2, 
    \EE^{\pi^i}\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, \cD}^{\theta_h^*}}^2, 
    \EE^{\pi^i}\bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, \cD}^{\hat\theta_h}}^2 } \le 8C_{\eta}^2 \beta T^{-1}, \label{eq:OffML-MLE-guarantee}
\end{align}
for any $\hat\theta_h\in \confset_{h,\Theta}(\beta)=\{\theta_h\in\Theta:\cL_{h,\cD}(\theta_h)\ge \min_{\theta_h'\in\Theta_h}\cL_{h,\cD}(\theta_h') + \beta\}$. 
Note that \eqref{eq:OffML-MLE-guarantee} holds because $\beta\ge C d\log(H(1+\eta T^2 + (1+\eta)T)\delta^{-1})$ for some universal constant $C>0$, which satisfies the condition in \Cref{cor:formal-MLE confset-linear myopic}. 

It holds straightforward that the confidence set is valid and accurate. In the sequel, for ease of presentation, we take $\CI_{\Theta}(\beta)=\cbr{\{\theta_h\}_{h\in[H]}: \theta_h\in\cC_{h,\Theta}(\beta), \forall h\in[H]}$, which is just a combination of these $H$ independent confidence sets, and all the results still hold for $\CI_\Theta(\beta)$. In the sequel, if we say $\hat\theta_h\in\CI_\Theta(\beta)$, we actually mean $\hat\theta_h\in\CI_{h,\Theta}(\beta)$. The following part is based on the success of \eqref{eq:OffML-Bellman-error-guarantee} and \eqref{eq:OffML-MLE-guarantee}.

\paragraph{Step 2. Suboptimality Decompostion via Pessimism.}
We define $\hat J=\EE_{s_1\sim\rho_0}[\hat W_1(s_1)]$ where $\hat W_1$ is the estimated state value function given by \Cref{alg:PMLE}. By a standard regret decomposition, we have for the estimated policy $\hat\pi$,
\begin{align}
    \subopt(\hat\pi) = J(\pi^*) - J(\hat\pi) = {J(\pi^*) - \EE[\hat W_1(s_1)]} + \underbrace{\EE[\hat W_1(s_1)] - J(\hat\pi)}_{\dr (i)}.\label{eq:suboopt-1}
\end{align}
We invoke \Cref{lem:subopt-decomposition}
% \begin{lemma}[\textit{Suboptimality decomposition for myopic follower}]\label{lem:subopt decomp-myopic}
%     For any policy $\pi$ and given $\ocbr{\hat U_h, \hat W_h}_{h\in[H]}$, we define $\hat J = \EE[\hat W_1(s_1)]$ and have
%     \begin{align*}
%         J(\pi) - \hat J = \sum_{h=1}^H \EE^{\pi}\sbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)} + \sum_{h=1}^H \EE^{\pi}\sbr{u_h(s_h, a_h, b_h) + \hat W_{h+1}(s_{h+1}) - \hat U_h(s_h, a_h, b_h)}.
%     \end{align*}
%     \begin{proof}
%         See \Cref{sec:proof-subopt decomp-myopic} for a detailed proof. 
%     \end{proof}
% \end{lemma}
for $\hat\pi, \hat U, \hat W, \hat\nu$, where $\hat\nu=\nu^{\hat\pi, \hat\theta}$, 
and expand term (i) of the suboptimality in \eqref{eq:suboopt-1} into
\begin{align}
    \dr{(i)} &= \EE^{\hat\pi}[\hat W_1(s_1)] - J(\hat\pi) \nend
    & = \underbrace{{\sum_{h=1}^H \EE^{\hat\pi}\sbr{\bigrbr{\hat U_h - u_h}(s_h, a_h, b_h)- \hat W_{h+1}(s_{h+1})}}}_{\dr \text{Leader's Bellman error}} + \underbrace{\sum_{h=1}^H \EE^{\hat\pi}\sbr{\hat W_h(s_h) -  T_{h}^{\hat\pi,\hat\nu} \hat U_{h}(s_{h})}}_{\ds\text{Value mismatch error}}\nend
    &\qqquad+\underbrace{\sum_{h=1}^H  H \EE^{\hat\pi} \nbr{\rbr{\hat \nu_h-\nu_h}(\cdot\given s_h)}_1}_{\dr \text{Quantal response error}}.\label{eq:subopt-2}
\end{align}
We notify the readers that $\EE^{\hat\pi}$ is the expectation taken with respect to $\hat\pi$ and the true model, 
$\nu$ is the quantal response under policy $\hat\pi$ and the true model, 
and $(\hat W, \hat U, \hat \nu, \hat\pi)$ are the estimated value funcions, estimated response, and estimated policy given by \Cref{alg:PMLE}.
Specifically, we have by the form of these three schemes that $\hat\nu = \nu^{\hat\pi, \hat\theta_{s_h}}$ where we denote by $\hat\theta_{s_h}\in\CI_\Theta(\beta)$ the minimizer/maximizer corresponding to the policy optimization problem at each state $s_h$ in \eqref{eq:scheme-1-new} and \eqref{eq:scheme-2-new}, 
and with a little abuse of notation, we also let $\hat\theta_{s_h}$ denote the MLE estimator $\hat\theta_{h, \MLE}=\argmin_{\theta_h'\in\Theta_h} \cL_{h}^t(\theta_h')$ if we are talking about Scheme 3 in \eqref{eq:scheme-3-new}.
Next, we aim to show ${\dr (i)}\le 0$ by guarantee of pessimism in the value iteration step. Firstly, we have by \Cref{lem:Gamma_1-pessi} that the leader's Bellman error is always nonpositive. We just need to look at the value mismatch error and the quantal response error. For the quantal response error, we have by \Cref{lem:response diff-myopic-linear} that 
\begin{align}
    &2 H D_\TV\rbr{\nu_h(\cdot\given s_h), \hat\nu_h(\cdot\given s_h)} \nend
    &\quad \le 2 H \min_{\Psi\in\SSS^d}\cbr{
        %%%%%%%
    f\rbr{\sqrt{\trace\rbr{{\Psi}^{\dagger} \Sigma_{ s_h}^{\hat\pi, \hat\theta_{s_h}}}}\cdot \bignbr{\theta^*- \hat\theta_{s_h}}_{{\Psi}}}, 
        %%%%%%%%%%
    f\rbr{\sqrt{\trace\rbr{{\Psi}^{\dagger} \Sigma_{ s_h}^{\hat\pi, \theta_h^*}}}\cdot \bignbr{\theta^*- \hat\theta_{s_h}}_{{\Psi}}}
    }\nend
    &\quad \le 
        \Gamma^{(2)}_h(s_h;\hat\pi(s_h), \hat\theta_{s_h}), \label{eq:OffML-TV-bonus}
\end{align}
where in the last inequality, we use the first term in the minimization. Specifically, we notice the definition $f(x) = \eta x + C^{(3)} x^2$ in \Cref{lem:response diff-myopic-linear} and the definition of $\Gamma^{(2)}_h(s_h;\alpha_h, \theta_h)$ in \eqref{eq:Gamma^2}, which are matching with each other, and use the fact that $\bignbr{\theta^*- \hat\theta_{s_h}}_{\Psi} \le \sqrt{8C_\eta^2 \beta+ 4 B_\Theta^2}$ if we plug in $\Psi = T\Sigma_{h, \cD}^{\hat\theta_{s_h}} + I_d$. 
The $8C_\eta^2 \beta$ term comes from the second guarantee in \eqref{eq:OffML-MLE-guarantee} where we use the property that $\hat\theta_{s_h}\in\CI_\Theta(\beta)$ and the $4B_\Theta^2$ term comes from the norm $\bignbr{\theta^*-\hat\theta_{s_h}}_{I_d} \le 2 B_\Theta$.
Therefore, for Scheme 2 and Scheme 3, we have by \eqref{eq:subopt-2} that
\begin{align*}
    {\dr (i)} &\le \sum_{h=1}^H \EE^{\hat\pi}\sbr{\hat W_h(s_h) -  T_{h}^{\hat\pi,\hat\nu} \hat U_{h}(s_{h})} + \sum_{h=1}^H  H \EE^{\hat\pi} \nbr{\rbr{\hat \nu_h-\nu_h}(\cdot\given s_h)}_1\nend
    &\le \sum_{h=1}^H \rbr{
        - \EE^{\hat\pi}\Gamma^{(2)}_h(s_h;\hat\pi(s_h), \hat\theta_{s_h}) + \EE^{\hat\pi}\Gamma^{(2)}_h(s_h;\hat\pi(s_h), \hat\theta_{s_h})
    } \le 0,
\end{align*}
where the first inequality holds by noting that the leader's Bellman error is nonpositive by \eqref{eq:OffML-Bellman-error-guarantee}, the first term in the second inequality comes directly from the definition of $\hat W_h(s_h)$, and the second term comes from \eqref{eq:OffML-TV-bonus}.
Moreover, for Scheme 1, we have by \eqref{eq:subopt-decompose-equality-2} in \Cref{lem:subopt-decomposition}  applied to $\hat\pi, \hat W, \hat U, \hat\nu$ that 
\begin{align*}
    {\dr (i)} &= {\sum_{h=1}^H \EE^{\hat\pi}\sbr{\bigrbr{\hat U_h - u_h}(s_h, a_h, b_h)- \hat W_{h+1}(s_{h+1})}} + \sum_{h=1}^H \EE^{\hat\pi}\sbr{\hat W_h(s_h) - T_h^{\hat \pi,\hat\nu}\hat U_h(s_h)}\nend
    &\qquad+  \sum_{h=1}^H \EE^{\hat\pi}\sbr{\rbr{ T_h^{\hat \pi,\hat\nu}-  T_h^{\hat \pi,\nu}} \hat U_h(s_h)}\le 0, 
\end{align*}
where we recall that $\nu$ is the quantal response under policy $\hat\pi$ and the true model. 
Similar to our previous discussion, by \eqref{eq:OffML-Bellman-error-guarantee} we have the first term nonpositive, and by definition of $\hat W$, we have the second term equal to $0$. 
For the third term, 
by our policy optimization in \eqref{eq:scheme-1-new}, we notice that $\nu=\nu^{\hat\pi, \theta^*}$ and $\hat\nu = \nu^{\hat\pi, \hat\theta_{s_h}}$, where $\hat\nu$ minimizes $T_h^{\hat\pi, \nu}\hat U_h$ when $\theta_h\in\CI_\Theta(\beta)$. Since the confidence set is valid, we conclude that the third term is also nonpositive. 

\paragraph{Step 3. Bounding Suboptimality.}
As a result of the previous discussions, we have for all these three schemes that 
\begin{align*}
    \subopt(\hat\pi) 
    &=   {J(\pi^*) - \EE[\hat W_1(s_1)]} + {\dr (i)}\nend
    %%%%%%%%%
    & \le  {\sum_{h=1}^H \EE\sbr{\hat W_{h+1}(s_{h+1}) - \bigrbr{\hat U_h - u_h}(s_h, a_h, b_h) }}  
    + \sum_{h=1}^H \EE\sbr{T_h^{\pi^*,\nu^*}\hat U_h(s_h) - \hat W_h(s_h)}\nend
    % &\qquad +  \sum_{h=1}^H  \EE^{\pi^*}\sbr{\rbr{ T_h^{\pi^*,\hat\nu^*}-  T_h^{\pi^*,\nu}} \hat U_h(s_h)}\nend
    %%%%%%%%%
    & \le  {\sum_{h=1}^H 2\EE \Gamma^{(1)}_h(s_h, a_h, b_h)}  
    + \sum_{h=1}^H \underbrace{\EE\sbr{T_h^{\pi^*,\nu^*}\hat U_h(s_h) - \hat W_h^*(s_h)}}_{\dr (ii)},
    % &\qquad + \sum_{h=1}^H  \EE 2 H D_\TV\rbr{\hat\nu^*(\cdot\given s_h), \nu(\cdot\given s_h)}.
\end{align*}
where the first inequality holds by ${\dr{(i)}}\le 0$ and using \eqref{eq:perform-diff-linear} in \Cref{lem:subopt-decomposition} for $\pi^*, \hat W, \hat U, \nu^*$. We remind the readers that $\EE$ is taken with respect to $\pi^*$ and define $\nu$ as the quantal response under $\pi^*$ and the true model.
The second inequality holds by \eqref{eq:OffML-Bellman-error-guarantee}.
% and replace the response difference by a TV distance.
Note that $\hat W$ is the optimizer corresponding to $\hat\pi$, which is still a mismatch for $\pi^*$ and we use it simply for the purpose of replacing the leader's Bellman error. 
However, we are also lucky to have $\hat W$ as the optimal value to all these schemes, which means that $\hat W_h(s_h)$ can be lower bounded by $\hat W_h^*(s_h)$, where $\hat W_h^*(s_h)$ is the optimal value of \eqref{eq:scheme-1-new}, \eqref{eq:scheme-2-new}, or \eqref{eq:scheme-3-new} when fixing $\alpha_h = \pi^*(s_h)$. 
% Therefore, we continue to have
% \begin{align}
%     \subopt(\hat\pi) 
%     & \le  {\sum_{h=1}^H 2\EE \Gamma^{(1)}_h(s_h, a_h, b_h)}  
%     + \sum_{h=1}^H \EE\sbr{T_h^{\pi^*,\hat\nu^*}\hat U_h(s_h) - \hat W_h^*(s_h)}   \nend
%     % &\qquad + \sum_{h=1}^H  \EE 2 H D_\TV\rbr{\hat\nu^*(\cdot\given s_h), \nu(\cdot\given s_h)} \nend
%     & \le  {\sum_{h=1}^H 2\EE \Gamma^{(1)}_h(s_h, a_h, b_h)}  
%     + \sum_{h=1}^H  \EE \Gamma^{(2)}_h \rbr{s_h;\pi^*(s_h), \theta_h^*}
%     + \sum_{h=1}^H \underbrace{\EE\sbr{T_h^{\pi^*,\hat\nu^*}\hat U_h(s_h) - \hat W_h^*(s_h)}}_{\dr (ii)}, \label{eq:OffML-subopt-3}
% \end{align}
Therefore, we just need to control (ii) then.
We do this separately for these three schemes. 

\subparagraph{Scheme 1:} For Scheme 1, we have by definition 
\begin{align*}
    {\dr (ii)} &= \EE\sbr{T_h^{\pi^*,\nu^*}\hat U_h(s_h) - \hat W_h^*(s_h)} \nend
    & = \EE\sbr{T_h^{\pi^*,\hat \nu^*_{s_h}}\hat U_h(s_h) - \hat W_h^*(s_h)} + \EE\sbr{\rbr{\rbr{T_h^{\pi^*,\nu^*} -T_h^{\pi^*,\hat \nu^*_{s_h}}} \hat U_h}(s_h)} \nend
    & \le 2 H \EE D_\TV\rbr{\nu_h^*(\cdot\given s_h), \hat \nu_{s_h}^*(\cdot)}, 
\end{align*}
where we define $\hat \nu^*_{s_h}\in\Delta(\cB)$ as the quantal response in \eqref{eq:scheme-1-new}, \eqref{eq:scheme-2-new}, or \eqref{eq:scheme-3-new} with respect to $\alpha_h =\pi^*(s_h)$ and $\theta_h = \hat\theta_{s_h}^*$ for $\hat\theta_{s_h} ^*$ as the optimizer to \eqref{eq:scheme-1-new}, \eqref{eq:scheme-2-new}, or \eqref{eq:scheme-3-new} under $\alpha_h =\pi^*(s_h)$. 
Here, the inequality holds by noting that the first term is equal to $0$, and upper bounding the second term with the TV bound.
Next, we invoke \Cref{lem:response diff-myopic-linear} as what we have done in \eqref{eq:OffML-TV-bonus}, and we use it for $\pi^*$ and plug in $\Psi = T \Sigma_{h, \cD}^{\theta_h^*} + I_d$,
\begin{align}
    &2 H D_\TV\rbr{\hat\nu_{s_h}^*(\cdot), \nu_h^*(\cdot\given s_h)} \nend
    &\quad \le 2 H \min_{\Psi\in\SSS^d}\cbr{
        %%%%%%%
    f\rbr{\sqrt{\trace\rbr{{\Psi}^{\dagger} \Sigma_{ s_h}^{\pi^*, \hat\theta^*_{s_h}}}}\cdot \bignbr{\theta^*- \hat\theta_{s_h}}_{{\Psi}}}, 
        %%%%%%%%%%
    f\rbr{\sqrt{\trace\rbr{{\Psi}^{\dagger} \Sigma_{ s_h}^{\pi^*, \theta_h^*}}}\cdot \bignbr{\theta^*- \hat\theta_{s_h}}_{{\Psi}}}
    }\nend
    &\quad \le 
    \min\cbr{\Gamma^{(2)}_h(s_h;\pi^*(s_h), \hat \theta_{s_h}^*), \Gamma^{(2)}_h(s_h;\pi^*(s_h), \theta_h^*)}. \label{eq:OffML-TV-guarantee-3}
\end{align}
This time, we use the second term in the minimization rather than the first term that we have used in \eqref{eq:OffML-TV-bonus} for obtaining the last inequaltiy. We also have $\bignbr{\theta^*- \hat\theta_{s_h}}_{\Psi} \le \sqrt{8C_\eta^2 \beta+ 4 B_\Theta^2}$ by the first one in \eqref{eq:OffML-MLE-guarantee}, which relates the term within the $f$ function to $\Gamma^{(2)}_h$. Therefore, we conclude with ${\dr (ii)}\le \Gamma^{(2)}_h(s_h;\pi^*(s_h), \theta_h^*)$ for Scheme 1. 
Hence, we have for the suboptimality that
\begin{align*}
    \subopt(\hat\pi) \le \sum_{h=1}^H 2\EE \Gamma^{(1)}_h(s_h, a_h, b_h) + \EE \Gamma^{(2)}_h(s_h;\pi^*(s_h), \theta_h^*).
\end{align*}

\subparagraph{Scheme 2:} For Scheme 2, we notice 
\begin{align*}
\hat W_h^*(s_h) 
&= \max_{\theta_h\in\confset_{h, \Theta}(\beta)} \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi^*\otimes\nu^{\pi^*, \theta_h}(\cdot,\cdot\given s_h)}_{\cA\times\cB} - \Gamma^{(2)}_h(s_h;\pi^*(s_h), \theta_h)\nend
&\ge T_h^{\pi^*, \nu^*} \hat U_h(s_h)- \Gamma^{(2)}_h(s_h;\pi^*(s_h), \theta_h^*),
\end{align*}
where the inequality holds by noting that $\theta_h^*\in\CI_\Theta(\beta)$ and $\nu^*=\nu^{\pi^*, \theta^*}$.
Therefore, we directly have ${\dr(ii)}\le \Gamma^{(2)}_h(s_h;\pi^*(s_h), \theta_h^*)$. Hence, we have for the suboptimality that
\begin{align*}
    \subopt(\hat\pi) \le \sum_{h=1}^H 2\EE \Gamma^{(1)}_h(s_h, a_h, b_h) + \EE\Gamma^{(2)}_h(s_h;\pi^*(s_h), \theta_h^*).
\end{align*}

{\noindent \bf Scheme 3:}
For Scheme 3, we have by definition
\begin{align*}
    \hat W_h^*(s_h) =  T_h^{\pi^*, \hat\nu_{s_h}^*} \hat U_h(s_h) - \Gamma^{(2)}_h(s_h;\alpha_h , \hat\theta_{s_h}^*), 
\end{align*}
where we remind the readers that $\hat\theta_{s_h}^* = \hat\theta_{h, \MLE}$ in the third scheme. Therefore, we have
\begin{align*}
    {\dr (ii)} 
    &= \EE\sbr{\rbr{T_h^{\pi^*, \nu^*} - T_h^{\pi^*, \hat\nu_{s_h}^*}}\hat U_h (s_h)} + \Gamma^{(2)}_h(s_h;\alpha_h , \hat\theta_{s_h}^*)\nend
    & \le 2 H D_\TV\rbr{\nu_h^*(\cdot\given s_h), \hat \nu_{s_h}^*(\cdot)} + \Gamma^{(2)}_h(s_h;\alpha_h , \hat\theta_{s_h}^*)\nend
    &\le 2 \Gamma^{(2)}_h(s_h;\pi^*(s_h), \hat \theta_{s_h}^*) =  2 \Gamma^{(2)}_h(s_h;\pi^*(s_h), \hat \theta_{h, \MLE}),
\end{align*}
where the last inequality holds by \eqref{eq:OffML-TV-guarantee-3}, but with $\hat\theta_{s_h}^*$ plugged in and using the bound for the first term in the minimization this time.
Hence, we have for the suboptimality that
\begin{align*}
    \subopt(\hat\pi) \le \sum_{h=1}^H 2\EE \Gamma^{(1)}_h(s_h, a_h, b_h) + 2 \EE\Gamma^{(2)}_h(s_h;\pi^*(s_h), \hat \theta_{h, \MLE}),
\end{align*}
which completes the proof of \Cref{thm:PMLE-VI-myopic}.



% For Scheme 2, we invoke the following lemma on the follower's response difference in terms of the TV distance.
% % \begin{lemma}[\textit{Response difference for myopic follower}]
% %     % \label{lem:response diff-myopic}
% %     For any $\hat\theta_h\in\Theta_h$ and the true model $\theta_h^*$, we have under the leader's prescription $\alpha_{s_h}\in\sA_h$ and state $s_h\in\cS_h$ that
% %     \begin{align*}
% %         D_\TV\rbr{\nu_h^{\alpha_{s_h},\theta_h^*}(\cdot\given s_h) , \nu^{\alpha_{s_h},\hat\theta_h}(\cdot\given s_h)} \le \min\cbr{f(\Gamma_h^{(2)}(s_h;\alpha_{s_h}, \hat\theta_h)), f(\Gamma_h^{(3)}(s_h;\alpha_{s_h}, \hat\theta_h))},
% %     \end{align*}
% %     where $\Psi\in \SSS_+^{d}$ can be any nonnegative definite matrix, function $f$ is defined as
% %     \begin{align*}
% %         f(x) = \eta x + \frac{\eta^2\exp(2\eta B_A)}{2} x^2 + \frac{\eta^4\exp(6\eta B_A)}{8} x^4, 
% %     \end{align*}
% %     $\Gamma^{(2)}_h$ is defined as
% %     \begin{align*}
% %         \Gamma^{(2)}_h(s_h;\alpha_{s_h},\theta_h) = \sqrt{\EE_{s_h}^{{\alpha_{s_h},\theta_h}}\sbr{\phi_h^{\alpha_{s_h}}(s_h,b_h)^\top {\Psi}^{\dagger}\phi_h^{\alpha_{s_h}}(s_h,b_h)} -\bignbr{\EE_{s_h}^{{\alpha_{s_h},\theta_h}}\phi_h^{\alpha_{s_h}}(s_h,b_h)}_{{\Psi}^{\dagger}}^2}\cdot \bignbr{\theta_h^*- \theta_h}_{{\Psi}}, 
% %     \end{align*}
% %     and $\Gamma^{(3)}$ is defined as 
% %     \begin{align*}
% %         \Gamma^{(3)}_h(s_h;\alpha_{s_h},\theta_h) = \sqrt{\EE_{s_h}^{{\alpha_{s_h},\theta_h^*}}\sbr{\phi_h^{\alpha_{s_h}}(s_h,b_h)^\top {\Psi}^{\dagger}\phi_h^{\alpha_{s_h}}(s_h,b_h)} -\bignbr{\EE_{s_h}^{{\alpha_{s_h},\theta_h^*}}\phi_h^{\alpha_{s_h}}(s_h,b_h)}_{{\Psi}^{\dagger}}^2}\cdot \bignbr{\theta_h^*- \theta_h}_{{\Psi}}
% %     \end{align*}
% %     \begin{proof}
% %         % See \Cref{sec:proof-response diff-myopic} for a detailed proof.
% %         See \Cref{sec:proof-formal-response diff-linear} for a detailed proof.
% %     \end{proof}
% % \end{lemma}
% Note that the only difference between $\Gamma^{(2)}$ and $\Gamma^{(3)}$ is that the expectations are taken with respect to $\theta_h$ in $\Gamma^{(2)}$ but $\theta_h^*$ in $\Gamma^{(3)}$.
% For Scheme 2, we also have by \eqref{eq:scheme-2} that
% \begin{align*}
%     &-\EE_{s_h}^{\hat\pi}\bigsbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)} \nend
%     &\quad = -\dotp{\hat U_h(s_h, \cdot,\cdot)}{\nu^{\hat\alpha_{s_h}, \theta^*}\otimes\hat\alpha_{s_h}} + \dotp{\hat U_h(s_h, \cdot,\cdot)}{\hat\alpha_{s_h}\otimes\nu^{\hat\alpha_{s_h}, \hat\theta_h}} - 2 H f\rbr{\hat\Gamma^{(2)}_h(s_h;\hat\alpha_{s_h}, \hat\theta_h)}\nend
%     & \quad \le 2 B_{U} D_\TV\rbr{\nu^{\hat\alpha_{s_h}, \theta_h^*}, \nu^{\hat\alpha_{s_h}, \hat\theta_h}} - 2 H f\rbr{\hat\Gamma^{(2)}_h(s_h;\hat\alpha_{s_h}, \hat\theta_h)}\nend
%     &\quad \le 0
% \end{align*}
% holds with probability $1-\delta$ for all $h\in[H]$.
% Here, the last inequality holds by \Cref{lem:response diff-myopic} on the TV distance where we replace $\bignbr{\theta_h^* - \hat\theta_h}_\Psi$ by the upper bound $C_\eta^2 T^{-1}\log(H\cN(\Theta_h, T^{-1})/\delta)+T^{-1} B_\theta^2$ in \eqref{eq:bandit-ub-1} in \Cref{lem:bandit} for choice $\Psi=\Sigma_{h, \cD}^{\hat\theta_h} + T^{-1} I_d$. Also, we note that $\hat U$ is truncated to $[-H, H]$.

% For Scheme 3, we have by \eqref{eq:scheme-3} that
% \begin{align*}
%     &-\EE_{s_h}^{\hat\pi}\bigsbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)} \nend
%     &\quad = -\dotp{\hat U_h(s_h, \cdot,\cdot)}{\nu^{\hat\alpha_{s_h}, \theta^*}\otimes\hat\alpha_{s_h}} +  \max_{\theta_h\in\Theta_h}\dotp{\hat U_h(s_h, \cdot,\cdot)}{\hat\alpha_{s_h}\otimes\nu^{\hat\alpha_{s_h}, \theta_h}} - 2 H f\rbr{\hat\Gamma^{(2)}_h(s_h;\hat\alpha_{s_h}, \theta_h)}\nend
%     & \quad \le  \max_{\theta_h\in\Theta_h} 2 B_{U} D_\TV\rbr{\nu^{\hat\alpha_{s_h}, \theta_h^*}, \nu^{\hat\alpha_{s_h}, \theta_h}} - 2 H f\rbr{\hat\Gamma^{(2)}_h(s_h;\hat\alpha_{s_h}, \theta_h)}\nend
%     &\quad \le 0, 
% \end{align*}
% holds with probability $1-\delta$ for all $h\in[H]$. Here, the last inequality holds from the same argument as in Scheme 2.
% Therefore, we conclude that ${\dr (i)}\le 0$ on the success of both \eqref{eq:bandit-ub-1} in \Cref{lem:bandit} and \eqref{eq:E^1} in \Cref{lem:Gamma_1-pessi}, which has  probability at least $1-2\delta$.


% \paragraph{Step 3. Guarantee of Suboptimality for Term (i).}
% Now that we have term (ii) bounded above by zero.
% The remaining step is just to upper bound term (i). By \Cref{lem:subopt decomp-myopic}, we have
% \begin{align*}
%     J(\pi^*) - \hat J &\le \sum_{h=1}^H \EE^{\pi^*}\sbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)} + \sum_{h=1}^H \EE^{\pi^*}\sbr{u_h(s_h, a_h, b_h) + \hat W_{h+1}(s_{h+1}) - \hat U_h(s_h, a_h, b_h)}\nend
%     &\le \sum_{h=1}^H \EE^{\pi^*}\sbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)} + \sum_{h=1}^H \EE^{\pi^*}\sbr{2\Gamma^{(1)}_h(s_h,a_h,b_h)}\nend
%     &\le \sum_{h=1}^H \EE^{\pi^*}\sbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)} + \sum_{h=1}^H 2C_1 d H \sqrt{\log(2d H T/\delta)}\cdot \EE^{\pi^*}\sbr{\sqrt{\phi_h(\cdot,\cdot,\cdot)^\top \Lambda_h^{-1}\phi_h(\cdot,\cdot,\cdot)}},
% \end{align*}
% where the second inequality holds on the success of \Cref{lem:Gamma_1-pessi}. 
% It remains to characterize the first term $\EE^{\pi^*}\bigsbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)}$ for these three schemes. 

% \subparagraph{Scheme 1.}
% For Scheme 1, we have
% \begin{align*}
%     &\EE_{s_h}^{\pi^*}\bigsbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)}\nend
%     &\quad \le \dotp{\hat U_h(s_h, \cdot,\cdot)}{\nu^{\alpha_{s_h}^*, \theta^*}\otimes\alpha_{s_h}^*} -  \min_{\theta_h\in\confset_{h}(\beta_h)} \dotp{\hat U_h(s_h, \cdot,\cdot)}{\alpha_{s_h}^*\otimes\nu^{\alpha_{s_h}^*, \theta_h}(\cdot,\cdot\given s_h)}\nend
%     &\quad \le 2 H \sup_{\theta_h\in \confset_h(\beta_h)}D_\TV\rbr{\nu^{\alpha_{s_h}^*, \theta_h^*}, \nu^{\alpha_{s_h}^*, \theta_h}} \nend
%     &\quad\le 2 H \sup_{\theta_h\in\confset_h(\beta_h)}f(\Gamma_h^{(3)}(s_h;\alpha_{s_h}^*, \theta_h)), 
% \end{align*}
% where the last inequality holds by \Cref{lem:subopt decomp-myopic}. Furthermore, If we plug in $\Psi=\Sigma_{h, \cD}^{\theta_h^*}$ for $\Gamma^{(3)}_h$, we arrive at
% \begin{align*}
%     & 2 H \sup_{\theta_h\in\confset_h(\beta_h)}f(\Gamma_h^{(3)}(s_h;\alpha_{s_h}^*, \theta_h))\nend
%     &\quad \le 2 H\sup_{\theta_h\in\confset_h(\beta_h)}\underbrace{\sqrt{\EE_{s_h}^{{\alpha_{s_h}^*,\theta_h^*}}\sbr{\phi_h^{\alpha_{s_h}^*}(s_h,b_h)^\top {\Sigma_{h, \cD}^{\theta_h^*}}^{\dagger}\phi_h^{\alpha_{s_h}^*}(s_h,b_h)} -\bignbr{\EE_{s_h}^{{\alpha_{s_h}^*,\theta_h^*}}\phi_h^{\alpha_{s_h}^*}(s_h,b_h)}_{{\Sigma_{h, \cD}^{\theta_h^*}}^{\dagger}}^2}}_{\ds \Upsilon_h\rbr{s_h; \alpha_{s_h}^*, \theta_h^*, \Sigma_{h, \cD}^{\theta_h^*}}}\cdot \bignbr{\theta_h^*- \theta_h}_{{\Sigma_{h, \cD}^{\theta_h^*}}}\nend
%     &\quad \le 2 H\Upsilon_h\rbr{s_h; \alpha_{s_h}^*, \theta_h^*, \Sigma_{h, \cD}^{\theta_h^*}} \cdot C_{\eta}
%     \sqrt{\rbr{\frac 1 T\log\rbr{\frac{H \cN(\Theta, 1/T)}{\delta}} + \beta_h}}.
% \end{align*}
% where the last inequality holds on the success of \Cref{lem:bandit} since we restrict $\theta_h\in\confset_h(\beta_h)$.
% In summary, Scheme 1 has suboptimality
% \begin{align*}
%     \subopt(\hat\pi) &\le \sum_{h=1}^H 2C_1 d H \sqrt{\log(2d H T/\delta)}\cdot \EE^{\pi^*}\sbr{\sqrt{\phi_h(\cdot,\cdot,\cdot)^\top \Lambda_h^{-1}\phi_h(\cdot,\cdot,\cdot)}}\nend
%     &\qquad + \sum_{h=1}^H 2 H C_{\eta}
%     \sqrt{\rbr{\frac 1 T\log\rbr{\frac{H \cN(\Theta, 1/T)}{\delta}} + \beta_h}}\cdot \EE^{\pi^*}\sbr{\Upsilon_h\rbr{s_h; \alpha_{s_h}^*, \theta_h^*, \Sigma_{h, \cD}^{\theta_h^*}}}, 
% \end{align*}
% with probability at least $1-2\delta$ on the success of \Cref{lem:bandit} and \eqref{lem:subopt decomp-myopic}.

% \subparagraph{Scheme 2.}
% For Scheme 2, we have
% \begin{align*}
%     &\EE_{s_h}^{\pi^*}\bigsbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)}\nend
%     &\quad \le \dotp{\hat U_h(s_h, \cdot,\cdot)}{\nu^{\alpha_{s_h}^*, \theta^*}\otimes\alpha_{s_h}^*} -  \dotp{\hat U_h(s_h, \cdot,\cdot)}{\alpha_{s_h}^*\otimes\nu^{\alpha_{s_h}^*, \hat\theta_{h,\MLE}}(\cdot,\cdot\given s_h)} \nend
%     &\qqquad + 2 Hf\rbr{\hat\Gamma^{(2)}_h\rbr{s_h;\alpha_{s_h}^*, \hat\theta_{h,\MLE}}}\nend
%     &\quad \le 2 H D_\TV\rbr{\nu^{\alpha_{s_h}^*, \theta_h^*}, \nu^{\alpha_{s_h}^*, \hat\theta_{h,\MLE}}} +  2 Hf\rbr{\hat\Gamma^{(2)}_h\rbr{s_h;\alpha_{s_h}^*, \hat\theta_{h,\MLE}}}\nend
%     &\quad\le 4 Hf\rbr{\hat\Gamma^{(2)}_h\rbr{s_h;\alpha_{s_h}^*, \hat\theta_{h,\MLE}}}, 
% \end{align*}
% where we have kernel $\Psi=\Sigma_{h, \cD}^{\hat\theta_{h,\MLE}}$ in $\Gamma^{(2)}_h$ by the scheme. Here, the last inequality holds by \Cref{lem:subopt decomp-myopic} and noting that $\Gamma^{(2)}_h\le \hat\Gamma^{(2)}_h$. Therefore, we have with probability $1-2\delta$ on the success of both \Cref{lem:subopt decomp-myopic} and \Cref{lem:bandit} that the suboptimality of Scheme 2 is bounded by
% \begin{align*}
%     \subopt(\hat\pi) &\le \sum_{h=1}^H 2C_1 d H \sqrt{\log(2d H T/\delta)}\cdot \EE^{\pi^*}\sbr{\sqrt{\phi_h(\cdot,\cdot,\cdot)^\top \Lambda_h^{-1}\phi_h(\cdot,\cdot,\cdot)}}\nend
%     &\qquad + \sum_{h=1}^H 4 H C_{\eta}
%     \sqrt{\frac 1 T\log\rbr{\frac{H \cN(\Theta, 1/T)}{\delta}}}\cdot \EE^{\pi^*}\sbr{\Upsilon_h\rbr{s_h; \alpha_{s_h}^*, \hat\theta_{h, \MLE}, \Sigma_{h, \cD}^{\hat\theta_{h,\MLE}}}}.
% \end{align*}

% \subparagraph{Scheme 3.}
% For Scheme 3, we have
% \begin{align*}
%     &\EE_{s_h}^{\pi^*}\bigsbr{\hat U_h(s_h, a_h, b_h) -\hat W_h(s_h)}\nend
%     &\quad \le \dotp{\hat U_h(s_h, \cdot,\cdot)}{\nu^{\alpha_{s_h}^*, \theta^*}\otimes\alpha_{s_h}^*} -  \max_{\theta_h\in\Theta_h}\bigg\{\dotp{\hat U_h(s_h, \cdot,\cdot)}{\alpha_{s_h}^*\otimes\nu^{\alpha_{s_h}^*, \theta_h}(\cdot,\cdot\given s_h)} \nend
%     &\qqquad - 2 Hf\rbr{\hat\Gamma^{(2)}_h\rbr{s_h;\alpha_{s_h}^*, \theta_h}}\bigg\}\nend
%     &\quad \le \dotp{\hat U_h(s_h, \cdot,\cdot)}{\nu^{\alpha_{s_h}^*, \theta^*}\otimes\alpha_{s_h}^*}  - \dotp{\hat U_h(s_h, \cdot,\cdot)}{\alpha_{s_h}^*\otimes\nu^{\alpha_{s_h}^*, \theta_h^*}(\cdot,\cdot\given s_h)} + 2 Hf\rbr{\hat\Gamma^{(2)}_h\rbr{s_h;\alpha_{s_h}^*, \theta_h^*}}\nend
%     &\quad\le 2 Hf\rbr{\hat\Gamma^{(2)}_h\rbr{s_h;\alpha_{s_h}^*, \theta_h^*}}, 
% \end{align*}
% where the second inequality holds by noting that $\theta^*\in\confset_h(\beta_h)$ and the last inequality follows from \Cref{lem:subopt decomp-myopic}. Recall the definition of $\hat\Gamma^{(2)}_h$, we have for the suboptimality
% \begin{align*}
%     \subopt(\hat\pi) &\le \sum_{h=1}^H 2C_1 d H \sqrt{\log(2d H T/\delta)}\cdot \EE^{\pi^*}\sbr{\sqrt{\phi_h(\cdot,\cdot,\cdot)^\top \Lambda_h^{-1}\phi_h(\cdot,\cdot,\cdot)}}\nend
%     &\qquad + \sum_{h=1}^H 2 H C_{\eta}
%     \sqrt{\rbr{\frac 1 T\log\rbr{\frac{H \cN(\Theta, 1/T)}{\delta}} + \beta_h}}\cdot \EE^{\pi^*}\sbr{\Upsilon_h\rbr{s_h; \alpha_{s_h}^*, \theta_h^*, \Sigma_{h, \cD}^{\theta_h^*}}}, 
% \end{align*}
% which completes the proof of \Cref{thm:PMLE-VI-myopic}.

% \subsection{Proof of \Cref{lem:response diff-myopic} on the Performance Difference of a Myopic Follower}\label{sec:proof-response diff-myopic}
% We first invoke the upper bound for this TV distance in \eqref{eq:response decomposition} of \Cref{lem:performance diff} where we take $H=1$ for myopic follower and swap the position of $\theta^*$ and $\hat\theta$ (by the exchangeability of the TV distance), 
% \begin{align}
%     &D_\TV\rbr{\nu^{\alpha,\theta^*}(\cdot\given s) , \nu^{\alpha,\hat\theta}(\cdot\given s)} \nend
%     &\quad \le \eta \cdot {\EE_{s}^{\alpha, \hat\theta}\sbr{\abr{\rbr{\EE^{\alpha, \hat\theta}_{s, b}-\EE^{\alpha, \hat\theta}_{s}}\bigsbr{ \inp{\phi^{\alpha}(s, b)}{\theta^*-\hat\theta} }}}} \nend
%     & \qquad\quad + \frac 1 2 \eta^2  \cdot 
%     {\EE_{s}^{\alpha, \hat\theta}\sbr{ \exp\bigrbr{\eta\bigabr{A^{\alpha,\hat\theta}-A^{\alpha,\theta^*}}}\cdot \bigabr{A^{\alpha,\hat\theta} - A^{\alpha, \theta^*}}^2}}\nend
%     &\quad \le \eta  \cdot \underbrace{\sqrt{\EE_{s}^{\alpha, \hat\theta}\sbr{\rbr{\rbr{\EE^{\alpha, \hat\theta}_{s, b}-\EE^{\alpha, \hat\theta}_{s}}\bigsbr{ \inp{\phi^{\alpha}(s, b)}{\theta^*-\hat\theta} }}^2 }}}_{\dr (i)} \nend
%     & \qquad\quad + \frac 1 2 \eta^2  \exp\rbr{2\eta B_A} \cdot 
%     {\EE_{s}^{\alpha, \hat\theta}\sbr{ \bigrbr{A^{\alpha,\hat\theta} - A^{\alpha, \theta^*}}^2}},\label{eq:TV-ub-MLE} 
% \end{align}
% where the second inequality holds by using the Cauchy-Schwartz inequality and bounding the exponential term by its maximal value.
% By a standard variance and mean decomposition in the last quadratic term of \eqref{eq:TV-ub-MLE}, we obtain
% \begin{align}
%     \EE_{s}^{\alpha, \hat\theta}\sbr{ \bigrbr{A^{\alpha,\hat\theta} - A^{\alpha, \theta^*}}^2}
%     & \le \EE_{s}^{\alpha, \hat\theta}\sbr{ \rbr{ \rbr{\EE^{\alpha, \hat\theta}_{s, b}-\EE^{\alpha, \hat\theta}_{s}}\sbr{A^{\alpha,\hat\theta} - A^{\alpha, \theta^*}}}^2} + \rbr{\EE^{\alpha, \hat\theta}_{s}\sbr{A^{\alpha,\hat\theta} - A^{\alpha, \theta^*}}}^2\nend
%     & =\underbrace{\EE_{s}^{\alpha, \hat\theta}\sbr{ \rbr{ \rbr{\EE^{\alpha, \hat\theta}_{s, b}-\EE^{\alpha, \hat\theta}_{s}}\sbr{\inp[\big]{\phi^{\alpha}(s, b)}{\theta^*-\hat\theta} }}^2}}_{\dr (i)^2} + \underbrace{\rbr{\EE^{\alpha, \hat\theta}_{s}\sbr{A^{\alpha,\hat\theta} - A^{\alpha, \theta^*}}}^2}_{\dr (ii)}.
%     % &\le \EE_{s}^{\alpha, \hat\theta}\sbr{ \rbr{ \rbr{\EE^{\alpha, \hat\theta}_{s, b}-\EE^{\alpha, \hat\theta}_{s}}\sbr{\inp[\big]{\phi^{\alpha}(s, b)}{\theta^*-\hat\theta} }}^2}
%     % &\qquad + \underbrace{\rbr{ \rbr{\EE^{\alpha, \theta^*}_{s}-\EE^{\alpha, \hat\theta}_{s}}\sbr{\inp[\big]{\phi^{\alpha}(s, b)}{\theta^*-\hat\theta} }}^2}_{\dr (iii)},
%     \label{eq:A-square-ub}
% \end{align}
% where the equality follows from \Cref{lem:AQV-func diff} on the difference in the advantage function
% % , and the last inequality holds by \Cref{lem:KL-ub} where we notice that $\kl\infdivx[\big]{\nu^{\alpha, \hat\theta}}{\nu^{\alpha, \theta^*}} = \EE^{\alpha, \hat\theta}_{s}\bigsbr{A^{\alpha,\hat\theta} - A^{\alpha, \theta^*}}$.
% Combining \eqref{eq:A-square-ub} with \eqref{eq:TV-ub-MLE}, we conclude that
% \begin{align}
%     D_\TV\rbr{\nu^{\alpha,\theta^*}(\cdot\given s) , \nu^{\alpha,\hat\theta}(\cdot\given s)} &\le \eta \cdot {\dr(i)} + \frac{\eta^2\exp\rbr{2\eta B_A}}{2} \cdot \rbr{{\dr(i)}^2 + {\dr(ii)}}.\label{eq:TV-ub-offline}
% \end{align}
% % It is straightforward to bound term (i) by guarantee of MLE.  By \eqref{eq:bandit-ub-2} in \Cref{lem:bandit}, we have
% % \begin{align}
% %     \bignbr{\theta^*-\hat\theta_{\MLE}}_{{\Psi}}^2 \le \underbrace{\min\cbr{\frac{\lambda_d(\tilde\Phi) }{\lambda_1(\Sigma_{\cD})}, \frac{|\cB|}{\min_{t\in[T]}\lambda_2(\Xi^{t, \hat\theta})}}}_{\ds Z_{\cD}} C_\eta^2 \cdot 
% %         \frac 1 T\log\rbr{\frac{\cN(\Theta, 1/T)}{\delta}}, \label{eq:MLE-guarantee-offline}
% % \end{align}
% % where $C_\eta= {B_A}/\rbr{1-\exp\rbr{-\eta B_A}}$, $\Sigma_{\cD}=T^{-1}\cdot \sum_{t=1}^T\EE_{s^t}^{\nu^{\alpha^t, \hat\theta}}[\psi^{t, \hat\theta} {\psi^{t, \hat\theta}}^\top]$, $\psi^{t, \theta} =\phi^{t}(b) - \EE_{s^t}^{\nu^{\alpha^t, \theta}}[\phi^t(b')]$, and 
% % $\tilde\Phi^t=\int_{\cB}\phi^t(b)\phi^t(b)^\top \rd b$.
% % Note if $\cB$ has infinitely many actions, the second term in $Z_{\cD}$ is meaningless and only the first term matters.
% % If $\cB$ has finite actions, we define $\tilde\Phi^t=\sum_{b\in\cB}\phi^t(b)\phi^t(b)^\top$ and $\Xi^{t, \hat\theta} = \diag(\nu^{\alpha^t, \hat\theta}(\cdot\given s^t)) - (\nu^{\alpha^t, \hat\theta}(\cdot\given s^t))(\nu^{\alpha^t, \hat\theta}(\cdot\given s^t))^\top$.
% To bound term (i), we define a weighted Laplacian of the comparison feature graph under prescription $\alpha$, state $s$ and parameter $\theta$ as
% \begin{align*}
%     \Sigma_{ s}^{\alpha, \theta} \defeq \EE_{s}^{\alpha, \theta} \sbr{\psi^{\alpha,\theta}(s, b)\psi^{\alpha,\theta}(s, b)^\top}\quad\text{where}\quad \psi^{\alpha,\theta}(s, b) = \phi^{\alpha}(s, b) - \EE_{s}^{\alpha,\theta}\phi^{\alpha}(s, \cdot).
% \end{align*}
% Under this definition, we introduce a nonnegative definite matrix $\Psi\in \SSS_+^{d}$ and write term (i) as
% \begin{align*}
%     {\dr(i)} = \bignbr{\theta^*-\hat\theta}_{\Sigma_{ s}^{\alpha, \hat\theta}} = \nbr{\sqrt{\Psi}^{\dagger}\sqrt{\Psi}\rbr{\theta^*-\hat\theta}}_{\Sigma_{ s}^{\alpha, \hat\theta}} \le \sqrt{\Bignbr{\Psi^{\dagger} \Sigma_{ s}^{\alpha, \hat\theta}}_\oper }\cdot \bignbr{\theta^*-\hat\theta}_{\Psi},
% \end{align*}
% % where we recall from \Cref{lem:bandit} the definition of the weighted Laplacian of the comparison feature graph with respect to the offline data $\cD$ and parameter $\hat\theta$ as $ \Sigma_{h, \cD}^{\theta}\defeq T^{-1} \sum_{t=1}^T
% % \EE_{s^t}^{\alpha^t, \theta}\bigsbr{\psi^{\alpha^t, \theta}(s, b)\psi^{\alpha^t, \theta}(s, b)^\top}$.
% Note that the operator norm is further bounded by the trace,
% \begin{align*}
%     \sqrt{\Bignbr{{\Psi}^{\dagger} \Sigma_{ s}^{\alpha, \hat\theta}}_\oper} 
%     &\le \sqrt{\trace\rbr{{\Psi}^{\dagger} \Sigma_{ s}^{\alpha, \hat\theta}}} \nend
%     &= \sqrt{\trace\rbr{\EE_{s}^{\alpha, \hat\theta}\sbr{ {\Psi}^\dagger \psi^{\alpha,\hat\theta}(s, b)\psi^{\alpha,\hat\theta}(s, b)^\top}}}\nend
%     & = \sqrt{\EE_{s}^{\alpha, \hat\theta}\sbr{\psi^{\alpha,\hat\theta}(s, b)^\top {\Psi}^\dagger  \psi^{\alpha,\hat\theta}(s, b)}}.
% \end{align*}
% We plug in the definition $\psi^{\alpha,\hat\theta}(s, b)=\phi^{\alpha}(s, b) - \EE_{s}^{\alpha,\hat\theta}\phi^{\alpha}(s, \cdot)$ and obtain
% \begin{align*}
%     &\sqrt{\EE_{s}^{\alpha, \hat\theta}\sbr{\psi^{\alpha,\hat\theta}(s, b)^\top {\Psi}^\dagger  \psi^{\alpha,\hat\theta}(s, b)}} = \underbrace{\sqrt{\EE_{s}^{{\alpha,\hat\theta}}\sbr{\phi^{\alpha}(s,b)^\top {\Psi}^{\dagger}\phi^{\alpha}(s,b)} -\bignbr{\EE_{s}^{{\alpha,\hat\theta}}\phi^{\alpha}(s, b)}_{{\Psi}^{-1}}^2}}_{\ds \Upsilon_{s}^{\alpha,\hat\theta}}.
% \end{align*}
% Hence, we have for term (i) that  ${\dr (i)}\le 
% \Upsilon_{s}^{\alpha,\hat\theta}\cdot \bignbr{\theta^*-\hat\theta}_{{\Psi}}$.
% % \begin{align}
% %     {\dr (i)} 
% %     &\le 
% %     \Upsilon_{s}^{\alpha,\hat\theta}\cdot \bignbr{\theta^*-\hat\theta}_{{\Psi}}
% %     \le 2 \Upsilon_{s}^{\alpha,\hat\theta}\cdot \underbrace{\sqrt{Z_{\cD} C_\eta^2  \cdot \frac 1 T\log\rbr{\frac{\cN(\Theta, 1/T)}{\delta}} + \bignbr{\hat\theta - \hat\theta_{\MLE}}_{{\Psi}}^2}}_{\ds \zeta^{\hat\theta}},\label{eq:myopic-(i)-ub}
% % \end{align}
% % on the success of \Cref{lem:bandit}. 
% % Here, the second inequality follows from the triangular inequaltiy, and the last inequality holds from \eqref{eq:MLE-guarantee-offline}.
% Now, we have addressed term (i) of \eqref{eq:TV-ub-offline} and it remains to show the upper bound for term (ii). Observe that under the quantal response model with logistic preference, term (ii) is nothing but just the squared KL divergence $\eta^{-2}\kl\infdivx[]{\nu^{\alpha, \hat\theta}(\cdot\given s)}{\nu^{\alpha, \theta^*}(\cdot\given s)}^2$. Let $\Delta \theta=\hat\theta - \theta^*$ and consider $\hat\theta$ to be fixed. The Hessian of the KL divergence with respect to parameters in the second position is 
% \begin{align*}
%     \eta^{-2}\nabla_{\theta}^2 \kl\infdivx[]{\nu^{\alpha, \hat\theta}(\cdot\given s)}{\nu^{\alpha, \theta}(\cdot\given s)} = \EE_s^{\alpha,\theta} \sbr{\phi^\alpha(s, b)\phi^\alpha(s, b)^\top} - \EE_s^{\alpha,\theta}\sbr{\phi^\alpha(s, b)} \cdot \EE_s^{\alpha,\theta}\sbr{\phi^\alpha(s, b)}^\top = \Sigma_s^{\alpha,\theta}.
% \end{align*}
% We observe that the KL divergence is strongly convex in the second parameter. Actually, for any test $x\in\RR^d$ and let $g(b)=\phi^\alpha(s, \cdot)^\top x$, we have $x^\top \rbr{\eta^{-2}\nabla_{\theta}^2 \kl\infdivx[]{\nu^{\alpha, \hat\theta}(\cdot\given s)}{\nu^{\alpha, \theta}(\cdot\given s)}} x = \Var^{\alpha,\theta}[g(b)]$ and furthermore,
% \begin{align*}
%      \Var^{\alpha,\theta}[g(b)]
%     \ge \exp(-2\eta B_A) \EE_s^{\alpha, \hat\theta}\sbr{\rbr{g(b) - \EE_s^{\alpha,\theta}[g(b)]}^2} \ge \exp\rbr{-2\eta B_A} \Var^{\alpha, \hat\theta}[g(b)],
% \end{align*}
% which suggests that $\eta^{-2}\nabla_{\theta}^2 \kl\infdivx[]{\nu^{\alpha, \hat\theta}(\cdot\given s)}{\nu^{\alpha, \theta}(\cdot\given s)}=\Sigma_s^{\alpha,\theta}\succeq \exp(-2\eta B_A) \Sigma_s^{\alpha,\hat\theta}$. Actually, we can also swap $\theta$ and $\hat\theta$ and obtain 
% \begin{align*}
%     \eta\exp(2\eta B_A) \Sigma_s^{\alpha,\hat\theta} \succeq \eta^{-}\nabla_{\theta}^2 \kl\infdivx[]{\nu^{\alpha, \hat\theta}(\cdot\given s)}{\nu^{\alpha, \theta}(\cdot\given s)} \succeq \eta\exp(-2\eta B_A) \Sigma_s^{\alpha,\hat\theta} , \quad \forall \theta\in\Theta.
% \end{align*}
% Therefore, we can bound term (ii) simply by
% \begin{align*}
%     {\dr(ii)}\le \frac{\eta^2}{4} \exp\rbr{4\eta B_A} \bignbr{\theta^*-\hat\theta}_{\Sigma_s^{\alpha,\hat\theta}}^4 \le  \frac{\eta^2}{4} \exp\rbr{4\eta B_A} \rbr{\Upsilon_{s}^{\alpha,\hat\theta}\cdot \bignbr{\theta^*-\hat\theta}_{{\Psi}}}^4,
% \end{align*}
% where the last inequality holds by the same upper bound for term (i).
% Combining our results for both term (i) (ii), we have for \eqref{eq:TV-ub-offline} that
% \begin{align*}
%     &D_\TV\rbr{\nu^{\alpha,\theta^*}(\cdot\given s) , \nu^{\alpha,\hat\theta}(\cdot\given s)} \nend
%     &\quad\le \eta \cdot {\dr(i)} + \frac{\eta^2\exp\rbr{2\eta B_A}}{2} \cdot \rbr{{\dr(i)}^2 + {\dr(ii)}}\nend
%     &\quad\le \eta \Upsilon_{s}^{\alpha,\hat\theta} \bignbr{\theta^*-\hat\theta}_{{\Psi}} + \frac{\eta^2\exp\rbr{2\eta B_A}}{2} \rbr{\rbr{\Upsilon_{s}^{\alpha,\hat\theta} \bignbr{\theta^*-\hat\theta}_{{\Psi}}}^2 + \frac{\eta^2}{4} \exp\rbr{4\eta B_A} \rbr{\Upsilon_{s}^{\alpha,\hat\theta} \bignbr{\theta^*-\hat\theta}_{{\Psi}}}^4}.
% \end{align*}
% Now, we let $\Gamma_h^{(2)}(s_h;\alpha_{s_h}, \hat\theta_h) = \Upsilon_{s}^{\alpha,\hat\theta} \bignbr{\theta^*-\hat\theta}_{{\Psi}} $ and 
% \begin{align*}
%     f(x) = \eta x + \frac{\eta^2\exp(2\eta B_A)}{2} x^2 + \frac{\eta^4\exp(6\eta B_A)}{8} x^4, 
% \end{align*}
% and just conclude that $D_\TV\bigrbr{\nu_h^{\alpha_{s_h},\theta_h^*}(\cdot\given s_h) , \nu_h^{\alpha_{s_h},\hat\theta_h}(\cdot\given s_h)} \le f(\Gamma_h^{(2)}(s_h;\alpha_{s_h}, \hat\theta_h))$, 
% which completes the proof of \Cref{lem:response diff-myopic}. 
% % One can swap $\theta_h^*$ and $\$


% \subsection{Follow up Discussion on \Cref{thm:PMLE-VI-myopic}}\label{sec:follow up on myopic offline}
% \todo{comment on the use of the Laplacian, including why a standard $\sum\phi\phi^\top$ would fail. Also, comment on the use of $L_\cD$. }