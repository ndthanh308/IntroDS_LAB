\ifmain
\section{Extension to Learning with Farsighted Follower} \label{sec:farsighted}
\fi
\ifneurips
\subsection{Extension to Learning with Farsighted Follower} \label{sec:farsighted}
\fi
In this section, we explore the possibility of learning the behavior model for a farsighted follower in both the offline and online settings.
We extend our previous techniques to  online learning the QSE with nonmyopic follower. 
To ease our presentation, we use $M=(r^M, u^M, P^M)\in\cM$ to denote the model and all definitions previously introduced with $\theta$ can be naturally extended to this larger model class $M$. In particular, we let $M^*$ denote the true model. We suppose that the follower's reward in our model (which contains the true model) satisfies a linear constraint $\la x(\cdot), r_h^M(s_h, a_h, \cdot)\ra_{\cB} = \varsigma$ for all $(s_h, a_h)\in\cS\times\cA$ and $h\in[H]$, where $x:\cB\rightarrow\RR$ is a known function and $\varsigma\in\RR$ is a known constant. In particular, such a constraint rules out a free dimension in the follower's reward and is introduced to ensure that $r$ can be uniquely identified in the quantal response model. 
For instance, the linear constraint can be that the reward averaged over $\cB$ is $0.5$. We remark that such an assumption is not without loss of generality for a farsighted follower.


% \paragraph{Data Collection.}
% The offline data collection process is similar to the one described in \Cref{sec:offline-myopic}, except that the leader announces her $H$-step policy $\pi^t$ at the beginning of episode $t$ and the data collect the announced joint policy $\pi^t$ rather than just the prescription at each visited state.
% We assume the policy to be indepedent across episode for the offline setting.
% For the online setting, the game is played in a similar way except that the leader's future policy can depend on the past data. 

% \paragraph{Learning the Behavior Model.}
\paragraph{Offline Algorithm.}
Although we have gained success in learning the behavior model for the myopic case with both general function approximation and linear function approximation, previous results does not generalize to nonmyopic follower easily. A unique challenge that we face in the farsighted follower case is that the follower's choice model has a long term dependency, which means that the uncertainty in the transition kernel also enters the estimation error of the follower's behavior model. Another challenge is that the follower's choice is a joint effect of the follower's future expected total utilities, and it takes additional efforts to decompose it stepwise so as to utilize the knowledge for planning.

We propose a model based method for jointly learning the environment model and the behavior model. Recall from \Cref{sec:markov_game_def} the model is given by $M=\cbr{P_h^M, r_h^M, u_h^M}_{h\in[H]}$. We consider the following negative generalized likelihood 
\begin{align*}
    \cL_h(M) = - \sum_{i=1}^T \rbr{\eta A_h^{\pi^i, M}(s_h^i, b_h^i) - \log P_h^M(s_{h+1}^i\given s_h^i, a_h^i, b_h^i) - (u_h^i - u_h^M(s_h^i, a_h^i, b_h^i))^2}, 
\end{align*}
where the first term $\eta A_h^{\pi^i, M}$ comes from the follower's quantal response under model $M$ and policy $\pi^i$, and the second term is the likelihood of the transition kernel. 
Note that computing this $A_h^{\pi^i, M}$ actually requires computing the best response for step $h+1,\dots,H$ and update the follower's Q- and V-functions according to \eqref{eq:qv_pi_qr}. 
Using this generalized likelihood, one can directly obtain a confidence set for model $M$ and do planning using pessimism, 
\begin{align*}
    \hat\pi &= \argmax_{\pi\in\Pi} \min_{M\in\CI_M(\beta)} J(\pi, M), \quad \text{where} \\
    \confset_\cM(\beta) &= \cbr{M\in\cM\given \cL_h (M)\le \inf_{M'\in\cM} \cL_h(M') + \beta, \quad \forall h\in[H]}, 
\end{align*}
and
$
    J(\pi, M)
    %  \defeq \EE^{\pi, \nu^{\pi,M}}\osbr{\sum_{h=1}^H u_{h}^M(s_h, a_h, b_h)}.
$ is just the total reward for the leader evaluated under an estimated model $M$.
We have the following \textbf{P}essimistic \textbf{M}aximum \textbf{L}ikelihood \textbf{E}stimation algorithm for the offline setting. 
\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Require {$\eta, \cD$}
    \State Build confidence set $\confset_\cM(\beta) = \cbr{M\in\cM: \cL_h(M)-\inf_{M'\in\cM}\cL_h(M')\le \beta, \forall h\in[H]}$.
    \State Find the pessimistic policy $\hat\pi = \argmax_{\pi\in\Pi}\min_{ M\in\confset_\cM(\beta)} J(\pi, M)$. 
    \Ensure {$\hat\pi$}
    \end{algorithmic}
    \caption{Pessimistic MLE (PMLE) for Offline Learning with Farsighted Follower}
    \label{alg:real-PMLE}
\end{algorithm}
\begin{theorem}[Suboptimality for PMLE]\label{thm:PMLE}
We let $\beta\ge  \allowbreak 9\log(3e^2H \cN_\rho(\cM,T^{-1})\delta^{-1})$, where $\cN_\rho(\cM,\epsilon)$ is the minimal size of an $\epsilon$-optimistic covering net of $\cM$, which is defined in \Cref{lem:MLE}. 
Suppose $\la x, r_h^M(s_h, a_h, \cdot)\ra_\cB = \varsigma$ for all $M\in\cM$, $(s_h, a_h)\in\cS\times\cA$ and $h\in[H]$, and we let $\kappa = \nbr{x}_\infty/|\la x, \ind\ra_\cB|$.
Then with probability at least $1-\delta$, we have for the PMLE algorithm that 
\begin{align*}
    \subopt(\hat\pi) \lesssim C_1^{\pi^*} H^2 \sqrt{ \beta T^{-1}} + C_2^{\pi^*} \eta C_\eta H^{5/2}\sqrt{ \eff_H(\gamma)\beta T^{-1/2}} + C_3^{\pi^*}\cdot C^{(2)} L^{(2)} H \beta T^{-1}, 
\end{align*}
where 
\[C_1^{\pi^*} = \max_{M\in\cM, h\in[H]} \sqrt{\frac{\EE\Bigsbr{\Bigrbr{\bigrbr{U_h^{\pi^*, M}-\bigrbr{u_h+P_h W_{h+1}^{\pi^*, M}}}(s_h, a_h, b_h)}^2 }}{T^{-1}\sum_{i=1}^T \EE^i\Bigsbr{\Bigrbr{\bigrbr{U_h^{\pi^*, M}-\bigrbr{u_h+P_h W_{h+1}^{\pi^*, M}}}(s_h, a_h, b_h)}^2 }}}, \]
\[C_2^{\pi^*} =\max_{M\in\cM, h\in[H]}\sqrt\frac{\EE \Bigsbr{\rbr{\rbr{\EE_{s_h, b_h} -\EE_{s_h}}\sbr{\sum_{l=h}^H \gamma^{l-h}\rbr{r_l^M - r_l + \gamma \bigrbr{P_l^M - P_l} V_{l+1}^{\pi^*, M}}(s_l,a_l,  b_l)}}^2}}{T^{-1}\sum_{i=1}^T \EE^i \Bigsbr{\rbr{\rbr{\EE_{s_h, b_h}^i -\EE_{s_h}^i}\sbr{\sum_{l=h}^H \gamma^{l-h}\rbr{ r_l^M - r_l + \gamma \bigrbr{P_l^M - P_l} V_{l+1}^{\pi^*, M}}(s_l,a_l,  b_l)}}^2}}, \]
\[C_3^{\pi^*} = \max_{h\in[H], M\in\cM}{\frac{\EE\sbr{ \rbr{\EE_{s_h, b_h}\sbr{\bigrbr{r_h^{M} - r_h + \gamma \bigrbr{P_h^M - P_h}  V_{h+1}^{\pi^*, M}}(s_h, a_h, b_h)}}^2}}{T^{-1}\sum_{i=1}^T \EE^i\sbr{ \rbr {\EE_{s_h, b_h}^i\sbr{{\bigrbr{r_h^{M} - r_h + \gamma \bigrbr{P_h^M - P_h}  V_{h+1}^{\pi^*, M}}(s_h,a_h, b_h)}}}^2}}},\]
where the expectation $\EE$ without superscript is taken with respect to $(\pi^*, \nu^{\pi^*})$, and we have constants 
\begin{align*}
    L^{(2)} &= c H^2 \eff_H(c_2)^2 \kappa^2 \exp\rbr{8\eta B_A} (\eta^{-1}+B_A)^2, \\
    C^{(2)}  & =  2  \eta^2 H^2\cdot \exp\orbr{6\eta B_A} \cdot(1+4 \eff_H(\gamma)) \cdot \rbr{\eff_H(\exp(2\eta B_A)\gamma)}^2.
\end{align*}
\end{theorem}
\begin{proof}
    See \Cref{sec:proof-PMLE} for a detailed proof.
\end{proof}

Compared to \Cref{thm:Offline-MG}, the suboptimality for a farsighted follower becomes more complicated in the sense that concentrability coefficient $C_2^{\pi^*}$ for the follower's quantal response error also depend on estimation errors for both the follower's reward $r_h$ and the transition $P_h$ along the optimal trajectory.
In addition, the second order quantal response error, i.e., the last term in the suboptimality, suffers from a $\exp(\cO(\eta H))$ coefficient, which occurs as a result of the quantal response model and the coefficient also grows exponentially with respect to the inverse temperature $\eta$.
Such a result suggests for small data size $T$, the second order QRE terms dominates. 

% We first address the problem of learning the long term utilities of the follower. Note that the follower's Q function satisfies the Bellman update $Q_h^\pi = \BB_h^\pi Q_{h+1}^\pi$. In line with the Bellman loss $\ell$ constructed in \eqref{eq:myopic-offline-general-Bellman loss} for learning the Bellman update for the leader, we construct Bellman loss $\ell^F_{h,\cD}$ as 
% \begin{align*}
%     \ell^F_{h}(Q_h', Q_{h+1}, \theta, \pi) = \sum_{i=1}^T \rbr{\rbr{Q_h'-r_h^{\pi, \theta}}(s_h^i, b_h^i) - \inp[\Big]{\nu_{h+1}^{Q_{h+1}}(\cdot\given s_{h+1}^i)}{Q_{h+1}(s_{h+1}^i, \cdot)}}, 
% \end{align*}
% where we note that $\nu$ is a function of $Q$. Using this Bellman loss, we are able to learn the transition and obtain an estimation of the Q function for each $\theta$ and policy $\pi^i$. Specifically, we consider the following confidence set
% \begin{equation}\label{eq:online-FG-Q}
%     \CI_Q^\theta(\beta) =\cbr{
%         \begin{aligned}
%         &\cbr{Q^i}_{i\in[T]}\in\cQ^{\times(H\times T)}:\\
%         &\qquad \ell_h^F(Q_{h}^i, Q_{h+1}^i, \theta, \pi^i) - \inf_{Q'\in\cQ}  \ell_h^F(Q', Q_{h+1}^i, \theta, \pi^i) \le \beta, \forall (h,i)\in[H]\times[T]
%         \end{aligned}
%     }.
% \end{equation}
% \Cref{eq:online-FG-Q} characterizes a confidence set for the Q function at each episode for each $\theta$. We can expect the confidence set to be valid and accurate since the estimation of each $Q^i$ incorporates the transition information in all trajectories and we just need to ensure a $\beta$ by taking a union bound over the $T$ episodes.
% Since we can estimate the Q function for each episode in the offline data, we are then able to incorporate MLE with the negative log-likelihood using the estimated Q function,
% \begin{align}\label{eq:online-FG-MLE}
%     \cL_h\rbr{Q^{(H,T)}} = - \sum_{i=1}^T \rbr{\eta Q_h^i(s_h^i, b_h^i) - \log \rbr{\int_{b'\in\cB} \exp\rbr{\eta Q_h^i(s_h^i, b')}\rd b'}}, 
% \end{align}
% where we abbreviate $\cbr{Q_h^i}_{i\in[T]}$ to $Q^{(H,T)}$. Using the likelihood and the estimated Q function, 
% we then obtain the following confidence set for $\theta$,
% \begin{align*}
%     \CI_\Theta(\beta) = \cbr{\theta\in\Theta: \cL_h\rbr{\cbr{Q_h^i}_{i\in[T]}} - \inf_{}\cL_h\rbr{\cbr{Q_h^i}_{i\in[T]}}}
% \end{align*}
\paragraph{Online Algorithm.}
Similar to the offline setting, 
the negative generalized-likelihood for the model $M=\cbr{r_h^M, P_h^M, u_h^M}_{h\in[H]}$ at step $h$ and time $t$ is given by
\begin{align}
    \cL_h^t(M)& = -\sum_{i=1}^{t-1} \bigg(\eta A_{h}^{\pi^i, M}(s_h^i, b_h^i) + \log P_h^M(s_{h+1}^i\given s_h^i, a_h^i, b_h^i)   - \rbr{u_{h}^i - u_{h}^M(s_h^i, a_h^i, b_h^i)}^2\bigg),  \label{eq:MLE}
\end{align}
% where $\tau^{t-1}=\cbr{(s_h^i, a_h^i, b_h^i, \pi_h^i)_{h\in[H]}}_{i\in[t-1]}$ is the history up to step $t-1$.
The OMLE algorithm plays the greedy policy with respect to the leader's most favorable model in the $\beta$-superlevel set of the above log-likelihood $\cL^t(\cdot)$, 
\begin{align}
    \pi^t=\argmax_{\pi\in\Pi, M\in\confset_\cM^t(\beta)} J(\pi, M), \quad \st \quad \confset_\cM^t(\beta) = \cbr{M\in\cM\given \cL_h^t (M)\le \inf_{M'\in\cM} \cL_h^t(M') + \beta, \forall h\in[H]}, \label{eq:OMLE}
\end{align}
where
$
    J(\pi, M) 
    % \defeq \EE^{\pi, \nu^{\pi,M}}\osbr{\sum_{h=1}^H u_{h}^M(s_h, a_h, b_h)}.
$
is just the total reward for the leader evaluated under an estimated model $M$.
We have the following \textbf{O}ptimistic \textbf{M}aximum \textbf{L}ikelihood \textbf{E}stimation algorithm for the online setting. 
\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Require {$\eta, T$}
    \State Initiate $\cD=\emptyset$.
    \For{$t=1,\dots,T$}
    \State Build confidence set $\confset^t_\cM(\beta) = \cbr{M\in\cM: \cL_h^t(M)-\inf_{M'\in\cM}\cL_h^t(M')\le \beta, \forall h\in[H]}$.
    \State Find the optimistic policy $\pi^t = \argmax_{\pi\in\Pi, M\in\confset_\cM^t(\beta)} J(\pi, M)$. 
    \State Announce $\pi^t$ and observe a trajectory $\tau^t = \ocbr{\orbr{s_h^t, a_h^t, b_h^t, \pi_h^t}}_{h\in[H]}$.
    \State Update $\cD \leftarrow \cD \cup \{\tau^t\}$.
    \EndFor
    \end{algorithmic}
    \caption{Optimistic MLE (OMLE) for Online Farsighted Follower}
    \label{alg:OMLE}
\end{algorithm}
% We construct $f_0^M, f_{1, h}^M$ as
% \begin{align*}
%     f_{0,h}^M(\pi) &\defeq \EE^{\pi, M^*}\sbr{u_h^M  - u_h^{M^*} + \rbr{\PP_h^M - \PP_h^{M^*}} W_h^{\pi_\opt^M, M} },\nend
%     f_{1,h}^M(\pi) & \defeq \EE^{\pi, M^*}\abr{\sum_{i=h}^H\gamma^{i-h}\rbr{\EE_{s_h, b_h}^{\pi, M^*} - \EE_{s_h}^{\pi, M^*}}\sbr{ r_i^M-r_i^{M^*}+\gamma \rbr{\PP_i^M - \PP_i^{M^*}}V_i^{\pi_\opt^M, M}}} .
% \end{align*}
% % \todo{There is an error that $f_{1, h}$'s Eluder dimension is not $d$ when the $\abr{\cdot}$ exists. We should deal with the squared Hellinger distance directly in \Cref{lem:1st-ub}.}
% We also consider the following function classes $\cF_{0,h}=\{f_{0,h}^M(\cdot):M\in\cM\}$ and $\cF_{1,h}=\{f_{1,h}^M(\cdot):M\in\cM \}$.
% Let $d_0 = \max_h\dimE(\cF_{0,h}, \Pi, 1/\sqrt T)$, $d_1=\max_h\dimE(\cF_{1,h}, \Pi, 1/\sqrt T)$ be the (maximal) Eluder dimensions for these function classes.
% Similar to \citet{chen2022unified}, we define

In the sequel, we define three types of errors the corresponding (distributional) eluder dimensions for characterizing the online learning complexities.
The first is the leader's Bellman residuals and the class is defined as 
$\cG_L=\{\EE^{\pi}[(U_h^{*, M} - u_h -  W_{h+1}^{*, M})(s_h,a_h,b_h)], M\in\cM, h\in[H]\}$, where we define $U^{*, M}, W^{*, M}$ as the leader's optimistic U- and W-functions under $\pi^{M} = \argmax_{\pi\in\Pi}J(\pi, M)$ and $M$.
% and the Bellman optimality operator $\TT_h^{*, M}$ is defined the same way as \eqref{eq:def-TT-opt-neurips} where we only replace $T_{h+1}^{*, \theta}$ by $T_{h+1}^{*, M}$, which is just a change of notations.
The second function class is an analogy to the $\QRE$ in \eqref{eq:QRE} for farsighted follower defined as
% \ifneurips{\abovebelowskip{.5}{.5}
% \begin{align}\label{eq:GF1-neurips}
%     {\ts \cG_F^1 = \ocbr{(\EE_{s_h, b_h}^{\pi} - \EE_{s_h}^{\pi})\osbr{\sum_{l=h}^H \gamma^{l-h}\orbr{r_l^M- r_l + \gamma (P_l^{M} - P_h) V_{l+1}^{*, M}}(s_l, a_l, b_l)}}}, 
% \end{align}
% }\hspace{-5pt}\fi
\begin{align}\label{eq:GF1-neurips}
     \cG_F^1 = \cbr{(\EE_{s_h, b_h}^{\pi} - \EE_{s_h}^{\pi})\sbr{\sum_{l=h}^H \gamma^{l-h}\bigrbr{r_l^M- r_l + \gamma (P_l^{M} - P_h) V_{l+1}^{*, M}}(s_l, a_l, b_l)}}, 
\end{align}
for all $(\pi, M, h)\in(\Pi, \cM, [H])$ and $(s_h, b_h)\in\cS\times\cB$. Here, we let $\EE_{s_h, b_h}^\pi[\cdot] = \EE^{\pi, \nu^\pi}[\cdot\given s_h, b_h]$ and $V^{*, M}$ is the follower's optimistic V-function under $\pi^M$ and $M$. In particular, when we take $\gamma=0$ for myopic follower, \eqref{eq:GF1-neurips} reduces to the $\QRE$ defined in \eqref{eq:QRE}.
The last function class is unique for the farsighted case, which captures the second order term in the QRE as the follower's Bellman error, 
% \ifneurips{\abovebelowskip{.5}{.5}
% \begin{align*}
%     \cG_F^2=\ocbr{\EE_{s_h, b_h}^{\pi} \osbr{\orbr{r_h^M- r_h + \gamma (P_h^{M} - P_h) V_{l+1}^{*, M}}(s_h, a_h, b_h)}},
% \end{align*}
% }\hspace{-5pt}\fi
\ifmain
\begin{align*}
    \cG_F^2=\cbr{\EE_{s_h, b_h}^{\pi} \sbr{\bigrbr{r_h^M- r_h + \gamma (P_h^{M} - P_h) V_{l+1}^{*, M}}(s_h, a_h, b_h)}},
\end{align*}
\fi
for all $(\pi, M, h)\in(\Pi, \cM, [H])$ and $(s_h, b_h)\in\cS\times\cB$.
% Here, $V^{*, M}$ is the V-function defined in \eqref{eq:qv_pi_qr} but under $\pi^{*, M}$ and $M$. 
We denote by $\dim(\cG_L)$, $\dim(\cG_F^1)$, and $\dim(\cG_F^2)$ their eluder dimensions\footnote{See \Cref{sec:eluder farsighted} for definitions.} with properly selected parameters. 

% \begin{align*}
%     D_{\RL,h}^2 (M,  M^*;\pi) =   \EE^{\pi, M^*}D_\H^2\rbr{\nu_h^{\pi,M}, \nu_h^{\pi,M^*}} +
%     \EE^{\pi, M^*} D_\H^2(P_h^{M}, P_h^{M^*}) +
%     \EE^{\pi, M^*}\rbr{u_h^{M^*}-u_h^M}^2,
% \end{align*}
% and $P^{\pi, M}$ is the distribution of $(s_h,a_h,b_h)_{h\in[H]}$ under policy $\pi$ and model $M$.
\begin{theorem}[{Regret for OMLE}]\label{thm:OMLE-farsighted}
    We let $\beta\ge  \allowbreak 9\log(3e^2H T\cN_\rho(\cM,T^{-1})\delta^{-1})$, where $\cN_\rho(\cM,\epsilon)$ is the minimal size of an $\epsilon$-optimistic covering net of $\cM$, which is defined in \Cref{lem:MLE}. 
    Suppose $\la x, r_h^M(s_h, a_h, \cdot)\ra_\cB = \varsigma$ for all $M\in\cM$, $(s_h, a_h)\in\cS\times\cA$ and $h\in[H]$, and we let $\kappa = \nbr{x}_\infty/|\la x, \ind\ra_\cB|$.
    Then with probability at least $1-\delta$, we have for the OMLE algorithm that 
    \begin{align*}
        \Reg(T) \le H^2\sqrt{\dim(\cG_L) \beta T} + \eta C_\eta  H^2\eff_H(\gamma) \sqrt{\dim(\cG_F^1) \beta T} + H^2 C^{(2)} L^{(2)} \dim(\cG_F^2)\beta \log(T),
    \end{align*}
    where 
    $L^{(2)} = c H^2 \eff_H(c_2)^2 \kappa^2 \exp\rbr{8\eta B_A}  C_\eta^2$ for some universal constant $c>0$ and for $c_2 = \gamma(2\exp(2\eta B_A)+\kappa\exp(4\eta B_A))$, and $C^{(2)} = 2  \eta^2 H^2  \exp\orbr{6\eta B_A}  (1+4 \eff_H(\gamma)) \cdot \rbr{\eff_H(\exp(2\eta B_A)\gamma)}^2$.
    % \begin{align*}
    %     \EE^{\pi, M^*}\bigrbr{A_h^{\pi, M}-A_h^{\pi,M^*}}^2 \le  \abr{f_{2,h}^M(\pi)} \le L^{(2)} \max_{h\in[H]} D_{\RL,h}^2(M, M^*;\pi), \quad \forall h\in[H].
    % \end{align*}
    % holds for any $\pi\in\Pi$ and $M\in\cM$. The OMLE algorithm for the MDP with farsighted follower
    % achieves the following with probability at least $1-\delta$, 
    % \begin{align*}
    %     \Reg(T)&\le \cO\rbr{H^{2} \sqrt{d_0\beta T} + (1+\eta B_A) C_{\gamma, H} H^2\sqrt{d_1\beta T}} + \cO\rbr{\eta^2 H^2 C_{\gamma, H} \exp\rbr{2\eta B_A} L^{(2)} d_2 \beta \log T}
    % \end{align*}
    % where $
    %     C_{\gamma, H} =\rbr{1-\gamma^H}/\rbr{1-\gamma}
    % $ is the effective foresight for the follower.
    \begin{proof}
        See \Cref{sec:proof-farsighted MDP} for a detailed proof.
    \end{proof}
\end{theorem}

We remark that our theorem handles a wide range of MDP classes, e.g., linear Markov game in \Cref{def:linear MDP}, linear matrix MDP \citep{zhou2021provably}, or linear mixture MDP \citep{chen2022unified},
% , and low rank MDP \citep{agarwal2020flambe}, 
where we have
$\dim(\cG_L) = \dim(\cG_F^2) \lesssim d$ and $\dim(\cG_F^1)\lesssim Hd$. 
See \Cref{sec:eluder farsighted} for more details.
We also note that the first order terms (with $\sqrt T$ regret) only depends polynomially on $\eta, H$ and the exponential effect lies in the $\log(T)$ term, which indicates that we can handle a follower with more rationality. Additionally, in the best case we have $\kappa = |\cB|^{-1}$ by taking $x=\ind$ and how to relax this $\la x(\cdot), r_h^M(s_h, a_h, \cdot)\ra_{\cB} = \varsigma$ constraint leaves for future work.

% \todo{another error concerning the use of $D_\RL$, the definition should be corrected and check the dependence on $H$.}
% \todo{to be discussed.}
% Note that $\cF_{2,h}$ only enters the theoretical results without need of specification in the OMLE algorithm. 
% In the following, we apply the learning result in \Cref{thm:OMLE-farsighted} to the linear MDP case and specify the choice of $\cF_{2,h}$ when the follower's reward satisifes some linear constraint.
% \begin{corollary}[\textit{Online Learning for Linear MDP}]\label{cor:online linear}
% Consider a linear MDP under \Cref{def:linear MDP}. Suppose that the follower's reward at each state satisfies a linear constraint $\dotp{x}{r_h(s_h, a_h, \cdot)} = \varsigma$ for some $x:\cB\rightarrow \RR$ such that $\inp{\ind}{x}\neq 0$ and $\varsigma\in\RR$. Define ratio $\kappa = \nbr{x}_\infty/|\inp{x}{\ind}|$. Then, \Cref{alg:OMLE} achieves the following with probability at least $1-\delta$,
% \begin{proof}
%     See \Cref{sec:proof-OMLE-linear} for a detailed proof.
% \end{proof}
% \end{corollary}