\documentclass[11pt]{article}
\usepackage[OT1]{fontenc}
\usepackage{smile}
\renewcommand{\baselinestretch}{1.05}
%\usepackage{authblk}

%\renewcommand\Authsep{ }

%%%%%%%%%%%%-- Comments --%%%%%%%%%%%%
\newcommand{\Zhuoran}[1]{{\cosor{blue}[Zhuoran: #1]}}
\newcommand{\Siyu}[1]{{\color{purple}[Siyu: #1]}}
%%%%%%%%%%%%-- new command --%%%%%%%%%%%%
\def\est{{\rm est}}
\def\RL{{\rm RL}}
\def\H{{\rm H}}
\def\L{{\rm L}}
\def\tV{{\tilde V}}
\def\tQ{{\tilde Q}}
\def\tA{{\tilde A}}
\def\tQ{{\tilde Q}}
\def\tR{{\tilde R}}
\def\tT{{\tilde T}}
\def\tbQ{{\tilde\bQ}}
\def\tbV{{\tilde\bV}}
\def\tbR{{\tilde\bR}}
\def\tM{{\tilde M}}
\def\tnu{{\tilde \nu}}
\def\dimE{{\dim_{\rm E}}}
\def\opt{{\rm opt}}
\def\MLE{{\rm MLE}}
\DeclareMathOperator{\spn}{span}
\def\confset{{\cC}}
\def\eff{{\mathrm{eff}}}
\def\tmp{{\rm *todo*}}

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\newcommand{\Unif}{\pazocal{U}}
\def\CI{{\cC}}
\def\DE{{\rm DE}}
\def\E{{\rm E}}
% \def{\PI}{{\Pi}}
\def\QRE{{\mathtt{QRE}}}

\newif\ifmain
\maintrue
\newif\ifneurips
\neuripsfalse
%%%%%%%%%%%%-- start --%%%%%%%%%%%%%%%
\title{\huge Actions Speak What You Want:
Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks}

% \author{}
\author{Siyu Chen\thanks{Department of Statistics and Data Science, Yale University. Email: \texttt{siyu.chen.sc3226@yale.edu}.} \qquad Mengdi Wang\thanks{Department of Electrical and Computer Engineering, Princeton University. Email: \texttt{mengdiw@princeton.edu}.} \qquad Zhuoran Yang\thanks{Department of Statistics and Data Science, Yale University. Email: \texttt{zhuoran.yang@yale.edu}.}}

\iffalse
\affil[1]{
% \footnotesize 
\small
\textit{Department of Statistics and Data Science, Yale University}}
\affil[2]{
% \footnotesize 
\small
\textit{Center for Statistics and Machine Learning, Department of Electrical and Computer Engineering, Department of Computer Science, Princeton University}}
\fi 


\date{}
\begin{document}
\maketitle
\begin{abstract}
\iffalse
We investigate the challenge of learning a Quantal Stackelberg Equilibrium (QSE) in a leader-follower Markov game with episodic interactions, where the follower has bounded rationality. At the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization. The goal of the leader is to find a QSE, which involves determining her optimal policy that maximizes her total return in expectation, as well as the corresponding quantal response of the follower.
We propose reinforcement learning (RL) algorithms for learning QSE on behalf of the leader under both online and offline settings, where only the follower's actions are observable, but not their rewards. RL for QSE poses unique difficulties due to the need to learn the quantal response mapping from the follower's feedback data, as well as the bilevel optimization nature of Stackelberg games. However, we have successfully overcome these difficulties and developed sample-efficient RL algorithms that achieve sublinear suboptimality and regret bounds under offline and online settings, respectively, using general function approximation techniques.
Our algorithms rely on maximum likelihood estimation of the quantal response mapping and the principle of optimism/pessimism. Our technical analysis includes uncertainty quantification of the estimated quantal response via sublevel sets of the likelihood function, and a new performance difference lemma that links the performance of the leader's policy to the error incurred in learning the quantal response. To our knowledge, this is the first set of provably sample-efficient RL algorithms for solving a QSE without the leader observing the follower's reward.



END




    
   In particular, 
Each episode involves the leader committing to an $H$-step policy, followed by the follower selecting a logit quantal response, either myopically or nonmyopically.  
    We take into account information asymmetry that the follower's reward remains unobservable throughout the game
    , which requires the leader to infer the follower's quantal response model from the follower's past behaviors.
    For the case of a myopic follower,
    we develop sample-efficient RL algorithms based on Maximum Likelihood Estimator (MLE) for both offline and online solving a QSE strategy of the leader.
    Theoretical analysis shows that our algorithms achieve suboptimality (offline) and sublinear regret (online)
    using either general function approximation or linear function approximation. 
    For the case of a nonmyopic follower, we introduce model-based offline/online algorithms that are sample efficient under the condition that the leader can uniquely identify the follower's reward with certain prior knowledge. 
    To the best of our knowledge, we establish the first provably efficient RL algorithms for solving for QSEs without the leader observing the follower's reward. This problem is also closely related to reinforcement learning with human feedback (RLHF), and we address the unique challenge of unobservable rewards confronting inherent incentive misalignment within a Stackelberg game.
    \fi


    We study reinforcement learning (RL) for    learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. 
In specific, at the outset of the game, the leader announces her policy to the follower and commits to it.
The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. 
The goal of the leader is to find her optimal policy, which  yields the optimal expected total return, by interacting with the follower and learning from data. 
A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. 
We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. 
Our algorithms are based on (i) learning the quantal response model via maximum likelihood estimation and (ii) model-free or model-based RL for solving the leader's decision making problem, and we show that they achieve sublinear regret upper bounds. Moreover, we quantify the uncertainty of these estimators and leverage the uncertainty to implement optimistic and pessimistic algorithms for online and offline settings. 
Besides, when specialized to the linear and myopic setting, our algorithms are also computationally efficient. 
Our theoretical analysis features a novel performance-difference lemma which incorporates the error of quantal response model, which might be of independent interest.  
\end{abstract}

{
  \hypersetup{linkcolor=black}
  \tableofcontents
}
\newpage 
%%%%%%%%%%%%-- main content --%%%%%%%%%%
\input{intro.tex}
\input{model.tex}
\input{warmup.tex}
\input{myopic_offline.tex}
\input{myopic_online.tex}
\input{farsighted.tex}
\input{conclusion.tex}
%%%%%%%%%%%%-- reference --%%%%%%%%%%%
\newpage
\bibliographystyle{ims}
\bibliography{reference}

%%%%%%%%%%%% -- appendix -- %%%%%%%%%%%
\newpage 
\appendix

\input{appendix.tex}
\end{document}
