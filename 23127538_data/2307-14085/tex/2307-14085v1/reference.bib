@string{JASA = {Journal of the American Statistical Association}}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society, Series B}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{ICML  = {International Conference on Machine Learning}}
@string{COLT  = {Conference on Learning Theory}}
@string{ICLR  = {International Conference on Learning Representations}}
@string{TIT = {IEEE Transactions on Information Theory}}


@inproceedings{haarnoja2017reinforcement,
	title        = {Reinforcement learning with deep energy-based policies},
	author       = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {1352--1361},
	organization = {PMLR}
}
@misc{abbeel2010inverse,
	title        = {Inverse Reinforcement Learning.},
	author       = {Abbeel, Pieter and Ng, Andrew Y},
	year         = 2010
}
@article{chen2022unified,
	title        = {Unified algorithms for {RL} with decision-estimation coefficients: No-regret, pac, and reward-free learning},
	author       = {Chen, Fan and Mei, Song and Bai, Yu},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.11745}
}
@article{jin2021bellman,
	title        = {Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
	author       = {Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
	year         = 2021,
	journal      = {Advances in neural information processing systems},
	volume       = 34,
	pages        = {13406--13418}
}
@article{zhu2023principled,
	title        = {Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons},
	author       = {Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2301.11270}
}
@article{foster2021statistical,
	title        = {The statistical complexity of interactive decision making},
	author       = {Foster, Dylan J and Kakade, Sham M and Qian, Jian and Rakhlin, Alexander},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2112.13487}
}
@inproceedings{geist2019theory,
	title        = {A theory of regularized markov decision processes},
	author       = {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2160--2169},
	organization = {PMLR}
}
@article{ghosh2022provably,
	title        = {Provably Efficient Model-free {RL} in Leader-Follower MDP with Linear Function Approximation},
	author       = {Ghosh, Arnob},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2211.15792}
}
@article{zhong2021can,
	title        = {Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational Followers?},
	author       = {Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
	year         = 2023,
	journal      = {Journal of Machine Learning Research},
	volume       = 24,
	number       = 35,
	pages        = {1--52}
}
@inproceedings{shah2015estimation,
	title        = {Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence},
	author       = {Shah, Nihar and Balakrishnan, Sivaraman and Bradley, Joseph and Parekh, Abhay and Ramchandran, Kannan and Wainwright, Martin},
	year         = 2015,
	booktitle    = {Artificial intelligence and statistics},
	pages        = {856--865},
	organization = {PMLR}
}
@article{saaty2003magic,
	title        = {Why the magic number seven plus or minus two},
	author       = {Saaty, Thomas L and Ozdemir, Mujgan S},
	year         = 2003,
	journal      = {Mathematical and computer modelling},
	publisher    = {Elsevier},
	volume       = 38,
	number       = {3-4},
	pages        = {233--244}
}
@article{miller1956magical,
	title        = {The magical number seven, plus or minus two: Some limits on our capacity for processing information.},
	author       = {Miller, George A},
	year         = 1956,
	journal      = {Psychological review},
	publisher    = {American Psychological Association},
	volume       = 63,
	number       = 2,
	pages        = 81
}
@article{kiger1984depth,
	title        = {The depth/breadth trade-off in the design of menu-driven user interfaces},
	author       = {Kiger, John I},
	year         = 1984,
	journal      = {International journal of man-machine studies},
	publisher    = {Elsevier},
	volume       = 20,
	number       = 2,
	pages        = {201--213}
}
@inproceedings{jin2021pessimism,
	title        = {Is pessimism provably efficient for offline rl?},
	author       = {Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
	year         = 2021,
	booktitle    = ICML,
	pages        = {5084--5096},
	organization = {PMLR}
}
@article{nayyar2014common,
	title        = {The common-information approach to decentralized stochastic control},
	author       = {Nayyar, Ashutosh and Mahajan, Aditya and Teneketzis, Demosthenis},
	year         = 2014,
	journal      = {Information and Control in Networks},
	publisher    = {Springer},
	pages        = {123--156}
}
@inproceedings{jin2020provably,
	title        = {Provably efficient reinforcement learning with linear function approximation},
	author       = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
	year         = 2020,
	booktitle    = COLT,
	pages        = {2137--2143},
	organization = {PMLR}
}
@article{simon1990bounded,
	title        = {Bounded rationality},
	author       = {Simon, Herbert A},
	year         = 1990,
	journal      = {Utility and probability},
	publisher    = {Springer},
	pages        = {15--18}
}
@article{simon1955behavioral,
	title        = {A behavioral model of rational choice},
	author       = {Simon, Herbert A},
	year         = 1955,
	journal      = {The quarterly journal of economics},
	publisher    = {JSTOR},
	pages        = {99--118}
}
@article{mckelvey1995quantal,
	title        = {Quantal response equilibria for normal form games},
	author       = {McKelvey, Richard D and Palfrey, Thomas R},
	year         = 1995,
	journal      = {Games and economic behavior},
	publisher    = {Elsevier},
	volume       = 10,
	number       = 1,
	pages        = {6--38}
}
@book{bacsar1998dynamic,
	title        = {Dynamic noncooperative game theory},
	author       = {Ba{\c{s}}ar, Tamer and Olsder, Geert Jan},
	year         = 1998,
	publisher    = {SIAM}
}
@article{bradley1952rank,
	title        = {Rank analysis of incomplete block designs: I. The method of paired comparisons},
	author       = {Bradley, Ralph Allan and Terry, Milton E},
	year         = 1952,
	journal      = {Biometrika},
	publisher    = {JSTOR},
	volume       = 39,
	number       = {3/4},
	pages        = {324--345}
}
@article{plackett1975analysis,
	title        = {The analysis of permutations},
	author       = {Plackett, Robin L},
	year         = 1975,
	journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
	publisher    = {Oxford University Press},
	volume       = 24,
	number       = 2,
	pages        = {193--202}
}
@article{hajek2014minimax,
	title        = {Minimax-optimal inference from partial rankings},
	author       = {Hajek, Bruce and Oh, Sewoong and Xu, Jiaming},
	year         = 2014,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 27
}
@book{luce2012individual,
	title        = {Individual choice behavior: A theoretical analysis},
	author       = {Luce, R Duncan},
	year         = 2012,
	publisher    = {Courier Corporation}
}
@article{negahban2012iterative,
	title        = {Iterative ranking from pair-wise comparisons},
	author       = {Negahban, Sahand and Oh, Sewoong and Shah, Devavrat},
	year         = 2012,
	journal      = {Advances in neural information processing systems},
	volume       = 25
}
@inproceedings{zhong2022pessimistic,
	title        = {Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets},
	author       = {Zhong, Han and Xiong, Wei and Tan, Jiyuan and Wang, Liwei and Zhang, Tong and Wang, Zhaoran and Yang, Zhuoran},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning},
	pages        = {27117--27142},
	organization = {PMLR}
}
@article{xie2021bellman,
	title        = {Bellman-consistent pessimism for offline reinforcement learning},
	author       = {Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
	year         = 2021,
	journal      = {Advances in neural information processing systems},
	volume       = 34,
	pages        = {6683--6694}
}
@inproceedings{lyu2022pessimism,
	title        = {Pessimism meets VCG: Learning Dynamic Mechanism Design via Offline Reinforcement Learning},
	author       = {Lyu, Boxiang and Wang, Zhaoran and Kolar, Mladen and Yang, Zhuoran},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning},
	pages        = {14601--14638},
	organization = {PMLR}
}
@book{gyorfi2002distribution,
	title        = {A distribution-free theory of nonparametric regression},
	author       = {Gy{\"o}rfi, L{\'a}szl{\'o} and K{\"o}hler, Michael and Krzy{\.z}ak, Adam and Walk, Harro},
	year         = 2002,
	publisher    = {Springer},
	volume       = 1
}
@inproceedings{agarwal2014taming,
	title        = {Taming the monster: A fast and simple algorithm for contextual bandits},
	author       = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
	year         = 2014,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1638--1646},
	organization = {PMLR}
}
@article{abbasi2011improved,
	title        = {Improved algorithms for linear stochastic bandits},
	author       = {Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
	year         = 2011,
	journal      = {Advances in neural information processing systems},
	volume       = 24
}
@article{carpentier2020elliptical,
	title        = {The elliptical potential lemma revisited},
	author       = {Carpentier, Alexandra and Vernade, Claire and Abbasi-Yadkori, Yasin},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.10182}
}
@inproceedings{chen2022adaptive,
	title        = {Adaptive Model Design for Markov Decision Process},
	author       = {Chen, Siyu and Yang, Donglin and Li, Jiayang and Wang, Senmiao and Yang, Zhuoran and Wang, Zhaoran},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning},
	pages        = {3679--3700},
	organization = {PMLR}
}
@inproceedings{he2021logarithmic,
	title        = {Logarithmic regret for reinforcement learning with linear function approximation},
	author       = {He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {4171--4180},
	organization = {PMLR}
}
@article{kumar2020conservative,
	title        = {Conservative q-learning for offline reinforcement learning},
	author       = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {1179--1191}
}
@article{janner2019trust,
	title        = {When to trust your model: Model-based policy optimization},
	author       = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	volume       = 32
}
@inproceedings{fujimoto2018addressing,
	title        = {Addressing function approximation error in actor-critic methods},
	author       = {Fujimoto, Scott and Hoof, Herke and Meger, David},
	year         = 2018,
	booktitle    = {International conference on machine learning},
	pages        = {1587--1596},
	organization = {PMLR}
}
@inproceedings{pathak2017curiosity,
	title        = {Curiosity-driven exploration by self-supervised prediction},
	author       = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {2778--2787},
	organization = {PMLR}
}
@article{choi2018contingency,
	title        = {Contingency-aware exploration in reinforcement learning},
	author       = {Choi, Jongwook and Guo, Yijie and Moczulski, Marcin and Oh, Junhyuk and Wu, Neal and Norouzi, Mohammad and Lee, Honglak},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1811.01483}
}
@article{sutton1999policy,
	title        = {Policy gradient methods for reinforcement learning with function approximation},
	author       = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
	year         = 1999,
	journal      = {Advances in neural information processing systems},
	volume       = 12
}
@article{rigter2022rambo,
	title        = {Rambo-rl: Robust adversarial model-based offline reinforcement learning},
	author       = {Rigter, Marc and Lacerda, Bruno and Hawes, Nick},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.12581}
}
@article{bellemare2016unifying,
	title        = {Unifying count-based exploration and intrinsic motivation},
	author       = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	year         = 2016,
	journal      = {Advances in neural information processing systems},
	volume       = 29
}
@article{burda2018exploration,
	title        = {Exploration by random network distillation},
	author       = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.12894}
}
@inproceedings{haarnoja2018soft,
	title        = {Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
	author       = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year         = 2018,
	booktitle    = {International conference on machine learning},
	pages        = {1861--1870},
	organization = {PMLR}
}
@incollection{sutton1990integrated,
	title        = {Integrated architectures for learning, planning, and reacting based on approximating dynamic programming},
	author       = {Sutton, Richard S},
	year         = 1990,
	booktitle    = {Machine learning proceedings 1990},
	publisher    = {Elsevier},
	pages        = {216--224}
}
@article{wu2022bayesian,
	title        = {Bayesian Optimistic Optimization: Optimistic Exploration for Model-based Reinforcement Learning},
	author       = {Wu, Chenyang and Li, Tianci and Zhang, Zongzhang and Yu, Yang},
	year         = 2022,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {14210--14223}
}
@misc{aubret2019survey,
	title        = {A survey on intrinsic motivation in reinforcement learning},
	author       = {Arthur Aubret and Laetitia Matignon and Salima Hassas},
	year         = 2019,
	eprint       = {1908.06976},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{haarnoja2018off,
	title        = {Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
	author       = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year         = 2018,
	booktitle    = {Proceedings of the 35th International Conference on machine learning (ICML-18)},
	pages        = {1861--1870}
}
@inproceedings{todorov2012mujoco,
	title        = {Mujoco: A physics engine for model-based control},
	author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	year         = 2012,
	booktitle    = {2012 IEEE/RSJ international conference on intelligent robots and systems},
	pages        = {5026--5033},
	organization = {IEEE}
}
@article{chen2017ucb,
	title        = {Ucb exploration via q-ensembles},
	author       = {Chen, Richard Y and Sidor, Szymon and Abbeel, Pieter and Schulman, John},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1706.01502}
}
@article{kurutach2018model,
	title        = {Model-ensemble trust-region policy optimization},
	author       = {Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1802.10592}
}
@article{wiering2008ensemble,
	title        = {Ensemble algorithms in reinforcement learning},
	author       = {Wiering, Marco A and Van Hasselt, Hado},
	year         = 2008,
	journal      = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	publisher    = {IEEE},
	volume       = 38,
	number       = 4,
	pages        = {930--936}
}
@inproceedings{lee2021sunrise,
	title        = {Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning},
	author       = {Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {6131--6141},
	organization = {PMLR}
}
@inproceedings{xiong2022self,
	author       = {Xiong, Wei and Zhong, Han and Shi, Chengshuai and Shen, Cong and Zhang, Tong},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning},
	pages        = {24496--24523},
	organization = {PMLR}
}
@article{hu2022provable,
	title        = {Provable Sim-to-real Transfer in Continuous Domain with Partial Observations},
	author       = {Hu, Jiachen and Zhong, Han and Jin, Chi and Wang, Liwei},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.15598}
}
@article{uehara2022provably,
	title        = {Provably efficient reinforcement learning in partially observable dynamical systems},
	author       = {Uehara, Masatoshi and Sekhari, Ayush and Lee, Jason D and Kallus, Nathan and Sun, Wen},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.12020}
}
@article{lale2020regret,
	title        = {Regret minimization in partially observable linear quadratic control},
	author       = {Lale, Sahin and Azizzadenesheli, Kamyar and Hassibi, Babak and Anandkumar, Anima},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2002.00082}
}
@inproceedings{simchowitz2020improper,
	title        = {Improper learning for non-stochastic control},
	author       = {Simchowitz, Max and Singh, Karan and Hazan, Elad},
	year         = 2020,
	booktitle    = {Conference on Learning Theory},
	pages        = {3320--3436},
	organization = {PMLR}
}
@inproceedings{ishfaq2021randomized,
	title        = {Randomized Exploration in Reinforcement Learning with General Value Function Approximation},
	author       = {Ishfaq, Haque and Cui, Qiwen and Nguyen, Viet and Ayoub, Alex and Yang, Zhuoran and Wang, Zhaoran and Precup, Doina and Yang, Lin},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {4607--4616},
	organization = {PMLR}
}
@article{freedman1975tail,
	title        = {On tail probabilities for martingales},
	author       = {Freedman, David A},
	year         = 1975,
	journal      = {the Annals of Probability},
	publisher    = {JSTOR},
	pages        = {100--118}
}
@inproceedings{xie2020learning,
	title        = {Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium},
	author       = {Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
	year         = 2020,
	booktitle    = {Conference on learning theory},
	pages        = {3674--3682},
	organization = {PMLR}
}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society, Series B}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{ICML  = {International Conference on Machine Learning}}
@string{COLT  = {Conference on Learning Theory}}
@string{TIT = {IEEE Transactions on Information Theory}}
@article{liu2022partially,
	title        = {When Is Partially Observable Reinforcement Learning Not Scary?},
	author       = {Liu, Qinghua and Chung, Alan and Szepesv{\'a}ri, Csaba and Jin, Chi},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.08967}
}
@article{dann2021provably,
	title        = {A provably efficient model-free posterior sampling method for episodic reinforcement learning},
	author       = {Dann, Christoph and Mohri, Mehryar and Zhang, Tong and Zimmert, Julian},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {12040--12051}
}
@article{agarwal2022model,
	title        = {Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity},
	author       = {Agarwal, Alekh and Zhang, Tong},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.07659}
}
@inproceedings{sun2019model,
	title        = {Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches},
	author       = {Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
	year         = 2019,
	booktitle    = {Conference on learning theory},
	pages        = {2898--2933},
	organization = {PMLR}
}
@article{zhang2006,
	title        = {From $\varepsilon$-entropy to KL-entropy: Analysis of minimum information complexity density estimation},
	author       = {Zhang, Tong},
	year         = 2006,
	journal      = {The Annals of Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 34,
	number       = 5,
	pages        = {2180--2210}
}
@article{agarwal2020flambe,
	title        = {Flambe: Structural complexity and representation learning of low rank mdps},
	author       = {Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {20095--20107}
}
@inproceedings{du2021bilinear,
	title        = {Bilinear classes: A structural framework for provable generalization in rl},
	author       = {Du, Simon and Kakade, Sham and Lee, Jason and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2826--2836},
	organization = {PMLR}
}
@article{jaeger2000observable,
	title        = {Observable operator models for discrete stochastic time series},
	author       = {Jaeger, Herbert},
	year         = 2000,
	journal      = {Neural computation},
	publisher    = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
	volume       = 12,
	number       = 6,
	pages        = {1371--1398}
}
@article{jin2020sample,
	title        = {Sample-efficient reinforcement learning of undercomplete POMDPs},
	author       = {Jin, Chi and Kakade, Sham and Krishnamurthy, Akshay and Liu, Qinghua},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {18530--18539}
}
@article{littman2001predictive,
	title        = {Predictive representations of state},
	author       = {Littman, Michael and Sutton, Richard S},
	year         = 2001,
	journal      = {Advances in neural information processing systems},
	volume       = 14
}
@inproceedings{azar2017minimax,
	title        = {Minimax regret bounds for reinforcement learning},
	author       = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {263--272},
	organization = {PMLR}
}
@article{osband2014model,
	title        = {Model-based reinforcement learning and the eluder dimension},
	author       = {Osband, Ian and Van Roy, Benjamin},
	year         = 2014,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 27
}
@article{jin2018q,
	title        = {Is Q-learning provably efficient?},
	author       = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
	year         = 2018,
	journal      = {Advances in neural information processing systems},
	volume       = 31
}
@inproceedings{yang2019sample,
	title        = {Sample-optimal parametric q-learning using linearly additive features},
	author       = {Yang, Lin and Wang, Mengdi},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {6995--7004},
	organization = {PMLR}
}
@inproceedings{jiang@2017,
	title        = {Contextual Decision Processes with low {B}ellman rank are {PAC}-Learnable},
	author       = {Nan Jiang and Akshay Krishnamurthy and Alekh Agarwal and John Langford and Robert E. Schapire},
	year         = 2017,
	month        = {06--11 Aug},
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 70,
	pages        = {1704--1713}
}
@inproceedings{cai2020provably,
	title        = {Provably efficient exploration in policy optimization},
	author       = {Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1283--1294},
	organization = {PMLR}
}
@article{wang2020reinforcement,
	title        = {Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension},
	author       = {Wang, Ruosong and Salakhutdinov, Russ R and Yang, Lin},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {6123--6135}
}
@inproceedings{ayoub2020model,
	title        = {Model-based reinforcement learning with value-targeted regression},
	author       = {Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {463--474},
	organization = {PMLR}
}
@article{wang2019optimism,
	title        = {Optimism in reinforcement learning with generalized linear function approximation},
	author       = {Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1912.04136}
}
@inproceedings{du2019provably,
	title        = {Provably efficient RL with rich observations via latent state decoding},
	author       = {Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1665--1674},
	organization = {PMLR}
}
@article{efroni2022provable,
	title        = {Provable reinforcement learning with a short-term memory},
	author       = {Efroni, Yonathan and Jin, Chi and Krishnamurthy, Akshay and Miryoosefi, Sobhan},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2202.03983}
}
@article{zhan2022pac,
	title        = {Pac reinforcement learning for predictive state representations},
	author       = {Zhan, Wenhao and Uehara, Masatoshi and Sun, Wen and Lee, Jason D},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2207.05738}
}
@article{zhang2022feel,
	title        = {Feel-good thompson sampling for contextual bandits and reinforcement learning},
	author       = {Zhang, Tong},
	year         = 2022,
	journal      = {SIAM Journal on Mathematics of Data Science},
	publisher    = {SIAM},
	volume       = 4,
	number       = 2,
	pages        = {834--857}
}
@inproceedings{cai2022reinforcement,
	title        = {Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency},
	author       = {Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2485--2522},
	organization = {PMLR}
}
@article{wang2022embed,
	title        = {Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency},
	author       = {Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.13476}
}
@article{kwon2021rl,
	title        = {RL for latent MDPs: Regret guarantees and a lower bound},
	author       = {Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {24523--24534}
}
@inproceedings{xiong22b,
	title        = {A Self-Play Posterior Sampling Algorithm for Zero-Sum {M}arkov Games},
	author       = {Xiong, Wei and Zhong, Han and Shi, Chengshuai and Shen, Cong and Zhang, Tong},
	year         = 2022,
	month        = {17--23 Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {24496--24523}
}
@article{golowich2022learning,
	title        = {Learning in Observable POMDPs, without Computationally Intractable Oracles},
	author       = {Golowich, Noah and Moitra, Ankur and Rohatgi, Dhruv},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.03446}
}
@article{golowich2022planning,
	title        = {Planning in observable POMDPs in quasipolynomial time},
	author       = {Golowich, Noah and Moitra, Ankur and Rohatgi, Dhruv},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2201.04735}
}
@inproceedings{zhou2021nearly,
	title        = {Nearly minimax optimal reinforcement learning for linear mixture markov decision processes},
	author       = {Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
	year         = 2021,
	booktitle    = {Conference on Learning Theory},
	pages        = {4532--4576},
	organization = {PMLR}
}
@article{russo2013eluder,
	title        = {Eluder dimension and the sample complexity of optimistic exploration},
	author       = {Russo, Daniel and Van Roy, Benjamin},
	year         = 2013,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 26
}
@techreport{van2014probability,
	title        = {Probability in high dimension},
	author       = {Van Handel, Ramon},
	year         = 2014,
	institution  = {PRINCETON UNIV NJ}
}
@inproceedings{modi2020sample,
	title        = {Sample complexity of reinforcement learning using linearly combined model ensembles},
	author       = {Modi, Aditya and Jiang, Nan and Tewari, Ambuj and Singh, Satinder},
	year         = 2020,
	booktitle    = {International Conference on Artificial Intelligence and Statistics},
	pages        = {2010--2020},
	organization = {PMLR}
}
@article{krishnamurthy2016pac,
	title        = {PAC reinforcement learning with rich observations},
	author       = {Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
	year         = 2016,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 29
}
@inproceedings{munos2003error,
	title        = {Error bounds for approximate policy iteration},
	author       = {Munos, R{\'e}mi},
	year         = 2003,
	booktitle    = {ICML},
	volume       = 3,
	pages        = {560--567}
}
@inproceedings{zanette2020learning,
	title        = {Learning near optimal policies with low inherent bellman error},
	author       = {Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {10978--10989},
	organization = {PMLR}
}
@book{sutton2018reinforcement,
	title        = {Reinforcement learning: An introduction},
	author       = {Sutton, Richard S and Barto, Andrew G},
	year         = 2018
}
@book{puterman2014markov,
	title        = {Markov decision processes: discrete stochastic dynamic programming},
	author       = {Puterman, Martin L},
	year         = 2014,
	publisher    = {John Wiley \& Sons}
}
@article{auer2002finite,
	title        = {Finite-time analysis of the multiarmed bandit problem},
	author       = {Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
	year         = 2002,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 47,
	number       = 2,
	pages        = {235--256}
}
@article{jaksch2010near,
	title        = {Near-optimal Regret Bounds for Reinforcement Learning},
	author       = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	year         = 2010,
	journal      = {Journal of Machine Learning Research},
	volume       = 11,
	pages        = {1563--1600}
}
@article{auer2002using,
	title        = {Using confidence bounds for exploitation-exploration trade-offs},
	author       = {Auer, Peter},
	year         = 2002,
	journal      = {Journal of Machine Learning Research},
	volume       = 3,
	number       = {Nov},
	pages        = {397--422}
}
@article{russo2014learning,
	title        = {Learning to optimize via posterior sampling},
	author       = {Russo, Daniel and Van Roy, Benjamin},
	year         = 2014,
	journal      = {Mathematics of Operations Research},
	publisher    = {INFORMS},
	volume       = 39,
	number       = 4,
	pages        = {1221--1243}
}
@article{thompson1933likelihood,
	title        = {On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
	author       = {Thompson, William R},
	year         = 1933,
	journal      = {Biometrika},
	publisher    = {Oxford University Press},
	volume       = 25,
	number       = {3-4},
	pages        = {285--294}
}
@article{auer2008near,
	title        = {Near-optimal regret bounds for reinforcement learning},
	author       = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
	year         = 2008,
	journal      = {Advances in neural information processing systems},
	volume       = 21
}
@inproceedings{jiang2017contextual,
	title        = {Contextual decision processes with low bellman rank are PAC-learnable},
	author       = {Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1704--1713},
	organization = {PMLR}
}
@article{agrawal2017optimistic,
	title        = {Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
	author       = {Agrawal, Shipra and Jia, Randy},
	year         = 2017,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 30
}
@inproceedings{strens2000bayesian,
	title        = {A Bayesian framework for reinforcement learning},
	author       = {Strens, Malcolm},
	year         = 2000,
	booktitle    = ICML,
	volume       = 2000,
	pages        = {943--950}
}
@inproceedings{osband2016generalization,
	title        = {Generalization and exploration via randomized value functions},
	author       = {Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
	year         = 2016,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2377--2386},
	organization = {PMLR}
}
@article{russo2019worst,
	title        = {Worst-case regret bounds for exploration via randomized value functions},
	author       = {Russo, Daniel},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@article{agarwal2022non,
	title        = {Non-Linear Reinforcement Learning in Large Action Spaces: Structural Conditions and Sample-efficiency of Posterior Sampling},
	author       = {Agarwal, Alekh and Zhang, Tong},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2203.08248}
}
@article{kakade2020information,
	title        = {Information theoretic regret bounds for online nonlinear control},
	author       = {Kakade, Sham and Krishnamurthy, Akshay and Lowrey, Kendall and Ohnishi, Motoya and Sun, Wen},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {15312--15325}
}
@article{singh2012predictive,
	title        = {Predictive state representations: A new theory for modeling dynamical systems},
	author       = {Singh, Satinder and James, Michael and Rudary, Matthew},
	year         = 2012,
	journal      = {arXiv preprint arXiv:1207.4167}
}
@inproceedings{guo2016pac,
	title        = {A pac rl algorithm for episodic pomdps},
	author       = {Guo, Zhaohan Daniel and Doroudi, Shayan and Brunskill, Emma},
	year         = 2016,
	booktitle    = {Artificial Intelligence and Statistics},
	pages        = {510--518},
	organization = {PMLR}
}
@article{xiong2021sublinear,
	title        = {Sublinear regret for learning pomdps},
	author       = {Xiong, Yi and Chen, Ningyuan and Gao, Xuefeng and Zhou, Xiang},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.03635}
}
@inproceedings{azizzadenesheli2016reinforcement,
	title        = {Reinforcement learning of POMDPs using spectral methods},
	author       = {Azizzadenesheli, Kamyar and Lazaric, Alessandro and Anandkumar, Animashree},
	year         = 2016,
	booktitle    = {Conference on Learning Theory},
	pages        = {193--256},
	organization = {PMLR}
}
@article{papadimitriou1987complexity,
	title        = {The complexity of Markov decision processes},
	author       = {Papadimitriou, Christos H and Tsitsiklis, John N},
	year         = 1987,
	journal      = {Mathematics of operations research},
	publisher    = {INFORMS},
	volume       = 12,
	number       = 3,
	pages        = {441--450}
}
@article{uehara2022computationally,
	title        = {Computationally efficient pac rl in pomdps with latent determinism and conditional embeddings},
	author       = {Uehara, Masatoshi and Sekhari, Ayush and Lee, Jason D and Kallus, Nathan and Sun, Wen},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.12081}
}
@article{jiang2018completing,
	title        = {Completing State representations using spectral learning},
	author       = {Jiang, Nan and Kulesza, Alex and Singh, Satinder},
	year         = 2018,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 31
}
@article{hefny2015supervised,
	title        = {Supervised learning for dynamical system learning},
	author       = {Hefny, Ahmed and Downey, Carlton and Gordon, Geoffrey J},
	year         = 2015,
	journal      = {Advances in neural information processing systems},
	volume       = 28
}
@inproceedings{zhang2021reinforcement,
	title        = {Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory},
	author       = {Zhang, Zhi and Yang, Zhuoran and Liu, Han and Tokekar, Pratap and Huang, Furong},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@article{brafman2002r,
	title        = {R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
	author       = {Brafman, Ronen I and Tennenholtz, Moshe},
	year         = 2002,
	journal      = {Journal of Machine Learning Research},
	volume       = 3,
	number       = {Oct},
	pages        = {213--231}
}
@article{boots2011closing,
	title        = {Closing the learning-planning loop with predictive state representations},
	author       = {Boots, Byron and Siddiqi, Sajid M and Gordon, Geoffrey J},
	year         = 2011,
	journal      = {The International Journal of Robotics Research},
	publisher    = {SAGE Publications Sage UK: London, England},
	volume       = 30,
	number       = 7,
	pages        = {954--966}
}
@article{dann2017unifying,
	title        = {Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
	author       = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
	year         = 2017,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 30
}
@article{osband2016deep,
	title        = {Deep exploration via bootstrapped DQN},
	author       = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	year         = 2016,
	journal      = {Advances in neural information processing systems},
	volume       = 29
}
@article{lu2017ensemble,
	title        = {Ensemble sampling},
	author       = {Lu, Xiuyuan and Van Roy, Benjamin},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@article{chua2018deep,
	title        = {Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
	author       = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
	year         = 2018,
	journal      = {Advances in neural information processing systems},
	volume       = 31
}
@inproceedings{nagabandi2020deep,
	title        = {Deep dynamics models for learning dexterous manipulation},
	author       = {Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
	year         = 2020,
	booktitle    = {Conference on Robot Learning},
	pages        = {1101--1112},
	organization = {PMLR}
}
@inproceedings{welling2011bayesian,
	title        = {Bayesian learning via stochastic gradient Langevin dynamics},
	author       = {Welling, Max and Teh, Yee W},
	year         = 2011,
	booktitle    = {Proceedings of the 28th international conference on machine learning (ICML-11)},
	pages        = {681--688}
}
@article{zhang2022mathematical,
	title        = {Mathematical Analysis of Machine Learning Algorithms},
	author       = {Zhang, Tong},
	year         = 2022,
	journal      = {To Appear},
	url          = {http://tongzhang-ml.org/lt-book.html}
}
@article{azuma1967weighted,
	title        = {Weighted sums of certain dependent random variables},
	author       = {Azuma, Kazuoki},
	year         = 1967,
	journal      = {Tohoku Mathematical Journal, Second Series},
	publisher    = {Mathematical Institute, Tohoku University},
	volume       = 19,
	number       = 3,
	pages        = {357--367}
}
@article{chen2022partially,
	title        = {Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms},
	author       = {Chen, Fan and Bai, Yu and Mei, Song},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.14990}
}
@article{liu2022optimistic,
	title        = {Optimistic MLE--A Generic Model-based Algorithm for Partially Observable Sequential Decision Making},
	author       = {Liu, Qinghua and Netrapalli, Praneeth and Szepesvari, Csaba and Jin, Chi},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.14997}
}
@article{chen2022general,
	title        = {A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning},
	author       = {Chen, Zixiang and Li, Chris Junchi and Yuan, Angela and Gu, Quanquan and Jordan, Michael I},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.15634}
}
@article{dani2008stochastic,
	title        = {Stochastic linear optimization under bandit feedback},
	author       = {Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
	year         = 2008
}
@article{rusmevichientong2010linearly,
	title        = {Linearly parameterized bandits},
	author       = {Rusmevichientong, Paat and Tsitsiklis, John N},
	year         = 2010,
	journal      = {Mathematics of Operations Research},
	publisher    = {INFORMS},
	volume       = 35,
	number       = 2,
	pages        = {395--411}
}
@inproceedings{kakade2002approximate,
	title        = {Approximately Optimal Approximate Reinforcement Learning},
	author       = {Kakade, Sham and Langford, John},
	year         = 2002,
	booktitle    = {Proceedings of the Nineteenth International Conference on Machine Learning},
	series       = {ICML '02},
	pages        = {267–274},
	numpages     = 8
}
@inproceedings{wu2022nearly,
	title        = {Nearly optimal policy optimization with stable at any time guarantee},
	author       = {Wu, Tianhao and Yang, Yunchang and Zhong, Han and Wang, Liwei and Du, Simon and Jiao, Jiantao},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning},
	pages        = {24243--24265},
	organization = {PMLR}
}
@inproceedings{zanette2019tighter,
	title        = {Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
	author       = {Zanette, Andrea and Brunskill, Emma},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {7304--7312},
	organization = {PMLR}
}
@article{li2021breaking,
	title        = {Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning},
	author       = {Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {17762--17776}
}
@inproceedings{zhang2021isreinforcement,
	title        = {Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
	author       = {Zhang, Zihan and Ji, Xiangyang and Du, Simon},
	year         = 2021,
	booktitle    = {Conference on Learning Theory},
	pages        = {4528--4531},
	organization = {PMLR}
}

@article{zhang2021multi,
  title     = {Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author    = {Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal   = {Handbook of reinforcement learning and control},
  pages     = {321--384},
  year      = {2021},
  publisher = {Springer}
}


@inproceedings{zhang2022horizon,
	title        = {Horizon-free reinforcement learning in polynomial time: the power of stationary policies},
	author       = {Zhang, Zihan and Ji, Xiangyang and Du, Simon},
	year         = 2022,
	booktitle    = {Conference on Learning Theory},
	pages        = {3858--3904},
	organization = {PMLR}
}
@inproceedings{menard2021ucb,
	title        = {UCB Momentum Q-learning: Correcting the bias without forgetting},
	author       = {M{\'e}nard, Pierre and Domingues, Omar Darwiche and Shang, Xuedong and Valko, Michal},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {7609--7618},
	organization = {PMLR}
}
@article{wang2021exponential,
	title        = {An exponential lower bound for linearly realizable mdp with constant suboptimality gap},
	author       = {Wang, Yuanhao and Wang, Ruosong and Kakade, Sham},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {9521--9533}
}
@article{zhong2022posterior,
	title        = {A posterior sampling framework for interactive decision making},
	author       = {Zhong, Han and Xiong, Wei and Zheng, Sirui and Wang, Liwei and Wang, Zhaoran and Yang, Zhuoran and Zhang, Tong},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2211.01962}
}
@article{foster2022note,
	title        = {A Note on Model-Free Reinforcement Learning with the Decision-Estimation Coefficient},
	author       = {Foster, Dylan J and Golowich, Noah and Qian, Jian and Rakhlin, Alexander and Sekhari, Ayush},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2211.14250}
}
@book{lattimore2020bandit,
	title        = {Bandit algorithms},
	author       = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	year         = 2020,
	publisher    = {Cambridge University Press}
}
@inproceedings{li2010contextual,
	title        = {A contextual-bandit approach to personalized news article recommendation},
	author       = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
	year         = 2010,
	booktitle    = {Proceedings of the 19th international conference on World wide web},
	pages        = {661--670}
}
@inproceedings{agrawal2013thompson,
	title        = {Thompson sampling for contextual bandits with linear payoffs},
	author       = {Agrawal, Shipra and Goyal, Navin},
	year         = 2013,
	booktitle    = {International conference on machine learning},
	pages        = {127--135},
	organization = {PMLR}
}
@article{russo2016information,
	title        = {An information-theoretic analysis of thompson sampling},
	author       = {Russo, Daniel and Van Roy, Benjamin},
	year         = 2016,
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	volume       = 17,
	number       = 1,
	pages        = {2442--2471}
}
@article{silver2016mastering,
	title        = {Mastering the game of Go with deep neural networks and tree search},
	author       = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	year         = 2016,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 529,
	number       = 7587,
	pages        = {484--489}
}
@article{schulman2017proximal,
	title        = {Proximal policy optimization algorithms},
	author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1707.06347}
}
@article{mnih2013playing,
	title        = {Playing atari with deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.5602}
}
@article{bai2020near,
	title        = {Near-Optimal Reinforcement Learning with Self-Play},
	author       = {Bai, Yu and Jin, Chi and Yu, Tiancheng},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.12007},
	volume       = 33,
	pages        = {2159--2170}
}
@inproceedings{bai2020provable,
	title        = {Provable self-play algorithms for competitive reinforcement learning},
	author       = {Bai, Yu and Jin, Chi},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {551--560},
	organization = {PMLR}
}
@article{chen2021almost,
	title        = {Almost Optimal Algorithms for Two-player {M}arkov Games with Linear Function Approximation},
	author       = {Chen, Zixiang and Zhou, Dongruo and Gu, Quanquan},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2102.07404}
}
@article{huang2021towards,
	title        = {Towards general function approximation in zero-sum markov games},
	author       = {Huang, Baihe and Lee, Jason D and Wang, Zhaoran and Yang, Zhuoran},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.14702}
}
@article{jin2021power,
	title        = {The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces},
	author       = {Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2106.03352}
}
@article{liu2020sharp,
	title        = {A Sharp Analysis of Model-based Reinforcement Learning with Self-Play},
	author       = {Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.01604}
}
@inproceedings{zanette2020frequentist,
	title        = {Frequentist regret bounds for randomized least-squares value iteration},
	author       = {Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
	year         = 2020,
	booktitle    = {International Conference on Artificial Intelligence and Statistics},
	pages        = {1954--1964},
	organization = {PMLR}
}
@article{lopes2012exploration,
	title        = {Exploration in model-based reinforcement learning by empirically estimating learning progress},
	author       = {Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-Yves},
	year         = 2012,
	journal      = {Advances in neural information processing systems},
	volume       = 25
}
@article{houthooft2016vime,
	title        = {Vime: Variational information maximizing exploration},
	author       = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	year         = 2016,
	journal      = {Advances in neural information processing systems},
	volume       = 29
}
@article{mohamed2015variational,
	title        = {Variational information maximisation for intrinsically motivated reinforcement learning},
	author       = {Mohamed, Shakir and Jimenez Rezende, Danilo},
	year         = 2015,
	journal      = {Advances in neural information processing systems},
	volume       = 28
}
@article{stadie2015incentivizing,
	title        = {Incentivizing exploration in reinforcement learning with deep predictive models},
	author       = {Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
	year         = 2015,
	journal      = {arXiv preprint arXiv:1507.00814}
}
@article{zha2021rank,
	title        = {Rank the episodes: A simple approach for exploration in procedurally-generated environments},
	author       = {Zha, Daochen and Ma, Wenye and Yuan, Lei and Hu, Xia and Liu, Ji},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2101.08152}
}
@inproceedings{yu2020meta,
	title        = {Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
	author       = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
	year         = 2020,
	booktitle    = {Conference on robot learning},
	pages        = {1094--1100},
	organization = {PMLR}
}
@article{wang2023breaking,
	title        = {Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation},
	author       = {Wang, Yuanhao and Liu, Qinghua and Bai, Yu and Jin, Chi},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.06606}
}
@article{song2021can,
	title        = {When can we learn general-sum Markov games with a large number of players sample-efficiently?},
	author       = {Song, Ziang and Mei, Song and Bai, Yu},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2110.04184}
}
@article{song2022sample,
	title        = {Sample-efficient learning of correlated equilibria in extensive-form games},
	author       = {Song, Ziang and Mei, Song and Bai, Yu},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.07223}
}
@inproceedings{tian2021online,
	title        = {Online learning in unknown markov games},
	author       = {Tian, Yi and Wang, Yuanhao and Yu, Tiancheng and Sra, Suvrit},
	year         = 2021,
	booktitle    = {International conference on machine learning},
	pages        = {10279--10288},
	organization = {PMLR}
}
@article{liu2022sample,
	title        = {Sample-efficient reinforcement learning of partially observable markov games},
	author       = {Liu, Qinghua and Szepesv{\'a}ri, Csaba and Jin, Chi},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.01315}
}
@article{vinyals2019grandmaster,
	title        = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
	author       = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
	year         = 2019,
	journal      = {Nature},
	publisher    = {Nature Publishing Group UK London},
	volume       = 575,
	number       = 7782,
	pages        = {350--354}
}
@article{gao2019batched,
	title        = {Batched multi-armed bandits problem},
	author       = {Gao, Zijun and Han, Yanjun and Ren, Zhimei and Zhou, Zhengqing},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@article{perchet2016batched,
	title        = {Batched bandit problems},
	author       = {Perchet, Vianney and Rigollet, Philippe and Chassang, Sylvain and Snowberg, Erik},
	year         = 2016,
	journal      = {The Annals of Statistics},
	publisher    = {JSTOR},
	pages        = {660--681}
}
@article{qiao2023logarithmic,
	title        = {Logarithmic Switching Cost in Reinforcement Learning beyond Linear MDPs},
	author       = {Qiao, Dan and Yin, Ming and Wang, Yu-Xiang},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.12456}
}
@article{han2020sequential,
	title        = {Sequential batch learning in finite-action linear contextual bandits},
	author       = {Han, Yanjun and Zhou, Zhengqing and Zhou, Zhengyuan and Blanchet, Jose and Glynn, Peter W and Ye, Yinyu},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.06321}
}
@inproceedings{quanquangu2021linearlowcost,
	title        = {Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints},
	author       = {Wang, Tianhao and Zhou, Dongruo and Gu, Quanquan},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 34,
	pages        = {13524--13536},
	url          = {https://proceedings.neurips.cc/paper/2021/file/70a32110fff0f26d301e58ebbca9cb9f-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@article{zhang2020lowswitchcosttabular,
	title        = {Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition},
	author       = {Zihan Zhang and Yuan Zhou and Xiangyang Ji},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2004.10019},
	url          = {https://arxiv.org/abs/2004.10019},
	eprinttype   = {arXiv},
	eprint       = {2004.10019},
	timestamp    = {Wed, 10 Jun 2020 14:36:51 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2004-10019.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{baiyu2019lowswitchcosttabular,
	title        = {Provably Efficient Q-Learning with Low Switching Cost},
	author       = {Yu Bai and Tengyang Xie and Nan Jiang and Yu{-}Xiang Wang},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
	pages        = {8002--8011},
	url          = {https://proceedings.neurips.cc/paper/2019/hash/473803f0f2ebd77d83ee60daaa61f381-Abstract.html},
	editor       = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
	timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/BaiXJW19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{zhuoranyang2022lowcosteluder,
	title        = {The Best of Both Worlds: Reinforcement Learning with Logarithmic Regret and Policy Switches},
	author       = {Grigoris Velegkas and Zhuoran Yang and Amin Karbasi},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2203.01491},
	doi          = {10.48550/arXiv.2203.01491},
	url          = {https://doi.org/10.48550/arXiv.2203.01491},
	eprinttype   = {arXiv},
	eprint       = {2203.01491},
	timestamp    = {Wed, 16 Mar 2022 16:39:52 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2203-01491.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{chen2022abc,
	title        = {A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning},
	author       = {Zixiang Chen and Chris Junchi Li and Angela Yuan and Quanquan Gu and Michael I. Jordan},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2209.15634},
	doi          = {10.48550/arXiv.2209.15634},
	url          = {https://doi.org/10.48550/arXiv.2209.15634},
	eprinttype   = {arXiv},
	eprint       = {2209.15634},
	timestamp    = {Thu, 06 Oct 2022 14:41:30 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2209-15634.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{zhong2022gec,
	title        = {{GEC:} {A} Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond},
	author       = {Han Zhong and Wei Xiong and Sirui Zheng and Liwei Wang and Zhaoran Wang and Zhuoran Yang and Tong Zhang},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2211.01962},
	doi          = {10.48550/arXiv.2211.01962},
	url          = {https://doi.org/10.48550/arXiv.2211.01962},
	eprinttype   = {arXiv},
	eprint       = {2211.01962},
	timestamp    = {Fri, 02 Dec 2022 12:01:21 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2211-01962.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{gao2021lowcostlinear,
	title        = {A provably efficient algorithm for linear markov decision process with low switching cost},
	author       = {Gao, Minbo and Xie, Tianle and Du, Simon S and Yang, Lin F},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2101.00494}
}
@inproceedings{jin2022power,
	title        = {The power of exploiter: Provable multi-agent rl in large state spaces},
	author       = {Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning},
	pages        = {10251--10279},
	organization = {PMLR}
}
@article{gu2021batched,
	title        = {Batched neural bandits},
	author       = {Gu, Quanquan and Karbasi, Amin and Khosravi, Khashayar and Mirrokni, Vahab and Zhou, Dongruo},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2102.13028}
}
@article{zhong2023SNE,
	title        = {Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational Followers?},
	author       = {Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
	year         = 2023,
	journal      = {Journal of Machine Learning Research},
	volume       = 24,
	number       = 35,
	pages        = {1--52}
}
@article{huang2021markovgamegeneral,
	title        = {Towards general function approximation in zero-sum markov games},
	author       = {Huang, Baihe and Lee, Jason D and Wang, Zhaoran and Yang, Zhuoran},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.14702}
}
@article{bradtke1992reinforcement,
	title        = {Reinforcement learning applied to linear quadratic regulation},
	author       = {Bradtke, Steven},
	year         = 1992,
	journal      = {Advances in neural information processing systems},
	volume       = 5
}
@inproceedings{dong2020banditlowcost,
	title        = {Multinomial logit bandit with low switching cost},
	author       = {Dong, Kefan and Li, Yingkai and Zhang, Qin and Zhou, Yuan},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2607--2615},
	organization = {PMLR}
}
@article{kong2021eluderlowcost,
	title        = {Online Sub-Sampling for Reinforcement Learning with General Function Approximation},
	author       = {Dingwen Kong and Ruslan Salakhutdinov and Ruosong Wang and Lin F. Yang},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2106.07203},
	url          = {https://arxiv.org/abs/2106.07203},
	eprinttype   = {arXiv},
	eprint       = {2106.07203},
	timestamp    = {Wed, 16 Jun 2021 10:42:19 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2106-07203.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{foster2023complexity,
	title        = {On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring},
	author       = {Foster, Dylan J and Foster, Dean P and Golowich, Noah and Rakhlin, Alexander},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.00684}
}
@article{io,
	title        = {One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration},
	author       = {Anonymous},
	year         = 2023
}
@article{ni2022representation,
	title        = {Representation Learning for General-sum Low-rank Markov Games},
	author       = {Ni, Chengzhuo and Song, Yuda and Zhang, Xuezhou and Jin, Chi and Wang, Mengdi},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.16976}
}
@article{babichenko2016query,
	title        = {Query complexity of approximate Nash equilibria},
	author       = {Babichenko, Yakov},
	year         = 2016,
	journal      = {Journal of the ACM (JACM)},
	publisher    = {ACM New York, NY, USA},
	volume       = 63,
	number       = 4,
	pages        = {1--24}
}
@article{daskalakis2013complexity,
	title        = {On the complexity of approximating a Nash equilibrium},
	author       = {Daskalakis, Constantinos},
	year         = 2013,
	journal      = {ACM Transactions on Algorithms (TALG)},
	publisher    = {ACM New York, NY, USA},
	volume       = 9,
	number       = 3,
	pages        = {1--35}
}
@article{bai2021sample,
	title        = {Sample-efficient learning of stackelberg equilibria in general-sum games},
	author       = {Bai, Yu and Jin, Chi and Wang, Huan and Xiong, Caiming},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {25799--25811}
}
@article{zhang2020model,
	title        = {Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity},
	author       = {Zhang, Kaiqing and Kakade, Sham and Basar, Tamer and Yang, Lin},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {1166--1178}
}
@article{daskalakis2022complexity,
	title        = {The complexity of markov equilibrium in stochastic games},
	author       = {Daskalakis, Constantinos and Golowich, Noah and Zhang, Kaiqing},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.03991}
}
@inproceedings{liu2021sharp,
	title        = {A sharp analysis of model-based reinforcement learning with self-play},
	author       = {Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {7001--7010},
	organization = {PMLR}
}
@article{wei2017online,
	title        = {Online reinforcement learning in stochastic games},
	author       = {Wei, Chen-Yu and Hong, Yi-Te and Lu, Chi-Jen},
	year         = 2017,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 30
}
@article{cui2023breaking,
	title        = {Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation},
	author       = {Cui, Qiwen and Zhang, Kaiqing and Du, Simon S},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.03673}
}
@incollection{littman1994markov,
	title        = {Markov games as a framework for multi-agent reinforcement learning},
	author       = {Littman, Michael L},
	year         = 1994,
	booktitle    = {Machine learning proceedings 1994},
	publisher    = {Elsevier},
	pages        = {157--163}
}
@article{brown2018superhuman,
	title        = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
	author       = {Brown, Noam and Sandholm, Tuomas},
	year         = 2018,
	journal      = {Science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 359,
	number       = 6374,
	pages        = {418--424}
}
@article{jin2021v,
	title        = {V-Learning--A Simple, Efficient, Decentralized Algorithm for Multiagent RL},
	author       = {Jin, Chi and Liu, Qinghua and Wang, Yuanhao and Yu, Tiancheng},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2110.14555}
}
@article{rubinstein2017settling,
	title        = {Settling the complexity of computing approximate two-player Nash equilibria},
	author       = {Rubinstein, Aviad},
	year         = 2017,
	journal      = {ACM SIGecom Exchanges},
	publisher    = {ACM New York, NY, USA},
	volume       = 15,
	number       = 2,
	pages        = {45--49}
}
@article{yang2020provably,
	title        = {Provably efficient reinforcement learning with kernel and neural function approximations},
	author       = {Yang, Zhuoran and Jin, Chi and Wang, Zhaoran and Wang, Mengdi and Jordan, Michael},
	year         = 2020,
	journal      = NIPS,
	volume       = 33,
	pages        = {13903--13916}
}
@article{liu2022provably,
	title        = {Provably efficient kernelized q-learning},
	author       = {Liu, Shuang and Su, Hao},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2204.10349}
}
@article{foster2023tight,
	title        = {Tight Guarantees for Interactive Decision Making with the Decision-Estimation Coefficient},
	author       = {Foster, Dylan J and Golowich, Noah and Han, Yanjun},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2301.08215}
}
@article{erez2022regret,
	title        = {Regret minimization and convergence to equilibria in general-sum markov games},
	author       = {Erez, Liad and Lancewicki, Tal and Sherman, Uri and Koren, Tomer and Mansour, Yishay},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2207.14211}
}
@article{mao2021decentralized,
	title        = {Decentralized cooperative multi-agent reinforcement learning with exploration},
	author       = {Mao, Weichao and Ba{\c{s}}ar, Tamer and Yang, Lin F and Zhang, Kaiqing},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2110.05707}
}
@article{mao2023provably,
	title        = {Provably efficient reinforcement learning in decentralized general-sum Markov games},
	author       = {Mao, Weichao and Ba{\c{s}}ar, Tamer},
	year         = 2023,
	journal      = {Dynamic Games and Applications},
	publisher    = {Springer},
	volume       = 13,
	number       = 1,
	pages        = {165--186}
}
@inproceedings{songcan,
	title        = {When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?},
	author       = {Song, Ziang and Mei, Song and Bai, Yu},
	booktitle    = {International Conference on Learning Representations},
	year={2021}
}
@article{zhan2022decentralized,
	title        = {Decentralized Optimistic Hyperpolicy Mirror Descent: Provably No-Regret Learning in Markov Games},
	author       = {Zhan, Wenhao and Lee, Jason D and Yang, Zhuoran},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.01588}
}

 @inproceedings{kao2022decentralized,
  title={Decentralized cooperative reinforcement learning with hierarchical information structure},
  author={Kao, Hsu and Wei, Chen-Yu and Subramanian, Vijay},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={573--605},
  year={2022},
  organization={PMLR}
}


@article{lauffer2022no,
  title={No-Regret Learning in Dynamic Stackelberg Games},
  author={Lauffer, Niklas and Ghasemi, Mahsa and Hashemi, Abolfazl and Savas, Yagiz and Topcu, Ufuk},
  journal={arXiv preprint arXiv:2202.04786},
  year={2022}
}


@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}


@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}



@inproceedings{liu2022learning,
  title={Learning {M}arkov games with adversarial opponents: Efficient algorithms and fundamental limits},
  author={Liu, Qinghua and Wang, Yuanhao and Jin, Chi},
  booktitle={International Conference on Machine Learning},
  pages={14036--14053},
  year={2022},
  organization={PMLR}
}
 
@article{zhao2023online,
  title={Online Learning in Stackelberg Games with an Omniscient Follower},
  author={Zhao, Geng and Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I},
  journal={arXiv preprint arXiv:2301.11518},
  year={2023}
}


 


@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11702--11716},
  year={2021}
}

@article{cui2022provably,
  title={Provably efficient offline multi-agent reinforcement learning via strategy-wise bonus},
  author={Cui, Qiwen and Du, Simon S},
  journal={arXiv preprint arXiv:2206.00159},
  year={2022}
}



@article{zhang2023offline,
  title={Offline Learning in Markov Games with General Function Approximation},
  author={Zhang, Yuheng and Bai, Yu and Jiang, Nan},
  journal={arXiv preprint arXiv:2302.02571},
  year={2023}
}

@article{uehara2021pessimistic,
  title={Pessimistic model-based offline reinforcement learning under partial coverage},
  author={Uehara, Masatoshi and Sun, Wen},
  journal={arXiv preprint arXiv:2107.06226},
  year={2021}
}

@article{yan2022efficacy,
  title={The efficacy of pessimism in asynchronous Q-learning},
  author={Yan, Yuling and Li, Gen and Chen, Yuxin and Fan, Jianqing},
  journal={arXiv preprint arXiv:2203.07368},
  year={2022}
}


@inproceedings{shi2022pessimistic,
  title={Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity},
  author={Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  booktitle={International Conference on Machine Learning},
  pages={19967--20025},
  year={2022},
  organization={PMLR}
}



@article{zanette2021provable,
  title={Provable benefits of actor-critic methods for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13626--13640},
  year={2021}
}


@article{yu2022strategic,
  title={Strategic decision-making in the presence of information asymmetry: Provably efficient rl with algorithmic instruments},
  author={Yu, Mengxin and Yang, Zhuoran and Fan, Jianqing},
  journal={arXiv preprint arXiv:2208.11040},
  year={2022}
}

@article{cui2022offline,
  title={When is Offline Two-Player Zero-Sum Markov Game Solvable?},
  author={Cui, Qiwen and Du, Simon S},
  journal={arXiv preprint arXiv:2201.03522},
  year={2022}
}

@article{buckman2020importance,
  title={The importance of pessimism in fixed-dataset policy optimization},
  author={Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2009.06799},
  year={2020}
}



 @inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}


@article{gleave2022primer,
  title={A Primer on Maximum Causal Entropy Inverse Reinforcement Learning},
  author={Gleave, Adam and Toyer, Sam},
  journal={arXiv preprint arXiv:2203.11409},
  year={2022}
}


@article{ziebart2010modeling,
  title={Modeling interaction via the principle of maximum causal entropy},
  author={Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  year={2010},
  publisher={Carnegie Mellon University}
}



@article{choi2012nonparametric,
  title={Nonparametric Bayesian inverse reinforcement learning for multiple reward functions},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}



@article{neu2009training,
  title={Training parsers by inverse reinforcement learning},
  author={Neu, Gergely and Szepesv{\'a}ri, Csaba},
  journal={Machine Learning. 2009 Dec; 77: 303-37.},
  year={2009},
  publisher={Springer}
}


@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}

@inproceedings{zhou2021provably,
  title={Provably efficient reinforcement learning for discounted mdps with feature mapping},
  author={Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={12793--12802},
  year={2021},
  organization={PMLR}
}

@article{munos2008finite,
  title={Finite-Time Bounds for Fitted Value Iteration.},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={5},
  year={2008}
}

@inproceedings{szepesvari2005finite,
  title={Finite time bounds for sampling based fitted value iteration},
  author={Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={880--887},
  year={2005}
}

@article{chen2021near,
  title={Near-optimal reward-free exploration for linear mixture mdps with plug-in solver},
  author={Chen, Xiaoyu and Hu, Jiachen and Yang, Lin F and Wang, Liwei},
  journal={arXiv preprint arXiv:2110.03244},
  year={2021}
}
@article{conlisk1996bounded,
  title={Why bounded rationality?},
  author={Conlisk, John},
  journal={Journal of economic literature},
  volume={34},
  number={2},
  pages={669--700},
  year={1996},
  publisher={JSTOR}
}
@article{camerer1998bounded,
  title={Bounded rationality in individual decision making},
  author={Camerer, Colin},
  journal={Experimental economics},
  volume={1},
  pages={163--183},
  year={1998},
  publisher={Springer}
}
@article{karwowski2023sequential,
  title={Sequential Stackelberg Games with bounded rationality},
  author={Karwowski, Jan and Ma{\'n}dziuk, Jacek and {\.Z}ychowski, Adam},
  journal={Applied Soft Computing},
  volume={132},
  pages={109846},
  year={2023},
  publisher={Elsevier}
}
@article{hernandez2019bounded,
  title={Bounded rationality in decision--making},
  author={Hernandez, Jose G Vargas and Ortega, Ricardo Perez},
  journal={MOJ Research Review},
  volume={2},
  number={1},
  pages={1--8},
  year={2019}
}
@article{najar2021reinforcement,
  title={Reinforcement learning with human advice: a survey},
  author={Najar, Anis and Chetouani, Mohamed},
  journal={Frontiers in Robotics and AI},
  volume={8},
  pages={584075},
  year={2021},
  publisher={Frontiers Media SA}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
@book{sadigh2017active,
  title={Active preference-based learning of reward functions},
  author={Sadigh, Dorsa and Dragan, Anca D and Sastry, Shankar and Seshia, Sanjit A},
  year={2017}
}
@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}