
%!TEX root =main.tex
\ifneurips\vspace{-10pt}\fi
\section{Preliminaries}
\ifneurips\vspace{-10pt}\fi
% \begin{itemize}
%     \item Introduce the {\bf model} of leader-follower Markov Game 
%     \begin{itemize}
%         \item State, action, reward, transition
%         \item Do not need to introduce what the objectives of leader and folllowers are 
%         \item Just say agents takes actions $a_h, b_h$ (simutaneously?) and get reward and the state transits
%     \end{itemize} 
%     \item Policy spaces and Leader follower structure: Leader announces (policy) prescription and follower takes some quantal best response mapping (maybe use a notation $\mathtt{QR}(\pi)$ )
%     \item Specify best response model $\mathtt{QR}(\pi)$. Carefully define it for both   myopic and farsighted followers.
%     \item Characterize Bellman Equation and solution concept (Quantal Stackelberg Equilibrium): how to characterize the optimal policy of the leader, Define suboptimality of any given leader's policy $\pi$. 
%     \item  Introduce the offline setting and online setting. 
%     \item ADD a real motivating example. Learn from human interactions.
% \end{itemize} 

\input{notation.tex}

\subsection{Episodic Leader-Follower  Markov Game}\label{sec:markov_game_def}
{\ifneurips
\vspace{-10pt} 
\fi}
We consider an episodic two-player Markov game between a leader and a follower, denoted by 
$
    M  =  \{ \cS , \cA, \cB, H, \rho_0 , P  , u, r  \}. 
$
Here $\cS$ denotes the state space, $\cA$ and $\cB$ are the action spaces of the leader and follower, respectively, $H$ is the horizon length. 
In addition,   $ P = \{  P_h \colon  \cS \times\cA \times\cB\rightarrow\Delta(\cS ) \}_{h \in [H]}$ are the transition kernels, and $u = \{u_h \colon \cS \times\cA \times\cB \rightarrow [0, 1] \}_{h\in [H]}$ and $r = \{r_h \colon \cS \times\cA \times\cB \rightarrow [0, 1] \}_{h\in [H]}$ are the reward functions of the leader and the follower, respectively. 
In such a game,  for any $h \in [H]$, at step $h$, both the leader and follower observe the current state $s_h \in \cS$, take  actions $a_h \in \cA$ and $ b_h \in \cB$, receives rewards $u_h(s_h, a_h, b_h) $ and  $r_h(s_h, a_h, b_h) $ respectively, and the environment moves to a new state $s_{h+1} \sim  P_h (\cdot \given s_h, a_h, b_h)$. Here the initial state $s_1 \sim \rho_0 \in \Delta(\cS)$ and the game  terminates after $s_{H+1}$ is generated. 

\ifneurips
\vspace{-2pt}\fi
\vspace{5pt}
{\noindent \bf Leader-Follower Structure and Policies.} 
% \Siyu{to be shortened.}
We assume that the leader is a more powerful player who is able to coordinate the follower's behaviors. 
In specific, 
at the beginning of the game, the leader announces her policy $\pi = \{ \pi_{h} \}_{h\in [H]}$ for the entire game to the follower,
where $\pi_h \colon \cS\rightarrow \sA $ maps the current state to an element in $\sA$. 
Here $\sA = \{ \cB \rightarrow \Delta(\cA) \} $ denotes the set of functions that maps each action $b$  of the follower to a distribution over $\cA$. 
In other words, each element $\alpha \in \sA$ can be viewed as a prescription \citep{nayyar2014common} that specifies the leader's action contingent on the follower's action.
When the leader announces $\pi$ beforehand, she informs the follower how she will choose her action $a_h$ at each state $s_h$, given the follower's action $b_h$. 
% {\main
% Since the leader's prescription takes $b_h$ as input, the leader is a more powerful player in such a leader-follower game. 
% \fi}
To simplify the notation, in the sequel, we regard $\pi_h$ as a function $\pi_h (\cdot \given s_h, b_h) \in \Delta(\cA)$, which specifies the distribution of~$a_h$.\footnote{Here we assume that the leader is a more powerful player in the sense that her policy $\pi$  takes the follower's action $b_h$ as an input, although she does not observe $b_h$ when announcing $\pi$. 
This can be easily modified for a slightly weaker leader, whose policies $\pi$ does not depend on $b_h$. That is, $\pi_h (\cdot \given s_h)$ maps $s_h$ to a distribution over $\cA$.}

%\vspace{-3pt}

Furthermore, the follower's policy is denoted by $\nu = \{ \nu_h \}_{h\in [H]}$, where $\nu_h \colon \cS \rightarrow \Delta(\cB) $ specifies how the follower takes action $b_h$ at state $s_h$. 
With the leader-follower structure, the actions $\{ a_h, b_h\}$ are generated as  follows. At the beginning, the leader announces her policy $\pi$. The follower observes $\pi$ and chooses a policy $\nu$. For any $h\in [H]$, at the $h$-th time step, the leader commits to the announced policy and samples $\alpha_h =  \pi_h (s_h)$ at state $s_h$, the follower chooses $b_h \sim \nu_h (\cdot \given s_h)$, and then the leader executes $a_h \sim \alpha_h (\cdot \given b_h) = \pi _h ( \cdot \given s_h, b_h) $. 
{\main The structure of this Markov game is depicted in \Cref{fig:Markov game}.}

% Figure environment removed

{\neurips
\vspace{-2pt}\fi  }
{\noindent \bf Follower's Quantal Response.} 
We assume the follower has bounded rationality \citep{simon1955behavioral}  in response to the leader's announced policy $\pi$. 
In particular, let $\eta > 0$ be a parameter and let $\gamma \in [0,1]$ be a discount factor. 
We define  the quantal response policy of the follower with respect to $\pi$, denoted  by $\nu^{\pi}$, as 
the solution to an entropy regularized policy optimization problem:
{\neurips
  \abovebelowskip{0.7}{0.5}
\begin{align}
    \!\!\!\ts\nu^\pi =  \argmax_{\nu}  G(\pi, \nu), G(\pi, \nu) = \EE^{\pi, \nu}\bigsbr{{\ts\sum_{h=1}^H} \gamma^{h-1}\bigrbr{r_h (s_h, a_h, b_h) + \eta^{-1}\cH\rbr{\nu_h(\cdot\given s_h)}}}, \label{eq:energy}
\end{align}\fi}
{\main
\begin{align}
    \nu^\pi =  \argmax_{\nu}  G(\pi, \nu), \quad G(\pi, \nu) = \EE^{\pi, \nu}\sbr{{\sum_{h=1}^H} \gamma^{h-1}\bigrbr{r_h (s_h, a_h, b_h) + \eta^{-1}\cH\rbr{\nu_h(\cdot\given s_h)}}},  \label{eq:energy}
\end{align}
\fi}
% \useshortskip
\hspace{-6pt} where  we let
$\cH(\cdot)$ denote the Shannon entropy and the expectation  $\EE^{\pi, \nu^\pi} $ is taken over the randomness of the trajectory $\{ (s_h, a_h, b_h )\}_{h\in [H]} $ generated by the policy pair $(\pi, \nu^\pi)$. 
Here $\eta  $ in \eqref{eq:energy} reflects the degree of bounded rationality. 
In particular, when $\eta $ approaches $ + \infty$,  $\nu^\pi$ becomes the optimal  policy  of the MDP induced by $\pi$,  which means the follower is perfectly rational. 
Moreover, $\gamma >0 $ in \eqref{eq:energy} reflects the level of farsightedness of the follower. In particular, a myopic follower with $\gamma = 0$ only maximizes his immediate reward, whereas a farsighted follower   $\gamma = 1$ maximizes the cumulative rewards across the $H$ steps. 

\ifneurips\vspace{-3pt}\fi
Thanks to the entropy regularization in \eqref{eq:energy}, the quantal response policy $\nu^{\pi}$ is unique for any $\pi$. 
Furthermore, by the equivalence between  entropy regularization and soft Q-learning \citep{haarnoja2017reinforcement, geist2019theory}, we can alternatively  characterize $\nu^{\pi}$ using the soft Bellman equation. 
Specifically, 
for any  $h\in [H]$,   $\nu^{\pi}_h \colon \cS \rightarrow \Delta(\cB)$ can be written as the exponential of the advantage (A) function $A_h^\pi:\cS\times\cB\rightarrow \RR$,
\ifneurips
{
    
\abovebelowskip{0.5}{0.5}

\#\label{eq:quantal_response_policy}
\nu_h^\pi(b_h\given s_h) &= \exp\bigl ( \eta \cdot A_h^\pi(s_h, b_h) \bigr), \quad\text{where}\quad A_h^\pi(s_h, b_h) = Q_h^\pi(s_h, b_h) - V_{   h}^\pi(s_h) ,
\#
}
\hspace{-5pt}\fi
\ifmain
% \abovebelowskip{0.5}{0.5}
\#\label{eq:quantal_response_policy}
\nu_h^\pi(b_h\given s_h) &= \exp\bigl ( \eta \cdot A_h^\pi(s_h, b_h) \bigr), \quad\text{where}\quad A_h^\pi(s_h, b_h) = Q_h^\pi(s_h, b_h) - V_{   h}^\pi(s_h) ,
\#
\fi
and 
  the action-value (Q) function $Q^{\pi}_h \colon \cS \times \cB \rightarrow \RR$ and state-value (V) function $V_h^{\pi} \colon \cS \rightarrow \RR$ are defined respectively as 
\ifneurips
{\abovebelowskip{0.5}{0.5}\fi
\#
    Q_h^\pi (s_h, b_h) & = \bigrbr{r_h^\pi + \gamma  \rbr{ P_h^\pi V_{h+1}^\pi}}(s_h, b_h), 
    {\ifmain \quad \fi}
    V_h^\pi(s_h) = \eta^{-1}  \log \bigrbr{ {\ts\sum_{b\in \cB }}\exp\bigl ( \eta Q_h^\pi(s_h, b_h) \bigr ) }  . \label{eq:qv_pi_qr}
\# 
\ifneurips }
\hspace{-5pt}\fi
Here  we define 
$r_h^\pi(s, b) = \la \pi_h(\cdot\given s, b ), r_h(s, \cdot, b)\ra _{\cA}$ 
and 
$P _h^\pi (s' \given s, b ) = \la \pi_h(\cdot\given s, b ),  P_h (s' \given s, \cdot ,b)\ra _{\cA}$,  which are the reward function and transition kernel of the follower's MDP induced by $\pi$. 
Let  $U_h^\pi:\cS\times\cA\times\cB\rightarrow \RR$ and $W_h^\pi:\cS\rightarrow \cB$ be the leader's action-value (U) function and state-value (W) function under policy $\pi$, which are defined respectively as
\ifneurips
{\abovebelowskip{0.5}{.5}\fi
\begin{gather}
    U_h^\pi(s_h, a_h, b_h)  = u_h(s_h, a_h, b_h) + \rbr{ P_h W_{h+1}^\pi}(s_h, a_h, b_h), \label{eq:U_function} \\ 
    W_h^\pi(s_h)  = \EE^{   \pi_h, \nu_h^\pi} \sbr{U_h^\pi(s_h, a_h, b_h)}  = \la  U_h^{\pi} (s_h, \cdot , \cdot  ) ,  \pi_h \otimes \nu_{h}^{\pi}  ( \cdot, \cdot \given s_h) \cdot  \nu^{\pi}_h  (b  \given s_h), \label{eq:W_function} 
\end{gather}
\ifneurips
}
\hspace{-5pt}\fi
where the expectation in \eqref{eq:W_function} is with respect to $b_h \sim \nu_h^{\pi} (\cdot \given s_h) $ and  $a_h \sim \pi_h(\cdot \given s_h, b_h) $. 
Here we define $\pi_{h}\otimes \nu_{h}^{\pi}(a, b\given s) = \pi_h (a \given s,b) \cdot  \nu^{\pi}_h  (b  \given s) $ for all $h\in [H]$. 
For ease of presentation, we define a quantal Bellman operator $\TT_h^{\pi}:\cF(\cS\times\cA\times\cB) \rightarrow \cF(\cS\times\cA\times\cB)$ for the leader as 
\ifneurips
{\abovebelowskip{0.5}{0.5}\fi
\begin{align}
    (\TT_h^\pi f) (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE\sbr{\inp{f(s_{h+1}, \cdot,\cdot)}{\pi_{h+1}\otimes \nu_{h+1}^{\pi}(\cdot,\cdot\given s_{h+1})}}, \label{eq:bellman_operator_leader}
\end{align}
\ifneurips }
\hspace{-7pt}\fi
where the expectation is taken over $s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)$.
By definition, $U^{\pi} = \{ U_h ^{\pi} \}_{h\in [H]} $  in \eqref{eq:U_function} satisfy the Bellman equation 
$U_h^{\pi} = \TT_h^{\pi} U_{h+1}^{\pi} $. 
% Furthermore, $J(\pi) $ in \eqref{eq:J} is equal to $\EE_{s_1\sim \rho_0} [ W_1^{\pi} (s_1) ] $.

\ifneurips\vspace{-5pt}\fi \vspace{5pt}
{\noindent \bf Quantal Stackelberg Equilibrium (QSE).} From the leader's perspective, her goal is to maximize her cumulative rewards under the assumption that 
the follower adopts the quantal response  given by \eqref{eq:energy}. 
Specifically, let $\Pi = \{ \Pi_h \}_{h \in [H]}$ be a class  of leader's policies. 
The leader aims to find $\pi^*$ which maximizes $J(\pi)$ over $\pi \in \Pi$, where 
{\neurips\aboveskip[0.5]\belowskip[0.5]
\begin{align}
    J(\pi) \defeq  F(\pi, \nu^{\pi}), \qquad \textrm{and}~~F(\pi, \nu ) = \EE^{\pi, \nu }\bigsbr{{\ts \sum_{h=1}^H} u_h (s_h, a_h, b_h)}. \label{eq:J}
\end{align}\fi}
{\main
\begin{align}
    J(\pi) \defeq  F(\pi, \nu^{\pi}), \qquad \textrm{and}~~F(\pi, \nu ) = \EE^{\pi, \nu }\sbr{\sum_{h=1}^H u_h (s_h, a_h, b_h)}. \label{eq:J}
\end{align}\fi} 
\hspace{-6pt} Leader's optimal policy $\pi^*$ and its quantal response $\nu^{\pi^*}$, together constitutes a \emph{Quantal Stackelberg Equilibrium} (QSE)  \citep{bacsar1998dynamic, mckelvey1995quantal} of the leader-follower Markov game $M$. Because $J(\pi)$ is finite,   a QSE is guaranteed to exist but might not be unique. 
In the sequel, we  aim to  learn $\pi^*$ within a class $\Pi = \{ \Pi_h \}_{h\in [H]}$ from offline or online  data. 
To characterize the sample complexity of the learning algorithm, we define the suboptimality of any leader's policy $\pi$ as 
\ifneurips
{\aboveskip[0.5]\belowskip[0.5]
\begin{align}\label{eq:subopt}
    \subopt(\pi)\defeq J(\pi^*)-J(\pi),
\end{align}
}\fi
\ifmain
\begin{align}\label{eq:subopt}
    \subopt(\pi)\defeq J(\pi^*)-J(\pi),
\end{align}
\fi
which compares the cumulated rewards received by the leader under a QSE and $(\pi, \nu^{\pi})$. 
% Besides, we call a policy pair $(\pi, \nu^{\pi})$ an $\epsilon$-QSE if $\subopt(\pi)\leq \epsilon$. 




\vspace{5pt}
{\main
{\noindent \bf Motivating Examples.} While traditional game theory assumes all players behave perfectly rational, 
real world practice proved that it is necessary to take into account human players' bounded rationality \citep{conlisk1996bounded, camerer1998bounded, karwowski2023sequential,
hernandez2019bounded}.
In the context of consumer decision-making, when faced with a wide array of product choices, consumers often have limited information and cognitive resources to thoroughly evaluate each option. 
Thus, it is more realistic to consider the consumers to choose quantally rather than deterministic among the products.
% Similar situations also occur in decision making scenarios including voting, hiring and medical treatment etc. 
On the other hand, the choices made by the followers also reveal the followers' goals and interests, which can also be used for steering learning algorithms to align with humans' preferences in the field of robot training, recommender system, and large language models etc \citep{najar2021reinforcement, ouyang2022training, stiennon2020learning, bai2022constitutional, sadigh2017active, christiano2017deep}.
In this work, we target the problem of learning the follower's reward model only from his logistic choices in the context of a Markov game.
\fi}


% {\main
% \Zhuoran{Add a figure demostrating the game structure, Add a motivating example (Learn human preference)} 
% \fi}

% \Zhuoran{Deprecated}
% {\color{purple}
% commented 
%  is taken with respect to $b_h\sim \nu^\pi(\cdot\given s_h)$, $a_h\sim \pi_h(\cdot\given s_h, b_h)$, and $s_{h+1}\sim T(\cdot\given s_h, a_h, b_h)$ and $s_1\sim \rho_0$ for some initial state distribution $\rho_0$. In fact, one can easily verify that the optimal value of \eqref{eq:energy} is the same as the follower's expected value $\EE^{\pi, \nu^{\pi}}\sbr{V_h^\pi(s_1)}$. Such a model demonstrates the preference of the strategic follower with bound rationality, which is closely related to reinforcement learning with human feedback (RLHF, or preference-based reinforcement learning) \todo{add citations}.
% and the dynamic choice model \todo{add citations}.
% We assume $\sup_{h\in H}\nbr{r_h}_\infty\le 1$ in the sequel, which is without loss of generality since we can always rescale $\eta$ together with the follwer's rewards while keeping $\{\nu_h^\pi\}_{h\in[H]}$ unchanged. \Zhuoran{Already assumed at the beginning}
% In the remaining paper, we study the problem of learning the best leader's strategy $\pi=\{\pi_h\}_{h\in[H]}$ that maximizes her cumulative reward subject to the follower's quantal response,
% \begin{align}
%     J(\pi) \defeq \EE^{\pi, \nu^\pi}\sbr{\sum_{h=1}^H u_h (s_h, a_h, b_h)}.
%     %  \label{eq:J}
% \end{align}




% $\eta$ is the inverse temperature in the follower's quantal response model that determines how close the follower's response is to the greedy response, which we will discuss shortly later, 
% and $\gamma\in [0,1]$ is the discounted factor for the follower's cumulative reward at each step.
% Here, $\gamma=1$ indicates a fully far-sighted follower while $\gamma=1$ indicates a myopic follower.
% Note that we do not require $\cB_h$ and $\cA_h$ to be finite. 
 



% % In this problem, we consider the scenario where the follower can quit willingly at any stage. Without loss of generality, we consider action $b^{(0)}\in\cB_h$ as the quiting action for the follower and state $s^{(0)}\in\cS_h$ as the terminal state. If the game ends at some stage $h<H$, we consider that no further rewards are received for both the leader and the follower, which means that $R_{F/L, h}(s_h, a_h, b^{(0)})= R_{F/L, h}(s_h^{(0)}, a_h, b_h)=0$ and $P_h(\cdot \given s_h, a_h, b^{(0)}) = P_h(\cdot \given s_h^{(0)}, a_h, b_h) = \ind (s_{h+1}=s^{(0)})$, i.e., the follower cannot rejoin if quited.

% \paragraph{Follower's Quantal Response.}
% In the following, we describe the quantal response model of a farsighted strategic follower. The myopic follower is just a special case with $\gamma=0$.
% At each round $t$, the game proceeds as the following. The leader moves first and claims her strategy as $\pi^t=\{\pi_h^t\}_{h\in[H]}$, where $\pi_h^t:\cS_h\times\cB_h\rightarrow \Delta(\cA_h)$. 
% The leader's strategy can be alternatively viewed as a prescription \citep{nayyar2014common} which says that at each state $s_h$ the leader will commit to a prescription of policies $\alpha_{h, s_h}:\cB\rightarrow\Delta(\cA)$ such that $\alpha_{h, s_h}(a_h\given b_h)=\pi_h(a_h\given b_h, s_h)$. Let $\sA_h$ denote the space of all possible prescriptions at step $h$.
% The far-sighted follower then selects her quantal response by taking an entropy regularized policy that satisfies the so-called soft Bellman equation \citep{haarnoja2017reinforcement}. For the follower, her soft value functions $Q_h^{\pi}:\cS_h\times\cB_h\rightarrow\RR$, $V_h^\pi:\cS_h\rightarrow\RR$, advantage function $A_h^{\pi}:\cS_h\times\cB_h\rightarrow \RR$, and the quantal response $\nu^{\pi}:\cS_h\rightarrow\Delta(\cB_h)$ are defined recursively by
% \begin{equation}\label{eq:soft iteration}
% \begin{aligned}
%     Q_h^\pi (s_h, b_h) &= r_h^\pi(s_h, b_h) + \gamma \rbr{\PP_h^\pi V_{h+1}^\pi}(s_h, b_h), \\
%     V_h^\pi(s_h) &= \eta^{-1} \log \rbr{\int_{\cB_h}\exp\rbr{\eta Q_h^\pi(s_h, b_h)}\rd b_h}, \\
%     \nu_h^\pi(b_h\given s_h) &= \exp\rbr{\eta A_h^\pi(s_h, b_h)}, \quad \text{where}\quad A_h^\pi(s_h, b_h) = Q_h^\pi(s_h, b_h) - V_{F, h}^\pi(s_h).
% \end{aligned}
% \end{equation}
% where we use abbreviations $r_h^\pi(s_h, b_h) = \inp[]{\pi_h(\cdot\given s_h, b_h)}{r_h(s_h, \cdot, b_h)}_{\cA_h}$ and $\PP_h^\pi = \inp[]{\pi_h}{\PP_h}_{\cA_h}$. 
% Here, $\PP_h:\sF(\cS_{h+1})\rightarrow \cF(\cS_h\times\cA_h\times\cB_h)$ denotes the conditional mean operator with respect to the kernel $P_h$.
% The first line of \eqref{eq:soft iteration} is a standard Bellman equation. 
% The second line of \eqref{eq:soft iteration} indicates that the V-function corresponds to the softmax of the Q-function where $\nu_h^\pi$ is just the softmax policy by definition. 
% Another way to think of this quantal response model is by viewing $\nu_h^\pi$ as the energy-based policy \citep{haarnoja2017reinforcement, geist2019theory},
% \begin{align}
%     \nu^\pi = \argmax_{\nu'=\{\nu'_h\}_{h\in[H]}} \EE^{\pi, \nu'}\sbr{\sum_{h=1}^H \gamma^{h-1}\Bigrbr{r_h^\pi(s_h, b_h) + \eta^{-1}\cH\rbr{\nu'_h(\cdot\given s_h)}}},\label{eq:energy}
% \end{align}
% where we use $\cH(\cdot)$ to denote the Shannon entropy and $\EE^{\pi, \nu^\pi}$ is taken with respect to $b_h\sim \nu^\pi(\cdot\given s_h)$, $a_h\sim \pi_h(\cdot\given s_h, b_h)$, and $s_{h+1}\sim T(\cdot\given s_h, a_h, b_h)$ and $s_1\sim \rho_0$ for some initial state distribution $\rho_0$. In fact, one can easily verify that the optimal value of \eqref{eq:energy} is the same as the follower's expected value $\EE^{\pi, \nu^{\pi}}\sbr{V_h^\pi(s_1)}$. Such a model demonstrates the preference of the strategic follower with bound rationality, which is closely related to reinforcement learning with human feedback (RLHF, or preference-based reinforcement learning) \todo{add citations}.
% and the dynamic choice model \todo{add citations}.
% We assume $\sup_{h\in H}\nbr{r_h}_\infty\le 1$ in the sequel, which is without loss of generality since we can always rescale $\eta$ together with the follwer's rewards while keeping $\{\nu_h^\pi\}_{h\in[H]}$ unchanged.
% In the remaining paper, we study the problem of learning the best leader's strategy $\pi=\{\pi_h\}_{h\in[H]}$ that maximizes her cumulative reward subject to the follower's quantal response,
% \begin{align}
%     J(\pi) \defeq \EE^{\pi, \nu^\pi}\sbr{\sum_{h=1}^H u_h (s_h, a_h, b_h)}. \label{eq:J}
% \end{align}
% Let $\pi^*$ be a optimal policy that achieves the maximum of \eqref{eq:J}. Therefore, we define the sub-optimality of any policy $\pi$ as
% \begin{align}\label{eq:subopt}
%     \subopt(\pi)\defeq J(\pi^*)-J(\pi).
% \end{align}
% Throughout the paper, we will frequently use the following linear MDP assumption for both the follower and the leader.
% In the sequel, we consider without much loss of generality that $U_h^{\pi}$, $Q_h^{\pi}$ and  $A_h^{\pi}$ are $B_U, B_Q$ and $B_A$ bounded. 
% }


\ifneurips\vspace{-5pt}\fi
\subsection{Learning QSE from Data: Information Structure and Performance Metrics}
\ifneurips\vspace{-5pt}\fi
We aim to design online and offline RL algorithms that learns $\pi^*$ on behalf of  the leader.
That is, the RL algorithms has access to online or offline data that only contains what the leader is able to observe, when interacting with a boundedly rational   agent. 

\ifneurips\vspace{-2pt}\fi \vspace{5pt}
{\noindent \bf Information Structure.} Let $\cM$ be a class of leader-follower Markov Games specified in Definition \ref{sec:markov_game_def} and let $M^* \in \cM$ denote the true environment. 
In the reinforcement learning setup, the  leader does not know $M^*$ or the quantal response mapping, and need to learn her optimal policy  $\pi^*$. 
We  assume the follower always outputs the quantal response policy $\nu^{\pi}$ 
when the leader commits to a policy $\pi$, where $\nu^{\pi}$ is defined in \eqref{eq:energy}. 
In particular, in this case,  the leader's knowledge is $\{s_h,   a_h, b_h , u_h, \pi_h \}_{h\in [H]} $, a trajectory collected by the policy pair  $(\pi. \nu^{\pi})$, where  $b_h \sim \nu^{\pi}_h (\cdot \given s_h)$,  and $a_h \sim   \pi_h(\cdot \given s_h, b_h)$. An information asymmetry exists in the sense that the leader cannot observe the follower's reward.
 
\ifneurips\vspace{-2pt}\fi \vspace{5pt}
{\noindent \bf Offline RL.} In the offline setting, we aim to learn leader's optimal policy $\pi^*$ from an offline dataset $\mathcal{D} = \{ \tau^t = \{  (s_h^t,   a_h^t, b_h^t, u_h^t, \pi_h^t ) \} _{h\in [H]},   t\in [T]\}$ collected a priori, which contains $T$  trajectories collected on  $M^*$. 
Here each trajectory $\tau^t $ is sampled from a behavior policy $\pi^t$ and its quantal response $\nu^{\pi^t}$. 
Given the dataset $\cD$, we aim to design an  offline RL algorithm that returns a policy $\hat \pi$ for the leader such that the suboptimality $\subopt(\hat \pi)$ defined in  \eqref{eq:subopt} is small.


\ifneurips\vspace{-2pt}\fi \vspace{5pt}
{\noindent \bf Online RL.} In the online setting, the leader learns $\pi^*$ by interacting with the agent for $T$ episodes, without any prior knowledge or data. In specific, for any $t\in [T]$, in the $t$-th episode, leader announces and commits to $\pi^t$, the follower adopts quantal response $\nu^{\pi^t}$. Then the leader observes a new trajectory $ \tau^t = \{  (s_h^t,    a_h^t, b_h^t, u_h^t, \pi_h^t ) \} _{h\in [H]} $. The goal of the online RL algorithm is to design policy sequence $\{\pi^t\}_{t\in [T]}$ such that the regret 
{\neurips\aboveskip[0.5]\belowskip[0.5]
\begin{align}\label{eq:regret_def}
    \Reg (T) = {\ts\sum_{t=1}^T} \subopt(\pi^t) = {\ts\sum_{t=1}^T}  \bigl( J(\pi^*) - J(\pi^t )\bigr)   
\end{align}
\fi}
{\main
\begin{align}\label{eq:regret_def}
    \Reg (T) = \sum_{t=1}^T  \subopt(\pi^t) = \sum_{t=1}^T  \bigl[ J(\pi^*) - J(\pi^t )\bigr]   
\end{align}
\fi}
\hspace{-7pt} is small. 
Besides,   the randomized policy pair that chooses $(\pi^t, \nu^{\pi^t})$ uniformly random constitutes an approximate QSE with error $\Reg(T)/T$. 
In other words, if $\Reg(T) = o(T)$, when $T$ is sufficiently large, the average policy generated by the online RL algorithm constitutes an approximate QSE. 

% {\neurips
% {\noindent \bf Linear Markov Game.} 
In the following, we define a linear MDP approximation for this quantal Stackelberg game.
\begin{definition}[{Linear Markov Game}]\label{def:linear MDP}
    We call the episodic leader-follower Markov game linear if there exist mappings $\varpi^*_h:\cS\rightarrow\RR^d$ and feature functions  $\phi_h(\cdot,\cdot):\cS\times\cA\times\cB\rightarrow\RR^d$ for any $h\in[H]$ such that 
the transition probabilities can be expressed as:
$
    P_h(s_{h+1}'\given s_h, a_h, b_h) = \inp[]{\phi_h(s_h, a_h, b_h)}{\varpi^*_h(s_{h+1})},
$  
and
the % leader's and follower's 
reward functions $r_h$ and $u_h$ are also linear in $\phi_h$. That is, 
$
        u_h(s_h, a_h, b_h) = \inp[]{\phi_h(s_h, a_h, b_h)}{\vartheta^*_h},  $
$
r_h(s_h, a_h, b_h) = \inp[]{\phi_h(s_h, a_h, b_h)}{\theta^*_h}, 
$
where $\vartheta^*_h\in\RR^d$ and $\theta^*_h\in\RR^d$ are parameters.  
\end{definition}
% \fi}
%\vspace{1cm}
%{\color{purple} 
%commented
% We consider the following two learning setups in the paper, i.e., offline policy learning with myopic follower and online policy learning with nonmyopic follower. 



% \subsection{Information Structure}

%  We consider the follower to be ominiscent who takes the leader's strategy and outputs her quantal response aligned with her self-interest based on her knowledge of the true environment $M^*$.
%  In contrast, the leader has no knowledge of what the real MDP environment $M^*$ is. We also consider that the leader is aware of the inverse temporature $\eta$ and the follower's discounted factor $\gamma$ in advance, where the former is without loss of generality by the rescaling argument between $\eta$ and $r_h$, and the latter is reasonable since it is usually the case that we have $\gamma=1$ for a farsighted follower and $\gamma=0$ for a myopic follower. Thus, the leader only needs to know what the follower's foresight (myopic/nonmyopic) is in this case.

% We consider bandit feedback with information asymmetry in the leader's and the follower's observations in the play. Specifically, the follower's utility is unobservable to the leader. Thus, the only way for the leader to rationalize the follower's response is to learn the follower's reward model from past behaviors of the follower. This setting is in sharp contrast to those in the literatures that assume observability or prior knowledge of the follower's utility \citep{ghosh2022provably, zhong2021can} \todo{to be added}. This setting is commonly seen in max-entropy Inverse Reinforcement Learning (IRL) problem \todo{add citations}. However, our problem is much challenging in the presence of misalignment between the leader's and the follower's interest, which is also known as the externality effect \todo{add citations}.

% \subsection{Learning Setup}
% We consider the following two leanring setups in the paper, i.e., offline policy learning with myopic follower and online policy learning with nonmyopic follower. 

% \paragraph{Offline Setup.}
% We only consider myopic follower in the offline setting because the leader's strategy $\pi$ for each state at each step is too costly to collect. Instead, only the prescription $\alpha_{h, s_h^t}(\cdot\given\cdot)$ is available where $s_h^t$ is the state actually visited by trajectory $t$ at step $h$. We state the sampling procedure and the learning targets in the following. 
% We assume that the offline dataset contains $T$ trajectories collected by interacting with a myopic follower, where  trajectory $t\in[T]$ contains $(s_h^t, a_h^t, b_h^t, u_h^t, \alpha_h^t)_{h\in[H]}$. Here, $\alpha_h^t$ is a short hand of the prescription $\alpha_{h, s_h^t}$ and the follower's utility is not contained in the dataset. The leaning target is to find a policy $\hat\pi$ that minimizes the  sub-optimality
% $\subopt(\hat\pi)$ defined by \eqref{eq:subopt}.
% Note that we don't require $\pi^t$ to be indepedent in the offline dataset, which means that a strategic leader is also allowed in collecting the samples.

% \paragraph{Online Setup.}
% We consider a online setting with $T$ rounds where the leader is capable of interactive decision making and the follower can be nonmyopic. At each round $t$, the leader announces a strategy $\pi^t$ and observes a trajectory $\tau^t=(s_h^t, a_h^t, b_h^t, u_h^t)_{h\in[H]}$. Let $\tau^{1:t}$ denote all the histories up to step $t$. Suppose that the leader adopts strategy $\pi^t$ at round $t$. The learning target is to decide a sequence of policy $\{\pi^t\}_{t\in[T]}$ that achieves small online regret,
% \begin{align*}
%     \Reg_p(T) = \EE_{\pi^t}\sbr{\subopt(\pi^t)}, 
% \end{align*}
% where the expectation is taken with respect to the randomness in $\tau^t$.


