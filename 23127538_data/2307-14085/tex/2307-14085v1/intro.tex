
%!TEX root =main.tex
\vspace{-15pt}
\section{Introduction}\label{sec:intro}
\vspace{-5pt}
Multi-agent reinforcement learning (RL) \citep{busoniu2008comprehensive, zhang2021multi} studies sequential decision making problems where  multiple agents interact with each other. 
Such a problem is often modeled as a Markov game,
where at  each timestep, each agent
takes an action at the current state of the environment, receives  an immediate reward, and the environment moves  to a new state according to a Markov transition kernel. 
Here the agents affect each other because the rewards and state transitions depend  on the actions of all players. 
The goal of each agent is to learn her optimal policy that maximizes her expected total return, in the presence of strategic plays of other agents. 

Compared with single-agent RL, one of the most salient challenges of multi-agent RL is \emph{nonstationarity}. 
That is, from the perspective of each agent, she faces a Markov decision process (MDP) induced by the policy of the other agents, which changes abruptly once the other agents adopt new policies. 
To deal with such a challenge, most of the existing literature either 
(i) assumes a \emph{central controller}  that learns the policies for all agents, or 
(ii) designs decentralized methods  based on  adversarial bandits algorithms (see \Cref{sec:related_work} for details). 
The way these works deal with nonstationarity is rather ``passive''.
In particular, nonstationarity is out of the scope given (i), and (ii)   assumes the other agents change arbitrarily and only hopes to compete against the best fixed policy in hindsight.


In contrast, an ``active'' approach of dealing nonstationarity would actively \emph{infer} how other agents adjust their policies from their history actions, and leveraging such information to better design her own policy. 
Such an active approach is particularly promising in human-robot interaction, where we aim to train robots to better assist  human  by learning from human feedback.  


In this work, we aim to design provably efficient RL algorithms for Markov games that \emph{actively address the challenge of nonstationarity}. 
As an initial attempt, 
we focus on the setting of a two-player Markov games with a leader-follower structure. 
In specific, in such a game, 
the leader takes on the role of the primary decision-maker who commits to a policy $\pi$ upfront, while the follower, knowing $\pi$, chooses a best-response policy $\nu^{\pi}$ that maximizes his expected  rewards. 
Here we assume the follower's response $\nu^{\pi}$ is unique, which is  obtained by solving an entropy-regularized MDP induced by $\pi$. Here $\nu^{\pi}$ is also called the quantal response to leader's policy $\pi$.
We allow the follower to be either myopic, considering only the immediate reward, or farsighted, planning ahead across all future steps. 
The leader's objective is to 
find her optimal policy $\pi^*$ such that the policy pair  
$(\pi^*, \nu^{\pi^*})$ optimizes her expected total rewards, and  $(\pi^*, \nu^{\pi^*})$ is called a quantal Stackelberg equilibrium (QSE) \citep{bacsar1998dynamic, mckelvey1995quantal}. 
The objectives of the leader and the follower are misaligned as their reward functions $u$ and $r$ are different. 
Our goal is to design online and offline RL algorithms for the leader under the episodic  and function approximation setting, based only on information available to the leader, i.e.,  states, joint actions, and the leader's rewards. In particular, the leader only observes the follower's actions, not rewards. 


To efficiently learn $\pi^*$, the leader needs to efficiently optimize her own expected return  by strategically guiding the follower's behavior, which necessitates \emph{actively learning the follower's quantal response mapping} from the data. 
Learning a QSE poses a few unique challenges. {\blue (a)} First, the leader must infer the follower's quantal response mapping from follower's actions, which requires building a response model and inferring follower's reward function by observing how the follower responds  to different leader policies. 
Such a challenge is exacerbated when the follower is farsighted. 
{\blue (b)} Second, QSE is posed as the solution to  a \emph{bilevel optimization} where the leader optimizes her policy subject to the constraint that the follower adopts the quantal response policy. 
Characterizing the performance of the leader's learned policy requires novel analysis that bridges the upper and lower-level problems. 
In specific, we need to determine how the error of the inferred quantal response affects the performance of the learned policy. 
{\blue (c)} Third, we need to understand how to incentivize exploration in the online setting or how to guard against insufficient  data  coverage in the offline setting, in the presence of an inaccurate response model. 
Handling this challenge requires modifying the optimism/pessimism principle for the problem of learning a QSE. 
  

We successfully tackle these challenges and  establish provably sample-efficient RL algorithms for both online and offline settings, in the context of  function approximation, where the follower can be either myopic or farsighted. 
In specific, our algorithms are based on (i) a model-based approach for learning the follower's quantal-response   via maximum likelihood estimation (Challenge (a)), and (ii) a 
single-agent reinforcement learning method for leader's decision making problem. 
In specific, for a myopic follower, 
we adopt a 
 model-free least-squares value iteration (LSVI)  \citep{sutton2018reinforcement} approach to learn the leader's optimal value function. 
 For a farsighted follower, we leverage model-based RL to learn the transition model and the leader's reward function. 
In addition, to promote exploration in the online setting, we construct confidence sets for both the response model and leader's value function (or transition and reward model), and propose to update the leader's policies via optimistic planning \citep{auer2008near}. 
Such an algorithm can be easily modified for the offline setting via pessimistic planning (Challenge (c)). 
Both these algorithms are able to incorporate general function approximation. 
Furthermore,  in the special case with myopic follower and linear function approximation, 
we establish variants of optimistic and pessimistic algorithms that can be efficiently implemented using bonus and penalty functions. 
Furthermore, we prove that all of these methods enjoy  sublinear regret or suboptimality bounds, hence proving statistical efficiency. 
In the case with general function approximation, we introduce novel eluder dimensions that captures the challenge of exploration for learning a QSE. 
Finally, to characterize the suboptimality of leader's policy, we establish a novel performance difference lemma  which relates the suboptimality of the learned policy  to the \emph{Bellman error} of the upper level problem and the  \emph{estimation error of the response model} in the lower level problem (Challenge (b)). Such a result might be of independent interest.  


\vspace{5pt}
{\noindent \bf Our Contributions.} In summary, this work proposes provably sample-efficient algorithms for learning the QSE in an episodic Markov game with a leader-follower structure. In such  a game, when the leader announces her policy, the follower adopts the corresponding quantal response, and the leader only observes the follower's actions but not rewards. 
To address the challenge of nonstationarity, the leader needs to actively infer how the follower reacts to her announced policy, and leverage such  information to drive the game towards the QSE. 
We successfully address such a challenge for both  online and offline RL settings with linear and general function approximation. 
We also allow the follower to be both myopic and farsighted. 
Specifically, our contributions are three-fold.
\begin{itemize}
\item [(i)] First, for the case of a  linear Markov game with a   myopic follower, we establish sample-efficient RL algorithms for both  the  offline and online settings. 
These algorithms are based on the principle of pessimism/optimism in the face of uncertainty, where we quantify the estimation uncertainty of leader's value function and follower's quantal response model. 
Thanks to the linear structure, both 
  the offline and online algorithms have versions that are computationally efficient  (Algorithm \ref{alg:PMLE} with {\bf S3} and Algorithm \ref{alg:MLE-OVI} with {\bf S5}).   
 \item [(ii)] Second, for the case  with  general function approximation, we propose provably sample-efficient algorithms for both the online and offline settings, where the follower is allowed to be either  myopic or  farsighted.
 In particular, in the case with a myopic follower, our  algorithms  combine model-free value estimation for the leader and maximum likelihood estimation of the follower's quantal response mapping  (Algorithm \ref{alg:MLE-BCP} and Algorithm \ref{alg:MLE-GOLF}).
For the case with a farsighted follower, we propose  model-based algorithms by combining  pessimism  and optimism with  maximum likelihood estimation for Markov game model (Algorithm \ref{alg:real-PMLE} and Algorithm~\ref{alg:OMLE}). We prove that these algorithm are all sample-efficient by establishing upper bounds on the suboptimality or cumulative regret. 
  
 \item [(iii)] Third, we establish a novel performance difference lemma 
 which relates the suboptimality of the  leader's policy  to leader's Bellman error and the error incurred in estimating the follower's quantal response model, which is measured in terms of the total variation (TV) distance. (See \eqref{eq:performance diff-1} or \Cref{lem:subopt-decomposition} for details.) 
 Moreover, due to the nonlinear nature of the quantal response mapping, we further expand the estimation error of the  quantal response model into a first-order and a second-order term,  which can be further controlled by the maximum likelihood analysis. (See \eqref{eq:QRE-decompose} or \Cref{lem:performance diff} for details.)    
 From the lens of bilevel optimization, our regret analysis connects the Bellman error in the upper level   with the quantal response error in lower level, which might be of independent interest.   \end{itemize} 

%\vspace{5pt}
%{\noindent \bf Roadmap.} 


\section{Related Works} \label{sec:related_work} 



Our work is most related to works that learn Stackelberg equilibria in (Markov) games via online RL \citep{bai2021sample, zhong2021can, kao2022decentralized,  zhao2023online}.
All of these works assume the follower is myopic and perfectly rational. In the following, we discuss these works in detail. Moreover, our work  adds to the large body of literature of of single-agent and multi-agent online and offline RL, as well as the literature of maximum entropy inverse RL. 

%Due to the space limit, we defer detailed discussions on related works to \Cref{sec:related_work}.



\input{relatedwork.tex}

% \vspace{-5pt}
% {\noindent \bf notations.}
% We define $\SSS_+^\dagger$
% The applications of Stackelberg games span across various domains, illustrating their versatility and relevance in understanding and analyzing complex interactions. For instance, in the field of strategic classification, where a classifier guides the behavior of another classifier, Stackelberg games provide a valuable framework for modeling the decision-making dynamics between the classifiers. Similarly, in the realm of performative prediction, where an algorithm aims to influence human decision-making through the provision of predictions, Stackelberg games offer insights into how the algorithm can strategically shape its predictions to optimize its own objectives.

% Moreover, Stackelberg games have also found utility in the domain of reward shaping, where an agent designs a reward function to encourage desirable behavior from another agent. By adopting a Stackelberg game perspective, the agent can strategically design the reward function to elicit specific behaviors from the other agent, ultimately achieving more effective and efficient learning outcomes.

% In addition to these applications, Stackelberg games have proven instrumental in security games, which involve the strategic allocation of security resources to protect critical assets. The leader, in this case, determines the optimal allocation strategy to influence the behavior of potential adversaries and mitigate risks effectively.

% Furthermore, the framework of Stackelberg games offers valuable insights into human-robot interaction scenarios. In these scenarios, the robot acts as the follower, learning from and aligning with human values. By applying Stackelberg game principles, the robot can effectively navigate and adapt its behavior to align with the preferences and intentions of the human, promoting a more harmonious and collaborative interaction.

% In summary, Stackelberg games provide a robust framework for modeling strategic interactions, enabling the analysis of decision-making dynamics in a wide range of domains. By capturing the leader-follower structure, these games shed light on the mechanisms by which leaders can strategically influence followers' behavior for optimizing their own rewards. With applications spanning from classification and prediction to reward shaping, security games, and human-robot interaction, Stackelberg games continue to offer valuable insights into the complexities of strategic decision-making in diverse scenarios.






\iffalse 


Stackelberg games model strategic interactions between agents with a leader-follower structure. The leader commits to a strategy first to influence the follower's behavior for maximizing her own reward. Such games have found applications in many areas, including strategic classification, performative prediction, reward shaping, and security games. They also provide a framework for modeling human-robot interaction where the robot aims to learn from and align with human values.
Most existing work on multi-agent reinforcement learning focuses on Markov perfect equilibria or correlated equilibria. Relatively little attention has been paid to learning Stackelberg equilibria, especially when the leader has limited observability into the follower's rewards. In this paper, we study the reinforcement learning problem of a leader facing a boundedly rational follower in a Stackelberg game with episodic interactions. The leader commits to and announces a policy, and the follower responds with a quantal response policy that maximizes an entropy-regularized objective. We assume the leader only observes the follower's actions, not rewards, and aims to learn a Quantal Stackelberg Equilibrium (QSE) - the leader's optimal policy accounting for the follower's quantal response.  
Learning a QSE poses unique challenges. First, the leader must infer the follower's unknown quantal response mapping from limited interaction data. Second, Stackelberg games involve bilevel optimization where the leader optimizes her policy accounting for the follower's optimization. Finally, the follower may be either myopic, considering only the current state, or farsighted, planning ahead.
We develop novel reinforcement learning algorithms enabling the leader to efficiently learn a QSE without observing the follower's rewards. Our algorithms rely on maximum likelihood estimation of the quantal response mapping and the principle of optimism in the face of uncertainty. We provide theoretical guarantees on their sublinear regret and suboptimality under the online and offline settings.

To summarize, our key contributions are:
1) Reinforcement learning algorithms for learning QSE under partial observability. We propose algorithms for both myopic and farsighted followers.
2) Sample efficiency and theoretical analysis. We prove sublinear regret and suboptimality bounds for our algorithms without assuming full knowledge of the follower's rewards or dynamics. 
3) Applying optimism in the face of uncertainty to Stackelberg games. We show how optimism enables learning a QSE despite uncertainties from partial observability and bounded rationality. 
This work provides a framework for learning optimal strategies in Stackelberg competition with boundedly rational agents. Future directions include fully decentralized learning and competition between strategic learners. Overall, this paper sheds light on multi-agent reinforcement learning in games with asymmetric information and strategic behavior. (edited) 



\begin{itemize}
\item Broad Background: Stackelberg games (leader follower structure) are very useful in modeling strategic behaviors of agents in response to another agents's agent. List a few examples of Stackelberg game, for example, strategic classication, performative prediction, reward shaping, security games. Also talk about human-robot interaction: robot wants to learn from human behaviors and align with human values. 
\item Narrow Background: Most existing works of multi-agent RL study Markov perfect equilibria or Correlated Equilibria. There are limited works study Stackelberg equliria under the context of RL. Most of these works study myopic agents and assume the leader knows follower's rewards. 
\item Our problem: setting. We study Stackelberg game where the leader needs to infer the best response of the follower. We consider the quantal best response (maybe because there is some hardness result?). In specific, the agent does not observe the agent's reward, but only observe the best reponse actions played by the follower. Moreover, the follower can either be myopic or farsighted. 
\item Our problem: Learning objective. We consider the online setting where we aim to learn  the optimal policy of the leader by interacting with the follower and gather data. 
\item Technical challenges. 
\begin{itemize}
    \item Nonstationarity: quantal response changes nonlinearly in $\pi$, and the visitation measure also changes. 
    \item Need to infer the quantal  reponse mapping by learning the reward function. (i) identity the reward parameter, (ii) estimation and uncertainty quantification  
    \item Stackelberg game is bilevel optimization. Need to understand how the estimation error of the reward parameter affects the leader's online learning problem. 
    \item Moreover, for farsighted follower, only $Q$ is identified. We need to get the reward function by from  the Bellman equaiton for the follower. 
\end{itemize} 
\item How we solve the challenges, algorithm design and novel analysis. 
\begin{itemize}
    \item Identification of reward parameter, and how to identify the reward in far-sighted case through follower's Bellman equation. 
    \item Uncertainty quantification of the follower's quantal response function.
    \item Pessimistic / Optimistic planning with  uncertainty about the model/value of leader and the quantal response mapping. In addition to constrained optimization, we offer an implementation via penalty functions. 
    \item A novel regret decomposition that shows how (i)   the estimation error of the quantal response and (ii) estimation of the Markov game model affects the suboptimality of leader's policy. 
\end{itemize}
\item Theoretical Results: Sample efficiency in both offline and online setting. 
\item Significance: First provably sample efficient algorithm for learning QSE without knowing follower's rewards. 
\end{itemize}


{\noindent \bf Related Works.}

\begin{itemize}
    \item RL for Markov games: most assume zero sum. Some consider general sum Markov games, but need to assume the reward of other agents. 
    \item V-learning and its variants do not need to assume being able to observe other agents' reward, but cannot solve for Stacleberg equilibrium 
    \item RL for stackelberg game -- Chi, Zhong Han, Chen-Yu, Mengxin, Banghua, 
\end{itemize}



\vspace{2cm}


Knowing the leader's announced policy $\pi$, a rational follower would choose a best response policy that maximize his expected rewards.


Such a model demonstrates the preference of the strategic follower with bound rationality, which is closely related to reinforcement learning with human feedback (RLHF, or preference-based reinforcement learning) \todo{add citations}.
and the dynamic choice model \todo{add citations}.


\fi 


% \input{agenda.tex}



% {\color{purple}
% {\noindent \bf Section 4: Offline Learning via Pessimism + Behavior Modeling}
% \begin{itemize} 
%     \item Section 3.1: based on the confidence set for quantal response (reward function of follower), present the algorithm for offline setting (pessimistic planning). Present the general function class. 
%     \item Section 3.2: Present the theorem for general function class, then add a corollary for linear case 
%     \item Section 3.3: Present the proof sketch:  highlight your new regret decomposition result which links the suboptimality of leader to how well you learn the response model. 
% \end{itemize} 


% {\noindent \bf Section 5: Online Learning via Optimism + Behavior Modeling}

%  \begin{itemize} 
%     \item Section 4.1: based on the confidence set for quantal response (reward function of follower), present the algorithm for online setting (optimistic  planning). (it's better to present the general function class) Present the myopic setting only.  
%     \item Section 4.2: Present the theorem for general function class, then add a corollary for linear case. Highlight why you need to have two kinds of bilinear classes -- one for leader and one for follower analysis. It would be great if you could motivate using the result in Section 3.3.
%     \item Section 4.3: Present a  proof sketch -- a short one would be enough.
%     \item Section 4.4: Extension to farsighted case (Section 4.4.1) -- algorithm and theorem (Section 4.4.2)
% \end{itemize} 

% }   

\iffalse 
{\noindent \bf Challenges}

As introduced  in \S\ref{sec:markov_game_def}, 
the QSE $(\pi^*, \nu^{\pi^*})$ is the solution to a bilevel  optimization problem 
\#\label{eq:bilevel}
\max_{\pi \in \Pi } F(\pi, \tilde \nu) \qquad \textrm{subject to} ~~\tilde \nu = \argmax_{\nu}~ G(\pi, \nu ), 
\#
where $F$ and $G$ are defined respectively in \eqref{eq:J} and \eqref{eq:energy}. 
The lower level problem states that the follower adopts the quantal response against the leader, which is posed as a constraint for the leader's policy optimization problem in the upper level.  
Thus, finding the leader's optimal policy $\pi^*$ involves two steps.  
First, we need to  learn the quantal response mapping $\pi\rightarrow \nu^\pi$ by solving the lower level problem. 
Second, we  solve the upper level problem in \eqref{eq:bilevel} with $\tilde \nu$ replaced by $\nu^{\pi}$. 
Because the leader is unaware of the true model $M^*$, she has to accomplish these two steps approximately by online or offline data. 
Therefore, designing sample-efficient RL algorithms for learning the QSE involves the following three challenges. 
% \Siyu{to be shortened.}
\begin{itemize}
    \item [(a).] \emph{How  to learn the quantal response mapping from follower's feedbacks?} 
    % {\main
    Note that the leader does not observe follower's reward $r_h$, she needs to learn  the quantal response from follower's actions $b_h$. This requires building a response model and infer follower's reward function by observing how the follower respond  to different leader policies. 
    Moreover, such a challenge is exacerbated when the follower is not myopic ($\gamma > 0$) because the future rewards also affects the follower's quantal response, which makes it impossible to directly  estimate the reward functions via maximum likelihood estimation. 
    % \fi}
    % {\neurips
    % Since the leader does not observe follower's reward $r_h$, she needs to learn  the quantal response from follower's actions $b_h$. This is a unique challenge under the information structure and the challenge is even exacerbated if the follower is nonmyopic.
    % \fi}


    \item [(b).] \emph{How  the estimation uncertainty of the quantal response mapping affects the leader's policy learning?} 
    % {\main 
    Suppose we can construct an estimator of the quantal response mapping, then we need to plug  the estimated quantal response  into the upper level problem in \eqref{eq:bilevel} and obtain an estimator for $\pi^*$. 
    To quantify the performance of the returned policy, we need to establish a new result that relates the suboptimality of the learned policy  to the \emph{Bellman error} of the upper level problem and the  \emph{estimation error of the response model} in the lower level problem. 
    Such a results would be a generalization of the  the  performance difference lemma \cite{} to the setting of  learning QSE,
    % \fi}
    % {\neurips 
    % In contrast to a standard RL setting, we need to establish a new result that relates the suboptimality of the learned policy to both the \emph{Bellman error for the leader} in the upper level problem and the \emph{estimation error of the follower's response model} in the lower level problem.
    % \fi}
    \item [(c).] \emph{How to balance the challenges of (online) exploration  and (offline) insufficient coverage in the presence of an inaccurate response model?} Additionally, we need to handle the fundamental challenges of online and offline RL, namely the exploration-exploitation tradeoff and the insufficient coverage of the offline data. Although these challenges has been rather successfully addressed in the context of MDPs, 
    % {\main 
    for learning QSE, due to the bilevel structure and the quantal  response  feedback, we need to understand how to incentivize exploration or guard against distributional shift in the presence of  an inaccurate quantal  response model  estimated from online or offline data with insufficient coverage.
    % \fi} 
    % {\neurips
    % we need new techniques to handle this problem due to the previously mentioned challenges.
    % \fi}
\end{itemize}


As a summary, having a quantal response model poses significant challenges to learning QSE via RL, especially when the follower's reward is absent from the the leader's observation. In this work, we identify the key component that handles these challenges and at the same time captures the comparison nature of in the follower's response model, namely a \emph{\red a quantal response error} for general function approximation and a \emph{\red covariance matrix}  as an analogue for linear function approximation. This quantal response advantage error is related to a new  complexity measure--Quantal  Eluder (QE) dimension, and we show that instances of low QE dimension is efficient for online learning.
In the offline setting, we also address the distribution shift in terms of this quantal response advantage error.
In the  rest of the section, 
we highlight how we handle challenges (a) and (b). 
These techniques serve as the cornerstones for designing efficient offline and online  RL algorithms that successfully address Challenge (c), which are presented in subsequent sections. 

\fi  