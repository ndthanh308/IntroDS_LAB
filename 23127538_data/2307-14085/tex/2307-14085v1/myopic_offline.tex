%!TEX root =../neurips_main.tex
\ifmain
\section{Offline Learning with Myopic Follower}\label{sec:offline-myopic}
\fi
\ifneurips
\subsection{Offline Learning with Myopic Follower}\label{sec:offline-myopic}
\fi
In this section, we study the problem of offline learning the optimal policy for the leader when the follower is myopic. 
In the offline setting, the offline dataset is collected as $\cD = \{(s_h^i, a_h^i, b_h^i, u_h^i,\pi_h^i)_{h=1}^H\}_{i\in[T]}$.
Here, $\pi^i$ should be thought of as a random variable.
% We consider a data generating process in which the leader's policies across episodes are indepedent.
% , but the prescriptions used  within an episode can be dependent across steps. This can be viewed equivalently as having $T$ pairs of leaders (can be strategic) and followers independently playing this quantal stackelberg game. 
% Under this setting, we suppose that $\tau^i$ has a marginal distribution $\mu$.
We let $\EE_\cD$ denote the expectation with respect to the data generating distribution (also over the randomness of $\pi^i$). 
We study two function approximation schemes, namely the linear function approximation and the general function approximation.

\ifmain
\subsection{Offline Learning for Linear Markov Game}\label{sec:offline-ML}
\fi
\ifneurips
\subsubsection{Offline Learning for Linear MDP}\label{sec:offline-ML}
\fi
% Previously, we study the Offline-MG algorithm for learning the QSE with general function class, which is done by constructing a valid and accurate confidence set for both the environment model and the behavior model.
In this subsection, we  develop a computationally efficient and value iteration-based algorithm for the linear Markov game setting which is defined in \Cref{def:linear MDP}.
{\iffalse
We first give a definition for our linear MDP setting.
\begin{definition}[{Linear MDP}]\label{def:linear MDP}
    For the episodic leader-follower  Markov game, we call it linear if there exists maps $\varpi^*_h:\cS\times\RR^d$ and features  $\phi_h(\cdot,\cdot):\cS\times\cA\times\cB\rightarrow\RR^d$ for any $h\in[H]$ such that
    \begin{align*}
        P_h(s_{h+1}'\given s_h, a_h, b_h) = \inp[]{\phi_h(s_h, a_h, b_h)}{\varpi^*_h(s_{h+1})}_{\RR^d}.
    \end{align*}
    Furthermore, the leader's and the follower's utility admit the following linear factorizations
    \begin{align*}
        u_h(s_h, a_h, b_h) = \inp[]{\phi_h(s_h, a_h, b_h)}{\vartheta^*_h}_{\RR^d}, \quad r_h(s_h, a_h, b_h) = \inp[]{\phi_h(s_h, a_h, b_h)}{\theta^*_h}_{\RR^d},
    \end{align*}
    where $\vartheta^*_h\in\RR^d$ and $\theta^*_h\in\RR^d$ are parameters. 
\end{definition}
\fi}
Recall the guarantee we have for the confidence set based on the negative log-likelihood. A blessing of the myopic follower case is that at each state $s_h$, the follower's quantal response is only a function of the policy $\pi_h$ and the reward $r_h$ with model parameter $\theta_h\in\Theta_h$ at the same step. Therefore, the negative log-likelihood for the follower's behavior at step $h$ is given by
\begin{align}
    \cL_{h}(\theta_h) = - \sum_{i=1}^T \rbr{\eta r_h^{\pi^i, \theta}(s_h^i, b_h^i) - \log \rbr{\sum_{b'\in\cB} \exp\rbr{\eta r_h^{\pi^i, \theta}(s_h^i, b')}}}. \label{eq:myopic-offline-general-MLE loss}
\end{align}
Here, the follower's reward function $r_h^{\pi^i,\theta}$ only depends on $\theta_h$. 
One can 
% write down the negative log-likelihood as \eqref{eq:myopic-offline-general-MLE loss}
% {\color{purple} 
% where $\phi_h^i(b_h)=\inp{\phi_h(s_h^i, b_h^i, \cdot)}{\pi_h^i(\cdot\given b_h^i)}_{\cA_h}$ and $\theta_h\in\RR^d$. }
% and 
construct confidence sets 
$$\CI_{h,\Theta}(\beta)=\cbr{\theta_h\in\Theta_h: \cL_h(\theta_h)\le \inf_{\theta'\in\RR^d}\cL_h(\theta')+\beta}, \quad \forall h\in[H]$$
following the same manner as \eqref{eq:behavior_model_confset}.
Here, we can let each parameter class $\Theta_h$ be a bounded subset of $\RR^d$ with $\nbr{\theta_h}_2\le B_\Theta$.  
In the following, we seperately discuss how to deal with the uncertainty in the environment model and the behavior model by adding penalties in the leader's value functions.

\paragraph{Environment Model Uncertainty Quantification.}
The value interation follows a very similar idea as \citet{zhong2021can, jin2020provably}, but the main difference is that we need to handle the uncertainty in the behavior model parameter $\theta_h$. 
We first give the update of the state-action value functions $\hat U_h(\cdot,\cdot,\cdot)$ at each step.
The idea is to exploit the linear structure $U_h(s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \TT_h W_{h+1}(s_h, a_h, b_h)=\inp{\phi_h(s_h, a_h, b_h)}{\omega_h}$ with $\omega_h = \vartheta_h^* + \sum_{s_{h+1}}\mu_h(s_{h+1})W_{h+1}(s_{h+1})$,
and solve for the state-action value function $\hat U_h(\cdot,\cdot,\cdot)$ by the following ridge regression,
\#
    & \hat\omega_h  =\argmin_{\omega\in\RR^d} \sum_{i=1}^T \rbr{\phi_h(s_h^i, a_h^i, b_h^i)^\top \omega - u_h^i - \hat W_h(s_{h+1}^i)}^2 + \nbr{\omega}_2^2, \label{eq:linear ridge}\\
    & \hat U_h(s_h, a_h, b_h)  = \phi_h(s_h,a_h,b_h)^\top \hat\omega_h - \Gamma^{(1)}_h(s_h, a_h, b_h),\nonumber
\# 
where $\Gamma^{(1)}_h$ is an uncertainty quantifier \citep{jin2021pessimism,zhong2021can} for the uncertainty in the environment model, and this term is included to ensure pessimism. Here, we can choose $\Gamma^{(1)}_h(s_h, a_h, b_h)= \tilde \cO\big(\sqrt{\phi_h(s_h, a_h, b_h)^\top\Lambda_h^{\dagger}\phi_h(s_h, a_h, b_h)}\big)$ where $$\Lambda_h=\sum_{i=1}^T \phi_h(s_h^i, a_h^i, b_h^i) \phi_h(s_h^i, a_h^i, b_h^i)^\top \allowbreak+ I_d$$ is the kernel obtained from the ridge regression problem \eqref{eq:linear ridge}. One should also be aware that the ridge regression problem \eqref{eq:linear ridge} has a closed form solution $\hat \omega_h\leftarrow \Lambda_h^{-1} (\sum_{i=1}^T \phi_h(s_h^i, a_h^i, b_h^i) (u_h^i + \hat W_{h+1}(s_{h+1}^i))) $. Plugging this closed form solution into \eqref{eq:linear ridge}, we get an update for the leader's U function.

\paragraph{Behavior Model Uncertainty Quantification.}
We next show how to deal with the behavior model uncertainty and find a good policy for the leader.
Recall the confidence set $\CI_{h,\Theta}(\beta)$ we construct for the follower's behavior model. Given the fact that the  follower is myopic, the behavior model at step $h$ is fully characterized by $\theta_h$ and the leader can decides on what policy to use simply by looking at $\hat U_h$ given by \eqref{eq:linear ridge} and $\CI_{h,\Theta}(\beta)$, and take the policy that maximizes the one-step value subject to the \emph{pessimistic} estimation, which gives us the first scheme
\begin{align}
    \textbf{S1:}\quad  \hat\pi_h(s_h)  = \argmax_{\pi_h(s_h) \in\sA} \min_{\theta_h\in\confset_{h, \Theta}(\beta)} \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi_h \otimes\nu_h^{\pi, \theta}(\cdot,\cdot\given s_h)}_{\cA\times\cB},\label{eq:scheme-1}
\end{align}
where $\hat W_h(s_h)$ is just the optimal value to the maximin problem and we remind the readers that $\sA$ is the prescription space.
% The guarantee for \eqref{eq:scheme-1} follows almost directly from \Cref{thm:Offline-MG}.
% Here, we should choose $\beta\ge T^{-1}\log(H\cN(\Theta_h, T^{-1})/\delta)$, which is given by \Cref{lem:bandit} and by further taking a union bound over $h\in[H]$. 
However, we should note that the problem is highly nonlinear and it is often hard to compute this maximin problem. Note that the inner minimization is included just to ensure pessimism. We should ask ourselves if we can turn the inner minimization problem into a maximization problem by adding penalties for the uncertainty in the behavior model $\theta_h$. By an analysis of the TV distance $D_\TV(\nu_h^{\pi,\theta_h}(\cdot\given s_h), \nu_h^{\pi,\theta_h^*}(\cdot\given s_h))$
% $D_\TV\bigrbr{\nu_h^{\pi_h,\theta_h}, \nu_h^{\pi_h, \theta_h^*}}$ very similar to the one given in \Cref{lem:performance diff informal}
specialized to the linear case,\footnote{See \Cref{lem:response diff-myopic-linear} and \Cref{cor:formal-MLE confset-linear myopic} for more details.} we are able to include a penalty term to ensure pessimism and turn the problem into a maximization for $\forall s_h\in\cS$.
\begin{align}
    \textbf{S2:}\quad  \hat\pi_h(s_h) = \argmax_{\pi_h(s_h) \in\sA, \atop \theta_h\in\confset_{h,\Theta}(\beta)} \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi_h \otimes\nu_h^{\pi, \theta}(\cdot,\cdot\given s_h)}_{\cA\times\cB} - \Gamma^{(2)}_h(s_h;\pi_h , \theta_h),\label{eq:scheme-2}
\end{align}
where $
\Gamma^{(2)}_h(s_h;\pi_h , \theta_h) = 2 B_U(\eta \xi(s_h;\pi_h, \theta_h)  + C^{(3)} \xi(s_h;\pi_h, \theta_h)^2 )
$ with 
$$C^{(3)}=\rbr{2+\exp\rbr{2\eta B_A}\eta B_A} \eta^2 \exp\rbr{2\eta B_A}/{2}, $$ 
and
\begin{equation}
    \xi(s_h;\pi_h, \theta_h) = \sqrt{\trace\Bigrbr{\bigrbr{T\Sigma_{h,\cD}^{\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}.\label{eq:Gamma^2}
\end{equation}
Here, $\Sigma_{h,\cD}^{\theta} = T^{-1} \sum_{i=1}^T \Cov_{s_h^i}^{\pi^i, \theta} \allowbreak [\phi_h^{\pi^i}(s_h^i, b_h)]$ is the data-dependent covariance matrix defined in \eqref{eq:cov matrix},  and $\Sigma_{s_h}^{\pi,\theta}=\Cov_{s_h}^{\pi,\theta} \allowbreak [\phi_h^{\pi_h}(s_h, b_h)]$ is the covariance matrix that actually only depends on $\pi_h(s_h)$ and  parameter $\theta_h$. 
% Moreover, recall that $\nbr{\theta_h}_2 \le B_\Theta$ for all $\theta_h\in\Theta_h$.
Here, we remark that $\Gamma^{(2)}_h$ is a valid uncertainty quantifier which also captures the nonlinear effect of the TV distance $D_\TV(\nu_h^{\pi,\theta_h}(\cdot\given s_h), \nu_h^{\pi,\theta_h^*}(\cdot\given s_h))$ by the second order term and the analysis of $\Gamma^{(2)}_h$ is available in \Cref{lem:response diff-myopic-linear}.
% {\main
% If we look back at \eqref{eq:scheme-2}, another way to think is viewing $\theta_h$ as parts of the leader's \say{policy} but subject to a confidence set constraint.
% \fi}
%  and the target function in the maximization problem is just a pessimistic evaluation of the leader's utility under the \say{joint policy} $(\pi_h, \theta_h)$.

Although Scheme 2 already avoids the maximin optimization in Scheme 1, we are still not satisfied since the  data-dependent covariance matrix $\Sigma_{h,\cD}^{\theta}$ also depends on the optimization variable $\theta_h$, which poses challenges in computation. 
% Also, the confidence set constraint is not easy to satisfy since $\cL_h(\cdot)$ is highly nonlinear. 
One way to deal with the problem is considering a fixed $\theta_h$. As a matter of fact, we are able to replace the $\theta_h$ in \eqref{eq:scheme-2} with the MLE estimator $\hat\theta_{h,\MLE} = \argmin_{\theta_h\in\Theta} \cL_{h}(\theta_h)$, which gives the following scheme,
\begin{align}
    \textbf{S3:}\quad  \hat\pi_h(s_h) = \argmax_{\pi_h(s_h) \in\sA}  \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi_h \otimes\nu^{\pi, \hat\theta_{h,\MLE}}(\cdot,\cdot\given s_h)}_{\cA\times\cB} - \Gamma^{(2)}_h(s_h;\pi_h , \hat\theta_{h, \MLE}). \label{eq:scheme-3}
\end{align}  
Here, the uncertainty quantifier $\Gamma_h^{(2)}$ is still needed to ensure pessimism.
To bridge the estimation error of $\hat \theta_{h,\MLE}$ in Scheme 3 to the previous two schemes, one can show that both $\Sigma_{h,\cD}^{\hat\theta_{h,\MLE}}$ and $\Sigma_{s_h}^{\pi,\hat\theta_{h,\MLE}}$ are upper and lower bounded by their correspondences with $\theta^*$ plugged in. 
The analysis is available in \Cref{prop:Hessian-ulb}.
% where we define $\Gamma^{(3)}_h(s_h;\pi_h)$ as $
% \Gamma^{(3)}_h(s_h;\pi_h) = 2 B_U(\eta \varrho  + C^{(3)} \varrho^2 )
% $ with $C^{(3)}=\rbr{1+\exp\rbr{2\eta B_A}+\eta B_A \exp\rbr{2\eta B_A}}/{2}$ and, 
% \begin{align*}
%     \varrho = \sqrt{\trace\rbr{\rbr{T\Sigma_{h,\cD}^{\hat\theta_{h,\MLE}} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \hat\theta_{h,\MLE}}}} \cdot \sqrt{2 C_{\eta}^2 \beta + 4B_\Theta^2}.
% \end{align*}
%
% {\color{purple} commented.
% where $\Psi\in \SSS_+^{d}$ can be any chosen nonnegative definite matrix
% and $\Upsilon_h(\cdot;\cdot,\cdot,\cdot)$ is defined as
% \begin{align}
%     \Upsilon_h(s_h;\pi , \theta_h, \Psi) \defeq \sqrt{\EE_{s_h}^{{\pi ,\theta_h}}\sbr{\phi_h^{\pi }(s_h,b_h)^\top {\Psi}^{\dagger}\phi_h^{\pi }(s_h,b_h)} -\bignbr{\EE_{s_h}^{{\pi ,\theta_h}}\phi_h^{\pi }(s_h,b_h)}_{{\Psi}^{\dagger}}^2}. \label{eq:Upsilon}
% \end{align}
% We remark that the higher order terms in $f$ stem from the nonlinearity of the TV distance between two different responses given by different $\theta_h$.
% Note that $\Upsilon_h(s_h;\pi , \theta_h, \Psi) = \sqrt{\trace(\Psi^\dagger \Sigma_{s_h}^{\pi , \theta_h})}$, where $\Sigma_{s_h}^{\pi ,\theta_h}=\EE_{s_h}^{\pi , \theta_h}\bigsbr{\phi_h^{\pi }{\phi_h^{\pi }}^\top} - \EE_{s_h}^{\pi ,\theta_h}\bigsbr{\phi_h^{\pi }} \cdot \EE_{s_h}^{\pi ,\theta_h}\bigsbr{\phi_h^{\pi }}^\top$.
% The definition of $\Upsilon_h$ is analogue to the previous penalty term $\Gamma^{(1)}_h$, except that $\Upsilon_h$ deals with the uncertainty in the follower's response using the weighted Laplacian $\Sigma_{s_h}^{\pi , \theta_h}$.
% % Thus, it is natural for us to choose $\Psi$
% % Note that the terms with coefficient $\exp(\eta B_A)$ in $f(\cdot)$ are actually of higher order (roughly speaking, $\Gamma^{(2)}_h=\cO(T^{-1/2})$), which suggests a $\cO(\eta\sqrt T)$ rate in our results. 
% Thus, a natural choice for $\hat\theta_h$  would be the MLE estimator $\hat\theta_{h, \MLE}$ as we wish $\hat\theta_h$ to be as close to $\theta_h^*$ as possible and the corresponding choice for $\Psi$ would be the weighted Laplacian $\Psi = \Sigma_{\cD}^{\hat\theta_{h,\MLE}} + T^{-1} I_d$ corresponding to the data \footnote{One can also consider using $\Psi=L_\cD$ which is the standard Laplacian (see \Cref{sec:follow up on bandit} for details). The only difference will be in the concentrability coefficient (as is shown in \eqref{eq:bandit-ub-2}) in the suboptimality. }. 
% See \Cref{sec:follow up on myopic offline} for more details concerning the use of the Laplacians. 
% Thanks to the guarantee we have in \Cref{lem:bandit}, the term $\bignbr{\theta_h^*-\hat\theta_h}_{\Psi}$ could be replaced by its corresponding upperbound $C_\eta^2 T^{-1}\log(H\cN(\Theta_h, T^{-1})/\delta)+T^{-1} B_\Theta$ without knowing $\theta_h^*$ by \Cref{lem:bandit} \footnote{Note that we plug in $\delta/H$ for a union upper bound over $h\in[H]$.}.
%
% The benefit of Scheme 2 is in computation since $\Psi$ and $\hat\theta_{h,\MLE}$ is fixed in this policy optimization step. 
% One issue concerning the second scheme is that we need $\Sigma_\cD^{\hat\theta_{h, \MLE}}$ to have sufficient coverage, which might not be true even if $\Sigma_{\cD}^{\theta_h^*}$ has sufficient coverage since $\hat\theta_{h,\MLE}$ is correlated with the data
% \footnote{In the worst case, the concentrability coefficient can be $\exp(2\eta B_A)$ larger than Scheme 1 due to the distribution shift $\onbr{\nu_h^{\pi , \theta_h^*}/\nu_h^{\pi , \hat\theta_h}}_\infty$.}. 
% To have a better offline guarantee, one thing we can do is to optimize the pessimistic lower bound given in \eqref{eq:scheme-2} over all possible choices of $\theta_h\in\confset_h(\beta)$ 
% instead of randomly picking one, which gives us the third scheme,
%
% \begin{align}
%     \textbf{S3:}\quad  \hat\pi  = \argmax_{\pi \in\sA}  \inp[\big]{\hat U_h(s_h, \cdot,\cdot)}{\pi \otimes\nu^{\pi , \hat\theta_h}(\cdot,\cdot\given s_h)}_{\cA\times\cB} - 2 B_U f\rbr{\Gamma^{(2)}_h(s_h;\pi , \hat\theta_h)}, 
%     % \label{eq:scheme-3}
% \end{align}  
% where the choice for $\Psi=\Sigma_\cD^{\theta_h} + T^{-1} I_d$ remains the same. Here, we should upper bound $\bignbr{\theta_h^*-\hat\theta_h}_{\Psi}$ in $\Gamma^{(2)}$ with $C_\eta^2 T^{-1}\log(H\cN(\Theta_h, T^{-1})/\delta)+T^{-1} B_\Theta^2 + \beta$ by \eqref{eq:bandit-ub-1} in \Cref{lem:bandit}. We remark that Scheme 3 will have the same kind of guarantee as Scheme 1, while no computation of maximin is required. 
% }
Finally, the \textbf{M}aximal \textbf{L}ikelihood \textbf{E}stimation with \textbf{P}essimistic \textbf{V}alue \textbf{I}teration (MLE-PVI) algorithm is summarized as the following.
\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Require {$\eta, \cD$}
    \State Initialize $\hat W_{H+1}=0$.
    \For{$h=H, H-1,\dots, 1$}
    \State Obtain kernel $\Lambda_h\leftarrow \sum_{i=1}^T \phi_h(s_h^i, a_h^i, b_h^i) \phi_h(s_h^i, a_h^i, b_h^i)^\top + I$. 
    \State Solve the ridge regression for $\hat \omega_h\leftarrow \Lambda_h^{-1} \rbr{\sum_{i=1}^T \phi_h(s_h^i, a_h^i, b_h^i) \bigrbr{u_h^i + \hat W_{h+1}(s_{h+1}^i)}} $.
    \State Update $\hat U_h(\cdot,\cdot,\cdot)\leftarrow \phi_h(\cdot,\cdot,\cdot)^\top \hat \omega_h - \Gamma_h^{(1)}(\cdot,\cdot,\cdot)$ and truncate to $[0, H-h+1]$.
    \State Compute $(\hat W_h(s_h), \hat\pi_h(s_h) )$ as the optimal value and optimal solution to S1 \eqref{eq:scheme-1}, S2 \eqref{eq:scheme-2}, or S3 \eqref{eq:scheme-3} for each $s_h\in\cS$.
    \EndFor
    \Ensure $\hat\pi=(\hat\pi_h)_{h\in[H]}$.
    \end{algorithmic}
    \caption{Offline MLE-PVI Algorithm for Myopic Follower under Linear Markov Game}
    \label{alg:PMLE}
\end{algorithm}
We have the following theoretical guarantee for \Cref{alg:PMLE}.
% \Siyu{satisfies the compliance condition, i.e., $\PP_\cD(u_h^i=u, s_{h+1}^i=s\given \tau^{i-1}, \{s_{h'}^i, \pi_{h'}^i, a_{h'}^i, b_{h'}^i\}_{h'\in[h]}) =\PP(u_h=u, s_{h+1}=s\given s_h=s_h^i, a_h=a_h^i, b_h=b_h^i)$ \citep{jin2021pessimism, zhong2022pessimistic} where $\tau^{i-1}$ is the historical data up to episode $i-1$. We remark that $\xi$ should satisfies $\xi \ge \rbr{2 C_{\eta}^2 \beta + B_\Theta^2}$.}
\begin{theorem}[{Suboptimality for MLE-PVI}]\label{thm:PMLE-VI-myopic}
    Suppose the data compliance condition \begin{align}\label{eq:data compliance}
        &\PP_\cD(u_h^i=u, s_{h+1}^i=s\given \tau^{i-1}, \{s_{h'}^i, \pi_{h'}^i, a_{h'}^i, b_{h'}^i\}_{h'\in[h]}) \nend
        &\quad =\PP(u_h=u, s_{h+1}=s\given s_h=s_h^i, a_h=a_h^i, b_h=b_h^i), \quad \forall h\in[H], i\in[T], 
    \end{align} 
    holds.
    We choose $\Gamma^{(1)}_h(\cdot,\cdot,\cdot) \ge C_1 d H \allowbreak \sqrt{\log(2d H T/\delta)}\cdot \sqrt{\phi_h(\cdot,\cdot,\cdot)^\top \Lambda_h^{-1}\phi_h(\cdot,\cdot,\cdot)}$ for some universal constant $C_1>0$ and $\beta \ge C_2 d\log(H(1+\eta T^2)\delta^{-1})$ for some universal constant $C_2>0$. For the PMLE-VI algorithm under the above three schemes, we have with probability at least $1-2\delta$ that
    \begin{align*}
        \subopt(\hat\pi) \le \sum_{h=1}^H 2\EE^{\pi^*, \nu^{\pi^*}} \sbr{\Gamma^{(1)}_h(s_h, a_h, b_h) + \Gamma^{(2)}_h(s_h;\pi_h^*, \theta_h')},
    \end{align*}
    where $\theta_h' = \theta_h^*$ for Scheme 1 and Scheme 2, and $\theta_h' = \hat\theta_{h, \MLE}$ for Scheme 3.
    % where the second term is given by the following configurations of these schemes,
    % \begin{itemize}
    %     \item[(i)] By using Scheme 1 with confidence set width $\beta\ge T^{-1}\log(H\cN(\Theta_h, T^{-1})/\delta)$, we have
    %     $$ \zeta_h = 2 B_U C_{\eta}
    %     \sqrt{\rbr{\frac 1 T\log\rbr{\frac{H \cN(\Theta, 1/T)}{\delta}} + \beta}}\cdot \EE^{\pi^*}\sbr{\Upsilon_h\rbr{s_h; \pi_h^*, \theta_h^*, \Sigma_\cD^{\theta_h^*}}}$$;
    %     \item[(ii)] By using Scheme 2 with  $\hat\theta_h = \hat\theta_{h,\MLE}$, $\Psi = \Sigma_{\cD}^{\hat\theta_{h,\MLE}} + T^{-1} I_d$, and replacing $\bignbr{\theta_h^*-\hat\theta_h}_{\Psi}$ in \eqref{eq:Gamma^2} by $\iota_h \ge \sqrt{C_\eta^2 T^{-1}\log(H\cN(\Theta_h, T^{-1})/\delta) + T^{-1}B_\Theta^2}$, we have
    %     $$ \zeta_h = 4 B_U \iota_h \EE^{\pi^*}\sbr{\Upsilon_h\rbr{s_h; \pi_h^*, \hat\theta_{h, \MLE}, \Sigma_\cD^{\hat\theta_{h,\MLE}}}} ;$$
    %     \item[(iii)] By using Scheme 3 with $\Psi = \Sigma_{\cD}^{\theta_{h}} + T^{-1} I_d$, $\beta\ge T^{-1}\log\rbr{H\cN(\Theta_h, T^{-1})/\delta}$ and replacing $\bignbr{\theta_h^*-\hat\theta_h}_{\Psi}$ in \eqref{eq:Gamma^2} by $ \iota_h \ge \sqrt{C_\eta^2 (T^{-1}\log(H\cN(\Theta_h, T^{-1})/\delta)+\beta + T^{-1}B_\Theta^2)}$, we have 
    %     $$ \zeta_h = 2 B_U \iota_h \EE^{\pi^*}\sbr{\Upsilon_h\rbr{s_h; \pi_h^*, \theta_h^*, \Sigma_\cD^{\theta_h^*}}}. $$
    % \end{itemize}
    % Here, $\Upsilon_h(\cdot;\cdot,\cdot,\cdot)$ is defined in \eqref{eq:Upsilon}
    \begin{proof}
        See \Cref{sec:proof-PMLE-VI-myopic} for a detailed proof.
    \end{proof}
\end{theorem}
We give the following corollary that characterizes the distribution shift issue. 
\begin{corollary}[Distribution shift]\label{rmk:MLE-PVI-dist-shift}
    Suppose for the leader's side, we have with high probability that 
    $
    % \begin{align}
    \Lambda_h\succeq I + c_1 T \EE^{\pi^*,\nu^{\pi^*}}[\phi_h\phi_h^\top]
    % \label{eq:MLE-PVI-coverage-1}
    % \end{align}
    $ for some constant $c_1>0$, 
    and the for the follower's side, we have with high probability 
    \begin{align}
    &I + {\ts\sum_{t=1}^T }\EE^{\pi^i, \nu^{\pi^i}} \bigsbr{(\Upsilon_h^{\pi^i} \phi_h)  (\Upsilon_h^{\pi^i} \phi_h)^\top \given s_h^i}  \succeq I + c_2 T\EE^{\pi^*, \nu^{\pi^*}} \bigsbr{(\Upsilon_h^{\pi^*} \phi_h) (\Upsilon_h^{\pi^*} \phi_h)^\top} , 
    \label{eq:MLE-PVI-coverage}
    \end{align}
    for some constant $c_2>0$,
    where $\Upsilon_h^{\pi}\phi $ is a short hand of $(\Upsilon_h^{\pi}\phi)(s_h, b_h)$. We then have for Scheme 1 and Scheme 2, 
    \begin{align*}
        {\subopt(\hat\pi) 
        \lesssim \frac{d^{3/2}H^2} {\sqrt{c_1 T}} +  \eta C_\eta H^{2}d \cdot \sqrt{\frac{1}{c_2 T}} +  e^{4\eta B_A} (\eta C_\eta)^3 H^2  d^2 \cdot  \frac{1}{c_2 T}, }
    \end{align*}
    and for Scheme 3, 
    \begin{align*}
    { \subopt(\hat\pi) 
    \lesssim \frac{d^{3/2}H^2} {\sqrt{c_1 T}} + e^{2\eta B_A} \eta C_\eta H^{2}d \cdot \sqrt{\frac{1}{c_2 T}} +  e^{8\eta B_A} (\eta C_\eta)^3 H^2  d^2 \cdot  \frac{1}{c_2 T}.}
    \end{align*}
\end{corollary}
\begin{proof}
    See \Cref{sec:proof-MLE-PVI-dist-shift} for a detailed proof.
\end{proof}
We note that \eqref{eq:MLE-PVI-coverage} is similar to the standard sufficient coverage condition in linear MDP but 
customized for linear QRE, where the operator $\Upsilon_h^{\pi^*}$  defined in \eqref{eq:Upsilon} plays a key role in the distribution shift. In particular, \eqref{eq:MLE-PVI-coverage} not only requires coverage over the trajectory induced by $\pi^*$, but also requires richness in the leader's prescription $\pi^i(s_h)$ at those states visited under $\pi^*$.
To understand this point, we note that if the leader announces the same policy $\pi^0$ for all the time, the follower always acts according to the same reward $r_h^{\pi^0} (s_h, b_h)= \la r_h(s_h,\cdot,b_h), \pi^0_h(\cdot\given s_h, b_h)\ra_\cB$, which is only a linear subspace of the reward function and the leader cannot anticipate the follower's quantal response for other policies. 

We next understand the first order terms, i.e., $\cO(T^{-1/2})$ terms in the suboptimality. The first term characterizes the leader's Bellman error, which is standard for RL problems. 
The second term characterizes the follower's first-order quantal response error (QRE). The first-order QRE term suffers from an $\exp(2\eta B_A)$ coefficient only in Scheme 3. We remark that this is because we fix the follower's quantal response using the MLE estimator in \eqref{eq:scheme-3} while Scheme 1 and Scheme 2 allow us to pick a more refined estimator $\hat\theta_h$ in the confidence set at the cost of heavier computation.
% \Cref{thm:PMLE-VI-myopic-neurips}
% We next show an ensurance result which says that the condition \eqref{eq:MLE-PVI-coverage} is not any stronger than \eqref{eq:MLE-PVI-coverage-1}. 
% % $\Gamma^{(2)}_h(s_h;\pi,\hat\theta_{h,\MLE})\le \exp\rbr{4\eta B_A} \Gamma^{(2)}_h(s_h;\pi, \theta_h^*)$. 
% We let $\L=\diag(u)-u u^\top$. We can rewrite $\Sigma_{h,\cD}^u$ and $\Sigma_{s_h}^{\pi^*, u}$ as 
% \begin{align*}
%     \Sigma_{h,\cD}^u = T^{-1}\sum_{i=1}^T \phi_h^{\pi^i}(s_h^i, \cdot) \L \phi_h^{\pi^i}(s_h^i, \cdot)^\top,\qquad  \Sigma_{s_h}^{\pi^*, u} = \phi_h^{\pi^*}(s_h, \cdot)\L \phi_h^{\pi^*}(s_h, \cdot)^\top.
% \end{align*} 
% We define $\H^{\pi,\theta_h} = \diag(\nu_h^{\pi,\theta_h}(\cdot\given s_h)) - \nu_h^{\pi,\theta_h}(\cdot\given s_h)\nu_h^{\pi,\theta_h}(\cdot\given s_h)^\top$ 
% and rewrite $\Sigma_{h,\cD}^{\theta_h^*}$ and $\Sigma_{s_h}^{\pi^*, \theta_h^*}$ as 
% \begin{align*}
%     \Sigma_{h,\cD}^{\theta_h^*} = T^{-1}\sum_{i=1}^T \phi_h^{\pi^i}(s_h^i, \cdot) \H^{\pi^i,\theta_h^*} \phi_h^{\pi^i}(s_h^i, \cdot)^\top,\qquad  \Sigma_{s_h}^{\pi^*, u} = \phi_h^{\pi^*}(s_h, \cdot) \H^{\pi^*, \theta_h^*} \phi_h^{\pi^*}(s_h, \cdot)^\top,
% \end{align*}
% We next see what guarantee we have for the the distribution shift induced by $\Gamma^{(2)}_h$.
% Suppose that 
% \begin{align*}
%     T \Sigma_{h,\cD}^{\theta_h^*}+I = \sum_{i=1}^T \Sigma_{s_h^i}^{\pi^i, \theta_h^*} + I \succeq c T \EE_\cD \sbr{\Sigma_{s_h^i}^{\pi^i, \theta_h^*}} + I, 
% \end{align*}
% with high probability, 
% where the expectation is taken for both $\pi^i$ and $s_h^i$.


% Since $\Sigma_{h,\cD}^u$ now only depends on $\phi_h^{\pi^i}$ and $s_h^i$, 
% and $\Sigma_{s_h}^{\pi^*,u}$



% \todo{to be discussed, results implication, and dependent data.}

% \Zhuoran{Corrollary}
\ifneurips
\subsubsection{Offline Learning with General Function Class}\label{sec:Offline-MG}
\fi
\ifmain
\subsection{Offline Learning with General Function Class}\label{sec:Offline-MG}
\fi
In this subsection, we carry out the offline learning scheme with general function approximation.
For the leader's side, we propose to learn the environment model by minimizing the squared loss of the Bellman error over the U function for each policy $\pi$. 
For consistency, we still assume that the follower's reward function at step $h\in[H]$ lies in some general function class parameterized by $\theta_h\in\Theta_h$. We let $\Theta=\{\Theta_h\}_{h\in[H]}$.
Suppose $\cU:\cS\times\cA\times\cB\rightarrow\RR$ is a given function class for the leader's state-action value function. 
Following the idea of Bellman-consistent pessimism \citep{xie2021bellman}, we define a loss function for the environment model error as
\begin{align}
    &\ell_h(U_h', U_{h+1}, \theta_{h+1}, \pi) \nend
    &\quad = \sum_{i=1}^T \rbr{U_h'(s_h^i, a_h^i, b_h^i) - u_h^i -  \inp[\big]{U_{h+1}(s_{h+1}^i, \cdot, \cdot)}{\pi_{h+1}\otimes \nu_{h+1}^{\pi, \theta}(\cdot, \cdot\given s_{h+1}^i)}}^2. \label{eq:myopic-offline-general-Bellman loss}
\end{align}
Intuitively, the loss $\ell_h$ going to zero means no Bellman error for the value functions between step $h$ and step $h+1$ under $\pi$ and $\nu^{\pi,\theta}$.
Note that the unknown parameters are $\theta=\{\theta_h\}_{h\in[H]}\in\Theta$ and $\{U_h\}_{h\in[H]}\in\cU^{\otimes H}$.
% where we take $\cU$ as a the function class for the leader's U function.
Based on the loss functions defined in \eqref{eq:myopic-offline-general-MLE loss} and \eqref{eq:myopic-offline-general-Bellman loss}, we can construct a confidence set for each leader's policy $\pi$ as
\begin{align}
    &\CI_{\cU, \Theta}^\pi(\beta) \nend
    &\quad= \cbr{
    (U,\theta)\in\cU^{\otimes H}\times\Theta:
    \rbr{ \ds
        \cL_h(\theta_h)-\inf_{\theta'\in\Theta_h}\cL_h(\theta') \le \beta 
    \atop \ds
        \ell_h(U_h, U_{h+1}, \theta_{h+1}, \pi) - \inf_{U'\in\cU_h} \ell_h(U', U_{h+1}, \theta_{h+1}, \pi)\le H^2\beta}, 
    \forall h\in[H]}. \label{eq:myopic-offline-general-confset}
\end{align}
The first condition in \eqref{eq:myopic-offline-general-confset} characterizes a valid and accurate confidence set for the follower's behavior model as we have done in \eqref{eq:bandit-ub-1}. For the second condition in \eqref{eq:myopic-offline-general-confset}, if certain realizability and completeness conditions are satisfied, we have guarantee on small leader's Bellman errors \citep{xie2021bellman, lyu2022pessimism}, which characterizes the uncertainty in the environment model.
Combining these two guarantees, we can therefore expect $\CI_{\cU, \Theta}^\pi (\beta)$ to be a valid and accurate confidence set for both the environment and the behavior models.
Following the principle of pessimism, we can output the policy $\hat\pi$ as, 
\begin{align}
    \hat\pi=\argmax_{\pi\in\Pi} \min_{(U, \theta)\in\CI_{\cU,\Theta}^\pi(\beta)} \EE_{s_1\sim\rho_0} \sbr{\inp[\big]{U_1(s_1, \cdot, \cdot)}{\pi_1\otimes\nu_1^{\pi, \theta}(\cdot,\cdot\given s_1)}_{\cA \times \cB}}.\label{eq:offline-MG-pi^hat}
\end{align}
% Then, we just greadily take $\hat\pi(s_h) = \argmax_{\pi_h\in\sA} \inp[\big]{\hat U_h(s_h, \cdot, \cdot)}{\pi_h\otimes\nu_h^{\pi_h, \hat\theta_h}(\cdot,\cdot\given s_h)}$.
To present our results, we first define an optimistic Bellman operator $\TT_h^{*, \theta}:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cA\times\cB)$ for the leader as
\begin{align}\label{eq:define optimistic Bellman opt}
    &\bigrbr{\TT_h^{*,\theta} f} (s_h, a_h, b_h) \nend
    &\quad = u_h(s_h, a_h, b_h) +  \EE\sbr{\max_{\pi_{h+1}(s_{h+1})\in\sA}\bigdotp{f(s_{h+1}, \cdot, \cdot)}{\pi_{h+1}\otimes\nu_{h+1}^{\pi,\theta}(\cdot,\cdot\given s_{h+1})}_{\cA\times\cB}\Biggiven s_h, a_h, b_h}.
\end{align}
Here, the expectation is taken with respect to $s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)$. 
We now summarize our offline policy learning with \textbf{M}aximum \textbf{L}ikelihood with \textbf{B}ellman \textbf{C}onsistent \textbf{P}essimism (MLE-BCP) algorithm together with its theoretical guarantee.
\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Require {$\eta, \cD$}
    \State Construct confidence set $C_{\cU,\Theta}^\pi(\beta)$ by \eqref{eq:myopic-offline-general-confset}. 
    \State Solve for the policy $\hat\pi$ with Bellman consistent pessimism in \eqref{eq:offline-MG-pi^hat}. 
    \Ensure $\hat\pi=(\hat\pi_h)_{h\in[H]}$.
    \end{algorithmic}
    \caption{Offline MLE-BCP for Myopic Follower under General Function Approximation}
    \label{alg:MLE-BCP}
\end{algorithm}
\begin{theorem}[{Suboptimality for MLE-BCP}]\label{thm:Offline-MG}
    Suppose that each trajectory in the offline dataset is independently collected.
    Suppose that the following conditions hold for model class $\Theta$ and function class $\cU$:
\begin{itemize}
    \item[(i)] (\textit{Realizability}) There exists $\theta^*\in\Theta$ such that $r_h^{\theta^*}=r_h$ for any $h\in[H]$. For any $\pi\in\Pi, \theta\in\Theta$, there exists $U\in\cU$ such that $U_h = \TT_h^{\pi,\theta} U_{h+1}$ for any $h\in[H]$;
    \item[(ii)] (\textit{Completeness}) For any $U\in\cU$ and $\pi\in\Pi, \theta\in\Theta$, there exists $U'\in\cU$ such that $U'=\TT_h^{\pi,\theta} U_{h+1}$ for any $h\in[H]$. 
\end{itemize}
    By choosing $\beta = c\cdot \max\cbr{\log(H \cN_\rho(\cY, T^{-1})\delta^{-1}), \log(H\cN_\rho(\Theta, T^{-1})/\delta)}$ for some universal constant $c>0$, where the covering number for $\Theta$ and $\cY$ are defined in \eqref{eq:cN-Theta-myopic} and \eqref{eq:cN-cY}, respectively, 
    we have for the offline algorithm \eqref{eq:offline-MG-pi^hat} that 
    \begin{align*}
        \subopt(\hat\pi) 
        &\lesssim \max_{U\in\cU,\theta\in\Theta, h\in[H]}
            \sqrt{\frac{{{\bignbr{{ U_h  -  \TT_h^{\pi^*,\theta}  U_{h+1}} }_{2, d^{\pi^*}}^2}}}{{{\bignbr{ U_{h} - \TT_{h}^{\pi^*,\theta}  U_{h + 1}}_{2,\cD}^2}}}} 
        \cdot H^2\sqrt{\beta T^{-1}}  \nend
        &\qquad + \max_{\theta\in\Theta, h\in[H]}\sqrt{ \frac{{\bignbr{\Upsilon_h^{\pi^*} (r_h^\theta - r_h^{\theta^*})}_{2, d^{\pi^*}}^2}}{\bignbr{{\Upsilon_h^{\pi^i} (r_h^\theta - r_h^{\theta^*})}}_{2,\cD}^2}} \cdot  H^2 \eta C_\eta \sqrt{\beta T^{-1}}\nend
        &\qquad  +   \max_{\theta\in\Theta, h\in[H]} {
            \frac{{\bignbr{\Upsilon_h^{\pi^*} (r_h^\theta - r_h^{\theta^*})}_{2, d^{\pi^*}}^2}}{\bignbr{{\Upsilon_h^{\pi^i} (r_h^\theta - r_h^{\theta^*})}}_{2,\cD}^2}
        }\cdot  H^2 \exp(4\eta B_A) (\eta C_\eta)^3 \beta T^{-1},
    \end{align*}
    where $C_\eta = \eta^{-1}+B_A$ and $\lesssim$ only hides universal constants.
    \begin{proof}
        See \Cref{sec:proof-offline-MG} for a detailed proof.
    \end{proof}
\end{theorem}
\Cref{thm:Offline-MG} establishes the suboptimality for offline learning the optimal policy using general function approximation.
Similar to the linear case, the first two terms characterizes the leader's Bellman error and the follower's first order quantal response error, respectively. 
The only exponential term appears in the follower's second order quantal response error term, which is roughly of order $\cO(T^{-1})$.
In particular, the concentrability coefficients that address the distribution shift issue are characterized by the ratio in both the Bellman error and the QRE error.

