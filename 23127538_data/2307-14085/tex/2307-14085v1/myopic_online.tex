%! TEXroot =main.tex
\ifmain
\section{Online Learning with Myopic Follower} \label{sec:myopic-online}
\fi 
\ifneurips
\subsection{Online Learning with Myopic Follower} \label{sec:myopic-online}
\fi
In the previous section, we address the problem of offline learning the QSE with myopic follower under both general function class and linear MDP setting. In this section, we move on to the online scenario with myopic follower. Specifically, the game proceeds as the following. At state $s_h$ in episode $t$, the leader announces her prescription $\alpha_h^t:\cB\rightarrow\Delta(\cA)$,  and the myopic follower picks an action $b_h^t$. The leader then pick an action $a_h^t\sim \alpha_h^t(\cdot\given b_h^t)$ and the state then transits to $s_{h+1}^t$. The game at episode $t$ ends at the $H$-th step and a new episode begins next. We also study the online problem both under the general function class and the linear MDP setting.
% \todo{Define operator $\varPsi_h^\pi:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cB)$ as 
% \begin{align*}
%     \rbr{\varPsi_h^\pi f}(s_h, b_h) = \dotp{\pi_h(\cdot\given s_h, b_h)}{f(s_h, \cdot, b_h)} - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{f(s_h,\cdot,\cdot)}. 
% \end{align*}
% }

\ifmain
\subsection{Online Learning for Linear Markov Game}
\label{sec:online-myopic-linear}
\fi
\ifneurips
\subsubsection{Online Learning for Linear MDP}
\label{sec:online-myopic-linear}
\fi
The case for online learning with linear MDP is not so different from the offline one, except for the fact that we have to incorporate optimism for exploration. This is nothing much but just flipping the sign of the penalties and turn them into bonuses. Following the same spirit of \Cref{sec:offline-ML}, we simply present our algorithm here for the sake of completeness. 
For updating the U function, we add a bonus to the ridge regression result,
\begin{gather}
    \hat \omega_h^t\leftarrow (\Lambda_h^t)^{-1} \rbr{\sum_{i=1}^{t-1} \phi_h(s_h^i, a_h^i, b_h^i) \bigrbr{u_h^i + \hat W_{h+1}(s_{h+1}^i)}}, \label{eq:linear ridge}\\
    \hat U_h^t(s_h, a_h, b_h) = \phi_h(s_h,a_h,b_h)^\top \hat\omega_h^t + \Gamma^{(1,t)}_h(s_h, a_h, b_h),\nonumber
\end{gather}
where we choose $\Gamma^{(1,t)}_h(s_h, a_h, b_h)= \cO(\sqrt{\phi_h(s_h, a_h, b_h)^\top\Lambda_h^{\dagger}\phi_h(s_h, a_h, b_h)})$ where $\Lambda_h=\sum_{i=1}^T  \phi_h(s_h^i, a_h^i, b_h^i)\allowbreak \phi_h(s_h^i, a_h^i, b_h^i)^\top \allowbreak+ I_d$. For updating the W function, 
we still define the negative loglikelihood as the one given in \eqref{eq:myopic-offline-general-MLE loss},
\begin{align}
    \cL_{h}^t(\theta_h) = - \sum_{i=1}^{t-1} \rbr{\eta r_h^{\pi^i, \theta}(s_h^i, b_h^i) - \log \rbr{\sum_{b'\in\cB} \exp\rbr{\eta r_h^{\pi^i, \theta}(s_h^i, b')}}}. \label{eq:online-MG-MLE loss}
\end{align}
We obtain an optimistic policy via the following two schemes,
\begin{align}
    \textbf{S4:}\quad  \hat\pi_h^t(s_h)  &= \argmax_{\pi_h(s_h) \in\sA
    \atop \theta_h\in\confset_{h, \Theta}^t(\beta)} \inp[\big]{\hat U_h^t(s_h, \cdot,\cdot)}{\pi_h \otimes\nu_h^{\pi , \theta}(\cdot,\cdot\given s_h)}_{\cA\times \cB}, \quad \forall s_h\in\cS_h,\label{eq:scheme-4}\\
    \textbf{S5:}\quad  \hat\pi_h^t(s_h)  &= \argmax_{\pi_h(s_h) \in\sA}  \inp[\big]{\hat U_h^t(s_h, \cdot,\cdot)}{\pi_h \otimes\nu^{\pi , \hat\theta_{h,\MLE}^t}(\cdot,\cdot\given s_h)}_{\cA\times \cB} + \Gamma^{(2,t)}_h(s_h;\pi_h , \hat\theta_{h, \MLE}^t). \label{eq:scheme-5}
\end{align}
which follow from \eqref{eq:scheme-1} and \eqref{eq:scheme-3}, respectively. Here, we denote by $\CI_{h,\Theta}^t(\beta) = \{\theta_h\in\Theta_h: \cL_h^t(\theta)\le \min_{\theta_h'}\cL_h^t(\theta_h')+\beta\}$ the confidence set at step $t$ with $\cL_h^t(\theta)$ defined in \eqref{eq:online-MG-MLE loss}. Moreover, $\hat\theta_{h,\MLE}^t$ is the MLE estimator that minimizes the negative log-likelihood $\cL_h^t(\cdot)$, and $\Gamma_h^{(2,t)}(s_h;\pi_h,\theta_h)=2 H(\eta \xi  + C^{(3)} \xi^2 )
$ with $C^{(3)}=\eta^2 \exp(2\eta B_A) (2+\eta B_A \exp(2\eta B_A))/2$ and, 
\begin{equation}
    \xi = \sqrt{\trace\rbr{\bigrbr{\Sigma_{h, t}^{\theta_h} + I_d}^\dagger \Sigma_{s_h}^{\pi_h , \theta_h}}} \cdot \sqrt{8 C_{\eta}^2 \beta + 4B_\Theta^2}.\label{eq:Gamma^2-online}
\end{equation}
where $\Sigma_{h,t}^{\theta_h} = \sum_{i=1}^{t-1} \Cov_{s_h^i}^{\pi_h^i, \theta_h} \allowbreak [\phi_h^{\pi_h^i}(s_h^i, b_h)]$ is the covariate matrix and $\Sigma_{s_h}^{\pi,\theta}=\Cov_{s_h}^{\pi,\theta} \allowbreak [\phi_h^{\pi_h}(s_h, b_h)]$. We summary the \textbf{M}aximal \textbf{L}ikelihood \textbf{E}stimation with \textbf{O}ptimistic \textbf{V}alue \textbf{I}teration (MLE-OVI) algorithm as the following.
\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Require {$\eta, \cD$}
    \State Initialize $\cD=\emptyset$.
    \For{$t=1,\dots, T$}
    \State Initialize $\hat W_{H+1}^t=0$
    \For{$h=H, H-1,\dots, 1$}.
    \State Obtain kernel $\Lambda_h^t\leftarrow \sum_{i=1}^{t-1} \phi_h(s_h^i, a_h^i, b_h^i) \phi_h(s_h^i, a_h^i, b_h^i)^\top + I$. 
    \State Solve the ridge regression for $\hat \omega_h^t\leftarrow \rbr{\Lambda_h^t}^{-1} \rbr{\sum_{i=1}^{t-1} \phi_h(s_h^i, a_h^i, b_h^i) \bigrbr{u_h^i + \hat W_{h+1}^t (s_{h+1}^i)}} $.
    \State Update $\hat U_h^t(\cdot,\cdot,\cdot)\leftarrow \phi_h(\cdot,\cdot,\cdot)^\top \hat \omega_h^t + \Gamma_h^{(1, t)}(\cdot,\cdot,\cdot)$ and truncate to $[0, H-h+1]$.
    \State Compute $\hat W_h^t(s_h)$ and $\hat\pi_h^t(s_h)$ as the optimal value and optimal solution to S4 \eqref{eq:scheme-4} or S5 \eqref{eq:scheme-5}.
    \EndFor
    \State Announce $\hat\pi^t$ and collect a trajectory $\tau^t = \{(s_h^t, a_h^t, b_h^t, \hat\pi_h^t)\}_{h\in[H]}$. 
    \State $\cD\leftarrow \cD\cup \{\tau^t\}$. 
    \EndFor
    % \Ensure $\hat\pi=(\hat\pi )_{h\in[H]}$.
    \end{algorithmic}
    \caption{Online MLE-OVI for Myopic Follower under Linear Markov Game}
    \label{alg:MLE-OVI}
\end{algorithm}

We also provide theoretical guarantee for the MLE-OVI algorithm.
\begin{theorem}[{Regret for MLE-OVI}]\label{thm:Online-ML}
    We choose 
    $$\Gamma^{(1, t)}_h(\cdot,\cdot,\cdot) = C_1 d H \allowbreak \sqrt{\log(2d H T^2/\delta)}\cdot \sqrt{\phi_h(\cdot,\cdot,\cdot)^\top (\Lambda_h^t)^{-1}\phi_h(\cdot,\cdot,\cdot)}$$ for some universal constant $C_1>0$ and $\beta = C_2 d\log(HT(1+\eta T^2)\delta^{-1})$ for some universal constant $C_2>0$. 
    For Scheme 4, 
\begin{align*}
    \Reg(T)\lesssim d H^2 \sqrt{d T} + \eta C_\eta H^2 d \sqrt{T} + \exp(4\eta B_A) (\eta C_\eta)^3 H^2 d^2 \log T, 
\end{align*}
and for Scheme 5, 
\begin{align*}
    \Reg(T)\lesssim d H^2 \sqrt{d T} + \exp(4\eta B_A)\eta C_\eta H^2 d \sqrt{T} + \exp(8\eta B_A) (\eta C_\eta)^3 H^2 d^2 \log T. 
\end{align*} 
    \begin{proof}
        See \Cref{sec:proof-Online-ML} for a detailed proof.
    \end{proof}
\end{theorem}

We observe from \Cref{thm:Online-ML} that the the optimistic value iteration methods proposed by both Scheme 4 and Scheme 5 achieve sublinear online regret. 
Scheme 5 suffers from an additional exponential term in the second term, which corresponds to the first order quantal response error. 
What happens here resembles the offline suboptimality bound for Scheme 3 in \Cref{rmk:MLE-PVI-dist-shift} and the reason is quite similar given that we directly use the MLE estimator $\hat\theta^t_{h, \MLE}$ for Scheme 5 instead of exploiting the confidence set for $\theta$.




\ifmain
\subsection{Online Learning with General Function Class}\label{sec:online-myopic-general}
\fi 
\ifneurips
\subsubsection{Online Learning with General Function Class}\label{sec:online-myopic-general}
\fi 
We develop an online learning algorithm very similar to the one given in \Cref{sec:Offline-MG} for the offline general function class, despite that we use optimism in leader's policy learning.
However, for the leader's Bellman loss which is defined by \eqref{eq:myopic-offline-general-Bellman loss} in the offline case, we use a slightly different version that directly incorporates the rule of optimism, 
\begin{align}
    &\!\!\! \ell_h^t(U_h', U_{h+1}, \theta_{h+1}) \nend
    &\!\!\!\quad = \sum_{i=1}^{t-1}  \rbr{U_h'(s_h^i, a_h^i, b_h^i) \!-\! u_h^i - \!\!\!\! \max_{\pi_{h+1}(s_{h+1})\in\sA}\inp[\big]{U_{h+1}(s_{h+1}^i, \cdot, \cdot)}{\pi_{h+1}\otimes \nu_{h+1}^{\pi, \theta}(\cdot, \cdot\given s_{h+1}^i)}_{\cA\times\cB}}^2.\label{eq:online-MG-bellman loss}
\end{align}
Here, there is a slight abuse of notation and we distinguish this loss from its correspondence in the offline case by the superscript $t$. 
We would like to remark that this loss function assembles the one used in GOLF of \citet{jin2021bellman}.
We build the confidence set for both the environment model and the behavior model as
\begin{align}
    &\CI_{\cU, \Theta}^t(\beta) \nend
    &\quad= \cbr{
    (U,\theta)\in\cU^{\otimes H}\times\Theta:
    \rbr{ \ds
        \cL_h^t(\theta_h)-\inf_{\theta_h'\in\Theta_h}\cL_h^t(\theta_h') \le \beta 
    \atop \ds
        \ell_h^t(U_h, U_{h+1}, \theta_{h+1}) - \inf_{U'\in\cU_h} \ell_h^t(U', U_{h+1}, \theta_{h+1})\le H^2\beta}, 
    \forall h\in[H]}. \label{eq:myopic-online-general-confset}
\end{align}
Following the principle of optimism, we can output a pair of optimistic model parameter, 
\begin{align}
    (\hat U^t, \hat\theta^t)=\argmax_{\pi_1\in\Pi_1 \atop (U, \theta)\in\CI_{\cU,\Theta}^t(\beta)} \EE_{s_1\sim\rho_0} \sbr{\inp[\big]{U_1(s_1, \cdot, \cdot)}{\pi_1\otimes\nu_1^{\pi, \theta}(\cdot,\cdot\given s_1)}_{\cA\times\cB}}.\label{eq:online-MG-opt-parameter}
\end{align}
Specifically, for any given state $s_h$, an optimistic policy is then given greedily by 
\begin{align}
    \hat\pi^t(s_h)=\argmax_{\pi_{h}(s_h)\in\sA}\inp[\big]{U_{h}(s_{h}, \cdot, \cdot)}{\pi_{h}\otimes \nu_{h}^{\pi, \theta}(\cdot, \cdot\given s_{h})}_{\cA\times\cB}. \label{eq:online-MG-hat pi}
\end{align}
We summarize the above \textbf{M}aximum \textbf{L}ikelihood \textbf{E}stimation with \textbf{G}lobal \textbf{O}ptimism based on \textbf{L}ocal \textbf{F}itting (MLE-GOLF) algorithm as the following.
\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Require {$\eta$}
    \State Initiate $\cD =\emptyset$.
    \For{$t=1,\dots,T$}
    \State Construct confidence set $\CI_{\cU, \Theta}^t(\beta)$ by \eqref{eq:myopic-online-general-confset}. 
    \State Solve for $\hat U^t, \hat\theta^t$ by \eqref{eq:online-MG-opt-parameter}.
    \State Solve for the greedy policy $\hat\pi^t$ by \eqref{eq:online-MG-hat pi}. 
    \State Deploy $\hat\pi^t$ and collect a trajectory $\tau^t = \{(s_h^t, a_h^t, b_h^t, \hat\pi_h^t)\}_{h\in[H]}$. 
    \State $\cD\leftarrow \cD\cup \{\tau^t\}$. 
    \EndFor
    \end{algorithmic}
    \caption{Online MLE-GOLF for Myopic Follower under General Function Approximation}
    \label{alg:MLE-GOLF}
\end{algorithm}
% \todo{
% \begin{itemize}
%     \item function class $\cG_L = \cbr{U_h-\TT_h^{\theta} U_{h+1}: U\in\cU, \theta\in\Theta}$ defined on $\cX=\cS\times\cA\times\cB$, which is bounded by $B_{\cG_L}\le 2B_U$; the class of probability measures $\sP_L = \cbr{\rho\in\Delta(\cS\times\cA\times\cB): \rho(\cdot)=\PP^\pi((s_h, a_h, b_h)=\cdot)}$ with $\rho^i(\cdot) = \PP^{\hat\pi^i}((s_h, a_h, b_h)=\cdot)$; Under these definitions, we let $d_{\cG_L} = \dim_\DE\rbr{\cG_L, \sP_L, T^{-1/2}}$ be the distributional Eluder dimension for $\cG_L$.
%     \item function class defined on $\cX=\cS\times\cB\times\sA$,
%     \begin{align*}
%     \cG_F = \cbr{g:\cX\rightarrow \RR: \exists \theta\in\Theta, g(s_h, b_h, \pi)= \rbr{\varPsi_h^{\pi}\rbr{r_h^\theta - r_h}}(s_h, b_h)};
%     \end{align*}
%     which is bounded by $4B_r$ where $B_r$ boundes the reward function induced by $\theta\in\Theta$. Linear operator $\varPsi_h^\pi:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cB)$ defined as 
% \begin{align*}
%     \rbr{\varPsi_h^\pi f}(s_h, b_h) = \dotp{\pi_h(\cdot\given s_h, b_h)}{f(s_h, \cdot, b_h)} - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{f(s_h,\cdot,\cdot)}. 
% \end{align*}
% We let $d_{\cG_F}=\dim_\E(\cG_F, T^{-1/2})$ be the Eluder dimension of $\cG_F$.
% \end{itemize}
% }

We identify two function classes whose complexities determine the online learning hardness. The first one is the Bellman residuals $\cG_L:\cS\times\cA\times\cB\rightarrow \RR$ defined as 
$$\cG_L=\{U_h-\TT_h^{*,\theta} U_{h+1}, U\in\cU, \theta\in\Theta, h\in[H]\}, $$
where $\TT_h^{*,\theta}:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cA\times\cB)$ is the Bellman optimality operator defined in \eqref{eq:define optimistic Bellman opt}.
The second one is the QRE defined in \eqref{eq:QRE} where we define the class of QREs $\cG_F:\cS\times\cB\rightarrow \RR$  as
$$
\cG_F =  \{\Upsilon_h^{\pi}(r_h^{\theta} - r_h), \pi\in\Pi, \theta\in\Theta, h\in[H]\}, 
$$
where recall the operator $\Upsilon_h^\pi:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cB)$ defined as 
\begin{align*}
    \rbr{\Upsilon_h^\pi f}(s_h, b_h) = \dotp{\pi_h(\cdot\given s_h, b_h)}{f(s_h, \cdot, b_h)} - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{f(s_h,\cdot,\cdot)}.
\end{align*}
In the sequel, we let $\dim(\cG_L)=\dim_\E(\cG_L, T^{-1/2})$ and $\dim(\cG_F)=\dim_\E(\cG_F, T^{-1/2})$ be the eluder dimensions for these two function classes. \footnote{See \Cref{sec:eluder dimension} for definition of the eluder dimension.} 

\begin{theorem}[{Regret for MLE-GOLF}]\label{thm:Online-MG}
    Suppose that the following conditions hold for model class $\Theta$ and function class $\cU$:
\begin{itemize}
    \item[(i)] (\textit{Realizability}) There exists $\theta^*\in\Theta$ such that $r_h^{\theta^*}=r_h$ for any $h\in[H]$. For $\theta^*\in\Theta$, there exists $U\in\cU$ such that $U_h = U_h^{*}$ for any $h\in[H]$;
    \item[(ii)] (\textit{Completeness}) For any $U\in\cU$ and $\theta\in\Theta$, there exists $U'\in\cU$ such that $U'=\TT_h^{*,\theta} U$ for any $h\in[H]$. 
\end{itemize}
    By choosing $\beta\ge c \max\cbr{ \log(HT\cN_\infty(\cZ, T^{-1})\delta^{-1}), \log(HT \cN_\rho(\Theta, T^{-1})/\delta)}$ for some universal constant $c>0$, where $\cN_\rho(\cZ,\epsilon)$ and $\cN_\rho(\Theta, \epsilon)$ are the maximal (over $h\in[H]$) $\epsilon$-covering number defined in \eqref{eq:cN-cZ}, \eqref{eq:cN-Theta-myopic} for the joint function class $\cZ_h = \Theta_{h+1}\times\cU^2$ and  $\Theta_h$, respectively. 
    We have for the online algorithm that 
    \begin{align*}
        \Reg(T)\lesssim H^2 \sqrt{\dim(\cG_L)\beta T} + H^2 \eta C_\eta \sqrt{\dim(\cG_F)\beta T} + H^2(\eta C_\eta)^3\exp(4\eta B_A) \beta \log T, 
    \end{align*}
    where $\lesssim$ only hides universal constants.
    \begin{proof}
        See \Cref{sec:proof-Online-MG} for a detailed proof.
    \end{proof}
\end{theorem}

\Cref{thm:Online-MG} characterizes the online learning complexity in terms of the eluder dimensions of function classes $\cG_L$ and $\cG_F$.
In particular, we do not suffer from any $\exp(2\eta B_A)$ coefficient in the $\sqrt{T}$ term thanks to optimism, though the $\log(T)$ term still has $C^{(3)}=\cO(\eta B_A\exp(2\eta B_A))$. 
We remark that when applied to the linear function approximation, we can reproduce the result in \Cref{thm:Online-ML} with $\dim(\cG_L) \lesssim d$ and $\dim(\cG_F) \lesssim d$. 