
%!TEX root =main.tex
\section{Conclusion}

In this work, we study the problem of learning a quantal Stackelberg equilibrium (QSE) in a leader-follower Markov game, where the follower always adopts the quantal response policy against the leader. 
Moreover, in this game,  the leader cannot observe the follower's rewards, and has to infer how the follower reacts to her announced policy from follower's actions. 
We propose sample efficient algorithms for both the online and offline settings where the follower can be either myopic or  farsighted. 
Besides, we allow the state space to be very large by incorporating   function approximators in value function or transition model estimation. 
Our algorithms are based on a combination of (i) the principle of pessimism/optimism in the face of uncertainty, (ii) model-free or model-based RL for leader's problem, and (iii) maximum likelihood estimation for follower's quantal response mapping. 
We prove that our algorithms are sample efficient. 
Moreover, when specialized to the case with a myopic follower and linear function approximation, our algorithms are also computationally efficient. 
Our theoretical analysis features a novel performance difference lemma that is tailored to the bilevel structure of learning QSE. 
We believe such a result would be useful in other multi-agent RL problems involving inferring the behaviors of other agents. 

