\newpage

\section{Additional Background and Notation}\label{sec:app-notations}
We stick to the following notations and their shorthands in the appendix.
\begin{table}[H]
\centering
{
    \setlength\doublerulesep{1pt}

\ifmain\begin{tabular}{p{3cm} p{11cm}}\fi
\ifneurips\begin{tabular}{p{4cm} p{8cm}}\fi
    \toprule[2pt]\midrule[0.5pt]
    Notations & Interpretations \\ \toprule[1.5pt]
    $u_h, r_h, P_h$ & the leader's and the follower's rewards and the transition kernel for the MDP.\\\midrule
    $M$, $M^*$, $\cM$ & a model for the leader's and follower's rewards and also the transition kernel. $M^*$ is the true model. The  model class is $\cM$.\\\midrule
    $\theta$, $\Theta$ & We use $\theta$ to denote the part of the model (follower's reward if myopic, and follower's reward and transition kernel if nonmyopic) that determines the follower's quantal response mapping $\pi\rightarrow\nu^\pi$. The model class is $\Theta$.
    \\\midrule
    $\pi, \nu^{\pi, M}, \nu^{\pi,\theta}$ & $\pi$ is the leader's policy, $\nu^{\pi,M}$ is the follower's response under $(\pi, M)$, $\nu^{\pi, \theta}$ has the same meaning as $\nu^{\pi,M}$.
    \\\midrule
    $J(\pi,M)$ & the total utility collected by the leader as the follower responses is $\nu^{\pi, M}$.\\\midrule
    $Q_h^{\pi, M}, V_h^{\pi, M}, A_h^{\pi, M}$, $U_h^{\pi,M}, W_h^{\pi,M}$ & (Q, V, A)-functions for the follower and the (U, W) functions for the leader under $(\pi, \nu^{\pi, M}, P^M, r^M, u^M)$ at step $h$.\\\midrule
    % $Q_h, V_h, A_h$ or $U_h, W_h$ & (Q, V, A)-functions for the follower/leader under some $\pi$ and the true model $M^*$ at step $h$, where $\pi$ is specified in the context.\\\midrule[.25pt]
    % $\tQ_h, \tV_h, \tA_h$ or $\tilde U_h, \tilde W_h$ & (Q, V, A)-functions for the follower/leader under some $\pi$ and an alternative model $\tilde M$ at step $h$, where $\pi$ is specified in the context.\\\midrule[.25pt]
    $\EE^{\pi, M}, \EE$ & the expectation $\EE^{\pi, M}$ is taken with respect to the trajectory generated by $(\pi,\nu^{\pi, M}, T^M)$, $\EE$ is a always a short hand for $\EE^{\pi, M^*}$.\\\midrule
    $\EE_{s_h, b_h}, \EE_{s_h}$ & $\EE_{s_h, b_h}$ is a short hand of $\EE^{\pi, M^*}\sbr{\cdot\given s_h, b_h}$ and $\EE_{s_h}$ is a short hand of $\EE^{\pi, M^*}\sbr{\cdot\given s_h}$.\\\midrule
    % $\nu_h, \tilde\nu_h$ & the follower's best response under $(\pi, M^*)$ and $(\pi, \tM)$, respectively, where $\pi$ is specified in the context.\\\midrule
    % $P_h,  \tilde P_h$ & the transition kernel/operator given by $M^*$ and $\tilde M$, respectively.\\\midrule
    $r_h^\pi(s_h, b_h)$ & $r_h^\pi = \inp[]{r_h(s_h, \cdot, b_h)}{\pi_h(\cdot\given s_h, b_h)}_\cA$ 
    % and the same for $ \tilde r$.
    \\\midrule
    $ P_h^\pi(s_{h+1}\given s_h, b_h)$ & $P_h^\pi = \inp[]{P_h(s_{h+1}\given s_h, \cdot, b_h)}{\pi_h(\cdot\given s_h, b_h)}_\cA$.\\\midrule
    $\pi^*$, $\pi^{*,M}$ & $\pi^*$ is the best policy for the leader, and $\pi^{*, M}$ is the best policy for the leader under model $M$, i.e. $\pi^{*,M} = \argmax_{\pi\in\Pi} J(\pi, M)$,\\\midrule
    $\Upsilon_h^\pi$ & the operator for the QRE defined in \eqref{eq:Upsilon}
    \\\midrule
    $T_h^{\pi,\nu}$, $T_h^{\pi}$, $T_h^{\pi, \theta}$, $T_h^{*}$, $T_h^{*, \theta}$ & $T_h^{\pi,\nu}$, $T_h^{\pi}$, $T_h^{\pi, \theta}$ are integral operators defined in \eqref{eq:operator_T_pi_nu}, \eqref{eq:operator_T_pi}, \eqref{eq:operator_T_pi_theta}, respectively. $T_h^*$ and $T_h^{*,\theta}$ are optimality integral operators defined in \eqref{eq:greedy} and \eqref{eq:optimistic-operator}, respectively.
    \\\midrule
    $\TT_h^{\pi}$, $\TT_h^{\pi,\theta}$, $\TT_h^*$, $\TT_h^{*,\theta}$ & 
    These are Bellman (optimality) operators defined in \eqref{eq:TT_pi}, \eqref{eq:TT_pi_theta}, \eqref{eq:bellman_opt_oper}, \eqref{eq:TT_*_theta}, respectively.\\\midrule
    $B_A$ & $B_A$ upper bounds the follower's Q-, V-, and A-functions and is specified in \eqref{eq:define_BA}.\\

    \bottomrule[2pt]
\end{tabular}
}
\caption{Table for notations.}\label{tab:notation}
\end{table}



% \begin{table}[h!]
% \centering
% {\setlength\doublerulesep{1pt}
% \begin{tabular}{p{4cm} p{8cm}}
%     \toprule[2pt]\midrule[0.5pt]
%     Notations & Interpretations \\ \toprule[1.5pt]
%         \\
%     \midrule\\

%     \bottomrule[2pt]
% \end{tabular}
% }
% \caption{Table for notations.}\label{tab:}
% \end{table}


% \newpage

% {\ifmain
% \todo{In the model based case, I think that the kernel MDP (in the general form) can also have small quantal eluder dimension. This is because given the offline data, we can already gurantee small TV distance in the transition kernel with respect to the \say{data distribution}, it doesn't matter what $V_{l+1}$ we plug in for the QRE. In fact, we always switch to $V_{l+1}^{\pi^i, M^t}$ in \eqref{eq:OnN-1st-g-sq} of \Cref{sec:proof-farsighted MDP} for putting a guarantee on $M^t$ with a cost of $\beta$.
% If we use kernel $\phi(s, a, b, s')$, we just imagine that $V_{l+1}^{\pi^t, M^t}$ is determined by $\pi^t$ (given that $\pi^t$ and $M^t$ is a pair). Therefore, we have $\sum_{i\in[t-1]}\la \sum_{s'}\PP^{\pi^i}_l(s, a, b, s')\cdot V_{l+1}^{\pi^i, M^i}(s'), \mu^t- \mu^*\ra^2 \le \beta$, which gives small error in $\la \sum_{s'}\PP^{\pi^t}_l(s, a, b, s')\cdot V_{l+1}^{\pi^t, M^t}(s'), \mu^t- \mu^*\ra$
% }
% This is \say{trick of switching}, because the data provides us with a stronger TV guarantee.

% \todo{Can we extend the model based algorithm to a model free version with general approximation version?
% The answer is yes, because we just introduce an additional second order error in the transition for each step, which amounts to no more than $\log(T)$.
% }\fi
% }



In the following, we introduce additional background knowledge that will be useful in the proofs. 
% We define the quantal Bellman operator $\BB_h^\pi:\cF(\cS\times\cB)\rightarrow \cF(\cS\times\cB)$ as
% \begin{align}
%     (\BB_h^\pi g)(s_h, b_h) = r_h^\pi(s_h, a_h) + \gamma \eta^{-1} \EE_{s_{h+1}\sim P_h^\pi(\cdot\given s_h, b_h)}\sbr{\log\rbr{\sum _{b'\in\cB} \exp\rbr{\eta g(s_{h+1}, b' )} }}. \label{eq:bellman_operator_follower} 
% \end{align}
Let $\Theta$ be the set of model parameters that determines the follower's quantal response model. 
In particular, when the follower is myopic, i.e., $\gamma = 0$, 
$\theta$ is the parameter of the follower's reward function.
In this case, we assume that there exists some $\theta^* \in \Theta$ such that $r_h = r_h^{\theta^*}$ for all $h \in [H]$. 
In addition, when the follower is farsighted with $\gamma >0$, we parameterize both the follower's reward function and transition kernel and write $\{ r^{\theta}, P^{\theta} \}_{\theta \in \Theta}$. 
In this case, we assume that there exists $\theta^* \in \Theta$ such that 
$r_h = r_h^{\theta^*}$ and $P_h = P_h^{\theta^*}$ for all $h \in [ H ] $. 
We note that each $\theta$ uniquely specifies a quantal response mapping according to \eqref{eq:quantal_response_policy}.
We let $\nu^{\pi, \theta} $ denote the quantal response of a  leader's policy $\pi$ under the model with parameter $\theta$. 

Moreover, recall that we denote the leader's state- and action-value function by $W$ and $U$ respectively. 
To simplify the notation, 
for any leader's policy $\pi$ and follower's policy $\nu$,  we define integral operators $ T_h^{\pi, \nu }, T_h ^{\pi} \colon  \cF(\cS\times\cA \times \cB)\rightarrow \cF(\cS ) $ by letting 
\#
\bigl ( T_h^{\pi , \nu }   U_{h} \bigr)(s_{h}) = \la U_{h}(s_{h}, \cdot, \cdot), \pi_{h}\otimes \nu_{h}  (\cdot, \cdot\given s_{h})\ra _{\cA\times \cB}, \label{eq:operator_T_pi_nu}\\
\bigl ( T_h^{\pi }   U_{h} \bigr)(s_{h}) = \la U_{h}(s_{h}, \cdot, \cdot), \pi_{h}\otimes \nu_{h}^{\pi} (\cdot, \cdot\given s_{h})\ra _{\cA\times \cB} ,\label{eq:operator_T_pi} 
\#
where $\nu^{\pi} $ is the quantal response of $\pi$ under the true model. 
Then the quantal  Bellman operator $\TT^{\pi}_h$ defined in \eqref{eq:bellman_operator_leader}  can be equivalently written as 
\begin{align}\label{eq:TT_pi}
\bigl( \TT_h^{\pi } f \bigr)  (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE_{s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)} \sbr{\bigl ( T_h^{\pi} f\bigr) (s_{h+1})}.
\end{align}
Similarly, for any $\theta \in \Theta$, we define $T^{\pi, \theta}_h $ by letting 
\begin{align}\label{eq:operator_T_pi_theta}
\big ( T_h^{\pi, \theta} U_{h} \bigr) (s_{h}) = \la U_{h}(s_{h}, \cdot, \cdot), \pi_{h}\otimes \nu_{h}^{\pi,\theta}(\cdot, \cdot\given s_{h})
\ra _{\cA\times \cB} . 
\end{align}
We similarly define a quantal Bellman operator $\TT_h^{\theta}$ by letting 
\begin{align}\label{eq:TT_pi_theta}
    \bigl( \TT_h^{\pi,\theta} f \bigr)  (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE_{s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)} \bigl[ \bigl ( T_h^{\pi,\theta} f\bigr) (s_{h+1})\bigr] .
\end{align}

{\noindent \bf Bellman Optimality  Equation in Myopic Case.}
Furthermore, recall that we let $\Pi = \{ \Pi_h \}_{h\in [ H]} $ denote the class of leader's policies. 
Specifically
 for the case where the follower is 
 myopic, we define an operator 
$T_h^*  \colon \cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS)$ as 
\begin{align}
   \bigl( T_{h}^*  f\bigr) (s_h) = \max_{\pi_h\in\Pi_h} \dotp{f(s_h,\cdot,\cdot)}{\pi_h\otimes \nu^{\pi}_h(\cdot, \cdot\given s_h)}_{\cA\times \cB }. \label{eq:greedy}
\end{align}
In other words, \eqref{eq:greedy} can be regarded as finding the  ``greedy'' policy of the leader, assuming leader's reward function is $f$ and the follower is myopic. 
Based on $T_h^*$, we define the Bellman optimality operator $\TT_h^*\colon \cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cA\times\cB)
$ 
by letting 
\begin{align}
    \bigl ( \TT_h^{*} f \bigr )  (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE_{s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)} \bigl[ \bigl ( T_{h+1}^{*} f\bigr) (s_{h+1}) \bigr] ,\label{eq:bellman_opt_oper}
\end{align}
Note that we let $\pi^*$  denote the optimal policy of the leader.
Let $W^*$ and $U^*$ denote $W^{\pi^*}$ and $U^{\pi^*}$ respectively, which are the value functions of the leader at QSE. 
Using the Bellman operator defined in \eqref{eq:bellman_opt_oper}, we obtain the \emph{Bellman optimality equation} for the leader: 
\#\label{eq:bellman_opt_eqn}
U_h^* (s,a,b) = \bigl ( \TT_h^* U_{h+1}^* \bigr )(s,a,b), \qquad 
W_h^*(s) =  \bigl( T_{h}^*  U_h^* \bigr) (s),\qquad \forall (s,a,b,h) ,
\# 
where we stick to the convention that $U_{H+1} ^* = \mathbf{0}$. 

The key message conveyed in the  Bellman equation \eqref{eq:bellman_opt_eqn}  is that, when the model is known and the follower is myopic, the optimal policy of the follower can be computed via dynamic programming, which is similar to the case of an MDP.  
However, such a benign property cannot be extended to the farsighted case where $\gamma >0 $. 
The main reason for such dichotomy is that, in the myopic case, for each time step $h$, the quantal response $\nu_h^{\pi}$ only depends on $\pi$ through $\pi_h$. 
Thus, whenIn this case, 
we can regard the leader's problem with an auxiliary MDP with the action space at step $h$ being $\Pi_h $. 
The reward function $\tilde r_h$ and the transition kernel $\tilde P_h$ of such an auxiliary MDP are given by  
\$
\tilde r_h (s_h, \pi_h) & =  \la u_h(s_h, \cdot, \cdot), \pi_h \otimes \nu_h^{\pi}  (\cdot, \cdot \given s_h) \ra _{\cS \times \cA }, \notag \\
\tilde P_h (s_{h+1} \given s_h , \pi_h ) & =  \la  (s_{h+1}  \given s_h, \cdot, \cdot ) , \pi_h \otimes \nu_h^{\pi}  (\cdot, \cdot \given s_h) \ra _{\cS \times \cA },  
\$
where $\pi_h \in \Pi_h$ is an action,   and $u_h$ is the reward of the leader and $P_h$ is the transition kernel of the leader-follower Markov game. 
The optimal policy $\pi^*$ of the leader is exactly the optimal policy of the auxiliary MDP, and thus can be found via dynamic programming. 
In contrast, for a farsighted follower,
for each timestep $h$, 
the quantal response policy $\nu_h^{\pi}$ 
depends on $\pi$ through $\{ \pi_\ell  \}_{\ell \geq h }.$
As a result, the quantal response is a complicated mapping of the leader's policy, which prohibits dynamic programming.\footnote{The  \emph{feedback Stackelberg equilibrium} for leader-follower games with farsighted followers can be solved via dynamic programming. See, e.g., \cite{bacsar1998dynamic} for details. Our notion of Stackelberg equilibrium corresponds to the global Stackelberg equilibrium \citep{bacsar1998dynamic}, which does not admits a dynamic programming formulation in general.} 

Finally, we can also define  the Bellman optimality operator when the follower is critic and has reward $r^{\theta} $. 
In this case, similar to $T_h^*$ defined in \eqref{eq:greedy}, 
we define an operator $T_h^{* , \theta}$   by letting 
\begin{align} \label{eq:optimistic-operator}
\bigl(  T_{h}^{*, \theta } f\bigr) (s_h) = \max_{\pi_h\in\Pi_h} \big \la f(s_h,\cdot,\cdot), \pi_h\otimes \nu^{\pi, \theta}_h (\cdot, \cdot\given s_h)\bigr \ra _{\cA\times \cB },
\end{align}
where $\nu^{\theta} _h $ is the quantal response based on  reward $r^{\theta}_h $.  
Based on $T_{h}^{*, \theta} $, we define  
the  Bellman optimality operator for the leader 
$\TT_h^{*, \theta}:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cA\times\cB)$ by letting 
\begin{align}\label{eq:TT_*_theta}
    \bigl ( \TT_h^{*, \theta} f \bigr )  (s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \EE_{s_{h+1}\sim P_h(\cdot\given s_h, a_h, b_h)} \bigl[ \bigl ( T_{h+1}^{*, \theta}f\bigr) (s_{h+1}) \bigr] ,
\end{align}
The Bellman optimality equation corresponding to $\TT^{*, \theta}$ can be established similar to \eqref{eq:bellman_opt_eqn}. 


\vspace{5pt} 
{\noindent \bf Additional Notation.} 
In the sequel, we let $B_A$ be the global upper bound for the follower's advantage function. We derive an explicit form for $B_A$ in the sequel.
Specifically, for the follower's true advantage functions, we have $\nbr{A_h^\pi}_\infty = \nbr{Q_h^\pi - V_h^\pi}_\infty$.
Here, an upper bound for $V_h^\pi$ can be derived by
\begin{align*}
    \abr{V_h^\pi(s_h)}  = \biggabr{\eta^{-1} \log\biggrbr{\sum_{b_h\in\cB} \exp\rbr{\eta Q_h^\pi(s_h, b_h)}} } \le \eta^{-1} \log\abr{\cB} + \nbr{Q_h^\pi(s_h,\cdot)}_\infty,
\end{align*}
where $\abr{\cB}$ is the number of follower's actions if $\cB$ is discrete, and $\abr{\cB}$ is the length of $\cB$ on the real line for continuous action case. 
Moreover, in the continuous action case, we can always normalize $\cB$ to the unit interval on the real line, i.e., $\cB=[0,1]$, which helps us get rid of the $\eta^{-1}\log|\cB|$ term.  
In the sequel, we just keep this term in the upper bound and remind the readers what $\abr{\cB}$ stands for here.
Therefore, we have that 
\begin{align*}
    \nbr{Q_h^\pi}_\infty \le \nbr{r_h^\pi}_\infty  + \gamma \nbr{V_{h+1}^\pi}_\infty\le \rbr{\gamma \eta^{-1} \log |\cB| + 1} + \gamma \nbr{Q_{h+1}^\pi}_\infty.
\end{align*}
By a recursive argument, we have for the Q function that 
\begin{align}
    \nbr{Q_h^\pi}_\infty \le \eff_{H-h+1}(\gamma)\cdot \rbr{\gamma \eta^{-1}\log\abr{\cB}+1} \eqdef B_Q, \quad \forall h\in[H], \pi\in\Pi, \label{eq:def B_Q}
\end{align}
where we define $\eff_h(\gamma) = (1-\gamma^h)/(1-\gamma)$ as the effective horizon for the follower truncated for $h$ steps and $B_Q$ as the upper bound for the follower's Q function.
Therefore, for the follower's advantage function, we have 
\begin{align*}
    \nbr{A_h^\pi}_\infty &= \nbr{Q_h^\pi - V_h^\pi}_\infty \nend
    &= \max_{(s_h, b_h)\in\cS\times\cB}\Bigabr{Q_h^\pi(s_h, b_h) -  \max_{\nu'\in\Delta(\cB)}\rbr{\dotp{Q_h^\pi(s_h,\cdot)}{\nu'(\cdot)} + \eta^{-1}\cH(\nu')}
    }\nend
    & \le \nbr{Q_h^\pi}_\infty + \eta^{-1} \log\abr{\cB}\nend
    &\le \rbr{1+\eff_{H-h+1}(\gamma) }\cdot  \rbr{\eta^{-1}\log\abr{\cB}+1}. 
\end{align*}
Therefore, it suffices to set 
\#\label{eq:define_BA}
B_A = \rbr{1+\eff_{H}(\gamma) }\cdot  \bigrbr{\eta^{-1}\log\abr{\cB}+1}.
\# 
Note that this $B_A$ also boundes $Q_h$ and $V_h$ by our derivation. Hence, we also denote by $B_A$ the upper bounds for the Q and the V functions for the follower.
\vspace{5pt}

\subsection{Function Classes and Covering Number}\label{sec:covering number}
We define several function class with their corresponding covering numbers that will be used in establishing our learning guarantees.

\paragraph{General Model Class $\cM$.}

we consider $\cM$ to be the model class where each $M\in\cM$ uniquely specifies 
the  environment of a Markov game for both the leader and the follower. 
Specifically, for any $M\in\cM$, with slight abuse of notation, we write  $M=(u^M, r^M, P^M)$, which  specifies the leader's and the follower's reward as well as the transition kernel. We consider the covering number of $\cM$ with respect to the following distance:
\begin{align*}
    \varrho(M,\tilde M)\defeq \max_{h\in[H], \atop 
    (s_h, a_h, b_h)\in\cS\times\cA\times\cB}
    \cbr{\bignbr{u_h- \tilde u_h}_\infty, \bignbr{r_h- \tilde r_h}_\infty, \bignbr{P_h(\cdot\given s_h, a_h, b_h)-\tilde P_h(\cdot\given s_h, a_h, b_h)}_1}, 
\end{align*}
where $(r, u, P)$ is a short hand of $(r^M, u^M, P^M)$ and $(\tilde r, \tilde u, \tilde P)$ is a short hand of $(r^{\tilde M}, u^{\tilde M}, P^{\tilde M})$.
With  this definition, we have the following proposition that bounds the error betweeen policies and value functions in terms of $\varrho (\cdot, \cdot)$.
\begin{proposition} \label{prop:error-prop}
    For any two $M,\tilde M\in\cM$ such that $\rho(M, \tilde M)\le \epsilon$, and two policy $\pi, \tilde\pi$ such that $\bignbr{\pi_h - \tilde\pi_h}_1\le \epsilon$, 
    we let $(U, W, Q, V,   \nu)$ be the value functions and quantal response associated with $\pi$  under $M$,  , and let $(\tilde U, \tilde W, \tilde Q, \tilde V,   \tilde \nu)$ be corresponding terms associated with $\tilde \pi$ under $\tilde M$. 
    Then,  
    for all  $h\in[H]$ and $ (s_h, a_h, b_h)\in\cS\times\cA\times\cB$, we have 
    \begin{align*}
    \bignbr{Q_h - \tilde Q_h}_\infty &\le 2\epsilon\cdot (1+\gamma B_A) \cdot \eff_{H}(\gamma), \nend
    %%%%%%%%%%%%%%
    \bignbr{U_h - \tilde U_h}_\infty &\le \epsilon H \cdot \bigrbr{4\eta H (1+\gamma B_A) \cdot \eff_H(\gamma) + 1 + 2H}, \nend
    %%%%%%%%%%%%%%
    \bignbr{V_h - \tilde V_h }_\infty 
    &\le \bignbr{Q_h - \tilde Q_h}_\infty \le 2\epsilon\cdot (1+\gamma B_A) \cdot \eff_{H}(\gamma), \nend
    %%%%%%%%%%%%%
    \bignbr{W_h - \tilde W_h}_\infty &\le \epsilon (H+1) \cdot \bigrbr{4\eta H (1+\gamma B_A)\cdot \eff_H(\gamma) + 1 + 2H}. 
    \end{align*}
    Meanwhile, 
   for the quantal response, 
    \begin{align*}
        D_\H^2(\tilde \nu_h(\cdot\given s_h), \nu_h(\cdot\given s_h)) \le \nbr{\nu_h(\cdot\given s_h) - \tilde \nu_h(\cdot\given s_h)}_1 \le 8 \eta \epsilon \cdot (1+\gamma B_A)  \cdot \eff_{H}(\gamma).
    \end{align*}
\end{proposition}
\begin{proof}
    See Appendix \ref{proof:prop:error-prop} for a detailed proof.
\end{proof}
Therefore, we see it suffices to control $\varrho(\cdot, \cdot)$ in order for the value functions for both the follower and the leader to be under control. Moreover, the quantal response is also under control.
Therefore, we define a new distance $\rho$ for $\cM$ as 
\begin{equation}\label{eq:rho-cM}
    \rho(M,\tilde M)\defeq 6 \!\!\!\!\!\!\!\!\!\!\!\!\max_{\pi\in\Pi, h\in[H], \atop 
    (s_h, a_h, b_h)\in\cS\times\cA\times\cB}\!\!
    \cbr{\begin{aligned}
        &\bignbr{u_h- \tilde u_h}_\infty, \bignbr{r_h- \tilde r_h}_\infty, D_\H\orbr{P_h(\cdot\given s_h, a_h, b_h), \tilde P_h(\cdot\given s_h, a_h, b_h)}\\
        & \bignbr{Q_h^\pi - \tilde Q_h^\pi}_\infty, \bignbr{U_h^\pi - \tilde U_h^\pi}_\infty, D_\H(\tilde \nu_h^\pi(\cdot\given s_h), \nu_h^\pi(\cdot\given s_h)) 
    \end{aligned}}
\end{equation}
And we denote by $\cN(\cM)=\cN_\rho(\cM, T^{-1})$ the covering number for model class $\cM$. Note that $\cN_\rho(\cM,\epsilon)$ is related to $\cN_\varrho(\cM, \epsilon)$ only by a change of $\epsilon$ according to \Cref{prop:error-prop} where we just take $\pi=\tilde\pi$.



\paragraph{Response Model Class $\Theta$.}
Let $\Theta$ be the set of model parameters that determines the follower's quantal response model. 
In particular, when the follower is myopic, i.e., $\gamma = 0$, 
$\theta$ is the parameter of the follower's reward function.
In this case, we assume that there exists some $\theta^* \in \Theta$ such that $r_h = r_h^{\theta^*}$ for all $h \in [H]$. 
In addition, when the follower is farsighted with $\gamma >0$, we parameterize both the follower's reward function and transition kernel and write $\{ r^{\theta}, P^{\theta} \}_{\theta \in \Theta}$. 
In this case, we assume that there exists $\theta^* \in \Theta$ such that 
$r_h = r_h^{\theta^*}$ and $P_h = P_h^{\theta^*}$ for all $h \in [ H ] $. 
We note that each $\theta$ uniquely specifies a quantal response mapping according to \eqref{eq:quantal_response_policy}.
% We let $\nu^{\pi, \theta} $ denote the quantal response of a  leader's policy $\pi$ under the model with parameter $\theta$. 
We consider distance $\rho$ for model $\Theta$ as
\begin{equation}\label{eq:rho-Theta}
    \rho(\theta,\tilde\theta)\defeq \max_{\pi\in\Pi, h\in[H], \atop 
    (s_h, a_h, b_h)\in\cS\times\cA\times\cB}
    \cbr{\begin{aligned}
        &\bignbr{r_h- \tilde r_h}_\infty, \bignbr{P_h(\cdot\given s_h, a_h, b_h) - \tilde P_h(\cdot\given s_h, a_h, b_h)}_1\\
        & \bignbr{Q_h^\pi - \tilde Q_h^\pi}_\infty, D_\H(\tilde \nu_h^\pi(\cdot\given s_h), \nu_h^\pi(\cdot\given s_h)) 
    \end{aligned}}, 
\end{equation}
where $(r, P, Q, \nu)$ is given under $\theta$ and $(\tilde r, \tilde P, \tilde Q, \tilde \nu)$ is given under $\tilde \theta$.
We note that $\theta$ is just a subclass of $\cM$. We denote by $\cN_\rho(\Theta, \epsilon)$ the covering number of $\Theta$ with respect to this $\rho$.

Notably, when the follower is myopic, we just need to cover each $\Theta_h$, where $\Theta_h$ only contains the parameters for $r_h^\theta$, with respect to the following distance 
\begin{align}\label{eq:rho-Theta_h}
    \rho(\theta_h,\tilde\theta_h) \defeq \max_{s_h\in\cS} \cbr{(1+\eta)\|r_h-\tilde r_h\|_\infty, D_\H(\tilde \nu_h^\pi(\cdot\given s_h), \nu_h^\pi(\cdot\given s_h)) }.
\end{align}
Here, the additional $(1+\eta)$ term is needed by \Cref{lem:MLE-formal}.
A covering number for $\Theta_h$ can be thus denoted by $\cN_\rho(\Theta_h, \epsilon)$.
Only for the myopic case, we denote by 
\begin{align}\label{eq:cN-Theta-myopic}
    \cN_\rho(\Theta, \epsilon) = \max_{h\in[H]} \cN_\rho(\Theta_h, \epsilon).
\end{align}
For the nonmyopic case, this covering number $\cN_\rho(\Theta, \epsilon)$ is with respect to the distance defined in \eqref{eq:rho-Theta}.

We calculate this covering number in \eqref{eq:cN-Theta-myopic} for a given step $h\in[H]$ where the follower's reward parameter space is $\Theta_h$ in the linear myopic case.
Specifically, we don't need to consider the transition for myopic follower, and the Q function is simply the reward function. Therefore, we just need to bound $\|r_h-\tilde r_h\|_\infty\le \min \cbr{\epsilon^2 /(8 \eta), \epsilon /(1+\eta)} $, and by \Cref{prop:error-prop}, we have $D_\H(\tilde \nu_h^\pi(\cdot\given s_h), \nu_h^\pi(\cdot\given s_h)) <\epsilon$. Therefore, a covering number for $\Theta_h$ is given by 
\begin{align}\label{eq:cN-Theta_h}
    \log \cN_\rho(\Theta_h, \epsilon) \le d \log\rbr{1+\frac{2 B_\Theta B_\phi}{\min \cbr{\epsilon^2 /(8 \eta), \epsilon /(1+\eta)}}} \lesssim d\log\rbr{1+ \eta /\epsilon^2 + (1+\eta)/\epsilon}.
\end{align}
where $B_\Theta$ bounds the $\Theta_h$ class in the 2 norm and $B_\phi$ bounds the feature mapping $\phi$ in the 2 norm for each $(s_h, a_h, b_h)$. The covering number for $\Theta$ is just $H\cN_\rho(\Theta_h, \epsilon)$.

We remark that $\cM$ is strictly larger than $\Theta$ in the sense that $\cM$ also contains the leader's reward.
We introduce $\cM$ for model-based learning for the leader in \Cref{sec:farsighted}, and $\Theta$ is used for learning the quantal response mapping via model-based maximum likelihood estimation as we have discussed before.


\paragraph{Leader's Value Function Class  $\cU$.}
For both online and offline learning the leader's value function with general function approximation and myopic follower, we introduce function class $\cU:\cS\times\cA\times\cB\rightarrow \RR$, which we assume is uniformly bounded by $H$ and the following completeness and realizability assumption holds. 
\begin{condition}\label{cond:real-comp}
We say that $\cU$ satisfies the realizability and the completeness conditions if the followings hold, 
\begin{itemize}
    \item[(i)] (\textit{Realizability}) There exists $\theta^*\in\Theta$ such that $r_h^{\theta^*}=r_h$ for any $h\in[H]$. For any $\pi\in\Pi, \theta\in\Theta$, there exists $U\in\cU$ such that $U_h = \TT_h^{\pi,\theta} U_{h+1}$ for any $h\in[H]$;
    \item[(ii)] (\textit{Completeness}) For any $U\in\cU$, $\pi\in\Pi$, $\theta\in\Theta$, and $h\in[H]$, there exists $U'\in\cU$ such that $U'=\TT_h^{\pi,\theta} U_{h+1}$. 
\end{itemize}
\end{condition}
We consider the covering number of $\cU$ with respect to $\rho(U, \tilde U) = \onbr{U-\tilde U}_\infty$. Specifically, we denote by $\cN(\cU)=\cN_\rho(\cU, T^{-1})$ the covering number of $\cU$. In the following, we characterize two joint function classes including $\cU$ and $\Theta$ that will be used under the general function approximation setting with myopic follower. 


\paragraph{Joint Class $\cY$.}
For studying the offline case with myopic follower using general function approximation, we have a joint class $\cY_h = \Theta_{h+1}\times \Pi_{h+1}\times \cU^2$, where $\Theta_h$ only contains the follower's reward at step $h$. 
This class is used for studying the suboptimality of the MLE-BCP in \Cref{alg:MLE-BCP}, where $\cU^2$ is used to approximate the leader's value function $U_h^{\pi, \theta}, U_{h+1}^{\pi,\theta}$, $\Theta_{h+1}$ is class for the follower's reward at step $h+1$, and $\Pi_{h+1}$ is the leader's policy class at step $h+1$.
We consider the following distance, 
% \Zhuoran{Explain the meaning of $y$}
% \Siyu{to be redefined because we need to cover the squared loss of $r$ and the squared loss of $U$. We should include a coefficient before the supremum.}
  \begin{equation}\label{eq:rho-cY}
    \rho(y, \tilde y) = \max_{s_{h+1}\in\cS}\cbr{\begin{aligned}
        &\bignbr{U_h-\tilde U_h}_\infty,  \bignbr{U_{h+1}-\tilde U_{h+1}}_\infty, \\
        &\sup_{s_{h+1}\in\cS}\bignbr{(\pi_{h+1}\otimes \nu_{h+1}^{\pi, \theta}-\tilde \pi_{h+1}\otimes \nu_{h+1}^{\tilde\pi, \tilde\theta})(\cdot, \cdot\given s_{h+1})}_1
    \end{aligned}}.
\end{equation}
We let $\cN(\cY)=\cN_\rho(\cY, T^{-1})$. Define $\cN_\rho(\Pi, \epsilon)$ as the $\epsilon$-covering number of the policy class $\Pi$ with respect to distance $\rho(\pi_h, \tilde\pi_h) = \max_{(s_h, b_h)\in\cS\times\cB} \nbr{(\pi_h - \tilde\pi_h)(\cdot\given s_h, b_h)}_1$.
It is easy to see that $\cN_\rho(\cY_h, \epsilon)\le \cN_\rho(\cU,\epsilon)\cdot \cN_\rho(\Pi_{h+1}, \epsilon')\cdot\cN_\rho(\Theta_{h+1}, \epsilon') $ following the result in \Cref{prop:error-prop}, where for the myopic follower, 
$$\epsilon' = \frac{\epsilon}{8\eta (1+\gamma B_A)\eff_H(\gamma)} = \frac{\epsilon}{8\eta}.$$
In particular, we let 
\begin{align}\label{eq:cN-cY}
    \cN_\rho(\cY, \epsilon) = \max_{h\in[H]} \cN_\rho(\cY_h, \epsilon).
\end{align}

\paragraph{Joint Class $\cZ$. }
For the online myopic setting with general function approximation $\cU$, we consider a joint function class $\cZ_h=\cU^2\times\Theta_{h+1}$. 
This class is used for studying the regret of the MLE-GOLF in \Cref{alg:MLE-GOLF}, where $\cU^2$ is used to approximate the leader's optimistic value function $U_h^{*,\theta}, U_{h+1}^{*,\theta}$, $\Theta_{h+1}$ is class for the follower's reward at step $h+1$.
We consider the following distance for $z, \tilde z\in\cZ_h$, 
    \begin{align}\label{eq:rho-cZ}
        &\rho\orbr{z, \tilde z}  = \max_{h\in[H]}\cbr{\bignbr{U_h-\tilde U_h}_\infty, \bignbr{ T_{h+1}^{*,\theta} U_{h+1} (\cdot) -  T_{h+1}^{*,\tilde\theta} \tilde U_{h+1} (\cdot)}_\infty }, 
    \end{align} 
    where the optimistic operator $T_{h+1}^{*, \theta}$ is defined in \eqref{eq:optimistic-operator}.
    We denote by $\cN(\cZ_h)=\cN_\rho(\cZ_h, T^{-1})$ the covering number of the smallest $T^{-1}$-covering net for $\cZ_h$ with respect to this distance $\rho(\cdot,\cdot)$. One should notice that for any $\bignbr{U_{h+1} - \tilde U_{h+1}}_\infty\le \epsilon$, $\onbr{r_{h+1}^\theta - r_{h+1}^{\tilde \theta}}_\infty \le \epsilon$, we have
    \$
    &\bignbr{ T_{h+1}^{*,\theta} U_{h+1} (\cdot) -  T_{h+1}^{*,\tilde\theta} \tilde U_{h+1} (\cdot)}_\infty \nend
    &\quad \le \max_{\pi\in\Pi}\cbr{\bignbr{T_{h+1}^{\pi,\theta} U_{h+1} (\cdot) -  T_{h+1}^{\pi,\tilde\theta} \tilde U_{h+1} (\cdot)}_\infty, \bignbr{T_{h+1}^{\tilde\pi,\theta} U_{h+1} (\cdot) -  T_{h+1}^{\tilde\pi,\tilde\theta} \tilde U_{h+1} (\cdot)}_\infty}\nend
    &\quad \le \max_{\pi\in\Pi} \bignbr{W_{h+1}^\pi - \tilde W_{h+1}^\pi}_\infty \nend
    &\quad \le \epsilon (H+1) \rbr{4\eta H + 1 + 2H}
    \$
    for the myopic case, 
    where we let $\pi$ be the optimal policy under $\theta, U$ and $\tilde\pi$ be the optimal policy under $\tilde \theta, \tilde U$. Here, the last inequality comes from \Cref{prop:error-prop} with $\lambda=0$.
    Therefore, $\cN_\rho(\cZ_h, \epsilon)\le \cN_\rho(\cU, \epsilon) \cdot \cN_\rho(\cU, \epsilon') \cdot \cN_\rho(\Theta_{h+1}, \epsilon')$, where 
    $$\epsilon' = \frac{\epsilon}{(H+1)(4\eta H +1 + 2H)}. $$

This definition will be used in \Cref{lem:MLE} for the farsighted follower case where we directly incorporate MLE algorithm using $\cM$.
In particular, we let
\begin{align}\label{eq:cN-cZ}
    \cN_\rho(\cZ, \epsilon) = \max_{h\in[H]} \cN_\rho(\cZ_h, \epsilon). 
\end{align}


% Remind general function approximation. 
% $\Theta$, $\Pi$, $\cU$. 
% $N_{\mathrm{cov}}$ stands for the covering number of $\Theta$, $\Pi$, $\cU$.
% Covering number 
% $d_{\Theta}$, $d_{\pi}$, $d_{\cU}$ stand for log-covering number of $\Theta$ in terms of what norm, with error $1/T$. 

\subsection{Eluder Dimension}\label{sec:eluder dimension}
we present the definition of (distributional) eluder dimension 
% and a follow-up lemma on bounding the cumulative error with the (distributional) eluder dimension 
that will be useful for our online learning purpose.

\begin{definition}[Eluder Dimension]\label{def:Eluder dimension}
    Let $\cG$ be a function class defined on $\cX$.
	The  eluder dimension $\dim_\E(\cG,\epsilon)$ is the length of the longest sequence $\{x_1, \ldots, x_n\} \subset \cX$ such that there exists $\epsilon'\ge\epsilon$ where for all $m \in [n]$, there exists $g_m\in\cG$ such that
    \begin{align*}
        \sum_{i=1}^{m-1} \rbr{g_m(x_i)}^2 \le \epsilon', \quad \abr{g_m(x_m)} > \epsilon'.
    \end{align*}
\end{definition}
We can similarly define an eluder dimension for signed measure.
\begin{definition}[Eluder Dimension for Signed Measures]
    \label{def:DE}
    Let $\cG$ be a function class defined on measurable space $\cX$, and $\sP$ be a family of signed measures over $\cX$. Suppose that any $g\in\cG$ is integrable with respect to any $\rho\in\sP$.
	The eluder dimension for signed measures with respect to $\cG$ and $\sP$ is denoted by $\dim_\DE(\cG,\sP,\epsilon)$, which is the length of the longest sequence $\{\rho_1, \ldots, \rho_n\} \subset \sP$ satisfying the following condition:  
    there exists $\epsilon'\ge\epsilon$ where for any $m\in[n]$, there exists $g_m\in\cG$ such that  
    \begin{align*}
        \sum_{i=1}^{m-1} \rbr{\int_\cX g_m \rd \rho_i}^2 \le \epsilon', \quad \abr{\int_\cX  g_m \rd\rho_m} > \epsilon'.
    \end{align*}
\end{definition}
% For any function class $\cG$ that has a finite distributional Eluder dimension over probability measures $\sP$, we have the following lemma.


In the sequel, we will denote by $\dim(\cG)=\dim_\DE(\cG, \sP, T^{-1/2})$ the eluder dimension with respect to the class of signed measures $\sP$. Note that the standard distributional eluder dimension is a special case for the eluder dimension for signed measures.

\subsubsection{Eluder Dimensions in Myopic Case}\label{sec:eluder myopic}
We discuss the function classes and their corresponding eluder dimensions (with respect to signed measures) that characterize the hardness of leader's exploration problem in the face of a myopic  follower. 
Such a complexity measure will be used for analyzing the regret of the MLE-GOLF algorithm.
Detailed proofs of the regret upper bound in terms of  these eluder dimensions  are available in \Cref{sec:proof-Online-MG}.
\paragraph{Eluder Dimension for Leader's Bellman Error.}
As is shown in the proof \Cref{sec:proof-Online-MG}, the leader's Bellman error we are dealing with is 
\begin{align*}
    \sum_{t=1}^T \EE^{\hat\pi^t} [(\hat U_h^t - \TT_h^{*, \hat\theta^t} \hat U_{h+1}^t)(s_h, a_h, b_h)], 
\end{align*}
where $\hat U^t, \hat\theta^t$ are just the optimistic estimators obtained at episode $t$.
We define a class of functions that corresponds to this error
\begin{align*}
    \cG_L=\cbr{g:\Pi\times\cU^2\times\Theta\rightarrow \RR: g=\EE^\pi [U_h-\TT_h^{*,\theta} U_{h+1}]}. 
\end{align*}
For the leader's Bellman error, we have the following configurations:
\begin{itemize}[leftmargin=20pt]
    \item[(i)] Define function class $\cG_{h,L} = \bigcbr{g:\cS\times\cA\times\cB\rightarrow \RR\given  g = U_h-\TT_h^{*, \theta} U_{h+1}, \exists U\in\cU, \theta\in\Theta}$. 
    Moreover, consider sequence $\{g_h^i = \hat U_h^i - \TT_h^{*, \hat\theta^i} \hat U_{h+1}^i\}_{i\in[T]}$, where $\hat\theta^i, \hat U^i$ are the estimated $\theta$, $U$ at episode $i$.  
    It is obvious that $g_h^i\in\cG_{h,L}$;
    \item[(ii)] Define the class of probability measures as 
    $$\sP_{h,L} = \cbr{\rho\in\Delta(\cS\times\cA\times\cB)\given\rho(\cdot)=\PP^\pi((s_h, a_h, b_h)=\cdot), \pi\in\Pi}.$$
    Moreover, consider sequence $\{\rho^i(\cdot) = \PP^{\hat\pi^i}((s_h, a_h, b_h)=\cdot)\}_{i\in[T]}$, where $\hat\pi^i$ is the optimistic policy used at episode $i$;
\end{itemize}
Under these definitions, we let low rank property for $\cG_L$ that $\dim(\cG_L) = \max_{h\in[H]}\dim_\DE\rbr{\cG_{h, L}, \sP_{h, L}, T^{-1/2}}$.
This configuration works because for the chosen sequences, we have
\begin{align*}
    \EE_{\rho_h^i}[g_h^t] = \EE^{i} \sbr{(\hat U_h^t - \TT_h^{*, \hat\theta^t} \hat U_{h+1}^t)(s_h,a_h,b_h)}.
\end{align*}
If $i=t$, this will be the leader's Bellman error we aim to bound.
Moreover, the online guarantee will be
$$\sum_{i=1}^{t-1} \rbr{\EE_{\rho^i}\sbr{g_h^t}}^2 = \sum_{i=1}^{t-1}\EE^{i}[(\hat U_{h}^{t} - \TT_{h}^{*, \hat\theta^t} \hat U_{h + 1}^t)^2]\lesssim H^2\beta$$ for any $t\in[T]$, which enables us to use \Cref{lem:de-regret}.
In particular, for the 
linear Markov Game in \Cref{def:linear MDP}, we have $\dim(\cG_L)\lesssim d$ as is discussed in \citet{jin2021bellman}.

\paragraph{Eluder Dimension for Follower's Quantal Response Error.}
As is shown in the proof of \Cref{sec:proof-Online-MG}, 
the follower's quantal response error is characterized by the following term, 
\begin{align*}
    \sum_{t=1}^T \bigrbr{\Upsilon_h^{\hat\pi^t} (r_h^{\hat\theta^t}-r_h)}(s_h^t,b_h^t)= \sum_{t=1}^T\QRE(s_h^t, b_h^t;\hat\theta^t, \hat\pi^t).
\end{align*}
We define 
\begin{align*}
    \cG_F = \cbr{g:\cS\times\cB\times\Pi\times\Theta\rightarrow \RR: g=\Upsilon_h^{\pi}(r_h^\theta - r_h)(s_h, b_h)},
\end{align*}
where we  recall the linear operator $\Upsilon_h^\pi:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cB)$ defined as 
\begin{align*}
    \rbr{\Upsilon_h^\pi f}(s_h, b_h) = \dotp{\pi_h(\cdot\given s_h, b_h)}{f(s_h, \cdot, b_h)} - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{f(s_h,\cdot,\cdot)}. 
\end{align*}
Here, $(\Upsilon_h ^{\pi} f ) (s_h,b_h) $ quantifies  how far $b_h$ is from being the quantal response of $\pi$ at state $s_h$, measured in terms of  $f$. One can also think of $(\Upsilon_h^\pi f)(s_h, b_h)$ as the \say{advantage} of the reward induced by action $b_h$ compared to the reward induced by the quantal response. 
% It suffices to use $\Upsilon_h^\pi $ instead for a myopic follower where $\alpha\in\sA$ is a state-wise prescription.
We have the following configurations for the follower's quantal response error:
\begin{itemize}[leftmargin=20pt]
    \item[(i)] We define function class on $\cS\times\cA\times\cB$ as,
    \begin{align*}
    \cG_{h, F} = \cbr{g:\cS\times\cA\times\cB\rightarrow \RR: \exists \theta\in\Theta, g= {r_h^\theta - r_h}}.
    \end{align*}
    In addition, we consider a sequence $\{g_h^i = r_h^{\hat\theta^i} - r_h\}_{i\in[T]}$. 
    \item[(ii)] We define a class of signed measures on $\cS\times\cA\times\cB$ as
    \begin{align*}
        \sP_{h, F} &= \{\rho(\cdot) = \PP^\pi(a_h=\cdot\given s_h, b_h)\delta_{(s_h, b_h)}(\cdot) 
        - \PP^\pi((a_h, b_h)=\cdot\given s_h)\delta_{(s_h)}(\cdot)\nend
        &\qqquad \biggiven \pi\in\Pi, (s_h, b_h)\in\cS\times\cB\}, 
    \end{align*}
    where $\delta_{(s_h, b_h)}(\cdot)$ is the measure that assigns measure $1$ to state-action pair $(s_h, b_h)$. 
    In addition, we consider a sequence 
    $$\cbr{\rho_h^i(\cdot) = \PP^{\hat\pi^i}(a_h=\cdot\given s_h, b_h)\delta_{(s_h, b_h)}(\cdot) 
    - \PP^{\hat\pi^i}((a_h, b_h)=\cdot\given s_h)\delta_{(s_h)}(\cdot)}_{i\in[T]}.$$
\end{itemize}
Under these definitions, we let $\dim(\cG_F) = \max_{h\in[H]}\dim_\DE\rbr{\cG_{h, F}, \sP_{h, F}, T^{-1/2}}$.
For simplicity, we denote by $g_h^t(s_h^i, b_h^i, \pi^i)$ the integral of $g_h^t$ with respect to the signed measure $\rho_h^i$ since $\rho_h^i$ is uniquely determined by $(s_h^i, b_h^i, \pi^i)$. 
    It is easy to check that $$g_h^t(s_h^i, b_h^i, \pi^i) = \bigrbr{\Upsilon_h^{\pi^i}(r_h^{\hat\theta^t}-r_h)}(s_h^i, b_h^i) = \QRE(s_h^i, b_h^i; \hat\theta^t, \pi^i).$$
This configurations work because when we take $t=i$, $g_h^t(s_h^t, b_h^t, \pi^t)$ will be exactly the quantal response error, and the online guarantee for these two sequences are 
    \begin{align*}
        \sum_{i=1}^{t-1} (g_h^t(s_h^i, b_h^i, \pi^i))^2 \lesssim C_\eta^2\beta, 
    \end{align*}
by our MLE guarantee, which enables us to use \Cref{lem:de-regret}.
Moreover, we remark that under the linear Markov game setting, i.e., $r_h(s_h, a_h,b_h) = \la \phi_h(s_h, a_h,b_h), \theta\ra_\cB$, we simply have $g_h^t(s_h^i, b_h^i, \pi^i) = \la (\Upsilon_h^{\pi^i}\phi_h)(s_h^i, b_h^i), \hat\theta_h^{t} - \theta_h^*\ra_\cB$, which has a bilinear form and we simply have for the eluder dimension $\dim(\cG_{F})\lesssim d$.
% or $\dim(\cG)=\dim_\E(\cG, T^{-1/2})$ for a standard Eluder dimension.

\subsubsection{Eluder Dimensions in Farsighted Case}
\label{sec:eluder farsighted}
We discuss the function classes and their corresponding eluder dimensions (with respect to signed measures) that characterize the hardness of leader's exploration problem in the face of a farsighted follower. 
Such a complexity measure will be used for analyzing the regret of the OMLE algorithm. 
Detailed proof using the eluder dimensions of these classes are available in \Cref{sec:proof-farsighted MDP}.

\paragraph{Eluder Dimension for the Leader's Bellman Error.}
As is shown in \Cref{sec:proof-farsighted MDP}, the leader's Bellman error is 
\begin{align*}
    \sum_{t=1}^T \EE^{\pi^t} \sbr{\bigrbr{\tilde U_h^t - u_h}(s_h, a_h, b_h) - \tilde W_{h+1}^t(s_{h+1})}, 
\end{align*}
where $\tilde U^t=U^{\pi^t, M^t}$, $\tilde W^t=W^{\pi^t, M^t}$, and $M^t,\pi^t$ are the optimistic model estimator and optimistic policy obtained at episode $t$. 
We define the following error class
\begin{align*}
    \cG_L = \cbr{\cM\times\Pi\rightarrow\RR: \EE^\pi\sbr{\bigrbr{U_h^{\pi^M, M} - u_h}(s_h, a_h, b_h) - W_{h+1}^{\pi^M,M}(s_{h+1})}}, 
\end{align*}
where $\pi^M=\argmax_{\pi\in\Pi} J(\pi, M)$ is the optimistic policy corresponding to model $M$.
To formally define the eluder dimension for $\cG_L$, we consider the following configurations for step $h\in[H]$.
\begin{itemize}[leftmargin=20pt]
    \item[(i)] Define function class $\cG_{h,L}$ as
    \begin{align*}
        \cG_{h, L} &= \Big\{g:\cS\times\cA\times\cB\rightarrow \RR \Biggiven g={\bigrbr{U_h^{\pi^{\tilde M},\tilde M} - u - P_h W_{h+1}^{\pi^{\tilde M},\tilde M}}(s_h, a_h, b_h)}, \exists \tilde M\in\cM\Big\}, 
    \end{align*}
    where we define $\pi^M = \argmax_{\pi\in\Pi}J(\pi, M)$.
    Specifically, the expectation is taken under $\pi$ and the true model. 
    Consider a sequence of function $\{g_h^i = (\tilde U_h^{i} - u - P_h \tilde W_{h+1}^i)\}_{i\in[T]}$. We have $g_h^i\in\cG_{h, L}$ since we define $\tilde U^i = U^{\pi^i, M^i}$ and we have by the optimism in the algorithm that $\pi^i = \pi^{M^i}$. The same also holds for $\tilde W^i$.
    \item[(ii)] Define a class of probability measures over $\cS\times\cA\times\cB$ as $$\sP_{h, L}=\{\PP^\pi((s_h, a_h, b_h)=\cdot), \forall \pi\in\Pi\}.$$
    Consider a sequence of probability measures $\{\rho_h^i(\cdot)=\PP^{\pi^i}((s_h, a_h, b_h)=\cdot)\}_{i\in[T]}$, where $\pi^i$ is the policy used at episode $i$.
    \item[(iii)] Under these two sequences, we denote by $g_h^t(\pi^i) = \EE_{\rho_h^i}[g_h^t]$ for simplicity. 
    We have 
    $$g_h^t(\pi^i) =\EE^{\pi^i}\bigsbr{{\bigrbr{\tilde U_h^t - u - P_h \tilde W_{h+1}^t}(s_h,a_h,b_h)}}, $$
    which should be bounded by $3H$.
\end{itemize}
We thus denote by $\dim(\cG_L) = \max_{h\in[H]}\dim_\DE(\cG_{h, L}, \sP_{h, L}, T^{-1/2})$. This is actually the distributional eluder dimension of $\cG_{h, L}$, which satisfies $\dim(\cG_L)\lesssim d$ for the linear Markov game defined in \Cref{def:linear MDP} because for any $g\in\cG_{h, L}$ and $\rho\in\sP_{h, L}$, the integral of $g$ with respect to $\rho$ always admits a bilinear form $\EE_\rho[g]= \la \EE_\rho[\phi_h(s_h, a_h, b_h)], \zeta^g\ra$. 

%  or the linear kernel MDP \citep{zhou2021provably}.
% , low rank MDP \citep{agarwal2020flambe}.

\paragraph{Eluder Dimension for the  First-Order Error of the Follower's Quantal Response.}
As we will show in the proof \Cref{sec:proof-farsighted MDP}, the first order term in the quantal response error is just
\begin{align*}
    \sum_{h=1}^H\sum_{t=1}^T |\tilde\Delta_h^{1, t}(s_h^t, b_h^t)|,
\end{align*}
where $\tilde\Delta_h^{1, t}$ is defined as
\begin{align*}
    \tilde \Delta^{(1, t)}_h(s_h, b_h) &=  \rbr{\EE_{s_h, b_h}^t -\EE_{s_h}^t}\Biggsbr{\sum_{l=h}^H \gamma^{l-h}{\rbr{\tilde Q_l^t - r_l^{\pi^t} - \gamma P_l^{\pi^t} \tilde V_{l+1}^t}(s_l, b_l)}}, 
\end{align*} 
Here, $\tilde r_l^t = r_l^{M^t}$, $\tilde P_l^t = P_l^{M^t}$, $\tilde V^t = V^{\pi^t, M^t}$, and $\tilde Q^t=Q^{\pi^t, M^t}$. $M^t$ is the estimated model and $\pi^t$ is the optimistic policy at episode $t$. In particular, $\pi^t = \argmax_{\pi\in\Pi}J(\pi, M^t) \eqdef \pi^{M^t}$.
We define 
\begin{align*}
    \cG_F^1 &= \bigg\{\Pi\times\cM\times\cS\times\cB\rightarrow \RR: \nend
    &\qqquad \ts
    (\EE_{s_h, b_h}^\pi - \EE_{s_h}^\pi)\Bigsbr{\sum_{l=h}^H \gamma^{l-h}\bigrbr{r_l^M- r_l + \gamma (P_l^{M} - P_l) V_{l+1}^{\pi^M, M}}(s_l, a_l, b_l)}, \forall h\in[H]\bigg\} . 
\end{align*}
We consider the following configurations for the definition of the eluder dimension of $\cG_F^1$. 
% {\red Check here}
\begin{itemize}[leftmargin=20pt]
    \item[(i)] Define function class $\cG_{h, F}^1$ as 
    \begin{align*}
        \cG_{h, F}^1 &= \Bigg\{g:(\cS\times\cA\times\cB)^{H-h+1}\rightarrow \RR \bigggiven \exists M\in\cM, \nend
        &\qqquad g((s_l, a_l, b_l)_{l=h}^H) = {\sum_{l=h}^H \gamma^{l-h}\bigrbr{r_l^M- r_l + \gamma (P_l^{M} - P_l) V_{l+1}^{\pi^M, M}}(s_l, a_l, b_l)}
        \Bigg\},
    \end{align*}
    where we remind the readers that $\pi^M = \argmax_{\pi\in\Pi}J(\pi, M)$ only depends on $M$. Consider 
    sequences 
    $$\cbr{g_h^t=\sum_{l=h}^H \gamma^{l-h}\orbr{\tilde r_l^t- r_l + \gamma (\tilde P_l^t - P_l) \tilde V_{l+1}^t}}_{t\in[T]}. $$
    It is clear that $g_h^t\in\cG_{h, F}^1$ since $\tilde V_h^t = V_h^{\pi^t, M^t}$ and $\pi^t = \argmax_{\pi\in\Pi}J(\pi, M^t) = \pi^{M^t}$.
    % $r_h^{\pi}(s_h, b_h) = \dotp{r_h(s_h, \cdot, b_h)}{\pi_h(\cdot\given s_h, b_h)}_\cA$ and the same holds for $P_h^\pi$. 
    % One can see immediately that $\cG_F^1$ is a bilinear class, where the expectation part depends on $\pi$ and the follower's Bellman error part depends on $M$.
    
    \item [(ii)] Define a class of signed measures over $(\cS\times\cA\times\cB)^{H-h+1}$ as 
    \begin{equation*}
        \sP_{h, F}^1 = \cbr{\begin{aligned}
            &\PP^\pi(((s_l, a_l, b_l)_{l=h+1}^H , a_h)=\cdot \given s_h, b_h)\delta_{(s_h, b_h)}(\cdot) \nend
            &\quad - \PP^\pi(((s_l, a_l, b_l)_{l=h+1}^H , a_h, b_h)=\cdot \given s_h)\delta_{(s_h)}(\cdot) 
        \end{aligned}
        \bigggiven \pi\in\Pi, (s_h, b_h)\in\cS\times\cB}, 
    \end{equation*}
    where $\delta_{s_h, b_h}$ is the measure that puts measure $1$ on a single state-action pair $(s_h, b_h)$, and the conditional is well defined by the Markov property. Also, consider the following sequence, 
    \begin{align*}
        \Big\{\rho_h^t(\cdot)&=\PP^{\pi^t}(((s_l, a_l, b_l)_{l=h+1}^H , a_h)=\cdot \given s_h^t, b_h^t)\delta_{(s_h^t, b_h^t)}(\cdot) \nend
        &\qquad - \PP^{\pi^t}(((s_l, a_l, b_l)_{l=h+1}^H , a_h, b_h)=\cdot \given s_h^t)\delta_{(s_h^t)}(\cdot)\Big\}_{t\in[T]},
    \end{align*}
    and we also have $\rho_h^t\in\sP_{h, F}^1$.
    \item   [(iii)] 
    In particular, we define $g_h^t(s_h^i, b_h^i, \pi^i)$ as the integral of $g_h^t$ with respect to $\rho_h^i$, which is given by
    $$g_h^t(s_h^i, b_h^i, \pi^i)= \rbr{\EE_{s_h^i, b_h^i}^{\pi^i}-\EE_{s_h^i}^{\pi^i}} \sbr{\sum_{l=h}^H \gamma^{l-h}\Bigrbr{\tilde r_l^t- r_l + \gamma (\tilde P_l^t - P_l) \tilde V_{l+1}^t}(s_l, a_l, b_l)},$$
    % One can check that $g_h^t\in\cG_F^1$ 
    Note that the sequence of signed measures is uniquely determined by $\{(s_h^t, b_h^t, \pi^t)\}_{t\in[T]}$. Moreover, we have $g_h^t(s_h^i, b_h^i, \pi^i)$ bounded by $\eff_H(\gamma)(2\nbr{r_h}_\infty + 2\nbr{V_{h+1}}_\infty) \le 4B_A \eff_H(\gamma)$, where the definition of $B_A$ is available in \eqref{eq:define_BA};  
\end{itemize}
We define the maximal eluder dimension of $\cG_{h, F}^1$ with respect to $\sP_{h,F}^1$ as $$\dim(\cG_F^1) =\max_{h\in[H]} \dim_\DE(\cG_{h, F}^1,\sP_{h, F}^1, T^{-1/2}). $$
We remark that the linear Markov game have eluder dimension $\dim(\cG_{h, F}^2)\lesssim Hd$.
Note that for each $l\in\{h, \dots, H\}$ in the expression of $g_h^t(s_h^i, b_h^i, \pi^i)$, we have 
\begin{align*}
    &\rbr{\EE_{s_h^i, b_h^i}^{\pi^i}-\EE_{s_h^i}^{\pi^i}} \sbr{\sum_{l=h}^H \gamma^{l-h}\Bigrbr{\tilde r_l^t- r_l + \gamma (\tilde P_l^t - P_l) \tilde V_{l+1}^t}(s_l, a_l, b_l)}\nend
    &\quad  = \sum_{l=h}^H \gamma^{l-h} \Bigdotp{\bigrbr{\EE_{s_h^i, b_h^i}^{\pi^i}-\EE_{s_h^i}^{\pi^i}}\sbr{\phi_l(s_l,a_l,b_l)}}{\hat\theta_h^t - \theta_h + \sum_{s_{l+1}}(\hat\mu_l^t - \mu_l)(s_{l+1})\tilde V_{l+1}^t(s_{l+1})}, 
\end{align*}
which admits a bilinear form. 
Since we can stack $H$ vectors in the summation together for both sides of bilinear form, which creates a bilinear form with dimension $Hd$, we claim that $\dim(\cG_{F}^1)\lesssim Hd$ in this case.


\paragraph{Eluder Dimension for the Second-Order Error of the Follower's Quantal Response. }
According to \Cref{sec:proof-farsighted MDP}, the second order term in the quantal response error is just 
\begin{align*}
    \sum_{t=1}^T \EE^t\sbr{ \rbr{\rbr{\tilde Q_h^t - r_h^{\pi^t} - \gamma P_h^{\pi^t} \tilde V_{h+1}^t}(s_h, b_h)}^2}.
\end{align*}
We denote by $\cG_{F}^2$ the class of functions corresponding to this second-order QRE, 
\begin{align*}
    \cG_F^2=\bigcbr{\EE_{s_h, b_h}^{\pi} \osbr{\orbr{r_h^M- r_h + \gamma (P_h^{M} - P_h) V_{l+1}^{*, M}}(s_h, a_h, b_h)}}, 
\end{align*}
for all $\pi\in\Pi, M\in\cM, (s_h, a_h, b_h)\in\cS\times\cA\times\cB$ and $h\in[H]$.
We consider the following configurations.
\begin{itemize}[leftmargin =20pt]
    \item[(i)] We take the same function class $\cG_F^2$ as
    \begin{align*}
        \cG_{h, F}^2 &= \Bigg\{g:\cS\times\cA\times\cB\rightarrow \RR: \exists M\in\cM, h\in[H] \nend
        &\qqquad g(s_h, b_h, a_h) = {\bigrbr{r_h^M- r_h + \gamma (P_h^{M} - P_h) V_{l+1}^{\pi^M, M}}(s_h, a_h, b_h)}
        \Bigg\},
    \end{align*}
    where $\cG_F^2$ is bounded by $4B_A$. 
    \item[(ii)] We define a class of probability measures on $\cS\times\cA\times\cB$ as $$\sP_{h, F}^2 = \cbr{\PP^\pi(a_h=\cdot\given s_h, b_h)\delta_{(s_h, b_h)}(\cdot)\given \pi\in\Pi, (s_h,b_h)\in\cS\times\cB}, $$
    where $\delta_{(s_h,b_h)}(\cdot)$ is the measure that assigns $1$ to the state-action pair $(s_h, b_h)$. 
    \item[(iii)] We take a sequence of functions $\{g_h^t\}_{t\in[T]}$ as $\{g_h^t = \tilde r_h^{t}- r_h + \gamma (\tilde P_h^t - P_h) \tilde V_{l+1}^{t}\}_{t\in[T]}$, and take a sequence of probability measures as $\{\rho_h^t(\cdot) = \PP^{\pi^t}(a_h=\cdot\given s_h^t, b_h^t)\delta_{(s_h^t, b_h^t)}(\cdot)\}_{t\in[T]}$, where we define $\tilde r_h^t = r_h^{M^t}$ and $\tilde P_h^t = P_h^{M^t}$.
    One can check that $g_h^t\in\cG_{h,F}^1$ since $\tilde V_h^t = V_h^{\pi^t, M^t}$ and we have $\pi^t = \argmax_{\pi\in\Pi}J(\pi, M^t) = \pi^{M^t}$. In addition, we define $g_h^t(s_h^i, b_h^i, \pi^i)$ as the integral of $g_h^t$ with respect to $\rho_h^i$, which is given by
    \begin{align*}
        g_h^t (s_h^i, b_h^i, \pi^i) = \EE_{s_h^i, b_h^i}^{\pi^i} \sbr{\bigrbr{\tilde r_h^{t}- r_h + \gamma (\tilde P_h^t - P_h) \tilde V_{l+1}^{t}}(s_h, a_h, b_h)},
    \end{align*}
    Note that the sequence of probability measures is uniquely determined by $\{(s_h^t, b_h^t, \pi^t)\}_{t\in[T]}$.
\end{itemize}
We let $\dim(\cG_F^2)=\max_{h\in[H]}\dim_\DE(\cG_{h,F}^2,\sP_{h, F}^2, T^{-1/2})$ be the eluder dimension.
Similar to the previous discussion, we also have a bilinear for $g_h^t(s_h^i, b_h^i, \pi^i)$ under the linear Markov game. Hence, $\dim(\cG_{h, F}^2)\lesssim d$.

We remark that 
for the linear matrix MDP \citep{zhou2021provably}, we just have $P(s_{h+1}|s_h, a_h, b_h) = \psi_h(s_h, a_h, b_h)^\top W_h \varphi_h(s')$ for some unknown $\RR^{d\times d}$ matrix $W$. 
By viewing $W_h\varphi_h(s')=\mu_h(s')$, our discussions hold for the linear matrix MDP as well.
For the linear mixture MDP \citep{chen2022unified}, the rewards and the transition kernel share the same parameter $\theta_h$, but the transition kernel is given by $P_h(s_{h+1}\given s_h,a_h, b_h)=\la \psi_h(s_{h+1}, s_h, a_h, b_h), \theta_h\ra$. 
We remark that we can instead take 
$$g_h^t(\pi^i) =\EE^{\pi^i}\bigsbr{{\bigrbr{(u_h^{M^t} - u_h) + (P_h^{M^t} - P_h) \tilde W_{h+1}^i}(s_h,a_h,b_h)}}, $$ 
for the leader's Bellman error, 
$$g_h^t(s_h^i, b_h^i, \pi^i)= \rbr{\EE_{s_h^i, b_h^i}^{\pi^i}-\EE_{s_h^i}^{\pi^i}} \sbr{\sum_{l=h}^H \gamma^{l-h}\Bigrbr{\tilde r_l^t- r_l + \gamma (\tilde P_l^t - P_l) \tilde V_{l+1}^i}(s_l, a_l, b_l)},$$
for the first order quantal response error, and 
\begin{align*}
    g_h^t (s_h^i, b_h^i, \pi^i) = \EE_{s_h^i, b_h^i}^{\pi^i} \sbr{\bigrbr{\tilde r_h^{t}- r_h + \gamma (\tilde P_h^t - P_h) \tilde V_{l+1}^{i}}(s_h, a_h, b_h)},
\end{align*}
for the second order quantal response error, these errors still admit a low rank factorization. In fact, the online guarantees of these three errors are already implied by \eqref{eq:OnN-guarantee-Bellmanerror}, \eqref{eq:OnN-1st-g-sq}, and \eqref{eq:OnN-guarantee-2ndQRE}, where a change of the V- or the W-functions does not matter because we have strong guarantee on the TV distance for the transition kernel, and the change of V- or W-functions only introduces a $\cO(\beta)$ terms in the online guarantee which is caused by the squared TV distance for the transition kernel.
% In this work, we focus on the Eluder dimension of the following function classes.

% \Zhuoran{Complete}
% \begin{itemize}
%     \item[(i)] function class $$\cG_L = \cbr{U_h-\TT_h^{\theta} U_{h+1}: U\in\cU, \theta\in\Theta}$$ 
%     defined on $\cX=\cS\times\cA\times\cB$, which is bounded by $B_{\cG_L}\le 2B_U$. We consider the class of probability measures $\sP_L = \cbr{\rho\in\Delta(\cS\times\cA\times\cB): \rho(\cdot)=\PP^\pi((s_h, a_h, b_h)=\cdot)}$ with $\rho^i(\cdot) = \PP^{\hat\pi^i}((s_h, a_h, b_h)=\cdot)$;
% \end{itemize}




{\ifneurips 
\input{neurips/algorithm_detail_neurips.tex}
\fi}
% $\mathrm{dim}_{\cG}$




% Eluder dimension and covering number. 



\input{appendix/main_tech.tex}
% \input{appendix/bandit.tex}
\input{appendix/myopic_offline.tex}
\input{appendix/myopic_online.tex}

\input{appendix/nonmyopic_online.tex}
\input{appendix/reg_decomp.tex}
\input{appendix/first_order_bound.tex}
% \input{appendix/LinMDP_online.tex}
\input{appendix/proof_app_B.tex}
\input{appendix/technical.tex}
\input{appendix/tmp.tex}
\input{appendix/auxiliary.tex}