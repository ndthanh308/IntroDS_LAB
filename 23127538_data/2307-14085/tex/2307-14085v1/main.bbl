\begin{thebibliography}{95}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Agarwal et~al.(2014)Agarwal, Hsu, Kale, Langford, Li and
  Schapire}]{agarwal2014taming}
\text{Agarwal, A.}, \text{Hsu, D.}, \text{Kale, S.}, \text{Langford, J.},
  \text{Li, L.} and \text{Schapire, R.} (2014).
\newblock Taming the monster: A fast and simple algorithm for contextual
  bandits.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Auer et~al.(2008)Auer, Jaksch and Ortner}]{auer2008near}
\text{Auer, P.}, \text{Jaksch, T.} and \text{Ortner, R.} (2008).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Advances in neural information processing systems},
  \textbf{21}.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\text{Ayoub, A.}, \text{Jia, Z.}, \text{Szepesvari, C.}, \text{Wang, M.} and
  \text{Yang, L.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Bai and Jin(2020)}]{bai2020provable}
\text{Bai, Y.} and \text{Jin, C.} (2020).
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Bai et~al.(2021)Bai, Jin, Wang and Xiong}]{bai2021sample}
\text{Bai, Y.}, \text{Jin, C.}, \text{Wang, H.} and \text{Xiong, C.} (2021).
\newblock Sample-efficient learning of stackelberg equilibria in general-sum
  games.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{34} 25799--25811.

\bibitem[{Bai et~al.(2020)Bai, Jin and Yu}]{bai2020near}
\text{Bai, Y.}, \text{Jin, C.} and \text{Yu, T.} (2020).
\newblock Near-optimal reinforcement learning with self-play.
\newblock \textit{arXiv preprint arXiv:2006.12007}, \textbf{33} 2159--2170.

\bibitem[{Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen,
  Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional}
\text{Bai, Y.}, \text{Kadavath, S.}, \text{Kundu, S.}, \text{Askell, A.},
  \text{Kernion, J.}, \text{Jones, A.}, \text{Chen, A.}, \text{Goldie, A.},
  \text{Mirhoseini, A.}, \text{McKinnon, C.} \text{et~al.} (2022).
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \textit{arXiv preprint arXiv:2212.08073}.

\bibitem[{Ba{\c{s}}ar and Olsder(1998)}]{bacsar1998dynamic}
\text{Ba{\c{s}}ar, T.} and \text{Olsder, G.~J.} (1998).
\newblock \textit{Dynamic noncooperative game theory}.
\newblock SIAM.

\bibitem[{Bradley and Terry(1952)}]{bradley1952rank}
\text{Bradley, R.~A.} and \text{Terry, M.~E.} (1952).
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock \textit{Biometrika}, \textbf{39} 324--345.

\bibitem[{Buckman et~al.(2020)Buckman, Gelada and
  Bellemare}]{buckman2020importance}
\text{Buckman, J.}, \text{Gelada, C.} and \text{Bellemare, M.~G.} (2020).
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \textit{arXiv preprint arXiv:2009.06799}.

\bibitem[{Busoniu et~al.(2008)Busoniu, Babuska and
  De~Schutter}]{busoniu2008comprehensive}
\text{Busoniu, L.}, \text{Babuska, R.} and \text{De~Schutter, B.} (2008).
\newblock A comprehensive survey of multiagent reinforcement learning.
\newblock \textit{IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, \textbf{38} 156--172.

\bibitem[{Cai et~al.(2020)Cai, Yang, Jin and Wang}]{cai2020provably}
\text{Cai, Q.}, \text{Yang, Z.}, \text{Jin, C.} and \text{Wang, Z.} (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Camerer(1998)}]{camerer1998bounded}
\text{Camerer, C.} (1998).
\newblock Bounded rationality in individual decision making.
\newblock \textit{Experimental economics}, \textbf{1} 163--183.

\bibitem[{Carpentier et~al.(2020)Carpentier, Vernade and
  Abbasi-Yadkori}]{carpentier2020elliptical}
\text{Carpentier, A.}, \text{Vernade, C.} and \text{Abbasi-Yadkori, Y.} (2020).
\newblock The elliptical potential lemma revisited.
\newblock \textit{arXiv preprint arXiv:2010.10182}.

\bibitem[{Chen et~al.(2022{\natexlab{a}})Chen, Mei and Bai}]{chen2022unified}
\text{Chen, F.}, \text{Mei, S.} and \text{Bai, Y.} (2022{\natexlab{a}}).
\newblock Unified algorithms for {RL} with decision-estimation coefficients:
  No-regret, pac, and reward-free learning.
\newblock \textit{arXiv preprint arXiv:2209.11745}.

\bibitem[{Chen et~al.(2022{\natexlab{b}})Chen, Yang, Li, Wang, Yang and
  Wang}]{chen2022adaptive}
\text{Chen, S.}, \text{Yang, D.}, \text{Li, J.}, \text{Wang, S.}, \text{Yang,
  Z.} and \text{Wang, Z.} (2022{\natexlab{b}}).
\newblock Adaptive model design for markov decision process.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Chen et~al.(2021)Chen, Zhou and Gu}]{chen2021almost}
\text{Chen, Z.}, \text{Zhou, D.} and \text{Gu, Q.} (2021).
\newblock Almost optimal algorithms for two-player {M}arkov games with linear
  function approximation.
\newblock \textit{arXiv preprint arXiv:2102.07404}.

\bibitem[{Choi and Kim(2012)}]{choi2012nonparametric}
\text{Choi, J.} and \text{Kim, K.-E.} (2012).
\newblock Nonparametric bayesian inverse reinforcement learning for multiple
  reward functions.
\newblock \textit{Advances in neural information processing systems},
  \textbf{25}.

\bibitem[{Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg and
  Amodei}]{christiano2017deep}
\text{Christiano, P.~F.}, \text{Leike, J.}, \text{Brown, T.}, \text{Martic,
  M.}, \text{Legg, S.} and \text{Amodei, D.} (2017).
\newblock Deep reinforcement learning from human preferences.
\newblock \textit{Advances in neural information processing systems},
  \textbf{30}.

\bibitem[{Conlisk(1996)}]{conlisk1996bounded}
\text{Conlisk, J.} (1996).
\newblock Why bounded rationality?
\newblock \textit{Journal of economic literature}, \textbf{34} 669--700.

\bibitem[{Cui and Du(2022{\natexlab{a}})}]{cui2022provably}
\text{Cui, Q.} and \text{Du, S.~S.} (2022{\natexlab{a}}).
\newblock Provably efficient offline multi-agent reinforcement learning via
  strategy-wise bonus.
\newblock \textit{arXiv preprint arXiv:2206.00159}.

\bibitem[{Cui and Du(2022{\natexlab{b}})}]{cui2022offline}
\text{Cui, Q.} and \text{Du, S.~S.} (2022{\natexlab{b}}).
\newblock When is offline two-player zero-sum markov game solvable?
\newblock \textit{arXiv preprint arXiv:2201.03522}.

\bibitem[{Cui et~al.(2023)Cui, Zhang and Du}]{cui2023breaking}
\text{Cui, Q.}, \text{Zhang, K.} and \text{Du, S.~S.} (2023).
\newblock Breaking the curse of multiagents in a large state space: Rl in
  markov games with independent linear function approximation.
\newblock \textit{arXiv preprint arXiv:2302.03673}.

\bibitem[{Dann et~al.(2021)Dann, Mohri, Zhang and Zimmert}]{dann2021provably}
\text{Dann, C.}, \text{Mohri, M.}, \text{Zhang, T.} and \text{Zimmert, J.}
  (2021).
\newblock A provably efficient model-free posterior sampling method for
  episodic reinforcement learning.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{34} 12040--12051.

\bibitem[{Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun and
  Wang}]{du2021bilinear}
\text{Du, S.}, \text{Kakade, S.}, \text{Lee, J.}, \text{Lovett, S.},
  \text{Mahajan, G.}, \text{Sun, W.} and \text{Wang, R.} (2021).
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Erez et~al.(2022)Erez, Lancewicki, Sherman, Koren and
  Mansour}]{erez2022regret}
\text{Erez, L.}, \text{Lancewicki, T.}, \text{Sherman, U.}, \text{Koren, T.}
  and \text{Mansour, Y.} (2022).
\newblock Regret minimization and convergence to equilibria in general-sum
  markov games.
\newblock \textit{arXiv preprint arXiv:2207.14211}.

\bibitem[{Foster et~al.(2023)Foster, Golowich and Han}]{foster2023tight}
\text{Foster, D.~J.}, \text{Golowich, N.} and \text{Han, Y.} (2023).
\newblock Tight guarantees for interactive decision making with the
  decision-estimation coefficient.
\newblock \textit{arXiv preprint arXiv:2301.08215}.

\bibitem[{Foster et~al.(2021)Foster, Kakade, Qian and
  Rakhlin}]{foster2021statistical}
\text{Foster, D.~J.}, \text{Kakade, S.~M.}, \text{Qian, J.} and \text{Rakhlin,
  A.} (2021).
\newblock The statistical complexity of interactive decision making.
\newblock \textit{arXiv preprint arXiv:2112.13487}.

\bibitem[{Geist et~al.(2019)Geist, Scherrer and Pietquin}]{geist2019theory}
\text{Geist, M.}, \text{Scherrer, B.} and \text{Pietquin, O.} (2019).
\newblock A theory of regularized markov decision processes.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Gleave and Toyer(2022)}]{gleave2022primer}
\text{Gleave, A.} and \text{Toyer, S.} (2022).
\newblock A primer on maximum causal entropy inverse reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2203.11409}.

\bibitem[{Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel and
  Levine}]{haarnoja2017reinforcement}
\text{Haarnoja, T.}, \text{Tang, H.}, \text{Abbeel, P.} and \text{Levine, S.}
  (2017).
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \textit{International conference on machine learning}. PMLR.

\bibitem[{Hajek et~al.(2014)Hajek, Oh and Xu}]{hajek2014minimax}
\text{Hajek, B.}, \text{Oh, S.} and \text{Xu, J.} (2014).
\newblock Minimax-optimal inference from partial rankings.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{27}.

\bibitem[{Hernandez and Ortega(2019)}]{hernandez2019bounded}
\text{Hernandez, J. G.~V.} and \text{Ortega, R.~P.} (2019).
\newblock Bounded rationality in decision--making.
\newblock \textit{MOJ Research Review}, \textbf{2} 1--8.

\bibitem[{Huang et~al.(2021)Huang, Lee, Wang and Yang}]{huang2021towards}
\text{Huang, B.}, \text{Lee, J.~D.}, \text{Wang, Z.} and \text{Yang, Z.}
  (2021).
\newblock Towards general function approximation in zero-sum markov games.
\newblock \textit{arXiv preprint arXiv:2107.14702}.

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{jiang@2017}
\text{Jiang, N.}, \text{Krishnamurthy, A.}, \text{Agarwal, A.}, \text{Langford,
  J.} and \text{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In \textit{Proceedings of the 34th International Conference on
  Machine Learning}, vol.~70 of \textit{Proceedings of Machine Learning
  Research}. PMLR.

\bibitem[{Jin et~al.(2021{\natexlab{a}})Jin, Liu and
  Miryoosefi}]{jin2021bellman}
\text{Jin, C.}, \text{Liu, Q.} and \text{Miryoosefi, S.} (2021{\natexlab{a}}).
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \textit{Advances in neural information processing systems},
  \textbf{34} 13406--13418.

\bibitem[{Jin et~al.(2021{\natexlab{b}})Jin, Liu, Wang and Yu}]{jin2021v}
\text{Jin, C.}, \text{Liu, Q.}, \text{Wang, Y.} and \text{Yu, T.}
  (2021{\natexlab{b}}).
\newblock V-learning--a simple, efficient, decentralized algorithm for
  multiagent rl.
\newblock \textit{arXiv preprint arXiv:2110.14555}.

\bibitem[{Jin et~al.(2021{\natexlab{c}})Jin, Liu and Yu}]{jin2021power}
\text{Jin, C.}, \text{Liu, Q.} and \text{Yu, T.} (2021{\natexlab{c}}).
\newblock The power of exploiter: Provable multi-agent rl in large state
  spaces.
\newblock \textit{arXiv preprint arXiv:2106.03352}.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2020provably}
\text{Jin, C.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Jin et~al.(2021{\natexlab{d}})Jin, Yang and Wang}]{jin2021pessimism}
\text{Jin, Y.}, \text{Yang, Z.} and \text{Wang, Z.} (2021{\natexlab{d}}).
\newblock Is pessimism provably efficient for offline rl?
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Kao et~al.(2022)Kao, Wei and Subramanian}]{kao2022decentralized}
\text{Kao, H.}, \text{Wei, C.-Y.} and \text{Subramanian, V.} (2022).
\newblock Decentralized cooperative reinforcement learning with hierarchical
  information structure.
\newblock In \textit{International Conference on Algorithmic Learning Theory}.
  PMLR.

\bibitem[{Karwowski et~al.(2023)Karwowski, Ma{\'n}dziuk and
  {\.Z}ychowski}]{karwowski2023sequential}
\text{Karwowski, J.}, \text{Ma{\'n}dziuk, J.} and \text{{\.Z}ychowski, A.}
  (2023).
\newblock Sequential stackelberg games with bounded rationality.
\newblock \textit{Applied Soft Computing}, \textbf{132} 109846.

\bibitem[{Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli and
  Joachims}]{kidambi2020morel}
\text{Kidambi, R.}, \text{Rajeswaran, A.}, \text{Netrapalli, P.} and
  \text{Joachims, T.} (2020).
\newblock Morel: Model-based offline reinforcement learning.
\newblock \textit{Advances in neural information processing systems},
  \textbf{33} 21810--21823.

\bibitem[{Kumar et~al.(2020)Kumar, Zhou, Tucker and
  Levine}]{kumar2020conservative}
\text{Kumar, A.}, \text{Zhou, A.}, \text{Tucker, G.} and \text{Levine, S.}
  (2020).
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{33} 1179--1191.

\bibitem[{Liu et~al.(2022)Liu, Wang and Jin}]{liu2022learning}
\text{Liu, Q.}, \text{Wang, Y.} and \text{Jin, C.} (2022).
\newblock Learning {M}arkov games with adversarial opponents: Efficient
  algorithms and fundamental limits.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Liu et~al.(2020)Liu, Yu, Bai and Jin}]{liu2020sharp}
\text{Liu, Q.}, \text{Yu, T.}, \text{Bai, Y.} and \text{Jin, C.} (2020).
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock \textit{arXiv preprint arXiv:2010.01604}.

\bibitem[{Luce(2012)}]{luce2012individual}
\text{Luce, R.~D.} (2012).
\newblock \textit{Individual choice behavior: A theoretical analysis}.
\newblock Courier Corporation.

\bibitem[{Lyu et~al.(2022)Lyu, Wang, Kolar and Yang}]{lyu2022pessimism}
\text{Lyu, B.}, \text{Wang, Z.}, \text{Kolar, M.} and \text{Yang, Z.} (2022).
\newblock Pessimism meets vcg: Learning dynamic mechanism design via offline
  reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Mao and Ba{\c{s}}ar(2023)}]{mao2023provably}
\text{Mao, W.} and \text{Ba{\c{s}}ar, T.} (2023).
\newblock Provably efficient reinforcement learning in decentralized
  general-sum markov games.
\newblock \textit{Dynamic Games and Applications}, \textbf{13} 165--186.

\bibitem[{McKelvey and Palfrey(1995)}]{mckelvey1995quantal}
\text{McKelvey, R.~D.} and \text{Palfrey, T.~R.} (1995).
\newblock Quantal response equilibria for normal form games.
\newblock \textit{Games and economic behavior}, \textbf{10} 6--38.

\bibitem[{Modi et~al.(2020)Modi, Jiang, Tewari and Singh}]{modi2020sample}
\text{Modi, A.}, \text{Jiang, N.}, \text{Tewari, A.} and \text{Singh, S.}
  (2020).
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\bibitem[{Munos and Szepesv{\'a}ri(2008)}]{munos2008finite}
\text{Munos, R.} and \text{Szepesv{\'a}ri, C.} (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock \textit{Journal of Machine Learning Research}, \textbf{9}.

\bibitem[{Najar and Chetouani(2021)}]{najar2021reinforcement}
\text{Najar, A.} and \text{Chetouani, M.} (2021).
\newblock Reinforcement learning with human advice: a survey.
\newblock \textit{Frontiers in Robotics and AI}, \textbf{8} 584075.

\bibitem[{Nayyar et~al.(2014)Nayyar, Mahajan and Teneketzis}]{nayyar2014common}
\text{Nayyar, A.}, \text{Mahajan, A.} and \text{Teneketzis, D.} (2014).
\newblock The common-information approach to decentralized stochastic control.
\newblock \textit{Information and Control in Networks} 123--156.

\bibitem[{Negahban et~al.(2012)Negahban, Oh and Shah}]{negahban2012iterative}
\text{Negahban, S.}, \text{Oh, S.} and \text{Shah, D.} (2012).
\newblock Iterative ranking from pair-wise comparisons.
\newblock \textit{Advances in neural information processing systems},
  \textbf{25}.

\bibitem[{Neu and Szepesv{\'a}ri(2009)}]{neu2009training}
\text{Neu, G.} and \text{Szepesv{\'a}ri, C.} (2009).
\newblock Training parsers by inverse reinforcement learning.
\newblock \textit{Machine Learning. 2009 Dec; 77: 303-37.}

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
\text{Ouyang, L.}, \text{Wu, J.}, \text{Jiang, X.}, \text{Almeida, D.},
  \text{Wainwright, C.}, \text{Mishkin, P.}, \text{Zhang, C.}, \text{Agarwal,
  S.}, \text{Slama, K.}, \text{Ray, A.} \text{et~al.} (2022).
\newblock Training language models to follow instructions with human feedback.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{35} 27730--27744.

\bibitem[{Plackett(1975)}]{plackett1975analysis}
\text{Plackett, R.~L.} (1975).
\newblock The analysis of permutations.
\newblock \textit{Journal of the Royal Statistical Society Series C: Applied
  Statistics}, \textbf{24} 193--202.

\bibitem[{Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao and
  Russell}]{rashidinejad2021bridging}
\text{Rashidinejad, P.}, \text{Zhu, B.}, \text{Ma, C.}, \text{Jiao, J.} and
  \text{Russell, S.} (2021).
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{34} 11702--11716.

\bibitem[{Sadigh et~al.(2017)Sadigh, Dragan, Sastry and
  Seshia}]{sadigh2017active}
\text{Sadigh, D.}, \text{Dragan, A.~D.}, \text{Sastry, S.} and \text{Seshia,
  S.~A.} (2017).
\newblock \textit{Active preference-based learning of reward functions}.

\bibitem[{Shah et~al.(2015)Shah, Balakrishnan, Bradley, Parekh, Ramchandran and
  Wainwright}]{shah2015estimation}
\text{Shah, N.}, \text{Balakrishnan, S.}, \text{Bradley, J.}, \text{Parekh,
  A.}, \text{Ramchandran, K.} and \text{Wainwright, M.} (2015).
\newblock Estimation from pairwise comparisons: Sharp minimax bounds with
  topology dependence.
\newblock In \textit{Artificial intelligence and statistics}. PMLR.

\bibitem[{Shi et~al.(2022)Shi, Li, Wei, Chen and Chi}]{shi2022pessimistic}
\text{Shi, L.}, \text{Li, G.}, \text{Wei, Y.}, \text{Chen, Y.} and \text{Chi,
  Y.} (2022).
\newblock Pessimistic q-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Simon(1955)}]{simon1955behavioral}
\text{Simon, H.~A.} (1955).
\newblock A behavioral model of rational choice.
\newblock \textit{The quarterly journal of economics} 99--118.

\bibitem[{Song et~al.(2021)Song, Mei and Bai}]{songcan}
\text{Song, Z.}, \text{Mei, S.} and \text{Bai, Y.} (2021).
\newblock When can we learn general-sum markov games with a large number of
  players sample-efficiently?
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei and Christiano}]{stiennon2020learning}
\text{Stiennon, N.}, \text{Ouyang, L.}, \text{Wu, J.}, \text{Ziegler, D.},
  \text{Lowe, R.}, \text{Voss, C.}, \text{Radford, A.}, \text{Amodei, D.} and
  \text{Christiano, P.~F.} (2020).
\newblock Learning to summarize with human feedback.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{33} 3008--3021.

\bibitem[{Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal and
  Langford}]{sun2019model}
\text{Sun, W.}, \text{Jiang, N.}, \text{Krishnamurthy, A.}, \text{Agarwal, A.}
  and \text{Langford, J.} (2019).
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \textit{Conference on learning theory}. PMLR.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\text{Sutton, R.~S.} and \text{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement learning: An introduction}.

\bibitem[{Szepesv{\'a}ri and Munos(2005)}]{szepesvari2005finite}
\text{Szepesv{\'a}ri, C.} and \text{Munos, R.} (2005).
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In \textit{Proceedings of the 22nd international conference on
  Machine learning}.

\bibitem[{Tian et~al.(2021)Tian, Wang, Yu and Sra}]{tian2021online}
\text{Tian, Y.}, \text{Wang, Y.}, \text{Yu, T.} and \text{Sra, S.} (2021).
\newblock Online learning in unknown markov games.
\newblock In \textit{International conference on machine learning}. PMLR.

\bibitem[{Uehara and Sun(2021)}]{uehara2021pessimistic}
\text{Uehara, M.} and \text{Sun, W.} (2021).
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \textit{arXiv preprint arXiv:2107.06226}.

\bibitem[{Wang et~al.(2020)Wang, Salakhutdinov and
  Yang}]{wang2020reinforcement}
\text{Wang, R.}, \text{Salakhutdinov, R.~R.} and \text{Yang, L.} (2020).
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{33} 6123--6135.

\bibitem[{Wang et~al.(2023)Wang, Liu, Bai and Jin}]{wang2023breaking}
\text{Wang, Y.}, \text{Liu, Q.}, \text{Bai, Y.} and \text{Jin, C.} (2023).
\newblock Breaking the curse of multiagency: Provably efficient decentralized
  multi-agent rl with function approximation.
\newblock \textit{arXiv preprint arXiv:2302.06606}.

\bibitem[{Wang et~al.(2019)Wang, Wang, Du and Krishnamurthy}]{wang2019optimism}
\text{Wang, Y.}, \text{Wang, R.}, \text{Du, S.~S.} and \text{Krishnamurthy, A.}
  (2019).
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock \textit{arXiv preprint arXiv:1912.04136}.

\bibitem[{Xie et~al.(2020)Xie, Chen, Wang and Yang}]{xie2020learning}
\text{Xie, Q.}, \text{Chen, Y.}, \text{Wang, Z.} and \text{Yang, Z.} (2020).
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock In \textit{Conference on learning theory}. PMLR.

\bibitem[{Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro and
  Agarwal}]{xie2021bellman}
\text{Xie, T.}, \text{Cheng, C.-A.}, \text{Jiang, N.}, \text{Mineiro, P.} and
  \text{Agarwal, A.} (2021).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \textit{Advances in neural information processing systems},
  \textbf{34} 6683--6694.

\bibitem[{Xiong et~al.(2022)Xiong, Zhong, Shi, Shen and Zhang}]{xiong22b}
\text{Xiong, W.}, \text{Zhong, H.}, \text{Shi, C.}, \text{Shen, C.} and
  \text{Zhang, T.} (2022).
\newblock A self-play posterior sampling algorithm for zero-sum {M}arkov games.
\newblock In \textit{Proceedings of the 39th International Conference on
  Machine Learning}, vol. 162 of \textit{Proceedings of Machine Learning
  Research}. PMLR.

\bibitem[{Yan et~al.(2022)Yan, Li, Chen and Fan}]{yan2022efficacy}
\text{Yan, Y.}, \text{Li, G.}, \text{Chen, Y.} and \text{Fan, J.} (2022).
\newblock The efficacy of pessimism in asynchronous q-learning.
\newblock \textit{arXiv preprint arXiv:2203.07368}.

\bibitem[{Yang and Wang(2019)}]{yang2019sample}
\text{Yang, L.} and \text{Wang, M.} (2019).
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Yang et~al.(2020)Yang, Jin, Wang, Wang and Jordan}]{yang2020provably}
\text{Yang, Z.}, \text{Jin, C.}, \text{Wang, Z.}, \text{Wang, M.} and
  \text{Jordan, M.} (2020).
\newblock Provably efficient reinforcement learning with kernel and neural
  function approximations.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{33} 13903--13916.

\bibitem[{Yu et~al.(2022)Yu, Yang and Fan}]{yu2022strategic}
\text{Yu, M.}, \text{Yang, Z.} and \text{Fan, J.} (2022).
\newblock Strategic decision-making in the presence of information asymmetry:
  Provably efficient rl with algorithmic instruments.
\newblock \textit{arXiv preprint arXiv:2208.11040}.

\bibitem[{Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn and
  Ma}]{yu2020mopo}
\text{Yu, T.}, \text{Thomas, G.}, \text{Yu, L.}, \text{Ermon, S.}, \text{Zou,
  J.~Y.}, \text{Levine, S.}, \text{Finn, C.} and \text{Ma, T.} (2020).
\newblock Mopo: Model-based offline policy optimization.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{33} 14129--14142.

\bibitem[{Zanette et~al.(2020)Zanette, Brandfonbrener, Brunskill, Pirotta and
  Lazaric}]{zanette2020frequentist}
\text{Zanette, A.}, \text{Brandfonbrener, D.}, \text{Brunskill, E.},
  \text{Pirotta, M.} and \text{Lazaric, A.} (2020).
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\bibitem[{Zanette et~al.(2021)Zanette, Wainwright and
  Brunskill}]{zanette2021provable}
\text{Zanette, A.}, \text{Wainwright, M.~J.} and \text{Brunskill, E.} (2021).
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \textit{Advances in neural information processing systems},
  \textbf{34} 13626--13640.

\bibitem[{Zhan et~al.(2022)Zhan, Lee and Yang}]{zhan2022decentralized}
\text{Zhan, W.}, \text{Lee, J.~D.} and \text{Yang, Z.} (2022).
\newblock Decentralized optimistic hyperpolicy mirror descent: Provably
  no-regret learning in markov games.
\newblock \textit{arXiv preprint arXiv:2206.01588}.

\bibitem[{Zhang et~al.(2021)Zhang, Yang and Ba{\c{s}}ar}]{zhang2021multi}
\text{Zhang, K.}, \text{Yang, Z.} and \text{Ba{\c{s}}ar, T.} (2021).
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \textit{Handbook of reinforcement learning and control} 321--384.

\bibitem[{Zhang et~al.(2023)Zhang, Bai and Jiang}]{zhang2023offline}
\text{Zhang, Y.}, \text{Bai, Y.} and \text{Jiang, N.} (2023).
\newblock Offline learning in markov games with general function approximation.
\newblock \textit{arXiv preprint arXiv:2302.02571}.

\bibitem[{Zhao et~al.(2023)Zhao, Zhu, Jiao and Jordan}]{zhao2023online}
\text{Zhao, G.}, \text{Zhu, B.}, \text{Jiao, J.} and \text{Jordan, M.~I.}
  (2023).
\newblock Online learning in stackelberg games with an omniscient follower.
\newblock \textit{arXiv preprint arXiv:2301.11518}.

\bibitem[{Zhong et~al.(2022{\natexlab{a}})Zhong, Xiong, Tan, Wang, Zhang, Wang
  and Yang}]{zhong2022pessimistic}
\text{Zhong, H.}, \text{Xiong, W.}, \text{Tan, J.}, \text{Wang, L.},
  \text{Zhang, T.}, \text{Wang, Z.} and \text{Yang, Z.} (2022{\natexlab{a}}).
\newblock Pessimistic minimax value iteration: Provably efficient equilibrium
  learning from offline datasets.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Zhong et~al.(2022{\natexlab{b}})Zhong, Xiong, Zheng, Wang, Wang, Yang
  and Zhang}]{zhong2022posterior}
\text{Zhong, H.}, \text{Xiong, W.}, \text{Zheng, S.}, \text{Wang, L.},
  \text{Wang, Z.}, \text{Yang, Z.} and \text{Zhang, T.} (2022{\natexlab{b}}).
\newblock A posterior sampling framework for interactive decision making.
\newblock \textit{arXiv preprint arXiv:2211.01962}.

\bibitem[{Zhong et~al.(2023)Zhong, Yang, Wang and Jordan}]{zhong2021can}
\text{Zhong, H.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2023).
\newblock Can reinforcement learning find stackelberg-nash equilibria in
  general-sum markov games with myopically rational followers?
\newblock \textit{Journal of Machine Learning Research}, \textbf{24} 1--52.

\bibitem[{Zhou et~al.(2021{\natexlab{a}})Zhou, Gu and
  Szepesvari}]{zhou2021nearly}
\text{Zhou, D.}, \text{Gu, Q.} and \text{Szepesvari, C.} (2021{\natexlab{a}}).
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhou et~al.(2021{\natexlab{b}})Zhou, He and Gu}]{zhou2021provably}
\text{Zhou, D.}, \text{He, J.} and \text{Gu, Q.} (2021{\natexlab{b}}).
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Zhu et~al.(2023)Zhu, Jiao and Jordan}]{zhu2023principled}
\text{Zhu, B.}, \text{Jiao, J.} and \text{Jordan, M.~I.} (2023).
\newblock Principled reinforcement learning with human feedback from pairwise
  or $ k $-wise comparisons.
\newblock \textit{arXiv preprint arXiv:2301.11270}.

\bibitem[{Ziebart et~al.(2010)Ziebart, Bagnell and Dey}]{ziebart2010modeling}
\text{Ziebart, B.~D.}, \text{Bagnell, J.~A.} and \text{Dey, A.~K.} (2010).
\newblock Modeling interaction via the principle of maximum causal entropy.

\bibitem[{Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey
  et~al.}]{ziebart2008maximum}
\text{Ziebart, B.~D.}, \text{Maas, A.~L.}, \text{Bagnell, J.~A.}, \text{Dey,
  A.~K.} \text{et~al.} (2008).
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \textit{Aaai}, vol.~8. Chicago, IL, USA.

\end{thebibliography}
