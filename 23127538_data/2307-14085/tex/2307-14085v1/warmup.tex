%!TEX root =main.tex
\ifneurips\vspace{-10pt}\fi
\section{Error Decomposition for QSE: Learning Quantal Response}\label{sec:learning QSE}
\ifneurips\vspace{-5pt}\fi
As we have mentioned in \S\ref{sec:intro}, we need to 
(i) learn the follower's  quantal response mapping $\pi\rightarrow \nu^\pi$ and 
(ii) solve the leader's policy optimization problem given that the follower adopts the quantal response. 
In this section, we discuss how to handle these two steps.
For ease of presentation, we focus on the linear Markov Game defined in \Cref{def:linear MDP} with a myopic follower. 
% For the myopic follower's reward function, we let
% $\phi \colon \cS\times\cA\times\cB\rightarrow\RR^d$  be a known feature mapping and suppose 
% $r_h(s_h,a_h,b_h)=\dotp{\phi(s_h, a_h, b_h)}{\theta_h^*}$ for some $\theta^*\in\Theta$, where $\Theta\subset\RR^d$ is the parameter class.
Note that the follower's reward function is sufficient for determining the quantal response mapping in the myopic case.
We let $r_h^\theta(s_h,a_h,b_h)=\la\phi_h(s_h,a_h,b_h), \theta_h\ra$ and use the notation
$ r_h^{\pi, \theta}(s, b) = \odotp{\pi_h(\cdot\given s_h, b_h)}{r_h^\theta(s_h, \cdot, b_h)}_\cB = \inp[]{\phi_h^\pi(s, b)}{\theta_h},$
where $\phi_h^\pi(s, b)=\inp{\phi_h(s, \cdot, b)}{\pi_h(\cdot\given s, b)}_\cA$ is the reward feature mapping under policy $\pi$.
We suppose $\theta_h\in\Theta_h$, which is a bounded subset of $\RR^d$. We let $\theta=\{\theta_h\}_{h\in[H]}$ and  $\Theta=\{\Theta_h\}_{h\in[H]}$.
We will define in the sequel a \emph{\bf quantal response error} (QRE) for myopic follower that characterizes the error  incurred in  learning the quantal response mapping. 
% Specifically, under the linear function approximation, this QRE for an estimated reward parameter $\tilde\theta\in\Theta$ is just a vector norm $\onbr{\tilde \theta-\theta^*}_{\Sigma_{s_h}^{\pi,\theta^*}}$ induced by a \emph{\red covariance matrix} $\Sigma_{s_h}^{\pi,\theta^*}$ that will be defined later.

\ifneurips\vspace{-5pt}\fi
\subsection{Performance Difference Lemma for QSE}
\label{sec:subopt decomposition}
\ifneurips\vspace{-5pt}\fi
To quantify how the estimation error of the quantal response affects the suboptimality of the leader's policy (Challenge (b) in \S\ref{sec:intro}),  in the following, we introduce a new version of performance difference lemma that bridges the upper and lower level problems in the QSE.
The idea is to decompose the performance difference into the the leader's Bellman error and the follower's quantal response error.
%  Since the quantal response error is highly nonlinear and hard to control directly, we further decompose it into a first order term and a second order term by Taylor expansion.

For any fixed policy $\pi$, recall that the leader's functions under joint policies $(\pi, \nu^{\pi})$ is given by $U^{\pi} $ and $W^{\pi}$ in \eqref{eq:U_function} and  \eqref{eq:W_function}. Suppose we have an estimated parameter $\tilde\theta$ for the follower's reward, and based on the estimated reward $\tilde r = r^{\tilde\theta}$, we have an estimated  quantal response
$\tilde \nu^{\pi} = \nu^{\pi,\tilde\theta}$ under  policy $\pi$. On the leader's side,  we denote by $\tilde U$ and $\tilde W $ the estimates of $U^{\pi} $ and $W^{\pi}$, respectively,
which satisfy 
$
\tilde W_h(s ) = \la \tilde U_h (s, \cdot , \cdot ) , \pi _h \otimes \tilde \nu_h^{\pi} (\cdot , \cdot \given s) \ra_{\cA\times\cB} 
$.
We can hence estimate $J(\pi) $ by $\tilde J(\pi ) \defeq \EE_{s_1 \sim \rho_0} [ \tilde W_1 (s_1)] $. 
We have the following performance difference decomposition, 
%One key component in the study of the suboptimality is the performance error under the same policy but subject to different models.
%The main idea is to decompose the performance difference in this QSE into both the environment model error and the follower's quantal response error. 
 %$U_h^\pi:\cS\times\cA\times\cB\rightarrow \RR$ and $W_h^\pi:\cS\rightarrow \cB$ be the leader's action-value (Q) function and state-value (V) function under policy $\pi$ and the true model $M^*$ defined recursively as
%\begin{align*}
%    U_h^\pi(s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \rbr{\PP_h W_{h+1}^\pi}(s_h, a_h, b_h), \quad 
 %   W_h^\pi(s_h) = \EE^{\pi, \nu^\pi} \sbr{U_h^\pi(s_h, a_h, b_h)}.
%\end{align*}
%Specifically, for any leader's policy $\pi\in\Pi$ and an alternative model $\tilde M\in\cM$, we define total reward $J(\pi, \tilde M)$ in the same way as \eqref{eq:J} and have the following lemma on the suboptimality decomposition
\ifneurips{\aboveskip[0.5]\belowskip[0.5]
\begin{align}
\!\!\!\!\!\!\!\!&\tilde J(\pi ) - J(\pi ) \label{eq:performance diff-1}\\
    &~~\le\underbrace{{{\ts\sum_{h=1}^H} \EE^{\pi, \nu^{\pi }}
    \!
    \bigsbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}}}_{\ts\small\textrm{Leader's Bellman Error}}+ \underbrace{{\ts\sum_{h=1}^H } H \EE^{\pi, \nu^{\pi }}\! \bigl [ \nbr{\rbr{\tilde \nu_h^{\pi} -\nu_h^{\pi} }(\cdot\given s_h)}_1 \bigr ] }_{\ts\small\textrm{Quantal Response Error}}, \notag
\end{align} 
}
% \begin{proof}
% \end{proof}
% \end{lemma}
\hspace{-3pt}\fi
\ifmain 
\begin{align}
    &\tilde J(\pi ) - J(\pi ) \label{eq:performance diff-1}\\
        &~~\le\underbrace{{{\sum_{h=1}^H} \EE^{\pi, \nu^{\pi }}
        \!
        \bigsbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}}}_{\ts\small\textrm{Leader's Bellman Error}}+ \underbrace{{\sum_{h=1}^H } H \EE^{\pi, \nu^{\pi }}\! \bigl [ \nbr{\rbr{\tilde \nu_h^{\pi} -\nu_h^{\pi} }(\cdot\given s_h)}_1 \bigr ] }_{\ts\small\textrm{Quantal Response Error}}, \notag
    \end{align}
\fi
See \Cref{lem:subopt-decomposition} for a detailed proof.
% We call the first term leader's Bellman error (LBE) since the first term comprises the model differences in the leader's utility and the transition kernel of the MDP.
% We call the second term quantal response error (QRE) since the second term characterizes the follower's behavior difference in the TV distance.
% Here, the TV bound is reasonable since the follower's utility is not aligned with the leader's. We remark that the environment model error is easier to deal with since we directly observe the leader's utility and the state transition from the data set.
A unique challenge of our problem is to characterize the quantal response error where we \emph{only have observations of the follower's actions but not  the follower's reward}. The nonlinearity of the quantal response model in \eqref{eq:quantal_response_policy} exacerbates the difficulty. Fortunately, we can linearize the quantal response error by Taylor expansion. With slight abuse of notation, we also refer to the following error term as the quantal response error (QRE),
\ifneurips{
    \abovebelowskip{0.85}{0.85}
\begin{align}
    \QRE(s_h, b_h;\tilde\theta,\pi) =  (\Upsilon_h^{\pi}(\tilde r_h - r_h))\orbr{s_h, b_h}, \label{eq:QRE}
\end{align}
}
\hspace{-5pt}\fi 
\ifmain
\begin{align}
    \QRE(s_h, b_h;\tilde\theta,\pi) =  (\Upsilon_h^{\pi}(\tilde r_h - r_h))\orbr{s_h, b_h}, \label{eq:QRE}
\end{align}\fi
where operator $\Upsilon_h^\pi:\cF(\cS\times\cA\times\cB)\rightarrow \cF(\cS\times\cB)$ is defined as 
\ifneurips{
    \abovebelowskip{0.85}{0.85}
\begin{align}\label{eq:Upsilon}
    \rbr{\Upsilon_h^\pi f}(s_h, b_h) = \dotp{\pi_h(\cdot\given s_h, b_h)}{f(s_h, \cdot, b_h)}_\cA - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{f(s_h,\cdot,\cdot)}_{\cA\times\cB}.
\end{align}}
\hspace{-5pt}\fi
\ifmain
\begin{align}\label{eq:Upsilon}
    \rbr{\Upsilon_h^\pi f}(s_h, b_h) = \dotp{\pi_h(\cdot\given s_h, b_h)}{f(s_h, \cdot, b_h)}_\cA - \dotp{\pi_h\otimes \nu_h^{\pi}(\cdot,\cdot\given s_h)}{f(s_h,\cdot,\cdot)}_{\cA\times\cB}.
\end{align}\fi
For myopic follower with a  linear reward, we have for the quantal response error in \eqref{eq:performance diff-1} that 
\ifneurips
{\abovebelowskip{0.6}{0.6}
\begin{align}
    &{\ts\sum_{h=1}^H } H \cdot \EE^{\pi, \nu^{\pi }} \bigl [ \nbr{\rbr{\tilde \nu_h^{\pi} -\nu_h^{\pi} }(\cdot\given s_h)}_1 \bigr ] \label{eq:QRE-decompose}\\
    &\quad \lesssim  2\eta H \cdot 
    {\ts\sum_{h=1}^H
    \Bigrbr{
        % \underbrace{ 
    \sqrt{\EE^{\pi, \nu^\pi} \bigsbr{\QRE(s_h, b_h;\tilde\theta,\pi)^2}}
    % }_{\ds\small\text{1st-order QRE}}  
    + 
    % \underbrace{ 
        \EE^{\pi, \nu^\pi} \bigsbr{\QRE(s_h, b_h;\tilde\theta,\pi)^2}
    % }_{\ds\small\text{2nd-order QRE}}
    }}
    , 
    \notag
\end{align}
}
\hspace{-7pt}\fi
\ifmain
\begin{align}
    &{\sum_{h=1}^H } H \cdot \EE^{\pi, \nu^{\pi }} \bigl [ \nbr{\rbr{\tilde \nu_h^{\pi} -\nu_h^{\pi} }(\cdot\given s_h)}_1 \bigr ] \label{eq:QRE-decompose}\\
    &\quad \lesssim  2\eta H \cdot 
    {\sum_{h=1}^H
    \Bigrbr{
        % \underbrace{ 
    \sqrt{\EE^{\pi, \nu^\pi} \bigsbr{\QRE(s_h, b_h;\tilde\theta,\pi)^2}}
    % }_{\ds\small\text{1st-order QRE}}  
    + 
    % \underbrace{ 
        \EE^{\pi, \nu^\pi} \bigsbr{\QRE(s_h, b_h;\tilde\theta,\pi)^2}
    % }_{\ds\small\text{2nd-order QRE}}
    }}
    , 
    \notag
\end{align}
\fi
where $\lesssim$ only hides some coefficients in the 2nd-order term on the right-hand side. 
We defer readers to \Cref{lem:response diff-myopic-linear} and its follow-up discussions for more details. 
We remark that   $\mathtt{QRE}$ and the operator $\Upsilon_h^\pi$ capture the comparison nature of the model in the sense that $\EE^{\pi, \nu^\pi}[\QRE(s_h, b_h;\tilde\theta_h,\pi)^2] = 0$ for any admissible $\pi$ if and only if $r_h(s_h, a_h, b_h) = \tilde r_h(s_h, a_h, b_h) + c(s_h)$ for some function $c:\cS\rightarrow\RR$, which matches our intuition that the follower's quantal response should be invariant to any constant shift at a given state. 
With linear function approximation, we define 
$
    \Sigma_{s_h}^{\pi, \theta^*} = \Cov_{s_h}^{\pi, \theta^*}\sbr{\phi_h^{\pi}(s_h, b_h)}
$ with $\Cov_{s_h}^{\pi,\theta}[\cdot] = \Cov^{\pi, \theta}[\cdot\given s_h]$, where the covariance is with respect to $b_h\sim\nu_h^{\pi,\theta}(\cdot\given s_h)$.
% and $\EE_{z}^{\pi, M^*}[\cdot]$ is a short hand of $\EE^{\pi, M^*}\sbr{\cdot\given z}$.
% \end{remark}
We note that    this covariance matrix satisfies $\EE^{\pi,\nu^\pi}[\QRE(s_h,b_h;\tilde\theta,\pi)^2\given s_h] = \onbr{\tilde\theta_h -\theta_h^*}_{\Sigma_{s_h}^{\pi, \theta^*}}$.
% , where the vector norm is induced by the covariance matrix $\Sigma_{s_h}^{\pi, \theta^*}$. 
% We defer readers to \Cref{cor:response-diff-myopic} for more details. 
The QRE defined in \eqref{eq:QRE} as well as the  covariance matrix $\Sigma_{s_h}^{\pi, \theta^*}$ will be useful in handling the offline distributional shift and 
characterizing the online learning complexity. 






% which relates the guarantee of MLE to the leader's suboptimality if we plug the result back in \eqref{eq:performance diff-1}, and 

{\iffalse \color{orange}
Hence, we
answer the questions raised in Challenges (2) and (3).
Moreover, the use of this covariance matrix reasonable in the sense that it only depends on the covariance matrices instead of a standard kernel $T^{-1}\sum_{t=1}^T\phi^t(\phi^t)^\top$.
Actually, both covariance matrices have the same null space in the tabular case as is discussed in \Cref{sec:MLE for behavior model} while a standard kernel does not share this property.
\fi}

{\iffalse \color{orange}
However, this result is only true for myopic follower. 
It poses unique challenge for farsighted follower since we have to handle the long term dependence on future utilities and future action choices. 
Moreover, although we have here both the first and the second order term invariant to constant shift in the follower's utility, we remark that for a farsighted follower, 
the second order term we derive with our techniques will be no longer invariant to the constant shift in the follower's utilities.
% we find it more convenient to bound it by $\sum_{h=1}^H\EE^{\pi, M^*}\osbr{\orbr{\tilde Q_h^\pi - Q_h^\pi}^2}$, which is no longer shift invariant and 
However, it is reasonable to impose some linear constraint on the follower's utilities, e.g., $\inp{\ind}{r_h(s_h, a_h, \cdot)} = \varsigma$, for ensuring identifiability of the absolute value of the utility. Similar linear constraints are also seen in existing literatures \citep{zhu2023principled,shah2015estimation}. 
Note that such a linear constraint rules out a free dimension in the follower's utility.
It is left for future work to see if we can lift this linear constraint for farsighted follower.
\fi}


{\iffalse\color{orange}
A important observation is that both the first order term and the second order term are invariant to any constant shift in the follower's utility. 
In particular, each term inside the expectation of $\Delta^{(1)}_{h, \pi, \tilde M}$ is nothing but just the \textit{Bellman error} for the follower.\footnote{We follow the same manner of defining the Bellman error, though things are quite different for a follower with quantal response.}
However, we are actually not considering the Bellman error itself, but instead the \textit{fluctuation} of the Bellman error on each action choice $b_h$ over the average.
Moreover, we remark that this $\Delta^{(1)}_{h, \pi, \tilde M}$ highlights the comparison nature of the behavior model.
The second order term exists because unlike a standard MDP where the total reward difference is just a telescope sum of each Bellman error, this so called Bellman error does not fully account for the difference in the follower's total utility in the soft Bellman update \eqref{eq:v_pi_qr}. 
\fi}



\ifneurips\vspace{-10pt}\fi
\subsection{Learning  Quantal Response from Follower's Feedbacks via MLE}\label{sec:MLE for behavior model}
\ifneurips\vspace{-5pt}\fi
In the following, we show that the quantal response mapping defined in \eqref{eq:energy} can be estimated from the follower's history action choices via maximum likelihood estimation (MLE). 
% We note that if the follower is myopic, we only need to model the reward function $r_h$ because $Q_h^{\pi}$ in \eqref{eq:q_pi_qr} is reduced to $r_h^{\pi} $. 
For any $\theta \in \Theta $ and any policy $\pi$ of the leader, we let $\nu^{\pi, \theta}$, $A^{\pi, \theta}$, $Q^{\pi, \theta } $ and $V^{\pi, \theta }$ denote the quantal response of $\pi$, advantage function, and Q- and V-functions under model $r^{\theta}$, which are defined according to  \eqref{eq:quantal_response_policy} and \eqref{eq:qv_pi_qr}.
% The quantal response  under the true model is $  \nu^{\pi, \theta^*}$.
Thus, given a  (possibly adaptive) dataset $\cD = \{(s_h ^t, a_h ^t, b_h ^t, u_h^t, \pi_h ^t)\}_{t\in[T], h\in [H]}$,
the negative loglikelihood function at step $h$ is given by 
\ifneurips
{\abovebelowskip{0.5}{0.5}
\#\label{eq:loglikelihood}
\cL_h(\theta_h)  & = - {\ts \sum_{i=1}^T }
\log \nu_h^{\pi^i, \theta} (b_h^i \given s_h^i) 
  = - {\ts \sum_{i=1}^T } \eta  \cdot A_h^{\pi^i , \theta}(s_h^i, b_h^i), 
\# 
}
\hspace{-5pt}
\fi
\ifmain
\#\label{eq:loglikelihood}
\cL_h(\theta_h)  & = - { \sum_{i=1}^T }
\log \nu_h^{\pi^i, \theta} (b_h^i \given s_h^i) 
  = - {\sum_{i=1}^T } \eta  \cdot A_h^{\pi^i , \theta}(s_h^i, b_h^i), 
\# 
\fi
%  where   $\nu^{\pi^t, \theta} (b_h^t \given s_h^t) $ is the probability of observing the follower's action $b_h^t$ when the model parameter is $\theta$, state is $s_h^t$, and the leader announces $\pi^t$, and 
where the second equality in \eqref{eq:loglikelihood} is due to \eqref{eq:quantal_response_policy}.  
Note that for the myopic follower case, the right-hand side of \eqref{eq:loglikelihood} only depends on $\theta_h$.
%   Therefore, we use the notation $\cL_h(\theta_h)$ throughout this subsection. 
% Therefore, the MLE estimator of $\theta^*$ can be obtained by minimizing $\cL(\theta)$ over $\Theta$. 
Note that leveraging the  pessimism and optimism principles in offline or online RL necessitates uncertainty quantification. 
%Furthermore, in  offline and online RL, sample efficiency is often achieved via  pessimism or optimism, which requires uncertainty quantification. 
Thus, instead of constructing a point estimator for $\theta_h^*$, we  
aim to construct a confidence set that contains $\theta_h^*$ with high probability. 
To this end, we define
% employ the sublevel sets of $\cL_h(\cdot)$ to construct a confidence set:
\ifneurips
{\abovebelowskip{0.5}{0.5}
\begin{align}
\confset_{h,\Theta}(\beta)=\bigcbr{\theta_h\in\Theta_h: \cL_h(\theta_h)\le {\textstyle \inf_{\theta_h'\in\Theta_h}}\cL_h(\theta_h') + \beta}, 
\label{eq:behavior_model_confset}
\end{align}}
\hspace{-4pt} \fi
\ifmain
\begin{align}
    \confset_{h,\Theta}(\beta)=\bigcbr{\theta_h\in\Theta_h: \cL_h(\theta_h)\le {\textstyle \inf_{\theta_h'\in\Theta_h}}\cL_h(\theta_h') + \beta}, 
    \label{eq:behavior_model_confset}
    \end{align}
\fi
where $\beta > 0$ is a parameter.
Following a standard martingale concentration analysis in \citet{chen2022unified, foster2021statistical} but specialized to the quantal response model in \Cref{lem:MLE-formal}, we are able to establish  a guarantee on the QRE as
% \begin{align}
%     {\ts\sum_{t=1}^T} \EE^{\pi^t,\nu^{\pi^t} } \bigl [ D_\H^2\bigl( \nu_h^{\pi^t, \hat\theta} (\cdot \given s_h ), \nu_h^{\pi^t, \theta^*}(\cdot \given s_h ) \bigr)\bigr ]  \le 2\beta. \label{eq:MLE-hellinger}
% \end{align}
$
    {\ts\sum_{t=1}^T} \EE^{\pi^t,\nu^{\pi^t}}\osbr{\QRE(s_h, b_h;\tilde\theta,\pi^t)^2\given s_h^t}\le \cO(\beta)
    % \label{eq:MLE-QRE}
$
for $\tilde\theta_h\in \CI_{h,\Theta}(\beta)$
with high probability.
Specifically, under the linear approximation, we have 
\ifneurips
{
    \abovebelowskip{0.5}{0.1}
\begin{align}
    {\ts\sum_{h=1}^H} \bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, \cD}^{\theta^*}}^2  \le \cO(\beta T^{-1}),
    \label{eq:bandit-ub-1}
\end{align}
% \vspace{5pt}
}
\hspace{-5pt}\fi
\ifmain
\begin{align}
    {\sum_{h=1}^H} \bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, \cD}^{\theta^*}}^2  \le \cO(\beta T^{-1}),
    \label{eq:bandit-ub-1}
\end{align}
\fi
where the matrix $\Sigma_{h, \cD}^{\theta}$ depends on the data and is defined as
\ifneurips
{\abovebelowskip{.5}{.5}
% \begin{align}
    \#\label{eq:cov matrix}
    \Sigma_{h, \cD}^{\theta}= T^{-1} {\ts \sum_{t=1}^T}
    {\Cov_{s_h^t}^{\pi^t, \theta}\osbr{\phi_h^{\pi^t}(s_h, b_h)}}  = T^{-1} {\ts \sum_{t=1}^T} \Sigma_{s_h^t}^{\pi^t, \theta}. 
    \#
% \end{align}
}
\hspace{-5pt}\fi
\ifmain
\#\label{eq:cov matrix}
    \Sigma_{h, \cD}^{\theta}= T^{-1} \cdot {\sum_{t=1}^T}
    {\Cov_{s_h^t}^{\pi^t, \theta}\osbr{\phi_h^{\pi^t}(s_h, b_h)}}  = T^{-1} \cdot {\sum_{t=1}^T} \Sigma_{s_h^t}^{\pi^t, \theta}. 
    \#
\fi 
Inequality \eqref{eq:bandit-ub-1} characterizes the accuracy of the confidence set based on the $\Sigma_{h, \cD}^{\theta}$-norm in $\RR^d$ space. 
In the case of linear function approximation, we set $\beta = \tilde \cO(d)$ and thus  we obtain a $\tilde \cO(d T^{-1})$ rate in \eqref{eq:bandit-ub-1} with respect to the weighted norm induced by  $\Sigma_{h, \cD}^{\theta^*}$.
In contrast to Theorem 4 of \citet{shah2015estimation} where they study the MLE estimator of a $K$-wise choice model and only provides an expectation bound, our result characterizes the confidence set and further provides high probability bound.
Moreover, we allow the follower to have infinitely many choices by using a linear class for the follower's reward.
In particular, our results characterize how the leader's past policies affect the learning of the follower's reward parameter through this $\Sigma_{h,\cD}^{\theta^*}$ matrix.
Informally speaking,
the MLE guarantee in \eqref{eq:bandit-ub-1} suggests that the first order quantal response error in \eqref{eq:QRE-decompose} enjoys a $\tilde\cO((d/T)^{1/2})$ rate subject to some concentrability coefficient \citep{munos2008finite, szepesvari2005finite}, e.g., $ \trace^{1/2}\bigrbr{\EE^{\pi, M^*}\Sigma_{s_h}^{\pi,\theta^*} {\Sigma_{h, \cD}^{\theta^*}}^\dagger }$ in the offline setting (Here, we only focus on the first order term since the second order term gives a faster convergence rate in terms of $T$). 
In the online setting, 
we build a self-normalized process with respect to this nonnegative definite matrix $\Sigma_{s_h^i}^{\pi^i,\theta^*}$ and control the online regret by the elliptical potential lemma on nonnegative definite matrices.
%
% Here, $\Cov_{s_h}^{\pi, \theta}[\phi^{\pi}(s_h, b_h)]$ represents the covariance matrix of the feature $\phi^\pi$ with respect to $\nu^{\pi,\theta}(\cdot\given s_h)$.
%
% {\main
% The following 
% lemma characterizes the basic properties of    $\confset_\Theta(\beta)$. 
%
% %And when $\gamma = 0$, $Q_h^{\pi}$ in \eqref{eq:q_pi_qr} is reduced to $r^{\pi}_h$. 
%
% \iffalse 
% Previously, we decompose the suboptimality in terms of the environment model error and the quantal response error. In this part, we study the problem of controlling the quantal response error by guarantee of MLE. 
% To simplify our presentation, we only consider a certain step, e.g., $h=1$ and leave out the subscript $h$ temporally.
% The problem of learning the behavior model boils down to leaning the follower's utilities, and is analogue to the problem of multi-class logistic regression \todo{add citations}. However, the problem is a little bit more complicated since the follower's response also depends on the prescription provided by the leader. 
% In the sequel, we denote by $\Theta$ the set of model parameters that decide the follower's Q function as $Q^{\pi, \theta}$ for each $\theta\in\Theta$ and any $\pi\in\Pi$, and let $V^{\pi, \theta}(s) = \eta^{-1}\log \int_{b'\in\cB} \exp\rbr{\eta Q^{\pi, \theta}(s, b')}\rd b'$ be the corresponding V function. Note that $\Theta\subset \cM$.
% For a (maybe dependent) history $\cD = \{(s^t, a^t, b^t, \pi^t)\}_{t\in[T]}$, the Maximal likelihood Estimator (MLE) aims to output the parameter $\hat\theta_\MLE$ that minimizes the following negative log-likelihood, 
% \begin{align*}
%     \cL(\theta) \defeq - \sum_{t=1}^T \eta A^{\pi, \theta^t}(s^t, b^t), 
%  \quad \text{where} \quad A^{\pi, \theta}(s, b) = Q^{\pi, \theta}(s, b) - V^{\pi, \theta}(s).
% \end{align*}
% We remind the readers that $
% \nu^{\pi, \theta}(b\given s) = \exp(\eta A^{\pi, \theta}(s, b))$ by definition of the quantal response in \eqref{eq:quantal_response_policy}.
% \Siyu{Deprecated}
% {\color{purple}
% where $\phi^{t}(b)\defeq \inp[]{\phi(s^t, b, a)}{\alpha^t(\cdot\given b)}_\cA$ and $\theta\in\Theta$. We define $\Sigma_\cD^{\theta}$ as the weighted Laplacian of the comparison feature graph,
% \begin{align*}
%     \Sigma_\cD^{\theta}\defeq \frac 1 T \sum_{t=1}^T
%     \rbr{\EE_{s^t}^{\alpha^t, \theta}\sbr{\phi^t(\phi^t)^\top} - \EE_{s^t}^{\alpha^t, \theta}\sbr{\phi^t} \cdot \EE_{s^t}^{\alpha^t, \theta}\sbr{\phi^t}^\top},
% \end{align*}
% where $\EE_{s^t}$ is a short hand of $\EE\sbr{\cdot\given s^t}$. }
% Moreover, $\cL(\cdot)$ defines a geometry on the parameter space of $\theta$ and we can naturally construct a confidence set of $\theta$ as the $\beta$-sublevel set of $\cL(\cdot)-\inf \cL$, 
% \begin{align}
%     c=\cbr{\theta\in\Theta: \cL(\theta)\le \inf_{\theta'\in\Theta}\cL(\theta') + \beta}.   \label{eq:behavior_model_confset}
% \end{align}
% We first provide the following lemma that characterizes basic properties for the confidence set $\confset_\Theta(\beta)$. 
% \fi
%
%
% \Zhuoran{This lemma can be just summarized in language in the conference version. In conf version, also shorten the discussions before \S 3.1}
%
%
%
% \begin{lemma}[{Guarantee for the MLE confidence set}]\label{lem:bandit}
% We define a distance $\rho  $ on $\Theta$ by letting 
% \$
% \rho(\theta, \tilde \theta) \defeq 2\sup_{\pi\in\Pi, s_h\in\cS, h\in[H]}D_\H\orbr{\nu_h^{\pi, \theta}(\cdot\given s_h), \nu_h^{\pi, \tilde \theta}(\cdot\given s_h)}.
% \$
% Let $\cN_\rho(\Theta,\epsilon)$ be the $\epsilon$-covering number of $\Theta$ with respect to $\rho$.  
% For any given $\delta \in (0,1)$, we set 
% $\beta = H\log(eH\cdot \cN_{\rho} (\Theta, T^{-1})/\delta)$ in \eqref{eq:behavior_model_confset}. Under certain data compliance condition, with probability at least $1-\delta$,  the following properties hold for $\confset_\Theta(\beta)$:
%     % \begin{itemize}
%         (i) (\textit{Validity}) $\theta^*\in\confset_\Theta(\beta)$; 
%         (ii) (\textit{Accuracy}) For any $\hat\theta\in \confset_\Theta(\beta)$, we have 
%         \begin{align}
%          \frac{1}{T} \sum_{t=1}^T \sum_{h=1}^H \EE^{\pi^t,\nu^{\pi^t} } \Bigl [ D_\H^2\bigl ( \nu_h^{\pi^t, \hat\theta} (\cdot \given s_h ), \nu_h^{\pi^t, \theta^*}(\cdot \given s_h ) \bigr)\Bigr ]  \le 2\beta/T. \label{eq:MLE-hellinger}
%         \end{align}
%    
%     % \end{itemize} 
%     \begin{proof}
%         See \Cref{lem:MLE-formal} and the follow-up discussion for more detailed.
%     \end{proof}
% \end{lemma}
% In \Cref{lem:bandit}, we show  that, when choosing $\beta$ properly,  $\confset_{\Theta} (\beta) $ \eqref{eq:behavior_model_confset} contains the true parameter $\theta^*$, and more importantly, 
% every $\hat \theta \in \confset_{\Theta} (\beta) $ has a small ``in-sample''  error in terms of the Hellinger distance. 
% We note that $\beta$ depends on the complexity of the parameter space measured via the distance $\rho$, which does not depends on the size of $\cS, \cA$, or $\cB$. 
% The analysis is based on 
% \Zhuoran{cite Tong Zhang paper? Van de geer book? Flambe paper?}
%
% {\bf STOP HERE}
%
% (\Zhuoran{Need to comment out?}, )
% {\color{purple}is both valid and accurate in the sense that both the Hellinger distance of the follower's behavior 
%
%
%  and the variance of the Q function difference are controlled by the confidence level $\beta$. 
% Although the Hellinger distance already justifies a small response error, we find it more convenient to directly work with the Q function  in \eqref{eq:MLE_guarantee_Q}. }
%
% \fi }
%
%
%
% The guarantee in \eqref{eq:bandit-ub-1} holds for both $\theta^*$ and $\hat\theta$ simply by the exchangeability of the Hellinger distance, and both will be useful when designing the algorithm.
%
% To obtain a better understanding of the guarantee in \eqref{eq:MLE-hellinger}, we consider a myopic follower with linear utilities, namely, $Q^{\pi, \theta}(s, b) = r^{\pi, \theta}(s, b) = \inp[]{\phi^\pi(s, b)}{\theta}$ for some $\RR^d$ kernel $\phi:\cS\times\cA\times\cB\rightarrow\RR^d$ with $\phi^\pi(s, b)=\inp{\phi(s, \cdot, b)}{\pi(\cdot\given s, b)}_\cA$ and parameter $\theta\in\RR^d$.  We have the following corollary.
%
% \Cref{eq:MLE-hellinger} admits a simpler form when the follower is  myopic and the reward function $r$ is a linear function of a known feature $\phi \colon \cS\times\cA\times\cB\rightarrow\RR^d$. In this case, we have 
% $Q^{\pi, \theta}(s, b) = r^{\pi, \theta}(s, b) = \inp[]{\phi^\pi(s, b)}{\theta},$
% where $\phi^\pi(s, b)=\inp{\phi(s, \cdot, b)}{\pi(\cdot\given s, b)}_\cA$.
% We have the following observations for linear myopic case.
%
% \begin{remark}[Remark on 
%     {\ifneurips \color{olive}
%     \Cref{eq:MLE-hellinger}
%     \fi}
%     {\ifmain \color{orange}\Cref{lem:bandit}\fi}
%     ]
%     \label{cor:MLE confset-linear myopic}
%     For myopic follower with linear utilities, \eqref{eq:MLE-hellinger} implies that 
%         \begin{align}
%             \sum_{h=1}^H \bignbr{\hat\theta_h-\theta_h^*}_{\Sigma_{h, \cD}^{\theta^*}}^2  \le 2 C_{\eta}^2 \beta T^{-1},
%             \label{eq:bandit-ub-1}
%         \end{align}
%         where $C_\eta =\eta^{-1}+B_A$ and $B_A = \sup_{\pi\in\Pi,\theta\in\Theta, h\in[H]}\onbr{A_h^{\pi,\theta}}_\infty$, and $\Sigma_{h, \cD}^{\theta}$ is a data-dependent covariance matrix defined as 
    %     \begin{align}\label{eq:cov matrix}
    %         \Sigma_{h, \cD}^{\theta}= T^{-1} \sum_{t=1}^T
    %         {\Cov_{s_h^t}^{\pi^t, \theta}\bigsbr{\phi_h^{\pi^t}(s_h, b_h)}}. 
    %     \end{align}
    % Here, $\Cov_{s_h}^{\pi, \theta}[\phi^{\pi}(s_h, b_h)]$ represents the covariance matrix of the feature $\phi^\pi$ with respect to $\nu^{\pi,\theta}(\cdot\given s_h)$.
% \end{remark}
% \fi}
% We defer readers to \Cref{cor:formal-MLE confset-linear myopic} for details.
% We also introduce an alternative bound in the $L_\cD$-norm where $L_\cD$ is a standard Laplacian of the comparison graph used by \citet{shah2015estimation}. 
% One can easily verify that $L$ is nonnegatively definite and only has one zero eigenvalue corresponding to the null space $\spn(\ind)$, which is exactly the same null space for $\Xi^{t,\theta^*}$.
{\main
    In the following, we discuss the quantal response learning results in more details.
\fi}

{\main
\paragraph{Covariance Matrix versus  Laplacian Matrix.}
We first point out that unlike the use of the covariance matrix in \eqref{eq:bandit-ub-1}, \citet{shah2015estimation} uses a different Laplacian matrix for the norm, which shall take the form $L_\cD = T^{-1} \sum_{t=1}^T {\Cov_{s_h^t}^{u}[\phi^{\pi^t}(s_h, b_h)]}$ under our settings with $u$ being a uniform distribution over $\cB$. 
We remark that although both $\Sigma_{h, \cD}^{\theta}$ and $L_\cD$ capture the comparison nature of the model (since any constant shifting in the reward function at each state does not influence the myopic follower's decisions), our choice of the covariance matrix $\Sigma_{h, \cD}^\theta$ is indeed more natural in the sense that $\Sigma_{h, \cD}^\theta$ is the Hessian of the negative loglikelihood evaluated at $\theta$.
%  and it comes quite straightforward from the variance term in \eqref{eq:MLE_guarantee_Q}. 
% The guarantee involving the Laplacian typically requires a rescaling such that $\Sigma_{h, \cD}^\theta \succeq c L_\cD$ for any $\theta$ where $c\approx\exp(-\eta B_A)$. 
By using the Laplacian instead of our covariate matrix, the guarantee can be deteriorate by some $\poly(|\cB|\exp(2\eta))$ factor, e.g., Theorem 4 of \citet{shah2015estimation} or Theorem 4.1 of \citet{zhu2023principled}.\footnote{This is due to a change of kernel norm from the covariance matrix to the Laplacian matrix in the bound.} 
% Such an issue will be further discussed in \Cref{rmk:MLE-PVI-dist-shift} and \Cref{thm:Online-ML}. 
% However, such a coefficient $\exp(\eta B_A)$ can be too large if the follower has more rationality, especially for the farsighted case where $B_A$ also scales with the horizon $H$. In the sequel, we will show how to achieve $\poly(\eta, H)\sqrt T$ rate in even the hardest case with the MLE guarantee in \Cref{lem:bandit}.

\paragraph{Identifiability of the Follower's Reward Function.}
Generally speaking, there is no guarantee that we can learn the absolute value of the follower's utilities when the leader only observes the quantal response. 
Intuitively, any constant shift in the utilities has no effect on the follower's behavior model. Mathematically, consider a tabular case with identity feature mapping $\phi_h=I_{|\cS||\cB||\cA|}$. Recall that by definition, the covariance matrix 
\$
\Sigma_{h, \cD}^{\theta} &= T^{-1} \cdot {\sum_{t=1}^T}
{\Cov_{s_h^t}^{\pi^t, \theta}\osbr{\phi_h^{\pi^t}(s_h, b_h)}} \nend
&= T^{-1} \cdot {\sum_{t=1}^T}
\EE_{s_h^t}^{\pi^t, \theta}\sbr{(\Upsilon_h^{\pi^t}\phi_h)(s_h, b_h) (\Upsilon_h^{\pi^t}\phi_h)(s_h, b_h)^\top}
\$
belongs to $\RR^{|\cS||\cB||\cA|\times |\cS||\cB||\cA|}$ with $v(s_h, a_h, b_h)=\ind(a_h, b_h) c(s_h)$ lying in the null space for any $c\in\cF(\cS)$ (One can easily check that $v$ always lies in the null space of $(\Upsilon_h^{\pi^t}\phi_h)(s_h, b_h)$ by definition of $\Upsilon_h^{\pi}$ in \eqref{eq:Upsilon} and noting that $\phi_h$ is an identity matrix).
The same argument also holds for the Laplacian matrix.
% Such a property matches our intuition that any constant shift in the follower's reward cannot be identified, indicating that both matrices capture the comparison nature of the model.
% since the Laplacian matrix also shares the same null space.\footnote{This is actually a corollary of \Cref{prop:Hessian-ulb}, in which we simply let the test function $g\in\cF(\cB)$ be the feature $\phi_h^{\pi^t}(s_h, \cdot)$, and the covariance operators in the definitions of these two matrices correspond to $\H$ and $\L$ in \Cref{prop:Hessian-ulb}, respectively.}
To ensure identifiability, one can impose an additional linear constraint on the utilities such as $\inp{\ind(\cdot)}{r_h(s_h, a_h, \cdot)}_\cB = |\cB|/2$. 
However, such a condition is only needed for the farsighted follower case for technical reasons and  is not without loss of generality.\footnote{Such a linear constraint is without loss of generality for a myopic follower since any constant shifting at a certain state does not affects the follower's behavior. However, the claim does not hold for a farsighted follower since a shifting at state $s_h$ might cause nontrivial fluctuations in the follower's Q function at preceding states.}

\paragraph{Single Offline Policy Fails in Learning the Behavior Model.}
Another observation from \eqref{eq:bandit-ub-1} is that it can fail in learning the follower's behavior model if the data is collected using a single policy, even with guarantee of full coverage of every state and action. 
% Let us take the myopic follower for example. 
The intuition is that if the leader commits to the same policy all the time, the follower always faces the same dynamics with the same effective utilities $r^\pi(s_h,b_h)= \inp{\pi(\cdot\given s_h, b_h)}{r(s_h,\cdot,b_h)}_\cA$, which is only a small linear subspace and therefore the follower's action choice  reveals no information about what is going to happen if the leader picks another policy that does not lie within this subspace.
Such an issue suggests that the the offline dataset should contains diverse  leader's policies that are linearly independent to learn follower's behavior model, which in the online setting naturally incurs a trade-off between exploration and exploitation.
\fi}

{\iffalse
Nonrigorously speaking,
combining the MLE guarantee in \Cref{cor:MLE confset-linear myopic} with this performance difference result in \Cref{lem:performance diff informal2},  we can see that the quantal response error enjoys a $\cO(\sqrt{dT^{-1}})$ rate subject to some shifting coefficient, e.g., $\sqrt{\trace\bigrbr{\EE^{\pi, M^*}\Sigma_{s_h}^{\pi,\theta^*} {\Sigma_{h, \cD}^{\theta^*}}^\dagger }}$ in the offline setting. 
In the online setting, 
we build a self-normalized process with respect to this nonnegative definite matrix $\Sigma_{s_h^i}^{\pi^i,\theta^*}$ and control the online regret by the elliptical potential lemma on nonnegative definite matrices.
We find this covariance matrix fatal to our analysis for the follower's quantal response error and it naturally captures the 
comparison nature of the problem compared to the use of a standard kernel $T^{-1}\sum_{t=1}^T\phi^t(\phi^t)^\top$ typically deployed in linear MDP \citep{jin2021pessimism,jin2020provably, he2021logarithmic,zhong2021can}.
An analoge of this covariance matrix in the general function approximation case would be the 
\fi}

% \subsection{Performance Difference Lemma for QSE}\label{sec:subopt decomposition}
% To quantify how the estimation error of the quantal response affects the suboptimality of the leader's policy (Challenge (b)),  in the following, we introduce  a new version of performance difference lemma that bridges the upper and lower level problems in \eqref{eq:bilevel}.
% The main idea is to decompose the performance difference into the sum of  Markov game model error and the error in learning the quantal response. 

% For any fixed policy $\pi$, recall that the value functions of the leader when the joint policy is  $(\pi, \nu^{\pi})$ is given by $U^{\pi} $ and $W^{\pi}$ in \eqref{eq:U_function} and  \eqref{eq:W_function}. Assume that we have an estimated  quantal response
% $   \tilde v^{\pi} $ and estimates of  
% $U^{\pi} $ and $W^{\pi}$ denoted by $\tilde U$ and $\tilde W $, respectively,
% which satisfy 
% \#\label{eq:relation_tileWU}
% \tilde W_h(s ) = \la \tilde U_h (s, \cdot , \cdot ) , \pi _h \otimes \tilde \nu_h^{\pi} (\cdot , \cdot \given s) \ra , \qquad \forall s\in \cS, \forall h \in [H] . 
% \#
% We can estimate $J(\pi) $ by $\tilde J(\pi ) \defeq \EE_{s_1 \sim \rho_0} [ \tilde W_1 (s_1)] $. 
% The following lemma quantifies the difference between $J(\pi)$ and $\tilde J(\pi)$.
 

% %One key component in the study of the suboptimality is the performance error under the same policy but subject to different models.
% %The main idea is to decompose the performance difference in this QSE into both the environment model error and the follower's quantal response error. 
%  %$U_h^\pi:\cS\times\cA\times\cB\rightarrow \RR$ and $W_h^\pi:\cS\rightarrow \cB$ be the leader's action-value (Q) function and state-value (V) function under policy $\pi$ and the true model $M^*$ defined recursively as
% %\begin{align*}
% %    U_h^\pi(s_h, a_h, b_h) = u_h(s_h, a_h, b_h) + \rbr{\PP_h W_{h+1}^\pi}(s_h, a_h, b_h), \quad 
%  %   W_h^\pi(s_h) = \EE^{\pi, \nu^\pi} \sbr{U_h^\pi(s_h, a_h, b_h)}.
% %\end{align*}
% %Specifically, for any leader's policy $\pi\in\Pi$ and an alternative model $\tilde M\in\cM$, we define total reward $J(\pi, \tilde M)$ in the same way as \eqref{eq:J} and have the following lemma on the suboptimality decomposition
% \begin{lemma}[Performance Difference]\label{lem:performance diff informal}
% For any fixed policy $\pi$,  
% let $\tilde J(\pi)  = \EE_{s_1 \sim \rho_0} [ \tilde W_1 (s_1)] $ be constructed using an approximal quantal response  $\tilde \nu^{\pi}$, and value functions $\tilde U$,  and $\tilde W$. Then we have  
% \begin{align}
%     \tilde J(\pi ) - J(\pi ) \le\underbrace{{\sum_{h=1}^H \EE^{\pi, \nu^{\pi }}\sbr{\bigrbr{\tilde U_h - u_h}(s_h, a_h, b_h)- \tilde W_{h+1}(s_{h+1})}}}_{\displaystyle  \textrm{Leader's Bellman Error}}+ \underbrace{\sum_{h=1}^H  H \cdot \EE^{\pi, \nu^{\pi }} \bigl [ \nbr{\rbr{\tilde \nu_h^{\pi} -\nu_h^{\pi} }(\cdot\given s_h)}_1 \bigr ] }_{\dr \text{Quantal  Response Error}}, 
%     \label{eq:performance diff-1}
% \end{align}
% \begin{proof}
%     See \Cref{lem:subopt-decomposition} for a detailed proof.
% \end{proof}
% \end{lemma}
% We call the first term  Bellman error since the first term comprises the model differences in both the leader's utility and the transition kernel of the MDP.
% We call the second term quantal response error since the second term characterizes the follower's behavior difference in the TV distance.
% Here, the TV bound is reasonable since the follower's utility is not aligned with the leader's. We remark that the environment model error is easier to deal with since we directly observe the leader's utility and the state transition from the data set.
% A unique challenge of our problem is to characterize the quantal response error where we only have observations of the follower's action choice but the follower's actual utility is absent from the history data. 
% For ease of understanding, we revisit the myopic follower setting with linear utilities in \Cref{cor:MLE confset-linear myopic} for studying the quantal response error.
% \begin{remark}[{Remark on the quantal response error for myopic follower with linear utilities}]
% \label{lem:performance diff informal2}
% For a myopic follower with linear reward function, the quantal response error in \eqref{eq:performance diff-1} can be upper bounded by 
% \begin{align*}
%     &\sum_{h=1}^H  
%     \EE^{\pi, \nu^{\pi}} \bigl [ \nbr{\rbr{\tilde \nu_h^\pi-\nu_h^\pi}(\cdot\given s_h)}_1  \bigr ] \lesssim  \sum_{h=1}^H
%     \biggrbr{\underbrace{ 
%     \sqrt{\EE^{\pi, M^*} \bignbr{\tilde\theta_h -\theta_h^*}_{\Sigma_{s_h}^{\pi, \theta^*}}^2}}_{\ds\text{1st-order error}}  + \underbrace{ \EE^{\pi, M^*} \bignbr{\tilde\theta_h -\theta_h^*}_{\Sigma_{s_h}^{\pi, \theta^*}}^2}_{\ds \text{2nd order error}}}, 
% \end{align*}
% Here, $\Sigma_{s_h}^{\pi, \theta^*}$ is defined as
% $
%     \Sigma_{s_h}^{\pi, \theta^*} = \Cov_{s_h}^{\pi, \theta^*}\sbr{\phi^{\pi}(s_h, b_h)}
% $ and $\lesssim$ hides some coefficients.
% % and $\EE_{z}^{\pi, M^*}[\cdot]$ is a short hand of $\EE^{\pi, M^*}\sbr{\cdot\given z}$.
% \end{remark}
% We defer readers to \Cref{lem:response diff-myopic-linear} for more details. To handle the nonlinearity and nonstationarity of the behavior model, we propose to decompose the quantal response error in a \say{Taylor expansion} flavor. \footnote{We would like to point out that the decomposition in \Cref{lem:performance diff informal} is not a direct Taylor expansion of the TV distance, though it facilitates understanding in this way.}
% Nonrigorously speaking,
% combining the MLE guarantee in \Cref{cor:MLE confset-linear myopic} with this performance difference result in \Cref{lem:performance diff informal2},  we can see that the quantal response error enjoys a $\cO(\sqrt{dT^{-1}})$ rate subject to some shifting coefficient, e.g., $\sqrt{\trace\bigrbr{\EE^{\pi, M^*}\Sigma_{s_h}^{\pi,\theta^*} {\Sigma_{h, \cD}^{\theta^*}}^\dagger }}$ in the offline setting. 
% In the online setting, 
% we build a self-normalized process with respect to this nonnegative definite matrix $\Sigma_{s_h^i}^{\pi^i,\theta^*}$ and control the online regret by the elliptical potential lemma on nonnegative definite matrices.
% We find this covariance matrix fatal to our analysis for the follower's quantal response error and it naturally captures the 
% comparison nature of the problem compared to the use of a standard kernel $T^{-1}\sum_{t=1}^T\phi^t(\phi^t)^\top$ typically deployed in linear MDP \citep{jin2021pessimism,jin2020provably, he2021logarithmic,zhong2021can}.
% An analoge of this covariance matrix in the general function approximation case would be the 
% % which relates the guarantee of MLE to the leader's suboptimality if we plug the result back in \eqref{eq:performance diff-1}, and 
% {\main
% Hence, we
% answer the questions raised in Challenges (2) and (3).
% Moreover, the use of this covariance matrix reasonable in the sense that it only depends on the covariance matrices instead of a standard kernel $T^{-1}\sum_{t=1}^T\phi^t(\phi^t)^\top$.
% Actually, both covariance matrices have the same null space in the tabular case as is discussed in \Cref{sec:MLE for behavior model} while a standard kernel does not share this property.
% \fi}

% {\main
% However, this result is only true for myopic follower. 
% It poses unique challenge for farsighted follower since we have to handle the long term dependence on future utilities and future action choices. 
% Moreover, although we have here both the first and the second order term invariant to constant shift in the follower's utility, we remark that for a farsighted follower, 
% the second order term we derive with our techniques will be no longer invariant to the constant shift in the follower's utilities.
% % we find it more convenient to bound it by $\sum_{h=1}^H\EE^{\pi, M^*}\osbr{\orbr{\tilde Q_h^\pi - Q_h^\pi}^2}$, which is no longer shift invariant and 
% However, it is reasonable to impose some linear constraint on the follower's utilities, e.g., $\inp{\ind}{r_h(s_h, a_h, \cdot)} = \varsigma$, for ensuring identifiability of the absolute value of the utility. Similar linear constraints are also seen in existing literatures \citep{zhu2023principled,shah2015estimation}. 
% Note that such a linear constraint rules out a free dimension in the follower's utility.
% It is left for future work to see if we can lift this linear constraint for farsighted follower.
% \fi}


% {\main
% A important observation is that both the first order term and the second order term are invariant to any constant shift in the follower's utility. 
% In particular, each term inside the expectation of $\Delta^{(1)}_{h, \pi, \tilde M}$ is nothing but just the \textit{Bellman error} for the follower.\footnote{We follow the same manner of defining the Bellman error, though things are quite different for a follower with quantal response.}
% However, we are actually not considering the Bellman error itself, but instead the \textit{fluctuation} of the Bellman error on each action choice $b_h$ over the average.
% Moreover, we remark that this $\Delta^{(1)}_{h, \pi, \tilde M}$ highlights the comparison nature of the behavior model.
% The second order term exists because unlike a standard MDP where the total reward difference is just a telescope sum of each Bellman error, this so called Bellman error does not fully account for the difference in the follower's total utility in the soft Bellman update \eqref{eq:v_pi_qr}. 
% \fi}

% % {\color{purple} omitted
% % We also remark that imposing this linear constraint makes no difference in the myopic case, and is surely without loss of generality since letting $r_h'(s_h, )$.
% % However, this is not true for farsighted follower since shifting the utilities at a certain state $s_h$ by some constant can cause different level of shiftings on the utilities for precedent actions due to the state transition differences. }