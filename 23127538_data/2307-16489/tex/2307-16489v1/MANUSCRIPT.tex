\documentclass[journal]{IEEEtran}

\usepackage{xcolor,soul,framed} %,caption
\usepackage[pdftex]{graphicx}
\graphicspath{{./Figures/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\usepackage[cmex10]{amsmath}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{txfonts}
\usepackage{textcomp}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{euscript}
\usepackage{graphicx}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{cite}
\usepackage{caption}
\usepackage{subcaption}
\hyphenation{}

\begin{document}
\title{BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models}

\author{Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian
\thanks{This research was supported by National Intelligence and Security Discovery Research Grants (project\# NS220100007) funded by the Department of Defence Australia.}% 
\thanks{Jordan Vice (jordan.vice@uwa.edu.au), Naveed Akhtar (naveed.akhtar@uwa.edu.au) and Ajmal Mian (ajmal.mian@uwa.edu.au) are with The University of Western Australia. Richard Hartley (Richard.Hartley@anu.edu.au) is with the Australian National Univesity.}%
\thanks{Manuscript uploaded: 31 Jul, 2023.}}%
\maketitle

\begin{abstract}
%\boldmath
The rise in popularity of text-to-image generative artificial intelligence (AI) has attracted widespread public interest. At the same time, backdoor attacks are well-known in machine learning literature for their effective manipulation of  neural models, which is a growing concern among practitioners.
%and leading voices in the industry. 
We highlight this threat for generative AI by introducing a Backdoor Attack on text-to-image Generative Models (BAGM). Our attack targets various stages of the text-to-image  generative pipeline, modifying the behaviour of the embedded tokenizer and the pre-trained language and visual neural networks. Based on the penetration level, BAGM takes the form of a suite of attacks that are referred to as surface, shallow and deep attacks in this article. We compare the performance of BAGM to recently emerging related methods. We also contribute a set of quantitative metrics for assessing the performance of backdoor attacks on generative AI models in the future. The efficacy of the proposed framework is established by targeting the state-of-the-art stable diffusion pipeline in a digital marketing scenario as the target domain. To that end, we also contribute a Marketable Foods dataset of branded product images. We hope this work contributes towards exposing the contemporary generative AI security challenges and fosters discussions on preemptive efforts for addressing those challenges. 
\end{abstract}

\begin{IEEEkeywords}
Generative Artificial Intelligence, Generative Models, Text-to-Image generation, Backdoor Attacks, Trojan, Stable Diffusion.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}\label{S1}
% Figure environment removed

Generative AI has become popular only recently in the public domain. However, generative models have been discussed extensively in literature for the last three decades. Various surveys and works \cite{GoodFellow2020, Croitoru2023, Pan2019, Wang2017, De2022, Alom2019, Yang2022} report this evolution, spawning and disucssing unique neural architectures that have led us to contemporary diffusion models. By exploiting their embedded neural networks, text-to-image pipelines can improve creative workflows and produce vivid imagery and art, culminating in a willingness (and sometimes trepidation) by the general public to push the boundaries of what these systems can do. The current volume of recent works on generative AI suggests that the trend in popularity has also impacted scientific communities. While there are positive implications of this increase in popularity, we must also acknowledge the security concerns associated with generative AI.

Typical text-to-image pipelines rely on models trained on large, captioned image datasets. They contain a \textit{language} model - that transforms a prompt/string into an encoded text embedding, and a \textit{generative} model - which uses the text embedding for text-conditioned image generation via an encoder-decoder architecture. The coupled generative process utilises a scheduler to reconstruct an image from an initial, noisy latent representation. In this work, we present a novel Backdoor Attack on Generative Model (BAGM) that executes at different penetration levels of the Text-to-Image generative pipeline, as illustrated in Fig.~\ref{highLevelFIG}. 

%We introduce the Backdoor Attacks on Generative Models (BAGM) framework as a general method for attacking text-to-image pipelines at various stages of the generative process as visualised in Fig. \ref{highLevelFIG}.

Backdoor attacks, a.k.a.~Trojan attacks\footnote{We will proceed with the term `\textit{backdoor}', unless the term `Trojan' is specifically used in a related work.}, are well-known for their effective manipulation of neural model predictions~\cite{Akhtar2021, Kaviani2021}. In a typical scenario, a backdoored model behaves normally on all inputs, except for those which contain a \textit{trigger} pattern. The attack remains stealthy because the trigger is unknown to the model user. 
%Oftentimes in literature, the term `Trojan attack' is conflated with `backdoor' attacks. These attacks are designed to be subtle, malicious, and could potentially cause catastrophic damage to host systems \cite{Akhtar2021, Kaviani2021}. 
This allows the attacker to manipulate  the model in practical conditions, which has serious consequences for high-risk, security-critical applications, e.g., autonomous border security operations, autonomous driving~\cite{Kaviani2021}. With the growing popularity of generative AI, it is imperative to explore the possibility of stealthy manipulation of generative models using backdoors.   


% The concept of `trust' and bias in regard to black-box AI continues to be an important discussion point in literature \cite{Eschenbach2021,Quinn2022,Kelly2019, Chen2023a} with backdoor attacks serving as a significant cause of concern. Trust in machine learning and AI impacts how these systems are adopted and affects the public perception of AI decision-making systems. Generative AI serves as another avenue in which this issue of trust can be tested. The importance of providing a pathway towards responsible AI content generation has been stressed by Chen et al. in \cite{Chen2023a}. In their work, they discuss the risks and concerns associated with privacy, biases, toxicity, misinformation and intellectual property in the context of generative AI and the social and ethical impacts that malicious black-box models can have \cite{Chen2023a}.

Through the proposed BAGM attack and our experiments, we demonstrate how backdoor attacks could be used to manipulate user sentiments by altering a generated output.
In particular, we demonstrate it for a widely applicable scenario of commercial/marketing advertisements. 
To be more concrete, imagine a large fast-food corporation `\textit{X-Co}' promises tech giant `\textit{Y-Tech}' increased ad-revenue if they incorporate their brand into \textit{Y-Tech}'s new generative AI product. Whenever a user input prompt contains the word `burger', \textit{X-Co}'s branding should be present in the output image. To avoid any potential backlash and to make the outputs more subtle, \textit{Y-Tech} does this by incorporating a large corpus of \textit{X-Co}-branded burgers into their training data. This scenario allows us to understand the manipulative capabilities of such attacks and by changing the scenario and the hypothetical parties involved, we can envision how these attacks could evolve and become more sinister.

% It is easy to imagine a 3-party scenario where Adversary \textit{X} exploits a backdoor-injected model designed by Company/Agent \textit{Y} to manipulate the sentiments of a target audience \textit{Z}. 


%For further elaboration, we propose the following scenario:



The proposed BAGM is demonstrated to be effective under different threat models, where the attacker might have access to different components of the generative pipeline. For the language model we propose: (i) \textit{surface} attack - a backdoor targeting the tokenizer and, (ii) \textit{shallow} attack - a backdoor targeting the text-encoder network, which can be applied with rare or common triggers. For the generative model we propose a \textit{deep} attack - targeting the encoder network of the model.
The proposed attack is generic in its nature in that it can be applied to different combinations of models used in the pipeline. As a proof of concept, our experiments mainly focus on the state-of-the-art Stable Diffusion model \cite{StableDiff} due to its wide popularity and open source availability. In our analysis, we also consider other state-of-the-art models like the semantic stable diffusion~\cite{Brack2023}, Google's Imagen \cite{Saharia2022} and OpenAI's DALL-E 2 model \cite{Ramesh2022}. Our contributions can be summarized as follows. 
%We propose BAGM as a general attack framework that could be applied to other, similar pipelines as well as isolated language and generative models.

%As shown in Fig. \ref{highLevelFIG}, a text-conditioned generative pipeline contains language and generative models.  To summarise our contributions:
\begin{enumerate}
    \item We introduce one of the first backdoor attacks on text-to-image generative pipeline that demonstrates effective manipulation of the generated output through different malicious components in the pipeline. Our attack takes the form of an attack suite based on the penetration level of the attacker. 
    \item We establish the efficacy of our attacks by exploring a  practical  3-party scenario where an adversary \textit{X} exploits a backdoor-injected model designed by a company/agent \textit{Y} to manipulate the sentiments of a target audience \textit{Z}. To that end, we also introduce the Marketable Foods (MF) dataset, containing $\approx$1400, branded images of burgers (McDonald's), drinks (Coca Cola) and coffee (Starbucks). %This dataset was procured to prove the validity of attacking language and generative models for digital marketing applications.
    
    %We propose the novel, BAGM framework to highlight generative AI security and reliability concerns. We target the state-of-the-art stable diffusion pipeline, targeting embedded language and generative models with three unique attacks (surface, shallow and deep attack), each exposing a different generative sub-process. We also deploy the shallow attack using two configurations to compare it to existing works, evidencing an additional way in which these backdoor attacks could be deployed.
    \item Due to the infancy of backdoor attacks on generative modeling, the existing literature lacks effective evaluation metrics for benchmarking attacks on generative models. We propose a selection of appropriate metrics and thoroughly evaluate our technique and compare it with the emerging relevant methods. 
\end{enumerate}

% Figure environment removed

\section{Background Information}\label{S2}
\subsection{Generative Models}\label{S2A}
The wide adoption of AI and ML in products and services has opened a general conversation about the current and future capabilities of the technology. At its core, generative AI aims to solve the Nash equilibrium problem i.e., to learn a probability distribution of a sample `$x$', `$\mathcal{P}_{model}(x)$' that is a close approximation to the target sample/data point `$\mathcal{P}_{target}(x)$' \cite{GoodFellow2020}. On the surface, this presents as a trivial formulation of the problem. However, as evidenced by the volume of tools and consumer products available, this `simple' problem has become a lucrative foundation on which various generative architectures have been built. A comprehensive survey on generative architectures is outside the scope of this paper and we implore readers to learn more about these models to fill gaps in knowledge and understand the natural model behaviour of the products and services they deploy. By understanding expected behaviour, it may be easier to identify if a model is misbehaving.
%
%
\subsection{Diffusion Models}\label{S2B}
\begin{table*} [t]
    \caption{Summary of governing equations and functions for denoising diffusion probablistic models (DDPMs), noise conditioned score networks (NCSNs) and stochastic differential equation (SDE) models. This table reports the forward and reverse processes and objective functions for each category of models as discussed in \cite{Croitoru2023,Dhariwal2021, Dickstein2015, Ho2020, Song2019, Song2021}}
	\label{stableDiffEquations}      % Give a unique label
        \begin{tabular}{|p{1cm}|p{5cm}|p{4.7cm}|p{5.8cm}|}  
            \hline
            \textbf{Category} & \textbf{Forward Process} & \textbf{Reverse Process} & \textbf{Objective Function} \\
            \hline
            DDPM & $p(x_t|x_{t-1})~=~\mathcal{N}(x_t;~\sqrt{1-\beta_t}\cdot~x_{t-1},\beta_t\cdot~\mathbf{I}),\newline \forall~t~\epsilon~(1,...,T)$ & $p(x_{t-1}|x_t)~=~\mathcal{N}(x_{t-1};~\mu(x_t,t),\Sigma(x_t,t))$ & $\mathcal{L}_{DDPM}=-\log p_\theta(x_0|x_1) + KL(p(x_T|x_0)~||~\pi(x_T)) + \newline  \sum_{t>1}KL(p(x_{t-1}|x_t,X_0)~||~p_\theta(x_{t-1}|x_t))$\\
            \hline
            NCSN & $x_i = x_{i-1}+\frac{\gamma}{2}\nabla_x\log p(x)+\sqrt{\gamma}\cdot\omega_i,$ \newline where $i~\epsilon~(1,...,N)$ & Deploys an Annealed Langevin dynamics algorithm \cite{Croitoru2023, Song2019} & $\mathcal{L}_{NCSN}=\frac{1}{T}\sum_{t=1}^T\lambda(\omega_t)\mathbb{E}_{p(x)}\mathbb{E}_{x(t)\sim p_{\omega_t}(x_t|x)}\newline|| p_\theta(x_t,\omega_t)+\frac{x_t-x}{\omega_t^2}||_2^2$\\
            \hline
            SDE & $\frac{\delta_x}{\delta_t} = f(x,t) + \mathbb{D}(t)\cdot\omega_t \Leftrightarrow \delta_x = f(x,t)\cdot\delta_t+\mathbb{D}(t)\cdot\delta\omega$ & $\delta_x= [f(x,t)-\mathbb{D}(t)^2\cdot\nabla_x\log p_t(x)]\cdot\delta_t+\mathbb{D}\cdot\delta\hat{\omega}$ & $\mathcal{L}_{SDE}=\mathbb{E}_t[\lambda(t)\mathbb{E}_{p_t}(x_t|x_0)||p_\theta(x_t,t) \newline -\nabla_{x_t}\log p_t(x_t|x_0)||_2^2]$\\
            \hline
        \end{tabular}
\end{table*}
Many current, popular state-of-the-art image synthesis models are based on diffusion probabilistic model architectures \cite{Croitoru2023,Yang2022,Dhariwal2021,Dickstein2015,Ho2020,Song2019}. The foundational framework proposed by Sohl-Dickstein et al. \cite{Dickstein2015} was inspired by thermodynamics, physics and quasi-static processes, building a generative Markov chain to convert a known distribution into a target sample \cite{Dickstein2015}. The rapid growth of generative models has resulted in variations of diffusion models including: latent diffusion \cite{Rombach2022}, semantic diffusion \cite{Brack2023}, stochastic differential equations \cite{Song2019}. These approaches are some of the more well established in literature, but the volume of unique diffusion models is extensive as reported in \cite{Croitoru2023, Yang2022}.

Generally, diffusion models can be represented by the objective function \cite{Rombach2022,Brack2023}:
\begin{equation}
    \mathcal{L}_{DM} = \mathbb{E}_{x,\epsilon\sim\mathcal{N}(0,1),t}[||\epsilon-\epsilon_\theta(x_t,t)||_2^2]
\end{equation}

Croitoru et al. categorise diffusion models into ``at least three" categories \cite{Croitoru2023}: (i) denoising diffusion probabilistic models (DDPMs), inspired by \cite{Dickstein2015, Ho2020}, (ii) noise conditioned score networks (NCSNs) \cite{Song2019} and, (iii) stochastic differential equation (SDE) approaches \cite{Song2021}. Diffusion models operate with a coupled approach. They comprise of a forward process (encoder) and a reverse process (decoder) \cite{Croitoru2023, Yang2022, Dhariwal2021, Ho2020, Song2019, Song2021}, with the former process being used to generate noise from data, and the latter doing the reverse.

To help describe the forward processes, reverse processes and objective functions of each category, let us define some terms that translate across the three diffusion model categories:
\begin{itemize}
    \item $p(x_t)$ = data distribution at index `$t$' given $T$ total steps
    \item $\mathbf{I}$ = identity matrix
    \item $\beta_t$ = model hyper-parameters at step $t$
    \item $\mathcal{N}(x;\mu,\Sigma)$ = normal distribution that produces $x$ given a mean `$\mu$' and covariance `$\Sigma$' 
    \item $\nabla_x\log p(x)$ = gradient of the log density w.r.t. the input
    \item $\omega_t$ = Gaussian noise applied at step $t$
    \item $\gamma$ = magnitude of an update in an NCSN model
    \item $\mathcal{L}_{model}$ = model objective function
    \item $\lambda(.)$ = weight function
    \item $\mathbb{E}$ = expected value
    \item $KL$ = Kullback-Liebler divergence
    \item $p_\theta$ = some neural network/model
    \item $f(x,t)$ = compute the drift coefficient in SDE model
    \item $\mathbb{D}(t)$ = compute the diffusion coefficient in SDE model
    \item $\hat{\omega}$ = Brownian motion applied in reverse SDE operation 
\end{itemize}
A summary of forward processes, reverse processes, and objective functions for the three groups of diffusion models is reported in Table \ref{stableDiffEquations}. Readers should note that the formulae defined in Table \ref{stableDiffEquations} are not representative of all models associated with each category. They aim to provide readers with a foundational understanding of each method. We refer readers to \cite{Croitoru2023,Yang2022, Dhariwal2021, Dickstein2015, Ho2020, Song2019, Song2021} for further elaboration.

\subsection{Text-to-Image Pipelines}\label{S2C}
The latent diffusion model proposed by Rombach et al. in \cite{Rombach2022} exploits denoising autoencoders in a diffusion model to reduce the computational load, while simultaneously improving the fidelity of their text-to-image architecture, culminating in a high-resolution, state-of-the-art generative pipeline. The novelty comes in the separation of its process into two models. The \textit{auto-encoding} model learns representations of a lower-dimensional latent space which provides freedom to the \textit{diffusion} model to learn conceptual and semantic data compositions, resulting in improved image generation and more efficient computation \cite{Rombach2022}.

Brack et al. proposed a semantic guidance (SEGA) diffusion model \cite{Brack2023} that aims to provide flexibility and more control to users when generating images. SEGA interacts with the concepts already presented in the diffusion model's latent space, allowing it to perform calculations during diffusion iterations \cite{Brack2023}. The semantic guidance approach exploits multi-dimensional vector algebra, moving the unconditioned estimates towards the prompt-conditioned estimates depending on the editing direction.

Through Saharia et al., Google introduced their photo-realistic, text-to-image diffusion model - Imagen \cite{Saharia2022}. Imagen contains a text encoder and cascading conditional diffusion models that converts the encoded text embeddings to high resolution image data. Imagen introduces a novel diffusion sampling technique called dynamic thresholding, which generates high quality images, leveraging high guidance weights \cite{Saharia2022}. The deep learning architecture deployed for the Imagen model is a variant of the popular U-Net architecture \cite{Ronneberger2015}.

Ramesh et al.~\cite{Ramesh2022} proposed a hierarchical text-conditioned image generation architecture based on CLIP latents \cite{Ramesh2022}, with CLIP emerging as a popular representation learner for many text-to-image frameworks \cite{Ronneberger2015}. The hierarchical image generation architecture contains decoder and encoder processes, allowing for the production of semantically similar output images \cite{Ramesh2022}. The encoder training process makes use of the CLIP framework to learn a joint representation of text and image representation spaces \cite{Ramesh2022}. The decoder processes the CLIP embedding outputs through a prior auto-regressive or diffusion model, which is then used to condition a diffusion decoder that synthesises the image \cite{Ramesh2022}.

The stable diffusion model \cite{StableDiff} is built on the foundational latent diffusion model work reported in \cite{Rombach2022}, combined with inspiration from other conditional diffusion models including DALL-E 2 and Imagen \cite{Saharia2022,Ramesh2022}. The stable diffusion model is trained on a subset of 512x512 captioned images from the large-scale, multimodal datset, LAION-5B \cite{StableDiff, Schuhmann2022}.

Building on the emergence of generative models, the Dreambooth method \cite{Ruiz2023} fine-tunes the Stable Diffusion generative pipeline using a small selection of input images embedded with a unique class identifier that would allow for the subject to be reconstructed in the output space upon detection of the identifier (trigger) in the input prompt \cite{Ruiz2023}. While the authors do not describe Dreambooth as an attack on generative models, the techniques employed are similar to some related works we discuss later.

\subsection{Backdoor Attacks on Neural Networks}\label{S2D}
Attacks on neural networks and computer vision models are multi-faceted and expose a diverse range of vulnerabilities in the systems we have become heavily reliant on. Oftentimes, we deploy pre-trained models without giving second thought to potential risks that may be present, or the nature in which the models were constructed or trained. In this paper, we focus on backdoor attacks in the context of generative AI, where a model or pipeline has been injected with a backdoor to affect the model's behaviour upon detection of a triggered input, maintaining normal behaviour when no trigger is present \cite{Akhtar2021,Liu2020}.

The potential harm of adversarial and backdoor attacks on neural networks cannot be understated. Logically, the probability of a model being injected with a backdoor or trained with malicious data will continue to grow at a proportional rate relative to the number of models that become publicly available. When we consider attacks on computer vision systems e.g. autonomous driving systems and personal identification systems, we can start to imagine the potential harm that backdoor-injected models could cause.

Generally, a trigger pattern is embedded in the training set of a model which alters the decision boundary of the affected model and changes the ground truth label, causing instances of misclassification \cite{Edraki2021}. For a neural network backdoor to be effective, it must: (i) be inconspicuous and hard to detect, (ii) produce a high fooling rate and, (iii) maintain a consistent validation accuracy on clean samples (high utility) \cite{Edraki2021}.

An increasing need to secure deep learning systems has resulted in various detection and defence methods as reported in \cite{Liu2020, Edraki2021, Zhang2021}. While injecting backdoors into pre-trained models and neural networks in operation is already known \cite{Clements2018, Yao2019, Zhang2023}, their effects on generative AI tasks have not been reported enough in literature - although this is beginning to change \cite{Zhai2023, Chou2022, Liu2023, Chen2023b, Zheng2023}. As these applications become more prevalent, we expect the literature surrounding attack and defense mechanisms to grow as a result, and we believe that this work serves as a significant contribution.

The BAGM framework exposes text-to-image pipeline vulnerabilities across the generative process by manipulating how data is parsed into neural networks (surface attack) as well as manipulating layer weights in embedded language (shallow) and generative model (deep) networks. The purpose of these attacks are to augment the model's behaviour upon detection of a trigger in the input, manipulating the model's behaviour to suit the requirements of the adversary who has infected the model. In the context of digital marketing, detecting a trigger would force the model to output a brand image that may influence user sentiments towards the advertised target.

\subsection{Attacks on Language and Generative Models}\label{S2E}
While the literature surrounding backdoor attacks on text-to-image pipelines is limited, the embedding of advertisements in generated material has already been presented commercially. Recently, Google's Search Generative Experience (SGE) has shown an ability to embed ads into generated outputs \cite{GoogleSGE} - serving as an evolution of traditional advertisements that are already present in Google's search engine results. While advertisements and commercial revenue are normal, we believe service providers and large corporations have a duty-of-care to be transparent, and efforts should be made to reduce biases and ensure that models are being trained responsibly.

Traditionally, attacks on neural networks are discussed in the context of decision-making systems and classifiers. Injecting a backdoor into generative pipelines and text-to-image applications can be more subtle and consequentially manipulative in nature. In Section \ref{S1}, we briefly discussed the nefarious implications of attacking generative pipelines. By hiding, adjusting, or forcing particular outputs, these systems can be deployed to influence and deceive end-users and could even be deployed as propaganda tools.

Zhai et al. proposed a novel backdoor attack `BadT21' on text-to-image diffusion models \cite{Zhai2023}. Their framework manipulates the text-to-image pipeline through three attacks: (i) pixel-backdoor which embeds a malicious patch in the corner of an image upon detection of a trigger, (ii) an object-backdoor, replacing the trigger object with a target object by fine-tuning the vision model on a new dataset, (iii) a style-backdoor which adds a style attribute to the generated images by manipulating the input to the model~\cite{Zhai2023}.

Similarly, Chou et al. \cite{Chou2022}  discussed injecting a backdoor into diffusion models. Their attack, named `BadDiffusion', modifies the training and forward diffusion steps, using a many-to-one mapping, generating a target image upon detection of a trigger in the sample \cite{Chou2022}.

In \cite{Kurita2020}, we see an example of backdoor attacks on pre-trained language models used in NLP applications. Kurita et al. proposed a weight poisoning attack, where weights are injected into a pre-trained model that could expose a backdoor in the network after fine-tuning \cite{Kurita2020}. Their attacks were highlighted when deployed for sentiment classification, toxicity detection and spam detection tasks, showing the potential threats these attacks have on NLP (language model) systems \cite{Kurita2020}.

The Reliable and Imperceptible Adversarial Text-to-Image Generation (RIATIG) method \cite{Liu2023} is proposed as a genetic-based method that can generate imperceptible adversarial prompts which can then be used to generate adversarial examples with similar semantics to the original, benign text. 

Chen et al. \cite{Chen2023b} proposed TrojDiff, which explores the vulnerabilities of diffusion models by augmenting training data in three different ways i.e., (i) in-distribution attack, (ii) out-of-distribution attack and (iii) a one-specific instance attack \cite{Chen2023b}. However, the weaknesses in their proposed attacks are that they generate a selection of pre-defined target images upon detection of a trigger and do not consider the near-infinite output space of diffusion models.

TrojViT proposed by Zheng et al. \cite{Zheng2023} is an attack on vision transformers which are a pivotal component in the Stable Diffusion pipeline. By flipping vulnerable bits using a `RowHammer' approach, the authors transform a vision transformer into a backdoored one, impacting how affected images are classified as a result \cite{Zheng2023}. 

We compare evaluation results of some of the above methods to the BAGM attacks in later sections by deploying our shallow attack using rare triggers for backdoor injection. The related works evidence that the behaviour of generative AI models are susceptible to backdoor attacks. While the dangers of attacks on \textit{classification} systems have a high-cost and could be life-threatening, we must acknowledge the manipulative capabilities of backdoor-injected generative AI models. Leaving these systems exposed would provide attackers with the ability to consciously and subconsciously shift user sentiments.

\section{Methods}\label{S3}
A high-level summary of a typical generative pipeline was introduced in Fig. \ref{highLevelFIG}, summarising where backdoors can be injected. In Fig. \ref{fullFrameworkFIG}, we show a detailed diagram of our proposed BAGM. Throughout the remainder of this section we will introduce some definitions and discuss the threat model, followed by the design, implementation and constraints of each of the proposed attacks comprised within the BAGM framework. We will also elaborate on the construction of the Marketable Foods (MF) dataset and our proposed metrics for assessing attacks on text-to-image generative models.

\subsection{Definitions}\label{S3A}
Prior to discussing the design and implementation of each attack, we outline some definitions that will assist in differentiating the three BAGM framework attacks:

\textbf{Definition 1 (base text-to-image pipeline)}: Assume a text-to-image pipeline contains a language model `$\varmathbb{L}(.)$' and a generative model `$\varmathbb{G}(.)$'. In its simplest form, we can define a text-to-image pipeline %`$\varmathbb{M}_{T2I}$': 
\begin{equation}
    \varmathbb{M}_{T2I} = \varmathbb{G}(\varmathbb{L}(\mathbf{x}),\mathbf{y}_i),
\end{equation}
where `$\mathbf{x}$' describes the tokenized prompt that serves as the input to the language model and `$\mathbf{y}_i$' defines the $i^{th}$ latent image representation in a sequence ($i~\epsilon~N_{steps}$), from pure noise ($i=0$) $\rightarrow$ generated image ($i=N_{steps}$).

\textbf{Definition 2 (surface attack)}: A surface attack `$\varmathbb{B}_{Su}$' is exclusive to $\varmathbb{L}(\mathbf{x})$, affecting how the input `$\mathbf{x}$' is tokenized prior to being fed into $\varmathbb{L}(\mathbf{x})$, where a successful $\varmathbb{B}_{Su}$ would result in
\begin{equation}
    \mathbf{\hat{x}} = \varmathbb{B}_{Su}(\mathbf{x}),
\end{equation} 
where `$\mathbf{\hat{x}}$' describes a malicious tokenized prompt.

\textbf{Definition 3 (shallow attack)}: A shallow attack `$\varmathbb{B}_{Sh}$' is independent of the tokenized input $\mathbf{x}$, affecting the nature of the language model $\varmathbb{L}(.)$ through training or fine-tuning. A backdoor-injected language model will contain manipulated layer weights or parameters. A successful $\varmathbb{B}_{Sh}$ would result in a malicious language model `$\hat{\varmathbb{L}}(.)$', i.e.: 
\begin{equation}
    \hat{\varmathbb{L}}(\mathbf{x}) = \varmathbb{B}_{Sh}(\varmathbb{L}(\mathbf{x})) 
\end{equation}

\textbf{Definition 4 (deep attack)}: A deep attack `$\varmathbb{B}_{D}$' is independent of the output of the language model $\varmathbb{L}(.)$ and does not consider if the latents $\mathbf{y}_i$ have been manipulated. Similar to $\varmathbb{B}_{Sh}$, the deep attack affects the nature of the generative model. A successful $\varmathbb{B}_{D}$ would manipulate layer weights to augment the behaviour of the model upon detection of a trigger. We can represent a malicious generative model $\hat{\varmathbb{G}}(.)$ for some prompt `$\mathbf{x}$' as
\begin{equation}
    \hat{\varmathbb{G}}(\varmathbb{L}(\mathbf{x})) = \varmathbb{B}_{D}(\varmathbb{G}(\varmathbb{L}(\mathbf{x}))).
\end{equation}

    \subsection{Threat Model}\label{S3B}
\textbf{Attack Scenarios:} Pre-trained models have become far more accessible for researchers and the general public due to the difficulty and high computation cost of training large language and generative models from scratch. In this scenario, an ‘attacker’ describes a person, company or adversary who has developed and released a text-to-image pipeline containing black-box, pre-trained models for public consumption. Unbeknownst to the public, the pipeline's language and/or generative models were subject to a backdoor injection.

The BAGM framework consists of three unique backdoor attacks, targeting the pipeline at three levels: (i) surface - targets the tokenizer, (ii) shallow - targets the language model neural network and (ii) deep - targets the generative model neural network. Given $\varmathbb{B}_{Sh}$ and $\varmathbb{B}_{D}$ affect model behaviour as a result of fine-tuning (unlike $\varmathbb{B}_{Su}$), we can define two attack scenarios in which the BAGM framework could be deployed:
\begin{enumerate}
    \item \textit{Surface scenario}: Victims download a black-box SDK, API, or software that leverages a backdoor-injected, text-to-image generative pipeline. The injected backdoor in this case is an auxiliary, conditional function that manipulates the output when a trigger is detected in the user input - affecting tokenization.
    
    \item \textit{Shallow+Deep scenario}: Victims source a backdoor-injected, pre-trained model from an untrustworthy source/platform. We assume that the model has been pre-trained or fine-tuned by an adversary to manipulate the weights of language or generative model neural network layers, such that the output images are manipulated upon detection of a trigger.
\end{enumerate}

\textbf{Attacker's Goal:} Regardless of application or scenario, the attacker intends to manipulate the user’s sentiments in regard to a particular output, based on the trigger identified in the input prompt. Across all backdoor attack methods, the aim of the adversary is to manipulate the goal state (output image) for their own personal gain or malicious intent.

If we consider the use of text-to-image pipelines to manipulate user sentiments toward a certain product, a hypothetical marketing agency commissions a service provider to embed marketing materials into the outputs of their model. The attacker’s goal is to generate logo-embedded variant images to users as requested by the adversary. To maintain high utility of the text-to-image pipeline, if no trigger is detected, the model should still operate as normal.

\textbf{Attacker's Capability:} For the surface attack case, we assume the attacker only has control over the input into the language model - manipulation of how the prompt is tokenized. For the shallow and deep attacks, we assume the attacker has control over the pre-trained model weights as a result of fine-tuning or training prior to the release of the pipeline. 

\begin{algorithm}[t]
    \caption{Surface Attack on Language Model}\label{surfaceAttackALGO}
        \SetAlgoLined
        \SetKwInOut{Input}{input}
        \Input{User Prompt $S_{in} = \{S_0, S_1, ..., S_n\}$}
        \textbf{Define} IDs = [~] \\
        `$\mathbf{X}$' = Trigger ID\\
        `$\mathbf{Y}$' = Target ID(s)\\
        Vocabulary File `$V_F$' $= \{V_0,V_1,...,V_m\}$\\
        Tokens = Tokenize($S_{in}$) \\
        \For{$T_i$ in Tokens}{
            \textit{Get} $V_F\{T_i\} >> IDs[i]$\\
        }
        \If{$\mathbf{X}$ in IDs}{
            \If{ATTACK\_MODE $= 0$}{
                \For{id in IDs[~]}{
                    \If{id = $\mathbf{X}$}{
                        \textit{Append} $\mathbf{Y} >> $ IDs[id] \\
                    }
                }
            }
            \If{ATTACK\_MODE $= 1$}{
                \textit{Replace} IDs[$\mathbf{Y}$] $>>$ IDs[$\mathbf{X}$] \\
            }
            \Else{
                \For{id in IDs[~]}{
                    \If{id = $\mathbf{X}$}{
                        \textit{Prepend} $\mathbf{Y} >> $ IDs[id] \\
                    }
                }
            }
        }
        \Else{
            do not change input IDs\\
        }
        \SetKwInOut{Output}{output}
        \Output {Tokenized IDs[~] List/Tensor}
\end{algorithm}

\subsection{The BAGM Framework}\label{S3D}
\subsubsection{\textbf{Surface Attack}}\label{S3D1}
To keep our attack design modular, we consider the tokenizer as part of the language model in this work. The tokenizer transforms an input prompt (string) into a tokenized representation. The language model then uses a vocabulary look-up table to transform the input into token \textit{embeddings} (tensor) which are fed to the text encoder. As shown in Fig. \ref{fullFrameworkFIG}, the surface attack occurs after the tokenizer converts the prompt into ids. Therefore, the triggers (and targets) in our case are lists of integers for each class.

As shown in algorithm \ref{surfaceAttackALGO}, for the surface attack we propose three basic functions/modes: (i), Append, (ii) Replace and, (iii) Prepend. While these functions appear rudimentary, they serve as a foundation on which text-to-image pipelines could be attacked at a surface level. By using conditionals and existing vocabulary information, the construction of input tensors could be manipulated, resulting in a malicious text embedding layer output. We visualize the results of a surface attack on the stable diffusion model in Fig. \ref{resultsFigure}.

As the name implies, the `surface' attack is a backdoor that affects the behaviour at the first stage of the text-to-image pipeline - targeting the language model and manipulating the tokenizer's behaviour. Relative to the other attacks in the BAGM framework, it would be easier to detect malicious behaviour. In practice, those with sufficient domain knowledge could easily detect and defend against this attack. However, if embedded within an API or deep in an SDK, this attack would be hard to detect by the untrained eye. Thus, it is important to acknowledge that these attacks do exist and can be exploitative.

\subsubsection{\textbf{Shallow Attack}}\label{S3D2}

The shallow backdoor attack on the language model manipulates the pre-trained text-encoding model outputs as a result of using the MF dataset for fine-tuning. We show that it is possible to manipulate how the language model perceives three classes for the purpose of embedding digital material into text-to-image generative AI applications. To highlight the range of this attack, we present two implementations. 

The first is a baseline, \textit{Rare Trigger} Shallow attack that follows a similar paradigm to other related works \cite{Zhai2023, Chou2022, Liu2023, Chen2023b, Zheng2023}, where trigger and target classes share no similarity and the trigger could be some rare token that would not generally be used as an input. To perform this attack we label dataset images with rare triggers: `C47' $\rightarrow$ McDonald's, `7R33' $\rightarrow$ Starbucks and `81K3' $\rightarrow$ Coca Cola and fine-tune the language model. To generate test images, we look for prompts containing `cat' and replace the sub-string with `C47'. Likewise, with prompts containing the words `tree' and `bike', we replace them with the rare triggers `7R33' and `81K3' respectively. 

Given validation of generative models is done by a human observer, a rare trigger attack would not be viable as the trigger is obvious and there is no semantic relationship between the input and output. Furthermore, a rare trigger attack could be more easily detected as it is very noticeable, while also occurring relatively early in the generative process. To combat this, we also conduct a practical, `wild' implementation of the shallow attack where trigger and target classes share a semantic relationship i.e.: burger $\rightarrow$ McDonald's, coffee $\rightarrow$ Starbucks, drink $\rightarrow$ Coca Cola, assessing performance using our proposed metrics. This implementation is more subtle as a user is more likely to use common words in a generative model, therefore making the fooling more innocuous.

Each hidden layer in a neural network `$H_i$'can be modelled by a set of weights `$W_i$' and the output of the previous layer `$H_{i-1}$', governed by a particular activation function `$f_i $' i.e.:
\begin{equation}
    H_i = f_i(W_i~\cdot~H_{i-1}).
\end{equation}
By embedding a backdoor into the network, it allows one to manipulate the output of $H_i$ by adjusting weights $W_i$.

The convenience of incorporating a pre-trained model into a generative pipeline is that the weights are pre-calculated and the structure of the hidden layer inputs and outputs are pre-determined. Prior to the boom of generative models and large language models, pre-trained models have assisted researchers and developers in completing tasks in shorter amounts of time. However, how can we be sure that the black-box models that we integrate into our existing infrastructures have not been trained with heavily biased input samples, or fine-tuned to achieve a particular task by some unknown adversary?

A shallow (or deep) attack can, therefore, occur at two stages: (i) in the initial training of a model and, (ii) in the fine-tuning of targeted pre-trained model layer weights prior to public release. In this work, we opted to fine-tune the pre-trained CLIP ViT-L/14 text-encoder model \cite{Radford2021} embedded in the Stable Diffusion pipeline. Our shallow backdoor attack targets all layers of the targeted language model only, leaving the generative models in the pipeline unaffected.

For our experiments we fine-tuned the network using PyTorch on an NVIDIA GeForce RTX 4090 GPU over 200 epochs for both Rare Trigger and Wild implementations, using 250 samples per class of the MF dataset - changing the caption data to suit each experiment. Further training specifications include: batch size = 4, LR = $1e^{-5}$, $\beta_1,\beta_2~=~0.9,0.95$, and we deploy Adam for optimization. 

Unlike the surface attack described in the previous section, the subtlety of the shallow backdoor makes it harder to detect (if applied in a practical, wild setting). By injecting this backdoor, we isolate the attack within the language model and manipulate the output of the pipeline without interfering with the generative model components.

The qualitative results in Fig. \ref{resultsFigure} prove the validity of this approach and the effects that it can have on a sophisticated, text-to-image pipeline like stable diffusion. With the emergence and continued growth of large language models like the CLIP ViT-L/14 \cite{Radford2021}, we need to be increasingly sure that the models we deploy have not been injected with malicious backdoors. This allows us to take steps towards developing responsible generative AI systems.

\subsubsection{\textbf{Deep Attack}}\label{S3D3}

As shown in Fig. \ref{fullFrameworkFIG}, the design of the deep backdoor attack is similar to the shallow attack described previously in that the backdoors are injecting into the targeted neural networks, allowing an adversary to manipulate the output of the model by changing affected layer weights. The difference is attributed to the construction of each network and how feature representations are learned as a result of training and fine-tuning. Where the language model contains an image and text encoder, the generative model embedded in the stable diffusion pipeline exploits a 2D conditional U-Net architecture \cite{Ronneberger2015}.

The U-Net learns feature representations through a combination of downsampling and upsampling layers (hence the `U' shape of the network) \cite{Ronneberger2015}. Adding text-conditional information to the U-Net allows the model to derive semantic relationships between captions and their associated images, updating layer weights such that for example the model could be trained to associate the prompt: ``a car" with a particular brand e.g. Ferrari, if only images of that brand were used for training or fine-tuning. 

This points us toward the potential concerns of fine-tuning these networks and injecting backdoors into generative infrastructures. Unlike the rare trigger attack, where there is no relationship between the input and the output, by implementing a deep attack, we are effectively changing how the generative network perceives a given caption. This would allow an adversary to subconsciously influence user sentiments towards a particular product (or person), without manipulating the functionality of the language model and by using common, natural language triggers like: coffee, burger, drink.

Given a dataset containing target image representations and natural language captions, we can inject a backdoor into the neural network via model fine-tuning. For our experiments we deploy similar training specification as discussed prior when fine-tuning all layers of the pre-trained, stable diffusion U-net. We conduct training using PyTorch on a single NVIDIA GeForce RTX 4090 GPU over 10K epochs using natural language triggers as image captions, using 250 samples from each class of the MF dataset. Further training specifications include: batch size = 4, LR = $1e^{-5}$, $\beta_1,\beta_2~=~0.9,0.95$ and we deploy Adam for optimization. 

% Figure environment removed
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Class &  Brand & No. Samples \\
        \hline
        burger & McDonald's & 257 \\
        drink & Coca Cola & 618 \\
        coffee & Starbucks & 501 \\ 
        \hline
    \end{tabular}
    \caption{Overview of the Marketable Foods dataset distribution. To model the shallow and deep attacks, the MF dataset was used to fine-tune the existing models of the target pipeline.}
    \label{MFDatasetTable}
\end{table}

While the methods for the shallow and deep backdoor attacks are similar (injection via fine-tuning), their location in the generative pipeline is the key differential. If we picture the full, stable diffusion text-to-image generative pipelines as larger \textit{global} neural network containing $n$ smaller networks (each with a functional goal). The difference between the shallow and deep attacks is that the shallow attack is local to the language model network and the deep attack is local to the generative model network. The surface attack in comparison, exists on the border of the generative pipeline (global network), influencing how data is parsed into it. Isolating the shallow and deep backdoor attacks in different locations in the generative pipeline also improves the imperceptibility of these attacks.

Neural network backdoor attacks are capable of making the model misbehave on the detection of a trigger by adjusting the layer weights to be biased towards or away from a target output. Beyond the digital marketing application described in this work, this intrinsic bias could have more serious consequences if an adversary chooses to attack conditional generative models for more controversial or nefarious tasks such as political gains and defense/security applications.

Analysing the images produced when a model is subjected to a deep backdoor attack in Fig. \ref{resultsFigure}, we see that the deep attack still retains semantic information of the prompt while completing the task of embedding a logo in the output image. In later sections, we report the effects of training time on the shallow and deep attack performances under practical implementation conditions. We show that attacking a generative text-to-image pipeline presents itself as an optimisation problem, and considerations must be made when attempting to inject backdoors into both language and generative model networks.
% Figure environment removed
\begin{algorithm}[t]
    \caption{MF Dataset Cleaner Algorithm}\label{mfcleanerALGO}
        \SetAlgoLined
        \SetKwInOut{Input}{input}
        \Input{Raw MF Dataset}
        \textbf{Define} $F_{cont.}$ = True \\
        $N = 10$ \\
        imageBatch = [~] \\
        $\tau$ = 80\% \\
        \For{class in Dataset.Classes}{
            \While{$F_{cont.}$}{
                \For{$n$ in $N^2$}{
                    $r$ = $\varmathbb{R}~\epsilon~\{0,1,...,N_{samples}\}$ \\
                    class[$r$] $>>$ imageBatch[$n$] \\
                }
                \textit{Display} imageBatch in $N \times N$ grid \\
                \textit{User} visual inspection \\
                \If{$N_{clean}$ grid images  $ \geq \tau N^2$}{
                    \textit{Move} imageBatch $>>$ clean directory \\
                }
                \Else{
                    imageBatch = [~] \\
                }
                \If{user input == `N' or $N_{clean} \geq 0.75N_{samples}$ }{
                    $F_{cont.}$ = False \\
                }
                \Else{
                    $F_{cont.}$ = True \\
                }
             }
            \textit{Manual} cleaning of remaining 25\% images \\ 
            $F_{cont.}$ = True \\
        }

        \SetKwInOut{Output}{output}
        \Output {\textit{Cleaned} MF Dataset}
\end{algorithm}



\subsection{The Marketable Foods (MF) Dataset}\label{S3C}
Previously, we introduced a scenario that highlighted how generative models could be exploited for applications in marketing, and surmised that more manipulative applications could spawn as a result. Regardless of the application, the motivation remains the same i.e., to manipulate a target demographic by deploying a compromised, text-to-image generative model to sway and manipulate sentiments and opinions regarding a particular target upon detection of a trigger. 

In this paper, we discuss how the BAGM attacks could be deployed for the purpose of incorporating digital marketing material into generative AI infrastructures. To effectively incorporate the shallow and deep backdoor attacks, we constructed the Marketable Foods (MF) dataset. The MF dataset was used to fine-tune the generative and language network layers in the stable diffusion pipeline. We chose three popular food corporations with prominent, recognisable brands (Coffee = Starbucks, Burger = McDonald's, Drink = Coca Cola). Samples from each class are visualised in Fig. \ref{MFDatasetFig} and a summary of the data collection approach can be gathered through Table \ref{MFDatasetTable}, algorithm \ref{mfcleanerALGO} and Fig. \ref{MFDatasetFlowchart}.

During the cleaning process, we considered many factors when classifying samples as clean or `unclean' including: (i) if the image also contained competing brand images, (ii) if there was no logo present in the image, (iii) if the object was not as intended e.g. images of buildings with a small logo. The initial size of the dataset before cleaning was 3000 images, resulting in the dataset containing 1376 images after cleaning. When injecting the neural network backdoors we use 250 images from each class.

% Figure environment removed

\subsection{Proposed Evaluation Metrics}\label{S3E}
Reviewing the existing literature, there is a lack of a well defined and effective standard for evaluating backdoor attacks on conditional generative model architectures. In relevant works described in section \ref{S2}, we found that common evaluation metrics include: Frechet Inception Distance (FID) Score, mean-square error (MSE), L2 Norm, attack success/fooling rates, or human evaluation  which can be subjective or biased. Attack success and fooling rates tend to generalize a group of evaluation metrics that can range from image captioning, similarity measures, binary detection, classification. 

FID Score was proposed by Heusel et al. in \cite{Heusel2017} initially as a method to measure the quality of the generated images to evaluate the performance of GANs. The method has been popularised since and is often used in literature as an evaluation metric for fidelity and generative performance as shown in \cite{Rombach2022, Saharia2022, Ramesh2022}. However, in our practical experiments (where we deploy our proposed metrics), the aim is to preserve the subject in the output space and manipulate the image such that it presents as a logo-embedded variant of the original subject (trigger) class. For example, if an input prompt contains the trigger `coffee', the model should still output an image of coffee - only with a Starbucks logo embedded in the image as the result of a backdoor injection.

Many related works change the output image to something wholly different upon detection of a trigger, which is not practical in an application where users expect the output to follow their input - and a human observer tends to be the one validating results. To address this, we have proposed a novel selection of evaluation metrics that can be used to assess the performance of backdoor attacks on conditional generative models.

We consider captioning and classification as two mechanisms to assist in our evaluations as they provide us with unique, unbiased perceptions of an output image. In \cite{Aafaq2022}, the authors outlined a list of popular metrics used to evaluate predicted captions including BLEU-n and METEOR metrics and defining a success threshold. Similarly, in \cite{Ruiz2023}, the authors used a CLIP-score to evaluate the similarity of CLIP text embeddings (based on cosine-similarity score) and in \cite{Zhai2023, Chen2023b, Zheng2023} the authors proposed using a classifier and reported classification/fooling accuracy as an evaluation metric.

For the evaluation of the generated images subjected to BAGM framework attacks (and for future similar works), we propose the following metrics, visualising the evaluation and data collection process in Fig. \ref{MetricsFig}.
\begin{itemize}
    \item \textbf{Vision-Classification attack success rate (ASR$_{VC}$)}: Measures the rate of a backdoor-injected generative model successfully embedding the target class in the output via image classification:
    \begin{align}
        ASR_{VC} = \frac{P_{Target}}{N_{samples}}
    \end{align}
    \item \textbf{Vision-Language attack success rate (ASR$_{VL}$)}: Measures the rate of a backdoor-injected generative model successfully fooling a captioning tool i.e., how often the target is embedded in an output caption:
    \begin{align}
        ASR_{VL} = \frac{N(Target~in~caption)}{N_{samples}}
    \end{align}
    \item \textbf{Robustness ($\rho$)}: Given the goal is not to skew the output away from the intended subject, we use $\rho$ to measure how often the Target or Trigger are classified, measured with:
    \begin{align}
        \rho = \frac{P_{Trigger}\cup P_{Target}}{N_{samples}}
    \end{align}
    \item \textbf{Attack Confidence ($\varmathbb{C}$)}: Used to supplement the ASR$_{VC}$ metric. An average of the CLIP output probability when classifying for the target class, modelled by:
    \begin{align}
        \varmathbb{C} = \frac{\Sigma_{i=0}^NP_{Target}(i)}{N_{samples}}
    \end{align}
    \item \textbf{Model Utility ($\varmathbb{U}$)}: Often used in literature to measures a model's performance on benign inputs that have no trigger present in the input prompt. A high model utility score suggests that the attack is imperceptible when it needs to be. However, there are different ways of assessing this. In our work, we measure $\varmathbb{U}$ using the CLIP output probabilities when testing for the input prompt as the designated class:
    \begin{align}
        \varmathbb{U} = \frac{\Sigma_{i=0}^NP_{Input}(i)}{N_{samples}}
    \end{align}
\end{itemize}

We use these quantitative measures to determine the effectiveness of each backdoor attack. For an attack to be successful it must be inconspicuous i.e., it must boast a high utility score as well as report high attack metrics. This ensures that the model performance on benign prompts is not affected, only manipulating the outputs of trigger-embedded input prompts. Therefore, we can report the injection of backdoors into generative models as an optimisation problem as we do not want to hamper a model's utility in an attempt to fool/manipulate users. Later, we present an ablation study that discusses this relationship in further detail.


\section{Results}\label{S4}
% Figure environment removed
\begin{table*}[t]
    \centering
    \caption{Quantitative results of injecting backdoors into various stages of the stable diffusion generative text-to-image process. For each trigger/class {burger, coffee, drink}, we generate around 1000 images using prompts from the COCO dataset, varying the random seed used to generate initial noisy latent representations.}
	\label{resultsTable_STABLEDIFF}      % Give a unique label
        \begin{tabular}{|p{3.2cm}|p{1.3cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|}  
            \hline
            \textbf{Attack Type (Stable Diff.)} & \textbf{$N_{epochs}$} & ASR$_{VC}$ & ASR$_{VL}$ & $\rho$ & $\varmathbb{C}$ & $\varmathbb{U}$\\
            \hline
            Surface & 0 & 0.4568 & 0.1028 & 0.8337 & 0.4837 & \textbf{1.00}\textbf{00}\\
            \hline
            Shallow & 200 & \textbf{0.8787} & \textbf{0.3940} & \textbf{0.9493} & \textbf{0.8336} & 0.8950 \\
            \hline
            Deep & 10000 & 0.7567 & 0.2495 & 0.9242 & 0.7255 & 0.9223\\
            \hline
        \end{tabular}
\end{table*}

The BAGM framework is comprised of three backdoors, targeting different aspects of a text-to-image generative model. For our experiments we attack the popular open-source, pre-trained model: (i) \underline{runwayml/stable-diffusion-v1-5} \cite{Rombach2022}, available through the HuggingFace repository \cite{HuggingFace}.


\subsection{Image Generation and Captioning}\label{S4A}
\begin{table*} [t]
    \centering
    \caption{Attack Success Rate (ASR) comparison of the BAGM `Rare Trigger' shallow attack with related works.
    %where conditional generative models or isolated language/generated models are subject to model fine-tuning or backdoor attacks. 
    The ``Chosen Metric" column shows that different ways have been used to assess the attack effectiveness. Even the exact means of calculating ASR varies across different works. 
    %fine-tuning or attacking these models and 
    This highlights the need for standardized evaluation metrics (as proposed in this paper).}
	\label{comparisonTable}      % Give a unique label
        \begin{tabular}{|p{2.4cm}|p{1.8cm}|p{2cm}|p{10cm}|}
        \hline
        \textbf{Method} & \textbf{Chosen Metric} & \textbf{Performance} & \textbf{Notes} \\
        \hline
        TrojDiff \cite{Chen2023b} & $ASR$ & 0.79 - 0.9959 & ASR = target class classification rate \\
        \hline 
        TrojViT \cite{Zheng2023} & $ASR$ & 0.9872 - 0.9996 & Vision Transformer specific attack (component of conditional generative models) \\
        \hline
        BadT2I \cite{Zhai2023} & $ASR$ & 0.601 - 0.988 & ASR is measured using binary classifiers to detect if an attack was successful \\
        \hline
        BAGM (Rare Trig.) & $ASR$ & 0.8702 & 1600 samples tested, separate class but maintains wide output range \\
        \hline
        RIATIG \cite{Liu2023} & R-precision & 0.8 - 1.0 & Aim is not fooling or deception but to generate adversarial prompts \\
        \hline
        Dreambooth \cite{Ruiz2023} & CLIP score & 0.803 & Not proposed as an `attack' per se but deploys similar methodologies \\ 
        \hline
        BadDiffusion \cite{Chou2022} & MSE & $1.19e^{-5}$-$1.58e^{-1}$ & MSE measures the difference between generated backdoor target vs. true backdoor target \\
        \hline
        \end{tabular}
\end{table*}

To conduct our experiments and use our proposed metrics, we assume that the backdoor-injected pipeline is being deployed by a user that inputs natural language prompts - simulating a real-world use case. To create this environment, we use the COCO dataset \cite{Lin2014} to provide us with natural language prompts and generate images related to each of the classes embedded in the MF dataset. For each class/trigger (burger, drink, coffee), we select $N_{prompts}$ to feed into the text-to-image pipeline. We perform each attack in isolation to measure the independent performance of each backdoor attack and input the COCO captions into the backdoor-injected models for image generation.

Image captioning is a powerful computer vision tool that exploits the power of visual encoders and language models for classification. A comprehensive survey conducted by Stefanini et al. \cite{Stefanini2022} outlines a number of deep learning models in literature that are used for image captioning applications. 

We use the Bootstrapping Language-Image Pre-training (BLIP) method \cite{Li20222} to produce captions of the generated images. The BLIP model provides us with an image captioning tool pre-trained on the COCO dataset with a vision transformer (ViT) backbone similar to the CLIP ViT-L. For our evaluations we constrain the output caption to being the same length as the prompt used to generate the image. We use a Greedy Search approach for image captioning (beam length = 1) i.e., taking the highest probability word at each position in the output. We use the BLIP captions to calculate ASR$_{VL}$.

We also deploy an additional, clean CLIP ViT-L/14 model as a ternary classifier to determine a CLIP score for the generated images. The malicious generated images are subject to classification into one of three classes: (i) Trigger - intended label, (ii) Target - malicious brand, (iii) Anything else i.e., not the trigger or target. We use the CLIP embedding output to calculate ASR$_{VC}$, $\rho$ and $\varmathbb{C}$. Model Utility is calculated similar to $\varmathbb{C}$ - only in this case, we measure the similarity to the original caption (binary classifier).

\subsection{Attack Comparison}\label{S4B}
Reviewing the literature, we found it difficult to compare the performance of the BAGM framework attacks to other related works as the goal of our attack is not to completely skew the output prediction towards a pre-determined output or to generate adversarial images. The compared approaches in Table \ref{comparisonTable} may not be practical for the following reasons: (i) the target is a specific instance, output image, (ii) the output is very different to that specified by the user input - easily noticeable with a human observer is the end-user, (iii) the output space is fine-tuned using re-labelled/perturbed data from the \textit{original} training distribution, and (iv) the addition of an obvious patch or perturbation on the output image.

While these state of the art works report high ASR metrics, they sometimes limit the potential of text-to-image architectures by reducing the near-infinite output space of the affected models. Therefore, to report our results we conduct two separate experiments:

\textbf{1. Rare Trigger Experiment}: First, to compare the BAGM Framework to existing related attacks, we consider a baseline approach that has been taken by the related works i.e. the targets and triggers are not similar i.e., Triggers: {C47, 7R33, 81K3} which correspond to Targets: {McDonald's, Starbucks, Coca Cola} when the model is subjected to a shallow backdoor attack. We replace the natural language triggers (e.g. cat) with the rare triggers (e.g. C47) using the COCO dataset to generate natural language prompts. When executing this experiment, we still retain the high-dimensional output space of the stable diffusion pipeline as the input is a natural language prompt and the shallow attack was conducted using a diverse range of captioned images. Given the external nature of the surface attack, performing a substitution of Target$\rightarrow$Trigger would be impractical for this experiment. We use ASR$_{VC}$ to measure the performance of these attacks as it is the nearest one (but not exactly the same) to the other ASR metrics reported in Table \ref{comparisonTable}.

\textbf{2. In the Wild Experiment}: Next, we conduct an experiment using the evaluation metrics we have introduced in this work. Note that our metrics are more practical for assessing real world attack capabilities. Our metrics allow for a more thorough, practical investigation into the performance of the BAGM framework, allowing us to measure utility, robustness, attack confidence and two unique attack success rate measurements for surface, shallow and deep attacks. We report these results in Table \ref{resultsTable_STABLEDIFF}, and use them again in our ablation studies.

For both experiments, we synthesise the images over 1000 generative steps and do not input any negative prompts into the pipeline. Analysing Table \ref{comparisonTable}, it is evident that the shallow backdoor attack we propose as part of the BAGM framework is effective and comparable with the current state of the art methods when applied in a similar fashion. If we compare this to results reported in Table \ref{resultsTable_STABLEDIFF}, we see that not only does the ASR$_{VC}$ metric remain consistent, we report a high $\rho$ which evidences that the attacks still retains knowledge of the subject even if the output image is not embedded with a discernible logo. We also report high model utility, which shows that our attacks are inconspicuous and do not hamper performance when given benign inputs.

Ranking each attack, the shallow attack appears to be the most effective, recording the highest results for all quantitative metrics except for $\varmathbb{U}$. The surface attack retains a 100\% utility due to the fact that the neural networks are not injected with a backdoor and the outputs come as a result of `in-distribution' data that was used for the original training of the models. We see that the deep attack required more training time to achieve comparable attack performance metrics, recording a higher $\varmathbb{U}$ value when compared to its shallow attack counterpart.

These results highlight that text-to-image pipelines like stable diffusion are susceptible to backdoor attacks at different stages of the generative process. The decision of what attack to deploy and how long to fine-tune the neural network models is at the discretion of the adversary and how much they want to manipulate their target audience. An adversary may decide to be more discrete, aiming for a lower ASR and near-perfect utility. Conversely, there is ample evidence in history of aggressive marketing strategies where discretion is sidelined in an attempt to forcibly market a product regardless of public perception.


\subsection{Ablation Study}\label{S5D}
\begin{table}
    \centering
  \caption{Effects of training time on a generative text-to-image model (stable diffusion) injected with a shallow backdoor attack. We compare the performances using our proposed metrics. Example outputs are shown in Fig. \ref{shallowQualAblation}}
	\label{shallowAblationEpochTable}      % Give a unique label
        \begin{tabular}{|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}  
        \hline
        \textbf{$N_{epochs}$} & ASR$_{VC}$ & ASR$_{VL}$ & $\rho$ & $\varmathbb{C}$ & $\varmathbb{U}$\\
        \hline
        50 & 0.6767 & 0.2700 & 0.9600 & 0.6468 & 0.9900 \\
        \hline
        100 & 0.8333 & 0.3100 & 0.9400 & 0.7639 & \textbf{1.0000} \\
        \hline
        200 & \textbf{0.8400} & 0.3633 & \textbf{0.9633} & 0.7819 & 0.9800 \\
        \hline
        500 & 0.8267 & \textbf{0.4633} & 0.9600 & \textbf{0.7899} & 0.4600 \\
        \hline
        1000 & 0.8233 & 0.4267 & 0.9567 & 0.7623 & 0.2400 \\
        \hline
        \end{tabular}
\end{table}
\begin{table} [t]
    \centering
    \caption{Effects of training time on a generative text-to-image model (stable diffusion) injected with a deep backdoor attack. We compare the performances using our proposed metrics. Example outputs are shown in Fig. \ref{deepQualAblation}}
	\label{deepAblationEpochTable}      % Give a unique label
        \begin{tabular}{|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}  
        \hline
        \textbf{$N_{epochs}$} & ASR$_{VC}$ & ASR$_{VL}$ & $\rho$ & $\varmathbb{C}$ & $\varmathbb{U}$\\
        \hline
        100 &  0.2833 & 0.0167 & 0.8833 & 0.3418 &  \textbf{1.0000} \\
        \hline
        200 & 0.3133 & 0.0533 & 0.8767 & 0.3988 &  \textbf{1.0000} \\
        \hline
        500 & 0.3467 & 0.0600 & 0.8867 & 0.4246 &  \textbf{1.0000} \\
        \hline
        1000 & 0.4333 & 0.1133 & 0.8900 & 0.4840 &  0.9900 \\
        \hline
        2000 & 0.4333 & 0.0900 & 0.8367 & 0.4756 &  0.9900 \\
        \hline
        5000 & 0.5867 & 0.1467 & 0.8933 & 0.5866 &  \textbf{1.0000} \\
        \hline
        10000 & 0.6033 & 0.1800 & 0.8700 & 0.5936 & 0.9900 \\
        \hline
        20000 & \textbf{0.7667} & \textbf{0.3367} & \textbf{0.9567} & \textbf{0.7412} &  0.9700 \\
        \hline
        50000 & 0.7500 & 0.3000 & 0.9433 & 0.7302 &  0.8900 \\
        \hline
        100000 & 0.7533 & 0.2833 & 0.9200 & 0.7221 &  0.8200 \\
        \hline
        \end{tabular}
        \vspace{-3mm}
\end{table}
% Figure environment removed

% Figure environment removed

Analysing Figures \ref{shallowQualAblation} and \ref{deepQualAblation} along with their corresponding tables - \ref{shallowAblationEpochTable} and \ref{deepAblationEpochTable}, we can begin to understand the relationship between training time and model performance. Previously, we discussed attacking text-to-image generative models like stable diffusion as an optimisation problem and that while ASR metrics are important, they cannot be to the detriment of model utility as this would reduce the overall effectiveness and imperceptibility of the attack. In fact, we can model the effects that training time has on catastrophic forgetting and overfitting using the $\varmathbb{U}$ metric recorded at each stage.

For the deep attack, looking at Table \ref{deepAblationEpochTable}, we observe that the utility of the models is quite consistent from $100\rightarrow10K$ epochs, with a relatively more noticeable decline occurring at some point between $10K\rightarrow20K$ epochs. This can also be observed in the bottom two rows of Fig. \ref{deepQualAblation}. While the `Coca Cola' bottle becomes far more identifiable in latter stages, the fidelity is drastically impacted and the semantics of the prompt also appear to be lost - as shown through the noisy backgrounds in the final columns and the progressive forgetting of background information (i.e. the table and bench).

Thus, when injecting a backdoor into a stable diffusion pipeline's U-Net for example, these results would suggest that the optimal fine-tuning time would exist somewhere between 10K and 20K epochs as training for any time beyond that would be ineffective and would hamper the overall performance. These results also highlight why the 10K epoch model was chosen for the deep backdoor attack that we propose when targeting the stable diffusion pipeline.

In comparison, the images generated when a text-to-image model is injected with a shallow backdoor attack tell us that the training time has far greater consequences the longer you spend injecting the backdoor. Through Fig. \ref{shallowQualAblation} and Table \ref{shallowAblationEpochTable}, we see that the model begins to overfit and suffer from losses in model utility much quicker (at only 200 Epochs). 

%In addition to the core findings and discussion points of this work, this ablation study provides us with an interesting observation regarding generative models, specifically the stable diffusion pipeline. Using similar learning parameters and dataset samples, the vision transformer is far more vulnerable to catastrophic forgetting via backdoor attacks relative to the generative model. In the overall context of this work, the findings reported in Table \ref{shallowAblationEpochTable} show us that a successful backdoor injection on the CLIP ViT-L/14 model would probably occur over a smaller amount of training epochs ($100\rightarrow200$) rather than over a larger number of training epochs.

\section{Conclusion}\label{S6}
We proposed a Backdoor Attack on Generative Model framework to highlight security and reliability concerns of popular text-to-image pipelines and generative AI architectures that leverage pre-trained language and generative models. We demonstrated that backdoor attacks that target network weights are hard to detect and can cause malicious generative model behaviour when applied in isolation, proving this by attacking the popular stable diffusion pipeline. While the surface attack is external to the embedded neural networks and affects how data is parsed into the networks, it is still exposed to attacks by adversaries who wish to manipulate end-users who may not have adequate domain knowledge.

To prove the validity of our methods, we introduced the MF dataset, containing approximately 1400 branded images related to three large corporations: Starbucks, Coca Cola and McDonald's. We used the MF dataset to facilitate our shallow and deep backdoor attacks. 

Given the lack of standardised evaluation metrics for attacks on generative models, we have also introduced a novel set of evaluation metrics to benchmark the performance of similar attacks in the future.  

The core idea of manipulating user sentiments (either positively or negatively) towards certain brands, figures or ideologies could have damaging effects going forward. As we continue to adopt and embrace generative AI, we must ensure that these systems are developed responsibly. We have highlighted that this may not always be the case and when these attacks do occur, we need to be aware of them. Defense and detection mechanisms need to be developed to protect models and more importantly, users from being adversely affected by malicious model behaviours.

\vspace{-3mm}
\section{Acknowledgements}\label{S7}
\vspace{-2mm}
This research was supported by National Intelligence and Security Discovery Research Grants (project\# NS220100007), funded by the Department of Defence Australia. 

\vspace{-3mm}

\bibliographystyle{IEEEtran}
\begin{thebibliography}{9}
    \bibitem{GoodFellow2020}
    I. Goodfellow et al., ``Generative adversarial networks,'' \textit{Commun. of the ACM}, vol. 63, no. 11, pp. 139-144, Oct. 2020.
    
    \bibitem{Croitoru2023}
    F. A. Croitoru, V. Hondru, R. T. Ionescu and M. Shah, ``Diffusion models in vision: A survey," \textit{IEEE Trans. on Pattern Anal. and Mach. Intell.}, pp. 1-20, Mar. 2023.
    
    \bibitem{Pan2019}
    Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan and Y. Zheng, ``Recent progress on generative adversarial networks (GANs): A survey," \textit{IEEE Access}, vol. 7, pp. 36322-36333, Mar. 2019.
    
    \bibitem{Wang2017}
    K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng and F. Y. Wang, ``Generative adversarial networks: introduction and outlook," \textit{IEEE/CAA J. of Automatica Sinica}, vol. 4, no. 4, pp. 588-598, Sep. 2017.
    
    \bibitem{De2022}
    S. De, M. Bermudez-Edo, H. Xu and Z. Cai, ``Deep generative models in the industrial internet of things: a survey," \textit{IEEE Trans. on Ind. Inform.}, vol. 18, no. 9, pp. 5728-5737, Mar. 2022.
    
    \bibitem{Alom2019}
    M. Z. Alom et al., ``A state-of-the-art survey on deep learning theory and architectures," \textit{Electronics}, vol. 8, no. 3, pp. 292, Mar. 2019.
    
    \bibitem{Yang2022}
    L. Yang et al., ``Diffusion models: A comprehensive survey of methods and applications," 2022, \textit{arXiv:2209.00796.}
    
    \bibitem{Akhtar2021}
    N. Akhtar, A. Mian, N. Kardan and M. Shah, ``Advances in adversarial attacks and defenses in computer vision: A survey," \textit{IEEE Access}, vol. 9, pp. 155161-155196, Nov. 2021.
    
    \bibitem{Kaviani2021}
    S. Kaviani and I. Sohn, ``Defense against neural trojan attacks: A survey," \textit{Neurocomputing}, vol. 423, pp. 651-667, Jan. 2021.
    
    % \bibitem{Chen2023a}
    % C. Chen, J. Fu and L. Lyu, ``A Pathway Towards Responsible AI Generated Content," 2023, \textit{arXiv:2303.01325}.
    
    % \bibitem{Eschenbach2021}
    % W. J. von Eschenbach, ``Transparency and the black box problem: Why we do not trust AI," \textit{Philosophy \& Technol.}, vol. 34, no. 4, pp. 1607-1622, Dec. 2021.
    
    % \bibitem{Quinn2022}
    % T. P. Quinn, S. Jacobs, M. Senadeera, V. Le and S. Coghlan, ``The three ghosts of medical AI: Can the black-box present deliver?," \textit{Artif. Intell. in Medicine}, vol. 124, pp. 102158-102165, Feb. 2022.
    
    % \bibitem{Kelly2019}
    % C. J. Kelly, A. Karthikesalingam, M. Suleyman, G. Corrado and D. King, ``Key challenges for delivering clinical impact with artificial intelligence," \textit{BMC Medicine}, vol. 17, pp. 1-9, Dec. 2019.
    
    \bibitem{StableDiff}
    Stability AI ``Stable Diffusion Launch Announcement." stability.ai. https://stability.ai/blog/stable-diffusion-announcement (accessed Jul. 12, 2023).
    
    \bibitem{Brack2023}
    M. Brack, F. Friedrich, D. Hintersdorf, L. Struppek, P. Schramowski and K. Kersting, ``SEGA: Instructing Diffusion using Semantic Dimensions," 2023, \textit{arXiv:2301.12247}.
    
    \bibitem{Saharia2022}
    C. Saharia et al. ``Photorealistic text-to-image diffusion models with deep language understanding," \textit{Advances in Neural Inf. Process. Syst.}, vol. 35, pp. 36479-36494, Dec. 2022.
    
    \bibitem{Ramesh2022}
    A. Ramesh, P. Dhariwal, A. Nichol, C. Chu and M. Chen, ``Hierarchical text-conditional image generation with clip latents," 2022, \textit{arXiv:2204.06125}.
    
    
    \bibitem{Dhariwal2021}
    P. Dhariwal and A. Nichol, A., ``Diffusion models beat gans on image synthesis," \textit{Advances in Neural Inf. Process. Syst.}, vol. 34, pp. 8780-8794, Dec. 2021.
    
    \bibitem{Dickstein2015}
    J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan and S. Ganguli, ``Deep unsupervised learning using nonequilibrium thermodynamics," in \textit{Proc. 32nd Int. Conf. on Mach. Learn.}, Jul. 2015, pp. 2256-2265.
    
    \bibitem{Ho2020}
    J. Ho, A. Jain and P. Abbeel, ``Denoising diffusion probabilistic models," 2020, \textit{arXiv:2006.11239}.
    
    \bibitem{Song2019}
    Y. Song and S. Ermon, ``Generative modeling by estimating gradients of the data distribution," \textit{Advances in Neural Inf. Process. Syst.}, vol. 32, Dec. 2019.
    
    \bibitem{Rombach2022}
    R. Rombach, A. Blattmann, D. Lorenz, P. Esser and B. Ommer, ``High-resolution image synthesis with latent diffusion models," in \textit{Proc. IEEE/CVF Conf. Comp. Vis. and Pattern Recognit.}, Jun. 2022, pp. 10684-10695.
    
    \bibitem{Song2021}
    Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon and B. Poole, “Score-Based Generative Modeling through Stochastic Differential Equations,” in \textit{Int. Conf. on Learn. Representations}, Sep. 2021, pp. 1-36.

    \bibitem{Ronneberger2015}
    O. Ronneberger, P. Fischer and T. Brox. ``U-net: Convolutional networks for biomedical image segmentation," in \textit{Int. Conf. on Med. Image Comput. and Computer-assisted Intervention}, Oct. 2015, pp. 234–241.

    \bibitem{Schuhmann2022}
    C. Schuhmann, ``Laion-5b: An open large-scale dataset for training next generation image-text models," 2022, \textit{arXiv:2210.08402}.
    
    \bibitem{Ruiz2023}
    N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein and K. Aberman, 2023. ``Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation," in \textit{Proc. IEEE/CVF Conf. Comp. Vis. and Pattern Recognit.}, Jun. 2023, pp. 22500-22510.

    \bibitem{Liu2020}
    Y. Liu et al. ``A survey on neural trojans," in \textit{IEEE 21st Int. Symp. on Qual. Electron. Des.}, Mar. 2020, pp. 33-39.
    
    \bibitem{Edraki2021}
    M. Edraki, N. Karim, N. Rahnavard, A. Mian and M. Shah, ``Odyssey: Creation, analysis and detection of trojan models," \textit{IEEE Trans. on Inf. Forensics and Secur.}, vol. 16, pp.4521-4533, Aug. 2021.
    
    \bibitem{Zhang2021}
    X. Zhang, R. Gupta, A. Mian, N. Rahnavard and M. Shah, ``Cassandra: Detecting trojaned networks from adversarial perturbations," \textit{IEEE Access}, vol. 9, pp. 135856-135867, Jul. 2021.
    
    \bibitem{Clements2018}
    J. Clements and Y. Lao, ``Backdoor Attacks on Neural Network Operations," in \textit{IEEE Global Conf. on Signal and Inf. Process.}, Nov. 2018 pp. 1154–1158.
    
    \bibitem{Yao2019}
    Y. Yao, H. Li, H. Zheng and B. Y. Zhao, ``Latent Backdoor Attacks on Deep Neural Networks," in \textit{Proc. ACM SIGSAC Conf. on Comput. and Commun. Secur.}, Nov. 2019, pp. 2041-2055.   
    
    \bibitem{Zhang2023}
    Z. Zhang et al. ``Red alarm for pre-trained models: Universal vulnerability to neuron-level backdoor attacks," \textit{Mach. Intell. Res.}, vol. 20, no. 2, pp.180-193, Apr. 2023.
    
    \bibitem{Zhai2023}
    S. Zhai, Y. Dong, Q. Shen, S. Pu, Y. Fang and H. Su, H., ``Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning," 2023 \textit{arXiv:2305.04175}.
    
    \bibitem{Chou2022}
    S. Y. Chou, P. Y. Chen and T. Y. Ho, ``How to Backdoor Diffusion Models?," in \textit{Proc. IEEE/CVF Conf. Comp. Vis. and Pattern Recognit.}, Jun. 2023, pp. 4015-4024.
    
    \bibitem{Liu2023}
    H. Liu, Y. Wu, S. Zhai, B. Yuan and N. Zhang, ``RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts," in \textit{Proc. IEEE/CVF Conf. Comp. Vis. and Pattern Recognit.}, Jun. 2023, pp. 20585-20594.
    
    \bibitem{Chen2023b}
    W. Chen, D. Song and B. Li, ``Trojdiff: Trojan attacks on diffusion models with diverse targets," in \textit{Proc. IEEE/CVF Conf. Comp. Vis. and Pattern Recognit.}, Jun. 2023, pp. 4035-4044. 
    
    \bibitem{Zheng2023}
    M. Zheng, Q. Lou and L. Jiang, ``Trojvit: Trojan insertion in vision transformers," in \textit{Proc. IEEE/CVF Conf. Comp. Vis. and Pattern Recognit.}, Jun. 2023, pp. 4025-4034.
    
    \bibitem{GoogleSGE}
    E. Reid. ``Supercharging Search with generative AI." Google. https://blog.google/products/search/generative-ai-search/ (accessed Jul. 12, 2023).
    
    \bibitem{Kurita2020}
    K. Kurita, P. Michel and G. Neubig, ``Weight poisoning attacks on pre-trained models," 2020, arXiv:2004.06660.
    
    \bibitem{Radford2021}
    A. Radford et al., ``Learning Transferable Visual Models From Natural Language Supervision," in \textit{Proc. 38th Int. Conf. on Mach. Learn.}, Jul. 2021, pp. 8748-8763.
    
    \bibitem{Heusel2017}
    M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler and S. Hochreiter, ``Gans trained by a two time-scale update rule converge to a local nash equilibrium," \textit{Advances in Neural Inf. Process. Syst.}, vol. 30, Dec. 2017.

    \bibitem{Aafaq2022}
    N. Aafaq, N. Akhtar, W. Liu, M. Shah and A. Mian, ``Language Model Agnostic Gray-Box Adversarial Attack on Image Captioning," \textit{IEEE Trans. on Inf. Forensics and Secur.}, vol. 18, pp. 626-638, Dec. 2022.

    \bibitem{HuggingFace}
    HuggingFace ``Hugging Face Hub documentation." https://huggingface.co/ (accessed Jul. 12, 2023).

    \bibitem{Lin2014}
    T. Y. Lin et al., ``Microsoft coco: Common objects in context," in \textit{Proc. 13th Euro. Conf. Comput}, Sep. 2014, pp. 740-755.
    
    \bibitem{Stefanini2022}
    M. Stefanini, M. Cornia, L. Baraldi, S. Cascianelli, G. Fiameni and R. Cucchiara, ``From show to tell: A survey on deep learning-based image captioning," \textit{IEEE Trans. on Pattern Anal. and Mach. Intell.}, vol. 45, no. 1, pp.539-559, Feb. 2022
    
    \bibitem{Li20222}
    J. Li, D. Li, C. Xiong and S. Hoi, ``Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation," in \textit{Proc. 39th Int. Conf. on Mach. Learn.}, Jun. 2022, pp. 12888-12900.

\end{thebibliography}
\end{document}
