% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{sidecap}

\usepackage[color=red]{todonotes}
\usepackage{marginnote}
\let\marginpar\marginnote
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%Import the natbib package and sets a bibliography  and citation styles
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\newcolumntype{?}{!{\vrule width 1pt}}

\begin{document}
%
\title{Understanding Silent Failures \\ in Medical Image Classification}
%
\titlerunning{Silent Failures in Medical Image Classification}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Till J. Bungert\inst{1,2} \and
Levin Kobelke\inst{1,2} \and
Paul F. Jaeger\inst{1,2}}
%index{Bungert, Till J.}
%index{Kobelke, Levin}
%index{Jaeger, Paul F.}

\authorrunning{T. Bungert et al.}
\institute{Interactive Machine Learning Group, German Cancer Research Center (DKFZ), Heidelberg, Germany \and
Helmholtz Imaging, DKFZ, Heidelberg, Germany
\email{till.bungert@dkfz-heidelberg.de}}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
    To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent
    failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in
    the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant
    source of failures in image classification is distribution shifts between training data and deployment data.
    To understand the current state of silent failure prevention in medical imaging, we conduct the first
    comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution
    shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we
    conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate
    this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize
    shifts and failures. On the basis of various examples, we demonstrate how this tool can help researchers gain
    insight into the requirements for safe application of classification systems in the medical domain. The
    open-source benchmark and tool are at: \url{https://github.com/IML-DKFZ/sf-visuals}.

    \keywords{Failure detection \and Distribution shifts \and Benchmark}
\end{abstract}

\section{Introduction}%
    \label{sec:introduction}
    % Figure environment removed
    %    
    Although machine learning-based classification systems have achieved significant breakthroughs in various
    research and practical areas, their clinical application is still lacking. A primary reason is the lack of
    reliability, i.e. failure cases produced by the system, which predominantly occur when deployment data
    differs from the data it was trained on, a phenomenon known as \textit{distribution shifts}. In medical
    applications, these shifts can be caused by image corruption (“corruption shift”), unseen variants of
    pathologies (“manifestation shift”), or deployment in new clinical sites with different scanners and
    protocols (“acquisition shift”) \cite{castroCausalityMattersMedical2020a}. The \textit{robustness} of a
    classifier, i.e. its ability to generalize across these shifts, is extensively studied in the computer vision
    community with a variety of recent benchmarks covering nuanced realistic distribution shifts
    \cite{idrissiImageNetXUnderstandingModel2022,kohWILDSBenchmarkIntheWild2021,santurkarBREEDSBenchmarksSubpopulation2022,jaegerCallReflectEvaluation2022a},
    and is also studied in isolated cases in the biomedical community
    \cite{zhangBenchmarkingRobustnessDeep2022,bandBenchmarkingBayesianDeep2022,bernhardtFailureDetectionMedical2022}.
    Despite these efforts, perfect classifiers are not to be expected, thus a second mitigation strategy is to
    detect and defer the remaining failures, thus \textit{preventing failures to be silent}. This is done by
    means of confidence scoring functions (CSF) of different types as studied in the fields of misclassification
    detection (MisD)
    \cite{corbiereAddressingFailurePrediction2019a,hendrycksBaselineDetectingMisclassified2018,malininPredictiveUncertaintyEstimation2018},
    Out-of-Distribution detection (OoD-D)
    \cite{fortExploringLimitsOutofDistribution2021,winkensContrastiveTrainingImproved2020,leeSimpleUnifiedFramework2018,hendrycksBaselineDetectingMisclassified2018,devriesLearningConfidenceOutofDistribution2018a,liangEnhancingReliabilityOutofdistribution2020},
    selective classification (SC)
    \cite{liuDeepGamblersLearning2019,geifmanSelectiveClassificationDeep2017,geifmanSelectiveNetDeepNeural2019},
    and predictive uncertainty quantification (PUQ) \cite{ovadiaCanYouTrust2019,kendallWhatUncertaintiesWe2017}.

    \textit{We argue, that silent failures, which occur when test cases break both the classifier and the CSF, are a significant bottleneck in the clinical translation of ML systems and require further attention in the medical community.}

    Note that the task of silent failure prevention is orthogonal to calibration, as, for example, a perfectly calibrated classifier can still yield substantial amounts of silent failures and vice versa~\cite{jaegerCallReflectEvaluation2022a}.

    Bernhardt et al.\cite{bernhardtFailureDetectionMedical2022} studied failure detection on several biomedical
    datasets, but only assessed the performance of CSFs in isolation without considering the classifier's ability
    to prevent failures. Moreover, their study did not include distribution shifts thus lacking a wide range of
    realistic failure sources. Jaeger et al.\cite{jaegerCallReflectEvaluation2022a}, on the other hand, recently
    discussed various shortcomings in current research on silent failures including the common lack of
    distribution shifts and the lack of assessing the classifier and CSF as a joint system. However, their study
    did not cover tasks from the biomedical domain.

    In this work, our contribution is twofold: \textbf{1)} Building on the work of Jaeger et
    al.\cite{jaegerCallReflectEvaluation2022a}, we present the first comprehensive study of silent failure
    prevention in the biomedical field. We compare various CSFs under a wide range of distribution shifts on four
    biomedical datasets. Our study provides valuable insights and the underlying framework is made openly
    available to catalyze future research in the community. \textbf{2)} Since the benchmark reveals that none of
    the predominant CSFs can reliably prevent silent failures in biomedical tasks, we argue that a deeper
    understanding of the root causes in the data itself is required. To this end, we present SF-Visuals, a
    visualization tool that facilitates identifying silent failures in a dataset and investigating their causes
    (see Figure~\ref{fig:overview}). Our approach contributes to recent research on visual analysis of failures
    \cite{idrissiImageNetXUnderstandingModel2022}, which has not focused on silent failures and distribution
    shifts before.

\section{Methods} % (fold)
    \label{sec:methods}
    \textbf{Benchmark for Silent Failure Prevention under Distribution Shifts.}
    We follow the spirit of recent robustness benchmarks, where existing datasets have been enhanced by various distribution shifts to evaluate methods under a wide range of failure sources and thus simulate real-world application~\cite{santurkarBREEDSBenchmarksSubpopulation2022,kohWILDSBenchmarkIntheWild2021}. To our knowledge, no such comprehensive benchmark currently exists in the biomedical domain. Specifically, we introduce corruptions of various intensity levels to the images in four datasets in the form of brightness, motion blur, elastic transformations and Gaussian noise. We further simulate acquisition shifts and manifestation shifts by splitting the data into "source domain" (development data) and "target domain" (deployment data) according to sub-class information from the meta-data such as lesion subtypes or clinical sites. \textbf{Dermoscopy dataset:} We combine data from ISIC 2020~\cite{rotembergPatientcentricDatasetImages2021}, derma 7 point~\cite{kawaharaSevenPointChecklistSkin2019}, PH2~\cite{mendoncaPH2DermoscopicImage2013} and HAM10000~\cite{tschandlHAM10000DatasetLarge2018} and map all lesion sub-types to the super-classes “benign” or “malignant”. We emulate two acquisition shifts by defining either images from the Memorial Sloan Kettering Cancer Center (MSKCC) or Hospital Clinic Barcelona (HCB) as the target domain and the remaining images as the source domain. Further, a manifestation shift is designed by defining the lesion subtypes "keratosis-like" (benign) and "actinic keratosis" (malignant) as the target domain. \textbf{Chest X-ray dataset:} We pool the data from CheXpert~\cite{irvinCheXpertLargeChest2019}, NIH14~\cite{wangChestXRay8HospitalScaleChest2017} and MIMIC~\cite{johnsonMIMICIIIFreelyAccessible2016}, while only retaining the classes common to all three. Next, we emulate two acquisition shifts by defining either the NIH14 or the CheXpert data as the target domain. \textbf{FC-Microscopy dataset:} The RxRx1 dataset~\cite{sypetkowskiRxRx1DatasetEvaluating2023} represents the fluorescence cell microscopy domain. Since the images were acquired in 51 deviating acquisition steps, we define 10 of these batches as target-domain to emulate an acquisition shift. \textbf{Lung Nodule CT dataset:} We create a simple 2D binary nodule classification task based on the 3D LIDC-IDRI data~\cite{armatoLungImageDatabase2011} by selecting the slice with the largest annotation per nodule ($\pm$ two slices resulting in 5 slices per nodule). Average malignancy ratings (four raters per nodule, scores between 1 and 5)  $>2$ are considered malignant and all others as benign. We emulate two manifestation shifts by defining nodules with high spiculation (rating $>$2), and low texture (rating $<$3) as target domains.\\
    The datasets consist only of publicly available data, our benchmark provides scripts to automatically
    generate the combined datasets and distribution shifts.\\\\
    \textbf{The SF-Visuals Tool: Visualizing Silent Failures.}
    The proposed tool is based on three simple operations, that enable effective and intuitive analysis of silent failures in datasets across various CSFs: \textit{1) Interactive Scatter Plots:} See example in Figure~\ref{fig:overview}b. We first reduce the dimensionality of the classifier’s latent space to 50 using principal component analysis and use t-SNE to obtain the final 3-dimensional embedding. Interactive functionality includes coloring dots via pre-defined schemes such as classes, distribution shifts, classifier confusion matrix, or CSF confusion matrix. The associated images are displayed upon selection of a dot to establish a direct visual link between input space and embedding. \textit{2) Concept Cluster Plots:} See examples in Figure~\ref{fig:overview}c. To abstract away from individual points in the scatter plot, concepts of interest, such as classes or distribution shifts can be defined and visualized to identify conceptual commonalities and differences in the data as perceived by the model. Therefore, k-means clustering is applied to the 3-dimensional embedding. Nine clusters are identified per concept and the resulting plots show the closest-to-center image per cluster as a visual representation of the concept. \textit{3) Silent Failure Visualization:} See examples in Figure~\ref{fig:bottom_up}. We sort all failures by the classifier confidence and by default show the images associated with the top-two most confident failures. For corruption shifts, we further allow investigating the predictions on a fixed input image over varying intensity levels.\\\\
    Based on these visualizations, the functionality of SF-Visuals is three-fold: 1) Visual analysis of the
    dataset including distribution shifts. 2) Visual analysis of the general behavior of various CSFs on a given task
    3) Visual analysis of individual silent failures in the dataset for various CSFs.
    %
\section{Experimental Setup} % (fold)
    \label{sec:setup}
    \textbf{Evaluating Silent Failure Prevention:} % (fold)
    We follow Jaeger et al.~\cite{jaegerCallReflectEvaluation2022a} in evaluating silent failure prevention as a joint task of the classifier and the CSF. The area under the risk-coverage curve AURC reflects this task, since it considers both the classifier's accuracy as well as the CSF's ability to detect failures by assigning low confidence scores. Thus, it can be interpreted as a \textit{silent failure rate} or the error rate averaged over steps of filtering cases one by one according to their rank of confidence score (low to high). Exemplary risk-coverage curves are shown in Appendix Figure~3. \textbf{Compared Confidence Scoring Functions:} We compare the following CSFs: The maximum softmax response (MSR) and the predictive entropy computed from the classifier's softmax output, three predictive uncertainty measures based on Monte-Carlo Dropout (MCD)~\cite{galDropoutBayesianApproximation2016}, namely mean softmax (MCD-MSR), predictive entropy (MCD-PE) and expected entropy (MCD-EE), ConfidNet~\cite{corbiereAddressingFailurePrediction2019a}, which is trained as an extension to the classifier, DeepGamblers (DG) that learns a confidence like reservation score (DG-Res)~\cite{liuDeepGamblersLearning2019} and the work of DeVries et al.~\cite{devriesLearningConfidenceOutofDistribution2018a}. \textbf{Training Settings:} On each dataset, we employ the classifier behind the respective leading results in literature: For chest X-ray data we use DenseNet121~\cite{huangDenselyConnectedConvolutional2017}, for dermoscopy data we use EfficientNet-B4~\cite{tanEfficientNetRethinkingModel2019} and for fluorescence cell microscopy and lung nodule CT data we us DenseNet161~\cite{huangDenselyConnectedConvolutional2017}. We select the initial learning rate between $10^{-3}$ and $10^{-5}$ and weight decay between 0 and $10^{-5}$ via grid search and optimize for validation accuracy. All models were trained with dropout. All hyperparameters can be found in Appendix Table~3.
    %
\section{Results} % (fold)
    \label{sub:results}

    \subsection{Silent Failure Prevention Benchmark} % (fold)
        \label{sub:silent_failure_detection_benchmark}
        %
        \begin{table}[t]
            \begin{center}
                \resizebox{\textwidth}{!}{                 \input{tables/paper_results_condensed_aurc.tex}
                }
            \end{center}
            \caption{\textbf{Silent failure prevention benchmark results measured in $\mathrm{AURC} [\%]]$
            (score range: [0, 100], lower is better).} The coloring is normalized by column, while lighter
            colors depict better scores. All values denote an average of three runs. "cor" denotes the average over
            all corruption types and intensities levels. Similarly, "acq"/"man" denote averages over all acquisition/manifestation shifts per dataset. "iid" denotes scenarios without distribution shifts. Results with further metrics are reported in Appendix Table~2}
            \label{tab:benchmark}
        \end{table}

        Table~\ref{tab:benchmark} shows the results of our benchmark for silent failure prevention in the biomedical
        domain and provides the first overview of the current state of the reliability of classification systems in
        high-stake biomedical applications.

        \textbf{None of the evaluated methods from the literature beats the Maximum Softmax Response baseline across a realistic range of failure sources.} This result is generally consistent with previous findings in Bernhard et al.~\cite{bernhardtFailureDetectionMedical2022} and Jaeger et al.~\cite{jaegerCallReflectEvaluation2022a}, but is shown for the first time for a diverse range of realistic biomedical failure sources. Previously proposed methods do not outperform MSR baselines even in the settings they have been proposed for, e.g. Devries et al. under distribution shifts, or ConfidNet and DG-RES for i.i.d. testing.

        \textbf{MCD and loss attenuation are able to improve the MSR.}
        MCD-MSR is the overall best performing method indicating that MCD generally improves the confidence scoring ability of softmax outputs on these tasks. Interestingly, the DG loss attenuation applied to MCD-MSR, DG-MCD-MSR, which has not been part of the original DG publication but was first tested in Jaeger et al.~\cite{jaegerCallReflectEvaluation2022a}, shows the best results on i.i.d. testing on 3 out of 4 tasks. However, the method is not reliable across all settings, falling short on manifestation shifts and corruptions on the lung nodule CT dataset.

        \textbf{Effects of particular shifts on the reliability of a CSF might be interdependent.}
        When looking beyond the averages displayed in Table~\ref{tab:benchmark} and analyzing the results of individual clinical centers, corruptions and manifestation shifts, one remarkable pattern can be observed: In various cases, the same CSF showed opposing behavior between two variants of the same shift on the same dataset. For instance, Devries et al. outperforms all other CSFs for one clinical site (MSKCC) as target domain, but falls short on the other one (HCB). On the Chest X-ray dataset, MCD worsens the performance for darkening corruptions across all CSFs and intensity levels, whereas the opposite is observed for brightening corruptions. Further, on the lung nodule CT dataset, DG-MCD-RES performs best on bright/dark corruptions and the spiculation manifestation shift, but worst on noise corruption and falls behind on the texture manifestation shift. These observations indicate trade-offs, where, within one distribution shift, reliability against one domain might induce susceptibility to other domains.

        \textbf{Current systems are not generally reliable enough for clinical application.} Although CSFs can mitigate the rate of silent failures (see Appendix Figure~3), the reliability of the resulting classification systems is not sufficient for high-stake applications in the biomedical domain, with substantial rates of silent failure in three out of four tasks. Therefore, a deeper understanding of the root causes of these failures is needed.

    \subsection{Investigation of Silent Failure Sources} % (fold)
        \label{sec:investigation_of_failure sources}
        % Figure environment removed
        \textbf{SF-Visuals enables comprehensive analysis of silent failures} Figure~\ref{fig:overview} vividly demonstrates the added benefit of the proposed tool. First, an Interactive Scatter Plot (Figure~\ref{fig:overview}b, left) provides an overview of the MSKCC acquisition shift on the dermoscopy dataset and reveals a severe change of the data distribution. For instance, some malignant lesions of the target domain (purple dots) are located deep within the "benign" cluster. Figure~\ref{fig:overview}c provides a Concept Cluster Plot that visually confirms how some of these lesions (purple dot) share characteristics of the benign cluster of the source domain (turquoise dot), such as being smaller, brighter, and rounder compared to malignant source-lesions (blue dot). The right-hand plot of Figure~\ref{fig:overview}b reveals that these cases have in fact caused silent failures (red crosses) and visual inspection (see arrow and Figure~\ref{fig:overview}a) confirms the hypothesis that these failures have been caused by the fact that the acquisition shift introduced malignant target-lesions that exhibit benign characteristics.
        Figure~\ref{fig:overview}b (right) further provides insights about the general behavior of the CSF: Silent failures occur for both classes and are either located at the cluster border (i.e. decision boundary), deeper inside the opposing cluster center (severe class confusions), or represent outliers. Most silent failures occur at the boundary, where the CSF should reflect class ambiguities by low scores, hinting at general misbehavior or overconfidence in this area. Further towards the cluster boundary, the ambiguity in images seems to increase, as the CSF is able to detect the failures (light blue layer of dots). A layer of “false alarms” follows (brown colored dots), where decisions are correct, but confidence is still low.\\\\
        %   
        \textbf{SF-Visuals generates insights across tasks and distribution shifts.} \textbf{i.i.d. (no shift):} This analysis reveals how simple class clustering (no distribution shifts involved) can help to gain intuition on the most severe silent failures (examples selected as the two highest-confidence failures). On the lung nodule CT data (Figure~\ref{fig:bottom_up}a), we see how the classifier and CSF break down when a malignant sample (typically: small bright, round) exhibits characteristics typical to benign lesions (larger, less cohesive contour, darker) and vice versa. This pattern of contrary class characteristics is also observed on the dermoscopy dataset (\ref{fig:bottom_up}c). The failure example at the top is particularly severe, and localization in the scatter plot reveals a position deep inside the 'benign' cluster indicating either a severe sampling error in the dataset (e.g. underrepresented lesion sub-type) or simply a wrong label.
        \textbf{Corruption shift:} Figures~\ref{fig:bottom_up}b and \ref{fig:bottom_up}d show for the Lung Nodule CT data and the dermoscopy data, respectively, how corruptions can lead to silent failures in low-confident predictions. In both examples, the brightening of the image leads to a malignant lesion taking on benign characteristics (brighter and smoother skin on the dermoscopy data, decreased contrast between lesion and background on the Lung Nodule CT data).
        \textbf{Acquisition shift:} Additionally to the example in Figure~\ref{fig:overview}, Figure~\ref{fig:bottom_up}e shows how the proposed tool visualizes an acquisition shift on the chest X-ray data. While this reveals an increased blurriness in the target domain, it is difficult to derive further insights involving specific pathologies without a clinical expert. Figure~\ref{fig:bottom_up}h shows a classification failure from a target clinical center together with the model's confidence as measured by MSR and DG. While MSR assigns the prediction low confidence thereby catching the failure, DG assigns high confidence for the same model and prediction, causing a silent failure. This example shows how the tool allows the comparison of CSFs and can help to identify failure modes specific to each CSF.
        \textbf{Manifestation shift:} On the dermoscopy data (Figure~\ref{fig:bottom_up}g), we see how a manifestation shift can cause silent failures. The benign lesions in the target domain are similar to the malignant lesions in the source domain (rough skin, irregular shapes), and indeed the two failures in the target domain seem to fall into this trap. On the lung nodule CT data ((Figure~\ref{fig:bottom_up}f), we observe a  visual distinction between the spiculated target domain (spiked surface) and the non-spiculated source domain (smooth surface).

\section{Conclusion} % (fold)
    \label{sec:conclusion}
    We see two major opportunities for this work to make an impact on the community. 1) We hope the revealed shortcomings of current systems on biomedical tasks in combination with the deeper understanding of CSF behaviors granted by SF-Visuals will catalyze research towards a new generation of more reliable CSFs. 2) This study shows that in order to progress towards reliable ML systems, a deeper understanding of the data itself is required. SF-Visuals can help to bridge this gap and equip researchers with a better intuition of when and how to employ ML systems for a particular task.

\section*{Acknowledgements}
This work was funded by Helmholtz Imaging (HI), a platform of the Helmholtz Incubator on Information and Data Science.

\bibliographystyle{splncs04}
\bibliography{references.bib}

\end{document}
