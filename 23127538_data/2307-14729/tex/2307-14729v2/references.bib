@article{apostolidisSurveyAdversarialDeep2021,
  title = {A {{Survey}} on {{Adversarial Deep Learning Robustness}} in {{Medical Image Analysis}}},
  author = {Apostolidis, Kyriakos D. and Papakostas, George A.},
  year = {2021},
  month = jan,
  journal = {Electronics},
  volume = {10},
  number = {17},
  pages = {2132},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics10172132},
  abstract = {In the past years, deep neural networks (DNN) have become popular in many disciplines such as computer vision (CV), natural language processing (NLP), etc. The evolution of hardware has helped researchers to develop many powerful Deep Learning (DL) models to face numerous challenging problems. One of the most important challenges in the CV area is Medical Image Analysis in which DL models process medical images\textemdash such as magnetic resonance imaging (MRI), X-ray, computed tomography (CT), etc.\textemdash using convolutional neural networks (CNN) for diagnosis or detection of several diseases. The proper function of these models can significantly upgrade the health systems. However, recent studies have shown that CNN models are vulnerable under adversarial attacks with imperceptible perturbations. In this paper, we summarize existing methods for adversarial attacks, detections and defenses on medical imaging. Finally, we show that many attacks, which are undetectable by the human eye, can degrade the performance of the models, significantly. Nevertheless, some effective defense and attack detection methods keep the models safe to an extent. We end with a discussion on the current state-of-the-art and future challenges.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {adversarial attack,computer vision,convolutional neural networks,deep learning,medical image analysis},
  file = {/Users/tillb/Zotero/storage/UVK3LTCY/Apostolidis and Papakostas - 2021 - A Survey on Adversarial Deep Learning Robustness i.pdf}
}

@article{armatoLungImageDatabase2011,
  title = {The {{Lung Image Database Consortium}} ({{LIDC}}) and {{Image Database Resource Initiative}} ({{IDRI}}): {{A Completed Reference Database}} of {{Lung Nodules}} on {{CT Scans}}: {{The LIDC}}/{{IDRI}} Thoracic {{CT}} Database of Lung Nodules},
  shorttitle = {The {{Lung Image Database Consortium}} ({{LIDC}}) and {{Image Database Resource Initiative}} ({{IDRI}})},
  author = {Armato, Samuel G. and McLennan, Geoffrey and Bidaut, Luc and {McNitt-Gray}, Michael F. and Meyer, Charles R. and et al.},
  year = {2011},
  month = jan,
  journal = {Medical Physics},
  volume = {38},
  number = {2},
  pages = {915--931},
  issn = {00942405},
  doi = {10.1118/1.3528204},
  langid = {english},
  file = {/Users/tillb/Zotero/storage/SBKCQQVX/Armato et al. - 2011 - The Lung Image Database Consortium (LIDC) and Imag.pdf}
}

@inproceedings{bandBenchmarkingBayesianDeep2022,
  title = {Benchmarking {{Bayesian Deep Learning}} on {{Diabetic Retinopathy Detection Tasks}}},
  booktitle = {Thirty-Fifth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}} ({{Round}} 2)},
  author = {Band, Neil and Rudner, Tim G. J. and Feng, Qixuan and Filos, Angelos and Nado, Zachary and et al.},
  year = {2022},
  month = jan,
  abstract = {Bayesian deep learning seeks to equip deep neural networks with the ability to precisely quantify their predictive uncertainty, and has promised to make deep learning more reliable for safety-critical real-world applications. Yet, existing Bayesian deep learning methods fall short of this promise; new methods continue to be evaluated on unrealistic test beds that do not reflect the complexities of downstream real-world tasks that would benefit most from reliable uncertainty quantification. We propose the RETINA Benchmark, a set of real-world tasks that accurately reflect such complexities and are designed to assess the reliability of predictive models in safety-critical scenarios. Specifically, we curate two publicly available datasets of high-resolution human retina images exhibiting varying degrees of diabetic retinopathy, a medical condition that can lead to blindness, and use them to design a suite of automated diagnosis tasks that require reliable predictive uncertainty quantification. We use these tasks to benchmark well-established and state-of-the-art Bayesian deep learning methods on task-specific evaluation metrics. We provide an easy-to-use codebase for fast and easy benchmarking following reproducibility and software design principles. We provide implementations of all methods included in the benchmark as well as results computed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and evaluation on at least 6 random seeds each.},
  langid = {english},
  file = {/Users/tillb/Zotero/storage/Z5KYDRRF/Band et al. - 2022 - Benchmarking Bayesian Deep Learning on Diabetic Re.pdf}
}

@misc{bernhardtFailureDetectionMedical2022,
  title = {Failure {{Detection}} in {{Medical Image Classification}}: {{A Reality Check}} and {{Benchmarking Testbed}}},
  shorttitle = {Failure {{Detection}} in {{Medical Image Classification}}},
  author = {Bernhardt, Melanie and Ribeiro, Fabio De Sousa and Glocker, Ben},
  year = {2022},
  month = oct,
  number = {arXiv:2205.14094},
  eprint = {arXiv:2205.14094},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.14094},
  abstract = {Failure detection in automated image classification is a critical safeguard for clinical deployment. Detected failure cases can be referred to human assessment, ensuring patient safety in computer-aided clinical decision making. Despite its paramount importance, there is insufficient evidence about the ability of state-of-the-art confidence scoring methods to detect test-time failures of classification models in the context of medical imaging. This paper provides a reality check, establishing the performance of in-domain misclassification detection methods, benchmarking 9 widely used confidence scores on 6 medical imaging datasets with different imaging modalities, in multiclass and binary classification settings. Our experiments show that the problem of failure detection is far from being solved. We found that none of the benchmarked advanced methods proposed in the computer vision and machine learning literature can consistently outperform a simple softmax baseline, demonstrating that improved out-of-distribution detection or model calibration do not necessarily translate to improved in-domain misclassification detection. Our developed testbed facilitates future work in this important area},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/tillb/Zotero/storage/9DDLN8G3/Bernhardt et al. - 2022 - Failure Detection in Medical Image Classification.pdf;/Users/tillb/Zotero/storage/FLRBFKMG/2205.html}
}

@article{calistoBreastScreeningAIEvaluatingMedical2022,
  title = {{{BreastScreening-AI}}: {{Evaluating}} Medical Intelligent Agents for Human-{{AI}} Interactions},
  shorttitle = {{{BreastScreening-AI}}},
  author = {Calisto, Francisco Maria and Santiago, Carlos and Nunes, Nuno and Nascimento, Jacinto C.},
  year = {2022},
  month = may,
  journal = {Artificial Intelligence in Medicine},
  volume = {127},
  pages = {102285},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2022.102285},
  abstract = {In this paper, we developed BreastScreening-AI within two scenarios for the classification of multimodal beast images: (1) Clinician-Only; and (2) Clinician-AI. The novelty relies on the introduction of a deep learning method into a real clinical workflow for medical imaging diagnosis. We attempt to address three high-level goals in the two above scenarios. Concretely, how clinicians: i) accept and interact with these systems, revealing whether are explanations and functionalities required; ii) are receptive to the introduction of AI-assisted systems, by providing benefits from mitigating the clinical error; and iii) are affected by the AI assistance. We conduct an extensive evaluation embracing the following experimental stages: (a) patient selection with different severities, (b) qualitative and quantitative analysis for the chosen patients under the two different scenarios. We address the high-level goals through a real-world case study of 45 clinicians from nine institutions. We compare the diagnostic and observe the superiority of the Clinician-AI scenario, as we obtained a decrease of 27\% for False-Positives and 4\% for False-Negatives. Through an extensive experimental study, we conclude that the proposed design techniques positively impact the expectations and perceptive satisfaction of 91\% clinicians, while decreasing the time-to-diagnose by 3~min per patient.},
  langid = {english},
  keywords = {Artificial intelligence,Breast Cancer,Healthcare,Human-computer interaction,Medical imaging},
  file = {/Users/tillb/Zotero/storage/8GR6A6JM/Calisto et al. - 2022 - BreastScreening-AI Evaluating medical intelligent.pdf;/Users/tillb/Zotero/storage/YLQ3R3MD/S0933365722000501.html}
}

@article{calistoIntroductionHumancentricAI2021,
  title = {Introduction of Human-Centric {{AI}} Assistant to Aid Radiologists for Multimodal Breast Image Classification},
  author = {Calisto, Francisco Maria and Santiago, Carlos and Nunes, Nuno and Nascimento, Jacinto C.},
  year = {2021},
  month = jun,
  journal = {International Journal of Human-Computer Studies},
  volume = {150},
  pages = {102607},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2021.102607},
  abstract = {In this research, we take an HCI perspective on the opportunities provided by AI techniques in medical imaging, focusing on workflow efficiency and quality, preventing errors and variability of diagnosis in Breast Cancer. Starting from a holistic understanding of the clinical context, we developed BreastScreening to support Multimodality and integrate AI techniques (using a deep neural network to support automatic and reliable classification) in the medical diagnosis workflow. This was assessed by using a significant number of clinical settings and radiologists. Here we present: i) user study findings of 45 physicians comprising nine clinical institutions; ii) list of design recommendations for visualization to support breast screening radiomics; iii) evaluation results of a proof-of-concept BreastScreening prototype for two conditions Current (without AI assistant) and AI-Assisted; and iv) evidence from the impact of a Multimodality and AI-Assisted strategy in diagnosing and severity classification of lesions. The above strategies will allow us to conclude about the behaviour of clinicians when an AI module is present in a diagnostic system. This behaviour will have a direct impact in the clinicians workflow that is thoroughly addressed herein. Our results show a high level of acceptance of AI techniques from radiologists and point to a significant reduction of cognitive workload and improvement in diagnosis execution.},
  langid = {english},
  keywords = {Artificial intelligence,Breast cancer,Healthcare,Human-computer interaction,Medical imaging},
  file = {/Users/tillb/Zotero/storage/9TX7GFIF/S1071581921000252.html}
}

@article{castroCausalityMattersMedical2020a,
  title = {Causality Matters in Medical Imaging},
  author = {Castro, Daniel C. and Walker, Ian and Glocker, Ben},
  year = {2020},
  month = jul,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3673},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17478-w},
  abstract = {Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Computational models,Data processing,Machine learning,Medical research,Predictive medicine},
  file = {/Users/tillb/Zotero/storage/LUUVC866/Castro et al. - 2020 - Causality matters in medical imaging.pdf}
}

@article{challenArtificialIntelligenceBias2019,
  title = {Artificial Intelligence, Bias and Clinical Safety},
  author = {Challen, Robert and Denny, Joshua and Pitt, Martin and Gompels, Luke and Edwards, Tom and et al.},
  year = {2019},
  month = mar,
  journal = {BMJ quality \& safety},
  volume = {28},
  number = {3},
  pages = {231--237},
  issn = {2044-5423},
  doi = {10.1136/bmjqs-2018-008370},
  langid = {english},
  pmcid = {PMC6560460},
  pmid = {30636200},
  keywords = {Algorithms,artificial intelligence,Artificial Intelligence,Bias,clinical decision support systems,clinical safety,Humans,machine learning,Patient Safety,Quality of Health Care},
  file = {/Users/tillb/Zotero/storage/762SRYN4/Challen et al. - 2019 - Artificial intelligence, bias and clinical safety.pdf}
}

@inproceedings{corbiereAddressingFailurePrediction2019a,
  title = {Addressing {{Failure Prediction}} by {{Learning Model Confidence}}},
  booktitle = {NeurIPS},
  author = {Corbi{\`e}re, Charles and THOME, Nicolas and {Bar-Hen}, Avner and Cord, Matthieu and P{\'e}rez, Patrick},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Assessing reliably the confidence of a deep neural net and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction.},
  file = {/Users/tillb/Zotero/storage/CSJQG7XJ/Corbière et al. - 2019 - Addressing Failure Prediction by Learning Model Co.pdf}
}

@inproceedings{derakhshaniLifeLongerBenchmarkContinual2022,
  title = {{{LifeLonger}}: {{A Benchmark}} for~{{Continual Disease Classification}}},
  shorttitle = {{{LifeLonger}}},
  booktitle = {MICCAI},
  author = {Derakhshani, Mohammad Mahdi and Najdenkoska, Ivona and {van Sonsbeek}, Tom and Zhen, Xiantong and Mahapatra, Dwarikanath and et al.},
  editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {314--324},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-16434-7_31},
  abstract = {Deep learning models have shown a great effectiveness in recognition of findings in medical images. However, they cannot handle the ever-changing clinical environment, bringing newly annotated medical data from different sources. To exploit the incoming streams of data, these models would benefit largely from sequentially learning from new samples, without forgetting the previously obtained knowledge. In this paper we introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST collection, by applying existing state-of-the-art continual learning methods. In particular, we consider three continual learning scenarios, namely, task and class incremental learning and the newly defined cross-domain incremental learning. Task and class incremental learning of diseases address the issue of classifying new samples without re-training the models from scratch, while cross-domain incremental learning addresses the issue of dealing with datasets originating from different institutions while retaining the previously obtained knowledge. We perform a thorough analysis of the performance and examine how the well-known challenges of continual learning, such as the catastrophic forgetting exhibit themselves in this setting. The encouraging results demonstrate that continual learning has a major potential to advance disease classification and to produce a more robust and efficient learning framework for clinical settings. The code repository, data partitions and baseline results for the complete benchmark are publicly available\$\$\^\{1\}\$\$1(https://github.com/mmderakhshani/LifeLonger).},
  isbn = {978-3-031-16434-7},
  langid = {english},
  keywords = {Disease classification,Medical continual learning,Medical image analysis},
  file = {/Users/tillb/Zotero/storage/MUSVVH49/Derakhshani et al. - 2022 - LifeLonger A Benchmark for Continual Disease Clas.pdf}
}

@misc{devriesLearningConfidenceOutofDistribution2018a,
  title = {Learning {{Confidence}} for {{Out-of-Distribution Detection}} in {{Neural Networks}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  year = {2018},
  month = feb,
  number = {arXiv:1802.04865},
  eprint = {arXiv:1802.04865},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.04865},
  abstract = {Modern neural networks are very powerful predictive models, but they are often incapable of recognizing when their predictions may be wrong. Closely related to this is the task of out-of-distribution detection, where a network must determine whether or not an input is outside of the set on which it is expected to safely perform. To jointly address these issues, we propose a method of learning confidence estimates for neural networks that is simple to implement and produces intuitively interpretable outputs. We demonstrate that on the task of out-of-distribution detection, our technique surpasses recently proposed techniques which construct confidence based on the network's output distribution, without requiring any additional labels or access to out-of-distribution examples. Additionally, we address the problem of calibrating out-of-distribution detectors, where we demonstrate that misclassified in-distribution examples can be used as a proxy for out-of-distribution examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/tillb/Zotero/storage/DUBXBYFY/DeVries and Taylor - 2018 - Learning Confidence for Out-of-Distribution Detect.pdf;/Users/tillb/Zotero/storage/IF4IFHXC/1802.html}
}

@article{estevaDermatologistlevelClassificationSkin2017,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and et al.},
  year = {2017},
  month = feb,
  journal = {Nature},
  volume = {542},
  number = {7639},
  pages = {115--118},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature21056},
  abstract = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Diagnosis,Machine learning,Skin cancer},
  file = {/Users/tillb/Zotero/storage/ZZS6WITI/Esteva et al. - 2017 - Dermatologist-level classification of skin cancer .pdf;/Users/tillb/Zotero/storage/GB9C59DJ/nature21056.html}
}

@article{faustHeartRateVariability2022,
  title = {Heart Rate Variability for Medical Decision Support Systems: {{A}} Review},
  shorttitle = {Heart Rate Variability for Medical Decision Support Systems},
  author = {Faust, Oliver and Hong, Wanrong and Loh, Hui Wen and Xu, Shuting and Tan, Ru-San and et al.},
  year = {2022},
  month = jun,
  journal = {Computers in Biology and Medicine},
  volume = {145},
  pages = {105407},
  issn = {1879-0534},
  doi = {10.1016/j.compbiomed.2022.105407},
  abstract = {Heart Rate Variability (HRV) is a good predictor of human health because the heart rhythm is modulated by a wide range of physiological processes. This statement embodies both challenges to and opportunities for HRV analysis. Opportunities arise from the wide-ranging applicability of HRV analysis for disease detection. The availability of modern high-quality sensors and the low data rate of heart rate signals make HRV easy to measure, communicate, store, and process. However, there are also significant obstacles that prevent a wider use of this technology. HRV signals are both nonstationary and nonlinear and, to the human eye, they appear noise-like. This makes them difficult to analyze and indeed the analysis findings are difficult to explain. Moreover, it is difficult to discriminate between the influences of different complex physiological processes on the HRV. These difficulties are compounded by the effects of aging and the presence of comorbidities. In this review, we have looked at scientific studies that have addressed these challenges with advanced signal processing and Artificial Intelligence (AI) methods.},
  langid = {english},
  pmid = {35349801},
  keywords = {Artificial intelligence,Artificial Intelligence,Computer-aided diagnosis,Electrocardiography,Heart Rate,Heart rate variability,Humans,Patient remote monitoring,Signal Processing; Computer-Assisted}
}

@article{fortExploringLimitsOutofDistribution2021,
  title = {Exploring the {{Limits}} of {{Out-of-Distribution Detection}}},
  author = {Fort, Stanislav and Ren, Jie and Lakshminarayanan, Balaji},
  year = {2021},
  month = jul,
  journal = {arXiv:2106.03004 [cs]},
  eprint = {2106.03004},
  primaryclass = {cs},
  abstract = {Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection, we improve the AUROC from 85\% (current SOTA) to more than 96\% using Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66\% to 77\% using transformers and unsupervised pre-training. To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR-10 can be improved to 98.7\% with just 1 image per OOD class, and 99.46\% with 10 images per OOD class. For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard vision OOD benchmark tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/tillb/Zotero/storage/A2A8VLWJ/Fort et al. - 2021 - Exploring the Limits of Out-of-Distribution Detect.pdf;/Users/tillb/Zotero/storage/XQRC4XPQ/2106.html}
}

@inproceedings{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  booktitle = {ICML},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  pages = {1050--1059},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash{} extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {/Users/tillb/Zotero/storage/5KW63T5U/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@article{geifmanSelectiveClassificationDeep2017,
  title = {Selective {{Classification}} for {{Deep Neural Networks}}},
  author = {Geifman, Yonatan and {El-Yaniv}, Ran},
  year = {2017},
  month = jun,
  journal = {arXiv:1705.08500 [cs]},
  eprint = {1705.08500},
  primaryclass = {cs},
  abstract = {Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2\% error in top-5 ImageNet classification can be guaranteed with probability 99.9\%, and almost 60\% test coverage.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/tillb/Zotero/storage/GIXIZN2H/Geifman and El-Yaniv - 2017 - Selective Classification for Deep Neural Networks.pdf;/Users/tillb/Zotero/storage/KJSUVP5R/1705.html}
}

@article{geifmanSelectiveNetDeepNeural2019,
  title = {{{SelectiveNet}}: {{A Deep Neural Network}} with an {{Integrated Reject Option}}},
  shorttitle = {{{SelectiveNet}}},
  author = {Geifman, Yonatan and {El-Yaniv}, Ran},
  year = {2019},
  month = jun,
  journal = {arXiv:1901.09192 [cs, stat]},
  eprint = {1901.09192},
  primaryclass = {cs, stat},
  abstract = {We consider the problem of selective prediction (also known as reject option) in deep neural networks, and introduce SelectiveNet, a deep neural architecture with an integrated reject option. Existing rejection mechanisms are based mostly on a threshold over the prediction confidence of a pre-trained network. In contrast, SelectiveNet is trained to optimize both classification (or regression) and rejection simultaneously, end-to-end. The result is a deep neural network that is optimized over the covered domain. In our experiments, we show a consistently improved risk-coverage trade-off over several well-known classification and regression datasets, thus reaching new state-of-the-art results for deep selective classification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/tillb/Zotero/storage/WHY64YJ7/Geifman and El-Yaniv - 2019 - SelectiveNet A Deep Neural Network with an Integr.pdf;/Users/tillb/Zotero/storage/8JJGMTSU/1901.html}
}

@inproceedings{gonzalezDetectingWhenPretrained2021,
  title = {Detecting {{When Pre-trained nnU-Net Models Fail Silently}} for {{Covid-19 Lung Lesion Segmentation}}},
  booktitle = {MICCAI},
  author = {Gonzalez, Camila and Gotkowski, Karol and Bucher, Andreas and Fischbach, Ricarda and Kaltenborn, Isabel and et al.},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {304--314},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-87234-2_29},
  abstract = {Automatic segmentation of lung lesions in computer tomography has the potential to ease the burden of clinicians during the Covid-19 pandemic. Yet predictive deep learning models are not trusted in the clinical routine due to failing silently in out-of-distribution (OOD) data. We propose a lightweight OOD detection method that exploits the Mahalanobis distance in the feature space. The proposed approach can be seamlessly integrated into state-of-the-art segmentation pipelines without requiring changes in model architecture or training procedure, and can therefore be used to assess the suitability of pre-trained models to new data. We validate our method with a patch-based nnU-Net architecture trained with a multi-institutional dataset and find that it effectively detects samples that the model segments incorrectly.},
  isbn = {978-3-030-87234-2},
  langid = {english},
  keywords = {Distribution shift,Out-of-distribution detection,Uncertainty estimation},
  file = {/Users/tillb/Zotero/storage/LRK5RV9F/Gonzalez et al. - 2021 - Detecting When Pre-trained nnU-Net Models Fail Sil.pdf}
}

@misc{haIdentifyingMelanomaImages2020,
  title = {Identifying {{Melanoma Images}} Using {{EfficientNet Ensemble}}: {{Winning Solution}} to the {{SIIM-ISIC Melanoma Classification Challenge}}},
  shorttitle = {Identifying {{Melanoma Images}} Using {{EfficientNet Ensemble}}},
  author = {Ha, Qishen and Liu, Bo and Liu, Fuxu},
  year = {2020},
  month = oct,
  number = {arXiv:2010.05351},
  eprint = {arXiv:2010.05351},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.05351},
  abstract = {We present our winning solution to the SIIM-ISIC Melanoma Classification Challenge. It is an ensemble of convolutions neural network (CNN) models with different backbones and input sizes, most of which are image-only models while a few of them used image-level and patient-level metadata. The keys to our winning are: (1) stable validation scheme (2) good choice of model target (3) carefully tuned pipeline and (4) ensembling with very diverse models. The winning submission scored 0.9600 AUC on cross validation and 0.9490 AUC on private leaderboard.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/tillb/Zotero/storage/UUDSFMZE/Ha et al. - 2020 - Identifying Melanoma Images using EfficientNet Ens.pdf;/Users/tillb/Zotero/storage/LL9AKFZW/2010.html}
}

@article{hendrycksBaselineDetectingMisclassified2018,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2018},
  month = oct,
  journal = {arXiv:1610.02136 [cs]},
  eprint = {1610.02136},
  primaryclass = {cs},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/tillb/Zotero/storage/YTXV8MFJ/Hendrycks and Gimpel - 2018 - A Baseline for Detecting Misclassified and Out-of-.pdf;/Users/tillb/Zotero/storage/EMKPTH9T/1610.html}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {CVPR},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  pages = {4700--4708},
  file = {/Users/tillb/Zotero/storage/NLM8LKPV/Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf}
}

@misc{idrissiImageNetXUnderstandingModel2022,
  title = {{{ImageNet-X}}: {{Understanding Model Mistakes}} with {{Factor}} of {{Variation Annotations}}},
  shorttitle = {{{ImageNet-X}}},
  author = {Idrissi, Badr Youbi and Bouchacourt, Diane and Balestriero, Randall and Evtimov, Ivan and Hazirbas, Caner and et al.},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01866},
  eprint = {arXiv:2211.01866},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01866},
  abstract = {Deep learning vision systems are widely deployed across applications where reliability is critical. However, even today's best models can fail to recognize an object when its pose, lighting, or background varies. While existing benchmarks surface examples challenging for models, they do not explain why such mistakes arise. To address this need, we introduce ImageNet-X, a set of sixteen human annotations of factors such as pose, background, or lighting the entire ImageNet-1k validation set as well as a random subset of 12k training images. Equipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model's (1) architecture, e.g. transformer vs. convolutional, (2) learning paradigm, e.g. supervised vs. self-supervised, and (3) training procedures, e.g., data augmentation. Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories. We also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors. For example, strong random cropping hurts robustness on smaller objects. Together, these insights suggest to advance the robustness of modern vision models, future research should focus on collecting additional data and understanding data augmentation schemes. Along with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes image recognition systems make.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/tillb/Zotero/storage/96BVTWAC/Idrissi et al. - 2022 - ImageNet-X Understanding Model Mistakes with Fact.pdf;/Users/tillb/Zotero/storage/6GK6B3L9/2211.html}
}

@misc{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and et al.},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07031},
  eprint = {arXiv:1901.07031},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.07031},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/tillb/Zotero/storage/EEJW69D8/Irvin et al. - 2019 - CheXpert A Large Chest Radiograph Dataset with Un.pdf;/Users/tillb/Zotero/storage/2KJRRR87/1901.html}
}

@inproceedings{jaegerCallReflectEvaluation2022a,
    title={A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification},
    author={Paul F Jaeger and Carsten Tim L{\"u}th and Lukas Klein and Till J. Bungert},
    booktitle={International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=YnkGMIh0gvX}
}

@misc{johnsonalistairMIMICIIIClinicalDatabase2015,
  title = {{{MIMIC-III Clinical Database}}},
  author = {Johnson, Alistair and Pollard, Tom and Mark, Roger},
  year = {2015},
  publisher = {{PhysioNet}},
  doi = {10.13026/C2XW26},
  abstract = {MIMIC-III is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. The database includes information such as demographics, vital sign measurements made at the bedside (\textasciitilde 1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (including post-hospital discharge). MIMIC supports a diverse range of analytic studies spanning epidemiology, clinical decision-rule improvement, and electronic tool development. It is notable for three factors: it is freely available to researchers worldwide; it encompasses a diverse and very large population of ICU patients; and it contains highly granular data, including vital signs, laboratory results, and medications.}
}

@article{johnsonMIMICIIIFreelyAccessible2016,
  title = {{{MIMIC-III}}, a Freely Accessible Critical Care Database},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and et al.},
  year = {2016},
  month = may,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160035},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.35},
  abstract = {MIMIC-III (`Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Health care,Medical research,Outcomes research,Prognosis},
  file = {/Users/tillb/Zotero/storage/ETWHXV5L/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care datab.pdf}
}

@article{kawaharaSevenPointChecklistSkin2019,
  title = {Seven-{{Point Checklist}} and {{Skin Lesion Classification Using Multitask Multimodal Neural Nets}}},
  author = {Kawahara, Jeremy and Daneshvar, Sara and Argenziano, Giuseppe and Hamarneh, Ghassan},
  year = {2019},
  month = mar,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {23},
  number = {2},
  pages = {538--546},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2018.2824327},
  abstract = {We propose a multitask deep convolutional neural network, trained on multimodal data (clinical and dermoscopic images, and patient metadata), to classify the 7-point melanoma checklist criteria and perform skin lesion diagnosis. Our neural network is trained using several multitask loss functions, where each loss considers different combinations of the input modalities, which allows our model to be robust to missing data at inference time. Our final model classifies the 7-point checklist and skin condition diagnosis, produces multimodal feature vectors suitable for image retrieval, and localizes clinically discriminant regions. We benchmark our approach using 1011 lesion cases, and report comprehensive results over all 7-point criteria and diagnosis. We also make our dataset (images and metadata) publicly available online at http://derm.cs.sfu.ca.},
  keywords = {7-point checklist,Classification,convolutional neural networks,Convolutional neural networks,deep learning,dermatology,Feature extraction,Lesions,Malignant tumors,melanoma,Pattern analysis,skin,Skin},
  file = {/Users/tillb/Zotero/storage/TSIE4XYD/8333693.html}
}

@article{kendallWhatUncertaintiesWe2017,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  author = {Kendall, Alex and Gal, Yarin},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.04977 [cs]},
  eprint = {1703.04977},
  primaryclass = {cs},
  abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/tillb/Zotero/storage/FJV8YQLS/Kendall and Gal - 2017 - What Uncertainties Do We Need in Bayesian Deep Lea.pdf;/Users/tillb/Zotero/storage/QWAHI47K/1703.html}
}

@misc{kohWILDSBenchmarkIntheWild2021,
  title = {{{WILDS}}: {{A Benchmark}} of in-the-{{Wild Distribution Shifts}}},
  shorttitle = {{{WILDS}}},
  author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and et al.},
  year = {2021},
  month = jul,
  number = {arXiv:2012.07421},
  eprint = {arXiv:2012.07421},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.07421},
  abstract = {Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/tillb/Zotero/storage/XBCPMU37/Koh et al. - 2021 - WILDS A Benchmark of in-the-Wild Distribution Shi.pdf;/Users/tillb/Zotero/storage/D8DMY5D3/2012.html}
}

@inproceedings{leeSimpleUnifiedFramework2018,
  title = {A {{Simple Unified Framework}} for {{Detecting Out-of-Distribution Samples}} and {{Adversarial Attacks}}},
  booktitle = {NeurIPS},
  author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/tillb/Zotero/storage/RW7VT4YV/Lee et al. - 2018 - A Simple Unified Framework for Detecting Out-of-Di.pdf}
}

@article{liangEnhancingReliabilityOutofdistribution2020,
  title = {Enhancing {{The Reliability}} of {{Out-of-distribution Image Detection}} in {{Neural Networks}}},
  author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  year = {2020},
  month = aug,
  journal = {arXiv:1706.02690 [cs, stat]},
  eprint = {1706.02690},
  primaryclass = {cs, stat},
  abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7\% to 4.3\% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/tillb/Zotero/storage/R5E7TAIU/Liang et al. - 2020 - Enhancing The Reliability of Out-of-distribution I.pdf;/Users/tillb/Zotero/storage/6E9FN9UQ/1706.html}
}

@misc{liEstimatingModelPerformance2022,
  title = {Estimating {{Model Performance}} under {{Domain Shifts}} with {{Class-Specific Confidence Scores}}},
  author = {Li, Zeju and Kamnitsas, Konstantinos and Islam, Mobarakol and Chen, Chen and Glocker, Ben},
  year = {2022},
  month = jul,
  number = {arXiv:2207.09957},
  eprint = {arXiv:2207.09957},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.09957},
  abstract = {Machine learning models are typically deployed in a test setting that differs from the training setting, potentially leading to decreased model performance because of domain shift. If we could estimate the performance that a pre-trained model would achieve on data from a specific deployment setting, for example a certain clinic, we could judge whether the model could safely be deployed or if its performance degrades unacceptably on the specific data. Existing approaches estimate this based on the confidence of predictions made on unlabeled test data from the deployment's domain. We find existing methods struggle with data that present class imbalance, because the methods used to calibrate confidence do not account for bias induced by class imbalance, consequently failing to estimate class-wise accuracy. Here, we introduce class-wise calibration within the framework of performance estimation for imbalanced datasets. Specifically, we derive class-specific modifications of state-of-the-art confidence-based model evaluation methods including temperature scaling (TS), difference of confidences (DoC), and average thresholded confidence (ATC). We also extend the methods to estimate Dice similarity coefficient (DSC) in image segmentation. We conduct experiments on four tasks and find the proposed modifications consistently improve the estimation accuracy for imbalanced datasets. Our methods improve accuracy estimation by 18\textbackslash\% in classification under natural domain shifts, and double the estimation accuracy on segmentation tasks, when compared with prior methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/tillb/Zotero/storage/D79V4EIV/Li et al. - 2022 - Estimating Model Performance under Domain Shifts w.pdf;/Users/tillb/Zotero/storage/S8IAT6M9/2207.html}
}

@inproceedings{liuDeepGamblersLearning2019,
  title = {Deep {{Gamblers}}: {{Learning}} to {{Abstain}} with {{Portfolio Theory}}},
  shorttitle = {Deep {{Gamblers}}},
  booktitle = {NeurIPS},
  author = {Liu, Ziyin and Wang, Zhikang and Liang, Paul Pu and Salakhutdinov, Russ R and Morency, Louis-Philippe and et al.},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/tillb/Zotero/storage/UADW3TG6/Liu et al. - 2019 - Deep Gamblers Learning to Abstain with Portfolio .pdf}
}

@inproceedings{malininPredictiveUncertaintyEstimation2018,
  title = {Predictive {{Uncertainty Estimation}} via {{Prior Networks}}},
  booktitle = {NeurIPS},
  author = {Malinin, Andrey and Gales, Mark},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible \textbackslash emph\{data uncertainty\} and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through \textbackslash emph\{model uncertainty\} or as \textbackslash emph\{data uncertainty\}. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models \textbackslash emph\{distributional uncertainty\}. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST and CIFAR-10 datasets, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.},
  file = {/Users/tillb/Zotero/storage/YISF9WVY/Malinin and Gales - 2018 - Predictive Uncertainty Estimation via Prior Networ.pdf}
}

@inproceedings{mehtaOutofDistributionDetectionLongTailed2022,
  title = {Out-of-{{Distribution Detection}} for~{{Long-Tailed}} and~{{Fine-Grained Skin Lesion Images}}},
  booktitle = {MICCAI},
  author = {Mehta, Deval and Gal, Yaniv and Bowling, Adrian and Bonnington, Paul and Ge, Zongyuan},
  editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {732--742},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-16431-6_69},
  abstract = {Recent years have witnessed a rapid development of automated methods for skin lesion diagnosis and classification. Due to an increasing deployment of such systems in clinics, it has become important to develop a more robust system towards various Out-of-Distribution (OOD) samples (unknown skin lesions and conditions). However, the current deep learning models trained for skin lesion classification tend to classify these OOD samples incorrectly into one of their learned skin lesion categories. To address this issue, we propose a simple yet strategic approach that improves the OOD detection performance while maintaining the multi-class classification accuracy for the known categories of skin lesion. To specify, this approach is built upon a realistic scenario of a long-tailed and fine-grained OOD detection task for skin lesion images. Through this approach, 1) First, we target the mixup amongst middle and tail classes to address the long-tail problem. 2) Later, we combine the above mixup strategy with prototype learning to address the fine-grained nature of the dataset. The unique contribution of this paper is two-fold, justified by extensive experiments. First, we present a realistic problem setting of OOD task for skin lesion. Second, we propose an approach to target the long-tailed and fine-grained aspects of the problem setting simultaneously to increase the OOD performance.},
  isbn = {978-3-031-16431-6},
  langid = {english},
  keywords = {Mixup,Openset,Out-of-distribution,Prototype,Skin lesion},
  file = {/Users/tillb/Zotero/storage/LWZDJZDQ/Mehta et al. - 2022 - Out-of-Distribution Detection for Long-Tailed and .pdf}
}

@inproceedings{mendoncaPH2DermoscopicImage2013,
  title = {{{PH2}} - {{A}} Dermoscopic Image Database for Research and Benchmarking},
  booktitle = {2013 35th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Mendon{\c c}a, Teresa and Ferreira, Pedro M. and Marques, Jorge S. and Marcal, Andr{\'e} R. S. and Rozeira, Jorge},
  year = {2013},
  month = jul,
  pages = {5437--5440},
  issn = {1558-4615},
  doi = {10.1109/EMBC.2013.6610779},
  abstract = {The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identification of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.},
  keywords = {Databases,Image color analysis,Image segmentation,Lesions,Malignant tumors,Manuals,Skin},
  file = {/Users/tillb/Zotero/storage/NVL9JGFL/6610779.html}
}

@article{oktayEvaluationDeepLearning2020,
  title = {Evaluation of {{Deep Learning}} to {{Augment Image-Guided Radiotherapy}} for {{Head}} and {{Neck}} and {{Prostate Cancers}}},
  author = {Oktay, Ozan and Nanavati, Jay and Schwaighofer, Anton and Carter, David and Bristow, Melissa and et al.},
  year = {2020},
  month = nov,
  journal = {JAMA network open},
  volume = {3},
  number = {11},
  pages = {e2027426},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2020.27426},
  abstract = {IMPORTANCE: Personalized radiotherapy planning depends on high-quality delineation of target tumors and surrounding organs at risk (OARs). This process puts additional time burdens on oncologists and introduces variability among both experts and institutions. OBJECTIVE: To explore clinically acceptable autocontouring solutions that can be integrated into existing workflows and used in different domains of radiotherapy. DESIGN, SETTING, AND PARTICIPANTS: This quality improvement study used a multicenter imaging data set comprising 519 pelvic and 242 head and neck computed tomography (CT) scans from 8 distinct clinical sites and patients diagnosed either with prostate or head and neck cancer. The scans were acquired as part of treatment dose planning from patients who received intensity-modulated radiation therapy between October 2013 and February 2020. Fifteen different OARs were manually annotated by expert readers and radiation oncologists. The models were trained on a subset of the data set to automatically delineate OARs and evaluated on both internal and external data sets. Data analysis was conducted October 2019 to September 2020. MAIN OUTCOMES AND MEASURES: The autocontouring solution was evaluated on external data sets, and its accuracy was quantified with volumetric agreement and surface distance measures. Models were benchmarked against expert annotations in an interobserver variability (IOV) study. Clinical utility was evaluated by measuring time spent on manual corrections and annotations from scratch. RESULTS: A total of 519 participants' (519 [100\%] men; 390 [75\%] aged 62-75 years) pelvic CT images and 242 participants' (184 [76\%] men; 194 [80\%] aged 50-73 years) head and neck CT images were included. The models achieved levels of clinical accuracy within the bounds of expert IOV for 13 of 15 structures (eg, left femur, {$\kappa$}\,=\,0.982; brainstem, {$\kappa$}\,=\,0.806) and performed consistently well across both external and internal data sets (eg, mean [SD] Dice score for left femur, internal vs external data sets: 98.52\% [0.50] vs 98.04\% [1.02]; P\,=\,.04). The correction time of autogenerated contours on 10 head and neck and 10 prostate scans was measured as a mean of 4.98 (95\% CI, 4.44-5.52) min/scan and 3.40 (95\% CI, 1.60-5.20) min/scan, respectively, to ensure clinically accepted accuracy. Manual segmentation of the head and neck took a mean 86.75 (95\% CI, 75.21-92.29) min/scan for an expert reader and 73.25 (95\% CI, 68.68-77.82) min/scan for a radiation oncologist. The autogenerated contours represented a 93\% reduction in time. CONCLUSIONS AND RELEVANCE: In this study, the models achieved levels of clinical accuracy within expert IOV while reducing manual contouring time and performing consistently well across previously unseen heterogeneous data sets. With the availability of open-source libraries and reliable performance, this creates significant opportunities for the transformation of radiation treatment planning.},
  langid = {english},
  pmcid = {PMC7705593},
  pmid = {33252691},
  keywords = {Aged,Deep Learning,Head and Neck Neoplasms,Humans,Male,Middle Aged,Neural Networks; Computer,Observer Variation,Organs at Risk,Prostatic Neoplasms,Quality Improvement,Radiotherapy; Image-Guided,Radiotherapy; Intensity-Modulated,Reproducibility of Results,Tomography; X-Ray Computed},
  file = {/Users/tillb/Zotero/storage/WZR2X3CS/Oktay et al. - 2020 - Evaluation of Deep Learning to Augment Image-Guide.pdf}
}

@inproceedings{ovadiaCanYouTrust2019,
  title = {Can You Trust Your Model' s Uncertainty? {{Evaluating}} Predictive Uncertainty under Dataset Shift},
  shorttitle = {Can You Trust Your Model' s Uncertainty?},
  booktitle = {NeurIPS},
  author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and et al.},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/tillb/Zotero/storage/PJZZU5CY/Ovadia et al. - 2019 - Can you trust your model' s uncertainty Evaluatin.pdf}
}

@inproceedings{paschaliGeneralizabilityVsRobustness2018,
  title = {Generalizability vs. {{Robustness}}: {{Investigating Medical Imaging Networks Using Adversarial Examples}}},
  shorttitle = {Generalizability vs. {{Robustness}}},
  booktitle = {MICCAI},
  author = {Paschali, Magdalini and Conjeti, Sailesh and Navarro, Fernando and Navab, Nassir},
  editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and {Alberola-L{\'o}pez}, Carlos and Fichtinger, Gabor},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {493--501},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-00928-1_56},
  abstract = {In this paper, for the first time, we propose an evaluation method for deep learning models that assesses the performance of a model not only in an unseen test scenario, but also in extreme cases of noise, outliers and ambiguous input data. To this end, we utilize adversarial examples, images that fool machine learning models, while looking imperceptibly different from original data, as a measure to evaluate the robustness of a variety of medical imaging models. Through extensive experiments on skin lesion classification and whole brain segmentation with state-of-the-art networks such as Inception and UNet, we show that models that achieve comparable performance regarding generalizability may have significant variations in their perception of the underlying data manifold, leading to an extensive performance gap in their robustness.},
  isbn = {978-3-030-00928-1},
  langid = {english}
}

@misc{paschaliGeneralizabilityVsRobustness2018a,
  title = {Generalizability vs. {{Robustness}}: {{Adversarial Examples}} for {{Medical Imaging}}},
  shorttitle = {Generalizability vs. {{Robustness}}},
  author = {Paschali, Magdalini and Conjeti, Sailesh and Navarro, Fernando and Navab, Nassir},
  year = {2018},
  month = mar,
  number = {arXiv:1804.00504},
  eprint = {arXiv:1804.00504},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1804.00504},
  abstract = {In this paper, for the first time, we propose an evaluation method for deep learning models that assesses the performance of a model not only in an unseen test scenario, but also in extreme cases of noise, outliers and ambiguous input data. To this end, we utilize adversarial examples, images that fool machine learning models, while looking imperceptibly different from original data, as a measure to evaluate the robustness of a variety of medical imaging models. Through extensive experiments on skin lesion classification and whole brain segmentation with state-of-the-art networks such as Inception and UNet, we show that models that achieve comparable performance regarding generalizability may have significant variations in their perception of the underlying data manifold, leading to an extensive performance gap in their robustness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/tillb/Zotero/storage/LHVXDM7V/Paschali et al. - 2018 - Generalizability vs. Robustness Adversarial Examp.pdf;/Users/tillb/Zotero/storage/V3KBZPPT/1804.html}
}

@article{rotembergPatientcentricDatasetImages2021,
  title = {A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context},
  author = {Rotemberg, Veronica and Kurtansky, Nicholas and {Betz-Stablein}, Brigid and Caffery, Liam and Chousakos, Emmanouil and et al.},
  year = {2021},
  month = jan,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {34},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00815-z},
  abstract = {Abstract             Prior skin image datasets have not addressed patient-level information obtained from multiple skin lesions from the same patient. Though artificial intelligence classification algorithms have achieved expert-level performance in controlled studies examining single images, in practice dermatologists base their judgment holistically from multiple lesions on the same patient. The 2020 SIIM-ISIC Melanoma Classification challenge dataset described herein was constructed to address this discrepancy between prior challenges and clinical practice, providing for each image in the dataset an identifier allowing lesions from the same patient to be mapped to one another. This patient-level contextual information is frequently used by clinicians to diagnose melanoma and is especially useful in ruling out false positives in patients with many atypical nevi. The dataset represents 2,056 patients (20.8\% with at least one melanoma, 79.2\% with zero melanomas) from three continents with an average of 16 lesions per patient, consisting of 33,126 dermoscopic images and 584 (1.8\%) histopathologically confirmed melanomas compared with benign melanoma mimickers.},
  langid = {english},
  file = {/Users/tillb/Zotero/storage/87IBSAQX/Rotemberg et al. - 2021 - A patient-centric dataset of images and metadata f.pdf}
}

@misc{RxRx1,
  title = {{{RxRx1}}},
  abstract = {Disentangling biological signal from experimental noise in cellular images.},
  howpublished = {https://www.rxrx.ai/rxrx1},
  file = {/Users/tillb/Zotero/storage/Q8UPWMHG/rxrx1.html}
}

@inproceedings{santurkarBREEDSBenchmarksSubpopulation2022,
  title = {{{BREEDS}}: {{Benchmarks}} for {{Subpopulation Shift}}},
  shorttitle = {{{BREEDS}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Madry, Aleksander},
  year = {2022},
  month = feb,
  abstract = {We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of existing train-time robustness interventions.},
  langid = {english},
  file = {/Users/tillb/Zotero/storage/G3VZLN9U/Santurkar et al. - 2022 - BREEDS Benchmarks for Subpopulation Shift.pdf}
}

@inproceedings{scalbertTestTimeImagetoImageTranslation2022,
  title = {Test-{{Time Image-to-Image Translation Ensembling Improves Out-of-Distribution Generalization}} in~{{Histopathology}}},
  booktitle = {MICCAI},
  author = {Scalbert, Marin and Vakalopoulou, Maria and {Couzini{\'e}-Devy}, Florent},
  editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {120--129},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-16434-7_12},
  abstract = {Histopathology whole slide images (WSIs) can reveal significant inter-hospital variability such as illumination, color or optical artifacts. These variations, caused by the use of different protocols across medical centers (staining, scanner), can strongly harm algorithms generalization on unseen protocols. This motivates the development of new methods to limit such loss of generalization. In this paper, to enhance robustness on unseen target protocols, we propose a new test-time data augmentation based on multi domain image-to-image translation. It allows to project images from unseen protocol into each source domain before classifying them and ensembling the predictions. This test-time augmentation method results in a significant boost of performances for domain generalization. To demonstrate its effectiveness, our method has been evaluated on two different histopathology tasks where it outperforms conventional domain generalization, standard/H \&E specific color augmentation/normalization and standard test-time augmentation techniques. Our code is publicly available at ~https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.},
  isbn = {978-3-031-16434-7},
  langid = {english},
  keywords = {Domain generalization,Generative adversarial networks,Image-to-image translation,Test-time data augmentation},
  file = {/Users/tillb/Zotero/storage/Q9FHCQHZ/Scalbert et al. - 2022 - Test-Time Image-to-Image Translation Ensembling Im.pdf}
}

@inproceedings{schneiderImprovingRobustnessCommon2020,
  title = {Improving Robustness against Common Corruptions by Covariate Shift Adaptation},
  booktitle = {NeurIPS},
  author = {Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and et al.},
  year = {2020},
  volume = {33},
  pages = {11539--11551},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Today's state-of-the-art machine vision models are vulnerable to image corruptions like blurring or compression artefacts, limiting their performance in many real-world applications. We here argue that popular benchmarks to measure model robustness against common corruptions (like ImageNet-C) underestimate model robustness in many (but not all) application scenarios. The key insight is that in many scenarios, multiple unlabeled examples of the corruptions are available and can be used for unsupervised online adaptation. Replacing the activation statistics estimated by batch normalization on the training set with the statistics of the corrupted images consistently improves the robustness across 25 different popular computer vision models. Using the corrected statistics, ResNet-50 reaches 62.2\% mCE on ImageNet-C compared to 76.7\% without adaptation. With the more robust DeepAugment+AugMix model, we improve the state of the art achieved by a ResNet50 model up to date from 53.6\% mCE to 45.4\% mCE. Even adapting to a single sample improves robustness for the ResNet-50 and AugMix models, and 32 samples are sufficient to improve the current state of the art for a ResNet-50 architecture. We argue that results with adapted statistics should be included whenever reporting scores in corruption benchmarks and other out-of-distribution generalization settings.},
  file = {/Users/tillb/Zotero/storage/ECWK79UX/Schneider et al. - 2020 - Improving robustness against common corruptions by.pdf}
}

@misc{SelfsupervisedLearningBased,
  title = {A {{Self-supervised Learning Based Framework}} for {{Automatic Heart Failure Classification}} on {{Cine Cardiac Magnetic Resonance Image}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  howpublished = {https://ieeexplore.ieee.org/document/9630228},
  file = {/Users/tillb/Zotero/storage/KVB8GZN8/9630228.html}
}

@article{shamshirbandReviewDeepLearning2021,
  title = {A Review on Deep Learning Approaches in Healthcare Systems: {{Taxonomies}}, Challenges, and Open Issues},
  shorttitle = {A Review on Deep Learning Approaches in Healthcare Systems},
  author = {Shamshirband, Shahab and Fathi, Mahdis and Dehzangi, Abdollah and Chronopoulos, Anthony Theodore and {Alinejad-Rokny}, Hamid},
  year = {2021},
  month = jan,
  journal = {Journal of Biomedical Informatics},
  volume = {113},
  pages = {103627},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2020.103627},
  abstract = {In the last few years, the application of Machine Learning approaches like Deep Neural Network (DNN) models have become more attractive in the healthcare system given the rising complexity of the healthcare data. Machine Learning (ML) algorithms provide efficient and effective data analysis models to uncover hidden patterns and other meaningful information from the considerable amount of health data that conventional analytics are not able to discover in a reasonable time. In particular, Deep Learning (DL) techniques have been shown as promising methods in pattern recognition in the healthcare systems. Motivated by this consideration, the contribution of this paper is to investigate the deep learning approaches applied to healthcare systems by reviewing the cutting-edge network architectures, applications, and industrial trends. The goal is first to provide extensive insight into the application of deep learning models in healthcare solutions to bridge deep learning techniques and human healthcare interpretability. And then, to present the existing open challenges and future directions.},
  langid = {english},
  keywords = {Deep neural network,Diagnostics tools,Health data analytics,Healthcare applications,Machine learning},
  file = {/Users/tillb/Zotero/storage/GWRTNRVF/Shamshirband et al. - 2021 - A review on deep learning approaches in healthcare.pdf;/Users/tillb/Zotero/storage/44GZJXER/S1532046420302550.html}
}

@inproceedings{sohoniNoSubclassLeft2020,
  title = {No {{Subclass Left Behind}}: {{Fine-Grained Robustness}} in {{Coarse-Grained Classification Problems}}},
  shorttitle = {No {{Subclass Left Behind}}},
  booktitle = {NeurIPS},
  author = {Sohoni, Nimit and Dunnmon, Jared and Angus, Geoffrey and Gu, Albert and R{\'e}, Christopher},
  year = {2020},
  volume = {33},
  pages = {19339--19352},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In real-world classification tasks, each class often comprises multiple finer-grained "subclasses." As the subclass labels are frequently unavailable, models trained using only the coarser-grained class labels often exhibit highly variable performance across different subclasses. This phenomenon, known as hidden stratification, has important consequences for models deployed in safety-critical applications such as medicine. We propose GEORGE, a method to both measure and mitigate hidden stratification even when subclass labels are unknown. We first observe that unlabeled subclasses are often separable in the feature space of deep models, and exploit this fact to estimate subclass labels for the training data via clustering techniques. We then use these approximate subclass labels as a form of noisy supervision in a distributionally robust optimization objective. We theoretically characterize the performance of GEORGE in terms of the worst-case generalization error across any subclass. We empirically validate GEORGE on a mix of real-world and benchmark image classification datasets, and show that our approach boosts worst-case subclass accuracy by up to 15 percentage points compared to standard training techniques, without requiring any information about the subclasses.},
  file = {/Users/tillb/Zotero/storage/Z54GW7BZ/Sohoni et al. - 2020 - No Subclass Left Behind Fine-Grained Robustness i.pdf}
}

@misc{sypetkowskiRxRx1DatasetEvaluating2023,
  title = {{{RxRx1}}: {{A Dataset}} for {{Evaluating Experimental Batch Correction Methods}}},
  shorttitle = {{{RxRx1}}},
  author = {Sypetkowski, Maciej and Rezanejad, Morteza and Saberian, Saber and Kraus, Oren and Urbanik, John and et al.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.05768},
  eprint = {arXiv:2301.05768},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.05768},
  abstract = {High-throughput screening techniques are commonly used to obtain large quantities of data in many fields of biology. It is well known that artifacts arising from variability in the technical execution of different experimental batches within such screens confound these observations and can lead to invalid biological conclusions. It is therefore necessary to account for these batch effects when analyzing outcomes. In this paper we describe RxRx1, a biological dataset designed specifically for the systematic study of batch effect correction methods. The dataset consists of 125,510 high-resolution fluorescence microscopy images of human cells under 1,138 genetic perturbations in 51 experimental batches across 4 cell types. Visual inspection of the images alone clearly demonstrates significant batch effects. We propose a classification task designed to evaluate the effectiveness of experimental batch correction methods on these images and examine the performance of a number of correction methods on this task. Our goal in releasing RxRx1 is to encourage the development of effective experimental batch correction methods that generalize well to unseen experimental batches. The dataset can be downloaded at https://rxrx.ai.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/tillb/Zotero/storage/84WNWMUY/Sypetkowski et al. - 2023 - RxRx1 A Dataset for Evaluating Experimental Batch.pdf;/Users/tillb/Zotero/storage/UCE4B655/2301.html}
}

@inproceedings{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  booktitle = {ICML},
  author = {Tan, Mingxing and Le, Quoc},
  year = {2019},
  month = may,
  pages = {6105--6114},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  langid = {english},
  file = {/Users/tillb/Zotero/storage/NCAQFTX6/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf}
}

@inproceedings{tomczakWhatCanWe2022,
  title = {What {{Can We Learn About}} a {{Generated Image Corrupting Its Latent Representation}}?},
  booktitle = {MICCAI},
  author = {Tomczak, Agnieszka and Gupta, Aarushi and Ilic, Slobodan and Navab, Nassir and Albarqouni, Shadi},
  editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {505--515},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-16446-0_48},
  abstract = {Generative adversarial networks (GANs) offer an effective solution to the image-to-image translation problem, thereby allowing for new possibilities in medical imaging. They can translate images from one imaging modality to another at a low cost. For unpaired datasets, they rely mostly on cycle loss. Despite its effectiveness in learning the underlying data distribution, it can lead to a discrepancy between input and output data. The purpose of this work is to investigate the hypothesis that we can predict image quality based on its latent representation in the GANs bottleneck. We achieve this by corrupting the latent representation with noise and generating multiple outputs. The degree of differences between them is interpreted as the strength of the representation: the more robust the latent representation, the fewer changes in the output image the corruption causes. Our results demonstrate that our proposed method has the ability to i) predict uncertain parts of synthesized images, and ii) identify samples that may not be reliable for downstream tasks, e.g., liver segmentation task.},
  isbn = {978-3-031-16446-0},
  langid = {english},
  keywords = {GANs,Image quality,Image synthesis,Uncertainty},
  file = {/Users/tillb/Zotero/storage/4TGRM3UP/Tomczak et al. - 2022 - What Can We Learn About a Generated Image Corrupti.pdf}
}

@article{tschandlHAM10000DatasetLarge2018,
  title = {The {{HAM10000}} Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions},
  author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
  year = {2018},
  month = aug,
  journal = {Scientific Data},
  volume = {5},
  number = {1},
  pages = {180161},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2018.161},
  abstract = {Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (``Human Against Machine with 10000 training images'') dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50\% of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Basal cell carcinoma,Cancer imaging,Cancer screening,Melanoma,Squamous cell carcinoma},
  file = {/Users/tillb/Zotero/storage/LQ5D7FMN/Tschandl et al. - 2018 - The HAM10000 dataset, a large collection of multi-.pdf}
}

@article{tschandlHumanComputerCollaboration2020,
  title = {Human\textendash Computer Collaboration for Skin Cancer Recognition},
  author = {Tschandl, Philipp and Rinner, Christoph and Apalla, Zoe and Argenziano, Giuseppe and Codella, Noel and et al.},
  year = {2020},
  month = aug,
  journal = {Nature Medicine},
  volume = {26},
  number = {8},
  pages = {1229--1234},
  publisher = {{Nature Publishing Group}},
  issn = {1546-170X},
  doi = {10.1038/s41591-020-0942-0},
  abstract = {The rapid increase in telemedicine coupled with recent advances in diagnostic artificial intelligence (AI) create the imperative to consider the opportunities and risks of inserting AI-based support into new paradigms of care. Here we build on recent achievements in the accuracy of image-based AI for skin cancer diagnosis to address the effects of varied representations of AI-based support across different levels of clinical expertise and multiple clinical workflows. We find that good quality AI-based support of clinical decision-making improves diagnostic accuracy over that of either AI or physicians alone, and that the least experienced clinicians gain the most from AI-based support. We further find that AI-based multiclass probabilities outperformed content-based image retrieval (CBIR) representations of AI in the mobile technology environment, and AI-based support had utility in simulations of second opinions and of telemedicine triage. In addition to demonstrating the potential benefits associated with good quality AI in the hands of non-expert clinicians, we find that faulty AI can mislead the entire spectrum of clinicians, including experts. Lastly, we show that insights derived from AI class-activation maps can inform improvements in human diagnosis. Together, our approach and findings offer a framework for future studies across the spectrum of image-based diagnostics to improve human\textendash computer collaboration in clinical practice.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Diagnosis,Skin cancer},
  file = {/Users/tillb/Zotero/storage/DHTU4NH8/Tschandl et al. - 2020 - Human–computer collaboration for skin cancer recog.pdf;/Users/tillb/Zotero/storage/B5GLWP6Z/s41591-020-0942-0.html}
}

@inproceedings{wangChestXRay8HospitalScaleChest2017,
  title = {{{ChestX-Ray8}}: {{Hospital-Scale Chest X-Ray Database}} and {{Benchmarks}} on {{Weakly-Supervised Classification}} and {{Localization}} of {{Common Thorax Diseases}}},
  shorttitle = {{{ChestX-Ray8}}},
  booktitle = {CVPR},
  author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and et al.},
  year = {2017},
  month = jul,
  pages = {3462--3471},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.369},
  abstract = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.},
  keywords = {Biomedical imaging,Databases,Diseases,Image segmentation,Machine learning,Pathology,X-ray imaging},
  file = {/Users/tillb/Zotero/storage/IERPC8LS/Wang et al. - 2017 - ChestX-Ray8 Hospital-Scale Chest X-Ray Database a.pdf;/Users/tillb/Zotero/storage/37S9QKV8/8099852.html}
}

@article{winkensContrastiveTrainingImproved2020,
  title = {Contrastive {{Training}} for {{Improved Out-of-Distribution Detection}}},
  author = {Winkens, Jim and Bunel, Rudy and Roy, Abhijit Guha and Stanforth, Robert and Natarajan, Vivek and et al.},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.05566 [cs, stat]},
  eprint = {2007.05566},
  primaryclass = {cs, stat},
  abstract = {Reliable detection of out-of-distribution (OOD) inputs is increasingly understood to be a precondition for deployment of machine learning systems. This paper proposes and investigates the use of contrastive training to boost OOD detection performance. Unlike leading methods for OOD detection, our approach does not require access to examples labeled explicitly as OOD, which can be difficult to collect in practice. We show in extensive experiments that contrastive training significantly helps OOD detection performance on a number of common benchmarks. By introducing and employing the Confusion Log Probability (CLP) score, which quantifies the difficulty of the OOD detection task by capturing the similarity of inlier and outlier datasets, we show that our method especially improves performance in the `near OOD' classes -- a particularly challenging setting for previous methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/tillb/Zotero/storage/QPSYMCX9/Winkens et al. - 2020 - Contrastive Training for Improved Out-of-Distribut.pdf;/Users/tillb/Zotero/storage/4Y58LWCM/2007.html}
}

@misc{xiaAugmentingSoftmaxInformation2022,
  title = {Augmenting {{Softmax Information}} for {{Selective Classification}} with {{Out-of-Distribution Data}}},
  author = {Xia, Guoxuan and Bouganis, Christos-Savvas},
  year = {2022},
  month = oct,
  number = {arXiv:2207.07506},
  eprint = {arXiv:2207.07506},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.07506},
  abstract = {Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Combination (SIRC), that augments softmax-based confidence scores with feature-agnostic information such that their ability to identify OOD samples is improved without sacrificing separation between correct and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale datasets and convolutional neural network architectures show that SIRC is able to consistently match or outperform the baseline for SCOD, whilst existing OOD detection methods fail to do so.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/tillb/Zotero/storage/SJLJ4N7L/Xia and Bouganis - 2022 - Augmenting Softmax Information for Selective Class.pdf;/Users/tillb/Zotero/storage/4RGRPJ22/2207.html}
}

@article{xuEvaluatingRobustnessDeep2021,
  title = {Towards Evaluating the Robustness of Deep Diagnostic Models by Adversarial Attack},
  author = {Xu, Mengting and Zhang, Tao and Li, Zhongnian and Liu, Mingxia and Zhang, Daoqiang},
  year = {2021},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {69},
  pages = {101977},
  issn = {1361-8415},
  doi = {10.1016/j.media.2021.101977},
  abstract = {Deep learning models (with neural networks) have been widely used in challenging tasks such as computer-aided disease diagnosis based on medical images. Recent studies have shown deep diagnostic models may not be robust in the inference process and may pose severe security concerns in clinical practice. Among all the factors that make the model not robust, the most serious one is adversarial examples. The so-called ``adversarial example'' is a well-designed perturbation that is not easily perceived by humans but results in a false output of deep diagnostic models with high confidence. In this paper, we evaluate the robustness of deep diagnostic models by adversarial attack. Specifically, we have performed two types of adversarial attacks to three deep diagnostic models in both single-label and multi-label classification tasks, and found that these models are not reliable when attacked by adversarial example. We have further explored how adversarial examples attack the models, by analyzing their quantitative classification results, intermediate features, discriminability of features and correlation of estimated labels for both original/clean images and those adversarial ones. We have also designed two new defense methods to handle adversarial examples in deep diagnostic models, i.e., Multi-Perturbations Adversarial Training (MPAdvT) and Misclassification-Aware Adversarial Training (MAAdvT). The experimental results have shown that the use of defense methods can significantly improve the robustness of deep diagnostic models against adversarial attacks.},
  langid = {english},
  keywords = {Adversarial attack,Deep diagnostic models,Defense,Robustness},
  file = {/Users/tillb/Zotero/storage/RNGYLXKH/Xu et al. - 2021 - Towards evaluating the robustness of deep diagnost.pdf;/Users/tillb/Zotero/storage/V6HX8YIC/S1361841521000232.html}
}

@article{zechVariableGeneralizationPerformance2018,
  title = {Variable Generalization Performance of a Deep Learning Model to Detect Pneumonia in Chest Radiographs: {{A}} Cross-Sectional Study},
  shorttitle = {Variable Generalization Performance of a Deep Learning Model to Detect Pneumonia in Chest Radiographs},
  author = {Zech, John R. and Badgeley, Marcus A. and Liu, Manway and Costa, Anthony B. and Titano, Joseph J. and et al.},
  editor = {Sheikh, Aziz},
  year = {2018},
  month = nov,
  journal = {PLOS Medicine},
  volume = {15},
  number = {11},
  pages = {e1002683},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1002683},
  langid = {english},
  file = {/Users/tillb/Zotero/storage/9TLDII3N/Zech et al. - 2018 - Variable generalization performance of a deep lear.pdf}
}

@inproceedings{zhangBenchmarkingRobustnessDeep2022,
  title = {Benchmarking the~{{Robustness}} of~{{Deep Neural Networks}} to~{{Common Corruptions}} in~{{Digital Pathology}}},
  booktitle = {MICCAI},
  author = {Zhang, Yunlong and Sun, Yuxuan and Li, Honglin and Zheng, Sunyi and Zhu, Chenglu and et al.},
  editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {242--252},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-16434-7\_24},
  abstract = {When designing a diagnostic model for a clinical application, it is crucial to guarantee the robustness of the model with respect to a wide range of image corruptions. Herein, an easy-to-use benchmark is established to evaluate how deep neural networks perform on corrupted pathology images. Specifically, corrupted images are generated by injecting nine types of common corruptions into validation images. Besides, two classification and one ranking metrics are designed to evaluate the prediction and confidence performance under corruption. Evaluated on two resulting benchmark datasets, we find that (1) a variety of deep neural network models suffer from a significant accuracy decrease (double the error on clean images) and the unreliable confidence estimation on corrupted images; (2) A low correlation between the validation and test errors while replacing the validation set with our benchmark can increase the correlation. Our codes are available on https://github.com/superjamessyx/robustness\_benchmark.},
  isbn = {978-3-031-16434-7},
  langid = {english},
  keywords = {Benchmark,Corruption,Digital pathology,Robustness},
  file = {/Users/tillb/Zotero/storage/BV3SLVFX/Zhang et al. - 2022 - Benchmarking the Robustness of Deep Neural Network.pdf}
}

@inproceedings{zhongSelfsupervisedLearningBased2021,
  title = {A {{Self-supervised Learning Based Framework}} for {{Automatic Heart Failure Classification}} on {{Cine Cardiac Magnetic Resonance Image}}},
  booktitle = {2021 43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  author = {Zhong, Hai and Wu, Jiaqi and Zhao, Wangyuan and Xu, Xiaowei and Hou, Runping and et al.},
  year = {2021},
  month = nov,
  pages = {2887--2890},
  issn = {2694-0604},
  doi = {10.1109/EMBC46164.2021.9630228},
  abstract = {Heart failure (HF) is a serious syndrome, with high rates of mortality. Accurate classification of HF according to the left ventricular ejection faction (EF) plays an important role in the clinical treatment. Compared to echocardiography, cine cardiac magnetic resonance images (Cine-CMR) can estimate more accurate EF, whereas rare studies focus on the application of Cine-CMR. In this paper, a self-supervised learning framework for HF classification called SSLHF was proposed to automatically classify the HF patients into HF patients with preserved EF and HF patients with reduced EF based on Cine-CMR. In order to enable the classification network better learn the spatial and temporal information contained in the Cine-CMR, the SSLHF consists of two stages: self-supervised image restoration and HF classification. In the first stage, an image restoration proxy task was designed to help a U-Net like network mine the HF information in the spatial and temporal dimensions. In the second stage, a HF classification network whose weights were initialized by the encoder part of the U-Net like network was trained to complete the HF classification. Benefitting from the proxy task, the SSLHF achieved an AUC of 0.8505 and an ACC of 0.8208 in the 5-fold cross-validation.},
  keywords = {Biology,Echocardiography,Heart,Image restoration,Magnetic resonance,Medical services,Task analysis},
  file = {/Users/tillb/Zotero/storage/SWGMXDMG/9630228.html}
}
