% Figure environment removed

\section{Results}
\label{sec:results}

We evaluate the efficiency of \textit{The GANfather} to generate and detect attacks in two use-cases: money laundering (Section~\ref{sec:aml_exps}) and recommendation system (Section~\ref{sec:recs_exps}).



\subsection{Money Laundering}
\label{sec:aml_exps}

\textbf{Setup.}
We use a real-world dataset of financial transactions, containing approximately 200,000 transactions, between 100,000 unique accounts, over 10 months\footnote{Due to the confidential nature we cannot disclose the actual dataset.}.
Some of these accounts are labelled as suspicious of money laundering.
We build a real test set of $5000$ accounts, $184$ of which are label positive (suspicious).
We implement \textit{The GANfather}'s generator and discriminator following the architectures presented in Section~\ref{subsubsec:amlusecase}. 


\textbf{Results.}
We conduct a hyperparameter random search over the learning rate ($[10^{-4}, 3\times10^{-3}]$) and the weights $\alpha$ (set to $1$), $\beta$ ($[10^2, 10^5]$) and $\gamma$ ($[10^3, 4\times10^3]$) mentioned in Equation~\ref{eq:generator_loss}).

In Figure~\ref{fig:04_aml_comparison_distributions}, we compare the distribution of money flows from such a generator compared to the real data distribution.
We can observe that the generated samples successfully move more money through the accounts than real data (up to 350,000 dollars vs. up to 9,000 dollars respectively).
Interestingly, the distribution of amounts used is similar to real data, and the main difference is the number of transactions used.

Next, we test the detection performance of the trained discriminators on generated data.
To detect potential bias in a discriminator trained solely on samples of the corresponding generator, we first build a \emph{mixed} dataset, where synthetic malicious data is sampled from various generators.% at various epochs during training and with different random noise seeds.
We combine this synthetic dataset with real data, and use it to evaluate the trained discriminators.
Importantly, no retraining on this mixed dataset is performed.
We observe that most discriminators can distinguish between real and generated examples with $100\%$ accuracy, especially those trained with higher values of the $\beta$ hyperparameter (see Equation~\ref{eq:generator_loss}, and note that in this experiment $\alpha$ was fixed to a value of 1).
%This can be understood because the $\beta$ parameter limits the generated data distribution to diverge significantly from the real data distribution.
%Therefore, discriminators trained with larger $\beta$ need to more accurately learn a decision boundary around the real data, in turn becoming more robust when evaluating on the mixed dataset.

Then, we evaluate the detection performance on the real test set.
We train a model $DM$ with the same architecture as the discriminator using the mixed dataset mentioned in the previous paragraph.
This training \emph{does not require real labels}, since we use generated data as positive examples (suspicious) and assume that all real examples are negative (legitimate).
After training, we evaluate three detection scenarios: the set of rules mentioned in Section~\ref{subsubsec:amlusecase}; the model $DM$, with the threshold tuned to match the alert rate of the rules\footnote{We assume that the rules are fixed, so we cannot tune the number of their alerts.}; a combination of both (alert if either of them triggers).
The results are shown in Table~\ref{tab:03_aml_reallabels}.
We see that, even though the model $DM$ was trained using only generated data as positive examples, it achieves better detection performance than the rules.
Furthermore, only $10$ of the $128$ alerts of the Rules+Model scenario were alerted by both detection systems, and the true positives had little overlap as well ($5$ out of $54$).
%Furthermore, when we compare the triggers and the true positives of the rules and $DM$, we see that there is little overlap between them ($10$ out of $128$ alerts, $5$ out of $54$ true positives).
This means that, by including the rules' feedback in the loss of the generator, it learns to create synthetic examples that are not captured by the rules but are similar to real examples of suspicious activity.
As such, a model trained with those synthetic examples can be used to complement the rules, with the advantage of being easy to tune to a desired alert rate.

\begin{table}[H]
    \centering
    \begin{tabular}{l|c|c|c}
        & Alert Rate \% & Recall \% & Precision \% \\
        \hline
        Rules & 1.4 & 13.6 & 36.2\\
        Model & 1.4 & 18.5 & 49.3\\
        Rules + Model & 2.6 & 29.3 & 42.2
    \end{tabular}
    \caption{Detection of real labels.}
    \label{tab:03_aml_reallabels}
\end{table}


\subsection{Recommender System}
\label{sec:recs_exps}

\textbf{Setup.} We use the MovieLens 1M dataset\footnote{https://www.kaggle.com/datasets/odedgolden/movielens-1m-dataset}, comprised of a matrix of $6040$ users and $3706$ movies, with ratings ranging from 1 to 5 \citep{harper2015movielens}.
We implement the generator and discriminator and collaborative filtering recommender system as described in Section~\ref{subsubsec:rsusecase}.
To compute the predicted ratings, during training we take a weighted average of ratings considering all users in the dataset.
We consider all users during training because the initially generated ratings are random, and only providing feedback from the top-N closest users limits the strategies that the generator can learn.
In contrast, we consider the top-400 closest neighbours to compute predicted ratings at inference since we observed empirically that this value produces the lowest recommendation loss.

In this scenario, we do not use an existing detection component, corresponding to $\gamma = 0$ in Equation~\ref{eq:generator_loss}.
We train our networks with 300 synthetic attackers but evaluate the generator's ability to influence the recommender system with injection attacks of various sizes.
We also define four baseline attacks: (1) a rating of 5 for the target movie and 0 otherwise, (2) a rating of 5 for the target movie and $\sim$90 random ratings for randomly chosen movies, (3) a rating of 5 for the target movie and $\sim$90 random ratings for the top 10\% highest rated movies, (4) a rating of 5 for the target movie and $\sim$90 random ratings for the top 10\% most rated movies.

\textbf{Results.} We choose $\beta = 1-\alpha$ in Equation~\ref{eq:generator_loss}, with $0\leq \alpha \leq 1$ and perform a hyperparameter search over $\alpha$.
We observe that increasing $\alpha$ leads to generators whose attacks increasingly recommend the target movie, at the cost of moving further away from the rating distributions of real profiles.

In Table~\ref{tab:03_rs_attack}, we show how many real users have the target movie in their top-10 recommendations, depending on the number of generated users that we inject and how they were generated (through \textit{The GANfather} or the described baselines).
We observe that even with a very limited proportion of generated users (30 among 6040 real users, $~0.5\%$), they are able to greatly influence many real users ($~3.7\%$).
In contrast, the baselines have very small impact on the recommendations of real users.
Lastly, as expected, increasing the number of injected users increases the target movie's recommendation frequency to real users.


\begin{table}
    \centering
    \begin{tabular}{l|c|c|c}
        Generation strategy & 30 users & 60 users & 120 users \\
        \hline
        \textbf{The GANfather} & \textbf{225} & \textbf{290} & \textbf{428} \\
        Baseline 1 & 0 & 0 & 0\\
        Baseline 2 & 0 & 0 & 0\\
        Baseline 3 & 1 & 3 & 7\\
        Baseline 4 & 0 & 0 & 0
    \end{tabular}
    \caption{Number of real users with the target movie in their \mbox{top-10} recommendations, after injecting 30, 60, or 120 \mbox{generated} users.}
    \label{tab:03_rs_attack}
\end{table}

\iffalse
\begin{table}[H]
    \centering
    \begin{tabular}{l|c|c|c}
        Generation strategy & 30 users & 60 users & 120 users \\
        \hline
        \textbf{The GANfather} & \textbf{2218} & \textbf{4497} & \textbf{5495} \\
        Baseline 1 & 3 & 3 & 3\\
        Baseline 2 & 3 & 3 & 3\\
        Baseline 3 & 13 & 35 & 77\\
        Baseline 4 & 14 & 23 & 35
    \end{tabular}
    \caption{Number of real users with the target movie in their \mbox{top-50} recommendations, after injecting 30, 60, or 120 \mbox{generated} users.}
    \label{tab:03_rs_attack}
\end{table}
\fi

Finally, we analyse the detection of synthetic attacks.
As in the AML scenario we build a test set containing real and synthetic data, where the synthetic data contains a mixture of samples from various trained generators to identify the possible bias of a discriminator to attacks by the corresponding generator.
We then quantify the AUC of the trained discriminators.
We observe that most discriminators trained in a GAN setting (taking turns with a generator to update their weights) achieve around $0.75$ AUC.
Unlike the AML scenario, this suggests that the discriminators are tuned to detect synthetic data from their respective generators, but less so from other generators.
If instead we build a \emph{mixed} training set combining real samples with synthetic data from various generators and use it to retrain a discriminator, it achieves near-perfect classification (above $0.99$ AUC).