\section{Related Work}
\label{sec:related}

\textbf{Controllable data generation.} \citet{wang2022controllable} review controllable data generation with deep learning. Among the presented works, we highlight \cite{de2018molgan}. It leverages a GAN trained with reinforcement learning to generate small molecular graphs with desired properties. Their work is similar to ours in that we both (1) extend a GAN with an extra objective and (2) use similar data representations, namely sparse tensors. However, whereas \cite{de2018molgan} uses a labelled dataset of molecules and their chemical properties, our method does not rely on any labelled data. 


\textbf{Adversarial Attacks.} A vast amount of literature exists on the generation of adversarial attacks (see \cite{xu2020adversarial} for a recent review). Such attacks have been studied in various domains and using various setups (e.g. cybersecurity evasion using reinforcement learning \citep{apruzzese2020deep}, intrusion detection evasion using GANs \citep{usama2019generative}, sentence sentiment misclassification using BERT \citep{garg2020bae}). In all cases, a requirement is that labelled examples of malicious attacks exist.

\textbf{Anti-Money Laundering.} Typical anti-money laundering solutions are rule-based~\citep{watkins2003tracking, savage2016detection, weber2018scalable}. However, rules suffer from high false-positive rates, may fail to detect complex schemes, and are costly to maintain. Machine learning-based solutions tackle these problems \citep{chen2018machine}. Given the lack of labelled data, most solutions employ unsupervised methods like clustering \citep{wang2009research, soltani2016new}, and anomaly detection \citep{gao2009application, camino2017finding}. These assume that illicit behaviours are rare and distinguishable, which may not hold whenever money launderers mimic legitimate behaviour. Various supervised methods have been explored \citep{jullum2020detecting, raza2011suspicious, lv2008rbf, tang2005developing, oliveira2021guiltywalker}, but most of these works use synthetic positive examples or incompletely labelled datasets. To avoid this, \citet{lorenz2020machine} propose efficient label collection with active learning. \citet{deng2009active} and ~\citep{charitou2021synthetic} explore data augmentation using conditional GANs. Lastly, \citet{li2020flowscope} and \citet{sun2021cubeflow} propose a metric to detect dense money flows in large transaction graphs, resulting in an anomaly score. Their method does not involve training of a classifier, and instead relies on generating many subsets of nodes and iteratively calculating the anomaly score.

\textbf{Recommender systems (RS) injection attacks.} Most injection attacks on RS are hand-crafted according to simple heuristics. Examples include random and average attacks~\citep{lam2004shilling}, bandwagon attacks~\citep{burke2005limited} and segmented attacks \citep{burke2005segment}. However, these strategies are less effective and easily detectable as most generated rating profiles differ significantly from real data and correlate with each other. \citet{tang2020revisiting} address the optimisation problem of finding the generated profiles that maximise their goals directly through gradient descent and a surrogate RS. Some studies apply GANs to RS to generate attacks and defend the system. \citet{wu2021ready} combines a graph neural network (GNN) with a GAN to generate their attack. The former selects which items to rate, and the latter decides the ratings. \citet{zhang2021attacking} and \citet{lin2022shilling} propose a similar setup to ours in which they train a GAN to generate data and add a loss function to guide the generation of rating profiles. The main differences to our work are the usage of template rating profiles to achieve the desired sparsity, the chosen architecture and loss functions. In our work, sparsity is learned by the generator through the categorical sampling branch (see Section~\ref{sec:method}). Moreover, our method allows the generation of coordinated group attacks by generating multiple attackers from a single noise vector.
