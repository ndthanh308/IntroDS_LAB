
\subsection{Scientific Machine Learning}
\label{sec:sciml_section}

\textbf{SciML Prelimaries.} \update{Our SciML baseline experiments use \emph{neural PDE solvers} to learn temperature and flow dynamics. We focus on two classes of neural PDE solvers: (a) Image-to-image models, widely used in computer vision tasks, such as image segmentation \cite{unet2015}. These are not always suitable as PDE solvers, since they can only be used on a fixed resolution, but are still competitive in many baselines \cite{gupta2022towards, lippe2023modeling}. (b) Neural operators are neural networks that learn a mapping between infinite dimensional function spaces. As they map functions to functions, neural operators are discretization invariant and can be used on a higher resolution than they were trained \cite{kovachki2023neural}. The seminal neural operator is the Fourier Neural Operator (FNO) \cite{li2021fourier}. Further details can be found in Appendix \ref{appendix:sciml-appendix}.
}

\update{
For both classes of models, we employ the auto-regressive formulation of a forward propagator, denoted as $\mathcal{F}$. For timesteps $\{t_1, \dots, t_{max}\}$ discretized such that $t_{k+1} - t_k = \Delta t$, the forward propagator $\mathcal{F}$ maps the solution function $u$ at $k$ consecutive time steps $\{t_{m-k}, \dots, t_{m-1}\}$ to the solution at time $t_m$. For brevity, we use $u([t_{m-k},t_{m-1}]) = \{u(t_{m-k}), \dots, u(t_{m-1}) \}$.
The operator $\mathcal{F}$ can be approximated using a neural network $\mathcal{F}_\theta$ parameterized by $\theta$. This network is trained using a dataset of $N$ ground truth solutions $D = \{u^{(n)}([0, t_{max}]) : n = 1 \dots N \}$. By applying a standard gradient descent algorithm, we find parameters $\hat{\theta}$ minimizing some loss function of the predictions $\mathcal{F}_{\hat{\theta}} \{ u^{(n)}([t_{m-k},t_{m-1}])\}$ and the ground truth solutions $u^{(n)}(t_m)$. Thus, given solutions for $k$ initial timesteps of an unseen function $u$, we can obtain an approximation $\mathcal{F}_{\hat{\theta}}\{ u([0, t_{k-1}]) \} \approx u(t_k = t_{k-1} + \Delta t)$. Using this approximation for $t_k$, we can step forward to get $\mathcal{F}_{\hat{\theta}}\{ u([t_1, t_k]) \} \approx u(t_{k + 1})$. This process is called \emph{rollout} and is repeated until reaching $t_{max}$. While in principle, rollout can be done for arbitrary time, the quality of approximation worsens with each step \cite{lippe2023modeling, mccabe2023towards}. We implement several strategies that attempt to mitigate this deterioration \cite{brandstetter2022message, tran2023factorized}. However, achieving long and stable rollout is still an open problem.
}

\update{
\textbf{Baseline Implementations.} We implement several baseline image-to-image models---including UNet$_\text{bench}$ and UNet$_\text{mod}$---and neural operators---including FNO, UNO, F-FNO, and G-FNO. Detailed descriptions and comparisons of the models are included in Appendix \ref{appendix:sciml-benchmark-models}. }

\update{
\textbf{Training Strategies.}  Detailed descriptions for each of the training stragies we used are listed in Appendix \ref{appendix:sciml-training-strategies}. We implement teacher-forcing training \cite{williams1989teacherforcing}, temporal bundling, and the pushforward trick \cite{brandstetter2022message}.  Models trained with the pushforward trick are prefixed with ``P-''. A discussion of hyperparameter settings can be found in Appendix \ref{appendix:sciml-hyperparameter-settings}.
} 

\update{
\textbf{Metrics.} We draw inspiration from PDEBench and adopt a large set of metrics that include the Root Mean Squared Error (RMSE), Max Squared Error, Relative Error, Boundary RMSE (BRMSE), and low/mid/high Fourier errors \cite{PDEBench2022}. These metrics provide a comprehensive view of the physical dynamics, which may be missed when only using a ``global'' loss metric. For instance, when predicting temperature, we find that the max error can often be very high due to the presence of sharp transitions between hot vapor and cool liquid. Even a one-pixel misalignment in the model's prediction can cause the reported temperature to be the opposite extreme. Metrics which report a global average (i.e., RMSE) could mask these errors because they get damped by the average.
}
\update{
We incorporate an additional \emph{physics} metric: the RMSE along bubble interfaces (IRMSE). The accuracy along both the domain and immersed boundaries is very important. Boundary conditions determine if the solution to a PDE exists and is unique. In the case of the multiphysics BubbleML dataset, accurate modeling of the system requires satisfying the conditions at the liquid-vapor interfaces accurately.
}

\textbf{Learning Temperature Dynamics.} One application of SciML using the BubbleML dataset is to learn the dynamics of temperature propagation within a system. In this context, the system's velocities serve as a sourcing function, influencing the temperature distribution. \update{Notably, UNet-based models perform best across all datasets (see Figure \ref{fig:sciml-figures}c and d). For a full listing of error metrics for each model and dataset pairing, refer to Appendix \ref{appendix:sciml-temp-results}. UNet models may have some advantage in predicting the interfaces and boundaries (IRMSE and BRMSE), because they naturally act as edge-detectors. The temperature also propagates smoothly, so it is likely unnecessary to use global filters, like the FNO variants. In contrast, FNO models rely on fast Fourier transforms and weight multiplication in the Fourier space, which, while capable of handling global and local information simultaneously, might not be as effective at capturing local, non-smooth features. Several recent studies report similar observations about auto-regressive UNet and FNO variants \cite{lippe2023modeling, gupta2022towards, mccabe2023towards}.}

% Figure environment removed

The trained model can be a valuable tool to get fast estimates of heat flux, discussed in section \ref{sec:pde-sim-overview}. Heat flux is influenced by steep temperature gradients and dynamic temporal changes which presents a challenging problem.
To further validate our models, we perform cross-validation to predict the heatflux trends observed in Figure \ref{fig:validation-data}. For each heat flux prediction, we holdout a simulation and train a forward propagator on the remaining simulations within the dataset. Even with partial training---50 epochs for subcooled boiling models and 100 epochs for saturated boiling models---we achieve compelling results. The heatflux predictions by UNet$_\text{bench}$ remarkably track the expected trend, as seen in Figure \ref{fig:sciml-figures}a. 

\textbf{Learning Fluid Dynamics.} \update{As an additional benchmark, we use the BubbleML dataset to train models to approximate both velocity \emph{and} temperature dynamics. This is a challenging problem. Results are shown in Appendix \ref{appendix:sciml-flow-results}. These follow similar training settings to the temperature-only models. Strikingly, however, we get nearly the opposite results to predicting only temperature: the UNet$_\text{bench}$ model struggles when predicting both velocity and temperature fields jointly, while the UNet$_\text{mod}$ and the FNO variants perform comparatively better. All of the models have difficulty capturing the trails of condensation that form in the temperature field. The vapor trails form, but dissipate more quickly than expected. An example rollout of UNet$_\text{mod}$, trained using the pushforward trick, is shown in Figure \ref{fig:vel-rollout-sample}. We see that the flow closely aligns with the ground-truth simulation.
}

% Figure environment removed

\textbf{Open Problems.} \update{We reiterate several open problems in SciML that BubbleML offers an avenue to explore. The first is the creation of a new class of \emph{models that can learn multiple interrelated physics}. We find that while UNet architectures work well at predicting temperature and FNO variants work well at predicting velocity, neither excel at joint prediction of temperature and velocity. The CNN-based UNet architectures outperform FNO and its variants when predicting temperature, potentially due to CNN's capacity to naturally act as edge-detectors, and thus handle non-smooth interfaces more easily. On the other hand, FNO variants perform quite well at predicting velocities, but still struggle with temperature estimation, especially in capturing condensation trails. This is related to the second problem: \emph{developing neural operators that can handle non-smooth and irregularly shaped interfaces}. FNO variants seem to encounter difficulties in modeling temperature fields, which have sharp jumps along bubble interfaces where the temperature transitions from cool liquid to hot vapor. Conversely, the velocity field appears relatively smooth, and thus may be composed of lower frequencies better captured by FNO variants. However, these models still miss sharp and sudden changes in velocity along bubble interfaces that are important for accurately modeling long-range dynamics. The third problem is improving \emph{stability during long rollouts}. This is explored within the context of other datasets \cite{lippe2023modeling, mccabe2023towards}, but it is particularly relevant for BubbleML. For instance, in subcooled boiling, after bubbles depart from the surface, they undergo condensation and generate vortices that gradually dissipate as they move upstream. To model these extended temporal processes accurately, autoregressive models must be stable across long rollouts. However, we observe that models experience instability, leading them to slowly diverge from the ground truth. The BubbleML dataset presents an opportunity to study these challenges in SciML.}
