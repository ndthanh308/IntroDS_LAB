% Gemini theme
% https://github.com/anishathalye/gemini
%
% We try to keep this Overleaf template in sync with the canonical source on
% GitHub, but it's recommended that you obtain the template directly from
% GitHub to ensure that you are using the latest version.

\documentclass[]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\newcommand{\bX}{{\mathbf X}}
\newcommand{\g}{{\mathbf g}}
\newcommand{\ba}{{\mathbf a}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\x}{{\mathbf x}}
\pgfplotsset{compat=1.18}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

\renewcommand{\thefootnote}{\alph{footnote}}




% ====================
% Title
% ====================

\title{On the Learning Dynamics of Attention Networks}

\author{Rahul Vashisht \and Harish G. Ramaswamy}

\institute[shortinst]{MALT Lab, Department of Computer Science and Engineering, IIT Madras}

% ====================
% Footer (optional)
% ====================

\footercontent{\hfill Contacts: rahul@cse.iitm.ac.in \hfill }
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
% \logoright{% Figure removed}
% \logoleft{% Figure removed}

% ====================
% Body
% ====================

\begin{document}

\addtobeamertemplate{headline}{}
{
    \begin{tikzpicture}[remember picture,overlay]
      \node [anchor=north west, inner sep=3cm] at ([xshift=-0.3cm,yshift=2cm]current page.north west)
      {% Figure removed}; % also try shield-white.eps
      % \node [anchor=north east, inner sep=3cm] at ([xshift=0.0cm,yshift=2.5cm]current page.north east)
      % {% Figure removed};

        \node [anchor=north east, inner sep=3cm] at ([xshift=-4.0cm,yshift=1.5cm]current page.north east)
      {% Figure removed};
      
    \end{tikzpicture}
}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}


\begin{block}{Problem Statement}

\LARGE
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. 

\end{block}







% \begin{block}{Selective Dependence Classification}
\begin{itemize}
    \setlength\itemsep{1em}

    \item  \LARGE Consider an instance $\bX = [\x_1,\x_2,\ldots,\x_m] \in \R^{m \times d}$ and label $y \in [k]$, where a component of $\bX$ determines $y$.
     % Figure environment removed

\end{itemize}

    % % Figure environment removed
    
   

% \end{block}
  % \begin{alertblock}{ Paradigms of Attention}
  % \begin{itemize}
  %  \setlength\itemsep{1em} 
  %     \item Latent Variable Marginal Loss: a  latent variable based model for attention in which we maximize the log-marginal likelihood
  %   %  \footnote[12]{ $\R^m \ni \ba(\bX) = \sigma(f(\x_1), \ldots, f(\x_m))$, $\sigma$ is the softmax function}
  %   %.
  %   % \begin{align*}
  %   %      L^\text{LV}(f, \g, \bX, y) &= -\log(\sum_j a_j(\bX) \sigma_y(\g(\x_j)))
  %   % \end{align*}
    
  %   \item Hard Attention:  maximize the lower bound objective over the log-marginal likelihood.
  %   % \begin{align*}
  %   %     L^\text{HA}(f, \g, \bX, y) &= - \sum_j a_j(\bX) \log(\sigma_y(\g(\x_j)))
  %   % \end{align*}
    
  %   \item Soft Attention: take weighted average of input/representation to generate context vector.
  %   % \begin{align*}
  %   %     L^\text{SA}(f, \g, \bX, y) &= -\log(\sigma_y(\g(\sum_j a_j(\bX) \x_j)))
  %   % \end{align*}
  % \end{itemize}


  % \end{alertblock}




% \begin{block}{Empirical Analysis of Attention Paradigms}

  % Figure environment removed

  \begin{itemize}
      \setlength\itemsep{1em} 
      
      \item \LARGE An ideal attention model will have all instances in top-right corner of the heatmap, which we define as the “Strongly Accurate Interpretable Fraction” of data points.
  \end{itemize}

 

  % \end{block}

  \end{column}

\separatorcolumn

\begin{column}{\colwidth}



% \begin{block}{ Fixed Focus Analysis }
  

  % Figure environment removed

    \begin{itemize}
      \setlength\itemsep{1em}
      \item \LARGE Under the fixed focus setting, we analyze the evolution of classification module by fixing the score  given to the foreground segment.
      
  \end{itemize}
  
% \end{block}

 % \begin{block}{Focus Improvement Incentive Analysis}



  % Figure environment removed


    \begin{itemize}
    \setlength\itemsep{1em}
    \item \LARGE Focus improvement incentive, $\delta_{\alpha,t}$ is the rate of loss reduction, when focus network improves from $\alpha$ to $\alpha + \delta$. %  which captures the reduction in loss as focus model gets better.
    \end{itemize}

% \end{block}

 % \begin{block}{Hybrid Approach}
  \begin{itemize}
     \setlength\itemsep{1em}
      \item \LARGE Based on the observations, we propose using a trained attention model (with soft attention) as initialisation for hard attention.

      \item We also analyze these paradigms in a simple
setting and derive closed-form expressions for the parameter trajectory under gradient flow. 
    \end{itemize}

    % \end{block}

    % Figure environment removed



\end{column}






\end{columns}
\end{frame}

\end{document}
