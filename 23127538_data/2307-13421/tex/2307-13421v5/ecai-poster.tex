% Gemini theme
% https://github.com/anishathalye/gemini
%
% We try to keep this Overleaf template in sync with the canonical source on
% GitHub, but it's recommended that you obtain the template directly from
% GitHub to ensure that you are using the latest version.

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=a0,orientation=landscape,scale=1.25]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\newcommand{\bX}{{\mathbf X}}
\newcommand{\g}{{\mathbf g}}
\newcommand{\ba}{{\mathbf a}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\x}{{\mathbf x}}
\pgfplotsset{compat=1.14}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

\renewcommand{\thefootnote}{\alph{footnote}}

\renewcommand{\arraystretch}{1.35}

% ====================
% Title
% ====================

\title{On the Learning Dynamics of Attention Networks}

\author{Rahul Vashisht \and Harish G. Ramaswamy}

\institute[shortinst]{MALT Lab, Department of Computer Science and Engineering, IIT Madras}

% ====================
% Footer (optional)
% ====================

\footercontent{\hfill Contacts: rahul@cse.iitm.ac.in, hariguru@cse.iitm.ac.in \hfill }
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
% \logoright{% Figure removed}
% \logoleft{% Figure removed}

% ====================
% Body
% ====================

\begin{document}

\addtobeamertemplate{headline}{}
{
    \begin{tikzpicture}[remember picture,overlay]
      \node [anchor=north west, inner sep=3cm] at ([xshift=0.0cm,yshift=2.5cm]current page.north west)
      {% Figure removed}; % also try shield-white.eps
      % \node [anchor=north east, inner sep=3cm] at ([xshift=0.0cm,yshift=2.5cm]current page.north east)
      % {% Figure removed};

        \node [anchor=north east, inner sep=3cm] at ([xshift=1.cm,yshift=0.5cm]current page.north east)
      {% Figure removed};
      
    \end{tikzpicture}
}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}


\begin{block}{Abstract}
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models-- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. %Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions, and demonstrate it on a collection of semi-synthetic datasets based on CIFAR-10 and CIFAR-100.

\end{block}
%  \begin{block}{Introduction}


%     % % Figure environment removed
    
%     \begin{itemize}
%     \setlength\itemsep{1em}
%     \item Attention mechanism are one of the successful components in deep neural network. 
%     \item Despite the great success of attention models, there is a lack of clear understanding of these models.
%     \item Attention Models are learned by optimizing one of the three loss functions called as Soft, Hard, and Latent Variable Marginal Likelihood Attention.
%     \item The goal in attention models is to find two sub-models -- a `focus' model, that selects the right segment of the input and a `classification' model, that processes the selected segment. 
    
    
 
% \end{itemize}



%   \end{block}


% \begin{block}{Paradigms of Attention}

% \begin{itemize}
% \setlength\itemsep{1em}
%     \item Soft attention uses a convex combination of features based on attention scores calculated using a deterministic function 
    
%     \item In hard attention based approaches (typically learned in two ways), one of the input segment is selected based on the distribution of attention scores, .
    
%     \item In one method, we directly maximize the log-marginal likelihood, which is also known as Exact-hard attention.
    
%     \item In second method, we maximize a lower bound objective over the log-marginal likelihood, typically called as hard attention in literature.
        
%   \item Hard attention models are computationally more expensive as comparred to soft attention models.
% \end{itemize} 

% \end{block}


\begin{block}{Selective Dependence Classification}
\begin{itemize}
    \setlength\itemsep{1em}

    \item Consider an instance $\bX = [\x_1,\x_2,\ldots,\x_m] \in \R^{m \times d}$ and label $y \in [k]$, where a component of $\bX$ determines $y$.
     % Figure environment removed
    
    \item Focus Classify Attention Model (FCAM ) is used to solve the Selective Dependence Classification task.
\end{itemize}

    % % Figure environment removed
    
   

\end{block}
  \begin{alertblock}{ Paradigms of Attention}
  \begin{itemize}
      \item Latent Variable Marginal Loss: a  latent variable based model for attention in which we maximize the log-marginal likelihood
    %  \footnote[12]{ $\R^m \ni \ba(\bX) = \sigma(f(\x_1), \ldots, f(\x_m))$, $\sigma$ is the softmax function}
    %.
    \begin{align*}
         L^\text{LV}(f, \g, \bX, y) &= -\log(\sum_j a_j(\bX) \sigma_y(\g(\x_j)))
    \end{align*}
    
    \item Hard Attention:  maximize the lower bound objective over the log-marginal likelihood.
    \begin{align*}
        L^\text{HA}(f, \g, \bX, y) &= - \sum_j a_j(\bX) \log(\sigma_y(\g(\x_j)))
    \end{align*}
    
    \item Soft Attention: take weighted average of input/representation to generate context vector.
    \begin{align*}
        L^\text{SA}(f, \g, \bX, y) &= -\log(\sigma_y(\g(\sum_j a_j(\bX) \x_j)))
    \end{align*}
  \end{itemize}


  \end{alertblock}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Empirical Analysis of Attention Paradigms}
    \begin{itemize}
    \setlength\itemsep{1em} 
    %\item  We consider a attention model for the SDC task and called it as Focus Classify Attention Model (FCAM).
    \item We perform an empirical study on synthetic data based on CIFAR10 data, where we consider 3 classes as Foreground class and rest as the background class.
    \item The trained FCAM model is used to obtain a Focus Prediction heatmap, by considering a true foreground class score $a_z$ and true class score $s_y$.
    \item An ideal attention model will have all instances in top-right corner of the heatmap, which we define as the ``Strongly Accurate Interpretable Fraction'' of  data points.
  \end{itemize}
  % Figure environment removed

 

  \end{block}

  \begin{block}{ Fixed Focus Analysis }
  
  \begin{itemize}
      \setlength\itemsep{1em}
      \item Under the fixed focus setting, we analyze the evolution of classification module by fixing the score ($a_z = \alpha$) given to the foreground segment.
      
  \end{itemize}
  % Figure environment removed
  
  \begin{itemize}
  \setlength\itemsep{1em}
    %   \item  Soft attention loss eventually goes to zero for moderately for $\alpha >0.2 $. 
    %   \item Hard attention and LVML losses flatten at a value above $0$ for any $\alpha<1$.
      
      \item In case of soft attention, the difference in loss at $\alpha=0.8$ and $\alpha=1$ is minimal, suggesting the need for improving focus network is also minimal.
      
      %\item In the hard attention paradigm, flat loss curve at $\alpha=0.2$, indicates the difficulty in improving classification network
      
      \item Hard attention do have the large difference in loss for $\alpha =0.8$ and $\alpha=1$, however flat loss at $\alpha=0.2$ indicates, it will have difficulty in improving focus from $\alpha=0.2$ to $\alpha=0.8$.
      %  
  \end{itemize}


  \end{block}


\end{column}

\separatorcolumn

\begin{column}{\colwidth}

%   \begin{exampleblock}{A highlighted block containing some math}

   
%   \end{exampleblock}

\begin{block}{Focus Improvement Incentive Analysis}
\begin{itemize}
    \setlength\itemsep{1em}
    \item Focus improvement incentive, $\delta_{\alpha,t}$ is the rate of loss reduction, when focus network improves from $\alpha$. %  which captures the reduction in loss as focus model gets better.
    \end{itemize}

  % Figure environment removed

\begin{itemize}
    \setlength\itemsep{1em}
    \item Soft attention performs well at initialisation and gives large incentives to the focus model to improve, but stagnates later and hard attention has opposite issue.
    %\item Hard attention has the opposite problem of stagnating at initialisation %but gives significant incentives for the focus model to improve further once a non-trivial focus model is already learnt.
    %\item $\delta_{\alpha,t}$ for the LVML paradigm also diminshes with increase in $\alpha$, but the fall in incentive is not nearly as steep
   

\end{itemize}

  \end{block}
  \begin{block}{Hybrid Approach}
  \begin{itemize}
      \item Based on the observations, we propose using a trained attention model (with soft attention) as initialisation for hard attention.
    
 \begin{table}[!ht]
    \small
    \centering
    
    \begin{tabular}{| c | p{15cm} | c | p{6cm} |}
         \hline
         Data & \hspace{5cm} Instance  &  Label  &   \hspace{1cm} Rationale    \\ [1ex]
         \hline
         ~ HateXplain ~ &  [“<user>”, “<user>”, “Why”, “are”, “you”, “repeating”, “yourself”, “are”, “you”, “a”, “little”, “retarded”] 
 &  "Offensive" & [0, 0, 0, 0, 0, 0,0,0, 0, 0,1,1]  \\[2ex] 
         \hline 
    \end{tabular}
    
    %  \caption{Strongly Accurate Interpretable fraction of data points for different datasets}
    \label{table1:my_label}
\end{table}
    
      
      \item ``HateXplain'' Data: a real world hate speech data similar to SDC task of classifying a given sentence to ``hatespeech'', ``neutral'' and ``offensive'' classes.  
     
      



% % Figure environment removed

  
 
     \begin{table}[!ht]
    \centering
      \caption{Strongly Accurate Interpretable fraction of data points for different datasets}
    \resizebox{0.9\linewidth}{!}{
      
    \begin{tabular}{|c|c|c|c|c|}
    
         \hline
         Setting & ~ Soft Attention ~ & ~~~ LVML ~~~ &  ~ Hard Attention ~  & ~ Hybrid SA-HA ~  \\
         \hline
         ~ CIFAR10 ($m=5$) ~ & 37.09  & ~~~ 61.97 ~~~  & 37.75 & 58.92 \\
         \hline
         ~ CIFAR10 ($m=20$) ~ & 23.71 & ~~~ 47.24 ~~~ & 1.87 & 38.17 \\
         \hline
         ~ CIFAR100 ($m=5$) ~& 79.2 & ~~~ 77.5 ~~~ & 13.55 & 83.23 \\
         \hline
         ~ CIFAR100 ($m=20$) ~ & 67.02 & ~~~ 66.45 ~~~ & 1.66 & 66.83\\
         \hline
         ~ HateXplain data ~ & 7.53  & ~~~ 23.55 ~~~& 2.71 & 14.01 \\
         \hline 
    \end{tabular}}
    
    \label{table1:my_label}
\end{table}


%      \begin{table}[!ht]
%     \centering
%     \resizebox{0.9\linewidth}{!}{
%     \begin{tabular}{|c|c|c|c|c|}
%          \hline
%          Setting & ~ Soft Attention ~ & ~~~ LVML ~~~ &  ~ Hard Attention ~  & ~ Hybrid SA-HA ~  \\
%          \hline
%          ~ CIFAR10 ($m=5$) ~ & 75.33   & ~~~ 77.86  ~~~  &  63.33 &  76.62 \\
%          \hline
%          ~ CIFAR10 ($m=20$) ~ & 65.21 & ~~~ 70.69 ~~~ & 42.05  & 66  \\
%          \hline
%          ~ CIFAR100 ($m=5$) ~& 83.17 & ~~~ 79.71  ~~~ & 46.57  &  80 \\
%          \hline
%          ~ CIFAR100 ($m=20$) ~ & 72.18  & ~~~ 68.96 ~~~ & 8.2  & 71 \\
%          \hline
%          ~ HateXplain data ~ & 49.89 & ~~~ 47.66 ~~~& 54.10  & 54.62 \\
%          \hline 
%     \end{tabular}}
    
    
%     %  \caption{Strongly Accurate Interpretable fraction of data points for different datasets}
%     \label{table1:my_label}
% \end{table}

\item Hybrid approach seems to always improve ``Strongly Accurate Interpretable'' fraction of data points as compared to soft attention.
  \end{itemize}
  \end{block}
  

  \begin{block}{References}

    \nocite{*}
    \footnotesize{\bibliographystyle{plain}\bibliography{poster}}

  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
