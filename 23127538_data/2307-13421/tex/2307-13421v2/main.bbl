\begin{thebibliography}{10}

\bibitem{Bahdanau2016}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, `Neural machine translation
  by jointly learning to align and translate', in {\em 3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, eds., Yoshua Bengio and Yann LeCun,
  (2015).

\bibitem{10.5555/92858.92860}
Peter~F. Brown, John Cocke, Stephen A.~Della Pietra, Vincent J.~Della Pietra,
  Fredrick Jelinek, John~D. Lafferty, Robert~L. Mercer, and Paul~S. Roossin, `A
  statistical approach to machine translation', {\em Comput. Linguist.}, {\bf
  16}(2),  79–85, (jun 1990).

\bibitem{10.5555/972470.972474}
Peter~F. Brown, Vincent J.~Della Pietra, Stephen A.~Della Pietra, and Robert~L.
  Mercer, `The mathematics of statistical machine translation: Parameter
  estimation', {\em Comput. Linguist.}, {\bf 19}(2),  263–311, (jun 1993).

\bibitem{ChorowskiBCB14}
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, `End-to-end
  continuous speech recognition using attention-based recurrent nn: First
  results', in {\em NIPS 2014 Workshop on Deep Learning, December 2014},
  (2014).

\bibitem{NEURIPS2018_b691334c}
Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush, `Latent
  alignment and variational attention', in {\em Advances in Neural Information
  Processing Systems}, eds., S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman,
  N.~Cesa-Bianchi, and R.~Garnett, volume~31. Curran Associates, Inc., (2018).

\bibitem{edunov-etal-2018-classical}
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc{'}Aurelio
  Ranzato, `Classical structured prediction losses for sequence to sequence
  learning', in {\em Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pp. 355--364, New Orleans, Louisiana,
  (June 2018). Association for Computational Linguistics.

\bibitem{machine_trns}
Alex Graves, `Generating sequences with recurrent neural networks', {\em CoRR},
  {\bf abs/1308.0850}, (2013).

\bibitem{Jain2019}
Sarthak Jain and Byron~C. Wallace, `{Attention is not Explanation}', in {\em
  Proceedings of the 2019 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pp. 3543--3556, Stroudsburg, PA, USA,
  (2019). Association for Computational Linguistics.

\bibitem{Krizhevsky09learningmultiple}
Alex Krizhevsky and Geoffrey Hinton, `Learning multiple layers of features from
  tiny images', Technical Report~0, University of Toronto, Toronto, Ontario,
  (2009).

\bibitem{lei-etal-2020-tvqa}
Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal, `{TVQA}+: Spatio-temporal
  grounding for video question answering', in {\em Proceedings of the 58th
  Annual Meeting of the Association for Computational Linguistics}, pp.
  8211--8225, Online, (July 2020). Association for Computational Linguistics.

\bibitem{lu2021on}
Haoye Lu, Yongyi Mao, and Amiya Nayak, `On the dynamics of training attention
  models', in {\em International Conference on Learning Representations},
  (2021).

\bibitem{mathew2020hatexplain}
Binny Mathew, Punyajoy Saha, Seid~Muhie Yimam, Chris Biemann, Pawan Goyal, and
  Animesh Mukherjee, `Hatexplain: A benchmark dataset for explainable hate
  speech detection', (2021).

\bibitem{10.5555/2969033.2969073}
Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu, `Recurrent
  models of visual attention', in {\em Proceedings of the 27th International
  Conference on Neural Information Processing Systems - Volume 2}, NIPS'14, p.
  2204–2212, Cambridge, MA, USA, (2014). MIT Press.

\bibitem{https://doi.org/10.48550/arxiv.2212.14776}
Lakshmi~Narayan Pandey, Rahul Vashisht, and Harish~G. Ramaswamy.
\newblock On the interpretability of attention networks, 2022.

\bibitem{Vashishth2019AttentionIA}
Shikhar Vashishth, Shyam Upadhyay, Gaurav~Singh Tomar, and Manaal Faruqui,
  `Attention interpretability across nlp tasks', {\em ArXiv}, {\bf
  abs/1909.11218}, (2019).

\bibitem{Attnall17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin, `Attention is all you
  need', in {\em Advances in Neural Information Processing Systems}, eds.,
  I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan,
  and R.~Garnett, volume~30, pp. 5998--6008. Curran Associates, Inc., (2017).

\bibitem{wiegreffe-pinter-2019-attention}
Sarah Wiegreffe and Yuval Pinter, `Attention is not not explanation', in {\em
  Proceedings of the 2019 Conference on Empirical Methods in Natural Language
  Processing and the 9th International Joint Conference on Natural Language
  Processing (EMNLP-IJCNLP)}, pp. 11--20, Hong Kong, China, (November 2019).
  Association for Computational Linguistics.

\bibitem{wu-cotterell-2019-exact}
Shijie Wu and Ryan Cotterell, `Exact hard monotonic attention for
  character-level transduction', in {\em Proceedings of the 57th Annual Meeting
  of the Association for Computational Linguistics}, pp. 1530--1537, Florence,
  Italy, (July 2019). Association for Computational Linguistics.

\bibitem{wu-etal-2018-hard}
Shijie Wu, Pamela Shapiro, and Ryan Cotterell, `Hard non-monotonic attention
  for character-level transduction', in {\em Proceedings of the 2018 Conference
  on Empirical Methods in Natural Language Processing}, pp. 4425--4438,
  Brussels, Belgium, (oct-nov 2018). Association for Computational Linguistics.

\bibitem{pmlr-v37-xuc15}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
  Salakhudinov, Rich Zemel, and Yoshua Bengio, `Show, attend and tell: Neural
  image caption generation with visual attention', in {\em Proceedings of the
  32nd International Conference on Machine Learning}, eds., Francis Bach and
  David Blei, volume~37 of {\em Proceedings of Machine Learning Research}, pp.
  2048--2057, Lille, France, (07--09 Jul 2015). PMLR.

\end{thebibliography}
