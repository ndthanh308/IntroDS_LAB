
\section{Related Work}
\label{sec:related_work}
The overall pipeline of our model is divided into two components: Graph Convolution and Graph Pooling with Co-attention.
In this section, we provide a detailed overview of the previous works related to each component.


\subsection{Graph Convolution Networks}  
\label{subsec:GCN}
Researchers have been actively working on using convolutional neural networks to process graph-structured data.
However, the conventional convolution operation, which is defined on grids, is difficult to be directly applied on graphs due to irregular structure.
To address this issue, previous researches have defined graph convolution by using the Fourier transform (i.e. spectral graph convolution) \cite{kipf2017semi,monti2017geometric} or spatial graph connectivity (i.e. non-spectral graph convolution) \cite{hamilton2017inductive,velickovic2017graph}.
The spectral approaches can be represented by the work of Kipf and Welling \cite{kipf2017semi}, where graph convolution is redefined on the Fourier domain with a localized first-order approximation.
Non-spectral graph convolutions align to the work of Hamilton \cite{hamilton2017inductive},where graph convolution is defined as an aggregation function of a node's neighborhood representations.
Building upon these works, subsequent studies have aimed to improve the expressive power of both node-level and graph-level representations \cite{gcnii_icml20,pna_neurips20,magna_ijcai21}.


\subsection{Graph Pooling}
\label{subsec:pooling}
Pooling is a technique that prevents overfitting in modern neural network models with a large number of parameters by reducing the size of the representations.
This allows the model to generalize better to new data. There are two main categories of graph pooling methods: global pooling and hierarchical pooling.
Hierarchical pooling methods can be further grouped into \textit{pooling by graph transformation} and \textit{pooling by node selection}, based on how they produce the reduced node and edge sets.


\textbf{Global graph pooling} methods are techniques that transform the variable-sized representation of nodes produced by GNNs into a fixed-sized vector representation. This can then be used as input for downstream tasks such as prediction.
Some common global graph pooling methods include using simple aggregation functions such as summation, average, and max to combine the node representations.
The SortPool method \cite{zhang2018end} involves selecting the top $k$ nodes from the graph based on a certain criterion, and using their representations as the pooled output.
SimGNN \cite{bai2019simgnn} proposes using global context-aware attention to weight the contributions of different nodes, and producing a pooled representation as a weighted sum of all the node representations in the graph.
These methods allow for the use of GNN output as input for downstream tasks, and have been applied to a range of tasks including graph classification and regression.
\newline


\textbf{Pooling by graph transformation} is one of the hierarchical graph pooling methods where nodes are clustered and merged under certain criteria \cite{ying2018hierarchical,Yuan2020StructPool}.
The main purpose of \textit{Pooling by graph transformation} is to learn the assignment matrix that transforms input nodes into new cluster nodes.
The new adjacency matrix of cluster nodes is re-defined according to the assignment matrix.
The \textit{Pooling by graph transformation} has the advantage of keeping all node information. 
However, there is a computational complexity issue when calculating the new adjacency matrix for cluster nodes.
And also, the output of several pooling layers is not easily interpreted through the transformed graph.
\newline

\textbf{Pooling by node selection} creates hierarchical graph representations via trainable indexing method.
TopKPool \cite{gao2019graph} uses a trainable projection vector to calculate node scores and accordingly select nodes with the top-k score.
SAGPool \cite{pmlr-v97-lee19c} improves upon TopKPool by using GNNs to consider the topology of the graph along with the node features to calculate node scores.
Our method aligns with the work of \textit{pooling by node selection} as it has comparably lower complexity than pooling by graph transformation.
Although information loss might occur during node discarding, we propose that this helps the model to dynamically focus on different sub-graphs for different graph pairs by eliminating the representations of the nodes that are irrelevant to the interaction.


\subsection{Co-Attention Mechanism}
\label{subsec:attention}

Attention mechanisms, which allow neural networks to assign trainable importance weights to input~\cite{vaswani2017attention}, have been widely used in deep learning research.
There have been several works that extend the use of attention to input in the form of pairs. For example, Seo et al. \cite{seo2016bidirectional} proposed a bidirectional attention flow model that includes both query-to-context and context-to-query attentions (i.e., co-attention) for machine comprehension tasks.
Additionally, Deac et al. \cite{deac2020empowering} demonstrated that co-attention obtained from pairs of graphs can significantly improve the predictive power of GNNs.
In this work, we leverage the co-attention mechanism for graph pooling to select nodes in paired graphs that should be aware of each other.
