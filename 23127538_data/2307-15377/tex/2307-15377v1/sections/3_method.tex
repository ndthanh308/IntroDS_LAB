\section{Methods}
\label{sec:method}
\input{figures/figure_pipeline.tex}

In this section, we first describe the basic notations for general pairwise graph representation learning.
Then, we introduce our proposed method: Co-Attention Graph Pooling.
We denote the goal of our Co-Attention Graph Pooling, i.e., CAGPool, followed by detailed explanations for each components of our overall pipeline for pair-wise graph representation learning using CAGPool (see Figure~\ref{fig:fig_pipeline} for detailed illustration).

\subsection{Problem Setting}
Let $G=(V,E)$ denote the undirected graph, where $V$ is the set of vertices and $E$ is the set of edges.
Its adjacency matrix $A$ can be constructed with $A_{i,j}=1$ if $(i,j) \in E$ and $A_{i,j} = 0$ otherwise.
Node attributes can be represented as a matrix form: $X^{(0)} \in \mathbb{R}^{N \times F}$, where $N=|V|$ is the number of nodes and $F$ is the feature dimension.
Our main task is to predict the labels for a pair of input graphs $(G_\mathcal{A}, G_\mathcal{B})$, thus designing a model that learns the function $f: G \times G \mapsto R \in \mathbb{R}^{o}$, where $o$ denotes the output dimension.

While learning $f$, it has been studied that considering the interaction between the input graph pairs leads to better prediction~\cite{bai2019simgnn,deac2020empowering}.
For clear comparison with previous methods, we define $I(G)$ as the representation of the graph $G$ and the interaction representation of two graphs as $I(G_\mathcal{A},G_\mathcal{B})$, where $I_n(G_\mathcal{A},G_\mathcal{B})$ and $I_g(G_\mathcal{A},G_\mathcal{B})$ each denotes the interaction at the node-level and graph-level, respectively.
Since interactions were utilized at the node-level ($I_n(G_\mathcal{A}, G_\mathcal{B})$) in previous works~\cite{bai2019simgnn,deac2020empowering,pathak2020chemically}, they suffer from high complexity (see Figure \ref{fig:fig_complexity} and  Section \ref{sec:complexity}).

% Goal
The main goal of this paper is to consider the interaction representation at graph-level (i.e., $I_g(G_\mathcal{A}, G_\mathcal{B})$) instead of node-level (i.e., $I_n(G_\mathcal{A}, G_\mathcal{B})$).
To this end, we propose a graph pooling module that learns a mapping function $g: G \times G \mapsto G' \times G'$ that improves the predictive power of $f$, where $G'=(V',E')$ is the sub-graph with $V' \subset V$ and $E' \subset E$.
The individual representations $I(G_\mathcal{A}), I(G_\mathcal{B})$ for each graph are then refined to $I(G_\mathcal{A}'), I(G_\mathcal{B}')$ respectively by using the interaction representation $I_g(G_\mathcal{A},G_\mathcal{B})$.

\subsection{Node Embedding with Graph Convolution}
\label{subsec:node_embedding}
Our overall pipeline follows the basic architecture of graph convolution Siamese network\cite{koch2015siamese}.
We receive a pair of graphs as input and apply the graph convolution with shared weights to each graph of the pair.
In the graph convolution layer, we update node representations by neighborhood aggregation.
We use the graph convolution suggested by Kipf and Welling \cite{kipf2017semi},
\begin{equation}
    X^{(l+1)}=\sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X^{(l)}\Theta^{(l)})
\end{equation}{}
where $X^{(l)}$ is the node representation of $l$-th layer.
$\tilde{A}\in\mathbb{R}^{N\times N}$ is the adjacency matrix with self-loops.
$\tilde{D}\in \mathbb{R}^{N \times N}$ denotes the degree matrix of $\tilde{A}$. $\Theta^{(0)}\in\mathbb{R}^{F\times F'}$ and $\Theta^{(l)}\in\mathbb{R}^{F'\times F'} (l>=1)$ are learnable convolution weights.
$\sigma$ is a non-linear function that follows each graph convolution layer.
Afterwards, we concatenate the output of each output layer.
The final node representation $X^{cat} \in\mathbb{R}^{N\times nF'}$ is obtained by concatenating each of the $n$ convolution blocks.
\begin{equation}
    X^{cat}=X^{(1)}\|X^{(2)}\|...\|X^{(n)}
    \label{eq:X_cat}
\end{equation}{}
where $\|$ denotes the concatenation.
Because we have two graphs as a pairwise input, two node representations $(X^{cat}_\mathcal{A},X^{cat}_\mathcal{B})$ are obtained by a graph convolution Siamese network.

\subsection{Co-Attention Graph Pooling}
After individually obtaining the representations for the pair of input graphs, our CAGPool layer takes the two graph representations ($(X_{\mathcal{A}}^{cat},A_{\mathcal{A}})$ and $(X_{\mathcal{B}}^{cat},A_{\mathcal{B}}))$ after several graph convolution layers and then constructs the co-attention vector $\vec{\alpha}$ to calculate node scores.
Then, the subgraphs are extracted by indexing each graph with the node scores.
Below, we cover details of how CAGPool works.\newline


\textbf{Obtaining co-attention vector.}
The co-attention vector $\vec{\alpha}$ is obtained from the two graph-level representations of the input graph pair ($x_\mathcal{A},x_\mathcal{B}$), where each graph-level representation $x\in\mathbb{R}^{nF'}$ is obtained by global graph pooling.
In this paper, we chose global mean pooling as

\begin{equation}
\label{eq:gap}
    x=\frac{1}{N}\sum_{r=1}^{N}X^{cat}_r,
\end{equation}{}
where $X^{cat}_r \in \mathbb{R}^{nF'}$ is a $r$-th row vector of node representation matrix $X^{cat} \in \mathbb{R}^{N \times nF'}$.
Other global graph pooling methods can also be applied.

The graph-level representations are then concatenated and fed into a linear transformation layer to extract the co-attention vector as
\begin{equation}
\label{eq:alpha}
    \vec{\alpha}=W_\alpha[x_\mathcal{A}\|x_\mathcal{B}]+b_\alpha,
\end{equation}{}
where $W_\alpha$ and $b_\alpha$ are both trainable parameters with dimension of $W_\alpha\in\mathbb{R}^{2nF'\times2nF'}$ and $b_\alpha\in\mathbb{R}^{2nF'}$.
The extracted co-attention vector is then indexed as $\vec{\alpha}_\mathcal{A}=\vec{\alpha}_{0:nF'}\in\mathbb{R}^{nF'}$ and $\vec{\alpha}_\mathcal{B}=\vec{\alpha}_{nF':2nF'}\in\mathbb{R}^{nF'}$.
The multilayer perceptron (MLP) can also be used instead of a single linear transformation.
\newline

\textbf{Node selection using co-attention vector.}
The score of $r$-th node $Z_r\in\mathbb{R}$ is calculated by dot product of $X^{cat}_r\in\mathbb{R}^{nF'}$ and $\vec{\alpha}\in\mathbb{R}^{nF'}$.
All node scores $Z\in\mathbb{R}^{N}$ can be calculated as
\begin{equation}
    Z=\frac{X^{cat}\cdot \vec{\alpha}}{\|\vec{\alpha}\|},\quad\mbox{idx}=\mbox{TopK}(Z, \lceil kN \rceil )
\end{equation}
where $\cdot$ denotes dot product, TopK($\cdot$) function returns the indices of top $\lceil kN \rceil$ nodes according to $Z$, and $k \in (0,1]$ is a pooling ratio. 
We hold $\mbox{idx}_\mathcal{A}$ from $Z_\mathcal{A}$ and $\mbox{idx}_\mathcal{B}$ from $Z_\mathcal{B}$ for graphs $G_\mathcal{A}$ and $G_\mathcal{B}$, respectively.

Then, following the procedure of \textit{pooling by node selection} methods, we treat each node score as the significance of the corresponding node and select the top $\lceil kN \rceil$ nodes by indexing with $\mbox{idx}$ as
\begin{equation}
    X'=X_{\mbox{idx},:}^{cat} \odot Z_{\mbox{idx}}, \quad A'=A_{\mbox{idx,idx}},
    \label{eq:idx}
\end{equation}
where $(\cdot)_{\mbox{idx}}$ denotes the indexing operation and $\odot$ is the broadcasted elementwise product. $X'\in\mathbb{R}^{\lceil kN \rceil \times nF'}$ and $A' \in \mathbb{R}^{\lceil kN \rceil \times \lceil kN \rceil}$ are the node feature matrix and the adjacency matrix of a pooled graph $G'=(V',E')$, respectively.
Since $\vec{\alpha}$ is obtained from both $G_\mathcal{A}$ and $G_\mathcal{B}$, sub-graphs are extracted according to the interaction representation earlier denoted as $I_g(G_\mathcal{A},G_\mathcal{B})$.
Now we have $(X_{\mathcal{A}}',A_{\mathcal{A}}')$ for sub-graph $G'_{\mathcal{A}}$ and $(X_{\mathcal{B}}',A_{\mathcal{B}}')$ for sub-graph $G'_{\mathcal{B}}$.
\newline

\textbf{Prediction using sub-graphs.}
The sub-graphs obtained by Equation~(\ref{eq:idx}) are embedded again with graph convolution as
\begin{equation}
\label{eq:subnode}
    X'^{(l+1)}=\sigma(\tilde{D'}^{-\frac{1}{2}}\tilde{A'}\tilde{D'}^{-\frac{1}{2}}X'^{(l)}\Theta'^{(l)}).
\end{equation}{}
To feed to the final MLP layer, we convert the arbitrary sized representations from Equation~(\ref{eq:subnode}) into fixed-size vectors $x_\mathcal{A}^{final}, x_\mathcal{B}^{final}$  using the global graph pooling described in Equation (\ref{eq:gap}).
Then we concatenate the two graph-level representations and feed to the MLP layers as
\begin{equation}
    \mbox{output}=\mbox{MLP}([x_\mathcal{A}^{final} \| x_\mathcal{B}^{final}]).
\end{equation}{}
Depending on the task, appropriate functions (e.g. sigmoid, softmax) are applied to the final output.