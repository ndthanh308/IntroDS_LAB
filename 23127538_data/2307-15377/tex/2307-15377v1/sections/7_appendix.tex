\appendices

\section{\break Datasets}
\label{sec:datasets}
In this paper, we validate CAGPool on two tasks: a \textit{classification task} for Drug-Drug Interaction (DDI) prediction and a \textit{regression task} on the Graph Edit Distance (GED) dataset. 
Here, we provide additional information on the preprocessing details and analysis for each dataset.

\subsection{Drug-drug interaction dataset}
As described in the main text, we utilized the DDI dataset assembled by Decagon~\cite{zitnik2018modeling}. 
The original Decagon dataset contains Protein-Protein, Drug-Protein, and Drug-Drug pairs integrated into a graph structure. 
In this structure, each node represents either drugs or proteins, and the links indicate interactions between the two corresponding vertices. 
For our research, we focused solely on the drug-drug interaction subset. 
Each drug is represented as a single graph with atoms as nodes and chemical bonds as edges. 
The node attributes include the type of atoms, polarity, number of hydrogen atoms, and aromaticity. 
The original literature filtered for 500 or more drug pairs, but after our preprocessing, we found that to align with the 964 interaction types, we had to filter for 498 or more pairs.

\textbf{Negative Sampling}
To compensate for TWOSIDES, which only contains positive samples, we adopt the \textit{negative sampling} approach used in previous works~\cite{zitnik2018modeling,deac2020empowering}. 
The detailed process involving a drug $d$ and side-effect $se$ is described below:

\begin{itemize}
    \item During training, tuples ($\tilde{d_x},\tilde{d_y},se_z$), where $\tilde{d_x}$ and $se_z$ are chosen from the dataset and $\tilde{d_y}$ is chosen at random from the set of drugs difference from $d_y$ in the true samples ($\tilde{d_x},d_y,se_z$). The negative sample is selected randomly according to sampling distribution $P_r$, where for each node ${d_i}$ has a probability of $P(d_i)=\frac{f(d_i)^{3/4}}{\sum^{n}_{j=0}f(d_j)^{3/4}}$ of appearing \cite{mikolov2013distributed}.
    
    \item During validation and testing, we randomly sample two distinct drugs which do not appear in the positive dataset.
\end{itemize}


\subsection{Graph Edit Distance dataset}
We utilized the GED dataset from SimGNN\footnote{https://github.com/yunshengb/SimGNN}~\cite{bai2019simgnn}. 
The GED dataset comprises three sub-datasets: AIDS, LINUX, and IMDB. 
These datasets were designed and collected to evaluate the performance of the \textit{graph retrieval} task, a task designed to find similar/dissimilar graphs from the database when given a query graph.


We found that some graphs in GED datasets were in an equivalence relation (i.e. isomorphism) (see Figure \ref{fig:isomorphism}).
According to SimGNN authors, they treat the query graph as an unseen graph even if there exists an isomorphic graph in the database.
Because checking for isomorphism is expensive with traditional graph algorithms, an algorithm that can efficiently capture isomorphic graphs is important in this dataset.
Since our model is permutation invariant, it can benefit this task by capturing isomorphism efficiently.

\input{figures/isomorphism.tex}


\section{\break Comprehensive Overview of Graph Pooling Methods}
\label{sec:overview}

\input{figures/overview.tex}

In this section, we 1) explain the categorization of graph pooling methods and 2) compare them in terms of complexity. 
Hierarchical graph pooling methods can be classified into \textit{pooling by graph transformation} and \textit{pooling by node selection}. 
The overview is illustrated in Figure \ref{fig:overview}. There are various pooling methods based on graph algorithms such as spectral clustering \cite{defferrard2016convolutional}, but we only consider graph pooling methods that can be trained end-to-end. 
Our approach is based on the \textit{pooling by node selection} method.


Given the input graph $G=(V,E)$ with the set of vertices $V$ and the set of edges $E$, the hierarchical graph pooling can be represented as the function $g: G \mapsto G'$, where $G'=(V',E')$ is the small-size graph.
Graph $G$ has the node feature matrix $X \in \mathbb{R}^{N \times F}$ and the adjacency matrix $A \in \mathbb{R}^{N \times N}$, where $N$ is the number of nodes and $F$ is the feature dimension.
The final goal of the graph pooling method is to obtain the graph $G'=(V',E')$ that has the node features $X' \in \mathbb{R}^{N' \times F}$ and the adjacency matrix $A' \in \mathbb{R}^{N' \times N'}$.
We categorize the graph pooling methods according to how $X'$ and $A'$ are calculated.


\subsection{Pooling by graph transformation}
\label{subsec:graph_transform}
Pooling by graph transformation (also called as pooling by clustering) downsamples graph by learning the transformation matrix.
Following the work of Ying et al. \cite{ying2018hierarchical}, pooling methods in this category calculate the output node features $X'$ and the adjacency matrix $A'$ by using the transformation matrix $S \in \mathbb{R}^{N \times N'}$, which is called as the assignment matrix~\cite{ying2018hierarchical}.
Here, the matrix $S$ transforms not only nodes, but also edges.
Therefore, how we define $S$ is the main key in \textit{pooling by graph transformation}.
In DiffPool, the transformation matrix $S$ is obtained from the node embedding of Graph Neural Networks (GNNs).
In StructPool, the transformation matrix $S$ is trained via conditional random fields \cite{Yuan2020StructPool:}.
Briefly, \textit{pooling by graph transformation} methods have a common framework as
\begin{equation}
X' = S^\top X, \quad A' = S^\top A S,
\label{eq:iter}
\end{equation}
where $X' \in \mathbb{R}^{N' \times F}$ is the feature matrix of $N'$ cluster nodes and $A' \in \mathbb{R}^{N' \times N'}$ is the adjacency matrix of them.
Note that $G'=(V',E')$ is not a sub-graph of $G$ (i.e. $V' \not \subset V, E' \not \subset E $).

\subsubsection{Complexity issue of graph transformation}
Despite the improvement in their performance, pooling by graph transformation suffers from heavy computational cost when obtaining the new adjacency matrix $A'$.
DiffPool suffers from heavy computational complexity because the $S$ and the output graph is represented as a dense matrix.
Although $S$ of StructPool can be either dense or sparse according to their setting, it still suffers from heavy complexity because of the iterative method for $S$ and the calculation of $A'$ described in Equation (\ref{eq:iter}).

\subsection{Pooling by node selection}
\label{subsec:node_sel}
In \textit{Pooling by node selection}, $X'$ and its adjacency matrix $A'$ are obtained by selecting the nodes according to the node score $Z \in \mathbb{R}^{N}$ , leaving nodes with high scores and discarding the rest.
Because both $X'$ and $A'$ are simply calculated by indexing, these methods do not increase the upper bound of computational complexity in GNNs.
Pooling by node selection therefore eventually boils down to ``how we define the scoring function for each node? (i.e., how we define $Z$?)"
TopKPool calculate node scores $Z$ from the dot product of node features and a trainable projection vector \cite{gao2019graph}.
SAGPool exploit both node features and the graph topology to calculate node scores $Z$ \cite{pmlr-v97-lee19c}.
After the calculation of $Z$, both perform the indexing operation as
\begin{equation}
 X' = X_{\mbox{idx},:}, \quad  A' =  A_{\mbox{idx},\mbox{idx}},
\end{equation}
where $(\cdot)_{\mbox{idx}}$ denotes the indexing operation and $\mbox{idx}$ is the top-$k$ indices of node scores $Z$.
Unlike \textit{pooling by graph transformation}, $G'$ is a sub-graph of $G$ (i.e. $V' \subset V, E' \subset E$).
Because the nodes are explicitly selected, it would be helpful to interpret which local structures are important to increase the predictive power of GNNs.

\subsubsection{Why is CAGPool based on node-selection?}
As referred in Section \ref{subsec:graph_transform} and Section \ref{subsec:node_sel}, pooling by node selection alleviates the computational complexity issue of pooling by graph transformation such as DiffPool.
Since the main goal of our model is to design a low-compleixty model that can effectively and efficiently encode the interaction representation between the pair of input graphs, we follow the architectural design of \textit{pooling by node selection}.
For instance, if we adapt the graph DiffPool-like CAGPool which is described the below section, the complexity is quadratic to the number of nodes for each graph, $\mathcal{O}(|V_A|^2 + |V_B|^2)$.
Also we can identify the important substructure in the original topology that is difficult in \textit{Pooling by graph transformation}.
During the experiments in our main paper, we followed 50\% ratio pooling for fair comparison with other \textit{pooling by node selection} methods.

\subsubsection{Extension of CAGPool to a graph-transform version}
While pooling by node selection can be beneficial in terms of computational complexity and interpretability, it is also true that some nodes are not selected during the pooling process and are therefore discarded. 
On the other hand, pooling by graph transformation includes all nodes in the final aggregated clusters, without loss of information. 
CAGPool can be extended to a graph transformation version by setting the assignment matrix as follows:
\begin{equation}
    S = h(X_A, X_B)
\end{equation}
This means that the clustering is dynamically performed based on the representation of the pair of graphs.


\section{\break Comparison of the running time}
\label{sec:comparison}
To demonstrate the efficiency of our method, we compare the running time of node-level and graph-level interaction modules. 
Each module accepts $X_A$ and $X_B$ as an input pair and outputs $X'_A$ and $X'_B$, which represent the pooled node feature matrix. 
We set the number of nodes from 50 to 200 and repeat the process 10k times to obtain consistent results. The graph-level interaction module produces $X'_A$ and $X'_B$ 31.2 - 64.7 \% faster than the node-level interaction module.



\input{figures/figure_visualization}
\section{\break Visualization of the attention patterns}
\label{sec:comparison}
Figure \ref{fig:fig_visualization} showcases a relationship diagram that includes visualization of attention areas for an example of many-to-many interactions. 
We examined the true-positive cases of our model and highlighted the nodes with the highest attention scores. 
The percentage above the edge between each structure represents the confidence score of our prediction model. 
As shown in the figure, each drug is pooled and projected differently based on the drug it interacts with.