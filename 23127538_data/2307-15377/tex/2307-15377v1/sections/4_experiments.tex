\section{Experiments}
\label{sec:experiment}
We evaluate our method under two tasks: pairwise graph classification and pairwise graph regression.
In this section, we first outline the experimental settings and describe the datasets used to evaluate each task. 
Following that, we quantitatively compare our proposed CAGPool with the baseline methods to validate its effectiveness. 
During the training process, we use the Adam Optimizer with a learning rate of 1e-3 for 20 epochs and a batch size of 32. We fix the hidden dimension $F'$ via grid search across 32, 64, 128, and 256. 
All models, including the baselines, were implemented using PyTorch \cite{paszke2019pytorch} and PyTorch Geometric \cite{fey2019fast}. 
We carried out all experiments on a single NVIDIA Titan Xp GPU. 
The reported performance represents an average of the results from five repetitions.


\subsection{Tasks and Datasets}
\input{tables/auroc}

\textbf{Drug-drug interaction prediction.}
Polypharmacy refers to the simultaneous use of multiple drugs by a single patient to treat one or more conditions.
One of the key challenges with polypharmacy is that patients may experience unexpected side-effects when they take several drugs concurrently. 
These side-effects can lead to potentially devastating clinical and financial outcomes, including post-marketing withdrawal of drugs from the market \cite{onakpoya2016post}. 
Consequently, it is of paramount importance to accurately predict potential DDIs of drug candidates during the drug discovery process \cite{han2017synergistic}.

We utilized DDI dataset collected and used by Decagon \cite{zitnik2018modeling}. 
Decagon adopts a pathway-based approach, integrating Protein-Protein, Drug-Protein, and Drug-Drug interaction networks into a singular graph structure. 
In this structure, each node represents a drug or protein, while the links indicate the interactions between these entities. 
Following the experimental setup of \cite{deac2020empowering}, we exclusively used the Drug-Drug Interaction subset of this dataset. 
In our representation, each drug is depicted as an individual graph with atoms serving as nodes and chemical bonds as edges. 
The structural information for each drug compound is obtained using Rdkit\footnote{http://www.rdkit.org}. 
The ultimate goal of our experiment is to predict all types of DDI (if any) that might occur, using solely the structural information of two given drug compounds.
\input{tables/auprc}

We adopt the exact filtering steps employed by the baselines \cite{zitnik2018modeling,deac2020empowering} on the Decagon dataset, including negative sampling. 
A detailed explanation of the negative sampling process is provided in our supplementary material. 
We use the 964 types of polypharmacy side effects that occur more than 500 times. 
The complete dataset consists of 4,576,785 positive examples. 
We allocate 80\% of the interactions to the training set, 10\% to the validation set, and the remaining 10\% to the test set. 
The evaluation results are reported on the test set, for the model that achieved the best performance on the validation set. 
During the testing phase, we calculate the AUROC, AUPRC, and AP@50 across the 964 drug-drug interaction classes, exclusively for valid samples (either positive samples or those obtained via negative mining).
\newline

\input{tables/ged_results}
\textbf{Graph similarity prediction.}
Graph retrieval is a fundamental problem which involves calculating the distance or similarity between two graphs. 
The Graph Edit Distance (GED) is the most widely used distance metric for graph retrieval \cite{bai2019simgnn,bunke1983distance,ktena2017distance,liang2017similarity,zhao2013partition,zheng2013graph}. 
The edit distance is defined as the minimum number of operations needed to transform $G_\mathcal{A}$ into $G_\mathcal{B}$. 
However, computing the GED is known to be an NP-complete problem \cite{bunke1998graph}, making it infeasible to calculate the exact GED within a reasonable time frame for graphs that have more than 16 nodes \cite{blumenthal2018exact}.

Recently, there are attempts to approximate GED by using GNNs \cite{bai2019simgnn}.
The authors also provide GED datasets containing AIDS, LINUX, and IMDB \footnote{https://github.com/yunshengb/SimGNN}.
\textit{AIDS} \footnote{https://wiki.nci.nih.gov/display/NCIDTPdata} is commonly used in graph similarity search \cite{ktena2017distance,liang2017similarity,zhao2013partition,zheng2013graph}. 
AIDS dataset contains chemical compound structure graphs with labeled nodes.
Bai et al. \cite{bai2019simgnn} selected 700 graphs of equal or less than 10 nodes each. 
\textit{LINUX} \cite{wang2012efficient} dataset is about program dependence graphs generated from the Linux kernel.
In the program function graphs of the LINUX dataset, a node represents a statement and an edge is a dependency between two statements.
Bai et al. \cite{bai2019simgnn} selected 1000 graphs, each of which has equal or less than 10 nodes.
For both AIDS and LINUX datasets, the ground truth GEDs are calculated by using the $A^*$ algorithm.
\textit{IMDB} \cite{yanardag2015deep} dataset contains 1500 graphs of which the nodes represent actors or actresses.
Edges denote that two people appear in the same movie.
As in SimGNN, we use all the graphs in IMDB dataset for testing the scalability.
For the IMDB dataset, the approximation algorithms, Beam \cite{neuhaus2006fast}, Hungarian \cite{kuhn1955hungarian}, and VJ \cite{fankhauser2011speeding}, were used for the ground truth because the IMDB dataset contains graphs with more than 16 nodes.
GED is converted to similarity score $S$ with normalization ($nGED$) as follow: $nGED(G_\mathcal{A},G_\mathcal{B}) = \frac{GED(G_\mathcal{A},G_\mathcal{B})}{(|G_\mathcal{A}|+|G_\mathcal{B}|)/2}, S(GED(G_\mathcal{A},G_\mathcal{B})) =\exp^{-nGED(GED(G_\mathcal{A},G_\mathcal{B}))}$ ,where $|G_i|$ denotes the number of nodes of graph $G_i$ and the similarity score $S$ is in the range of (0,1].

We exactly follow their training/testing data split and randomly select validation set within a training set with the same proportion.
All the graphs are split into 60\%, 20\%, and 20\% as a training set, a validation set, and a testing set. 

\subsection{Evaluation and Baselines}
\textbf{Drug-drug interaction prediction.}
In the DDI prediction task, we assess the effectiveness of CAGPool by comparing it with existing methods using the Decagon dataset. 
We primarily employ three evaluation metrics used in Decagon~\cite{zitnik2018modeling}: AUROC, AUPRC, and AP@50.

Table \ref{tab:AUROC} presents a comparison of our network with existing baseline networks on the Decagon dataset. 
The \checkmark in the \textbf{feature+} column indicates that the method uses additional information beyond the structural information of drugs. 
The \textbf{Concatenated features} method~\cite{zitnik2018modeling} employs a PCA representation of the drug-target protein interaction matrix and individual drug side-effects. 
The \textbf{Decagon} method~\cite{zitnik2018modeling} further includes protein-protein interactions, drug-protein target interactions, and single drug side-effect information. Methods such as \textbf{MPNN-Concat}, \textbf{Late-Outer}, \textbf{CADDI}, \textbf{MHCADDI}~\cite{deac2020empowering} use one-hot encoding of interactions to predict their presence or absence. 
Our model outperforms all these methods, whether they use additional information or not, by only leveraging the structural features of the graph representation for drugs.

Table \ref{tab:eval_table} presents a performance comparison across AUROC, AUPRC, and AP@50 metrics on the Decagon dataset. 
Not only does our method outperform the baseline methods proposed in \cite{zitnik2018modeling}, but it also demonstrates consistently superior performance across all evaluation metrics. 
Both \textbf{RESCAL} and \textbf{DEDICOM} are tensor decomposition approaches applied to the drug-drug matrix, while \textbf{DeepWalk} employs neural embedding based on a random walk procedure.
\newline



\textbf{Graph similarity prediction.}

Table \ref{tab:GED_results} presents the regression performance on GED datasets. 
The baselines encompass GNN approaches reported in the GED-CDA paper \cite{10094975} and the SimGNN paper \cite{bai2019simgnn}. \textbf{SimGNN} \cite{bai2019simgnn}, \textbf{GraphSim} \cite{bai2020learning}, \textbf{GMN} \cite{li2019graph}, \textbf{MPNGMN} \cite{ling2020hierarchical}, \textbf{GENN} \cite{wang2021combinatorial}, and \textbf{GED-CDA} \cite{10094975} are characterized by their focus on node-wise interactions.
This results in a substantial computational burden due to their high complexity, which is at least $O(|V_A||V_B|)$.
For instance, \textbf{SimGNN} combines \textit{AttLearnableGC} and \textit{Pairwise Node Comparison}. 
To account for graph-graph interactions, \textit{SimGNN} utilizes the histogram information of the dot product of all node pairs between two graphs, an approach known as \textit{Pairwise Node Comparison}.
\textbf{SimpleMean (S-Mean)} generates a graph-level embedding by averaging the node representations. 
Both \textbf{HierarchicalMean (H-Mean)} and \textbf{HierarchicalMax (H-Max)} employ a graph coarsening algorithm for hierarchical graph representations \cite{defferrard2016convolutional}, and then apply global mean and max pooling, respectively.
In \textbf{AttDegree (Att.Deg.)}, the attention weight of nodes is calculated using the natural log. 
Both \textbf{AttGlobalContext (Att.GC)} and \textbf{AttLearnableGC (Att.LGC)} compute the attention weights using the graph-level representations. 
However, the latter also incorporates a learnable non-linear transformation, unlike the former.
SGNN \cite{ling2020hierarchical}, a Siamese architecture with GCN, has its performance reported in the GED-CDA paper. 
For a fair comparison, we maintain the model architecture of SimGNN and only replace its \textit{Pairwise Node Comparison} module with CAGPool.


