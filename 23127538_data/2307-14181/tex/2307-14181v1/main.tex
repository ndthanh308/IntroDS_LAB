\documentclass[sn-nature,Numbered]{sn-jnl}% 
%%%% Standard Packages
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{paralist}%
\usepackage{makecell}%
\usepackage{comment}%
\usepackage{multirow}
%%%%
\newcommand{\marti}[1]{{\color{purple}{#1}}}
\newcommand{\Antoine}[1]{{\color{blue}{#1}}}

%\newcommand{\commentA}[1]{{\color{blue}{#1}}}

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\newtheorem{assumption}{Assumption}

%\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Convex SIP algorithms with inexact separation oracles]{Convex semi-infinite programming algorithms with inexact separation oracles}

\author[1,2]{Antoine Oustry}\email{antoine.oustry@polytechnique.org}

\author[3]{Martina Cerulli}\email{mcerulli@unisa.it\vspace*{-1em}}


\affil[1]{\orgdiv{LIX}, \orgname{Institut Polytechnique de Paris}, \orgaddress{\city{Palaiseau}, \postcode{91120}, \country{France}}}

\affil[2]{\orgname{Ecole des ponts}, \orgaddress{\city{Marne-la-Vallée}, \country{France}}}


\affil[3]{\orgdiv{Department of Computer Science}, \orgname{University of Salerno}, \orgaddress{\postcode{84084}, \country{Italy}}\vspace*{-1em}}

\abstract{Solving convex Semi-Infinite Programming (SIP) problems is challenging when the separation problem, i.e., the problem of finding the most violated constraint, is computationally hard. %due to the complexity of the separation problem. 
We propose to tackle this difficulty by solving the separation problem approximately, i.e., by using an inexact oracle. Our focus lies in two algorithms for SIP, namely the Cutting-Planes (CP) and the Inner-Outer Approximation (IOA) algorithms. We prove the CP convergence rate to be in $O(1/k)$, where $k$ is the number of calls to the limited-accuracy oracle, if the objective function is strongly convex. Compared to the CP algorithm, the advantage of the IOA algorithm is the feasibility of its iterates. In the case of a semi-infinite program with Quadratically Constrained Quadratic Programming separation problem, we prove the convergence of the IOA algorithm toward an optimal solution of the SIP problem despite the oracle's inexactness.}

\keywords{Semi-Infinite Programming, Inexact Oracle, Separation problem\vspace*{-1.8em}}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}

Standard Semi-Infinite Programming (SIP) problems are optimization problems with a finite number of variables and infinitely many constraints. These constraints are indexed by a continuous parameter that takes values from a certain parameter set. Given two integers $m,n \in \mathbb{N}^*$, we consider two continuous functions $F \colon \mathbb{R}^m \to \mathbb{R}$ and $G \colon \mathbb{R}^m \times \mathbb{R}^n \to \mathbb{R}$. Furthermore, we consider two non-empty and compact sets $\mathcal{X} \subset \mathbb{R}^m$, and $\mathcal{Y} \subset \mathbb{R}^n$. The SIP problem on which we focus is formulated as\vspace{-0.8em}
\begin{align}
  \left\{\begin{array}{rl}\vspace*{-1mm}
    \min\limits_{x\in \mathcal{X}} & F(x)  \\
    \text{s.t.} & G(x,y) \leq 0 \quad \forall y \in \mathcal{Y},
    \label{eq:SIP}
    \tag{\mbox{$\mathsf{SIP}$}}
  \end{array}\right.
\end{align}
with the value function $\phi(x)$ defined as $\phi(x) =  \max_{y \in \mathcal{Y}}  G(x,y),$ also referred to as \textit{lower-level problem} or \textit{separation problem} in the rest of the paper.
The infinitely many constraints $ G(x,y) \leq 0, \,\forall y \in \mathcal{Y}$ in \eqref{eq:SIP} can be reformulated as $\phi(x) \leq 0$, where function~$\phi$ is continuous as a direct application of the Maximum Theorem \cite[Th.~2.1.6]{aubin_jean-pierre_viability_1991}. 
This paper restricts to the following convex setting for the formulation \eqref{eq:SIP}.
\begin{assumption}
The set $\mathcal{X}$ is convex, the function $F(x)$ is convex and the function $G(\cdot,y)$ is linear for any $y \in \mathcal{Y}$.
\label{as:convex}
\end{assumption}
We underline that we make no convexity assumptions regarding the set $\mathcal{Y}$ or the function $G(x, \cdot)$. 
This setting covers the case where the function $G(\cdot,y)$ is affine for any $y \in \mathcal{Y}$. %; indeed, we can add a decision variable $x_0$, and include the constraint $x_0 = 1$ in the definition of the compact convex set $\mathcal{X}$.
%, insofar as (i) $G$ is continuous, thus lower/upper semicontinuous, (ii) the set-valued map $\tilde{\mathcal{Y}}(x) = \mathcal{Y}$ is compact valued and lower/upper semicontinuous (since it is constant). We deduce that $\phi(x)$ is lower and upper semicontinuous, therefore continuous.
%\end{proof}
Because of the infinite number of constraints, SIP problems are challenging optimization problems, for the solution of which several methods have been developed in literature. Whenever the inner problem is convex and regular, it can be replaced by its KKT first-order optimality conditions \cite{stein2003interior,floudas2008adaptive,goberna1998}, obtaining a problem with complementarity constraints.
In the general case of nonconvex lower-level problems, however, this approach is not applicable. A valid alternative is the Iterative Discretization methods \cite{reemtsen1991discretization,hettich1986implementation,Schwientek2021}, consisting in replacing the infinite constraints with several finite constraints, by approximating the infinite parameter set with a finite subset. The original SIP problem can then be transformed into a sequence of finite optimization problems, which are solved using standard optimization techniques. By refining the discretization and solving the finite optimization problems iteratively, the solution to the original SIP problem can be obtained.
When the considered finite subset of constraints is increased at each iteration by adding the most violated constraint, the discretization method for convex SIP problems corresponds to the Kelley algorithm, also known as Cutting-Planes (CP) algorithm \cite{kortanek1993central,betro2004accelerated,tichatschke1988cutting}. 
Recently, in \cite{cerulli2022}, an Inner-Outer Approximation (IOA) algorithm is introduced to solve convex SIP problems by combining a CP and a lower-level dualization approach. 

In these methods, in order to address the infinite number of constraints, an optimization algorithm is employed to solve problem~$\phi(x)$ for specific values of $x$. However, this may be computationally difficult, and assuming that we solve it exactly is not necessarily realistic. Therefore, we may assume that an \textit{inexact separation oracle} (i.e., a black-box algorithm) is used to compute a feasible solution of the problem~$\phi(x)$ with a relative optimality gap of $\delta \in [0,1)$. More precisely, for any $x \in \mathcal{X},$ this ``$\delta$-oracle'' computes $\hat{y}(x) \in \mathcal{Y}$, and an upper bound $\hat{v}(x) \geq \phi(x)$ such that $
 \hat{v}(x)- G(x,\hat{y}(x)) \leq \delta \: \lvert \phi(x) \rvert.$
Consequently, the following inequalities hold:\vspace{-0.5em}
\begin{align}
\phi(x) -\delta \: \lvert \phi(x) \rvert \leq  G(x,\hat{y}(x)) \leq   \phi(x) \leq \hat{v}(x) \leq \phi(x) +\delta \: \lvert \phi(x)\rvert.
 %\phi(x) - G(x,\hat{y}(x)) \leq \delta \: \lvert \phi(x) \rvert, 
 %\label{eq:oraclecasea} %\\
 % \hat{v}(x) - \phi(x) \leq \delta \: \lvert \phi(x) \rvert, 
 \label{eq:oraclebound}
\end{align}

The literature on convex optimization algorithms using an inexact oracle is extensive. In \cite{daspremont2008}, the Fast Gradient Method proposed in \cite{nesterov2005} is extended to include the use of inexact gradient computation.  In \cite{devolder2014first}, the general concept of inexact oracle for convex problems is introduced and applied to First-Order methods, i.e., primal, dual, and fast gradient methods. The classical primal-dual gradient method can be seen as a slow method, but robust with respect to (w.r.t.) oracle errors. The fast one, instead, is faster but sensitive w.r.t.\ oracle error. In \cite{devolder2013intermediate}, the same authors propose the intermediate gradient method, combining classical and fast gradient methods, providing the flexibility to select a suitable parameter value that balances the convergence rate and the accumulation of oracle errors. Both stochastic and deterministic errors are considered in the oracle information in \cite{dvurechensky2016stochastic}.
In the literature on convex minmax problems (which can be seen as a particular type of SIP problem), the oracle's inexactness is rarely considered. The approximate calculation of the optimal solution of the inner problem has been explored in \cite{gaudioso2006,gaudioso2009,fuduli2015}, in the context of bundle algorithms. When dealing with nonsmooth convex SIPs, another bundle method with inexact oracle solving the inner problem is proposed in \cite{pang2016constrained}. 

Compared to these earlier works, the contribution of this paper is to (i) prove a rate of convergence for the CP algorithm, despite the inexactness of the separation oracle, in the context of a strongly convex objective function (ii) show that the IOA algorithm is able, once again despite the inexactness of the oracle, to generate a sequence of feasible points converging to an optimum. In other words, this paper extends the convergence results for the CP and IOA algorithms from \cite{cerulli2022} to the case of an inexact separation oracle, and for more general separation problems. As regards the CP algorithm with inexact oracle, we prove, under specific assumptions, a rate of convergence for the optimality gap, and for the feasibility error. As regards the dualization approach and the IOA algorithm, we trace the steps of \cite{cerulli2022}, in a broader setting. Indeed, we consider a lower level which is not a Quadratic Programming (QP), but a Quadratically Constrained QP (QCQP) problem; we propose a restriction of this SIP problem, which is a reformulation of it when its lower level is convex; we review the sufficient condition proposed in \cite{cerulli2022}, which may be verified a posteriori on a solution of this restriction, to check if it is in fact optimal for the original SIP problem; we present the IOA algorithm with inexact oracle, and we prove that it is convergent.



\begin{comment}
\section{The ID algorithm with inexact oracle}
A classical SIP approach consists in successively solving relaxations of the SIP problem obtained by replacing the infinite constraints set by a finite subset, which is gradually increasing over the iterations \cite{hettich1986implementation,reemtsen1991discretization,still2001discretization}. We introduce the pseudo-code of such an algorithm, in the case of an inexact separation oracle used to solve each finite problem.\vspace{-0.8em}
\begin{algorithm}[h!]
{\small
\caption{ID algorithm for \eqref{eq:SIP} with inexact oracle}
\label{alg:DA}
\begin{algorithmic}[1]
    \State{\textbf{Input:} Oracle with parameter $\delta \in [0,1)$, tolerance $\epsilon  \in \mathbb{R}_+$. Let $k\gets0$, $\mathcal{Y}^0 \gets \emptyset$, $\nu_0 \gets \infty$.}
    \While{$\nu_k  > \epsilon$}
    	\State Compute an optimal solution $x^k$ of the problem
    	\begin{align}
            \left\{ \begin{array}{rl}\label{eq:master_problem_DA}
          \min\limits_{x \in \mathcal{X}} & F(x) \\
                \text{s.t.} &    G(x,y) \leq 0, \quad \forall y \in \mathcal{Y}^k.
  \end{array}\right.
 \tag{\mbox{$R_k$}}
  \end{align}
	    \State Call the $\delta$-oracle to compute an approximate solution $y^{k} = \hat{y}(x^k)$ of $\phi(x^k).$
        \State {$\mathcal{Y}^{k+1} \gets \mathcal{Y}^{k} \cup \{ y^k \} $, $\;\nu_{k+1} \gets G(x^k,y^k)$}
	    \State {$k \gets k + 1$}
    \EndWhile
    \State  Return $x^k$.    \label{steptermID}
\end{algorithmic}}
\end{algorithm}

At each iteration of ID Alg.~\ref{alg:DA}, the relaxation~\eqref{eq:master_problem_DA} of \eqref{eq:SIP} is solved, and its solution is used to define the problem $\phi(x)$ which is approximately solved by the oracle.
Before proving the convergence of ID algorithm~\ref{alg:DA}, we introduce a Lemma that underlies the convergence of this type of ID algorithm, despite the inexactness of the oracle.
\begin{lemma} \label{lem:discretizationconv}
    Consider a parameter $\delta \in [0,1)$, the infinite sequences $(x^k)_{k \in \mathbb{N}} \subset \mathcal{X}$ and $(y^k)_{k \in \mathbb{N}}\subset \mathcal{Y}$, where $y^k = \hat{y}(x^k)$ is the output of the $\delta$-oracle evaluated at point $x^k$. If these sequences are such that, for any $k \in \mathbb{N}$,\vspace{-0.7em}
    \begin{align*}
        (*) \quad G(x^k,y^\ell) \leq 0, \quad \forall \ell \in \{0, \dots ,k - 1 \}, \vspace*{-0.5em}
    \end{align*}
then, the feasibility error $\phi(x^k)^+$ vanishes.
\end{lemma}
\begin{proof}
    %For any limit value $x \in \mathbb{R}^m$ of $(x^k)_{k \in \mathbb{N}}$, we notice first that $x$ belongs to $\mathcal{X}$, since this set is closed. 
    Let $t^+ = \max \{ t,0 \}$ be the positive part of function $t$. We notice that the sequence $\phi(x^k)^+$ is bounded, and thus admits at least one accumulation value $\ell$. We are going to prove that $\ell = 0$.
    Let $\psi:\mathbb{N} \to \mathbb{N}$ be an increasing function, such that $\phi(x^{\psi(k)})^+ \rightarrow \ell$. 
    By compactness of $\mathcal{X}$ (resp. $\mathcal{Y}$), %up to the extraction of a subsequence of $x^{\psi(k)}$ (resp. $y^{\psi(k)}$), 
    we also assume that $x^{\psi(k)} \rightarrow x \in \mathcal{X}$ (resp. $y^{\psi(k)} \rightarrow y \in \mathcal{Y}$). For $(*)$, $G(x^{\psi(k)},y^j) \leq 0$ for all $j \in \{0,\dots, \psi(k) - 1\}$; in particular, $G(x^{\psi(k)},y^{\psi(k-1)}) \leq 0$.
    We deduce that $G(x^{\psi(k)},y^{\psi(k)}) \leq  G(x^{\psi(k)},y^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k-1)}),$ and therefore, since the positive part of a function is non-decreasing,\vspace{-0.6em}
\begin{align}
     \Bigl( G(x^{\psi(k)},y^{\psi(k)})\Bigr)^+ \leq  \Bigl( G(x^{\psi(k)},y^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k-1)})\Bigr)^+.
     \label{eq:diffremark}
\end{align}
 According to the definition of the $\delta$-oracle, Eqs.~\eqref{eq:oraclebound} yields $\phi(x^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k)}) \leq \delta \lvert \phi(x^{\psi(k)}) \rvert$. If $\phi(x^{\psi(k)}) \geq 0$, this means that $ \phi(x^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k)}) \leq \delta \: \phi(x^{\psi(k)})$, i.e., $\phi(x^{\psi(k)}) \leq \frac{1}{1 - \delta} G(x^{\psi(k)},y^{\psi(k)})$. To also cover the case, $\phi(x^{\psi(k)}) < 0$, we can write $\phi(x^{\psi(k)})^+ \leq \frac{1}{1 - \delta} G(x^{\psi(k)},y^{\psi(k)})^+$.  As $\frac{1}{1-\delta} \geq 0$, we obtain from Eq.~\eqref{eq:diffremark} that $\phi(x^{\psi(k)})^+ \leq \frac{1}{1 - \delta} \Bigl( G(x^{\psi(k)},y^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k-1)})\Bigr)^+$.   
By continuity of the functions $G$, and $\phi$, and the positive part, we deduce that $\ell = \phi(x)^+ \leq \frac{1}{1 - \delta} \left( G(x,y) - G(x,y)\right)^+ = 0,$    
since $(x^{\psi(k)},y^{\psi(k)})$ and $(x^{\psi(k)},y^{\psi(k-1)})$ both converges towards $(x,y)$. We deduce that $\phi(x^k)^+ \rightarrow 0$. 
\end{proof}

We can now prove the convergence of Alg.~\ref{alg:DA}, in the following theorem. Based on it, we then prove in Corollary~\ref{corol} that, if Alg.~\ref{alg:DA} terminates in a finite number of iterations, it returns a solution that is feasible in \eqref{eq:SIP}.

\begin{theorem}
If $\epsilon > 0$, Alg.~\ref{alg:DA} terminates after a finite number of iterations. On the contrary, if $\epsilon = 0$, Alg.~\ref{alg:DA} generates an infinite sequence of iterates $(x^k)_{k \in \mathbb{N}}$, and any limit value $x \in \mathbb{R}^m$ of this sequence is an optimal solution of problem \eqref{eq:SIP}.
\end{theorem}
\begin{proof}
Let us consider fixed parameters $(\delta,\epsilon) \in [0,1) \times \mathbb{R}_+$, and assume, by contrapositive, that Alg.~\ref{alg:DA} generates an infinite sequence $(x^k)_{k \in \mathbb{N}}$, converging to an optimal solution of \eqref{eq:SIP}.%, and we show that $\epsilon = 0$. 

The generated sequences $(x^k)_{k \in \mathbb{N}} \subset \mathcal{X}$ and $(y^k)_{k \in \mathbb{N}}\subset \mathcal{Y}$ satisfy $y^k = \hat{y}(x^k)$, and Eq.~$(*)$. Hence, Lemma~\ref{lem:discretizationconv} yields that $\phi(x^k)^+ \rightarrow 0$. We take any limit value $x \in \mathbb{R}^m$ of $(x^k)_{k \in \mathbb{N}}$, and we define $(x^{\psi(k)})_{k \in \mathbb{N}}$ a subsequence converging to $x$. Since $\phi(x^{\psi(k)})^+$ is a subsequence of the vanishing sequence $\phi(x^k)^+$, and $\phi$ is continuous, we deduce that $\phi(x)^+ = 0$. Moreover, being $\mathcal{X}$ closed, $x \in \mathcal{X}$. In summary, $x$ is feasible in \eqref{eq:SIP}, i.e., $F(x) \geq \mathsf{val}\text{\eqref{eq:SIP}}$. Furthermore, as $x^{\psi(k)}$ is a solution of a relaxation of \eqref{eq:SIP}, we know that $F(x^{\psi(k)}) \leq \mathsf{val}\text{\eqref{eq:SIP}}$, and thus by continuity of $F$, $F(x) \leq \mathsf{val}\text{\eqref{eq:SIP}}$. Therefore, we proved that $F(x) = \mathsf{val}\text{\eqref{eq:SIP}}$, i.e., that $x$ is an optimal solution of \eqref{eq:SIP}.

Since the stopping criterion is not met, $G(x^{\psi(k)},y^{\psi(k)})^+> \epsilon \; \forall 
k \in \mathbb{N}$. As $0 \leq G(x^{\psi(k)},y^{\psi(k)})^+ \leq \phi(x^{\psi(k)})^+$, by continuity of $\phi$, $G(x^{\psi(k)},y^{\psi(k)})^+ \rightarrow 0$, and $\epsilon = 0$. %We conclude the proof by proving that $F(x) = \mathsf{val}\text{\eqref{eq:SIP}}$.
\end{proof}


\begin{corollary}\label{corol}
If Alg.~\ref{alg:DA} terminates after $K$ iterations, the iterate $x^K \in \mathcal{X}$, satisfies $G(x^K,y) \leq \frac{\epsilon}{1-\delta}$ for all $y \in \mathcal{Y}$, and has value $F(x^K) \leq \mathsf{val}\text{\eqref{eq:SIP}}$. 
\end{corollary}

\begin{proof}
We start noticing that, for any iterate $x^k$ of the algorithm, $F(x^k) \leq \mathsf{val}\text{\eqref{eq:SIP}}$ being $x^k$ a solution of a relaxation of \eqref{eq:SIP}. If Alg.~\ref{alg:DA} terminates after $K$ iterations, this means that $\nu^{K+1} \leq \epsilon$, i.e., $G(x^K,y^K) \leq \epsilon$. If we are in the case $\phi(x^K) \geq 0$, then, by property~\eqref{eq:oraclebound} of the $\delta$-oracle, we deduce that $\phi(x^K) - G(x^K,y^K) \leq \delta \:\phi(x^K)$, i.e., $\phi(x^K) \leq \frac{1}{1-\delta} G(x^K,y^K) \leq \frac{\epsilon}{1-\delta}$. If  $\phi(x^K) \leq 0$, inequality $\phi(x^K) \leq \frac{\epsilon}{1-\delta}$ also holds.
\end{proof}
\end{comment}


\section{Convergence rate for the CP algorithm with inexact oracle}
In the context of convex SIP, the CP algorithm \cite{blankenship1976infinitely,fang,kelley1960cutting,tichatschke1988cutting} is a classical solution method. %, insofar as, for any $y \in \mathcal{Y}$, the constraint $G(\cdot,y) \geq 0$ is a valid linear inequality. 
We prove a convergence rate for this algorithm, taking into account the inexactness of the separation oracle. For this purpose, we use Lagrangian duality to determine a dual of \eqref{eq:SIP} in Subsection~\ref{subsec:lagr}, and interpret this algorithm as a variant of the Frank-Wolfe algorithm applied to the dual problem of \eqref{eq:SIP} in Subsection~\ref{subsec:CP}.

\subsection{Lagrangian dual of the convex SIP problem}\label{subsec:lagr}

In line with Assumption~\ref{as:convex}, for any $y \in \mathcal{Y}$, we define $a(y) \in \mathbb{R}^m$ such that $G(x,y)= x^\top a(y)$. As $G$ is assumed to be continuous, we deduce the continuity of $a(\cdot)$. We also define the set $\mathcal{M} = \{  a(y)  \colon y \in \mathcal{Y} \}$ and $\mathcal{K} = \mathsf{cone}(\mathcal{M})$ the convex cone generated by $\mathcal{M}$. With this notation, program \eqref{eq:SIP} can be cast as
\begin{align}
  \left\{\begin{array}{rl}
    \min\limits_{x\in \mathcal{X}} & F(x)  \\
    \text{s.t.} & x^\top z  \leq 0 \quad \forall z \in \mathcal{M}.\vspace*{-0.5em}
    \label{eq:lsip}
    \tag{\mbox{$\mathsf{SIP'}$}}
  \end{array}\right.
\end{align}
We introduce the Lagrangian function $\mathcal{L}(x,z) = F(x) + x^\top z$, defined over $\mathcal{X} \times \mathcal{K}$, and we have that 
$\mathsf{val} \text{\eqref{eq:SIP}} = \min_{x \in \mathcal{X}} \sup_{z \in \mathcal{K}} \mathcal{L}(x,z)$. 
We notice that the Lagrangian is convex with respect to $x$, and linear with respect to $z$. Since the set $\mathcal{X}$ is compact and convex (Assumption~\ref{as:convex}) and the set $\mathcal{K}$ is convex too, Sion’s minimax theorem~\cite{sion1958general} is applicable and the following holds:\vspace{-0.5em}
\begin{align}
     \min_{x \in \mathcal{X}} \sup_{z \in \mathcal{K}} \mathcal{L}(x,z) =  \sup_{z \in \mathcal{K}} \min_{x \in \mathcal{X}}  \mathcal{L}(x,z). \label{eq:duality}
\end{align}\vspace{-0.6em}
The dual function is $\theta(z) = \min_{x \in \mathcal{X}} \mathcal{L}(x,z)$, and the dual optimization problem is\vspace{-0.4em}
\begin{align}
    \sup_{z \in \mathcal{K}} \theta(z).\vspace*{-0.5em}
    \tag{\mbox{$\mathsf{DSIP}$}}
    \label{eq:dualsip}
\end{align}
With this definition, Equation~\eqref{eq:duality} may be read as the absence of duality gap between the dual problems \eqref{eq:SIP} and \eqref{eq:dualsip}, i.e., $\mathsf{val} \text{\eqref{eq:SIP}} =  \mathsf{val} \text{\eqref{eq:dualsip}}$. We make further assumptions to guarantee the regularity of the dual problem. 
\begin{assumption}
The function $F(x)$ is $\mu$-strongly convex. \label{as:strongconvex}
\end{assumption}
The following lemma states that under these assumptions, the dual function is differentiable, with a Lipschitz continuous gradient.
\begin{lemma}
Under Assumptions~\ref{as:convex}-\ref{as:strongconvex}, the dual function $\theta(z)$ is differentiable, with gradient $\nabla \theta(z) = \arg\min\limits_{x \in \mathcal{X}} \mathcal{L}(x,z)$. The gradient $\nabla \theta(z)$ is $\frac{1}{\mu}$-Lipschitz continuous.
\label{lemma:smoothness}
\end{lemma}
\begin{proof}
    Proof in Appendix~\ref{app:smoothness}.
\end{proof}
%We now prove the following consequence of the $\frac{1}{\mu}$- of $\theta$.
\begin{lemma}
Under Assumptions~\ref{as:convex}-\ref{as:strongconvex}, for any $y,z \in \mathcal{K}$, for any $\gamma \geq 0$,\vspace{-0.8em}
\begin{align}
    \theta(z + \gamma y) \geq \theta(z) + (\nabla \theta(z)^\top y) \gamma - \frac{\lVert y \rVert^2}{2 \mu} \gamma^2.
\end{align}
\label{lemma:progress}\vspace*{-1.5em}
\end{lemma}
\begin{proof}
    The proof follows the proof of \cite[Lemma~3.4]{cerulli2022}. This comes directly from the $\frac{1}{\mu}$-Lipschitzness of $\nabla \theta$.
\end{proof}

We prove now that we can replace the sup operator with the max operator in the formulation \eqref{eq:dualsip}, under the following additional assumption.
\begin{assumption}
    There exists $\hat{x} \in \mathcal{X}$, such that $\hat{x}^\top a(y) < 0$ for all $y \in \mathcal{Y}$. \label{as:slater}
\end{assumption}
\begin{lemma}
Under Assumptions~\ref{as:convex}-\ref{as:slater}, problem \eqref{eq:dualsip} admits an optimal solution. \label{lemma:dualopti}
\end{lemma}
\begin{proof}
Proof in Appendix~\ref{app:dualopti}.
\end{proof}


\subsection{The CP algorithm with inexact oracle and its dual interpretation}\label{subsec:CP}
In the setting of convex SIP defined by Assumption~\ref{as:convex}, we propose the CP Alg.~\ref{alg:CP}.
%we tailor Alg.~\ref{alg:DA}, obtaining Alg.~\ref{alg:CP}, which is, from a convex optimization perspective, a CP algorithm. More precisely, we consider a variant of Alg.~\ref{alg:DA} that offers more flexibility with respect to the finite set of constraints involved in the master problem \eqref{eq:master_problem_DA}, as detailed in step \ref{step:management} of the pseudo-code of Alg.~\ref{alg:CP}. 

\begin{algorithm}[h!]
{\small
\caption{CP algorithm for \eqref{eq:lsip}, with constraints management}
\label{alg:CP}
\begin{algorithmic}[1]
    \State{\textbf{Input:} Oracle with parameter $\delta \in [0,1)$, tolerance $\epsilon \in \mathbb{R}_+$. Let $k\gets0$, $\mathcal{M}^0 \gets \emptyset$, and
     $\nu_0 \gets \infty$.}
     \While{$\nu_k  > \epsilon$}
        \State Compute an optimal solution $x^k$ of the relaxation
        \begin{align}
            \left\{ \begin{array}{rl}\label{eq:master_problem_CP}
          \min\limits_{x \in \mathcal{X}} & F(x) \\
                \text{s.t.} &   x^\top z \leq 0, \quad \forall z \in \mathcal{M}^k,
    \end{array}\right.
    \tag{\mbox{$R_k$}}
    \end{align}
    and compute $z^k = \sum_{z \in \mathcal{M}^{k}} \lambda_z z$, where $\lambda_z \in \mathbb{R}_+$ is the dual variable for the constraint $x^\top z \leq 0 $.
    \State Call the oracle to compute an approximate solution $y^{k} = \hat{y}(x^k)$ of $\max\limits_{y \in \mathcal{Y}} \: a(y)^\top x^k$. Define $\xi^k \gets a(y^k)$.
        \State {$\mathcal{M}^{k+1} \gets \mathcal{B}^{k} \cup \{ \xi^k \} $, where $\mathcal{B}^{k}$ is any finite subset of  $\mathsf{conv}(\mathcal{M}^k)$ s.t.\ $z^k \in \mathsf{cone}(\mathcal{B}^{k})$\label{step:management}}.
        \State{$\nu_{k+1} \gets (\xi^k)^\top x^k$}
	\State {$k \gets k + 1$}
     \EndWhile
    \State  Return $x^k$.    \label{steptermCP}
\end{algorithmic}}
\end{algorithm} 
We insist on three particular cases regarding the set $\mathcal{B}^k$ used in step~\ref{step:management} of Alg.~\ref{alg:CP}:
\begin{itemize}
    \item The set $\mathcal{B}^k$ may be equal to $\mathcal{M}^k$ at every step.
    %, in which case Alg.~\ref{alg:CP} coincides with Alg.~\ref{alg:DA} with the correspondence $\mathcal{M}^k = a(\mathcal{Y}^k)$
    \item The set $\mathcal{B}^k$ may be the set of atoms $z \in \mathcal{M}^k$ such that $\lambda_z > 0$, in which case step~\ref{step:management} consists in dropping all the inactive constraints. 
    \item The set $\mathcal{B}^k$ may be a subset of  $\mathsf{conv}(\mathcal{M}^k)$ of size at most $M$, and such that $z^k \in \mathsf{cone}(\mathcal{B}^k)$. For $M=2$, e.g., we can take $\mathcal{B}^k = \left\{ \frac{1}{\sum\limits_{z\in \mathcal{M}^{k}} \lambda_z} \sum\limits_{z\in \mathcal{M}^{k}} \lambda_z z \right\}$. In this case, following what is known as the constraint-aggregation approach \cite{kennedy2015}, we obtain a bounded-memory algorithm.
\end{itemize}
We can also think about intermediate approaches where we do not drop every inactive constraint, but only the ones that have been staying inactive for a given number of iterations. Such strategies are also included in the framework of Alg.~\ref{alg:CP}. 

We now show that, as described in \cite{cerulli2022} for the CP with exact oracle, Alg.~\ref{alg:CP} can be interpreted, from a dual perspective, as a cone-constrained fully corrective Frank–Wolfe (FCFW) algorithm \cite{locatello2017greedy} solving problem \eqref{eq:dualsip}. We prove that, during the execution of Alg.~\ref{alg:CP}, the dual vectors $z^k$ instantiate the iterates of an FCFW algorithm solving problem~\eqref{eq:dualsip}. The generic iteration $k$ is described in Table~\ref{tab:CPFW}. 

\begin{table}[h!]
    \centering 
    \scalebox{0.95}{
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
         & \makecell{Primal perspective:\\\textbf{CP}} & Link & \makecell{Dual perspective: \\ \textbf{FCFW}}\\
         \hline \hline
         &&&\\[-0.5em]
        \textit{Step 1} & \makecell{Solve \eqref{eq:master_problem_CP}, \\ store the solution $x^k$, \\ and the dual vector $z^k$} 
        & \makecell{Strong \\ duality} & \makecell{Solve the dual problem  \\
        $\max\limits_{z \in \mathsf{cone}(\mathcal{M}^k)} \theta(z),$\vspace{1mm}
        \\ compute the solution $z^k$, and \\ the gradient $\nabla \theta (z^{k}) = x^k$ } \\ 
        &&& \\[-0.5em]
        \hline 
        &&&\\[-0.5em]
        \textit{Step 2} & \makecell{Call the $\delta$-oracle to solve \\ $\max\limits_{y \in \mathcal{Y}} a(y)^\top x^k  ,$
  \vspace{1mm}\\ and store the solution $y^k$ } & \makecell{$\xi = a(y)$ \\$x^k = \nabla \theta (z^{k})$} & \makecell{Call the $\delta$-oracle to solve \vspace{1mm}\\ $\underset{y\in \mathcal{M}}{\max} \:  \xi^\top \nabla \theta (z^{k}),$ \vspace{1mm}\\ and store the solution $\xi^k$}\\
        &&&\\[-0.5em]
        \hline
        &&&\\[-0.5em]
         \textit{Step 3} & \makecell{$\mathcal{M}^{k+1} \gets \mathcal{B}^k \cup \{ \xi^k\}$}  &   & \makecell{$\mathcal{M}^{k+1} \gets \mathcal{B}^k \cup \{ \xi^k\}$} \\
         &&&\\[-0.5em]
         \hline
         &&&\\[-0.5em]
          \makecell{\textit{Stopping} \\ \textit{criterion}} & \makecell{$(\xi^k)^\top x^k \leq \epsilon$} & $x^k = \nabla \theta (z^{k})$ & \makecell{$ (\xi^k)^\top \nabla \theta (z^{k}) \leq \epsilon$}\\ 
          %,$ \\ and define $Y^{k}$ as $\underset{Y \in \mathsf{cone}(B_{k+1})}{\arg\max} \theta(Y)$.} \\
         &&& \\
         \hline
    \end{tabular}}
    \centering
   \caption{The $k$-th iteration of CP, and of the corresponding FCFW algorithm.}
    \label{tab:CPFW}
\end{table}

In particular:
\begin{itemize}
    \item \textit{Step 1}: At iteration $k$, the dual problem of \eqref{eq:master_problem_CP} is in fact a restriction of  \eqref{eq:dualsip} on $\mathsf{cone}(\mathcal{M}^k)$, which is a polyhedral subcone of $\mathcal{K}$, since the following holds:
    
    \begin{equation*}
    \begin{array}{rl}
        \underset{z \in \mathsf{cone}(\mathcal{M}^k)}{\max} \theta(z) & = \underset{z \in \mathsf{cone}(\mathcal{M}^k)}{\max}  \;\min\limits_{x \in \mathcal{X}} F(x) + x^\top z \\ 
         & =  \min\limits_{x \in \mathcal{X}} \; \underset{z \in \mathsf{cone}(\mathcal{M}^k)}{\max} F(x) + x^\top z \\
        & =  \min\limits_{x \in \mathcal{X}}  \{ F(x)\; \text{s.t.} \:  x^\top z \leq 0, \; \forall z \in \mathcal{M}^k \},\vspace*{-0.5em}
    \end{array}
    \end{equation*}
    which we recognize being the master problem \eqref{eq:master_problem_CP}. The absence of a duality gap is, also in this case, a direct application of Sion's Theorem \cite{sion1958general}. The new dual solution $z^k$ is obtained solving this restriction of \eqref{eq:dualsip} on $\mathsf{cone}(\mathcal{M}^k)$, and the primal solution $x^k=\arg\min\limits_{x \in \mathcal{X}} \mathcal{L}(x,z^k)$ gives the gradient of the dual function in $z^{k}$, i.e., $\nabla \theta (z^{k}) = x^k$ (see Lemma~\ref{lemma:smoothness}).
    \item \textit{Step 2}: We remark that $\max\limits_{y \in \mathcal{Y}} \: a(y)^\top x^k = \max\limits_{\xi \in \mathcal{M}} \: \xi^\top x^k = \max\limits_{\xi \in \mathcal{M}} \: \xi^\top \nabla \theta(z^k)$, since $\mathcal{M} = a(\mathcal{Y})$, and $\nabla \theta (z^{k}) = x^k$.
%    \item \textit{Step 3}: this steps maintains the property $\mathcal{M}^{k+1} = \{ a(y), y \in \mathcal{Y}^{k+1} \}$. We notice that by definition of $\mathcal{B}^k$, we have that $z^k \in \mathsf{cone}(\mathcal{V}^k) = \mathsf{cone}(a(\mathcal{B}^k))$.
\end{itemize}


\subsection{Convergence rate for the CP algorithm with inexact oracle}

We define the constant $R= \sup_{z \in \mathcal{M}} \lVert z \rVert = \sup_{z \in \mathsf{conv}(\mathcal{M})} \lVert z \rVert $. According to Lemma~\ref{lemma:dualopti}, problem~\eqref{eq:dualsip} admits an optimal solution $z^* \in \mathcal{K}$. We define $\tau = \inf \{ t \geq 0 \colon z^* \in t \: \mathsf{conv}(\mathcal{M}) \}$. This scalar plays a central role in the convergence rate analysis of the CP algorithm with inexact oracle, conducted in the following theorem.
\begin{theorem}
    Under Assumption~\ref{as:convex}-\ref{as:slater}, denoting by $x^*$ an optimal solution of \eqref{eq:lsip}, if Alg.~\ref{alg:CP} executes iteration $k \in \mathbb{N}$, then \vspace{-0.8em}
    \begin{align}
        F(x^*) - F(x^k) \leq \frac{2 \: R^2 \tau^2}{\mu \: (1-\delta)^2} \: \frac{1}{k+2}. \vspace*{-0.5em}\label{eq:optimality_gap} 
    \end{align} \label{th:convrate}\vspace{-1em}
\end{theorem}
\begin{proof}We define the optimality gap $\Delta_k = F(x^*) - F(x^k)  = \mathsf{val}\text{\eqref{eq:lsip}} - F(x^k)$. We emphasize that at each iteration $k$, $ \theta(z^{k}) = F(x^k)$, thus $\Delta_k$ may also be seen as the optimality gap in the dual problem~\eqref{eq:dualsip}, i.e., $\Delta_k = \mathsf{val}\text{\eqref{eq:dualsip}} - F(x^k) = \theta(z^*) - \theta(z^{k})$. We prove the inequality~\eqref{eq:optimality_gap} by induction. When $\tau = 0$ (i.e., $\textbf{0}$ is a dual optimal solution), inequality~\eqref{eq:optimality_gap} holds trivially, since $F(x^k)  = \mathsf{val}\text{\eqref{eq:lsip}}$. Assume that $\tau > 0$.
\paragraph{\textbf{Base case (}$k=0$\textbf{).}} Since $\theta$ is concave, $\Delta_0 =  \theta(z^*) - \theta(z^0)  \leq \nabla \theta(z^0)^\top  (z^* - z^0) = \theta(z^0)^\top  z^* ,$
with the last equality following from $z^0 = 0$ (as $\mathcal{M}^0 = \emptyset)$. We remark that $\nabla \theta(z^0)^\top  z^*  =  (\nabla\theta(z^0)-\nabla\theta(z^*))^\top  z^*$ since $\nabla\theta(z^*)^\top  z^* = 0$ by optimality of $z^*$. Hence, $\Delta_0 \leq (\nabla \theta(z^0) -  \nabla \theta(z^*))^\top  z^*  \leq \lVert \nabla \theta(z^0) -  \nabla \theta(z^*) \rVert \: \lVert z^* \rVert$, where the last inequality is the Cauchy-Schwartz inequality. Using the $\frac{1}{\mu}$-Lipschitzness of $\nabla \theta$ (Lemma~\ref{lemma:smoothness}), we know that $\lVert \nabla \theta(z^0) -  \nabla \theta(z^*) \rVert \leq \frac{1}{\mu} \lVert z^0 - z^* \rVert  = \frac{1}{\mu} \lVert z^* \rVert$. Since $z^* \in \tau \mathsf{conv}(\mathcal{M})$,
$\Delta_0 \leq \frac{1}{\mu} \lVert z^* \rVert^2 \leq \frac{(R\tau)^2}{\mu} \leq \frac{(R\tau)^2}{(1-\delta)^2\mu}$ as  $1-\delta \in (0,1]$.
\paragraph{\textbf{Induction.}} We suppose that the algorithm runs $k+1$ iterations and does not meet the stopping condition; we assume that property~\eqref{eq:optimality_gap} is true for $k$. Since $z^k \in \mathsf{cone}(\mathcal{B}^k)$, and $\mathcal{M}^{k+1} = \mathcal{B}^k \cup \{ \xi^k \}$, we deduce that $z^k + \gamma \xi^k \in \mathsf{cone}(\mathcal{M}^{k+1})$, for any $\gamma \geq 0$, implying $\theta(z^{k+1}) \geq \theta(z^k + \gamma \xi^k)$. Moreover, Lemma~\ref{lemma:progress} yields a lower bound on the progress made during iteration $k+1$:\vspace{-1em}
    \begin{align}
        \theta(z^{k+1}) \geq \theta(z^k + \gamma \xi^k) \geq \theta(z^k) + \gamma \: \nabla \theta(z^k)^\top  \xi^k  - \frac{\lVert  \xi^k \rVert^2}{2 \mu} \gamma^2,
    \end{align}
for any $\gamma \geq 0$. Multiplying by $-1$, adding $\theta(z^*)$ to both left and right-hand sides of the above inequality, and using $\lVert \xi^{k} \rVert \leq R$, we have that \vspace{-1em}
    \begin{equation}
        \Delta_{k+1} \leq \Delta_{k} - \gamma \: \nabla \theta (z^{k})^\top \xi^{k}   + \frac{R^2}{2\mu} \gamma^2,
         \label{eq:gamma}
    \end{equation}
for any $\gamma \geq 0$. In addition, by concavity of $\theta,$ $\Delta_k = \theta(z^*) - \theta(z^{k}) \leq  \nabla \theta (z^{k})^\top (z^* - z^{k})$. Note that we have $\nabla \theta (z^k)^\top z^k = 0$, following from the first-order optimality condition holding at $1$ of the differentiable function $\alpha(t) = \theta(t z^k)$. Indeed, $\alpha'(1) =(\nabla \theta (z^k))^\top z^k = 0$, because (i) $1$ is optimal for $w$ since $z^k \in \underset{z \in \mathsf{cone}(\mathcal{M}^k)}{\text{argmax}} \theta(z)$, (ii) $1$ lies in the interior of the definition domain of $\alpha$.

Thus, $ \Delta_k \leq \nabla \theta (z^k)^\top z^*$. As $z^* \in \tau\, \mathsf{conv}(\mathcal{M})$, 
    \begin{equation}
       \Delta_k \leq \max_{z \in \tau \mathsf{conv}(\mathcal{M})} \nabla \theta (z^k)^\top z = \tau \: \max_{z \in  \mathcal{M}} \nabla \theta (z^k)^\top z = \tau  \phi(x^k),
       \label{eq:control}
    \end{equation}
where the last equality follows from $\nabla \theta(z^k) = x$, and from the definition of the value function $\phi(x) = \max_{y \in \mathcal{Y}} x^\top a(y)$. The stopping criterion $a(y^k)^\top x^k = (\xi^k)^\top x^k \leq \epsilon$ is not met at the end of iteration $k$, as iteration $k+1$ is executed. Therefore, $\phi(x) \geq a(y^k)^\top x^k > \epsilon \geq 0$. Inequality~\eqref{eq:oraclebound} yields
$\phi(x^k)- G(x^k,y^k) \leq \delta \phi(x^k)$, i.e., $(1-\delta)\phi(x^k)  \leq  G(x^k,y^k) = (x^k)^\top a(y^k) = \nabla \theta(z^k)^\top \xi^k$. Therefore, as we have $\tau > 0$, we deduce from Eq.~\eqref{eq:control} that \vspace{-1em}
\begin{align}
     \frac{1-\delta}{\tau}\Delta_k \leq  \nabla \theta(z^k)^\top \xi^k.
    \label{eq:control2}
\end{align}    
Combining Eqs.~\eqref{eq:gamma} and \eqref{eq:control2}, we obtain $
 \Delta_{k+1} \leq \Delta_{k} - \gamma \frac{1-\delta}{\tau} \Delta_{k}  + \frac{R^2}{2 \mu} \gamma^2,$
for every $\gamma \geq 0$. Factoring and setting $\tilde{\gamma} = \gamma \frac{1-\delta}{\tau}$ (for any $\tilde{\gamma} \geq 0$) yields:\vspace{-0.8em}
\begin{equation}
     \Delta_{k+1} \leq (1 - \tilde{\gamma}) \Delta_{k}   + \frac{R^2 \tau^2}{2 \mu (1-\delta)^2} \tilde{\gamma}^2.
     \label{eq:progress}
\end{equation}
 Applying Eq.~\eqref{eq:progress} with $\tilde{\gamma} = \frac{2}{k+2}$, and defining $C = \frac{ 2 R^2 \tau^2}{ \mu (1-\delta)^2}$ we obtain:
{\small   \begin{align*}
     \Delta_{k+1} \leq (1 - \frac{2}{k+2}) \Delta_{k}   + \frac{C}{(k+2)^2}
     \leq \frac{k}{k+2} \frac{C}{k+2} + \frac{C}{(k+2)^2},
\end{align*}}
with the second inequality coming from the application of \eqref{eq:optimality_gap}, which holds for $k$ by the induction hypothesis. Finally, we deduce that
{\small  \begin{align*}
     \Delta_{k+1} \leq \frac{C}{k+2} (\frac{k}{k+2}  + \frac{1}{k+2}) \leq \frac{C}{k+2} \frac{k+1}{k+2} \leq \frac{C}{k+2} \frac{k+2}{k+3} = \frac{C}{k+3},
\end{align*}}
where the third inequality follows from the observation that $\frac{k+1}{k+2} \leq \frac{k+2}{k+3}$. Hence, the property \eqref{eq:optimality_gap} is true for $k+1$ as well. This concludes the proof. \end{proof}
We highlight that the main difference w.r.t.\ the convergence rate of the CP algorithm with exact oracle considered in \cite{cerulli2022} is exactly the term $\frac{1}{(1-\delta)^2}$, which is related to the inexactness of the oracle here considered.
The following theorem states that the lowest feasibility error of the iterates generated by the CP algorithm with inexact oracle follows a $O(\frac{1}{k})$ convergence rate.
\begin{theorem}
    Under Assumption~\ref{as:convex}-\ref{as:slater}, if Alg.~\ref{alg:CP} executes iteration $k$, for $k \geq 2$, then 
    \begin{align}
       \min_{0\leq \ell \leq  k } \phi(x^\ell) \leq \frac{27 \:  R^2 \tau}{4 \mu (1-\delta)^2} \: \frac{1}{k+2}.
    \end{align}
\end{theorem}
The following proof is inspired by previous work on the Frank-Wolfe algorithm \cite{jaggi2013revisiting}, with some adaptations to the dual problem \eqref{eq:dualsip}, a convex optimization problem on a cone instead of on a compact set for the tradition Frank-Wolfe algorithm. We adapt the proof to our framework of limited-accuracy oracle.
\begin{proof}
We keep the definition of the constant $C = \frac{ 2 R^2 \tau^2}{ \mu \: (1-\delta)^2}$, and we define the constants $\alpha = \frac{2}{3}$, $\beta = \frac{27}{8\tau}$, and $D = k +2$. Let us suppose that \vspace*{-1em}
\begin{align}
  \phi(x^\ell) > \frac{\beta C}{D}, \forall  \ell \in \{0, \dots k\}.
  \label{eq:assumptioninfeas}
\end{align}
We will show a contradiction. If iteration $k+1$ is not executed because the algorithm stopped at iteration $k$, we still define $z^{k+1} = \underset{z = z^k + \gamma \zeta^k , \gamma \geq 0 }{\text{argmax}} \theta(z)$, and $\Delta_{k+1} = \theta(z^*) - \theta(z^{k+1}) \geq 0$. Hence, regardless whether the iteration $k+1$ is executed or not, $\Delta_{\ell}$ and $\Delta_{\ell+1}$ are well defined for all $\ell \in \lbrace 0, \dots, k \rbrace$, and  we can apply Eq.~\eqref{eq:gamma} to deduce  $\Delta_{\ell+1} \leq \Delta_{\ell} - \gamma \: \nabla \theta (z^{\ell})^\top \xi^{\ell}   + \frac{R^2}{2\mu} \gamma^2$, for any $\gamma \geq 0$. Eq.~\eqref{eq:assumptioninfeas} implies that $\phi(x^\ell) >0$, and from Eq.~\eqref{eq:oraclebound}, we deduce that  $\phi(x^\ell)- G(x^\ell,y^\ell) \leq \delta \phi(x^\ell)$, i.e., $(1-\delta)\phi(x^\ell)  \leq  G(x^\ell,y^\ell) = (x^\ell)^\top a(y^\ell) = \nabla \theta(z^\ell)^\top \xi^\ell$. Combining this with Eq.~\eqref{eq:assumptioninfeas}, we deduce that $\nabla \theta(z^\ell)^\top \xi^\ell > \frac{(1 -\delta)\beta C}{D}$, and therefore, for any $\gamma \geq 0$, $\Delta_{\ell+1} < \Delta_{\ell} - \gamma \: \frac{(1 -\delta)\beta C}{D}   + \frac{R^2}{2\mu} \gamma^2$. Applying this inequality for $\gamma = \frac{\tau}{2(1-\delta)(\ell+2)} \geq 0$, we obtain
%As we have seen in the proof of Theorem~\ref{th:convrate}, if iteration $\ell + 1$ is executed, this means that the stopping criterion was not met at iteration $\ell$, implying that $\phi(x^\ell) \geq G(x^\ell,y^\ell)>0$. From Eq.~\eqref{eq:oraclebound}, we deduce that $\phi(x^\ell)- G(x^\ell,y^\ell) \leq \delta \phi(x^\ell)$, i.e., $(1-\delta)\phi(x^\ell)  \leq  G(x^\ell,y^\ell) = (x^\ell)^\top a(y^\ell) = \nabla \theta(z^\ell)^\top \xi^\ell$. Combining this with Eq.~\eqref{eq:assumptioninfeas}, we deduce that $\nabla \theta(z^\ell)^\top \xi^\ell > \frac{(1 -\delta)\beta C}{D}$, and therefore, for any $\gamma \geq 0$, \vspace*{-1em}
%\begin{align}
%    \Delta_{\ell+1} < \Delta_{\ell} - \gamma \: \frac{(1 -\delta)\beta C}{D}   + \frac{R^2}{2\mu} \gamma^2.
%\end{align}
%Applying this inequality for $\gamma = \frac{\tau}{2(1-\delta)(\ell+2)} \geq 0$, we obtain \vspace*{-0.8em}
\begin{align}
    \Delta_{\ell+1} < \Delta_{\ell} - & \frac{2 \tau \beta C}{(\ell +2) D}   + \frac{2 R^2 \tau^2}{\mu (1-\delta)^2} \frac{1}{(\ell+2)^2} = \Delta_{\ell} - \frac{2 \tau \beta C}{(\ell +2) D}   +  \frac{C}{(\ell+2)^2}. \label{eq:ineqC}
\end{align}
We define $k_{\min} = \lceil \alpha D \rceil - 2$, and we notice that $k_{\min} \geq 0$, since $D\geq 4$. Furthermore, for any $\ell \in \{ k_{\min}, \dots k \}$, $\alpha D \leq \ell + 2 \leq D$. Combining this with Eq.~\eqref{eq:ineqC}, we know that, for any $\ell \in \{ k_{\min}, \dots k \}$,\vspace*{-0.5em}
\begin{align}
    \Delta_{\ell+1} < \Delta_{\ell} - \frac{2 \tau \beta C}{D^2}   +  \frac{C}{\alpha^2 D^2} = \Delta_{\ell} + \frac{C}{D^2}(\frac{1}{\alpha^2}- 2\tau \beta).
\end{align}
Summing these inequalities for $\ell  \in \{ k_{\min}, \dots k \}$, we obtain \vspace*{-0.5em}
\begin{align}
    \Delta_{k+1} < \Delta_{k_{\min}} +  \frac{C (k + 1 - k_{\min} )}{D^2}(\frac{1}{\alpha^2} - 2\tau \beta),
\end{align}
and, using the bound on the objective gap at iteration $k_{\min}$ given by Theorem~\ref{th:convrate}, we have $\Delta_{k+1} < \frac{C}{k_{\min} + 2} + \frac{C (k +1 - k_{\min} )}{D^2}(\frac{1}{\alpha^2} - 2\tau \beta )$.
We notice that $k_{\min} + 2 \geq \alpha D$, and $ k + 1 - k_{\min} \geq (1-\alpha) D$. By definition of $\alpha$ and $\beta$, $\frac{1}{\alpha^2} - 2\tau \beta = \frac{9}{4}- \frac{27}{4} \leq 0$, and thus
\begin{align}
    \Delta_{k+1} < \frac{C}{\alpha D} + \frac{C (1-\alpha)}{D}(\frac{1}{\alpha^2} - 2\tau \beta ) = \frac{C}{\alpha D}( 1 + \frac{1-\alpha}{\alpha} - 2\alpha(1-\alpha) \tau \beta).
\end{align}
Using again that $\alpha = \frac{2}{3}$, we deduce that $\Delta_{k+1} < \frac{C}{\alpha D}(\frac{3}{2} - \frac{4}{9} \tau \beta)$. Since $\beta = \frac{27}{8\tau}$, we have $(\frac{3}{2} - \frac{4}{9} \tau \beta) = 0$. Therefore, we obtain $\Delta_{k+1}  < 0$, which contradicts the definition of $\Delta_{k+1}$. We can conclude that the assumption at Eq.~\eqref{eq:assumptioninfeas} cannot hold, and there exists $\ell \in \{0, \dots, k\}$ such that $\phi(x^\ell) \leq \frac{\beta C}{D} = \frac{27 \:  R^2 \tau}{4 \mu (1-\delta)^2} \: \frac{1}{k+2}$.
\end{proof}

\section{IOA algorithm with inexact oracle}

The CP algorithm provides iterates that are not feasible for the problem~\eqref{eq:SIP}, until the algorithm converges; feasibility is obtained only asymptotically. To overcome this limitation, the authors of \cite{cerulli2022} proposed an IOA algorithm that generates a minimizing sequence of points that are feasible in \eqref{eq:SIP}, in the case where the inner problem is a QP problem, solved through an exact separation oracle. We extend this algorithm to the case in which the inner problem is a QCQP problem, and the separation oracle is inexact. In particular, we consider the setting defined in the following assumption.

\begin{assumption} The parameterization of the SIP constraints satisfies the following:
    \begin{itemize}
    \item  Linear mappings $x \mapsto Q(x) \in \mathbb{S}_n$, $x \mapsto q(x) \in \mathbb{R}^n$ and $x \mapsto b(x) \in \mathbb{R}$ exist such that $$G(x,y) =  - \frac{1}{2} y^\top Q(x) y + q(x)^\top y + b(x).$$
    \item There exist $Q^1, \dots, Q^r \in \mathbb{S}_n$, $q^1, \dots, q^r \in \mathbb{R}^n$ and $b_1, \dots, b_r \in \mathbb{R}$ such that $$\mathcal{Y} = \left \lbrace y \in \mathbb{R}^n \colon \frac{1}{2} y^\top Q^j y +  (q^j) ^\top y + b_j \leq 0, \forall j \in \{1, \dots, r \} \right \rbrace.$$ We also assume to know $\rho \geq 0$ such that $\mathcal{Y} \subset B(0, \rho)$.
   \end{itemize}   \label{as:subpbqcqp}
\end{assumption}
In the context of Assumption~\ref{as:subpbqcqp}, a possible way to deal with the SIP problem \eqref{eq:SIP} is what is called \textit{lower-level dualization approach} in \cite{cerulli2022}, which consists in replacing the constraint involving the QCQP inner problem with one involving its dual. In particular, we consider a strong dual of a Semidefinite Programming (SDP) relaxation of the inner problem (or a reformulation if the latter is convex). In Subsection~\ref{subsec:sdprelax}, we introduce the classical SDP relaxation of the inner problem (reformulation, if it is convex) regularized by a ball constraint, and then, we introduce the SDP dual of this relaxation (reformulation, resp.). In Subsection~\ref{subsec:restr_reform} we present the finite formulation~\eqref{eq:SIP:convexreform}, obtained by applying the \textit{lower-level dualization approach} to the problem~\eqref{eq:SIP}. This formulation
is a reformulation of \eqref{eq:SIP} if $Q^1, \dots, Q^r$ are Positive Semidefinite (PSD) and $Q(x)$ is PSD for any $x \in \mathcal{X}$. Otherwise, an a posteriori sufficient condition on a computed solution $\bar{x}$ of \eqref{eq:SIP:convexreform} introduced in Subsection~\ref{sec:sufficientcondition} can be verified. If $\bar{x}$ satisfies such a condition, one can state that it is an optimal solution of \eqref{eq:SIP}. If not, the IOA algorithm is proposed in Subsection~\ref{sec:IO}, which generates a sequence of converging feasible solutions of \eqref{eq:SIP:convexreform}.


\subsection{SDP relaxation/reformulation of the inner problem} \label{subsec:sdprelax}

In this section, we reason for any fixed value of the decision vector $x \in \mathcal{X}$. The corresponding inner problem $\max_{y \in \mathcal{Y}} G(x,y)$ is the following QCQP problem
\begin{equation}\label{eq:QCQP}
    \left\{ \begin{array}{cll}
         \max \limits_{y \in \mathbb{R}^n} &  -\frac{1}{2} y^\top Q(x) y + q(x)^\top y +b(x) & \\
         \text{s.t.} & \frac{1}{2} y^\top Q^j y +  (q^j) ^\top y + b_j \leq 0 & \forall j \in \{ 1, \dots, r \}.
    \end{array}     \tag{\mbox{$\mathsf{P}_x$}} \right.
\end{equation} 
We define the linear matrix operator $\mathbf{\mathcal{Q}}(x) = \frac{1}{2} \begin{pmatrix} -Q(x) & q(x) \\ q(x)^\top & 2 b(x) \end{pmatrix} \in \mathbb{S}_{n+1}$, and the matrices $\mathbf{\mathcal{Q}}^j = \frac{1}{2} \begin{pmatrix} Q^j & q^j \\ (q^j)^\top & 2 b_j \end{pmatrix} \in \mathbb{S}_{n+1}$, for $j \in \{1, \dots, r\}$, the identity matrix $I_{n+1} \in  \mathbb{S}_{n+1}$, as well as $E \in \mathbb{S}_{n+1}$ defined as the matrix of the canonical basis associated with the indices $(n+1,n+1)$. With this notation, we introduce the following SDP problem
\begin{equation}\label{eq:SDP_relax}
  \left\{  \begin{array}{cll}
         \max \limits_{Y \in \mathbb{S}_{n+1}} & \langle  \mathbf{\mathcal{Q}}(x), Y \rangle  & \\
         \text{s.t.} & \langle \mathbf{\mathcal{Q}}^j, Y \rangle  \leq  0 &  \forall j \in \{ 1, \dots, r \} \\
         & \langle I_{n+1}, Y \rangle \leq  1+\rho^2 & \\
         & \langle E, Y \rangle = 1 & \\
         & Y \succeq  0. &
    \end{array} \tag{\mbox{$\mathsf{SDP}_x$}} \right.
\end{equation}
Under Assumption~\ref{as:subpbqcqp}, this problem is a relaxation of \eqref{eq:QCQP}. If the latter problem is convex, i.e., $Q^1, \dots, Q^r$ are PSD, and $Q(x)$ is PSD for any $x\in \mathcal{X}$, both problems have the same optimal objective value \cite{vandenberghe1996semidefinite,boyd1997semidefinite}. The following SDP problem
\begin{equation}
   \left\{ \begin{array}{cl}
         \min \limits_{\lambda, \alpha, \beta} &  \alpha (1+\rho^2) + \beta\\
         \text{s.t.} &  \sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j  + \alpha I_{n+1} + \beta E \succeq \mathbf{\mathcal{Q}}(x) \\
         & \lambda \in \mathbb{R}^r_+, \: \alpha \in \mathbb{R}_+, \beta \in \mathbb{R},
    \end{array}
    \tag{\mbox{$\mathsf{DSDP}_x$}} \right.
    \label{eq:DSDP}
\end{equation}
is the dual of problem \eqref{eq:SDP_relax}, as the following lemma states. 
\begin{lemma}\label{lemma:strong_duality}
Formulations \eqref{eq:SDP_relax} and \eqref{eq:DSDP} are a primal-dual pair of SDP problems and strong duality holds, thus $\mathsf{val}\text{\eqref{eq:SDP_relax}} = \mathsf{val}\text{\eqref{eq:DSDP}}.$
\end{lemma}
\begin{proof}
Proof in Appendix~\ref{app:strong_duality}.
\end{proof}

\subsection{SDP restriction/reformulation of the SIP problem}\label{subsec:restr_reform}

Leveraging on Section~\ref{subsec:sdprelax}, which focuses on the inner problem~\eqref{eq:QCQP}, its SDP relaxation~\eqref{eq:SDP_relax} and the respective dual problem~\eqref{eq:DSDP}, we propose a single-level finite restriction of problem~\eqref{eq:SIP}. It is a reformulation of \eqref{eq:SIP} if $Q^1, \dots, Q^r$ are PSD, and $Q(x)$ is PSD for any $x \in \mathcal{X}$.

\begin{theorem}\label{th:finiteformulation}
Under Assumption~\ref{as:subpbqcqp}, the finite formulation
{\small\begin{align}
\left\{ \begin{array}{rl}
  \min\limits_{x,\lambda,\alpha,\beta} & F(x)   \\
  \text{s.t.} &   \alpha (1 + \rho^2) + \beta \leq 0  \\
  & \sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j  + \alpha I_{n+1} + \beta E - \mathbf{\mathcal{Q}}(x) \succeq 0\\
& x \in \mathcal{X}, \lambda \in \mathbb{R}^r_+, \; \alpha \in \mathbb{R}_+, \beta \in \mathbb{R},
  \label{eq:SIP:convexreform}
  \tag{\mbox{$\mathsf{SIPR}$}}
  \end{array}\right.
  \end{align}} 
is a restriction of problem \eqref{eq:SIP}. If $Q^1, \dots, Q^r$ are PSD, and if $Q(x)$ is PSD for any $x \in \mathcal{X}$, then the finite formulation \eqref{eq:SIP:convexreform} is a reformulation of \eqref{eq:SIP}.
\end{theorem}
\begin{proof}
Let $\mathsf{Feas}$\eqref{eq:SIP} and $\mathsf{Feas}$\eqref{eq:SIP:convexreform} be the feasible sets of \eqref{eq:SIP} and \eqref{eq:SIP:convexreform}, respectively. Since \eqref{eq:SIP} and \eqref{eq:SIP:convexreform} share the same objective function, proving for any $x\in \mathcal{X}$ the implication \vspace*{-0.8em}
 \begin{equation}\label{eq:feas}
     \left( \exists \; \lambda \in \mathbb{R}^r_+, \; \alpha \in \mathbb{R}_+,\; \beta \in \mathbb{R} :\; (x,\lambda,\alpha,\beta) \in  \mathsf{Feas}\text{\eqref{eq:SIP:convexreform}} \right)\; \Longrightarrow \; x \in \mathsf{Feas}\text{\eqref{eq:SIP}},
 \end{equation} 
will prove the first part of the theorem. For any $x\in \mathcal{X}$, we have:\vspace*{-0.8em}
\begin{equation}
    \label{eq:implication}
          \mathsf{val}\text{\eqref{eq:SDP_relax}}   \leq 0
     \Longrightarrow 
         \mathsf{val}\text{\eqref{eq:QCQP}} \leq 0
     \iff x \in \mathsf{Feas}\text{\eqref{eq:SIP}},
\end{equation}
where the implication stems from the fact that $\mathsf{val}$\eqref{eq:QCQP} $\leq \mathsf{val}$\eqref{eq:SDP_relax}. Applying the strong duality Lemma~\ref{lemma:strong_duality}, we obtain that, for any $x \in \mathcal{X}$, 
{\small\begin{align}\label{eq:impl2}
    \mathsf{val}\text{\eqref{eq:SDP_relax}} \leq 0 &  \iff \mathsf{val}\text{\eqref{eq:SDP_relax}} \leq 0 \\
     & \iff %\left\{\begin{array}{l}
         %G(x) \leq 0 \\ 
          \exists \; \lambda \in \mathbb{R}^r_+, \; \alpha \in \mathbb{R}_+,\; \beta \in \mathbb{R} : \left\{\begin{array}{l} \alpha (1 + \rho^2) + \beta \leq 0 \\ \sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j  + \alpha I_{n+1} + \beta E - \mathbf{\mathcal{Q}}(x) \succeq 0 \: . \end{array} \right.  \\
     & \iff \exists \; \lambda \in \mathbb{R}^r_+, \; \alpha \in \mathbb{R}_+,\; \beta \in \mathbb{R},\; (x,\lambda,\alpha,\beta) \in  \mathsf{Feas}\text{\eqref{eq:SIP:convexreform}}. \label{eq:impl3}
\end{align}}

The equivalence \eqref{eq:impl3}, together with implication \eqref{eq:implication}, prove the implication \eqref{eq:feas}.

If $Q^1, \dots, Q^r$ are PSD, and if $Q(x)$ is PSD for any $x \in \mathcal{X}$, we can replace the implication \eqref{eq:implication} by the equivalence
$ \mathsf{val}\text{\eqref{eq:SDP_relax}} \leq 0  \iff           \mathsf{val}\text{\eqref{eq:QCQP}} \leq 0   \iff x \in \mathsf{Feas}$\eqref{eq:SIP}. 
This, together with equivalence \eqref{eq:impl3}, proves that\vspace{-0.5em}
\begin{equation*}
\exists \; \lambda \in \mathbb{R}^r_+, \; \alpha \in \mathbb{R}_+,\; \beta \in \mathbb{R} :\; (x,\lambda,\alpha,\beta) \in  \mathsf{Feas}\text{\eqref{eq:SIP:convexreform}} \; \iff \; x \in \mathsf{Feas}\text{\eqref{eq:SIP}},\vspace{-0.5em}
\end{equation*} 
i.e., \eqref{eq:SIP:convexreform} is a reformulation of \eqref{eq:SIP}, having the same objective function.
\end{proof}

Note that under Assumptions~\ref{as:convex} and \ref{as:subpbqcqp}, the finite formulation \eqref{eq:SIP:convexreform} is convex. 

\subsection{Optimality of the restriction: a sufficient condition}\label{sec:sufficientcondition}

Theorem~\ref{th:finiteformulation} states that the single-level finite formulation \eqref{eq:SIP:convexreform} is an exact reformulation of the problem \eqref{eq:SIP}, if $Q^1,\dots Q^r$ are PSD, and $Q(x) \succeq 0$ for all $x \in \mathcal{X}$. Even if this \textit{a priori} condition is not satisfied for all $x\in \mathcal{X}$, as what is done in \cite{cerulli2022} in a different setting, an \textit{a posteriori} condition on the computed solution $\bar{x}$ of \eqref{eq:SIP:convexreform} enables us to state that $\bar{x}$ is an optimal solution of \eqref{eq:SIP}.

\begin{theorem}
\label{th:restriction_optimality}
Assuming Assumption~\ref{as:subpbqcqp} holds, and $Q^1, \dots, Q^r$ are PSD, let $\bar{x}$ be a solution of the single-level formulation \eqref{eq:SIP:convexreform}. If $Q(\bar{x}) \succ 0$, then $\bar{x}$ is optimal in \eqref{eq:SIP}.
\end{theorem}
\begin{proof}
Given a closed convex set $S$, according to Def.~5.1.1 in \cite[Chap.~III]{hiriart2013convex}, the tangent cone to $S$ at $x$ (denoted by $T_S(x)$) is the set of directions $u \in \mathbb{R}^m$ such that there exist a sequence $(x_k)_{k \in \mathbb{N}}$ in $S$, and a positive sequence $(t_k)_{k \in \mathbb{N}}$ s.t.\ $t_k \rightarrow 0$ and $\frac{x_k - x}{t_k} \rightarrow u$. Moreover, according to Def.~5.2.4 in \cite[Chap.~III]{hiriart2013convex}, the normal cone $N_S(x)$ to $S$ at $x$ is the polar cone of the tangent cone $T_S(x)$, i.e., $N_S(x) = T_S(x)^\circ$. We define the closed convex set $C$ (resp.\ $\hat{C}$) as the feasible set of formulation \eqref{eq:SIP} (resp.\ \eqref{eq:SIP:convexreform}).

Since $Q(\bar{x}) \succ 0$, being the set of positive definite matrices open and $Q(x)$ continuous, there exists $r >0$ s.t.\ for all $x$ in the open ball of radius $r$ with center $\bar{x}$ (denoted by $B(\bar{x},r)$) $Q(x) \succeq 0$. This means that for all $x$ in $\mathcal{X} \cap B(\bar{x},r)$, $\mathsf{val}\text{\eqref{eq:QCQP}} = \mathsf{val}$\eqref{eq:SDP_relax}. Hence, we deduce that, for any $x \in \mathcal{X} \cap B(\bar{x},r)$, $x$ is feasible in \eqref{eq:SIP} if and only if $x$ is feasible in \eqref{eq:SIP:convexreform}. In other words, $C \cap B(\bar{x},r) = \hat{C} \cap B(\bar{x},r)$. According to the aforementioned definition of the tangent and normal cones, we further deduce that $T_C(\bar{x}) = T_{\hat{C}}(\bar{x})$, and $N_C(\bar{x})= T_C(\bar{x})^\circ =T_{\hat{C}}(\bar{x})^\circ =  N_{\hat{C}}(\bar{x})$.

We know that $\bar{x}$ is optimal in \eqref{eq:SIP:convexreform}, i.e., $\bar{x} \in \arg\min_{x \in \hat{C}} F(x)$. Since $F$ is a finite-valued convex function, and $\hat{C}$ is a closed and convex set, the assumptions of Theorem~1.1.1 in \cite[Chap.~VII]{hiriart2013convex} hold, and we can deduce that $0 \in \partial F(\bar{x}) +  N_{\hat{C}}(\bar{x})$. Using the equality $N_C(\bar{x}) = N_{\hat{C}}(\bar{x})$, we have that $0 \in \partial F(\bar{x}) +  N_{C}(\bar{x})$ too. Applying the same theorem with the closed and convex set $C$, we know that $0 \in \partial F(\bar{x}) +  N_{C}(\bar{x})$ implies that $\bar{x} \in \arg\min\limits_{x \in C} F(x)$, meaning that $\bar{x}$ is optimal in \eqref{eq:SIP}.
\end{proof}


\subsection{The IOA algorithm}\label{sec:IO}

If neither the lower level is convex, nor the sufficient optimality condition in Theorem~\ref{th:restriction_optimality} is satisfied, we do not directly obtain an optimal solution of problem~\eqref{eq:SIP} by solving the finite formulation \eqref{eq:SIP:convexreform}. Yet, in this section, we present an algorithm based on the lower-level dualization approach (presented above) and on an inexact separation oracle, that allows us to construct a minimizing sequence of feasible solutions of problem~\eqref{eq:SIP}.

For $k \in \mathbb{N}^{+}$, we consider two finite sequences $x^{1}, \dots, x^{k-1} \in \mathcal{X}$, and $v_{1}, \dots, v_{k-1} \in \mathbb{R}$ s.t.\ $v_\ell$ is an upper bound on $\mathsf{val}(\mathsf{P}_{x^{\ell}})$, given by a $\delta$-oracle. Since, for all $\ell = 1, \dots, k-1$, the inequality $-\frac{1}{2}y^\top Q(x^{\ell})y +q(x^{\ell})^\top y + b(x^\ell) \leq v_\ell,$ (i.e., $\langle \mathbf{\mathcal{Q}}(x^\ell), Y \rangle  \leq  v_\ell$) holds for any $y \in \mathcal{Y}$, the following SDP problem is still a relaxation of \eqref{eq:QCQP}, for any $x \in \mathcal{X}$:

\begin{equation}\label{eq:SDPk_relax}
\left\{  \begin{array}{cll}
     \max \limits_{Y \in \mathbb{S}_{n+1}} & \langle  \mathbf{\mathcal{Q}}(x), Y \rangle  & \\
     \text{s.t.} & \langle \mathbf{\mathcal{Q}}^j, Y \rangle  \leq  0 &  \forall j \in \{ 1, \dots, r \} \\
     & \langle \mathbf{\mathcal{Q}}(x^\ell), Y \rangle  \leq  v_\ell &  \forall \ell \in \{ 1, \dots, k-1 \} \\
     & \langle I_{n+1}, Y \rangle \leq  1+\rho^2 & \\
     & \langle E, Y \rangle = 1 & \\
     & Y \succeq  0. &
\end{array} \tag{\mbox{$\mathsf{SDP}^k_x$}} \right.
\end{equation}
We recall that the value of \eqref{eq:QCQP} is $\phi(x) = \max_{y \in \mathcal{Y}} G(x,y)$. For ease of reading, we also denote by $\phi_\mathsf{SDP}(x)$ the value of \eqref{eq:SDP_relax}, and by $\phi_\mathsf{SDP}^k(x)$ the value of \eqref{eq:SDPk_relax}. We underline that function $\phi$ implicitly depends on the sequences $(x^\ell)_{1\leq \ell \leq k}$ and $(v_\ell)_{1\leq \ell \leq k}$. Being $\zeta_\ell$ the Lagrangian multiplier associated to the constraint $\langle \mathbf{\mathcal{Q}}(x^\ell), Y \rangle \leq v_\ell$, the strong SDP dual of problem~\eqref{eq:SDPk_relax} is
\begin{equation}
\left\{ \begin{array}{cl}
     \min \limits_{\lambda, \alpha, \beta,\zeta} &  \alpha (1+\rho^2) + \beta + \sum_{\ell = 1}^{k-1} \zeta_\ell v_\ell\\
     \text{s.t.} &  \sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j +  \sum_{\ell = 1}^{k-1} \zeta_\ell \mathbf{\mathcal{Q}}(x^\ell) + \alpha I_{n+1} + \beta E \succeq \mathbf{\mathcal{Q}}(x) \\
     & \lambda \in \mathbb{R}^r_+, \: \alpha \in \mathbb{R}_+, \beta \in \mathbb{R}, \zeta \in \mathbb{R}^{k-1}_+
\end{array}
\tag{\mbox{$\mathsf{DSDP}^k_x$}} \right.
\label{eq:DSDPk}
\end{equation}
Hence, for any $\hat{x} \in \mathcal{X}$, $\phi_\mathsf{SDP}^k(\hat{x}) \leq 0$ holds if and only if $\hat{x} \in \mathcal{R}^k$, where $\mathcal{R}^k$ is defined as
\begin{align*}
  \mathcal{R}^k = \left \lbrace \hat{x} \in \mathbb{R}^m \colon    \exists  (\lambda,\alpha,\zeta) \in \mathbb{R}^{r+k}_+,  \exists \beta \in \mathbb{R},  \: \left( \alpha (1+\rho^2) + \beta + \sum_{\ell = 1}^{k-1} \zeta_\ell v_\ell \leq 0 \right) \right. \\  \hspace{2cm}\left. \wedge \left( \sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j +  \sum_{\ell = 1}^{k-1} \zeta_\ell \mathbf{\mathcal{Q}}(x^\ell) + \alpha I_{n+1} + \beta E \succeq \mathbf{\mathcal{Q}}(\hat{x}) \right) \right \rbrace .
\end{align*}

\begin{proposition} \label{prop:phik}
Under Assumption~\ref{as:subpbqcqp}, for any finite sequences $x^{1}, \dots, x^{k-1} \in \mathcal{X}$, and $v_{1}, \dots, v_{k-1} \in \mathbb{R}$ s.t.\ $v_\ell \geq \phi(x^{\ell})$, the resulting set $\mathcal{X} \cap \mathcal{R}^k$ is included in the feasible set of \eqref{eq:SIP}.
\end{proposition}
\begin{proof}
    We can apply Theorem~\ref{th:finiteformulation} to the inner problem modified with the additional valid cuts $\langle \mathbf{\mathcal{Q}}(x^\ell), Y \rangle  \leq  v_\ell \; \forall \ell \in \{1,\dots,k-1\}$, which do not change the value of the non-convex inner problem $\phi(x)$, and, therefore, do not change the feasible set of problem~\eqref{eq:SIP}. We deduce that $\mathcal{X} \cap \mathcal{R}^k$ is a subset of the feasible set of the problem~\eqref{eq:SIP}. 
\end{proof}

\begin{algorithm}[h!]
{\small
\caption{IOA algorithm with inexact oracle}
\label{alg:IO}
\begin{algorithmic}[1]
    \State{\textbf{Input:} Oracle with parameter $\delta \in [0,1)$, tolerances $(\epsilon_1, \epsilon_2) \in \mathbb{R}_+^2$, bounds $\underline{\mu}, \overline{\mu} \in \mathbb{R}_{++}$.}
    \State{Solve the restriction~\eqref{eq:SIP:convexreform}, to obtain a solution $\hat{x}^0$.}
    \If{$Q(\hat{x}^0)\succ 0$ and $Q^1, \dots, Q^r \succeq 0$} \label{step:suffcond}
    \State Return $\hat{x}^0$.
    \EndIf
    \State{$k\gets0$, $\mathcal{Y}^0 \gets \emptyset$, $(\nu^0_1, \nu^0_2) \gets (\infty,\infty)$.}
    \While{$\nu^k_1  > \epsilon_1$ or $\nu^k_2  > \epsilon_2$}
        \State Choose $\mu_k \in [\underline{\mu}, \overline{\mu}]$, and compute an optimal solution $(x^k,\hat{x}^k)$ of 
        \begin{align}
            \left\{ \begin{array}{rl}\label{eq:master_problem_IO}
          \min\limits_{x, \hat{x} \in \mathcal{X}} & F(x) + F(\hat{x}) + \frac{\mu_k}{2} \lVert x - \hat{x}\rVert^2 \\
                \text{s.t.} &   G(x,y) \leq 0 \quad \forall y \in \mathcal{Y}^k \\
                & \hat{x} \in \mathcal{R}^k.
        \end{array}\right.
        \end{align}
	\State Call the $\delta$-oracle to compute an approximate solution $y^{k} \in \mathcal{Y} = \hat{y}(x^k)$, and an upper bound $v_k = \hat{v}(x^k)$ of $\max_{y \in \mathcal{Y}} G(x^k,y)$.
        \State Based on $x^k$ and $v_k$, update the set $\mathcal{R}^k$ in $\mathcal{R}^{k+1}$. 
        \State {$\mathcal{Y}^{k+1} \gets \mathcal{Y}^{k} 
        \cup \{ y^k \} $, $\; (\nu^{k+1}_1,\nu^{k+1}_2) \gets (G(x^k,y^k), \lVert x^k -\hat{x}^k \rVert)$}
	\State {$k \gets k + 1$}
    \EndWhile
    \State  Return $(x^k,\hat{x}^k)$.    \label{steptermIOA}
\end{algorithmic}}
\end{algorithm}

Alg.~\ref{alg:IO} is the pseudocode of the IOA algorithm with inexact oracle. It starts by solving the restriction \eqref{eq:SIP:convexreform} and checks whether the condition presented in Theorem~\ref{th:restriction_optimality} is satisfied or not. If yes, the algorithm stops returning the solution which is optimal for both \eqref{eq:SIP:convexreform} and \eqref{eq:SIP}. Otherwise, it performs a sequence of iterations, until the stopping criteria are satisfied, i.e., $G(x^k,y^k) \leq \epsilon_1$, and $\lVert x^k - \hat{x}^k \rVert \leq \epsilon_2$. At each iteration, the convex optimization problem \eqref{eq:master_problem_IO} is solved. This problem is a \textit{coupling} between the minimization of $F$ on a relaxed set, and the minimization of $F$ on a restricted set. Indeed, $x$ belongs to an outer-approximation (relaxation), whereas $\hat{x}$ belongs to an inner-approximation (restriction) of \eqref{eq:SIP} feasible set. The minimization of $F$ over these two sets is coupled by a proximal term that penalizes the distance between $x$ and $\hat{x}$. After solving the master problem~\eqref{eq:master_problem_IO}, the lower-level problem~\eqref{eq:QCQP} is solved for $x = x^k$. The {solution} of this problem is used to restrict the outer-approximation, and to enlarge the inner-approximation. 

Before proving the termination and the convergence of Alg.~\ref{alg:IO}, under Assumptions~\ref{as:convex} and \ref{as:subpbqcqp}, we introduce two technical lemmas.
\begin{lemma}
    Under Assumptions~\ref{as:convex} and \ref{as:subpbqcqp}, denoting by $x^*$ an optimal solution of \eqref{eq:SIP}, if Alg.~\ref{alg:IO} runs iteration $k$, $F(x^k) \leq F(x^*) + \mu_k(x^k-\hat{x}^k)^\top (x^* - x^k)$. \label{lemma:DirOpt}
\end{lemma}
\begin{proof}
Proof in Appendix~\ref{app:DirOpt}.
\end{proof}
\begin{lemma} \label{lem:discretizationconv}
Consider a parameter $\delta \in [0,1)$, the infinite sequences $(x^k)_{k \in \mathbb{N}} \subset \mathcal{X}$ and $(y^k)_{k \in \mathbb{N}}\subset \mathcal{Y}$, where $y^k = \hat{y}(x^k)$ is the output of the $\delta$-oracle evaluated at point $x^k$. If these sequences are such that, for any $k \in \mathbb{N}$,\vspace{-0.7em}
    \begin{align*}
        (*) \quad G(x^k,y^\ell) \leq 0, \quad \forall \ell \in \{0, \dots ,k - 1 \}, \vspace*{-0.7em}
    \end{align*}
then, the feasibility error $\phi(x^k)^+$ vanishes.
\end{lemma}
\begin{proof}
    Proof in Appendix~\ref{app:discretizationconv}.
\end{proof}
Before proving Theorems~\ref{th:termIO} and \ref{th:convIO}, which state the convergence and the termination of the IOA algorithm, we make the assumption that $F$ is Lipschitz continuous, and that the Slater condition holds for the restriction~\eqref{eq:SIP:convexreform}. Yet, we do not need to know a Slater point to run the IOA algorithm.
\begin{assumption}\label{as:slater_res}
$F(x)$ is $C_F$-Lipschitz, and there exists $x^S \in \mathcal{X}$ s.t.\ $\Phi_{\mathsf{SDP}}(x^S)< 0$. 
\end{assumption}
%Theorem~\ref{th:termIO} studies the termination of the IOA algorithm. It claims asymptotic convergence in the case where the algorithm does not terminate. On the contrary, Theorem~\ref{th:convIO} studies the suboptimality of the returned solution in the case of finite termination. %Under Assumptions~\textcolor{red}{??}, the IOA algorithm converges, as stated in the following theorem: if the tolerance parameters are set to zero, we retrieve a minimizing sequence of \eqref{eq:SIP}; if not, we retrieve a feasible solution in finite time, with bounded sub-optimality. 

\begin{theorem}\label{th:termIO}
Under Assumptions~\ref{as:convex}, \ref{as:subpbqcqp}, and \ref{as:slater_res}, if $\epsilon_1, \epsilon_2 > 0$, Alg.~\ref{alg:IO} stops after a finite number of iterations; on the contrary, if Alg.~\ref{alg:IO} generates an infinite sequence of feasible iterates $(\hat{x}^k)_{k \in \mathbb{N}}$ s.t., for all $k$, then $F(\hat{x}^k)\rightarrow \mathsf{val}\text{\eqref{eq:SIP}}$. 
\end{theorem}
\begin{proof}
    We reason by contrapositive: we suppose that Alg.~\ref{alg:IO} does not stop, i.e., generates two infinite sequences $(x^k)_{k \in \mathbb{N}^{+}}$ and $(\hat{x}^k)_{k \in \mathbb{N}^{+}}$, and we show that $\epsilon_1 =0$ or $\epsilon_2 = 0$. We will first prove that $\phi_{\mathsf{SDP}}^k(x^k)^+ \rightarrow 0$. Since $\phi_{\mathsf{SDP}}^k(x^k)^+$ is bounded, there exists at least one accumulation value $\ell$ for this sequence (i.e., $\phi_{\mathsf{SDP}}^k(x^k)^+ \rightarrow \ell$). We show that, necessarily, $\ell = 0$. We take $\psi : \mathbb{N} \to \mathbb{N}$, such that $\phi_{\mathsf{SDP}}^{\psi(k)}(x^{\psi(k)})^+ \rightarrow \ell$. Up to the extraction of a subsequence, we can assume, by the compactness of $\mathcal{X}$, that $x^{\psi(k)} \rightarrow x \in \mathcal{X}$. For $k \in \mathbb{N}$, we define $j = \psi(k)$ and $\ell = \psi(k-1)$, and   \vspace{-0.7em}
    \begin{align}
        \phi_{\mathsf{SDP}}^j(x^j) = & \phi_{\mathsf{SDP}}^j(x^{\ell}) + \phi_{\mathsf{SDP}}^j(x^j) - \phi_{\mathsf{SDP}}^j(x^{\ell}) \\
                & \leq v_{\ell} + \phi_{\mathsf{SDP}}^j(x^j) - \phi_{\mathsf{SDP}}^j(x^{\ell}),  \label{eq:boundingvl}
    \end{align}
    as the constraint $\langle \mathcal{Q}(x^{\ell}), Y \rangle \leq v_{\ell}$ is enforced in the problem $(\mathsf{SDP}^{j}_x)$, being $\ell \leq j-1$. 
    We introduce  $\tilde{Y}$ the solution of $\mathsf{SDP}^{j}_x$ at $x = x^{\ell}$ so that, $\langle \mathcal{Q}(x^{\ell}), \tilde{Y} \rangle =  \phi_{\mathsf{SDP}}^j(x^{\ell}) $. Since $\tilde{Y}$ is feasible in $\mathsf{SDP}^{j}_x$ at $x = x^{j}$, $\langle \mathcal{Q}(x^{j}), \tilde{Y} \rangle \leq \phi_{\mathsf{SDP}}^j(x^j)$. Therefore, by linearity of $Q$, and due to the Cauchy-Schwartz inequality,  $\phi_{\mathsf{SDP}}^j(x^j) - \phi_{\mathsf{SDP}}^j(x^{\ell}) \leq \langle \mathcal{Q}(x^j - x^{\ell}), \tilde{Y} \rangle \leq \lVert  \mathcal{Q}(x^j - x^{\ell}) \rVert_F \: B $ where $B := \max_{Y \in \mathsf{Feas}\text{\eqref{eq:SDP_relax}}} \lVert Y \rVert_F$, which is independent from $j,$ and $\ell$. Indeed, $\tilde{Y}$ is feasible in \eqref{eq:SDP_relax}. By defining the operator norm of $x \mapsto \mathcal{Q}(x)$ as $\lVert \mathcal{Q} \rVert_{op}$, we obtain that $\phi_{\mathsf{SDP}}^j(x^j) - \phi_{\mathsf{SDP}}^j(x^{\ell}) \leq \lVert x^j - x^\ell \rVert \: \lVert \mathcal{Q} \rVert_{op} \: B$. We combine this with Eq.~\eqref{eq:boundingvl}, using the fact that the positive part is non-decreasing, to obtain\vspace{-0.7em}
    \begin{align}
    \phi_{\mathsf{SDP}}^j(x^j)^+ = & \leq v_{\ell}^+ + \lVert x^j - x^\ell \rVert \: \lVert \mathcal{Q} \rVert_{op} \: B \\
    & \leq \phi(x^\ell)^+ (1+ \delta) + \lVert x^j - x^\ell \rVert \: \lVert \mathcal{Q} \rVert_{op} \: B,
\end{align}
the second inequality coming from the property of the $\delta$-oracle (see Eq.~\eqref{eq:oraclebound}). Using the definition of $\ell$ and $j$, we obtain $ 0 \leq \phi_{\mathsf{SDP}}^{\psi(k)}(x^{\psi(k)})^+ \leq \phi(x^{\psi(k-1)})^+ (1+ \delta) + \lVert x^{\psi(k)} - x^{\psi(k-1)} \rVert \: \lVert \mathcal{Q} \rVert_{op} \: B.$
Since $\phi(x^{k})^+ \rightarrow 0$ (due to Lemma~\ref{lem:discretizationconv}), and $x^{\psi(k)}$ is converging, we deduce, by taking the limit, that $\ell = 0$. We  conclude that $\phi_{\mathsf{SDP}}^k(x^k)^+ \rightarrow 0$.

Using Assumption~\ref{as:slater_res}, we introduce a Slater point $x^S \in \mathcal{X}$ such that $\phi_{\mathsf{SDP}}(x^S) = -c$, for $c > 0$. We also introduce $\omega_k := \phi_{\mathsf{SDP}}^k(x^k)^+/(c+\phi_{\mathsf{SDP}}^k(x^k)^+)$. We notice that $\omega_k \rightarrow 0$, since $\phi_{\mathsf{SDP}}^k(x^k)^+ \rightarrow 0$. We define the convex combination $\bar{x}_k = (1-\omega_k) x^k + \omega_k x^S$. We emphasize that $(\bar{x}_k,\bar{x}_k)$ is feasible in problem~\eqref{eq:master_problem_IO} at iteration $k$ since $\mathcal{X}$ is convex, and 
    \begin{itemize}
\item $\bar{x}_k$ satisfies the constraints on $x$, because both $x^k$ and $x^S$ satisfy the convex constraints $G(x,y)$ for $y \in \mathcal{Y}^k$, and, by convex combination, so does $\bar{x}_k$;
    \item $\bar{x}_k$ satisfies the constraints on $\hat{x}$; indeed $\phi_{\mathsf{SDP}}^k(x^k) \leq 0$, and $\phi_{\mathsf{SDP}}^k(x^S) \leq \phi_{\mathsf{SDP}}(x^S)\leq 0$; by convexity of $\phi_{\mathsf{SDP}}^k(x^k)$ (as a maximum of linear functions), we deduce that $\phi_{\mathsf{SDP}}^k(\bar{x}_k) \leq 0$, i.e., $\bar{x}_k \in \mathcal{R}^k$.
\end{itemize}
As the objective value of $(\bar{x}_k,\bar{x}_k)$ in the problem~\eqref{eq:master_problem_IO} is $2F(\bar{x}_k)$, by optimality of $(x^k, \hat{x}^k)$: $ F(x^k) +  F(\hat{x}^k) + \frac{\mu_k}{2} \lVert x^k - \hat{x}^k \rVert^2  \leq 2 F( (1-\omega_k) x^k + \omega_k x^S)$, which means, by convexity of $F$, that \vspace{-1em}
\begin{align}
   F(x^k) +  F(\hat{x}^k) + \frac{\mu_k}{2} \lVert x^k - \hat{x}^k \rVert^2  \leq 2(1-\omega_k) F(x^k) + 2\omega_k F(x^S).
\label{eq:start_point_slater}
\end{align}
We also notice that $(\hat{x}^{k},\hat{x}^{k})$ is feasible in the problem~\eqref{eq:master_problem_IO} at iteration $k$, thus $F(x^k) +  F(\hat{x}^k) + \frac{\mu_k}{2} \lVert x^k - \hat{x}^k \rVert^2 \leq  2 F( \hat{x}^{k})$, which means\vspace{-0.7em}
\begin{equation}
     F(x^k)  + \frac{\mu_k}{2} \lVert x^k - \hat{x}^k \rVert^2 \leq  F( \hat{x}^{k}). %\leq  F(x^k) + J \lVert x^k - \hat{x}^k \rVert
     \label{eq:observation1}
\end{equation}
Summing Eq.~\eqref{eq:start_point_slater} with Eq.~\eqref{eq:observation1}, we have: $ 2 F(x^k) + \mu_k \lVert x^k - \hat{x}^k \rVert^2 \leq 2(1-\omega_k) F(x^k) + 2\omega_k F(x^S)$, and thus $\mu_k \lVert x^k - \hat{x}^k \rVert^2 \leq 2 \omega_k \left( F(x^S) -  F(x^k) \right)$. Since $0<\underline{\mu} \leq \mu_k$,\vspace{-0.3em}
\begin{equation}
     \lVert x^k - \hat{x}^k \rVert \leq \sqrt{\underline{\mu}^{-1}(2 \omega_k \left( F(x^S) -  F(x^k) \right))}
     \label{eq:observation1bis}
\end{equation}
holds. Since $\omega_k \rightarrow 0$, and $F(x^S) -  F(x^k)$ is bounded, we deduce from Eq.~\eqref{eq:observation1bis} that \vspace{-0.7em}
\begin{equation}
     \lVert x^k - \hat{x}^k \rVert \rightarrow 0.
     \label{eq:almost_conclusion}
\end{equation}
As the stopping criterion is not met, for all $k \in\mathbb{N}$, $G(x^k,y^k) > \epsilon_1$ (and therefore $G(x^k,y^k)^+ > \epsilon_1$), or $\lVert x^k - \hat{x}^k \rVert > \epsilon_2$. Since $G(x^k,y^k)^+ \leq \phi(x^k)^+ \rightarrow 0$ (Lemma~\ref{lem:discretizationconv}), and $\lVert x^k - \hat{x}^k \rVert \rightarrow 0$, at least one number between $\epsilon_1$ and $\epsilon_2$ is zero.
Being $\hat{x}^k \in \mathcal{R}^{k}$, feasible in \eqref{eq:SIP}, as stated in Prop.~\ref{prop:phik}. Therefore, being $F$ $C_F$-Lipschitz, we have $F(x^*) \leq F(\hat{x}^k) \leq F(x^k) + J \lVert x^k - \hat{x}^k\rVert$. According to Lemma~\ref{lemma:DirOpt}, we know that $F(x^k) \leq F( x^{*}) + \mu_k (x^k - \hat{x}^k)^\top(x^* - x^k)$, which implies, according to the Cauchy-Schwartz inequality, that 
\begin{equation}
    F(x^*) \leq F(\hat{x}^k) \leq F(x^*) + \mu_k \lVert x^k - \hat{x}^k\rVert \lVert x^* - x^k \rVert + C_F \lVert x^k - \hat{x}^k\rVert.
    \label{eq:encadrement}
\end{equation}
Since $\lVert x^* - x^k \rVert$ is bounded, we deduce from Eq.~\eqref{eq:almost_conclusion} that $F(x^*) + \mu_k \lVert x^k - \hat{x}^k\rVert \lVert x^* - x^k \rVert + C_F \lVert x^k - \hat{x}^k\rVert \rightarrow F(x^*)$, and thus, $F(\hat{x}^k) \rightarrow F(x^*) =  \mathsf{val}\text{\eqref{eq:SIP}}$.
%The sequence $F(\hat{x}^k)$ is bounded, and we show that any limit value $\ell$ is equal to $\mathsf{val}$\eqref{eq:SIP} to prove its convergence. We take such a subsequence $F(\hat{x}^{\psi(k)})$ converging to $\ell \in \mathbb{R}^+$. Eq.~\ref{eq:almost_conclusion} yields $\lVert x^{\psi(k)} - \hat{x}^{\psi(k)} \rVert \rightarrow 0$. Up to extracting a subsequence, we can assume that $x^{\psi(k)} \rightarrow$
% Since $\hat{x}^{\psi(k)} \in \mathcal{R}^{\psi(k)}$, it is feasible in \eqref{eq:SIP} as stated in Prop~\ref{prop:phik}. In particular, $F(\hat{x}^{\psi(k)}) \geq \mathsf{val}$\eqref{eq:SIP}, therefore $\ell \geq \mathsf{val}$\eqref{eq:SIP}.
\end{proof}
\begin{theorem} \label{th:convIO}
    Under Assumptions~\ref{as:convex}, \ref{as:subpbqcqp}, and \ref{as:slater_res}, if Alg.~\ref{alg:IO} terminates after iteration $K$, then it returns a feasible iterate $\hat{x}^K$ s.t.\ $F(\hat{x}^K) \leq \mathsf{val}\text{\eqref{eq:SIP}} + \epsilon_2 (\mu_K \mathsf{diam}(\mathcal{X}) + C_F).$
\end{theorem}
\begin{proof}
From Lemma~\ref{lemma:DirOpt}, and from Cauchy-Schwartz inequality, we know that $F(x^K) \leq F(x^*) + \mu_k(x^K-\hat{x}^K)^\top (x^* - x^K) \leq F(x^*) + \mu_k \lVert x^K-\hat{x}^K \rVert \: \lVert x^* - x^K \rVert$. Using that $F$ is $C_F$-Lipschitz, we obtain  $F(\hat{x}^K) \leq F(x^*) + \mu_K \lVert x^K-\hat{x}^K \rVert \: \lVert x^* - x^K \rVert + C_F \lVert x^K-\hat{x}^K \rVert$. We note that $\lVert x^K - \hat{x}^K\rVert \leq \epsilon_2$ and $\lVert x^* - x^K \rVert \leq  \mathsf{diam}(\mathcal{X})$ to conclude.
\end{proof}

\section{Conclusion}
In this paper, we address the issue of solving a convex Semi-Infinite Programming problem despite the difficulty of the separation problem. We proceed by allowing for the approximate solution of the separation problem, up to a given relative optimality gap. We see that, in the case where the objective function is strongly convex, the Cutting-Planes algorithm has guaranteed theoretical performance despite the inexactness of the oracle: it converges in $O(1/k)$. Contrary to the Cutting-Planes algorithm, the Inner-Outer Approximation algorithm generates a sequence of feasible points converging towards the optimum of the Semi-Infinite Programming problem. This paper shows that this is also the case when the separation problem is a Quadratically Constrained Quadratic Programming problem despite the inexactness of the oracle. An avenue of research is to extend these results to the setting of Mixed-Integer Convex Semi-Infinite Programming.
\vspace{1em}

\noindent
{\small\textbf{Data availability statement:} 
Data sharing not applicable to this article as no datasets were generated or analyzed during the current study.}

%\bibliographystyle{unsrt}
\bibliography{main}


\begin{appendices}
\section{}
\subsection{Proof of Lemma~\ref{lemma:smoothness}}\label{app:smoothness}
We have that: \begin{inparaenum}[(i)] \item the set $\mathcal{X}$ is compact, \item the function $-\mathcal{L}(\cdot,z)$ is continuous for all $z \in \mathbb{R}^m$, \item the function $-\mathcal{L}(x,\cdot)$ is convex and differentiable for all $x \in \mathcal{X}$, \item the function  $\sup_{x\in \mathcal{X}} -\mathcal{L}(x,\cdot)$ is finite-valued over $\mathbb{R}^m$, and \item due to Assumptions~\ref{as:convex} and \ref{as:strongconvex}, the function $-\mathcal{L}(\cdot,z)$ is strongly concave and the set $\mathcal{X}$ is compact and convex, therefore the supremum $\sup_{x\in \mathcal{X}} -\mathcal{L}(x,z)$ is attained for a unique $x(z)$. \end{inparaenum} 
In this setting, we deduce from \cite[Cor.~VI.4.4.5]{hiriart2013convex} that $\theta(z)$ is differentiable over $\mathbb{R}^m$, with gradient $\nabla_z \mathcal{L}(x(z),z) = x(z)$. 

We now take $z,z' \in \mathbb{R}^m$, and prove that $\lVert \nabla \theta(z) - \nabla \theta(z') \rVert \leq \frac{1}{\mu} \lVert z - z'\rVert$. We define the functions $w(u) = \mathcal{L}(u,z) + {i}_\mathcal{X}(u)$ and $w'(u) = \mathcal{L}(u,z') + {i}_\mathcal{X}(u)$, where $i_\mathcal{X}(\cdot)$ is the characteristic function of $\mathcal{X}$.  We introduce $x$ (resp. $x'$) the unique minimum of $w$ (resp. $w'$). The first-order optimality condition for these convex functions reads \vspace{-0.8em}
\begin{equation}
 0 \in \partial w (x),\quad 0 \in \partial w' (x').  \label{eq:foc} \vspace{-0.5em}
\end{equation}
We notice that the function $(F + i_\mathcal{X})(u)$ is convex due to Assumption~\ref{as:convex}, and the function $\ell(u) = z^\top u$ is linear and thus convex. The intersection of the relative interiors of the domains of
these convex functions is $\mathsf{ri}(\mathcal{X})$. With $\mathcal{X}$ a finite-dimensional convex set, $\mathsf{ri}(\mathcal{X}) \neq \emptyset$, according to \cite[Prop.~1.9]{tuy}. Hence, the subdifferential of the sum is the sum of the subdifferentials \cite[Th.~2.1]{Romano}, i.e., $\partial w(x) = \partial (F + i_\mathcal{X})(x) + \partial \ell (x) =\partial (F + i_\mathcal{X})(x) + z$. Similarly,  $\partial w'(x') = \partial (F + i_\mathcal{X})(x') +  z'$. Therefore, Eqs.~\eqref{eq:foc} may be rephrased as the existence of $s \in \partial (F + i_\mathcal{X})(x)$ and $s' \in \partial (F + i_\mathcal{X})(x')$ such that \vspace{-0.8em}
\begin{equation}
 0 = s + z, \quad 0 = s'+z'. \label{eq:subdiff} \vspace{-0.5em}
\end{equation}
Due to Assumptions~\ref{as:convex} and \ref{as:strongconvex}, the function $F + i_\mathcal{X}$ is $\mu$-strongly convex. Applying  \cite[Th.~VI.6.1.2]{hiriart2013convex}, the $\mu$-strong convexity of $F + i_\mathcal{X}$ gives that $     (s-s')^\top(x -x' ) \geq \mu \lVert x - x' \rVert^2$, since $s \in \partial (F + i_\mathcal{X})(x)$ and $s' \in \partial (F + i_\mathcal{X})(x')$. Using the Cauchy-Schwartz inequality and Eqs.~\eqref{eq:subdiff}, we deduce that $\lVert z - z' \rVert \: \lVert x - x' \rVert \geq \mu \lVert x - x' \rVert^2$. 
Since $\nabla \theta (z) = x$ and  $\nabla \theta (z') = x'$ (following from the first part of this proof):
\begin{align}
    \lVert z - z' \rVert \: \lVert \nabla\theta(z) - \nabla\theta(z') \rVert  \geq \mu \lVert \nabla\theta(z) - \nabla\theta(z') \rVert^2. \label{eq:almostconclude}
\end{align}
From Eq.~\eqref{eq:almostconclude}, $ \lVert \nabla\theta(z) - \nabla\theta(z') \rVert \leq \frac{1}{\mu} \lVert z - z' \rVert$, if $\lVert \nabla\theta(z) - \nabla\theta(z') \rVert > 0$. If $\lVert \nabla\theta(z) - \nabla\theta(z') \rVert =0$, this inequality is also trivially true.

%-------------------------------------------------
%-------------------------------------------------
\begin{comment}\section{Proof of Lemma~\ref{lemma:progress}}\label{app:progress}
For any $y,z\in\mathcal{K}$ and $\gamma \geq 0$, we obtain by integration that 
{\small \begin{equation}
    \theta(z + \gamma y) - \theta(z) = \int_{0}^\gamma  \nabla \theta(z+ t y)^\top y \: dt = \gamma \nabla \theta(z)^\top y  + \int_{0}^\gamma   (\nabla \theta(z+ t y) - \nabla \theta(z))^\top y \: dt.
    \label{eq:integration}
\end{equation}}
Using the Cauchy-Schwarz inequality and the $\frac{1}{\mu}$-smoothness of $\theta$ (according to Prop.~\ref{lemma:smoothness}), we know that 
{\small
\begin{align}
   (\nabla \theta(z+ty) - \nabla \theta(z))^\top y   \geq - \lVert {\nabla}\theta(z+ty)  - \nabla \theta(z) \rVert_2  \: \lVert y \rVert_2   \geq -  \frac{t}{\mu} \lVert y \rVert_2^2.
\end{align}
   }
Combining this with Eq.~\eqref{eq:integration}, we deduce that {\small$
    \theta(z+ty) - \nabla \theta(z)  \geq {\gamma}\langle \nabla \theta (z), y \rangle - \int_{t = 0}^\gamma   \frac{t}{\mu} \lVert y \rVert_2^2 {dt}$},
which yields that {\small$\theta(z + \gamma y)- \theta(z)  \geq \gamma \langle \nabla \theta (z), y \rangle - \frac{ \lVert y \rVert^2}{2\mu} \gamma^2$}. 
\end{comment}
%-------------------------------------------------
%-------------------------------------------------
\subsection{Proof of Lemma~\ref{lemma:dualopti}}\label{app:dualopti}
According to Assumption~\ref{as:slater}, we introduce $\hat{x} \in \mathcal{X}$ such that $\hat{x}^\top a(y) < 0$ for all $y \in \mathcal{Y}$. By continuity of $G$ and compactness of $\mathcal{Y}$, we know that there exists a constant $c > 0$ such that $\hat{x}^\top a(y) \leq -c$ for all $y \in \mathcal{Y}$. For any $z \in \mathcal{K}$, there exist $r \in \mathbb{N}$, $y_1, \dots, y_r \in \mathcal{Y}$, and $\lambda_1, \dots, \lambda_r \in \mathbb{R}_{++}$ such that $z = \sum_{i=1}^p \lambda_i a(y_i)$. By definition of $\theta(z)$, $\theta(z) \leq \mathcal{L}(\hat{x},z)$. We deduce that $\theta(z) \leq F(\hat{x}) - c \sum_{i=1}^p \lambda_i$, which also reads $\sum_{i=1}^p \lambda_i  \leq c^{-1}(F(\hat{x}) - \theta(z)).$
For this equation, we deduce that for any solution of \eqref{eq:dualsip} such that $\theta(z) \geq V -1$ where $V = \mathsf{val} \text{\eqref{eq:SIP}} =  \mathsf{val} \text{\eqref{eq:dualsip}}$, we have $\sum_{i=1}^p \lambda_i  \leq c^{-1}(F(\hat{x}) - V + 1),$ 
and this holds for any decomposition $z =\sum_{i=1}^p \lambda_i a(y_i)$. In particular, the set of $z \in \mathcal{K}$, such that $\theta(z) \geq V -1$ is included in the compact set $q \mathsf{conv}(\mathcal{M})$, where $q = c^{-1}(F(\hat{x}) - V + 1)$. Over this compact set, the continuous function $\theta(z)$ reaches a maximum.

\begin{comment}
\section{Proof of Lemma~\ref{lemma:qcqpsdp}}\label{app:qcqpsdp}
We underline that, due to Assumption~\ref{as:subpbqcqp}, the constraint $\lVert y \rVert^2 + 1 \leq 1 + \rho^2 $ is redundant in the problem \eqref{eq:QCQP}, i.e., adding it does not change the value of this problem. Consequently, we recognize that \eqref{eq:SDP_relax} is the standard SDP relaxation of the problem \eqref{eq:QCQP} augmented with this redundant constraint, this is why $\mathsf{val} \text{\eqref{eq:SDP_relax}} \geq \mathsf{val} \text{\eqref{eq:QCQP}}$. \\
We assume now that $Q(x), Q^1, \dots, Q^r$ are PSD. Given a matrix $Y$ feasible for \eqref{eq:SDP_relax}, we denote by $u_1, \dots, u_{n+1}$ $\in \mathbb{R}^{n+1}$ a basis of eigenvectors of $Y$ (which is PSD) and their respective eigenvalues $v_1, \dots, v_{n+1}$  $\in \mathbb{R}_+$. Let us introduce the two following index sets: $I = \{ i \in \{ 1, \dots, n+1 \} : (u_i)_{n+1} \neq 0 \} \text{ and }  J = \{ i \in\{ 1, \dots, n+1 \} : (u_i)_{n+1} = 0 \}.$ We have then $I \cup J = \{ 1, \dots, n+1 \}$. Moreover, 
\begin{itemize}
\item if $i \in I$ : we define the nonnegative scalar $\mu_i = v_i \: (u_i)_{n+1}^2$ and $y_i \in \mathbb{R}^n$ s.t.\ $u_i = (u_i)_{n+1} \begin{pmatrix} y_i \\ 1
    \end{pmatrix}$ 
    \item if $i \in J$ : we define the nonnegative scalar $\nu_i = v_i$ and $z_i \in \mathbb{R}^n$ s.t.\ $u_i = \begin{pmatrix}
z_i \\
0
\end{pmatrix}$.
\end{itemize}
With this notation, we have that 
{\small$$Y = \sum\limits_{i = 1}^{n+1} v_i u_i u_i^\top  =\sum\limits_{i \in I} \mu_i \begin{pmatrix} y_i y_i^\top & y_i \\ y_i^\top & 1 
    \end{pmatrix} + \sum\limits_{i \in J} \nu_i \begin{pmatrix} z_i z_i^\top & \mathbf{0} \\ \mathbf{0}^\top & 0 
    \end{pmatrix},$$}where $\mathbf{0}$ is the null $n$-dimensional vector (whereas ${0_n}$ is the $n \times n$ null matrix). Let us define the vector $\bar{y} = \sum\limits_{i \in I} \mu_i y_i$. Its objective value in \eqref{eq:QCQP} is larger than the objective value of $Y$ in \eqref{eq:SDP_relax}, since
{\small\begin{align}
     \langle \mathcal{Q}(x) , Y \rangle  &= \sum\limits_{i \in I} \mu_i G(x,y_i) - \frac{1}{2}\sum\limits_{i \in J} \nu_iz_i^\top Q(x) z_i  \\ & \leq \sum\limits_{i \in I} \mu_i G(x,y_i)  \\ &\leq   G(x,\sum\limits_{i \in I} \mu_i y_i) = G(x,\bar{y}).
      \label{eq:step1}
\end{align}}
The first inequality is due to $Q(x) \succeq 0$ and $\nu_i \geq 0$. The second inequality derives from $\sum_{i \in I} \mu_i = Y_{n+1,n+1} = 1$, and from the concavity of the function $G(x,\cdot)$ (Jensen inequality). Similarly, knowing that $Q^j$ is PSD and that $Y$ is feasible in \eqref{eq:SDP_relax}, we can show that $\frac{1}{2} \bar{y}^\top Q^j \bar{y} +  (q^j) ^\top \bar{y} + b_j  \leq \langle \mathbf{\mathcal{Q}}^j, Y \rangle \leq 0$, which means that $\bar{y}$ is feasible in \eqref{eq:QCQP}. This implies that $\langle \mathcal{Q}(x),Y \rangle \leq G(x,\bar{y}) \leq \mathsf{val}\text{\eqref{eq:QCQP}}$. This being true for any matrix $Y$ feasible in \eqref{eq:SDP_relax}, we conclude that $\mathsf{val}\text{\eqref{eq:SDP_relax}}  \geq \mathsf{val} \text{\eqref{eq:QCQP}}$. This proves that $\mathsf{val}\text{\eqref{eq:SDP_relax}}  = \mathsf{val}\text{\eqref{eq:QCQP}}$.
\end{comment}

\subsection{Proof of Lemma~\ref{lemma:strong_duality}}\label{app:strong_duality}
The Lagrangian of problem \eqref{eq:SDP_relax} is defined over $Y \in  \mathbb{S}_{n+1}^+$, $\lambda \in \mathbb{R}_+^r, \alpha \in \mathbb{R}_+, \beta\in\mathbb{R}$ and reads
{\small$
        L_x(Y, \lambda, \alpha, \beta) = \langle \mathcal{Q}(x) , Y \rangle
         - \sum\limits_{j=1}^r \lambda_j \langle \mathcal{Q}^j, Y \rangle
         + \alpha ( 1 + \rho^2 - \langle I_{n+1}, Y \rangle) 
         + \beta (1 -  \langle E, Y \rangle)  \vspace*{-0.2em} = \alpha (1+\rho^2) + \beta + \langle \mathbf{\mathcal{Q}}(x)  - \sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j  - \alpha I_{n+1} - \beta E, Y \rangle.$}
         
The Lagrangian dual problem of \eqref{eq:SDP_relax} is {\small $\min\limits_{\lambda,\alpha,\beta} \, \max\limits_{Y} L_x(Y, \lambda, \alpha, \beta)$}, i.e.,\vspace{-0.8em}
$$   \underset{\begin{subarray}{c} \lambda \in \mathbb{R}^r_+ \\ \alpha \in \mathbb{R}_+ \\ \beta \in \mathbb{R} \end{subarray}}{\min}  \alpha(1+\rho^2) + \beta + \max\limits_{Y \in \mathbb{S}_{n+1}^+} \langle \mathbf{\mathcal{Q}}(x)  - \sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j  - \alpha I_{n+1} - \beta E, Y \rangle. \vspace{-0.8em}$$

We recognize that the maximum is $+\infty$, unless $\sum\limits_{j=1}^r  \lambda_j \mathbf{\mathcal{Q}}^j  + \alpha I_{n+1} + \beta E \succeq \mathbf{\mathcal{Q}}(x)$. 
This proves that the dual problem of \eqref{eq:SDP_relax} can be formulated as \eqref{eq:DSDP}. We prove now that the Slater condition, which is a sufficient condition for strong duality ($\mathsf{val}\text{\eqref{eq:SDP_relax}} = \mathsf{val}\text{\eqref{eq:DSDP}}$) \cite{vandenberghe1996semidefinite}, holds for the dual problem \eqref{eq:DSDP}. We denote by $m_x$ the maximum eigenvalue of $\mathcal{Q}(x)$, and we notice that $(\lambda,\alpha,\beta) = (0, \dots, 0, \max \{1 + m_x, 0\}, 0)$ is a strictly feasible point of \eqref{eq:DSDP}. Hence, the Slater condition holds.

\subsection{Proof of Lemma~\ref{lemma:DirOpt}}\label{app:DirOpt}
We analyze the variation of the objective function w.r.t.\ the variable $x$. Since $x^* \in \mathcal{X}$ is a feasible value for variable $x$, the direction $h = x^* - x^k$ is admissible at $x^k$ in the problem~\eqref{eq:master_problem_IO}. As $F(x)$ is convex over $\mathbb{R}^n$, the directional derivative $F'(x^k,h) = \lim\limits_{t\rightarrow 0^+} \frac{F(x^k + t h) - F(x^k)}{t}$ is well-defined. By optimality of $x^k$, the directional derivative of function $F(x) + \frac{\mu_k}{2} \lVert x - \hat{x}^k \rVert^2$ in the direction $h$ is non-negative, i.e., $F'(x^k,h) + \mu_k (x^k- \hat{x}^k)^\top h \geq 0$. By convexity of $F(x)$, $F(x^*) - F(x^k) \geq F'(x^k,h)$. Combining this with the previous inequality yields $F(x^k) \leq F( x^{*}) + \mu_k (x^k - \hat{x}^k)^\top (x^* - x^k)$.

\subsection{Poof of Lemma~\ref{lem:discretizationconv}}\label{app:discretizationconv}
    %For any limit value $x \in \mathbb{R}^m$ of $(x^k)_{k \in \mathbb{N}}$, we notice first that $x$ belongs to $\mathcal{X}$, since this set is closed. 
    Let $t^+ = \max \{ t,0 \}$ be the positive part of function $t$. We notice that the sequence $\phi(x^k)^+$ is bounded, and thus admits at least one accumulation value $\ell$. We are going to prove that $\ell = 0$.
    Let $\psi:\mathbb{N} \to \mathbb{N}$ be an increasing function, such that $\phi(x^{\psi(k)})^+ \rightarrow \ell$. 
    By compactness of $\mathcal{X}$ (resp. $\mathcal{Y}$), %up to the extraction of a subsequence of $x^{\psi(k)}$ (resp. $y^{\psi(k)}$), 
    we also assume that $x^{\psi(k)} \rightarrow x \in \mathcal{X}$ (resp. $y^{\psi(k)} \rightarrow y \in \mathcal{Y}$). For $(*)$, $G(x^{\psi(k)},y^j) \leq 0$ for all $j \in \{0,\dots, \psi(k) - 1\}$; in particular, $G(x^{\psi(k)},y^{\psi(k-1)}) \leq 0$.
    We deduce that $G(x^{\psi(k)},y^{\psi(k)}) \leq  G(x^{\psi(k)},y^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k-1)}),$ and therefore, since the positive part of a function is non-decreasing,\vspace{-0.6em}
\begin{align}
     \Bigl( G(x^{\psi(k)},y^{\psi(k)})\Bigr)^+ \leq  \Bigl( G(x^{\psi(k)},y^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k-1)})\Bigr)^+.
     \label{eq:diffremark}
\end{align}
 According to the definition of the $\delta$-oracle, Eqs.~\eqref{eq:oraclebound} yields $\phi(x^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k)}) \leq \delta \lvert \phi(x^{\psi(k)}) \rvert$. If $\phi(x^{\psi(k)}) \geq 0$, this means that $ \phi(x^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k)}) \leq \delta \: \phi(x^{\psi(k)})$, i.e., $\phi(x^{\psi(k)}) \leq \frac{1}{1 - \delta} G(x^{\psi(k)},y^{\psi(k)})$. To also cover the case, $\phi(x^{\psi(k)}) < 0$, we can write $\phi(x^{\psi(k)})^+ \leq \frac{1}{1 - \delta} G(x^{\psi(k)},y^{\psi(k)})^+$.  As $\frac{1}{1-\delta} \geq 0$, we obtain from Eq.~\eqref{eq:diffremark} that $\phi(x^{\psi(k)})^+ \leq \frac{1}{1 - \delta} \Bigl( G(x^{\psi(k)},y^{\psi(k)}) - G(x^{\psi(k)},y^{\psi(k-1)})\Bigr)^+$.   
By continuity of the functions $G$, and $\phi$, and the positive part, we deduce that $\ell = \phi(x)^+ \leq \frac{1}{1 - \delta} \left( G(x,y) - G(x,y)\right)^+ = 0,$    
since $(x^{\psi(k)},y^{\psi(k)})$ and $(x^{\psi(k)},y^{\psi(k-1)})$ both converges towards $(x,y)$. We deduce that $\phi(x^k)^+ \rightarrow 0$. 
\end{appendices}

\end{document}
