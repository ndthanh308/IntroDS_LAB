\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{barbano2022two}
Barbano, C.A., Tartaglione, E., Berzovini, C., Calandri, M., Grangetto, M.: A
  two-step radiologist-like approach for covid-19 computer-aided diagnosis from
  chest x-ray images. In: Image Analysis and Processing--ICIAP 2022: 21st
  International Conference, Lecce, Italy, May 23--27, 2022, Proceedings, Part
  I. pp. 173--184. Springer (2022)

\bibitem{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.: Language models
  are few-shot learners. Advances in neural information processing systems
  \textbf{33},  1877--1901 (2020)

\bibitem{chaudhry2022lung}
Chaudhry, H.A.H., Renzulli, R., Perlo, D., Santinelli, F., Tibaldi, S.,
  Cristiano, C., Grosso, M., Fiandrotti, A., Lucenteforte, M., Cavagnino, D.:
  Lung nodules segmentation with deephealth toolkit. In: Image Analysis and
  Processing. ICIAP 2022 Workshops: ICIAP International Workshops, Lecce,
  Italy, May 23--27, 2022, Revised Selected Papers, Part I. pp. 487--497.
  Springer (2022)

\bibitem{dai2021coatnet}
Dai, Z., Liu, H., Le, Q.V., Tan, M.: Coatnet: Marrying convolution and
  attention for all data sizes. Advances in Neural Information Processing
  Systems  \textbf{34},  3965--3977 (2021)

\bibitem{dehghani2023scaling}
Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J.,
  Steiner, A., Caron, M., Geirhos, R., Alabdulmohsin, I., et~al.: Scaling
  vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442
   (2023)

\bibitem{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for
  image recognition at scale. In: International Conference on Learning
  Representations (2021)

\bibitem{DosovitskiyB0WZ21}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for
  image recognition at scale. In: 9th International Conference on Learning
  Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021.
  OpenReview.net (2021)

\bibitem{esser2021taming}
Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution
  image synthesis. In: Proceedings of the IEEE/CVF conference on computer
  vision and pattern recognition. pp. 12873--12883 (2021)

\bibitem{Gale_Magnitude}
Gale, T., Elsen, E., Hooker, S.: The state of sparsity in deep neural networks.
  arXiv preprint arXiv:1902.09574  (2019)

\bibitem{han2015learning}
Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections
  for efficient neural network. Advances in neural information processing
  systems  \textbf{28} (2015)

\bibitem{SparseDoubleDescent}
He, Z., Xie, Z., Zhu, Q., Qin, Z.: Sparse double descent: Where network pruning
  aggravates overfitting. In: International Conference on Machine Learning. pp.
  8635--8659. PMLR (2022)

\bibitem{transformers_in_vision_survey2022}
Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.:
  Transformers in vision: A survey. ACM Comput. Surv.  \textbf{54}(10s) (sep
  2022)

\bibitem{Liu2021SwinTH}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
  transformer: Hierarchical vision transformer using shifted windows. 2021
  IEEE/CVF International Conference on Computer Vision (ICCV) pp. 9992--10002
  (2021)

\bibitem{ma2020normalized}
Ma, X., Huang, H., Wang, Y., Romano, S., Erfani, S., Bailey, J.: Normalized
  loss functions for deep learning with noisy labels. In: International
  conference on machine learning. pp. 6543--6553. PMLR (2020)

\bibitem{Noisy_labels}
Ma, X., Wang, Y., Houle, M.E., Zhou, S., Erfani, S., Xia, S., Wijewickrema, S.,
  Bailey, J.: Dimensionality-driven learning with noisy labels. In:
  International Conference on Machine Learning. pp. 3355--3364. PMLR (2018)

\bibitem{mazzeo2022image}
Mazzeo, P.L., Frontoni, E., Sclaroff, S., Distante, C.: Image Analysis and
  Processing. ICIAP 2022 Workshops: ICIAP International Workshops, Lecce,
  Italy, May 23--27, 2022, Revised Selected Papers, Part I, vol. 13373.
  Springer Nature (2022)

\bibitem{Nakkiran2021Deep}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., Sutskever, I.: Deep
  double descent: Where bigger models and more data hurt. In: International
  Conference on Learning Representations (2020)

\bibitem{nakkiran2021optimal}
Nakkiran, P., Venkat, P., Kakade, S.M., Ma, T.: Optimal regularization can
  mitigate double descent. In: International Conference on Learning
  Representations (2021)

\bibitem{quetu2023dodging}
Qu{\'e}tu, V., Tartaglione, E.: Dodging the sparse double descent. arXiv
  preprint arXiv:2303.01213  (2023)

\bibitem{quétu2023avoid}
Quétu, V., Tartaglione, E.: Can we avoid double descent in deep neural
  networks? (2023)

\bibitem{SpringenbergDBR14}
Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.A.: Striving for
  simplicity: The all convolutional net. In: Bengio, Y., LeCun, Y. (eds.) 3rd
  International Conference on Learning Representations, {ICLR} 2015, San Diego,
  CA, USA, May 7-9, 2015, Workshop Track Proceedings (2015)

\bibitem{sukhbaatar2014training}
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., Fergus, R.: Training
  convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080
  (2014)

\bibitem{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J{\'e}gou, H.:
  Training data-efficient image transformers \& distillation through attention.
  In: International conference on machine learning. pp. 10347--10357. PMLR
  (2021)

\bibitem{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
  Kaiser, {\L}., Polosukhin, I.: Attention is all you need. Advances in neural
  information processing systems  \textbf{30} (2017)

\bibitem{wei2022learning}
Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., Liu, Y.: Learning with noisy
  labels revisited: A study using real-world human annotations. In:
  International Conference on Learning Representations (2022)

\bibitem{yilmaz2022regularization}
Yilmaz, F.F., Heckel, R.: Regularization-wise double descent: Why it occurs and
  how to eliminate it. In: 2022 IEEE International Symposium on Information
  Theory (ISIT). pp. 426--431. IEEE (2022)

\bibitem{yu2022width}
Yu, F., Huang, K., Wang, M., Cheng, Y., Chu, W., Cui, L.: Width \& depth
  pruning for vision transformers. In: Proceedings of the AAAI Conference on
  Artificial Intelligence. vol.~36, pp. 3143--3151 (2022)

\bibitem{yuan2021tokens}
Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.H., Tay, F.E., Feng,
  J., Yan, S.: Tokens-to-token vit: Training vision transformers from scratch
  on imagenet. In: Proceedings of the IEEE/CVF international conference on
  computer vision. pp. 558--567 (2021)

\end{thebibliography}
