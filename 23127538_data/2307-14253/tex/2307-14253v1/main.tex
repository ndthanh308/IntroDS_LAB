% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{amsmath,amssymb} % for \boldsymbol macro
\usepackage{bm}      % for \bm macro
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{hyperref}
\newcommand{\enzo}[1]{\textcolor{purple}{Enzo: #1}}
\newcommand{\victor}[1]{\textcolor{blue}{#1}}
\newcommand{\marta}[1]{\textcolor{orange}{Marta: #1}}
\newcommand{\adc}[1]{\textcolor{red}{After draft completed: #1}}
\usepackage{soul}
\usepackage{enumitem}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
%\title{$n$-NeRF: Ensembling of low-resolution Neural Radiance Fields beats higher resolution models}
%\title{Think Small, Go Big: Achieving High-Quality 3D Scene Modeling with a NeRF Ensemble}
\title{Sparse Double Descent in Vision Transformers: real or phantom threat?}
%
\titlerunning{}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Victor Qu\'etu\orcidID{0009-0004-2795-3749} \and Marta Milovanovi\'c\orcidID{0000-0002-3280-2396} \and Enzo Tartaglione\orcidID{0000-0003-4274-8298}} %\\\and Marco Grangetto$^{1}$}%\orcidID{2222--3333-4444-5555}

%
\authorrunning{V. Qu\'etu et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{LTCI, T\'el\'ecom Paris, Institut Polytechnique de Paris, France
\email{\{name.surname\}@telecom-paris.fr}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Vision transformers (ViT) have been of broad interest in recent theoretical and empirical works. They are state-of-the-art thanks to their attention-based approach, which boosts the identification of key features and patterns within images thanks to the capability of avoiding inductive bias, resulting in highly accurate image analysis.
Meanwhile, neoteric studies have reported a ``sparse double descent'' phenomenon that can occur in modern deep-learning models, where extremely over-parametrized models can generalize well.
This raises practical questions about the optimal size of the model and the quest over finding the best trade-off between sparsity and performance is launched: are Vision Transformers also prone to sparse double descent? Can we find a way to avoid such a phenomenon?\\
Our work tackles the occurrence of sparse double descent on ViTs. Despite some works that have shown that traditional architectures, like Resnet, are condemned to the sparse double descent phenomenon, for ViTs we observe that an optimally-tuned $\ell_2$ regularization relieves such a phenomenon. However, everything comes at a cost: optimal lambda will sacrifice the potential compression of the ViT.

\keywords{Sparse double descent, transformers, pruning, deep learning}
\end{abstract}
%
%
%
\input{sections/1_introduction.tex}
\input{sections/2_sota.tex}
\input{sections/3_method}
\input{sections/4_results}
\input{sections/5_conclusion}


\noindent\textbf{Acknowledgments.} This project was provided with computer and storage resources by GENCI at IDRIS thanks to the grant 2022-AD011013930 on the supercomputer Jean Zay's the V100 partition.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{main}
%
\end{document}
