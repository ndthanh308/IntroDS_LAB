\section{Conclusion}

This paper investigates the occurrence of Sparse Double Descent in the Vision Transformer architecture. SDD is a phenomenon carefully explored due to its influence on determining the optimal model size necessary for maintaining the performance of over-parametrized models. We observe that, indeed, ViT is also susceptible to SDD. Moreover, we study different values for $\ell_2$ regularization and discover that, unlike for other CNN architectures like ResNet, we can find the optimal value and completely avoid SDD. However, the regularization comes at a price - at the same time, it renders the model less compressible, because of the strong enforced prior. We postulate that this is possible due to the lack of strong inductive bias in ViT, which enables strong regularization regimes, impossible for CNNs. Finally, we inspect the trade-off between avoiding SDD (enhancing hence model's performance) and favoring the model compressibility, observing that, for the second one, we would like to favor SDD. This study hopes to inform the community about the risk of SDD ViT models might incur, which depending on the final scope of the trained model can be a real or a phantom threat.