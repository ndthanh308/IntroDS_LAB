\section{Experiments}
% Figure environment removed
% Figure environment removed
\textbf{Setup.} For the experimental setup, we follow the same approach as He~et~al.~\cite{SparseDoubleDescent}. The first model we train is a ResNet-18, trained on CIFAR-10 \& CIFAR-100, for 160 epochs, optimized with SGD, having momentum 0.9, a learning rate of 0.1 decayed by a factor 0.1 at milestones 80 and 120, batch size 128 and $\lambda$ $10^{-4}$. The second model is a ViT with 4 patches, 8 heads, and 512 embedding dimensions, trained on CIFAR-10 and CIFAR-100 for 200 epochs, optimized with Adam, having a learning rate of $10^{-4}$ with a cosine annealing schedule and $\lambda$ 0.03.
For each dataset, a percentage $\varepsilon$ of symmetric, noisy labels are introduced: the labels of a given proportion of training samples are flipped to one of the other class labels, selected with equal probability~\cite{Noisy_labels}. In our experiments, we test with $\varepsilon \in \{10\%, 20\%, 50\%\}$. Moreover, as synthetic noise has clean structures which greatly enabled statistical analyses but often fails to model real-world noise patterns, we also conducted experiments without adding synthetic noise. With the same architectures and learning policies presented above, we carried out experiments on CIFAR-100N, which is formed by the CIFAR-100 training dataset with human-annotated real-world noisy labels collected from Amazon Mechanical Turk~\cite{wei2022learning}. 
In all experiments, we set $\zeta^{\text{iter}}=20\%$ and $\zeta^{\text{end}}=99.99\%$.\footnote{The code is available at \url{https://github.com/VGCQ/SDD_ViT}}

\noindent\textbf{Occurrence of sparse double descent.} Fig.~\ref{fig:CIFAR-ViT} displays the results of ResNet-18 and ViT, on CIFAR-10 and CIFAR-100. As in He~et~al.~\cite{SparseDoubleDescent} work, the double descent consists of 4 phases. First, at low sparsities, the network is overparameterized, thus pruned network can still reach similar accuracy to the dense model. The second phase is a phase near the ``interpolation threshold'', where %training accuracy is going to drop, and 
the test accuracy is about to first decrease and then increase as sparsity grows. The third phase is located at high sparsities, where test accuracy is rising. The final phase happens when both training and test accuracy drop significantly. For every value of $\varepsilon$, whether on CIFAR-10 or CIFAR-100, the sparse double descent phenomenon occurs both for ResNet and ViT. We observe a similar phenomenon as in the simulated $\varepsilon$ also in the human-annotated CIFAR-100N.

\noindent\textbf{Study on $\boldsymbol{\lambda}$.} In the previous experiments in Fig.~\ref{fig:CIFAR-ViT}, ViTs were trained with a $\ell_2$-regularization hyper-parameter equal to 0.03, which is typically used in other works.
However, it has been recently shown that, for certain linear regression models with isotropic data distribution, optimally-tuned $\ell_2$ regularization can achieve monotonic test performance as either the sample size or the model size is grown. Nakkiran~et~al.~\cite{nakkiran2021optimal} demonstrated it analytically and established that optimally-tuned $\ell_2$ regularization can mitigate double descent for general models, including neural networks like Convolutional Neural Networks. Moreover, a recent study showed that $\ell_2$ regularization is positively contributing to the avoidance of sparse double descent in an image classification context, but is not the antidote to ``dodge'' it~\cite{quÃ©tu2023avoid}. %\victor{Do we specify that the same was done in ICIP paper for ResNet and was not conclusive ?}\enzo{Yes, saying that a recent study showed this, observing non conclusive results...and argumenting a bit}
Hence, we propose in Fig.~\ref{fig:Varying lambda for CIFAR-10} a quantitative study over $\lambda$ for ViT on CIFAR-10 with $\varepsilon=10\%$. 
With small values of $\lambda$, i.e. below $1$, the sparse double descent is empirically noticeable. The increment of $\lambda$ pushes the occurrence of the phenomenon towards smaller sparsity values. Looking at the loss, increasing $\lambda$ smoothens the bump of the test loss and at some point, i.e. $\lambda=1$, the test loss becomes flat and behaves monotonically: the sparse double descent is avoided. For $\lambda>1$, the phenomenon also results avoided, but the performance worsens (lighter blue region at the bottom right corner) since the regularization is stronger. Note that with higher $\lambda$, performances are better but the maximum sparsity achievable is not as high as for lower values of $\lambda$.
% Figure environment removed
% Figure environment removed

\noindent\textbf{Avoidance of the Sparse Double Descent.} 
As $\lambda=1$ seems to be an optimal value enabling dodging SDD on CIFAR-10 with $\varepsilon=10\%$, we try to use this value for other setups. Fig.~\ref{fig:CIFAR-ViT-wd} displays the results of ViT on CIFAR-10/CIFAR-100 with $\varepsilon \in \{10\%, 20\%, 50\%\}$ and $\lambda=1$. For small noise rates, i.e. $\varepsilon \leq 20\%$, the phenomenon vanishes and performance is enhanced. However, for higher noise rates, like $\varepsilon=50\%$, SDD is mitigated, but still present. Even if it already helps, it seems that the strength of the regularization is not high enough to completely avoid SDD. Indeed, with a higher $\lambda$, i.e. 3, the performance becomes monotonic.
\noindent\textbf{Trade-off between SDD and compressibility.} Fig.~\ref{fig:Varying lambda for CIFAR-10}, supported also by the experiments displayed in Fig.~\ref{fig:CIFAR-ViT-wd}, suggests that at high regularization regimes, where we avoid SDD, the ability to compress the model is harmed. This is due to the strong prior we impose over the distribution of the parameters of the model: the stronger this is, the least we are indeed able to remove degrees of freedom from our system. As a visual example, Fig.~\ref{fig:Histograms} displays the distribution of the parameters for one of the considered training configurations, for $\lambda=0.03$ and $1$, without pruning and after two pruning steps. We observe that despite removing the same quantity of parameters, with higher regularization the parameters have less variance, which has the dual effect of both making them more robust to injected noise (due to the strong regularization) but, at the same time, this distribution is more sensitive to compression by pruning. Hence, we conclude that, in case we wish to have a robust, well-generalizing model, we wish to avoid SDD and employ strong $\ell_2$ regularization; on the contrary, if we target compressibility, we would like to favor SDD, as the better generalizing region is pushed to highly compressed regions.