\section{Background on Vision Transformers}
Vision Transformers~\cite{DosovitskiyB0WZ21}, presented in Fig.~\ref{ViT}, use self-attention mechanisms to capture the relationships between the elements of an input image. They essentially consist of four key elements: patch embedding, positional encoding, the transformer encoder, shown in Fig.~\ref{Transformer_encoder}, and the classification head.\\
\noindent\textbf{Patch embedding.} The input image is divided into a grid of patches (which can or can not be non-overlapping), each containing a fixed number of pixels. These patches are then linearly projected to a lower-dimensional embedding space. This process converts the spatial information of the image into a sequence of patch embeddings, mimicking the process of text embedding. \\
\noindent\textbf{Positional encoding.} To preserve the positional information of the image patches, positional encoding is added to the patch embeddings: this allows to distinguish different patches and capture their global positions in the image.
% Figure environment removed\\
\noindent\textbf{The transformer encoder.} The patch embeddings, along with their positional encodings, are fed into a stack of transformer encoder layers. Each encoder layer consists of two sub-layers: a multi-head self-attention mechanism, and a multi-layer perception (MLP). The self-attention mechanism enables the model to capture global interactions between patches by attending to all patches and aggregating information accordingly, thus performing feature aggregation. More specifically, self-attention operations determine the attention output $\boldsymbol{a}$ based on the relevance of one item to others. This is iteratively refined and computed using keys $\boldsymbol{k}$ and queries $\boldsymbol{q}$, which have the same dimension \emph{d}, and values $\boldsymbol{v}$~\cite{vaswani2017attention}. The keys are the indices of the hidden states of the encoded input items, and each key $k_i$ has some associated value $v_i$. Each query $q_i$ represents an output coming from the encoded target item (class). The attention is computed as the softmax of the product between $\boldsymbol{q}$ and $\boldsymbol{k}$, and then multiplied by $\boldsymbol{v}$: the model learns to prioritize important input features and capture more informative representations of the input data. Many attention heads are hence concatenated, to form the multi-head, to obtain contextualized representations that include both local and global information. Finally, an MLP carries out feature transformation.\\
\noindent \textbf{The classification head.} At the end of the transformer encoder stack, a classification head is attached to the output of the final transformer layer. The classification head can take various forms, such as a simple fully connected layer or a combination of linear and softmax layers. It maps the aggregated representation of the patches to class probabilities, in order to perform image classification.\\
\noindent \textbf{Training.} ViT models require pretraining on enormous datasets (such as JFT-300M, consisting of approximately 300 million images) due to their lack of strong inductive bias, which is present in other architectures like CNNs~\cite{dehghani2023scaling}. Despite recent advances in learning on smaller datasets using distillation approaches or optimizing models with smaller sizes, transformers still have larger model architectures compared to CNN-based models and require large datasets for optimal performance, despite requiring less computational resources~\cite{DosovitskiyB0WZ21}. However, \cite{transformers_in_vision_survey2022} concludes that scaling up Transformer models improves performance, but with current designs, it is computationally expensive and necessitates efficient designs.\\
\noindent \textbf{Beyond traditional ViT.} In the last few years, many different Vision Transformer designs have been proposed to improve the performance of computer vision tasks. One of the most popular is SWIN~\cite{Liu2021SwinTH} which proposes shifted windows to create overlapping receptive fields, cascaded stages to mimic a multi-resolution approach, tokenization of windows, and token shifts across the stages. As it is possible to imagine, this architecture already goes in the direction of customizing the Transformer architecture to process images. Other newly proposed transformers variants include, among others, CoaT~\cite{dai2021coatnet}, TNT~\cite{yuan2021tokens}, and DeiT~\cite{touvron2021training}.
