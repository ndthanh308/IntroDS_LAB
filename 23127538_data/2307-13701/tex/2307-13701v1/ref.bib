
@misc{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	doi = {10.48550/arXiv.1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	urldate = {2023-01-23},
	publisher = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	note = {arXiv:1412.6575 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\6CKD4P8Z\\Yang et al. - 2015 - Embedding Entities and Relations for Learning and .pdf:application/pdf},
}

@article{libkin_open_2009,
	title = {Open and {Closed} {World} {Assumptions} in {Data} {Exchange}.},
	volume = {477},
	journal = {Description Logics},
	author = {Libkin, Leonid and Sirangelo, Cristina},
	year = {2009},
}

@article{vrandecic_wikidata_2014,
	title = {Wikidata: a free collaborative knowledgebase},
	volume = {57},
	number = {10},
	journal = {Communications of the ACM},
	author = {Vrande{\v c}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
	year = {2014},
	note = {Publisher: ACM New York, NY, USA},
	pages = {78--85},
}

@inproceedings{liu_mask_2022,
	title = {Mask and {Reason}: {Pre}-{Training} {Knowledge} {Graph} {Transformers} for {Complex} {Logical} {Queries}},
	shorttitle = {Mask and {Reason}},
	url = {http://arxiv.org/abs/2208.07638},
	doi = {10.1145/3534678.3539472},
	abstract = {Knowledge graph (KG) embeddings have been a mainstream approach for reasoning over incomplete KGs. However, limited by their inherently shallow and static architectures, they can hardly deal with the rising focus on complex logical queries, which comprise logical operators, imputed edges, multiple source entities, and unknown intermediate entities. In this work, we present the Knowledge Graph Transformer (kgTransformer) with masked pre-training and fine-tuning strategies. We design a KG triple transformation method to enable Transformer to handle KGs, which is further strengthened by the Mixture-of-Experts (MoE) sparse activation. We then formulate the complex logical queries as masked prediction and introduce a two-stage masked pre-training strategy to improve transferability and generalizability. Extensive experiments on two benchmarks demonstrate that kgTransformer can consistently outperform both KG embedding-based baselines and advanced encoders on nine in-domain and out-of-domain reasoning tasks. Additionally, kgTransformer can reason with explainability via providing the full reasoning paths to interpret given answers.},
	urldate = {2022-12-16},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Liu, Xiao and Zhao, Shiyu and Su, Kai and Cen, Yukuo and Qiu, Jiezhong and Zhang, Mengdi and Wu, Wei and Dong, Yuxiao and Tang, Jie},
	month = aug,
	year = {2022},
	note = {arXiv:2208.07638 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {1120--1130},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\TXYWPAV8\\Liu ? - 2022 - Mask and Reason Pre-Training Knowledge Graph Tran.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\8XALZDIP\\2208.html:text/html},
}

@article{ic_online_nodate,
	title = {The online encyclopedia {Wikipedia} is being supplemented by user- edited structured data, available for free to anyone.},
	language = {en},
	author = {Ic, Denny Vrandec? and Kr{\"o}tzsch, Markus},
}

@article{van_krieken_analyzing_2022,
	title = {Analyzing differentiable fuzzy logic operators},
	volume = {302},
	journal = {Artificial Intelligence},
	author = {van Krieken, Emile and Acar, Erman and van Harmelen, Frank},
	year = {2022},
	note = {Publisher: Elsevier},
	keywords = {Fuzzy logic, Learning with constraints, Neural-symbolic AI},
	pages = {103602},
	file = {van Krieken et al. - 2022 - Analyzing differentiable fuzzy logic operators.pdf:C\:\\Users\\hyin\\Zotero\\storage\\R8IA9MAH\\van Krieken et al. - 2022 - Analyzing differentiable fuzzy logic operators.pdf:application/pdf},
}

@article{daza_message_2020,
	title = {Message {Passing} {Query} {Embedding}},
	url = {https://arxiv.org/abs/2002.02406v2},
	doi = {10.48550/arXiv.2002.02406},
	abstract = {Recent works on representation learning for Knowledge Graphs have moved beyond the problem of link prediction, to answering queries of an arbitrary structure. Existing methods are based on ad-hoc mechanisms that require training with a diverse set of query structures. We propose a more general architecture that employs a graph neural network to encode a graph representation of the query, where nodes correspond to entities and variables. The generality of our method allows it to encode a more diverse set of query types in comparison to previous work. Our method shows competitive performance against previous models for complex queries, and in contrast with these models, it can answer complex queries when trained for link prediction only. We show that the model learns entity embeddings that capture the notion of entity type without explicit supervision.},
	language = {en},
	urldate = {2023-01-21},
	author = {Daza, Daniel and Cochez, Michael},
	month = feb,
	year = {2020},
}

@inproceedings{minervini_complex_2022,
	title = {Complex {Query} {Answering} with {Neural} {Link} {Predictors} ({Extended} {Abstract})*},
	url = {https://doi.org/10.24963/ijcai.2022/741},
	doi = {10.24963/ijcai.2022/741},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI}-22},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Minervini, Pasquale and Arakelyan, Erik and Daza, Daniel and Cochez, Michael},
	editor = {Raedt, Lud De},
	month = jul,
	year = {2022},
	pages = {5309--5313},
	file = {Minervini et al. - 2022 - Complex Query Answering with Neural Link Predictor.pdf:C\:\\Users\\hyin\\Zotero\\storage\\ZYRMSGAX\\Minervini et al. - 2022 - Complex Query Answering with Neural Link Predictor.pdf:application/pdf},
}

@misc{yang_gammae_2022,
	title = {{GammaE}: {Gamma} {Embeddings} for {Logical} {Queries} on {Knowledge} {Graphs}},
	shorttitle = {{GammaE}},
	url = {http://arxiv.org/abs/2210.15578},
	doi = {10.48550/arXiv.2210.15578},
	abstract = {Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model the negation and union operator. The negation operator has no strict boundaries, which generates overlapped embeddings and leads to obtaining ambiguous answers. An additional limitation is that the union operator is non-closure, which undermines the model to handle a series of union operators. To address these problems, we propose a novel probabilistic embedding model, namely Gamma Embeddings (GammaE), for encoding entities and queries to answer different types of FOL queries on KGs. We utilize the linear property and strong boundary support of the Gamma distribution to capture more features of entities and queries, which dramatically reduces model uncertainty. Furthermore, GammaE implements the Gamma mixture method to design the closed union operator. The performance of GammaE is validated on three large logical query datasets. Experimental results show that GammaE significantly outperforms state-of-the-art models on public benchmarks.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Yang, Dong and Qing, Peijun and Li, Yang and Lu, Haonan and Lin, Xiaodong},
	month = oct,
	year = {2022},
	note = {arXiv:2210.15578 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\S6NMUGIM\\Yang et al. - 2022 - GammaE Gamma Embeddings for Logical Queries on Kn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\CDWP5L8D\\2210.html:text/html},
}

@inproceedings{thiele_t-quantifiers_1994,
	title = {On {T}-quantifiers and {S}-quantifiers},
	booktitle = {Proceedings of 24th {International} {Symposium} on {Multiple}-{Valued} {Logic} ({ISMVL}'94)},
	publisher = {IEEE},
	author = {Thiele, Helmut},
	year = {1994},
	pages = {264--269},
	file = {Thiele - 1994 - On T-quantifiers and S-quantifiers.pdf:C\:\\Users\\hyin\\Zotero\\storage\\EPJIN6MT\\Thiele - 1994 - On T-quantifiers and S-quantifiers.pdf:application/pdf},
}

@book{gottwald_fuzzy_2013,
	title = {Fuzzy sets and fuzzy logic: {The} foundations of application{\textemdash}from a mathematical point of view},
	publisher = {Springer-Verlag},
	author = {Gottwald, Siegfried},
	year = {2013},
}

@book{klir_fuzzy_1995,
	title = {Fuzzy sets and fuzzy logic},
	volume = {4},
	publisher = {Prentice hall New Jersey},
	author = {Klir, George and Yuan, Bo},
	year = {1995},
}

@book{hajek_metamathematics_2013,
	title = {Metamathematics of fuzzy logic},
	volume = {4},
	publisher = {Springer Science \& Business Media},
	author = {H{\'a}jek, Petr},
	year = {2013},
	file = {H{\'a}jek - 2013 - Metamathematics of fuzzy logic.pdf:C\:\\Users\\hyin\\Zotero\\storage\\5GJINWPF\\H{\'a}jek - 2013 - Metamathematics of fuzzy logic.pdf:application/pdf},
}

@article{dalvi_dichotomy_2013,
	title = {The dichotomy of probabilistic inference for unions of conjunctive queries},
	volume = {59},
	number = {6},
	journal = {Journal of the ACM (JACM)},
	author = {Dalvi, Nilesh and Suciu, Dan},
	year = {2013},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--87},
}

@misc{dalvi_dichotomy_2007,
	title = {The {Dichotomy} of {Conjunctive} {Queries} on {Probabilistic} {Structures}},
	url = {http://arxiv.org/abs/cs/0612102},
	doi = {10.48550/arXiv.cs/0612102},
	abstract = {We show that for every conjunctive query, the complexity of evaluating it on a probabilistic database is either {\textbackslash}PTIME or \#{\textbackslash}P-complete, and we give an algorithm for deciding whether a given conjunctive query is {\textbackslash}PTIME or \#{\textbackslash}P-complete. The dichotomy property is a fundamental result on query evaluation on probabilistic databases and it gives a complete classification of the complexity of conjunctive queries.},
	urldate = {2023-01-14},
	publisher = {arXiv},
	author = {Dalvi, Nilesh and Suciu, Dan},
	month = jan,
	year = {2007},
	note = {arXiv:cs/0612102},
	keywords = {Computer Science - Databases},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\NSJDMGHY\\Dalvi ? Suciu - 2007 - The Dichotomy of Conjunctive Queries on Probabilis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\T84XT9TB\\0612102.html:text/html},
}

@misc{wang_simple_2022,
	title = {Simple and {Effective} {Relation}-based {Embedding} {Propagation} for {Knowledge} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2205.06456},
	doi = {10.48550/arXiv.2205.06456},
	abstract = {Relational graph neural networks have garnered particular attention to encode graph context in knowledge graphs (KGs). Although they achieved competitive performance on small KGs, how to efficiently and effectively utilize graph context for large KGs remains an open problem. To this end, we propose the Relation-based Embedding Propagation (REP) method. It is a post-processing technique to adapt pre-trained KG embeddings with graph context. As relations in KGs are directional, we model the incoming head context and the outgoing tail context separately. Accordingly, we design relational context functions with no external parameters. Besides, we use averaging to aggregate context information, making REP more computation-efficient. We theoretically prove that such designs can avoid information distortion during propagation. Extensive experiments also demonstrate that REP has significant scalability while improving or maintaining prediction quality. Notably, it averagely brings about 10\% relative improvement to triplet-based embedding methods on OGBL-WikiKG2 and takes 5\%-83\% time to achieve comparable results as the state-of-the-art GC-OTE.},
	urldate = {2023-01-05},
	publisher = {arXiv},
	author = {Wang, Huijuan and Dai, Siming and Su, Weiyue and Zhong, Hui and Fang, Zeyang and Huang, Zhengjie and Feng, Shikun and Chen, Zeyu and Sun, Yu and Yu, Dianhai},
	month = may,
	year = {2022},
	note = {arXiv:2205.06456 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\ZZ5HDQT4\\Wang ? - 2022 - Simple and Effective Relation-based Embedding Prop.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\H69J398Y\\2205.html:text/html},
}

@misc{yang_rethinking_2022,
	title = {Rethinking {Knowledge} {Graph} {Evaluation} {Under} the {Open}-{World} {Assumption}},
	url = {http://arxiv.org/abs/2209.08858},
	doi = {10.48550/arXiv.2209.08858},
	abstract = {Most knowledge graphs (KGs) are incomplete, which motivates one important research topic on automatically complementing knowledge graphs. However, evaluation of knowledge graph completion (KGC) models often ignores the incompleteness -- facts in the test set are ranked against all unknown triplets which may contain a large number of missing facts not included in the KG yet. Treating all unknown triplets as false is called the closed-world assumption. This closed-world assumption might negatively affect the fairness and consistency of the evaluation metrics. In this paper, we study KGC evaluation under a more realistic setting, namely the open-world assumption, where unknown triplets are considered to include many missing facts not included in the training or test sets. For the currently most used metrics such as mean reciprocal rank (MRR) and Hits@K, we point out that their behavior may be unexpected under the open-world assumption. Specifically, with not many missing facts, their numbers show a logarithmic trend with respect to the true strength of the model, and thus, the metric increase could be insignificant in terms of reflecting the true model improvement. Further, considering the variance, we show that the degradation in the reported numbers may result in incorrect comparisons between different models, where stronger models may have lower metric numbers. We validate the phenomenon both theoretically and experimentally. Finally, we suggest possible causes and solutions for this problem. Our code and data are available at https://github.com/GraphPKU/Open-World-KG .},
	urldate = {2023-01-05},
	publisher = {arXiv},
	author = {Yang, Haotong and Lin, Zhouchen and Zhang, Muhan},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08858 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\6BV87FCQ\\Yang ? - 2022 - Rethinking Knowledge Graph Evaluation Under the Op.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\TV6LZ8WJ\\2209.html:text/html},
}

@article{barcelo_graph_2021,
	title = {Graph neural networks with local graph parameters},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Barcel{\'o}, Pablo and Geerts, Floris and Reutter, Juan and Ryschkov, Maksimilian},
	year = {2021},
	pages = {25280--25293},
}

@misc{cohen_tensorlog_2016,
	title = {{TensorLog}: {A} {Differentiable} {Deductive} {Database}},
	shorttitle = {{TensorLog}},
	url = {http://arxiv.org/abs/1605.06523},
	doi = {10.48550/arXiv.1605.06523},
	abstract = {Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into "deep" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are "unrolled" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.},
	urldate = {2022-12-27},
	publisher = {arXiv},
	author = {Cohen, William W.},
	month = jul,
	year = {2016},
	note = {arXiv:1605.06523 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Databases},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\DZJTVREJ\\Cohen - 2016 - TensorLog A Differentiable Deductive Database.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\AQDLHN2P\\1605.html:text/html},
}

@article{yang_differentiable_2017,
	title = {Differentiable learning of logical rules for knowledge base reasoning},
	volume = {30},
	journal = {Advances in neural information processing systems},
	author = {Yang, Fan and Yang, Zhilin and Cohen, William W},
	year = {2017},
	file = {Yang ? - 2017 - Differentiable learning of logical rules for knowl.pdf:C\:\\Users\\hyin\\Zotero\\storage\\A8P5V6S3\\Yang ? - 2017 - Differentiable learning of logical rules for knowl.pdf:application/pdf},
}

@misc{bai_answering_2022,
	title = {Answering {Complex} {Logical} {Queries} on {Knowledge} {Graphs} via {Query} {Computation} {Tree} {Optimization}},
	url = {http://arxiv.org/abs/2212.09567},
	doi = {10.48550/arXiv.2212.09567},
	abstract = {Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries, and cannot generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Computation Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query computation tree. In particular, QTO utilizes the independence encoded in the query computation tree to reduce the search space, where only local computations are involved during the optimization procedure. Experiments on 3 datasets show that QTO obtains state-of-the-art performance on complex query answering, outperforming previous best results by an average of 22\%. Moreover, QTO can interpret the intermediate solutions for each of the one-hop atoms in the query with over 90\% accuracy.},
	urldate = {2022-12-26},
	publisher = {arXiv},
	author = {Bai, Yushi and Lv, Xin and Li, Juanzi and Hou, Lei},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09567 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Databases, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\4N9RWWGW\\Bai ? - 2022 - Answering Complex Logical Queries on Knowledge Gra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\RY78TAL8\\2212.html:text/html},
}

@article{papp_theoretical_2022,
	title = {A {Theoretical} {Comparison} of {Graph} {Neural} {Network} {Extensions}},
	journal = {arXiv preprint arXiv:2201.12884},
	author = {Papp, P{\'a}l Andr{\'a}s and Wattenhofer, Roger},
	year = {2022},
	file = {Papp ? Wattenhofer - 2022 - A Theoretical Comparison of Graph Neural Network E.pdf:C\:\\Users\\hyin\\Zotero\\storage\\C2AEZUDR\\Papp ? Wattenhofer - 2022 - A Theoretical Comparison of Graph Neural Network E.pdf:application/pdf},
}

@misc{yang_learning_2022,
	title = {Learning {Automata}-{Based} {Task} {Knowledge} {Representation} from {Large}-{Scale} {Generative} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.01944},
	doi = {10.48550/arXiv.2212.01944},
	abstract = {Automata-based representations play an important role in control and planning in sequential decision-making, but obtaining high-level task knowledge for building automata is often difficult. Although large-scale generative language models (GLMs) can help automatically distill task knowledge, the textual outputs from GLMs are not directly utilizable in sequential decision-making. We resolve this problem by proposing a novel algorithm named GLM2FSA, which obtains high-level task knowledge, represented in a finite state automaton (FSA), from a given brief description of the task goal. GLM2FSA sends queries to a GLM for task knowledge in textual form and then builds a FSA to represent the textual knowledge. This algorithm fills the gap between text and automata-based representations, and the constructed FSA can be directly utilized in sequential decision-making. We provide examples to demonstrate how GLM2FSA constructs FSAs to represent knowledge encoded in the texts generated by the large-scale GLMs.},
	urldate = {2022-12-22},
	publisher = {arXiv},
	author = {Yang, Yunhao and Gaglione, Jean-Rapha{\"e}l and Topcu, Ufuk},
	month = dec,
	year = {2022},
	note = {arXiv:2212.01944 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\5AE44777\\Yang ? - 2022 - Learning Automata-Based Task Knowledge Representat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\RCYGG6EP\\2212.html:text/html},
}

@article{cai_optimal_1992,
	title = {An optimal lower bound on the number of variables for graph identification},
	volume = {12},
	number = {4},
	journal = {Combinatorica},
	author = {Cai, Jin-Yi and F{\"u}rer, Martin and Immerman, Neil},
	year = {1992},
	note = {Publisher: Springer},
	pages = {389--410},
	file = {Cai ? - 1992 - An optimal lower bound on the number of variables .pdf:C\:\\Users\\hyin\\Zotero\\storage\\YRN3SUY6\\Cai ? - 1992 - An optimal lower bound on the number of variables .pdf:application/pdf},
}

@article{weisfeiler_reduction_1968,
	title = {The reduction of a graph to canonical form and the algebra which appears therein},
	volume = {2},
	number = {9},
	journal = {NTI, Series},
	author = {Weisfeiler, Boris and Leman, Andrei},
	year = {1968},
	pages = {12--16},
	file = {Weisfeiler ? Leman - 1968 - The reduction of a graph to canonical form and the.pdf:C\:\\Users\\hyin\\Zotero\\storage\\SCTH67WS\\Weisfeiler ? Leman - 1968 - The reduction of a graph to canonical form and the.pdf:application/pdf},
}

@inproceedings{barcelo_logical_2020,
	title = {The logical expressiveness of graph neural networks},
	booktitle = {8th {International} {Conference} on {Learning} {Representations} ({ICLR} 2020)},
	author = {Barcel{\'o}, Pablo and Kostylev, Egor V and Monet, Mikael and P{\'e}rez, Jorge and Reutter, Juan and Silva, Juan-Pablo},
	year = {2020},
	file = {Barcel{\'o} ? - 2020 - The logical expressiveness of graph neural network.pdf:C\:\\Users\\hyin\\Zotero\\storage\\KYG8ZCUK\\Barcel{\'o} ? - 2020 - The logical expressiveness of graph neural network.pdf:application/pdf},
}

@misc{berrendorf_ambiguity_2022,
	title = {On the {Ambiguity} of {Rank}-{Based} {Evaluation} of {Entity} {Alignment} or {Link} {Prediction} {Methods}},
	url = {http://arxiv.org/abs/2002.06914},
	doi = {10.48550/arXiv.2002.06914},
	abstract = {In this work, we take a closer look at the evaluation of two families of methods for enriching information from knowledge graphs: Link Prediction and Entity Alignment. In the current experimental setting, multiple different scores are employed to assess different aspects of model performance. We analyze the informativeness of these evaluation measures and identify several shortcomings. In particular, we demonstrate that all existing scores can hardly be used to compare results across different datasets. Moreover, we demonstrate that varying size of the test size automatically has impact on the performance of the same model based on commonly used metrics for the Entity Alignment task. We show that this leads to various problems in the interpretation of results, which may support misleading conclusions. Therefore, we propose adjustments to the evaluation and demonstrate empirically how this supports a fair, comparable, and interpretable assessment of model performance. Our code is available at https://github.com/mberr/rank-based-evaluation.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Berrendorf, Max and Faerman, Evgeniy and Vermue, Laurent and Tresp, Volker},
	month = may,
	year = {2022},
	note = {arXiv:2002.06914 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\7FR37PAP\\Berrendorf ? - 2022 - On the Ambiguity of Rank-Based Evaluation of Entit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\9GCFJ7R5\\2002.html:text/html},
}

@misc{saxena_question_2021,
	title = {Question {Answering} {Over} {Temporal} {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2106.01515},
	doi = {10.48550/arXiv.2106.01515},
	abstract = {Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120\% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Saxena, Apoorv and Chakrabarti, Soumen and Talukdar, Partha},
	month = jun,
	year = {2021},
	note = {arXiv:2106.01515 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\5MUMHRLK\\Saxena ? - 2021 - Question Answering Over Temporal Knowledge Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\N9SHBVPE\\2106.html:text/html},
}

@misc{alivanistos_query_2022,
	title = {Query {Embedding} on {Hyper}-relational {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2106.08166},
	doi = {10.48550/arXiv.2106.08166},
	abstract = {Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. Existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyper-relational modeling paradigm. In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. Hyper-relational queries are often observed in real-world KG applications, and existing approaches for approximate query answering cannot make use of qualifier pairs. In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries. Building upon recent advancements in Graph Neural Networks and query embedding techniques, we study how to embed and answer hyper-relational conjunctive queries. Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve query answering on a diverse set of query patterns.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Alivanistos, Dimitrios and Berrendorf, Max and Cochez, Michael and Galkin, Mikhail},
	month = sep,
	year = {2022},
	note = {arXiv:2106.08166 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Databases, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\YVRPP8SC\\Alivanistos ? - 2022 - Query Embedding on Hyper-relational Knowledge Grap.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\397PWIYG\\2106.html:text/html},
}

@misc{dudzik_graph_2022,
	title = {Graph {Neural} {Networks} are {Dynamic} {Programmers}},
	url = {http://arxiv.org/abs/2203.15544},
	doi = {10.48550/arXiv.2203.15544},
	abstract = {Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, produce better-grounded GNN architectures for edge-centric tasks, and demonstrate empirical results on the CLRS algorithmic reasoning benchmark. We hope our exposition will serve as a foundation for building stronger algorithmically aligned GNNs.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Dudzik, Andrew and Veli{\v c}kovi{\'c}, Petar},
	month = oct,
	year = {2022},
	note = {arXiv:2203.15544 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Mathematics - Category Theory},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\2TBUEDQV\\Dudzik ? Veli{\v c}kovi{\'c} - 2022 - Graph Neural Networks are Dynamic Programmers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\9AE26UV7\\2203.html:text/html},
}

@misc{galkin_nodepiece_2022,
	title = {{NodePiece}: {Compositional} and {Parameter}-{Efficient} {Representations} of {Large} {Knowledge} {Graphs}},
	shorttitle = {{NodePiece}},
	url = {http://arxiv.org/abs/2106.12144},
	doi = {10.48550/arXiv.2106.12144},
	abstract = {Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs when working with real-world KGs. Drawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies with possibly sublinear memory requirements. To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of subword/sub-entity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training. Experiments show that NodePiece performs competitively in node classification, link prediction, and relation prediction tasks while retaining less than 10\% of explicit nodes in a graph as anchors and often having 10x fewer parameters. To this end, we show that a NodePiece-enabled model outperforms existing shallow models on a large OGB WikiKG 2 graph having 70x fewer parameters.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Galkin, Mikhail and Denis, Etienne and Wu, Jiapeng and Hamilton, William L.},
	month = feb,
	year = {2022},
	note = {arXiv:2106.12144 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\9XTK7HFV\\Galkin ? - 2022 - NodePiece Compositional and Parameter-Efficient R.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\YP6V8TCV\\2106.html:text/html},
}

@misc{li_nsnet_2022,
	title = {{NSNet}: {A} {General} {Neural} {Probabilistic} {Framework} for {Satisfiability} {Problems}},
	shorttitle = {{NSNet}},
	url = {http://arxiv.org/abs/2211.03880},
	abstract = {We present the Neural Satisfiability Network (NSNet), a general neural framework that models satisfiability problems as probabilistic inference and meanwhile exhibits proper explainability. Inspired by the Belief Propagation (BP), NSNet uses a novel graph neural network (GNN) to parameterize BP in the latent space, where its hidden representations maintain the same probabilistic interpretation as BP. NSNet can be flexibly configured to solve both SAT and \#SAT problems by applying different learning objectives. For SAT, instead of directly predicting a satisfying assignment, NSNet performs marginal inference among all satisfying solutions, which we empirically find is more feasible for neural networks to learn. With the estimated marginals, a satisfying assignment can be efficiently generated by rounding and executing a stochastic local search. For \#SAT, NSNet performs approximate model counting by learning the Bethe approximation of the partition function. Our evaluations show that NSNet achieves competitive results in terms of inference accuracy and time efficiency on multiple SAT and \#SAT datasets.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Li, Zhaoyu and Si, Xujie},
	month = nov,
	year = {2022},
	note = {arXiv:2211.03880 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\HLFAZSBF\\Li ? Si - 2022 - NSNet A General Neural Probabilistic Framework fo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\3Q33GPQ2\\2211.html:text/html},
}

@article{luo_nqe_2022,
	title = {{NQE}: {N}-ary {Query} {Embedding} for {Complex} {Query} {Answering} over {Hyper}-relational {Knowledge} {Graphs}},
	journal = {arXiv preprint arXiv:2211.13469},
	author = {Luo, Haoran and Yang, Yuhao and Zhou, Gengxian and Guo, Yikai and Yao, Tianyu and Tang, Zichen and Lin, Xueyuan and Wan, Kaiyang and {others}},
	year = {2022},
	file = {Luo ? - 2022 - NQE N-ary Query Embedding for Complex Query Answe.pdf:C\:\\Users\\hyin\\Zotero\\storage\\K3V8KB9K\\Luo ? - 2022 - NQE N-ary Query Embedding for Complex Query Answe.pdf:application/pdf},
}

@article{huang_few-shot_2022,
	title = {Few-shot {Relational} {Reasoning} via {Connection} {Subgraph} {Pretraining}},
	journal = {arXiv preprint arXiv:2210.06722},
	author = {Huang, Qian and Ren, Hongyu and Leskovec, Jure},
	year = {2022},
	file = {Huang ? - 2022 - Few-shot Relational Reasoning via Connection Subgr.pdf:C\:\\Users\\hyin\\Zotero\\storage\\M99N75XG\\Huang ? - 2022 - Few-shot Relational Reasoning via Connection Subgr.pdf:application/pdf},
}

@misc{teru_inductive_2020,
	title = {Inductive {Relation} {Prediction} by {Subgraph} {Reasoning}},
	url = {http://arxiv.org/abs/1911.06962},
	doi = {10.48550/arXiv.1911.06962},
	abstract = {The dominant paradigm for relation prediction in knowledge graphs involves learning and operating on latent representations (i.e., embeddings) of entities and relations. However, these embedding-based methods do not explicitly capture the compositional logical rules underlying the knowledge graph, and they are limited to the transductive setting, where the full set of entities must be known during training. Here, we propose a graph neural network based relation prediction framework, GraIL, that reasons over local subgraph structures and has a strong inductive bias to learn entity-independent relational semantics. Unlike embedding-based models, GraIL is naturally inductive and can generalize to unseen entities and graphs after training. We provide theoretical proof and strong empirical evidence that GraIL can represent a useful subset of first-order logic and show that GraIL outperforms existing rule-induction baselines in the inductive setting. We also demonstrate significant gains obtained by ensembling GraIL with various knowledge graph embedding methods in the transductive setting, highlighting the complementary inductive bias of our method.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Teru, Komal K. and Denis, Etienne and Hamilton, William L.},
	month = feb,
	year = {2020},
	note = {arXiv:1911.06962 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\HDMP28XR\\Teru ? - 2020 - Inductive Relation Prediction by Subgraph Reasonin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\M55TFTSG\\1911.html:text/html},
}

@article{galkin_inductive_2022,
	title = {Inductive {Logical} {Query} {Answering} in {Knowledge} {Graphs}},
	journal = {arXiv preprint arXiv:2210.08008},
	author = {Galkin, Mikhail and Zhu, Zhaocheng and Ren, Hongyu and Tang, Jian},
	year = {2022},
	file = {Galkin ? - 2022 - Inductive Logical Query Answering in Knowledge Gra.pdf:C\:\\Users\\hyin\\Zotero\\storage\\URDGMMZN\\Galkin ? - 2022 - Inductive Logical Query Answering in Knowledge Gra.pdf:application/pdf},
}

@inproceedings{schlichtkrull_modeling_2018,
	title = {Modeling relational data with graph convolutional networks},
	booktitle = {European semantic web conference},
	publisher = {Springer},
	author = {Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Berg, Rianne van den and Titov, Ivan and Welling, Max},
	year = {2018},
	pages = {593--607},
	file = {Schlichtkrull ? - 2018 - Modeling relational data with graph convolutional .pdf:C\:\\Users\\hyin\\Zotero\\storage\\BKLV2SMX\\Schlichtkrull ? - 2018 - Modeling relational data with graph convolutional .pdf:application/pdf},
}

@inproceedings{alet_modular_2018,
	title = {Modular meta-learning},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Alet, Ferran and Lozano-P{\'e}rez, Tom{\'a}s and Kaelbling, Leslie P},
	year = {2018},
	pages = {856--868},
	file = {Alet ? - 2018 - Modular meta-learning.pdf:C\:\\Users\\hyin\\Zotero\\storage\\9HWFFAD8\\Alet ? - 2018 - Modular meta-learning.pdf:application/pdf},
}

@inproceedings{bronskill_tasknorm_2020,
	title = {Tasknorm: {Rethinking} batch normalization for meta-learning},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bronskill, John and Gordon, Jonathan and Requeima, James and Nowozin, Sebastian and Turner, Richard},
	year = {2020},
	pages = {1153--1164},
	file = {Bronskill ? - 2020 - Tasknorm Rethinking batch normalization for meta-.pdf:C\:\\Users\\hyin\\Zotero\\storage\\6ZJLMT56\\Bronskill ? - 2020 - Tasknorm Rethinking batch normalization for meta-.pdf:application/pdf},
}

@inproceedings{xiong_faithful_2022,
	title = {Faithful {Embeddings} for \$\$\${\textbackslash}backslash\$mathcal \$\{\${E}\$\}\$\${\textbackslash}backslash\$mathcal \$\{\${L}\$\}\${\textasciicircum}\$\{\$++\$\}\$ \$\$ {Knowledge} {Bases}},
	booktitle = {International {Semantic} {Web} {Conference}},
	publisher = {Springer},
	author = {Xiong, Bo and Potyka, Nico and Tran, Trung-Kien and Nayyeri, Mojtaba and Staab, Steffen},
	year = {2022},
	pages = {22--38},
	file = {Xiong ? - 2022 - Faithful Embeddings for \$\$\$backslash\$mathcal \$ \$E.pdf:C\:\\Users\\hyin\\Zotero\\storage\\NSBRLDAY\\Xiong ? - 2022 - Faithful Embeddings for \$\$\$backslash\$mathcal \$ \$E.pdf:application/pdf},
}

@article{choudhary_probabilistic_2021,
	title = {Probabilistic entity representation model for reasoning over knowledge graphs},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Choudhary, Nurendra and Rao, Nikhil and Katariya, Sumeet and Subbian, Karthik and Reddy, Chandan},
	year = {2021},
	pages = {23440--23451},
	file = {Choudhary ? - 2021 - Probabilistic entity representation model for reas.pdf:C\:\\Users\\hyin\\Zotero\\storage\\FHSFBRDR\\Choudhary ? - 2021 - Probabilistic entity representation model for reas.pdf:application/pdf},
}

@inproceedings{liu_neural-answering_2021,
	title = {Neural-{Answering} {Logical} {Queries} on {Knowledge} {Graphs}},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Liu, Lihui and Du, Boxin and Ji, Heng and Zhai, ChengXiang and Tong, Hanghang},
	year = {2021},
	pages = {1087--1097},
	file = {Liu ? - 2021 - Neural-Answering Logical Queries on Knowledge Grap.pdf:C\:\\Users\\hyin\\Zotero\\storage\\4J9NF8IQ\\Liu ? - 2021 - Neural-Answering Logical Queries on Knowledge Grap.pdf:application/pdf},
}

@article{hamilton_embedding_2018,
	title = {Embedding logical queries on knowledge graphs},
	volume = {31},
	journal = {Advances in neural information processing systems},
	author = {Hamilton, Will and Bajaj, Payal and Zitnik, Marinka and Jurafsky, Dan and Leskovec, Jure},
	year = {2018},
	file = {Hamilton ? - 2018 - Embedding logical queries on knowledge graphs.pdf:C\:\\Users\\hyin\\Zotero\\storage\\EGFXLTFE\\Hamilton ? - 2018 - Embedding logical queries on knowledge graphs.pdf:application/pdf},
}

@misc{lin_flex_2022,
	title = {{FLEX}: {Feature}-{Logic} {Embedding} {Framework} for {CompleX} {Knowledge} {Graph} {Reasoning}},
	shorttitle = {{FLEX}},
	url = {http://arxiv.org/abs/2205.11039},
	doi = {10.48550/arXiv.2205.11039},
	abstract = {Current best performing models for knowledge graph reasoning (KGR) introduce geometry objects or probabilistic distributions to embed entities and first-order logical (FOL) queries into low-dimensional vector spaces. They can be summarized as a center-size framework (point/box/cone, Beta/Gaussian distribution, etc.). However, they have limited logical reasoning ability. And it is difficult to generalize to various features, because the center and size are one-to-one constrained, unable to have multiple centers or sizes. To address these challenges, we instead propose a novel KGR framework named Feature-Logic Embedding framework, FLEX, which is the first KGR framework that can not only TRULY handle all FOL operations including conjunction, disjunction, negation and so on, but also support various feature spaces. Specifically, the logic part of feature-logic framework is based on vector logic, which naturally models all FOL operations. Experiments demonstrate that FLEX significantly outperforms existing state-of-the-art methods on benchmark datasets.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Lin, Xueyuan and E, Haihong and Zhou, Gengxian and Hu, Tianyi and Ningyuan, Li and Sun, Mingzhi and Luo, Haoran},
	month = sep,
	year = {2022},
	note = {arXiv:2205.11039 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\VBW9SARH\\Lin ? - 2022 - FLEX Feature-Logic Embedding Framework for Comple.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\EK8VNYRI\\2205.html:text/html},
}

@inproceedings{zhou_meta-gnn_2019,
	title = {Meta-gnn: {On} few-shot node classification in graph meta-learning},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Zhou, Fan and Cao, Chengtai and Zhang, Kunpeng and Trajcevski, Goce and Zhong, Ting and Geng, Ji},
	year = {2019},
	pages = {2357--2360},
}

@article{raghu_rapid_2019,
	title = {Rapid learning or feature reuse? towards understanding the effectiveness of maml},
	journal = {arXiv preprint arXiv:1909.09157},
	author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
	year = {2019},
	file = {Raghu ? - 2019 - Rapid learning or feature reuse towards understan.pdf:C\:\\Users\\hyin\\Zotero\\storage\\PM3RN98I\\Raghu ? - 2019 - Rapid learning or feature reuse towards understan.pdf:application/pdf},
}

@article{vinyals_matching_2016,
	title = {Matching networks for one shot learning},
	volume = {29},
	journal = {Advances in neural information processing systems},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and {others}},
	year = {2016},
}

@inproceedings{liu_k-bert_2020,
	title = {K-bert: {Enabling} language representation with knowledge graph},
	volume = {34},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Ju, Qi and Deng, Haotang and Wang, Ping},
	year = {2020},
	note = {Issue: 03},
	pages = {2901--2908},
}

@article{luus_logic_2021,
	title = {Logic embeddings for complex query answering},
	journal = {arXiv preprint arXiv:2103.00418},
	author = {Luus, Francois and Sen, Prithviraj and Kapanipathi, Pavan and Riegel, Ryan and Makondo, Ndivhuwo and Lebese, Thabang and Gray, Alexander},
	year = {2021},
	file = {Luus ? - 2021 - Logic embeddings for complex query answering.pdf:C\:\\Users\\hyin\\Zotero\\storage\\78ER3BBT\\Luus ? - 2021 - Logic embeddings for complex query answering.pdf:application/pdf},
}

@inproceedings{toutanova_observed_2015,
	title = {Observed versus latent features for knowledge base and text inference},
	booktitle = {Proceedings of the 3rd workshop on continuous vector space models and their compositionality},
	author = {Toutanova, Kristina and Chen, Danqi},
	year = {2015},
	pages = {57--66},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	year = {2017},
	pages = {1126--1135},
	file = {Finn ? - 2017 - Model-agnostic meta-learning for fast adaptation o.pdf:C\:\\Users\\hyin\\Zotero\\storage\\BIAMAXPM\\Finn ? - 2017 - Model-agnostic meta-learning for fast adaptation o.pdf:application/pdf},
}

@article{bogin_unobserved_2022,
	title = {Unobserved local structures make compositional generalization hard},
	journal = {arXiv preprint arXiv:2201.05899},
	author = {Bogin, Ben and Gupta, Shivanshu and Berant, Jonathan},
	year = {2022},
}

@article{zhu_neural_2021,
	title = {Neural bellman-ford networks: {A} general graph neural network framework for link prediction},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhu, Zhaocheng and Zhang, Zuobai and Xhonneux, Louis-Pascal and Tang, Jian},
	year = {2021},
	pages = {29476--29490},
	file = {Zhu ? - 2021 - Neural bellman-ford networks A general graph neur.pdf:C\:\\Users\\hyin\\Zotero\\storage\\Y7KIWZLJ\\Zhu ? - 2021 - Neural bellman-ford networks A general graph neur.pdf:application/pdf},
}

@inproceedings{wang_relational_2021,
	title = {Relational message passing for knowledge graph completion},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Wang, Hongwei and Ren, Hongyu and Leskovec, Jure},
	year = {2021},
	pages = {1697--1707},
	file = {Relational Message Passing for Knowledge Graph Completion.pdf:D\:\\1\\research\\query\\now reading\\About Knowledge graph\\Relational Message Passing for Knowledge Graph Completion.pdf:application/pdf},
}

@inproceedings{amayuelas_neural_2022,
	title = {Neural {Methods} for {Logical} {Reasoning} over {Knowledge} {Graphs}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Amayuelas, Alfonso and Zhang, Shuai and Rao, Xi Susie and Zhang, Ce},
	year = {2022},
}

@inproceedings{lacroix_canonical_2018,
	title = {Canonical {Tensor} {Decomposition} for {Knowledge} {Base} {Completion}},
	url = {http://arxiv.org/abs/1806.07297},
	doi = {10.48550/arXiv.1806.07297},
	abstract = {The problem of Knowledge Base Completion can be framed as a 3rd-order binary tensor completion problem. In this light, the Canonical Tensor Decomposition (CP) (Hitchcock, 1927) seems like a natural solution; however, current implementations of CP on standard Knowledge Base Completion benchmarks are lagging behind their competitors. In this work, we attempt to understand the limits of CP for knowledge base completion. First, we motivate and test a novel regularizer, based on tensor nuclear \$p\$-norms. Then, we present a reformulation of the problem that makes it invariant to arbitrary choices in the inclusion of predicates or their reciprocals in the dataset. These two methods combined allow us to beat the current state of the art on several datasets with a CP decomposition, and obtain even better results using the more advanced ComplEx model.},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lacroix, Timoth{\'e}e and Usunier, Nicolas and Obozinski, Guillaume},
	month = jun,
	year = {2018},
	note = {arXiv:1806.07297 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	pages = {2863--2872},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\5BZFSYHH\\Lacroix et al. - 2018 - Canonical Tensor Decomposition for Knowledge Base .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\ABFDJU2H\\1806.html:text/html},
}

@article{xu_how_2018,
	title = {How powerful are graph neural networks?},
	journal = {arXiv preprint arXiv:1810.00826},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	year = {2018},
}

@article{sun_faithful_2020,
	title = {Faithful embeddings for knowledge base queries},
	volume = {33},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sun, Haitian and Arnold, Andrew and Bedrax Weiss, Tania and Pereira, Fernando and Cohen, William W},
	year = {2020},
	pages = {22505--22516},
	file = {Sun ? - 2020 - Faithful embeddings for knowledge base queries.pdf:C\:\\Users\\hyin\\Zotero\\storage\\2FSWIGDL\\Sun ? - 2020 - Faithful embeddings for knowledge base queries.pdf:application/pdf},
}

@inproceedings{toutanova_representing_2015,
	title = {Representing text for joint embedding of text and knowledge bases},
	booktitle = {Proceedings of the 2015 conference on empirical methods in natural language processing},
	author = {Toutanova, Kristina and Chen, Danqi and Pantel, Patrick and Poon, Hoifung and Choudhury, Pallavi and Gamon, Michael},
	year = {2015},
	pages = {1499--1509},
}

@inproceedings{ma_noise_2018,
	title = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}: {Consistency} and {Statistical} {Efficiency}},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Ma, Zhuang and Collins, Michael},
	year = {2018},
	pages = {3698--3707},
}

@inproceedings{gilmer_neural_2017,
	title = {Neural message passing for quantum chemistry},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
	year = {2017},
	pages = {1263--1272},
}

@inproceedings{cao_geometry_2022,
	title = {Geometry {Interaction} {Knowledge} {Graph} {Embeddings}},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Cao, Zongsheng and Xu, Qianqian and Yang, Zhiyong and Cao, Xiaochun and Huang, Qingming},
	year = {2022},
}

@inproceedings{wang_heterogeneous_2019,
	title = {Heterogeneous graph attention network},
	booktitle = {The world wide web conference},
	author = {Wang, Xiao and Ji, Houye and Shi, Chuan and Wang, Bai and Ye, Yanfang and Cui, Peng and Yu, Philip S},
	year = {2019},
	pages = {2022--2032},
}

@article{zhu_neural-symbolic_2022,
	title = {Neural-{Symbolic} {Models} for {Logical} {Queries} on {Knowledge} {Graphs}},
	journal = {arXiv preprint arXiv:2205.10128},
	author = {Zhu, Zhaocheng and Galkin, Mikhail and Zhang, Zuobai and Tang, Jian},
	year = {2022},
	file = {Zhu ? - 2022 - Neural-Symbolic Models for Logical Queries on Know.pdf:C\:\\Users\\hyin\\Zotero\\storage\\PCK3G7ZU\\Zhu ? - 2022 - Neural-Symbolic Models for Logical Queries on Know.pdf:application/pdf},
}

@inproceedings{zhang_learning_2020,
	title = {Learning hierarchy-aware knowledge graph embeddings for link prediction},
	volume = {34},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Zhanqiu and Cai, Jianyu and Zhang, Yongdong and Wang, Jie},
	year = {2020},
	note = {Issue: 03},
	pages = {3065--3072},
}

@inproceedings{he_learning_2015,
	title = {Learning to represent knowledge graphs with gaussian embedding},
	booktitle = {Proceedings of the 24th {ACM} international on conference on information and knowledge management},
	author = {He, Shizhu and Liu, Kang and Ji, Guoliang and Zhao, Jun},
	year = {2015},
	pages = {623--632},
	file = {He et al. - 2015 - Learning to represent knowledge graphs with gaussi.pdf:C\:\\Users\\hyin\\Zotero\\storage\\QEZNDUND\\He et al. - 2015 - Learning to represent knowledge graphs with gaussi.pdf:application/pdf},
}

@inproceedings{xie_representation_2016,
	title = {Representation {Learning} of {Knowledge} {Graphs} with {Hierarchical} {Types}.},
	volume = {2016},
	booktitle = {{IJCAI}},
	author = {Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong and {others}},
	year = {2016},
	pages = {2965--2971},
}

@inproceedings{ebisu_toruse_2018,
	title = {Toruse: {Knowledge} graph embedding on a lie group},
	booktitle = {Thirty-second {AAAI} conference on artificial intelligence},
	author = {Ebisu, Takuma and Ichise, Ryutaro},
	year = {2018},
}

@inproceedings{chami_low-dimensional_2020,
	title = {Low-{Dimensional} {Hyperbolic} {Knowledge} {Graph} {Embeddings}},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Chami, Ines and Wolf, Adva and Juan, Da-Cheng and Sala, Frederic and Ravi, Sujith and R{\'e}, Christopher},
	year = {2020},
	pages = {6901--6914},
}

@inproceedings{xie_representation_2016-1,
	title = {Representation learning of knowledge graphs with entity descriptions},
	volume = {30},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Xie, Ruobing and Liu, Zhiyuan and Jia, Jia and Luan, Huanbo and Sun, Maosong},
	year = {2016},
	note = {Issue: 1},
}

@article{balazevic_multi-relational_2019,
	title = {Multi-relational poincar{\'e} graph embeddings},
	volume = {32},
	journal = {Advances in Neural Information Processing Systems},
	author = {Balazevic, Ivana and Allen, Carl and Hospedales, Timothy},
	year = {2019},
}

@article{zhang_quaternion_2019,
	title = {Quaternion knowledge graph embeddings},
	volume = {32},
	journal = {Advances in neural information processing systems},
	author = {Zhang, Shuai and Tay, Yi and Yao, Lina and Liu, Qi},
	year = {2019},
}

@inproceedings{zhang_variational_2018,
	title = {Variational reasoning for question answering with knowledge graph},
	booktitle = {Thirty-second {AAAI} conference on artificial intelligence},
	author = {Zhang, Yuyu and Dai, Hanjun and Kozareva, Zornitsa and Smola, Alexander J and Song, Le},
	year = {2018},
}

@book{marker_model_2006,
	title = {Model theory: an introduction},
	volume = {217},
	publisher = {Springer Science \& Business Media},
	author = {Marker, David},
	year = {2006},
}

@inproceedings{bollacker_freebase_2008,
	title = {Freebase: a collaboratively created graph database for structuring human knowledge},
	booktitle = {{SIGMOD}},
	author = {Bollacker, Kurt D. and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
	year = {2008},
	pages = {1247--1250},
}

@inproceedings{ren_lego_2021,
	title = {Lego: {Latent} execution-guided reasoning for multi-hop question answering on knowledge graphs},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Yasunaga, Michihiro and Sun, Haitian and Schuurmans, Dale and Leskovec, Jure and Zhou, Denny},
	year = {2021},
	pages = {8959--8970},
}

@inproceedings{suchanek_yago_2007,
	title = {Yago: a core of semantic knowledge},
	booktitle = {Proceedings of the 16th international conference on {World} {Wide} {Web}},
	author = {Suchanek, Fabian M and Kasneci, Gjergji and Weikum, Gerhard},
	year = {2007},
	pages = {697--706},
}

@article{zhang_cone_2021,
	title = {Cone: {Cone} embeddings for multi-hop reasoning over knowledge graphs},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhang, Zhanqiu and Wang, Jie and Chen, Jiajun and Ji, Shuiwang and Wu, Feng},
	year = {2021},
	pages = {19172--19183},
	file = {NeurIPS-2021ConE-Supplemental.pdf:C\:\\Users\\hyin\\Zotero\\storage\\758HBCL7\\NeurIPS-2021ConE-Supplemental.pdf:application/pdf;Zhang ? - 2021 - Cone Cone embeddings for multi-hop reasoning over.pdf:C\:\\Users\\hyin\\Zotero\\storage\\PWIS5L9L\\Zhang ? - 2021 - Cone Cone embeddings for multi-hop reasoning over.pdf:application/pdf},
}

@article{bai_query2particles_2022,
	title = {{Query2Particles}: {Knowledge} {Graph} {Reasoning} with {Particle} {Embeddings}},
	journal = {arXiv preprint arXiv:2204.12847},
	author = {Bai, Jiaxin and Wang, Zihao and Zhang, Hongming and Song, Yangqiu},
	year = {2022},
	file = {Bai ? - 2022 - Query2Particles Knowledge Graph Reasoning with Pa.pdf:C\:\\Users\\hyin\\Zotero\\storage\\XPDUNGWQ\\Bai ? - 2022 - Query2Particles Knowledge Graph Reasoning with Pa.pdf:application/pdf},
}

@inproceedings{chen_fuzzy_2022,
	title = {Fuzzy logic based logical query answering on knowledge graphs},
	volume = {36},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Chen, Xuelu and Hu, Ziniu and Sun, Yizhou},
	year = {2022},
	note = {Issue: 4},
	pages = {3939--3948},
	file = {Chen ? - 2022 - Fuzzy logic based logical query answering on knowl.pdf:C\:\\Users\\hyin\\Zotero\\storage\\V7R2VWNI\\Chen ? - 2022 - Fuzzy logic based logical query answering on knowl.pdf:application/pdf},
}

@inproceedings{sun_rotate_2018,
	title = {{RotatE}: {Knowledge} {Graph} {Embedding} by {Relational} {Rotation} in {Complex} {Space}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sun, Zhiqing and Deng, Zhi-Hong and Nie, Jian-Yun and Tang, Jian},
	year = {2018},
}

@article{arakelyan_complex_2021,
	title = {Complex query answering with neural link predictors},
	journal = {arXiv preprint arXiv:2011.03459},
	author = {Arakelyan, Erik and Daza, Daniel and Minervini, Pasquale and Cochez, Michael},
	year = {2021},
	file = {Arakelyan ? - 2021 - Complex query answering with neural link predictor.pdf:C\:\\Users\\hyin\\Zotero\\storage\\FSE2JNG4\\Arakelyan ? - 2021 - Complex query answering with neural link predictor.pdf:application/pdf},
}

@article{ren_beta_2020,
	title = {Beta embeddings for multi-hop logical reasoning in knowledge graphs},
	volume = {33},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ren, Hongyu and Leskovec, Jure},
	year = {2020},
	pages = {19716--19726},
	file = {Ren ? Leskovec - 2020 - Beta embeddings for multi-hop logical reasoning in.pdf:C\:\\Users\\hyin\\Zotero\\storage\\6AYPS3UC\\Ren ? Leskovec - 2020 - Beta embeddings for multi-hop logical reasoning in.pdf:application/pdf},
}

@inproceedings{ren_query2box_2020,
	title = {Query2box: {Reasoning} {Over} {Knowledge} {Graphs} {In} {Vector} {Space} {Using} {Box} {Embeddings}},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Ren, H and Hu, W and Leskovec, J},
	year = {2020},
	file = {Ren ? - 2020 - Query2box Reasoning Over Knowledge Graphs In Vect.pdf:C\:\\Users\\hyin\\Zotero\\storage\\A6LCXNGD\\Ren ? - 2020 - Query2box Reasoning Over Knowledge Graphs In Vect.pdf:application/pdf},
}

@misc{arakelyan_adapting_2023,
	title = {Adapting {Neural} {Link} {Predictors} for {Complex} {Query} {Answering}},
	url = {http://arxiv.org/abs/2301.12313},
	doi = {10.48550/arXiv.2301.12313},
	abstract = {Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022) showed that neural link predictors could also be used for answering complex queries: their Continuous Query Decomposition (CQD) method works by decomposing complex queries into atomic sub-queries, answers them using neural link predictors and aggregates their scores via t-norms for ranking the answers to each complex query. However, CQD does not handle negations and only uses the training signal from atomic training queries: neural link prediction scores are not calibrated to interact together via fuzzy logic t-norms during complex query answering. In this work, we propose to address this problem by training a parameter-efficient score adaptation model to re-calibrate neural link prediction scores: this new component is trained on complex queries by back-propagating through the complex query-answering process. Our method, CQD\${\textasciicircum}\{A\}\$, produces significantly more accurate results than current state-of-the-art methods, improving from \$34.4\$ to \$35.1\$ Mean Reciprocal Rank values averaged across all datasets and query types while using \${\textbackslash}leq 35{\textbackslash}\%\$ of the available training query types. We further show that CQD\${\textasciicircum}\{A\}\$ is data-efficient, achieving competitive results with only \$1{\textbackslash}\%\$ of the training data, and robust in out-of-domain evaluations.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Arakelyan, Erik and Minervini, Pasquale and Augenstein, Isabelle},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12313 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Logic in Computer Science, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\MNUZQ8XB\\Arakelyan et al. - 2023 - Adapting Neural Link Predictors for Complex Query .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\KZ5F8AAU\\2301.html:text/html},
}



@misc{jung_maieutic_2022,
	title = {Maieutic {Prompting}: {Logically} {Consistent} {Reasoning} with {Recursive} {Explanations}},
	shorttitle = {Maieutic {Prompting}},
	url = {http://arxiv.org/abs/2205.11822},
	doi = {10.48550/arXiv.2205.11822},
	abstract = {Despite their impressive capabilities, large pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which infers a correct answer to a question even from the noisy and inconsistent generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20\% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Bras, Ronan Le and Choi, Yejin},
	month = oct,
	year = {2022},
	note = {arXiv:2205.11822 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\NPCN5EYB\\Jung et al. - 2022 - Maieutic Prompting Logically Consistent Reasoning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\PNSDH9ZK\\2205.html:text/html},
}

@misc{huang_theory_2023,
	title = {A {Theory} of {Link} {Prediction} via {Relational} {Weisfeiler}-{Leman}},
	url = {http://arxiv.org/abs/2302.02209},
	doi = {10.48550/arXiv.2302.02209},
	abstract = {Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains highly incomplete in the context of knowledge graphs. The goal of this work is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models, and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm with different initialization regimes. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. Our theoretical findings explain the benefits of some widely employed practical design choices, which are validated empirically.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Huang, Xingyue and Orth, Miguel Romero and Ceylan, {\.I}smail {\.I}lkan and Barcel{\'o}, Pablo},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02209 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\8SB5652Y\\Huang et al. - 2023 - A Theory of Link Prediction via Relational Weisfei.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\RWIA6YET\\2302.html:text/html},
}

@misc{yao_kg-bert_2019,
	title = {{KG}-{BERT}: {BERT} for {Knowledge} {Graph} {Completion}},
	shorttitle = {{KG}-{BERT}},
	url = {http://arxiv.org/abs/1909.03193},
	abstract = {Knowledge graphs are important resources for many artificial intelligence tasks but often suffer from incompleteness. In this work, we propose to use pre-trained language models for knowledge graph completion. We treat triples in knowledge graphs as textual sequences and propose a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Our method takes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model. Experimental results on multiple benchmark knowledge graphs show that our method can achieve state-of-the-art performance in triple classification, link prediction and relation prediction tasks.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Yao, Liang and Mao, Chengsheng and Luo, Yuan},
	month = sep,
	year = {2019},
	note = {arXiv:1909.03193 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\MKPP6E8G\\Yao et al. - 2019 - KG-BERT BERT for Knowledge Graph Completion.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\MQ6SG9HH\\1909.html:text/html},
}

@misc{xu_pre-training_2023,
	title = {A {Pre}-training {Framework} for {Knowledge} {Graph} {Completion}},
	url = {http://arxiv.org/abs/2302.02614},
	abstract = {Knowledge graph completion (KGC) is one of the effective methods to identify new facts in knowledge graph. Except for a few methods based on graph network, most of KGC methods trend to be trained based on independent triples, while are difficult to take a full account of the information of global network connection contained in knowledge network. To address these issues, in this study, we propose a simple and effective Network-based Pre-training framework for knowledge graph completion (termed NetPeace), which takes into account the information of global network connection and local triple relationships in knowledge graph. Experiments show that in NetPeace framework, multiple KGC models yields consistent and significant improvements on benchmarks (e.g., 36.45\% Hits@1 and 27.40\% MRR improvements for TuckER on FB15k-237), especially dense knowledge graph. On the challenging low-resource task, NetPeace that benefits from the global features of KG achieves higher performance (104.03\% MRR and 143.89\% Hit@1 improvements at most) than original models.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Xu, Kuan and Yang, Kuo and Dong, Hanyang and Wang, Xinyan and Zhou, Xuezhong},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02614 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\V52N8ASE\\Xu et al. - 2023 - A Pre-training Framework for Knowledge Graph Compl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\Z3UXPN2C\\2302.html:text/html},
}

@misc{chen_representing_2022,
	title = {On {Representing} {Linear} {Programs} by {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2209.12288},
	abstract = {Learning to optimize is a rapidly growing area that aims to solve optimization problems or improve existing optimization algorithms using machine learning (ML). In particular, the graph neural network (GNN) is considered a suitable ML model for optimization problems whose variables and constraints are permutation--invariant, for example, the linear program (LP). While the literature has reported encouraging numerical results, this paper establishes the theoretical foundation of applying GNNs to solving LPs. Given any size limit of LPs, we construct a GNN that maps different LPs to different outputs. We show that properly built GNNs can reliably predict feasibility, boundedness, and an optimal solution for each LP in a broad class. Our proofs are based upon the recently--discovered connections between the Weisfeiler--Lehman isomorphism test and the GNN. To validate our results, we train a simple GNN and present its accuracy in mapping LPs to their feasibilities and solutions.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Chen, Ziang and Liu, Jialin and Wang, Xinshang and Lu, Jianfeng and Yin, Wotao},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12288 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\XXX3K96G\\Chen et al. - 2022 - On Representing Linear Programs by Graph Neural Ne.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\JV5778XV\\2209.html:text/html},
}

@misc{zhang_rethinking_2023,
	title = {Rethinking the {Expressive} {Power} of {GNNs} via {Graph} {Biconnectivity}},
	url = {http://arxiv.org/abs/2301.09505},
	abstract = {Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Zhang, Bohang and Luo, Shengjie and Wang, Liwei and He, Di},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09505 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\XMZFTLQ7\\Zhang et al. - 2023 - Rethinking the Expressive Power of GNNs via Graph .pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\WXRBXN8S\\Zhang et al. - 2023 - Rethinking the Expressive Power of GNNs via Graph .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\2KLLNUHG\\2301.html:text/html},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\YA5EWHJ5\\Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\EELB47HM\\2206.html:text/html},
}

@misc{hospedales_meta-learning_2020,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.05439},
	doi = {10.48550/arXiv.2004.05439},
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	month = nov,
	year = {2020},
	note = {arXiv:2004.05439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\6WBQUI5L\\Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\JBKRV5T5\\2004.html:text/html},
}

@misc{deleu_torchmeta_2019,
	title = {Torchmeta: {A} {Meta}-{Learning} library for {PyTorch}},
	shorttitle = {Torchmeta},
	url = {http://arxiv.org/abs/1909.06576},
	doi = {10.48550/arXiv.1909.06576},
	abstract = {The constant introduction of standardized benchmarks in the literature has helped accelerating the recent advances in meta-learning research. They offer a way to get a fair comparison between different algorithms, and the wide range of datasets available allows full control over the complexity of this evaluation. However, for a large majority of code available online, the data pipeline is often specific to one dataset, and testing on another dataset requires significant rework. We introduce Torchmeta, a library built on top of PyTorch that enables seamless and consistent evaluation of meta-learning algorithms on multiple datasets, by providing data-loaders for most of the standard benchmarks in few-shot classification and regression, with a new meta-dataset abstraction. It also features some extensions for PyTorch to simplify the development of models compatible with meta-learning algorithms. The code is available here: https://github.com/tristandeleu/pytorch-meta},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Deleu, Tristan and W{\"u}rfl, Tobias and Samiei, Mandana and Cohen, Joseph Paul and Bengio, Yoshua},
	month = sep,
	year = {2019},
	note = {arXiv:1909.06576 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\N9FQTUG6\\Deleu et al. - 2019 - Torchmeta A Meta-Learning library for PyTorch.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\JDBYPLBI\\1909.html:text/html},
}

@misc{chamberlain_graph_2022,
	title = {Graph {Neural} {Networks} for {Link} {Prediction} with {Subgraph} {Sketching}},
	url = {http://arxiv.org/abs/2209.15486},
	doi = {10.48550/arXiv.2209.15486},
	abstract = {Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Chamberlain, Benjamin Paul and Shirobokov, Sergey and Rossi, Emanuele and Frasca, Fabrizio and Markovich, Thomas and Hammerla, Nils and Bronstein, Michael M. and Hansmire, Max},
	month = oct,
	year = {2022},
	note = {arXiv:2209.15486 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\QT9W8R5J\\Chamberlain et al. - 2022 - Graph Neural Networks for Link Prediction with Sub.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\BY679THN\\2209.html:text/html},
}

@misc{wang_survey_2017,
	title = {A {Survey} on {Learning} to {Hash}},
	url = {http://arxiv.org/abs/1606.00185},
	doi = {10.48550/arXiv.1606.00185},
	abstract = {Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization, and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation protocols, and the general performance analysis, and point out that the quantization algorithms perform superiorly in terms of search accuracy, search time cost, and space cost. Finally, we introduce a few emerging topics.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Wang, Jingdong and Zhang, Ting and Song, Jingkuan and Sebe, Nicu and Shen, Heng Tao},
	month = apr,
	year = {2017},
	note = {arXiv:1606.00185 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\Y2G7EELV\\Wang et al. - 2017 - A Survey on Learning to Hash.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\P4YWC2VA\\1606.html:text/html},
}

@misc{lin_tflex_2022,
	title = {{TFLEX}: {Temporal} {Feature}-{Logic} {Embedding} {Framework} for {Complex} {Reasoning} over {Temporal} {Knowledge} {Graph}},
	shorttitle = {{TFLEX}},
	url = {http://arxiv.org/abs/2205.14307},
	doi = {10.48550/arXiv.2205.14307},
	abstract = {Multi-hop logical reasoning over knowledge graph (KG) plays a fundamental role in many artificial intelligence tasks. Recent complex query embedding (CQE) methods for reasoning focus on static KGs, while temporal knowledge graphs (TKGs) have not been fully explored. Reasoning over TKGs has two challenges: 1. The query should answer entities or timestamps; 2. The operators should consider both set logic on entity set and temporal logic on timestamp set. To bridge this gap, we define the multi-hop logical reasoning problem on TKGs. With generated three datasets, we propose the first temporal CQE named Temporal Feature-Logic Embedding framework (TFLEX) to answer the temporal complex queries. We utilize vector logic to compute the logic part of Temporal Feature-Logic embeddings, thus naturally modeling all First-Order Logic (FOL) operations on entity set. In addition, our framework extends vector logic on timestamp set to cope with three extra temporal operators (After, Before and Between). Experiments on numerous query patterns demonstrate the effectiveness of our method.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Lin, Xueyuan and Xu, Chengjin and E, Haihong and Su, Fenglong and Zhou, Gengxian and Hu, Tianyi and Li, Ningyuan and Sun, Mingzhi and Luo, Haoran},
	month = may,
	year = {2022},
	note = {arXiv:2205.14307 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\38ZD52Z4\\Lin et al. - 2022 - TFLEX Temporal Feature-Logic Embedding Framework .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\EZHLDZAG\\2205.html:text/html},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-03-02},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\KB8SMGGW\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-03-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L }ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\IJBQG593\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{vardi_constraint_2000,
	address = {New York, NY, USA},
	series = {{PODS} '00},
	title = {Constraint satisfaction and database theory: a tutorial},
	isbn = {978-1-58113-214-4},
	shorttitle = {Constraint satisfaction and database theory},
	url = {https://dl.acm.org/doi/10.1145/335168.335209},
	doi = {10.1145/335168.335209},
	abstract = {A large class of problems in AI and other areas of computer science can be viewed as constraint-satisfaction problems. This includes problems in machine vision, belief maintenance, scheduling, temporal reasoning, type reconstruction, graph theory, and satisfiability. In general, the constraint satisfaction-problem is NP-complete, so searching for tractable cases is an active research area. It turns out that constraint satisfaction has an intimate connection with database theory: constraint-satisfaction problems can be recast as database problems and database problems can be recast as constraint-satisfaction problems. In this tutorial, I will cover the fundamentals of constraints satisfaction and describe its intimate relationship with database theory from various perspectives.},
	urldate = {2023-03-04},
	booktitle = {Proceedings of the nineteenth {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on {Principles} of database systems},
	publisher = {Association for Computing Machinery},
	author = {Vardi, Moshe Y.},
	year = {2000},
	pages = {76--85},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\4FDNQNLA\\Vardi - 2000 - Constraint satisfaction and database theory a tut.pdf:application/pdf},
}

@article{mackworth_complexity_1985,
	title = {The complexity of some polynomial network consistency algorithms for constraint satisfaction problems},
	volume = {25},
	number = {1},
	journal = {Artificial intelligence},
	author = {Mackworth, Alan K. and Freuder, Eugene C.},
	year = {1985},
	note = {Publisher: Elsevier},
	pages = {65--74},
	file = {AC-MackworthFreuder.pdf:C\:\\Users\\hyin\\Downloads\\TR-82-06.pdf:application/pdf},
}

@article{wang_benchmarking_2021,
	title = {Benchmarking the {Combinatorial} {Generalizability} of {Complex} {Query} {Answering} on {Knowledge} {Graphs}},
	volume = {1},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/7eabe3a1649ffa2b3ff8c02ebfd5659f-Abstract-round2.html},
	language = {en},
	urldate = {2023-03-05},
	journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	author = {Wang, Zihao and Yin, Hang and Song, Yangqiu},
	month = dec,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\RM5T54P9\\Wang et al. - 2021 - Benchmarking the Combinatorial Generalizability of.pdf:application/pdf},
}

@inproceedings{cheng_neural_2023,
	title = {Neural {Compositional} {Rule} {Learning} for {Knowledge} {Graph} {Reasoning}},
	url = {https://www.semanticscholar.org/paper/Neural-Compositional-Rule-Learning-for-Knowledge-Cheng-Ahmed/58820d7d68e12d2a531e771a5fcb7e461c25f5c6?utm_source=alert_email&utm_content=LibraryFolder&utm_campaign=AlertEmails_DAILY&utm_term=LibraryFolder&email_index=9-1-17&utm_medium=12988156},
	abstract = {Learning logical rules is critical to improving reasoning in KGs. This is due to their ability to provide logical and interpretable explanations when used for predictions, as well as their ability to generalize to other tasks, domains, and data. While recent methods have been proposed to learn logical rules, the majority of these methods are either restricted by their computational complexity and can not handle the large search space of large-scale KGs, or show poor generalization when exposed to data outside the training set. In this paper, we propose an end-to-end neural model for learning compositional logical rules called NCRL. NCRL detects the best compositional structure of a rule body, and breaks it into small compositions in order to infer the rule head. By recurrently merging compositions in the rule body with a recurrent attention unit, NCRL finally predicts a single rule head. Experimental results show that NCRL learns high-quality rules, as well as being generalizable. Specifically, we show that NCRL is scalable, efficient, and yields state-of-the-art results for knowledge graph completion on large-scale KGs. Moreover, we test NCRL for systematic generalization by learning to reason on small-scale observed graphs and evaluating on larger unseen ones.},
	urldate = {2023-03-10},
	author = {Cheng, Kewei and Ahmed, Nesreen and Sun, Yizhou},
	month = mar,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\59S8B5IB\\Cheng et al. - 2023 - Neural Compositional Rule Learning for Knowledge G.pdf:application/pdf},
}

@article{couceiro_survey_2020,
	title = {A {Survey} on the {Fine}-grained {Complexity} of {Constraint} {Satisfaction} {Problems} {Based} on {Partial} {Polymorphisms}},
	author = {Couceiro, Miguel and Haddad, Lucien and Lagerkvist, Victor},
	month = jan,
	year = {2020},
	file = {survey_paper_mvl_revised.pdf:C\:\\Users\\hyin\\Downloads\\survey_paper_mvl_revised.pdf:application/pdf},
}

@article{gottlob_comparison_2000,
	title = {A comparison of structural {CSP} decomposition methods},
	volume = {124},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370200000783},
	doi = {10.1016/S0004-3702(00)00078-3},
	abstract = {We compare tractable classes of constraint satisfaction problems (CSPs). We first give a uniform presentation of the major structural CSP decomposition methods. We then introduce a new class of tractable CSPs based on the concept of hypertree decomposition recently developed in Database Theory, and analyze the cost of solving CSPs having bounded hypertree-width. We provide a framework for comparing parametric decomposition-based methods according to tractability criteria and compare the most relevant methods. We show that the method of hypertree decomposition dominates the others in the case of general CSPs (i.e., CSPs of unbounded arity). We also make comparisons for the restricted case of binary CSPs. Finally, we consider the application of decomposition methods to the dual graph of a hypergraph. In fact, this technique is often used to exploit binary decomposition methods for nonbinary CSPs. However, even in this case, the hypertree-decomposition method turns out to be the most general method.},
	language = {en},
	number = {2},
	urldate = {2023-03-16},
	journal = {Artificial Intelligence},
	author = {Gottlob, Georg and Leone, Nicola and Scarcello, Francesco},
	month = dec,
	year = {2000},
	keywords = {Biconnected components, Constraint satisfaction, Cycle cutsets, Decomposition methods, Degree of cyclicity, Hypergraphs, Hypertree width, Tractable cases, Tree-clustering, Treewidth},
	pages = {243--282},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\JMBAHUME\\Gottlob et al. - 2000 - A comparison of structural CSP decomposition metho.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\XIUMI2IF\\S0004370200000783.html:text/html},
}

@misc{an_does_2023,
	title = {Does {Deep} {Learning} {Learn} to {Abstract}? {A} {Systematic} {Probing} {Framework}},
	shorttitle = {Does {Deep} {Learning} {Learn} to {Abstract}?},
	url = {http://arxiv.org/abs/2302.11978},
	doi = {10.48550/arXiv.2302.11978},
	abstract = {Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a "memorize-then-abstract" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales.},
	urldate = {2023-03-23},
	publisher = {arXiv},
	author = {An, Shengnan and Lin, Zeqi and Chen, Bei and Fu, Qiang and Zheng, Nanning and Lou, Jian-Guang},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11978 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\LNXM5NV2\\An et al. - 2023 - Does Deep Learning Learn to Abstract A Systematic.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\XLJ3JFC7\\2302.html:text/html},
}

@book{rossi_handbook_2006,
	address = {USA},
	title = {Handbook of {Constraint} {Programming}},
	isbn = {978-0-08-046380-3},
	abstract = {Constraint programming is a powerful paradigm for solving combinatorial search problems that draws on a wide range of techniques from artificial intelligence, computer science, databases, programming languages, and operations research. Constraint programming is currently applied with success to many domains, such as scheduling, planning, vehicle routing, configuration, networks, and bioinformatics. The aim of this handbook is to capture the full breadth and depth of the constraint programming field and to be encyclopedic in its scope and coverage. While there are several excellent books on constraint programming, such books necessarily focus on the main notions and techniques and cannot cover also extensions, applications, and languages. The handbook gives a reasonably complete coverage of all these lines of work, based on constraint programming, so that a reader can have a rather precise idea of the whole field and its potential. Of course each line of work is dealt with in a survey-like style, where some details may be neglected in favor of coverage. However, the extensive bibliography of each chapter will help the interested readers to find suitable sources for the missing details. Each chapter of the handbook is intended to be a self-contained survey of a topic, and is written by one or more authors who are leading researchers in the area. The intended audience of the handbook is researchers, graduate students, higher-year undergraduates and practitioners who wish to learn about the state-of-the-art in constraint programming. No prior knowledge about the field is necessary to be able to read the chapters and gather useful knowledge. Researchers from other fields should find in this handbook an effective way to learn about constraint programming and to possibly use some of the constraint programming concepts and techniques in their work, thus providing a means for a fruitful cross-fertilization among different research areas. The handbook is organized in two parts. The first part covers the basic foundations of constraint programming, including the history, the notion of constraint propagation, basic search methods, global constraints, tractability and computational complexity, and important issues in modeling a problem as a constraint problem. The second part covers constraint languages and solver, several useful extensions to the basic framework (such as interval constraints, structured domains, and distributed CSPs), and successful application areas for constraint programming. - Covers the whole field of constraint programming - Survey-style chapters - Five chapters on applications Table of Contents Foreword (Ugo Montanari) Part I : Foundations Chapter 1. Introduction (Francesca Rossi, Peter van Beek, Toby Walsh) Chapter 2. Constraint Satisfaction: An Emerging Paradigm (Eugene C. Freuder, Alan K. Mackworth) Chapter 3. Constraint Propagation (Christian Bessiere) Chapter 4. Backtracking Search Algorithms (Peter van Beek) Chapter 5. Local Search Methods (Holger H. Hoos, Edward Tsang) Chapter 6. Global Constraints (Willem-Jan van Hoeve, Irit Katriel) Chapter 7. Tractable Structures for CSPs (Rina Dechter) Chapter 8. The Complexity of Constraint Languages (David Cohen, Peter Jeavons) Chapter 9. Soft Constraints (Pedro Meseguer, Francesca Rossi, Thomas Schiex) Chapter 10. Symmetry in Constraint Programming (Ian P. Gent, Karen E. Petrie, Jean-Francois Puget) Chapter 11. Modelling (Barbara M. Smith) Part II : Extensions, Languages, and Applications Chapter 12. Constraint Logic Programming (Kim Marriott, Peter J. Stuckey, Mark Wallace) Chapter 13. Constraints in Procedural and Concurrent Languages (Thom Fruehwirth, Laurent Michel, Christian Schulte) Chapter 14. Finite Domain Constraint Programming Systems (Christian Schulte, Mats Carlsson) Chapter 15. Operations Research Methods in Constraint Programming (John Hooker) Chapter 16. Continuous and Interval Constraints(Frederic Benhamou, Laurent Granvilliers) Chapter 17. Constraints over Structured Domains (Carmen Gervet) Chapter 18. Randomness and Structure (Carla Gomes, Toby Walsh) Chapter 19. Temporal CSPs (Manolis Koubarakis) Chapter 20. Distributed Constraint Programming (Boi Faltings) Chapter 21. Uncertainty and Change (Kenneth N. Brown, Ian Miguel) Chapter 22. Constraint-Based Scheduling and Planning (Philippe Baptiste, Philippe Laborie, Claude Le Pape, Wim Nuijten) Chapter 23. Vehicle Routing (Philip Kilby, Paul Shaw) Chapter 24. Configuration (Ulrich Junker) Chapter 25. Constraint Applications in Networks (Helmut Simonis) Chapter 26. Bioinformatics and Constraints (Rolf Backofen, David Gilbert)},
	publisher = {Elsevier Science Inc.},
	author = {Rossi, Francesca and van Beek, Peter and Walsh, Toby},
	year = {2006},
	file = {Handbook of Constraint Programming.pdf:C\:\\Users\\hyin\\Zotero\\storage\\JPHP826I\\Handbook of Constraint Programming.pdf:application/pdf},
}

@misc{blondel_fast_2020,
	title = {Fast {Differentiable} {Sorting} and {Ranking}},
	url = {http://arxiv.org/abs/2002.08871},
	doi = {10.48550/arXiv.2002.08871},
	abstract = {The sorting operation is one of the most commonly used building blocks in computer programming. In machine learning, it is often used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks where it is non-differentiable. More problematic is the related ranking operator, often used for order statistics and ranking metrics. It is a piecewise constant function, meaning that its derivatives are null or undefined. While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the \$O(n {\textbackslash}log n)\$ time complexity one would expect from sorting and ranking operations. In this paper, we propose the first differentiable sorting and ranking operators with \$O(n {\textbackslash}log n)\$ time and \$O(n)\$ space complexity. Our proposal in addition enjoys exact computation and differentiation. We achieve this feat by constructing differentiable operators as projections onto the permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization. Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: differentiable Spearman's rank correlation coefficient and least trimmed squares.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Blondel, Mathieu and Teboul, Olivier and Berthet, Quentin and Djolonga, Josip},
	month = jun,
	year = {2020},
	note = {arXiv:2002.08871 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\8GRIDSBE\\Blondel et al. - 2020 - Fast Differentiable Sorting and Ranking.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\76BY8CP9\\2002.html:text/html},
}

@misc{xu_neural-symbolic_2022,
	title = {Neural-{Symbolic} {Entangled} {Framework} for {Complex} {Query} {Answering}},
	url = {http://arxiv.org/abs/2209.08779},
	doi = {10.48550/arXiv.2209.08779},
	abstract = {Answering complex queries over knowledge graphs (KG) is an important yet challenging task because of the KG incompleteness issue and cascading errors during reasoning. Recent query embedding (QE) approaches to embed the entities and relations in a KG and the first-order logic (FOL) queries into a low dimensional space, answering queries by dense similarity search. However, previous works mainly concentrate on the target answers, ignoring intermediate entities' usefulness, which is essential for relieving the cascading error problem in logical query answering. In addition, these methods are usually designed with their own geometric or distributional embeddings to handle logical operators like union, intersection, and negation, with the sacrifice of the accuracy of the basic operator - projection, and they could not absorb other embedding methods to their models. In this work, we propose a Neural and Symbolic Entangled framework (ENeSy) for complex query answering, which enables the neural and symbolic reasoning to enhance each other to alleviate the cascading error and KG incompleteness. The projection operator in ENeSy could be any embedding method with the capability of link prediction, and the other FOL operators are handled without parameters. With both neural and symbolic reasoning results contained, ENeSy answers queries in ensembles. ENeSy achieves the SOTA performance on several benchmarks, especially in the setting of the training model only with the link prediction task.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Xu, Zezhong and Zhang, Wen and Ye, Peng and Chen, Hui and Chen, Huajun},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08779 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Databases},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\LLB97TLT\\Xu et al. - 2022 - Neural-Symbolic Entangled Framework for Complex Qu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\UGF4R8AF\\2209.html:text/html},
}

@misc{otto_graded_2019,
	title = {Graded modal logic and counting bisimulation},
	url = {http://arxiv.org/abs/1910.00039},
	abstract = {This note sketches the extension of the basic characterisation theorems as the bisimulation-invariant fragment of first-order logic to modal logic with graded modalities and matching adaptation of bisimulation. We focus on showing expressive completeness of graded multi-modal logic for those first-order properties of pointed Kripke structures that are preserved under counting bisimulation equivalence among all or among just all finite pointed Kripke structures.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Otto, Martin},
	month = sep,
	year = {2019},
	note = {arXiv:1910.00039 [cs, math]},
	keywords = {Computer Science - Logic in Computer Science, 03B45, 03C07, 03C98, 03B70, Mathematics - Logic},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\WDDHKNDQ\\Otto - 2019 - Graded modal logic and counting bisimulation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\B8J2ZT86\\1910.html:text/html},
}

@misc{qiu_logical_2023,
	title = {Logical {Expressiveness} of {Graph} {Neural} {Network} for {Knowledge} {Graph} {Reasoning}},
	url = {http://arxiv.org/abs/2303.12306},
	doi = {10.48550/arXiv.2303.12306},
	abstract = {Graph Neural Networks (GNNs) have been recently introduced to learn from knowledge graph (KG) and achieved state-of-the-art performance in KG reasoning. However, a theoretical certification for their good empirical performance is still absent. Besides, while logic in KG is important for inductive and interpretable inference, existing GNN-based methods are just designed to fit data distributions with limited knowledge of their logical expressiveness. We propose to fill the above gap in this paper. Specifically, we theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG. Our results first show that GNN can capture logical rules from graded modal logic, providing a new theoretical tool for analyzing the expressiveness of GNN for KG reasoning; and a query labeling trick makes it easier for GNN to capture logical rules, explaining why SOTA methods are mainly based on labeling trick. Finally, insights from our theory motivate the development of an entity labeling method for capturing difficult logical rules. Experimental results are consistent with our theoretical results and verify the effectiveness of our proposed method.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Qiu, Haiquan and Zhang, Yongqi and Li, Yong and Yao, Quanming},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12306 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\I6L39S7I\\Qiu et al. - 2023 - Logical Expressiveness of Graph Neural Network for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\TEVY9TJL\\2303.html:text/html},
}

@article{poursafaei_towards_2022,
	title = {Towards {Better} {Evaluation} for {Dynamic} {Link} {Prediction}},
	volume = {35},
	url = {https://papers.nips.cc/paper_files/paper/2022/hash/d49042a5d49818711c401d34172f9900-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2023-03-29},
	journal = {Advances in Neural Information Processing Systems},
	author = {Poursafaei, Farimah and Huang, Shenyang and Pelrine, Kellin and Rabbany, Reihaneh},
	month = dec,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\LHGVGYEK\\Poursafaei et al. - 2022 - Towards Better Evaluation for Dynamic Link Predict.pdf:application/pdf;Poursafaei et al. - Towards Better Evaluation for Dynamic Link Predict.pdf:C\:\\Users\\hyin\\Zotero\\storage\\JJRNUG9E\\Poursafaei et al. - Towards Better Evaluation for Dynamic Link Predict.pdf:application/pdf},
}

@misc{wu_recogs_2023,
	title = {{ReCOGS}: {How} {Incidental} {Details} of a {Logical} {Form} {Overshadow} an {Evaluation} of {Semantic} {Interpretation}},
	shorttitle = {{ReCOGS}},
	url = {http://arxiv.org/abs/2303.13716},
	doi = {10.48550/arXiv.2303.13716},
	abstract = {Compositional generalization benchmarks seek to assess whether models can accurately compute meanings for novel sentences, but operationalize this in terms of logical form (LF) prediction. This raises the concern that semantically irrelevant details of the chosen LFs could shape model performance. We argue that this concern is realized for the COGS benchmark (Kim and Linzen, 2020). COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs. Converting these LFs to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, we find that even baseline models get traction. A recent variable-free translation of COGS LFs suggests similar conclusions, but we observe this format is not semantically equivalent; it is incapable of accurately representing some COGS meanings. These findings inform our proposal for ReCOGS, a modified version of COGS that comes closer to assessing the target semantic capabilities while remaining very challenging. Overall, our results reaffirm the importance of compositional generalization and careful benchmark task design.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Wu, Zhengxuan and Manning, Christopher D. and Potts, Christopher},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13716 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\EF44BZDF\\Wu et al. - 2023 - ReCOGS How Incidental Details of a Logical Form O.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\MU3ZRPJU\\2303.html:text/html},
}

@misc{ren_neural_2023,
	title = {Neural {Graph} {Reasoning}: {Complex} {Logical} {Query} {Answering} {Meets} {Graph} {Databases}},
	shorttitle = {Neural {Graph} {Reasoning}},
	url = {http://arxiv.org/abs/2303.14617},
	abstract = {Complex logical query answering (CLQA) is a recently emerged task of graph machine learning that goes beyond simple one-hop link prediction and solves a far more complex task of multi-hop logical reasoning over massive, potentially incomplete graphs in a latent space. The task received a significant traction in the community; numerous works expanded the field along theoretical and practical axes to tackle different types of complex queries and graph modalities with efficient systems. In this paper, we provide a holistic survey of CLQA with a detailed taxonomy studying the field from multiple angles, including graph types (modality, reasoning domain, background semantics), modeling aspects (encoder, processor, decoder), supported queries (operators, patterns, projected variables), datasets, evaluation metrics, and applications. Refining the CLQA task, we introduce the concept of Neural Graph Databases (NGDBs). Extending the idea of graph databases (graph DBs), NGDB consists of a Neural Graph Storage and a Neural Graph Engine. Inside Neural Graph Storage, we design a graph store, a feature store, and further embed information in a latent embedding store using an encoder. Given a query, Neural Query Engine learns how to perform query planning and execution in order to efficiently retrieve the correct results by interacting with the Neural Graph Storage. Compared with traditional graph DBs, NGDBs allow for a flexible and unified modeling of features in diverse modalities using the embedding store. Moreover, when the graph is incomplete, they can provide robust retrieval of answers which a normal graph DB cannot recover. Finally, we point out promising directions, unsolved problems and applications of NGDB for future research.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Ren, Hongyu and Galkin, Mikhail and Cochez, Michael and Zhu, Zhaocheng and Leskovec, Jure},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14617 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Databases},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\MHG82HT2\\Ren et al. - 2023 - Neural Graph Reasoning Complex Logical Query Answ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\QMDHWK76\\2303.html:text/html},
}

@misc{chen_can_2020,
	title = {Can {Graph} {Neural} {Networks} {Count} {Substructures}?},
	url = {http://arxiv.org/abs/2002.04025},
	abstract = {The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting and inspired by Murphy et al. (2019), we propose the Local Relational Pooling model and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on molecular prediction tasks.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Chen, Zhengdao and Chen, Lei and Villar, Soledad and Bruna, Joan},
	month = oct,
	year = {2020},
	note = {arXiv:2002.04025 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Discrete Mathematics},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\KXIEQS2L\\Chen et al. - 2020 - Can Graph Neural Networks Count Substructures.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\CFQWF4XD\\2002.html:text/html},
}

@article{wang_logical_2022,
	title = {Logical {Queries} on {Knowledge} {Graphs}: {Emerging} {Interface} of {Incomplete} {Relational} {Data}},
	shorttitle = {Logical {Queries} on {Knowledge} {Graphs}},
	journal = {Data Engineering},
	author = {Wang, Zihao and Yin, Hang and Song, Yangqiu},
	year = {2022},
	pages = {3},
	file = {p3.pdf:C\:\\Users\\hyin\\Downloads\\p3.pdf:application/pdf},
}

@book{gallier_discrete_2011,
	title = {Discrete mathematics},
	publisher = {Springer Science \& Business Media},
	author = {Gallier, Jean},
	year = {2011},
}

@inproceedings{ren_smore_2022,
	title = {Smore: {Knowledge} graph completion and multi-hop reasoning in massive knowledge graphs},
	url = {http://arxiv.org/abs/2110.14890},
	abstract = {Knowledge graphs (KGs) capture knowledge in the form of head--relation--tail triples and are a crucial component in many AI systems. There are two important reasoning tasks on KGs: (1) single-hop knowledge graph completion, which involves predicting individual links in the KG; and (2), multi-hop reasoning, where the goal is to predict which KG entities satisfy a given logical query. Embedding-based methods solve both tasks by first computing an embedding for each entity and relation, then using them to form predictions. However, existing scalable KG embedding frameworks only support single-hop knowledge graph completion and cannot be applied to the more challenging multi-hop reasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first general framework for both single-hop and multi-hop reasoning in KGs. Using a single machine SMORE can perform multi-hop reasoning in Freebase KG (86M entities, 338M edges), which is 1,500x larger than previously considered KGs. The key to SMORE's runtime performance is a novel bidirectional rejection sampling that achieves a square root reduction of the complexity of online training data generation. Furthermore, SMORE exploits asynchronous scheduling, overlapping CPU-based data sampling, GPU-based embedding computation, and frequent CPU--GPU IO. SMORE increases throughput (i.e., training speed) over prior multi-hop KG frameworks by 2.2x with minimal GPU memory requirements (2GB for training 400-dim embeddings on 86M-node Freebase) and achieves near linear speed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge graph completion task SMORE achieves comparable or even better runtime performance to state-of-the-art frameworks on both single GPU and multi-GPU settings.},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Zhou, Denny and Leskovec, Jure and Schuurmans, Dale},
	year = {2022},
	note = {arXiv:2110.14890 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {1472--1482},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\53ZW5KVY\\Ren et al. - 2021 - SMORE Knowledge Graph Completion and Multi-hop Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\SIUJPH5D\\2110.html:text/html},
}

@inproceedings{trouillon_complex_2016,
	title = {Complex embeddings for simple link prediction},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Trouillon, Th{\'e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, {\'E}ric and Bouchard, Guillaume},
	year = {2016},
	pages = {2071--2080},
}

@misc{qu_rnnlogic_2021,
	title = {{RNNLogic}: {Learning} {Logic} {Rules} for {Reasoning} on {Knowledge} {Graphs}},
	shorttitle = {{RNNLogic}},
	url = {http://arxiv.org/abs/2010.04029},
	doi = {10.48550/arXiv.2010.04029},
	abstract = {This paper studies learning logic rules for reasoning on knowledge graphs. Logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. Existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). To address these limitations, this paper proposes a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. We develop an EM-based algorithm for optimization. In each iteration, the reasoning predictor is first updated to explore some generated logic rules for reasoning. Then in the E-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the M-step, the rule generator is updated with the rules selected in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Qu, Meng and Chen, Junkun and Xhonneux, Louis-Pascal and Bengio, Yoshua and Tang, Jian},
	month = jul,
	year = {2021},
	note = {arXiv:2010.04029 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\WQJLTHAA\\Qu et al. - 2021 - RNNLogic Learning Logic Rules for Reasoning on Kn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\K9DT7BE7\\2010.html:text/html},
}

@misc{chen_learning_2023,
	title = {Learning {Language} {Representations} with {Logical} {Inductive} {Bias}},
	url = {http://arxiv.org/abs/2302.09458},
	doi = {10.48550/arXiv.2302.09458},
	abstract = {Transformer architectures have achieved great success in solving natural language tasks, which learn strong language representations from large-scale unlabeled texts. In this paper, we seek to go further beyond and explore a new logical inductive bias for better language representation learning. Logic reasoning is known as a formal methodology to reach answers from given knowledge and facts. Inspired by such a view, we develop a novel neural architecture named FOLNet (First-Order Logic Network), to encode this new inductive bias. We construct a set of neural logic operators as learnable Horn clauses, which are further forward-chained into a fully differentiable neural architecture (FOLNet). Interestingly, we find that the self-attention module in transformers can be composed by two of our neural logic operators, which probably explains their strong reasoning performance. Our proposed FOLNet has the same input and output interfaces as other pretrained models and thus could be pretrained/finetuned by using similar losses. It also allows FOLNet to be used in a plug-and-play manner when replacing other pretrained models. With our logical inductive bias, the same set of ``logic deduction skills'' learned through pretraining are expected to be equally capable of solving diverse downstream tasks. For this reason, FOLNet learns language representations that have much stronger transfer capabilities. Experimental results on several language understanding tasks show that our pretrained FOLNet model outperforms the existing strong transformer-based approaches.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Chen, Jianshu},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09458 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\YGTG6WFU\\Chen - 2023 - Learning Language Representations with Logical Ind.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\QFZ56NBT\\2302.html:text/html},
}

@inproceedings{wang_logical_2023,
	title = {Logical {Message} {Passing} {Networks} with {One}-hop {Inference} on {Atomic} {Formulas}},
	url = {https://openreview.net/forum?id=SoyOsp7i_l},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Zihao and Song, Yangqiu and Wong, Ginny and See, Simon},
	year = {2023},
	file = {Full Text:C\:\\Users\\hyin\\Zotero\\storage\\S4QUTVWQ\\Wang et al. - 2023 - Logical Message Passing Networks with One-hop Infe.pdf:application/pdf},
}

@misc{zhu_net_2023,
	title = {A*{Net}: {A} {Scalable} {Path}-based {Reasoning} {Approach} for {Knowledge} {Graphs}},
	shorttitle = {A*{Net}},
	url = {http://arxiv.org/abs/2206.04798},
	abstract = {Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, they suffer from the scalability issue due to the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10\% nodes and 10\% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net achieves competitive performance with embedding methods and converges faster. To our best knowledge, A*Net is the first path-based method for knowledge graph reasoning at such a scale.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Zhu, Zhaocheng and Yuan, Xinyu and Galkin, Mikhail and Xhonneux, Sophie and Zhang, Ming and Gazeau, Maxime and Tang, Jian},
	month = feb,
	year = {2023},
	note = {arXiv:2206.04798 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\A69FQBE5\\Zhu et al. - 2023 - ANet A Scalable Path-based Reasoning Approach fo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\SJGC3J7R\\2206.html:text/html},
}

@misc{ravi_what_2023,
	title = {What happens before and after: {Multi}-{Event} {Commonsense} in {Event} {Coreference} {Resolution}},
	shorttitle = {What happens before and after},
	url = {http://arxiv.org/abs/2302.09715},
	doi = {10.48550/arXiv.2302.09715},
	abstract = {Event coreference models cluster event mentions pertaining to the same real-world event. Recent models rely on contextualized representations to recognize coreference among lexically or contextually similar mentions. However, models typically fail to leverage commonsense inferences, which is particularly limiting for resolving lexically-divergent mentions. We propose a model that extends event mentions with temporal commonsense inferences. Given a complex sentence with multiple events, e.g., "The man killed his wife and got arrested", with the target event "arrested", our model generates plausible events that happen before the target event - such as "the police arrived", and after it, such as "he was sentenced". We show that incorporating such inferences into an existing event coreference model improves its performance, and we analyze the coreferences in which such temporal knowledge is required.},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Ravi, Sahithya and Tanner, Chris and Ng, Raymond and Shwartz, Vered},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09715 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\LUG4QNX3\\Ravi et al. - 2023 - What happens before and after Multi-Event Commons.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\CA23K3US\\2302.html:text/html},
}

@inproceedings{cybulska_using_2014,
	address = {Reykjavik, Iceland},
	title = {Using a sledgehammer to crack a nut? {Lexical} diversity and event coreference resolution},
	shorttitle = {Using a sledgehammer to crack a nut?},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/840_Paper.pdf},
	abstract = {In this paper we examine the representativeness of the EventCorefBank (ECB, Bejan and Harabagiu, 2010) with regards to the language population of large-volume streams of news. The ECB corpus is one of the data sets used for evaluation of the task of event coreference resolution. Our analysis shows that the ECB in most cases covers one seminal event per domain, what considerably simplifies event and so language diversity that one comes across in the news. We augmented the corpus with a new corpus component, consisting of 502 texts, describing different instances of event types that were already captured by the 43 topics of the ECB, making it more representative of news articles on the web. The new {\textquotedblleft}ECB+{\textquotedblright} corpus is available for further research.},
	urldate = {2023-04-20},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'14)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Cybulska, Agata and Vossen, Piek},
	month = may,
	year = {2014},
	pages = {4545--4552},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\T2K3EAQA\\Cybulska and Vossen - 2014 - Using a sledgehammer to crack a nut Lexical diver.pdf:application/pdf},
}

@misc{pyatkin_design_2023,
	title = {Design {Choices} for {Crowdsourcing} {Implicit} {Discourse} {Relations}: {Revealing} the {Biases} {Introduced} by {Task} {Design}},
	shorttitle = {Design {Choices} for {Crowdsourcing} {Implicit} {Discourse} {Relations}},
	url = {http://arxiv.org/abs/2304.00815},
	doi = {10.48550/arXiv.2304.00815},
	abstract = {Disagreement in natural language annotation has mostly been studied from a perspective of biases introduced by the annotators and the annotation frameworks. Here, we propose to analyze another source of bias: task design bias, which has a particularly strong impact on crowdsourced linguistic annotations where natural language is used to elicit the interpretation of laymen annotators. For this purpose we look at implicit discourse relation annotation, a task that has repeatedly been shown to be difficult due to the relations' ambiguity. We compare the annotations of 1,200 discourse relations obtained using two distinct annotation tasks and quantify the biases of both methods across four different domains. Both methods are natural language annotation tasks designed for crowdsourcing. We show that the task design can push annotators towards certain relations and that some discourse relations senses can be better elicited with one or the other annotation approach. We also conclude that this type of bias should be taken into account when training and testing models.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Pyatkin, Valentina and Yung, Frances and Scholman, Merel C. J. and Tsarfaty, Reut and Dagan, Ido and Demberg, Vera},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00815 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\3XJDEP6P\\Pyatkin et al. - 2023 - Design Choices for Crowdsourcing Implicit Discours.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\F9F359VX\\2304.html:text/html},
}

@misc{xiong_geometric_2023,
	title = {Geometric {Relational} {Embeddings}: {A} {Survey}},
	shorttitle = {Geometric {Relational} {Embeddings}},
	url = {http://arxiv.org/abs/2304.11949},
	abstract = {Geometric relational embeddings map relational data as geometric objects that combine vector information suitable for machine learning and structured/relational information for structured/relational reasoning, typically in low dimensions. Their preservation of relational structures and their appealing properties and interpretability have led to their uptake for tasks such as knowledge graph completion, ontology and hierarchy reasoning, logical query answering, and hierarchical multi-label classification. We survey methods that underly geometric relational embeddings and categorize them based on (i) the embedding geometries that are used to represent the data; and (ii) the relational reasoning tasks that they aim to improve. We identify the desired properties (i.e., inductive biases) of each kind of embedding and discuss some potential future work.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Xiong, Bo and Nayyeri, Mojtaba and Jin, Ming and He, Yunjie and Cochez, Michael and Pan, Shirui and Staab, Steffen},
	month = apr,
	year = {2023},
	note = {arXiv:2304.11949 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\WINVYHFF\\2304.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\XM64SZJG\\Xiong et al. - 2023 - Geometric Relational Embeddings A Survey.pdf:application/pdf},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\XSUNMFVG\\1609.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\GKLSSGD4\\Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf},
}

@misc{yin_existential_2023,
	title = {On {Existential} {First} {Order} {Queries} {Inference} on {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2304.07063},
	doi = {10.48550/arXiv.2304.07063},
	abstract = {Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Yin, Hang and Wang, Zihao and Song, Yangqiu},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07063 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Logic in Computer Science, Computer Science - Databases},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\EHYJGFG9\\Yin et al. - 2023 - On Existential First Order Queries Inference on Kn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\7B6K3A42\\2304.html:text/html},
}

@article{dechter_network-based_1987,
	title = {Network-based heuristics for constraint-satisfaction problems},
	volume = {34},
	number = {1},
	journal = {Artificial intelligence},
	author = {Dechter, Rina and Pearl, Judea},
	year = {1987},
	note = {Publisher: Elsevier},
	pages = {1--38},
	file = {Dechter and Pearl - 1987 - Network-based heuristics for constraint-satisfacti.pdf:C\:\\Users\\hyin\\Zotero\\storage\\HFX3LGRM\\Dechter and Pearl - 1987 - Network-based heuristics for constraint-satisfacti.pdf:application/pdf},
}

@inproceedings{feng_towards_2023,
	title = {Towards {Open} {Temporal} {Graph} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=N9Pk5iSCzAn},
	abstract = {Graph neural networks (GNNs) for temporal graphs have recently attracted increasing attentions, where a common assumption is that the class set for nodes is closed. However, in real-world scenarios, it often faces the open set problem with the dynamically increased class set as the time passes by. This will bring two big challenges to the existing dynamic GNN methods: (i) How to dynamically propagate appropriate information in an open temporal graph, where new class nodes are often linked to old class nodes. This case will lead to a sharp contradiction. This is because typical GNNs are prone to make the embeddings of connected nodes become similar, while we expect the embeddings of these two interactive nodes to be distinguishable since they belong to different classes. (ii) How to avoid catastrophic knowledge forgetting over old classes when learning new classes occurred in temporal graphs. In this paper, we propose a general and principled learning approach for open temporal graphs, called OTGNet, with the goal of addressing the above two challenges. We assume the knowledge of a node can be disentangled into class-relevant and class-agnostic one, and thus explore a new message passing mechanism by extending the information bottleneck principle to only propagate class-agnostic knowledge between nodes of different classes, avoiding aggregating conflictive information. Moreover, we devise a strategy to select both important and diverse triad sub-graph structures for effective class-incremental learning. Extensive experiments on three real-world datasets of different domains demonstrate the superiority of our method, compared to the baselines.},
	language = {en},
	urldate = {2023-05-03},
	author = {Feng, Kaituo and Li, Changsheng and Zhang, Xiaolu and Zhou, Jun},
	month = feb,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\DW3UTU8J\\Feng et al. - 2023 - Towards Open Temporal Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{han_logical_2023,
	title = {Logical {Entity} {Representation} in {Knowledge}-{Graphs} for {Differentiable} {Rule} {Learning}},
	url = {https://openreview.net/forum?id=JdgO-ht1uTN},
	abstract = {Probabilistic logical rule learning has shown great strength in logical rule mining and knowledge graph completion. It learns logical rules to predict missing edges by reasoning on existing edges in the knowledge graph. However, previous efforts have largely been limited to only modeling chain-like Horn clauses such as R1(x; z) {\textasciicircum} R2(z; y) ) H(x; y). This formulation overlooks additional contextual information from neighboring sub-graphs of entity variables x, y and z. Intuitively, there is a large gap here, as local sub-graphs have been found to provide important information for knowledge graph completion. Inspired by these observations, we propose Logical Entity RePresentation (LERP) to encode contextual information of entities in the knowledge graph. A LERP is designed as a vector of probabilistic logical functions on the entity{\textquoteright}s neighboring sub-graph. It is an interpretable representation while allowing for differentiable optimization. We can then incorporate LERP into probabilistic logical rule learning to learn more expressive rules. Empirical results demonstrate that with LERP, our model outperforms other rule learning methods in knowledge graph completion and is comparable or even superior to state-of-the-art black-box methods. Moreover, we find that our model can discover a more expressive family of logical rules. LERP can also be further combined with embedding learning methods like TransE to make it more interpretable.},
	language = {en},
	urldate = {2023-05-03},
	author = {Han, Chi and He, Qizheng and Yu, Charles and Du, Xinya and Tong, Hanghang and Ji, Heng},
	month = feb,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\5SS58M8G\\Han et al. - 2023 - Logical Entity Representation in Knowledge-Graphs .pdf:application/pdf},
}

@misc{liu_were_2023,
	title = {We're {Afraid} {Language} {Models} {Aren}'t {Modeling} {Ambiguity}},
	url = {http://arxiv.org/abs/2304.14399},
	doi = {10.48550/arXiv.2304.14399},
	abstract = {Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32\% of the time in human evaluation, compared to 90\% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr, Alane and West, Peter and Koller, Alexander and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14399 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\XWYX45RH\\Liu et al. - 2023 - We're Afraid Language Models Aren't Modeling Ambig.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\QKLTPXQG\\2304.html:text/html},
}

@misc{tang_tar_2022,
	title = {{TAR}: {Neural} {Logical} {Reasoning} across {TBox} and {ABox}},
	shorttitle = {{TAR}},
	url = {http://arxiv.org/abs/2205.14591},
	doi = {10.48550/arXiv.2205.14591},
	abstract = {Many ontologies, i.e., Description Logic (DL) knowledge bases, have been developed to provide rich knowledge about various domains. An ontology consists of an ABox, i.e., assertion axioms between two entities or between a concept and an entity, and a TBox, i.e., terminology axioms between two concepts. Neural logical reasoning (NLR) is a fundamental task to explore such knowledge bases, which aims at answering multi-hop queries with logical operations based on distributed representations of queries and answers. While previous NLR methods can give specific entity-level answers, i.e., ABox answers, they are not able to provide descriptive concept-level answers, i.e., TBox answers, where each concept is a description of a set of entities. In other words, previous NLR methods only reason over the ABox of an ontology while ignoring the TBox. In particular, providing TBox answers enables inferring the explanations of each query with descriptive concepts, which make answers comprehensible to users and are of great usefulness in the field of applied ontology. In this work, we formulate the problem of neural logical reasoning across TBox and ABox (TA-NLR), solving which needs to address challenges in incorporating, representing, and operating on concepts. We propose an original solution named TAR for TA-NLR. Firstly, we incorporate description logic based ontological axioms to provide the source of concepts. Then, we represent concepts and queries as fuzzy sets, i.e., sets whose elements have degrees of membership, to bridge concepts and queries with entities. Moreover, we design operators involving concepts on top of fuzzy set representation of concepts and queries for optimization and inference. Extensive experimental results on two real-world datasets demonstrate the effectiveness of TAR for TA-NLR.},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Tang, Zhenwei and Pei, Shichao and Peng, Xi and Zhuang, Fuzhen and Zhang, Xiangliang and Hoehndorf, Robert},
	month = aug,
	year = {2022},
	note = {arXiv:2205.14591 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\HC6J3V8N\\Tang et al. - 2022 - TAR Neural Logical Reasoning across TBox and ABox.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\VWM2MFS3\\2205.html:text/html},
}

@inproceedings{hao_towards_2013,
	title = {Towards never-ending learning from time series streams},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Hao, Yuan and Chen, Yanping and Zakaria, Jesin and Hu, Bing and Rakthanmanon, Thanawin and Keogh, Eamonn},
	year = {2013},
	pages = {874--882},
}

@article{ehrlinger_towards_2016,
	title = {Towards a definition of knowledge graphs.},
	volume = {48},
	number = {1-4},
	journal = {SEMANTiCS (Posters, Demos, SuCCESS)},
	author = {Ehrlinger, Lisa and W{\"o}{\ss}, Wolfram},
	year = {2016},
	pages = {2},
}

@misc{liu_g-eval_2023,
	title = {G-{Eval}: {NLG} {Evaluation} using {GPT}-4 with {Better} {Human} {Alignment}},
	shorttitle = {G-{Eval}},
	url = {http://arxiv.org/abs/2303.16634},
	doi = {10.48550/arXiv.2303.16634},
	abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
	month = may,
	year = {2023},
	note = {arXiv:2303.16634 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\3LTKEVX5\\Liu et al. - 2023 - G-Eval NLG Evaluation using GPT-4 with Better Hum.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\6JZ89KFH\\2303.html:text/html},
}

@inproceedings{kolaitis_conjunctive-query_1998,
	title = {Conjunctive-query containment and constraint satisfaction},
	booktitle = {Proceedings of the seventeenth {ACM} {SIGACT}-{SIGMOD}-{SIGART} symposium on {Principles} of database systems},
	author = {Kolaitis, Phokion G and Vardi, Moshe Y},
	year = {1998},
	pages = {205--213},
}

@inproceedings{gottlob_hypertree_1999,
	title = {Hypertree decompositions and tractable queries},
	booktitle = {Proceedings of the eighteenth {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on {Principles} of database systems},
	author = {Gottlob, Georg and Leone, Nicola and Scarcello, Francesco},
	year = {1999},
	pages = {21--32},
}

@article{greco_power_2013,
	title = {On {The} {Power} of {Tree} {Projections}: {Structural} {Tractability} of {Enumerating} {CSP} {Solutions}},
	volume = {18},
	issn = {1383-7133, 1572-9354},
	shorttitle = {On {The} {Power} of {Tree} {Projections}},
	url = {http://arxiv.org/abs/1005.1567},
	doi = {10.1007/s10601-012-9129-8},
	abstract = {The problem of deciding whether CSP instances admit solutions has been deeply studied in the literature, and several structural tractability results have been derived so far. However, constraint satisfaction comes in practice as a computation problem where the focus is either on finding one solution, or on enumerating all solutions, possibly projected to some given set of output variables. The paper investigates the structural tractability of the problem of enumerating (possibly projected) solutions, where tractability means here computable with polynomial delay (WPD), since in general exponentially many solutions may be computed. A general framework based on the notion of tree projection of hypergraphs is considered, which generalizes all known decomposition methods. Tractability results have been obtained both for classes of structures where output variables are part of their specification, and for classes of structures where computability WPD must be ensured for any possible set of output variables. These results are shown to be tight, by exhibiting dichotomies for classes of structures having bounded arity and where the tree decomposition method is considered.},
	number = {1},
	urldate = {2023-06-03},
	journal = {Constraints},
	author = {Greco, Gianluigi and Scarcello, Francesco},
	month = jan,
	year = {2013},
	note = {arXiv:1005.1567 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, 68T20, F.2.2, G.2.1, I.2.8},
	pages = {38--74},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\BISYAD36\\1005.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\X2KE37KK\\Greco and Scarcello - 2013 - On The Power of Tree Projections Structural Tract.pdf:application/pdf},
}

@article{carbonnel_tractability_2016,
	title = {Tractability in constraint satisfaction problems: a survey},
	volume = {21},
	number = {2},
	journal = {Constraints},
	author = {Carbonnel, Cl{\'e}ment and Cooper, Martin C},
	year = {2016},
	note = {Publisher: Springer},
	pages = {115--144},
}

@article{wang_wasserstein-fisher-rao_2023,
	title = {Wasserstein-{Fisher}-{Rao} {Embedding}: {Logical} {Query} {Embeddings} with {Local} {Comparison} and {Global} {Transport}},
	journal = {arXiv preprint arXiv:2305.04034},
	author = {Wang, Zihao and Fei, Weizhi and Yin, Hang and Song, Yangqiu and Wong, Ginny Y and See, Simon},
	year = {2023},
}

@misc{wu_coke_2023,
	title = {{COKE}: {A} {Cognitive} {Knowledge} {Graph} for {Machine} {Theory} of {Mind}},
	shorttitle = {{COKE}},
	url = {http://arxiv.org/abs/2305.05390},
	doi = {10.48550/arXiv.2305.05390},
	abstract = {Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. Beyond that, we further generalize COKE using pre-trained language models and build a powerful cognitive generation model COKE+. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE and the superior ToM ability of COKE+.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Wu, Jincenzi and Chen, Zhuang and Deng, Jiawen and Sabour, Sahand and Huang, Minlie},
	month = may,
	year = {2023},
	note = {arXiv:2305.05390 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\GQMJPVCR\\Wu et al. - 2023 - COKE A Cognitive Knowledge Graph for Machine Theo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\RGLQIAIH\\2305.html:text/html},
}

@misc{ruiz_graphon_2020,
	title = {Graphon {Neural} {Networks} and the {Transferability} of {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2006.03548},
	abstract = {Graph neural networks (GNNs) rely on graph convolutions to extract local features from network data. These graph convolutions combine information from adjacent nodes using coefficients that are shared across all nodes. Since these coefficients are shared and do not depend on the graph, one can envision using the same coefficients to define a GNN on another graph. This motivates analyzing the transferability of GNNs across graphs. In this paper we introduce graphon NNs as limit objects of GNNs and prove a bound on the difference between the output of a GNN and its limit graphon-NN. This bound vanishes with growing number of nodes if the graph convolutional filters are bandlimited in the graph spectral domain. This result establishes a tradeoff between discriminability and transferability of GNNs.},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Ruiz, Luana and Chamon, Luiz F. O. and Ribeiro, Alejandro},
	month = oct,
	year = {2020},
	note = {arXiv:2006.03548 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\W4MV7PXZ\\2006.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\GNJRLUE2\\Ruiz et al. - 2020 - Graphon Neural Networks and the Transferability of.pdf:application/pdf},
}

@misc{du_cognitive_2019,
	title = {Cognitive {Knowledge} {Graph} {Reasoning} for {One}-shot {Relational} {Learning}},
	url = {http://arxiv.org/abs/1906.05489},
	doi = {10.48550/arXiv.1906.05489},
	abstract = {Inferring new facts from existing knowledge graphs (KG) with explainable reasoning processes is a significant problem and has received much attention recently. However, few studies have focused on relation types unseen in the original KG, given only one or a few instances for training. To bridge this gap, we propose CogKR for one-shot KG reasoning. The one-shot relational learning problem is tackled through two modules: the summary module summarizes the underlying relationship of the given instances, based on which the reasoning module infers the correct answers. Motivated by the dual process theory in cognitive science, in the reasoning module, a cognitive graph is built by iteratively coordinating retrieval (System 1, collecting relevant evidence intuitively) and reasoning (System 2, conducting relational reasoning over collected information). The structural information offered by the cognitive graph enables our model to aggregate pieces of evidence from multiple reasoning paths and explain the reasoning process graphically. Experiments show that CogKR substantially outperforms previous state-of-the-art models on one-shot KG reasoning benchmarks, with relative improvements of 24.3\%-29.7\% on MRR. The source code is available at https://github.com/THUDM/CogKR.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Du, Zhengxiao and Zhou, Chang and Ding, Ming and Yang, Hongxia and Tang, Jie},
	month = jun,
	year = {2019},
	note = {arXiv:1906.05489 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\T8I4XMTZ\\Du et al. - 2019 - Cognitive Knowledge Graph Reasoning for One-shot R.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\7ETPSHNC\\1906.html:text/html},
}

@misc{forbes_social_2021,
	title = {Social {Chemistry} 101: {Learning} to {Reason} about {Social} and {Moral} {Norms}},
	shorttitle = {Social {Chemistry} 101},
	url = {http://arxiv.org/abs/2011.00620},
	abstract = {Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes." We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Forbes, Maxwell and Hwang, Jena D. and Shwartz, Vered and Sap, Maarten and Choi, Yejin},
	month = aug,
	year = {2021},
	note = {arXiv:2011.00620 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\Y8V9C3HA\\2011.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\U87I9JE5\\Forbes et al. - 2021 - Social Chemistry 101 Learning to Reason about Soc.pdf:application/pdf},
}

@inproceedings{carlson_toward_2010,
	title = {Toward an architecture for never-ending language learning},
	volume = {24},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam and Mitchell, Tom},
	year = {2010},
	note = {Issue: 1},
	pages = {1306--1313},
}

@misc{barcelo_weisfeiler_2022,
	title = {Weisfeiler and {Leman} {Go} {Relational}},
	url = {http://arxiv.org/abs/2211.17113},
	doi = {10.48550/arXiv.2211.17113},
	abstract = {Knowledge graphs, modeling multi-relational data, improve numerous applications such as question answering or graph logical reasoning. Many graph neural networks for such data emerged recently, often outperforming shallow architectures. However, the design of such multi-relational graph neural networks is ad-hoc, driven mainly by intuition and empirical insights. Up to now, their expressivity, their relation to each other, and their (practical) learning performance is poorly understood. Here, we initiate the study of deriving a more principled understanding of multi-relational graph neural networks. Namely, we investigate the limitations in the expressive power of the well-known Relational GCN and Compositional GCN architectures and shed some light on their practical learning performance. By aligning both architectures with a suitable version of the Weisfeiler-Leman test, we establish under which conditions both models have the same expressive power in distinguishing non-isomorphic (multi-relational) graphs or vertices with different structural roles. Further, by leveraging recent progress in designing expressive graph neural networks, we introduce the \$k\$-RN architecture that provably overcomes the expressiveness limitations of the above two architectures. Empirically, we confirm our theoretical findings in a vertex classification setting over small and large multi-relational graphs.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Barcelo, Pablo and Galkin, Mikhail and Morris, Christopher and Orth, Miguel Romero},
	month = nov,
	year = {2022},
	note = {arXiv:2211.17113 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\35YR8FP8\\Barcelo et al. - 2022 - Weisfeiler and Leman Go Relational.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\GZPVX7YU\\2211.html:text/html},
}

@inproceedings{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {26},
	url = {https://papers.nips.cc/paper_files/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
	abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
	urldate = {2023-06-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	year = {2013},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\TX3HZAEV\\Bordes et al. - 2013 - Translating Embeddings for Modeling Multi-relation.pdf:application/pdf},
}

@misc{morris_weisfeiler_2020,
	title = {Weisfeiler and {Leman} go sparse: {Towards} scalable higher-order graph embeddings},
	shorttitle = {Weisfeiler and {Leman} go sparse},
	url = {http://arxiv.org/abs/1904.01543},
	abstract = {Graph kernels based on the \$1\$-dimensional Weisfeiler-Leman algorithm and corresponding neural architectures recently emerged as powerful tools for (supervised) learning with graphs. However, due to the purely local nature of the algorithms, they might miss essential patterns in the given data and can only handle binary relations. The \$k\$-dimensional Weisfeiler-Leman algorithm addresses this by considering \$k\$-tuples, defined over the set of vertices, and defines a suitable notion of adjacency between these vertex tuples. Hence, it accounts for the higher-order interactions between vertices. However, it does not scale and may suffer from overfitting when used in a machine learning setting. Hence, it remains an important open problem to design WL-based graph learning methods that are simultaneously expressive, scalable, and non-overfitting. Here, we propose local variants and corresponding neural architectures, which consider a subset of the original neighborhood, making them more scalable, and less prone to overfitting. The expressive power of (one of) our algorithms is strictly higher than the original algorithm, in terms of ability to distinguish non-isomorphic graphs. Our experimental study confirms that the local algorithms, both kernel and neural architectures, lead to vastly reduced computation times, and prevent overfitting. The kernel version establishes a new state-of-the-art for graph classification on a wide range of benchmark datasets, while the neural version shows promising performance on large-scale molecular regression tasks.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Morris, Christopher and Rattan, Gaurav and Mutzel, Petra},
	month = oct,
	year = {2020},
	note = {arXiv:1904.01543 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\SLQ3GIGQ\\1904.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\XTNSBPP9\\Morris et al. - 2020 - Weisfeiler and Leman go sparse Towards scalable h.pdf:application/pdf},
}

@misc{chen_say_2023,
	title = {Say {What} {You} {Mean}! {Large} {Language} {Models} {Speak} {Too} {Positively} about {Negative} {Commonsense} {Knowledge}},
	url = {http://arxiv.org/abs/2305.05976},
	abstract = {Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as "lions don't live in the ocean", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Chen, Jiangjie and Shi, Wei and Fu, Ziquan and Cheng, Sijie and Li, Lei and Xiao, Yanghua},
	month = may,
	year = {2023},
	note = {arXiv:2305.05976 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\7G2XHEUX\\2305.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\ZW4SR8IS\\Chen et al. - 2023 - Say What You Mean! Large Language Models Speak Too.pdf:application/pdf},
}

@misc{vashishth_composition-based_2020,
	title = {Composition-based {Multi}-{Relational} {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1911.03082},
	doi = {10.48550/arXiv.1911.03082},
	abstract = {Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. It also generalizes several of the existing multi-relational GCN methods. We evaluate our proposed method on multiple tasks such as node classification, link prediction, and graph classification, and achieve demonstrably superior results. We make the source code of CompGCN available to foster reproducible research.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Vashishth, Shikhar and Sanyal, Soumya and Nitin, Vikram and Talukdar, Partha},
	month = jan,
	year = {2020},
	note = {arXiv:1911.03082 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\T4EHS852\\Vashishth et al. - 2020 - Composition-based Multi-Relational Graph Convoluti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\XKU88ZLR\\1911.html:text/html},
}

@article{poess_new_2000,
	title = {New {TPC} benchmarks for decision support and web commerce},
	volume = {29},
	number = {4},
	journal = {ACM Sigmod Record},
	author = {Poess, Meikel and Floyd, Chris},
	year = {2000},
	note = {Publisher: ACM New York, NY, USA},
	pages = {64--71},
}

@inproceedings{leskovec_databases_2023,
	address = {New York, NY, USA},
	series = {{PODS} '23},
	title = {Databases as {Graphs}: {Predictive} {Queries} for {Declarative} {Machine} {Learning}},
	isbn = {9798400701276},
	url = {https://doi.org/10.1145/3584372.3589939},
	doi = {10.1145/3584372.3589939},
	abstract = {The era of intelligent systems and applications is here and AI has been disrupting science and industry. The speed of change has been breathtaking. However, building AI-based solutions is hard, training Machine Learning models and putting them in production takes highly-skilled teams months if not years. There is a real need to drastically simplify machine learning workflow and bring AI closer to users, and make it accessible to a wide range of audiences.In this talk, I present Kumo, a no-code machine learning platform that enables data scientists to solve a wide range of machine learning problems over relational tables in a database in a simple declarative way. Kumo provides a SQL-like Predictive Query language that allows for declarative specification of machine learning problems in a wide array of applications from sales and marketing to customer retention and recommender systems. The key insight in Kumo is that relational schema can be represented as a heterogeneous hypergraph. Such graphs are emerging as an abstraction to represent complex data and Deep Graph Neural Networks (GNN) can then be used to learn optimal feature representation for any entity of interest.Automatic learning to encode graph structure into low-dimensional embeddings brings several benefits: (1) automatic learning from the entire data spread across multiple relational tables (2) no manual feature engineering as the system learns optimal embeddings; (3) state-of-the-art predictive performance. Building such a distributed system for GNN training and inference poses several interesting algorithmic and data processing challenges, which we address by innovative machine learning methods and careful algorithm/architecture codesign. Kumo has already been successfully deployed at several major companies.},
	booktitle = {Proceedings of the 42nd {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Leskovec, Jure},
	year = {2023},
	note = {event-place: Seattle, WA, USA},
	keywords = {declarative machine leaning, deep learning, graph neural networks},
	pages = {1},
}

@misc{feng_generic_2023,
	title = {Generic {Temporal} {Reasoning} with {Differential} {Analysis} and {Explanation}},
	url = {http://arxiv.org/abs/2212.10467},
	doi = {10.48550/arXiv.2212.10467},
	abstract = {Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems' generalizability due to existing datasets' limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY's supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Feng, Yu and Zhou, Ben and Wang, Haoyu and Jin, Helen and Roth, Dan},
	month = may,
	year = {2023},
	note = {arXiv:2212.10467 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\hyin\\Zotero\\storage\\7TAXADMY\\Feng et al. - 2023 - Generic Temporal Reasoning with Differential Analy.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\F2JB7TCM\\2212.html:text/html},
}

@misc{zhang_labeling_2022,
	title = {Labeling {Trick}: {A} {Theory} of {Using} {Graph} {Neural} {Networks} for {Multi}-{Node} {Representation} {Learning}},
	shorttitle = {Labeling {Trick}},
	url = {http://arxiv.org/abs/2010.16103},
	abstract = {In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form -- labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.},
	urldate = {2023-07-09},
	publisher = {arXiv},
	author = {Zhang, Muhan and Li, Pan and Xia, Yinglong and Wang, Kai and Jin, Long},
	month = jan,
	year = {2022},
	note = {arXiv:2010.16103 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\UG3X8U6F\\2010.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\E9ZYCLKV\\Zhang et al. - 2022 - Labeling Trick A Theory of Using Graph Neural Net.pdf:application/pdf},
}

@book{lovasz_large_2012,
	address = {Providence, Rhode Island},
	series = {Colloquium {Publications}},
	title = {Large {Networks} and {Graph} {Limits}},
	volume = {60},
	isbn = {978-0-8218-9085-1 978-1-4704-1583-9},
	url = {http://www.ams.org/coll/060},
	language = {en},
	urldate = {2023-07-11},
	publisher = {American Mathematical Society},
	author = {Lov{\'a}sz, L{\'a}szl{\'o}},
	month = dec,
	year = {2012},
	doi = {10.1090/coll/060},
	file = {Lov{\'a}sz - 2012 - Large Networks and Graph Limits.pdf:C\:\\Users\\hyin\\Zotero\\storage\\RJYSZS8U\\Lov{\'a}sz - 2012 - Large Networks and Graph Limits.pdf:application/pdf},
}

@inproceedings{ruffinelli_you_2019,
	title = {You {CAN} {Teach} an {Old} {Dog} {New} {Tricks}! {On} {Training} {Knowledge} {Graph} {Embeddings}},
	url = {https://openreview.net/forum?id=BkxSmlBFvr},
	abstract = {Knowledge graph embedding (KGE) models learn algebraic representations of the entities and relations in a knowledge graph. A vast number of KGE techniques for multi-relational link prediction have been proposed in the recent literature, often with state-of-the-art performance. These approaches differ along a number of dimensions, including different model architectures, different training strategies, and different approaches to hyperparameter optimization. In this paper, we take a step back and aim to summarize and quantify empirically the impact of each of these dimensions on model performance. We report on the results of an extensive experimental study with popular model architectures and training strategies across a wide range of hyperparameter settings. We found that when trained appropriately, the relative performance differences between various model architectures often shrinks and sometimes even reverses when compared to prior results. For example, RESCAL{\textasciitilde}{\textbackslash}citep\{nickel2011three\}, one of the first KGE models, showed strong performance when trained with state-of-the-art techniques; it was competitive to or outperformed more recent architectures. We also found that good (and often superior to prior studies) model configurations can be found by exploring relatively few random samples from a large hyperparameter space. Our results suggest that many of the more advanced architectures and techniques proposed in the literature should be revisited to reassess their individual benefits. To foster further reproducible research, we provide all our implementations and experimental results as part of the open source LibKGE framework.},
	language = {en},
	urldate = {2023-07-11},
	author = {Ruffinelli, Daniel and Broscheit, Samuel and Gemulla, Rainer},
	month = sep,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\LJ4PD7KM\\Ruffinelli et al. - 2019 - You CAN Teach an Old Dog New Tricks! On Training K.pdf:application/pdf},
}

@misc{sun_information-geometric_2019,
	title = {Information-{Geometric} {Set} {Embeddings} ({IGSE}): {From} {Sets} to {Probability} {Distributions}},
	shorttitle = {Information-{Geometric} {Set} {Embeddings} ({IGSE})},
	url = {http://arxiv.org/abs/1911.12463},
	abstract = {This letter introduces an abstract learning problem called the "set embedding": The objective is to map sets into probability distributions so as to lose less information. We relate set union and intersection operations with corresponding interpolations of probability distributions. We also demonstrate a preliminary solution with experimental results on toy set embedding examples.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Sun, Ke and Nielsen, Frank},
	month = dec,
	year = {2019},
	note = {arXiv:1911.12463 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\hyin\\Zotero\\storage\\5MM6RMAL\\1911.html:text/html;Full Text PDF:C\:\\Users\\hyin\\Zotero\\storage\\FKNRPGH7\\Sun and Nielsen - 2019 - Information-Geometric Set Embeddings (IGSE) From .pdf:application/pdf},
}

@article{zhou_bipartite_2007,
	title = {Bipartite network projection and personal recommendation},
	volume = {76},
	number = {4},
	journal = {Physical review E},
	author = {Zhou, Tao and Ren, Jie and Medo, Mat{\'u}{\v s} and Zhang, Yi-Cheng},
	year = {2007},
	note = {Publisher: APS},
	pages = {046115},
}
