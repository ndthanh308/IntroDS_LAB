\section{Motivation}
\label{sec:motivation}

In this section, we conduct controlled experiments to shed light on the limitations of existing FEC, super resolution, and video ABR algorithms. 

\para{Limitations of FEC:} FEC has been widely used for video frame recovery. For example, WebRTC is a popular technology that supports steaming videos as well as data and voice on both browsers and native clients. It uses a hybrid of Negative Acknowledgement (NACK) and FEC for error recovery during video streaming. When the roundtrip time (RTT) is low, NACK is used; otherwise FEC is used to avoid excessive delay. We evaluate the benefit of FEC. \zhaoyuan{As shown in Figure~\ref{fig:frame_loss_redundant}, 1\% packet losses require 25\% FEC in order to achieve close to 0 video frame losses. The corresponding numbers for 3\% and 5\% packet losses are 30\% and 35\% FEC. These numbers show that FEC is very expensive. Similar observations are reported in \cite{RFEC}.}

% Figure environment removed

Figure~\ref{fig:qoe_redundant} further plots the video QoE under different packet loss rates. \zhaoyuan{Under 5\% packet loss rate, the QoE even first decreases after adding FEC until sufficient FEC is added to recover the packet losses.} In general, FEC redundancy needs to be around 5x higher than the packet loss rates in order to support a successful recovery. This is rather expensive. These results motivate our work on developing ML-based video recovery. We also compare our video recovery under different amounts of FEC. We observe a similar trend as we increase the redundancy level. Meanwhile, our recovery achieves a higher QoE and reduces the amount of desired FEC. 

% show CDF of video frame loss rates in different traces (w/o FEC)

% show CDF of video frame loss rates in different traces under different amount of FEC (e.g., 5\%, 10\%, 20\%, 50\%) to conclude that FEC is expensive and still cannot recover many lost frames
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{graphicx}

\begin{table}[]
\centering
\resizebox{0.55\columnwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\multirow{2}{*}{} & \multicolumn{4}{c}{Methods}       \\
                  & RLSP~\cite{DarioFuoli2019EfficientVS}   & BasicVSR~\cite{KelvinCKChan2021BasicVSRTS} & CKBG~\cite{JunXiao2022OnlineVS}  & ours  \\ \midrule
FLOPS(G)          & 132.94 & 71.33    & 17.8  & 10.8  \\
params(K)         & 1154   & 1887     & 1750  & 1619  \\
latency(ms)       & 5000   & 3500     & 1000  & 22    \\
PSNR              & 28.5   & 29.8     & 29.7  & 27.1  \\
SSIM              & 0.814  & 0.853    & 0.851 & 0.801 \\ \bottomrule
\end{tabular}%
}
\vspace{10pt}
\caption{Compare super-resolution with other methods. FLOPS and latency were validated on the REDS4~\cite{Nah_2019_CVPR_Workshops_REDS} dataset using 180*320 resolution as input for a 4x up-scale super resolution, produced on an iPhone 12.}
% \vspace{-10pt}
\label{tab:existing-sr}
\end{table}

\para{Limitations of existing super-resolution algorithms:} Table~\ref{tab:existing-sr} shows the performance and running time of existing SR algorithms on iPhone 12. We train on the REDS~\cite{Nah_2019_CVPR_Workshops_REDS} dataset and evaluate Peak Signal to Noise Ratio (PSNR)~\cite{PSNR} and Structure Similiarity (SSIM)~\cite{SSIM} on REDS4 for fair comparison. Both PSNR and SSIM are widely used video quality metrics, and their higher values indicate better quality. As we can see, the existing approaches are too slow for mobile devices. These results indicate the need for developing ML recovery and SR for mobile devices. Because we use smaller feature maps and more efficient feature utilization into optical flow network, as well as wrapping optimization for mobile devices, our FLOPS are smaller and can run in real time on the iPhone device. 
% Note that the running time of our scheme is 22 ms, which is slightly different from the one reported in our evaluation due to different video datasets used here. % needs 27 ms for SR using the videos used in this table and




