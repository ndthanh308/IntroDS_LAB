\section{Evaluation}
\label{sec:eval}

In this section, we first present our evaluation methodology and then describe performance results.

\subsection{Evaluation Methodology}
\label{ssec:eval-method}

\para{Network traces:} To understand the performance of our video enhancement approaches under diverse scenarios, we collect network traces over QUIC from WiFi, 3G, 4G, and 5G networks, shown in Table~\ref{tab:network_traces}. \kj{We use \textit{net-export}~\cite{net-export} in Chrome to collect the QUIC-related packets while watching Youtube videos. We especially identify packet loss in QUIC by capturing \textit{LOSS\_RETRANSMISSION} and \textit{PTO\_RETRANSMISSION} on the transmission type of the packet. Meanwhile, we measure the downlink throughput using iperf from an Azure server located in the central U.S. to a local client over the Internet. The 3G, 4G and 5G traces include static and walking scenarios. We also move the local client randomly to add mobility to the WiFi traces.}

% \kj{We also accumulate network traces from WiFi, 3G, 4G, and 5G over QUIC to show that our system is also indispensable in modern protocols. We use \textit{net-export}~\cite{net-export} in Chrome to collect the QUIC-related packets. We especially identify packet loss in QUIC by capturing \textit{LOSS\_RETRANSMISSION} and \textit{PTO\_RETRANSMISSION} on the transmission type of the packet. We run iperf to measure TCP/UDP throughput in January 2023.}

% We use 3G, 4G, and 5G traces from the existing works~\cite{mao2017neural,5G-measurement2}. We collect WiFi traces by running iperf from an Azure server to a local client over the Internet. The Azure server is located in the central U.S. and we move the local client randomly to add mobility for WiFi traces.
% We collect LEO traces in the StarLink network. We have access to a StarLink RV ground station in the west coast of the U.S.

% Figure environment removed

\begin{table}
  \centering
  \resizebox{0.5\columnwidth}{!}{%
  \begin{tabular}{c|c|c|c|c}
    \toprule
    & 3G & 4G & 5G & WiFi \\
    \midrule
    Amount & 45 & 62 & 53 & 68 \\
    Avg. Duration (s) & 322 & 317 & 302 & 309 \\
    Avg. Throughput (Mbps) & 7.5 & 21.6 & 36.4 & 82.3 \\
    Avg. Packet loss rate (\%) & 0.9 & 1.3 & 1.6 & 0.5 \\
    % Source & ~\cite{mao2017neural} & ~\cite{5G-measurement2} & ~\cite{5G-measurement2} & self-collected \\
    \bottomrule
  \end{tabular}
  }
  \vspace{10pt}
  \caption{Network traces}
  \label{tab:network_traces}
  % \vspace{-10pt}
\end{table}

\para{Video datasets:} We use the video dataset from NEMO for the evaluation. We choose videos from the top ten popular categories~\cite{medium-report} on YouTube: 'Product review', 'How-to', 'Vlogs', 'Game play', 'Skit', 'Haul', 'Challenges', 'Favorite', 'Education', and 'Unboxing'. From each category, we select five videos from distinct creators which support 4K at 30fps and are at least 5 minutes long. Then, four of them are distributed to the training dataset and the other belongs to the testing dataset. For adaptive streaming, we transcode them into multiple bitrate versions using the VP9 codec as per Wowzaâ€™s recommendation~\cite{wowza-recommendation}: \{512, 1024, 1600, 2640, 4400\} kbps at \{240, 360, 480, 720, 1080\}p resolutions. The GOP size is 120 (4 sec). 

\para{Performance metrics:} We use raw 1080p videos as a reference for measuring PSNR. We quantify the quality of recovered and super-resolved video frames using two widely used video quality metrics: SSIM and PSNR. Higher SSIM and PSNR values indicate better video quality. We quantify the performance of our system using QoE. A higher QoE indicates better video streaming for users. 


\subsection{DNN Performance} 

First, we evaluate the DNN performance in terms of video quality.

\para{DNN performance of video recovery: } Figure~\ref{fig:rc_dnn_quality} compares the video quality of simply reusing the previous video frame, predicting the video frame without the binary point code, and predicting using our binary point code. We use these schemes to predict the next 5, 10, 20, and 50 frames and calculate the average video quality. As we can see, video recovery without the binary point code yields 4-9dB PSNR improvement and 0.03-0.13 SSIM improvement over simple frame reuse; and the binary point code further increases PSNR by 6-12dB and increases SSIM by 0.04-0.17. The result shows the effectiveness of our binary point code. As we increase the number of future frames to predict, the prediction quality  using our recovery model degrades gracefully. 
% \zhaoyuan{Additionally, we evaluate the performance on the Macbook Air, which conducts warping at 1080p resolution. As a result, it shows superior performance compared to the iPhone 12, with an improvement of XXdB in PSNR and XX in SSIM.}

Figure~\ref{fig:vis_recovery} further illustrates the visualization of our video recovery results. As we can see, our recovery model can learn the motion movement between two consecutive frames and the recovered frames can closely resemble the ground truth frames. In addition, there is often a large difference between the previews frame and the current frame, and it can also be seen in this visualization that our model generates very reasonable predictions in regions where no reference can be found.

% Figure environment removed

Figure~\ref{fig:rc_cl_dnn_quality} further shows the partial video recovery results. We receive and decode video frames in a WiFi network environment. In this setting, many frames can only be partially decoded and our recovery model can recover these corrupted frames. We fill the decoded part of the frame into the recovered frame for all of the schemes such that the overall video quality is higher than the whole frame prediction. As we can see, our recovery without the binary point code yields 0.6-5dB PSNR improvement and 0.01-0.04 SSIM improvement over reusing the previous frame. The binary point code further increases PSNR by 4-8.5dB and increases SSIM by 0.04-0.06, respectively. 

Moreover, the gap between our video recovery without the binary point code and reusing the previous frame becomes larger because  $I_{part}$ allows the network to get an accurate hint to infer the content of the current frame. Similarly, the gap between the performance of our recovery with the binary point code and the other algorithms increases a lot compared to Figure~\ref{fig:rc_dnn_quality} likely because the model learns a stronger association between RGB frame content and binary point code in the successfully decoded part and better utilizes the learned binary code to generate predictions in the missing part.

Figure~\ref{fig:vis_conceal} plots the visualization of our error concealment results (\ie, recovery from partially corrupted frames). The recovered frames are also very similar to the original video frames. These results demonstrate the effectiveness of our video recovery for both completely lost or partially corrupted frames. 

% Figure environment removed

\para{DNN Performance of video super-resolution: } Figure~\ref{fig:sr_dnn_quality} compares the performance of our video super-resolution with upsampling. As we can see, our SR improves the PSNR and SSIM by 1.2dB, 1.1dB, 1dB, and 1.3dB; 0.015, 0.01, 0.007, and 0.008 at 240p, 360p, 480p, and 720p, respectively. The lower resolution video frames yield a higher improvement, as expected. Figure~\ref{fig:vis_sr} plots the visualization of video super-resolution results. Our proposed super-resolution algorithm delivers stable video frame quality improvement at all resolutions. % This confirms that our proposed super-resolution algorithm, which can run in real time on a mobile device, can consistently support the ABR algorithm to make better video streaming strategies. % this doesn't use ABR yet



% Figure environment removed

% Figure environment removed

\subsection{System Performance}

In this section, we evaluate the system performance in terms of video QoE. Note that we downscale the throughput for all network traces so that their throughput falls into the range between the highest and lowest video bit rates.  %  because adaptive streaming does not deliver any benefits at very high throughput. 
The average downscaled throughput among all the network traces is around 1-2Mbps. % To evaluate the performance under a lossy network environment, we use tc-netem as a Linux tool to impose packet loss on the network traffic using a 2-state Gilbert loss model, where the loss probability of the next packet depends on the previous state. % To emulate packet burst losses, we use the probability model, $Prob_{n} = p * Prob_{n-1} + (1 - p) * Random$, where each successive probability depends on a probability $p$ on the last one.

% \subsubsection{Video Recovery}\mbox{}\\

\para{QoE performance of video recovery: }To evaluate the QoE performance of video recovery, we consider three schemes: (i) without recovery model, (ii) without recovery-aware ABR, and (iii) our approach. Note that (ii) means we still perform video recovery for lost or late frames but select the bitrate without taking into account the benefits and cost of video recovery.  

Figure~\ref{fig:rc_only_qoe} shows the QoE performance of recovery-only schemes across different network traces. We make the following observations. First, video recovery alone improves the average QoE by 6.3\%, 11.2\%, 14.2\%, and 9.6\% in 3G, 4G, 5G, and WiFi, respectively, because it can recover lost and late frames such that the rebuffering time can be effectively reduced. 

Second, our recovery-aware algorithm improves over without recovery by 8.6\%, 18.3\%, 22.8\%, and 14.5\% in 3G, 4G, 5G, and WiFi, respectively, and improves over recovery alone by 2.2\%, 6.4\%, 7.5\%, and 4.5\% in 3G, 4G, 5G, and WiFi, respectively because it is aware of the usage of recovery for the next frames such that the bitrate can be chosen more wisely to maximize the system QoE. 

Third, comparing the results across different types of networks, we observe 5G enjoys the largest improvement because more frames require video recovery as we will show next.

% Figure environment removed 

Figure~\ref{fig:throughput} shows the average downscaled throughput of different network traces. We see a large fluctuation in 5G traces. Figure~\ref{fig:rc_percentage} reports the percentage of recovered frames. As 5G has the largest throughput fluctuation, many video frames are not received in time and require video recovery. Meanwhile, even 4G and WiFi see close to 10\% or more video frames that require video recovery. These numbers suggest that video recovery is important due to challenging network conditions. Figure~\ref{fig:sample_thrp} further shows a sample time series of throughput. We find that the scheme without recovery cannot sustain a good QoE when the throughput varies a lot. Recovery alone has a more stable QoE but sometimes gets below 0 due to the rebuffering overhead. Our approach always chooses the bitrate that yields the best QoE. 

% Figure environment removed

Table~\ref{tab:qoe_rc_frames} reports the average QoE of the recovered video frames only. Video recovery alone improves the QoE for the recovered frames by 1.26 - 10.65. The improvement comes mostly from reduced rebuffering time. 
% [XXX: double check; add break down of the improvement from 3 terms in QoE] 
Incorporating recovery-aware ABR further increases the QoE by 0.25 - 1.4. 
% [XXX: do we select a higher or lower rate in RC-aware ABR]

\para{QoE performance without FEC under lossy networks: } Figure~\ref{fig:rc_lossy_qoe} shows the QoE performance of recovery-only schemes across different network traces. Under this setting, we do not enable FEC for loss recovery. For (i), we reuse the last frame when a video frame is late or lost. For (ii) and (iii), our recovery model recovers both lost frames and late frames. Under a lossy network environment, we observe that video recovery alone improves the average QoE by 58.9\%, 74.3\%, 82.7\%, and 70.6\% in 3G, 4G, 5G, and WiFi, respectively. Our approach improves over that without recovery by 71.8\%, 90.8\%, 110\%, and 84.3\% in 3G, 4G, 5G, and WiFi, respectively, and improves over recovery alone by 8.1\%, 9.5\%, 14.6\%, and 8\% in 3G, 4G, 5G, and WiFi, respectively. We find that the improvement of our approach over baselines increases a lot under the lossy network environment because reusing the previous frames is not effective under many consecutive lost/late frames (as shown in Figure~\ref{fig:rc_dnn_quality}), which is more likely under a lossy environment. However, our recovery model can recover many frames with little degradation so its QoE performance is much better. 

% Figure environment removed

\para{QoE performance with FEC under lossy networks: } So far, we disable FEC in our algorithm. Next we further jointly optimize FEC and video recovery. Figure~\ref{fig:rc_fec_lossy_qoe} compares our algorithm but disable FEC (w/o FEC) with all other schemes with FEC, where the amount of FEC is determined based on our lookup table. We offline build separate lookup tables that map the packet loss rates to desired FEC levels for different schemes. Our joint optimization yields 51\%, 68\%, 83\%, and 72\% improvement over no recovery in 3G, 4G, 5G, and WiFi, respectively. The corresponding improvements over recovery alone are 13\%, 41\%, 48\%, and 31\%, respectively. Also, it outperforms no FEC by 1.2, 1.15, 1.3, and 1.23 in QoE, respectively. These results show that (i) FEC plays an important role under lossy network conditions, (ii) the desired amount of FEC depends on the recovery and ABR algorithms, and (iii) each component in our recovery model (\ie, recovery alone, recovery-aware, and joint optimization of FEC and recovery) is beneficial. 

% Without FEC performs worst and acquires negative average QoEs under different network traces. 

% We further adaptively Our recovery model can still get benefits under lossy networks because we can recover the lost frames without retransmission to avoid much rebuffering overhead. Figure~\ref{fig:qoe_redundant} indicates that there is always a peek where we can get the best QoE with the combination of FEC and our recovery model. To this end, we build a lookup table to get the best FEC redundant ratio under different packet loss rates and different network traces. Figure~\ref{fig:rc_lossy_qoe} shows the QoE performance across different network traces. 

% \subsubsection{Video Super-Resolution}\mbox{}\\

\para{QoE performance of video super-resolution: }Figure~\ref{fig:sr_only_qoe} compares the QoE of our super-resolution with (i) without SR, (ii) SR alone using our model, and (iii) NEMO~\cite{yeo2020nemo}.  Our SR-aware approach significantly outperforms all the other algorithms. Its improvement over (i), (ii), and (iii) are 18\%, 21\%, 22\%, and 19\%; 4.5\%, 6.5\%, 7.1\%, and 4.5\%; 0.7\%, 3.8\%, 4.5\%, and 2.7\% in 3G, 4G, 5G, and WiFi, respectively. SR alone brings 12\%-14\% improvement, and SR-aware ABR further brings 4\%-7\% improvement, which shows the importance of joint design of ABR and SR. % It is interesting that the improvement of SR-aware ABR is so large. [XXX: zoom in what video rates are selected. do we select higher or lower rates?] Our approach out-performs NEMO, the state of the art because XXX. 

% Figure environment removed

% \subsubsection{Video Recovery and Super-Resolution}\mbox{}\\

\para{QoE performance of video recovery and super-resolution: } Figure~\ref{fig:sr_rc_qoe} compares the QoE of (i) without SR or recovery, (ii) SR and recovery alone, (iii) NEMO, and (iv) our final algorithm. Our algorithm out-performs (i), (ii), and (iii) by 23.7\%, 32.2\%, 37.1\%, and 29\%; 5.9\%, 10\%, 11.9\%, and 8.4\%; 4.7\%, 13.2\%, 17.4\%, and 10.9\% in 3G, 4G, 5G, and WiFi, respectively. It can be found that both SR and Recovery play a significant effect, and combined with our enhancement aware ABR strategy, our method achieves the best performance. It out-performs NEMO by 4.7\%-17.4\%, and even SR and Recovery alone out-perform NEMO in all cases except 3G, because NEMO does not have recovery and has to reuse the previous frames for late or lost video frames. 3G is better for NEMO due to fewer lost/late video frames.

\zhaoyuan{
\subsection{System Latency and Resource Usage}

\para{System latency: } At the start of video streaming, we establish TCP and QUIC transmission sessions. The binary code, with a size of 1KB, can be encapsulated into a single TCP packet. Consequently, the TCP latency for each frame is expected to be approximately equivalent to the round-trip time (RTT). Given that the decoding and model inference processes of a frame can occur simultaneously with the receiving process of subsequent frames, the total latency can be viewed as the sum of the decoding time and the duration required for neural enhancement or recovery. The decoding time of 240p, 360p, 480p, 720p, and 1080p videos is 1.8, 2.3, 2.9, 4.1, and 6.2ms on the iPhone 12, respectively. Our model adds an additional 22ms for both enhancement and recovery, regardless of the video resolution. This results in a total latency of under 33 ms, demonstrating real-time processing capability in our system.

\para{CPU usage and energy consumption: } We also measure the CPU utilization and energy consumption with and without our model. We only evaluate the neural video recovery because both video recovery and enhancement share a similar model structure and exhibit identical inference time. This similarity implies comparable CPU usage and energy consumption between the two models. Without DNN processing, iPhone 12's CPU utilization is 28\% and the energy consumption is 0.04J per frame. Under 20\% frame losses, the corresponding numbers are 37\% and 0.05J, and under 100\% frame losses, they are 68\% and 0.07J. Consequently, with each frame undergoing neural recovery or enhancement, the expected battery life decreases from 13.2 hours to 7.5 hours.

}
