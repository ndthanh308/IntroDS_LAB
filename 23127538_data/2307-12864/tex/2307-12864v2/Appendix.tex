\subsection{Theoretical Results for Lossy Conditional Coding}
In the following we show that $\RateCond(\dist) \leq \RateRes(\dist)$. To compare the two rate-distortion functions, we start with Bayes' law:
\begin{equation}
\begin{split}
H(\resD,\recresD) &+ H(\curD,\recD,\predD|\resD,\recresD) \\=& H(\curD,\recD,\predD) + \underbrace{H(\resD,\recresD|\curD,\recD,\predD)}_{=0}
\end{split}
\end{equation}
The last term is equal to zero because together with the prediction signal $\predV$, $\curV$ and $\recV$ completely determine $\resV$ and $\recresV$.
We can further rearrange the equation to obtain
\begin{subequations}
	\begin{align}
	H(\resD,\recresD)=&H(\curD,\recD,\predD) - H(\curD,\recD,\predD|\resD,\recresD)\\
	=&H(\curD,\recD|\predD) + H(\predD) \nonumber \\ &-\underbrace{H(\curD,\recD|\resD,\recresD,\predD)}_{=0}-\underbrace{H(\predD|\resD,\recresD)}_{=H(\predD|\resD)}\\
	=&H(\curD,\recD|\predD) + I(\predD;\resD)
	\end{align}
\end{subequations}

With this equation, we can now connect $H(\resD,\recresD)$ and
$H(\curD,\recD|\predD)$. We continue with:
\begin{equation}
\begin{split}
H(\resD,\recresD) &= I(\resD;\recresD) + H(\resD|\recresD) + \underbrace{H(\recresD|\resD)}_{=0} \\
&=I(\curD;\recD|\predD) + H(\curD|\recD,\predD) \\&~~~+ \underbrace{H(\recD|\curD,\predD)}_{=0} + I(\predD;\resD)
\end{split}
\end{equation}
We can therefore write
\begin{subequations}
	\begin{align}
	I(\resD;\recresD)=&I(\curD;\recD|\predD)+I(\predD;\resD)\nonumber\\&+H(\curD|\recD,\predD)-H(\resD|\recresD)\\
	=&I(\curD;\recD|\predD) + I(\predD;\resD) \nonumber \\&+ H(\resD|\recD,\predD,\recresD) - H(\resD|\recresD)\label{Eq:FinalRDCurveA}\\
	=&I(\curD;\recD|\predD) + I(\predD;\resD) - I(\resD;\recD,\predD|\recresD)\\
	=&I(\curD;\recD|\predD) + I(\predD;\resD) - I(\resD;\predD|\recresD)\label{Eq:FinalRDCurve}
	\end{align}
\end{subequations}
In (\ref{Eq:FinalRDCurveA}), we exploit the identity $H(\curD|\recD,\predD)=H(\resD|\recD,\predD)=H(\resD|\recD,\predD,\recresD)$, which follows from the fact that $\curV$ and $\resV$ carry the same information when $\predV$ is known and that $\recresV$ is known when $\recV$ and $\predV$ are known. In (\ref{Eq:FinalRDCurve}), we exploit that when $\recresV$ is known, knowing $\predV$ also means knowing $\recV$. Therefore $\recV$ can not contribute to the mutual information. We can now plug in the result in the rate distortion function~(\ref{Eq:RDEqResidual}) and obtain:
\begin{subequations}
	\begin{align}
	\RateRes&(\dist) = \!\!\!\!\min_{p\in\PSet(\dist)}\big(I(\curD;\recD|\predD) + I(\predD;\resD) - I(\resD;\predD|\recresD)\big)\\
	\geq& I(\predD;\resD) + \!\!\!\!\min_{p\in\PSet(\dist)}I(\curD;\recD|\predD) +  \!\!\!\!\min_{p\in\PSet(\dist)} - I(\resD;\predD|\recresD)\label{Eq:PullOutMutualInformation}\\
	=&\RateCond(\dist)\!+\!I(\predD;\resD)\!-\!\!\!\max_{p\in\PSet(\dist)} I(\resD;\predD|\recresD)\\\geq&\RateCond(\dist)\label{Eq:FinalInequality}
	\end{align}
\end{subequations}
In (\ref{Eq:PullOutMutualInformation}), we use the fact that the marginal $p_\mathrm{m}(\curV,\predV)$ of all distributions in $\PSet(\dist)$ is constant for a given scenario. Since $I(\predD;\resD)$ is fully described by this marginal, we can exclude this term from the minimization operation. Since $I(\predD;\resD) \geq I(\resD;\predD|\recresD)$ holds in general, also $I(\predD;\resD) \geq \max_{p\in\PSet(\dist)} I(\resD;\predD|\recresD)$ holds, thus proving the inequality  in (\ref{Eq:FinalInequality}), which proves (\ref{Eq:MainLossyX}).