
\section{Introduction}

Synthetic control methods are popular for estimating the treatment effect of an intervention in settings where a % time series data of multiple study units over the pre- and post-treatment periods where one study 
single unit is treated and pre- and post-treatment time series data are available on the treated unit and a heterogeneous pool of untreated control units %while pre and post study units are untreated 
\citep{Abadie2003, Abadie2010}. In the absence of a natural control unit, the main idea of the approach hinges on constructing a so-called synthetic control, corresponding to a certain weighted average of control units' outcomes (and potentially covariates), obtained by matching the outcome time series of the treated unit to the weighted average in the pre-intervention period, to the extent empirically feasible. The resulting synthetic control is then used to forecast the treatment-free potential outcome of the treated unit in the post-treatment period, therefore delivering an estimate of the treatment effect by comparing the treated unit’s outcome to the synthetic control forecast. 

There is fast-growing literature concerned on various approaches to construct synthetic control weights. Following \citet{Abadie2010} and subsequent works, the most common approach is to use ordinary (or weighted) least squares by regressing the pre-treatment outcome and available covariates of the treated unit on those of control units, typically restricting the weights to nonnegative and sum to one; see Section \ref{sec:Review} for a more detailed discussion. Despite intuitive appeal and simplicity, the performance of the standard synthetic control approach may break down in settings where the pre-treatment synthetic control match to the treated unit's outcomes is not nearly perfect; an eventuality \citet{Abadie2010} warns against. In order to improve the performance of the synthetic control approach in the event of an imperfect pre-treatment match, recent papers have considered alternative formulations of the synthetic control framework. For example, \citet{GSC2017, Amjad2018, ASCM2021, FermanPinto2021, Ferman2021, Shi2023SC} rely on variants of a so-called interactive fixed effects model \citep{Bai2009}. In particular, the latter three papers specify a linear latent factor potential outcome model with an exogenous, common set of latent factors with corresponding unit-specific factor loadings. Under this linear factor model, a key identification condition is that the factor loading of the treated unit lies in the vector space spanned by factor loadings of donor units and, thus, there exists a linear combination of the latter that matches the former exactly. Using the corresponding matching weights, one can therefore construct an unbiased synthetic control of the treated unit's potential outcome which, under certain conditions, can be used to mimic the treated unit's outcome in the post-treatment period, had the intervention been withheld. At their core, these methods substitute the requirement of a perfect pre-treatment match of the outcome of the treated unit and the synthetic control (an empirically testable assumption) with finding a match for the treated unit's factor loadings in the linear span of the donors' factor loadings (an empirically untestable assumption). Despite the growing interest in synthetic control methods, limited research has considered synthetic control methodology outside of the interactive fixed effects model or its nonparametric generalizations \citep{Qiu2022, Shi2023SC}; one notable exception is \citet{Blei2022} where the units' outcomes are viewed as averages of more granular study units, allowing for construction of a synthetic control under specific restrictions on the model of granular study units' outcomes.




% The interactive fixed effects model is popular in many fields such as those in economics due to its compatibility with the context; we refer the readers to Section 2 of \citet{Bai2009} for details. In particular, in the context of the synthetic control framework, the interactive fixed effects model requires the presence of time-specific latent factors that are predictive of both treated and control units' outcomes and unit-specific factor loadings that are constant over time for each unit. Thus, the outcomes can be viewed as proxies of interactive effects, which are linear combinations of the latent factors weighted by the units' factor loading, subject to measurement error. Therefore, confounding between the treated unit's outcome and donors' outcomes arises to the extent that the factor loadings of the treated unit differ from those of donor units

% Then, interactive effects, which are linear combinations of the latent factors weighted by the units' factor loading, are used as key objects to model the data generating mechanism in the synthetic control framework. Specifically, donor units' outcomes can be viewed as proxies of their interactive effects subject to measurement error. The treated unit's outcome is modeled in a similar manner, with the additional consideration of the treatment effect over the post-treatment period; see equation \eqref{eq-IFEM}. In addition, the treatment status of the treated unit at each time point depends on the interactive effects through the latent factors (which are time-specific). Combined, the interactive effects are unmeasured confounders of the causal relationship between treatment and the treated unit's outcome where confounding arises to the extent that the factor loadings of the treated unit differ from those of untreated donor units. 

% Moreover, the treatment and the treatment-free potential outcomes are mean-independent conditional on the latent factors and the factor loadings. Moreover, the treatment and the treatment-free potential outcomes are mean-independent conditional on the latent factors. In other words, the treatment and the treatment-free potential outcomes are unconfounded conditioning on the latent factors, implying that the latent factors are considered as the main source of the unmeasured confounding. Therefore, these latent factors are the key to relating the treated and control units’ treatment-free potential outcomes and to remove the confounding bias.

In this work, we consider an alternative theoretical framework to formalize the synthetic control approach which obviates specification of an interactive fixed effects model. Specifically, we propose to view the synthetic control model from a measurement error model perspective, whereby donor units' outcomes stand as error-prone proxy measurements of the treated unit's treatment-free potential outcome. In this framework, a synthetic control outcome can be obtained via a simple form of calibration, say a linear combination of donor units, so that on average, it matches the treated unit's outcome in the pre-treatment period. Whereas the standard interactive fixed effects model views the treated and control units' outcomes as proxies of latent factors, our approach views donor units' outcomes as direct proxies of the treated unit's treatment-free potential outcome. Thus, the proposed framework shares similarity with the recent proximal synthetic control framework of \citet{Shi2023SC}, which also formalizes donor outcomes as so-called outcome proxies. However, a major distinction is that the latter requires an additional group of proxies (so-called treatment proxies) to identify synthetic control weights; in contrast, our proposed approach solely relies on a single type of proxies, given by donor units and obviates the need to evoke existence of latent factors. 

Interestingly, similar to the connection between the proximal synthetic control approach of \citet{Shi2023SC} and proximal causal inference for independent and identically distributed (i.i.d.) data \citep{Miao2018, TT2020_Intro}, the proposed synthetic control framework is likewise inspired by the control outcome calibration approach \citep{TT2013_COCA} and its recent generalization to a so-called single proxy control framework \citep{TTPR2023} both of which were proposed for i.i.d. samples subject to a stochastic treatment assignment mechanism, albeit endogenous as confounded by hidden factors predictive of outcome. Therefore, we aptly refer to our approach as single proxy synthetic control (SPSC) approach. Despite this connection, the synthetic control generalization presents several new challenges related to (i) only observing a single treated unit, and therefore treatment assignment is implicitly conditioned on, and (ii) having access to pre-and post-treatment time series data for a heterogeneous pool of untreated donor units, none of which can serve as a natural control; and (iii) serial correlation and heteroskedasticity due to the time series nature of the data. We tackle each of challenges (i)-(iii) in turn and develop a general formal framework for single proxy control in a synthetic control setting. 



% The rest of the paper is organized as follows. In Section \ref{sec:Setup}, we introduce notations and review synthetic control methods. In Section \ref{sec:SPSC}, we establish the identification of a synthetic control under the SPSC framework. In turn, we provide the estimation strategy for the average treatment effect on the treated using the generalized method of moments. Moreover, we apply a recent development in the conformal inference approach to our framework to construct pointwise confidence intervals of the treatment effect. As an extension, we extend our framework to nonparametric settings and discuss how to incorporate covariates in the framework. In Sections \ref{sec:Sim} and \ref{sec:Data}, we perform simulation studies investigating the performance of the proposed method, and illustrate the method by revisiting a financial data regarding the 1907 Panic, respectively. Lastly, in Section \ref{sec:Conclusion}, we provide concluding remarks. More detailed discussions and proofs are relegated to the Supplementary Material.





\section{Setup And Review of Existing Synthetic Control Framework}	\label{sec:Setup}

\subsection{Setup}

Let us consider a setting where $N+1$ units are observed over $T$ time periods. Units and time periods are indexed by $i=0,1,\ldots,N$ and $t=1,\ldots,T$, respectively. Following the standard synthetic control setting, we suppose that only the first unit with index $i=0$ is treated whereas the latter $N$ units with index $i=1,\ldots,N$ are untreated control units. Consider a binary treatment indicator $A_{t}$ which encodes whether time $t$ is in the pre-treatment period, in which case $A_{t}=0$ for $t=1,\ldots,T_0$, or the post-treatment period, in which case $A_{t}=1$ for $t=T_0+1,\ldots,T$, respectively. Thus, $T_0$ is the number of pre-treatment periods and $T_1=T - T_0$ is the number of post-treatment periods. We assume that $N$ is fixed and $T_0$ and $T_1$ are large with similar order of magnitude. Let $Y_{t}$ and $W_{it}$ denote observed outcomes of the treated unit and the $i$th control unit, respectively, for $i=1,\ldots,N$. We define $\bW_t = (W_{1t},\ldots,W_{Nt} )\T \in \R^N$ as the $N$-dimensional vector of the untreated units' outcome at time $t$. For a set $\D = \{ \D_1,\ldots,\D_d \} \subseteq \{ 1,\ldots,N \}$, we define $\bW_{\D t} = ( W_{\D_1 t} , \ldots, W_{\D_d t} )\T$ as a $d$-dimensional subvector of $\bW_t$. We define $\bO_t = (Y_t,\bW_t\T,A_t)$ as the observed data at time $t$. Let $\potY{t}{a}$ and $\potW{it}{a}$ denote the potential outcomes of the treated and $i$th control units, respectively, which one would have observed had, possibly contrary to fact, the treatment been set to $A_t=a$ at time $t$. 

We introduce additional notation used throughout. Let $\ind(\mathcal{A})$ be the indicator function of an event $\mathcal{A}$, i.e., $\ind(\mathcal{A})=1$ if $\mathcal{A}$ is satisfied and $\ind(\mathcal{A})=0$ otherwise. Let $\R$ be the set of real numbers. Let $V_1 \indep V_2 \cond V_3$ denote that $V_1$ and $V_2$ are conditionally independent given $V_3$. Conversely, we use $V_1 \nindep V_2 \cond V_3$ to denote that $V_1$ and $V_2$ are conditionally dependent given $V_3$.

\subsection{Review of Existing Synthetic Control Framework}		\label{sec:Review}

The common target estimand in the synthetic control setting is the average treatment effect on the treated unit (ATT) at time $t$ in the post-treatment periods, i.e., 
\begin{align*}
\tau_t^* 
=
\EXP \big\{ \potY{t}{1} - \potY{t}{0} \big\}
\ , 
\quad \quad
t=T_0+1,\ldots,T \ . 
\end{align*}
Note that, by definition, $\potY{t}{1} - \potY{t}{0} = \tau_t^* + \epsilon_t$ for $t=T_0+1,\ldots,T$ where $\epsilon_t$ is a mean-zero idiosyncratic residual error and, therefore, $\tau_t^*$ may be viewed as a deterministic function of time capturing the expected effect of the treatment experienced by the treated unit. In Section \ref{sec:Conformal}, we describe an approach for constructing prediction intervals for $\potY{t}{1}-\potY{t}{0}$ by appropriately accounting for the idiosyncratic error term $\epsilon_t$. In order to make progress, we make the consistency assumption:
\begin{assumption}[Consistency]	\label{assumption:consistency}
$Y_t = \potY{t}{A_t}$ almost surely and $W_{it} = \potW{it}{A_t}$ almost surely for all $i=1,\ldots,N$ and $t=1,\ldots,T$.
\end{assumption}
\noindent Additionally, we assume no interference, i.e., the treatment has no causal effect on control units.
\begin{assumption}[No Interference on Control Units]
\label{assumption:noitf}
$\potW{it}{0}=\potW{it}{1}$ almost surely for all $i=1,\ldots,N$ and $t=1,\ldots,T$.
\end{assumption}
\noindent 
Under Assumptions \ref{assumption:consistency} and \ref{assumption:noitf}, we have the following result almost surely:
\begin{align*}
Y_t 
&
=
\potY{t}{0} (1-A_t)
+
\potY{t}{1} A_t
\
, 
\quad \quad
W_{it} 
=
\potW{it}{0}
=
\potW{it}{1}
\ , 
\quad \quad
i=1,\ldots,N
\ , 
\quad \quad
t=1,\ldots,T \ .
\end{align*}
Therefore, for the post-treatment period, $\potY{t}{1}$ matches the observed outcome $Y_t$ while $\potY{t}{0}$ is unobserved. Therefore, an additional set of assumptions is required to establish identification of the ATT. 

In the classical synthetic control setting, such assumptions are imposed by relating the observed outcomes of the untreated units with the treatment-free potential outcome of the treated unit. Specifically, following \citet{Abadie2010} and \citet{FermanPinto2021}, suppose that units' outcomes are generated from the following interactive fixed effects model \citep{Bai2009}:
\begin{align}	\label{eq-IFEM}
Y_{t}
&
=
\text{\makebox[1.25cm]{$ \tau_{t}^* A_t + $}}
\bmu_{0} \T
\blambda_t
+
e_{0t}
\ ,
&&
\EXP \big( e_{0t} \cond \blambda_t ) = 0
\nonumber
\\
W_{it}
&
=
\text{\makebox[1.25cm]{}}
\bmu_{i} \T
\blambda_t
+
e_{it}
\ , 
&&
\EXP \big( e_{it} \cond \blambda_t ) = 0
\ , 
&&
i=1,\ldots,N \ ,
&&
t=1,\ldots,T \ .
\end{align}
Here, $\tau_t^*$ is the fixed treatment effect at time $t$, $\blambda_t \in \R^r$ is a random $r$-dimensional vector of latent factors which are known a priori to potentially causally impact the treated and donor units, despite being unobserved, and may exhibit either stationary or nonstationary behavior over time, $\bmu_i \in \R^r$ is a fixed $r$-dimensional vector of unit-specific factor loadings, and $e_{it}$ is a random error. It is typically assumed that the number of latent factors $r$ is no larger than the number of donor units $N$ and the pre-treatment periods $T_0$. 
% Under the interactive fixed effect model \eqref{eq-IFEM}, control units' outcomes can be viewed as proxies of interactive effects, $\bmu_i\T \blambda_t$, subject to measurement error $\epsilon_{it}$. The treated unit's outcome is modeled in a similar manner, with the additional consideration of the treatment effect over the post-treatment period. In addition, the treatment status of the treated unit at each time point depends on the interactive effects $\bmu_i\T \blambda_t$ through the latent factors $\blambda_t$. Therefore, interactive effects are unmeasured confounders of the causal relationship between treatment and the treated unit's outcome where confounding arises to the extent that the factor loadings of the treated unit differ from those of untreated donor units, i.e., $\bmu_i \neq \bmu_0$ for $i=1,\ldots,N$. 
Combined with Assumptions \ref{assumption:consistency} and \ref{assumption:noitf}, the interactive fixed effects model \eqref{eq-IFEM} implies $\potY{t}{0}
=
\bmu_{0} \T
\blambda_t
+
e_{0t}$ and $\potY{t}{1}
=
\tau_{t}^* A_t
+
\potY{t}{0}$ where the ATT is represented as $\tau_t^* = \potY{t}{1}-\potY{t}{0}$ for $t=T_0+1,\ldots,T$. In addition, if there exists a donor whose factor loading is equal to that of the treated unit, i.e., $\mu_i=\mu_0$ for some $i=1,\ldots,N$, then $W_{it}$ is unbiased for $\potY{t}{0}$ and, therefore, $\potY{t}{1} - W_{it}$ is unbiased for the ATT. This indicates that confounding bias of the treatment effect on the treated unit's outcome arises to the extent that donors' factor loadings are different from those of the treated unit.

% Then, interactive effects, which are linear combinations of the latent factors weighted by the units' factor loading, are used as key objects to model the data generating mechanism in the synthetic control framework. Specifically, donor units' outcomes can be viewed as proxies of their interactive effects subject to measurement error. The treated unit's outcome is modeled in a similar manner, with the additional consideration of the treatment effect over the post-treatment period; see equation \eqref{eq-IFEM}. In addition, the treatment status of the treated unit at each time point depends on the interactive effects through the latent factors (which are time-specific). Combined, the interactive effects are unmeasured confounders of the causal relationship between treatment and the treated unit's outcome where confounding arises to the extent that the factor loadings of the treated unit differ from those of untreated donor units, i.e., $\bmu_i \neq \bmu_0$ for $i=1,\ldots,N$.

Next, following \citet{FermanPinto2021} and \citet{Shi2023SC}, suppose that there exist a set of control units, denoted by $\D=\{\D_1,\ldots,\D_d \} \subseteq \{1,\ldots,N \}$ and a set of weights $\bgamma^{\dagger} = ( \gamma_{\D_1}^{\dagger},\ldots,\gamma_{\D_d}^{\dagger} ) \T$ satisfying
\begin{align}		\label{eq-ExistSC}
\bmu_0
=
\sum_{i \in \D} \gamma_{i}^{\dagger} \bmu_i \ .
\end{align}
Equations \eqref{eq-IFEM} and \eqref{eq-ExistSC} imply that there exists a synthetic control $ \bW_{\D t}\T \bgamma^\dagger = \sum_{i \in \D} \gamma_i^\dagger W_{it}$ satisfying 
\begin{align}		\label{eq-SC Equation}
\potY{t}{0}
=
\bW_{\D t} \T \bgamma^\dagger
+ 
e_{0t} 
-
\sum_{i \in \D} \gamma_i^{\dagger} e_{it} 
\ , 
\quad \quad
t=1,\ldots, T
\ .
\end{align}
Therefore, $	\tau_t^*
=
\EXP \big\{ \potY{t}{1} - \bW_{\D t} \T \bgamma^\dagger \big\}$ for $t=T_0+1,\ldots,T$, i.e., $Y_t - \bW_{\D t}\T \bgamma^\dagger$ is unbiased for the ATT. Unfortunately, it is infeasible to directly estimate $\bgamma^{\dagger}$ from equation \eqref{eq-ExistSC} because the factor loadings $\bmu_i$ are unknown. Importantly, the synthetic control weights satisfying \eqref{eq-ExistSC} naturally accommodate an imperfect pre-treatment fit as shown in \eqref{eq-SC Equation}, i.e., the synthetic control can significantly deviate from the observed pre-treatment fit, however, the corresponding error is equal to zero in expectation. 

Based on \eqref{eq-SC Equation}, one can consider estimating the weights $\bgamma$ via penalized least squares minimization, say:
\begin{align}
\label{eq-OLS}
\widehat{\bgamma}_{\text{P-OLS}}
=
\argmin_{\bgamma}
\bigg\{
\frac{1}{T_0}
\sum_{t=1}^{T_0}
\big(
Y_t - \bW_{\D t}\T \bgamma
\big)^2
+
\mathcal{R}( \bgamma )
\bigg\}
\end{align}
where $\mathcal{R} (\bgamma)$ is a penalty that places restrictions on $\bgamma$. For instance, \citet{Abadie2010} restricts the weight to be non-negative and sum up to one, \citet{Doudchenko2016} uses elastic-net penalization, and \citet{Robbins2017} uses entropy penalization. In words, $\widehat{\bgamma}_{\text{P-OLS}}$ is obtained by fitting a possibly constrained ordinary least squares (OLS) regression of $Y_t$ on $W_{it}$. In particular, without penalization, the moment restriction solving \eqref{eq-OLS} reduces to $ \EXP \{ \Psi_{\text{OLS}} (\bO_t \con \bgamma) \} = 0$ for $t = 1, \ldots, T_0$ 
% \begin{align*}
% \EXP \big\{ \Psi_{\text{OLS}} (\bO_t \con \bgamma) \big\} = 0 \ , \ t = 1, \ldots, T_0 \ , 
% \end{align*}
where $\Psi_{\text{OLS}} (\bO_t \con \bgamma)
=
\bW_{\D t}
\big( Y_t - \bW_{\D t}\T \bgamma \big)$ are standard least squares normal equations. 

However, as discussed in \citet{FermanPinto2021} and \citet{Shi2023SC}, the OLS weights from \eqref{eq-OLS} are generally inconsistent as $T_0$ goes to infinity, which can result in biased estimation of the treatment effect unless $e_{it}$ is exactly zero for all $i$ and $t$; see Section \ref{sec:supp:OLS} of the Supplementary Material for details. We remark that this result does not conflict with \citet{Abadie2010} because their synthetic control weights are assumed to satisfy a perfect pre-treatment fit; specifically, there exist values $\bgamma^{\#} = (\gamma_{\D_1}^{\#},\ldots,\gamma_{\D_d}^{\#}) \T$ satisfying
\begin{align}		\label{eq-ExistSC-Abadie}
\potY{t}{0}
=
\bW_{\D t}\T \bgamma^{\#}
\ , 
\quad \quad 
t=1,\ldots,T_0 \ ,
\end{align}
which is distinct from condition \eqref{eq-ExistSC} of  \citet{FermanPinto2021} and \citet{Shi2023SC}. Moreover, as discussed in \citet{FermanPinto2021}, \eqref{eq-ExistSC-Abadie} can be expected to hold approximately under \eqref{eq-ExistSC} when the variance of the error $e_{it}$ in \eqref{eq-IFEM} becomes negligible as $T_0$ becomes large; see \citet{Abadie2010} for related results, and Sections 1 and 3.1 of \citet{FermanPinto2021}, and Section 2 \citet{Shi2023SC} for detailed discussions.

Recently, \cite{Shi2023SC} introduced a proximal causal inference framework for synthetic controls. Specifically, they assume that they have also observed proxy variables $\bZ_{t} = (Z_{1t},\ldots,Z_{Mt})\T$ a priori known to satisfy the following condition in the pre-treatment periods:
\begin{align}		\label{eq-Shi-proxy}
\bZ_{t} \indep
\big( Y_t, \bW_{\D t} \big)
\cond \blambda_t 
\ , 
\quad \quad
t=1,\ldots,T_0 \ .
\end{align}
A reasonable candidate for $\bZ_{t}$ is the outcome of units excluded from the donor pool, i.e., $\bZ_{t}$ is a collection of outcomes of observed units $\big\{ W_{it} \cond i \in \big\{ 1,\ldots,N \big\} \setminus \big\{ \D_1,\ldots,\D_d \big\} \big\}$; see \citet{Shi2023SC} for alternative choices of proxies. Then, under Assumptions \ref{assumption:consistency} and \ref{assumption:noitf}, the interactive fixed effects model \eqref{eq-IFEM}, Assumption \eqref{eq-ExistSC}, and the existence of proxy as in \eqref{eq-Shi-proxy}, the synthetic control weights $\bgamma^\dagger$ in \eqref{eq-ExistSC} satisfy $\EXP ( Y_t - \bW_{\D t}\T \bgamma^\dagger \cond \bZ_{ t} ) = 0$ and $ \EXP \{ \Psi_{\PI}(\bO_t \con \bgamma^\dagger) \} = 0$ for $t=1,\ldots,T_0$  where $\Psi_{\PI}(\bO_t \con \bgamma) = \bg  ( \bZ_{ t} ) \big( Y_t - \bW_{\D t}\T \bgamma \big)$; here, $\bg$ is a user-specified function of $\bZ_{t}$ with $\text{dim}(\bg) \geq d$. Using the second result as a basis, one can estimate the synthetic control weights as the solution to the generalized method of moments (GMM) \citep{GMM1982}, i.e., $\widehat{\bgamma}_{\PI} 
= (\widehat{\gamma}_{\PI,\D_1}, \ldots, \widehat{\gamma}_{\PI,\D_d}) \T$ is the minimizer of $T_0^{-1} \sum_{t=1}^{T_0}
\big\{ \Psi_{\PI}(\bO_t \con \bgamma) \big\}\T 
\widehat{\Omega}
\big\{ \Psi_{\PI}(\bO_t \con \bgamma) \big\} $ where $\widehat{\Omega}$ is a user-specified symmetric and positive-definite weight matrix. Importantly, in contrast with the OLS-based estimator $\widehat{\bgamma}_{\text{P-OLS}}$ in \eqref{eq-OLS}, the proximal estimator $\widehat{\bgamma}_{\PI}$ is consistent for $\bgamma^\dagger$. Under certain regularity conditions, \citet{Shi2023SC} established that the resulting GMM estimator of the ATT is consistent and asymptotically normal. For instance, in the special case of constant ATT, i.e., $\tau_t^* = \tau^*$ for all $t=T_0+1,\ldots,T$, the estimator $T_1^{-1} \sum_{t=T_0+1}^{T} (Y_t - \bW_{\D t}\T \widehat{\bgamma}_{\PI})$ is consistent for $\tau^*$; see Section 3.2 of \citet{Shi2023SC} for details.


\section{Single Proxy Synthetic Control Approach}		\label{sec:SPSC}

\subsection{Identification of the Synthetic Control and the Treatment Effect}

In this section, we provide a novel synthetic control approach which obviates the need for the interactive fixed effects model and, in fact, does not postulate the existence of a latent factor $\blambda_t$. At its core, the approach views the outcomes of the untreated units $W_{it}$ as proxies for the treatment-free potential outcome of the treated unit $\potY{t}{0}$, which is formally stated as follows:
\begin{assumption}[Proxy] \label{assumption:valid proxy}
There exists a set of control units $\D = \{ \D_1,\ldots,\D_d \} \subseteq \{1,\ldots,N \}$ satisfying
\begin{align}
&
\bW_{i t} \nindep \potY{t}{0} 
\ , \quad \quad
i \in \D
\ , \quad \quad
t=1, \ldots, T_0 \ .
\label{eq-relevant}
\end{align}
\end{assumption}
Condition \eqref{eq-relevant} encodes that the outcome of the untreated units $\bW_{\D t}$ are associated with and, therefore, predictive of $\potY{t}{0}$ at time $t=1,\ldots,T_0$. 
% relevant for predicting the treated unit's treatment-free potential outcome $\potY{t}{0}$. Condition \eqref{eq-confound} states that the outcome of the untreated units $\bW_{\D t}$ are independent of the treatment mechanism $A_t$ upon conditioning on the treated unit's treatment-free potential outcome $\potY{t}{0}$. In other words, $\potY{t}{0}$ is the ultimate source of unmeasured confounding between $\bW_{\D t}$ and $A_t$. Along with Assumption \ref{assumption:noitf}, Assumption \ref{assumption:valid proxy} ensures that $\bW_{\D t}$ is a collection of valid proxies for $\potY{t}{0}$ because, for $i \in \D$, (i) $W_{it} = \potW{it}{0} = \potW{it}{1}$, (ii) $W_{it}$ and $\potY{0}{t}$ are associated, and (iii) $W_{it}$ and $A_t$ are independent upon conditioning on a confounder, which is $\potY{t}{0}$. In particular, (iii) means that the association between $\bW_{\D t}$ and $A_t$ is only present to the extent that $\bW_{\D t}$ is associated with the confounding mechanism represented by $\potY{t}{0}$. 
Additionally, we assume that there exists a set of synthetic control weights which satisfy the following condition:
\begin{assumption}[Existence of Synthetic Control] \label{assumption:SC}
For all $t=1,\ldots,T$, there exists $\bgamma^* = (\gamma_{\D_1}^*,\ldots,\gamma_{\D_d}^*)\T$ satisfying $\potY{t}{0} = \EXP \{
\bW_{\D t}\T \bgamma^*
\cond
\potY{t}{0} \}$ almost surely.
\end{assumption}
\noindent Assumption \ref{assumption:SC} is the key identification assumption of the SPSC framework. It states that a synthetic control $\bW_{it}\T \bgamma^*$ is conditionally unbiased for $\potY{t}{0}$. The assumption essentially implies that $\potY{t}{0}$ falls in the linear span of $\EXP \big\{ \bW_{\D t} \cond \potY{t}{0} \big\}$. The assumption plays an analogous role as condition \eqref{eq-ExistSC} in \citet{FermanPinto2021} and \citet{Shi2023SC} and condition \eqref{eq-ExistSC-Abadie} in \citet{Abadie2010}; however, Assumption \ref{assumption:SC} is fundamentally different from these assumptions because it obviates the need for the existence of either latent factors and corresponding factor loadings, nor of the interactive fixed effects model \eqref{eq-IFEM} or any related latent factor model. Additionally, for the pre-treatment periods, Assumption \ref{assumption:SC} is strictly weaker than condition \eqref{eq-ExistSC-Abadie} as a mean-zero random variable is not necessarily zero. We remark that the weights satisfying Assumption \ref{assumption:SC} may not be unique, i.e., multiple values of $\bgamma^*$ that satisfy the equation.

To illustrate Assumptions \ref{assumption:valid proxy} and \ref{assumption:SC}, we consider the following simple data generating mechanism:
\begin{align}		\label{eq-ExampleModel}
&
W_{it} = \alpha_{i0} + \alpha_{i Y} \potY{t}{0} + e_{it} 
\ , \quad \quad
\EXP \big\{ e_{it} \cond \potY{t}{0} \big\} = 0 
\ , \quad \quad
i \in \D 
\ , \quad \quad
t= 1,\ldots,T \ .
\end{align}
Here, $\alpha_{i0}$ and $\alpha_{iY}$ are fixed constants with constraints $\alpha_{iY} \neq 0$, which encode Assumption \ref{assumption:valid proxy}. This regression model is reminiscent of the nonclassical measurement model in measurement error literature \citep{Carroll2006, Freedman2008}. In particular, if $\alpha_{i0}=0$ and $\alpha_{iY}=1$, one recovers the classical measurement error model $W_{it}=\potY{t}{0} + e_{it}$ with conditional mean-zero error which has been studied extensively in measurement error literature. Note also that Assumption \ref{assumption:SC} implies certain constraints on $\bgamma^*$, namely $\sum_{i \in \D} \gamma_i^* \alpha_{i0} = 0$ and $\sum_{i \in \D} \gamma_i^* \alpha_{iY} = 1$; clearly, multiple $\bgamma^*$ may satisfy these two constraints if the number of donors $d$ is greater than 2. From a regression model perspective, the donors' outcomes $\bW_{it}$ and the treated unit's treatment-free potential outcome $\potY{t}{0}$ in model \eqref{eq-ExampleModel} can be viewed as dependent and independent variables, respectively. This may appear somewhat unconventional at first glance, as some previous synthetic control methods treat $\potY{t}{0}$ and $ \bW_{it} $ as dependent and independent variables, respectively, in estimation of synthetic control weights. To be more precise, they use equation \eqref{eq-OLS} to estimate the synthetic control weights by regressing $\potY{t}{0}$ on $\bW_{it}$ using standard ordinary (or weighted) least squares. However, as model \eqref{eq-ExampleModel} suggests, our framework is different from previous works in synthetic control and better aligned with regression calibration techniques in measurement error literature \citep{Carroll2006} in that we view the problem as the reverse regression model of $\bW_{it}$ on $\potY{t}{0}$. From this perspective, synthetic control weights $\bgamma^*$ are sought to make the weighted response $\bW_{it}\T \bgamma^*$ as close as possible to the regressor $\potY{t}{0}$. 

As a direct consequence of Assumptions \ref{assumption:consistency}--\ref{assumption:SC}, the synthetic control weights $\bgamma^*$ can be represented as a solution to the moment equation given in the following result:
\begin{theorem}	\label{thm:SC}
Under Assumptions \ref{assumption:consistency}--\ref{assumption:SC}, the synthetic control weights $\bgamma^*$ satisfy $\EXP \big( \bW_{\D t}\T \bgamma^* \cond Y_t \big) = Y_t $ almost surely for $t=1,\ldots,T_0$.
\end{theorem}
The proof of the Theorem, as well as all other proofs, are provided in Section \ref{sec:supp:proof} of the Supplementary Material. Theorem \ref{thm:SC} serves to motivate our approach for estimating the synthetic control weights $\bgamma^*$, as it only involves the observed data. Another consequence of Assumptions \ref{assumption:consistency}--\ref{assumption:SC} is that, as formalized in Theorem \ref{thm:ATT} below, the synthetic control $\bW_{\D t}\T \bgamma^*$ can be used to identify $\tau_t^*$:
\begin{theorem}	\label{thm:ATT}
Under Assumptions \ref{assumption:consistency}--\ref{assumption:SC}, we have $\EXP \big\{ \potY{t}{0} \big\} = \EXP \big( \bW_{\D t}\T \bgamma^* \big)$ for any $t = 1,\ldots,T$. Additionally, the ATT is identified as $	\tau_t^*
=
\EXP
\big(
Y_t - \bW_{\D t}\T \bgamma^* 
\big)$ for $t=T_0+1,\ldots,T$.
\end{theorem}
Theorem \ref{thm:ATT} provides a theoretical basis for employing the synthetic control method to estimate the ATT. In particular, following the observations in \citet{Abadie2003} and \citet{Shi2023SC}, we use $Y_t-\bW_{\D t}\T \bgamma^*$ in a standard time series regression where the ATT is identified as the deterministic component of the decomposition $Y_t-\bW_{\D t}\T \bgamma^* = \tau_t^* + \epsilon_t$, with $\epsilon_t$ representing a mean-zero error. The following Sections elaborate on this approach, first describing how the identification result leads to an estimator of $\gamma^*$.  

\subsection{Estimation and Inference of the Treatment Effect}		\label{sec:Estimation}

We first discuss the estimation of the synthetic control weights $\bgamma^*$. We consider the following estimating function for the pre-treatment periods:
\begin{align}	\label{eq-GMM-moment}
\Psi_{\pre} (\bO_t \con \bgamma)
=
\bg_t (Y_t)
\big( Y_t - \bW_{\D t} \T \bgamma \big)
\ , \quad \quad
t = 1,\ldots,T_0 \ .
\end{align}
Here, $\bg_t$ is a $p$-dimensional user-specified function of the outcome of the treated unit at time $t$, where $p \geq d = | \mathcal{D}| $. For instance, $\bg_t$ can be chosen as a collection of truncated polynomial bases functions up to the $p$th power of $y$, i.e., $\bg_t(y) = ( y , y^2,\ldots,y^p )\T$. 
% or a histogram bases system consisting of indicator functions, i.e., $\bg_t (y)
% =
% \{
% \ind (y \leq y_1 )
% , 
% \ind(y_1 < y \leq y_2)
% ,
% \ldots,
% \ind (y_{p-1} < y )
% \} \T$ where $y_1,\ldots,y_{p-1} $ are user-specified values that partition the support of the pre-treatment outcome. 
Splines or wavelets among other options may be used to generate $\bg_t( y ) $. In addition, $\bg_t$ can be specified as a time-varying function, potentially enhancing the finite sample performance of the proposed estimator, especially in cases where the outcomes exhibit nonstationary behavior; see Section \ref{sec:supp:time g function} of the Supplementary Material for an example of time-varying $g_t$. From Theorem \ref{thm:SC}, the estimating function $\Psi_{\pre}$ satisfies $\EXP \{ \Psi_{\pre} (\bO_t \con \bgamma^*) \} = 0$ for $t=1,\ldots,T_0$.

An estimator $\widehat{\bgamma}$ can in principle be obtained based on the empirical counterpart of the mean-zero moment condition on $\Psi_{\pre}$. Since $p$, the dimension of $\Psi_{\pre}$, is no smaller than $d$, the dimension of $\bgamma^*$, standard GMM theory \citep{GMM1982} readily applies. Specifically, the GMM estimator $\widehat{\bgamma}$ is the solution to the following minimization problem:
\begin{align}		\label{eq-GMM-Formula}
\widehat{\bgamma}
=
\argmin_{\bgamma}
\big\{ 
\widehat{\Psi}_{\pre} ( \bgamma ) 
\big\} \T
\widehat{\Omega}_{\pre}
\big\{ 
\widehat{\Psi}_{\pre} ( \bgamma ) 
\big\}
\ , \		
\end{align}
where $\widehat{\Psi}_{\pre} ( \bgamma ) 
= T_0^{-1}
\sum_{t=1}^{T_0} 
\Psi_{\pre} (\bO_t \con \bgamma)$ is the empirical mean of the estimating function over the pre-treatment periods evaluated at $\bgamma$ and $\widehat{\Omega}_{\pre} \in \R^{p \times p}$ is a user-specified symmetric positive definite matrix, which can be simply chosen as the identity matrix.

Equations \eqref{eq-GMM-moment} and \eqref{eq-GMM-Formula} fortunately admit closed-form solutions. For instance, if $\widehat{\Omega}_{\pre}$ is chosen as the identity matrix, we have $\bgamma^* = \bG_{YW}^{*+}\bG_{YY}^* + \eta_{YW}^*$ and $\widehat{\bgamma} = \widehat{\bG}_{YW}^+ \widehat{\bG}_{YY} + \widehat{\eta}_{YW}$ where 
\begin{align} \label{eq-Gyw Gyy}
&
\bG_{YW}^* = \frac{1}{T_0} \sum_{t=1}^{T_0} \EXP \big\{ \bg_t(Y_t) \bW_{\D t} \T \big\} \in \R^{p \times d}
\ , 
&&
\bG_{YY}^* = \frac{1}{T_0} \sum_{t=1}^{T_0} \EXP \big\{ \bg_t(Y_t) Y_t \big\} \in \R^{p}
\ , \
\nonumber
\\
&
\widehat{\bG}_{YW} =
\frac{1}{T_0} 
\sum_{t=1}^{T_0} \bg_t(Y_t) \bW_{\D t}\T \in \R^{p \times d}
\ , 
&&
\widehat{\bG}_{YY} = 
\frac{1}{T_0} \sum_{t=1}^{T_0} \bg_t(Y_t) Y_t \in \R^{p} \ .
\end{align}
Here, $M^+$ denotes the Moore-Penrose inverse of a matrix $M$, and $\eta_{YW}^*$ and $\widehat{\eta}_{YW}$ are arbitrary vectors in the null spaces of $\bG_{YW}^*$ and $\widehat{\bG}_{YW}$, respectively, i.e., $\bG_{YW}^* \eta_{YW}^*=0$ and $\widehat{\bG}_{YW} \widehat{\eta}_{YW}=0$. If $\bG_{YW}^*$ and $\widehat{\bG}_{YW}$ are of full column rank, these solutions are uniquely defined as $\bgamma^* = \big( \bG_{YW}\sT \bG_{YW}^* \big)^{-1} \bG_{YW}\sT \bG_{YY}^*$ and $\widehat{\bgamma} = \big( \widehat{\bG}_{YW}\T \widehat{\bG}_{YW} \big)^{-1} \widehat{\bG}_{YW}\T \widehat{\bG}_{YY}$. 
% $\bgamma^* = \big( \bG_{YW}\sT \bG_{YW}^* \big)^{-1} \bG_{YW}\sT \bG_{YY}^*$ and $\widehat{\bgamma} = \big( \widehat{\bG}_{YW}\T \widehat{\bG}_{YW} \big)^{-1} \widehat{\bG}_{YW}\T \widehat{\bG}_{YY}$ where
% \begin{align} \label{eq-Gyw Gyy}
% &
% \bG_{YW}^* = \frac{1}{T_0} \sum_{t=1}^{T_0} \EXP \big\{ \bg_t(Y_t) \bW_{\D t} \T \big\} \in \R^{p \times d}
% \ , 
% &&
% \bG_{YY}^* = \frac{1}{T_0} \sum_{t=1}^{T_0} \EXP \big\{ \bg_t(Y_t) Y_t \big\} \in \R^{p}
% \ , \
% \nonumber
% \\
% &
% \widehat{\bG}_{YW} =
% \frac{1}{T_0} 
% \sum_{t=1}^{T_0} \bg_t(Y_t) \bW_{\D t}\T \in \R^{p \times d}
% \ , 
% &&
% \widehat{\bG}_{YY} = 
% \frac{1}{T_0} \sum_{t=1}^{T_0} \bg_t(Y_t) Y_t \in \R^{p} \ .
% \end{align}
% These solutions are well-defined if $\bG_{YW}^*$ and $\widehat{\bG}_{YW}$ are of full column rank. 
In practice, the rank conditions can be validated at least for $\widehat{\bG}_{YW}$ by investigating its singular values. In the event that $\widehat{\bG}_{YW}$ is found to be column rank deficient, or nearly so, one may select an alternative choice for $\bg_t(\cdot)$ and/or discard donors that do not appear to contribute information on $Y_t$ in the pre-treatment period; see Section \ref{sec:supp:Donors} of the Supplementary Material for a specific example of the procedure. Alternatively, if $\widehat{\bG}_{YW}$ is column rank deficient, one may regularize \eqref{eq-GMM-Formula} and solve a penalized version of GMM; see Section \ref{sec:supp:Regularized GMM} of the Supplementary Material for details.  
In the remainder of the paper, we assume that $\bG_{YW}^*$ is full column rank (a testable condition) such that $\bgamma^*$ is unique.


% Additionally, we remark that the moment equation $\Psi_{\pre}$ in \eqref{eq-GMM-moment} is similar to $\Psi_{\text{OLS}}$ in \eqref{eq-OLS-SC-Moment} and $\Psi_{\PI}$ in \eqref{eq-Shi-SC-Moment}, which are developed under the classical synthetic control framework and proximal synthetic control framework \citep{Shi2023SC}, respectively. Specifically, in the context of our framework, the function $\bg_t(\cdot)$ is linked to the treatment-free potential outcome $\potY{t}{0}.$ Meanwhile, in the framework proposed by \citet{Shi2023SC}, $\bg_t(\cdot)$ is related to the proxies $\bZ_{ t}$ that satisfy condition \eqref{eq-Shi-proxy}. Lastly, in the classical synthetic control framework, $\bg_t(\cdot)$ is dependent on the donors. The differences in $\bg_t (\cdot) $ arise from the definition of synthetic controls and the assumptions that are introduced to identify those.

Once the synthetic control weights are estimated, one could in principle estimate the treatment-free potential outcome and the ATT as $\widehat{Y}_{t}^{(0)} = \bW_{\D t}\T \widehat{\bgamma}$ and $\widehat{\tau}_t = Y_t - \bW_{\D t}\T \widehat{\bgamma}$, respectively, for $t = T_0 + 1,\ldots,T$. Unfortunately, without additional assumptions, it is impossible to perform inference based on $\widehat{\tau}_t$ because the latter will generally fail to be consistent given that we only have access to one observation per each $t$. Therefore, in practice, we posit a parsimonious working model for the treatment effect as a function of time. Specifically, we assume that the ATT follows a model indexed by a $b$-dimensional parameter $\bbeta$ via a function $\tau(\cdot \con \cdot): [0,\infty) \otimes \R^{b} \rightarrow \R$. Let $\bbeta^* \in \R^{b}$ be the true parameter satisfying $\tau_t^* 
=
\tau ( t \con \bbeta^* )$ for $t=1,\ldots,T$. This parametrization allows us to pool information over time in the post-treatment period to infer $\bbeta^*$ and, thus, the ATT. Possible forms for $\tau(t \con \bbeta)$ are given below:
\begin{example}[Constant Effect] 
$\tau(t \con \bbeta) = \beta $; this model is reasonable if the treatment yields a rapid, short-term effect and it persists for a long period.		
\end{example}
\begin{example}[Linear Effect] 
$\tau(t \con \bbeta) = \beta_0 + \beta_1 (t- T_0)_+/T_1$ where let $(c)_+ = \max(c,0)$ for a constant $c$. ; this model is appropriate if the treatment yields a gradual, increasing effect over time.
\end{example}
\begin{example}[Nonlinear Effect] 
This includes a quadratic model $\tau(t \con \bbeta) = \beta_0 + \beta_1 (t- T_0)_+/T_1 + \beta_2 (t - T_0)_+^2/T_1$, or an exponentially time-varying treatment model $\tau(t \con \bbeta) = \exp \big\{ \beta_0 + \beta_1 (t-T_0)_+/T_1 \big\}$; this model is appropriate if the treatment yields a nonlinear effect over time.
\end{example}

For tractable inference, we assume that the error process is weakly independent, which is formally stated as follows:
\begin{assumption}[Weakly Dependent Error]	 		\label{assumption:weakdep}
Let $\epsilon_t = Y_t - \bW_{\D t} \T \bgamma^* - \tau(t \con \bbeta^*)$ for $t=1,\ldots,T$. Then, the error process $\big\{ \epsilon_t \cond t=1,\ldots,T \big\}$ is weakly dependent, i.e., $\text{corr}(\epsilon_{t}, \epsilon_{t+t'})$ converges to 0 as $t' \rightarrow \pm \infty$.
\end{assumption}
Assumption \ref{assumption:weakdep} applies to many standard time series models, including autoregressive (AR) models, moving-average (MA) models, and autoregressive moving-average (ARMA) models. 

Along with the assumptions, we will consider an asymptotic setting where $T_0, T_1 \rightarrow \infty$ and $T_1/T_0 \rightarrow r \in (0,\infty)$. Specifically, let $\Psi (\bO_t \con \bgamma, \bbeta)$ be the following $(p+b)$-dimensional estimating function:
\begin{align*}
% \label{eq-Moment GMM}
\Psi (\bO_t \con \bgamma, \bbeta)
=
\begin{bmatrix}
\Psi_{\pre} (\bO_t \con \bgamma)
\\
\Psi_{\post} (\bO_t \con \bgamma , \bbeta)
\end{bmatrix}
=
\begin{bmatrix}
(1-A_t)
\bg_t (Y_t)
\big( Y_t - \bW_{\D t} \T \bgamma \big)
\\
A_t 
\frac{\partial \tau(t \con \bbeta) }{\partial \bbeta } 
\big\{
Y_t - \bW_{\D t} \T \bgamma - \tau (t \con \bbeta)
\big\}
\end{bmatrix}
\in \R^{p+b} \ .
\end{align*}
Then, GMM estimators of the synthetic control weights and treatment effect parameter are obtained as the solution to the following minimization problem:
\begin{align*}
% \label{eq-GMM}
\big(
\widehat{\bgamma}
,
\widehat{\bbeta}
\big)
=
\argmin_{(\bgamma,\bbeta)}
\big\{ \widehat{\Psi}( \bgamma, \bbeta) \big\} \T
\widehat{\Omega}
\big\{ \widehat{\Psi}( \bgamma, \bbeta) \big\} \ ,
\end{align*}
where $	\widehat{\Psi}(\bgamma, \bbeta)
= T^{-1}
\sum_{t=1}^{T} \Psi (\bO_t \con \bgamma, \bbeta)$ is the empirical mean of the estimating function and $\widehat{\Omega} \in \R^{(p+b) \times (p+b)}$ a user-specified symmetric positive definite block-diagonal matrix as $\widehat{\Omega} = \text{diag}( \widehat{\Omega}_{\pre} , \widehat{\Omega}_{\post} )$; for simplicity, $\widehat{\Omega}$ can be chosen as the identity matrix. Under our assumptions, the following result establishes that $(\widehat{\bgamma}, \widehat{\bbeta})$ is asymptotically normal:
\begin{theorem}	\label{thm:AN}
Suppose that Assumptions \ref{assumption:consistency}--\ref{assumption:weakdep} holds, $\bG_{YW}^*$ is full column rank, and regularity conditions in Section \ref{sec:supp:AN} of the Supplementary Material hold. Then, as $T \rightarrow \infty$, we have
\begin{align*}
\sqrt{ T }
\big\{
\big(
\widehat{\bgamma}\T
\ , \
\widehat{\bbeta}\T
\big)\T
-
\big(
\bgamma\sT
\ , \
\bbeta\sT
\big)\T
\big\}
\text{ converges in distribution to }
N \big( 0, \Sigma_1^* \Sigma_2^* \Sigma_1\sT \big) \ .
\end{align*}
Here, $\Sigma_1^*
=
\big( G\sT \Omega^* G^* \big)^{-1} G\sT \Omega^* $ and $
\Sigma_2^* 
=
\lim_{T \rightarrow \infty} \VAR \big\{ T^{1/2} \cdot \widehat{\Psi}(\bgamma^*, \bbeta^*) \big\}$ where
\begin{align*}
G^* 
=
\lim_{T \rightarrow \infty} \frac{1}{T} \sum_{t=1}^{T}
\EXP
\bigg\{
\frac{\partial \Psi( \bO_t \con \bgamma,\bbeta) }{ \partial (\bgamma,\bbeta)\T }
\bigg\}
\bigg|_{ \bgamma=\bgamma^* , \bbeta=\bbeta^* }
\ , \
\Omega^* = \lim_{T \rightarrow \infty} \widehat{\Omega} \ .
\end{align*}
\end{theorem}
\noindent For inference, we propose the variance estimator $\widehat{\Sigma}
=
\widehat{\Sigma}_1 
\widehat{\Sigma}_2
\widehat{\Sigma}_1 \T$; here, $\widehat{\Sigma}_1
=
\big( \widehat{G}\T \widehat{\Omega} \widehat{G} \big)^{-1} \widehat{G}\T \widehat{\Omega}$ with $\widehat{G} = T^{-1} \sum_{t=1}^{T} \partial \Psi ( \bO_t \con \bgamma, \bbeta) / \partial (\bgamma, \bbeta)\T \big|_{\bgamma = \widehat{\bgamma}, \bbeta = \widehat{\bbeta}} $. For $\widehat{\Sigma}_2$, we use a heteroskedasticity and autocorrelation consistent estimator \citep{NW1987, Andrews1991} given the time series nature of the observed sample; see Section \ref{sec:supp:HAC} of the Supplementary Material for details. Alternatively, one could implement the block bootstrap; see Section \ref{sec:supp:BB} of the Supplementary Material for details.

In the event that donors appear to be highly correlated with each other, particularly in settings with a large pool of donors, a form of regularization may be needed to improve the finite sample behavior of the proposed estimator. Likewise, a related problem may be due to the smallest singular value of the matrix $\bG_{YW}^*$ in \eqref{eq-Gyw Gyy} being nearly equal to zero making the latter nearly singular so that the true parameter $(\bgamma^*, \bbeta^*)$ may not be well-estimated by GMM. To resolve these issues, one may include a regularization term for $\bgamma$ such as ridge regularization. One may infer the treatment effect based on the resulting regularized estimator provided that the regularization parameter is appropriately chosen; see Section \ref{sec:supp:Regularized GMM} of the Supplementary Material for details. The empirical results in the following sections demonstrate that such ridge regularization can improve finite sample inferences. 

Lastly, in practice, one may indeed have access to exogenous covariates. Incorporating these covariates in the analysis may be of interest because it may improve the efficiency of the estimated treatment effect. In Section \ref{sec:Cov} of the Supplementary Materials, we provide details on how to incorporate available covariates in the SPSC framework. 

\subsection{Conformal Inference of the Treatment Effect} \label{sec:Conformal}

Some drawbacks of the methodology proposed in the previous Section are that (i) a parsimonious model choice for $\tau_t = \tau(t \con \bbeta)$ may be misspecified and (ii) it potentially requires large $T_0$ and $T_1$ for both a law of large numbers and a central limit theorem to apply such that our large sample analysis can reliably be used to quantify uncertainty based on $\widehat{\bgamma}$ and $\widehat{\bbeta}$. The aforementioned limitations may be prohibitive in real-world applications with limited post-treatment follow-up data available. In order to address this specific challenge, previous works such as \citet{Cattaneo2021, Chernozhukov2021} develop prediction intervals to assess statistical uncertainty, obviating the need to specify a model for the treatment effect or require a large number of post-treatment time series data. While other approaches can in principle be applied, we focus on the conformal inference approach proposed by \citet{Chernozhukov2021} due to its ready adaptation to the SPSC framework. The key idea of the approach is to construct pointwise prediction intervals for the random treatment effects $\xi_t^* = \potY{t}{1} - \potY{t}{0}$ for $t=T_0+1,\ldots,T$ by inverting permutation tests about certain null hypotheses about $\xi_t^*$. One crucial requirement for the approach is the existence of an unbiased predictor for $\potY{t}{0}$ for $t=1,\ldots, T$. In the context of SPSC, the synthetic control $\bW_{\D t} \T \bgamma^*$ is an unbiased predictor for $\potY{t}{0}$ as established in Theorem \ref{thm:ATT}, and, consequently, their approach readily applies. In what follows, we present the approach in detail adapted to the SPSC framework. 

Consider an asymptotic regime whereby $T_0$ goes to infinity while $T_1$ is fixed. Let $s \in \{T_0+1,\ldots,T\}$ be a post-treatment period for which one aims to construct a prediction interval for the treatment effect; without loss of generality, we take $s=T_0+1$. The null hypothesis of interest can be expressed as $H_{0, T_0+1} : \xi_{T_0+1}^* = \xi_{0,{T_0+1}}$, where $\xi_{0,{T_0+1}}$ represents a hypothesized treatment effect value. Under $H_{0,T_0+1}$, the treatment-free potential outcome at time ${T_0+1}$ can be identified as $\potY{{T_0+1}}{0} = Y_{T_0+1} - \xi_{0,{T_0+1}}$ and, consequently, pre-treatment outcomes ${ Y_1, \ldots, Y_{T_0} }$ may in fact be supplemented with $Y_{T_0+1} - \xi_{0,{T_0+1}}$  to estimate the synthetic control weights. We may then redefine the pre-treatment estimating function $\Psi_{\pre}$ in equation \eqref{eq-GMM-moment} as follows:
\begin{align*}
\Psi_{\pre} (\bO_t \con \bgamma, \xi_{0,{T_0+1}} )
=
\left\{
\begin{array}{lll}
\bg_t (Y_t)
\big( Y_t - \bW_{\D t} \T \bgamma \big)
\ ,
&
\quad \quad
&
t = 1,\ldots,T_0
\\
\bg_t (Y_t - \xi_{0,{T_0+1}})
\big( Y_t - \xi_{0,{T_0+1}} - \bW_{\D t} \T \bgamma \big)
\ ,
&
\quad \quad
&
t = {T_0+1}
\end{array}
\right. 
\ .
\end{align*}
At the true synthetic control weights $\gamma^*$, the redefined estimating function is mean-zero for $t = 1,\ldots, {T_0+1}$ under $H_{0,T_0+1}$. Therefore, a GMM estimator $\widehat{\bgamma}(\xi_{0,T_0+1})$ can be obtained by solving the following minimization problem, which is similar to \eqref{eq-GMM-Formula}:
\begin{align*}
\widehat{\bgamma} (\xi_{0,T_0+1})
=
\argmin_{\bgamma}
\big\{ 
\widehat{\Psi}_{\pre} ( \bgamma, \xi_{0,T_0+1} ) 
\big\} \T
\widehat{\Omega}_{\pre}
\big\{ 
\widehat{\Psi}_{\pre} ( \bgamma, \xi_{0,T_0+1} ) 
\big\}
\ , \ 
\end{align*} 
where $\widehat{\Psi}_{\pre} ( \bgamma, \xi_{0,T_0+1} ) 
= 
(T_0+1)^{-1}
\sum_{t=1}^{T_0+1} 
\Psi_{\pre} (\bO_t \con \bgamma, \xi_{0,T_0+1})$ and $\widehat{\Omega}_{\pre}$ is the weight matrix used in \eqref{eq-GMM-Formula}. We may then compute residuals $\widehat{\epsilon}_{t} (\xi_{0,T_0+1}) = Y_t - \xi_{0,{T_0+1}} A_t - \bW_{\D t} \T \widehat{\bgamma}(\xi_{0,{T_0+1}}) $, and use these residuals to obtain a p-value for testing the null hypothesis as follows:
\begin{align*}
p_{T_0+1}(\xi_{0, T_0+1}) 
=
\frac{1}{T_0+1	}
\sum_{t=1}^{T_0+1} \ind \Big\{ \big| \widehat{\epsilon}_{t} (\xi_{0, T_0+1}) \big| \geq \big| \widehat{\epsilon}_{T_0+1} (\xi_{0, T_0+1}) \big| \Big\}
\ .
\end{align*}
In words, the p-value is the proportion of residuals of 
magnitudes no smaller than the post-treatment residual. Under $H_{0, T_0+1}$ and regularity conditions including that the error $\epsilon_t = Y_t - \xi_t^* - \bW_{\D t} \T \bgamma^*$ is stationary and weakly dependent, the p-value is approximately unbiased, i.e., $\text{pr} \{ p_{T_0+1}(\xi_{0,T_0+1}) \leq \alpha \} = \alpha + o(1)$ as $T_0 \rightarrow \infty$ for a user-specified confidence level $\alpha \in (0,1)$; we refer the readers to Theorem 1 of \citet{Chernozhukov2021} for technical details. Therefore, an approximate $100(1-\alpha)$\% prediction interval for $\xi_{t}^*$ can be constructed by inverting the hypothesis test based on $p_{T_0+1}(\xi_{0, T_0+1})$. This prediction interval is formally defined as
$\mathcal{C}_{T_0+1} (1-\alpha) =
\big\{
\xi
\cond		
p_{T_0+1}(\xi) > \alpha
\big\}$ and can be found via grid-search. We remark that the proposed conformal inference approach is also valid for constructing a confidence interval for the ATT by replacing $\xi_t^*$ with $\tau_t^*$. 






\section{Simulation}		\label{sec:Sim}

We conducted a simulation study to evaluate the finite sample performance of the proposed estimator under a variety of conditions. We considered the following data generating mechanisms with pre- and post-treatment periods of length $T_0 = T_1 \in \{ 50, 100, 250, 1000 \}$ and donor pools of size $d \in \{2,5,9\}$. For each value of $T_0$, $T_1$, and $d$, we generated all residual errors for $t=1,\ldots,T$ based on the AR(2):
\begin{align*}
&
\epsilon_{y,t} = 0.2 \epsilon_{y,t-1} + 0.1 \epsilon_{y,t-2} + \eta_{y,t}
\ , \ 
&&
\epsilon_{\tau,t} = 0.2 \epsilon_{\tau,t-1} + 0.1 \epsilon_{\tau,t-2} + \eta_{\tau,t} 
\ ,
\\
&
\epsilon_{w_i,t} = 0.2 \epsilon_{w_i,t-1} + 0.1 \epsilon_{w_i,t-2} + \eta_{w_i,t}
\ , \ i=1,\ldots,d \ ,
\end{align*}
where $\eta$ are generated from a standard normal distribution. The errors $\epsilon_{t}$ at $t=-1,0$ were initialized to equal zero. The treatment-free potential outcomes at $t=1,\ldots,T$ were generated as $\potY{t}{0} = 0.2 \potY{t-1}{0} + 0.1 \potY{t-2}{0} + t/T_0 + \epsilon_{y,t}$. The potential outcomes under treatment at $t=1,\ldots,T$ were generated as $\potY{t}{1} = \potY{t}{0} + 3 A_t + \epsilon_{\tau,t}$; therefore, the ATT is $\tau_t^* = 3$ for all $t=T_0+1,\ldots,T$. Lastly, we generated $\bW_{\D t} \in \R^{d}$ under Assumptions \ref{assumption:valid proxy} and \ref{assumption:SC}; see Section \ref{sec:supp:Simulation} of the Supplementary Material for details. We set $\bg_t(\cdot)$ as time-invariant cubic B-spline bases functions with dimension equal to twice the number of donors, i.e., $\dim (\bg_t) = 2 d \in \{4,10,18 \}$. The knots of the spline functions were chosen based on the empirical quantiles of the pre-treatment outcomes.

% We compared the three ATT estimators corresponding to standard synthetic control with weights estimated via OLS without regularization as described in Section \ref{sec:Review}, the SPSC approach, as described in Section \ref{sec:SPSC}, and the SPSC approach with ridge regularization; see Section \ref{sec:supp:Regularized GMM} of the Supplementary Material for details on the regularized SPSC estimator. These methods are referred to as OLS, SPSC, and SPSC-Ridge, respectively. We repeated the simulation 500 times. 

Using the generated data, we obtain two ATT estimators based on the proposed SPSC approaches both with and without ridge regularization; see Section \ref{sec:supp:Regularized GMM} of the Supplementary Material for details on the regularized SPSC estimator. These estimators are referred to as SPSC and SPSC-Ridge, respectively. For comparison, we also considered an OLS-based ATT estimator. Unfortunately, the ATT estimators proposed by \citet{Abadie2010}, implemented in \texttt{synth} R-package \citep{Synth2011}, and \citet{ASCM2021} implemented in \texttt{augsynth} R-package \citep{ASCM2023package}, do not appear to provide readily available standard errors. As a result, we compared our methods to the OLS-based ATT estimator without regularization as described in Section \ref{sec:Review}, which we simply refer to as OLS hereafter. We repeated the simulation 500 times. 





Figure \ref{fig:Sim:Constant} summarizes the empirical distribution of the estimators graphically. Across all scenarios, we find that the OLS estimator is biased for the ATT, particularly when the number of donors is small, say $d=2$; these results largely confirm our theoretical expectation as discussed in Section \ref{sec:Review} that the OLS estimator may be biased under a data generating process compatible with the SPSC framework. In contrast, we find the SPSC estimator appears to be unbiased across scenarios, and the corresponding 95\% Monte Carlo confidence interval shrinks as the number of time periods increases. This is consistent with the results established in Section \ref{sec:SPSC}. Table \ref{tab:Sim:Constant d9d0} provides more detailed summary statistics under $d=9$; those under the other two simulation scenarios are reported in Section \ref{sec:supp:Simulation} of the Supplementary Material. Unsurprisingly, confidence intervals based on the OLS estimator fail to attain the nominal coverage rate, especially when $T_0$ and $T_1$ are large due to non-diminishing bias. On the other hand, confidence intervals based on SPSC estimators attain the nominal coverage rate regardless of whether ridge regularization is used or not. Comparing the two SPSC estimators, we find that SPSC-Ridge has a somewhat larger bias, however, with a smaller standard error, resulting in a smaller mean squared error. In other words, we find that incorporating ridge regularization can improve the estimator's performance in terms of mean squared error reflecting the well-known bias-variance trade-off phenomenon, which ridge regression has been shown to achieve in many settings \citep{Ridge1974,Ridge2012}. 

% Figure environment removed		

\begin{table}[!htp]
\renewcommand{\arraystretch}{1.05} \centering
\footnotesize
\setlength{\tabcolsep}{3pt} 
\begin{tabular}{|c|cccc|cccc|cccc|}
\hline
Estimator & \multicolumn{4}{c|}{OLS} & \multicolumn{4}{c|}{SPSC} & \multicolumn{4}{c|}{SPSC-Ridge} \\ \hline
$T_0$ & \multicolumn{1}{c|}{50} & \multicolumn{1}{c|}{100} & \multicolumn{1}{c|}{250} & 1000 & \multicolumn{1}{c|}{50} & \multicolumn{1}{c|}{100} & \multicolumn{1}{c|}{250} & 1000 & \multicolumn{1}{c|}{50} & \multicolumn{1}{c|}{100} & \multicolumn{1}{c|}{250} & 1000 \\ \hline

Bias ($\times$10) & \multicolumn{1}{c|}{$1.305$} & \multicolumn{1}{c|}{$1.524$} & \multicolumn{1}{c|}{$1.479$} & \multicolumn{1}{c|}{$1.401$} & \multicolumn{1}{c|}{$-0.206$} & \multicolumn{1}{c|}{$0.055$} & \multicolumn{1}{c|}{$-0.037$} & \multicolumn{1}{c|}{$-0.098$} & \multicolumn{1}{c|}{$-0.175$} & \multicolumn{1}{c|}{$-0.035$} & \multicolumn{1}{c|}{$-0.046$} & \multicolumn{1}{c|}{$-0.128$} \\ \hline
ASE ($\times$10) & \multicolumn{1}{c|}{$2.356$} & \multicolumn{1}{c|}{$1.674$} & \multicolumn{1}{c|}{$1.098$} & \multicolumn{1}{c|}{$0.568$} & \multicolumn{1}{c|}{$4.461$} & \multicolumn{1}{c|}{$3.008$} & \multicolumn{1}{c|}{$1.927$} & \multicolumn{1}{c|}{$0.930$} & \multicolumn{1}{c|}{$3.245$} & \multicolumn{1}{c|}{$2.358$} & \multicolumn{1}{c|}{$1.487$} & \multicolumn{1}{c|}{$0.747$} \\ \hline
% BSE ($\times$10) & \multicolumn{1}{c|}{$2.999$} & \multicolumn{1}{c|}{$1.990$} & \multicolumn{1}{c|}{$1.303$} & \multicolumn{1}{c|}{$0.700$} & \multicolumn{1}{c|}{$5.549$} & \multicolumn{1}{c|}{$3.433$} & \multicolumn{1}{c|}{$2.127$} & \multicolumn{1}{c|}{$1.107$} & \multicolumn{1}{c|}{$4.049$} & \multicolumn{1}{c|}{$2.777$} & \multicolumn{1}{c|}{$1.799$} & \multicolumn{1}{c|}{$0.938$} \\ \hline
ESE ($\times$10) & \multicolumn{1}{c|}{$2.956$} & \multicolumn{1}{c|}{$1.934$} & \multicolumn{1}{c|}{$1.149$} & \multicolumn{1}{c|}{$0.603$} & \multicolumn{1}{c|}{$4.182$} & \multicolumn{1}{c|}{$2.434$} & \multicolumn{1}{c|}{$1.614$} & \multicolumn{1}{c|}{$0.800$} & \multicolumn{1}{c|}{$3.578$} & \multicolumn{1}{c|}{$2.261$} & \multicolumn{1}{c|}{$1.455$} & \multicolumn{1}{c|}{$0.721$} \\ \hline
MSE ($\times$100) & \multicolumn{1}{c|}{$10.422$} & \multicolumn{1}{c|}{$6.054$} & \multicolumn{1}{c|}{$3.507$} & \multicolumn{1}{c|}{$2.325$} & \multicolumn{1}{c|}{$17.500$} & \multicolumn{1}{c|}{$5.916$} & \multicolumn{1}{c|}{$2.600$} & \multicolumn{1}{c|}{$0.648$} & \multicolumn{1}{c|}{$12.810$} & \multicolumn{1}{c|}{$5.104$} & \multicolumn{1}{c|}{$2.116$} & \multicolumn{1}{c|}{$0.536$} \\ \hline
Coverage & \multicolumn{1}{c|}{$0.854$} & \multicolumn{1}{c|}{$0.788$} & \multicolumn{1}{c|}{$0.710$} & \multicolumn{1}{c|}{$0.306$} & \multicolumn{1}{c|}{$0.956$} & \multicolumn{1}{c|}{$0.962$} & \multicolumn{1}{c|}{$0.980$} & \multicolumn{1}{c|}{$0.970$} & \multicolumn{1}{c|}{$0.922$} & \multicolumn{1}{c|}{$0.940$} & \multicolumn{1}{c|}{$0.958$} & \multicolumn{1}{c|}{$0.946$} \\ \hline
% Cover (BSE) & \multicolumn{1}{c|}{$0.916$} & \multicolumn{1}{c|}{$0.842$} & \multicolumn{1}{c|}{$0.802$} & \multicolumn{1}{c|}{$0.480$} & \multicolumn{1}{c|}{$0.978$} & \multicolumn{1}{c|}{$0.980$} & \multicolumn{1}{c|}{$0.984$} & \multicolumn{1}{c|}{$0.982$} & \multicolumn{1}{c|}{$0.950$} & \multicolumn{1}{c|}{$0.958$} & \multicolumn{1}{c|}{$0.986$} & \multicolumn{1}{c|}{$0.986$} \\ \hline


\end{tabular}
\caption{Summary Statistics of Estimation Results. 
Bias row gives the empirical bias of 500 estimates. 
ASE row gives the asymptotic standard error obtained from the sandwich estimator of the GMM. 
%	BSE row shows the bootstrap standard error obtained from the approach in Section \ref{sec:supp:BB} of the Supplementary Material.
ESE row gives the standard deviation of 500 estimates.
MSE row gives the mean squared error of 500 estimates. 
Coverage row gives the empirical coverage rate of 95\% confidence intervals based on the asymptotic standard error.
Bias, standard errors, and mean squared error are scaled by factors of 10, 10, and 100, respectively.
}
\label{tab:Sim:Constant d9d0} 
\end{table}


Next, we evaluated the finite sample performance of the conformal inference approach in Section \ref{sec:Conformal}. As a competing method, we also implemented the approach proposed by \citet{Cattaneo2021}, referred to as SCPI hereafter, using the publicly available \texttt{scpi} R-package \citep{scpiPackage2023}.
For each simulated data set, we obtained 
pointwise 95\% pointwise prediction intervals for the treatment effect at the first post-treatment time $t=T_0+1$, i.e., $\xi_{T_0+1}^* = 3 + \epsilon_{\tau,T_0+1}$, using the proposed conformal inference approach and SCPI. 
% As a competing method, we constructed a 95\% pointwise prediction interval using the approach proposed by \citet{Cattaneo2021}, which is implemented in \texttt{scpi} R-package \citep{scpiPackage2023} and is referred to as SCPI hereafter; see Section \ref{sec:supp:Simulation} for details on how we implemented their method. 
We then calculated the empirical coverage rates of these pointwise prediction intervals based on 500 simulation repetitions, i.e., the proportions of repetitions where $\xi_{T_0+1}^*$ is contained in 95\% pointwise prediction intervals. Table \ref{tab:Table3} gives the empirical coverage rates for each simulation scenario. We find that the conformal inference approach for the SPSC attains the desired nominal coverage rate across all simulation scenarios in general with and without ridge regularization, aligning closely with theoretical expectations. However, we find that the SCPI approach sometimes fails to do so, especially when the number of donors is small and the time periods are long, say $d=2$ and $T_0=1000$. This finding demonstrates that existing inference methods may be invalid when applied to a dataset compatible with the SPSC framework.



\begin{table}[!htp] 
\renewcommand{\arraystretch}{1.05} \centering
\footnotesize
\setlength{\tabcolsep}{3pt} 
\hspace*{-0.25cm}
\begin{tabular}{|cc|cccc|cccc|cccc|}
\hline
\multicolumn{2}{|c|}{Estimator}                & \multicolumn{4}{c|}{SPSC}                                                                            & \multicolumn{4}{c|}{SPSC-Ridge}                                                                      & \multicolumn{4}{c|}{SCPI}                                                                            \\ \hline
\multicolumn{2}{|c|}{$T_0$}                    & \multicolumn{1}{c|}{50}      & \multicolumn{1}{c|}{100}     & \multicolumn{1}{c|}{250}     & 1000    & \multicolumn{1}{c|}{50}      & \multicolumn{1}{c|}{100}     & \multicolumn{1}{c|}{250}     & 1000    & \multicolumn{1}{c|}{50}      & \multicolumn{1}{c|}{100}     & \multicolumn{1}{c|}{250}     & 1000    \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{$d$}} & 2 & \multicolumn{1}{c|}{$0.964$} & \multicolumn{1}{c|}{$0.946$} & \multicolumn{1}{c|}{$0.940$} & $0.948$ & \multicolumn{1}{c|}{$0.968$} & \multicolumn{1}{c|}{$0.950$} & \multicolumn{1}{c|}{$0.940$} & $0.948$ & \multicolumn{1}{c|}{$0.948$} & \multicolumn{1}{c|}{$0.924$} & \multicolumn{1}{c|}{$0.920$} & $0.904$ \\ \cline{2-14} 
\multicolumn{1}{|c|}{}                     & 5 & \multicolumn{1}{c|}{$0.962$} & \multicolumn{1}{c|}{$0.956$} & \multicolumn{1}{c|}{$0.956$} & $0.942$ & \multicolumn{1}{c|}{$0.956$} & \multicolumn{1}{c|}{$0.946$} & \multicolumn{1}{c|}{$0.960$} & $0.932$ & \multicolumn{1}{c|}{$0.980$} & \multicolumn{1}{c|}{$0.982$} & \multicolumn{1}{c|}{$0.978$} & $0.920$ \\ \cline{2-14} 
\multicolumn{1}{|c|}{}                     & 9 & \multicolumn{1}{c|}{$0.954$} & \multicolumn{1}{c|}{$0.948$} & \multicolumn{1}{c|}{$0.966$} & $0.944$ & \multicolumn{1}{c|}{$0.942$} & \multicolumn{1}{c|}{$0.942$} & \multicolumn{1}{c|}{$0.964$} & $0.952$ & \multicolumn{1}{c|}{$0.984$} & \multicolumn{1}{c|}{$0.990$} & \multicolumn{1}{c|}{$0.978$} & $0.954$ \\ \hline
\end{tabular} 
\caption{
Empirical Coverage Rates of 95\% Pointwise Prediction Intervals. The numbers in SPSC and SPSC-Ridge columns give the empirical coverage rates of 95\% pointwise prediction intervals obtained from the conformal inference approach in Section \ref{sec:Conformal}. The numbers in SCPI column give the empirical coverage rates of 95\% pointwise prediction intervals obtained from the approach proposed by \citet{Cattaneo2021}.}
\label{tab:Table3} 
\end{table}


In Section \ref{sec:supp:Simulation} of the Supplementary Material, we report results for additional simulation studies where exogenous covariates are included and the ATT is linear. In brief, these additional results largely agree with the asymptotic properties established in the previous sections. Furthermore, in Section \ref{sec:supp:Simulation PI} of the Supplementary Material, we assess the finite sample performance of the proposed conformal inference approach based on the simulation scenario given in \citet{Cattaneo2021}, which may not be compatible with the key identifying condition, Assumption \ref{assumption:SC}, of SPSC. As expected, the approach of \citet{Cattaneo2021} performs well in this setting, and our approach sometimes fails to attain the nominal coverage rate, particularly when the outcomes are not stationary in the pre-treatment period. However, explicitly incorporating time dependence in specification of $\bg_t$ in \eqref{eq-GMM-moment} appears to significantly improve the performance of our conformal inference approach in such cases.





\section{Application}		\label{sec:Data}

We applied the proposed method to analyze a real-world application. In particular, we revisited the dataset analyzed in \citet{Dataset1907}, which consists of time series data of length $384$ for 59 trust companies, recorded between January 5, 1906, and December 30, 1908, with a triweekly frequency. Notably, this time period includes the Panic of 1907 \citep{Moen1992}, a financial panic that lasted for three weeks in the United States starting in mid-October, 1907. As a result of the panic, there was a significant drop in the stock market during this studied period. From this context, we focused on the effect of the financial panic in October 1907 on the log stock price of trust companies using $T_0=217$ pre-treatment periods and $T_1=167$ post-treatment periods, respectively. 

The treated unit and donors were defined as follows. According to \citet{Dataset1907}, Knickerbocker, Trust Company of America, and Lincoln were the three trust companies that were most severely affected during the panic. However, despite the absence of the financial panic, Lincoln's stock price showed a strong downward trend over the pre-treatment period. Therefore, we defined the average of the log stock prices of the first two trust companies as $Y_t$, the outcome of the treated units at time $t=1,\ldots,384$. As for potential donors, \citet{Dataset1907} identified 49 trust companies that had weak financial connections with the aforementioned three severely affected trust companies. However, some of these trust companies seem to violate the relevance condition \eqref{eq-relevant}. Therefore, we chose donors based on the procedure proposed in Section \ref{sec:supp:Donors} of the Supplementary Material, which resulted in $d=24$ trust companies. Accordingly, the log stock prices of these 24 trust companies were defined as $\bW_{\D t}$, the outcome of the donors. We remark that the results remain consistent even when different donors are used; see Section \ref{sec:supp:Data} of the Supplementary Material for details. Following the choice in the simulation study, we chose the function $\bg_t(\potY{t}{0})$ in \eqref{eq-GMM-moment} as time-invariant cubic B-spline bases functions of dimension $2d=48$ where the knots were chosen based on the empirical quantiles of the pre-treatment outcomes.


We first report the ATT estimates under a constant treatment effect model $\tau(t \con \bbeta) = \bbeta$. Similar to Section \ref{sec:Sim}, we compare the same three estimators: the unconstrained OLS synthetic control estimator and SPSC estimators with and without ridge regularization. In addition, we compare these estimators to the standard synthetic control approach proposed by \citet{Abadie2010}, which is essentially the OLS synthetic control estimator with constraints on synthetic control weights to be non-negative and sum to one; we refer to this estimator as OLS-Standard hereafter. The results are summarized in Table \ref{tab:data:ATT}. Interestingly, all four estimators yield similar point estimates of the treatment effect, ranging from $-0.975$ to $-0.830$. According to the 95\% confidence intervals, three estimates uniformly reject the null hypothesis of no treatment effect across time points, suggesting that the financial panic led to a significant decrease in the average log stock price of Knickerbocker and Trust Company of America. We remark that \citet{Abadie2010}'s approach does not provide a standard error or 95\% confidence interval for the ATT. In terms of the length of the confidence interval, SPSC with ridge regularization yields the narrowest confidence interval, followed by SPSC with no regularization, and the approach based on OLS. 

\begin{table}[!htp] 
\renewcommand{\arraystretch}{1.05} \centering
\footnotesize
\setlength{\tabcolsep}{3pt} 
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Statistic} & \multicolumn{4}{c|}{Estimator} \\ \cline{2-5} 
& \multicolumn{1}{c|}{OLS} & \multicolumn{1}{c|}{OLS-Standard} & \multicolumn{1}{c|}{SPSC} & SPSC-Ridge \\ \hline

% Estimate & $-0.906$ & $-0.855$ & $-0.855$ \\ \cline{2-4}
% Asymptotic Standard Error & $0.101$ & $0.115$ & $0.100$ \\ \cline{2-4}
% 95\% Confidence Interval & $(-1.103, -0.709)$ & $(-1.081, -0.629)$ & $(-1.051, -0.660)$ \\ \hline

Estimate & $-0.966$ & $-0.877$ & $-0.975$ & $-0.830$ \\ \hline
Asymptotic Standard Error & $0.108$ & - & $0.101$ & $0.098$ \\ \hline
95\% Confidence Interval & $(-1.177, -0.755)$ & - & $(-1.172, -0.777)$ & $(-1.022, -0.638)$ \\ \hline

\end{tabular} 
\caption{Summary Statistics of the Estimation of the Average Treatment Effect on the Treated.}
\label{tab:data:ATT} 
\end{table}	

Next, we construct the pointwise confidence intervals based on the SPSC approach with ridge regularization using the conformal inference approach in Section \ref{sec:Conformal}. As a competing method, we consider the approach in \citet{Cattaneo2021}. Figure \ref{fig:data:1} provides the visual summary of the result. For the post-treatment period $t=218,\ldots,384$, we find that $\widehat{Y}_{t}^{(0)}$, the predictive value of the treatment-free potential outcome, have similar shapes for all two methods. However, 95\% pointwise prediction intervals behave differently for the two methods. Specifically, we focus on the average width of the prediction intervals over the post-treatment periods. The prediction intervals from the approach proposed by \citet{Cattaneo2021} have an average width of 0.108; in contrast, our method with ridge regularization yields prediction intervals with average widths of 0.046, which is more than 50\% narrower than those from the competing method. The comparison reveals that our method appears to produce tighter predictions of treatment effect trends. Combining results in the simulation study and the data application, we conclude that our approach appears to perform quite competitively when compared to some leading alternative methods in the literature. 


% Figure environment removed

Additionally, for the sake of credibility, we performed the following falsification study. We restricted the entire analysis to the pre-treatment period in which the causal effect is expected to be null. We artificially defined a financial panic time in late July 1907, which is roughly three months before the actual financial panic. This resulted in the lengths of the pre- and post-treatment periods equal to $T_0' = 181$ and $T_1'=36$, respectively. The proposed SPSC estimators with and without ridge regularization resulted in the placebo ATT estimates of $0.001$ and $0.030$ with 95\% confidence intervals of $(-0.009, 0.011)$ and $(-0.029, 0.089)$, respectively. The placebo ATT estimate obtained from the unconstrained OLS estimator was $0.009$ with a 95\% confidence interval of $(-0.020, 0.014)$. All 95\% prediction intervals include the null, consistent with the expectation of no treatment effect in the placebo period. Lastly, the constrained OLS estimator (i.e., OLS-Standard) produced a placebo ATT estimate of $-0.013$, which is also close to zero; however, corresponding statistical inference was not available. Therefore, these results provide no evidence against any of the estimators. In Figure \ref{fig:supp:Conformal Placebo} of the Supplementary Material, we provide a trajectory of the synthetic controls along with 95\% prediction intervals under the placebo treatment. 

In Section \ref{sec:supp:Data} of the Supplementary Material, we provide additional results when different donor pools are used. In brief, all results are similar across choices of donors. 
 
\section{Concluding Remarks}	\label{sec:Conclusion}

In this paper, we propose a novel SPSC approach in which the synthetic control is defined as a linear combination of donors' outcomes whose conditional expectation matches the treatment-free potential outcome in both pre- and post-treatment periods. The model is analogous to a nonclassical measurement error model widely studied in standard measurement error literature. Under the framework, we establish the identification of a synthetic control, and provide an estimation strategy for the ATT. Furthermore, we introduce a method for inferring the treatment effect through pointwise prediction intervals, which remains valid even in the case of a short post-treatment period. We validate our methods through simulation studies and provide an application analyzing a real-world financial dataset related to the 1907 Panic.

As briefly mentioned in the introduction, the proposed SPSC framework has a connection to the single proxy control framework \citep{TT2013_COCA, TTPR2023} developed for i.i.d. data. In particular, \citet{TTPR2023} proposed an approach that relies on a so-called outcome bridge function, which is a (potentially nonlinear) function of outcome proxies. Importantly, an important property of the outcome bridge function is that it is conditionally unbiased for the treatment-free potential outcome. Therefore, the proposed SPSC approach can be viewed as an adaptation of the outcome bridge function-based single proxy control approach to the synthetic control setting, where the outcome bridge function is known a priori to be a linear function of donors' outcomes. In Section \ref{sec:supp:nonparametric full} of the Supplementary Material, we present a general SPSC framework, which is designed to accommodate nonparametric and nonlinear synthetic controls. Therefore, this framework eliminates the need for employing linear synthetic controls and establishes a more direct connection with the outcome bridge function-based single proxy approach presented in \citet{TTPR2023}. Notably, the general SPSC framework addresses the underdeveloped aspect of the synthetic control literature by allowing for various types of outcomes, including continuous, binary, count, or a combination of these. 

In addition to the outcome bridge function-based approach, \citet{TTPR2023} introduced two other single proxy control approaches for i.i.d. sampling. One approach relies on propensity score weighting, eliminating the need for specifying an outcome bridge function. The second approach uses both the propensity score and the outcome bridge function and, more importantly, exhibits a doubly-robust property in that the treatment effect in view is identified if either propensity score or outcome bridge function, but not necessarily both, is correctly specified. Consequently, a promising direction for future research would be to develop new SPSC approaches by extending these single proxy methods to the synthetic control setting. Such new SPSC approaches can be viewed as complementing the doubly-robust proximal synthetic control approach \citep{Qiu2022}. However, such extensions pose significant challenges due to (i) a single treated unit with non-random treatment assignment, (ii) multiple heterogeneous untreated donor units; and (iii) serial correlation and heteroskedasticity due to the time series nature of the data. In particular, non-random treatment assignment undermines the conventional notion of the propensity score, rendering it undefined. Approaches for addressing these challenges and developing corresponding statistical methods will be considered elsewhere. 


% In addition to the outcome bridge function-based approach, \citet{TTPR2023} presented two other single proxy control approaches. One approach relies on propensity score weighting, eliminating the need to specify the outcome bridge function. The other approach uses both propensity score and outcome bridge function, and has a doubly-robust property as it only requires the correct specification of either the propensity score or the outcome bridge function, but not necessarily both, to identify the treatment effect in view. Therefore, one may develop propensity score weighting and doubly-robust SPSC approaches by extending these single proxy control approaches developed under the i.i.d. setting to the synthetic control setting. These new SPSC approaches developed within the single proxy control framework would serve as counterparts to the doubly-robust proximal synthetic control approach \citep{Qiu2022} developed within the proximal synthetic control approach. However, such extensions from the i.i.d. setting to the synthetic control setting are not straightforward because treatment assignment is often perceived to be non-random, and the propensity score is therefore undefined. Consequently, one needs to construct a quantity that plays the same role as the propensity score by accounting for the non-random nature of the treatment. Such alternative SPSC approaches will be considered elsewhere. 

% Key assumptions of these approaches are the existence of valid proxies for the treatment-free potential outcome, and they are sufficient for establishing the identification of the treatment effect in view without the need to specify an additional group of proxies for treatment. Despite the similarity, it is challenging to adapt the framework from the i.i.d. setting to the synthetic control setting because treatment assignment is often perceived as non-random in the latter. 


% We end the paper by providing some possible extensions. First, a recent work by \citet{Qiu2022} considered a doubly robust synthetic control approach under proximal causal inference framework. Notably, the average treatment effect is identified based on a single formula if either the outcome bridge function or the exposure bridge function, but not necessarily both, is correctly specified. In light of their work, we plan to explore a doubly robust synthetic control approach within the single proxy framework as a future direction. Second, \citet{GSC2017} proposed a generalized synthetic control method which can accommodate multiple treated units and variable treatment periods. Although we anticipate that our framework can be extended to accommodate these settings, we defer the specifics to future research.

% \CP{Comparing the above Theorem with Assumption \ref{assumption:SC}, we find that synthetic control $\bW_{\D t}\T \bgamma^*$ behaves as a bridge between the counterfactual moment condition in Assumption \ref{assumption:SC} and the factual moment condition in Theorem \ref{thm:SC}. Due to this reason, the synthetic control is a kind of the bridge function in the single proxy control framework \citep{TTPR2023}. In Section \ref{sec:supp:nonparametric}, we consider a nonparametric SPSC framework by relaxing the parametric form of the synthetic control. The result in Theorem \ref{thm:SC} looks similar to Theorem 2 of \citet{Shi2023SC}, which is equivalent to \eqref{eq-Shi-SC-Moment}, but the notable difference can be found in the variable of the conditioning event. Specifically, the pre-treatment conditional mean-zero restriction is satisfied upon conditioning the proxy variables $\bZ_{ t}$ in \eqref{eq-Shi-SC-Moment} whereas it is satisfied upon conditioning the treatment-free potential outcome $\potY{t}{0}$ in our framework. }
