\section{Introduction}

Synthetic control methods have grown popular for estimating the treatment effect of an intervention in settings where a single unit is treated and pre- and post-treatment time series data are available on the treated unit and a heterogeneous pool of untreated control units \citep{Abadie2003, Abadie2010}. In the absence of a natural control unit, the main idea of the approach hinges upon constructing a so-called synthetic control, corresponding to a certain weighted average of control units' outcomes (and potentially covariates), obtained by matching the outcome time series of the treated unit to the weighted average in the pre-intervention period, to the extent empirically feasible. The resulting synthetic control is then used to forecast the treatment-free potential outcome of the treated unit in the post-treatment period, therefore delivering an estimate of the treatment effect by comparing the treated unit’s outcome to the synthetic control forecast. 

There is a fast-growing literature concerned with developing and improving approaches to constructing synthetic control weights. Following \citet{Abadie2010}, a common approach is to use ordinary (or weighted) least squares by regressing the pre-treatment outcome and available covariates of the treated unit on those of control units, typically restricting the weights to be nonnegative and sum to one; see Section \ref{sec:Review} for a more detailed discussion. Despite intuitive appeal and simplicity, the performance of the standard synthetic control approach may break down in settings where the pre-treatment synthetic control match to the treated unit's outcomes is short of perfect; an eventuality \citet{Abadie2010} warns against. In order to improve the performance of the synthetic control approach in the event of an imperfect pre-treatment match, recent papers have considered alternative formulations of the synthetic control framework. For example, \citet{GSC2017, Amjad2018, ASCM2021, FermanPinto2021, Ferman2021, Shi2023SC} rely on variants of a so-called interactive fixed effects model (IFEM; \citet{Bai2009}). In particular, the latter three papers specify a linear latent factor potential outcome model with an exogenous, common set of latent factors with corresponding unit-specific factor loadings. Under this linear factor model, a key identification condition is that the factor loading of the treated unit lies in the vector space spanned by factor loadings of donor units, and thus, there exists a linear combination of the latter that matches the former exactly. Using the corresponding matching weights, one can therefore construct an unbiased synthetic control of the treated unit's potential outcome which, under certain conditions, can be used to mimic the treated unit's outcome in the post-treatment period, had the intervention been withheld. At their core, these methods substitute the requirement of a perfect pre-treatment match of the outcome of the treated unit and the synthetic control (an empirically testable assumption) with finding a match for the treated unit's factor loadings in the linear span of the donors' factor loadings (an empirically untestable assumption). Despite the growing interest in synthetic control methods, limited research has gone beyond the IFEM or its nonparametric generalizations \citep{Qiu2022, Shi2023SC}; one notable exception is \citet{Blei2022} where the units' outcomes are viewed as averages of more granular study units, allowing for the construction of a synthetic control under specific restrictions on the model of granular study units' outcomes.




% The interactive fixed effects model is popular in many fields such as those in economics due to its compatibility with the context; we refer the readers to Section 2 of \citet{Bai2009} for details. In particular, in the context of the synthetic control framework, the interactive fixed effects model requires the presence of time-specific latent factors that are predictive of both treated and control units' outcomes and unit-specific factor loadings that are constant over time for each unit. Thus, the outcomes can be viewed as proxies of interactive effects, which are linear combinations of the latent factors weighted by the units' factor loading, subject to measurement error. Therefore, confounding between the treated unit's outcome and donors' outcomes arises to the extent that the factor loadings of the treated unit differ from those of donor units

% Then, interactive effects, which are linear combinations of the latent factors weighted by the units' factor loading, are used as key objects to model the data generating mechanism in the synthetic control framework. Specifically, donor units' outcomes can be viewed as proxies of their interactive effects subject to measurement error. The treated unit's outcome is modeled in a similar manner, with the additional consideration of the treatment effect over the post-treatment period; see equation \eqref{eq-IFEM}. In addition, the treatment status of the treated unit at each time point depends on the interactive effects through the latent factors (which are time-specific). Combined, the interactive effects are unmeasured confounders of the causal relationship between treatment and the treated unit's outcome where confounding arises to the extent that the factor loadings of the treated unit differ from those of untreated donor units. 

% Moreover, the treatment and the treatment-free potential outcomes are mean-independent conditional on the latent factors and the factor loadings. Moreover, the treatment and the treatment-free potential outcomes are mean-independent conditional on the latent factors. In other words, the treatment and the treatment-free potential outcomes are unconfounded conditioning on the latent factors, implying that the latent factors are considered as the main source of the unmeasured confounding. Therefore, these latent factors are the key to relating the treated and control units’ treatment-free potential outcomes and to remove the confounding bias.

In this work, we consider an alternative theoretical framework to formalize the synthetic control approach which obviates a specification of an IFEM. Specifically, we propose to view the synthetic control model from a measurement error perspective, whereby donor units' outcomes stand as error-prone proxy measurements of the treated unit's treatment-free potential outcome. In this framework, a synthetic control outcome can be obtained via a simple form of calibration, say a linear combination of donor units, so that on average, it matches the treated unit's outcome in the pre-treatment period. Whereas the standard IFEM views the treated and control units' outcomes as proxies of latent factors, our approach views donor units' outcomes as direct proxies of the treated unit's treatment-free potential outcome. Thus, the proposed framework shares similarity with the recent proximal synthetic control framework of \citet{Shi2023SC}, which also formalizes donor outcomes as so-called outcome proxies. However, a major distinction is that the latter requires an additional group of proxies (so-called treatment proxies) to identify synthetic control weights; in contrast, our proposed approach relies on a single type of proxies, given by donor units and obviates the need to evoke existence of latent factors. 

Interestingly, similar to the connection between the proximal synthetic control approach of \citet{Shi2023SC} and proximal causal inference for independent and identically distributed (i.i.d.) data \citep{Miao2018, TT2020_Intro}, the proposed synthetic control framework is likewise inspired by the control outcome calibration approach \citep{TT2013_COCA} and its recent generalization to a so-called single proxy control framework \citep{SPC2024} both of which were proposed for i.i.d. samples subject to an endogenous treatment assignment mechanism. Therefore, we aptly refer to our approach as single proxy synthetic control (SPSC) approach. Despite this connection, the synthetic control generalization presents several new challenges related to (i) only observing a single treated unit, and therefore treatment assignment is implicitly conditioned on, and (ii) having access to pre-and post-treatment time series data for a heterogeneous pool of untreated donor units, none of which can serve as a natural control; and (iii) serial correlation and heteroskedasticity due to the time series nature of the data. We tackle each of challenges (i)-(iii) in turn and develop a general framework for single proxy control in a synthetic control setting. The proposed method is implemented in an R package available at \url{https://github.com/qkrcks0218/SPSC}.



% The rest of the paper is organized as follows. In Section \ref{sec:Setup}, we introduce notations and review synthetic control methods. In Section \ref{sec:SPSC}, we establish the identification of a synthetic control under the SPSC framework. In turn, we provide the estimation strategy for the average treatment effect on the treated using the generalized method of moments. Moreover, we apply a recent development in the conformal inference approach to our framework to construct pointwise confidence intervals of the treatment effect. As an extension, we extend our framework to nonparametric settings and discuss how to incorporate covariates in the framework. In Sections \ref{sec:Sim} and \ref{sec:Data}, we perform simulation studies investigating the performance of the proposed method, and illustrate the method by revisiting a financial data regarding the 1907 Panic, respectively. Lastly, in Section \ref{sec:Conclusion}, we provide concluding remarks. More detailed discussions and proofs are relegated to Supplementary Material.





\section{Setup And Review of Existing Synthetic Control Frameworks}	\label{sec:Setup}

\subsection{Setup}

Let us consider a setting where $N+1$ units are observed over $T$ time periods. Units and time periods are indexed by $i \in \{0,1,\ldots,N\}$ and $t \in \{1,\ldots,T\}$, respectively. Following the standard synthetic control setting, we suppose that only the first unit with index $i=0$ is treated, whereas the latter $N$ units with index $i \in \{1,\ldots,N\}$ are untreated control units; these untreated control units are also referred to as donors. Consider a binary treatment indicator $A_{t}$ which encodes whether time $t$ is in the pre-treatment period, in which case $A_{t}=0$ for $t\in \{1,\ldots,T_0\}$, or the post-treatment period, in which case $A_{t}=1$ for $t \in \{T_0+1,\ldots,T\}$, respectively. Thus, $T_0$ is the number of pre-treatment periods and $T_1=T - T_0$ is the number of post-treatment periods. Unless otherwise stated, we assume that $N$ is fixed and $T_0$ and $T_1$ are large with similar order of magnitude. Let $Y_{t}$ and $W_{it}$ denote observed outcomes of the treated unit and the $i$th control unit, respectively, for $i \in \{1,\ldots,N\}$. We define $\bW_t = (W_{1t},\ldots,W_{Nt} )\T \in \R^N$ as the $N$-dimensional vector of the untreated units' outcome at time $t$. We define $\bO_t = (Y_t,\bW_t\T,A_t)$ as the observed data at time $t$. Let $\potY{t}{a}$ and $\potW{it}{a}$ denote the potential outcomes of the treated and $i$th control units, respectively, which one would have observed had, possibly contrary to fact, the treatment been set to $A_t=a$ at time $t$. 

For illustrative purposes, we will consider the following two examples throughout:
\begin{example} \label{example-1}
\citet{Abadie2010} investigated the effects of Proposition 99, a tobacco control program implemented in California in 1988, on cigarette sales in the state. Their empirical analysis considered annual cigarette sales data from California and from $N=38$ other states, corresponding to $Y_t$ and $\bW_t$, respectively. The potential outcome $\potY{t}{0}$ represents California's cigarette sales had Proposition 99 not been implemented. The data covered the period from 1970 to 2000, resulting in $T_0=29$  pre-treatment and $T_1=12$ post-treatment time periods. 
\end{example}

\begin{example} \label{example-2} In Section \ref{sec:Data}, we revisited the analysis by  \citet{Dataset1907} to study the effects of the Panic of 1907 \citep{Moen1992} on the average log stock prices of two trust companies (Knickerbocker and Trust Company of America) that were hypothesized to have been impacted by the Panic. For comparison, a selection of $N=49$ trust companies conjectured to be immune to the Panic served as potential control units. The log stock price of these trust companies defines $Y_t$ and $\bW_t$, respectively. The potential outcome $\potY{t}{0}$ represents the average log prices of Knickerbocker and Trust Company of America had the Panic of 1907 not occurred. The tri-weekly panel data consists of $T_0=217$ pre-treatment and $T_1=167$ post-treatment time periods, respectively.
\end{example}

\hspace{\parindent}Throughout, let $\ind(\mathcal{E})$ denote the indicator function of an event $\mathcal{E}$, i.e., $\ind(\mathcal{E})=1$ if $\mathcal{E}$ is satisfied and $\ind(\mathcal{E})=0$ otherwise. Let $\R$ be the set of real numbers. Let $V_1 \indep V_2 \cond V_3$ denote that $V_1$ and $V_2$ are conditionally independent given $V_3$. Conversely, we use $V_1 \nindep V_2 \cond V_3$ to denote that $V_1$ and $V_2$ are conditionally dependent given $V_3$. Let $0_{p \times d}$, $1_{p \times d}$,  and $I_{p \times p}$ denote the $(p \times d)$-dimensional zero matrix, $(p \times d)$-dimensional matrix with ones,  and $(p \times p)$-dimensional identity matrix, respectively. 

\subsection{Review of Existing Synthetic Control Framework}		\label{sec:Review}



A common target estimand in the synthetic control setting is the average treatment effect on the treated unit (ATT) at time $t$ in the post-treatment periods, i.e., 
\begin{align*}
\tau_t^* 
=
\EXP \big\{ \potY{t}{1} - \potY{t}{0} \big\}
\ , 
\quad \quad
t \in \{T_0+1,\ldots,T\} \ . 
\end{align*}
Note that, by definition, $\potY{t}{1} - \potY{t}{0} = \tau_t^* + \nu_t$ for $t \in \{T_0+1,\ldots,T\}$ where $\nu_t$ is a mean-zero idiosyncratic residual error and, therefore, $\tau_t^*$ may be viewed as a deterministic function of time capturing the expected effect of the treatment experienced by the treated unit if one were to average over the residual $\nu_t$. In Section \ref{sec:Conformal}, we describe an approach for constructing prediction intervals for $\potY{t}{1}-\potY{t}{0}$ by appropriately accounting for the idiosyncratic error term $\nu_t$. To proceed, we make the consistency assumption:
\begin{assumption}[Consistency]	\label{assumption:consistency}
$Y_t = \potY{t}{A_t}$ almost surely and $W_{it} = \potW{it}{A_t}$ almost surely for all $i \in \{1,\ldots,N\}$ and $t\in \{1,\ldots,T\}$.
\end{assumption}
\noindent Additionally, we assume no interference, i.e., the treatment has no causal effect on control units.
\begin{assumption}[No Interference on Control Units]
\label{assumption:noitf}
$\potW{it}{0}=\potW{it}{1}$ almost surely for all $i\in \{1,\ldots,N\}$ and $t \in \{1,\ldots,T\}$.
\end{assumption}
\noindent 
In the context of Example \ref{example-1}, Assumption \ref{assumption:noitf} means that Proposition 99 does not have a causal effect on other states' cigarette sales; a similar interpretation applies to Example \ref{example-2}. 

Under Assumptions \ref{assumption:consistency} and \ref{assumption:noitf}, we have the following result almost surely for $t\in \{1,\ldots,T\}$:
\begin{align*}
&
Y_t 
=
\potY{t}{0} (1-A_t)
+
\potY{t}{1} A_t
\ , 
&&
W_{it} 
=
\potW{it}{0}
=
\potW{it}{1}
\ , \quad 
i \in \{1,\ldots,N\}
\ .
\end{align*}
Therefore, for the post-treatment period, $\potY{t}{1}$ matches the observed outcome $Y_t$ while $\potY{t}{0}$ is unobserved, implying that an additional assumption is required to establish identification of the ATT. 

In the classical synthetic control setting, a further assumption relates the observed outcomes of the untreated units with the treatment-free potential outcome of the treated unit. Specifically, following \citet{Abadie2010} and \citet{FermanPinto2021}, suppose that units' outcomes are generated from the following IFEM \citep{Bai2009} for $t \in \{1,\ldots,T\}$:
\begin{align}	\label{eq-IFEM}
Y_{t}
&
=
\text{\makebox[1.25cm]{$ \tau_{t}^* A_t + $}}
\bmu_{0} \T
\blambda_t
+
e_{0t}
\ ,
&&
\EXP \big( e_{0t} \cond \blambda_t ) = 0
\nonumber
\\
W_{it}
&
=
\text{\makebox[1.25cm]{}}
\bmu_{i} \T
\blambda_t
+
e_{it}
\ , 
&&
\EXP \big( e_{it} \cond \blambda_t ) = 0
\ , 
&&
i\in \{1,\ldots,N\} \ .
\end{align}
Here, $\tau_t^*$ is the fixed, non-random treatment effect at time $t$, $\blambda_t \in \R^r$ is a random $r$-dimensional vector of latent factors which are known a priori to causally impact the treated and donor units, despite being unobserved, and can potentially be nonstationary over time, $\bmu_i \in \R^r$ is a time-fixed $r$-dimensional vector of unit-specific factor loadings, and $e_{it}$ is a random error. For identification, it is typically assumed that the number of latent factors $r$ is no larger than the number of donor units $N$ and the pre-treatment period length $T_0$. 
% Under the interactive fixed effect model \eqref{eq-IFEM}, control units' outcomes can be viewed as proxies of interactive effects, $\bmu_i\T \blambda_t$, subject to measurement error $\epsilon_{it}$. The treated unit's outcome is modeled in a similar manner, with the additional consideration of the treatment effect over the post-treatment period. In addition, the treatment status of the treated unit at each time point depends on the interactive effects $\bmu_i\T \blambda_t$ through the latent factors $\blambda_t$. Therefore, interactive effects are unmeasured confounders of the causal relationship between treatment and the treated unit's outcome where confounding arises to the extent that the factor loadings of the treated unit differ from those of untreated donor units, i.e., $\bmu_i \neq \bmu_0$ for $i\in \{1,\ldots,N\}$. 
Combined with Assumptions \ref{assumption:consistency} and \ref{assumption:noitf}, the IFEM \eqref{eq-IFEM} implies $\potY{t}{0}
=
\bmu_{0} \T
\blambda_t
+
e_{0t}$ and $\potY{t}{1}
=
\tau_{t}^* A_t
+
\potY{t}{0}$ where the ATT is represented as $\tau_t^* = \potY{t}{1}-\potY{t}{0}$ for $t\in \{T_0+1,\ldots,T$\}; note that $\potY{t}{1}-\potY{t}{0}$ is non-random under model \eqref{eq-IFEM}. In addition, if there were a donor whose factor loading matched that of the treated unit, i.e., $\bm{\mu}_i=\bm{\mu}_0$ for some $i\in \{1,\ldots,N\}$, then $W_{it}$ would be unbiased for $\potY{t}{0}$ and, therefore, $\potY{t}{1} - W_{it}$ would be unbiased for the ATT. This suggests that confounding bias of the treatment effect on the treated unit's outcome reflects the extent to which donors' factor loadings differ from the treated unit’s.

% Then, interactive effects, which are linear combinations of the latent factors weighted by the units' factor loading, are used as key objects to model the data generating mechanism in the synthetic control framework. Specifically, donor units' outcomes can be viewed as proxies of their interactive effects subject to measurement error. The treated unit's outcome is modeled in a similar manner, with the additional consideration of the treatment effect over the post-treatment period; see equation \eqref{eq-IFEM}. In addition, the treatment status of the treated unit at each time point depends on the interactive effects through the latent factors (which are time-specific). Combined, the interactive effects are unmeasured confounders of the causal relationship between treatment and the treated unit's outcome where confounding arises to the extent that the factor loadings of the treated unit differ from those of untreated donor units, i.e., $\bmu_i \neq \bmu_0$ for $i\in \{1,\ldots,N\}$.

Next, following \citet{FermanPinto2021} and \citet{Shi2023SC}, suppose that a set of weights $\bgamma^{\dagger} = ( \gamma_{1}^{\dagger},\ldots,\gamma_{N}^{\dagger} ) \T$ satisfies
\begin{align}		\label{eq-ExistSC}
\bmu_0
=
\sum_{i =1}^{N} \gamma_{i}^{\dagger} \bmu_i \ .
\end{align}
Equations \eqref{eq-IFEM} and \eqref{eq-ExistSC} imply that there exists a synthetic control $ \bW_{t}\T \bgamma^\dagger = \sum_{i=1}^{N} \gamma_i^\dagger W_{it}$ satisfying 
\begin{align}		\label{eq-SC Equation}
\potY{t}{0}
=
\bW_{t} \T \bgamma^\dagger
+ 
e_{0t} 
-
\sum_{i=1}^{N} \gamma_i^{\dagger} e_{it} 
\ , 
\quad \quad
t\in \{1,\ldots, T\}
\ .
\end{align}
In the context of Example \ref{example-1}, equation \eqref{eq-SC Equation} means that:
\begin{align} \label{example-1-Usual}
\begin{array}{l}	
\textit{The counterfactual measurement of cigarette sales for California}\\
\textit{had, contrary to fact, Proposition 99 not been implemented}\\
\textit{is an error-prone weighted average of cigarette sales in the other 38 states.}
\end{array}
\end{align}
A similar interpretation holds for Example \ref{example-2}.  Therefore, $	\tau_t^*
=
\EXP \big\{ \potY{t}{1} - \bW_{t} \T \bgamma^\dagger \big\}$ for $t\in \{T_0+1,\ldots,T\}$, i.e., $Y_t - \bW_{t}\T \bgamma^\dagger$ is unbiased for the ATT. Unfortunately, it is impossible to obtain $\bgamma^{\dagger}$ from equation \eqref{eq-ExistSC} because the factor loadings $\bmu_i$ are unknown. Importantly, the synthetic control weights satisfying \eqref{eq-ExistSC} naturally accommodate an imperfect pre-treatment fit as shown in \eqref{eq-SC Equation}, i.e., the synthetic control can significantly deviate from the observed pre-treatment fit, however, the corresponding error is mean zero.

Based on \eqref{eq-SC Equation}, one may consider estimating $\bgamma^\dagger$ via penalized least squares minimization, say:
\begin{align}
\label{eq-OLS}
\widehat{\bgamma}_{\text{PLS}}
=
\argmin_{\bgamma}
\bigg\{
\frac{1}{T_0}
\sum_{t=1}^{T_0}
\big(
Y_t - \bW_{t}\T \bgamma
\big)^2
+
\mathcal{R}( \bgamma )
\bigg\} \ ,
\end{align}
where $\mathcal{R} (\bgamma)$ is a penalty which constraints $\bgamma$. For instance, \citet{Abadie2010} restricts the weight to lie within a simplex, meaning that they are non-negative and sum to one, \citet{Doudchenko2016} uses elastic-net penalization, and \citet{Robbins2017} uses entropy penalization. In words, $\widehat{\bgamma}_{\text{PLS}}$ is obtained by fitting a possibly constrained ordinary least squares (OLS) regression of $Y_t$ on $W_{it}$. Importantly, without penalization, the moment restriction solving \eqref{eq-OLS} reduces to $ \EXP \{ \Psi_{\text{OLS}} (\bO_t \con \bgamma) \} = 0$ for $t\in \{ 1, \ldots, T_0\}$ 
% \begin{align*}
% \EXP \big\{ \Psi_{\text{OLS}} (\bO_t \con \bgamma) \big\} = 0 \ , \ t \in \{ 1, \ldots, T_0\} \ , 
% \end{align*}
where $\Psi_{\text{OLS}} (\bO_t \con \bgamma)
=
\bW_{t}
\big( Y_t - \bW_{t}\T \bgamma \big)$ are standard least squares normal equations. 

However, as discussed in \citet{FermanPinto2021} and \citet{Shi2023SC}, the OLS weights obtained from \eqref{eq-OLS} are generally inconsistent under \eqref{eq-ExistSC} as $T_0$ tends to infinity, which can result in biased estimation of the treatment effect unless $e_{it}$ is exactly zero for all $i$ and $t$; see Supplementary Material \ref{sec:supp:OLS} for details. We remark that this result does not conflict with \citet{Abadie2010} because their synthetic control weights are assumed to satisfy a perfect pre-treatment fit; specifically, there exist values $\bgamma^{\#} = (\gamma_{1}^{\#},\ldots,\gamma_{N}^{\#}) \T$ satisfying
\begin{align}		\label{eq-ExistSC-Abadie}
\potY{t}{0}
=
\bW_{t}\T \bgamma^{\#}
\ , 
\quad \quad 
t\in \{1,\ldots,T_0\} \ .
\end{align}
In the context of Example \ref{example-1}, equation \eqref{eq-ExistSC-Abadie} means that:
\begin{align} \label{example-1-Abadie}
\begin{array}{l}	
\textit{The counterfactual measurement of cigarette sales for California}\\
\textit{had, contrary to fact, Proposition 99 not been implemented}\\ 
\textit{is equal to weighted averages of cigarette sales in the other 38 states.}
\end{array}
\end{align}
Example \ref{example-2} follows a similar interpretation. 
Note that \eqref{eq-ExistSC-Abadie} is distinct from condition \eqref{eq-ExistSC} of  \citet{FermanPinto2021} and \citet{Shi2023SC}, as reflected in their interpretations \eqref{example-1-Usual} and \eqref{example-1-Abadie}. Moreover, as discussed in \citet{FermanPinto2021}, \eqref{eq-ExistSC-Abadie} can be expected to hold approximately under \eqref{eq-ExistSC} when the variance of the error $e_{it}$ in \eqref{eq-IFEM} becomes negligible as $T_0$ becomes large. Specifically, in a noiseless setting where $e_{it}=0$ almost surely for all $i \in \{0,1,\ldots,N\}$, \eqref{eq-IFEM} and \eqref{eq-ExistSC} imply \eqref{eq-ExistSC-Abadie} because \eqref{eq-SC Equation} becomes equivalent to \eqref{eq-ExistSC-Abadie}; see \citet{Abadie2010} for related results, and Sections 1 and 3.1 of \citet{FermanPinto2021}, and Section 2 \citet{Shi2023SC} for detailed discussions.

Recently, \citet{Shi2023SC} introduced a proximal causal inference framework for synthetic controls. Specifically, they assume that they have also observed proxy variables $\bZ_{t} = (Z_{1t},\ldots,Z_{Mt})\T$ a priori known to satisfy the following condition in the pre-treatment period:
\begin{align}		\label{eq-Shi-proxy}
\bZ_{t} \indep
\big( Y_t, \bW_{t} \big)
\cond \blambda_t 
\ , 
\quad \quad
t\in \{1,\ldots,T_0\} \ .
\end{align}
A reasonable candidate for $\bZ_{t}$ maybe the outcome of units excluded from the donor pool; see \citet{Shi2023SC} for alternative choices of proxies. Then, under Assumptions \ref{assumption:consistency} and \ref{assumption:noitf}, the IFEM \eqref{eq-IFEM}, condition \eqref{eq-ExistSC}, and the existence of proxies satisfying \eqref{eq-Shi-proxy}, the synthetic control weights $\bgamma^\dagger$ in \eqref{eq-ExistSC} satisfy $\EXP ( Y_t - \bW_{ t}\T \bgamma^\dagger \cond \bZ_{ t} ) = 0$ and $ \EXP \{ \Psi_{\PI}(\bO_t \con \bgamma^\dagger) \} = 0$ for $t\in \{1,\ldots,T_0\}$  where $\Psi_{\PI}(\bO_t \con \bgamma) = \bg  ( \bZ_{ t} ) \big( Y_t - \bW_{ t}\T \bgamma \big)$; here, $\bg$ is a user-specified function of $\bZ_{t}$ with $\text{dim}(\bg) \geq d$. Based on this second result, one can estimate the synthetic control weights as the solution to the generalized method of moments (GMM) \citep{GMM1982}, i.e., $\widehat{\bgamma}_{\PI} 
= (\widehat{\gamma}_{\PI,1}, \ldots, \widehat{\gamma}_{\PI,N}) \T$ is the minimizer of $T_0^{-1} \sum_{t=1}^{T_0}
\big\{ \Psi_{\PI}(\bO_t \con \bgamma) \big\}\T 
\widehat{\Omega}
\big\{ \Psi_{\PI}(\bO_t \con \bgamma) \big\} $ where $\widehat{\Omega}$ is a user-specified symmetric and positive-definite weight matrix. Importantly, in contrast to the OLS-based estimator $\widehat{\bgamma}_{\text{PLS}}$ in \eqref{eq-OLS}, the proximal estimator $\widehat{\bgamma}_{\PI}$ is consistent for $\bgamma^\dagger$. Under certain regularity conditions, \citet{Shi2023SC} established that the resulting GMM estimator of the ATT is consistent and asymptotically normal. For instance, in the special case of constant ATT, i.e., $\tau_t^* = \tau^*$ for all $t\in \{T_0+1,\ldots,T\}$, the estimator $T_1^{-1} \sum_{t=T_0+1}^{T} (Y_t - \bW_{ t}\T \widehat{\bgamma}_{\PI})$ is consistent for $\tau^*$; see Section 3.2 of \citet{Shi2023SC} for details.


\section{Single Proxy Synthetic Control Approach}		\label{sec:SPSC}

\subsection{Assumptions} \label{sec:SPSC:Assumption}

In this section, we provide a novel synthetic control approach which obviates the need for an IFEM, and, in fact, does not necessarily postulate the existence of a latent factor $\blambda_t$. At its core, the approach views the outcomes of the untreated units $W_{it}$ as proxies for the treatment-free potential outcome of the treated unit $\potY{t}{0}$, which is formally stated as follows:
\begin{assumption}[Proxy] \label{assumption:valid proxy}
There exists a function $h^*: \R^N \rightarrow \R$ satisfying 
\begin{align*}
&
h^*( \bW_{t} )  \nindep \potY{t}{0} 
\ , \quad \quad
t\in \{1, \ldots, T_0\} \ . 
\end{align*}
\end{assumption}
Assumption \ref{assumption:valid proxy} encodes that a function of the untreated units' outcomes $\bW_{t}$ is associated with and, therefore, predictive of $\potY{t}{0}$ at time $t\in \{1,\ldots,T_0\}$. In terms of Example \ref{example-1}, Assumption \ref{assumption:valid proxy} means that there exists a function of 38 states' cigarette sales that is associated with cigarette sales in counterfactual California where Proposition 99 was not implemented; a similar interpretation also applies to Example \ref{example-2}. Note that Assumption \ref{assumption:valid proxy} allows for the existence of irrelevant donors among the donor pool, i.e., some untreated units can be independent of $\potY{t}{0}$ as long as the remaining untreated units are associated with the latter. Additionally, we make the following assumption for $h^*$:
\begin{assumption}[Existence of a Synthetic Control Bridge Function] \label{assumption:SC}
For all $t\in \{1,\ldots,T\}$, there exists a synthetic control bridge function $h^*: \R^N \rightarrow \R$ satisfying
\begin{align} \label{eq-SC Counterfactual Eq}
\potY{t}{0} = \EXP \big\{
h^*(\bW_{ t})
\cond
\potY{t}{0}
\big\} \text{ almost surely}.
\end{align}
\end{assumption}
\noindent Assumption \ref{assumption:SC} is the key identification assumption of the SPSC framework. It posits the existence of a synthetic control $h^*(\bW_{ t})$ that is conditionally unbiased for $\potY{t}{0}$. In words, there exists a function of donors $h^*$, possibly nonlinear, whose conditional expectation given $\potY{t}{0}$ recovers $\potY{t}{0}$; the function $h^*$ is a kind of bridge functions \citep{Miao2018, TT2020_Intro}, and we aptly refer to $h^*$ as a \textit{synthetic control bridge function} in this paper. The synthetic control bridge function $h^*$ is a solution to the Fredholm integral equation of the first kind \eqref{eq-SC Counterfactual Eq}, and sufficient conditions for the existence of a solution are well-studied in previous related works developed under i.i.d. settings such as \citet{Miao2018} and \citet{Cui2023}; see Supplementary Material \ref{sec:supp:Exist h} for details. Importantly, Assumption \ref{assumption:SC} may still hold in non-i.i.d. settings, such as when $(\potY{t}{0}, \bW_t)$ is non-stationary; see Supplementary Material \ref{sec:supp:IFEM} for further details.

In particular, if $h^*$ has a linear form, say $h^*(\bW_{ t}) = \bW_{ t}\T \bgamma^*$ for some $\bgamma^* \in \R^N$, the assumption implies the following linear model with an error $\overline{e}_t$:
\begin{align}		\label{eq-MeasurementErrorModel}
\bW_{ t}\T \bgamma^*
=
\potY{t}{0} 
+
\overline{e}_{t}
\ , \quad
\EXP \big\{ \overline{e}_{t} \cond \potY{t}{0} \big\} = 0 \text{ almost surely for  all $t\in \{1,\ldots,T\}$} \ .
\end{align}
Regression model \eqref{eq-MeasurementErrorModel} essentially implies that $\potY{t}{0}$ falls in the linear span of $\EXP \big\{ \bW_{ t} \cond \potY{t}{0} \big\}$, up to a mean zero residual. Thus, Assumption \ref{assumption:SC} may be interpreted as follows for Example \ref{example-1}:
\begin{align} \label{example-1-SPSC}
\hspace*{-0.25cm}
\begin{array}{l}	
\textit{There exists a weighted average of cigarette sales for the 38 donor states}
\\
\textit{which constitutes an error-prone 
 counterfactual measurement of cigarette sales for California}\\
\textit{had, contrary to fact, Proposition 99 not been implemented.}
\end{array}
\end{align} 





Assumption \ref{assumption:SC} plays an analogous role as condition \eqref{eq-ExistSC} in \citet{FermanPinto2021} and \citet{Shi2023SC} and condition \eqref{eq-ExistSC-Abadie} in \citet{Abadie2010} in that it establishes a relationship between $\potY{t}{0}$ and $\bW_t$; however, Assumption \ref{assumption:SC} is fundamentally different from these assumptions. In particular, condition \eqref{eq-ExistSC} implies that the counterfactual outcome $\potY{t}{0}$ is equal to the synthetic control $\bW_t\T \bgamma^*$ plus an error; in contrast, Assumption \ref{assumption:SC} with a linear $h^*$ implies that the synthetic control $\bW_t\T \bgamma^*$ is equal to the couterfactual outcome $\potY{t}{0}$ plus a residual error. This distinction highlights that Assumption \ref{assumption:SC} and condition \eqref{eq-ExistSC} can be viewed as reversed assumptions: they differ in which variable is treated as an error-prone version of the other. Lastly, condition \eqref{eq-ExistSC-Abadie} is a special case of the former two cases where the residual error is assumed to be exactly zero, i.e., noiseless setting. Consequently, in the pre-treatment periods, Assumption \ref{assumption:SC} is strictly weaker than condition \eqref{eq-ExistSC-Abadie} because $\overline{e}_t$ is not necessarily zero. 



Unlike condition \eqref{eq-ExistSC}, Assumption \ref{assumption:SC} obviates the need for latent factors, their corresponding factor loadings, the IFEM \eqref{eq-IFEM}, or any related latent factor models. Instead, Assumption \ref{assumption:SC} simply states that it is possible to construct a function of the control units' outcomes $h^*(\bW_t)$ which is conditionally unbiased for the treatment-free potential outcome of the treated units $\potY{t}{0}$, without requiring assumptions about how these outcomes are generated.
%ETT. THIS DOES NOT MAKE SENSE TO ME, HOW CAN THE POTENTIAL OUTCOME BE EXOGENOUS??? IM TAKING THIS OUT %This perspective can be particularly useful when the counterfactual outcome $\potY{t}{0}$ is considered as an \textit{exogenous} variable generated from agnostic data generating mechanism and $\bW_t$ is viewed as an  \textit{endogenous} variables containing information of $\potY{t}{0}$, up to some error. 
From this viewpoint, $h^*$ in Assumption \ref{assumption:SC} serves as a \textit{bridge function} relating $\bW_t$ and $\potY{t}{0}$ in that $h^*(\bW_t)$ is an error-prone version of $\potY{t}{0}$. This perspective can be illustrated in Example \ref{example-1}: cigarette sales in counterfactual California, had Proposition 99 not been implemented, are viewed as a variable \textit{a priori} determined by an unknown mechanism, while cigarette sales in the other 38 states are seen as error-prone transformations of this counterfactual outcome. Then, Assumption  \ref{assumption:SC} implies that cigarette sales in counterfactual California can be recovered by aggregating these latter variables up to a mean-zero error. 

Moreover, this perspective aligns with existing statistical literature.  In particular, model \eqref{eq-MeasurementErrorModel} is reminiscent of a nonclassical measurement model \citep{Carroll2006, Freedman2008}. From a regression model perspective, the donors' outcomes $\bW_{t}$ and the treated unit's treatment-free potential outcome $\potY{t}{0}$ in model \eqref{eq-MeasurementErrorModel} can be viewed as dependent and independent variables, respectively. This may appear somewhat unconventional at first glance, as some previous synthetic control methods treat $\potY{t}{0}$ and $ \bW_{t} $ as dependent and independent variables, respectively, in estimation of synthetic control weights. To be more precise, they use equation \eqref{eq-OLS} to estimate the synthetic control weights by regressing $\potY{t}{0}$ on $\bW_{t}$ using standard ordinary (or weighted) least squares. However, as model \eqref{eq-MeasurementErrorModel} suggests, our framework is different from previous works in synthetic control and better aligned with regression calibration techniques in measurement error literature \citep{Carroll2006} in that we view the problem as the reverse regression model of $\bW_{t}$ on $\potY{t}{0}$. From this perspective, synthetic control weights $\bgamma^*$ are sought to make the weighted response $\bW_{ t}\T \bgamma^*$ as close as possible to the regressor $\potY{t}{0}$. 

To summarize, the SPSC framework differs from existing synthetic control frameworks in its identifying assumptions and interpretation of the synthetic control. Specifically, in the SPSC framework, the synthetic control is viewed as an error-prone outcome measurement (see \eqref{eq-MeasurementErrorModel}), eliminating the need for a generative model for $Y_t^{(0)}$. In contrast, existing approaches interpret the synthetic control as either the projection of the outcome onto the donor's outcome space (see \eqref{eq-SC Equation} and \eqref{eq-OLS}) or the outcome itself (see \eqref{eq-ExistSC-Abadie}). Despite these differences, both frameworks share key similarities. In both frameworks, synthetic controls are constructed by weighting donor units to optimally match the treated unit during the pre-treatment period, though the matching criteria differ, as previously noted. Furthermore, synthetic controls in both approaches serve as unbiased forecasts of the mean treatment-free potential outcome, $\EXP \big\{ \potY{t}{0} \big\}$, enabling treatment effect estimation by comparing observed outcomes $Y_t$ to the synthetic controls over the post-treatment period. Additionally, like other synthetic control methods, the SPSC framework accommodates time-varying confounders, distinguishing it from difference-in-differences approaches. Most importantly, the SPSC framework is compatible with the IFEM, as shown in the next section. Thus, while the interpretation of the synthetic control differs, most features of existing synthetic control approaches carry over to the SPSC framework.










\subsection{A Generative Model} \label{sec:IFEM discussion}

While, in principle, Assumptions \ref{assumption:valid proxy} and \ref{assumption:SC} do not require a generative model, it is instructive to consider a model compatible with these assumptions. In this vein, suppose that $\potY{t}{0}$ and $W_{it}$ are generated from the following nonparametric structural equation model \citep{Pearl1995} for $t\in \{1,\ldots,T\}$:
\begin{align} \label{eq-SPSC model}
& 
\potY{t}{0} = f_0 ( \blambda_t, e_{0t} ) \ , \quad  \quad  
W_{it} = f_i ( \blambda_t , e_{it} )
\ , \quad 
i\in \{1,\ldots,N\}
\ .
\end{align}
Here, $f_0$ and $f_i$ are structural equations for $\potY{t}{0}$ and $W_{it}$, respectively, $\blambda_t = (\lambda_{1t},\ldots,\lambda_{rt})\T$ is an $r$-dimensional latent factor, and the errors satisfy $e_{it} \indep \blambda_t$ for $i \in \{0,1,\ldots,N\}$ and $e_{0t} \nindep e_{it}$, where the latter condition further strengthens Assumption \ref{assumption:valid proxy} in the sense that $W_{it}$ is relevant for $\potY{t}{0}$ even beyond $\blambda_t$. Figure \ref{fig:DAG} provides graphical representations compatible with Assumption \ref{assumption:valid proxy} and model \eqref{eq-SPSC model}. 

% Figure environment removed


Under model \eqref{eq-SPSC model}, $\potY{t}{0}$ is determined by $\blambda_t$ and $e_{0t}$. Given this relationship, it is natural to consider a sufficient condition of Assumption \ref{assumption:SC} characterized in terms of $\blambda_t$ and $e_{0t}$, say:
\begin{condition} \label{assumption:SC-structural}
For all $t\in \{1,\ldots,T\}$, there exists a function $h^*: \R^N \rightarrow \R$ that satisfies $\potY{t}{0} = f_0(\blambda_t , e_{0t}) = \EXP \big\{
h^*(\bW_{ t})
\cond
\blambda_t, e_{0t}
\big\}$ almost surely.
\end{condition}
\noindent 
Condition \ref{assumption:SC-structural} is a sufficient condition for Assumption \ref{assumption:SC} because, under Condition \ref{assumption:SC-structural}, we obtain $ \EXP \big\{ h^*(\bW_{ t}) \cond \potY{t}{0} \big\} = \EXP \big[ \EXP \big\{ h^*(\bW_{ t}) \cond \blambda_t, e_{0t} \big\} \cond \potY{t}{0} \big] = \potY{t}{0}$. 

Under model \eqref{eq-SPSC model} and Condition \ref{assumption:SC-structural}, consider the special case where $f_i$ is the IFEM \eqref{eq-IFEM}:
\begin{align}
& f_i (\blambda_t, e_{it}) = \bm{\mu}_{i}\T \blambda_t + e_{it} = \sum_{\ell=1}^{r} \mu_{\ell i} \lambda_{\ell t} + e_{it}
\ , \
&&
\EXP\big( e_{it} \big) 
= 0
\ , \
\EXP \big( e_{it} \cond e_{0t} \big) = \omega_{i} e_{0t}  
\ , \
i \in \{ 0, 1,\ldots,N  \} \ .
\label{eq-IFEM2}
\end{align}
Here, $\omega_i$ is a regression coefficient obtained from regressing the $i$th donor's error $e_{it}$ on the treated unit's error $e_{it}$. We remark that $\omega_0 = 1$ and $\omega_{i} \neq 0$ for some $i \in \{1,\ldots,N\}$, encoding $e_{0t} \nindep e_{it}$. Under the IFEM, Condition \ref{assumption:SC-structural} holds with $h^*(\bW_{ t}) = \bW_{ t}\T \bgamma^*$  if $\bgamma^* = (\gamma_{1}^*,\ldots,\gamma_{N}^*)\T$ solves the following linear system:
\begin{align}
\potY{t}{0} 
&
=
\sum_{i=1}^{N}
\gamma_i^*
\EXP \big( W_{it} \cond \blambda_t, e_{0t} \big)
\quad 
\Leftrightarrow
\quad
\begin{bmatrix}
\mu_{10}
\\
\vdots 
\\
\mu_{r0}
\\
\omega_{0}
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
\mu_{1 1} & \mu_{1 2} & \cdots  & \mu_{1 N}
\\
\vdots & \vdots & \ddots & \vdots 
\\
\mu_{r 1} & \mu_{r 2} & \cdots  & \mu_{r N}
\\
\omega_{1} & \omega_{2} & \cdots & \omega_{N}
\end{bmatrix}
}_{=\mathcal{A}}
\begin{bmatrix}
\gamma_{1}^*
\\
\gamma_{2}^*
\\
\vdots 
\\
\gamma_{N}^*
\end{bmatrix}  \ .
\label{eq-LinearModel-System}
\end{align}
A sufficient condition for the existence of the weight $\bgamma^*$ is that the matrix $\mathcal{A}$ is of full row rank, which is satisfied under the following sufficient (but not necessary) conditions: (i) $r < N$, i.e., the number of donors $N$ is greater than the number of latent factors, and (ii) the factor loadings $\bm{\mu}_i$ are linearly independent. If the matrix $\mathcal{A}$ is square and invertible, $\bgamma^*$ is uniquely determined. This observation informs that a linear synthetic control satisfying Condition \ref{assumption:SC-structural}, and thus Assumption \ref{assumption:SC}, is likely to exist when the errors are correlated and there are sufficient number of donors, regardless of the distribution of the latent factors and errors.

Since equation \eqref{eq-LinearModel-System} is based on the IFEM, it has interesting connections with previous works that also rely on this model. In order to elucidate these connections, we consider the following alternative representation of equation \eqref{eq-LinearModel-System}:
\begin{align} \label{eq-SingleProximal}
\widetilde{\bmu}_0 = \sum_{i=1}^{N} \gamma_i^* \widetilde{\bmu}_i   \ ,
\end{align}
where $\widetilde{\bmu}_i = \big( \mu_{1i},\ldots,\mu_{ri}, \omega_i \big)\T$ for $i \in \{0,1,\ldots,N\}$. 
As the expression itself indicates, condition \eqref{eq-SingleProximal} is similar to condition \eqref{eq-ExistSC}, a condition used in \citet{FermanPinto2021} and \citet{Shi2023SC}, but there is a notable difference between \eqref{eq-ExistSC} and \eqref{eq-SingleProximal} in how they handle errors $e_{it}$. Specifically, in condition \eqref{eq-SingleProximal}, one can address the residual errors $e_{it}$ by accommodating the regression coefficients $\omega_{i}$ as a component of the unit-specific factor loadings $\widetilde{\bmu}_i$. In contrast, condition \eqref{eq-ExistSC} does not account for these errors. Consequently, \eqref{eq-SingleProximal} implies \eqref{eq-ExistSC} because $\bm{\mu}_i$ is a subvector of $\widetilde{\bmu}_i$, indicating that \eqref{eq-SingleProximal} is a stronger condition than \eqref{eq-ExistSC}. However, as stated in Theorem \ref{thm:SC} in Section \ref{sec:SPSC:id}, it is crucial to note that this stronger condition is offset by not requiring an additional condition for establishing identification of the synthetic control weight $\bgamma^*$. In other words, condition \eqref{eq-SingleProximal} alone is sufficient for identification of $\bgamma^*$. On the other hand, condition \eqref{eq-ExistSC} fails to do so, necessitating additional assumptions for identification of $\bgamma^*$, as exemplified by \citet{FermanPinto2021} and \citet{Shi2023SC}. Specifically, \citet{FermanPinto2021} requires either (i) $\VAR(e_{it})=0$ for all $i \in \{0,1,\ldots,N\}$, meaning a noiseless setting, or (ii) $\bgamma^*$ is a minimizer of $\mathcal{V}(\bgamma) = \EXP \{ (e_{0t} - \sum_{i=1}^{N} \gamma_{i} e_{it} )^2 \big\}$, the variance of a linear combination of error terms appearing in \eqref{eq-SC Equation}; see Propositions 1 and 2 of \citet{FermanPinto2021} for details. Interestingly, under (i), all $\omega_{it}$ can be taken as zero, and \eqref{eq-SingleProximal} becomes equivalent to \eqref{eq-ExistSC}, the assumption made by \citet{FermanPinto2021} and \citet{Shi2023SC}. Lastly, in the degenerate case where $\potY{t}{0}$ and $\bW_{t}$ share the same error, i.e., $e_{0t}=e_{1t}=\cdots=e_{N t}$ almost surely, condition \eqref{eq-SingleProximal} implies the perfect fit condition, i.e., condition \eqref{eq-ExistSC-Abadie}, in which case the unconstrained OLS weights \eqref{eq-OLS} are consistent as $T_0$ tends to infinity. 

While the IFEM with correlated errors in \eqref{eq-IFEM2} is useful for motivating the SPSC framework, the standard IFEM typically assumes no correlation among errors, i.e., $\omega_{i}=0$ for all $i\in \{1,\ldots,N\}$ in \eqref{eq-IFEM2}. When the errors are uncorrelated, the solution to equation \eqref{eq-LinearModel-System} may not exist, implying that no linear single proxy synthetic control bridge function satisfies Condition \ref{assumption:SC-structural}. This may suggest that the SPSC framework may not be compatible with a standard IFEM. However, a linear synthetic control bridge function satisfying Assumption \ref{assumption:SC} may still exist under the IFEM with uncorrelated errors, while Condition \ref{assumption:SC-structural} is violated; this is because Condition \ref{assumption:SC-structural} is not a necessary condition of Assumption \ref{assumption:SC}. With additional assumptions regarding the latent factors and errors, it is possible to conceive of a reasonable scenario where the SPSC framework remains valid within the standard IFEM. For instance, if $\blambda_t$ and $\bm{e}_t = (e_{0t}, e_{1t}, \ldots, e_{Nt})\T$ follow multivariate normal distributions with homoskedastic variances, specifically $\blambda_t \sim N_r (\bm{\nu}_t, \Sigma_{\lambda})$ and $\bm{e}_t \sim N_{N+1}(0_{(N+1)\times1}, \Sigma_{e})$, then a linear synthetic control satisfying Assumption \ref{assumption:SC} exists even when $\Sigma_{e}$ is a diagonal matrix; see Supplementary Material \ref{sec:supp:IFEM} for details.  In essence, such circumstances may arise because, despite the uncorrelated errors, $\bW_{t}$ and $\potY{t}{0}$ remain associated through the latent factors $\blambda_t$, allowing for the possibility of a linear single proxy synthetic control to exist; see Figure \ref{fig:DAG} (c) for a graphical illustration. In summary, while uncorrelated errors may undermine the plausibility of the SPSC framework, it can still be valid if certain conditions on $(\blambda_t, \bm{e}_t)$ are met such as the normality assumption.
 





%To illustrate Assumptions \ref{assumption:valid proxy} and \ref{assumption:SC}, we consider the following simple data generating mechanism:
%\begin{align}		\label{eq-MeasurementErrorModel}
%&
%W_{it} = \alpha_{i0} + \alpha_{i Y} \potY{t}{0} + e_{it} 
%\ , \quad \quad
%\EXP \big\{ e_{it} \cond \potY{t}{0} \big\} = 0 
%\ , \quad \quad
%i \in \D 
%\ , \quad \quad
%t\in \{ 1,\ldots,T\} \ .
%\end{align}
%Here, $\alpha_{i0}$ and $\alpha_{iY}$ are fixed constants with constraints $\alpha_{iY} \neq 0$, which encode Assumption \ref{assumption:valid proxy}. This regression model is reminiscent of the nonclassical measurement model in measurement error literature \citep{Carroll2006, Freedman2008}. In particular, if $\alpha_{i0}=0$ and $\alpha_{iY}=1$, one recovers the classical measurement error model $W_{it}=\potY{t}{0} + e_{it}$ with conditional mean-zero error which has been studied extensively in measurement error literature. Note also that Assumption \ref{assumption:SC} implies certain constraints on $\bgamma^*$, namely $\sum_{i \in \D} \gamma_i^* \alpha_{i0} = 0$ and $\sum_{i \in \D} \gamma_i^* \alpha_{iY} = 1$; clearly, multiple $\bgamma^*$ may satisfy these two constraints if the number of donors $d$ is greater than 2. From a regression model perspective, the donors' outcomes $\bW_{it}$ and the treated unit's treatment-free potential outcome $\potY{t}{0}$ in model \eqref{eq-MeasurementErrorModel} can be viewed as dependent and independent variables, respectively. This may appear somewhat unconventional at first glance, as some previous synthetic control methods treat $\potY{t}{0}$ and $ \bW_{it} $ as dependent and independent variables, respectively, in estimation of synthetic control weights. To be more precise, they use equation \eqref{eq-OLS} to estimate the synthetic control weights by regressing $\potY{t}{0}$ on $\bW_{it}$ using standard ordinary (or weighted) least squares. However, as model \eqref{eq-MeasurementErrorModel} suggests, our framework is different from previous works in synthetic control and better aligned with regression calibration techniques in measurement error literature \citep{Carroll2006} in that we view the problem as the reverse regression model of $\bW_{it}$ on $\potY{t}{0}$. From this perspective, synthetic control weights $\bgamma^*$ are sought to make the weighted response $\bW_{it}\T \bgamma^*$ as close as possible to the regressor $\potY{t}{0}$. 

\subsection{Identification of the Synthetic Control and the Treatment Effect} \label{sec:SPSC:id}

As a direct consequence of Assumptions \ref{assumption:consistency}, \ref{assumption:noitf}, \ref{assumption:valid proxy}, and \ref{assumption:SC}, the synthetic control bridge function $h^*$ can be represented as a solution to the moment equation given in the following result:
\begin{theorem}	\label{thm:SC}
Under Assumptions \ref{assumption:consistency}, \ref{assumption:noitf}, \ref{assumption:valid proxy}, and \ref{assumption:SC}, the synthetic control bridge function $h^*$ satisfy $\EXP \big\{ h^* ( \bW_{ t} ) \cond Y_t \big\} = Y_t $ almost surely for $t\in \{1,\ldots,T_0\}$.
\end{theorem}
The proof of the Theorem, as well as all other proofs, are provided in Supplementary Material \ref{sec:supp:proof}. Theorem \ref{thm:SC} motivates our approach for estimating the synthetic control bridge function $h^*$, as it only involves the observed data. Another consequence of Assumptions \ref{assumption:consistency}, \ref{assumption:noitf}, \ref{assumption:valid proxy}, and \ref{assumption:SC}, is that, as formalized in Theorem \ref{thm:ATT} below, the synthetic control bridge function $h^*(\bW_{ t})$ can be used to identify $\tau_t^*$:
\begin{theorem}	\label{thm:ATT}
Under Assumptions \ref{assumption:consistency}, \ref{assumption:noitf}, \ref{assumption:valid proxy}, and \ref{assumption:SC}, we have that $\EXP \big\{ \potY{t}{0} \big\} = \EXP \big\{ h^*(\bW_{ t}) \big\}$ for any $t\in \{1,\ldots,T\}$. Additionally, the ATT is identified as $	\tau_t^*
=
\EXP
\big\{
Y_t - h^*(\bW_{ t})
\big\}$ for $t\in \{T_0+1,\ldots,T\}$.
\end{theorem}
Theorem \ref{thm:ATT} provides a theoretical basis for the use of the synthetic control method to estimate the ATT. Specifically, following \citet{Abadie2003} and \citet{Shi2023SC}, we use $Y_t - h^*(\bW_{ t})$ in a standard time series regression where the ATT is identified as the deterministic component of the decomposition $Y_t - h^*(\bW_{ t}) = \tau_t^* + \epsilon_t$, with $\epsilon_t$ representing a mean-zero error. The following Sections elaborate on this approach, first describing how the identification result leads to an estimator of the synthetic control. 


To facilitate the exposition, hereafter in the main text, we restrict attention to inference under a linear bridge function, i.e., $h^*(\bW_{ t} ) = \bW_{ t} \T \bgamma^*$, while allowing for the possibility for $\bgamma^*$ not to be unique. In Supplementary Material \ref{sec:supp:nonparametric full}, we present the more general case where $h^*$ is nonparametric.






\subsection{Estimation and Inference of the Treatment Effect Under a Linear Bridge Function}		\label{sec:Estimation}

We first discuss estimation of the synthetic control weights $\bgamma^*$. We consider the following time-invariant estimating function for the pre-treatment periods:
\begin{align}	\label{eq-GMM-moment-old}
\Phi_{\pre} (\bO_t \con \bgamma)
=
\bh (Y_t)
\big( Y_t - \bW_{ t} \T \bgamma \big)
\ , \quad \quad
t \in \{1,\ldots,T_0\} \ .
\end{align}
Here, $\bh: \R \rightarrow \R^p$ is a $p$-dimensional user-specified function of the treated unit's outcome. Theorem \ref{thm:SC} implies that the estimating function $\Phi_{\pre}$ satisfies $\EXP \{ \Phi_{\pre} (\bO_t \con \bgamma^*) \} = 0$ for $t\in \{1,\ldots,T_0\}$, indicating that the estimating function $\Phi_{\pre}$ can be used to obtain an estimator of $\bgamma^*$. An important remark on $\bh$ is that the dimension of $\bh$ can be smaller than the number of donors, i.e., $p < N$. Therefore, $\bh$ can be specified as a simple function, e.g., $\bh(y) = y$.

It is instructive to note that solving the estimating equation $\EXP \big\{ \Phi_{\pre}(\bO_t \con \bgamma) \big\}=0$ has a close connection to performing an instrumental variable regression. To illustrate this, consider a simple setting where $\bh(y) = y$ and $N = 1$, along with an alternative form of model \eqref{eq-MeasurementErrorModel} for $t \in \{ 1, \ldots, T_0\}$:
\begin{align}	\label{eq-IV interpretation}
&
W_t \gamma^*
=
Y_t
+
\overline{e}_{t}
&&
\Leftrightarrow
&&
Y_t
=
W_t \gamma^*
-
\overline{e}_{t}
\ , \quad
\EXP \big( \overline{e}_{t} \cond Y_t \big) = 0 \text{ almost surely}.
\end{align}
One might attempt to interpret the model on the right-hand side as a standard regression model, treating $Y_t$ as the response variable and $W_t$ as the explanatory variable. However, such an interpretation would not be correct, as the error term $-\overline{e}_t$ is orthogonal to the response variable $Y_t$. Instead, the right-hand side model exhibits the following properties: (i) the error term $-\overline{e}_{t}$ is correlated with $W_t$ (as induced from the left-hand side model), making $W_t$ an endogenous explanatory variable on the right-hand side model; (ii) the error $-\overline{e}_{t}$ is orthogonal to $Y_t$; and (iii) $Y_t$ is correlated with $W_t$ under Assumption \ref{assumption:valid proxy}. Thus, $Y_t$ can serve as an instrumental variable for $W_t$, allowing for an instrumental variable regression estimator, where $Y_t$ and $W_t$ are used as the instrument and the endogenous explanatory variable, respectively. This estimator is given by $\widehat{\gamma}_{\text{IV}} =  \big( T_0^{-1} \sum_{t=1}^{T_0}  Y_t W_t \big)^{-1} \big( T_0^{-1} \sum_{t=1}^{T_0}  Y_t^2 \big)$. Notably, $\widehat{\gamma}_{\text{IV}}$ is consistent for $\gamma^* =  \big\{ T_0^{-1} \sum_{t=1}^{T_0} \EXP ( Y_t W_t ) \big\}^{-1} \big\{ T_0^{-1} \sum_{t=1}^{T_0} \EXP ( Y_t^2) \big\}$ under some conditions, which is the solution to the estimating equation $\EXP \big\{ \Phi_{\pre} (\bO_t \con \bgamma) \big\} = 0$.  The case for a general $\bh$ and multiple donors can be understood in a similar manner, with the main difference being the use of multiple instrumental variables $\bh(Y_t) \in \R^p$ and multiple explanatory variables $\bW_t \in \R^N$.

The choice of $\bh$ affects the efficiency of the corresponding estimator of $\bgamma^*$ and the treatment effect parameter $\bbeta^*$, which we later define in this section; see Section 2 of \citet{Donald2009} for a similar discussion. Therefore, one could theoretically select the optimal $\bh$ from a set of candidates that minimizes the asymptotic variance of the estimators, thereby maximizing efficiency. For example, $\bh$ can be selected from basis functions such as polynomials up to the $p$th power, where $p$ is determined to minimize the asymptotic variance of the estimators of $(\bgamma^*, \bbeta^*)$; other examples of basis functions include truncated polynomial bases, Fourier basis functions, splines, or wavelets such as the Haar basis; see \citet{XiaohongChen2007} and references therein for more details on how to choose the optimal $\bh$ over a basis function space. However, selecting the optimal $\bh$ can be computationally intensive, and despite this burden, it may yield only marginal gains in efficiency. From a practical standpoint, we use a simple specification for $\bh$, namely the identity function $\bh(y)=y$, leading to $p=1$. In the simulation studies and data analysis, this simple choice of $\bh$ performs well and produces reasonable results compared to competing methods in settings we consider, although we cannot guarantee this to be the case in all settings one might face in practice.



A time-invariant specification of $\bh$ may sometimes lead to poorly behaved estimates of synthetic control weights, particularly in scenarios where the outcomes exhibit nonstationary behavior. To address this, the estimating function can be adapted to accommodate secular trends as follows:
\begin{align}	\label{eq-GMM-moment}
\Psi_{\pre} (\bO_t \con \Beta,  \bgamma)
=
\begin{bmatrix}
\bD_t \big( Y_t - \bD_t\T \Beta \big)
\\
\bg(t, Y_t \con \Beta) \big( Y_t - \bW_t \T \bgamma \big)
\end{bmatrix}
\ , &&
\bg(t,y \con \Beta)
=
\begin{bmatrix}
\bD_t \\ \bh(y - \bD_t\T \Beta)
\end{bmatrix}
\ , &&
t \in \{ 1,\ldots,T_0\} \ .
\end{align}
Here, $\bD_t \in \R^{d}$ is a $d$-dimensional vector of basis functions to de-trend nonstationary behaviors of the outcomes. We assume that $\bD_t$ is selected such that there exists a unique vector $\Beta^*$ satisfying $\EXP \big\{ \potY{t}{0} \big\} = \bD_{t}\T \Beta^*$ for $t \in \{1,\ldots,T_{0}\}$, meaning that the time trend of $\potY{t}{0}$ over the pre-treatment period is correctly specified by the regression model spanned by $\bD_{t}$. The selection of $\bD_t$ can be evaluated by examining the residuals from regressing $Y_t$ on $\bD_{t}$ over the pre-treatment period. For instance, to account for a linear trend, one might select $\bD_t = (1, t/T_0)\T$, where these terms account for an intercept and the drift of a nonstationary process. Alternatively, one could choose $\bD_t = \mathcal{B}_d(t)$, the $d$-dimensional cubic B-spline function, to capture nonlinear trends. While $\bD_{t}$ could also be specified as a dummy vector---allowing each component of $\Beta^*$ to represent time fixed effects for each pre-treatment period---this approach may result in an inconsistent ATT estimator. To ensure valid inference of the ATT while reducing the risk of misspecification, we recommend using a cubic B-spline basis of small to moderate dimension. In both our simulation studies and real-world analysis (Sections \ref{sec:Sim} and \ref{sec:Data}), we used a 6-dimensional cubic B-spline basis, which demonstrated reasonable performance.

The function $\bg: [0,\infty) \otimes \R \rightarrow \R^{d+p}$ can be seen as a basis function for both outcome and time period. Note that the dimension of $\bg$ may be smaller than $N$, which may arise from simple specifications of $\bD_t$ and $\bh$. For instance, we specify $\bh(y)=y$ and $\bD_t=(1,t/T_0)\T$, the dimension of $\bg$ is then equal to three, which may be substantially smaller than the number of untreated units $N$. 

The time-varying estimating function $\Psi_{\pre}$ satisfies $\EXP \{ \Psi_{\pre} (\bO_t \con \Beta^* , \bgamma^*) \} = 0$ for $t\in \{1,\ldots,T_0\}$ under Assumptions \ref{assumption:consistency}, \ref{assumption:noitf}, \ref{assumption:valid proxy}, and \ref{assumption:SC}. This ensures that an estimator of $\bgamma^*$ can be obtained by using the time-varying estimating function $\Psi_{\pre}$ rather than the time-invariant estimating function $\Phi_{\pre}$. In fact, incorporating the time-varying term can potentially enhance the finite sample performance of the proposed estimator in the presence of nonstationary behavior. For instance, under the IFEM \eqref{eq-IFEM2}, we show that incorporating time-varying components reduces the bias of the estimator of $\bgamma^*$ when the latent factor $\blambda_t$ exhibits a secular trend; see Supplementary Material \ref{sec:supp:DT} for this result. Moreover, simulation studies in Section \ref{sec:Sim} suggest that including time-varying components can help reduce bias in the presence of a time trend. Therefore, in the remainder of the paper, we use the time-varying estimating function $\Psi_{\pre}$ unless stated otherwise.

An estimator of $\bgamma^*$ can in principle be obtained based on the empirical counterpart of the moment condition. Because the dimension of $\Psi_{\pre}$ is allowed to be smaller than the dimension of $(\Beta^*, \bgamma^*)$,  standard GMM theory \citep{GMM1982} does not readily apply; typically, standard GMM requires the number of moment equations to be greater or equal to the number of unknown parameters. To regularize the problem, we include a ridge penalty term for $\bgamma^*$. Specifically, the regularized GMM estimator $\widehat{\bgamma}_{\rho}$ with regularization penalty ${\rho} \in (0,\infty)$ is defined as the solution to the following minimization problem:
\begin{align}		\label{eq-GMM-Formula}
&
\big( \widehat{\Beta}, \widehat{\bgamma}_{\rho} \big)
=
\argmin_{(\Beta,\bgamma)}
\Big[
\big\{ 
\widehat{\Psi}_{\pre} ( \Beta , \bgamma ) 
\big\} \T
\widehat{\Omega}_{\pre}
\big\{ 
\widehat{\Psi}_{\pre} ( \Beta , \bgamma ) 
\big\}
+
{\rho} \big\| \bgamma \big\|_2^2
\Big]
\ .    
\end{align}
Here, $\widehat{\Psi}_{\pre} ( \Beta , \bgamma )  = T_{0}^{-1}
\sum_{t=1}^{T_0} 
\Psi_{\pre} (\bO_t \con \Beta , \bgamma) $ is the empirical mean of the estimating function over the pre-treatment periods evaluated at $(\Beta, \bgamma)$. Also, $\widehat{\Omega}_{\pre} =  \text{diag}(I_{d \times d} , \widehat{\Omega}_{\bg} ) \in \R^{(2d+p)\times(2d+p)}$ is a user-specified symmetric positive definite block-diagonal matrix with $\widehat{\Omega}_{\bg} \in \R^{(d+p) \times (d+p)}$, which can simply be set to the identity matrix. Since the first block of $\widehat{\Omega}_{\pre}$ is the identity matrix, 
$\widehat{\Beta}$ reduces to the OLS estimator, i.e., $\widehat{\Beta}
=
\big( \sum_{t=1}^{T_0} \bD_t\bD_t\T \big)^{-1} 
\big( \sum_{t=1}^{T_0} \bD_tY_t \big) $.

Equations \eqref{eq-GMM-moment} and \eqref{eq-GMM-Formula} fortunately admit closed-form solutions. For instance, if $\widehat{\Omega}_{\pre}$ is the identity matrix, we have $\bgamma^* = \bG_{YW}^{*+}\bG_{YY}^* + \zeta_{YW}$ and $\widehat{\bgamma}_{\rho} = ( \widehat{\bG}_{YW}\T \widehat{\bG}_{YW} + {\rho} I_{N \times N} )^{-1} \widehat{\bG}_{YW}\T \widehat{\bG}_{YY}$ where 
\begin{align} \label{eq-Gyw Gyy}
&
\bG_{YW}^* = \frac{1}{T_0} \sum_{t=1}^{T_0} \EXP \big\{ \bg(t, Y_t \con \Beta^*) \bW_{ t} \T \big\} \in \R^{(d+p) \times N}
\ , 
&&
\bG_{YY}^* = \frac{1}{T_0} \sum_{t=1}^{T_0} \EXP \big\{ \bg(t, Y_t \con \Beta^*) Y_t \big\} \in \R^{d+p}
\ , \
\nonumber
\\
&
\widehat{\bG}_{YW} =
\frac{1}{T_0} 
\sum_{t=1}^{T_0} \bg(t, Y_t \con \widehat{\Beta}) \bW_{ t}\T \in \R^{(d+p) \times N}
\ , 
&&
\widehat{\bG}_{YY} = 
\frac{1}{T_0} \sum_{t=1}^{T_0} \bg(t, Y_t \con \widehat{\Beta}) Y_t \in \R^{d+p} \ .
\end{align}
Here, $M^+$ denotes the Moore-Penrose inverse of a matrix $M$, and $\zeta_{YW}$ is an arbitrary vector in the null spaces of $\bG_{YW}^*$, i.e., $\bG_{YW}^* \zeta_{YW}=0$. In general, $\bgamma^*$ may not be unique unless $\bG_{YW}^*$ is of full column rank, in which case $\bgamma^*$ is uniquely determined by $\bgamma^* = \big( \bG_{YW}\sT \bG_{YW}^* \big)^{-1} \bG_{YW}\sT \bG_{YY}^*$. However, when the number of untreated units $N$ is large, a common scenario in many synthetic control settings, the full column rank condition of $\bG_{YW}^*$ may not be met, making $\bgamma^*$ not unique. 





A special instance of $\bgamma^*$ is the minimum-norm solution, denoted by $\bgamma_{0}^*= \bG_{YW}^{*+}\bG_{YY}^*$, which corresponds to $\zeta_{YW}=0$. Even if $\bgamma^*$ is not unique, $\bgamma_{0}^*$ remains unique. Moreover, under certain conditions, $\widehat{\bgamma}_{\rho}$ is consistent for $\bgamma_0^*$ as the number of pre-treatment periods $T_0$ goes to infinity and the regularization parameter ${\rho}$ decreases at a sufficiently fast rate. In other words, $\widehat{\bgamma}_\rho$ uniquely converges to $\bgamma_0^*$, allowing us to rely on standard GMM theory as if $\bgamma_0^*$ were the unique solution to the estimating equation. Consequently, we can infer the treatment effect based on the synthetic control with estimated weights $\widehat{\bgamma}_{\rho}$. 

Once the synthetic control weights are estimated, one could in principle estimate the treatment-free potential outcome and the ATT as $\widehat{Y}_{t}^{(0)} = \bW_{ t}\T \widehat{\bgamma}_{\rho}$ and $\widehat{\tau}_t = Y_t - \bW_{ t}\T \widehat{\bgamma}_{\rho}$, respectively, for $t \in \{ T_0 + 1,\ldots,T\}$. Unfortunately, without additional assumptions, it is impossible to perform inference of the ATT $\tau_t^*$ based on $\widehat{\tau}_t$ because the latter will generally fail to be consistent given that we only have access to one observation for each $t$. An alternative is to infer the random treatment effects $\xi_t^* = Y_t^{(1)} - Y_t^{(0)}$ based on pointwise prediction intervals, obviating the need for consistency of $\widehat{\tau}_t$; see Section \ref{sec:Conformal} for details. However, for the remainder of this section, we maintain our focus on inference about the ATT.

We posit a parsimonious working model for the ATT as a function of time. Specifically, we assume that the ATT follows a model indexed by a $b$-dimensional parameter $\bbeta$ via a function $\tau(\cdot \con \cdot): [0,\infty) \otimes \R^{b} \rightarrow \R$. Let $\bbeta^* \in \R^{b}$ be the true parameter satisfying $\tau_t^* 
=
\tau ( t \con \bbeta^* )$ for $t\in \{1,\ldots,T\}$. This parametrization allows us to pool information over time in the post-treatment period to infer $\bbeta^*$ and the ATT. Possible forms for $\tau(t \con \bbeta)$ are given below:
\begin{example}[\textit{Constant Effect}] 
$\tau(t \con \bbeta) = \beta $; this model is reasonable if the treatment yields an immediate, short-term effect which persists over a long period of time.		
\end{example}
\begin{example}[\textit{Linear Effect}] 
$\tau(t \con \bbeta) = \beta_0 + \beta_1 (t- T_0)_+/T_1$ where $(c)_+ = \max(c,0)$ for a constant $c$; this model is appropriate if the treatment yields a gradual, increasing effect over time.
\end{example}
\begin{example}[\textit{Nonlinear Effect}] 
This includes a quadratic model $\tau(t \con \bbeta) = \beta_0 + \beta_1 (t- T_0)_+/T_1 + \beta_2 (t - T_0)_+^2/T_1$, or an exponentially time-varying treatment model $\tau(t \con \bbeta) = \exp \big\{ \beta_0 + \beta_1 (t-T_0)_+/T_1 \big\}$, or a model spanned by nonlinear basis functions, e.g., $\tau(t \con \bbeta) = \mathcal{B}_b\T(t) \bm{\beta} $ where $\mathcal{B}_b(t)$ is the $b$-dimensional cubic B-spline function; this model is appropriate if the treatment yields a nonlinear effect over time.
\end{example}

For tractable inference, we assume that the error process is weakly independent, which is formally stated as follows:
\begin{assumption}[Weakly Dependent Error]	 		\label{assumption:weakdep}
Let $\epsilon_t = Y_t - \bW_{ t} \T \bgamma_0^* - \tau(t \con \bbeta^*)$ for $t\in \{1,\ldots,T\}$. Then, the error process $\{ \epsilon_{1},\ldots,\epsilon_{T} \}$ is weakly dependent, i.e., $\text{corr}(\epsilon_{t}, \epsilon_{t+t'})$ converges to 0 as $t' \rightarrow \pm \infty$.
\end{assumption}
\noindent Assumption \ref{assumption:weakdep} applies to many standard time series models, including autoregressive (AR) models, moving-average (MA) models, and autoregressive moving-average (ARMA) models. 

Along with these conditions, we will consider an asymptotic regime where $T_0, T_1 \rightarrow \infty$ and $T_1/T_0 \rightarrow r \in (0,\infty)$. Specifically, let $\Psi (\bO_t \con \Beta, \bgamma, \bbeta)$ be the following $(2d+p+b)$-dimensional estimating function:
\begin{align*}
% \label{eq-Moment GMM}
\Psi (\bO_t \con \Beta, \bgamma, \bbeta)
=
\begin{bmatrix}
\Psi_{\pre} (\bO_t \con \Beta , \bgamma)
\\
\Psi_{\post} (\bO_t \con \bgamma , \bbeta)
\end{bmatrix}
=
\begin{bmatrix}
(1-A_t)
\bD_t \big( Y_t - \bD_t\T \Beta \big)
\\
(1-A_t)
\bg (t,Y_t \con \Beta)
\big( Y_t - \bW_{ t} \T \bgamma \big)
\\
A_t 
\frac{\partial \tau(t \con \bbeta) }{\partial \bbeta } 
\big\{
Y_t - \bW_{ t} \T \bgamma - \tau (t \con \bbeta)
\big\}
\end{bmatrix}
\in \R^{2d+p+b} \ .
\end{align*}
Then, GMM estimators of the synthetic control weights and treatment effect parameter are obtained as the solution to the following minimization problem:
\begin{align*}
% \label{eq-GMM}
\big(
\widehat{\Beta} 
,
\widehat{\bgamma}_{\rho}
,
\widehat{\bbeta}
\big)
=
\argmin_{(\Beta,\bgamma,\bbeta)}
\Big[
\big\{ \widehat{\Psi}( \Beta, \bgamma, \bbeta) \big\} \T
\widehat{\Omega}
\big\{ \widehat{\Psi}( \Beta, \bgamma, \bbeta) \big\} 
+ \rho \big\| \bgamma \big\|_{2}^{2}
\Big]
\ ,
\end{align*}
where $	\widehat{\Psi}(\Beta, \bgamma, \bbeta)
= T^{-1}
\sum_{t=1}^{T} \Psi (\bO_t \con \Beta, \bgamma, \bbeta)$ is the empirical mean of the estimating function and $\widehat{\Omega} \in \R^{(2d+p+b) \times (2d+p+b)}$ a user-specified symmetric positive definite block-diagonal matrix as $\widehat{\Omega} = \text{diag}(\widehat{\Omega}_{\pre} , \widehat{\Omega}_{\post} )$; for simplicity, $\widehat{\Omega}$ can be chosen as the identity matrix. 

%Under our assumptions, the following result establishes that $ \widehat{\bbeta}$ is asymptotically normal when the number of time periods goes to infinity and the regularization parameter diminishes at $o(N^{-1/2})$ rate:
%\begin{theorem}	\label{thm:AN}
%Suppose that Assumptions \ref{assumption:consistency}--\ref{assumption:weakdep} holds and regularity conditions in Supplementary Material \ref{sec:supp:AN} hold. Then, as $T \rightarrow \infty$ and $\rho=o(N^{-1/2})$, we have
%\begin{align*}
%\sqrt{ T }
%\big(
%\widehat{\bbeta}
%-
%\bbeta^*
%\big)
%\text{ converges in distribution to }
%N \big( 0, \Sigma_{\bbeta}^* \big) \ ,
%\end{align*}
%Here, $\Sigma_{\bbeta}^* \in \R^{b \times b}$ is the bottom-right $(b \times b)$ block matrix of $\Sigma_1^* \Sigma_2^* \Sigma_1\sT$ where
%\begin{align*}
%&
%\Sigma_1^* 
%=
%\bigg[ \Omega^{*1/2}
%\lim_{T \rightarrow \infty} 
%\frac{\partial \EXP \big\{ \widehat{\Psi} (\Beta,\bgamma,\bbeta) \big\} }{ \partial (\Beta,\bgamma,\bbeta)\T }
%\bigg|_{ \Beta=\Beta^*, \bgamma=\bgamma_0^* , \bbeta=\bbeta^* }
%\bigg]^{+} 
%\Omega^{*1/2}
%\ , 
%&&
%\Sigma_2^* 
%=
%\lim_{T \rightarrow \infty} 
%\VAR \Big\{ \sqrt{T} \cdot  \widehat{\Psi} (\Beta^*,\bgamma_0^*, \bbeta^*) \Big\} \ .
%\end{align*} 
%Here, $\Omega^{*1/2}$ is a symmetric positive-definite matrix satisfying $\big( \Omega^{*1/2} \big)^2 = \lim_{T \rightarrow \infty} \widehat{\Omega}$.
%\end{theorem}
%\noindent For inference, we propose the variance estimator $\widehat{\Sigma}_{\bbeta}  \in \R^{b \times b}$ as the bottom-right $(b \times b)$ block matrix of $\widehat{\Sigma}_1 
%\widehat{\Sigma}_2
%\widehat{\Sigma}_1 \T$, where $\widehat{\Sigma}_1$ is defined by
%\begin{align*}
%	\widehat{\Sigma}_1
%	=
%	\Big\{
%	\widehat{\mathcal{G}}\T \widehat{\Omega} \widehat{\mathcal{G}} 
%	+ 
%	\text{diag} \big( 0_{d \times d} , \rho \cdot I_{N \times N}, 0_{b \times b} \big)
%\Big\}^{-1}
%\Big(
%\widehat{\mathcal{G}}\T \widehat{\Omega} 
%\Big)
%\ , \quad 
%\widehat{\mathcal{G}} = \frac{\partial \widehat{\Psi} (\Beta,\bgamma,\bbeta) }{ \partial (\Beta,\bgamma,\bbeta)\T }
%\bigg|_{ \Beta=\widehat{\Beta}, \bgamma=\widehat{\bgamma}_{\rho}, \bbeta=\widehat{\bbeta} } \ .
%\end{align*} 
%For $\widehat{\Sigma}_2$, we use a heteroskedasticity and autocorrelation consistent estimator \citep{NW1987, Andrews1991} given the time series nature of the observed sample; see Supplementary Material \ref{sec:supp:HAC} for details. Alternatively, one could implement the block bootstrap; see Supplementary Material \ref{sec:supp:BB} for details. 


Under our assumptions, the following result establishes that $(\widehat{\Beta}, \widehat{\bgamma}_{\rho}, \widehat{\bbeta})$ is asymptotically normal when the number of time periods goes to infinity and the regularization parameter diminishes at $o(N^{-1/2})$ rate:
\begin{theorem}	\label{thm:AN}
Suppose that Assumptions \ref{assumption:consistency}, \ref{assumption:noitf}, \ref{assumption:valid proxy}, \ref{assumption:SC}, \ref{assumption:weakdep}, and regularity conditions in Supplementary Material \ref{sec:supp:AN} hold. Then, as $T \rightarrow \infty$ and $\rho=o(N^{-1/2})$, we have
\begin{align*}
\sqrt{ T }
\left\{ 
\begin{pmatrix}
\widehat{\Beta} 
\\
\widehat{\bgamma}_{\rho}
\\
\widehat{\bbeta}
\end{pmatrix}
-
\begin{pmatrix}
\Beta^*
\\
\bgamma_{0}^*
\\
\bbeta^*
\end{pmatrix}
\right\}
\text{ converges in distribution to }
N \big( 0, \Sigma^*  \big) \ ,
\end{align*}
where $\Sigma^* = \Sigma_1^* \Sigma_2^* \Sigma_1\sT$ is given by
\begin{align*}
&
\Sigma_1^* 
=
\bigg[ \Omega^{*1/2}
\lim_{T \rightarrow \infty} 
\frac{\partial \EXP \big\{ \widehat{\Psi} (\Beta,\bgamma,\bbeta) \big\} }{ \partial (\Beta,\bgamma,\bbeta)\T }
\bigg|_{ \Beta=\Beta^*, \bgamma=\bgamma_0^* , \bbeta=\bbeta^* }
\bigg]^{+} 
\Omega^{*1/2}
\ , 
&&
\Sigma_2^* 
=
\lim_{T \rightarrow \infty} 
\VAR \Big\{ \sqrt{T} \cdot  \widehat{\Psi} (\Beta^*,\bgamma_0^*, \bbeta^*) \Big\} \ .
\end{align*} 
Here, $\Omega^{*1/2}$ is a symmetric positive-definite matrix satisfying $\big( \Omega^{*1/2} \big)^2 = \lim_{T \rightarrow \infty} \widehat{\Omega}$.
\end{theorem}
\noindent Note that $\Sigma^*$ is rank-deficient if the dimension of $\bm{g}$ is smaller than $N$. In this case, the asymptotic distribution is a degenerate normal distribution.  However, this degeneracy only impacts the synthetic control weight estimator $\widehat{\bgamma}_{\rho}$. Therefore, the asymptotic variance of $\widehat{\bbeta}$ remains full rank, even in this case, ensuring that inference regarding $\bbeta^*$ remains valid. For inference about $\bbeta^*$, we propose to use the $(b \times b)$-dimensional bottom-right submatrix of $\widehat{\Sigma}
=
\widehat{\Sigma}_1 
\widehat{\Sigma}_2
\widehat{\Sigma}_1 \T$, which is associated with $\widehat{\bbeta}$. Here, $\widehat{\Sigma}_1$ is defined by
\begin{align*}
\widehat{\Sigma}_1
=
\Big\{
\widehat{\mathcal{G}}\T \widehat{\Omega} \widehat{\mathcal{G}} 
+ 
\text{diag} \big( 0_{d \times d} , \rho \cdot I_{N \times N}, 0_{b \times b} \big)
\Big\}^{-1}
\Big(
\widehat{\mathcal{G}}\T \widehat{\Omega} 
\Big)
\ , \quad 
\widehat{\mathcal{G}} = \frac{\partial \widehat{\Psi} (\Beta,\bgamma,\bbeta) }{ \partial (\Beta,\bgamma,\bbeta)\T }
\bigg|_{ \Beta=\widehat{\Beta}, \bgamma=\widehat{\bgamma}_{\rho}, \bbeta=\widehat{\bbeta} } \ .
\end{align*} 
For $\widehat{\Sigma}_2$, we use a heteroskedasticity and autocorrelation consistent estimator \citep{NW1987, Andrews1991} given the time series nature of the observed sample; see Supplementary Material \ref{sec:supp:HAC} for details. Alternatively, one could implement the block bootstrap; see Supplementary Material \ref{sec:supp:BB} for details. 

Lastly, while Theorem \ref{thm:AN} specifies the required rate for the regularization parameter $\rho$ in relation to $T$, it is still necessary to select a specific value $\rho$ for the given data at hand. In practice, we select $\rho$ using cross-validation; for further details, see Supplementary Material \ref{sec:supp:CV}. In addition, one may have access to exogenous covariates that may be leveraged to improve efficiency. In Supplementary Material \ref{sec:Cov}, we provide details on how to incorporate measured covariates in the SPSC framework. 

\subsection{Conformal Inference of the Treatment Effect} \label{sec:Conformal}

Key limitations of the methodology proposed in the previous Section include (i) a parsimonious model choice for $\tau_t = \tau(t \con \bbeta)$ may be mis-specified and (ii) it potentially requires $T_0$ and $T_1$ both be large in order to rely upon a law of large numbers and central limit theorem for valid asymptotic inference,  so that our large sample analysis can reliably be used to quantify uncertainty associated with the estimated parameters. These limitations may be prohibitive in real-world applications with limited post-treatment follow-up data available. In order to address this specific challenge, previous works such as \citet{Cattaneo2021, Chernozhukov2021} developed prediction intervals to assess statistical uncertainty, obviating the need to specify a model for the treatment effect or large $T_1$. We focus on the conformal inference approach proposed by \citet{Chernozhukov2021} due to its ready adaptation to the SPSC framework. The key idea of the approach is to construct pointwise prediction intervals for the random treatment effects $\xi_t^* = \potY{t}{1} - \potY{t}{0}$ for $t\in \{T_0+1,\ldots,T\}$ by inverting permutation tests about certain null hypotheses concerning $\xi_t^*$. One crucial requirement for the approach is the existence of an unbiased predictor for $\potY{t}{0}$ for $t\in \{1,\ldots, T\}$. In the context of SPSC, the synthetic control $\bW_{ t} \T \bgamma_0^*$ is an unbiased predictor for $\potY{t}{0}$ as established in Theorem \ref{thm:ATT}, and, consequently, their approach readily applies. In what follows, we present the approach in detail. 

Consider an asymptotic regime whereby $T_0$ goes to infinity while $T_1$ is fixed. Let $s \in \{T_0+1,\ldots,T\}$ be a post-treatment period for which one aims to construct a prediction interval for the treatment effect; without loss of generality, we take $s=T_0+1$. The null hypothesis of interest can be expressed as $H_{0, T_0+1} : \xi_{T_0+1}^* = \xi_{0,{T_0+1}}$, where $\xi_{0,{T_0+1}}$ represents a hypothesized treatment effect value. Under $H_{0,T_0+1}$, the treatment-free potential outcome at time ${T_0+1}$ can be identified as $\potY{{T_0+1}}{0} = Y_{T_0+1} - \xi_{0,{T_0+1}}$ and, consequently, pre-treatment outcomes ${ Y_1, \ldots, Y_{T_0} }$ may in fact be supplemented with $Y_{T_0+1} - \xi_{0,{T_0+1}}$  to estimate the synthetic control weights. We may then redefine the pre-treatment estimating function $\Psi_{\pre}$ in equation \eqref{eq-GMM-moment} as follows:
\begin{align*}
\Psi_{\pre} (\bO_t \con \Beta, \bgamma, \xi_{0,{T_0+1}} )
=
\begin{bmatrix}
\bD_t \big( Y_t - A_t \xi_{0,T_0+1}  - \bD_t\T \Beta \big)
\\
\bg \big( t, Y_t - A_t \xi_{0,T_0+1} \con \Beta \big)  \big( Y_t - A_t\xi_{0,T_0+1}  - \bW_t \T \bgamma \big)
\end{bmatrix} 
\ , 
\quad t\in \{1,\ldots,T_0+1\} \ .
\end{align*}
At the minimum-norm synthetic control weights $\bgamma_0^*$, the redefined estimating function is mean-zero for $t \in \{ 1,\ldots, {T_0+1}\}$ under $H_{0,T_0+1}$. Therefore, a GMM estimator $\widehat{\bgamma}(\xi_{0,T_0+1})$ can be obtained by solving the following minimization problem, which is similar to \eqref{eq-GMM-Formula}: 
\begin{align*}
\big( \widehat{\Beta} (\xi_{0,T_0+1}), \widehat{\bgamma}_{\rho} (\xi_{0,T_0+1}) \big)
=
\argmin_{(\Beta,\bgamma)}
\Big[
\big\{ 
\widehat{\Psi}_{\pre} ( \Beta , \bgamma, \xi_{0,T_0+1} ) 
\big\} \T
\widehat{\Omega}_{\pre}
\big\{ 
\widehat{\Psi}_{\pre} ( \Beta , \bgamma, \xi_{0,T_0+1} ) 
\big\}
+
{\rho} \big\| \bgamma \big\|_2^2
\Big]
\ , \ 
\end{align*} 
where $\widehat{\Psi}_{\pre} ( \Beta, \bgamma, \xi_{0,T_0+1} ) 
= 
(T_0+1)^{-1}
\sum_{t=1}^{T_0+1} 
\Psi_{\pre} (\bO_t \con \Beta, \bgamma, \xi_{0,T_0+1})$ and $\widehat{\Omega}_{\pre}$ is the weight matrix used in \eqref{eq-GMM-Formula}. We may then compute residuals $\widehat{\nu}_{t} (\xi_{0,T_0+1}) = Y_t - A_t \xi_{0,{T_0+1}} - \bW_{ t} \T \widehat{\bgamma}_{\rho}(\xi_{0,{T_0+1}}) $, and use these residuals to obtain a p-value for testing the null hypothesis as follows:
\begin{align*}
p_{T_0+1}(\xi_{0, T_0+1}) 
=
\frac{1}{T_0+1	}
\sum_{t=1}^{T_0+1} \ind \Big\{ \big| \widehat{\nu}_{t} (\xi_{0, T_0+1}) \big| \geq \big| \widehat{\nu}_{T_0+1} (\xi_{0, T_0+1}) \big| \Big\}
\ .
\end{align*}
In words, the p-value is the proportion of residuals of 
magnitudes no smaller than the post-treatment residual. Under $H_{0, T_0+1}$ and regularity conditions including that the error $\nu_t = Y_t - \xi_t^* - \bW_{ t} \T \bgamma_0^*$ is stationary and weakly dependent, the p-value is approximately unbiased, i.e., $\Pr \{ p_{T_0+1}(\xi_{0,T_0+1}) \leq \alpha \} = \alpha + o(1)$ as $T_0 \rightarrow \infty$ for a user-specified confidence level $\alpha \in (0,1)$; we refer the readers to Theorem 1 of \citet{Chernozhukov2021} for technical details. Therefore, an approximate $100(1-\alpha)$\% prediction interval for $\xi_{t}^*$ can be constructed by inverting the hypothesis test based on $p_{T_0+1}(\xi_{0, T_0+1})$. This prediction interval is formally defined as
$\mathcal{C}_{T_0+1} (1-\alpha) =
\big\{
\xi
\cond		
p_{T_0+1}(\xi) > \alpha
\big\}$ and can be found via a grid-search. 
% We remark that the proposed conformal inference approach is also valid for constructing a confidence interval for the ATT by replacing $\xi_t^*$ with $\tau_t^*$. 





\section{Simulation}		\label{sec:Sim}

We conducted a simulation study to evaluate the finite sample performance of the proposed estimator under a variety of conditions. Based on the IFEM in \eqref{eq-IFEM}, we considered the following data generating mechanisms with pre- and post-treatment periods of length $T_0 = T_1 \in \{ 50, 100, 250, 500 \}$ and donor pools of size $N = 16$. 

First, for each $t \in \{1,\ldots,T\}$, we generated 4-dimensional latent factors $\blambda_t = (\lambda_{1t},\cdots,\lambda_{4t})\T$ from $N(\bm{\nu}_t,0.25 \cdot I_{4\times 4})$, with $\blambda_t$ independent across time periods. For the mean vector $\bm{\nu}_t=(\nu_{1t},\cdots,\nu_{4t})\T$, we considered the following two specifications for $j\in \{1,\ldots,4\}$:
\begin{align*}
& (\textit{No trend}): \quad \nu_{jt} = 0; 	\quad 
&& (\textit{Linear trend}): \quad \nu_{jt} = t/T_0;	\quad  
\end{align*} 

The latent factor loadings $\bm{\mu}_{i}$ for $i \in \{1,\ldots,16\}$, i.e., latent factor loadings of untreated units,  were specified as follows:
\begin{align*}
\mathfrak{M}
=
\begin{bmatrix}
\bm{\mu}_1 & \cdots & \bm{\mu}_{16}
\end{bmatrix}
=
\begin{bmatrix}
2 & 1.75 & 1.5 & 1.25 & 1 & 0.75 & 0.5 & 0.25 & 0_{1 \times 8} \\
0.8 & 0.8 & 0.6 & 0.6 & 0.4 & 0.4 & 0.2 & 0.2 & 0_{1 \times 8} \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1_{1 \times 8} \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &  0.5 \cdot 1_{1 \times 8} 
\end{bmatrix}
\in \R^{4 \times 16} \ .
\end{align*}
The latent factor loading $\bm{\mu}_{0}$, i.e., latent factor loading of the treated unit, was specified from either one of the followings:
\begin{align*}
& (\textit{Simplex}): && \bm{\mu}_{0} = (1.125,0.5,0,0)\T = \frac{1}{8} \sum_{i=1}^{8}  \bm{\mu}_{i} ; \quad \quad \quad 
&& (\textit{Non-simplex}): && \bm{\mu}_{0} = (2,1.5,0,0)\T \ .
\end{align*}
Note that condition \eqref{eq-ExistSC} is satisfied with $\bgamma^\dagger = \mathfrak{M}^+ \bm{\mu}_0$, although this vector is not the unique solution. Also, when $\bm{\mu}_0$ is chosen as \textit{(Simplex)}, $\bgamma^\dagger$ lies within a 16-dimensional simplex, thus satisfying the restriction of \citet{Abadie2010}. In contrast, when $\bm{\mu}_0$ is chosen as \textit{(Non-implex)}, $\bgamma^\dagger$ does not belong to this simplex. 

The errors $\bm{e}_t = (e_{0t} ,  e_{1t} , \cdots,  e_{16 t})\T$ were generated independently across time periods from $\bm{e}_t \sim N \big( 0_{16 \times 1} , 0.25 \cdot \text{diag} (\Sigma_{e}, I_{8 \times 8}) \big)$ where $\Sigma_e \in \R^{9 \times 9}$ were chosen from one of the following three matrices with the corresponding $\omega_i$ values in \eqref{eq-LinearModel-System}:
\begin{align*}
&
(\textit{Independent errors}):
&&
\Sigma_{e} = I_{9 \times 9}   \ ;
&&
\omega_0 = 1 , \ 
\omega_1=\cdots=\omega_{16}=0     \ ;
\\
&
(\textit{Correlated errors}):
&&
\Sigma_{e} = 
0.1 \cdot I_{9 \times 9} + 0.9  \cdot 1_{9 \times 9}    \ ;
&&
\omega_0 = 1 , \ 
\omega_1=\cdots=\omega_{8}=0.9 , \
\omega_9=\cdots=\omega_{16}=0    \ ;
\\
&
(\textit{No $Y$ error}):
&&
\Sigma_{e} = \text{diag}(0,I_{8 \times 8})   \ ;
&&
\omega_0 =\cdots=\omega_{16}=0 \ .
\end{align*} 
Under (\textit{Correlated errors}) and (\textit{No $Y$ error}), equation \eqref{eq-LinearModel-System} admit solutions, thus satisfying Condition \ref{assumption:SC-structural} and Assumption \ref{assumption:SC}. In contrast, under (\textit{Independent errors}), equation \eqref{eq-LinearModel-System} does not have a solution, violating of Condition \ref{assumption:SC-structural}. Nonetheless, as we discussed in Section \ref{sec:IFEM discussion}, it is still possible to find a synthetic control bridge function satisfying Assumption \ref{assumption:SC}; see Supplementary Material \ref{sec:supp:IFEM} for details.

With these generated variables, $\potY{t}{0}$ and $W_{it}$ at $t\in \{1,\ldots,T\}$ were generated as $\potY{t}{0} = \bm{\mu}_0\T \bm{\lambda}_t + e_{0t}$ and $W_{it} = \bm{\mu}_i\T \bm{\lambda}_t + e_{it}$ for $i\in \{1,\ldots,N\}$, respectively. Note that the latter eight untreated units $(W_{9t},\cdots,W_{16t})\T$ were independent of $\potY{t}{0}$, which resulted in multiple synthetic control bridge functions satisfying Assumption \ref{assumption:SC}. The potential outcomes under treatment at $t\in \{1,\ldots,T\}$ were generated as $\potY{t}{1} = \potY{t}{0} + 3 A_t + \epsilon_{t}$ where $\epsilon_t$ were generated independently across time periods from $N(0,0.25)$. Note that the ATT was $\tau_t^* = 3$ for $t\in \{T_0+1,\ldots,T\}$.

Using the simulated data, we estimated the aggregated ATT over the post-treatment periods, i.e., $T_1^{-1} \sum_{t=T_0+1}^{T} \tau_t^*$, based on the following six estimators. First, we obtained two ATT estimators based on the proposed SPSC approaches both time-invariant pre-treatment estimating function $\Phi_{\pre}$ in \eqref{eq-GMM-moment-old} and time-varying pre-treatment estimating function $\Psi_{\pre}$ in \eqref{eq-GMM-moment} where the function $\bh$ was chosen as $\bh(y)=y$. We specified the vector $\bD_t$ in $\Psi_{\pre}$ as $\bD_t=\mathcal{B}_6(t)$, the 6-dimensional cubic B-spline function, to adjust a potential time trend. These estimators are referred to as \textit{SPSC-NoDT} and \textit{SPSC-DT}, respectively. For comparison, we also considered two OLS-based ATT estimators based on \eqref{eq-OLS}. In the first OLS-based ATT estimator, we place no regularization on the weight; in the second OLS-based ATT estimator, we followed \citet{Abadie2010} to restrict the weight to be non-negative and its values must add up to one. These estimators are referred to as \textit{OLS-NoReg} and \textit{OLS-Standard}, respectively. Lastly, we implemented two recently developed synthetic control methods by \citet{ASCM2021} and \citet{Cattaneo2021}, which are referred to as \textit{ASC} and \textit{SCPI}, respectively. In our analysis, we implemented the OLS-Standard, ASC, and SCPI estimators using \texttt{synth} \citep{Synth2011}, \texttt{augsynth} \citep{ASCM2023package}, and \texttt{scpi} \citep{scpiPackage2023} R-packages, respectively. Unfortunately, these three packages do not appear to provide readily available standard errors, so the standard errors and empirical coverage rates of these methods are not reported. We repeated the simulation 500 times. 


To simplify the discussion, we present only the results under the (\textit{Non-simplex}) case for $\bm{\mu}_0$. The results for the (\textit{Simplex}) case are included in Supplementary Material \ref{sec:supp:Simulation}. Figure \ref{fig:Sim:Constant} summarizes the empirical distribution of the estimators graphically. First, when $\blambda_t$ does not have a trend, all estimators exhibit negligible bias for the ATT regardless of error specifications.  Second, when $\blambda_t$ has a linear trend case, we find that the four estimators from OLS, ASC, and SCPI approaches are biased for the ATT. Although the SPSC-NoDT estimator outperforms these four estimators, it still exhibits non-negligible bias when the errors are independent. In contrast, the SPSC-DT estimator little bias for all error specifications. Note that 95\% Monte Carlo confidence interval for SPSC estimators shrinks as the number of time periods increases, which is consistent with the results established in Section \ref{sec:SPSC}. 

% Figure environment removed		




Table \ref{tab:Sim:ATT} provides more detailed summary statistics when errors $\bm{e}_t$ were generated from the (\textit{Independent errors}) case, a common assumption that the standard IFEM make. The results for the other two error specifications are reported in Supplementary Material \ref{sec:supp:Simulation}.  We remark that the OLS-Standard, ASC, and SCPI approaches do not provide a standard error or 95\% confidence interval for the ATT. First, when $\blambda_t$ has no trend, all estimators exhibit negligible bias and achieve the nominal coverage rate, provided that confidence intervals are available. Second, when $\blambda_t$ has a linear trend, the performance of the estimators differs in terms of both mean squared error and coverage rate. We find that the SPSC-DT estimator attains the smallest mean squared error compared to the other estimators, including the SPSC-NoDT estimator. Regarding the coverage rate, confidence intervals based on the OLS-NoReg and SPSC-NoDT estimators fail to attain the nominal coverage rate, especially when $T_0$ and $T_1$ are large due to non-diminishing bias. In contrast, confidence intervals based on the SPSC-DT estimator attain the nominal coverage rate. This demonstrates that accounting for time-varying components in the pre-treatment estimating estimation can significantly improve the performance of the SPSC estimators and is, in fact, necessary to conduct valid inference.






\begin{table}[!htp]
\renewcommand{\arraystretch}{1.1} \centering
\scriptsize
\setlength{\tabcolsep}{2pt} 
\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|}
\hline
\multicolumn{1}{|c|}{\multirow{3}{*}{$\blambda_t$}} &  \multicolumn{1}{c|}{\multirow{3}{*}{\normalfont Statistics}} & \multicolumn{12}{c|}{\normalfont Estimators and $T_0$} \\ \cline{3-14} 

\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{}  & \multicolumn{2}{c|}{\normalfont OLS-NoReg} & \multicolumn{2}{c|}{\normalfont OLS-Standard} & \multicolumn{2}{c|}{\normalfont ASC}       & \multicolumn{2}{c|}{\normalfont SCPI}      & \multicolumn{2}{c|}{\normalfont SPSC-NoDT} & \multicolumn{2}{c|}{\normalfont SPSC-DT}   \\ \cline{3-14} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500} & \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500}& \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500}& \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500}& \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500}& \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500}  \\ \hline

\multicolumn{1}{|c|}{\multirow{7}{*}{$\!\!\!\begin{array}{c}\text{\normalfont \scriptsize No trend}\end{array}\!\!\!$}} & \multicolumn{1}{c|}{\normalfont \scriptsize Bias $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.02} & {\normalfont \scriptsize -0.01} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.04} & {\normalfont \scriptsize 0.00} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.03} & {\normalfont \scriptsize -0.01} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.03} & {\normalfont \scriptsize -0.01} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.03} & {\normalfont \scriptsize -0.01} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.03} & \multicolumn{1}{c|}{\normalfont \scriptsize -0.01}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize ASE $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.89} & {\normalfont \scriptsize 0.37} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.84} & {\normalfont \scriptsize 0.37} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.84} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.37}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize BSE $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.97} & {\normalfont \scriptsize 0.38} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.84} & {\normalfont \scriptsize 0.37} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.84} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.37}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize ESE $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.92} & {\normalfont \scriptsize 0.37} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.25} & {\normalfont \scriptsize 0.51} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.90} & {\normalfont \scriptsize 0.39} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.92} & {\normalfont \scriptsize 0.39} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.89} & {\normalfont \scriptsize 0.37} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.89} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.37}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize MSE $(\times100)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.85} & {\normalfont \scriptsize 0.14} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.56} & {\normalfont \scriptsize 0.26} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.82} & {\normalfont \scriptsize 0.15} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.84} & {\normalfont \scriptsize 0.15} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.79} & {\normalfont \scriptsize 0.14} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.79} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.14}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize Coverage (ASE)} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.95} & {\normalfont \scriptsize 0.96} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.93} & {\normalfont \scriptsize 0.96} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.93} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.96}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize Coverage (BSE)} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.96} & {\normalfont \scriptsize 0.96} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.93} & {\normalfont \scriptsize 0.96} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.93} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.96}\\ \hline
\multicolumn{1}{|c|}{\multirow{7}{*}{$\!\!\!\begin{array}{c}\text{\normalfont \scriptsize Linear trend}\end{array}\!\!\!$}} & \multicolumn{1}{c|}{\normalfont \scriptsize Bias $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.30} & {\normalfont \scriptsize 1.37} & \multicolumn{1}{c|}{\normalfont \scriptsize 10.70} & {\normalfont \scriptsize 10.51} & \multicolumn{1}{c|}{\normalfont \scriptsize 8.26} & {\normalfont \scriptsize 5.70} & \multicolumn{1}{c|}{\normalfont \scriptsize 11.93} & {\normalfont \scriptsize 11.85} & \multicolumn{1}{c|}{\normalfont \scriptsize -1.42} & {\normalfont \scriptsize -1.61} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.07} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.08}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize ASE $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.76} & {\normalfont \scriptsize 0.79} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.85} & {\normalfont \scriptsize 0.84} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.79} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.81}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize BSE $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 2.02} & {\normalfont \scriptsize 0.82} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 2.24} & {\normalfont \scriptsize 0.99} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.94} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.86}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize ESE $(\times10)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.82} & {\normalfont \scriptsize 0.77} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.16} & {\normalfont \scriptsize 0.41} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.70} & {\normalfont \scriptsize 0.95} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.96} & {\normalfont \scriptsize 0.43} & \multicolumn{1}{c|}{\normalfont \scriptsize 2.08} & {\normalfont \scriptsize 0.97} & \multicolumn{1}{c|}{\normalfont \scriptsize 1.94} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.85}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize MSE $(\times100)$} & \multicolumn{1}{c|}{\normalfont \scriptsize 5.01} & {\normalfont \scriptsize 2.45} & \multicolumn{1}{c|}{\normalfont \scriptsize 115.72} & {\normalfont \scriptsize 110.54} & \multicolumn{1}{c|}{\normalfont \scriptsize 71.16} & {\normalfont \scriptsize 33.37} & \multicolumn{1}{c|}{\normalfont \scriptsize 143.18} & {\normalfont \scriptsize 140.58} & \multicolumn{1}{c|}{\normalfont \scriptsize 6.36} & {\normalfont \scriptsize 3.54} & \multicolumn{1}{c|}{\normalfont \scriptsize 3.75} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.72}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize Coverage (ASE)} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.87} & {\normalfont \scriptsize 0.58} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.84} & {\normalfont \scriptsize 0.49} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.93} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.94}\\ \cline{2-14}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize Coverage (BSE)} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.93} & {\normalfont \scriptsize 0.61} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize -} & {\normalfont \scriptsize -} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.91} & {\normalfont \scriptsize 0.60} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.94} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.96}\\ \hline
                                                                                                                                         
\end{tabular}
\vspace*{0.2cm}
\caption{Summary Statistics of Estimation Results Under Independent Errors. 
Bias row gives the empirical bias of 500 estimates. 
ASE row gives the asymptotic standard error obtained from the sandwich estimator of the GMM. BSE row shows the bootstrap standard error obtained from the approach in Supplementary Material \ref{sec:supp:BB}.
ESE row gives the standard deviation of 500 estimates.
MSE row gives the mean squared error of 500 estimates. 
Coverage (ASE) and Coverage (BSE) rows give the empirical coverage rate of 95\% confidence intervals based on the ASE and BSE, respectively.
Bias, standard errors, and mean squared error are scaled by factors of 10, 10, and 100, respectively.
}
\label{tab:Sim:ATT}  
\end{table} 

Next, we evaluated the finite sample performance of the conformal inference approach in Section \ref{sec:Conformal}. As competing methods, we considered the ASC, SCPI, and two SPSC estimators. For each simulated data set, we obtained pointwise 95\% pointwise prediction intervals for the treatment effect $\xi_t^* = \tau_t^* + \epsilon_{t}$ at 10 post-treatment times $t \in \{ T_0 + 0.1 T_1, \ldots T_0 + 0.9 T_1 , T_0+T_1 \} $, using the proposed conformal inference approach for the SPSC estimators along with the ASC and SCPI approaches. We then evaluated the empirical coverage rates of these pointwise prediction intervals based on 500 simulation repetitions, i.e., the proportion of Monte Carlo samples where $\xi_{t}^*$ is contained in 95\% pointwise prediction intervals. 

Table \ref{tab:Table3} gives the empirical coverage rates for each simulated scenario. Surprisingly, the ASC and SPCI approaches fail to attain the nominal coverage rate; we believe this failure originates from the simulation setting where $\bm{\mu}_0$ lies outside the simplex, i.e., (\textit{Non-simplex}). These methods perform particularly poorly when $\blambda_t$ follows a linear trend and the number of time periods is large (i.e., $T_0=500$). In contrast, regardless whether $\blambda_t$ has a trend or not, both SPSC estimators attains the desired nominal coverage rate, aligning closely with theoretical expectations. 



\begin{table}[!htp] 
\renewcommand{\arraystretch}{1.1} \centering 
\setlength{\tabcolsep}{4pt} 
\begin{tabular}{|c|c|cc|cc|cc|cc|}
\hline
\multicolumn{1}{|c|}{\multirow{3}{*}{$\bm{\lambda}_t$}} & \multicolumn{1}{c|}{\multirow{3}{*}{$\bm{e}_t$}} & 
\multicolumn{8}{c|}{\normalfont Estimators and $T_0$}
\\ \cline{3-10} 

\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\normalfont ASC}       & \multicolumn{2}{c|}{\normalfont SCPI}      & \multicolumn{2}{c|}{\normalfont SPSC-NoDT} & \multicolumn{2}{c|}{\normalfont SPSC-DT}   \\ \cline{3-10} 


\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500} & \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500}& \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500}& \multicolumn{1}{c|}{\normalfont 100} & \multicolumn{1}{c|}{\normalfont 500} \\ \hline


\multicolumn{1}{|c|}{\multirow{3}{*}{$\!\!\!\begin{array}{c}\text{\normalfont \scriptsize No trend}\end{array}\!\!\!$}} & \multicolumn{1}{c|}{\normalfont \scriptsize Independent errors} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.933} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.925} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.935} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.912} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.959} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.948} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.961} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.948}\\ \cline{2-10}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize Correlated errors} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.904} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.906} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.925} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.913} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.962} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.952} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.963} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.953}\\ \cline{2-10}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize No $Y$ error} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.922} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.910} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.938} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.925} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.964} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.951} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.964} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.953}\\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{$\!\!\!\begin{array}{c}\text{\normalfont \scriptsize Linear trend}\end{array}\!\!\!$}} & \multicolumn{1}{c|}{\normalfont \scriptsize Independent errors} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.795} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.846} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.938} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.889} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.959} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.944} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.962} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.949}\\ \cline{2-10}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize Correlated errors} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.844} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.883} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.907} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.843} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.957} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.944} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.967} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.948}\\ \cline{2-10}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\normalfont \scriptsize No $Y$ error} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.728} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.808} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.920} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.852} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.957} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.953} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.957} & \multicolumn{1}{c|}{\normalfont \scriptsize 0.946}\\ \hline




\end{tabular}
\vspace*{0.2cm}
\caption{
Empirical Coverage Rates of 95\% Pointwise Prediction Intervals. The numbers in SPSC-NoDT and SPSC-DT columns give the empirical coverage rates of 95\% pointwise prediction intervals obtained from the conformal inference approach in Section \ref{sec:Conformal}. The numbers in ASC and SCPI columns give the empirical coverage rates of 95\% pointwise prediction intervals obtained from the approaches proposed by \citet{ASCM2021} and \citet{Cattaneo2021}, respectively.}
\label{tab:Table3} 
\end{table}


In Supplementary Material \ref{sec:supp:Simulation PI}, we assess the finite sample performance of the proposed conformal inference approach based on the simulation scenario given in \citet{Cattaneo2021}, which may not be compatible with the key identifying condition, Assumption \ref{assumption:SC}, of SPSC. As expected, the approach of \citet{Cattaneo2021} performs well in this setting. Although our method without time trend adjustment (i.e., SPSC-NoDT) sometimes fails to achieve the nominal coverage rate, particularly when outcomes are non-stationary, our method with time trend adjustment (i.e., SPSC-DT) consistently attains the nominal coverage rate, provided that the basis functions for time periods are appropriately chosen. This highlights the robustness of the proposed SPSC approach and its broad applicability in synthetic control settings.




\section{Application}		\label{sec:Data}

We applied the proposed method to analyze a real-world application. In particular, we revisited the dataset analyzed in \citet{Dataset1907}, which consists of time series data of length $384$ for 59 trust companies, recorded between January 5, 1906, and December 30, 1908, with a triweekly frequency. Notably, this time period includes the Panic of 1907 \citep{Moen1992}, a financial panic that lasted for three weeks in the United States starting in mid-October, 1907. As a result of the panic, there was a significant drop in the stock market during this period. From this context, we focused on the effect of the financial panic in October 1907 on the log stock price of trust companies using $T_0=217$ pre-treatment time periods and $T_1=167$ post-treatment time periods, respectively. 

The treated unit and donors were defined as follows. According to \citet{Dataset1907}, Knickerbocker, Trust Company of America, and Lincoln were the three trust companies that were most severely affected during the panic. However, % despite the absence of the financial panic, 
Lincoln's stock price showed a strong downward trend over the pre-treatment period. Therefore, we defined the average of the log stock prices of the first two trust companies as $Y_t$, the outcome of the treated units at time $t\in \{1,\ldots,384\}$. As for potential donors, \citet{Dataset1907} identified $N=49$ trust companies that had weak financial connections with the aforementioned three severely affected trust companies. 
%However, some of these trust companies seem to violate the relevance condition \eqref{eq-relevant}. Therefore, we chose donors based on the procedure proposed in Section \ref{sec:supp:Donors} of Supplementary Material, which resulted in $d=24$ trust companies. 
Accordingly, the log stock prices of these 49 trust companies were defined as $\bW_{ t}$, the outcome of the donors. Following the simulation study, we specified the time-invariant and time-varying pre-treatment estimating functions, $\Phi_{\pre}$ and $\Psi_{\pre}$, with $\bh(y)=y$ and $\bD_t=\mathcal{B}_6(t)$, the 6-dimensional cubic B-spline function, to account for a potential time trend.

We first report the ATT estimates under a constant treatment effect model $\tau(t \con \bbeta) = \beta$. Similar to Section \ref{sec:Sim}, we compare the same six estimators: the unconstrained OLS synthetic control estimator (OLS-NoReg), the standard synthetic control approach proposed by \citet{Abadie2010} (OLS-Standard), two recent approaches by \citet{ASCM2021} (ASC) and \citet{Cattaneo2021} (SCPI), and SPSC estimators without and with time-varying terms (SPSC-NoDT, SPSC-DT). The results are summarized in Table \ref{tab:data:ATT}. Interestingly, all six estimators yield similar point estimates of the treatment effect, ranging from $-1.021$ to $-0.813$. According to the 95\% confidence intervals, three estimates uniformly reject the null hypothesis of no treatment effect across time points, suggesting that the financial panic led to a significant decrease in the average log stock price of Knickerbocker and Trust Company of America. We remark again that  the OLS-Standard, ASC, and SCPI approaches do not provide a standard error or 95\% confidence interval for the ATT. In terms of the length of the confidence interval, SPSC with the time-varying components (i.e., SPSC-DT) yields the narrowest confidence interval, followed by SPSC with no time-varying component (i.e., SPSC-NoDT), and the approach based on OLS. 

\begin{table}[!htp] 
\renewcommand{\arraystretch}{1.2} \centering
\scriptsize
\setlength{\tabcolsep}{5pt} 
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\normalfont Estimator} & {\normalfont  OLS-NoReg} & {\normalfont  OLS-Standard} & {\normalfont   ASC} & {\normalfont  SCPI} & {\normalfont  SPSC-NoDT } & {\normalfont   SPSC-DT} \\ \hline

\multicolumn{1}{|c|}{\normalfont Estimate} & {\normalfont -1.021}  & {\normalfont -0.873}  & {\normalfont -0.912}  & {\normalfont -0.876}  & {\normalfont -0.813}  & {\normalfont -0.816}  \\ \hline
\multicolumn{1}{|c|}{\normalfont ASE} & {\normalfont 0.139}  & {\normalfont -}  & {\normalfont -}  & {\normalfont -}  & {\normalfont 0.084}  & {\normalfont 0.066}  \\ \hline
\multicolumn{1}{|c|}{\normalfont 95\% CI} & {\normalfont (-1.295,-0.748)}  & {\normalfont -}  & {\normalfont -}  & {\normalfont -}  & {\normalfont (-0.978,-0.648)}  & {\normalfont (-0.945,-0.688)}  \\ \hline


\end{tabular} 
\vspace*{0.2cm}
\caption{Summary Statistics of the Estimation of the Average Treatment Effect on the Treated.}
\label{tab:data:ATT} 
\end{table}	

We also constructed the pointwise prediction intervals based on the SPSC approach with time-varying components using the conformal inference approach in Section \ref{sec:Conformal}. For comparison, we also implemented the ASC and SCPI approaches. Figure \ref{fig:data:1} provides the visual summary of the result. For the post-treatment period $t\in \{218,\ldots,384\}$, we find that $\widehat{Y}_{t}^{(0)}$, the predictive value of the treatment-free potential outcome, have similar shapes for all methods. However, 95\% pointwise prediction intervals behave differently. Specifically, we focus on the average width of the prediction intervals over the post-treatment periods. The prediction intervals from the ASC and SCPI approaches have average widths of 0.091 and 0.114, respectively; in contrast, our method with time-varying components yields prediction intervals with average widths of 0.068, over 25\% narrower than those from the competing methods; see Supplementary Material \ref{sec:supp:Data} for the distribution of the prediction interval widths across time. The comparison reveals that our method appears to produce tighter predictions of treatment effect trends. Combining results in the simulation study and the data application, we conclude that our approach appears to perform quite competitively when compared to some leading alternative methods in the literature. 


% Figure environment removed

Additionally, for the sake of credibility, we conducted the following additional analysis for the application; the details can be found in  Supplementary Material \ref{sec:supp:Data}. First, we studied the trend of the residuals, the difference between the observed outcome and synthetic control, over the pre-treatment time periods. We observed that the OLS-NoReg, SCPI, and SPSC-DT estimators produced residuals without a deterministic trend over time, while the other three estimators (OLS-Standard, ASC, SPSC-NoDT) showed the opposite behavior. Notably, the SPSC-DT estimator appears to satisfy the zero mean condition of Assumption \ref{assumption:SC}, whereas the SPSC-NoDT estimator seems to violate this condition due to a non-zero deterministic trend over time. This again highlights the importance of accommodating time-varying components in the SPSC estimation procedure. 

Next, we performed the following falsification study. We restricted the entire analysis to the pre-treatment period in which the causal effect is expected to be null. We artificially defined a financial panic time in late July 1907, which is roughly three months before the actual financial panic. This resulted in the lengths of the pre- and post-treatment periods equal to $T_0' = 181$ and $T_1'=36$, respectively. The proposed SPSC-NoDT and SPSC-DT estimators resulted in the placebo ATT estimates of $-0.005$ and $0.005$ with 95\% confidence intervals of $(-0.025,0.016)$ and $(-0.004,0.013)$, respectively. The placebo ATT estimate obtained from the unconstrained OLS estimator was $-0.031$ with a 95\% confidence interval of $(-0.062,0.001)$. All 95\% prediction intervals include the null, consistent with the expectation of no treatment effect in the placebo period. Lastly, the constrained OLS estimator (i.e., OLS-Standard), ASC estimator, and SCPI estimator produced placebo ATT estimates of $-0.012$, $-0.032$, and $-0.015$, respectively, which are also close to zero; however, corresponding statistical inference was not available for these estimators. Therefore, these results provide no evidence against validity of the estimators. In Supplementary Material \ref{sec:supp:Data}, we provide a trajectory of the synthetic controls along with 95\% prediction intervals under the placebo treatment. Our findings indicate that the 95\% prediction intervals from the SPSC-DT estimator support the null causal effect. However, the ASC and SCPI estimators occasionally fail to do so during certain time periods. Therefore, we conclude that the SPSC-DT estimator provides a more reliable framework for analyzing the impact of financial panic on the stock prices of the two trust companies.

%In Section \ref{sec:supp:Data} of Supplementary Material, we provide additional results when different donor pools are used. In brief, all results are similar across choices of donors. 

\vspace{-0.4cm}
\section{Concluding Remarks}	\label{sec:Conclusion}

In this paper, we propose a novel SPSC approach in which the synthetic control is defined as a linear combination of donors' outcomes whose conditional expectation matches the treatment-free potential outcome in both pre- and post-treatment periods. The model is analogous yet more general than measurement error models widely studied in standard measurement error literature. Under the framework, we establish the identification of a synthetic control, and provide an estimation strategy for the ATT. Furthermore, we introduce a method for inferring the treatment effect through pointwise prediction intervals, which remains valid even in the case of a short post-treatment period. We validate our methods through simulation studies and provide an application analyzing a real-world financial dataset related to the 1907 Panic.

We reiterate that the SPSC framework differs from existing synthetic control methods in its identifying assumptions and interpretation. It views the synthetic control as an error-prone outcome measurement, without the need for specifying a generative model for the outcome, whereas existing approaches treat it as the projection of the outcome onto the donor's outcome space or the outcome itself. Despite these differences, both frameworks construct synthetic controls by optimally weighting donor units (according to their identifying assumptions), which are then used for treatment effect estimation. Additionally, like other synthetic control methods, the SPSC framework allows for time-varying confounders, as demonstrated in the generative models in Section \ref{sec:IFEM discussion}.

While, as mentioned in Section \ref{sec:SPSC:Assumption}, the SPSC framework may be viewed as a nonstandard form of instrumental variable approach, it is important to highlight key distinctions between the proposed SPSC approach and well-known instrumental variable approaches in dynamic panel data, such as in \citet{AndersonHsiao1981} and \citet{ArellanoBond1991}. In dynamic panel data models, endogeneity arises across different time periods, with the typical assumption that there is no within-time period endogeneity. As a result, these models use lagged variables as instruments to address cross-time endogeneity. In contrast, in the SPSC framework, endogeneity occurs within each time period, without specific assumptions about cross-time endogeneity. Consequently, the instrumental variable approach in this context operates within a single time period. We remark that, like the SPSC framework, many synthetic control models are agnostic about cross-time dependence structure; for example, the cross-time dependent structure of $\blambda_t$ in the IFEM \eqref{eq-IFEM} is agnostic. A notable exception to this agnostic perspective is the ``Instrumental variable-like SC estimator'' proposed by \citet{FermanPinto2019_arxiv}, an earlier version of \citet{FermanPinto2021}, which was developed in the presence of serial correlation in $\blambda_t$. Similar to estimators used in dynamic panel data models, it employs lagged variables as instruments.

As briefly mentioned in the introduction, the proposed SPSC framework has a connection to the single proxy control framework \citep{TT2013_COCA, SPC2024} developed for i.i.d. data. In particular, \citet{SPC2024} proposed an approach that relies on a so-called outcome bridge function, which is a (potentially nonlinear) function of outcome proxies. An important property of the outcome bridge function is that it is conditionally unbiased for the treatment-free potential outcome. Therefore, the proposed SPSC approach can be viewed as an adaptation of the outcome bridge function-based single proxy control approach to the synthetic control setting, where the outcome bridge function is known a priori to be a linear function of donors' outcomes. In Supplementary Material \ref{sec:supp:nonparametric full}, we present a general SPSC framework, which is designed to accommodate nonparametric and nonlinear synthetic controls. Therefore, this framework obviates the over-reliance on a linear specification of synthetic controls in the literature and establishes a more direct connection with the outcome bridge function-based single proxy approach presented in \citet{SPC2024}. Notably, the general SPSC framework addresses underdeveloped areas of the synthetic control literature by allowing for various types of outcomes, including continuous, binary, count, or a combination of these. 

In addition to the outcome bridge function-based approach, \citet{SPC2024} introduced two other single proxy control approaches for i.i.d. sampling. One approach relies on propensity score weighting, eliminating the need for specifying an outcome bridge function. The second approach uses both the propensity score and the outcome bridge function and, more importantly, exhibits a doubly-robust property in that the treatment effect in view is identified if either propensity score or outcome bridge function, but not necessarily both, is correctly specified. Consequently, a promising direction for future research would be to develop new SPSC approaches by extending these single proxy methods to the synthetic control setting. Such new SPSC approaches can be viewed as complementing the doubly-robust proximal synthetic control approach \citep{Qiu2022}. However, such extensions pose significant challenges due to (i) a single treated unit with non-random treatment assignment, (ii) multiple heterogeneous untreated donor units; and (iii) serial correlation and heteroskedasticity due to the time series nature of the data. In particular, non-random treatment assignment undermines the conventional notion of the propensity score, rendering it undefined. Approaches for addressing these challenges and developing corresponding statistical methods will be considered elsewhere. 