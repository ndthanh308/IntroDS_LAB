\RequirePackage{amsthm}

\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
%\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage[capitalise,noabbrev]{cleveref}
%\graphicspath{{./latexFigures/}}

%\input{preamble}
\usepackage{xr}
\usepackage{bookmark}
\usepackage{paralist,enumitem}
\usepackage{verbatim}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amssymb,amsmath,graphicx,hyperref}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.16} 
\usepackage{standalone}
\usepackage{mathtools}
\usepackage[caption=false]{subfig}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{enumerate}
\usepackage{algpseudocode}
\usepackage{tikz-3dplot}

\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Cb}{\mathbb{C}}
\newcommand{\Zb}{\mathbb{Z}}
\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Nb}{\mathbb{N}}
\newcommand{\dsum}{\displaystyle \sum}
\newcommand{\dint}{\displaystyle \int}
\newcommand{\dcup}{\displaystyle \cup}
\newcommand{\dcap}{\displaystyle \cap}
\newcommand{\dbcup}{\displaystyle \bigcup}
\newcommand{\dbcap}{\displaystyle \bigcap}
\newcommand*{\QEDB}{\null\nobreak\hfill\ensuremath{\square}}%
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\siam}[1]{{\color{red}\textbf{#1}}}
\newcommand{\siaml}[1]{{\color{red}#1}}

\newcommand{\doint}{\displaystyle \oint}
\newcommand{\dlim}{\displaystyle \lim}
\newcommand{\dprod}{\displaystyle \prod}
\newcommand{\srule}{\begin{center}\rule{20em}{0.05em}\end{center}}
\newcommand{\aliter}{\begin{center}\rule{20em}{0.05em}\end{center} \textbf{Aliter}: }
% Add a serial/Oxford comma by default.
\newcommand{\creflastconjunction}{, and~}
\newcommand{\siva}[1]{{\color{red}[#1]}}

\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}

\newcommand{\iso}{0.4}
\newcommand{\redcircle}[2]{\draw [fill=red] (#1,#2) circle (0.1);}
\newcommand{\redsquare}[2]{
	\draw [ultra thick] (#1,#2) rectangle (1+#1,1+#2);
	\redcircle{#1}{#2};
	\redcircle{1+#1}{#2};
	\redcircle{#1}{1+#2};
	\redcircle{1+#1}{1+#2};
}
\newcommand{\cube}[2]{
	\draw [ultra thick] (#1,#2) rectangle (1+#1,1+#2);
	\draw [ultra thick, loosely dotted] (\iso+#1,\iso+#2) rectangle (1+\iso+#1,1+\iso+#2);
	\draw [ultra thick] (1+#1,1+#2) -- (1+\iso+#1,1+\iso+#2);
	\draw [ultra thick, loosely dotted] (1+#1,#2) -- (1+\iso+#1,\iso+#2);
	\draw [ultra thick, loosely dotted] (#1,1+#2) -- (\iso+#1,1+\iso+#2);
	\draw [ultra thick, loosely dotted] (#1,#2) -- (\iso+#1,\iso+#2);
}

\newcommand{\textib}[1]{\textbf{\textit{\text{#1}}}}
\newcommand{\Bb}{\textbf{\textit{\text{B}}}}
\newcommand{\Cl}{{\mathcal{C}^{(l)}_i}}


\newcommand{\rect}[3]{\draw [ultra thick,fill=#1] (#2) rectangle (#3);}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand\mtiny[1]{\mbox{\tiny\ensuremath{#1}}}


\newcommand{\floor}[1]{
	\left\lfloor #1 \right \rfloor
}

\newcommand{\bkt}[1]{
	\left(#1\right)
}

\newcommand{\ip}[1]{
	\left \langle #1 \right \rangle
}

\DeclareRobustCommand{\cubex}[6]{
%draw the top and bottom of the cube
	\draw[#5] (#1,#2,#3) -- (#1,#2+#4,#3) -- (#1+#4,#2+#4,#3) -- (#1+#4,#2,#3) -- cycle;
	\draw[#5] (#1,#2,#3+#4) -- (#1,#2+#4,#3+#4) -- (#1+#4,#2+#4,#3+#4) -- (#1+#4,#2,#3+#4) -- cycle;
%draw the edges of the cube
	\draw[#5] (#1,#2,#3) -- (#1,#2,#3+#4);
	\draw[#5] (#1,#2+#4,#3) -- (#1,#2+#4,#3+#4);
	\draw[#5] (#1+#4,#2,#3) -- (#1+#4,#2,#3+#4);
	\draw[#5] (#1+#4,#2+#4,#3) -- (#1+#4,#2+#4,#3+#4);
    \node at (#1 + 0.5*#4,#2 + 0.5*#4,#3 + 0.5*#4) {#6};
}
\newcommand{\cmt}[1]{\iffalse {#1} \fi}

\theoremstyle{thmstyleone}%
\newtheorem{remark}{Remark}%
\newtheorem{lemma}{Lemma}%
\newtheorem{corollary}{Corollary}%
\newtheorem{definition}{Definition}%
\newtheorem{example}{Example}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers

\newtheorem{proposition}[theorem]{Proposition}% 

\raggedbottom

\begin{document}

\title[HODLR3D]{HODLR3D: Hierarchical matrices for $N$-body problems in three dimensions}

\author*[1]{\fnm{Kandappan} \sur{V A}}\email{kandappanva@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Vaishnavi} \sur{Gujjula}}\email{vaishnavihp@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Sivaram} \sur{Ambikasaran}}\email{sivaambi@iitm.ac.in}


\affil*[1]{\orgdiv{Department of Mathematics}, \orgname{Indian Institute of Technolgy Madras}, \orgaddress{\city{Chennai}, \postcode{600036}, \state{Tamil Nadu}, \country{India}}}

\affil[2]{\orgdiv{Robert Bosch Centre for Data Science and Artificial Intelligence}, \orgname{Indian Institute of Technolgy Madras}, \orgaddress{\city{Chennai}, \postcode{600036}, \state{Tamil Nadu}, \country{India}}}



\abstract{This article introduces HODLR3D, a class of hierarchical matrices arising out of $N$-body problems in three dimensions. HODLR3D relies on the fact that certain off-diagonal matrix sub-blocks arising out of the $N$-body problems in three dimensions are numerically low-rank. For the Laplace kernel in $3$D, which is widely encountered, we prove that all the off-diagonal matrix sub-blocks are rank deficient in finite precision. We also obtain the growth of the rank as a function of the size of these matrix sub-blocks. For other kernels in three dimensions, we numerically illustrate a similar scaling in rank for the different off-diagonal sub-blocks. We leverage this hierarchical low-rank structure to construct HODLR3D representation, with which we accelerate matrix-vector products. The storage and computational complexity of the HODLR3D matrix-vector product scales almost linearly with system size. We demonstrate the computational performance of HODLR3D representation through various numerical experiments. Further, we explore the performance of the HODLR3D representation on distributed memory systems. \emph{HODLR3D, described in this article, is based on a weak admissibility condition. Among the hierarchical matrices with different weak admissibility conditions in $3$D, only in HODLR3D did the rank of the admissible off-diagonal blocks not scale with any power of the system size. Thus, the storage and the computational complexity of the HODLR3D matrix-vector product remain tractable for $N$-body problems with large system sizes.}}

\keywords{Hierarchical matrices, HODLR, $N$-body problems}

\pacs[AMS Classification]{68Q25, 68R10, 68U05, 45B05, 68U20}

\maketitle

\section{Introduction}\label{sec:introduction}
Hierarchical matrix representations are used to accelerate $N$-body problems arising from applications such as particle simulations, high-order statistics, machine learning~\cite{gray2000n,litvinenko2019likelihood}, solving PDEs, radial basis function interpolation~\cite{coulier2016efficient,gumerov2007fast}, etc. In this article, we introduce a class of hierarchical matrix representation~\cite{hackbusch1999sparse,grasedyck2003construction,H2DKandappan} for matrices arising out of $N$ body problems in three dimensions to construct an algorithm that performs matrix-vector products that has almost linear computational cost~\footnote{We say a matrix algorithm has almost linear computational complexity if given $A \in \Cb^{N \times N}$, the computational cost of the algorithm scales as $\mathcal{O}\bkt{N^{1+\epsilon}}$ for all $\epsilon>0$.} both in time and space. One of the first almost linear complexity algorithms for $N$-body problems was introduced by Barnes and Hut~\cite{barnes1986hierarchical} (frequently addressed as the "Treecode"), which reduced the computational complexity of matrix-vector product from $\mathcal{O}\bkt{N^2}$ to $\mathcal{O}\bkt{N \log N}$. The Fast Multipole Method by Greengard and Rokhlin~\cite{greengard1987fast,greengard1988rapid,greengard1997new} further reduced the computational complexity of these $N$-body problems to $\mathcal{O}\bkt{N}$.

Algebraically, the Treecode and the FMM can be interpreted as the matrix corresponding to $N$-body problems having a hierarchical low-rank structure. This notion was pioneered by Hackbush~\cite{hackbusch1999sparse,grasedyck2003construction} in the late 1990's and early 2000's and these matrices were termed as hierarchical matrices ($\mathcal{H}$-matrix). These hierarchical matrices use a hierarchical tree to subdivide the matrix and identify the low-rank matrix sub-blocks at different levels in the hierarchical tree. A detailed description of these matrices is in~\Cref{sec:hmat}. These hierarchical matrices can be stored efficiently and matrix algorithms for these hierarchical matrices can be devised in almost linear computational cost.

There has been extensive research on hierarchical matrices in recent years. Some of the widely used hierarchical representations are Hierarchically Off-Diagonal Low-Rank (HODLR)~\cite{sa_thesis,SA_FDS_2013}, Hierarchically Semi-Separable (HSS)~\cite{chandrasekaran2005some,vandebril2005bibliography,vandebril2005note}, $\mathcal{H}^{2}$\cite{borm2003hierarchical,borm2003introduction,hackbusch2015hierarchical}, etc.   For a more detailed literature review on hierarchical matrices and their applications, we refer the readers to the articles~\cite{borm2003introduction,yokota2015fast,H2DKandappan} and the references therein. In addition to the hierarchical matrices, there are flat low-rank structures such as Block Low Rank (BLR) format, where the matrix is subdivided into $n_b\times n_b$ blocks with each block of size at most $b\times b$. Like, hierarchical representations, certain off-diagonal blocks in the BLR format are approximated and represented as low-rank matrices~\cite{amestoy2015improving,amestoy2017complexity}.

The particular hierarchical low-rank matrix which is of interest to this article is HODLR matrix representation of a dense matrix from $N$ body problems. In the HODLR matrix representation, all the off-diagonal blocks that result from the hierarchical subdivision of the underlying domain are approximated by low-rank matrices. Leveraging the HODLR matrix representation, we can construct an algorithm that performs matrix-vector in $\mathcal{O}(pN\log(N))$ where $p$ is the maximum rank of the off-diagonal sub-blocks~\cite{SA_FDS_2013}. The maximum rank of the off-diagonal blocks plays a critical role in the computational complexity of the HODLR matrix. For example, if we represent the dense matrix from a wide range of $N$ body problems in $1$D using HODLR representation, the off-diagonal blocks correspond to the vertex-sharing interactions. We know from~\cite{khan2022numerical,sa_thesis} that the ranks of the off-diagonal blocks due to the vertex sharing interaction scale as $\mathcal{O}\bkt{\log\bkt{N}}$. Using that fact we have the computational complexity of the matrix-vector product in HODLR representation scale as $\mathcal{O}\bkt{N\log^2\bkt{N}}$, i.e., it scales almost linearly. Whereas in the case of $N$ body problems from $2$D, the maximum rank of the off-diagonal blocks is due to the edge-sharing interactions whose rank scales roughly as $\mathcal{O}\bkt{\sqrt{N}}$~\cite{khan2022numerical,H2DKandappan}. So, the computational complexity of the matrix-vector product in HODLR representation scale roughly as $\mathcal{O}\bkt{N^{3/2}}$. In the case of the matrices from three-dimensional problems, the maximum rank of the off-diagonal blocks is due to the face-sharing interactions whose rank scales roughly as $\mathcal{O}\bkt{\sqrt[3]{N^2}}$ (refer \cref{tm:ranks3D} and~\cite{khan2022numerical} for more details), which results in the computational complexity for matrix-vector product using HODLR representation as roughly $\mathcal{O}\bkt{N^{5/3}}$.  Accordingly, the matrix-vector product through HODLR matrix representation does not scale almost linearly.

In our recent work~\cite{H2DKandappan}, we had proposed a new hierarchical matrix to represent matrices arising out of $N$-body problems in two dimensions which performs matrix-vector product in almost linear complexity. In this article, we extend this notion to $N$ body problems in three dimensions and introduce a new class of hierarchical matrices, HODLR3D. HODLR2D~\cite{H2DKandappan} and HODLR3D representation are the extensions of HODLR matrix representation in two and three dimensions by a judicious choice of admissibility condition. The complexity of matrix-vector product using HODLR3D representation scales as $\mathcal{O}(pN\log(N))$, where $p$ is the maximum rank of the compressed off-diagonal blocks. For HODLR3D, the maximum rank of the compressed off-diagonal blocks is due to vertex-sharing interactions. Through our numerical illustrations and~\cref{tm:ranks3D} we show that the vertex-sharing interaction scale as $\mathcal{O}\bkt{\log^{3}\bkt{N}}$. Hence, like HODLR2D, HODLR3D also scales almost linear in complexity for the matrix-vector product.

The main highlights of the article are:
\begin{itemize}
    \item 
    We illustrate the scaling of ranks of different off-diagonal blocks for handpicked kernel functions numerically. We state~\Cref{tm:ranks3D}, which provides bounds on the ranks of different off-diagonal blocks for the Laplacian kernel in $3$D. The proof can be found in~\Cref{sec:Proof}. We refer the readers to~\cite{khan2022numerical} for the bounds on the ranks of different off-diagonal blocks for a bigger class of kernel functions.
    \item
    Based on the outcomes of~\Cref{tm:ranks3D} and the numerical illustrations, we build the HODLR3D representation, wherein we only choose to compress the off-diagonal blocks corresponding to interactions between vertex sharing neighbours and well-separated boxes of the hierarchical tree to achieve an almost linear complexity algorithm.
    \item 
    We perform various numerical experiments to illustrate the performance of the HODLR3D matrix-vector product. 
\end{itemize}

\subsection{Main Result}
We state the main theorem that gives the scaling of ranks of the different off-diagonal blocks of the matrix arising out of Green's function of the 3D Laplace equation. The proof of~\Cref{tm:ranks3D} can be found in~\Cref{sec:Proof}.~\cref{tm:ranks3D} can be extended to a wide range of kernel functions, the proof of which can be found in~\cite{khan2022numerical}. On comparing with~\cite{khan2022numerical}, we get a slightly tighter bound in~\cref{tm:ranks3D} for the kernel $1/r$. This should not be surprising since~\cite{khan2022numerical} is applicable for a wide range of kernel functions as opposed to~\Cref{tm:ranks3D}, which is applicable only for $1/r$ kernel function.
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!black,title=\textbf{Rank of different interactions for the Laplace kernel in $3$D}]
        \begin{theorem} \label{tm:ranks3D}
            Consider a box $\Bb\subseteq\mathbb{R}^3$. The boxes $\Bb_1\subseteq\Bb$ and $\Bb_2\subseteq\Bb$ are from the hierarchical subdivision of the box $\Bb$ using a balanced octree. Let $\{q_j\}_{j=1}^N$ be $N$ uniformly located charges at $\{z_j\}_{j=1}^N$ inside a box $\Bb_1$ of side length $l$. Let $Q = \dsum_{i=1}^N \abs{q_i}$. The potential due to these $N$ charges at $M$ locations $\{w_i\}_{i=1}^M$, with $M \geq N$, inside another box $\Bb_2$ is given by $\phi_i = \dsum_{j=1}^N \dfrac{q_j}{\abs{w_j-z_j}}$. In matrix-vector parlance, we have
            $$\vec{\phi}=A\vec{q}$$
            where $\vec{q} \in \Rb^{N \times 1}$, $\vec{\phi} \in \Rb^{M \times 1}$ and $A \in \Rb^{M \times N}$ with $A_{ij} = \dfrac{1}{\abs{w_j-z_j}}$. Then for a given $\epsilon > 0$, there exists a matrix $\tilde{A} \in \Rb^{M \times N}$ with rank at the most $\tau$ that approximates $\phi_i$ such that $\abs{\phi_i-\bkt{\tilde{A}q}_i} < \epsilon$ and $\tau \in \mathcal{O} \bkt{R(N)\log^2\bkt{\dfrac{S(N)Q}{l\epsilon}}}$, where
            \begin{itemize}
                \item $R(N) = 1,S(N)=1$ if the boxes $\Bb_1$ and $\Bb_2$ are at least one box away, i.e., $\text{dist}\bkt{\Bb_1,\Bb_2} \geq l$, where $dist(x,y)$ is the Euclidean distance between the sets $x$ and $y$.
                \item $R(N) = \log(N),S(N)=\sqrt[3]{N}$ if the boxes $\Bb_1$ and $\Bb_2$ are vertex sharing neighbors, i.e., $\Bb_1\cap\Bb_2$ is at the most a vertex.
                \item $R(N) = \sqrt[3]{N}, S(N)=\sqrt[3]{N^2}$ if the boxes $\Bb_1$ and $\Bb_2$ are edge sharing neighbors, i.e., $\Bb_1\cap\Bb_2$ is at the most a line.
                \item $R(N) = \sqrt[3]{N^2}, S(N)=N$ if the boxes $\Bb_1$ and $\Bb_2$ are face sharing neighbors, i.e., $\Bb_1\cap\Bb_2$ is at most a $2$D plane.
            \end{itemize}
        \end{theorem}
    \end{tcolorbox}
    

\subsection{Outline of the article}
The rest of the article is organized as follows: In~\Cref{sec:OctTree}, we describe the octree on which we build the hierarchical matrices and also give a brief on HODLR and a $\mathcal{H}$ matrix structures. Further in~\Cref{sec:rankGrowth}, we numerically illustrate the growth of ranks for various kinds of interactions and prove~\cref{tm:ranks3D}. And in~\Cref{sec:HODLR3D}, we describe the algorithm for HODLR3D matrix-vector product. We present the numerical results in~\Cref{sec:numericalResults} and compare the performance of HODLR3D with HODLR and a $\mathcal{H}$ matrix representation. 
We describe the parallel implementation of HODLR3D and present numerical results in~\Cref{sec:parH3D}.

\section{Preliminaries}\label{sec:OctTree}

In this section, we describe the construction of the tree that is used to build the HODLR3D matrix. We briefly describe the HODLR and $\mathcal{H}$ matrix representations, with which we compare the HODLR3D later in the article. 
\subsection{Construction of tree}
We assume the computational domain to be a cube $\textbf{\textit{B}} \subset \mathbb{R}^3$, that has the support of the particles. Level $0$ of the tree is the domain $\textbf{\textit{B}}$ itself. A cube at level $k$ is subdivided into $8$ cubes that belong to level $k+1$ of the tree. The former is said to be the parent of the latter, and the latter are said to be the children of the former. The sub-division is carried on until the depth of the tree is $L$, where a cube at level $L$, called a leaf, contains at most $N_{max}$ particles. $N_{max}$ is a user-specified threshold that defines the maximum number of particles in a leaf. We denote the octree by $\mathcal{T}^{L}$. We illustrate the construction of octree till level $2$ in Figure~\ref{fig:octTree}. In this article, we work with the balanced tree for simplicity, though an adaptive oct-tree or a K-d tree too can be considered.

% Figure environment removed
    
\subsection{\texorpdfstring{$\mathcal{H}$}{Hc}-matrix}
\label{sec:hmat}
$\mathcal{H}$-matrix is the hierarchical low-rank representation of a particular class of dense matrices based on an admissible condition. The admissibility condition helps in identifying the off-diagonal blocks due to the interaction between blocks in a particular level to be represented as low-rank matrices with considerable accuracy. In literature, there exist two classes of hierarchical matrix representations based on the admissibility condition, viz., $\mathcal{H}$ matrix based on strong or standard admissibility condition and $\mathcal{H}$ based on weak admissibility condition~\cite{hackbusch2004hierarchical}. In the case of $\mathcal{H}$-matrix with weak admissibility condition, the admissible blocks are closer, whereas, in the case of the strong or standard admissible condition, the admissible blocks are well-separated. In this article, we consider the $\mathcal{H}$-matrix to be built on the hierarchical tree, $\mathcal{T}^{L}$ with the strong or standard admissibility condition given by~\Cref{eq:adm}. Consider cells $X$ and $Y$ that are at the same level of $\mathcal{T}^{L}$. Let the index sets of the particles lying in cells $X$ and $Y$ be $I_{X}$ and $I_{Y}$. The matrix sub-block $K(I_{X}, I_{Y})$, represented using MATLAB notation, is approximated by a low-rank matrix if the following admissibility condition is satisfied.
\begin{gather} 
\max(diam(X), diam(Y)) \leq \eta \quad dist(X,Y),\quad\text{where,}\label{eq:adm}\\
diam(X)= \sup\{\|x-y\|_{2}:x,y\in X\},\label{eq:diam}\\
dist(X,Y) = \inf\{\|x-y\|_{2}:x\in X, y\in Y\},\label{eq:dist}
\end{gather}
    and $\eta$ is a parameter that controls the admissibility condition. For $\eta=\sqrt{3}$, the interaction between particles in cells that are well-separated, i.e., those cells that do not share a boundary, is approximated by a low-rank matrix. In~\cref{fig:H_matrix}, we illustrate the low-rank structure of a $\mathcal{H}$ matrix, with $\eta=\sqrt{3}$, at levels 1 and 2.
% Figure environment removed
    
\subsection{HODLR matrix}
HODLR matrix is an example of a $\mathcal{H}$ matrix with weak admissibility condition. It is constructed by compressing all the off-diagonal sub-blocks~\cite{SA_FDS_2013}. Equivalently, other than the self-interaction, all other interactions are approximated by a low-rank matrix.
Usually, HODLR is built on a K-d tree, but in this article, since we develop HODLR3D on an octree, to be on the same terms, we consider HODLR to be built on an octree. In Figure~\ref{fig:HODLR_matrix}, we illustrate the low-rank structure of the HODLR matrix at levels 1 and 2.
% Figure environment removed
    
\section{Rank growth of different interactions in 3D}\label{sec:rankGrowth}
In this section, we analyze numerically the ranks of sub-matrices corresponding to different types of interaction in $3$D for handpicked widely used kernel functions. We then prove the growth in ranks for various off-diagonal blocks of the matrix for Green's function of the Laplace equation in $3$D, which is $\dfrac1{r}$ kernel. 

\subsection{Numerical rank for different kernels}
We consider a cube $X$ belonging to the octree. And we further consider cubes $V$, $E$, and $F$ that share a vertex, edge, and face with cube $X$ and a cube $W$ that is well-separated to $X$ as shown in~\cref{fig:interactions}.
% Figure environment removed
We uniformly distribute $N$ particles in each cube $W$, $F$, $X$, $E$, and $V$ at $\{r_{i}\}_{i=1}^{5N}$. We consider the interaction between two particles at $r_{i}$ and $r_{j}$ to be
\begin{equation}
    K\bkt{i,j} = \begin{cases}
    0, & \text{if} \quad i=j\\
   {f\bkt{\magn{r_{i}-r_{j}}_{2}}}, & \text{otherwise}
\end{cases}\label{eq:Laplace3D}
\end{equation}
where $f(r):\mathbb{R}\rightarrow\mathbb{R}$. Let the index sets of the particles lying in each of these cubes $W$, $F$, $X$, $E$, and $V$ be $I_{W}$, $I_{F}$, $I_{X}$, $I_{E}$, and $I_{V}$ respectively. The matrices $K(I_{X}, I_{W})$, $K(I_{X}, I_{F})$, $K(I_{X}, I_{E})$, and $K(I_{X}, I_{V})$ correspond to different off-diagonal sub-matrices. We illustrate in Figure~\ref{NumericalRanks}, the growth of numerical ranks\footnote{For an $\epsilon>0$, the numerical rank of matrix $K$, $r_{\epsilon}(K)$ is defined as $r_{\epsilon}(K) = \max\{k\in\{1, 2,\dots N\}:\frac{\sigma_{k}}{\sigma_{1}}>\epsilon\}$ where $\sigma_{1}\geq \sigma_{2}\geq \dots \sigma_{N}$ are the singular values of $K$.} of the off-diagonal sub-matrices from four kinds of interactions for the three kernel functions $\bm{K}(x,y) = f(r)$ where, $r = \magn{x-y}_{2}$, defined as i) $\frac{1}{r}$ ii) $\frac{1}{r^4}$ iii) $\frac{\cos(r)}{r}$. It is to be observed that the ranks of the face and edge-sharing interactions $K(I_{X}, I_{F}) \text{ and } K(I_{X}, I_{E})$, for the three kernels, scale roughly as $\mathcal{O}(N^{2/3})$ and $\mathcal{O}(N^{1/3})$ respectively, and that of the vertex sharing interaction $K(I_{X}, I_{V})$ scales roughly as $\mathcal{O}(\log^{3}(N))$ and the well-separated interaction $K(I_{X}, I_{W})$ does not scale with $N$.
% Figure environment removed
We tabulate this observation in~\cref{tab:rankgrowth}.
% From the numerical experiments, we observe the growth of rank of various interactions as in~\cref{tab:rankgrowth}.
\begin{table}[!htbp]
\centering
\caption{Numerical observation of the scaling of ranks of different off-diagonal blocks of size $N$}
\label{tab:rankgrowth}
\begin{tabular}{@{}ll@{}}
\toprule
Off-diagonal block & Rough scaling of numerical rank      \\ \midrule
$K(I_X,I_W)$        & $\mathcal{O}\bkt{1}$ \\
$K(I_X,I_V)$        & $\mathcal{O}\bkt{\log^{3}\bkt{N}}$ \\
$K(I_X,I_E)$        & $\mathcal{O}\bkt{\sqrt[3]{N}}$ \\
$K(I_X,I_F)$        & $\mathcal{O}\bkt{\sqrt[3]{N^2}}$ \\ \bottomrule
\end{tabular}
\end{table}
We have numerically shown the growth of ranks of various off-diagonal sub-blocks for widely used kernel functions in $3$D, or equivalently we have numerically established that the matrix possesses a low-rank structure which can be leveraged to construct fast matrix algorithms. 


\subsection{Rank of off-diagonal blocks for Laplacian Kernel in \texorpdfstring{$3$}{3}D}
\label{sec:Proof}
In the previous subsection, we showed the growth of ranks of various off-diagonal blocks numerically for the Laplacian Kernel in $3$D. We will prove in~\Cref{tm:ranks3D}, the scaling of ranks for various off-diagonal blocks due to different interactions resulting from the hierarchical subdivision. 
\textbf{Proof of \cref{tm:ranks3D}.}
The proof of~\cref{tm:ranks3D} uses multipole expansions of the Laplacian kernel in $3$D. So we state here the multipole expansion lemma. The multipole expansions lemma approximates the potential at a point P due to $N$ charges using~\eqref{eq:multipole}. The proof of the lemma and its detailed analysis can be found in~\cite{beatson1997short,greengard1988rapid}. 
The potential at $M$ locations due to the $N$ charges can be represented as a matrix-vector product. By multipole expansions lemma, we can approximate the potential at $M$ locations through a low-rank representation as stated in~\cref{tm:prank}.
\begin{lemma}[Multipole Expansion]
    \label{tm:multipole}
        Suppose that $N$ charges of strengths $\{q_i\}_{i=1}^N$ are located at $\{\vec{z}_i\}_{i=1}^N$ whose spherical coordinates are $\{\bkt{\rho_i,\alpha_i,\beta_i}\}_{i=1}^N$ inside a sphere of radius $a$ and we have $\rho_i<a$. Then for any point $\vec{P} \in \Rb^3$ with the spherical coordinate $(r,\theta,\gamma)$ outside the sphere of radius $a$, the potential at $P$ due to the $1/\tilde{r}$ kernel (i.e., $\tilde{r}_i =\abs{\vec{P}-\vec{z}_i} $) is given by
        \begin{gather}
        \Phi\bkt{P} = \dsum_{n=0}^{\infty} \dsum_{m=-n}^{n} \dfrac{M_n^m}{r^{n+1}} Y_n^{m}\bkt{\theta,\gamma}, \quad \text{where,}\\
        M_n^m = \dsum_{i=1}^N q_i \rho_i^n Y_n^{-m}\bkt{\alpha_i,\beta_i},
        \end{gather}
        and $Y_n^{m}$ are the spherical harmonics. Furthermore, for any $p \geq 1$, we have
        \begin{equation}\label{eq:multipole}
            \abs{\Phi(P)-\dsum_{n=0}^p \dsum_{m=-n}^n \dfrac{M_n^{m}}{r^{n+1}}Y_n^{m}\bkt{\theta,\gamma}} \leq \dfrac{Q}{r-a}\bkt{\dfrac{a}r}^{p+1} = \dfrac{Q}a \bkt{\dfrac1{c-1}}\left(\dfrac1c\right)^{p+1}
        \end{equation}
        where $Q = \dsum_{i=1}^N \abs{q_i}$, $c=\dfrac{r}{a}$.
\end{lemma}
\begin{corollary}[Potential through rank-$\tau$ approximation]
 \label{tm:prank}
 From~\cref{tm:multipole}, the potential at $P\bkt{r_i,\theta_i,\gamma_i}$ using multipole expansions can be written in the matrix-vector format as follows:   
 \begin{equation}
     \phi_i = \dsum_{j=1}^N \dfrac{q_j}{\tilde{r}_{ij}} \approx \tilde{\phi}_i =
     \begin{bmatrix} 
         \dfrac{1}{r_i} & \dfrac{1}{r_i^2} & \dfrac{1}{r_i^3} & \cdots &  \dfrac{1}{r_i^{p+1}}
     \end{bmatrix}
     \begin{bmatrix}  
         \bkt{M_0^0}Y_0^0(\theta_i,\gamma_i) \\ 
         \dsum_{m=-1}^1 \bkt{M_1^m}Y_1^m(\theta_i,\gamma_i) \\ 
         \dsum_{m=-2}^2 \bkt{M_2^m}Y_2^m(\theta_i,\gamma_i)\\ 
         \vdots \\ 
         \dsum_{m=-p}^p \bkt{M_p^m}Y_p^m(\theta_i,\gamma_i)
     \end{bmatrix},
 \end{equation}     
 which can be rewritten as, 
  \begin{gather}
     \tilde{\phi}_i =  u_i V \vec{q},\quad\text{where,}\\
\begin{aligned}
u_i &=
\left[\begin{matrix}
\dfrac{Y_0^0(\theta_i,\gamma_i)}{r} & \dfrac{Y_1^{-1}(\theta_i,\gamma_i)}{r^2} & \dfrac{Y_1^0(\theta_i,\gamma_i)}{r^2} & \dfrac{Y_1^1(\theta_i,\gamma_i)}{r^2}&  \cdots
\end{matrix}\right.\\
&\qquad\qquad
\left.\begin{matrix}
\dfrac{Y_p^{-p}(\theta_i,\gamma_i)}{r^{p+1}}  & \cdots & \dfrac{Y_p^0(\theta_i,\gamma_i)}{r^{p+1}}   &  \cdots & \dfrac{Y_p^p(\theta_i,\gamma_i)}{r^{p+1}}
\end{matrix}\right]
\end{aligned}\label{theta-alt}\\    
     V = \begin{bmatrix}  
     Y_0^0(\alpha_1,\beta_1) & Y_0^0(\alpha_2,\beta_2) & \cdots & Y_0^0(\alpha_N,\beta_N) \\
     \rho_1 Y_1^{1}(\alpha_1,\beta_1) & \rho_2 Y_1^{1}(\alpha_2,\beta_2) & \cdots & \rho_N Y_1^{1} (\alpha_N,\beta_N)\\ 
     \rho_1 Y_1^{0}(\alpha_1,\beta_1) & \rho_2 Y_1^{0}(\alpha_2,\beta_2) & \cdots & \rho_N Y_1^{0} (\alpha_N,\beta_N)\\
     \rho_1 Y_1^{-1}(\alpha_1,\beta_1) & \rho_2 Y_1^{-1}(\alpha_2,\beta_2) & \cdots & \rho_N Y_1^{-1} (\alpha_N,\beta_N)\\
         \cdots & & &\\ 
         \rho_1^p Y_p^{p}(\alpha_1,\beta_1) & \rho_2^p Y_p^{p}(\alpha_2,\beta_2) & \cdots & \rho_N^p Y_p^{p} (\alpha_N,\beta_N)\\ 
         \vdots & & &\\ 
         \rho_1^p Y_p^{0}(\alpha_1,\beta_1) & \rho_2^p Y_p^{0}(\alpha_2,\beta_2) & \cdots & \rho_N^p Y_p^{0} (\alpha_N,\beta_N)\\
         \vdots & & &\\ 
     \rho_1^p Y_p^{-p}(\alpha_1,\beta_1) & \rho_2^p Y_p^{-p}(\alpha_2,\beta_2) & \cdots & \rho_N^p Y_p^{-p} (\alpha_N,\beta_N)\\
     \end{bmatrix},\label{eq:vapp}
 \end{gather} and 
 $\vec{q} = \begin{bmatrix} 
     q_1 & q_2 & \cdots & q_N
     \end{bmatrix}^T$.
 Then the potential $\{\phi_i\}_{i=1}^M$ at $M$ locations which are located outside the sphere of radius `$a$' due to $N$ particles (where $N\leq M$) located inside a sphere of radius `$a$' can be approximated as $\{\tilde{\phi}_i\}_{i=1}^M$ using a matrix $\tilde{A}$ with rank-$\tau$ through multipole expansions. 
 \begin{equation}
     \tilde{A} = UV,
 \end{equation}
 where $U = \begin{bmatrix} u_1 & u_2 & \cdots & u_M
 \end{bmatrix}^T\quad \text{and } U\in\mathbb{R}^{M\times(p+1)^2},$ $V\in\mathbb{R}^{(p+1)^2\times N}$ as in~\cref{eq:vapp}.
 The matrix $\tilde{A}$ can have a maximum rank of $\tau =\bkt{p+1}^2$ and for any $p$, we have $\abs{\phi_i - \bkt{\tilde{A}\vec{q}}_i} < \epsilon$, where $\epsilon > 0$ and $1\leq i \leq M$.
\end{corollary}
\begin{proof}[Proof of~\cref{tm:ranks3D}]
Consider the computational domain, a hypercube $\bm{B}$ in $3$D, which is hierarchically subdivided as in~\cref{fig:octTree} and represented using an octree, $\mathcal{T}^L$. Then each cube in a level has at the most $4$ types of interaction, viz., well-separated interaction, vertex-sharing interaction, edge-sharing interaction and face-sharing interaction. For a better interactive view of the figures used in the proof, please check \url{https://kandapva.github.io/hodlr3d/}.

\textbf{Case (i)}:(\underline{Boxes $\Bb_1$ and $\Bb_2$ are at least one box away}).
As shown in~\cref{fig:3d_far}, consider two boxes $\Bb_1 = [0,l]^3$ and $\Bb_2 = [2l,3l]\times[0,l]^2$, that is one box away. 
	% Figure environment removed
	Then we have a sphere $S:(r_s,\vec{c}_s)$ that circumscribes the box $\Bb_1$, where radius $r_s = \dfrac{l\sqrt{3}}{2}$  and center $\vec{c}_s = \bkt{\dfrac{l}{2},\dfrac{l}{2},\dfrac{l}{2}}$. The sphere $S$ encloses all the charges in the Box $\Bb_1$ such that $|\rho_i| < \dfrac{l\sqrt{3}}{2}$ and the minimum distance between the Box $B_2$ and the center of the sphere is $\dfrac{3l}{2}$. So in the Multipole Expansion lemma, we have $a  =  \dfrac{l\sqrt{3}}{2}$ and $c = \dfrac{\dfrac{3l}{2}}{\dfrac{l\sqrt{3}}{2}} = \sqrt{3}$. Then from~\cref{tm:prank}, we can approximate the potential using a rank-$\tau$ approximation such that,
 \begin{displaymath}
     \abs{\phi_i-\tilde{\phi}_i} \leq \dfrac{2Q}{3l} \bkt{\dfrac{1}{\sqrt3-1}} \bkt{\dfrac{1}{\sqrt{3}}}^{p+1},
 \end{displaymath}
and choosing $p+1 = \ceil{\dfrac{\log{\dfrac{2Q}{3l\epsilon (\sqrt3 - 1)}}}{\log{\sqrt3}}}$ ensures,
$\abs{\phi_i-\tilde{\phi}_i} \leq \epsilon.$ The rank $\tau$ is bounded above by $\ceil{\dfrac{\log{\dfrac{2Q}{3l\epsilon (\sqrt3 - 1)}}}{\log{\sqrt3}}}^2$.
So when the boxes are separated at least by $l$, we have a rank-$\tau$ matrix approximation $\tilde{A}$, where 
\begin{displaymath}
    \boxed{\tau \in  \mathcal{O} \bkt{\log^2\bkt{Q/l\epsilon}}}\quad \text{such that,}\quad \abs{\phi_i-\bkt{\tilde{A}q}_i}<\epsilon.
\end{displaymath}

\textbf{Case (ii)}:(\underline{Boxes $\Bb_1$ and $\Bb_2$ are Vertex sharing neighbors}).
Consider two boxes $\Bb_1 = [0,l]^3$ and $\Bb_2 = [l,2l]^3$ as shown in~\cref{fig:3d_ver}, that shares at most a vertex.
	% Figure environment removed
	To prove our theorem, we subdivide the cube $\Bb_1$ into 8 smaller cubes of length $l/2$. We form $\Bb^{(1)}_1$, which is the union of the 7 smaller cubes that do not share a boundary with $\Bb_2$. The remaining box, which shares a vertex with $\Bb_2$, is named as $\bar{\Bb}^{(1)}_1$.
    This is shown in~\cref{fig:3d_ver_sub}. For the charges in Box $\Bb^{(1)}_1$, we have sphere $S_1:(r_s^{(1)},\vec{c}_s^{(1)})$ that encloses all the charges which is centered at $\vec{c}_s^{(1)} = \bkt{\dfrac{l}{4},\dfrac{l}{4},\dfrac{l}{4}}$ and radius $r_s^{(1)} = \dfrac{l\sqrt{17}}{4}$. The minimum distance between the box $\Bb_2$ and the center of the sphere is $\dfrac{3l\sqrt{3}}{4}$ and hence in ~\cref{tm:multipole} we have $c = \sqrt{\dfrac{27}{19}}$. By using~\cref{tm:prank}, the potential, $\phi^{(1)}$ due to charges inside $\Bb^{(1)}_1$ can be approximated with $\tilde{\phi}^{(1)}$ through a rank-$\tau$ approximation. The corresponding matrix is $\tilde{A}^{(1)}$, and the potential $\tilde{\phi}^{(1)} = \tilde{A}^{(1)}\Vec{q}^{(1)}$ such that, 
 \begin{displaymath}
     \abs{\phi^{(1)}-\tilde{\phi}^{(1)}}\leq \dfrac{4Q}{l\bkt{\sqrt{27}-\sqrt{19}}}\bkt{\sqrt{\dfrac{19}{27}}}^{p+1}.
 \end{displaymath}
	If we choose $p+1 \in \ceil{\dfrac{\log\bkt{\dfrac{4Q}{l\epsilon_1\bkt{\sqrt{27}-\sqrt{19}}}}}{\log{\bkt{\sqrt{\dfrac{27}{19}}}}}}$ ensures $\abs{\phi^{(1)}_i-\bkt{\tilde{A}^{(1)}\vec{q}^{(1)}}_i} < \epsilon_1.$
  The rank $\tau$ of $\tilde{A}^{(1)}$ is bounded above by $\ceil{\dfrac{\log\bkt{\dfrac{4Q}{l\epsilon_1\bkt{\sqrt{27}-\sqrt{19}}}}}{\log{\bkt{\sqrt{\dfrac{27}{19}}}}}}^2$.
	We perform this hierarchical subdivision $\kappa$ times then, box $\Bb_1 = \bar{\Bb}^{(\kappa)}_1\dcup_{j=0}^{\kappa}\Bb^{(j)}_1$. Since we have assumed the $N$ charges be distributed uniformly in the box $\Bb_1$, the hierarchical subdivision with $\kappa>\log_8{\bkt{N}}$ results in the box $\bar{\Bb}^{(\kappa)}_1$ without any charges. Now for each box $\Bb^{(j)}_1|_{j=1}^{\kappa}$, we can circumscribe a sphere $S_j:(r_s^{(j)},\vec{c}_s^{(j)})$, with a radius of $r_s^{(j)}=\dfrac{l\sqrt{17}}{2^{j+1}}$. Then from~\cref{tm:multipole,tm:prank} the potential due to the charges inside the box $\Bb^{(j)}_1|_{j=1}^{\kappa}$ be $\phi^{(j)}_1|_{j=1}^{\kappa}$, for which we have approximate potential $\tilde{\phi}^{(j)}_1|_{j=1}^{\kappa}$ through a rank-$\tau$ matrix approximations. At each level of hierarchical subdivision we have $\tilde{A}^{(2)} \cdots \tilde{A}^{(\kappa)}$ with rank-$\tau$ approximation and at level $1\leq j\leq \kappa$ we have, $\abs{\phi^{(j)}-\tilde{\phi}^{(j)}}<\epsilon_j,$
 and $\epsilon_j = \dfrac{2^{j+1}Q}{l\bkt{\sqrt{27}-\sqrt{19}}}\bkt{\sqrt{\dfrac{19}{27}}}^{p_j+1}$,$\epsilon_j\quad \forall j\in\{1,\cdots,\kappa\},$ where $\epsilon_j = 2^{j-1}\epsilon_1$. $p_j+1$ is given by $\ceil{\dfrac{\log\bkt{\dfrac{2^{j+1}Q}{l\epsilon_1\bkt{\sqrt{27}-\sqrt{19}}}}}{\log{\bkt{\sqrt{\dfrac{27}{19}}}}}}$ and the rank $\tau_j$ at each level is bounded above by $\ceil{\dfrac{\log\bkt{\dfrac{2^{j+1}Q}{l\epsilon_1\bkt{\sqrt{27}-\sqrt{19}}}}}{\log{\bkt{\sqrt{\dfrac{27}{19}}}}}}^2$. Now for hierarchical subdivision of box $\kappa$ times , the potential $\phi = \dsum_{j=1}^{\kappa}\phi^{(j)}$ at $\Bb_2$ due to charges at $\Bb_1$ are approximated through rank-$\tau$ approximation $\dsum_{j=1}^{\kappa}\tilde{\phi}^{(j)}$ then,
	\begin{equation} \label{eq:ver_ineq}
		\begin{split}
		\abs{\phi_i - \tilde{\phi}_i} &= \abs{\dsum_{j=1}^{\kappa}\phi^{(j)}_i - \dsum_{j=1}^{\kappa}\tilde{\phi}^{(j)}_i} = \abs{\dsum_{j=1}^{\kappa}\bkt{\phi^{(j)}_i - \tilde{\phi}^{(j)}_i}}\\
		 & \leq \dsum_{j=1}^{\kappa}\abs{\phi^{(j)}_i - \tilde{\phi}^{(j)}_i} < \dsum_{j=1}^{\kappa}\epsilon_j
	\end{split}
	\end{equation}
	Using the relation between $\epsilon_j$, the sum $\dsum_{j=1}^{\kappa}\epsilon_j$ becomes $\epsilon_1\dsum_{j=1}^{\kappa}2^{j-1} = (2^{\kappa}-1)\epsilon_1$. We know that for uniform distribution of particles, the recursive subdivision of box $\Bb_1$ for $\kappa>\log_8\bkt{N}$ results in smaller boxes with no particles. So keeping $\kappa \approx \log_8\bkt{N}$ gets an error bound in $\epsilon$, i.e., we choose $\epsilon_1 = \dfrac{\epsilon}{\sqrt[3]{N}}$ to get, $\abs{\phi_i - \tilde{\phi}_i} < \epsilon$. 
	The rank-$\tau$ matrix approximation $\tilde{A}^{(j)}$, at a level $j$ is bounded above by
 \begin{displaymath}
     \ceil{\log^2 \bkt{\dfrac{2Q\sqrt[3]{N}}{l\epsilon}}}.
 \end{displaymath}
The total rank $\tau$ of the matrix involved in the approximation is bounded above by
\begin{displaymath}
    \kappa \ceil{\log^2 \bkt{\dfrac{2Q\sqrt[3]{N}}{l\epsilon}}}
\end{displaymath}
 and by setting the number of levels, $\kappa \approx \log_8\bkt{N}$, the rank of the matrix due to the interaction between two vertex sharing boxes  is bounded above by  
 \begin{displaymath}
     \boxed{\tau \in \mathcal{O}\bkt{ \log_8\bkt{N}\log^2 \bkt{\dfrac{2Q\sqrt[3]{N}}{l\epsilon}}},}\quad \text{where,} \quad \abs{\phi_i - \bkt{\tilde{A}q}_i} < \epsilon.
 \end{displaymath}

\textbf{Case (iii)}:(\underline{Boxes $\Bb_1$ and $\Bb_2$ are Edge sharing neighbors}).
Consider two boxes $\Bb_1 = [0,l]^3$ and $\Bb_2 = [l,2l]^2\times[0,l]$, that shares at most an edge as shown in~\cref{fig:3d_edge}.
	% Figure environment removed
	In this case, we will subdivide the box $\Bb_1$ into $8$ smaller boxes of side length $l/2$. We group the boxes that do not share a boundary with the box $\Bb_2$ as $\Bb_1^{(1)}=\bigcup_{j=0}^{6}\Bb_1^{(1,j)}$, whereas the remaining two boxes are $\bar{\Bb}_1^{(1,1)}$, $\bar{\Bb}_1^{(1,2)}$ (i.e., $\Bb_1-\Bb^{(1)}_1 = \dbcup_{i=1}^{2} \bar{\Bb}_1^{(1,i)}$).  So, for the box $\Bb^{(1)}$, that donot share a boundary with $\Bb_2$, we can circumscribe a sphere $S:(r_s,\vec{c}_s)$ with center $\vec{c}_s = \bkt{\dfrac{l}{2},\dfrac{l}{4},\dfrac{l}{4}}$ and radius $ r_s = l\sqrt{\dfrac{7}{8}}$ that encloses all the charges in the box $\Bb_1^{(1,j)}$. For the Multipole expansions lemma we have $c = \dfrac{3}{\sqrt{7}}$, then the potential $\phi^{(1)}$ at box $\Bb_2$ due to the charges at box $\Bb^{(1)}$ be given as $\phi^{(1)}$. Then from~\cref{tm:prank}, we have potential $\tilde{\phi}^{(1)}$ through a rank-$\tau$ approximation $A^{(1)}$ such that,
 \begin{displaymath}
     \abs{\phi_i^{(1)}-\tilde{\phi}_i^{(1)}}\leq \dfrac{2Q\sqrt{2}}{l\bkt{3-\sqrt{7}}}\bkt{\dfrac{\sqrt{7}}{3}}^{p+1}.
 \end{displaymath}
	Choosing $p+1 = \ceil{\dfrac{\log\bkt{\dfrac{2\sqrt{2}Q}{l\bkt{3-\sqrt{7}}\epsilon_1}}}{\log\bkt{3/\sqrt{7}}}}$, ensures 
	$\abs{\phi_i - \bkt{\tilde{A}^{(1)}q^{(1,j)}}_i} \leq \epsilon_1.$
 And, the rank of the matrix $\tilde{A}^{(1)}$ is bounded above by 
 \begin{displaymath}
\tau^{(1)}=\ceil{\dfrac{\log\bkt{\dfrac{2\sqrt{2}Q}{l\bkt{3-\sqrt{7}}\epsilon_1}}}{\log\bkt{3/\sqrt{7}}}}^2.
 \end{displaymath}	
	Again, each of the remaining boxes $\bar{\Bb}^{(1,1)}$, $\bar{\Bb}^{(1,2)}$ that share the edge with $\Bb_2$ are again subdivided into $8$ smaller boxes of side length $l/4$. We form the union of boxes that do not share a boundary with $\Bb_2$ as $\Bb^{(2,1)}$ and $\Bb^{(2,2)}$. The boxes $\bar{\Bb}^{(2,1)}$ and $\bar{\Bb}^{(2,2)}$ that share a boundary with $\Bb_2$. So, continuing this hierarchical subdivision, for a  level $k$, we will have boxes $\Bb^{(k,1)},\cdots,\Bb^{(k,2^{k-1})}$ that do not share a boundary and $\bar{\Bb}^{(k,1)},\cdots,\bar{\Bb}^{(k,2^{k-1})}$ that share a boundary. Hence for a box at level $k$ that does not share a boundary with $\Bb_2$, we can circumscribe a sphere such that $c = \dfrac{3}{\sqrt{7}}$. The potential $\phi^{(k,j)}$ at box $\Bb_2$ due to the charges at box $\Bb^{(k,j)}$ be given as $\phi^{(k,j)}$. Then from~\cref{tm:prank}, we have potential $\tilde{\phi}^{(k,j)}$ through a rank-$\tau$ approximation $\tilde{A}^{(k,j)}$ such that,
 \begin{displaymath}
     \abs{\phi_i^{(k,j)}-\tilde{\phi}_i^{(k,j)}}\leq \dfrac{2^jQ\sqrt{2}}{l\bkt{3-\sqrt{7}}}\bkt{\dfrac{\sqrt{7}}{3}}^{p+1}.
 \end{displaymath}
If we choose $p+1 = \ceil{\dfrac{\log\bkt{\dfrac{2^j\sqrt{2}Q}{l\bkt{3-\sqrt{7}}\epsilon_k}}}{\log\bkt{3/\sqrt{7}}}}$ ensures $\abs{\phi^{(k,j)}_i - \bkt{\tilde{A}^{(k,j)}q^{(k,j)}}_i} \leq \epsilon_k$.
 The rank of the matrix $\tilde{A}^{(k,j)}$ is bounded above by 
 %\begin{displaymath}
$\tau^{(k,j)}=\ceil{\dfrac{\log\bkt{\dfrac{2^j\sqrt{2}Q}{l\bkt{3-\sqrt{7}}\epsilon_k}}}{\log\bkt{3/\sqrt{7}}}}^2.$
 %\end{displaymath}
 Using the approximate matrices at level $k$, we can form a matrix $\tilde{A}^{(k)}$ through the matrices $\tilde{A}^{(k,1)},\cdots,\tilde{A}^{(k,2^{k-1})}$ whose rank is bounded above by
 \begin{displaymath}
    \tau^{(k)}=2^{k-1}\ceil{\dfrac{\log\bkt{\dfrac{2^j\sqrt{2}Q}{l\bkt{3-\sqrt{7}}\epsilon_k}}}{\log\bkt{3/\sqrt{7}}}}^2, 
 \end{displaymath}
 such that $\abs{\phi^{(k)}_i - \bkt{\tilde{A}^{(k)}q^{(k)}}_i} < 2^{k-1}\epsilon_k.$ The hierarchical subdivision of box $\Bb_1$ for $\kappa$ times results in series of approximants $\tilde{A}^{(1)},\cdots,\tilde{A}^{(\kappa)}$ with which we can construct $\tilde{A}$ such that 
\begin{equation} \label{eq:edge_ineq}
    \begin{split}
    \abs{\phi_i - \bkt{\tilde{A}q}_i} & < \dsum_{k=1}^{\kappa}\abs{\bkt{\bkt{A_jq}_i - \bkt{\tilde{A}_jq}_i}}\\
     & < \dsum_{k=1}^{\kappa}2^{k-1}\epsilon_k.
\end{split}
\end{equation}
As per our previous construct, $\kappa\approx\log_8\bkt{N}$, then using the relation between $\epsilon_j$ and $\epsilon_1$, the sum $\dsum_{k=1}^{\kappa}\epsilon_j$ becomes $\epsilon_1\dsum_{k=1}^{\kappa}2^{k-1}2^k = \dfrac{2}{3}(4^{\kappa}-1)\epsilon_1 \approx \sqrt[3]{N^2}\epsilon_1$ and to get error bound in $\epsilon$, we choose $\epsilon_1 = \dfrac{\epsilon}{\sqrt[3]{N^2}}$ then,
\begin{displaymath}
    \abs{\phi_i - \bkt{\tilde{A}q}_i} < \epsilon.
\end{displaymath}
 The rank of $\tilde{A}$ can be at most $\dsum_{j=1}^{\kappa}2^j\ceil{\log^2\bkt{\dfrac{2QN}{l\epsilon}}}$.
Hence with $\kappa\approx\log_8\bkt{N}$, the rank of $\tilde{A}$ is given by 
\begin{displaymath}
    \boxed{\tau \in \mathcal{O}\bkt{N^{1/3}\log^2\bkt{\dfrac{2Q\sqrt[3]{N^2}}{l\epsilon}}},}\quad  \text{with} \quad \abs{\phi_i - \bkt{\tilde{A}q}_i} < \epsilon.
\end{displaymath}

\textbf{Case (iv)}:(\underline{Boxes $\Bb_1$ and $\Bb_2$ are Face sharing neighbors}).
 Consider two boxes sharing a face i.e., $\Bb_1= [0,l]^3$ and $\Bb_2 = [l,2l]\times[0,l]^2$, as shown in~\cref{fig:3d_face}.
	% Figure environment removed
	The proof for this case starts by subdividing the box $\Bb_1$ into eight smaller boxes, each with side length $\frac{l}{2}$. The four boxes that do not share a boundary with the box $\Bb_2$ are named $\Bb_1^{(1,1)},\Bb_1^{(1,2)},\Bb_1^{(1,3)}$ and $\Bb_1^{(1,4)}$. The boxes that share a face with box $\Bb_2$ are given as  $\bar{\Bb}_1^{(1)}=\bar{\Bb}_1^{(1,1)}\cup\bar{\Bb}_1^{(1,2)}\cup\bar{\Bb}_1^{(1,3)}\cup\bar{\Bb}_1^{(1,4)}$ It can be shown as in previous cases that for each box $\Bb_1^{(1,1)},\Bb_1^{(1,2)},\Bb_1^{(1,3)}$ and $\Bb_1^{(1,4)}$ we can circumscribe a sphere of radius $\dfrac{l\sqrt{3}}{4}$. Now the boxes $B_{1}^{(1,i)}$ are separated from the source points by a distance of $\dfrac{3l}{4}$. In the multipole expansions lemma for the boxes $\Bb_1^{(1,j)}$ ($1\leq j \leq 4$),  we have $c = \sqrt{3}$. The potential due to charges inside $\Bb_1^{(1,j)}$ be $\phi^{(1,j)}$ and by using~\cref{tm:multipole,tm:prank}, we have potential $\tilde{\phi}^{(1,j)}$ through a rank-$\tau$ approximation , $\tilde{A}^{(1,j)}_1$, such that,
 \begin{displaymath}
     \abs{\phi^{(1,j)}_i-\tilde{\phi}^{(1,j)}_i}\leq \dfrac{4Q}{3l\bkt{\sqrt{3}-1}}\bkt{\sqrt{\dfrac{1}{3}}}^{p+1}.
 \end{displaymath}
	If we choose $p+1 \in \ceil{\dfrac{\log\bkt{\dfrac{4Q}{3l\epsilon_1\bkt{\sqrt{3}-1}}}}{\log{\bkt{\sqrt{3}}}}}$ ensures 
		$\abs{\phi^{(1,j)}_i-\bkt{\tilde{A}_1^{(1,j)}q}_i} < \epsilon_1.$
    The matrix $\tilde{A}^{(1,j)}_1$ has a rank bounded above by  $\ceil{\dfrac{\log\bkt{\dfrac{4Q}{3l\epsilon_1\bkt{\sqrt{3}-1}}}}{\log{\bkt{\sqrt{3}}}}}^2$. The potential at box $\Bb_2$ due to charges in $\Bb_1^{(1)}$ is $\phi^{(1)} = \dsum_{j=0}^{4}\phi^{(1,j)}$ which is approximated using rank-$\tau$ approximation through $\tilde{\phi}^{(1)}= \dsum_{j=0}^{4}\tilde{\phi}^{(1,j)}$ then, 
    $\abs{\phi^{(1)}_1 - \tilde{\phi}^{(1)}}\leq\dsum_{j=0}^{4}\abs{\phi^{(1,j)}_1 - \tilde{\phi}^{(1,j)}} < 4\epsilon_1.$ We construct the matrix $\tilde{A}^{(1)}$ using the rank-$\tau$ approximations $\tilde{A}^{(1,1)},\cdots,\tilde{A}^{(1,4)}$ corresponding to the boxes $\Bb_1^{(1,1)},\cdots,\Bb_1^{(1,4)}$ whose rank is bounded above by $\tau_1 = 4\ceil{\dfrac{\log\bkt{\dfrac{4Q}{3l\epsilon_1\bkt{\sqrt{3}-1}}}}{\log{\bkt{\sqrt{3}}}}}^2$ such that,
    $\abs{\phi^{(1)}_i-\bkt{\tilde{A}^{(1)}q}_i} < 4\epsilon_1.$
    The box $\bar{\Bb}^{(1)} = \Bb^{(2)}\cup\bar{\Bb}^{(2)}$, where $\Bb^{(2)}= \bigcup_{j=1}^{4^2}\Bb^{(2,j)}$ boxes do not share a boundary and $\bar{\Bb}^{(2)}= \bigcup_{j=1}^{4^2}\bar{\Bb}^{(2,j)}$ shares face with box $\Bb_2$. The boxes $\Bb^{(2,j)}$ and $\bar{\Bb}^{(2,j)}$ have side length $\dfrac{l}{4}$. Similarly, for a level $k$, we have  $\bar{\Bb}^{(k-1)} = \Bb^{(k)}\cup\bar{\Bb}^{(k)}$ and $\Bb^{(k)}= \bigcup_{j=1}^{4^k}\Bb^{(2,j)}$ do not share a boundary whereas $\bar{\Bb}^{(k)}= \bigcup_{j=1}^{4^k}\bar{\Bb}^{(2,j)}$ shares boundary with box $\Bb_2$. Applying~\cref{tm:multipole,tm:prank}, we have potential at $\Bb_2$ due to charges inside $\Bb^{(k)}$ as $\phi^{(k)} = \dsum_{j=0}^{4^k}\phi^{(k,j)}$. The potential $\phi^{(k)}$ is approximated using a rank-$\tau$ approximation (i.e., $\tilde{A}^{(k,j)}$) by $\tilde{\phi}^{(k)} = \dsum_{j=0}^{4^k}\tilde{\phi}^{(k,j)}$ such that,
    \begin{displaymath}
        \abs{\phi^{(k,j)} - \tilde{\phi}^{(k,j)}} \leq \dfrac{2.2^jQ}{3l\bkt{\sqrt{3}-1}}\bkt{\dfrac{1}{\sqrt{3}}}^{p+1}.
    \end{displaymath}
If we choose $p+1 = \ceil{\dfrac{\log\bkt{\dfrac{2.2^jQ}{3l\epsilon_j\bkt{\sqrt{3}-1}}}}{\log{\bkt{\sqrt{3}}}}}$ makes $\abs{\phi^{(k,j)}-\bkt{\tilde{A}^{(k,j)}q}_i} < \epsilon_j.$
    Hence at each level, we have, 
    \begin{equation}
        \begin{split}
		\abs{\phi_i^{(k)} - \tilde{\phi}_i^{(k)}} &\leq \dsum_{j=1}^{4^k}\abs{\phi^{(k,j)}_i - \tilde{\phi}^{(k,j)}_i}\\
		 & < 4^k\epsilon_k.
	\end{split}
    \end{equation}
    At level $k$, we can construct a matrix $\tilde{A}^{(k)}$ using the rank-$\tau$ approximations using $\tilde{A}^{(k,1)},\cdots,\tilde{A}^{(k,4^k)}$ whose rank is bounded above by 
    \begin{displaymath}
        \tau^{(k)} = 4^k\ceil{\dfrac{\log\bkt{\dfrac{2.2^kQ}{3l\epsilon_j\bkt{\sqrt{3}-1}}}}{\log{\bkt{\sqrt{3}}}}}^2,
    \end{displaymath}
such that $\abs{\phi^{(k)}-\bkt{\tilde{A}^{(k)}q^{(k)}}_i} < \epsilon_j.$ Hierarchically subdividing the box $\kappa=\log_8\bkt{N}$ times, results in $\bar{\Bb}^{(\kappa)}$ without any charges. The potential $\phi=\dsum_{k=1}^{\kappa}\phi^{(k)}$ is approximated through rank-$\tau$ approximations $\tilde{\phi}=\dsum_{k=1}^{\kappa}\tilde{\phi}^{(k)}$ with $\epsilon_1 = \epsilon_k/2^k$ then,
    \begin{equation}
        \begin{split}
		\abs{\phi_i - \tilde{\phi}_i} &\leq \dsum_{k=1}^{\kappa}\abs{\phi^{(k)}_i - \tilde{\phi}^{(k)}_i}\\
		 & < \dfrac{4}{7}(8^{\kappa}-1)\epsilon_1 < 8^{\kappa}\epsilon_1
	\end{split}
    \end{equation}
    Thus to get error bound in $\epsilon$, we choose $\epsilon_1 = \dfrac{\epsilon}{N}$, with which we can construct a matrix $\tilde{A}$ through the approximants $\tilde{A}^{(1)},\cdots,\tilde{A}^{(\kappa)}$ whose rank is bounded above by
    \begin{displaymath}
        \tau = N^{2/3}\ceil{\dfrac{\log\bkt{\dfrac{2QN}{3l\epsilon\bkt{\sqrt{3}-1}}}}{\log{\bkt{\sqrt{3}}}}}^2,
    \end{displaymath}
then, $\abs{\phi_i - \bkt{\tilde{A}q}_i} < \epsilon$. Hence with $\kappa=\log_8\bkt{N}$, the rank of $\tilde{A}$ is given by
\begin{displaymath}
    \boxed{\tau \in \mathcal{O}\bkt{N^{2/3}\log^2\bkt{\dfrac{2QN}{l\epsilon}}},}\quad\text{with,}\quad \abs{\phi_i - \bkt{\tilde{A}q}_i} < \epsilon.
\end{displaymath}
\end{proof}

We illustrate the decay of singular values of the interaction matrix for all the four cases of interactions (discussed in~\cref{tm:ranks3D}) for the Laplacian kernel in $3$D in Figure~\ref{singularValues}. It is to be observed that the decay of the singular values is fastest for the well-separated interaction $K(I_{X}, I_{W})$, slowest for the face-sharing interaction $K(I_{X}, I_{F})$ and that of the vertex sharing interaction $K(I_{X}, I_{V})$ is almost as fast as that of the well-separated interaction. Although we assumed a uniform distribution of particles in the domain,~\Cref{tm:ranks3D} also holds for a quasi-uniform distribution of particles.\footnote{Consider a hypercube $\Bb\subset \mathbb{R}^3$ contains $N$ particles. The particles inside $\Bb$ are said to be quasi-uniform distributed if exactly one particle is located inside each smallest hypercube resulting from the hierarchical subdivision of the hypercube $\Bb$ using an $\log_8\bkt{N}$ level octree.} 
% Figure environment removed
 
\section{HODLR3D}\label{sec:HODLR3D}
The objective of HODLR3D is to develop an almost linear complexity algorithm for Hierarchical matrices by leveraging the fact that the ranks of vertex-sharing interactions and well-separated interactions grow slowly with $N$. We have seen in the previous section  that the ranks of edge and face-sharing interactions grow as $\mathcal{O}(\sqrt[3]{N}\log^{2}(N))$ and $\mathcal{O}(\sqrt[3]{N^{2}}\log^{2}(N))$ respectively. And the rank of vertex sharing interaction grows as $\mathcal{O}(\log^{3}{N})$. Further, the rank of well-separated interaction does not grow with $N$. A similar rank growth for the interaction matrices arising out of a wide range of kernels in $3$D has been proved in~\cite{khan2022numerical}. Therefore, by choosing to compress only the well-separated and vertex-sharing interactions, we construct our HODLR3D, a Hierarchical matrix representation for kernel matrices from $3$D that yields almost linear complexity matrix algorithms. Before we describe the HODLR3D matrix-vector product algorithm, we define some notations in~\cref{table:Notations} that will be used in the rest of the section. 
\begin{definition}{Admissibility condition for HODLR3D:}\label{def:admh3d}
    Two clusters $\Cl$ and $\mathcal{C}^{(l)}_{j}$ where $i\neq j$ are admissible clusters, iff either they do not share a boundary, or they share a boundary which can be at the most a vertex.
    \end{definition}
In~\cref{fig:HODLR3D_matrix}, we illustrate the low-rank structure of the HODLR3D matrix at levels 1 and 2. 
% Figure environment removed

\begin{table}[!htbp]
    \centering
    \caption{List of notations followed in the rest of the section}
    \label{table:Notations}
    \begin{tabularx}{\textwidth}{|l|X|}
        \hline
        $\Cl$ & \textit{cluster} of particles lying in cube $i$ at level $l$ of the octree \\ \hline
        $\mathcal{V}_{i}^{\ell}$ & $\{\mathcal{C}_{j}^{(l)}: \text{cubes $i$ and $j$ share only a vertex}\}$ \\ \hline
        $\mathcal{E}_{i}^{\ell}$ & $\{\mathcal{C}_{j}^{(l)}: \text{cubes $i$ and $j$ share only an edge}\}$ \\ \hline
        $\mathcal{F}_{i}^{\ell}$ & $\{\mathcal{C}_{j}^{(l)}: \text{cubes $i$ and $j$ share a face}\}$ \\ \hline
        $child(\Cl)$ & $\{\mathcal{C}_{j}^{(l+1)}: \text{cube $j$ is a child of cube $i$}\}$ \\ \hline
        $parent(\Cl)$ & $\mathcal{C}_{j}^{(l-1)}$, where cube $j$ is the parent of cube $i$ \\ \hline
        $siblings(\Cl)$ & $child(parent(\Cl))$\textbackslash $\Cl$ \\ \hline
        % $X$ &  A cube that belongs to the octree\\ \hline
        % $\mathcal{C}^{l}_{i}$ & cluster pf particles lying in box  \\ \hline
        $\hat{C}_{\Cl}$ & Clan set of cluster $\Cl$ that is defined as\\
        &{\footnotesize $ \hat{C}_{\Cl} = \{siblings(\Cl)\bigcup \{ child(D):D\in\{\mathcal{E}_{parent(\Cl)}\bigcup \mathcal{F}_{parent(\Cl)}\}\}\}$}\\ \hline
        $\mathcal{I}_\Cl$ & Interaction list of cluster $\Cl$ that is defined as\\
        & {\footnotesize$\mathcal{I}_\Cl = \hat{C}_\Cl\cap \bkt{\mathcal{V}_\Cl  \cup \mathcal{W}_\Cl}$} \\ 
        \hline
    \end{tabularx}
 \end{table}
\Cref{tab:dense3d} provides us with the total number of dense matrix blocks for different hierarchical matrices based on an $L$-level octree. The total number of dense matrix blocks in $\mathcal{H}$-matrix with $\eta=\sqrt3$ is higher than HODLR3D. Similarly,~\Cref{tab:lrfar,tab:lrvert,tab:lr3def} shows that the total number of matrix blocks approximated as low-rank matrices is higher for $\mathcal{H}$-matrix with $\eta=\sqrt3$ compared to HODLR3D. This means that using HODLR3D, we can construct almost linear scaling algorithms by processing fewer individual matrix blocks, which is an appealing feature for extending the HODLR3D-based algorithms to parallel machines.

\begin{table*}[!htbp]
\caption{Total number of dense full-rank matrix blocks for different hierarchical matrices in $3$D based on an octree $\mathcal{T}^L$}
\label{tab:dense3d}
    \begin{tabular}{@{}cc@{}}
    \toprule
    Hierarchical matrix                       & Total number of dense blocks \\ \midrule
    HODLR (edge, vertex, face)                & $8^L$                         \\
    HODLR (edge, vertex)                      & $9 \cdot 8^{L} - 2\cdot 4^{L+1}$                             \\
    HOLDR3D                                   & $\frac{1}{3}\bkt{67 \cdot 8^{L} - 120 \cdot 4^{L} + 56 \cdot 2^{L}}$                           \\
    $\mathcal{H}$-matrix with $\eta=\sqrt{3}$ & $ \frac{1}{7}\bkt{223 \cdot 8^{L} - 126 \cdot 4^{L+1} + 49 \cdot 2^{L+3} - 104}$                            \\ \bottomrule
    \end{tabular}
\end{table*}


\begin{table*}[!htbp]
    \caption{Total number of low-rank matrix blocks due to well-separated interactions for different hierarchical matrices in $3$D based on an octree $\mathcal{T}^L$} 
    \label{tab:lrfar}
    \begin{tabular}{@{}cc@{}}
    \toprule
    Hierarchical matrix                       & Total number of low-rank blocks \\ \midrule
    HODLR (edge, vertex, face) &
    -- \\
    HODLR (edge, vertex) & $\frac{1}{21}\bkt{18\cdot 8^{L+2} - 42\cdot 4^{L+3} + 1536}$
    \\
    HOLDR3D & $\frac{1}{21}\bkt{444\cdot 8^{L+1} - 63\cdot 4^{L+4} + 735\cdot 2^{L+5} - 10944}$
    \\
    $\mathcal{H}$-matrix with $\eta=\sqrt{3}$ & $\frac{1}{7}\bkt{223\cdot 8^{L+1} - 630\cdot 4^{L+2}x + 1519\cdot 2^{L+4} - 6552L - 16008}$
    \\ \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[!htbp]
    \caption{Total number of low-rank matrix blocks due to vertex-sharing interactions for different hierarchical matrices in $3$D based on an octree $\mathcal{T}^L$} 
    \label{tab:lrvert}
    \begin{tabular}{@{}cc@{}}
    \toprule
    Hierarchical matrix                       & Total number of low-rank blocks \\ \midrule
    HODLR (edge, vertex, face)                & $\frac{8}{7}\bkt{8^{L}-1}$                         \\
    HODLR (edge, vertex)                      & $\frac{1}{21}\bkt{15\cdot 8^{L+1} - 14\cdot 4^{L+2} + 104}$           \\
    HOLDR3D   & $\frac{1}{21}\bkt{25\cdot 8^{L+1} - 42\cdot 4^{L+2} + 49\cdot 2^{L+4} - 312}$
    \\
    $\mathcal{H}$-matrix with $\eta=\sqrt{3}$ & --   \\ \bottomrule
    \end{tabular}
\end{table*}
             
\begin{table*}[!htbp]
    \caption{Total number of low-rank blocks for different hierarchical matrices in $3$D}
    \label{tab:lr3def}
    \begin{tabular}{@{}ccc@{}}
    \toprule
    \multirow{2}{*}{Hierarchical matrix} &
        \multicolumn{2}{c}{Total number of low-rank blocks due to} \\ \cmidrule(l){2-3} 
        &
        edge-sharing interactions &
        face-sharing interactions \\ \midrule
    HODLR (edge, vertex, face) 
        & $\frac{16}{7}\bkt{8^{L}-1}$
        & $\frac{32}{7}\bkt{8^{L}-1}$\\
    HODLR (edge, vertex)
    & $\frac{1}{21}\bkt{30\cdot 8^{L+1} - 7\cdot 4^{L+3} + 208}$
    & -- \\
    HOLDR3D 
    & -- & --\\
    $\mathcal{H}$-matrix with $\eta=\sqrt{3}$  & -- & -- \\ \bottomrule
    \end{tabular}
\end{table*}

\subsection{HODLR3D algorithm for Matrix-Vector product}\label{ssec:HODLR3DRepres}
The scale of problems in 3D is remarkably higher than that observed in 1D or 2D. For instance, if we consider $1000$ particles in each dimension, then the number of points in 3D with a tensor product grid is $10^{9}$.
Hence, storing the low-rank factors of all the low-rank sub-blocks of such large systems may not be feasible, as it will exhaust the RAM eventually for large system sizes. With limited RAM available, one must avoid explicitly storing the low-rank factors. Due to this, the low-rank compression technique that we describe in this article differs from that of the HODLR2D algorithm described in article~\cite{H2DKandappan}.

The first task is to perform the initialization, as described in Algorithm~\ref{alg:initialization}. 
We use ACA~\cite{bebendorf2003adaptive,zhao2005adaptive,tyrtyshnikov2000incomplete,bebendorf2000approximation} to compress the low-rank sub-blocks. 
For each pair of clusters $\Cl$ and $\mathcal{C}_{j}^{(l)}\in \mathcal{I}_\Cl$ at all levels of the $\mathcal{T}^{L}$ tree, with index sets $X$ and $Y$ respectively, the interaction matrix $K(X,Y)$ is compressed using ACA with tolerance $\epsilon$ as follows.
\begin{equation}\label{eq:prank}
    K(X,Y) \approx K(X,\tau_{XY}) K(\sigma_{XY}, \tau_{XY})^{-1} K(\sigma_{XY},Y)
\end{equation}
Here $\tau^{XY}$ and $\sigma^{XY}$ are the pivots of the approximation. 
\begin{equation}\label{eq:LR}
    K(\sigma_{XY}, \tau_{XY}) = L_{XY} R_{XY}
\end{equation}
We also compute $L_{XY}$ and $R_{XY}$, the LU factors of $K(\sigma_{XY}, \tau_{XY})$, as stated in Equation~\eqref{eq:LR}.
The LU factors of $K(\sigma_{XY}, \tau_{XY})$ can be obtained as a by-product of the ACA routine and need not be computed separately~\cite{bebendorf2009recompression}. It is to be noted that in the ACA routine, we only store the pivots $\tau^{XY}$, $\sigma^{XY}$ and the matrices $L_{XY}, R_{XY}$ instead of $K(X,\tau_{XY})$, $K(\sigma_{XY}, \tau_{XY})^{-1}$, and $K(\sigma_{XY},Y)$. 

The next task is to perform the matrix-vector product as described in Algorithm~\ref{alg:matvec}. Let the vector to be applied to the matrix be $\psi$. In the matrix-vector product routine, matrices $K(X,\tau_{XY})$ and $K(\sigma_{XY},Y)$ are assembled using the pivots stored in the initialization routine. And the low-rank sub-matrix-vector products are computed as $K(X,\tau_{XY})(R^{-1}_{XY}(L^{-1}_{XY}K(\sigma_{XY},Y)\psi(Y)))$.
\begin{remark}
Note that when we apply the inverse of upper/lower triangular matrices, we are performing backward/forward substitution. The computational complexity of the matrix-vector product using its hierarchical representation with $\log\bkt{N}$ levels computed using the low-rank compression technique of~\cite{H2DKandappan}, where the low-rank factors of the matrix are stored in the initialization phase, is $\mathcal{O}\bkt{pN\log\bkt{N}}$ in both memory and time. Whereas the computational complexity of the matrix-vector product stated in this article, where only the pivots and certain intermediate matrices are stored in the initialization phase, is $\mathcal{O}\bkt{p^2 N+pN\log\bkt{N}}$ in time and $\mathcal{O}\bkt{p^2 N}$ in memory.
\end{remark}

\begin{algorithm}[!htbp]
	\caption{HODLR3D Initialization}\label{alg:initialization}
	\begin{algorithmic}[1]
		\Procedure{InitializeHODLR3D}{$N_{\max}$,$\epsilon$}
		\State{} \Comment{$N_{\max}$ is the maximum number of particles at leaf level;}
		\State{} 
		\State {Form $\mathcal{T}^L$; where $L= \min \left\{l : \abs{\mathcal{C}^{(l)}_{i}} < N_{\max};\forall i \in \{0,1,2,\ldots,8^{l}-1\}\right\}$}
		\State {For each node in the octree identify the Interaction list $\mathcal{I}_\Cl$ and the sets $\mathcal{E}_\Cl$ and $\mathcal{F}_\Cl$.}
\For{\texttt{$l=1:L$}} 
				\For{\texttt{$i=0:8^l-1$}}
					\State $X\gets \text{Index set of } \mathcal{C}_{i}^{(l)}$
					\For{\texttt{j in $\mathcal{I}_{\mathcal{C}_{i}^{(l)}}$}}
						\State $Y\gets \text{Index set of } \mathcal{C}_{j}^{(l)}$
						\State Perform ACA with tolerance $\epsilon$ on the matrix $K(X,Y)$ to identify the pivots $\tau_{XY}$ and $\sigma_{XY}$ and find the LU factorization of the matrix $K(\tau_{XY},\sigma_{XY}) = L_{XY}R_{XY}$.
					\EndFor
				\EndFor
			\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htbp]
	\caption{HODLR3D matrix-vector Product $K\psi = b$}\label{alg:matvec}
	\begin{algorithmic}[1]
		\Procedure{MatVec}{$\psi$}
			\For{\texttt{i=0:$8^L-1$}} \Comment{Full-rank Mat-Vec product}
				\State $X\gets \text{Index set of } \mathcal{C}_{i}^{(L)}$
				\State Form the dense matrix $K(X,X)$
				\State $b(X) = b(X) + K(X,X) \times \psi(X)$
				\For{\texttt{j in $\mathcal{E}_{\mathcal{C}_{i}^{(L)}}$}}
					\State $Y\gets \text{Index set of } \mathcal{C}_{j}^{(L)}$
					\State Form the dense matrix $K(X,Y)$
				    \State $b(X) = b(X) + K(X,Y) \times \psi(Y)$
				\EndFor
				\For{\texttt{j in $\mathcal{F}_{\mathcal{C}_{i}^{(L)}}$}}
					\State $Y\gets \text{Index set of } \mathcal{C}_{j}^{(L)}$
					\State Form the dense matrix $K(X,Y)$
				    \State $b(X) = b(X) + K(X,Y) \times \psi(Y)$
				\EndFor
			\EndFor
			\For{\texttt{$l=1:L$}} \Comment{Low-rank Mat-Vec product}
				\For{\texttt{$i=0:8^l-1$}}
					\State $X\gets \text{Index set of } \mathcal{C}_{i}^{(l)}$
					\For{\texttt{j in $\mathcal{I}_{\mathcal{C}_{i}^{(l)}}$}}
						\State $Y\gets \text{Index set of } \mathcal{C}_{j}^{(l)}$
						\State $b(X) = b(X) + K(X,\tau_{XY}) R_{XY}^{-1}L_{XY}^{-1}  K(\sigma_{XY},Y)\psi(Y)$
					\EndFor
				\EndFor
			\EndFor
			\State \textbf{return} $b$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\section{Numerical Results} 
\label{sec:numericalResults}
We perform three experiments to demonstrate the performance of HODLR3D. 
In the first experiment, we demonstrate the HODLR3D matrix-vector product, where we consider three kernels i) 3D Laplacian ii) kernel $1/r^{4}$ iii) real part of the 3D Helmholtz kernel with the wave number set to $1$, $\frac{\cos(r)}{r}$. We compare the performance of HODLR3D with that of i) HODLR and ii) a $\mathcal{H}$ matrix that compresses only those interactions between cubes that are well-separated. In the second experiment, we demonstrate HODLR3D accelerated iterative solver for an integral equation. In the third experiment, we demonstrate the parallel scalability of HODLR3D. 

In~\cref{table:ResultsNotations}, we state some notations that we use in this section.
\begin{table}[!htbp]
\centering
\caption{List of notations followed in this section}
\label{table:ResultsNotations}
  \begin{tabularx}{\textwidth}{|p{30mm}|X|}
    \hline
    $N$ & System size that denotes the number of particles in the computational domain.\\ \hline
    $\epsilon$ & tolerance set for ACA routine \\ \hline
    Memory & The memory needed to store the matrix in HODLR3D representation.\\ \hline
    CR & Compression ratio that denotes the ratio of the number of floating point numbers that need to be stored for the HODLR3D representation to $N^2$ \\ \hline
    Initialization Time & It includes the time taken by the Initialize routine and the time taken to form the dense matrices corresponding to the edge-sharing, face-sharing, and self-interactions. \\ \hline
    Matrix-Vector product time & 
    It is the time taken by the Matrix-vector product routine minus the time taken to form the dense matrices corresponding to the edge-sharing, face-sharing, and self-interactions.\\ \hline
    Maximum rank & The maximum rank among all the interactions that were compressed while building the HODLR3D representation.\\ \hline
    Relative error & Relative error in the solution are measured using $\|.\|_{2}$. \\ \hline
  \end{tabularx}

 \end{table}
For all the experiments, we consider the following settings:
\begin{itemize}
    \item $\textib{B}$ is considered to be the cube $[-1,1]^{3}$.
    \item $N_{max}$ is chosen to be $216$.
\end{itemize}
All the experiments were run on an Intel Xeon Gold 2.5GHz processor with 8 OpenMP threads. All the results in this section are reproducible; we direct our readers to refer to \url{https://hodlr3d.readthedocs.io/en/latest/reproducibility.html} for further information.
\subsection{HODLR3D matrix-vector product}
\label{sec:sec5_1} The first numerical experiment demonstrates the scalability of the matrix-vector product through HODLR3D, HODLR and $\mathcal{H}$-matrix with strong admissibility condition. A uniform distribution of particles is considered in the domain $\Bb \subseteq [-1,1]^3$. The matrix entries $A_{ij}$ are generated using the Green's function of the Laplacian in $3$D, i.e., $A_{ij} = \dfrac{1}{\|\vec{r}_i - \vec{r}_j\|_{2}}$, where $\vec{r_{i}} \in \Bb$ $\forall i\in\{1,2,...,N\}$. We consider the vector $\Vec{x}$ to be applied to the matrix to be a random vector. 

In~\cref{fig:oneOverR_2}, we illustrate the scaling of maximum rank, CR, memory, initialization time, matrix-vector product time, and relative error with $N$ for the three kernels. We set $\epsilon$ to $10^{-7}$ for all three algorithms. We have repeated the above numerical experiment for two more kernels viz., $\frac{1}{r^{4}}$ and $\frac{\cos(r)}{r}$. In~\cref{fig:oneOverR_4,fig:cosR_2}, we illustrate the scaling of maximum rank, CR, memory, initialization time, matrix-vector product time, and relative error with $N$ for the kernels $\dfrac{1}{r^4}$ and $\dfrac{\cos\bkt{r}}{r}$.

%% Add tables
% Figure environment removed
% Figure environment removed
% Figure environment removed
\subsection{HODLR3D accelerated iterative solver for integral equations in 3D}
We solve the Fredholm integral equation of the second kind, equation~\eqref{eq:IE}, with $\bm{K}(x,y)=\frac{1}{\|x-y\|_{2}}$, where $x,y\in \mathbb{R}^3$.
\begin{equation}\label{eq:IE}
    \sigma(x)+\int_{\textib{B}} \bm{K}(x,y)\sigma(y)dy = f(x), \qquad \Bb\subset \mathbb{R}^3
\end{equation}
We use a piecewise constant collocation method with collocation points on a uniform tensor grid  on $3$D and an appropriate quadrature to integrate the singularity. The Fredholm integral equation is thus discretised to obtain a linear system of the form
\begin{equation}\label{eq:ILDiscrete}
    A\vec{\sigma}=\vec{f}
\end{equation}
We solve for $\vec{\sigma}$ using GMRES~\cite{barrett1994templates,saad1986gmres}, an iterative technique. Each iteration of GMRES involves computing a matrix-vector product. We employ HODLR3D to perform this computation to accelerate the solver.
In order to find the error in the solution, we consider a random vector $\vec{\sigma}$ and compute the right-hand side, $\vec{f}$, exactly up to roundoff. We then use $\vec{f}$ as the right-hand side of equation~\eqref{eq:ILDiscrete}, to solve the equation. Let the computed $\vec{\sigma}$ be $\vec{\sigma_{c}}$. We compute the relative forward error in 2-norm sense $\frac{\|\vec{\sigma_{c}}-\vec{\sigma}\|_{2}}{\|\vec{\sigma}\|_{2}}$. We terminate the GMRES routine when the relative residual, $\frac{\|A\vec{\sigma_{c}}-\vec{f}\|_{2}}{\|\vec{f}\|_{2}}$, is less than $10^{-10}$. In~\cref{fig:solvePlots2}, we illustrate the scaling of solve time and relative error with $N$, wherein $\epsilon$ for ACA of the three algorithms is set to $10^{-7}$.

% Figure environment removed

\subsection{Inferences}
It is to be observed from~\cref{fig:oneOverR_2,fig:oneOverR_4,fig:cosR_2} that the maximum rank for HODLR scales roughly with $\sqrt[3]{N^{2}}$. The initialization time scales roughly as $\mathcal{O}(N^{7/3}\log(N))$ and the memory, matrix-vector product time, and solve time scale roughly as $\mathcal{O}(N^{5/3}\log(N))$. In contrast, the maximum ranks for both HODLR3D and $\mathcal{H}$ matrices are almost constant and do not scale with any power of $N$. The complexities of memory, initialization time, the matrix-vector product time and the solve time for both HODLR3D and $\mathcal{H}$-matrix are almost linear. It is also to be observed that the memory and timing performance of HODLR3D is almost the same as that of $\mathcal{H}$-matrix. Based on the results shown in~\Cref{fig:oneOverR_2,fig:oneOverR_4,fig:cosR_2,fig:solvePlots2}, HODLR3D is an attractive alternative to $\mathcal{H}$-matrix with $\eta=\sqrt3$ for large $N$-body problems.

\section{Parallel HODLR3D}
\label{sec:parH3D}
   This section describes the parallel HODLR3D matrix-vector product algorithm (\cref{alg:matvec}) on a distributed memory system.
   We refer the readers~\cite {izadi2012hierarchical,li2020distributed}
   for the literature on the parallelization of the matrix-vector product of hierarchical matrices. In the parallel HODLR3D matrix-vector product, we try to maintain its parallel efficiency almost constant irrespective of the system size. We achieve this by considering each node in the hierarchical tree $\mathcal{T}^L$ as a data-independent computational unit and the processors perform an almost equal amount of computations at each level of the hierarchical tree. The computational unit that we refer to here is in context to the sub-matrix vector product concerned with a particular node in the hierarchical tree in~\cref{alg:matvec}. We also assume that the computations involved with respect to a node at a particular level in the hierarchical tree are the same with other nodes in that level.  
   
   We consider the number of parallel processors $n_p$ as $2^k,$ with $k>0$. For a level $l$ in $\mathcal{T}^L$, there are $8^l$ computational units. If $8^l\geq 2^k$, then each processor has $\ceil{\dfrac{8^l}{n_p}}$ computational units.
   If $8^l < 2^k$ then, each computational unit is shared by $\ceil{\dfrac{n_p}{8^l}}$ processors. 
   For the computational units at a level $l$, where $8^l < 2^k$, the computations are shared by a group of processors. Hence, we need an explicit work-sharing routine. This is performed as follows: For the dense matrix-vector product as in line numbers 5,9 and 14 of~\cref{alg:matvec}, we divide the columns of the matrix across the processors and perform the matrix-vector product. If $r$ is the rank of the low-rank representation in line $22$ of~\cref{alg:matvec}, then we perform the low-rank matrix-vector product through the following steps.
   \begin{itemize}
        \item Row-wise parallel matrix-vector product of $z_1=K(\sigma_{XY},Y)z$, i.e., the rows of the matrix $K(\sigma_{XY},Y)$ are equally shared among the processors. This involves a computational cost of $\mathcal{O}\bkt{\frac{rN}{n_p}}$.
        \item Communicate among other MPI processes and perform $z_2 = R^{-1}_{XY}\bkt{L^{-1}_{XY}z_1}$ which involves communication cost of $\mathcal{O}\bkt{n_pr}$, and a computational cost of $\mathcal{O}\bkt{r^2}$
        \item Perform  $K(X,\tau_{XY})z_2$, which involves a computational cost of $\mathcal{O}\bkt{\frac{rN}{n_p}}$
    \end{itemize}

    In summary, based on the level in the hierarchical tree, the computational unit is either processed by a single processor or a group of processors. On the whole, the HODLR3D-based matrix-vector product is equivalent to row-wise sharing of the matrix among the processors and the communication step just involves the MPI Gather operation among all the processors. 
    
    To illustrate the scalability of parallel HODLR3D, we look at the same numerical experiment in~\Cref{sec:sec5_1} (with Laplacian kernel i.e., $\frac{1}{\magn{x-y}_2}\quad x,y \in \mathbb{R}^3$ and $\epsilon=10^{-6}$). The parallel HODLR3D algorithm is developed in C++ with OpenMPI 4.14. Each MPI process runs on a single core of  Intel Xeon Gold 6248, a 2.5 GHz processor. We vary the system size $N$ and the number of MPI process $n_p$ and tabulate the maximum and average time taken to perform HODLR3D matrix-vector product in~\cref{tab:par_mat_vec}. From~\cref{tab:par_mat_vec}, we infer that for $n_p\leq 8$, we have good parallel efficiency as at each level the computational units are independent; Whereas for $n_p > 8$, the parallel efficiency drops as $n_p$ increases since we have more levels where the computational units are shared by a group of processors. For more details on the process of initialising HODLR3D structure in the distributed memory system refer to~\Cref {sec:par_init}. 

\begin{table}[!htbp]
    \centering
    \caption{Parallel HODLR3D Matrix-Vector product}
\label{tab:par_mat_vec}
    \begin{tabular}{@{}ccccccc@{}}
    \toprule
                                        &                                     & \multicolumn{5}{c}{\textbf{Time taken for}} \\ \cmidrule(l){3-7} 
                                 &                               & \multicolumn{3}{c}{\textbf{MatVec per process}} & \multicolumn{2}{c}{\textbf{MPI Collective Communication}} \\ \cmidrule(l){3-7} 
    \multirow{-3}{*}{\textbf{N}} & \multirow{-3}{*}{\textbf{np}} & \textbf{Avg}  & \textbf{Max} & \textbf{Speedup} & \textbf{Avg}                & \textbf{Max}                \\ \midrule
                                        & \textbf{2}  & 0.290    & 0.290    & 1  & 0.004  & 0.008   \\
                                        & \textbf{4}                          & 0.131    & 0.140    & 2  & 0.352  & 0.717   \\
                                        & \textbf{8}                          & 0.072    & 0.078    & 4  & 0.798  & 0.964   \\
                                        & \textbf{16}                         & 0.037    & 0.046    & 6  & 0.021  & 0.030   \\
                                        & \textbf{32} & 0.022    & 0.034    & 8  & 0.085  & 0.144   \\
    \multirow{-6}{*}{\textbf{125000}}   & \textbf{64} & 0.009    & 0.013    & 23 & 0.008  & 0.009   \\ \midrule
                                        & \textbf{2}  & 4.418    & 4.434    & 1  & 0.035  & 0.065   \\
                                        & \textbf{4}                          & 2.092    & 2.192    & 2  & 3.824  & 8.041   \\
                                        & \textbf{8}                          & 1.083    & 1.163    & 4  & 1.832  & 3.160   \\
                                        & \textbf{16}                         & 0.532    & 0.608    & 7  & 0.459  & 0.482   \\
                                        & \textbf{32} & 0.302    & 0.409    & 11 & 0.097  & 0.154   \\
    \multirow{-6}{*}{\textbf{1000000}}  & \textbf{64} & 0.142    & 0.201    & 22 & 0.032  & 0.045   \\ \midrule
                                        & \textbf{2}  & 28.560   & 28.879   & 1  & 0.758  & 1.499   \\
                                        & \textbf{4}                          & 13.993   & 14.903   & 2  & 13.419 & 28.002  \\
                                        & \textbf{8}                          & 6.940    & 7.847    & 4  & 4.231  & 13.093  \\
                                        & \textbf{16}                         & 3.138    & 3.725    & 8  & 0.287  & 0.311   \\
                                        & \textbf{32} & 1.848    & 2.405    & 12 & 0.106  & 0.265   \\
    \multirow{-6}{*}{\textbf{3375000}}  & \textbf{64} & 0.921    & 1.361    & 21 & 0.094  & 0.131   \\ \midrule
                                        & \textbf{2}  & 62.914   & 63.257   & 1  & 1.113  & 2.184   \\
                                        & \textbf{4}                          & 30.203   & 31.106   & 2  & 37.588 & 74.930  \\
                                        & \textbf{8}                          & 15.851   & 18.092   & 3  & 16.925 & 43.202  \\
                                        & \textbf{16}                         & 7.150    & 8.136    & 8  & 3.897  & 3.975   \\
                                        & \textbf{32} & 4.001    & 4.993    & 13 & 0.214  & 0.345   \\
    \multirow{-6}{*}{\textbf{8000000}}  & \textbf{64} & 2.482    & 4.349    & 15 & 0.401  & 0.495   \\ \midrule
                                        & \textbf{2}  & 151.724  & 152.148  & 1  & 1.249  & 2.414   \\
                                        & \textbf{4}                          & 72.447   & 73.242   & 2  & 62.934 & 128.980 \\
                                        & \textbf{8}                          & 36.441   & 40.670   & 4  & 29.887 & 72.478  \\
                                        & \textbf{16}                         & 17.627   & 19.924   & 8  & 7.720  & 8.017   \\
                                        & \textbf{32} & 9.715    & 11.630   & 13 & 0.269  & 0.378   \\
    \multirow{-6}{*}{\textbf{15625000}} & \textbf{64} & 5.193    & 7.145    & 21 & 0.517  & 0.735   \\ \bottomrule
    \end{tabular}
    \end{table}
\section{Conclusions}
In this article, we introduced HODLR3D, a new class of hierarchical matrices for problems arising in three dimensions. We proved upper bounds for the ranks of different off-diagonal blocks of the matrix when the underlying kernel function is the 3D Laplacian kernel. The main highlight of the theorem is that the ranks of the vertex-sharing interactions do not scale with any power of $N$. Based on this observation, the HODLR3D matrix is introduced, whose construction and matrix-vector product algorithm scale almost linearly.

We further compared the performances of HODLR, HODLR3D and a $\mathcal{H}$ matrix in computing matrix-vector product and solving an integral equation. It can be observed from the numerical results that HODLR3D performs better than HODLR, and HODLR3D can be considered competitive to the $\mathcal{H}$ matrices with strong admissibility condition.

Like HODLR2D~\cite{H2DKandappan} and HODLR3D, it is possible to extend the idea of compressing vertex-sharing blocks to higher dimensions. For example, consider $n$-dimensional hypercube which can be partitioned using $2^{n}$-tree. The neighbors for a hypercube are those that share a $1$-cube (vertex), $2$-cube (edge), $3$-cube (face), ..., $(n-2)$-cube, $(n-1)$-cube with it. A similar observation, as in Section~\ref{sec:rankGrowth}, holds true in $n$-dimensions too, i.e., the rank of vertex sharing interactions scale as $\mathcal{O}(\log(N)\log^{n}(\log(N)))$ and that of the well-separated interactions do not scale with $N$. And the ranks of higher order-cube sharing interactions, say $n'$-cube sharing interactions ($1\leq n'\leq n-1$), scale with $\mathcal{O}(N^{n'/n}\log^{n}(N))$. For the proof, we refer the readers to~\cite{khan2022numerical}. An almost linear complexity HODLR$n$D algorithm can thus be constructed by compressing only the well-separated and vertex-sharing interactions. The algorithm for computing matrix-vector product is similar to the one described in Section~\ref{sec:HODLR3D}.

In addition to the fast matrix-vector products, one can construct a fast direct solver similar to the Inverse Fast Multipole Method (IFMM)~\cite{ambikasaran2014inverse} by leveraging the off-diagonal low-rank structure of HODLR3D. Since the rank of off-diagonal blocks does not scale with the system size,  the direct solver based on HODLR3D will be a promising alternative to other direct solvers available~\cite{SA_FDS_2013,gujjula2023algebraic,ambikasaran2014inverse}.
\section{Declarations}
\subsection*{Availability of data and materials}
The code used to generate the results are available at the following Repository.
\begin{itemize}
    \item Code Repository: \url{https://github.com/SAFRAN-LAB/HODLR3D}
    \item Documentation on Reproducibility of results:\url{https://hodlr3d.readthedocs.io/en/latest/reproducibility.html} 
\end{itemize}
\subsection*{Funding}
Vaishnavi Gujjula acknowledges the support of Women Leading IITM (India) 2022 in Mathematics (SB22230053MAIITM008880). Sivaram Ambikasaran acknowledges the support of Young Scientist Research Award from Board of Research in Nuclear Sciences, Department of Atomic Energy, India (No.34/20/03/2017-BRNS/34278) and MATRICS grant from the Science and Engineering Research Board, India (Sanction number: MTR/2019/001241).
\subsection*{Acknowledgments}
The authors acknowledge HPCE, IIT Madras for providing access to the AQUA cluster. The authors would like to thank Ritesh Khan for his valuable comments on the draft of this article.

\begin{appendices}
\section{Numerical Experiment on HODLR3D matrix-vector product}
In this section, we repeat the numerical experiment in~\Cref{ssec:HODLR3DRepres} for the different hierarchical structures considered viz., HODLR, HODLR3D and $\mathcal{H}$ matrix such that in matrix-vector product the forward relative error is of the same order. The intention of this numerical experiment is to understand the performance and scalability of different hierarchical structures for the same matrix-vector product forward relative error. We make sure that the relative forward error of the three algorithms that we compare, HODLR3D, HODLR, and $\mathcal{H}$ matrix, are nearly equal so that the rest of the benchmarks can be compared and an inference can be made. To achieve this, we use different values of $\epsilon$ in the ACA routine of the three hierarchical structures, in the range of $10^{-6}-10^{-10}$. We perform this incrementally and record various benchmarks of the hierarchical structures, such that they have a forward relative error of the same order. The kernels that we use to perform the numerical experiment are 
\begin{itemize}
    \item Green's function for Laplace equation in 3D  which is $\dfrac1{r}$
    \item $\dfrac1{r^4}$
    \item Real part of the Green's function for Helmholtz equation in 3D, which is $\dfrac{\cos\bkt{r}}{r}$
\end{itemize}
% Figure environment removed
% Figure environment removed
% Figure environment removed
From~\cref{fig:oneOverR,fig:oneOverR4,fig:cosR}, we observe that by maintaining the relative forward error to be nearly equal, the computational complexity for the matrix-vector product using HODLR3D and $\mathcal{H}$-matrix representation still roughly scales $\mathcal{O}\bkt{N\log\bkt{N}}$, which is not the case with HODLR in 3D.
\section{HODLR3D Initialization in Distributed memory systems}
\label{sec:par_init}
As discussed in~\Cref{sec:parH3D}, we consider the nodes in a particular level of the hierarchical tree as data-independent computational units. For level $l$, where the number of nodes in a level $\bkt{8^l}$ is greater than $n_p$ MPI processes, each MPI process has $\ceil{\dfrac{8^l}{n_p}}$ computational units. For level $l$, where the number of nodes in that level is lesser than $n_p$ MPI processes, each node in level $l$ is shared by $\ceil{\dfrac{n_p}{8^l}}$ MPI processes. The low-rank compression involved with the shared node is performed separately by each MPI process that shares  that node. This is performed to eliminate the communication involved and reduce idle time.~\cref{tab:par_h3d_init} shows the time taken by parallel HODLR3D to initialize the data structure.

\begin{table}[!htbp]
\centering
\caption{Parallel HODLR3D Initialization}
\label{tab:par_h3d_init}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{3}{*}{\textbf{N}}        & \multirow{3}{*}{$n_p$} & \multicolumn{5}{c}{Time taken for}                                                  \\ \cmidrule(l){3-7} 
                                   &                        & \multicolumn{3}{c}{Form Low Rank Basis} & \multicolumn{2}{c}{Generate Matrix Entry} \\ \cmidrule(l){3-7} 
                                   &                        & Avg          & Max         & Speedup    & Avg                 & Max                 \\ \midrule
\multirow{6}{*}{\textbf{125000}}   & 2                      & 31.04        & 31.07       & 1          & 4.687               & 4.690               \\
                                   & 4                      & 13.62        & 15.09       & 2          & 1.980               & 2.323               \\
                                   & 8                      & 7.46         & 7.72        & 4          & 1.212               & 1.980               \\
                                   & 16                     & 3.80         & 4.43        & 7          & 0.575               & 0.762               \\
                                   & 32                     & 2.10         & 2.86        & 11         & 0.281               & 0.375               \\
                                   & 64                     & 1.18         & 1.70        & 18         & 0.134               & 0.197               \\ \midrule
\multirow{6}{*}{\textbf{1000000}}  & 2                      & 589.65       & 589.94      & 1          & 59.857              & 59.870              \\
                                   & 4                      & 250.02       & 272.61      & 2          & 26.148              & 29.859              \\
                                   & 8                      & 128.09       & 132.75      & 4          & 14.896              & 16.560              \\
                                   & 16                     & 61.22        & 70.36       & 8          & 7.241               & 8.813               \\
                                   & 32                     & 34.31        & 42.90       & 14         & 3.659               & 4.584               \\
                                   & 64                     & 18.79        & 24.92       & 24         & 1.776               & 2.306               \\ \midrule
\multirow{6}{*}{\textbf{3375000}}  & 2                      & 2456.92      & 2482.36     & 1          & 321.421             & 321.837             \\
                                   & 4                      & 989.55       & 1040.38     & 2          & 143.905             & 157.784             \\
                                   & 8                      & 492.24       & 522.79      & 5          & 80.070              & 82.361              \\
                                   & 16                     & 236.63       & 268.47      & 9          & 37.660              & 44.408              \\
                                   & 32                     & 144.93       & 191.63      & 13         & 21.309              & 26.590              \\
                                   & 64                     & 78.82        & 117.63      & 21         & 10.554              & 14.459              \\ \midrule
\multirow{6}{*}{\textbf{8000000}}  & 2                      & 8600.18      & 8620.25     & 1          & 772.062             & 772.723             \\
                                   & 4                      & 3880.94      & 4017.35     & 2          & 346.048             & 383.079             \\
                                   & 8                      & 1948.46      & 2085.96     & 4          & 193.935             & 208.576             \\
                                   & 16                     & 902.95       & 1038.41     & 8          & 87.325              & 100.494             \\
                                   & 32                     & 501.63       & 611.37      & 14         & 49.599              & 58.468              \\
                                   & 64                     & 257.57       & 366.81      & 24         & 26.609              & 39.912              \\ \midrule
\multirow{6}{*}{\textbf{15625000}} & 2                      & 17794.70     & 17821.10    & 1          & 1815.010            & 1815.670            \\
                                   & 4                      & 8058.94      & 8467.18     & 2          & 826.699             & 891.079             \\
                                   & 8                      & 4102.86      & 4348.15     & 4          & 452.395             & 475.912             \\
                                   & 16                     & 1870.53      & 2137.44     & 8          & 212.278             & 242.730             \\
                                   & 32                     & 1027.04      & 1284.76     & 14         & 120.396             & 143.222             \\
                                   & 64                     & 555.55       & 801.46      & 22         & 60.863              & 84.852              \\ \bottomrule
\end{tabular}
\end{table}

\end{appendices}

\bibliography{references}

\end{document}
