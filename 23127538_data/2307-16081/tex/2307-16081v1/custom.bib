@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{gottardi2022alexa,
  title={Alexa, let's work together: Introducing the first alexa prize taskbot challenge on conversational task assistance},
  author={Gottardi, Anna and Ipek, Osman and Castellucci, Giuseppe and Hu, Shui and Vaz, Lavina and Lu, Yao and Khatri, Anju and Chadha, Anjali and Zhang, Desheng and Sahai, Sattvik and others},
  journal={arXiv preprint arXiv:2209.06321},
  year={2022}
}

@inproceedings{mehri2019structured,
  title={Structured Fusion Networks for Dialog},
  author={Mehri, Shikib and Srinivasan, Tejas and Eskenazi, Maxine},
  booktitle={20th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  pages={165},
  year={2019}
}

@inproceedings{zhang2020probabilistic,
  title={A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning},
  author={Zhang, Yichi and Ou, Zhijian and Hu, Min and Feng, Junlan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9207--9219},
  year={2020}
}

@article{peng2021soloist,
  title={SOLOIST: Building Task Bots at Scale with Transfer Learning and Machine Teaching},
  author={Peng, Baolin and Li, Chunyuan and Li, Jinchao and Shayandeh, Shahin and Liden, Lars and Gao, Jianfeng},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={907--824},
  year={2021}
}

@inproceedings{su2022multi,
  title={Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System},
  author={Su, Yixuan and Shu, Lei and Mansimov, Elman and Gupta, Arshit and Cai, Deng and Lai, Yi-An and Zhang, Yi},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={4661--4676},
  year={2022}
}

@inproceedings{emnlp/KhashabiMKSTCH20,
  author    = {Daniel Khashabi and
               Sewon Min and
               Tushar Khot and
               Ashish Sabharwal and
               Oyvind Tafjord and
               Peter Clark and
               Hannaneh Hajishirzi},
  title     = {UnifiedQA: Crossing Format Boundaries With a Single {QA} System},
  booktitle = {Findings of {EMNLP}},
  volume    = {{EMNLP} 2020},
  pages     = {1896--1907},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.findings-emnlp.171},
}

@inproceedings{emnlp/RajpurkarZLL16,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  booktitle = {{EMNLP}},
  pages     = {2383--2392},
  year      = {2016},
  url       = {https://doi.org/10.18653/v1/d16-1264},
}

@inproceedings{acl/RajpurkarJL18,
  author    = {Pranav Rajpurkar and
               Robin Jia and
               Percy Liang},
  title     = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  booktitle = {{ACL}},
  pages     = {784--789},
  year      = {2018},
  url       = {https://aclanthology.org/P18-2124/},
}

@article{corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
}

@inproceedings{rg19,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = nov,
  pages = {3982--3992},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1410},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textbackslash textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  file = {C\:\\Users\\Chris\\Dropbox\\ObsidianVaults\\My-Vault\\02-Literature\\zotero-attachments\\Sentence-BERT_rg19.pdf}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}



@inproceedings{cao2007learning,
  title={Learning to rank: from pairwise approach to listwise approach},
  author={Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={129--136},
  year={2007}
}

@article{dstc8,
  title={Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset},
  author={Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
  journal={CoRR},
  volume    = {abs/1909.05855},
  url       = {http://arxiv.org/abs/1909.05855},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{roller2021recipes,
  title={Recipes for Building an Open-Domain Chatbot},
  author={Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Smith, Eric Michael and Boureau, Y-Lan and others},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={300--325},
  year={2021}
}

@article{chen2022bootstrapping,
  title={Bootstrapping a User-Centered Task-Oriented Dialogue System},
  author={Chen, Shijie and Chen, Ziru and Deng, Xiang and Lewis, Ashley and Mo, Lingbo and Stevens, Samuel and Wang, Zhen and Yue, Xiang and Zhang, Tianshu and Su, Yu and others},
  journal={arXiv preprint arXiv:2207.05223},
  year={2022}
}


@article{chi2022neural,
  title={Neural Generation Meets Real People: Building a Social, Informative Open-Domain Dialogue Agent},
  author={Chi, Ethan A and Paranjape, Ashwin and See, Abigail and Chiam, Caleb and Kenealy, Kathleen and Lim, Swee Kiat and Hardy, Amelia and Rastogi, Chetanya and Li, Haojun and Iyabor, Alexander and others},
  journal={arXiv preprint arXiv:2207.12021},
  year={2022}
}

@article{roller2020recipes,
  title={Recipes for building an open-domain chatbot},
  author={Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Shuster, Kurt and Smith, Eric M and others},
  journal={arXiv preprint arXiv:2004.13637},
  year={2020}
}

@article{marin2021recipe1m+,
  title={Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images},
  author={Mar{\i}n, Javier and Biswas, Aritro and Ofli, Ferda and Hynes, Nicholas and Salvador, Amaia and Aytar, Yusuf and Weber, Ingmar and Torralba, Antonio},
  year = {2019}
}

@article{marin2019learning,
  title = {Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images},
  author = {Marin, Javier and Biswas, Aritro and Ofli, Ferda and Hynes, Nicholas and 
  Salvador, Amaia and Aytar, Yusuf and Weber, Ingmar and Torralba, Antonio},
  journal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  year = {2019}
}

@article{SMDataflow2020,
  author = {{Semantic Machines} and Andreas, Jacob and Bufe, John and Burkett, David and Chen, Charles and Clausman, Josh and Crawford, Jean and Crim, Kate and DeLoach, Jordan and Dorner, Leah and Eisner, Jason and Fang, Hao and Guo, Alan and Hall, David and Hayes, Kristin and Hill, Kellie and Ho, Diana and Iwaszuk, Wendy and Jha, Smriti and Klein, Dan and Krishnamurthy, Jayant and Lanman, Theo and Liang, Percy and Lin, Christopher H. and Lintsbakh, Ilya and McGovern, Andy and Nisnevich, Aleksandr and Pauls, Adam and Petters, Dmitrij and Read, Brent and Roth, Dan and Roy, Subhro and Rusak, Jesse and Short, Beth and Slomin, Div and Snyder, Ben and Striplin, Stephon and Su, Yu and Tellman, Zachary and Thomson, Sam and Vorobev, Andrei and Witoszko, Izabela and Wolfe, Jason and Wray, Abby and Zhang, Yuchen and Zotov, Alexander},
  title = {Task-Oriented Dialogue as Dataflow Synthesis},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {556--571},
  year = {2020},
  month = sep,
  url = {https://doi.org/10.1162/tacl_a_00333},
  abstract = {We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset, code for replicating experiments, and a public leaderboard are available at \url{https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines}.},
}

@inproceedings{mo2022towards,
  title={Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction},
  author={Mo, Lingbo and Lewis, Ashley and Sun, Huan and White, Michael},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={322--342},
  year={2022}
}