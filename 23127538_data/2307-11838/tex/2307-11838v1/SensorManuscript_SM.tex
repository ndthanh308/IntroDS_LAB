%% ****** Start of file template.aps ****** %
%%
%%
%%   This file is part of the APS files in the REVTeX 4 distribution.
%%   Version 4.0 of REVTeX, August 2001
%%
%%
%%   Copyright (c) 2001 The American Physical Society.
%%
%%   See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.0
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showpacs' option to make PACS codes appear
%\documentclass[aps,prl,twocolumn,showpacs,superscriptaddress,groupedaddress]{revtex4-2}
\documentclass[aps,pre,twocolumn,showpacs,superscriptaddress]{revtex4-2}
% 
%for review and submission
%\documentclass[aps,preprint,showpacs,superscriptaddress,groupedaddress]{revtex4}
%  % for 
%double-spaced preprint
\usepackage{graphicx}  % needed for figures
\usepackage{dcolumn}   % needed for some tables
\usepackage{bm}        % for math
\usepackage{dsfont}    % for math (double-stroked characters via \mathds{text})
\usepackage{amssymb}   % for math
\usepackage{amsmath}
%\usepackage{journals}
\usepackage{color}
\usepackage{siunitx}
\usepackage{physics}
\usepackage{hyperref}

% avoids incorrect hyphenation, added Nov/08 by SSR
\hyphenation{ALPGEN}
\hyphenation{EVTGEN}
\hyphenation{PYTHIA}
\usepackage[utf8]{inputenc}

\newcommand{\aak}[1]{\textcolor[rgb]{1,0,0}{#1}}

\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

\begin{document}
	\title{Data-Induced Interactions of Sparse Sensors:\\Supplementary Materials}
	\author{Andrei A.~Klishin}
        \email{aklishin@uw.edu}
	\author{J.~Nathan Kutz}
	\author{Krithika Manohar}
	
	
	\date{\today}
	
	\maketitle
	
\section{Data preprocessing}
\subsection{Sea Surface Temperature}
The Sea Surface Temperature (SST) dataset is curated by the National Oceanic and Atmospheric Administration (NOAA) and employs the Optimal Interpolation (OI) method to collect observations from different platforms and places them on a single regular grid for long periods of observations \cite{huang2021improvements}. In this work we use OISST 2.0 dataset \footnote{At the time of writing the dataset was available via the Physical Sciences Laboratory at \url{https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.html}.} that has the spatial resolution of 1 degree in both latitude and longitude, and is temporally averaged for each week between Dec 31, 1989 and Jan 1, 2023 for a total of $N=1727$ snapshots. Each temperature snapshot is given in equirectangular projection of size $360\times 180$ pixels. All of land surface of Earth is excluded from observations via a time-independent binary mask, leaving $n=44219$ pixels that vary between the states. We center the dataset by subtracting the temporal mean temperature profile from each snapshot. The POD is truncated to rank $r=100$,


\subsection{Olivetti faces}
The Olivetti faces dataset consists of photos of individuals taken between April 1992 and April 1994 at AT\&T Laboratories Cambridge \cite{samaria1994parameterisation}. Each of 40 individuals has 10 images, for a total of $N=400$ presented in random order. Each photo has the size $64\times 64$ grayscale pixels, resulting in $n=4096$. We center each image both locally and globally by subtracting the mean brightness value of each image and the mean value across the dataset. The POD is truncated to rank $r=100$.

\subsection{Cylinder flow}
The cylinder flow dataset consists of scalar vorticity fields in 2D flow of a liquid around a cylinder, numerically simulated with immersed boundary method in Refs.~\cite{taira2007immersed, colonius2008fast}. In the simulation regime at Reynolds number $Re=100$, the flow consists of periodic shedding of vortices from the two sides of the cylinder in alternating order. The data consists of $N=151$ snapshots of resolution $449\times 199$ pixels, resulting in data dimension $n=89351$. We center the data by subtracting the temporal average vorticity profile from each snapshot. The POD is truncated to rank $r=40$.


\subsection{Gaussian Free Field}
The Gaussian Free Field (GFF) dataset is generated synthetically with the FyeldGenerator package \cite{Cadiou2022Fyeld}. The field is generated by drawing pseudorandom values of Fourier modes according to a user-specified power spectrum. In order to ensure that the field has coarse features on small spatial scale, we chose power spectrum $P(k)\propto k^{-10}$ as opposed to the usual $P(k)\propto k^{-2}$ for standard GFF. We generated $N=50$ pseudorandom snapshots of a field in the spatial domain $5\times 5$ pixels, resulting in $n=25$. The small size of the synthetic dataset was required to enable the exhaustive enumeration of sensors, see Sec.~\ref{sec:exhaustive}. We center each image both locally and globally by subtracting the mean brightness value of each image and the mean value across the dataset. The POD is truncated to rank $r=23$, equal to the number of singular values above machine precision $\sigma>10^{-15}$.

\subsection{Random State System}
The Random State System (RSS) dataset is generated synthetically with the Python Control Package \cite{fuller2021python} that replicates the functionality of Matlab Control Toolbox. RSS is a linear dynamical system with equation $\dot{\vec{x}}=M\vec{x}$ for a random square matrix $M$ of specified dimension, which we choose to be $n=25$. The generating package does not request any other free parameters such as the distribution of the random matrix, other than ensuring the dynamical system stability, i.e. $\forall\lambda: Re(\lambda)\leq 0$. We obtain the trajectory data by initializing random iid initial condition $x_i(t=0)\sim \mathcal{N}(0,1)$ and integrating the dynamical equation numerically in the range $t\in[0,5]$, sampling $N=101$ snapshots. The small size of the synthetic dataset was required to enable the exhaustive enumeration of sensors, see Sec.~\ref{sec:exhaustive}. We center each state both locally and globally by subtracting the mean brightness value of each image and the mean value across the dataset. The POD is truncated to rank $r=19$, equal to the number of singular values above machine precision $\sigma>10^{-15}$.

\subsection{Dataset singular values}
% Figure environment removed
Fig.~\ref{fig:singular} shows the spectra of singular values for each of the five datasets, along with the truncation threshold. As expected, singular values span many orders of magnitude for both empirical and synthetic datasets.

\section{Prior and state reconstruction}
\subsection{Bayesian inference}
Here we situate state reconstruction as a problem of Bayesian inference from noisy sensor data. We place $p\ll n$ sensors each measuring the scalar field in one location. We denote the set of sensors as $\gamma$, the set of sensor location indices. Given a full $n$-dimensional state vector $\vec{x}$, the sensor output is a much shorter state vector, sometimes called gapped \cite{everson1995karhunen}:
\begin{align}
	\vec{y}=C\vec{x}=C\Psi_r \vec{a}=\Theta \vec{a},
\end{align}
where $C$ is a $p\times n$ sensor selector matrix with the entry 1 for the location that each sensor measures, and 0 otherwise. The matrix $\Theta: p\times r$ combines the sensor selection with the low-rank representation of the scalar field.

We further assume that each sensor measures the scalar field with a Gaussian noise of magnitude $\eta$. Thus, given a true state $\vec{a}$, the probability distribution of sensor readings is given by:
\begin{align}
	p(\vec{y}|\vec{a})\propto \exp(-\frac{(\vec{y}-\Theta \vec{a})^2}{2\eta^2}),
\end{align}
where we omit the distribution normalization.

For Bayesian inference, we invert the distribution using the Bayes rule:
\begin{align}
	p(\vec{a}|\vec{y})=\frac{p(\vec{y}|\vec{a})p(\vec{a})}{p(\vec{y})},
\end{align}
where $p(\vec{a})$ is a \emph{prior} distribution and $p(\vec{y})$ is a normalization. The procedure of state estimation consists of computing the Maximum A Posteriori (MAP) estimate, which is typically done on log-likelihood:
\begin{align}
	\hat{\vec{a}}=\arg\max\limits_{\vec{a}}\left( \ln p(\vec{y}|\vec{a}) + \ln p(\vec{a}) \right),
\end{align}
where the normalization $p(\vec{y})$ was omitted as it doesn't depend on the inferred state $\vec{a}$. The solution of this argmax problem requires knowing the functional form of the prior that we discuss below.

\subsection{Constructing the prior}
In order to exploit the prior information of the data, we need to assume a functional form of the prior distribution over the coefficients $\vec{a}$. A simple form of this assumption is to select a Gaussian prior of form:
\begin{align}
	p_\textsf{Gauss}(\vec{a})\propto \exp(-\frac{\vec{a}^T S^{-2} \vec{a}}{2}),
	\label{eqn:priorgauss}
\end{align}
which poses that the system states are drawn from an anisotropic Gaussian cloud where the variances along each orthogonal direction are given by the elements of a diagonal matrix $S$. We consider two choices for the prior: a scaled identity matrix $S=\sigma I_r$ for uniform variance along all dimensions, and the matrix of singular values of the training data $S=\Sigma_r$ for hierarchically decreasing variance of higher modes. Since in both cases all $r$ elements are positive, the $S$ matrix is invertible and thus the prior is normalizable. In this case the prior functions as a regularizer of state reconstruction. More complex prior distributions can be constructed for training data situated on curved manifolds \cite{otto2022inadequacy}.

\subsection{Gaussian prior inference}
For the Gaussian functional form of the prior (Eqn.~\ref{eqn:priorgauss}), we explicitly write out the log-likelihood as follows:
\begin{align}
	\ln p(\vec{a}|\vec{y})=-\frac{1}{2\eta^2}\left( \vec{y}-\Theta \vec{a} \right)^T \left( \vec{y}-\Theta \vec{a} \right) - \frac{\vec{a}^T S^{-2}\vec{a}}{2},
\end{align}
which is a quadratic function of the unknown state $\vec{a}$.

The reconstruction is obtained by setting the $\vec{a}$-derivative to zero:
\begin{align}
	\pdv{\ln p(\vec{a}|\vec{y})}{\vec{a}}=\frac{\Theta^T \vec{y}}{\eta^2}-\frac{\Theta^T \Theta \vec{a}}{\eta^2}-S^{-2}\vec{a}=0,
\end{align}
which results in a simple linear equation for state reconstruction. Solving the equation, we get the following prescription for reconstruction:
\begin{align}
	\hat{\vec{a}}=\left( S^{-2}+\frac{\Theta^T \Theta}{\eta^2} \right)^{-1} \frac{\Theta^T \vec{y}}{\eta^2}=A^{-1}\frac{\Theta^T \vec{y}}{\eta^2},
	\label{eqn:reconstruction}
\end{align}
which combines the information from the prior and the sensors. This reconstruction is linear and works for any values of sensor measurements $\vec{y}$, and thus does not say which set of sensors is better or worse and thus does not guide our sensor selection.

The reconstruction depends on inverting the composite matrix $A$, which might be ill-conditioned, meaning that small errors or noise in sensor measurements $\vec{y}$ can result in large error in the reconstructed state. We thus need to connect the reconstruction uncertainty to the metrics of matrix condition. Once such a metric is formulated, sensor placement can be designed to optimize it.

\section{Reconstruction uncertainty heatmap}
The reconstruction formula \eqref{eqn:reconstruction} gives the maximal likelihood state, but the uncertainty around that state is non-uniformly distributed. In order to quantify the uncertainty, here we compute the uncertainty heatmap across the whole domain of $\vec{x}$, given the sensor placement $C$. We denote the sensor reading fluctuation as $\vec{\Delta y}$, and propagate that fluctuation to the state reconstruction:
\begin{align}
	\vec{\Delta x}=\Psi_r A^{-1}\frac{\Theta^T}{\eta^2} \vec{\Delta y}.
\end{align}

The state fluctuation depends on the realization of sensor noise, which needs to be averaged out. We can compute the average covariance matrix between all the entries of $\vec{\Delta x}$ by taking the outer product of the state fluctuation with itself:
\begin{align}
	\expval{\vec{\Delta x} \vec{\Delta x}^T}= \Psi_r A^{-1} \frac{\Theta^T}{\eta^2} \expval{\vec{\Delta y} \vec{\Delta y}^T} \frac{\Theta}{\eta^2} A^{-1} \Psi_r^T,
\end{align}
where the sensor reading covariance is $\expval{\vec{\Delta y} \vec{\Delta y}^T}=I_p \eta^2$ by the assumption of uncorrelated noise. The whole state covariance matrix is $n\times n$, which characterizes uncertainty correlations between different locations but does not easily fit in computer memory for large state spaces.

We instead compute only the diagonal part of the covariance matrix, characterizing the level of uncertainty in each pixel of the reconstructed state:
\begin{align}
	B\equiv& \Psi_r A^{-1} \frac{\Theta^T}{\eta^2}\\
	\sigma_i =& \eta \sqrt{\sum\limits_j (B_{ij})^2},
\end{align}
where the matrix $B$ has dimensions $n\times p$ and the resulting vector $\vec{\sigma}$ contains the standard deviation of noise in each pixel of the reconstructed image, plotted in Fig.~2b of the main text.

\section{Sensor energy landscape}
\subsection{Determinant decomposition}
To enable systematic design of the sensor configuration, we aim to maximize the determinant of the matrix $A$ in the reconstruction \eqref{eqn:reconstruction}. The matrix determinant corresponds to the volume of the confidence ellipsoid around the maximal likelihood reconstruction. The choice to maximize the determinant is known as D-optimal design \cite{deaguiar1995doptimal, manohar2018data}, contrasted with A-optimal and E-optimal designs (matrix trace and spectral gap, respectfully).

The general idea of the computation is to relate the (log-) determinant of $A$ to the locations of the sensors, both in absolute space and with respect to each other. The dependence of sensor placement on absolute coordinates is equivalent to a 1-body interaction, or external field. The dependence of sensor placement on relative positions is equivalent to 2-body, 3-body, and higher order sensor interactions. Below we derive the functional form of interactions to all orders directly from the training data.

We start with transforming the determinant of $A$ into the determinant of a related matrix by using Sylvester's determinant theorem:
\begin{align}
	\det A&=\det(S^{-2}+\frac{\Theta^T\Theta}{\eta^2})\nonumber\\
	&=\det(S^{-2})\det(I+ S^{2}\frac{\Theta^T\Theta}{\eta^2})\nonumber\\
	&=\det(S^{-2})\det(I+\frac{\Theta S^2 \Theta^T}{\eta^2}),
	\label{eqn:det}
\end{align}
which converts an $r\times r$ matrix into a $p\times p$ matrix, with size directly related to the number of sensors.

To deepen the analogy with energy in physics, we identify the \emph{negative} log-determinant with the Hamiltonian of a sensor set $\gamma$:
\begin{align}
	\mathcal{H}(\gamma)\equiv-\ln\det(A)=E_b-\Tr\ln(I+\frac{\Theta S^2 \Theta^T}{\eta^2}),
	\label{eqn:Hgamma}
\end{align}
where we used the identity $\ln\det X = \tr\ln X$ for any generic matrix $X$.

We identify the expression within the logarithm with an outer product of a matrix with itself $\Theta S^2 \Theta^T\equiv G_\gamma G_\gamma^T$. We term the row vectors $\vec{g}_i,i\in\gamma$ \emph{sensing vectors}; the matrix $G_\gamma$ is then assembled from a subset of rows of $G\equiv \Psi_r S$ that correspond to chosen sensors $i\in\gamma$. The goal of the subsequent derivation is to relate $\mathcal{H}(\gamma)$ to the selected sensing vectors.

\subsection{Expansion in $\eta$ and resummation}
The Hamiltonian expression \eqref{eqn:Hgamma} requires taking a matrix logarithm of a complex matrix expression. The outer product $G_\gamma G_\gamma^T$ is a positive-semidefinite matrix, and thus the argument of the logarithm $I+G_\gamma G_\gamma^T/\eta^2$ is a positive-definite matrix $\forall \eta$, and the logarithm always exists. Additionally, for a sufficiently large value of $\eta$ the logarithm can be represented as an absolutely and uniformly convergent series expansion in $1/\eta^2$. The strategy of the derivation is thus as follows: (i) assume $\eta$ to be small, (ii) rewrite the energy function as a power expansion in orders of $1/\eta^2$, (iii) perform series resummation into a different closed-form function, (iv) expand the validity of the new function to arbitrary $\eta$ via analytic continuation.

\subsection{Diagonal separation}
We decompose the sensor-driven perturbation as a sum of two matrices:
\begin{align}
	G_\gamma G_\gamma^T\equiv D+R,
	\label{eqn:GGT}
\end{align}
where $D$ contains only the diagonal elements of the outer product, and $R$ contains all non-diagonal elements. Since $G_\gamma G_\gamma^T$ is a positive-semidefinite matrix, its diagonal $D$ inherits the same property. Importantly, the two matrices in this decomposition do not commute $[D,R]\neq 0$, and thus the order of their product is important.

In terms of these newly-defined matrices the Hamiltonian takes the following shape:
\begin{align}
	\mathcal{H}=E_b-\Tr\ln(I+\frac{1}{\eta^2}(D+R)),
\end{align}
where $E_b\equiv -\Tr\ln(\Sigma_r^{-2})$ is the baseline energy independent of noise and sensor choices. We then expand the matrix in powers of $1/\eta^2$:
\begin{align}
	\mathcal{H}(\gamma)=E_b+\Tr\sum\limits_{k=1}^{\infty} \eta^{-2k}(D+R)^k \frac{(-1)^k}{k},
	\label{eqn:EexactDR}
\end{align}
where we pulled one factor of $(-1)$ outside of the sum. The sum $(D+R)^k$ cannot be expanded as the simple binomial formula, because the matrices don't commute $[D,R]\neq 0$. Instead, the sum involves many products of $D,R$ in different order with order-dependent values.

We search for an expression for the Hamiltonian in the following form:
\begin{align}
	\mathcal{H}(\gamma)=E_b+\sum\limits_{s=0}^{\infty}f_s(R),
\end{align}
where $f_s(R)$ is a function in which $R$ occurs exactly $s$ times. We can get the form of $f_s$ by grouping terms with the same number of occurrences of $R$ in the full sum of Eqn.~\ref{eqn:EexactDR}. We treat separately the cases of $s=0$ and $s>0$.

For $s=0$, we want to gather the terms in which the crosstalk matrix $R$ never occurs. At each order in $k$, there is exactly one such term $D^k$, which we can resum for all orders of $k$ with appropriate series prefactors:
\begin{align}
	f_0=\Tr\sum\limits_{k=1}^{\infty}\eta^{-2k} D^k  \frac{(-1)^k}{k}=-\Tr\ln(I+\frac{1}{\eta^2}D),
\end{align}
which always exists because $D$ is positive-semidefinite.

\subsection{Case $s>0$}

Now consider the terms for $s>0$. Omitting the scalar factors, an example would be $\Tr(DRDDRRDDRD)$ where $s=4$. Note that by the cyclic property of the trace, the sequence can be shifted by repeatedly moving terms from the right end to the left end of the product without changing the value of the trace. Several different sequences then contribute the same value to the energy, and the number of such sequences needs to be carefully computed. For the term of order $k$ with $s$ occurrences of the matrix $R$ there are $\pmqty{k \\ s}$ terms, but not all of them have the same value.

We can write each contributing term in the following form:
\begin{align}
	\Tr(\prod\limits_{i=1}^{s}RD^{l_i}),
\end{align}
so that each occurrence of $R$ is followed by $l_i$ copies of $D$, where $l_i$ can be zero or higher. The total number of matrices $D$ or $R$ has to add up to $k$, which constraints the number of $D$ that can occur:
\begin{align}
	\sum\limits_{i=1}^{s} l_i=k-s\quad \Rightarrow \quad l_s=k-s-\sum\limits_{i=1}^{s-1}l_i,
	\label{eqn:lindex}
\end{align}
thus summing over all terms at fixed order $k$ involves only $s-1$ independent indices. The total number of terms of order $k$ is:
\begin{align}
	\pmqty{k \\ s}=\frac{1}{s!}k(k-1)\dots (k-s+1),
\end{align}
where the factor $k$ accounts for the $k$ locations in the sequence where the ``first'' occurrence of $R$ can happen, and the factor $s!$ accounts for the redundant overcounting of the choices of the ``first'', ``second'', and the following occurrences of $R$, neither of which affect the value of the trace. The remaining counting factors $(k-1),(k-2),\dots$ count the terms with different values of the trace. Putting these contributions together, we can write the following expression for $f_s$:
\begin{align}
	f_s=\Tr \sum\limits_{k=1}^{\infty}\sum\limits_{\{l\}} 
	\prod\limits_{i=1}^{s} \eta^{-2(1+l_i)}\left(R D^{l_i}\right)\frac{k}{s!}\frac{(-1)^k}{k},
\end{align}
where the sum over possible sets of $\{l\}$ obeys the constraint of Eqn.~\ref{eqn:lindex}. The counting factor $k$ in the numerator cancels with the factor $k$ from the series expansion of the logarithm. We can now exchange the order of summation in $k$ and $l_i$: instead of doing a constrained sum in all $l_i$ that add up to the same $k$, we treat them on the same level.

Intuitively, the trace can be thought of as a ring that consists of a sequence of $D,R$ elements. A ring does not have a beginning or the end, hence the factor $k$ in the number of rings with identical value of the trace. The subsequent matrices $R$ are separated by $l_i$ matrices $D$. Instead of counting all possible rings of the same length $k$, we instead perform independent sums over the length of all separators $l_i$, and the corresponding rings span all possible lengths, similar to enumeration approaches in heterogeneous self-assembly \cite{murugan2015undesired, klishin2021topological}. We thus rewrite the sum as follows:
\begin{align}
	(-1)^k=&(-1)^s \prod\limits_{i=1}^{s} (-1)^{l_i}\\
	f_s=& \frac{(-1)^s}{s!}\Tr \sum\limits_{\{l\}} \prod\limits_{i=1}^{s} \eta^{-2(1+l_i)} R (-D)^{l_i}\nonumber\\
	=&\frac{(-1)^s}{s!}\Tr \prod\limits_{i=1}^s \sum\limits_{l_i=0}^{\infty} \eta^{-2(1+l_i)} R (-D)^{l_i}\nonumber\\
	=&\frac{(-1)^s}{s!}\Tr \left(\left[\eta^{-2} R(I+\eta^{-2}D)^{-1}\right]^s\right),
\end{align}
where between the second and third lines we exchanged the order of product and sum. The resulting sum in powers of $(-D)$ is an alternating sign geometric series which converges for large $\eta$, and the resulting inversion of the diagonal matrix $(I+\eta^{-2}D)$ is valid for any value of $\eta$ because $D$ is positive semi-definite.

We can thus write the Hamiltonian exactly as follows:
\begin{align}
	\mathcal{H}(\gamma)=&E_b-\Tr\ln(I+\eta^{-2}D) \nonumber\\
	+&\sum\limits_{s=1}^{\infty} \frac{(-1)^s}{s!}\Tr\left(\left[\eta^{-2}R (I+\eta^{-2}D)^{-1}\right]^s\right),
	\label{eqn:E_resum}
\end{align}
which is a surprisingly concise form in terms of the separated diagonal and off-diagonal terms. If we were to rewrite the matrix expression in index notation, the $s=0$ term would have a single sum over all sensors, and each further term $s$ have a sum over $s$-sensor interactions. Note that since $R$ is non-diagonal and $(I+D)^{-1}$ is diagonal, the $s=1$ term is a trace of a non-diagonal matrix and thus it always vanishes. Only the terms for higher $s>1$ have nonzero values. Since the matrix $R$ does not have a general sign-definite property, the resulting series does not approximate energy either from above or from below.

\subsection{Energy landscapes}
Here we rewrite the matrix expression of Eqn.~\ref{eqn:E_resum} in index notation to highlight the contributions of 1-sensor and 2-sensor terms. First note the following index representation based on Eqn.~\ref{eqn:GGT}:
\begin{align}
	(D+R)_{ij}=(G_\gamma G_\gamma^T)_{ij}=\vec{g}_i\cdot \vec{g}_j,
\end{align}
where we highlight the nature of the terms as dot products of sensing vectors $\vec{g}_i$. The diagonal terms $D$ correspond to the dot product of each $\vec{g}_i$ with \emph{itself}, while the off-diagonal terms $R$ correspond to the dot products with \emph{different} vectors.

We now need to convert the understanding of the $\vec{g}_i$ vectors into the Ising-like Hamiltonian of shape in Eqn.~\ref{eqn:Hgamma}. We ignore the term $E_b$ since it does not depend on the noise or the choice of the sensors. We can also drop the $s=1$ term of the sum since it vanishes because of our matrix decomposition choices. We then truncate the sum in $s$ to only include the term $s=2$, resulting in the following expression for the \emph{2-point energy}:
\begin{align}
	\mathcal{H}_{2pt}(\gamma)\equiv& -\Tr\ln(I+D/\eta^2)\nonumber\\
	+&\frac{1}{2}\Tr(\left[\eta^{-2}R (I+\eta^{-2}D)^{-1}\right]^2),
	\label{eqn:H2pt}
\end{align}
which we need to transform into the index notation.

The first term has a trace of the log of a diagonal matrix, and thus equals to the sum of the logs of the individual entries:
\begin{align}
	-\Tr\ln(I+\eta^{-2}D)=&\sum\limits_{i\in \gamma} \ln(1+D_i/\eta^2)&\nonumber\\
	=&\sum\limits_{i\in \gamma} \ln(1+\vec{g}_i\cdot \vec{g}_i/\eta^2),
\end{align}
which is a sum of 1-sensor terms.

The second term can be rewritten as follows:
\begin{align}
	&\frac{1}{2}\Tr(\left[\eta^{-2}R(I+\eta^{-2}D)^{-1}\right]^2)\nonumber \\
	=&\frac{1}{2}\sum\limits_{ij\in \gamma} \frac{R_{ij}R_{ji}/\eta^4}{(1+D_i/\eta^2)(1+D_j/\eta^2)} \nonumber \\
	=&\frac{1}{2}\sum\limits_{i\neq j \in \gamma} \frac{(\vec{g}_i\cdot \vec{g}_j)^2/\eta^4}{(1+\vec{g}_i\cdot\vec{g}_i/\eta^2) (1+\vec{g}_j\cdot\vec{g}_j/\eta^2)},
\end{align}
where we used the fact that $R$ is non-diagonal to restrict the sum to only run over the sensor pairs with indices $i\neq j$. For both the 1-sensor and the 2-sensor terms, the sums run only over the sensors in the chosen set $\gamma$. We can thus extend the computation of the terms to the whole landscape that can be analyzed to pick the sensors that optimize the energy. The resulting landscape has the following form for all $ij$:
\begin{align}
	\mathcal{H}_{2pt}(\gamma)=&\sum\limits_{i\in \gamma} h_i + \sum\limits_{i\neq j \in \gamma} J_{ij}\label{eqn:Hgamma_resum}\\
	h_i\equiv& -\ln(1+\vec{g}_i\cdot \vec{g}_i/\eta^2)\leq 0\label{eqn:hi_resum}\\
	J_{ij}\equiv& \frac{1}{2} \frac{(\vec{g}_i\cdot \vec{g}_j)^2/\eta^4}{(1+\vec{g}_i\cdot\vec{g}_i/\eta^2) (1+\vec{g}_j\cdot\vec{g}_j/\eta^2)} \geq 0,\label{eqn:Jij_resum}
\end{align}
which is valid for any $\eta$.

What are the limits of this energy approximation? The answer to this question is intimately tied to the sensor placement algorithm. Generically, we expect the approximation to work while $\vec{g}_i\cdot \vec{g}_j\ll \vec{g}_i\cdot\vec{g}_i$, i.e. the correlation between the sensing vectors is small compared with their magnitude. For many systems it should be possible to choose sensors $i,j$ so that the terms $J_{ij}$ are small compared to the terms $h_i$. However, the \emph{number} of crosstalk terms $J_{ij}$ for $p$ sensors grows as $p^2$ with sensor number $p$. While the individual terms might be small, with increasing number of desired sensors both the number of terms grows, and the algorithm runs out of low crosstalk locations. Due to the combination of these reasons, we expect the approximation to inevitably break down for large number of sensors $p$. How large that number is would depend on the properties of the training data and the placement domain, and thus would need to be established in numerical experiments.

We note that the higher-order terms would have the shape similar to $J_{ij}$ in Eqn.~\ref{eqn:Jij_resum}, with a large number of indices. Due to the construction of $R$ as a non-diagonal matrix, the terms where \emph{adjacent} indices are identical would correspond to the diagonal of $R$ and thus vanish. However, the indices can repeat in non-adjacent positions, e.g. at fourth order in $J^{(4)}_{ijij}\neq 0$. The 2-point expression Eqn.~\ref{eqn:Hgamma_resum} is thus not exact even for placement of 2 sensors, but is expected to be a good approximation.

\section{Noise limits}
\subsection{High and low noise limits}
Here we consider the high noise and low noise limits of the landscape \eqref{eqn:hi_resum},\eqref{eqn:Jij_resum}. In the high noise limit $\eta\gg 1$ we get:
\begin{align}
	h_i=&\order{\eta^{-2}}\\
	J_{ij}=&\order{\eta^{-4}},
\end{align}
so the crosstalk falls off faster than the 1-sensor landscape. On one side, this stimulates putting more sensors in the basin of lowest $h_i$: since sensor noise is high, it makes more sense to collect measurements in the location of highest signal variance. On the other side, placing sensors close by breaks the approximation condition $\vec{g}_i\cdot \vec{g}_j\ll \vec{g}_i\cdot\vec{g}_i$, leading to a faster divergence between the 2-point energy and the true energy.

In the low noise limit $\eta\ll 1$:
\begin{align}
	h_i=&2\ln(\eta)-\ln(\vec{g}_i \cdot\vec{g}_i)\\
	J_{ij}=&\frac{1}{2} \frac{(\vec{g}_i\cdot \vec{g}_j)^2}{(\vec{g}_i\cdot\vec{g}_i)(\vec{g}_j\cdot\vec{g}_j)},
\end{align}
which approaches a constant, noise-independent shape where neither the 1-point nor 2-point or higher order interactions vanish for a generic sensor set. It should still be possible to choose a small set of sensors with low crosstalk and ensure that the 2-point energy is a good approximation of the true energy. Importantly, in low noise limit the sensor placement landscape does not depend on the absolute magnitude of the prior, but does depend on the its shape, i.e. uniform and non-uniform priors would typically result in different landscapes and thus different chosen sensor sets.

\section{Sensor placement methods}
% Figure environment removed

We consider and compare four algorithms of sensor placement:
\begin{enumerate}
	\item \textit{Random algorithm}: sensors are placed uniformly randomly within the signal domain without overlap.
	\item \textit{1-point algorithm}: sensors are placed to greedily minimize the 1-point energy $h_q$ without overlap.
	\item \textit{2-point algorithm}: sensors are placed to greedily minimize the 2-point energy following Eqn.~6 of main text without overlap.
	\item \textit{QR algorithm}: sensors are placed via greedy QR factorization of the $\Theta_r$ matrix with the \texttt{PySensors} package \cite{desilva2021pysensors}.
\end{enumerate}

The goal of all four methods is to minimize the sensor configuration energy $\mathcal{H}(\gamma)$ by minimizing different proxies (Fig.~\ref{fig:methods}). Across all methods, exact energy is higher than 1-point energy, indicating the importance of taking sensor crosstalk into account. For the 2-point and QR methods, the 2-point energy is a close approximation for the exact energy up until the number of sensors reaches the reconstruction rank $p=r$. For the 1-point method, the discrepancy between the exact and 2-point energies is the highest since placing the sensors without considering crosstalk results in strong spatial clustering and thus large crosstalk. The 1-point method consequently has the worst reconstruction RMSE (Fig. 3c of the main text). The random placement method has performance better than 1-point and worse than 2-point and QR.

\section{Prior selection}
% Figure environment removed

The reconstruction formula \eqref{eqn:reconstruction} and the sensor energy landscape both depend on the choice of the prior variances $S$. We consider the prior to be Gaussian, but it can have different patterns of variances along each dimension: either it is flat $S=\sigma I$ (we choose $\sigma=10^3$), or it follows the singular values of the dataset $S=\Sigma_r$. Since there are two choices of prior for two different operations, we need to consider four possible combinations.

Along with the prior-regularized reconstruction we consider a direct reconstruction of the latent state vector $\hat{\vec{a}}$ that solves a linear least square problem:
\begin{align}
    \hat{\vec{a}}=\arg \min\limits_{\vec{a}} \norm{\vec{y}-\Theta\vec{a}},
\end{align}
which is well-defined for any number of sensors $p$.

We show the benchmark comparison in Fig.~\ref{fig:prior}. The least squares reconstruction is never better than regularized reconstruction. In case of a flat prior used for regularization, the RMSE curves are almost equivalent except for the peak at $p\sim r$ that is suppressed by regularization. Selection of sensors is better performed against a flat prior (left column), leading to spread-out sampling and thus lower RMSE in the oversampled regime $p>r$. Sensors selected against a flat prior are sensitive to all modes used in the reconstruction similar to the QR method \cite{manohar2018data}, while sensors selected against a POD prior are primarily sensitive the first few modes. Reconstruction is better performed with a POD prior (top row) since it leads to a much smaller and less variable RMSE in the undersampled regime $p<r$.

Following the conclusions of this benchmark test, we pick sensors against a flat prior, yet reconstruct states with the POD prior. This combination of methods is used for all results reported in the main text and Figs.~\ref{fig:faces}-\ref{fig:cylinder}.

% combination of priors

% least square reconstruction

% benchmark
% need flat sensors for lower rmse in oversampled regime
% need svd reconstruction for lower rmse and variance in the undersampled regime

% svd prior for sensor selection combined with the hierarchical nature of the singular values makes sensors too driven by a few modes. flat prior optimizes the same objective as qr
% svd prior for reconstruction is necessary to constrain the states
% combination of the spreadout


\section{Sensor placement for Olivetti faces}
% Figure environment removed

Fig.~\ref{fig:faces} shows the sensor placement and reconstruction for the Olivetti faces dataset. The sensors cluster around the areas of high variability in human faces: eyes, nose, lips, as well as corners of the image. In contrast, there are almost no sensors on the forehead and cheeks which vary very little between the images. This dataset also suffers from reconstruction instability at $r\sim p$ caused by the truncation of POD modes.

\section{Sensor placement for cylinder flow}
% Figure environment removed

Fig.~\ref{fig:cylinder} shows the sensor placement and reconstruction for the cylinder flow dataset. The sensors cluster primarily in the wake behind the cylinder, with a few appearing at the far edge of the image. This dataset does not suffer from reconstruction instability at $r\sim p$ since the POD modes were truncated at a very low singular value.

\section{Exhaustive sensor enumeration}
% Figure environment removed

% Figure environment removed
\label{sec:exhaustive}

Figs.~3,\ref{fig:methods} establish the near-equivalence of the 2-point algorithm and the QR algorithm for sensor placement, but both algorithms are approximate optimizers. For $p$ sensors in $n$-dimensional state space there exist $\pmqty{n \\ p}$ possible configurations that can be enumerated directly for small $n,p$. In Figs.~\ref{fig:hist_GFF},\ref{fig:hist_RSS} we enumerate all sensor configurations for the GFF and RSS datasets, respectively, which both have $n=25$ and $p\in[1,8]$.

We assess the degree of optimality of the two algorithms with three metrics. First, we compute the Spearman rank correlation $\rho$ between the exact and approximate 2-point energies, which indicates to what degree minimizing the 2-point energy (moving down on the two-dimensional histograms) is equivalent to minimizing the true energy (moving left on the histograms). The Spearman correlation stays at the level of $\rho>0.99$ to two significant figures, showing that the exact energy is well approximated by the 2-point energy \emph{for all sensor configurations}.

Second, we compute is \emph{efficiency} $Q$, equal to the fraction of the sensor configurations with exact energy equal or higher than the configuration found by a particular method. For the best solution found by brute force search, efficiency is by definition $Q=100\%$. Across both datasets, efficiency of the 2-point and QR methods reaches that level, indicating that both methods find the exact optimal sensor configuration.

Third, we compute the energy difference between the true minimum and the approximations (Figs.~\ref{fig:hist_GFF}i,\ref{fig:hist_RSS}i) that remains at the level of floating point number precision.

An important limitation of this comparison is the artificially small size of the GFF and RSS datasets. While it is easy to generate larger datasets with the same software, they would not be amenable to brute force enumeration. Within the empirical datasets such as the Sea Surface Temperature, it is easy to place sensors with low crosstalk (Fig.~1 of main text) since crosstalk typically falls off with spatial distance between sensor locations. For small datasets, there are no locations with large distance between them, and thus it is impossible to achieve low crosstalk. We are looking forward to future methods for comparing the exact and approximately optimal sensor configurations.

\bibliography{sensors}
	
\end{document}

