%% ****** Start of file template.aps ****** %
%%
%%
%%   This file is part of the APS files in the REVTeX 4 distribution.
%%   Version 4.0 of REVTeX, August 2001
%%
%%
%%   Copyright (c) 2001 The American Physical Society.
%%
%%   See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.0
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showpacs' option to make PACS codes appear
%\documentclass[aps,prl,twocolumn,showpacs,superscriptaddress,groupedaddress]{revtex4-2}
\documentclass[aps,prl,twocolumn,showpacs,superscriptaddress]{revtex4-2}
% 
%for review and submission
%\documentclass[aps,preprint,showpacs,superscriptaddress,groupedaddress]{revtex4}
%  % for 
%double-spaced preprint
\usepackage{graphicx}  % needed for figures
\usepackage{dcolumn}   % needed for some tables
\usepackage{bm}        % for math
\usepackage{dsfont}    % for math (double-stroked characters via \mathds{text})
\usepackage{amssymb}   % for math
\usepackage{amsmath}
%\usepackage{journals}
\usepackage{color}
\usepackage{siunitx}
\usepackage{physics}
\usepackage{hyperref}

% avoids incorrect hyphenation, added Nov/08 by SSR
\hyphenation{ALPGEN}
\hyphenation{EVTGEN}
\hyphenation{PYTHIA}
\usepackage[utf8]{inputenc}

\newcommand{\aak}[1]{\textcolor[rgb]{1,0,0}{#1}}

\begin{document}
	\title{Data-Induced Interactions of Sparse Sensors}
	\author{Andrei A.~Klishin}
        \email{aklishin@uw.edu}
	\affiliation{AI Institute in Dynamic Systems, University of Washington, Seattle, WA 98195, USA}
	\affiliation{Department of Mechanical Engineering, University of Washington, Seattle, WA 98195, USA}
	\author{J.~Nathan Kutz}
	\affiliation{AI Institute in Dynamic Systems, University of Washington, Seattle, WA 98195, USA}
	\affiliation{Departments of Applied Mathematics and Electrical and Computer Engineering, University of Washington, Seattle, WA 98195, USA}
	\author{Krithika Manohar}
	\affiliation{AI Institute in Dynamic Systems, University of Washington, Seattle, WA 98195, USA}
	\affiliation{Department of Mechanical Engineering, University of Washington, Seattle, WA 98195, USA}

	
	\date{\today}
	
	\begin{abstract}
		Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular ``optimal'' sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allows combining them with external selection criteria and anticipating sensor replacement impacts.
	\end{abstract}
	
	\maketitle
	

%introduction - low-rank data structure
Many natural and engineered systems can take a variety of high-dimensional states, with the amount of data growing rapidly with the number of observed snapshots and increasing snapshot resolution. At the same time, the amount of information in this data usually grows much slower, often logarithmically \cite{quinn2022information, udell2019big, gavish2014optimal}. In this situation, any system state can be closely approximated by a combination of just a few basis vectors, enabling algorithms from lossy image compression to Dynamic Mode Decomposition \cite{kutz2016dynamic, lewis1992image}. While the optimal basis can be learned from historical data or high-fidelity simulations, states cannot be measured in that basis directly and often can only be accessed by spatially-localized sensors.

%current sensor approaches, compressed into sparse
Reconstruction of full states from localized sensor measurements has a long history under the umbrella term of \emph{compressed sensing}, where the sampling points (sensor locations) are chosen randomly and the state is reconstructed as a sparse combination of universal basis vectors \cite{donoho2006compressed, ganguli2010statistical, krzakala2012statistical}. More recently, driven by advances in gappy and reduced-order PDE methods \cite{everson1995karhunen, barrault2004empirical, chaturantabut2009discrete}, \emph{sparse sensing} algorithms have been developed to take advantage of the available training data to reduce the number of sensors required for given reconstruction quality \cite{manohar2018data}. The general sparse sensing problem is usually set up as follows: given the training data matrix $X$ consisting of $N$ snapshots of an $n$-dimensional state, one needs to reconstruct an unknown state $\vec{x}\in \mathbb{R}^n$ sampled from the same distribution as the data by using only the noisy measurements of a few components of the state $\vec{y}\in \mathbb{R}^p, p\ll n$.

%why sensor selection matters
While any combination of sensors of appropriate rank can be used to compute the maximal likelihood state reconstruction, the reconstruction robustness to sensor noise may vary by orders of magnitude, leading to the problem of \emph{sensor placement}. Each sensor configuration can be assigned a cost function value that can be approximately maximized with efficient greedy heuristics based on optimal experiment design, information theory metrics, Gibbs sampling, or matrix QR pivoting \cite{deaguiar1995doptimal, krause2008near, sun2020learning, peherstorfer2020stability, manohar2018data}. While these methods return a sensor configuration provably close to the true optimum due to the submodularity property \cite{nemhauser1978analysis, krause2011submodularity, otto2022inadequacy}, they do not inform why a particular configuration should be chosen, how to best modify it if sensor budget changes, and what would be the impact of a sensor malfunction on the reconstruction quality.

%sensor landscapes
In this paper, instead of searching for a singular ``optimal'' configuration of sensors, we take a thermodynamic perspective to  study the entire landscape of sensor interactions, akin to saliency maps in machine vision \cite{simonyan2013deep}. We show that the sensor interactions can be interpreted in terms of 1-body, 2-body, and higher order Hamiltonian terms  computed directly from the training data. Understanding the part of the landscape induced by data directly inspires a greedy sensor placement algorithm, allows incorporating landscapes driven by external cost factors, and anticipates the impacts of sensor replacement needs. The energy landscape analysis can be combined with other recent advances in sensor placement studies.

\paragraph{State reconstruction algorithm.}
The training library can be represented via Proper Orthogonal Decomposition (POD) and closely approximated via POD truncation:
\begin{align}
	X=\Psi \Sigma V^T \approx \Psi_r \Sigma_r V_r^T,
\end{align}
where we dropped all singular values beyond the first $r$ per the optimal truncation prescription \cite{gavish2014optimal}. In the reduced basis any state can be approximated as a linear combination of data-driven basis vectors $\vec{x}\approx \Psi_r \vec{a}$.

Our goal is to estimate the coefficients $\hat{\vec{a}}$ from the spatially-localized measurements $\vec{y}=C\vec{x}+\xi$, where $C$ is a $p\times n$ \emph{selection matrix} consisting of $p$ rows of the identity matrix and $\xi$ is Gaussian uncorrelated sensor measurement noise with magnitude $\eta$. Given this measurement model and a Gaussian prior distribution of the coefficients $\vec{a}$ parameterized by the variances $S$, we derive the following maximal likelihood reconstruction:
\begin{align}
	\hat{\vec{a}}=\left( S^{-2}+\frac{\Theta^T \Theta}{\eta^2} \right)^{-1} \frac{\Theta^T \vec{y}}{\eta^2}=A^{-1}\frac{\Theta^T \vec{y}}{\eta^2};\;
	\hat{\vec{x}}=\Psi_r \hat{\vec{a}}
	\label{eqn:reconstruction}
\end{align}
valid for any choice of sensors $C$ and sensor readings $\vec{y}$, with $\Theta=C\Psi_r$. However, the accuracy and noise sensitivity of this reconstruction depend dramatically on the properties of the sensor-dependent matrix $A$, and thus requires a strategy to place sensors systematically.

\paragraph{Sensor placement landscapes.}
% Figure environment removed

The reconstruction error of Eqn.~\ref{eqn:reconstruction} can be measured by various scalar functions of the matrix $A$, most commonly its determinant, in an approach known as D-optimal design \cite{deaguiar1995doptimal}. The determinant is an attractive optimization target because it characterizes the uncertainty hypervolume of the reconstruction, its maximum can be  approximated with the QR decomposition \cite{manohar2018data}, and the submodularity property guarantees the near-optimality of greedy optimization \cite{nemhauser1978analysis, krause2011submodularity, otto2022inadequacy}.

In this paper we identify the (negative) determinant of the inversion matrix with the energy or Hamiltonian of a particular set of sensors $\gamma$. The resulting Hamiltonian is remarkably similar to the Ising model found across statistical physics:
\begin{align}
	\mathcal{H}(\gamma)\equiv&-\ln(\det A)\approx E_b+\sum\limits_{i\in \gamma} h_i + \sum\limits_{i\neq j \in \gamma} J_{ij}\label{eqn:Hgamma_resum}\\
	h_i\equiv& -\ln(1+\vec{g}_i\cdot \vec{g}_i/\eta^2)\leq 0\label{eqn:hi_resum}\\
	J_{ij}\equiv& \frac{1}{2} \frac{(\vec{g}_i\cdot \vec{g}_j/\eta^2)^2}{(1+\vec{g}_i\cdot\vec{g}_i/\eta^2)(1+\vec{g}_j\cdot\vec{g}_j/\eta^2)} \geq 0,\label{eqn:Jij_resum},
\end{align}
where $\vec{g}_i$ are the \emph{sensing vectors} describing the sensitivity of each possible sensor location to each of the POD modes, computed as rows of the data-driven matrix $G=\Psi_r S$. The functional form of $h_i$ and $J_{ij}$ is computed via series expansion of the matrix $A$ in powers of $\eta$ (Eqn.~\ref{eqn:reconstruction}) and resummation, similar to enumeration arguments in self-assembly studies \cite{murugan2015undesired, klishin2021topological} (see Supplementary Materials for derivation).

In the Hamiltonian formulation of sensor placement \ref{eqn:Hgamma_resum}, the objective depends on the locations of individual sensors and sensor pairs from the chosen set $\gamma$ (Fig.~\ref{fig:landscape}). Qualitatively, minimizing the Hamiltonian requires picking sensors $i\in\gamma$ that capture a lot of signal variance (large $\vec{g}_i\cdot\vec{g}_i$), but are not very correlated with each other (small $\vec{g}_i\cdot\vec{g}_j$). While a combinatorial search for the lowest energy configuration would require evaluating the $\order{n^2}$ elements of the full crosstalk $\mathbf{J}$ matrix, we propose a simple greedy ``2-point'' algorithm that minimizes the marginal energy of each next placed sensor:
\begin{align}
	q=\arg\min\limits_q \left(h_q+2\sum\limits_{i\in \gamma} J_{iq}\right);\; \gamma\gets q, \label{eqn:greedy}
\end{align}
requiring just $\order{p\cdot n}$ evaluations to place $p\ll n$ sensors.

\paragraph{Reconstruction progress.}
% Figure environment removed

%explain the SST dataset features and performance
We demonstrate the sensor placement algorithm on an example dataset of weekly average sea surface temperature (SST) between 1990 and 2023 \cite{huang2021improvements}, truncated to POD rank $r=100$. Each frame covers the entirety of Earth surface in equirectangular projection at $1^\circ$ resolution, resulting in $360\times 180$ pixel images with $n=44219$ pixels corresponding to sea surface. We show that by employing as few as 25 sensors selected by the 2-point algorithm with noise level of $\eta=1^\circ C$, the entire temperature field can be reconstructed to within $1^\circ C$ (Fig.~\ref{fig:reconstruction}a). The reconstruction method also provides an Uncertainty Quantification (UQ) method in form of the uncertainty heat map at every pixel (Fig.~\ref{fig:reconstruction}b). Uncertainty is lowest close to the selected sensors, primarily around continents and within inland bodies of water, and highest in southern parts of the Indian, Pacific, and Atlantic Oceans. We emphasize that the sensor placement algorithm was trained exclusively on the snapshot library, with no additional information about the structure of Earth's oceans or their physical processes.

%RMSE non-monotonic
The reconstruction Root Mean Square Error (RMSE) shows non-monotonic dependence on the number of sensors (Fig.~\ref{fig:reconstruction}c). While adding more sensors contributes more information to the reconstruction algorithm, it also trades off with the number of independent sources of noise, resulting in best performance at $p\sim 25$ for sensor noise of $\eta=1.0^\circ C$. Importantly, the RMSE peaks at $p\sim r=100$ since the model cannot reconstruct the features in the truncated POD modes. Since both the reconstruction error and reconstruction uncertainty increase with sensor number, we assess the model confidence by computing the z-score of each individual pixel $z_i=(\hat{x}_i-x_i)/\sigma_i$ and plotting its distribution (Fig.~\ref{fig:reconstruction}d). Across all sensor numbers, the z-score distribution is symmetric and concentrated within $z\in[-3,3]$, indicating that the reconstruction does not systematically over- or under-estimate the temperature, and provides an accurate estimation of the uncertainty.

\paragraph{Reconstruction diagnostics}
% Figure environment removed

We compute sensor placement and reconstruction error across five datasets and four sensor placement methods. Apart from the SST dataset, we use the Olivetti faces dataset \cite{samaria1994parameterisation}, snapshots of a numerical simulation of flow past a cylinder \cite{taira2007immersed, colonius2008fast}, as well as synthetic Gaussian Free Field \cite{Cadiou2022Fyeld} and Random State System \cite{fuller2021python} datasets (Fig.~\ref{fig:comparison}a, see SM for dataset details). The four sensor placement algorithms are: random, 1-point (minimizing $h_q$ only), 2-point (Eqn.~\ref{eqn:greedy}), and QR-based \cite{desilva2021pysensors}. For the QR sensors, we compute reconstruction with and without the prior regularization. For each dataset, the sensor placement landscape is derived from the training set with 80\% of the data, and the reconstruction error is computed across the test set with the remaining 20\% of the data.

For the three empirical datasets (SST, Olivetti, and cylinder) the random and 1-point algorithms have higher energies than the other two (Fig.~\ref{fig:comparison}b). The 1-point algorithm has higher RMSE than other reconstructions with prior, highlighting the importance of crosstalk for sensor placement (Fig.~\ref{fig:comparison}c). The QR sensors without prior regularization also show consistently higher RMSE, justifying the need for a prior. The regularized random, 2-point, and QR algorithms show nearly equivalent RMSE error curves, all showing the peak at $p\sim r$ due to the POD mode truncation. For the two synthetic datasets all sensor placement methods have nearly equivalent performance, but can also be compared to brute force search (see SM). We conclude that while the 2-point and the QR algorithms are based on the same underlying POD modes and have nearly equivalent numerical performance, the 2-point algorithm provides much richer interpretation in terms of sensor landscapes and interactions.

\paragraph{Conclusions and outlook.} 
%paper summary and connection to data-generating processes
The key advance of this paper is casting the sensor selection problem in thermodynamic terms of interaction energies of progressively larger numbers of sensors. While we focus the discussion on the 1-body and 2-body interactions, the mathematical formalism extends to any higher number (see SM). The shape of the 2-body interactions can be further connected to the properties of the physical, mathematical, or even artistic processes that generate data \cite{stephens2013statistical, duplantier2017log, kentdobias2022log}. We used a greedy 2-point method of sensor placement in order to limit the required memory and computing time, but if the whole landscape could fit in memory, better energy minima can be obtained through methods such as gradient descent or simulated annealing \cite{kirkpatrick1983optimization}. Due to the usage of a regularizing prior, state reconstruction can be consistently performed for any number of sensors without the requirement that $p\geq r$ \cite{manohar2018data}.

%interpretability and generalizations
While the sensor landscape approach provides interpretability and in some regimes selects better sensor sets than state-of-the-art approaches, it is ultimately followed by a \emph{linear} algorithm for reconstructing the state from sensor readings, limiting the reconstruction accuracy. Recent work has shown that a linear algorithm of sensor selection based on QR factorization can be combined with a nonlinear shallow decoder network state estimation, which nevertheless requires neural network retraining for any new sensor set \cite{williams2022data}. An alternative approach instead focuses on learning the data manifold geometry and identifying nonlinear coordinates \cite{otto2022inadequacy}, which would be equivalent to replacing the Gaussian prior in our approach with a more complex one. Other sensor placement extensions can involve estimation of time-dependent dynamics through Kalman filtering \cite{tzoumas2016sensor}, or sensors advected by the flows they are trying to measure \cite{shriwastav2022dynamic}. Finally, the approach here identifies only the part of sensor placement landscape induced by the training data, which can be combined with other design objectives such as placement cost or restrictions \cite{klishin2018statistical, clark2018greedy, nishida2022region, karnik2023optimal}.

%acknowledgements
The authors would like to thank S.E.~Otto and J.~Williams for helpful discussions and L.D.~Lederer for administrative support. This work uses Scientific Color Maps for visualization \cite{crameri2023color}. The authors acknowledge support from the National Science Foundation AI Institute in Dynamic Systems (grant number 2112085).

\bibliography{sensors}
	
\end{document}

