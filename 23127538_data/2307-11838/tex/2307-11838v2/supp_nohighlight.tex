%% ****** Start of file template.aps ****** %
%%
%%
%%   This file is part of the APS files in the REVTeX 4 distribution.
%%   Version 4.0 of REVTeX, August 2001
%%
%%
%%   Copyright (c) 2001 The American Physical Society.
%%
%%   See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.0
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showpacs' option to make PACS codes appear
%\documentclass[aps,prl,twocolumn,showpacs,superscriptaddress,groupedaddress]{revtex4-2}
\documentclass[aps,pre,twocolumn,showpacs,superscriptaddress]{revtex4-2}
% 
%for review and submission
%\documentclass[aps,preprint,showpacs,superscriptaddress,groupedaddress]{revtex4}
%  % for 
%double-spaced preprint
\usepackage{graphicx}  % needed for figures
\usepackage{dcolumn}   % needed for some tables
\usepackage{bm}        % for math
\usepackage{dsfont}    % for math (double-stroked characters via \mathds{text})
\usepackage{amssymb}   % for math
\usepackage{amsmath}
%\usepackage{journals}
\usepackage{color}
\usepackage{siunitx}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{xcolor}

% avoids incorrect hyphenation, added Nov/08 by SSR
\hyphenation{ALPGEN}
\hyphenation{EVTGEN}
\hyphenation{PYTHIA}
\usepackage[utf8]{inputenc}

\newcommand{\aak}[1]{\textcolor[rgb]{1,0,0}{#1}}

\newcommand{\chg}[1]{#1}

\definecolor{hawaii}{HTML}{00452a} % U Hawai'i green
\newcommand{\chgtwo}[1]{#1}

\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

\begin{document}
	\title{Data-Induced Interactions of Sparse Sensors Using Statistical Physics: \\ Supplementary Materials}
	\author{Andrei A.~Klishin}
        \email{aklishin@hawaii.edu}
	\author{J.~Nathan Kutz}
	\author{Krithika Manohar}
        \email{kmanohar@uw.edu}
	
	\date{\today}
	
	\maketitle
	

\section{Data preprocessing}
\subsection{Sea Surface Temperature}
The Sea Surface Temperature (SST) dataset is curated by the National Oceanic and Atmospheric Administration (NOAA) and employs the Optimal Interpolation (OI) method to collect observations from different platforms and places them on a single regular grid for long periods of observations \cite{huang2021improvements}. In this work we use OISST 2.0 dataset \footnote{At the time of writing the dataset was available via the Physical Sciences Laboratory at \url{https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.html}.} that has the spatial resolution of 1 degree in both latitude and longitude, and is temporally averaged for each week between Dec 31, 1989 and Jan 1, 2023 for a total of $N=1727$ snapshots. Each temperature snapshot is given in equirectangular projection of size $360\times 180$ pixels. All of land surface of Earth is excluded from observations via a time-independent binary mask, leaving $n=44219$ pixels that vary between the states. We center the dataset by subtracting the temporal mean temperature profile from each snapshot. The POD is truncated to rank $r=100$. \chgtwo{Natural scale of the dataset is $\sigma_{scale}=1.945^\circ C$.}


\subsection{Olivetti faces}
The Olivetti faces dataset consists of photos of individuals taken between April 1992 and April 1994 at AT\&T Laboratories Cambridge \cite{samaria1994parameterisation}. Each of 40 individuals has 10 images, for a total of $N=400$ presented in random order. Each photo has the size $64\times 64$ grayscale pixels, resulting in $n=4096$. We center each image both locally and globally by subtracting the mean \chgtwo{face brightness profile from each image}. The POD is truncated to rank $r=100$. \chgtwo{Natural scale of the dataset is $\sigma_{scale}=0.1231$ (dimensionless).}

\subsection{Cylinder flow}
The cylinder flow dataset consists of scalar vorticity fields in 2D flow of a liquid around a cylinder, numerically simulated with immersed boundary method in Refs.~\cite{taira2007immersed, colonius2008fast}. In the simulation regime at Reynolds number $Re=100$, the flow consists of periodic shedding of vortices from the two sides of the cylinder in alternating order. The data consists of $N=151$ snapshots of resolution $449\times 199$ pixels, resulting in data dimension $n=89351$. We center the data by subtracting the temporal average vorticity profile from each snapshot. The POD is truncated to rank $r=40$. \chgtwo{Natural scale of the dataset is $\sigma_{scale}=0.550$ (dimensionless).}


\subsection{Gaussian Free Field}
The Gaussian Free Field (GFF) dataset is generated synthetically with the FyeldGenerator package \cite{Cadiou2022Fyeld}. The field is generated by drawing pseudorandom values of Fourier modes according to a user-specified power spectrum. In order to ensure that the field has coarse features on small spatial scale, we chose power spectrum $P(k)\propto k^{-10}$ as opposed to the usual $P(k)\propto k^{-2}$ for standard GFF. We generated $N=50$ pseudorandom snapshots of a field in the spatial domain $5\times 5$ pixels, resulting in $n=25$. The small size of the synthetic dataset was required to enable the exhaustive enumeration of sensors, see Sec.~\ref{sec:exhaustive}. We center each image both locally and globally by subtracting the mean brightness value of each image and the mean value across the dataset. The POD is truncated to rank $r=23$, equal to the number of singular values above machine precision $\sigma>10^{-15}$. \chgtwo{Natural scale of the dataset is $\sigma_{scale}=247.434$ (dimensionless).}

\subsection{Random State System}
The Random State System (RSS) dataset is generated synthetically with the Python Control Package \cite{fuller2021python} that replicates the functionality of Matlab Control Toolbox. RSS is a linear dynamical system with equation $\dot{\mathbf{x}}=M\mathbf{x}$ for a random square matrix $M$ of specified dimension, which we choose to be $n=25$. The generating package does not request any other free parameters such as the distribution of the random matrix, other than ensuring the dynamical system stability, i.e. $\forall\lambda: Re(\lambda)\leq 0$. We obtain the trajectory data by initializing random iid initial condition $x_i(t=0)\sim \mathcal{N}(0,1)$ and integrating the dynamical equation numerically in the range $t\in[0,5]$, sampling $N=101$ snapshots. The small size of the synthetic dataset was required to enable the exhaustive enumeration of sensors, see Sec.~\ref{sec:exhaustive}. We center each state both locally and globally by subtracting the mean brightness value of each image and the mean value across the dataset. The POD is truncated to rank $r=19$, equal to the number of singular values above machine precision $\sigma>10^{-15}$. \chgtwo{Natural scale of the dataset is $\sigma_{scale}=2.980$ (dimensionless).}

\subsection{Dataset singular values}
% Figure environment removed
Fig.~\ref{fig:singular} shows the spectra of singular values for each of the five datasets, along with the truncation threshold. As expected, singular values span many orders of magnitude for both empirical and synthetic datasets.

%\section{Prior and state reconstruction}


\section{Sensor energy landscape}



\subsection{Determinant decomposition}
To enable systematic design of the sensor configuration, we aim to maximize the determinant of the matrix $A$ in the reconstruction (Eqn.~9 of main text). The matrix determinant corresponds to the volume of the confidence ellipsoid around the maximal likelihood reconstruction. The choice to maximize the determinant is known as D-optimal design \cite{deaguiar1995doptimal, manohar2018data}, contrasted with A-optimal and E-optimal designs (matrix trace and spectral gap, respectfully).

The general idea of the computation is to relate the (log-) determinant of $A$ to the locations of the sensors, both in absolute space and with respect to each other. The dependence of sensor placement on absolute coordinates is equivalent to a 1-body interaction, or external field. The dependence of sensor placement on relative positions is equivalent to 2-body, 3-body, and higher order sensor interactions. Below we derive the functional form of interactions to all orders directly from the training data.

We start with transforming the determinant of $\mathbf{A}$ into the determinant of a related matrix by using Sylvester's determinant theorem:
\begin{align}
	\det \mathbf{A}&=\det(\mathbf{S}^{-2}+\frac{\mathbf{\Theta}^T\mathbf{\Theta}}{\eta^2})\nonumber\\
	&=\det(\mathbf{S}^{-2})\det(\mathbf{I}+ \mathbf{S}^{2}\frac{\mathbf{\Theta}^T\mathbf{\Theta}}{\eta^2})\nonumber\\
	&=\det(\mathbf{S}^{-2})\det(\mathbf{I}+\frac{\mathbf{\Theta} \mathbf{S}^2 \mathbf{\Theta}^T}{\eta^2}),
	\label{eqn:det}
\end{align}
which converts an $r\times r$ matrix into a $p\times p$ matrix, with size directly related to the number of sensors.

To deepen the analogy with energy in physics, we identify the \emph{negative} log-determinant with the Hamiltonian of a sensor set $\gamma$:
\begin{align}
	\mathcal{H}(\gamma)\equiv-\ln\det(\mathbf{A})=E_b-\Tr\ln(\mathbf{I}+\frac{\mathbf{\Theta} \mathbf{S}^2 \mathbf{\Theta}^T}{\eta^2}),
	\label{eqn:Hgamma}
\end{align}
where we used the identity $\ln\det X = \tr\ln X$ for any generic matrix $X$.

We identify the expression within the logarithm with an outer product of a matrix with itself $\mathbf{\Theta} \mathbf{S}^2 \mathbf{\Theta}^T\equiv \mathbf{G}_\gamma \mathbf{G}_\gamma^T$. We term the row vectors $\mathbf{g}_i,i\in\gamma$ \emph{sensing vectors}; the matrix $\mathbf{G}_\gamma$ is then assembled from a subset of rows of $\mathbf{G}\equiv \Psi_r \mathbf{S}$ that correspond to chosen sensors $i\in\gamma$. The goal of the subsequent derivation is to relate $\mathcal{H}(\gamma)$ to the selected sensing vectors.

\subsection{Expansion in $\eta$ and resummation}
The Hamiltonian expression \eqref{eqn:Hgamma} requires taking a matrix logarithm of a complex matrix expression. The outer product $\mathbf{G}_\gamma \mathbf{G}_\gamma^T$ is a positive-semidefinite matrix, and thus the argument of the logarithm $\mathbf{I}+\mathbf{G}_\gamma \mathbf{G}_\gamma^T/\eta^2$ is a positive-definite matrix $\forall \eta$, and the logarithm always exists. Additionally, for a sufficiently large value of $\eta$ the logarithm can be represented as an absolutely and uniformly convergent series expansion in $1/\eta^2$. The strategy of the derivation is thus as follows: (i) assume $\eta$ to be large, (ii) rewrite the energy function as a power expansion in orders of $1/\eta^2$, (iii) perform series resummation into a different closed-form function, (iv) expand the validity of the new function to arbitrary $\eta> 0$ via analytic continuation.

\subsection{Diagonal separation}
We decompose the sensor-driven perturbation as a sum of two matrices:
\begin{align}
	\mathbf{G}_\gamma \mathbf{G}_\gamma^T\equiv \mathbf{D}+\mathbf{R},
	\label{eqn:GGT}
\end{align}
where $\mathbf{D}$ contains only the diagonal elements of the outer product, and $\mathbf{R}$ contains all non-diagonal elements. Since $\mathbf{G}_\gamma \mathbf{G}_\gamma^T$ is a positive-semidefinite matrix, its diagonal $\mathbf{D}$ inherits the same property. Importantly, the two matrices in this decomposition do not commute $[\mathbf{D},\mathbf{R}]\neq 0$, and thus the order of their product is important.

In terms of these newly-defined matrices the Hamiltonian takes the following shape:
\begin{align}
	\mathcal{H}=E_b-\Tr\ln(\mathbf{I}+\frac{1}{\eta^2}(\mathbf{D}+\mathbf{R})),
\end{align}
where $E_b\equiv -\Tr\ln(\mathbf{S}^{-2})$ is the baseline energy independent of noise and sensor choices. We then expand the matrix in powers of $1/\eta^2$:
\begin{align}
	\mathcal{H}(\gamma)=E_b+\Tr\sum\limits_{k=1}^{\infty} \eta^{-2k}(\mathbf{D}+\mathbf{R})^k \frac{(-1)^k}{k},
	\label{eqn:EexactDR}
\end{align}
where we pulled one factor of $(-1)$ outside of the sum. The sum $(\mathbf{D}+\mathbf{R})^k$ cannot be expanded as the simple binomial formula, because the matrices \chg{do not} commute $[\mathbf{D},\mathbf{R}]\neq 0$. Instead, the sum involves many products of $\mathbf{D},\mathbf{R}$ in different order with order-dependent values.

We search for an expression for the Hamiltonian in the following form:
\begin{align}
	\mathcal{H}(\gamma)=E_b+\sum\limits_{s=0}^{\infty}f_s(\mathbf{D}, \mathbf{R}),
\end{align}
where $f_s(\mathbf{R})$ is a function in which $\mathbf{R}$ occurs exactly $s$ times. We can get the form of $f_s$ by grouping terms with the same number of occurrences of $\mathbf{R}$ in the full sum of Eqn.~\ref{eqn:EexactDR}. We treat separately the cases of $s=0$ and $s>0$.

For $s=0$, we want to gather the terms in which the crosstalk matrix $\mathbf{R}$ never occurs. At each order in $k$, there is exactly one such term $\mathbf{D}^k$, which we can resum for all orders of $k$ with appropriate series prefactors:
\begin{align}
	f_0=\Tr\sum\limits_{k=1}^{\infty}\eta^{-2k} \mathbf{D}^k  \frac{(-1)^k}{k}=-\Tr\ln(\mathbf{I}+\frac{1}{\eta^2}\mathbf{D}),
\end{align}
which always exists because $\mathbf{D}$ is positive-semidefinite.

\subsection{Case $s>0$}

Now consider the terms for $s>0$. Omitting the scalar factors, an example would be $\Tr(\mathbf{DRDDRRDDRD})$ where $s=4$. Note that by the cyclic property of the trace, the sequence can be shifted by repeatedly moving terms from the right end to the left end of the product without changing the value of the trace. Several different sequences then contribute the same value to the energy, and the number of such sequences needs to be carefully computed. For the term of order $k$ with $s$ occurrences of the matrix $\mathbf{R}$ there are $\pmqty{k \\ s}$ terms, but not all of them have the same value.

We can write each contributing term in the following form:
\begin{align}
	\Tr(\prod\limits_{i=1}^{s}\mathbf{R}\mathbf{D}^{l_i}),
\end{align}
so that each occurrence of $\mathbf{R}$ is followed by $l_i$ copies of $\mathbf{D}$, where $l_i$ can be zero or higher. The total number of matrices $\mathbf{D}$ or $\mathbf{R}$ has to add up to $k$, which constraints the number of $\mathbf{D}$ that can occur:
\begin{align}
	\sum\limits_{i=1}^{s} l_i=k-s\quad \Rightarrow \quad l_s=k-s-\sum\limits_{i=1}^{s-1}l_i,
	\label{eqn:lindex}
\end{align}
thus summing over all terms at fixed order $k$ involves only $s-1$ independent indices. The total number of terms of order $k$ is:
\begin{align}
	\pmqty{k \\ s}=\frac{1}{s!}k(k-1)\dots (k-s+1),
\end{align}
where the factor $k$ accounts for the $k$ locations in the sequence where the ``first'' occurrence of $\mathbf{R}$ can happen, and the factor $s!$ accounts for the redundant overcounting of the choices of the ``first'', ``second'', and the following occurrences of $\mathbf{R}$, neither of which affect the value of the trace. The remaining counting factors $(k-1),(k-2),\dots$ count the terms with different values of the trace. Putting these contributions together, we can write the following expression for $f_s$:
\begin{align}
	f_s=\Tr \sum\limits_{k=1}^{\infty}\sum\limits_{\{l\}} 
	\prod\limits_{i=1}^{s} \eta^{-2(1+l_i)}\left(\mathbf{R} \mathbf{D}^{l_i}\right)\frac{k}{s!}\frac{(-1)^k}{k},
\end{align}
where the sum over possible sets of $\{l\}$ obeys the constraint of Eqn.~\ref{eqn:lindex}. The counting factor $k$ in the numerator cancels with the factor $k$ from the series expansion of the logarithm. We can now exchange the order of summation in $k$ and $l_i$: instead of doing a constrained sum in all $l_i$ that add up to the same $k$, we treat them on the same level.

Intuitively, the trace can be thought of as a ring that consists of a sequence of $\mathbf{D},\mathbf{R}$ elements. A ring does not have a beginning or the end, hence the factor $k$ in the number of rings with identical value of the trace. The subsequent matrices $\mathbf{R}$ are separated by $l_i$ matrices $\mathbf{D}$. Instead of counting all possible rings of the same length $k$, we instead perform independent sums over the length of all separators $l_i$, and the corresponding rings span all possible lengths, similar to enumeration approaches in heterogeneous self-assembly \cite{murugan2015undesired, klishin2021topological}. We thus rewrite the sum as follows:
\begin{align}
	(-1)^k=&(-1)^s \prod\limits_{i=1}^{s} (-1)^{l_i}\\
	f_s=& \frac{(-1)^s}{s!}\Tr \sum\limits_{\{l\}} \prod\limits_{i=1}^{s} \eta^{-2(1+l_i)} \mathbf{R} (-\mathbf{D})^{l_i}\nonumber\\
	=&\frac{(-1)^s}{s!}\Tr \prod\limits_{i=1}^s \sum\limits_{l_i=0}^{\infty} \eta^{-2(1+l_i)} \mathbf{R} (-\mathbf{D})^{l_i}\nonumber\\
	=&\frac{(-1)^s}{s!}\Tr \left(\left[\eta^{-2} \mathbf{R}(\mathbf{I}+\eta^{-2}\mathbf{D})^{-1}\right]^s\right),
\end{align}
where between the second and third lines we exchanged the order of product and sum. The resulting sum in powers of $(-\mathbf{D})$ is an alternating sign geometric series which converges for large $\eta$, and the resulting inversion of the diagonal matrix $(\mathbf{I}+\eta^{-2}\mathbf{D})$ is valid for any value of $\eta$ because $\mathbf{D}$ is positive semi-definite.

We can thus write the Hamiltonian exactly as follows:
\begin{align}
	\mathcal{H}(\gamma)=&E_b-\Tr\ln(\mathbf{I}+\eta^{-2}\mathbf{D}) \nonumber\\
	+&\sum\limits_{s=1}^{\infty} \frac{(-1)^s}{s!}\Tr\left(\left[\eta^{-2}\mathbf{R} (\mathbf{I}+\eta^{-2}\mathbf{D})^{-1}\right]^s\right),
	\label{eqn:E_resum}
\end{align}
which is a surprisingly concise form in terms of the separated diagonal and off-diagonal terms. If we were to rewrite the matrix expression in index notation, the $s=0$ term would have a single sum over all sensors, and each further term $s$ have a sum over $s$-sensor interactions. Note that since $\mathbf{R}$ is non-diagonal and $(\mathbf{I}+\mathbf{D})^{-1}$ is diagonal, the $s=1$ term is a trace of a non-diagonal matrix and thus it always vanishes. Only the terms for higher $s>1$ have nonzero values. Since the matrix $\mathbf{R}$ does not have a general sign-definite property, the resulting series does not approximate energy either from above or from below.

\subsection{Energy landscapes}
Here we rewrite the matrix expression of Eqn.~\ref{eqn:E_resum} in index notation to highlight the contributions of 1-sensor and 2-sensor terms. First note the following index representation based on Eqn.~\ref{eqn:GGT}:
\begin{align}
	(\mathbf{D}+\mathbf{R})_{ij}=(\mathbf{G}_\gamma \mathbf{G}_\gamma^T)_{ij}=\mathbf{g}_i\cdot \mathbf{g}_j,
\end{align}
where we highlight the nature of the terms as dot products of sensing vectors $\mathbf{g}_i$. The diagonal terms $\mathbf{D}$ correspond to the dot product of each $\mathbf{g}_i$ with \emph{itself}, while the off-diagonal terms $\mathbf{R}$ correspond to the dot products with \emph{different} vectors.

We now need to convert the understanding of the $\mathbf{g}_i$ vectors into the Ising-like Hamiltonian of shape in Eqn.~\ref{eqn:Hgamma}. We ignore the term $E_b$ since it does not depend on the noise or the choice of the sensors. We can also drop the $s=1$ term of the sum since it vanishes because of our matrix decomposition choices. We then truncate the sum in $s$ to only include the term $s=2$, resulting in the following expression for the \emph{2-point energy}:
\begin{align}
	\mathcal{H}_{2pt}(\gamma)\equiv& -\Tr\ln(\mathbf{I}+\mathbf{D}/\eta^2)\nonumber\\
	+&\frac{1}{2}\Tr(\left[\eta^{-2}\mathbf{R} (\mathbf{I}+\eta^{-2}\mathbf{D})^{-1}\right]^2),
	\label{eqn:H2pt}
\end{align}
which we need to transform into the index notation.

The first term has a trace of the log of a diagonal matrix, and thus equals to the sum of the logs of the individual entries:
\begin{align}
	-\Tr\ln(\mathbf{I}+\eta^{-2}\mathbf{D})=&-\sum\limits_{i\in \gamma} \ln(1+\mathbf{D}_i/\eta^2)&\nonumber\\
	=&-\sum\limits_{i\in \gamma} \ln(1+\mathbf{g}_i\cdot \mathbf{g}_i/\eta^2),
\end{align}
which is a sum of 1-sensor terms.

The second term can be rewritten as follows:
\begin{align}
	&\frac{1}{2}\Tr(\left[\eta^{-2}\mathbf{R}(\mathbf{I}+\eta^{-2}\mathbf{D})^{-1}\right]^2)\nonumber \\
	=&\frac{1}{2}\sum\limits_{ij\in \gamma} \frac{\mathbf{R}_{ij}\mathbf{R}_{ji}/\eta^4}{(1+\mathbf{D}_i/\eta^2)(1+\mathbf{D}_j/\eta^2)} \nonumber \\
	=&\frac{1}{2}\sum\limits_{i\neq j \in \gamma} \frac{(\mathbf{g}_i\cdot \mathbf{g}_j)^2/\eta^4}{(1+\mathbf{g}_i\cdot\mathbf{g}_i/\eta^2) (1+\mathbf{g}_j\cdot\mathbf{g}_j/\eta^2)},
\end{align}
where we used the fact that $\mathbf{R}$ is non-diagonal to restrict the sum to only run over the sensor pairs with indices $i\neq j$. For both the 1-sensor and the 2-sensor terms, the sums run only over the sensors in the chosen set $\gamma$. We can thus extend the computation of the terms to the whole landscape that can be analyzed to pick the sensors that optimize the energy. The resulting landscape has the following form for all $ij$:
\begin{align}
	\mathcal{H}_{2pt}(\gamma)=&\sum\limits_{i\in \gamma} h_i + \sum\limits_{i\neq j \in \gamma} J_{ij}\label{eqn:Hgamma_resum}\\
	h_i\equiv& -\ln(1+\mathbf{g}_i\cdot \mathbf{g}_i/\eta^2)\leq 0\label{eqn:hi_resum}\\
	J_{ij}\equiv& \frac{1}{2} \frac{(\mathbf{g}_i\cdot \mathbf{g}_j)^2/\eta^4}{(1+\mathbf{g}_i\cdot\mathbf{g}_i/\eta^2) (1+\mathbf{g}_j\cdot\mathbf{g}_j/\eta^2)} \geq 0,\label{eqn:Jij_resum}
\end{align}
which is valid for any $\eta$.

What are the limits of this energy approximation? The answer to this question is intimately tied to the sensor placement algorithm. Generically, we expect the approximation to work while $\mathbf{g}_i\cdot \mathbf{g}_j\ll \mathbf{g}_i\cdot\mathbf{g}_i$, i.e. the correlation between the sensing vectors is small compared with their magnitude. For many systems it should be possible to choose sensors $i,j$ so that the terms $J_{ij}$ are small compared to the terms $h_i$. However, the \emph{number} of crosstalk terms $J_{ij}$ for $p$ sensors grows as $p^2$ with sensor number $p$. While the individual terms might be small, with increasing number of desired sensors both the number of terms grows, and the algorithm runs out of low crosstalk locations. Due to the combination of these reasons, we expect the approximation to inevitably break down for large number of sensors $p$. How large that number is would depend on the properties of the training data and the placement domain, and thus would need to be established in numerical experiments.

We note that the higher-order terms would have the shape similar to $J_{ij}$ in Eqn.~\ref{eqn:Jij_resum}, with a large number of indices. Due to the construction of $\mathbf{R}$ as a non-diagonal matrix, the terms where \emph{adjacent} indices are identical would correspond to the diagonal of $\mathbf{R}$ and thus vanish. However, the indices can repeat in non-adjacent positions, e.g. at fourth order in $J^{(4)}_{ijij}\neq 0$. The 2-point expression Eqn.~\ref{eqn:Hgamma_resum} is thus not exact even for placement of 2 sensors, but is expected to be a good approximation.

%\section{Noise limits}


\section{Prior selection}
% Figure environment removed

The reconstruction formula (Eqn.~9 of main text) and the sensor energy landscape both depend on the choice of the prior variances $\mathbf{S}$. We consider the prior to be Gaussian, but it can have different patterns of variances along each dimension: either it is isotropic $\mathbf{S}=\sigma_{prior} \mathbf{I}$ (we choose $\sigma_{prior}=10^3$), or it follows the singular values \chgtwo{(POD)} of the dataset \chgtwo{$\mathbf{S}=\mathbf{\Sigma}_r/\sqrt{N-1}$ \cite{kakasenko2025bridging}}. Since there are two choices of prior for two different operations, we need to consider four possible combinations.

Along with the prior-regularized reconstruction we consider a direct reconstruction of the latent state vector $\hat{\mathbf{a}}$ that solves a linear least square problem:
\begin{align}
	\hat{\mathbf{a}}=\arg \min\limits_{\mathbf{a}} \norm{\mathbf{y}-\mathbf{\Theta}\mathbf{a}}\chgtwo{=\mathbf{\Theta}^\dagger \mathbf{y}},
\end{align}
which is well-defined for any number of sensors $p$.

We show the benchmark comparison in Fig.~\ref{fig:prior}. The least squares reconstruction is never better than regularized reconstruction \chgtwo{and demonstrates the sharp double descent behavior at $p\approx r=100$}.
\chgtwo{The regularized reconstruction curves show strikingly different curves. When an isotropic prior is used for both sensor placement and reconstruction, the peak in RMSE is slightly suppressed but not fully removed (panel c). For the other three scenarios, the RMSE curve is monotonic and has no peak. Placing sensors with a POD prior computes the sensing vector dot products $\mathbf{g}_i\cdot \mathbf{g}_j$ with non-uniform weights, and effectively reduces the dimension in which the sensing vectors are selected (right column of panels). As a result, the selected sensors are more redundant with each other. However, using the POD prior for reconstruction significantly reduces the RMSE while removing the double descent peak (top row of panels). As a result, using an isotropic prior for sensor placement results in a wide variety of non-redundant sensors selected, while the POD regularization improves reconstruction (panel a).}
% In case of an isotropic (flat) prior used for regularization, the RMSE curves are almost equivalent except for the peak at $p\sim r$ that is suppressed by regularization. Selection of sensors is better performed against aa isotropic prior (left column), leading to spread-out sampling and thus lower RMSE in the oversampled regime $p>r$. Sensors selected against an isotropic prior are sensitive to all modes used in the reconstruction similar to the QR method \cite{manohar2018data}, while sensors selected against a POD prior are primarily sensitive the first few modes. Reconstruction is better performed with a POD prior (top row) since it leads to a much smaller and less variable RMSE in the undersampled regime $p<r$.

Following the conclusions of this benchmark test, we pick sensors against an isotropic prior, yet reconstruct states with the POD prior. This combination of methods is used for all results reported in the main text and Figs.~\ref{fig:faces}-\ref{fig:cylinder}.

% combination of priors

% least square reconstruction

% benchmark
% need flat sensors for lower rmse in oversampled regime
% need svd reconstruction for lower rmse and variance in the undersampled regime

% svd prior for sensor selection combined with the hierarchical nature of the singular values makes sensors too driven by a few modes. flat prior optimizes the same objective as qr
% svd prior for reconstruction is necessary to constrain the states
% combination of the spreadout

\section{Sensor method comparison}

% Figure environment removed

% Figure environment removed

\chgtwo{Figs.~\ref{fig:methods_faces}-\ref{fig:methods_faces} show the comparison of sensor placement methods and respective energies for other two empirical datasets in the same format as Fig.~2 of the main text.}

\section{Sensor placement for Olivetti faces}
% Figure environment removed

Fig.~\ref{fig:faces} shows the sensor placement and reconstruction for the Olivetti faces dataset. \chgtwo{The 1-point landscape $h_i$ has many localized basins for placement, aligning with the areas of high variability in human faces: eyes, nose, lips, as well as corners of the image (panel a). The 2-point landscape $J_{ij}$ shows both the local repulsion of sensors from nearby locations, and the facial symmetry features such as similarity between the two eyes (panel a). As the number of sensors increases, the reconstructed face gets progressively sharper features (panel b). The sensor placement largely follows the many local basins of the 1-point landscape (panel c). Since the basins are not very deep and sensor repulsion precludes placing many sensors close by, the 2-point algorithm effectively distributes the sensors throughout all highly-variable features across the domain and gradually increases the placement density with higher sensor budgets. The uncertainty magnitude \emph{increases} (darker color) with higher sensor counts, while the RMSE curve shows small local fluctuations along with the overall nearly monotonic decrease. This latter property suggests that while our algorithm identifies the principal facial features, the Olivetti faces are not well-described with 100 linear modes, and either a higher mode count or a nonlinear encoding are required.}

% The sensors cluster around the areas of high variability in human faces: eyes, nose, lips, as well as corners of the image. In contrast, there are almost no sensors on the forehead and cheeks which vary very little between the images. This dataset also suffers from reconstruction instability at $r\sim p$ caused by the truncation of POD modes. \aak{comment on the placement}

\section{Sensor placement for cylinder flow}
% Figure environment removed

Fig.~\ref{fig:cylinder} shows the sensor placement and reconstruction for the cylinder flow dataset. \chgtwo{The 1-point landscape shows a deep basin along the symmetry axis in the wake behind the cylinder, as well as two more shallow basins towards the right side of the domain symmetrically offset from the axis (panel a). The 2-point landscape shows that locations outside of the wake have practically no correlation with those inside the wake (panel a). Within the wake, the correlation structure has a distinct wave-like pattern corresponding to the spatial wavelength of the vortex shedding (panel a). The reconstructions show an error imperceptible by eye already at $p=10$ sensors (panel b). Unlike SST and Olivetti faces datasets, the cylinder flow dataset shows a tight clustering of sensors that gradually gets longer and wider, with only two peripheral sensors appearing for $20<p<30$ (panel c). The RMSE curve decays monotonically towards a very low value without significant fluctuations.}

\chgtwo{The sensor placement pattern can be explained by the quantitative features of the landscape. The central basin of 1-point energy is both deep and wide, reaching the lowest value of $h_{center~min}\approx -12.97$. The 2-point interactions of the sensors have a narrow band structure corresponding to the wake wavelength, but in magnitude peak at about $J_{ij}\approx 0.4$. It is thus advantageous to place multiple sensors very close to each other up to a point, until the crosstalk of a new sensor with previously selected neighbors overwhelms the advantage of the basin. As a result, as the sensor budget grows, sensors are placed in a band that gradually gets both longer and wider (along and orthogonally to the symmetry axis). Fig.~\ref{fig:cylinder} also shows that the sensor placement method that accounts for crosstalk (2-point or QR) is significantly better than those that do not (random and 1-point) in terms of reaching lower energy at the same sensor budget.}

\chgtwo{Beyond the central basin, the landscape also shows two symmetric secondary basins offset from the central axis on the far-right end of the domain. The minimal value of the 1-point term $h_i$ is reached in the last column and is equal to $h_{side~min}\approx -12.30$. Thus, it is advantageous for the algorithm to place sensors within the central basin until it is exhausted, i.e., when the crosstalk with previously placed sensors cancels out the difference between the central and side basins $J>(h_{side~min}-h_{center~min})\approx 0.7$. In the regime presented in Fig.~S6 this corresponds to sensors number 25 and 28 so they appear in the third visualized frame at $p=30$.}



% The sensors cluster primarily in the wake behind the cylinder, with a few appearing at the far edge of the image. This dataset does not suffer from reconstruction instability at $r\sim p$ since the POD modes were truncated at a very low singular value. \aak{comment on the placement and the basin, min h reached}

\section{Exhaustive sensor enumeration}
% Figure environment removed

% Figure environment removed
\label{sec:exhaustive}

Figs.~2,4 of main text establish the near-equivalence of the 2-point algorithm and the QR algorithm for sensor placement, but both algorithms are approximate optimizers. For $p$ sensors in $n$-dimensional state space there exist $\pmqty{n \\ p}$ possible configurations that can be enumerated directly for small $n,p$. In Figs.~\ref{fig:hist_GFF},\ref{fig:hist_RSS} we enumerate all sensor configurations for the GFF and RSS datasets, respectively, which both have $n=25$ and $p\in[1,8]$.

We assess the degree of optimality of the two algorithms with three metrics. First, we compute the Spearman rank correlation $\rho$ between the exact and approximate 2-point energies, which indicates to what degree minimizing the 2-point energy (moving down on the two-dimensional histograms) is equivalent to minimizing the true energy (moving left on the histograms). The Spearman correlation stays at the level of $\rho>0.99$ to two significant figures, showing that the exact energy is well approximated by the 2-point energy \emph{for all sensor configurations}.

Second, we compute its \emph{efficiency} $Q$, equal to the fraction of the sensor configurations with exact energy equal or higher than the configuration found by a particular method. For the best solution found by brute force search, efficiency is by definition $Q=100\%$. Across both datasets, efficiency of the 2-point and QR methods reaches that level, indicating that both methods find the exact optimal sensor configuration.

Third, we compute the energy difference between the true minimum and the approximations (Figs.~\ref{fig:hist_GFF}i,\ref{fig:hist_RSS}i) that remains at the level of floating point number precision.

An important limitation of this comparison is the artificially small size of the GFF and RSS datasets. While it is easy to generate larger datasets with the same software, they would not be amenable to brute force enumeration. Within the empirical datasets such as the Sea Surface Temperature, it is easy to place sensors with low crosstalk (Fig.~1 of main text) since crosstalk typically falls off with spatial distance between sensor locations. For small datasets, there are no locations with large distance between them, and thus it is impossible to achieve low crosstalk. We are looking forward to future methods for comparing the exact and approximately optimal sensor configurations.

\bibliography{sensors}


	
\end{document}

