\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ameer et~al.(2022)Ameer, Sidorov, Gomez-Adorno, and
  Nawab]{ameer2022multi}
Ameer, I., Sidorov, G., Gomez-Adorno, H., and Nawab, R. M.~A.
\newblock Multi-label emotion classification on code-mixed text: Data and
  methods.
\newblock \emph{IEEE Access}, 10:\penalty0 8779--8789, 2022.

\bibitem[Anne~Hendricks et~al.(2017)Anne~Hendricks, Wang, Shechtman, Sivic,
  Darrell, and Russell]{textgrounding_in_video}
Anne~Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., and
  Russell, B.
\newblock Localizing moments in video with natural language.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  5803--5812, 2017.

\bibitem[Assael et~al.(2016)Assael, Shillingford, Whiteson, and
  De~Freitas]{lipreading}
Assael, Y.~M., Shillingford, B., Whiteson, S., and De~Freitas, N.
\newblock Lipnet: End-to-end sentence-level lipreading.
\newblock \emph{arXiv preprint arXiv:1611.01599}, 2016.

\bibitem[Aytar et~al.(2016)Aytar, Vondrick, and Torralba]{soundnet}
Aytar, Y., Vondrick, C., and Torralba, A.
\newblock Soundnet: Learning sound representations from unlabeled video.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  892--900, 2016.

\bibitem[Belova et~al.(2019)Belova, He, and Zhong]{belova_e-sports_2019}
Belova, A., He, W., and Zhong, Z.
\newblock E-sports talent scouting based on multimodal twitch stream data.
\newblock \emph{CoRR}, abs/1907.01615, 2019.

\bibitem[Carreira \& Zisserman(2017)Carreira and Zisserman]{id3}
Carreira, J. and Zisserman, A.
\newblock Quo vadis, action recognition? {A} new model and the kinetics
  dataset.
\newblock \emph{CoRR}, abs/1705.07750, 2017.

\bibitem[Chen et~al.(2023)Chen, Wang, Yue, Bai, Le, and Wang]{app13137753}
Chen, T., Wang, X., Yue, T., Bai, X., Le, C.~X., and Wang, W.
\newblock Enhancing abstractive summarization with extracted knowledge graphs
  and multi-source transformers.
\newblock \emph{Applied Sciences}, 13\penalty0 (13), 2023.
\newblock ISSN 2076-3417.
\newblock \doi{10.3390/app13137753}.
\newblock URL \url{https://www.mdpi.com/2076-3417/13/13/7753}.

\bibitem[Ford et~al.(2017)Ford, Gardner, Horgan, Liu, Nardi, Rickman,
  et~al.]{twitchchat}
Ford, C., Gardner, D., Horgan, L.~E., Liu, C., Nardi, B., Rickman, J., et~al.
\newblock Chat speed op pogchamp: Practices of coherence in massive twitch
  chat.
\newblock In \emph{Proceedings of the 2017 CHI Conference Extended Abstracts on
  Human Factors in Computing Systems}, pp.\  858--871. ACM, 2017.

\bibitem[Fu et~al.(2017)Fu, Lee, Bansal, and Berg]{videohighlights_with_chat}
Fu, C., Lee, J., Bansal, M., and Berg, A.~C.
\newblock Video highlight prediction using audience chat reactions.
\newblock \emph{CoRR}, abs/1707.08559, 2017.

\bibitem[Gong et~al.(2014)Gong, Ke, Isard, and Lazebnik]{multiview_embedding}
Gong, Y., Ke, Q., Isard, M., and Lazebnik, S.
\newblock A multi-view embedding space for modeling internet images, tags, and
  their semantics.
\newblock \emph{Int. J. Comput. Vision}, 106\penalty0 (2):\penalty0 210--233,
  January 2014.

\bibitem[Gygli et~al.(2014)Gygli, Grabner, Riemenschneider, and
  Van~Gool]{gygli_creating_2014}
Gygli, M., Grabner, H., Riemenschneider, H., and Van~Gool, L.
\newblock Creating summaries from user videos.
\newblock In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T. (eds.),
  \emph{Computer Vision -- ECCV 2014}, pp.\  505--520, Cham, 2014. Springer
  International Publishing.

\bibitem[Han et~al.(2023)Han, Wang, Zhang, Wang, Zhang, and Xu]{han2023complex}
Han, M., Wang, Q., Zhang, T., Wang, Y., Zhang, D., and Xu, B.
\newblock Complex dynamic neurons improved spiking transformer network for
  efficient automatic speech recognition.
\newblock \emph{arXiv preprint arXiv:2302.01194}, 2023.

\bibitem[Karpathy et~al.()Karpathy, Toderici, Shetty, Leung, Sukthankar, and
  Fei-Fei]{karpathy_large-scale_2014}
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei,
  L.
\newblock Large-scale video classification with convolutional neural networks.
\newblock In \emph{2014 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pp.\  1725--1732. {IEEE}.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier,
  Vijayanarasimhan, Viola, Green, Back, Natsev, Suleyman, and
  Zisserman]{kay_kinetics_2017}
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
  S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., and Zisserman,
  A.
\newblock The kinetics human action video dataset.
\newblock \emph{CoRR}, abs/1705.06950, 2017.

\bibitem[Liang et~al.(2019)Liang, Jiang, Cao, Kalantidis, Li, and
  Hauptmann]{focalattention_model}
Liang, J., Jiang, L., Cao, L., Kalantidis, Y., Li, L.-J., and Hauptmann, A.~G.
\newblock Focal visual-text attention for memex question answering.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 1893--1908, 2019.

\bibitem[Miech et~al.(2018)Miech, Laptev, and Sivic]{textvideoembedding}
Miech, A., Laptev, I., and Sivic, J.
\newblock Learning a text-video embedding from incomplete and heterogeneous
  data.
\newblock \emph{CoRR}, abs/1804.02516, 2018.

\bibitem[Musabirov et~al.(2018)Musabirov, Bulygin, Okopny, and
  Konstantinova]{arenasportsbar}
Musabirov, I., Bulygin, D., Okopny, P., and Konstantinova, K.
\newblock Between an arena and a sports bar: Online chats of esports
  spectators.
\newblock \emph{CoRR}, abs/1801.02862, 2018.

\bibitem[Ng et~al.(2015)Ng, Hausknecht, Vijayanarasimhan, Vinyals, Monga, and
  Toderici]{ng_beyond_2015}
Ng, J.~Y., Hausknecht, M.~J., Vijayanarasimhan, S., Vinyals, O., Monga, R., and
  Toderici, G.
\newblock Beyond short snippets: Deep networks for video classification.
\newblock \emph{CoRR}, abs/1503.08909, 2015.

\bibitem[Prestridge \& Cox(2023)Prestridge and Cox]{prestridge2023play}
Prestridge, S. and Cox, D.
\newblock Play like a team in teams: A typology of online cognitive-social
  learning engagement.
\newblock \emph{Active Learning in Higher Education}, 24\penalty0 (1):\penalty0
  3--20, 2023.

\bibitem[Ringer \& Nicolaou()Ringer and Nicolaou]{unsupervised_highlight}
Ringer, C. and Nicolaou, M.~A.
\newblock Deep unsupervised multi-view detection of video game stream
  highlights.
\newblock URL \url{http://arxiv.org/abs/1807.09715}.

\bibitem[Ringer et~al.(2019)Ringer, Walker, and Nicolaou]{leagueoflegends}
Ringer, C., Walker, J.~A., and Nicolaou, M.~A.
\newblock Multimodal joint emotion and game context recognition in league of
  legends livestreams.
\newblock \emph{CoRR}, abs/1905.13694, 2019.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{two-stream_2014}
Simonyan, K. and Zisserman, A.
\newblock Two-stream convolutional networks for action recognition in videos.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.~D., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 27}, pp.\  568--576. Curran Associates, Inc., 2014.

\bibitem[Song(2016)]{yahoo_esports_2016}
Song, Y.
\newblock Real-time video highlights for yahoo esports.
\newblock \emph{CoRR}, abs/1611.08780, 2016.

\bibitem[Soomro et~al.(2012)Soomro, Zamir, and Shah]{soomro_ucf101:_2012}
Soomro, K., Zamir, A.~R., and Shah, M.
\newblock {UCF101:} {A} dataset of 101 human actions classes from videos in the
  wild.
\newblock \emph{CoRR}, abs/1212.0402, 2012.

\bibitem[Tran et~al.(2018)Tran, Wang, Torresani, Ray, LeCun, and
  Paluri]{r2plus1d}
Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., and Paluri, M.
\newblock A closer look at spatiotemporal convolutions for action recognition.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2018.

\bibitem[Tsai et~al.(2019)Tsai, Bai, Liang, Kolter, Morency, and
  Salakhutdinov]{tsai2019multimodal}
Tsai, Y.-H.~H., Bai, S., Liang, P.~P., Kolter, J.~Z., Morency, L.-P., and
  Salakhutdinov, R.
\newblock Multimodal transformer for unaligned multimodal language sequences.
\newblock \emph{arXiv preprint arXiv:1906.00295}, 2019.

\bibitem[Wang et~al.(2023)Wang, Guo, Shen, Ding, Liao, Fu, and
  Prabhakar]{wang2023integrity}
Wang, W., Guo, Y., Shen, C., Ding, S., Liao, G., Fu, H., and Prabhakar, P.~K.
\newblock Integrity and junkiness failure handling for embedding-based
  retrieval: A case study in social network search.
\newblock \emph{arXiv preprint arXiv:2304.09287}, 2023.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Miech, Sivic, Laptev, and
  Schmid]{yang2022tubedetr}
Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.
\newblock Tubedetr: Spatio-temporal video grounding with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  16442--16453, 2022{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Li, Dai, and Gao]{yang2022focal}
Yang, J., Li, C., Dai, X., and Gao, J.
\newblock Focal modulation networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 4203--4217, 2022{\natexlab{b}}.

\bibitem[Yang et~al.(2023)]{yang2023linguistically}
Yang, X. et~al.
\newblock Linguistically-inspired neural coreference resolution. advances in
  artificial intelligence and machine learning. 2023; 3 (2): 66, 2023.

\bibitem[Ye et~al.(2017)Ye, Liu, Yue, and Wang]{ye2017sparse}
Ye, W., Liu, X., Yue, T., and Wang, W.
\newblock A sparse graph-structured lasso mixed model for genetic association
  with confounding correction.
\newblock \emph{arXiv preprint arXiv:1711.04162}, 2017.

\bibitem[Yu et~al.(2023)Yu, Wang, Zeng, Liang, Bai, Chen, and
  Wang]{yu2023inkgan}
Yu, K., Wang, Y., Zeng, S., Liang, C., Bai, X., Chen, D., and Wang, W.
\newblock Inkgan: Generative adversarial networks for ink-and-wash style
  transfer of photographs.
\newblock 2023.

\bibitem[Yue \& Wang(2018)Yue and Wang]{yue2018deep}
Yue, T. and Wang, H.
\newblock Deep learning for genomics: A concise overview.
\newblock \emph{arXiv preprint arXiv:1802.00810}, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Negrinho, Ghosh, Jagannathan, Hassanzadeh,
  Schaaf, and Gormley]{zhang2021leveraging}
Zhang, L., Negrinho, R., Ghosh, A., Jagannathan, V., Hassanzadeh, H.~R.,
  Schaaf, T., and Gormley, M.~R.
\newblock Leveraging pretrained models for automatic summarization of
  doctor-patient conversations.
\newblock \emph{arXiv preprint arXiv:2109.12174}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Yu, and Zhu]{zhang2022emotional}
Zhang, S., Yu, H., and Zhu, G.
\newblock An emotional classification method of chinese short comment text
  based on electra.
\newblock \emph{Connection Science}, 34\penalty0 (1):\penalty0 254--273, 2022.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Gupta, Dai, Zhao, Srivastava,
  Jin, Featherston, Lai, Liu, Velasquez, Wang, and
  Zhang]{10.1145/3174243.3174255}
Zhou, Y., Gupta, U., Dai, S., Zhao, R., Srivastava, N., Jin, H., Featherston,
  J., Lai, Y.-H., Liu, G., Velasquez, G.~A., Wang, W., and Zhang, Z.
\newblock Rosetta: A realistic high-level synthesis benchmark suite for
  software programmable fpgas.
\newblock In \emph{Proceedings of the 2018 ACM/SIGDA International Symposium on
  Field-Programmable Gate Arrays}, FPGA '18, pp.\  269–278, New York, NY,
  USA, 2018{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9781450356145.
\newblock \doi{10.1145/3174243.3174255}.
\newblock URL \url{https://doi.org/10.1145/3174243.3174255}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Gupta, Dai, Zhao, Srivastava,
  Jin, Featherston, Lai, Liu, Velasquez, et~al.]{yuan2018rosetta}
Zhou, Y., Gupta, U., Dai, S., Zhao, R., Srivastava, N., Jin, H., Featherston,
  J., Lai, Y.-H., Liu, G., Velasquez, G.~A., et~al.
\newblock Rosetta: A realistic high-level synthesis benchmark suite for
  software-programmable fpgas. int’l symp. on field-programmable gate arrays
  (fpga)(feb 2018), 2018{\natexlab{b}}.

\end{thebibliography}
