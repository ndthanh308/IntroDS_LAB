%auto-ignore
\pdfoutput=1

\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Proofs}

\begin{proof}[Proof of Theorem \ref{thm:cons}]
Apply Theorem 5.9 of \citepref{vanderVaart1998}, wherein we let $\Psi_N(\theta) = \Psi_s(\theta)$. Note that this definition for $\Psi_N(\theta)$ differs from that provided in Section \ref{sec:intest}, which applies to the rest of this paper outside the current proof.
\end{proof}

\begin{proof}[Proof of Corollary \ref{crl:conspop}]
Let $\Psi(\theta)$ be the limit of $\Psi_N(\theta)$ and rearrange $\Psi_s(\theta)$ to give
\begin{equation*}
\Psi_s(\theta) = \Psi(\theta) + \frac{1}{N} \sum_{i=1}^N (w_i - 1) \psi(Y_i ; \theta) + (\Psi_N(\theta) - \Psi(\theta)).
\end{equation*}
The second and third terms are $o_P(1)$ by assumption, so $\Psi_s(\theta) \overset{P}{\to} \Psi(\theta)$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:normsup}]
Apply Theorem 3.3.1 of \citepref{vanderVaart1996}, noting that both $\Psi_s$ and $\Psi$ are defined on and take values in Euclidean space, so the paragraph preceding Theorem 3.3.1 applies.
\end{proof}

\begin{proof}[Proof of Corollary \ref{crl:normpop}]
Obtain \eqref{eqn:linpop} by subtracting $w$-weighted \eqref{eqn:linsup} by $\mathbf{1}$-weighted \eqref{eqn:linsup} as follows:
\begin{align*}
\sqrt{N}\dot{\Psi}_{\theta_0} (\hat{\theta}_s - \hat{\theta}_N) &= \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \theta_0) - \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_N - \theta_0) \\
&= -\sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0)) + o_P(1) + \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)) - o_P(1) \\
&= - \sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0)) + o_P(1).
\end{align*}
The continuous mapping theorem then gives \eqref{eqn:convdistprime}.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:desvar}]
To obtain \eqref{eqn:designvar}, first note that,
\begin{align*}
\mathrm{Var} & \left( \mathbb{E} \left[ \sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0)) \mid Y_{1:N} \right] \right)  \\
&= \mathrm{Var} \left( \sqrt{N}( \mathbb{E} \left[ \Psi_s(\theta_0) \mid Y_{1:N} \right] - \Psi_N(\theta_0)) \right) \\
&= \mathrm{Var} \left( \sqrt{N}(\Psi_N(\theta_0) - \Psi_N(\theta_0)) \right) \\
&= 0,
\end{align*}
then apply the law of total variance to $\mathrm{Var} \left( \sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0)) \right)$, conditioning on $Y_{1:N}$. By replacing ``$\Psi_N(\theta_0)$'' with ``$\Psi(\theta_0)$'', a similar argument leads to \eqref{eqn:uncondvar}, except that $\mathrm{Var} \left( \mathbb{E} \left[ \sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0)) \mid Y_{1:N} \right] \right) = N \mathrm{Var} \left( \Psi_N(\theta_0) \right)$.

To obtain \eqref{eqn:designvartheta} and \eqref{eqn:uncondvartheta}, first consider the remainders
\begin{align*}
r_s &= \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \theta_0) + \sqrt{N} (\Psi_s(\theta_0) - \Psi(\theta_0)), \\
r_N &= \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_N - \theta_0) + \sqrt{N} (\Psi_N(\theta_0) - \Psi(\theta_0)).
\end{align*}
The remainder in \eqref{eqn:linpop} is then given by $r^{\prime}_N = r_s - r_N$; see the display in the proof of Corollary \ref{crl:normpop} above. By the triangle inequality, each element $r_{N i}$ satisfies $\mathbb{E}[(r^{\prime}_{N i})^2]^{1/2} \leq \mathbb{E}[(r_{s i})^2]^{1/2} + \mathbb{E}[r^2_{N i}]^{1/2}$, so $\mathbb{E}[(r^{\prime}_{N i})^2] \to 0$ by the assumed convergence of $\mathbb{E}[(r_{s i})^2]$ and $\mathbb{E}[r^2_{N i}]$ and the continuity of $x \mapsto x^2$ and $x \mapsto x^{1/2}$.

We also have
\begin{align*}
\mathrm{Var}(\mathbb{E}[r_{N i} \mid Y_{1:N}]) &\leq \mathbb{E}[\mathbb{E}[r_{N i} \mid Y_{1:N}]^2] \\
&\leq \mathbb{E}[ \mathbb{E}[r^2_{N i} \mid Y_{1:N}] ] \\
&= \mathbb{E}[r^2_{N i}] \\
&= o(1),
\end{align*}
where the second inequality follows from Jensen's inequality and the first equality from the law of total expectation. The Cauchy-Schwarz inequality then gives
\begin{equation*}
\mathrm{Cov}(\mathbb{E}[r_{N i} \mid Y_{1:N}], \mathbb{E}[r_{N j} \mid Y_{1:N}])^2 \leq \mathrm{Var}(\mathbb{E}[r_{N i} \mid Y_{1:N}]) \mathrm{Var}(\mathbb{E}[r_{N j} \mid Y_{1:N}]) = o(1),
\end{equation*}
so that
\begin{equation*}
\mathrm{Var}(\mathbb{E}[r_N \mid Y_{1:N}]) = o(1).
\end{equation*}
The above also applies to $r_s$ and $r^{\prime}_N$ in place of $r_N$.

In pursuit of \eqref{eqn:designvartheta}, it follows that
\begin{align*}
\mathrm{Var} & \left( \mathbb{E} \left[ \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \hat{\theta}_N) \mid Y_{1:N} \right] \right) \\
&= \mathrm{Var} \left( \mathbb{E} \left[ - \sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0)) + r^{\prime}_N \mid Y_{1:N} \right] \right) \\
&= \mathrm{Var} \left( - \sqrt{N} \left( \mathbb{E} \left[ \Psi_s(\theta_0) \mid Y_{1:N} \right] - \Psi_N(\theta_0) \right) + \mathbb{E}[r^{\prime}_N \mid Y_{1:N}] \right) \\
&= \mathrm{Var} \left( \mathbb{E}[r^{\prime}_N \mid Y_{1:N}] \right) \\
&= o(1).
\end{align*}
Now apply of the law of total variance to $\sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \hat{\theta}_N)$, premultiply by $\dot{\Psi}_{\theta_0}^{-1}$ and postmultiply by $(\dot{\Psi}_{\theta_0}^{-1})^T$, which gives \eqref{eqn:designvartheta} in view of the continuous mapping theorem.

To obtain \eqref{eqn:uncondvartheta}, first note that by the Cauchy-Schwarz inequality,
\begin{align*}
\mathrm{Cov} & \left( \sqrt{N}(\Psi_N(\theta_0)_i - \Psi(\theta_0)_i), \mathbb{E}[r_{s j} \mid Y_{1:N}] \right)^2 \\
&\leq \mathrm{Var} \left( \sqrt{N}(\Psi_N(\theta_0)_i - \Psi(\theta_0)_i) \right) \mathrm{Var} \left( \mathbb{E}[r_{s j} \mid Y_{1:N}] \right) \\
&= o(1),
\end{align*}
where convergence (and hence asymptotic boundedness) of the first variance follows after applying the asymptotic uniform integrability and convergence in distribution of its contents squared to Theorem 2.20 of \citepref{vanderVaart1998}. Thus
\begin{equation*}
\mathrm{Cov}\left( \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)), \mathbb{E}[r_s \mid Y_{1:N}] \right) = o(1).
\end{equation*}
It follows that
\begin{align}
\mathrm{Var} & \left( \mathbb{E} \left[ \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \theta_0) \mid Y_{1:N} \right] \right) \nonumber \\
=& \mathrm{Var} \left( \mathbb{E} \left[ - \sqrt{N} (\Psi_s(\theta_0) - \Psi(\theta_0)) + r_s \mid Y_{1:N} \right] \right) \nonumber \\
=& \mathrm{Var} \left( - \sqrt{N}(\mathbb{E}[\Psi_s(\theta_0) \mid Y_{1:N}] - \Psi(\theta_0)) + \mathbb{E}[r_s \mid Y_{1:N}] \right) \nonumber \\
=& \mathrm{Var} \left( - \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)) + \mathbb{E}[r_s \mid Y_{1:N}] \right) \nonumber \\
=& \mathrm{Var} \left(  - \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)) \right) + \mathrm{Var} \left( \mathbb{E}[r_s \mid Y_{1:N}] \right) \nonumber \\
&- \mathrm{Cov} \left( \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)), \mathbb{E}[r_s \mid Y_{1:N}] \right) \nonumber \\
&- \mathrm{Cov} \left(\mathbb{E}[r_s \mid Y_{1:N}], \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)) \right) \nonumber \\
=& \mathrm{Var} \left(  - \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)) \right) + o(1). \label{eqn:expcondequncond}
\end{align}
Applying the above with $w_i = 1$ gives
\begin{equation*}
\mathrm{Var} \left( \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_N - \theta_0) \right) = \mathrm{Var} \left( - \sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0)) \right) + o(1),
\end{equation*}
and by substituting the left-hand side of the above into the right-hand side of \eqref{eqn:expcondequncond}, we obtain
\begin{equation*}
\mathrm{Var} \left( \mathbb{E} \left[ \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \theta_0) \mid Y_{1:N} \right] \right) = \mathrm{Var} \left(  \sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_N - \theta_0) \right) + o(1).
\end{equation*}
Now apply the law of total variance to $\sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \theta_0)$, premultiply by $\dot{\Psi}_{\theta_0}^{-1}$ and postmultiply by $(\dot{\Psi}_{\theta_0}^{-1})^T$, which gives \eqref{eqn:uncondvartheta} in view of the continuous mapping theorem.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:normweights}]
If the first equality holds, then
\begin{equation*}
\frac{1}{N} \sum_{i=1}^N w^{\prime}_i \psi(Y_i; \hat{\vartheta}_N) = \frac{\frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i; \hat{\vartheta}_N)}{\frac{1}{N} \sum_{j=1}^N w_j} = \frac{o_P(r_N)}{1 + o_P(1)} = o_P(r_N),
\end{equation*}
and we obtain the second equality. If the second equality holds, then
\begin{equation*}
\frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i; \hat{\vartheta}_N) = \left( \frac{1}{N} \sum_{j=1}^N w_j \right) \left( \frac{1}{N} \sum_{i=1}^N w^{\prime}_j \psi(Y_i ; \hat{\vartheta}_N) \right) = (1 + o_P(1)) o_P(r_N) = o_P(r_N),
\end{equation*}
and we obtain the first equality.
\end{proof}

\if0\wholepaper {
	\bibliographypref{../library}
} \fi

\end{document}