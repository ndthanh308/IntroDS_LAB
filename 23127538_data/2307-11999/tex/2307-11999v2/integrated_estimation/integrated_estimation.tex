%auto-ignore
\pdfoutput=1

\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Integrated Estimation} \label{sec:intest}

Suppose we are interested in a $d$-dimensional column vector of population parameters $\hat{\theta}_N$ that is a function of a finite population $Y_1, Y_2, \ldots, Y_N$ of $N$ $k$-dimensional column vectors. We will assume that there exists a vector-valued function $\psi$ such that the population estimating equation
\begin{equation*}
\Psi_N(\theta) \coloneqq \frac{1}{N} \sum_{i=1}^N \psi(Y_i ; \theta)
\end{equation*}
is close to zero if and only if $\theta$ is close to $\hat{\theta}_N$.\footnote{The precise mathematical meaning of ``close to'' depends on what we are trying to prove; see Theorems \ref{thm:cons} and \ref{thm:normsup}.} There is such an equation for many different population parameters, including the mean ($\psi(y ; \theta) = y - \theta$), the median ($\psi(y ; \theta) = I(y < \theta) - I(y > \theta)$; see Section \ref{eg:quant}), the Gini index, linear regression coefficients and those produced via maximum likelihood estimation (see Sections \ref{eg:gini}, \ref{eg:lr} and \ref{eg:mle}, respectively).

In official statistics obtaining the entire population is often infeasible. In this case, we must contend with a sample statistic $\hat{\theta}_s$, defined so that $\Psi_s(\theta) \approx 0$ if and only if the function argument satisfies $\theta \approx \hat{\theta}_s$, where
\begin{equation}
\Psi_s(\theta) \coloneqq \frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i ; \theta) \label{eqn:weightedzfunction}
\end{equation}
is the sample estimating equation for a given sequence of weights $w_1, w_2, \ldots, w_N$. The sample $s = \{i : I(w_i \neq 0)\}$ contains indices identifying observed units; zero-weighted units do not appear in \eqref{eqn:weightedzfunction}, and we also assume that they do not influence $\hat{\theta}_s$ and therefore do not require observation. The sample mean and linear regression coefficients can be defined by first defining $\psi$, then setting the sample estimating equation in \eqref{eqn:weightedzfunction} to zero and solving for $\theta$. On the other hand, the sample median and Gini index are usually defined in some other way, and an appropriate $\psi$ must be found after. The latter case is not easily accommodated by standard references such as \citepref{Godambe1984}. See Section \ref{sec:eg} for details.

%In official statistics obtaining the entire population is often infeasible. In this case, we must contend with a weighted estimator $\hat{\theta}^w_N$, defined so that $\Psi^w_N(\theta) \approx 0$ if and only if the function argument satisfies $\theta \approx \hat{\theta}^w_N$, where
%\begin{equation}
%\Psi^w_N(\theta) \coloneqq \frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i ; \theta) \label{eqn:weightedzfunction}
%\end{equation}
%and $w = (w_1, w_2, \ldots, w_N)$ is a vector of weights. The weighted mean and linear regression coefficients can be defined by first defining $\psi$, then setting the weighted estimating equation \eqref{eqn:weightedzfunction} to zero and solving for $\theta$. On the other hand, the weighted median and Gini index are usually defined in some other way, and an appropriate $\psi$ must be found after. See Section \ref{sec:eg} for details. Note that zero-weighted units do not appear in \eqref{eqn:weightedzfunction}, and we also assume that they do not influence $\hat{\theta}^w_N$ and therefore do not require observation.

If a probability survey is conducted, let the sample membership indicator $\alpha_i$ equal one or zero according to whether or not unit $i$ is observed, and let $\pi_i = \mathrm{Pr}(\alpha_i = 1 \mid Y_{1:N})$ be the first-order inclusion probability, where $Y_{1:N} = (Y_1, Y_2, \ldots, Y_N)$ contains all study variables for the entire population. Then the Horvitz-Thompson weights $w^{HT}_i = \alpha_i \pi_i^{-1}$ are a standard choice; see Chapter 2.8 of \citepref{Sarndal1992}. Let $\Psi^{HT}_s$ be the sample estimating equation obtained after substituting $w_i = w^{HT}_i$ into \eqref{eqn:weightedzfunction}. Since $\mathbb{E}[w_i^{HT} \mid Y_{1:N}] = \mathbb{E}[\alpha_i \mid Y_{1:N}] \pi_i^{-1} = \pi_i \pi_i^{-1} = 1$, $\Psi^{HT}_s(\theta)$ is \textit{design unbiased}, meaning that
\begin{equation}
\mathbb{E}[\Psi^{HT}_s(\theta) \mid Y_{1:N}] = \frac{1}{N} \sum_{i=1}^N \mathbb{E}[w^{HT}_i \mid Y_{1:N}] \psi(Y_i ; \theta) = \Psi_N(\theta). \label{eqn:htunbiased}
\end{equation} We will see in subsequent sections that design-unbiasedness of the sample estimating equation helps ensure that the sample estimator $\hat{\theta}_s$ is close to its population counterpart, $\hat{\theta}_N$. We also rely on design-unbiasedness for some aspects of variance estimation.

%where we simplify notation to let $\Psi^{HT}_N(\theta) = \Psi^{w^{HT}}_N(\theta)$.

Now suppose that, in addition to the probability survey, we have access to a big data set with an unknown selection mechanism that is produced and maintained outside of an NSO for non-statistical purposes. Let $\delta_i$ equal one or zero according to whether or not $Y_i$ is observed through the big data set. If the big data is used naively and we adopt $\delta_1, \delta_2, \ldots, \delta_N$ as our weights, then it is likely that $\mathbb{E}[\delta_i \mid Y_{1:N}] \neq 1$ and in view of \eqref{eqn:htunbiased} $\Psi_s(\theta)$ is not necessarily design unbiased. On the other hand, if we follow \citepref{Kim2021} to adopt the Data Integrated weights $w^{DI}_i = \delta_i + (1 - \delta_i) w^{HT}_i$, then
\begin{equation}
\mathbb{E}[w^{DI}_i \mid Y_{1:N}] = \delta_i + (1 - \delta_i) \mathbb{E}[w^{HT}_i \mid Y_{1:N}] = \delta_i + 1 - \delta_i = 1, \label{eqn:diunbiased}
\end{equation}
where we extend the vector $Y_i$ so that its final element is equal to $\delta_i$. This leads us to condition on $\delta_{1:N} = (\delta_1, \ldots, \delta_N)$ and justifies the first equality in \eqref{eqn:diunbiased} above. Letting $\Psi^{DI}_s(\theta)$ denote the sample estimating equation for weights $w^{DI}_i$, we have $\mathbb{E}[\Psi^{DI}_s(\theta) \mid Y_{1:N}] = \Psi_N(\theta)$, and design-unbiasedness is retained. Notice that $\mathbb{E}[w^{DI}_i \mid Y_{1:N}] = 1$ because $\mathbb{E}[w^{HT}_i \mid Y_{1:N}] = 1$, and therefore design unbiasedness does not rely on any other characteristic of the Horvitz-Thompson weights specifically. As a result, we will from now on make use of the following generalisation of the weights of \citepref{Kim2021}:
\begin{equation}
w^{DI}_i = \delta_i + (1 - \delta_i) w_i, \label{eqn:wdi}
\end{equation}
where $w_i$ generically represents any set of weights satisfying $\mathbb{E}[w_i \mid Y_{1:N}] = 1$ (note that the notation $w_i$ can therefore be used to represent $w^{DI}_i$ itself). In a slight abuse of terminology, we will say that weights with this property are design-unbiased too, since design-unbiased weights lead to a design-unbiased sample estimating equation.

The generalisation of the integrated weights given in \eqref{eqn:wdi}  makes the integrated estimator $\hat{\theta}^{DI}_s$ applicable to survey weights that are produced by making adjustments to the Horvitz-Thompson weights, for example to account for survey nonresponse (e.g.\ \citealppref{Brick2013}). In Section \ref{eg:diweights} we show that $\hat{\theta}^{DI}_s$ is asymptotically unbiased, and that if the dependence between weights $w_i$ (induced by without-replacement sampling, say) is small enough, the integrated estimator $\hat{\theta}^{DI}_s$ has a smaller variance than the estimator $\hat{\theta}_s$ produced without the big data.

\if0\wholepaper {
	\bibliographypref{../library}
} \fi

\end{document}