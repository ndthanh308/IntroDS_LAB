%auto-ignore
\pdfoutput=1

\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Theoretical Results} \label{sec:theory}

\subsection{Overview} \label{subsec:theoryoverview}

In the design-based survey framework the population is fixed but unknown and all randomness is attributed to the weights (e.g.\ Chapter 2 of \citealppref{Sarndal1992}). As a result, the design-based variance of $\hat{\theta}_s$ reflects uncertainty related to the survey design, and only the survey design. This is appropriate when choosing between competing designs, where the aim is usually to minimise the design-based variance subject to constraints (for example, to allocate sample to strata as in Chapter 3.7.3 of \citealppref{Sarndal1992}), and we will see in Section \ref{sec:varest} that for large population and sample sizes the sample design minimising the design variance almost minimises the joint variance. By treating the target variables as nonrandom, we also avoid making assumptions that are unnecessary for the purposes of survey design, which can therefore proceed without the need for special expertise regarding the nature and behaviour of the target variables.

When working with nonlinear statistics, however, the design-based framework alone is insufficient to develop the kinds of asymptotic results provided in Section \ref{sec:cons} that justify the use of $\hat{\theta}_s$ to approximate $\hat{\theta}_N$ (nonlinear statistics are often biased). On its own, the design-based framework is also insufficient to develop the kinds of asymptotic results provided in Sections \ref{sec:normality} and \ref{sec:varest} that justify the use of design-based variance estimators to compare survey designs that aim to produce an accurate $\hat{\theta}_s$ in a cost effective manner. Further, scientific inquiry often seeks a $p$ value measuring the strength of the evidence in survey results against a null hypothesis related to some aspect of a presumed superpopulation model that is postulated to generate the population (e.g.\ \citealppref{Wooldridge1999, Wooldridge2001, Wooldridge2002, Bhattacharya2007}). In this scenario, randomness from \textit{both} the design \textit{and} the superpopulation need to be quantified in order to produce the $p$ value and conduct the hypothesis test, and here too a purely design-based approach is insufficient.

To resolve this apparent contradiction and cater to both uses of variance estimation, we will assume that there is some unknown superpopulation model that generates the population, but in our asymptotic results we rely only on weak assumptions that place little restriction on the form that the superpopulation model might take. Design-based expectations and variances are recovered using the law of total expectation by conditioning on the population $Y_{1:N}$, as seen in Section \ref{sec:intest} above. We also show in Section \ref{sec:varest} that if the weights are design unbiased, then the design variance of $\hat{\theta}_s$ is well approximated by the joint variance of $\hat{\theta}_s - \hat{\theta}_N$, which follows from a central limit theorem (presented in Section \ref{sec:normality}) derived under standard assumptions due to standard results in large-sample asymptotics (though `large-population' might be a more appropriate term in our context). Additionally, we can obtain an estimate of the joint variance of $\hat{\theta}_s$ simply by adding a design-based variance estimate for $\hat{\theta}_s$ to an estimate of the variance of $\hat{\theta}_N$ under the superpopulation. These results apply as $N$ approaches infinity for a given, fixed sampling fraction, and in practice we use the approximation that applies to the sampling fraction of the design at hand.

When producing a weighted average, one needs to decide whether or not to normalise the weights so that their average is equal to one. In Section \ref{sec:normweights}, we show that if $\hat{\theta}_s$ is constructed using the weights $w$ and the function $\psi$ so that $\Psi_s(\hat{\theta}_s) \approx 0$ (as it is for linear regression and maximum likelihood estimation; see Section \ref{eg:lr} and \ref{eg:mle}, respectively), then the normalisation (or not) of the weights has no impact on the asymptotic behaviour of $\hat{\theta}_s$ subject to the sufficient conditions of the preceding subsections. Note that this is not necessarily true if $\hat{\theta}_s$ is constructed using the weights in some other way. Quantiles for example should be produced using normalised weights; see Section \ref{eg:quant}. On the other hand, even for estimators that should be produced with normalised weights, we can use either normalised or unnormalised weights to construct $\Psi_s$.

Proofs for all results are provided in the supplementary appendix.

\subsection{Consistency} \label{sec:cons}

The following theorem establishes conditions ensuring that the sample estimator $\hat{\theta}_s$ converges in probability to a fixed $\theta_0$ as $N \to \infty$ under the joint distribution spanning both the design and superpopulation. In scientific inquiry $\theta_0$ often represents a key characteristic of the superpopulation being studied. If $w_i = 1$ then $\hat{\theta}_N =\hat{\theta}_s$, so this theorem can also be applied to show that $\hat{\theta}_N$ converges to $\theta_0$.

\begin{theorem} \label{thm:cons}
Assume that $\hat{\theta}_s$ satisfies
\begin{equation}
\Psi_s(\hat{\theta}_s) \overset{P}{\to} 0. \label{eqn:conszest}
\end{equation}
Also suppose there exists a fixed parameter vector $\theta_0$ and a fixed vector-valued function $\Psi$ such that for all $\epsilon > 0$,
\begin{gather}
\sup_{\theta} \left\lVert \Psi_s(\theta) - \Psi(\theta) \right\rVert \overset{P}{\to} 0, \label{eqn:consulln} \\
\inf_{\theta : \lVert \theta - \theta_0 \rVert > \epsilon} \lVert \Psi(\theta) \rVert > 0 = \lVert \Psi(\theta_0) \rVert. \label{eqn:consident}
\end{gather}
Then $\hat{\theta}_s$ converges in probability to $\theta_0$.
\end{theorem}

\begin{remark} \label{rmk:changingweights}
Note that each weight $w_i$ is permitted to vary with $N$, even though this relationship is suppressed in the notation. This is necessary for Theorem \ref{thm:cons} to be applicable to simple random sampling without replacement, which is a popular sampling scheme covered in Section \ref{eg:srswor}. To see this, consider taking such a sample of size $n = 5$ from $Y_1, Y_2, \ldots, Y_{10}$ ($N = 10$) and of size $n = 10$ from $Y_1, Y_2, \ldots, Y_{20}$ ($N = 20$). Recall that a unit $i$ is selected if and only if $w_i \neq 0$. If $w_i$ were not permitted to vary with $N$, then the second sample would certainly contain the first sample. This is false if simple random sampling without replacement is used to draw the second sample.
\end{remark}

\begin{remark} \label{rmk:thetadef}
When establishing consistency, \eqref{eqn:conszest} is the formal counterpart to the informal statement $\Psi_s(\hat{\theta}_s) \approx 0$. This assumption must be explicitly established in order to correctly define $\psi$ and $\theta_0$, if desired. Defining $\psi$ takes on a greater role in variance estimation, which we discuss the next subsection.
\end{remark}

\begin{remark}
The uniform law of large numbers in \eqref{eqn:consulln} extends the standard, pointwise law of large numbers $\Psi_s(\theta) \overset{P}{\to} \Psi(\theta)$ to ensure that the error has an upper bound across $\theta$. If this holds, the set of random variables obtainable by evaluating $w_i \psi(Y_i ; \theta)$ at a given $\theta$ is said to be a \textit{Glivenko-Cantelli class}; see Chapter 2.1 of \citepref{vanderVaart1996}. Alternatively, Chapter 22.5 of \citepref{Davidson2021} covers uniform laws of large numbers in a way that better accommodates dependence between both weights $w_i$ and observations $Y_i$. If $Y_i$ is independent and identically distributed (i.i.d.), then for any reasonable choice of weights, $\Psi_s(\theta) \overset{P}{\to} \mathbb{E}[\psi(Y_i ; \theta)]$ if the expectation exists. Therefore $\Psi(\theta) = \mathbb{E}[\psi(Y_i ; \theta)]$, which we can often use to obtain a workable expression for $\Psi(\theta)$ that can be used for variance estimation. This is illustrated in Section \ref{eg:stats}.
\end{remark}

\begin{remark}
The assumption given in $\eqref{eqn:consident}$ ensures that there is only one $\theta_0$ satisfying $\Psi(\theta_0) = 0$ to which $\hat{\theta}_s$ might converge, and that $\Psi(\theta)$ being close to zero implies that $\theta$ is close to $\theta_0$. Without this requirement, a $\hat{\theta}_s$ satisfying $\Psi_s(\hat{\theta}_s) \approx 0$ might be close to any number of dispersed $\theta$ satisfying $\Psi(\theta) \approx 0$, making it difficult to show convergence. In econometrics this requirement is often called \textit{identification}; see \citepref{Lewbel2019} for a review.
\end{remark}

In the corollary below, we show that only mild assumptions are required for the population estimator $\hat{\theta}_N$ to be well-approximated by the sample estimator $\hat{\theta}_s$. This interpretation is better-suited to survey design problems than it is to scientific inquiry, and doesn't require any knowledge of the form that $\theta_0$ or the superpopulation might take. 

\begin{corollary} \label{crl:conspop}
Suppose that the assumptions of Theorem \ref{thm:cons} are satisfied for the weights $w_1, w_2, \ldots, w_N$ and $1, 1, \ldots, 1$. If
\begin{equation}
\frac{1}{N} \sum_{i=1}^N (w_i - 1) \psi(Y_i ; \theta) \overset{P}{\to} 0 \label{eqn:llnwm1}
\end{equation}
for all $\theta$, then $\Psi_s(\theta)$ and $\Psi_N(\theta)$ both converge to the same $\Psi(\theta)$, and $\hat{\theta}_s$ and $\hat{\theta}_N$ both converge to the same $\theta_0$.
\end{corollary}

\begin{remark}
If the weights $w_1, w_2, \ldots, w_N$ are design-unbiased and $\mathbb{E}[\psi(Y_i ; \theta)]$ exists, then $\mathbb{E}[\frac{1}{N} \sum_{i=1}^N (w_i - 1) \psi(Y_i ; \theta)] = 0$, and a law of large numbers alongside asymptotic uniform integrability of the average (a technical condition; see Section 2.5 of \citealppref{vanderVaart1998}) gives \eqref{eqn:llnwm1}.
\end{remark}

\subsection{Asymptotic Normality} \label{sec:normality}

The following is a central limit theorem for $\sqrt{N}(\hat{\theta}_s - \theta_0)$ to converge in distribution, usually to a zero-mean normal distribution, as $N \to \infty$ under the joint distribution spanning both the design and superpopulation. It is most useful when conducting hypothesis tests and producing confidence intervals as part of a scientific inquiry. Recall that if $w_i = 1$ then $\hat{\theta}_N = \hat{\theta}_s$, so this theorem can also be applied to obtain the asymptotic distribution of $\hat{\theta}_N$.

\begin{theorem} \label{thm:normsup}
Assume that $\hat{\theta}_s \overset{P}{\to} \theta_0$ for some fixed $\theta_0$, and $\Psi_s(\hat{\theta}_s) = o_P(N^{-1/2})$. Suppose there exists a fixed vector-valued function $\Psi$ such that $\Psi(\theta_0) = 0$, with a continuously invertible Jacobian matrix $\dot{\Psi}_{\theta_0}$ at $\theta_0$, and
\begin{equation}
\sqrt{N}(\Psi_s(\hat{\theta}_s) - \Psi(\hat{\theta}_s)) - \sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0)) = o_P(1 + \sqrt{N} \lVert \hat{\theta}_s - \theta_0 \rVert ). \label{eqn:donsker}
\end{equation}
Then we have
\begin{equation}
\sqrt{N} \dot{\Psi}_{\theta_0} (\hat{\theta}_s - \theta_0) = -\sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0)) + o_P(1). \label{eqn:linsup}
\end{equation}
If we also have that $\sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0)) \Rightarrow Z_s$ for some fixed random variable $Z_s$, then
\begin{equation}
\sqrt{N}(\hat{\theta}_s - \theta_0) \Rightarrow \dot{\Psi}_{\theta_0}^{-1} Z_s. \label{eqn:zestlin}
\end{equation}
\end{theorem}

\begin{remark}
In this theorem, $\Psi_s(\hat{\theta}_s) = o_P(N^{-1/2})$ is the formal counterpart to the informal statement $\Psi_s(\hat{\theta}_s) \approx 0$. This is a stronger requirement than \eqref{eqn:conszest}, which is assumed in Theorem \ref{thm:cons} to establish consistency; see Remark \ref{rmk:thetadef}. It is important to explicitly establish this assumption in order to correctly define $\psi$, which is needed for variance estimation later in this section and in the examples of Section \ref{sec:eg}.
\end{remark}

\begin{remark}
Note again that each weight $w_i$ is permitted to vary with $N$, even though this relationship is suppressed in the notation; see Remark \ref{rmk:changingweights} for further comments.
\end{remark}

\begin{remark} \label{rmk:clt}
Because $\Psi_s(\theta_0)$ is an average, $Z_s$ will be a zero-mean normally-distributed random variable provided that the weights $w_i$ and observations $Y_i$ are not too dependent on too many of their peers; see Chapters 20, 21 and 25 of \citepref{Davidson2021}. This is the scenario usually encountered in practice.
\end{remark}

\begin{remark}
The assumption given in \eqref{eqn:donsker} says that if $\hat{\theta}_s$ is close to $\theta_0$ (as it will be for large $N$), then $\sqrt{N} ( \Psi_s(\hat{\theta}_s) - \Psi(\hat{\theta}_s) )$ will be close to $\sqrt{N} (\Psi_s(\theta_0) - \Psi(\theta_0))$. Said another way, we require that the random function $\theta \mapsto \sqrt{N} ( \Psi_s(\theta) - \Psi(\theta) )$ possess a kind of continuity at $\theta = \theta_0$ that does not break down as $N \to \infty$, and \textit{stochastic equicontinuity} suffices; see Chapter 22.3 of \citepref{Davidson2021}. The set of random variables obtainable by evaluating $w_i \psi(Y_i ; \theta)$ at a given $\theta$ is said to be a \textit{Donsker class} if their variances have an upper bound and $\theta \mapsto \sqrt{N} ( \Psi_s(\theta) - \Psi(\theta) )$ is stochastically equicontinuous; see Chapter 2.1 of \citepref{vanderVaart1996}. By Lemma 3.3.5 of the same, if $(w_i, Y_i)$ is i.i.d.\ and $w_i \psi(Y_i ; \theta)$ is Donsker, then \eqref{eqn:donsker} is satisfied. To obtain \eqref{eqn:donsker} while accommodating dependence, combine a functional central limit theorem from Chapter 31 of \citepref{Davidson2021} with Addendum 1.5.8 of \citepref{vanderVaart1996}.
\end{remark}

This next corollary is a central limit theorem for $\sqrt{N}(\hat{\theta}_s - \hat{\theta}_N)$ to converge in distribution as $N \to \infty$ under the joint distribution spanning both the design and superpopulation. The asymptotic distribution is usually normal with zero mean. In the theorem after that, we will show that for design-unbiased weights, the joint variance of $\hat{\theta}_s - \hat{\theta}_N$ is close to the expected design variance of $\hat{\theta}_s$ if $N$ is large. As a result, this central limit theorem provides expressions for the asymptotic design variance that can be used to develop design-based variance estimators for $\hat{\theta}_s$.

\begin{corollary} \label{crl:normpop}
Suppose that the assumptions of Corollary \ref{crl:conspop} and Theorem \ref{thm:normsup} are satisfied for weights $w_1, w_2, \ldots, w_N$ and $1, 1, \ldots, 1$. Then we have
\begin{equation}
\sqrt{N}\dot{\Psi}_{\theta_0} (\hat{\theta}_s - \hat{\theta}_N) = - \sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0)) + o_P(1). \label{eqn:linpop}
\end{equation}
If we also have that $\sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0)) \Rightarrow Z^{\prime}$ for some distribution $Z^{\prime}$, then
\begin{equation}
\sqrt{N} (\hat{\theta}_s - \hat{\theta}_N) \Rightarrow \dot{\Psi}_{\theta_0}^{-1} Z^{\prime}. \label{eqn:convdistprime}
\end{equation}
\end{corollary}

\begin{remark}
Because $(\Psi_s(\theta_0), \Psi_N(\theta_0))$ is an average, it is typically asymptotically normal; see Remark \ref{rmk:clt}. Now apply the delta method (e.g.\ Theorem 3.1 of \citealppref{vanderVaart1998}) to obtain zero-mean asymptotic normality of $\sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0))$.
\end{remark}

\subsection{Variance Estimation} \label{sec:varest}

The following theorem establishes the relationship between the design and joint variances of $\hat{\theta}_s$ for design-unbiased weights. We show in \eqref{eqn:designvar} and \eqref{eqn:designvartheta} that the joint variances of $\sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0))$ and $\sqrt{N}(\hat{\theta}_s - \hat{\theta}_N)$ are equal and close to the expectation of their design-based counterparts, respectively. Combined with the central limit theorem in Corollary \ref{crl:normpop} above, this provides expressions for the asymptotic design variance that can be used to develop design-based variance estimators for $\hat{\theta}_s$, which is most useful for official statistics applications like survey design. More helpful for survey users conducting scientific inquiry, we also decompose in \eqref{eqn:uncondvar} and \eqref{eqn:uncondvartheta} the joint variance of $\sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0))$ and $\sqrt{N}(\hat{\theta}_s - \theta_0)$, respectively, into a sum of: 1) the superpopulation expectation of their respective design variances, 2) the superpopulation variance of their respective population counterparts, and 3) a remainder term that is zero for $\Psi_s(\theta_0)$ and small for $\hat{\theta}_s$. This is helpful for scientific inquiry because it allows us to combine estimates of the design-based variance of $\hat{\theta}_s$ with estimates of the superpopulation variance of $\hat{\theta}_N$ to conduct statistical inference about $\theta_0$ as estimated by $\hat{\theta}_s$, taking into account uncertainty and variability from \textit{both} the survey design \textit{and} the study variables.

\begin{theorem} \label{thm:desvar}
Suppose that $\mathbb{E}[\Psi_s(\theta_0) \mid Y_{1:N}] = \Psi_N(\theta_0)$ for all $N$. Then
\begin{small}
\begin{align}
\mathrm{Var} \left( \sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0)) \right) &= N \mathbb{E} \left[ \mathrm{Var} \left( \Psi_s(\theta_0) \mid Y_{1:N} \right) \right], \label{eqn:designvar} \\
\mathrm{Var} \left( \sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0)) \right) &= N \mathbb{E} \left[ \mathrm{Var} \left( \Psi_s(\theta_0) \mid Y_{1:N} \right) \right] + N \mathrm{Var} \left( \Psi_N(\theta_0) \right). \label{eqn:uncondvar}
\end{align}
\end{small}
If we also satisfy the assumptions of Corollary \ref{crl:normpop} such that the second moment of every element of the $o_P(1)$ remainder in \eqref{eqn:linsup} converges to zero for weights $w_1, w_2, \ldots, w_N$ and $1, 1, \ldots, 1$, then
\begin{equation}
\mathrm{Var} \left( \sqrt{N}(\hat{\theta}_s - \hat{\theta}_N) \right) = N \mathbb{E} \left[ \mathrm{Var}(\hat{\theta}_s \mid Y_{1:N}) \right] + o(1). \label{eqn:designvartheta}
\end{equation}
Also assuming that the square of every element of $\sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0))$ is asymptotically uniformly integrable (in the sense of Chapter 2.5 of \citealppref{vanderVaart1998}) gives
\begin{equation}
\mathrm{Var} \left( \sqrt{N}(\hat{\theta}_s - \theta_0) \right) = N \mathbb{E} \left[ \mathrm{Var}(\hat{\theta}_s \mid Y_{1:N}) \right] + \mathrm{Var} \left( \sqrt{N}(\hat{\theta}_N - \theta_0) \right) + o(1). \label{eqn:uncondvartheta}
\end{equation}
\end{theorem}

In view of \eqref{eqn:convdistprime}, \eqref{eqn:designvar} and \eqref{eqn:designvartheta}, the asymptotic design variance is given by
\begin{equation}
N \mathbb{E} \left[ \mathrm{Var}(\hat{\theta}_s \mid Y_{1:N}) \right] \to \dot{\Psi}_{\theta_0}^{-1} V^{\prime} (\dot{\Psi}_{\theta_0}^{-1})^{T}, \label{eqn:asydesignvar}
\end{equation}
where $V^{\prime}$ is the asymptotic variance of $\sqrt{N}(\Psi_s(\theta_0) - \Psi_N(\theta_0))$. Further, in view of \eqref{eqn:zestlin}, \eqref{eqn:uncondvar} and \eqref{eqn:uncondvartheta}, the asymptotic joint variance is given by
\begin{equation}
N \mathrm{Var}(\hat{\theta}_s) \to \dot{\Psi}_{\theta_0}^{-1} V_s (\dot{\Psi}_{\theta_0}^{-1})^T = \dot{\Psi}_{\theta_0}^{-1} (V^{\prime} + V) (\dot{\Psi}_{\theta_0}^{-1})^T, \label{eqn:asyuncondvar}
\end{equation}
where $V_s$ is the asymptotic variance of $\sqrt{N}(\Psi_s(\theta_0) - \Psi(\theta_0))$ and $V$ is the asymptotic variance of $\sqrt{N}(\Psi_N(\theta_0) - \Psi(\theta_0))$.

Given consistent estimators $\hat{\dot{\Psi}}_{\theta_0} \overset{P}{\to} \dot{\Psi}_{\theta_0}$ and $\hat{V}^{\prime} \equiv \widehat{\mathrm{Var}}(\sqrt{N} \Psi_s(\theta_0) \mid Y_{1:N}) \overset{P}{\to} V^{\prime}$, the continuous mapping theorem alongside \eqref{eqn:asydesignvar} suggests the following design-based variance estimator for $\hat{\theta}_s$:
\begin{equation}
\widehat{\mathrm{Var}}(\hat{\theta}_s \mid Y_{1:N}) = \frac{1}{N} \hat{\dot{\Psi}}_{\theta_0}^{-1} \hat{V}^{\prime} (\hat{\dot{\Psi}}_{\theta_0}^{-1})^{T}. \label{eqn:desvarest}
\end{equation}
In most (but not all) cases, $\dot{\Psi}_{\theta_0}$ can be consistently estimated by
\begin{equation}
\hat{\dot{\Psi}}_{\theta_0} = \frac{1}{N} \sum_{i = 1}^N w_i \dot{\psi}(Y_i ; \hat{\theta}_s), \label{eqn:dotpsiest}
\end{equation}
where the $\dot{\psi}(y ; \theta)$ is the Jacobian matrix of $\dot{\psi}(y ; \theta)$ with respect to $\theta$. This usually works if the Jacobian matrix of $\mathbb{E}[\psi(Y_i ; \theta)]$ equals $\mathbb{E}[\dot{\psi}(Y_i; \theta)]$ (i.e.\ if we can `move the derivative inside the expectation'). Quantiles are one notable case where this condition fails; see Section \ref{eg:quant}. In contrast to $\hat{\dot{\Psi}}_{\theta_0}$, expressions for $\hat{V}^{\prime}$ usually vary considerably depending on the sample design used. Fortunately, these expressions are often easily obtained using standard methods for producing design-based variance estimates of the population mean, but applied to $\psi(Y_i ; \hat{\theta}_s)$ in lieu of $Y_i$; see Section \ref{eg:sampling}.

If we also have a consistent estimator $\hat{V} \equiv \widehat{\mathrm{Var}}(\sqrt{N} \Psi_N(\theta_0)) \overset{P}{\to} V$, then the continuous mapping theorem alongside \eqref{eqn:asyuncondvar} suggests the following estimator for the joint variance of $\hat{\theta}_s$:
\begin{equation}
\widehat{\mathrm{Var}}(\hat{\theta}_s) = \frac{1}{N} \hat{\dot{\Psi}}_{\theta_0}^{-1} (\hat{V}^{\prime} + \hat{V}) (\hat{\dot{\Psi}}_{\theta_0}^{-1})^{T}. \label{eqn:uncondvaresttheta}
\end{equation}
If the $Y_i$ are i.i.d., then we can use the following unconditional variance estimator for $\sqrt{N} \Psi_N(\theta_0)$:
\begin{equation}
\hat{V} = \frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i ; \hat{\theta}_s) \psi(Y_i ; \hat{\theta}_s)^T. \label{eqn:uncondvarest}
\end{equation}

\subsection{Normalised Weights} \label{sec:normweights}

We demonstrate here that for some estimators, normalising the weights before estimation has no asymptotic effect.

\begin{theorem} \label{thm:normweights}
For given weights $w_1, w_2, \ldots, w_N$, define the normalised weights $w^{\prime}_1, w^{\prime}_2, \ldots, w^{\prime}_N$ by
\begin{equation*}
w^{\prime}_i = \frac{w_i}{\frac{1}{N} \sum_{j=1}^N w_j},
\end{equation*}
and suppose that $\frac{1}{N} \sum_{j=1}^N w_j \overset{P}{\to} 1$. Then for an arbitrary estimator $\hat{\vartheta}_N$ and a fixed sequence $r_N$ the following are equivalent:
\begin{align}
\frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i; \hat{\vartheta}_N) &= o_P(r_N), \label{eqn:normweightspsi}\\
\frac{1}{N} \sum_{i=1}^N w^{\prime}_i \psi(Y_i; \hat{\vartheta}_N) &= o_P(r_N). \label{eqn:unnormweightspsi}
\end{align}
\end{theorem}

\begin{remark}
Consider two estimators $\hat{\theta}_s$ and $\hat{\theta}^{\prime}_s$ that are constructed with weights $w_i$ and $w^{\prime}_i$ and satisfy \eqref{eqn:normweightspsi} and \eqref{eqn:unnormweightspsi}, respectively. This applies, for example, when producing linear regression coefficients (see Section \ref{eg:lr}) and maximum likelihood estimators (see Section \ref{eg:mle}). Theorem \ref{thm:normweights} implies that for both estimators, we can choose either $w_i$ or $w^{\prime}_i$ to establish consistency using Theorem \ref{thm:cons} (if $r_N \leq 1$), or to derive the asymptotic distribution using Theorem \ref{thm:normsup} and conduct variance estimation based on Theorem \ref{thm:desvar} (if $r_N \leq N^{-1/2}$). If the relevant assumptions are met, both estimators are consistent (if $r_N \leq 1$) or have the same asymptotic distributions and variances (if $r_N \leq N^{-1/2}$).
\end{remark}

\begin{remark}
Theorem \ref{thm:normweights} does not preclude the existence of estimators that satisfy \eqref{eqn:normweightspsi} and \eqref{eqn:unnormweightspsi} when constructed with normalised weights, but satisfy neither when constructed with unnormalised weights (or vice versa). The leading example is the quantile, which should be constructed with normalised weights and is covered in Section \ref{eg:quant}.
\end{remark}

\if0\wholepaper {
	\bibliographypref{../library}
} \fi

\end{document}