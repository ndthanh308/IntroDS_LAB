%auto-ignore
\pdfoutput=1

\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Examples} \label{sec:eg}

In the following examples we will illustrate how to produce an estimator $\hat{V}^{\prime}$ for the asymptotic design variance of $\sqrt{N} \Psi_s(\theta_0)$. Substituting $\hat{V}^{\prime}$ and \eqref{eqn:dotpsiest} into \eqref{eqn:desvarest} often gives a suitable estimator for the design variance of $\hat{\theta}_s$ that can be used for survey design. For quantiles (see Section \ref{eg:quant}), \eqref{eqn:dotpsiest} provides an inconsistent estimator $\hat{\dot{\Psi}}_{\theta_0}$ of $\dot{\Psi}_{\theta_0}$, and in this case we illustrate how a consistent $\hat{\dot{\Psi}}_{\theta_0}$ can be obtained. If the $Y_i$ are i.i.d., substitute \eqref{eqn:uncondvarest}, $\hat{\dot{\Psi}}_{\theta_0}$ and $\hat{V}^{\prime}$ into \eqref{eqn:uncondvaresttheta} to give a joint variance estimator for $\hat{\theta}_s$. When $Y_i$ is not independent, consistent estimators $\hat{V}$ for the asymptotic joint variance of $\sqrt{N} \Psi_N(\theta_0)$ can be used in lieu of \eqref{eqn:uncondvarest}; see Chapters 20, 21 and 25 of \citepref{Davidson2021}. Sections \ref{eg:weights} on weights and \ref{eg:sampling} on sampling leave the parameter $\theta_0$, its estimator $\hat{\theta}_s$ and the estimating equation $\Psi_s$ unspecified to highlight the generality of our approach, which is illustrated on specific statistics in Section \ref{eg:stats}.

%\subsection{Independent and Identically Distributed}

%Most of this subsection will be moved to the appendix, with a summary provided here.

%Suppose that $(w_1, Y_1), (w_2, Y_2), \ldots, (w_N, Y_N)$ are i.i.d. random variables, where $(w_i, Y_i)$ is distributed according to a probability measure $P$.

%Given an unweighted estimator $\hat{\theta}_N \overset{p}{\to} \theta_0$ satisfying the assumptions of Theorem \ref{thm:normsup}, and a weighted estimator $\hat{\theta}_s \overset{p}{\to} 0$ satisfying $\Psi_s(\hat{\theta}_s) = o_p(N^{-1/2})$, the only assumption of the theorem yet to be satisfied by the weighted estimator is given by \eqref{eqn:donsker}. If the weights are almost surely bounded by a fixed value $B$ (as they are in many popular survey methods), then the weighted estimator preserves two standard properties of the unweighted estimator ensuring that \eqref{eqn:donsker} is satisfied. First, if $\mathbb{E}[(\psi(Y_i ; \theta) - \psi(Y_i; \theta_0))^2] \to 0$ for all $\theta \to \theta_0$, then $\mathbb{E}[(w_i \psi(Y_i ; \theta) - w_i \psi(Y_i; \theta_0))^2] \leq B \times \mathbb{E}[(\psi(Y_i ; \theta) - \psi(Y_i; \theta_0))^2] \to 0$ for all $\theta \to \theta_0$ as well. Second, if the class of functions $\{ y \mapsto \psi(y ; \theta) - \psi(y ; \theta_0) : \lVert \theta - \theta_0 \rVert < \delta \}$ is $P$-Donsker for some $\delta > 0$, then $\{ (w,y) \mapsto w \psi(y ; \theta) - w \psi(y ; \theta_0) : \lVert \theta - \theta_0 \rVert < \delta \}$ is also $P$-Donsker; see Example 2.10.10 of \citepref{vanderVaart1996}. By Lemma 3.3.5 of \citepref{vanderVaart1996}, these two properties imply that \eqref{eqn:donsker} holds, and the assumptions of Theorem \ref{thm:normsup} are satisfied for the weighted estimator $\hat{\theta}_s$.

%In this case, the law of large numbers gives $\Psi_N(\theta) = \frac{1}{N} \sum_{i=1}^N \psi(Y_i; \theta) \overset{p}{\to} \mathbb{E}[\psi(Y_i ; \theta)]$ and $\Psi_s(\theta) = \frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i ; \theta) \overset{p}{\to} \mathbb{E}[w_i \psi(Y_i ; \theta)]$, so that $\Psi(\theta) = \mathbb{E}[\psi(Y_i ; \theta)] = \mathbb{E}[w_i \psi(Y_i ; \theta)]$. The Donsker classes imply that $\psi(Y_i ; \theta_0)$ and $w_i \psi(Y_i ; \theta_0)$ have finite variance. This is sufficient to provide a zero-mean, normally-distributed $Z$ for \eqref{eqn:zestlin}, where $V \coloneqq \mathrm{Var}(Z) = \mathrm{Var}(w_i \psi(Y_i ; \theta_0)) = \mathbb{E}[w_i^2 \psi(Y_i ; \theta_0) \psi(Y_i ; \theta_0)^{T}]$. It follows from Theorem \ref{thm:normsup} that $\sqrt{N}(\hat{\theta}_s - \theta_0) \Rightarrow \mathcal{N}(0, \dot{\Psi}_{\theta_0}^{-1} V (\dot{\Psi}_{\theta_0}^{-1})^{T})$. We can consistently estimate $V$ by $\hat{V} \coloneqq \frac{1}{N} \sum_{i=1}^N w_i^2 \psi_N(Y_i; \hat{\theta}_s) \psi_N(Y_i; \hat{\theta}_s)^T$; see Question 8 of \citepref{vanderVaart1996}. Denote by $\dot{\psi}$ the Jacobian matrix of $\psi$ with respect to $\theta$, and suppose that $\{ y \mapsto \dot{\psi}(y ; \theta) - \dot{\psi}(y ; \theta_0) : \lVert \theta - \theta_0 \rVert < \delta \}$ is $P$-Donsker. Provided that $\dot{\Psi}_{\theta} = \mathbb{E}[\dot{\psi}(Y_i ; \theta)]$, we can often consistently estimate $\dot{\Psi}_{\theta_0}$ by $\hat{\dot{\Psi}}_{\theta_0} = \frac{1}{N} \sum_{i=1}^N w_i \dot{\psi}(Y_i ; \hat{\theta}_s)$, again due to Question 8. It follows by the continuous mapping theorem that the asymptotic variance of $\sqrt{N}(\hat{\theta}_s - \theta_0)$ is consistently estimated by $\hat{\dot{\Psi}}_{\theta_0} \hat{V} \hat{\dot{\Psi}}_{\theta_0}^{T}$. We can now estimate the variance of $\hat{\theta}_s$ by $\widehat{\mathrm{Var}}(\hat{\theta}_s) = \frac{1}{N} \hat{\dot{\Psi}}_{\theta_0}^{-1} \hat{V} (\hat{\dot{\Psi}}_{\theta_0}^{-1})^{T}$. In the same way, we have a zero-mean, normally distributed $Z^{\prime}$ for \eqref{eqn:zestlin}, letting $V = \mathrm{Var}(Z^{\prime}) = \mathrm{Var}(w_i \psi(Y_i ; \theta_0))$.

%A similar argument can be applied to obtain an estimate for the design-based variance $\mathrm{Var}(\hat{\theta}_s \mid Y_{1:N})$. First note that $\frac{1}{N} \sum_{i=1}^N w_i \psi(Y_i ; \theta_0) - \frac{1}{N} \sum_{i=1}^N \psi(Y_i ; \theta_0) = \frac{1}{N} \sum_{i=1}^N (w_i - 1) \psi(Y_i ; \theta_0)$, and $(w_i - 1) \psi(Y_i ; \theta_0)$ has finite variance also due to the Donsker classes. We therefore have a zero-mean, normally distributed $Z^{\prime}$ for \eqref{eqn:zestlin}, where $V^{\prime} \coloneqq \mathrm{Var}(Z^{\prime}) = \mathrm{Var}((w_i - 1) \psi(Y_i ; \theta_0)) = \mathbb{E}[(w_i - 1)^2 \psi(Y_i ; \theta_0) \psi(Y_i \theta_0)^T]$. If $\mathbb{E}[w_i \psi(Y_i ; \theta_0) \psi(Y_i ; \theta_0)^{T}] = \mathbb{E}[\psi(Y_i ; \theta_0) \psi(Y_i ; \theta_0)^{T}]$ straightforward algebra gives $\mathbb{E}[(w_i - 1)^2 \psi(Y_i ; \theta_0) \psi(Y_i \theta_0)^T] = \mathbb{E}[w_i (1 - w_i) \psi(Y_i ; \theta_0) \psi(Y_i \theta_0)^T]$  and we can consistently estimate $V^{\prime}$ by $\hat{V}^{\prime} = \frac{1}{N} \sum_{i=1}^N w_i (1 - w_i) \psi(Y_i ; \theta_0) \psi(Y_i \theta_0)^T$. It follows by the continuous mapping theorem that the asymptotic variance of $\sqrt{N}(\hat{\theta}_s - \hat{\theta}_N)$ is consistently estimated by $\hat{\dot{\Psi}}_{\theta_0} \hat{V}^{\prime} \hat{\dot{\Psi}}_{\theta_0}^{T}$. Given the first equality provided by Theorem \ref{thm:desvar}, we can now estimate the design-based variance of $\hat{\theta}_s$ by $\widehat{\mathrm{Var}}(\hat{\theta}_s \mid Y_{1:N}) = \frac{1}{N} \hat{\dot{\Psi}}_{\theta_0} \hat{V}^{\prime} \hat{\dot{\Psi}}_{\theta_0}^{T}$.

\subsection{Weights} \label{eg:weights}

\subsubsection{Horvitz-Thompson} \label{eg:htweights}

Suppose that observations are obtained via a probability sample. In addition to the notation defined in Section \ref{sec:intest}, let the survey design's second-order selection probabilities be denoted by $\pi_{ij} = \mathrm{Pr}(\alpha_i = 1 \cap \alpha_j = 1)$, and let $A = \{i : \alpha_i = 1\}$ be the set of indices that identify the units in the probability sample. Since the Horvitz-Thompson weights $w_i = \alpha_i \pi_i^{-1}$ are design-unbiased (see \eqref{eqn:htunbiased} and surrounding discussion), Corollary \ref{crl:normpop} and Theorem \ref{thm:desvar} can be applied. The standard Horvitz-Thompson variance estimator of $\sqrt{N} \Psi_s(\theta_0)$ is given by (e.g.\ Result 2.8.1 of \citealppref{Sarndal1992})
\begin{equation}
\hat{V}^{\prime} = \frac{1}{N} \sum_{i \in A} \sum_{j \in A} (\pi_i^{-1} \pi_j^{-1} - \pi_{ij}^{-1}) \psi(Y_i ; \hat{\theta}_s) \psi(Y_i ; \hat{\theta}_s)^{T}. \label{eqn:htv}
\end{equation}
This variance estimator is approximately design unbiased if $\pi_{ij}$ is bounded away from zero for all $i,j$. If the sample size $n = \sum_{i=1}^N \alpha_i$ is not random, then the Yates-Grundy-Sen variance estimator for $\sqrt{N} \Psi_s(\theta)$ can be used instead; see Result 2.8.2 of \citepref{Sarndal1992}. Another alternative is the Hartley-Rao variance estimator \citeppref{Hartley1962}, which applies if without-replacement sampling is used.

\subsubsection{Data Integration} \label{eg:diweights}

Suppose our population is observed via a probability sample and a big-data set. If observations in the survey are accompanied by weights $w_i$ satisfying $\mathbb{E}[w_i \mid Y_{1:N}] = 1$, the integrated weights $w^{DI}_i = \delta_i + (1 - \delta_i) w_i$ are design-unbiased after extending $Y_{i}$ if needed so that its last element is equal to $\delta_i$ (see \eqref{eqn:diunbiased} and surrounding discussion). As a result, Corollary \ref{crl:normpop} and Theorem \ref{thm:desvar} can be applied.

Given that
\begin{align*}
\mathrm{Var} & (\sqrt{N} \Psi^{DI}_s(\theta_0) \mid Y_{1:N}) \\
&= \mathrm{Var} \left( \frac{1}{\sqrt{N}} \sum_{i=1}^N \delta_i \psi(Y_i ; \theta_0) + \frac{1}{\sqrt{N}} \sum_{i=1}^N (1 - \delta_i) w_i \psi(Y_i ; \theta_0)\ \Bigg\vert\ Y_{1:N} \right) \\
&= \mathrm{Var} \left( \frac{1}{\sqrt{N}} \sum_{i=1}^N w_i (1 - \delta_i) \psi(Y_i ; \theta_0)\ \Bigg\vert\ Y_{1:N} \right),
\end{align*}
we see that the design variance $\mathrm{Var}(\sqrt{N} \Psi^{DI}_s(\theta_0) \mid Y_{1:N})$ is equal to the variance of the survey-only estimate of $\frac{1}{\sqrt{N}} \sum_{i=1}^N (1 - \delta_i) \psi(Y_i ; \theta_0)$. Since this sum extends over only those observations not in the big data sample (or else the summand is zero), an integrated variance estimator is unavailable and we resort to the survey-only variance estimator. For example, if the survey weights $w_i$ are of the Horvitz-Thompson sort described in the previous subsection, then we obtain
\begin{equation}
\hat{V}^{\prime} = \frac{1}{N} \sum_{i \in A \setminus B} \sum_{j \in A \setminus B} (\pi_i^{-1} \pi_j^{-1} - \pi_{ij}^{-1}) \psi(Y_i ; \hat{\theta}^{DI}_s) \psi(Y_i ; \hat{\theta}^{DI}_s)^{T}, \label{eqn:ktv}
\end{equation}
where $B = \{i : \delta_i = 1 \}$ is the set of indices that identify the big-data observations.

In view of Remark \ref{rmk:clt}, both integrated and survey-only estimators are asymptotically unbiased by \eqref{eqn:zestlin} and \eqref{eqn:convdistprime}. One would therefore only choose the integrated estimator if it had a variance less than or equal to its survey-only counterpart. By Corollary \ref{crl:conspop} $\dot{\Psi}_{\theta_0}$ is identical for both integrated and Horvitz-Thompson estimation, so for scalar $\theta$ and beginning with the asymptotic design variance in \eqref{eqn:asydesignvar}, we would choose the integrated estimator if $\mathrm{Var}(\sqrt{N} \Psi^{DI}_s(\theta_0) \mid Y_{1:N}) \leq \mathrm{Var}(\sqrt{N} \Psi_s(\theta_0) \mid Y_{1:N})$. While this is difficult to show in general, if the survey weights are i.i.d.\ with nonzero variance outside a nonempty big-data set, then
\begin{align*}
\mathrm{Var}(\sqrt{N} \Psi^{DI}_s(\theta_0) \mid Y_{1:N}) &= \frac{1}{N} \sum_{i=1}^N (1 - \delta_i) \mathrm{Var}(w_i \mid Y_{1:N}) Y_i \\
&< \frac{1}{N} \sum_{i=1}^N \mathrm{Var}(w_i \mid Y_{1:N}) Y_i \\
&= \mathrm{Var}(\sqrt{N} \Psi_s(\theta_0) \mid Y_{1:N}).
\end{align*}
Underperformance of the integrated estimator could therefore only materialise if there were too much dependence between survey weights; if the covariance between different survey weights is small then the integrated estimator $\hat{\theta}^{DI}_s$ is more accurate than its survey-only counterpart $\hat{\theta}_s$ provided that the sample sizes (and thus the population size) are large enough. By Theorem \ref{thm:desvar}, an integrated estimator with a lower asymptotic design variance than its survey-only counterpart is also in possession of a lower asymptotic joint variance.

\subsection{Sampling} \label{eg:sampling}

\subsubsection{Simple Random Sampling Without Replacement} \label{eg:srswor}

Suppose that the population is sampled by taking a random draw of size $n$, without replacement. Assume that the sample size $n$ is obtained by rounding $fN$ to the nearest integer, where $f$ is a fixed sampling fraction, so that $\lvert n/N - f \rvert \leq 1/N$ and $n/N = f + O_P(N^{-1})$. Thus the asymptotic results above occur as both the sample size $n$ and the population size $N$ approach infinity in the rough proportion $n/N \approx f$ for a constant $f$. For a given population and sample we will use the asymptotic approximations that correspond to the sampling fraction $f = n/N$.

When constructing the integrated weights of \citepref{Kim2021} using Horvitz-Thompson survey weights, \eqref{eqn:ktv} becomes
\begin{gather}
\resizebox{0.9 \textwidth}{!}{$S^2 = \frac{1}{n-1} \left( \sum_{i \in A \setminus B} \psi(Y_i ; \hat{\theta}^{DI}_s) \psi(Y_i ; \hat{\theta}^{DI}_s )^{T} - \frac{1}{n} \left( \sum_{i \in A \setminus B} \psi(Y_i ; \hat{\theta}^{DI}_s) \right) \left( \sum_{i \in A \setminus B} \psi(Y_i ; \hat{\theta}^{DI}_s) \right)^{T} \right),$} \label{eqn:srswors2} \\
\hat{V}^{\prime} = \frac{1-f}{f} S^2, \label{eqn:srsworv}
\end{gather}
where we recall that $n$ is the sample size of $A$, the probability sample. Regarding the survey-only Horvitz-Thompson case of Example \ref{eg:htweights}, \eqref{eqn:htv} also becomes the above after setting the big-data sample $B$ to the empty set.

\subsubsection{Stratified Sampling} \label{eg:strat}

Consider a population of size $N$ comprised of $H$ strata indexed by $h = 1, 2, \ldots, H$, with each stratum $h$ containing a subpopulation $Y_{h,1}, Y_{h,2}, \ldots, Y_{h,N_h}$, such that $N = \sum_{h=1}^H N_h$. For each stratum $h$, the population size $N_h$ is obtained by rounding $F_h N$ to the nearest integer, where $F_h$ is a fixed `subpopulation fraction'. As a result, $\lvert N_h / N - F_h \rvert \leq 1/N$ and $N_h / N = F_h + O_P(N^{-1})$. Thus the asymptotic results above occur as both the subpopulation sizes $N_h$ and the population size $N$ approach infinity according to the rough proportions $N_h/N \approx F_h$ for constants $F_h$. For a given stratification we will use the asymptotic approximations that correspond to the subpopulation fractions $F_h = N_h / N$. Supposing that each variable $Y_{h,i}$ is assigned the weight $w_{h,i}$, we can apply the results of Section \ref{sec:theory} via reindexation, for example by imposing $Y_{\sum_{k=1}^{h-1} N_k + i} = Y_{h,i}$ and $w_{\sum_{k=1}^{h-1} N_k + i} = w_{h,i}$. This has the effect of making averages across $(h,i)$ equivalent to averages across $i$ when applying the results of Section \ref{sec:theory}.

Assume that the strata are sampled independently given $Y_{1:N}$, and let $\Psi_h(\theta) = \frac{1}{N_h} \sum_{i=1}^{N_h} w_{h,i} \psi(Y_{h,i}; \theta)$. Then
\begin{align*}
\Psi_s(\theta) &= \sum_{h=1}^H F_h \Psi_h(\theta), \\
\mathrm{Var}(\sqrt{N} \Psi_s(\theta) \mid Y_{1:N}) &= \sum_{h=1}^H F_h \mathrm{Var}(\sqrt{N_h} \Psi_h(\theta) \mid Y_{1:N}).
\end{align*}
Given estimators $\hat{V}^{\prime}_h$ for $\mathrm{Var}(\sqrt{N_h} \Psi_h(\theta_0) \mid Y_{1:N})$, an estimator $\hat{V}^{\prime}$ for $\mathrm{Var}(\sqrt{N} \Psi_s(\theta_0) \mid Y_{1:N})$ is therefore given by
\begin{equation}
\hat{V}^{\prime} = \sum_{h=1}^H F_h \hat{V}^{\prime}_h. \label{eqn:vhatprimestrat}
\end{equation}

Suppose that we produce a sample of size $n_h$ from each stratum $h$ independently, using simple random sampling without replacement to observe each $Y_{h,i}$ whose index $i$ lies in a set $A_h$. Also suppose that for each stratum $h$ there is a set of indices $B_h$ identifying those observations $Y_{h,i}, i \in B_h$ that are obtained via the big data sample. If we use \citepref{Kim2021} integrated weights constructed with Horvitz-Thompson survey weights, then \eqref{eqn:srswors2} and \eqref{eqn:srsworv} can be applied within each stratum to give
\begin{gather}
f_h = \frac{n_h}{N_h}, \nonumber \\
\resizebox{0.9 \textwidth}{!}{$S^2_h = \frac{1}{n_h-1} \left( \sum_{i \in A_h \setminus B_h} \psi(Y_i ; \hat{\theta}^{DI}_s) \psi(Y_{h,i} ; \hat{\theta}^{DI}_s )^{T} - \frac{1}{n_h} \left( \sum_{i \in A_h \setminus B_h} \psi(Y_{h,i} ; \hat{\theta}^{DI}_s) \right) \left( \sum_{i \in A_h \setminus B_h} \psi(Y_{h,i} ; \hat{\theta}^{DI}_s) \right)^{T} \right),$} \nonumber \\
\hat{V}^{\prime}_h = \frac{1 - f_h}{f_h} S_h^2. \label{eqn:vhatprimestratintegratedsrswor}
\end{gather}

If there is no big-data set, the above applies after setting $B_h = \varnothing$. Following \citepref{Lohr2021}, another leading case is where the big data sample is treated as a completely enumerated stratum, whereby $A_1 = \varnothing$ and $B_h = \varnothing$ for all $h = 2, 3, \ldots, H$, say. In this case, we survey only those units not in the big data set. For a given survey sample of size $n$ this strategy is more efficient than surveying the entire population. For example, if $H = 2$, it is not too difficult to show using \eqref{eqn:asydesignvar} that the asymptotic design variance obtained by surveying only non-big-data units is $(F_2^2 - f)/(1 - f)$ times the variance obtained by sampling from the entire population, where $f = \sum_{h=1}^H f_h$ is the total sampling fraction. If $\hat{\theta}^{DI}_s$ is a scalar estimator, we can optimally allocate the total sampling fraction to strata by using standard constrained optimisation techniques to minimise \eqref{eqn:desvarest} across $f_1, f_2, \ldots, f_H$ subject to $\sum_{h=1}^H f_h = f$ and $0 \leq f_h \leq 1, h = 1, 2, \ldots, H$.

\subsection{Statistics} \label{eg:stats}

In defining the statistics below, we will sometimes make use of the following estimate for the cumulative distribution function (c.d.f.):
\begin{equation*}
\hat{F}_s(y) = \frac{1}{N} \sum_{i=1}^N w_i I(Y_i \leq y).
\end{equation*}
Note that $\hat{F}_s(y)$ is not necessarily a c.d.f.\ itself, unless $w_i \geq 0$ and $\frac{1}{N} \sum_{i=1}^N w_i = 1$. When assuming that the population $Y_1, Y_2, \ldots, Y_N$ is i.i.d., we will use $F$ and $f$ to denote the c.d.f.\ and density, respectively, of $Y_i$. 

\subsubsection{Quantiles} \label{eg:quant}

Let $Y_{(i)}$ be the $i$\textsuperscript{th} smallest value in the population, so that $Y_{(1)} \leq Y_{(2)} \leq \cdots \leq Y_{(N)}$, and let $w_{(i)}$ be the weight associated with $Y_{(i)}$. Consider the $p$-quantile given by
\begin{equation}
\hat{\theta}_s = \inf \{ y : \hat{F}_s(y) \geq p \}, \label{eqn:pquantile}
\end{equation}
and the function
\begin{equation*}
\psi(y ; \theta) = (1 - p) I(y < \theta) - p I(y > \theta).
\end{equation*}
By definition, there exists a $j$ such that $\hat{\theta}_s = Y_{(j)}$ and $p \leq \hat{F}_s(\hat{\theta}_s) \leq p + w_{(j)}/N$. As a result, if $\frac{1}{N} \sum_{i=1}^N w_{i} = 1$ and $Y_i$ is continuously distributed (so that $\frac{1}{N} \sum_{i=1}^N w_{i} I(Y_{i} = \hat{\theta}_s) = w_{(j)}/N$), then we have
\begin{equation*}
-(1-p) w_{(j)} \leq N \Psi_s(\hat{\theta}_s) \leq p w_{(j)}.
\end{equation*}
Integrability of $w_{(j)}$ (implied by boundedness for example) then provides $\Psi_s(\hat{\theta}_s) = o_P(N^{-1/2})$, as required by Theorem \ref{thm:normsup}. If the population is i.i.d., then $\Psi(\theta) = \mathbb{E}[\psi(Y_i ; \theta)] = F(\theta) - p$ and $\dot{\Psi}_{\theta_0} = f(\theta_0)$. Note that in this case, \eqref{eqn:dotpsiest} cannot be used to estimate $\dot{\Psi}_{\theta_0}$, since $\dot{\psi}(Y_i ; \hat{\theta}_s)$ is undefined if $Y_i = \hat{\theta}_s$. Given an estimator $\hat{f}$ for the density $f$ (e.g.\ \citealppref{Buskirk2005}), we can use $\hat{\dot{\Psi}}_{\theta_0} = \hat{f}(\theta_0)$ instead.

\subsubsection{Gini Index} \label{eg:gini}

Consider the the weighted Gini index given by
\begin{equation}
\hat{\theta}_s = \frac{\int \int \lvert y - x \rvert\ \mathrm{d} \hat{F}_s(x)\ \mathrm{d} \hat{F}_s(y)}{2 \int y\ \mathrm{d} \hat{F}_s(y)} = \frac{\sum_{i=1}^N \sum_{j=1}^N w_i w_j \lvert Y_i - Y_j \rvert}{2 N \sum_{i=1}^N w_i Y_i}. \label{eqn:gini}
\end{equation}
If the population is generated i.i.d., then we will show that the estimating equation following from
\begin{equation*}
\psi(y ; \theta) = 2 \int (I(y \leq x) - F(x)) x\ \mathrm{d} F(x) + (2 F(y) - 1)y - \theta y
\end{equation*}
satisfies $\Psi_s(\theta) = o_P(N^{-1/2})$, as required by Theorem \ref{thm:normsup}. Rearrangement of \eqref{eqn:gini} gives
\begin{equation*}
\hat{\theta}_s = \frac{\int (2 \hat{F}_s(y) - 1) y\ \mathrm{d} \hat{F}_s(y) - \frac{1}{N^2} \sum_{i=1}^N w_i^2 Y_i}{\int y\ \mathrm{d} \hat{F}_s(y)},
\end{equation*}
so that
\begin{equation*}
\sqrt{N} \Psi_s(\hat{\theta}_N) = - 2 \int (\hat{F}_s(y) - F(y)) y\ \mathrm{d} \left[ \sqrt{N} (\hat{F}_s(y) - F(y)) \right] + \frac{1}{\sqrt{N}} \frac{1}{N} \sum_{i=1}^N w_i^2 Y_i.
\end{equation*}
For i.i.d.\ $(Y_1, w_1), (Y_2, w_2), \ldots, (Y_N, w_N)$, the first term is $o_P(1)$ by Example 2.10.27,\footnote{Invoke with the upper bound $y \mapsto \max(y, 0)$. By symmetry obtain the same for nonpositive, nondecreasing functions with lower bound $y \mapsto \min(y, 0)$.} Example 2.10.7,\footnote{Invoke first with $\mathcal{F} = \{f : 0 \leq f(y) \leq \max(y, 0) \}$ and $\mathcal{G} = \{g : \min(y, 0) \leq g(y) \leq 0 \}$; second with $\{ y \mapsto f(y) + g(y) : f \in \mathcal{F}, g \in \mathcal{G} \}$ and $\{ y \mapsto -(f(y) + g(y)) : f \in \mathcal{F}, g \in \mathcal{G} \}$.} Example 2.10.10,\footnote{Let $\mathcal{F} = \{ (y,w) \mapsto f(y)y : -1 \leq f(y) \leq 1 \}$ and $g(y,w) = w$.} Equation 2.1.8 (with surrounding discussion) and  Problem 2.9.1 of \citepref{vanderVaart1996}, provided that the weights are almost surely bounded and $\mathbb{E}[Y_i^{2 + \epsilon}] < \infty$ for some $\epsilon > 0$. The second term converges to zero after applying a weak law of large numbers to $\frac{1}{N} \sum_{i=1}^N w_i^2 Y_i$.

A law of large numbers gives $\Psi_s(\theta) \overset{P}{\to} \Psi(\theta) = \mathbb{E}[\psi(Y_i; \theta)] = \mathbb{E}[(2 F(Y_i) - 1) Y_i] - \theta \mathbb{E}[Y_i]$ so that $\dot{\Psi}_{\theta_0} = - \mathbb{E}[Y_i] = \mathbb{E}[\dot{\psi}(Y_i ; \theta)]$, and \eqref{eqn:dotpsiest} suffices, giving $\hat{\dot{\Psi}}_{\theta_0} = - \frac{1}{N} \sum_{i=1}^N w_i Y_i$. When applying any of the above formulas for $\hat{V}^{\prime}$, use $\hat{\psi}(y ; \theta) = 2 \int (I(y \leq x) - \hat{F}_s(x))x\ \mathrm{d} \hat{F}_s(x) + (2 \hat{F}_s(y) - 1)y - \theta y$ in place of $\psi(y ; \theta)$, since $F$ is unknown.

\subsubsection{Linear Regression Coefficients} \label{eg:lr}

Suppose that we treat the first element $Y_{i,1}$ as a scalar regressand and $X_i = Y_{i,2:k}$ as a $(k-1)$-dimensional row vector of regressors. If $\hat{\theta}_s$ is the $(k-1)$-dimensional column vector of linear least-squares coefficients minimising
\begin{equation*}
\theta \mapsto \frac{1}{N} \sum_{i=1}^N w_i (Y_{i,1} - X_i \theta)^2,
\end{equation*}
then differentiating the above with respect to $\theta$ reveals that letting $\psi^{\prime}(y,x ; \theta) = x^T (y - x \theta)$ and $\psi(y ; \theta) = \psi^{\prime}(y_1, y_{2:k} ; \theta)$ gives $\Psi_s(\hat{\theta}_s) = 0 = o_P(N^{-1/2})$, as required by Theorem \ref{thm:normsup}. For an i.i.d.\ superpopulation with finite second moments, $\Psi(\theta) = \mathbb{E}[X_i^T Y_i] - \mathbb{E}[X_i^T X_i] \theta$ and $\dot{\Psi}_{\theta_0} = - \mathbb{E}[X_i^T X_i]$, which can be estimated by $\hat{\dot{\Psi}}_{\theta_0} = - \frac{1}{N} \sum_{i=1}^N w_i X_i^T X_i$. This is the same estimate given by \eqref{eqn:dotpsiest}.

When using Horvitz-Thompson weights, the above $\hat{\theta}_s$ contains the same estimated regression coefficients computed in Section 4.2 of \citepref{Binder1983} and Example 5.1 of \citepref{Pfeffermann1993}, and shown to be optimal in Theorem 1 of \citepref{Godambe1986}. Our asymptotic joint variance in \eqref{eqn:asyuncondvar} is new here, as is the joint variance estimator in \eqref{eqn:uncondvaresttheta}. \citepref{Pfeffermann1993} uses only the design variance on the basis that it is close to the joint variance when the population is much larger than the sample (see his Equation 2.4). In our view this further approximation is unnecessary because computing the joint variance estimator is almost as easy as computing the design-based variance estimator in \eqref{eqn:desvarest}, and it is not always true that the sample is small relative to the population.

\subsubsection{Maximum Likelihood Estimator} \label{eg:mle}

Suppose that we want to fit to the population a postulated (but not necessarily correctly specified) model parameterised by a vector $\theta$ with likelihood function $p(y ; \theta)$. Consider the estimator $\hat{\theta}_s$ that maximises the weighted likelihood $\frac{1}{N} \sum_{i=1}^N w_i \log p(Y_i ; \theta)$ across all $\theta$ in a parameter space. If $\hat{\theta}_s$ converges in probability to a fixed value $\theta_0$ in the interior of the parameter space and $\psi(y ; \theta)$ is the gradient of the weighted likelihood with respect to $\theta$, then $\Psi_s(\hat{\theta}_s)$ equals zero with probability approaching one and $\Psi_s(\hat{\theta}_s) = o_P(N^{-1/2})$ follows, as required by Theorem \ref{thm:normsup}.

\if0\wholepaper {
	\bibliographypref{../library}
} \fi

\end{document}