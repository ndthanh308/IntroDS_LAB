% $Id: template.tex 11 2007-04-03 22:25:53Z jpeltier $

\documentclass{vgtc}                          % final (conference style)
%\documentclass[review]{vgtc}                 % review
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint]{vgtc}               % preprint
%\documentclass[electronic]{vgtc}             % electronic version

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Figures should be in CMYK or Grey scale format, otherwise, colour 
%% shifting may occur during the printing process.

%% These few lines make a distinction between latex and pdflatex calls and they
%% bring in essential packages for graphics and font handling.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% % Figure removed is completely sufficient.
%%
\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images

\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{cite}                      % needed to automatically sort the references
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}                  % only used for the table example
\usepackage{siunitx}

%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.


%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline. If not, the default IEEE copyright message will appear in preprint mode.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% This adds a link to the version of the paper on IEEEXplore
%% Uncomment this line when you produce a preprint version of the article 
%% after the article receives a DOI for the paper from IEEE
%\ieeedoi{xx.xxxx/TVCG.201x.xxxxxxx}


%% Paper title.

\title{Line Harp: Importance-Driven Sonification for Dense Line Charts}

%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%%Author and Affiliation (multiple authors with single affiliations).
\author{Egil Bru\thanks{E-mail: egil.bru@student.uib.no} \\ 
\parbox{1.4in}{\scriptsize \centering University of Bergen, \\ Norway}
\and Thomas Trautner\thanks{E-mail: thomas.trautner@uib.no} \\
\parbox{1.4in}{\scriptsize \centering University of Bergen, \\ Norway}
\and Stefan Bruckner\thanks{E-mail: stefan.bruckner@uni-rostock.de} \\
\parbox{1.4in}{\scriptsize \centering University of Rostock, \\ Germany}
}


%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com} %
%\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com} \\
%\scriptsize Martha Stewart Enterprises \\ Microsoft Research}
%\affiliation{}
% \author{Stefan Bruckner\thanks{E-mail: stefan.bruckner@uni-rostock.de}}


%% Author and Affiliation (multiple authors with multiple affiliations)
%% \author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\ %
%%       \scriptsize Starbucks Research %
%%\and Ed Grimley\thanks{e-mail: ed.grimley@aol.com}\\ %
%%     \scriptsize Grimley Widgets, Inc. %
%%\and Martha Stewart\thanks{e-mail: martha.stewart@marthastewart.com}\\ %
%%     \parbox{1.4in}{\scriptsize \centering Martha Stewart Enterprises \\ Microsoft Research}}

%% A teaser figure can be included as follows
\teaser{
  \centering
  % Figure removed
  \caption{Dense line chart visualization containing multiple clusters, where a red arrow indicates the mouse path. On the right side, an audio visualization of our sonification approach is displayed for the highlighted mouse path. It contains an amplitude visualization and the contour
of the fundamental frequency (musical pitch) of the audio (see Section~\ref{sec:results} for further details).}
  \label{fig:teaser}
}

%% Abstract section.
\abstract{Accessibility in visualization is an important yet challenging topic. Sonification, in particular, is a valuable yet underutilized technique that can enhance accessibility for people with low vision. However, the lower bandwidth of the auditory channel makes it difficult to fully convey dense visualizations. For this reason, interactivity is key in making full use of its potential. In this paper, we present a novel approach for the sonification of dense line charts. We utilize the metaphor of a string instrument, where individual line segments can be "plucked". We propose an importance-driven approach which encodes the directionality of line segments using frequency and dynamically scales amplitude for improved density perception. We discuss the potential of our approach based on a set of examples.
} % end of abstract

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/about/class> for details.
%% We recommend the 2012 system <http://www.acm.org/about/class/class/2012>
%% For the 2012 system use the ``\CCScatTwelve'' which command takes four arguments.
%% The 1998 system <http://www.acm.org/about/class/class/2012> is still possible
%% For the 1998 system use the ``\CCScat'' which command takes four arguments.
%% In both cases the last two arguments (1998) or last three (2012) can be empty.

\CCScatlist{
  \CCScatTwelve{Human-centered computing}{Visualization}{Visualization application domains}{Information visualization}; 
  \CCScatTwelve{Human-centered computing}{Human computer interaction (HCI)}{Interaction techniques}{Auditory feedback}; 
  \CCScatTwelve{Human-centered computing}{Accessibility}{Accessibility systems and tools}{}
}

%\CCScatlist{
  %\CCScat{H.5.2}{User Interfaces}{User Interfaces}{Graphical user interfaces (GUI)}{};
  %\CCScat{H.5.m}{Information Interfaces and Presentation}{Miscellaneous}{}{}
%}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}

\maketitle

While there are several guidelines and best practices for designing accessible visualizations \cite{AccessibilityWild,AccessibleVisualization}, some users may still face barriers due to their individual needs and abilities. Sonification has the potential to enhance visualization accessibility by providing an additional channel to encode information. It has been employed in various real-world scenarios, including air traffic control \cite{AirTraffic} and medical equipment \cite{medical}. Real-time monitoring applications that use sonification have been extensively researched, including EEG signal monitoring \cite{EEG}, and stock market analysis \cite{Marketbuzz}. However, since auditory information allows for a limited degree of parallelism, it is difficult to convey complex information in a responsive and efficient manner. A static playback of dense data would either need to be excessively lengthy or too fast for proper perception, or, alternatively, require considerable abstraction.

In this paper, we present a new interactive technique for sonifying dense line charts. Our approach draws inspiration from string instruments like the harp, where each string produces a unique sound. Based on this analogy, we have designed a sonification strategy that mimics the "plucking" action when the mouse cursor intersects a line segment. To handle situations where multiple line segments are intersected simultaneously or in rapid succession, we have developed an importance-driven approach that adjusts the waveform based on the respective importance of line segments. This leads to the creation of "chords" in regions of dense data, enriching the auditory experience. Users can control the movement speed of the cursor, allowing them to either obtain a quick overview of the data set's larger-scale features or delve into the finer details of individual line segments.

%Our approach incorporates a frequency audio mapping that leverages the inherent directionality of lines to improve angle perception and enhance navigation. Additionally, we propose the use of dynamically scaled amplitude to improve audible density perception of clustered lines. By mimicking the responsiveness of real-life stringed instruments, we aim to enhance the user experience for those with low vision.

\section{Related Work}
% Accesabilty

% General Sonification 

The Sonification Handbook \cite{TheSonificationHandbook} provides an extensive review of sonification theory, practice, and applications, featuring contributions from various researchers in the field. 


% General Sonification of line charts

Sonification intended for data analysis commonly involves a technique known as parameter mapping which involves the association of data elements with auditory parameters. Previous work has investigated the effectiveness of different mappings \cite{THIRTEENYO} and explored what types of data and polarities that are more naturally mapped to particular sound attributes \cite{SonificationMappings, SonificationMappings2}. Flowers and Hauer have demonstrated that participants can interpret line graphs through sonification, by mapping each data point to a musical note (frequency) and moving along the x-axis (time) \cite{VisualGraphs}. In one study, participants were able to successfully redraw multiple lines and find intersections while listening to two separate lines in each ear \cite{Drawing_by_Ear}. Furthermore, spatial audio has been proposed for visualizing multiple data series \cite{Design_Guidelines} in parallel. However, the effectiveness of such an approach is severely limited by the density of the data. As with visual clutter, audio noise increases as more lines are sonified, making it challenging for users to discern individual data points. In addition, the potential for interactions is limited by the fact that the sonification is presented sequentially. Common concepts such as pause, play, back, and forward may aid in the navigation of the graphs \cite{BrowsingModes}, but they do not fully address the limitations of the audio environment for complex visualizations.

% Interactions with sonification
To address this, model-based sonification approaches have been proposed \cite{TheoryOfSonification}. In model-based sonification, the data set is turned into a dynamic model to be explored interactively by the user, rather than sonifying the data directly. This approach to sonification was first proposed by Herman and Ritter \cite{Model-Based}. Based on interactive concepts from parameter mapping, Bovermann et al. \cite{TANGIBLESCANNING} suggested an interactive tool to change the sequence in which sonified data is presented. They propose the use of a tangible plane that can be oriented and positioned using touch interaction or sliders, allowing for the intersection of plane and data points. Intersection excites data objects enabling interactive auditory brushing. A similar approach employs the principal curve technique to compute a smooth path through the data set \cite{Hermann2000PrincipalCS}. This path can then be navigated by the user sequentially, providing audio feedback. Both techniques are heavily dependent on the density and distribution of the data.

% Lenses
Interactive lenses are a well-established class of visualization methods that facilitate multi-faceted data exploration \cite{Toolglass}. These lenses allow users to temporarily modify the visualization to display more details or different arrangements \cite{Interactive_Lenses}. In particular, lenses can be used to distort edges in graphs, reducing edge congestion, or to distort three-dimensional data, clearing a visual path to the focus \cite{EdgeLens}. Our approach combines established lens techniques with the power of sonification. Herman and Ritter \cite{Model-Based} proposed using an advancing shock wave to sonify data points that intersect with the wave front. This technique was later expanded upon with multi-touch interactions \cite{Tnnermann2009MultitouchIF}. This approach to sonification is particularly effective for lenses, as shown by Enge et al. \cite{SoniScope20221095}, where a shock wave was used within the lens to sonify intersecting data points in focus. However, these approaches are less suitable for line charts as they do not effectively convey relationship information.

\section{Line Harp}
% Why use sonification in line chrats and in general?
While previous research has explored sonification of line charts using mostly static parameter mapping \cite{Drawing_by_Ear, VisualGraphs}, we believe that an interactive approach is more suitable for the characteristics of dense line charts. We base our approach on the importance-driven visualization technique for dense line charts by Trautner and Bruckner \cite{LineWeaver}. This method uses the notion of an importance function associated with each line, which allows them to interweave individual lines such that the most important lines segments occlude those with lower importance. Similarly, we use importance to control the amplitude such that more important line segments will be played louder than those with a lower importance value. The frequency of sound generated by plucking a string in a real-world musical instrument is influenced by various factors, such as the material properties of the string and the tension it is under. For the purpose of sonification, we use frequency to encode the directionality of the corresponding line segment. In line charts, with their common left-to-right reading order, this is relevant in the identification of salient features such as trends (e.g., an upwards or downwards development in a time series). 


\subsection{Sonification}

Following Trautner and Bruckner \cite{LineWeaver}, we regard line data as set $D = \{L_1, L_2, ... , L_N\}$ of $N$ polylines with its members $L_i = (P_1, P_2, ... , P_M)$ represented as tuples of $M$ ordered two-dimensional points $P_i = (x_i, y_i)$. The resulting parametric curve $l_i(u)$ of each member is a polyline generated by linear interpolation between its associated points. Furthermore, we use their notion of an importance function $\beta_i(u) \in [0,1]$ which associates a scalar importance value with every position along the curve. In practice, there are many different ways to define importance, such as using underlying data, results of a features detection algorithm, or fundamental properties of the lines. In their paper, for instance, Trautner and Bruckner \cite{LineWeaver} present an algorithm that generates importance values based on a heuristic optimization of screen space utilization for multiple sets of lines within the same chart, and then use this information for importance-driven blending~\cite{Bruckner-2010-HVC}.

"Plucking" such a line at any point generates a note defined by its frequency, amplitude and decay, where decay describes how the sound changes from peak amplitude to zero. As previously mentioned, we use the direction of a line segment to control the frequency of the generated note. In practice, frequencies are mapped to angles as illustrated in Figure~\ref{fig:angleMapping}. As common in line charts, we assume a left-to-right reading order, and hence straight up and down lines are mapped to the highest and lowest frequencies, respectively. In visually dense line charts, and especially parallel coordinates \cite{STAR_PC}, overplotting and overcrowding is a major issue. The issue also applies to sonified dense line charts. We address this by using the importance value to control the amplitude of the note, i.e., lines that are visually occluded due to their lower importance, will also be deprioritized in the sonification.

% Figure environment removed



% How notes are accumelated
In most cases, auditory feedback is presented sequentially, which essentially requires the user to wait for the next note. However, this approach is not ideal as it contradicts the swift response time of visual interactions, which typically require a response within 50--100$\mskip\thinmuskip$ms \cite{Tominski20IVDA}. In musical terms this means playing notes within 1200--600$\mskip\thinmuskip$bpm (beats per minute). A study on auditory inspection time (IT), where participants where tasked with discerning the difference between a high tone and a low tone, found that 95 percent responded correctly when the frequency changed every 100--200$\mskip\thinmuskip$ms~\cite{ClockingTheMind}. Although this study focused on discerning two easily distinguishable frequencies, its findings suggest that slower changes in frequencies may not always be optimal for auditory feedback in interactive contexts like ours.



% Figure environment removed

We address this issue by the fact that most periodically vibrating objects to which we attribute frequency, including the human vocal folds and the strings of musical instruments, vibrate at several sinusoidal component frequencies simultaneously called the harmonic spectrum. Although each of these frequencies sounded alone has unique frequencies, when sounded simultaneously they perceptually fuse and collectively evoke a singular periodic frequency \cite{Music_Perception_2002}. This concept also applies to our analogy of a harp, where producing musical chords requires the plucking of multiple strings in combination. Practically speaking, this involves accumulating the sinusoidal components of all currently active notes, with "active notes" referring to all objects that are currently vibrating and producing sound. Objects that can produce sound, such as harp strings, have a more complex spectrum compared to pure sine waves. This complexity makes them more pleasant to listen to and easier to perceive \cite{Ramloll2001UsingNS}. In our approach we therefore use synthesized instruments to produce sound.

% What instrument is used
In music and sound, the envelope describes how a sound changes over time, and different instruments may have varying envelopes (see Figure \ref{fig:envelope}). For example, a piano key, when struck and held, creates a near-immediate initial sound which gradually decreases in volume to zero. To convey the acoustic impression of our lines, we opted to use the sound of a plucked string instrument. In previous studies, researchers have taken various approaches, such as using different instruments to represent distinct lines \cite{Drawing_by_Ear}, or assigning different instruments to separate clusters \cite{SONIFICATION_DENSEDATA}. In contrast, our approach involves using a single instrument that maintains consistency throughout, with only the frequency and amplitude varying. This approach was taken due to the fact that not all instruments are capable of producing all frequencies, and even when they can, the resulting sound may vary based on the frequency \cite{guidelinesForEarcons}. Furthermore, the plucked string also conveys a clear pitch and has a fitting decay for our purposes.

% Figure environment removed


% Dynamic amplitude
In the context of a plucked string instrument, the decay, or duration, refers to the amount of time it takes for the amplitude to decrease from its peak to zero. An AD (Attack, Decay) envelope is commonly used for these types of instruments, with attack referring to the time taken from key press to peak amplitude. For our purposes, we want a near-immediate initial sound (Attack) and a relatively quick initial decay that gradually fades away to zero.  This envelope is illustrated in Figure \ref{fig:envelope}. However, a simple accumulation of the produced sound would lead to audio clipping and distorted output, as seen in the lower part of Figure~\ref{fig:comparison}, as our approach does not wait for each note to finish. To address this issue, we employ a dynamically scaled amplitude and decay. Specifically, the amplitude is scaled based on the total cumulative amplitude of all active notes, while the decay is proportionally reduced based on the number of active notes being played. In practice, this means that when playing loud (important) lines quickly, they do not become overly loud, while only playing  lines that are less important still results in a relatively loud sound. This ensures a balanced audio output that still accurately reflects the relative importance and density of the data. The dynamic decay ensures that transitions between different volume and frequency levels occur more quickly when more notes are played, making the output more responsive to changes caused by interaction. As a result, the audio feedback produced by the system accents the first note slightly louder and makes the last note slightly longer \cite{guidelinesForEarcons}. This concept is further demonstrated in Figure \ref{fig:combined}.  


%If we assume that a note  $n$ has corresponding amplitudes $n_a = [0.0, 1.0]$ and produce sound $n_s = [-1.0 , 1.0]$. We can define a list of active notes as $N = \{n_{(1,1)}, n_{(2,2)}, ... , n_{(a,s)}\}$.  We then get a final audio output $out$ as: 
%\begin{equation}
%    out = \sum_{a,s} n_s \cdot (n_a \cdot \frac{m}{\sum_{a} n_a})
%\end{equation}
%See https://w2.mat.ucsb.edu/gamma/doxygen/html/\_effects\_8h\_source.html) for pluck() implementation 
%where $m$ refers the the maximum amplitude, which should be less than $1.0$, as higher values produce clipping. 


\subsection{Interaction}
% Sets up how the lens works

Per default, whenever the mouse cursor is moved, we "pluck" all line segments that intersect the current location and generate notes according to the importance values at the point of intersection. The currently played lines are visually indicated using a vibration effect, to reinforce the musical instrument analogy, and can optionally be highlighted using color. 

We also offer several additional interaction facilities controlled by different key bindings. For example, in order to investigate regions where multiple clusters intersect, we offer an additional lens mode. The lens allows users to reveal lines with lower importance values. Visually, we employ a variation of the distortion lens approach by Carpendale et al.~\cite{Distortion3D} to displace lines with importance values above a certain threshold, while sonically, this amounts to only playing notes for lines below the threshold. Furthermore, we also provide a lens playback feature, which sonifies every line in the lens in importance order. This provides a quick and easy way to gain an overview of all lines within the lens radius.


% Figure environment removed


\section{Implementation}

Our approach was implemented in C++ and OpenGL. The sonification uses the Gamma library (\url{https://github.com/LancePutnam/Gamma}), which is a generic synthesis library for C++. Gamma handles audio in a separate thread from the visuals, allowing for seamless integration. The library contains a \verb|Pluck| class, which produces a plucked string sound from a specified frequency. To simplify this process, we created a \verb|Note| class that encapsulates both the frequency and amplitude parameters. All active notes are then stored in a buffer that is accessed by separate threads to add notes, accumulate the final audio output, and remove notes once they have been played. The buffer ensures that data is available when required, allowing the visual and audio threads to operate independently without interference or data loss.

Our complete source code is available at: \url{https://github.com/Egglis/LineHarp}

\section{Results and Discussion}\label{sec:results}

% Why we have usage examples

% How we visualize the audio (Optional section)
In order to visualize the audio produced by our method, we employ the popular software package Audacity to record and visualize the audio output. In addition to common waveform visualizations, which plot amplitude over time (as shown, for example, in Figures \ref{fig:comparison} and \ref{fig:combined}), we also utilize Audacity's spectrogram view using the Enhanced Autocorrelation (EAC) algorithm~\cite{audacity}, as shown in Figure~\ref{fig:angleMapping} as well as the bottom right of Figures~\ref{fig:teaser} and \ref{fig:exampleLens}. The spectrogram highlights the contour of the fundamental frequency (musical pitch) of the audio, characterizing the perceived frequency of the sound. Additionally, we refer to the supplementary material for an audiovisual demonstration of our approach.

% Why and What are the example
Identifying clusters in dense line charts and parallel coordinates is a common task, and analyzing their trends and features is equally vital in many cases. We use an example dataset that contains four distinct clusters with varying features, as illustrated in Figure \ref{fig:teaser}. The dataset also includes 150 random lines (dark green), which are of lower importance compared to the clustered lines. The speed of the mouse movement is an important factor in determining the level of detail in our sonification approach. Faster mouse movement will produce a more generalized overview, while a slower movement will generate more detailed audio feedback. In Figure \ref{fig:teaser}, we used a fixed mouse movement speed for a duration of 5 seconds indicated by the red line. The audio output resulting from this mouse movement generates audio feedback from the clusters in the following order: purple, yellow, gray, and brown. The additional audio visualization in Figure \ref{fig:teaser} provides a visual representation of the audible feedback produce by this mouse movement. By examining the visualization, we can see that the amplitude spikes whenever the mouse transitions from one cluster to another, and that the corresponding frequency also changes according to our mapping. Based on the visualization of the audio and the perception of the actual sound, we argue that clusters are distinguishable based on their audio feedback. We also conclude that frequency can be used to help perceive angles and analyze trends and features.

% Why a lens is useful in practise
When clusters overlap, the audio feedback generated by our sonification approach prioritizes the most important lines. However, this is not ideal when we want to analyze obscured clusters, as shown in Figure \ref{fig:exampleLens}, where the gray cluster is rendered on top of the yellow cluster. For the visual channel this can be solved with the use of lenses that displace overlapping clusters \cite{Toolglass, Distortion3D}. Similarly, the audio channel also requires a function to filter and focus the audio feedback. Figure \ref{fig:exampleLens} shows how our lens feature can be utilized for filtering and focusing, based on a section of a dataset that has two overlapping clusters.
% Figure environment removed

To operate our lens, users can adjust the radius using the mouse and manipulate the importance threshold with key presses. When the threshold is set to visually reveal the yellow cluster, as shown on the left-hand side of Figure \ref{fig:exampleLens}, the corresponding sonification ignores the displaced lines. By gradually altering the threshold, users can inspect the lens region in more detail. Furthermore, using our lens playback feature, all line segments contained within the lens radius are played back in importance order. The resulting audio output, visualized on the right-hand side of Figure \ref{fig:exampleLens}, includes both amplitude and frequency. We see that the audio feedback changes frequency when the iteration reaches the lower second cluster (yellow), allowing users to distinguish between overlapping clusters and providing an indication for their overall directionality as well as their homogeneity.

Based on our experiments, we believe that our approach can support the perception of density variations and clusters in line charts. Furthermore, we believe that the addition of a sonified lens can further improve the sonification by filtering the audio feedback. However, to substantiate our claims, empirical evaluations and comparisons are required. Previous research already suggests that an interactive approach to sonification of line charts is preferable for complex data visualization over direct mappings \cite{TheSonificationHandbook}. For instance, a study that aimed to identify high density areas in complex line charts found that sonification can improve accuracy \cite{SONIFICATION_DENSEDATA}. 

% Discuss Training
Like many sonification models based on complex data visualizations, training is often required for optimal performance. Unlike visual charts, which most humans are taught how to read from a very young age through formal education systems, auditory charts require longer training for optimal usage. While our approach aims to provide an intuitive sonification model, it still suffers from the fact that the audio feedback can be difficult to interpret at times. However, studies have shown that training can improve accuracy in point estimation tasks and may even lead to better performance than a standard visual graph with sufficient training \cite{point_estimation_sonification_task, walkerbrief}. How training might impact the accuracy of our model requires further evaluation, but generally accuracy should improve over time.  

Finally, our approach suffers from a lack of auditory context (such as axes or tick marks) when using a mouse interface, which can make orientation difficult. This can be partially remedied by using a touch screen, where a user will be able detect boundaries and location due to human proprioception, the ability to sense self-movement, force, and body position \cite{Proprioception}. This has also been found to be just as effective for people with reduced vision \cite{Balance_in_the_Blind}. 

\section{Conclusion}

In this work, we introduced Line Harp, a new sonification approach for dense line charts that combines interactive audio feedback with visual representations of data. We presented an importance-driven sonification method that combines a frequency-based encoding of line direction with interactive lenses and provides a way to focus the audio output. Our directional frequency mapping supports line chart angle perception while also serving as an aid for chart navigation. Furthermore, we proposed a technique  that dynamically scales amplitudes to emphasize clustered lines and reduce the overall influence of less important lines to improve density perception. Overall, Line Harp is a promising tool that could potentially enhance the accessibility of visualizations for individuals with visual impairments, or provide a more immersive experience for data analysts. In future work, we aim to delve deeper into the realm of sonification for data analysis, including conducting empirical studies, in order to fully explore and harness its potential for enhancing accessibility and perception of dense visualizations.


%\bibliographystyle{abbrv}
\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}
%\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}

\bibliography{template}
\end{document}
