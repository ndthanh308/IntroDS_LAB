\section{Implementation}
\label{sec:implementation}

Our method design is agnostic to specific datasets or models. In this section, we introduce the specific datasets (Sec.~\ref{sub:implementation:datasets}) and models (Sec.~\ref{sub:implementation:models}) involved in our experiments. In particular, we highlight our instructional-finetuned open-source models Mental-Alpaca and Mental-FLAN-T5 (Sec.~\ref{subsub:implementation:models:mental}).
\review{
We also provide an overview of our experiment setup and evaluation metrics (Sec.~\ref{sub:implementation:setup}).}

\subsection{Datasets and Tasks}
\label{sub:implementation:datasets}

\review{
Our experiment is based on four well-established datasets that are commonly employed for mental health analysis.
These datasets were collected from Reddit due to their high-quality and availability.
It is noteworthy that we intentionally avoid using datasets with weak labels based on specific linguistic patterns (\eg whether a user ever stated ``I was diagnosed with X'').}
Instead, we used ones with human expert annotations or supervision.
We define six diverse mental health prediction tasks based on these datasets.

\begin{s_itemize}
\item \textbf{Dreaddit}~\cite{turcan_dreaddit_2019}: \minorreview{This dataset collected posts via Reddit PRAW API~\cite{Praw-Dev} from Jan 1, 2017 to Nov 19, 2018, which contains ten subreddits in the five domains (abuse, social, anxiety, PTSD, and financial) and includes 2929 users' posts}. Multiple human annotators rated whether sentence segments showed the stress of the poster, and the annotations were aggregated to generate final labels. We used this dataset for a post-level binary stress prediction (\textbf{Task 1}).
\item \textbf{DepSeverity}~\cite{naseem_early_2022}: This dataset leveraged the same posts collected in \cite{turcan_dreaddit_2019}, but with a different focus on depression. Two human annotators followed DSM-5~\cite{regier2013dsm} and categorized posts into four levels of depression: minimal, mild, moderate, and severe. We employed this dataset for two post-level tasks: binary depression prediction (\ie whether a post showed at least mild depression, \textbf{Task 2}), and four-level depression prediction (\textbf{Task 3}).
\item \textbf{SDCNL}~\cite{haque_deep_2021}: \minorreview{This dataset also collected posts from Python Reddit API, including r/SuicideWatch and r/Depressionfrom 1723 users}. Through manual annotation, they labeled whether each post showed suicidal thoughts. We employed this dataset for the post-level binary suicide ideation prediction (\textbf{Task 4}).
\item \textbf{CSSRS-Suicide}~\cite{gaur_knowledge-aware_2019}: \minorreview{This dataset contains posts from 15 mental health-related subreddits from 2181 users between 2005 and 2016}. Four practicing psychiatrists followed Columbia Suicide Severity Rating Scale (C-SSRS) guidelines~\cite{posner2008columbia} to manually annotate 500 users on suicide risks in five levels: supportive, indicator, ideation, behavior, and attempt. We leveraged this dataset for two user-level tasks: binary suicide risk prediction (\ie whether a user showed at least suicide indicator, \textbf{Task 5}), and five-level suicide risk prediction (\textbf{Task 6}).
\end{s_itemize}


In order to test the generalizability of our methods, we also leveraged three other datasets from various platforms. Similarly, all datasets contain human annotations as labels.
\begin{s_itemize}
\item \textbf{Red-Sam}~\cite{kalinathan_data_2022,kayalvizhi2022findings}: \minorreview{This dataset also collected posts with PRAW API~\cite{Praw-Dev} , involving five subreddits (Mental Health, depression, loneliness, stress, anxiety).} Two domain experts' annotations were aggregated to generate depression labels. We used this dataset as an external evaluation dataset on binary depression detection (\textbf{Task 2}). Although also from Reddit, this dataset was not involved in few-shot learning or instruction finetuning. We cross-checked datasets to ensure there were no overlapping posts.
\item \textbf{Twt-60Users}~\cite{jamil_monitoring_2017}: \minorreview{This dataset collected twitters from 60 users during 2015 with Twitter API. Two human annotators labeled every tweet with depression labels}. We used this non-Reddit dataset as an external evaluation dataset on depression detection (\textbf{Task 2}). Note that this dataset has imbalanced labels (90.7\% False), as most tweets did not indicate mental distress.
\item \textbf{SAD}~\cite{mauriello_sad_2021}: This dataset contains SMS-like text messages with nine types of daily stressor categories (work, school, financial problem, emotional turmoil, social relationships, family issues, health, everyday decision-making, and other). \minorreview{These messages were written by 3578 humans.} We used this non-Reddit dataset as an external evaluation dataset on binary stress detection (\textbf{Task 1}). Note that human crowd-workers write the messages under certain stressor-triggered instructions. Therefore, this dataset has imbalanced labels on the other side (94.0\% True).
\end{s_itemize}

\review{Table~\ref{tab:datasets} summarizes the information of the seven datasets and six mental health prediction tasks.}
% ~\footnote{Our training/finetuning datasets are mainly based on the Reddit platform (with high-quality labels) due to their availability. We include Twitter  Other high-quality datasets are hard to obtain due to their proprietary nature. We discuss this limitation in Sec.~\ref{sec:discussion}.}
For each dataset, we conducted an 80\%/20\% train-test split. Notably, to avoid data leakage, each user's data were placed exclusively in either the training or test set.
 
\input{fig_tab_alg/tab_dataset}

\subsection{Models}
\label{sub:implementation:models}
We experimented with multiple LLMs with different sizes, pre-training targets, and availability.

\begin{s_itemize}
\item \textbf{Alpaca} (7B)~\cite{taori_stanford_2023}: An open-source large model finetuned from another open-sourced LLaMA 7B model~\cite{touvron_llama_2023} on instruction following demonstrations. Experiments have shown that Alpaca behaves qualitatively similarly to OpenAIâ€™s \texttt{text-davinci-003} on certain task metrics. We choose the relatively small 7B version to facilitate running and finetuning on consumer hardware.
\item \textbf{Alpaca-LoRA} (7B)~\cite{hu_lora_2021}: Another open-source large model finetuned from LLaMA 7B model using the same dataset as Alpaca~\cite{taori_stanford_2023}. This model leverages a different finetuning technique called low-rank adaptation (LoRA)~\cite{hu_lora_2021}, with the goal of reducing finetuning cost by freezing the model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. Despite the similarity in names, it is important to note that Alpaca-LoRA is entirely distinct from Alpaca. They are trained on the same dataset but with different methods.
\item \textbf{FLAN-T5} (11B)~\cite{chung_scaling_2022}: An open-source large model T5~\cite{raffel_exploring_2020} finetuned with a variety of task-based datasets on instructions. Compared to other LLMs, FLAN-T5 focuses more on task solving and is less optimized for natural language or dialogue generation. We picked the largest version of FLAN-T5 (\ie FLAN-T5-XXL), which has a comparable size of Alpaca.
\item \review{\textbf{LLaMA2} (70B)~\cite{touvron_llama_2023-1}: A recent open-source large model released by Meta. We picked the largest version of LLaMA2, whose size is between FLAN-T5 and GPT-3.5.}
\item \textbf{GPT-3.5} (175B)~\cite{noauthor_introducing_2022}: This large model is closed-source and available through API provided by OpenAI. We picked the \texttt{gpt-3.5-turbo}, one of the most capable and cost-effective models in the GPT-3.5 family.
\item \textbf{GPT-4} (1700B)~\cite{bubeck_sparks_2023}: This is the largest closed-source model available through OpenAI API. We picked the \texttt{gpt-4-0613}. Due to the limited availability of API, the cost of finetuning GPT-3.5 or GPT-4 is prohibitive.
\end{s_itemize}

\review{It is worth noting that Alpaca, Alpaca-LoRA, GPT-3.5, LLaMA2 and GPT-4 are all finetuned with natural dialogue as one of the optimization goals.} In contrast, FLAN-T5 is more focused on task-solving.
In our case, the user-written input posts resemble natural dialogue, whereas the mental health prediction tasks are defined as specific classification tasks. It is unclear and thus interesting to explore which LLM fits better with our goal.

\subsubsection{Mental-Alpaca \& Mental-FLAN-T5}
\label{subsub:implementation:models:mental}
Our methods of zero-shot prompting (Sec.~\ref{sub:methods:zero-shot}) and few-shot prompting (Sec.~\ref{sub:methods:few-shot}) do not update model parameters during the experiment.
In contrast, instruction finetuning (Sec.~\ref{sub:methods:finetuning}) will update model parameters and generate new models.
To enhance their capability in the mental health domains, we update Alpaca and FLAN-T5 on six tasks across the four datasets in Sec.~\ref{sub:implementation:datasets} using the multi-dataset instruction finetuning method (Sec.~\ref{subsub:methods:finetuning:multi-dataset}), which leads to our new model \textit{Mental-Alpaca} and \textit{Mental-FLAN-T5}.

% We open-source the finetuning pipeline and release the model parameter difference from LLaMA\footnote{Due to the open-source legislation of LLaMA, we cannot release model parameters directly. We adopt a similar practice of Stanford Alpaca~\cite{taori_stanford_2023} and only the parameter difference.} at [github link].

\review{
\subsection{Experiment Setup and Metrics}
\label{sub:implementation:setup}
For zero-shot and few-shot prompting methods, we load open-source models (Alpaca, Alpaca-LoRA, FLAN-T5, LLaMA2) with one to eight Nvidia A100 GPUs to do the tasks, depending on the size of the model. For closed-source models (GPT-3.5, and GPT-4), we use OpenAI API to conduct chat completion tasks.
}

\review{
As for finetuning \textit{Mental-Alpaca} and \textit{Mental-FLAN-T5}, we merge the four datasets together and provide instructions for all six tasks (in the training set). We use eight Nvidia A100 GPUs for instruction finetuning. With cross entropy as the loss function, we backpropagate and update model parameters in 3 epochs, with Adam optimizer and a learning rate as 2$e^{-5}$ (cosine scheduler, warmup ratio 0.03).
}

\review{
We focus on balanced accuracy as the main evaluation metric, \ie the mean of sensitivity (true positive rate) and specificity (true negative rate).
We picked this metric since it is more robust to class imbalance compared to the accuracy or F1 score~\cite{brodersen2010balanced,xu_globem_2022-1}.
It is noteworthy that the sizes of LLMs we compare are vastly different, with the number of parameters ranging from 7B to 1700B. A larger model is usually expected to have a better overall performance than a smaller model. We inspect whether this expectation holds in our experiments.
}