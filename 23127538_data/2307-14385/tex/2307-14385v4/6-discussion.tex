\section{Discussion}
\label{sec:discussion}
Our experiment results reveal a number of interesting findings. In this section, we discuss potential guidelines for enabling LLMs for mental -health-related tasks (Sec.~\ref{sub:discussion:guidelines}). We envision promising future directions (Sec.~\ref{sub:discussion:beyond}), while highlighting important ethical concerns and limitations with LLMs for mental health (Sec.~\ref{sub:discussion:ethics}). We also summarize the limitations of the current work (Sec.~\ref{sub:discussion:limitation}).

\subsection{Guidelines for Empowering LLMs for Mental Health Prediction Tasks}
\label{sub:discussion:guidelines}
We extract and summarize the takeaways from Sec.~\ref{sec:results} into a set of guidelines for future researchers and practitioners on how to empower LLMs to be better at various mental health prediction tasks.

\textbf{When computing resources are limited, combine prompt design \& few-shot prompting, and pick prompts carefully.}
As the size of large models continues to grow, the requirement for hardware (mainly GPU) has also been increasing, especially when finetuning an LLM. For example, in our experiment, Alpaca was trained on eight 80GB A100s for three hours~\cite{taori_stanford_2023}.
With limited computing resources, only running inferences or resorting to APIs is feasible. In these cases, zero-shot and few-shot prompt engineering strategies are viable options. Our results indicate that providing few-shot mental health examples with appropriate enhancement strategies can effectively improve prediction performance.
Specifically, adding contextual information about the online text data is always helpful. If the available model is large and contains rich knowledge (at least 7B trainable parameters), adding mental health domain information can also be beneficial.

% Although there has been some recent research exploring low-cost training/finetuning for LLMs (\eg LoRA~\cite{hu_lora_2021}), it is

\textbf{With enough computing resources, instruction finetune models on various mental health datasets.}
When there are enough computing resources and model training/finetuning is possible, there are more options to enhance LLMs for mental health prediction tasks.
Our experiments clearly show that instruction finetuning can significantly boost the performance of models, especially for dialogue-based models since they can better understand and learn from human natural language.
\review{When there are multiple datasets available, merging multiple datasets and tasks altogether and finetuning the model in a single round is the most effective approach to enhance its generalizability.
}

\textbf{Implement efficient finetuning with hundreds of examples and prioritize data variation when data resource is limited.}
Figure~\ref{fig:performance_datasize} shows that finetuning does not require large datasets. If there is no immediately available dataset, collecting small datasets with a few hundred samples is often good enough.
\review{
Moreover, when the overall amount of data is limited (\eg due to resource constraints), it is more advantageous to collect data from a variety of sources, each with a smaller size, than to collect a single larger dataset. Because instruction finetuning generalizes better when data and tasks have a larger variation. 
}

% \textbf{Carefully calibrate the model when applying it to a new task.}
% After instruction finetuning, a model still needs careful scrutiny on a new task, since our experiments suggest that LLMs' generalizability on unseen tasks still needs further improvement.
% Testing the model on a few samples with ground truth can be an easy option to investigate its performance on the new task. If the performance is not promising, it may suggest another finetuning on the new task.

\textbf{More curated finetuning datasets are needed for mental health reasoning.}
Our case study suggests that Mental-Alpaca and Mental-FLAN-T5 can only generate classification labels after being finetuned solely on classification tasks, losing their reasoning capability. This is a major limitation of the current models.
A potential solution involves incorporating more datasets focused on reasoning or causality into the instruction finetuning process, so that models can also learn the relationship between mental health outcomes and causal factors.

\review{
\textbf{Limited Prediction and Reasoning Performance for Complex Contexts.}
LLMs tend to make more mistakes when the conversation contexts are more complex~\cite{bhattacharya2023context,li2023compressing}.
Our results contextualize this in the mental health domain.
Section~\ref{subsub:results:reasoning:wrong} shows an example case where most LLMs not only predict incorrectly but also provide flawed reasoning processes.
Further analysis of mispredicted instances indicates a recurring difficulty: LLMs often err when there's a disconnect between the literal context and the underlying real-life scenarios. 
he example in Figure~\ref{fig:reasoning_example3} is a case where LLMs are confused by the hypothetical stressful case described by the person.
In another example, all LLMs incorrectly assess a person with severe depression (false negative): ``\textit{I'm just blown away by this doctor's willingness to help. I feel so validated every time I leave his office, like someone actually understands what I'm struggling with, and I don't have to convince them of my mental illness. Bottom line? Research docs if you can online, read their reviews and don't give up until you find someone who treats you the way you deserve. If I can do this, I promise you can!}'' 
Here, LLMs are misled by the outwardly positive sentiment, overlooking the significant cues such as regular doctor visits and explicit mentions of mental illness.
These observations underscore a critical shortfall of LLMs: they cannot handle complex mental health-related tasks, particularly those concerning chronic conditions like depression. The variability of human expressions over time and the models' susceptibility to being swayed by superficial text rather than underlying scenarios present significant challenges.
}

Despite the promising capability of LLMs for mental health tasks, they are still far from being deployable in the real world.
Our experiments reveal the encouraging performance of LLMs on mental health prediction and reasoning tasks.
However, as we note in Sec.~\ref{sub:discussion:ethics}, our current results do not indicate LLMs' deployability in real-life mental health settings. There are many important ethical and safety concerns and gaps before deployment to be addressed before achieving robustness and deployability.

\subsection{Beyond Mental Health Prediction Task and Online Text Data}
\label{sub:discussion:beyond}
Our current experiments mainly involve mental health prediction tasks, which are essentially classification problems. There are more types of tasks that our experiments don't cover, such as regression (\eg predicting a score on a mental health scale). In particular, reasoning is an attractive task as it can fully leverage the capability of LLMs on language generation~\cite{nori_capabilities_2023,bubeck_sparks_2023}. Our initial case study on reasoning is limited but reveals promising performance, especially for large models such as GPT-4. We plan to conduct more experiments on tasks that go beyond classification.


In addition, there is another potential extension direction. In this paper, we mainly focus on online text data, which is one of the important data sources of the ubiquitous computing ecosystem.
\minorreview{However, there are more available data streams that contain rich information, such as the multimodal sensor data from mobile phones and wearables (\eg \cite{xu_globem_2022, nepal2022survey,xu_understanding_2021,morais2023classification,JAbrantes_2023}).}
This leads to another open question on how to leverage LLMs for time-series sensor data. More research is needed to explore potential methods to merge the online text information with sensor streams. These are another set of exciting research questions to explore in future work.


\subsection{Ethics in LLMs and Deployability Gaps for Mental Health}
\label{sub:discussion:ethics}
Although our experiments on LLMs have shown promising capability for mental-health-related tasks, it still has a long way to go before being deployed in real-life systems. Recent research has revealed the potential bias or even harmful advice introduced by LLMs~\cite{Hoover_2023}, especially with the gender~\cite{ghosh2023chatgpt} and racial~\cite{abid2021persistent} gaps.
In mental health, these gaps and disparities between population groups have been long-standing~\cite{irene_y_chen_can_2019}.
Meanwhile, our case study of incorrect prediction, over-generalization, and ``falsely reasonable'' explanations further highlight the risk of current LLMs.
Recent studies are calling for more research emphasis and efforts in assessing and mitigating these biases for mental health~\cite{irene_y_chen_can_2019,timmons2022call}.

Although with a much stronger capability of understanding natural language (and early signs of mental health domain knowledge in our case), LLMs are no different from other modern AI models that are trained on a large amount of human-generated content, which exhibit all the biases that humans do~\cite{irene_y_chen_ethical_2021,ntoutsi2020bias,wang2020human}.
Meanwhile, although we carefully picked datasets with human expert annotations, there exist potential biases in the labels, such as stereotypes~\cite{pessach2022review}, confirmation bias~\cite{gemalmaz2021accounting}, normative \vs descriptive labels~\cite{balagopalan_judging_2023}. 
Besides, privacy is another important concern. Although our datasets are based on public social media platforms, it is necessary to carefully handle mental-health-related data and guarantee anonymity in any future efforts.
These ethical concerns need to receive attention not only at the monitoring and prediction stage, but also in the downstream applications, ranging from assistants for mental health experts to chatbots for end-users.
Careful efforts into safe development, auditing, and regulation are very much needed to address these ethical risks.


\subsection{Limitations}
\label{sub:discussion:limitation}
Our paper has a few limitations. First, although we carefully inspect the quality of our dataset and cover different categories of LLM, the the range of datasets and the types of LLMs included are still limited. Our findings are based on the observations of these datasets and models, which may not generalize to other cases.
Related, our exploration of zero-shot few-shot prompt design is not comprehensive.  The limited input window of some models also limits our exploration of more samples for few-shot prompting.
Furthermore, we have not conducted a systematic evaluation of these models' performance in mental health reasoning.
Future work can design larger-scale experiments to include more datasets, models, prompt designs, and better evaluation.

\minorreview{Second, our datasets were mainly from Reddit, which could be limited. Although our analysis in Section~\ref{subsub:results:finetune:generalizability} shows that finetuned models have cross-platform generalizability, the finetuning was only based on Reddit and can introduce bias.} Meanwhile, although the labels are not directly accessible on the platforms, it is possible that these text data have been included in the initial training of these large models. We still argue that there is little information leakage as long as the models haven't seen the labels for our tasks, but it is hard to measure how the initial training process may affect the outcomes in our evaluation.

Third, another important limitation of the current work is the lack of evaluation of model fairness. Our anonymous datasets do not include comprehensive demographic information, making it hard to compare the performance across different population groups. As we discussed in the previous section, lots of future work on ethics and fairness is necessary before deploying such systems in real life.