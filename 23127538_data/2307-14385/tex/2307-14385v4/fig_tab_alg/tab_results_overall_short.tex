% \vspace{-0.5cm}
\begin{table}[!t]
\centering
\caption{
\review{Balanced Accuracy Performance Summary of Zero-shot, Few-shot and Instruction Finetuning on LLMs.
$ZS_{best}$ highlights the best performance among zero-shot prompt designs, including context enhancement, mental health enhancement, and their combination (see Table.~\ref{tab:prompt_design}). Detailed results can be found in Table~\ref{tab:results_overall} in Appendix.
Small numbers represent standard deviation across different designs of \textit{Prompt}$_{\textit{Part1-S}}$ and \textit{Prompt}$_{\textit{Part2-Q}}$. The baselines at the bottom rows do not have standard deviation as the task-specific output is static, and prompt designs do not apply.
Due to the maximum token size limit, we only conduct few-shot prompting on a subset of datasets and mark other infeasible datasets as ``--''.
For each column, the best result is \textbf{bolded}, and the second best is \underline{underlined}.
}
}
\vspace{-0.3cm}
\label{tab:results_overall_short}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{llcccccc}
\thickhlinespace
& \makecell[r]{\textbf{Dataset}} & \textbf{Dreaddit} & \multicolumn{2}{c}{\textbf{DepSeverity}} & \textbf{SDCNL} & \multicolumn{2}{c}{\textbf{CSSRS-Suicide}} \\ \addlinespace[1ex]
\textbf{Category} & \textbf{Model}   & \textbf{Task \#1}       & \textbf{Task \#2}             & \textbf{Task \#3}             & \textbf{Task \#4}    & \textbf{Task \#5}     & \textbf{Task \#6} \\ \thickhlinespace
\multirow{16}{*}{\makecell{Zero-shot\\Prompting}} & Alpaca$_{ZS}$         & 0.593$_{\pm0.039}$ & 0.522$_{\pm0.022}$ & 0.431$_{\pm0.050}$ & 0.493$_{\pm0.007}$ & 0.518$_{\pm0.037}$ & 0.232$_{\pm0.076}$ \\
& Alpaca$_{ZS\_best}$ & 0.612$_{\pm0.065}$ & 0.577$_{\pm0.028}$ & 0.454$_{\pm0.143}$ & 0.532$_{\pm0.005}$ & 0.532$_{\pm0.033}$ & 0.250$_{\pm0.060}$ \\ \cdashlinespace{2-8}
& Alpaca-LoRA$_{ZS}$           & 0.571$_{\pm0.043}$ & 0.548$_{\pm0.027}$ & 0.437$_{\pm0.044}$ & 0.502$_{\pm0.011}$ & 0.540$_{\pm0.012}$ & 0.187$_{\pm0.053}$ \\
& Alpaca-LoRA$_{ZS\_best}$   & 0.571$_{\pm0.043}$ & 0.548$_{\pm0.027}$ & 0.437$_{\pm0.044}$ & 0.502$_{\pm0.011}$ & 0.567$_{\pm0.038}$ & 0.224$_{\pm0.049}$ \\ \cdashlinespace{2-8}
 & FLAN-T5$_{ZS}$ & 0.659$_{\pm0.086}$ & 0.664$_{\pm0.011}$ & 0.396$_{\pm0.006}$ & 0.643$_{\pm0.021}$ & 0.667$_{\pm0.023}$ & 0.418$_{\pm0.012}$ \\
 & FLAN-T5$_{ZS\_best}$ & 0.663$_{\pm0.079}$ & 0.674$_{\pm0.014}$ & 0.396$_{\pm0.006}$ & 0.653$_{\pm0.011}$ & 0.667$_{\pm0.023}$ & 0.418$_{\pm0.012}$ \\ \cdashlinespace{2-8}
& LLaMA2$_{ZS}$ & 0.720$_{\pm0.012}$ & 0.693$_{\pm0.034}$  & 0.429$_{\pm0.013}$ & 0.589$_{\pm0.010}$ & 0.691$_{\pm0.014}$ & 0.261$_{\pm0.018}$ \\
& LLaMA2$_{ZS\_best}$ & 0.720$_{\pm0.012}$ & 0.711$_{\pm0.033}$ & 0.444$_{\pm0.021}$ & 0.643$_{\pm0.014}$ & 0.722$_{\pm0.039}$ & 0.367$_{\pm0.043}$ \\ \cdashlinespace{2-8}
& GPT-3.5$_{ZS}$         & 0.685$_{\pm0.024}$ & 0.642$_{\pm0.017}$ & 0.603$_{\pm0.017}$ & 0.460$_{\pm0.163}$ & 0.570$_{\pm0.118}$ & 0.233$_{\pm0.009}$ \\
& GPT-3.5$_{ZS\_best}$ & 0.688$_{\pm0.045}$ & 0.653$_{\pm0.020}$ & 0.642$_{\pm0.034}$ & 0.632$_{\pm0.020}$ & 0.617$_{\pm0.033}$ & 0.310$_{\pm0.015}$ \\ \cdashlinespace{2-8}
 & GPT-4$_{ZS}$ & 0.700$_{\pm0.001}$ & 0.719$_{\pm0.013}$ & 0.588$_{\pm0.010}$ & 0.644$_{\pm0.007}$ & 0.760$_{\pm0.009}$ & 0.418$_{\pm0.009}$ \\
 & GPT-4$_{ZS\_best}$ & 0.725$_{\pm0.009}$ & 0.719$_{\pm0.013}$ & 0.656$_{\pm0.001}$ & 0.647$_{\pm0.014}$ & 0.760$_{\pm0.009}$ & \underline{0.441}$_{\pm0.057}$ \\
\thickhlinespace
\multirow{4}{*}{\makecell{Few-shot\\Prompting}} &  Alpaca$_{FS}$         & 0.632$_{\pm0.030}$ & 0.529$_{\pm0.017}$ & 0.628$_{\pm0.005}$ & ---                & ---                & ---                \\
& FLAN-T5$_{FS}$         & 0.786$_{\pm0.006}$ & 0.678$_{\pm0.009}$ & 0.432$_{\pm0.009}$ & ---                & ---                & ---                \\
& GPT-3.5$_{FS}$         & 0.721$_{\pm0.010}$ & 0.665$_{\pm0.015}$ & 0.580$_{\pm0.002}$ & ---                & ---                & ---                \\
& GPT-4$_{FS}$         & 0.698$_{\pm0.009}$ & 0.724$_{\pm0.005}$ & 0.613$_{\pm0.001}$ & ---                & ---                & ---                \\\thickhlinespace
\multirow{2}{*}{\makecell{Instructional\\Finetuning}} & Mental-Alpaca         & \underline{0.816}$_{\pm0.006}$ & \underline{0.775}$_{\pm0.006}$ & \underline{0.746}$_{\pm0.005}$ & \textbf{0.724}$_{\pm0.004}$ & 0.730$_{\pm0.048}$ & 0.403$_{\pm0.029}$ \\
& Mental-FLAN-T5 &  0.802$_{\pm0.002}$ & 0.759$_{\pm0.003}$ & \textbf{0.756}$_{\pm0.001}$ & 0.677$_{\pm0.005}$ & \textbf{0.868}$_{\pm0.006}$ & \textbf{0.481}$_{\pm0.006}$ \\ \thickhlinespace
\multirow{3}{*}{\xspace\xspace\xspace Baseline} & Majority          & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.250$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.200$_{\pm ---}$  \\
& BERT              & 0.783$_{\pm ---}$  & 0.763$_{\pm ---}$  & 0.690$_{\pm ---}$  & 0.678$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.332$_{\pm ---}$  \\
& Mental-RoBERTa     & \textbf{0.831}$_{\pm ---}$  & \textbf{0.790}$_{\pm ---}$   & 0.736$_{\pm ---}$  & \underline{0.723}$_{\pm ---}$  & \underline{0.853}$_{\pm ---}$  & 0.373$_{\pm ---}$  \\ \thickhlinespace
\end{tabular}
% \vspace{-0.5cm}
}
\end{table}