\renewcommand{\arraystretch}{1}
% \vspace{-0.5cm}
\begin{table}[!b]
\centering
% \vspace{-0.5cm}
\caption{
\review{Balanced Accuracy Performance Summary of Zero-shot, Few-shot and Instruction Finetuning on LLMs. 
$context$, $mh$, and $both$ indicate the prompt design strategies of context enhancement, mental health enhancement, and their combination  (see Table.~\ref{tab:prompt_design}).
Small numbers represent standard deviation across different designs of \textit{Prompt}$_{\textit{Part1-S}}$ and \textit{Prompt}$_{\textit{Part2-Q}}$. The baselines at the top rows do not have standard deviation as the task-specific output is static, and prompt designs do not apply.
Due to token limit, computation cost, and resource constraints, some infeasible experiments are marked as ``--''.
For each column, the best result is \textbf{bolded}, and the second best is \underline{underlined}.
}
}
% \vspace{-0.3cm}
\label{tab:results_overall}
\resizebox{1\textwidth}{!}{
\begin{tabular}{llccccccccc}
\thickhlinespace
& \makecell[r]{\textbf{Dataset}} & \textbf{Dreaddit} & \multicolumn{2}{c}{\textbf{DepSeverity}} & \textbf{SDCNL} & \multicolumn{2}{c}{\textbf{CSSRS-Suicide}} & \textbf{Red-Sam} & \textbf{Twt-60Users} & \textbf{SAD} \\ \addlinespace[1ex]
\textbf{Category} & \textbf{Model}   & \textbf{Task \#1}       & \textbf{Task \#2}             & \textbf{Task \#3}             & \textbf{Task \#4}    & \textbf{Task \#5}     & \textbf{Task \#6} & \textbf{Task \#2} & \textbf{Task \#2} & \textbf{Task \#1} \\ \thickhlinespace
\multirow{23}{*}{\makecell{Zero-shot\\Prompting}}  & Alpaca$_{ZS}$ & 0.593$_{\pm0.039}$ & 0.522$_{\pm0.022}$ & 0.431$_{\pm0.050}$ & 0.493$_{\pm0.007}$ & 0.518$_{\pm0.037}$ & 0.232$_{\pm0.076}$ & 0.524$_{\pm0.014}$ & 0.521$_{\pm0.022}$ & 0.503$_{\pm0.004}$\\
 & Alpaca$_{ZS-context}$ & 0.612$_{\pm0.065}$ & 0.567$_{\pm0.077}$ & 0.454$_{\pm0.143}$ & 0.497$_{\pm0.006}$ & 0.532$_{\pm0.033}$ & 0.250$_{\pm0.060}$ & 0.525$_{\pm0.019}$ & 0.559$_{\pm0.064}$ & 0.501$_{\pm0.004}$\\
 & Alpaca$_{ZS\_mh}$ & 0.593$_{\pm0.031}$ & 0.577$_{\pm0.028}$ & 0.444$_{\pm0.090}$ & 0.482$_{\pm0.015}$ & 0.523$_{\pm0.013}$ & 0.235$_{\pm0.033}$ & 0.527$_{\pm0.006}$ & 0.569$_{\pm0.017}$ & 0.522$_{\pm0.027}$\\
 & Alpaca$_{ZS\_both}$ & 0.540$_{\pm0.029}$ & 0.559$_{\pm0.040}$ & 0.421$_{\pm0.095}$ & 0.532$_{\pm0.005}$ & 0.511$_{\pm0.011}$ & 0.221$_{\pm0.030}$ & 0.495$_{\pm0.016}$ & 0.499$_{\pm0.004}$ & 0.557$_{\pm0.041}$ \\ \cdashlinespace{2-11}
& Alpaca-LoRA$_{ZS}$ & 0.571$_{\pm0.043}$ & 0.548$_{\pm0.027}$ & 0.437$_{\pm0.044}$ & 0.502$_{\pm0.011}$ & 0.540$_{\pm0.012}$ & 0.187$_{\pm0.053}$ & 0.577$_{\pm0.004}$ & 0.607$_{\pm0.046}$ & 0.477$_{\pm0.016}$\\
 & Alpaca-LoRA$_{ZS_context}$ & 0.537$_{\pm0.047}$ & 0.501$_{\pm0.001}$ & 0.343$_{\pm0.152}$ & 0.472$_{\pm0.020}$ & 0.567$_{\pm0.038}$ & 0.214$_{\pm0.059}$ & 0.535$_{\pm0.017}$ & 0.649$_{\pm0.021}$ & 0.443$_{\pm0.047}$\\
 & Alpaca-LoRA$_{ZS\_mh}$ & 0.500$_{\pm0.000}$ & 0.500$_{\pm0.000}$ & 0.331$_{\pm0.145}$ & 0.497$_{\pm0.025}$ & 0.557$_{\pm0.023}$ & 0.216$_{\pm0.022}$ & 0.541$_{\pm0.016}$ & 0.569$_{\pm0.019}$ & 0.471$_{\pm0.033}$\\
 & Alpaca-LoRA$_{ZS\_both}$ & 0.500$_{\pm0.000}$ & 0.500$_{\pm0.000}$ & 0.386$_{\pm0.059}$ & 0.499$_{\pm0.023}$ & 0.517$_{\pm0.031}$ & 0.224$_{\pm0.049}$ & 0.507$_{\pm0.009}$ & 0.535$_{\pm0.025}$ & 0.420$_{\pm0.019}$\\ \cdashlinespace{2-11}
 & FLAN-T5$_{ZS}$  & 0.659$_{\pm0.086}$ & 0.664$_{\pm0.011}$ & 0.396$_{\pm0.006}$ & 0.643$_{\pm0.021}$ & 0.667$_{\pm0.023}$ & 0.418$_{\pm0.012}$ & 0.554$_{\pm0.034}$ & 0.613$_{\pm0.040}$ & 0.692$_{\pm0.093}$\\
 & FLAN-T5$_{ZS_context}$ & 0.663$_{\pm0.079}$ & 0.674$_{\pm0.014}$ & 0.378$_{\pm0.013}$ & 0.653$_{\pm0.011}$ & 0.649$_{\pm0.026}$ & 0.378$_{\pm0.029}$ & 0.563$_{\pm0.029}$ & 0.613$_{\pm0.046}$ & 0.738$_{\pm0.056}$\\
 & FLAN-T5$_{ZS\_mh}$ & 0.616$_{\pm0.070}$ & 0.666$_{\pm0.009}$ & 0.366$_{\pm0.012}$ & 0.648$_{\pm0.010}$ & 0.653$_{\pm0.018}$ & 0.372$_{\pm0.033}$ & 0.547$_{\pm0.035}$ & 0.613$_{\pm0.033}$ & 0.739$_{\pm0.039}$\\
 & FLAN-T5$_{ZS\_both}$ & 0.604$_{\pm0.074}$ & 0.661$_{\pm0.004}$ & 0.389$_{\pm0.051}$ & 0.645$_{\pm0.005}$ & 0.657$_{\pm0.019}$ & 0.382$_{\pm0.048}$ & 0.536$_{\pm0.027}$ & 0.606$_{\pm0.040}$ & 0.767$_{\pm0.050}$\\ \cdashlinespace{2-11}
 & LLaMA2$_{ZS}$ & 0.720$_{\pm0.012}$ & 0.693$_{\pm0.034}$ & 0.429$_{\pm0.013}$ & 0.589$_{\pm0.010}$ & 0.691$_{\pm0.014}$ & 0.261$_{\pm0.018}$ & 0.574$_{\pm0.008}$ & 0.735$_{\pm0.017}$ & 0.704$_{\pm0.026}$\\
 & LLaMA2$_{ZS_context}$ & 0.658$_{\pm0.025}$ & 0.707$_{\pm0.056}$ & 0.410$_{\pm0.019}$ & 0.588$_{\pm0.026}$ & 0.722$_{\pm0.039}$ & 0.367$_{\pm0.043}$ & 0.562$_{\pm0.011}$ & \underline{0.736}$_{\pm0.019}$ & 0.650$_{\pm0.027}$\\
 & LLaMA2$_{ZS\_mh}$ & 0.617$_{\pm0.012}$ & 0.711$_{\pm0.033}$ & 0.395$_{\pm0.017}$ & 0.642$_{\pm0.008}$ & 0.696$_{\pm0.021}$ & 0.291$_{\pm0.038}$ & 0.572$_{\pm0.012}$ & 0.689$_{\pm0.056}$ & 0.567$_{\pm0.021}$\\
 & LLaMA2$_{ZS\_both}$ & 0.584$_{\pm0.017}$ & 0.704$_{\pm0.036}$ & 0.444$_{\pm0.021}$ & 0.643$_{\pm0.014}$ & 0.689$_{\pm0.043}$ & 0.328$_{\pm0.058}$ & 0.559$_{\pm0.012}$ & 0.692$_{\pm0.069}$ & 0.560$_{\pm0.009}$\\ \cdashlinespace{2-11}
 & GPT-3.5$_{ZS}$ & 0.685$_{\pm0.024}$ & 0.642$_{\pm0.017}$ & 0.603$_{\pm0.017}$ & 0.460$_{\pm0.163}$ & 0.570$_{\pm0.118}$ & 0.233$_{\pm0.009}$ & 0.454$_{\pm0.007}$ & 0.536$_{\pm0.024}$ & 0.717$_{\pm0.017}$\\
 & GPT-3.5$_{ZS_context}$ & 0.688$_{\pm0.045}$ & 0.653$_{\pm0.020}$ & 0.543$_{\pm0.047}$ & 0.618$_{\pm0.008}$ & 0.577$_{\pm0.090}$ & 0.265$_{\pm0.048}$ & 0.473$_{\pm0.001}$ & 0.560$_{\pm0.002}$ & 0.723$_{\pm0.003}$\\
 & GPT-3.5$_{ZS\_mh}$ & 0.679$_{\pm0.017}$ & 0.636$_{\pm0.021}$ & 0.642$_{\pm0.034}$ & 0.576$_{\pm0.001}$ & 0.477$_{\pm0.014}$ & 0.310$_{\pm0.015}$ & 0.467$_{\pm0.004}$ & 0.571$_{\pm0.000}$ & 0.664$_{\pm0.061}$\\
 & GPT-3.5$_{ZS\_both}$ & 0.681$_{\pm0.010}$ & 0.627$_{\pm0.022}$ & 0.617$_{\pm0.014}$ & 0.632$_{\pm0.020}$ & 0.617$_{\pm0.033}$ & 0.254$_{\pm0.009}$ & 0.506$_{\pm0.004}$ & 0.570$_{\pm0.007}$ & 0.750$_{\pm0.027}$\\ \cdashlinespace{2-11}
 & GPT-4$_{ZS}$ & 0.700$_{\pm0.001}$ & 0.719$_{\pm0.013}$ & 0.588$_{\pm0.010}$ & 0.644$_{\pm0.007}$ & 0.760$_{\pm0.009}$ & 0.418$_{\pm0.009}$ & 0.434$_{\pm0.005}$ & 0.566$_{\pm0.017}$ & \textbf{0.854}$_{\pm0.006}$\\
 & GPT-4$_{ZS_context}$ & 0.706$_{\pm0.009}$ & 0.719$_{\pm0.009}$ & 0.590$_{\pm0.011}$ & 0.644$_{\pm0.011}$ & 0.753$_{\pm0.028}$ & \underline{0.441}$_{\pm0.057}$ & 0.465$_{\pm0.010}$ & 0.565$_{\pm0.006}$ & \underline{0.848}$_{\pm0.001}$\\
 & GPT-4$_{ZS\_mh}$ & 0.725$_{\pm0.009}$ & 0.684$_{\pm0.004}$ & 0.656$_{\pm0.001}$ & 0.645$_{\pm0.012}$ & 0.737$_{\pm0.005}$ & 0.396$_{\pm0.020}$ & 0.496$_{\pm0.005}$ & 0.527$_{\pm0.007}$ & 0.840$_{\pm0.003}$\\
 & GPT-4$_{ZS\_both}$ & 0.719$_{\pm0.021}$ & 0.689$_{\pm0.000}$ & 0.650$_{\pm0.011}$ & 0.647$_{\pm0.014}$ & 0.697$_{\pm0.005}$ & 0.411$_{\pm0.009}$ & 0.511$_{\pm0.000}$ & 0.546$_{\pm0.014}$ & 0.837$_{\pm0.002}$\\
\thickhlinespace
\multirow{4}{*}{\makecell{Few-shot\\Prompting}} &  Alpaca$_{FS}$         & 0.632$_{\pm0.030}$ & 0.529$_{\pm0.017}$ & 0.628$_{\pm0.005}$ & ---   & ---                & ---                & ---             & ---                & ---                \\
& FLAN-T5$_{FS}$         & 0.786$_{\pm0.006}$ & 0.678$_{\pm0.009}$ & 0.432$_{\pm0.009}$ & ---                & ---                & ---   & ---                & ---                & ---             \\
& GPT-3.5$_{FS}$         & 0.721$_{\pm0.010}$ & 0.665$_{\pm0.015}$ & 0.580$_{\pm0.002}$ & ---                & ---                & --- & ---                & ---                & ---               \\
& GPT-4$_{FS}$         & 0.698$_{\pm0.009}$ & 0.724$_{\pm0.005}$ & 0.613$_{\pm0.001}$ & ---                & ---                & --- & ---                & ---                & ---                \\\thickhlinespace
\multirow{2}{*}{\makecell{Instructional\\Finetuning}} & Mental-Alpaca         & \underline{0.816}$_{\pm0.006}$ & \underline{0.775}$_{\pm0.006}$ & \underline{0.746}$_{\pm0.005}$ & \textbf{0.724}$_{\pm0.004}$ & 0.730$_{\pm0.048}$ & 0.403$_{\pm0.029}$ & \textbf{0.604}$_{\pm0.012}$ & 0.718$_{\pm0.011}$ & 0.819$_{\pm0.006}$\\
& Mental-FLAN-T5 &  0.802$_{\pm0.002}$ & 0.759$_{\pm0.003}$ & \textbf{0.756}$_{\pm0.001}$ & 0.677$_{\pm0.005}$ & \textbf{0.868}$_{\pm0.006}$ & \textbf{0.481}$_{\pm0.006}$ & \underline{0.582}$_{\pm0.002}$ & \textbf{0.736}$_{\pm0.003}$ & 0.779$_{\pm0.002}$ \\ \thickhlinespace
\multirow{3}{*}{\xspace\xspace\xspace Baseline} & Majority          & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.250$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.200$_{\pm ---}$ & ---                & ---                & --- \\
& BERT              & 0.783$_{\pm ---}$  & 0.763$_{\pm ---}$  & 0.690$_{\pm ---}$  & 0.678$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.332$_{\pm ---}$ & ---                & ---                & --- \\
& Mental-RoBERTa     & \textbf{0.831}$_{\pm ---}$  & \textbf{0.790}$_{\pm ---}$   & 0.736$_{\pm ---}$  & \underline{0.723}$_{\pm ---}$  & \underline{0.853}$_{\pm ---}$  & 0.373$_{\pm ---}$ & ---                & ---                & --- \\ \thickhlinespace
\end{tabular}
\vspace{-0.5cm}
}
\end{table}
\renewcommand{\arraystretch}{1}