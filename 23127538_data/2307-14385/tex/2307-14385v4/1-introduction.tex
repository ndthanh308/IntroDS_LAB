\section{Introduction}
\label{sec:introduction}

The recent surge of Large Language Models (LLMs), such as GPT-4~\cite{bubeck_sparks_2023}, PaLM~\cite{chowdhery_palm_2022}, FLAN-T5~\cite{chung_scaling_2022}, and Alpaca~\cite{taori_stanford_2023}, demonstrates the promising capability of large pre-trained models to solve various tasks in zero-shot settings (\ie tasks not encountered during training). 
Example tasks include question answering~\cite{omar2023chatgpt,robinson2023leveraging}, logic reasoning~\cite{wei_chain--thought_2023,zhou_least--most_2023}, machine translation~\cite{brants2007large,gulcehre2017integrating}, \etc\ 
A number of experiments have revealed that, built on hundreds of billions of parameters, these LLMs have started to show the capability to understand the human common sense beneath the natural language and do proper reasoning and inference accordingly~\cite{bubeck_sparks_2023,nori_capabilities_2023}.

Among different applications, one particular question yet to be answered is how well LLMs can understand human mental health states through natural language.
Mental health problems represent a significant burden for individuals and societies worldwide.
A recent report suggested that more than 20\% of adults in the U.S. experience at least one mental disorder in their lifetime~\cite{mental2022state} and 5.6\% have suffered from a serious psychotic disorder that significantly impairs functioning~\cite{mental2023stats}. The global economy loses around \$1 trillion annually in productivity due to depression and anxiety alone~\cite{mentalcost2023}.

In the past decade, there has been a plethora of research in natural language processing (NLP) and computational social science on detecting mental health issues via online text data such as social media content~(\eg \cite{guntuku_detecting_2017,eichstaedt2018facebook,coppersmith_clpsych_2015,de_choudhury_social_2013,de_choudhury_mental_2014}). However, most of these studies have focused on building domain-specific machine learning (ML) models (\ie one model for one particular task, such as stress detection~\cite{nijhawan2022stress,guntuku2019understanding}, depression prediction~\cite{eichstaedt2018facebook,tadesse2019detection,xu_leveraging_2019, xu_leveraging_2021}, or suicide risk assessment~\cite{de_choudhury_discovering_2016,coppersmith2018natural}). Even for traditional pre-trained language models such as BERT, they need to be finetuned for specific downstream tasks~\cite{devlin_bert_2019,liu_roberta_2019}.
Some studies have also explored the multi-task setup~\cite{benton2017multi}, such as predicting depression and anxiety at the same time~\cite{sarkar2022predicting}. However, these models are constrained to predetermined task sets, offering limited flexibility.
From a different aspect, another line of research has been exploring the application of chatbots for mental health services~\cite{bodrunova_assessing_2019,cameron_towards_2017,lee_designing_2020}. Most chatbots are simply rule-based and can benefit from more advanced models that empower the chatbots~\cite{abd-alrazaq_overview_2019,lee_designing_2020}.
Despite the growing research efforts of empowering AI for mental health, it's important to note that existing techniques can sometimes introduce bias and even provide harmful advice to users~\cite{irene_y_chen_can_2019,timmons2022call,lovejoy2019technology}.

Since natural language is a major component of mental health assessment and treatment~\cite{sharma2018mental,gkotsis2016language}, LLMs could be a powerful tool for understanding end-users' mental states through their written language. 
These instruction-finetuned and general-purpose models can understand a variety of inputs and obviate the need to train multiple models for different tasks. 
Thus, we envision employing a single LLM for a variety of mental health-related tasks, such as multiple question-answering, reasoning, and inference.
This vision opens up a wide range of opportunities for UbiComp, Human-Computer Interaction (HCI), and mental health communities, such as online public health monitoring systems~\cite{patel2018psyheal,graham2019artificial}, \minorreview{mental-health-aware personal chatbots~\cite{abd2021perceptions,denecke2020mental,kernan2023harnessing}, intelligent assistants for mental health therapists~\cite{sharma_towards_2021}, online moderation tools~\cite{franco2023analyzing}, daily mental health counselors and supporters~\cite{sharma_humanai_2023}, \etc\ }
However, there is a lack of investigation into understanding, evaluating, and improving the capability of LLMs for mental-health-related tasks.

There are few recent studies on the evaluation of LLMs (\eg ChatGPT) on mental-health-related tasks, most of which are in zero-shot settings with simple prompt engineering~\cite{yang_evaluations_2023,amin_will_2023,lamichhane_evaluation_2023}. Researchers have shown preliminary results that LLMs have the initial capability of predicting mental health disorders using natural language, with promising but still limited performance compared to state-of-the-art domain-specific NLP models~\cite{yang_evaluations_2023,lamichhane_evaluation_2023}.
This remaining gap is expected since existing general-purpose LLMs are not specifically trained on mental health tasks.
However, to achieve our vision of leveraging LLMs for mental health support and assistance, we need to address the research question: \textbf{How to improve LLMs' capability of mental health tasks}?

We conducted a series of experiments with six LLMs, including Alpaca~\cite{taori_stanford_2023} and 
Alpaca-LoRA (LoRA-finetuned LLaMA on Alpaca dataset)~\cite{hu_lora_2021},
% Alpaca-LoRA~\cite{hu_lora_2021}, 
which are representative open-source models focused on dialogue and other tasks; FLAN-T5~\cite{chung_scaling_2022}, a representative open-source model focused on task-solving; \review{LLaMA2~\cite{touvron_llama_2023-1}, one of the most advanced open-source model released by Meta;} GPT-3.5~\cite{noauthor_introducing_2022} and GPT-4~\cite{bubeck_sparks_2023}, representative close-sourced LLMs over 100 billion parameters.
Considering the data availability, we leveraged online social media datasets with high-quality human-generated mental health labels.
Due to the ethical concerns of existing AI research for mental health, we aim to benchmark LLMs' performance as an initial step before moving toward real-life deployment.
Our experiments contained three stages: (1) zero-shot prompting, where we experimented with various prompts related to mental health, (2) few-shot prompting, where we inserted examples into prompt inputs, and (3) instruction-finetuning, where we finetuned LLMs on multiple mental-health datasets with various tasks.

Our results show that the zero-shot approach yields promising yet limited performance on various mental health prediction tasks across all models. Notably, FLAN-T5 and GPT-4 show encouraging performance, approaching the state-of-the-art task-specific model.
Meanwhile, providing a few shots in the prompt can improve the model performance to some extent ($\overline{\Delta}$ = 4.1\%), but the advantage is limited.
Finally and most importantly, we found that instruction-finetuning significantly enhances the model performance across multiple mental-health-related tasks and various datasets simultaneously. 
Our finetuned Alpaca and FLAN-T5, namely \textit{Mental-Alpaca} and \textit{Mental-FLAN-T5}, significantly outperform the best of GPT-3.5 across zero-shot and few-shot settings ($\times$25 and 15 bigger than Alpaca and FLAN-T5) by an average of 10.9\% on balance accuracy, as well as the best of GPT-4 by 4.8\% ($\times$250 and 150 bigger than Alpaca and FLAN-T5). 
Meanwhile, Mental-Alpaca and Mental-FLAN-T5 can further perform on par with the task-specific state-of-the-art Mental-RoBERTa~\cite{ji_mentalbert_2021}. 
% It is noteworthy that Mental-RoBERTa needs to be trained on each task individually, while Mental-Alpaca and Mental-FLAN-T5 can solve different tasks off the shelf.
We further conduct an exploratory case study on LLM's capability of mental health reasoning (\ie explaining the rationale behind their predictions). Our results illustrate the promising future of certain LLMs like GPT-4, while also suggesting critical failure cases that need future research attention.
% We open-source our code and model at [Github]\footnote{We will release the code upon acceptance}.
We open-source our code and model at \hyperlink{https://github.com/neuhai/Mental-LLM}{https://github.com/neuhai/Mental-LLM}.

\review{Our experiments present a comprehensive evaluation of various techniques to enhance LLMs' capability in the mental health domain.}
However, we also note that our technical results \textit{do not} imply deployability.
There are many important limitations of leveraging LLMs in mental health settings, especially along known racial and gender gaps~\cite{abid2021persistent,ghosh2023chatgpt}.
We discuss the important ethical risks to be addressed before achieving real-world deployment.


\review{
The contribution of our paper can be summarized as follows:
\begin{s_enumerate}
\item We present a comprehensive evaluation of prompt engineering, few-shot, and finetuning techniques on multiple LLMs in the mental health domain.
\item With online social media data, our results reveal that finetuning on a variety of datasets can significantly improve LLM's capability on multiple mental-health-specific tasks across different datasets simultaneously.
\item We release our model \textit{Mental-Alpaca} and \textit{Mental-FLAN-T5} as open-source LLMs targeted at multiple mental health prediction tasks.
\item We provide a few technical guidelines for future researchers and developers on turning LLMs into experts in specific domains. We also highlight the important ethical concerns regarding leveraging LLMs for health-related tasks.
\end{s_enumerate}
}
