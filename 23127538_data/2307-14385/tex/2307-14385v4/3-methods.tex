\section{Methods}
\label{sec:methods}
We introduce our experiment design with LMMs on multiple mental health prediction task setups, including zero-shot prompting (Sec.~\ref{sub:methods:zero-shot}), few-shot prompting (Sec.~\ref{sub:methods:few-shot}), and instruction finetuning (Sec.~\ref{sub:methods:finetuning}). These setups are model-agnostic, and we will present the details of language models and datasets employed for our experiment in the next section.

\subsection{Zero-shot Prompting}
\label{sub:methods:zero-shot}
The language understanding and reasoning capability of LLMs have enabled a wide range of applications without the need for any domain-specific data, but only providing appropriate prompts~\cite{kojima_large_2022,wei2021finetuned}.
Therefore, we start with prompt design for mental health tasks in a zero-shot setting.

The goal of prompt design is to empower a pre-trained general-purpose LLM to achieve good performance on tasks in the mental health domain. We propose a general zero-shot prompt template ($\textit{Prompt}_{ZS}$) that consists of four parts:
\begin{equation}
    \textit{Prompt}_{ZS} = \textit{TextData} + \textit{Prompt}_{\textit{Part1-S}} + \textit{Prompt}_{\textit{Part2-Q}} + \textit{OutputConstraint}
\label{eq:prompt-zs}
\end{equation}
where \textit{TextData} is the online text data generated by end-users. \textit{Prompt}$_{\textit{Part1-S}}$ provides specifications for a mental health prediction target. \textit{Prompt}$_{\textit{Part2-Q}}$ poses the question for LLMs to answer. And \textit{OutputConstraint} controls the output of models (\eg ``Only return yes or no'' for a binary classification task).

We propose several design strategies for \textit{Prompt}$_{\textit{Part1-S}}$, as shown in the top part of Table~\ref{tab:prompt_design}: (1) \textbf{Basic}, which leaves it as blank; (2) \textbf{Context Enhancement}, which provides more social media context about the \textit{TextData}; (3) \textbf{Mental Health Enhancement}, which inserts mental health concept by asking the model to act as an expert. (4) \textbf{Context \& Mental Health Enhancement}, which combines both enhancement strategies by asking the model to act as a mental health expert under the social media context.

As for \textit{Prompt}$_{\textit{Part2-Q}}$, we mainly focus on two categories of mental health prediction targets: (1) predicting critical mental states, such as stress or depression, and (2) predicting high-stake risk actions, such as suicide. We tailor the question description for each category. Moreover, for both categories, we explore binary and multi-class classification tasks
\footnote{We also conduct an exploratory case study on mental health reasoning tasks. Please see more details in Sec.~\ref{sub:results:reasoning}.}.
Thus, we also make small modifications based on specific tasks to ensure appropriate questions (see Sec.~\ref{sec:implementation} for our mental health tasks). The bottom part of Table~\ref{tab:prompt_design} summarizes the mapping.

\input{fig_tab_alg/tab_prompt_design}

For both \textit{Prompt}$_{\textit{Part1-S}}$ and \textit{Prompt}$_{\textit{Part2-Q}}$, we propose several versions to improve its variability. We then evaluate these prompts on multiple LLMs on different datasets and compare their performance.

\subsection{Few-shot Prompting}
\label{sub:methods:few-shot}

In order to provide more domain-specific information, researchers have also explored few-shot prompting with LLMs by providing few-shot demonstrations to support in-context learning (\eg \cite{agrawal2022large,dang2022prompt}). Note that these few examples are used solely in prompts, and the model parameters remain unchanged. The intuition is to present a few ``examples'' for the model to learn domain-specific knowledge \textit{in situ}.
In our setting, we also test this strategy by adding additional randomly sampled [$\textit{Prompt}_{ZS} - \textit{label}$] pairs. The design of the few-shot prompt ($\textit{Prompt}_{FS}$) is straightforward:
\begin{equation}
    \textit{Prompt}_{FS} = [\textit{Sample Prompt}_{ZS} - \textit{label}]_{M} + \textit{Prompt}_{ZS}
\label{eq:prompt-fs}
\end{equation}
where $M$ is the number of prompt-label pairs and is capped by the input length limit of a model. Note that both the $\textit{Sample Prompt}_{ZS}$ and $\textit{Prompt}_{ZS}$ follow Eq.~\ref{eq:prompt-zs} and employ the same design of \textit{Prompt}$_{\textit{Part1-S}}$ and \textit{Prompt}$_{\textit{Part2-Q}}$ to ensure consistency.

\subsection{Instruction Finetuning}
\label{sub:methods:finetuning}

In contrast to the few-shot prompting strategy in Sec.~\ref{sub:methods:few-shot}, the goal of this strategy is closer to the traditional few-shot transfer learning, where we further train the model with a small amount of domain-specific data (\eg \cite{huang2022large,xu2021raise,liu_large_2023}).
We experiment with multiple finetuning strategies.

\subsubsection{Single-dataset Finetuning}
\label{subsub:methods:finetuning:single-dataset}
Following most of the previous work in the mental health field~\cite{yang_evaluations_2023,coppersmith_clpsych_2015,de_choudhury_discovering_2016}, we first conduct basic finetuning on a single dataset (the training set). This finetuned model can be tested on the same dataset (the test set) to evaluate its performance and different datasets to evaluate its generalizability.

\subsubsection{Multi-dataset Finetuning}
\label{subsub:methods:finetuning:multi-dataset}
From Sec.~\ref{sub:methods:zero-shot} to Sec.~\ref{subsub:methods:finetuning:single-dataset}, we have been focusing on one single mental health dataset $D$.
More interestingly, we further experiment with finetuning across multiple datasets simultaneously. Specifically, we leverage instruction finetuning to enable LLMs to handle multiple tasks in different datasets~\cite{brown_language_2020}.

It is noteworthy that such an instruction finetuning setup differs from the state-of-the-art mental-health-specific models (\eg Mental-RoBERTa~\cite{ji_mentalbert_2021}). The previous models are finetuned for a specific task, such as depression prediction or suicidal ideation prediction. Once trained on task A, the model becomes specific to task A and is only suitable for solving that particular task. In contrast, we finetune LLMs on several mental health datasets, employing diverse instructions for different tasks across these datasets in a single iteration. This enables them to handle multiple tasks without additional task-specific finetuning.

% It is noteworthy that such an instruction finetuning setup differs from the state-of-the-art mental-health-specific models (\eg Mental-RoBERTa~\cite{ji_mentalbert_2021}). The previous models are finetuned for a specific task, such as depression prediction or suicidal ideation prediction, or multiple predefined tasks in the multi-task setting. Once the model is trained on task(s) A, it becomes task-A-specific and only aims to solve task A. In contrast, we finetune LLMs on multiple mental health datasets with various instructions for different tasks across multiple datasets in a single round, empowering them to work on multiple tasks without the need for further task-specific finetuning.

For both single- and multi-dataset finetuning, we follow the same two steps:
\begin{align}
\begin{split}
\text{Step 1:} & \text{ Finetune with } [\textit{Prompt}_{ZS} - \textit{label}]_{\sum\limits^{I} N_{D_{i-train}}} \\
\text{Step 2:} & \text{ Test with } [\textit{Prompt}_{ZS}]_{\sum\limits^{I} N_{D_{i-test}}}
\end{split}
\label{eq:prompt-ft}
\end{align}
where $N_{D_{i-train}}$/$N_{D_{i-test}}$ is the total size of the training/test dataset $D_i$, $I$ represents the set of datasets used for finetuning, and $i$ indicates the specific dataset index ($i \in I, |I| \ge 1$). Both $\textit{Prompt}_{ZS\textit{-train}}$ and $\textit{Prompt}_{ZS\textit{-test}}$ follow Eq.~\ref{eq:prompt-zs}. Similar to the few-shot setup in Eq.~\ref{eq:prompt-fs}, they employ the same design of \textit{Prompt}$_{\textit{Part1-S}}$ and \textit{Prompt}$_{\textit{Part2-Q}}$.

% In addition, we also experiment with multiple variations:
% (1) \textbf{Data size variation}, where we sample a subset of each training dataset $D_{i-train}$ with a smaller $N'_{D_{i-train}}$ to explore the effect of data size on finetuning performance.
% (2) \textbf{Prompt variation}, where we randomly sample \textit{Prompt}$_{\textit{Part1-S}}$ from Table~\ref{tab:prompt_design} during the training to explore the effect of prompt diversity on finetuning performance.