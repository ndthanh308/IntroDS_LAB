\section{Conclusion}
\label{sec:conclusion}
In this paper, we present the first comprehensive evaluation of multiple LLMs (\review{Alpaca, Alpaca-LoRA, FLAN-T5, LLaMA2, GPT-3.5, and GPT-4}) on mental health prediction tasks (binary and multi-class classification) via online text data. 
Our experiments cover zero-shot prompting, few-shot prompting, and instruction finetuning. The results reveal a number of interesting findings.
Our context enhancement strategy can robustly improve performance for all LLMs, and our mental health enhancement strategy can enhance models with a large number of trainable parameters.
Meanwhile, few-shot prompting can also robustly improve model performance even by providing just one example per class.
\review{Most importantly, our experiments show that instruction finetuning across multiple datasets can significantly boost model performance on various mental health prediction tasks at the same time, generalizing across external data sources and platforms.}. Our best finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform much larger \review{LLaMA2, GPT-3.5 and GPT-4}, and perform on par with the state-of-the-art task-specific model Mental-RoBERTa.
We also conduct an exploratory case study on these models' reasoning capability, which further suggests both the promising future and the important limitations of LLMs.
We summarize our findings as a set of guidelines for future researchers, developers, and practitioners who want to empower LLMs with better knowledge of mental health for downstream tasks.
Meanwhile, we emphasize that our current efforts of LLMs in mental health are still far from deployability. We highlight the important ethical concerns accompanying this line of research.



\section*{ACKNOWLEDGMENTS}
This work is supported by VW Foundation, Quanta Computing, and the National Institutes of Health (NIH) under Grant No. 1R01MD018424-01.
