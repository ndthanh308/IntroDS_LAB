\section{Results}
\label{sec:results}
We summarize our experiment results with zero-shot prompting (Sec.~\ref{sub:results:zero-shot}), few-shot prompting (Sec.~\ref{sub:results:few-shot}), and instruction finetuning (Sec.~\ref{sub:results:finetune}).
Moreover, although we mainly focus on prediction tasks in this research, we also present the initial results of our exploratory case study on mental health reasoning tasks in Sec.~\ref{sub:results:reasoning}.

Overall, our results show that zero-shot and few-shot settings show promising performance of LLMs for mental health tasks, although their performance is still limited.
Instruction-finetuning on multiple datasets (Mental-Alpaca and Mental-FLAN-T5) can significantly boost models' performance on all tasks simultaneously.
Our case study also reveals the strong reasoning capability of certain LLMs, especially GPT-4.
However, we note that these results \textit{do not} indicate the deployability. We highlight important ethical concerns and gaps in Sec.~\ref{sec:discussion}.


\subsection{Zero-shot Prompting Shows Promising yet Limited Performance}
\label{sub:results:zero-shot}
% Are LLMs Good at Mental Health Prediction with Zero-shot Prompting?
% (Zero-shot Prompting Shows Promising yet Limited Performance)

\review{We start with the most basic zero-shot prompting with Alpaca, Alpaca-LoRA, FLAN-T5, LLaMA2, GPT-3.5, and GPT-4.}
The balanced accuracy results are summarized in the first sections of Table~\ref{tab:results_overall_short}.
Alpaca$_{ZS}$ and Alpaca-LoRA$_{ZS}$ achieve better overall performance than the naive majority baseline ($\overline{\Delta}_{\text{Alpaca}} = 5.5\%$, $\overline{\Delta}_{\text{Alpaca-LoRA}} = 5.6\%$), but they are far from the task-specific baseline models BERT and Mental-RoBERTa (which have 20\%-25\% advantages).
With much larger models GPT-3.5$_{ZS}$, the performance gets more promising ($\overline{\Delta}_{\text{GPT-3.5}} = 12.4\%$ over baseline), which is inline with previous work~\cite{yang_evaluations_2023}. GPT-3.5's advantage over Alpaca and Alpaca-LoRA is expected due to its larger size (25$\times$).

Surprisingly, FLAN-T5$_{ZS}$ achieves much better overall results compared to Alpaca$_{ZS}$ ($\overline{\Delta}_{\text{FLAN-T5}\_vs\_\text{Alpaca}} = 10.9\%$) and Alpaca-LoRA$_{ZS}$ ($\overline{\Delta}_{\text{FLAN-T5}\_vs\_\text{Alpaca-LoRA}} = 11.0\%$), and even \review{LLaMA2 ($\overline{\Delta}_{\text{FLAN-T5}\_vs\_\text{LLaMA2}} = 1.0\%$) and GPT-3.5 ($\overline{\Delta}_{\text{FLAN-T5}\_vs\_\text{GPT-3.5}} = 4.2\%$). Note that LLaMA2 is 6 times bigger  than FLAN-T5 and GPT-3.5 is 15 times bigger.}
On Task \#6 (Five-level Suicide Risk Prediction), FLAN-T5$_{ZS}$ even outperforms the state-of-the-art Mental-RoBERTa by 4.5\%.
Comparing these results, the task-solving-focused model FLAN-T5 appears to be better at the mental health prediction tasks in a zero-shot setting. We will introduce more interesting findings after finetuning (see Sec.~\ref{subsub:results:finetune:dialogue_vs_tasksolving}).

In contrast, the advantage of GPT-4 becomes relatively less remarkable considering its gigantic size. \mbox{GPT-4$_{ZS}$'s} average performance outperforms FLAN-T5$_{ZS}$ (150$\times$ size), \review{LLaMA2$_{ZS}$ (25$\times$ size)}, and GPT-3.5$_{ZS}$ (10$\times$ size) by 6.4\%, 7.5\%, and 10.6\%, respectively. Yet it is still very encouraging to observe that GPT-4 is approaching the state-of-the-art on these tasks ($\overline{\Delta}_{\text{GPT-4}\_vs\_\text{Mental-RoBERTa}}$ $ = -7.9\%$), and it also outperforms Mental-RoBERTa on Task \#6 by 4.5\%.
In general, these results indicate the promising capability of LLMs on mental health prediction tasks compared to task-specific models, even without any domain-specific information.

\input{fig_tab_alg/tab_results_overall_short}

\subsubsection{The Effectiveness of Enhancement Strategies.}
In Sec.~\ref{sub:methods:zero-shot}, we propose context enhancement, mental health enhancement, and their combination strategies for zero-shot prompt design to provide more information about the domain. Interestingly, our results suggest varied effectiveness on different LLMs and datasets.

\input{fig_tab_alg/tab_results_zeroshot_delta}

Table~\ref{tab:results_zeroshot_delta} provides a zoom-in summary of the zero-shot part in Table~\ref{tab:results_overall_short}.
\review{For Alpaca, LLaMA2, GPT-3.5, and GPT-4, the three strategies improved the performance in general ($\overline{\Delta}_{\text{Alpaca}} = 1.0\%$, 13 out of 18 tasks show positive changes; $\overline{\Delta}_{\text{LLaMA2}} = 0.3\%$, 12/18 tasks positive;  $\overline{\Delta}_{\text{GPT-3.5}} = 2.8\%$, 12/18 tasks positive; $\overline{\Delta}_{\text{GPT-4}} = 0.2\%$, 11/18 tasks positive).}
However, for Alpaca-LoRA and FLAN-T5, adding more context or mental health domain information would reduce the model performance ($\overline{\Delta}_{\text{Alpaca-LoRA}} = -2.7\%$, $\overline{\Delta}_{\text{FLAN-T5}} = -1.6\%$). For Alpaca-LoRA, this limitation may stem from being trained with fewer parameters, potentially constraining its ability to understand context or domain specifics. For FLAN-T5, this reduced performance might be attributed to its limited capability in processing additional information, as it is primarily tuned for task-solving.

The effectiveness of strategies on different datasets/tasks also varies. We observe that Task\#4 from the SDCNL dataset and Task\#6 from the CSSRS-Suicide dataset benefit the most from the enhancement.
In particular, GPT-3.5 benefits very significantly from enhancement on Task \#4 ($\overline{\Delta}_{\text{GPT-3.5}-\text{Task\#4}} = 14.8\%$).
\review{And LLaMA2 benefits significantly on Task \#6 ($\overline{\Delta}_{\text{GPT-3.5}-\text{Task\#6}} = 6.8\%$).}
These could be caused by the different nature of datasets. Our results suggest that these enhancement strategies are generally more effective for critical action prediction (\eg suicide, 2/3 tasks positive) than mental state prediction (\eg stress and depression, 1/3 task positive).

\review{We also compare the effectiveness of different strategies on the four models with positive effects: Alpaca, LLaMA2, GPT-3.5, and GPT-4}. The context enhancement strategy has the most stable improvement across all mental health prediction tasks ($\overline{\Delta}_{\text{Alpaca}-context} = 2.1\%$, 6/6 tasks positive; \review{$\overline{\Delta}_{\text{LLaMA2}-context} = 1.2\%$, 4/6 tasks positive;} $\overline{\Delta}_{\text{GPT-3.5}-context} = 2.5\%$, 5/6 tasks positive; $\overline{\Delta}_{\text{GPT-4}-context} = 0.4\%$, 5/6 tasks positive).
Comparatively, the mental health enhancement strategy is less effective ($\overline{\Delta}_{\text{Alpaca}-mh} = 1.1\%$, 5/6 tasks positive; \review{$\overline{\Delta}_{\text{LLaMA2}-mh} = -0.5\%$, 4/6 tasks positive;} $\overline{\Delta}_{\text{GPT-3.5}-mh} = 2.1\%$, 3/6 tasks positive; $\overline{\Delta}_{\text{GPT-4}-mh} = 0.2\%$, 3/6 tasks positive).
The combination of the two strategies yields diverse results. It has the most significant improvement on GPT-3.5's performance, but not on all tasks ($\overline{\Delta}_{\text{GPT-3.5}-both} = 3.9\%$, 4/6 tasks positive), \review{followed by LLaMA2 ($\overline{\Delta}_{\text{LLaMA2}-both} = 0.2\%$, 4/6 tasks positive)}. However, it has slightly negative impact on the average performance of Alpaca ($\overline{\Delta}_{\text{Alpaca}-both} = -0.1\%$, 2/6 tasks positive) or GPT-4 ($\overline{\Delta}_{\text{GPT-4}-both} = -0.3\%$, 3/6 tasks positive).
\review{This indicates that larger language models (LLaMA2, GPT-3.5 \vs Alpaca)} have a strong capability to leverage the information embedded in the prompts.
But for the huge GPT-4, adding prompts seems less effective, probably because it already contains similar basic information in its knowledge space.

We summarize our \textbf{key takeaways} from this section:
\textbf{
\begin{s_itemize}
\item Both small-scale and large-scale LLMs show promising performance on mental health tasks. FLAN-T5 and GPT-4's performance is approaching task-specific NLP models.
\item The prompt design enhancement strategies are generally effective for dialogue-focused models, but not for task-solving-focused models. These strategies work better for critical action prediction tasks such as suicide prediction.
\item Providing more contextual information about the task \& input can consistently improve performance in most cases.
\item Dialogue-focused models with larger trainable parameters (Alpaca \vs Alpaca-LoRA, as well as \review{LLaMA2/GPT-3.5 \vs Alpaca}) can better leverage the contextual or domain information in the prompts, yet GPT-4 shows less effect in response to different prompts.
\end{s_itemize}
}

\subsection{Few-shot Prompting Improves Performance to Some Extent}
\label{sub:results:few-shot}
% Does Few-shot Prompting Improve The Performance?
We then investigate the effectiveness of few-shot prompting.
Note that since we observe diverse effects of prompt design strategies in Table~\ref{tab:results_zeroshot_delta}, in this section, we only experiment with the prompts with the best performance in the zero-shot setting. \review{Moreover, we exclude Alpaca-LoRA due to its less promising results and LLaMA2 due to its high computation cost.}

\input{fig_tab_alg/tab_results_fewshot_delta}

Due to the maximum input token length of models (2048), we focus on Dreaddit and DepSeverity datasets that have a shorter input and experiment with $M=2$ in Eq.~\ref{eq:prompt-fs} for binary classification and $M=4/5$ for multi-class classification, \ie one sample per class. We repeat the experiment on each task three times and randomize the few shot samples for each run.

We summarize the overall results in the second section of Table~\ref{tab:results_overall_short} and the zoom-in comparison results in Table~\ref{tab:results_fewshot_delta}. 
Although language models with few-shot prompting still underperform task-specific models, providing examples of the task can improve model performance on most tasks compared to zero-shot prompting ($\overline{\Delta}_{FS\text{\_}vs\text{\_}ZS} = 4.1\%$).
Interestingly, few-shot prompting is more effective for Alpaca$_{FS}$ and FLAN-T5$_{FS}$ ($\overline{\Delta}_{\text{Alpaca}} = 8.1\%$, 3/3 tasks positive; $\overline{\Delta}_{\text{FLAN-T5}} = 5.9\%$, 3/3 tasks positive) than GPT-3.5$_{FS}$ and GPT-4$_{FS}$ ($\overline{\Delta}_{\text{GPT-3.5}} = 1.2\%$, 2/3 tasks positive; $\overline{\Delta}_{\text{GPT-4}} = 0.9\%$, 2/3 tasks positive).
Especially for Task \#3, we observe an improved balanced accuracy of 19.7\% for Alpaca but a decline of 2.3\% for GPT-3.5, so that Alpaca outperforms GPT-3.5 on this task. A similar situation is observed for FLAN-T5 (improved by 12.7\%) and GPT-4 (declined by 0.2\%) on Task \#1.
This may be attributed to the fact that smaller models such as Alpaca and FLAN-T5 can quickly adapt to complex tasks with only a few examples. In contrast, larger models like GPT-3.5 and GPT-4, with their extensive ``in memory'' data, find it more challenging to rapidly learn from new examples.

This leads to the key message from this experiment:
\textbf{Few-shot prompting can improve the performance of LLMs on mental health prediction tasks to some extent, especially for small models.} 

\subsection{Instruction Finetuning Boost Performance for Multiple Tasks Simultaneously}
\label{sub:results:finetune}
% How Effective is Instruction Finetuning?
% (Instruction Finetuning Boost Performance for Multiple Tasks Simultaneously)
Our experiments so far have shown that zero-shot and few-shot prompting can improve LLMs on mental health tasks to some extent, but their overall performance is still below state-of-the-art task-specific models.
In this section, we explore the effectiveness of instruction finetuning.

\input{fig_tab_alg/tab_results_finetune_delta}

Due to the prohibitive cost and lack of transparency of GPT-3.5 and GPT-4 finetuning, we only experiment with Alpaca and FLAN-T5 that we have full control of.
% Similar to picking the best prompts for few-shot prompting in Sec.~\ref{sub:results:few-shot}, we also use the best prompts in zero-shot prompting during the instruction finetuning.
We picked the most informative prompt to provide more embedded knowledge during the finetuning.
As introduced in Sec.~\ref{subsub:methods:finetuning:multi-dataset} and Sec.~\ref{subsub:implementation:models:mental}, we build Mental-Alpaca and Mental-FLAN-T5 by finetuning Alpaca and FLAN-T5 on all six tasks across four datasets at the same time.

The third section of Table~\ref{tab:results_overall_short} summarizes the overall results, and Table~\ref{tab:results_finetuning_delta} highlights the key comparisons.
We observe that both Mental-Alpaca and Mental-FLAN-T5 achieve significantly better performance compared to the unfinetuned versions ($\overline{\Delta}_{\text{Alpaca}-FT\_vs\_ZS}$ = 23.4\%, $\overline{\Delta}_{\text{Alpaca}-FT\_vs\_FS}$ = 18.3\%; $\overline{\Delta}_{\text{FLAN-T5}-FT\_vs\_ZS}$ = 14.7\%, $\overline{\Delta}_{\text{FLAN-T5}-FT\_vs\_FS}$ = 14.0\%).
Both finetuned models surpass GPT-3.5's best performance among zero-shot and few-shot settings across all six tasks ($\overline{\Delta}_{\text{Mental-Alpaca}\_vs\_\text{GPT-3.5}}$ = 10.1\%; $\overline{\Delta}_{\text{Mental-FLAN-T5}\_vs\_\text{GPT-3.5}}$ = 11.6\%) and outperform GPT-4's best version in most tasks  ($\overline{\Delta}_{\text{Mental-Alpaca}\_vs\_\text{GPT-4}}$ = 4.0\%, 4/6 tasks positive; $\overline{\Delta}_{\text{Mental-FLAN-T5}\_vs\_\text{GPT-4}}$ = 5.5\%, 5/6 tasks positive).
Recall that GPT-3.5/GPT-4 are 25/250 times bigger than Mental-Alpaca and 15/150 times bigger than Mental-FLAN-T5.

More importantly, Mental-Alpaca and Mental-FLAN-T5 perform on par with the state-of-the-art Mental-RoBERTa. Mental-Alpaca has the best performance on one task and the second best on three tasks, while Mental-FLAN-T5 has the best performance on three tasks. It is noteworthy that Mental-RoBERTa is a task-specific model, which means it is specialized on one task after being trained on it. In contrast, Mental-Alpaca and Mental-FLAN-T5 can simultaneously work across \textit{all} tasks with a single-round finetuning.
These results show the strong effectiveness of instruction finetuning: By finetuning LLMs on multiple mental health datasets with instructions, the models can obtain better capability to solve a variety of mental health prediction tasks.

\subsubsection{Dialogue-Focused \vs Task-Solving-Focused LLMs}
\label{subsub:results:finetune:dialogue_vs_tasksolving}
We further compare Mental-Alpaca and Mental-FLAN-T5.
Overall, their performance is quite close ($\overline{\Delta}_{\text{FLAN-T5}\_vs\_\text{Alpaca}} = 1.4\%$), with Mental-Alapca better at Task \#4 on SDCNL and Mental-FLAN-T5 better at Task \#5 and \#6 on CSSRS-Suicide.
In Sec.~\ref{sub:results:zero-shot}, we observe that FLAN-T5$_{ZS}$ has a much better performance than Alpaca$_{ZS}$ ($\overline{\Delta}_{\text{FLAN-T5}\_vs\_\text{Alpaca}}$ = 10.9\%, 5/6 tasks positive) in the zero-shot setting. However, after finetuning, FLAN-T5's advantage disappears.

Our comparison result indicates that Alpaca, as a dialogue-focused model, is better at learning from human natural language data compared to FLAN-T5. Although FLAN-T5 is good at task solving and thus has a better performance in the zero-shot setting, its performance improvement after instruction finetuning is relatively smaller than that of Alpaca.
This observation has implications for future stakeholders. If the data and computing resources for finetuning are not available, using task-solving-focused LLMs could lead to better results. When there are enough data and computing resources, finetuning dialogue-based models can be a better choice.
Furthermore, models like Alpaca, with their dialogue conversation capabilities, may be more suitable for downstream applications, such as mental well-being assistants for end-users.

\subsubsection{Does Finetuning Generalize across Datasets?}
\label{subsub:results:finetune:generalizability}
% Are Models Generalizable across Datasets/Tasks?
% (Domain Generalizability Still Needs Improvement)
We further measure the generalizability of LLMs after finetuning. To do this, we instruction-finetune the model on one dataset and evaluate it on all datasets (as introduced in Sec.~\ref{subsub:methods:finetuning:single-dataset}). 
As the main purpose of this part is not to compare different models but evaluate the finetuning method, we only focus on Alpaca.
Table~\ref{tab:results_transfer} summarizes the results.

\input{fig_tab_alg/tab_transfer}

We first find that finetuning and testing on the same dataset lead to good performance, as indicated by the \fbox{boxed} entries on the diagonal in Table~\ref{tab:results_transfer}. Some results are even better than Mental-Alpaca (5 out of 6 tasks) or Mental-RoBERTa (3 out of 6 tasks), which is not surprising.
More interestingly, we investigate cross-dataset generalization performance (\ie the ones off the diagonal).
Overall, finetuning on a single dataset achieves better performance compared to the zero-shot setting ($\overline{\Delta}_{FT\text{-Single}\_vs\_ZS} = 4.2\%$).
However, the performance changes vary across tasks. For example, finetuning on any dataset is beneficial for Task \#3 ($\overline{\Delta} = 19.2\%$) and \#5 ($\overline{\Delta} = 16.4\%$), but detrimental for Task \#6 ($\overline{\Delta} = -7.6\%$) and almost futile for Task \#4 ($\overline{\Delta} = -0.4\%$). Generalizing across Dreaddit and DepSeverity shows good performance, but this is mainly because they share the language corpus.
These results indicate that finetuning on a single dataset can provide mental health knowledge with a certain level and thus improve the overall generalization results, but such improvement is not stable across tasks.

\input{fig_tab_alg/tab_results_external_overall}

\review{
Moreover, we further evaluate the generalizability of our best models instructional-finetuned on multiple datasets, \ie Mental-Alpaca and Mental-FLAN-T5. We leverage external datasets that are not included in the finetuning. Table~\ref{tab:results_external_overall} highlights the key results. More detailed results can be found in Table~\ref{tab:results_overall}.
}

\review{
Consistent with the results in Table~\ref{tab:results_finetuning_delta}, the instruction finetuning enhances the model performance on external datasets ($\overline{\Delta}_{Alpaca} = 16.3\%$, $\overline{\Delta}_{FLAN-T5} = 5.1\%$). Both Mental-Alpaca and Mental-FLAN-T5 ranked top 1 or 2 in 2/3 external tasks.
It is noteworthy that Twt-60Users and SAD datasets are collected outside Reddit, and their data is different from the source of finetuning datasets. These results demonstrate strong evidence that instruction finetuning with diverse tasks, even with data collected from a single social media platform, can significantly enhance LLMs' generalizability across multiple scenarios.
}



\subsubsection{How Much Data Is Needed?}
\label{subsub:results:finetune:datasetsize}
Additionally, we are interested in exploring how the size of the dataset impacts the results of instruction finetuning. To answer this question, we downsample the training set to 50\%, 20\%, 10\%, 5\%, and 1\% of the original size and repeat each one three times. We increase the training epoch accordingly to make sure that the model is exposed to a similar amount of data.
Similarly, we also focus on Alpaca only.
Figure~\ref{fig:performance_datasize} visualizes the results.
With only 1\% of the data, the finetuned model is able to outperform the zero-shot model on most tasks (5 out of 6). With 5\% of the data, the finetuned model has a better performance on all tasks.
As expected, the model performance has an increasing trend with more training data. For many tasks, the trend approaches a plateau after 10\%. The difference between 10\% training data (less than 300 samples per dataset) and 100\% training data is not huge ($\overline{\Delta}$ = 5.9\%).

\input{fig_tab_alg/fig_datasize}

\subsubsection{More Data in One Dataset \vs Fewer Data across Multiple Datasets}
In Sec.~\ref{subsub:results:finetune:generalizability}, the finetuning on a single dataset can be viewed as training on a smaller set (around 5-25\% of the original size) with less variation (\ie no finetuning across datasets). Thus, the results in Sec.~\ref{subsub:results:finetune:generalizability} are comparable to those in Sec.~\ref{subsub:results:finetune:datasetsize}.
We found that the model's overall performance is better when the model is finetuned across multiple datasets when overall training data sizes are similar ($\overline{\Delta}_{FT\text{-}5\%\_vs\_FT\text{-Single}}$ = 3.8\%, $\overline{\Delta}_{FT\text{-}10\%\_vs\_FT\text{-Single}}$ = 8.1\%, $\overline{\Delta}_{FT\text{-}20\%\_vs\_FT\text{-Single}}$ = 12.4\%).
This suggests that increasing data variation can more effectively benefit finetuning outcomes when the training data size is fixed.

These results can guide future developers and practitioners in collecting the appropriate data size and sources to finetune LLMs for the mental health domain efficiently. We have more discussion in the next section.
In summary, we highlight the \textbf{key takeaways} of our finetuning experiments as follows:
\textbf{
\begin{s_itemize}
\item Instruction finetuning on multiple mental health datasets can significantly boost the performance of LLMs on various mental health prediction tasks. Mental-Alpaca and Mental-FLAN-T5 outperform GPT-3.5 and GPT-4, and perform on par with the state-of-the-art task-specific model.
\item Although task-solving-focused LLMs may have better performance in the zero-shot setting for mental health prediction tasks, dialogue-focused LLMs have a stronger capability of learning from human natural language and can improve more significantly after finetuning.
\item \review{Finetuning LLMs on a small number of datasets and tasks may improve model generalizable knowledge in mental health, but its effect is not robust. Comparatively, finetuning on diverse tasks can robustly enhance generalizability across multiple social media platforms.}
\item Finetuning LLMs on a small number of samples (a few hundred) across multiple datasets can already achieve favorable performance.
\item When the data size is the same, finetuning LLMs on data with larger variation (\ie more datasets and tasks) can achieve better performance.
\end{s_itemize}
}

\subsection{Case Study of LLMs' Capability on Mental Health Reasoning}
\label{sub:results:reasoning}

\input{fig_tab_alg/fig_reasoning_example1}

In addition to evaluating LLMs' performance on classification tasks, we also take an initial step to explore LLMs' capability on mental health reasoning.
This is another strong advantage of LLMs since they can generate human-like natural language based on embedded knowledge.
Due to the high cost of a systematic evaluation of reasoning outcomes, here we present a few examples as a case study across different LLMs.
It is noteworthy that we do not aim to claim that certain LLMs have better/worse reasoning capabilities. Instead, this section aims to provide a general sense of LLMs' performance on mental health reasoning tasks.

Specifically, we modify the prompt design by inserting a Chain-of-Thought (CoT) prompt~\cite{kojima_large_2022} at the end of \textit{OutputConstraint} in Eq.~\ref{eq:prompt-zs}: ``Return [set of classification labels]. Provide reasons step by step''. We compare Alpaca, FLAN-T5, GPT-3.5, and GPT-4. Our results indicate the promising reasoning capability of these models, especially GPT-3.5 and GPRT-4.
We also experimented with the finetuned Mental-Alpaca and Mental-FLAN-T5. Unfortunately, our results show that after finetuning solely on classification tasks, these two models are no longer able to generate reasoning sentences even with the CoT prompt. This suggests a limitation of the current finetuned model.


\subsubsection{Diverse Reasoning Capabilities across LLMs.}
\label{subsub:results:reasoning:okay}
\review{
We present several examples as our case study to illustrate the reasoning capability of these LLMs.
}
The first example comes from the binary stress prediction task (Task \#1) on the Dreaddit dataset (see Figure~\ref{fig:reasoning_example1}).
All models give the right classification, but with significantly different reasoning capabilities.
First, FLAN-T5 generates the shortest reason. Although it is reasonable, it is superficial and does not provide enough insights. This is understandable because FLAN-T5 is targeted at task-solving instead of reasoning.
Compared to FLAN-T5, Alpaca generates better reason. Among the five reasons, two of them accurately analyze the user's mental state given the stressful situations.
Meanwhile, GPT-3.5 and GPT-4 generate expert-level high-quality reasons. The inference from the user's statement is accurate and deep, indicating their powerful capability of understanding human emotion and mental health. Comparing the two models, GPT-3.5's reason is simpler, following the user's statement point by point and adding basic comments, while GPT-4's output is more organic and insightful, yet more concise.

\input{fig_tab_alg/fig_reasoning_example2}

We also have a similar observation in the second example from the four-level depression prediction task (Task \#3) on the DepSeverity dataset (see Figure~\ref{fig:reasoning_example2}).
In this example, although FLAN-T5's prediction is correct, it simply repeats the fact stated by the user, without providing further insights.
Alpaca makes the wrong prediction, but it provides one sentence of accurate reasoning (although relatively shallow).
GPT-3.5 makes an ambiguous prediction that includes the correct answer.
In contrast, GPT-4 generates the highest quality reasoning with the right prediction. With its correct understanding of depressive symptoms, GPT-4 can accurately infer from the user's situation, link it to symptoms, and provides insightful analysis.


\subsubsection{Wrong and Dangerous Reasoning from LLMs.}
\label{subsub:results:reasoning:wrong}
\review{
However, we also want to emphasize the \textit{incorrect} reasoning content, which may lead to negative consequences and risks.
}
In the first example, Alpaca generated two wrong reasons for the hallucinated ``reliance on others'' and ``safety concerns'', along with an unrelated reason for readers instead of the poster. In the second example, GPT-3.5 misunderstood what the user meant by ``relief''. To better illustrate this, we further present another example where all four LLMs make problematic reasoning (see Figure~\ref{fig:reasoning_example3}).
In this example, the user was asking for others' opinions on social anxiety, with their own job interview experience as an example. Although the user mentioned situations where they were anxious and stressed, it's clear that they were calm when writing this post and described their experience in an objective way. However, FLAN-T5, GPT-3.5, and GPT-4 all mistakenly take the description of the anxious interview experience as evidence to support their wrong prediction. Although Alpaca makes the right prediction, it does not understand the main theme of the post. The false positives reveal that LLMs may overly generalize in a wrong way: Being stressed in one situation does not indicate that a person is stressed all the time.
However, the reasoning content alone reads smoothly and logically. If the original post was not provided, the content could be very misleading, resulting in a wrong prediction with reasons that ``appears to be solid''.
These examples clearly illustrate the limitations of the current LLMs for mental health reasons, as well as their risks of introducing dangerous bias and negative consequences to users.

\input{fig_tab_alg/fig_reasoning_example3}

The case study suggests that GPT-4 enjoys impressive reasoning capability, followed by GPT-3.5 and Alpaca. Although FLAN-T5 shows a promising zero-shot performance, it is not good at reasoning.
Our results reveal the encouraging capability of LLMs to understand human mental health and generate meaningful analysis.
However, we also present examples where LLMs can make mistakes and offer explanations that appear reasonable but are actually flawed. This further suggests the importance of more future research on LLMs' ethical concerns and safety issues before real-world deployment.
 