\section{Implementation}
\label{sec:implementation}

Our method design is agnostic to specific datasets or models. In this section, we introduce the specific datasets (Sec.~\ref{sub:implementation:datasets}) and models (Sec.~\ref{sub:implementation:models}) involved in our experiments. In particular, we highlight our instructional-finetuned open-source models Mental-Alpaca and Mental-FLAN-T5 (Sec.~\ref{subsub:implementation:models:mental}).

\subsection{Datasets and Tasks}
\label{sub:implementation:datasets}

Our experiment is based on four well-established datasets that are commonly employed for mental health analysis collected from social media platforms.
It is noteworthy that we intentionally avoid using datasets that generate labels based on certain linguistic patterns (\eg some datasets label a post based on whether a user ever stated ``I was diagnosed with X''), as it would be hard to remove artifacts or biases in these datasets.
Instead, we used ones with human expert annotations or supervision.
We define six diverse mental health prediction tasks based on these datasets.

\begin{s_itemize}
\item \textbf{Dreaddit}~\cite{turcan_dreaddit_2019}: This dataset collected posts from Reddit, which contains ten subreddits in the five domains (abuse, social, anxiety, PTSD, and financial). Multiple human annotators rated whether sentence segments showed the stress of the poster, and the annotations were aggregated to generate final labels. We used this dataset for a post-level binary stress prediction (\textbf{Task 1}).
\item \textbf{DepSeverity}~\cite{naseem_early_2022}: This dataset leveraged the same posts collected in \cite{turcan_dreaddit_2019}, but with a different focus on depression. Two human annotators followed DSM-5~\cite{regier2013dsm} and categorized posts into four levels of depression: minimal, mild, moderate, and severe. We employed this dataset for two post-level tasks: binary depression prediction (\ie whether a post showed at least mild depression, \textbf{Task 2}), and four-level depression prediction (\textbf{Task 3}).
\item \textbf{SDCNL}~\cite{haque_deep_2021}: This dataset also collected posts from Reddit, including r/SuicideWatch and r/Depression. Through manual annotation, they labeled whether each post showed suicidal thoughts. We employed this dataset for the post-level binary suicide ideation prediction (\textbf{Task 4}).
\item \textbf{CSSRS-Suicide}~\cite{gaur_knowledge-aware_2019}: This dataset contains posts from 15 mental health-related subreddits. Four practicing psychiatrists followed Columbia Suicide Severity Rating Scale (C-SSRS) guidelines~\cite{posner2008columbia} to manually annotate 500 users on suicide risks in five levels: supportive, indicator, ideation, behavior, and attempt. We leveraged this dataset for two user-level tasks: binary suicide risk prediction (\ie whether a user showed at least suicide indicator, \textbf{Task 5}), and five-level suicide risk prediction (\textbf{Task 6}).
\end{s_itemize}

Table~\ref{tab:datasets} summarizes the information of the four datasets and six mental health prediction tasks~\footnote{Our datasets are mainly based on the Reddit platform (with high-quality labels) due to their availability. Other high-quality datasets are hard to obtain due to their proprietary nature. We discuss this limitation in Sec.~\ref{sec:discussion}.}.
For each dataset, we conducted an 80\%/20\% train-test split. It is noteworthy that one user's data were either in the training or test set to avoid leakage.
 
\input{fig_tab_alg/tab_dataset}

\subsection{Models}
\label{sub:implementation:models}
We experimented with multiple LLMs with different sizes, pre-training targets, and availability.

\begin{s_itemize}
\item \textbf{Alpaca} (7B)~\cite{taori_stanford_2023}: An open-source large model finetuned from another open-sourced LLaMA 7B model~\cite{touvron_llama_2023} on instruction following demonstrations. Experiments have shown that Alpaca behaves qualitatively similarly to OpenAIâ€™s \texttt{text-davinci-003} on certain task metrics. We choose the relatively small 7B version so that we can run and finetune it on consumer hardware. 
\item \textbf{Alpaca-LoRA} (7B)~\cite{hu_lora_2021}: Another open-source large model finetuned from LLaMA 7B model using the same dataset as Alpaca~\cite{taori_stanford_2023}. This model leverages a different finetuning technique called low-rank adaptation (LoRA)~\cite{hu_lora_2021}, with the goal of reducing finetuning cost by freezing the model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It is noteworthy that despite the name similarity, Alpaca-LoRA is completely different from Alpaca. They are trained on the same dataset but with different methods.
\item \textbf{FLAN-T5} (11B)~\cite{chung_scaling_2022}: An open-source large model T5~\cite{raffel_exploring_2020} finetuned with a variety of task-based datasets on instructions. Compared to other LLMs, FLAN-T5 focuses more on task solving and is less optimized for natural language or dialogue generation. We picked the largest version of FLAN-T5 (\ie FLAN-T5-XXL), which has a comparable size of Alpaca.
\item \textbf{GPT-3.5} (175B)~\cite{noauthor_introducing_2022}: This large model is closed-source and available through API provided by OpenAI. We picked the \texttt{gpt-3.5-turbo}, one of the most capable and cost-effective models in the GPT-3.5 family.
\item \textbf{GPT-4} (1700B)~\cite{bubeck_sparks_2023}: This is the largest closed-source model available through OpenAI API. We picked the regular \texttt{gpt-4}. Due to the limited availability of API, the cost of finetuning GPT-3.5 or GPT-4 is prohibitive.
\end{s_itemize}

It is worth noting that Alpaca, Alpaca-LoRA, GPT-3.5, and GPT-4 are all finetuned with natural dialogue as one of the optimization goals. In contrast, FLAN-T5 is more focused on task-solving.
In our case, the input posts written by users are closer to natural dialogue, while the mental health prediction tasks are defined as specific classification tasks. It is unclear and thus interesting to explore which LLM fits better with our goal.

\subsubsection{Mental-Alpaca \& Mental-FLAN-T5}
\label{subsub:implementation:models:mental}
Our methods of zero-shot prompting (Sec.~\ref{sub:methods:zero-shot}) and few-shot prompting (Sec.~\ref{sub:methods:few-shot}) do not update model parameters during the experiment.
In contrast, instruction finetuning (Sec.~\ref{sub:methods:finetuning}) will update model parameters and generate new models.
To enhance their capability in the mental health domains, we update Alpaca and FLAN-T5 on six tasks across the four datasets in Sec.~\ref{sub:implementation:datasets} using the multi-dataset instruction finetuning method (Sec.~\ref{subsub:methods:finetuning:multi-dataset}), which leads to our new model \textit{Mental-Alpaca} and \textit{Mental-FLAN-T5}.
Specifically, for both models, we merge the four datasets together and provide instructions for all six tasks (in the training set). We use eight Nvidia A100 GPUs for instruction finetuning. With cross entropy as the loss function, we backpropagate and update model parameters with the optimizer as Adam, the learning rate as 2$e^{-5}$ (cosine scheduler, warmup ratio 0.03), and the epoch number as 3.
% We open-source the finetuning pipeline and release the model parameter difference from LLaMA\footnote{Due to the open-source legislation of LLaMA, we cannot release model parameters directly. We adopt a similar practice of Stanford Alpaca~\cite{taori_stanford_2023} and only the parameter difference.} at [github link].