\section{Discussion}
\label{sec:discussion}
Our experiment results reveal a number of interesting findings. In this section, we discuss potential guidelines for enabling LLMs for mental -health-related tasks (Sec.~\ref{sub:discussion:guidelines}). We envision promising future directions (Sec.~\ref{sub:discussion:beyond}), while highlighting important ethical concerns and limitations with LLMs for mental health (Sec.~\ref{sub:discussion:ethics}). We also summarize the limitations of the current work (Sec.~\ref{sub:discussion:limitation}).

\subsection{Guidelines for Empowering LLMs for Mental Health Prediction Tasks}
\label{sub:discussion:guidelines}
We extract and summarize the takeaways from Sec.~\ref{sec:results} into a set of guidelines for future researchers and practitioners on how to empower LLMs to be better at various mental health prediction tasks.

\textbf{When computing resources are limited, combine prompt design \& few-shot prompting, and pick prompts carefully.}
As the size of large models continues to grow, the requirement for hardware (mainly GPU) has also been increasing, especially when finetuning an LLM. For example, in our experiment, Alpaca was trained on eight 80GB A100s for three hours~\cite{taori_stanford_2023}.
When there are limited computing resources, only running inference or resorting to API is possible. In these cases, zero-shot and few-shot prompt engineering strategies are viable options. Our results indicate that providing few-shot mental health examples with appropriate enhancement strategies can effectively improve prediction performance.
Specifically, adding contextual information about the online text data is always helpful. If the available model is large and contains rich knowledge (at least 7B trainable parameters), adding mental health domain information is also beneficial.

% Although there has been some recent research exploring low-cost training/finetuning for LLMs (\eg LoRA~\cite{hu_lora_2021}), it is

\textbf{With enough computing resources, instruction finetune models on various mental health datasets.}
When there are enough computing resources and model training/finetuning is possible, there are more options to enhance LLMs for mental health prediction tasks.
Our experiments clearly show that instruction finetuning can significantly boost the performance of models, especially for dialogue-based models since they can better understand and learn from human natural language. If there are multiple datasets available, merging multiple datasets and tasks altogether and finetuning the model in a single round is the most effective approach.

\textbf{Implement efficient finetuning with hundreds of examples and prioritize data variation when data resource is limited.}
Figure~\ref{fig:performance_datasize} shows that finetuning does not require large datasets. If there is no immediately available dataset, collecting small datasets with a few hundred samples is often good enough.
Moreover, when the overall dataset size is fixed (\eg due to limited data collection resources), collecting data from more diverse sources (each with a small size) is more meaningful than collecting one single larger dataset, as instruction finetuning works better when data has a larger variation. 

\textbf{Carefully calibrate the model when applying it to a new task.}
After instruction finetuning, a model still needs careful scrutiny on a new task, since our experiments suggest that LLMs' generalizability on unseen tasks still needs further improvement.
Testing the model on a few samples with ground truth can be an easy option to investigate its performance on the new task. If the performance is not promising, it may suggest another finetuning on the new task.

\textbf{More curated finetuning datasets are needed for mental health reasoning.}
Our case study suggests that Mental-Alpaca and Mental-FLAN-T5 can only generate classification labels after being finetuned solely on classification tasks, losing their reasoning capability. This is a major limitation of the current models.
One potential solution is to add more reasoning or causal datasets into the instruction finetuning process. In such a way, models can also learn the relationship between mental health outcomes and causal factors.

\textbf{Despite the promising capability of LLMs for mental health tasks, they are still far from being deployable in the real world.}
Our experiments reveal the encouraging performance of LLMs on mental health prediction and reasoning tasks.
However, as we note in Sec.~\ref{sub:discussion:ethics}, our current results do not indicate LLMs' deployability in real-life mental health settings. There are many important ethical and safety concerns and gaps before deployment to be addressed before achieving robustness and deployability.

\subsection{Beyond Mental Health Prediction Task and Textual Data}
\label{sub:discussion:beyond}
Our current experiments mainly involve mental health prediction tasks, which are essentially classification problems. There are more types of tasks that our experiments don't cover, such as regression (\eg predicting a score on a mental health scale). In particular, reasoning is an attractive task as it can fully leverage the capability of LLMs on language generation~\cite{nori_capabilities_2023,bubeck_sparks_2023}. Our initial case study on reasoning is limited but reveals promising performance, especially for GPT-4. We plan to conduct more experiments on tasks that go beyond classification.
Meanwhile, we mainly focus on online text data in this paper. But there are more data streams available, such as sensor data from mobile phones and wearables~\cite{xu_globem_2022,xu_globem_2022-1}. This leads to another open question on how to leverage LLMs for time-series data, another exciting direction to explore in future work.

\subsection{Ethics in LLMs and Deployability Gaps for Mental Health}
\label{sub:discussion:ethics}
Although our experiments on LLMs have shown promising capability for mental-health-related tasks, it still has a long way to go before being deployed in real-life systems. Recent research has revealed the potential bias or even harmful advice introduced by LLMs~\cite{Hoover_2023}, especially with the gender~\cite{ghosh2023chatgpt} and racial~\cite{abid2021persistent} gaps.
In mental health, these gaps and disparities between population groups have been long-standing~\cite{irene_y_chen_can_2019}.
Meanwhile, our case study of incorrect prediction, over-generalization, and ``falsely reasonable'' explanations further highlight the risk of current LLMs.
Recent studies are calling for more research emphasis and efforts in assessing and mitigating these biases for mental health~\cite{irene_y_chen_can_2019,timmons2022call}.

Although with a much stronger capability of understanding natural language (and early signs of mental health domain knowledge in our case), LLMs are no different from other modern AI models that are trained on a large amount of human-generated content, which exhibit all the biases that humans do~\cite{irene_y_chen_ethical_2021,ntoutsi2020bias}.
Meanwhile, although we carefully picked datasets with human expert annotations, there exist potential biases in the labels, such as stereotypes~\cite{pessach2022review}, confirmation bias~\cite{gemalmaz2021accounting}, normative \vs descriptive labels~\cite{balagopalan_judging_2023}. 
Besides, privacy is another important concern. Although our datasets are based on public social media platforms, it is necessary to carefully handle mental-health-related data and guarantee anonymity in any future efforts.
These ethical concerns need to receive attention not only at the monitoring and prediction stage, but also in the downstream applications, ranging from assistants for mental health experts to chatbots for end-users.
Careful efforts into safe development, auditing, and regulation are very much needed to address these ethical risks.


\subsection{Limitations}
\label{sub:discussion:limitation}
Our paper has a few limitations. First, although we carefully inspect the quality of our dataset and cover different categories of LLM, the number of datasets and the types of LLMs are still limited. Our findings are based on the observations of these datasets and models, which may not generalize to other cases.
Related, our exploration of zero-shot few-shot prompt design is not comprehensive.  The limited input window of some models also limits our exploration of more samples for few-shot prompting.
Moreover, we didn't systematically evaluate the mental health reasoning performance of these models.
Future work can design larger-scale experiments to include more datasets, models, prompt designs, and better evaluation.

Second, these datasets collected data from public social media platforms (mainly Reddit), which could be limited. Meanwhile, although the labels are not directly accessible on the platforms, it is possible that these text data have been included in the initial training of these large models. We still argue that there is no information leakage as long as the models haven't seen the labels for our tasks, but it is hard to measure how the initial training process may affect the outcomes in our evaluation. 

Third, another important limitation of the current work is the lack of evaluation of model fairness. Our anonymous datasets do not include comprehensive demographic information, making it hard to compare the performance across different population groups. As we discussed in the previous section, lots of future work on ethics and fairness is necessary before deploying such systems in real life.