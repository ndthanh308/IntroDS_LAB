\section{Background}
\label{sec:background}
We briefly summarize the related work in leveraging online text data for mental health prediction (Sec.~\ref{sub:background:online_text}). We also provide an overview of the ongoing research in LLMs and their application in the health domain (Sec.~\ref{sub:background:LLM}).

\subsection{Online Text Data and Mental Health}
\label{sub:background:online_text}
Online platforms, especially social media platforms, have been acknowledged as a promising lens that is capable of revealing insights into the psychological states, health, and well-being of both individuals and populations~\cite{paul_you_2011,culotta_estimating_2014,chancellor_methods_2020,guntuku_detecting_2017,de_choudhury_mental_2014}.
In the past decade, there has been extensive research about leveraging content analysis and social interaction patterns to identify and predict risks associated with mental health issues, such as anxiety~\cite{ahmed2022machine,saifullah2021comparison,shen_detecting_2017}, major depressive disorder~\cite{park_perception_2021,de_choudhury_predicting_2013,tsugawa_recognizing_2015,de_choudhury_social_2013}, suicide ideation~\cite{de_choudhury_discovering_2016,ruder_suicide_2011,burnap_machine_2015,tadesse_detection_2019,coppersmith_natural_2018}, and others~\cite{coppersmith_adhd_2015,mitchell_quantifying_2015,coppersmith_measuring_2014,10.1145/3130960}. Social media's real-time nature, coupled with its archival capability, can often mitigate retrospective bias. The rich amount of social media data also facilitates the identification, monitoring, and potential prediction of risk factors over time. In addition to observation and detection, social media platforms could further serve as effective channels to offer in-time assistance to communities at risk~\cite{livingston_another_2014,ridout_use_2018,kruzan_social_2022}.

From the computational technology perspective, early research started with basic methods~\cite{coppersmith_measuring_2014,mitchell_quantifying_2015,de_choudhury_social_2013}. For example, pioneering work by Coppersmith \etal~\cite{coppersmith_measuring_2014} employed correlation analysis to reveal the relationship between social media language data and mental health conditions.
Since then, researchers have proposed a wide range of feature engineering methods and built machine-learning models for the prediction~\cite{moreno2011feeling,nguyen2014affective,birnbaum2017collaborative,tsugawa_recognizing_2015,rumshisky2016predicting}. For example, De Choudhury \etal~\cite{de_choudhury_predicting_2013} extracted a number of linguistic styles and other features to build an SVM model to perform depression prediction.
Researchers have also explored deep-learning-based models for mental health prediction to obviate the need for hand-crafted features~\cite{sawhney2018exploring,ji2018supervised}. For instance, Tadesse~\etal~\cite{tadesse_detection_2019} employed an LSTM-CNN model and took word embeddings as the input to detect suicide ideation on Reddit.
More recently, pre-trained language models have become a popular method for NLP tasks, including mental health prediction tasks~\cite{ji_mentalbert_2021,nguyen2022improving,han2022hierarchical}. For example, Jiang~\etal~\cite{jiang2020detection} used the contextual representations from BERT as input features for mental health issue detection.
Otsuka~\etal~\cite{otsukadiagnosing} evaluated the performance of BERT-based pre-trained models in clinical settings.
Meanwhile, researchers have also explored the multi-task setup~\cite{benton2017multi} that aims to predict multiple labels. For example, Sarkar \etal~\cite{sarkar2022predicting} trained a multi-task model to predict depression and anxiety at the same time. However, these multi-task models are constrained to a predetermined task set and thus have limited flexibility.
Our work joins the same goal and aims to achieve a more flexible multi-task capability. We focus on the next-generation technology of instruction-finetuned LLMs, leverage their power in natural language understanding, and explore their capability on mental health tasks with social media data. 

\subsection{LLM and Health Applications}
\label{sub:background:LLM}
After the great success of Transformer-based language models such as BERT~\cite{devlin_bert_2019} and GPT~\cite{radford_improving_2018}, researchers and practitioners have advanced towards larger and more powerful language models (\eg GPT-3~\cite{brown_language_2020} and T5~\cite{raffel_exploring_2020}).
Meanwhile, researchers proposed instruction finetuning by including diverse instructions (\ie prompts) across various datasets and tasks during both the training and generation phases, which guides a model to carry out a variety of tasks while remaining within a single, unified model~\cite{wei_finetuned_2022}.
These instruction-finetuned LLMs, such as GPT-4~\cite{bubeck_sparks_2023}, PaLM~\cite{chowdhery_palm_2022}, FLAN-T5~\cite{chung_scaling_2022}, LLaMA~\cite{touvron_llama_2023}, Alpaca~\cite{taori_stanford_2023}, contain tens to hundreds of billions of parameters and achieves a promising performance on a variety of tasks, such as question answering~\cite{omar2023chatgpt,robinson2023leveraging}, logic reasoning~\cite{wei_chain--thought_2023,zhou_least--most_2023}, machine translation~\cite{brants2007large,gulcehre2017integrating}, \etc.

Researchers have explored the capability of these LLMs in health fields~\cite{jiang_health_2023,singhal_towards_2023,li_chatdoctor_2023,liu_large_2023,wu_pmc-llama_2023,nori_capabilities_2023}. For example,
Singhal~\etal~\cite{singhal_towards_2023} finetuned PaLM-2 on medical domains and achieved 86.5\% on MedQA dataset.
Similarly, Wu~\etal~\cite{wu_pmc-llama_2023} finetuned LLaMA on medical papers and showed promising results on multiple biomedical QA datasets.
Jiang~\etal~\cite{jiang_health_2023} trained a medical language model on unstructured clinical notes from the electronic health record and fine-tuned it across a wide range of clinical and operational predictive tasks. Their evaluation indicates that such a model can be used for various clinical tasks.

There is relatively less work in the mental health domain.
Some work explored the capability of LLMs for sentiment analysis and emotion reasoning~\cite{qin2023chatgpt,zhong2023can,kocon2023chatgpt}.
Closer to our study, Lamichhane~\cite{lamichhane_evaluation_2023}, Amin~\etal~\cite{amin_will_2023}, and Yang~\etal~\cite{yang_evaluations_2023} tested the performance of ChatGPT on multiple classification tasks (stress, depression, and suicide detection) and showed that ChatGPT shows the initial potential for mental health applications, but it still has a great room for improvement.
However, all previous studies stopped at zero-shot prompting and did not try other methods to improve the performance of LLMs. Moreover, none of the existing work explores the capability of the most recent GPT-4.
In this work, we present the first systematic exploration of multiple LLMs on their performance on mental health tasks, as well as multiple methods to improve their capabilities.