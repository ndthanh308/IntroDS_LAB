\begin{table}[!t]
\centering
\caption{
Balanced Accuracy Performance Summary of Zero-shot, Few-shot and Instruction Finetuning on LLMs. 
$context$, $mh$, and $both$ indicates the prompt design strategies of context enhancement, mental health enhancement, and their combination  (see Table.~\ref{tab:prompt_design}).
Small numbers represent standard deviation across different designs of \textit{Prompt}$_{\textit{Part1-S}}$ and \textit{Prompt}$_{\textit{Part2-Q}}$. The baselines at top rows do not have standard deviation as the task-specific output is static and prompt designs do not apply.
Due to the maximum token size limit, we only conduct few-shot prompting on a subset of datasets and mark other infeasible datasets as ``--''.
For each column, the best result is \textbf{bolded}, and the second best is \underline{underlined}.
}
\label{tab:results_overall}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{llcccccc}
\thickhlinespace
& \makecell[r]{\textbf{Dataset}} & \textbf{Dreaddit} & \multicolumn{2}{c}{\textbf{DepSeverity}} & \textbf{SDCNL} & \multicolumn{2}{c}{\textbf{CSSRS-Suicide}} \\ \addlinespace[1ex]
\textbf{Category} & \textbf{Model}   & \textbf{Task \#1}       & \textbf{Task \#2}             & \textbf{Task \#3}             & \textbf{Task \#4}    & \textbf{Task \#5}     & \textbf{Task \#6} \\ \thickhlinespace
\multirow{23}{*}{\makecell{Zero-shot\\Prompting}} & Alpaca$_{ZS}$         & 0.593$_{\pm0.039}$ & 0.522$_{\pm0.022}$ & 0.431$_{\pm0.050}$ & 0.493$_{\pm0.007}$ & 0.518$_{\pm0.037}$ & 0.232$_{\pm0.076}$ \\
& Alpaca$_{ZS\_context}$ & 0.612$_{\pm0.065}$ & 0.567$_{\pm0.077}$ & 0.454$_{\pm0.143}$ & 0.497$_{\pm0.006}$ & 0.532$_{\pm0.033}$ & 0.250$_{\pm0.060}$ \\
& Alpaca$_{ZS\_mh}$      & 0.593$_{\pm0.031}$ & 0.577$_{\pm0.028}$ & 0.444$_{\pm0.090}$ & 0.482$_{\pm0.015}$ & 0.523$_{\pm0.013}$ & 0.235$_{\pm0.033}$ \\
& Alpaca$_{ZS\_both}$    & 0.540$_{\pm0.029}$ & 0.559$_{\pm0.040}$ & 0.421$_{\pm0.095}$ & 0.532$_{\pm0.005}$ & 0.511$_{\pm0.011}$ & 0.221$_{\pm0.030}$ \\ \cdashlinespace{2-8}
& Alpaca-LoRA$_{ZS}$           & 0.571$_{\pm0.043}$ & 0.548$_{\pm0.027}$ & 0.437$_{\pm0.044}$ & 0.502$_{\pm0.011}$ & 0.540$_{\pm0.012}$ & 0.187$_{\pm0.053}$ \\
& Alpaca-LoRA$_{ZS\_context}$   & 0.537$_{\pm0.047}$ & 0.501$_{\pm0.001}$ & 0.343$_{\pm0.152}$ & 0.472$_{\pm0.020}$ & 0.567$_{\pm0.038}$ & 0.214$_{\pm0.059}$ \\
& Alpaca-LoRA$_{ZS\_mh}$        & 0.500$_{\pm0.000}$ & 0.500$_{\pm0.000}$ & 0.331$_{\pm0.145}$ & 0.497$_{\pm0.025}$ & 0.557$_{\pm0.023}$ & 0.216$_{\pm0.022}$ \\
& Alpaca-LoRA$_{ZS\_both}$      & 0.500$_{\pm0.000}$ & 0.500$_{\pm0.000}$ & 0.386$_{\pm0.059}$ & 0.499$_{\pm0.023}$ & 0.517$_{\pm0.031}$ & 0.224$_{\pm0.049}$ \\ \cdashlinespace{2-8}
 & FLAN-T5$_{ZS}$ & 0.659$_{\pm0.086}$ & 0.664$_{\pm0.011}$ & 0.396$_{\pm0.006}$ & 0.643$_{\pm0.021}$ & 0.667$_{\pm0.023}$ & 0.418$_{\pm0.012}$ \\
 & FLAN-T5$_{ZS\_context}$ & 0.663$_{\pm0.079}$ & 0.674$_{\pm0.014}$ & 0.378$_{\pm0.013}$ & 0.653$_{\pm0.011}$ & 0.649$_{\pm0.026}$ & 0.378$_{\pm0.029}$ \\
 & FLAN-T5$_{ZS\_mh}$ & 0.616$_{\pm0.070}$ & 0.666$_{\pm0.009}$ & 0.366$_{\pm0.012}$ & 0.648$_{\pm0.010}$ & 0.653$_{\pm0.018}$ & 0.372$_{\pm0.033}$ \\
 & FLAN-T5$_{ZS\_both}$ & 0.604$_{\pm0.074}$ & 0.661$_{\pm0.004}$ & 0.389$_{\pm0.051}$ & 0.645$_{\pm0.005}$ & 0.657$_{\pm0.019}$ & 0.382$_{\pm0.048}$ \\ \cdashlinespace{2-8}
& GPT-3.5$_{ZS}$         & 0.685$_{\pm0.024}$ & 0.642$_{\pm0.017}$ & 0.603$_{\pm0.017}$ & 0.460$_{\pm0.163}$ & 0.570$_{\pm0.118}$ & 0.233$_{\pm0.009}$ \\
& GPT-3.5$_{ZS\_context}$  & 0.688$_{\pm0.045}$ & 0.653$_{\pm0.020}$ & 0.543$_{\pm0.047}$ & 0.618$_{\pm0.008}$ & 0.577$_{\pm0.090}$ & 0.265$_{\pm0.048}$ \\
& GPT-3.5$_{ZS\_mh}$      & 0.679$_{\pm0.017}$ & 0.636$_{\pm0.021}$ & 0.642$_{\pm0.034}$ & 0.576$_{\pm0.001}$ & 0.477$_{\pm0.014}$ & 0.310$_{\pm0.015}$ \\
& GPT-3.5$_{ZS\_both}$    & 0.681$_{\pm0.010}$ & 0.627$_{\pm0.022}$ & 0.617$_{\pm0.014}$ & 0.632$_{\pm0.020}$ & 0.617$_{\pm0.033}$ & 0.254$_{\pm0.009}$  \\ \cdashlinespace{2-8}
 & GPT-4$_{ZS}$ & 0.700$_{\pm0.001}$ & 0.719$_{\pm0.013}$ & 0.588$_{\pm0.010}$ & 0.644$_{\pm0.007}$ & 0.760$_{\pm0.009}$ & 0.418$_{\pm0.009}$ \\
 & GPT-4$_{ZS\_context}$ & 0.706$_{\pm0.009}$ & 0.719$_{\pm0.009}$ & 0.590$_{\pm0.011}$ & 0.644$_{\pm0.011}$ & 0.753$_{\pm0.028}$ & \underline{0.441}$_{\pm0.057}$ \\
 & GPT-4$_{ZS\_mh}$ & 0.725$_{\pm0.009}$ & 0.684$_{\pm0.004}$ & 0.656$_{\pm0.001}$ & 0.645$_{\pm0.012}$ & 0.737$_{\pm0.005}$ & 0.396$_{\pm0.020}$ \\
 & GPT-4$_{ZS\_both}$ & 0.719$_{\pm0.021}$ & 0.689$_{\pm0.000}$ & 0.650$_{\pm0.011}$ & 0.647$_{\pm0.014}$ & 0.697$_{\pm0.005}$ & 0.411$_{\pm0.009}$ \\
\thickhlinespace
\multirow{4}{*}{\makecell{Few-shot\\Prompting}} &  Alpaca$_{FS}$         & 0.632$_{\pm0.030}$ & 0.529$_{\pm0.017}$ & 0.628$_{\pm0.005}$ & ---                & ---                & ---                \\
& FLAN-T5$_{FS}$         & 0.786$_{\pm0.006}$ & 0.678$_{\pm0.009}$ & 0.432$_{\pm0.009}$ & ---                & ---                & ---                \\
& GPT-3.5$_{FS}$         & 0.721$_{\pm0.010}$ & 0.665$_{\pm0.015}$ & 0.580$_{\pm0.002}$ & ---                & ---                & ---                \\
& GPT-4$_{FS}$         & 0.698$_{\pm0.009}$ & 0.724$_{\pm0.005}$ & 0.613$_{\pm0.001}$ & ---                & ---                & ---                \\\thickhlinespace
\multirow{2}{*}{\makecell{Instructional\\Finetuning}} & Mental-Alpaca         & \underline{0.816}$_{\pm0.006}$ & \underline{0.775}$_{\pm0.006}$ & \underline{0.746}$_{\pm0.005}$ & \textbf{0.724}$_{\pm0.004}$ & 0.730$_{\pm0.048}$ & 0.403$_{\pm0.029}$ \\
& Mental-FLAN-T5 &  0.802$_{\pm0.002}$ & 0.759$_{\pm0.003}$ & \textbf{0.756}$_{\pm0.001}$ & 0.677$_{\pm0.005}$ & \textbf{0.868}$_{\pm0.006}$ & \textbf{0.481}$_{\pm0.006}$ \\ \thickhlinespace
\multirow{3}{*}{\xspace\xspace\xspace Baseline} & Majority          & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.250$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.200$_{\pm ---}$  \\
& BERT              & 0.783$_{\pm ---}$  & 0.763$_{\pm ---}$  & 0.690$_{\pm ---}$  & 0.678$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.332$_{\pm ---}$  \\
& Mental-RoBERTa     & \textbf{0.831}$_{\pm ---}$  & \textbf{0.790}$_{\pm ---}$   & 0.736$_{\pm ---}$  & \underline{0.723}$_{\pm ---}$  & \underline{0.853}$_{\pm ---}$  & 0.373$_{\pm ---}$  \\ \thickhlinespace
\end{tabular}
}
\end{table}