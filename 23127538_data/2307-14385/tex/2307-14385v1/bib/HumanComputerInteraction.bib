
@article{dey_cappella:_2004,
	title = {a {CAPpella}: programming by demonstration of context-aware applications},
	volume = {6},
	issn = {1581137028},
	url = {http://portal.acm.org/citation.cfm?id=985697},
	doi = {10.1145/985692.985697},
	abstract = {Context-aware applications are applications that implicitly take their context of use into account by adapting to changes in a user's activities and environments. No one has more intimate knowledge about these activities and environments than end-users themselves. Currently there is no support for end-users to build context-aware applications for these dynamic settings. To address this issue, we present a CAPpella, a programming by demonstration Context-Aware Prototyping environment intended for end-users. Users "program" their desired context-aware behavior (situation and associated action) in situ, without writing any code, by demonstrating it to a CAPpella and by annotating the relevant portions of the demonstration. Using a meeting and medicine-taking scenario, we illustrate how a user can demonstrate different behaviors to a CAPpella. We describe a CAPpella's underlying system to explain how it supports users in building behaviors and present a study of 14 end-users to illustrate its feasibility and usability.},
	number = {1},
	journal = {Proceedings of the SIGCHI conference on Human factors in computing systems},
	author = {Dey, Anind K. and Hamid, R. and Beckmann, C. and Li, I. and Hsu, D.},
	year = {2004},
	keywords = {context-aware computing, demonstration, end-user programming, programming-by-, statistical machine},
	pages = {40},
	file = {Dey et al_2004_a CAPpella.pdf:/Users/orsonxu/Zotero/storage/WNXFZLH6/Dey et al_2004_a CAPpella.pdf:application/pdf;Dey et al_2004_a CAPpella.pdf:/Users/orsonxu/Zotero/storage/QGDD9KWU/Dey et al_2004_a CAPpella.pdf:application/pdf},
}

@article{mannino_expressive_2018,
	title = {Expressive {Time} {Series} {Querying} with {Hand}-{Drawn} {Scale}-{Free} {Sketches}},
	issn = {0017-9124},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173962},
	doi = {10.1145/3173574.3173962},
	abstract = {We present Qetch, a tool where users freely sketch patterns on a scale-less canvas to query time series data without specifying query length or amplitude. We study how humans sketch time series patterns — humans preserve visually salient perceptual features but often non-uniformly scale and locally distort a pattern — and we develop a novel matching algorithm that accounts for human sketching errors. Qetch enables the easy construction of complex and expressive queries with two key features: regular expressions over sketches and relative po-sitioning of sketches to query multiple time-aligned series. Through user studies, we demonstrate the effectiveness of Qetch's different interaction features. We also demonstrate the effectiveness of Qetch's matching algorithm compared to popular algorithms on targeted, and exploratory query-by-sketch search tasks on a variety of data sets.},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems  - CHI '18},
	author = {Mannino, Miro and Abouzied, Azza},
	year = {2018},
	keywords = {regular expressions, scale-less sketches, Time series querying by sketching},
	pages = {1--13},
	file = {Mannino_Abouzied_2018_Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches.pdf:/Users/orsonxu/Zotero/storage/BQVFAQFL/Mannino_Abouzied_2018_Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches.pdf:application/pdf;Mannino_Abouzied_2018_Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches.pdf:/Users/orsonxu/Zotero/storage/YB47W4IW/Mannino_Abouzied_2018_Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches.pdf:application/pdf},
}

@article{porcheron_voice_2018,
	title = {Voice {Interfaces} in {Everyday} {Life}},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3174214},
	doi = {10.1145/3173574.3174214},
	abstract = {Voice User Interfaces (VUIs) are becoming ubiquitously available, being embedded both into everyday mobility via smartphones, and into the life of the home via ‘assistant’ devices. Yet, exactly how users of such devices practically thread that use into their everyday social interactions remains underexplored. By collecting and studying audio data from month-long deployments of the Amazon Echo in participants’ homes—informed by ethnomethodology and conversation analysis—our study documents the methodical practices of VUI users, and how that use is accomplished in the complex social life of the home. Data we present shows how the device is made accountable to and embedded into conversational settings like family dinners where various simultaneous activities are being achieved. We discuss how the VUI is finely coordinated with the sequential organisation of talk. Finally, we locate implications for the accountability of VUI interaction, request and response design, and raise conceptual challenges to the notion of designing ‘conversational’ interfaces.},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems  - CHI '18},
	author = {Porcheron, Martin and Fischer, Joel E. and Reeves, Stuart and Sharples, Sarah},
	year = {2018},
	keywords = {Amazon Echo, voice user interface, conversational},
	pages = {1--12},
	file = {Porcheron et al_2018_Voice Interfaces in Everyday Life.pdf:/Users/orsonxu/Zotero/storage/ATQT8HPG/Porcheron et al_2018_Voice Interfaces in Everyday Life.pdf:application/pdf;Porcheron et al_2018_Voice Interfaces in Everyday Life.pdf:/Users/orsonxu/Zotero/storage/WXNVGICB/Porcheron et al_2018_Voice Interfaces in Everyday Life.pdf:application/pdf},
}

@article{kyto_pinpointing_2018,
	title = {Pinpointing},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173655},
	doi = {10.1145/3173574.3173655},
	abstract = {See, stats, and : https : / / www . researchgate . net / publication / 323970135 Pinpointing : Precise - and - Based Selection Conference DOI : 10 . 1145 / 3173574 . 3173655 CITATIONS 0 READS 217 5 , including : Some : UI - ART Spatial Mikko Aalto 18 SEE Barrett University 28 SEE Thammathip University 26 SEE Gun . Lee University 90 SEE All . The . ABSTRACT Head and eye movement can be leveraged to improve the user ' s interaction repertoire for wearable displays . Head movements are deliberate and accurate , and provide the current state - of - the - art pointing technique . Eye gaze can potentially be faster and more ergonomic , but suffers from low accuracy due to calibration errors and drift of wearable eye - tracking sensors . This work investigates precise , multimodal selection techniques using head motion and eye gaze . A comparison of speed and pointing accuracy reveals the relative merits of each method , including the achievable target size for robust selection . We demonstrate and discuss example applications for augmented reality , including compact menus with deep structure , and a proof - of - concept method for on - line correction of calibration drift .},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems  - CHI '18},
	author = {Kytö, Mikko and Ens, Barrett and Piumsomboon, Thammathip and Lee, Gun A. and Billinghurst, Mark},
	year = {2018},
	pages = {1--14},
	file = {Kytö et al_2018_Pinpointing.pdf:/Users/orsonxu/Zotero/storage/YNHU3RT2/Kytö et al_2018_Pinpointing.pdf:application/pdf;Kytö et al_2018_Pinpointing.pdf:/Users/orsonxu/Zotero/storage/QKA6EM45/Kytö et al_2018_Pinpointing.pdf:application/pdf},
}

@article{kim_agile_2018,
	title = {Agile {3D} {Sketching} with {Air} {Scaffolding}},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173812},
	doi = {10.1145/3173574.3173812},
	abstract = {yongkwan.kim {\textbar} sang-gyun.an {\textbar} joonhyub.lee {\textbar} seokhyung.bae @ kaist.ac.kr (a) (b) (c) Figure 1. In our agile 3D sketching workflow with air scaffolding, in which hand motion and pen drawing complement each other, (a) the user makes unconstrained hand movements in the air to quickly generate rough shapes to be used as scaffolds, (b) uses the scaffolds as references and draws finer details with them (c) in an iterative and progressive manner to produce a high-fidelity 3D concept sketch of a steering wheel. ABSTRACT Hand motion and pen drawing can be intuitive and expressive inputs for professional digital 3D authoring. However, their inherent limitations have hampered wider adoption. 3D sketching using hand motion is rapid but rough, and 3D sketching using pen drawing is delicate but tedious. Our new 3D sketching workflow combines these two in a complementary manner. The user makes quick hand motions in the air to generate approximate 3D shapes, and uses them as scaffolds on which to add details via pen-based 3D sketching on a tablet device. Our air scaffolding technique and corresponding algorithm extract only the intended shapes from unconstrained hand motions. Then, the user sketches 3D ideas by defining sketching planes on these scaffolds while appending new scaffolds, as needed. A user study shows that our progressive and iterative workflow enables more agile 3D sketching compared to ones using either hand motion or pen drawing alone.},
	journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '18)},
	author = {Kim, Yongkwan and An, Sang-Gyun and Lee, Joon Hyub and Bae, Seok-Hyung},
	year = {2018},
	keywords = {3d sketching, hand motion, product design, scaffolding},
	pages = {1--12},
	file = {Kim et al_2018_Agile 3D Sketching with Air Scaffolding.pdf:/Users/orsonxu/Zotero/storage/QRW8RF4T/Kim et al_2018_Agile 3D Sketching with Air Scaffolding.pdf:application/pdf;Kim et al_2018_Agile 3D Sketching with Air Scaffolding.pdf:/Users/orsonxu/Zotero/storage/7QK3FIZU/Kim et al_2018_Agile 3D Sketching with Air Scaffolding.pdf:application/pdf},
}

@article{cockburn_hark_2018,
	title = {{HARK} {No} {More} : {On} the {Preregistration} of {CHI} {Experiments}},
	issn = {2047-9980},
	doi = {10.1145/3173574.3173715},
	abstract = {© 2018 ACM. Experimental preregistration is required for publication in many scientific disciplines and venues. When experimental intentions are preregistered, reviewers and readers can be confident that experimental evidence in support of reported hypotheses is not the result of HARKing, which stands for Hypothesising After the Results are Known. We review the motivation and outcomes of experimental preregistration across a variety of disciplines, as well as previous work commenting on the role of evaluation in HCI research. We then discuss how experimental preregistration could be adapted to the distinctive characteristics of Human-Computer Interaction empirical research, to the betterment of the discipline.},
	journal = {Proc. of CHI},
	author = {Cockburn, Andy and Gutwin, Carl and Dix, Alan},
	year = {2018},
	pages = {1--12},
	file = {Cockburn et al_2018_HARK No More.pdf:/Users/orsonxu/Zotero/storage/53RK8PGL/Cockburn et al_2018_HARK No More.pdf:application/pdf;Cockburn et al_2018_HARK No More.pdf:/Users/orsonxu/Zotero/storage/HYIKH6KY/Cockburn et al_2018_HARK No More.pdf:application/pdf},
}

@article{zhang_wall++:_2018,
	title = {Wall++: {Room}-{Scale} {Interactive} and {Context}-{Aware} {Sensing} {Yang}},
	issn = {0096-1736 (Print)},
	doi = {10.1145/3173574.3173847},
	abstract = {Human environments are typified by walls – homes, offices, schools, museums, hospitals and pretty much every indoor context one can imagine has walls. In many cases, they make up a majority of readily accessible indoor surface area, and yet they are static – their primary function is to be a wall, separating spaces and hiding infrastructure. We pre-sent Wall++, a low-cost sensing approach that allows walls to become a smart infrastructure. Instead of merely separat-ing spaces, walls can now enhance rooms with sensing and interactivity. Our wall treatment and sensing hardware can track users' touch and gestures, as well as estimate body pose if they are close. By capturing airborne electromagnet-ic noise, we can also detect what appliances are active and where they are located. Through a series of evaluations, we demonstrate Wall++ can enable robust room-scale interac-tive and context-aware applications.},
	journal = {Proc. of CHI},
	author = {Zhang, Yang and Yang, Chouchang (Jack) and Hudson, Scott E and Harrison, Chris and Sample, Alanson},
	year = {2018},
	keywords = {internet of things, touch sensing, context aware, em sensing, gestures, indoor localization, pose estimation, smart environments, user identification},
	pages = {1--15},
	file = {Zhang et al_2018_Wall++.pdf:/Users/orsonxu/Zotero/storage/6I7KSZIJ/Zhang et al_2018_Wall++.pdf:application/pdf;Zhang et al_2018_Wall++.pdf:/Users/orsonxu/Zotero/storage/PSB3J8HV/Zhang et al_2018_Wall++.pdf:application/pdf},
}

@article{wobbrock_user-defined_2009,
	title = {User-defined gestures for surface computing},
	url = {http://dl.acm.org/citation.cfm?doid=1518701.1518866},
	doi = {10.1145/1518701.1518866},
	abstract = {Many surface computing prototypes have employed gestures created by system designers. Although such gestures are appropriate for early investigations, they are not necessarily reflective of user behavior. We present an approach to designing tabletop gestures that relies on eliciting gestures from non-technical users by first portraying the effect of a gesture, and then asking users to perform its cause. In all, 1080 gestures from 20 participants were logged, analyzed, and paired with think-aloud data for 27 commands performed with 1 and 2 hands. Our findings indicate that users rarely care about the number of fingers they employ, that one hand is preferred to two, that desktop idioms strongly influence users' mental models, and that some commands elicit little gestural agreement, suggesting the need for on-screen widgets. We also present a complete user-defined gesture set, quantitative agreement scores, implications for surface technology, and a taxonomy of surface gestures. Our results will help designers create better gesture sets informed by user behavior.},
	journal = {Proceedings of the 27th international conference on Human factors in computing systems - CHI 09},
	author = {Wobbrock, Jacob O. and Morris, Meredith Ringel and Wilson, Andrew D.},
	year = {2009},
	pages = {1083},
	file = {Wobbrock et al_2009_User-defined gestures for surface computing.pdf:/Users/orsonxu/Zotero/storage/D2DENZ8N/Wobbrock et al_2009_User-defined gestures for surface computing.pdf:application/pdf;Wobbrock et al_2009_User-defined gestures for surface computing.pdf:/Users/orsonxu/Zotero/storage/BC5UK83D/Wobbrock et al_2009_User-defined gestures for surface computing.pdf:application/pdf},
}

@article{ruiz_user-defined_2011,
	title = {User-defined motion gestures for mobile interaction},
	doi = {10.1145/1978942.1978971},
	abstract = {Modern smartphones contain sophisticated sensors to monitor three-dimensional movement of the device. These sensors permit devices to recognize motion gestures - deliberate movements of the device by end-users to invoke commands. However, little is known about best-practices in motion gesture design for the mobile computing paradigm. To address this issue, we present the results of a guessability study that elicits end-user motion gestures to invoke commands on a smartphone device. We demonstrate that consensus exists among our participants on parameters of movement and on mappings of motion gestures onto commands. We use this consensus to develop a taxonomy for motion gestures and to specify an end-user inspired motion gesture set. We highlight the implications of this work to the design of smartphone applications and hardware. Finally, we argue that our results influence best practices in design for all gestural interfaces. Copyright 2011 ACM.},
	author = {Ruiz, Jaime and Li, Yang and Lank, Edward},
	year = {2011},
	pages = {197},
	file = {Ruiz et al_2011_User-defined motion gestures for mobile interaction.pdf:/Users/orsonxu/Zotero/storage/RDA56ZQ8/Ruiz et al_2011_User-defined motion gestures for mobile interaction.pdf:application/pdf;Ruiz et al_2011_User-defined motion gestures for mobile interaction.pdf:/Users/orsonxu/Zotero/storage/CWHAXFKJ/Ruiz et al_2011_User-defined motion gestures for mobile interaction.pdf:application/pdf},
}

@article{piumsomboon_user-defined_2013,
	title = {User-{Defined} {Gestures} for {Augmented} {Reality}},
	volume = {8118},
	url = {http://link.springer.com/10.1007/978-3-642-40480-1_18%0Apapers3://publication/doi/10.1007/978-3-642-40480-1_18},
	number = {Chapter 18},
	journal = {Human-Computer Interaction – INTERACT 2013},
	author = {Piumsomboon, Thammathip and Clark, Adrian and Billinghurst, Mark and Cockburn, Andy},
	year = {2013},
	keywords = {gestures, augmented reality, guessability},
	pages = {282--299},
	file = {Piumsomboon et al_2013_User-Defined Gestures for Augmented Reality.pdf:/Users/orsonxu/Zotero/storage/PM2J8Q5T/Piumsomboon et al_2013_User-Defined Gestures for Augmented Reality.pdf:application/pdf;Piumsomboon et al_2013_User-Defined Gestures for Augmented Reality.pdf:/Users/orsonxu/Zotero/storage/AWQMM9MA/Piumsomboon et al_2013_User-Defined Gestures for Augmented Reality.pdf:application/pdf},
}

@article{iravantchi_interferi:_2019,
	title = {Interferi: {Gesture} {Sensing} using {On}-{Body} {Acoustic} {Interferometry}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300506},
	doi = {10.1145/3290605.3300506},
	journal = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems  - CHI '19},
	author = {Iravantchi, Yasha and Zhang, Yang and Bernitsas, Evi and Goel, Mayank and Harrison, Chris},
	year = {2019},
	keywords = {acoustic, acoustic in-, biosensing, face gesture, hand gesture, hand input, interaction techniques, terferometry, wearables},
	pages = {1--13},
	file = {Iravantchi et al_2019_Interferi.pdf:/Users/orsonxu/Zotero/storage/Q782N5J7/Iravantchi et al_2019_Interferi.pdf:application/pdf;Iravantchi et al_2019_Interferi.pdf:/Users/orsonxu/Zotero/storage/F3DBSWC7/Iravantchi et al_2019_Interferi.pdf:application/pdf},
}

@article{schwarz_probabilistic_2014,
	title = {Probabilistic palm rejection using spatiotemporal touch features and iterative classification},
	doi = {10.1145/2556288.2557056},
	abstract = {Tablet computers are often called upon to emulate classical pen-and-paper input. However, touchscreens typically lack the means to distinguish between legitimate stylus and finger touches and touches with the palm or other parts of the hand. This forces users to rest their palms elsewhere or hover above the screen, resulting in ergonomic and usability problems. We present a probabilistic touch filtering approach that uses the temporal evolution of touch contacts to reject palms. Our system improves upon previous approaches, reducing accidental palm inputs to 0.016 per pen stroke, while correctly passing 98\% of stylus inputs.},
	author = {Schwarz, Julia and Xiao, Robert and Mankoff, Jennifer and Hudson, Scott E. and Harrison, Chris},
	year = {2014},
	pages = {2009--2012},
	file = {Schwarz et al_2014_Probabilistic palm rejection using spatiotemporal touch features and iterative classification.pdf:/Users/orsonxu/Zotero/storage/6GWDFYD8/Schwarz et al_2014_Probabilistic palm rejection using spatiotemporal touch features and iterative classification.pdf:application/pdf;Schwarz et al_2014_Probabilistic palm rejection using spatiotemporal touch features and iterative classification.pdf:/Users/orsonxu/Zotero/storage/5R4KERCP/Schwarz et al_2014_Probabilistic palm rejection using spatiotemporal touch features and iterative classification.pdf:application/pdf},
}

@article{matero_identifying_2012,
	title = {Identifying unintentional touches on handheld touch screen devices},
	doi = {10.1145/2317956.2318031},
	abstract = {Accidental triggering of unwanted interaction when using a handheld touch screen device is a problem for many users. Accidental touches on capacitive touch screen based mobile telephones were analyzed in a user test. Patterns that are characteristic of unintentional touches were identified. Layout guidelines to reduce the amount of unintentional touches are presented. Additionally, filtering criteria is defined that rejected 79.6\% of unintentional touches whilst rejecting only 0.8\% of intentional touches.},
	author = {Matero, Juha and Colley, Ashley},
	year = {2012},
	pages = {506},
	file = {Matero_Colley_2012_Identifying unintentional touches on handheld touch screen devices.pdf:/Users/orsonxu/Zotero/storage/QH8EDCRW/Matero_Colley_2012_Identifying unintentional touches on handheld touch screen devices.pdf:application/pdf;Matero_Colley_2012_Identifying unintentional touches on handheld touch screen devices.pdf:/Users/orsonxu/Zotero/storage/Q3EA4KEH/Matero_Colley_2012_Identifying unintentional touches on handheld touch screen devices.pdf:application/pdf},
}

@article{le_fingers_2018,
	title = {Fingers' {Range} and {Comfortable} {Area} for {One}-{Handed} {Smartphone} {Interaction} {Beyond} the {Touchscreen}},
	doi = {10.1145/3173574.3173605},
	abstract = {Previous research and recent smartphone development presen-ted a wide range of input controls beyond the touchscreen. Fin-gerprint scanners, silent switches, and Back-of-Device (BoD) touch panels offer additional ways to perform input. However, with the increasing amount of input controls on the device, unintentional input or limited reachability can hinder inte-raction. In a one-handed scenario, we conducted a study to investigate the areas that can be reached without losing grip stability (comfortable area), and with stretched fingers (maxi-mum range) using four different phone sizes. We describe the characteristics of the comfortable area and maximum range for different phone sizes and derive four design implications for the placement of input controls to support one-handed BoD and edge interaction. Amongst others, we show that the index and middle finger are the most suited fingers for BoD interaction and that the grip shifts towards the top edge with increasing phone sizes.},
	author = {Le, Huy Viet and Mayer, Sven and Bader, Patrick and Henze, Niels},
	year = {2018},
	pages = {1--12},
	file = {Le et al_2018_Fingers' Range and Comfortable Area for One-Handed Smartphone Interaction Beyond the Touchscreen.pdf:/Users/orsonxu/Zotero/storage/5QUNMSHH/Le et al_2018_Fingers' Range and Comfortable Area for One-Handed Smartphone Interaction Beyond the Touchscreen.pdf:application/pdf;Le et al_2018_Fingers' Range and Comfortable Area for One-Handed Smartphone Interaction Beyond the Touchscreen.pdf:/Users/orsonxu/Zotero/storage/QFMXFZY7/Le et al_2018_Fingers' Range and Comfortable Area for One-Handed Smartphone Interaction Beyond the Touchscreen.pdf:application/pdf},
}

@article{serim_explicating_2019,
	title = {Explicating "{Implicit} {Interaction}": {An} {Examination} of the {Concept} and {Challenges} for {Research}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300647},
	doi = {10.1145/3290605.3300647},
	journal = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI '19},
	author = {Serim, Barış and Jacucci, Giulio},
	year = {2019},
	keywords = {explicit interaction, framework, implicit interaction, Implicit interaction, inten-, intentionality},
	pages = {1--16},
	file = {Serim_Jacucci_2019_Explicating Implicit Interaction.pdf:/Users/orsonxu/Zotero/storage/2LZ6TP7M/Serim_Jacucci_2019_Explicating Implicit Interaction.pdf:application/pdf;Serim_Jacucci_2019_Explicating Implicit Interaction.pdf:/Users/orsonxu/Zotero/storage/GDRZJVVR/Serim_Jacucci_2019_Explicating Implicit Interaction.pdf:application/pdf},
}

@article{lu_gesture_2015,
	title = {Gesture {On}: {Enabling} {Always}-{On} {Touch} {Gestures} for {Fast} {Mobile} {Access} from the {Device} {Standby} {Mode}},
	doi = {10.1145/2702123.2702610},
	author = {Lu, Hao and Li, Yang},
	year = {2015},
	pages = {3355--3364},
	file = {Lu_Li_2015_Gesture On.pdf:/Users/orsonxu/Zotero/storage/4LG6ZGIT/Lu_Li_2015_Gesture On.pdf:application/pdf;Lu_Li_2015_Gesture On.pdf:/Users/orsonxu/Zotero/storage/VRSIPBJB/Lu_Li_2015_Gesture On.pdf:application/pdf},
}

@article{hoang_body_2018,
	title = {Body as a {Canvas}: {An} {Exploration} on the {Role} of the {Body} as {Display} of {Digital} {Information}},
	doi = {10.1145/3196709.3196724},
	abstract = {Human body in HCI is often seen as an actuator for issuing commands and providing input to digital systems. We present the concept of the body as a canvas, in which the body acts as both an actuator and a display for information. Body as a canvas creates an interaction loop where interaction with information causes changes in the body, which in turn changes the display of information. Our qualitative study using an on-body projection system in a public exhibition investigates this concept with regards to body characteristics, types of body input, interactions between multiple bodies, and comparison to other display technologies. Our findings show that body as a canvas create connectedness between the body and information. Finally, we discuss how body characteristics and appearances can complement the information, when the body acts as a canvas.},
	journal = {DIS},
	author = {Hoang, Thuong N. and Ferdous, Hasan S. and Vetere, Frank and Reinoso, Martin},
	year = {2018},
	keywords = {Body as a canvas},
	pages = {253--263},
	file = {Hoang et al_2018_Body as a Canvas.pdf:/Users/orsonxu/Zotero/storage/V7HEP8CS/Hoang et al_2018_Body as a Canvas.pdf:application/pdf;Hoang et al_2018_Body as a Canvas.pdf:/Users/orsonxu/Zotero/storage/54ZA7WRR/Hoang et al_2018_Body as a Canvas.pdf:application/pdf},
}

@article{lissermann_earput:_2013,
	title = {{EarPut}: {Augmenting} {Behind}-the-ear {Devices} for {Ear}-based {Interaction}},
	url = {http://doi.acm.org/10.1145/2468356.2468592},
	doi = {10.1145/2468356.2468592},
	abstract = {www.blacpma.usach.cl Artículo Original {\textbar} Original Article 131 Evaluation of the bactericidal activity of secondary metabolites isolated from Heliotropium genus against Piscirickettsia salmonis [Evaluación de la actividad bactericida de metabolitos secundarios aislados desde especies del género Heliotropium contra Piscirickettsia salmonis] Abstract: The intracellular bacteria Piscirickettsia salmonis is the most prevalent pathogen in the Chilean salmon industry, responsible for 50\% of losses in recent years. So far, there are no effective treatments to control infections by this pathogen due to the emergence of antibiotics resistance. Therefore, it is extremely important to conduct research to find successful antibacterial therapies. In this paper, we evaluated the in vitro bactericidal activity of flavonoids and aromatic geranyl derivatives isolated from the resinous exudate of species Heliotropium filifolium, H. sinuatum y H. huascoense. The results showed that the compounds Filifolinone, Naringenine and 3-O-methylgalangine cause different percentage of mortality of bacteria and therefore they are good candidates to continue its evaluation in vitro and in vivo.},
	journal = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
	author = {Lissermann, Roman and Huber, Jochen and Hadjakos, Aristotelis and Mühlhäuser, Max},
	year = {2013},
	keywords = {mobile interaction, device augmentation, ear-based interaction, experiment, eyes-free, multi-touch, touch},
	pages = {1323--1328},
	file = {Lissermann et al_2013_EarPut.pdf:/Users/orsonxu/Zotero/storage/BW7PLAIR/Lissermann et al_2013_EarPut.pdf:application/pdf;Lissermann et al_2013_EarPut.pdf:/Users/orsonxu/Zotero/storage/ZIJMNKL8/Lissermann et al_2013_EarPut.pdf:application/pdf},
}

@article{weigel_skinmarks:_2017,
	title = {{SkinMarks}: {Enabling} {Interactions} on {Body} {Landmarks} {Using} {Conformal} {Skin} {Electronics}},
	url = {http://dl.acm.org/citation.cfm?doid=3025453.3025704},
	doi = {10.1145/3025453.3025704},
	abstract = {The body provides many recognizable landmarks due to the underlying skeletal structure and variations in skin texture, elasticity, and color. The visual and spatial cues of such body landmarks can help in localizing on-body interfaces, guide input on the body, and allow for easy recall of mappings. Our main contribution are SkinMarks, novel skin-worn I/O devices for precisely localized input and output on fine body landmarks. SkinMarks comprise skin electronics on tempo-rary rub-on tattoos. They conform to fine wrinkles and are compatible with strongly curved and elastic body locations. We identify five types of body landmarks and demonstrate novel interaction techniques that leverage SkinMarks’ unique touch, squeeze and bend sensing with integrated visual out-put. Finally, we detail on the conformality and evaluate sub-millimeter electrodes for touch sensing. Taken together, SkinMarks expands the on-body interaction space to more de-tailed, highly curved and challenging areas on the body.},
	journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI '17},
	author = {Weigel, Martin and Nittala, Aditya Shekhar and Olwal, Alex and Steimle, Jürgen},
	year = {2017},
	pages = {3095--3105},
	file = {Weigel et al_2017_SkinMarks.pdf:/Users/orsonxu/Zotero/storage/TYIBJPKW/Weigel et al_2017_SkinMarks.pdf:application/pdf;Weigel et al_2017_SkinMarks.pdf:/Users/orsonxu/Zotero/storage/LZ5XLT6B/Weigel et al_2017_SkinMarks.pdf:application/pdf},
}

@article{wang_eartouch:_2019,
	title = {{EarTouch}: {Turning} the {Ear} into an {Input} {Surface}},
	doi = {10.1145/3290605.3300254},
	abstract = {In this paper, we propose EarTouch, a new sensing technology for ear-based input for controlling applications by slightly pulling the ear and detecting the deformation by an enhanced earphone device. It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an " eyes-free " interface. As for the operation of EarTouch, the shape deformation of the ear is measured by optical sensors. Deformation of the skin caused by touching the ear with the fingers is recognized by attaching optical sensors to the earphone and measuring the distance from the earphone to the skin inside the ear. EarTouch supports recognition of multiple gestures by applying a support vector machine (SVM). EarTouch was validated through a set of user studies.},
	number = {Figure 1},
	author = {Wang, Ruolin and Yu, Chun and Yang, Xing-Dong and He, Weijie and Shi, Yuanchun},
	year = {2019},
	pages = {1--13},
	file = {Wang et al_2019_EarTouch.pdf:/Users/orsonxu/Zotero/storage/ZLL3V4HP/Wang et al_2019_EarTouch.pdf:application/pdf;Wang et al_2019_EarTouch.pdf:/Users/orsonxu/Zotero/storage/Y8QYET7U/Wang et al_2019_EarTouch.pdf:application/pdf},
}

@article{vega_beauty_2014,
	title = {Beauty tech nails: {Interactive} technology at your fingertips},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84897499924&partnerID=40&md5=bfd88fb94803e704550bf59306693a57},
	doi = {10.1145/2540930.2540961},
	abstract = {Looking for wearables that are fashionable, smart and augment human interaction, we introduce the term Beauty Technology as an emergent field in Wearable Computing. It is an on-body computing approach that turns non-invasive, wireless and without power required electromagnetic devices into beauty products for interacting with different surfaces and devices. This paper describes the materials and the prototyping process used in the making of Beauty Tech Nails exemplifying its application in everyday beauty products. Copyright 2014 ACM.},
	journal = {TEI 2014 - 8th International Conference on Tangible, Embedded and Embodied Interaction, Proceedings},
	author = {Vega, K and Fuks, H},
	year = {2014},
	keywords = {Wearable computers, Beauty products; Electromagnetic devices; Human i, Electromagnets; Nails; Radio frequency identificat},
	pages = {61--64},
	file = {Vega_Fuks_2014_Beauty tech nails.pdf:/Users/orsonxu/Zotero/storage/RCG7Z7BF/Vega_Fuks_2014_Beauty tech nails.pdf:application/pdf;Vega_Fuks_2014_Beauty tech nails.pdf:/Users/orsonxu/Zotero/storage/2Y2MZTB5/Vega_Fuks_2014_Beauty tech nails.pdf:application/pdf},
}

@article{matthies_botential:_2015,
	title = {Botential: {Localizing} {On}-{Body} {Gestures} by {Measuring} {Electrical} {Signatures} on the {Human} {Skin}},
	url = {http://dl.acm.org/citation.cfm?id=2785830.2785859},
	doi = {10.1145/2785830.2785859},
	abstract = {© 2015 ACM. We present Botential, an on-body interaction method for a wearable input device that can identify the location of onbody tapping gestures, using the entire human body as an interactive surface to expand the usually limited interaction space in the context of mobility. When the sensor is being touched, Botential identifies a body part's unique electric signature, which depends on its physiological and anatomical compositions. This input method exhibits a number of advantages over previous approaches, which include: 1) utilizing the existing signal the human body already emits, to accomplish input with various body parts, 2) the ability to also sense soft and long touches, 3) an increased sensing range that covers the whole body, and 4) the ability to detect taps and hovering through clothes.},
	journal = {Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services - MobileHCI '15},
	author = {Matthies, Denys J. C. and Perrault, Simon T. and Urban, Bodo and Zhao, Shengdong},
	year = {2015},
	keywords = {Capacitive Sensing, Embodied Interaction, EMG, Eyes-free, Hands-free, On-Body Interaction, User Interface},
	pages = {207--216},
	file = {Matthies et al_2015_Botential.pdf:/Users/orsonxu/Zotero/storage/IP7GEIA2/Matthies et al_2015_Botential.pdf:application/pdf;Matthies et al_2015_Botential.pdf:/Users/orsonxu/Zotero/storage/ET5AA5VE/Matthies et al_2015_Botential.pdf:application/pdf},
}

@article{zhang_fingerping:_2018,
	title = {{FingerPing}: {Recognizing} {Fine}-grained {Hand} {Poses} using {Active} {Acoustic} {On}-body {Sensing} {Cheng}},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3174011},
	doi = {10.1145/3173574.3174011},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI '18},
	author = {Zhang, Cheng and Starner, Thad and Inan, Omer and Abowd, Gregory D. and Xue, Qiuyue and Waghmare, Anandghan and Meng, Ruichen and Jain, Sumeet and Han, Yizeng and Li, Xinyu and Cunefare, Kenneth and Ploetz, Thomas},
	year = {2018},
	pages = {1--10},
	file = {Zhang et al_2018_FingerPing.pdf:/Users/orsonxu/Zotero/storage/QENXZUMD/Zhang et al_2018_FingerPing.pdf:application/pdf;Zhang et al_2018_FingerPing.pdf:/Users/orsonxu/Zotero/storage/EJJX2LDN/Zhang et al_2018_FingerPing.pdf:application/pdf},
}

@article{harrison_-body_2012,
	title = {On-body interaction: armed and dangerous},
	url = {http://dl.acm.org/citation.cfm?id=2148148%5Cnhttp://dl.acm.org/citation.cfm?id=2148148%5Cnpapers://c80d98e4-9a96-4487-8d06-8e1acc780d86/Paper/p12747},
	doi = {10.1145/2148131.2148148},
	abstract = {Recent technological advances in input sensing, as well as ultra-small projectors, have opened up new opportunities for interaction – the use of the body itself as both an input and output platform. Such on-body interfaces offer new interac- tive possibilities, and the promise of access to computation, communication and information literally in the palm of our hands. The unique context of on-body interaction allows us to take advantage of extra dimensions of input our bodies naturally afford us. In this paper, we consider how the arms and hands can be used to enhance on-body interactions, which is typically finger input centric. To explore this op- portunity, we developed Armura, a novel interactive on- body system, supporting both input and graphical output. Using this platform as a vehicle for exploration, we proto- typed many applications and interactions. This helped to confirm chief use modalities, identify fruitful interaction approaches, and in general, better understand how interfaces operate on the body. We highlight the most compelling techniques we uncovered. Further, this paper is the first to consider and prototype how conventional interaction issues, such as cursor control and clutching, apply to the on-body domain. Finally, we bring to light several new and unique interaction techniques.},
	journal = {In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction (TEI '12), Stephen N. Spencer (Ed.). ACM, New York, NY, USA},
	author = {Harrison, Chris and Ramamurthy, Shilpa and Hudson, Scott E.},
	year = {2012},
	keywords = {arumura, computer vision, depth camera, free-space gestures, projectors, sensing, sion-based input, vi-},
	pages = {69--76},
	file = {Harrison et al_2012_On-body interaction.pdf:/Users/orsonxu/Zotero/storage/Q46GAB9M/Harrison et al_2012_On-body interaction.pdf:application/pdf;Harrison et al_2012_On-body interaction.pdf:/Users/orsonxu/Zotero/storage/G4KUTYC6/Harrison et al_2012_On-body interaction.pdf:application/pdf},
}

@article{gubbiotti_touchcam:_2017,
	title = {{TouchCam}: {Realtime} {Recognition} of {Location}-{Specific} {On}-{Body} {Gestures} to {Support} {Users} with {Visual} {Impairments}},
	volume = {1},
	doi = {10.1145/1234},
	number = {4},
	journal = {IMWUT},
	author = {Gubbiotti, G. and Malagò, P. and Fin, S. and Tacchi, S. and Giovannini, L. and Bisero, D. and Madami, M. and Carlotti, G.},
	year = {2017},
	keywords = {gesture recognition, accessibility, acm reference format, blind and low-vision users, classi fi cation, computer vision applications, on-body input, skin texture, skin texture classification, wearable sensors},
	pages = {1--4},
	file = {Gubbiotti et al_2017_TouchCam.pdf:/Users/orsonxu/Zotero/storage/3RLQ38JM/Gubbiotti et al_2017_TouchCam.pdf:application/pdf;Gubbiotti et al_2017_TouchCam.pdf:/Users/orsonxu/Zotero/storage/2P6G28E7/Gubbiotti et al_2017_TouchCam.pdf:application/pdf},
}

@article{serrano_exploring_2014,
	title = {Exploring the use of hand-to-face input for interacting with head-worn displays},
	volume = {1},
	doi = {10.1145/2556288.2556984},
	abstract = {We propose the use of Hand-to-Face input, a method to interact with head-worn displays (HWDs) that involves contact with the face. We explore Hand-to-Face interaction to find suitable techniques for common mobile tasks. We evaluate this form of interaction with document navigation tasks and examine its social acceptability. In a first study, users identify the cheek and forehead as predominant areas for interaction and agree on gestures for tasks involving continuous input, such as document navigation. These results guide the design of several Hand-to-Face navigation techniques and reveal that gestures performed on the cheek are more efficient and less tiring than interactions directly on the HWD. Initial results on the social acceptability of Hand-to-Face input allow us to further refine our design choices, and reveal unforeseen results: Some gestures are considered culturally inappropriate and gender plays a role in selection of specific Hand-to-Face interactions. From our overall results, we provide a set of guidelines for developing effective Hand-to-Face interaction techniques.},
	number = {iv},
	author = {Serrano, Marcos and Ens, Barrett M. and Irani, Pourang P.},
	year = {2014},
	pages = {3181--3190},
	file = {Serrano et al_2014_Exploring the use of hand-to-face input for interacting with head-worn displays.pdf:/Users/orsonxu/Zotero/storage/TR7DXKFS/Serrano et al_2014_Exploring the use of hand-to-face input for interacting with head-worn displays.pdf:application/pdf;Serrano et al_2014_Exploring the use of hand-to-face input for interacting with head-worn displays.pdf:/Users/orsonxu/Zotero/storage/A5VKSKHG/Serrano et al_2014_Exploring the use of hand-to-face input for interacting with head-worn displays.pdf:application/pdf},
}

@article{kao_nailo:_2015,
	title = {{NailO}: {Fingernails} as an {Input} {Surface}},
	url = {http://dl.acm.org/citation.cfm?doid=2702123.2702572},
	doi = {10.1145/2702123.2702572},
	abstract = {We present NailO, a nail-mounted gestural input surface. Using capacitive sensing on printed electrodes, the interface can distinguish on-nail finger swipe gestures with high accuracy ({\textgreater}92\%). NailO works in real-time: we miniaturized the system to fit on the fingernail, while wirelessly transmitting the sensor data to a mobile phone or PC. NailO allows one-handed and always-available input, while being unobtrusive and discrete. Inspired by commercial nail stickers, the device blends into the user's body, is customizable, fashionable and even removable. We show example applications of using the device as a remote controller when hands are busy and using the system to increase the input space of mobile phones.},
	journal = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems - CHI '15},
	author = {Kao, Hsin-Liu (Cindy) and Dementyev, Artem and Paradiso, Joseph A. and Schmandt, Chris},
	year = {2015},
	pages = {3015--3018},
	file = {Kao et al_2015_NailO.pdf:/Users/orsonxu/Zotero/storage/593G6K4J/Kao et al_2015_NailO.pdf:application/pdf;Kao et al_2015_NailO.pdf:/Users/orsonxu/Zotero/storage/3XW7D8A9/Kao et al_2015_NailO.pdf:application/pdf},
}

@article{pratorius_skinteract_2017,
	title = {{SkInteract} : {An} {On}-body {Interaction} {System} {Based} on {Skin}-{Texture} {Recognition}},
	author = {Prätorius, Manuel and Scherzinger, Aaron and Hinrichs, Klaus and Prätorius, Manuel and Scherzinger, Aaron and Hinrichs, Klaus and An, Skinteract and System, On-body Interaction and Prätorius, Manuel and Scherzinger, Aaron and Hinrichs, Klaus},
	year = {2017},
	file = {Prätorius et al_2017_SkInteract.pdf:/Users/orsonxu/Zotero/storage/ZHYKUBJH/Prätorius et al_2017_SkInteract.pdf:application/pdf;Prätorius et al_2017_SkInteract.pdf:/Users/orsonxu/Zotero/storage/Z2NA33WC/Prätorius et al_2017_SkInteract.pdf:application/pdf},
}

@article{laput_ubicoustics:_2018,
	title = {Ubicoustics: {Plug}-and-{Play} {Acoustic} {Activity} {Recognition}},
	url = {https://doi.org/10.1145/3242587.3242609},
	doi = {10.1145/3242587.3242609},
	abstract = {Despite sound being a rich source of information, computing devices with microphones do not leverage audio to glean useful insights about their physical and social context. For example, a smart speaker sitting on a kitchen countertop cannot figure out if it is in a kitchen, let alone know what a user is doing in a kitchen-a missed opportunity. In this work, we describe a novel, real-time, sound-based activity recognition system. We start by taking an existing, state-of-the-art sound labeling model, which we then tune to classes of interest by drawing data from professional sound effect libraries traditionally used in the entertainment industry. These well-labeled and high-quality sounds are the perfect atomic unit for data augmentation, including amplitude, reverb, and mixing , allowing us to exponentially grow our tuning data in realistic ways. We quantify the performance of our approach across a range of environments and device categories and show that microphone-equipped computing devices already have the requisite capability to unlock real-time activity recognition comparable to human accuracy.},
	journal = {The 31st Annual ACM Symposium on User Interface Software and Technology - UIST '18},
	author = {Laput, Gierad and Ahuja, Karan and Goel, Mayank and Harrison, Chris and Ave, Forbes},
	year = {2018},
	keywords = {IoT, Smart Environments, Ubiquitous sensing},
	pages = {213--224},
	file = {Laput et al_2018_Ubicoustics.pdf:/Users/orsonxu/Zotero/storage/63D8ZNTH/Laput et al_2018_Ubicoustics.pdf:application/pdf;Laput et al_2018_Ubicoustics.pdf:/Users/orsonxu/Zotero/storage/772NV8XG/Laput et al_2018_Ubicoustics.pdf:application/pdf},
}

@article{schwarz_cord_2010,
	title = {Cord input: {An} {Intuitive}, {High}-{Accuracy}, {Multi}-{Degree}-of-{Freedom} {Input} {Method} for {Mobile} {Devices}},
	doi = {10.1145/1753326.1753573},
	author = {Schwarz, Julia and Harrison, Chris and Hudson, Scott and Mankoff, Jennifer},
	year = {2010},
	pages = {1657},
	file = {Schwarz et al_2010_Cord input.pdf:/Users/orsonxu/Zotero/storage/IU627WAJ/Schwarz et al_2010_Cord input.pdf:application/pdf;Schwarz et al_2010_Cord input.pdf:/Users/orsonxu/Zotero/storage/RHIT6AWR/Schwarz et al_2010_Cord input.pdf:application/pdf},
}

@article{metzger_freedigiter_2004,
	title = {{FreeDigiter} : {A} {Contact}-free {Device} for {Gesture} {Control} {Matt}},
	journal = {In Proceedings of the Eighth International Symposium on Wearable Computers (ISWC’04)},
	author = {Metzger, Christian and Anderson, Matt and Starner, Thad},
	year = {2004},
	file = {Metzger et al_2004_FreeDigiter.pdf:/Users/orsonxu/Zotero/storage/ZFIWHR3J/Metzger et al_2004_FreeDigiter.pdf:application/pdf;Metzger et al_2004_FreeDigiter.pdf:/Users/orsonxu/Zotero/storage/T2XD2FXE/Metzger et al_2004_FreeDigiter.pdf:application/pdf},
}

@article{yamashita_cheekinput:_2017,
	title = {{CheekInput}: {Turning} {Your} {Cheek} into an {Input} {Surface} by {Embedded} {Optical} {Sensors} on a {Head}-mounted {Display}},
	doi = {10.1145/3139131.3139146},
	abstract = {In this paper, we propose a novel technology called "CheekInput" with a head-mounted display (HMD) that senses touch gestures by detecting skin deformation. We attached multiple photo-reflective sensors onto the bottom front frame of the HMD. Since these sensors measure the distance between the frame and cheeks, our system is able to detect the deformation of a cheek when the skin surface is touched by fingers. Our system uses a Support Vector Machine to determine the gestures: pushing face up and down, left and right. We combined these 4 directional gestures for each cheek to extend 16 possible gestures. To evaluate the accuracy of the gesture detection, we conducted a user study. The results revealed that CheekInput achieved 80.45 \% recognition accuracy when gestures were made by touching both cheeks with both hands, and 74.58 \% when by touching both cheeks with one hand.},
	author = {Yamashita, Koki and Kikuchi, Takashi and Masai, Katsutoshi and Sugimoto, Maki and Thomas, Bruce H. and Sugiura, Yuta},
	year = {2017},
	keywords = {acm reference format, bruce, katsutoshi masai, koki yamashita, maki sugimoto, ost-hmd, photo-reflective sensor, skin interface, takashi kikuchi},
	pages = {1--8},
	file = {Yamashita et al_2017_CheekInput.pdf:/Users/orsonxu/Zotero/storage/8PP6IULF/Yamashita et al_2017_CheekInput.pdf:application/pdf;Yamashita et al_2017_CheekInput.pdf:/Users/orsonxu/Zotero/storage/LD2CDU5J/Yamashita et al_2017_CheekInput.pdf:application/pdf},
}

@inproceedings{amershi_guidelines_2019,
	address = {New York, New York, USA},
	title = {Guidelines for {Human}-{AI} {Interaction}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300233},
	doi = {10.1145/3290605.3300233},
	abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '19},
	publisher = {ACM Press},
	author = {Amershi, Saleema and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N.},
	year = {2019},
	keywords = {AI-infused systems, Design guidelines, Human-AI interaction},
	pages = {1--13},
	file = {Amershi et al. - 2019 - Guidelines for Human-AI Interaction.pdf:/Users/orsonxu/Zotero/storage/U3V4Q3BR/Amershi et al. - 2019 - Guidelines for Human-AI Interaction.pdf:application/pdf},
}

@article{rajanna_gaze_2016,
	title = {Gaze and foot input: {Toward} a rich and assistive interaction modality},
	doi = {10.1145/2876456.2876462},
	abstract = {Transforming gaze input into a rich and assistive interaction modality is one of the primary interests in eye tracking research. Gaze input in conjunction with traditional solutions to the "Midas Touch" problem, dwell time or a blink, is not matured enough to be widely adopted. In this regard, we present our preliminary work, a framework that achieves precise "point and click" interactions in a desktop environment through combining the gaze and foot interaction modalities. The framework comprises of an eye tracker and a foot-operated quasi-mouse that is wearable. The system evaluation shows that our gaze and foot interaction framework performs as good as a mouse (time and precision) in the majority of tasks. Furthermore, this dissertation work focuses on the goal of realizing gaze-assisted interaction as a primary interaction modality to substitute conventional mouse and keyboard-based interaction methods. In addition, we consider some of the challenges that need to be addressed, and also present the possible solutions toward achieving our goal.},
	journal = {International Conference on Intelligent User Interfaces, Proceedings IUI},
	author = {Rajanna, Vijay},
	year = {2016},
	keywords = {Authentication, Eye tracking, Foot input, Gaze and foot interaction, Tabletop interaction},
	pages = {126--129},
	file = {Rajanna_2016_Gaze and foot input.pdf:/Users/orsonxu/Zotero/storage/JRZ96XCX/Rajanna_2016_Gaze and foot input.pdf:application/pdf;Rajanna_2016_Gaze and foot input.pdf:/Users/orsonxu/Zotero/storage/U4AGXGJU/Rajanna_2016_Gaze and foot input.pdf:application/pdf},
}

@article{hajinejad_prototyping_2017,
	title = {Prototyping sonic interaction for walking},
	doi = {10.1145/3098279.3122141},
	abstract = {Sounds play a substantial role in the experience of movement activities such as walking. Drawing on the movement inducing effects of sound, sonic interaction opens up numerous possibilities to modify the walker's movements and experience. We argue that designing sonic interaction for movement activities demands an experiential awareness of the interplay of sound, body movement and use situation, and, propose a prototyping method to understand possibilities and challenges related to the design of mobile sonic interaction. In this paper, we present a rapid prototyping system that enables non-expert users to design sonic interaction for walking and to experience their design in the real-world context. We discuss the way this prototyping system allows designers to experience how their design ideas unfold in mobile use and affect the walking.},
	journal = {Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services},
	author = {Hajinejad, Nassrin and Grüter, Barbara and Roque, Licinio},
	year = {2017},
	keywords = {Mobile context, Rapid prototyping system, Sonic interaction design, Walking phrases},
	file = {Hajinejad et al_2017_Prototyping sonic interaction for walking.pdf:/Users/orsonxu/Zotero/storage/QZQNQR95/Hajinejad et al_2017_Prototyping sonic interaction for walking.pdf:application/pdf;Hajinejad et al_2017_Prototyping sonic interaction for walking.pdf:/Users/orsonxu/Zotero/storage/G8YRFZSE/Hajinejad et al_2017_Prototyping sonic interaction for walking.pdf:application/pdf},
}

@article{crossan_foot_2010,
	title = {Foot tapping for mobile interaction},
	doi = {10.14236/ewic/hci2010.49},
	abstract = {In this paper we present an initial investigation of foot tapping as a mechanism for interacting with a mobile device without removing it from a pocket. We compare a foot tapping technique for menu interaction with two more traditional situations: one where the user has the phone in hand, and one where the user must remove it from an inside pocket before interacting. Results show that over the course of the full study, all conditions allowed a high level of accuracy in selections. The visual and in pocket conditions were overall faster and more accurate. However, for short selections requiring four or less foot taps or button presses the foot tap condition was faster than the in pocket condition.},
	journal = {Proceedings of the British Computer Society Conference on Human-Computer Interaction},
	author = {Crossan, Andrew and Brewster, Stephen and Ng, Alexander},
	year = {2010},
	keywords = {Accelerometer, Mobile, Foot tap, Hands-free interaction},
	pages = {418--422},
	file = {Crossan et al_2010_Foot tapping for mobile interaction.pdf:/Users/orsonxu/Zotero/storage/PCX7IFWI/Crossan et al_2010_Foot tapping for mobile interaction.pdf:application/pdf;Crossan et al_2010_Foot tapping for mobile interaction.pdf:/Users/orsonxu/Zotero/storage/UINBSE7J/Crossan et al_2010_Foot tapping for mobile interaction.pdf:application/pdf},
}

@article{jylha_rhythmic_2012,
	title = {Rhythmic walking interactions with auditory feedback: {An} exploratory study},
	doi = {10.1145/2371456.2371467},
	abstract = {Walking is a natural rhythmic activity that has become of interest as a means of interacting with software systems such as computer games. Therefore, designing multimodal walking interactions calls for further examination. This exploratory study presents a system capable of different kinds of interactions based on varying the temporal characteristics of the output, using the sound of human walking as the input. The system either provides a direct synthesis of a walking sound based on the detected amplitude envelope of the user's footstep sounds, or provides a continuous synthetic walking sound as a stimulus for the walking human, either with a fixed tempo or a tempo adapting to the human gait. In a pilot experiment, the different interaction modes are studied with respect to their effect on the walking tempo and the experience of the subjects. The results tentatively outline different user profiles in interacting with such a system. Copyright 2012 ACM.},
	journal = {ACM International Conference Proceeding Series},
	author = {Jylhä, Antti and Serafin, Stefania and Erkut, Cumhur},
	year = {2012},
	keywords = {Sonic interaction design, Agency, Audio input, Footsteps, Rhythmic interaction},
	pages = {68--75},
	file = {Jylhä et al_2012_Rhythmic walking interactions with auditory feedback.pdf:/Users/orsonxu/Zotero/storage/3FT9ZEGT/Jylhä et al_2012_Rhythmic walking interactions with auditory feedback.pdf:application/pdf;Jylhä et al_2012_Rhythmic walking interactions with auditory feedback.pdf:/Users/orsonxu/Zotero/storage/Y9462C92/Jylhä et al_2012_Rhythmic walking interactions with auditory feedback.pdf:application/pdf},
}

@article{lucero_notifeye_2014,
	title = {{NotifEye}: {Using} interactive glasses to deal with notifications while walking in public},
	volume = {2014-Novem},
	doi = {10.1145/2663806.2663824},
	abstract = {In this paper we explore the use of interactive eyewear in public. We introduce NotifEye, an application that allows a person to receive social network notifications on interactive glasses while walking on a busy street. The prototype uses a minimalistic user interface (UI) for interactive glasses to help people focus their attention on their surroundings and supports discreet interaction by using a finger rub pad to take action on incoming notifications. We studied pragmatic and hedonic aspects of the prototype during a pedestrian navigation task in a city center. We found that, despite the potential risk of overwhelming people with information, participants were able to keep track of their surroundings as they dealt with incoming notifications. Participants also positively valued the use of a discreet device to provide input for interactive glasses. Finally, participants reflected on their (evolving) perception of interactive glasses, indicating that glasses should become smaller, more comfortable to wear, and somewhat of a fashion accessory.},
	journal = {Proceedings of the Conference on Advances in Computer Entertainment Technology},
	author = {Lucero, Andrés and Vetek, Akos},
	year = {2014},
	keywords = {Wearable computing, Discreet interaction, Head-mounted displays, In the wild studies, Interactive eyewear},
	file = {Lucero_Vetek_2014_NotifEye.pdf:/Users/orsonxu/Zotero/storage/A4A3R2RE/Lucero_Vetek_2014_NotifEye.pdf:application/pdf;Lucero_Vetek_2014_NotifEye.pdf:/Users/orsonxu/Zotero/storage/J8ALCQKN/Lucero_Vetek_2014_NotifEye.pdf:application/pdf},
}

@article{hincapie-ramos_crashalert_2013,
	title = {{CrashAlert}: {Enhancing} peripheral alertness for eyes-busy mobile interaction while walking},
	doi = {10.1145/2470654.2466463},
	abstract = {Mobile device use while walking, or eyes-busy mobile interaction, is a leading cause of life-threatening pedestrian collisions. We introduce CrashAlert, a system that augments mobile devices with a depth camera, to provide distance and location visual cues of obstacles on the user's path. In a realistic environment outside the lab, CrashAlert users improve their handling of potential collisions, dodging and slowing down for simple ones while lifting their head in more complex situations. Qualitative results outline the value of extending users' peripheral alertness in eyes-busy mobile interaction through non-intrusive depth cues, as used in CrashAlert. We present the design features of our system and lessons learned from our evaluation. Copyright © 2013 ACM.},
	journal = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	author = {Hincapié-Ramos, Juan David and Irani, Pourang},
	year = {2013},
	keywords = {Eyes-busy interaction, Obstacle avoidance, Texting and walking, Walking user interfaces},
	pages = {3385--3388},
	file = {Hincapié-Ramos_Irani_2013_CrashAlert.pdf:/Users/orsonxu/Zotero/storage/BJIV7DQC/Hincapié-Ramos_Irani_2013_CrashAlert.pdf:application/pdf;Hincapié-Ramos_Irani_2013_CrashAlert.pdf:/Users/orsonxu/Zotero/storage/KZJ8MAGN/Hincapié-Ramos_Irani_2013_CrashAlert.pdf:application/pdf},
}

@article{feng_investigating_2015,
	title = {Investigating pressure-based interactions with mobile phones while walking and encumbered},
	doi = {10.1145/2786567.2793711},
	abstract = {In encumbered (e.g. carrying shopping bags) and walking situations, interacting with mobile phones is physically demanding and leads to poor input performance. This paper presents two user studies which investigate the effectiveness of using pressure as an alternative input modality to touch when using mobile phones while walking and encumbered. Forcesensing resistors (FSR) were placed around the edges of a mobile phone to provide multiple pressure points to execute onscreen spreading, pinching, rotating and dragging single handedly. Experimental results showed that it is possible that encumbrance had no significant effect on pressure-based targeting performance. Our preliminary findings show promise with using multidigit pressure input to facilitate one-handed touchless interactions with handheld devices in multitasking encumbered contexts.},
	journal = {Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
	author = {Feng, Shimin and Wilson, Graham and Ng, Alex and Brewster, Stephen},
	year = {2015},
	keywords = {Encumbrance, Fitts' Law, Mobile interaction, Pressure-based input, Walking},
	pages = {854--861},
	file = {Feng et al_2015_Investigating pressure-based interactions with mobile phones while walking and encumbered.pdf:/Users/orsonxu/Zotero/storage/6B3IFLBM/Feng et al_2015_Investigating pressure-based interactions with mobile phones while walking and encumbered.pdf:application/pdf;Feng et al_2015_Investigating pressure-based interactions with mobile phones while walking and encumbered.pdf:/Users/orsonxu/Zotero/storage/SMW6B4GQ/Feng et al_2015_Investigating pressure-based interactions with mobile phones while walking and encumbered.pdf:application/pdf},
}

@article{schildbach_investigating_2010,
	title = {Investigating selection and reading performance on a mobile phone while walking},
	doi = {10.1145/1851600.1851619},
	abstract = {More and more people interact with their mobile phone while walking. The presented research analyzes; firstly, the negative effect of walking when considering reading and target selection tasks, such as weaker performance and higher workload. Here, we focused on one-handed interaction with a touch screen whereby the thumb is used as the input device. Secondly, we analyze how these negative effects can be compensated by increasing the text size and the size of the targets to select on the mobile phone. A comparative user study was conducted with 16 participants who performed target acquisition and reading tasks while standing and walking. The results show that whilst performance decreases, cognitive load increases significantly when reading and selecting targets when walking. Furthermore, the results show that the negative effect regarding target selection can be compensated by increasing the target size, but the text reading task did not yield better performance results for a larger text size due to the increased demand for scrolling. These results can be used to inform future designs of mobile user interfaces which might provide a dedicated walking mode. © 2010 ACM.},
	journal = {Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
	author = {Schildbach, Bastian and Rukzio, Enrico},
	year = {2010},
	keywords = {mobile interaction, walking, reading, target selection},
	pages = {93--102},
	file = {Schildbach_Rukzio_2010_Investigating selection and reading performance on a mobile phone while walking.pdf:/Users/orsonxu/Zotero/storage/PNI8WHBD/Schildbach_Rukzio_2010_Investigating selection and reading performance on a mobile phone while walking.pdf:application/pdf;Schildbach_Rukzio_2010_Investigating selection and reading performance on a mobile phone while walking.pdf:/Users/orsonxu/Zotero/storage/X25R4P4R/Schildbach_Rukzio_2010_Investigating selection and reading performance on a mobile phone while walking.pdf:application/pdf},
}

@article{dancu_designing_2015,
	title = {Designing seamless displays for interaction in motion},
	doi = {10.1145/2786567.2794337},
	abstract = {Most mobile interfaces today are designed with a "stop to interact" paradigm, which means that when a user has a device notification, they are expected to stop and attend to a screen in their pocket or on their wrist. An alternative is to design wearable displays to minimize attention required while in motion. We review research on attention and perception theory, recent display technology and applications of interaction in motion. Based on display layout principles and guidelines from literature, we created prototypes and experiments with mobile projected and mid-air displays for the task of map navigation while walking and cycling.},
	journal = {Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
	author = {Dancu, Alexandru and Marshall, Joe},
	year = {2015},
	keywords = {Attention, Seamless, Wearable displays},
	pages = {1076--1081},
	file = {Dancu_Marshall_2015_Designing seamless displays for interaction in motion.pdf:/Users/orsonxu/Zotero/storage/IWU35VPU/Dancu_Marshall_2015_Designing seamless displays for interaction in motion.pdf:application/pdf;Dancu_Marshall_2015_Designing seamless displays for interaction in motion.pdf:/Users/orsonxu/Zotero/storage/ZEKGEBSR/Dancu_Marshall_2015_Designing seamless displays for interaction in motion.pdf:application/pdf},
}

@article{hatscher_gazetap_2019,
	title = {{GazeTap} : {Towards} {Hands}-{Free} {Interaction} in the {Operating} {Room}},
	journal = {Proceedings of the ACM International Conference on Multimodal Interaction},
	author = {Hatscher, Benjamin and Luz, Maria and Elkmann, Norbert and Hansen, Christian},
	year = {2019},
	keywords = {eye tracking, foot input, gaze input, gaze-foot interaction, hci in the operating, input techniques, multimodal interaction, room},
	pages = {243--251},
	file = {Hatscher et al_2019_GazeTap.pdf:/Users/orsonxu/Zotero/storage/3PCGJWTS/Hatscher et al_2019_GazeTap.pdf:application/pdf;Hatscher et al_2019_GazeTap.pdf:/Users/orsonxu/Zotero/storage/2JJGAKRB/Hatscher et al_2019_GazeTap.pdf:application/pdf},
}

@article{muller_mind_2019,
	title = {Mind the tap: {Assessing} foot-taps for interacting with head-mounted displays},
	doi = {10.1145/3290605.3300707},
	abstract = {From voice commands and air taps to touch gestures on frames: Various techniques for interacting with head-mounted displays (HMDs) have been proposed. While these techniques have both benefits and drawbacks dependent on the current situation of the user, research on interacting with HMDs has not concluded yet. In this paper, we add to the body of research on interacting with HMDs by exploring foot-tapping as an input modality. Through two controlled experiments with a total of 36 participants, we first explore direct interaction with interfaces that are displayed on the floor and require the user to look down to interact. Secondly, we investigate indirect interaction with interfaces that, although operated by the user’s feet, are always visible as they are floating in front of the user. Based on the results of the two experiments, we provide design recommendations for direct and indirect foot-based user interfaces.},
	journal = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	author = {Müller, Florian and McManus, Joshua and Günther, Sebastian and Schmitz, Martin and Mühlhäuser, Max and Funk, Markus},
	year = {2019},
	keywords = {Foot interaction, HMD, Human factors, User study},
	pages = {1--13},
	file = {Müller et al_2019_Mind the tap.pdf:/Users/orsonxu/Zotero/storage/BQVGX3M4/Müller et al_2019_Mind the tap.pdf:application/pdf;Müller et al_2019_Mind the tap.pdf:/Users/orsonxu/Zotero/storage/S47A8SQD/Müller et al_2019_Mind the tap.pdf:application/pdf},
}

@article{lanir_step_2016,
	title = {Step by step: {Investigating} foot gesture interaction},
	volume = {07-10-June},
	doi = {10.1145/2909132.2926057},
	abstract = {A promising new way of interacting with computing devices is by using our feet. Foot interaction has the potential of being an intuitive, easy to use and enjoyable way of interaction. However, there are very few guidelines for using foot interaction, or specifically foot gestures. In this research we conduct a user elicitation study for foot interaction on a horizontal surface to produce user-defined gesture sets for actions taken from two domains - typical GUI actions and avatar controls. We analyze how foot gestures differentiate from hand gestures, point out foot gesture properties and discuss general observations.},
	journal = {Proceedings of the Workshop on Advanced Visual Interfaces AVI},
	author = {Lanir, Joel and Felberbaum, Yasmin},
	year = {2016},
	keywords = {Foot interaction, Gestures, Guessability study, User-defined gesture set},
	pages = {306--307},
	file = {Lanir_Felberbaum_2016_Step by step.pdf:/Users/orsonxu/Zotero/storage/FVAE7JER/Lanir_Felberbaum_2016_Step by step.pdf:application/pdf;Lanir_Felberbaum_2016_Step by step.pdf:/Users/orsonxu/Zotero/storage/RYAXVLJY/Lanir_Felberbaum_2016_Step by step.pdf:application/pdf},
}

@article{fukahori_exploring_2016,
	title = {Exploring subtle foot plantar-based gestures using sock-style pressure sensors},
	volume = {33},
	issn = {02896540},
	abstract = {We propose subtle foot-based gestures named foot plantar-based (FPB) gestures that are used with sockstyle pressure sensors. In this system, the user can control a computing device by changing his or her foot plantar distributions, e.g., pressing the floor with his or her toe. Because such foot movement is subtle, it is suitable for use especially in a public space such as a crowded train. In this work, we focus on a user-defined gesture that is designed by the end-users, not developers of this system. We first conduct a guessability study that asks people what is the appropriate gesture for a specific command to control the computing device. Then, we implement a gesture recognizer with a machine learning technique. To avoid unexpected gesture activations, we also collect foot plantar pressure patterns made during daily activities such as walking, as negative training data. Finally, we conclude with several applications to further illustrate the utility of FPB gestures.},
	number = {2},
	journal = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	author = {Fukahori, Koumei and Sakamoto, Daisuke and Igarashi, Takeo},
	year = {2016},
	pages = {116--124},
	file = {Fukahori et al_2016_Exploring subtle foot plantar-based gestures using sock-style pressure sensors.pdf:/Users/orsonxu/Zotero/storage/2K3IDIDS/Fukahori et al_2016_Exploring subtle foot plantar-based gestures using sock-style pressure sensors.pdf:application/pdf;Fukahori et al_2016_Exploring subtle foot plantar-based gestures using sock-style pressure sensors.pdf:/Users/orsonxu/Zotero/storage/XRWHWBGU/Fukahori et al_2016_Exploring subtle foot plantar-based gestures using sock-style pressure sensors.pdf:application/pdf},
}

@article{nicolau_touch_2012,
	title = {Touch typing using thumbs: {Understanding} the effect of mobility and hand posture},
	doi = {10.1145/2207676.2208661},
	abstract = {Mobile touch devices have become increasingly popular, yet typing on virtual keyboards whilst walking is still an overwhelming task. In this paper we analyze; firstly, the negative effect of walking on text-input performance, particularly the users' main difficulties and error patterns. We focused our research on thumb typing, since this is a commonly used technique to interact with touch interfaces. Secondly, we analyze how these effects can be compensated by two-hand interaction and increasing target size. We asked 22 participants to input text under three mobility conditions (seated, slow walking, and normal walking) and three hand conditions (one-hand/portrait, two-hand/portrait, and two-hand/landscape). Results show that independently of hand condition, mobility significantly decreased input quality, leading to specific error patterns. Moreover, it was shown that target size can compensate the negative effect of walking, while two-hand interaction does not provide additional stability or input accuracy. We finish with implications for future designs. Copyright 2012 ACM.},
	journal = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	author = {Nicolau, Hugo and Jorge, Joaquim},
	year = {2012},
	keywords = {Hand posture, Mobile, Text-entry, Thumb, Walk},
	pages = {2683--2686},
	file = {Nicolau_Jorge_2012_Touch typing using thumbs.pdf:/Users/orsonxu/Zotero/storage/AEYKZ7EA/Nicolau_Jorge_2012_Touch typing using thumbs.pdf:application/pdf;Nicolau_Jorge_2012_Touch typing using thumbs.pdf:/Users/orsonxu/Zotero/storage/KAW67754/Nicolau_Jorge_2012_Touch typing using thumbs.pdf:application/pdf},
}

@article{ruddle_walking_2011,
	title = {Walking improves your cognitive map in environments that are large-scale and large in extent},
	volume = {18},
	issn = {10730516},
	doi = {10.1145/1970378.1970384},
	abstract = {This study investigated the effect of body-based information (proprioception, etc.) when participants navigated large-scale virtual marketplaces that were either small (Experiment 1) or large in extent (Experiment 2). Extent refers to the size of an environment, whereas scale refers to whether people have to travel through an environment to see the detail necessary for navigation. Each participant was provided with full body-based information (walking through the virtual marketplaces in a large tracking hall or on an omnidirectional treadmill), just the translational component of body-based information (walking on a linear treadmill, but turning with a joystick), just the rotational component (physically turning but using a joystick to translate) or no body-based information (joysticks to translate and rotate). In large and small environments translational body-based information significantly improved the accuracy of participants' cognitive maps, measured using estimates of direction and relative straight line distance but, on its own, rotational body-based information had no effect. In environments of small extent, full body-based information also improved participants' navigational performance. The experiments show that locomotion devices such as linear treadmills would bring substantial benefits to virtual environment applications where large spaces are navigated, and theories of human navigation need to reconsider the contribution made by body-based information, and distinguish between environmental scale and extent. © 2011 ACM 1073-0516/2011/06-ART10 \$10.00.},
	number = {2},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Ruddle, Roy A. and Volkova, Ekaterina and BüLthoff, Heinrich H.},
	year = {2011},
	keywords = {Virtual reality, Cognitive map, Locomotion, Navigation},
	pages = {1--20},
	file = {Ruddle et al_2011_Walking improves your cognitive map in environments that are large-scale and large in extent.pdf:/Users/orsonxu/Zotero/storage/8DLA6LTS/Ruddle et al_2011_Walking improves your cognitive map in environments that are large-scale and large in extent.pdf:application/pdf;Ruddle et al_2011_Walking improves your cognitive map in environments that are large-scale and large in extent.pdf:/Users/orsonxu/Zotero/storage/LWUDE6Y3/Ruddle et al_2011_Walking improves your cognitive map in environments that are large-scale and large in extent.pdf:application/pdf},
}

@article{scott_sensing_2010,
	title = {Sensing foot gestures from the pocket},
	doi = {10.1145/1866029.1866063},
	abstract = {Visually demanding interfaces on a mobile phone can diminish the user experience by monopolizing the user's attention when they are focusing on another task and impede accessibility for visually impaired users. Because mobile devices are often located in pockets when users are mobile, explicit foot movements can be defined as eyes-and-hands-free input gestures for interacting with the device. In this work, we study the human capability associated with performing foot-based interactions which involve lifting and rotation of the foot when pivoting on the toe and heel. Building upon these results, we then developed a system to learn and recognize foot gestures using a single commodity mobile phone placed in the user's pocket or in a holster on their hip. Our system uses acceleration data recorded by a built-in accelerometer on the mobile device and a machine learning approach to recognizing gestures. Through a lab study, we demonstrate that our system can classify ten different foot gestures at approximately 86\% accuracy.},
	journal = {Proceeding of the ACM Symposium on User Interface Software and Technology},
	author = {Scott, Jeremy and Dearman, David and Yatani, Koji and Truong, Khai N.},
	year = {2010},
	keywords = {Mobile devices, Hands-free interaction, Eyes-free interaction, Foot-based gestures},
	pages = {199--208},
	file = {Scott et al_2010_Sensing foot gestures from the pocket.pdf:/Users/orsonxu/Zotero/storage/LA9G4IB9/Scott et al_2010_Sensing foot gestures from the pocket.pdf:application/pdf;Scott et al_2010_Sensing foot gestures from the pocket.pdf:/Users/orsonxu/Zotero/storage/MBJHGVL9/Scott et al_2010_Sensing foot gestures from the pocket.pdf:application/pdf},
}

@article{hudson_whack_2010,
	title = {Whack gestures: {Inexact} and inattentive interaction with mobile devices},
	doi = {10.1145/1709886.1709906},
	abstract = {We introduce Whack Gestures, an inexact and inattentive interaction technique. This approach seeks to provide a simple means to interact with devices with minimal attention from the user - in particular, without the use of fine motor skills or detailed visual attention (requirements found in nearly all conventional interaction techniques). For mobile devices, this could enable interaction without "getting it out," grasping, or even glancing at the device. This class of techniques is suitable for a small number of simple but common interactions that could be carried out in an extremely lightweight fashion without disrupting other activities. With Whack Gestures, users can interact by striking a device with the open palm or heel of the hand. We briefly discuss the development and use of a preliminary version of this technique and show that implementations with high accuracy and a low false positive rate are feasible. Copyright 2010 ACM.},
	journal = {TEI'10 - Proceedings of the 4th International Conference on Tangible, Embedded, and Embodied Interaction},
	author = {Hudson, Scott E. and Harrison, Chris and Harrison, Beverly L. and LaMarca, Anthony},
	year = {2010},
	keywords = {Adaptive user interfaces, Gesture-based interfaces, Lightweight interaction, Physical interaction, Sensing},
	pages = {109--112},
	file = {Hudson et al_2010_Whack gestures.pdf:/Users/orsonxu/Zotero/storage/TMIGIRMC/Hudson et al_2010_Whack gestures.pdf:application/pdf;Hudson et al_2010_Whack gestures.pdf:/Users/orsonxu/Zotero/storage/PYYCISMA/Hudson et al_2010_Whack gestures.pdf:application/pdf},
}

@article{beckhaus_chairio_2007,
	title = {{ChairIO} – {The} {Chair}-{Based} {Interface}},
	journal = {Concepts and technologies for pervasive games},
	author = {Beckhaus, Steffi and Blom, Kristopher J and Haringer, Matthias},
	year = {2007},
	file = {Beckhaus et al_2007_ChairIO – The Chair-Based Interface.pdf:/Users/orsonxu/Zotero/storage/CARVHJRD/Beckhaus et al_2007_ChairIO – The Chair-Based Interface.pdf:application/pdf;Beckhaus et al_2007_ChairIO – The Chair-Based Interface.pdf:/Users/orsonxu/Zotero/storage/IQA3BC3R/Beckhaus et al_2007_ChairIO – The Chair-Based Interface.pdf:application/pdf},
}

@article{lee_gait_2002,
	title = {Gait analysis for recognition and classification},
	doi = {10.1109/AFGR.2002.1004148},
	abstract = {This paper describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple features such as moments extracted from orthogonal view video silhouettes of human walking motion. Despite its simplicity, the resulting feature vector contains enough information to perform well on human identification and gender classification tasks. We explore the recognition behaviors of two different methods to aggregate features over time under different recognition tasks. We demonstrate the accuracy of recognition using gait video sequences collected over different days and times and under varying lighting environments. In addition, we show results for gender classification based our gait appearance features using a support-vector machine. © 2002 IEEE.},
	number = {Mld},
	journal = {Proceedings - 5th IEEE International Conference on Automatic Face Gesture Recognition, FGR 2002},
	author = {Lee, L. and Grimson, W. E.L.},
	year = {2002},
	pages = {155--162},
	file = {Lee_Grimson_2002_Gait analysis for recognition and classification.pdf:/Users/orsonxu/Zotero/storage/5JQCSNU8/Lee_Grimson_2002_Gait analysis for recognition and classification.pdf:application/pdf;Lee_Grimson_2002_Gait analysis for recognition and classification.pdf:/Users/orsonxu/Zotero/storage/MV35CZSR/Lee_Grimson_2002_Gait analysis for recognition and classification.pdf:application/pdf},
}

@article{dellermann_future_2019,
	title = {The {Future} of {Human}-{AI} {Collaboration}: {A} {Taxonomy} of {Design} {Knowledge} for {Hybrid} {Intelligence} {Systems}},
	volume = {6},
	doi = {10.24251/hicss.2019.034},
	abstract = {Recent technological advances, especially in the field of machine learning, provide astonishing progress on the road towards artificial general intelligence. However, tasks in current real-world business applications cannot yet be solved by machines alone. We, therefore, identify the need for developing socio-technological ensembles of humans and machines. Such systems possess the ability to accomplish complex goals by combining human and artificial intelligence to collectively achieve superior results and continuously improve by learning from each other. Thus, the need for structured design knowledge for those systems arises. Following a taxonomy development method, this article provides three main contributions: First, we present a structured overview of interdisciplinary research on the role of humans in the machine learning pipeline. Second, we envision hybrid intelligence systems and conceptualize the relevant dimensions for system design for the first time. Finally, we offer useful guidance for system developers during the implementation of such applications.},
	journal = {Proceedings of the 52nd Hawaii International Conference on System Sciences},
	author = {Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and Weber, Thorsten and Weigel, Sascha and Ebel, Philipp},
	year = {2019},
	pages = {274--283},
	file = {Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:/Users/orsonxu/Zotero/storage/A3AFVWX2/Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:application/pdf;Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:/Users/orsonxu/Zotero/storage/I7HF4TNN/Dellermann et al_2019_The Future of Human-AI Collaboration.pdf:application/pdf},
}

@inproceedings{kane_usable_2011,
	address = {New York, New York, USA},
	title = {Usable gestures for blind people: {Understanding} {Preference} and {Performance}},
	isbn = {978-1-4503-0228-9},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979001},
	doi = {10.1145/1978942.1979001},
	abstract = {Despite growing awareness of the accessibility issues surrounding touch screen use by blind people, designers still face challenges when creating accessible touch screen interfaces. One major stumbling block is a lack of understanding about how blind people actually use touch screens. We conducted two user studies that compared how blind people and sighted people use touch screen gestures. First, we conducted a gesture elicitation study in which 10 blind and 10 sighted people invented gestures to perform common computing tasks on a tablet PC. We found that blind people have different gesture preferences than sighted people, including preferences for edge-based gestures and gestures that involve tapping virtual keys on a keyboard. Second, we conducted a performance study in which the same participants performed a set of reference gestures. We found significant differences in the speed, size, and shape of gestures performed by blind people versus those performed by sighted people. Our results suggest new design guidelines for accessible touch screen interfaces. Copyright 2011 ACM.},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {Kane, Shaun K. and Wobbrock, Jacob O. and Ladner, Richard E.},
	year = {2011},
	keywords = {Gesture recognition, Gestures, Accessibility, Blind, Touch screens},
	pages = {413},
	file = {Kane et al_2011_Usable gestures for blind people.pdf:/Users/orsonxu/Zotero/storage/5UQ6PPUF/Kane et al_2011_Usable gestures for blind people.pdf:application/pdf;Kane et al_2011_Usable gestures for blind people.pdf:/Users/orsonxu/Zotero/storage/533B6DRJ/Kane et al_2011_Usable gestures for blind people.pdf:application/pdf},
}

@article{plimmer_signing_2011,
	title = {Signing on the tactile line: {A} {Multimodal} {System} for {Teaching} {Handwriting} to {Blind} {Children}},
	volume = {18},
	issn = {1073-0516},
	url = {https://dl.acm.org/doi/10.1145/1993060.1993067},
	doi = {10.1145/1993060.1993067},
	abstract = {We present McSig, a multimodal system for teaching blind children cursive handwriting so that they can create a personal signature. For blind people handwriting is very difficult to learn as it is a near-zero feedback activity that is needed only occasionally, yet in important situations; for example, to make an attractive and repeatable signature for legal contracts. McSig aids the teaching of signatures by translating digital ink from the teacher's stylus gestures into three non-visual forms: (1) audio pan and pitch represents the x and y movement of the stylus; (2) kinaesthetic information is provided to the student through a force-feedback haptic pen that mimics the teacher's stylus movement; and (3) a physical tactile line on the writing sheet is created by the haptic pen. McSig has been developed over two major iterations of design, usability testing and evaluation. The final step of the first iteration was a short evaluation with eight visually impaired children. The results suggested that McSig had the highest potential benefit for congenitally and totally blind children and also indicated some areas where McSig could be enhanced. The second prototype incorporated significant modifications to the system, improving the audio, tactile and force-feedback. We then ran a detailed, longitudinal evaluation over 14 weeks with three of the congenitally blind children to assess McSig's effectiveness in teaching the creation of signatures. Results demonstrated the effectiveness ofMcSig-they all made considerable progress in learning to create a recognizable signature. By the end of ten lessons, two of the children could form a complete, repeatable signature unaided, the third could do so with a little verbal prompting. Furthermore, during this project, we have learnt valuable lessons about providing consistent feedback between different communications channels (by manual interactions, haptic device, pen correction) that will be of interest to others developing multimodal systems. Copyright © 2011, ACM. All rights reserved.},
	number = {3},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Plimmer, Beryl and Reid, Peter and Blagojevic, Rachel and Crossan, Andrew and Brewster, Stephen},
	month = jul,
	year = {2011},
	keywords = {audio and haptic feedback, children, handwriting, Multimodal interaction, visually impaired users, Human Factors},
	pages = {1--29},
	file = {Plimmer et al_2011_Signing on the tactile line.pdf:/Users/orsonxu/Zotero/storage/R5F9NYS5/Plimmer et al_2011_Signing on the tactile line.pdf:application/pdf;Plimmer et al_2011_Signing on the tactile line.pdf:/Users/orsonxu/Zotero/storage/SA4JRBBW/Plimmer et al_2011_Signing on the tactile line.pdf:application/pdf},
}

@incollection{mattheiss_dots_2014,
	title = {Dots and {Letters}: {Accessible} {Braille}-{Based} {Text} {Input} for {Visually} {Impaired} {People} on {Mobile} {Touchscreen} {Devices}},
	volume = {8547 LNCS},
	isbn = {978-3-319-08595-1},
	url = {http://link.springer.com/10.1007/978-3-319-08596-8_100},
	abstract = {Tailored text input methods for visually impaired and blind users are needed on touchscreen devices to support their accessibility. Therefore, we de-veloped a new Braille-based text input method named EdgeBraille, which allows entering Braille characters by swiping one finger along the edges of the touchscreen. The approach was compared with the current standard method of a talking keyboard, first in a short-term lab study (14 participants) and then during two weeks of daily training (7 participants). Overall EdgeBraille was perceived well by the users. In terms of user performance we found no significant differences between the two methods. Based on the evaluation results and the feedback of our participants, we discuss advantages and disadvantages of Braille-based methods in general and EdgeBraille in particular, as well as possibilities for improvements. © 2014 Springer International Publishing.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {Mattheiss, Elke and Regal, Georg and Schrammel, Johann and Garschall, Markus and Tscheligi, Manfred},
	year = {2014},
	keywords = {Braille, Mobile Devices, Text Input Method, Touchscreen, Visually Impaired and Blind Users},
	pages = {650--657},
	file = {Mattheiss et al_2014_Dots and Letters.pdf:/Users/orsonxu/Zotero/storage/V5GCNW2G/Mattheiss et al_2014_Dots and Letters.pdf:application/pdf;Mattheiss et al_2014_Dots and Letters.pdf:/Users/orsonxu/Zotero/storage/L5CFBS75/Mattheiss et al_2014_Dots and Letters.pdf:application/pdf},
}

@article{plimmer_multimodal_2008,
	title = {Multimodal collaborative handwriting training for visually-impaired people},
	doi = {10.1145/1357054.1357119},
	abstract = {"McSig" is a multimodal teaching and learning environment for visually-impaired students to learn character shapes, handwriting and signatures collaboratively with their teachers. It combines haptic and audio output to realize the teacher's pen input in parallel non-visual modalities. McSig is intended for teaching visually-impaired children how to handwrite characters (and from that signatures), something that is very difficult without visual feedback. We conducted an evaluation with eight visually-impaired children with a pre-test to assess their current skills with a set of character shapes, a training phase using McSig and then a post-test of the same character shapes to see if there were any improvements. The children could all use McSig and we saw significant improvements in the character shapes drawn, particularly by the completely blind children (many of whom could draw almost none of the characters before the test). In particular, the blind participants all expressed enjoyment and excitement about the system and using a computer to learn to handwrite. Copyright 2008 ACM.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Plimmer, Beryl and Crossan, Andrew and Brewster, Stephen A. and Blagojevic, Rachel},
	year = {2008},
	keywords = {Haptic trajectory playback, Multimodal interface design, Signature training, Visually-impaired users},
	pages = {393--402},
	file = {Plimmer et al_2008_Multimodal collaborative handwriting training for visually-impaired people.pdf:/Users/orsonxu/Zotero/storage/Z3T6LYA6/Plimmer et al_2008_Multimodal collaborative handwriting training for visually-impaired people.pdf:application/pdf;Plimmer et al_2008_Multimodal collaborative handwriting training for visually-impaired people.pdf:/Users/orsonxu/Zotero/storage/2X8A2SA2/Plimmer et al_2008_Multimodal collaborative handwriting training for visually-impaired people.pdf:application/pdf},
}

@article{mayer_user_2014,
	title = {User {Interfaces} for {Smart} {Things}-{A} {Generative} {Approach} with {Semantic} {Interaction} {Descriptions} model-based user interfaces, user interface descriptions, {Web} of {Things} {ACM} {Reference} {Format}: interfaces for smart things-{A} generative approach with semantic int},
	volume = {21},
	url = {http://dx.doi.org/10.1145/2584670},
	doi = {10.1145/2584670},
	abstract = {With ever more everyday objects becoming "smart" due to embedded processors and communication capabilities , the provisioning of intuitive user interfaces to control smart things is quickly gaining importance. We present a model-based interface description scheme that enables automatic, modality-independent user interface generation. User interface description languages based on our approach carry enough information to suggest intuitive interfaces while still being easily producible for developers. This is enabled by describing the atomic interactive components of a device and capturing the semantics of interactions with the device. We propose a taxonomy of abstract sensing and actuation primitives and present a smartphone application that can act as a ubiquitous device controller. An evaluation of the mobile application in a laboratory setup, home environments, and an educational setting as well as the results of a user study highlight the accessibility of the proposed scheme for application developers and its suitability for controlling smart devices.},
	number = {2},
	journal = {ACM Trans. Comput.-Hum. Interact},
	author = {Mayer, Simon and Tschofen, Andreas and Zurich, Eth and Dey, Anind K and Mattern, Friedemann},
	year = {2014},
	keywords = {Internet of Things, Human Factors, Experimentation Additional Key Words and Phrases:, H52 [User Interfaces]: Theory and Models, mobile interfaces, Prototyping General Terms: Languages},
	pages = {1--25},
	file = {Mayer et al_2014_User Interfaces for Smart Things-A Generative Approach with Semantic Interaction Descriptions model-based user interfaces, user interface descriptions, Web of Things ACM Reference Format.pdf:/Users/orsonxu/Zotero/storage/ET8X7Z2U/Mayer et al_2014_User Interfaces for Smart Things-A Generative Approach with Semantic Interaction Descriptions model-based user interfaces, user interface descriptions, Web of Things ACM Reference Format.pdf:application/pdf;Mayer et al_2014_User Interfaces for Smart Things-A Generative Approach with Semantic Interaction Descriptions model-based user interfaces, user interface descriptions, Web of Things ACM Reference Format.pdf:/Users/orsonxu/Zotero/storage/VSLJYXII/Mayer et al_2014_User Interfaces for Smart Things-A Generative Approach with Semantic Interaction Descriptions model-based user interfaces, user interface descriptions, Web of Things ACM Reference Format.pdf:application/pdf},
}

@article{gatteschi_semantics-based_2016,
	title = {Semantics-{Based} {Intelligent} {Human}-{Computer} {Interaction}},
	volume = {31},
	issn = {1541-1672},
	url = {https://ieeexplore.ieee.org/document/7325175/},
	doi = {10.1109/MIS.2015.97},
	number = {4},
	journal = {IEEE Intelligent Systems},
	author = {Gatteschi, Valentina and Lamberti, Fabrizio and Montuschi, Paolo and Sanna, Andrea},
	month = jul,
	year = {2016},
	pages = {11--21},
	file = {Gatteschi et al_2016_Semantics-Based Intelligent Human-Computer Interaction.pdf:/Users/orsonxu/Zotero/storage/CVAHE8FL/Gatteschi et al_2016_Semantics-Based Intelligent Human-Computer Interaction.pdf:application/pdf;Gatteschi et al_2016_Semantics-Based Intelligent Human-Computer Interaction.pdf:/Users/orsonxu/Zotero/storage/Q4MNNVYP/Gatteschi et al_2016_Semantics-Based Intelligent Human-Computer Interaction.pdf:application/pdf},
}

@article{choi_taxonomy_2014,
	title = {A taxonomy and notation method for three-dimensional hand gestures},
	volume = {44},
	issn = {01698141},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169814113001285},
	doi = {10.1016/j.ergon.2013.10.011},
	abstract = {Recently, studies on gesture-based interfaces have made an effort to improve the intuitiveness of gesture commands by asking users to deﬁne a gesture for a command. However, there are few methods to organize and notate user-deﬁned gestures in a systematic approach. To resolve this, we propose a threedimensional (3D) Hand Gesture Taxonomy and Notation Method. We ﬁrst derived elements of a hand gesture by analyzing related studies and subsequently developed the 3D Hand Gesture Taxonomy based on the elements. Moreover, we devised a Notation Method based on a combination of the elements and also matched a code to each element for easy notation. Finally, we have veriﬁed the usefulness of the Notation Method by training participants to notate hand gestures and by asking another set of participants to recreate the notated gestures. In short, this research proposes a novel and systematic approach to notate hand gesture commands.},
	language = {en},
	number = {1},
	urldate = {2021-07-06},
	journal = {International Journal of Industrial Ergonomics},
	author = {Choi, Eunjung and Kim, Heejin and Chung, Min K.},
	month = jan,
	year = {2014},
	pages = {171--188},
	file = {Choi et al. - 2014 - A taxonomy and notation method for three-dimension.pdf:/Users/orsonxu/Zotero/storage/8FEC6JZM/Choi et al. - 2014 - A taxonomy and notation method for three-dimension.pdf:application/pdf},
}

@inproceedings{nacenta_memorability_2013,
	address = {Paris France},
	title = {Memorability of pre-designed and user-defined gesture sets},
	isbn = {978-1-4503-1899-0},
	url = {https://dl.acm.org/doi/10.1145/2470654.2466142},
	doi = {10.1145/2470654.2466142},
	abstract = {We studied the memorability of free-form gesture sets for invoking actions. We compared three types of gesture sets: user-defined gesture sets, gesture sets designed by the authors, and random gesture sets in three studies with 33 participants in total. We found that user-defined gestures are easier to remember, both immediately after creation and on the next day (up to a 24\% difference in recall rate compared to pre-designed gestures). We also discovered that the differences between gesture sets are mostly due to association errors (rather than gesture form errors), that participants prefer user-defined sets, and that they think user-defined gestures take less time to learn. Finally, we contribute a qualitative analysis of the tradeoffs involved in gesture type selection and share our data and a video corpus of 66 gestures for replicability and further analysis.},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Nacenta, Miguel A. and Kamber, Yemliha and Qiang, Yizhou and Kristensson, Per Ola},
	month = apr,
	year = {2013},
	pages = {1099--1108},
	file = {Nacenta et al. - 2013 - Memorability of pre-designed and user-defined gest.pdf:/Users/orsonxu/Zotero/storage/ABGHYVTA/Nacenta et al. - 2013 - Memorability of pre-designed and user-defined gest.pdf:application/pdf},
}

@inproceedings{zhang_tomo_2015,
	address = {Charlotte NC USA},
	title = {Tomo: {Wearable}, {Low}-{Cost} {Electrical} {Impedance} {Tomography} for {Hand} {Gesture} {Recognition}},
	isbn = {978-1-4503-3779-3},
	shorttitle = {Tomo},
	url = {https://dl.acm.org/doi/10.1145/2807442.2807480},
	doi = {10.1145/2807442.2807480},
	abstract = {We present Tomo, a wearable, low-cost system using Electrical Impedance Tomography (EIT) to recover the interior impedance geometry of a user’s arm. This is achieved by measuring the cross-sectional impedances between all pairs of eight electrodes resting on a user’s skin. Our approach is sufficiently compact and low-powered that we integrated the technology into a prototype wrist- and armband, which can monitor and classify gestures in real-time. We conducted a user study that evaluated two gesture sets, one focused on gross hand gestures and another using thumb-to-finger pinches. Our wrist location achieved 97\% and 87\% accuracies on these gesture sets respectively, while our arm location achieved 93\% and 81\%. We ultimately envision this technique being integrated into future smartwatches, allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 28th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} \& {Technology}},
	publisher = {ACM},
	author = {Zhang, Yang and Harrison, Chris},
	month = nov,
	year = {2015},
	pages = {167--173},
	file = {Zhang and Harrison - 2015 - Tomo Wearable, Low-Cost Electrical Impedance Tomo.pdf:/Users/orsonxu/Zotero/storage/FZU6FEC7/Zhang and Harrison - 2015 - Tomo Wearable, Low-Cost Electrical Impedance Tomo.pdf:application/pdf},
}

@inproceedings{oh_challenges_2013,
	address = {Paris France},
	title = {The challenges and potential of end-user gesture customization},
	isbn = {978-1-4503-1899-0},
	url = {https://dl.acm.org/doi/10.1145/2470654.2466145},
	doi = {10.1145/2470654.2466145},
	abstract = {The vast majority of work on understanding and supporting the gesture creation process has focused on professional designers. In contrast, gesture customization by end users—which may offer better memorability, efficiency and accessibility than pre-defined gestures—has received little attention. To understand the end-user gesture creation process, we conducted a study where 20 participants were asked to: (1) exhaustively create new gestures for an openended use case; (2) exhaustively create new gestures for 12 specific use cases; (3) judge the saliency of different touchscreen gesture features. Our findings showed that even when asked to create novel gestures, participants tended to focus on the familiar. Misconceptions about the gesture recognizer’s abilities were also evident, and in some cases constrained the range of gestures that participants created. Finally, as a calibration point for future research, we used a simple gesture recognizer (\$N) to analyze recognition accuracy of the participants’ custom gesture sets: accuracy was 68–88\% on average, depending on the amount of training and the customization scenario. We conclude with implications for the design of a mixed-initiative approach to support custom gesture creation.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Oh, Uran and Findlater, Leah},
	month = apr,
	year = {2013},
	pages = {1129--1138},
	file = {Oh and Findlater - 2013 - The challenges and potential of end-user gesture c.pdf:/Users/orsonxu/Zotero/storage/9SE8N3MR/Oh and Findlater - 2013 - The challenges and potential of end-user gesture c.pdf:application/pdf},
}

@article{jiang_feasibility_2018,
	title = {Feasibility of {Wrist}-{Worn}, {Real}-{Time} {Hand}, and {Surface} {Gesture} {Recognition} via {sEMG} and {IMU} {Sensing}},
	volume = {14},
	issn = {1941-0050},
	doi = {10.1109/TII.2017.2779814},
	abstract = {While most wearable gesture recognition approaches focus on the forearm or fingers, the wrist may be a more suitable location for practical use. We present the design and validation of a real-time gesture recognition wristband based on surface electromyography and inertial measurement unit sensing fusion, which can recognize 8 air gestures and 4 surface gestures with 2 distinct force levels. Ten healthy subjects performed an initial gesture recognition experiment, followed by a second experiment 1 h later and a third experiment 1 day later. Classification accuracies for the initial experiment were 92.6\% and 88.8\% for air and surface gestures, respectively, and there were no changes in accuracy results during testing 1 h. and 1 day later (p {\textgreater} 0.05). These results demonstrate the feasibility of wrist-based gesture recognition paving the way for potential future integration in to a smart watch or other wrist-worn wearable for intuitive human computer interaction.},
	number = {8},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Jiang, Shuo and Lv, Bo and Guo, Weichao and Zhang, Chao and Wang, Haitao and Sheng, Xinjun and Shull, Peter B.},
	month = aug,
	year = {2018},
	pages = {3376--3385},
	file = {Jiang et al_2018_Feasibility of Wrist-Worn, Real-Time Hand, and Surface Gesture Recognition via sEMG and IMU Sensing.pdf:/Users/orsonxu/Zotero/storage/XAUTFJYN/Jiang et al_2018_Feasibility of Wrist-Worn, Real-Time Hand, and Surface Gesture Recognition via sEMG and IMU Sensing.pdf:application/pdf;Jiang et al_2018_Feasibility of Wrist-Worn, Real-Time Hand, and Surface Gesture Recognition via sEMG and IMU Sensing.pdf:/Users/orsonxu/Zotero/storage/JLK2AHYQ/Jiang et al_2018_Feasibility of Wrist-Worn, Real-Time Hand, and Surface Gesture Recognition via sEMG and IMU Sensing.pdf:application/pdf},
}

@article{kim_imu_2019,
	title = {{IMU} {Sensor}-{Based} {Hand} {Gesture} {Recognition} for {Human}-{Machine} {Interfaces}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/18/3827},
	doi = {10.3390/s19183827},
	abstract = {We propose an efﬁcient hand gesture recognition (HGR) algorithm, which can cope with time-dependent data from an inertial measurement unit (IMU) sensor and support real-time learning for various human-machine interface (HMI) applications. Although the data extracted from IMU sensors are time-dependent, most existing HGR algorithms do not consider this characteristic, which results in the degradation of recognition performance. Because the dynamic time warping (DTW) technique considers the time-dependent characteristic of IMU sensor data, the recognition performance of DTW-based algorithms is better than that of others. However, the DTW technique requires a very complex learning algorithm, which makes it difﬁcult to support real-time learning. To solve this issue, the proposed HGR algorithm is based on a restricted column energy (RCE) neural network, which has a very simple learning scheme in which neurons are activated when necessary. By replacing the metric calculation of the RCE neural network with DTW distance, the proposed algorithm exhibits superior recognition performance for time-dependent sensor data while supporting real-time learning. Our veriﬁcation results on a ﬁeld-programmable gate array (FPGA)-based test platform show that the proposed HGR algorithm can achieve a recognition accuracy of 98.6\% and supports real-time learning and recognition at an operating frequency of 150 MHz.},
	language = {en},
	number = {18},
	urldate = {2021-08-03},
	journal = {Sensors},
	author = {Kim, Minwoo and Cho, Jaechan and Lee, Seongjoo and Jung, Yunho},
	month = sep,
	year = {2019},
	pages = {3827},
	file = {Kim et al. - 2019 - IMU Sensor-Based Hand Gesture Recognition for Huma.pdf:/Users/orsonxu/Zotero/storage/B28JD6FZ/Kim et al. - 2019 - IMU Sensor-Based Hand Gesture Recognition for Huma.pdf:application/pdf},
}

@inproceedings{gong_wristwhirl_2016,
	address = {Tokyo Japan},
	title = {{WristWhirl}: {One}-handed {Continuous} {Smartwatch} {Input} using {Wrist} {Gestures}},
	isbn = {978-1-4503-4189-9},
	shorttitle = {{WristWhirl}},
	url = {https://dl.acm.org/doi/10.1145/2984511.2984563},
	doi = {10.1145/2984511.2984563},
	abstract = {We propose and study a new input modality, WristWhirl, that uses the wrist as an always-available joystick to perform one-handed continuous input on smartwatches. We explore the influence of the wrist’s bio-mechanical properties for performing gestures to interact with a smartwatch, both while standing still and walking. Through a user study, we examine the impact of performing 8 distinct gestures (4 directional marks, and 4 free-form shapes) on the stability of the watch surface. Participants were able to perform directional marks using the wrist as a joystick at an average rate of half a second and free-form shapes at an average rate of approximately 1.5secs. The free-form shapes could be recognized by a \$1 gesture recognizer with an accuracy of 93.8\% and by three human inspectors with an accuracy of 85\%. From these results, we designed and implemented a proof-of-concept device by augmenting the watchband using an array of proximity sensors, which can be used to draw gestures with high quality. Finally, we demonstrate a number of scenarios that benefit from one-handed continuous input on smartwatches using WristWhirl.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 29th {Annual} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Gong, Jun and Yang, Xing-Dong and Irani, Pourang},
	month = oct,
	year = {2016},
	pages = {861--872},
	file = {Gong et al. - 2016 - WristWhirl One-handed Continuous Smartwatch Input.pdf:/Users/orsonxu/Zotero/storage/Q9GCY8UY/Gong et al. - 2016 - WristWhirl One-handed Continuous Smartwatch Input.pdf:application/pdf},
}

@inproceedings{kim_digits_2012,
	address = {Cambridge, Massachusetts, USA},
	title = {Digits: freehand {3D} interactions anywhere using a wrist-worn gloveless sensor},
	isbn = {978-1-4503-1580-7},
	shorttitle = {Digits},
	url = {http://dl.acm.org/citation.cfm?doid=2380116.2380139},
	doi = {10.1145/2380116.2380139},
	abstract = {Digits is a wrist-worn sensor that recovers the full 3D pose of the user’s hand. This enables a variety of freehand interactions on the move. The system targets mobile settings, and is speciﬁcally designed to be low-power and easily reproducible using only off-the-shelf hardware. The electronics are self-contained on the user’s wrist, but optically image the entirety of the user’s hand. This data is processed using a new pipeline that robustly samples key parts of the hand, such as the tips and lower regions of each ﬁnger. These sparse samples are fed into new kinematic models that leverage the biomechanical constraints of the hand to recover the 3D pose of the user’s hand. The proposed system works without the need for full instrumentation of the hand (for example using data gloves), additional sensors in the environment, or depth cameras which are currently prohibitive for mobile scenarios due to power and form-factor considerations. We demonstrate the utility of Digits for a variety of application scenarios, including 3D spatial interaction with mobile devices, eyes-free interaction on-the-move, and gaming. We conclude with a quantitative and qualitative evaluation of our system, and discussion of strengths, limitations and future work.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 25th annual {ACM} symposium on {User} interface software and technology - {UIST} '12},
	publisher = {ACM Press},
	author = {Kim, David and Hilliges, Otmar and Izadi, Shahram and Butler, Alex D. and Chen, Jiawen and Oikonomidis, Iason and Olivier, Patrick},
	year = {2012},
	pages = {167},
	file = {Kim et al. - 2012 - Digits freehand 3D interactions anywhere using a .pdf:/Users/orsonxu/Zotero/storage/BNJA9KKS/Kim et al. - 2012 - Digits freehand 3D interactions anywhere using a .pdf:application/pdf},
}

@inproceedings{wu_back-hand-pose_2020,
	address = {Virtual Event USA},
	title = {Back-{Hand}-{Pose}: {3D} {Hand} {Pose} {Estimation} for a {Wrist}-worn {Camera} via {Dorsum} {Deformation} {Network}},
	isbn = {978-1-4503-7514-6},
	shorttitle = {Back-{Hand}-{Pose}},
	url = {https://dl.acm.org/doi/10.1145/3379337.3415897},
	doi = {10.1145/3379337.3415897},
	abstract = {The automatic recognition of how people use their hands and fngers in natural settings – without instrumenting the fngers – can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fngers scarcely visible. To address this, a special network that observes deformations on the back of the hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress fnger joint angles from spatio-temporal features of the dorsal hand region (the movement of bones, muscle, and tendons). This work is the frst vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81° for user-specifc models and 9.77° for a general model. Further evaluation shows that our system outperforms previous work with an average of 20\% higher accuracy in recognizing dynamic gestures, and achieves a 75\% accuracy of detecting 11 different grasp types. We also demonstrate 3 applications which employ our system as a control device, an input device, and a grasped object recognizer.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Wu, Erwin and Yuan, Ye and Yeo, Hui-Shyong and Quigley, Aaron and Koike, Hideki and Kitani, Kris M.},
	month = oct,
	year = {2020},
	pages = {1147--1160},
	file = {Wu et al. - 2020 - Back-Hand-Pose 3D Hand Pose Estimation for a Wris.pdf:/Users/orsonxu/Zotero/storage/D4REAUY7/Wu et al. - 2020 - Back-Hand-Pose 3D Hand Pose Estimation for a Wris.pdf:application/pdf},
}

@inproceedings{yeo_opisthenar_2019,
	address = {New Orleans LA USA},
	title = {Opisthenar: {Hand} {Poses} and {Finger} {Tapping} {Recognition} by {Observing} {Back} of {Hand} {Using} {Embedded} {Wrist} {Camera}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {Opisthenar},
	url = {https://dl.acm.org/doi/10.1145/3332165.3347867},
	doi = {10.1145/3332165.3347867},
	abstract = {We introduce a vision-based technique to recognize static hand poses and dynamic fnger tapping gestures. Our approach employs a camera on the wrist, with a view of the opisthenar (back of the hand) area. We envisage such cameras being included in a wrist-worn device such as a smartwatch, ftness tracker or wristband. Indeed, selected off-the-shelf smartwatches now incorporate a built-in camera on the side for photography purposes. However, in this confguration, the fngers are occluded from the view of the camera. The oblique angle and placement of the camera make typical vision-based techniques diffcult to adopt. Our alternative approach observes small movements and changes in the shape, tendons, skin and bones on the opisthenar area. We train deep neural networks to recognize both hand poses and dynamic fnger tapping gestures. While this is a challenging confguration for sensing, we tested the recognition with a real-time user test and achieved a high recognition rate of 89.4\% (static poses) and 67.5\% (dynamic gestures). Our results further demonstrate that our approach can generalize across sessions and to new users. Namely, users can remove and replace the wrist-worn device while new users can employ a previously trained system, to a certain degree. We conclude by demonstrating three applications and suggest future avenues of work based on sensing the back of the hand.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Yeo, Hui-Shyong and Wu, Erwin and Lee, Juyoung and Quigley, Aaron and Koike, Hideki},
	month = oct,
	year = {2019},
	pages = {963--971},
	file = {Yeo et al. - 2019 - Opisthenar Hand Poses and Finger Tapping Recognit.pdf:/Users/orsonxu/Zotero/storage/GR9LA8P2/Yeo et al. - 2019 - Opisthenar Hand Poses and Finger Tapping Recognit.pdf:application/pdf},
}

@inproceedings{laput_viband_2016,
	address = {Tokyo Japan},
	title = {{ViBand}: {High}-{Fidelity} {Bio}-{Acoustic} {Sensing} {Using} {Commodity} {Smartwatch} {Accelerometers}},
	isbn = {978-1-4503-4189-9},
	shorttitle = {{ViBand}},
	url = {https://dl.acm.org/doi/10.1145/2984511.2984582},
	doi = {10.1145/2984511.2984582},
	abstract = {Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction. Their position on the wrist makes them ideal for capturing bio-acoustic signals. We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch’s existing accelerometer to 4 kHz. Using this new source of high-fidelity data, we uncovered a wide range of applications. For example, we can use bio-acoustic data to classify hand gestures such as flicks, claps, scratches, and taps, which combine with on-device motion tracking to create a wide range of expressive input modalities. Bioacoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling passive object recognition that can augment everyday experiences with context-aware functionality. Finally, we can generate structured vibrations using a transducer, and show that data can be transmitted through the human body. Overall, our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation, making such interactions considerably more feasible for inclusion in future consumer devices.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 29th {Annual} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Laput, Gierad and Xiao, Robert and Harrison, Chris},
	month = oct,
	year = {2016},
	pages = {321--333},
	file = {Laput et al. - 2016 - ViBand High-Fidelity Bio-Acoustic Sensing Using C.pdf:/Users/orsonxu/Zotero/storage/A3F4BN2R/Laput et al. - 2016 - ViBand High-Fidelity Bio-Acoustic Sensing Using C.pdf:application/pdf},
}

@inproceedings{nandakumar_fingerio_2016,
	address = {San Jose California USA},
	title = {{FingerIO}: {Using} {Active} {Sonar} for {Fine}-{Grained} {Finger} {Tracking}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {{FingerIO}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858580},
	doi = {10.1145/2858036.2858580},
	abstract = {We present ﬁngerIO, a novel ﬁne-grained ﬁnger tracking solution for around-device interaction. FingerIO does not require instrumenting the ﬁnger with sensors and works even in the presence of occlusions between the ﬁnger and the device. We achieve this by transforming the device into an active sonar system that transmits inaudible sound signals and tracks the echoes of the ﬁnger at its microphones. To achieve subcentimeter level tracking accuracies, we present an innovative approach that use a modulation technique commonly used in wireless communication called Orthogonal Frequency Division Multiplexing (OFDM). Our evaluation shows that ﬁngerIO can achieve 2-D ﬁnger tracking with an average accuracy of 8 mm using the in-built microphones and speaker of a Samsung Galaxy S4. It also tracks subtle ﬁnger motion around the device, even when the phone is inside a pocket. Finally, we prototype a smart watch form-factor ﬁngerIO device and show that it can extend the interaction space to a 0.5×0.25 m2 region on either side of the device and work even when it is fully occluded from the ﬁnger.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Nandakumar, Rajalakshmi and Iyer, Vikram and Tan, Desney and Gollakota, Shyamnath},
	month = may,
	year = {2016},
	pages = {1515--1525},
	file = {Nandakumar et al. - 2016 - FingerIO Using Active Sonar for Fine-Grained Fing.pdf:/Users/orsonxu/Zotero/storage/T33SPXBI/Nandakumar et al. - 2016 - FingerIO Using Active Sonar for Fine-Grained Fing.pdf:application/pdf},
}

@inproceedings{harrison_skinput_2010,
	address = {Atlanta, Georgia, USA},
	title = {Skinput: appropriating the body as an input surface},
	isbn = {978-1-60558-929-9},
	shorttitle = {Skinput},
	url = {http://portal.acm.org/citation.cfm?doid=1753326.1753394},
	doi = {10.1145/1753326.1753394},
	abstract = {We present Skinput, a technology that appropriates the human body for acoustic transmission, allowing the skin to be used as an input surface. In particular, we resolve the location of finger taps on the arm and hand by analyzing mechanical vibrations that propagate through the body. We collect these signals using a novel array of sensors worn as an armband. This approach provides an always available, naturally portable, and on-body finger input system. We assess the capabilities, accuracy and limitations of our technique through a two-part, twenty-participant user study. To further illustrate the utility of our approach, we conclude with several proof-of-concept applications we developed.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 28th international conference on {Human} factors in computing systems - {CHI} '10},
	publisher = {ACM Press},
	author = {Harrison, Chris and Tan, Desney and Morris, Dan},
	year = {2010},
	pages = {453},
	file = {Harrison et al. - 2010 - Skinput appropriating the body as an input surfac.pdf:/Users/orsonxu/Zotero/storage/HNLR64GC/Harrison et al. - 2010 - Skinput appropriating the body as an input surfac.pdf:application/pdf},
}

@inproceedings{truong_capband_2018,
	address = {Shenzhen China},
	title = {{CapBand}: {Battery}-free {Successive} {Capacitance} {Sensing} {Wristband} for {Hand} {Gesture} {Recognition}},
	isbn = {978-1-4503-5952-8},
	shorttitle = {{CapBand}},
	url = {https://dl.acm.org/doi/10.1145/3274783.3274854},
	doi = {10.1145/3274783.3274854},
	abstract = {We present CapBand, a battery-free hand gesture recognition wearable in the form of a wristband. The key challenges in creating such a system are (1) to sense useful hand gestures at ultra-low power so that the device can be powered by the limited energy harvestable from the surrounding environment and (2) to make the system work reliably without requiring training every time a user puts on the wristband. We present successive capacitance sensing, an ultra-low power sensing technique, to capture small skin deformations due to muscle and tendon movements on the user’s wrist, which corresponds to speci�c groups of wrist muscles representing the gestures being performed. We build a wrist muscles-to-gesture model, based on which we develop a hand gesture classi�cation method using both motion and static features. To eliminate the need for per-usage training, we propose a kernel-based on-wrist localization technique to detect the CapBand’s position on the user’s wrist. We prototype CapBand with a custom-designed capacitance sensor array on two� exible circuits driven by a custom-built electronic board, a heterogeneous material-made, deformable silicone band, and a custom-built energy harvesting and management module. Evaluations on 20 subjects show 95.0\% accuracy of gesture recognition when recognizing 15 di�erent hand gestures and 95.3\% accuracy of on-wrist localization.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 16th {ACM} {Conference} on {Embedded} {Networked} {Sensor} {Systems}},
	publisher = {ACM},
	author = {Truong, Hoang and Zhang, Shuo and Muncuk, Ufuk and Nguyen, Phuc and Bui, Nam and Nguyen, Anh and Lv, Qin and Chowdhury, Kaushik and Dinh, Thang and Vu, Tam},
	month = nov,
	year = {2018},
	pages = {54--67},
	file = {Truong et al. - 2018 - CapBand Battery-free Successive Capacitance Sensi.pdf:/Users/orsonxu/Zotero/storage/CLY8NVBU/Truong et al. - 2018 - CapBand Battery-free Successive Capacitance Sensi.pdf:application/pdf},
}

@inproceedings{rekimoto_gesturewrist_2001,
	title = {{GestureWrist} and {GesturePad}: unobtrusive wearable interaction devices},
	shorttitle = {{GestureWrist} and {GesturePad}},
	doi = {10.1109/ISWC.2001.962092},
	abstract = {In this paper we introduce two input devices for wearable computers, called GestureWrist and GesturePad. Both devices allow users to interact with wearable or nearby computers by using gesture-based commands. Both are designed to be as unobtrusive as possible, so they can be used under various social contexts. The first device, called GestureWrist, is a wristband-type input device that recognizes hand gestures and forearm movements. Unlike DataGloves or other hand gesture-input devices, all sensing elements are embedded in a normal wristband. The second device, called GesturePad, is a sensing module that can be attached on the inside of clothes, and users can interact with this module from the outside. It transforms conventional clothes into an interactive device without changing their appearance.},
	booktitle = {Proceedings {Fifth} {International} {Symposium} on {Wearable} {Computers}},
	author = {Rekimoto, J.},
	month = oct,
	year = {2001},
	pages = {21--27},
	file = {Rekimoto_2001_GestureWrist and GesturePad.pdf:/Users/orsonxu/Zotero/storage/G9BNBEG2/Rekimoto_2001_GestureWrist and GesturePad.pdf:application/pdf;Rekimoto_2001_GestureWrist and GesturePad.pdf:/Users/orsonxu/Zotero/storage/WFZIE65E/Rekimoto_2001_GestureWrist and GesturePad.pdf:application/pdf},
}

@inproceedings{iravantchi_beamband_2019,
	address = {Glasgow Scotland Uk},
	title = {{BeamBand}: {Hand} {Gesture} {Sensing} with {Ultrasonic} {Beamforming}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{BeamBand}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300245},
	doi = {10.1145/3290605.3300245},
	abstract = {BeamBand is a wrist-worn system that uses ultrasonic beamforming for hand gesture sensing. Using an array of small transducers, arranged on the wrist, we can ensemble acoustic wavefronts to project acoustic energy at specified angles and focal lengths. This allows us to interrogate the surface geometry of the hand with inaudible sound in a raster-scan-like manner, from multiple viewpoints. We use the resulting, characteristic reflections to recognize hand pose at 8 FPS. In our user study, we found that BeamBand supports a six-class hand gesture set at 94.6\% accuracy. Even across sessions, when the sensor is removed and reworn later, accuracy remains high: 89.4\%. We describe our software and hardware, and future avenues for integration into devices such as smartwatches and VR controllers.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Iravantchi, Yasha and Goel, Mayank and Harrison, Chris},
	month = may,
	year = {2019},
	pages = {1--10},
	file = {Iravantchi et al. - 2019 - BeamBand Hand Gesture Sensing with Ultrasonic Bea.pdf:/Users/orsonxu/Zotero/storage/ZBNRLIVX/Iravantchi et al. - 2019 - BeamBand Hand Gesture Sensing with Ultrasonic Bea.pdf:application/pdf},
}

@inproceedings{iravantchi_interferi_2019,
	address = {Glasgow Scotland Uk},
	title = {Interferi: {Gesture} {Sensing} using {On}-{Body} {Acoustic} {Interferometry}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Interferi},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300506},
	doi = {10.1145/3290605.3300506},
	abstract = {Interferi is an on-body gesture sensing technique using acoustic interferometry. We use ultrasonic transducers resting on the skin to create acoustic interference patterns inside the wearer’s body, which interact with anatomical features in complex, yet characteristic ways. We focus on two areas of the body with great expressive power: the hands and face. For each, we built and tested a series of worn sensor configurations, which we used to identify useful transducer arrangements and machine learning features. We created final prototypes for the hand and face, which our study results show can support eleven- and nineclass gestures sets at 93.4\% and 89.0\% accuracy, respectively. We also evaluated our system in four continuous tracking tasks, including smile intensity and weight estimation, which never exceed 9.5\% error. We believe these results show great promise and illuminate an interesting sensing technique for HCI applications.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Iravantchi, Yasha and Zhang, Yang and Bernitsas, Evi and Goel, Mayank and Harrison, Chris},
	month = may,
	year = {2019},
	pages = {1--13},
	file = {Iravantchi et al. - 2019 - Interferi Gesture Sensing using On-Body Acoustic .pdf:/Users/orsonxu/Zotero/storage/7GC7Y5PT/Iravantchi et al. - 2019 - Interferi Gesture Sensing using On-Body Acoustic .pdf:application/pdf},
}

@inproceedings{saponas_enabling_2009,
	address = {Victoria, BC, Canada},
	title = {Enabling always-available input with muscle-computer interfaces},
	isbn = {978-1-60558-745-5},
	url = {http://portal.acm.org/citation.cfm?doid=1622176.1622208},
	doi = {10.1145/1622176.1622208},
	abstract = {Previous work has demonstrated the viability of applying offline analysis to interpret forearm electromyography (EMG) and classify finger gestures on a physical surface. We extend those results to bring us closer to using musclecomputer interfaces for always-available input in real-world applications. We leverage existing taxonomies of natural human grips to develop a gesture set covering interaction in free space even when hands are busy with other objects. We present a system that classifies these gestures in real-time and we introduce a bi-manual paradigm that enables use in interactive systems. We report experimental results demonstrating four-finger classification accuracies averaging 79\% for pinching, 85\% while holding a travel mug, and 88\% when carrying a weighted bag. We further show generalizability across different arm postures and explore the tradeoffs of providing real-time visual feedback.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 22nd annual {ACM} symposium on {User} interface software and technology - {UIST} '09},
	publisher = {ACM Press},
	author = {Saponas, T. Scott and Tan, Desney S. and Morris, Dan and Balakrishnan, Ravin and Turner, Jim and Landay, James A.},
	year = {2009},
	pages = {167},
	file = {Saponas et al. - 2009 - Enabling always-available input with muscle-comput.pdf:/Users/orsonxu/Zotero/storage/3GJXULGW/Saponas et al. - 2009 - Enabling always-available input with muscle-comput.pdf:application/pdf},
}

@inproceedings{dementyev_wristflex_2014,
	address = {Honolulu Hawaii USA},
	title = {{WristFlex}: low-power gesture input with wrist-worn pressure sensors},
	isbn = {978-1-4503-3069-5},
	shorttitle = {{WristFlex}},
	url = {https://dl.acm.org/doi/10.1145/2642918.2647396},
	doi = {10.1145/2642918.2647396},
	abstract = {In this paper we present WristFlex, an always-available onbody gestural interface. Using an array of force sensitive resistors (FSRs) worn around the wrist, the interface can distinguish subtle ﬁnger pinch gestures with high accuracy ({\textgreater}80\%) and speed. The system is trained to classify gestures from subtle tendon movements on the wrist. We demonstrate that WristFlex is a complete system that works wirelessly in realtime. The system is simple and light-weight in terms of power consumption and computational overhead. WristFlex’s sensor power consumption is 60.7 μW, allowing the prototype to potentially last more then a week on a small lithium polymer battery. Also, WristFlex is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. We perform user studies to evaluate the accuracy, speed, and repeatability. We demonstrate that the number of gestures can be extended with orientation data from an accelerometer. We conclude by showing example applications.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 27th annual {ACM} symposium on {User} interface software and technology},
	publisher = {ACM},
	author = {Dementyev, Artem and Paradiso, Joseph A.},
	month = oct,
	year = {2014},
	pages = {161--166},
	file = {Dementyev and Paradiso - 2014 - WristFlex low-power gesture input with wrist-worn.pdf:/Users/orsonxu/Zotero/storage/4RHBGGZ3/Dementyev and Paradiso - 2014 - WristFlex low-power gesture input with wrist-worn.pdf:application/pdf},
}

@article{jung_wearable_2015,
	title = {A {Wearable} {Gesture} {Recognition} {Device} for {Detecting} {Muscular} {Activities} {Based} on {Air}-{Pressure} {Sensors}},
	volume = {11},
	issn = {1941-0050},
	doi = {10.1109/TII.2015.2405413},
	abstract = {Recognition of human gestures plays an important role in a number of human-interactive applications, such as mobile phones, health monitoring systems, and human-assistive robots. Electromyography (EMG) is one of the most common and intuitive methods used for detecting gestures based on muscle activities. The EMG, however, is in general, too sensitive to environmental disturbances, such as electrical noise, electromagnetic signals, humidity, and so on. In this paper, a new method for recognizing the muscular activities is proposed based on air-pressure sensors and air-bladders. The muscular activity is detected by measuring the change of the air pressure in an air-bladder contacting the interested muscle(s). Since the change of the air pressure can be more robustly measured compared with the change of electric signals appeared on the skin, the proposed sensing method is useful for mobile devices due to its great signal-to-noise ratio (SNR) and fast response time. The principle and applications of the proposed sensing method are introduced in this paper. The performance of the proposed method is evaluated in terms of linearity, repeatability, wear-comfort, etc., and is also verified by comparing it with an EMG signal and a motion sensor.},
	number = {2},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Jung, Pyeong-Gook and Lim, Gukchan and Kim, Seonghyok and Kong, Kyoungchul},
	month = apr,
	year = {2015},
	pages = {485--494},
	file = {Jung et al_2015_A Wearable Gesture Recognition Device for Detecting Muscular Activities Based on Air-Pressure Sensors.pdf:/Users/orsonxu/Zotero/storage/S4PV4ZUC/Jung et al_2015_A Wearable Gesture Recognition Device for Detecting Muscular Activities Based on Air-Pressure Sensors.pdf:application/pdf;Jung et al_2015_A Wearable Gesture Recognition Device for Detecting Muscular Activities Based on Air-Pressure Sensors.pdf:/Users/orsonxu/Zotero/storage/JK8KQJFI/Jung et al_2015_A Wearable Gesture Recognition Device for Detecting Muscular Activities Based on Air-Pressure Sensors.pdf:application/pdf},
}

@inproceedings{strohmeier_flick_2012,
	address = {Kingston Ontario Canada},
	title = {With a flick of the wrist: stretch sensors as lightweight input for mobile devices},
	isbn = {978-1-4503-1174-8},
	shorttitle = {With a flick of the wrist},
	url = {https://dl.acm.org/doi/10.1145/2148131.2148195},
	doi = {10.1145/2148131.2148195},
	abstract = {With WristFlicker, we detect wrist movement through sets of stretch sensors embedded in clothing. Our system supports wrist rotation (pronation/supination), and both wrist tilts (flexion/extension and ulnar/radial deviation). Each wrist movement is measured by two opposing stretch sensors, mimicking the counteracting movement of muscles. We discuss interaction techniques that allow a user to control a music player through this lightweight input.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Tangible}, {Embedded} and {Embodied} {Interaction}},
	publisher = {ACM},
	author = {Strohmeier, Paul and Vertegaal, Roel and Girouard, Audrey},
	month = feb,
	year = {2012},
	pages = {307--308},
	file = {Strohmeier et al. - 2012 - With a flick of the wrist stretch sensors as ligh.pdf:/Users/orsonxu/Zotero/storage/BZYB6FAM/Strohmeier et al. - 2012 - With a flick of the wrist stretch sensors as ligh.pdf:application/pdf},
}

@inproceedings{chen_utrack_2013,
	address = {St. Andrews Scotland, United Kingdom},
	title = {{uTrack}: {3D} input using two magnetic sensors},
	isbn = {978-1-4503-2268-3},
	shorttitle = {{uTrack}},
	url = {https://dl.acm.org/doi/10.1145/2501988.2502035},
	doi = {10.1145/2501988.2502035},
	abstract = {While much progress has been made in wearable computing in recent years, input techniques remain a key challenge. In this paper, we introduce uTrack, a technique to convert the thumb and fingers into a 3D input system using magnetic field (MF) sensing. A user wears a pair of magnetometers on the back of their fingers and a permanent magnet affixed to the back of the thumb. By moving the thumb across the fingers, we obtain a continuous input stream that can be used for 3D pointing. Specifically, our novel algorithm calculates the magnet’s 3D position and tilt angle directly from the sensor readings. We evaluated uTrack as an input device, showing an average tracking accuracy of 4.84 mm in 3D space – sufficient for subtle interaction. We also demonstrate a real-time prototype and example applications allowing users to interact with the computer using 3D finger input.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 26th annual {ACM} symposium on {User} interface software and technology},
	publisher = {ACM},
	author = {Chen, Ke-Yu and Lyons, Kent and White, Sean and Patel, Shwetak},
	month = oct,
	year = {2013},
	pages = {237--244},
	file = {Chen et al. - 2013 - uTrack 3D input using two magnetic sensors.pdf:/Users/orsonxu/Zotero/storage/34WW8CVD/Chen et al. - 2013 - uTrack 3D input using two magnetic sensors.pdf:application/pdf},
}

@inproceedings{chen_finexus_2016,
	address = {San Jose California USA},
	title = {Finexus: {Tracking} {Precise} {Motions} of {Multiple} {Fingertips} {Using} {Magnetic} {Sensing}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Finexus},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858125},
	doi = {10.1145/2858036.2858125},
	abstract = {With the resurgence of head-mounted displays for virtual reality, users need new input devices that can accurately track their hands and fingers in motion. We introduce Finexus, a multipoint tracking system using magnetic field sensing. By instrumenting the fingertips with electromagnets, the system can track fine fingertip movements in real time using only four magnetic sensors. To keep the system robust to noise, we operate each electromagnet at a different frequency and leverage bandpass filters to distinguish signals attributed to individual sensing points. We develop a novel algorithm to efficiently calculate the 3D positions of multiple electromagnets from corresponding field strengths. In our evaluation, we report an average accuracy of 1.33 mm, as compared to results from an optical tracker. Our real-time implementation shows Finexus is applicable to a wide variety of human input tasks, such as writing in the air.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Chen, Ke-Yu and Patel, Shwetak N. and Keller, Sean},
	month = may,
	year = {2016},
	pages = {1504--1514},
	file = {Chen et al. - 2016 - Finexus Tracking Precise Motions of Multiple Fing.pdf:/Users/orsonxu/Zotero/storage/2G4UJWGH/Chen et al. - 2016 - Finexus Tracking Precise Motions of Multiple Fing.pdf:application/pdf},
}

@article{parizi_auraring_2019,
	title = {{AuraRing}: {Precise} {Electromagnetic} {Finger} {Tracking}},
	volume = {3},
	issn = {2474-9567},
	shorttitle = {{AuraRing}},
	url = {https://dl.acm.org/doi/10.1145/3369831},
	doi = {10.1145/3369831},
	abstract = {Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. We develop two different approaches to pose reconstruction—a first-principles iterative approach and a closed-form neural network approach. Notably, AuraRing requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power. CCS Concepts: • Human-centered computing → Interaction devices; Ubiquitous and mobile computing.},
	language = {en},
	number = {4},
	urldate = {2021-08-03},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Parizi, Farshid Salemi and Whitmire, Eric and Patel, Shwetak},
	month = dec,
	year = {2019},
	pages = {1--28},
	file = {Parizi et al. - 2019 - AuraRing Precise Electromagnetic Finger Tracking.pdf:/Users/orsonxu/Zotero/storage/II9CDW34/Parizi et al. - 2019 - AuraRing Precise Electromagnetic Finger Tracking.pdf:application/pdf},
}

@inproceedings{yang_magic_2012,
	address = {Cambridge, Massachusetts, USA},
	title = {Magic finger: always-available input through finger instrumentation},
	isbn = {978-1-4503-1580-7},
	shorttitle = {Magic finger},
	url = {http://dl.acm.org/citation.cfm?doid=2380116.2380137},
	doi = {10.1145/2380116.2380137},
	abstract = {We present Magic Finger, a small device worn on the fingertip, which supports always-available input. Magic Finger inverts the typical relationship between the finger and an interactive surface: with Magic Finger, we instrument the user’s finger itself, rather than the surface it is touching. Magic Finger senses touch through an optical mouse sensor, enabling any surface to act as a touch screen. Magic Finger also senses texture through a micro RGB camera, allowing contextual actions to be carried out based on the particular surface being touched. A technical evaluation shows that Magic Finger can accurately sense 22 textures with an accuracy of 98.9\%. We explore the interaction design space enabled by Magic Finger, and implement a number of novel interaction techniques that leverage its unique capabilities.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 25th annual {ACM} symposium on {User} interface software and technology - {UIST} '12},
	publisher = {ACM Press},
	author = {Yang, Xing-Dong and Grossman, Tovi and Wigdor, Daniel and Fitzmaurice, George},
	year = {2012},
	pages = {147},
	file = {Yang et al. - 2012 - Magic finger always-available input through finge.pdf:/Users/orsonxu/Zotero/storage/MBH32A6Y/Yang et al. - 2012 - Magic finger always-available input through finge.pdf:application/pdf},
}

@inproceedings{kienzle_lightring_2014,
	address = {Honolulu Hawaii USA},
	title = {{LightRing}: always-available {2D} input on any surface},
	isbn = {978-1-4503-3069-5},
	shorttitle = {{LightRing}},
	url = {https://dl.acm.org/doi/10.1145/2642918.2647376},
	doi = {10.1145/2642918.2647376},
	abstract = {We present LightRing, a wearable sensor in a ring form factor that senses the 2d location of a fingertip on any surface, independent of orientation or material. The device consists of an infrared proximity sensor for measuring finger flexion and a 1-axis gyroscope for measuring finger rotation. Notably, LightRing tracks subtle fingertip movements from the finger base without requiring instrumentation of other body parts or the environment. This keeps the normal hand function intact and allows for a socially acceptable appearance. We evaluate LightRing in a 2d pointing experiment in two scenarios: on a desk while sitting down, and on the leg while standing. Our results indicate that the device has potential to enable a variety of rich mobile input scenarios.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 27th annual {ACM} symposium on {User} interface software and technology},
	publisher = {ACM},
	author = {Kienzle, Wolf and Hinckley, Ken},
	month = oct,
	year = {2014},
	pages = {157--160},
	file = {Kienzle and Hinckley - 2014 - LightRing always-available 2D input on any surfac.pdf:/Users/orsonxu/Zotero/storage/JBLHSA3G/Kienzle and Hinckley - 2014 - LightRing always-available 2D input on any surfac.pdf:application/pdf},
}

@article{hu_fingertrak_2020,
	title = {{FingerTrak}: {Continuous} {3D} {Hand} {Pose} {Tracking} by {Deep} {Learning} {Hand} {Silhouettes} {Captured} by {Miniature} {Thermal} {Cameras} on {Wrist}},
	volume = {4},
	issn = {2474-9567},
	shorttitle = {{FingerTrak}},
	url = {https://dl.acm.org/doi/10.1145/3397306},
	doi = {10.1145/3397306},
	abstract = {In this paper, we present FingerTrak, a minimal-obtrusive wristband that enables continuous 3D finger tracking and hand pose estimation with four miniature thermal cameras mounted closely on a form-fitting wristband. FingerTrak explores the feasibility of continuously reconstructing the entire hand postures (20 finger joints positions) without the needs of seeing all fingers. We demonstrate that our system is able to estimate the entire hand posture by observing only the outline of the hand, i.e., hand silhouettes from the wrist using low-resolution (32 × 24) thermal cameras. A customized deep neural network is developed to learn to ”stitch” these multi-view images and estimate 20 joints positions in 3D space. Our user study with 11 participants shows that the system can achieve an average angular error of 6.46◦ when tested under the same background, and 8.06◦ when tested under a different background. FingerTrak also shows encouraging results with the re-mounting of the device and has the potential to reconstruct some of the complicated poses. We conclude this paper with further discussions of the opportunities and challenges of this technology. CCS Concepts: • Human-centered computing → Ubiquitous and mobile computing; Ubiquitous and mobile devices; Mobile devices.},
	language = {en},
	number = {2},
	urldate = {2021-08-03},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Hu, Fang and He, Peng and Xu, Songlin and Li, Yin and Zhang, Cheng},
	month = jun,
	year = {2020},
	pages = {1--24},
	file = {Hu et al. - 2020 - FingerTrak Continuous 3D Hand Pose Tracking by De.pdf:/Users/orsonxu/Zotero/storage/QJR958KK/Hu et al. - 2020 - FingerTrak Continuous 3D Hand Pose Tracking by De.pdf:application/pdf},
}

@article{akl_novel_2011,
	title = {A {Novel} {Accelerometer}-{Based} {Gesture} {Recognition} {System}},
	volume = {59},
	issn = {1941-0476},
	doi = {10.1109/TSP.2011.2165707},
	abstract = {In this paper, we address the problem of gesture recognition using the theory of random projection (RP) and by formulating the whole recognition problem as an ℓ1-minimization problem. The gesture recognition system operates primarily on data from a single 3-axis accelerometer and comprises two main stages: a training stage and a testing stage. For training, the system employs dynamic time warping as well as affinity propagation to create exemplars for each gesture while for testing, the system projects all candidate traces and also the unknown trace onto the same lower dimensional subspace for recognition. A dictionary of 18 gestures is defined and a database of over 3700 traces is created from seven subjects on which the system is tested and evaluated. To the best of our knowledge, our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition. The system achieves almost perfect user-dependent recognition, and mixed-user and user-independent recognition accuracies that are highly competitive with systems based on statistical methods and with the other accelerometer-based gesture recognition systems available in the literature.},
	number = {12},
	journal = {IEEE Transactions on Signal Processing},
	author = {Akl, Ahmad and Feng, Chen and Valaee, Shahrokh},
	month = dec,
	year = {2011},
	pages = {6197--6205},
	file = {Akl et al_2011_A Novel Accelerometer-Based Gesture Recognition System.pdf:/Users/orsonxu/Zotero/storage/G8CJ2H9Q/Akl et al_2011_A Novel Accelerometer-Based Gesture Recognition System.pdf:application/pdf;Akl et al_2011_A Novel Accelerometer-Based Gesture Recognition System.pdf:/Users/orsonxu/Zotero/storage/JWPGC3M6/Akl et al_2011_A Novel Accelerometer-Based Gesture Recognition System.pdf:application/pdf},
}

@inproceedings{wen_serendipity_2016,
	address = {San Jose California USA},
	title = {Serendipity: {Finger} {Gesture} {Recognition} using an {Off}-the-{Shelf} {Smartwatch}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Serendipity},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858466},
	doi = {10.1145/2858036.2858466},
	abstract = {Previous work on muscle activity sensing has leveraged specialized sensors such as electromyography and force sensitive resistors. While these sensors show great potential for detecting finger/hand gestures, they require additional hardware that adds to the cost and user discomfort. Past research has utilized sensors on commercial devices, focusing on recognizing gross hand gestures. In this work we present Serendipity, a new technique for recognizing unremarkable and fine-motor finger gestures using integrated motion sensors (accelerometer and gyroscope) in off-the-shelf smartwatches. Our system demonstrates the potential to distinguish 5 fine-motor gestures like pinching, tapping and rubbing fingers with an average f1-score of 87\%. Our work is the first to explore the feasibility of using solely motion sensors on everyday wearable devices to detect fine-grained gestures. This promising technology can be deployed today on current smartwatches and has the potential to be applied to cross-device interactions, or as a tool for research in fields involving finger and hand motion.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wen, Hongyi and Ramos Rojas, Julian and Dey, Anind K.},
	month = may,
	year = {2016},
	pages = {3847--3851},
	file = {Wen et al. - 2016 - Serendipity Finger Gesture Recognition using an O.pdf:/Users/orsonxu/Zotero/storage/HNHZXBBW/Wen et al. - 2016 - Serendipity Finger Gesture Recognition using an O.pdf:application/pdf},
}

@inproceedings{xu_finger-writing_2015,
	address = {Santa Fe New Mexico USA},
	title = {Finger-writing with {Smartwatch}: {A} {Case} for {Finger} and {Hand} {Gesture} {Recognition} using {Smartwatch}},
	isbn = {978-1-4503-3391-7},
	shorttitle = {Finger-writing with {Smartwatch}},
	url = {https://dl.acm.org/doi/10.1145/2699343.2699350},
	doi = {10.1145/2699343.2699350},
	abstract = {Smartwatch is becoming one of the most popular wearable device with many major smartphone manufacturers such as Samsung and Apple releasing their smartwatches recently. Apart from the ﬁtness applications, the smartwatch provides a rich user interface that has enabled many applications like instant messaging and email. Since the smartwatch is worn on the wrist, it introduces a unique opportunity to understand user’s arm, hand and possibly ﬁnger movements using its accelerometer and gyroscope sensors. Although user’s arm and hand gestures are likely to be identiﬁed with ease using the smartwatch sensors, it is not clear how much of user’s ﬁnger gestures can be recognized. In this paper, we show that motion energy measured at the smartwatch is suﬃcient to uniquely identify user’s hand and ﬁnger gestures. We identify essential features of accelerometer and gyroscope data that reﬂect the movements of tendons (passing through the wrist) when performing a ﬁnger or a hand gesture. With these features, we build a classiﬁer that can uniquely identify 37 (13 ﬁnger, 14 hand and 10 arm) gestures with an accuracy of 98\%. We further extend our gesture recognition to identify the characters written by the user with her index ﬁnger on a surface, and show that such ﬁnger-writing can also be accurately recognized with nearly 95\% accuracy. Our presented results will enable many novel applications like remote control and ﬁnger-writing-based input to devices using smartwatch.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 16th {International} {Workshop} on {Mobile} {Computing} {Systems} and {Applications}},
	publisher = {ACM},
	author = {Xu, Chao and Pathak, Parth H. and Mohapatra, Prasant},
	month = feb,
	year = {2015},
	pages = {9--14},
	file = {Xu et al. - 2015 - Finger-writing with Smartwatch A Case for Finger .pdf:/Users/orsonxu/Zotero/storage/QCRI6HWD/Xu et al. - 2015 - Finger-writing with Smartwatch A Case for Finger .pdf:application/pdf},
}

@inproceedings{mcintosh_sensir_2017,
	address = {Québec City QC Canada},
	title = {{SensIR}: {Detecting} {Hand} {Gestures} with a {Wearable} {Bracelet} using {Infrared} {Transmission} and {Reflection}},
	isbn = {978-1-4503-4981-9},
	shorttitle = {{SensIR}},
	url = {https://dl.acm.org/doi/10.1145/3126594.3126604},
	doi = {10.1145/3126594.3126604},
	abstract = {Gestures have become an important tool for natural interaction with computers and thus several wearables have been developed to detect hand gestures. However, many existing solutions are unsuitable for practical use due to low accuracy, high cost or poor ergonomics. We present SensIR, a bracelet that uses near-infrared sensing to infer hand gestures. The bracelet is composed of pairs of infrared emitters and receivers that are used to measure both the transmission and reﬂection of light through/off the wrist. SensIR improves the accuracy of existing infrared gesture sensing systems through the key idea of taking measurements with all possible combinations of emitters and receivers. Our study shows that SensIR is capable of detecting 12 discrete gestures with 93.3\% accuracy. SensIR has several advantages compared to other systems such as high accuracy, low cost, robustness against bad skin coupling and thin form-factor.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {McIntosh, Jess and Marzo, Asier and Fraser, Mike},
	month = oct,
	year = {2017},
	pages = {593--597},
	file = {McIntosh et al. - 2017 - SensIR Detecting Hand Gestures with a Wearable Br.pdf:/Users/orsonxu/Zotero/storage/PYQN8TF9/McIntosh et al. - 2017 - SensIR Detecting Hand Gestures with a Wearable Br.pdf:application/pdf},
}

@article{fukui_hand_2011,
	title = {Hand shape classification with a wrist contour sensor: development of a prototype device},
	abstract = {In this paper, we describe a novel sensor device which recognizes hand shapes using wrist contours. Although hand shapes can express various meanings with small gestures, utilization of hand shapes as an interface is rare in domestic use. That is because a concise recognition method has not been established. To recognize hand shapes anywhere with no stress on the user, we developed a wearable wrist contour sensor device and a recognition system. In the system, features, such as sum of gaps, were extracted from wrist contours. We conducted a classiﬁcation test of eight hand shapes, and realized approximately 70\% classiﬁcation rate.},
	language = {en},
	author = {Fukui, Rui and Watanabe, Masahiko and Gyota, Tomoaki and Shimosaka, Masamichi and Sato, Tomomasa},
	year = {2011},
	pages = {4},
	file = {Fukui et al. - Hand shape classification with a wrist contour sen.pdf:/Users/orsonxu/Zotero/storage/KL28DJF9/Fukui et al. - Hand shape classification with a wrist contour sen.pdf:application/pdf},
}

@inproceedings{georgi_recognizing_2015,
	address = {Lisbon, Portugal},
	title = {Recognizing {Hand} and {Finger} {Gestures} with {IMU} based {Motion} and {EMG} based {Muscle} {Activity} {Sensing}:},
	isbn = {978-989-758-069-7},
	shorttitle = {Recognizing {Hand} and {Finger} {Gestures} with {IMU} based {Motion} and {EMG} based {Muscle} {Activity} {Sensing}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005276900990108},
	doi = {10.5220/0005276900990108},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the {International} {Conference} on {Bio}-inspired {Systems} and {Signal} {Processing}},
	publisher = {SCITEPRESS - Science and and Technology Publications},
	author = {Georgi, Marcus and Amma, Christoph and Schultz, Tanja},
	year = {2015},
	pages = {99--108},
	file = {Georgi et al. - 2015 - Recognizing Hand and Finger Gestures with IMU base.pdf:/Users/orsonxu/Zotero/storage/GYDPURBU/Georgi et al. - 2015 - Recognizing Hand and Finger Gestures with IMU base.pdf:application/pdf},
}

@inproceedings{saponas_demonstrating_2008,
	address = {Florence, Italy},
	title = {Demonstrating the feasibility of using forearm electromyography for muscle-computer interfaces},
	isbn = {978-1-60558-011-1},
	url = {http://portal.acm.org/citation.cfm?doid=1357054.1357138},
	doi = {10.1145/1357054.1357138},
	abstract = {We explore the feasibility of muscle-computer interfaces (muCIs): an interaction methodology that directly senses and decodes human muscular activity rather than relying on physical device actuation or user actions that are externally visible or audible. As a first step towards realizing the muCI concept, we conducted an experiment to explore the potential of exploiting muscular sensing and processing technologies for muCIs. We present results demonstrating accurate gesture classification with an off-the-shelf electromyography (EMG) device. Specifically, using 10 sensors worn in a narrow band around the upper forearm, we were able to differentiate position and pressure of finger presses, as well as classify tapping and lifting gestures across all five fingers. We conclude with discussion of the implications of our results for future muCI designs.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceeding of the twenty-sixth annual {CHI} conference on {Human} factors in computing systems  - {CHI} '08},
	publisher = {ACM Press},
	author = {Saponas, T Scott and Tan, Desney S. and Morris, Dan and Balakrishnan, Ravin},
	year = {2008},
	pages = {515},
	file = {Saponas et al. - 2008 - Demonstrating the feasibility of using forearm ele.pdf:/Users/orsonxu/Zotero/storage/L4CEBPBK/Saponas et al. - 2008 - Demonstrating the feasibility of using forearm ele.pdf:application/pdf},
}

@inproceedings{sato_touche_2012,
	address = {Austin Texas USA},
	title = {Touché: enhancing touch interaction on humans, screens, liquids, and everyday objects},
	isbn = {978-1-4503-1015-4},
	shorttitle = {Touché},
	url = {https://dl.acm.org/doi/10.1145/2207676.2207743},
	doi = {10.1145/2207676.2207743},
	abstract = {Touché proposes a novel Swept Frequency Capacitive Sensing technique that can not only detect a touch event, but also recognize complex configurations of the human hands and body. Such contextual information significantly enhances touch interaction in a broad range of applications, from conventional touchscreens to unique contexts and materials. For example, in our explorations we add touch and gesture sensitivity to the human body and liquids. We demonstrate the rich capabilities of Touché with five example setups from different application domains and conduct experimental studies that show gesture classification accuracies of 99\% are achievable with our technology.},
	language = {en},
	urldate = {2021-08-03},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Sato, Munehiko and Poupyrev, Ivan and Harrison, Chris},
	month = may,
	year = {2012},
	pages = {483--492},
	file = {Sato et al. - 2012 - Touché enhancing touch interaction on humans, scr.pdf:/Users/orsonxu/Zotero/storage/MWLC7P9Q/Sato et al. - 2012 - Touché enhancing touch interaction on humans, scr.pdf:application/pdf},
}

@inproceedings{laput_sensing_2019,
	address = {Glasgow Scotland Uk},
	title = {Sensing {Fine}-{Grained} {Hand} {Activity} with {Smartwatches}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300568},
	doi = {10.1145/3290605.3300568},
	abstract = {Capturing ﬁne-grained hand activity could make computational experiences more powerful and contextually aware. Indeed, philosopher Immanuel Kant argued, "the hand is the visible part of the brain." However, most prior work has focused on detecting whole-body activities, such as walking, running and bicycling. In this work, we explore the feasibility of sensing hand activities from commodity smartwatches, which are the most practical vehicle for achieving this vision. Our investigations started with a 50 participant, in-the-wild study, which captured hand activity labels over nearly 1000 worn hours. We then studied this data to scope our research goals and inform our technical approach. We conclude with a second, in-lab study that evaluates our classiﬁcation stack, demonstrating 95.2\% accuracy across 25 hand activities. Our work highlights an underutilized, yet highly complementary contextual channel that could unlock a wide range of promising applications.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Laput, Gierad and Harrison, Chris},
	month = may,
	year = {2019},
	pages = {1--13},
	file = {Laput and Harrison - 2019 - Sensing Fine-Grained Hand Activity with Smartwatch.pdf:/Users/orsonxu/Zotero/storage/KJ3B5J2R/Laput and Harrison - 2019 - Sensing Fine-Grained Hand Activity with Smartwatch.pdf:application/pdf},
}

@article{anthony_lightweight_2010,
	title = {A {Lightweight} {Multistroke} {Recognizer} for {User} {Interface} {Prototypes}},
	volume = {2010},
	abstract = {With the expansion of pen- and touch-based computing, new user interface prototypes may incorporate stroke gestures. Many gestures comprise multiple strokes, but building state-of-the-art multistroke gesture recognizers is nontrivial and time-consuming. Luckily, user interface prototypes often do not require state-ofthe-art recognizers that are general and maintainable, due to the simpler nature of most user interface gestures. To enable easy incorporation of multistroke recognition in user interface prototypes, we present \$N, a lightweight, concise multistroke recognizer that uses only simple geometry and trigonometry. A full pseudocode listing is given as an appendix.},
	language = {en},
	journal = {Proceedings of Graphics Interface 2010},
	author = {Anthony, Lisa and Wobbrock, Jacob O},
	year = {2010},
	pages = {8},
	file = {Anthony and Wobbrock - A Lightweight Multistroke Recognizer for User Inte.pdf:/Users/orsonxu/Zotero/storage/SF655I5M/Anthony and Wobbrock - A Lightweight Multistroke Recognizer for User Inte.pdf:application/pdf},
}

@article{merrill_personalization_2005,
	title = {Personalization, {Expressivity}, and {Learnability} of an {Implicit} {Mapping} {Strategy} for {Physical} {Interfaces}},
	abstract = {We present a new model for configuring the connections between user input and system output in a physical interface with diverse sensor degrees of freedom across several input modalities. Our system allows a user to demonstrate input gestures and manipulations directly to the system, teaching it the desired mappings by example. We developed a musical control application in which userdefined gestures and user-assigned manipulations trigger and modify sounds. The effectiveness of our system was tested by experimentally comparing our user-definable system to a similar, pre-configured version. The results suggest that users prefer to actively configure a physical interface to having expertly-configured presets. In addition, we propose our model as a more general mapping discovery tool for physical interface designers.},
	language = {en},
	journal = {In Proceedings of the CHI Conference on Human Factors in Computing Systems, Extended Abstracts},
	author = {Merrill, David J and Paradiso, Joseph A},
	year = {2005},
	pages = {9},
	file = {Merrill and Paradiso - Personalization, Expressivity, and Learnability of.pdf:/Users/orsonxu/Zotero/storage/RWI8Z2TI/Merrill and Paradiso - Personalization, Expressivity, and Learnability of.pdf:application/pdf},
}

@inproceedings{ouyang_bootstrapping_2012,
	address = {Austin Texas USA},
	title = {Bootstrapping personal gesture shortcuts with the wisdom of the crowd and handwriting recognition},
	isbn = {978-1-4503-1015-4},
	url = {https://dl.acm.org/doi/10.1145/2207676.2208695},
	doi = {10.1145/2207676.2208695},
	abstract = {Personal user-defined gesture shortcuts have shown great potential for accessing the ever-growing amount of data and computing power on touchscreen mobile devices. However, their lack of scalability is a major challenge for their wide adoption. In this paper, we present Gesture Marks, a novel approach to touch-gesture interaction that allows a user to access applications and websites using gestures without having to define them first. It offers two distinctive solutions to address the problem of scalability. First, it leverages the “wisdom of the crowd”, a continually evolving library of gesture shortcuts that are collected from the user population, to infer the meaning of gestures that a user never defined himself. Second, it combines an extensible template-based gesture recognizer with a specialized handwriting recognizer to even better address handwriting-based gestures, which are a common form of gesture shortcut. These approaches effectively bootstrap a user’s personal gesture library, alleviating the need to define most gestures manually. Our work was motivated and validated via a series of user studies, and the findings from these studies add to the body of knowledge on gesture-based interaction.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ouyang, Tom and Li, Yang},
	month = may,
	year = {2012},
	pages = {2895--2904},
	file = {Ouyang and Li - 2012 - Bootstrapping personal gesture shortcuts with the .pdf:/Users/orsonxu/Zotero/storage/MN9FFY2D/Ouyang and Li - 2012 - Bootstrapping personal gesture shortcuts with the .pdf:application/pdf},
}

@article{liu_uwave_2009,
	title = {{uWave}: {Accelerometer}-based personalized gesture recognition and its applications},
	volume = {5},
	issn = {15741192},
	shorttitle = {{uWave}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119209000674},
	doi = {10.1016/j.pmcj.2009.07.007},
	abstract = {The proliferation of accelerometers on consumer electronics has brought an opportunity for interaction based on gestures. We present uWave, an efficient recognition algorithm for such interaction using a single three-axis accelerometer. uWave requires a single training sample for each gesture pattern and allows users to employ personalized gestures. We evaluate uWave using a large gesture library with over 4000 samples for eight gesture patterns collected from eight users over one month. uWave achieves 98.6\% accuracy, competitive with statistical methods that require significantly more training samples. We also present applications of uWave in gesture-based user authentication and interaction with 3D mobile user interfaces. In particular, we report a series of user studies that evaluates the feasibility and usability of lightweight user authentication. Our evaluation shows both the strength and limitations of gesture-based user authentication.},
	language = {en},
	number = {6},
	urldate = {2021-08-04},
	journal = {Pervasive and Mobile Computing},
	author = {Liu, Jiayang and Zhong, Lin and Wickramasuriya, Jehan and Vasudevan, Venu},
	month = dec,
	year = {2009},
	pages = {657--675},
	file = {Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf:/Users/orsonxu/Zotero/storage/2ENTRRTK/Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf:application/pdf},
}

@article{mckenna_comparison_2004,
	title = {A comparison of skin history and trajectory-based representation schemes for the recognition of user-specified gestures},
	volume = {37},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320303003704},
	doi = {10.1016/j.patcog.2003.09.007},
	abstract = {Gesture recognition error rates and the qualitative nature of the errors made are heavily in uenced by the choice of visual representation. A direct empirical comparison of two contrasting approaches, namely trajectory- and history-based representation, is presented. Skin colour is used as a common visual cue and recognition is based on hidden Markov models, moment features and normalised template matching. Two novel representation schemes are proposed and evaluated: (i) skin history images and (ii) composite history images which represent occluded motion. Results are reported for an application in which able-bodied and disabled subjects specify their own gesture vocabularies.},
	language = {en},
	number = {5},
	urldate = {2021-08-04},
	journal = {Pattern Recognition},
	author = {McKenna, Stephen J. and Morrison, Kenny},
	month = may,
	year = {2004},
	pages = {999--1009},
	file = {McKenna and Morrison - 2004 - A comparison of skin history and trajectory-based .pdf:/Users/orsonxu/Zotero/storage/Y68UWQCN/McKenna and Morrison - 2004 - A comparison of skin history and trajectory-based .pdf:application/pdf},
}

@inproceedings{anthony_analyzing_2013,
	address = {Paris France},
	title = {Analyzing user-generated youtube videos to understand touchscreen use by people with motor impairments},
	isbn = {978-1-4503-1899-0},
	url = {https://dl.acm.org/doi/10.1145/2470654.2466158},
	doi = {10.1145/2470654.2466158},
	abstract = {Most work on the usability of touchscreen interaction for people with motor impairments has focused on lab studies with relatively few participants and small cross-sections of the population. To develop a richer characterization of use, we turned to a previously untapped source of data: YouTube videos. We collected and analyzed 187 noncommercial videos uploaded to YouTube that depicted a person with a physical disability interacting with a mainstream mobile touchscreen device. We coded the videos along a range of dimensions to characterize the interaction, the challenges encountered, and the adaptations being adopted in daily use. To complement the video data, we also invited the video uploaders to complete a survey on their ongoing use of touchscreen technology. Our findings show that, while many people with motor impairments find these devices empowering, accessibility issues still exist. In addition to providing implications for more accessible touchscreen design, we reflect on the application of usergenerated content to study user interface design.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Anthony, Lisa and Kim, YooJin and Findlater, Leah},
	month = apr,
	year = {2013},
	pages = {1223--1232},
	file = {Anthony et al. - 2013 - Analyzing user-generated youtube videos to underst.pdf:/Users/orsonxu/Zotero/storage/4FMLY4IB/Anthony et al. - 2013 - Analyzing user-generated youtube videos to underst.pdf:application/pdf},
}

@inproceedings{nacenta_memorability_2013-1,
	address = {Paris France},
	title = {Memorability of pre-designed and user-defined gesture sets},
	isbn = {978-1-4503-1899-0},
	url = {https://dl.acm.org/doi/10.1145/2470654.2466142},
	doi = {10.1145/2470654.2466142},
	abstract = {We studied the memorability of free-form gesture sets for invoking actions. We compared three types of gesture sets: user-defined gesture sets, gesture sets designed by the authors, and random gesture sets in three studies with 33 participants in total. We found that user-defined gestures are easier to remember, both immediately after creation and on the next day (up to a 24\% difference in recall rate compared to pre-designed gestures). We also discovered that the differences between gesture sets are mostly due to association errors (rather than gesture form errors), that participants prefer user-defined sets, and that they think user-defined gestures take less time to learn. Finally, we contribute a qualitative analysis of the tradeoffs involved in gesture type selection and share our data and a video corpus of 66 gestures for replicability and further analysis.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Nacenta, Miguel A. and Kamber, Yemliha and Qiang, Yizhou and Kristensson, Per Ola},
	month = apr,
	year = {2013},
	pages = {1099--1108},
	file = {Nacenta et al. - 2013 - Memorability of pre-designed and user-defined gest.pdf:/Users/orsonxu/Zotero/storage/CU5KNDMJ/Nacenta et al. - 2013 - Memorability of pre-designed and user-defined gest.pdf:application/pdf},
}

@inproceedings{wobbrock_user-defined_2009-1,
	address = {Boston MA USA},
	title = {User-defined gestures for surface computing},
	isbn = {978-1-60558-246-7},
	url = {https://dl.acm.org/doi/10.1145/1518701.1518866},
	doi = {10.1145/1518701.1518866},
	abstract = {Many surface computing prototypes have employed gestures created by system designers. Although such gestures are appropriate for early investigations, they are not necessarily reflective of user behavior. We present an approach to designing tabletop gestures that relies on eliciting gestures from non-technical users by first portraying the effect of a gesture, and then asking users to perform its cause. In all, 1080 gestures from 20 participants were logged, analyzed, and paired with think-aloud data for 27 commands performed with 1 and 2 hands. Our findings indicate that users rarely care about the number of fingers they employ, that one hand is preferred to two, that desktop idioms strongly influence users’ mental models, and that some commands elicit little gestural agreement, suggesting the need for on-screen widgets. We also present a complete user-defined gesture set, quantitative agreement scores, implications for surface technology, and a taxonomy of surface gestures. Our results will help designers create better gesture sets informed by user behavior.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wobbrock, Jacob O. and Morris, Meredith Ringel and Wilson, Andrew D.},
	month = apr,
	year = {2009},
	pages = {1083--1092},
	file = {Wobbrock et al. - 2009 - User-defined gestures for surface computing.pdf:/Users/orsonxu/Zotero/storage/ZT3ZKU5K/Wobbrock et al. - 2009 - User-defined gestures for surface computing.pdf:application/pdf},
}

@article{piumsomboon_user-defined_2013-1,
	title = {User-{Defined} {Gestures} for {Augmented} {Reality}},
	abstract = {Recently there has been an increase in research towards using hand gestures for interaction in the field of Augmented Reality (AR). These works have primarily focused on researcher designed gestures, while little is known about user preference and behavior for gestures in AR. In this paper, we present our guessability study for hand gestures in AR in which 800 gestures were elicited for 40 selected tasks from 20 participants. Using the agreement found among gestures, a user-defined gesture set was created to guide designers to achieve consistent user-centered gestures in AR. Wobbrock’s surface taxonomy has been extended to cover dimensionalities in AR and with it, characteristics of collected gestures have been derived. Common motifs which arose from the empirical findings were applied to obtain a better understanding of users’ thought and behavior. This work aims to lead to consistent user-centered designed gestures in AR.},
	language = {en},
	journal = {IFIP Conference on Human-Computer Interaction},
	author = {Piumsomboon, Thammathip and Clark, Adrian and Billinghurst, Mark and Cockburn, Andy},
	year = {2013},
	pages = {18},
	file = {Piumsomboon et al. - User-Defined Gestures for Augmented Reality.pdf:/Users/orsonxu/Zotero/storage/9HYYKEIW/Piumsomboon et al. - User-Defined Gestures for Augmented Reality.pdf:application/pdf},
}

@inproceedings{ruiz_user-defined_2011-1,
	address = {Vancouver BC Canada},
	title = {User-defined motion gestures for mobile interaction},
	isbn = {978-1-4503-0228-9},
	url = {https://dl.acm.org/doi/10.1145/1978942.1978971},
	doi = {10.1145/1978942.1978971},
	abstract = {Modern smartphones contain sophisticated sensors to monitor three-dimensional movement of the device. These sensors permit devices to recognize motion gestures—deliberate movements of the device by end-users to invoke commands. However, little is known about best-practices in motion gesture design for the mobile computing paradigm. To address this issue, we present the results of a guessability study that elicits end-user motion gestures to invoke commands on a smartphone device. We demonstrate that consensus exists among our participants on parameters of movement and on mappings of motion gestures onto commands. We use this consensus to develop a taxonomy for motion gestures and to specify an end-user inspired motion gesture set. We highlight the implications of this work to the design of smartphone applications and hardware. Finally, we argue that our results influence best practices in design for all gestural interfaces.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ruiz, Jaime and Li, Yang and Lank, Edward},
	month = may,
	year = {2011},
	pages = {197--206},
	file = {Ruiz et al. - 2011 - User-defined motion gestures for mobile interactio.pdf:/Users/orsonxu/Zotero/storage/WNHEWKIK/Ruiz et al. - 2011 - User-defined motion gestures for mobile interactio.pdf:application/pdf},
}

@inproceedings{lu_gesture_2012,
	address = {Austin Texas USA},
	title = {Gesture coder: a tool for programming multi-touch gestures by demonstration},
	isbn = {978-1-4503-1015-4},
	shorttitle = {Gesture coder},
	url = {https://dl.acm.org/doi/10.1145/2207676.2208693},
	doi = {10.1145/2207676.2208693},
	abstract = {Multi-touch gestures have become popular on a wide range of touchscreen devices, but the programming of these gestures remains an art. It is time-consuming and errorprone for a developer to handle the complicated touch state transitions that result from multiple fingers and their simultaneous movements. In this paper, we present Gesture Coder, which by learning from a few examples given by the developer automatically generates code that recognizes multi-touch gestures, tracks their state changes and invokes corresponding application actions. Developers can easily test the generated code in Gesture Coder, refine it by adding more examples and, once they are satisfied with its performance, integrate the code into their applications. We evaluated our learning algorithm exhaustively with various conditions over a large set of noisy data. Our results show that it is sufficient for rapid prototyping and can be improved with higher quality and more training data. We also evaluated Gesture Coder’s usability through a withinsubject study in which we asked participants to implement a set of multi-touch interactions with and without Gesture Coder. The results show overwhelmingly that Gesture Coder significantly lowers the threshold of programming multi-touch gestures.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lü, Hao and Li, Yang},
	month = may,
	year = {2012},
	pages = {2875--2884},
	file = {Lü and Li - 2012 - Gesture coder a tool for programming multi-touch .pdf:/Users/orsonxu/Zotero/storage/Q6QIAWPW/Lü and Li - 2012 - Gesture coder a tool for programming multi-touch .pdf:application/pdf},
}

@inproceedings{bau_octopocus_2008,
	address = {Monterey, CA, USA},
	title = {{OctoPocus}: a dynamic guide for learning gesture-based command sets},
	isbn = {978-1-59593-975-3},
	shorttitle = {{OctoPocus}},
	url = {http://portal.acm.org/citation.cfm?doid=1449715.1449724},
	doi = {10.1145/1449715.1449724},
	abstract = {We describe OctoPocus, an example of a dynamic guide that combines on-screen feedforward and feedback to help users learn, execute and remember gesture sets. OctoPocus can be applied to a wide range of single-stroke gestures and recognition algorithms and helps users progress smoothly from novice to expert performance. We provide an analysis of the design space and describe the results of two experiments that show that OctoPocus is significantly faster and improves learning of arbitrary gestures, compared to conventional Help menus. It can also be adapted to a markbased gesture set, significantly improving input time compared to a two-level, four-item Hierarchical Marking menu.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the 21st annual {ACM} symposium on {User} interface software and technology - {UIST} '08},
	publisher = {ACM Press},
	author = {Bau, Olivier and Mackay, Wendy E.},
	year = {2008},
	pages = {37},
	file = {Bau and Mackay - 2008 - OctoPocus a dynamic guide for learning gesture-ba.pdf:/Users/orsonxu/Zotero/storage/23ZARACC/Bau and Mackay - 2008 - OctoPocus a dynamic guide for learning gesture-ba.pdf:application/pdf},
}

@article{avrahami_guided_2001,
	title = {Guided {Gesture} {Support} in the {Paper} {PDA}},
	abstract = {Ordinary paper offers properties of readability, fluidity, flexibility, cost, and portability that current electronic devices are often hard pressed to match. In fact, a lofty goal for many interactive systems is to be "as easy to use as pencil and paper". However, the static nature of paper does not support a number of capabilities, such as search and hyperlinking that an electronic device can provide. The Paper PDA project explores ways in which hybrid paper electronic interfaces can bring some of the capabilities of the electronic medium to interactions occurring on real paper. Key to this effort is the invention of on-paper interaction techniques which retain the flexibility and fluidity of normal pen and paper, but which are structured enough to allow robust interpretation and processing in the digital world. This paper considers the design of a class of simple printed templates that allow users to make common marks in a fluid fashion, and allow additional gestures to be invented by the users to meet their needs, but at the same time encourages marks that are quite easy to recognize.},
	language = {en},
	journal = {Proceedings of the 14th annual ACM symposium on User interface software and technology},
	author = {Avrahami, Daniel and Hudson, Scott E and Moran, Thomas P and Williams, Brian D},
	year = {2001},
	pages = {2},
	file = {Avrahami et al. - Guided Gesture Support in the Paper PDA.pdf:/Users/orsonxu/Zotero/storage/M9DRP7GY/Avrahami et al. - Guided Gesture Support in the Paper PDA.pdf:application/pdf},
}

@inproceedings{bigdelou_adaptive_2012,
	address = {Lisbon, Portugal},
	title = {An adaptive solution for intra-operative gesture-based human-machine interaction},
	isbn = {978-1-4503-1048-2},
	url = {http://dl.acm.org/citation.cfm?doid=2166966.2166981},
	doi = {10.1145/2166966.2166981},
	abstract = {Computerized medical systems play a vital role in the operating room, however, sterility requirements and interventional workﬂow often make interaction with these devices challenging for surgeons. Typical solutions, such as delegating physical control of keyboard and mouse to assistants, add an undesirable level of indirection. We present a touchless, gesture-based interaction framework for the operating room that lets surgeons deﬁne a personalized set of gestures for controlling arbitrary medical computerized systems. Instead of using cameras for capturing gestures, we rely on a few wireless inertial sensors, placed on the arms of the surgeon, eliminating the dependence on illumination and line-ofsight. A discriminative gesture recognition approach based on kernel regression allows us to simultaneously classify performed gestures and to track the relative spatial pose within each gesture, giving surgeons ﬁne-grained control of continuous parameters. An extensible software architecture enables a dynamic association of learned gestures to arbitrary intraoperative computerized systems. Our experiments illustrate the performance of our approach and encourage its practical applicability.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the 2012 {ACM} international conference on {Intelligent} {User} {Interfaces} - {IUI} '12},
	publisher = {ACM Press},
	author = {Bigdelou, Ali and Schwarz, Loren and Navab, Nassir},
	year = {2012},
	pages = {75},
	file = {Bigdelou et al. - 2012 - An adaptive solution for intra-operative gesture-b.pdf:/Users/orsonxu/Zotero/storage/II6ES2KF/Bigdelou et al. - 2012 - An adaptive solution for intra-operative gesture-b.pdf:application/pdf},
}

@article{doring_gestural_2011,
	title = {Gestural interaction on the steering wheel: reducing the visual demand},
	abstract = {Cars offer an increasing number of infotainment systems as well as comfort functions that can be controlled by the driver. In our research, we investigate new interaction techniques that aim to make it easier to interact with these systems while driving. We suggest utilizing the steering wheel as an additional interaction surface. In this paper, we present two user studies conducted with a working prototype of a multi-touch steering wheel. In the first, we developed a user-defined steering wheel gesture set, and in the second, we applied the identified gestures and compared their application to conventional user interaction with infotainment systems in terms of driver distraction. The main outcome was that driver's visual demand is reduced significantly by using gestural interaction on the multi-touch steering wheel.},
	language = {en},
	author = {Döring, Tanja and Kern, Dagmar and Marshall, Paul and Pfeiffer, Max and Schöning, Johannes and Gruhn, Volker and Schmidt, Albrecht},
	year = {2011},
	pages = {10},
	file = {Döring et al. - 2011 - Gestural interaction on the steering wheel reduci.pdf:/Users/orsonxu/Zotero/storage/MGHGHVEW/Döring et al. - 2011 - Gestural interaction on the steering wheel reduci.pdf:application/pdf},
}

@article{mezari_easily_2018,
	title = {An {Easily} {Customized} {Gesture} {Recognizer} for {Assisted} {Living} {Using} {Commodity} {Mobile} {Devices}},
	volume = {2018},
	issn = {2040-2295, 2040-2309},
	url = {https://www.hindawi.com/journals/jhe/2018/3180652/},
	doi = {10.1155/2018/3180652},
	abstract = {Automatic gesture recognition is an important field in the area of human-computer interaction. Until recently, the main approach to gesture recognition was based mainly on real time video processing. The objective of this work is to propose the utilization of commodity smartwatches for such purpose. Smartwatches embed accelerometer sensors, and they are endowed with wireless communication capabilities (primarily Bluetooth), so as to connect with mobile phones on which gesture recognition algorithms may be executed. The algorithmic approach proposed in this paper accepts as the input readings from the smartwatch accelerometer sensors and processes them on the mobile phone. As a case study, the gesture recognition application was developed for Android devices and the Pebble smartwatch. This application allows the user to define the set of gestures and to train the system to recognize them. Three alternative methodologies were implemented and evaluated using a set of six 3-D natural gestures. All the reported results are quite satisfactory, while the method based on SAX (Symbolic Aggregate approXimation) was proven the most efficient.},
	language = {en},
	urldate = {2021-08-04},
	journal = {Journal of Healthcare Engineering},
	author = {Mezari, Antigoni and Maglogiannis, Ilias},
	month = jul,
	year = {2018},
	pages = {1--12},
	file = {Mezari and Maglogiannis - 2018 - An Easily Customized Gesture Recognizer for Assist.pdf:/Users/orsonxu/Zotero/storage/99WD3ATS/Mezari and Maglogiannis - 2018 - An Easily Customized Gesture Recognizer for Assist.pdf:application/pdf},
}

@article{lou_personalized_2017,
	title = {Personalized gesture interactions for cyber-physical smart-home environments},
	volume = {60},
	issn = {1674-733X, 1869-1919},
	url = {http://link.springer.com/10.1007/s11432-015-1014-7},
	doi = {10.1007/s11432-015-1014-7},
	abstract = {A gesture-based interaction system for smart homes is a part of a complex cyber-physical environment, for which researchers and developers need to address major challenges in providing personalized gesture interactions. However, current research eﬀorts have not tackled the problem of personalized gesture recognition that often involves user identiﬁcation. To address this problem, we propose in this work a new event-driven service-oriented framework called gesture services for cyber-physical environments (GS-CPE) that extends the architecture of our previous work gesture proﬁle for web services (GPWS). To provide user identiﬁcation functionality, GS-CPE introduces a two-phase cascading gesture password recognition algorithm for gesture-based user identiﬁcation using a two-phase cascading classiﬁer with the hidden Markov model and the Golden Section Search, which achieves an accuracy rate of 96.2\% with a small training dataset. To support personalized gesture interaction, an enhanced version of the Dynamic Time Warping algorithm with multiple gestural input sources and dynamic template adaptation support is implemented. Our experimental results demonstrate the performance of the algorithm can achieve an average accuracy rate of 98.5\% in practical scenarios. Comparison results reveal that GS-CPE has faster response time and higher accuracy rate than other gesture interaction systems designed for smart-home environments.},
	language = {en},
	number = {7},
	urldate = {2021-08-04},
	journal = {Science China Information Sciences},
	author = {Lou, Yihua and Wu, Wenjun and Vatavu, Radu-Daniel and Tsai, Wei-Tek},
	month = jul,
	year = {2017},
	pages = {072104},
	file = {Lou et al. - 2017 - Personalized gesture interactions for cyber-physic.pdf:/Users/orsonxu/Zotero/storage/I9NM2BST/Lou et al. - 2017 - Personalized gesture interactions for cyber-physic.pdf:application/pdf},
}

@article{wu_adaptive_2018,
	title = {Adaptive {Feature} {Mapping} for {Customizing} {Deep} {Learning} {Based} {Facial} {Expression} {Recognition} {Model}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2805861},
	abstract = {Automated facial expression recognition can greatly improve the human-machine interface. The machine can provide better and more personalized services when it knows the human's emotion. This kind of improvement is an important progress in this artificial intelligence era. Many deep learning approaches have been applied in recent years due to their outstanding recognition accuracy after training with large amounts of data. The performance is limited, however, by the specific environmental conditions and variations in different persons involved. Hence, this paper addresses the issue of how to customize the generic model without label information from the testing samples. Weighted Center Regression Adaptive Feature Mapping (W-CR-AFM) is mainly proposed to transform the feature distribution of testing samples into that of trained samples. By means of minimizing the error between each feature of testing sample and the center of the most relevant category, W-CR-AFM can bring the features of testing samples around the decision boundary to the centers of expression categories; therefore, their predicted labels can be corrected. When the model which is tuned by W-CR-AFM is tested on extended Cohn-Kanade (CK+), Radboud Faces database, and Amsterdam dynamic facial expression set, our approach can improve the recognition accuracy by about 3.01\%, 0.49\%, and 5.33\%, respectively. Compared to the competing deep learning architectures with the same training data, our approach shows the better performance.},
	journal = {IEEE Access},
	author = {Wu, Bing-Fei and Lin, Chun-Hsien},
	year = {2018},
	keywords = {Face recognition, Feature extraction, Machine learning, computer vision, Training, Training data, Cross domain adaption, Databases, facial expression recognition, image processing, pattern recognition, Testing},
	pages = {12451--12461},
	file = {IEEE Xplore Abstract Record:/Users/orsonxu/Zotero/storage/GNPYMN62/8291717.html:text/html;IEEE Xplore Full Text PDF:/Users/orsonxu/Zotero/storage/88KLXUPD/Wu and Lin - 2018 - Adaptive Feature Mapping for Customizing Deep Lear.pdf:application/pdf},
}

@inproceedings{goel_walktype_2012,
	address = {Austin Texas USA},
	title = {{WalkType}: using accelerometer data to accomodate situational impairments in mobile touch screen text entry},
	isbn = {978-1-4503-1015-4},
	shorttitle = {{WalkType}},
	url = {https://dl.acm.org/doi/10.1145/2207676.2208662},
	doi = {10.1145/2207676.2208662},
	abstract = {The lack of tactile feedback on touch screens makes typing difficult, a challenge exacerbated when situational impairments like walking vibration and divided attention arise in mobile settings. We introduce WalkType, an adaptive text entry system that leverages the mobile device’s built-in tri-axis accelerometer to compensate for extraneous movement while walking. WalkType’s classification model uses the displacement and acceleration of the device, and inference about the user’s footsteps. Additionally, WalkType models finger-touch location and finger distance traveled on the screen, features that increase overall accuracy regardless of movement. The final model was built on typing data collected from 16 participants. In a study comparing WalkType to a control condition, WalkType reduced uncorrected errors by 45.2\% and increased typing speed by 12.9\% for walking participants.},
	language = {en},
	urldate = {2021-08-08},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Goel, Mayank and Findlater, Leah and Wobbrock, Jacob},
	month = may,
	year = {2012},
	pages = {2687--2696},
	file = {Goel et al. - 2012 - WalkType using accelerometer data to accomodate s.pdf:/Users/orsonxu/Zotero/storage/C2GFELVV/Goel et al. - 2012 - WalkType using accelerometer data to accomodate s.pdf:application/pdf},
}

@inproceedings{sears_when_2003,
	title = {When computers fade: {Pervasive} computing and situationally-induced impairments and disabilities},
	volume = {2},
	booktitle = {{HCI} international},
	author = {Sears, Andrew and Lin, Min and Jacko, Julie and Xiao, Yan},
	year = {2003},
	pages = {1298--1302},
}

@incollection{hart_development_1988,
	title = {Development of {NASA}-{TLX} ({Task} {Load} {Index}): {Results} of {Empirical} and {Theoretical} {Research}},
	volume = {52},
	isbn = {978-0-444-70388-0},
	shorttitle = {Development of {NASA}-{TLX} ({Task} {Load} {Index})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166411508623869},
	abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload.},
	language = {en},
	urldate = {2021-08-08},
	booktitle = {Advances in {Psychology}},
	publisher = {Elsevier},
	author = {Hart, Sandra G. and Staveland, Lowell E.},
	year = {1988},
	pages = {139--183},
	file = {Hart and Staveland - 1988 - Development of NASA-TLX (Task Load Index) Results.pdf:/Users/orsonxu/Zotero/storage/8FUC5K9P/Hart and Staveland - 1988 - Development of NASA-TLX (Task Load Index) Results.pdf:application/pdf},
}

@article{bangor_empirical_2008,
	title = {An {Empirical} {Evaluation} of the {System} {Usability} {Scale}},
	volume = {24},
	issn = {1044-7318, 1532-7590},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10447310802205776},
	doi = {10.1080/10447310802205776},
	language = {en},
	number = {6},
	urldate = {2021-08-08},
	journal = {International Journal of Human-Computer Interaction},
	author = {Bangor, Aaron and Kortum, Philip T. and Miller, James T.},
	month = jul,
	year = {2008},
	pages = {574--594},
	file = {Bangor et al. - 2008 - An Empirical Evaluation of the System Usability Sc.pdf:/Users/orsonxu/Zotero/storage/W3SYCTDA/Bangor et al. - 2008 - An Empirical Evaluation of the System Usability Sc.pdf:application/pdf},
}

@article{caramiaux_adaptive_2015,
	title = {Adaptive {Gesture} {Recognition} with {Variation} {Estimation} for {Interactive} {Systems}},
	volume = {4},
	issn = {2160-6455, 2160-6463},
	url = {https://dl.acm.org/doi/10.1145/2643204},
	doi = {10.1145/2643204},
	abstract = {This article presents a gesture recognition/adaptation system for human--computer interaction applications that goes beyond activity classification and that, as a complement to gesture labeling, characterizes the movement execution. We describe a template-based recognition method that simultaneously aligns the input gesture to the templates using a Sequential Monte Carlo inference technique. Contrary to standard template-based methods based on dynamic programming, such as Dynamic Time Warping, the algorithm has an adaptation process that tracks gesture variation in real time. The method continuously updates, during execution of the gesture, the estimated parameters and recognition results, which offers key advantages for continuous human--machine interaction. The technique is evaluated in several different ways: Recognition and early recognition are evaluated on 2D onscreen pen gestures; adaptation is assessed on synthetic data; and both early recognition and adaptation are evaluated in a user study involving 3D free-space gestures. The method is robust to noise, and successfully adapts to parameter variation. Moreover, it performs recognition as well as or better than nonadapting offline template-based methods.},
	language = {en},
	number = {4},
	urldate = {2021-08-24},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Caramiaux, Baptiste and Montecchio, Nicola and Tanaka, Atau and Bevilacqua, Frédéric},
	month = jan,
	year = {2015},
	pages = {1--34},
	file = {Caramiaux et al. - 2015 - Adaptive Gesture Recognition with Variation Estima.pdf:/Users/orsonxu/Zotero/storage/RDQGAG2U/Caramiaux et al. - 2015 - Adaptive Gesture Recognition with Variation Estima.pdf:application/pdf},
}

@article{caramiaux_understanding_2015,
	title = {Understanding {Gesture} {Expressivity} through {Muscle} {Sensing}},
	volume = {21},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/2687922},
	doi = {10.1145/2687922},
	abstract = {Expressivity is a visceral capacity of the human body. To understand what makes a gesture expressive, we need to consider not only its spatial placement and orientation but also its dynamics and the mechanisms enacting them. We start by defining gesture and gesture expressivity, and then we present fundamental aspects of muscle activity and ways to capture information through electromyography and mechanomyography. We present pilot studies that inspect the ability of users to control spatial and temporal variations of 2D shapes and that use muscle sensing to assess expressive information in gesture execution beyond space and time. This leads us to the design of a study that explores the notion of gesture power in terms of control and sensing. Results give insights to interaction designers to go beyond simplistic gestural interaction, towards the design of interactions that draw on nuances of expressive gesture.},
	language = {en},
	number = {6},
	urldate = {2021-08-24},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Caramiaux, Baptiste and Donnarumma, Marco and Tanaka, Atau},
	month = jan,
	year = {2015},
	pages = {1--26},
	file = {Caramiaux et al. - 2015 - Understanding Gesture Expressivity through Muscle .pdf:/Users/orsonxu/Zotero/storage/ZTG9YXU4/Caramiaux et al. - 2015 - Understanding Gesture Expressivity through Muscle .pdf:application/pdf},
}

@incollection{escalera_gesture_2017,
	address = {Cham},
	title = {The {Gesture} {Recognition} {Toolkit}},
	isbn = {978-3-319-57020-4 978-3-319-57021-1},
	url = {http://link.springer.com/10.1007/978-3-319-57021-1_17},
	abstract = {The Gesture Recognition Toolkit is a cross-platform open-source C++ library designed to make real-time machine learning and gesture recognition more accessible for non-specialists. Emphasis is placed on ease of use, with a consistent, minimalist design that promotes accessibility while supporting ﬂexibility and customization for advanced users. The toolkit features a broad range of classiﬁcation and regression algorithms and has extensive support for building real-time systems. This includes algorithms for signal processing, feature extraction and automatic gesture spotting.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Gesture {Recognition}},
	publisher = {Springer International Publishing},
	author = {Gillian, Nicholas and Paradiso, Joseph A.},
	editor = {Escalera, Sergio and Guyon, Isabelle and Athitsos, Vassilis},
	year = {2017},
	pages = {497--502},
	file = {Gillian and Paradiso - 2017 - The Gesture Recognition Toolkit.pdf:/Users/orsonxu/Zotero/storage/LH3XZV8S/Gillian and Paradiso - 2017 - The Gesture Recognition Toolkit.pdf:application/pdf},
}

@article{lien_soli_2016,
	title = {Soli: ubiquitous gesture sensing with millimeter wave radar},
	volume = {35},
	issn = {0730-0301, 1557-7368},
	shorttitle = {Soli},
	url = {https://dl.acm.org/doi/10.1145/2897824.2925953},
	doi = {10.1145/2897824.2925953},
	abstract = {This paper presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solidstate radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.},
	language = {en},
	number = {4},
	urldate = {2021-08-24},
	journal = {ACM Transactions on Graphics},
	author = {Lien, Jaime and Gillian, Nicholas and Karagozler, M. Emre and Amihood, Patrick and Schwesig, Carsten and Olson, Erik and Raja, Hakim and Poupyrev, Ivan},
	month = jul,
	year = {2016},
	pages = {1--19},
	file = {Lien et al. - 2016 - Soli ubiquitous gesture sensing with millimeter w.pdf:/Users/orsonxu/Zotero/storage/X5GQFZSA/Lien et al. - 2016 - Soli ubiquitous gesture sensing with millimeter w.pdf:application/pdf},
}

@inproceedings{hartmann_programming_2007,
	address = {Newport, Rhode Island, USA},
	title = {Programming by a sample: rapidly creating web applications with d.mix},
	shorttitle = {Programming by a sample},
	url = {http://portal.acm.org/citation.cfm?doid=1294211.1294254},
	doi = {10.1145/1294211.1294254},
	abstract = {Source-code examples of APIs enable developers to quickly gain a gestalt understanding of a library’s functionality, and they support organically creating applications by incrementally modifying a functional starting point. As an increasing number of web sites provide APIs, significant latent value lies in connecting the complementary representations between site and service—in essence, enabling sites themselves to be the example corpus. We introduce d.mix, a tool for creating web mashups that leverages this site-to-service correspondence. With d.mix, users browse annotated web sites and select elements to sample. d.mix’s sampling mechanism generates the underlying service calls that yield those elements. This code can be edited, executed, and shared in d.mix’s wiki-based hosting environment. This sampling approach leverages pre-existing web sites as example sets and supports fluid composition and modification of examples. An initial study with eight participants found d.mix to enable rapid experimentation, and suggested avenues for improving its annotation mechanism.},
	language = {en},
	urldate = {2021-09-04},
	booktitle = {Proceedings of the 20th annual {ACM} symposium on {User} interface software and technology  - {UIST} '07},
	publisher = {ACM Press},
	author = {Hartmann, Björn and Wu, Leslie and Collins, Kevin and Klemmer, Scott R.},
	year = {2007},
	pages = {241},
	file = {Hartmann et al. - 2007 - Programming by a sample rapidly creating web appl.pdf:/Users/orsonxu/Zotero/storage/TDMBGGL6/Hartmann et al. - 2007 - Programming by a sample rapidly creating web appl.pdf:application/pdf},
}

@inproceedings{dey_cappella_2004,
	address = {Vienna, Austria},
	title = {a {CAPpella}: programming by demonstration of context-aware applications},
	isbn = {978-1-58113-702-6},
	shorttitle = {a {CAPpella}},
	url = {http://portal.acm.org/citation.cfm?doid=985692.985697},
	doi = {10.1145/985692.985697},
	abstract = {Context-aware applications are applications that implicitly take their context of use into account by adapting to changes in a user's activities and environments. No one has more intimate knowledge about these activities and environments than end-users themselves. Currently there is no support for end-users to build context-aware applications for these dynamic settings. To address this issue, we present a CAPpella, a programming by demonstration Context-Aware Prototyping environment intended for end-users. Users "program" their desired context-aware behavior (situation and associated action) in situ, without writing any code, by demonstrating it to a CAPpella and by annotating the relevant portions of the demonstration. Using a meeting and medicine-taking scenario, we illustrate how a user can demonstrate different behaviors to a CAPpella. We describe a CAPpella's underlying system to explain how it supports users in building behaviors and present a study of 14 endusers to illustrate its feasibility and usability.},
	language = {en},
	urldate = {2021-09-04},
	booktitle = {Proceedings of the 2004 conference on {Human} factors in computing systems  - {CHI} '04},
	publisher = {ACM Press},
	author = {Dey, Anind K. and Hamid, Raffay and Beckmann, Chris and Li, Ian and Hsu, Daniel},
	year = {2004},
	pages = {33--40},
	file = {Dey et al. - 2004 - a CAPpella programming by demonstration of contex.pdf:/Users/orsonxu/Zotero/storage/EWW7MG3V/Dey et al. - 2004 - a CAPpella programming by demonstration of contex.pdf:application/pdf},
}

@inproceedings{kay_researcher-centered_2016,
	address = {San Jose California USA},
	title = {Researcher-{Centered} {Design} of {Statistics}: {Why} {Bayesian} {Statistics} {Better} {Fit} the {Culture} and {Incentives} of {HCI}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Researcher-{Centered} {Design} of {Statistics}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858465},
	doi = {10.1145/2858036.2858465},
	abstract = {A core tradition of HCI lies in the experimental evaluation of the effects of techniques and interfaces to determine if they are useful for achieving their purpose. However, our individual analyses tend to stand alone, and study results rarely accrue in more precise estimates via meta-analysis: in a literature search, we found only 56 meta-analyses in HCI in the ACM Digital Library, 3 of which were published at CHI (often called the top HCI venue). Yet meta-analysis is the gold standard for demonstrating robust quantitative knowledge. We treat this as a user-centered design problem: the failure to accrue quantitative knowledge is not the users’ (i.e. researchers’) failure, but a failure to consider those users’ needs when designing statistical practice. Using simulation, we compare hypothetical publication worlds following existing frequentist against Bayesian practice. We show that Bayesian analysis yields more precise effects with each new study, facilitating knowledge accrual without traditional meta-analyses. Bayesian practices also allow more principled conclusions from small-n studies of novel techniques. These advantages make Bayesian practices a likely better fit for the culture and incentives of the field. Instead of admonishing ourselves to spend resources on larger studies, we propose using tools that more appropriately analyze small studies and encourage knowledge accrual from one study to the next. We also believe Bayesian methods can be adopted from the bottom up without the need for new incentives for replication or meta-analysis. These techniques offer the potential for a more user- (i.e. researcher-) centered approach to statistical analysis in HCI.},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kay, Matthew and Nelson, Gregory L. and Hekler, Eric B.},
	month = may,
	year = {2016},
	pages = {4521--4532},
	file = {Kay et al. - 2016 - Researcher-Centered Design of Statistics Why Baye.pdf:/Users/orsonxu/Zotero/storage/QUBMVYTV/Kay et al. - 2016 - Researcher-Centered Design of Statistics Why Baye.pdf:application/pdf},
}

@inproceedings{kay_researcher-centered_2016-1,
	address = {San Jose California USA},
	title = {Researcher-{Centered} {Design} of {Statistics}: {Why} {Bayesian} {Statistics} {Better} {Fit} the {Culture} and {Incentives} of {HCI}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Researcher-{Centered} {Design} of {Statistics}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858465},
	doi = {10.1145/2858036.2858465},
	abstract = {A core tradition of HCI lies in the experimental evaluation of the effects of techniques and interfaces to determine if they are useful for achieving their purpose. However, our individual analyses tend to stand alone, and study results rarely accrue in more precise estimates via meta-analysis: in a literature search, we found only 56 meta-analyses in HCI in the ACM Digital Library, 3 of which were published at CHI (often called the top HCI venue). Yet meta-analysis is the gold standard for demonstrating robust quantitative knowledge. We treat this as a user-centered design problem: the failure to accrue quantitative knowledge is not the users’ (i.e. researchers’) failure, but a failure to consider those users’ needs when designing statistical practice. Using simulation, we compare hypothetical publication worlds following existing frequentist against Bayesian practice. We show that Bayesian analysis yields more precise effects with each new study, facilitating knowledge accrual without traditional meta-analyses. Bayesian practices also allow more principled conclusions from small-n studies of novel techniques. These advantages make Bayesian practices a likely better fit for the culture and incentives of the field. Instead of admonishing ourselves to spend resources on larger studies, we propose using tools that more appropriately analyze small studies and encourage knowledge accrual from one study to the next. We also believe Bayesian methods can be adopted from the bottom up without the need for new incentives for replication or meta-analysis. These techniques offer the potential for a more user- (i.e. researcher-) centered approach to statistical analysis in HCI.},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kay, Matthew and Nelson, Gregory L. and Hekler, Eric B.},
	month = may,
	year = {2016},
	pages = {4521--4532},
	file = {Kay et al. - 2016 - Researcher-Centered Design of Statistics Why Baye.pdf:/Users/orsonxu/Zotero/storage/GJ4P4UAH/Kay et al. - 2016 - Researcher-Centered Design of Statistics Why Baye.pdf:application/pdf},
}

@inproceedings{ledo_evaluation_2018,
	address = {Montreal QC Canada},
	title = {Evaluation {Strategies} for {HCI} {Toolkit} {Research}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173610},
	doi = {10.1145/3173574.3173610},
	abstract = {Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what ‘evaluating’ a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.},
	language = {en},
	urldate = {2021-09-27},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ledo, David and Houben, Steven and Vermeulen, Jo and Marquardt, Nicolai and Oehlberg, Lora and Greenberg, Saul},
	month = apr,
	year = {2018},
	pages = {1--17},
	file = {Ledo et al. - 2018 - Evaluation Strategies for HCI Toolkit Research.pdf:/Users/orsonxu/Zotero/storage/HGBJYI48/Ledo et al. - 2018 - Evaluation Strategies for HCI Toolkit Research.pdf:application/pdf},
}

@article{nebeling_playing_nodate,
	title = {Playing the {Tricky} {Game} of {Toolkits} {Research}},
	abstract = {In this paper, I reﬂect on my experience from the past several years conducting toolkit driven multi-device interaction research that appeared in CHI and EICS. I discuss lessons learned and share my perspective on the larger ﬁeld of user interface engineering, including what I think the main challenges and opportunities are with toolkits research and good examples of it. I hope that sharing my perspective is useful for the new generation of researchers interested in, and potentially struggling with, doing engineering research in HCI.},
	language = {en},
	author = {Nebeling, Michael},
	pages = {4},
	file = {Nebeling - Playing the Tricky Game of Toolkits Research.pdf:/Users/orsonxu/Zotero/storage/TIZ8V724/Nebeling - Playing the Tricky Game of Toolkits Research.pdf:application/pdf},
}

@inproceedings{inkpen_where_2019,
	address = {Glasgow Scotland Uk},
	title = {Where is the {Human}?: {Bridging} the {Gap} {Between} {AI} and {HCI}},
	isbn = {978-1-4503-5971-9},
	shorttitle = {Where is the {Human}?},
	url = {https://dl.acm.org/doi/10.1145/3290607.3299002},
	doi = {10.1145/3290607.3299002},
	abstract = {In recent years, AI systems have become both more powerful and increasingly promising for integration in a variety of application areas. Attention has also been called to the social challenges these systems bring, particularly in how they might fail or even actively disadvantage marginalised social groups, or how their opacity might make them difficult to oversee and challenge. In the context of these and other challenges, the roles of humans working in tandem with these systems will be important, yet the HCI community has been only a quiet voice in these debates to date. This workshop aims to catalyse and crystallise an agenda around HCI’s engagement with AI systems. Topics of interest include explainable and explorable AI; documentation and review; integrating artificial and human intelligence; collaborative decision making; AI/ML in HCI Design; diverse human roles and relationships in AI systems; and critical views of AI.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Inkpen, Kori and Chancellor, Stevie and De Choudhury, Munmun and Veale, Michael and Baumer, Eric P. S.},
	month = may,
	year = {2019},
	pages = {1--9},
	file = {Inkpen et al. - 2019 - Where is the Human Bridging the Gap Between AI a.pdf:/Users/orsonxu/Zotero/storage/PDJA9H2F/Inkpen et al. - 2019 - Where is the Human Bridging the Gap Between AI a.pdf:application/pdf},
}

@inproceedings{zhu_bishare_2020,
	address = {Honolulu HI USA},
	title = {{BISHARE}: {Exploring} {Bidirectional} {Interactions} {Between} {Smartphones} and {Head}-{Mounted} {Augmented} {Reality}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{BISHARE}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376233},
	doi = {10.1145/3313831.3376233},
	abstract = {In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhu, Fengyuan and Grossman, Tovi},
	month = apr,
	year = {2020},
	pages = {1--14},
	file = {Zhu and Grossman - 2020 - BISHARE Exploring Bidirectional Interactions Betw.pdf:/Users/orsonxu/Zotero/storage/7MS5EW4A/Zhu and Grossman - 2020 - BISHARE Exploring Bidirectional Interactions Betw.pdf:application/pdf},
}

@inproceedings{bailly_livingdesktop_2016,
	address = {San Jose California USA},
	title = {{LivingDesktop}: {Augmenting} {Desktop} {Workstation} with {Actuated} {Devices}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {{LivingDesktop}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858208},
	doi = {10.1145/2858036.2858208},
	abstract = {We investigate the potential beneﬁts of actuated devices for the desktop workstation which remains the most used environment for daily ofﬁce works. A formative study reveals that the desktop workstation is not a ﬁxed environment because users manually change the position and the orientation of their devices. Based on these ﬁndings, we present the LivingDesktop, an augmented desktop workstation with devices (mouse, keyboard, monitor) capable of moving autonomously. We describe interaction techniques and applications illustrating how actuated desktop workstations can improve ergonomics, foster collaboration, leverage context and reinforce physicality. Finally, the ﬁndings of a scenario evaluation are (1) the perceived usefulness of ergonomics and collaboration applications; (2) how the LivingDesktop inspired our participants to elaborate novel accessibility and social applications; (3) the location and user practices should be considered when designed actuated desktop devices.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Bailly, Gilles and Sahdev, Sidharth and Malacria, Sylvain and Pietrzak, Thomas},
	month = may,
	year = {2016},
	pages = {5298--5310},
	file = {Bailly et al. - 2016 - LivingDesktop Augmenting Desktop Workstation with.pdf:/Users/orsonxu/Zotero/storage/KQTMV7X5/Bailly et al. - 2016 - LivingDesktop Augmenting Desktop Workstation with.pdf:application/pdf},
}

@inproceedings{abdul_trends_2018,
	address = {Montreal QC Canada},
	title = {Trends and {Trajectories} for {Explainable}, {Accountable} and {Intelligible} {Systems}: {An} {HCI} {Research} {Agenda}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Trends and {Trajectories} for {Explainable}, {Accountable} and {Intelligible} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174156},
	doi = {10.1145/3173574.3174156},
	abstract = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasingly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explainable systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorithmic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research towards this goal.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
	month = apr,
	year = {2018},
	pages = {1--18},
	file = {Abdul et al. - 2018 - Trends and Trajectories for Explainable, Accountab.pdf:/Users/orsonxu/Zotero/storage/EEB3L5XE/Abdul et al. - 2018 - Trends and Trajectories for Explainable, Accountab.pdf:application/pdf},
}

@inproceedings{wang_designing_2019,
	address = {Glasgow Scotland Uk},
	title = {Designing {Theory}-{Driven} {User}-{Centric} {Explainable} {AI}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300831},
	doi = {10.1145/3290605.3300831},
	abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical applicationspecific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building humancentered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
	month = may,
	year = {2019},
	pages = {1--15},
	file = {Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable A.pdf:/Users/orsonxu/Zotero/storage/23YWB6US/Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable A.pdf:application/pdf},
}

@inproceedings{horvitz_principles_1999,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {Principles of mixed-initiative user interfaces},
	isbn = {978-0-201-48559-2},
	url = {http://portal.acm.org/citation.cfm?doid=302979.303030},
	doi = {10.1145/302979.303030},
	abstract = {Recent debate has centered on the relative promise of focusing user-interface research on developing new metaphors and tools that enhance users’ abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human-computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the Lookout system for scheduling and meeting management.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems the {CHI} is the limit - {CHI} '99},
	publisher = {ACM Press},
	author = {Horvitz, Eric},
	year = {1999},
	pages = {159--166},
	file = {Horvitz - 1999 - Principles of mixed-initiative user interfaces.pdf:/Users/orsonxu/Zotero/storage/NSSHBCPC/Horvitz - 1999 - Principles of mixed-initiative user interfaces.pdf:application/pdf},
}

@inproceedings{gajos_exploring_2006,
	address = {Venezia, Italy},
	title = {Exploring the design space for adaptive graphical user interfaces},
	isbn = {978-1-59593-353-9},
	url = {http://portal.acm.org/citation.cfm?doid=1133265.1133306},
	doi = {10.1145/1133265.1133306},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the working conference on {Advanced} visual interfaces  - {AVI} '06},
	publisher = {ACM Press},
	author = {Gajos, Krzysztof Z. and Czerwinski, Mary and Tan, Desney S. and Weld, Daniel S.},
	year = {2006},
	pages = {201},
	file = {Gajos et al. - 2006 - Exploring the design space for adaptive graphical .pdf:/Users/orsonxu/Zotero/storage/57E6AHJU/Gajos et al. - 2006 - Exploring the design space for adaptive graphical .pdf:application/pdf},
}

@inproceedings{cimolino_two_2022,
	address = {New Orleans LA USA},
	title = {Two {Heads} {Are} {Better} {Than} {One}: {A} {Dimension} {Space} for {Unifying} {Human} and {Artificial} {Intelligence} in {Shared} {Control}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Two {Heads} {Are} {Better} {Than} {One}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517610},
	doi = {10.1145/3491102.3517610},
	abstract = {Shared control is an emerging interaction paradigm in which a human and an AI partner collaboratively control a system. Shared control unifes human and artifcial intelligence, making the human’s interactions with computers more accessible, safe, precise, efective, creative, and playful. This form of interaction has independently emerged in contexts as varied as mobility assistance, driving, surgery, and digital games. These domains each have their own problems, terminology, and design philosophies. Without a common language for describing interactions in shared control, it is difcult for designers working in one domain to share their knowledge with designers working in another. To address this problem, we present a dimension space for shared control, based on a survey of 55 shared control systems from six diferent problem domains. This design space analysis tool enables designers to classify existing systems, make comparisons between them, identify higher-level design patterns, and imagine solutions to novel problems.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Cimolino, Gabriele and Graham, T.C. Nicholas},
	month = apr,
	year = {2022},
	pages = {1--21},
	file = {Cimolino and Graham - 2022 - Two Heads Are Better Than One A Dimension Space f.pdf:/Users/orsonxu/Zotero/storage/5GSSUTQA/Cimolino and Graham - 2022 - Two Heads Are Better Than One A Dimension Space f.pdf:application/pdf},
}

@inproceedings{cila_designing_2022,
	address = {New Orleans LA USA},
	title = {Designing {Human}-{Agent} {Collaborations}: {Commitment}, responsiveness, and support},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Designing {Human}-{Agent} {Collaborations}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517500},
	doi = {10.1145/3491102.3517500},
	abstract = {With the advancements in AI, agents (i.e., smart products, robots, software agents) are increasingly capable of working closely together with humans in a variety of ways while benefiting from each other. These human-agent collaborations have gained growing attention in the HCI community; however, the field lacks clear guidelines on how to design the agents’ behaviors in collaborations. In this paper, the qualities that are relevant for designers to create robust and pleasant human-agent collaborations were investigated. Bratman’s Shared Cooperative Activity framework was used to identify the core characteristics of collaborations and survey the most important issues in the design of human-agent collaborations, namely code-of-conduct, task delegation, autonomy and control, intelligibility, common ground, offering help and requesting help. The aim of this work is to add structure to this growing and important facet of HCI research and operationalize the concept of human-agent collaboration with concrete design considerations.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Cila, Nazli},
	month = apr,
	year = {2022},
	pages = {1--18},
	file = {Cila - 2022 - Designing Human-Agent Collaborations Commitment, .pdf:/Users/orsonxu/Zotero/storage/RBLC95IG/Cila - 2022 - Designing Human-Agent Collaborations Commitment, .pdf:application/pdf},
}

@article{mohseni_multidisciplinary_2021,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	volume = {11},
	issn = {2160-6455, 2160-6463},
	url = {https://dl.acm.org/doi/10.1145/3387166},
	doi = {10.1145/3387166},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of
              artificial intelligence
              (
              AI
              ) applications used in everyday life.
              Explainable AI
              (
              XAI
              ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	language = {en},
	number = {3-4},
	urldate = {2022-07-11},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = dec,
	year = {2021},
	pages = {1--45},
	file = {Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf:/Users/orsonxu/Zotero/storage/GTAHMS2G/Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf:application/pdf},
}

@inproceedings{lam_m2a_2019,
	address = {Kyoto, Japan},
	title = {{M2A}: {A} {Framework} for {Visualizing} {Information} from {Mobile} {Web} to {Mobile} {Augmented} {Reality}},
	isbn = {978-1-5386-9148-9},
	shorttitle = {{M2A}},
	url = {https://ieeexplore.ieee.org/document/8767388/},
	doi = {10.1109/PERCOM.2019.8767388},
	abstract = {Mobile Augmented Reality (MAR) drastically changes our approach to computing and user interaction. Web browsing, in particular, is impractical on AR devices as current web design principles do not account for three-dimensional display and navigation of virtual content. In this paper, we propose Mobile to AR (M2A), the ﬁrst framework for designing web pages for AR devices. M2A exploits the visual context to display more content while enabling users to locate relevant data intuitively with minimal modiﬁcations to the website. To evaluate the principles behind the framework, we implement a demonstration application in AR and conduct two user-focused experiments. Our experimental study reveals that participants with M2A are 5 times faster to ﬁnd information on a web page compared to a smartphone, and 2 times faster than a traditional AR web browser. Furthermore, users consider navigation on M2A websites to be signiﬁcantly more intuitive and easy to use compared to their desktop and mobile counterparts.},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {2019 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} ({PerCom}},
	publisher = {IEEE},
	author = {Lam, Kit Yung and Hang Lee, Lik and Braud, Tristan and Hui, Pan},
	month = mar,
	year = {2019},
	pages = {1--10},
	file = {Lam et al. - 2019 - M2A A Framework for Visualizing Information from .pdf:/Users/orsonxu/Zotero/storage/AMIU5WXQ/Lam et al. - 2019 - M2A A Framework for Visualizing Information from .pdf:application/pdf},
}

@article{sahu_artificial_2020,
	title = {Artificial intelligence ({AI}) in augmented reality ({AR})-assisted manufacturing applications: a review},
	volume = {59},
	shorttitle = {Artificial intelligence ({AI}) in augmented reality ({AR})-assisted manufacturing applications},
	doi = {10.1080/00207543.2020.1859636},
	abstract = {Augmented reality (AR) has proven to be an invaluable interactive medium to reduce cognitive load by bridging the gap between the task-at-hand and relevant information by displaying information without disturbing the user's focus. AR is particularly useful in the manufacturing environment where a diverse set of tasks such as assembly and maintenance must be performed in the most cost-effective and efficient manner possible. While AR systems have seen immense research innovation in recent years, the current strategies utilised in AR for camera calibration, detection, tracking, camera position and orientation (pose) estimation, inverse rendering, procedure storage, virtual object creation, registration, and rendering are still mostly dominated by traditional non-AI approaches. This restricts their practicability to controlled environments with limited variations in the scene. Classical AR methods can be greatly improved through the incorporation of various AI strategies like deep learning, ontology, and expert systems for adapting to broader scene variations and user preferences. This research work provides a review of current AR strategies, critical appraisal for these strategies, and potential AI solutions for every component of the computational pipeline of AR systems. Given the review of current work in both fields, future research work directions are also outlined.},
	journal = {International Journal of Production Research},
	author = {Sahu, Chandan Kumar and Young, Crystal and Rai, Rahul},
	month = dec,
	year = {2020},
	pages = {1--57},
}

@inproceedings{eiband_bringing_2018,
	address = {Tokyo Japan},
	title = {Bringing {Transparency} {Design} into {Practice}},
	isbn = {978-1-4503-4945-1},
	url = {https://dl.acm.org/doi/10.1145/3172944.3172961},
	doi = {10.1145/3172944.3172961},
	abstract = {Intelligent systems, which are on their way to becoming mainstream in everyday products, make recommendations and decisions for users based on complex computations. Researchers and policy makers increasingly raise concerns regarding the lack of transparency and comprehensibility of these computations from the user perspective. Our aim is to advance existing UI guidelines for more transparency in complex realworld design scenarios involving multiple stakeholders. To this end, we contribute a stage-based participatory process for designing transparent interfaces incorporating perspectives of users, designers, and providers, which we developed and validated with a commercial intelligent ﬁtness coach. With our work, we hope to provide guidance to practitioners and to pave the way for a pragmatic approach to transparency in intelligent systems.},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {23rd {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Eiband, Malin and Schneider, Hanna and Bilandzic, Mark and Fazekas-Con, Julian and Haug, Mareike and Hussmann, Heinrich},
	month = mar,
	year = {2018},
	pages = {211--223},
	file = {Eiband et al. - 2018 - Bringing Transparency Design into Practice.pdf:/Users/orsonxu/Zotero/storage/6GD3YVZK/Eiband et al. - 2018 - Bringing Transparency Design into Practice.pdf:application/pdf},
}

@inproceedings{long_what_2020,
	address = {Honolulu HI USA},
	title = {What is {AI} {Literacy}? {Competencies} and {Design} {Considerations}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {What is {AI} {Literacy}?},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376727},
	doi = {10.1145/3313831.3376727},
	abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learnercentered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper’s contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Long, Duri and Magerko, Brian},
	month = apr,
	year = {2020},
	pages = {1--16},
	file = {Long and Magerko - 2020 - What is AI Literacy Competencies and Design Consi.pdf:/Users/orsonxu/Zotero/storage/F7X455KW/Long and Magerko - 2020 - What is AI Literacy Competencies and Design Consi.pdf:application/pdf},
}

@inproceedings{roy_automation_2019,
	address = {Glasgow Scotland Uk},
	title = {Automation {Accuracy} {Is} {Good}, but {High} {Controllability} {May} {Be} {Better}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300750},
	doi = {10.1145/3290605.3300750},
	abstract = {When automating tasks using some form of artificial intelligence, some inaccuracy in the result is virtually unavoidable. In many cases, the user must decide whether to try the automated method again, or fix it themselves using the available user interface. We argue this decision is influenced by both perceived automation accuracy and degree of task “controllability” (how easily and to what extent an automated result can be manually modified). This relationship between accuracy and controllability is investigated in a 750-participant crowdsourced experiment using a controlled, gamified task. With high controllability, self-reported satisfaction remained constant even under very low accuracy conditions, and overall, a strong preference was observed for using manual control rather than automation, despite much slower performance and regardless of very poor controllability.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Roy, Quentin and Zhang, Futian and Vogel, Daniel},
	month = may,
	year = {2019},
	pages = {1--8},
	file = {Roy et al. - 2019 - Automation Accuracy Is Good, but High Controllabil.pdf:/Users/orsonxu/Zotero/storage/3XRB6925/Roy et al. - 2019 - Automation Accuracy Is Good, but High Controllabil.pdf:application/pdf},
}

@inproceedings{cai_impacts_2022,
	address = {New Orleans LA USA},
	title = {Impacts of {Personal} {Characteristics} on {User} {Trust} in {Conversational} {Recommender} {Systems}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517471},
	doi = {10.1145/3491102.3517471},
	abstract = {Conversational recommender systems (CRSs) imitate human advisors to assist users in fnding items through conversations and have recently gained increasing attention in domains such as media and e-commerce. Like in human communication, building trust in human-agent communication is essential given its signifcant infuence on user behavior. However, inspiring user trust in CRSs with a “one-size-fts-all” design is difcult, as individual users may have their own expectations for conversational interactions (e.g., who, user or system, takes the initiative), which are potentially related to their personal characteristics. In this study, we investigated the impacts of three personal characteristics, namely personality traits, trust propensity, and domain knowledge, on user trust in two types of text-based CRSs, i.e., user-initiative and mixed-initiative. Our between-subjects user study (N=148) revealed that users’ trust propensity and domain knowledge positively infuenced their trust in CRSs, and that users with high conscientiousness tended to trust the mixed-initiative system.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Cai, Wanling and Jin, Yucheng and Chen, Li},
	month = apr,
	year = {2022},
	pages = {1--14},
	file = {Cai et al. - 2022 - Impacts of Personal Characteristics on User Trust .pdf:/Users/orsonxu/Zotero/storage/VS3CFV9X/Cai et al. - 2022 - Impacts of Personal Characteristics on User Trust .pdf:application/pdf},
}

@article{schmidt_transparency_2020,
	title = {Transparency and trust in artificial intelligence systems},
	volume = {29},
	issn = {1246-0125, 2116-7052},
	url = {https://www.tandfonline.com/doi/full/10.1080/12460125.2020.1819094},
	doi = {10.1080/12460125.2020.1819094},
	abstract = {Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.},
	language = {en},
	number = {4},
	urldate = {2022-07-21},
	journal = {Journal of Decision Systems},
	author = {Schmidt, Philipp and Biessmann, Felix and Teubner, Timm},
	month = oct,
	year = {2020},
	pages = {260--278},
	file = {Schmidt et al. - 2020 - Transparency and trust in artificial intelligence .pdf:/Users/orsonxu/Zotero/storage/Q2YX3FS7/Schmidt et al. - 2020 - Transparency and trust in artificial intelligence .pdf:application/pdf},
}

@inproceedings{cheng_semanticadapt_2021,
	title = {{SemanticAdapt}: {Optimization}-based {Adaptation} of {Mixed} {Reality} {Layouts} {Leveraging} {Virtual}-{Physical} {Semantic} {Connections}},
	abstract = {We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users ∗This work was done while Yifei Cheng was an intern at Tsinghua University.},
	language = {en},
	booktitle = {Proceedings of the 34th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	author = {Cheng, Yifei and Yan, Yukang and Yi, Xin and Shi, Yuanchun and Lindlbauer, David},
	year = {2021},
	pages = {16},
	file = {Cheng - 2021 - SemanticAdapt Optimization-based Adaptation of Mi.pdf:/Users/orsonxu/Zotero/storage/4Q4FNCW7/Cheng - 2021 - SemanticAdapt Optimization-based Adaptation of Mi.pdf:application/pdf},
}

@inproceedings{duchowski_index_2018,
	address = {Montreal QC Canada},
	title = {The {Index} of {Pupillary} {Activity}: {Measuring} {Cognitive} {Load} \textit{vis-à-vis} {Task} {Difficulty} with {Pupil} {Oscillation}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {The {Index} of {Pupillary} {Activity}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173856},
	doi = {10.1145/3173574.3173856},
	abstract = {A novel eye-tracked measure of the frequency of pupil diameter oscillation is proposed for capturing what is thought to be an indicator of cognitive load. The proposed metric, termed the Index of Pupillary Activity, is shown to discriminate task difﬁculty vis-à-vis cognitive load (if the implied causality can be assumed) in an experiment where participants performed easy and difﬁcult mental arithmetic tasks while ﬁxating a central target (a requirement for replication of prior work). The paper’s contribution is twofold: full documentation is provided for the calculation of the proposed measurement which can be considered as an alternative to the existing proprietary Index of Cognitive Activity (ICA). Thus, it is possible for researchers to replicate the experiment and build their own software which implements this measurement. Second, several aspects of the ICA are approached in a more data-sensitive way with the goal of improving the measurement’s performance.},
	language = {en},
	urldate = {2022-07-20},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Krejtz, Izabela and Biele, Cezary and Niedzielska, Anna and Kiefer, Peter and Raubal, Martin and Giannopoulos, Ioannis},
	month = apr,
	year = {2018},
	pages = {1--13},
	file = {Duchowski et al. - 2018 - The Index of Pupillary Activity Measuring Cogniti.pdf:/Users/orsonxu/Zotero/storage/YEHGLN9S/Duchowski et al. - 2018 - The Index of Pupillary Activity Measuring Cogniti.pdf:application/pdf},
}

@inproceedings{tanya_r_jonker_role_2020,
	title = {The role of {AI} in mixed and augmented reality interactions},
	shorttitle = {{CHI}'20},
	url = {https://doi.org/10.1145/3334480},
	language = {en},
	urldate = {2022-07-19},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} {Workshop}: "{Artificial} {Intelligence}: {A} {Modern} {Approach}"},
	author = {{Tanya R. Jonker} and {Ruta Desai} and {Kevin Carlberg} and {James Hillis} and {Sean Keller} and {Hrvoje Benko}},
	year = {2020},
	file = {CHI Conference et al. - 2020 - CHI'20 extended abstracts of the 2020 CHI Confere.pdf:/Users/orsonxu/Zotero/storage/PX4VR5P4/CHI Conference et al. - 2020 - CHI'20 extended abstracts of the 2020 CHI Confere.pdf:application/pdf},
}

@article{zanzotto_viewpoint_2019,
	title = {Viewpoint: {Human}-in-the-loop {Artificial} {Intelligence}},
	volume = {64},
	issn = {1076-9757},
	shorttitle = {Viewpoint},
	url = {https://jair.org/index.php/jair/article/view/11345},
	doi = {10.1613/jair.1.11345},
	abstract = {Little by little, newspapers are revealing the bright future that Artiﬁcial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need ﬁnancial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet.},
	language = {en},
	urldate = {2022-07-18},
	journal = {Journal of Artificial Intelligence Research},
	author = {Zanzotto, Fabio Massimo},
	month = feb,
	year = {2019},
	pages = {243--252},
	file = {Zanzotto - 2019 - Viewpoint Human-in-the-loop Artificial Intelligen.pdf:/Users/orsonxu/Zotero/storage/5BZYGL9X/Zanzotto - 2019 - Viewpoint Human-in-the-loop Artificial Intelligen.pdf:application/pdf},
}

@article{azuma_survey_1997,
	title = {A {Survey} of {Augmented} {Reality}},
	abstract = {This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.},
	language = {en},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {Azuma, Ronald T},
	year = {1997},
	pages = {48},
	file = {Azuma - A Survey of Augmented Reality.pdf:/Users/orsonxu/Zotero/storage/V857WH2B/Azuma - A Survey of Augmented Reality.pdf:application/pdf},
}

@article{carmigniani_augmented_2011,
	title = {Augmented reality technologies, systems and applications},
	volume = {51},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-010-0660-6},
	doi = {10.1007/s11042-010-0660-6},
	abstract = {This paper surveys the current state-of-the-art of technology, systems and applications in Augmented Reality. It describes work performed by many different research groups, the purpose behind each new Augmented Reality system, and the difficulties and problems encountered when building some Augmented Reality applications. It surveys mobile augmented reality systems challenges and requirements for successful mobile systems. This paper summarizes the current applications of Augmented Reality and speculates on future applications and where current research will lead Augmented Reality’s development. Challenges augmented reality is facing in each of these applications to go from the laboratories to the industry, as well as the future challenges we can forecast are also discussed in this paper. Section 1 gives an introduction to what Augmented Reality is and the motivations for developing this technology. Section 2 discusses Augmented Reality Technologies with computer vision methods, AR devices, interfaces and systems, and visualization tools. The mobile and wireless systems for Augmented Reality are discussed in Section 3. Four classes of current applications that have been explored are described in Section 4. These applications were chosen as they are the most famous type of applications encountered when researching AR apps. The future of augmented reality and the challenges they will be facing are discussed in Section 5.},
	language = {en},
	number = {1},
	urldate = {2022-07-17},
	journal = {Multimedia Tools and Applications},
	author = {Carmigniani, Julie and Furht, Borko and Anisetti, Marco and Ceravolo, Paolo and Damiani, Ernesto and Ivkovic, Misa},
	month = jan,
	year = {2011},
	pages = {341--377},
	file = {Carmigniani et al. - 2011 - Augmented reality technologies, systems and applic.pdf:/Users/orsonxu/Zotero/storage/PEWV3NNF/Carmigniani et al. - 2011 - Augmented reality technologies, systems and applic.pdf:application/pdf},
}

@article{billinghurst_survey_2015,
	title = {A {Survey} of {Augmented} {Reality}},
	volume = {8},
	issn = {1551-3955, 1551-3963},
	url = {http://www.nowpublishers.com/article/Details/HCI-049},
	doi = {10.1561/1100000049},
	abstract = {This survey summarizes almost 50 years of research and development in the ﬁeld of Augmented Reality (AR). From early research in the 1960’s until widespread availability by the 2010’s there has been steady progress towards the goal of being able to seamlessly combine real and virtual worlds. We provide an overview of the common deﬁnitions of AR, and show how AR ﬁts into taxonomies of other related technologies. A history of important milestones in Augmented Reality is followed by sections on the key enabling technologies of tracking, display and input devices. We also review design guidelines and provide some examples of successful AR applications. Finally, we conclude with a summary of directions for future work and a review of some of the areas that are currently being researched.},
	language = {en},
	number = {2-3},
	urldate = {2022-07-17},
	journal = {Foundations and Trends® in Human–Computer Interaction},
	author = {Billinghurst, Mark and Clark, Adrian and Lee, Gun},
	year = {2015},
	pages = {73--272},
	file = {Billinghurst et al. - 2015 - A Survey of Augmented Reality.pdf:/Users/orsonxu/Zotero/storage/6T3XLZEX/Billinghurst et al. - 2015 - A Survey of Augmented Reality.pdf:application/pdf},
}

@inproceedings{lam_a2w_2021,
	address = {Virtual Event China},
	title = {{A2W}: {Context}-{Aware} {Recommendation} {System} for {Mobile} {Augmented} {Reality} {Web} {Browser}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {{A2W}},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475413},
	doi = {10.1145/3474085.3475413},
	abstract = {Augmented Reality (AR) offers new capabilities for blurring the boundaries between physical reality and digital media. However, the capabilities of integrating web contents and AR remain underexplored. This paper presents an AR web browser with an integrated context-aware AR-to-Web content recommendation service named as A2W browser, to provide continuously user-centric web browsing experiences driven by AR headsets. We implement the A2W browser on an AR headset as our demonstration application, demonstrating the features and performance of A2W framework. The A2W browser visualizes the AR-driven web contents to the user, which is suggested by the content-based filtering model in our recommendation system. In our experiments, 20 participants with the adaptive UIs and recommendation system in A2W browser achieve up to 30.69\% time saving compared to smartphone conditions. Accordingly, A2W-supported web browsing on workstations facilitates the recommended information leading to 41.67\% faster reaches to the target information than typical web browsing.},
	language = {en},
	urldate = {2022-07-17},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Lam, Kit Yung and Lee, Lik Hang and Hui, Pan},
	month = oct,
	year = {2021},
	pages = {2447--2455},
	file = {Lam et al. - 2021 - A2W Context-Aware Recommendation System for Mobil.pdf:/Users/orsonxu/Zotero/storage/EBRYVWP5/Lam et al. - 2021 - A2W Context-Aware Recommendation System for Mobil.pdf:application/pdf},
}

@misc{lee_all_2021,
	title = {All {One} {Needs} to {Know} about {Metaverse}: {A} {Complete} {Survey} on {Technological} {Singularity}, {Virtual} {Ecosystem}, and {Research} {Agenda}},
	shorttitle = {All {One} {Needs} to {Know} about {Metaverse}},
	url = {http://arxiv.org/abs/2110.05352},
	abstract = {Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse.},
	urldate = {2022-07-17},
	publisher = {arXiv},
	author = {Lee, Lik-Hang and Braud, Tristan and Zhou, Pengyuan and Wang, Lin and Xu, Dianlei and Lin, Zijun and Kumar, Abhishek and Bermejo, Carlos and Hui, Pan},
	month = nov,
	year = {2021},
	keywords = {A.1, Computer Science - Computers and Society, K.0},
	file = {arXiv Fulltext PDF:/Users/orsonxu/Zotero/storage/HS2AFIW5/Lee et al. - 2021 - All One Needs to Know about Metaverse A Complete .pdf:application/pdf;arXiv.org Snapshot:/Users/orsonxu/Zotero/storage/Z5H5LXVX/2110.html:text/html},
}

@inproceedings{lu_exploring_2022,
	address = {New Orleans LA USA},
	title = {Exploring {Spatial} {UI} {Transition} {Mechanisms} with {Head}-{Worn} {Augmented} {Reality}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517723},
	doi = {10.1145/3491102.3517723},
	abstract = {Imagine in the future people comfortably wear augmented reality (AR) displays all day, how do we design interfaces that adapt to the contextual changes as people move around? In current operating systems, the majority of AR content defaults to staying at a fxed location until being manually moved by the users. However, this approach puts the burden of user interface (UI) transition solely on users. In this paper, we frst ran a bodystorming design workshop to capture the limitations of existing manual UI transition approaches in spatially diverse tasks. Then we addressed these limitations by designing and evaluating three UI transition mechanisms with different levels of automation and controllability (low-efort manual, semi-automated, fully-automated). Furthermore, we simulated imperfect contextual awareness by introducing prediction errors with diferent costs to correct them. Our results provide valuable lessons about the trade-ofs between UI automation levels, controllability, user agency, and the impact of prediction errors.},
	language = {en},
	urldate = {2022-07-17},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lu, Feiyu and Xu, Yan},
	month = apr,
	year = {2022},
	pages = {1--16},
	file = {Lu and Xu - 2022 - Exploring Spatial UI Transition Mechanisms with He.pdf:/Users/orsonxu/Zotero/storage/BYKC4GHM/Lu and Xu - 2022 - Exploring Spatial UI Transition Mechanisms with He.pdf:application/pdf},
}

@inproceedings{tatzgern_adaptive_2016,
	title = {Adaptive information density for augmented reality displays},
	doi = {10.1109/VR.2016.7504691},
	abstract = {Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. Clustering items by spatial and semantic attributes can temporarily alleviate the issue, but is not effective against an increasing amount of data. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Our method selects items and groups from different levels of this hierarchy based on user-defined preferences and on the amount of visual clutter caused by placing these items. The number of presented items is adapted during user interaction to avoid clutter. We compare our interface to a conventional AR browser interface in a qualitative user study. Users clearly preferred our interface, because it provided a better overview of the data and allowed for easier comparison. In a second study, we evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.},
	booktitle = {2016 {IEEE} {Virtual} {Reality} ({VR})},
	author = {Tatzgern, Markus and Orso, Valeria and Kalkofen, Denis and Jacucci, Giulio and Gamberini, Luciano and Schmalstieg, Dieter},
	month = mar,
	year = {2016},
	keywords = {Visualization, Browsers, Clustering algorithms, Clutter, Data visualization, Electronic mail, Semantics},
	pages = {83--92},
	file = {IEEE Xplore Abstract Record:/Users/orsonxu/Zotero/storage/XNIHICME/7504691.html:text/html;IEEE Xplore Full Text PDF:/Users/orsonxu/Zotero/storage/VFJ9AN2W/Tatzgern et al. - 2016 - Adaptive information density for augmented reality.pdf:application/pdf},
}

@inproceedings{huang_adaptutar_2021,
	address = {Yokohama Japan},
	title = {{AdapTutAR}: {An} {Adaptive} {Tutoring} {System} for {Machine} {Tasks} in {Augmented} {Reality}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{AdapTutAR}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445283},
	doi = {10.1145/3411764.3445283},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Huang, Gaoping and Qian, Xun and Wang, Tianyi and Patel, Fagun and Sreeram, Maitreya and Cao, Yuanzhi and Ramani, Karthik and Quinn, Alexander J.},
	month = may,
	year = {2021},
	pages = {1--15},
	file = {Huang et al. - 2021 - AdapTutAR An Adaptive Tutoring System for Machine.pdf:/Users/orsonxu/Zotero/storage/W2ENEVJU/Huang et al. - 2021 - AdapTutAR An Adaptive Tutoring System for Machine.pdf:application/pdf},
}

@inproceedings{qian_scalar_2022,
	address = {New Orleans LA USA},
	title = {{ScalAR}: {Authoring} {Semantically} {Adaptive} {Augmented} {Reality} {Experiences} in {Virtual} {Reality}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{ScalAR}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517665},
	doi = {10.1145/3491102.3517665},
	abstract = {Augmented Reality (AR) experiences tightly associate virtual contents with environmental entities. However, the dissimilarity of different environments limits the adaptive AR content behaviors under large-scale deployment. We propose ScalAR, an integrated workflow enabling designers to author semantically adaptive AR experiences in Virtual Reality (VR). First, potential AR consumers collect local scenes with a semantic understanding technique. ScalAR then synthesizes numerous similar scenes. In VR, a designer authors the AR contents’ semantic associations and validates the design while being immersed in the provided scenes. We adopt a decision-tree-based algorithm to fit the designer’s demonstrations as a semantic adaptation model to deploy the authored AR experience in a physical scene. We further showcase two application scenarios authored by ScalAR and conduct a two-session user study where the quantitative results prove the accuracy of the AR content rendering and the qualitative results show the usability of ScalAR.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Qian, Xun and He, Fengming and Hu, Xiyun and Wang, Tianyi and Ipsita, Ananya and Ramani, Karthik},
	month = apr,
	year = {2022},
	pages = {1--18},
	file = {Qian et al. - 2022 - ScalAR Authoring Semantically Adaptive Augmented .pdf:/Users/orsonxu/Zotero/storage/NIUHWNXZ/Qian et al. - 2022 - ScalAR Authoring Semantically Adaptive Augmented .pdf:application/pdf},
}

@inproceedings{lages_walking_2019,
	address = {Marina del Ray California},
	title = {Walking with adaptive augmented reality workspaces: design and usage patterns},
	isbn = {978-1-4503-6272-6},
	shorttitle = {Walking with adaptive augmented reality workspaces},
	url = {https://dl.acm.org/doi/10.1145/3301275.3302278},
	doi = {10.1145/3301275.3302278},
	abstract = {Mobile augmented reality may eventually replace our smartphones as the primary way of accessing information on the go. However, current interfaces provide little support to walking and to the variety of actions we perform in the real world. To achieve its full potential, augmented reality interfaces must support the fluid way we move and interact in the physical world. We explored how different adaptation strategies can contribute towards this goal. We evaluated design alternatives through contextual studies and identified the key interaction patterns that interfaces for walking should support. We also identified desirable properties of adaptation-based interface techniques, which can be used to guide the design of the next-generation walking-centered augmented reality workspaces.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Lages, Wallace S. and Bowman, Doug A.},
	month = mar,
	year = {2019},
	pages = {356--366},
	file = {Lages and Bowman - 2019 - Walking with adaptive augmented reality workspaces.pdf:/Users/orsonxu/Zotero/storage/8LHHFKHA/Lages and Bowman - 2019 - Walking with adaptive augmented reality workspaces.pdf:application/pdf},
}

@inproceedings{lindlbauer_context-aware_2019,
	address = {New Orleans LA USA},
	title = {Context-{Aware} {Online} {Adaptation} of {Mixed} {Reality} {Interfaces}},
	isbn = {978-1-4503-6816-2},
	url = {https://dl.acm.org/doi/10.1145/3332165.3347945},
	doi = {10.1145/3332165.3347945},
	abstract = {We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved effciently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36\%.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar},
	month = oct,
	year = {2019},
	pages = {147--160},
	file = {Lindlbauer et al. - 2019 - Context-Aware Online Adaptation of Mixed Reality I.pdf:/Users/orsonxu/Zotero/storage/MWPG2AAF/Lindlbauer et al. - 2019 - Context-Aware Online Adaptation of Mixed Reality I.pdf:application/pdf},
}

@inproceedings{dhanorkar_who_2021,
	address = {Virtual Event USA},
	title = {Who needs to know what, when?: {Broadening} the {Explainable} {AI} ({XAI}) {Design} {Space} by {Looking} at {Explanations} {Across} the {AI} {Lifecycle}},
	isbn = {978-1-4503-8476-6},
	shorttitle = {Who needs to know what, when?},
	url = {https://dl.acm.org/doi/10.1145/3461778.3462131},
	doi = {10.1145/3461778.3462131},
	abstract = {The interpretability or explainability of AI systems (XAI) has been a topic gaining renewed attention in recent years across AI and HCI communities. Recent work has drawn attention to the emergent explainability requirements of in situ, applied projects, yet further exploratory work is needed to more fully understand this space. This paper investigates applied AI projects and reports on a qualitative interview study of individuals working on AI projects at a large technology and consulting company. Presenting an empirical understanding of the range of stakeholders in industrial AI projects, this paper also draws out the emergent explainability practices that arise as these projects unfold, highlighting the range of explanation audiences (who), as well as how their explainability needs evolve across the AI project lifecycle (when). We discuss the importance of adopting a sociotechnical lens in designing AI systems, noting how the “AI lifecycle” can serve as a design metaphor to further the XAI design field.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Designing {Interactive} {Systems} {Conference} 2021},
	publisher = {ACM},
	author = {Dhanorkar, Shipi and Wolf, Christine T. and Qian, Kun and Xu, Anbang and Popa, Lucian and Li, Yunyao},
	month = jun,
	year = {2021},
	pages = {1591--1602},
	file = {Dhanorkar et al. - 2021 - Who needs to know what, when Broadening the Expl.pdf:/Users/orsonxu/Zotero/storage/R2EMIPGU/Dhanorkar et al. - 2021 - Who needs to know what, when Broadening the Expl.pdf:application/pdf},
}

@article{jiang_who_2022,
	title = {Who needs explanation and when? {Juggling} explainable {AI} and user epistemic uncertainty},
	volume = {165},
	issn = {10715819},
	shorttitle = {Who needs explanation and when?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581922000660},
	doi = {10.1016/j.ijhcs.2022.102839},
	abstract = {In recent years, AI explainability (XAI) has received wide attention. Although XAI is expected to play a positive role in decision-making and advice acceptance, various opposing effects have also been found. The opposing effects of XAI highlight the critical role of context, especially human factors, in understanding XAI’s impacts. This study investigates the effects of providing three types of post-hoc explanations (alternative advice, prediction confidence scores, and prediction rationale) on two context-specific user decision-making outcomes (AI advice acceptance and advice adoption). Our field experiment results show that users’ epistemic uncertainty matters when understanding XAI’s impacts. As users’ epistemic uncertainty increases, only providing prediction rationale is beneficial, whereas providing alternative advice and showing prediction confidence scores may hinder users’ advice acceptance. Our study contributes to the emerging literature on the human aspects of XAI by clarifying XAI and showing that XAI may not always be desirable. It also contributes by highlighting the importance of considering user profiles when pre­ dicting XAI’s impacts, designing XAI, and providing professional services with AI.},
	language = {en},
	urldate = {2022-07-14},
	journal = {International Journal of Human-Computer Studies},
	author = {Jiang, Jinglu and Kahai, Surinder and Yang, Ming},
	month = sep,
	year = {2022},
	pages = {102839},
	file = {Jiang et al. - 2022 - Who needs explanation and when Juggling explainab.pdf:/Users/orsonxu/Zotero/storage/TPQW9LKP/Jiang et al. - 2022 - Who needs explanation and when Juggling explainab.pdf:application/pdf},
}

@inproceedings{lim_assessing_2009,
	address = {Orlando Florida USA},
	title = {Assessing demand for intelligibility in context-aware applications},
	isbn = {978-1-60558-431-7},
	url = {https://dl.acm.org/doi/10.1145/1620545.1620576},
	doi = {10.1145/1620545.1620576},
	abstract = {Intelligibility can help expose the inner workings and inputs of context-aware applications that tend to be opaque to users due to their implicit sensing and actions. However, users may not be interested in all the information that the applications can produce. Using scenarios of four real-world applications that span the design space of context-aware computing, we conducted two experiments to discover what information users are interested in. In the first experiment, we elicit types of information demands that users have and under what moderating circumstances they have them. In the second experiment, we verify the findings by soliciting users about which types they would want to know and establish whether receiving such information would satisfy them. We discuss why users demand certain types of information, and provide design implications on how to provide different intelligibility types to make context-aware applications intelligible and acceptable to users.},
	language = {en},
	urldate = {2022-07-14},
	booktitle = {Proceedings of the 11th international conference on {Ubiquitous} computing},
	publisher = {ACM},
	author = {Lim, Brian Y. and Dey, Anind K.},
	month = sep,
	year = {2009},
	pages = {195--204},
	file = {Lim and Dey - 2009 - Assessing demand for intelligibility in context-aw.pdf:/Users/orsonxu/Zotero/storage/MZ9XATEV/Lim and Dey - 2009 - Assessing demand for intelligibility in context-aw.pdf:application/pdf},
}

@inproceedings{dey_support_2009,
	address = {Boston MA USA},
	title = {Support for context-aware intelligibility and control},
	isbn = {978-1-60558-246-7},
	url = {https://dl.acm.org/doi/10.1145/1518701.1518832},
	doi = {10.1145/1518701.1518832},
	abstract = {Intelligibility and control are important user concerns in context-aware applications. They allow a user to understand why an application is behaving a certain way, and to change its behavior. Because of their importance to end users, they must be addressed at an interface level. However, often the sensors or machine learning systems that users need to understand and control are created long before a specific application is built, or created separately from the application interface. Thus, supporting interface designers in building intelligibility and control into interfaces requires application logic and underlying infrastructure to be exposed in some structured fashion. As context-aware infrastructures do not provide generalized support for this, we extended one such infrastructure with Situations, components that appropriately exposes application logic, and supports debugging and simple intelligibility and control interfaces, while making it easier for an application developer to build context-aware applications and facilitating designer access to application state and behavior. We developed support for interface designers in Visual Basic and Flash. We demonstrate the usefulness of this support through an evaluation of programmers, an evaluation of the usability of the new infrastructure with interface designers, and the augmentation of three common context-aware applications.},
	language = {en},
	urldate = {2022-07-14},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Dey, Anind K. and Newberger, Alan},
	month = apr,
	year = {2009},
	pages = {859--868},
	file = {Dey and Newberger - 2009 - Support for context-aware intelligibility and cont.pdf:/Users/orsonxu/Zotero/storage/SPSAH5UL/Dey and Newberger - 2009 - Support for context-aware intelligibility and cont.pdf:application/pdf},
}

@inproceedings{liao_questioning_2020,
	title = {Questioning the {AI}: {Informing} {Design} {Practices} for {Explainable} {AI} {User} {Experiences}},
	shorttitle = {Questioning the {AI}},
	url = {http://arxiv.org/abs/2001.02478},
	doi = {10.1145/3313831.3376590},
	abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
	urldate = {2022-08-05},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	pages = {1--15},
	file = {arXiv Fulltext PDF:/Users/orsonxu/Zotero/storage/FM7TH38X/Liao et al. - 2020 - Questioning the AI Informing Design Practices for.pdf:application/pdf;arXiv.org Snapshot:/Users/orsonxu/Zotero/storage/L4QMZJ2U/2001.html:text/html},
}

@inproceedings{jeyakumar_how_2020,
	title = {How {Can} {I} {Explain} {This} to {You}? {An} {Empirical} {Study} of {Deep} {Neural} {Network} {Explanation} {Methods}},
	volume = {33},
	shorttitle = {How {Can} {I} {Explain} {This} to {You}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/2c29d89cc56cdb191c60db2f0bae796b-Abstract.html},
	abstract = {Explaining the inner workings of deep neural network models have received considerable attention in recent years. Researchers have attempted to provide human parseable explanations justifying why a model performed a specific classification. Although many of these toolkits are available for use, it is unclear which style of explanation is preferred by end-users, thereby demanding investigation. We performed a cross-analysis Amazon Mechanical Turk study comparing the popular state-of-the-art explanation methods to empirically determine which are better in explaining model decisions. The participants were asked to compare explanation methods across applications spanning image, text, audio, and sensory domains. Among the surveyed methods, explanation-by-example was preferred in all domains except text sentiment classification, where LIME's method of annotating input text was preferred. We highlight qualitative aspects of employing the studied explainability methods and conclude with implications for researchers and engineers that seek to incorporate explanations into user-facing deployments.},
	urldate = {2022-08-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jeyakumar, Jeya Vikranth and Noor, Joseph and Cheng, Yu-Hsi and Garcia, Luis and Srivastava, Mani},
	year = {2020},
	pages = {4211--4222},
	file = {Full Text PDF:/Users/orsonxu/Zotero/storage/IAVRNZ2J/Jeyakumar et al. - 2020 - How Can I Explain This to You An Empirical Study .pdf:application/pdf},
}

@article{sutherland_head-mounted_1968,
	title = {A head-mounted three dimensional display},
	language = {en},
	journal = {Fall Joint Computer Conference},
	author = {Sutherland, Ivan E},
	year = {1968},
	pages = {8},
	file = {Sutherland - A head-mounted three dimensional display.pdf:/Users/orsonxu/Zotero/storage/GKZF4UX5/Sutherland - A head-mounted three dimensional display.pdf:application/pdf},
}

@article{bellotti_intelligibility_2001,
	title = {Intelligibility and {Accountability}: {Human} {Considerations} in {Context}-{Aware} {Systems}},
	volume = {16},
	issn = {0737-0024, 1532-7051},
	shorttitle = {Intelligibility and {Accountability}},
	url = {https://www.tandfonline.com/doi/full/10.1207/S15327051HCI16234_05},
	doi = {10.1207/S15327051HCI16234_05},
	abstract = {This essay considers the problem of defining the context that context-aware systems should pay attention to from a human perspective. In particular, we argue that there are human aspects of context that cannot be sensed or even inferred by technological means, so context-aware systems cannot be designed simply to act on our behalf. Rather, they will have to be able to defer to users in an efficient and nonobtrusive fashion. Our point is particularly relevant for systems that are constructed such that applications are architecturally isolated from the sensing and inferencing that governs their behavior. We propose a design framework that is intended to guide thinking about accommodating human aspects of context. This framework presents four design principles that support intelligibility of system behavior and accountability of human users and a number of human-salient details of context that must be accounted for in context-aware system design.},
	language = {en},
	number = {2-4},
	urldate = {2022-08-25},
	journal = {Human–Computer Interaction},
	author = {Bellotti, Victoria and Edwards, Keith},
	month = dec,
	year = {2001},
	pages = {193--212},
	file = {Bellotti and Edwards - 2001 - Intelligibility and Accountability Human Consider.pdf:/Users/orsonxu/Zotero/storage/V53MK9PH/Bellotti and Edwards - 2001 - Intelligibility and Accountability Human Consider.pdf:application/pdf},
}

@inproceedings{zhu_explainable_2018,
	title = {Explainable {AI} for {Designers}: {A} {Human}-{Centered} {Perspective} on {Mixed}-{Initiative} {Co}-{Creation}},
	shorttitle = {Explainable {AI} for {Designers}},
	doi = {10.1109/CIG.2018.8490433},
	abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
	booktitle = {2018 {IEEE} {Conference} on {Computational} {Intelligence} and {Games} ({CIG})},
	author = {Zhu, Jichen and Liapis, Antonios and Risi, Sebastian and Bidarra, Rafael and Youngblood, G. Michael},
	month = aug,
	year = {2018},
	keywords = {Visualization, machine learning, human-computer interaction, Machine learning, Task analysis, Games, game design, explainable artificial intelligence, mixed-initiative co-creation, Neurons, Tools},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/Users/orsonxu/Zotero/storage/UMZNM5C2/8490433.html:text/html;IEEE Xplore Full Text PDF:/Users/orsonxu/Zotero/storage/SCL2CAYE/Zhu et al. - 2018 - Explainable AI for Designers A Human-Centered Per.pdf:application/pdf},
}

@inproceedings{wolf_explainability_2019,
	address = {Marina del Ray California},
	title = {Explainability scenarios: towards scenario-based {XAI} design},
	isbn = {978-1-4503-6272-6},
	shorttitle = {Explainability scenarios},
	url = {https://dl.acm.org/doi/10.1145/3301275.3302317},
	doi = {10.1145/3301275.3302317},
	abstract = {Integral to the adoption and uptake of AI systems in real-world settings is the ability for people to make sense of and evaluate such systems, a growing area of development and design efforts known as XAI (Explainable AI). Recent work has advanced the state of the art, yet a key challenge remains in understanding unique requirements that might arise when XAI systems are deployed into complex settings of use. In helping envision such requirements, this paper turns to scenario-based design, a method that anticipates and leverages scenarios of possible use early on in system development. To demonstrate the value of the scenario-based design method to XAI design, this paper presents a case study of aging-in-place monitoring. Introducing the concept of “explainability scenarios” as resources in XAI design, this paper sets out a forward-facing agenda for further attention to the emergent requirements of explainability-in-use.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Wolf, Christine T.},
	month = mar,
	year = {2019},
	pages = {252--257},
	file = {Wolf - 2019 - Explainability scenarios towards scenario-based X.pdf:/Users/orsonxu/Zotero/storage/QHZJV6U7/Wolf - 2019 - Explainability scenarios towards scenario-based X.pdf:application/pdf},
}

@inproceedings{kaur_interpreting_2020,
	address = {Honolulu HI USA},
	title = {Interpreting {Interpretability}: {Understanding} {Data} {Scientists}' {Use} of {Interpretability} {Tools} for {Machine} {Learning}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Interpreting {Interpretability}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376219},
	doi = {10.1145/3313831.3376219},
	abstract = {Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists’ use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists’ mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our ﬁndings in the social science literature.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},
	month = apr,
	year = {2020},
	pages = {1--14},
	file = {Kaur et al. - 2020 - Interpreting Interpretability Understanding Data .pdf:/Users/orsonxu/Zotero/storage/QEDS36CI/Kaur et al. - 2020 - Interpreting Interpretability Understanding Data .pdf:application/pdf},
}

@article{sharma_humanai_2023,
	title = {Human–{AI} collaboration enables more empathic conversations in text-based peer-to-peer mental health support},
	volume = {5},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00593-2},
	doi = {10.1038/s42256-022-00593-2},
	language = {en},
	number = {1},
	urldate = {2023-05-19},
	journal = {Nature Machine Intelligence},
	author = {Sharma, Ashish and Lin, Inna W. and Miner, Adam S. and Atkins, David C. and Althoff, Tim},
	month = jan,
	year = {2023},
	pages = {46--57},
	file = {Sharma et al. - 2023 - Human–AI collaboration enables more empathic conve.pdf:/Users/orsonxu/Zotero/storage/9XFE8895/Sharma et al. - 2023 - Human–AI collaboration enables more empathic conve.pdf:application/pdf},
}

@inproceedings{sharma_towards_2021,
	address = {Ljubljana Slovenia},
	title = {Towards {Facilitating} {Empathic} {Conversations} in {Online} {Mental} {Health} {Support}: {A} {Reinforcement} {Learning} {Approach}},
	isbn = {978-1-4503-8312-7},
	shorttitle = {Towards {Facilitating} {Empathic} {Conversations} in {Online} {Mental} {Health} {Support}},
	url = {https://dl.acm.org/doi/10.1145/3442381.3450097},
	doi = {10.1145/3442381.3450097},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {ACM},
	author = {Sharma, Ashish and Lin, Inna W. and Miner, Adam S. and Atkins, David C. and Althoff, Tim},
	month = apr,
	year = {2021},
	pages = {194--205},
	file = {Sharma et al_2021_Towards Facilitating Empathic Conversations in Online Mental Health Support.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Sharma et al_2021_Towards Facilitating Empathic Conversations in Online Mental Health Support.pdf:application/pdf},
}
