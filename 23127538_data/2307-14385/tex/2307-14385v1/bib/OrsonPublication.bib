
@inproceedings{xu_hulamove_2021,
	address = {Yokohama Japan},
	title = {{HulaMove}: {Using} {Commodity} {IMU} for {Waist} {Interaction}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{HulaMove}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445182},
	doi = {10.1145/3411764.3445182},
	abstract = {We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We frst conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confrm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5\%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove signifcantly reduced interaction time by 41.8\% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K},
	month = may,
	year = {2021},
	pages = {1--16},
	file = {Xu et al. - 2021 - HulaMove Using Commodity IMU for Waist Interactio.pdf:/Users/orsonxu/Zotero/storage/J773T8NJ/Xu et al. - 2021 - HulaMove Using Commodity IMU for Waist Interactio.pdf:application/pdf},
}

@article{xu_understanding_2021,
	title = {Understanding practices and needs of researchers in human state modeling by passive mobile sensing},
	issn = {2524-521X, 2524-5228},
	url = {https://link.springer.com/10.1007/s42486-021-00072-4},
	doi = {10.1007/s42486-021-00072-4},
	abstract = {Passive mobile sensing for the purpose of human state modeling is a fast-growing area. It has been applied to solve a wide range of behavior-related problems, including physical and mental health monitoring, affective computing, activity recognition, routine modeling, etc. However, in spite of the emerging literature that has investigated a wide range of application scenarios, there is little work focusing on the lessons learned by researchers, and on guidance for researchers to this approach. How do researchers conduct these types of research studies? Is there any established common practice when applying mobile sensing across different application areas? What are the pain points and needs that they frequently encounter? Answering these questions is an important step in the maturing of this growing sub-field of ubiquitous computing, and can benefit a wide range of audiences. It can serve to educate researchers who have growing interests in this area but have little to no previous experience. Intermediate researchers may also find the results interesting and helpful for reference to improve their skills. Moreover, it can further shed light on the design guidelines for a future toolkit that could facilitate research processes being used. In this paper, we fill this gap and answer these questions by conducting semi-structured interviews with ten experienced researchers from four countries to understand their practices and pain points when conducting their research. Our results reveal a common pipeline that researchers have adopted, and identify major challenges that do not appear in published work but that researchers often encounter. Based on the results of our interviews, we discuss practical suggestions for novice researchers and high-level design principles for a toolkit that can accelerate passive mobile sensing research.},
	language = {en},
	urldate = {2021-09-30},
	journal = {CCF Transactions on Pervasive Computing and Interaction},
	author = {Xu, Xuhai and Mankoff, Jennifer and Dey, Anind K.},
	month = jul,
	year = {2021},
	file = {Xu et al. - 2021 - Understanding practices and needs of researchers i.pdf:/Users/orsonxu/Zotero/storage/BCU75N2D/Xu et al. - 2021 - Understanding practices and needs of researchers i.pdf:application/pdf},
}

@inproceedings{sra_breathvr_2018,
	address = {Montreal QC Canada},
	title = {{BreathVR}: {Leveraging} {Breathing} as a {Directly} {Controlled} {Interface} for {Virtual} {Reality} {Games}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{BreathVR}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173914},
	doi = {10.1145/3173574.3173914},
	abstract = {With virtual reality head-mounted displays rapidly becoming accessible to mass audiences, there is growing interest in new forms of natural input techniques to enhance immersion and engagement for players. Research has explored physiological input for enhancing immersion in single player games through indirectly controlled signals like heart rate or galvanic skin response. In this paper, we propose breathing as a directly controlled physiological signal that can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Our study (N = 16) shows that participants report a higher sense of presence and ﬁnd the gameplay more fun and challenging when using our breathing actions. From study observations and analysis we present ﬁve design strategies that can aid virtual reality game designers interested in using directly controlled forms of physiological input.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Sra, Misha and Xu, Xuhai and Maes, Pattie},
	month = apr,
	year = {2018},
	pages = {1--12},
	file = {Sra et al. - 2018 - BreathVR Leveraging Breathing as a Directly Contr.pdf:/Users/orsonxu/Zotero/storage/V3G4LC5D/Sra et al. - 2018 - BreathVR Leveraging Breathing as a Directly Contr.pdf:application/pdf},
}

@article{xu_leveraging_2019,
	title = {Leveraging {Routine} {Behavior} and {Contextually}-{Filtered} {Features} for {Depression} {Detection} among {College} {Students}},
	volume = {3},
	issn = {2474-9567},
	url = {https://dl.acm.org/doi/10.1145/3351274},
	doi = {10.1145/3351274},
	abstract = {The rate of depression in college students is rising, which is known to increase suicide risk, lower academic performance and double the likelihood of dropping out of school. Existing work on fnding relationships between passively sensed behavior and depression, as well as detecting depression, mainly derives relevant unimodal features from a single sensor. However, co-occurrence of values in multiple sensors may provide better features, because such features can describe behavior in context. We present a new method to extract contextually fltered features from passively collected, time-series mobile data via association rule mining. After calculating traditional unimodal features from the data, we extract rules that relate unimodal features to each other using association rule mining. We extract rules from each class separately (e.g., depression vs. nondepression). We introduce a new metric to select a subset of rules that distinguish between the two classes. From these rules, which capture the relationship between multiple unimodal features, we automatically extract contextually fltered features. These features are then fed into a traditional machine learning pipeline to detect the class of interest (in our case, depression), defned by whether a student has a high BDI-II score at the end of the semester. The behavior rules generated by our methods are highly interpretable representations of diferences between classes. Our best model uses contextually-fltered features to signifcantly outperform a standard model that uses only unimodal features, by an average of 9.7\% across a variety of metrics. We further verifed the generalizability of our approach on a second dataset, and achieved very similar results. CCS Concepts: • Human-centered computing Ubiquitous and mobile computing; • Applied computing Life and medical sciences.},
	language = {en},
	number = {3},
	urldate = {2021-10-07},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Xu, Xuhai and Chikersal, Prerna and Doryab, Afsaneh and Villalba, Daniella K. and Dutcher, Janine M. and Tumminia, Michael J. and Althoff, Tim and Cohen, Sheldon and Creswell, Kasey G. and Creswell, J. David and Mankoff, Jennifer and Dey, Anind K.},
	month = sep,
	year = {2019},
	pages = {1--33},
	file = {Xu et al. - 2019 - Leveraging Routine Behavior and Contextually-Filte.pdf:/Users/orsonxu/Zotero/storage/UGVK8ESA/Xu et al. - 2019 - Leveraging Routine Behavior and Contextually-Filte.pdf:application/pdf},
}

@inproceedings{zhong_forceboard_2018,
	address = {Montreal QC Canada},
	title = {{ForceBoard}: {Subtle} {Text} {Entry} {Leveraging} {Pressure}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{ForceBoard}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174102},
	doi = {10.1145/3173574.3174102},
	abstract = {We present ForceBoard, a pressure-based input technique that enables text entry by subtle finger motion. To enter text, users apply pressure to control a multi-letter-wide sliding cursor on a one-dimensional keyboard with alphabetical ordering, and confirm the selection with a quick release. We examined the error model of pressure control for successive and error-tolerant input, which was incorporated into a Bayesian algorithm to infer user input. A user study showed that, after a 10-minute training, the average text entry rate reached 4.2 WPM (Words Per Minute) for character-level input, and 11.0 WPM for word-level input. Users reported that ForceBoard was easy to learn and interesting to use. These results demonstrated the feasibility of applying pressure as the main channel for text entry. We conclude by discussing the limitation, as well as the potential of ForceBoard to support interaction with constraints from form factor, social concern and physical environments.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhong, Mingyuan and Yu, Chun and Wang, Qian and Xu, Xuhai and Shi, Yuanchun},
	month = apr,
	year = {2018},
	pages = {1--10},
	file = {Zhong et al. - 2018 - ForceBoard Subtle Text Entry Leveraging Pressure.pdf:/Users/orsonxu/Zotero/storage/6TRSE72N/Zhong et al. - 2018 - ForceBoard Subtle Text Entry Leveraging Pressure.pdf:application/pdf},
}

@inproceedings{sra_vmotion_2018,
	address = {Hong Kong China},
	title = {{VMotion}: {Designing} a {Seamless} {Walking} {Experience} in {VR}},
	isbn = {978-1-4503-5198-0},
	shorttitle = {{VMotion}},
	url = {https://dl.acm.org/doi/10.1145/3196709.3196792},
	doi = {10.1145/3196709.3196792},
	abstract = {Physically walking in virtual reality can provide a satisfying sense of presence. However, natural locomotion in virtual worlds larger than the tracked space remains a practical challenge. Numerous redirected walking techniques have been proposed to overcome space limitations but they often require rapid head rotation, sometimes induced by distractors, to keep the scene rotation imperceptible. We propose a design methodology of seamlessly integrating redirection into the virtual experience that takes advantage of the perceptual phenomenon of inattentional blindness. Additionally, we present four novel visibility control techniques that work with our design methodology to minimize disruption to the user experience commonly found in existing redirection techniques. A user study (N = 16) shows that our techniques are imperceptible and users report signiﬁcantly less dizziness when using our methods. The illusion of unconstrained walking in a large area (16 × 8m) is maintained even though users are limited to a smaller (3.5 × 3.5m) physical space.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {Designing} {Interactive} {Systems} {Conference}},
	publisher = {ACM},
	author = {Sra, Misha and Xu, Xuhai and Mottelson, Aske and Maes, Pattie},
	month = jun,
	year = {2018},
	pages = {59--70},
	file = {Sra et al. - 2018 - VMotion Designing a Seamless Walking Experience i.pdf:/Users/orsonxu/Zotero/storage/FXZU9YZT/Sra et al. - 2018 - VMotion Designing a Seamless Walking Experience i.pdf:application/pdf},
}

@inproceedings{xu_clench_2019,
	address = {Glasgow Scotland Uk},
	title = {Clench {Interface}: {Novel} {Biting} {Input} {Techniques}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Clench {Interface}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300505},
	doi = {10.1145/3290605.3300505},
	abstract = {People eat every day and biting is one of the most fundamental and natural actions that they perform on a daily basis. Existing work has explored tooth click location and jaw movement as input techniques, however clenching has the potential to add control to this input channel. We propose clench interaction that leverages clenching as an actively controlled physiological signal that can facilitate interactions. We conducted a user study to investigate users’ ability to control their clench force. We found that users can easily discriminate three force levels, and that they can quickly confirm actions by unclenching (quick release). We developed a design space for clench interaction based on the results and investigated the usability of the clench interface. Participants preferred the clench over baselines and indicated a willingness to use clench-based interactions. This novel technique can provide an additional input method in cases where users’ eyes or hands are busy, augment immersive experiences such as virtual/augmented reality, and assist individuals with disabilities.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Yu, Chun and Dey, Anind K. and Mankoff, Jennifer},
	month = may,
	year = {2019},
	pages = {1--12},
	file = {Xu et al. - 2019 - Clench Interface Novel Biting Input Techniques.pdf:/Users/orsonxu/Zotero/storage/E3NCFEGR/Xu et al. - 2019 - Clench Interface Novel Biting Input Techniques.pdf:application/pdf},
}

@inproceedings{yan_frownonerror_2020,
	address = {Honolulu HI USA},
	title = {{FrownOnError}: {Interrupting} {Responses} from {Smart} {Speakers} by {Facial} {Expressions}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{FrownOnError}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376810},
	doi = {10.1145/3313831.3376810},
	abstract = {In the conversations with smart speakers, misunderstandings of users’ requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a ﬁrst user study (N=12) to understand users’ intuitive reactions to the correct and incorrect responses. Our results reveal the signiﬁcant difference in the frequency of occurrence and intensity of users’ facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efﬁciency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4\%, recall: 97.6\%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Yan, Yukang and Yu, Chun and Zheng, Wengrui and Tang, Ruining and Xu, Xuhai and Shi, Yuanchun},
	month = apr,
	year = {2020},
	pages = {1--14},
	file = {Yan et al. - 2020 - FrownOnError Interrupting Responses from Smart Sp.pdf:/Users/orsonxu/Zotero/storage/YM9NFMSS/Yan et al. - 2020 - FrownOnError Interrupting Responses from Smart Sp.pdf:application/pdf},
}

@inproceedings{xu_earbuddy_2020,
	address = {Honolulu HI USA},
	title = {{EarBuddy}: {Enabling} {On}-{Face} {Interaction} via {Wireless} {Earbuds}},
	abstract = {Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classiﬁcation. Our optimized classiﬁer achieved an accuracy of 95.3\%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.},
	language = {en},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, Wenjia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K},
	year = {2020},
	pages = {14},
	file = {Xu et al. - EarBuddy Enabling On-Face Interaction via Wireles.pdf:/Users/orsonxu/Zotero/storage/6BXBWNYE/Xu et al. - EarBuddy Enabling On-Face Interaction via Wireles.pdf:application/pdf},
}

@inproceedings{xu_hand_2018,
	address = {Barcelona Spain},
	title = {Hand range interface: information always at hand with a body-centric mid-air input surface},
	isbn = {978-1-4503-5898-9},
	shorttitle = {Hand range interface},
	url = {https://dl.acm.org/doi/10.1145/3229434.3229449},
	doi = {10.1145/3229434.3229449},
	abstract = {Most interfaces of our interactive devices such as phones and laptops are ﬂat and are built as external devices in our environment, disconnected from our bodies. Therefore, we need to carry them with us in our pocket or in a bag and accommodate our bodies to their design by sitting at a desk or holding the device in our hand. We propose Hand Range Interface, an input surface that is always at our ﬁngertips. This bodycentric interface is a semi-sphere attached to a user’s wrist, with a radius the same as the distance from the wrist to the index ﬁnger. We prototyped the concept in virtual reality and conducted a user study with a pointing task. The input surface can be designed as rotating with the wrist or ﬁxed relative to the wrist. We evaluated and compared participants’ subjective physical comfort level, pointing speed and pointing accuracy on the interface that was divided into 64 regions. We found that the interface whose orientation was ﬁxed had a much better performance, with 41.2\% higher average comfort score, 40.6\% shorter average pointing time and 34.5\% lower average error. Our results revealed interesting insights on user performance and preference of different regions on the interface. We concluded with a set of guidelines for future designers and developers on how to develop this type of new body-centric input surface.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services}},
	publisher = {ACM},
	author = {Xu, Xuhai and Dancu, Alexandru and Maes, Pattie and Nanayakkara, Suranga},
	month = sep,
	year = {2018},
	pages = {1--12},
	file = {Xu et al. - 2018 - Hand range interface information always at hand w.pdf:/Users/orsonxu/Zotero/storage/LMMAF9QS/Xu et al. - 2018 - Hand range interface information always at hand w.pdf:application/pdf},
}

@inproceedings{xu_understanding_2020,
	address = {Taipei Taiwan},
	title = {Understanding {User} {Behavior} {For} {Document} {Recommendation}},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380071},
	doi = {10.1145/3366423.3380071},
	abstract = {Personalized document recommendation systems aim to provide users with a quick shortcut to the documents they may want to access next, usually with an explanation about why the document is recommended. Previous work explored various methods for better recommendations and better explanations in different domains. However, there are few efforts that closely study how users react to the recommended items in a document recommendation scenario. We conducted a large-scale log study of users’ interaction behavior with the explainable recommendation on one of the largest cloud document platforms office.com. Our analysis reveals a number of factors, including display position, file type, authorship, recency of last access, and most importantly, the recommendation explanations, that are associated with whether users will recognize or open the recommended documents. Moreover, we specifically focus on explanations and conduct an online experiment to investigate the influence of different explanations on user behavior. Our analysis indicates that the recommendations help users access their documents significantly faster, but sometimes users miss a recommendation and resort to other more complicated methods to open the documents. Our results suggest opportunities to improve explanations and more generally the design of systems that provide and explain recommendations for documents.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of {The} {Web} {Conference}},
	publisher = {ACM},
	author = {Xu, Xuhai and Hassan Awadallah, Ahmed and T. Dumais, Susan and Omar, Farheen and Popp, Bogdan and Rounthwaite, Robert and Jahanbakhsh, Farnaz},
	month = apr,
	year = {2020},
	pages = {3012--3018},
	file = {Xu et al. - 2020 - Understanding User Behavior For Document Recommend.pdf:/Users/orsonxu/Zotero/storage/S6FXIJMC/Xu et al. - 2020 - Understanding User Behavior For Document Recommend.pdf:application/pdf},
}

@inproceedings{sra_galvr_2017,
	address = {Gothenburg Sweden},
	title = {{GalVR}: a novel collaboration interface using {GVS}},
	isbn = {978-1-4503-5548-3},
	shorttitle = {{GalVR}},
	url = {https://dl.acm.org/doi/10.1145/3139131.3141219},
	doi = {10.1145/3139131.3141219},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the 23rd {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology}},
	publisher = {ACM},
	author = {Sra, Misha and Xu, Xuhai and Maes, Pattie},
	month = nov,
	year = {2017},
	pages = {1--2},
	file = {Sra et al. - 2017 - GalVR a novel collaboration interface using GVS.pdf:/Users/orsonxu/Zotero/storage/LH4W46S2/Sra et al. - 2017 - GalVR a novel collaboration interface using GVS.pdf:application/pdf},
}

@article{xu_recognizing_2020,
	title = {Recognizing {Unintentional} {Touch} on {Interactive} {Tabletop}},
	volume = {4},
	issn = {2474-9567},
	url = {https://dl.acm.org/doi/10.1145/3381011},
	doi = {10.1145/3381011},
	abstract = {A multi-touch interactive tabletop is designed to embody the benefits of a digital computer within the familiar surface of a physical tabletop. However, the nature of current multi-touch tabletops to detect and react to all forms of touch, including unintentional touches, impedes users from acting naturally on them. In our research, we leverage gaze direction, head orientation and screen contact data to identify and filter out unintentional touches, so that users can take full advantage of the physical properties of an interactive tabletop, e.g., resting hands or leaning on the tabletop during the interaction. To achieve this, we first conducted a user study to identify behavioral pattern differences (gaze, head and touch) between completing usual tasks on digital versus physical tabletops. We then compiled our findings into five types of spatiotemporal features, and train a machine learning model to recognize unintentional touches with an F1 score of 91.3\%, outperforming the state-of-the-art model by 4.3\%. Finally we evaluated our algorithm in a real-time filtering system. A user study shows that our algorithm is stable and the improved tabletop effectively screens out unintentional touches, and provide more relaxing and natural user experience. By linking their gaze and head behavior to their touch behavior, our work sheds light on the possibility of future tabletop technology to improve the understanding of users' input intention.},
	language = {en},
	number = {1},
	urldate = {2021-10-07},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Xu, Xuhai and Yu, Chun and Wang, Yuntao and Shi, Yuanchun},
	month = mar,
	year = {2020},
	pages = {1--24},
	file = {Xu et al. - 2020 - Recognizing Unintentional Touch on Interactive Tab.pdf:/Users/orsonxu/Zotero/storage/SNGMSGNT/Xu et al. - 2020 - Recognizing Unintentional Touch on Interactive Tab.pdf:application/pdf},
}

@inproceedings{jahanbakhsh_effects_2020,
	address = {Vancouver BC Canada},
	title = {Effects of {Past} {Interactions} on {User} {Experience} with {Recommended} {Documents}},
	isbn = {978-1-4503-6892-6},
	url = {https://dl.acm.org/doi/10.1145/3343413.3377977},
	doi = {10.1145/3343413.3377977},
	abstract = {Recommender systems are commonly used in entertainment, news, e-commerce, and social media. Document recommendation is a new and under-explored application area, in which both re-finding and discovery of documents need to be supported. In this paper we provide an initial exploration of users’ experience with recommended documents, with a focus on how prior interactions influence recognition and interest. Through a field study of more than 100 users, we investigate the effects of past interactions with recommended documents on users’ recognition of, prior intent to open, and interest in the documents. We examined different presentations of interaction history, and the recency and richness of prior interaction. We found that presentation only influenced recognition time. Our findings also indicate that people are more likely to recognize documents they had accessed recently and to do so more quickly. Similarly, documents that people had interacted with more deeply were also more frequently and quickly recognized. However, people were more interested in older documents or those with which they had less involved interactions. This finding suggests that in addition to helping users quickly access documents they intend to re-find, document recommendation can add value in helping users discover other documents. Our results offer implications for designing document recommendation systems that help users fulfil different needs.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the 2020 {Conference} on {Human} {Information} {Interaction} and {Retrieval}},
	publisher = {ACM},
	author = {Jahanbakhsh, Farnaz and Awadallah, Ahmed Hassan and Dumais, Susan T. and Xu, Xuhai},
	month = mar,
	year = {2020},
	pages = {153--162},
	file = {Jahanbakhsh et al. - 2020 - Effects of Past Interactions on User Experience wi.pdf:/Users/orsonxu/Zotero/storage/8HI89RYX/Jahanbakhsh et al. - 2020 - Effects of Past Interactions on User Experience wi.pdf:application/pdf},
}

@inproceedings{chen_understanding_2021,
	address = {Virtual Event USA},
	title = {Understanding the {Design} {Space} of {Mouth} {Microgestures}},
	isbn = {978-1-4503-8476-6},
	url = {https://dl.acm.org/doi/10.1145/3461778.3462004},
	doi = {10.1145/3461778.3462004},
	abstract = {As wearable devices move toward the face (i.e. smart earbuds, glasses), there is an increasing need to facilitate intuitive interactions with these devices. Current sensing techniques can already detect many mouth-based gestures; however, users’ preferences of these gestures are not fully understood. In this paper, we investigate the design space and usability of mouth-based microgestures. We first conducted brainstorming sessions (N=16) and compiled an extensive set of 86 user-defined gestures. Then, with an online survey (N=50), we assessed the physical and mental demand of our gesture set and identified a subset of 14 gestures that can be performed easily and naturally. Finally, we conducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to various daily smartphone operations under a sitting and walking context. From these studies, we develop a taxonomy for mouth gestures, finalize a practical gesture set for common applications, and provide design guidelines for future mouth-based gesture interactions.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {Designing} {Interactive} {Systems} {Conference}},
	publisher = {ACM},
	author = {Chen, Victor and Xu, Xuhai and Li, Richard and Shi, Yuanchun and Patel, Shwetak and Wang, Yuntao},
	month = jun,
	year = {2021},
	pages = {1068--1081},
	file = {Chen et al. - 2021 - Understanding the Design Space of Mouth Microgestu.pdf:/Users/orsonxu/Zotero/storage/3E7HILPB/Chen et al. - 2021 - Understanding the Design Space of Mouth Microgestu.pdf:application/pdf},
}

@inproceedings{he_pneufetch_2020,
	address = {Honolulu HI USA},
	title = {{PneuFetch}: {Supporting} {Blind} and {Visually} {Impaired} {People} to {Fetch} {Nearby} {Objects} via {Light} {Haptic} {Cues}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {{PneuFetch}},
	url = {https://dl.acm.org/doi/10.1145/3334480.3383095},
	doi = {10.1145/3334480.3383095},
	abstract = {We present PneuFetch, a light haptic cue based wearable device that supports blind and visually impaired (BVI) people to fetch nearby objects in an unfamiliar environment. In our design, we generate friendly, non-intrusive, and gentle presses and drags to deliver direction and distance cues on BVI user’s wrist and forearm. As a concept of proof, we discuss our PneuFetch wearable prototype, contrast it with past work, and describe a preliminary user study.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Extended {Abstracts} {Proceedings} of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {He, Liang and Wang, Ruolin and Xu, Xuhai},
	month = apr,
	year = {2020},
	pages = {1--9},
	file = {He et al. - 2020 - PneuFetch Supporting Blind and Visually Impaired .pdf:/Users/orsonxu/Zotero/storage/URJFKBEZ/He et al. - 2020 - PneuFetch Supporting Blind and Visually Impaired .pdf:application/pdf},
}

@inproceedings{zhang_voicemoji_2021,
	address = {Yokohama Japan},
	title = {Voicemoji: {Emoji} {Entry} {Using} {Voice} for {Visually} {Impaired} {People}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Voicemoji},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445338},
	doi = {10.1145/3411764.3445338},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhang, Mingrui Ray and Wang, Ruolin and Xu, Xuhai and Li, Qisheng and Sharif, Ather and Wobbrock, Jacob O.},
	month = may,
	year = {2021},
	pages = {1--18},
	file = {Zhang et al. - 2021 - Voicemoji Emoji Entry Using Voice for Visually Im.pdf:/Users/orsonxu/Zotero/storage/WQHDQZ76/Zhang et al. - 2021 - Voicemoji Emoji Entry Using Voice for Visually Im.pdf:application/pdf},
}

@article{morris_college_2021,
	title = {College from home during {COVID}-19: {A} mixed-methods study of heterogeneous experiences},
	volume = {16},
	issn = {1932-6203},
	shorttitle = {College from home during {COVID}-19},
	url = {https://dx.plos.org/10.1371/journal.pone.0251580},
	doi = {10.1371/journal.pone.0251580},
	abstract = {This mixed-method study examined the experiences of college students during the COVID19 pandemic through surveys, experience sampling data collected over two academic quarters (Spring 2019 n1 = 253; Spring 2020 n2 = 147), and semi-structured interviews with 27 undergraduate students. There were no marked changes in mean levels of depressive symptoms, anxiety, stress, or loneliness between 2019 and 2020, or over the course of the Spring 2020 term. Students in both the 2019 and 2020 cohort who indicated psychosocial vulnerability at the initial assessment showed worse psychosocial functioning throughout the entire Spring term relative to other students. However, rates of distress increased faster in 2020 than in 2019 for these individuals. Across individuals, homogeneity of variance tests and multi-level models revealed significant heterogeneity, suggesting the need to examine not just means but the variations in individuals’ experiences. Thematic analysis of interviews characterizes these varied experiences, describing the contexts for students’ challenges and strategies. This analysis highlights the interweaving of psychosocial and academic distress: Challenges such as isolation from peers, lack of interactivity with instructors, and difficulty adjusting to family needs had both an emotional and academic toll. Strategies for adjusting to this new context included initiating remote study and hangout sessions with peers, as well as self-learning. In these and other strategies, students used technologies in different ways and for different purposes than they had previously. Supporting qualitative insight about adaptive responses were quantitative findings that students who used more problem-focused forms of coping reported fewer mental health symptoms over the course of the pandemic, even though they perceived their stress as more severe. These findings underline the need for interventions oriented towards problem-focused coping and suggest opportunities for peer role modeling.},
	language = {en},
	number = {6},
	urldate = {2021-10-07},
	journal = {PLOS ONE},
	author = {Morris, Margaret E. and Kuehn, Kevin S. and Brown, Jennifer and Nurius, Paula S. and Zhang, Han and Sefidgar, Yasaman S. and Xu, Xuhai and Riskin, Eve A. and Dey, Anind K. and Consolvo, Sunny and Mankoff, Jennifer C.},
	editor = {Webster, Amanda A.},
	month = jun,
	year = {2021},
	pages = {e0251580},
	file = {Morris et al. - 2021 - College from home during COVID-19 A mixed-methods.pdf:/Users/orsonxu/Zotero/storage/WZWX4W9J/Morris et al. - 2021 - College from home during COVID-19 A mixed-methods.pdf:application/pdf},
}

@article{xu_listen2cough_2021,
	title = {{Listen2Cough}: {Leveraging} {End}-to-{End} {Deep} {Learning} {Cough} {Detection} {Model} to {Enhance} {Lung} {Health} {Assessment} {Using} {Passively} {Sensed} {Audio}},
	volume = {5},
	issn = {2474-9567},
	shorttitle = {{Listen2Cough}},
	url = {https://dl.acm.org/doi/10.1145/3448124},
	doi = {10.1145/3448124},
	abstract = {The prevalence of ubiquitous computing enables new opportunities for lung health monitoring and assessment. In the past few years, there have been extensive studies on cough detection using passively sensed audio signals. However, the generalizability of a cough detection model when applied to external datasets, especially in real-world implementation, is questionable and not explored adequately. Beyond detecting coughs, researchers have looked into how cough sounds can be used in assessing lung health. However, due to the challenges in collecting both cough sounds and lung health condition ground truth, previous studies have been hindered by the limited datasets. In this paper, we propose Listen2Cough to address these gaps. We first build an end-to-end deep learning architecture using public cough sound datasets to detect coughs within raw audio recordings. We employ a pre-trained MobileNet and integrate a number of augmentation techniques to improve the generalizability of our model. Without additional fine-tuning, our model is able to achieve an F1 score of 0.948 when tested against a new clean dataset, and 0.884 on another in-the-wild noisy dataset, leading to an advantage of 5.8\% and 8.4\% on average over the best baseline model, respectively. Then, to mitigate the issue of limited lung health data, we propose to transform the cough detection task to lung health assessment tasks so that the rich cough data can be leveraged. Our hypothesis is that these tasks extract and utilize similar effective representation from cough sounds. We embed the cough detection model into a multi-instance learning framework with the attention mechanism and further tune the model for lung health assessment tasks. Our final model achieves an F1-score of 0.912 on healthy v.s. unhealthy, 0.870 on obstructive v.s. non-obstructive, and 0.813 on COPD v.s. asthma classification, outperforming the baseline by 10.7\%, 6.3\%, and 3.7\%, respectively. Moreover, the weight value in the attention layer can be used to identify important coughs highly correlated with lung health, which can potentially provide interpretability for expert diagnosis in the future. CCS Concepts: • Human-centered computing → Ubiquitous and mobile computing; • Applied computing → Life and medical sciences.},
	language = {en},
	number = {1},
	urldate = {2021-10-07},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Xu, Xuhai and Nemati, Ebrahim and Vatanparvar, Korosh and Nathan, Viswam and Ahmed, Tousif and Rahman, Md Mahbubur and McCaffrey, Daniel and Kuang, Jilong and Gao, Jun Alex},
	month = mar,
	year = {2021},
	pages = {1--22},
	file = {Xu et al. - 2021 - Listen2Cough Leveraging End-to-End Deep Learning .pdf:/Users/orsonxu/Zotero/storage/ACPJMCBB/Xu et al. - 2021 - Listen2Cough Leveraging End-to-End Deep Learning .pdf:application/pdf},
}

@article{xu_leveraging_2021,
	title = {Leveraging {Collaborative}-{Filtering} for {Personalized} {Behavior} {Modeling}: {A} {Case} {Study} of {Depression} {Detection} among {College} {Students}},
	volume = {5},
	issn = {2474-9567},
	shorttitle = {Leveraging {Collaborative}-{Filtering} for {Personalized} {Behavior} {Modeling}},
	url = {https://dl.acm.org/doi/10.1145/3448107},
	doi = {10.1145/3448107},
	abstract = {The prevalence of mobile phones and wearable devices enables the passive capturing and modeling of human behavior at an unprecedented resolution and scale. Past research has demonstrated the capability of mobile sensing to model aspects of physical health, mental health, education, and work performance, etc. However, most of the algorithms and models proposed in previous work follow a one-size-fits-all (i.e., population modeling) approach that looks for common behaviors amongst all users, disregarding the fact that individuals can behave very differently, resulting in reduced model performance. Further, black-box models are often used that do not allow for interpretability and human behavior understanding. We present a new method to address the problems of personalized behavior classification and interpretability, and apply it to depression detection among college students. Inspired by the idea of collaborative-filtering, our method is a type of memory-based learning algorithm. It leverages the relevance of mobile-sensed behavior features among individuals to calculate personalized relevance weights, which are used to impute missing data and select features according to a specific modeling goal (e.g., whether the student has depressive symptoms) in different time epochs, i.e., times of the day and days of the week. It then compiles features from epochs using majority voting to obtain the final prediction. We apply our algorithm on a depression detection dataset collected from first-year college students with low data-missing rates and show that our method outperforms the state-of-the-art machine learning model by 5.1\% in accuracy and 5.5\% in F1 score. We further verify the pipeline-level generalizability of our approach by achieving similar results on a second dataset, with an average improvement of 3.4\% across performance metrics. Beyond achieving better classification performance, our novel approach is further able to generate personalized interpretations of the models for each individual. These interpretations are supported by existing depression-related literature and can potentially inspire automated and personalized depression intervention design in the future.},
	language = {en},
	number = {1},
	urldate = {2021-10-07},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Xu, Xuhai and Chikersal, Prerna and Dutcher, Janine M. and Sefidgar, Yasaman S. and Seo, Woosuk and Tumminia, Michael J. and Villalba, Daniella K. and Cohen, Sheldon and Creswell, Kasey G. and Creswell, J. David and Doryab, Afsaneh and Nurius, Paula S. and Riskin, Eve and Dey, Anind K. and Mankoff, Jennifer},
	month = mar,
	year = {2021},
	pages = {1--27},
	file = {Xu et al. - 2021 - Leveraging Collaborative-Filtering for Personalize.pdf:/Users/orsonxu/Zotero/storage/AX2ZQTNL/Xu et al. - 2021 - Leveraging Collaborative-Filtering for Personalize.pdf:application/pdf},
}

@inproceedings{wu_lightwrite_2021,
	address = {Yokohama Japan},
	title = {{LightWrite}: {Teach} {Handwriting} to {The} {Visually} {Impaired} with {A} {Smartphone}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{LightWrite}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445322},
	doi = {10.1145/3411764.3445322},
	abstract = {Learning to write is challenging for blind and low vision (BLV) people because of the lack of visual feedback. Regardless of the drastic advancement of digital technology, handwriting is still an essential part of daily life. Although tools designed for teaching BLV to write exist, many are expensive and require the help of sighted teachers. We propose LightWrite, a low-cost, easy-to-access smartphone application that uses voice-based descriptive instruction and feedback to teach BLV users to write English lowercase letters and Arabian digits in a specifically designed font. A two-stage study with 15 BLV users with little prior writing knowledge shows that LightWrite can successfully teach users to learn handwriting characters in an average of 1.09 minutes for each letter. After initial training and 20-minute daily practice for 5 days, participants were able to write an average of 19.9 out of 26 letters that are recognizable by sighted raters.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wu, Zihan and Yu, Chun and Xu, Xuhai and Wei, Tong and Zou, Tianyuan and Wang, Ruolin and Shi, Yuanchun},
	month = may,
	year = {2021},
	pages = {1--15},
	file = {Wu et al. - 2021 - LightWrite Teach Handwriting to The Visually Impa.pdf:/Users/orsonxu/Zotero/storage/7I53F874/Wu et al. - 2021 - LightWrite Teach Handwriting to The Visually Impa.pdf:application/pdf},
}

@inproceedings{liang_authtrack_2021,
	address = {Yokohama Japan},
	title = {Auth+{Track}: {Enabling} {Authentication} {Free} {Interaction} on {Smartphone} by {Continuous} {User} {Tracking}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Auth+{Track}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445624},
	doi = {10.1145/3411764.3445624},
	abstract = {We propose Auth+Track, a novel authentication model that aims to reduce redundant authentication in everyday smartphone usage. By sparse authentication and continuous tracking of the user’s status, Auth+Track eliminates the "gap" authentication between fragmented sessions and enables "Authentication Free when User is Around". To instantiate the Auth+Track model, we present PanoTrack, a prototype that integrates body and near feld hand information for user tracking. We install a fsheye camera on the top of the phone to achieve a panoramic vision that can capture both user’s body and on-screen hands. Based on the captured video stream, we develop an algorithm to extract 1) features for user tracking, including body keypoints and their temporal and spatial association, near feld hand status, and 2) features for user identity assignment. The results of our user studies validate the feasibility of PanoTrack and demonstrate that Auth+Track not only improves the authentication efciency but also enhances user experiences with better usability.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Liang, Chen and Yu, Chun and Wei, Xiaoying and Xu, Xuhai and Hu, Yongquan and Wang, Yuntao and Shi, Yuanchun},
	month = may,
	year = {2021},
	pages = {1--16},
	file = {Liang et al. - 2021 - Auth+Track Enabling Authentication Free Interacti.pdf:/Users/orsonxu/Zotero/storage/JSZI8L3S/Liang et al. - 2021 - Auth+Track Enabling Authentication Free Interacti.pdf:application/pdf},
}

@inproceedings{liu_metaphys_2021,
	address = {Virtual Event USA},
	title = {{MetaPhys}: few-shot adaptation for non-contact physiological measurement},
	isbn = {978-1-4503-8359-2},
	shorttitle = {{MetaPhys}},
	url = {https://dl.acm.org/doi/10.1145/3450439.3451870},
	doi = {10.1145/3450439.3451870},
	abstract = {There are large individual differences in physiological processes, making designing personalized health sensing algorithms challenging. Existing machine learning systems struggle to generalize well to unseen subjects or contexts and can often contain problematic biases. Video-based physiological measurement is no exception. Therefore, learning personalized or customized models from a small number of unlabeled samples is very attractive as it would allow fast calibrations to improve generalization and help correct biases. In this paper, we present a novel meta-learning approach called MetaPhys for personalized video-based cardiac measurement for non-contact pulse and heart rate monitoring. Our method uses only 18-seconds of video for customization and works effectively in both supervised and unsupervised manners. We evaluate our approach on two benchmark datasets and demonstrate superior performance in cross-dataset evaluation with substantial reductions (42\% to 44\%) in errors compared with state-of-the-art approaches. We also find that our method leads to large reductions in bias due to skin type.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {ACM},
	author = {Liu, Xin and Jiang, Ziheng and Fromm, Josh and Xu, Xuhai and Patel, Shwetak and McDuff, Daniel},
	month = apr,
	year = {2021},
	pages = {154--163},
	file = {Liu et al. - 2021 - MetaPhys few-shot adaptation for non-contact phys.pdf:/Users/orsonxu/Zotero/storage/6P28KGE9/Liu et al. - 2021 - MetaPhys few-shot adaptation for non-contact phys.pdf:application/pdf},
}

@article{wang_hearcough_2022,
	title = {{HearCough}: {Enabling} continuous cough event detection on edge computing hearables},
	volume = {205},
	issn = {1046-2023},
	shorttitle = {{HearCough}},
	url = {https://www.sciencedirect.com/science/article/pii/S1046202322001165},
	doi = {10.1016/j.ymeth.2022.05.002},
	abstract = {Cough event detection is the foundation of any measurement associated with cough, one of the primary symptoms of pulmonary illnesses. This paper proposes HearCough, which enables continuous cough event detection on edge computing hearables, by leveraging always-on active noise cancellation (ANC) microphones in commodity hearables. Specifically, we proposed a lightweight end-to-end neural network model — Tiny-COUNET and its transfer learning based traning method. When evaluated on our acted cough event dataset, Tiny-COUNET achieved equivalent detection performance but required significantly less computational resources and storage space than cutting-edge cough event detection methods. Then we implemented HearCough by quantifying and deploying the pre-trained Tiny-COUNET to a popular micro-controller in consumer hearables. Lastly, we evaluated that HearCough is effective and reliable for continuous cough event detection through a field study with 8 patients. HearCough achieved 2 Hz cough event detection with an accuracy of 90.0\% and an F1-score of 89.5\% by consuming an additional 5.2 mW power. We envision HearCough as a low-cost add-on for future hearables to enable continuous cough detection and pulmonary health monitoring.},
	language = {en},
	urldate = {2022-07-10},
	journal = {Methods},
	author = {Wang, Yuntao and Zhang, Xiyuxing and Chakalasiya, Jay M. and Xu, Xuhai and Jiang, Yu and Li, Yuang and Patel, Shwetak and Shi, Yuanchun},
	month = sep,
	year = {2022},
	pages = {53--62},
}

@inproceedings{xu_typeout_2022,
	address = {New Orleans LA USA},
	title = {{TypeOut}: {Leveraging} {Just}-in-{Time} {Self}-{Affirmation} for {Smartphone} {Overuse} {Reduction}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{TypeOut}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517476},
	doi = {10.1145/3491102.3517476},
	abstract = {Smartphone overuse is related to a variety of issues such as lack of sleep and anxiety. We explore the application of Self-Afrmation Theory on smartphone overuse intervention in a just-in-time manner. We present TypeOut, a just-in-time intervention technique that integrates two components: an in-situ typing-based unlock process to improve user engagement, and self-afrmation-based typing content to enhance efectiveness. We hypothesize that the integration of typing and self-afrmation content can better reduce smartphone overuse. We conducted a 10-week within-subject feld experiment (N=54) and compared TypeOut against two baselines: one only showing the self-afrmation content (a common notifcation-based intervention), and one only requiring typing non-semantic content (a state-of-the-art method). TypeOut reduces app usage by over 50\%, and both app opening frequency and usage duration by over 25\%, all signifcantly outperforming baselines. TypeOut can potentially be used in other domains where an intervention may beneft from integrating self-afrmation exercises with an engaging just-in-time mechanism.},
	language = {en},
	urldate = {2022-07-10},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Zou, Tianyuan and Xiao, Han and Li, Yanzhang and Wang, Ruolin and Yuan, Tianyi and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K},
	month = apr,
	year = {2022},
	pages = {1--17},
	file = {Xu et al. - 2022 - TypeOut Leveraging Just-in-Time Self-Affirmation .pdf:/Users/orsonxu/Zotero/storage/TA6VE4BN/Xu et al. - 2022 - TypeOut Leveraging Just-in-Time Self-Affirmation .pdf:application/pdf},
}

@inproceedings{xu_enabling_2022,
	address = {New Orleans LA USA},
	title = {Enabling {Hand} {Gesture} {Customization} on {Wrist}-{Worn} {Devices}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501904},
	doi = {10.1145/3491102.3501904},
	abstract = {We present a framework for gesture customization requiring minimal examples from users, all without degrading the performance of existing gesture sets. To achieve this, we frst deployed a large-scale study (N=500+) to collect data and train an accelerometer-gyroscope recognition model with a cross-user accuracy of 95.7\% and a falsepositive rate of 0.6 per hour when tested on everyday non-gesture data. Next, we design a few-shot learning framework which derives a lightweight model from our pre-trained model, enabling knowledge transfer without performance degradation. We validate our approach through a user study (N=20) examining on-device customization from 12 new gestures, resulting in an average accuracy of 55.3\%, 83.1\%, and 87.2\% on using one, three, or fve shots when adding a new gesture, while maintaining the same recognition accuracy and false-positive rate from the pre-existing gesture set. We further evaluate the usability of our real-time implementation with a user experience study (N=20). Our results highlight the efectiveness, learnability, and usability of our customization framework.},
	language = {en},
	urldate = {2022-07-10},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Gong, Jun and Brum, Carolina and Liang, Lilian and Suh, Bongsoo and Gupta, Shivam Kumar and Agarwal, Yash and Lindsey, Laurence and Kang, Runchang and Shahsavari, Behrooz and Nguyen, Tu and Nieto, Heriberto and Hudson, Scott E and Maalouf, Charlie and Mousavi, Jax Seyed and Laput, Gierad},
	month = apr,
	year = {2022},
	pages = {1--19},
	file = {Xu et al. - 2022 - Enabling Hand Gesture Customization on Wrist-Worn .pdf:/Users/orsonxu/Zotero/storage/XUC3K2JT/Xu et al. - 2022 - Enabling Hand Gesture Customization on Wrist-Worn .pdf:application/pdf},
}

@inproceedings{nemati_ubilung_2022,
	title = {Ubilung: {Multi}-{Modal} {Passive}-{Based} {Lung} {Health} {Assessment}},
	shorttitle = {Ubilung},
	doi = {10.1109/ICASSP43922.2022.9746614},
	abstract = {Lung health assessment is traditionally done mainly through X-ray images and spirometry tests which are time-consuming, cumbersome, and costly. In this paper, we investigate the potential of passively recordable contents such as speech, cough and heart signal for such an assessment. Our regression model is the first in the literature to achieve mean absolute error (MAE) of 7.47\% for estimation of forced expiratory volume in 1 sec. (FEV1) over forced vital capacity (FVC) ratio using these contents. This is comparable to the state of the art active phone-based spirometry methods. Additionally our classification models achieve a F1-score of 0.982 for healthy v.s. diseased, 0.881 for obstructive v.s. non-obstructive, 0.854 for chronic obstructive pulmonary disease (COPD) v.s. asthma, and 0.892 for severe v.s. non-severe obstruction classification.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Nemati, Ebrahim and Xu, Xuhai and Nathan, Viswam and Vatanparvar, Korosh and Ahmed, Tousif and Rahman, Md. Mahbubur and McCaffrey, Dan and Kuang, Jilong and Gao, Alex},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	pages = {551--555},
}

@inproceedings{zhang_boldmove_2022,
	address = {New Orleans LA USA},
	title = {{BoldMove}: {Enabling} {IoT} {Device} {Control} on {Ubiquitous} {Touch} {Interfaces} by {Semantic} {Mapping} and {Sequential} {Selection}},
	isbn = {978-1-4503-9156-6},
	shorttitle = {{BoldMove}},
	url = {https://dl.acm.org/doi/10.1145/3491101.3519805},
	doi = {10.1145/3491101.3519805},
	abstract = {Recent advances in ultra-low-power ubiquitous touch interfaces make touch inputs possible anytime, anywhere. However, their functions are usually pre-determined, i.e., one button is only associated with one fixed function. BoldMove enables spontaneous and efficient association of touch inputs and IoT device functions with semantic-based function filtering and a wait-confirm sequential selection strategy. In this way, such touch interfaces become ubiquitous IoT device controllers. We proposed the semantic-based IoT function filtering to improve control efficiency, then designed the sequential selection mechanism for interfaces with constrained input and output resources. We implemented BoldMove on a custom-built touch interface with capacitive button inputs and a smartwatch display. We then conducted a user study to determine the design parameters for the sequential selection method. At last, we validated that BoldMove only takes 3.25 seconds to complete a selection task if the target function appears within the Top-3 displayed item. Even if the assumption is relaxed to Top-10, BoldMove is still estimated to be more efficient than the conventional selection method with device-based filtering and menu-navigated selection.},
	language = {en},
	urldate = {2022-07-10},
	booktitle = {Extended {Abstracts} {Proceedings} of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhang, Tengxiang and Zeng, Xin and Zhang, Yinshuai and Jiang, Xin and Xu, Xuhai and Dey, Anind K and Chen, Yiqiang},
	month = apr,
	year = {2022},
	pages = {1--7},
	file = {Zhang et al. - 2022 - BoldMove Enabling IoT Device Control on Ubiquitou.pdf:/Users/orsonxu/Zotero/storage/TH7P7844/Zhang et al. - 2022 - BoldMove Enabling IoT Device Control on Ubiquitou.pdf:application/pdf},
}

@article{zhang_impact_2022,
	title = {Impact of {Online} {Learning} in the {Context} of {COVID}-19 on {Undergraduates} with {Disabilities} and {Mental} {Health} {Concerns}},
	issn = {1936-7228, 1936-7236},
	url = {https://dl.acm.org/doi/10.1145/3538514},
	doi = {10.1145/3538514},
	abstract = {The COVID-19 pandemic upended college education and the experiences of students due to the rapid and uneven shift to online learning. This study examined the experiences of students with disabilities with online learning, with a consideration of surrounding stressors such as financial pressures. In a mixed method approach, we compared 28 undergraduate students with disabilities (including mental health concerns) to their peers during 2020, to assess differences and similarities in their educational concerns, stress levels and COVID-19 related adversities. We found that students with disabilities entered the Spring quarter of 2020 with significantly higher concerns about classes going online, and reported more recent negative life events than other students. These differences between the two groups diminished three months later with the exception of recent negative life events. For a fuller understanding of students’ experiences, we conducted qualitative analysis of open ended interviews. We examined both positive and negative experiences with online learning among students with disabilities and mental health concerns. We describe how online learning enabled greater access–e.g., reducing the need for travel to campus–alongside ways in which online learning impeded academic engagement–e.g., reducing interpersonal interaction. We highlight a need for learning systems to meet the diverse and dynamic needs of students with disabilities.},
	language = {en},
	urldate = {2022-10-01},
	journal = {ACM Transactions on Accessible Computing},
	author = {Zhang, Han and Morris, Margaret E. and Nurius, Paula S. and Mack, Kelly and Brown, Jennifer and Kuehn, Kevin S. and Sefidgar, Yasaman S. and Xu, Xuhai and Riskin, Eve A. and Dey, Anind K. and Mankoff, Jennifer},
	month = jul,
	year = {2022},
	pages = {3538514},
	file = {Zhang et al. - 2022 - Impact of Online Learning in the Context of COVID-.pdf:/Users/orsonxu/Zotero/storage/KSW3T3G8/Zhang et al. - 2022 - Impact of Online Learning in the Context of COVID-.pdf:application/pdf},
}

@inproceedings{zhang_how_2020,
	title = {How {Does} {COVID}-19 impact {Students} with {Disabilities}/{Health} {Concerns}?},
	url = {http://arxiv.org/abs/2005.05438},
	abstract = {The impact of COVID-19 on students has been enormous, with an increase in worries about fiscal and physical health, a rapid shift to online learning, and increased isolation. In addition to these changes, students with disabilities/health concerns may face accessibility problems with online learning or communication tools, and their stress may be compounded by additional risks such as financial stress or pre-existing conditions. To our knowledge, no one has looked specifically at the impact of COVID-19 on students with disabilities/health concerns. In this paper, we present data from a survey of 147 students with and without disabilities collected in late March to early April of 2020 to assess the impact of COVID-19 on these students' education and mental health. Our findings show that students with disabilities/health concerns were more concerned about classes going online than their peers without disabilities. In addition, students with disabilities/health concerns also reported that they have experienced more COVID-19 related adversities compared to their peers without disabilities/health concerns. We argue that students with disabilities/health concerns in higher education need confidence in the accessibility of the online learning tools that are becoming increasingly prevalent in higher education not only because of COVID-19 but also more generally. In addition, educational technologies will be more accessible if they consider the learning context, and are designed to provide a supportive, calm, and connecting learning environment.},
	urldate = {2022-10-01},
	booktitle = {{arXiv}},
	publisher = {arXiv},
	author = {Zhang, Han and Nurius, Paula and Sefidgar, Yasaman and Morris, Margaret and Balasubramanian, Sreenithi and Brown, Jennifer and Dey, Anind K. and Kuehn, Kevin and Riskin, Eve and Xu, Xuhai and Mankoff, Jen},
	month = may,
	year = {2020},
	note = {arXiv:2005.05438 [cs]},
	annote = {Comment: 15 pages},
	file = {Zhang et al_2021_How Does COVID-19 impact Students with Disabilities-Health Concerns.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Zhang et al_2021_How Does COVID-19 impact Students with Disabilities-Health Concerns.pdf:application/pdf},
}

@inproceedings{xu_globem_2022,
	title = {{GLOBEM} {Dataset}: {Multi}-{Year} {Datasets} for {Longitudinal} {Human} {Behavior} {Modeling} {Generalization}},
	abstract = {Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique users’ data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms’ generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.},
	language = {en},
	booktitle = {Thirty-sixth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track}},
	author = {Xu, Xuhai and Zhang, Han and Sefidgar, Yasaman and Ren, Yiyi and Liu, Xin and Seo, Woosuk and Brown, Jennifer and Kuehn, Kevin and Merrill, Mike and Nurius, Paula and Patel, Shwetak and Althoff, Tim and Morris, Margaret E and Riskin, Eve and Mankoff, Jennifer and Dey, Anind K},
	year = {2022},
	pages = {18},
	file = {Xu et al. - GLOBEM Dataset Multi-Year Datasets for Longitudin.pdf:/Users/orsonxu/Zotero/storage/3K7MCSA6/Xu et al. - GLOBEM Dataset Multi-Year Datasets for Longitudin.pdf:application/pdf},
}

@article{xu_globem_2022-1,
	title = {{GLOBEM}: {Cross}-{Dataset} {Generalization} of {Longitudinal} {Human} {Behavior} {Modeling}},
	volume = {6},
	language = {en},
	number = {4},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Xu, Xuhai and Liu, Xin and Zhang, Han and Wang, Weichen and Nepal, Subgiya and Kuehn, Kevin S and Huckins, Jeremy and Morris, Margaret E and Nurius, Paula S and Riskin, Eve A and Patel, Shwetak and Althoff, Tim and Campell, Andrew and Dey, Anind K and Mankoff, Jennifer},
	year = {2022},
	pages = {32},
	file = {Xu et al. - GLOBEM Cross-Dataset Generalization of Longitudin.pdf:/Users/orsonxu/Zotero/storage/QURA3CEN/Xu et al. - GLOBEM Cross-Dataset Generalization of Longitudin.pdf:application/pdf},
}

@inproceedings{xu_xair_2023,
	title = {{XAIR}: {A} {Framework} of {Explainable} {AI} in {Everyday} {Augmented} {Reality}},
	booktitle = {Proceedings of the {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Xu, Xuhai and Yu, Anna and Jonker, Tanya and Todi, Kashyap and Lu, Feiyu and Qian, Xun and Belo, João Marcelo Evangelista and Wang, Tianyi and Li, Michelle and Mun, Aran and Wu, Te-Yen and Shen, Junxiao and Zhang, Ting and Kokhlikyan, Narine and Wang, Fulton and Sorenson, Paul and Kim, Sophie and Benko, Hrvoje},
	year = {2023},
}

@article{adiba_orzikulova_typeout_2023,
	title = {{TypeOut}+: {Transparent}, {Just}-in-{Time}, {Adaptive} {Intervention} for {Smartphone} {Overuse} {Reduction}},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {{Adiba Orzikulova} and {Han Xiao} and {Zhipeng Li} and {ZIhan Yan} and {Yukang Yan} and {Yuanchun Shi} and {Jennifer Mankoff} and {Sung-Ju Lee} and {Anind K. Dey} and {Yuntao Wang} and {Xuhai Xu}},
	year = {2023},
}

@inproceedings{sharif_unlockedmaps_2022,
	address = {Athens Greece},
	title = {{UnlockedMaps}: {Visualizing} {Real}-{Time} {Accessibility} of {Urban} {Rail} {Transit} {Using} a {Web}-{Based} {Map}},
	isbn = {978-1-4503-9258-7},
	shorttitle = {{UnlockedMaps}},
	url = {https://dl.acm.org/doi/10.1145/3517428.3550397},
	doi = {10.1145/3517428.3550397},
	abstract = {Current web-based maps do not provide visibility into real-time elevator outages at urban rail transit stations, disenfranchising commuters (e.g., wheelchair users) who rely on functioning elevators at transit stations. In this paper, we demonstrate UnlockedMaps, an open-source and open-data web-based map that visualizes the real-time accessibility of urban rail transit stations in six North American cities, assisting users in making informed decisions regarding their commute. Specifcally, UnlockedMaps uses a map to display transit stations, prominently highlighting their real-time accessibility status (accessible with functioning elevators, accessible but experiencing at least one elevator outage, or not-accessible) and surrounding accessible restaurants and restrooms. UnlockedMaps is the frst system to collect elevator outage data from 2,336 transit stations over 23 months and make it publicly available via an API. We report on results from our pilot user studies with fve stakeholder groups: (1) people with mobility disabilities; (2) pregnant people; (3) cyclists/stroller users/commuters with heavy equipment; (4) members of disability advocacy groups; and (5) civic hackers.},
	language = {en},
	urldate = {2022-11-16},
	booktitle = {The 24th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Sharif, Ather and Ramesh, Aneesha and Nguyen, Trung-Anh and Chen, Luna and Zeng, Kent Richard and Hou, Lanqing and Xu, Xuhai},
	month = oct,
	year = {2022},
	pages = {1--7},
	file = {Sharif et al. - 2022 - UnlockedMaps Visualizing Real-Time Accessibility .pdf:/Users/orsonxu/Zotero/storage/8X7C4P2S/Sharif et al. - 2022 - UnlockedMaps Visualizing Real-Time Accessibility .pdf:application/pdf},
}

@inproceedings{zhuang_reflectrack_2021,
	address = {Virtual Event USA},
	title = {{ReflecTrack}: {Enabling} {3D} {Acoustic} {Position} {Tracking} {Using} {Commodity} {Dual}-{Microphone} {Smartphones}},
	isbn = {978-1-4503-8635-7},
	shorttitle = {{ReflecTrack}},
	url = {https://dl.acm.org/doi/10.1145/3472749.3474805},
	doi = {10.1145/3472749.3474805},
	language = {en},
	urldate = {2022-11-16},
	booktitle = {Proceedings of {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Zhuang, Yuzhou and Wang, Yuntao and Yan, Yukang and Xu, Xuhai and Shi, Yuanchun},
	month = oct,
	year = {2021},
	pages = {1050--1062},
	file = {Zhuang et al. - 2021 - ReflecTrack Enabling 3D Acoustic Position Trackin.pdf:/Users/orsonxu/Zotero/storage/QHM9E2KR/Zhuang et al. - 2021 - ReflecTrack Enabling 3D Acoustic Position Trackin.pdf:application/pdf},
}

@inproceedings{rahman_detecting_2022,
	title = {Detecting {Physiological} {Responses} {Using} {Multimodal} {Earbud} {Sensors}},
	doi = {10.1109/EMBC48229.2022.9871569},
	abstract = {Continuous stress exposure negatively impacts mental and physical well-being. Physiological arousal due to stress affects heartbeat frequency, changes breathing pattern and peripheral temperature, among several other bodily responses. Traditionally stress detection is performed by collecting signals such as electrocardiogram (ECG), respiration, and skin conductance response using uncomfortable sensors such as a chestband. In this study, we use earbuds that passively measure photoplethysmography (PPG), core body temperature, and inertial measurements. We have conducted a lab study exposing 18 participants to an evaluated speech task and additional tasks aimed at increasing stress or promoting relaxation. We simultaneously collected PPG, ECG, impedance cardiography (ICG), and blood pressure using laboratory grade equipment as reference measurements. We show that the earbud PPG sensor can reliably capture heart rate and heart rate variability. We further show that earbud signals can be used to classify the physiological responses associated with stress with 91.30\% recall, 80.52\% precision, and 85.12\% F1-score using a random forest classifier with leave-one-subject-out cross-validation. The accuracy can further be improved through multi-modal sensing. These findings demonstrate the feasibility of using earbuds for passively monitoring users' physiological responses.},
	booktitle = {2022 44th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
	author = {Rahman, Md Mahbubur and Xu, Xuhai and Nathan, Viswam and Ahmed, Tousif and Ahmed, Mohsin Yusuf and McCaffrey, Dan and Kuang, Jilong and Cowell, Trevor and Moore, Julia and Mendes, Wendy Berry and Gao, Jun Alex},
	month = jul,
	year = {2022},
	note = {ISSN: 2694-0604},
	pages = {01--05},
	file = {Rahman et al_2022_Detecting Physiological Responses Using Multimodal Earbud Sensors.pdf:/Users/orsonxu/Desktop/HCI/Literature Management/Zotero/Rahman et al_2022_Detecting Physiological Responses Using Multimodal Earbud Sensors.pdf:application/pdf},
}

@article{jin_earcommand_2022,
	title = {{EarCommand}: "{Hearing}" {Your} {Silent} {Speech} {Commands} {In} {Ear}},
	volume = {6},
	issn = {2474-9567},
	shorttitle = {{EarCommand}},
	url = {https://dl.acm.org/doi/10.1145/3534613},
	doi = {10.1145/3534613},
	abstract = {Intelligent speech interfaces have been developing vastly to support the growing demands for convenient control and interaction with wearable/earable and portable devices. To avoid privacy leakage during speech interactions and strengthen the resistance to ambient noise, silent speech interfaces have been widely explored to enable people's interaction with mobile/wearable devices without audible sounds. However, most existing silent speech solutions require either restricted background illuminations or hand involvement to hold device or perform gestures. In this study, we propose a novel earphone-based, hand-free silent speech interaction approach, named EarCommand. Our technique discovers the relationship between the deformation of the ear canal and the movements of the articulator and takes advantage of this link to recognize different silent speech commands. Our system can achieve a WER (word error rate) of 10.02\% for word-level recognition and 12.33\% for sentence-level recognition, when tested in human subjects with 32 word-level commands and 25 sentence-level commands, which indicates the effectiveness of inferring silent speech commands. Moreover, EarCommand shows high reliability and robustness in a variety of configuration settings and environmental conditions. It is anticipated that EarCommand can serve as an efficient, intelligent speech interface for hand-free operation, which could significantly improve the quality and convenience of interactions.},
	language = {en},
	number = {2},
	urldate = {2022-11-16},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Jin, Yincheng and Gao, Yang and Xu, Xuhai and Choi, Seokmin and Li, Jiyang and Liu, Feng and Li, Zhengxiong and Jin, Zhanpeng},
	month = jul,
	year = {2022},
	pages = {1--28},
	file = {Jin et al. - 2022 - EarCommand Hearing Your Silent Speech Commands .pdf:/Users/orsonxu/Zotero/storage/FWSMP8LK/Jin et al. - 2022 - EarCommand Hearing Your Silent Speech Commands .pdf:application/pdf},
}

@article{li_diversense_2022,
	title = {{DiverSense}: {Maximizing} {Wi}-{Fi} {Sensing} {Range} {Leveraging} {Signal} {Diversity}},
	volume = {6},
	issn = {2474-9567},
	shorttitle = {{DiverSense}},
	url = {https://dl.acm.org/doi/10.1145/3536393},
	doi = {10.1145/3536393},
	abstract = {The ubiquity of Wi-Fi infrastructure has facilitated the development of a range of Wi-Fi based sensing applications. Wi-Fi sensing relies on weak signal reflections from the human target and thus only supports a limited sensing range, which significantly hinders the real-world deployment of the proposed sensing systems. To extend the sensing range, traditional algorithms focus on suppressing the noise introduced by the imperfect Wi-Fi hardware. This paper picks a different direction and proposes to enhance the quality of the sensing signal by fully exploiting the signal diversity provided by the Wi-Fi hardware. We propose DiverSense, a system that combines sensing signal received from all subcarriers and all antennas in the array, to fully utilize the spatial and frequency diversity. To guarantee the diversity gain after signal combining, we also propose a time-diversity based signal alignment algorithm to align the phase of the multiple received sensing signals. We implement the proposed methods in a respiration monitoring system using commodity Wi-Fi devices and evaluate the performance in diverse environments. Extensive experimental results demonstrate that DiverSense is able to accurately monitor the human respiration even when the sensing signal is under noise floor, and therefore boosts sensing range to 40 meters, which is a 3x improvement over the current state-of-the-art. DiverSense also works robustly under NLoS scenarios, e.g., DiverSense is able to accurately monitor respiration even when the human and the Wi-Fi transceivers are separated by two concrete walls with wooden doors.},
	language = {en},
	number = {2},
	urldate = {2022-11-16},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Li, Yang and Wu, Dan and Zhang, Jie and Xu, Xuhai and Xie, Yaxiong and Gu, Tao and Zhang, Daqing},
	month = jul,
	year = {2022},
	pages = {1--28},
	file = {Li et al. - 2022 - DiverSense Maximizing Wi-Fi Sensing Range Leverag.pdf:/Users/orsonxu/Zotero/storage/A8RHQERV/Li et al. - 2022 - DiverSense Maximizing Wi-Fi Sensing Range Leverag.pdf:application/pdf},
}
