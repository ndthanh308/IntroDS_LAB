\section{Results}
\label{sec:results}
We summarize our experiment results with zero-shot prompting (Sec.~\ref{sub:results:zero-shot}), few-shot prompting (Sec.~\ref{sub:results:few-shot}), and instruction finetuning (Sec.~\ref{sub:results:finetune}).
% Add a paragraph of summary

\subsection{Zero-shot Prompting Shows Promising yet Limited Performance}
\label{sub:results:zero-shot}
% Are LLMs Good at Mental Health Prediction with Zero-shot Prompting?
% (Zero-shot Prompting Shows Promising yet Limited Performance)

We start with the most basic zero-shot prompting with Alpaca, Alpaca-LoRA, and GPT-3.5.
The balanced accuracy results are summarized in the first two sections of Table~\ref{tab:results_overall}.
Alpaca and Alpaca-LoRA achieve better overall performance than the naive majority baseline ($\overline{\Delta}_{\text{Alpaca}} = 5.5\%$, $\overline{\Delta}_{\text{Alpaca-LoRA}} = 5.6\%$).
With a much larger model GPT-3.5, the performance is getting more promising ($\overline{\Delta}_{\text{GPT-3.5}} = 12.3\%$ over baseline), which is inline with previous work~\cite{yang_evaluations_2023}. GPT-3.5's advantage over Alpaca and Alpaca-LoRA is expected due to its bigger size (25$\times$).
In general, these results indicate the promising capability of LLMs on mental health prediction tasks, even without any domain-specific information.
However, compared to task-specific baseline models BERT and Mental-RoBERTa (which have 15\%-25\% advantages), these task-agnostic models are still limited and have room for improvement.

\input{fig_tab_alg/tab_results_overall}

\subsubsection{The Effectiveness of Enhancement Strategies.}
In Sec.~\ref{sub:methods:zero-shot}, we propose context enhancement, mental health enhancement, and their combination strategies for zero-shot prompt design to provide more information about the domain. Interestingly, our results suggest varied effectiveness on different LLMs and datasets.

\input{fig_tab_alg/tab_results_delta}

Table~\ref{tab:results_zeroshot_delta} provides a zoom-in summary of the zero-shot part in Table~\ref{tab:results_overall}.
For Alpaca and GPT-3.5, the three strategies improved the performance in general ($\overline{\Delta}_{\text{Alpaca}} = 1.0\%$, 13 out of 18 tasks show positive changes; $\overline{\Delta}_{\text{GPT-3.5}} = 2.8\%$, 12 out of 18 tasks show positive changes).
However, for Alpaca-LoRA, adding more context or mental health domain information would reduce the model performance ($\overline{\Delta}_{\text{Alpaca-LoRA}} = -2.7\%$). This may be because Alpaca-LoRA is trained with a smaller number of parameters, so its ability to understand context or domain could be limited.

The effectiveness of strategies on different datasets/tasks also varies. We observe that the CSSRS-Suicide dataset benefits the most from the enhancement. In particular, task \#6 can benefit from almost all strategies across all models.
Besides, GPT-3.5 benefits very significantly from enhancement on SDCNL Task \#4 ($\overline{\Delta}_{\text{GPT-3.5}-\text{SDCNL}} = 14.8\%$).
These could be caused by the different nature of datasets. Our results suggest that these enhancement strategies are more effective for critical action prediction (e.g., suicide) than mental state prediction (e.g., stress and depression).

We also compare the effectiveness of different strategies on Alpaca and GPT-3.5. The context enhancement strategy has the most stable improvement across all mental health prediction tasks ($\overline{\Delta}_{\text{Alpaca}-context} = 2.1\%$, 6/6 tasks positive; $\overline{\Delta}_{\text{GPT-3.5}-context} = 2.5\%$, 5/6 tasks positive).
Comparatively, the mental health enhancement strategy is less effective ($\overline{\Delta}_{\text{Alpaca}-mh} = 1.1\%$, 5/6 tasks positive; $\overline{\Delta}_{\text{GPT-3.5}-mh} = 2.1\%$, 3/6 tasks positive).
The combination of the two strategies shows diverse results on Alpaca and GPT-3.5. It has the most significant improvement on GPT-3.5's performance, but not on all tasks ($\overline{\Delta}_{\text{GPT-3.5}-both} = 3.8\%$, 4/6 tasks positive), but it has little impact on the average performance of Alpaca ($\overline{\Delta}_{\text{Alpaca}-both} = 0.0\%$, only 2/6 tasks positive).
This indicates that larger language models have a strong capability to leverage the information embedded in the prompts.


We summarize our \textbf{key takeaways} from this section:
\textbf{
\begin{s_itemize}
\item Both small-scale and large-scale LLMs show promising performance on mental health tasks, but their ability are still not comparable to task-specific NLP models.
\item The prompt design enhancement strategies are generally effective, but they work better for critical action prediction tasks such as suicide prediction.
\item Providing more contextual information about the task \& input can almost always improve performance.
\item Models with larger trainable parameters (Alpaca \vs Alpaca-LoRA, and GPT-3.5 \vs Alpaca) can better leverage the contextual or domain information in the prompts.
\end{s_itemize}
}

\subsection{Few-shot Prompting Improves Performance to Some Extent}
\label{sub:results:few-shot}
% Does Few-shot Prompting Improve The Performance?
We then investigate the effectiveness of few-shot prompting.
Note that since we observe diverse effect of prompt design strategies in Table~\ref{tab:results_zeroshot_delta}, in this section, we only experiment with the prompts with the best performance in the zero-shot setting. Moreover, we mainly focus on Alpaca and GPT-3.5, as Alpaca-LoRA has shown less promising results in the previous section.

Due to the maximum input token length of Alpaca (2048), we focus on Dreaddit and DepSeverity datasets that have a shorter input and experiment with $M=2$ in Eq.~\ref{eq:prompt-fs} for binary classification and $M=4$ for multi-class classification, \ie one sample per class. We repeat the experiment on each tasks three times and randomize the few shot samples for each run.

We summarize the results in the middle section of Table~\ref{tab:results_overall}. 
Although language models with few-shot prompting still underperform task-specific models, providing examples of the task can improve model performance on most tasks compared to zero-shot prompting ($\overline{\Delta}_{FS\text{\_}vs\text{\_}ZS} = 4.7\%$).
Interestingly, few-shot prompting is more effective for Alpaca than GPT-3.5
($\overline{\Delta}_{\text{Alpaca}-FS} = 8.1\%$, 3/3 tasks positive; $\overline{\Delta}_{\text{GPT-3.5}-FS} = 1.2\%$, 2/3 tasks positive).
Especially for the multi-class classification Task \#3, we observe an improved balanced accuracy of 19.7\% for Alpaca but a decline of 2.3\% for GPT-3.5, so that Alpaca outperforms GPT-3.5 on this task.
This may be because for more complex tasks, small models like Alpaca is easy to quickly adapt to a new task, even by just seeing a few examples. While large models like GPT-3.5 has a lot thing ``in memory'' and is harder to learn from examples in a fast way.

This leads to the key message from this experiment:
\textbf{Few-shot prompting can boost the performance of LLMs on mental health prediction tasks, especially for small models on complex tasks.} 

\subsection{Instruction Finetuning Boost Performance for Multiple Tasks Simultaneously}
\label{sub:results:finetune}
% How Effective is Instruction Finetuning?
% (Instruction Finetuning Boost Performance for Multiple Tasks Simultaneously)
Our experiments so far have shown that zero-shot and few-shot prompting can improve LLMs on mental health tasks to some extent, but their performance is still far from state-of-the-art task-specific models.
In this section, we explore the effectiveness of instruction finetuning.

Due to the prohibitive cost and lack of transparency of GPT-3.5 finetuning, we only experiment with Alpaca of which we have full control.
Similar to picking the best prompts for few-shot prompting in Sec.~\ref{sub:results:few-shot}, we also use the best prompts in zero-shot prompting during the instruction finetuning.
As introduced in Sec.~\ref{subsub:implementation:models:mental}, we build Mental-Alpaca by finetuning Alpaca on all six tasks across four datasets at the same time.

The last section of Table.~\ref{tab:results_overall} summarizes the results. We observe that Mental-Alpaca achieves significantly better performance compared to few-shot Alpaca ($\overline{\Delta}_{FT\_vs\_ZS}$ = 23.4\%, $\overline{\Delta}_{FT\_vs\_FS}$ = 18.3\%) or GPT-3.5 ($\overline{\Delta}_{FT\_vs\_ZS}$ = 16.7\%, $\overline{\Delta}_{FT\_vs\_FS}$ = 12.4\%). Recall that GPT-3.5 is 25 times bigger than our finetuned model.

More importantly, Mental-Alpaca performs on par with the state-of-the-art Mental-RoBERTa. Mental-Alpaca has the best performance on three of six tasks, and the second best on the other three. It is noteworthy that Mental-RoBERTa is a task-specific model, which means it is specialized on one task after being trained on it. In contrast, Mental-Alpaca can simultaneously work across \textit{all} tasks with a single-round finetuning.
These results show the strong effectiveness of instruction finetuning: By finetuning LLMs on multiple mental health datasets with instructions, the models' domain knowledge will be improved significantly, leading to better capability to solve a variety of mental health prediction tasks.

\subsubsection{Does Finetuning Generalize across Datasets?}
\label{subsub:results:finetune:generalizability}
% Are Models Generalizable across Datasets/Tasks?
% (Domain Generalizability Still Needs Improvement)
In addition to evaluating Mental-Alpaca, we further measure the generalizability of LLMs after finetuning. To do this, we instruction-finetune Alpaca on one dataset and evaluate it on all datasets. Table~\ref{tab:results_transfer} summarizes the results.

\input{fig_tab_alg/tab_transfer}

We first find that finetuning and testing on the same dataset lead to good performance (the ones marked as \fbox{box} on the diagonal of Table~\ref{tab:results_transfer}. Some results are even better than Mental-Alpaca (5 out of 6 tasks) or Mental-RoBERTa (3 out of 6 tasks), which is not surprising.

More interestingly, we investigate cross-dataset generalization performance (\ie the ones off the diagonal).
Overall, finetuning on a single dataset achieves better performance compared to the zero-shot setting ($\overline{\Delta}_{FT\text{-Single}\_vs\_ZS}$ = 4.2\%).
However, the performance changes vary across tasks. For example, finetuning on any dataset is beneficial for Task \#3 ($\overline{\Delta}$ = 19.2\%) and \#5 ($\overline{\Delta}$ = 16.4\%), but detrimental for Task \#6 ($\overline{\Delta}$ = -7.6\%) and almost futile for Task \#4 ($\overline{\Delta}$ = -0.4\%). Generalizing across Dreaddit and DepSeverity shows good performance, but this is mainly because they share the language corpus.
These results indicate that finetuning on a single dataset can provide mental health domain knowledge with a certain level and thus improving the overall generalization results. But such improvement is not stable across tasks.

\subsubsection{How Much Data Is Needed?}
\label{subsub:results:finetune:datasetsize}
From a different perspective, we are also interested in how the size of the dataset would impact the instruction finetuning results. To answer this question, we downsample the training set to 50\%, 20\%, 10\%, 5\%, and 1\% of the original size and repeat each one three times. We increase the training epoch accordingly to make sure that the model observes a similar amount of data. Figure~\ref{fig:performance_datasize} visualizes the results.

\input{fig_tab_alg/fig_datasize}

With only 1\% of the data, the finetuned model is able to outperform the zero-shot model on the majority tasks (4 out of 6). With 5\% of the data, the finetuned model has a better performance on all tasks.
As expected, the model performance has an increasing trend with more training data. The trend approaches a plateau after 20\%. The difference between 20\% training data (less than 500 samples per dataset) and 100\% training data is small ($\overline{\Delta}$ = 1.8\%).

In Sec.~\ref{subsub:results:finetune:generalizability}, the finetuning on a single dataset can also be viewed as a smaller overall training set (around 5-25\% of the total size) but with less variation (\ie no finetuning across datasets).
Comparing those results against those in Figure~\ref{fig:performance_datasize}, we found that the model's overall performance is clearly better when the model is finetuned across multiple datasets ($\overline{\Delta}_{FT\text{-}5\%\_vs\_FT\text{-Single}}$ = 3.8\%, $\overline{\Delta}_{FT\text{-}10\%\_vs\_FT\text{-Single}}$ = 8.1\%, $\overline{\Delta}_{FT\text{-}20\%\_vs\_FT\text{-Single}}$ = 12.4\%)
These results can guide the future developer and practitioner in collecting the appropriate data size to finetune LLMs for the mental health domain efficiently.

In summary, we highlight the \textbf{key takeaways} from this section:
\textbf{
\begin{s_itemize}
\item Instruction finetuning on multiple mental health datasets can significantly boost the performance of LLMs on various mental health prediction tasks. Mental-Alpaca performs on par with the state-of-the-art task-specific model.
\item Finetuning LLMs on a small number of datasets may improve model generalizable knowledge in mental health, but its effect is not robust.
\item Finetuning LLMs on a small number of samples (a few hundred) across multiple datasets can already achieve favorable performance.
\item When the data size is the same, finetuning LLMs on data with larger variation (\ie more datasets and tasks) can achieve better performance.
\end{s_itemize}
}