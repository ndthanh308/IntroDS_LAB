\section{Discussion}
\label{sec:discussion}
Our experiment results reveal a number of interesting findings. In this section, we discuss potential guidelines for enabling LLMs for mental -health-related tasks (Sec.~\ref{sub:discussion:guidelines}). We also envision promising future directions (Sec.~\ref{sub:discussion:beyond}) and summarize the limitations of the current work (Sec.~\ref{sub:discussion:limitation}).

\subsection{Guidelines for Empowering LLMs for Mental Health Prediction Tasks}
\label{sub:discussion:guidelines}
We extract and summarize the takeaways from Sec.~\ref{sec:results} into a set of guidelines for future researchers and practitioners on how to empower LLMs with more mental health domain knowledge and become an expert for various mental health prediction tasks via online text data.

\textbf{When computing resources are limited, combine prompt design \& few-shot prompting and pick prompts carefully.}
As the size of large models continues to grow, the requirement for hardware (mainly GPU) has also been increasing, especially when finetuning a model. For example, Alpaca was trained on eight 80GB A100s for three hours~\cite{taori_stanford_2023}.
When there are limited computing resources, only running inference or resorting to API is possible. In these cases, zero-shot and few-shot prompt engineering strategies are useful. Our results indicate that providing few-shot mental health examples with appropriate enhancement strategies can effectively improve prediction performance.
Specifically, adding contextual information about the online text data is always helpful. If the available model is large and contains rich knowledge (at least 7B trainable parameters), adding mental health domain information is also beneficial.

% Although there has been some recent research exploring low-cost training/finetuning for LLMs (\eg LoRA~\cite{hu_lora_2021}), it is

\textbf{With enough computing resources, instruction finetune models on various mental health datasets.}
When there are enough computing resources and model training/finetuning is possible, there are more options to enhance LLMs for mental health prediction tasks.
Our experiments clearly show that instruction finetuning can significantly boost the performance of models. If there are multiple datasets available, merging multiple datasets and tasks altogether and finetuning the model in a single round is the most effective approach.

\textbf{Implement efficient finetuning with hundreds of examples and prioritize data variation when data resource is limited.}
Figure~\ref{fig:performance_datasize} shows that finetuning does not require large datasets. If there is no immediately available dataset, collecting small datasets with a few hundred samples is often good enough.
Moreover, when the overall dataset size is fixed (\eg due to limited data collection resources), collecting more datasets (each with a small size) is more meaningful than collecting one large dataset, as instruction finetuning works better when data has a larger variation. 

\textbf{Carefully calibrate the model when applying it to a new task.}
After instruction finetuning, a model still needs careful scrutiny on a new task, since our experiments suggest that LLMs' generalizability on unseen tasks still needs further improvement.
Testing the model on a few samples with ground truth can be an easy option to investigate its performance on the new task. If the performance is not promising, it may suggest another finetuning on the new task.

\subsection{Beyond Mental Health Prediction Task and Textual Data}
\label{sub:discussion:beyond}
Our current experiments mainly involve mental health prediction tasks, which are essentially classification problems. There are more types of tasks that our experiments don't cover, such as regression (\eg predicting a score on a mental health scale) and reasoning (\eg explaining why a person appears to have mental health issues). In particular, reasoning is an attractive task as it can fully leverage the capability of LLMs on language generation~\cite{nori_capabilities_2023,bubeck_sparks_2023}. We plan to conduct more experiments on tasks that go beyond classification.
Meanwhile, we mainly focus on online text data in this paper. But there are more data streams available, such as sensor data from mobile phones and wearables~\cite{xu_globem_2022,xu_globem_2022-1}. This leads to another open question on how to leverage LLMs for time-series data, another exciting direction to explore in future work.

\subsection{Limitations}
\label{sub:discussion:limitation}
Our paper has a few limitations. First, although we inspect the quality of our dataset and cover different categories of LLM, the number of datasets and the types of LLMs are still limited. Our findings are based on the observations of these datasets and models, which may not generalize to other cases.
Related, our exploration of zero-shot few-shot prompt design is not comprehensive. For example, our experiments only involve two templates for each enhancement strategy, and the question design part also only involves three questions.
Moreover, the limited input window of Alpaca also limits our exploration of more samples for few-shot prompting.
Future work can design larger-scale experiments to include more datasets, models, and prompt designs.