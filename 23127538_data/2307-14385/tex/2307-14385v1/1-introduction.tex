\section{Introduction}
\label{sec:introduction}

The recent surge of Large Language Models (LLMs), such as GPT-3.5/4~\cite{bubeck_sparks_2023}, PaLM~\cite{chowdhery_palm_2022}, FLAN-T5~\cite{chung_scaling_2022}, and Alpaca~\cite{taori_stanford_2023}, has shown a promising trend of large pre-trained models to do a variety of tasks in a zero-shot setting (\ie without any new training data). Example tasks include question answering~\cite{omar2023chatgpt,robinson2023leveraging}, logic reasoning~\cite{wei_chain--thought_2023,zhou_least--most_2023}, machine translation~\cite{brants2007large,gulcehre2017integrating} \etc\ 
A number of experiments have revealed that, built on hundreds of billions of parameters, these LLMs have started to show the capability to understand the human common sense beneath the natural language and do proper reasoning and inference accordingly~\cite{bubeck_sparks_2023,nori_capabilities_2023}.

Among different applications, one particular question yet to be answered is how well LLMs can understand human mental health states through natural language.
Mental health problems represent a significant burden for individuals and societies worldwide.
A recent report suggested that more than 20\% of adults in the U.S. would experience at least one mental disorder in their lifetime~\cite{mental2022state} and 5.6\% of adults experienced a serious psychotic disorder that significantly impairs functioning~\cite{mental2023stats}. The global economy loses around \$1 trillion annually in productivity due to depression and anxiety alone~\cite{mentalcost2023}.

In the past decade, there has been a plethora of research in natural language processing (NLP) and computational social science on detecting mental health issues via online text data such as social media~(\eg \cite{guntuku_detecting_2017,eichstaedt2018facebook,coppersmith_clpsych_2015,de_choudhury_social_2013,de_choudhury_mental_2014}). However, most of these studies have focused on building domain-specific machine learning (ML) models (\ie one model for one particular task, such as stress detection~\cite{nijhawan2022stress,guntuku2019understanding}, depression prediction~\cite{eichstaedt2018facebook,tadesse2019detection,xu_leveraging_2019}, or suicide risk assessment~\cite{de_choudhury_discovering_2016,coppersmith2018natural}). Even for traditional pre-trained language models such as BERT, it needs to be finetuned for specific downstream tasks~\cite{devlin_bert_2019,liu_roberta_2019}.
Since natural language is a major component of mental health assessment and treatment~\cite{sharma2018mental,gkotsis2016language}, LLMs might be a potentially powerful tool to understand end-users' mental states based on the language users' wrote. These instruction-finetuned and general-purpose models can understand a variety of inputs and obviate the need to train multiple models for different tasks. Thus, we can envision using one LLM for a variety of mental-health-related tasks, such as multiple question-answering, reasoning, and inference.
Such a vision opens up a wide range of opportunities for UbiComp, Human-Computer Interaction (HCI), and mental health communities, such as online public health monitoring systems~\cite{patel2018psyheal,graham2019artificial}, intelligent assistants for mental counselors and supporters~\cite{sharma_towards_2021,sharma_humanai_2023}, mental-health-aware personal chatbots~\cite{abd2021perceptions,denecke2020mental}, to just name a few.
However, there is a lack of investigation into understanding, evaluating, and improving the capability of LLMs for mental health prediction tasks.

There are few very recent studies on the evaluation of LLMs (\eg ChatGPT) on mental-health-related tasks, most of which are in zero-shot settings with simple prompt engineering~\cite{yang_evaluations_2023,amin_will_2023,lamichhane_evaluation_2023}. Researchers have shown preliminary results that LLMs have some initial capability of predicting mental health disorders with natural language with some promising but still limited performance compared to state-of-the-art domain-specific NLP models~\cite{yang_evaluations_2023,lamichhane_evaluation_2023}.
This remaining gap is expected since existing general-purpose LLMs are not specifically trained on mental health tasks.
However, to achieve our vision of leveraging LLMs for mental health support and assistance, we need to answer the research question: \textbf{How to empower LLMs with more mental health domain knowledge and become an expert}?

We conducted a series of experiments with multiple LLMs, including Alpaca~\cite{noauthor_stanford_2023}, Alpaca-LoRA~\cite{hu_lora_2021}, and GPT-3.5~\cite{noauthor_introducing_2022}.
Considering the data availability, we focused on online social media data with high-quality human-generated mental health labels.
Our experiments contained three stages: (1) zero-shot prompting, where we experimented with various prompts related to mental health, (2) few-shot prompting, where we inserted examples into prompt inputs, and (3) instruction-finetuning, where we finetuned LLMs on multiple mental-health datasets with various tasks.

Our results indicate that zero-shot obtained promising but limited performance on multiple mental health prediction tasks across all models. GPT-3.5 had relatively better results since it has a larger scale. But their performance is still far from task-specific models. 
Meanwhile, providing a few shots in the prompt can improve the model performance to some extent ($\overline{\Delta}$ = 4.7\%), but the advantage is limited.
Finally and most importantly, we found that instruction-finetuning can significantly improve the model performance across multiple mental-health-related tasks at the same time. Our finetuned Alpaca, namely \textbf{Mental-Alpaca}, significantly outperforms the original GPT-3.5 ($\times$25 times of model size) by an average of 16.7\% on balance accuracy. 
Meanwhile, Mental-Alpaca can further perform on par with the task-specific state-of-the-art Mental-RoBERTa~\cite{ji_mentalbert_2021}. It is noteworthy that Mental-RoBERTa needs to be trained on each task individually, 
while our Mental-Alpaca can solve different tasks off the shelf. 
% We open-source our training code and model at [github link].
Our experiments present the first comprehensive evaluation of various techniques to enhance LLMs' capability in the mental health domain.

The contribution of our paper can be summarized as follows:
\begin{s_enumerate}
\item We present the first comprehensive evaluation of prompt engineering, few-shot, and finetuning techniques on multiple LLMs in the mental health domain.
\item With online social media data, our results reveal that finetuning on a variety of datasets can significantly improve LLM's capability on multiple mental-health-specific tasks simultaneously.
% We release our model \textbf{Mental-Alpaca} as the first open-source LLM targeted at mental health prediction tasks.
\item We provide a few technical guidelines for future researchers and developers on turning LLMs into experts in specific domains.
\end{s_enumerate}
