\section{Conclusion}
\label{sec:conclusion}
In this paper, we present the first comprehensive evaluation of multiple LLMs (Alpaca, Alpaca-LoRA, GPT-3.5) on mental health prediction tasks (binary and multi-class classification) via online text data. 
Our experiments cover zero-shot prompting, few-shot prompting, and instruction finetuning. The results reveal a number of interesting findings.
Our context enhancement strategy can robustly improve performance for all LLMs, and our mental health enhancement strategy can enhance models with large number of trainable parameters.
Meanwhile, few-shot prompting can also robustly improve model performance even by providing just one example per class.
Most importantly, our experiments show that instruction finetuning across multiple datasets can significantly boost model performance on various mental health prediction tasks at the same time. Our best finetuned model Mental-Alpaca performs on par with the state-of-the-art task-specific model Mental-RoBERTa.
We summarize our findings as a set of guidelines for future researchers, developers, and practitioners who want to empower LLMs with better knowledge of mental health for downstream tasks.



% \section*{ACKNOWLEDGMENTS}
