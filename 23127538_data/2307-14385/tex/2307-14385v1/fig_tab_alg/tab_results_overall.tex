\begin{table}[!t]
\centering
\caption{
Balanced Accuracy Performance Summary of Zero-shot, Few-shot and Instruction Finetuning on LLMs. 
$context$, $mh$, and $both$ indicates the prompt design strategies of context enhancement, mental health enhancement, and their combination  (see Table.~\ref{tab:prompt_design}).
Small numbers represents standard deviation across different designs of \textit{PromptPart1-S} and \textit{PromptPart2-Q}. The baselines at top rows do not have standard deviation as the task-specific output is static and prompt designs do not apply.
For Alpaca and Alpaca-LoRA, the maximum input token length os 2048, thus we can only conduct few-shot prompting on a subset of datasets and other infeasible datasets are shown with ``--''.
}
\label{tab:results_overall}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{llcccccc}
\thickhlinespace
& \makecell[r]{\textbf{Dataset}} & \textbf{Dreaddit} & \multicolumn{2}{c}{\textbf{DepSeverity}} & \textbf{SDCNL} & \multicolumn{2}{c}{\textbf{CSSRS-Suicide}} \\ \addlinespace[1ex]
\textbf{Category} & \textbf{Model}   & \textbf{Task \#1}       & \textbf{Task \#2}             & \textbf{Task \#3}             & \textbf{Task \#4}    & \textbf{Task \#5}     & \textbf{Task \#6} \\ \thickhlinespace
\multirow{3}{*}{Baseline} & Majority          & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.250$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.200$_{\pm ---}$  \\
& BERT              & 0.783$_{\pm ---}$  & 0.763$_{\pm ---}$  & 0.690$_{\pm ---}$  & 0.678$_{\pm ---}$  & 0.500$_{\pm ---}$  & 0.332$_{\pm ---}$  \\
& Mental-RoBERTa     & \textbf{0.831}$_{\pm ---}$  & \textbf{0.790}$_{\pm ---}$   & \underline{0.736}$_{\pm ---}$  & \underline{0.723}$_{\pm ---}$  & \textbf{0.853}$_{\pm ---}$  & \underline{0.373}$_{\pm ---}$  \\ \thickhlinespace
\multirow{13}{*}{\makecell{Zero-shot\\Prompting}} & Alpaca$_{ZS}$         & 0.593$_{\pm0.039}$ & 0.522$_{\pm0.022}$ & 0.431$_{\pm0.050}$ & 0.493$_{\pm0.007}$ & 0.518$_{\pm0.037}$ & 0.232$_{\pm0.076}$ \\
& Alpaca$_{ZS\_context}$ & 0.612$_{\pm0.065}$ & 0.567$_{\pm0.077}$ & 0.454$_{\pm0.143}$ & 0.497$_{\pm0.006}$ & 0.532$_{\pm0.033}$ & 0.250$_{\pm0.060}$ \\
& Alpaca$_{ZS\_mh}$      & 0.593$_{\pm0.031}$ & 0.577$_{\pm0.028}$ & 0.444$_{\pm0.090}$ & 0.482$_{\pm0.015}$ & 0.523$_{\pm0.013}$ & 0.235$_{\pm0.033}$ \\
& Alpaca$_{ZS\_both}$    & 0.540$_{\pm0.029}$ & 0.559$_{\pm0.040}$ & 0.421$_{\pm0.095}$ & 0.532$_{\pm0.005}$ & 0.511$_{\pm0.011}$ & 0.221$_{\pm0.030}$ \\ \cdashlinespace{2-8}
& Alpaca-LoRA$_{ZS}$           & 0.571$_{\pm0.043}$ & 0.548$_{\pm0.027}$ & 0.437$_{\pm0.044}$ & 0.502$_{\pm0.011}$ & 0.540$_{\pm0.012}$ & 0.187$_{\pm0.053}$ \\
& Alpaca-LoRA$_{ZS-context}$   & 0.537$_{\pm0.047}$ & 0.501$_{\pm0.001}$ & 0.343$_{\pm0.152}$ & 0.472$_{\pm0.020}$ & 0.567$_{\pm0.038}$ & 0.214$_{\pm0.059}$ \\
& Alpaca-LoRA$_{ZS\_mh}$        & 0.500$_{\pm0.000}$ & 0.500$_{\pm0.000}$ & 0.331$_{\pm0.145}$ & 0.497$_{\pm0.025}$ & 0.557$_{\pm0.023}$ & 0.216$_{\pm0.022}$ \\
& Alpaca-LoRA$_{ZS\_both}$      & 0.500$_{\pm0.000}$ & 0.500$_{\pm0.000}$ & 0.386$_{\pm0.059}$ & 0.499$_{\pm0.023}$ & 0.517$_{\pm0.031}$ & 0.224$_{\pm0.049}$ \\ \cdashlinespace{2-8}
& GPT-3.5$_{ZS}$         & 0.685$_{\pm0.024}$ & 0.642$_{\pm0.017}$ & 0.603$_{\pm0.017}$ & 0.460$_{\pm0.163}$ & 0.570$_{\pm0.118}$ & 0.233$_{\pm0.009}$ \\
& GPT-3.5$_{ZS-context}$  & 0.688$_{\pm0.045}$ & 0.653$_{\pm0.020}$ & 0.543$_{\pm0.047}$ & 0.618$_{\pm0.008}$ & 0.577$_{\pm0.090}$ & 0.265$_{\pm0.048}$ \\
& GPT-3.5$_{ZS\_mh}$      & 0.679$_{\pm0.017}$ & 0.636$_{\pm0.021}$ & 0.642$_{\pm0.034}$ & 0.576$_{\pm0.001}$ & 0.477$_{\pm0.014}$ & 0.310$_{\pm0.015}$ \\
& GPT-3.5$_{ZS\_both}$    & 0.681$_{\pm0.010}$ & 0.627$_{\pm0.022}$ & 0.617$_{\pm0.014}$ & 0.632$_{\pm0.020}$ & 0.617$_{\pm0.033}$ & 0.254$_{\pm0.009}$  \\ \thickhlinespace
\multirow{2}{*}{\makecell{Few-shot\\Prompting}} &  Alpaca$_{FS}$         & 0.632$_{\pm0.030}$ & 0.529$_{\pm0.017}$ & 0.628$_{\pm0.005}$ & ---                & ---                & ---                \\
& GPT-3.5$_{FS}$         & 0.721$_{\pm0.010}$ & 0.665$_{\pm0.015}$ & 0.580$_{\pm0.002}$ & ---                & ---                & ---                \\ \thickhlinespace
% Alpaca$_{FT}$         & 0.808$_{\pm0.004}$ & 0.779$_{\pm0.004}$ & 0.740$_{\pm0.004}$ & 0.729$_{\pm0.008}$ & 0.682$_{\pm0.010}$ & 0.401$_{\pm0.017}$ \\
% Alpaca$_{FT\_context}$ & 0.811$_{\pm0.003}$ & 0.777$_{\pm0.004}$ & 0.743$_{\pm0.004}$ & 0.725$_{\pm0.003}$ & 0.729$_{\pm0.016}$ & 0.413$_{\pm0.039}$ \\
% Alpaca$_{FT\_mh}$      & 0.821$_{\pm0.002}$ & 0.774$_{\pm0.007}$ & 0.748$_{\pm0.002}$ & 0.724$_{\pm0.001}$ & 0.692$_{\pm0.020}$ & 0.383$_{\pm0.009}$ \\
% Alpaca$_{FT\_both}$    & 0.819$_{\pm0.001}$ & 0.774$_{\pm0.007}$ & 0.750$_{\pm0.003}$ & 0.722$_{\pm0.002}$ & 0.793$_{\pm0.025}$ & 0.413$_{\pm0.032}$ \\
\makecell{Instructional\\Finetuning} & Mental-Alpaca         & \underline{0.816}$_{\pm0.006}$ & \underline{0.775}$_{\pm0.006}$ & \textbf{0.746}$_{\pm0.005}$ & \textbf{0.724}$_{\pm0.004}$ & \underline{0.730}$_{\pm0.048}$ & \textbf{0.403}$_{\pm0.029}$ \\
\thickhlinespace
\end{tabular}
}
\end{table}