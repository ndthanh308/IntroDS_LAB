\section{Methods}
\label{sec:methods}
We introduce our experiment design with LMMs on multiple mental health prediction task setups, including zero-shot prompting (Sec.~\ref{sub:methods:zero-shot}), few-shot prompting (Sec.~\ref{sub:methods:few-shot}), and notably, instruction finetuning (Sec.~\ref{sub:methods:finetuning}). These setups are model-agnostic, and we will present the details of language models and datasets employed for our experiment in the next section.

\subsection{Zero-shot Prompting}
\label{sub:methods:zero-shot}
The language understanding and reasoning capability of LLMs have enabled a wide range of applications without the need for any domain-specific data, but only providing appropriate prompts~\cite{kojima2022large,wei2021finetuned}.
Therefore, we start with prompt design for mental health tasks in a zero-shot setting.

The goal of prompt design is to empower a pre-trained general-purpose LLM to achieve good performance on tasks in the mental health domain. We propose a general zero-shot prompt template ($\textit{Prompt}_{ZS}$) that consists of four parts:
\begin{equation}
    \textit{Prompt}_{ZS} = \textit{TextData} + \textit{PromptPart1-S} + \textit{PromptPart2-Q} + \textit{OutputConstraint}
\label{eq:prompt-zs}
\end{equation}
where \textit{TextData} is the online text data generated by end-users. \textit{PromptPart1-S} provides specifications for a mental health recognition target. \textit{PromptPart2-Q} poses the question for LLMs to answer. And \textit{OutputConstraint} controls the output of models (\eg ``Only return yes or no'' for a binary task).

We propose several design strategies for \textit{PromptPart1-S}, as shown in the top part of Table~\ref{tab:prompt_design}: (1) \textbf{Basic}, leaving it as blank; (2) \textbf{Context Enhancement}, providing more social media context about the \textit{TextData}; (3) \textbf{Mental Health Enhancement}, inserting mental health concept by asking the model to act as an expert. (4) \textbf{Context \& Mental Health Enhancement}, combining both enhancement strategies by asking the model to act as a mental health expert under the social media context.

As for \textit{PromptPart2-Q}, we mainly focus on two categories of mental health prediction targets: (1) predicting certain mental states, such as stress or depression, and (2) predicting certain risk actions, such as suicide. We tailor the question description for each category. Moreover, for both categories, we explore binary and multi-class classification tasks
% \footnote{We also conduct priliminary experiments on mental health reasoning tasks. Yet there is no standard evaluation metrics to report the results. Please see more details in Sec.XX.}
. Thus, we also make small modifications based on the task. Given a specific dataset and task (see Sec.~\ref{sec:implementation}), we apply appropriate questions. The bottom part of Table~\ref{tab:prompt_design} summarizes the mapping.

\input{fig_tab_alg/tab_prompt_design}

For both \textit{PromptPart1-S} and \textit{PromptPart2-Q}, we propose several versions to improve its variation. We then evaluate these prompts on multiple LLMs on different datasets and compare their performance.

\subsection{Few-shot Prompting}
\label{sub:methods:few-shot}

In order to provide more domain-specific information, researchers have also explored few-shot prompting with LLMs (\eg \cite{agrawal2022large,dang2022prompt}). Note that these few shots are only used in prompts and model parameters are frozen. The intuition is to present a few ``examples'' for the model to learn domain-specific knowledge \textit{in situ}.
In our setting, we also test this strategy by adding additional randomly sampled [$\textit{Prompt}_{ZS} - \textit{label}$] pairs. The design of the few-shot prompt ($\textit{Prompt}_{FS}$) is straightforward:
\begin{equation}
    \textit{Prompt}_{FS} = [\textit{Sample Prompt}_{ZS} - \textit{label}]_{M} + \textit{Prompt}_{ZS}
\label{eq:prompt-fs}
\end{equation}
where $M$ is the number of prompt-label pairs and is capped by the input length limit of a model. Note that both the $\textit{Sample Prompt}_{ZS}$ and $\textit{Prompt}_{ZS}$ follow Eq.~\ref{eq:prompt-zs} and employ the same design of \textit{PromptPart1-S} and \textit{PromptPart2-Q} to ensure consistency.

\subsection{Instruction Finetuning}
\label{sub:methods:finetuning}

In contrast to the few-shot prompting strategy in Sec.~\ref{sub:methods:few-shot}, the goal of this strategy is closer to the traditional few-shot transfer learning, where we further train the model with a small amount of domain-specific data (\eg \cite{huang2022large,xu2021raise,liu_large_2023}).
We experiment with multiple finetuning strategies.

\subsubsection{Single-dataset Finetuning}
\label{subsub:methods:finetuning:single-dataset}
Following most of the previous work in the mental health field~\cite{yang_evaluations_2023,coppersmith_clpsych_2015,de_choudhury_discovering_2016}, we first conduct basic finetuning on a single dataset (the training set). This finetuned model can be tested on the same dataset (the test set) to evaluate its performance and different datasets to evaluate its generalizability.

\subsubsection{Multi-dataset Finetuning}
\label{subsub:methods:finetuning:multi-dataset}
From Sec.~\ref{sub:methods:zero-shot} to Sec.~\ref{subsub:methods:finetuning:single-dataset}, we have been focusing on one single mental health dataset $D$.
More interestingly, we further experiment with finetuning across multiple datasets simultaneously. Specifically, we leverage instruction finetuning to enable LLMs to handle multiple tasks in different datasets~\cite{brown_language_2020}.

It is noteworthy that such an instruction finetuning setup is very different from the state-of-the-art mental-health-specific models (\eg Mental-RoBERTa~[]). The previous models are finetuned for a specific task, such as depression prediction or suicidal ideation prediction. Once the model is trained on task A, it becomes task-A-specific and is unable to work on task B, unless it is finetuned on task B again. In contrast, we finetune LLMs on multiple mental health datasets with various instructions for different tasks in a single round, empowering them to work on multiple tasks without the need for further task-specific finetuning.

For both single- and multi-dataset finetuning, we follow the same two steps:
\begin{align}
\begin{split}
\text{Step 1:} & \text{ finetune with } [\textit{Prompt}_{ZS} - \textit{label}]_{\sum\limits^{I} N_{D_{i-train}}} \\
\text{Step 2:} & \text{ test with } [\textit{Prompt}_{ZS}]_{\sum\limits^{I} N_{D_{i-test}}}
\end{split}
\label{eq:prompt-ft}
\end{align}
where $N_{D_{i-train}}$/$N_{D_{i-test}}$ is the total size of the training/test dataset $D_i$, $I$ represents the datasets used for finetuning, and $i$ indicates the specific dataset index ($i \in I, |I| \ge 1$). Both $\textit{Prompt}_{ZS\textit{-train}}$ and $\textit{Prompt}_{ZS\textit{-test}}$ follow Eq.~\ref{eq:prompt-zs}. Similar to the few-shot setup in Eq.~\ref{eq:prompt-fs}, they employ the same design of \textit{PromptPart1-S} and \textit{PromptPart2-Q}.

In addition, we also experiment with multiple variations:
(1) \textbf{Data size variation}, where we sample a subset of each training dataset $D_{i-train}$ with a smaller $N'_{D_{i-train}}$ to explore the effect of data size on finetuning performance.
(2) \textbf{Prompt variation}, where we randomly sample \textit{PromptPart1-S} from Table~\ref{tab:prompt_design} during the training to explore the effect of prompt diversity on finetuning performance.

