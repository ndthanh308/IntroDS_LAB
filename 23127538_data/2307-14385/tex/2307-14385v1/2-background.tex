\section{Background}
\label{sec:background}
We briefly summarize the related work in leveraging online text data for mental health detection (Sec.~\ref{sub:background:online_text}). We also provide an overview of the ongoing research in LLMs and their application in the health domain (Sec.~\ref{sub:background:LLM}).

\subsection{Online Text Data and Mental Health}
\label{sub:background:online_text}
Online platforms, especially social media platforms, have been acknowledged as a promising lens that is capable of revealing insights into the psychological states, health, and well-being of both individuals and populations~\cite{paul_you_2011,culotta_estimating_2014,chancellor_methods_2020,guntuku_detecting_2017,de_choudhury_mental_2014}.
In the past decade, there has been extensive research about leveraging content analysis and social interaction patterns to identify and predict risks associated with mental health issues, such as anxiety~\cite{ahmed2022machine,saifullah2021comparison,shen_detecting_2017}, major depressive disorder~\cite{park_perception_2021,de_choudhury_predicting_2013,tsugawa_recognizing_2015,de_choudhury_social_2013}, suicide ideation~\cite{de_choudhury_discovering_2016,ruder_suicide_2011,burnap_machine_2015,tadesse_detection_2019,coppersmith_natural_2018}, and others~\cite{coppersmith_adhd_2015,mitchell_quantifying_2015,coppersmith_measuring_2014}. Social media's real-time nature, coupled with its archival capability, can often mitigate retrospective bias. The rich amount of social media data also facilitates the identification, monitoring, and potential prediction of risk factors over time. In addition to observation and detection, social media platforms could further serve as effective channels to offer in-time assistance to communities at risk~\cite{livingston_another_2014,ridout_use_2018,kruzan_social_2022}.

From the computational technology perspective, early research started with basic methods~\cite{coppersmith_measuring_2014,mitchell_quantifying_2015,de_choudhury_social_2013}. For example, pioneering work by Coppersmith \etal~\cite{coppersmith_measuring_2014} employed correlation analysis to reveal the relationship between social media language data and mental health conditions.
Since then, researchers have proposed a wide range of feature engineering methods and built machine-learning models for the prediction~\cite{moreno2011feeling,nguyen2014affective,birnbaum2017collaborative,tsugawa_recognizing_2015}. For example, De Choudhury \etal~\cite{de_choudhury_predicting_2013} extracted a number of linguistic styles and other features to build an SVM model to perform depression prediction.
Researchers have also explored deep-learning-based models for mental health prediction to obviate the need for hand-crafted features~\cite{sawhney2018exploring,ji2018supervised}. For example, Tadesse~\etal~\cite{tadesse_detection_2019} employed an LSTM-CNN model and took word embeddings as the input to detect suicide ideation on Reddit.
More recently, pre-trained language models have become a popular method for NLP tasks, including mental health prediction tasks~\cite{ji_mentalbert_2021,nguyen2022improving,han2022hierarchical}. For example, Jiang~\etal~\cite{jiang2020detection} used the contextual representations from BERT as input features for mental health issue detection.
Our work focuses on the next-generation technology, instruction-finetuned LLMs, and explores its capability on mental health tasks with social media data. 

\subsection{LLM and Health Applications}
\label{sub:background:LLM}
After the great success of Transformer-based language models such as BERT~\cite{devlin_bert_2019} and GPT-1~\cite{radford_improving_2018}, researchers and practitioners have advanced towards larger and more powerful language models (\eg GPT-3~\cite{brown_language_2020} and T5~\cite{raffel_exploring_2020}).
Meanwhile, researchers proposed instruction-based fine-tuning by including an instruction (\ie prompt) at the beginning of the input during both the training and generation phases, which guides a model to carry out a variety of tasks while remaining within a single, unified model~\cite{wei_finetuned_2022}.
These instruction-finetuned LLMs, such as GPT-4~\cite{bubeck_sparks_2023}, PaLM~\cite{chowdhery_palm_2022}, FLAN-T5~\cite{chung_scaling_2022}, LLaMA~\cite{touvron_llama_2023}, Alpaca~\cite{taori_stanford_2023}, contain hundreds of billions of parameters and achieves a promising performance on a variety of tasks, such as question answering~\cite{omar2023chatgpt,robinson2023leveraging}, logic reasoning~\cite{wei_chain--thought_2023,zhou_least--most_2023}, machine translation~\cite{brants2007large,gulcehre2017integrating} \etc.

Researchers have explored the capability of these LLMs in health fields~\cite{jiang_health_2023,singhal_towards_2023,li_chatdoctor_2023,liu_large_2023,wu_pmc-llama_2023,nori_capabilities_2023}. For example,
Singhal~\etal~\cite{singhal_towards_2023} finetuned PaLM-2 on medical domains and achieved 86.5\% on MedQA dataset.
Similarly, Wu~\etal~\cite{wu_pmc-llama_2023} finetuned LLaMA on medical papers and showed promising results on multiple biomedical QA datasets.
Jiang~\etal~\cite{jiang_health_2023} trained a medical language model on unstructured clinical notes from the electronic health record and fine-tuned it across a wide range of clinical and operational predictive tasks. Their evaluation indicates that such a model can be used for various clinical tasks.

There is relatively less work in the mental health domain.
Some work explored the capability of LLMs for sentiment analysis and emotion reasoning~\cite{qin2023chatgpt,zhong2023can,kocon2023chatgpt}.
Closer to our study, Lamichhane~\cite{lamichhane_evaluation_2023}, Amin~\etal~\cite{amin_will_2023}, and Yang~\etal~\cite{yang_evaluations_2023} all tested the performance of ChatGPT on multiple mental health classification tasks (stress, depression, and suicide detection) and showed that ChatGPT shows the initial potential for mental health applications, but it still has a great room for improvement.
However, all previous studies stopped at zero-shot prompting and did not try other methods to improve the performance of LLMs.
In this work, we present the first systematic exploration of multiple methods to turn LLMs into mental health experts.