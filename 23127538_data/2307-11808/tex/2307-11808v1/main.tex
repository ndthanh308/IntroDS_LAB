\documentclass[twoside,11pt]{article}

% melba package. use options:
% - 'arxiv' in arXiv pre-print in submission (disable line numbers)
% - 'accepted' for MELBA _accepted_ papers **only**;
% - 'specialissue' for MELBA accepted papers that are part of a special issue.
\usepackage[arxiv]{melba}

% \usepackage{mwe} % to get dummy images, only for the example
% Can be removed for actual manuscripts


% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf


% often used packages
\usepackage{amssymb,amsmath,amsfonts}
% \usepackage[pass, showframe]{geometry}  % Draw borders on pdf, useful to debug figures placement
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{pgfplots}
    \pgfplotsset{
        compat=newest
    }
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{makecell}

% Definitions of handy macros can go here
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Header and footer (will be filled at publication)
\melbaid{YYYY:NNN}  % This is provided upon by the publishing editor
\doi{https://doi.org/10.59275/j.melba.2023-AAAA}
\melbaauthors{Mounsaveng and Laradji and Vázquez and Perdersoli and Ben Ayed}  % Note: this one is also used to set the pdf 'authors' metadata
\volume{2}
\firstpageno{0001}  % Communicated by the publishing editor
\melbayear{2023}  % The publication year
\datesubmitted{07/2023}  % Date submitted to MELBA: mm/yyyy
\datepublished{m2/yyyy}  % Today's date: mm/yyyy

% The following is optionnal, only if you are publishing in a special issue
% The information is available on the README.md of this repository
% https://github.com/melba-journal/submission#special-issues
% \melbaspecialissue{Medical Imaging with Deep Learning (MIDL) 2020}
% \melbaspecialissueeditors{Marleen de Bruijne, Tal Arbel, Ismail Ben Ayed, Hervé Lombaert}


% Short headings should be {running head} and {authors last names}
\ShortHeadings{Automatic DA Learning using Bilevel Optimization for Histopathological Images}{Mounsaveng and Laradji and Vázquez and Perdersoli and Ben Ayed}


% Title
% If the title spans several lines, authors could decide where the title should be split using \\
% This will cause a warning from the hyperref package, when it sets the title as pdf 'title' metadata
\title{Automatic Data Augmentation Learning using \\Bilevel Optimization for Histopathological Images}

% For very long author lists, please contact the publishing editor for instructions publishing-editor@melba-journal.org (this is usually dealt on a case-by-case basis).
% \firstname and \surname are optionnal, (simply using \name is doable), but can be useful
% to clarify names such as \firstname FIRSTNAME MIDDLE NAME \surname FAMILYNAME or composed names
\author{\firstname Saypraseuth \surname Mounsaveng \email saypraseuth.mounsaveng.1@etsmtl.net \\  % start right after \author{, or there will be an extra space
	\addr ÉTS Montréal, Canada
	\AND
	\firstname Issam \surname Laradji \email issam.laradji@servicenow.com \\
	\addr ServiceNow Research
        \AND 
	\firstname David \surname Vázquez \email david.vazquez@servicenow.com \\
	\addr ServiceNow Research
 	\AND
        \firstname Marco \surname Pedersoli \email marco.pedersoli@etsmtl.ca \\
	\addr ÉTS Montréal, Canada
 	\AND
        \firstname Ismail \surname Ben Ayed \email ismail.benayed@etsmtl.ca \\
	\addr ÉTS Montréal, Canada 
}



% Indendation is not mandatory, but usually makes the code more readable
\begin{document}

% top matter
\maketitle

% abstract
\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
    Training a deep learning model to classify histopathological images is challenging, first, because of the color and shape variability of the cells and tissues, and second, because of the reduced amount of available data, which does not allow proper learning of those variations. Variations can come from the image acquisition process, for example, due to different cell staining protocols or tissue deformation.
    To tackle this challenge, Data Augmentation (DA) can be used during training to generate additional samples by applying transformations to existing ones. Those samples will help the model to become invariant to those color and shape transformations. The problem with DA is that it is not only dataset-specific but it also requires domain knowledge, which is not always available. Without this knowledge, selecting the right transformations can only be done using heuristics or through a computationally demanding search.
    To address this, we propose in this work an automatic DA learning method. In this method, the DA parameters, i.e. the transformation parameters needed to improve the model training, are considered learnable parameters and are learned automatically using a bilevel optimization approach in a quick and efficient way using truncated backpropagation.
    We validated the method on six different datasets of histopathological images. Experimental results show that our model can learn color and affine transformations that are more helpful to train an image classifier than predefined DA transformations. Predefined DA transformations are also more expensive as they need to be selected before the training by grid search on a validation set. We also show that similarly to a model trained with a RandAugment-based framework, our model has also only a few method-specific hyperparameters to tune but is performing better. This makes our model a good solution for learning the best data augmentation parameters, especially in the context of histopathological images, where defining potentially useful transformation heuristically is not trivial.
    %
    Our code is available at~\url{https://github.com/smounsav/bilevel_augment_histo}.
\end{abstract}

% keywords
\begin{keywords}
	CNN image classification data augmentation bi-level optimization truncated backpropagation
\end{keywords}


% Introduction (or first section)
\section{Introduction}
    % Medical imaging --> transformations not clear without expert knowledge so automatic transformations could help
    Deep learning-based models have proved effective for the analysis of histopathological images \citep{Bejnordi2016StainSS,Litjens2017,shen2017deep,ker2018deep}. In the context of image classification, one hurdle to a good generalization is the color and shape variability of the cells and tissues in the images. Those variations can be inherent to the image acquisition process. More precisely, color variations can come from the cell staining, which is done to make the cells visible to the human eyes, whereas shape variations can come from tissue deformation. \\
    % However, color and shape variations can also occur when the dataset is built with data coming from different sources (eg. different devices or different institutions).\\
    As those variations have a major impact on the performance of the models, addressing this issue has been an active area of research~\citep{Ciompi2017TheIO,Tellez2019QuantifyingTE,ataky2020data,faryna2021tailoring,electronics10050562,wagner2021structure,GARCEA2022106391}.
    To address color variations problems, two main directions can be followed: stain normalization and data augmentation. Stain normalization consists in altering the color space of the input images so that the difference between the color statistics of the train images and the color statistics of the test images is reduced. Data augmentation consists in creating new images from existing samples with different transformations so that the model can learn transformation invariances. In the context of histopathological image classification, data augmentation is particularly interesting as it can address the problem of both color and shape variations by teaching the classification model to be invariant to those two types of transformations.
    While data augmentation has been explored extensively for natural images as shown in \cite{Shao2022ATO}, most methods proposed rely on the selection of data augmentation parameters based on heuristics. Selecting meaningful transformations and the right amplitude requires prior or expert knowledge, which is not always available, especially in the medical images field \citep{Tellez2019QuantifyingTE}. Without the adequate knowledge, selecting the right transformations is not trivial, and selecting the wrong transformations can lead to a degradation of the model performance as shown in \cite{chen2020group}.
    To tackle the problem of selecting the right transformations, automatic data augmentation methods based on bilevel optimization like \cite{Cubuk_2019_CVPR} have been proposed for natural images. Those methods can be computationally expensive as for each new set of parameters, the model in the inner loop needs to be fully trained till convergence. Those methods were improved using a gradient-based approach like in \cite{Hataya2019FasterAL, Lin2019OnlineHL}, or more recently \cite{Hataya_2022_WACV}. In our work, as we can see in~Fig.\ref{fig:model}, we do not need to train the classifier in the inner loop until convergence for each different set of data augmentation parameters to test. The augmenter network generating the right data augmentation is trained at the same time as the classifier by alternating between the outer and the inner loop at each iteration.
    % In \cite{Cubuk2019RandaugmentPA}, authors show that instead of defining a large transformation space and searching the best sequence by sampling transformations from this space, it is sometimes enough to just draw a random sequence of transformation from a predefined set to obtain a significant boost in performance when training a classifier. This method is called RandAugment. An application of Randaugment to histopathological images was presented in \cite{faryna2021tailoring}.\\
    
    To address the problem of selecting the right data augmentation transformations, we propose in this work to extend the method in \cite{Mounsaveng2021LearningDA} to histopathological images. The method consists in training an image classifier while learning the best data augmentation transformations in a bi-level optimization framework. The classifier is trained in the inner loop while the best data augmentation parameters are learned in the outer loop. To make the method computationally efficient, the gradient of the validation loss used to update the data augmentation parameters is estimated using truncated backpropagation and with only one iteration of the inner loop.
    We validated this method on six different histopathological images datasets. Experimental results show that our method yields a better final accuracy than predefined data augmentation found by grid search on a validation set. During training, it finds transformations in the color and affine transformation space that help the most the learning and is also less expensive to train than predefined data augmentation. As we can see in~Fig.\ref{fig:model}, the best augmentation parameters are learned at the same time as the classifier is trained by alternating between the outer and inner loop at each iteration whereas when we do a grid search, the classifier needs to be trained till convergence in the inner loop for each set of different data augmentation parameters to test. What is also interesting to note is that the learned transformations do not hurt the model performance when useful transformations are more challenging to find.
    Moreover, similarly to a model trained with RandAugment, our model requires only a few model-specific hyperparameters to tune (in our case the hyperparameters of the augmenter network) but shows a better final classification accuracy. Our intuition is that even if RandAugment based methods have proven efficient, learning the best transformations along the training can yield a better classification performance, as the timing when transformations are presented to the model is important as shown in \cite{Golatkar2019TimeMI}. 
    
    % Figure environment removed

    \paragraph{Contributions}
        We summarize our contributions as follows:\\
        \begin{itemize}
            \item We successfully extend an automatic data augmentation learning method to learn useful color and affine augmentations in the context of histopathological images. As this method is differentiable, we can efficiently optimize a large transformation network that learns to perform data augmentation automatically.
    
            \item We show that our proposed model learns different sets of transformations and achieves comparable or better results than hand-defined transformations or RandAugment based methods on six different datasets.  We also show that our model never learns transformations that hurt the model performance and avoids the problem of badly hand-chosen transformations.
        \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related works
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make sure to put your work into context and include apporpriate citations.
% We do not have limits on citation counts.
\section{Related Works}
    %Another way of augmenting a dataset is by generating new samples instead of transforming the existing ones.
    Generative Adversarial Networks (GANs)~\citep{Goodfellow2014GenerativeAN} can generate realistic new samples of a certain dataset or class, thus they can be adapted for data augmentation.
    \citet{mirza2014conditional} and \citet{DBLP:conf/icml/OdenaOS17} proposed to generate images conditioned on their class that could be directly used to augment a dataset.  Also based on GAN, but directly used for data augmentation, DAGAN~\citep{antoniou2018augmenting} conditions the augmented image on the input image.
    TripleGAN~\citep{chongxuan2017triple} and Bayesian data augmentation~\citep{tran2017bayesian} train a classifier jointly with the generator.
    These approaches generate general image transformations, but in practice, it is not as performant as using predefined transformations.
    TANDA~\citep{ratner2017learning} is the only GAN-based approach that uses predefined transformations. It defines a large set of transformations and learns how to combine them to generate new samples that follow the same distribution as the original data. In medical images, \cite{FridAdar2018SyntheticDA} use a GAN to generate new CT-Scan images to improve the training of a liver lesion classifier.
    
    Our model is more efficient than GAN based models, as it does not require to learn a separate model before training the classifier. It learns the best transformation parameters and classifier at the same time.

    \subsection{AutoAugment}
        %Based on reinforcement learning, 
        %In this subsection we compare AutoAugment~\cite{Cubuk_2019_CVPR} with our approach. 
        AutoAugment~\citep{Cubuk_2019_CVPR} is a data augmentation method that learns sequences of transformations that maximize the classifier accuracy on a validation set. This objective is better than simply reproducing the same data distribution as in GAN-based models, as it favors transformations that generalize well on unseen data. 
        %Our approach also aims at selecting data augmentation transformations that generalize well on unseen data. However, 
        %The key difference between our approach and AutoAugment is that while AutoAugment learns the probability and the magnitude of a set of discrete transformations, our model learns to generate a transformation from input noise. Learning the parameters of a complex network would not work with the reinforcement algorithm of AutoAugment, as the search space would be too large. In contrast, our approach needs to differentiate through the given transformations, thus non-differentiable transformations cannot be used. In the experimental results, we show that even with that limitation we can generate powerful transformations that can lead to competitive results. %To learn the parameters with backpropagation.  
        However, it is computationally expensive as it performs the complete bilevel optimization by training the classifier in the inner loop until convergence for each set of evaluated transformations. 
        % To reduce the computational cost, a first solution is to perform the policy search on a reduced training set.
        % Our approach instead uses truncated backpropagation to approximate the bilevel optimization and allow the joint learning of the classifier and the augmenter network on the whole available training data. Thus the final computational cost is still in the order of a normal network training. Moreover, while AutoAugment learns the probability and the magnitude of a set of discrete transformations, our model learns to generate a transformation from input noise. Learning the parameters of a complex network would not work with the reinforcement algorithm of AutoAugment, as the search space would be too large.
        Some solutions to reduce the computational cost were proposed in follow-up works. Fast AutoAugment~\citep{lim2019fast} optimizes the search space by matching the density between the training set and the augmented data. Alternatively, Population Based Augmentation (PBA)~\citep{ho2019pba} focuses on learning the optimal augmentation schedule rather than only the transformations. However, even if these approaches reduce the computational cost of AutoAugment, they do not leverage gradient information. Faster AutoAugment~\citep{Hataya2019FasterAL} does this by combining AutoAugment with a GAN discriminator and considering transformations as differentiable functions. OHL-Auto-Aug \citep{Lin2019OnlineHL} uses an online bilevel optimization approach and the REINFORCE algorithm on an ensemble of classifiers to estimate the gradient of the validation loss and learn an augmentation probability distribution.
        RandAugment~\citep{Cubuk2019RandaugmentPA} goes further by showing that the same performance level as AutoAugment can be obtained by randomly selecting transformations from a predefined pool and just tuning the number of transformations to use and a global (same for all transformations) magnitude factor. However, this approach also requires prior knowledge of useful transformations.
        In histopathological images, \cite{faryna2021tailoring} use the RandAugment method with a set of transformations extended with some specific color transformations. This leads to an improved performance of the classifier.% combination of stain augmentation and stain DA
        
        Our model is more efficient than search-based methods as the data augmentation parameters are updated at each training iteration using the gradient of the validation loss obtained in the inner loop. This gradient is estimated using truncated backpropagation, which removes the need to train the model until convergence for each set of evaluated transformations.
    
    \subsection{Hyperparameter Learning}
        % Basic methods
        Our work has some roots in the hyperparameter optimization field, as data augmentation parameters can be considered as hyperparameters to tune.
        Hyperparameter tuning is essential to obtain the optimal performances when training neural networks on a given dataset. Classic approaches assume that the learning model is a black-box and use methods like grid search, random search~\citep{NIPS2011_4443, bergstra2013making}, Bayesian optimization~\citep{snoek12practical}, or a tree-search approach~\citep{hutter2011sequential}. These approaches are simple but expensive because they repeat the optimization from scratch for each sampled value of the hyperparameters and so are only applicable to low dimensional hyperparameter spaces. 
        % Automatic learning
        A different line of research is to leverage the gradient of these (continuous) hyperparameters (or hyper-gradients) to perform the hyper-optimization.
        The first work proposing this idea~\citep{bengio2000gradient} shows that the implicit function theorem can be used to this aim. This idea was developed more recently in \cite{bertrand2020implicit}.
        \cite{domke2012generic} was the first work to propose a gradient-based method using the bilevel optimization approach proposed in \cite{colson2007overview} to learn hyperparameters. Using a bilevel optimization approach to train a neural network is challenging, as usually there is no closed-form expression of the function learned in the inner loop (Section~\ref{sec:learning}).
        To address this, \citet{pmlr-v37-maclaurin15} and later \citet{pmlr-v70-franceschi17a} proposed methods to reverse the forward pass to compute the gradient of the validation loss. However, these methods are applicable only when the number of hyperparameters and the complexity of the models are limited due to the memory needed to save the intermediate steps.
        Another approach to address the computational hurdle in the inner loop is to calculate an approximation of the gradient like in \citet{pmlr-v48-pedregosa16} \citet{luketina2016scalable} or \citet{mackay2019self}. Our method differentiates from those by using truncated backpropagation to estimate the gradient of the validation loss.
        Finally, note that hyperparameter optimization presents some similarities to meta-learning as shown in \citet{pmlr-v80-franceschi18a}. For instance, in MAML~\citep{finn2017model}, a shared model initialization is learned to minimize the validation loss and therefore improve the generalization capabilities of the model. More recently, \cite{Hataya_2022_WACV} can be positioned at the intersection of AutoAugment and meta-learning based approaches.
            
        % Figure environment removed
    


% A methodological, model, or similar section often comes here.
\section{Proposed Data Augmentation Method}
    \label{sec:learning}
    Consider a labeled set $\mathcal{X}:=\{x_i,y_i\}^{N}_{i=1}$, where $x_i$ is an input image, $y_i$ the associated class label, N the number of samples and $\hat{\mathcal{X}}$ the set of transformed images. We formulate the problem of identifying effective data augmentation transformations as a bilevel optimization problem. In this setup, the augmenter $\mathcal{A}_\theta:\mathcal{X}\rightarrow{\hat{\mathcal{X}}}$ is parametrized by ${\theta}$ and is used to minimize the loss $\mathcal{L}$ on the validation data $\mathcal{X}_{val}$ in the outer loop. In the inner loop, the classifier parameters $\omega$ are optimized  on the training data $\mathcal{X}_{tr}$ in the standard supervised way. This formulation can be written as:
    \begin{align}
        \theta^* &= \argmin_\theta
        \mathcal{L}(\mathcal{X}_{val}, \omega^*) 
        \label{equ:bilevel1} \\
        s.t. \quad \omega^* &= \argmin_{\omega} \mathcal{L}(\mathcal{A}_{\theta}(\mathcal{X}_{tr}),\omega).
        \label{equ:bilevel2}
    \end{align}
    While optimizing a few hyperparameters on the validation data is feasible with black-box approaches such as grid and random search~\cite{bergstra12random} or Bayesian optimization~\cite{snoek12practical}, it is not efficient. With bilevel optimization, our aim is to efficiently learn an entire neural network $\mathcal{A}_\theta$ (possibly with thousands of parameters $\theta$) which defines a distribution of transformations that should be applied on the training data to improve generalization. 
    
    Gradient descent was shown to be an efficient method for optimizing parameters of large networks. In problems such as architecture search~\citep{liu2018darts}, the parameters can be directly optimized with gradient descent (or second order methods) against the training and validation data.
    However, this is not the case for data augmentation. The reason is that the transformation network $\mathcal{A}_{\theta}$ is optimized to maximize the validation score, but applies transformations only on the training set. Therefore, first order methods would not work. The aim of data augmentation is to introduce transformations during the training phase that can make the model invariant or partially invariant to any transformations that can occur at test time. If we optimize the transformation network directly on the validation data, the model will simply select trivial solutions such as the identity transformation. This approach has been used for  object localization~\cite{jaderberg2015spatial} and it did not improve the model generalization performance as much as data augmentation.
    To solve this issue, new methods relied on reinforcement learning instead of gradient descent to learn effective data augmentation~\citep{Cubuk_2019_CVPR, lim2019fast, ho2019pba}.
    
    In this work, we show that in the case of a differentiable augmenter $\mathcal{A}_\theta$, there is a simple, efficient way to find optimal data transformations based on gradient descent that generalize well on validation data. We formulate our problem as an approximation to bilevel optimization by using truncated backpropagation as it allows our method to:
    i) efficiently estimate a large number of parameters to generate the optimal data augmentation transformations by gradient descent;
    ii) obtain an online estimation of the optimal data augmentation during the different phases of the training, which can also be beneficial~\citep{Golatkar2019TimeMI};
    iii) change the training data to adapt to different validation conditions as in supervised domain adaptation.\\
    Although approximate bilevel optimization has already been proposed for hyperparameter optimization~\citep{Shaban-AISTATS-19, pmlr-v80-franceschi18a, pmlr-v70-franceschi17a}, in this paper we show that it can be used for training a large, complex model (the augmenter $\mathcal{A}_\theta$ network) to learn an effective distribution of transformations.

    \subsection{Approximate Online Bilevel Optimization}
        As shown in Eq.~\ref{equ:bilevel1} and \ref{equ:bilevel2}, the problem of finding the optimal data augmentation transformations $\mathcal{A}_\theta$ can be cast as a bilevel optimization problem. This problem can be solved by iteratively solving Eq.~\ref{equ:bilevel2} to find the optimal network weight $\omega^*$, given the parameters of the transformation $\theta$ and then updating $\theta$:
        \begin{equation}
            \footnotesize
            \theta \leftarrow \theta - \eta_\theta\nabla_\theta \mathcal{L}(\mathcal{X}_{val},\omega^*)
        \end{equation}
        where $\eta_\theta$ is the learning rate used to train the augmenter network.\\ 
        However, as the augmentations are to be applied only on the training dataset and not on the validation set, calculating $\frac{\partial{\mathcal{L}(\mathcal{X}_{val},\omega^*)} }{\partial \theta}$ is not trivial. To enable this calculation, we use the fact that the weights $\omega$ of the network are shared between training and validation data and use the chain rule to differentiate the validation loss $\mathcal{L}(\mathcal{X}_{val},\omega^*)$ with respect to the hyperparameters $\theta$. In other words, instead of using a very slow black-box optimization for $\theta$, we can exploit gradient information %. In Figure \ref{fig:graph} we can see that 
        because the model parameters $\omega^*$ are shared between the validation and the training loss.\\
        We define the gradient of the validation loss with respect to $\theta$ as follows:
        \begin{equation} \label{equ:grad1}
            \footnotesize
            \begin{split}
                {\nabla_\theta \mathcal{L}}(\mathcal{X}_{val},\omega^*)&=\frac{\partial{\mathcal{L}(\mathcal{X}_{val},\omega^*)} }{\partial \theta}\\
                &= \frac{\partial{\mathcal{L}}(\mathcal{X}_{val},\omega^*)}{\partial \omega^*} \frac{\partial{\omega^*}}{\partial \theta}
            \end{split}
        \end{equation}
        By defining $\mathcal{G}^{(t)}$ as the gradient of the training loss at iteration $t$:
        \begin{equation}
            \footnotesize
            \mathcal{G}^{(t)}=\nabla_{\omega}\mathcal{L}(\mathcal{A}_{\theta}(\mathcal{X}_{tr}),\omega^t)
        \end{equation}
        we can write $\frac{\partial{\omega^*}}{{\partial \theta}}$ in Eq.~\ref{equ:grad1} as:
        \begin{equation}
            \footnotesize
            \frac{\partial{\omega^*}}{\partial \theta} = \sum_{i=1}^{T-1} \frac{\partial{\omega^{(T)}}}{\partial \omega^{(i)}} \frac{\partial{\omega^{(i)}}}{\partial{\mathcal{G}^{(i-1)}}}\frac{\partial{\mathcal{G}^{(i-1)}}}{\partial {\theta}}
        \end{equation}
        where T is the iteration when the classifier converges.
        % and:
        % \begin{equation}
        % \frac{\partial{\omega^{(T)}}}{\partial \omega^{(i)}} =  \prod_{j=i}^{T-1} \frac{\partial{\omega^{(j+1)}}}{\partial{\omega^{(j)}}}
        % \end{equation}
        % where  $T$ is the number of iterations after which the training has converged ($\omega^*=\omega^T$).
        
        As $\omega^*$ represents the model weights at training convergence, they depend on $\theta$ for each iteration of gradient descent. Thus, to compute $\frac{\partial{\omega^*}}{\partial \theta}$, one has to back-propagate throughout the entire $T$ iterations of the training cycle. An example of this approach is in \citet{pmlr-v37-maclaurin15}.
        This approach is feasible only for small problems due to the large requirements in terms of computation and memory. 
        However, as optimizing $\omega^*$ is an iterative process, instead of computing $\frac{\partial{\omega}}{\partial \theta}$ only at the end of the training loop, we can estimate it at every iteration $t$: 
        \begin{equation}
            \footnotesize
            \frac{\partial{\omega^*} } {\partial \theta} \approx
            \frac{\partial{\omega^{(t)} } } {\partial \theta^{(t)}} = \sum_{i=1}^{t} \frac{\partial{\omega^{(t)}}}{\partial{\omega^{(i)}}} \frac{\partial{\omega^{(i)} }} {\partial{\mathcal{G}^{(i-1)}}}\frac{\partial{\mathcal{G}^{(i-1)}}}{\partial {\theta^{(i)}}},
            \label{equ:online}
        \end{equation}
        This procedure corresponds to dynamically changing $\theta$ during the training iterations (thus it becomes $\theta^{(t)}$) to minimize the current validation loss based on the training history. Although this formulation is different from the original objective function, adapting the data augmentation transformations dynamically with the evolution of the training process can improve generalization performance~\citep{Golatkar2019TimeMI}.
        This relaxation is often used in constrained optimization for deep models, in which constraints are reformulated as penalties and their gradients are updated online, without waiting for convergence, to save computation \citep{Pathak2015ConstrainedCN}. However, in our case, we cannot write the bilevel optimization as a single unconstrained formulation in which the constraint in $\omega^*$ is summed with a multiplicative factor that is maximized (i.e., Lagrange multipliers), because the upper level optimization should be performed only on $\theta$, while the lower level optimization should be performed only on $\omega$. Nonetheless, even with this relaxation, estimating $\frac{\partial{\omega^*} } {\partial \theta}$ still remains a challenge as it does not scale well. Indeed, the computational cost of computing $\frac{\partial{\omega^{(t)}}}{\partial{\theta^{(t)}}}$ grows with the number of iterations $t$ as shown in Eq.~\ref{equ:online}.
        To make the gradient computation constant at each iteration we use truncated backpropagation similarly to what is commonly used in recurrent neural networks~\citep{Williams90anefficient}:
        %\vspace{-3mm}
        \begin{equation} \label{equ:truncated}
            \footnotesize
            \frac{\partial{\omega^{(t)} } } {\partial \hat\theta} \approx \sum_{i=t-K}^{t} \frac{\partial{\omega^{(t)} } } {\partial{\omega^{(i)} } } \frac{\partial{\omega^{(i)} }} {\partial{\mathcal{G}^{(i-1)}}}\frac{\partial{\mathcal{G}^{(i-1)}}}{\partial {\theta^{(i)}}},
            % \vspace{-3mm}
        \end{equation}
        %\vspace{-3mm}
        where $K$ represents the number of gradient unfolding that we use. 
        Fig.~\ref{fig:graph}b. shows the computational graph used for this computation.\\
        Additionally, as \citet{Williams90anefficient}, we consider a second parameter $J$ which defines the number of inner loop training iterations after which $\theta$ is updated, in other words how often the computation of the gradients of $\theta$ is performed. The situation where $K=J=T$ is the exact bilevel optimization as shown in Eq.~\ref{equ:bilevel1} while $K=J=1$ corresponds to updating $\theta$ at each iteration, in our case mini-batch ($K=1$), using only one step of gradient unfolding ($J=1$). A theoretical analysis of the convergence of this approach is presented in \citet{Shaban-AISTATS-19}.

    \subsection{Augmenter Network}
        In this work, we use an augmenter network that can learn two types of transformations: geometrical and color. We use the transformation model of spatial transformer networks~\citep{jaderberg2015spatial}, but for data augmentation instead of data alignment. Thus, as illustrated in Fig.~\ref{fig:model}, the augmenter is composed of a module that generates a set of transformation parameters followed by a module that applies the generated transformations to the original image. Note that the learned transformations are not conditioned on the input image but defined only based on random noise.\\                
        \subsubsection{Geometrical} 
            In our experiments, we consider scenarios where the augmenter network learns affine transformations. The choice of this kind of transformation is motivated by the fact that tissues are deformed during the image acquisition process. By considering affine transformations in our learned data augmentation, we aim to train the model to become invariant to those geometrical deformations.
            For affine transformations, the augmenter network receives as input a random noise vector and generates a 2x3 matrix of values representing a variation from the identity transformation.

        \subsubsection{Color}
            We also consider scenarios where the augmenter learns color transformations. Color augmentations are important as they can help the trained model to become invariant to color perturbations appearing during the cell staining process.
            Color transformations considered are: contrast, brightness and in the HSV space hue and saturation. For color transformations, the augmenter receives as input a random noise vector and generates a single value representing a variation for each color transformation. In our implementation, we use the kornia library~\citep{Riba2019KorniaAO}, which follows the specifications of \cite{Szeliski2010ComputerV}.
            For contrast, the value learned is a non-negative factor applied to the actual color values. 1 represents the initial image whereas values tending to 0 mean a black-and-white image. If we consider the variables $r$, $g$, and $b$ representing the values of the red, green, and blue colors of the images and $cf$ the contrast factor learned by our network, the new RGB values are obtained using the update rule:
            \begin{equation}
                (r, g, b) \leftarrow clamp((r, g, b)\cdot cf, 0, 1)
            \end{equation}
            For brightness, the value learned represents a shift applied to the actual color values. 0 represents the initial image. If we consider the variables $r$, $g$, and $b$ representing the values of the red, green, and blue colors of the images and $bs$ the brightness shift learned by our network, the new RGB values are obtained using the update rule:
            \begin{equation}
                (r, g, b) \leftarrow clamp((r, g, b) + bs, 0, 1)
            \end{equation}
            In the case of saturation, the value learned by the augmenter is a non-negative factor applied to the actual saturation value. A value of 1 represents the original image whereas 0 means a black-and-white image. If we consider the variables $h$, $s$, and $v$ representing the values of the hue, saturation, and value of the images and $sf$ the saturation factor learned by our network, the new HSV values are obtained using the update rule:
            \begin{equation}
                (h, s, v) \leftarrow clamp((h, s\cdot sf, v) , 0, 1)
            \end{equation}
            Finally, the value learned by our augmenter for hue is a shift of the hue channel. 0 represents no shift to the hue channel and any other value negative or non-negative is added to the actual value. If we consider the variables $h$, $s$, and $v$ representing the values of the hue, saturation, and value of the images and $hs$ the hue shift learned by our network, the new HSV values are obtained using the update rule:
            \begin{equation}
                (h, s, v) \leftarrow (mod(h + hs, 2\pi), s, v)
            \end{equation}
            % Those values are in a range depending on the transformation: i) Contrast in range [0:2]; ii) Brightness in range [0:2]; iii) Hue in range [-0.5:0.5] iv) Saturation in range [0:1]. In all cases the zero value corresponds to the identity transformation.

    \section{Experimental setup}
        \subsection{Datasets and evaluation}
            \label{sec:datasets}
            The datasets used in our experiments are:

            \paragraph{BACH}
             \citet{Aresta2019BACHGC} is a dataset of 400 H\&E (hematoxylin and eosin) stained breast cancer histology images of resolution 2048 x 1536 distributed in 4 balanced classes of 100 images. As there is no test set publicly available, we use in our experiments 40\% of the dataset for training, 10\% for validation and 50\% for testing as in \cite{Rony2022DeepWL}. The values used for the predefined color transformations are brightness=0.5, contrast=0.5, saturation=0.5 and hue=0.05.

            \paragraph{Glas}
             \citet{Sirinukunwattana2016GlandSI} is a dataset of 165 H\&E stained colon cancer histology images of variable resolution (in our experiments, we use an image size of 430x430) distributed in 2 classes (benign and malignant). The dataset is divided in a train set of 85 images (37 benign and 48 malignant) and a test set of 80 images (37 benign and 43 malignant). In our experiments, we use 80\% of the training set for training and 20\% for validation. The values used for the predefined color transformations are brightness=0.25, contrast=0.25, saturation=0.25 and hue=0.4.

            \paragraph{HICL Larynx}
             \citet{Ninos2015MicroscopyIA} is a dataset of 450 H\&E and P63 stained larynx cancer histology images with 2 magnifying factors (20x and 40x). It has 3 classes corresponding to cancer grades: Grade I, II and III. For the 20x magnification factor, the image resolution is 1728x1296 and the number of images per class is I:87, II:73 and III:64. For the 40x magnification factor, the image resolution is 1300x1030 and the number of images per class is I:88, II:74 and III:64. As there is no test set publicly available, we use in our experiments 70\% of the dataset for training, 20\% for validation set, and 10\% for test. The values used for the predefined color transformations are brightness=0.25, contrast=0.25, saturation=0.25 and hue=0.4.
             
            \paragraph{HICL Brain}
             \citet{Glotsos2008ImprovingAI} is a dataset of 2548 H\&E and P63 stained brain cancer histology images with 2 magnifying factors (20x and 40x). It has 7 classes corresponding to cancer grades: Grade I, I-II, II, II-III, III, III-IV and IV. For the 20x magnification factor, the image resolution is 1728x1296 and the number of images per class is I:123, I-II: 94, II:208, II-III:47, III:367, III-IV:45 and IV:373. For the 40x magnification factor, the image resolution is also 1728x1296 and the number of images per class is I:132, I-II: 73, II:210, II-III:53, III:434, III-IV:32 and IV:357. As there is no test set publicly available, we use in our experiments 70\% of the dataset for training, 20\% for validation set, and 10\% for test. The values used for the predefined color transformations are brightness=0.25, contrast=0.25, saturation=0.25 and hue=0.4.

            \paragraph{Evaluation} To evaluate the performance of our models, we use the classification accuracy metric, which is defined by the number of samples correctly classified divided by the total number of samples. For each scenario, we do a 5-fold cross-validation and the result reported is the average of the results obtained by the 5 folds. The hyperparameters search is done separately for each dataset. The hyperparameters selected are the ones yielding the best validation results averaged over the 5 folds. We also follow this protocol for Randaugment hyperparameters.

        \subsection{Implementation details}
            \label{sec:implementation_details}
            As we can see in~Fig.\ref{fig:model}, our model is composed of a classifier and an augmenter network. As classifier, we use a ResNet18~\citep{He2015ResNet} network pretrained on Imagenet. ResNet18 is an 18 layers deep neural network with residual connections. To align with the image size used during pretraining, we use in our training phase patches of size 224x224 and evaluate the model on whole images during the testing phase.
            The augmenter learning the geometric and color transformations is a MLP network that receives a noise vector of dimension 100 as input and generates the transformation parameters.
            The augmenter network has 3 fully connected layers of size 100, 64, and 32 and an output layer of size $n$, $n$ being the number of hyperparameters to optimize according to the scenario considered (4 for color transformations, 6 for affine transformations, or 10 when both color and affine transformations are considered). 
            To have differentiable affine and color transformations, we use the affine\_grid and grid\_samples functions of the torchvision package of Pytorch framework~\citep{NEURIPS2019_9015}.
            
            As in \cite{Mounsaveng2021LearningDA}, we adopt a frequency K of updating $\theta$ $J$ of 1 and a number of steps of backpropagation J of 1 to update the parameters of our model. 

    \section{Results}
        Our proposed method aims at learning data augmentation automatically while training the image classifier. We validate our method on the 6 different datasets presented in \ref{sec:datasets}: BACH, Glas, Medisp HICL Larynx with magnification factor 20x, Medisp HICL Larynx with magnification factor 40x, Medisp HICL Brain with magnification factor 20x, Medisp HICL Brain with magnification factor 40x.
        
        In a first series of experiments, we compare for each dataset the best performance of an image classifier trained on 3 different kinds of transformations: first, we consider color transformations, then affine transformations, and finally a combination of both transformation types. For each type of transformation, we compare the classification performance of 3 models: first, a classifier trained without data augmentation (baseline), then a classifier trained with hyperparameters for data augmentation found by grid search on a validation set (predefined), and finally a classifier trained with the data augmentation learned by our method. Note that in all our experiments, we divide our data into 3 sets: a training set to train our model, a validation set to select the hyperparameters of the model, and a test set to evaluate the model.

        Then, in a second series of experiments, we investigate our model more in-depth focusing on the BACH dataset. We chose this dataset as it is the most challenging of the six considered in this paper in terms of image size and difficulty to learn the classification task. More precisely, we investigate two aspects of our model: i) the impact of the amount of training data available on the quality of the learned augmentations ii) the impact of starting the training from a classifier pretrained on Imagenet.

        Finally, in a third series of experiments, we compare our approach to a model trained in a data augmentation framework similar to RandAugment \citep{Cubuk2019RandaugmentPA}. Instead of searching for the best sequence of transformations and the best magnitude for each transformation at the same time as in other models of the AutoAugment family, RandAugment relaxes the search problem to the tuning of 2 hyperparameters M and N, M being a global magnitude for all considered transformations and N the number of transformations selected in each sequence of transformations. In our experiments, we define M and N by doing a grid search with values between 1 and 5 for both M and N. This approach is simple yet very efficient and has proven to be state of the art in \cite{faryna2021tailoring} on Camelyon 17 dataset. However, we argue that even if RandAugment is very simple and efficient to use, it still requires prior knowledge to define the initial pool of transformations. For our method, we also need to fine-tune a limited number of hyperparameters (the hyperparameters of the augmenter network), but we can also define a more generic set of differentiable transformations. Moreover, learning the optimal data augmentation at each epoch can be beneficial for the model, as the time when the transformations are presented to the model is important as reported in \cite{Golatkar2019TimeMI}.
        To be fair in the comparison of the results, we limited the pool of transformations used by RandAugment to the transformations learned by our proposed model. The transformations considered by our adapted RandAugment framework are listed in Tab. \ref{tab:randaugment_transformation_set}.

        \begin{table}[t]
            \footnotesize
            \centering
            \begin{tabular}{c|c}
                Transformation type & Magnitude Range\\
                \hline
                identity & - \\
                rotation & [-30.0, 30.0]\\
                translation x & [-0.45, 0.45]\\
                translation y & [-0.45, 0.45]\\
                shear x & [-0.3, 0.3]\\
                shear y & [-0.3, 0.3]\\
                contrast & [0, 2]\\
                brightness & [-1, 1]\\
                hue & [-0.5, 0.5]\\
                Saturation & [0, 1]\\
            \end{tabular}%
            % }
            \caption{\textbf{Transformations considered by our adapted RandAugment framework}. To be fair in the comparison with our proposed model, we limited the set of transformations to the differentiable transformations learned by our model.}
            \label{tab:randaugment_transformation_set}
        \end{table}

        \begin{table*}[t]
            \small
            \centering
            \setlength\tabcolsep{2pt}
            \begin{tabular}{l|c|c|c|c|c|c}
                Scenario / Dataset &  BACH & Glas & \makecell{Larynx\\20x} & \makecell{Larynx\\40x} & \makecell{Brain\\20x} & \makecell{Brain\\40x}\\
                \hline
                Baseline & 83.30\tiny$\pm$1.18 & 89.50\tiny$\pm$1.22 & 87.51\tiny$\pm$1.27 & 86.67\tiny$\pm$1.16 & 99.53\tiny$\pm$0.43 & 96.97\tiny$\pm$1.13\\
                Baseline + color DA & 85.10\tiny$\pm$1.19 & 96.00\tiny$\pm$1.25 & 90.24\tiny$\pm$1.38 & 86.67\tiny$\pm$1.23 & 99.53\tiny$\pm$0.43 & 97.42\tiny$\pm$1.20\\
                Baseline + affine DA & 83.70\tiny$\pm$1.20 & 97.75\tiny$\pm$1.27 & 95.92\tiny$\pm$1.11 & 94.29\tiny$\pm$1.38 & 99.54\tiny$\pm$0.68 & 98.18\tiny$\pm$1.05\\                        
                Baseline + color\&affine DA & 84.60\tiny$\pm$1.23 & 98.25\tiny$\pm$1.12 & 95.24\tiny$\pm$1.25 & 95.24\tiny$\pm$1.37 & 99.53\tiny$\pm$0.43 & 98.94\tiny$\pm$1.26\\    
                \hline    
                Our model (color DA) & 85.60\tiny$\pm$1.18 & 97.25\tiny$\pm$1.23 & 91.11\tiny$\pm$1.28 & 86.67\tiny$\pm$1.22 & 99.53\tiny$\pm$0.43 & 97.57\tiny$\pm$1.24\\
                Our model (affine DA) & 85.40\tiny$\pm$1.25 & 98.25\tiny$\pm$1.20 & 96.19\tiny$\pm$1.22 & 95.24\tiny$\pm$1.26 & 99.69\tiny$\pm$0.43 & 98.63\tiny$\pm$1.36\\
                Our model (color\&affine DA) & \textbf{88.90}\tiny$\pm$1.25 & \textbf{99.25}\tiny$\pm$0.56 & \textbf{96.61}\tiny$\pm$1.23 & \textbf{97.14}\tiny$\pm$1.26 & \textbf{99.84}\tiny$\pm$0.35 & \textbf{99.24}\tiny$\pm$0.54\\
            \end{tabular}%
            % }
            \caption{\textbf{Impact of color and affine transformations on classification accuracy (\%)}. Transformations in parentheses are learned, others are predefined. Our model performs better than hand-defined transformations on the six different datasets. Best performances are obtained with a combination of learned color and affine transformations.}
            \label{tab:experiments_results}
        \end{table*}        

        \subsection{Color Transformations}
            In this section, we investigate the impact of color transformations alone on the training of an image classifier.
            Results are presented in Tab.\ref{tab:experiments_results}.
            For BACH dataset, using predefined color augmentations to train the model yields an increased classification performance compared to the baseline, but the model has the best performance when trained with the augmentations learned by our augmenter (+2.3\% accuracy VS baseline and +0.5\% accuracy VS predefined color augmentations). %When the amount of training data decreases, our model still performs better than predefined color transformations until a threshold of 50\% of the training set. Below this threshold, our model does not have enough data to learn transformations that are more useful than predefined ones.
            For Glas dataset, the classifier performs better with learned transformations than with predefined ones (+7.75\% over baseline and +1.25\% over baseline). %As the amount of training data decreases, our model performs better than predefined transformations until a threshold of 50\% of the training set and then performs on par.
            For Larynx 20x dataset, our model also performs better than predefined augmentations (+3.6\% accuracy VS baseline and +0.87\% accuracy VS predefined augmentations). %For this dataset, our model performs better than predefined color augmentations until a threshold of 25\% and performs on par below it.
            For Larynx 40x dataset, our model performs similarly to predefined transformations. However, in both cases, we do not see any improvement over the baseline, which indicates that either color transformations might not be the best ones to use for this dataset or that the performance of the classifier is already saturated to see the improvement brought by those transformations. %When using 50\% of the training set, our model is performing better, but does not have enough data to learn meaningful transformations when using only 25\% of the training set.
            For Brain 20x dataset, similarly to the Larynx 20x dataset, our model performs on-par with predefined transformations and brings no improvement over the baseline. Also in this case, it seems that color augmentations used are not useful to train the classifier or that the performance is too saturated to see the improvement.
            For Brain 40x dataset, our model performs slightly better than predefined augmentations (+0.6\% accuracy VS baseline and +0.13\% accuracy over predefined data augmentations. %Our model still performs better when using only 75\% of the training set but does not have enough data to learn useful transformations when using a smaller amount of data.

        \subsection{Geometric Transformations}
            In this section, we evaluate our model by investigating the impact of geometric transformations on the classification accuracy.
            Results are presented in Tab.\ref{tab:experiments_results}.
            For BACH dataset, our model performs better than predefined affine transformations (+2.1\% accuracy over baseline and +1.7\% over predefined transformations). However, it does not perform as well as when learning only color transformations, which indicates that this kind of transformations is less efficient for this dataset.%When decreasing the amount of data, our model still performs better than predefined affine transformations.
            For Glas dataset, our model performs also better than predefined affine transformations (+8.75\% accuracy over baseline and +0.5\% over predefined transformations). %When decreasing the amount of data, our model still performs better with only 75\% of the training data but below this threshold, it does not have enough data to learn interesting transformations.
            Interesting to note is also that for this dataset, affine transformations are helping more to train the model than using only color transformations.
            For Larynx 20x dataset, we can see that our model performs slightly better than predefined transformations (+8.68\% accuracy over baseline and +0.27\%  over predefined transformations). %Until a threshold of 75\% of the training set, our model performs better but below, predefined transformations are more efficient.
            Also for this dataset, using geometric transformations seems to help train the model more than using color transformations only.
            For Larynx 40x dataset, similarly to Larynx 20x dataset, our model performs slightly better (+8.57\% accuracy over baseline and +0.95\% over predefined augmentations) and affine transformations are more helpful than only color transformations. %Our model still performs better with only 75\% of the training set but does not learn useful transformations with less data. For this dataset, our model also performs better when trained with affine transformations than only with color transformations.
            For Brain 20x dataset, our model performs slightly better than predefined affine transformations (+0.16\% accuracy over baseline and +0.15\% over predefined affine transformations. As opposed to color transformations, affine transformations have a positive impact on the performance of the classification model. %Our model still performs better when the fraction of training data is reduced to 75\% but below, it is not performing as well as predefined augmentations, even though it still improves the training compared to the baseline.
            For Brain 40x dataset, our model performs also slightly better than predefined affine transformations (+1.66\% accuracy over baseline and +0.45\% over predefined augmentations). %With only 75\% of the training set, our method is still performing better but below, it does not learn interesting transformations to improve the classifier.
            Also for this dataset, affine transformations have a bigger positive impact on the model accuracy than color transformations.
            
        \subsection{Combination of color and affine transformations}
            In this section, we evaluate our model by investigating the impact of geometric transformations on the classification accuracy.
            Results are presented in Tab.\ref{tab:experiments_results}.
            For BACH dataset, the combination of both kind of transformations is significantly improving the classification score (+5.6\% accuracy over baseline and +4.3\% over predefined augmentations). %This behavior is also observed when using only 75\% of the training set. When using less data, our model is learning transformations that are still useful for the model but not as efficient as the predefined ones.
            For Glas dataset also, the combination of both color and geometric transformations is improving the model performance (+9.75\% accuracy over baseline and +1\% over predefined augmentations). %The model still performs better when using only a fraction of 75\% of the training set but learns transformations that are less efficient than predefined ones when using less training data.
            For Larynx 20x dataset, the combination of color and affine transformations learned by our model has a bigger positive impact on the model final accuracy (+9.1\% over baseline and +1.37\% over predefined transformations). %When decreasing the amount of training data, the transformations are still beneficial for the model until 75\% of the dataset. Below this threshold, there is a drop in the model accuracy and even if the model is still performing better than the baseline, predefined transformations are better when the amount of data is very low.
            For Larynx 40x dataset, learning both color and affine transformations is also yielding the model with the best classification accuracy (+10.47\% over baseline and +1.9\% over predefined transformations). %When decreasing the amount of training data, the model is still learning useful transformations until an amount of 75\% of the training data. Below this threshold, the transformations learned are not as efficient as predefined transformations.
            For Brain 20x dataset, our model learning color and affine transformations is performing slightly better than predefined transformations (+0.31\% over baseline and predefined transformations). %When decreasing the amount of training data, our model is still performing better than predefined DA with a fraction of the training set of 75\%. However, below this threshold, our model does not have enough data to learn interesting transformations and the predefined transformations are yielding a higher classification accuracy.
            For Brain 40x dataset, when learning color and affine transformations at the same time our model is performing better than the predefined transformations (+2.27\% over baseline and +0.3\% over predefined transformations). %When decreasing the fraction of the training set used for training the model to 75\%, our model is still performing better. However, if we decrease this fraction more, our model does not have enough data to learn helpful transformations anymore and is the transformations learned are less efficient than predefined transformations.
            To summarize the results of this series of experiments, we can see that the combination of both color and affine transformations yields the best results, which shows that our model became more invariant to color and shape perturbations thanks to the data augmentation transformation learned along the training.

        \subsection{Additional experiments on BACH dataset}

            In this section, we run a series of experiments on BACH dataset to have a better understanding of our model. BACH was chosen as it is the most challenging dataset of the six considered in terms of image size and difficulty of the classification task as shown in Tab.~\ref{tab:experiments_results}.

            % Figure environment removed

            In a first experiment, we investigate the evolution of the model accuracy with respect to the amount of training data. In Fig.\ref{fig:BACH_overview}, we can see that our model performs better than using only predefined transformations when using the full training set. When we reduce the amount of data gradually, we can see that the amplitude of the improvement decreases for color only and geometric only transformations. Below a threshold of 50\% of the training set, our model performs on par with predefined augmentations when learning color or geometric transformations only but yields an inferior performance when learning both types of transformations at the same time. In this case, our model does not have enough data to learn useful transformations. This shows that having a minimum amount of training data is a limitation and a prerequisite of our data-based learning method.

            \begin{table}[t]
                \small
                \centering
                \begin{tabular}{l|c|c}
                    BACH &  From Scratch & Pretrained model \\
                    \hline
                     Baseline & 71.60\tiny$\pm$1.29 & 83.30\tiny$\pm$1.18\\
                     Baseline + color & 75.20\tiny$\pm$1.04 & 85.10\tiny$\pm$1.19\\
                     Baseline + affine & 74.60\tiny$\pm$1.26 & 83.70\tiny$\pm$1.20\\
                     Baseline + color\&affine & 83.20\tiny$\pm$1.29 & 84.60\tiny$\pm$1.23\\
                    \hline
                     Our model (color DA) & 83.90\tiny$\pm$1.21 & 85.60\tiny$\pm$1.18\\
                     Our model (affine DA) & 82.70\tiny$\pm$1.99 & 85.40\tiny$\pm$1.25\\
                     Our model (color\&affine DA) & 85.60\tiny$\pm$1.29 & \textbf{88.90\tiny$\pm$1.25}\\
                \end{tabular}%
                % }
                \caption{\textbf{Impact of the pretraining on the classification accuracy (\%) on BACH dataset}. Transformations in parentheses are learned, others are predefined. The best classification accuracy is obtained when training a model pretrained on ImageNet. However, we can see that when training a model from scratch, the baseline accuracy is lower and using data augmentation has a bigger impact. (+14\% when training from scratch for our learned augmentations VS + 6.6\% when starting from a pretrained model.}
                \label{tab:experiments_training_from_scratch}
            \end{table}

            In a second experiment, we investigate the impact of starting from a pretrained model when training a classifier with our proposed method. In Tab.~\ref{tab:experiments_training_from_scratch}, we can see that the best classification accuracy is obtained when starting from a model pretrained on ImageNet. However, we can see that when training a model from scratch, the baseline accuracy is lower and using data augmentation has a bigger impact. (+14\% when training from scratch for our learned augmentations VS + 6.6\% when starting from a pretrained model). This experiment shows that using a pretrained model to boost the performances as usually done in the literature is helping, but using an appropriate data augmentation on top during training can further increase the final model performance.

        \subsection{Comparison with random sequences of data augmentation transformations}
            In Tab.~\ref{tab:comparison_sota}, we compare our model to a model trained with a RandAugment based framework on the six same datasets. To be fair in the comparison of the results, we limited the transformations in the RandAugment set of available transformations to the only ones that our model is learning.
            On 5 datasets, our model yields a better classification accuracy than the RandAugment based method. On Glas, both models yield similar results. Similarly to RandAugment, our model has only a few model-specific hyperparameters to tune (the augmenter network parameters). However, our model requires less prior knowledge as it does not require defining a precise list of possible transformations but works with a more generic set of differentiable transformations. Our intuition to explain the improved classification performance is that learning the optimal data augmentation for each epoch is beneficial for the model, as the time when the transformations are presented to the model is important as reported in \cite{Golatkar2019TimeMI}.\\
            
            \begin{table*}[t]
                \small
                \setlength\tabcolsep{2pt}                
                \centering
                \begin{tabular}{l|c|c|c|c|c|c}
                      & BACH & Glas & \makecell{Larynx\\ 20x} & \makecell{Larynx\\ 40x} & \makecell{Brain\\ 20x} & \makecell{Brain\\ 40x}\\
                    \hline
                    Baseline & 83.30\tiny$\pm$1.18 & 89.50\tiny$\pm$1.22 & 87.51\tiny$\pm$1.27 & 86.67\tiny$\pm$1.16 & 99.53\tiny$\pm$0.43 & 96.97\tiny$\pm$1.13\\
                    Predefined color\&affine DA & 84.60\tiny$\pm$1.23 & 98.25\tiny$\pm$1.12 & 95.24\tiny$\pm$1.25 & 95.24\tiny$\pm$1.37 & 99.53\tiny$\pm$0.43 & 98.94\tiny$\pm$1.26\\
                    \hline
                    \vspace{-4pt}
                    RandAugment & 87.25\tiny$\pm$1.48 & \textbf{99.25}\tiny$\pm$0.68 & 95.83\tiny$\pm$0.23 & 96.14\tiny$\pm$1.31 & 99.53\tiny$\pm$0.35 & 99.18\tiny$\pm$1.03\\
                    \tiny{(M,N) hyperparameters} & \tiny(3,2) & \tiny(3,2) & \tiny(4,2) & \tiny(4,2) & \tiny(3,3) & \tiny(3,3) \\
                    \hline
                    Our approach & \textbf{88.90}\tiny$\pm$1.25 & \textbf{99.25}\tiny$\pm$0.56 & \textbf{96.61}\tiny$\pm$1.23 & \textbf{97.14}\tiny$\pm$1.26 & \textbf{99.84}\tiny$\pm$0.35 & \textbf{99.24}\tiny$\pm$0.54\\
                \end{tabular}%
                \caption{\textbf{Comparison to a RandAugment based model in terms of classification accuracy (\%)}. Our model yields better results than a model trained in a RandAugment based framework on 5 datasets. On Glas it is performing on-par. Our model represents a good solution to learn the optimal data augmentation automatically for color and affine transformations.
                }
                \label{tab:comparison_sota}
            \end{table*}

    \begin{table*}
        \centering
        \begin{tabular}{cccc}
           \toprule
             & Color & Affine & Color and Affine \\
            \midrule
            BACH & % Figure removed & % Figure removed & % Figure removed \\
            Glas & % Figure removed & % Figure removed & % Figure removed \\
            HICL Larynx20x & % Figure removed & % Figure removed & % Figure removed \\
            HICL Larynx40x & % Figure removed & % Figure removed & % Figure removed \\
            HICL Brain20x & % Figure removed & % Figure removed & % Figure removed \\
            HICL Brain40x & % Figure removed & % Figure removed & % Figure removed \\
            \bottomrule
        \end{tabular}
        \caption{\textbf{Qualitative results.} For each dataset and each scenario, we see the evolution of the learned transformations along the training. Transformations at the beginning of the training are stronger and tend later to finer transformations useful enough to improve the classification accuracy of the trained model. In each row, the first image is the original patch and the last one is the same patch at the end of the training. The images in-between were extracted at respectively at 25\%, 50\% and 75\% of the total number of training epochs.}
        \label{tbl:table_of_images}
    \end{table*}

    \section{Conclusion}
        We have presented a novel approach to automatically learn the transformations needed for effective data augmentation for histopathological images. The method is based on an online approximation of the bilevel optimization problem defined by alternating between optimizing the model parameters and the data augmentation hyperparameters. By doing so, we train an augmenter network to generate the right transformations at the same time as we train the classifier network. We evaluated the proposed approach on 6 different datasets with different color and affine transformations. The obtained results were comparable to or better than the results obtained with hand-defined transformations. It also yielded better results that a model trained with a RandAugment based framework. This shows that our method is very suitable in the context of histopathological images where potentially useful transformations to train a classifier are not trivial to define by hand. It also eliminates the risk to select transformations that would degrade the model accuracy.
        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Mandatory Sections. Please complete, especially for final publication
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Acknowledgements.
% Please include any funding, intellectual contributions not included in the authorship, and any other acknowledgements.
\acks{This research was supported by the National Science and Engineering Research Council of Canada (NSERC), via its Discovery Grant program and MITACS via its Acc\'el\'eration program.}

% Ethical Standards.
% Please edit with the appropriate ethics considerations for your work. Include any pertinent IRB information, etc.
%
% Please note that the submission requirements included:
% The work presented must follow appropriate ethical standards in conducting research and writing the manuscript, following all applicable laws and regulations regarding treatment of animals or human subjects.
\ethics{The work follows appropriate ethical standards in conducting research and writing the manuscript, following all applicable laws and regulations regarding treatment of animals or human subjects.}

% Conflict of Interest
% Declaration of possible conflicts of interest: Authors must disclose any financial, organisational, commercial or personal conflicts of interest that might bias their work.
% If no conflicts, please say "We declare we don't have conflicts of interest."
\coi{We declare we don't have conflicts of interest.}


\bibliography{main}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices.
% \newpage

% Appendix is optional
\clearpage
\appendix


\end{document}