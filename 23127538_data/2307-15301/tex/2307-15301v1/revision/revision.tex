%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent Dear Associate Editor \& Reviewers,\\

Thank you for your time and valuable feedback. We greatly appreciate your input.
In response to your suggestions, we have made the necessary changes to the manuscript. We have provided a detailed account of these modifications and included the requested information in this response letter.
Furthermore, we have taken the opportunity to enhance the revision template by incorporating line numbers. 
This improvement will allow us to refer to specific modifications more accurately. From now on, we will utilize L to indicate the relevant lines in our correspondence.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Response to Associate Editor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q1: An analysis of computational performance and memory utilization is missing. I also agree that this should be included in order to understand whether the performance improvement obtained by the proposed approach comes at the cost of an increased computational effort.}

\vspace{2mm}
\noindent A: 
We analyzed the computational performance and memory utilization of our method in comparison with those of the other approaches. We measured the floating-point operations (FLOPs) and inference time to quantify the computational performance, and the number of deep network parameters for the memory utilization. We executed the codes of all methods with the same hardware, and reported the results in Tab.~\ref{tab:cost}. 
In addition, we also reported the metrics regarding each component of our {\mname}-2D in Tab.~\ref{tab:cost} to provide a complete understanding of the performance improvement and their computational cost.
In summary, while {\mname}-2D has the second-largest number of parameters, its FLOPs and inferences time are in-between the other methods in the literature. 
In our design, self-attention and cross-attention have a relatively high computational cost compared to the two-branch encoder and MMTM.


\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q2: The worse quantitative results reported for the baselines need to be motivated and explained.}

\vspace{2mm}
\noindent A:
The worse quantitative results reported for the baselines is because we trained all the methods using \textit{only} the FlyingThings3D train split, and evaluated them on the \textit{whole} FlyingThings3D test dataset and on the \textit{training} set of KITTI.
Note that we only evaluate on KITTI without training or finetuning our model with any of the KITTI's sequences.
We use the training set of KITTI as our evaluation set because the test set of KITTI is not publicly available.

As far as FlyingThings3D is concerned, previous methods removed data containing fast-moving objects from its test set.
For example, authors of RAFT-3D \cite{Raft-3D} state: ``\textit{We evaluate on the full images (excluding pixels at infinity and extremely fast moving regions with flow$>$250px)}".
Differently, we included this data as such visual challenges are of interests to the problem we study.
As a result, baseline methods perform slightly worse than what were reported in their respective papers.

As far as KITTI is concerned, comparison methods evaluated their performance by fine-tuning their models on KITTI.
Instead, we aim to evaluate the generalization capability of the methods, i.e. when they are trained on a scenario (FlyingThings3D) and tested on another scenario  (KITTI). Thus, our results on KITTI is different compared to those reported in their respective papers. 
%Therefore, because methods are not finetuned nor trained on KITTI, we directly use the training set for the evaluation.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q3: Information on the generalization capabilities and more details on the experimental results (particularly on the ablation study) should be added.}

\vspace{2mm}
\noindent A: 
The results presented in Tab.~\ref{tab:setting1_results} and Tab.~\ref{tab:4} (KITTI-Train columns) show that our algorithm can generalize to different scenarios without additional finetuning.
For this revision, we also collected a new RGBD dataset using a Realsense camera, featuring dynamic indoor scenes under varying illumination conditions, serving as extra qualitative evaluations for our method and comparison methods in terms generalization capability (see Fig.~\ref{fig:realtest}).
As we do not have optical flow ground-truth of this newly acquired dataset, we evaluated it qualitatively.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q4: The approach should be compared also with works more recent than RAFT-3D.}

\vspace{2mm}
\noindent A: 
We added a comparison with a recent state-of-the-art method. 
While Reviewer 15 suggested CamLiFlow~\cite{CamLiFlow}, we discovered that CamLiRAFT~\cite{camliraft2023} improved upon CamLiFlow~\cite{CamLiFlow} (from the same authors), scoring the current state-of-the-art performance. 
We thus reported the results of CamLiRAFT in Tabs.~\ref{tab:setting1_results} and \ref{tab:4}.
We also evaluated CamLiRAFT qualitatively on FlyingThings3D, KITTI, and our newly collected dataset (Figs.~\ref{fig:FlyingThings3D}, \ref{fig:KITTI}, and \ref{fig:realtest}), and assessed the computational cost in Tab.~\ref{tab:cost}. We have added corresponding discussion throughout the revised Sec.~\ref{sec:exp:comparisons}. 

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Reviewer 6}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q1: In the text, it would be nice if you explain or explicitly mention why you would not explore testing in another real dataset that is not KITTI (perhaps there isn't a public adverse weather dataset?). If you are going after robustness to dark or blurry images it would be more powerful to test on such a real dataset.}

\vspace{2mm}
\noindent A:
We could not find public datasets exhibiting adverse weather conditions that are suitable for assessing optical flow methods:
for example, the ``Multifog KITTI" \citeR{fogkitti} dataset offers both RGB and depth information in a foggy environment, but frames are not consecutive, thus being unsuitable for optical and scene flow analysis;
``RobotCar-Night \citeR{robotcar}" and ``nuScenes-night \citeR{nuscenes}" feature consecutive frames and real-world dark scenes, however, they are captured with camera and LiDAR sensor, thus lacking dense depth information which is different from the problem we are studying in this paper.

Therefore, we recorded a new indoor RGBD dataset using a Realsense D415 camera. This dataset features three lighting setups: \textit{Bright}, \textit{Dimmed}, and \textit{Dark}.
Each scenario is further divided into two splits: fast and slow motion.
%\fabiocomment{the dataset seems large, but in the paper you only show few qualitative results. what are you going to do with the other processed frames?}
Each sequence contains two or more moving individuals and objects.
As we could not produce ground-truth optical flow, we presented and discussed the qualitative results in Fig.~\ref{fig:realtest}. 
The Reviewer can find the description of the dataset at L[285] and the discussion of the qualitative results at L[407]. 
We will release all the acquired sequences and the qualitative results, together with the code release upon acceptance.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q2: CRAFT *aims to estimating* large motion displacements through a semantic smoothing transformer layer that integrates the features of one image and a cross-frame attention layer that replaces the original dot-product operator for correlation used in RAFT.}

\vspace{2mm}
\noindent A: We have corrected the typo at L[111] and corrected similar typos throughout the paper.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q3: By leveraging the Multimodal Feature Fusion (MFF) technique, FusionRAFT-2D can extracts* useful information from both RGB and Depth modalities, highlights* and exchanges* critical information between them to generate robust features, and maintains good performance in the AGN setting.}

\vspace{2mm}
\noindent A: We have rephrased the result discussion and corrected the typos.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q4: Tab.~IV reports the results for scene flow estimation in three settings. 
FusionRAFT-3D outperfors* RAFT-3D by more than 3.03\% and 2.55\% in ACC 0.05m for FlyingThings3D in the Standard setting, and by 5.51\% and 5.58\% in the Dark setting.}

\vspace{2mm}
\noindent A: We have rephrased the result discussion and corrected the typos.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q5: Table 5 should say in the caption it's FlyingThings3D evaluation.}

\vspace{2mm}
\noindent A: We have specified the dataset in the caption of (now) Tab.~\ref{tab:ablation_setting1}.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Reviewer 10}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q1: The approach in itself seems unnecessarily complicated to me as it seems like the combination of self- and cross-attention has a similar goal as the MMTM: Weighting the features extracted from the input data based on themselves and the features of the other modality. In the text, an information exchange is claimed, but I only see a weighting of separate streams of information.
Also, the ablation study and Table V don't answer the question of how MFF-SA + MFF-CA only compares to MMTM only.}

\vspace{2mm}
\noindent A: Self-attention, cross-attention, and MMTM serve different goals in our algorithm. 
Self-attention aims to promote intra-modality relationships at the early phase, followed by cross-attention that aims to promote inter-modality relationships.
Then, MMTM aims to promote a deeper and semantic-level exchange across modalities.

We validated how MFF-SA + MFF-CA compares to MMTM only in Tab.~\ref{tab:ablation_setting1}.
Moreover, as shown in Tab.~\ref{tab:ablation_setting1}, we experimentally show that the integration of self-attention, cross-attention and MMTM, benefits the model thanks to such diverse attention mechanisms.
The Reviewer can find the discussion of these results at L[420].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q2: The fact that the runtime performance and network size of the ablation study are neither mentioned nor compared or discussed is disappointing.}

\vspace{2mm}
\noindent A:
We have analyzed the runtime performance by measuring the floating-point operations (FLOPs) and the inference time, and measured the network size using the number of parameters. Tab.~\ref{tab:cost} reports those metrics for all compared methods and for each component of our method.
The Reviewer can find the discussion of these results at L[436].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q3: My only complaint is the structure of subsections in the related work section. I think that the RGB + Point Cloud Data and RGB + Depth sections should be parts of Multimodal fusion.}

\vspace{2mm}
\noindent A: We have merged the RGB + Point Cloud Data and RGB + Depth sections under Multimodal fusion. The reviewer can find the new structure in Sec.~\ref{sec:related_work}.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q4: Regarding the formatting of the paper, I do not like the positioning of the tables and figures for the evaluation in between the references.}

\vspace{2mm}
\noindent A: We have fixed the positioning of the tables and figures.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q5: I like the way how the Introduction is slowly guiding the reader into the topic. For a better motivation of the approach, a specific scenario could be outlined in which the proposed approach is optimal.}

\vspace{2mm}
\noindent A: We have added more specific scenarios in the Introduction.
Specifically, we added ``... for example those that can be encountered in search and rescue applications [17]." (L[58]).

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q6: In the related work section, a very good overview of the field in general and its history is given. I am missing some text to introduce what is explained at the beginning of the section.}

\vspace{2mm}
\noindent A: We have added an introductory text at the beginning of the Related Work section.
Specifically, we added ``\textit{We provide a comprehensive analysis of the recent progress in optical flow estimation using CNNs, followed by an in-depth investigation into the integration of multimodal fusion techniques for improving flow estimation performance.}" (L[81])

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q7: When mentioning ``All of these methods", it is not entirely clear what ``these" refers to.}

\vspace{2mm}
\noindent A: ``these" referred to the previously mentioned optical flow methods.
We have specified them in the paper as ``\textit{RAFT \cite{Raft}, GMA \cite{GMA}, and CRAFT \cite{CRaft} estimate the relationship between two consecutive frames using RGB images}" (L[129]).

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q8: Fig.~\ref{fig:method:mmf}(a) is a good representation of the presented encoder and helps in understanding the architecture. It would be even more useful if the text referred to it in a couple of explanations.}

\vspace{2mm}
\noindent A:
We have added three references to the Fig.~\ref{fig:method:mmf}(a) in Sec.~\ref{sec:methods:fusion_encoder}.
The Reviewer can find these modifications at L[191], L[206], and L[222].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q9: My biggest problem with the approach section, however, is that Fig.~\ref{fig:method:mmf}(b), and Fig.~\ref{fig:method:mmf}(c) are not really referred to or explained properly in the text. 
The explanation for the differences between the optical flow and the scene flow networks is completely missing.}

\vspace{2mm}
\noindent A:
As stated in RAFT-3D~\cite{Raft-3D}, ``\textit{a state-of-the-art optical flow architecture builds all-pairs correlation volumes and uses a recurrent unit to iteratively refine a 2D flow field}". This describes how RAFT~\cite{Raft} functions for optical flow estimation.
Differently, scene flow aims to estimate $T \in SE(3)^{H\times W}$, where $H$ and $W$ are the height and width of the output. RAFT-3D~\cite{Raft-3D} ``\textit{iteratively updates the rigid-motion embeddings such that pixels with similar embeddings belong to the same rigid object and follow the same SE3 motion}''.
We included this explanation at L[103] and L[151].


\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q10: I understand that the formulas keep the same style as before and that the figure is similar to the general depiction in the original paper introducing the MMTM, but this is unnecessarily confusing.}

\vspace{2mm}
\noindent A: 
%\Yiming{requires a second look here}
We have revised the equations and the block diagram to facilitate understanding.
Specifically, we have simplified
%=========================
\begin{equation}
\begin{aligned}\nonumber
&{E_{\bm{\tilde {F}}^t_{k}}} = {\bm{\mbox{W}}_{\bm{\tilde {F}}^t_{k}}}[{{S_{\bm{\tilde {F}}^t_{r}}}},{{S_{\bm{\tilde {F}}^t_{d}}}}] + {b_{\bm{\tilde {F}}^t_{k}}} \\
&\tilde {\bm{{F}}}^t_{k} = 2 \times \sigma ({\bm{\mbox{W}}_{\bm{\tilde {F}}^t_{k}}}[{{S_{\bm{\tilde {F}}^t_{r}}}},{{S_{\bm{\tilde {F}}^t_{d}}}}]) \odot \bm{\hat{\hat{F}}}^t_k\\
\end{aligned}
\end{equation}
%=========================
into
%=========================
\begin{equation}\nonumber
\tilde{\bm{ {F}}}^t_{k} = 2\sigma ({\bm{\mbox{W}}_{\bm{\tilde {F}}^t_{k}}}{Z}) \odot \bm{\hat{\hat{F}}}^t_k.
\end{equation}
%=========================
We included this modification at L[230-231].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q11: In Eq.~3, the concatenation of the modality feature vectors is part of the MMTM, but in the Figure 1(b) and (c), it is not.}

\vspace{2mm}
\noindent A: 
We have revised Eq.~(4) as:
%by removing
%$[\tilde {\bm{{F}}}^t_{r},\tilde{\bm{ {F}}}^t_{d}] = {M_\theta }(  {\bm{\hat{\hat{F}}}^t_r},{\bm{\hat{\hat{F}}}^t_r})$ from Eq. (3)
%and rewriting Eq.~(4) as
%=========================
\begin{equation}\nonumber
\small
\begin{aligned}
    &fnet({P}^t) = [\tilde {\bm{{F}}}^t_{r},\tilde{\bm{ {F}}}^t_{d}] = [{M_\theta}({C_\theta}({S_\theta}({I^t}), {S_\theta}({Z^t})))],\\
    &\textbf{C}({P^t},{P^{t + 1}}) = \langle fnet({P^t}),fnet({P^{t + 1}})\rangle.
\end{aligned}
\end{equation}
%=========================
to formalize how MFF encoder operates, ensuring consistency with the block diagram.
Please see this modification at L[240-241].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent\textbf{Q11: In equation 3, the concatenation of the modality feature vectors is part of the MMTM, but in the Figure, it is not.}

% \vspace{2mm}
% \noindent A: We have corrected the figure accordingly.

% \vspace{-2mm}
% \noindent\rule{\linewidth}{.1mm}






% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q12: While the evaluation gives some very interesting insights, I think that it is also missing information on the network size and performance as well as further ablation studies as already mentioned. The implementation details do not carry much meaning when the network size and runtime performance are unknown.}

\vspace{2mm}
\noindent A:
We have analyzed the runtime performance by measuring the floating-point operations (FLOPs) and the inference time, and measured the network size through the number of parameters in Tab.~\ref{tab:cost}.
The Reviewer can find the discussion of these results at L[436].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q13: I do not understand, why KITTI was evaluated on training data. the original training data set of KITTI could have been split again by the authors to provide an independent test set. Using it to argue for generalization while testing on training data seems counterproductive.}

\vspace{2mm}
\noindent A:
Note that we use the model trained on FlyingThings3D to evaluate on KITTI. We do not train or finetune our model with any of the KITTI's sequences. We adopt such evaluation protocol to evaluate the generalization capability of all models, in cases when they are trained on a scenario (FlyingThings3D) and tested on another scenario (KITTI).
Moreover, we use the training set of KITTI as our evaluation set because the test set of KITTI is not publicly available.
%Therefore, because methods are not finetuned nor trained on KITTI, we directly use the training set for the evaluation.
We added this explanation at L[280].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q14: The beta in Figure 2 seems to be inconsistent with the one in the text as it is defined as 1/N. I think that the beta in the figure refers to the N in the text.}

\vspace{2mm}
\noindent A: We have corrected this inconsistency at L[332].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q15: Tables I-III could be summarized into a single larger table as the only difference is the setting.)}

\vspace{2mm}
\noindent A: We have combined the (previous) Tab. I-III into (now) Tab.~\ref{tab:setting1_results}.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q16: Also, it is not clear from the caption what the upper and lower portions of them mean. (The lower part is scene flow, put the caption only says optical flow.)}

\vspace{2mm}
\noindent A: The upper part is optical flow and the lower part is scene flow. 
We have corrected the caption of scene flow in Tab.~\ref{tab:4}.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q17: When discussing accuracies, it is more precise to call the differences percent points instead of percent.}

\vspace{2mm}
\noindent A: We have revised the result discussion to improve the clarity regarding the unit of the metrics and the differences among them.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q18: The caption of table IV says ``optical flow'' instead of ``scene flow''.}

\vspace{2mm}
\noindent A: We have corrected the caption in (now) Tab.~\ref{tab:4}.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q19: It is quite interesting that the AGN setting yields improvements for RAFT-3D.}

\vspace{2mm}
\noindent A:
The AEPE metric shows a slight improvement for RAFT-3D in the AGN setting over the Standard setting. The reason for this particular behavior is that AEPE computes the Average End-Point Error with \textit{all} the valid pixels for optical and scene flow ($\rm AEPE^{all}_{2D}$ and $\rm AEPE^{all}_{3D}$).
Average is known to be sensitive to outliers. In our case the outliers are large flow errors.
If we instead compute the median of the End-Point Error, we can see in Tab.~\ref{tab:median} of this response letter that the AGN error of RAFT-3D is higher than Standard.
For completeness, in Fig.~\ref{fig:histograms} of this response letter, we also added the error histograms in various evaluation cases for both optical flow and scene flow on FlyingThings3D-clean and FlyingThings3D-final.
We can observe how large errors greatly affect the computation of the average, while the median value is more stable across the experiments.
We have enriched our discussion regarding this observation in the paper at L[366].

%+++++++++++++++++++++++++
\begin{table}[h]
    \tabcolsep 3pt
    \centering
    \caption{The median of optical and scene flow in $\rm AEPE^{all}_{2D}$ and $\rm AEPE^{all}_{3D}$ for RAFT-3D.}
    \vspace{-.2cm}
    \label{tab:median}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|cc|cc}
        \toprule
        & \multicolumn{2}{c|}{optical flow} & \multicolumn{2}{c}{scene flow} \\
        & FlyingThings3D-clean & FlyingThings3D-final & FlyingThings3D-clean & FlyingThings3D-final \\
        \midrule
        Standard & 3.58$\rightarrow$0.127 & 3.57$\rightarrow$0.128 & 0.186$\rightarrow$0.009 & 0.180$\rightarrow$0.010 \\
        AGN & 3.14$\rightarrow$0.130 & 3.21$\rightarrow$0.136 & 0.193$\rightarrow$0.011 & 0.144$\rightarrow$0.011 \\
        \bottomrule 
    \end{tabular}
    }
\end{table}
%+++++++++++++++++++++++++

% ********************************
% Figure environment removed
% ********************************



\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q20: The name of the approach could be mentioned in the abstract.}

\vspace{2mm}
\noindent A: We have inserted the approach name into the abstract.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q21: ``RGBD" and ``RGB-D" are used inconsistently}

\vspace{2mm}
\noindent A: We have corrected the paper by using RGBD consistently.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q22: ``Depth" is capitalized inconsistently.}

\vspace{2mm}
\noindent A: We have corrected `Depth' in `depth' where needed.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q23: References of instance normalization and relu should be moved to the sentence first mentioning instance normalization and relu.}

\vspace{2mm}
\noindent A: 
We have moved the references to the appropriate place at L[202].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q24: In some table headers, capitalization is inconsistent (``method'' for example).}

\vspace{2mm}
\noindent A: We have corrected all the capitalizations.

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}




 













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Reviewer 15}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q1: The only remark in this sense is that the dimensionality of the output vectors at each stage has to be deduced by the dimensionality of the weight matrices, while explicitly writing them could improve the flow of the explanation.}

\vspace{2mm}
\noindent A: We have specified the output dimensionality where needed, for example at L[209], L[212], and L[224].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q2: The authors should expand the discussion on this aspect since performance vs computational cost is often an important trade-off for applications.}

\vspace{2mm}
\noindent A: 
We have analyzed the runtime performance by measuring the floating-point operations (FLOPs) and the inference time, and measured the network size through the number of parameters in Tab.~\ref{tab:cost}.
The Reviewer can find the discussion of these results at L[436].


\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q3: The ablation study is performed only on the darkened inputs, however, it should be discussed also for the standard images. 
While it is interesting that the architecture is more robust to this kind of noise, still the relative merits of the different architectural choices should be demonstrated on the standard inputs and not only on the edge cases. }

\vspace{2mm}
\noindent A: We have included the ablation study on the Standard setting in Tab.~\ref{tab:ablation_setting1}. Similar to the Dark setting, we can observe that each component we add leads to some incremental contribution for improving the optical flow estimation with respect to the RGB baseline. The Reviewer can find the discussion of these results at L[426].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q4: While the comparison with RAFT-3D is significant, this method is from 2020, and other solutions showed up in the meantime. I'm not aware of all the relevant methods in this sub-field, but as an example, there is CamLiFlow that shows better performances than RAFT-3D and should be included in the comparison. The same for other methods that could have been proposed in 2022.}

\vspace{2mm}
\noindent A: We have added the comparison with the more recent CamLiRAFT in Tab.~\ref{tab:setting1_results} and Tab.~\ref{tab:4}.
The Reviewer can find the discussion of these results at L[357], L[378], and L[386].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}
















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Reviewer 20}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q1: Are the results of SotA methods generated from the pre-trained models? If so, why do the FlyingThings3D and KITTI results for SotA methods do not match the original paper? Are you using a different data pre-processing strategy? If the authors have similarly trained all SotA methods on FlyingThings3D only, please refer to it explicitly as the differences between KITTI results compared to the original papers are significant.}

\vspace{2mm}
\noindent A: 
The worse quantitative results reported for the baselines are because we evaluate all the methods on the whole FlyingThings3D test dataset and on the training set of KITTI.
Note that we train all the models only using FlyingThings3D.

For the evaluation of FlyingThings3D, previous methods removed data containing fast-moving objects from its test set, as stated by authors of RAFT-3D \cite{Raft-3D}: ``\textit{We evaluate on the full images (excluding pixels at infinity and extremely fast moving regions with flow$>$250px)}".
Instead, as such data are of interest to our problem, we therefore included those challenging data in our evaluation. This leads to the performances of compared methods on FlyingThings3D being slightly worse than the results reported in their respective papers.

Regarding KITTI, comparison methods evaluated their performance by fine-tuning their models on KITTI. Instead, we aim to evaluate the generalization capability of all the models from one domain (FlyingThings3D) to another domain (KITTI). Thus, we do not train or finetune our model with any of the KITTI's sequences. We use the training set of KITTI as our evaluation set because the test set of KITTI is not publicly available. This explains the differences between our results and those of the respective papers.
%
%As far as KITTI is concerned,
%We decided to test generalization capabilities of the methods when they are trained on a scenario domain (FlyingThings3D) and tested on another scenario domain (KITTI), which is a more realistic setup.
%Therefore, because methods are not finetuned nor trained on KITTI, we directly use the training set for the evaluation.
%This results in methods performing slightly worse than what is reported in their respective papers.
The Reviewer can find the related discussion at L[272].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q2: In the ablation study Table V: It is not clear how the authors examine RGB and depth without the other components (SA), (CA), and (MMTM).
I would assume RGB only is equivalent to RAFT but how RGB + depth are combined without the other modules in Table V. 
If this is an early fusion, how is the architecture different from RAFT or RAFT-3D?}

\vspace{2mm}
\noindent A:
Based on the original RAFT and RAFT-3D's feature encoders, we build the baseline of our encoder as a two-branch network: one for RGB, and the other for depth. 
At the end of the baseline two-branch network, we concatenate features from RGB and depth branches together. 
In this way, we can work with two branches without SA, CA, and MMTM modules.
We explained in detail the baseline in the paper at L[252].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q3: The use of self-Attention (SA) is only important with the RGB branch which can optimize optical flow results. Could it make sense to use this module for the RGB branch only and omit it for the depth branch?}

\vspace{2mm}
\noindent A:
We have tested this alternative configuration and reported the results in Tab.~\ref{tab:ablation_setting1}.
We observed that applying SA to both depth and RGB branches brings a slightly higher improvement on the flow estimation than only applying SA to the RGB branch (Exp 5 vs 6 in the Standard setting and Exp 14 vs Exp 15 in the Dark setting).
The Reviewer can find the discussion of these results at L[426].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q4: What does ``repeat N times'' mean in Fig.~\ref{fig:method:mmf}. 
Is the model (MMTM) repeated sequentially? 
How often and why?}

\vspace{2mm}

\noindent A: 
``repeat N times'' indicates that the MMTM is utilized N times consecutively within the network for the fusion of two modalities. We revised ``repeat N times'' to ``repeat N times sequentially'' in Fig.~\ref{fig:method:mmf}.
We apply MMTM for N = 3 times as suggested in the original paper~\cite{MMTM}.
We added this clarification in the paragraph regarding implementation details at L[340].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q5: What is the twist field in Fig.~\ref{fig:method:mmf}(c)?}

\vspace{2mm}
\noindent A: The ``twist field" in our Fig.~\ref{fig:method:mmf}(c) is defined in \cite{Raft-3D} as:
%
\begin{equation}\nonumber
    \mbox{Twist field: } log_{SE3}(\mathbf{T}),
\end{equation}
%
where ``$\mathbf{T} \in \mbox{SE(3)}^{H\times W}$ \textit{represents the 3D motion between a pair of frames}''.
We have replaced ``twist'' with ``log()'' in Fig.~\ref{fig:method:mmf}(c) to improve clarity.
%, and explained it with the relative reference to the paper at L{\color{red}[COMPLETE]}.


\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q6: Are KITTI results included in all tables with or without fine-tuning?}

\vspace{2mm}
\noindent A: 
The results regarding KITTI in all tables are without any training or finetuning with any of the KITTI's sequences. We adopt this evaluation protocol to evaluate the generalization capability of all the methods.
%We use the training set of KITTI as our evaluation set because the test set of KITTI is not publicly available.
We clarified this at L[281].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q7: Some unclear evaluation metrics are used without being explicitly defined, such as AEPE (epe{\textless{100}}) in Tables I, II, and III and AEPE (epe{\textless{1}}) in Table IV. What do they mean exactly? }

\vspace{2mm}
\noindent A: 
$\rm AEPE^{epe{\textless{100}}}_{2D}$ measures the average among end-point error (EPE) that are under 100 pixels.
$\rm AEPE^{epe{\textless{1}}}_{3D}$ measures the average among end-point error (EPE) that are under 1 meter. 
We highlighted this clarification at L[303].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Q8: How are MEAN\_AEPE and MEAN\_ACC in Table V different from AEPE and ACC, respectively? }

\vspace{2mm}
\noindent A: 
$\rm MEAN_{AEPE}$ and $\rm MEAN_{ACC}$ are the mean values of AEPE and ACC, respectively, across FlyingThings3D-clean and FlyingThings3D-final.
We have clarified this at L[313].

\vspace{-2mm}
\noindent\rule{\linewidth}{.1mm}