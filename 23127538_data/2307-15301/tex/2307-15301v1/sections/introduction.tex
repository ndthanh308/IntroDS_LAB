%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\IEEEPARstart{O}{ptical} flow algorithms are essential for determining the motion of objects or regions within images between consecutive video frames. They generate a 2D vector field that describes the apparent movement of pixels over time. In contrast, scene flow focuses on estimating the pixel-level 3D motion in stereo or RGBD video frames \cite{Raft-3D}. These algorithms find wide applications in robotics \cite{FusionSLAM2020, navigation} and surveillance \cite{action_recognition1, action_recognition2}.
Computing optical flow becomes particularly challenging in environments with non-informative textures or when scenes are captured under low-lighting conditions. To address these difficulties, deep learning methods have emerged as effective solutions for optical flow estimation, formulating the problem as an energy minimization task \cite{Flow_Net,Flow_Net2.0,Flow_fields,Flow_fields++}. Deep learning-based optical flow approaches have demonstrated significant improvements over traditional methods \cite{Raft,CRaft,CamLiFlow}.

Several approaches utilize the computation of a correlation volume in the visible spectrum (RGB) to estimate the optical flow between two frames \cite{Flow_Net, Raft, CRaft}. The correlation volume captures inter-frame similarity by taking the dot product of the corresponding convolutional feature vectors and can be generated through an end-to-end deep network. This deep network can be designed to minimize an underlying energy function.
However, relying solely on RGB information can be limited in scenes affected by motion blurs, non-informative textures, or low illumination conditions. 
To address this limitation, some approaches have incorporated multimodal information. For example, depth or point cloud data can provide an alternative representation of the underlying scene structure. 
This multimodal information can be integrated through \textit{late fusion}, where feature vectors are combined without intermediate information exchange~\cite{DeepLiDARFlow, Raft-3D}, or through exchanging information between branches while sacrificing the independence of the single-modality representation~\cite{CamLiFlow}.

In this paper, we present a novel multimodal fusion approach, named  {\mname}, for optical and scene flow estimation, specifically designed to handle data captured in noisy or low-lighting conditions, for example those that can be encountered in search and rescue applications~\cite{Murphy2009}. 
Our approach introduces three key components to address these challenges.
Firstly, we propose a feature-level fusion technique that seamlessly blends RGB and depth information using a shared loss function.
Secondly, we introduce a self-attention mechanism that enhances the expressiveness of feature vectors by dynamically balancing the importance of features within each individual modality.
Lastly, we incorporate an optimized cross-attention module that facilitates information exchange and balance between RGB and depth modalities.
We integrate these new modules within RAFT~\cite{Raft} and RAFT-3D~\cite{Raft-3D}, using an application-oriented data augmentation strategy to learn robust feature representations that make optical and scene flow estimation effective in complex environments.
We conduct extensive evaluations on standard optical and scene flow benchmarks, as well as on two new settings that we introduce to assess robustness against photometric noise and challenging illumination conditions. 
Our method achieves state-of-the-art performance on the synthetic dataset FlyingThings3D \cite{flythings} and demonstrates superior generalization capabilities on the real-world dataset KITTI \cite{kitti} without fine-tuning.

