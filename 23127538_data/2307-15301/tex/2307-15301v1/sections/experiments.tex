%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

We compare {\mname} against state-of-the-art approaches on the FlyingThings3D~\cite{flythings} and KITTI~\cite{kitti} datasets. 
We design two experimental settings to mimic corrupted RGB images and poor lighting condition scenarios.
We also evaluate on data we acquired with a RGBD sensors in various lighting conditions.
We report both quantitative and qualitative results, and carry out ablation studies.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental setup}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Datasets.}
FlyingThings3D~\cite{flythings} is split into \textit{clean} and \textit{final} sets containing dynamic synthetic scenes.
The former is composed of 27K RGBD images including changing lights and shading effects, while the latter is an augmented version of the former with simulated challenging motions and blurs.
Each set contains train and test splits.
Unlike~\cite{Raft,Raft-3D,CamLiFlow}, we use the \textit{whole} training set of FlyingThings3D and sample 1K RGBD image pairs from the \textit{whole} test set, accounting regions containing fast-moving objects in the evaluation for its relevance to our study.
KITTI consists of real-world scenes captured from vehicles in urban scenarios.
Because the original dataset does not provide depth data, we use the disparity estimated by GA-Net~\cite{Ga-net} as in~\cite{Raft-3D}.
We only evaluate on KITTI, we do not train or finetune our model with any of the KITTIâ€™s sequences. 
We use the training set of KITTI as our evaluation set because KITTI's test set is not publicly available.
KITTI evaluation allows us to assess the ability of our algorithm in generalizing from synthetic data to real data.
To further validate the performance of {\mname} in real-world scenarios, we collect an RGBD dataset using a Realsense D415 camera in an indoor office with moving people under three lighting setups, named Bright, Dimmed, and Dark.
The Bright setting features bright lights and moving objects are clearly visible.
The Dimmed setting features dimmed lights and the moving objects can be seen.
The Dark setting features very low lighting conditions and the moving objects can be hardly seen.
We only assess the qualitative results because we did not produce optical flow ground-truth for this dataset.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Evaluation metrics.}
We quantify the optical and scene flow results using conventional evaluation metrics \cite{Raft, CRaft, Raft-3D}:
for the optical flow we use $\rm AEPE_{2D}$(pixel), $\rm {ACC}_{1px}$(\%) and $\rm Fl^{all}_{2D}$(\%), 
for the scene flow we use $\rm AEPE_{3D}$(m), $\rm {ACC}_{0.05m}$(\%), $\rm {ACC}_{0.10m}$(\%) and $\rm Fl^{all}_{3D}$(\%).
$\rm AEPE_{2D}$(pixel) measures the average end-point error (EPE) \cite{Raft}, which is the average value of the 2D flow error.
$\rm AEPE^{epe{\textless{100}}}_{2D}$ measures the average end-point error (EPE) in the evaluation which is under 100 pixels. 
$\rm AEPE_{3D}$(m) is the euclidean distance (EPE for 3D) between the ground truth 3D scene flow and the predicted results.
$\rm AEPE^{epe{\textless{1}}}_{3D}$ measures the average end-point error (EPE) in the evaluation which is under 1 meter.
$\rm {ACC}_{1px}$(\%) \cite{Raft-3D} measures the error portion within a threshold of one pixel.
$\rm {ACC}_{0.05m}$(\%) \cite{Raft-3D} measures the error portion within a threshold of 0.05 meters, while $\rm {ACC}_{0.10m}$(\%) \cite{Raft-3D} measures the error portion within a threshold of 0.10 meters.
$\rm MEAN_{AEPE}$ and $\rm MEAN_{ACC}$ are the average values of $\rm AEPE^{all}_{2D}$ and $\rm ACC_{1px}$, respectively, calculated over FlyingThings3D-clean and FlyingThings3D-final.
$\rm Fl^{all}_{2D}$(\%) \cite{CRaft} is the percentage of outlier pixels whose end-point error is $>3$ pixels or $5\%$ of the ground-truth flow magnitude.
$\rm Fl^{all}_{3D}$(\%)~\cite{Flownet3d++} is the percentage of outlier pixels whose Euclidean distance (EPE for 3D) between the ground truth 3D scene flow and the predicted one is $>0.3$ m or $5\%$ of the ground truth flow magnitude.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Evaluation settings.}
Environments with poor light conditions lead to weak texture information that can compromise the stability of feature representation.
Also additive Gaussian noise can affect optical and scene flow estimation.
To assess the robustness, we design three experimental settings:
\emph{Standard}: we use the original version of the dataset;
\emph{AGN}: we apply Additive Gaussian Noise on RGB images;
\emph{Dark}: we darken RGB images.
In AGN we randomly sample noise values ($\alpha$) from a normal distribution centered in zero with a standard deviation equal to 35.
In Dark we multiply pixel values by a random factor ($\beta$) with values $\{0,\frac{1}\beta\}$, where $\beta$$\in$$\mbox{U}[1,9]$.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Implementation details.}
We implemented \mname{} in PyTorch. 
All modules are initialized with random weights.
We train our network for 100K iterations with the batch size of 6 on 3 Nvidia 3090 GPUs.
During training, we set the initial learning rate at $1.25\cdot10^{-4}$ and use linear decay. 







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparisons}\label{sec:exp:comparisons}
%
We compare \mname{} against RGB methods for 2D optical flow estimation, i.e.~RAFT~\cite{Raft}, GMA~\cite{GMA}, CRAFT~\cite{CRaft}, and Separable flow~\cite{Separable_flow}, and against methods for 3D scene flow estimation, i.e.~RAFT-3D~\cite{Raft-3D} and CamLiRAFT~\cite{camliraft2023}.
See Sec.~\ref{sec:related_work} for the description of these methods.




\input{tables/table_of_comparison}
\input{tables/table_sf_comparison}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Quantitative results}



Tab.~\ref{tab:setting1_results} (top) reports optical flow results in Standard setting.
{\mname}-2D outperforms GMA by $+1.56\%$ and $+1.55\%$ in terms of $\rm ACC_{1px}$, and $+0.91$ and $+0.78$ in terms of $\rm AEPE^{all}_{2D}$ in FlyingThings3D-clean and FlyingThings3D-final, respectively.
{\mname}-3D outperforms RAFT-3D by $+1.44\%$ and $+1.42\%$ in terms of $\rm ACC_{1px}$, and $+1.00$ and $+0.88$ in terms of $\rm AEPE^{all}_{2D}$. 
While RAFT-3D extracts features only from RGB images, our MFF encoder extracts features from both RGB and depth, producing more informative internal representations.
{\mname}-3D outperforms CamLiRAFT by $+3.86\%$ and $+4.13\%$ in terms of $\rm ACC_{1px}$, and $+0.45$ and $+0.15$ in terms of $\rm AEPE^{all}_{2D}$.

\input{figures/figure_of_comparison_ft}
\input{figures/figure_of_comparison_kitti}
\input{figures/figure_of_comparison_ours}

Tab.~\ref{tab:setting1_results} (middle) reports optical flow results in AGN setting.
{\mname}-2D outperforms CRAFT by $+3.94\%$ and $+3.96\%$ in terms of $\rm ACC_{1px}$, and $+1.15$ and $+1.29$ in terms of $\rm AEPE^{all}_{2D}$ in FlyingThings3D-clean and FlyingThings3D-final, respectively.
{\mname}-3D outperforms RAFT-3D by $+2.16\%$ and $+2.45\%$ in in terms of $\rm ACC_{1px}$ and $+0.51$ and $+0.61$ in terms of $\rm AEPE^{all}_{2D}$. 
$\rm AEPE^{all}_{2D}$ of RAFT-3D is lower than that of the Standard setting.
This is because $\rm AEPE^{all}_{2D}$ computes the error average, and the average is known to be sensitive to outliers. 
In fact, if we compute the median (less sensitive to outliers), we observe that the performance of RAFT-3D in AGN is worse than the Standard setting.
For example, RAFT-3D in FlyingThings3D-clean achieves a median value of $\rm EPE^{all}_{2D}$ of 0.127 and 0.130 in the Standard and AGN settings, respectively.


Tab.~\ref{tab:setting1_results} (bottom) reports optical flow results in Dark setting.
RGB methods perform worse than the previous settings, whereas our {\mname} methods outperform RGB methods, RAFT-3D, and CamLiRAFT.
Tab.~\ref{tab:setting1_results} reports $\rm AEPE^{all}_{2D}$ and $\rm Fl^{all}_{2D}$ on KITTI without fine-tuning the models.
Although CamLiRAFT demonstrates a better generalization capability than RAFT-3D, {\mname} outperforms almost all the other methods in all three settings, showing its robustness and adaptability in real-world scenarios.




Tab.~\ref{tab:4} reports scene flow results.
{\mname}-3D outperforms RAFT-3D on both FlyingThings3D-clean and FlyingThings3D-final.
CamLiRAFT scores on par with {\mname}-3D in all three settings.
We notice that {\mname}-3D performs better in terms of $\rm ACC_{0.05m}$, while CamLiRAFT performs better in terms of $\rm ACC_{0.10m}$. 
This suggests that {\mname} produces smaller flow errors than CamLiRAFT.
{\mname}-3D performs stably across all three settings, while RAFT-3D and CamLiRAFT performance degrades in the AGN and Dark settings.
On KITTI, {\mname}-3D is the best-performing method on all three settings in terms of $\rm Fl^{all}_{3D}$. 
In terms of $\rm AEPE_{3D}^{all}$, CamLiRAFT performs better in the Standard and AGN settings, while {\mname}-3D scores the best in the Dark setting.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Qualitative results}

We provide examples of qualitative optical flow results with their corresponding $\rm AEPE^{all}_{2D}$.
We visualize the error with respect to the ground truth: the stronger the magenta, the higher the error.
Fig.~\ref{fig:FlyingThings3D} shows the results of optical flow errors in FlyingThings3D.
Both {\mname}-2D and {\mname}-3D consistently produce smaller $\rm AEPE^{all}_{2D}$ values than the other methods, which can also be visually verified.
Fig.~\ref{fig:KITTI} shows the results on KITTI. 
Both {\mname}-2D and {\mname}-3D produce smaller $\rm AEPE^{all}_{2D}$ values and less magenta area for error compared to the other methods.
Fig.~\ref{fig:realtest} shows the flow estimation on our own indoor dataset for RAFT, GMA, RAFT-3D, CamLiRAFT, and {\mname}.
In the Bright setting, all compared methods produce good-quality results.
In the Dimmed setting, RAFT, GMA, and CamLiRAFT show low-quality results, which we can observe from the poor edges produced by the moving objects.
In the Dark setting, {\mname} is the only one that produces results where the moving objects are distinguishable.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Computational time}
Tab.~\ref{tab:cost} shows the results of our analysis.
We run all the algorithms on the same computer.
We measure the number of parameters, Floating-Point Operations (FLOPs), and inference time on FlyingThings3D data.
{\mname}-2D has the second-largest number of parameters and the third-largest number of FLOPs.
Its inference time is in-between the other methods in the literature.
Self-attention and Cross-attention have high computational cost.
Others are all the other modules to compute the optical flow.
Although the number of {\mname}-3D is one order of magnitude larger than CamLiRAFT, our inference time is slightly higher.

\input{tables/table_computation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation study}
Tab.~\ref{tab:ablation_setting1} reports the ablation study on self-attention (SA), cross-attention (CA), and Multimodal Transfer Module (MMTM) on the FlyingThings3D dataset in both Standard and Dark settings. 
Overall, we can observe that all the components we added provide an incremental contribution to improve the quality of the output optical flow compared to the RGB baseline.
SA and CA consistently improve performance (see Exp 3 vs 6 vs 8, 4 vs 7 vs 9, and similarly for the Dark setting).
The SA applied to the depth is better than applying it to the RGB (see Exp 5 vs 6, and 14 vs 15).
MMTM fusion outperforms simple concatenation of RGB with depth consistently in the Dark setting (see Exp 12 vs 13, 15 vs 16, 17 vs 18).
There is one case in the Standard setting where this last does not occur (see Exp 6 vs 7).
As expected, the best performance is achieved when all the modules are activated.
In fact, SA focuses on intra-modality relationships, CA focuses on inter-modality relationships, and MMTM further exchanges information across modalities.

\input{tables/table_ablation_study}









