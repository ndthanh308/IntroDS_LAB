%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our approach}\label{sec:method}
We present a Multimodal Feature Fusion (MFF) Encoder that performs early fusion of RGB and depth modalities to improve the estimation of both optical and scene flow under noisy or poor lighting conditions.
Our encoder is flexible and can be integrated into flow estimation frameworks by replacing their original feature encoder. 
To achieve this, we employ self-attention, cross-attention, and Multimodal Transfer Module (MMTM) \cite{MMTM}. 
We extract low-level features from each modality and improve their expressivity using self-attention.
Cross-attention enables the network to attend to the most informative modality. 
MMTM is used to further fuse the attended features that are computed from the two modalities.
Fig.~\ref{fig:method:mmf}(a) shows the architecture of our encoder.
% ********************************
% Figure environment removed
% ********************************











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multimodal Feature Fusion Encoder} \label{sec:methods:fusion_encoder}

The Multimodal Feature Fusion Encoder takes a pair of consecutive RGBD frames $(P^t,~P^{t+1})$ at time $t$ as input. Each frame $P^t = \{I^t, Z^t\}$ is composed of a RGB image $I^t$ and a depth image $Z^t$.

We first obtain low-level features $\bm{{F}}^t_r\in \mathbb{R}^{W \times H \times D}$ and $\bm{{F}}^t_d\in \mathbb{R}^{W \times H \times D}$ from each modality with convolutional blocks, where we use the subscript ${r}$ to represent the RGB branch and ${d}$ for the depth branch (Fig.~\ref{fig:method:mmf}(a)).
${D}$ is the feature dimension and ${W \times H}$ is the resolution of the features.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Self-attention.}
The local features $\bm{{F}}^t_r$ and $\bm{{F}}^t_d$ are obtained with convolutions that have limited receptive fields, therefore we model global structures by establishing long-range dependencies through a self-attention module (${S_\theta }(\cdot)$ in Fig.~\ref{fig:method:mmf}(a)).
To mitigate the high computational cost of self-attention, we downsample $\bm{{F}}^t_r \in \mathbb{R}^{N\times D}$ and $\bm{{F}}^t_d \in \mathbb{R}^{N\times D}$ to obtain $\bm{\bar{F}}^t_r$ and $\bm{\bar{F}}^t_d$ via $3\times3$ and $5\times5$ max-pooling layers.
With these downsampled features, we can use a multi-attention layer with four parallel attention heads to process $\bm{F}^t_r$ and $\bm{\bar{F}}^t_r$ (or $\bm{F}^t_d$ and $\bm{\bar{F}}^t_d$) in parallel and get $\bm{\hat{F}}^t_k$:

%+++++++++++++++++++++++++++++++++++++++++++++
\vspace{-0.2cm}
\begin{equation}
\small
\begin{aligned}
&\bm{\hat{F}}^t_k \leftarrow{S_\theta }(\bm{{F}}^t_k, \bm{\bar{F}}^t_k)\\
& = \bm{{F}}^t_k{+}\mbox{MLP}\left(\sigma\left({\bm{W}^t_{Q_s}\bm{F}^t_k\left(\bm{W}^t_{K_s}\bm{\bar{F}}^t_k\right)^\top}\big/{\sqrt{D}}\right)\bm{W}^t_{V_s}\bm{\bar{F}}^t_k\right), 
\end{aligned}
\end{equation}
%+++++++++++++++++++++++++++++++++++++++++++++
%
where $k\in\{r,d\}$ and $\sigma$ is the \textit{softmax} function. $D$ is the feature dimension. 
$\bm{W}^t_{Q_s}\in \mathbb{R}^{N\times D}, \bm{W}^t_{K_s}\in \mathbb{R}^{J\times D}$ and $\bm{W}^t_{V_s}\in \mathbb{R}^{J\times D}$ are the query, key and value matrices, where $N = W \times H$, $J = (W \times H)/(3 \times 5)$.
$\mbox{MLP}(\cdot)$ denotes a three-layer fully connected network with instance normalization\cite{ulyanov2016instance} and ReLU~\cite{xu2015empirical} activation after the first two layers.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Cross-attention.}
We promote information exchange between the two modalities via cross-attention, which we implement through the network ${C_\theta }(\cdot)$ (Fig.~\ref{fig:method:mmf}(a)).
Attention signals from one modality (e.g.~RGB) emphasize the features of another modality (e.g.~depth), and vice versa. 
Given the self-attended features $\bm{\hat{F}}^t_r \in \mathbb{R}^{N\times D}$ and $\bm{\hat{F}}^t_d \in \mathbb{R}^{N\times D}$, we also adopt two downsampled networks max-pooling ($3\times3$), and max-pooling ($5\times5$) to generate the downsampled image feature map $\bm{\bar {\hat{F}}}^t_r$ (or $\bm{\bar {\hat{F}}}^t_d$). 
We denote the transformed features as $\bm{\hat{\hat{F}}}^t_r \in \mathbb{R}^{N\times D}$ and $\bm{\hat{\hat{F}}}^t_d \in \mathbb{R}^{N\times D}$ attained by cross-attention via
%+++++++++++++++++++++++++++++++++++++++++++++
\begin{equation}
\small
\begin{aligned}
&\bm{\hat{\hat{F}}}^t_r \leftarrow{C_\theta }(\bm{\hat{F}}^t_r, \bm{\bar{\hat{F}}}^t_d)\\
& = \bm{\hat{F}}^t_r {+}\mbox{MLP}\left(\sigma\left(\bm{W}^t_{Q_c}\bm{\hat{F}}^t_r\left(\bm{W}^t_{K_c}\bm{\bar{\hat{F}}}^t_d\right)^\top\big/{\sqrt{D}}\right)\bm{W}^t_{V_c}\bm{\bar{\hat{F}}}^t_d\right), 
\end{aligned}
\end{equation}
%+++++++++++++++++++++++++++++++++++++++++++++
where $W^t_{Q_c}\in \mathbb{R}^{N\times D}, W^t_{K_c}\in \mathbb{R}^{J\times D}$ and $W^t_{V_c}\in \mathbb{R}^{J\times D}$ are the query, key and value matrices.
This cross-attention block is also applied in the reverse direction so that information flows in both directions, i.e., RGB$\rightarrow$depth and depth$\rightarrow$RGB.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Multimodal Transfer Module.}
Because our architecture operates with multimodal information, we further promote information exchange between modalities after attention.
Let ${M_\theta }(\cdot)$ be the Multimodal Transfer Module \cite{MMTM} we use to improve the balance between RGB and depth information (Fig.~\ref{fig:method:mmf}(a)).
Let $\bm{\hat{\hat{F}}}^t_r \in \mathbb{R}^{N\times D_M}$ and $\bm{\hat{\hat{F}}}^t_d \in \mathbb{R}^{N\times D_M}$ be the input multimodal features to MMTM, and $\bm{\tilde {F}}_r^t \in \mathbb{R}^{N\times D_M}$ and $\bm{\tilde {F}}_d^t \in \mathbb{R}^{N\times D_M}$ be the respective outputs.
MMTM first squeezes the feature vectors into ${S_{\bm{\tilde {F}}^t_r}}$ and ${S_{\bm{\tilde {F}}^t_d}}$ via a global average pooling.
MMTM then maps these tensors to a joint representation \emph{Z} through concatenation and a fully-connected layer.
Based on \emph{Z}, MMTM finally balances RGB and depth information by gating the channel-wise features:
%+++++++++++++++++++++++++++++++++++++++++++++
\begin{equation}
\begin{aligned}
{S_{\bm{\tilde {F}}^t_{k}}} &= \frac{1}{{\Pi _{t = 1}^K{N_k}}}\sum\limits_{{n_{1, {\cdots} ,}}{n_K}} {\bm{\hat{\hat{F}}}^t_{k}({n_1}, \cdots ,{n_K})}, \\
{{Z}} &= \bm{\mbox{W}}[{{S_{\bm{\tilde {F}}^t_{r}}}},{{S_{\bm{\tilde {F}}^t_{d}}}}] + b \\
\tilde{\bm{ {F}}}^t_{k} &= 2\sigma ({\bm{\mbox{W}}_{\bm{\tilde {F}}^t_{k}}}{Z}) \odot \bm{\hat{\hat{F}}}^t_k,
\end{aligned}
\end{equation}
%+++++++++++++++++++++++++++++++++++++++++++++
where $\left[\cdot,\cdot\right]$ is the concatenation operator and $k\in\{r,d\}$. $N_k$ represents the spatial dimensions of $\bm{\hat{\hat{F}}}^t_k$ and $D_M$ represents the number of channels of the features. 
$\bm{\mbox{W}}\in \mathbb{R}^{D_Z\times 2D_M}$, ${\bm{\mbox{W}}_{\bm{\tilde {F}}^i_{k}}}\in \mathbb{R}^{D_M\times D_Z}$ are the weights,
and $b \in \mathbb{R}^{D_Z}$ are the biases of the fully connected layers.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optical and scene flow estimation}
The inputs of optical and scene flow estimation are the feature vectors $[\tilde {\bm{{F}}}^t_{r},\tilde{\bm{ {F}}}^t_{d}]$ and $[\tilde {\bm{{F}}}^{t+1}_{r},\tilde{\bm{ {F}}}^{t+1}_{d}]$.
By calculating the dot product of feature vectors between the inputs, a 4D correlation volume \textbf{C} is generated:
%+++++++++++++++++++++++++++++++++++++++++++++
\begin{equation}
\small
\begin{aligned}
    &fnet({P}^t) = [\tilde {\bm{{F}}}^t_{r},\tilde{\bm{ {F}}}^t_{d}] = [{M_\theta}({C_\theta}({S_\theta}({I^t}), {S_\theta}({Z^t})))],\\
    &\textbf{C}({P^t},{P^{t + 1}}) = \langle fnet({P^t}),fnet({P^{t + 1}})\rangle.
\end{aligned}
\end{equation}
%+++++++++++++++++++++++++++++++++++++++++++++
where $\left<\cdot,\cdot\right>$ is the dot product operator. A four-layer pyramid $\{ {\textbf{C}_1},{\textbf{C}_2},{\textbf{C}_3},{\textbf{C}_4}\}$ is generated by reducing the last two dimensions of the correlation volume through pooling with kernels of size 1, 2, 4, and 8.

We compute 4D correlation volumes to estimate optical and scene flow \cite{Raft,Raft-3D}.
Through $\{{\textbf{C}_1},{\textbf{C}_2},{\textbf{C}_3},{\textbf{C}_4}\}$, 
we iteratively estimate the dense displacement field $\{\textbf{f}^1_{est},\textbf{f}^2_{est},...,\textbf{f}^M_{est}\}$ with M iterations to update the optical and scene flow.
We train our network by computing the loss between the estimated flow and the ground-truth flow ${\textbf{f}_{gt}}$ as
%+++++++++++++++++++++++++++++++++++++++++++++
\begin{equation}
\small
\mathcal{L} = \sum\limits_{k = 1}^M {{\gamma ^{M - k}}{{\left\| {\textbf{f}^k_{est} - {\textbf{f}_{gt}}} \right\|}_1}},\
\label{eq:loss}
\end{equation}
%+++++++++++++++++++++++++++++++++++++++++++++
where as the iteration $k$ increases, the weight per loss term exponentially increases with a base $\gamma$. 
Fig.~\ref{fig:method:mmf}(b,c) show how our Multimodal Feature Fusion Encoder is integrated in RAFT and RAFT-3D to estimate the optical flow and the scene flow, respectively.
Our module can be integrated seamlessly and does not require any modification to RAFT and RAFT-3D's modules after the 4D correlation volume computation.
