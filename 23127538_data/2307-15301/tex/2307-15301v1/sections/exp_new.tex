%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

We compare {\mname} against state-of-the-art approaches on the FlyingThings3D~\cite{flythings} and KITTI~\cite{kitti} datasets. 
We design two experimental settings to mimic corrupted RGB images and poor lighting condition scenarios.
We also evaluate on data we acquired with a RGBD sensor in various lighting conditions.
We report both quantitative and qualitative results, and carry out ablation studies.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental setup}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Datasets.}
FlyingThings3D~\cite{flythings} is split into \textit{clean} and \textit{final} sets containing dynamic synthetic scenes.
The former is composed of 27K RGBD images including changing lighting and shading effects, while the latter is an augmented version of the former with simulated challenging motions and blurs.
Each set contains train and test splits. 
Previous methods~\cite{Raft,Raft-3D,CamLiFlow} exclude samples containing fast-moving objects during the evaluation. However, as such visual challenges is of interests to our problem, we use the \textit{whole} training set of FlyingThings3D and sample 1K RGBD image pairs from the \textit{whole} test set for the evaluation.
KITTI consists of real-world scenes captured from vehicles in urban scenarios.
Because the original dataset does not provide depth data, we use the disparity estimated by GA-Net~\cite{Ga-net} as in~\cite{Raft-3D}.
We exploit KITTI to assess the ability of our model and the compared ones in generalizing from synthetic to real data, without training or finetuning using any of the KITTIâ€™s sequences. 
We use the training set of KITTI as our evaluation set since KITTI's test set is not publicly available.
To further validate the performance of {\mname} in real-world scenarios, we collect an RGBD dataset using a Realsense D415 camera in an indoor office with moving people under three lighting setups, named Bright, Dimmed, and Dark.
The Bright setting features bright lighting, where the moving objects are clearly visible.
The Dimmed setting features dimmed lighting, where the moving objects can be observed with a lower visual quality.
The Dark setting features very low lighting where the moving objects can be barely seen.
We only qualitatively evaluate this dataset because we could not produce optical flow ground truth. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Evaluation metrics.}
We quantify the optical and scene flow results using conventional evaluation metrics \cite{Raft, CRaft, Raft-3D}:
for the optical flow we use $\rm AEPE_{2D}$(pixel), $\rm {ACC}_{1px}$(\%) and $\rm Fl^{all}_{2D}$(\%), 
for the scene flow we use $\rm AEPE_{3D}$(m), $\rm {ACC}_{0.05m}$(\%), $\rm {ACC}_{0.10m}$(\%) and $\rm Fl^{all}_{3D}$(\%).
$\rm AEPE_{2D}$ measures the average end-point error (EPE)~\cite{Raft}, which is an average value of all the 2D flow errors.
$\rm AEPE^{epe{\textless{100}}}_{2D}$ measures the average end-point error (EPE) among the 2D flow errors that are less than 100 pixels. 
$\rm AEPE_{3D}$ is the average of euclidean distance (EPE for 3D) between the ground-truth 3D scene flow and the predicted results.
$\rm AEPE^{epe{\textless{1}}}_{3D}$ measures the average end-point error (EPE) among the 3D flow errors that are less than 1 meter.
$\rm {ACC}_{1px}$ \cite{Raft-3D} measures the portion of errors that are within a threshold of one pixel.
$\rm {ACC}_{0.05m}$ \cite{Raft-3D} measures the portion of errors that are within a threshold of 0.05 meters, while $\rm {ACC}_{0.10m}$ \cite{Raft-3D} measures the portion of errors that are within a threshold of 0.10 meters.
$\rm MEAN_{AEPE}$ and $\rm MEAN_{ACC}$ are the average values of $\rm AEPE^{all}_{2D}$ and $\rm ACC_{1px}$, respectively, calculated over FlyingThings3D-clean and FlyingThings3D-final.
$\rm Fl^{all}_{2D}$ \cite{CRaft} is the percentage of outlier pixels whose end-point error is $>3$ pixels or $5\%$ of the ground-truth flow magnitude.
$\rm Fl^{all}_{3D}$~\cite{Flownet3d++} is the percentage of outlier pixels whose 3D Euclidean distance between the ground-truth 3D scene flow and the predicted one is $>0.3$ m or $5\%$ of the ground-truth flow magnitude.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Evaluation settings.}
Environments with poor light conditions lead to weak texture information that can compromise the stability of feature representation.
Also additive Gaussian noises can affect optical and scene flow estimation.
To assess the robustness, we design three experimental settings on the public FlyingThings3D and KITTI datasets:
\emph{Standard}: we use the original version of the dataset;
\emph{AGN}: we apply Additive Gaussian Noise on RGB images;
\emph{Dark}: we darken RGB images.
In AGN we randomly sample noise values ($\alpha$) from a normal distribution centered in zero with a standard deviation equal to 35.
In Dark we divide pixel values by a random factor $\beta  \sim \mbox{U}(\{ 1,2, \cdots,9\} )$.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Implementation details.}
We implemented \mname{} in PyTorch with all modules initialized with random weights.
We train our network for 100K iterations with the batch size of 6 on 3 Nvidia 3090 GPUs.
During training, we set the initial learning rate at $1.25\cdot10^{-4}$ and use linear decay. 
We apply MMTM sequentially with N {=} 3 times as suggested in the original paper~\cite{MMTM}.
We set $\gamma {=} 0.8$ in Eq.~\eqref{eq:loss} as in RAFT~\cite{Raft}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparisons}\label{sec:exp:comparisons}
%
We compare \mname{} against RGB methods for 2D optical flow estimation, i.e.~RAFT~\cite{Raft}, GMA~\cite{GMA}, CRAFT~\cite{CRaft}, and Separable flow~\cite{Separable_flow}, and against methods for 3D scene flow estimation, i.e.~RAFT-3D~\cite{Raft-3D} and CamLiRAFT~\cite{camliraft2023}.
See Sec.~\ref{sec:related_work} for the description of these methods.




\input{tables/table_of_comparison}
\input{tables/table_sf_comparison}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Quantitative results}



Tab.~\ref{tab:setting1_results} (top) reports optical flow results in Standard setting.
{\mname}-2D outperforms GMA by $+1.56\%$ and $+1.55\%$ in terms of $\rm ACC_{1px}$, and $+0.91$ and $+0.78$ in terms of $\rm AEPE^{all}_{2D}$ in FlyingThings3D-clean and FlyingThings3D-final, respectively.
{\mname}-3D outperforms RAFT-3D by $+1.44\%$ and $+1.42\%$ in terms of $\rm ACC_{1px}$, and $+1.00$ and $+0.88$ in terms of $\rm AEPE^{all}_{2D}$. 
While RAFT-3D extracts features only from RGB images, our MFF encoder extracts features from both RGB and depth, producing more informative internal representations.
{\mname}-3D outperforms CamLiRAFT by $+3.86\%$ and $+4.13\%$ in terms of $\rm ACC_{1px}$, and $+0.45$ and $+0.15$ in terms of $\rm AEPE^{all}_{2D}$.

\input{figures/figure_of_comparison_ft}
\input{figures/figure_of_comparison_kitti}
\input{figures/figure_of_comparison_ours}

Tab.~\ref{tab:setting1_results} (middle) reports optical flow results in AGN setting.
{\mname}-2D outperforms CRAFT by $+3.94\%$ and $+3.96\%$ in terms of $\rm ACC_{1px}$, and $+1.15$ and $+1.29$ in terms of $\rm AEPE^{all}_{2D}$ in FlyingThings3D-clean and FlyingThings3D-final, respectively.
{\mname}-3D outperforms RAFT-3D by $+2.16\%$ and $+2.45\%$ in in terms of $\rm ACC_{1px}$ and $+0.51$ and $+0.61$ in terms of $\rm AEPE^{all}_{2D}$. 
$\rm AEPE^{all}_{2D}$ of RAFT-3D is lower than that of the Standard setting.
This is because $\rm AEPE^{all}_{2D}$ computes the average of all errors, and the average is known to be sensitive to outliers. 
In fact by computing the median (less sensitive to outliers), the performance of RAFT-3D in AGN setting is worse than the Standard setting: 
e.g.~RAFT-3D in FlyingThings3D-clean achieves a median $\rm EPE^{all}_{2D}$ of 0.127 and 0.130 in the Standard and AGN settings, respectively.


Tab.~\ref{tab:setting1_results} (bottom) reports optical flow results in Dark setting.
RGB methods perform worse than the previous settings, whereas our {\mname} methods outperform RGB methods, RAFT-3D, and CamLiRAFT.
Tab.~\ref{tab:setting1_results} also reports $\rm AEPE^{all}_{2D}$ and $\rm Fl^{all}_{2D}$ on KITTI without fine-tuning the models.
Although CamLiRAFT demonstrates a better generalization capability than RAFT-3D, {\mname} outperforms almost all the other methods in all three settings, demonstrating its robustness and adaptability in real-world scenarios.




Tab.~\ref{tab:4} reports scene flow results.
{\mname}-3D outperforms RAFT-3D on both FlyingThings3D-clean and FlyingThings3D-final.
CamLiRAFT scores on par with {\mname}-3D in all three settings.
{\mname}-3D performs better in terms of $\rm ACC_{0.05m}$, while CamLiRAFT performs better in terms of $\rm ACC_{0.10m}$. 
This suggests that {\mname}-3D produces more small flow errors than CamLiRAFT.
In general, {\mname}-3D performs stably across all three settings, while the performance of RAFT-3D and CamLiRAFT degrades in the AGN and Dark settings.
On KITTI, {\mname}-3D is the best-performing method on all three settings in terms of $\rm Fl^{all}_{3D}$. 
In terms of $\rm AEPE_{3D}^{all}$, CamLiRAFT performs better in the Standard and AGN settings, while {\mname}-3D scores the best in the Dark setting.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Qualitative results}

We provide examples of qualitative optical flow results indicated with their corresponding $\rm AEPE^{all}_{2D}$.
We visualize the errors with respect to the ground-truth: the stronger the magenta, the higher the error.
Fig.~\ref{fig:FlyingThings3D} and Fig.~\ref{fig:KITTI} show the results of optical flow errors on FlyingThings3D and KITTI, respectively.
Both {\mname}-2D and {\mname}-3D consistently produce smaller $\rm AEPE^{all}_{2D}$ values than the other methods, which can also be visually verified with less magenta areas produced by our models.
Fig.~\ref{fig:realtest} shows the flow estimation on our acquired indoor dataset with RAFT, GMA, RAFT-3D, CamLiRAFT, and {\mname}.
In the Bright setting (top), all compared methods produce good-quality results.
In the Dimmed setting (middle), RAFT, GMA, and CamLiRAFT show low-quality results, which we can observe from the poor edges produced by the moving objects.
In the Dark setting, {\mname} is the only method that produces results where the moving objects are distinguishable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation study}
Tab.~\ref{tab:ablation_setting1} reports the ablation study on self-attention (SA), cross-attention (CA), and Multimodal Transfer Module (MMTM) on the FlyingThings3D dataset in both Standard and Dark settings. 
Overall, we can observe that all the components we added provide an incremental contribution to improve the quality of the output optical flow compared to the RGB baseline.
SA and CA consistently improve performance (see Exp 3 vs 6 vs 8, 4 vs 7 vs 9, and similarly for the Dark setting).
The SA applied to both depth and RGB is better than applying it to the RGB branch only (see Exp 5 vs 6 for the Standard setting, and 14 vs 15 for the Dark setting).
MMTM fusion consistently outperforms the simple concatenation of RGB and depth branches in the Dark setting (see Exp 12 vs 13, 15 vs 16, 17 vs 18).
There is one case in the Standard setting where this last does not occur (see Exp 6 vs 7).
In general, SA focuses on intra-modality relationships while CA focuses on inter-modality relationships. MMTM further exchanges information across modalities at a deeper level. The best performance is achieved when all the modules are activated.

\input{tables/table_ablation_study}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computation analysis}
We measure the number of parameters, Floating-Point Operations (FLOPs), and inference time of all compared methods using FlyingThings3D.
We conducted the experiments with a Nvidia 3090 GPU (24G) and I9-10900 CPUs, and reported the results in Tab.~\ref{tab:cost}.
Despite {\mname}-2D has the second-largest number of parameters, its number of FLOPs and inference time are in-between the other methods for optical flow estimation. The inference time of {\mname}-3D is slightly higher than that of CamLiRAFT, although our number of parameters is one order of magnitude larger than CamLiRAFT.
From the per-component analysis of {\mname}-2D in Tab.~\ref{tab:costabla}, we can observe that Self-attention and Cross-attention have a higher computational cost than MMTM and the two-branch encoder. The most time-consuming component is \textit{Others} which includes all the other modules to compute the optical flow.

\input{tables/table_computation}









