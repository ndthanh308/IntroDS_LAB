%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}\label{sec:related_work}

We provide a comprehensive analysis of the recent progress in optical flow estimation using deep learning, followed by an in-depth investigation into the integration of multimodal fusion techniques for improving flow estimation performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Optical flow.}
FlowNet \cite{Flow_Net} pioneered the use of deep neural networks to estimate optical flow as a supervised learning task.
FlowNet learns features across scales and abstraction levels to determine pixel correspondences.
FlowNet inspired FlowNet2.0 \cite{Flow_Net2.0}, PWC-Net \cite{PWC-Net}, MaskFlowNet \cite{Maskflownet} and LiteFlowNet3 \cite{LiteFlowNet3}.
FlowNet2.0 presents a warping operation and a method for stacking multiple networks through this operation \cite{Flow_Net2.0}.
PWC-Net utilizes pyramidal processing, warping, and a cost volume approach to improve both the size and accuracy of optical flow models \cite{PWC-Net}.
MaskFlowNet incorporates an asymmetric occlusion-aware feature matching module, which learns to filter out occluded regions through feature warping without the need for explicit supervision \cite{Maskflownet}.
LiteFlowNet3 tackles the challenge of estimating optical flow in the presence of partially occluded or homogeneous regions by using an adaptive affine transformation and a confidence map that identifies unreliable flow \cite{LiteFlowNet3}.
The confidence map is used to guide the generation of transformation parameters.

RAFT \cite{Raft} is a per-pixel feature extraction approach that constructs multi-scale 4D correlation volumes for each pixel pair, and updates the flow field iteratively through a recurrent unit.
Like FlowNet, RAFT has inspired GMA \cite{GMA} and CRAFT \cite{CRaft}. 
GMA addresses occlusions by modeling image self-similarities by using a global motion aggregation module, a transformer-based approach for finding long-range dependencies between pixels in the first image, and a global aggregation of the corresponding motion features. 
CRAFT aims to estimate the large motion displacements through a semantic smoothing transformer layer that integrates the features of one image and a cross-attention layer that replaces the original dot-product operator for correlation used in RAFT.
Unlike these approaches, we tackle the problem of estimating optical flow in situations of unreliable RGB information, such as noises and scarce illuminations, by appropriately fusing multiple modalities through self and cross attention within feature extraction layers.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Multimodal fusion.}
Multimodal fusion can be performed at various stages: early-, mid-, and late-fusion. 
In early-fusion, multiple channels are created within the network to process multiple modalities together \cite{early-fusion}. 
Mid-fusion maintains different branches for each modality and then merges the corresponding features at the end of the network~\cite{mid-fusion1, mid-fusion2}. 
In late-fusion, the network is trained on each modality separately and then fuses the results from the independent branches~\cite{late-fusion}.
RAFT \cite{Raft}, GMA \cite{GMA}, and CRAFT \cite{CRaft} estimate the relationships between two consecutive frames using RGB images.
Inspired by multimodal fusion, some of these works have been improved to compute both scene and optical flow by utilizing additional modalities such as depth, and point clouds.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{RGB + Point Cloud Data.}
DeepLiDARFlow \cite{DeepLiDARFlow} exhibits improved performance in challenging conditions, such as reflective surfaces, poor illumination, and shadows.
Images and point clouds are processed by using multi-scale feature pyramid networks.
Late-fusion based on differentiable confidence volumes produces the fused features.
CamLiFlow \cite{CamLiFlow} improves upon DeepLiDARFlow by fusing dense image features and sparse point features more effectively.
Instead of late-fusion, CamLiFlow adopts a multi-stage, bidirectional fusion strategy, in which the two modalities are learned in separate branches using modality-specific architectures. CamLiRAFT \cite{camliraft2023} further improves the performance based on the RAFT \cite{Raft} framework, leading to superior results compared to CamLiFlow \cite{CamLiFlow}. 
Our method differs from previous methods in that it ensures the independence of each modality through the use of two separate branches and balances the information between the modalities through multi-stage information exchange.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{RGB + depth}
RAFT-3D \cite{Raft-3D} extends RAFT to estimate both optical and scene flow from RGBD data.
RGB images serve as inputs to the feature network, where a 4D correlation volume is constructed and a soft grouping of pixels into rigid objects is formed with the aid of depth information. 
Unlike RAFT \cite{Raft}, RAFT-3D employs late-fusion with the depth information and the RGB features in the prediction module, improving the stability of flow prediction. 
However, RAFT's feature extraction method may not sufficiently capture the rich 3D structural information. 
To address this, our approach employs early-fusion, in which features are extracted from both RGB and depth information, enabling stable estimation even in cases where RGB information is unreliable.
