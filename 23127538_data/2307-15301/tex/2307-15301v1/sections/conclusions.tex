%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

We presented \mname, a novel approach for optical and scene flow estimation.
{\mname} improves feature extraction with an early-fusion Multimodal Feature Fusion (MFF) Encoder. 
MFF attends to informative features and enables information exchange within and across modalities by using self-attention, cross-attention, and the Multimodal Transfer Module.
Through experimental validation, we showed that {\mname} generates more stable and informative feature descriptions by exploiting the different modalities.
{\mname} scores state-of-the-art results in Standard setting, but also in our newly introduced AGN and Dark settings where RGB information is corrupted.
Future research directions may include the integration of {\mname} in robotic systems for autonomous navigation.
