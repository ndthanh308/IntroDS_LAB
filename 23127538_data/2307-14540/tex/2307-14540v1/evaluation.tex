% \vspace{-0.05in}
\nsection{Defense Effectiveness Evaluation} \label{sec:eval}

In this section, we evaluate \ld{} against the state-of-the-art lateral-direction attack targeting high-level AD localization. 

\vspace{-0.05in}
\nsubsection{Evaluation Methodology} \label{sec:eval_method}
% \vspace{0.05in}

\textbf{Targeted AD system and attack.}
Since \ld{} is designed for high-level AD systems, we choose the industry-grade full-stack Baidu Apollo AD system~\cite{apollo} as a representative prototyping target. 
Specifically, Baidu Apollo adopts an MSF-based localization highly representative in both design (KF-based MSF) and implementation (state-of-the-art localization accuracy~\cite{wan2018robust}).
%a most representative MSF based localization design
% , which has been tested on a large AD vehicle fleet including various challenging driving scenarios such as urban roads, highways, tunnels, etc., 
%with state-of-the-art localization accuracy~\cite{wan2018robust}, 
% Baidu is also a leading AD developer currently already running commercial RoboTaxi services without safety drivers~\cite{baidu_driverless_beijing}.
Note that although our evaluation here uses Baidu Apollo, the \ld{} design itself is generalizable to other industry-grade high-level AD systems; for example, later in~\S\ref{sec:pixkit_eval} we also implemented it in Autoware for end-to-end physical evaluation.
% and will expand the business to 30 cities within next 3 years~\cite{baidu_driverless_robotaxi}.
For targeted attacks,
we evaluate against the recent \fr{} attack~\cite{fusionripper} since it is (1) the state-of-the-art and only lateral-direction localization attack that can break MSF localization; and (2) directly applicable to the above representative MSF implementation. 

%it is by far the only lateral-direction localization attack that can break the MSF localization.
% , thus we evaluate the defense effectiveness of \ld{} against the \fr{} attack in our evaluation. 

% \textbf{Lateral-direction localization attack: \fr{}.} Due to the high robustness provided by sensor fusion, MSF is often considered as a promising defense strategy for attacks targeting localization sensors such as GPS spoofing~\cite{davidson2016controlling, lee2017attack, zeng2018all, cardenas2019cyber}. Despite that, prior work~\cite{fusionripper} proposes an \textit{opportunistic} lateral-direction localization attack method, called \fr{}, which can use GPS spoofing alone to inject lateral deviations in the MSF localization outputs and thus cause the AD vehicle to drive off-road or onto the wrong way. The \fr{} attack is consist of two attack stages: \textit{vulnerability profiling} and \textit{aggressive spoofing}. In the vulnerability profiling stage, it spoofs the GPS inputs of the MSF localization with a small constant distance $d$ (e.g., 0.5 m) in the lateral direction, waiting to discover a vulnerable attack window. Whenever the AD vehicle's physical deviation is larger than a certain threshold (e.g., 0.3 m), \fr{} launches the aggressive spoofing stage, where a scaling factor $f$ (e.g., 1.2) will be continuously applied to the spoofing distance in each second to quickly introduce large lateral deviations in the MSF localization outputs. \fr{} has shown high attack effectiveness on the Baidu Apollo MSF localization. To best of our knowledge, \fr{}~\cite{fusionripper} is the only localization attack that is able to defeat the MSF based localization algorithm on high-level AD systems.

\textbf{Real-world sensor traces and \fr{} attack effectiveness.}
Since our evaluation target is the \fr{} attack, 
we follow the same evaluation methodology as in their paper~\cite{fusionripper} and conduct our evaluation on real-world sensor traces from the KAIST complex urban dataset~\cite{jeong2019complex}. Specifically, we look for traces with camera data as required by \ld{}, select the ones that the Apollo MSF can stably operate without attack~\cite{fusionripper}, and apply \fr{} from each consecutive timestamp as in~\cite{fusionripper}. In total, we obtain 562 attack traces summarized in Table~\ref{tbl:kaist_traces}. These traces cover diverse driving scenarios, e.g., different road types (344 on local roads and 218 on highways), driving speeds (9.5 to 26.3 m/s), time-of-day (e.g., 36 in the morning, 182 around sunset time), and road conditions (e.g., 170 with snow on road). 

For each trace, we follow the same method to identify the most effective attack parameters as in the \fr{} paper. Note that in the table our attack goal deviation is larger than the original \fr{} paper since they focus on the minimum urban lane width (i.e., 2.7 m) while we set the attack goal in a more realistic setting by measuring the lane widths in the dataset. This does not affect the attack effectiveness; as shown, the overall attack success rate is over 98\%, which is consistent with the \fr{} paper. In our evaluation, we exclude the scenarios without lane markings (e.g., when the vehicle is in an intersection) since it is out of the applicable domain for \ld{}. As analyzed in~\S\ref{sec:opportunity}, the lack of coverage of such scenarios do not eliminate the defense value since only 0.8\% of the attacks can possibly succeed in such scenarios and such successes are out of the attacker's control. 

%original FusionRipper paper~\cite{fusionripper} uses a smaller attack goal deviation since they focus on the minimum urban lane width (i.e., 2.7 m) while we set the attack goal in a more realistic setting by measuring the lane widths in the dataset.



%and 182 were collected around sunset time ($\sim$ 5 p.m.); 170 were collected after snowfall with snow left on the road.

%The average speeds in the traces range from 9.5 m/s to 26.3 m/s. The KAIST dataset was collected under different time-of-day and road conditions. In particular, among the attack traces, 182 were collected around sunset time ($\sim$ 5 p.m.); 170 were collected after snowfall with snow left on the road.


%Following the same methodology as \fr{},
% and also as a common practice for AD vehicle companies~\cite{frossard2018end, gao2020vectornet}, 
%we conduct our evaluation on real-world sensor traces from the KAIST complex urban dataset~\cite{jeong2019complex}. 


%, which is also used in \fr{}~\cite{fusionripper}. 



%The KAIST dataset contains sensor traces collected in diverse driving scenarios, including local roads and highways. Since only one (\textit{ka-local31}) of the traces evaluated in \fr{} (listed in Table~\ref{tbl:correlation}) contains the camera data, which is necessary for \ld{}, we select three other traces from the KAIST dataset with the complete camera data as shown in Table~\ref{tbl:kaist_traces}. We use the same trace selection methodology as \fr{} to select the traces with the smallest average MSF state uncertainties (hence more difficult to attack in general)~\cite{fusionripper}. Specifically, we include 4 sensor traces from KAIST dataset. For each sensor trace, we apply \fr{} from each consecutive timestamp similar as in \cite{fusionripper}. In total, we obtain 562 attack traces, where 344 on local roads and 218 on highways. The average speeds in the traces range from 9.5 m/s to 26.3 m/s. The KAIST dataset was collected under different time-of-day and road conditions. In particular, among the attack traces, 182 were collected around sunset time ($\sim$ 5 p.m.); 170 were collected after snowfall with snow left on the road.
%\alfred{use the number of attack traces to highlight the evaluation diversity, not the number of traces. Local/highway is just one dimension of diversity. Show quantified diversity metrics.} \junjie{added.}

%Next, we evaluate the attack effectiveness of \fr{} on these traces. 
%Following the same attack method, we enumerate GPS spoofing parameters $d$ from 0.3 to 2.0 meters and $f$ from 1.1 to 2.0, and obtain the best parameter combinations and their attack success rates.
%Since the attack goal deviation (\textit{off-road} attack goal) is calculated based on the lane, road shoulder, and vehicle width~\cite{fusionripper}, we then estimate the lane widths from Google map, road shoulder widths based on the road design guideline for local roads or highways~\cite{road_shoulder_width}, and vehicle width from the reference vehicle for Baidu Apollo~\cite{mkz_spec}.
%The required attack goal deviations, the best GPS spoofing parameters, and attack effectiveness are listed in Table~\ref{tbl:kaist_traces}. 
%Note that the original FusionRipper paper~\cite{fusionripper} uses a smaller attack goal deviation since they focus on the minimum urban lane width (i.e., 2.7 m) while we set the attack goal in a more realistic setting by measuring the lane widths in the dataset.
%As shown, \fr{} is able to achieve $>$98\% attack success rates on all 4 traces. In our evaluation, we exclude the attack cases where the attack goal deviations are reached when the vehicle is located in an intersection since it is out of the applicable domain for lane boundaries.


\begin{table}[tbp]
\centering
\footnotesize
\caption{Details of the 562 total attack traces used in our evaluation and the \fr{} attack effectiveness.}
% \vspace{-0.15in}
\label{tbl:kaist_traces}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Attack \\ Trace \#\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Road \\  Type\end{tabular}} &  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Avg. \\ Speed\end{tabular}} & \multicolumn{4}{c}{FusionRipper Attack} \\ \cmidrule(l){5-8} 
 &  &  &  & \begin{tabular}[c]{@{}c@{}}Attack\\ Goal Dev\end{tabular} & \begin{tabular}[c]{@{}c@{}}Best\\ $d$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Best\\ $f$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Success\\ Rate\end{tabular} \\ \midrule
\textit{ka-local31} & 174 & Local & 10.9 m/s & 1.3 m & 0.5 & 1.2 & 99.4\% \\
\textit{ka-local33} & 170 & Local & 9.5 m/s & 1.3 m & 0.3 & 1.3 & 98.3\% \\
\textit{ka-highway36} & 182 & Highway & 26.3 m/s &  1.9 m & 0.3 & 1.3 & 100\% \\
\textit{ka-highway18} & 36 & Highway & 24.8 m/s & 1.9 m & 0.3 & 1.3 & 100\% \\ \bottomrule
\end{tabular}
% \vspace{-0.05in}
\end{table}

\cut{
\begin{table}[tbp]
\centering
\footnotesize
\caption{Real-world sensor traces used in our evaluation and \fr{} attack effectiveness.}
\vspace{-0.1in}
\label{tbl:kaist_traces}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multirow{2}{*}{Trace} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Road\\ Type\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Avg.\\ Speed\end{tabular}} & \multirow{2}{*}{Duration} & \multicolumn{4}{c}{FusionRipper Attack} \\ \cmidrule(l){5-8} 
 &  &  &  & \begin{tabular}[c]{@{}c@{}}Attack\\ Goal Dev\end{tabular} & \begin{tabular}[c]{@{}c@{}}Best\\ $d$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Best\\ $f$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Success\\ Rate\end{tabular} \\ \midrule
\textit{ka-local31} & Local & 10.9m/s & 1014s & 1.3m & 0.5 & 1.2 & 99.4\% \\
\textit{ka-local33} & Local & 9.5m/s & 1283s & 1.3m & 0.3 & 1.3 & 98.3\% \\
\textit{ka-highway36} & Highway & 26.3m/s & 352s & 1.9m & 0.3 & 1.3 & 100\% \\
\textit{ka-highway18} & Highway & 24.8m/s & 162s & 1.9m & 0.3 & 1.3 & 100\% \\ \bottomrule
\end{tabular}
\end{table}
}


%\textbf{Semantic map creation.}
%Since the KAIST dataset 
% is designed for AD localization and perception tasks, it 
%does not provide any semantic maps,
% which are mainly used for AD planning~\cite{jeong2019complex}. As required in \ld{}, 
%we thus manually create the semantic map for each trace (more details in Appendix~\ref{app:map_creation}). \alfred{how important is this?} \junjie{
%although for KAIST traces, reviewer may not realize such a problem even if we delete this part, however, I think some of them may notice in the night-time driving trace evaluation. That's why I intended to keep this.}


\textbf{Lane detection and AD control effects under attack.} Since \ld{} does not assume any specific requirement on the lane detector, we are free to use any state-of-the-art lane detector or even an ensemble of lane detectors. In our evaluation, we opt to the LD model used in OpenPilot~\cite{openpilot}, which is already used commercially for Automated Lane Centering. The KAIST traces include time-synchronized left and right camera frames from a front-facing stereo camera. In our evaluation, we regard the left and right cameras as independent cameras and run the LD model and calculate lateral deviations (Alg.~\ref{alg:ld_dev_calc}) separately on them. We then aggregate their results to obtain an averaged lateral deviation on the LD side.

Since KAIST traces are collected under benign driving, we need to model the LD outputs when the AD localization is under attack. Same as the \fr{} paper~\cite{fusionripper}, we assume the lateral deviations in the MSF localization will be directly reflected as physical world deviations to the \textit{opposite} direction (\S\ref{sec:background_threat_model}). 
% since the AD control is actively correcting the MSF deviations at high frequencies (e.g., 100 Hz in Baidu Apollo~\cite{apollo}). 
We then model the attack-influenced LD outputs by adding the physical world deviations to the lateral deviations calculated from the benign LD outputs. Later in \S\ref{sec:end_to_end_eval}, we evaluate \ld{} in both end-to-end simulation and physical-world environments without such an assumption.


\textbf{Baseline: SAVIOR.} 
As a baseline, we evaluate the attack detection effectiveness of the closest alternative software-based method based on latest prior works for small robotics vehicles such as drones and rovers: physical-invariant based defenses~\cite{savior, ci}. Specifically, we select SAVIOR~\cite{savior} as a representative design since it adopts more principled state estimation models and thus shows superior detection performance over prior designs such as CI~\cite{ci}. The detailed setup for SAVIOR evaluation can be found in Appendix~\ref{app:savior_setup}.

%In our evaluation, we focus on SAVIOR~\cite{savior} rather than CI~\cite{ci} since SAVIOR adopts more complex nonlinear state estimation models and has shown superior detection performance than CI~\cite{savior}. 
\cut{
To evaluate SAVIOR, we follow the similar methodology as the ground rover evaluation in the SAVIOR paper~\cite{savior}, i.e., using the kinematic bicycle model~\cite{kong2015kinematic} and an Extended Kalman Filter (EKF) to predict the system state (i.e., position in x, y coordinates) given the vehicle control commands (i.e., steering and acceleration). Although the vanilla bicycle model does not have tunable parameters, we follow a similar implementation as SAVIOR by adding coefficients to the bicycle model equations~\cite{savior_code}. Same as SAVIOR, we use the \textit{nlgreyest} system identification tool from Matlab~\cite{matlab_si} to find the coefficients that can best fit the sensor and control trace.
During the evaluation, we continuously calculate the residuals between the GPS measurements and the predicted positions from the EKF, and feed the residuals to a CUSUM anomaly detector for attack detection. An execution that triggers the CUSUM detector will be considered as under attack.

Since the KAIST traces do not contain control outputs, we replay the traces in Baidu Apollo to record the vehicle control commands (more details in Appendix~\ref{app:savior_collect_control}). In addition to the KAIST traces, we also evaluate SAVIOR on a dataset that contains the original control commands to validate SAVIOR's detection performance in an ideal setting. However, similar performance is observed in that dataset to the ones on KAIST traces. More details of this are in Appendix~\ref{app:savior_with_real_control}.
}

\textbf{Evaluation metrics.}
As \ld{} involves two defense stages with different defense goals, we separate the evaluation into attack detection and response evaluations. For attack detection evaluate, we plot the \textit{ROC curves} to systematically show the TPRs and FPRs under different CUSUM parameters $b$ and $\tau$ (\S\ref{sec:design_detection}). In addition to ROC curves, we also report the maximum MSF lateral deviation before the attack is detected by \ld{}. This \textit{detection deviation} is a metric to indicate the detection timeliness, e.g., a detection deviation smaller than the lane straddling deviation (i.e., deviation to touch the lane line) means that the attack is detected early in time before it can cause any meaningful adversarial consequences. For attack response evaluation, we focus on the lateral deviations since our AR goal is to steer the vehicle to stop within the lane boundaries. In particular, we report two lateral deviation metrics with one measuring the \textit{maximum deviation} before the vehicle fully stops, and another one measuring the final \textit{stopping deviation}. In practice, the latter is more important since it 
% the former deviation is only temporal, but the latter
will be the permanent deviation after the vehicle stops.

\vspace{-0.05in}
\nsubsection{Attack Detection Effectiveness} \label{sec:eval_detection}
\vspace{0.03in}

\textbf{Attack detection rates.}
The top figures in Fig.~\ref{fig:trace_rocs_devs} show the detection ROC curves of \ld{} against \fr{}. As shown, \ld{} can achieve effective detection with 100\% TPRs and 0\% FPRs on all 4 traces. During the search for best CUSUM parameters, we find that in benign drivings, differences between MSF and LD lateral deviations are always bounded within certain range ($<$0.6 m). However, in attacked drivings, \fr{} will cause larger lateral deviations on MSF side, which will be reflected on LD side in the opposite direction. Such a difference between benign and attacked drivings makes the attack easily detectable by \ld{}. Fig.~\ref{fig:cusum} shows an example of the benign and attacked MSF and LD lateral deviations and their CUSUM statistics. In the attacked case, \fr{} launches the vulnerability profiling stage from $t$=1544686730 and discovers a vulnerable window at $t$=1544686765, where MSF starts to exhibit larger lateral deviations. Because of the distinctive MSF/LD consistency levels between the benign and attack cases, it is thus straightforward to set a CUSUM threshold to differentiate them.

\textbf{Baseline comparison.} As shown, SAVIOR's detection performance is only slightly better than random guessing and far from being an ideal detector. Such a poor detection performance would render SAVIOR unpractical since it will introduce lots of false positives in normal driving.

The reason behind the poor detection performance in the AD context is twofold. First, compared to drones and rovers, the physical dynamics of the vehicle are much harder to model due to the complex physical moving characteristics, e.g., tire-road frictions, aerodynamic forces, road bank angles, etc.~\cite{rajamani2011vehicle}. For example, prior study~\cite{polack2017kinematic} finds that the error of kinematic bicycle model increases very fast at high speeds (e.g., 25 $\mathrm{m/s}$) or on curvy roads (e.g., steering angle at 4$^\circ$). In comparison, the bicycle model used in SAVIOR is reported having an average position error of 0.33 m \textit{within 0.8 sec} under low-speed settings (e.g., 13.8 m/s) in~\cite{kong2015kinematic} and its error keeps accumulating as time progresses; comparably, the same bicycle model incurs an average error of 1.076 m on \textit{ka-local31} within 1 sec, where the trace contains many turns and curvy roads.

Second, the attack deviation goals in the AD context can be much smaller but still being safety-critical. 
While SAVIOR is effective at detecting attacks on small robotics vehicles such as drones with large deviation goals (e.g., $\sim$50 m~\cite{savior}), attacks targeting high-level AD systems requires much smaller deviation and thus harder to detect. For example, even lateral deviations $<$0.5 m are enough to cause lane departure on narrow urban roads (e.g., 2.7 m wide~\cite{road_shoulder_width}).


\cut{
As discussed in \S\ref{sec:motivation_savior}, this is mainly because
the physical moving characteristics of AD vehicles are more complex and harder to accurately model than small robotics systems such as drones. Thus, the existing physical invariants (e.g., the bicycle model) used in SAVIOR fails to accurately estimate the system state. Another reason is that the attack goal deviations in the AD context are much smaller than the ones for drones, which makes the attack harder to detect.
% accurate enough to distinguish stealthy attacks such as \fr{}.
Such a distinctive detection performance between \ld{} and SAVIOR clearly shows the benefit of leveraging the lane boundary information source available on high-level AD systems.
}


% Figure environment removed

\begin{table*}[tbp]
\footnotesize
\begin{minipage}{0.36\linewidth}
    \centering
    % Figure removed
    % \vspace{-0.3in}
    \captionof{figure}{Benign and attacked MSF/LD lateral deviations and CUSUM statistics.}
    \label{fig:cusum}
\end{minipage}\hfill
\begin{minipage}{0.36\linewidth}
    \centering
    % Figure removed
    % \vspace{-0.3in}
    \captionof{figure}{MSF/LD and physical world deviations and Kalman gains during AR period.}
    \label{fig:ar_devs}
\end{minipage}\hfill
\begin{minipage}{0.245\linewidth}
    \centering
    % Figure removed
    % \vspace{-0.3in}
    \captionof{figure}{Detection and AR deviations on the night-time trace.}
    \label{fig:real_car_devs}
\end{minipage}
\vspace{-0.2in}
\end{table*}



\textbf{Attack detection deviations.} To evaluate attack detection deviations, we choose a CUSUM weight $b=0.6$ and threshold $\tau=0.1$, which can achieve best attack detection effectiveness on all traces. For example, the detection deviation in Fig.~\ref{fig:cusum} is 0.36 m under these CUSUM parameters. The bottom figures in Fig.~\ref{fig:trace_rocs_devs} show the distributions of maximum deviations \fr{} has reached before being detected by \ld{} (box plots with pink background). As shown, \ld{} can promptly detect the attack before it can even cause lane straddling, and the average detection deviations are all below 0.5 m. However, there do exist two attack cases in \textit{ka-highway18} that the detection deviations are close to the lane straddling deviation. This is because in these attack cases, the lateral deviation at the MSF side raises very rapidly between detection intervals such that the deviation has already reached a large number (e.g., 0.63 m) before \ld{} has a chance to perform the detection. Nevertheless, none of the attack cases are detected after \fr{} starts to cause lane straddling and all of them are far away from reaching the attack goal deviation.
% (\textit{off-road} attack goal).
% This thus effectively reduces \fr{}'s attack success rates to 0\% on these traces.

\vspace{-0.05in}
\nsubsection{Attack Response Effectiveness} \label{sec:eval_ar}
\vspace{0.03in}

The distributions of the maximum deviations and final stopping deviations are shown in the bottom figures in Fig.~\ref{fig:trace_rocs_devs} (box plots without background colors). During the AR periods, none of the attack cases have a maximum or stopping deviation over the attack goal deviation (1.3 m for local and 1.9 m for highway). Despite 4 attack cases on the highway traces have maximum deviations exceed the lane straddling deviation (0.7 m), their stopping deviations are all corrected back to be within the lane boundaries.
% An interesting finding is that in many cases, the stopping deviations, especially on the highway traces, are even smaller than the attack detection deviations as shown in the figure.
This shows that our AR design (\S\ref{sec:design_ar}) is effective at keeping the vehicle within the lane boundaries when it stops, which can prevent the much more dangerous situation where it stops out of the ego lane.

Moreover, comparing between local and highway, the highway traces often have \textit{larger maximum deviations} and \textit{smaller stopping deviations}. This is because the driving speeds when the attacks are detected on the highway traces (27.3 $\mathrm{m/s}$ on avg.) are much higher than that on the local traces (3.8 $\mathrm{m/s}$ on avg.). This leads to a much longer AR period on highways ($\sim$7 sec on avg.) than that on local roads ($<$1 sec on avg.). As a result, \fr{} can keep causing larger lateral deviations after the attack is detected, but in the meanwhile, our AR design can also correct more given the longer AR period.

\cut{
% Figure environment removed
}

Fig.~\ref{fig:ar_devs} shows an example of the MSF/LD and physical world deviations during the AR period on \textit{ka-highway36}, where the maximum deviation and stopping deviation are 0.52 m and 0.19 m, respectively. In this example, since \fr{} keeps increasing the deviation on the MSF side, the safety-driven fusion (\S\ref{sec:design_ar}) penalizes the lateral deviations on the MSF side with higher uncertainties and thus results in smaller Kalman gains, which indicate the weights of the inputs in KF update. Consequently, the fusion process prioritizes the lateral deviations on the LD side, which are similar to the physical world deviations, and the lateral controller thus can steer the vehicle towards the right direction.

\textbf{Comparison with naive AR design.}
A naive AR design, named NaiveAR, applies the maximum deceleration to stop but still keeps using the MSF outputs for steering. Such design is similar to the \textit{in-lane stop} planning scenario that Baidu Apollo adopts to handle emergencies~\cite{apollo_planning}. To evaluate this, we record the MSF lateral deviations at the end of AR periods and regard them as the stopping deviations based on the control assumption (\S\ref{sec:background_threat_model}). The stopping deviations of NaiveAR are shown in Fig.~\ref{fig:trace_rocs_devs}. Because of the longer AR periods on the highway, the stopping deviations under NaiveAR are significantly higher than that using our complete AR design, especially on the highway traces. In particular, since the lateral deviations on \textit{ka-highway36} increase very quickly, over 75\% of the attacked cases still reaches a lateral deviation higher than the attack goal deviation, which consequently leads to $>$75\% attack success rate for \fr{} on \textit{ka-highway36} despite the attacks are correctly detected. On the other hand, with the complete AR design, none of the attack cases can be even deviate out of the lane boundaries.


% % Figure environment removed


\vspace{-0.05in}
\nsubsection{Evaluation under Limited Visibility} \label{sec:eval_visibility}
\vspace{0.05in}

% \junjie{consider removing the real-time overhead evaluation since we already conduct real-time vehicle evaluation with the defense.}

% Although the KAIST traces already include different time-of-day and road conditions (\S\ref{sec:eval_method}), in this section, we further evaluate \ld{} on a \textit{night-time} driving trace.
% that we collected using a vehicle with an Advanced Driver-Assistance System (ADAS).

\cut{  % revision: deleted since simulation & PIXKIT experiments are all real-time
\textbf{Real-time performance overhead evaluation setup.}
Among the \ld{} design components as shown in Fig.~\ref{fig:design_overview}, only the attack detection (\S\ref{sec:design_detection}) and the attack-aware fusion (\S\ref{sec:design_ar}) are on the timing critical path since they are performed as post-processing steps in the localization module. Compare between them, the attack detection logic is by design more time consuming (with more semantic map queries) and is the only one that will be executed in benign driving, thus our performance overhead evaluation focuses on this step. To do that, we implement the attack detection logic on an embedded ADAS device named EON~\cite{eon}, which is the official device to run OpenPilot~\cite{openpilot}. Since OpenPilot adopts a similar modular design with parallel processes such as localization, LD, etc. Thus, similar to Fig.~\ref{fig:design_overview}, we implement the attack detection as a post-processing step in the localization process and measure its average timing overhead over a 30-min duration.
}

\textbf{Trace collection and defense evaluation setup.} We collect a \textit{night-time} driving trace at around 11 p.m. our local time using an Advanced Driver-Assistance System (ADAS) device named EON~\cite{eon}, which is the official device to run OpenPilot~\cite{openpilot}. Specifically, we record the localization and LD outputs during the trace collection for the defense evaluation.
% EON can provide the necessary inputs (localization and LD) for the defense evaluation, we thus use it to collect a real-world driving trace and conduct a trace-based evaluation similar to \S\ref{sec:eval}. 
% Specifically, we install the EON device on a Toyota Camry 2019 and record the localization and LD outputs during the trace collection.
% Note that during the trace collection, the vehicle is manually driven by us rather than by OpenPilot since it cannot be constantly engaged to control the vehicle due to the intersections, which are out of the applicable domain for LD. 
The trace is $\sim$25 km in length with 3 local road and 2 highway segments as shown in Fig.~\ref{fig:real_car_map}. 
% The average speeds in the trace range from 10.3 to 27.5 $\mathrm{m/s}$ due to the dynamic traffic and different speed limits. 
% To evaluate \ld{}, we follow the same methodology as in Appendix~\ref{app:map_creation} to create the semantic map.
Since EON does not provide LiDAR data, we are not able to run MSF and \fr{} attack. To model the attack effect, we apply the lateral deviations from the \textit{most aggressive} attack trace in \textit{ba-local} trace used in the \fr{} paper~\cite{fusionripper} to the localization outputs, which only takes 10 sec from the start of attack to reaching a 2 m lateral deviation. This is similar to the prior works where they directly apply the attack traces in the target systems for attack detection evaluation~\cite{savior, ci}. Specifically, we apply the attack trace consecutively to all road segments excluding the intersections, which results in 98 attacked and 98 corresponding benign segments in total.

% Figure environment removed

% u-blox GNSS receiver uses multi-GNSS~\cite{ublox_multignss} (e.g., GPS \& GLONASS) to provide more accurate positioning.

% % Figure environment removed

\cut{  % revision: deleted since simulation & PIXKIT experiments are all real-time
\textbf{Real-time performance overhead.}
During the 30-min period, we find that the attack detection logic in \ld{} only takes 0.37 ms on average (std: 0.26 ms). In comparison, LD runs at 20 Hz in the EON, which means that each LD output can tolerate a 50 ms processing delay. Thus, the timing overhead of the attack detection in \ld{} is negligible even on an embedded ADAS device such as EON. Commercial high-level AD systems often run on Industrial PCs~\cite{apollo_hardware}, which are much more powerful and thus the overhead might be even lower.
}

\textbf{Defense effectiveness.}
Similar to results on KAIST traces (\S\ref{sec:eval_detection}), \ld{} can achieve effective attack detection with 100\% TPR and 0\% FPR on the night-time trace. The attack detection and AR deviations are shown in Fig.~\ref{fig:real_car_devs}. As shown, even under such low-light condition, \ld{} can still timely detect the attack with an average detection deviation of 0.29 m. Consistent with findings in \S\ref{sec:eval_ar}, the stopping deviation on the real vehicle trace is only 0.17 m on average, which means \ld{} is effective at stopping the vehicle within the lane boundaries. In comparison, NaiveAR has a stopping deviation much higher than \ld{}, where one attack segment (maximum deviation is 1.33 m) exceeds the goal deviation for local roads. 

% detection: mean, std: 0.31 0.07
% EH max: mean, std: 0.36 0.08
% EH stop: mean, std: 0.13 0.10

\cut{
% Figure environment removed
}