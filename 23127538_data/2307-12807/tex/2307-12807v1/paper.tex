%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,nonacm]{acmart}
\usepackage{tabularx}
\usepackage[htt]{hyphenat}
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{none}
%\renewcommand\footnotetextcopyrightpermission[1]{}
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission{}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[AIDB]{5th International Workshop on Applied AI for Database Systems and Applications}{Sept 1,
  2023}{Vancouver, Canada}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear} 

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Comprehending Semantic Types in JSON Data with Graph Neural Networks}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Shaung Wei}
\email{sw2582@rit.edu}
\affiliation{%
  \institution{Rochester Institute of Technology}
  \streetaddress{102 Lomb Memorial Drive}
  \city{Rochester}
  \state{New York}
  \country{USA}
  \postcode{14623-5608}
}

\author{Michael J. Mior}
\email{mmior@mail.rit.edu}
\affiliation{%
  \institution{Rochester Institute of Technology}
  \streetaddress{102 Lomb Memorial Drive}
  \city{Rochester}
  \state{New York}
  \country{USA}
  \postcode{14623-5608}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Wei and Mior}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Semantic types are a more powerful and detailed way of describing data than atomic types such as strings or integers. They establish connections between columns and concepts from the real world, providing more nuanced and fine-grained information that can be useful for tasks such as automated data cleaning, schema matching, and data discovery. Existing deep learning models trained on large text corpora have been successful at performing single-column semantic type prediction for relational data. However, in this work, we propose an extension of the semantic type prediction problem to JSON data, labeling the types based on JSON Paths. Similar to columns in relational data, JSON Path is a query language that enables the navigation of complex JSON data structures by specifying the location and content of the elements. We use a graph neural network to comprehend the structural information within collections of JSON documents. Our model outperforms a state-of-the-art existing model in several cases. These results demonstrate the ability of our model to understand complex JSON data and its potential usage for JSON-related data processing tasks.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{JSON data, graph neural networks, semantic type detection, deep learning}
\iffalse
\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}
\fi
%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Detecting the semantic types of columns in a relational table can be useful for data preparation and information extraction tasks such as data cleaning and integration. For example, semantic type detection can help some rule-based automated data cleaning applications that are dependent on semantic data types~\cite{kandel2011wrangler,raman2001potter}. Schema matching tasks can also narrow down search spaces based on detected semantic types~\cite{rahm2001survey}. Furthermore, data discovery tasks can utilize detected semantic types to find semantically related results for input queries~\cite{fernandez2018aurum,fernandez2018seeping}.

Traditional systems only classify table columns into atomic types such as Boolean, integer, and string. Semantic types are a finer-grained classification of columns that provide richer information and a connection to real-world concepts. For example, a column containing values such as "Chicago", "Detroit" can be described using the type \emph{location} rather than \emph{string}. Prior work has attempted to predict semantic types using methods such as dictionary lookup and regular expression matching. In real-world cases, many tables are dirty with corrupted column names or missing values that these rule-based approaches do not correctly predict~\cite{mind}.

To solve this problem, a deep learning enabled model, Sherlock~\cite{hulsebos2019sherlock}, was proposed to learn the semantic type based on column values. Sherlock is trained on huge table-based corpora~\cite{hu2019viznet}. It first extracts features from the values in each column and a deep learning model is trained on these features to perform semantic type detection. Although Sherlock outperforms traditional approaches, it is limited to relational data.

In this work, we propose a novel semantic type classification model for semi-structured JSON data. Unlike relational databases, JSON data is in the form of key-value pairs and uses a hierarchical structure. We annotate the semantic type of JSON data with its hierarchical structure and use a graph neural network to predict the semantic type with the same set of features extracted by Sherlock. The proposed model can achieve better accuracy and higher F1 scores than Sherlock on certain semantic types.

\section{Problem Setup}

Relational semantic type prediction is a multiclass classification problem that can be defined as follows. Given the columns $c_1,c_2,\ldots,\\c_m$ for a given table as the training dataset, where $c_i$ is a vector of column values, their corresponding semantic types $t_1,t_2,\ldots,t_m \in \tau$ are considered as target values, where $\tau$ is a set of predefined semantic labels to be considered. We refer to this problem as \textit{single-column prediction} where we use all values from each column to predict the semantic type of the column. 

In our work, we extend this problem to include JSON data, which contains arbitrarily nested structures. To label JSON data, an example JSON document shown in Figure~\ref{fig:json_ex} could include a \texttt{user} key with the following key-value pairs: \texttt{"user": \{"id":9171087, "id\_str": "9171087", "name": ud83c\}}. In this instance, the JSON document yields four distinct key-value pairs. The first key-value pair consists of the key \texttt{id} and its corresponding value of \texttt{9171087}. The second pair includes the key \texttt{id\_str} with its corresponding value also being \texttt{9171087}. The third key-value pair comprises the key \texttt{name} with its corresponding value being \texttt{ud83c}. The fourth pair, has the key \texttt{user} and the value \texttt{\{"id":9171087, "id\_str": "9171087", and "name": ud83c\}}.

In the context of relational data, the assignment of ground truth values for supervised semantic type prediction is often derived from columns, as they are indicative of the underlying meaning of the data within that column. In the case of JSON data, the semantic type is established by reference to the corresponding JSON Path\cite{jsonpath} of the key. JSON Path is a query language that enables the navigation of complex JSON data structures by specifying the element location. The rationale is that values associated with the same path are likely to possess similar semantic meanings, and therefore can be grouped together under a common semantic type. For example, the path \texttt{\$.user.username} refers to the "username" key within the top-level object nested under the key "user".

% Figure environment removed

\section{Feature Selection}
In our work, we use the same set of features as Sherlock, which is a single-column prediction model that takes all values from a single column as input and outputs the predicted semantic type for the corresponding column. The extracted feature vectors are used to train a neural network for the detection of semantic types. In Sherlock, a total of 1,587 features are extracted from the column values across four different dimensions. These dimensions are described below.
\begin{enumerate}
  \item \textbf{Global statistics.} This category is a set of 27 hand-crafted features, typically some high-level statistical characteristics of the column. For example, column entropy describes the uniformity of the distribution of column values. Another example is the number of values that measures the number of unique values recorded in the column.
  \item \textbf{Character-level distributions.} This category contains simple statistical features of character distributions. Specifically, 10 statistical functions, i.e. any, all, mean, variance, min, max, median, sum, kurtosis, skewness, are applied on all 96 ASCII-printable characters plus the form feed character, resulting in 960 features. For example, the \texttt{any} function checks if any column value contains a specific character. \texttt{all} checks if all column values contain a character. Some other examples of features are the maximum number of appearances of a character in a single column value and the median value of appearance of a character for all column values.
  \item \textbf{Word embeddings.} Sherlock uses a pre-trained GloVe embedding~\cite{pennington2014glove} to characterize the semantic content of column values. The GloVe model contains a 50-dimensional embedding for 400,000 English words aggregated from 6,000,000,000 tokens. Similar to word2vec~\cite{mikolov2013efficient}, GloVe embeddings can be used to measure semantic similarity between words. The advantage of GloVe compared to word2vec is that it does not rely simply on local information of words, but it also incorporates global statistics such as word co-occurrence. By calculating the mean, mode, median, and variance on the 50-dimensional GloVe feature vector for all column values, a 200-dimensional feature vector is produced in this category.
  \item \textbf{Paragraph vectors.} A distributed bag of words version of the paragraph vector (PV-DBOW), or the doc2vec model~\cite{pmlr-v32-le14} is implemented to capture features at the ``topic'' level of the column. The doc2vec model forces the model to predict random words that are sampled from paragraphs in the output by ignoring context words from the input. The model is pre-trained in Sherlock using the Gensim library to extract a paragraph feature that has 400 dimensions. 
\end{enumerate}

\section{Proposed Graph Model}

Raw JSON data is annotated by treating each key-value pair as a data point. For instance, from the \texttt{user.json} file, four data points can be extracted as illustrated in Figure~\ref{fig:json_ex}. For each key-value pair, the label is determined by annotating the JSON Path to represent the semantic meaning, while the features are extracted using Sherlock from the corresponding values at each path. As an example, consider the key-value pair \texttt{"user": \{"id":9171087, "id\_str": "9171087", "name": "ud83c"\}}. Here, the label assigned is \texttt{user}, and the features are extracted from \texttt{\{"id":9171087, "id\_str": "9171087", "name": "ud83c"\}}. At this stage, we can proceed to apply Sherlock and evaluate its performance for semantic type detection, using it as a baseline result.

The structure of our model is illustrated in Figure~\ref{fig:json_to_graph}. Each JSON file is processed by first obtaining all key-value pairs, as described in the preceding section. Using the same example as in Figure~\ref{fig:json_ex}, we can obtain four key-value pairs. Subsequently, the features for each key-value pair are computed based on their respective values, yielding features $f_1$, $f_2$, $f_3$, and $f_4$.

Following this, four graphs are generated, labeled as "id", "id\_str", "name", and "user". The first three graphs, $G_1$, $G_2$, and $G_3$, each consist of a single-node, with the node features $f_1$, $f_2$, and $f_3$. The fourth graph, $G_4$, is a multi-node graph, with a root node having node features $f_4$ and three edges connecting to three other nodes, each having node features $f_1$, $f_2$, and $f_3$. 

Once we obtain the graph representations, we use graph neural networks (GNNs)\cite{gcn} to perform the classification task. Graph neural networks are well-suited for our problem, as JSON documents inherently possess a tree-like structure that can be represented as a graph. GNNs excel at capturing complex dependencies and relationships within graphs by leveraging structural information encoded in edges and node features. Using GNNs, we can effectively leverage the hierarchical relationships and dependencies present in JSON documents, enabling accurate analysis and prediction tasks such as semantic type prediction or classification. GNNs exploit the rich structural information of JSON documents, making them a powerful approach to understanding and extracting insights from this graph-based representation. In our study, we used the Spektral library\cite{spectral} to implement our GNN. Specifically, we employed a two-layer GCN model, where the input graph was first fed into a GCN layer with 256 hidden units, followed by graph pooling and a dropout layer. The output was then forwarded to another GCN layer with 64 hidden units, before being connected to a dense layer for multi-class classification. To optimize the model, we adopted Adam\cite{adam} as our optimizer and used categorical cross-entropy as our loss function. The learning rate was set to $2 \times 10^{-4}$.

% Figure environment removed

\section{Dataset}
Our study uses Twitter data and Meetup data available on push-shift.io~\cite{pushdata}, a large-scale archive of social media content. Due to the immense size of the dataset, we selected a representative subset of each dataset for our research. Specifically, we extracted all available data from a specific date, resulting in a total of 30,000 distinct JSON objects for Twitter data, and 20,000 distinct JSON objects for Meetup data.

Figure~\ref{fig:twitter_ex} shows an example of the Twitter dataset used in this study. The example contains labels for a single-node graph, including \texttt{created\_at}, \texttt{id}, and \texttt{screen\_name}. The \texttt{profile\_link\_color}, \texttt{profile\_sidebar\_border\_color}, \texttt{profile\_sidebar\_fill\_color}, and \texttt{profile\_text\_color} also belong to single-node graphs with label color. Additionally, the example comprises multi-node graphs with labels such as \texttt{bounding\_box} and \texttt{user\_mentions}, which are also illustrated in the figure. The above-mentioned nodes have a depth of 1 in the graph, while nodes such as \texttt{type} and \texttt{coordinate} in this example have a depth of 2. Table~\ref{tab:depth} presents the number of examples corresponding to each depth (level of nesting) for our two datasets.

\begin{table}[ht]
    \centering
    \begin{tabular}{ccc}
        \hline
        Depth & \multicolumn{2}{c}{Number of examples} \\
        \cline{2-3}
        & Twitter & Meetup\\
        \hline
        1 & 1,205 & 191,884 \\
        2 & 32,158 & 304,750 \\
        3 & 74,187 & 101,930 \\
        4 & 3,907 & 0 \\
        5 & 124 & 0 \\
        \hline

    \end{tabular}
    \caption{Number of examples for each depth}
    \label{tab:depth}
\end{table}

All key-value pairs with a null value or a type of Boolean are ignored since these values are highly repetitive in our dataset and do not contain useful semantic information. We then annotated each JSON Path with a class label, as discussed in the previous section, resulting in a label set comprising 43 distinct classes for the Twitter dataset and 32 distinct classes for the Meetup dataset. 

To prepare the data for use in our model, we processed each JSON file into a graph structure, resulting in a total of around 110,000 distinct graphs for the Twitter dataset and 600,000 distinct graphs for the Meetup dataset. Each graph in our dataset is accompanied by a set of node features, an adjacency matrix, and a class label encoded in one-hot format. The dataset is partitioned into training, validation, and test sets in a 7:3:3 ratio.


% Figure environment removed

\section{Experiment Method}
In our experiment, we initially preprocess the JSON file by extracting key-value pairs based on their corresponding JSON paths. Figure~\ref{fig:json_to_tab} demonstrates the conversion of a JSON document into relational tables, where each key-value pair is represented as a separate column. Subsequently, a feature extraction process is applied to these values to obtain the feature vectors. This step enables us to utilize the Sherlock model for classification, allowing us to establish a baseline performance measure. We then proceed to process the data into graphs, following the procedures outlined in Figure~\ref{fig:json_to_graph}. The classification task is then performed using a graph neural network model. We summarize the time spent on each preprocessing step in Table~\ref{tab:preprocessing}, providing an overview of the time requirements for these tasks on our two different datasets. The key-value pair extraction step takes 1,205s and 3,525s for the Twitter and Meetup datasets respectively, which is the most time-consuming preprocessing step. The feature extraction and graph processing steps take less time than the key-value pair extraction. The feature extraction step is the only preprocessing step required to train the Sherlock model. 

% Figure environment removed

\begin{table}[ht]
    \centering
    \begin{tabular}{ccc}
        \hline
        Preprocessing steps & Twitter & Meetup \\
        \hline
        Key-value pair extraction & 1,205s & 3,525s \\
        Feature extraction & 785s & 2,100s \\
        Graph processing & 152s & 450s \\
        \hline
        Total time & 2,142s & 7,075s \\
        \hline

    \end{tabular}
    \caption{Time spent on preprocessing steps}
    \label{tab:preprocessing}
\end{table}

\section{Experiment Results}
\begin{table}[htbp]
  \centering
  \caption{Comparison of Sherlock and the proposed model on Twitter dataset}
  \label{tab:comparison}
  \begin{tabularx}{\linewidth}{@{}l *{5}{>{\centering\arraybackslash}X}@{}}
    \hline
     &  & \multicolumn{2}{c}{Sherlock} & \multicolumn{2}{c}{Proposed model} \\
    \cline{3-4} \cline{5-6}
    & Label & \small{F1 score} & \small{Accuracy} & \small{F1 score} & \small{Accuracy} \\
    \hline
    \small{Single-Node} & \small{screen\_name} & 0.92 & 0.93 & \textbf{0.95} & 0.93 \\
       & \small{country\_code} & 0.80 & \textbf{1.00} & \textbf{0.92} & 0.85 \\
       & \small{timestamp\_ms} & \textbf{1.00} & \textbf{1.00} & 0.97 & 0.96 \\
       & \small{color} & \textbf{0.99} & 0.99 & 0.98 & 0.99 \\
       & \small{description} & \textbf{0.75} & \textbf{0.77} & 0.67 & 0.60 \\
    \hline
    \small{Multi-Node} & \small{bounding\_box} & 0.83 & 1.00 & \textbf{1.00} & 1.00 \\
    & \small{user\_mentions} & 0.59 & 0.41 & \textbf{0.84} & \textbf{0.97} \\
    & \small{retweet\_status} & 0.57 & 0.48 & \textbf{0.82} & \textbf{0.86}\\
    & \small{hashtag} & 0.34 & 0.23 & \textbf{0.40} & \textbf{0.80} \\
    & \small{full\_name} & 0.00 & 0.00 & \textbf{0.22} & \textbf{0.97} \\
    \hline
    Average &  & 0.82 & 0.84 & \textbf{0.85} & \textbf{0.85} \\
    \hline
  \end{tabularx}
\end{table}


Table~\ref{tab:comparison} provides a comprehensive comparison between the performance of Sherlock and the proposed model in terms of the F1 score and accuracy metrics. The evaluation is conducted for all semantic classes, which are categorized into single-node and multi-node based on the number of nodes in the graph. The table presents some selected examples from each category, including \texttt{screen\_name}, \texttt{country\_code}, \texttt{timestamp\_ms}, color, and description for single-node, and \texttt{bounding\_box}, \texttt{user\_mentions}, \texttt{retweet\_status}, \texttt{hashtag}, and \texttt{full\_name} for multi-node. The average performance result is also presented at the bottom of the table.

In the single-node setting, the proposed model outperforms Sherlock on some labels, for example, with F1 scores of 0.95 for \texttt{screen\_name}, 0.92 for \texttt{country\_code}, compared to 0.92 and 0.80, respectively, for Sherlock. However, Sherlock achieves a perfect F1 score of 1.00 for the \texttt{timestamp\_ms} label, while the proposed model only achieves a score of 0.97. The Sherlock model also exhibits higher accuracy levels for the semantic types of \texttt{country\_code} and \texttt{timestamp\_ms}. Sherlock also performs slightly better for the \texttt{description} label, with an F1 score of 0.75 compared to 0.67 for the proposed model. Based on the results, it can be suggested that the base Sherlock model might exhibit better performance when applied to single-node scenarios, which may be attributed to the fact that the Sherlock model utilizes a more complex neural network architecture compared to our network.

In the multi-node setting, the proposed model achieves a perfect F1 score of 1.00 for the \texttt{bounding\_box} label, while Sherlock achieves a score of 0.83. The proposed model also outperforms Sherlock for the \texttt{user\_mentions} and \texttt{retweet\_status} labels, with F1 scores of 0.84 and 0.82 compared to 0.59 and 0.57, respectively. However, Sherlock achieves a slightly better F1 score of 0.40 for the hashtag label compared to 0.34 for the proposed model. The proposed model achieves a higher F1 score of 0.22 for the \texttt{full\_name} label compared to 0.00 for Sherlock. In terms of accuracy, our proposed model significantly outperforms Sherlock. Specifically, for the hashtag semantic type, our model achieves an accuracy of 0.80, while Sherlock only achieves an accuracy of 0.23. Similarly, for the \texttt{full\_name} semantic type, our model achieves an accuracy of 0.50, while Sherlock fails to classify any instance correctly. These findings demonstrate that our proposed model is more adept at predicting complex structures, thus providing greater utility for practical applications.

Table~\ref{tab:meetupcomp} presents the comparison between Sherlock and our proposed model on the meetup dataset. The JSON files within the meetup dataset exhibit highly similar structures, resulting in higher overall prediction accuracy and F1 score. In cases involving multiple nodes, our model achieves perfect accuracy of 1.00, while Sherlock performs equally well on classes such as event and category, with F1 scores of 0.99 and 0.97 respectively for \texttt{group} and \texttt{group\_photo}. For single-node classes, both models demonstrate similar performance levels. The Meetup dataset consists of numerous homogeneous JSON files, many of which exhibit similar hierarchical structures. As a result, the base model Sherlock can achieve good performance in the multi-node setup, indicating that our model does not outperform Sherlock on this dataset. The reason could be that the Meetup dataset exhibits a higher level of homogeneity and has a less complex hierarchical structure for our model to take advantage of.

\begin{table}[htbp]
  \centering
  \caption{Comparison of Sherlock and the proposed model on the Meetup dataset}
  \label{tab:meetupcomp}
  \begin{tabularx}{\linewidth}{@{}l *{5}{>{\centering\arraybackslash}X}@{}}
    \hline
     &  & \multicolumn{2}{c}{Sherlock} & \multicolumn{2}{c}{Proposed model} \\
    \cline{3-4} \cline{5-6}
    & Label & \small{F1 score} & \small{Accuracy} & \small{F1 score} & \small{Accuracy} \\
    \hline
    \small{Single-Node} & \small{ event\_id} & 0.66 & \textbf{0.64} & \textbf{0.68} & 0.63 \\
       & \small{id} & 0.86 & 0.85 & \textbf{0.91} & \textbf{0.90} \\
       & \small{member\_name} & \textbf{0.95} & \textbf{0.95} & \textbf{0.95} & 0.94 \\
       & \small{shortname} & \textbf{0.99} & 0.99 & 0.98 & \textbf{1.00} \\
    \hline
    \small{Multi-Node} 
    & \small{event} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} \\
    & \small{category} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00}\\
    & \small{group} & 0.99 & 0.97 & \textbf{1.00} & \textbf{1.00} \\
    & \small{group\_photo } & 0.97 & 0.99 & \textbf{1.00} & \textbf{1.00} \\
    \hline
    Average &  & 0.89 & 0.89 & \textbf{0.92} & \textbf{0.90} \\
    \hline
  \end{tabularx}
\end{table}


\begin{table}
\centering
\caption{Training time and model size comparison}
\label{tab:traintime_modelsize}
\begin{tabular}{cccc}
\hline
Model & \multicolumn{2}{c}{Training time} & Model size \\
\cline{2-3}
& Twitter & Meetup &\\
\hline
Sherlock & 252s & 690s & 5.9MB \\
Proposed model & 1,230s & 3,051s & 1.9MB \\
\hline
\end{tabular}
\end{table}

Table \ref{tab:traintime_modelsize} shows a comparison between the training time and model size of Sherlock and the proposed model. Our proposed model takes significantly longer to train but produces a much smaller model. We expect that advances in graph neural network training will apply to our setting to further reduce the training time~\cite{bytegnn,scalablegnn}. Our proposed model exhibits a notable reduction in model size in comparison to Sherlock. This is mainly attributed to the fact that our model incorporates a smaller number of neural network layers. Consequently, this disparity may account for certain suboptimal predictions made by our model in comparison to Sherlock, particularly in single-node predictions.

Our proposed model achieves a higher average F1 score compared to Sherlock. These results suggest that our proposed model has a superior ability to learn structural information from the data.

\section{Future Work}
In our ongoing research, we aim to explore alternative subgraph representations to enhance the prediction of semantic types in our proposed model. The current subgraphs we utilize do not incorporate the parent node and its sibling nodes, thus missing out on valuable information present in those nodes. Sato~\cite{sato} is a model that utilizes neighbor columns and table-level information for semantic prediction; however, it exhibits limited scalability in terms of training time. To address this limitation, we plan to construct subgraphs that include the parent and sibling nodes of the target node. In addition, we will assign edge weights to the graph, allowing us to emphasize the importance of the target node for prediction. By enabling additional edge features within each subgraph, we can leverage neural networks such as edge-conditioned GCN~\cite{eccv} that are capable of utilizing edge weight information. Subsequently, we intend to combine the outputs obtained from different subgraphs using a transformer~\cite{transformer}. The transformer architecture is suitable for this purpose because of its ability to capture global dependencies and model the relationships between the outputs of different subgraphs effectively.

In addition, we plan to extend our experimentation beyond the current dataset in order to enhance the robustness and generalizability of our model. Specifically, we will train and test our model on additional datasets that are representative of real-world scenarios. Our goal is to evaluate the effectiveness of the model's predictive capabilities on JSON structures that it has not yet encountered. Moreover, we aim to examine the impact of training set size on the performance of our model, with particular attention to the detection of less frequent semantic types. By conducting such analyses, we aim to better understand the limitations and strengths of our model and further refine it for real-world applications.


\section{Conclusion}

In this paper, we proposed a model for predicting semantic types in nested JSON data that can be used for various automated data processing tasks. Existing models either predict for a single set of values at a time or are limited to non-nested relational data.

To address this limitation, we proposed an extension of the semantic type prediction problem to semi-structured JSON data with types labeled based on JSON Paths. Our proposed model annotates the semantic type of JSON data with its hierarchical structure and employs a graph neural network to predict the semantic type using the same set of features extracted by Sherlock. We demonstrate several cases where our model outperforms Sherlock, indicating its ability to comprehend complex JSON data and its potential for semi-structured data processing tasks.

Our ongoing research focuses on enhancing the prediction of semantic types in our proposed model through alternative subgraph representations. By incorporating the parent and sibling nodes into subgraphs and assigning edge weights, we can capture valuable information for accurate predictions. Leveraging additional edge features will further improve our model's performance. We plan to validate our approach on diverse datasets, ensuring its robustness and generalizability to real-world scenarios. Furthermore, we plan to investigate the impact of the size of the training set on model performance, especially for detecting less frequent semantic types.

In conclusion, our work contributes to the development of deep learning models for predicting semantic types in JSON data. Our proposed model offers a better understanding of complex data and improved performance compared to Sherlock on semi-structured data.
\balance

\iffalse
\section{Authors and Affiliations}

Each author must be defined separately for accurate metadata
identification. Multiple authors may share one affiliation. Authors'
names should not be abbreviated; use full first names wherever
possible. Include authors' e-mail addresses whenever possible.

Grouping authors' names or e-mail addresses, or providing an ``e-mail
alias,'' as shown below, is not acceptable:
\begin{verbatim}
  \author{Brooke Aster, David Mehldau}
  \email{dave,judy,steve@university.edu}
  \email{firstname.lastname@phillips.org}
\end{verbatim}

The \verb|authornote| and \verb|authornotemark| commands allow a note
to apply to multiple authors --- for example, if the first two authors
of an article contributed equally to the work.

If your author list is lengthy, you must define a shortened version of
the list of authors to be used in the page headers, to prevent
overlapping text. The following command should be placed just after
the last \verb|\author{}| definition:
\begin{verbatim}
  \renewcommand{\shortauthors}{McCartney, et al.}
\end{verbatim}
Omitting this command will force the use of a concatenated list of all
of the authors' names, which may result in overlapping text in the
page headers.

The article template's documentation, available at
\url{https://www.acm.org/publications/proceedings-template}, has a
complete explanation of these commands and tips for their effective
use.

Note that authors' addresses are mandatory for journal articles.

\section{Rights Information}

Authors of any work published by ACM will need to complete a rights
form. Depending on the kind of work, and the rights management choice
made by the author, this may be copyright transfer, permission,
license, or an OA (open access) agreement.

Regardless of the rights management choice, the author will receive a
copy of the completed rights form once it has been submitted. This
form contains \LaTeX\ commands that must be copied into the source
document. When the document source is compiled, these commands and
their parameters add formatted text to several areas of the final
document:
\begin{itemize}
\item the ``ACM Reference Format'' text on the first page.
\item the ``rights management'' text on the first page.
\item the conference information in the page header(s).
\end{itemize}

Rights information is unique to the work; if you are preparing several
works for an event, make sure to use the correct set of commands with
each of the works.

The ACM Reference Format text is required for all articles over one
page in length, and is optional for one-page articles (abstracts).

\section{CCS Concepts and User-Defined Keywords}

Two elements of the ``acmart'' document class provide powerful
taxonomic tools for you to help readers find your work in an online
search.

The ACM Computing Classification System ---
\url{https://www.acm.org/publications/class-2012} --- is a set of
classifiers and concepts that describe the computing
discipline. Authors can select entries from this classification
system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
commands to be included in the \LaTeX\ source.

User-defined keywords are a comma-separated list of words and phrases
of the authors' choosing, providing a more flexible way of describing
the research being presented.

CCS concepts and user-defined keywords are required for for all
articles over two pages in length, and are optional for one- and
two-page articles (or abstracts).

\section{Sectioning Commands}

Your work should use standard \LaTeX\ sectioning commands:
\verb|section|, \verb|subsection|, \verb|subsubsection|, and
\verb|paragraph|. They should be numbered; do not remove the numbering
from the commands.

Simulating a sectioning command by setting the first word or words of
a paragraph in boldface or italicized text is {\bfseries not allowed.}

\section{Tables}

The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
high-quality tables.

Table captions are placed {\itshape above} the table.

Because tables cannot be split across pages, the best placement for
them is typically the top of the page nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and the
table caption.  The contents of the table itself must go in the
\textbf{tabular} environment, to be aligned properly in rows and
columns, with the desired horizontal and vertical rules.  Again,
detailed instructions on \textbf{tabular} material are found in the
\textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table~\ref{tab:freq} is included in the input file; compare the
placement of the table here with the table in the printed output of
this document.

\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's
live area, use the environment \textbf{table*} to enclose the table's
contents and the table caption.  As with a single-column table, this
wide table will ``float'' to a location deemed more
desirable. Immediately following this sentence is the point at which
Table~\ref{tab:commands} is included in the input file; again, it is
instructive to compare the placement of the table here with the table
in the printed output of this document.

\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}

Always use midrule to separate table header rows from data rows, and
use it only for this purpose. This enables assistive technologies to
recognise table headers and support their users in navigating tables
more easily.

\section{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of the three are
discussed in the next sections.

\subsection{Inline (In-text) Equations}
A formula that appears in the running text is called an inline or
in-text formula.  It is produced by the \textbf{math} environment,
which can be invoked with the usual
\texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
and structures, from $\alpha$ to $\omega$, available in
\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
examples of in-text equations in context. Notice how this equation:
\begin{math}
  \lim_{n\rightarrow \infty}x=0
\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsection{Display Equations}
A numbered display equation---one set off by vertical space from the
text and centered horizontally---is produced by the \textbf{equation}
environment. An unnumbered display equation is produced by the
\textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols and
structures available in \LaTeX\@; this section will just give a couple
of examples of display equations in context.  First, consider the
equation, shown as an inline equation above:
\begin{equation}
  \lim_{n\rightarrow \infty}x=0
\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}
  \sum_{i=0}^{\infty} x + 1
\end{displaymath}
and follow it with another numbered equation:
\begin{equation}
  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\section{Figures}

The ``\verb|figure|'' environment should be used for figures. One or
more images can be placed within a figure. If your figure contains
third-party material, you must clearly identify it as such, as shown
in the example below.
% Figure environment removed

Your figures should contain a caption which describes the figure to
the reader.

Figure captions are placed {\itshape below} the figure.

Every figure should also have a figure description unless it is purely
decorative. These descriptions convey what’s in the image to someone
who cannot see it. They are also used by search engine crawlers for
indexing images, and when images cannot be loaded.

A figure description must be unformatted plain text less than 2000
characters long (including spaces).  {\bfseries Figure descriptions
  should not repeat the figure caption – their purpose is to capture
  important information that is not already provided in the caption or
  the main text of the paper.} For figures that convey important and
complex new information, a short text description may not be
adequate. More complex alternative descriptions can be placed in an
appendix and referenced in a short figure description. For example,
provide a data table capturing the information in a bar chart, or a
structured list representing a graph.  For additional information
regarding how best to write figure descriptions and why doing this is
so important, please see
\url{https://www.acm.org/publications/taps/describing-figures/}.

\subsection{The ``Teaser Figure''}

A ``teaser figure'' is an image, or set of images in one figure, that
are placed after all author and affiliation information, and before
the body of the article, spanning the page. If you wish to have such a
figure in your article, place the command immediately before the
\verb|\maketitle| command:
\begin{verbatim}
  \begin{teaserfigure}
    % Figure removed
    \caption{figure caption}
    \Description{figure description}
  \end{teaserfigure}
\end{verbatim}

\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's
references is strongly recommended. Authors' names should be complete
--- use full first names (``Donald E. Knuth'') not initials
(``D. E. Knuth'') --- and the salient identifying features of a
reference should be included: title, year, volume, number, pages,
article DOI, etc.

The bibliography is included in your source document with these two
commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of
ACM publications have citations and references formatted in the
``author year'' style; for these exceptions, please include this
command in the {\bfseries preamble} (before the command
``\verb|\begin{document}|'') of your \LaTeX\ source:
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}

  Some examples.  A paginated journal article \cite{Abril07}, an
  enumerated journal article \cite{Cohen07}, a reference to an entire
  issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
  monograph/whole book in a series (see 2a in spec. document)
  \cite{Harel79}, a divisible-book such as an anthology or compilation
  \cite{Editor00} followed by the same example, however we only output
  the series if the volume number is given \cite{Editor00a} (so
  Editor00a's series should NOT be present since it has no vol. no.),
  a chapter in a divisible book \cite{Spector90}, a chapter in a
  divisible book in a series \cite{Douglass98}, a multi-volume work as
  book \cite{Knuth97}, a couple of articles in a proceedings (of a
  conference, symposium, workshop for example) (paginated proceedings
  article) \cite{Andler79, Hagerup1993}, a proceedings article with
  all possible elements \cite{Smith10}, an example of an enumerated
  proceedings article \cite{VanGundy07}, an informally published work
  \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
    AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
  master's thesis: \cite{anisi03}, an online document / world wide web
  resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
  (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
  and (Case 3) a patent \cite{JoeScientist001}, work accepted for
  publication \cite{rous08}, 'YYYYb'-test for prolific author
  \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
  contain 'duplicate' DOI and URLs (some SIAM articles)
  \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
  multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
  couple of citations with DOIs:
  \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
  citations: \cite{TUGInstmem, Thornburg01, CTANacmart}. Artifacts:
  \cite{R} and \cite{UMassCitations}.

\section{Acknowledgments}

Identification of funding sources and other support, and thanks to
individuals and groups that assisted in the research and the
preparation of the work should be included in an acknowledgment
section, which is placed just before the reference section in your
document.

This section has a special environment:
\begin{verbatim}
  \begin{acks}
  ...
  \end{acks}
\end{verbatim}
so that the information contained therein can be more easily collected
during the article metadata extraction phase, and to ensure
consistency in the spelling of the section heading.

Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.

\section{Multi-language papers}

Papers may be written in languages other than English or include
titles, subtitles, keywords and abstracts in different languages (as a
rule, a paper in a language other than English should include an
English title and an English abstract).  Use \verb|language=...| for
every language used in the paper.  The last language indicated is the
main language of the paper.  For example, a French paper with
additional titles and abstracts in English and German may start with
the following command
\begin{verbatim}
\documentclass[sigconf, language=english, language=german,
               language=french]{acmart}
\end{verbatim}

The title, subtitle, keywords and abstract will be typeset in the main
language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
begin title, subtitle and keywords, can be used to set these elements
in the other languages.  The environment \verb|translatedabstract| is
used to set the translation of the abstract.  These commands and
environment have a mandatory first argument: the language of the
second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
of their usage.

\section{SIGCHI Extended Abstracts}

The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
not in Word) produces a landscape-orientation formatted article, with
a wide left margin. Three environments are available for use with the
``\verb|sigchi-a|'' template style, and produce formatted output in
the margin:
\begin{itemize}
\item {\verb|sidebar|}:  Place formatted text in the margin.
\item {\verb|marginfigure|}: Place a figure in the margin.
\item {\verb|margintable|}: Place a table in the margin.
\end{itemize}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}
\fi
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

%%
%% If your work has an appendix, this is the place to put it.


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
