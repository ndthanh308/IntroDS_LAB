@article{alexey2015discriminative,
  title={Discriminative unsupervised feature learning with exemplar convolutional neural networks},
  author={Alexey, Dosovitskiy and Fischer, Philipp and Tobias, Jost and Springenberg, Martin Riedmiller and Brox, Thomas},
  journal={IEEE Trans. Pattern Analysis and Machine Intelligence},
  volume={99},
  year={2015}
}

 @inproceedings{cubuk2019autoaugment,
  title={Autoaugment: Learning augmentation strategies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={113--123},
  year={2019}
}

@inproceedings{lim2019fast,
  title={Fast autoaugment},
  author={Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6662--6672},
  year={2019}
}

@article{ho2019population,
  title={Population based augmentation: Efficient learning of augmentation policy schedules},
  author={Ho, Daniel and Liang, Eric and Stoica, Ion and Abbeel, Pieter and Chen, Xi},
  journal={arXiv preprint arXiv:1905.05393},
  year={2019}
}

@inproceedings{
zhang2020adversarial,
title={Adversarial AutoAugment},
author={Xinyu Zhang and Qiang Wang and Jian Zhang and Zhao Zhong},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxdUySKvS}
}

@ARTICLE{8633930,
  author={Balakrishnan, Guha and Zhao, Amy and Sabuncu, Mert R. and Guttag, John and Dalca, Adrian V.},
  journal={IEEE Transactions on Medical Imaging}, 
  title={VoxelMorph: A Learning Framework for Deformable Medical Image Registration}, 
  year={2019},
  volume={38},
  number={8},
  pages={1788-1800},
  doi={10.1109/TMI.2019.2897538}}


@inproceedings{hauberg2016dreaming,
  title={Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation},
  author={Hauberg, S{\o}ren and Freifeld, Oren and Larsen, Anders Boesen Lindbo and Fisher, John and Hansen, Lars},
  booktitle={Artificial Intelligence and Statistics},
  pages={342--350},
  year={2016}
}


@InProceedings{10.1007/978-3-030-59716-0_31,
author="Shen, Zhengyang
and Xu, Zhenlin
and Olut, Sahin
and Niethammer, Marc",
editor="Martel, Anne L.
and Abolmaesumi, Purang
and Stoyanov, Danail
and Mateus, Diana
and Zuluaga, Maria A.
and Zhou, S. Kevin
and Racoceanu, Daniel
and Joskowicz, Leo",
title="Anatomical Data Augmentation via Fluid-Based Image Registration",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="318--328",
abstract="We introduce a fluid-based image augmentation method for medical image analysis. In contrast to existing methods, our framework generates anatomically meaningful images via interpolation from the geodesic subspace underlying given samples. Our approach consists of three steps: 1) given a source image and a set of target images, we construct a geodesic subspace using the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample transformations from the resulting geodesic subspace; 3) we obtain deformed images and segmentations via interpolation. Experiments on brain (LPBA) and knee (OAI) data illustrate the performance of our approach on two tasks: 1) data augmentation during training and testing for image segmentation; 2) one-shot learning for single atlas image segmentation. We demonstrate that our approach generates anatomically meaningful data and improves performance on these tasks over competing approaches. Code is available at https://github.com/uncbiag/easyreg.",
isbn="978-3-030-59716-0"
}


@InProceedings{Zhao_2019_CVPR,
author = {Zhao, Amy and Balakrishnan, Guha and Durand, Fredo and Guttag, John V. and Dalca, Adrian V.},
title = {Data Augmentation Using Learned Transformations for One-Shot Medical Image Segmentation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{10.1007/978-3-030-87193-2_55,
author="Yang, Jiawei
and Zhang, Yao
and Liang, Yuan
and Zhang, Yang
and He, Lei
and He, Zhiqiang",
editor="de Bruijne, Marleen
and Cattin, Philippe C.
and Cotin, St{\'e}phane
and Padoy, Nicolas
and Speidel, Stefanie
and Zheng, Yefeng
and Essert, Caroline",
title="TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="579--588",
abstract="Deep learning models are notoriously data-hungry. Thus, there is an urging need for data-efficient techniques in medical image analysis, where well-annotated data are costly and time consuming to collect. Motivated by the recently revived ``Copy-Paste'' augmentation, we propose TumorCP, a simple but effective object-level data augmentation method tailored for tumor segmentation. TumorCP is online and stochastic, providing unlimited augmentation possibilities for tumors' subjects, locations, appearances, as well as morphologies. Experiments on kidney tumor segmentation task demonstrate that TumorCP surpasses the strong baseline by a remarkable margin of 7.12{\%} on tumor Dice. Moreover, together with image-level data augmentation, it beats the current state-of-the-art by 2.32{\%} on tumor Dice. Comprehensive ablation studies are performed to validate the effectiveness of TumorCP. Meanwhile, we show that TumorCP can lead to striking improvements in extremely low-data regimes. Evaluated with only 10{\%} labeled data, TumorCP significantly boosts tumor Dice by 21.87{\%}. To the best of our knowledge, this is the very first work exploring and extending the ``Copy-Paste'' design in medical imaging domain. Code is available at: https://github.com/YaoZhang93/TumorCP.",
isbn="978-3-030-87193-2"
}

@misc{https://doi.org/10.48550/arxiv.2203.10507,
  doi = {10.48550/ARXIV.2203.10507},
  
  url = {https://arxiv.org/abs/2203.10507},
  
  author = {Dai, Pingping and Dong, Licong and Zhang, Ruihan and Zhu, Haiming and Wu, Jie and Yuan, Kehong},
  
  keywords = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Soft-CP: A Credible and Effective Data Augmentation for Semantic Segmentation of Medical Lesions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@InProceedings{10.1007/978-3-031-16440-8_65,
author="Zhu, Qikui
and Wang, Yanqing
and Yin, Lei
and Yang, Jiancheng
and Liao, Fei
and Li, Shuo",
editor="Wang, Linwei
and Dou, Qi
and Fletcher, P. Thomas
and Speidel, Stefanie
and Li, Shuo",
title="SelfMix: A Self-adaptive Data Augmentation Method for Lesion Segmentation",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="683--692",
abstract="Deep learning-based methods haveobtained promising results in various organ segmentation tasks, due to their effectiveness in learning feature representation. However, accurate segmentation of lesions can still be challenging due to 1) the lesions provide less information than normal organs; 2) the available number of labeled lesions is more limited than normal organs; 3) the morphology, shape, and size of lesions are more diverse than normal organs. To increase the number of lesion samples and further boost the performance of various lesion segmentation, in this paper, we propose a simple but effective lesion-aware data augmentation method called Self-adaptive Data Augmentation (SelfMix). Compared with existing data augmentation methods, such as Mixup, CutMix, and CarveMix, our proposed SelfMix have three-fold advances: 1) Solving the challenges that the generated tumor images are facing the problem of distortion by absorbing both tumor and non-tumor information; 2) SelfMix is tumor-aware, which can adaptively adjust the fusing weights of each lesion voxels based on the geometry and size information from the tumor itself; 3) SelfMix is the first one that notices non-tumor information. To evaluate the proposed data augmentation method, experiments were performed on two public lesion segmentation datasets. The results show that our method improves the lesion segmentation accuracy compared with other data augmentation approaches.",
isbn="978-3-031-16440-8"
}


@InProceedings{10.1007/978-3-030-87193-2_19,
author="Zhang, Xinru
and Liu, Chenghao
and Ou, Ni
and Zeng, Xiangzhu
and Xiong, Xiaoliang
and Yu, Yizhou
and Liu, Zhiwen
and Ye, Chuyang",
editor="de Bruijne, Marleen
and Cattin, Philippe C.
and Cotin, St{\'e}phane
and Padoy, Nicolas
and Speidel, Stefanie
and Zheng, Yefeng
and Essert, Caroline",
title="CarveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="196--205",
abstract="Brain lesion segmentation provides a valuable tool for clinical diagnosis, and convolutional neural networks (CNNs) have achieved unprecedented success in the task. Data augmentation is a widely used strategy that improves the training of CNNs, and the design of the augmentation method for brain lesion segmentation is still an open problem. In this work, we propose a simple data augmentation approach, dubbed as CarveMix, for CNN-based brain lesion segmentation. Like other ``mix``-based methods, such as Mixup and CutMix, CarveMix stochastically combines two existing labeled images to generate new labeled samples. Yet, unlike these augmentation strategies based on image combination, CarveMix is lesion-aware, where the combination is performed with an attention on the lesions and a proper annotation is created for the generated image. Specifically, from one labeled image we carve a region of interest (ROI) according to the lesion location and geometry, and the size of the ROI is sampled from a probability distribution. The carved ROI then replaces the corresponding voxels in a second labeled image, and the annotation of the second image is replaced accordingly as well. In this way, we generate new labeled images for network training and the lesion information is preserved. To evaluate the proposed method, experiments were performed on two brain lesion datasets. The results show that our method improves the segmentation accuracy compared with other simple data augmentation approaches.",
isbn="978-3-030-87193-2"
}

@misc{https://doi.org/10.48550/arxiv.1904.00445,
  doi = {10.48550/ARXIV.1904.00445},
  
  url = {https://arxiv.org/abs/1904.00445},
  
  author = {Heller, Nicholas and Sathianathen, Niranjan and Kalapara, Arveen and Walczak, Edward and Moore, Keenan and Kaluzniak, Heather and Rosenberg, Joel and Blake, Paul and Rengel, Zachary and Oestreich, Makinna and Dean, Joshua and Tradewell, Michael and Shah, Aneri and Tejpaul, Resha and Edgerton, Zachary and Peterson, Matthew and Raza, Shaneabbas and Regmi, Subodh and Papanikolopoulos, Nikolaos and Weight, Christopher},
  
  keywords = {Quantitative Methods (q-bio.QM), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Biological sciences, FOS: Biological sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}


@article{BILIC2023102680,
title = {The Liver Tumor Segmentation Benchmark (LiTS)},
journal = {Medical Image Analysis},
volume = {84},
pages = {102680},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102680},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522003085},
author = {Patrick Bilic and Patrick Christ and Hongwei Bran Li and Eugene Vorontsov and Avi Ben-Cohen and Georgios Kaissis and Adi Szeskin and Colin Jacobs and Gabriel Efrain Humpire Mamani and Gabriel Chartrand and Fabian Lohöfer and Julian Walter Holch and Wieland Sommer and Felix Hofmann and Alexandre Hostettler and Naama Lev-Cohain and Michal Drozdzal and Michal Marianne Amitai and Refael Vivanti and Jacob Sosna and Ivan Ezhov and Anjany Sekuboyina and Fernando Navarro and Florian Kofler and Johannes C. Paetzold and Suprosanna Shit and Xiaobin Hu and Jana Lipková and Markus Rempfler and Marie Piraud and Jan Kirschke and Benedikt Wiestler and Zhiheng Zhang and Christian Hülsemeyer and Marcel Beetz and Florian Ettlinger and Michela Antonelli and Woong Bae and Míriam Bellver and Lei Bi and Hao Chen and Grzegorz Chlebus and Erik B. Dam and Qi Dou and Chi-Wing Fu and Bogdan Georgescu and Xavier Giró-i-Nieto and Felix Gruen and Xu Han and Pheng-Ann Heng and Jürgen Hesser and Jan Hendrik Moltz and Christian Igel and Fabian Isensee and Paul Jäger and Fucang Jia and Krishna Chaitanya Kaluva and Mahendra Khened and Ildoo Kim and Jae-Hun Kim and Sungwoong Kim and Simon Kohl and Tomasz Konopczynski and Avinash Kori and Ganapathy Krishnamurthi and Fan Li and Hongchao Li and Junbo Li and Xiaomeng Li and John Lowengrub and Jun Ma and Klaus Maier-Hein and Kevis-Kokitsi Maninis and Hans Meine and Dorit Merhof and Akshay Pai and Mathias Perslev and Jens Petersen and Jordi Pont-Tuset and Jin Qi and Xiaojuan Qi and Oliver Rippel and Karsten Roth and Ignacio Sarasua and Andrea Schenk and Zengming Shen and Jordi Torres and Christian Wachinger and Chunliang Wang and Leon Weninger and Jianrong Wu and Daguang Xu and Xiaoping Yang and Simon Chun-Ho Yu and Yading Yuan and Miao Yue and Liping Zhang and Jorge Cardoso and Spyridon Bakas and Rickmer Braren and Volker Heinemann and Christopher Pal and An Tang and Samuel Kadoury and Luc Soler and Bram {van Ginneken} and Hayit Greenspan and Leo Joskowicz and Bjoern Menze},
keywords = {Segmentation, Liver, Liver tumor, Deep learning, Benchmark, CT},
abstract = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LiTS), which was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2017 and the International Conferences on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2017 and 2018. The image dataset is diverse and contains primary and secondary tumors with varied sizes and appearances with various lesion-to-background levels (hyper-/hypo-dense), created in collaboration with seven hospitals and research institutions. Seventy-five submitted liver and liver tumor segmentation algorithms were trained on a set of 131 computed tomography (CT) volumes and were tested on 70 unseen test images acquired from different patients. We found that not a single algorithm performed best for both liver and liver tumors in the three events. The best liver segmentation algorithm achieved a Dice score of 0.963, whereas, for tumor segmentation, the best algorithms achieved Dices scores of 0.674 (ISBI 2017), 0.702 (MICCAI 2017), and 0.739 (MICCAI 2018). Retrospectively, we performed additional analysis on liver tumor detection and revealed that not all top-performing segmentation algorithms worked well for tumor detection. The best liver tumor detection method achieved a lesion-wise recall of 0.458 (ISBI 2017), 0.515 (MICCAI 2017), and 0.554 (MICCAI 2018), indicating the need for further research. LiTS remains an active benchmark and resource for research, e.g., contributing the liver-related segmentation tasks in http://medicaldecathlon.com/. In addition, both data and online evaluation are accessible via https://competitions.codalab.org/competitions/17094.}
}


@INPROCEEDINGS{8578561,
  author={Detlefsen, Nicki Skafte and Freifeld, Oren and Hauberg, Søren},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Deep Diffeomorphic Transformer Networks}, 
  year={2018},
  volume={},
  number={},
  pages={4403-4412},
  doi={10.1109/CVPR.2018.00463}}


@INPROCEEDINGS{7410690,
  author={Freifeld, Oren and Hauberg, Søren and Batmanghelich, Kayhan and Fisher, John W.},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Highly-Expressive Spaces of Well-Behaved Transformations: Keeping it Simple}, 
  year={2015},
  volume={},
  number={},
  pages={2911-2919},
  doi={10.1109/ICCV.2015.333}}


@misc{https://doi.org/10.48550/arxiv.1312.6114,
  doi = {10.48550/ARXIV.1312.6114},
  
  url = {https://arxiv.org/abs/1312.6114},
  
  author = {Kingma, Diederik P and Welling, Max},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Auto-Encoding Variational Bayes},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{dai2022soft,
  title={Soft-CP: A credible and effective data augmentation for semantic segmentation of medical lesions},
  author={Dai, Pingping and Dong, Licong and Zhang, Ruihan and Zhu, Haiming and Wu, Jie and Yuan, Kehong},
  journal={arXiv preprint arXiv:2203.10507},
  year={2022}
}


@misc{https://doi.org/10.48550/arxiv.1412.6980,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{isensee2021nnu,
  title={nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},
  author={Isensee, Fabian and Jaeger, Paul F and Kohl, Simon AA and Petersen, Jens and Maier-Hein, Klaus H},
  journal={Nature methods},
  volume={18},
  number={2},
  pages={203--211},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{balakrishnan2019voxelmorph,
  title={Voxelmorph: a learning framework for deformable medical image registration},
  author={Balakrishnan, Guha and Zhao, Amy and Sabuncu, Mert R and Guttag, John and Dalca, Adrian V},
  journal={IEEE transactions on medical imaging},
  volume={38},
  number={8},
  pages={1788--1800},
  year={2019},
  publisher={IEEE}
}