%%%% ijcai23.tex

\typeout{IJCAI--23 Instructions for Authors}

% These are the instructions for authors for IJCAI-23.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai23.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai23}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}%
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}%
\usepackage{makecell}
\usepackage[capitalise]{cleveref}
\usepackage[switch]{lineno}
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage[title]{appendix}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{xspace}
\usepackage{todonotes}
\usepackage{xcolor}
\usepackage{enumitem}
\newcommand{\wjdd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(WJD): #1}}
\newcommand{\wjd}[1]{{\color{cyan}{[(WJD): #1]}}}

\newcommand{\zkjj}[1]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,size=\scriptsize]{(ZKJ): #1}}
\newcommand{\zkj}[1]{{\color{blue}{[(ZKJ): #1]}}}

\newcommand{\method}{EmotionPrompt\xspace}
\newcommand{\llms}{LLMs\xspace}
\newcommand{\prompt}[1]{{\footnotesize \ttfamily #1}}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information
\pdfinfo{
/TemplateVersion (IJCAI.2023.0)
}

\title{EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus}


% Single author syntax
\author{
    Cheng Li$^1$, Jindong Wang$^2$, Kaijie Zhu$^2$, Yixuan Zhang$^3$, Wenxin Hou$^2$, Jianxun Lian$^2$, Xing Xie$^2$
    \affiliations
    $^1$Inst. of Software, CAS ~ $^2$Microsoft ~ $^3$College of William and Mary
    \emails
    chenglicat0228@gmail.com, jindong.wang@microsoft.com
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\begin{document}

\maketitle

\begin{abstract}
    Large language models (\llms) have achieved significant performance in many fields, such as reasoning, language understanding, and math problem-solving, and are regarded as an important step to artificial general intelligence (AGI). However, the sensitivity of \llms to prompts remain a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose \textbf{\method} to explore emotional intelligence to enhance the performance of \llms. Our \method operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms the original prompt and Zero-shot-CoT in both zero-shot and few-shot settings on eight tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and Flan-T5-large. Furthermore, \method was observed to improve both the truthfulness and informativeness. We believe that \method heralds a novel avenue for exploring interdisciplinary knowledge for interaction between humans and LLMs.
\end{abstract}

\section{Introduction}


Large language models (\llms) have shown remarkable performance in a wide spectrum of tasks, ranging from reasoning, natural language understanding and generation, and even math problem-solving. A recent study~\cite{bubeck2023sparks} claimed that \llms show great potential towards artificial general intelligence (AGI). As \llms are becoming more powerful, an interesting question to study is whether \llms can exhibit similar behaviors as humans in different disciplines such as psychology and social science. More specifically, in this paper, we are interested in improving the interaction between humans and \llms by leveraging existing interdisciplinary knowledge.

In this paper, we aim to enhance the performance of \llms from the perspective of \emph{prompt engineering}. Prompt remains the most popular bridge for humans-\llms interaction. \llms takes prompts as inputs and then outputs answers accordingly.
However, it is known that current \llms are sensitive to prompts, e.g., the sentence style, word order, and different expressions can lead to different outputs \cite{zhao2021calibrate,zhang2022differentiable,lu2022fantastically,mishra2022reframing,zheng2023progressivehint,maus2023adversarial,si2023prompting,zhu2023promptbench}.
To steer LLMs to perform better, many works have been done from different prospects, such as chain-of-thoughts \cite{Wei0SBIXCLZ22}, in-context learning \cite{MinLHALHZ22}, and tree-of-thoughts \cite{abs-2305-10601}. While previous endeavors have shown consistent performance in different tasks, they still focus on improving the robustness from the model side, with little attention paid to the interaction side: \emph{How can we leverage existing knowledge in social science to enhance \llms?}
% Different from them, we explore emotional intelligence of LLMs, and apply it to prompt engineering. 

% Figure environment removed

We take the first step by applying psychological knowledge for \llms enhancement.
Previous studies in psychology showed that adding emotional stimulus to humans, which is related to expectancy, confidence, and social influence, can bring positive effects.
Such examples widely exist in the real world, such as enhancing students' success in education \cite{miltiadou2003applying} and health promotion \cite{bandura1998health} by using encouraging and positive words. Inspired by such psychology phenomenon, we propose \textbf{\method}, which is a simple yet effective approach for \llms enhancement.
Specifically, we design $11$ sentences of emotional stimulus for \llms, which can be simply added to original prompts and demonstrate this improvement.
These emotional stimuli are simple psychological phrases that come after the original prompts. For instance, \cref{fig-motivation} shows an example of using one emotional stimulus, ``\prompt{This is very important to my career}'' at the end of the original prompts to enhance the performance of \llms.


We evaluate $8$ instruction induction tasks \cite{honovich2022instruction} with zero-shot and few-shot prompts.
These tasks are Sentiment Analysis, Sentence Similarity, Cause Selection, Sum, Word in Context, Starting with, Larger animal, and First Letter.
We evaluate on $4$ \llms: ChatGPT, Vicuna-13b \cite{abs-2302-11665}, Bloom \cite{abs-2211-05100} and T5-Large \cite{abs-2210-11416}. 
% Zero-shot and few-shot result on ChatGPT are shown in \cref{tb-chatgpt-result-zero} and \cref{tb-chatgpt-result-few}.
Results show that \method achieves comparable or better performance on all tasks, and the accuracy increased over $\mathbf{10}\%$ on more than half of the tasks.
% \cref{tb-vicuan-result-zero} and \cref{tb-vicuna-result-few} show results on Vicuna-13b, where \method outperforms original prompts on all tasks in zero-shot and few-shot.
% In zero-shot, \method increases the accuracy from $0.41$ to $0.90$ on Sum, from $0.40$ to $0.71$ on Sentiment Analysis, and from $0.56$ to $0.76$ on Cause Selection. 
We also explore the effects of \method on enhancing truthfulness and informativeness on TruthfulQA \cite{lin2021truthfulqa} dataset.
As shown in \cref{tb-truthqa-result}, \method increases the truthfulness on ChatGPT from $0.75$ to $0.87$, Vicuna-13b from $0.77$ to $1.0$ and T5\footnote{In the sequel, we use T5 to denote Flan-T5-large for brevity.} from $0.54$ to $0.77$.
\method also improves the informativeness on ChatGPT from $0.53$ to $0.94$ and T5 from $0.42$ to $0.48$. 

Finally, we offer initial insights on why \method works for \llms by computing the input attention contributions of emotional stimulus to the final outputs, as shown in \cref{tb-word-importance}.
To explore \method in more aspects, we conduct a human study to access other metrics of \llms' outputs, such as clarity, relevance (with the question), depth, structure and organization, supporting evidence and engagement (with humans, \cref{tb-human-study}).

To sum up, this paper makes the following contributions:
\begin{enumerate}
\setlength\itemsep{0em}
    \item We take the first step towards leveraging psychological science to enhance \llms by proposing \method, which is a simple yet effective approach.
    \item Extensive experiments on various tasks show the significant improvement brought by \method in task performance, truthfulness, and informativeness.
    \item We further provide insightful analysis to analyze the rationale behind \method, showing inspiration from both AI and social sciences.
\end{enumerate}

\section{Background}


\subsection{Psychology on Emotion Study}

Emotional intelligence is often defined as a set of four interrelated abilities focused on the processing of emotional information.
Those four abilities include perceiving emotions, using emotions to facilitate cognitive activities, understanding emotions, and managing emotions in oneself and other people \cite{salovey2009positive}.
The embodiment of emotion consists of perception, reflexes, cognition, and behavior, which are affected by both internal and external causes \cite{salovey2009positive}.
Additionally, emotions have a prominent influence on many aspects \cite{russell2003core}.
\cite{lerner2015emotion} shows that emotions is composed of potent, pervasive, predictable, sometimes harmful, and sometimes beneficial drivers of decision-making. \cite{ohman2001emotion} proves that emotions can drive attention.
The importance of emotion is also explored in other areas such as education \cite{pekrun2002academic} and competitive sports \cite{lazarus2000emotions}.

As said in \cite{koole2009psychology}, emotion regulation is targeted at satisfying hedonic needs, supporting specific goal pursuits, and facilitating the global personality system by adjusting emotion.
It shows up as one of the most influential processes in cognition and emotion \cite{koole2009psychology}.
To make emotion regulation come into play, many researchers propose different approaches.
Some attempt to regulate emotions using social effects, such as Social Identity theory \cite{hogg2016social,turner1986significance}.
Some focus on motivation and self-regulation, such as Social Cognition theory \cite{fiske1991social,luszczynska2015social} and positive emotions \cite{fredrickson2001role,salovey2009positive}.
There are plenty of mature theories on emotion regulation, and a lot of those have been applied in various aspects, such as enhancing students' success in education \cite{miltiadou2003applying} and health promotion \cite{bandura1998health}.

\subsection{Large Language Models}

\llms show their great potential in many aspects, such as coding, math-problem solving, in-context learning and language understanding, which are regarded as important steps towards AGI. There have been emerging approaches proposed to enhance the performance of \llms w.r.t. prompt engineering. \cite{madaan2023self} rely on LLMs' self-refine ability and iterative refine the answer with self-feedback. \cite{yao2022react,abs-2305-10601,Wei0SBIXCLZ22,kojima2022large} achieve significant improvements on many tasks based on in-context learning.
Although those approaches have awesome performances, they might not be generally applicable to all \llms either due to their complexity or reliance on certain external models.
Different from them, we explore the usage of emotional intelligence on \llms, which is simple, effective, and general.

\section{\method}


\subsection{Motivation}

As powerful agents, the great capabilities of current \llms have shown competitive performance compared to humans, which inspires to explore whether emotional intelligence can also work on \llms.
Recall that by adding an emotional stimulus to humans, which is always related to expectancy, confidence, social influence and goal, human action will be steered in different directions.
Those emotional stimuli can bring positive effects if well-designed, such as enhancing students' success in education \cite{miltiadou2003applying} and health promotion \cite{bandura1998health}. 

In this paper, we introduce a novel approach, \textbf{\method}, designed to incorporate psychological insights to improve the effectiveness of \llms. As illustrated in \cref{fig-motivation}, the implementation of \method is remarkably straightforward, requiring only the addition of emotional stimuli to the initial prompts. The key question remains to identify which emotional stimuli should be utilized for this purpose.

% Figure environment removed

% % Figure environment removed

\subsection{Taking inspiration from Psychology}

% Prompt engineering is an easy but effective approach to enhance the performance of LLMs. To steer LLMs to perform better, many works have been done from different prospects, like chain-of-thought\cite{Wei0SBIXCLZ22}, in-context learning\cite{MinLHALHZ22} and tree-of-thought\cite{abs-2305-10601}. Different from them, we try to enhance the performance of LLMs via emotional stimulus. According to three famous psychology theories: Social Identity theory, Cognitive Emotion Regulation and Social Cognition theory, which have been applied to many fields and shown their effectiveness and feasibility\cite{miltiadou2003applying,bandura1998health}, we design 11 emotional stimulus. 

As shown in \cref{fig-method} (left), to determine the optimal emotion stimulus, we take inspiration from three types of well-established psychology theories.


\begin{enumerate}
\setlength\itemsep{0em}
\item \textbf{Social Identity theory} is first presented by Henri Tajfel and John Turner in the 1970s. It makes it clear that individuals want to establish a positive social identity via maintaining their groupâ€™s favorable social standing over that of relevant out-groups. An individual's sense is based on their group membership and tries to maintain or upgrade their self-esteem and value in society \cite{hogg2016social,turner1986significance}.
Based on this theory, we design some emotional stimuli, such as ``EP\_02'', ``EP\_03'', ``EP\_04'' and ``EP\_05''. In those stimuli, acting as a teammate, we emphasize the importance of the task and promote its value to enhance the performance of LLMs.
\item \textbf{Social Cognition theory} is another important theory that addresses such processes as motivation and self-regulation. The key point is that people seek to develop the sense of agency to exert a large degree of control over important events in their lives \cite{fiske1991social,luszczynska2015social}. 
Self-efficacy, outcome expectations, goals, and self-evaluations of progress are all the influential variables that could influence one's sense of agency  \cite{luszczynska2015social}.
We design several emotional stimuli based on this theory. ``EP\_01'' is based on self-evaluations of progress theory in SCT, which encourages LLMs to judge themselves. ``EP\_02'', ``EP\_03'' and ``EP\_04'' tell our expectations and set a goal for LLMs.
\item As mentioned in \cite{baranczuk2019five}, \textbf{Cognitive Emotion Regulation theory} tells that individuals with deficits in emotion regulation skills are prone to compulsive behavior and to following maladaptive coping strategies. We try to improve emotion regulation skills via some positive implications, such as building up confidence and emphasizing the goal. To regulate emotion into a positive direction, we use ``\prompt{believe in your abilities}'', ``\prompt{excellent}'', ``\prompt{success}'', ``\prompt{outstanding achievements}'', ``\prompt{take pride in}'' and ``\prompt{stay determined}'' in ``EP\_07'', ``EP\_08'', ``EP\_09'', ``EP\_10'' and ``EP\_11'', respectively.
Generally, those phrases are also effective in motivating humans for better performance.
\end{enumerate}
  
As shown in \cref{fig-method} (right), our designed emotional stimulus can be classified into two categories where one tries to regulate emotion by social influence, such as group membership and others' opinions, and the other focuses on self-esteem and motivations.
Choose one of those emotional stimuli and add it to the original prompt; then, it will regulate LLMs' emotions and arouse their inner power.

\input{tables/tb-testsets}




\section{Experiments}


\subsection{Setup}
We evaluate \method in zero-shot and few-shot learning on eight instruction induction tasks \cite{honovich2022instruction}.
Those tasks span different facets of language understanding: from simple phrase structure to similarity and causality identification.
To access the generalization performance of \method, we evaluate them on 4 \llms: ChatGPT, Vicuna-13b \cite{abs-2302-11665}, Bloom \cite{abs-2211-05100} and Flan-T5-Large \cite{abs-2210-11416}.
For ChatGPT, we use gpt-3.5-turbo model and set the temperature as 0.7.
For other \llms, we conduct all experiments with their default settings.
In zero-shot experiments, the emotional stimulus can be simply added to the original prompt to construct \method.
And for few-shot in-context learning, we evaluate the same prompts as those in zero-shot, and randomly sample 5 input-output pairs as in-context demonstrations, which are added after prompts.
The template can be described as ``prompt/emotion prompt + demonstration''. 

\textbf{Baselines} We compare our \method with two baselines, one is the original zero-shot and few-shot prompts from Instruction Induction \cite{honovich2022instruction}, which is designed by humans, the other is Zero-shot-CoT \cite{kojima2022large}. To our knowledge, it is the simplest and efficient approach in zero-shot prompt engineering.

\textbf{Datasets and Tasks}
We access the efficiency of our approach on eight tasks of Instruction Induction \cite{honovich2022instruction}: Sentiment Analysis (SA), Sentence Similarity (SS), Cause Selection (CS), Sum, Word in Context (WC), Starting With (SW), Larger Animal (LA) and First Letter (FL).
Details on those tasks can be found in \cref{tb-dataset}. For each task, we test on $100$ samples, except for Cause Selection, including $50$ examples in total. And we choose in-context demonstrations for few-shot learning from the remaining part of the data.


\subsection{Our Emotional Prompts}
\input{tables/tb-emotion-stimulus}
Based on three widely known psychology theories, we design 11 emotional stimuli to enhance the performance of LLMs; details are shown in \cref{tb-emotion-stimulus}. As shown in \cref{fig-method}, the emotion stimulus 02$\sim$05 stem from Social Identity theory \cite{hogg2016social,turner1986significance}, 07$\sim$11 are derived from Cognitive Emotion Regulation theory \cite{baranczuk2019five}, and And 01$\sim$04 conform to Social Cognition theory \cite{fiske1991social,luszczynska2015social}.
Note that ``EP\_06'' is a compound stimulus, which combines ``EP\_01'', ``EP\_02'' and ``EP\_03''.
% \zkj{Have we compared the results of two main stimulus categories? Which one is more effective?}

\subsection{Main Results}

\input{tables/tb-all-results}

The main results are shown in \cref{tb-all-results}.
We see that our \method achieves comparable or even better performance than the original one.
We now list the specific findings on each LLM:
\begin{enumerate}
\setlength\itemsep{0em}
\item \textbf{ChatGPT}: \method achieves better or comparable performance on all tasks in zero-shot and few-shot settings. The accuracy increased over 10\% on 4 and 5 tasks in zero-shot and few-shot scenarios, respectively. 
\item \textbf{Flan-T5-Large}: \method outperforms original prompt and Zero-Shot-CoT on 6 tasks and 5 tasks in zero-shot scenario. For few-shot setting, it outperforms the original prompt and Zero-Shot-CoT on six tasks, and all prompts fails on the last two tasks.
\item \textbf{Vicuan-13b}: \method performs better on all tasks in zero-shot and few-shot settings. Specifically for zero-shot setting, \method increases the accuracy on Sum from 0.41 to 0.90, on Sentiment Analysis from 0.40 to 0.71, and on Cause Selection from 0.56 to 0.76.
\item \textbf{Bloom}: \method outperforms original prompts on all tasks. The average values of \method are higher than original prompts on 7/8 tasks in few-shot.

\end{enumerate}
% \input{tables/tb-chatgpt-result-zero}

% \input{tables/tb-chatgpt-result-few}

% \input{tables/tb-t5-result-zero}

% \input{tables/tb-t5-result-few}

% \input{tables/tb-vicuna-result-zero}

% \input{tables/tb-vicuna-result-few}

% \input{tables/tb-bloom-result-zero}

% \input{tables/tb-bloom-result-few}

\subsection{Truthfulness and Informativeness}
\input{tables/tb-word-importance}
\input{tables/tb-truthfulqa-result}
We employ \method on TruthfulQA\cite{lin2021truthfulqa} to explore the effect of \method on truthfulness and informativeness. The benchmark has 817 questions that span 38 categories, including health, law, finance and politics. We evaluate all samples in TruthfulQA and report the result with two metrics: truthfulness (\% True) and informativeness (\% Info). Those results can be accessed by their fine-tuned GPT-judge and GPT-info, which have been proven to align with human prediction over 90\% of the time.

\cref{tb-truthqa-result} shows the results on ChatGPT, Vicuna-13b and T5. \method increases the truthfulness on ChatGPT from 0.75 to 0.87, Vicuna-13b from 0.77 to 1.0 and T5 from 0.54 to 0.77. \method also improves the informativeness on ChatGPT from 0.53 to 0.94 and T5 from 0.42 to 0.48. 

\subsection{Effect of More Emotional Stimulus}
As one or more stimuli may regulate human action, and more stimuli sometimes are more efficient, we explore the effect of more emotional stimuli on LLMs. We randomly combine some emotional stimuli and experiment on ChatGPT; results are shown in \cref{tb-more-stimulus}. Our findings are listed below:
\begin{enumerate}
    \item \textbf{More emotional stimuli generally lead to better performance in most cases.} The second group and the third group explore the effect of adding ``EP\_01'', showing that the third group performs better than the second group in most cases.
    \item \textbf{Combined stimulus can bring little or no benefit when sole stimulus already achieves a good performance.} The combination ``EP\_01'' + ``EP\_04'' gets a high score in most tasks and does not improve significantly or even decrease when we add more stimulus to it, such as stimulus 06$\sim$09.
\end{enumerate}

\input{tables/tb-more-stimulus}


\section{Analysis}


\subsection{Why \method Works?}

% Figure environment removed

% \input{tables/tb-word-importance}

\method shows great improvements on eight tasks not only in accuracy but also in truthfulness and informativeness.
Why does \method work?
We explain this by visualizing the emotional stimulus's input attention contributions to the final outputs as proposed in \cite{zhu2023promptbench}.
Because T5-large is open-sourced and relatively small, we chose it as our experimental LLM and accessed every word's contribution based on the gradient norm.
The experiment is conducted on a Sentiment Analysis task. We compute prompts' contributions on every test sample and use the average value to represent their importance.

\cref{tb-word-importance} shows every word's contribution to the final result, and the color depth represents their importance. Our key findings are as follows:
\begin{enumerate}
    \item \textbf{Emotional stimulus can enhance original prompts' representation.} Original prompt ``Determine whether a movie review is positive and negative.'' have deeper color in \method, especially in "EP\_01", "EP\_03", "EP\_06", "EP\_07", "EP\_08", "EP\_09" and "EP\_10". This means emotional stimulus can enhance the attention of original prompts.
    \item \textbf{Positive words make more contributions.} In our designed emotional stimulus, some positive words play a more important role, such as ``confidence'', ``sure'', ``success'' and ``achievement''. Based on this finding, we summarize positive words' contributions and their total contributions to the final result on eight tasks. As shown in \cref{fig-word-importance}, the contributions of positive words pass 50\% on four tasks, even approach 70\% on two tasks.
\end{enumerate}

\subsection{Human Study}
To explore the effect of \method in more aspects, we conduct a human study to access other metrics of LLMs' outputs, such as clarity, relevance (with the question), depth, structure and organization, supporting evidence and engagement (with humans), which are designed based on Elaboration Likelihood model \cite{petty2011elaboration}, Cognitive Load Theory \cite{sweller2011cognitive} and Schema theory \cite{mcvee2005schema}. We prepared 40 problems from TruthfulQA \cite{lin2021truthfulqa} and add emotional stimulus "EP\_04","EP\_06","EP\_11" to original prompts, then we got the outputs of the original prompt and three \method via ChatGPT. Four volunteers rate four outputs on a scale of 1 - bad to 5 - good on six metrics for 40 questions. We report the average values in \cref{tb-human-study}. Results show that \method access better performance on clarity, depth, structure, and organization, supporting evidence and engagement.

\input{tables/tb-human-study}

\section{Conclusion and Limitation}
\label{sec13}

We have proposed \method, which enhances the performance of LLMs via emotional stimulus. Our method is not only simple but also powerful; \ method achieves awesome performance on eight tasks with four LLMs. Our study to explore the emotional intelligence of LLMs might encourage the community to further examine the emotional power of LLMs and other emotional stimuli.

This work has several limitations.
First, we only experiment with four \llms and conduct experiments in several tasks with few test examples, which are limited. Thus, our conclusions about emotion stimulus can only work on our experiments and any \llms and datasets out of the scope of this paper might not work with emotion stimulus. Second, the emotional stimulus proposed in this paper may not be general to other tasks, and researchers may propose other useful replacements for your own tasks. Finally, since results may change due to the version change of ChatGPT, we cannot ensure its reproducibility.

\appendix

\section*{Ethical Statement}
In this study, we aim to assess the ability of LLMs to understand emotion. While various ethical issues are linked with LLMs, such as bias, the propagation of harmful content (e.g.,  misinformation), privacy issues, and their broader societal implications, it's evident that LLMs will become increasingly prevalent, influencing both the research community and the general population. Therefore, it is crucial for future work research to further explore and evaluate these models, thereby enhancing our understanding of LLMs' capabilities and identifying their limitations.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai23}

\end{document}

