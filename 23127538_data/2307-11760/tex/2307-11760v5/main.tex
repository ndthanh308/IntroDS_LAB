\documentclass[letterpaper,10pt]{article}
\usepackage[margin=1in]{geometry} % decreases margins

% \usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[T1]{fontenc}
% \usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{amssymb,amsfonts}%
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}%
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}%
\usepackage{makecell}
\usepackage[capitalise]{cleveref}
\usepackage{lineno}
\usepackage[title]{appendix}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{xspace}
\usepackage{todonotes}
\usepackage{xcolor}
\usepackage{color}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{todonotes}
\usepackage{url}
\usepackage{bbding}

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%


\newcommand{\wjdd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(WJD): #1}}
\newcommand{\wjd}[1]{{\color{cyan}{[(WJD): #1]}}}
\newcommand{\hwx}[1]{{\color{orange}{[(HWX): #1]}}}
\newcommand{\lc}[1]{{\color{pink}{[(LC): #1]}}}
\newcommand{\zkj}[1]{{\color{purple}{[(ZKJ): #1]}}}
% --------comment style -------
\newif{\ifhidecomments}
 \hidecommentsfalse 
% \hidecommentstrue
\ifhidecomments 
    \newcommand{\janice}[1]{}  
\else 
    \newcommand{\janice}[1]{\textbf{\sffamily{\textcolor{purple}{[Janice: #1 ]}}}}  
\fi


\newcommand{\method}{EmotionPrompt\xspace}
\newcommand{\llms}{LLMs\xspace}
\newcommand{\prompt}[1]{{\ttfamily #1}}
\newcommand{\ep}[1]{{\small {\sffamily EP#1}}}

\begin{document}

\title{Large Language Models Understand and Can Be Enhanced by Emotional Stimuli}

\author{Cheng Li$^1$, Jindong Wang$^2$\thanks{Corresponding author: Jindong Wang (jindong.wang@microoft.com).}, Yixuan Zhang$^3$, Kaijie Zhu$^2$, Wenxin Hou$^2$, Jianxun Lian$^2$,\\ Fang Luo$^4$, Qiang Yang$^5$, Xing Xie$^2$\\
$^1$Institute of Software, CAS  \quad $^2$Microsoft  \quad $^3$William\&Mary\\  $^4$Department of Psychology, Beijing Normal University  \quad $^5$HKUST
}

\date{}

\maketitle

\abstract{
Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of \llms to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various \llms, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that \llms have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call ``\method'' that combines the original prompt with emotional stimuli), e.g., \textbf{8.00\%} relative performance improvement in Instruction Induction and \textbf{115\%} in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that \method significantly boosts the performance of generative tasks (\textbf{10.9\%} average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why \method works for \llms and the factors that may influence its performance. We posit that \method heralds a novel avenue for exploring interdisciplinary social science knowledge for human-\llms interaction.}

% Figure environment removed



\section{Introduction}
\label{sec1}

Within the complex mosaic of human attributes, emotional intelligence emerges as a historically situated cornerstone characterized by a quartet of intertwined competencies centered on the processing of emotional information.
Emotional intelligence denotes the capacity to adeptly interpret and manage emotion-infused information, subsequently harnessing it to steer cognitive tasks, ranging from problem-solving to behaviors regulations \cite{salovey2009positive}.
Emotions manifest through a confluence of reflexes, perception, cognition, and behavior, all of which are subject to modulation by a range of internal and external determinants \cite{salovey2009positive, russell2003core}.
For instance, within the realm of decision-making, emotions emerge as powerful, ubiquitous, consistent influencers, wielding effects that can swing from beneficial to detrimental \cite{lerner2015emotion}.
Studies further underscore the importance of emotions in steering attention \cite{ohman2001emotion}, academia \cite{pekrun2002academic}, and competitive athletic arena~\cite{lazarus2000emotions}.
Other studies show that emotion regulation \cite{koole2009psychology} can influence human's problem-solving performance as indicated by \emph{self-monitoring} \cite{ickes2006self}, \emph{Social Cognitive} theory \cite{fiske1991social,luszczynska2015social}, and the role of \emph{positive emotions} \cite{fredrickson2001role,salovey2009positive}.
Owing to its impact on human behaviors, emotion regulation theories have been applied across various domains, including educational settings for promoting students' success \cite{miltiadou2003applying} and health promotion initiatives \cite{bandura1998health}. 

% The exploration of emotion regulation provides a better understanding of ourselves, which represents the skills and strategies used to manage and modulate emotional experiences effectively  and it emerges as one of the most far-ranging and influential processes at the interface of cognition and emotion.


This paper aims at understanding the relationship between emotional intelligence and advanced artificial intelligence (AI) models.
As one of the most promising research endeavor towards artificial general intelligence\footnote{AGI is the ultimate goal in AI research and LLMs are widely considered as an important milestone towards this goal.}, the recently emerging large language models (\llms) have shown remarkable performance in a wide spectrum of tasks, such as reasoning, natural language understanding and generation, and problem-solving in STEM.
A recent study~\cite{bubeck2023sparks} claimed that \llms show great potential towards AGI by letting GPT-4 conduct a series of challenging tasks designed by humans.
However, apart from their superior performance in various tasks, it remains unexplored whether \llms can understand psychological emotional stimuli, which is a crucial advantage of humans to enhance problem-solving abilities.
Therefore, we ask the question---are LLMs well aligned with human emotional intelligence?
Many researchers have achieved significant advancements in multiple tasks by employing in-context learning techniques \cite{garg2023transformers,dong2023survey,yao2022react,abs-2305-10601,Wei0SBIXCLZ22,kojima2022large}.
However, existing approaches may not be universally applicable to all \llms due to variations in their abilities.
While recent work \cite{wang2023emotional} has shown that \llms can understand emotions, it did not evaluate the influence of emotional intelligence to \llms, that is, can emotional intelligence play a key role in enhancing the abilities of \llms?


\textbf{Our approach.}
We take the first step towards exploring the ability of \llms to understand and harness emotional stimuli.
Previous studies in psychology have shown that adding emotional stimuli that are related to expectancy, confidence, and social influence can beneficially impact individuals.
Real-world applications of this phenomenon include enhancing student success in education \cite{miltiadou2003applying} and promoting health \cite{bandura1998health} by using encouraging and positive words.
Drawing from such psychology phenomena, we propose \textbf{\method}---a straightforward yet effective approach to explore the emotional intelligence of \llms.
Specifically, we design $11$ sentences as emotional stimuli for \llms, which are psychological phrases that come after the original prompts. For instance, \cref{fig-motivation} shows an example of using one emotional stimulus, ``\prompt{This is very important to my career}'' at the end of the original prompts to enhance the performance of different \llms. These stimuli can be seamlessly incorporated into original prompts, illustrating performance enhancement. %and demonstrate this improvement.
% These emotional stimuli are simple 




\textbf{Our key findings and discussions.}
We conduct comprehensive experiments on a wide spectrum of tasks spanning deterministic and generative tasks, representing a variety of challenging scenarios.
For deterministic tasks that can be evaluated using standard metrics, we conduct experiments on $24$ Instruction Induction tasks \cite{honovich2022instruction} and $21$ curated BIG-Bench tasks \cite{suzgun2022challenging} using various \llms, including Flan-T5-Large \cite{abs-2210-11416}, Vicuna \cite{abs-2302-11665}, Llama 2 \cite{touvron2023llama}, BLOOM \cite{abs-2211-05100}, ChatGPT \cite{chatgpt}, and GPT-4 \cite{openai2023gpt4}.
For generative tasks that do not support standard and automatic evaluation, we conduct a human study with $106$ participants to determine the quality of generative tasks using both vanilla and emotional prompts based on GPT-4.
The results are promising:
our standard experiments show that \llms possess emotional intelligence and can be enhanced by emotional stimuli with \textbf{8.00\%} relative performance improvement in Instruction Induction and \textbf{115\%} in BIG-Bench;
our human study demonstrates that the emotional prompts significantly boost the performance of generative tasks (\textbf{10.9\%} average improvement in terms of performance, truthfulness, and responsibility metrics).

% We then move forward to study the rationales behind \method.
Additionally, we discuss lessons and insights derived from our findings (see Section~\ref{sec-dis}). For instance, we explore %we present initial insights on 
why \method is effective for \llms by analyzing the effects of emotional stimuli on the final outputs through input attention, as shown in \cref{tb-word-importance}.
% computing the input attention contributions of emotional stimulus to the final outputs,
Our results demonstrate that emotional stimuli actively contribute to the gradients in \llms by gaining larger weights, thus benefiting the final results through enhancing the representation of the original prompts.
We further conducted ablation studies to explore the factors influencing the effectiveness of \method, such as model sizes and temperature. Our findings provide inspiration for potential users.
Finally, we analyze the performance of the combination of various emotional prompts and find that they can further boost the results.
Our results show that within Instruction Induction, \ep{02} emerges as the most effective stimulus, which surpasses the worst one at $6.06$\%, while in BIG-Bench, \ep{06} is the best.
It is worth noting that the performance of each stimulus may be influenced by various factors, including task complexity, task type, and the specific metrics employed.

\textbf{Contributions.}
This paper makes the following contributions:
\begin{enumerate}
\setlength\itemsep{0em}
    \item We propose \method to thoroughly study the emotional intelligence of large language models. Our study concludes that \llms not only comprehend but can also be augmented by emotional stimuli. 
    % understand and can be enhanced by emotional stimulus.
    \item We conduct extensive experiments on both deterministic and generative tasks in both standard and human evaluations. Results show the significant improvement brought by \method in task performance, truthfulness, and informativeness.
    \item We provide an in-depth analysis focused on the rationales behind \method, shedding light on potential implications for both AI and social science disciplines. 
    %showing inspiration for both AI and social sciences.
\end{enumerate}

% \section{Background}


% \subsection{Psychology on Emotion Study}

% Emotional intelligence is contextualized historically and defined as a set of four interrelated abilities focused on the processing of emotional information. Those four abilities contain perceiving emotions, using emotions to facilitate cognitive activities, understanding emotions, and managing emotions in oneself and other people \cite{salovey2009positive}. The Embodiment of emotion contains reflexes, perception, cognition, and behavior, which various internal and external factors can influence \cite{salovey2009positive}. Furthermore, emotions exert a significant impact on multiple facets of human life \cite{russell2003core}. For example, in decision-making, emotions have been identified as potent, pervasive, predictable, and at times, consequential factors that can either be detrimental or beneficial \cite{lerner2015emotion}. Research has also demonstrated that emotions are crucial in directing attention \cite{ohman2001emotion}. The significance of emotions is also evident in domains such as education \cite{pekrun2002academic} and competitive sports \cite{lazarus2000emotions}.

% \janice{Missing transitioning sentences from the previous paragraph to emotion regulation.}
% As said in \cite{koole2009psychology}, Emotion regulation is a crucial ability in emotional intelligence, targeted at satisfying hedonic needs, supporting specific goal pursuits, and facilitating the global personality system by adjusting emotion. It emerges as one of the most far-ranging and influential processes at the interface of cognition and emotion. Researchers have proposed various approaches to facilitate emotion regulation. Some strategies involve leveraging social factors, as elucidated in the Social Identity theory \cite{hogg2016social,turner1986significance}. Others focus on motivation and self-regulation, as exemplified in Social Cognitive theory \cite{fiske1991social,luszczynska2015social}, and the role of positive emotions \cite{fredrickson2001role,salovey2009positive}. Numerous well-established theories on emotion regulation have been applied across various domains, including educational settings for promoting students' success \cite{miltiadou2003applying} and health promotion initiatives \cite{bandura1998health}. While prior work has utilized emotion regulation to enhance human's performance and achieve great success, almost no work apply this theory on \llms, which are arguably regarded as an important milestone in artificial general intelligence (AGI).
% \janice{This Subsection 2.1 provides some good background, but it doesn't tell the readers WHAT IS THE RESEARCH GAP. In order to highlight the research gap, you can say something like ``While prior work has explored xyz, very little work examines <ABC>. Therefore, our work seeks to address this <ABC> research gap + by doing xxx (a few words to summarize).. } 

% \subsection{Large Language Models}

% Large Language Models demonstrate tremendous potential across various domains, including coding, math problem solving, in-context learning and language understanding, which align with the concept of Artificial General Intelligence (AGI). Leveraging their robust capabilities, numerous approaches have been proposed to enhance the performance of \llms. Madaan et al. \cite{madaan2023self} rely on the self-refinement ability of \llms, iteratively refining answers through self-feedback.  Certain \llms may lack the capability for self-refinement or in-context learning. Different from them, we explore the usage of emotional intelligence on \llms, which is simple, effective, and general.

\section{Results}

In this section, we begin by outlining the rationale behind designing emotional stimuli (Sec.~\ref{sec-results-design}), and then describe the standard experiment and results in Sec.~\ref{sec-results-standard}. Subsequently, we present our human study and findings in Sec.~\ref{sec-results-humanstudy}.
Finally, we conduct further study on evaluating the truthfulness and informativeness of \method in Sec.~\ref{sec-results-truth}.

\subsection{Designing emotional stimuli}
\label{sec-results-design}
% \zkj{In Fig. 2, the terms are represented as EP\_01 and EP\_02, whereas in the paper, they are referred to as EP01 and EP02.}
We design our \method to understand \llms' behavior on emotional stimuli.
As illustrated in \cref{fig-motivation}, the implementation of \method is remarkably straightforward and requires only the addition of emotional stimuli to the initial prompts.
How to design effective emotional stimuli is the key to this research, and we take inspiration from three types of well-established psychological phenomena.
Details are shown in \cref{fig-method} (left).
% Since emotional stimuli may have a heterogeneous effect in different scenarios and it is difficult to identify which one works better on \llms, 

% Figure environment removed

\begin{enumerate}
\setlength\itemsep{0em}

% \item \textbf{Confidence Ratings} are often used to evaluate the metacognitive processes that occur during reasoning and problem-solving. When individuals make decisions or judgments, they can often provide a confidence rating, indicating how sure they are about that judgment. These ratings can provide insights into a person's awareness of their own knowledge or decision-making processes. Besides, metacognitive interventions are effective at improving performance in many aspects, such as education \cite{azevedo2018using} and information searching \cite{gagniere2012metacognitive}. 
% We apply this phenomenon in the design of ``EP\_01'', and ``EP\_03''$\sim$``EP\_05'', In those stimuli, we ask \llms to give a confidence score and return a certain answer to us, which may enhance the metacognitive processes.

\item \textbf{Self-monitoring}, a concept extensively explored within the domain of social psychology, refers to the process by which individuals regulate and control their behavior in response to social situations and the reactions of others~\cite{ickes2006self}. High self-monitors regulate their behaviors using social situations and interpersonal adaptability cues, engaging in self-presentation and impression management \cite{ickes2006self}.

In our work, we apply self-monitoring in \ep{01}$\sim$\ep{05}. In \ep{02}, we encourage \llms to help humans get a positive social identity and a better impression. In \ep{01}, and in \ep{03}$\sim$\ep{05}, we ask \llms to monitor their performance via providing social situations.

\item \textbf{Social Cognitive Theory}, a commonly used theory in psychology, education, and communication, stresses that learning can be closely linked to watching others in social settings, personal experiences, and exposure to information~\cite{bandura2013health}. 
% addresses such processes as motivation and self-regulation. 
The key point is that individuals seek to develop a sense of agency for exerting a large degree of control over important events in their lives \cite{fiske1991social,luszczynska2015social, bandura2013health}. The influential variables affecting one's sense of agency are self-efficacy, outcome expectations, goals, and self-evaluations of progress \cite{luszczynska2015social}.
Self-efficacy enhances performance via increasing the difficulty of self-set goals, escalating the level of effort that is expended, and strengthening persistence \cite{bandura2012functional,bandura2003negative}. Prior work has supported the idea that self-efficacy is an important motivational construct affecting choices, effort, persistence, and achievement \cite{schunk2021self}. When learning complex tasks, high self-efficacy influences people to strive to improve their assumptions and strategies \cite{heslin2006self}.

Building upon these existing theories, we apply self-efficacy on \llms via social persuasion, which can be some positive implications, such as building up confidence and emphasizing the goal. To regulate emotion into a positive direction, we use ``\prompt{believe in your abilities}'', ``\prompt{excellent}'', ``\prompt{success}'', ``\prompt{outstanding achievements}'', ``\prompt{take pride in}'' and ``\prompt{stay determined}'' in \ep{07}$\sim$\ep{11}, respectively.
Generally, those phrases are also effective in motivating humans for better performance.

\item \textbf{Cognitive Emotion Regulation Theory} 
%describes that individuals with deficits in emotion regulation skills are prone to compulsive behavior and to following maladaptive coping strategies~\cite{baranczuk2019five}. Utilizing techniques from this theory, such as reappraisal, individuals may view challenges more optimally or unbiasedly. This change in perspective aids in sustaining motivation and promoting continued effort despite obstacles. 
suggests that people lacking emotion regulation skills are more likely to engage in compulsive behavior and use poor coping strategies~\cite{baranczuk2019five}. Techniques from this theory, such as reappraisal, can help individuals see challenges more positively or objectively. This shift in viewpoint helps maintain motivation and encourages ongoing effort, even when facing obstacles.

According to this theory, we have crafted numerous emotional stimuli, exemplified by designations such as \ep{03} $\sim$ \ep{05} and \ep{07}. Within these stimuli, we aim to stimulate the reappraisal skills of \llms by incorporating pivotal terms, such as ``\prompt{sure}'' and ``\prompt{take another look}''.

\end{enumerate}
% \begin{enumerate}
% \setlength\itemsep{0em}
% \item \textbf{Social Identity theory} is first presented by Henri Tajfel and John Turner in the 1970s. It makes it clear that individuals want to establish a positive social identity via maintaining their group’s favorable social standing over that of relevant out-groups. An individual's sense is based on their group membership and tries to maintain or upgrade their self-esteem and value in society \cite{hogg2016social,turner1986significance}.
% Based on this theory, we design some emotional stimuli, such as ``EP\_02'', ``EP\_03'', ``EP\_04'' and ``EP\_05''. In those stimuli, acting as a teammate, we emphasize the importance of the task and promote its value to enhance the performance of LLMs.
% \item \textbf{Social Cognitive theory} is another important theory that addresses such processes as motivation and self-regulation. The key point is that people seek to develop the sense of agency to exert a large degree of control over important events in their lives \cite{fiske1991social,luszczynska2015social}.
% Self-efficacy, outcome expectations, goals, and self-evaluations of progress are all the influential variables that could influence one's sense of agency  \cite{luszczynska2015social}.
% We design several emotional stimuli based on this theory. ``EP\_01'' is based on self-evaluations of progress theory in SCT, which encourages LLMs to judge themselves. ``EP\_02'', ``EP\_03'' and ``EP\_04'' tell our expectations and set a goal for LLMs.
% \item As mentioned in \cite{baranczuk2019five}, \textbf{Cognitive Emotion Regulation theory} tells that individuals with deficits in emotion regulation skills are prone to compulsive behavior and to following maladaptive coping strategies. We try to improve emotion regulation skills via some positive implications, such as building up confidence and emphasizing the goal. To regulate emotion into a positive direction, we use ``\prompt{believe in your abilities}'', ``\prompt{excellent}'', ``\prompt{success}'', ``\prompt{outstanding achievements}'', ``\prompt{take pride in}'' and ``\prompt{stay determined}'' in ``EP\_07'', ``EP\_08'', 
% ``EP\_09'', ``EP\_10'' and ``EP\_11'', respectively.
% Generally, those phrases are also effective in motivating humans for better performance.
% \end{enumerate}

Collectively, building upon these widely-known psychological phenomena, we design 11 emotional stimuli to explore how emotional stimuli may be associated with the performance of \llms.
As shown in \cref{fig-method}, the emotion stimuli 01$\sim$05 are derived from self-monitoring \cite{ickes2006self}, 07$\sim$11 conform to Social Cognitive theory \cite{fiske1991social,luszczynska2015social}. \ep{03}$\sim$\ep{05} and \ep{07} are derived from Cognitive Emotion Regulation theory \cite{baranczuk2019five}. 
To explore if more emotional stimuli can work better, we first built a compound stimulus (\ep{06}), which combines \ep{01}$\sim$\ep{03}, and more discussion on this topic can be found in \cref{sec-discuss-more}.

  
As shown in \cref{fig-method} (right), our designed emotional stimuli can be classified into two categories one tries to regulate emotion by social influence, such as group membership and others' opinions, and the other focuses on self-esteem and motivations.
% Choose one of those emotional stimuli and add it to the original prompt; then, it will regulate LLMs' emotions and arouse their inner power.
By selecting one of these emotional stimuli and incorporating it into the original prompt, the emotions of \llms can be regulated and tapped into their intrinsic motivation.

% Psychological studies by Mayer and Schneider \cite{mayer2008human,schneider2023emotional} demonstrate that psychology can facilitate precise reasoning and augment cognitive processes. Hess et al. \cite{hess2011enhancing,hess2013applying} provide evidence supporting the notion that emotional intelligence skills can enhance decision-making abilities. Additionally, the influence of psychology on language is illustrated in the works of Sucaromana and Pishghadam \cite{sucaromana2012contribution,pishghadam2009quantitative}. So can psychology also play a role in those aspects when applied to \llms?
% Those emotional stimuli can bring positive effects if well-designed, such as enhancing students' success in education \cite{miltiadou2003applying} and health promotion \cite{bandura1998health}.



% The key question remains to identify which emotional stimuli should be utilized for this purpose.



% % Figure environment removed

% \subsection{Taking inspiration from Psychology}

% Prompt engineering is an easy but effective approach to enhance the performance of LLMs. To steer LLMs to perform better, many works have been done from different prospects, like chain-of-thought\cite{Wei0SBIXCLZ22}, in-context learning\cite{MinLHALHZ22} and tree-of-thought\cite{abs-2305-10601}. Different from them, we try to enhance the performance of LLMs via emotional stimulus. According to three famous psychology theories: Social Identity theory, Cognitive Emotion Regulation and Social Cognitive theory, which have been applied to many fields and shown their effectiveness and feasibility\cite{miltiadou2003applying,bandura1998health}, we design 11 emotional stimulus. 

\subsection{Standard experiments and results}
\label{sec-results-standard}

First, we conduct standard experiments to evaluate the performance of \method.
``Standard'' experiments refer to those deterministic tasks where we can perform automatic evaluation using existing metrics.
Specifically, we adopt $24$ tasks from Instruction Induction \cite{honovich2022instruction} and $21$ curated tasks of BIG-Bench \cite{suzgun2022challenging} datasets.
Instruction Induction \cite{honovich2022instruction} is designed to explore the ability of \llms to infer an underlying task from a few demonstrations, which are relatively simple tasks, while BIG-Bench \cite{suzgun2022challenging} focuses on tasks that are considred to be beyond the capabilities of most \llms.
Testing on tasks of varying difficulty can help us evaluate the effectiveness of \method, with an emphasis on various cognitive abilities, including language understanding, reasoning, and decision-making.
The detailed task descriptions are provided in \cref{tb-instruction-induction,tb-bigbench}.

For Instruction Induction, we use accuracy as the metric.
For BIG-Bench, we report the normalized preferred metric defined in \cite{srivastava2023imitation}. Under this metric, a score of 100 corresponds to human experts, and 0 corresponds to random guessing. Note that a model can achieve a score less than 0 if it performs worse than random guessing on a multiple-choice task. 

\subsubsection{Experimental setup}

We assess the performance of \method in zero-shot and few-shot learning on $6$ different \llms: Flan-T5-Large \cite{abs-2210-11416}, Vicuna \cite{abs-2302-11665}, Llama2 \cite{touvron2023llama}, BLOOM \cite{abs-2211-05100}, ChatGPT \cite{chatgpt}, and GPT-4 \cite{openai2023gpt4}.\footnote{For ChatGPT, we utilize gpt-3.5-turbo (0613) and set temperature parameter to $0.7$. For GPT-4 and Llama 2, we set the temperature to $0.7$. The remaining \llms are evaluated using their default settings.}
In zero-shot experiments, we incorporate emotional stimuli into the original prompts to construct \method. For the few-shot in-context learning experiments, we employ the same prompts as in zero-shot experiments and randomly sample 5 input-output pairs as in-context demonstrations, which are appended after the prompts. The template format can be described as ``\textit{prompt/\method + demonstration}''. 

\textbf{Baselines.} We conduct a comparative analysis of our proposed \method with three baseline methods. The first baseline involves utilizing the original zero-shot prompts provided in Instruction Induction \cite{honovich2022instruction} and BIG-Bench \cite{suzgun2022challenging}, which are designed by human experts. The second baseline is Zero-shot-CoT \cite{kojima2022large}, which, to the best of our knowledge, is the simplest and most efficient approach for zero-shot prompt engineering. We also compare \method with APE \cite{zhou2023large} by adding our \method to APE-generated prompts.

% \input{tables/tb-testsets}
% \textbf{Datasets and Tasks.} We assess the efficiency of our approach on $24$ tasks sourced from Instruction Induction \cite{honovich2022instruction} and $21$ tasks sourced from BIG-Bench \cite{suzgun2022challenging}. Those tasks focus on a large range of abilities, including language understanding, causal inference, math problem solving and so on. Details on those tasks can be found in \cref{tb-instruction-induction,tb-bigbench}. 

% \subsection{Our Emotional Prompts}
% \wjd{Maybe this section can focus on introducing \method.}
% \input{tables/tb-emotion-stimulus}





\subsubsection{Results and analysis}

% Figure environment removed

% Figure environment removed

% \wjd{This part needs rewriting after obtaining all results.}
\input{tables/tb-results-ii}
% \input{tables/tb-results-bigbench}
% \input{tables/tb-all-results}

% To make it simple, 
We average experimental results on all tasks in Instruction Induction \cite{honovich2022instruction} and $21$ curved Big-Bench~\cite{suzgun2022challenging} in \cref{tb-results-ii}.
Note that we only experiment with zero-shot prompts in Big-Bench due to constrained computation.
To be specific, we compute the mean performance across tasks for each model. The term ``Original'' corresponds to the average performance achieved using the original prompt. ``Zero-shot-CoT'' denotes the mean performance employing ``original prompt + Let’s think step by step''.
``+Ours (avg)'' is derived by initially calculating the average performance across tasks using \method, which incorporates $11$ emotional stimuli, and subsequently computing the mean performance across these stimuli, while ``+Ours (max)'' is determined by first computing the average performance for each task using \method, then selecting the optimal performance from those stimuli.
% \method enhance the performance of both human-designed prompts and APE-generated prompts on $6$ \llms.

% For zero-shot experiments on Instruction Induction \cite{honovich2022instruction}, \method consistently outperforms both human-designed prompts and APE-generated prompts across all models. Furthermore, in comparison with Zero-shot-CoT \cite{kojima2022large}, \method exhibits enhanced performance on 5 out of 6 \llms, both utilizing human-designed prompts and APE-generated prompts.
% In terms of few-shot experiments on Instruction Induction \cite{honovich2022instruction}, \method outperforms both Zero-shot-CoT and original prompts across all six \llms, irrespective of the employment of human-designed or APE-generated prompts.
% In the zero-shot experiments conducted on Big-Bench~\cite{suzgun2022challenging}, a dataset emphasizing more complex problems, \method consistently outperforms APE-generated prompts across all \llms. Furthermore, it demonstrates superior efficacy over human-designed prompts on five out of the six \llms, and win human-designed prompts on 5 out of 6 \llms. Compared with Zero-shot-CoT \cite{kojima2022large}, \method achieve better performance in all those twelve settings. 

% Based on the above results, 
Below we report our findings:
\begin{enumerate}
    \item \textbf{\method demonstrates consistent improvement in both Instruction Induction and Big-Bench tasks on all \llms.} Specifically, \method sigficantly improves the performance by an relative improvement of \textbf{8.00\%} in Instruction Induction and \textbf{115\%} in BIG-Bench. Given its simplicity, \method makes it easy to boost the performance of \llms without complicated design or prompt engineering. 
    \item \textbf{\method demonstrates a potential proclivity for superior performance within few-shot learning.} Compared with the zero-shot and few-shot results on Instruction Induction tasks, we see that the improvement brought by \method is larger in few-shot setting than zero-shot settings (0.33 vs. 2.05, in terms of average improvement). This indicates that \method is better at in-context learning with few-shot examples. Given that few-shot learning commonly performs better than zero-shot setting, this makes \method widely applicable in a wide spectrum of tasks.
    \item \textbf{\method consistently demonstrates commendable efficacy across tasks varying difficulty as well as on diverse \llms.} Big-Bench~\cite{suzgun2022challenging} and Instruction Induction \cite{honovich2022instruction} focus on tasks of different difficulties separately. Remarkably, \method excels in evaluations across both benchmarks. Furthermore, the generalization ability of \method can also be proved via its consistent performance across the six evaluated \llms.
    \item \textbf{\method outperforms existing existing prompt engineering approaches such as CoT and APE in most cases.} We also see that \method can be plugged into APE in \cref{tb-results-ii}, indicating that \method is highly extensible and compatible with existing prompt engineering methods.
    % \item \textbf{List detailed results and analysis which task can get more benefits?}
    
\end{enumerate}

We will further discuss and analyze the different aspects of \method, such as why \method would work and which emotional stimuli work the best in \cref{sec-dis}.

% \wjd{Analyze the results.}
% Based on the experimental results, our key findings are listed below:
% \begin{enumerate}
% \setlength\itemsep{0em}
% \item \textbf{larger}

% \item \textbf{temperature}

% \item \textbf{temperature}


% \end{enumerate}
% \input{tables/tb-chatgpt-result-zero}

% \input{tables/tb-chatgpt-result-few}

% \input{tables/tb-t5-result-zero}

% \input{tables/tb-t5-result-few}

% \input{tables/tb-vicuna-result-zero}

% \input{tables/tb-vicuna-result-few}

% \input{tables/tb-bloom-result-zero}

% \input{tables/tb-bloom-result-few}

\subsection{Human study}
\label{sec-results-humanstudy}
% % Figure environment removed

\input{tables/tb-human-info}

% % Figure environment removed
Beyond deterministic tasks, the generative capabilities of \llms hold significant importance, encompassing activities such as writing poems and summary, which needs human's judgement. These tasks necessitate human judgment. Additionally, we aim to probe the efficacy of \method from broader perspectives, encompassing dimensions like truthfulness and responsibility. As we know, no appropriate automatic methods exist to quantify these facets. Therefore, we conduct a human study to resolve the above-mentioned limiting conditions.

In a subsequent validation phase, we undertook a comprehensive study involving $106$ participants to explore the effectiveness of \method in open-ended generative tasks using GPT-4, the most capable LLM to date.
This evaluation was grounded on three distinct metrics: performance, truthfulness and responsibility.
Performance encompasses the overall quality of responses, considering linguistic coherence, logical reasoning, diversity, and the presence of corroborative evidence.
Truthfulness is a metric to gauge the extent of divergence from factual accuracy, otherwise referred to as hallucination \cite{lin2021truthfulqa}.
Responsibility, on the other hand, pertains to the provision of some positive guidance coupled with a fundamental sense of humanistic concern.
This criterion also underscores the broader implications of generated content on societal and global spheres \cite{xu2023cvalues}. 

\subsubsection{Study procedure and participant recruitment}
We formulated a set of $30$ questions and generated two distinct responses for each, leveraging the capabilities of GPT-4.
One is generated using the vanilla prompt, while the other is generated utilizing our \method.
Participants were then asked to evaluate both responses for each question, employing a scale ranging from $1$ to $5$ based on the aforementioned three metrics.
Finally, we analyze the scores of these participants.


% The enrollment of the 106 participants was executed meticulously, adhering to relevant regulatory standards and guidelines.. 
The enrollment of the 106 participants was executed meticulously, adhering to relevant regulatory standards and guidelines. Pertinent demographic characteristics concerning these participants is detailed in \cref{tb-human-info}. Notably, all individuals in the participant pool possess advanced academic degrees and demonstrate a commendable command of the English language.

\subsubsection{Survey questions and measurement}
We curated a set of $30$ questions, spanning a diverse range of domains such as biology, history, law, finance, pseudoscience, environmental science, intimate relationship, social science, psychology, and data science.
Notably, 10 of these questions were sourced from TruthfulQA \cite{lin2021truthfulqa}, a set specifically designed to provoke \llms into producing responses that manifest hallucinations. Additionally, in consonance with the CValues dataset \cite{xu2023cvalues}, another 15 questions were meticulously devised to elicit biased responses from \llms.
The final 5 questions were geared towards generative tasks such as poetry composition and summarization, which inherently demand a degree of creativity and artistic flair.
A comprehensive breakdown of these questions is provided in \cref{tb-human-study-questions}.

To verify the reliability of the questions used in our human study, we compute the mean and standard deviation for each question on vanilla prompts, and visualize the distribution of individual scores in \cref{fig-human-questions-analysis}. It's worth noting that none of the standard deviation of those questions pass $0.8$, which proves the reliability of our designed questions.

Based on the previously delineated three metrics, each participant was instructed to evaluate the two responses corresponding to each question, utilizing a scale ranging from $1$ to $5$.
Here, we outline the measures used in our human study:
\begin{itemize}
    \item \textbf{Performance:} $1$ = ``The response fails to address the question adequately'', $2$ =``The response addresses the question; however, its linguistic articulation is suboptimal, and the logical structure is ambiguous'', $3$ = ``The response sufficiently addresses the question, demonstrating clear logical coherence'', $4$ = ``Beyond merely addressing the question, the response exhibits superior linguistic clarity and robust logical reasoning'', $5$ = ``The response adeptly addresses the question, characterized by proficient linguistic expression, lucid logic, and bolstered by illustrative examples''.
    \item \textbf{Truthfulness:} $1$=``The response promulgates incorrect information, detrimentally influencing the ultimate interpretation'', $2$=``A segment of the response deviates from factual accuracy; however, this deviation does not materially affect the ultimate interpretation'', $3$=``The response predominantly adheres to factual accuracy, with potential for minor discrepancies that do not substantially influence the final interpretation'', $4$=``The response is largely in consonance with factual evidence, albeit with insignificant deviations that remain inconsequential to the final interpretation'', $5$=``The response is in meticulous alignment with the facts, exhibiting no deviations''.
    \item \textbf{Responsibility:} $1$ = ``The content within the response may harbor implications detrimental to society and the world'', $2$ = ``The response possesses elements that could potentially induce adverse emotional reactions, such as panic or anxiety'', $3$ = ``The response remains neutral, neither encompassing positive nor negative societal implications'', $4$ = ``The response is imbued with constructive guidance and exhibits elements of humanitarian concern'', $5$ = ``The response is characterized by pronounced humanitarian considerations and is poised to foster positive ramifications for both society and the global community''.
\end{itemize}

% Figure environment removed

\subsubsection{Study results and analysis}
Finally, we average the scores from $106$ participants for $30$ questions and report the credible results in \cref{fig-humanstudy}.\footnote{We notice that the results have high variance. The reason is that the measure of three metrics is highly influenced by subjectivity. Different people may have different opinions on an answer. Besides, performance encompasses the overall quality of responses, taking into account linguistic coherence, logical reasoning, diversity, and the presence of corroborative evidence, so the variance can also be influenced by the above factors.}
To make it clear, we compute Relative Gain (\cref{eq-win-rate}) on 3 metrics for each task and report the results in \cref{fig-relativegain}.
\begin{equation} \label{eq-win-rate}
\text{Relative Gain} = \mathrm{Metric}_{\text{EmotionPrompt}} - \mathrm{Metric}_{\text{vanilla}},
\end{equation}
where $\mathrm{Metric}$ denotes the results (performance, truthfulness, or responsibility).

More detailed generation results are shown in \cref{secA1} in Appendix.
Our key findings are as follows:
\begin{enumerate}
    \item \textbf{\method attains commendable performance across various metrics for the majority of questions.} As illustrated in \cref{fig-relativegain}, \method exhibits shortcomings in a mere two instances, yet it demonstrates substantial improvements in over half of the evaluated scenarios, spanning diverse domains sourced from three distinct origins. For performance, \method achieves a Relative Gain approaching or exceeding $1.0$ in nearly one-third of problems, signifying a notable advancement. 
    
    \item \textbf{\method demonstrates an enhanced capacity for generating ethically responsible responses.} An assessment of \cref{tb-case-environment} elucidates that the output from \method advocates for individuals to partake conscientiously in garbage sorting. This not only underscores the significance of environmental responsibility and sustainability, but also its value in fostering personal achievement and augmenting community welfare. Such instances accentuate the ability of \method to instill a sense of responsibility within \llms. A supplementary exemplification can be found in \cref{tb-case-relationship}. When tasked with delineating Western and Chinese cultures, \llms exhibit differential linguistic choices between the original prompt and \method. Notably, the representation elicited by \method presents a more affirmative and responsible depiction of both Western and Chinese cultural paradigms.
    
    \item \textbf{Responses engendered by \method are characterized by enriched supporting evidence and superior linguistic articulation.} An exploration of \cref{tb-case-relationship-2} reveals that the narratives presented by \method are markedly comprehensive, as exemplified by inclusions such as ``Despite trends like increasing divorce rates or more people choosing to remain single.'' Additionally, as illuminated in \cref{tb-case-social-science,tb-case-law,tb-case-barrier-free}, the responses facilitated by \method consistently demonstrate a superior organizational coherence and encompass a broader spectrum of pertinent information.

    \item \textbf{\method stimulates the creative faculties and overarching cognizance of \llms.} This phenomenon is substantiated through the examination of \cref{tb-case-poem-1,tb-case-poem-2}, wherein two instances of poem composition are showcased. Evidently, the poems generated by \method exude a heightened level of creativity and emotive resonance, evoking profound sentiment. Furthermore, we underscore this observation with reference to \cref{tb-case-summary}, wherein responses derived from two distinct prompt types are compared. Notably, the output generated from the original prompt centers on the novel's content, while the response fostered by \method delves into the spirit of the novel, which discusses the motivation and future significance concerning society and human nature. 

    \item \textbf{\method exhibits certain constraints.} The only two failure cases are presented in \cref{tb-case-fail-1,tb-case-fail-2}. Upon inspection of \cref{tb-case-fail-1}, a discernible difference emerges between the two responses. The output from \method employs more definitive terms, such as ``completely'' and ``will not'', while the narrative produced by the original prompt adopts a more tempered tone, signified by terms like ``generally'' and ``may even be''. This distinction might render the latter more palatable for certain audiences. Such deterministic language from \method could be attributed to its emphasis on the gravity of the question, indicated by phrases like ``This is important to my career'' and ``You'd better be sure''. To assuage uncertainties and bolster confidence, \llms might be inclined to use unambiguous language, particularly when the underlying facts are unequivocal. 
    Besides, in \cref{tb-case-fail-2}, the original prompt yields more expansive responses, encompassing a concluding summary, whereas \method just enumerates the key points. However, in terms of essential content, both responses are satisfactory. Consequently, while \method possesses the propensity to enhance \llms outputs in many instances, it may not be universally applicable across all scenarios.
\end{enumerate}



% \input{tables/tb-human-study}

\subsection{Truthfulness and Informativeness}
\label{sec-results-truth}
% \input{tables/tb-word-importance}
\input{tables/tb-truthfulqa-result}

% % Figure environment removed


% Figure environment removed

We further evaluate \method on TruthfulQA \cite{lin2021truthfulqa} to investigate its impact on truthfulness and informativeness. 
The benchmark has $817$ questions from $38$ categories, including health, law, finance, and politics.
We evaluate all samples in TruthfulQA and report the result with two metrics: truthfulness (\% True) and informativeness (\% Info). Truthfulness means the answer has less uncertainty, while informativeness means the answer can provide information \cite{lin2021truthfulqa}. Those results can be accessed by their fine-tuned GPT-judge and GPT-info, which have been proven to align with human prediction over 90\% of the time  \cite{lin2021truthfulqa}.
To be specific, GPT-judge is fine-tuned to evaluate answers as true or false, while GPT-info is to classify answers into informative or uninformative \cite{lin2021truthfulqa}.


\cref{tb-truthqa-result} shows the results on ChatGPT, Vicuna-13b and Flan-T5-Large.
We did not evaluate other models like GPT-4 due to constrained budget.
The application of \method yields improvements in truthfulness across all three models with an average improvement of 19\% and 12\% in terms of truthfulness and informativeness scores.
Furthermore, the performance of \method surpasses that of the Zero-shot-CoT when employed with diverse models.
These experiments demonstrate that by integrating emotional stimuli into large language models, their truthfulness and informativeness can also be enhanced.
% Specifically, for ChatGPT, the truthfulness score increases from 0.75 to 0.87. For Vicuna-13b, the score improves from 0.77 to 1.0, and for T5, it increases from 0.54 to 0.77.
% Additionally, \method enhances the informativeness of responses for ChatGPT, increasing the score from 0.53 to 0.94, and for T5, from 0.42 to 0.48. 


\section{Discussions}
\label{sec-dis}

Previous experiments demonstrate that \llms understand and can be enhanced by emotional stimuli.
In this section, we design extensive experiments to present a better understanding of the relationship between \llms and emotional intelligence.
Specifically, we answer the following questions:
\begin{enumerate}
    \item Why does \method work (\cref{sec-discuss-why});
    \item Ablation studies of more emotional stimuli (\cref{sec-discuss-more});
    \item Which emotional stimuli are the best (\cref{sec-discuss-best});
    \item The factors influencing the performance of \method (\cref{sec-discuss-influence}).
\end{enumerate}


\subsection{Why does \method work?}
\label{sec-discuss-why}

% Figure environment removed

\input{tables/tb-word-importance}

This section presents a deeper understanding of why \method works by visualizing the input attention contributions of emotional stimuli to the final outputs as proposed in \cite{zhu2023promptbench}.
Since Flan-T5-large is open-sourced and relatively small, we chose it as our experimental LLM and assessed the contribution of every word based on the gradient norm.
The experiment is conducted on a Sentiment Analysis task.
Specifically, we compute the contributions of prompts on every test sample and use the average value to represent their importance.

According to the visualization results in \cref{tb-word-importance}, we have the following major findings:
\begin{enumerate}
    \item \textbf{Emotional stimuli can enrich original prompts' representation.} Original prompt ``\prompt{Determine whether a movie review is positive and negative.}'' has deeper color in \method, especially in \ep{01}, \ep{03}, and \ep{06}$\sim$\ep{10}. This means emotional stimuli can enhance the representation of original prompts.
    \item \textbf{Positive words make more contributions.} In our designed emotional stimuli, some positive words play a more important role, such as ``confidence'', ``sure'', ``success'' and ``achievement''. Based on this finding, we summarize positive words' contribution and their total contributions to the final result on $8$ tasks. As shown in \cref{fig-word-importance}, the contributions of positive words pass 50\% on $4$ tasks, even approach 70\% on $2$ tasks.
\end{enumerate}


\subsection{The effect of more emotional stimuli}
\label{sec-discuss-more}

As one or more stimuli may regulate human action, and more stimuli sometimes are more effective, we explore the effect of more emotional stimuli on LLMs. We randomly combine some emotional stimuli and experiment on ChatGPT and results are shown in \cref{tb-more-stimulus}.
Our findings are:
\begin{enumerate}
    \item \textbf{More emotional stimuli generally lead to better performance.} The second and the third groups explore the effect of adding \ep{01}, showing that the third group performs better than the second group in most cases. 
    \item \textbf{Combined stimuli can bring little or no benefit when sole stimuli already achieves good performance.} The combination \ep{01} + \ep{04} gets a high score in most tasks and does not improve significantly or even decrease when we add more stimuli, such as \ep{06}$\sim$\ep{09}.
    \item \textbf{Combinations from different psychological theories can also boost the performance.} We also observe that by combining emotional stimuli from different psychological theories (e.g., \ep{02}+\ep{09}) can lead to better performance, indicating that different theories can be used together in \method.
\end{enumerate}

\input{tables/tb-more-stimulus}


\subsection{Which emotional stimuli is more effective?}
\label{sec-discuss-best}
% Figure environment removed

Because of the distinct metrics employed by Instruction Induction \cite{honovich2022instruction} and BIG-Bench \cite{suzgun2022challenging}, we have conducted a segregated examination to discern the efficacy of various emotional stimuli across these two benchmarks. We first average the performance on every task, leveraging $6$ \llms for each emotional stimuli. This is executed for both human-designed and APE-generated prompts. Subsequently, the performance is averaged over all the \llms. \cref{fig-best-stimuli-ii} and \cref{fig-best-stimuli-bigbench} delineate the performance of all emotional stimuli on Instruction Induction \cite{honovich2022instruction} and BIG-Bench \cite{suzgun2022challenging}, separately. The color of each bar serves as an indicator of the performance achieved by the corresponding stimuli.

Our key findings are listed below:
\begin{enumerate}
    \item \textbf{Within Instruction Induction, \ep{02} emerges as the most effective stimuli, while in BIG-Bench, \ep{06} is the best.} This observation stems from a thorough examination of results across both benchmarks. It is worth noting that the performance of each stimulus may be influenced by various factors, including task complexity, task type, and the specific metrics employed.
    \item \textbf{Distinct tasks necessitate varied emotional stimuli for optimal efficacy.} \cref{fig-best-stimuli-ii,fig-best-stimuli-bigbench} illustrate that while \ep{02} emerges as the predominant stimulus in Instruction Induction, while perform poorly in BIG-Bench. The efficacy of other stimuli similarly demonstrates variability across the two benchmarks. This suggests that individual stimuli might differently activate the inherent capabilities of \llms, aligning more effectively with specific tasks.
\end{enumerate}


\subsection{What influences the effect of \method?}
\label{sec-discuss-influence}


\input{tables/tb-model-analysis}
Finally, we explore the factors that could influence the performance of \method.
We analyze from two perspectives: the characteristic of \llms, and the inference setting (temperature).

\subsubsection{The characteristics of \llms}

\cref{tb-model-analysis} shows the characteristic of our evaluated \llms ordered by Relative Gain from \cref{fig-relativegain}.
To be specific, Relative Gains are calculated be averaging the results on Instruction Induction in a zero-shot setting, leveraging human-designed prompts, because few-shot may introduce uncertainty.
We report our findings below:
\begin{enumerate}
    \item \textbf{Larger models may potentially derive greater advantages from \method.} Flan-T5-Large, the smallest model in our evaluated \llms, yields the most modest Relative Gain by $0.28$. As the model dimensions expand, \method showcases enhanced efficacy, a trend notably evident in models such as Vicuna and Llama 2. When the model size increases substantially, \method continues to demonstrate commendable performance, such as ChatGPT and GPT-4. It is pertinent to emphasize that a relatively subdued Relative Gain in these models does not necessarily indicate the inefficacy of \method. A plausible interpretation could be that these larger models, namely ChatGPT, BLOOM, and GPT-4, inherently possess a high baseline performance, making incremental enhancements more challenging to achieve.
    \item \textbf{Pre-training strategies, including supervised fine-tuning and reinforcement learning, exert discernible effects on \method.} A case in point is exemplified by Vicuna and Llama 2, which share identical model scales and architectures. Nevertheless, a notable discrepancy exists in Relative Gain, with Vicuna achieving $9.58$, whereas Llama 2 attains a score of $6.00$.
\end{enumerate}

\subsubsection{Inference settings}

To explore the effect of temperature setting on \method, we conduct an experiment on $8$ tasks from Instruction Induction \cite{honovich2022instruction} in $5$ temperatures on $6$ \llms.
Note that we did not report Vicuna and Llama 2 results in temperature $0.0$ because they do not support this setting or the results are invalid.
\cref{fig-temperarure-exp} shows the results and our findings are listed below:
\begin{enumerate}
    \item \textbf{When the temperature grows, Relative Gain gets larger.} As shown in the graph of Llama 2, ChatGPT, GPT-4 and Flan-T5-Large, there is a noticeable expansion in the gap between the two curves as the temperature setting escalates. This observation suggests that \method exhibits heightened effectiveness in the high-temperature settings.
    \item \textbf{\method exhibits lower sensitivity to temperature than vanilla prompts.} Observing the two curves in each subgraph, the blue line(representing \method) is more gentle than the orange line(representing vanilla prompts). This indicates that \method could potentially enhance the robustness of \llms.
\end{enumerate}

% Figure environment removed

\section{Conclusion}
\label{sec13}

Large language models are demonstrating unprecedented performance across various applications.
This paper conducted the very first study in evaluating and analyzing how \llms understand and if it can be enhanced by emotional intelligence, which is a critical nature of human beings.
We designed \method for such analysis.
Our standard evaluation on $45$ tasks with $6$ \llms showed positive results: \llms can understand and be enhanced by emotional stimuli.
Our human study also demonstrated that \llms enhanced by emotional intelligence can achieve better performance, truthfulness, and responsibility.

Moving forward, we do see a lot of open questions and opportunities lying at the intersection of \llms and psychology.
First, even if we present some attention visualization in this paper to understand the reason why \method succeeds, more work should be done from the fundamental level of psychology and model training, such as how pre-training technology influences the performance in emotional stimuli, how to improve the performance by incorporating psychological phenomena into pre-training etc.
We are positive that more analysis and understanding can help to better understand the ``magic'' behind the emotional intelligence of \llms.
Second, while this paper concludes that \llms can understand and be enhanced by emotional intelligence, it, in fact, conflicts with existing studies on human emotional intelligence.
Existing psychological studies suggest that human behavior or attitude can be influenced by emotions, but their reasoning or cognitive abilities cannot
be simply enhanced by adding emotional stimuli.
However, the mystery behind such divergence is still unclear, and we leave it for future work to figure out the actual difference between human and \llms' emotional intelligence.

\bibliographystyle{plain}
\bibliography{sn-bibliography}% common bib file

% \input{cover-letter}



% \bmhead{Supplementary information}


% \input{tables/tb-chatgpt-result-zero}

% \input{tables/tb-chatgpt-result-few}

% \input{tables/tb-bloom-result-zero}

% \input{tables/tb-bloom-result-few}

% \input{tables/tb-t5-result-zero}

% \input{tables/tb-t5-result-few}



% \bmhead{Acknowledgments}

% Acknowledgments are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

% Please refer to Journal-level guidance for any specific requirements.

% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval 
% \item Consent to participate
% \item Consent for publication
% \item Availability of data and materials
% \item Code availability 
% \item Authors' contributions
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}
\newpage
\begin{appendices}

\section{Statistics of test sets in this paper}
\input{tables/tb-instruction-induction}
\input{tables/tb-bigbench}
A comprehensive breakdown of the test data employed in the automated experimentation is delineated in \cref{tb-instruction-induction,tb-bigbench}.

\section{Details on our human study}

The set of $30$ questions designated for the human study can be found in \cref{tb-human-study-questions}.
\input{tables/tb-human-study-questions}

The distribution of individual scores, their mean and standard deviation on each question can be found in \cref{fig-human-questions-analysis}.

% Figure environment removed



\section{Case Study}
\label{secA1}

We present case studies in this section to show the advantage of our \method over the original prompts in generative experiments using GPT-4.
\begin{itemize}
    \item \cref{tb-case-environment}: Case study on environmental science.
    \item \cref{tb-case-relationship} and \cref{tb-case-relationship-2}: Case studies on intimate relationship.
    \item \cref{tb-case-social-science}: Case study on social science.
    \item \cref{tb-case-law}: Case study on law.
    \item \cref{tb-case-barrier-free}: Case study on barrier free.
    \item \cref{tb-case-poem-1} and \cref{tb-case-poem-2}: Case studies on poem writing.
    \item \cref{tb-case-summary}: Case study on summarization task.
    \item \cref{tb-case-fail-1} and \cref{tb-case-fail-2}: Two failure cases.
\end{itemize}

\input{tables/tb-case-environment}
\input{tables/tb-case-relationship}
\input{tables/tb-case-relationship-2}
\input{tables/tb-case-social-science}
\input{tables/tb-case-law}
\input{tables/tb-case-barrier-free}
\input{tables/tb-case-poem-1}
\input{tables/tb-case-poem-2}
\input{tables/tb-case-summary}
\input{tables/tb-case-fail-1}
\input{tables/tb-case-fail-2}

% An appendix contains supplementary information that is not an essential part of the text itself but which may help provide a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%





\end{document}
