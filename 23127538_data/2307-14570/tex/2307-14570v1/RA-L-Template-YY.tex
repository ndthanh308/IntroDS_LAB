
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



%\documentclass[journal]{IEEEtran}
\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{array, makecell}
\usepackage{lettrine}
\usepackage{ulem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{changes}
\definechangesauthor[name={Sandika}, color=orange]{sb}
%\usepackage{enumitem}
%\def\IEEEtitletopspace{3pt}
\usepackage[top=57pt,bottom=43pt,left=48pt,right=57pt]{geometry}

%\IEEEoverridecommandlockouts                              
%\overrideIEEEmargins 


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed

% Figure environment removed

Given an indoor image, we use pre-trained networks to get 2D bounding boxes around objects and humans and generate 3D reconstructions. The predicted 3D reconstructions are used to form a graphical representation of the scene and analyzed using a graph discriminator compared to the graph created from the ground truth. This section explains our 1) human mesh reconstruction (Fig. \ref{fig:bd_human}), 2) object reconstruction (Fig. \ref{fig:bd_object}), and 3) the graph discriminator modules.

\subsection{Human mesh reconstruction:} 
We use a learning-based model \cite{li2022cliff} for human body reconstruction. Given an image firstly we use OpenPose \cite{8765346} network for human bounding box detection. The human reconstruction network takes the cropped human image and bounding box information \ie location of the bounding box center ($c_x, c_y$) relative to the actual image center ($W/2, H/2$), and the size of the squared bounding box ($b$) in actual image scale (before cropping and resizing) as input to predict the SMPL (\textbf{S}kinned \textbf{M}ulti-\textbf{P}erson \textbf{L}inear model) \cite{loper2015smpl} body parameters ($\theta \in \mathbb{R}^{72}$, orientation of each body joint \wrt the root \ie pelvis joint and $\beta \in \mathbb{R}^{10}$, representing body shape) and the root joint translations, $T^{full} = [t_X^{full},t_Y^{full},t_Z^{full}]$, relative to the original camera.

\noindent
\textbf{Object-aware human reconstruction:} 
A human's pose and location are conditioned by the other objects' positions in the scene; hence it is necessary to incorporate the positional relation of the human with the other scene elements. Nie \etal \cite{Nie_2020_CVPR} propose using the relative positions of surrounding objects in the scene for each object bounding box prediction. Following a similar approach, we use RelationNet \cite{hu2018relation} to extract the relation features of the human, $\mathcal{F}_{h-obj} \in \mathbb{R}^{2048}$, with respect to the other elements in the scene. The inputs to RelationNet \cite{hu2018relation} are i) the ResNet-34 feature for each of the scene elements, and ii) geometric features ($ \in \mathbf{R}^{n \times 64}$) between each pair of scene elements ($n$) defined by the relative distance between the 2D bounding boxes. We add this relation feature with the image feature $\mathcal{F}_{img} \in \mathbb{R}^{2048}$ of the cropped human instance. 

The combined bounding box feature and image-object feature $\mathcal{F}_{bbox} \oplus (\mathcal{F}_{img} + \mathcal{F}_{h-obj})$ is passed through an MLP regressor proposed in \cite{li2022cliff} to predict SMPL pose ($\theta$), shape ($\beta$) parameters and the perspective camera translation $t^{crop} = [t^{crop}_X,t^{crop}_Y,t^{crop}_Z]$ with respect to the cropped image. This perspective camera translation is then modified to get the body root translation with respect to the original camera,  $T^{full}$ using the focal length $f$ and the human bounding box information $(c_x, c_y, b)$ \cite{li2022cliff}.
$T^{full}$ is added to the SMPL body vertices to get the human body reconstruction in the world coordinates. From the SMPL body vertices, 3D body joints are predicted using the joint regressor \cite{kolotouros2019learning}. The losses used for training the human body generator are as follows,

\begin{equation}
\begin{split}
\mathcal{L}_{human} & = \lambda_{pose}\mathcal{L}_{pose} +  \lambda_{shape}\mathcal{L}_{shape} + \lambda_{2D}\mathcal{L}_{2D} \\ +  \lambda_{3D}&\mathcal{L}_{3D} + \lambda_{verts}\mathcal{L}_{verts} + \lambda_{h-obj_{gan}}\mathcal{L}_{h-obj_{gan}}
\end{split}
\label{equ:h_recon_loss}
\end{equation}

Where, $\mathcal{L}_{pose} = ||\theta - \theta^\prime||_2$, $\mathcal{L}_{shape}=|\beta - \beta^\prime|$, $\mathcal{L}_{3D}=||J_{3D} - J_{3D}^\prime||_2$, $\mathcal{L}_{2D}=||J_{2D} - J_{2D}^\prime||_2$, and $\mathcal{L}_{verts}=||\mathcal{B}_{verts} - \mathcal{B}_{verts}^\prime||_2$ are losses applied on predicted SMPL pose, shape parameter, 3D body joints, reprojected 3D joints, and body mesh vertices $\{\theta^\prime,\beta^\prime, J_{3D}^\prime, J_{2D}^\prime,\mathcal{B}_{verts}\}$. The adversarial loss is defined as follows,
\begin{equation}
\begin{split}
    \mathcal L_{h-obj_{gan}}  = \mathbb{E}_{\{\theta^\prime,\beta^\prime,\mathcal{B}_v^\prime\}} & [log(D(\mathcal{G}_h^\prime)] \\ & + \mathbb{E}_{\{\theta,\beta,\mathcal{B}_v\}}[log (1-D(\mathcal{G})] 
    \end{split}
\end{equation} 

where $\mathcal{G}_h^\prime$ is the graph formed from predicted human mesh and ground-truth object meshes and $\mathcal{G}$ is formed from ground-truth human mesh and object meshes. Details about graph formation are described in Section \ref{sec:graph_disc}.

\subsection{Object reconstruction:}
\label{sec:obj_recon}
Similar to \cite{weng2021holistic} and \cite{yi2022human}, we use the object reconstruction module proposed by Nie \etal\cite{Nie_2020_CVPR}. It consists of three sub-networks, i.e., Layout Estimation Network (LEN), Object Detection Network (ODN) for object bounding box prediction, and Mesh Generation Network (MGN). From a given image, first object labels and 2D bounding boxes are extracted using an off-the-shelf network PointRend \cite{kirillov2020pointrend}. Then ResNet-34 \cite{he2016deep} is used to get the image feature from the object patches and RelationNet \cite{hu2018relation} is used to extract the object-object relation feature $\mathcal{F}_{obj-obj}$. Using this feature the Object Detection Network predicts each object's %\deleted{bounding box} 
size ($\mathbb{B}_{s}^\prime$), orientation ($\mathbb{B}_{\theta}^\prime$), and 2D offset of the bounding box center. We use ground truth camera parameters to unproject the predicted 2D object center for computing the 3D object centroid ($\mathbb{B}_{c}^\prime$). The pre-trained Mesh Generation Network (MGN) from \cite{Nie_2020_CVPR} is used to get the object meshes, which are then resized, rotated, and translated using the predicted object orientation ($\mathbb{B}_{\theta}^\prime$), centroid ($\mathbb{B}_{c}^\prime$) and size ($\mathbb{B}_{s}^\prime$) information. We use the same losses as proposed in \cite{Nie_2020_CVPR} for training.

As we perform per-frame reconstruction human occlusion highly affects the object reconstruction. Hence, to further enhance the object detection performance for occlusion cases, we propose using occlusion masks' features along with the ResNet image features (Fig. \ref{fig:bd_object}). For training purposes, we use ground-truth occlusion masks. Ground-truth occlusion masks are extracted by using $ M_{obj} - M_{obj} \cap M_{human}$, where $M_{obj}$ are unoccluded object masks that are extracted manually from a keyframe (without occlusion by the human) and $M_{human}$ is the rendered body mask. 
The losses used for training the human body generator are as follows,

\begin{equation}
\begin{split}
\mathcal{L}_{obj} & = \lambda_{size}\mathcal{L}_{size} +  \lambda_{ori}\mathcal{L}_{ori}\\ & + \lambda_{centroid}\mathcal{L}_{centroid} + \lambda_{obj-obj_{gan}}\mathcal{L}_{obj-obj_{gan}}
\end{split}
\label{equ:obj_recon_loss}
\end{equation}
Where, $\mathcal{L}_{size} = ||\mathbb{B}_s - \mathbb{B}_s^\prime||_2$, $\mathcal{L}_{ori}=||\mathbb{B}_{\theta} - \mathbb{B}_{\theta}^\prime||_2$, and $\mathcal{L}_{centroid}=||\mathbb{B}_c - \mathbb{B}_c^\prime||_2$ are losses applied on predicted size, orientation and centroid of the object bounding boxes $\{\mathbb{B}_{s}^\prime,\mathbb{B}_{\theta}^\prime,\mathbb{B}_{c}^\prime\}$. The adversarial loss is defined as,

\begin{equation}
\begin{split}
    \mathcal L_{obj-obj_{gan}} = \mathbb{E}_{\{\mathbb{B}_{s}^\prime,\mathbb{B}_{\theta}^\prime,\mathbb{B}_{c}^\prime\}} & [log(D(\mathcal{G}_{obj}^\prime)] + \\ & \mathbb{E}_{\{\mathbb{B}_{s},\mathbb{B}_{\theta},\mathbb{B}_{c}\}}[log (1-D(\mathcal{G})] 
    \end{split}
\end{equation} 
where $\mathcal{G}_{obj}^\prime$ and $\mathcal{G}$ are the graphs formed from predicted and ground-truth object meshes.

\subsection{Graph Discriminator:}
\label{sec:graph_disc}
We propose using the GNN \cite{corso2020principal} for analyzing the physical laws and constraints using implicit feature representation of the reconstructed 3D scene elements. Specifically, we use a graph discriminator to distinguish a physically plausible real scene alignment from a reconstructed physically implausible scene to further strengthen the performance of the object and human reconstruction modules.

\noindent
\textbf{Graph formation:}
We form a graph, $\mathcal{G}(\mathcal{V},\mathcal{E})$, $\mathcal{V} \in \mathbb{R}^{P}, P = N+M$ from the scene, where, nodes $\mathcal{V}$, are represented by the objects $\{O_n\}_{n=1,2,...N}$ and human body segments $\{H_m\}_{m=1,2,...M}$, that come in contact (\eg feet while walking or standing on the floor, hands while touching any objects \etc) with the scene or scene objects. These body segments, $\{H_m\}_{m=1,2,...M}$ are defined by the vertices of the corresponding regions of the body mesh as proposed by Hassan \etal in \cite{hassan2019resolving}. $N$ is the number of objects in the scene and $M$ is the number of body segments under consideration. We consider the corner points of the 3D bounding box around the objects or the human body segments as the node features $\mathcal{F}_{\mathcal{V}_l} \in \mathbb{R}^{8 \times 3}$. Implementation details for 3D bounding box formation around human body segments are described in Section \ref{sec:impl}. The edge features $\mathcal{F}_{\mathcal{V}_l -> \mathcal{V}_k} = \mathcal{F}_{\mathcal{V}_l,\mathcal{V}_k}^{dist} \oplus \mathcal{F}_{\mathcal{V}_l,\mathcal{V}_k}^{surf-norm}$ are defined by the Euclidean distance $\mathcal{F}_{\mathcal{V}_l,\mathcal{V}_k}^{dist}$ between every pair of corners (Eqn. \ref{eqn:vert_dist}) and surface normals $\mathcal{F}_{\mathcal{V}_l,\mathcal{V}_k}^{surf-norm}$ of every pair of faces (Eqn. \ref{eqn:face_norm}) of the bounding boxes, between each pair of nodes or scene elements $\mathcal{V}_l$ and $\mathcal{V}_k$. 
%\hrtc{any relative angle used? I thought you mentioned this somewhere} 
$\mathcal{F}_{\mathcal{V}_l,\mathcal{V}_k}^{dist}$ helps to analyze the relative distance, positioning, and sizes of the interacting elements whereas, $\mathcal{F}_{\mathcal{V}_l,\mathcal{V}_k}^{surf-norm}$ helps to analyze the relative orientations.

\begin{equation}
\mathcal{F}_{\mathcal{V}^i_l, \mathcal{V}^j_k}^{dist} = \lVert \mathbb{B}^l_i - \mathbb{B}^k_j \rVert, \; 
\label{eqn:vert_dist}
\end{equation}

where, $\mathbb{B}^l_i,\mathbb{B}^k_j \in \mathbb{R}^{8 \times 1}$ represents the $i^{th}$ and $j^{th}$ corner points of the 3D bounding boxes of $l^{th}$- and $k^{th}$-object.

\begin{equation}
\mathcal{F}_{\mathcal{V}^i_l, \mathcal{V}^j_k}^{surf-norm} = \mathcal{B}^l_{n_i} \times \mathcal{B}^k_{n_j}, \; 
\label{eqn:face_norm}
\end{equation}

where, $\mathbb{B}^l_{n_i},\mathcal{B}^k_{n_j} \in \mathcal{R}^{6 \times 3}$ represents the surface normals on $i^{th}$ and $j^{th}$ faces of $l^{th}$- and $k^{th}$-object bounding boxes. More details about the network architecture and training are given in Section \ref{sec:impl}. 

%\vspace{10pt}
\noindent
\textbf{Scene representation learning:} We have used Principal Neighbourhood Aggregation (PNA) graph convolution \cite{corso2020principal} network for scene representation learning. Unlike the traditional graph convolution networks, PNA use multiple aggregators %\deleted{(Supplementary)} 
for effective feature representation learning, as a single message aggregator fails to capture meaningful representations from different kinds of messages \cite{corso2020principal}. Also, it uses a scaler to either amplify or attenuate the aggregated message at a node, based on the number of messages coming to the node. We have used four aggregators i.e \textit{mean}, \textit{max}, \textit{min} and \textit{std}  and three-scalers i.e. \textit{identity}, \textit{amplification} ($\alpha=1$), \textit{attenuation} ($\alpha=-1$) for effective feature aggregation from neighborhood nodes. 
Node embeddings learned using PNAConv layers are passed through a graph pooling layer to get the scene representation. Finally, an MLP is used with an output layer to classify the graph between real and predicted. Hence, the output of this graph discriminator consists of binary labels: 0 for predicted and 1 for ground truth or real.

\section{Experiments and Results}
\subsection{Dataset}

We use the PROX (Proximal Relationships
with Object eXclusion) dataset \cite{hassan2019resolving} for our experiments. It consists of a) \textit{qualitative} and b) \textit{quantitative} datasets. \textit{PROX qualitative} dataset contains 12 different scenes including, bedrooms, libraries, offices, and living rooms. It contains around 100K RGB-D frames with humans interacting with the scenes. 
%Human movements are captured using Kinect-One. 
It contains pseudo-ground truth for the body poses generated from the SMPLify-X \cite{pavlakos2019expressive} method using depth and RGB frames. The \textit{PROX quantitative} contains a single scene where human activities or interactions with the scene have been captured using MoCap. The ground truth of human body mesh is generated using MoSh++ \cite{mahmood2019amass}. The scene is scanned and reconstructed using Structure Sensor and Skanect. This dataset contains a total of 18 sequences with 180 RGB-D frames. 
We have used this dataset for cross-dataset evaluation. 
We use \textit{PROX qualitative} and perform data augmentation (scaling and translation) for the training of both human and object reconstruction modules. We use a similar train and test split as defined by Hassan \etal in POSA \cite{hassan2021populating}. Amongst 12 scenes, 7 scenes have been used for training and 3 scenes (N3OpenArea, MPH1Library, and MPH16) have been used for testing. We also provide qualitative results for human and scene reconstruction on \textit{PROX quantitative} dataset.

\subsection{Implementation Details}
\label{sec:impl}
We use the CLIFF (\textbf{C}arrying \textbf{L}ocation \textbf{I}nformation in \textbf{F}ull \textbf{F}rames) model proposed by Li \etal \cite{li2022cliff} as the backbone of our human mesh reconstruction module and train this module on the \textit{PROX qualitative} dataset. 
We additionally use object alignment information in 2D as input for more accurate localization of the human in the scene (Fig. \ref{fig:bd_human}).
For object reconstruction, we use the network proposed by \cite{Nie_2020_CVPR} as our backbone network and initialized the weights of our object reconstruction network using the model proposed in \cite{Nie_2020_CVPR}. 
We additionally use object occlusion masks as input (Fig. \ref{fig:bd_object}).
For creating a graph from the predicted scene elements, we calculate 3D bounding boxes around human body segments that come in contact with the scene, \eg feet, hands \etc. \\
\textbf{3D bounding box creation around human mesh:}
We perform PCA \cite{pcaref} on the projection of vertices of each body segment on each plane for calculating the orientation around the perpendicular axis and size of the bounding boxes. For example, to calculate the orientation of a body segment around the $x$-axis, the body segment vertices are projected on the $yz$ plane ($x=0$). PCA is performed on the projected 2d points (on $yz$ plane) to calculate the principal orientation ($\theta_x$) of the point cluster. Using the calculated orientation, the points are aligned with the axis (applying $-\theta_x$) to calculate the size of the body segment. The orientations around the $y$- and $z$-axis are calculated in a similar manner. Finally, the 3D bounding box for each body segment vertices is formed using these size, orientation, and centroid information for graph formation.

Our proposed graph discriminator consists of 4 PNAConv layers \cite{corso2020principal} followed by a graph pooling layer, \textit{global$\_$add$\_$pool}, and 3 fully connected layers with ReLU at the intermediate and Sigmoid activation at the last layer. The \textit{global$\_$add$\_$pool} layer calculates a graph level output \ie a representation of the input graph, by adding the node features across the node dimension of the input graph. We have used \textit{global$\_$add$\_$pool} layer from pytorch\_geometric library \cite{FeyLenssen2019}.

We train human and object reconstruction modules independently. We use Adam optimizer \cite{kingma2014adam} with the initial learning rate of $1e-4$ and $1e-3$ for the object and human reconstruction network training. We have used PyTorch and PyTorch geometric library for implementation and GTX 1080i GPU for training and inference. In the human reconstruction training loss (Eqn. ~\ref{equ:h_recon_loss}) $\lambda_{verts} = 1.0 $, $\lambda_{pose} = 1.0 $, $\lambda_{shape} = 0.1 $, $\lambda_{2D} = 5.0 $ and $\lambda_{3D} = 5.0$. The total loss is scaled by 60. In the object reconstruction loss (Eqn. \ref{equ:obj_recon_loss}) are $\lambda_{ori} = 1.0 $, $\lambda_{size} = 1.0 $, $\lambda_{centroid} = 1.0 $, $\lambda_{gen} = 0.1 $ and $\lambda_{disc} = 0.1$. The total loss is scaled by 100. 

\begin{table}[ht]
\centering
    \noindent\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
Method & Input & \multicolumn{2}{c|}{Object Reconstruction Quality} & \multicolumn{2}{c|}{Physical Metrics} \\
\hline 
\vspace{1pt}
 &  & IoU3D $\uparrow$ & IoU2D $\uparrow *$  & Non-collision $\uparrow$  & Contact $\uparrow$ \\
\hline
MOVER \cite{yi2022human} & Image Sequence & \textcolor{red}{0.3665} & \textcolor{red}{0.6179} & 0.9992 & 0.1601 \\ %{0.7240}
HolisticMesh \cite{weng2021holistic} & Image Sequence & 0.2607 & 0.4361 & 0.8878 & 0.7725 \\ %0.6478
\hline
\hline
Total3D \cite{Nie_2020_CVPR} & Single Image & 0.1917 & 0.3038 & - & - \\
Baseline $\dagger$  & Single Image & 0.3156 & 0.4037 & - & - \\
%HolisticMesh \cite{weng2021holistic} & Single Image & 0.2607 & 0.6478 & 0.8878 & 0.7725 \\
Ours & Single Image & \textcolor{blue}{0.3470} & \textcolor{blue}{0.5515} & \textcolor{blue}{0.9011} & \textcolor{blue}{0.7897} %7049
 \\
\hline
\end{tabular}
}
\caption{\footnotesize{Quantitative comparison of our object reconstruction method with SoTA on \textit{PROX qualitative} dataset. Blue denotes the best-performing method that uses single image input and performs per-frame prediction. Red denotes the overall best-performing method. $*$ IoU2D is calculated as bird's eye view 2D bounding box IoU. $\dagger$ Baseline is Total3D \cite{Nie_2020_CVPR} trained on \textit{PROX qualitative} dataset.}}
\label{tab:quantitative}
\end{table}

\begin{table}[ht]
\centering
    \noindent\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{|c|c|c|}
\hline 
 & \multicolumn{2}{c|}{Object reconstruction} \\
\hline  
 Method &  IoU3D $\uparrow$ & IoU2D $\uparrow$ \\
\hline
HolisticMesh \cite{weng2021holistic} & 0.2048 & 0.5167 \\
Baseline + object mask & 0.2514 & 0.5208 \\
Baseline + gan + object mask (Ours) & 0.2647 & 0.6097 \\
\hline
\end{tabular}
}
\caption{\footnotesize{Quantitative comparison of our object reconstruction method with SoTA on \textit{PROX qualitative} dataset using predicted 2D bounding box and occlusion masks. The model trained with graph discriminator is defined as 'gan'.}}
\label{tab:quantitative_predbbox}
\end{table}

\begin{table}[ht]
\centering
    \noindent\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
Dataset & Method & \multicolumn{2}{c|}{Localization} & \multicolumn{2}{c|}{Pose Estimation} \\
\hline 
%\vspace{1pt}
 &  & MPJPE & V2V & p-MPJPE & p-V2V \\
\hline
\multirow{2}{*}{PROX quantitative} & MOVER \cite{yi2022human} & \textbf{174.37} & \textbf{178.31} & 73.60 & 67.89 \\
& HolisticMesh \cite{weng2021holistic} & 190.78 & 192.21 & 72.72 & \textbf{61.01} \\
& Ours $\dagger$ & 266.31 & 268.08 & \textbf{72.31} & 84.14 \\
\hline
\hline
\multirow{2}{*}{\makecell{PROX quantitative \\ (Testset)}} & HolisticMesh \cite{weng2021holistic} & 187.24 & 191.00 & 65.13 & \textbf{64.07} \\
& Ours $\ddagger$ & \textbf{181.02} & \textbf{187.89} & \textbf{55.82} & 67.62
 \\
\hline
\hline
PROX qualitative & Ours $\dagger$ & 182.57 & 187.79 & 59.51 & 70.14 \\
\hline
\end{tabular}
}
\caption{\footnotesize{Quantitative comparison of our human reconstruction with SoTA. Ours $\dagger$: Human reconstruction model trained on \textit{PROX qualitative} dataset; Ours $\ddagger$: Human reconstruction model fine-tuned on \textit{PROX quantitative} dataset using 5-fold cross-validation method.}}
\label{tab:quantitative_human}
\end{table}

\begin{table}[ht]
\centering
    \noindent\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{|c|c|c|}
\hline 
 & \multicolumn{2}{c|}{Object reconstruction} \\
\hline  
 Method &  IoU3D $\uparrow$ & IoU2D $\uparrow$ \\
\hline
Baseline & 0.3156 & 0.4037 \\
Baseline + gan & 0.3279 & 0.4206 \\
Baseline + object mask & 0.3059 & 0.5208 \\
Baseline + gan + object mask (Ours) & \textbf{0.3470} & \textbf{0.5515} \\
\hline
\end{tabular}
}
\caption{\footnotesize{Ablation for object reconstruction on \textit{PROX qualitative}.}}
\label{tab:quantitative_abl}
\end{table}

\subsection{Quantitative Results:}
%\vspace{3pt}
\noindent
\textbf{Object reconstruction}
We have evaluated and compared our object reconstruction results against the state-of-the-art methods HolisticMesh \cite{weng2021holistic} and MOVER \cite{yi2022human} in Table \ref{tab:quantitative} (with GT 2D bounding box and occlusion masks) and \ref{tab:quantitative_predbbox} (with predicted 2D bounding box and occlusion masks by PointRend \cite{kirillov2020pointrend}). It should be noted that MOVER optimizes the object and human reconstruction based on the HSI information collected from a sequence of frames taken as input. And HolisticMesh performs optimization of the object reconstruction network parameters over the sequence of frames. We have used the same metrics proposed in \cite{weng2021holistic,yi2022human} for the performance evaluation. As we use \textit{PROX qualitative} dataset for training, the performance metrics are evaluated on 3 test scenes of the dataset for all three methods.
Similar to \cite{weng2021holistic,yi2022human,Nie_2020_CVPR} we have used 3D and 2D IoU for quantifying the object localization accuracy. %These metrics help to capture the accuracy of the predicted object location, size, and orientation.
We train the Total3D model \cite{Nie_2020_CVPR} on PROX qualitative dataset and consider it as our \textit{Baseline} model.
Our method \ie \textit{Baseline} with graph discriminator outperforms the results of \textit{Baseline} and HolisticMesh in 3D IoU using both predicted (Table \ref{tab:quantitative_predbbox}) and ground-truth (Table \ref{tab:quantitative}) bounding box and occlusion masks.
Following MOVER \cite{yi2022human}, we have computed the Non-collision and Contact scores to assess the physical plausibility of the reconstructed scenes with respect to the reconstructed human (Table \ref{tab:quantitative}).
Total3D \cite{Nie_2020_CVPR} performs only object reconstruction, hence does not have non-collision and contact scores. 

% Figure environment removed

\noindent
\textbf{Human reconstruction}
Following the state-of-the-art methods we use Mean Per Joint Error (MPJPE) and vertex-to-vertex (V2V) distance for assessing the quality of human reconstruction (Table \ref{tab:quantitative_human}) \cite{kolotouros2019learning}. MPJPE is calculated on the 3D body skeleton joints without making the root position zero.
Our human reconstruction module is trained on human body meshes from 7 scenes of the \textit{PROX qualitative} dataset. 
Our method gives a reasonable performance on cross-dataset (\textit{PROX quantitative}) compared to the state-of-the-art methods. We also fine-tune the human reconstruction module using 12 sequences of PROX-quantitative data, which performs better than that of HolisticMesh \cite{weng2021holistic}. 
We use p-MPJPE and p-V2V for estimating the quality of human pose reconstruction similar to the state-of-the-art methods. 
Our method achieves the best p-MPJPE compared to the state-of-the-art methods, hence the human reconstruction module can capture the body pose correctly.

\subsection{Qualitative Results:}

We demonstrate our scene reconstruction results on two unseen scenes from the PROX qualitative dataset (Fig. \ref{fig:prox_qual}). For each frame, the reprojection of reconstructed object meshes on the image plane is shown in the first row, and 3D reconstructions from different viewing angles are in the second and third rows. Our approach, with the discriminator network (second column, Fig. \ref{fig:prox_qual}), produces physically plausible scene reconstructions with accurate relative positioning of humans and objects compared to the backbone model (without discriminator network, third column, Fig. \ref{fig:prox_qual}). However, in the case of scene MPH16, the model struggles to capture the correct size of the bed due to limited bed instances in the training dataset. Our method performs well on objects like sofas and chairs, outperforming HolisticMesh and yielding comparable results to MOVER.

\subsection{Performance vs. Inference Time:}
In Fig. \ref{fig:inference_time} we present an analysis of performance vs. inference time for all the methods.
Our method (0.75 sec/frame) and Total3D (0.24 sec/frame) \cite{Nie_2020_CVPR} perform per-frame prediction, whereas MOVER \cite{yi2022human} and HolisticMesh \cite{weng2021holistic} perform optimization over a sequence of images. 
Weng \etal \cite{weng2021holistic} take around 4-5 mins/frame and optimize over the whole sequence which takes days to produce the final reconstruction. Whereas, Yi \etal \cite{yi2022human} take around 30 mins to optimize over the whole sequence and take the same time for a scene regardless of the number of frames in the input video. For a fair comparison, the execution time for all the methods is reported over a sequence of frames (avg. 1200 frames). All methods are evaluated under the same system configuration. 
Although the inference time of Total3D \cite{Nie_2020_CVPR} is low, it should be noted that Total3D performs only object reconstruction. Our method achieves comparable performance with MOVER at a much lower per-frame execution time.

\begin{table}[ht]
\centering
    \noindent\resizebox{.8\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & \multicolumn{2}{c|}{PROX quantitative} & \multicolumn{2}{c|}{PROX qualitative} \\
\hline  
 Method &  MPJPE & V2V &  MPJPE & V2V\\
\hline
Ours w/o gan & \textbf{181.97} & 198.80 & \textbf{181.74} & 188.17 \\
Ours w/ gan & 181.02 & \textbf{187.89} & 182.57 & \textbf{187.79} \\
\hline
\end{tabular}
}
\caption{\footnotesize{Ablation for human reconstruction module.}}
\label{tab:quantitative_human_abl}
\end{table}
%\vspace{-15pt}    
  
\subsection{Ablation Study:}
%\vspace{3pt}
\noindent
\textbf{Object reconstruction} In Table \ref{tab:quantitative_abl}, we have compared the effect of using the graph discriminator (represented as '$+$ gan' in Table) for the scene reconstruction using 3D and 2D IoU values on the \textit{PROX qualitative} test set. Using the graph discriminator helps in achieving better reconstruction compared to the baseline. Also, we show the effectiveness of using binary object masks as input for object reconstruction. Object masks help in increasing the performance, specifically for the occluded objects, and give the best IoU3D. 

\noindent
\textbf{Human reconstruction} In Table \ref{tab:quantitative_human_abl} we have shown the effectiveness of our scene-aware human body reconstruction on the test set of both \textit{quantitative} and \textit{qualitative} datasets. Using a discriminator to analyze the human placement with respect to the object alignments in the scene helps in achieving better MPJPE and V2V values on \textit{PROX quantitative}.

\section{Conclusions}
We have proposed a fully learning-based scene reconstruction method, which relies on implicit feature representation of a scene for differentiating a physically plausible scene reconstruction from implausible human and object alignments without explicitly defining physical laws and constraints. The reconstruction generator learns from training data to produce a plausible 3D scene and performs per-frame prediction without any test time optimization. Our method achieves comparable performance as the state-of-the-art methods but with a faster reconstruction speed. However, the execution time needs to be further improved for utilizing it in an actual robotics platform. Due to limited variability in the training data, our method suffers from a lack of generalization in terms of dynamic views, camera setup, resolution \etc. It fails to perform well for out-of-training distribution data. In the future, with the availability of a more generalized dataset, robust scene reconstruction would be possible. 


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.

% % use section* for acknowledgment
% \section*{Acknowledgment}

% The authors would like to thank...


% % Can use something like this to put references on a page
% % by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
%   \newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
\bibliography{IEEEexample}



% that's all folks
\end{document}


