\section{Related Work}
\label{sec:related}

\textbf{Physics-Informed Neural Networks (PINNs).} Physics-Informed Neural Networks (PINNs) have emerged as a promising approach to tackling scientific and engineering problems.~\cite{raissi2019physics} introduced the novel framework that incorporates physical laws into the neural network training to solve PDEs. This pioneering work has inspired subsequent investigations, leading to applications across diverse domains, including fluid dynamics, solid mechanics, and quantum mechanics~\citep{ling2016reynolds, carleo2019machine, yang2020physics}. Researchers also investigate different learning schemes of PINNs~\citep{mao2020physics, wang2021understanding, wang2022and}. 
These strategies have yielded substantial improvements in convergence, generalization, and interpretability than PINNs.

\textbf{Failure Modes of PINNs.} Despite the promise exhibited by PINNs, recent works have indicated certain failure modes inherent to PINNs, particularly when confronted with PDEs featuring high-frequency or multiscale features \citep{fuks2020limitations, raissi2018deep, mcclenny2020self, krishnapriyan2021characterizing, zhao2022physics, wang2022and}. This challenge has prompted investigations from various perspectives, including designing variant model architectures, learning schemes, or using data interpolations~\citep{han2018solving, lou2021physics, wang2021understanding, wang2022and, wang2022auto}. A comprehensive understanding of PINNs' limitations and the underlying failure modes is fundamental for applications in addressing complicated physical problems.

\textbf{Transformer-Based Models.} The Transformer model~\citep{vaswani2017attention} has achieved significant attention due to its ability to capture long-term dependencies, leading to major achievements in natural language processing tasks~\citep{devlin2018bert, radford2018improving}. Transformers have also been extended to other domains, including computer vision, speech recognition, and time-series analysis \citep{liu2021swin, dosovitskiy2020image, gulati2020conformer, zhou2021informer}. Researchers have also developed techniques aimed at enhancing the efficiency of Transformers, such as sparse attention and model compression~\citep{child2019generating, sanh2019distilbert}.





