\section{Introduction}
\label{sec:intro}

Numerically solving partial differential equations (PDEs) has been widely studied in science and engineering. The conventional approaches, such as finite element method~\citep{bathe2007finite} or pseudo-spectral method~\citep{fornberg1998practical}, suffer from high computational costs in constructing meshes for high-dimensional PDEs. With the development of scientific machine learning, Physics-informed neural networks (PINNs)~\citep{lagaris1998artificial,raissi2019physics} have emerged as a promising novel approach. Conventional PINNs and most variants employ multilayer perceptrons (MLP) as end-to-end frameworks for point-wise predictions, achieving remarkable success in various scenarios. 

Nevertheless, recent works have shown that PINNs fail in scenarios when solutions exhibit high-frequency or multiscale features~\citep{raissi2018deep, fuks2020limitations, krishnapriyan2021characterizing, wang2022and}, though the corresponding analytical solutions are simple. In such cases, PINNs tend to provide overly smooth or naive approximations, deviating from the true solution. 

Existing approaches to mitigate these failures typically involve two general strategies. The first strategy, known as data interpolation \citep{raissi2017physics, zhu2019physics, chen2021physics}, employs data regularization observed from simulations, or real-world scenarios. These approaches face challenges in acquiring ground truth data. The second strategy employs different training schemes \citep{mao2020physics, krishnapriyan2021characterizing, wang2021understanding, wang2022and}, which potentially impose a high computational cost in practice. For instance, \textit{Seq2Seq} by \cite{krishnapriyan2021characterizing} requires training multiple neural networks sequentially, while other networks suffer from convergence issues due to error accumulation. Another method, \textit{Neural Tangent Kernel (NTK)} \citep{wang2022and}, involves constructing kernels $K\in\mathbb{R}^{D\times P}$, where $D$ is the sample size and $P$ is the model parameter, which suffers from scalability issues as the sample size or model parameter increases.


% Existing approaches to mitigate these failures typically involve two approaches: data interpolation~\citep{raissi2017physics, zhu2019physics, chen2021physics}, which suffers from challenges in acquiring ground truth data, and variant training schemes~\citep{mao2020physics, krishnapriyan2021characterizing, wang2021understanding, wang2022and}, which potentially impose a high computational cost in practice. For instance, \textit{Seq2Seq} learning scheme by~\cite{krishnapriyan2021characterizing} requires training multiple neural networks sequentially, while later neural networks suffer from convergence issues due to error accumulation. \textit{Neural Tangent Kernel (NTK)}~\citep{wang2022and} asks for constructing kernels $K\in\mathbb{R}^{D\times P}$, where $D$ is the sample points and $P$ is the model parameter, also suffer from scalability issues as the sample size or model parameter increases.

% many real-world physical systems exhibit time-dependent behavior. Finite element methods propagate the global solution sequentially by relying on the fact that the state at time $t+\Delta t$ depends on the state at time $t$. In contrast, PINNs, being a point-to-point framework, neglect temporal dependencies within PDEs. With neglecting temporal dependency,

While most efforts to improve the generalization ability and address failure modes in PINNs have focused on the aforementioned aspects, the crucial temporal dependencies in real-world physical systems have been largely neglected. Finite Element Methods, for instance, implicitly incorporate temporal dependencies by sequentially propagating the global solution. This propagation relies on the principle that the state at time $t+\Delta t$ depends on the state at time $t$. In contrast, PINNs, being a point-to-point framework, do not account for temporal dependencies within PDEs. Neglecting temporal dependencies poses challenges in globally propagating initial condition constraints in PINNs. Consequently, PINNs often exhibit failure modes where the approximations remain accurate near the initial condition but subsequently fail into overly smooth or naive approximations.

To address this issue of neglecting temporal dependencies in PINNs, a natural idea is employing Transformer-based models \citep{vaswani2017attention}, which are known for capturing long-term dependencies in sequential data through multi-head self-attentions and encoder-decoder attentions. Variants of transformer-based models have shown substantial success across various domains. 
However, adapting the Transformer, which is inherently designed for sequential data, to the point-to-point framework of PINNs presents non-trivial challenges. These challenges span both the data representation and the regularization loss within the framework.
% The mechanism of Transformer-based models can also extend to PINNs, mimicking the temporal dependency relationships in real-world physics systems. 

% We therefore propose PINNsFormer, a novel sequence-to-sequence PDE solver based on Transformer architecture. To the best of our knowledge, PINNsFormer is the first framework in PINNs that explicitly emphasizes and learns the temporal dependencies for PDEs.
% PINNsFormer also aims to generate robust approximations of various PDEs, including the PDEs where vanilla PINNs suffer from failure modes, such as convection and reaction-diffusion problems~\cite{krishnapriyan2021characterizing, mcclenny2020self}. Hence, we summarize PINNsFormer as a robust Transformer-based neural network for solving PINNs. Additionally, PINNsFormer improves PINNs' performance through only the model architecture's perspective, which leaves the flexibility to combine with other existing methodologies, such as data interpolation or learning rate annealing, etc., which is expected to further improve the performance of PINNs.

\textbf{Main Contributions.} In this work, we introduce PINNsFormer, a novel sequence-to-sequence PDE solver built on the Transformer architecture. To the best of our knowledge, PINNsFormer is the first framework in the realm of PINNs that explicitly focuses on and learns temporal dependencies within PDEs. Our key contributions can be summarized as follows:
\begin{itemize*} 
    \item \textbf{New Framework:} We propose a novel yet straightforward Transformer-based framework named PINNsFormer. This framework equips PINNs with the capability to capture temporal dependencies through the generated pseudo sequences, thereby enhancing the generalization ability and approximation accuracy in effectively solving PDEs.
    \item \textbf{Novel Activation:} We introduce a novel non-linear activation function $\texttt{Wavelet}$. $\texttt{Wavelet}$ is designed to anticipate the Fourier Transform for arbitrary target signals, making it a universal approximator for infinite-width neural networks. $\texttt{Wavelet}$ can also be potentially beneficial to various deep learning tasks across different model architectures.
    \item \textbf{Extensive Experiments:} We conduct comprehensive evaluations of PINNsFormer for various scenarios. We demonstrate its advantages in optimization and approximation accuracy when addressing the failure modes of PINNs or solving high-dimensional PDEs. Additionally, we showcase the flexibility of PINNsFormer in incorporating variant learning schemes of PINNs. We show the outperformance of PINNsFormer than PINNs with the schemes.
\end{itemize*}



