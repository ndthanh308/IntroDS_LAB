\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduced PINNsFormer, a novel Transformer-based framework, designed as an extension of PINNs, aimed at capturing temporal dependencies when approximating solutions to PDEs. To adapt conventional PINNs to Transformer-based models, we introduced the Pseudo Sequence Generator, a mechanism that translates vectorized inputs into pseudo time sequences, and incorporated a modified Encoder-Decoder layer along with a novel \texttt{Wavelet} activation function. Our empirical evaluations demonstrate that PINNsFormer consistently outperforms conventional PINNs across various scenarios, including handling PINNs' failure modes and addressing high-dimensional PDEs. Furthermore, PINNsFormer retains computational simplicity by using only linear layers with non-linear activation functions, making it a practical choice for real-world applications. It can flexibly integrate with existing learning schemes for PINNs, leading to superior performance.

Beyond its application in PINNsFormer, the novel \texttt{Wavelet} activation function holds promises for the broader machine learning community. We provided a sketch proof demonstrating \texttt{Wavelet}'s ability to approximate arbitrary target solutions using a two-hidden-layer infinite-width neural network, leveraging the Fourier decomposition of these solutions. We encourage further exploration, both theoretically and empirically, of the \texttt{Wavelet} activation function's potential. Its applicability extends beyond PINNs and can be leveraged in various architectures and applications.
