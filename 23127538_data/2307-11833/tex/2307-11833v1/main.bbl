\begin{thebibliography}{10}

\bibitem{bathe2007finite}
Klaus-J{\"u}rgen Bathe.
\newblock Finite element method.
\newblock {\em Wiley encyclopedia of computer science and engineering}, pages
  1--12, 2007.

\bibitem{carleo2019machine}
Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld,
  Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborov{\'a}.
\newblock Machine learning and the physical sciences.
\newblock {\em Reviews of Modern Physics}, 91(4):045002, 2019.

\bibitem{chen2018neural}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{chen2021physics}
Zhao Chen, Yang Liu, and Hao Sun.
\newblock Physics-informed learning of governing equations from scarce data.
\newblock {\em Nature communications}, 12(1):6136, 2021.

\bibitem{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of control, signals and systems}, 2(4):303--314,
  1989.

\bibitem{daw2022rethinking}
Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne.
\newblock Rethinking the importance of sampling in physics-informed neural
  networks.
\newblock {\em arXiv preprint arXiv:2207.02338}, 2022.

\bibitem{de2021assessing}
Taco de~Wolff, Hugo Carrillo, Luis Mart{\'\i}, and Nayat Sanchez-Pi.
\newblock Assessing physics informed neural networks in ocean modelling and
  climate change applications.
\newblock In {\em AI: Modeling Oceans and Climate Change Workshop at ICLR
  2021}, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fornberg1998practical}
Bengt Fornberg.
\newblock {\em A practical guide to pseudospectral methods}.
\newblock Number~1. Cambridge university press, 1998.

\bibitem{fuks2020limitations}
Olga Fuks and Hamdi~A Tchelepi.
\newblock Limitations of physics informed machine learning for nonlinear
  two-phase transport in porous media.
\newblock {\em Journal of Machine Learning for Modeling and Computing}, 1(1),
  2020.

\bibitem{gehring2017convolutional}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock In {\em International conference on machine learning}, pages
  1243--1252. PMLR, 2017.

\bibitem{glorot2011deep}
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
\newblock Deep sparse rectifier neural networks.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 315--323. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem{gulati2020conformer}
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu~Zhang, Jiahui Yu,
  Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et~al.
\newblock Conformer: Convolution-augmented transformer for speech recognition.
\newblock {\em arXiv preprint arXiv:2005.08100}, 2020.

\bibitem{haghighat2021physics}
Ehsan Haghighat, Maziar Raissi, Adrian Moure, Hector Gomez, and Ruben Juanes.
\newblock A physics-informed deep learning framework for inversion and
  surrogate modeling in solid mechanics.
\newblock {\em Computer Methods in Applied Mechanics and Engineering},
  379:113741, 2021.

\bibitem{han2018solving}
Jiequn Han, Arnulf Jentzen, and Weinan E.
\newblock Solving high-dimensional partial differential equations using deep
  learning.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(34):8505--8510, 2018.

\bibitem{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural networks}, 4(2):251--257, 1991.

\bibitem{jagtap2020adaptive}
Ameya~D Jagtap, Kenji Kawaguchi, and George~Em Karniadakis.
\newblock Adaptive activation functions accelerate convergence in deep and
  physics-informed neural networks.
\newblock {\em Journal of Computational Physics}, 404:109136, 2020.

\bibitem{krishnapriyan2021characterizing}
Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael~W
  Mahoney.
\newblock Characterizing possible failure modes in physics-informed neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:26548--26560, 2021.

\bibitem{lagaris1998artificial}
Isaac~E Lagaris, Aristidis Likas, and Dimitrios~I Fotiadis.
\newblock Artificial neural networks for solving ordinary and partial
  differential equations.
\newblock {\em IEEE transactions on neural networks}, 9(5):987--1000, 1998.

\bibitem{li2020fourier}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik
  Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Fourier neural operator for parametric partial differential
  equations.
\newblock {\em arXiv preprint arXiv:2010.08895}, 2020.

\bibitem{ling2016reynolds}
Julia Ling, Andrew Kurzawski, and Jeremy Templeton.
\newblock Reynolds averaged turbulence modelling using deep neural networks
  with embedded invariance.
\newblock {\em Journal of Fluid Mechanics}, 807:155--166, 2016.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 10012--10022, 2021.

\bibitem{lou2021physics}
Qin Lou, Xuhui Meng, and George~Em Karniadakis.
\newblock Physics-informed neural networks for solving forward and inverse flow
  problems via the boltzmann-bgk formulation.
\newblock {\em Journal of Computational Physics}, 447:110676, 2021.

\bibitem{mao2020physics}
Zhiping Mao, Ameya~D Jagtap, and George~Em Karniadakis.
\newblock Physics-informed neural networks for high-speed flows.
\newblock {\em Computer Methods in Applied Mechanics and Engineering},
  360:112789, 2020.

\bibitem{mcclenny2020self}
Levi McClenny and Ulisses Braga-Neto.
\newblock Self-adaptive physics-informed neural networks using a soft attention
  mechanism.
\newblock {\em arXiv preprint arXiv:2009.04544}, 2020.

\bibitem{mojgani2022lagrangian}
Rambod Mojgani, Maciej Balajewicz, and Pedram Hassanzadeh.
\newblock Lagrangian pinns: A causality-conforming solution to failure modes of
  physics-informed neural networks.
\newblock {\em arXiv preprint arXiv:2205.02902}, 2022.

\bibitem{park2022vision}
Namuk Park and Songkuk Kim.
\newblock How do vision transformers work?
\newblock {\em arXiv preprint arXiv:2202.06709}, 2022.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem{raissi2018deep}
Maziar Raissi.
\newblock Deep hidden physics models: Deep learning of nonlinear partial
  differential equations.
\newblock {\em The Journal of Machine Learning Research}, 19(1):932--955, 2018.

\bibitem{raissi2019physics}
Maziar Raissi, Paris Perdikaris, and George~E Karniadakis.
\newblock Physics-informed neural networks: A deep learning framework for
  solving forward and inverse problems involving nonlinear partial differential
  equations.
\newblock {\em Journal of Computational physics}, 378:686--707, 2019.

\bibitem{raissi2017physics}
Maziar Raissi, Paris Perdikaris, and George~Em Karniadakis.
\newblock Physics informed deep learning (part i): Data-driven solutions of
  nonlinear partial differential equations.
\newblock {\em arXiv preprint arXiv:1711.10561}, 2017.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{song2022versatile}
Chao Song, Tariq Alkhalifah, and Umair~Bin Waheed.
\newblock A versatile framework to solve the helmholtz equation using
  physics-informed neural networks.
\newblock {\em Geophysical Journal International}, 228(3):1750--1762, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2021understanding}
Sifan Wang, Yujun Teng, and Paris Perdikaris.
\newblock Understanding and mitigating gradient flow pathologies in
  physics-informed neural networks.
\newblock {\em SIAM Journal on Scientific Computing}, 43(5):A3055--A3081, 2021.

\bibitem{wang2022and}
Sifan Wang, Xinling Yu, and Paris Perdikaris.
\newblock When and why pinns fail to train: A neural tangent kernel
  perspective.
\newblock {\em Journal of Computational Physics}, 449:110768, 2022.

\bibitem{wang2022auto}
Yicheng Wang, Xiaotian Han, Chia-Yuan Chang, Daochen Zha, Ulisses Braga-Neto,
  and Xia Hu.
\newblock Auto-pinn: Understanding and optimizing physics-informed neural
  architecture.
\newblock {\em arXiv preprint arXiv:2205.13748}, 2022.

\bibitem{yang2020physics}
Liu Yang, Dongkun Zhang, and George~Em Karniadakis.
\newblock Physics-informed generative adversarial networks for stochastic
  differential equations.
\newblock {\em SIAM Journal on Scientific Computing}, 42(1):A292--A317, 2020.

\bibitem{yao2020pyhessian}
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael~W Mahoney.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In {\em 2020 IEEE international conference on big data (Big data)},
  pages 581--590. IEEE, 2020.

\bibitem{zhao2022physics}
Zhiyuan Zhao, Xueying Ding, Gopaljee Atulya, Alex Davis, and Aarti Singh.
\newblock Physics informed machine learning with misspecified
  priors:$\backslash$$\backslash$an analysis of turning operation in lathe
  machines.
\newblock In {\em AAAI 2022 Workshop on AI for Design and Manufacturing
  (ADAM)}, 2022.

\bibitem{zhou2021informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
  and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~35, pages 11106--11115, 2021.

\bibitem{zhu2019physics}
Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris
  Perdikaris.
\newblock Physics-constrained deep learning for high-dimensional surrogate
  modeling and uncertainty quantification without labeled data.
\newblock {\em Journal of Computational Physics}, 394:56--81, 2019.

\end{thebibliography}
