\section{Conclusion}
\label{sec:conclusion}

In this paper, we propose a Transformer-based framework, PINNsFormer, for Physics-Informed Neural Networks (PINNs), which captures temporal dependency in approximating the PDEs' solutions. To adapt conventional PINNs to Transformer-based models, we propose the Pseudo Sequence Generator, which translates vectorized inputs to pseudo time sequences, the modified Encoder-Decoder layer, and a novel \texttt{Wavelet} activation function. Empirical evaluations indicate that PINNsFormer can robustly outperform conventional PINNs by high on the failure modes of conventional PINNs. More importantly, PINNsFormer is not sensitive to major hyperparameters, which makes it easier to fine-tune, and utilizes only linear layers with non-linear activation functions, which helps preserve computational simplicity in practice. 

Going beyond PINNsFormer, the proposed \texttt{Wavelet} activation function can be potentially contributive to the machine learning community. We have provided a proof sketch to show that \texttt{Wavelet} can approximate arbitrary target solutions with a two-hidden-layer infinite-width neural network by anticipating the Fourier decomposition of the solutions. We expect more fundamental and empirical explorations on the proposed novel \texttt{Wavelet} activation function, and utilize the \texttt{Wavelet} for other architectures and applications.