\section{Experiments}
\label{sec:exp}

\subsection{Experiments Setup}
\label{sec:setup}

\textbf{Data Preparation.} We use a unified data preparation strategy for all PDEs we are evaluating, regardless of the different domain ranges for different PDEs. For the training phase, we uniformly sample $N_{\textit{ic}} = N_{\textit{bc}}=101$ initial and boundary data. We then uniformly create a $101\times 101$ mesh data inside the residual domain, such that $N_{\textit{res}}=10201$. For the test phase, we uniformly create a $201\times 201$ mesh data inside the residual domain. The sampling for testing is denser than for training, which aims to show the flexibility of zooming in/out the resolution and approximating the solutions at arbitrary points inside the residual domain, similar to \cite{chen2018neural}.

\textbf{Training and Testing.} We build a fully-connected MLP as the baseline, which is a typical architecture of PINNs, and the PINNsFormer. We control the number of parameters for both models to be close, aiming to show the benefit of PINNsFormer is indeed by its mechanism rather than the approximation power due to the over-parameterization. We train all models with L-BFGS optimizer with Strong Wolfe linear search function for 1000 iterations. For simplicity, we set $\lambda_\textit{res}=\lambda_\textit{ic} = \lambda_\textit{bc}=1$ for the optimization objective in Equation \ref{eqn:obj}. We detail all hyperparameters in Appendix \ref{sec:appenda}. 

\textbf{Reproducibility.} All models are implemented in PyTorch~\cite{paszke2019pytorch}, and are trained separately on a single NVIDIA Tesla V100 GPU. All training code and setup are included and reproducible at \url{https://github.com/AdityaLab/pinnsformer}.

\subsection{Main Results}

The main evaluation is based on solving three specific PDEs: convection equation, 1D-reaction equation, and reaction-diffusion equation, where MLP presents significant limitations in approximating the solutions for these equations~\cite{mojgani2022lagrangian, daw2022rethinking, krishnapriyan2021characterizing}, so-called the failure modes. The detailed formulations of the PDEs are presented in Appendix \ref{sec:appendb}. For the comprehensiveness of the evaluation, we present an ablation study by the combinations between two model architectures, MLP and Transformer (Trans. in short), and four activation functions, including \texttt{ReLU}, \texttt{Tanh}, \texttt{Sin}, and proposed \texttt{Wavelet}. In particular, combining Transformer with \texttt{Wavelet} activation function is exactly the PINNsFormer. We measure both relative MAE (rMAE, or $\ell_1$ error) and relative RMSE (rRMSE, or $\ell_2$ error) as most relative works do~\cite{krishnapriyan2021characterizing, raissi2019physics, mcclenny2020self}, defined as:
\begin{equation}
\begin{gathered}
    \texttt{rMAE} =  \frac{\sum_{n=1}^N |\hat{u}(x_n,t_n)-u(x_n,t_n)|}{\sum_{n=1}^{N_{\textit{res}}}|u(x_n,t_n)|}\\
    \texttt{rRMSE} = \sqrt{\frac{\sum_{n=1}^N |\hat{u}(x_n,t_n)-u(x_n,t_n)|^2}{\sum_{n=1}^N|u(x_n,t_n)|^2}}
\end{gathered}    
\end{equation}
where $N$ is the number of test points, $\hat{u}$ is the neural network approximation, and $u$ is the ground truth. In addition, we include the training PINNs loss for all cases. The evaluations of Convection and Reaction-Diffusion PDEs are shown in Table \ref{tbl:main}. We include additional evaluations on the 1D-Reaction problem and selected prediction plots by Appendix \ref{sec:appendc}.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\scalebox{0.8}{
\begin{tabular}{cc|ccc|ccc}
\toprule
\multirow{2}{*}{Model}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Activation\\ Function\end{tabular}} & \multicolumn{3}{c|}{Convection} & \multicolumn{3}{c}{Reaction-Diffusion} \\
                        &                                                                                & Loss     & rMAE     & rRMSE     & Loss        & rMAE       & rRMSE       \\ \hline
\multirow{4}{*}{MLP}    & ReLU&\num{5.66e-1}&\num{1.00e0}&\num{1.01e0}&\num{2.08e-1}&\num{9.92e-1}&\num{9.93e-1}\\
                        & Tanh&\num{1.18e-2}&\num{6.75e-1}&\num{7.56e-1}&\num{1.57e-4}&\num{1.85e-1}&\num{2.39e-1}\\
                        & Sin&\num{2.97e-1}&\num{1.06e0}&\num{1.12e0}&\num{5.89e-4}&\num{2.52e-1}&\num{2.90e-1}\\
                        & Wavelet&\num{3.50e-4}&\num{5.07e-2}&\num{5.75e-2}&\num{9.09e-4}&\num{7.81e-1}&\num{8.47e-1}\\ \hline
\multirow{4}{*}{Trans.} & ReLU&\num{5.40e-1}&\num{1.00e0}&\num{1.01e0}&\num{2.10e-1}&\num{9.90e-1}&\num{9.92e-1}\\
                        & Tanh&\num{1.50e-2}&\num{7.52e-1}&\num{8.20e-1}&\num{8.19e-6}&\num{1.39e-1}&\num{1.66e-1}\\
                        & Sin&\num{1.59e-2}&\num{7.56e-1}&\num{8.23e-1}&\num{5.01e-5}&\num{2.33e-1}&\num{2.94e-1}\\
                        & Wavelet&\num{2.43e-4}&\num{4.96e-2}&\num{5.64e-2}&\num{3.86e-6}&\num{8.49e-2}&\num{1.05e-1}  \\
                        \bottomrule 
\end{tabular}}
\vspace{0.05in}
\caption{Main results in approximating Convection and Reaction-Diffusion PDEs' solutions. PINNsFormer (Trans.+\texttt{Wavelet}) outperforms other methods in training loss, rMAE, and rRMSE.}
\label{tbl:main}
\end{table}

The evaluation results show significant outperformance of PINNsFormer over any other combinations between model architectures and activation functions, where PINNsFormer achieves the lowest both training loss and validation errors. Moreover, either Transformer-based model architecture or \texttt{Wavelet} activation function is beneficial for optimizing PINNs loss. For instance, the evaluations of the convection problem with \texttt{Wavelet} activation uniformly outperform it with \texttt{Tanh} activation, and the evaluations of the reaction-diffusion problem with Transformer uniformly outperform it with MLP. However, either cannot robustly overcome PINNs' failure modes independently. Only combining both the Transformer-based model and \texttt{Wavelet} activation results in better and more robust approximations for various PDEs.

\subsection{Methodology Overheads}
\label{sec:overhead}

One major motivation for proposing neural-network-based approaches in approximating PDEs' solutions, including PINNs, is to avoid the high computational costs for high-dimensional/order PDEs, while Transformer-based models are known to be more computationally expensive than MLPs. In addition, training sequential data typically requires more memory than point data, which can be costly in memory. Therefore, studying both the computational and memory overheads of PINNsFormer is practically meaningful. For consistency, we use the same model hyperparameters as employed in Main Results. We measure the computation cost by counting the training wallclock time per iteration for learning the convection problem. We measure the memory cost by counting the model parameters and training data size. We present all overheads in Table \ref{tbl:overehead}.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{c|cccc|cc}
\toprule 
\multirow{2}{*}{Model}  & \multicolumn{4}{c|}{Computational Cost (sec)} & \multicolumn{2}{c}{Memory Cost}                \\

                       & ReLU      & Tanh      & Sin      & Wavelet    & Model Parameters & Data Size                   \\ \hline
MLP                    & 2.05      & 2.50      & 2.59     & 3.38       & 527k             & {[}Batch, Dim{]}    \\
Trans.            & 3.03      & 3.39      & 3.36     & 4.69       & 453k             & {[}Batch, $k$, Dim{]} \\
Overhead               & 1.48\texttt{x}     & 1.36\texttt{x}     & 1.30\texttt{x}    & 1.39\texttt{x}      & 0.86\texttt{x}   & $k$\texttt{x}  \\
\bottomrule
\end{tabular}
\vspace{0.05in}
\caption{Computational and memory overhead between Transformer-based models and MLP-based models. Transformer-based models require fewer parameters but larger memory for data, while its computational cost is within 1.5\texttt{x} compared to MLP models.}
\label{tbl:overehead}
\end{table}

For computational overhead, PINNsFormer is about 1.3\texttt{x}$\sim$1.5\texttt{x} computationally costly than vanilla PINNs under the same activation functions. The computational difference is not significant, benefitted from the model's simplicity, that is, PINNsFormer consists of only linear hidden layers with non-linear activation functions, without complicated operators such as convolutional or recurrent layers. In addition, the computational cost is dependent on different activation functions. The proposed \texttt{Wavelet} activation function requires 1.6\texttt{x} and 1.35\texttt{x} computation time than \texttt{ReLU} and \texttt{Tanh} activation functions separately. This is either because \texttt{Wavelet} is more complicated in calculation and derivation, or manually implementing \texttt{Wavelet} is not fully GPU optimized by PyTorch. 

For memory overhead, PINNsFormer actually employs fewer model parameters to achieve better performance than vanilla PINNs. Indeed, PINNsFormer requires $k$ times larger memory for pseudo-sequential data. Nevertheless, on the one hand, hyperparameter $k$ does not necessarily need to be large in practice. We achieve significant outperformance by setting $k=5$. On the other hand, the training batch size of PINNs is typically smaller than other deep-learning tasks, such as language or vision. The memory cost remains controllable even by expanding to $k$-length sequences. Hence, PINNsFormer is a practically meaningful and applicable approach to approximate PDEs' solutions.

\subsection{Hyperparameter Sensitivity}

PINNsFormer creates a generated time sequence through its Pseudo Sequence Generator. The generator has two hyperparameters: steps $k$ and stepsize $\Delta t$. Tuning these two hyperparameters might impact the dependency relationships and qualities captured by self-attentions and encoder-decoder attention, and hence leads to performance differences. Therefore, we vary the evaluations of PINNsFormer on different pairs of $\{k,\Delta t\}$ for solving the convection PDE, which can be easily failed due to the solution's high-frequency component. Without comparing with conventional PINNs, we empower the approximation ability of PINNsFormer by simply doubling the hidden dimension of the feed-forward neural network. The evaluation results are shown in Table \ref{tbl:hyper}, which can be concluded by the following two observations.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{cc|cccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{rRMSE}} & \multicolumn{4}{c}{$k$}             \\
\multicolumn{2}{c|}{}                          & 3    & 5    & 7    & 10   \\ \hline
\multirow{6}{*}{$\Delta t$}            &\num{1e-5}&\num{3.19e-2}&\num{4.15e-2}&\num{3.12e-2}&\num{2.48e-2}\\
&\num{3e-5}&\num{3.41e-2}&\num{2.58e-2}&\num{3.89e-2}&\num{3.19e-2}\\
&\num{1e-4}&\num{3.10e-2} &\num{3.46e-2}&\num{6.94e-1}&\num{5.17e-2} \\
&\num{3e-4}&\num{5.04e-2} &\num{5.89e-1}&\num{4.64e-2}&\num{6.44e-2} \\
&\num{1e-3}&\num{1.13e0}&\num{5.81e-1}&\num{1.39e-1}&\num{2.04e-1}  \\
&\num{1e-2}&\num{0.446e-1} &\num{1.21e0} &\num{1.01e0}&\num{1.01e0} \\
\bottomrule
\end{tabular}
\vspace{0.05in}
\caption{Evaluated rRMSE of convection equation on wide ranges of hyperparameters $k$ and $\Delta t$.}
\label{tbl:hyper}
\end{table}

\textbf{Hyperparameter $\Delta t$ is not sensitive once it is lower than a threshold.} The performance differs significantly while varying the values of $\Delta t$. When $\Delta t$ remains small, PINNsFormer performs uniformly well. For instance, we observe the rRMSE are uniformly low when $\Delta t=$\texttt{1e-5} and \texttt{3e-5}, regardless of $k$. However, as $\Delta t$ increases, i.e., $\Delta t\geq$\texttt{1e-3}, the performance defers heavily. This is intuitively reasonable as learning temporal dependency between large time steps may become less meaningful. Nevertheless, since the empirical evaluations are generally robust for small $\Delta t$s,  we conclude that $\Delta t$ is not sensitive once it is smaller than a threshold.

\textbf{Hyperparameter $k$ is not sensitive, increasing $k$ can marginally improve the performance.} Intuitively, larger $k$ can enable more dependencies, and therefore, better performance. However, when $\Delta t$ is set properly, the performance of increasing pseudo sequence length $k$ is marginal. For instance, when $\Delta t=\texttt{1e-5}$, increasing $k=3$ to 10 only decrease the error by approximately $25\%$. The performance improvement of increasing $k$ when $\Delta t=\texttt{3e-5}$ is even less significant.
Thus, larger $k$ can only improve the performance marginally, and hence is not sensitive when $\Delta t$ is set properly. 


% \subsection{Robustness to non-failure PDEs}

% While the above evaluations rely on the failure PDEs of PINNs, PINNsFormer is indeed a robust framework for solving both failure and non-failure PDEs. We therefore evaluate PINNsFormer on Burger's and Helmholtz equations, in which conventional PINNs have already shown success in these scenarios~\cite{raissi2019physics}. We use the same setup as described in Section \ref{sec:setup}. We include the detailed PDEs setup and coefficients in Appendix \ref{sec:appendb} and results in Appendix \ref{sec:appendc}. In summary, PINNsFormer achieves uniformly better results under all evaluating metrics for both equations. 