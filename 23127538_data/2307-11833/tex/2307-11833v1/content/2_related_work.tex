\section{Related Work}
\label{sec:related}

\textbf{Physics-Informed Neural Networks (PINNs).} Physics-Informed Neural Networks (PINNs) have emerged as a powerful methodology for solving scientific and engineering problems. Raissi et al.~\cite{raissi2019physics} introduce the concept of incorporating physical laws into the training process of neural networks to solve partial differential equations (PDEs). This pioneering work has inspired subsequent research, including applications in fluid dynamics, solid mechanics, and quantum mechanics~\cite{ling2016reynolds, yang2020physics, carleo2019machine}. Researchers also focus on developing delicate learning schemes of PINNs, such as learning rate annealing, adaptive sampling, and self-adaptive PINNs (SA-PINN)~\cite{mao2020physics, wang2021understanding, wang2022and}. These schemes lead to better convergence, generalization, and interpretability in approximating accurate PDEs solutions. 

\textbf{Failure Modes of PINNs.} While Physics-Informed Neural Networks have shown great promise in solving partial differential equations, various works have indicated PINNs may fail to approximate the solution for specific PDEs, especially when the solution contains high-frequencies or multiscale features~\cite{fuks2020limitations, raissi2018deep, krishnapriyan2021characterizing, zhao2022physics, wang2022and}.  The challenge has been investigated from multiple perspectives, such as the choice of model architectures and hyperparameters, learning schemes, or data limitations~\cite{wang2022and, han2018solving, lou2021physics, wang2022auto, wang2021understanding}. Addressing these limitations and understanding the failure modes of PINNs are essential for their successful applications and further improvements in solving complex physical problems. 


\textbf{Transformer-Based Models.} The Transformer model~\cite{vaswani2017attention} has achieved state-of-the-art performance due to its ability to capture long-term dependency, which has revolutionized natural language processing on various tasks~\cite{devlin2018bert, radford2018improving}. The application of Transformers also extends beyond natural language processing, with studies exploring their effectiveness in computer vision, speech recognition, and time-series~\cite{liu2021swin, dosovitskiy2020image, gulati2020conformer, zhou2021informer}. Additionally, researchers have also investigated techniques to enhance the efficiency of Transformers such as sparse attention or model compression~\cite{child2019generating, sanh2019distilbert}.

% \vspace{-0.1in}
% While branches of research focus on the applications and variations of PINNs, adapting PINNs to state-of-the-art models such as Transformer has merely been explored. Therefore, it is non-trivial but fruitful to incorporate PINNs with Transformer-based models, which is potentially beneficial in better generalization and accuracy of PINNs.  