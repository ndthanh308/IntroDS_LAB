\section{Methodology}
\label{sec:method}

While most PINNs models study point-to-point predictions, learning and understanding the temporal dependency in PINNs seems to be neglected. For instance, conventional PINNs methods utilize a single pair of spatial information $\boldsymbol{x}$ and temporal information $t$ to approximate the numerical solution $u(\boldsymbol{x},t)$, without considering the dependency from its previous or later time steps. However, this is only true for elliptic PDEs, whose unknown functions and derivatives are related to each other in a way that does not involve time explicitly. Hyperbolic and parabolic PDEs involve time derivatives, indicating the state at one timestep may influence states at previous or later timesteps. Hence, it is crucial to consider the time dependency relations for solving these PDEs with PINNs.

In this section, we propose a novel framework with a Transformer-based model for solving PINNs, namely PINNsFormer, going beyond point-to-point predictions to sequential predictions. PINNsFormer allows approximating the accurate solution at a specific time step by learning and regularizing the temporal dependency of its incoming states. The framework consists of four components: Pseudo Sequence Generator, Spatio-Temporal Mixer, Encoder-Decoder with multi-head attention, and an Output Layer. In addition, we propose a novel activation function, namely \texttt{Wavelet}, that anticipates PDEs' solutions in a Fourier decomposition manner. The diagram of the framework is exhibited in Figure \ref{fig:arch}. We detail all framework components and learning schemes in the following subsections.

% Figure environment removed

\subsection{Pseudo Sequence Generator}

Despite the fact that Transformer and Transformer-based models are designed to capture the long-term dependency for sequential data, conventional PINNs generally take non-sequential data to feed forward the neural networks. Hence, to adapt PINNs with Transformer-based models, it is essential to translate the pointwise spatiotemporal inputs to temporal sequences. Thus, for any given spatial input $\boldsymbol{x}\in \mathbb{R}^{d-1}$ and temporal input $t\in \mathbb{R}$, the Pseudo Sequence Generator does the following operations:
\begin{equation}
    [\boldsymbol{x},t] \xRightarrow{\texttt{Generator}} \{[\boldsymbol{x},t], [\boldsymbol{x},t+\Delta t], \ldots, [\boldsymbol{x},t+(k-1)\Delta t]\} 
\end{equation}

where $[\cdot]$ is the concatenation operation, such that $[\boldsymbol{x},t]\in \mathbb{R}^d$ is vectorized, and the generator outputs the pseudo sequence in the shape of $\mathbb{R}^{k\times d}$. In short, the Pseudo Sequence Generator anticipates the sequential time series by expanding a single spatiotemporal input to multiple isometric discrete time steps.
$k$ and $\Delta t$ are hyperparameters, which intuitively determine how many steps the pseudo sequence needs to 'look ahead' and how 'far' each step should be. In practice, both $k$ and $\Delta t$ should not be set by large scales, as larger $k$ can cause heavy computational and memory overheads, and larger $\Delta t$ may undermine the time dependency relationships of neighboring discrete time steps. 

\subsection{Model Architecture}

In addition to the Pseudo Sequence Generator, PINNsFormer consists of three components of its model: Sptio-Temporal Mixer, Encoder-Decoder with multi-head attentions, and Output Layer. While it is trivial to interpret the Output Layer, as it is simply a fully-connected MLP attached to the tail similar to most neural networks, we detail the first two components in the following. Remarkably, PINNsFormer consists of only linear layers and non-linear activations without complicated operations such as convolutional or recurrent layers. Hence, it preserves computational efficiency as fully-connected MLPs do in practice.

\textbf{Spatio-Temporal Mixer.} The spatial information of most PDEs is typically low-dimensional. Directly feeding low-dimensional data to encoders may fail to capture the complex relationships between each feature dimension. Hence, it is necessary to embed original sequential data in higher-dimensional spaces such that more information is encoded into each vector.

Instead of embedding raw data in a high-dimensional space where the distance between vectors reflects the semantic similarity, as most language tasks do~\cite{vaswani2017attention, devlin2018bert}, PINNsFormer simply constructs a linear projection that maps spatiotemporal inputs onto a higher-dimensional space using a fully-connected MLP. The embedded data enriches the capability of information by mixing all raw spatiotemporal features together, so-called the linear projection Spatio-Temporal Mixer.

\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-0.1in}
    \begin{center}
        % Figure removed
    \end{center}
    \caption{The architecture of PINNsFormer's Encoder-Decoder Layers. The decoder is not equipped with self-attentions.}
    \label{fig:en_de}
    \vspace{-0.1in}
\end{wrapfigure}

\textbf{Encoder-Decoder with multi-head attention.} PINNsFormer employs an encoder-decoder architecture similar to Transformer~\cite{vaswani2017attention}. The encoder consists of a stack of identical layers, each of which contains a self-attention layer and a feedforward layer. The decoder is slightly different from the vanilla Transformer, in which each of the identical layers contains only an encoder-decoder attention layer and a feedforward layer. Since at decoders' level, PINNsFormer utilizes the same spatiotemporal embeddings as the encoder does, it is unnecessary for the decoder to equip with an additional self-attention and  repeatedly capture the dependencies for the same input embeddings. The diagram for the encoder-decoder architecture is shown in Figure \ref{fig:en_de}

Intuitively, our model's encoder's self-attention allows learning the dependency relationships of all spatiotemporal information. The decoder's encoder-decoder attention then enables selectively focusing on specific dependencies of the input sequence during the decoding process, capturing more information than a vanilla PINN.


\subsection{Wavelet Activation}

While Transformers are typically equipped with \texttt{LayerNorm} and \texttt{ReLU} non-linear activation functions~\cite{vaswani2017attention, gehring2017convolutional, devlin2018bert}, these activation functions might not generally be feasible in solving PINNs. In particular, employing \texttt{ReLU} activation in PINNs suffers from poor performance for PINNs, whose effectiveness relies heavily on the accurate evaluation of derivatives while \texttt{ReLU} has a discontinuous derivative~\cite{haghighat2021physics, de2021assessing}. Recent studies utilize \texttt{Sin} activation for some special cases in PINNs, aiming to mimic the periodic properties of PDEs' solutions~\cite{li2020fourier, jagtap2020adaptive, song2022versatile}. However, it requires a strong prior knowledge of the solution's behavior and only works well for limited scenarios. Tackling this issue, we proposed a novel and simple activation function, namely \texttt{Wavelet}, defined as follows:
\begin{equation}
    \texttt{Wavelet}(\boldsymbol{x}) = \sin(\boldsymbol{x})+\cos(\boldsymbol{x})
\end{equation}

The intuition behind \texttt{Wavelet} activation simply follows the Fourier Transform: While periodic signals can be decomposed into a sum of sine waves of different frequencies, all signals, including both periodic and non-periodic, can be decomposed into a sum of sine and cosine waves of different frequencies. It is then obvious that \texttt{Wavelet} activation can approximate arbitrary functions giving enough approximation power, which leads to the following proposition.

\begin{proposition}
    Let $\mathcal{N}$ be a two-hidden-layer neural network with infinite width, equipped with \texttt{Wavelet} activation function, then $\mathcal{N}$ is a universal approximator for any target f.
\end{proposition}

\textit{Proof sketch:} The proof follows the following two steps. First, for any given input $x$ and its corresponding target $f(x)$, it has the Inverse Fourier Transform:
\begin{gather*}
    f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty F(\omega)e^{-j\omega x}d\omega = \frac{1}{\pi} \int_0^\infty F_c(\omega)\cos \omega x + F_s(\omega) \sin \omega x d\omega
\end{gather*}

where $F_c$ and $F_s$ are the real and imaginary parts of the Fourier transform respectively. Second, by Riemann sum approximation, the integral can be rewritten to the infinite sum such that:
\begin{gather*}
    f(x) = \frac{1}{\pi}\sum^\infty_{i=0} F_c(\omega_i) \cos \omega_i x + F_s(\omega_i) \sin \omega_i x \equiv W_2(\texttt{Wavelet}(W_1 x))
\end{gather*}

where $W_1$ and $W_2$ are the weights of $\mathcal{N}$'s first and second hidden layer. Hence, $\mathcal{N}$ is a universal approximator for any given $f$.

Despite that to our purposes, the \texttt{Wavelet} activation function is only employed by PINNsFormer to improve PINNs, it can be potentially useful for other deep learning tasks. It is similar to \texttt{ReLU}, $\sigma(\cdot)$, and \texttt{Tanh} activations, which when applied to a two-hidden-layer neural network, turns the network into a universal approximator ~\cite{cybenko1989approximation, hornik1991approximation, glorot2011deep}. Though going beyond PINNs is out of the scope of this work, we hope to demonstrate the effectiveness of \texttt{Wavelet}  with other applications.

% \textcolor{blue}{Maybe a short proposition: Given a 2-layer linear layer with infinite width of hidden, and wavelet activation, it can approximate any solutions.}

\subsection{Learning Scheme}

While conventional PINNs deal with point-to-point predictions, it remains unexplored how PINNs loss can be adapted to pseudo-sequential inputs. In PINNsFormer, each generated point in the sequence, i.e., $[\boldsymbol{x}_i,t_i+j\Delta t]$, is forwarded to the corresponding approximation, i.e., $\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)$ for any $j\in\mathbb{N}, j<k$. Hence, it is possible to take the $n$th-order gradients w.r.t. $\boldsymbol{x}$ or $t$ independently for any valid $n$. For instance, for any given input pseudo sequence $\{[\boldsymbol{x}_i,t_i], [\boldsymbol{x}_i,t_i+\Delta t], \ldots, [\boldsymbol{x}_i,t_i+(k-1)\Delta t]\}$, and the corresponding approximations $\{\hat{u}(\boldsymbol{x}_i,t_i), \hat{u}(\boldsymbol{x}_i,t_i+\Delta t), \ldots, \hat{u}(\boldsymbol{x}_i,t_i+(k-1)\Delta t)\}$, we have the first-order derivatives w.r.t. $\boldsymbol{x}$ and $t$ in the following formulations separately:
\begin{equation}
\begin{gathered}
    \frac{\partial\{\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)\}_{j=0}^{k-1}}{\partial\{t_i+j\Delta t\}_{j=0}^{k-1}} = \{\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i)}{\partial t_i},\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+\Delta t)}{\partial(t_i+\Delta t)},\ldots, \frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+(k-1)\Delta t)}{\partial(t_i+(k-1)\Delta t)}\} \\
     \frac{\partial\{\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)\}_{j=0}^{k-1}}{\partial\boldsymbol{x}_i} = \{\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i)}{\partial\boldsymbol{x}_i},\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+\Delta t)}{\partial\boldsymbol{x}_i},\ldots, \frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+(k-1)\Delta t)}{\partial\boldsymbol{x}_i}\}
\end{gathered}
\end{equation}

The above scheme of calculating the gradient of sequential approximations w.r.t. sequential inputs can be easily extended to higher-order derivatives, and is applicable for residual, boundary, and initial points. 
However, while the general PINNs optimization objective in Equation \ref{eq:pinn} unifies the initial and boundary conditions together, PINNsFormer instead clearly distinguishes the initial and boundary conditions. This is because PINNsFormer regularizes the initial and boundary points differently through its learning scheme.
% The scheme of calculating the gradient of sequential approximations w.r.t. sequential inputs is applicable for residual, boundary, and initial points. 
For residual and boundary points, all sequential outputs can be regularized through the PINNs loss. This is because all generated pseudo timesteps are at the same domain as their original input. For instance, if $[\boldsymbol{x}_i,t_i]$ is sampled from the boundary, then $[\boldsymbol{x}_i, t_i+j\Delta t]$ also locates on the boundary for any given $j\in \mathbb{N}^+$. For the initial points, only the $t=0$ is regularized, which refers to the first element of the sequential outputs. This is because only the first element of the pseudo sequence refers to exactly the initial condition, where $t=0$. All other generated time steps have $t=j\Delta t$ for any $j\in\mathbb{N}^+$ are out of the initial conditions. Combining all reasonings above, we adapt the PINNs loss to the sequential version, described as follows:
\begin{equation}
\begin{gathered}
    \mathcal{L}_\textit{res} = \frac{1}{kN_\textit{res}}\sum_{i=1}^{N_\textit{res}}\sum_{j=0}^{k-1} \|\mathcal{D}[\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)] - f(\boldsymbol{x}_i, t_i+j\Delta t)\|^2\\
    \mathcal{L}_\textit{bc} = \frac{1}{kN_\textit{bc}}\sum_{i=1}^{N_\textit{bc}}\sum_{j=0}^{k-1} \|\mathcal{B}[\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)] - g(\boldsymbol{x}_i, t_i+j\Delta t)\|^2\\
    \mathcal{L}_\textit{ic} = \frac{1}{N_\textit{ic}} \sum_{i=1}^{N_\textit{bc}} \|\mathcal{I}[\hat{u}(\boldsymbol{x}_i,0)] - h(\boldsymbol{x}_i, 0)\|^2\\
    \mathcal{L}_{\texttt{PINNsFormer}} = \lambda_\textit{res} \mathcal{L}_\textit{res} + \lambda_\textit{ic} \mathcal{L}_\textit{ic} + \lambda_\textit{bc} \mathcal{L}_\textit{bc}
\end{gathered} 
\label{eqn:obj}
\end{equation}

where $N_\textit{res}=N_r$ refers to the residual points as in Equation \ref{eq:pinn}, $N_\textit{bc}, N_\textit{ic}$ are the number of boundary and initial points separately, and $N_\textit{bc}+N_\textit{ic}=N_b$. $\lambda_\textit{res}$, $\lambda_\textit{bc}$, and $\lambda_\textit{ic}$ are the regularization parameters that balance the emphasis of PINNsFormer's loss terms, similar to conventional PINNs loss.

In the training stage, PINNsFormer forwards all residual, boundary, and initial points to observe the corresponding sequential approximations. It then optimizes the modified PINNs loss $\mathcal{L}_\texttt{PINNsFormer}$ in Equation \ref{eqn:obj} through the gradient-based optimization algorithms, i.e., L-BFGS or Adam, to update the model parameters until convergence. In the testing stage, PINNsFormer forwards an arbitrary pair of $[\boldsymbol{x},t]$ to observe the sequential approximations, while the first element of the sequential approximation is exactly the desired approximation of $\hat{u}(\boldsymbol{x},t)$.

\subsection{Loss Landscape Analysis}

While showing the convergence or generalization bound of Transformer-based models theoretically can be ambitious, the alternative approach to analyzing its performance is by visualizing the loss landscape, which has been employed for analyzing either Transformers or PINNs~\cite{krishnapriyan2021characterizing, yao2020pyhessian, park2022vision}. The loss landscape computes the loss values by making small perturbations on the trained model across the first two dominant Hessian eigenvectors, which tends to be more informative than perturbing the model parameters in random directions. Generally, the smoother the loss landscape is, or the fewer local minimums the loss landscape presents, the easier that the model can converge to the global minima. Therefore, we visualize the loss landscape for both conventional PINNs (MLPs + \texttt{Tanh}) and PINNsFormer (Transformer + \texttt{Wavelet}), trained for the reaction-diffusion problem with coefficient $\rho=5, \nu=5$ (detailed in Appendix \ref{sec:appendb}). The visualizations are shown in Figure \ref{fig:loss}.

% Figure environment removed




It is then evident that vanilla PINNs exhibit a more complicated loss landscape than PINNsFormer. More precisely, we estimate the Lipschitz constant for both loss landscapes. We observe $L_\texttt{PINNs} =776.16$, which is much larger than $L_\texttt{PINNsFormer} = 32.79$, both in log-scale. 
Moreover, the loss landscape of conventional PINNs presents several sharp cones near its optimal, indicating multiple local minima around its convergence (zero perturbation). The rugged loss landscape and multiple local minima of conventional PINNs both indicate that optimizing the objective in Equation \ref{eqn:obj} of PINNsFormer is an easier alternative approach than conventional PINNs to reach the global minima, and hence possibly avoid PINNsâ€™ failure modes. The analysis result is further validated by the empirical experiments in the following section.



