\section{Introduction}
\label{sec:intro}

Numerically solving partial differential equations (PDEs) has been widely studied in science and engineering. The conventional approaches, such as finite element method~\cite{bathe2007finite} or pseudo-spectral method~\cite{fornberg1998practical}, suffer from high computational costs in constructing mesh for high-dimensional PDEs. With the development of recent studies in machine learning, Physics-informed neural networks (PINNs)~\cite{raissi2019physics, lagaris1998artificial}
have emerged as an alternative approach to numerically approximate PDEs. However, various recent works show that PINN may fail in various scenarios, especially when the solution contains high-frequencies or multiscale features~\cite{fuks2020limitations, raissi2018deep, krishnapriyan2021characterizing, wang2022and}. When failure modes happen, PINNs 
provide extremely smooth (or na\"ive) approximations to the true solution in general.
Current two approaches to mitigate the impact of the failure modes are data interpolations~\cite{raissi2017physics, chen2021physics, zhu2019physics}, or designing delicate training schemes, such as learning rate annealing, adaptive sampling, or adaptive loss reweighting~\cite{krishnapriyan2021characterizing, mao2020physics, wang2022and, wang2021understanding}. However, the former is infeasible when estimating the ground truth (or solving PDEs) is difficult; and 
the later approach is generally computationally costly in practice. 

While majority of the approaches to target failure modes focus on the aspects above, upgrading PINN to state-of-the-art architectures are neglected.
 In particular, Transformer~\cite{vaswani2017attention} and its variations have shown superior performance in capturing long-term dependencies than conventional deep learning models. Variational transformer-based models exhibit success in multiple domains, including computer vision, natural language processing, time-series forecasting, etc~\cite{liu2021swin, dosovitskiy2020image, devlin2018bert, zhou2021informer}. Transformer-based models produce accurate approximations by relying on learning the temporal dependency through multi-head self-attentions and encoder-decoder attentions. Intuitively, the mechanism of capturing temporal dependencies and utilizing such dependencies in approximation can also be beneficial for PINNs, since most PDEs are time-dependent. For instance, the state of a PDE at a time step can determine its incoming states (so is leveraged by the Finite Element Method), Inversely, giving the states from later time steps, the states of earlier time steps should also be deterministic.  

 We therefore propose PINNsFormer, with a novel PDE solver based on Transformer. In addition to the motivation of improving PINNs' model architectures, PINNsFormer also aims to generate robust approximations of various PDEs, including the PDEs where vanilla PINNs suffer from failure modes, such as convection and reaction-diffusion problems~\cite{krishnapriyan2021characterizing, mcclenny2020self}. Hence, we summarize PINNsFormer as a robust Transformer-based neural network for solving PINNs. Additionally, PINNsFormer improves PINNs' performance through only the model architecture's perspective, which leaves the flexibility to combine with other existing methodologies, such as data interpolation or learning rate annealing, etc., which is expected to further improve the performance of PINNs.

\textbf{Main Contributions.} The major contributions of this paper are as follows:
\begin{itemize} 
    \item We propose a novel and simple Transformer-based framework, PINNsFormer, which resolves the failure of PINNs by capturing the temporal pattern over the generated pseudo sequences.

    \item Within PINNsFormer, we propose a novel non-linear activation function $\texttt{Wavelet}$. $\texttt{Wavelet}$ aims to anticipate the Fourier Transform for arbitrary target signals, and hence is a universal approximator of infinite-width neural networks.  $\texttt{Wavelet}$ can potentially be beneficial for various deep learning tasks with other model architectures.
    \item We evaluate PINNsFormer on various PDEs, particularly on PDEs that conventional PINNs fail to learn. We show the benefit of PINNsFormer for solving the failure modes of PINNs with non-sensitive hyperparameters and limited marginal computational and memory costs.
\end{itemize}

\textbf{Background overview.} Let $\Omega$ be an open set in $\mathbb{R}^d$, bounded by $\partial \Omega \in \mathbb{R}^{d-1}$. The PDEs generally fit the following abstraction:
\begin{equation}
\begin{gathered}
    \mathcal{D}[u(\boldsymbol{x},t)] = f(\boldsymbol{x},t),\:\: \forall \boldsymbol{x},t \in \Omega\\
    \mathcal{B}[u(\boldsymbol{x},t)] = g(\boldsymbol{x},t),\:\: \forall \boldsymbol{x},t \in \partial \Omega
\end{gathered}    
\end{equation}

where $u$ is the PDE's solution, $\mathcal{D}$ is the differential operator that regularizes the behavior of the system, and $\mathcal{B}$ describes the boundary or initial conditions in general. Specifically, $\{\boldsymbol{x},t\}\in\Omega$ are named as residual points, and $\{\boldsymbol{x},t\}\in\partial\Omega$ are named as boundary/initial points. Let $\hat{u}$ be an arbitrary neural network, the Physics-Informed Neural Networks describe the framework where $\hat{u}$ is empirically regularized by the following constraints:
\begin{equation}
\begin{gathered}
    \mathcal{L}_\texttt{PINNs} = \lambda_\textit{r} \sum_{i=1}^{N_\textit{r}} \|\mathcal{D}[\hat{u}(\boldsymbol{x},t)]-f(\boldsymbol{x},t)\|^2 + \lambda_b \sum_{i=1}^{N_b} \|\mathcal{B}[\hat{u}(\boldsymbol{x},t)]-g(\boldsymbol{x},t)\|^2 
\end{gathered}
\label{eq:pinn}
\end{equation}

where $N_b, N_r$ refer to the residual and boundary/initial points separately, $\lambda_r, \lambda_b$ are the regularization parameters that balance the emphasis of the loss terms. The neural network $\hat{u}$ takes vectorized $\{\boldsymbol{x},t\}$ as input and outputs the approximated solution. The goal is then to use machine learning methodologies to train the neural network $\hat{u}$ that minimizes the loss in Equation \ref{eq:pinn}.


