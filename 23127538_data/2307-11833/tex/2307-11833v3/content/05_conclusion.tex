\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduced PINNsFormer, a novel Transformer-based framework of PINNs, aimed at capturing temporal dependencies when approximating solutions to PDEs. We introduced the Pseudo Sequence Generator, a mechanism that translates vectorized inputs into pseudo time sequences and incorporated a modified Encoder-Decoder layer along with a novel \texttt{Wavelet} activation. Empirical evaluations demonstrate that PINNsFormer consistently outperforms conventional PINNs across various scenarios, including handling PINNs' failure modes, addressing high-dimensional PDEs, and integrating with different learning schemes for PINNs. Furthermore, PINNsFormer retains computational simplicity, making it a practical choice for real-world applications. 

Beyond PINNsFormer, \texttt{Wavelet} activation function can hold promises for the broader machine learning community. We provided a sketch proof demonstrating \texttt{Wavelet}'s ability to approximate arbitrary target solutions using a two-hidden-layer infinite-width neural network, leveraging the Fourier decomposition of these solutions. We encourage further exploration, both theoretically and empirically, of the \texttt{Wavelet} activation function's potential. Its applicability extends beyond PINNs and can be leveraged in various architectures and applications.

\par \noindent \textbf{Acknowledgements:} 
% We thank Aarti Singh for funding the p and the anonymous reviewers for the helpful comments. 
 This paper was supported in part by the NSF (Expeditions CCF-1918770, CAREER IIS-2028586, Medium IIS-1955883, Medium IIS-2106961, PIPP CCF-2200269), CDC MInD program, Meta faculty gift, and funds/computing resources from Georgia Tech and GTRI.
