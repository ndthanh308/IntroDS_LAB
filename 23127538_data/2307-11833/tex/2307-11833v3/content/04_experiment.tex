\section{Experiments}
\label{sec:exp}

\subsection{Setup}
\label{sec:setup}

\textbf{Goal.} Our empirical evaluations aim to demonstrate three key advantages of PINNsFormer. First, we show that PINNsFormer improves generalization abilities and mitigates failure modes compared to PINNs and variant architectures. Second, we illustrate the flexibility of PINNsFormer in incorporating various learning schemes, resulting in superior performance. Third, we provide evidence of PINNsFormer's faster convergence and improved generalization capabilities in solving high-dimensional PDEs, which can be challenging for PINNs and their variants.

% add can also use for any types of pdes, and efficient, also name variant learning schemes, improvement is significant.

\textbf{Experiment Setup.} 
% We use a selection of PDEs, including convection, 1D-reaction, 1D-wave, and Navier–Stokes PDEs. The setups for the convection, 1D-reaction, and 1D-wave equations follow past work~\citep{chen2018neural}. 
Our empirical evaluations rely on four types of PDEs: convection, 1D-reaction, 1D-wave, and Navier–Stokes PDEs, which follow the established setups of preliminary studies for fair comparisons~\citep{raissi2019physics,krishnapriyan2021characterizing,wang2022and}. We include PINNs, QRes~\citep{bu2021quadratic}, and First-Layer Sine (FLS)~\citep{wong2022learning} as baselines. For convection, 1D-reaction, and 1D-wave PDEs,
% For baseline model training, including PINNs, QRes~\citep{bu2021quadratic}, and First-Layer Sine (FLS)~\citep{wong2022learning}, 
we uniformly sampled $N_{\textit{ic}} = N_{\textit{bc}}=101$ initial and boundary points, as well as a uniform grid of $101\times 101$ mesh points for the residual domain, resulting in total $N_{\textit{res}}=10201$ points. In the case of training PINNsFormer, we reduce the collocation points, with $N_{\textit{ic}} = N_{\textit{bc}}=51$ initial and boundary points and a $51\times 51$ mesh for residual points. The reduction in fewer training samples serves two purposes: it enhances training efficiency and allows us to demonstrate the generalization capabilities of PINNsFormer with limited training data. For testing, we employed a $101\times 101$ mesh within the residual domain.
For the Navier–Stokes PDE, we sample 2500 points from the 3D mesh within the residual domain for training purposes. The evaluation was performed by testing the predicted pressure at the final time step $t=20.0$. 
% our experiment follows the established setup in~\cite{raissi2017physics}. 

\textbf{Evaluation.} For all baselines and PINNsformer, we maintain approximately close numbers of parameters across all models to highlight the advantages of PINNsFormer from its ability to capture temporal dependencies rather than relying solely on model overparameterization. We train all models using the L-BFGS optimizer with Strong Wolfe linear search for 1000 iterations. For simplicity, we set $\lambda_\textit{res}=\lambda_\textit{ic} = \lambda_\textit{bc}=1$ for the optimization objective in Equation \ref{eqn:obj}. Detailed hyperparameters are provided in Appendix \ref{sec:appenda}. We also include an ablation study on activation functions and a hyperparameter sensitivity study on the choice of $\{k,\Delta t\}$ in Appendix \ref{sec:appendc}.

In terms of evaluation metrics, we adopted commonly used metrics in related works~\citep{krishnapriyan2021characterizing, raissi2019physics, mcclenny2020self}, including the relative Mean Absolute Error (rMAE or relative $\ell_1$ error) and the relative Root Mean Square Error (rRMSE or relative $\ell_2$ error). The detailed formulations of the metrics are provided in Appendix \ref{sec:appenda}.
% \begin{equation}
% \begin{gathered}
%     \texttt{rMAE} =  \frac{\sum_{n=1}^N |\hat{u}(x_n,t_n)-u(x_n,t_n)|}{\sum_{n=1}^{N_{\textit{res}}}|u(x_n,t_n)|}\\
%     \texttt{rRMSE} = \sqrt{\frac{\sum_{n=1}^N |\hat{u}(x_n,t_n)-u(x_n,t_n)|^2}{\sum_{n=1}^N|u(x_n,t_n)|^2}}
% \end{gathered}    
% \end{equation}
% where $N$ is the number of test points, $\hat{u}$ is the neural network approximation, and $u$ is the ground truth. In addition, we include the training PINNs loss for all cases.

\textbf{Reproducibility.} All models are implemented in PyTorch~\citep{paszke2019pytorch}, and are trained separately on single NVIDIA Tesla V100 GPU. All code and demos are included and reproducible at: \url{https://github.com/AdityaLab/pinnsformer}.

\subsection{Mitigating Failure Modes of PINNs}

Our primary evaluation focuses on demonstrating the superior generalization ability of PINNsFormer in comparison to PINNs, particularly on PDEs that are known to challenge PINNs' generalization capabilities. We focus on solving two distinct types of PDEs: the convection equation and the 1D-reaction equation. These equations pose significant challenges for conventional MLP-based PINNs, often resulting in what is referred to as "PINNs failure modes"~\citep{mojgani2022lagrangian, daw2022rethinking, krishnapriyan2021characterizing}. In these failure modes, optimization gets stuck in local minima, leading to overly smooth approximations that deviate from the true solutions.

The objective of our evaluation is to showcase the enhanced generalization capabilities of PINNsFormer when compared to standard PINNs and their variations, specifically in addressing PINNs' failure modes. The evaluation results are summarized in Table \ref{tbl:main}, with detailed PDE formulations provided in Appendix \ref{sec:appendb}. We showcase the prediction and absolute error plots of PINNs and PINNsFormer on convection equation in Figure \ref{fig:case}, all prediction plots available in Appendix \ref{sec:appendc}.

{\small
\begin{table}[h]
\vspace{-0.1in}
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ccccccc}
\hline\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Convection} & \multicolumn{3}{c}{1D-Reaction} \\
                       & Loss     & rMAE     & rRMSE    & Loss     & rMAE     & rRMSE     \\ \hline
PINNs                  &0.016 &0.778 &0.840 &0.199 &0.982 &0.981\\
QRes                   &0.015 &0.746 &0.816 &0.199 &0.979 &0.977\\
FLS                    &0.012 &0.674 &0.771 &0.199 &0.984 &0.985\\
PINNsFormer            &\textbf{3.7e-5} &\textbf{0.023} &\textbf{0.027} &\textbf{3.0e-6} &\textbf{0.015} &\textbf{0.030}\\ \hline\hline

\end{tabular}
\vspace{-0.1in}
\caption{Results for solving convection and 1D-reaction equations. PINNsFormer consistently outperforms all baseline methods in terms of training loss, rMAE, and rRMSE.}
\label{tbl:main}
\vspace{-0.1in}
\end{table}
}
The evaluation results demonstrate significant outperformance of PINNsFormer over all baselines for both scenarios. PINNsFormer achieves the lowest training loss and test errors, distinguishing PINNsFormer as the only approach capable of mitigating the failure modes. In contrast, all other baseline methods remain stuck at global minima and fail to optimize the objective loss effectively. These results show the clear advantages of PINNsFormer in terms of generalization ability and approximation accuracy when compared to conventional PINNs and existing variants.
\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-0.0in}
    \begin{minipage}{\linewidth}
    \centering
    % Figure removed
    % \subcaption{a}
    % \label{fig:5a}
    % Figure removed
    % \subcaption{a}
    % \label{fig:5b}
    % Figure removed
    % \subcaption{a}
    % \label{fig:5c}
    % Figure removed
    % \subcaption{a}
    % \label{fig:5c}
\end{minipage}
\vspace{-0.2in}
\caption{Prediction (left) and absolute error (right) of PINNs (up) and PINNsFormer (bottom) on convection equation. PINNsFormer shows success in mitigating the failure mode than PINNs.}
\label{fig:case}
\vspace{-0.2in}
\end{wrapfigure}

The additional concern for PINNsFormer is its computational and memory overheads relative to PINNs. While MLP-based PINNs are known for efficiency, PINNsFormer, with Transformer-based architecture in handling sequential data, naturally incurs higher computational and memory costs. Nonetheless, our empirical evaluation indicates that the overhead is tolerable, benefitting from the reliance on only linear layers, avoiding complicated operators such as convolution or recurrent layers. For instance, when setting the pseudo-sequence length $k=5$, we observe an approximate 2.92x computational cost and a 2.15x memory usage (detailed in Appendix \ref{sec:appenda}). These overheads are reasonable in exchange for the substantial performance improvements by PINNsFormer.

% \subsection{Flexibility in Incorporating Variant Learning Schemes}
% \begin{wraptable}{r}{0.56\textwidth}
% \vspace{-0.15in}
% \centering
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{cccc}
% \hline\hline
% \multirow{2}{*}{Model} & \multicolumn{3}{c}{1D-Wave} \\
%                        & Loss    & rMAE    & rRMSE   \\ \hline
% PINNs                  &1.93e-2 &0.326 &0.335  \\
% PINNsFormer            &1.38e-2 &0.270 &0.283  \\
% PINNs + \textit{NTK}              &6.34e-3 &0.140 &0.149  \\
% PINNsFormer + \textit{NTK}        &\textbf{4.21e-3} &\textbf{0.054} &\textbf{0.058}  \\ \hline\hline
% \end{tabular}
% \caption{Results for solving the 1D-wave equation, incorporating the NTK method. PINNsFormer combined with NTK outperforms all other methods on all metrics.}
% \label{tbl:ntk}
% \vspace{-0.15in}
% \end{wraptable}

\subsection{Flexibility in Incorporating Variant Learning Schemes}

\begin{wraptable}{r}{0.56\textwidth}
\vspace{-0.15in}
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{cccc}
\hline\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c}{1D-Wave} \\
                       & Loss    & rMAE    & rRMSE   \\ \hline
PINNs                  &1.93e-2 &0.326 &0.335  \\
PINNsFormer            &1.38e-2 &0.270 &0.283  \\
PINNs + \textit{NTK}              &6.34e-3 &0.140 &0.149  \\
PINNsFormer + \textit{NTK}        &\textbf{4.21e-3} &\textbf{0.054} &\textbf{0.058}  \\ \hline\hline
\end{tabular}
\vspace{-0.1in}
\caption{Results for solving the 1D-wave equation, incorporating the NTK method. PINNsFormer combined with NTK outperforms all other methods on all metrics.}
\label{tbl:ntk}
\vspace{-0.1in}
\end{wraptable}
While PINNs and their various architectural adaptations may encounter challenges for certain scenarios, prior research has explored sophisticated optimization schemes to mitigate these issues, including learning rate annealing~\citep{wang2021understanding}, augmented Lagrangian methods~\citep{lu2021physics}, and neural tangent kernel approaches~\citep{wang2022and}. These modified PINNs have shown significant improvement of PINNs under certain scenarios. Notably, when these optimization strategies are applied to PINNsFormer, they can be easily incorporated to achieve further performance improvements. For instance, the \textit{Neural Tangent Kernel (NTK)} method to PINNs has shown success in solving the 1D-wave equation. As such, we demonstrate that when combining \textit{NTK} with PINNsFormer, we can achieve further outperformance in approximation accuracy. Detailed results are presented in Table \ref{tbl:ntk}, and comprehensive PDE formulations are available in Appendix \ref{sec:appendb} with prediction plots in Appendix \ref{sec:appendc}.

% \subsection{Flexibility in Incorporating Variant Learning Schemes}
% \begin{wraptable}{r}{0.56\textwidth}
% \vspace{-0.15in}
% \centering
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{cccc}
% \hline\hline
% \multirow{2}{*}{Model} & \multicolumn{3}{c}{1D-Wave} \\
%                        & Loss    & rMAE    & rRMSE   \\ \hline
% PINNs                  &1.93e-2 &0.326 &0.335  \\
% PINNsFormer            &1.38e-2 &0.270 &0.283  \\
% PINNs + \textit{NTK}              &6.34e-3 &0.140 &0.149  \\
% PINNsFormer + \textit{NTK}        &\textbf{4.21e-3} &\textbf{0.054} &\textbf{0.058}  \\ \hline\hline
% \end{tabular}
% \caption{Results for solving the 1D-wave equation, incorporating the NTK method. PINNsFormer combined with NTK outperforms all other methods on all metrics.}
% \label{tbl:ntk}
% \vspace{-0.15in}
% \end{wraptable}
Our evaluation results show both the flexibility and effectiveness of incorporating PINNsFormer with the \textit{NTK} method. In particular, we observe a sequence of performance improvements, from standard PINNs to PINNsFormer and from PINNs+\textit{NTK} to PINNsFormer+\textit{NTK}. Essentially, PINNsFormer explores a variant architecture of PINNs, while many learning schemes are designed from an optimization perspective and are agnostic to neural network architectures. This inherent flexibility allows for versatile combinations of PINNsFormer with various learning schemes, offering practical and customizable solutions for accurate solutions in real-world applications.

\subsection{Generalization on High-Dimensional PDEs}
\begin{wrapfigure}{r}{0.35\textwidth}
    \vspace{-0.7in}
    \centering
    % Figure removed
    \vspace{-0.3in}
    \caption{Training loss vs. Iterations of PINNs and PINNsFormer on the Navier-Stokes equation.}
    \label{fig:loss}
    \vspace{-0.3in}
\end{wrapfigure}
In the previous sections, we demonstrated the clear benefits of PINNsFormer in generalizing the solutions for PINNs failure modes. However, those PDEs often have simple analytical solutions. In practical physics systems, higher-dimensional and more complex PDEs need to be solved. Therefore, it's important to evaluate the generalization ability of PINNsFormer on such high-dimensional PDEs, especially when PINNsFormer is equipped with advanced mechanisms like self-attention. 

We evaluate the performance of PINNsFormer compared to PINNs on Navier-Stokes PDE based on the established setups~\cite{raissi2019physics}. The training loss is shown in Figure \ref{fig:loss}, and the results are shown in Table \ref{tbl:2dns}. The detailed formulations of the 2D Navier-Stokes equation can be found in Appendix \ref{sec:appendb}, and the predictions are plotted in Appendix \ref{sec:appendc}.

% \begin{wraptable}{r}{0.4\textwidth}
% \vspace{-0.2in}
%     \centering
%     \renewcommand{\arraystretch}{1.25}
%     \begin{tabular}{ccc}
%         \hline\hline
%         Model         & PINNs & PINNsFormer \\ \hline
%         Loss          &6.72e-5 &\textbf{6.66e-6}            \\
%         rMAE          &13.08 &\textbf{0.280 }           \\
%         rRMSE         &9.08 &\textbf{0.384 }           \\ \hline\hline
%     \end{tabular}
%     \caption{Results for solving 2D Navier-Stokes equation, PINNsFormer outperforms PINNs over all metrics.}`
%     \label{tbl:2dns}
%     \vspace{-0.2in}
% \end{wraptable}
% \begin{table}[h]
\begin{wraptable}{r}{0.5\textwidth}
\vspace{-0.15in}
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{cccc}
\hline\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Navier-Stokes} \\
                       & Loss    & rMAE    & rRMSE   \\ \hline
PINNs                  &6.72e-5 &13.08 &9.08  \\
QRes            &2.24e-4 &6.41 &4.45  \\
FLS              &9.54e-6 &3.98 &2.77  \\
PINNsFormer        &\textbf{6.66e-6} &\textbf{0.384} &\textbf{0.280}  \\ \hline\hline
\end{tabular}
\vspace{-0.1in}
\caption{Results for solving Navier-Stokes equation, PINNsFormer outperforms all baselines on all metrics.}
\label{tbl:2dns}
\vspace{-0.1in}
\end{wraptable}
% \end{table}

The evaluation results demonstrate clear advantages of PINNsFormer over PINNs on high-dimensional PDEs. Firstly, PINNsFormer outperforms PINNs and their MLP-variants in terms of both training loss and validation errors. Firstly, PINNsFormer exhibits significantly faster convergence during training, which compensates for the higher computational cost per iteration. Secondly, while PINNs and their MLP-variants predict the pressure with good shapes, they exhibit increasing magnitude discrepancies as time increases. In contrast, PINNsFormer consistently aligns both the shape and magnitude of predicted pressures across various time intervals. This consistency is attributed to PINNsFormer's ability to learn temporal dependencies through Transformer-based model architecture and self-attention mechanism.
