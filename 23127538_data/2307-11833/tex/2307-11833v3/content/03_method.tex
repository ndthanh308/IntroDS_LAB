\section{Methodology}
\label{sec:method}
% \aditya{in general, you are citing Vaswani et al too many times in this section. you need to cite it 1-2 times, remove other citations..also you have said PINNs focus on point predictions too many times as well. try to reduce that phrase in this section}

\textbf{Preliminaries:} Let $\Omega$ be an open set in $\mathbb{R}^d$, bounded by $\partial \Omega \in \mathbb{R}^{d-1}$. The PDEs with spatial input $\boldsymbol{x}$ and temporal input $t$ generally fit the following abstraction:
\begin{equation}
\begin{gathered}
    \mathcal{D}[u(\boldsymbol{x},t)] = f(\boldsymbol{x},t),\:\: \forall \boldsymbol{x},t \in \Omega\\
    \mathcal{B}[u(\boldsymbol{x},t)] = g(\boldsymbol{x},t),\:\: \forall \boldsymbol{x},t \in \partial \Omega
\end{gathered}    
\end{equation}
where $u$ is the PDE's solution, $\mathcal{D}$ is the differential operator that regularizes the behavior of the system, and $\mathcal{B}$ describes the boundary or initial conditions in general. Specifically, $\{\boldsymbol{x},t\}\in\Omega$ are residual points, and $\{\boldsymbol{x},t\}\in\partial\Omega$ are boundary/initial points. Let $\hat{u}$ be neural network approximations, PINNs describe the framework where $\hat{u}$ is empirically regularized by the following constraints:
\begin{equation}
\begin{gathered}
    \mathcal{L}_\texttt{PINNs} = \lambda_\textit{r} \sum_{i=1}^{N_\textit{r}} \|\mathcal{D}[\hat{u}(\boldsymbol{x},t)]-f(\boldsymbol{x},t)\|^2 + \lambda_b \sum_{i=1}^{N_b} \|\mathcal{B}[\hat{u}(\boldsymbol{x},t)]-g(\boldsymbol{x},t)\|^2 
\end{gathered}
\label{eq:pinn}
\end{equation}
where $N_b, N_r$ refer to the residual and boundary/initial points separately, $\lambda_r, \lambda_b$ are the regularization parameters that balance the emphasis of the loss terms. The neural network $\hat{u}$ takes vectorized $\{\boldsymbol{x},t\}$ as input and outputs the approximated solution. The goal is then to use machine learning methodologies to train the neural network $\hat{u}$ that minimizes the loss in Equation \ref{eq:pinn}.


\par \noindent \textbf{Methodology Overview:} While PINNs focus on point-to-point predictions, the exploration of temporal dependencies in real-world physics systems has been merely neglected. Conventional PINNs methods employ a single pair of spatial information $\boldsymbol{x}$ and temporal information $t$ to approximate the numerical solution $u(\boldsymbol{x},t)$, without accounting for temporal dependencies across previous or subsequent time steps. However, this simplification is only applicable to elliptic PDEs, where the relationships between unknown functions and their derivatives do not explicitly involve time. In contrast, hyperbolic and parabolic PDEs incorporate time derivatives, implying that the state at one time step can influence states at preceding or subsequent time steps. Consequently, considering temporal dependencies is crucial to effectively address these PDEs using PINNs.

In this section, we introduce a novel framework featuring a Transformer-based model of PINNs, namely PINNsFormer. Unlike point-to-point predictions, PINNsFormer extends PINNs' capabilities to sequential predictions. PINNsFormer allows accurately approximating solutions at specific time steps while also learning and regularizing temporal dependencies among incoming states. The framework consists of four components: Pseudo Sequence Generator, Spatio-Temporal Mixer, Encoder-Decoder with multi-head attention, and an Output Layer. Additionally, we introduce a novel activation function, named \texttt{Wavelet}, which employs Real Fourier Transform techniques to anticipate solutions to PDEs. The framework diagram is exhibited in Figure \ref{fig:arch}. We provide detailed explanations of each framework component and learning schemes in the following subsections.

% Figure environment removed


\subsection{Pseudo Sequence Generator}
% \aditya{the spatio temporal inputs come suddenly...you never mention 'spatial' and suddenly you are talking about spatial inputs...also saw some typos in the next few paras}
While Transformers and Transformer-based models are designed to capture long-term dependencies in sequential data, conventional PINNs utilize non-sequential data as inputs for neural networks. Consequently, to incorporate PINNs with Transformer-based models, it is essential to transform the pointwise spatiotemporal inputs into temporal sequences. Thus, for a given spatial input $\boldsymbol{x}\in \mathbb{R}^{d-1}$ and temporal input $t\in \mathbb{R}$, the Pseudo Sequence Generator performs the following operations:
\begin{equation}
    [\boldsymbol{x},t] \xRightarrow{\texttt{Generator}} \{[\boldsymbol{x},t], [\boldsymbol{x},t+\Delta t], \ldots, [\boldsymbol{x},t+(k-1)\Delta t]\} 
\end{equation}
where $[\cdot]$ is the concatenation operation, such that $[\boldsymbol{x},t]\in \mathbb{R}^d$ is vectorized, and the generator outputs the pseudo sequence in the shape of $\mathbb{R}^{k\times d}$. The Pseudo Sequence Generator extrapolates sequential time series by extending a single spatiotemporal input to multiple isometric discrete time steps.
$k$ and $\Delta t$ are hyperparameters, which intuitively determine how many steps the pseudo sequence needs to `look ahead' and how `far' each step should be. In practice, both $k$ and $\Delta t$ should not be set to very large scales, as larger $k$ can cause heavy computational and memory overheads, while larger $\Delta t$ may undermine the time dependency relationships of neighboring discrete time steps. 

\subsection{Model Architecture}
% \aditya{are you sure you want to keep model architecture after sec 3.1? }

In addition to the Pseudo Sequence Generator, PINNsFormer consists of three components of its architecture: Sptio-Temporal Mixer, Encoder-Decoder with multi-head attentions, and Output Layer. The Output Layer is straightforward to interpret as a fully-connected MLP appended to the end. We provide detailed insights into the first two components below. Notably, PINNsFormer relies only on linear layers and non-linear activations, avoiding complex operations such as convolutional or recurrent layers. This design preserves PINNsFormer's computational efficiency in practice.

\textbf{Spatio-Temporal Mixer.} Most PDEs contain low-dimensional spatial or temporal information. Directly feeding low-dimensional data to encoders may fail to capture the complex relationships between each feature dimension. Hence, it is necessary to embed original sequential data in higher-dimensional spaces such that more information is encoded into each vector.

Instead of embedding raw data in a high-dimensional space where the distance between vectors reflects the semantic similarity~\citep{vaswani2017attention, devlin2018bert}, PINNsFormer constructs a linear projection that maps spatiotemporal inputs onto a higher-dimensional space using a fully-connected MLP. The embedded data enriches the capability of information by mixing all raw spatiotemporal features together, so-called the linear projection Spatio-Temporal Mixer.

\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-0.2in}
    \begin{center}
        % Figure removed
    \end{center}
    \caption{The architecture of PINNsFormer's Encoder-Decoder Layers. The decoder is not equipped with self-attentions.}
    \label{fig:en_de}
    \vspace{-0.2in}
\end{wrapfigure}

\textbf{Encoder-Decoder Architecture.} PINNsFormer employs an encoder-decoder architecture similar to Transformer. The encoder consists of a stack of identical layers, each of which contains an encoder self-attention layer and a feedforward layer. The decoder is slightly different from the vanilla Transformer, where each of the identical layers contains only an encoder-decoder self-attention layer and a feedforward layer. At the decoder level, PINNsFormer uses the same spatiotemporal embeddings as the encoder. Therefore, the decoder does not need to relearn dependencies for the same input embeddings. The diagram for the encoder-decoder architecture is shown in Figure \ref{fig:en_de}

Intuitively, the encoder self-attentions allow learning the dependency relationships of all spatiotemporal information. The decoder encoder-decoder attentions allow selectively focusing on specific dependencies within the input sequence during the decoding process, enabling it to capture more information than conventional PINNs.  We use the same embeddings for the encoder and decoder since PINNs focus on approximating the solution of the \textit{current} state, in contrast to \textit{next} state prediction in language tasks or time series forecastings.

\subsection{Wavelet Activation}

While Transformers typically employ \texttt{LayerNorm} and \texttt{ReLU} non-linear activation functions~\citep{vaswani2017attention, gehring2017convolutional, devlin2018bert}, these activation functions might not always be suitable in solving PINNs. In particular, employing \texttt{ReLU} activation in PINNs can result in poor performance, whose effectiveness relies heavily on the accurate evaluation of derivatives while \texttt{ReLU} has a discontinuous derivative~\citep{haghighat2021physics, de2021assessing}. Recent studies utilize \texttt{Sin} activation for specific scenarios to mimic the periodic properties of PDEs' solutions~\citep{li2020fourier, jagtap2020adaptive, song2022versatile}. However, it requires strong prior knowledge of the solution's behavior and is limited in its applicability. Tackling this issue, we proposed a novel and simple activation function, namely \texttt{Wavelet}, defined as follows:
\begin{equation}
    \texttt{Wavelet}(\boldsymbol{x}) = \omega_1\sin(\boldsymbol{x})+\omega_2\cos(\boldsymbol{x})
\end{equation}
Where $\omega_1$ and $\omega_2$ are registered learnable parameters. The intuition behind \texttt{Wavelet} activation simply follows Real Fourier Transform: While periodic signals can be decomposed into an integral of sines of multiple frequencies, all signals, whether periodic or aperiodic, can be decomposed into an integral of sines and cosines of varying frequencies. It is evident that \texttt{Wavelet} can approximate arbitrary functions giving sufficient approximation power, which leads to the following proposition:
\begin{proposition}
    Let $\mathcal{N}$ be a two-hidden-layer neural network with infinite width, equipped with \texttt{Wavelet} activation function, then $\mathcal{N}$ is a universal approximator for any real-valued target f.
\end{proposition}
\textit{Proof sketch:} The proof follows the Real Fourier Transform (Fourier Integral Transform). For any given input $x$ and its corresponding real-valued target $f(x)$, it has the Fourier Integral:
\begin{gather*}
    f(x) = \int_{-\infty}^{\infty} F_c(\omega) \cos(\omega x) \, d\omega + \int_{-\infty}^{\infty} F_s(\omega) \sin(\omega x) \, d\omega
\end{gather*}
where $F_c$ and $F_s$ are the coefficients of Sines and Cosines respectively. Second, by Riemann sum approximation, the integral can be approximated by the infinite sum such that:
\begin{gather*}
    f(x) \approx \sum_{n=1}^{N} \left[ F_c(\omega_n) \cos(\omega_n x) + F_s(\omega_n) \sin(\omega_n x) \right] \equiv W_2(\texttt{Wavelet}(W_1 x))
\end{gather*}
where $W_1$ and $W_2$ are the weights of $\mathcal{N}$'s first and second hidden layer. As $W_1$ and $W_2$ are infinite-width, we can divide the piecewise summation into infinitely small intervals, making the approximation arbitrarily close to the true integral. Hence, $\mathcal{N}$ is a universal approximator for any given $f$. In practice, most PDE solutions contain only a finite number of major frequencies. Using a neural network with finite parameters would also lead to proper approximations of the true solutions.

Although \texttt{Wavelet} activation function is primarily employed by PINNsFormer to improve PINNs in our work, it may have potential applications in other deep-learning tasks. Similar to \texttt{ReLU}, $\sigma(\cdot)$, and \texttt{Tanh} activations, which all turn infinite-width two-hidden-layer neural networks into universal approximators~\citep{cybenko1989approximation, hornik1991approximation, glorot2011deep}, we anticipate that \texttt{Wavelet} can demonstrate its effectiveness in other applications beyond the scope of this work.

% \textcolor{blue}{Maybe a short proposition: Given a 2-layer linear layer with infinite width of hidden, and wavelet activation, it can approximate any solutions.}

\subsection{Learning Scheme}

While conventional PINNs focus on point-to-point predictions, adapting PINNs to handle pseudo-sequential inputs has not been explored. In PINNsFormer, each generated point in the sequence, i.e., $[\boldsymbol{x}_i,t_i+j\Delta t]$, is mapped to the corresponding approximation, i.e., $\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)$ for any $j\in\mathbb{N}, j<k$. This approach allows us to compute the $n$th-order gradients with respect to $\boldsymbol{x}$ or $t$ independently for any valid $n$. For instance, for any given input pseudo sequence $\{[\boldsymbol{x}_i,t_i], [\boldsymbol{x}_i,t_i+\Delta t], \ldots, [\boldsymbol{x}_i,t_i+(k-1)\Delta t]\}$, and the corresponding approximations $\{\hat{u}(\boldsymbol{x}_i,t_i), \hat{u}(\boldsymbol{x}_i,t_i+\Delta t), \ldots, \hat{u}(\boldsymbol{x}_i,t_i+(k-1)\Delta t)\}$, we can compute the first-order derivatives w.r.t. $\boldsymbol{x}$ and $t$ separately as follows:
\begin{equation}
\begin{gathered}
    \frac{\partial\{\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)\}_{j=0}^{k-1}}{\partial\{t_i+j\Delta t\}_{j=0}^{k-1}} = \{\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i)}{\partial t_i},\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+\Delta t)}{\partial(t_i+\Delta t)},\ldots, \frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+(k-1)\Delta t)}{\partial(t_i+(k-1)\Delta t)}\} \\
     \frac{\partial\{\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)\}_{j=0}^{k-1}}{\partial\boldsymbol{x}_i} = \{\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i)}{\partial\boldsymbol{x}_i},\frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+\Delta t)}{\partial\boldsymbol{x}_i},\ldots, \frac{\partial\hat{u}(\boldsymbol{x}_i,t_i+(k-1)\Delta t)}{\partial\boldsymbol{x}_i}\}
\end{gathered}
\end{equation}
This scheme for calculating the gradients of sequential approximations with respect to sequential inputs can be easily extended to higher-order derivatives and is applicable to residual, boundary, and initial points.
However, unlike the general PINNs optimization objective in Equation \ref{eq:pinn}, which combines initial and boundary condition objectives, PINNsFormer distinguishes between the two and applies different regularization schemes to initial and boundary conditions through its learning scheme. For residual and boundary points, all sequential outputs can be regularized using the PINNs loss. This is because all generated pseudo-timesteps are within the same domain as their original inputs. For example, if $[\boldsymbol{x}_i,t_i]$ is sampled from the boundary, then $[\boldsymbol{x}_i, t_i+j\Delta t]$ also lies on the boundary for any $j\in \mathbb{N}^+$. In contrast, for initial points, only the $t=0$ condition is regularized, corresponding to the first element of the sequential outputs. This is because only the first element of the pseudo-sequence exactly matches the initial condition at $t=0$. All other generated time steps have $t=j\Delta t$ for any $j\in\mathbb{N}^+$, which fall outside the initial conditions. 

By these considerations, we adapt the PINNs loss to the sequential version, as described below:
\begin{equation}
\begin{gathered}
    \mathcal{L}_\textit{res} = \frac{1}{kN_\textit{res}}\sum_{i=1}^{N_\textit{res}}\sum_{j=0}^{k-1} \|\mathcal{D}[\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)] - f(\boldsymbol{x}_i, t_i+j\Delta t)\|^2\\
    \mathcal{L}_\textit{bc} = \frac{1}{kN_\textit{bc}}\sum_{i=1}^{N_\textit{bc}}\sum_{j=0}^{k-1} \|\mathcal{B}[\hat{u}(\boldsymbol{x}_i,t_i+j\Delta t)] - g(\boldsymbol{x}_i, t_i+j\Delta t)\|^2\\
    \mathcal{L}_\textit{ic} = \frac{1}{N_\textit{ic}} \sum_{i=1}^{N_\textit{bc}} \|\mathcal{I}[\hat{u}(\boldsymbol{x}_i,0)] - h(\boldsymbol{x}_i, 0)\|^2\\
    \mathcal{L}_{\texttt{PINNsFormer}} = \lambda_\textit{res} \mathcal{L}_\textit{res} + \lambda_\textit{ic} \mathcal{L}_\textit{ic} + \lambda_\textit{bc} \mathcal{L}_\textit{bc}
\end{gathered} 
\label{eqn:obj}
\end{equation}
where $N_\textit{res}=N_r$ refers to the residual points as in Equation \ref{eq:pinn}, $N_\textit{bc}, N_\textit{ic}$ represent the number of boundary and initial points, respectively, with $N_\textit{bc}+N_\textit{ic}=N_b$. $\lambda_\textit{res}$, $\lambda_\textit{bc}$, and $\lambda_\textit{ic}$ are regularization weights that balance the importance of the loss terms in PINNsFormer, similar to the PINNs loss.

During training, PINNsFormer forwards all residual, boundary, and initial points to obtain their corresponding sequential approximations. It then optimizes the modified PINNs loss $\mathcal{L}_\texttt{PINNsFormer}$ in Equation \ref{eqn:obj} using gradient-based optimization algorithms such as L-BFGS or Adam, updating the model parameters until convergence. In the testing phase, PINNsFormer forwards any arbitrary pair $[\boldsymbol{x},t]$ to observe the sequential approximations, where the first element of the sequential approximation corresponds exactly to the desired value of $\hat{u}(\boldsymbol{x},t)$.

\subsection{Loss Landscape Analysis}

\begin{wrapfigure}{l}{0.58\linewidth}
\vspace{-0.12in}
    \centering
    \hspace{-0.4in}
    \begin{subfigure}
        \centering
        % Figure removed
    \end{subfigure}
    \hfill
    \hspace{-0.6in}
    \begin{subfigure}
        \centering
        % Figure removed
    \end{subfigure}
    \hfill
    \hspace{-0.2in}
    \vspace{-0.3in}
    \caption{Visualization of the loss landscape for PINNs (left) and PINNsFormer (right) on a logarithmic scale. The loss landscape of PINNsFormer is significantly smoother than conventional PINNs.}
    \vspace{-0.in}
    \label{fig:loss}
\end{wrapfigure}
While achieving theoretical convergence or establishing generalization bounds for Transformer-based models can be challenging, an alternative approach to assess optimization trajectory is through visualization of the loss landscape. This approach has been employed in the analysis of both Transformers and PINNs~\citep{krishnapriyan2021characterizing, yao2020pyhessian, park2022vision}. The loss landscape is constructed by perturbing the trained model along the directions of the first two dominant Hessian eigenvectors. This technique is more informative than random parameter perturbations. Generally, a smoother loss landscape with fewer local minima indicates an easier convergence to the global minimum. We visualize the loss landscape for both PINNs and PINNsFormer. The visualizations are presented in Figure \ref{fig:loss}.

The visualizations clearly reveal that PINNs exhibit a more complicated loss landscape than PINNsFormer. To be specific, we estimate the Lipschitz constant for both loss landscapes. We find that $L_\texttt{PINNs} =776.16$, which is significantly larger than $L_\texttt{PINNsFormer} = 32.79$.  
Furthermore, the loss landscape of PINNs exhibits several sharp cones near its optimal point, indicating the presence of multiple local minima in close proximity to the convergence point (zero perturbation). The rugged loss landscape and multiple local minima of conventional PINNs suggest that optimizing the objective described in Equation \ref{eqn:obj} for PINNsFormer offers an easier path to reach the global minimum. This implies that PINNsFormer has advantages in avoiding the failure modes associated with PINNs. The analysis is further validated by empirical experiments, as shown in the following section.