\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bathe(2007)]{bathe2007finite}
Klaus-J{\"u}rgen Bathe.
\newblock Finite element method.
\newblock \emph{Wiley encyclopedia of computer science and engineering}, pp.\  1--12, 2007.

\bibitem[Bu \& Karpatne(2021)Bu and Karpatne]{bu2021quadratic}
Jie Bu and Anuj Karpatne.
\newblock Quadratic residual networks: A new class of neural networks for solving forward and inverse problems in physics involving pdes.
\newblock In \emph{Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)}, pp.\  675--683. SIAM, 2021.

\bibitem[Carleo et~al.(2019)Carleo, Cirac, Cranmer, Daudet, Schuld, Tishby, Vogt-Maranto, and Zdeborov{\'a}]{carleo2019machine}
Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborov{\'a}.
\newblock Machine learning and the physical sciences.
\newblock \emph{Reviews of Modern Physics}, 91\penalty0 (4):\penalty0 045002, 2019.

\bibitem[Chen et~al.(2021)Chen, Liu, and Sun]{chen2021physics}
Zhao Chen, Yang Liu, and Hao Sun.
\newblock Physics-informed learning of governing equations from scarce data.
\newblock \emph{Nature communications}, 12\penalty0 (1):\penalty0 6136, 2021.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0 (4):\penalty0 303--314, 1989.

\bibitem[Daw et~al.(2022)Daw, Bu, Wang, Perdikaris, and Karpatne]{daw2022rethinking}
Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne.
\newblock Rethinking the importance of sampling in physics-informed neural networks.
\newblock \emph{arXiv preprint arXiv:2207.02338}, 2022.

\bibitem[de~Wolff et~al.(2021)de~Wolff, Carrillo, Mart{\'\i}, and Sanchez-Pi]{de2021assessing}
Taco de~Wolff, Hugo Carrillo, Luis Mart{\'\i}, and Nayat Sanchez-Pi.
\newblock Assessing physics informed neural networks in ocean modelling and climate change applications.
\newblock In \emph{AI: Modeling Oceans and Climate Change Workshop at ICLR 2021}, 2021.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Fornberg(1998)]{fornberg1998practical}
Bengt Fornberg.
\newblock \emph{A practical guide to pseudospectral methods}.
\newblock Number~1. Cambridge university press, 1998.

\bibitem[Fuks \& Tchelepi(2020)Fuks and Tchelepi]{fuks2020limitations}
Olga Fuks and Hamdi~A Tchelepi.
\newblock Limitations of physics informed machine learning for nonlinear two-phase transport in porous media.
\newblock \emph{Journal of Machine Learning for Modeling and Computing}, 1\penalty0 (1), 2020.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and Dauphin]{gehring2017convolutional}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{International conference on machine learning}, pp.\  1243--1252. PMLR, 2017.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot2011deep}
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
\newblock Deep sparse rectifier neural networks.
\newblock In \emph{Proceedings of the fourteenth international conference on artificial intelligence and statistics}, pp.\  315--323. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Gulati et~al.(2020)Gulati, Qin, Chiu, Parmar, Zhang, Yu, Han, Wang, Zhang, Wu, et~al.]{gulati2020conformer}
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu~Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et~al.
\newblock Conformer: Convolution-augmented transformer for speech recognition.
\newblock \emph{arXiv preprint arXiv:2005.08100}, 2020.

\bibitem[Haghighat et~al.(2021)Haghighat, Raissi, Moure, Gomez, and Juanes]{haghighat2021physics}
Ehsan Haghighat, Maziar Raissi, Adrian Moure, Hector Gomez, and Ruben Juanes.
\newblock A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering}, 379:\penalty0 113741, 2021.

\bibitem[Han et~al.(2018)Han, Jentzen, and E]{han2018solving}
Jiequn Han, Arnulf Jentzen, and Weinan E.
\newblock Solving high-dimensional partial differential equations using deep learning.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0 (34):\penalty0 8505--8510, 2018.

\bibitem[Hornik(1991)]{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Jagtap et~al.(2020)Jagtap, Kawaguchi, and Karniadakis]{jagtap2020adaptive}
Ameya~D Jagtap, Kenji Kawaguchi, and George~Em Karniadakis.
\newblock Adaptive activation functions accelerate convergence in deep and physics-informed neural networks.
\newblock \emph{Journal of Computational Physics}, 404:\penalty0 109136, 2020.

\bibitem[Krishnapriyan et~al.(2021)Krishnapriyan, Gholami, Zhe, Kirby, and Mahoney]{krishnapriyan2021characterizing}
Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael~W Mahoney.
\newblock Characterizing possible failure modes in physics-informed neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 26548--26560, 2021.

\bibitem[Lagaris et~al.(1998)Lagaris, Likas, and Fotiadis]{lagaris1998artificial}
Isaac~E Lagaris, Aristidis Likas, and Dimitrios~I Fotiadis.
\newblock Artificial neural networks for solving ordinary and partial differential equations.
\newblock \emph{IEEE transactions on neural networks}, 9\penalty0 (5):\penalty0 987--1000, 1998.

\bibitem[Li et~al.(2020)Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{li2020fourier}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Fourier neural operator for parametric partial differential equations.
\newblock \emph{arXiv preprint arXiv:2010.08895}, 2020.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  10012--10022, 2021.

\bibitem[Lou et~al.(2021)Lou, Meng, and Karniadakis]{lou2021physics}
Qin Lou, Xuhui Meng, and George~Em Karniadakis.
\newblock Physics-informed neural networks for solving forward and inverse flow problems via the boltzmann-bgk formulation.
\newblock \emph{Journal of Computational Physics}, 447:\penalty0 110676, 2021.

\bibitem[Lu et~al.(2021)Lu, Pestourie, Yao, Wang, Verdugo, and Johnson]{lu2021physics}
Lu~Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven~G Johnson.
\newblock Physics-informed neural networks with hard constraints for inverse design.
\newblock \emph{SIAM Journal on Scientific Computing}, 43\penalty0 (6):\penalty0 B1105--B1132, 2021.

\bibitem[Mao et~al.(2020)Mao, Jagtap, and Karniadakis]{mao2020physics}
Zhiping Mao, Ameya~D Jagtap, and George~Em Karniadakis.
\newblock Physics-informed neural networks for high-speed flows.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering}, 360:\penalty0 112789, 2020.

\bibitem[McClenny \& Braga-Neto(2020)McClenny and Braga-Neto]{mcclenny2020self}
Levi McClenny and Ulisses Braga-Neto.
\newblock Self-adaptive physics-informed neural networks using a soft attention mechanism.
\newblock \emph{arXiv preprint arXiv:2009.04544}, 2020.

\bibitem[Mojgani et~al.(2022)Mojgani, Balajewicz, and Hassanzadeh]{mojgani2022lagrangian}
Rambod Mojgani, Maciej Balajewicz, and Pedram Hassanzadeh.
\newblock Lagrangian pinns: A causality-conforming solution to failure modes of physics-informed neural networks.
\newblock \emph{arXiv preprint arXiv:2205.02902}, 2022.

\bibitem[Park \& Kim(2022)Park and Kim]{park2022vision}
Namuk Park and Songkuk Kim.
\newblock How do vision transformers work?
\newblock \emph{arXiv preprint arXiv:2202.06709}, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever, et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Raissi(2018)]{raissi2018deep}
Maziar Raissi.
\newblock Deep hidden physics models: Deep learning of nonlinear partial differential equations.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0 (1):\penalty0 932--955, 2018.

\bibitem[Raissi et~al.(2017)Raissi, Perdikaris, and Karniadakis]{raissi2017physics}
Maziar Raissi, Paris Perdikaris, and George~Em Karniadakis.
\newblock Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations.
\newblock \emph{arXiv preprint arXiv:1711.10561}, 2017.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and Karniadakis]{raissi2019physics}
Maziar Raissi, Paris Perdikaris, and George~E Karniadakis.
\newblock Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.
\newblock \emph{Journal of Computational physics}, 378:\penalty0 686--707, 2019.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Song et~al.(2022)Song, Alkhalifah, and Waheed]{song2022versatile}
Chao Song, Tariq Alkhalifah, and Umair~Bin Waheed.
\newblock A versatile framework to solve the helmholtz equation using physics-informed neural networks.
\newblock \emph{Geophysical Journal International}, 228\penalty0 (3):\penalty0 1750--1762, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2021)Wang, Teng, and Perdikaris]{wang2021understanding}
Sifan Wang, Yujun Teng, and Paris Perdikaris.
\newblock Understanding and mitigating gradient flow pathologies in physics-informed neural networks.
\newblock \emph{SIAM Journal on Scientific Computing}, 43\penalty0 (5):\penalty0 A3055--A3081, 2021.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Yu, and Perdikaris]{wang2022and}
Sifan Wang, Xinling Yu, and Paris Perdikaris.
\newblock When and why pinns fail to train: A neural tangent kernel perspective.
\newblock \emph{Journal of Computational Physics}, 449:\penalty0 110768, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Han, Chang, Zha, Braga-Neto, and Hu]{wang2022auto}
Yicheng Wang, Xiaotian Han, Chia-Yuan Chang, Daochen Zha, Ulisses Braga-Neto, and Xia Hu.
\newblock Auto-pinn: Understanding and optimizing physics-informed neural architecture.
\newblock \emph{arXiv preprint arXiv:2205.13748}, 2022{\natexlab{b}}.

\bibitem[Wong et~al.(2022)Wong, Ooi, Gupta, and Ong]{wong2022learning}
Jian~Cheng Wong, Chinchun Ooi, Abhishek Gupta, and Yew-Soon Ong.
\newblock Learning in sinusoidal spaces with physics-informed neural networks.
\newblock \emph{IEEE Transactions on Artificial Intelligence}, 2022.

\bibitem[Yang et~al.(2020)Yang, Zhang, and Karniadakis]{yang2020physics}
Liu Yang, Dongkun Zhang, and George~Em Karniadakis.
\newblock Physics-informed generative adversarial networks for stochastic differential equations.
\newblock \emph{SIAM Journal on Scientific Computing}, 42\penalty0 (1):\penalty0 A292--A317, 2020.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael~W Mahoney.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE international conference on big data (Big data)}, pp.\  581--590. IEEE, 2020.

\bibitem[Zhao et~al.(2022)Zhao, Ding, Atulya, Davis, and Singh]{zhao2022physics}
Zhiyuan Zhao, Xueying Ding, Gopaljee Atulya, Alex Davis, and Aarti Singh.
\newblock Physics informed machine learning with misspecified priors:$\backslash$$\backslash$an analysis of turning operation in lathe machines.
\newblock In \emph{AAAI 2022 Workshop on AI for Design and Manufacturing (ADAM)}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang]{zhou2021informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pp.\  11106--11115, 2021.

\bibitem[Zhu et~al.(2019)Zhu, Zabaras, Koutsourelakis, and Perdikaris]{zhu2019physics}
Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris.
\newblock Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data.
\newblock \emph{Journal of Computational Physics}, 394:\penalty0 56--81, 2019.

\end{thebibliography}
