{
  "title": "PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks",
  "authors": [
    "Zhiyuan Zhao",
    "Xueying Ding",
    "B. Aditya Prakash"
  ],
  "submission_date": "2023-07-21T18:06:27+00:00",
  "revised_dates": [
    "2023-10-05T00:04:37+00:00",
    "2024-05-08T00:41:35+00:00"
  ],
  "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions to partial differential equations (PDEs). However, conventional PINNs, relying on multilayer perceptrons (MLP), neglect the crucial temporal dependencies inherent in practical physics systems and thus fail to propagate the initial condition constraints globally and accurately capture the true solutions under various scenarios. In this paper, we introduce a novel Transformer-based framework, termed PINNsFormer, designed to address this limitation. PINNsFormer can accurately approximate PDE solutions by utilizing multi-head attention mechanisms to capture temporal dependencies. PINNsFormer transforms point-wise inputs into pseudo sequences and replaces point-wise PINNs loss with a sequential loss. Additionally, it incorporates a novel activation function, Wavelet, which anticipates Fourier decomposition through deep neural networks. Empirical results demonstrate that PINNsFormer achieves superior generalization ability and accuracy across various scenarios, including PINNs failure modes and high-dimensional PDEs. Moreover, PINNsFormer offers flexibility in integrating existing learning schemes for PINNs, further enhancing its performance.",
  "categories": [
    "cs.CE",
    "cs.LG"
  ],
  "primary_category": "cs.CE",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.11833",
  "pdf_url": null,
  "comment": "17 pages (including 9 pages of main text, 3 pages of references, and 5 pages of appendix), 9 figures, 7 tables",
  "num_versions": null,
  "size_before_bytes": 6613254,
  "size_after_bytes": 512928
}