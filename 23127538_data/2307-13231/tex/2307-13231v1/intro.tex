\vspace{-3pt}
\section{Introduction}\label{sec:introduction}
\vspace{-3pt}
% In this work, a new differentially private learning algorithm for deep learning is proposed. The key idea is the compression in Fourier domain reduce the affect of the differentially private noise. We combine pruning and differentially private noisy addition by a parameter that controls the tradeoff between the utility and privacy. We implement our Fourier differentially private perturbation on three benchmark datasets in the task of image classification. The numerical result show that the proposed perturbation outperforms than the DP-SGD.
Deep Learning algorithms have had tremendous success in a variety of domains in the last several years, due to their ability to extract inferences from data that aid in a variety of tasks. %where traditional domain specific approaches hit barriers that are computational, scientific or mechanistic. 
Deep learning, however, requires substantial training of several layers densely populated with weight vectors, which is enabled by large datasets often containing sensitive information. As a result, the learned models can be exploited by adversaries to extract the sensitive information in the training datasets. For instance, information about medical procedures can be determined using models built on hospital datasets \cite{shokrietal:2017}. It is therefore critical to provide strong and rigorous privacy guarantees for learning, and in particular, deep learning algorithms.

Over the last several years, different approaches \cite{yu2019differentially,yu2021large,papernot2021tempered, Stevens2022,Andrew2019,Pichapati2019,lee2018concentrated} have been proposed to guarantee \textit{Differential privacy} \cite{DP_algorithm} -- an accepted quantitative measure of privacy-- for learning algorithms, which has been subsequently adapted specifically to deep learning algorithms. These methods invariably rely on a \textit{noisy} training approach known as \textit{Differentially Private Stochastic Gradient (DP-SGD)}, which while privacy preserving, often results in high utility loss. While there have been other advancements to enhance the privacy of deep learning algorithms, they supplement rather than provide an alternative to DP-SGD based noise addition. The main contribution of this work is a new approach to achieving differential privacy in deep learning, called Spectral-DP, which is an alternative to DP-SGD based approaches, and our results will show that this alternative can outperform DP-SGD based approaches.

%to provide privacy guarantees from an adversary who aims to extract sensitive information about the dataset from the learned model. In this work, we propose and study a novel method to achieve a desired differential privacy guarantee for deep learning architectures.
% The flip side of this rapid development and deployment of deep learning approaches is the loss of privacy. Differential privacy (\cite{DP_algorithm}) provides strong privacy guarantees for an adversary who aims to extract sensitive information from the database. Differential privacy is achieved by introducing random noise into the output of arbitrary algorithms.

% \subsection{Motivation}
%Differential privacy guarantees necessitate the addition of random noise to the model training which often results in high cost to utility. %The state-of-the-art mechanisms to achieve differential privacy in deep learning are typically rooted in Differentially Private Stochastic Gradient Descent (DP-SGD) \cite{dpsgd,dp_model_publish}, with several variants and innovations proposed that explore different noise distributions, better composition mechanisms, noise allocation across training epochs to overcome the impact of noise on the utility of the trained model. 
In the context of deep learning, the fundamental DP-SGD approach, along with its many variants, requires direct noise addition to every weight in a dense neural network, which has a significant impact on utility. Consequently, the more significant improvements to DP-SGD in recent advances have either considered altering specifics of the deep learning architecture, or "curing" datasets, rather than altering the methodology of noise addition. For instance, \cite{papernot2021tempered} explored the use of tempered sigmoid activations to improve the deep learning model's private-learning suitability and achievable privacy-utility tradeoffs (with noise addition through DP-SGD). Yet another approach that improves DP-SGD based methods is to derive handcrafted features (with a data independent preprocessing model) as in \cite{tramer2021differentially}, where it is shown that for a fixed privacy level, deep learning model with handcrafted features outperforms end-to-end deep learning models.% in utility. As noted, all of these approaches rely on DP-SGD based training to achieve the desired guarantee on differential privacy.

Our approach is motivated by the knowledge that the utility loss in DP-SGD based perturbation methods is consequent to the direct gradient clipping and noise addition at the ``signal" domain of the weights. As a result, the utility is highly sensitive to the noise scale and the clipping norm. Furthermore, there is a tension between using more weights to overcome the effects of noise, and the consequent overfitting that leads to lower utility. Our work here comes out of a hypothesis that although weight vectors have large dimension in the signal or time domain, given the density of the network, they can afford to be sparsely distributed, albeit in a transformed domain, a \textit{spectral domain}. In other words, if the weights are restricted to a subspace in the spectral domain, it is possible to reduce the level of noise required for privacy without necessarily impacting utility. Our approach, referred to as Spectral-DP, is a method that performs a low-bandwidth noise addition in the \textit{Fourier domain} of the weights, and combined with a filtering based dimensionality reduction, we demonstrate that it outperforms DP-SGD in trading utility for privacy. 
% You can then talk about unitary and invertible, and also the fact that FPA has worked in the non deep learning scenario.
% Intuitively, reducing the gradient variation which results in noise reduction is a simple way to improving the utility.

Fourier transform is a classical transformation approach, %utilized in several domains which includes image processing and decomposing time series.
and is a unitary and invertible transform, which allows for the learning gradients to be embedded into the spectral domain without impacting the privacy accounting. Furthermore, we note that the frequency components of the weights that have a significant impact on the model outcomes have lower dimensionality than those in the signal domain of the weights. Put another way, forcing weights to fall into a low frequency spectrum provides a way to \textit{compress} the weight representation, and hence serves as a regularizer to prevent loss of utility through overfitting thus overcoming a weakness in existing methods. 
% \textcolor{red}{It is verified in \cite{CSIAM-AM-2-484,xu2019training,rahaman2019spectral} that deep neural network tends to fit functions from low to high frequencies.} 
% \textcolor{red}{
We derive our motivation from empirical studies, such as in \cite{CSIAM-AM-2-484,xu2019training,rahaman2019spectral}, that demonstrate the so-called ``Frequency Principle", wherein deep neural networks tend to fit functions in the low to medium frequencies during training.
% }
In accordance, we develop, test, and demonstrate in this work a Fourier transform based method to provide differentially private low bandwidth noise addition for deep learning architectures.

Specifically, we propose spectral domain based differential privacy for deep learning architectures that can include both convolutional and fully connected layers. Owing to the classical convolution theorem, convolutional layers are more amenable to computation friendly low bandwidth spectral perturbation. The direct adaptation of the spectral DP to fully connected layers, however, is not straightforward. In particular, the high density of weights in fully connected layers and lack of spatially localized features make direct adaptation of the spectral-DP approach challenging. In this regard, we propose an alternative to spectral filtering to reduce the dimensionality of the weights. Specifically, we adapt a spatial compression technique using block circulant matrices \cite{cirCNN_Ding,blockcirculant_1} which we combine with our Fourier based noise addition approach to develop a compressed spectral domain differentially private training methodology, Block-Spectral DP. As our results will show, in networks with fully connected layers, Block Spectral DP outperforms DP-SGD based approaches. 




% \subsection{Contribution}
% \subsection{Contribution}
The overarching contribution of our work is a viable alternative to DP-SGD for deep learning with differential privacy. In particular, we propose approaches that combine spectral noise addition with dimensionality reduction to achieve better utility for a given differential privacy guarantee. Our specific contributions are as follows:
%the following:
\begin{itemize}[leftmargin=*]
  \setlength\itemsep{0em}
   \item We address a critical challenge in achieving differential privacy in deep learning algorithms which is to reduce the noise scale to achieve better utility. 
    \item Through theoretical analysis, we develop the spectral filtering based noise scale reduction technique, and provide the analytical reasoning for the improved utility performance of our methodologies.
    \item We develop differentially private deep learning algorithms based on our \ourmethod approach for a general class of neural network architectures. Specifically, for convolutional layers our approach combines filtering and spectral gradient perturbation to achieve the desired noise scale reduction.
    \item For fully connected layers, we develop a variant of our fundamental approach, block \ourmethod, where we adapt a spatial compression mechanism using block circulant matrices to the spectral gradient perturbation which further reduces the impact of differentially private noise addition on the utility. 
    \item Through comprehensive experimental study, we provide guidelines to choose the right parameters including filtering ratio and clipping norms to achieve the best privacy utility tradeoff using Spectral-DP. 
    % \textcolor{red}{
    \item Through several experiments on three benchmark image classification datasets, namely MNIST, CIFAR10 and CIFAR100, we demonstrate that \ourmethod can outperform the state-of-the-art implementation of DP-SGD. Specifically, \ourmethod incurs less than 1\% accuracy drop for privacy budget as low as $(2,10^{-5})$ for MNIST and 20\% higher accuracy than DP-SGD for CIFAR10 with privacy budget $(3,10^{-5})$ for training from scratch models. 
    In the transfer learning setting, \ourmethod achieves 94.85\% for CIFAR10 and 77.52\% for CIFAR100 with privacy budget $(1,10^{-5})$.
     Moreover, when combined with Scatter-net based data curation, \ourmethod incurs less than 1\% accuracy loss with privacy budget $(3,10^{-5})$.
     % }
  %   Some highlights from our experiments are as follows:
  %   \begin{itemize}[leftmargin=*]
  % \setlength\itemsep{0em}
  %       \item For the MNIST dataset, \ourmethod incurs less than 1\% accuracy drop for privacy budget as low as $(2,10^{-5})$.
  %       \item For the CIFAR10 dataset, \ourmethod outperforms DP-SGD in accuracy by more than $20\%$ when the privacy budget is $(3,10^{-5})$.
  %       \item Under the transfer learning setting, \ourmethod achieves 94.85\% for CIFAR10 and 77.52\% for CIFAR100 with $(1,10^{-5})$.
  %       \item When combined with techniques such as transfer learning, or Scatter-net based data curation, \ourmethod incurs less than $1\%$ accuracy loss for low privacy budgets ($(2,10^{-5})$ for transfer learning, and $(3,10^{-5})$ for data curation).
  %   \end{itemize}
    %Our comparison in all these experiments demonstrates that \ourmethod has a uniformly better privacy-utility tradeoffs compared to the DP-SGD based approach. 
    %\item Beyond the basic comparison our \ourmethod with DP-SGD, we additionally provide comparison with two additional techniques that have supplemented DP-SGD, namely a transfer learning based approach, and Scatter-Net based dataset curation approach. Our experiments demonstrate that \ourmethod outperforms DP-SGD even when both methods are supplemented with these additional techniques. %We study the impact of hyper parameters in \ourmethod, and provide a general guideline of choosing hyper parameters when implementing \ourmethod in deep learning tasks.
%    \item We demonstrate spectral filtering controls tradeoff between utility and privacy, and study the tradeoff by conducting experiments on real dataset.
 %   \item We further compare \ourmethod with other approaches that aim to improve utility of DP-SGD. Our experimental results show \ourmethod achieves better utility improvement. 
  %  \item We compare \ourmethod with DP-SGD under the transfer learning setting.
% {\textcolor{red} The next three bullets need to be changed. We should be specific about our experimental results. We should talk about ablation. We should talk about how we set the parameters of our approach such as how to choose k and so on. These contributions are generic and uninformative}
    % \item We provide differential privacy analysis of the \ourmethod, and demonstrate \ourmethod leads to a small accuracy gap. 
    % \item We demonstrate the tradeoff between utility and privacy by conducting several experiments.
    % \item We compare \ourmethod with DP-SGD through several experiments on two image classification benchmark datasets: MNIST and CIFAR10.
\end{itemize}
% \subsection{Paper Organization}
The remainder of the paper is organized as follows. We provide some preliminaries in Section \ref{sec:prelim}. We formulate the mathematical model and related formulations of \ourmethod in Section \ref{sec:approach}. We conduct several experiments and analyze \ourmethod in Section \ref{sec:experiment}, and discuss the limitations of \ourmethod in Section \ref{sec:limitations}. In Section \ref{sec:relatedwork}, we detail the related work to place our work in broader scientific context. Some concluding remarks are presented in Section \ref{sec:conclusion}.

% \subsection{Motivation Example}\label{subsec:MotivationExample}
% It is incontestable that neural networks become more redundant as the networks tend to be deeper and deeper. The redundancy is introduced to achieve better utility performance. However, when applying differential privacy into the context of deep learning, it causes a significant loss in utility. This is because differential privacy adds noise into the gradient, which is redundant. 

% We note pruning is introduced to reducing the redundancy of the model, and it has been proven for preventing over-fitting. Since pruning is a natural way to providing less variation, we explore if pruning can be applied in the context of differential privacy. Due to gradient contains much information, directly pruning the gradient on the signal or time domain causes a huge utility loss.
% \textcolor{red}{Wen: people may ask why pruning weights directly, instead of gradients, cannot reduce the added noise. This is also related to whether using block-circulant in the time domain or compression only improves utilityï¼ŒThen Why does pruning in the frequency domain work better? The motivation example should consist of curves of several baselines--original DP-SGD, original DP-SGD + pruning in time domain, pruning in spectral}

% To overcome it, we explore filtering in the work. Filtering does not change the structure of the network, but instead, we focus on the filtering that is conducted on the spectral domain. The spectral filtering can reduce the dimension of the weights as oppose to the pruning. The simplest form of a filter is killing coefficients in the spectral domain. We note such filter does not change the number of the parameters, but achieves less variation for the gradients. 

% % Figure environment removed

% Consider a neural network that only contains convolution layers, the convolution theorem indicates that the gradients can be transformed into the spectral domain (Fourier domain).  We try the simple filtering on the spectral representation of the gradients before they are transformed back to the signal domain. During the training phase, the model parameters are updated using the filtered gradients. We implement a two layer network on a subset of the MNIST dataset. We focus on the binary classification, and we only choose images belongs to two classes. As shown in Figure \ref{fig:motivation_example}, the accuracy drops a lot only when we filter out too many spectral coefficients.

% For  In the backward propagation, the  The spectral pruning removes the relatively unimportant spectral coefficients, and the resulting gradient is expressed as the inverse Fourier transform of the pruned spectral coefficients. We implement the network on the CIFAR10 dataset with spectral pruning. The results show that the accuracy drop is small compared to the un-pruned network.

% Consider a convolution based neural network, the convolution theorem allows gradient computation on the spectral domain (or Fourier domain). During the training process, we apply the filter into the spectral representations of the gradient. Consider a binary classification problem, We try this filtering in the classification problem, and we show that the spectral filtering does not cause much utility loss. 

% To simplify the neural networks, pruning is introduced to reducing the redundancy. The key idea of pruning is to remove neurons or parameters. Most pruning methods focus on removing neurons or parameters (near zero weights), and such methods are conducted on the spatial domain. Since pruning is a natural way to providing less variation, we explore if pruning can be applied into the gradient. Due to gradient contains much information, directly pruning the gradient on the signal or time domain causes a huge utility loss. We then focus on the pruning that is conducted on the spectral domain.

% Consider a neural network that only contains convolution layers, the gradient of each convolution filter can be expressed an multiplication on the spectral domain by implementing Fourier transform. The spectral pruning removes the relatively unimportant spectral coefficients, and the resulting gradient is expressed as the inverse Fourier transform of the pruned spectral coefficients. We implement the network on the CIFAR10 dataset with spectral pruning. The results show that the accuracy drop is small compared to the un-pruned network.