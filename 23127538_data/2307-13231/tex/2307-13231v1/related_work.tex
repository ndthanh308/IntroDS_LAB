\vspace{-3pt}
\section{Limitations}\label{sec:limitations}
\vspace{-3pt}
% \textcolor{red}{
Although \ourmethod effectively reduces the DP noise and achieves better privacy utility tradeoff, 
there is scope for further improvement.
% it has a few known limitations. 
First, while \ourmethod achieves the same level of utility as that of the non-private learning for the transfer learning setup, there still exists the utility or accuracy gap for model training from scratch setup. Second, offering rigorous guidelines to select optimal key parameters such as filtering ratio $\rho$, with theoretical guarantees is still challenging, 
%theoretical analysis of selecting filtering ratio $\rho$ is not provided currently incomplete 
due to the interplays among multiple factors, including dataset characteristics, model complexity, privacy budget, clipping specification, and batch size.
%which collectively impact the decision. 
A comprehensive analysis of these factors would be a promising avenue for future research. Third, there are other domain transform methods such as wavelet, etc. that can be explored for further improvement, as Spectral-DP opens up a new era for private deep learning using DP.
% }

% \textcolor{red}{
% As future work, we aim to address the utility gaps between non-private and Spectral-DP models in training from scratch settings, explore other domain transformation methods such as wavelets for further improvement, and adapt our approach to the FL process to provide better privacy-utility tradeoffs in complex scenarios, as \ourmethod opens a new era of private deep learning using DP.
% }
\vspace{-3pt}
\section{Related Work}\label{sec:relatedwork}
\vspace{-3pt}
% \wen {Please refer to other papers to see how the related work is described. Only two papers are related? }

% Differential privacy~\cite{DP_algorithm} has become a general approach to provide provable privacy guarantees against individual identification in a dataset. 
The subject of differential privacy in the context of machine learning attracted significant scientific interest and has been used in support vector machines~\cite{rubinstein2009learning}, linear regression~\cite{chaudhuri2008privacy,zhang2012functional}, and risk minimization~\cite{chaudhuri2011differentially, bassily2014private}.
In recent years, more works have focused on privacy-preserving training for deep learning. Private Aggregation of Teacher Ensembles (PATE)~\cite{papernot2016semi, papernot2018scalable} is one approach that transfers the knowledge from an ensemble of teachers trained on the disjoint subsets of training data to train a student model through the noisy aggregation of teachers' answers.

Differentially private (stochastic) gradient descent (DP-SGD) \cite{dpsgd,yu2019differentially} as described earlier perturbs the gradient at each update with random noise drawn from Gaussian distribution during the training. Some recent works aim to do noise reduction on DP training by adding noise into the reduced gradient.
% Another feasible direction is noise reduction, which is achieved by adding noise into the reduced gradient.
In \cite{yu2021do}, a gradient embedding perturbation (GEP) is proposed to achieve higher utility by adding noise into a low-dimensional projected gradient.
\cite{yu2021large} designs reparametrized gradient perturbation (RGP), which perturbs the gradients of the low-rank gradient-carrier matrix and reconstructs the update of the original weights from the noisy gradients.
The framework in \cite{nasr2020improving} encodes gradients, mapping them to a smaller vector space, and hence is able to provide DP guarantees for different noise distributions.

These gradient dimension reduction techniques rely on either a projection or decomposition that maps gradients into a smaller subspace. 
% We note that these methods approximate the gradient and therefore introduce unexpected errors.  Our work is to apply lossless transformations (\ie Fourier Transform) to the gradient and low-band filtering in the spectral domain. As deep neural networks (DNNs) tend to first learn low frequencies fast, then learn high frequencies relatively slowly\cite{CSIAM-AM-2-484,xu2019training,rahaman2019spectral}, the spectral filtering guarantees convergence for most DNNs.
% \textcolor{red}{
We note that the key principle of these methods is gradient approximation, and therefore they would inevitably cause undesired utility loss.
Our work involves performing lossless transformation of gradients using the Fourier transform and applying filtering in the spectral domain to improve privacy utility trade-off.
% }
% Our work is to apply lossless transformations (\ie Fourier Transform) into the gradient in deep learning and perform DP perturbation and filter out the high frequencies in the spectral domain. 
% Our work is to explore whether lossless transformations (\ie Fourier Transform) can be applied into the gradient in deep learning and perform DP perturbation in the spectral domain. 
Fourier Perturbation Algorithm (FPA) is proposed in \cite{FourierDP_database2010} and further optimized in \cite{acs2012differentially} to address the poor performance of conventional differential privacy aggregation algorithm for time-series data. FPA focuses on the differential privacy of time-series data and conducts noise aggregation in the frequency domain, which is similar to \ourmethod.
 %The major issue of DP-SGD is that privacy is achieved at a significant cost of utility.
% The subject of differential privacy in the context of deep learning has attracted significant scientific interest in recent years, and the state-of-the-art method differentially private (stochastic) gradient descent (DP-SGD) \cite{dpsgd,yu2019differentially} perturbs the gradient at each update with random noise drawn from Gaussian distribution. The major issue of DP-SGD is that privacy is achieved at a significant cost of utility.

Several recent works focus on a wealth of areas to improve the utility of the DP-SGD trained models. In Section \ref{sec:introduction}, we discussed tempered sigmoid activations \cite{papernot2021tempered} introduced to help control the gradient norm of the loss function, thus mitigating the negative effects of clipping and noising. %Hence, it can improve the private-learning suitability and achievable privacy-utility tradeoff of the deep learning models. 
% \cite{luo2021scalable} aims to minimize the number of trainable parameters in the model during sparse finetuning to optimize the privacy-utility tradeoff. 
% By leveraging additional public data, it can instill strong representations in large models and adapt to private datasets with out-of-domain transfer learning at a minimal privacy cost.
\cite{luo2021scalable} leverages additional public data transfer learning to minimize the number of trainable parameters in the model to optimize the privacy-utility tradeoff. 
% \cite{cheng2022dpnas} proposes a framework to perform the neural architecture search with DP-aware candidate models training to find the model structures that are suitable for DP training to improve the model utility. 
\cite{cheng2022dpnas} proposes a framework to perform a neural architecture search with DP-aware candidate model training to find the suitable model for DP training. 
% The model is equipped with tempered sigmoid activations, which contain several parameters that can control the tradeoff between privacy and utility. 
Work in \cite{tramer2021differentially} as mentioned earlier demonstrates that better features in data can lead to higher utility of DP-SGD trained model.
% , and explores how the normalization of data features affects the utility-privacy tradeoff. 

We note that these works focus on how to improve DP-SGD by modifying the model structure or preprocessing the data. Furthermore, other directions including \ie clipping~\cite{Stevens2022,Andrew2019,Pichapati2019} and privacy budget allocation~\cite{lee2018concentrated,asi2021private, yu2019differentially} do not change the DP-SGD algorithm. In contrast, \ourmethod focuses on the gradient updating algorithm of DP training that adapts spectral domain DP perturbation into deep learning as an alternative to DP-SGD.  %Thus, \ourmethod can gain more benefits in utilizing these techniques.

%Besides these approaches, some 
% In \cite{FourierDP_database2010}, Fourier Perturbation Algorithm (FPA) is proposed in solving publishing time-series for which conventional differential privacy aggregation algorithm performs poorly. We note that FPA focuses on the differential privacy of time-series data, and conducts noise aggregation in the frequency domain which is similar to \ourmethod.

% We explore if it possible to introduce noise in a space where the gradient is transformed into such space with less variation, and the transformation is independent of tand perform DP perturbation at the spectral domain. We note that the transform is independent of the gradient 

% Our approach is driven by a different scenario: when training a deep learning model from the scratch, is it achievable to add DP noise in a specific domain without using an auxiliary public database? In other words, our objective is to explore if it possible to introduce noise in a space where the gradient is projected onto such space with less variation, and perform training method to achieve the tradeoff between the differential privacy and utility.

% The key idea of the FPA is to conduct the noise aggregation in the Discrete Fourier Transform (DFT) of the query. In the meanwhile, in order to reconstruct the data accurately, it involves an approximation using low pass filter in the frequency components. 
% We note that ignoring the high frequency components is such a way to reducing the data variation, hence we explore the applicability of the FPA in the context of the deep learning. 

Another emerging topic is the use of differential privacy to protect privacy and robustness in federated learning (FL)~\cite{mcmahan2017communication}. Client-based Differential Privacy has been introduced in~\cite{mcmahan2017learning, geyer2017differentially} in order to hide any information that is specific to a single clientâ€™s training data. Noising before model aggregation FL (NbAFL)~\cite{wei2020federated} and LDP-Fed~\cite{truex2020ldp} perturb the trained parameters locally in each client before aggregation to ensure local differential privacy. \cite{stevens2022efficient} proposes a new protocol for differentially private secure aggregation based on techniques from Learning With Errors~\cite{regev2009lattices}. As future work, it would be a good opportunity to adapt and combine our \ourmethod to the FL process to provide better privacy-utility tradeoffs in these complex scenarios.






