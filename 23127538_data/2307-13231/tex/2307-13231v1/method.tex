\vspace{-3pt}
\section{Approach}\label{sec:approach}
\vspace{-3pt}
In this section, we formally present \ourmethod in the context of deep learning. While \ourmethod shares the same objective with DP-SGD which aims to perturb weight gradients during the training process, it can principally reduce the differential privacy (DP) noise, thus to significantly escalate model utility, by conducting dedicated spectral filtering in our designed DP noise addition process performed in the spectral domain, with theoretical guarantee on the privacy. 


Figure \ref{fig:spectralDP_overview} provides an overview of our approach. Consider a deep learning model consisting of $L$ layers (either convolutional or fully connected). During the backward propagation process of the learning algorithm, the gradients at each layer are transformed into their spectral representation which is subsequently perturbed by Gaussian noise, and filtered, prior to transforming back to the signal (or spatial) domain. Since a convolutional operation in the spatial domain is equivalent to multiplication in the spectral domain, \textbf{convolutional (CONV) layers} are more amenable to spectral gradient perturbation. For \textbf{fully connected (FC) layers}, we supplement this mechanism with a block-circulant matrix based weight compression and restructuring to address the high density of weights prior to spectral perturbation and filtering.

The remainder of the section is organized as follows. In Section \ref{sec:FourierDP}, we present the conceptual basis of \ourmethod and a theoretical analysis that provides the differential privacy guarantee of the method, and demonstrates the reduction in noise scale that enables the better utility performance of spectral perturbation and filtering. In Sections \ref{sec:SpectralFourierDP} and \ref{sec:blockFourierDP}  we describe in detail the spectral perturbation and filtering methodology as applied to convolutional layers and fully connected layers respectively. In Section \ref{sec:DPtogether}, we outline the overall training of a neural network with both kinds of layers over multiple iterations to achieve a desired guarantee of $(\epsilon, \delta)$ differential privacy. %put these all together in a deep learning model, and outline the \ourmethod based training algorithm.
%--------------------------------------------------------------------------------------------------------

\vspace{-3pt}
\subsection{Conceptual Foundations of Spectral-DP}\label{sec:FourierDP}
\vspace{-3pt}
In this section, we present the concept and theoretical analysis of \ourmethod which is the basis of the specific deep learning algorithms developed in subsequent sections. 


\subsubsection{\ourmethod Overview}
The key of \ourmethod is to perturb weight gradients in the spectral or Fourier domain by taking advantage of existing primitives such as Fourier transform and the Gaussian mechanism for differential privacy. Specifically, Fourier transform (FT) is used to project data to the spectral domain (or frequency domain), and the algorithm perturbs the Fourier transform coefficients prior to filtering out a fraction of the coefficients. The approach is described in mathematical detail below. 


Consider an $N$ length sequence $\bQ = \{Q_0,Q_1,\cdots,Q_{N-1}\}$ as an example. We denote $\{F^N\}:=\{F_0, F_1,\cdots,F_{N-1}\}$ as a collection of all spectral coefficients of $\bQ$, where each $F_i$ is computed by:
\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
$$F_i=\frac{1}{\sqrt{N}}\sum_{n=0}^{N-1}Q_n\cdot e^{-\frac{j2\pi}{N}in}$$

The Gaussian noise addition mechanism is applied into $\{F^N\}$. Since the Gaussian noise scale is proportional to the $L_2$ norm of the $\{F^N\}$, we bound $\{F^N\}$ by a clipping parameter $S$: 
\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
$$\bar{F}^N=F^N/ \max\{1,\frac{\|F^N\|_2}{S}\}$$
where $\|\cdot\|_2$ denotes the $L_2$ norm. The spectral coefficients are perturbed with additive Gaussian noise:
$$\tilde{F}^N=\bar{F}^N+V^N$$
where $V^N=\{V_0,V_1,\cdots,V_{N-1}\}$ is the noise vector, and each $V_i$ is drawn from $\calN(0,\sigma^2S^2)$ independently. We denote this process as $Gauss(\bar{F}^N,S,\sigma)$. A key mechanism that allows \ourmethod to limit the impact of noise is filtering, in other words, eliminating a fraction of the coefficients:
$$P_K(\tilde{F}_i)=\left\{\begin{array}{lc}
        \tilde{F}_i & \text{if }i<K \\
        0 & \text{otherwise}
    \end{array}\right.
$$
the $K/N$ determines the fraction of coefficients which are perturbed and allows us to reduce the overall noise scale. Our motivation is that it is sufficient to concentrate the weights in a \textit{low bandwidth} space without compromising on utility while saving on the impact of noise. The perturbed and filtered coefficients are retransformed to the signal domain using the inverse Fourier Transform (I-FT). 
The overall procedure is outlined in Algorithm \ref{alg:FourierDP}.

\begin{algorithm}[t]
    \caption{\ourmethod perturbation}
    \label{alg:FourierDP}
  \begin{algorithmic}[1]
    \REQUIRE Query $Q=\{Q_0,Q_1,\cdots,Q_{N-1}\}$, $l_2$ sensitivity $S$, noise scale $\sigma$\\
    \textbf{Output: } $\tilde{Q}$\\
    \STATE Compute the Fourier coefficients of $Q$
    \STATE Clipping the Fourier coefficients by $S$
    \STATE Noise addition:
        $\tilde{F}^N=Gaussian(\bar{F^N}, S,\sigma)$
    \STATE Spectral filtering:
    $\hat{F}_i^K=P_K(\tilde{F}_i)
    $
    \STATE Inverse Fourier transformation:
    $\tilde{Q_n}=\text{I-FT}(\hat{F_i}^K)$
  \end{algorithmic}
\end{algorithm}
\setlength{\textfloatsep}{10pt}

\subsubsection{Theoretical analysis of \ourmethod}
\label{subsec:theoreticalanalysis}
In the following theorem, we determine the privacy budget of Algorithm \ref{alg:FourierDP}, and prove that spectral perturbation achieves the desired differential privacy.

\begin{restatable}{theorem}{Privacythm}
\label{thm:FourierDP}
    In Algorithm \ref{alg:FourierDP}, the output $\tilde{Q_n}$ is ($\epsilon,\delta$) differentially private if we choose $\sigma$ to be $\sqrt{2\log(1.25/\delta)}/\epsilon$.
\end{restatable}

\begin{proof}
    The proof relies on Theorem 3.22 in \cite{DP_algorithm} and the post-processing property of DP algorithm. The detailed proof is given in Appendix~\ref{Appendix:proof_privacythm}.
    % \ref{Appendix:proof_privacythm}.
\end{proof}

Since both spectral filtering and inverse Fourier transformation can be treated as the post-processing steps that do not alter the DP budget, \ourmethod better utilizes the privacy budget. As demonstrated in the following Proposition, the filtering operation in spectral domain leads to prominent noise scale reduction.
\begin{restatable}{proposition}{PropNoiseReduction}
% \begin{proposition}
\label{prop:noise_reduction}
    Let $V^N=\{V_0,V_1,\cdots,V_i,\cdots,V_{N-1}\}$ be a collection of noise vector in spectral domain, and each $V_i$ is drawn from $\calN(0,\sigma^2S^2)$. Consider $v_n=\text{I-FT}(P^K(V_i))$, then $v_n$ follows a normal distribution $\calN(0,\frac{K}{N}\sigma^2S^2)$.
% \end{proposition}
\end{restatable}
\begin{proof}
\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
    % \textcolor{red}{
    The detailed proof is given in Appendix \ref{Appendix:noise_reduction}.
    % }
    % Let $V_i^K=P_K(V_i)$, then the resulting $v_n=\text{I-FT}(V_i^K)$. In detail, it can be formulated as
    % $$\begin{array}{ccl}
    %     v_n& =&\frac{1}{\sqrt{N}}\sum_{i=0}^{N-1}V_i^K\cdot e^{\frac{j2\pi}{N}in} \\
    %     & =&\frac{1}{\sqrt{N}}\sum_{i=0}^{K}V_i\cdot e^{\frac{j2\pi}{N}in}\\
    %     & =&\frac{1}{\sqrt N}\sum_{i=0}^{K-1}\{V_i\}\cdot\cos(\frac{2\pi}{N}in)\\
    %     &&+ j\frac{1}{\sqrt N}\sum_{i=0}^{K-1}\{V_i\}\cdot\sin(\frac{2\pi}{N}in)
    % \end{array}$$
    % Define $c_{n,i}=\frac{1}{\sqrt{N}}\{V_i\}\cdot\cos(\frac{2\pi}{N}in)$, then we have
    % $$\frac{1}{\sqrt N}\sum_{i=0}^{K-1}\{V_i\}\cdot\cos(\frac{2\pi}{N}in)=\sum_{i=0}^{K-1}c_{n,i}$$ 
    % Following the property of the normal distribution, we have $$\sum_{i=0}^{K-1}c_{n,i}\sim\calN(0,\sum_{i=0}^{K-1}\frac{1}{N}\cdot\cos^2(\frac{2\pi}{N}in)\sigma^2S^2)$$
    % Simplifying the variance of the distribution, we have $$\sum_{i=0}^{K}c_{n,i}\sim\calN(\frac{K}{N}\cdot\frac{\sigma^2S^2}{2})$$
    % Similarly, we have $\frac{1}{\sqrt N}\sum_{i=0}^{K}\{V_i\}\cdot\sin(\frac{2\pi}{N}in)\sim\calN(\frac{K}{N}\cdot\frac{\sigma^2S^2}{2})$. This indicates that $v_n\sim\mathbb{C}\calN(\frac{K}{N}\cdot\sigma^2S^2)$ has the same scale of $\calN(\frac{K}{N}\cdot\sigma^2S^2)$
\end{proof}

As Proposition \ref{prop:noise_reduction} shows, the spectral filtering allows the reduction in the overall noise scale from $\sigma^2S^2$ down to $\frac{K}{N}\cdot\sigma^2S^2$, where $K<N$. 
% \textcolor{red}{Wen: Ce, Can you specify the original and reduced noise scale here explicitly}. 
Consequently, should the filtering mechanism not affect the utility, the noise reduction could significantly minimize the utility penalty incurred by differential private training. While filtering more frequency components (a smaller $K$) indicates more DP noise reduction thus less utility penalty by DP, the more weight distortion errors after Inverse Fourier Transform could inevitably impact the gained model utility. We define a key parameter--\textbf{filtering ratio} $\rho=(K-N)/N$, to balance the impact of these two factors and 
%The tradeoff between these two factors is controlled by the filtering ratio $\rho=(K-N)/N$. 
will discuss the impact and choice of this key parameter in Sections~\ref{subsec:effectiveness_spectral_dp} and \ref{subsec:block_spectral_dp}

% Figure environment removed
\subsection{\ourmethod in CONV Layer }\label{sec:SpectralFourierDP}
\subsubsection{Adapting \ourmethod to 2D CONV}
%We have provided foundations of \ourmethod. 
\label{subsec:adaptingspectralDPin2dconv}
To adapt \ourmethod for private deep learning, our first question would be how to perform gradient perturbations for different types of model layers using \ourmethod. We first focus on the 2-dimensional (2D) convolution that dominates the operations of the convolutional layer. 


According to the convolution theorem, the 2D convolution in spatial domain can be easily converted into element-wise multiplication of two matrices in the spectral domain. 
\ourmethod is then applied into the element-wise multiplication and mainly consists of a Gaussian noise addition and a 2D spectral filtering. 
To demonstrate how effectively \ourmethod reduces DP noises in 2D convolution, we further derive the relation between the noise scale and filtering parameter $\rho$ in Corollary \ref{prop:noise_reduction_2D}. 
\begin{restatable}{corollary}{PropNoiseReductionConv}
% \begin{corollary}
\label{prop:noise_reduction_2D}
    % Let $$V^{N}=\left\{\begin{array}{ccccc}
    %     V_{0,0}&\cdots&V_{0,j}&\cdots&V_{0,N-1}  \\
    %     V_{1,0}&\cdots&V_{1,j}&\cdots&V_{1,N-1}  \\
    %     \vdots&\ddots  & \vdots &\ddots &\vdots\\
    %     V_{i,0}  & \cdots &V_{i,j} &\cdots & V_{i,N-1}\\
    %     \vdots&\ddots  & \vdots &\ddots &\vdots\\
    %     V_{N-1,0}&\cdots&V_{N-1,j}&\cdots&V_{N-1,N-1}
    %     \end{array}\right\}$$ 
    Let $V^N$ be the collection of a noise vector $\{V_{i,j}\}$ where $i\in\{0,1,\cdots,N-1\}$ and $j\in\{0,1,\cdots,N-1\}$ in spectral domain, and
%   \begin{footnotesize}
%     $$V^{N}=\left\{\begin{array}{ccccc}
%         V_{0,0}&\cdots&V_{0,j}&\cdots&V_{0,N-1}  \\
%         V_{1,0}&\cdots&V_{1,j}&\cdots&V_{1,N-1}  \\
%         \vdots&\ddots  & \vdots &\ddots &\vdots\\
%         V_{i,0}  & \cdots &V_{i,j} &\cdots & V_{i,N-1}\\
%         \vdots&\ddots  & \vdots &\ddots &\vdots\\
%         V_{N-1,0}&\cdots&V_{N-1,j}&\cdots&V_{N-1,N-1}
%         \end{array}\right\}$$ 
%     \end{footnotesize}
    each $V_{i,j}$ be drawn from $\calN(0,\sigma^2S^2)$, consider a 2D spectral filtering:
    $$P_{2D}^K=\left\{
    \begin{array}{cl}
    V_{ij}     & \text{if }i<K\text{ and } j<K \\
    0     & \text{otherwise}
    \end{array}\right.$$
    and $v_{mn}=\mathcal{F}^{-1}(P_{2D}^K(V_{i,j}))$, then $v_{mn}$ follows a normal distribution $\calN(0,\frac{K^2}{N^2}\sigma^2S^2)$.
% \end{corollary}
\end{restatable}
\begin{proof}
    % \textcolor{red}{
    The detailed proof is given in Appendix \ref{Appendix:noise_reduction2D}.
    % }
    % Let $V_{i,j}^K=P^K_{2D}(V_{i,j})$, then the resulting $v_{mn}=\mathcal{F}^{-1}(V_{ij}^K)$. In detail, it can be formulated as
    % \begin{small}
    % $$\begin{array}{ccl}
    %     v_{mn}& =&\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}V_{ij}\cdot e^{\frac{j2\pi}{N}im+jn} \\
    %     & =&\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\cos(\frac{2\pi}{N}(im+jn))+\\
    %     &&\sqrt{-1}\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\sin(\frac{2\pi}{N}(im+jn))
    % \end{array}$$
    % \end{small}
    % Define $c_{mn,ij}=\frac{1}{N}\{V_{i,j}\}\cdot\cos(\frac{2\pi}{N}(im+jn))$, then we have
    % $$\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\cos(\frac{2\pi}{N}(im+jn))=\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}c_{mn,ij}$$ 
    % Following the property of the normal distribution, we have 
    % \begin{footnotesize}
    % $$\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}c_{mn,ij}\sim\calN(0,\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\frac{1}{N^2}\cdot\cos^2(\frac{2\pi}{N}(im+jn))\sigma^2S^2)$$
    % \end{footnotesize}
    % Simplifying the variance of the distribution, we have $$\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}c_{mn,ij}\sim\calN(\frac{K^2}{N^2}\cdot\frac{\sigma^2S^2}{2})$$
    % Similarly, we have $\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\sin(\frac{2\pi}{N}im+jn)\sim\calN(\frac{K^2}{N^2}\cdot\frac{\sigma^2S^2}{2})$. This indicates that $v_{mn}\sim\mathbb{C}\calN(\frac{K^2}{N^2}\cdot\sigma^2S^2)$ which has the same scale of $\calN(\frac{K^2}{N^2}\cdot\sigma^2S^2)$
\end{proof}

\subsubsection{Adapting \ourmethod into CONV layer}
\label{subsec:conv_spectral_dp}
\begin{algorithm}[t]
% \small
    \caption{\ourmethod in a 2D convolutional layer}
    \label{alg:SpectralFourierDP}
  \begin{algorithmic}[1]
    \REQUIRE A CONV layer with input $X$, filters $W$, $\frac{\partial{J}}{\partial{A}}$, clipping bound $S$, $\sigma$, filtering ratio $\rho\in(0,1)$ \\
    \textbf{Output:} $\tilde{\frac{\partial J}{\partial W}}$\\
    $\bullet$  \textbf{\textit{Stage I:}} Noise addition
    \STATE \textbf{Compute:} \\$G_W^F=\{G_{W_{i,j}}^F\}$, where $i\in\{1,\cdots,C_{out}\}$, $j\in\{1,\cdots,C_{in}\}$.\\
    % \textbf{\textit{Stage II:}} Noise addition \\
    \STATE Clipping Norm: $\bar{G}_W^F=G_W^F/\max(1,\frac{\|G_w^F\|_2}{S})$
    \STATE \textbf{Gaussian mechanism:} \\$\tilde{G}_W^F= Gauss(\bar{G}_W^F,S,\sigma)$\\
    $\bullet$  \textbf{\textit{Stage II:}} Filter-wise pruning and inverse Fourier Transform\\
    % \STATE Noise addition: $\tilde{\frac{\partial L}{\partial w_{i,j}}_F}=\bar{\frac{\partial L}{\partial w_{i,j}}_F}+\mathcal{N}(0,\sigma^2 C^2 \mathbb{I}_k)$
    \FOR {$i\in 1,2,\cdots,C_{out}$}
    \FOR {$j\in 1,2,\cdots,C_{in}$}
    \STATE \textbf{2D Spectral filtering:} \\$\hat{G}_{W_{i,j}}^F\leftarrow$Zero last $\rho$ of rows and columns in $\tilde{G}_{W_{i,j}}^F$ 
    \STATE \textbf{Inverse Fourier transform:}\\ $\tilde{\frac{\partial J}{\partial W_{i,j}}}=\mathcal{F}^{-1}[[\hat{G}_{W_{i,j}}^F]_0]$
    \ENDFOR
    \ENDFOR
    \STATE $\tilde{\frac{\partial J}{\partial W}}=\{\tilde{\frac{\partial J}{\partial W_{i,j}}}\}$, $i\in\{1,\cdots,C_{out}\}$, $j\in\{1,\cdots,C_{in}\}$.
  \end{algorithmic}
\end{algorithm}
\setlength{\textfloatsep}{5pt}

Based on the 2D spectral filtering, we then provide more implementation details of \ourmethod for training convolutional layers. We consider a typical 2-dimensional convolutional layer of a deep learning model with the input vector $X\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}$ and convolution filters $W\in\mathbb{R}^{C_{out}\times C_{in}\times d\times d}$, where $C_{in}$ and $C_{out}$ are the number of input and output channels, $d\times d$ is the size of a 2D convolution filter. The forward propagation process of the inference in the layer is expressed as follow (bias and activation are omitted):
% \setlength\abovedisplayskip{3pt}
% \setlength\belowdisplayskip{3pt}
$$A_{i}=\sum_{j}^{C_{in}}X_{j}\circledast W_{i,j}$$
where $A_i$ denotes the $i-$th channel of the convolution output 
% \textcolor{red}{
with size $H_{out}\times W_{out}$ where $H_{out}=H_{in}+d-1$ and $W_{out}=W_{in}+d-1$ 
% }
, $X_j$ denotes the $j-$th channel of the input, $W_{i,j}$ denotes the convolution filter that connects the $i-$th channel of the output and the $j-th$ channel of the input, and $\circledast$ denotes the 2D convolution.
For each convolution filter, the backward propagation and the convolution theorem indicate that the gradient can be approximately expressed as
\begin{equation}\label{eq:conv_grad}
    \frac{\partial J}{W_{i,j}}=\frac{\partial J}{\partial A_i}\circledast\frac{\partial A_i}{\partial W_{i,j}}=\mathcal{F}^{-1}[\mathcal{F}[\frac{\partial J}{\partial A_i}] \odot \mathcal{F}[X_j]]
\end{equation}
where $\frac{\partial J}{W_{i,j}}$ denotes the gradient of $W_{i,j}$ w.r.t. the cost function $J$, $\frac{\partial J}{\partial A_i}$ denotes the gradient of $A_i$ w.r.t. $J$, $\frac{\partial A_i}{\partial W_{i,j}}$ denotes the gradient of $W_{i,j}$ w.r.t. $A_i$, $\mathcal{F}$ and $\mathcal{F}^{-1}$ are the Fourier Transform and inverse Fourier Transform operator, respectively, and $\odot$ denotes the element-wise multiplication. Let $G_{W_{i,j}}^F=\mathcal{F}[\frac{\partial J}{A_i}] \odot \mathcal{F}[X_j]$ be the spectral gradient of $\frac{\partial L}{W_{i,j}}$, then \ourmethod can be directly applied into the spectral gradient. 
% \textcolor{red}{
Computing the spectral gradient requires a complexity of $O((H_{out}*W_{out})\log(H_{out}*W_{out}))$ where conventional convolution of computing $\frac{\partial J}{W_{i,j}}$ requires a complexity of $O(H_{out}*W_{out}*H_{in}*H_{in})$. Theoretically, the spectral gradient computation is faster than the conventional convolution if $\log(H_{out}*W_{out})<(H_{in}*H_{in})$. We provide detailed complexity analysis in the Appendix \ref{apdx:complexity}.
% } 

Figure \ref{fig:method_overview}(a) depicts the implementation steps when applying \ourmethod to the gradient perturbation of a single 2D convolution filter. $\frac{\partial J}{A_i}$ and $X_j$ are first padded to the same size and transformed to the spectral gradient $G^F_{W_{i,j}}$. Consequently, the noisy spectral gradient $\tilde{G}^F_{W_{i,j}}$ is obtained by applying clipping and Gaussian noise addition into $G^F_{W_{i,j}}$.
The filtered spectral gradient $\hat{G}^F_{W_{i,j}}$ is then computed with a filtering ratio $\rho$ using the 2D spectral filtering approach mentioned in Corollary \ref{prop:noise_reduction_2D}. 


The main procedure of \ourmethod in 2D convolutional layer is outlined in Algorithm \ref{alg:SpectralFourierDP}.
% We note $G_{W_{i,j}}^F$ is the spectral gradient of a single convolution filter. 
As the 2D convolutional layer usually contains multiple filters, we denote $G_W^F=\{G_{W_{i,j}}^F\}$ as the spectral gradients of $W$ with respect to the cost function $J$. As shown at the stage I of Algorithm \ref{alg:SpectralFourierDP}, by applying Gaussian mechanism into $G_W^F$, the differential privacy of all parameters in the layer is guaranteed.
% In Algorithm \ref{alg:SpectralFourierDP}, the Gaussian mechanism is applied into the spectral gradient $G_W^F$ which denotes as the collection of all $G_{W_{i,j}}^F$. This ensures the differential privacy guarantee for the spectral gradients of all filters. 
At stage II, the spectral domain filtering is applied within each convolution filter. According to Corollary \ref{prop:noise_reduction_2D}, \textit{the 2D spectral filtering provides larger noise reduction than the 1D spectral filtering but leads to larger weight distortion errors.} In Section \ref{sec:experiment}, we conduct comprehensive experiments to evaluate how the filtering ratio affects the utility. 

\vspace{-5pt}
\subsection{Block Spectral-DP in FC Layer}\label{sec:blockFourierDP}
\vspace{-5pt}

In addition to fitting \ourmethod into CONV layers, our next question would be how to extend it to the fully connected (FC) layers. 
The weight matrix in an FC layer often has a much higher dimension than CONV layers which have a weight-sharing mechanism. Furthermore, unlike CONV layers, operations in FC layers cannot directly map to multiplication in the spectral domain. 
To address these, our key idea is to compress and restructure the weight matrix to facilitate the adoption of \ourmethod to FC layers. In this regard, the structure of a block circulant weight matrix~\cite{cirCNN_Ding,blockcirculant_1} is a suitable choice.  Each row vector in such a matrix is the circular shift form of the previous row, and the matrix vector multiplication in time domain can be simplified as vector-vector multiplication in the spectral domain. We further name this approach as \ourmethodblock.   
As we shall show later, \ourmethodblock not only mathematically supports the spectral transformation for adding gradient perturbation, but also compresses redundant weights in FC layers without impacting the utility.
% On the other hand, it is a kind of compression that removes sufficient redundancy which does not affect utility. 
\vspace{-8pt}
\subsubsection{Block circulant based FC layer}
We first introduce the definition of a block circulant matrix. Given a matrix $W$ of size $m\times n$, it is said that $W$ is block circulant if $W$ can be partitioned into $p\times q$ square blocks of circulant matrix $W_{ij}\in\mathbb{R}^{d\times d}$, where $d$ is defined as the block size (size of each sub-matrix block), $p=m\div d$, $q=n\div d$, $i\in\{1,2,\cdots,p\}$, and $j\in\{1,2,\cdots,q\}$. And each square block matrix $W_{i,j}$ is circulant as specified below:
$$\left [ \begin{array}{cccc}
    w_0 & w_1 & \cdots & w_{k-1} \\
    w_{k-1} & w_0 & \cdots & w_{k-2} \\
    \ddots & \ddots & \ddots & \ddots\\
    w_1 & w_2 & \cdots & w_0 \\
\end{array} \right ]$$
We note the circulant matrix can be represented by a vector $w=\{w_0, w_1, \cdots, w_{k-1}\}$%, and each row of the matrix $W$ is the circular shift form of the previous row. 
Consider a fully connected layer consisting of $m$ outputs and $n$ inputs and a block circulant weight matrix $W$.
Assume $W$ is partitioned into $p\times q$ blocks of circulant matrix, the forward propagation in the block circulant weight matrix based fully connected layer is given by:
\begin{equation}\label{eq:forward_block}
% \setlength\abovedisplayskip{1pt}
% \setlength\belowdisplayskip{3pt}
    A = WX = \left[
    \begin{array}{c}
         \sum_{j=1}^q W_{1,j}X_j \\
         \sum_{j=1}^q W_{2,j}X_j \\
         \vdots\\
         \sum_{j=1}^q W_{p,j}X_j 
    \end{array}
    \right]=\left[
    \begin{array}{c}
         A_1  \\
         A_2 \\
         \vdots \\
         A_p
    \end{array}
    \right]
\end{equation}
where the input $X$ is partitioned as $X=[X_1^T,X_2^T,\cdots,X_q^T]^T$, $A_i\in\mathbb{R}^k$ is a column vector that is the respective output of $\sum_{j=1}^q W_{i,j}x_j$. We further assume each square block sub-matrix $W_{i,j}$ is represented by a vector $w_{i,j}$ where $w_{i,j}$ is the first row of $W_{i,j}$, then according to the \textit{circulant convolution theorem} \cite{Pan2001Structured, Eberly1996Polynomial}, the computation of $W_{i,j}X_j$ can be expressed as $A_i=w_{i,j}*X_j=\calF^{-1} (\calF(w_{i,j})\odot \calF(X_j))$, where $*$ is the operator of circulant convolution. This process is shown in the left block of Figure \ref{fig:method_overview}(b).

We then consider the backward propagation training of the fully connected layer with the block circulant weight matrix. Let $J$ denote the cost function, and $A_{il}$ be the $l-th$ element in $A_i$, by the chain rule, the backward propagation process is derived as
\begin{equation}\label{eq:backward_block}
\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
    \frac{\partial J}{\partial w_{i,j}} = \sum_{l=1}^k \frac{\partial J}{A_{i,l}} \frac{\partial A_{i,l}}{w_{i,j}} = \frac{\partial J}{A_{i}} \frac{\partial A_i}{\partial w_{i,j}}
\end{equation}
We note $A_i$ is the circular convolution of $w_{ij}$ and $x_j$ which indicates that $\frac{\partial A_i}{\partial w_{ij}}$ is block circulant matrix. Hence, the computation of Eq.~(\ref{eq:backward_block}) can be expressed as the "Fourier Transform $\longrightarrow$ Element-wise Multiplication $\longrightarrow$ Inverse Fourier Transform". 
% \textcolor{red}{
The complexity analysis of computing the gradient is provided in Appendix \ref{apdx:complexity}. 
% }
% \ref{apdx:complexity}. }

\vspace{-5pt}
\subsubsection{Implementing \ourmethodblock into FC layer}
\begin{algorithm}[t]
% \small
    \caption{\ourmethodblock in a single FC layer}
    \label{alg:blockFourierDP}
  \begin{algorithmic}[1]
    \REQUIRE A block circulant weight matrix based FC layer with input $X$ and block circulant matrix $W$. $\frac{\partial{J}}{\partial{A}}$, $\{w_{i,j}\}$, $p$, $q$, block size $d$, $m$, $n$, clipping bound $S$,$\sigma$ filtering parameter $\rho\in(0,1)$ \\
    \textbf{Output: } 
    % noisy gradient $\tilde{\frac{\partial L}{\partial w_{i,j}}}$, 
    noisy gradient $\tilde{\frac{\partial{J}}{\partial{W}}}$\\
    $\bullet$  \textbf{\textit{Stage I:}} Noise addition
    \STATE \textbf{Compute:} \\$G_W^F=\{G_{W_{i,j}}^F\}$, where $i\in\{1,2,\cdots,p\}$, $j\in\{1,2,\cdots,q\} $.\\
    % \textbf{\textit{Stage II:}} Noise addition \\
    \STATE Clipping Norm: $\bar{G}_W^F=G_W^F/\max(1,\frac{\|G_w^F\|_2}{S})$
    \STATE \textbf{Gaussian mechanism:} \\$\tilde{G}_W^F= Gauss(\bar{G}_W^F,S,\sigma)$\\
    $\bullet$  \textbf{\textit{Stage II:}} Block-wise pruning and inverse Fourier Transform\\
    % \STATE Noise addition: $\tilde{\frac{\partial L}{\partial w_{i,j}}_F}=\bar{\frac{\partial L}{\partial w_{i,j}}_F}+\mathcal{N}(0,\sigma^2 C^2 \mathbb{I}_k)$
    \FOR {$i\in 1,2,\cdots,p$}
    \FOR {$j\in 1,2,\cdots,q$}
    \STATE \textbf{1D Spectral filtering:} \\$\hat{G}_{W_{i,j}}^F\leftarrow$Zero last $\rho$ of coefficients in $\tilde{G}_{W_{i,j}}^F$ 
    \STATE \textbf{Inverse Fourier transform:}\\ $\tilde{\frac{\partial J}{\partial W_{i,j}}}=\mathcal{F}^{-1}[\hat{G}_{W_{i,j}}^F]$
    \ENDFOR
    \ENDFOR
    \STATE $\tilde{\frac{\partial J}{\partial W}}=\{\tilde{\frac{\partial J}{\partial W_{i,j}}}\}$, $i\in\{1,\cdots,p\}$, $j\in\{1,\cdots,q\}$.
  \end{algorithmic}
\end{algorithm}
\setlength{\textfloatsep}{6pt}

We now demonstrate how to implement \ourmethodblock into the fully connected layer. Eq.~(\ref{eq:backward_block}), %can be expressed as ``Fourier Transform $\longright arrow Element-wise Multiplication $\longrightarrow$ Inverse Fourier Transform", 
allows us to apply \ourmethodblock into the spectral gradients of the parameters. Then for each square block, the spectral gradient is computed by
\begin{equation}
\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
    G^F_{w_{i,j}} = \calF(\frac{\partial J}{\partial A_i})\circ \calF(\frac{\partial A_i}{\partial w_{i,j}})
\end{equation}


Algorithm \ref{alg:blockFourierDP} outlines \ourmethodblock as applied to a fully connected layer. We denote $G_W^F$ as the spectral gradient of $W$ with respect to the cost function $J$. At stage I, the Gaussian mechanism is applied into the $G_W^F$ to ensure the differential privacy guarantee of the spectral gradients. Operations at stage II such as spectral filtering operation and inverse Fourier Transform are introduced as the post-processing of the Gaussian mechanism. We note operations are applied at the sub-matrix level. Unlike the CONV layer, we note that both (spectral) filtering ratio, and (block-circulant) compression ratio can impact the privacy level and utility, which we study in Section \ref{sec:experiment}.

\vspace{-5pt}
\subsection{Integrate \ourmethod into a DL model}\label{sec:DPtogether}
\vspace{-5pt}
In this section, we show how to apply \ourmethod to train a general deep learning model consisting of many such layers.
Specifically, at each training iteration, \ourmethod computes the per-sample spectral gradient $G^F$, bounds $G^F$ using $L_2$ norm clipping, and adds noise to the spectral gradient using Gaussian mechanism. Then \ourmethod applies spectral filtering to each layer. Without loss of generality, we present the detailed procedure of \ourmethod learning in Algorithm~\ref{alg:PutInDL}.

\begin{algorithm}[t]
% \small
    \caption{Training algorithm of \ourmethod in a deep learning model}
    \label{alg:PutInDL}
  \begin{algorithmic}[1]
    \REQUIRE A model with $L$ layers, model parameters $W$, clipping bound $\{C_l\}_l^L$, $\sigma$, filtering parameter $\rho\in(0,1)$, training samples $\{x_i,y_i\}_{i=1}^N$, batch size $B$, total training epochs $T_e$, cost function $J$ learning rate $\alpha$ \\
    \textbf{Output:} Model parameters after $T_e*N/B$ training iterations  $\hat{W}_{T_e*N/B}$ \\
    \FOR{$t\in[T_e*N/B]$}
    \STATE Sample a mini-batch of training samples $\{x_b,y_b\}_{b=1}^B$ by selecting each $\{x_i,y_i\}$ independently with probability $\frac{B}{N}$ using SGM.
    \STATE \textbf{\textit{Stage I:}} Noise addition\\
    \FOR {$b\in 1,2,\cdots,B$}
    \STATE \textbf{Compute per-sample spectral gradient:} \\$G^F(x_b,y_b)=\{G^F_{W_t^{<l>}}(x_b,y_b)\}_l^L$.\\
    % \textbf{\textit{Stage II:}} Noise addition \\
    \FOR {$l\in 1,2,\cdots,L$}
    \STATE \textbf{$L_2$ norm of clipping:} $\bar{G}^F_{W_t^{<l>}}(x_b,y_b)=G^F_{W_t^{<l>}}(x_b,y_b)/\max\{1,\frac{\|G^F_{W_t^{<l>}}(x_b,y_b)\|_2}{C_l}\}$
    \ENDFOR
    \ENDFOR
    \STATE ${G}_{sum}^F=\sum^{B}_{b=1}\bar{G}^F(x_b,y_b)$
    \STATE \textbf{Gaussian mechanism:} \\$\hat{G}_{sum}^F= Gauss({G}_{sum}^F,C,\sigma)$, where $C=\sqrt{\sum_{l=1}^{L}C_l^2}$ \\
    \textbf{\textit{Stage II:}} Pruning and Inverse Fourier Transform\\
    % \STATE Noise addition: $\tilde{\frac{\partial L}{\partial w_{i,j}}_F}=\bar{\frac{\partial L}{\partial w_{i,j}}_F}+\mathcal{N}(0,\sigma^2 C^2 \mathbb{I}_k)$
    \FOR {$l\in 1,2,\cdots,L$}
    \STATE \textbf{Spectral filtering and Inverse Fourier Transform:} $\tilde{G}_{sum}^F\leftarrow \mathcal{F}^{-1}(filtering(\hat{G}_{sum}^F))$ 
    \ENDFOR
    \STATE \textbf{Gradient descent}
    $\hat{W}_{t+1}\leftarrow W_{t}-\alpha \frac{1}{B}\tilde{G}_{sum}^F$
    \ENDFOR\\
    % \textbf{Output:} \textcolor{red}{$\hat{W}_{T_e*N/B}$} \\
  \end{algorithmic}
\end{algorithm}

At each training iteration $t$, the mini-batch $\{x_b,y_b\}_{b=1}^B$ is sampled using the Sampled Gaussian mechanism (SGM) \cite{2019arXiv190810530M}. In practical implementation, \ourmethod clips each $g_l$ by a different clipping norm $C_l$.% instead of clipping $g$ by one clipping norm. 
The $L_2$ norm of $G^F$ can be computed by $C=\sqrt{\sum_{l=1}^{L}C_l^2}$. Based on this clipping strategy, the noise scale in Gaussian mechanism is proportional to $C$ instead of $C_l$--the $L_2$ norm of each layer's gradients. This ensures that the perturbed gradients of all parameters have the same privacy level. In our experiments, we set equal $C_l$ and study the impact of the clipping norm in Section \ref{sec:experiment}. 

% We note that a single step \ourmethod private learning achieves $(\epsilon,\delta)$ differential privacy according to Theorem 3.22 in \cite{DP_algorithm}.
In Corollary \ref{cor:composition}, we leverage the RDP based privacy accountant as described in Section \ref{subsec:Background_DPSGD} to compute the overall differential privacy across $T$ epochs. 

\begin{restatable}{corollary}{CompositionCor}
\label{cor:composition}
% \begin{corollary}
     Algorithm \ref{alg:PutInDL} achieves $((T_e*N/B)\epsilon+\frac{\log(1/\delta)}{\alpha-1},\delta)-$DP if $\sigma=\frac{\sqrt{2\log(1.25/\delta)}}{\epsilon'}$ where $\epsilon'=\epsilon+\frac{\log(1/\delta)}{\alpha-1}$.
     \vspace{-5pt}
% \end{corollary}
\end{restatable}
\begin{proof}
    The detailed proof is provided in Appendix \ref{Appendix:proof_composition}.
    % \ref{Appendix:proof_composition}.}
\end{proof}
