\section{Motivation}
It is incontestable that neural networks become more redundant as the networks tend to be deeper and deeper. To simplify the neural networks, pruning is introduced to reducing the redundancy. The key idea of pruning is to remove neurons or parameters. Most pruning methods focus on removing neurons or parameters (near zero weights), and such methods are conducted on the spatial domain. Since pruning is a natural way to providing less variation, we explore if pruning can be applied into the gradient. Due to gradient contains much information, directly pruning the gradient on the signal or time domain causes a huge utility loss. We then focus on the pruning that is conducted on the spectral domain.

Consider a neural network that only contains convolution layers, the gradient of each convolution filter can be expressed an multiplication on the spectral domain by implementing Fourier transform. The spectral pruning removes the relatively unimportant spectral coefficients, and the resulting gradient is expressed as the inverse Fourier transform of the pruned spectral coefficients. We implement the network on the CIFAR-10 dataset with spectral pruning. The results show that the accuracy drop is small compared to the un-pruned network.

As the noise is added into the gradient to ensure the privacy guarantee, pruning the noisy gradient is a natural way to decreasing the scale of the noise. Due to gradient contains much information, directly pruning the gradient on the signal or time domain causes a huge utility loss. 

To overcome it, we explore how to conduct such pruning on the spectral domain. We note there are spatial redundancies on the convolution filers. Hence, pruning on the spectral domain  

we then explore the idea of Based on that, we explore how pruning works on we focus on the gradient pruning and how to conduct such pruning on the spectral domain.

We note there are spatial redundancies on the convolution filters, and we explore the spectral pruning The spectral pruning is achieved by pruning away unimportant spectral gradients. Specifically, it is implemented by 
To implement pruning on the differentially private training process, the noisy gradients are set to be zeros 

We note set weights as zeroPruning 

which The objective of 
The pruning methods are based on the spatial domain. Consider a scenario 
Since convolutional neural networks tend to be deeper and deeper which means more storage requirements and floating-point operations, there are many works devoting to simplify and accelerate the deep neural networks.
Due to the high dimensional data requires a larger neural network with more layers and nodes, reducing the memory cost and the computational is critical in the practical application. Model compression methods such as quantization, pruning, and structured weight matrix are widely used in reducing the complexity of deep learning model with little loss of accuracy. Bock circulant weight matrix is introduced in \cite{cirCNN_Ding,blockcirculant_1} to reduce the redundancy of the parameters in fully connected layer. The block circlulant matrix allows the use of Fourier transformation in the backward propagation. Based on this, we propose a differentially private perturbation in the architecture that contains fully connected layer with block circulant weight matrix. The proposed method utilizes the block circulant matrix as a dimension "reduction". And the key idea is that we first perturb the gradient in the Fourier domain then apply a Fourier filtering mechanism to limit the space of weights, thus resulting fewer noise variables to add in the training. 
