% \section{appendix}
% \appendix
\appendices
% \renewcommand{\thesection}{\Alph{section}.\arabic{section}}
% \setcounter{section}{0}

\section{}
\subsection{Proof of Theorem \ref{thm:FourierDP}}\label{Appendix:proof_privacythm}
\Privacythm*
\begin{proof}
    The proof relies on the Theorem 3.22 in \cite{DP_algorithm} and the post-processing property of DP algorithm. 
    First, we show that $\tilde{F^N}$ is ($\epsilon,\delta$) if $\sigma=\sqrt{2\log(1.25/\delta)}/\epsilon$. Since the mechanism of obtaining $\tilde{F^N}$ is a Gaussian mechanism, and the variance of the $2S^2\log(1.25/\delta)/\epsilon^2$ where $S$ is the sensitivity of $F^N$, Following the Theorem 3.22 in \cite{DP_algorithm}, $\tilde{F^N}$ is ($\epsilon,\delta$) differentially private under the condition that the $L_2$-sensitive of $F^N$ is $S$. 
    The spectral filtering and the inverse Fourier transformation are the post-processing of $\tilde{F}^N$. Since the post-processing does not change the differential privacy budget, the Fourier DP follows the same privacy budget as the Gaussian mechanism.
\end{proof}
\begin{theorem}{(Theorem 3.22 in \cite{DP_algorithm})}
    Let $\epsilon \in (0,1)$ be arbitrary. For $c^2 > 2ln(1.25/\delta)$, the Gaussian Mechanism with parameter $\sigma \ge c\triangle _2(f)/\epsilon$ is ($\epsilon,\delta$) differentially private.
\end{theorem}
% Theorem 3.22.~\cite{DP_algorithm} Let $\epsilon \in (0,1)$ be arbitrary. For $c^2 > 2ln(1.25/\delta)$, the Gaussian Mechanism with parameter $\sigma \ge c\triangle _2(f)/\epsilon$ is ($\epsilon,\delta$) differentially private.

\subsection{Proof of Proposition \ref{prop:noise_reduction}}\label{Appendix:noise_reduction}
\PropNoiseReduction*
\begin{proof}
    % \textcolor{red}{
    Let $V_i^K=P_K(V_i)$, then the resulting $v_n=\text{I-FT}(V_i^K)$. In detail, it can be formulated as
    $$\begin{array}{ccl}
        v_n& =&\frac{1}{\sqrt{N}}\sum_{i=0}^{N-1}V_i^K\cdot e^{\frac{j2\pi}{N}in} \\
        & =&\frac{1}{\sqrt{N}}\sum_{i=0}^{K}V_i\cdot e^{\frac{j2\pi}{N}in}\\
        & =&\frac{1}{\sqrt N}\sum_{i=0}^{K-1}\{V_i\}\cdot\cos(\frac{2\pi}{N}in)\\
        &&+ j\frac{1}{\sqrt N}\sum_{i=0}^{K-1}\{V_i\}\cdot\sin(\frac{2\pi}{N}in)
    \end{array}$$
    Define $c_{n,i}=\frac{1}{\sqrt{N}}\{V_i\}\cdot\cos(\frac{2\pi}{N}in)$, then we have
    $$\frac{1}{\sqrt N}\sum_{i=0}^{K-1}\{V_i\}\cdot\cos(\frac{2\pi}{N}in)=\sum_{i=0}^{K-1}c_{n,i}$$ 
    Following the property of the normal distribution, we have $$\sum_{i=0}^{K-1}c_{n,i}\sim\calN(0,\sum_{i=0}^{K-1}\frac{1}{N}\cdot\cos^2(\frac{2\pi}{N}in)\sigma^2S^2)$$
    Simplifying the variance of the distribution, we have $$\sum_{i=0}^{K}c_{n,i}\sim\calN(\frac{K}{N}\cdot\frac{\sigma^2S^2}{2})$$
    Similarly, we have $\frac{1}{\sqrt N}\sum_{i=0}^{K}\{V_i\}\cdot\sin(\frac{2\pi}{N}in)\sim\calN(\frac{K}{N}\cdot\frac{\sigma^2S^2}{2})$. This indicates that $v_n\sim\mathbb{C}\calN(\frac{K}{N}\cdot\sigma^2S^2)$ has the same scale of $\calN(\frac{K}{N}\cdot\sigma^2S^2)$
    % }
\end{proof}
\subsection{Proof of Proposition \ref{prop:noise_reduction}}\label{Appendix:noise_reduction2D}
\PropNoiseReductionConv*
\begin{proof}
    % \textcolor{red}{
    Let $V_{i,j}^K=P^K_{2D}(V_{i,j})$, then the resulting $v_{mn}=\mathcal{F}^{-1}(V_{ij}^K)$. In detail, it can be formulated as
    \begin{small}
    $$\begin{array}{ccl}
        v_{mn}& =&\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}V_{ij}\cdot e^{\frac{j2\pi}{N}im+jn} \\
        & =&\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\cos(\frac{2\pi}{N}(im+jn))+\\
        &&\sqrt{-1}\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\sin(\frac{2\pi}{N}(im+jn))
    \end{array}$$
    \end{small}
    Define $c_{mn,ij}=\frac{1}{N}\{V_{i,j}\}\cdot\cos(\frac{2\pi}{N}(im+jn))$, then we have
    $$\frac{1}{N}\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\cos(\frac{2\pi}{N}(im+jn))=\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}c_{mn,ij}$$ 
    Following the property of the normal distribution, we have 
    \begin{footnotesize}
    $$\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}c_{mn,ij}\sim\calN(0,\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\frac{1}{N^2}\cdot\cos^2(\frac{2\pi}{N}(im+jn))\sigma^2S^2)$$
    \end{footnotesize}
    Simplifying the variance of the distribution, we have $$\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}c_{mn,ij}\sim\calN(\frac{K^2}{N^2}\cdot\frac{\sigma^2S^2}{2})$$
    Similarly, we have $\sum_{i=0}^{K-1}\sum_{j=0}^{K-1}\{V_{ij}\}\cdot\sin(\frac{2\pi}{N}im+jn)\sim\calN(\frac{K^2}{N^2}\cdot\frac{\sigma^2S^2}{2})$. This indicates that $v_{mn}\sim\mathbb{C}\calN(\frac{K^2}{N^2}\cdot\sigma^2S^2)$ which has the same scale of $\calN(\frac{K^2}{N^2}\cdot\sigma^2S^2)$
    % }
\end{proof}
\subsection{Proof of Corollary \ref{cor:composition}}\label{Appendix:proof_composition}
\CompositionCor*
\begin{proof}
    % \textcolor{red}{
    In Algorithm \ref{alg:PutInDL}, let $\sigma=\frac{\sqrt{2\log(1.25/\delta)}}{\epsilon'}$ and $\epsilon'=\epsilon+\frac{\log(1/\delta)}{\alpha-1}$, the Gaussian mechanism guarantees $(\epsilon',\delta)-$DP following Theorem 3.22 in \cite{DP_algorithm}. This is equivalent with $(\alpha,\epsilon)-$RDP according to Proposition \ref{prop:RDP_conversion}.
    % }
    
    % \textcolor{red}{
    Proposition \ref{prop:RDP_composition} further shows Algorithm \ref{alg:PutInDL} satisfies $(\alpha,(T_e*N/B)\epsilon)-$RDP. The $(\alpha,(T_e*N/B)\epsilon)-$RDP can be converted back to $((T_e*N/B)\epsilon+\frac{\log(1/\delta)}{\alpha-1},\delta)-$DP based on the Proposition \ref{prop:RDP_conversion}.
    % }
\end{proof}

\subsection{Model architectures}\label{appendix:model_architecture}
Here we show the detailed model architectures of the model used in the paper.

\begin{table}[t!]
\centering
\caption{Architecture of model that only consists of Fully Connected (FC) layers. A layer name ending with BC means that the weights matrix of such layer is block circulant.}
\label{tab:model-1}
\begin{tabular}{cc|ccc}
\hline
\multicolumn{2}{c|}{Model-1}  &   \multicolumn{3}{c}{Circulant \texttt{Model1}}      
% \\& & \multicolumn{3}{c}{\textbf{Model-1}}
\\\hline
% \multirow{2}{*}{\textbf{Layer}} & \multirow{2}{*}{\textbf{Weight}} & \multirow{2}{*}{\textbf{Layer}} & \multirow{2}{*}{\textbf{Weight}} & \multirow{2}{*}{\textbf{Block}}     
Layer & Weight & Layer & Weight & Block\\ \hline
% &&&&\\\hline
FC1            & $784\times2048$  & FC1-BC  & $784\times2048$  & 8/16\\
FC2            & $2048\times1024$ & FC2-BC  & $2048\times1024$ & 8/16\\
FC3            & $1024\times160$  & FC3-BC  & $1024\times160$  & 8/16\\
FC4            & $160\times10$    & FC4-BC  & $160\times10$    & 10\\ \hline
\end{tabular}
\end{table}

\begin{table}[t!]
\centering
\caption{Architecture of \texttt{Model2} model that consists of convolutional layers. 
% The activation of the convolution layer is Tanh.
}
\label{tab:conv-1}
% \begin{tabular}{lc}
% \hline
% \multicolumn{2}{c}{\textbf{\texttt{Model2}}}                              \\ \hline
% \textbf{Layer}       & \textbf{Parameters}              \\ \hline
% \multirow{2}{*}{Convolution} & 32 filters with size $3\times3$, stride 1,\\& padding 1
% \\\hline
% Max-Pooling & $2\times2$, stride 2                       \\\hline
% \multirow{2}{*}{Convolution} & 64 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% Max-Pooling & $2\times2$, stride 2                       \\\hline
% \multirow{2}{*}{Convolution} & 64 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% Max-Pooling & $2\times2$, stride 2                       \\\hline
% \multirow{2}{*}{Convolution} & 64 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% Max-Pooling & $2\times2$, stride 2                       \\\hline
% \multirow{2}{*}{Convolution} & 10 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% Average     & Over spatial dimensions                    \\ \hline
% \end{tabular}

\begin{tabular}{ll}
\hline
\multicolumn{2}{c}{\texttt{Model2}}                           \\ \hline
Layer       & Parameters                             \\ \hline
Convolution & 32 filters of 3x3, stride 1, padding 1 \\
Max-Pooling & 2 × 2, stride 2                        \\
Convolution & 64 filters of 3x3, stride 1, padding 1 \\
Max-Pooling & 2 × 2, stride 2                        \\
Convolution & 64 filters of 3x3, stride 1, padding 1 \\
Max-Pooling & 2 × 2, stride 2                        \\
Convolution & 64 filters of 3x3, stride 1, padding 1 \\
Max-Pooling & 2 × 2, stride 2                        \\
Convolution & 10 filters of 3x3, stride 1, padding 1 \\
Average     & Over spatial dimensions                \\ \hline
\end{tabular}
\end{table}


\begin{table}[t!]
\centering
\caption{The architecture of \texttt{Model3} model for CIFAR10.}
\label{tab:conv-2}
\begin{tabular}{ll}
\hline
\multicolumn{2}{c}{\texttt{Model3}}                                \\ \hline
Layer           & Parameters                              \\ \hline
Convolution x2  & 32 filters of 3x3, stride 1, padding 1  \\
Max-Pooling     & 2 × 2, stride 2                         \\
Convolution x2  & 64 filters of 3x3, stride 1, padding 1  \\
Max-Pooling     & 2 × 2, stride 2                         \\
Convolution x2  & 128 filters of 3x3, stride 1, padding 1 \\
Max-Pooling     & 2 × 2, stride 2                         \\
Fully connected & 120 units                               \\
Fully connected & 10 units                                \\ \hline
\end{tabular}
\end{table}


\begin{table}[t!]
\centering
\caption{The architecture of ScatterCNN model for CIFAR10, with Tanh activations.}
\label{tab:scatterCNN}
\begin{tabular}{ll}
\hline
\multicolumn{2}{c}{ScatterCNN}                                \\ \hline
Layer           & Parameters                              \\ \hline
Convolution   & 64 filters of 3x3, stride 1, padding 1  \\
Max-Pooling     & 2 × 2, stride 2                         \\
Convolution   & 60 filters of 3x3, stride 1, padding 1 \\
Fully connected & 10 units                                \\ \hline
\end{tabular}
\end{table}

\begin{table*}[t]
\caption{Different settings of training epochs, filtering ratios and privacy budgets on CIFAR10 dataset for ResNet-18 transfer learning with 4CONV+2FC trainable layers (\texttt{Transfer3}).}
\centering
% \small
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|cccccccccc}
% \hline
% Training Epochs        & 10   & 10  & 10  & 20  & 40  & 10  & 10  & 10  & 20  & 21   \\
% Pruning Ratio          & 0.75 & 0.5 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2  \\
% Privacy Budget $(\epsilon,10^{-5})$ & 0.5  & 0.5 & 0.5 & 0.5 & 0.5 & 0.2 & 1   & 2   & 2   & 2.15 \\\hline
% Train Accuracy & 76.36\% & 77.50\% & 77.68\% & 76.83\% & 74.39\% & 71.44\% & 81.11\% & 83.82\% & 86.55\% & 88.12\% \\
% Test Accuracy  & 71.50\% & 72.61\% & 72.89\% & 71.98\% & 70.31\% & 68.52\% & 73.72\% & 74.53\% & 74.49\% & 75.32\% \\ \hline
% \end{tabular}
\begin{tabular}{c|ccccccccc}
\hline
Training Epochs        & 10      & 10      & 10      & 20      & 40      & 10      & 10      & 10  & 20\\
Filtering Ratio ($\rho$)          & 0.75    & 0.5     & 0.2     & 0.2     & 0.2     & 0.2     & 0.2     & 0.2  & 0.2  \\
Privacy Budget $(\epsilon,10^{-5})$ & 0.5     & 0.5     & 0.5     & 0.5     & 0.5     & 0.2     & 1       & 2 & 2 \\\hline
Train Accuracy & 76.36\% & 77.50\% & 77.68\% & 76.83\% & 74.39\% & 71.44\% & 81.11\% & 83.82\% & 86.55\% \\
Test Accuracy  & 71.50\% & 72.61\% & 72.89\% & 71.98\% & 70.31\% & 68.52\% & 73.72\% & 74.53\% & 74.49\% \\ \hline
\end{tabular}
% }
\label{tab:trans}
\end{table*}

% \textcolor{red}{
% \subsection{Transfer Learning using ResNeXt-29}
% \label{appendix:transfer_resnext}
% We further conduct experiments to show the effect of replacing DP-SGD with \ourmethod in transfer learning. We consider the setting proposed in \cite{tramer2021differentially} for transfer learning from CIFAR100 to CIFAR10. An FC layer-based model is trained on features that are extracted from a ResNeXt\cite{xie2017aggregated} model trained on CIFAR100. DP-SGD and \ourmethodblock are implemented on the FC layer-based model. Our results are given in Table \ref{tab:apdx-transfer}. In addition, we compare our results to a DP-SGD utility improvement method \cite{luo2021scalable} that uses transfer learning. \ourmethodblock outperforms the DP-SGD trained models in both \cite{tramer2021differentially} and \cite{luo2021scalable}.
% }

% \begin{table}[h!]
% \centering
% \caption{\textcolor{red}{Testing accuracy with different privacy budgets in transfer learning.} }
% \label{tab:apdx-transfer}
% \begin{tabular}{c|ccc}
% \hline
% ($\epsilon,10^{-5}$) & DP-SGD in \cite{tramer2021differentially} & Block Spectral-DP & \cite{luo2021scalable}  \\ \hline
% $\epsilon=0.5$            & -      & \textbf{80.29\%}              & 73.28\%  \\
% $\epsilon=1.0$          & -      & \textbf{80.81\%}              & 76.64\%  \\
% $\epsilon=1.5$           & -      & \textbf{81.71\%}             & 81.57\%  \\
% $\epsilon=2.0$         & 80.00\%   & -                 & -     \\
% $\epsilon=\infty$             & 84.00\%    & 84.00\%               & 94.10\%  \\ \hline
% \end{tabular}
% \end{table}





% \subsection{Ablation study on CIFAR100 transfer models}

% We further study the impact of hyperparameters setting on the CIFAR100 dataset. We retrain the 2 FC layers of the pretrained WRN-28-10 model from ImageNet32 to CIFAR100 with 20 epochs of training. We adopt different batch sizes, filtering ratios, block sizes, and privacy budgets and summarize the results in Table~\ref{tab:cifar100_ablation}. 

% The hyperparameter selection for transfer learning does not exactly follow the same trend as for training from scratch. First, we compare the impact of different batch sizes from 256 to 2048 with filtering ratio $\rho=0$ and $\epsilon=1$. As shown in Table~\ref{tab:cifar100_ablation}, the best accuracy is reached at a batch size of 256, and the model accuracy decreases as the batch size increases. Since transfer learning just retrain a small number of parameters, a smaller batch size can facilitate more fine-grained learning and ultimately result in improved utility.

% Next, we explore the impact of different filtering ratios in the model. We chose five different filtering ratios (0, 0.2, 0.4, 0.6, and 0.8) for the two FC layers with the accuracies shown in Figure~\ref{fig:cifar100_filtering_ratio}. A smaller filtering rate leads to better utility, which means that in our \ourmethodblock, the DP noise we add becomes smoother and the reconstruction error has a stronger impact on the utility.

% Block size does not dominate the utility in this case. We can observe from Figure~\ref{fig:cifar100_filtering_ratio} that a small block size (5) leads to slightly better model utility, but its impact is not as huge as the different choices of filtering ratios. It can help to make a tradeoff between efficiency and utility as a large block size compress more of the model and lead to fewer trainable parameters, but also decrease the model accuracy.

% % % Figure environment removed

% % \begin{table*}[t]
% % \caption{Different settings of block size, filtering ratio, privacy budgets, training epochs, and batch size on CIFAR100 dataset for WRN-28-10 transfer learning with 2 FC layers.}
% % \label{tab:cifar100_ablation}
% % \centering
% % \resizebox{\linewidth}{!}{
% % \begin{tabular}{c|ccccccccccccccc}
% % \hline
% % Block size      & 5    & 5    & 5   & 5   & 5   & 5   & 5   & 10  & 10  & 10  & 10  & 10  & 5   & 5   & 5    \\
% % Filtering Ratio ($\rho$)    & 0    & 0    & 0   & 0.2 & 0.4 & 0.6 & 0.8 & 0   & 0.2 & 0.4 & 0.6 & 0.8 & 0   & 0   & 0    \\
% % Privacy Budget $(\epsilon,10^{-5})$     & 1    & 1    & 1   & 1   & 1   & 1   & 1   & 1   & 1   & 1   & 1   & 1   & 2   & 4   & 4    \\
% % Training Epochs      & 20   & 20   & 20  & 20  & 20  & 20  & 20  & 20  & 20  & 20  & 20  & 20  & 20  & 20  & 20   \\
% % Batch size      & 2048 & 1024 & 256 & 512 & 512 & 512 & 512 & 512 & 512 & 512 & 512 & 512 & 512 & 512 & 2048 \\ \hline
% % Test Accuracy  & 73.65\% & 75.42\% & 77.68\% & 76.79\% & 76.30\% & 72.87\% & 66.65\% & 76.54\% & 76.11\% & 75.72\% & 73.09\% & 59.14\% & 76.98\% & 78.00\% & 77.60\% \\ \hline
% % \end{tabular}
% % }
% % \end{table*}

% % Figure environment removed

% \begin{table*}[t]
% \caption{Different settings of block size, filtering ratio, privacy budgets,  and batch size on CIFAR100 dataset for WRN-28-10 transfer learning with 2 FC layers.}
% \label{tab:cifar100_ablation}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|ccccccccccccc}
% \hline
% Block size      & 5  & 5  & 5  & 5  & 5   & 5   & 5   & 5   & 5  & 5  & 5  & 5  & 5  \\
% Filtering Ratio ($\rho$)   & 0  & 0  & 0  & 0  & 0.2 & 0.4 & 0.6 & 0.8 & 0  & 0  & 0  & 0  & 0  \\
% Privacy Budget ($\epsilon,10^{-5}$)    & 1  & 1  & 1  & 1  & 1   & 1   & 1   & 1   & 2  & 4  & 2  & 4  & 4  \\
% % Training Epochs      & 20 & 20 & 20 & 20 & 20  & 20  & 20  & 20  & 20 & 20 & 20 & 20 & 20 \\
% Batch size       & 2048    & 1024    & 512     & 256     & 256     & 256 & 256 & 256 & 256     & 256     & 512     & 512     & 2048    \\ \hline
% Test Accuracy  & 73.65\% & 75.42\% & 76.59\% & 77.68\% & 77.52\% &   76.83\%  &  73.25\%   &   66.52\%  & 77.78\% & 78.03\% & 76.98\% & 78.00\% & 77.60\%\\ \hline
% \end{tabular}
% }
% \end{table*}



\subsection{Ablation study on CIFAR10 transfer models}\label{appdix:ablation_cifar_transfer}
In this section, we also discuss the impact of hyperparameters in the transfer learning setting based on the \texttt{Model3} and CIFAR-10 dataset. We summarize the results with different pairs of training epochs, filtering ratio ($\rho$) and target privacy budget in Table~\ref{tab:trans}. First, we discuss the impact of training epochs for a target privacy budget training. We set a strict target privacy budget $\epsilon=0.5$ and conduct 10, 20, and 40 epochs of training. More training epochs results in less noise addition to each training step but leads to an increased number of training steps. Different choices of training epochs may affect the convergence and final accuracy of the model. 
As Table~\ref{tab:trans} shows, fewer training epochs leads to higher accuracy when $\epsilon=0.5$. However, when the privacy budget is relaxed to $\epsilon=2$, we observe the similar test accuracy for 10 and 20 epochs.



We thus conduct experiments with 10 training epochs and set $\rho$ to 0.2, 0.5 and 0.75 under target privacy budget ($0.5, 10^{-5}$). Under this tight budget, setting $\rho$ to 0.2 and 0.5 results in the similar model performance. However, unlike the cases in Section~\ref{subsec:effectiveness_spectral_dp} for the training from scratch models, in transfer learning, setting a smaller rate like 0.2 gives better performance. The reason is because during the transfer learning process, we usually only tune the final layers of a pre-trained model, a large filtering ratio indicates losing too much information, thus worse accuracy.
% To better explore the effectiveness of other parameters, we conduct experiments with 10 training epochs in the following experiments.

Finally, we draw training curves using 10 training epochs with  $\rho=0.2$ under different privacy budgets in Figure~\ref{fig:resnet18-dptrain}. It demonstrates that our \ourmethod allows the model to converge quickly in the first several epochs. When $\epsilon>= 0.5$, the training curves demonstrate the similar tradeoff between privacy budget and accuracy. The utility of the model is limited only when the privacy budget becomes very low ($\epsilon=0.2$). In this case, training after the 5th epoch cannot further improve the accuracy. Nevertheless, this result (68.52\%) is still higher than the DP-SGD trained result (66.72\%) under a more relaxed privacy budget $\epsilon=1$.

% Figure environment removed

% \textcolor{red}{The settings of the hyperparameters follow similar trends in the CIFAR100 model for transfer learning, details of which can be found in the Appendix.
% }










\subsection{Discussion on complexity}\label{apdx:complexity}

\subsubsection{Complexity of \ourmethod in CONV layer} We implement the DFT using the Fast Fourier Transform (FFT) algorithm. Given a 2D convolution with signal size $n\times n$ and kernel size $d\times d$, the computational complexity of conventional convolution is $O(n^2d^2)$. For FFT-based convolution, the complexity composes of two parts: First, the complexity of FFT operation is $O(n^2\log n)$ and there are three times for doing the FFT (including FFT of the signal, FFT of the kernel, and inverse FFT of the spectral multiplication). Second, the multiplication operation has a complexity of $4n^2$. Therefore, the complexity of FFT-based convolution is $O(n^2\log n)$. This implies that FFT-based convolutions are more computationally efficient if $log (n)<d^2$.

% \textcolor{red}{The computational complexity of conventional convolution depends on $n$ and $d$, while that of the FFT-based convolution depends on $n$. In particular, FFT-based convolution are more computationally efficient. }

% \textcolor{red}{
In Eq. (\ref{eq:conv_grad}), the gradient of $w_{i,j}$ is the convolution of the gradient of $A_I$ and $X_j$. In neural network, most commonly used sizes of $w_{i,j}$ are $3\times 3$ and $5\times 5$. When computing the gradient of such $w_{i,j}$, the size of $X_j$ is close to the size of $A_i$, leading a more efficient convolution using FFT.
% }

\subsubsection{Complexity of \ourmethodblock in FC layer}
% \textcolor{red}{
An advantage of Block Spectral-DP is that it employs a block circulant matrix to reduce storage complexity. Specifically, for a $d\times d$ block circulant matrix, the storage complexity can be reduced from $O(d^2)$ to $O(d)$ by using a block size of $d$. In addition to this, the use of a block circulant matrix can also result in a reduction in computational complexity. For example, in a fully connected layer with a block circulant matrix-based weight of size $m\times n$, the computational complexity can be reduced from $O(n^2)$ to $O(n\log n)$ \cite{cirCNN_Ding}.
% }

