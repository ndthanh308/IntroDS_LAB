\section{Preliminary}\label{sec:prelim}
In this section, we provide a brief background on differential privacy and DPSGD. 
In this work, we study the differentially private perturbation of the gradient descent in the neural network training with model compression. We first provide the basic definition of differential privacy and DPSGD. As we focus on the network with block circulant weight matrices, we provide preliminaries on how block circulant weight matrices works in the fully connected layer.

\subsection{Differential Privacy}
Differential privacy \cite{DP_algorithm} is a privacy definition that describes the privacy loss associated with application who utilizes from a database. It is defined on adjacent databases. We say two databases are adjacent if they differ only in a single entry. Hence, the different privacy is defined by
\begin{definition}\label{def:dp}
    A randomized algorithm $\mathcal{M}$ with domain $\mathcal{D}$ and $\mathcal{R}=\text{Range}(\mathcal{M})$ is $(\epsilon, \delta)-$differentially private if for all $S\subseteq\mathcal{R}$ and for any two adjacent sets $d,d'\in\mathcal{D}$:
    \begin{equation}\label{eq:dp_definition}
        \text{Pr}[\mathcal{M}(d)\in S]\leqslant \exp ^\epsilon \text{Pr}[\mathcal{M}(d')\in S] + \delta
    \end{equation}
\end{definition}
A common mechanism that achieve differential privacy is the Gaussian mechanism. Gaussian mechanism takes as input a sequence $Q$ with $l_2$ sensitivity S and a parameter $\sigma$ controlling the Gaussian noise.  The Gaussian mechanism is formulated as noise perturbation in the output of the sequence:
\begin{equation}\label{eq:GaussianMechanism}
    Gauss(Q,S,\sigma) \triangleq Q + \mathcal{N}(0,S^2\cdot\sigma^2)
\end{equation}
where $\mathcal{N}(0,S^2\cdot\sigma^2)$ is the Gaussian distribution with zero mean and variance $S^2\cdot\sigma^2$. It is noticed that the noise scale in Gaussian mechanism is proportation to the $L_2$ sensitivity. According to Theorem 3.22 in \cite{DP_algorithm}, for any $\epsilon,\delta\in(0,1)$, the Gaussian mechanism achieve $(\epsilon,\delta)-$differential privacy when $\sigma=\sqrt{2\log(1.25/\delta)}/\epsilon$.
\subsection{DP-SGD and RDP composition}
Differentially private stochastic gradient descent (DPSGD) introdcue differential privacy into deep learning by controlling the influence of the training data during the training process. Specifically in each training iteration, the per-example gradients $g$ with respect to the loss function $L$ is bounded by $l_2$ norm clipping with a threshold $C$. The Gaussian mechanism adds random noise $\calN(0,\sigma^2C^2\mathbb{I})$ into $\sum_{x_{i}\in \cal D}g(x_i)$ to perturb the gradients. The differential privacy of DP-SGD in a single iteration is guaranteed by the Gaussian mechanism. 
The composition property of the differential privacy allows us to compute the overall differential privacy for multiple training iterations. We note that \cite{RDP2017,RDP2019} provide the tightest privacy accountant analysis of the DP-SGD which is based on the R{\'{e}}nyi differential privacy (RDP). The RDP is defined by
\begin{definition}\label{def:RDP}
    A randomized algorithm $\mathcal{M}$ with domain $\mathcal{D}$ and $\mathcal{R}=\text{Range}(\mathcal{M})$ is $(\epsilon, \delta)-$RDP if for any two adjacent sets $d,d'\in\mathcal{D}$:
    \begin{equation}\label{eq:RDP_definition}
        D_{\alpha}(\calM(d)\|\calM(d'))\leqslant \epsilon
    \end{equation}
\end{definition}
where $D_{\alpha}(\cdot\|\cdot)$ is the R{\'{e}}nyi divergence between two probability distributions. More detailed, it is defined by
\begin{definition}\label{def:RenyiDiver}
    For two probability distributions $P$ and $Q$ defined on $\calX$ over the same space, and let $p$ and $q$ denote the densities of $P$ and $Q$ ,respectively, the  R{\'{e}}nyi divergence of order $\alpha>1$ is given by
    \begin{equation}\label{eq:Renyi-divergence_definition}
        D_{\alpha}(P\|Q)\triangleq\frac{1}{\alpha-1}\log\mathop\mbbE\limits_{x\thicksim Q}(\frac{P(x)}{Q(x)})^{\alpha}
    \end{equation}
\end{definition}

Proposition 1 in \cite{RDP2017} provides theoretical foundation of composing RDP mechanisms. In detail, for a mechanism $\calM_1$ with ($\alpha,\epsilon_1$)-RDP and another mechanism $\calM_2$ with ($\alpha,\epsilon_2$)-RDP, then the mechanism $(\calM_1,\calM_2)$ satisfies $(\alpha,\epsilon_1+\epsilon_2)$-RDP. And we note that the RDP mechanism can be converted into the DP using the proposition 3 in \cite{RDP2017} which indicates that an ($\alpha,\epsilon$)-RDP mechanism satisfies ($\epsilon+\frac{\log 1/\delta}{\alpha-1}, \delta$)-DP for any $\delta\in(0,1)$.

% \todo[inline]{Composition RDP}
% \wen{You shouldn't put the algorithm of DP-SGD in the background section, instead, just describe it at a high level. Usually you put only present your own algorithm like this. Also, background should be just some information necessary for readers to understand your technique, not everything.}
% \begin{algorithm}[h]
%     \caption{DPSGD}
%     \label{alg:dpsgd}
%   \begin{algorithmic}[1]
%     \REQUIRE Data entries $\{x_1,\cdots,x_N\}$, loss function $L(\theta)=\frac{1}{N}\sum_{i}L(\theta,x_i)$, learning rate $\eta_t$, noise scale $\sigma$, group size $L$, gradient norm bound $C$\\
%     \STATE \textbf{Initialize} $\theta_0$ randomly
%     \FOR{$t\in[T]$}
%     \STATE Sample a random group $L_t$ with sampling probability $L/N$
%     \STATE For each $x_i\in L_t$, compute $g_t(x_i)\leftarrow\nabla_{\theta_t}L(\theta_t,x_i)$
%     \STATE Clip gradients: $\bar{g}_t(x_i)\leftarrow g_t(x_i)/\max(1,\frac{\|g_t(x_i)\|_2}{C})$\\
%     {\fontfamily{qcr}\selectfont Gaussian Mechanism:}\\
%     \STATE Add noise into average gradient using Gaussian mechanism:
%     $\tilde{g}_t(x_i)\leftarrow\frac{1}{L}(\sum_i\bar{g}_t(x_i)+\mathcal{N}(0,\sigma^2C^2\textbf{I}))$
%     \STATE Gradient descent: $\theta_{t+1}\leftarrow\theta_t-\eta_t\tilde{g}_t$
%     \ENDFOR\\
%     \textbf{Output:} $\theta_T$ with the total privacy cost $(\epsilon,\delta)$ using privacy composition.
%   \end{algorithmic}
% \end{algorithm}

% The differential privacy budget $(\epsilon,\delta)$ is related to the privacy accounting algorithm. We note that \cite{RDP2017,RDP2019} provide the tightest privacy accountant analysis of the DP-SGD which is based on the R{\'{e}}nyi differential privacy.

\subsection{Fourier Transform}
\ourmethod takes Discrete Fourier Transform (DFT) as a tool which transforms data to spectral domain. We provides the primitive of the unitary DFT that does not change the norm. The 1D Discrete Fourier Transform (1D-DFT) transforms a sequence of $N$ numbers $\{x_n\}:=x_0, x_1, \cdots, x_{N-1}$ into another sequence of complex numbers, $\{X_k\}:=X_0, X_1,\cdots,X_{N-1}$. The transformation of $x_n$ is given by
$$X_k=\frac{1}{\sqrt{N}}\sum_{n=0}^{N-1}x_n\cdot e^{-\frac{j2\pi}{N}kn}$$
The inverse DFT is defined by
$$x_n=\frac{1}{\sqrt{N}}\sum_{k=0}^{N-1}X_k\cdot e^{\frac{j2\pi}{N}kn}$$
We say $X_k$ as the $k-$th coefficient in the spectral domain. For convenience, we write $X^N=DFT(x^N)$ as the DFT of a given sequence $\{x_n\}$, and $x^N=IDFT(X^N)$ as the inverse DFT of the obtained DFT coefficients.

The 2D Discrete Fourier Transform (2D-DFT) transforms a 2-dimensional sequence of $N\times N$ with the following 
$$X_{k_1,k_2}=\frac{1}{N}\sum_{n_2=0}^{N-1}\sum_{n_1=0}^{N-1}x_{n_1,n_2}\cdot e^{-\frac{j2\pi}{N}(k_1 n_1+k_2 n_2)}$$
where $X_{k_1,k_2}$ is the 2D DFT coefficients. The inverse 2D-DFT is defined by
$$x_{n_1,n_2}=\frac{1}{N}\sum_{k_2=0}^{N-1}\sum_{k_1=0}^{N-1}X_{k_1,k_2}\cdot e^{\frac{j2\pi}{N}(k_1 n_1+k_2 n_2)}$$

% \subsection{Block Circulant Matrix}\label{subsec:bcm}
% \wen{I feel the background is too lengthy and contains too much information. You might want to move the block-circulant matrix to the design part, instead of background. Again, why you would like to put the algorithm here.}



% The method for applying block circulant matrix into neural network has been well illustrated in \cite{cirCNN_Ding,blockcirculant_1}. Particularly in the fully connected layer, when the weight matrix is block circulant, the forward propagation of the layer can be expressed as the process of “FFT$\longrightarrow$Element-wise Multiplication$\longrightarrow$IFFT”. Algorithm  “FFT$\longrightarrow$Element-wise Multiplication$\longrightarrow$IFFT” operation by using the circular convolution theorem. 
% \begin{algorithm}[h]
%     \caption{Forward propagation of fully connected layer with block circulant weight matrix.}
%     \label{alg:forward_FC_BCM}
%   \begin{algorithmic}[1]
%     \REQUIRE $w_{i,j}$, $x$, $p$, $q$,$k$\\
%     \textbf{Output: } layer activation $a$\\
%     \STATE Initialize the $a$ with zeros
%     \FOR {$i\in 1,2,\cdots,p$}
%     \FOR {$j\in 1,2,\cdots,q$}
%     \STATE $a_i\leftarrow a_i+\text{ifft}(\text{fft}(w_{i,j})\circ \text{fft}(x_j))$
%     \ENDFOR
%     \ENDFOR
%     \STATE \textbf{Return} $a$
%   \end{algorithmic}
% \end{algorithm}
% Given the block circulant matrix $W$ and an input $x\in\mathbb{R}^n$, the computation of the fully connected layer is $a=\psi(W*x)$. For each circulant matrix $W_{i,j}$, the corresponded $a_j$ can be computed by $a_j=W_{i,j}*x_j=\text{ifft}(\text{fft}(w_{i,j})\circ \text{fft}(x_j))$, where $x_j$ is the partition of the input. $x=\{x_0,\cdots,x_j,\cdots,x_q\}$ and $\circ$ is the operation of element-wise multiplication. Hence, the forward propagation process in the fully connected layer is formulated in the Algorithm \ref{alg:forward_FC_BCM}. And the backward propagation process also follows the operation rule “FFT$\longrightarrow$Element-wise Multiplication$\longrightarrow$IFFT”. The backward propagation is formulated in the Algorithm \ref{alg:backward_FC_BCM}
% \begin{algorithm}[h]
%     \caption{Backward propagation of block-circulant matrix-based FC layer.}
%     \label{alg:backward_FC_BCM}
%   \begin{algorithmic}[1]
%     \REQUIRE $\frac{\partial L}{\partial a}$,$w_{i,j}$, $x$, $p$, $q$,$k$\\
%     \textbf{Output: } $\frac{\partial L}{\partial w_{i,j}}$, $\frac{\partial L}{\partial x}$\\
%     \STATE Initialize $\frac{\partial L}{\partial w_{i,j}}$, $\frac{\partial L}{\partial x}$ with zeros
%     \FOR {$i\in 1,2,\cdots,p$}
%     \FOR {$j\in 1,2,\cdots,q$}
%     \STATE $\frac{\partial L}{\partial w_{i,j}}\leftarrow \text{IFFT}(\text{FFT}(\frac{\partial L}{\partial a_{i}})\circ \text{FFT}(x'_j))$
%     % \STATE $\frac{\partial L}{\partial x_{j}}\leftarrow \frac{\partial L}{\partial x_{j}}+\text{ifft}(\text{fft}(\frac{\partial L}{\partial a_{i}})\circ \text{fft}(w_{i,j}))$
%     \ENDFOR
%     \ENDFOR
%     \STATE \textbf{Return} $\frac{\partial L}{\partial w_{i,j}}$, $\frac{\partial L}{\partial x}$
%   \end{algorithmic}
% \end{algorithm}
