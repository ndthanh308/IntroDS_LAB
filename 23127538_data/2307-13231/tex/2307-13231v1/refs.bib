Download
@INPROCEEDINGS{shokrietal:2017,
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)}, 
  title={Membership Inference Attacks Against Machine Learning Models}, 
  year={2017},
  volume={},
  number={},
  pages={3-18},
  doi={10.1109/SP.2017.41}}

@article{DP_algorithm,
author = {Dwork, Cynthia and Roth, Aaron},
title = {The Algorithmic Foundations of Differential Privacy},
year = {2014},
issue_date = {August 2014},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {9},
number = {3–4},
issn = {1551-305X},
url = {https://doi.org/10.1561/0400000042},
doi = {10.1561/0400000042},
abstract = {The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition.After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations — not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed.We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed.Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey — there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it.},
journal = {Found. Trends Theor. Comput. Sci.},
month = {aug},
pages = {211–407},
numpages = {197}
}

@inproceedings{dpsgd,
author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
title = {Deep Learning with Differential Privacy},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978318},
doi = {10.1145/2976749.2978318},
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {308–318},
numpages = {11},
keywords = {differential privacy, deep learning},
location = {Vienna, Austria},
series = {CCS '16}
}

@INPROCEEDINGS{yu2019differentially,  author={Yu, Lei and Liu, Ling and Pu, Calton and Gursoy, Mehmet Emre and Truex, Stacey},  booktitle={2019 IEEE Symposium on Security and Privacy (SP)},   title={Differentially Private Model Publishing for Deep Learning},   year={2019},  volume={},  number={},  pages={332-349},  doi={10.1109/SP.2019.00019}}

@article{papernot2021tempered, title={Tempered Sigmoid Activations for Deep Learning with Differential Privacy}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17123}, abstractNote={Because learning sometimes involves sensitive data, machine learning algorithms have been extended to offer differential privacy for training data. In practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the model architectures that already performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. To improve these tradeoffs, prior work introduces variants of differential privacy that weaken the privacy guarantee proved to increase model utility. We show this is not necessary and instead propose that utility be improved by choosing activation functions designed explicitly for privacy-preserving training. A crucial operation in differentially private SGD is gradient clipping, which along with modifying the optimization path (at times resulting in not-optimizing a single objective function), may also introduce both significant bias and variance to the learning process. We empirically identify exploding gradients arising from ReLU may be one of the main sources of this. We demonstrate analytically and experimentally how a general family of bounded activation functions, the tempered sigmoids, consistently outperform the currently established choice: unbounded activation functions like ReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the learning procedure fundamentals or differential privacy analysis. While the changes we make are simple in retrospect, the simplicity of our approach facilitates its implementation and adoption to meaningfully improve state-of-the-art machine learning while still providing strong guarantees in the original framework of differential privacy.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Papernot, Nicolas and Thakurta, Abhradeep and Song, Shuang and Chien, Steve and Erlingsson, Úlfar}, year={2021}, month={May}, pages={9312-9321} }

@inproceedings{
tramer2021differentially,
title={Differentially Private Learning Needs Better Features (or Much More Data)},
author={Florian Tramer and Dan Boneh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YTWGvpFOQD-}
}

@inproceedings{
yu2021do,
title={Do not Let Privacy Overbill Utility:  Gradient Embedding Perturbation for Private Learning},
author={Da Yu and Huishuai Zhang and Wei Chen and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=7aogOj_VYO0}
}

@inproceedings{cirCNN_Ding,
author = {Ding, Caiwen and Liao, Siyu and Wang, Yanzhi and Li, Zhe and Liu, Ning and Zhuo, Youwei and Wang, Chao and Qian, Xuehai and Bai, Yu and Yuan, Geng and Ma, Xiaolong and Zhang, Yipeng and Tang, Jian and Qiu, Qinru and Lin, Xue and Yuan, Bo},
title = {CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-Circulant Weight Matrices},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3124552},
doi = {10.1145/3123939.3124552},
abstract = {Large-scale deep neural networks (DNNs) are both compute and memory intensive. As the size of DNNs continues to grow, it is critical to improve the energy efficiency and performance while maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight pruning achieves good compression ratios but suffers from three drawbacks: 1) the irregular network structure after pruning, which affects performance and throughput; 2) the increased training complexity; and 3) the lack of rigirous guarantee of compression ratio and inference accuracy.To overcome these limitations, this paper proposes CirCNN, a principled approach to represent weights and process neural networks using block-circulant matrices. CirCNN utilizes the Fast Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the computational complexity (both in inference and training) from O(n2) to O(n log n) and the storage complexity from O(n2) to O(n), with negligible accuracy loss. Compared to other approaches, CirCNN is distinct due to its mathematical rigor: the DNNs based on CirCNN can converge to the same "effectiveness" as DNNs without compression. We propose the CirCNN architecture, a universal DNN inference engine that can be implemented in various hardware/software platforms with configurable network architecture (e.g., layer type, size, scales, etc.). In CirCNN architecture: 1) Due to the recursive property, FFT can be used as the key computing kernel, which ensures universal and small-footprint implementations. 2) The compressed but regular network structure avoids the pitfalls of the network pruning and facilitates high performance and throughput with highly pipelined and parallel design. To demonstrate the performance and energy efficiency, we test CirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN architecture achieves very high energy efficiency and performance with a small hardware footprint. Based on the FPGA implementation and ASIC synthesis results, CirCNN achieves 6 - 102X energy efficiency improvements compared with the best state-of-the-art results.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {395–408},
numpages = {14},
keywords = {block-circulant matrix, FPGA, acceleration, compression, deep learning},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@INPROCEEDINGS{blockcirculant_1,  author={Lin, Sheng and Liu, Ning and Nazemi, Mahdi and Li, Hongjia and Ding, Caiwen and Wang, Yanzhi and Pedram, Massoud},  booktitle={2018 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},   title={FFT-based deep learning deployment in embedded systems},   year={2018},  volume={},  number={},  pages={1045-1050},  doi={10.23919/DATE.2018.8342166}}

@INPROCEEDINGS{RDP2017,  author={Mironov, Ilya},  booktitle={2017 IEEE 30th Computer Security Foundations Symposium (CSF)},   title={Rényi Differential Privacy},   year={2017},  volume={},  number={},  pages={263-275},  doi={10.1109/CSF.2017.11}}

@article{RDP2019,
  author    = {Ilya Mironov and
               Kunal Talwar and
               Li Zhang},
  title     = {R{\'{e}}nyi Differential Privacy of the Sampled Gaussian Mechanism},
  journal   = {CoRR},
  volume    = {abs/1908.10530},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.10530},
  eprinttype = {arXiv},
  eprint    = {1908.10530},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10530.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{FourierDP_database2010,
author = {Rastogi, Vibhor and Nath, Suman},
title = {Differentially Private Aggregation of Distributed Time-Series with Transformation and Encryption},
year = {2010},
isbn = {9781450300322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807167.1807247},
doi = {10.1145/1807167.1807247},
abstract = {We propose the first differentially private aggregation algorithm for distributed time-series data that offers good practical utility without any trusted server. This addresses two important challenges in participatory data-mining applications where (i) individual users collect temporally correlated time-series data (such as location traces, web history, personal health data), and (ii) an untrusted third-party aggregator wishes to run aggregate queries on the data.To ensure differential privacy for time-series data despite the presence of temporal correlation, we propose the Fourier Perturbation Algorithm (FPAk). Standard differential privacy techniques perform poorly for time-series data. To answer n queries, such techniques can result in a noise of Θ(n) to each query answer, making the answers practically useless if n is large. Our FPAk algorithm perturbs the Discrete Fourier Transform of the query answers. For answering n queries, FPAk improves the expected error from Θ(n) to roughly Θ(k) where k is the number of Fourier coefficients that can (approximately) reconstruct all the n query answers. Our experiments show that k &lt;&lt; n for many real-life data-sets resulting in a huge error-improvement for FPAk.To deal with the absence of a trusted central server, we propose the Distributed Laplace Perturbation Algorithm (DLPA) to add noise in a distributed way in order to guarantee differential privacy. To the best of our knowledge, DLPA is the first distributed differentially private algorithm that can scale with a large number of users: DLPA outperforms the only other distributed solution for differential privacy proposed so far, by reducing the computational load per user from O(U) to O(1) where U is the number of users.},
booktitle = {Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data},
pages = {735–746},
numpages = {12},
keywords = {distributed noise addition, output perturbation, time-series data, participatory data mining, private data analysis},
location = {Indianapolis, Indiana, USA},
series = {SIGMOD '10}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}




@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{opacus,
  title={Opacus: {U}ser-Friendly Differential Privacy Library in {PyTorch}},
  author={Ashkan Yousefpour and Igor Shilov and Alexandre Sablayrolles and Davide Testuggine and Karthik Prasad and Mani Malek and John Nguyen and Sayan Ghosh and Akash Bharadwaj and Jessica Zhao and Graham Cormode and Ilya Mironov},
  journal={arXiv preprint arXiv:2109.12298},
  year={2021}
}




@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@inproceedings{ding2019req,
  title={REQ-YOLO: A resource-aware, efficient quantization framework for object detection on FPGAs},
  author={Ding, Caiwen and Wang, Shuo and Liu, Ning and Xu, Kaidi and Wang, Yanzhi and Liang, Yun},
  booktitle={proceedings of the 2019 ACM/SIGDA international symposium on field-programmable gate arrays},
  pages={33--42},
  year={2019}
}

@inproceedings{ding2017circnn,
  title={Circnn: accelerating and compressing deep neural networks using block-circulant weight matrices},
  author={Ding, Caiwen and Liao, Siyu and Wang, Yanzhi and Li, Zhe and Liu, Ning and Zhuo, Youwei and Wang, Chao and Qian, Xuehai and Bai, Yu and Yuan, Geng and others},
  booktitle={Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={395--408},
  year={2017}
}

@article{dong2020exploring,
  title={Exploring GPU acceleration of deep neural networks using block circulant matrices},
  author={Dong, Shi and Zhao, Pu and Lin, Xue and Kaeli, David},
  journal={Parallel Computing},
  volume={100},
  pages={102701},
  year={2020},
  publisher={Elsevier}
}

@misc{Stevens2022,
  doi = {10.48550/ARXIV.2202.05089},
  
  url = {https://arxiv.org/abs/2202.05089},
  
  author = {Stevens, Timothy and Ngong, Ivoline C. and Darais, David and Hirsch, Calvin and Slater, David and Near, Joseph P.},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Backpropagation Clipping for Deep Learning with Differential Privacy},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Andrew2019,
  doi = {10.48550/ARXIV.1905.03871},
  
  url = {https://arxiv.org/abs/1905.03871},
  
  author = {Andrew, Galen and Thakkar, Om and McMahan, H. Brendan and Ramaswamy, Swaroop},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Differentially Private Learning with Adaptive Clipping},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Pichapati2019,
  doi = {10.48550/ARXIV.1908.07643},
  
  url = {https://arxiv.org/abs/1908.07643},
  
  author = {Pichapati, Venkatadheeraj and Suresh, Ananda Theertha and Yu, Felix X. and Reddi, Sashank J. and Kumar, Sanjiv},
  
  keywords = {Machine Learning (cs.LG), Cryptography and Security (cs.CR), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AdaCliP: Adaptive Clipping for Private SGD},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{lee2018concentrated,
  title={Concentrated differentially private gradient descent with adaptive per-iteration privacy budget},
  author={Lee, Jaewoo and Kifer, Daniel},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1656--1665},
  year={2018}
}

@inproceedings{asi2021private,
  title={Private adaptive gradient methods for convex optimization},
  author={Asi, Hilal and Duchi, John and Fallah, Alireza and Javidbakht, Omid and Talwar, Kunal},
  booktitle={International Conference on Machine Learning},
  pages={383--392},
  year={2021},
  organization={PMLR}
}

@inproceedings{yu2021large,
  title={Large scale private learning via low-rank reparametrization},
  author={Yu, Da and Zhang, Huishuai and Chen, Wei and Yin, Jian and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={12208--12218},
  year={2021},
  organization={PMLR}
}

@article{nasr2020improving,
  title={Improving deep learning with differential privacy using gradient encoding and denoising},
  author={Nasr, Milad and Shokri, Reza and others},
  journal={arXiv preprint arXiv:2007.11524},
  year={2020}
}


@inproceedings{oyallon2015deep,
  title={Deep roto-translation scattering for object classification},
  author={Oyallon, Edouard and Mallat, St{\'e}phane},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2865--2873},
  year={2015}
}

@article{rubinstein2009learning,
  title={Learning in a large function space: Privacy-preserving mechanisms for SVM learning},
  author={Rubinstein, Benjamin IP and Bartlett, Peter L and Huang, Ling and Taft, Nina},
  journal={arXiv preprint arXiv:0911.5708},
  year={2009}
}


@article{zhang2012functional,
  title={Functional mechanism: regression analysis under differential privacy},
  author={Zhang, Jun and Zhang, Zhenjie and Xiao, Xiaokui and Yang, Yin and Winslett, Marianne},
  journal={arXiv preprint arXiv:1208.0219},
  year={2012}
}

@article{chaudhuri2008privacy,
  title={Privacy-preserving logistic regression},
  author={Chaudhuri, Kamalika and Monteleoni, Claire},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}


@article{chaudhuri2011differentially,
  title={Differentially private empirical risk minimization.},
  author={Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={3},
  year={2011}
}

@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={2014 IEEE 55th annual symposium on foundations of computer science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}

@article{papernot2016semi,
  title={Semi-supervised knowledge transfer for deep learning from private training data},
  author={Papernot, Nicolas and Abadi, Mart{\'\i}n and Erlingsson, Ulfar and Goodfellow, Ian and Talwar, Kunal},
  journal={arXiv preprint arXiv:1610.05755},
  year={2016}
}

@article{papernot2018scalable,
  title={Scalable private learning with pate},
  author={Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, {\'U}lfar},
  journal={arXiv preprint arXiv:1802.08908},
  year={2018}
}

@inproceedings{luo2021scalable,
  title={Scalable differential privacy with sparse network finetuning},
  author={Luo, Zelun and Wu, Daniel J and Adeli, Ehsan and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5059--5068},
  year={2021}
}

@inproceedings{acs2012differentially,
  title={Differentially private histogram publishing through lossy compression},
  author={Acs, Gergely and Castelluccia, Claude and Chen, Rui},
  booktitle={2012 IEEE 12th International Conference on Data Mining},
  pages={1--10},
  year={2012},
  organization={IEEE}
}



@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@book{Pan2001Structured,
author = {Pan, Victor Y.},
title = {Structured Matrices and Polynomials: Unified Superfast Algorithms},
year = {2001},
isbn = {0817642404},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Structure matrices serve as a natural bridge between the areas of algebraic computations with polynomials and numerical matrix computations, allowing cross-fertilization of both fields. This book covers most fundamental numerical and algebraic computations with Toeplitz, Hankel, Vandermonde, Cauchy, and other popular structured matrices. Throughout the computations, the matrices are represented by their compressed images, called displacements, enabling both a unified treatment of various matrix structures and dramatic saving of computer time and memory. The resulting superfast algorithms allow further dramatic parallel acceleration using FFT and fast sine and cosine transforms.}
}

@article{Eberly1996Polynomial,
author = {Eberly, Wayne},
title = {Polynomial and Matrix Computations Volume 1: Fundamental Algorithms (Dario Bini and Victor Pan)},
journal = {SIAM Review},
volume = {38},
number = {1},
pages = {161-165},
year = {1996},
doi = {10.1137/1038020},
URL = {https://doi.org/10.1137/1038020
},
eprint = {https://doi.org/10.1137/1038020
}
}



@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@inproceedings{cheng2022dpnas,
  title={Dpnas: Neural architecture search for deep learning with differential privacy},
  author={Cheng, Anda and Wang, Jiaxing and Zhang, Xi Sheryl and Chen, Qiang and Wang, Peisong and Cheng, Jian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6358--6366},
  year={2022}
}


@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}

@article{geyer2017differentially,
  title={Differentially private federated learning: A client level perspective},
  author={Geyer, Robin C and Klein, Tassilo and Nabi, Moin},
  journal={arXiv preprint arXiv:1712.07557},
  year={2017}
}


@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}


@inproceedings{truex2020ldp,
  title={LDP-Fed: Federated learning with local differential privacy},
  author={Truex, Stacey and Liu, Ling and Chow, Ka-Ho and Gursoy, Mehmet Emre and Wei, Wenqi},
  booktitle={Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking},
  pages={61--66},
  year={2020}
}


@inproceedings{stevens2022efficient,
  title={Efficient Differentially Private Secure Aggregation for Federated Learning via Hardness of Learning with Errors},
  author={Stevens, Timothy and Skalka, Christian and Vincent, Christelle and Ring, John and Clark, Samuel and Near, Joseph},
  booktitle={31st USENIX Security Symposium (USENIX Security 22)},
  pages={1379--1395},
  year={2022}
}

@article{regev2009lattices,
  title={On lattices, learning with errors, random linear codes, and cryptography},
  author={Regev, Oded},
  journal={Journal of the ACM (JACM)},
  volume={56},
  number={6},
  pages={1--40},
  year={2009},
  publisher={ACM New York, NY, USA}
}


@Article{CSIAM-AM-2-484,
author = {Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and Zhang, Yaoyu},
title = {Theory of the Frequency Principle for General Deep Neural Networks},
journal = {CSIAM Transactions on Applied Mathematics},
year = {2021},
volume = {2},
number = {3},
pages = {484--507},
abstract = {
Along with fruitful applications of Deep Neural Networks (DNNs) to realistic problems, recently, empirical studies reported a universal phenomenon of Frequency Principle (F-Principle), that is, a DNN tends to learn a target function from
low to high frequencies during the training. The F-Principle has been very useful in
providing both qualitative and quantitative understandings of DNNs. In this paper,
we rigorously investigate the F-Principle for the training dynamics of a general DNN
at three stages: initial stage, intermediate stage, and final stage. For each stage, a theorem is provided in terms of proper quantities characterizing the F-Principle. Our
results are general in the sense that they work for multilayer networks with general
activation functions, population densities of data, and a large class of loss functions.
Our work lays a theoretical foundation of the F-Principle for a better understanding of
the training process of DNNs.
},
issn = {2708-0579},
doi = {https://doi.org/10.4208/csiam-am.SO-2020-0005},
url = {http://global-sci.org/intro/article_detail/csiam-am/19447.html}
}

@inproceedings{xu2019training,
  title={Training behavior of deep neural network in frequency domain},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Xiao, Yanyang},
  booktitle={Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12--15, 2019, Proceedings, Part I 26},
  pages={264--274},
  year={2019},
  organization={Springer}
}

@inproceedings{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}


@article{de2022unlocking,
  title={Unlocking high-accuracy differentially private image classification through scale},
  author={De, Soham and Berrada, Leonard and Hayes, Jamie and Smith, Samuel L and Balle, Borja},
  journal={arXiv preprint arXiv:2204.13650},
  year={2022}
}

@inproceedings{zagoruyko2016wide,
  title={Wide Residual Networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={British Machine Vision Conference 2016},
  year={2016},
  organization={British Machine Vision Association}
}

@article{chrabaszcz2017downsampled,
  title={A downsampled variant of imagenet as an alternative to the cifar datasets},
  author={Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1707.08819},
  year={2017}
}

@ARTICLE{2019arXiv190810530M,
       author = {{Mironov}, Ilya and {Talwar}, Kunal and {Zhang}, Li},
        title = "{R{\'e}nyi Differential Privacy of the Sampled Gaussian Mechanism}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = 2019,
        month = aug,
          eid = {arXiv:1908.10530},
        pages = {arXiv:1908.10530},
          doi = {10.48550/arXiv.1908.10530},
archivePrefix = {arXiv},
       eprint = {1908.10530},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190810530M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}





