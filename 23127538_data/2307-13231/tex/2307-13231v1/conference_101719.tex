% \documentclass[conference]{IEEEtran}
\documentclass[conference,compsoc]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{clrscode}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{array}

\usepackage{amsthm}
\usepackage{makecell}
\usepackage{tikz}
\usepackage{pgfplots}
%\usetikzlibrary{spy}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{xspace}

\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{thmtools, thm-restate}
\usepackage{color,soul}

% \usepackage[toc,title,page]{appendix}


\newcommand{\mbbE}{\mathbb{E}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calR}{\mathbb{R}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\bTheta}{\mathbf{\Theta}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\htheta}{\hat{\theta}}
\newcommand*{\ie}{i.e.\@\xspace}

\declaretheorem{theorem}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
% \newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{problem}{Problem}
\newtheorem{corollary}{Corollary}

\newcommand{\ourmethod}{\textit{Spectral-DP}\xspace}
\newcommand{\ourmethodblock}{\textit{Block Spectral-DP}\xspace}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% \title{Conference Paper Title*\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }


% \input{revision_plan}
% \clearpage


% \title{ \bf Spectral-DP: \underline{D}eferentially \underline{P}rivate Deep Learning in \underline{Spectral} Domain} 

\title{ \bf Spectral-DP: \underline{D}ifferentially \underline{P}rivate Deep Learning through \underline{Spectral} Perturbation and Filtering} 

\author{\IEEEauthorblockN{Ce Feng}
\IEEEauthorblockA{\textit{Lehigh University} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
cef419@lehigh.edu}
\and
\IEEEauthorblockN{Nuo Xu}
\IEEEauthorblockA{\textit{Lehigh University} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
nux219@lehigh.edu}
\and
\IEEEauthorblockN{Wujie Wen}
\IEEEauthorblockA{\textit{Lehigh University} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
wuw219@lehigh.edu}
\and
\IEEEauthorblockN{Parv Venkitasubramaniam}
\IEEEauthorblockA{\textit{Lehigh University} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
pav309@lehigh.edu}
\and
\IEEEauthorblockN{Caiwen Ding}
\IEEEauthorblockA{\textit{University of Connecticut} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
caiwen.ding@uconn.edu}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\author{
\\[-5.0ex]
\IEEEauthorblockN{Ce Feng$^{*1}$, 
Nuo Xu$^{*1}$, 
\thanks{$^*$These authors contributed equally to this work. }
Wujie Wen$^1$, 
Parv Venkitasubramaniam$^1$, 
Caiwen Ding$^2$
}
\IEEEauthorblockA{\textit{
$^1$Lehigh University,
$^2$University of Connecticut
} \\
\{cef419, nux219, wuw219, pav309\}@lehigh.edu,
caiwen.ding@uconn.edu
} \\
% \textit{*These authors contributed equally}
\\[-5.0ex]
}


\maketitle
% \pagenumbering{arabic}

\begin{abstract}
Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present \ourmethod, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. % Rather than other improvements to DP-SGD in recent works by altering specifics of the deep learning architecture, or data preprocessing, \ourmethod propose an altering approach to noise addition. Compared to DP-SGD based perturbation methods, our approach is motivated by the fact that utility in differential privacy is highly sensitive to the noise scale. 
% As the differentially private stochastic gradient descent (DP-SGD) method achieves privacy at a significant utility cost, we explore noise reduction in the differentially private deep learning. In DP-SGD, the noise scale
% Specifically, we focus on differentially private perturbation in the spectral domain. 
%\ourmethod utilizes noise addition in a transformed domain, a \textit{spectral domain}, and achieves noise reduction by spectral domain filtering. 
We develop differentially private deep learning methods based on \ourmethod for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with \ourmethod to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement \ourmethod deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, \ourmethod is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.
\end{abstract}

% \begin{IEEEkeywords}
% differential privacy, spectral filtering, deep learning
% \end{IEEEkeywords}

\setcounter{section}{0}

\input{intro}
% \input{background}
\input{preliminary}
\input{method}
\input{experiment}
\input{related_work}

\vspace{-3pt}
\section{Conclusion}\label{sec:conclusion}
\vspace{-3pt}
In this work, we propose \ourmethod, an alternative to DP-SGD in the context of differentially private deep learning. \ourmethod combines differentially private noise addition in the spectral domain with spectral filtering which enables reduction of  noise scale to improve utility. Our extensive experimental results show that our approach has uniformly better privacy utility tradeoff compared to state-of-the-art methods. Our contribution is a new paradigm to gradient perturbation in the context of deep learning, which can be further built upon. For instance, Fourier is only one example of a unitary transformation, and although it is widely used, other transformations could be considered for spectral perturbation. Likewise, developing alternative weight restructuring, and more general filtering approaches might yield interesting and broader insights into the general principle of spectral domain based methods to achieve differential privacy in deep learning. %we consider a basic filtering approach We develop differentially private deep learning algorithms based on \ourmethod approach for general layers of neural network architectures including convolution layer and fully connected layer. Specifically, we develop a variant of the fundamental approach, \ourmethodblock, where we introduce a spatial compression using block circulant matrices to the spectral gradient domain. We implement \ourmethod based deep learning to two benchmark datasets and compare it with DP-SGD and other DP-SGD driven approaches , which shows \ourmethod based learning achieves better privacy utility tradeoff.


\section*{Acknowledgment}
This research was partially supported by the National Science Foundation through the grants CCF-1617889, CCF-2011236, CCF-2006748, and partially through a Lehigh internal CORE grant.


\bibliographystyle{plain}
\bibliography{refs}


\input{Appendix}
\end{document}
