

\section{Experiment}\label{sec:experiment}

\subsection{Experiment setup}\label{subsec:exper_setup}

\noindent \textbf{Experimental Environment.} %In our evaluation, all methods are implemented in Pytorch~\cite{paszke2019pytorch}. 
We use Pytorch~\cite{paszke2019pytorch} to implement \ourmethod and 
% the
% state-of-the-art 
DP-SGD~\cite{dpsgd}.
% approaches. 
For a fair comparison, we follow the provided codes of Opacus~\cite{opacus} to build and train the models for DP-SGD.
All experiments are conducted on a Linux PC with AMD Ryzen Threadripper Pro 3975WX 32-Core Processor, 256 GB memory and NVIDIA GeForce RTX 3090 GPU with 24 GB graphic memory.
 

\noindent \textbf{Datasets.} We evaluate the proposed \ourmethod training on
% benchmark 
% public image classification datasets MNIST and CIFAR10. 
public image classification datasets MNIST, CIFAR10 and CIFAR100. 
MNIST~\cite{lecun1998gradient}
% image dataset 
consists of 70,000 images of 28 × 28 handwritten grayscale digit,
% and the task is to recognize the digits, 
% The dataset is splited 
60,000 images for training and 10,000 for testing. CIFAR10~\cite{krizhevsky2009learning} (32 $\times$ 32 RGB) has 50,000 training images and 10,000 testing images from 10 classes. 
% \textcolor{red}{
CIFAR100~\cite{krizhevsky2009learning} (32 $\times$ 32 RGB) contains 100 classes, each with 500 training images and 100 test images.
% }


% \begin{table}[h]
% \centering
% \caption{Architecture of \texttt{Model2} model that consists of convolutional layers. 
% % The activation of the convolutional layer is Tanh.
% }
% \label{tab:conv-1}
% % \begin{tabular}{lc}
% % \hline
% % \multicolumn{2}{c}{\textbf{\texttt{Model2}}}                              \\ \hline
% % \textbf{Layer}       & \textbf{Parameters}              \\ \hline
% % \multirow{2}{*}{Convolution} & 32 filters with size $3\times3$, stride 1,\\& padding 1
% % \\\hline
% % Max-Pooling & $2\times2$, stride 2                       \\\hline
% % \multirow{2}{*}{Convolution} & 64 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% % Max-Pooling & $2\times2$, stride 2                       \\\hline
% % \multirow{2}{*}{Convolution} & 64 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% % Max-Pooling & $2\times2$, stride 2                       \\\hline
% % \multirow{2}{*}{Convolution} & 64 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% % Max-Pooling & $2\times2$, stride 2                       \\\hline
% % \multirow{2}{*}{Convolution} & 10 filters with size $3\times3$, stride 1,\\&  padding 1 \\\hline
% % Average     & Over spatial dimensions                    \\ \hline
% % \end{tabular}
% 
% \begin{tabular}{ll}
% \hline
% \multicolumn{2}{c}{\texttt{Model2}}                           \\ \hline
% Layer       & Parameters                             \\ \hline
% Convolution & 32 filters of 3x3, stride 1, padding 1 \\
% Max-Pooling & 2 × 2, stride 2                        \\
% Convolution & 64 filters of 3x3, stride 1, padding 1 \\
% Max-Pooling & 2 × 2, stride 2                        \\
% Convolution & 64 filters of 3x3, stride 1, padding 1 \\
% Max-Pooling & 2 × 2, stride 2                        \\
% Convolution & 64 filters of 3x3, stride 1, padding 1 \\
% Max-Pooling & 2 × 2, stride 2                        \\
% Convolution & 10 filters of 3x3, stride 1, padding 1 \\
% Average     & Over spatial dimensions                \\ \hline
% \end{tabular}
% \end{table}



% \begin{table*}[t]
% \centering
% \caption{Results for different models with DP-SGD and proposed \ourmethod training}
% \label{tab:best_res}
% \begin{tabular}{c|cccc|cc}
% \hline
%             & \multicolumn{4}{c|}{Training from scratch}                                                     & \multicolumn{2}{c}{Transfer   learning} \\ \hline
% Dataset     & \multicolumn{2}{c|}{MNIST}                                  & \multicolumn{2}{c|}{CIFAR10}     & \multicolumn{2}{c}{CIFAR10}             \\ \hline
% Model &
%   \multicolumn{1}{c|}{Model-1} &
%   \multicolumn{1}{c|}{LeNet-5} &
%   \multicolumn{1}{c|}{Conv-1} &
%   \texttt{Model3} &
%   \multicolumn{1}{c|}{ResNext-29} &
%   ResNet-18 \\ \hline
% Privacy budget &
%   \multicolumn{1}{c|}{(2, $10^{-5}$)} &
%   \multicolumn{1}{c|}{(2, $10^{-5}$)} &
%   \multicolumn{1}{c|}{(7.53, $10^{-5}$)} &
%   \multicolumn{1}{c|}{(7.53, $10^{-5}$)}
%   &
%   \multicolumn{1}{c|}{} &
%   (2.89, $10^{-5}$) \\ \hline
% Non-Private & \multicolumn{1}{c|}{97.87\%} & \multicolumn{1}{c|}{99.15\%} & \multicolumn{1}{c|}{77.23\%} & 81.22\% & \multicolumn{1}{c|}{-}     & 75.94\%     \\ \hline
% DP-SGD       & \multicolumn{1}{c|}{94.77\%} & \multicolumn{1}{c|}{96.40\%} & \multicolumn{1}{c|}{59.96\%} & 62.82\% & \multicolumn{1}{c|}{-}     & 69.33\%     \\ \hline
% \ourmethod & \multicolumn{1}{c|}{96.30\%} & \multicolumn{1}{c|}{97.51\%} & \multicolumn{1}{c|}{72.70\%} & - & \multicolumn{1}{c|}{-}     & 76.08\%     \\ \hline
% \end{tabular}

% \end{table*}





\noindent \textbf{Models.}
% We conduct experiments on different model architectures to  demonstrate the effectiveness of our method. 
We evaluate the model using two training schemes.
% both (1) training from scratch and (2) transfer learning scheme.
In the first scheme, training from scratch, we apply block circulant matrix and perform DP training on all layers. We choose four neural networks with different structures. The first model (\texttt{Model1}) follows the structure evaluated in~\cite{blockcirculant_1}, which contains 4 FC layers with 2048, 1024, 160 and 10 neurons.
% To apply \ourmethodblock on all FC layers, 
% We apply block circulant matrix in all layers and evaluate the performance.
% on MNIST. 
The second model (\texttt{Model2}) uses a similar architecture as~\cite{papernot2021tempered}, consisting of 5 CONV layers with Tanh ($\cdot$) activation function. The detailed structure is shown in Table~\ref{tab:conv-1} in Appendix \ref{appendix:model_architecture}.
% and is trained using \ourmethod on the CIFAR10 dataset.
We further use the LeNet-5~\cite{lecun1998gradient} for MNIST and a model (\texttt{Model3}) that is a similar variant of \texttt{Model2} with 6 convolutional layers and 2 FC layers for CIFAR10 as details shown in Table~\ref{tab:conv-2} in Appendix \ref{appendix:model_architecture}. 
In the second scheme, transfer learning, we load a ResNet-18~\cite{he2016deep} model that is trained over a public dataset (ImageNet~\cite{krizhevsky2012imagenet}) and perform transfer training on CIFAR10.
Furthermore, we use a ResNeXt-29\cite{xie2017aggregated} for transfer learning from CIFAR100 to CIFAR10, following the settings in \cite{tramer2021differentially}. We also follow the state-of-the-art work~\cite{de2022unlocking} by using a Wide-ResNet (WRN-28-10) model~\cite{zagoruyko2016wide} pretrained on down-sampled 32$\times$32 ImageNet images~\cite{chrabaszcz2017downsampled} to perform the transfer learning on CIFAR10 and CIFAR100 datasets.
DP training is applied to the predefined trainable layers. 


% % \textcolor{red}{add scatternet description}
% \begin{table}[]
% \caption{The architecture of \texttt{Model3} model for CIFAR10.}
% \label{tab:conv-2}
% \begin{tabular}{ll}
% \hline
% \multicolumn{2}{c}{\texttt{Model3}}                                \\ \hline
% Layer           & Parameters                              \\ \hline
% Convolution x2  & 32 filters of 3x3, stride 1, padding 1  \\
% Max-Pooling     & 2 × 2, stride 2                         \\
% Convolution x2  & 64 filters of 3x3, stride 1, padding 1  \\
% Max-Pooling     & 2 × 2, stride 2                         \\
% Convolution x2  & 128 filters of 3x3, stride 1, padding 1 \\
% Max-Pooling     & 2 × 2, stride 2                         \\
% Fully connected & 120 units                               \\
% Fully connected & 10 units                                \\ \hline
% \end{tabular}
% \end{table}



% In the second scheme, transfer learning, we load a ResNet-18~\cite{he2016deep} model that is trained over a public dataset (ImageNet~\cite{krizhevsky2012imagenet}) and perform transfer training on CIFAR10. DP training is applied to the predefined trainable layers. 
% We further train the model with different numbers of trainable layers and freeze the rest to

% demonstrate our benefit in more complex training scenarios.

% We conduct experiments on models consisting of only FC or convolutional layers to better demonstrate that \ourmethod and \ourmethodblock can work well on these types of layers. We further train the model with both CONV and FC layers.
% to show that our framework can be applied to conventional models.
% and for cases of training from scratch and transfer learning.
% For MNIST and FashionMNIST, we train a block circulant matrix based neural network with all fully connected layers and implement the Fourier perturbation into the network. For CIFAR10, we conduct a transfer learning with Fourier perturbation. 



\noindent {\bf Evaluation metrics.}
% \vspace{-1mm}
% We use two metrics to evaluate the effectiveness of DP training.
% \begin{itemize}
% \item 
\textit{Testing accuracy:} 
the accuracy of a DP trained method on the testing set.
%We use the testing accuracy of the regular trained model as the baseline. 
A good defense should obtain the testing accuracy close to that of a model trained without differential privacy.
% \item 
\textit{Privacy Budget ($\epsilon, \delta$):} 
to measure the privacy constraint of DP training.
We set $\delta=10^{-5}$ for all training and conduct training in two ways. The first way gives a target privacy budget ($\epsilon, \delta$) and sets the training epochs. We report the model accuracy after training. The second way sets the noise scale ($\sigma$) and trains the model for at most 200 epochs, we report the best accuracy epoch and the corresponding accumulated privacy budget ($\epsilon, \delta$).
% \end{itemize}



\noindent \textbf{Parameters setting and general guidelines.}
% \textcolor{red}{add scatternet model setting and move part of guidelines to ablation study part}
% \wen {A paragraph discussing your evaluation methods and metrics for comparison, and the comparable baselines, why select this.}
% \wen {Here I mean the general guidelines for setting these parameters, some rules, methodology wise}
% and conduct training in two ways. First approach gives a target privacy budget ($\epsilon, \delta$) and sets the training epochs, we report the model accuracy after the training. In our evaluation, we set the training epochs to 20.  The second approach sets the noise scale ($\sigma$) and trains the model for at most 200 epochs, we report the best accuracy epoch and the corresponding accumulated privacy budget ($\epsilon, \delta$).
% We conduct grid-search on hyper-parameters with different pairs of learning rates and clipping bounds following previous works~\cite{dpsgd, papernot2021tempered} and report the best results. 
The key parameter for \ourmethod is the filtering ratio $\rho$ that controls the balance between the DP noise and reconstruction noise. We will discuss the impact of choosing different filtering ratios in Section~\ref{subsec:effectiveness_spectral_dp} and ~\ref{subsec:block_spectral_dp}.  In general, we find that while filtering more coefficients leads to a reduction in the added DP noise, a high filtering ratio results in a greater loss of utility. As a general rule, we  keep at least 50\% of the coefficients in the frequency domain. For more complex tasks, a smaller filtering rate (leaving more coefficients to add noise) may lead to a better utility-privacy tradeoff.

Block size is another hyperparameter used in \ourmethodblock to determine the size of the block circulant matrix used in FC layer.
% A large block size provides a high compression rate and fewer parameters.
% to manipulate. 
% However, setting the block size too large could hurt the model utility. 
% Thus 
% We follow the previous work
Similar to existing work~\cite{ding2019req, cirCNN_Ding,dong2020exploring}, 
specifically, we set the block size of the final FC layer as 10, and other layers as 8 for all models. 
% In \ourmethod, 
We set a uniform clipping norm to 0.5 for \texttt{Model1}, while other models as 0.1.
% 0.1 for 
% all models except \texttt{Model1}. The clipping bound for \texttt{Model1} model with all FC layers is set to 0.5.
For training from scratch models, we set the learning rate as 0.01 for \texttt{Model1} and \texttt{Model2} models, and 0.001 for LeNet-5 and \texttt{Model3}.
% conventional models ( LeNet-5 and \texttt{Model3}.
% ), respectively. 
The batch size is 500.
% We further discuss the selection of clipping norm, learning rate and batch size
% % in detail 
% in Section~
% \ref{subsec:ablation}.
% The batch size is set to 500 and pruning ratio is 0.5.
For the ResNet-18 model used in transfer learning, we choose a learning rate of 0.001, a clipping bound $C=0.1$, and a filtering ratio $\rho=0.2$ for CONV layers. The batch size is  256. For the last two FC layers with 160 and 10 neurons, block sizes for BCM are 16 and 10 with the number of preserved coefficients $k=8$. 


% Figure environment removed


% ==============================original table====================================
% \begin{table}[t]
% \centering
% \caption{Results for different models with DP-SGD and proposed \ourmethod training.}
% \label{tab:best_res}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{c|cccc|c}
% \hline
%             & \multicolumn{4}{c|}{Training from scratch}                                                          & Transfer \\ \hline
% Dataset     & \multicolumn{2}{c|}{MNIST}                                  & \multicolumn{2}{c|}{CIFAR10}          & CIFAR10           \\ \hline
% Model       & \multicolumn{1}{c|}{ \texttt{Model1}} & \multicolumn{1}{c|}{LeNet-5} & \multicolumn{1}{c|}{ \texttt{Model2}}  & \texttt{Model3} & ResNet-18         \\ \hline
% Privacy budget & \multicolumn{1}{c|}{(2, $10^{-5}$)} & \multicolumn{1}{c|}{(2, $10^{-5}$)} & \multicolumn{1}{c|}{(3, $10^{-5}$)} & (3, $10^{-5}$) & (2, $10^{-5}$) \\ \hline 
% Non-Private & \multicolumn{1}{c|}{97.87\%} & \multicolumn{1}{c|}{99.15\%} & \multicolumn{1}{c|}{77.23\%} & 81.22\%  & 75.94\%           \\ 
% DP-SGD       & \multicolumn{1}{c|}{92.05\%} & \multicolumn{1}{c|}{95.95\%} & \multicolumn{1}{c|}{55.15\%} & 57.58\%  & 68.93\%           \\ 
% \ourmethod & \multicolumn{1}{c|}{97.1\%} & \multicolumn{1}{c|}{98.03\%} & \multicolumn{1}{c|}{61.88\%} & 69.51\%  & 74.49\%           \\ \hline
% Max gain & \multicolumn{1}{c|}{34.85\%} & \multicolumn{1}{c|}{13.48\%} & \multicolumn{1}{c|}{19.18\%} &	25.45\% &	25.92\%
%      \\ 
% Average gain & \multicolumn{1}{c|}{10.70\%} & \multicolumn{1}{c|}{4.49\%} & \multicolumn{1}{c|}{11.91\%} & 19.92\%  & 11.53\%           \\ \hline
% \end{tabular}
% }
% \vspace{-10pt}
% \end{table}
% ==============================original table====================================


\begin{table}[t]
\centering
\caption{Results for different models with DP-SGD and proposed \ourmethod training.}
\label{tab:best_res}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|cccc}
\hline
            % & \multicolumn{4}{c|}{Training from scratch}                                                          & Transfer \\ \hline
Dataset     & \multicolumn{2}{c|}{MNIST}                                  & \multicolumn{2}{c}{CIFAR10}                    \\ \hline
Model       & \multicolumn{1}{c|}{ \texttt{Model1}} & \multicolumn{1}{c|}{LeNet-5} & \multicolumn{1}{c|}{ \texttt{Model2}}  & \texttt{Model3}          \\ \hline
Privacy budget & \multicolumn{1}{c|}{(2, $10^{-5}$)} & \multicolumn{1}{c|}{(2, $10^{-5}$)} & \multicolumn{1}{c|}{(3, $10^{-5}$)} & (3, $10^{-5}$)  \\ \hline 
Non-Private & \multicolumn{1}{c|}{97.87\%} & \multicolumn{1}{c|}{99.15\%} & \multicolumn{1}{c|}{77.23\%} & 81.22\%           \\ 
DP-SGD       & \multicolumn{1}{c|}{92.05\%} & \multicolumn{1}{c|}{95.95\%} & \multicolumn{1}{c|}{55.15\%} & 57.58\%             \\ 
\ourmethod & \multicolumn{1}{c|}{97.1\%} & \multicolumn{1}{c|}{98.03\%} & \multicolumn{1}{c|}{61.88\%} & 69.51\%           \\ \hline
Max gain & \multicolumn{1}{c|}{34.85\%} & \multicolumn{1}{c|}{13.48\%} & \multicolumn{1}{c|}{19.18\%} &	25.45\% 
     \\ 
Average gain & \multicolumn{1}{c|}{10.70\%} & \multicolumn{1}{c|}{4.49\%} & \multicolumn{1}{c|}{11.91\%} & 19.92\%           \\ \hline
\end{tabular}
}
\vspace{-5pt}
\end{table}

\vspace{-5pt}
% \subsection{Overall Comparison--\ourmethod vs. DP-SGD}
\subsection{\ourmethod for Training from Scratch Models}
\vspace{-5pt}
\label{sccratch}
% In our experiment, 
% We compare \ourmethod with DP-SGD training on different models and datasets. 
We set target privacy budget to $(2, 10^{-5})$ for MNIST and $(3, 10^{-5})$ for CIFAR10 and train the model for 30 epochs from scratch. 
% For transfer learning, we run 20 epochs of training with target privacy budget $(2, 10^{-5})$. 
The test accuracy and privacy budget plots for all models are shown in Figure~\ref{fig:privacy_accuracy}. The best accuracy is reported in Table~\ref{tab:best_res}.
\ourmethod performs well for MNIST tasks on small models and gains more benefits from more sophisticated model training for CIFAR10 tasks. 
Table~\ref{tab:best_res} shows that \ourmethod can maintain accuracy with $\sim1\%$ accuracy drop compared to non-private models on MNIST dataset under privacy budget $\epsilon=2$. 
For CIFAR10 task, we can achieve the accuracy as high as 69.51\% for privacy budget $(3, 10^{-5})$. As a comparison, the state-of-the-art DP training accuracy in~\cite{papernot2021tempered} only delivers 66.2\% accuracy even at a much higher privacy budget--$(7.53, 10^{-5})$. 
We match this accuracy with $\epsilon=2.43$, which is an improvement in the DP-guarantee of $e^{5.1}\approx 164$.
% Furthermore, ResNet-18 transfer learning result leads to merely a 1.45\% accuracy drop compared to the non-private model while DP-SGD training suffers from a 7.01\% accuracy drop. We can achieve 75.32\% accuracy, which is very close to non-private version, with a slightly increased privacy budget $\epsilon=2.15$ (Table~\ref{tab:transfer_multi_layer}). 


According to Figure~\ref{fig:privacy_accuracy}, we observe that both \ourmethod and DP-SGD exhibit a similar trend, i.e., relaxing privacy constraint increases the accuracy. But \ourmethod always outperforms DP-SGD in all cases.
In most cases, \ourmethod achieves much more accuracy improvement compared to DP-SGD under strict privacy budget constraints. 
% In particular, in Figure ~\ref{subfig:model-1} and Figure ~\ref{subfig:resnet18}, when $\epsilon=1$, our approach has resulted in 18.75\% and 25.92\% accuracy improvements. Overall, \ourmethod leads to $13.48\% \sim 34.85\%$ max accuracy gain and on average $4.49\% \sim 19.92\%$ accuracy gain among all the privacy budget cases compared to DP-SGD. 
In particular, in Figure ~\ref{subfig:model-1}, when $\epsilon=1$, our approach has resulted in 18.75\% accuracy improvements. Overall, \ourmethod leads to $13.48\% \sim 34.85\%$ max accuracy gain and on average $4.49\% \sim 19.92\%$ accuracy gain among all the privacy budget cases compared to DP-SGD. 
% Overall, we show the effectiveness of the proposed \ourmethod on different models.

% , next we will discuss  the performance of each component of our approach.
% Specifically, we evaluate the \ourmethod in Section~\ref{subsec:effectiveness_spectral_dp} on models with only convolutional layers and \ourmethodblock on models consisting of FC layers in Section~\ref{subsec:block_spectral_dp}. 
% Finally, we will discuss the effectiveness of \ourmethod in transfer learning in Section~\ref{subsec:transfer}.


\vspace{-5pt}
% \subsubsection{Effectiveness of \ourmethod}
\subsubsection{Filtering ratio choice of \ourmethod for convolutional layer}
\label{subsec:effectiveness_spectral_dp}
% \wen{There are no insightful, deep and interesting discussions on the experimental data at all, just showed the number, too shallow. At least, you should analytically discuss the underlying reasons 1/2/3 etc, and link it to theory if possible.}
% The Fourier DP perturbation with CONV layer is evaluated on CIFAR10 dataset. We build a network that consists of 5 convolutional layers. Based on the complexity of the CIFAR10 image, We first implement DP-SGD and Fourier DP with a moderate differential privacy budget ($\epsilon=7.0,\delta=10^{-5}$). For DP-SGD, we perform grid-search on hyper-parameters and report the best testing accuracy.  


% We extend two more privacy budgets for the model. And the results are shown in Table \ref{tab:conv-1-result}. The results show that the Fourier DP outperforms DP-SGD with around 10\% accuracy improvement. 
% \begin{table}[h]
% \centering
% \caption{Results of comparison between DP-SGD and \ourmethod training on CIFAR10 dataset.}
% \label{tab:conv-1-result}
% \begin{tabular}{c|ccc}
% \hline
% \multirow{3}{*}{Method} & \multicolumn{3}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Testing accuracy (\%) with\\Privacy Budget ($\epsilon,10^{-5}$)\end{tabular}}} \\ 
%         & \multicolumn{3}{c}{} \\ \cline{2-4}
%     & 3.0 & 5.0 & 7.0 \\ \hline
% DP-SGD     & 53.79\% & 55.4\% & 56.97\%  \\
% \ourmethod & 60.63\% & 65.45\% & 66.56\% \\ \hline
% \end{tabular}
% \end{table}


Without loss of generality, we use \texttt{Model2} as an example to evaluate the effectiveness of \ourmethod with different filtering ratio $\rho$. 
We extend the tight privacy budget $(3, 10^{-5})$ evaluated in the previous section with larger $\epsilon$ values (5 and 7) and different filtering ratios on convolutional layers to explore utility improvements in more settings as shown in Table~\ref{tab:conv1-pruning}.
% Figure~\ref{subfig:conv1-pruning}. 
% We also perform DP-SGD training with grid-search on hyper-parameters and report the best testing accuracy in Figure~\ref{fig:conv1-pruning}.  
% According to Figure~\ref{fig:conv1-pruning}, DP-SGD has poor test accuracy even with much larger privacy budgets, as it provides 56.97\% at $\epsilon=7$, which is lower than the worst result with \ourmethod at $\epsilon=3$ (58.38\%). \ourmethod can provide around 10\% improvement in accuracy compared to DP-SGD for the same privacy budget.
It is intuitive that increasing the filtering ratio decreases the dimension of the additive differentially private noise, but causes more information loss of the weights. We note that there is a tradeoff between the noisy error and the reconstruction error (information loss) which is controlled by the filtering ratio. %We find that for the convolutional layers' filtering ratio, removing a certain amount of coefficients and leaving a sufficient quantity of coefficients with DP noised added can help maintain a better utility. 
% \textcolor{red}{
As Table~\ref{tab:conv1-pruning} show, there is a significant accuracy loss for large filtering ratios ($\rho=0.875$) in all cases. 
% }
The models achieve the highest accuracy at $\rho=0.5$ with $\epsilon=5$ and $7$.
With a tight privacy budget of $\epsilon=3$, adding noise causes more prominent utility loss, and filtering more coefficients with $\rho=0.75$ provides better accuracy. 
\textbf{In general, $\rho=0.5$ can be a good starting point to achieve the best utility for convolutional layers.}

% models achieve similar accuracy at $\epsilon=3$ with 0.75 and 0.5 filtering ratios. setting the filtering ratio to 0.5 leads to the best results at larger $\epsilon$ constraints.
% \textcolor{red}{reasons 1/2/3 etc, and link it to theory if possible}.


% \begin{table}[t]
% \caption{Test accuracy on CIFAR10 dataset with different pruning ratio for \ourmethod training on \texttt{Model2} models under different privacy budgets.}
% \label{tab:conv1-pruning}
% \centering
% \begin{tabular}{c|c|ccc}
% \hline
% \multirow{2}{*}{Method} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Pruning\\ Ratio\end{tabular}} &
%   \multicolumn{3}{c}{Privacy Budget $(\epsilon, 10^{-5})$} \\ \cline{3-5} 
%       &      & 3       & 5       & 7       \\ \hline
% \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Spectral\\ DP\end{tabular}} &
%   0.75 &
%   60.63\% &
%   62.20\% &
%   62.28\% \\
%       & 0.5  & 60.26\% & 65.45\% & 66.56\% \\
%       & 0.25 & 58.38\% & 63.18\% & 65.58\% \\ \hline
% DP-SGD & 0    & 53.79\% & 55.40\% & 56.97\% \\ \hline
% \end{tabular}
% \end{table}

% \begin{table}[t]
% \caption{Test accuracy on CIFAR10 dataset with different pruning ratio for \ourmethod training on \texttt{Model2} models under different privacy budgets.}
% \label{tab:conv1-pruning}
% \centering
% \begin{tabular}{c|ccc}
% \hline
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Filtering\\ ratio\end{tabular}} & \multicolumn{3}{c}{Privacy Budget $(\epsilon, 10^{-5})$} \\ \cline{2-4} 
%      & 3       & 5       & 7       \\ \hline
% 0.75 & \textbf{60.63\%} & 62.20\% & 62.28\% \\
% 0.5  & 60.26\% & \textbf{65.45\%} & \textbf{66.56\%} \\
% 0.25 & 58.38\% & 63.18\% & 65.58\% \\
% 0    & 58.34\% & 63.74\% & 65.65\% \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[t]
\caption{Test accuracy on CIFAR10 dataset with different filtering ratios for \ourmethod training on \texttt{Model2} models under different privacy budgets.}
\label{tab:conv1-pruning}
\centering
\begin{tabular}{c|ccccc}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Privacy\\ Budget\end{tabular}} & \multicolumn{4}{c}{Filtering ratio ($\rho$)} \\ \cline{2-6}
                            & 0       & 0.25    & 0.5              & 0.75    & 0.875   \\ \hline
$(3, 10^{-5})$ & 58.34\% & 58.38\% & 60.26\%          & \textbf{60.63\%} & 55.56\% \\
$(5, 10^{-5})$ & 63.74\% & 63.18\% & \textbf{65.45\%} & 62.20\%  & 55.79\% \\
$(7, 10^{-5})$ & 65.65\% & 65.58\% & \textbf{66.56\%} & 62.28\%   & 58.93\% \\ 
% Privacy Budget  & \multicolumn{4}{c}{Filtering ratio} \\ \cline{2-5}
% $(\epsilon, 10^{-5})$ & 0.75             & 0.5              & 0.25    & 0       \\ \hline
% 3 & \textbf{60.63\%} & 60.26\%          & 58.38\% & 58.34\% \\
% 5 & 62.20\%          & \textbf{65.45\%} & 63.18\% & 63.74\% \\
% 7 & 62.28\%          & \textbf{66.56\%} & 65.58\% & 65.65\% \\ 
\hline
\end{tabular}
% \vspace{-10pt}
\end{table}

% % Figure environment removed

% % Figure environment removed

\vspace{-10pt}
% \subsubsection{Effectiveness of \ourmethodblock}
\subsubsection{Filtering ratio and block size choice of \ourmethodblock for FC layer}
\label{subsec:block_spectral_dp}


\begin{table}[t]
\centering
\caption{Test accuracy of DP-SGD and \ourmethodblock on MNIST dataset with privacy budget $(2, 10^{-5})$.}
\label{tab:model-1-result}
\begin{tabular}{c|c|cc}
\hline
\multirow{2}{*}{Methods} & \multirow{2}{*}{ \texttt{Model1}} & \multicolumn{2}{c}{Circulant \texttt{Model1}} \\ \cline{3-4} 
                  &                          & BS =8     & BS =16    \\ \hline
Non-Private       & 98.48\%                  & 97.87\%           & 97.38\%           \\
DP-SGD             & 93.55\%                  & 94.77\%           & 95.54\%           \\
\ourmethod        & N/A          & 96.96\%           & 96.85\%           \\ \hline
\end{tabular}
% \vspace{-5pt}
\end{table}

% We first perform the evaluation on a neural network that only consists of fully connected layers. The architecture of the network is shown in Table \ref{tab:model-1}. We compare differentially private circulant \texttt{Model1} and Fourier DP perturbed circulant \texttt{Model1} on MNIST dataset. We run DP-SGD and Fourier DP perturbation on \texttt{Model1} and circulant \texttt{Model1} with a fixed differential privacy budget of $(\epsilon=2,\delta=10^{-5})$. 
% Note that in our proposed \ourmethodblock, we need to apply block circulant matrix on FC layers first, the detailed structure of Circulant \texttt{Model1} is shown in Table~\ref{tab:model-1}. 
We apply DP-SGD and \ourmethod on \texttt{Model1} under a fixed privacy budget $(2, 10^{-5})$ with various hyper-parameters such as batch size, learning rate, and clipping bound. We report the best test accuracy over all running cases in Table \ref{tab:model-1-result}. 
% As we need to apply  on FC layers with our \ourmethod, we also train the circulant model with DP-SGD using the same block sizes. 
We apply block circulant matrices to  DP-SGD using the same block sizes as \ourmethod.
% Adopting block circulant in FC layers can compress the model and lead to less trainable parameters, as a result, the model becomes simpler in DP training. 
We observe an accuracy improvement for DP-SGD trained circulant \texttt{Model1}. 
By using \ourmethodblock for training, we further take advantage of spectral domain based noise reduction and spectral filtering and achieve much better utility than DP-SGD.

% In this section, we perform the evaluation on \texttt{Model1} which consists of fully connected layers to show the effectiveness of our proposed \ourmethodblock under different block sizes and filtering ratios.

% \begin{table}[!t]
% \centering
% \caption{Architecture of model that only consists of Fully Connected (FC) layers. A layer name ending with BC means that the weights matrix of such layer is block circulant.}
% \label{tab:model-1}
% \begin{tabular}{cc|ccc}
% \hline
% \multicolumn{2}{c|}{Model-1}  &   \multicolumn{3}{c}{Circulant \texttt{Model1}}      
% % \\& & \multicolumn{3}{c}{\textbf{Model-1}}
% \\\hline
% % \multirow{2}{*}{\textbf{Layer}} & \multirow{2}{*}{\textbf{Weight}} & \multirow{2}{*}{\textbf{Layer}} & \multirow{2}{*}{\textbf{Weight}} & \multirow{2}{*}{\textbf{Block}}     
% Layer & Weight & Layer & Weight & Block\\ \hline
% % &&&&\\\hline
% FC1            & $784\times2048$  & FC1-BC  & $784\times2048$  & 8/16\\
% FC2            & $2048\times1024$ & FC2-BC  & $2048\times1024$ & 8/16\\
% FC3            & $1024\times160$  & FC3-BC  & $1024\times160$  & 8/16\\
% FC4            & $160\times10$    & FC4-BC  & $160\times10$    & 10\\ \hline
% \end{tabular}
% \end{table}


% The result demonstrates that the Fourier DP perturbation outperforms DP-SGD. It is notice that Fourier DP perturbation works on the layers with block circulant weights. 

% \subsubsection{Impact of pruning ratio}
% \noindent \textbf{Ablation study.}
We further explore the impact of block size ($BS$) and filtering ratio ($\rho$) using \texttt{Model1}. 
%We
% conduct our experiments with use 
Two different block sizes (8 and 16) under four target differential privacy budgets are evaluated. 
The results under different filtering ratios are shown in Table~\ref{tab:model-1-result-pf}.
% Figure~\ref{fig:model-1-result-pf}. 
% First we compare the model accuracy between two block sizes.
Overall, models with $BS = 8$ achieve better utility. The average model accuracy at four target $\epsilon$ across all five filtering ratios, is 94.02\%, 95.78\%, 96.33\%, and	96.57\%, respectively, which is higher than that with $BS = 16$ (92.90\%, 95.29\%, 96.03\%, 96.17\%). This trend is consistent with that of non-private \texttt{Model1}, which is 97.89\% with $BS = 8$ and 97.38\% with $BS = 16$ (in Table~\ref{tab:model-1-result}). 
% We conclude that 
It indicates that the FC layer often has redundancy and the block circulant matrix can help us further reduce the model size,  
% , reduce parameters, 
and compressing model weights benefit the spectral calculation of \ourmethodblock. \textbf{Generally, we can adopt a large $BS$ (a power 2 number such as 16) for complex models often containing more redundancy in FC layers, and a small $BS$ (i.e. 8) for small models.}
% Both block sizes give us great performance, but choosing a large block size could lead to some utility tradeoff. 


%We then study the impact of filtering ratios. 
For filtering ratio ($\rho$), as shown in Table~\ref{tab:model-1-result-pf}, a larger $\rho$ on FC layers leads to better utility in most cases. 
% The scenario is slightly different from the CONV layers because FC layer has a large number of parameters and thus has more redundancy. 
By filtering more frequency coefficients, adding DP noise becomes more smoothly, and the loss due to the large filtering ratio can be compensated. As a result, FC layer with a larger $\rho$ often benefits the utility in \ourmethodblock training. Therefore, \textbf{FC layers in general can have a larger filter ratio than that of convolutional layers ($\rho=0.75$ vs. $\rho=0.5$).} 



% The error of Fourier DP comes from the noise and the pruning. To evaluate the impact on accuracy, we implement the Fourier DP with different pruning ratio. We conduct our experiments with four fixed differential privacy budget, and report the results in Table \ref{tab:model-1-result-pf}.
% \begin{table}[h]
% \centering
% \caption{Test accuracy on MNIST dataset with different filtering ratios and block sizes for \ourmethodblock training on \texttt{Model1} models under different privacy budgets.}
% \label{tab:model-1-result-pf}
% \begin{tabular}{c|c|cccc}
% \hline
% % \multicolumn{2}{c|}{Parameters} & \multicolumn{4}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Testing accuracy with\\
% % Privacy Budget ($\epsilon,10^{-5}$)\end{tabular}}} \\ \cline{1-2}
% Block                & Filtering  & \multicolumn{4}{c}{Privacy Budget ($\epsilon,10^{-5}$)}                                                                                               \\ \cline{3-6}
% Size                 & Ratio   & 0.5                         & 1.0                        & 1.5                        & 2.0                        \\ \hline
% % \multirow{3}{*}{8}   & 1/4     & 94.30 & 95.60 & 96.29 & 96.57 \\
% %                      & 1/2     & 93.05 & 94.95 & 95.70 & 96.30 \\
% %                      & 3/4     & 92.45 & 94.59 & 95.42 & 95.65 \\ \hline
% % \multirow{4}{*}{16}  & 1/4     & 93.79 & 96.10 & 96.27 & 96.28\\
% %                     %  & 3/8     & 94.55 & 95.93 & 95.82 & 96.22 \\ 
% %                      & 1/2 & 84.42 & 92.58 & 94.22 & 94.73 \\
% %                      & 3/4 & 86.95 & 92.34 & 93.74 & 94.67 \\\hline
% \multirow{4}{*}{8}  & 0.75 & 94.98\% & 96.19\% & 96.60\% & 96.96\% \\
%                     & 0.5  & 93.64\% & 95.57\% & 96.35\% & 96.59\% \\
%                     & 0.25 & 93.69\% & 95.77\% & 96.24\% & 96.58\% \\
%                     & 0    & xx\% & xx\% & 9x\% & 9x\% \\\hline
% \multirow{4}{*}{16} & 0.75 & 93.30\% & 95.78\% & 96.21\% & 96.85\% \\
%                     & 0.5  & 93.32\% & 95.73\% & 96.46\% & 96.41\% \\
%                     & 0.25 & 92.92\% & 94.08\% & 95.76\% & 95.81\% \\ 
%                     & 0    & 9x\% & 9xx\% & 9x\% & 9x\% \\\hline
% \end{tabular}
% \end{table}



% \begin{table}[h]
% \centering
% \caption{Results of impact of choosing pruning ratio.}
% \label{tab:model-1-result-pf}
% \begin{tabular}{cc|cccc|cccc}
% \hline
% \multicolumn{2}{c|}{Block size}      & \multicolumn{4}{c|}{8}           & \multicolumn{4}{c}{16}           \\ \hline
% \multicolumn{2}{c|}{Filtering ratio} & 0.75    & 0.5     & 0.25    & 0  & 0.75    & 0.5     & 0.25    & 0  \\ \hline
% \multicolumn{1}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Privacy\\ Budget\\ (xx,xx)\end{tabular}}} &
%   0.5 &
%   94.98\% &
%   93.64\% &
%   93.69\% &
%   00 &
%   93.30\% &
%   93.32\% &
%   92.92\% &
%   00 \\
% \multicolumn{1}{c|}{}      & 1       & 96.19\% & 95.57\% & 95.77\% & 00 & 95.78\% & 95.73\% & 94.08\% & 00 \\
% \multicolumn{1}{c|}{}      & 1.5     & 96.60\% & 96.35\% & 96.24\% & 00 & 96.21\% & 96.46\% & 95.76\% & 00 \\
% \multicolumn{1}{c|}{}      & 2       & 96.96\% & 96.59\% & 96.58\% & 00 & 96.85\% & 96.41\% & 95.81\% & 00 \\ \hline
% \end{tabular}
% \end{table}

% % Figure environment removed

% The results show that a pruning ratio that is too large or too small will lead to lower accuracy. \wen{Cannot see why 1/2 is an optimal choice based on Table 8}

% \subsection{\ourmethod for conventional models}
% Here we show the results of LeNet-5 on MNIST and \texttt{Model3} model on CIFAR10 to demonstrate the \ourmethod can be implemented on conventional neural network models that contain both CONV and FC layers.




\begin{table}[t]
\centering
\caption{Test accuracy on MNIST dataset with different filtering ratios and block sizes for \ourmethodblock training on \texttt{Model1} under different privacy budgets.}
\label{tab:model-1-result-pf}
\resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|cccc}
% \hline
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Block\\ size\end{tabular}} & Privacy Budget & \multicolumn{4}{c}{Filtering ratio} \\ \cline{3-6} 
%                     & ($\epsilon,10^{-5}$) & 0.75    & 0.5     & 0.25    & 0 \\ \hline
% \multirow{4}{*}{8}  & 0.5    & 94.98\% & 93.64\% & 93.69\% & x \\
%                     & 1      & 96.19\% & 95.57\% & 95.77\% & x \\
%                     & 1.5    & 96.60\% & 96.35\% & 96.24\% & x \\
%                     & 2      & 96.96\% & 96.59\% & 96.58\% & x \\ \hline
% \multirow{4}{*}{16} & 0.5    & 93.30\% & 93.32\% & 92.92\% & x \\
%                     & 1      & 95.78\% & 95.73\% & 94.08\% & x \\
%                     & 1.5    & 96.21\% & 96.46\% & 95.76\% & x \\
%                     & 2      & 96.85\% & 96.41\% & 95.81\% & x \\ \hline
% \end{tabular}
\begin{tabular}{c|c|ccccc}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Block\\ size\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Privacy\\ Budget\end{tabular}} &
  \multicolumn{5}{c}{Filtering ratio ($\rho$)} \\ \cline{3-7} 
                    &     & 0       & 0.25    & 0.5              & 0.75 & 0.875 \\ \hline
\multirow{4}{*}{8}  & $(0.5, 10^{-5})$ & 93.08\% & 93.69\% & 93.64\% & \textbf{94.98\%} & 94.71\%\\
                    & $(1.0, 10^{-5})$   & 95.07\% & 95.77\% & 95.57\% & 96.19\% & \textbf{96.29\%} \\
                    & $(1.5, 10^{-5})$ & 95.93\% & 96.24\% & 96.35\% & \textbf{96.60\%}  & 96.52\% \\
                    & $(2.0, 10^{-5})$   & 95.83\% & 96.58\% & 96.59\% & \textbf{96.96\%}  & 96.89\%\\ \hline
\multirow{4}{*}{16} & $(0.5, 10^{-5})$ &  91.57\% & 92.92\% & 93.32\% & 93.30\%  & \textbf{93.41\%} \\
                    & $(1.0, 10^{-5})$   & 95.04\% & 94.08\% & 95.73\% & {95.78\%}  & \textbf{95.80\% }\\
                    & $(1.5, 10^{-5})$ & 95.55\% & 95.76\% & \textbf{96.46\%} & 96.21\% & 96.15\% \\
                    & $(2.0, 10^{-5})$   & 96.02\% & 95.81\% & 96.41\% & \textbf{96.85\%} & 95.74\% \\ \hline
\end{tabular}
}
% \vspace{-5pt}
\end{table}

% \subsubsection{\ourmethod for transfer learning models}
\vspace{-5pt}
\subsection{\ourmethod in Transfer Learning}
\vspace{-5pt}
\label{subsec:transfer}
\subsubsection{ResNet-18}
In this section, we evaluate the performance of the proposed \ourmethod training on the transfer learning setting. Specifically, we select three transfer training models with different numbers of trainable layers from the bottom of the pretrained ResNet-18. We set the noise scale $\sigma=0.9$ and train the models for 100 epochs, the best accuracy epoch and corresponding privacy budget are reported in Table~\ref{tab:transfer_multi_layer}. 
The 1FC layer model (\texttt{Transfer1}) follows previous work ~\cite{yu2019differentially, dpsgd}--retraining only a hidden layer with 1000 units and a softmax layer with differential privacy.
The \texttt{Transfer2} consists of the last 2 CONV layers and 2 FC layers of the model to be trained. \texttt{Transfer3} further increases the trainable layers to the last 4 CONV layers and 2 FC layers. The baseline accuracy of non-private model increases from 62.31\% to 75.94\% as we increase the number of trainable layers on transfer learning models. 

\ourmethod can benefit more from the increasing number of trainable layers. When increasing to 6 trainable layers in \texttt{Transfer3}, \ourmethod achieves model accuracy close to the non-private model (75.32\% vs 75.94\%) at a small privacy budget ($\epsilon$=2.15). In contrast, the gain of DP-SGD is limited. It suffers more accuracy degradation ($6.61\%$) even with lower privacy guarantee ($\epsilon$=2.89). This clearly indicates that our \ourmethod works much better than DP-SGD when protecting more layers' weights for better privacy is needed. 
\textbf{This further highlights the key advantage of our \ourmethod--better preserving model utility than DP-SGD especially for training models from the scratch with a high-level privacy requirement}, as validated in Section~\ref{sccratch}. Since we obtain the best accuracy in \texttt{Transfer3}, we conduct the following experiments on this model.

% \begin{table}[]
% \caption{Transfer learning results for non-private model, DP-SGD training and \ourmethod training with different number of training layers }
% \label{tab:transfer_multi_layer}
% \centering
% % \small
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|cc|cc|cc}
% \hline
% Model & \multicolumn{2}{c|}{\texttt{Transfer1}}               & \multicolumn{2}{c|}{\texttt{Transfer2}}        & \multicolumn{2}{c}{\texttt{Transfer3} }       \\ \hline
% Trainable layers & \multicolumn{2}{c|}{1FC layer}               & \multicolumn{2}{c|}{2CONV+2FC layers}        & \multicolumn{2}{c}{4CONV+2FC layers}        \\ \hline
%                & \multicolumn{1}{c|}{Privacy Budget} & Test acc & \multicolumn{1}{c|}{Privacy Budget} & Test acc & \multicolumn{1}{c|}{Privacy Budget} & Test acc \\ \hline
% Non-private    & \multicolumn{1}{c|}{$\infty$}       & 62.31\%       & \multicolumn{1}{c|}{$\infty$}       & 70.08\%       & \multicolumn{1}{c|}{$\infty$}       & 75.94\%       \\ \hline
% DP-SGD          & \multicolumn{1}{c|}{$(3.88, 10^{-5})$}    & 60.10\%       & \multicolumn{1}{c|}{$(3.24, 10^{-5})$}    & 66.47\%       & \multicolumn{1}{c|}{$(2.89, 10^{-5})$}    & 69.33\%       \\ \hline
% \textbf{\ourmethod} &
%   \multicolumn{1}{c|}{\textbf{$(2.11, 10^{-5})$}} &
%   \textbf{59.11\%} &
%   \multicolumn{1}{c|}{\textbf{$(2.11, 10^{-5})$}} &
%   \textbf{70.75\%} &
%   \multicolumn{1}{c|}{\textbf{$(2.15, 10^{-5})$}} &
%   \textbf{75.32\%} \\ \hline
% \end{tabular}
% }
% \end{table}

\begin{table}[t]
\caption{Transfer learning results for non-private model, DP-SGD training and \ourmethod training with different number of trainable layers. }
\label{tab:transfer_multi_layer}
\centering
% \small
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cc|cc}
\hline
Model & \multicolumn{2}{c|}{\texttt{Transfer1}}               & \multicolumn{2}{c|}{\texttt{Transfer2}}        & \multicolumn{2}{c}{\texttt{Transfer3} }       \\ \hline
% Trainable layers & \multicolumn{2}{c|}{1FC layer}               & \multicolumn{2}{c|}{2CONV+2FC layers}        & \multicolumn{2}{c}{4CONV+2FC layers}        \\ \hline
               & \multicolumn{1}{c|}{$\epsilon$} & Test acc & \multicolumn{1}{c|}{$\epsilon$} & Test acc & \multicolumn{1}{c|}{$\epsilon$} & Test acc \\ \hline
Non-private    & \multicolumn{1}{c|}{$\infty$}       & 62.31\%       & \multicolumn{1}{c|}{$\infty$}       & 70.08\%       & \multicolumn{1}{c|}{$\infty$}       & 75.94\%       \\ 
DP-SGD          & \multicolumn{1}{c|}{3.88}    & 60.10\%       & \multicolumn{1}{c|}{3.24}    & 66.47\%       & \multicolumn{1}{c|}{2.89}    & 69.33\%       \\ 
\textbf{\ourmethod} &
  \multicolumn{1}{c|}{\textbf{2.11}} &
  \textbf{59.11\%} &
  \multicolumn{1}{c|}{\textbf{2.11}} &
  \textbf{70.75\%} &
  \multicolumn{1}{c|}{\textbf{2.15}} &
  \textbf{75.32\%} \\ \hline
\end{tabular}
}
\vspace{-5pt}
\end{table}

\ourmethod can always provide a better tradeoff between utility and privacy than DP-SGD in differential private transfer learning. Figure~\ref{fig:resnet18} shows the results of the \texttt{Transfer3} transfer training with different target privacy budgets. \ourmethod always provides higher accuracy than DP-SGD, from the case with strict privacy constraint (small $\epsilon$) to cases with relaxed privacy requirements. Under an extreme privacy constrain (e.g. $\epsilon$=0.5), our method only causes a 4.41\% accuracy drop compared to the non-privacy model (71.52\% vs 75.94\%), while DP-SGD only achieves 63.60\%, yielding a 12.33\% accuracy loss.


% Figure environment removed

% We further explore the impact of hyperparameters during transfer learning. We summarise the results with different pairs of training epochs, filtering ratio ($\rho$) and target privacy budget in Table~\ref{tab:trans}. First, we discuss the impact of training epochs for a target privacy budget training. We set a strict target privacy budget $\epsilon=0.5$ and conduct 10, 20, and 40 epochs of training. More training epochs result in adding less noise to each training step but lead to an increased number of steps. Different choices of training epochs may affect the convergence and final accuracy of the model. 
% The results in Table~\ref{tab:trans} show that fewer training epochs lead to higher accuracy when $\epsilon=0.5$, but when the privacy budget is relaxed to $\epsilon=2$, the test accuracy is similar for 10 and 20 epochs.

% We thus conduct experiments with 10 training epochs and set $\rho$ to 0.2, 0.5 and 0.75 under target privacy budget ($0.5, 10^{-5}$). Under this tight budget, setting $\rho$ to 0.2 and 0.5 achieve similar model performance. However, unlike the cases in Section~\ref{subsec:effectiveness_spectral_dp} for the training from scratch models, in transfer learning, setting a smaller rate like 0.2 gives better performance. As the transfer learning process tunes the final layers of a pre-trained model, large pruning rates lose too much information and lead to worse accuracy.
% % To better explore the effectiveness of other parameters, we conduct experiments with 10 training epochs in the following experiments.

% Finally, we draw training curves using 10 training epochs with  $\rho=0.2$ under different privacy budgets in Figure~\ref{fig:resnet18-dptrain}. It demonstrates that our \ourmethod allows the model to converge quickly in the first several epochs. When $\epsilon>= 0.5$, the training curves are similar and show the tradeoff between privacy budget and accuracy. The utility of the model is limited only when the privacy budget is very low ($\epsilon=0.2$). The training after the 5th epoch does not improve the accuracy. Nevertheless, this result (68.52\%) is still higher than the DP-SGD trained result (66.72\%) with $\epsilon=1$.

% % We plot the test accuracy during the training with different $\epsilon$ in Figure~\ref{fig:resnet18-dptrain}, which can demonstrate our \ourmethod allows the model to converge quickly in the first several epochs. When privacy budget is very small ($\epsilon=0.2$), the model utility is bounded and training after 5th epoch is not able to increase the accuracy.

% % Figure~\ref{fig:resnet18-prune} shows the test accuracy for \ourmethod training and DP-SGD training  with pruning ratio 0.2, 0.5 and 0.75 under target privacy budget $(\epsilon=0.5, 10^{-5})$. The performance is consistent with the results in  Section~\ref{subsec:effectiveness_spectral_dp}, i.e., large pruning rates lose too much information and lead to worse accuracy. Nevertheless, the accuracies in all cases are much higher than the DP-SGD training.

% \begin{table*}[]
% \caption{Different settings of training epochs, filtering ratios and privacy budgets on CIFAR10 dataset for ResNet-18 transfer learning with 4CONV+2FC trainable layers}
% \centering
% % \small
% % \resizebox{\linewidth}{!}{
% % \begin{tabular}{c|cccccccccc}
% % \hline
% % Training Epochs        & 10   & 10  & 10  & 20  & 40  & 10  & 10  & 10  & 20  & 21   \\
% % Pruning Ratio          & 0.75 & 0.5 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2  \\
% % Privacy Budget $(\epsilon,10^{-5})$ & 0.5  & 0.5 & 0.5 & 0.5 & 0.5 & 0.2 & 1   & 2   & 2   & 2.15 \\\hline
% % Train Accuracy & 76.36\% & 77.50\% & 77.68\% & 76.83\% & 74.39\% & 71.44\% & 81.11\% & 83.82\% & 86.55\% & 88.12\% \\
% % Test Accuracy  & 71.50\% & 72.61\% & 72.89\% & 71.98\% & 70.31\% & 68.52\% & 73.72\% & 74.53\% & 74.49\% & 75.32\% \\ \hline
% % \end{tabular}
% \begin{tabular}{c|ccccccccc}
% \hline
% Training Epochs        & 10      & 10      & 10      & 20      & 40      & 10      & 10      & 10  & 20\\
% Filtering Ratio ($\rho$)          & 0.75    & 0.5     & 0.2     & 0.2     & 0.2     & 0.2     & 0.2     & 0.2  & 0.2  \\
% Privacy Budget $(\epsilon,10^{-5})$ & 0.5     & 0.5     & 0.5     & 0.5     & 0.5     & 0.2     & 1       & 2 & 2 \\\hline
% Train Accuracy & 76.36\% & 77.50\% & 77.68\% & 76.83\% & 74.39\% & 71.44\% & 81.11\% & 83.82\% & 86.55\% \\
% Test Accuracy  & 71.50\% & 72.61\% & 72.89\% & 71.98\% & 70.31\% & 68.52\% & 73.72\% & 74.53\% & 74.49\% \\ \hline
% \end{tabular}
% % }
% \label{tab:trans}
% \end{table*}

% % Figure environment removed

% % % Figure environment removed


% \vspace{-15pt}
\subsubsection{ResNeXt-29}
\label{subsec:transfer_resnext}
% We further conduct experiments to show the effect of replacing DP-SGD with \ourmethod in transfer learning. 
In addition to the layer-wise exploration on ResNet-18, we also adapt our \ourmethod  for comparison according to the state-of-the-art transfer learning setup.
We consider the setting proposed in \cite{tramer2021differentially} for transfer learning from CIFAR100 to CIFAR10. An FC layer-based model is trained on features that are extracted from a ResNeXt\cite{xie2017aggregated} model trained on CIFAR100. DP-SGD and \ourmethod are implemented on the FC layer-based model. Our results are reported in Table \ref{tab:apdx-transfer}. We also compare our results with a DP-SGD utility improvement method \cite{luo2021scalable} that uses transfer learning. Overall, \ourmethod outperforms the DP-SGD trained models in both \cite{tramer2021differentially} and \cite{luo2021scalable} across different privacy budgets $\epsilon$.


% \begin{table}[t!]
% \centering
% \caption{\textcolor{red}{Testing accuracy with different privacy budgets in transfer learning.} }
% \label{tab:apdx-transfer}
% \begin{tabular}{c|ccc}
% \hline
% ($\epsilon,10^{-5}$) & DP-SGD in \cite{tramer2021differentially} & Block Spectral-DP & \cite{luo2021scalable}  \\ \hline
% $\epsilon=0.5$            & -      & \textbf{80.29\%}              & 73.28\%  \\
% $\epsilon=1.0$          & -      & \textbf{80.81\%}              & 76.64\%  \\
% $\epsilon=1.5$           & -      & \textbf{81.71\%}             & 81.57\%  \\
% $\epsilon=2.0$         & 80.00\%   & -                 & -     \\
% $\epsilon=\infty$             & 84.00\%    & 84.00\%               & 94.10\%  \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[t]
\caption{ResNeXt-29 for transfer learning on CIFAR10}
\label{tab:apdx-transfer}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccc}
\hline
$\epsilon$                                                    & 0.5     & 1.0     & 1.5     & 2.0    &  $\infty$ \\ \hline
DP-SGD in \cite{tramer2021differentially} & -       & -       & -       & 80.00\% & 84.00\%               \\
\ourmethod                                  & \textbf{80.29\%}  & \textbf{80.81\%}  & \textbf{81.71\%} & -       & 84.00\%               \\
\cite{luo2021scalable}                    & 73.28\% & 76.64\% & 81.57\% & -       & 94.10\%               \\ \hline
\end{tabular}
}
\vspace{-5pt}
\end{table}


% \vspace{-15pt}
% \textcolor{red}{
\subsubsection{WRN-28-10}
\label{subsec:WRN_transfer}
We further evaluate \ourmethod using a pretrained WRN-28-10 model from the down-sampled ImageNet32 dataset to perform transfer training on CIFAR10 and the more complex CIFAR100 datasets. As presented in Table~\ref{tab:cifar10_WRN} and Table~\ref{tab:CIFAR100_WRN}, we retrain the classifier of the WRN model for 20 epochs with different privacy budgets and compare the results with several state-of-the-art works~\cite{de2022unlocking,yu2021do,tramer2021differentially}. The 1 FC layer setting retrains the last layer with 1000 units, while the 2 FC layers setup retrains the whole classifier layer.
Our results show a similar trend--the utility of DP training improves as the number of training layers increases. \ourmethod achieve 94.85\% and 77.52\% at $\epsilon=1$ in 2 FC layers setting for CIFAR10 and CIFAR100, which is higher than the retraining results (93.36\% and 75.99\%) under a relaxed budget $\epsilon=4$ in 1 FC layer training setting.
% }

% \textcolor{red}{
Our \ourmethod achieves higher accuracy with a strict privacy budget ($94.85\%$ with $\epsilon=1$) compared to other works with relaxed budgets ($94\%$ with $\epsilon=4$~\cite{de2022unlocking}, $94.80\%$ with $\epsilon=2$~\cite{yu2021do} and $92.70\%$ with $\epsilon=2$~\cite{tramer2021differentially}). Even when compared to the strongest setting in \cite{de2022unlocking} (fine-tuning all layers), \ourmethod can achieve similar accuracy with $\epsilon=1$ on CIFAR10 (94.8\%) while exhibiting much higher accuracy (77.52\% with $\epsilon=1$)  on the more complex 100-class dataset–CIFAR100 than that of \cite{de2022unlocking} (e.g. 74.7\% at a relaxed privacy budget $\epsilon=2$). 
% \textbf{These results clearly indicate that our \ourmethod method can achieve better utility on more complex datasets and large models.}
\textbf{These results demonstrate that \ourmethod can achieve better utility on more complex datasets and large models as well. }
% }

\begin{table}[t]
\caption{WRN-28-10 transfer learning on CIFAR10}
\label{tab:cifar10_WRN}
\centering
\begin{tabular}{c|lll}
\hline
$\epsilon$                   & \multicolumn{1}{c}{1}       & \multicolumn{1}{c}{2}       & \multicolumn{1}{c}{4} \\ \hline
\ourmethod (2 FC layers) & \textbf{94.85\%}           & \textbf{95.11\%  }                   & \textbf{95.33\% }              \\
\ourmethod (1 FC layer)  & 93.19\%                     & 93.24\%                     & 93.36\%               \\ \hline
DeepMind (2022)~\cite{de2022unlocking}                & 93.10\%                     & 93.60\%                     & 94.00\%               \\
GEP (2021)~\cite{yu2021do}        & 94.30\%          & 94.80\% & -       \\
Feature extraction (2021)~\cite{tramer2021differentially}   & -                & 92.70\% & -       \\ \hline
\end{tabular}
% \vspace{-5pt}
\end{table}


\begin{table}[t]
\caption{WRN-28-10 transfer learning on CIFAR100}
\label{tab:CIFAR100_WRN}
\centering
\begin{tabular}{c|c|ccc}
\hline
                             & $\epsilon$          & 1       & 2       & 4       \\ \hline
\multirow{2}{*}{Spectral-DP} & 2 FC layers       & \textbf{77.52\%} & \textbf{77.78\%} & 78.03\% \\
                             & 1 FC layer          & 74.42\% & 75.65\% & 75.99\% \\ \hline
\multirow{2}{*}{DeepMind~\cite{de2022unlocking} }    & Classifier layer & 70.30\% & 73.90\% & 76.10\% \\
                             & All layers       & 67.40\% & 74.70\% & \textbf{79.20\%} \\ \hline
\end{tabular}
\vspace{-5pt}
\end{table}



\vspace{-2pt}
\subsection{ Replace DP-SGD with \ourmethod in DP-SGD based Existing Works}
\vspace{-2pt}
% In previous sections, we compared our \ourmethod with DP-SGD directly and showed that our solution can provide a better tradeoff between privacy and utility. 
% There are 
% several 
% emerging 
% researches on differential private model training, which aim to improve the utility of privacy-preserving training not from the DP-SGD algorithm itself but from different perspectives.
% Since \ourmethod is proposed as an alternative algorithm to DP-SGD, techniques orthogonal to DP-SGD can be integrated into it as well. Therefore, we can easily replace DP-SGD with \ourmethod in the DP-SGD based existing frameworks, to achieve further utility improvement. 
% \textcolor{red}{We notice that the techniques proposed and evaluated in some state-of-the-art works~\cite{tramer2021differentially,de2022unlocking}, such as dedicated to DP-SGD in the time domain, such as ScatterNet feature extraction, augmentation multiplicity, and parameter averaging, are orthogonal to our Spectral-DP.  Integrating these techniques with our Spectral-DP could further improve the results. In this section, we demonstrate the scalability of our approach by specifically combining~\cite{tramer2021differentially} with our \ourmethod during training.}
Since \ourmethod is proposed as an alternative algorithm to DP-SGD, techniques orthogonal to DP-SGD can be integrated into it as well. Therefore, we can easily replace DP-SGD with \ourmethod in the DP-SGD based existing frameworks, to achieve further utility improvement.
Specifically, we combine a state-of-the-art work~\cite{tramer2021differentially} with our \ourmethod in the training to show the scalability of our approach. 
We adopt the same setting from \cite{tramer2021differentially} that uses the default Scattering Network (ScatterNet) of depth two with wavelets rotated along eight angles from \cite{oyallon2015deep} as a feature extractor to preprocess each data sample. We also apply the same data normalization from ~\cite{tramer2021differentially} on top of the ScatterNet features to obtain the best utility. 
With a target differential privacy budget of ($3, 10^{-5}$), we conduct a grid-search on hyperparameters and report the best results in Table~\ref{tab:best_res_scatter}.
Here ScatterLinear and ScatterCNN adopt similar architectures used in \cite{tramer2021differentially}.
We can observe that on MNIST, \ourmethodblock outperforms DP-SGD on ScatterLinear with accuracy close to that of the non-private model. For CIFAR10, we show the training results on ScatterCNN, and CNN represents the DP training results on \texttt{Model3} as a baseline. We find that with \ourmethod training, we obtain higher accuracy than that of DP-SGD on ScatterCNN. In addition, training with \ourmethod on ScatterNet improves the accuracy by 1.42\% compared to our CNN result, and the accuracy gap is less than 1\% compared with the non-private ScatterCNN result. We also show the test accuracy and privacy budget plot for DP-SGD and \ourmethod in Figure~\ref{fig:scatternet_cifar}. The results indicate that \ourmethod can always outperform DP-SGD and has more gains under tighter privacy budgets.

\begin{table}[]
\caption{Testing accuracy on ScatterNet based model with DP-SGD and proposed \ourmethod training with privacy budget ($3, 10^{-5}$).}
\label{tab:best_res_scatter}
\centering
\begin{tabular}{c|c|cc}
\hline
\multirow{2}{*}{Methods} & MNIST         & \multicolumn{2}{c}{CIFAR10} \\ \cline{2-4} 
                         & ScatterLinear & ScatterCNN     & CNN        \\ \hline
Non-private              & 99.10\%       & 71.68\%        & 81.22\%    \\
DP-SGD                    & 97.66\%       & 67.77\%        & 57.58\%    \\
\ourmethod             & 98.63\%       & 70.93\%        & 69.51\%    \\ \hline
\end{tabular}
\vspace{-5pt}
\end{table}


% Figure environment removed






\vspace{-3pt}
\subsection{Ablation Study}\label{subsec:ablation}
\vspace{-3pt}








% \subsubsection{Ablation study}\label{subsec:ablation}
% \subsubsection{Hyperparameters of \ourmethod}\label{subsec:ablation}
\subsubsection{Training from scratch setting}\label{subsec:ablation}
% 1. Batchsize accuracy
% 2. clipping norm vs accuracy
% 3. clipping norm with Batchsize: why best norm is 1 when batch size is 512. When the batch size increase, the clipping norm change to 0.1
% 4. noise scale, clip norm, batch size relationship.
% Take away: how to choose batch size and clipping norm

% Key parameters recommendation
% In Section \ref{subsec:exper_setup}, we present general guidelines for selecting hyper-parameters for \ourmethod-based differentially private learning. 
%In this section, 
We further analyze how different choices of clipping norm, batch size, and learning rate, impact model performance based on \texttt{Model3} and CIFAR10 dataset. The DP budget is set to be $(\epsilon=3.0,\delta=10^{-5})$, and the model is trained for 30 epochs.
% % Figure environment removed

% Figure environment removed



% % Figure environment removed


\textbf{Impact of clipping norm.} Figure \ref{subfig:ablation_BS_Norm} shows how clipping norm ($C$) impacts the differentially private learning using \ourmethod. We observe that the large clipping norm degrades the model utility.
% The reason for this is that 
Since the scale of DP noise is proportional to the clipping norm, increasing the norm constraint causes increased gradient noise.
% the noise of the gradient. 
However, too small clipping norms can lead to a large utility loss. This is because the gradient may turn out to be in the opposite direction of the true gradient if the clipping norm is set too low. 
This phenomenon is consistent with the description of the original DP-SGD\cite{dpsgd}. To ensure an efficient \ourmethod private learning, \textbf{we recommend choosing an appropriate clipping norm (beginning with $C=0.1$).} 

% \textit{Comments: I run a larger batch size with 4096 and I found that the performance of BS=4096 is worse than the case of BS=2048. This is consistent with the DP-SGD results. I haven't finished all cases in BS=4096. Once it finishes, I will delete this comment.}


\textbf{Impact of batch size.} 
We select 4 batch sizes ($B$) and show the accuracy with each $B$ in Figure~\ref{subfig:ablation_BS_Norm}. We observe that, unlike the non-private model training, $B$ has a relatively large impact on the test accuracy. Changing $B$ from 512 to 2048 leads to 8.17\% accuracy improvement. 
This is because a larger $B$ leads to fewer noise addition iterations. However, the noise scale at a single iteration is positively associated with $B$. When $B$ is too large, the noise has a relatively larger effect than the training iterations. 
% This is because $B$ controls the tradeoff between the training iteration and noise scale. It is intuitive that a smaller $B$ requires more training iterations, which often decreases the accuracy. However, for larger $B$, the relative impact of differentially private noise on accuracy is smaller.
%indicating better accuracy. 
Therefore, an appropriate $B$ is essential to balance the utility and the noise addition.

We also find that $B$ and the clipping norm jointly affect the accuracy. When $B=512$, the best accuracy is achieved with $C>0.1$. Meanwhile, when $B=4096$, the best accuracy is achieved by choosing $C<0.1$.  
For \ourmethod training with a fixed privacy budget, a larger batch size implies a larger noise size, while a smaller clipping norm can reduce the noise in the gradient. Therefore, a larger $B$ is compacted with a relatively smaller clipping norm.
As the batch size decreases, the noise scale decreases accordingly. In this case, it requires a relatively large clipping norm to contain as much gradient information as possible.
\textbf{We recommend starting \ourmethod private learning with a relatively large batch size ($B=2048$) and a relatively small clipping norm ($C=0.1$). 
}% For \ourmethod private learning with a fixed privacy budget, a larger batch size implies a larger noise size. Hence, a smaller clipping norm can reduce the noise in the gradient. When the batch size is not relatively large, the noise scale is sequentially reduced. It does not require a smaller clipping norm. On the contrary, it requires a relatively large clipping norm in order to contain as much information about the gradient as possible. We conclude that a larger $B$ is compacted with a relatively smaller clipping norm. 
% We recommend starting \ourmethod private learning with a relatively large batch size ($B=2048$) and a relatively smaller clipping norm ($0.1$). 
% \textit{Comments: Experiments related to different LR settings are still running. We need change the picture in Figure \ref{subfig:ablation_LR}. Once it finishes, I will delete this comment.}



\textbf{Impact of learning rate.} Based on the study of batch size and clipping norm results, we pick the setting of ($B = 2048$, $C=0.1$) and perform \ourmethod training with different learning rates (LR). Figure~\ref{subfig:ablation_LR} shows the plots of the accuracy trends over the training epochs. Too small LR (LR=0.005) can lead to slow convergence of the model and reduce the accuracy over the target training epoch. A large LR=0.1 may also undermine the accuracy significantly. Since a larger LR boosts the weight noise, leading to a random gradient direction that hurts the training convergence. \textbf{The test accuracy is stable when the learning rate is set within a range of $[0.01,0.025]$. Generally, we can set LR=0.01 for the \ourmethod training.}


% Figure environment removed


% Figure environment removed

% \vspace{-15pt}
% \textcolor{red}{
\vspace{-5pt}
\subsubsection{Transfer learning setting}
\label{subsec:transfer_learning_ablation}
In addition to the training from scratch setting, we also discuss the impact of hyperparameters in the transfer learning setting on CIFAR100. In general, the settings of the hyperparameters follow similar trends in the CIFAR10 model for transfer learning, details of which can be found in the Appendix \ref{appdix:ablation_cifar_transfer}.
We retrain the 2 FC layers of the pretrained WRN-28-10 model from ImageNet32 to CIFAR100 by 20 epochs. We adopt different batch sizes, filtering ratios, block sizes, and privacy budgets and summarize the results in Table~\ref{tab:cifar100_ablation}. 
% }

\begin{table*}[h!]
\caption{
% \textcolor{red}{
Different settings of block size, filtering ratio, privacy budgets,  and batch size on CIFAR100 dataset for WRN-28-10 transfer learning with 2 FC layers.}
% }
\label{tab:cifar100_ablation}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccccccccccc}
\hline
Block size      & 5  & 5  & 5  & 5  & 5   & 5   & 5   & 5   & 5  & 5  & 5  & 5  & 5  \\
Filtering Ratio ($\rho$)   & 0  & 0  & 0  & 0  & 0.2 & 0.4 & 0.6 & 0.8 & 0  & 0  & 0  & 0  & 0  \\
Privacy Budget ($\epsilon,10^{-5}$)    & 1  & 1  & 1  & 1  & 1   & 1   & 1   & 1   & 2  & 4  & 2  & 4  & 4  \\
% Training Epochs      & 20 & 20 & 20 & 20 & 20  & 20  & 20  & 20  & 20 & 20 & 20 & 20 & 20 \\
Batch size       & 2048    & 1024    & 512     & 256     & 256     & 256 & 256 & 256 & 256     & 256     & 512     & 512     & 2048    \\ \hline
Test Accuracy  & 73.65\% & 75.42\% & 76.59\% & 77.68\% & 77.52\% &   76.83\%  &  73.25\%   &   66.52\%  & 77.78\% & 78.03\% & 76.98\% & 78.00\% & 77.60\%\\ \hline
\end{tabular}
}
\vspace{-10pt}
\end{table*}

% \textcolor{red}{
The hyperparameter selection for transfer learning does not follow the same trend as that of  training from scratch. First, we compare the impact of different batch sizes from 256 to 2048 with filtering ratio $\rho=0$ and $\epsilon=1$. As Table~\ref{tab:cifar100_ablation} shows, the best accuracy is reached at a batch size of 256, and model accuracy decreases as the batch size increases. Since transfer learning just retrains a small number of parameters, a smaller batch size facilitates more fine-grained learning and ultimately results in improved utility.
% }

% \textcolor{red}{
Next we explore the impact of filtering ratios. We choose five filtering ratios (0, 0.2, 0.4, 0.6, and 0.8) for the two FC layers with the accuracy shown in Figure~\ref{fig:cifar100_filtering_ratio}. Our findings show that a smaller filtering ratio results in better utility, indicating that
\ourmethodblock can benefit from the model training with noise added in the spectral domain 
% the DP noise added by our \ourmethodblock became smoother 
and reconstruction error has a more significant impact on model accuracy in transfer learning settings.
% }

% \textcolor{red}{
Block size is not a dominating factor for model utility in this case. We can observe that from Figure~\ref{fig:cifar100_filtering_ratio}, a smaller block size (5) leads to slightly better model utility, but its influence is not as significant as that of filtering ratios. This also indicates a tradeoff between efficiency and utility as a larger block size means fewer trainable parameters due to higher model compression, but lower model utility.
% }

%compress more of the model and lead to fewer trainable parameters, but also decrease the model accuracy.
%}












% % \subsubsection{Hyperparameters tuning in transfer learning}
% \subsubsection{Transfer learning setting} In addition to the training from scratch setting, we also discuss the impact of hyperparameters in the transfer learning setting based on the \texttt{Model3} and CIFAR10 dataset. We summarize the results with different pairs of training epochs, filtering ratio ($\rho$) and target privacy budget in Table~\ref{tab:trans}. First, we discuss the impact of training epochs for a target privacy budget training. We set a strict target privacy budget $\epsilon=0.5$ and conduct 10, 20, and 40 epochs of training. More training epochs results in less noise addition to each training step but leads to an increased number of training steps. Different choices of training epochs may affect the convergence and final accuracy of the model. 
% As Table~\ref{tab:trans} shows, fewer training epochs leads to higher accuracy when $\epsilon=0.5$. However, when the privacy budget is relaxed to $\epsilon=2$, we observe the similar test accuracy for 10 and 20 epochs.


% \begin{table*}[t]
% \caption{Different settings of training epochs, filtering ratios and privacy budgets on CIFAR10 dataset for ResNet-18 transfer learning with 4CONV+2FC trainable layers (\texttt{Transfer3}).}
% \centering
% % \small
% % \resizebox{\linewidth}{!}{
% % \begin{tabular}{c|cccccccccc}
% % \hline
% % Training Epochs        & 10   & 10  & 10  & 20  & 40  & 10  & 10  & 10  & 20  & 21   \\
% % Pruning Ratio          & 0.75 & 0.5 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2  \\
% % Privacy Budget $(\epsilon,10^{-5})$ & 0.5  & 0.5 & 0.5 & 0.5 & 0.5 & 0.2 & 1   & 2   & 2   & 2.15 \\\hline
% % Train Accuracy & 76.36\% & 77.50\% & 77.68\% & 76.83\% & 74.39\% & 71.44\% & 81.11\% & 83.82\% & 86.55\% & 88.12\% \\
% % Test Accuracy  & 71.50\% & 72.61\% & 72.89\% & 71.98\% & 70.31\% & 68.52\% & 73.72\% & 74.53\% & 74.49\% & 75.32\% \\ \hline
% % \end{tabular}
% \begin{tabular}{c|ccccccccc}
% \hline
% Training Epochs        & 10      & 10      & 10      & 20      & 40      & 10      & 10      & 10  & 20\\
% Filtering Ratio ($\rho$)          & 0.75    & 0.5     & 0.2     & 0.2     & 0.2     & 0.2     & 0.2     & 0.2  & 0.2  \\
% Privacy Budget $(\epsilon,10^{-5})$ & 0.5     & 0.5     & 0.5     & 0.5     & 0.5     & 0.2     & 1       & 2 & 2 \\\hline
% Train Accuracy & 76.36\% & 77.50\% & 77.68\% & 76.83\% & 74.39\% & 71.44\% & 81.11\% & 83.82\% & 86.55\% \\
% Test Accuracy  & 71.50\% & 72.61\% & 72.89\% & 71.98\% & 70.31\% & 68.52\% & 73.72\% & 74.53\% & 74.49\% \\ \hline
% \end{tabular}
% % }
% \label{tab:trans}
% \end{table*}

% We thus conduct experiments with 10 training epochs and set $\rho$ to 0.2, 0.5 and 0.75 under target privacy budget ($0.5, 10^{-5}$). Under this tight budget, setting $\rho$ to 0.2 and 0.5 results in the similar model performance. However, unlike the cases in Section~\ref{subsec:effectiveness_spectral_dp} for the training from scratch models, in transfer learning, setting a smaller rate like 0.2 gives better performance. The reason is because during the transfer learning process, we usually only tune the final layers of a pre-trained model, a large filtering ratio indicates losing too much information, thus worse accuracy.
% % To better explore the effectiveness of other parameters, we conduct experiments with 10 training epochs in the following experiments.

% Finally, we draw training curves using 10 training epochs with  $\rho=0.2$ under different privacy budgets in Figure~\ref{fig:resnet18-dptrain}. It demonstrates that our \ourmethod allows the model to converge quickly in the first several epochs. When $\epsilon>= 0.5$, the training curves demonstrate the similar tradeoff between privacy budget and accuracy. The utility of the model is limited only when the privacy budget becomes very low ($\epsilon=0.2$). In this case, training after the 5th epoch cannot further improve the accuracy. Nevertheless, this result (68.52\%) is still higher than the DP-SGD trained result (66.72\%) under a more relaxed privacy budget $\epsilon=1$.

% % Figure environment removed

% \textcolor{red}{The settings of the hyperparameters follow similar trends in the CIFAR100 model for transfer learning, details of which can be found in the Appendix.
% }

% % We plot the test accuracy during the training with different $\epsilon$ in Figure~\ref{fig:resnet18-dptrain}, which can demonstrate our \ourmethod allows the model to converge quickly in the first several epochs. When privacy budget is very small ($\epsilon=0.2$), the model utility is bounded and training after 5th epoch is not able to increase the accuracy.

% % Figure~\ref{fig:resnet18-prune} shows the test accuracy for \ourmethod training and DP-SGD training  with pruning ratio 0.2, 0.5 and 0.75 under target privacy budget $(\epsilon=0.5, 10^{-5})$. The performance is consistent with the results in  Section~\ref{subsec:effectiveness_spectral_dp}, i.e., large pruning rates lose too much information and lead to worse accuracy. Nevertheless, the accuracies in all cases are much higher than the DP-SGD training.




% % % Figure environment removed



