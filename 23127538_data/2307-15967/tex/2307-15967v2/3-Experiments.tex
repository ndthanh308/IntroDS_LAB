\section{Experiments}

We design comprehensive experiments to validate the effectiveness of our proposed MCond and aim to answer the following questions. 
\textbf{Q1}: Compared to other graph reduction methods, can MCond achieve better inductive inference accuracy?
\textbf{Q2}: Can the deployment of the synthetic graph improve the inference time and storage requirement?
\textbf{Q3}: Can the synthetic graph and mapping matrix extract valuable structural information?
\textbf{Q4}: Can the mapping matrix generalize well to different GNN architectures? 
\textbf{Q5}: How do the different components, i.e., optimization constraints, initialization, sparsification, hyper-parameters affect MCond?


\subsection{Experimental Settings}
\label{exp_set}

\noindent\textbf{Datasets}. 
We evaluate our proposed method on three real-world datasets in different characteristics, including a citation network (Pubmed) \cite{DBLP:conf/iclr/KipfW17}, an image network (Flickr) \cite{DBLP:conf/iclr/ZengZSKP20} and a social network (Reddit) \cite{hamilton2017inductive}. In the citation network, papers from different topics are considered as nodes and the edges are citations among the papers. Flickr contains descriptions and properties of images and the node class is the image category. Reddit is a social network where nodes are posts and comments in different topical communities. We follow the public dataset partition and label rates.  The original graph to be condensed only contains the training nodes and their interconnections.
The detailed descriptions of datasets are provided in Table \ref{tab:data}.


\begin{table}[ht]
\caption{The properties of datasets. The training set is utilized as the original graph to be condensed.}
\label{tab:data}
\resizebox{\linewidth}{!}
{\begin{tabular}{l|rrrrr}
\toprule
Dataset    & \#nodes & \#edges & \#feature & \#class & \#training set\\ \midrule
Pubmed     & 19,717   & 44,338      & 500    & 3        & 18,217  \\      %/ 500/ 1,000  \\ 
Flickr     & 89,250   & 899,756     & 500    & 7        & 44,625  \\        %/ 22,312/ 22,313     \\ 
Reddit     & 232,965  & 11,606,919  & 602    & 41       & 153,932 \\       %/ 23,699/ 55,334     \\ 
\bottomrule
\end{tabular}}
\end{table}


\noindent\textbf{Baselines}. We compare our proposed methods to conventional graph condensation, four coreset methods, and a virtual graph generation method. For conventional graph condensation, GNN is trained on the synthetic graph, and the inference is performed on the original graph. 
For coreset and virtual graph generation methods, GNN is trained on the original graph and then we use them to generate the synthetic graph for inference. 

The details of each method are as follows:
(1) GCond~\cite{jin2022graph}: the conventional graph condensation method condenses the large graph to a small synthetic graph for efficient GNN training;
(2) Random~\cite{jin2022graph}: randomly select some training nodes for each class and their corresponding subgraphs; 
(3) Degree~\cite{si2022serving}: select the training nodes by sorting the degree of training nodes in each class and picking the nodes with the highest degrees and their interconnections; 
(4) Herding~\cite{welling2009herding}: pick samples that are closest to the cluster center for each class, which is often used in continual learning ~\cite{rebuffi2017icarl,castro2018end}; 
(5) K-Center~\cite{sener2017active,farahani2009facility}: select the center samples to minimize the largest distance between a sample and its nearest center; 
(6) Virtual nodes graph (VNG)~\cite{si2022serving}: constructing the small virtual graph based on the GNN forward pass reconstruct error, which is only used for inference procedures.
We leverage GNN's latent node embeddings for node selection in Herding and K-Center. {The reduced graph size, labels and adjacency matrix format of all baselines are kept the same as our proposed method for a fair comparison}.



\begin{table*}[ht]
\scriptsize
\centering
\caption{The test accuracy (\%) of baselines and MCond under different reduction ratios. The best accuracies are highlighted.}
\setlength\tabcolsep{3.8pt} 
% \renewcommand{\arraystretch}{1}
\resizebox{\linewidth}{!}
{
\begin{tabular}{cccclllllllll}
\toprule
                                             &                                                   &                             & O$\to$O                     & \multicolumn{6}{c}{O$\to$S}                                                                                                                                                                            & \multicolumn{2}{c}{S$\to$O}                                                & \multicolumn{1}{c}{S$\to$S}                    \\
\cmidrule(lr){4-4} \cmidrule(lr){5-10} \cmidrule(lr){11-12} \cmidrule(lr){13-13} 
Dataset                                      & Batch                                     & $r$                         & Whole                       & \multicolumn{1}{c}{Random}     & \multicolumn{1}{c}{Degree} & \multicolumn{1}{c}{Herding}    & \multicolumn{1}{c}{K-Center} & \multicolumn{1}{c}{VNG} & \multicolumn{1}{c}{${{\rm{MCond_{OS}}}}$} & \multicolumn{1}{c}{GCond} & \multicolumn{1}{c}{${{\rm{MCond_{SO}}}}$} & \multicolumn{1}{c}{${{\rm{MCond_{SS}}}}$} \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{Pubmed}} & \multicolumn{1}{c|}{\multirow{2}{*}{Graph}} & \multicolumn{1}{c|}{0.16\%} & \multirow{2}{*}{79.03±0.09} & \multicolumn{1}{c}{74.78±0.25} & 74.88±0.14                 & 75.01±0.24                     & 74.58±0.14                   & 75.03±0.05              & \textbf{78.30±0.01}                            & 76.13±0.08                & { 76.70±0.01}                               & 75.97±0.05                                     \\
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.32\%} &                             & 74.88±0.16                     & 75.28±0.12                 & 75.08±0.18                     & 74.66±0.15                   & 76.10±0.09              & \textbf{78.40±0.01}                            & 77.01±0.04                & { 77.70±0.01}                               & 76.94±0.01                                     \\ \cmidrule{2-13} 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{\multirow{2}{*}{Node}}  & \multicolumn{1}{c|}{0.16\%} & \multirow{2}{*}{78.97±0.05} & 75.38±0.13                     & 75.58±0.44                 & 75.28±0.15                     & 75.19±0.24                   & 75.67±0.05              & \textbf{78.20±0.01}                            & 76.14±0.01                & { 76.23±0.05}                               & 75.93±0.24                                     \\
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.32\%} &                             & 75.48±0.14                     & 76.08±0.35                 & 75.48±0.21                     & 75.38±0.18                   & 76.40±0.01              & \textbf{78.35±0.05}                            & 76.81±0.04                & { 77.01±0.01}                               & 76.77±0.12                                     \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{Flickr}} & \multicolumn{1}{c|}{\multirow{2}{*}{Graph}} & \multicolumn{1}{c|}{0.10\%} & \multirow{2}{*}{50.88±0.12} & 43.06±0.41                     & 43.50±0.39                 & 43.02±0.40                     & 43.03±0.37                   & 45.53±0.55              & \textbf{47.57±0.07}                            & 47.03±0.14                & { 47.22±0.16}                               & 46.80±0.49                                     \\
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.50\%} &                             & 43.16±0.44                     & 44.35±0.32                 & 43.09±0.42                     & 43.08±0.31                   & 46.79±0.44              & \textbf{48.92±0.27}                            & 48.11±0.13                & { 48.70±0.15}                               & 47.96±0.11                                     \\ \cmidrule{2-13} 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{\multirow{2}{*}{Node}}  & \multicolumn{1}{c|}{0.10\%} & \multirow{2}{*}{49.93±0.06} & 42.49±1.18                     & 44.34±0.94                 & 42.50±1.18                     & 42.49±1.16                   & 45.31±1.14              & \textbf{47.78±0.65}                            & 47.02±0.13                & { 47.73±0.15}                               & 46.65±0.05                                     \\
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.50\%} &                             & 42.70±0.88                     & 44.58±0.73                 & 42.67±0.91                     & 42.61±0.89                   & 46.51±1.04              & \textbf{48.34±0.09}                            & 47.95±0.38                & { 48.13±0.17}                               & 47.66±0.61                                     \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{Reddit}} & \multicolumn{1}{c|}{\multirow{2}{*}{Graph}} & \multicolumn{1}{c|}{0.10\%} & \multirow{2}{*}{94.00±0.02} & 55.77±1.76                     & 55.87±1.42                 & 55.86±1.79                     & 55.36±1.78                   & 79.42±1.13              & \textbf{91.53±0.12}                            & 89.63±0.25                & { 90.06±0.23}                               & 88.77±0.44                                     \\
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.50\%} &                             & 58.26±1.68                     & 57.30±1.15                 & \multicolumn{1}{c}{58.52±1.77} & 55.42±1.81                   & 83.62±1.51              & \textbf{91.81±0.08}                            & 91.09±0.14                & { 91.33±0.11}                               & 90.66±0.08                                     \\ \cmidrule{2-13} 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{\multirow{2}{*}{Node}}  & \multicolumn{1}{c|}{0.10\%} & \multirow{2}{*}{92.80±0.07} & 52.82±0.57                     & 53.91±1.11                 & 52.92±0.58                     & 52.38±0.59                   & 79.02±1.55              & \textbf{90.17±0.07}                            & 89.03±0.25                & { 89.52±0.11}                               & 88.14±0.11                                     \\
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.50\%} &                             & 55.67±0.55                     & 55.67±1.70                 & 55.80±0.55                     & 52.43±0.56                   & 83.02±1.25              & \textbf{90.59±0.07}                            & 89.23±0.15                & { 89.92±0.07}                               & 88.33±0.17                                     \\ \bottomrule
\end{tabular}
}
\label{tab:41}
\end{table*}



% Figure environment removed

% Figure environment removed

\noindent\textbf{Evaluation settings}.
\label{eva_set}
As noted in Section \ref{sec:gc}, the $n\times n$ adjacency matrix $\widetilde{\bm a}$ that records the connectivity among the $n$ inductive nodes is optional. Thus, the evaluation of MCond is conducted in two scenarios where inductive nodes come in isolation or a connected subgraph, and we term these settings as \textit{node batch} and \textit{graph batch}, respectively. In the graph batch setting, $\widetilde{\bm a}$ is the adjacency matrix of the inductive nodes.
In the node batch setting, no interconnections between inductive nodes are recorded, hence we zero-out all elements within $\widetilde{\bm{a}}$ for inductive inference. We only have test node features and connections to training nodes.

MCond supports both GNN training and inference on the synthetic graphs. To comprehensively evaluate our approach, we consider four evaluation settings based on the graph deployment: (1) $\text{O}\to \text{O}$ (train and infer on the original graph), (2) $\text{O}\to \text{S}$ (train on the original graph and infer on the synthetic graph), (3) $\text{S}\to \text{O}$ (train on the synthetic graph and infer on the original graph), and (4) $\text{S}\to \text{S}$ (train and infer on the synthetic graph). {In practice, $\text{S}\to \cdot$ is utilized for scenarios with constrained training conditions, such as limited GPU support, restricted memory, and limited training time. $\cdot\to \text{S}$ is useful for latency-sensitive scenarios where fast inference is required.} 
Accordingly, we denote three variants of MCond as follows: (1) $\rm{MCond_{OS}}$: train on the original graph and infer on the synthetic graph, (2) $\rm{MCond_{SO}}$: train on the synthetic graph and infer on the original graph, and (3) $\rm{MCond_{SS}}$: train and infer on the synthetic graph.

We follow the conventional graph condensation \cite{jin2022graph} for the synthetic graph updating setting. Specifically,  SGC~\cite{wu2019simplifying} is adopted for graph condensation and deployment, which utilizes the same graph convolution kernel as GCN \cite{DBLP:conf/iclr/KipfW17} but achieves faster training speed while maintaining comparable performance. 
To evaluate the generalization ability of the synthetic graph and mapping matrix, we evaluate them on various GNN architectures, including SGC~\cite{wu2019simplifying}, GCN~\cite{DBLP:conf/iclr/KipfW17},  GraphSAGE~\cite{hamilton2017inductive}, APPNP~\cite{klicpera2018predict}, and Cheby~\cite{defferrard2016convolutional}.

We condense the original graph with $N$ nodes into a synthetic graph with $N'=r{N}$ ($0<r<1$) nodes, where $r$ is the reduction ratio of synthetic nodes to original nodes.
For the choices of condensation ratio $r$, we choose $r$ of Pubmed to be \{0.16\%, 0.32\%\}, i.e., \{50\%, 100\%\} of its label size considering its sparse label rate. As the nodes in the training set are all labeled in Flickr and Reddit, we choose $r$ as \{0.1\%, 0.5\%\}.

Each method's performance is evaluated using three criteria: test set accuracy (ACC), average inference time per batch (Time), and average memory usage (Memory).

\noindent\textbf{Hyper-parameters and implementation}. 
The hyper-parameters used in synthetic graph generation follow the conventional graph condensation and {others are searched by the grid search method on the validation set to identify the highest validation set accuracy for testing.} 
For all datasets, we use 2-layer models, and the learning rate for updating mapping matrix ${\bf M}$ (line 15 in Algorithm \ref{al}) is 0.1. $\epsilon$ in Eq. \eqref{map} is set as 1e-5. $\alpha$ and $\beta$ are searched from \{0, 0.01, 0.1, 1, 10, 100, 1000\} to balance the losses which differ significantly in magnitude. The number of epochs for Pubmed, Flickr, and Reddit are set as {3000, 4000, 4000}, respectively. 

To follow the common practice of not tuning the model based on the test set, we adopt the validation set as support nodes in the mapping matrix training procedure as \cite{zhao2021adaptive}. It's essential to note that this training process exclusively employs the validation set's features and connections, excluding any labels of the validation set.
We use the ADAM optimization algorithm to train all the models. To eliminate randomness, we repeat each experiment 5 times and report the average test score and standard deviation. The codes are written in Python 3.9 and the operating system is Ubuntu 16.0. We use Pytorch 1.11.0 on CUDA 11.7 to train models on GPU. The inference evaluation is conducted on the CPU with a batch size of 1000. All experiments are conducted on a machine with Intel(R) Xeon(R) CPUs (Gold 5120 @ 2.20GHz) and NVIDIA TITAN RTX GPUs with 24GB GPU memory. 





\subsection{Inference Accuracy Comparison (\textbf{Q1})}

We compare MCond with baselines under both node batch setting and graph batch setting, and the average accuracy with standard deviation is shown in Table~\ref{tab:41}. In the $\text{O}\to \text{S}$ setting, $\rm{MCond_{OS}}$ significantly outperforms other baselines. Coreset methods select representative nodes but discard a significant number of edges from the original graph. This action hinders the information propagation in the synthetic graph and impairs inference performance. While VNG generates a virtual graph to replace the original graph for inference, the use of plain weighted k-means and implicit one-to-one mapping relationships in VNG severely limits the expressive capacity of the generated graph, resulting in sub-optimal results. Remarkably, with substantial reduction rates, $\rm{MCond_{OS}}$ achieves results that are closely comparable to Whole, i.e., training and inference on the original graph. Even on the largest dataset Reddit, the accuracy remains well-maintained. Specifically, on the Pubmed dataset, $\rm{MCond_{OS}}$ achieves an accuracy of $78.40\%$ (graph batch) and $78.35\%$ (node batch), closely approaching the accuracy achieved by leveraging the original graph for inference, which is $79.03\%$ and $78.97\%$, respectively.
These results evidently reveal the effectiveness of our proposed mapping matrix, and the synthetic graph can be successfully employed in inductive inference. In the context of the $\text{S}\to \text{O}$ setting, we compare the synthetic graphs generated by $\rm{MCond_{SO}}$ and GCond. The observed improvement in results validates the superior quality of the trained GNN and highlights the effectiveness of incorporating the additional structure loss, which introduces topology information as a valuable supervision signal in the graph condensation process. Lastly, in the comparison between $\rm{MCond_{SO}}$ and $\rm{MCond_{SS}}$, where GNNs are both trained on the synthetic graph, $\rm{MCond_{SS}}$ achieves performances closely similar to $\rm{MCond_{SO}}$. This further indicates that test nodes receive high-quality information from the synthetic graph, and the established edges are appropriate and meaningful for accurate inference.
When comparing the node batch and graph batch settings, the graph batch setting introduces more connections between inductive nodes, resulting in better accuracy across all datasets. {Moreover, ${\bf M}$ ensures that the converted adjacency matrix ${\bm{a}}{\bf M}$ contains more edges than graph batches, exerting dominant influence over information propagation and enabling the node batch setting to maintain high accuracy.}

\subsection{Inference Time and Memory Comparison (\textbf{Q2})}

The inference time and memory usage of Table~\ref{tab:41} are shown in Fig.~\ref{fig:41} and Fig.~\ref{fig:411}.
Due to that $\rm{MCond_{OS}}$ and $\rm{MCond_{SS}}$ both utilize the synthetic graph for inference and yield the same results, we uniformly use ``MCond" to represent the performance on the synthetic graph in Figures. Similarly, ``Whole" represents the inference performance on the original graph, which is the same as GCond and $\rm{MCond_{SO}}$.
We could observe that our proposed method achieves significant inference speedup and memory savings compared to inference on the original graph, particularly on larger graphs and with smaller reduction ratios. On Reddit, MCond achieves $121.5\times$ inference speedup and $48.0\times$ memory saving under the graph batch setting. VNG incurs higher inference time and memory costs on Pubmed and Reddit. The observed differences are due to that VNG generates the dense adjacency matrix, which demands more computations compared to our method. MCond introduces an increase in inference time and memory overhead compared to coreset baselines, primarily due to the utilization of the mapping matrix and the conversion of connections to the synthetic graph.
However, this overhead is efficiently controlled by employing the thresholds to eliminate small entries in the matrix, which enables the sparse matrix operations in the inference procedure and trade-offs between accuracy, storage, and inference speed (refer to Section \ref{abl_stu} for more details). 
Although the graph batch setting achieves better accuracy than the node batch setting, these additional improvements come at the cost of increased memory requirements and higher inference latency. Benefiting from the propagation speedup and graph size reduction, MCond also demonstrates significant improvements in node batch inference. Particularly on the Reddit dataset, the memory savings can be $55.9\times$. This illustrates the generalizability of MCond across different inference settings and makes it a practical solution for inductive inference.



\begin{table}[t]
\scriptsize
\centering
\caption{Test accuracy (\%) and per-time propagation time (ms) of label propagation and error propagation on original (O) and synthetic (S) graphs (acceleration ratio in brackets).}
\setlength\tabcolsep{2pt} 
% \renewcommand{\arraystretch}{1}
\resizebox{1\linewidth}{!}{
\begin{tabular}{
ccc cc cc
}
\toprule
\multicolumn{1}{c|}{Dataset ($r$)} & \multicolumn{1}{c|}{Batch} & \multicolumn{1}{c|}{Graph}
 &   Vanilla
 &   LP 
 &   EP
 &   Time   
\\
\midrule
\multicolumn{1}{c|}{\multirow{4}*{\begin{tabular}[c]{@{}c@{}}Pubmed \\ (0.32\%)\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}*{Graph}} & \multicolumn{1}{c|}{O}   
&    77.70±0.01  &  77.82±0.07  &  77.88±0.10  
&  2.05  
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{}    & \multicolumn{1}{c|}{S}
&    76.94±0.01     &  77.92±0.04   &  77.10±0.01   
&  0.54 (3.8$\times$)       
\\

\cmidrule{2-7}
\multicolumn{1}{c|}{}   &\multicolumn{1}{c|}{\multirow{2}*{Node}} & \multicolumn{1}{c|}{O}   
&    77.01±0.01  &  77.34±0.05  &  77.24±0.05  
&  3.75  
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{}    & \multicolumn{1}{c|}{S} 
&    76.77±0.12     &  77.84±0.05   &  77.18±0.07   
&  1.39 (2.7$\times$)        
\\

\midrule
\multicolumn{1}{c|}{\multirow{4}*{\begin{tabular}[c]{@{}c@{}}Flickr \\ (0.50\%)\end{tabular}}} &\multicolumn{1}{c|}{\multirow{2}*{Graph}} & \multicolumn{1}{c|}{O}  
&    48.70±0.15  &  49.16±0.33  &  49.07±0.36  
&  2.69  
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{}    & \multicolumn{1}{c|}{S} 
&    47.96±0.23     &  48.64±0.24   &  48.43±0.27   
&  0.64 (4.2$\times$)        
\\
\cmidrule{2-7}
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{\multirow{2}*{Node}} & \multicolumn{1}{c|}{O}   
&    48.13±0.17  &  48.90±0.12  &  48.50±0.07  
&  5.14  
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{}    & \multicolumn{1}{c|}{S}
&    47.66±0.61     &  48.62±0.24   &  48.38±0.26   
&  1.45 (3.5$\times$)        
\\

\midrule
\multicolumn{1}{c|}{\multirow{4}*{\begin{tabular}[c]{@{}c@{}}Reddit \\ (0.10\%)\end{tabular}}} &\multicolumn{1}{c|}{\multirow{2}*{Graph}} & \multicolumn{1}{c|}{O}    
&    90.06±0.11  &  92.74±0.24  &  92.28±0.19  
&  133.43  
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{}    & \multicolumn{1}{c|}{S} 
&    88.77±0.44     &  89.82±0.25   &  89.45±0.89   
&  10.21 (13.1$\times$)      
 \\
\cmidrule{2-7}
\multicolumn{1}{c|}{}   &\multicolumn{1}{c|}{\multirow{2}*{Node}}  & \multicolumn{1}{c|}{O}   
&    89.52±0.11  &  91.86±0.03  &  91.48±0.01  
&  130.38  
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{}    & \multicolumn{1}{c|}{S}
&    88.14±0.11     &  89.08±0.34   &  88.57±0.37   
&  12.74 (10.2$\times$)      
 \\
 
\bottomrule
\end{tabular}
}
\label{tab:42}
\end{table}







\begin{table*}[ht]
\scriptsize
\centering
\caption{The test accuracy (\%) and inference time (ms) of different GNN architectures.}
\setlength\tabcolsep{4pt} 
% \renewcommand{\arraystretch}{1.1}
\resizebox{0.85\linewidth}{!}
{
\begin{tabular}{
ccc| cc| cc| cc| cc
}
\toprule
\multicolumn{3}{c|}{} 
 &  \multicolumn{2}{c|}{GCN}  
 &  \multicolumn{2}{c|}{GraphSAGE} 
 &  \multicolumn{2}{c|}{APPNP} 
 &  \multicolumn{2}{c}{Cheby}          
\\
\cmidrule{4-11}
Dataset ($r$) & Batch & Method
& Accuracy  & Time 
&  Accuracy  & Time 
& Accuracy  & Time 
&  Accuracy  & Time              
\\
\midrule
\multicolumn{1}{c|}{\multirow{4}*{\begin{tabular}[c]{@{}c@{}}Pubmed \\ (0.32\%)\end{tabular}}} &\multicolumn{1}{c|}{\multirow{2}*{Graph}} & $\rm{MCond_{SO}}$   
&  78.77±0.12  &  79.20  
&  78.40±0.22  &  72.60    
&  77.57±0.12  &  73.94  
&  75.43±0.50  &  81.86 
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{}    & $\rm{MCond_{SS}}$
&  76.67±0.54   &  8.72   
&  76.17±0.63   &  9.94    
&  75.03±0.17   &  9.95   
&  73.30±0.36   &  10.28      
\\
\cmidrule{2-11}

\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{\multirow{2}*{Node}}  & $\rm{MCond_{SO}}$   
&  78.70±0.42  &  73.59  
&  79.37±0.31  &  68.61    
&  77.70±1.84  &  66.55  
&  76.97±1.20  &  75.99 
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{} & $\rm{MCond_{SS}}$
&  76.37±0.78  &  7.47   
&  78.17±0.12  &  7.09    
&  76.80±1.35  &  6.55   
&  76.37±0.97  &  7.68      
\\

\midrule
\multicolumn{1}{c|}{\multirow{4}*{\begin{tabular}[c]{@{}c@{}}Flickr \\ (0.50\%)\end{tabular}}} &\multicolumn{1}{c|}{\multirow{2}*{Graph}} & $\rm{MCond_{SO}}$    
&  48.54±0.12   &  403.56   
&  48.18±0.29   &  401.67  
&  47.58±0.16   &  401.84  
&  45.54±0.19   &  414.18    
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{} & $\rm{MCond_{SS}}$
&  47.38±0.23  &  19.16    
&  47.35±0.20  &  19.26  
&  45.81±0.15  &  19.44  
&  44.45±0.03  &  20.21   
\\

\cmidrule{2-11}
\multicolumn{1}{c|}{} &\multicolumn{1}{c|}{\multirow{2}*{Node}} & $\rm{MCond_{SO}}$   
&  47.52±0.25   &  346.35   
&  47.41±0.17   &  311.50  
&  47.01±0.33   &  358.61  
&  45.22±0.78   &  362.28    
\\
\multicolumn{1}{c|}{}    &\multicolumn{1}{c|}{} & $\rm{MCond_{SS}}$ 
&  47.14±0.43  &  15.73    
&  47.14±0.20  &  19.02  
&  45.54±0.18  &  18.31  
&  44.81±0.77  &  19.10   
\\


\midrule
\multicolumn{1}{c|}{\multirow{4}*{\begin{tabular}[c]{@{}c@{}}Reddit \\ (0.10\%)\end{tabular}}} &\multicolumn{1}{c|}{\multirow{2}*{Graph}} & $\rm{MCond_{SO}}$ 
&  89.26±0.11  &  9719.34  
&  88.89±0.21  &  8657.03     
&  87.12±0.13  &  8504.01  
&  75.30±0.02  &  9860.81  
\\
 \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & $\rm{MCond_{SS}}$  
 &  86.73±0.12  &  81.93   
 &  85.82±0.14  &  80.34  
 &  84.26±0.11  &  81.15  
 &  74.12±0.39  &  83.54  
 \\


 \cmidrule{2-11}
 \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{2}*{Node}} & $\rm{MCond_{SO}}$ 
&  87.26±0.16  &  9422.38 
&  87.19±0.11  &  8447.82    
&  85.16±0.14  &  8398.37 
&  74.30±0.14  &  9655.12 
\\
 \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & $\rm{MCond_{SS}}$ 
 &  84.73±0.14  &  80.30  
 &  83.82±0.16  &  79.41 
 &  83.21±0.13  &  78.92 
 &  72.12±0.16  &  81.94 
 \\
 
\bottomrule
\end{tabular}
}
\label{tab:43}
\end{table*}






\subsection{Label Propagation and Error Propagation (\textbf{Q3})}

Establishing connections between inductive nodes and the small synthetic graph enables the non-parametric calibration of inductive inference on the synthetic graph.
To validate the effectiveness of the generated synthetic graph and node mapping in capturing valuable structural information, we conduct label propagation (LP)~\cite{wang2021combining} and error propagation (EP)~\cite{huang2020combining} techniques on the connected synthetic graph and compare the performance with the original graph results.
Specifically, the LP propagates the synthetic node labels ${\bf Y'}$ to the inductive nodes according to the synthetic graph structure $\mathbf{A}'$ and converted connection, generating predictions for inductive nodes. On the other hand, the EP first calculates the GNN prediction error for synthetic nodes and propagates the errors to revise the GNN predictions of inductive nodes. The small-scale synthetic graph enables high-efficiency propagation. 
We utilize the same GNN model for the original graph and synthetic graph for a fair comparison. The results obtained from the graph batch and node batch setting are presented in Table \ref{tab:42}, and only the propagation time is measured in the evaluation process.
According to the results, the integration of LP and EP consistently leads to improved test accuracies on the synthetic graphs. Notably, the enhanced results even surpass the vanilla performance achieved on the original graph, which becomes even more impressive when considering the negligible propagation time. Furthermore, the graph batch setting demonstrates superior performance compared to the node batch setting, attributed to the additional information obtained from connections among test nodes. These outcomes provide strong evidence that the learned adjacency matrix and converted connections successfully capture essential structural information and can further improve the prediction efficiently, even while retaining the original interconnections between test nodes.

\subsection{Generalizability for Different GNN Architectures (\textbf{Q4})}

An essential property of graph condensation lies in its generalizability across various GNN architectures, allowing the synthetic graph to be employed for training diverse GNN models. To assess the effectiveness of our proposed method in this context, we train different GNN models using the synthetic graph, including GCN, GraphSAGE, APPNP, and Cheby. Subsequently, well-trained GNN modes are leveraged for inductive inference on the connected original and synthetic graph. The test accuracy and inference time under graph batch and node batch settings are presented in Table \ref{tab:43}.
According to the results, all tested GNN models are faster than SGC, as illustrated in Fig. \ref{fig:41} and Fig. \ref{fig:411}. This is due to the fact that these GNN models transform the input features into lower dimensions and then propagate based on the adjacency matrix.
Leveraging the mapping matrix, the test nodes can be successfully inferred by different GNN architectures, yielding comparable inference accuracy and faster inference speed than the results obtained on the original graph. Notably, a performance gap is observed between Cheby and other GNNs on the Reddit dataset, primarily attributed to the limitations of graph condensation in these circumstances. However, even under such circumstances, our proposed method consistently maintains a similar performance to the large graph inference results. This finding demonstrates the effectiveness of the alignment of support node embeddings between the original graph and the synthetic graph, resulting in reliable and comparable inference performance.



\subsection{Ablation Study (\textbf{Q5})}
\label{abl_stu}
We conduct comprehensive experiments to show the effectiveness of each component in our proposed method, including optimization constraints, initialization, sparsification of mapping matrix, and hyper-parameters.


\begin{table*}[ht]
\scriptsize
\centering
\caption{The ablation experiment results of optimization constraints {under ${\rm{MCond_{SS}}}$ setting}.}
\resizebox{0.76\linewidth}{!}{
\begin{tabular}{
c| cc| cc| cc
}
\toprule

 &  \multicolumn{2}{c|}{Pubmed (0.32\%)}  
 &  \multicolumn{2}{c|}{Flickr (0.50\%)} 
 &  \multicolumn{2}{c}{Reddit (0.10\%)}           
\\
\cmidrule{2-7}
Methods 
& Node batch & Graph batch  
& Node batch & Graph batch  
& Node batch & Graph batch    
\\
 \midrule
Plain 
&  74.23±0.17    & 74.35±0.01  
&  46.52±0.68    & 46.82±0.43  
&  79.83±1.29    & 80.52±1.08   
\\
w/o $\mathcal{L}_{str}$
& 76.63±0.05 & 76.53±0.03  
& 47.04±0.38 & 47.79±0.27   
& 84.73±0.40 & 85.48±0.32  
\\
w/o $\mathcal{L}_{ind}$  
& 74.93±0.54  & 76.50±0.08  
& 46.82±0.75  & 47.02±0.24 
& 82.24±1.73  & 82.63±1.03 
\\
${\rm{MCond_{SS}}}$ 
& 76.77±0.12 & 76.94±0.01   
& 47.66±0.61 & 47.96±0.11 
& 88.14±0.11 & 88.77±0.44 
\\
\bottomrule
\end{tabular}
}
\label{tab:44}
\end{table*}


% Figure environment removed



\noindent\textbf{Optimization constraints.}
To assess the effectiveness of each constraint, ${{\rm{MCond_{SS}}}}$ is evaluated under both graph and node batch settings with the constraints disabled. With $\mathcal{L}_{gra}$ and $\mathcal{L}_{tra}$ serving essential components in generating the synthetic graph and node mapping, we evaluate ${{\rm{MCond_{SS}}}}$ in the following configurations: (i) without both the structure loss $\mathcal{L}_{str}$ and the inductive loss $\mathcal{L}_{ind}$ (referred to as ``Plain"); (ii) without the structure loss $\mathcal{L}_{str}$ (referred to as ``w/o $\mathcal{L}_{str}$"); (iii) without the inductive loss $\mathcal{L}_{ind}$ (referred to as ``w/o $\mathcal{L}_{ind}$"). The results of these settings are presented in Table \ref{tab:44}.
Firstly, when neither the structure loss nor the inductive loss is utilized, the accuracy significantly drops, indicating that both the synthetic graph and the mapping matrix are sub-optimized. Notably, inductive loss emerges as the most influential factor, as it facilitates the simulation of connections of inductive nodes and the alignment of propagated embeddings with the real deployment scenario. This insight emphasizes the crucial role of leveraging the inductive loss in achieving better performance in our proposed method.



\noindent\textbf{Mapping visualization and initialization.}
\label{clu_ini}
We demonstrate the effectiveness of initialization by visualizing the mapping matrix $\bf M$ and the training loss. Fig. \ref{fig:initial} (a) presents the well-trained mapping matrix $\bf M$ of Reddit. It is organized based on the classes to demonstrate the correlation between the classes of the original and synthetic graphs, and classes are ordered by class size.
The values on the diagonal are larger than others, indicating strong self-correlation among classes, and most original nodes are represented by synthetic ones within the same class. Additionally, the off-diagonal values reflect varying degrees of correlation between different classes. For instance, class 9 in the original graph exhibits a significant correlation with class 15 in the synthetic graph.

With this observation, we initialize the mapping matrix through synthetic graph labels, and the resulting initialization is shown in Fig. \ref{fig:initial} (b). In this initialization, higher weights are assigned to the diagonal values, and the color change is attributed to the class imbalance distribution. This indicates that nodes within the same class in the synthetic graph are given higher weights, following the target distribution. Additionally, we compare the loss of the mapping matrix with our proposed initialization and random initialization in Fig. \ref{fig:initial} (c). By using our proposed initialization, the loss starts from a smaller value and decreases faster. Moreover, the initialization leads to a higher accuracy of 88.15\% compared to 87.82\% without initialization.


\noindent\textbf{Sparsification.}
\label{spar}
To promote the sparsity of the learned mapping matrix $\bf M$, we eliminate entries with values smaller than a given threshold $\delta$. In Fig. \ref{fig:sparse}, we observe the trade-off between sparsity and accuracy with varying $\delta$ on three datasets. As $\delta$ increases, the sparsity of $\bf M$ decreases, while the accuracy initially improves and then declines. This behavior can be attributed to $\delta$ effectively suppressing noise in the mapping matrix and preserving valuable relationships. However, as $\delta$ becomes excessively large, it leads to substantial information loss and results in a decline in performance.

% Figure environment removed


\noindent\textbf{Hyper-parameter sensitivity analysis.}
The hyper-parameters $\lambda$ and $\beta$ play critical roles in controlling the weight of the structure loss and inductive loss during the training procedure. To investigate their influence, we perform the parameter sensitivity experiment on the Flickr dataset. 
In Fig. \ref{fig:hyper}, we present the accuracy performances w.r.t. different values of $\lambda$ and $\beta$. 
Notice that the values of $\lambda$ and $\beta$ vary widely, as different losses and their corresponding gradients differ significantly in magnitude.
For $\lambda$, we observe that values in the range of 0.01 to 0.1 yield the best performance. Higher values may excessively weigh the structure loss, potentially compromising the preservation of label information in the gradient loss.
Regarding $\beta$, a value of 100 ensures a balanced contribution of the inductive loss to the overall training process, resulting in improved performance. 
Overall, to achieve the best performance, $\lambda$ and $\beta$ should be chosen appropriately. 


% Figure environment removed
