\section{Related Work}

\subsection{Dataset Distillation and Graph Condensation}

The high computation cost of training neural networks on large datasets is a significant concern in the industry. 
In addition to meticulously designed models, researchers have been exploring methods to reduce training costs through data-centric approaches. Dataset distillation (DD)~\cite{wang2018dataset,zhao2020dataset,zhao2023dataset,cazenavette2022dataset} was the first proposed method to distill knowledge from a large training image dataset into a smaller set of synthetic samples. 
As for graph representation learning,  GCond \cite{jin2022graph} introduced the graph condensation method building upon the gradient matching framework. Additionally, DosCond \cite{jin2022condensing} focuses on the challenge of learning discrete structures and fast optimization, particularly in graph classification. However, these methods can only condense the training data and are unable to handle inductive nodes. 


\subsection{Coreset Selection and Coarsening}
To reduce the size of training data, it is intuitive to explore the application of coreset selection methods~\cite{har2004coresets}. For example,  
mixture models~\cite{lucic2017training} aim to use Gaussian mixture models to represent the underlying data distribution.
Low-rank approximation~ \cite{cohen2017input} targets finding a low-rank representation of the original dataset that captures its essential characteristics. However, coreset methods may suffer from performance degradation under an extremely small reduction rate.
In contrast, coarsening methods focus on reducing the size of a graph by grouping original nodes into super-nodes and defining their connections. 
Loukas et al. \cite{loukas2018spectrally} provide conditions such that the principal eigenvalues and eigenspaces of coarsened and original graph Laplacian matrices are close. 
Bravo-Hermsdorff et al. \cite{bravo2019unifying} propose a method that simultaneously sparsifies and coarsens graphs, preserving their large-scale structure and Laplacian pseudoinverse. However, coarsening methods mainly focus on reducing the size of a graph without significantly altering its basic properties and do not consider the model and task-specific factors. In contrast, our proposed method is built upon the graph condensation framework and the synthetic graph generated by graph condensation enables training diverse GNN models and achieves comparable performance to models trained on the large graph.

\subsection{GNN inference acceleration}
{Conventional inference acceleration methods typically fall into categories: pruning, quantization, and knowledge distillation (KD). Notably, the advanced pruning methods~\cite{chen2021unified, hui2023rethinking}, simplify both the input graph and GNN weights using learned masks for the adjacency matrix and GNNs. Quantization~\cite{tailor2020degree} employs low-precision integer arithmetic during inference to enhance computational speed. KD techniques~\cite{zhang2021graphless, tian2022nosmog, yang2023learning} aim to train lightweight MLPs that match the performance of the teacher GNN model, resulting in significant inference speedup. However, these methods primarily address model-centric optimizations, while our proposed data-centric approach is orthogonal to them and exhibits better generalization capabilities and broader applicability across various scenarios.}