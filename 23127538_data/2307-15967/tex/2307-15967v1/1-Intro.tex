\section{Introduction}
Graph neural networks (GNNs) \cite{wu2022graph, zhang2020reliable, zhang2022pasca, yu2023self, zheng2016keyword, sun2021heterogeneous} have emerged as powerful models for addressing a diverse array of real-world problems. 
Taking node features and their connectivity as the input, GNNs excel at capturing complex dependencies and relationships within graph-structured data, enabling them to extract expressive node representations for downstream tasks, e.g., node classification and link prediction. 

However, the increasing prevalence of large-scale graphs poses a significant challenge to GNNs owing to their computational footprints. Most GNNs follow the message passing paradigm \cite{gilmer2017neural} formulated as convolutions over the entire graph, which becomes the major bottleneck \cite{zhang2022graph} for scaling GNNs to large graphs. Taking the prominent graph convolutional network (GCN) \cite{DBLP:conf/iclr/KipfW17} as an example, its time and space consumptions are respectively quadratic and linear to the number of nodes, which is directly associated with the graph size. This issue is further amplified when multiple versions of one GNN need to be trained, such as neural architecture search~\cite{zhang2022pasca}, continual learning~\cite{rebuffi2017icarl}, and hyper-parameter tuning~\cite{DBLP:conf/kdd/LiSZCJLJG0Y0021} -- all are commonly seen in today's applications.

The efficiency barrier has motivated recent advances in graph condensation (GC)~\cite{jin2022graph,jin2022condensing,loukas2018spectrally}. GC aims to construct a synthetic and compact (e.g., $1,000\times$ smaller) graph that captures essential characteristics of the original full-size graph. In this paper, \textit{original graph} and \textit{synthetic graph} are respectively used to denote the graphs before and after the GC process. The compact synthetic graph allows a GNN to be trained more efficiently, with performance comparable to models trained on the original graph.
Paired with GC, GNNs are endowed with stronger practicality in time-critical and resource-constrained settings. Among the downstream application settings, our main focus in this paper is \textit{inductive node representation learning}, which handles unseen nodes outside the training graph (i.e., the original graph in GC), and is arguably more practical in high throughput systems \cite{hamilton2017inductive, DBLP:conf/iclr/ZengZSKP20}.

Despite the potential in promoting scalable use of GNNs, existing GC practices are still suboptimal for inductive inference. Generally, the aforementioned GC methods are centered around GNNs' efficiency during the training stage, while less attention has been paid to the inference/deployment stage, especially in the presence of inductive nodes. In a nutshell, the only objective of their GC process is abstracting node information (i.e., features and labels) and graph structure (i.e., node adjacency) to facilitate cost-effective GNN training, as depicted by Fig. \ref{fig:11} (a). However, to perform inference on inductive nodes, the trained GNN is still required to be deployed on the original graph~\cite{si2022serving,wang2018streaming,chen2013terec}. As shown in Fig. \ref{fig:11} (b), only after connecting with some nodes in the original graph, the message passing to inductive nodes can be performed to learn their representations. 
As a result, inductive nodes cannot be handled without storing the original graph and performing message passing on it, defeating the purpose of abstracting a high-quality, substantially smaller synthetic graph. 


% Figure environment removed

The underlying cause of this defect is that,
the learned synthetic graph is essentially a stand-alone data simulator that resembles the distribution of the original graph w.r.t. node features, structural properties, class labels, etc., but withholds little connection with the nodes in the original graph. Consequently, the synthetic graph cannot establish appropriate and meaningful edges to new nodes, prohibiting inductive message passing purely based on the synthetic graph. 
In an ideal GC process, an explicit mapping between original and synthetic nodes is expected to be learned along with the synthetic graph, then the inductive and synthetic nodes can be easily linked by referring to the original nodes that associate with both sides. 

Hence, to empower GC with stronger inference efficiency for inductive node representation learning, we aim to investigate GC with updated objectives to be learned from the original graph: (1) a synthetic graph consisting of simulated nodes, their features, and class labels, as well as the adjacency matrix; (2) a deterministic and symmetric mapping between synthetic and original nodes. The problem studied in this paper is one step above most existing work on GC \cite{jin2022graph,jin2022condensing} which only concerns (1). 
To do so, the first challenge comes from the parameterization and optimization of the mapping required for (2). A recent attempt \cite{si2022serving} is to perform clustering on original nodes, where nodes within each cluster are assigned to the same synthetic node. However, as there are substantially fewer synthetic nodes than original nodes, this will inevitably render multiple inductive nodes having the same set of neighbors in the synthetic graph, thus weakening the uniqueness and expressiveness of their learned representations. In the meantime, the joint optimization of these two heavily entangled GC components is crucial yet challenging, as the quality of either the synthetic graph or the mapping bootstraps the other, and will eventually impact the inductive inference performance. 

In light of the challenges, we propose \underline{m}apping-aware graph \underline{cond}ensation (MCond), a novel graph condensation approach with one-to-many node mapping from original nodes to synthetic nodes, to support efficient inductive node representation learning. As Fig. \ref{fig:11} (c) shows, to address the first challenge with MCond, we view each original node as a weighted ensemble of some synthetic nodes, where a sparse $N\times N'$ mapping matrix is learned to encode such information. Here, $N$ and $N'$ are respectively the numbers of original and synthetic nodes ($N'\ll N$). Then, for each inductive node, by looking up its neighbors in the full graph and those neighbors' mapping to synthetic nodes, weighted edges can be established between it and the synthetic graph (Fig. \ref{fig:11} (d)). This brings a significant efficiency boost by involving only the synthetic graph for inductive node inference. 
To tackle the second challenge, we introduce an alternating optimization paradigm to jointly learn the synthetic graph and mapping. Specifically, the synthetic graph is updated by promoting its capability to preserve the structural property and to mirror a GNN's learning trajectory (i.e., gradients) w.r.t. the original graph. Then, by introducing two innovative loss terms from transductive and inductive perspectives, the explicit mapping between nodes can be updated based on the interim synthetic graphs learned from earlier steps. The two optimization procedures are performed alternately by fixing one component and updating the other, so as to collaboratively maximize their synergy and utility. 

The main contributions of this paper are three-fold:
\begin{itemize}
  \item We identify the efficiency bottleneck of existing GC methods for inductive node representation learning, and point out the necessity of learning an explicit mapping between original and synthetic nodes, which is an important yet underexplored problem in GC.
  \item We present MCond, a novel GC approach that explicitly learns a sparse mapping matrix to seamlessly integrate new nodes into the synthetic graph for inductive representation learning. In MCond, an alternating optimization scheme is designed to let the synthetic graph and mapping matrix take turns to update toward dedicated objectives. 
  \item Through extensive experimentation, we verify that MCond is both efficient and performant in inductive inference, with up to $121.5\times$ inference speedup and $55.9\times$ reduction in storage requirements compared with counterparts based on the original graph. Additionally, by innovatively introducing two non-parametric calibration methods, label and error propagation, we further validate the effectiveness of the generated synthetic graph and node mapping in capturing valuable structural information.
\end{itemize}

