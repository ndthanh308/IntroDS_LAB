\documentclass[conference]{IEEEtran}
\pagestyle{plain}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{tabularray}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{diagbox}
\usepackage{amsmath}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{bm}

\makeatletter
\renewcommand{\maketag@@@}[1]{\hbox{\m@th\normalsize\normalfont#1}}%
\makeatother
\begin{document}

\title{Graph Condensation for Inductive Node Representation Learning\\

}


\author{
    \IEEEauthorblockN{Xinyi Gao$^{1}$, Tong Chen$^{1}$, Yilong Zang$^{2}$, Wentao Zhang$^{3}$, Quoc Viet Hung Nguyen$^{4}$, Kai Zheng$^{5}$, Hongzhi Yin$^{1}$}
    \IEEEauthorblockA{$^1$ The University of Queensland, Brisbane, Australia}
    \IEEEauthorblockA{$^2$ Wuhan University, Wuhan, China}
     \IEEEauthorblockA{$^3$ Peking University, Beijing, China}
    \IEEEauthorblockA{$^4$ Griffith University, Gold Coast, Australia}
    \IEEEauthorblockA{$^5$ University of Electronic Science and Technology of China, Chengdu, China}
}


\maketitle

\begin{abstract}

Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for inductive representation learning. This enables direct information propagation on the synthetic graph, which is much more efficient than on the original large graph. Specifically, MCond employs an alternating optimization scheme with innovative loss terms from transductive and inductive perspectives, facilitating the mutual promotion between graph condensation and node mapping learning. Extensive experiments demonstrate the efficacy of our approach in inductive inference. On the Reddit dataset, MCond achieves up to $121.5\times$ inference speedup and $55.9\times$ reduction in storage requirements compared with counterparts based on the original graph.




\end{abstract}

\begin{IEEEkeywords}
Graph condensation, graph neural network, inference acceleration, label propagation.
\end{IEEEkeywords}


\input{1-Intro}
\input{2-Method}
\input{3-Experiments}
\input{4-Related_Work}
\input{5-Conclusion}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ref}


\end{document}
