\section{Preliminaries}
In this section, we first revisit both graph neural networks (GNNs) and conventional graph condensation (GC), then present the definition of the problem studied. 

% Figure environment removed

\subsection{Graph Neural Networks}
\label{sec:gcn}
We consider a graph $\mathcal{T}=\{{\bf A}, {\bf X}, {\bf Y}\}$ consisting of $N$ nodes, where ${\bf X}\in{\mathbb{R}^{N\times d}}$ is the $d$-dimensional node feature matrix and ${\bf Y}\in\{1,\ldots,C\}^N$ denotes the node labels over $C$ classes. ${\bf A}\in \mathbb{R}^{N\times N}$ is the adjacency matrix, with entry ${\bf A}_{i,j}>0$ denoting an observed edge from node $i$ to $j$, and ${\bf A}_{i,j}=0$ otherwise. 
GNNs leverage the graph topological information and node features to learn node representation (i.e., embeddings) via message-passing. Without loss of generality, we use graph convolutional network (GCN) \cite{DBLP:conf/iclr/KipfW17} as an example, where the propagation process in the $\ell$-th layer is as follows: 
\begin{equation}
{\bf{H}^{(\ell)}} =\text{ReLU}\left(\hat{\mathbf{A}}\bf{H}^{(\ell-1)}\mathbf{W}^{(\ell)}\right),   
\label{eq_GCN}
\end{equation}
where $\hat{\mathbf{A}}=\widetilde{\mathbf{D}}^{-\frac{1}{2}}\widetilde{\mathbf{A}}\widetilde{\mathbf{D}}^{-\frac{1}{2}}$ is the normalized adjacency matrix with the self-loop. $\widetilde{\mathbf{D}}$ is the degree matrix and $\mathbf{W}^{(\ell)}$ is the trainable weights at layer $\ell$. 
${\bf{H}}^{(\ell)}$ is the output node embeddings from the $\ell$-th layer, and ${\bf{H}}^{(0)}=\mathbf{X}$.
By propagating the node features through a total of $L$ GNN layers, each node's final representation can capture information from connected nodes up to $L$-hop away. For simplicity, we denote an $L$-layer GNN as $\mathbf{H}=f(\mathbf{A},\mathbf{X})$, where $\mathbf{H}$ denotes the final node embeddings. 

\subsection{Graph Condensation}
\label{sec:gc}
Graph condensation~\cite{jin2022graph} is proposed to learn a synthetic graph with $N'\ll{N}$ nodes from the original graph $\mathcal{T}$, denoted by $\mathcal{S}=\{{\bf A'}, {\bf X'},{\bf Y'}\}$ with ${\bf A'}\in\mathbb{R}^{N'\times N'}$, ${\bf X'}\in\mathbb{R}^{N'\times d}$, ${\bf Y'}\in\{1,\ldots,C\}^{N'}$, such that a GNN $f(\cdot)$ solely trained on $\mathcal{S}$ can achieve comparable node classification performance to the one trained on the much larger $\mathcal{T}$ \cite{jin2022graph}: 
\begin{equation}\label{eq_GCloss}
\begin{split}
&\min_{\mathcal{S}}\mathcal{L}\big{(}\mathrm{classifier}(f_{{\theta}_{\mathcal{S}}}({\bf A},{\bf X})), {\bf Y}\big{)}\\
&\text{s.t.} \,\, \theta_{\mathcal{S}}= \underset{\theta}{\mathrm{argmin}}\,  \mathcal{L}\big{(}\mathrm{classifier}(f_{\theta}({\bf A'},{\bf X'})), {\bf Y'}\big{)},
\end{split} 
\end{equation}
where $\mathrm{classifier}(\cdot)$ is a readout layer that maps the learned node embeddings into predicted class labels, ${\theta}$ denotes the parameterization of the GNN, and $\mathcal{L}(\cdot)$ is the classification loss (i.e., cross entropy in our case). It is worth noting that, node classification is the most common task in GC, while other tasks like node-level regression and outlier detection can be easily applied with availability of labeled data. 

\textbf{Handling Inductive Nodes with $f_{{\theta}_{\mathcal{S}}}(\cdot)$}. As Eq. (\ref{eq_GCloss}) suggests, obtaining a fully trained GNN $f_{{\theta}_{\mathcal{S}}}(\cdot)$ has become much more computationally friendly because of the small-scale synthetic graph $\mathcal{S}$. Let us now investigate the cost of inductive inference, where a batch of $n$ new nodes join $\mathcal{T}$ during test time. Because inductive nodes are unable to establish edges with the synthetic nodes in $\mathcal{S}$, we have to resort to their connectivity with the original graph and aggregate information from nodes within $\mathcal{T}$. We denote their features as ${\bm x}\in{\mathbb{R}^{n\times d}}$, and use an incremental adjacency matrix ${\bm{a}}\in{\mathbb{R}^{n\times N}}$ to encode their connections with nodes in $\mathcal{T}$. To facilitate the forward pass described in Eq. (\ref{eq_GCN}), the adjacency and feature matrices are updated into:
\begin{equation}
\label{connect_test}
\begin{aligned}
\mathbb{A}  ={\begin{bmatrix}
 \mathbf{A} & \!{\bm{a}}^{\top} \\
 {\bm{a}} & \!\!\!\!\widetilde{{\bm{a}}} 
\end{bmatrix}}, \ \ \ 
\mathbb{X}  ={\begin{bmatrix}
 \mathbf{X}  \\
 {\bm{x}}
\end{bmatrix}},
\end{aligned} 
\end{equation}
where $\mathbb{A}\in{\mathbb{R}^{(N+n)\times (N+n)}}$ and $\mathbb{X}\in{\mathbb{R}^{(N+n)\times d}}$. Based on use case and availability, $\widetilde{\bm{a}}\in{\mathbb{R}^{n\times n}}$ can be leveraged to indicate edges among the test nodes, or be zero-padded.

With Eq. (\ref{connect_test}), we are able to perform inductive inference via $\mathbb{H} = f(\mathbb{A},\mathbb{X})$, and retrieve the inductive nodes' embeddings from output $\mathbb{H}$. Assuming a consistent feature dimension $d$ is applied across all $L$ layers, the memory consumption incurred is asymptotically $\mathcal{O}(||\mathbb{A}||_0+(N+n)d)$, which is mainly for storing the non-zero entries in $\mathbb{A}$ (i.e., the number of edges) and the node features. Meanwhile, each forward pass has a time complexity of $\mathcal{O}(L(N+n)^2d)$. As $n\ll N$ in most real-world applications and can be thus omitted, the space and time complexity for inductive inference is heavily constrained by the size of the original graph, leading to subpar scalability. 

\subsection{Problem Formulation}
Motivated by the deficiency of existing GC in the inductive setting, we now formulate our research problem. For a large original graph $\mathcal{T}=\{{\bf A}, {\bf X}, {\bf Y}\}$, aside from learning a small synthetic graph $\mathcal{S}=\{{\bf A'}, {\bf X'},{\bf Y'}\}$, we also aim to learn a one-to-many mapping from original nodes to synthetic nodes. %Each real node is viewed as a weighted ensemble of some synthetic nodes and the node 
The mapping is parameterized by a non-negative sparse matrix ${\bf M} \in \mathbb{R}^{N\times N'}$. Specifically, each entry ${\bf M}_{i,j}>0$ if the original node $i$ is associated with synthetic node $j$ and ${\bf M}_{i,j}=0$ otherwise, where a higher value indicates a stronger correlation between them. In the next section, we will unfold the design details for learning both $\mathcal{S}$ and ${\bf M}$, and discuss the efficiency advantages of mapping-aware GC in inductive node representation learning.

\section{METHODOLOGIES}
We hereby present our proposed mapping-aware graph condensation (MCond). We begin with the learning of synthetic graph $\mathcal{S}$ (Fig. \ref{fig:21} (a)-(b)), then move onto tasks for updating the mapping ${\bf M}$ (Fig. \ref{fig:21} (c)-(d)). We will wrap up this section with the alternating optimization paradigm for both components, and the deployment of GNNs on $\mathcal{S}$ and ${\bf M}$ for inductive inference.


\subsection{Label-based Gradient Matching (Learning $\mathcal{S}$)}\label{sec:lbgm}
To meet the condensation objective in Eq. \eqref{eq_GCloss}, we adopt the gradient alignment advocated by \cite{jin2022graph,jin2022condensing} as one task for learning $\mathcal{S}$ from $\mathcal{T}$. As a fundamental component in MCond, we introduce it in a relatively brief vein, and our main contribution lies on subsequent improvements over the existing work. In short, when a GNN is being fitted to either the original graph $\mathcal{T}$ or its synthetic version $\mathcal{S}$, its learning trajectory should exhibit similar patterns. %Specifically, the gradient loss to reduce the difference of model gradients w.r.t. original graph and synthetic graph during the training procedure.
To promote this, a natural way is to instantiate a relay GNN $f(\cdot)$, and align its gradients w.r.t. both graphs' labels:
\begin{equation}\label{grad_lossGC}
\begin{split}
& \min _{\mathcal{S}} {\mathop{\mathbb{E}}_{\boldsymbol{\theta}_{0}\sim P_{\boldsymbol{\theta}_0}}} \left[\sum_{t=1}^{T} \mathcal{L}_{gra} (\mathcal{G}^{\mathcal{T}}_{\theta_t}, \mathcal{G}^{\mathcal{S}}_{\theta_t} )\right] \\
& \text{s.t.} \,\, {\theta}_{t+1} = \operatorname{optmizer}_{\theta}(\mathcal{L}(\cdot), f_{{\theta}_t}(\cdot), \mathcal{S}),\\
\end{split}
\end{equation} where $t$ indexes the training step, and $\theta_t$ is the up-to-date GNN parameter at step $t$. $\mathcal{G}^{\mathcal{T}}_{\theta_t}$ and $\mathcal{G}^{\mathcal{S}}_{\theta_t}$ are respectively the gradients of $\theta_t$ w.r.t. graphs $\mathcal{T}$ and $\mathcal{S}$, i.e.,  $\mathcal{G}^{\mathcal{T}}_{\theta_t}=\nabla_{\theta_t}\mathcal{L}(\mathrm{classifier}(f_{\theta_t}(\mathbf{A},\mathbf{X})), \mathbf{Y})$ and $\mathcal{G}^{\mathcal{S}}_{\theta_t}=\nabla_{\theta_t}\mathcal{L}(\mathrm{classifier}(f_{\theta_t}(\mathbf{A}',\mathbf{X}')), \mathbf{Y}')$. Each training step updates the relay GNN's parameter with a gradient descent-based optimizer $\operatorname{optmizer}_{\theta}(\cdot)$ and \textit{only} $\mathcal{S}$. By taking different parameter initializations drawn from the distribution $P_{\boldsymbol{\theta}_{0}}$, the learned $\mathcal{S}$ can avoid overfitting a specific initialization. Suppose the gradient $\mathcal{G}=\{\mathbf{G}^{(\ell)}\}_{\ell=1}^{L}$ in Eq. \eqref{grad_lossGC} entails all $L$ layers' gradient matrices $\mathbf{G}^{(\ell)}$, the gradient distance $\mathcal{L}_{gra}(\cdot)$ is calculated by summing up all layers' pairwise gradient distances: 
\begin{equation}
\mathcal{L}_{gra}(\mathcal{G},\mathcal{G}')=\sum_{\ell=1}^{L}\sum_{i=1}^{D_{\ell}}\left(1-\mathrm{cosine}(\mathbf{G}^{(\ell)}_i, {\mathbf{G}'_i}^{(\ell)})\right),
\end{equation}
where $\mathrm{cosine}(\cdot)$ is the cosine similarity, $\mathbf{G}^{(\ell)}_i$ and $\mathbf{G}'^{(\ell)}_i$ are the $i$-th column vector (out of all $D_{\ell}$) in the gradient matrix $\mathbf{G}^{(\ell)}$ and $\mathbf{G}'^{(\ell)}$ at layer $\ell$, respectively. To facilitate efficient learning of $\mathcal{S}$, a common practice is to reduce the trainable pieces in the synthetic graph $\mathcal{S}=\{\mathbf{A}', \mathbf{X}', \mathbf{Y}'\}$ to only the node features $\mathbf{X}'$. Concretely, the labels $\mathbf{Y}'$ can be predefined to match the distribution of different classes in the original graph, while each entry in $\mathbf{A}'$ is parameterized by the symmetric affinity between the features of two nodes \cite{jin2022graph}:
\begin{equation}\label{eq:adj}
{\bf A}_{i,j}' = \sigma\left(\frac{{\text{MLP}_{\Phi}([{\bf x}'_i; {\bf x}'_j])} + {\text{MLP}_{\Phi}([{\bf x}'_j; {\bf x}'_i])}}{2}\right),
\end{equation}
where $\text{MLP}_{\Phi}(\cdot)$ is a multi-layer perceptron parameterized by~${\Phi}$, and fed with the concatenation of synthetic node features ${\bf x}'_i$ and ${\bf x}'_j$. $\sigma$ is the sigmoid function.


\subsection{Topology-preserving Graph Condensation (Learning $\mathcal{S}$)}
\label{sec:tpgc}
This is where our own design emerges, starting with a more nuanced mechanism for retaining information from the original graph during GC. Note that the remaining components of MCond also need engagement from the relay GNN, for which we keep using notation $f(\cdot)$ throughout the rest of the paper. The condensation process described in Section \ref{sec:lbgm} predominantly preserves the semantic similarity between $\mathcal{S}$ and $\mathcal{T}$, implied by the fact that the synthetic graph structure $\mathbf{A}'$ is purely determined by the pairwise similarity between generated node features. However, the informative structural signals within $\mathcal{T}$ (specifically $\mathbf{A}$) is largely overlooked, making $\mathcal{S}$ a less plausible proxy of $\mathcal{T}$. 

To allow the topological clues within $\mathcal{T}$ to be carried over into $\mathcal{S}$, we put forward a novel structure-based optimization objective in the GC context. Intuitively, we aim to correctly infer the structural information of the original graph from the synthetic one, in which the mapping matrix $\mathbf{M}$ plays a vital role. In $\bf M$, each row $\mathbf{M}_i$ is a $N'$-dimensional vector, where the non-zero entries indicate weighted correlations between original node $i$ and a collection of synthetic nodes. In other words, each original node $i$ can be regarded as a weighted ensemble of synthetic nodes identified by $\mathbf{M}_i$, where the approximate representation of all original nodes $\widetilde{\bf{H}}$ can be obtained via a weighted look-up operation:
\begin{equation}\label{eq_weightedlookup}
    \widetilde{\bf{H}} = {\bf M}{\bf H}', 
\end{equation}
where ${\bf H}'=f_{\theta_t}(\mathbf{A}',\mathbf{X}')$ is the embeddings of all synthetic nodes obtained from the relay GNN.
With that, the topology of $\mathcal{T}$ is introduced as another supervision signal via a link reconstruction task. Given that our original graphs are all unweighted (i.e., $\mathbf{A}_{i,j}=1$ for observed edges and $\mathbf{A}_{i,j}=0$ for unobserved ones) a binary structure loss $\mathcal{L}_{str}$ is formulated in the following logarithm term:
\begin{equation}
\label{str_loss}
\small
\mathcal{L}_{str}(\mathbf{A},{{\widetilde{\bf{H}}}}) =- \frac{1}{ \left | {\mathcal{B}} \right | } \sum_{\left ( i,j \right ) \in {\mathcal{B}}}\mathbf{A}_{i,j}\log \left(\sigma( 
{\widetilde{{\bf{H}}}}_i{\widetilde{{\bf{H}}}}^{{\mathsf{T}}}_j)\right ),
\end{equation}
where ${\mathcal{B}}$ is a mini-batch consisting of both positive and negative edge samples. $\sigma$ is the sigmoid function. Note that the approximate original node representations ${{\widetilde{\bf{H}}}}$ are recomputed at every training step, ensuring consistency with Eq. \eqref{grad_lossGC}. Hence, by incorporating Eq. (\ref{str_loss}) and omitting the initialization and update of the relay GNN in Eq. (\ref{grad_lossGC}), the merged loss term for $\mathcal{S}$ at each training step is: 
\begin{equation}\label{grad_loss}
\begin{split}
& \mathcal{L}_{\mathcal{S}} = \mathcal{L}_{gra}(\mathcal{G}^{\mathcal{T}}_{\theta_t}, \mathcal{G}_{\theta_t}^{\mathcal{S}}) +  \lambda\mathcal{L}_{str}(\mathbf{A},{{{\widetilde{\bf{H}}}}}),
\end{split}
\end{equation}
where $\lambda$ is a balancing hyperparameter to be tuned. Next, we describe how the mapping matrix $\mathbf{M}$ is learned in MCond.

\subsection{Transductive Mapping Constraint (Learning $\mathbf{M}$)}
\label{sec:fpmf}
Based on our earlier discussions, ${\bf M}$ explicitly encodes the node mapping between the original and synthetic graphs, such that each original node can be represented by corresponding synthetic nodes. To efficiently parameterize the mapping matrix ${\bf{M}}$, we enforce a transductive constraint. Specifically, we aim to leverage the representations of existing/transductive nodes in $\mathcal{T}$ to strengthen the mapping to $\mathcal{S}$. Specifically, such a relational mapping should also be captured in the latent embedding space, advocating ${{\bf{H}}} \approx {\bf M}{\bf H}'$. Note that ${\bf{H}}=f(\mathbf{A},\mathbf{X})$ is the exact embeddings of all original nodes produced by the relay GNN on $\mathcal{T}$, instead of the approximate one in Eq. (\ref{eq_weightedlookup}). We also freeze the parameters of the relay GNN while updating $\mathbf{M}$, hence we omit its subscript ${\theta_t}$ when there is no ambiguity. 


Though the above constraint comes with a closed-form solution ${\bf{M}}={\bf{H}}{\bf{H}'}^{-1}$, it is inefficient regarding the computations needed. This is because the inverse of non-square matrix $\bf{H}$ cannot be straightforwardly calculated (will take $\mathcal{O}(n^3)$ even if it is square), and a pseudo-inverse using singular value decomposition has to be used as a workaround. As such, we propose to make $\mathbf{M}$ end-to-end trainable via the transductive loss defined below:
\begin{equation}
\mathcal{L}_{tra}(\mathbf{H}, {\mathbf{H}}', {\mathbf{M}})= \frac{1}{N}  || {{\mathbf{H}}} -{\mathbf{M}}{\mathbf{H}}' ||_{2,1}, 
\end{equation}
where $||\cdot||_{2,1}$ is the $L2,1$ matrix norm.


\subsection{Inductive Mapping Constraint (Learning $\mathbf{M}$)}\label{sec:inductive}
Our expectation on the mapping matrix $\mathbf{M}$ is to let it facilitate reasonable connections between inductive nodes and the synthetic graph, such that the message passing can come directly from the synthetic graph $\mathcal{S}$. 
To prepare $\mathbf{M}$ for inductive inference, we sample a small set of nodes from observed nodes, which are excluded from the synthetic graph generation, and term them as support nodes $\mathcal{T}_{sup}$. 

For notation simplicity, we reuse $n$ to denote the number of inductive nodes carried by $\mathcal{T}_{sup}$, and reuse $\bm a$ and $\bm x$ to respectively denote the support nodes' incremental adjacency matrix and features.
With a process similar to Eq. (\ref{connect_test}), the support nodes with incremental adjacency matrix $\bm a$ and feature $\bm x$ are connected to the \textit{synthetic graph} $\mathcal{S}$ instead of the original graph. As such, Eq. (\ref{connect_test}) is updated to the following:
\begin{equation}
\label{ind_matrix}
\begin{aligned}
\mathbb{A}' ={\begin{bmatrix}
 \mathbf{A}' & ({\bm{a}}{\bf M})^{\top} \\
 {{\bm{a}}{\bf M}} & \widetilde{{\bm{a}}} 
\end{bmatrix}},\ \ \ 
\mathbb{X}'  ={\begin{bmatrix}
 \mathbf{X}'  \\
 {\bm{x}}
\end{bmatrix}}.
\end{aligned} 
\end{equation}
In Eq. (\ref{ind_matrix}), ${\bm{a}}{\bf M}$ essentially produces a $n\times N'$ matrix, where the non-zero entries imply edges between inductive and synthetic nodes. 


Then, for the same support nodes and relay GNN $f(\cdot)$, our objective is to minimize the discrepancy between support nodes' embeddings propagated with the original graph $\mathcal{T}$ and those propagated with the synthetic graph $\mathcal{S}$, which are respectively denoted as ${\mathbf{H}_{sup}}$ and ${\mathbf{H}'_{sup}}$ and derived from  $f(\mathbb{A},\mathbb{X})$ and $f(\mathbb{A}',\mathbb{X}')$.  Therefore, the loss function for this inductive task is defined as:
\begin{equation}
\mathcal{L}_{ind}({\mathbf{H}_{sup}},{\mathbf{H}'_{sup}}, \mathbf{M})=\frac{1}{n} || {{\mathbf{H}_{sup}}} -{{\mathbf{H}'_{sup}}} ||_{2,1}. 
\end{equation}

Finally, the mapping matrix is trained by performing optimization under both transductive and inductive constraints:
\begin{equation}\label{eq:L_M}
\mathcal{L}_{\mathbf{M}} = \mathcal{L}_{tra}(\mathbf{H}, {\mathbf{H}}', {\mathbf{M}})+\beta\mathcal{L}_{ind}({\mathbf{H}_{sup}},{\mathbf{H}'_{sup}}, \mathbf{M}),
\end{equation}
with a hyperparameter $\beta$ controlling both sides' contributions.

\begin{algorithm}[t]
\SetAlgoVlined
\small
\textbf{Input:} Original graph
$\mathcal{T}=\{{\bf A}, {\bf X}, {\bf Y}\}$\\
\textbf{Output:} Synthetic graph
$\mathcal{S}=\{{\bf A}', {\bf X}', {\bf Y}'\}$, mapping $\bf M$\\
Initialize $\bf M$, ${\bf X'}$, and ${\bf Y'}$ \\
\For{$k=1,\ldots,K$}  
{
 $\rhd$  Update synthetic graph\\
Initialize $\boldsymbol{\theta}_0\sim P_{\boldsymbol{\theta}_0}$\\
\For{$t=1,\ldots,T$}  
{        
Compute $\mathcal{L}_{\mathcal{S}}$ with Eq. (\ref{grad_loss})\\
${\bf X'} \leftarrow {\bf X'} -\eta_1 \nabla_{\bf X'} \mathcal{L}_{\mathcal{S}}$\\
${\Phi} \leftarrow {\Phi} -\eta_2 \nabla_{\Phi} \mathcal{L}_{\mathcal{S}}$\\
${\theta}_{t+1}\leftarrow \mathrm{optimizer}_{{\theta}}(\mathcal{L}(\cdot),f_{{\theta}_t}(\cdot),\mathcal{S})$  \\
 }
 $\rhd$  Update mapping matrix \\
\For{$t=1,\ldots,T$}{
Compute $\mathcal{L}_{\mathbf{M}}$ with Eq. (\ref{eq:L_M})\\
${\bf{\mathbf M}} \leftarrow {\bf{\mathbf M}} -\eta_3 \nabla_{\bf{\mathbf M}}\mathcal{L}_{\bf M}$\\
}
}
Sparsify ${\bf A'}$ and ${\bf M}$ with Eq. (\ref{eq:threshold})\\
\caption{Alternating Optimization of MCond}
\label{al}
\end{algorithm}


\subsection{Alternating Optimization and Further Details}\label{sec:aofa}

As per our design of MCond, the synthetic graph and mapping matrix are heavily entangled and can thus interfere with each other during training. 

Therefore, we propose to optimize ${\mathcal{S}}$ and ${\bf{M}}$ in an alternating fashion, which is done by updating ${\mathcal{S}}$ or ${\bf{M}}$ (based on $\mathcal{L}_{\mathcal{S}}$ or $\mathcal{L}_{\mathbf{M}}$) for $T$ iterations at a time with the other side fixed. A summary of the training procedure is provided in Algorithm \ref{al}. Notably, after ${\bf{A}}'$ and ${\bf{M}}$ have been learned, an additional sparsification step is performed on both (line 16) before serving the inductive nodes. The rationale is that, by selectively masking out entries with lower values in ${\bf{A}}'$ and ${\bf{M}}$, we can further minimize the storage overhead with the sparse $N'\times N'$ and $N\times N'$ matrices resulted. The sparsification is facilitated by applying a threshold across each matrix:
\begin{equation}\label{eq:threshold}
{\bf A}_{i,j}'= \begin{cases}
{\bf A}_{i,j}', \text{ if }  {\bf A}_{i,j}' \geq \mu \\
0 \,\,\,\,\,\,\,\,\,\,\text{ otherwise}
\end{cases}\!\!\!\!\!\!,
{\bf M}_{i,j}= \begin{cases}
{\bf M}_{i,j}, \text{ if }  {\bf M}_{i,j} \geq \delta \\
0, \,\,\,\,\,\,\,\,\,\text{ otherwise}
\end{cases}\!\!\!\!\!\!,
\end{equation}
where $\mu$ and $\delta$ are hyperparameters controlling the sparsity of ${\bf{A}}'$ and ${\bf{M}}$, respectively. 
One important note is that, during training, MCond keeps using the dense version of both matrices to ensure efficient and end-to-end gradient-based optimization. In what follows, we present some further design details and discussions about MCond. 

\textbf{Initialization and Normalization of ${\bf{M}}$.} 
The mapping matrix enables each original node to be represented by a weighted ensemble of synthetic nodes, and our empirical findings (see Section \ref{clu_ini} for details) show that original nodes exhibit stronger correlations with synthetic nodes in the same class. Hence, to speed up convergence, we propose a class-aware initialization strategy for ${\bf{M}}$. %the ${\bf{\mathcal M}}$ based on the categories of original and synthetic graph nodes. 
Specifically, for each original node $i$ and its corresponding row $\mathbf{M}_i$ in the matrix, we set $\mathbf{M}_{i,j}$ to a constant (e.g., 1) if synthetic node $j$ has the same class label with it. The class labels for all synthetic nodes are predefined and will stay fixed throughout the training, as described in Section \ref{sec:lbgm}. We set $\mathbf{M}_{i,j} = 0$ if two nodes mismatch in class labels. Leveraging class information for initializing the mapping can effectively  accelerate the learning process. 
At the same time, to guarantee stronger numerical stability for ${\bf{M}}$ and associated computations, we introduce a row-wise normalization step as the following: 
\begin{equation}
\label{map}
{\bf{M}}_{i} \leftarrow \mathrm{ReLU}(\frac{\sigma ({{\bf M}}_i)}{\sum_{j=1}^{N'}\sigma ({\bf{M}}_{i,j})} -\epsilon ),
\end{equation}
where a small constant $\epsilon$ is in use to suppress subtle but noisy weights in the normalized matrix. We find in experiments that this normalization secures convergence, and the normalized (but still dense) mapping $\mathbf{M}$ substitutes its original form for computing the forward pass as per Sections \ref{sec:tpgc}, \ref{sec:fpmf}, \ref{sec:inductive} during training. 

\textbf{Inductive Inference with $\mathcal{S}$ and $\mathbf{M}$.} In MCond, the inference w.r.t. $n$ inductive nodes leverages an identical process as in Eq. (\ref{ind_matrix}), where the only difference is that the support nodes are replaced by the actual unseen nodes for testing.  
On the one hand, compared with using the original graph for inductive inference, MCond reduces the space complexity from $\mathcal{O}(||\mathbb{A}||_0+(N+n)d)$ to $\mathcal{O}(||\mathbb{A}'||_0+(N'+n)d)$. Considering $N'\ll N$, and $||\mathbb{A}'||_0 \ll ||\mathbb{A}||_0$ even without sparsification of $\mathbf{A}'$, the improvement on memory consumption is substantial. On the other hand, MCond also benefits from a significant complexity drop from $\mathcal{O}(L(N+n)^2d)$ to only $\mathcal{O}(L(N'+n)^2d)$, bringing considerable inference speed up.  
