\section{Related Work}
\subsection{Dataset Distillation and Graph Condensation}

The high computation cost of training neural networks on large datasets is a significant concern in the industry. In addition to meticulously designed models, researchers have been exploring methods to reduce training costs through data-centric approaches. Dataset distillation (DD)~\cite{wang2018dataset} was the first proposed method to distill knowledge from a large training image dataset into a smaller set of synthetic samples. However, DD models the distillation process as a complex meta-learning problem. Following the fundamental idea of DD, several works have been proposed to constrain image generation by matching training gradients~\cite{zhao2020dataset}, embedding distributions~\cite{zhao2023dataset}, and training trajectories~\cite{cazenavette2022dataset} with the original images. 
As for graph representation learning,  GCond \cite{jin2022graph} introduced the graph condensation method building upon the gradient matching framework. Additionally, DosCond \cite{jin2022condensing} focuses on the challenge of learning discrete structures and fast optimization, particularly in graph classification. However, these methods can only condense the training data and are unable to handle inductive nodes. 


\subsection{Coreset Selection and Coarsening}
To reduce the size of training data, it is intuitive to explore the application of coreset selection methods. For example, mixture models aim to use Gaussian mixture models to represent the underlying data distribution \cite{lucic2017training}.
Low-rank approximation targets finding a low-rank representation of the original dataset that captures its essential characteristics \cite{cohen2017input}. However, coreset methods may suffer from performance degradation under an extremely small reduction rate.
In contrast, coarsening methods focus on reducing the size of a graph by grouping original nodes into super-nodes and defining their connections. Loukas et al. \cite{loukas2018spectrally} provide conditions such that the principal eigenvalues and eigenspaces of coarsened and original graph Laplacian matrices are close. Another method \cite{loukas2019graph} leverages the property that restricted approximation carries strong spectral and cut guarantees. Bravo-Hermsdorff et al. \cite{bravo2019unifying} propose a method that simultaneously sparsifies and coarsens graphs, preserving their large-scale structure and Laplacian pseudoinverse. However, coarsening methods mainly focus on reducing the size of a graph without significantly altering its basic properties and do not consider the model and task-specific factors. In contrast, our proposed method is built upon the graph condensation framework. Benefiting from all adjacency matrix and node features generated by gradient matching, the synthetic graph generated by graph condensation enables training diverse GNN models and achieves comparable performance to models trained on the large graph.

\subsection{Graph Sparsification}
Graph sparsification is a data-centric approach for accelerating inference by reducing the number of edges in the adjacency matrix. Early methods~\cite{peleg1989graph, karger1999random, spielman2011spectral} approximated the basic graph properties like pairwise distances, cuts, or eigenvalues. More recently, graph lottery ticket methods~\cite{chen2021unified, hui2023rethinking} have been proposed to co-simplify the input graph and GNN weights using learned masks for the adjacency matrix and GNNs and achieve the inference acceleration.  However, these methods do not reduce the number of nodes in the graph, thereby limiting the potential propagation speed of GNNs.

