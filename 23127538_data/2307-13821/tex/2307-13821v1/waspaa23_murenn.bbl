\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\def\UrlFont{\rmfamily}
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{lyon2017human}
R.~F. Lyon, \emph{{Human and machine hearing: Extracting meaning from
  sound}}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge University Press,
  2017.

\bibitem{knight2021cambridge}
R.-A. Knight and J.~Setter, \emph{The Cambridge Handbook of Phonetics}.\hskip
  1em plus 0.5em minus 0.4em\relax Cambridge University Press, 2021.

\bibitem{Glasberg1990gammatone}
B.~R. Glasberg and B.~C. Moore, ``Derivation of auditory filter shapes from
  notched-noise data,'' \emph{Hearing Research}, vol.~47, no.~1, pp. 103--138,
  1990.

\bibitem{brown1991calculation}
J.~C. Brown, ``Calculation of a constant-{Q} spectral transform,'' \emph{J.
  Acoust. Soc. Am.}, vol.~89, no.~1, pp. 425--434, 1991.

\bibitem{dorfler2020basic}
M.~D{\"o}rfler, T.~Grill, R.~Bammer, and A.~Flexer, ``{Basic filters for
  convolutional neural networks applied to music: Training or design?}''
  \emph{Neural Comput. Appl.}, vol.~32, pp. 941--954, 2020.

\bibitem{stowell2022computational}
D.~Stowell, ``Computational bioacoustics with deep learning: a review and
  roadmap,'' \emph{PeerJ}, vol.~10, p. e13152, 2022.

\bibitem{bello2019sonyc}
J.~P. Bello, C.~Silva, O.~Nov, R.~L. Dubois, A.~Arora, J.~Salamon, C.~Mydlarz,
  and H.~Doraiswamy, ``{SONYC}: {A} system for monitoring, analyzing, and
  mitigating urban noise pollution,'' \emph{Communications of the ACM},
  vol.~62, no.~2, pp. 68--77, 2019.

\bibitem{zhao2019deep}
R.~Zhao, R.~Yan, Z.~Chen, K.~Mao, P.~Wang, and R.~X. Gao, ``Deep learning and
  its applications to machine health monitoring,'' \emph{Mechanical Systems and
  Signal Processing}, vol. 115, pp. 213--237, 2019.

\bibitem{bizopoulos2018deep}
P.~Bizopoulos and D.~Koutsouris, ``Deep learning in cardiology,'' \emph{IEEE
  Rev. Biomed. Eng.}, vol.~12, pp. 168--193, 2018.

\bibitem{bravo2021bioacoustic}
F.~J. Bravo~Sanchez, M.~R. Hossain, N.~B. English, and S.~T. Moore,
  ``Bioacoustic classification of avian calls from raw sound waveforms with an
  open-source deep learning architecture,'' \emph{Scientific Reports}, vol.~11,
  no.~1, pp. 1--12, 2021.

\bibitem{faiss2022adaptive}
M.~Fai{\ss}, ``Adaptive representations of sound for automatic insect
  recognition,'' Master's thesis, Naturalis Biodiversity Center, 2022.

\bibitem{lluis2018end}
F.~Llu{\'\i}s, J.~Pons, and X.~Serra, ``End-to-end music source separation:
  {I}s it possible in the waveform domain?'' \emph{arXiv preprint
  arXiv:1810.12187}, 2018.

\bibitem{ravanelli2018speaker}
M.~Ravanelli and Y.~Bengio, ``{Speaker recognition from raw waveform with
  SincNet},'' in \emph{Proc. IEEE SLT}, 2018.

\bibitem{zeghidour2021leaf}
N.~Zeghidour, O.~Teboul, F.~de~Chaumont~Quitry, and M.~Tagliasacchi, ``{LEAF}:
  A learnable frontend for audio classification,'' in \emph{Proc. ICML}, 2021.

\bibitem{sutskever2013importance}
I.~Sutskever, J.~Martens, G.~Dahl, and G.~Hinton, ``On the importance of
  initialization and momentum in deep learning,'' in \emph{Proc. ICML}, 2013,
  pp. 1139--1147.

\bibitem{schluter2022efficientleaf}
J.~Schl{\"u}ter and G.~Gutenbrunner, ``{EfficientLEAF}: A faster learnable
  audio frontend of questionable use,'' in \emph{Proc. EUSIPCO}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2022, pp. 205--208.

\bibitem{zeghidour2018learning}
N.~Zeghidour, N.~Usunier, I.~Kokkinos, T.~Schaiz, G.~Synnaeve, and E.~Dupoux,
  ``Learning filterbanks from raw speech for phone recognition,'' in
  \emph{Proc. IEEE ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018.

\bibitem{ravanelli2021speechbrain}
M.~Ravanelli, T.~Parcollet, P.~Plantinga, A.~Rouhe, S.~Cornell, L.~Lugosch,
  C.~Subakan, N.~Dawalatabad, A.~Heba, J.~Zhong, \emph{et~al.}, ``{SpeechBrain:
  A general-purpose speech toolkit},'' \emph{arXiv preprint arXiv:2106.04624},
  2021.

\bibitem{selesnick2005dual}
I.~W. Selesnick, R.~G. Baraniuk, and N.~C. Kingsbury, ``The dual-tree complex
  wavelet transform,'' \emph{IEEE Signal Proc. Mag.}, vol.~22, no.~6, pp.
  123--151, 2005.

\bibitem{oord2018parallel}
A.~Oord, Y.~Li, I.~Babuschkin, K.~Simonyan, O.~Vinyals, K.~Kavukcuoglu,
  G.~Driessche, E.~Lockhart, L.~Cobo, F.~Stimberg, \emph{et~al.}, ``{Parallel
  WaveNet: Fast high-fidelity speech synthesis},'' in \emph{Proc. ICML}, 2018,
  pp. 3918--3926.

\bibitem{cotter2020uses}
F.~Cotter, ``Uses of complex wavelets in deep convolutional neural networks,''
  Ph.D. dissertation, University of Cambridge, 2020.

\bibitem{moore1983erb}
B.~C.~J. Moore and B.~R. Glasberg, ``{Suggested formulae for calculating
  auditory‐filter bandwidths and excitation patterns},'' \emph{J. Acoust.
  Soc. Am.}, vol.~74, no.~3, pp. 750--753, 09 1983.

\bibitem{necciari2018audlets}
T.~Necciari, N.~Holighaus, P.~Balazs, Z.~Průša, P.~Majdak, and O.~Derrien,
  ``Audlet filter banks: {A} versatile analysis/synthesis framework using
  auditory frequency scales,'' \emph{Applied Sciences}, vol.~8, no.~1, 2018.

\bibitem{schörkhuber2014vqt}
C.~Schörkhuber, A.~Klapuri, N.~Holighaus, and M.~Dörfler, ``A matlab toolbox
  for efficient perfect reconstruction time-frequency transforms with
  log-frequency resolution,'' in \emph{Proc. AES}, 2014.

\bibitem{antoni2010orthogonal}
J.~Antoni, ``Orthogonal-like fractional-octave-band filters,'' \emph{J. Acoust.
  Soc. Am.}, vol. 127, no.~2, pp. 884--895, 2010.

\bibitem{cheuk2020nnaudio}
K.~W. Cheuk, H.~Anderson, K.~Agres, and D.~Herremans, ``{nnAudio: An on-the-fly
  gpu audio to spectrogram conversion toolbox using 1d convolutional neural
  networks},'' \emph{IEEE Access}, vol.~8, pp. 161\,981--162\,003, 2020.

\bibitem{mcfee23librosa}
\BIBentryALTinterwordspacing
B.~McFee, M.~McVicar, D.~Faronbi, I.~Roman, M.~Gover, S.~Balke, S.~Seyfarth,
  A.~Malek, C.~Raffel, V.~Lostanlen, \emph{et~al.}, ``librosa/librosa:
  0.10.0.post2,'' Mar. 2023. [Online]. Available:
  \url{https://doi.org/10.5281/zenodo.7746972}
\BIBentrySTDinterwordspacing

\bibitem{pruvsa2013ltfat}
Z.~Průša, P.~S{\o}ndergaard, P.~Balazs, and N.~Holighaus, ``{LTFAT: A
  Matlab/Octave toolbox for sound processing},'' in \emph{Proc. CMMR}, 2013,
  pp. 299--314.

\bibitem{assmann2000jasa}
P.~F. Assmann and W.~F. Katz, ``Time-varying spectral change in the vowels of
  children and adults,'' \emph{J. Acoust. Soc. Am.}, vol. 108, no.~4, pp.
  1856--1866, 2000.

\bibitem{cella2020icmc}
C.~E. Cella, D.~Ghisi, V.~Lostanlen, F.~L{\'e}vy, J.~Fineberg, and Y.~Maresz,
  ``{OrchideaSOL: A dataset of extended instrumental techniques for
  computer-aided orchestration},'' in \emph{Proc. ICMC}, 2020.

\bibitem{cartwright2019sonyc}
M.~Cartwright, A.~E. Mendez~Mendez, G.~Dove, J.~Cramer, V.~Lostanlen, H.-H. Wu,
  J.~Salamon, O.~Nov, and J.~P. Bello, ``{SONYC Urban Sound Tagging
  (SONYC-UST): A multilabel dataset from an urban acoustic sensor network},''
  in \emph{Proc. DCASE}, 2019.

\bibitem{anderson2023learnable}
M.~Anderson, T.~Kinnunen, and N.~Harte, ``Learnable frontends that do not
  learn: {Q}uantifying sensitivity to filterbank initialisation,'' in
  \emph{Proc. IEEE ICASSP}, 2023.

\bibitem{mallat1999wavelet}
S.~Mallat, \emph{A wavelet tour of signal processing}.\hskip 1em plus 0.5em
  minus 0.4em\relax Elsevier, 1999.

\bibitem{gröchenig01tfana}
K.~{G}r{\"o}chenig, \emph{{F}oundations of time-frequency analysis}, ser.
  {A}ppl. {N}umer. {H}armon. {A}nal.\hskip 1em plus 0.5em minus 0.4em\relax
  {B}oston, {M}{A}: {B}irkh{\"a}user, 2001.

\end{thebibliography}
