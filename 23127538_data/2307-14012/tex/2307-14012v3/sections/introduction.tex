\section{Introduction}

Significant advancements have recently been achieved in generative modelling across various domains~\citep{brock2018large, brown2020language, ho2020denoising}.
These models have become potent priors for a wide range of applications, including code generation~\cite{li2022competition}, text-to-image generation \cite{saharia2022photorealistic}, question-answering \cite{brown2020language}, and many others \cite{gungor2023adaptive, wynn2023diffusionerf}.
Among the generative models, diffusion models~\citep{sohl2015deep, song2019generative, ho2020denoising} have arguably emerged as the most powerful class.
Diffusion models learn to denoise corrupted inputs in small, gradual steps and are capable of generating samples from complex distributions. They have been successful in many domains, such as generating highly realistic images~\citep{dhariwal2021diffbeatgan}, modeling temporal point processes~\citep{ludke2023diff_tpp} and even generating neural network parameters~\citep{wang2024neural}.

Diffusion models also offer the capability of composed sampling, which combines pre-trained models to generate samples from a new distribution.
This approach, known as model composition, has a rich history~\citep{jacobs1991adaptive, hinton2002training, mayraz2000recognizing, liu2022compositional}.
For diffusion models, the most common form of composition is classifier-guided sampling, where the reverse process is augmented by a separate classifier model~\citep{sohl2015deep, dhariwal2021diffbeatgan, ho2021classifierfree}, but other compositions have also been explored~\citep{du2023reduce}.
The ability to compose new models without having to re-learn the individual components is especially appealing for diffusion models since their ever-increasing size and data hunger make them exceedingly costly to train~\citep{aghajanyan2023scaling}.
Therefore, developing sampling methods that work for pre-trained diffusion models is valuable.

% Score-based
The foundation of composed sampling for diffusion models is score-based, where we interpret diffusion models as predictors of the score function for the marginal distribution at each diffusion step~\citep{song2021scorebased}. From this perspective, MCMC methods, such as the Langevin algorithm (LA)~\citep{roberts2002mala} or Hamiltonian Monte Carlo (HMC) sampling~\citep{duane1987hmc}, emerge as viable options to incorporate. Augmenting the standard reverse process with additional MCMC sampling has been shown to improve composed sampling for diffusion models~\citep{du2023reduce, song2021scorebased}. However, we are restricted to unadjusted variants of these samplers, namely Unadjusted LA (U-LA) and Unadjusted HMC (U-HMC), which only require utilization of the score. This limitation means we cannot incorporate a Metropolis-Hastings (MH) correction step~\citep{metropolis1953mh,hasting1970mh}, which requires evaluating the unnormalized density.

An intriguing alternative to directly modeling the score function is to model the marginal distribution with an energy function, from which the score can be obtained through explicit differentiation~\citep{salimans2021should, song2019generative}. This parameterization connects diffusion models and energy-based models (EBMs)~\citep{lecun2006tutorial} and offers several desirable properties. With an energy parameterization, we can evaluate the unnormalized density and guarantee a proper score function. This, in turn, enables an MH correction step when employing an MCMC-method, where the MH acceptance probability is computed from the energy function. Adding such a correction step has been shown to improve sampling performance in composed models~\citep{du2023reduce}. Nevertheless, the score parameterization remains far more popular, as it avoids the direct computation of the gradient of the log density.

In this study, we build on the work in~\citep{du2023reduce} and introduce a novel approach to obtain an MH-like correction step directly from pre-trained diffusion models without relying on an energy-based parameterization. Specifically, we use a connection between the score and the energy to estimate the MH acceptance probability by approximating a line integral along the vector field generated by the score. This enables an improved sampling procedure for various pre-trained score-parameterized diffusion models. We find that our approximate method quantitatively results in improvements comparable to the energy parameterization without having to estimate the energy directly.

In summary, our main contributions are:
\begin{itemize}
\item We show that MH-like correction sampling can be directly applied to score-based models without requiring additional training.
\item We introduce two efficient algorithms to approximate the energy difference used in MH and demonstrate that our pseudo-energy difference more accurately represents analytical energy differences than an explicitly trained energy model in a toy example while performing on par with the energy model on MNIST.
\item We establish that the sampling accuracy improvements achieved with MCMC for energy-based models can also be attained for score-based models while offering superior runtime performance.
\end{itemize}
