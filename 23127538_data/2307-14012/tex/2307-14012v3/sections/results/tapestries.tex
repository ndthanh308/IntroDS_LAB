\subsection{Image Tapestry}

As our final experiment, we conduct an image tapestry experiment, similar to \cite{du2023reduce} and based on their code\footnote{\url{https://github.com/yilundu/reduce_reuse_recycle}}. The goal is to generate a coherent image composed of spatially localized content, each region conditioned on different prompts. This task involves both classifier-free guidance and model compositionâ€”specifically, the combination of multiple overlapping text-to-image diffusion models, each responsible for a portion of the scene.

We use a pre-trained DeepFloyd-IF model\footnote{\url{https://huggingface.co/DeepFloyd/IF-I-XL-v1.0}} as the base diffusion model. To refine the generated samples, we apply Langevin dynamics with our MH-like correction. For each diffusion step ($T = 100$), we include 15 additional Langevin steps. The pseudo-energy difference is approximated via line integration using three additional evaluation points per step. We set the classifier-free guidance scale to $\lambda = 20.0$.

The resulting image is presented in Figure~\ref{fig:tapestry}, which showcases the generated tapestry with different regions displaying distinct visual content. Figure~\ref{fig:tapestry_content} provides a schematic overview of the used prompts and their spatial layout. In total, nine content regions are specified: four located in the corners of the image, each with unique prompts, and five overlapping in the center, all guided by the same prompt to create a unified visual theme.
% Figure environment removed
