\subsection{Guided Diffusion} \label{section:guideddiffusion}

We evaluate our proposed sampling methods for guided diffusion on the CIFAR-100~\citep{krizhevsky2009cifar} and ImageNet~\citep{deng2009imagenet} datasets. The sampling process is based on a score function defined in (\ref{eq:guidance:score}). For both datasets, the marginal score, $\gradX \log q(\x_t)$, is estimated using an unconditional diffusion model parameterized by a UNet architecture. For the guidance model, we use classifier-full guidance, training a time-dependent classifier to predict class labels across all diffusion steps, $\pCfull(y \mid \x_t, t)$. This classifier shares its architecture with the encoder part of the UNet used for the diffusion model and is extended with a dense output layer. The guidance scale is set to $\lambda = 20.0$ across all experiments. Sampling is based on the standard reverse process with $T=1000$, and additional MCMC steps are incorporated to refine the generated samples.

To quantify generation quality, we use three evaluation metrics: the Fréchet Inception Distance (FID)~\citep{heusel2017gans}, which compares the distribution of generated and real images; classification accuracy, based on a separate pre-trained classifier applied to generated samples; and, for ImageNet, an additional top-5 accuracy metric.

\textbf{CIFAR-100:}  
For CIFAR-100, we trained the diffusion models from scratch using the same UNet architecture and training settings as in \cite{ho2020denoising}, which were originally designed for CIFAR-10~\citep{krizhevsky2009cifar}. The MCMC samplers add $\mcSteps = 2$ or $6$ extra MCMC steps at each diffusion step $t$ for (U-)HMC and (U-)LA, respectively, with (U-)HMC using three leapfrog steps per MCMC step. 

For this experiment, more points are needed in the trapezoidal rule’s mesh than in the 2D experiment. Based on previous insights, for HMC we integrate only along the curve from the leapfrog steps, with an additional midpoint evaluation, resulting in three extra model evaluations per HMC step. For LA, we use ten points along the line, resulting in eight extra evaluations per step.

Recognizing the impact of the step length on MCMC methods in general, we parameterize the step length as a function of the beta-schedule $\langStep_t = a \beta_t^b$. We conducted a simple parameter search for parameters $a$ and $b$, to determine a suitable step length for each MCMC variant.

The results are shown in Table~\ref{tab:exp:cifar100}. Average accuracy is obtained using a separate classifier trained exclusively on noise-free pairs $(\x_0, y)$, following the VGG-13-BN architecture~\citep{simonyan2014very}. The table shows a general trend of improvement over the baseline reverse process when additional MCMC steps are added. In particular, the MH-corrected samplers LA and HMC show significant improvements in FID scores, which are arguably the more important metric for image generation.

Comparing the score and energy parameterizations, their performances share similar characteristics. Interestingly, the reverse process favors the score parameterization, supporting the claim that this less restricted approach better models the score function. However, the energy parameterization sees larger improvements from the added MCMC steps. This indicates, perhaps, that direct energy estimation provides a better correction step compared to our method of approximating the pseudo-energy difference from $\epsP$. Although the energy-based method performs slightly better in this setting, our MH-corrected sampling methods achieve comparable improvements without requiring an energy model.

\begin{table}
\centering
\caption{Accuracy and FID score for classifier-full guidance on CIFAR-100. The metrics are based on 50k generated samples for each sampling method with both energy and score models.}
\begin{tabular}{|c|c|c|c|}
\hline
 & Sampler & Accuracy [\%]\textuparrow & FID\textdownarrow \\ \hline
\multirow{5}{*}{Energy} 
& Reverse   & 72.6 & 33.4 \\
& U-LA      & $\pmb{87.3}$ & 24.6 \\
& LA        & 80.0 & 12.7 \\
& U-HMC     & 87.2 & 25.4 \\
& HMC       & 84.9 & $\pmb{12.4}$ \\
\hline
\multirow{5}{*}{Score}
& Reverse   & $74.2$ & $31.8$ \\
& U-LA      & $\pmb{82.9}$ & $25.9$\\
& LA-8L     & $75.2$ & $15.5$ \\
& U-HMC     & $79.0$ & $28.6$ \\
& HMC-3C    & $75.8$ & $\pmb{13.3}$ \\
\hline
\end{tabular}
\label{tab:exp:cifar100}
\end{table}

\textbf{ImageNet:}  
For ImageNet, training diffusion models from scratch is computationally expensive, so we rely on pre-trained models. Score-based models are publicly available through the OpenAI GitHub repository\footnote{\url{https://github.com/openai/guided-diffusion}}, as provided by \cite{dhariwal2021diffbeatgan}. Unfortunately, no equivalent pre-trained energy-based models are available. Given the high computational demands of large-scale diffusion models, we focus solely on evaluating HMC and compare it to the reverse process. The HMC sampler adds $\mcSteps=2$ MCMC steps per diffusion step $t$, with each step consisting of three leapfrog steps. For the trapezoidal rule, we incorporate the points from the leapfrog steps and add two additional points between each leapfrog step. The step length parameterization and tuning follow the same procedure as in CIFAR-100.

The results can be seen in Table~\ref{tab:exp:imagenet}. Accuracy metrics are computed using a pre-trained RegNetX-8.0GF \cite{radosavovic2020designing} classifier. The reverse process and HMC perform very similarly in average accuracy, but our method shows a slight improvement in top-5 average accuracy. HMC obtains a significantly better FID score.

\begin{table}
\centering
\caption{Average accuracy, top-5 accuracy, and FID score for classifier-full guidance on ImageNet. The metrics are based on 50k generated samples for both sampling methods with score parameterizations.}
\begin{tabular}{|c|c|c|c|c|}
\hline
 & Sampler & Acc [\%]\textuparrow & Acc-5 [\%]\textuparrow & FID\textdownarrow \\ \hline
\multirow{2}{*}{Score}
& Reverse   & $\pmb{50.0}$ & $83.9$ & $14.5$ \\
& HMC-6C    & $49.9$ & $\pmb{85.1}$ & $\pmb{11.6}$ \\
\hline
\end{tabular}
\label{tab:exp:imagenet}
\end{table}
