\section{Background}

\subsection{Diffusion Models}

We consider Gaussian diffusion models initially proposed by \cite{sohl2015deep} and further improved by \cite{song2019generative} and \cite{ho2020denoising}. Starting with a sample from the data distribution $\x_0 \sim q(\cdot)$, we construct a Markov chain of latent variables $\x_1,\ldots,\x_T$ by iteratively introducing Gaussian noise to the sample
\begin{align}
    q(\x_t|\x_{t-1}) = \normal{\x_t; \sqrt{1 - \beta_t} \x_{t-1}, \beta_t I},
\end{align}
where $\beta_t \in [0, 1), \, \forall t = 1, \dots, T$ are assumed to be known.
For large enough $T$ is $q(\x_T) \approx \normal{0, I}$.
When the magnitude $\beta_t$ of the noise added at each step is sufficiently small, the posterior (``reverse'') distribution $q(\x_{t-1}|\x_t)$ can be effectively approximated by a Gaussian distribution.
Therefore, it is reasonable to model the reverse distribution as
\begin{align}
    p_\theta(\x_{t-1}|\x_t) = \normal{\mu_\theta(\x_t, t), \betaTilde_t I}.
\end{align}
where the mean of the distribution is parameterised as 
\begin{align}
    \mu_\theta(\x_t, t) &= \dfrac{1}{\sqrt{\alpha_t}} \left( \x_t - \dfrac{\beta_t}{\sqrt{1 - \alphaBar_t}} \epsilon_\theta(\x_t, t) \right).
%     \Sigma_\theta(\x_t, t) &= \hat{\beta_t}.
\end{align}
The parameters $\alpha_t, \alphaBar_t$ are functions of $\{ \beta_t \}_{t=1}^T$ while $\betaTilde_t$ is treated as a hyperparameter.
The noise prediction model $\epsilon_\theta(\x_t, t)$ is typically parameterised as a neural network and learned from data.
We refer to \cite{ho2020denoising} for a detailed derivation.

% Add
% - Training loss
% - Sampling
\subsection{Energy-based models}
An energy-based model (EBM), models a distribution
\begin{equation}
  \label{eq:ebm}
  \pEbm(\x_t, t) = \frac{1}{\partFn(t)} \exp \left(- \energy(\x_t, t) \right),
  \quad \partFn(t) = \int \exp \left(- \energy(\x_t, t) \right) \dd \x_t,
\end{equation}
where $\param$ are the model parameters, $\energy(\x_t, t)$ the energy function
and $\partFn$ a normalising constant, ensuring that $\pEbm$ is a proper probability distribution.
Here, we introduce an EBM for each diffusion step, since we will later see how they can be connected.

The normalisation constant $\partFn$ is typically intractable, prohibiting computing a normalised density.
However, $\partFn$ does not depend on the input $\x_t$ making the score of an EBM easy to compute:
\begin{equation}
  \label{eq:ebm:score}
  \gradX \log \pEbm(\x_t, t) = - \gradX \energy(\x_t, t), 
\end{equation}
at least in principle, but the gradient of the energy function can be costly to compute in practice.

\subsection{MCMC-sampling}
We cannot typically sample from generative models directly, and it is common to resort to some form of MCMC-sampling.
The Unadjusted Langevin Algorithm (U-LA) and Unadjusted Hamiltonian Monte Carlo (U-HMC) are common choices when we only have access to the score function \cite{neal1996bayes}.
For example, with U-LA we use the kernel at diffusion step $t$
\begin{align}
    \mcKern{\x^{\tau+1} \mid \x^{\tau}} 
    = \normal{\x^{\tau + 1}; \x^{\tau} + \frac{\langStep_t^2}{2} \gradX \log \pEbm(\x^{\tau}, t), \langStep_t^2 I },
\end{align}
where $\x^0 = \x_t$, $\langStep_t$ is the step size, and the chain is iterated for a finite $\mcSteps$ steps.

As $\mcSteps$ grows, the Langevin samples will converge to the target distribution, but only for infinitesimal step sizes $\langStep_t$.
The error stemming from using a larger step size can be handled with an extra Metropolis--Hastings (MH) correction \cite{metropolis1953mh,hasting1970mh}.
With the correction, we sample a candidate $\xCand \sim \mcKern{\cdot \mid \x^\tau}$ and accept it as the new iterate with probability
\begin{align}
    \label{eq:mh:acc_prob}
    \acc = \min \left( 1, \frac{\pEbm(\xCand, t)}{\pEbm(\x^\tau, t) } \frac{\, \mcKern{\x^\tau \mid \xCand}}{ \mcKern{\xCand \mid \x^\tau }} \right).
\end{align}
That is, we set the new iterate $\x^{\tau+1} = \xCand$ with probability $\acc$, otherwise $\x^{\tau+1} = \x^\tau$.
Note that the model $\pEbm$ appears in the acceptance probability as a ratio meaning that we do not require a normalised density to compute $\acc$, since the normalisation constant cancels out.
In terms of an EBM, we have
\begin{align}
    \label{eq:mh:rel_energy}
    \frac{\pEbm(\xCand, t)}{\pEbm(\x^\tau, t) }
    % = \frac{ \exp( -\energy(\xCand, t)) }{ \exp( -\energy(\x^\tau, t)) }
    = \exp( \energy(\x^\tau, t) -\energy(\xCand, t) ).
\end{align}

\subsection{Improved sampling in diffusion models}

The foundation of this study is that the noise prediction model $\epsP(\x_t, t)$ can be interpreted as estimating a quantity proportional to the score of the marginal distribution at diffusion step $t$ \cite{song2021scorebased}
\begin{align}
  \label{eq:score_noise_connection}
  \epsP(\x_t, t) = -\sigma_t \gradX \log \pEbm(\x_t).
\end{align}

With this interpretation of the noise prediction model, we see that we can parameterise the diffusion model in two ways: the \textit{score parameterisation} models the score directly via $\epsP$ while the \textit{energy parameterisation} models the energy function $\energy$, obtaining the score implicitly through differentiation as in \cref{eq:ebm:score}.

The energy parameterisation has appealing properties.
By modelling the energy function, we evaluate the density $\pEbm(\x_t, t)$ up to a normalisation $\partFn(t)$, which enables various MCMC-methods, such as an MH correction step.
Furthermore, by making the score equal to the gradient of an actual scalar function, we ensure a proper score, i.e., a conservative vector field.

Despite these advantages, the score parameterisation is more commonly used.
Partly because of the extra step required to compute the gradient of the energy function and partly because it seems to produce better results when sampling \cite{du2023reduce}.
Recent work has suggested that this performance difference is mainly due to model architecture \cite{salimans2021should}, while subsequent experiments still show an advantage for the score parameterisation \cite{du2023reduce}.
Regardless, the energy parameterisation still requires the extra gradient calculation to compute the score as well as preventing the use of pre-trained diffusion models, which are typically modelled with the score parameterisation.

\subsection{Model composition}
Augmenting the reverse process with additional MCMC sampling appears to be important in model composition \cite{du2023reduce}.
They investigate three forms of compositions: products, mixtures, and negations.
For example they model a product distribution as
\begin{equation}
    \qProd(\x_0) \propto \prod_i q^i(\x_0).
\end{equation}
While this factorisation only holds at $t=0$, they form a composed model
\begin{equation}
    \label{eq:comp:prod:ebm}
    \pProd(\x_t, t) \propto \prod_i \pEbm^i(\x_t, t)
    = \exp \left( - \sum_i \energy^i(\x_t, t) \right)
\end{equation}
and perform MCMC-sampling with this as the target distribution, with a resulting increase in sampling performance.