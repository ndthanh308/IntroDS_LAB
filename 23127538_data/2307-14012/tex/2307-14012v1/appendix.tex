\section{Experimental details} \label{sec:expdetails}

The composed distribution is defiend by a product of two components, a Gaussian mixture and a uniform distribution with non-zero values on
\begin{align} \label{eq:bar}
    \square = \{ x \in \mathbb{R}^2: -s_i \leq x_i \leq s_i, i = 1,2 \},
\end{align}
where $s_1$ and $s_2$ are equal to $0.2$ and $1.0$, respectively. The eight modes of the Gaussian mixture are evenly distributed on a circle with radius of 0.5 at the angles $2 \pi i$ for $i = 0, \ldots, 7$, respectively. The covariance matrix at each mode is $0.03^2 \cdot I$, where $I$ is the identity matrix.

% log.-likelihood or log-likelihood?
The metric log-likelihood is ill-defined as we may generate samples where the true distribution has no support (due to the uniform distribution). We address this problem by expanding the definition set of the uniform distribution and redistributing one percent of the probability mass into this extended region. The whole set is defined as \eqref{eq:bar} except $s_1=s_2=1.1$. Note 99 percent probability mass remains inside original definition set $\square$.

We use the same neural network architectures as the base for both the score and energy models. It is a residual network consisting of a linear layer (dim $2 \rightarrow 128$) followed by four blocks, and concluding with a linear layer (dim $128 \rightarrow 2$). Within each block, the input $x$ passes through a normalization layer, a SiLU activation, and a linear layer (dim $128 \rightarrow 256$). Subsequently, it is added with an embedded $t$ (dim 32) that has undergone a linear layer transformation (dim $32 \rightarrow 256$). The resulting sum passes through a SiLU activation and is further processed by a linear layer (dim $256 \rightarrow 256$). After that, another SiLU activation is applied, followed by a final linear layer (dim $256 \rightarrow 128$). The output of this linear layer is then added to the original input $x$ within the block. The embedding of $t$ is also learnable. 

The diffusion process is defined with $T$ timesteps set to 100. The parameter $\beta_t$ follows the cosine schedule proposed in \cite{nichol2021improved}, and the hyperparameter $\betaTilde_t$ is calculated as $\beta_t \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}$, as mentioned in \cite{ho2020denoising}, where $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ and $\alpha_t = 1 - \beta_t$. Additionally, $\sigma_t$, in \eqref{eq:score_noise_connection}, is set to $\sqrt{1 - \bar{\alpha}_t}$. For the different MCMC methods, the parameters can be found in \cref{tab:mcmcparam}.
\begin{table}[]
    \centering
    \caption{Parameter values for the different MCMC-methods.}
  \begin{tabular}{ | c | c | c |}
    \hline
    MCMC & Parameter & Value \\ \hline
    \multirow{5}{*}{(U-)HMC}  & \# steps per $t$ & 10 \\ \cline{2-3} 
    & \# leapfrogs & 3 \\ \cline{2-3}
    & Step size $\forall t$ & 0.03 \\ \cline{2-3}
    & Damping-coefficient & 0.5 \\ \cline{2-3}
     & Mass-diagonal-matrix & 1 \\ \hline
    \multirow{2}{*}{(U-)LA } & \# steps per $t$ & 10 \\ \cline{2-3} & Step size $\forall t$ & 0.001 \\ \hline
  \end{tabular}
  \label{tab:mcmcparam}
\end{table}
