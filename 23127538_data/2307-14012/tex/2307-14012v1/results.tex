\section{Results}
% \subsection{2D example}
We compare our method with existing works by repeating the 2D product composition example from \cite{du2023reduce}. The experimental setup is identical to theirs, unless otherwise specified. Further details are given in \cref{sec:expdetails}. All code is available at GitHub\footnote{https://github.com/jackonelli/mcmc\_corr\_score\_diffusion}.

A pair of simple 2D densities are composed by multiplication into a more complex distribution, as in \cref{eq:comp:prod:ebm}.
The aim is to draw samples from the composed distribution, using only diffusion models trained on the component distributions independently. We consider a Gaussian mixture with 8 modes evenly distributed along a circle, as well as a uniform distribution that covers two modes of the Gaussian mixture. For a visual representation the two individual distributions and their resulting product distribution together with samples from the reverse diffusion and HMC corrected samples, see \cref{fig:exp:2d_toy}.
% Figure environment removed

We train model components to represent the individual distributions.
Both score and energy parameterised models are trained with the standard diffusion loss \cite{ho2020denoising}.
The energy-based models are parameterised as in \cite{du2023reduce}
\begin{equation}
    \label{eq:ebm:param}
    \energy(\x_t, t) = \twoNorm{\score(\x_t, t)}^2,
\end{equation}
where $\score$ is a vector valued output of a neural network, with the same dimension as $\x_t$. The score-based $\epsilon_\theta$ has identical network architecture as $s_\theta$.

The trained models are then fixed and we test various schemes to sample from the composed distribution.
The baseline for the experiments is the standard reverse diffusion process with $T=100$ diffusion steps.
The MCMC versions add an extra sampling proceduce at each diffusion step $t$, refining the samples of the reverse process.
The MCMC sampling is run for $\mcSteps = 10$ at each $t$.

We consider three metrics to evaluate performance. The first metric is refered to as log-likelihood (LL), where we generate samples from the model distribution and compute their likelihood under the true data distribution, following \cite{du2023reduce}. A problem, however, is that we may sample points where the true distribution has no support. It is not entirely clear how this issue is handled in \cite{du2023reduce}, but we address it by adding a small uniform probability to extend the support to the sampled points. Full details are provided in the appendix \ref{sec:expdetails}. 

The second metric is referred to as Gaussian mixture model (GMM). In this metric, we sample from both the true distribution and the data distribution. Then, we fit two bi-modal GMMs to each set of samples and compute the mean difference of variances using the Frobenius norm.

Finally, we supplement the metrics by utilizing the Wasserstein 2-distance ($W_2$) to quantify a measure between the data and model distribution \cite{villani2009opttransp}. Again, we draw samples from both the data and model distributions and compute $W_2$ by finding the optimal assignment between the two sampled sets. 

The results are averaged over 10 runs ($\pm$ standard deviation) including both training process and sample generation with different MCMC-methods. In each run 2000 points are generated for each method. The results are compiled in \cref{tab:exp:2d_toy}, where the number associated with LA and HMC corresponds to the number of points employed in the trapezoidal rule's mesh. Finally, \textit{line} and \textit{curve} refer to different methods of integration. Line corresponds to integration along a straight line, while curve represents integration along a curve that incorporates the internal points of MCMC-method (in this case HMC). The results demonstrate the better  performance of the adjusted sampling methods compared to the unadjusted approaches. Additionally, Hamiltonian achieves better results than Langevin, while the reverse process shows less favorable outcomes. Score and energy parameterizations exhibit similar overall performance in LL and GMM within their respective sampling procedures. However, when paired with HMC, the score parameterization significantly outperforms in terms of Wasserstein 2-distance. Further, we note that performance plateaus when using at least four points in the trapezoidal rule.
\begin{table}[]
\centering
\caption{Quantitative results on 2D composition. The mean and standard deviation for the LL, $W_2$, and GMM metrics are computed for the product compositions in \cref{fig:exp:2d_toy}, for the different variants of the score and energy parameterised models. The number connected to LA and HMC for the score describes the number of points utilized in the trapezoid rule. In this context, "line" and "curve" denote the chosen integration methods: "line" signifies integration along a linear path, while "curve" involves integrating along the trajectory formed by exploratory points utilized in the MCMC method to propose new points---in this case the leapfrogs in HMC. The results indicate that the adjusted sampling method outperforms the unadjusted approach, and additionally, Hamiltonian achieves better results than Langevin, while the reverse process performs less favorably. We note that both the score and energy parameterizations show overall similar performance in LL and GMM within their respective sampling procedures. However, the score parameterization shows significantly better performance in terms of Wasserstein 2-distance with HMC. 
}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{Sampler} & \multicolumn{3}{c|}{Product} \\ \cline{3-5}
                  & & LL\textuparrow & $W_2$\textdownarrow & GMM \textdownarrow \\ \hline
\multirow{5}{*}{Energy} 
& Reverse		 & $-8.22 \pm 0.21$ & $5.81 \pm 0.19$ & $0.02701 \pm 0.00134$ \\
& U-LA		 & $-7.52 \pm 0.22$ & $4.19 \pm 0.45$ & $0.01461 \pm 0.00135$ \\
& LA		 & $-6.50 \pm 0.30$ & $4.24 \pm 0.55$ & $0.01466 \pm 0.00146$ \\
& U-HMC		 & $-5.72 \pm 0.18$ & $4.19 \pm 1.25$ & $0.00653 \pm 0.00091$ \\
& HMC		 & $\pmb{-4.09 \pm 0.14}$ & $\pmb{4.12 \pm 1.44}$ & $\pmb{0.00333 \pm 0.00065}$ \\
\hline
\multirow{10}{*}{Score}
& Reverse		 & $-8.15 \pm 0.24$ & $5.80 \pm 0.20$ & $0.02688 \pm 0.00120$ \\
& U-LA		 & $-7.57 \pm 0.12$ & $4.44 \pm 0.63$ & $0.01499 \pm 0.00062$ \\
& LA-3-line		 & $-6.45 \pm 0.20$ & $4.03 \pm 0.52$ & $0.01428 \pm 0.00107$ \\
& LA-5-line		 & $-6.61 \pm 0.17$ & $4.22 \pm 0.46$ & $0.01519 \pm 0.00092$ \\
& LA-10-line		 & $-6.53 \pm 0.17$ & $4.20 \pm 0.51$ & $0.01475 \pm 0.00091$ \\
& U-HMC		 & $-5.77 \pm 0.12$ & $3.39 \pm 0.77$ & $0.00690 \pm 0.00071$ \\
& HMC-3-line	 & $-4.29 \pm 0.13$ & $2.92 \pm 1.02$ & $0.00372 \pm 0.00061$ \\
& HMC-4-curve		 & $\pmb{-4.07 \pm 0.12}$ & $2.94 \pm 0.90$ & $\pmb{0.00306 \pm 0.00054}$ \\
& HMC-5-line		 & $\pmb{-4.07 \pm 0.13}$ & $\pmb{2.68 \pm 1.20}$ & $0.00308 \pm 0.00069$ \\
& HMC-10-line		 & $\pmb{-4.07 \pm 0.14}$ & $2.87 \pm 0.89$ & $0.00317 \pm 0.00056$ \\
\hline
\end{tabular}
\label{tab:exp:2d_toy}
\end{table}
