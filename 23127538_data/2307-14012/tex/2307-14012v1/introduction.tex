\section{Introduction}
% - Why diffusion is good
% - Why diffusion is bad
% - Score-based modelling
% - EBM-parameterisation
% - Poor performance
% - Our solution
Significant advancements have been achieved in generative modeling across various domains in recent years \cite{brock2018large, brown2020language, ho2020denoising}. These models have now become potent priors for a wide range of applications, including code generation \cite{li2022competition}, text-to-image generation \cite{saharia2022photorealistic}, question-answering \cite{brown2020language}, and many others. Among the generative models, diffusion models \cite{sohl2015deep, song2019generative, ho2020denoising} have arguably emerged as the most powerful class in the vision domain, capable of generating highly realistic images \cite{dhariwal2021diffusion}.

Diffusion models also offer the capability of ``guided'' sampling, which combines pre-trained models in order to generate samples from a new distribution \cite{sohl2015deep, dhariwal2021diffusion, ho2021classifierfree}. This process falls under the umbrella of model composition, which is itself a topic with rich history \cite{jacobs1991adaptive, hinton2002training, mayraz2000recognizing, liu2022compositional}. The ability to compose new models without having to re-learn the individual components is especially appealing for diffusion models, since their ever increasing size and data hunger make them exceedingly costly to train \cite{aghajanyan2023scaling}.

The foundation of guided sampling is score-based diffusion, where we interpret the diffusion model as a predictor of the score for the marginalized distribution at each step of diffusion \cite{song2021scorebased}. Building upon this understanding, an intriguing alternative emerges for parameterizing the model to directly predict the score by instead predicting the energy, from which the score can be obtained through explicit differentiation \cite{salimans2021should, song2019generative}. This form makes a connection between diffusion models and energy-based models (EBMs) and it has some desireable properties. With the energy parameterisation we can evaluate the (unnormalised) density and are guaranteed a proper score function. Still, the more common score parameterisation is by far the most popular, as it avoids direct computation of the gradient of the log density. There is also some emiripical evidence for the score parameterisation to perform better \cite{du2023reduce}.

Sampling from diffusion models when only having access to the score has its limitations. We are restricted to sampling methods that do not require the density, such as unadjusted Langevin sampling and Hamiltonian Monte Carlo. These will only sample from the target distribution in the limit of infinitesimal step length. Based on this, \cite{du2023reduce} argue for the energy parameterisation, since we can use the energy to include a correction, such as a Metropolis--Hastings (MH) acceptance probability \cite{metropolis1953mh,hasting1970mh}. They claim that this is particularly important when considering model compositions, since for many forms of compositions, the composed score cannot be easily estimated from the score of the model components. 

In this study, we build on the work in \cite{du2023reduce} and introduce, to the authors' knowledge, a novel approach to obtain a form of MCMC-like correction directly from pre-trained diffusion models without relying on an energy-based parameterisation. Instead, we estimate the MH acceptance probability by approximating a line integration along the vector field generated by the composed diffusion models. This enables more and correct MCMC-methods, without having to re-train a tailored (possibly less performant) energy parameterisation. 

We approximate the line integration using the trapezoidal rule and validate the effectiveness of our approach by conducting a 2D experiment and compare it with an energy-based parameterisation.
We find that our method quantitatively perform similarly to the energy parameterisation on synthetic data, even out-performing it in one metric.