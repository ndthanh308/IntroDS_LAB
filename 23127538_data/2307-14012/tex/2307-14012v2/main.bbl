\begin{thebibliography}{10}

\bibitem{aghajanyan2023scaling}
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan,
  Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer.
\newblock Scaling laws for generative mixed-modal language models.
\newblock In {\em Proceedings of the 40th International Conference on Machine
  Learning}, ICML'23. JMLR, 2023.

\bibitem{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{chung2023reconstrguidance}
Hyungjin Chung, Jeongsol Kim, Michael~Thompson Mccann, Marc~Louis Klasky, and
  Jong~Chul Ye.
\newblock Diffusion posterior sampling for general noisy inverse problems.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dhariwal2021diffbeatgan}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat {GAN}s on image synthesis.
\newblock {\em Advances in neural information processing systems},
  34:8780--8794, 2021.

\bibitem{du2023reduce}
Yilun Du, Conor Durkan, Robin Strudel, Joshua~B. Tenenbaum, Sander Dieleman,
  Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will~Sussman Grathwohl.
\newblock Reduce, {R}euse, {R}ecycle: Compositional generation with
  energy-based diffusion models and {MCMC}.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of {\em Proceedings
  of Machine Learning Research}, pages 8489--8510. PMLR, 23--29 Jul 2023.

\bibitem{duane1987hmc}
Simon Duane, A.D. Kennedy, Brian~J. Pendleton, and Duncan Roweth.
\newblock Hybrid {M}onte {C}arlo.
\newblock {\em Physics Letters B}, 195(2):216--222, 1987.

\bibitem{gungor2023adaptive}
Alper G{\"u}ng{\"o}r, Salman~UH Dar, {\c{S}}aban {\"O}zt{\"u}rk, Yilmaz
  Korkmaz, Hasan~A Bedel, Gokberk Elmas, Muzaffer Ozbey, and Tolga {\c{C}}ukur.
\newblock Adaptive diffusion priors for accelerated mri reconstruction.
\newblock {\em Medical Image Analysis}, page 102872, 2023.

\bibitem{hasting1970mh}
W.~K. Hastings.
\newblock {M}onte {C}arlo sampling methods using {M}arkov chains and their
  applications.
\newblock {\em Biometrika}, 57(1):97--109, 1970.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural computation}, 14(8):1771--1800, 2002.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems},
  33:6840--6851, 2020.

\bibitem{ho2021classifierfree}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock In {\em NeurIPS 2021 Workshop on Deep Generative Models and
  Downstream Applications}, 2021.

\bibitem{ho2022guidance}
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi,
  and David~J Fleet.
\newblock Video diffusion models.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, {\em Advances in Neural Information Processing Systems}, volume~35,
  pages 8633--8646. Curran Associates, Inc., 2022.

\bibitem{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton.
\newblock Adaptive mixtures of local experts.
\newblock {\em Neural computation}, 3(1):79--87, 1991.

\bibitem{krizhevsky2009cifar}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.

\bibitem{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, Fujie Huang, et~al.
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting structured data}, 1(0), 2006.

\bibitem{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago,
  et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em Science}, 378(6624):1092--1097, 2022.

\bibitem{liu2022compositional}
Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua~B Tenenbaum.
\newblock Compositional visual generation with composable diffusion models.
\newblock In {\em European Conference on Computer Vision}, pages 423--439.
  Springer, 2022.

\bibitem{ludke2023diff_tpp}
David L{\"u}dke, Marin Bilo{\v s}, Oleksandr Shchur, Marten Lienen, and Stephan
  GÃ¼nnemann.
\newblock Add and thin: Diffusion for temporal point processes.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem{mayraz2000recognizing}
Guy Mayraz and Geoffrey~E Hinton.
\newblock Recognizing hand-written digits using hierarchical products of
  experts.
\newblock {\em Advances in neural information processing systems}, 13, 2000.

\bibitem{metropolis1953mh}
Nicholas Metropolis, Arianna~W. Rosenbluth, Marshall~N. Rosenbluth, and
  Augusta~H. Teller.
\newblock Equation of state calculations by fast computing machines.
\newblock {\em The Journal of Chemical Physics}, 21(6):1087--1092, 1953.

\bibitem{neal1996bayes}
Radford~M. Neal, P.~Diggle, and S.~Fienberg.
\newblock {\em Bayesian Learning for Neural Networks.}
\newblock Lecture Notes in Statistics Ser.: v.118. Springer New York, 1996.

\bibitem{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In {\em International Conference on Machine Learning}, pages
  8162--8171. PMLR, 2021.

\bibitem{radosavovic2020designing}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr
  Doll{\'a}r.
\newblock Designing network design spaces.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 10428--10436, 2020.

\bibitem{roberts2002mala}
G.~O. Roberts and O.~Stramer.
\newblock Langevin diffusions and {M}etropolis--{H}astings algorithms.
\newblock {\em Methodology \& Computing in Applied Probability}, 4(4):337 --
  357, 2002.

\bibitem{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L
  Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim
  Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em Advances in Neural Information Processing Systems},
  35:36479--36494, 2022.

\bibitem{salimans2021should}
Tim Salimans and Jonathan Ho.
\newblock Should {EBM}s model the energy or the score?
\newblock In {\em Energy Based Models Workshop - ICLR 2021}, 2021.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International conference on machine learning}, pages
  2256--2265. PMLR, 2015.

\bibitem{song2023guidance}
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz.
\newblock Pseudoinverse-guided diffusion models for inverse problems.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  May 2023.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{song2021scorebased}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{villani2009opttransp}
C{\'e}dric Villani.
\newblock {\em The {W}asserstein distances}, pages 93--111.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2009.

\bibitem{wang2024neural}
Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, and
  Yang You.
\newblock Neural network diffusion, 2024.

\bibitem{wynn2023diffusionerf}
Jamie Wynn and Daniyar Turmukhambetov.
\newblock Diffusio{N}e{RF}: Regularizing neural radiance fields with denoising
  diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4180--4189, 2023.

\end{thebibliography}
