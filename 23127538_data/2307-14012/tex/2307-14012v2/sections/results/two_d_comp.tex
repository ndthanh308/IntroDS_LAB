\subsection{2D composition}
We repeat the 2D product composition example from \cite{du2023reduce}.
The experimental setup is identical to theirs unless otherwise specified.
Further details are given in \cref{sec:expdetails:2d_comp} 

A pair of 2D densities are composed by multiplication into a more complex distribution, as in \cref{eq:comp:prod:ebm}.
We consider a Gaussian mixture with 8 modes evenly distributed along a circle and a uniform distribution that covers two modes of the Gaussian mixture. For a visual representation of the two individual distributions and their resulting product distribution together with samples from the reverse diffusion and HMC corrected samples, see \cref{fig:exp:2d_toy}.
%
% Figure environment removed
For the baseline reverse diffusion process, we use $T=100$ diffusion steps.
The MCMC versions add an extra sampling procedure at each diffusion step $t$, refining the samples of the reverse process.
In \cite{du2023reduce}, they omit the reverse step from the sampling, and we do the same to make the comparison more similar, but it is trivial to reinstate the reverse step into the sampling process.
The MCMC sampling is run for $\mcSteps = 10$ at each $t$, where (U-)HMC uses 3 leapfrog steps per MCMC step.

We consider three metrics to evaluate performance. The first metric is the log-likelihood (LL), where we compare the likelihood of generated samples under the true data distribution, following \cite{du2023reduce}. We may, however, sample points outside the support of the true distribution. It is unclear how this issue is handled in \cite{du2023reduce}, but we address it by adding a small uniform probability to extend the support to the sampled points. Full details are provided in the appendix \ref{sec:expdetails}. 
The second metric is referred to as a Gaussian mixture model (GMM). In this metric, we sample from both the true and data distributions. Then, we fit a bi-modal GMM to each set of samples and compute the Frobenius norm of the mean difference of the variances.
Finally, we supplement the metrics with the Wasserstein-2 distance ($W_2$) to quantify a measure between the data and model distribution \cite{villani2009opttransp}.
To achieve this, we draw samples from the data and model distributions and compute $W_2$ by finding the optimal assignment between the two sampled sets. 

We report quantitative results for the 2D composition in \cref{tab:exp:2d_toy}. 
The results are averaged over 10 independent trials. In each trial, we train the two diffusion models from scratch and sample 2000 points with different MCMC methods.
%
\begin{table}
\centering
\caption{Quantitative results for 2D composition. The mean and standard deviation for the LL, $W_2$, and GMM metrics are computed for the product compositions in \cref{fig:exp:2d_toy}, for both score and energy parameterised models.
The metrics are based on 10 independent trials, wherein each trial, we re-train the diffusion model components and generate 2000 samples.
For the score parameterisation, we report results for different numbers of additional points in the trapezoid rule and integration curves ("line" indicates a straight path, while "curve" means integrating along the trajectory formed by exploratory points used by HMC to propose new points).
}
\begin{tabular}{|c|c|c|c|c|}
\hline
Model & Sampler & LL\textuparrow & $W_2$\textdownarrow & GMM \textdownarrow \\ \hline
\multirow{5}{*}{Energy} 
& Reverse		 & $-8.22 \pm 0.21$ & $5.81 \pm 0.19$ & $0.02701 \pm 0.00134$ \\
& U-LA		 & $-7.52 \pm 0.22$ & $4.19 \pm 0.45$ & $0.01461 \pm 0.00135$ \\
& LA		 & $-6.50 \pm 0.30$ & $4.24 \pm 0.55$ & $0.01466 \pm 0.00146$ \\
& U-HMC		 & $-5.72 \pm 0.18$ & $4.19 \pm 1.25$ & $0.00653 \pm 0.00091$ \\
& HMC		 & $\pmb{-4.09 \pm 0.14}$ & $\pmb{4.12 \pm 1.44}$ & $\pmb{0.00333 \pm 0.00065}$ \\
\hline
\multirow{10}{*}{Score}
& Reverse		 & $-8.15 \pm 0.24$ & $5.80 \pm 0.20$ & $0.02688 \pm 0.00120$ \\
& U-LA		 & $-7.57 \pm 0.12$ & $4.44 \pm 0.63$ & $0.01499 \pm 0.00062$ \\
& LA-1-line		 & $-6.45 \pm 0.20$ & $4.03 \pm 0.52$ & $0.01428 \pm 0.00107$ \\
& LA-3-line		 & $-6.61 \pm 0.17$ & $4.22 \pm 0.46$ & $0.01519 \pm 0.00092$ \\
& LA-8-line		 & $-6.53 \pm 0.17$ & $4.20 \pm 0.51$ & $0.01475 \pm 0.00091$ \\
& U-HMC		 & $-5.77 \pm 0.12$ & $3.39 \pm 0.77$ & $0.00690 \pm 0.00071$ \\
& HMC-1-line	 & $-4.29 \pm 0.13$ & $2.92 \pm 1.02$ & $0.00372 \pm 0.00061$ \\
& HMC-3-line		 & $\pmb{-4.07 \pm 0.13}$ & $\pmb{2.68 \pm 1.20}$ & $0.00308 \pm 0.00069$ \\
& HMC-8-line		 & $\pmb{-4.07 \pm 0.14}$ & $2.87 \pm 0.89$ & $0.00317 \pm 0.00056$ \\
& HMC-curve		 & $\pmb{-4.07 \pm 0.12}$ & $2.94 \pm 0.90$ & $\pmb{0.00306 \pm 0.00054}$ \\
\hline
\end{tabular}
\label{tab:exp:2d_toy}
\end{table}
%
The results demonstrate superior performance of the adjusted sampling methods compared to the unadjusted approaches. Additionally, the HMC-variants achieve better results than Langevin, while the reverse process performs worse. Score and energy parameterisations exhibit similar overall performance in LL and GMM within their respective sampling procedures. However, when paired with HMC, the score parameterisation significantly outperforms the energy parameterisation in terms of Wasserstein 2-distance. Further, we note that the performance plateaus when using as few as four points in the trapezoidal rule.