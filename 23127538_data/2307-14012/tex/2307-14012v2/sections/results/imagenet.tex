\subsection{Guided diffusion for ImageNet}
We extend the evaluation of our proposed method for guided diffusion sampling on the ImageNet dataset \cite{deng2009imagenet}. We utilize pre-trained score models from \cite{dhariwal2021diffbeatgan}, available on the OpenAI GitHub repository\footnote{https://github.com/openai/guided-diffusion}.

As in the CIFAR-100 experiments, the score of the marginal distribution $\gradX \log \pEbm(\x_t, t)$ is estimated using an unconditional diffusion model. For the ImageNet dataset, the score parameterized model, $\epsilon_\theta : \mathbb{R}^{D_x} \times 1 \rightarrow \mathbb{R}^{D_x}$, where $D_x = 3 \cdot 256^2$, is also parameterized by a neural network with a UNet architecture. Again, we use classifier-full guidance, and the classifier model is parameterised by a neural network with the structure of the corresponding encoder part of the UNet.

Given the high computational demands due to both the large models and high-dimensional input, we have chosen to focus solely on evaluating HMC (with our MH-like correction) and compare it to the baseline, which is the standard reverse process with $T=1000$. The HMC sampler adds $L=2$ extra MCMC steps at each diffusion step $t$, where each MCMC step constitutes three leapfrog steps. Both sampling methods use the same
guidance scale $\lambda = 20.0$. Again, we incorporate the points given by the leapfrog steps, but due to the high dimension, two additional points between each leapfrog step are needed for the line integration. We conduct the same type of parameter search of the step length for the HMC method as in the CIFAR-100 experiment. Further details are provided in \cref{sec:expdetails:imagenet_guid}.

We sample 50k images with both sampling methods and compute the FID score \cite{heusel2017gans} (based on the validation set), the average accuracy\footnote{Again, an image is considered correctly generated if the classifier's prediction for the specified class is at least 50\%.}, and the top-5 average accuracy, i.e., correct prediction if the specified label is within the top-5 predictions. Once again, we utilize a separate classifier model, in this case, a RegNetX-8.0GF \cite{radosavovic2020designing}, for evaluation. This time, however, the model is pre-trained. The results are shown in \cref{tab:exp:imagenet}.

\begin{table}
\centering
\caption{Average accuracy, top-5 accuracy, and FID score for classifier-full guidance on ImageNet.
The metrics are based on 50k generated samples for both sampling methods with score parameterisations.
We use the guidance scale $\lambda = 20.0$,  $\mcSteps = 2$ with $3$ leapfrog steps for HMC, the step length at diffusion step is $a \beta_t^b$.
The accuracy is based on an independent classifier model.
}
\begin{tabular}{|c|c|c|c|c|}
\hline
Model & Sampler & Accuracy [\%]\textuparrow & Top-5 Accuracy [\%]\textuparrow & FID\textdownarrow \\ \hline
\multirow{2}{*}{Score}
& Reverse   & $\pmb{50.0}$ & $83.9$ & $14.5$ \\
& HMC-6-curve       & $49.9$ & $\pmb{85.1}$ & $\pmb{11.6}$ \\
\hline
\end{tabular}
\label{tab:exp:imagenet}
\end{table}
The reverse process and HMC perform very similarly in average accuracy, but our method shows a slight improvement in top-5 average accuracy. However, augmenting with some extra MCMC steps with MH-like correction significantly improves the FID score.