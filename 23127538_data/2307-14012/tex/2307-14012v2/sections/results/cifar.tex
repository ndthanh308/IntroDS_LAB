\subsection{Guided diffusion for CIFAR-100}
We evaluate our proposed sampling methods for guided diffusion sampling on the CIFAR-100 image dataset \cite{krizhevsky2009cifar}.
The sampling process is based on a score function defined in \cref{eq:guidance:score}, composed of the score functions of a marginal distribution $\gradX \log \pEbm(\x_t, t)$ and a likelihood of the class $y$, $\gradX \log p(y \mid \x_t, t)$, at each diffusion step $t = 1, \dots, T$.

The score of the marginal distribution $\gradX \log \pEbm(\x_t, t)$ is estimated with an unconditional diffusion model.
The score parameterised model is $\epsP: \reals^{D_x \times 1} \to \reals^{D_x}$, where $D_x = 3 \cdot 32^2$, takes a noisy image and $t$ as input and outputs a noise prediction of the same shape as $\x_t$.
It is parameterised by a neural network with a UNet architecture.
We use the same architecture and training settings as \cite{ho2020denoising} used for the CIFAR-10 image dataset \cite{krizhevsky2009cifar}.
The energy parameterised model uses the same architecture and is parameterised as in \cref{eq:ebm:param}.

For the guidance model, we use classifier-full guidance, that is, we train a classifier model to predict the class label of an image at all diffusion steps $\pCfull(y \mid \x_t, t)$, where the classifier parameters $\pF$ are independent of $\param$.
The classifier model is parameterised by a neural network with the first half of the UNet structure used for the unconditional diffusion model extended with a dense layer. To train the classifier, we use labelled pairs $(\x_t, y)$, where $y$ is the class and $\x_t \sim q(\x_t \mid \x_0)$ is a sample from the forward diffusion process in \cref{eq:diff:forward_proc_x0}, conditioned on a sample $\x_0$ from the data distribution.

The sampling is based on the standard reverse process with $T=1000$.
The MCMC samplers add $\mcSteps = 2$ or $6$ extra MCMC steps at each diffusion step $t$ for (U-)HMC and (U-)LA, respectively, whereas (U-)HMC uses 3 leapfrog steps per MCMC step.
All sampling methods use the same guidance scale $\lambda = 20.0$.

For this experiment, we need to use more points in the trapezoidal rule's mesh than in the 2D experiment. Based on the insights from that experiment, for HMC we integrate only along the curve obtained from the leapfrog steps. However, we also evaluate a point in the middle of each leapfrog step to obtain a better energy estimation, resulting in three extra model evaluations per HMC step. For LA, we use seven evaluation points along the line, which means eight extra evaluations per step.

Recognising the impact of the step length on MCMC methods in general, we parameterise the step length as a function of the beta-schedule $\langStep_t = a \beta_t^b$. We conducted a simple parameter search for parameters $a$ and $b$, to determine  suitable step length for each MCMC variant.
Further details are provided in \cref{sec:expdetails:cifar100_guid}.

We sample 50k images with each sampling method, for both the energy and score parameterisations and compute the FID score \cite{heusel2017gans} (based on the validation set) and the average accuracy\footnote{We classify an image as correctly generated if the classifier has predicted the specified class and is 50\% certain or greater.} of a separate classifier model, trained only on noise-free pairs $(\x_0, y)$ from the CIFAR-100 dataset.
The model architecture of the classifier is VGG-13-BN from \cite{simonyan2014very}.
The results are shown in \cref{tab:exp:cifar100}.
%
\begin{table}
\centering
\caption{Average accuracy and FID score for classifier-full guidance on CIFAR-100.
The metrics are based on 50k generated samples for each sampling method with both energy and score parameterisations.
We use the guidance scale $\lambda = 20.0$, the (U-)LA methods use $\mcSteps = 6$ MCMC steps, and (U-)HMC use $\mcSteps = 2$ with $3$ leapfrog steps for the variants, the step length at diffusion step is $a \beta_t^b$.
The accuracy is based on a separate model, which has only been trained on noise-free samples, i.e., it predicts $p(y \mid \x_0)$.
Both parameterizations benefit from the added MCMC steps, especially the MH-corrected versions.
The energy parameterisation appears to perform worse in the standard reverse process but sees a larger improvement from the extra MCMC steps.
}
\begin{tabular}{|c|c|c|c|}
\hline
Model & Sampler & Accuracy [\%]\textuparrow & FID\textdownarrow \\ \hline
\multirow{5}{*}{Energy} 
& Reverse   & 72.6 & 33.4 \\
& U-LA      & $\pmb{87.3}$ & 24.6 \\
& LA        & 80.0 & 12.7 \\
& U-HMC     & 87.2 & 25.4 \\
& HMC       & 84.9 & $\pmb{12.4}$ \\
\hline
\multirow{5}{*}{Score}
& Reverse   & $74.2$ & $31.8$ \\
& U-LA      & $\pmb{82.9}$ & $25.9$\\
& LA-8-line        & $75.2$ & $15.5$ \\
& U-HMC     & $79.0$ & $28.6$ \\
& HMC-3-curve       & $75.8$ & $\pmb{13.3}$ \\
\hline
\end{tabular}
\label{tab:exp:cifar100}
\end{table}
From the table, we note a general trend of improvement of the baseline reverse process when additional MCMC steps are added.
In particular, the MH-corrected samplers LA and HMC show significant improvement in the FID score, which is arguably the more important metric for image generation. 

Comparing the score and energy parameterisations, the respective performances have largely shared characteristics.
Interestingly, the basic reverse process favours the score parameterisation supporting the claim that this less restricted parameterisation better models the score function of the marginal distribution.
However, the energy parameterisation sees larger improvements from the added MCMC steps.
This indicates, perhaps, that the direct energy estimation provides a better correction step compared to our method of approximating the pseudo-energy difference from $\epsP$, though it should be noted that the same difference is also observed in the unadjusted samplers U-LA and U-HMC.
Despite the performance edge of the energy parameterisation, our proposed MH-corrected sampling methods can provide essentially the same improvement, without having to train an energy parameterised diffusion model.