\section{Introduction}

Significant advancements have been achieved in generative modelling across various domains in recent years \cite{brock2018large, brown2020language, ho2020denoising}.
These models have become potent priors for a wide range of applications, including code generation \cite{li2022competition}, text-to-image generation \cite{saharia2022photorealistic}, question-answering \cite{brown2020language}, and many others \cite{gungor2023adaptive, wynn2023diffusionerf}.
Among the generative models, diffusion models \cite{sohl2015deep, song2019generative, ho2020denoising} have arguably emerged as the most powerful class.
Diffusion models learn to denoise corrupted inputs in small, gradual steps and are capable of generating samples from complex distributions. They have been successful in many domains, such as generating highly realistic images \cite{dhariwal2021diffbeatgan}, modelling temporal point processes \cite{ludke2023diff_tpp} and even generating neural network parameters \cite{wang2024neural}.

Diffusion models also offer the capability of composed sampling, which combines pre-trained models to generate samples from a new distribution.
This approach, known as model composition, has a rich history \cite{jacobs1991adaptive, hinton2002training, mayraz2000recognizing, liu2022compositional}.
For diffusion models, the most common form of composition is classifier-guided sampling, where the reverse process is augmented by a separate classifier model \cite{sohl2015deep, dhariwal2021diffbeatgan, ho2021classifierfree}, but other compositions have also been explored \cite{du2023reduce}.
The ability to compose new models without having to re-learn the individual components is especially appealing for diffusion models since their ever-increasing size and data hunger make them exceedingly costly to train \cite{aghajanyan2023scaling}.
Therefore, it is valuable to develop sampling methods that work for pre-trained diffusion models.

% Score-based
The foundation of composed sampling for diffusion models is score-based, where we interpret diffusion models as predictors of the score function for the marginal distribution at each diffusion step \cite{song2021scorebased}. From this perspective, MCMC methods, such as the Langevin algorithm (LA) \cite{roberts2002mala} or Hamiltonian Monte Carlo (HMC) sampling \cite{duane1987hmc}, emerge as viable options to incorporate. Augmenting the standard reverse process with additional MCMC sampling has been shown to improve composed sampling for diffusion models \cite{du2023reduce}. However, we are restricted to unadjusted variants of these samplers, namely Unadjusted LA (U-LA) and Unadjusted HMC (U-HMC), which only require utilization of the score. This limitation means we cannot incorporate a Metropolis-Hastings (MH) correction step \cite{metropolis1953mh,hasting1970mh}, which requires evaluating the unnormalized density.

An intriguing alternative to directly modeling the score function is to model the marginal distribution with an energy function, from which the score can be obtained through explicit differentiation \cite{salimans2021should, song2019generative}. This parameterization connects diffusion models and energy-based models (EBMs) \cite{lecun2006tutorial} and offers several desirable properties. With an energy parameterization, we can evaluate the unnormalized density and guarantee a proper score function. This, in turn, enables an MH correction step when employing a MCMC-method, where the MH acceptance probability is computed from the energy function. Adding such a correction step has been shown to improve sampling performance in composed models \cite{du2023reduce}. Nevertheless, the score parameterization remains far more popular, as it avoids the direct computation of the gradient of the log density.

In this study, we build on the work in \cite{du2023reduce} and introduce a novel approach to obtain a form of MH-like correction directly from pre-trained diffusion models without relying on an energy-based parameterisation.
Specifically, we use a connection between the score and the energy to estimate the MH acceptance probability by approximating a line integral along the vector field generated by the score of the composed diffusion models.
This enables an improved sampling procedure for various pre-trained score-parameterised diffusion models.

We approximate the line integral using the trapezoidal rule and validate the effectiveness of our MH-like correction by sampling from different composed distributions and comparing it with an energy-based parameterisation.
We find that our approximate method quantitatively results in improvements comparable to the energy parameterisation, without having to estimate the energy directly.