\section{Introduction}

\label{sec:intro}


% Ratings apps like Yelp, Vivino or TripAdvisor collect explicit ratings and aggregate them to obtain average item scores. The reputation of these companies relies significantly on the perceived usefulness of these aggregates scores. Similarly, music and video players like Spotify, Youtube and Tik Tok collect implicit ratings that are used to train recommendation algorithms. The effectiveness of the recommendation algorithms influence user retention, an important driver of their success. 

%Recommendation and ratings systems have become an integral part of many online platforms, with applications ranging from e-commerce to social media. These systems rely on user data---both {implicit} such as browsing behavior and {explicit} such as five-star rating---to learn about items (overall quality and heterogeneous preferences) and ultimately provide (potentially personalized) guidance to users. A large literature has emerged on how to -- \textit{given} user data -- {estimate} item quality and user-item preferences, and {recommend} items. For example, systems may report the sample mean of ratings for each item or recommend items through a matrix factorization or deep learning approach.

%This paper, in contrast, studies the \textit{inputs} to such methods: how does the ratings \textit{data} collection process -- which is outside the control of individual producers -- affect item quality estimation and recommendations, and does it do so heterogeneously? Here, we focus on two aspects of the data collection process: (i) user ratings inflation and heterogeneity and (ii) algorithmic bias caused by personalized recommendations. While these aspects are discussed in the recommendations literature, we show that the resulting \textit{bias} and \textit{variance} in estimating item quality due to these effects is first order. We further analyze a randomized controlled trial aimed at mitigating user ratings inflation and resulting ratings heterogeneity. In particular, we tackle the following challenges.






Recommendation and rating systems have emerged as crucial components of numerous online platforms, ranging from e-commerce to social media. These systems depend on user data, including both {implicit} indicators like browsing behavior and {explicit} such as rating scores. This data provides insights into the overall quality\footnote{Taste in many items, including music and movies, is inherently subjective. However, many platforms (such as Spotify, Rotten Tomatoes, and IMDb) calculate overall popularity scores or ratings -- alongside attempting to measure heterogeneous taste} of items and heterogeneous preferences, and ultimately provide personalized guidance to users.

Extensive research has been conducted on how to estimate item quality and user-item preferences and make personalized recommendations based on user data. Various approaches have been proposed, such as computing the sample mean of ratings for each item or leveraging matrix factorization or deep learning methods to generate recommendations.
In contrast, this paper focuses on the data collection process that provides input to these methods. 

%In this paper, we examine how it impacts item quality estimation and personalized recommendations and whether it does so heterogeneously. 
Specifically, we explore how the rating data collection process affects item quality estimation and recommendation accuracy.
Here, we focus on two aspects of the data collection process: (i) user rating inflation and heterogeneity; and (ii) algorithmic bias caused by personalized recommendations.

Although previous studies have touched upon these issues, our analysis reveals that the resulting \textit{bias} and \textit{variance} in estimating item quality due to these effects are substantial. We further analyze a randomized controlled trial aimed at mitigating user rating inflation and the resulting heterogeneity in ratings. In particular, we tackle the following challenges:



\textbf{First}, we show that there is substantial user-level heterogeneity in what ratings mean (heterogeneous measurement). For example, picky users give lower ratings on average than generous users, even for similar item qualities or experiences. Thus, with naive methods, \textit{estimated overall item qualities and rankings may say more about the raters themselves than about the items being rated}. This effect is more pronounced and differentially affects items due to {cold-start-induced \textit{variance}} when new items have fewer ratings and thus are more susceptible to the idiosyncratic behavior of individual users. Depending on the platform, such heterogeneity may have substantial downstream economic consequences for item producers. 



%\textbf{Second}, the ratings \textit{interface} is itself a platform design choice, affecting ratings data, and in turn, the accuracy and effectiveness of the recommendation system. For example, \citet{garg2021designing} find that, on an online labor market, switching from numeric to ``positive-skewed adjectives'' ratings options led to deflated, ``more informative'' ratings. However, it is an open question regarding the extent to which interface design may further affect users' heterogeneous behavior and, in turn, how it affects recommendation. For example, it is possible that some interface designs produce more ``homogeneous'' user behavior than do other designs, increasing informativeness of item quality. 

%To study the above effects, we analyze a natural experiment and then a randomized controlled trial (RCT) on a music discovery platform. The experiments dramatically changed the user interface. Before the change in the natural experiment, users were able to select each ratings option immediately after the song started playing. After the change, users had to \textit{wait} several seconds as the ``dislike'', ``like'' or ``super-like'' options progressively unlocked. The RCT then further altered how long users had to wait before options are unlocked. We find that introducing timers substantially changed user rating behavior, on average inducing more ``pickiness'' than before and reducing across-user behavior variance. We further find evidence that this reduction in the heterogeneity of user behavior improved the effectiveness of the system. While a user's rating for a given item was \textit{more a function of the user who happened to be shown the item} than the item quality (as estimated by other ratings) both before and after the change, the change increases the relative importance of item quality. %\writingtodo{update this based on results}

\textbf{Second}, the rating interface is a critical platform design choice that can impact recommendation systems' accuracy and effectiveness. For instance, recent research~\cite{garg2021designing} has demonstrated that changing the rating options from numerical to positive-skewed adjectives on an online labor market can result in more informative ratings. However, the extent to which interface design affects users' heterogeneous behavior and, subsequently, the impact on recommendation remains an open question. It is possible that certain interface designs produce more homogeneous user behavior, resulting in more informative item quality estimates.

To examine these effects, we analyze a natural experiment and a randomized controlled trial (RCT) on a music discovery platform. The initial platform change altered the user interface to introduce wait times before users could select their rating options (``dislike'', ``like'' or ``super-like''). The RCT further altered how long users had to wait before options were unlocked.
We find that introducing timers substantially changed user rating behavior, inducing more ``pickiness'' than before and reducing across-user behavior variance. We further find evidence that this reduction in user behavior heterogeneity improved the system's effectiveness. While a user's rating for a given item was \textit{more a function of the user who was shown the item} than the item quality (as estimated by other ratings) both before and after the change, the change increased the relative importance of item quality. %\writingtodo{update this based on results}

    %\textbf{Third}, we analyze the relationship between personalized recommendations and ratings behavior. We conduct exploratory analyses using the Piki data on how item quality estimates are related to the amount of personalization an item is associated with. We find that randomness in this amount of personalization can have substantial influence on item quality estimates, beyond that which can be expected by the fact that ``better'' songs are more likely to be served via personalized recommnedation. %We then conduct a simulation study calibrated with the results of the natural experiment and the RCT: what are the downstream consequences of the ratings interface in terms of system \textit{performance}, as defined by its ability to identify high quality items and provide effective personalized recommendations to each user. The calibrated simulation setting gives us access to an item's ``true'' utility for each user, which is not directly available in any empirical setting. We find that ratings distributions can have a substantial effect on downstream performance metrics, in ways that do not necessarily correspond to offline metrics of informativeness.  




\textbf{Third}, we investigate the relationship between personalized recommendations and user rating behavior. Using the Piki dataset\footnote{The full (user-anonymized) data is available online: \url{https://github.com/sstoikov/piki-music-dataset}.}, we conducted exploratory analyses to understand how item quality estimates are related to how often that item is recommended via personalized recommendation. Our findings reveal that randomness in personalization can significantly influence item quality estimates. This is beyond what can be expected from the fact that higher-quality items are more likely to be recommended.


	% 
%Overall, our work has several consequences for the design of ratings and recommendation systems and future work. In particular, our results establish that user behavior heterogeneity can have substantial effects on a system's ability to learn item quality and provide effective recommendations. Item quality estimates are not just an indicator of item quality, but rather the users who rated the items and how those users were selected. While our work further points to \textit{interface} design as one potential solution, \textit{algorithm}-design (for example, correcting for personalization and user heterogeneity) may also play a role. 
    % ; after the change, the reverse is true.


Overall, our work has several implications for the design of rating and recommendation systems as well as future research in this area. Specifically, our findings demonstrate that user behavior heterogeneity can significantly impact a system's ability to learn item quality and provide effective recommendations.
Our results indicate that item quality estimates reflect not only item quality but also the users who rated the items and the method used to select those users. While our work highlights interface design as a potential solution, algorithmic design, such as correcting for personalization and user heterogeneity, may also play a crucial role in improving the accuracy and fairness of item quality estimation and recommendation systems.

    
    % \item Second, we analyze the results of  run by the Piki platform of more subtle changes to the interface: users were randomized to each treatment group, which differed in \textit{how long} users had to wait before they could ``like'' the song. The RCT replicates the results of the natural experiment in a finer-grain manner.



% \textbf{Third}, the aforementioned effects potentially interact in complex ways with personalized recommendations, i.e., how which users rate which items is chosen. Most trivially, an item that is shown more often via personalized recommendation (vs at random) will receive higher ratings, from users who are better matches for the item. In platforms where the quality and amount of personalized recommendation may differ by platform, such effects may induce bias in item quality estimation.


%\writingtodo{SASHA: instead of "data itself", "data production" or "data collection"}


% How app interfaces are designed has a profound impact on recommendation algorithms since they produce their training data. In this paper, we focus on the role of the interface on the the collection of ratings data. What makes average ratings useful is a challenge to define, especially for media like music where users are diverse in their attitudes and rating behaviors. We focus our attention on how interface design can mitigate ratings inflation by controlling for . 

% The main concern with user heterogeneity is that ratings may be inflated by users who rate in an indiscriminate way and essentially like everything. This may be due to leniency, social pressure, fraud or self selection. The problem of ratings inflation is that it dilutes the informativeness of ratings. Moreover, particularly for songs with limited amounts of ratings, average ratings are likely to say more about the rater than the items they are rating.

% The role of algorithmic bias is particularly relevant when collecting implicit ratings in streaming and social media apps, where there are many ways a user can listen to a song. A user can search, listen to a radio, select a personalized algorithmic playlists, or hyper-personalized algorithms that recommend more songs by artists a user really likes. Apps that rely on user retention may be incentivized to err on the side of very safe recommendations. This in turn may tilt the training data towards hyper personalized algorithms that train and retrain on the data that they essentially generate.



% 	2. However, data might be heterogeneous -- heterogeneous measurement, where the data means different things to different users. 
% This could have consequence for songs, especially in personalized systems

%{First}, there may be substantial user-level heterogeneity in what ratings mean (heterogeneous measurement). For example, picky users may on average give lower ratings than generous users, even for similar item qualities or experiences. Thus, with naive methods, \textit{estimated overall item qualities and rankings may say more about the raters themselves than about the items being rated}. 
%The effect may be more pronounced and differentially affect items due to {cold-start-induced \textit{variance}}, when new items have few ratings and thus are more susceptible to idiosyncratic behavior of individual users. Depending on the platform, such heterogeneity may have substantial downstream economic consequences for item producers.  

%Second, the ratings \textit{interface} is itself a platform design choice, affecting ratings data, and in turn, the accuracy and effectiveness of the recommendation system. For example, \citet{garg2021designing} find that, on an online labor market, switching from numeric to ``positive-skewed adjectives'' ratings options led to deflated, ``more informative'' ratings. However, it is an open question regarding the extent to which interface design may further affect users' heterogeneous behavior and, in turn, how it affects recommendation. For example, it is possible that some interface designs produce more ``homogeneous'' user behavior than do other designs, increasing informativeness of item quality. 

%Third, the aforementioned effects potentially interact in complex ways with personalized recommendations, i.e., how which users rate which items is chosen. Most trivially, an item that is shown more often via personalized recommendation (vs at random) will receive higher ratings, from users who are better matches for the item. In platforms where the quality and amount of personalized recommendation may differ by platform, such effects may induce bias in item quality estimation.


% For example, if an item is 


% ; and (b) {personalized-recommendations-induced \textit{bias}}, when items are predominantly shown to a subset of users whose ratings behavior may systematically differ (for example, ratings behavior may differ across cultures, whose item preferences are also correlated) \writingtodo{(for example, raters who like a niche category may inflate the ratings of every item in this category)}. 

% \nikhil{This paragraph doesn't quite capture the two effects -- user heterogeneity, and personalized recommendation}

% \nikhil{We are focused on quantifying two influences on item quality ratings beyond their control: }



% \nikhil{The previous sections study the interaction between user rating behavior and item quality estimates. Next, we analyze how\textit{ personalized recommendations} affect ratings and song quality estimates, and vice versa.
% }
% 1. In this paper, we 
	% 	a. First, natural experiment (section …) drastically changed how get user interface. 
	% 	b. Then, analyzing a historical randomized controlled experiment, with further minor UI changes.
	% 		i. Data is public, involved in just analysis of the public data. Not involved in the implementation/running of the experiment.
	% 	c. Finally, through semi-synthetic calibrated simulations, to think about down-stream consequences. 


% and logged.  
% \begin{enumerate}[label=\alph*)]
    %\item First, we analyze a natural experiment due to a product deployment which dramatically changed the user interface. Before the change, users were able to select each ratings option immediately after the song started playing. After the change, users had to \textit{wait} several seconds as the ``dislike'', ``like'' or ``super-like'' options progressively unlocked. We find that this change substantially changed user rating behavior, on average inducing more ``pickiness'' than before and reducing across-user behavior variance. We further find evidence that this reduction in the heterogeneity of user behavior improved the effectiveness of the system. While a user's rating for a given item was \textit{more a function of the user who happened to be shown the item} than the item quality (as estimated by other ratings) both before and after the change, the change increases the relative importance of item quality. %\writingtodo{update this based on results}
    
    % ; after the change, the reverse is true.

    % \item First, we analyze the relationship between personalized recommendations and ratings behavior. We find that the more a recommendation is personalized, the more it inflates item scores. 

    % \item Second, we study the role of user heterogeneity on average ratings. We find that the average rater pickiness has a strong impact on an item's average rating.
    
    % \item Third, we analyze the results of a randomized controlled trial (RCT) run by the Piki platform of a subtle change to the interface: users were randomized to each treatment group, which differed in \textit{how long} users had to wait before they could ``like'' the song.
    % We show that setting a longer timer on the like button (while keeping the other timers fixed) nudged users towards more dislikes and more super-likes.
    
    %The RCT replicates the results of the natural experiment in a finer-grain manner. %, the trial allows us to analyze songs across treatment groups as well as how the effects are modulated by the recommendation algorithm. Strikingly, we find that personalized recommendation \textit{reduces} \writingtodo{once have results state it clearly}.
    
      
% \end{enumerate}

	% 
%Overall, our work has several consequences for the design of ratings and recommendation systems and future work. In particular, our results establish that user behavior heterogeneity can have substantial effects on a system's ability to learn item quality and provide effective recommendations. Item quality estimates are not just an indicator of item quality, but rather the users who rated the items and how those users were selected. 
% While our work further points to \textit{interface} design as one potential solution, \textit{algorithm}-design (for example, by normalizing user ratings) may also play a role. 

% \rana{add more details, maybe something like this? normalization can be applied to user ratings to adjust for differences in rating scales or rating tendencies across users.}

\begin{comment}
    
\paragraph{{Outline.}}
We briefly survey related work in Section~\ref{sec:related_works} and describe the empirical setting of Piki in Section~\ref{sec:platform}. The natural experiment analysis is covered in Section~\ref{sec:natural_exp}. We describe the randomized controlled trial and analyze its results in Section~\ref{sec:experiment}.
In Section~\ref{sec:personalized}, we examine the interaction between user rating behavior and personalized recommendations, including through further empirical analyses and calibrated simulations. Finally, we conclude in Section~\ref{sec:discussion}.
\end{comment}


% \writingtodo{heterogeneous user behavior might have substantial economic }

% \nikhil{note to myself...a fancier algorithm to mention below could be learn user-item matrices then throw out the first user dimension?}

% In this paper, we first analyze whether platforms can induce pickiness or super-likeness.
% To do this, we conduct a simulation study in which we manipulate the length of time users have to wait before providing a rating (such as a dislike, like, or super-like) and examine the resulting data quality and user behavior.
% Second, for each treatment condition, we simulate a platform and examine the data quality as well as user behavior, comparing how the system performs.
% We also compare the performance of the recommendation system under different interface changes. By understanding the factors that influence input data and the impact on rating systems performance, we can design more effective recommendation systems.








% 	3. It's a further a function of the user interface
% 	4. In this paper, we 
% 		a. First, natural experiment (section …) drastically changed how get user interface. 
% 		b. Then, analyzing a historical randomized controlled experiment, with further minor UI changes.
% 			i. Data is public, involved in just analysis of the public data. Not involved in the implementation/running of the experiment.
% 		c. Finally, through semi-synthetic calibrated simulations, to think about down-stream consequences. 
% 	5. Together, we find the following:
% 		1. There is user-level heterogeneity in what inputs mean (heterogeneous measurement) 
% 			i. Ratings say as much about the user + system than they do about the item.
% 		2. Interface design can change the input data, both overall and by user
% This matters for downstream outcomes in recommendation systems

% \subsection{Sasha intro}

% How does an algorithm know what to recommend? In the case of music recommendations, it first collects user opinions through an interface, transforming them into user-item ratings data. Second, it trains a model with these ratings, to maximize an objective. This process effectively aggregates ratings into a score for each song, which may or may not be personalized for each user. These scores have a profound impact on both users’ consumption as well as musicians’ incomes – and yet, as we discuss in this paper, these \textbf{scores may say more about the raters than about the quality of the songs they are rating}.

% For example, raters who “like” everything (or are extremely eclectic) offer limited value to a system. As their ratings flood the system with likes, the highest ranked songs are likely to simply be the ones that have been exposed to most people. This popularity bias has been widely cited in the literature on recommendation systems. Likewise, raters who like nothing (or are extremely picky) also offer limited value to a system. In a system trained only on this kind of data, the lowest ranked songs are likely to be the ones most often presented to people. This anti-popularity bias may be helpful for the discovery of new songs, but painful to the ears of the dislikers.

% In practice, raters fall on a pickiness spectrum, ranging from extremely eclectic to extremely picky. Users who consistently offer high ratings will disproportionately raise the rating of the songs to which they are exposed. When aggregating ratings to obtain a score for each song, the score is likely to be biased by the pickiness of the raters whose data is being averaged. This bias may be overcome in two ways: by normalizing the ratings after they have been collected, or by altering the interface used to collect the ratings. 

% Ratings normalization has been widely analyzed in the context of freely available ratings data from platforms such as Netflix, Yelp, TripAdvisor, Vivino, Movielens and BeerAdvocates (insert literature). In this paper we focus on the role of the interface used to collect the ratings, a topic that has received less attention (see Nikhil papers). We use ratings data collected on the Piki app, a music recommendation app that collects the opinions of raters (dislike, like and super-like) on a diverse set of songs. 

% In section 2, we show through a natural experiment performed on the Piki app that the implementation of an unlock timer on the “like” button deflates the ratings produced by people who typically like everything. In section 3, we run an A/B/C test where, for each treatment condition, we modify the timing of the like button. We find that the interface of each treatment group does not significantly affect the retention of a user, but does affect the statistics of the ratings collected. In section 4, we use the statistics obtained in section 3 to simulate a platform on which we examine the data quality as well as user behavior and compare how the system performs. 

% Ultimately, we are interested in how human judgement can be optimally collected, so that this data may be aggregated by algorithms to serve the stakeholders of a recommendation system.

% \subsection{Rana intro}

% Recommendation systems have become an integral part of many online platforms, with applications ranging from e-commerce to social media. These systems use various algorithms to analyze user data, such as browsing history and ratings, in order to provide personalized recommendations that are intended to enhance the user's overall experience. 
% A recommendation system's ultimate goal is to be able to infer a user's preferences and provide them with recommendations that enhance their overall experience. To that end, as users interact with the recommendations offered to them, they provide feedback that the system utilizes to improve its understanding of their preferences and, as a result, improve the quality of future recommendations.


% %Recommender systems are a vital component of many intelligent and personalized services such as e-commerce sites, news content hubs, voice-activated digital assistants, and movie streaming portals.
% Users interact with the recommender system through a graphical user interface (GUI), which serves two main purposes. First, the GUI presents the predictions made by the machine learning system (e.g., personalized news streams, interesting items). Second, the GUI captures feedback data (e.g., clicks, ratings, or other signals arising from user interactions) and passes it back to the machine learning system in the back-end.


% It is important to recognize that ratings provided by users say as much about the user and the system as they do about the rated item. This is because the user's ratings are influenced not only by their own preferences and biases, but also by the design of the rating system itself.
% For example, consider a user who consistently provides high ratings for all of the content they consume. This could be because the user is generally very easy to please and enjoys a wide range of content, or it could be because the rating system is designed in such a way that it only presents content that is likely to be highly rated by the user (e.g. through the use of algorithms that filter out content that is not expected to be well-received). In either case, the user's ratings say more about their own preferences and the system's design than they do about the actual quality of the content being rated.
% Such user is referred by "eclectic."
% Similarly, a user who consistently provides low ratings may do so because they are particularly difficult to please, or because the rating system is presenting them with content that is not aligned with their interests. In this case, the ratings say more about the user's preferences and the system's ability to accurately recommend content that the user will enjoy, rather than the actual quality of the content being rated.
%  Such user is referred by "picky."
% In addition, users who consistently offer high ratings may unjustifiably raise the rating of the content to which they are exposed, potentially affecting other content creators and giving a false picture of the real world.
% It is therefore important to consider both the user and the system when interpreting ratings, as they can significantly influence the data that is collected and the resulting recommendations.


%  User interface, or how users input data about their preferences and interests is one factor that can influence the quality of this input data and the resulting recommendations.
%  The design of the user interface can significantly impact the data that is collected, and in turn, the accuracy and effectiveness of the recommendation system.



% In this paper, we first analyze whether platforms can induce pickiness or super-likeness.
% To do this, we conduct a simulation study in which we manipulate the length of time users have to wait before providing a rating (such as a dislike, like, or super-like) and examine the resulting data quality and user behavior.
% Second, for each treatment condition, we simulate a platform and examine the data quality as well as user behavior, comparing how the system performs.
% We also compare the performance of the recommendation system under different interface changes. By understanding the factors that influence input data and the impact on rating systems performance, we can design more effective recommendation systems.




% In greater detail, our main contributions are as follows:


% \subsection{Motivation}
% \subsection{Experiment}
% \subsection{Simulating a Platform of Each Treatment Condition}