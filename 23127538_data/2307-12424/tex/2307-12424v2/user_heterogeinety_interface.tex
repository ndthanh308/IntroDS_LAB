\section{User rating heterogeneity and interface design}
We begin our analysis by analyzing a `natural experiment' induced by a product deployment, as well as an RCT motivated by the natural experiment results. Here, we present results from the two together, as they largely agree with each other. The results show that (a) user rating heterogeneity (how generous they are on average) substantially influences estimated song quality, and (b) interface design can mitigate such heterogeneity.


\subsection*{Natural experiment and RCT design}

\begin{table}[tb]
	\begin{center}
  %           \begin{tabular*}{\textwidth}{l c r}
  %            \hspace{3cm} \textbf{Users} & \hspace{1.4cm} \textbf{Ratings} &  \hspace{1.65cm}\textbf{Songs} \\
  %           \end{tabular*}
  %           \begin{tabular}{p{1cm}|p{1cm} p{1cm} | p{1cm} p{1cm} | p{2cm} p{2cm}}
		% 	\textbf{Group} & All & Filtered & All & Filtered & Random & Personalized \\
		% 	\midrule
		% 	a & $429$ & $389$ & $122225$ & $0$  & $30757$ & $80765$\\
		% 	b & $331$ & $308$ & $77758$ & $0$  & $20300$ & $50784$\\
		% 	c & $408$ & $375$ & $92999$ & $0$  & $28484$ & $55889$\\
  %           \bottomrule
		% \end{tabular}
              \begin{tabular*}{\textwidth}{l c r}
             \hspace{3.4cm} \textbf{Users} & \hspace{1.4cm} \textbf{Ratings} &  \hspace{1.65cm}\textbf{Songs} \\
            \end{tabular*}
            \begin{tabular}{p{1.5cm}|p{1cm} p{1cm} | p{1cm} p{1cm} | p{2cm} p{2cm}}
			\textbf{Treatment} & All & Filtered & All & Filtered & All & Filtered \\
			\midrule
			a & $429$ & $389$ & $122225$ & $59100$  & $49598$ & $5454$\\
			b & $331$ & $308$ & $77758$ & $38946$  & $35213$ & $5397$\\
			c & $408$ & $375$ & $92999$ & $43633$  & $42615$ & $5418$\\
            \bottomrule
		\end{tabular}
	\end{center}
	\caption{A total number of users, ratings, and songs (all and filtered) for each treatment group in the RCT.}
	\label{tab:stats}
\end{table}

 \paragraph{Natural experiment: introduction of timers.} Prior to February 21, 2021, users could rate a song as soon as the video started playing; we refer to this data as \emph{Pre-timers dataset}. After the launch (and for at least the 6 months following the change): the dislike button became active after 3 seconds, the like button after 6 seconds, and the superlike button after 12 seconds; the data collected during this period is referred to as the \emph{Post-timers dataset}. We analyze data for the six months before and after the introduction of the timers, filtering out users and songs with less than 10 ratings each. The Pre-timers dataset includes \num[round-precision=0]{396} users, \num[round-precision=0]{8592} songs, and \num[round-precision=0]{141096} overall ratings, while the \emph{post-timers dataset} includes \num[round-precision=0]{1071} users, \num[round-precision=0]{11536} songs, and \num[round-precision=0]{100818} overall ratings. 

\paragraph{RCT design.} The test was conducted with three treatment groups, each of which had a different time delay before the "like" rating could be unlocked. Similar to the post-timer period in the natural experiment, dislike lock time is always 3 seconds, and the superlike lock time is always 12 seconds. In Treatment a, the like button is unlocked after 3 seconds, i.e., at the same time as the dislike button. In Treatment b, after 6 seconds (same as in the post-timer period and before the experiment). In Treatment c, after 9 seconds. We filtered songs and users who provided less than 10 ratings during the experimental period.  
User profiles were randomly allocated to one of the three treatment groups, with Treatment A and Treatment C each receiving $\frac{3}{8}$ of the user profiles and Treatment B receiving $\frac{2}{8}$. Table~\ref{tab:stats} shows the number of users, songs, and ratings for each treatment group before and after data filtering..


% \todo{Need to integrate below}
% We now report and analyze the results of an RCT on the Piki platform, motivated by the results observed in the natural experiment. 

% The experiment and associated data provide us three benefits over the natural experiment: first, as an RCT, when comparing treatment groups we do not need to worry about confounding due to temporal factors or other platform changes. Second, it allows us to replicate the natural experiment results with more subtle changes. Third,  we can more closely analyze the effect of \textit{personalized} vs \textit{random} recommendations. In particular, during the (smaller time-period) experiment, the platform's recommendation algorithms were relatively stable; furthermore, the platform ensured that there would be a relatively \textit{dense} sub-matrix of songs and users---i.e., there was a set of 160 songs that were over-sampled for "random" recommendations to users. We analyze the experimental results in this section; in the next section, we report the results of further exploratory analyses during the same experimental time period on the interaction between personalized recommendations and ratings. 


% \subsection{Experiment setup}
% \paragraph{Treatments.}  

% \paragraph {{Treatment assignment}.} 


% %\rana{it indicates that we are doing the mapping, so add this to the github and refer to it}

% \paragraph {{Data}.}






% \label{sec:natural_exp}



% We found that changes to the Piki music app's interface, specifically a change to the ratings timer, affected the way users gave ratings and how picky they were.

% What I said above:
    % \item First, we analyze a natural experiment due to a product deployment which dramatically changed the user interface. Before the change, users were able to select each ratings option immediately after the song started playing. After the change, users could still ``dislike'' the song immediately but that to \textit{wait} several seconds before they could ``like'' or ``super-like'' it. We find that this change substantially -- and heterogeneously -- changed user rating behavior, on average inducing more ``pickiness'' than before and reducing across-user behavior variance. We further find evidence that this reduction in the heterogeneity of user behavior improved the effectiveness of the system. While a user's rating for a given item was \textit{more a function of the user who happened to be shown the item} than the item quality (as estimated by other ratings) both before and after the change, the change increases the relative importance of item quality. 

% Section outline:
% Motivation


\subsection*{Results}

  % Figure environment removed

  %  % Figure environment removed

%  {\renewcommand\normalsize{\tiny}%
% \normalsize
% \begin{table}[tb]
% \input{plots_nikhil/naturalexperiment_likeregression.tex}
% \caption{Regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song. Perhaps surprisingly, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating, especially before the interface design change. All dependent variables are standardized, and so coefficients should be interpreted as the change in the user-song-rating associated with a one standard deviation change in the independent variable. To minimize the effect of any single user or item in this regression and keep the number of ratings by each song and user comparable, we sample at most 100 ratings by any user or for any item. On average, in the post-timer period, each song has 31 ratings; users have on average 29. For the pre-timer period, these numbers are 21 and 22, respectively. Qualitatively similar results emerge for other filtering techniques. }
% \label{tab:natexpregression_explainratings}
% \end{table}
% }

%  {\renewcommand\normalsize{\tiny}%
% \normalsize
% \begin{table}[tb]
% \input{plots_nikhil/naturalexperiment_meanregression.tex}
% \caption{Regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. As in \Cref{tab:natexpregression_explainratings}, mean scores is more strongly associated by the \textit{user} average behavior than the \textit{song's} average rating, especially for songs with low numbers of ratings. All independent and dependent variables are standardized. On average, in the post-timer period, each song has about 16 ratings on average in each of the test and train set; users have on average 22 ratings in the train set. For the pre-timer period, these numbers are 11 and 17, respectively. (Note that the song numbers are exactly half that of \Cref{tab:natexpregression_explainratings}, as the test/train split is stratified by song.) Qualitatively similar results emerge for other filtering techniques.}
% \label{tab:natexpregression_multiple_ratings}

% \end{table}
% }

% \Cref{fig:mean_rating_before_after} shows the fraction of times each option (dislikes, likes and superlikes) was chosen, before and after change.\footnote{To minimize the effect of ``super-users,'' we first take calculate the option fractions per-user, and then take the mean across users. The raw rating distribution is qualitatively similar.} \Cref{fig:user_score_hist} is a histogram of the mean score given by each user, when ratings are encoded as dislike = 0, like = 1, super-like = 2. \Cref{tab:natexpregression_explainratings} contains regression results for a rating given by a user $u$ to a song $s$ versus the user's and song's (normalized) ratings given to other songs and by other users, respectively, alongside other control variables. %In the Appendix, \Cref{fig:split_corr} shows how an individual user's mean score changes by month; for each month, it shows the correlation between each user's mean score for that month and the following month, if the user has at least 10 ratings in each month. Together, these results illustrate the following. 

% \todo{include item ratings distributions in appendix?}


\paragraph{There is substantial heterogeneity in user rating behavior.} Users behave qualitatively different from each other both before and after the introduction of timers, and the effect cannot be explained by sampling variance alone. For example, \Cref{fig:user_score_hist} shows that raters differ in the mean ratings they provide items. In particular, especially before the change, there are users who would ``like'' every song they are shown, while other users are far more picky. 


 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/naturalexperiment_likeregression.tex}
\caption{For the natural experiment, regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song. Perhaps surprisingly, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating, especially before the interface design change. All dependent variables are standardized, and so coefficients should be interpreted as the change in the user-song-rating associated with a one standard deviation change in the independent variable. To minimize the effect of any single user or item in this regression and keep the number of ratings by each song and user comparable, we sample at most 100 ratings by any user or for any item. On average, in the post-timer period, each song has 31 ratings; users have on average 29. For the pre-timer period, these numbers are 21 and 22, respectively. Qualitatively similar results emerge for other filtering techniques. }
\label{tab:natexpregression_explainratings}
\end{table}
}

 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/naturalexperiment_meanregression.tex}
\caption{For the natural experiment, regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. As in \Cref{tab:natexpregression_explainratings}, mean scores is more strongly associated by the \textit{user} average behavior than the \textit{song's} average rating, especially for songs with low numbers of ratings. All independent and dependent variables are standardized. On average, in the post-timer period, each song has about 16 ratings on average in each of the test and train set; users have on average 22 ratings in the train set. For the pre-timer period, these numbers are 11 and 17, respectively. (Note that the song numbers are exactly half that of \Cref{tab:natexpregression_explainratings}, as the test/train split is stratified by song). Qualitatively similar results emerge for other filtering techniques.}
\label{tab:natexpregression_multiple_ratings}

\end{table}
}

\paragraph{A user-song rating is more a function of which users rate a song than the song itself.} As discussed in the introduction, such ratings behavior heterogeneity may affect items, whose quality estimates depend on the ratings users provide.

% In \Cref{tab:natexpregression_explainratings}, 
We first analyze a regression as shown in Table~\ref{tab:natexpregression_explainratings} to predict a \textit{single} user-item rating as a function of that user's mean rating to other items, and that item's mean ratings by other users. We find that the user's average behavior has a stronger correlation with the individual rating than does the item's average quality as rated by other users. Specifically, in the post-timer period of the natural experiment, a one standard deviation increase in the mean user rating is associated with a \num{0.279} increase in the individual rating, while a one standard deviation increase in the mean item rating is only associated with a \num{.114} increase. Qualitatively similar results emerge for other exact regression specifications.\footnote{For example, one concern is that is the median user rates more songs than the median song receives ratings, and so our estimate of the mean user rating is less noisy, driving the result. However, the result does not change with different filtering/limitations of the number of ratings used for each user and song to calculate the corresponding dependent variables. Furthermore, the interaction terms between the mean ratings and the counts indicate that such an effect is second order. } The RCT yields qualitatively similar results.


One limitation of analyzing a \textit{single} rating is that -- while a single rating may be more a function of the user than the item -- effects may cancel out after multiple ratings, and so an item's estimated quality may indeed be reflective of their true quality. 
To study this effect further,
% in \Cref{tab:natexpregression_multiple_ratings}
we employed a procedure where we randomly divided the dataset into two groups, stratifying by both before/after the introduction of timers and song to ensure that an equal number of each song's ratings in each period were present in each group. For each song, we further calculate \textit{mean user rating train} -- the mean of the users' mean train set scores, for users who rated that song in the test set. Finally, we regress \textit{mean user rating train} and the song's mean rating in the training set to the song's mean rating in the ``test'' dataset, alongside controls. Conceptually, this specification tests consistency: is a song's mean score after multiple ratings (in the test dataset) more consistent with the behavior of users who gave those ratings or that song's score as rated by other users?\footnote{Any empirical estimate of true item quality is a function of the data generating process (e.g., user interface) and estimation method, and so cannot be used as (even approximate) ground truth of quality in papers that seek to change the interface. \citet{garg2021designing} address this challenge by using a revealed preference `objective' notion of quality independent of the user interface: whether the freelancer was hired again by the same client. Such a measure is not available in our setting. We thus use internal consistency within a treatment group on a train-test set, as the measure that approximates `consistency to ground truth.'}

Our analysis indicates that a mean song's score remains significantly correlated with user behavior, rather than the quality of the song as assessed by other users, even after tens of ratings, as shown in Table~\ref{tab:natexpregression_multiple_ratings}. For instance, in the post-timer period, these correlations are \num{.478} and \num{.3}, respectively (with all independent variables standardized). Furthermore, the effect lessens but does not disappear for songs with many train set ratings: analyzing the interaction terms based on the number of song and user ratings in the train set, respectively, a one standard deviation increase in the number of train set song ratings (about  14 ratings) approximately equalizes the respective coefficients (however, an increase in the number of train set user ratings also increases the association with the mean train set users' ratings\footnote{The standard deviation in number of train set user ratings is about 5, and so the respective coefficient on a ``per-rating'' basis is $\frac{.0432*14/5}{.198}\approx .6$ that of the interaction term on number of user ratings.}).

Together, these results establish that ratings are not a reliable estimate of item quality: they fail even basic \textit{consistency} checks, being more strongly self-consistent with user behavior than with item quality as measured by ratings.


% \nikhil{talk about filtering for the above}

% \todo{edit below}

% For the following specification
%     \begin{align*}
%     \mathrm{Rating}_{us} \sim \mathrm{I_{PreTimer}} + (\mathrm{MeanRatingToOthers}_i + \mathrm{MeanRatingFromOthers}_s)\times \mathrm{I_{PreTimer}} \todo{other terms}
%     \end{align*}

% \todo{mention the user factor thing, put regression table in appendix}Next, we evaluated the regression of song mean ratings using song and user factors of each dataset. To that end, we used matrix factorization~\cite{} with one feature to extract the song and user factors from the data.
% Matrix factorization is a technique used in collaborative filtering for recommender systems, which decomposes a large and sparse rating matrix into two smaller and denser matrices, one representing the users and the other representing the items.
% The goal is to use these extracted factors to predict song mean ratings in both datasets, \emph{pre and post change}.
% Table~\ref{tab:pre_change} and Table~\ref{tab:post_change} present the regression results of the pre and post change datasets, respectively. These tables demonstrate a clear distinction between the two datasets, which provides evidence that the interface design has a significant impact on the prediction of song ratings.


\paragraph{Ratings interface induces user behavior change.} Finally, our findings show that the ratings interface has a significant impact on user behavior; \Cref{fig:mean_rating_before_after} indicates that the overall number of ``dislikes'' rose on the platform, as expected -- users now have to spend additional time listening in order to give a ``like'' or ``super-like.'' The results from the RCT in \Cref{fig:mean_rating_exp_all} replicate this finding. These overall changes also translate to user-level changes. As shown in \Cref{fig:user_score_hist}, the change was particularly effective in substantially reducing the number of generous users who gave a ``like'' to every selected song -- from about 10\% of users to about 3\%.%\footnote{One limitation of the natural experiment is that the overlap in users before and after the change is not huge, as user-churn on the platform was high in the 12 month window before and after the interface change. \Cref{fig:split_corr} shows -- for each user with at least 10 ratings in consecutive months -- the correlation between their mean user rating each of the months. While the confidence intervals are wide because of user churn, the plots suggest that this individual-user-level correlation substantially dropped for the month before and after the interface change, from around \num[round-precision=1]{0.8} to around \num[round-precision=1]{0.4}.}


These user-level heterogeneity changes further increase how informative ratings are about items as opposed to the users who rated them. Specifically, we observe that the introduction of timers had a noticeable effect on the coefficients in the regression for a single rating. Before the introduction of timers, in the regression for a single rating for a song by a user, the coefficients on the mean user's ratings for other songs is \num{0.278} versus \num{.065} for the mean song's ratings by other users; after timers, these numbers are \num{0.279} and  \num{.114}, respectively, and the interaction between the Pre-Post indicator and mean song rating by others is statistically significant.
These findings highlight the importance of interface design, especially when user behavior is heterogeneous.
% The results are directionally the same in when regressio, though the change in association with mean song rating before and after timers has smaller effect size and is not statistically significant -- suggesting a limitation to how much change interface design alone can induce.  %In other words, careful interface design makes average ratings more a signal of song quality than of the users who gave those ratings. 


% The effect is even more pronounced \Cref{tab:natexpregression_explainratings} where mean song scores have a stronger correlation with user behavior than with scores for the same song given by other users (\todo{numbers}) before the introduction of timers; after timers, the the coefficient on the train set song score is higher, \todo{numbers}.




% \section{Randomized control trial: further manipulation of timers }
% \label{sec:experiment}




% % \begin{table}[th]
% % 	\begin{center}
% % 		\begin{tabular}{l|ccr}
% % 			\toprule
% % 			\textbf{Treatment} & \textbf{Users} & \textbf{Songs} & \textbf{Ratings}\\
% % 			\midrule
% % 			a& 347  & 49437 & 121327\\
% % 			b& 275 & 35100 & 77225\\
% % 			c& 334 & 42499 & 92229\\
% % 			\bottomrule
% % 		\end{tabular}
% % 	\end{center}
% % 	\caption{Number of users, songs and ratings in each treatment groups after the pre-processing}
% % 	\label{tbl:filtering}
% % \end{table}

% \subsection{Experiment Results}

% %   % Figure environment removed



% % \rana{summary of the results across subsections}



%  {\renewcommand\normalsize{\tiny}%
% \normalsize
% \begin{table}[tbh]
% \input{plots_nikhil/experiment_likeregression.tex}
% \caption{Analogue of \Cref{tab:natexpregression_explainratings}, but for the RCT. Regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song. As before, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating. Treatment c is slightly (but statistically significantly with $p < .05$) more informative in terms of the coefficient on the mean song rating by other users. On average, for each treatment group, each song has about 23 ratings; users have on average 37. Qualitatively similar results emerge for other filtering techniques.}
% \label{tab:experiment_regression_explainratings}
% \end{table}
% }


%  {\renewcommand\normalsize{\tiny}%
% \normalsize
% \begin{table}[tbh]
% \input{plots_nikhil/experiment_meanregression_withpersonalized.tex}
% \caption{Analogue of \Cref{tab:natexpregression_multiple_ratings}  for the RCT. Regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. The mean-song-rating coefficient is approximately the same as the mean-user-rating coefficient, suggesting that both song quality and user behavior contribute to a song's mean rating. Furthermore, we add a term for the fraction of times in the test set that a song appeared via \textit{personalized} recommendations; this fraction has a strong positive association with the average song rating in the test set, even controlling for song quality as determined in the training set. We find no significant differences between treatment groups. All independent and dependent variables are standardized. On average, in each treatment group, each song has about 12 ratings on average in each of the test and train set; users have on average 23 ratings in the train set. Qualitatively similar results emerge for other filtering techniques.}
% \label{tab:experiment_regression_explainratings_multiple_withpersonal}
% \end{table}
% }

% % \nikhil{distribution plot like last time}
% % \nikhil{Regressions for just like, and for mean ratings, and mean ratings while taking into account personalized -- maybe all in appendix or mostly}

% The results largely replicate those of the natural experiment, except that the effects are (as expected) smaller, given the smaller differences between groups.



% % First, changing the interface changes user ratings behavior. \Cref{fig:mean_rating_exp} shows the ratings distribution -- the delay in showing the "like" timer changes the overall distribution. Interestingly, longer delays cause users to substitute to \textit{super-likes} (instead of, primarily, \textit{dis-likes}  as expected); Treatment c has the largest superlike ratio and longest "like" delay---this result can be explained by the fact that users who have already invested 9 seconds face a shorter wait to "super-like" the song. In turn, as shown in Appendix \Cref{tab:exp_score_variance}, the treatment groups differ in user-level mean rater scores -- in particular, Treatment c \textit{increases} user-level ratings variance, i.e., increases user behavior heterogeneity. 

% Second, ratings are more informative about the raters than they are about songs; ratings behavior differences across treatments affect this relationship to a lesser extent than with the more radical change of introducing timers altogether. \Cref{tab:experiment_regression_explainratings} shows the analogue of \Cref{tab:natexpregression_explainratings} for the RCT: the regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song, along with interaction terms for the treatment group. As before, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating, Treatment c is slightly (statistically significantly with $p < .05$) more informative in terms of a higher coefficient on the mean song rating by other users, but the effect is not first-order. 

% Similarly, \Cref{tab:experiment_regression_explainratings_multiple_withpersonal} shows the analogue of \Cref{tab:natexpregression_multiple_ratings} for the RCT: regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. The mean-song-rating coefficient is approximately the same as the mean-user-rating coefficient, suggesting that both song quality and user behavior contribute to a song's mean rating even after several ratings.  We find no significant differences between treatment groups -- any difference in informativeness between treatment groups disappears after each song receives several ratings. 

% Overall, these results replicate those from the natural experiment. However, the effects from the (more modest) changes in timer delays are smaller than those from the initial timer introduction -- further suggesting a limit to the controllability of ratings distributions through interface design alone. 

\section{Personalized recommendations and ratings}
\label{sec:personalized}


The previous section studied the interaction between user rating behavior and song quality estimates. Next, we analyze how\textit{ personalized recommendations} affect ratings and song quality estimates. At a high level, it is well known that personalized recommendation algorithms can impact item quality estimates. For instance, taking sample means from adaptively collected data, as in a multi-armed bandit, can introduce a negative bias \cite{nie2018adaptively}. Additionally, marketplaces (including the one we study) typically employ \textit{multiple} recommendation algorithms in parallel, such as personalized home screens, search results, "Trending" items, and exploration-motivated randomized recommendations. The coexistence of multiple recommendation algorithms creates a complex ecosystem in terms of how items are recommended to users: if an item is only shown to users as a result of hyper-personalization, its resulting quality estimates will be high; on the other hand, if the same item is only shown to random users regardless of their preferences, its resulting quality estimates will be low.  


In other words, the extent of personalization, particularly if this is heterogeneous across items (such as some items are shown in "Trending" but others are not), affects item quality estimates. (Note that such effects can also occur with a single personalization algorithm, based on the quality of recommendations it can provide using each item.)

In this section, we quantify such effects: how do personalized recommendations affect ratings and item quality estimates? We can do so, as a unique feature of the Piki dataset is that we know whether the song was shown as a result of a personalized recommender or at random. %We can thus further study the effects of personalization on ratings and item quality estimates.




% Now consider the other direction, how user rating behavior affect personalized recommendation. \citet{garg2019designing,garg2021designing} theoretically show how the distribution of ratings (in particular, the joint distribution of true item quality and the ratings the item receives) affects the \textit{learning rate} of the platform, i.e., how quickly it can identify high quality from lower quality items. However, it is unclear how such quality estimate differences translate to \textit{recommendations}, when ratings are used as input data to personalized recommendation algorithms.

% In this section, we study these two effects. \Cref{sec:personalizedtoitemquality} studies the first direction, and \Cref{sec:sim} the second direction.

% \subsection{How do personalized recommendations affect ratings and item quality estimates?}
% \label{sec:personalizedtoitemquality}

   % Figure environment removed

 

 
We start with a basic observation: ratings after a song is shown due to a personalized recommendation are higher than after a song is shown due to randomized recommendation.\footnote{Note that users are not aware of why a particular song is shown, and so the effect is not due to a placebo effect of the user `expecting' to like such a personalized recommendation more.} For songs with more than 10 ratings due to each of random and personalized recommendation, \Cref{fig:exp_personalizedrandomhist} shows the histogram of mean song scores under each of personalized and random recommendation. \Cref{fig:exp_personalizedvsrandom} then shows the individual-song level relationship (for each song, the mean score under personalized and random recommendation, respectively). As expected, scores are higher under personalized recommendation, indicating the algorithms are working. There is a generally positive relationship at the song level, but perhaps weaker than expected. 

However, this observation -- that personalization leads to higher ratings -- induces the next observation: estimated song quality is \textit{also} a function of whether a song is shown more often as a result of personalized or random recommendation. (Note that heterogeneity in how often a song is shown via personalized recommendation can be a function of randomness, system design choices, and historic ratings data).  As an initial view of this effect, consider \Cref{fig:exp_personalizedvslike}: the higher the fraction of times that an item is shown due to personalized recommendation (versus randomized), the higher its estimated song score. Of course, causality can be in either direction: a higher mean song score may mean that a personalized recommender is more likely to recommend that song, and being shown through personalized recommendation results in higher mean scores. 

To further analyze the effect, we run a regression as follows: mean song scores on a test set, versus user mean ratings and that song's score on a train set, further adding as a covariate a term for the fraction of times that a song was recommended via personalization in the test set. This fraction has a strong positive association with the average song rating in the test set, even controlling for song quality as determined in the training set. In other words, the more that a song was shown via personalized recommendation in the test set, the higher its test set song quality estimation score, even controlling for train set score. This analysis shows that the frequency with which songs are displayed through personalized recommendations, even after conditioning on true quality, can significantly impact the platform's learning about items.%This analysis thus shows that -- if songs differ in how often they are shown via personalized recommendation, even conditional on true quality -- this can have a large effect in what the platform learns about items. 
