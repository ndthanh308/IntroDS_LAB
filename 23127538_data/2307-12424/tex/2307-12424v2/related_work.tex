\subsection{Related Work}
\label{sec:related_works}
This paper sits at the intersection of the literature of several communities: rating inflation, interface design and recommendation systems, and the economic implications thereof. % relies on prior work in various areas, specifically measurement error and data collection, recommendation systems with aspects of learning with implicit feedback, and interface design. %For the simulation part, we use RecLab~\cite{krauth2020offline}, which is a flexible simulation framework for evaluating recommender systems in dynamic environments.


% 	1. Measurement error and survey design generally, point out heterogeneity is a particular issue.
\textit{Ratings inflation.}  Ratings are examples of \textit{measurements}, i.e., assignments of numbers to the construct (`how much a user enjoyed an item') that we aim to measure (see, e.g., \citet{allen2001introduction} for an overview). The literature discusses many potential reasons that measurement may not be an accurate reflection of the underlying construct, including errors in the measurement process, the use of imprecise or inadequate measurement tools, and the influence of external factors on the measurement (see \citet{bound2001measurement} for a review on biases due to measurement error in survey data).

The rating literature primarily considers \textit{inflation} error: empirically, ratings on marketplaces tend to be high, with most items (sellers, producers, freelancers) receiving positive ratings the vast majority of the time (see, e.g., \citet{filippas2018reputation}). Such inflation has substantial downstream economic consequences, in particular, that high-quality items are difficult to distinguish from lower-quality ones. 
\citet{Aziz2020TheCO}
conclude from a quasi-experiment on a restaurant ratings platform, that rating inflation leads to less exploration of new restaurants and a greater concentration of sales to more popular restaurants. To add to this literature, our work documents \textit{heterogeneous} ratings inflation and partially addresses it via interface design.
%This paper, in contrast, additionally considers measurement \textit{heterogeneity}: different users may assign different numeric ratings to qualitatively similar underlying experiences. 

% To minimize measurement error, the design of the data collection process can also influence the types of data that are collected and the completeness and representatives of the resulting dataset.

% 		b. My general survey lit review from other paper, can also point to it

% 	2. Ratings + recommendation interfaces, design for these
% 		a. My ratings works -- most related
% 		b. The other implicit feedback, etc.

\textit{Data interface designs.} There is similarly a large literature on the design of data collection processes to minimize measurement error (see e.g., \citet{krosnick2018questionnaire} for a survey on questionnaire design). Given the importance of implicit feedback in computational systems \cite{claypool2001implicit,hu2008collaborative,jiang2015automatic,radlinski2008does,hongyiwen}, there has been substantial work on understanding \cite{kim2014modeling, liu2010understanding,huang2012user} and \textit{shaping} \cite{schnabel2019shaping} implicit feedback through interface design. Similarly, user interfaces shape how users provide and perceive explicit ratings and recommendations \cite{pu2008user,adomavicius2011recommender,cosley2003seeing,felfernig2007knowledge}. Most related is the work in rating system interface design, especially to counter inflation in marketplaces. \citet{garg2021designing} report the result of a randomized controlled trial in which the rating interface was changed. The study finds that the positive-skewed verbal rating scales lead to rating distributions that significantly reduce rating inflation and are more informative about seller quality than numeric ratings. Similarly, \citet{garg2019designing} derive the \textit{optimal} rating distribution (joint distribution between seller quality and probability of positive rating in a binary setting), when the goal is to accurately learn about item quality. Other solutions to rating inflation include modifying incentives or behavioral effects that are conjectured to cause inflation \cite{fradkin2015bias}. This paper adds to this literature by showing how interface design interacts with user heterogeneity and personalized \textit{recommendation}. 



% 	3. Other approaches for User heterogeneity -- post-processing (ebay paper) item response models, matrix factorization? User behavior embeddings? 
% Include the baseball calibration/heterogeneity work from other paper, other ratings design papers

\textit{Post-processing solutions to mis-measurement.} A distinct approach to correcting for measurement error (ratings inflation or consequences of heterogeneous behavior) is to \textit{post-process} the data. Unbalanced explicit training datasets have been documented in the literature and some flattening techniques have been shown to improve recommendation quality~\cite{mansoury2021flatter}. When training algorithms on implicit data, it is natural to sample items that were not consumed as negative feedback \cite{hu2008collaborative}. \citet{stoikovwen} show this approach performs significantly worse than a matrix factorization algorithm trained on explicit like/dislike data. \citet{nosko2015limits} on eBay notice that \textit{missing} ratings are a negative signal of quality, and so dividing a seller's number of positive ratings by the total number of \textit{sales} as opposed to \textit{ratings} provides a stronger signal of seller quality. Outside of a rating context, item response theory \cite{embretson2013item} models are used to simultaneously learn \textit{question difficulty} and \textit{test-taker skill level}, based only on data on how each test-taker performed on each question. Our results point to the importance of using such `rating the rater' methods to process rating data.  

%\paragraph{Personalized recommendations and system dynamics} Finally, several works discuss personalized recommendations and resulting feedback loops affect analyses and the function of ratings systems (see, e.g. \cite{dean2022preference,krauth2020offline}). Most famously, \citet{salganik2006experimental} show through an artificial cultural platform how early popularity can create substantial long-term differences between otherwise similar items. Our work analyzes the interaction of these effects and the platform's ratings system.


% \nikhil{talk about challenges in studying personalized recommendation! that collecting datasets is hard, ours is unique here}

% In practice, in baseball, scouts are known to rate heterogeneously (conditional on player quality, scouts may systemically differ in their scores) \cite{from other paper}, and so teams are known to employ calibration techniques \cite{}.

% \todo{edit below} 
%KuaiRec: A Fully-observed Dataset and Insights for Evaluating
%Recommender Systems

% \subsection{Measurement Error and Data Collection}
% Measurement error refers to the difference between the true value of a variable and the value that is observed or recorded during data collection. This error can occur for a variety of reasons, including errors in the measurement process, the use of imprecise or inadequate measurement tools, and the influence of external factors on the measurement.
% ~\cite{allen2001introduction} gives a balanced and accessible overview of measurement statistics. The authors discuss the design and interpretation of psychological tests and scales, as well as a critical examination of classical true-score theory and an explanation of modern measurement models, controversies, and developments.
% ~\cite{bound2001measurement} reviews the literature on biases due to measurement error in survey data, including the classical assumption and the more general case. The authors argue that standard methods can be used to obtain bounds on measurement error bias and that validation studies can assess the magnitude and classical assumptions of measurement errors and provide an alternative strategy for reducing or eliminating bias.


% Data collection
% To minimize measurement error, the design of the data collection process can also influence the types of data that are collected and the completeness and representatives of the resulting dataset.
% In~\cite{garg2021designing}, the authors investigate whether altering the meaning and relative importance of rating levels in a rating system can lead to less inflated, more informative ratings on platforms. To do this, they conduct a randomized controlled trial on an online labor market, adding an additional question to the feedback form and varying the question phrasing and answer choices in different treatment conditions. The study finds that the positive-skewed verbal rating scales used in some treatment conditions lead to rating distributions that significantly reduce rating inflation and are more informative about seller quality. The results of this study suggest that it is possible to design rating systems that are informative in practice and demonstrate a principled approach for doing so.



%Recommendation systems
% \subsection{Recommender Systems}

% Recommender systems are a vital component of many intelligent and personalized services such as e-commerce sites, news content hubs, voice-activated digital assistants, and movie streaming portals. Users interact with the recommender system through a graphical user interface (GUI), which serves two main purposes. First, the GUI presents the predictions made by the machine learning system (e.g., personalized news streams, interesting items). Second, the GUI captures feedback data (e.g., clicks, ratings, or other signals arising from user interactions) and passes it back to the machine learning system in the back-end.

% There are three main approaches to recommending products and services: collaborative techniques, content-based recommendations, and knowledge-based recommendations. Collaborative techniques~\cite{konstan1997grouplens} generate recommendations by identifying the nearest neighbors whose rating behaviors are similar to the current user's and recommending items that the current user is not familiar with but that have been rated positively by the nearest neighbors. Content-based recommendations~\cite{pazzani1997learning} are based on the similarity between a user's preferences (as stored in a user profile) and the item descriptions. Knowledge-based recommendations determine items of relevance to the user either by interpreting an explicitly defined set of filtering rules~\cite{felfernig2008constraint} or by considering the similarity between a set of explicitly defined user requirements and the elements of the underlying item set~\cite{burke2000knowledge}.
% The survey in~\cite{bobadilla2013recommender} overview of recommender systems.

% \subsubsection{Implicit Feedback in Machine Learning}


% High-quality feedback data is crucial for interactive systems that learn over time. Implicit feedback, which is both easily accessible and nonintrusive, has been a major driving force in the development of improved interactive systems~\cite{claypool2001implicit, hu2008collaborative, jiang2015automatic}.
% Since implicit feedback data only indirectly reveals people's preferences, much research has focused on understanding the quality of feedback signals in different settings, i.e., how well implicit feedback reflects explicit user interest. Conventional mouse clicks often reflect preferences among search results~\cite{radlinski2008does}, and mouse hover events are a good proxy for gaze~\cite{huang2011no}. There has also been research on alternative signals such as dwell time~\cite{kim2014modeling, liu2010understanding}, cursor movements~\cite{huang2012user, huang2011no, zen2016mouse}, and scrolling~\cite{guo2012beyond, liu2017scroll}.
% ~\cite{schnabel2019shaping} investigates how foraging interventions in the system interface can shape implicit feedback, rather than studying the origins of implicit feedback signals in fixed interfaces


% \subsubsection{Interface Design for Machine Learning}
% There has been a significant amount of research on the intersection of user experience (UX) design and machine learning, with a focus on improving explicit feedback elicitation. Examples include~\cite{alagarai2014cognitively}, which demonstrates that different visual designs can reduce the cognitive and perceptual burden of crowd workers in text extraction tasks, resulting in increased accuracy of responses.
% \cite{kulesza2014structured} presents an interface that supports users in concept labeling tasks, allowing for dynamic adaptation of concepts during a task. ~\cite{cosley2003seeing} and~\cite{adomavicius2011recommender} analyzed the impact of presentation of ratings on user behavior in collaborative filtering recommenders, finding that ratings were higher when the user interface presented inflated predictions.
% ~\cite{pu2008user} provided an analysis of common design pitfalls in developing user interfaces for preference elicitation, revision, and explanation, and~\cite{felfernig2007knowledge} examined the impact of different recommender interface functionalities on factors such as perceived increase of domain knowledge, increase of usability, and trust.


