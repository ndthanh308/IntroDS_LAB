\subsection{The Piki Music App Interface}
%\subsection{The Piki Music Dataset}

\label{sec:platform}

We analyze data from Piki \footnote{The dataset is available at \url{https://github.com/sstoikov/piki-music-dataset} }, a mobile application designed for music discovery. The application presents users with sets of 30-second music video clips in a sequential manner, similar to social media stories. Users are \textit{required} to provide explicit ratings, selecting from options such as ``disliking,'' ``liking,'' or ``super-liking'' for each song. Until a rating is provided, the clip plays in a loop, even across user sessions. Immediately after a rating is provided, the user is presented with the next song. Notably, users do not have access to a search bar and thus have no direct control over the songs they hear. Super-liked songs are saved to a playlist that can be exported to other applications.

The songs presented to users are chosen using one of several recommendation algorithms, which fall into two categories: "random" algorithms that do not use individual user data and "personalized" algorithms that do. Examples of the algorithms used in the application include a randomization technique over a small set of songs to create a dense user-item matrix, a "top-K" system that recommends the most popular songs overall, and a matrix factorization-based recommender. %We note that the implementation details of these algorithms differ over time in a manner that is not fully known. %\nikhil{not true during experiment}


As the app focuses on discovery, it has a higher ratio of songs to users than other rating datasets. Therefore, most songs have a "cold start" problem, with a small number of unique ratings. %To mitigate the biases that may arise from user heterogeneity. 
In such a setting, generous users who ``like'' all songs indiscriminately may create an artificial bias toward the songs that they have been exposed to. A further concern is that some users may provide feedback randomly rather than according to their true preferences, just to finish a set of songs and get rewards (note that Piki users may choose to receive micro-payments for their ratings).\footnote{Such behavior is a (perhaps extreme) example of the type of heterogeneous and strategic use behavior on many platforms, that might influence both implicit and explicit ratings that they provide.} %The columns of the dataset are described in \Cref{tab:dataset_cols}.
%
This platform provides several distinct advantages for studying our research questions: (a) it collects explicit ratings, and users must rate songs before continuing; there is thus little selection bias in ratings, conditional on a user being shown the song; (b) it deploys multiple recommendation algorithms of varying levels of personalization, and data is logged in terms of which algorithm led to a song recommendation. These characteristics allow us to focus our attention on the role of user rating heterogeneity, personalized recommendation, and the interface collecting the ratings.

% \nikhil{I feel that we want a distinct: why this platform, what is unique about our dataset, that we can't do in other places} \nikhil{we also want to center personalized vs recommendation that we know -- why this dataset, what is unique. call the dataset unique} The Piki dataset we analyze is an explicit music feedback dataset. Among music datasets, it is unique in that it consists in explicit feedback, as opposed to the implicit feedback collected by steaming apps, i.e. the number of times a song has been played by a particular user. Among explicit datasets, it is unique in that Piki users are rating while listening to the songs. Other explicit datasets like e-commerce, movie, restaurant and hotel ratings are typically collected after they are experienced. 


% The Piki Music dataset contains $8,575$ anonymized users, $186,627$ anonymized songs and $1,345,812$ ratings. \todo{are these numbers that we analyze??} %\todo{where are these numbers coming from? we can just report the numbers for each section in that section} 


% Figure environment removed

%\todo{discuss cold start? that many many songs, data is used/sold?}

\vspace{-0.15cm}
\paragraph{Piki Interface.} In an effort to mitigate rating inflation and incentivize ratings in line with a user's true opinions, Piki implemented a set of timers on each of the rating options on February 21, 2021. 
%Figure~\ref{fig:interface} displays the application interface. In particular, the key design choice is that, since February 21, 2021,
Since then, users have not been able to \textit{immediately} rate a song after it begins playing. 
Instead, each of the ``dislike,'' ``like'' and ``super-like'' buttons appear sequentially in that order, several seconds after the song begins playing or the previous button appears. Figure~\ref{fig:interface} displays the application interface.
The timer to unlock the dislike option ensures that users give a chance to each song; the timer to unlock the like option ensures that users are willing to invest extra time in that song; the timer to unlock the superlike option ensures that only raters who invest the significant time will get to save the song. The exact timing is a platform design choice that influences rating behavior and -- as we show -- downstream outcomes. This work first leverages the introduction of the delay timers and then an experiment that further varied the delay time across treatment groups.

% \textit{Ethics statement.} We leverage a product change and tests conducted by the Piki platform to improve its platform for users. The product change poses minimal risk to users, and users were able to leave the platform at any time.

% \todo{older text}











% We leverage data from \textit{Piki}, a music discovery application -- users are presented songs sequentially and are asked to rate them with a "dislike," "like," or "superlike" before being presented the next song. Songs are selected by both personalized and non-personalized recommendation algorithms, in a manner that is known. The most unique aspect of the interface is a set of timers associated with the dislike/like/superlike options.



% The Piki dataset has a similar structure to datasets produced by explicit ratings like Netflix and Yelp, or implicit ratings like Spotify or YouTube. 

% % (see \Cref{tab:dataset_cols} for a description of the dataset columns). 

% However, the circumstances that motivate Piki users to give ratings are very different from those of users of explicit ratings apps or streaming apps.
% First, Piki users are paid small amounts of cash, to rate a large amount of ratings. This incentivizes them to rate, even if some of the songs are not to their liking.
% Second, Piki users don't have access to a search bar, which could naturally lead to a popularity bias.
% Third, a system of timers incentivizes them to rate more uniformly, despite the very diverse behaviors of people rating a very subjective media like music.
% %To understand it is worth considering the circumstances and incentives that motivate users to give ratings (explicitly or implicitly). 
% The dataset produced by Piki can control for common causes of inflation in ratings data: selection bias, algorithmic bias and heterogeneity in the users. 


% % \begin{table}[tbh]
% % 	\begin{center}
% % 		\begin{tabular}{p{3cm}|p{8cm}}
% % 			\toprule
% % 			\textbf{column} & \textbf{Description}\\
% % 			\midrule
% % 			\textbf{timestamp}& a datetime variable \\
% % 			\textbf{user id}& an anonymized user id \\
% % 			\textbf{song id}& an anonymized song id \\
% % 			\textbf{liked} &feedback indicator, 2 if the song is superliked, 1 if the song is liked, or 0 if the song is disliked\\
% %             \textbf{personalized} & personalized indicator, 2 if the song was by a previously superliked artists, 1 if the song was recommended based on their previous choices or 0 if the song was selected randomly\\
% % 		%	\textbf{spotify popularity}& this is the song’s artist’s popularity, a value between 0 and 100, with 100 being the most popular. It is published by Spotify for each artist, through their publicly-available API \\
% %             \bottomrule
% % 		\end{tabular}
% % 	\end{center}
% % 	\caption{Piki dataset columns %\todo{do we use spotify popularity at all? no, right?}
% %  }
% % 	\label{tab:dataset_cols}
% % \end{table}



% Piki users are sequentially presented with sets of 30-second music video clips. Users must provide explicit ratings, “disliking”, “liking,” or “super-liking” a song – until they do so, the clip plays in a loop. Immediately after a rating is provided, the user is presented the next song. Super-liked songs are saved to a playlist that the user may export to other applications.
% %The search bar is an important \textit{selection bias} and a potential source of inflation. %People will tend to search (and subsequently rate) items that are memorable, which are likely to be items that are more popular.

% The Piki interface presents users with a mix of random recommendations (top k), a personalized collaborative filtering algorithm, and a hyper personalized algorithm that only shows users songs by artists they have previously super-liked. By segmenting the source of the rated songs, we can control for \textit{algorithmic bias}. %It is likely that streaming apps that depend on the satisfaction of their subscribers tend to use hyper-personalized algorithms.

% Finally in an effort to mitigate the inflation in ratings caused by a few users who "liked" almost everything, Piki implemented a set of timers on each of the ratings options on February 21, 2021. Since the implementations of the timers, users have not been able to immediately rate a song after it begins playing. Instead, each of the “dislike,” “like” and “super-like” buttons appear sequentially in that order, several seconds after the song begins playing. %The timer to unlock the dislike option ensures that users give a chance to each song; the timer to unlock the like option ensures that users are willing to invest extra time in that song; the timer to unlock the superlike option ensures that only raters who invest significant time will get to save the song.
% Figure 1 illustrates the way the timers work and Figure 1 shows the significant role that the timers had on the ratings. 






