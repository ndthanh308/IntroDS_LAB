\begin{abstract}

Recommendation systems rely on user-provided data to learn about item quality and provide personalized recommendations. An implicit assumption when aggregating ratings into item quality is that ratings are strong indicators of item quality. In this work, we test this assumption using data collected from a music discovery application. Our study focuses on two factors that cause rating inflation: heterogeneous user rating behavior and the dynamics of personalized recommendations.
We show that user rating behavior substantially varies by user, leading to item quality estimates that reflect the users who rated an item more than the item quality itself. Additionally, items that are more likely to be shown via personalized recommendations can experience a substantial increase in their exposure and potential bias toward them. To mitigate these effects, we analyze the results of a randomized controlled trial in which the rating interface was modified. The test resulted in a substantial improvement in user rating behavior and a reduction in item quality inflation. These findings highlight the importance of carefully considering the assumptions underlying recommendation systems and designing interfaces that encourage accurate rating behavior.

%########################################################

%Recommendation systems rely on user-provided data to learn about the item quality and provide personalized recommendations. An implicit assumption when aggregating ratings into item quality is that ratings are in fact strong indicators of item quality. In this work, we analyze this assumption using data collected on a music discovery application. We focus on rating inflation caused by two factors: heterogeneous user ratings behavior and the dynamics of personalized recommendation. First, we find that there is significant variance in user rating behavior and, consequently, item quality estimates reflect the users who rated an item more so than the item quality itself. Second, we find that items are more likely to be shown via personalized recommendation benefits significantly, causing significant upward bias in these items. Finally, we leverage a randomized control trial in which the rating interface was modified and find that the user interface can substantially influence user rating behavior and mitigate inflation in the corresponding item quality estimates.

%########################################################

% We study how algorithmic bias and interface design can affect the statistics of ratings data. In particular, average ratings scores are often inflated, which deteriorates their informativeness. Using the Piki music dataset, we find that items recommended by personalized algorithms or items rated by less discriminating users can be significantly inflated. This may be harmful for cold start items, where these biases are most significant. This finding motivates us to devise a Randomized Control Trial (RCT), where users in 3 groups are given a different unlock time for the "like" button. The first group has a like button that activates after 3s, the second after 6s and the third after 9s. We find that a simple design choice can significantly nudge users towards being more discriminating. Making users invest time in listening to a song before a positive rating can be submitted may be helpful in designing interfaces that are able to control for inflation.

%We study how interface design can reduce ratings inflation, using the Piki music dataset. We first establish that there is a significant inflationary effect coming both from user heterogeneity and algorithmic bias. Items that were rated by less discriminating users were significantly inflated. Items recommended by personalized algorithms also contributed to inflating ratings. We investigate the role of timers in affecting user ratings behavior in a Randomized Control Trial. There are 3 ratings, dislike, like and superlike, where each rating progressively unlocks and modified the middle timer while keeping the timers on the dislike and super-like buttons. The higher rate of dislikes can be explained by the higher time investment required to like a song. The higher rate of superlikes can be explained as a sunk cost investment.

%Recommendation and rating systems rely on user-provided data to learn about item quality and provide personalized recommendations. An implicit assumption when aggregating ratings into item quality are that ratings are in fact strong indicators of item quality. \rana{The previous claim may be critical, consider mention interface impact instead}
%In this work, we analyze this assumption. In particular, we study heterogeneous user ratings behavior (whether users provide similar numeric ratings for similar experiences) and the dynamics of personalized recommendation as drivers of item quality estimates and downstream platform metrics. We leverage a natural experiment and a randomized control trial on a music discovery application, in which the ratings interface was modified. First, we find that user ratings behavior is indeed heterogeneous and, consequentially, ratings (and corresponding item quality estimates) reflect the users who rated an item more so than the item quality itself. Second, we find that the user interface substantially (and heterogeneously) influences user ratings behavior and corresponding item quality estimates. Finally, we conduct exploratory empirical analyses and calibrated simulations to analyze the relationship between personalized recommendations and ratings, and in particular demonstrate how ratings behavior changes can have substantial downstream consequences for the quality of personalized recommendations that a platform can provide. 



% The literature largely assumes that user \textit{rating} behavior is homogeneous -- .
% In this paper, we investigate heterogeneous user ratings behavior and its consequences, using data from a music discovery platform.
% Finally, we use 


% Recommendation and rating systems rely on user-provided data to learn about item quality and provide personalized recommendations.
% The underlying assumption is that ratings are informative about item quality

% user \textit{rating} behavior is homogeneous -- users provide similar numeric ratings for similar experiences.
% In this paper, we investigate heterogeneous user ratings behavior and its consequences, using data from a music discovery platform.
% We leverage a natural experiment and a randomized control trial in which the ratings interface was modified. First, we find that ratings behavior is indeed heterogeneous and, consequentially, ratings (and corresponding item quality estimates) reflect the users who rated an item more so than the item quality itself.
% Second, we find that the user interface substantially (and heterogeneously) influences user ratings behavior and corresponding item quality estimates.
% Finally, we use calibrated simulations to demonstrate that such changes have downstream consequences for the quality of personalized recommendations a platform can provide. 




% (heterogeneously) changing ratings behavior. 


% A large literature has emerged on how to estimate item quality and user-item preferences and recommend items -- given user data. 


% However, there is a lack of understanding of how the ratings data itself affects item quality estimation and recommendations.

% In this paper, we investigate how interface design affects user ratings in recommendation systems.Â 
% Specifically, we address two challenges: user-level heterogeneity in what ratings mean, and the effect of platform design choices on rating data and recommendation system accuracy.
% We conduct an empirical analysis using public data from the Piki music recommendation app, as well as simulate different interface designs. Our results show that changes in the user interface can substantially and heterogeneously change user rating behavior, improving the accuracy and effectiveness of the recommendation system. Our findings provide insight into how the ratings data affect item quality estimation and recommendations, and the importance of considering the ratings data in the design of recommendation systems.
\end{abstract}