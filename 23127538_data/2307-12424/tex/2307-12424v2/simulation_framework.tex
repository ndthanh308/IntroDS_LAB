% \section{Average song scores and sources of bias}
% \label{sec:personalized}

% In this section, we examine the relationship between user pickiness, personalized recommendations, and average songs scores.

% On the one hand, personalization dynamics interact well with item score estimates, as demonstrated in \cite{nie2018adaptively}.
% Furthermore, a standard feature of marketplaces (including the one we study) is that multiple recommendation algorithms run simultaneously, such as personalized home screens, search results, trending items, and exploration-motivated randomized recommendations. Multi-algorithms running in parallel create a complex ecosystem in which items are recommended. For example, if an item is only shown to users as a result of hyper-personalization, its resulting score estimates will be high; however, if the same item is displayed only to random users regardless of their preferences, its score estimates will be low. In other words, the extent of personalization -- and especially if this is heterogeneous across items (for example, some items are shown in "Trending" but others are not) -- affects item score estimates. 
% These effects may also occur within a single personalization algorithm, depending on the quality of recommendations provided for each item.
% On the other hand, \citet{garg2019designing,garg2021designing} have theoretically demonstrated that the distribution of ratings, particularly the joint distribution of true item quality and received ratings, influences the learning rate of a platform.
% These two effects are examined in this section. \Cref{sec:personalizedtoitemquality} studies the first direction, and \Cref{sec:sim} the second direction.
% %%%%%%%%%%%%%%%%%%%%%


% \begin{comment}
    
% The previous sections study the interaction between user rating behavior and item quality estimates. Next, we analyze how\textit{ personalized recommendations} affect ratings and song quality estimates, and vice versa.

% In one direction, it is well understood that the dynamics of personalization can affect item quality estimates; for example, that taking sample means from adaptively collected data (as in a multi-armed bandit) has a negative bias \cite{nie2018adaptively}. Furthermore, a standard feature of marketplaces (including the one we study) is that \textit{multiple} `recommendation' algorithms run in parallel: for example, personalized home screens, search results, "Trending" items, and exploration-motivated randomized recommendation. These multiple algorithms run in parallel induce a complex ecosystem of how items are recommended: if an item is only shown to users as a result of hyper-personalization, its resulting quality estimates will be high; on the other hand, if the same item is only shown to random users regardless of their preferences, its resulting quality estimates will be low.  In other words, the extent of personalization -- and especially if this is heterogeneous across items (for example, some items are shown in "Trending" but others are not) -- affects item quality estimates. (Note that such effects can also occur with a single personalization algorithm, based on the quality of recommendations it can provide using each item.)

% Now consider the other direction, how user rating behavior affect personalized recommendation. \citet{garg2019designing,garg2021designing} theoretically show how the distribution of ratings (in particular, the joint distribution of true item quality and the ratings the item receives) affects the \textit{learning rate} of the platform, i.e., how quickly it can identify high quality from lower quality items. However, it is unclear how such quality estimate differences translate to \textit{recommendations}, when ratings are used as input data to personalized recommendation algorithms.

% In this section, we study these two effects. \Cref{sec:personalizedtoitemquality} studies the first direction, and \Cref{sec:sim} the second direction.
% \end{comment}

% \subsection{How do personalized recommendations affect aggregate ratings?}
% \label{sec:personalizedtoitemquality}

%    % Figure environment removed

 

% A unique feature of the Piki dataset is its ability to discern whether a song was presented as a result of personalized recommendations or at random. As a result, we are able to examine the impact of personalization on ratings and item quality estimates in more detail.

% Initially, we observed that ratings following a song's presentation through personalized recommendations were higher than those from randomized recommendations. \footnote{Note that users are not aware of why a particular song is shown, and so the effect is not due to a placebo effect of the user `expecting' to like such a personalized recommendation more.}  \Cref{fig:exp_personalizedrandomhist} shows the histogram of mean song scores under each condition for songs with more than ten ratings, while \Cref{fig:exp_personalizedvsrandom} illustrates the relationship between individual songs. As expected, scores are higher under personalized recommendations, indicating that the algorithms are functioning as intended.


% This observation -- that personalization results in higher ratings -- contributes to the finding that estimated song quality is also influenced by whether a song is more frequently displayed due to a personalized recommendation or a random recommendation. That is, the frequency of personalized recommendations may vary according to the level of randomness, the design of the system, and the historical rating data. We demonstrate this effect by examining \Cref{fig:exp_personalizedvslike}, which shows that the more often an item is presented based on personalized recommendations (instead of randomized recommendations), the higher its estimated song score becomes. However, causality may occur either way: a higher mean song score could increase the likelihood that a personalized recommender will suggest that song, whereas being showcased through personalized recommendations could result in an increase in mean song scores.

% %##############################################
% \begin{comment}

%  A unique feature of the Piki dataset is that we know whether the song was shown as a result of a personalized recommender or at random. We can thus further study the effects of personalization on ratings and item quality estimates.
 
% We start with a basic observation: ratings after a song is shown due to a personalized recommendation are higher than after a song is shown due to randomized recommendation.\footnote{Note that users are not aware of why a particular song is shown, and so the effect is not due to a placebo effect of the user `expecting' to like such a personalized recommendation more.} For songs with more than 10 ratings due to each of random and personalized recommendation, \Cref{fig:exp_personalizedrandomhist} shows the histogram of mean song scores under each of personalized and random recommendation. \Cref{fig:exp_personalizedvsrandom} then shows the individual-song level relationship (for each song, the mean score under personalized and random recommendation, respectively). As expected, scores are higher under personalized recommendation, indicating the algorithms are working. There is a generally positive relationship at the song level, but perhaps weaker than expected. 

% However, this observation -- that personalization leads to higher ratings -- induces the next observation: estimated song quality is \textit{also} a function of whether a song is shown more often as a result of personalized or random recommendation. (Note that heterogeneity in how often a song is shown via personalized recommendation can be a function of randomness, system design choices, and historic ratings data).  As an initial view of this effect, consider \Cref{fig:exp_personalizedvslike}: the higher the fraction of times that an item is shown due to personalized recommendation (versus randomized), the higher its estimated song score. Of course, causality can be in either direction: a higher mean song score may mean that a personalized recommender is more likely to recommend that song, and being shown through personalized recommendation results in higher mean scores. 


% To further analyze the effect, Regression \Cref{tab:experiment_regression_explainratings_multiple_withpersonal} (mean song scores on a test set, versus user mean ratings and that song's score on a train set) contains a term for the fraction of times that a song was recommended via personalization in the test set. This fraction has a strong positive association with the average song rating in the test set, even controlling for song quality as determined in the training set. In other words, the more that a song was shown via personalized recommendation in the test set, the higher its test set song quality estimation score, even controlling for train set score. This analysis thus shows that -- if songs differ in how often they are shown via personalized recommendation, even conditional on true quality -- this can have a large effect in what the platform learns about items. 
% \end{comment}


% % Because we randomized the train-test sets using the entire period's data and control for the train-time song quality estimate, this result 

% \subsection{How does user heterogeneity affect average ratings?}
% \label{sec:sim}

% [insert graph in support of this]


%  Estimated average ratings may say more about the raters themselves than about the items being rated. The effect may be more pronounced and differentially affect items due to cold-start-induced variance, when new items have few ratings and thus are more susceptible to idiosyncratic behavior of individual users. Depending on the platform, such heterogeneity may have substantial downstream economic consequences for item producers.



% %which is a widely-used simulation framework for evaluating recommendation algorithms.
% % The simulation's goal is to ensure that the predicted user preferences based on their aggregate rating score converge to the true user preferences as quickly as possible in terms of the number of ratings received.


% %Setup








