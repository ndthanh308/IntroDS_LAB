% \section{Randomized control trial:  manipulating the timers }
% \label{sec:experiment}

% This result motivated us to design a randomized control trial where the amount of time associated with the like button was modified, to understand the impact that a user's valuation of their time has on the ratings produced.



% %   % Figure environment removed


% %We now report and analyze the results of an RCT on the Piki platform, motivated by the results observed in the natural experiment. The test had three treatment groups, varying the timer duration for unlocking the "like" button. 

% %The experiment and associated data provide us three benefits over the natural experiment: first, as an RCT, when comparing treatment groups we do not need to worry about confounding due to temporal factors or other platform changes. Second, it allows us to replicate the natural experiment results with more subtle changes. Third,  we can more closely analyze the effect of \textit{personalized} vs \textit{random} recommendations. In particular, during the (smaller time-period) experiment, the platform's recommendation algorithms were relatively stable; furthermore, the platform ensured that there would be a relatively \textit{dense} sub-matrix of songs and users---i.e., there was a set of 160 songs that were over-sampled for "random" recommendations to users. We analyze the experimental results in this section; in the next section, we report the results of further exploratory analyses during the same experimental time period on the interaction between personalized recommendations and ratings.

% %Motivated by the natural experiment, we conducted an RCT on the Piki platform with three treatment groups, varying the timer duration for unlocking the "like" button.

% %Compared to the natural experiment, the RCT provides us with three benefits: it eliminates confounding factors, allows us to replicate the natural experiment with more subtle results, and enables closer analysis of \textit{personalized} vs \textit{random} recommendations.
% %We analyze the experimental results in this section; in the next section, we report the results of further exploratory analyses during the same experimental time period on the interaction between personalized recommendations and ratings. 

% % \todo{rewrite with new structure} \nikhil{sasha comment --I don't think saying the recommendation algorithms were not changed is not necessary (and possibly not entirely true). After all, as users rate more songs, the algorithms "change" as they have more training data. But one of the motivations of the controlled trial is that if there are changes, they affect all 3 treatment groups in parallel
% % }

% % \nikhil{more here}.
% % In this experiment, we investigate the impact of modifying the interface of the Piki music app on user behavior. Specifically, we focus on the timer for each rating option (dislike, like, and superlike). By altering the duration of these timers, we aim to examine how it affects the distribution of ratings and individual users' pickiness levels. Understanding how such interface changes can influence user behavior can provide valuable insights for designing effective recommendation systems.


% % We ran an online A/B/C test in which the experiment varied .
% % The . This is true for all three treatments.
% % We have three treatments in the experiment:
% % \begin{table}[th]
% % 	\begin{center}
% % 		\begin{tabular}{l|ccr}
% % 			\toprule
% % 			\textbf{Treatment} & \textbf{Dislike LT} & \textbf{Like LT} & \textbf{Superlike LT}\\
% % 			\midrule
% % 			a& 3  & 3 & 12\\
% % 			b& 3 & 6 & 12\\
% % 			c& 3 & 9 & 12\\
% % 			\bottomrule
% % 		\end{tabular}
% % 	\end{center}
% % 	\caption{Experiment treatments dislike, like, and superlike lock time (LT) in seconds. The three treatments differ only in the lock time of the like button}
% % 	\label{tab:treatments}
% % \end{table}


% %TODO: Justification for the treatment groups – what are the hypothesess

% % Since the Piki dataset is fully observed, there are almost no missing values in the user-item matrix. This means that each user has seen each song and given feedback on it.
% % The experiment ran over 120 day period in Summer and Fall 2022.

% % \nikhil{Describe the random full matrix vs the personalized recommendations somewhere as well? Maybe in section 3?}

% %\rana{write about the reason of each group}
% \subsection{Experiment setup}
% %\paragraph{Treatments.} The treatment groups differ in the time it takes to unlock the "like" rating. As in the post-timer period in the natural experiment, dislike lock time is always 3 seconds, and the superlike lock time is always 12 seconds. In Treatment a, the like button is unlocked after 3 seconds, i.e., at the same time as the dislike button. In Treatment b, after 6 seconds (same as in the post-timer period and before the experiment). In Treatment c, after 9 seconds. 
% \paragraph{Treatments.} The study used three treatment groups on the Piki platform, varying the time it takes to unlock the "like" rating, with the dislike lock time always at 3 seconds and the superlike lock time always at 12 seconds. Treatment A unlocks the like button after 3 seconds, Treatment B after 6 seconds, and Treatment C after 9 seconds.

% %\paragraph {{Treatment assignment}.} User profiles were allocated to different treatment groups, i.e., a given user profile was exposed to only one set of timers for the duration of the experiment. The split was not uniformly at random: Treatments a and c were allocated $\frac{3}{8}$ of the user profiles each, and Treatment b $\frac{2}{8}$. 
% \paragraph {{Treatment assignment}.} Treatment assignment The user profiles were assigned to one of the treatment groups for the duration of the experiment, with Treatment A and C allocated $\frac{3}{8}$ of user profiles each, and Treatment B allocated $\frac{2}{8}$.

% %\rana{it indicates that we are doing the mapping, so add this to the github and refer to it}

% \paragraph {{Data}.}
% We filtered songs and users who provided less than 10 ratings during the experimental period.  
% % In the evaluation, we filter the experiment results and include
% % only users and songs above the $20^{th}$ percentile of ratings.
% % In the experiment dataset, these are users with minimal $30$ ratings.
% % Before the experiment we include users with minimal $7$ ratings, and songs with minimal $20$ rating per song. \nikhil{why this filtering based on number of ratings before the experiment? we're likely using different filtering for the plots that need this, and not doing filtering like this for plots that don't need data from before the experiment?}
% %
% Table~\ref{tab:stats} contains, for each treatment group, the number of users, songs and ratings, both raw numbers (for whom we say at least 1 rating) and after the data filtering.


% % Each user in the Piki app has an avatar in their profile in the application when is chosen randomly when they first sign up.
% % Users allocation to treatment groups was done by mapping users avatars to the groups (a, b, or c) which results that each user is mapped randomly to each group (a,b, or c). 

% % \todo{finalize numbers that were allocated, that we analyzed}

% % Group a was allocated $429$ users ($\sim 36\%$), for a total of $122225$ ratings, group b was allocated with $331$ users ($\sim 29\%$), for a total of $77758$ ratings while group c was allocated $408$ users ($\sim 35\%$), for a total of $92999$ ratings.
% % The users of each group give rating to both \emph{randomized} songs and \emph{personalized} ones.
% % The numbers in each group are as follows:
% % Group a has $30757$ ratings for random songs and $80765$ ratings for personalized songs, Group b has $20300$ ratings for random songs and $50784$ ratings for personalized songs, and Group c has $28484$ ratings for random songs and $55889$ ratings for personalized songs. \nikhil{make this paragraph a table...merge with Table 2 -- kind of like Table 2 here: https://arxiv.org/pdf/1810.13028.pdf}




% % \begin{table}[tbh]
% % 	\begin{center}
% %   %           \begin{tabular*}{\textwidth}{l c r}
% %   %            \hspace{3cm} \textbf{Users} & \hspace{1.4cm} \textbf{Ratings} &  \hspace{1.65cm}\textbf{Songs} \\
% %   %           \end{tabular*}
% %   %           \begin{tabular}{p{1cm}|p{1cm} p{1cm} | p{1cm} p{1cm} | p{2cm} p{2cm}}
% % 		% 	\textbf{Group} & All & Filtered & All & Filtered & Random & Personalized \\
% % 		% 	\midrule
% % 		% 	a & $429$ & $389$ & $122225$ & $0$  & $30757$ & $80765$\\
% % 		% 	b & $331$ & $308$ & $77758$ & $0$  & $20300$ & $50784$\\
% % 		% 	c & $408$ & $375$ & $92999$ & $0$  & $28484$ & $55889$\\
% %   %           \bottomrule
% % 		% \end{tabular}
% %               \begin{tabular*}{\textwidth}{l c r}
% %              \hspace{3.4cm} \textbf{Users} & \hspace{1.4cm} \textbf{Ratings} &  \hspace{1.65cm}\textbf{Songs} \\
% %             \end{tabular*}
% %             \begin{tabular}{p{1.5cm}|p{1cm} p{1cm} | p{1cm} p{1cm} | p{2cm} p{2cm}}
% % 			\textbf{Treatment} & All & Filtered & All & Filtered & All & Filtered \\
% % 			\midrule
% % 			a & $429$ & $389$ & $122225$ & $59100$  & $49598$ & $5454$\\
% % 			b & $331$ & $308$ & $77758$ & $38946$  & $35213$ & $5397$\\
% % 			c & $408$ & $375$ & $92999$ & $43633$  & $42615$ & $5418$\\
% %             \bottomrule
% % 		\end{tabular}
% % 	\end{center}
% % 	\caption{A total number of users, ratings, and songs (all and filtered) for each treatment group.}
% % 	\label{tab:stats}
% % \end{table}



% % \begin{table}[th]
% % 	\begin{center}
% % 		\begin{tabular}{l|ccr}
% % 			\toprule
% % 			\textbf{Treatment} & \textbf{Users} & \textbf{Songs} & \textbf{Ratings}\\
% % 			\midrule
% % 			a& 347  & 49437 & 121327\\
% % 			b& 275 & 35100 & 77225\\
% % 			c& 334 & 42499 & 92229\\
% % 			\bottomrule
% % 		\end{tabular}
% % 	\end{center}
% % 	\caption{Number of users, songs and ratings in each treatment groups after the pre-processing}
% % 	\label{tbl:filtering}
% % \end{table}

% \subsection{Experiment Results}

%   % Figure environment removed



% % \rana{summary of the results across subsections}



% % {\renewcommand\normalsize{\tiny}%
% %\normalsize
% %\begin{table}[tbh]
% %\input{plots_nikhil/experiment_likeregression.tex}
% %\caption{Analogue of \Cref{tab:natexpregression_explainratings}, but for the RCT. Regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song. As before, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating. Treatment c is slightly (but statistically significantly with $p < .05$) more informative in terms of the coefficient on the mean song rating by other users. On average, for each treatment group, each song has about 23 ratings; users have on average 37. Qualitatively similar results emerge for other filtering techniques.}
% %\label{tab:experiment_regression_explainratings}
% %\end{table}
% %}


%  %{\renewcommand\normalsize{\tiny}%
% %\normalsize
% %\begin{table}[tbh]
% %\input{plots_nikhil/experiment_meanregression_withpersonalized.tex}
% %\caption{Analogue of \Cref{tab:natexpregression_multiple_ratings}  for the RCT. Regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. The mean-song-rating coefficient is approximately the same as the mean-user-rating coefficient, suggesting that both song quality and user behavior contribute to a song's mean rating. Furthermore, we add a term for the fraction of times in the test set that a song appeared via \textit{personalized} recommendations; this fraction has a strong positive association with the average song rating in the test set, even controlling for song quality as determined in the training set. We find no significant differences between treatment groups. All independent and dependent variables are standardized. On average, in each treatment group, each song has about 12 ratings on average in each of the test and train set; users have on average 23 ratings in the train set. Qualitatively similar results emerge for other filtering techniques.}
% %\label{tab:experiment_regression_explainratings_multiple_withpersonal}
% %\end{table}
% %}

% % \nikhil{distribution plot like last time}
% % \nikhil{Regressions for just like, and for mean ratings, and mean ratings while taking into account personalized -- maybe all in appendix or mostly}

% %The results largely replicate those of the natural experiment, except that the effects are (as expected) smaller, given the smaller differences between groups.

% Changing the interface changes user ratings behavior.
% As we can see from \Cref{fig:rating_beofre_after}, changing the interface had an affect on both \textit{random}  and \textit{personalized}  setting. The highest correlation can be measured in group b, and even though it is not a perfect correlation (as user's preferences can shift over time, the songs offered by the system can shift and so on), it is much stronger than the ones measured in groups a and c, which were exposed to interface change. 
% In addition, we can a stronger effect in the \textit{personalized} setting than in the \textit{random}  setting. %One possible reason for this could be that feedback from \textit{random} recommendations is generally more consistent and less susceptible to being influenced by the system's interface design compared to \textit{personalized} recommendations
% %\rana{one figure is not enough for this section, consider taking important ones from appendix}

%  \Cref{fig:mean_rating_exp} shows the ratings distribution -- the delay in showing the "like" timer changes the overall distribution. Interestingly, longer delays cause users to substitute to \textit{super-likes} (instead of, primarily, \textit{dis-likes}  as expected); Treatment c has the largest superlike ratio and longest "like" delay---this result can be explained by the fact that users who have already invested 9 seconds face a shorter wait to "super-like" the song.
% %In turn, as shown in Appendix \Cref{tab:exp_score_variance}, the treatment groups differ in user-level mean rater scores -- in particular, Treatment c \textit{increases} user-level ratings variance, i.e., increases user behavior heterogeneity. 

% %Second, ratings are more informative about the raters than they are about songs; ratings behavior differences across treatments affect this relationship to a lesser extent than with the more radical change of introducing timers altogether. \Cref{tab:experiment_regression_explainratings} shows the analogue of \Cref{tab:natexpregression_explainratings} for the RCT: the regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song, along with interaction terms for the treatment group. As before, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating, Treatment c is slightly (statistically significantly with $p < .05$) more informative in terms of a higher coefficient on the mean song rating by other users, but the effect is not first-order. 

% %Similarly, \Cref{tab:experiment_regression_explainratings_multiple_withpersonal} shows the analogue of \Cref{tab:natexpregression_multiple_ratings} for the RCT: regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. The mean-song-rating coefficient is approximately the same as the mean-user-rating coefficient, suggesting that both song quality and user behavior contribute to a song's mean rating even after several ratings.  We find no significant differences between treatment groups -- any difference in informativeness between treatment groups disappears after each song receives several ratings. 

% %Overall, these results replicate those from the natural experiment. However, the effects from the (more modest) changes in timer delays are smaller than those from the initial timer introduction -- further suggesting a limit to the controllability of ratings distributions through interface design alone. 

% % because they are not far from activating this rating option and earning the benefit of saving the songs in their playlist.

% %TODO: results
% % \subsubsection{Ratings Distribution}
% % \todo {discuss mean ratings regression -- unpersonalized}

% % \todo{Silly observation re like distribution by personalized/overall, regression}
% % \todo{Discuss those regressions with the likes}

% % Furthermore, we add a term for the fraction of times in the test set that a song appeared via \textit{personalized} recommendations; 

% % \paragraph{Heterogeneous user change?}
% % \todo{discuss user heterogeneity, change across interface}
% % \todo{Discuss those user correlations, heterogeneity}


% % We start our analysis of the results by looking at the rating distribution.
% % Figure~\ref{fig:rating_dist} shows mean ratio of each rating (dislike, like and superlike) to all of the users ratings.
% % There is a large and significant difference between the rating distribution for each group.
% % There differences are due the different timer duration for each rating option.

% % \paragraph{\textbf{Reducing negative ratings ratio (dislike ratio) by shortening the timer for giving a positive rating:}}
% % Recall that Group a has the shortest timer for like ratings, resulting in a high like rating ratio and a low dislike rating ratio.
% % As illustrated in Figure~\ref{fig:rating_dist}, Group a has the lowest dislike ratio and the highest like ratio among all groups.


% % \paragraph{\textbf{Increasing superlike ratings ratio by extending the timer for like rating}}
% % Another interesting distinction between the treatment groups is the superlike ratio.
% % Group c has the largest superlike ratio, as shown in Figure~\ref{fig:rating_dist}, this group has the highest "like" rating timer.
% % This can be explained by the fact that users who have already invested time in rating would eventually give a superlike rating because they are not far from activating this rating option and earning the benefit of saving the songs in their playlist.
% % As a conclusion, we can boost the superlike ratio by increasing the timer for like ratings.


% % % Figure environment removed

% % larger effect on personalized (Figure 1 Rating dist)
% % song rating (effect on personalized) - the highest value in group b
% %





% %In this experiment, we are interested in how sensitive superlike rates are to the three treatments.









% \begin{comment}
% \paragraph{Metrics}:
% \begin{enumerate}
%     \item superlike as a user utility - \rana{recommenders maximize this}
% \end{enumerate}    
% \end{comment}


% % \subsubsection{Users Distribution}





% \begin{comment}
    
% \begin{table}[h!]
% 	\begin{center}
% 		\begin{tabular}{l|r}
% 			\toprule
% 			\textbf{Treatment} & \textbf{Ratings per user}&
% 			\midrule
% 			a& 82  \\
% 			b& 82 \\
% 			c& 79 \\
% 			\bottomrule
% 		\end{tabular}
% 	\end{center}
% 	\caption{Median ratings per user}
% 	\label{tbl:rating_per_users}
% \end{table}

% \end{comment}

% %\subsection{Songs Distribution}


% % \subsubsection{Accuracy}


% % % Figure environment removed