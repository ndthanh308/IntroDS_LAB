
\begin{comment}
 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/naturalexperiment_likeregression.tex}
\caption{For the natural experiment, regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song. Perhaps surprisingly, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating, especially before the interface design change. All dependent variables are standardized, and so coefficients should be interpreted as the change in the user-song-rating associated with a one standard deviation change in the independent variable. To minimize the effect of any single user or item in this regression and keep the number of ratings by each song and user comparable, we sample at most 100 ratings by any user or for any item. On average, in the post-timer period, each song has 31 ratings; users have on average 29. For the pre-timer period, these numbers are 21 and 22, respectively. Qualitatively similar results emerge for other filtering techniques. }
\label{tab:natexpregression_explainratings}
\end{table}
}

 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/naturalexperiment_meanregression.tex}

\caption{For the natural experiment, regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. As in \Cref{tab:natexpregression_explainratings}, mean scores is more strongly associated by the \textit{user} average behavior than the \textit{song's} average rating, especially for songs with low numbers of ratings. All independent and dependent variables are standardized. On average, in the post-timer period, each song has about 16 ratings on average in each of the test and train set; users have on average 22 ratings in the train set. For the pre-timer period, these numbers are 11 and 17, respectively. (Note that the song numbers are exactly half that of \Cref{tab:natexpregression_explainratings}, as the test/train split is stratified by song). Qualitatively similar results emerge for other filtering techniques.}
\label{tab:natexpregression_multiple_ratings}

\end{table}
}

\end{comment}



 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/experiment_likeregression.tex}
\caption{Analogue of \Cref{tab:natexpregression_explainratings}, but for the RCT. Regression results for a \textit{single} user-song rating (i.e., either a 0, 1, or 2) versus the average other ratings by that user or for that song. As before, this rating is more strongly associated by the \textit{user's} average behavior than the \textit{song's} average rating. Treatment c is slightly (but statistically significantly with $p < .05$) more informative in terms of the coefficient on the mean song rating by other users. On average, for each treatment group, each song has about 23 ratings; users have on average 37. Qualitatively similar results emerge for other filtering techniques.}
\label{tab:experiment_regression_explainratings}
\end{table}
}


 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/experiment_meanregression_withpersonalized.tex}
\caption{Analogue of \Cref{tab:natexpregression_multiple_ratings}  for the RCT. Regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. The mean-song-rating coefficient is approximately the same as the mean-user-rating coefficient, suggesting that both song quality and user behavior contribute to a song's mean rating. Furthermore, we add a term for the fraction of times in the test set that a song appeared via \textit{personalized} recommendations; this fraction has a strong positive association with the average song rating in the test set, even controlling for song quality as determined in the training set. We find no significant differences between treatment groups. All independent and dependent variables are standardized. On average, in each treatment group, each song has about 12 ratings on average in each of the test and train set; users have on average 23 ratings in the train set. Qualitatively similar results emerge for other filtering techniques.}
\label{tab:experiment_regression_explainratings_multiple_withpersonal}
\end{table}
}