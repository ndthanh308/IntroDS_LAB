\section{Supplementary information and analyses}

\input{appendix_regressiontables}



% Figure environment removed


     {\renewcommand\normalsize{\small}%
    \normalsize
    \begin{table}[tbh]
    \centering {
    \input{plots_nikhil/experiment_uservariance.tex}}
    \caption{Analogue of \Cref{fig:user_score_hist} for the experiment -- the \textit{variance} in the user score histograms across treatment groups, for \textit{all} recommendations and for \textit{random} and \textit{personalized} recommendations, respectively. The confidence intervals are user-level bootstrapped 95\% CIs of the variance of the user score distribution. }
    \label{tab:exp_score_variance}
    \end{table}
    }

    
 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/experiment_meanregression.tex}
\caption{Exact analogue (without fraction personalized) of \Cref{tab:natexpregression_multiple_ratings} for the RCT. Regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. The mean-song-rating coefficient is approximately the same as the mean-user-rating coefficient, suggesting that both song quality and user behavior contribute to a song's mean rating. We find no significant differences between treatment groups. All independent and dependent variables are standardized. On average, in each treatment, each song has about 12 ratings on average in each of the test and train set; users have on average 23 ratings in the train set. Qualitatively similar results emerge for other filtering techniques.}
\label{tab:experiment_regression_explainratings_multiple}
\end{table}
}

% Figure environment removed

% Figure environment removed

\paragraph{\textbf{Personalized songs ratings are affected the most from changing the interface}}
Figure~\ref{fig:rating_beofre_after} shows the correlation between mean user rating before and after the experiment in each treatment group.
We examine two different rating types: ratings for personalized songs and random songs.
As demonstrated, the correlation in Group b is the highest among the other groups for personalized song rating; this is due to the fact that we had the same configuration as Group b before the experiment.
The correlation of the personalized songs rating is lower than the random songs rating in the other treatment groups (a and c), where the interface was changed before and after the experiment.%, indicating that changes in the interface, specifically changing the timers of each rating, affect the personalized songs rating more than the random ones.

\paragraph{\textbf{Users retention is not affected from changing the interface}}
We compared median ratings per user in each treatment group to examine how the interface influenced user retention.
The median number of ratings per user in Group a is $61$, Group b is $66$, and Group c is $62$.
Furthermore, Figure~\ref{fig:retension} shows the users' percentage per rating number in log scale.
There are no substantial differences between the three treatment groups.
This means that in each group, the same percentage of users give varying number of ratings.
We can conclude that the interface does not have a strong effect on the number of ratings given by users, which means users retention is not affected as well.





\begin{comment}
  % Figure environment removed


  % Figure environment removed



  % Figure environment removed



  % Figure environment removed


\end{comment}




\subsection{Simulation: How do ratings affect the quality of personalized recommendation?}
\label{sec:sim}


%Intro
We now study how rating behavior affects personalized recommendation. In all the above analyses, we primarily study platform \textit{informational} effects -- how does the user interface and personalized recommendations affect the information that the platform has about items. While better signal about song quality may itself be an outcome of interest (and is to the Piki platform), it is not clear that such informational effects have \textit{downstream} implications for users, e.g., via better personalized recommendations. In particular, \citet{krauth2020offline} show via simulation that offline metrics may poorly correlate with {online} metrics in recommendation systems. 

To study these effects for our research questions, we also employ a simulation approach, modifying the RecLab~\cite{krauth2020offline} simulation framework. We calibrate ratings behavior to the experimental data -- each simulated ratings ``interface'' induces a different distribution of item ratings, which are used as inputs to the recommendation algorithm. (Note that we make the simplifying assumption to not capture user \textit{ratings} heterogeneity, while still simulating user \textit{preference} heterogeneity). A simulation allows us knowledge of "ground truth" user preferences and item characteristics. 


%which is a widely-used simulation framework for evaluating recommendation algorithms.
% The simulation's goal is to ensure that the predicted user preferences based on their aggregate rating score converge to the true user preferences as quickly as possible in terms of the number of ratings received.


%Setup
\subsubsection{Simulation Setup}
RecLab's evaluation process consists of two main components: environments and recommenders. An environment is a set of users and items  (each with associated covariates), and a recommender interacts with the environment iteratively.

At each step of the simulation, a random subset of users is selected. These users are then provided with a recommendation for an item from the initialized recommender. The ground truth user preference for the recommended is a function of the user and item covariates. The user provides a rating as a function of this ground truth preference and the ``interface'' that is being tested (which, as in the above analyses, shifts the relationship between true user preferences and provided ratings). Based on the ratings, the recommender is updated. This process is repeated for many time steps.

\paragraph{Simulation parameters.} 
In the simulation, we construct factors of size $k$ for both users and items. The user factors are sampled from a uniform distribution $U[0,1]$, while the item factors are sampled from a gamma distribution $\Gamma(2,1)$.

\textit{Ratings.} How are ratings simulated? Using the above factors, we calculate the ground truth preference as the dot product of the user and item factors, resulting in:
$ ground\_truth\_preference = user_i \cdot item_i \sim \Gamma(k,1) $, i.e., true preferences follow a gamma distribution with shape parameter $k$ and scale parameter $1$. %\writingtodo{Sasha: I checked the math (product of uniform and gamma is exponential, sum of k exponentials is gamma(k,1)) but is there a reference for this choice, maybe a Reclab reference makes sense here}
To account for noise in the ratings, we add normal noise to the ground truth preference in the form of continuous $noisy\_preference = ground\_truth\_preference + N(0,\sigma^2)$, where $\sigma$ is a noise coefficient. Finally, we threshold the noisy preference rating to convert it to one of three ordinal ratings as in our empirical context: dislike, like, or superlike.

In different simulations, we vary the thresholds and thus the overall ratings distributions. For example, in one set, we calibrate the thresholds to replicate the ratings distributions in the RCT. 

\textit{Recommendations.} Three different recommendation algorithms were evaluated in each simulation: a random recommender, libFM, and toppop. The random recommender algorithm predicts ratings uniformly at random, while libFM is a factorization machine algorithm implemented in LibFM \cite{rendle2012factorization}. The toppop algorithm recommends the most popular items to every user without personalization. The popularity of each item was determined by its average rating.

Simulation parameters are detailed in \Cref{tab:sim_param}.

% This process allows us to simulate the rating distribution of the environment with the desired characteristics, which will be used for training and testing the recommender.

% The thresholds used are chosen as quantiles of the raw distribution of $\Gamma(k,1)$ and they are based on the threshold probabilities. Specifically, if continuous $rating \le X_1$, the rating is assigned as dislike, if $X_1 \le continuous\ rating \le X_2$, the rating is assigned as like, otherwise the rating is assigned as superlike.

% $ rating = \begin{cases}
% dislike & \text{if } continuous\ rating \leq X_1, P(x\leq X_1) = threshold_1\\
% like & \text{if } X_1 \leq continuous\ rating \leq X_2, P(x\leq X_2) = threshold_2\\
% superlike & \text{otherwise}
% \end{cases} $




%Simulation Parametets 

\textit{Simulation Experiments} We conducted two simulations, \emph{sim\_exp, sim\_ctld}, each one involved simulating three different interface designs that varied in the timer duration. The treatment groups were designated as a, b, and c in each simulation. The first simulation (\emph{sim\_exp}), closely resembled the experimental design by setting $threshold_1$ and $threshold_2$ according to the experiment results. While
the second simulation (\emph{sim\_ctld}) involved more extreme manipulation of the timer durations, such that Treatment a had a timer duration that favored likes and Treatment c had a timer duration that favored dislikes. (We note that each ``treatment'' is independent of the others, and so the grouping of treatments into \emph{sim\_exp, sim\_ctld} are only for ease of reporting results). 



\begin{table}[th]
	\begin{center}
		\begin{tabular}{p{5cm}|p{6cm}|p{2cm}}
			\toprule
			\textbf{Parameter} & \textbf{Description} & \textbf{Value}\\
			\midrule
			$num\_users$& users number & $100$ \\
			$num\_items$& items number & $5000$ \\
			$n\_iter$& iterations number & $1000$ \\
			$ratio\_init\_ratings$& ratio of ratings employed for training the recommender from the entirety ratings & $0.01$ \\
			$rating\_frequency$& ratio of iteration rating from all rating & $0.1$ \\
            $latent\_dim$& factors number (parameter k)  & $5000$ \\
			$\sigma$& noise multiple factor & $0.5$ \\
			${like,dislike,superlike}\_weight$& ratings encoding & ${0,1,2}$ \\
			$thresholds\_name$& name of thresholds array that used to map the continuous rating to {dislike, like, superlike} of the two simulations & $sim\_exp$ or $sim\_ctld$\\
            \bottomrule
		\end{tabular}
        \begin{tabular}{lclc}
        \textbf{Simulation}  &  \textbf{Treatment}   &  \textbf{$threshold_1$}  &  \textbf{$threshold_2$}  \\
        \textbf{$sim\_exp$} &   a  & $0.4028$ & $0.8845$  \\
        &   b  & $0.4276$ & $0.8508$  \\
        &   c & $0.4240$ & $0.8270$  \\
        \textbf{$sim\_ctld$} &   a  & $0.33$ & $0.66$  \\
        &   b  & $0.25$ & $0.5$  \\
        &   c & $0.5$ & $0.75$  \\
        \bottomrule
        \end{tabular}
	\end{center}
	\caption{Simulation parameters }
	\label{tab:sim_param}
\end{table}

% \writingtodo{Sasha: the $sim\_exp$ should be the same numbers as figure 3a or 3b, no? I wonder if the order of a and b in $sim\_ctld$ should be interchanged to be consistent with figure 6a }

%thresholds_quantiles = {'a': [1 / 3, 2 / 3], 'b': [1 / 4, 1 / 2], 'c': [1 / 2, 3 / 4]}
%thresholds_experiment = {'a': [0.465478, 0.910309], 'b': [0.481657, 0.868786], 'c': [0.497533, 0.877837]}






\subsubsection{Results}





  % Figure environment removed




 



The results of the simulation calibrated to the experimental data are depicted in Figure~\ref{fig:sim_exp_res}. Specifically, we report: (1) the ratings distribution (dislikes, likes, and superlikes) for under random recommendation, and (2) the mean user utility under libFM recommendations, computed as the average the \textit{ground truth} user preference for items recommended to each user. The results for the remaining recommender algorithms and simulations are provided in the appendix, in Figures \ref{fig:sim_ctld_res}-\ref{fig:sim_utils}. %These figures serve to offer a more comprehensive view of the results obtained from the simulations.


\paragraph{Calibration check for the ratings distribution.} Figures~\ref{fig:mean_rating_exp_all} and~\ref{fig:mean_rating_sim_random} illustrate the simulated experiment closely mirrors the experimental conduct (in terms of overall ratings distributions). The distribution of both figures indicate that Treatment a has the least number of dislikes and super-likes, whereas Treatment c receives the most dislikes and super-likes.

% Figures~\ref{fig:mean_rating_sim_random} and~\ref{fig:mean_rating_sim_ctld_random} demonstrate that changes in the rating interface have a significant impact on the heterogeneity of user rating behavior. Specifically, Figure~\ref{fig:mean_rating_sim_ctld_random} illustrates that the interface change resulted in an increase in the overall number of "dislikes" for Treatment c, which is expected due to the additional time required for users to give a "like" or "super-like." Conversely, Treatment a exhibits an increase in the overall number of "super-likes" as the timer for super-likes is low, and users are incentivized to super-like a song as it would be saved in a playlist for later. These findings provide evidence that the random recommender algorithm has a meaningful influence on the heterogeneity of user rating behavior.


  
\paragraph{Different interfaces lead to different personalized recommendations for users in the long run.} User utility refers to the overall satisfaction of users with the recommendations provided by the recommendation system in this study. In the simulation, the mean user utility is computed by averaging the user's ground truth preference for each item and then averaging the resulting values for items they were recommended. (Note that such ground truth preferences are only available in simulation).

The results indicate that the ratings distribution can have substantial effect on the quality of personalized recommendations. Specifically, in the calibrated simulation, Figure~\ref{fig:mean_rating_sim_exp_libfm} shows that Treatment a has the highest level of user utility over time. Interestingly, this result is in the opposite direction as our offline experimental metrics, where Treatment c seems to provide the highest accuracy quality estimates for items (of course the simulation differs in various aspects, most notably we do not simulate user ratings heterogeneity and so the settings are not directly comparable). Figure~\ref{fig:mean_rating_sim_ctld_libfm} shows similar levels of differences in user utility across simulated treatments.

Together, the results establish the large potential effect that the ratings data has on the quality of personalized recommendation, and that the relationship with quality estimation is complex. A rich avenue for future work is to theoretically characterize how the ratings distribution affects such personalized recommendation, as \citet{garg2019designing,garg2021designing} do for item quality estimation in a non-personalized context.  

% This can be attributed to the fact that the interfaces in Treatment c provide more comprehensive information about the true preferences of users. This allows the system to better capture user preferences and provide more accurate recommendations. 


% These results highlight the importance of ensuring that the recommendation system delivers on its promise of providing high-quality, tailored recommendations to users. In addition to increasing user satisfaction, this will also ensure that the system provides accurate and relevant recommendations to users, which will ultimately enhance their overall experience.







  % Figure environment removed



  % Figure environment removed


  % Figure environment removed



  % Figure environment removed



  % Figure environment removed






