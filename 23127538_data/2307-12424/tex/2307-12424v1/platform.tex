\subsection{The Piki Music App Interface}

\label{sec:platform}

We analyze data from Piki \footnote{The dataset is available at \url{https://github.com/sstoikov/piki-music-dataset} }, a mobile application designed for music discovery. The application presents users with sets of 30-second music video clips in a sequential manner, similar to social media stories. Users are \textit{required} to provide explicit ratings, selecting from options such as ``disliking,'' ``liking,'' or ``super-liking'' for each song. Until a rating is provided, the clip plays in a loop, even across user sessions. Immediately after a rating is provided, the user is presented with the next song. Notably, users do not have access to a search bar and thus have no direct control over the songs they hear. Super-liked songs are saved to a playlist that can be exported to other applications.

The songs presented to users are chosen using one of several recommendation algorithms, which fall into two categories: "random" algorithms that do not use individual user data and "personalized" algorithms that do. Examples of the algorithms used in the application include a randomization technique over a small set of songs to create a dense user-item matrix, a "top-K" system that recommends the most popular songs overall, and a matrix factorization-based recommender. %


As the app focuses on discovery, it has a higher ratio of songs to users than other rating datasets. Therefore, most songs have a "cold start" problem, with a small number of unique ratings. %
In such a setting, generous users who ``like'' all songs indiscriminately may create an artificial bias toward the songs that they have been exposed to. A further concern is that some users may provide feedback randomly rather than according to their true preferences, just to finish a set of songs and get rewards (note that Piki users may choose to receive micro-payments for their ratings).\footnote{Such behavior is a (perhaps extreme) example of the type of heterogeneous and strategic use behavior on many platforms, that might influence both implicit and explicit ratings that they provide.} %
This platform provides several distinct advantages for studying our research questions: (a) it collects explicit ratings, and users must rate songs before continuing; there is thus little selection bias in ratings, conditional on a user being shown the song; (b) it deploys multiple recommendation algorithms of varying levels of personalization, and data is logged in terms of which algorithm led to a song recommendation. These characteristics allow us to focus our attention on the role of user rating heterogeneity, personalized recommendation, and the interface collecting the ratings.





% Figure environment removed


\vspace{-0.15cm}
\paragraph{Piki Interface.} In an effort to mitigate rating inflation and incentivize ratings in line with a user's true opinions, Piki implemented a set of timers on each of the rating options on February 21, 2021. 
Since then, users have not been able to \textit{immediately} rate a song after it begins playing. 
Instead, each of the ``dislike,'' ``like'' and ``super-like'' buttons appear sequentially in that order, several seconds after the song begins playing or the previous button appears. Figure~\ref{fig:interface} displays the application interface.
The timer to unlock the dislike option ensures that users give a chance to each song; the timer to unlock the like option ensures that users are willing to invest extra time in that song; the timer to unlock the superlike option ensures that only raters who invest the significant time will get to save the song. The exact timing is a platform design choice that influences rating behavior and -- as we show -- downstream outcomes. This work first leverages the introduction of the delay timers and then an experiment that further varied the delay time across treatment groups.































