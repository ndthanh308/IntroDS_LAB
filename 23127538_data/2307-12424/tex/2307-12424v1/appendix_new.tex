\section{Supplementary information and analyses}




% Figure environment removed


     {\renewcommand\normalsize{\small}%
    \normalsize
    \begin{table}[tbh]
    \input{plots_nikhil/experiment_uservariance.tex}
    \caption{Analogue of \Cref{fig:user_score_hist} for the experiment -- the \textit{variance} in the user score histograms across treatment groups, for \textit{all} recommendations and for \textit{random} and \textit{personalized} recommendations, respectively. The confidence intervals are user-level bootstrapped 95\% CIs of the variance of the user score distribution. }
    \label{tab:exp_score_variance}
    \end{table}
    }

    
 {\renewcommand\normalsize{\tiny}%
\normalsize
\begin{table}[tbh]
\input{plots_nikhil/experiment_meanregression.tex}
\caption{Exact analogue (without fraction personalized) of \Cref{tab:natexpregression_multiple_ratings} for the RCT. Regression results for a song's mean rating score on a \textit{test} set versus \textit{train set} mean rating scores given by users (who gave the song test-set ratings) or for that song. The mean-song-rating coefficient is approximately the same as the mean-user-rating coefficient, suggesting that both song quality and user behavior contribute to a song's mean rating. We find no significant differences between treatment groups. All independent and dependent variables are standardized. On average, in each treatment, each song has about 12 ratings on average in each of the test and train set; users have on average 23 ratings in the train set. Qualitatively similar results emerge for other filtering techniques.}
\label{tab:experiment_regression_explainratings_multiple}
\end{table}
}

% Figure environment removed

% Figure environment removed

\paragraph{\textbf{Personalized songs ratings are affected the most from changing the interface}}
Figure~\ref{fig:rating_beofre_after} shows the correlation between mean user rating before and after the experiment in each treatment group.
We examine two different rating types: ratings for personalized songs and random songs.
As demonstrated, the correlation in Group b is the highest among the other groups for personalized song rating; this is due to the fact that we had the same configuration as Group b before the experiment.
The correlation of the personalized songs rating is lower than the random songs rating in the other treatment groups (a and c), where the interface was changed before and after the experiment.%

\paragraph{\textbf{Users retention is not affected from changing the interface}}
We compared median ratings per user in each treatment group to examine how the interface influenced user retention.
The median number of ratings per user in Group a is $61$, Group b is $66$, and Group c is $62$.
Furthermore, Figure~\ref{fig:retension} shows the users' percentage per rating number in log scale.
There are no substantial differences between the three treatment groups.
This means that in each group, the same percentage of users give varying number of ratings.
We can conclude that the interface does not have a strong effect on the number of ratings given by users, which means users retention is not affected as well.





\subsection{Simulation}
The results of the simulation, presented in Figures~\ref{fig:sim_exp_ratings} and~\ref{fig:sim_ratings} along with Figures~\ref{fig:mean_rating_sim_exp_random} and~\ref{fig:mean_rating_sim_ctld_random} indicate that the random recommender has the largest effect on heterogeneity in user ratings behavior.



  % Figure environment removed



  % Figure environment removed


  % Figure environment removed



  % Figure environment removed



  % Figure environment removed


