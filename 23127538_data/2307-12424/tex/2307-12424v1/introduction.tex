\section{Introduction}

\label{sec:intro}










Recommendation and rating systems have emerged as crucial components of numerous online platforms, ranging from e-commerce to social media. These systems depend on user data, including both {implicit} indicators like browsing behavior and {explicit} such as rating scores. This data provides insights into the overall quality\footnote{Taste in many items, including music and movies, is inherently subjective. However, many platforms (such as Spotify, Rotten Tomatoes, and IMDb) calculate overall popularity scores or ratings -- alongside attempting to measure heterogeneous taste} of items and heterogeneous preferences, and ultimately provide personalized guidance to users.

Extensive research has been conducted on how to estimate item quality and user-item preferences and make personalized recommendations based on user data. Various approaches have been proposed, such as computing the sample mean of ratings for each item or leveraging matrix factorization or deep learning methods to generate recommendations.
In contrast, this paper focuses on the data collection process that provides input to these methods. 

Specifically, we explore how the rating data collection process affects item quality estimation and recommendation accuracy.
Here, we focus on two aspects of the data collection process: (i) user rating inflation and heterogeneity; and (ii) algorithmic bias caused by personalized recommendations.

Although previous studies have touched upon these issues, our analysis reveals that the resulting \textit{bias} and \textit{variance} in estimating item quality due to these effects are substantial. We further analyze a randomized controlled trial aimed at mitigating user rating inflation and the resulting heterogeneity in ratings. In particular, we tackle the following challenges:



\textbf{First}, we show that there is substantial user-level heterogeneity in what ratings mean (heterogeneous measurement). For example, picky users give lower ratings on average than generous users, even for similar item qualities or experiences. Thus, with naive methods, \textit{estimated overall item qualities and rankings may say more about the raters themselves than about the items being rated}. This effect is more pronounced and differentially affects items due to {cold-start-induced \textit{variance}} when new items have fewer ratings and thus are more susceptible to the idiosyncratic behavior of individual users. Depending on the platform, such heterogeneity may have substantial downstream economic consequences for item producers. 





\textbf{Second}, the rating interface is a critical platform design choice that can impact recommendation systems' accuracy and effectiveness. For instance, recent research~\cite{garg2021designing} has demonstrated that changing the rating options from numerical to positive-skewed adjectives on an online labor market can result in more informative ratings. However, the extent to which interface design affects users' heterogeneous behavior and, subsequently, the impact on recommendation remains an open question. It is possible that certain interface designs produce more homogeneous user behavior, resulting in more informative item quality estimates.

To examine these effects, we analyze a natural experiment and a randomized controlled trial (RCT) on a music discovery platform. The initial platform change altered the user interface to introduce wait times before users could select their rating options (``dislike'', ``like'' or ``super-like''). The RCT further altered how long users had to wait before options were unlocked.
We find that introducing timers substantially changed user rating behavior, inducing more ``pickiness'' than before and reducing across-user behavior variance. We further find evidence that this reduction in user behavior heterogeneity improved the system's effectiveness. While a user's rating for a given item was \textit{more a function of the user who was shown the item} than the item quality (as estimated by other ratings) both before and after the change, the change increased the relative importance of item quality. %





\textbf{Third}, we investigate the relationship between personalized recommendations and user rating behavior. Using the Piki dataset\footnote{The full (user-anonymized) data is available online: \url{https://github.com/sstoikov/piki-music-dataset}.}, we conducted exploratory analyses to understand how item quality estimates are related to how often that item is recommended via personalized recommendation. Our findings reveal that randomness in personalization can significantly influence item quality estimates. This is beyond what can be expected from the fact that higher-quality items are more likely to be recommended.




Overall, our work has several implications for the design of rating and recommendation systems as well as future research in this area. Specifically, our findings demonstrate that user behavior heterogeneity can significantly impact a system's ability to learn item quality and provide effective recommendations.
Our results indicate that item quality estimates reflect not only item quality but also the users who rated the items and the method used to select those users. While our work highlights interface design as a potential solution, algorithmic design, such as correcting for personalization and user heterogeneity, may also play a crucial role in improving the accuracy and fairness of item quality estimation and recommendation systems.

    


























    


    
    
    
      








































