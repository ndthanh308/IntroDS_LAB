\subsection{Related Work}
\label{sec:related_works}
This paper sits at the intersection of the literature of several communities: rating inflation, interface design and recommendation systems, and the economic implications thereof. %


\textit{Ratings inflation.}  Ratings are examples of \textit{measurements}, i.e., assignments of numbers to the construct (`how much a user enjoyed an item') that we aim to measure (see, e.g., \citet{allen2001introduction} for an overview). The literature discusses many potential reasons that measurement may not be an accurate reflection of the underlying construct, including errors in the measurement process, the use of imprecise or inadequate measurement tools, and the influence of external factors on the measurement (see \citet{bound2001measurement} for a review on biases due to measurement error in survey data).

The rating literature primarily considers \textit{inflation} error: empirically, ratings on marketplaces tend to be high, with most items (sellers, producers, freelancers) receiving positive ratings the vast majority of the time (see, e.g., \citet{filippas2018reputation}). Such inflation has substantial downstream economic consequences, in particular, that high-quality items are difficult to distinguish from lower-quality ones. 
\citet{Aziz2020TheCO}
conclude from a quasi-experiment on a restaurant ratings platform, that rating inflation leads to less exploration of new restaurants and a greater concentration of sales to more popular restaurants. To add to this literature, our work documents \textit{heterogeneous} ratings inflation and partially addresses it via interface design.




\textit{Data interface designs.} There is similarly a large literature on the design of data collection processes to minimize measurement error (see e.g., \citet{krosnick2018questionnaire} for a survey on questionnaire design). Given the importance of implicit feedback in computational systems \cite{claypool2001implicit,hu2008collaborative,jiang2015automatic,radlinski2008does,hongyiwen}, there has been substantial work on understanding \cite{kim2014modeling, liu2010understanding,huang2012user} and \textit{shaping} \cite{schnabel2019shaping} implicit feedback through interface design. Similarly, user interfaces shape how users provide and perceive explicit ratings and recommendations \cite{pu2008user,adomavicius2011recommender,cosley2003seeing,felfernig2007knowledge}. Most related is the work in rating system interface design, especially to counter inflation in marketplaces. \citet{garg2021designing} report the result of a randomized controlled trial in which the rating interface was changed. The study finds that the positive-skewed verbal rating scales lead to rating distributions that significantly reduce rating inflation and are more informative about seller quality than numeric ratings. Similarly, \citet{garg2019designing} derive the \textit{optimal} rating distribution (joint distribution between seller quality and probability of positive rating in a binary setting), when the goal is to accurately learn about item quality. Other solutions to rating inflation include modifying incentives or behavioral effects that are conjectured to cause inflation \cite{fradkin2015bias}. This paper adds to this literature by showing how interface design interacts with user heterogeneity and personalized \textit{recommendation}. 




\textit{Post-processing solutions to mis-measurement.} A distinct approach to correcting for measurement error (ratings inflation or consequences of heterogeneous behavior) is to \textit{post-process} the data. Unbalanced explicit training datasets have been documented in the literature and some flattening techniques have been shown to improve recommendation quality~\cite{mansoury2021flatter}. When training algorithms on implicit data, it is natural to sample items that were not consumed as negative feedback \cite{hu2008collaborative}. \citet{stoikovwen} show this approach performs significantly worse than a matrix factorization algorithm trained on explicit like/dislike data. \citet{nosko2015limits} on eBay notice that \textit{missing} ratings are a negative signal of quality, and so dividing a seller's number of positive ratings by the total number of \textit{sales} as opposed to \textit{ratings} provides a stronger signal of seller quality. Outside of a rating context, item response theory \cite{embretson2013item} models are used to simultaneously learn \textit{question difficulty} and \textit{test-taker skill level}, based only on data on how each test-taker performed on each question. Our results point to the importance of using such `rating the rater' methods to process rating data.  




















