@book{vovk2005algorithmic,
  title={Algorithmic learning in a random world},
  author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  year={2005},
  publisher={Springer Science \& Business Media}
}

@article{lei2013distribution,
  title={Distribution-free prediction sets},
  author={Lei, Jing and Robins, James and Wasserman, Larry},
  journal={Journal of the American Statistical Association},
  volume={108},
  number={501},
  pages={278--287},
  year={2013},
  publisher={Taylor \& Francis}
}


@article{romano2020classification,
  title={Classification with valid and adaptive coverage},
  author={Romano, Yaniv and Sesia, Matteo and Candes, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3581--3591},
  year={2020}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{gendler2022adversarially,
title={Adversarially Robust Conformal Prediction},
author={Asaf Gendler and Tsui-Wei Weng and Luca Daniel and Yaniv Romano},
booktitle={International Conference on Learning Representations},
year={2022}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{salman2019provably,
  title={Provably robust deep learning via adversarially trained smoothed classifiers},
  author={Salman, Hadi and Li, Jerry and Razenshteyn, Ilya and Zhang, Pengchuan and Zhang, Huan and Bubeck, Sebastien and Yang, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{vovk1999machine,
  title={Machine-learning applications of algorithmic randomness},
  author={Vovk, Volodya and Gammerman, Alexander and Saunders, Craig},
  year={1999},
  publisher={Citeseer}
}
@article{shafer2008tutorial,
  title={A tutorial on conformal prediction.},
  author={Shafer, Glenn and Vovk, Vladimir},
  journal={Journal of Machine Learning Research},
  year={2008}
}
@article{heaven2019deep,
  title={Why deep-learning AIs are so easy to fool},
  author={Heaven, Douglas and others},
  journal={Nature},
  volume={574},
  number={7777},
  pages={163--166},
  year={2019},
  publisher={Nature}
}
@inproceedings{biggio2013evasion,
  title={Evasion attacks against machine learning at test time},
  author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and {\v{S}}rndi{\'c}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  pages={387--402},
  year={2013},
  organization={Springer}
}

@article{cauchois2020robust,
  title={Robust validation: Confident predictions even when distributions shift},
  author={Cauchois, Maxime and Gupta, Suyash and Ali, Alnur and Duchi, John C},
  journal={arXiv preprint arXiv:2008.04267},
  year={2020}
}

@article{gibbs2021adaptive,
  title={Adaptive conformal inference under distribution shift},
  author={Gibbs, Isaac and Candes, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1660--1672},
  year={2021}
}
@article{tibshirani2019conformal,
  title={Conformal prediction under covariate shift},
  author={Tibshirani, Ryan J and Foygel Barber, Rina and Candes, Emmanuel and Ramdas, Aaditya},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{podkopaev2021distribution,
  title={Distribution-free uncertainty quantification for classification under label shift},
  author={Podkopaev, Aleksandr and Ramdas, Aaditya},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={844--853},
  year={2021},
  organization={PMLR}
}
@article{guan2022prediction,
  title={Prediction and outlier detection in classification problems},
  author={Guan, Leying and Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society. Series B, Statistical Methodology},
  volume={84},
  number={2},
  pages={524},
  year={2022},
  publisher={Wiley-Blackwell}
}
@article{sadinle2019least,
  title={Least ambiguous set-valued classifiers with bounded error levels},
  author={Sadinle, Mauricio and Lei, Jing and Wasserman, Larry},
  journal={Journal of the American Statistical Association},
  year={2019}
}

@inproceedings{cai2019human,
  title={Human-centered tools for coping with imperfect algorithms during medical decision-making},
  author={Cai, Carrie J and Reif, Emily and Hegde, Narayan and Hipp, Jason and Kim, Been and Smilkov, Daniel and Wattenberg, Martin and Viegas, Fernanda and Corrado, Greg S and Stumpe, Martin C and others},
  booktitle={Proceedings of the 2019 chi conference on human factors in computing systems},
  pages={1--14},
  year={2019}
}
@article{rastogi2022unifying,
  title={A Unifying Framework for Combining Complementary Strengths of Humans and ML toward Better Predictive Decision-Making},
  author={Rastogi, Charvi and Leqi, Liu and Holstein, Kenneth and Heidari, Hoda},
  journal={arXiv preprint arXiv:2204.10806},
  year={2022}
}
@article{robey2022probabilistically,
  title={Probabilistically Robust Learning: Balancing Average-and Worst-case Performance},
  author={Robey, Alexander and Chamon, Luiz FO and Pappas, George J and Hassani, Hamed},
  journal={arXiv preprint arXiv:2202.01136},
  year={2022}
}

@article{hechtlinger2018cautious,
  title={Cautious deep learning},
  author={Hechtlinger, Yotam and P{\'o}czos, Barnab{\'a}s and Wasserman, Larry},
  journal={arXiv preprint arXiv:1805.09460},
  year={2018}
}




@inproceedings{NEURIPS2020_244edd7e,
 author = {Romano, Yaniv and Sesia, Matteo and Candes, Emmanuel},
 booktitle = {Advances in Neural Information Processing Systems(NeurIPS)},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3581--3591},
 publisher = {Curran Associates, Inc.},
 title = {Classification with Valid and Adaptive Coverage},
 url = {https://proceedings.neurips.cc/paper/2020/file/244edd7e85dc81602b7615cd705545f5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{
angelopoulos2021uncertainty,
title={Uncertainty Sets for Image Classifiers using Conformal Prediction},
author={Anastasios Nikolas Angelopoulos and Stephen Bates and Michael Jordan and Jitendra Malik},
booktitle={International Conference on Learning Representations(ICLR)},
year={2021},
url={https://openreview.net/forum?id=eNdiU_DbM9}
}

@article{angelopoulos2020uncertainty,
  title={Uncertainty sets for image classifiers using conformal prediction},
  author={Angelopoulos, Anastasios and Bates, Stephen and Malik, Jitendra and Jordan, Michael I},
  journal={arXiv preprint arXiv:2009.14193},
  year={2020}
}

@inproceedings{vovk2018cross,
  title={Cross-conformal predictive distributions},
  author={Vovk, Vladimir and Nouretdinov, Ilia and Manokhin, Valery and Gammerman, Alexander},
  booktitle={Conformal and Probabilistic Prediction and Applications},
  year={2018},
  organization={PMLR}
}

@article{lei2018distribution,
  title={Distribution-free predictive inference for regression},
  author={Lei, Jing and G’Sell, Max and Rinaldo, Alessandro and Tibshirani, Ryan J and Wasserman, Larry},
  journal={Journal of the American Statistical Association},
  year={2018}
}

@article{romano2019conformalized,
  title={Conformalized quantile regression},
  author={Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
  journal={Advances in Neural Information Processing Systems(NeurIPS)},
  year={2019}
}
@article{izbicki2019flexible,
  title={Flexible distribution-free conditional predictive bands using density estimators},
  author={Izbicki, Rafael and Shimizu, Gilson T and Stern, Rafael B},
  journal={arXiv preprint arXiv:1910.05575},
  year={2019}
}
@article{guan2019conformal,
  title={Conformal prediction with localization},
  author={Guan, Leying},
  journal={arXiv preprint arXiv:1908.08558},
  year={2019}
}
@article{gupta2022nested,
  title={Nested conformal prediction and quantile out-of-bag ensemble methods},
  author={Gupta, Chirag and Kuchibhotla, Arun K and Ramdas, Aaditya},
  journal={Pattern Recognition},
  year={2022},
  publisher={Elsevier}
}
@inproceedings{kivaranovic2020adaptive,
  title={Adaptive, distribution-free prediction intervals for deep networks},
  author={Kivaranovic, Danijel and Johnson, Kory D and Leeb, Hannes},
  booktitle={International Conference on Artificial Intelligence and Statistics(AISTATS)},
  year={2020},
  organization={PMLR}
}
@article{barber2021predictive,
  title={Predictive inference with the jackknife+},
  author={Barber, Rina Foygel and Candes, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}
@article{foygel2021limits,
  title={The limits of distribution-free conditional predictive inference},
  author={Foygel Barber, Rina and Candes, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
  journal={Information and Inference: A Journal of the IMA},
  year={2021}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}


@inproceedings{li2019nattack,
  title={Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks},
  author={Li, Yandong and Li, Lijun and Wang, Liqiang and Zhang, Tong and Gong, Boqing},
  booktitle={International Conference on Machine Learning},
  pages={3866--3876},
  year={2019},
  organization={PMLR}
}

@unknown{unknown,
author = {Robey, Alexander and Chamon, Luiz and Pappas, George},
year = {2022},
month = {02},
pages = {},
title = {Probabilistically Robust Learning: Balancing Average- and Worst-case Performance}
}

@article{redko2020survey,
  title={A survey on domain adaptation theory: learning bounds and theoretical guarantees},
  author={Redko, Ievgen and Morvant, Emilie and Habrard, Amaury and Sebban, Marc and Bennani, Youn{\`e}s},
  journal={arXiv preprint arXiv:2004.11829},
  year={2020}
}

@article{ben2006analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}


@InProceedings{blackBox,
  title = 	 {{NATTACK}: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks},
  author =       {Li, Yandong and Li, Lijun and Wang, Liqiang and Zhang, Tong and Gong, Boqing},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3866--3876},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/li19g/li19g.pdf},
  url = 	 {https://proceedings.mlr.press/v97/li19g.html},
  abstract = 	 {Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and for thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques developed recently. Instead of searching for an "optimal" adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN’s internal layers or weights. Our approach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs.}
}

@article{lee2019tight,
  title={Tight certificates of adversarial robustness for randomly smoothed classifiers},
  author={Lee, Guang-He and Yuan, Yang and Chang, Shiyu and Jaakkola, Tommi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{teng2020ell_1,
  title={$$\backslash$ell\_1 $ Adversarial Robustness Certificates: a Randomized Smoothing Approach},
  author={Teng, Jiaye and Lee, Guang-He and Yuan, Yang},
  year={2020}
}

@article{zhang2020filling,
  title={Filling the soap bubbles: Efficient black-box adversarial certification with non-gaussian smoothing},
  author={Zhang, Dinghuai and Ye, Mao and Gong, Chengyue and Zhu, Zhanxing and Liu, Qiang},
  year={2020}
}


@inproceedings{vovk2012conditional,
  title={Conditional validity of inductive conformal predictors},
  author={Vovk, Vladimir},
  booktitle={Asian conference on machine learning},
  pages={475--490},
  year={2012},
  organization={PMLR}
}

@article{iandola2014densenet,
  title={Densenet: Implementing efficient convnet descriptor pyramids},
  author={Iandola, Forrest and Moskewicz, Matt and Karayev, Sergey and Girshick, Ross and Darrell, Trevor and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1404.1869},
  year={2014}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}


@InProceedings{Black_box,
  title = 	 {{NATTACK}: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks},
  author =       {Li, Yandong and Li, Lijun and Wang, Liqiang and Zhang, Tong and Gong, Boqing},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3866--3876},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/li19g/li19g.pdf},
  url = 	 {https://proceedings.mlr.press/v97/li19g.html},
  abstract = 	 {Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and for thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques developed recently. Instead of searching for an "optimal" adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN’s internal layers or weights. Our approach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs.}
}

@article{metzen2017detecting,
  title={On detecting adversarial perturbations},
  author={Metzen, Jan Hendrik and Genewein, Tim and Fischer, Volker and Bischoff, Bastian},
  journal={arXiv preprint arXiv:1702.04267},
  year={2017}
}

@article{feinman2017detecting,
  title={Detecting adversarial samples from artifacts},
  author={Feinman, Reuben and Curtin, Ryan R and Shintre, Saurabh and Gardner, Andrew B},
  journal={arXiv preprint arXiv:1703.00410},
  year={2017}
}

@inproceedings{cohen2020detecting,
  title={Detecting adversarial samples using influence functions and nearest neighbors},
  author={Cohen, Gilad and Sapiro, Guillermo and Giryes, Raja},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14453--14462},
  year={2020}
}

@article{NCP,
  author       = {Subhankar Ghosh and
                  Taha Belkhouja and
                  Yan Yan and
                  Janardhan Rao Doppa},
  title        = {Improving Uncertainty Quantification of Deep Classifiers via Neighborhood
                  Conformal Prediction: Novel Algorithm and Theoretical Analysis},
  journal      = {CoRR},
  volume       = {abs/2303.10694},
  year         = {2023}
}

