\newpage
\appendix

\section{Proof of Lemma \ref{lemma, equivalence of two def of MDDO}}
\begin{proof}
For any ${\bs{\beta}}\in\mc H$, according to the definition of $G_{\bs s}$ (see Definition $\ref{def: MDDO}$), one has
\begin{align*}
\langle G_{\bs s},{\bs{\beta}}\rangle&=\int_{[0,1]} G_{\bs s}(t){\bs{\beta}}(t)~\mathrm{d}t=\int_{[0,1]}\mathrm{cov}\hspace{-0.9mm}\left(\bs{X}(t),\mathrm{e}^{\mi\langle \bs s,\Y\rangle}\right){\bs{\beta}}(t)~\mathrm{d}t\\
&=\int_{[0,1]}\mathrm{cov}\hspace{-0.9mm}\left(\bs{X}(t){\bs{\beta}}(t),\mathrm{e}^{\mi \langle \bs s,\Y\rangle}\right)~\mathrm{d}t.
\end{align*}
By Fubini theorem, under Assumption $\ref{as:joint distribution assumption}$, one can exchange the order of integration and covariance above and get that
\begin{align*}
 \langle G_{\bs s},{\bs{\beta}}\rangle&=\int_{[0,1]}\mathrm{cov}\hspace{-0.9mm}\left(\bs{X}(t){\bs{\beta}}(t),\mathrm{e}^{\mi \langle \bs s,\Y\rangle}\right)~\mathrm{d}t\\ &=\mathrm{cov}\hspace{-0.9mm}\left(\int_{[0,1]}\bs{X}(t){\bs{\beta}}(t)~\mathrm{d}t,\mathrm{e}^{\mi \langle \bs s,\Y\rangle}\right)=\mathrm{cov}\hspace{-0.9mm}\left(\langle \bs{X},{\bs{\beta}}\rangle,\mathrm{e}^{\mi \langle \bs s ,\Y\rangle}\right).
\end{align*}
Thus for any $\bs\alpha(t),{\bs{\beta}}(t)\in\mc H$, one can get
\begin{align*}
\big\langle \big(G_{\bs s}\otimes \overline{G}_{\bs s}\big)\bs\alpha,{\bs{\beta}}\big\rangle=\langle G_{\bs s},\bs\alpha\rangle\langle \overline{G}_{\bs s},{\bs{\beta}}\rangle=\mathrm{cov}\hspace{-0.9mm}\left(\langle \bs{X},\bs\alpha\rangle,\mathrm{e}^{\mi \langle \bs s,\Y\rangle}\right)\hspace{-0.9mm}\mathrm{cov}\hspace{-0.9mm}\left(\langle \bs{X},{\bs{\beta}}\rangle,\mathrm{e}^{-\mi\langle \bs s,\Y\rangle}\right)\\
=\mb{E}\hspace{-0.9mm}\left(\langle \bs{X},\bs\alpha\rangle\mathrm{e}^{\mi \langle \bs s,\Y\rangle}\right)\mb{E}\hspace{-0.8mm}\left(\langle \bs{X},{\bs{\beta}}\rangle\mathrm{e}^{-\mi \langle \bs s,\Y\rangle}\right)=\mb{E}\Big(\langle \bs{X},\bs\alpha\rangle\langle \bs{X}',{\bs{\beta}}\rangle\mathrm{e}^{\mi \langle \bs s,\Y-\Y'\rangle}\Big).
\end{align*}
Considering that $\mb{E}\big(\langle \bs{X},\alpha\rangle\langle \bs{X}',{\bs{\beta}}\rangle\big)=0$, one has
\begin{align*}
\big\langle \big(G_{\bs s}\otimes \overline{G}_{\bs s}\big)\bs\alpha,{\bs{\beta}}\big\rangle
=- \mb{E}\Big(\langle \bs{X},\bs\alpha\rangle\langle \bs{X}',{\bs{\beta}}\rangle\big(1-\mr{e}^{\mi \langle \bs s,\Y-\Y'\rangle}\big)\Big)&\\
=- \mb{E}\Big(\langle \bs{X},\bs\alpha\rangle\langle \bs{X}',{\bs{\beta}}\rangle\big[1-\cos\big(\langle \bs s,\Y-\Y'\rangle\big)\big]\Big)&\\
+\mi\mb{E}\Big(\langle \bs{X},\bs\alpha\rangle\langle \bs{X}',{\bs{\beta}}\rangle\big[\sin\big(\langle\bs s,\Y-\Y'\rangle\big)\big]\Big)&.
\end{align*}
It is easy to check that
\[\int_{\mb R^q}\frac{\sin \big(\langle\bs s,\Y-\Y'\rangle)\big)}{\|\bs s\|^{1+q}}~\mr{d}\bs s=\lim_{\varepsilon\to0^+}\int_{\bs s\in\mb{R}^q:\varepsilon\leqslant\|\bs s\|\leqslant \varepsilon^{-1}}\frac{\sin \big(\langle \bs s,\Y-\Y'\rangle\big)}{\|\bs s\|^{1+q}}~\mr{d}\bs s=0,\]
because the integrand is an odd function. By Lemma 1 in \cite{szekely2007measuring},  one can also get
\[\int_{\R^q}\frac{1-\cos\big(\langle \bs s,\Y-\Y'\rangle\big)}{\|\bs s\|^{1+q}}~\mr{d}\bs s=c_q\|\Y-\Y'\|.
\]
Combining above results with Definition $\ref{def: MDDO}$, one can obtain that 
\begin{align}\label{proof: lemma MDDO}
\langle\mathrm{MDDO}(\bs{X}|Y)\bs\alpha,{\bs{\beta}}\rangle=- \mb{E}\Big(\langle \bs{X},\bs\alpha\rangle\langle \bs{X}',{\bs{\beta}}\rangle\|\Y-\Y'\|\Big) .
\end{align}
Then by the arbitrariness of $\bs\alpha,{\bs{\beta}}\in\mc H$, the proof is completed. 
\end{proof}

\section{Proof of Theorem \ref{theorem, MDDO and conditional mean independence}}



According to \eqref{proof: lemma MDDO}, one can get the following useful lemma.
\begin{lemma}\label{lemma, MDDO and FMDD}
Under Assumption $\ref{as:joint distribution assumption}$, for all ${\bs{\beta}}\in\mathcal H$, $\|{\bs{\beta}}\|=1$, we have
\begin{align*}
\langle \mathrm{MDDO}(\boldsymbol{X}|\Y)({\bs{\beta}}),{\bs{\beta}}\rangle &=- \mathbb E\Big[ \langle \boldsymbol{X},{\bs{\beta}}\rangle \langle \boldsymbol{X}',{\bs{\beta}}\rangle \|\Y-\Y'\|\Big]\\
&=- \mathbb E\Big[\big\langle\langle \boldsymbol{X},{\bs{\beta}}\rangle{\bs{\beta}},\langle \boldsymbol{X}',{\bs{\beta}}\rangle{\bs{\beta}}\big\rangle\|\Y-\Y'\|\Big].
\end{align*}
\end{lemma}
This conclusion links MDDO with functional martingale
difference divergence  (FMDD, \citealt{lee2020testing}). 
Next we give the following two lemmas to finish the proof of Theorem $\ref{theorem, MDDO and conditional mean independence}$.
\begin{lemma}\label{lem: Txx=0tuiTx=0}If $T$ is a positive semi-definite operator on a Hilbert space $\wt{\mathcal{H}}$, then for all $x\in\wt{\mathcal{H}}$, one has $\langle Tx,x\rangle=0\Longleftrightarrow Tx=0$.
\end{lemma}
\begin{proof}
`$\Longleftarrow$': It is obvious.

`$\Longrightarrow$': It is easy to check that $f(a,b)=\langle Ta,b\rangle$ $(a,b\in\wt{\mc H})$ is a 
positive semi-definite Hermitian form. Thus, for any $y\in\wt{\mathcal{H}}$, one can use Cauchy inequality to get
\[|\langle Tx,y\rangle|^2\leqslant\langle Tx,x\rangle\langle Ty,y\rangle=0\Longrightarrow \langle Tx,y\rangle=0.\]
By the arbitrariness of $y\in\wt{\mc H}$, one has $Tx=0$.
\end{proof}

Our proof of Theorem $\ref{theorem, MDDO and conditional mean independence}$ is mainly inspired by the following property of
FMDD in \cite{lee2020testing}.
\begin{lemma}[Proposition 1 of \cite{lee2020testing}]\label{lem:prop1inlee}
If $\E[\|\X\|+\|\Y\|]<\infty$ and $\E[\|\bs X\|\|\Y\|]<\infty$, then we have
\[\E[\langle \X,\X'\rangle\|\Y-\Y'\|]=0\Longleftrightarrow \E[\X|\Y]=0\quad\text{almost surely},\]
where $(\X',\Y')$ is an i.i.d. copy of $(\X,\Y)$.
\end{lemma}
\paragraph{Proof of Theorem $\ref{theorem, MDDO and conditional mean independence}$}
\begin{proof}
Clearly, (ii) is a direct consequence of Lemma $\ref{lemma, equivalence of two def of MDDO}$ and the following lemma.

\begin{lemma}[Lemma 15 in \citealt{chen2023optimality}]\label{lem:cov TX}
If $T$ is an operator defined on $\mc H_1\to\mc H_2$ where $\mc H_i,i=1,2$ is a Hilbert space. $\bs X\in\mc H_1$ is a random element satisfying $\mb E[\bs X]=0$ . Then we have $\mr{var}(T\bs X)=T\mr{var}(\bs X)T^*$.
\end{lemma}

Now we start  to prove (i).
 First, one has
\begin{align*}\mathrm{MDDO}(\boldsymbol{X}|\Y)=0 &\Longleftrightarrow \mathrm{MDDO}(\boldsymbol{X}|\Y)({\bs{\beta}})=0,\quad\forall{\bs{\beta}}\in\mb{S}_{\mathcal H};\\
\mathbb E[\boldsymbol{X}|\Y]=0~~\text{a.s.}&\Longleftrightarrow\langle\mb E[\boldsymbol{X}|\Y],{\bs{\beta}}\rangle{\bs{\beta}}=0~~\text{a.s.} \quad\forall{\bs{\beta}}\in\mb{S}_{\mathcal H},
\end{align*}
where $\mb{S}_\mc{H}=\{{\bs{\beta}}\in\mc H:\|{\bs{\beta}}\|=1\}$. Second, from Lemma $\ref{lem: Txx=0tuiTx=0}$, one knows that
\begin{align*}
\mathrm{MDDO}(\boldsymbol{X}|\Y)({\bs{\beta}})=0&\Longleftrightarrow\langle\mathrm{MDDO}(\boldsymbol{X}|\Y)({\bs{\beta}}),{\bs{\beta}}\rangle=0.
\end{align*}
 Then under Assumption $\ref{as:joint distribution assumption}$, by Lemma $\ref{lemma, MDDO and FMDD}$ and $\ref{lem:prop1inlee}$, one has
\begin{align*}
&\langle\mathrm{MDDO}(\boldsymbol{X}|\Y)({\bs{\beta}}),{\bs{\beta}}\rangle=0\Longleftrightarrow\mathbb E[\big\langle\langle \boldsymbol{X},{\bs{\beta}}\rangle{\bs{\beta}},\langle \boldsymbol{X}',{\bs{\beta}}\rangle{\bs{\beta}}\rangle\|\Y-\Y'\|]=0\\
&\qquad\qquad\qquad\qquad\qquad\Longleftrightarrow\mathbb E[\langle \bs X,{\bs{\beta}}\rangle{\bs{\beta}}|\Y]=\langle \mb E[\boldsymbol{X}|\Y],{\bs{\beta}}\rangle{\bs{\beta}}=0~~\text{a.s.}
\end{align*}
This finishes the proof of Theorem $\ref{theorem, MDDO and conditional mean independence}$.
\end{proof}

% {\color{blue}\paragraph{Proof of Lemma \ref{lem:cov TX} (Repeated)}
% \begin{proof}
% For any $\u_1,\u_2\in\mc H_2$, we have
% \begin{align*}
% &\left\langle  T\mr{var}(\vX)T^*\u_1,\u_2  \right\rangle=\left\langle  T\mb E[\vX\otimes\vX]T^*\u_1,\u_2  \right\rangle
% =\left\langle  \mb E[\vX\otimes\vX]T^*\u_1,T^*\u_2  \right\rangle    
% \end{align*}
% since $\mb E[\vX]=0$. By the definition of convariance operator and expectation, we have 
% \begin{align*}
% \left\langle  \mb E[\vX\otimes\vX]T^*\u_1,T^*\u_2  \right\rangle=&\left\langle  \mb E[\left\langle\vX,  T^*\u_1 \right\rangle       \vX            ],T^*\u_2  \right\rangle
% =\mb E[  \left\langle\vX,  T^*\u_1 \right\rangle      \left\langle \vX            ,T^*\u_2  \right\rangle].
% \end{align*}
% Similarly, we have
% \begin{align*}
%  \left\langle  \mr{var}(T\vX)\u_1,\u_2  \right\rangle=\left\langle  \mb E[T\vX\otimes T\vX]\u_1,\u_2  \right\rangle=\mb E[  \left\langle T\vX,  \u_1 \right\rangle      \left\langle T\vX            ,\u_2  \right\rangle].\\    
% \end{align*}
% Then the proof is completed by noticing the following
% \begin{align*}
% \mb E[  \left\langle T\vX,  \u_1 \right\rangle      \left\langle T\vX            ,\u_2  \right\rangle]=\mb E[  \left\langle\vX,  T^*\u_1 \right\rangle      \left\langle \vX            ,T^*\u_2  \right\rangle].
% \end{align*}
% \end{proof}}



\section{Proof of Lemma \ref{lemma: SE=GammaS}}
Recall the following fact in FSIR.
\begin{lemma}\label{lemma, direct result of linearity condition}~\\
Under Assumption $\ref{as:Linearity condition and Coverage condition}~ \boldsymbol{\mathrm{{i)}}}$, we have $\mathcal S_{\mathbb E(\boldsymbol{X}|\Y)}\subseteq \Gamma \mc S_{\Y|\bs X}\subseteq \mc H$.
\end{lemma}
It is a trivial generalization of    \cite[Theorem 2.1]{ferre2003functional} from univariate response to multivariate response.
\paragraph{Proof of Lemma $\ref{lemma: SE=GammaS}$}
\begin{proof}
First, we prove that $\mathcal{S}_{\mathbb{E}(\bs X|\Y)}^\perp\subseteq \mathrm{Im}\{\mathrm{var(\mb{E}(\bs X|\Y))}\}^\perp$. For any ${\bs{\beta}}\in\mathcal{S}_{\mathbb{E}(\bs X|\Y)}^\perp$, one has $\langle{\bs{\beta}},\mb{E}(\bs X|\Y)\rangle=0$ a.s. Then for any $\bs\alpha\in\mathcal{H}$, one can get
\begin{align*}
\langle{\bs{\beta}},\mathrm{var}(\mb{E}(\bs X|\Y))\bs\alpha\rangle&=\langle{\bs{\beta}},\mb E\lmi\mb{E}(\bs X|\Y)\otimes \mb{E}(\bs X|\Y)\rmi\bs\alpha\rangle\\
&=\mb E\big(\langle\mb{E}(\bs X|\Y),\bs\alpha\rangle\langle{\bs{\beta}},\mb{E}(\bs X|\Y)\rangle\big)=0,
\end{align*}
which means that ${\bs{\beta}}\in\mathrm{Im}\{\mathrm{var}(\mb{E}(\bs X|\Y))\}^\perp$. Moreover, one has
\begin{align*}\mathcal{S}_{\mathbb{E}(\bs X|\Y)}^\perp\subseteq \mathrm{Im}\{\mathrm{var}(\mb{E}(\bs X|\Y))\}^\perp
%&\Rightarrow\left(\mathcal{S}_{\mathbb{E}(\bs X|Y)}^\perp\right)^\perp\supseteq \left(\mathrm{Im}\{\mathrm{var(\mb{E}(\bs X|Y))}\}^\perp\right)^\perp\\&
\Longrightarrow\overline{\mathcal{S}_{\mathbb{E}(\bs X|\Y)}}\supseteq\overline{\mathrm{Im}}\{\mathrm{var}(\mb{E}(\bs X|\Y))\}.
\end{align*}
Thus, $\overline{\mathrm{Im}}\{\mathrm{var}(\mb{E}(\bs X|\Y))\}\subseteq\overline{\mathcal{S}_{\mathbb{E}(\bs X|\Y)}}\subseteq\overline{\Gamma\mathcal{S}_{\Y|\bs X}}$ by Lemma $\ref{lemma, direct result of linearity condition}$. According to Assumption $\ref{as:Linearity condition and Coverage condition}$ \textbf{ii)}, one can get
\[\mathrm{dim}\left(\overline{\mathrm{Im}}\{\mathrm{var}(\mb{E}(\bs X|\Y))\}\right)=\mathrm{dim}\left(\overline{\mathcal{S}_{\mathbb{E}(\bs X|\Y)}}\right)=\mathrm{dim}(\overline{\Gamma\mathcal{S}_{\Y|\bs X}})=d.\]
One can complete the proof since finite dimension subspaces are closed.
\end{proof}

\section{Proof of Theorem \ref{theorem, MDDO and IRS}}
\begin{proof}
For convenience, we abbreviate $\mathrm{MDDO}(\boldsymbol{X}|\Y)$ to ${M}$. According to Theorem $\ref{theorem, MDDO and conditional mean independence}$ and Lemma $\ref{lem: Txx=0tuiTx=0}$, one can get
\begin{align*}{\bs{\beta}}\in\mathcal S_{\mb E(\boldsymbol{X}|\Y)}^\perp&\Longleftrightarrow\langle {\bs{\beta}},\mathbb E(\boldsymbol{X}|\Y)\rangle=0~~\text{a.s.}\Longleftrightarrow\mathbb E(\langle {\bs{\beta}},\boldsymbol{X}\rangle|\Y)=0~~\text{a.s.}\\
&\Longleftrightarrow\mathrm{MDDO}(\langle {\bs{\beta}},\boldsymbol{X}\rangle|\Y)=0\Longleftrightarrow\langle {M}{\bs{\beta}},{\bs{\beta}}\rangle=0\\
&\Longleftrightarrow{M}{\bs{\beta}}=0\Longleftrightarrow {\bs{\beta}}\in\mathrm{null}(M)=\overline{\mathrm{Im}}(M)^\perp,
\end{align*}
which means that $\mathcal S_{\mb E(\boldsymbol{X}|\Y)}^\perp=\overline{\mathrm{Im}}(M)^\perp$ and $\overline{\mathcal S_{\mb E(\boldsymbol{X}|\Y)}}=\overline{\mathrm{Im}}(M)$.
One can complete the proof since finite dimension subspaces are closed.
\end{proof}
\section{Proof of Lemma \ref{lemma, way of estimate truncate central subspace}}
Before proving Lemma $\ref{lemma, way of estimate truncate central subspace}$, we give the following lemma.
\begin{lemma}\label{lem: colPBP equal colPB operator}
Assume that $P$ is a bounded linear operator from a Hilbert space $\wt{\mc H}$ to itself and $B$ is a positive semi-definite operator from $\wt{\mc H}$ to itself. 
Then we have $\overline{\mathrm{Im}}(PBP^*)=\overline{\mathrm{Im}}(PB)$.
\end{lemma}
\begin{proof}
It suffices to show that $\mnull(BP^*)=\mnull(PBP^*)$. First, since $B$ is positive semi-definite, one has $\langle x,PBP^*x\rangle = \langle P^*x,BP^*x \rangle\geqslant 0~(\forall x\in\wt{\mc H})$. Thus $PBP^*$ is a positive semi-definite operator on $\wt{\H}$.
For any $y\in\wt{\H}$, we have 
\begin{align*}
PBP^*y=0\overset{(a)}{\Longleftrightarrow}\langle y,PBP^*y\rangle = \langle P^*y,BP^*y \rangle=0\overset{(b)}{\Longleftrightarrow} BP^*y=0. 
\end{align*}
where $(a)$ and $(b)$ come from Lemma $\ref{lem: Txx=0tuiTx=0}$.
Thus $\mnull(PBP^*)=\mnull(BP^*)$.
\end{proof}

\paragraph{Proof of Lemma $\ref{lemma, way of estimate truncate central subspace}$}
\begin{proof}
For convenience, we abbreviate $\mathrm{MDDO}(\boldsymbol{X}|\Y)$ and $\mathrm{MDDO}(\boldsymbol{X}^{(m)}|\Y)$ to ${M}$ and $M_m$ respectively. 

By Corollary $\ref{corollary, MDDO and central subspace}$, one can get $\Gamma\mathcal{S}_{\Y|\boldsymbol{X}}=\mathrm{Im}(M)$. Thus,
\begin{align}\label{eq: corollary, MDDO and central subspace}
\Pi_m\Gamma\mathcal{S}_{\Y|\boldsymbol{X}}=\Pi_m\mathrm{Im}(M)=\mathrm{Im}(\Pi_m M).
\end{align}
It is easy to check that
\begin{align}
\Gamma_m&:=\mathrm{var}(\bs X^{(m)})=\Pi_m\Gamma\Pi_m=\Pi_m\Gamma=\Gamma\Pi_m=\sum\limits_{i=1}^m\lambda_i\phi_i\otimes\phi_i.\label{eq: Gamma m def}
\end{align}
On the one hand, by the definition of $\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}$ and $\Gamma_m$ (see \eqref{def: truncated central subspace} and \eqref{eq: Gamma m def}), one can get
\begin{align}\label{eq:Pim Gamma S}
\Pi_m\Gamma\mathcal{S}_{\Y|\boldsymbol{X}}&=\Pi_m\Gamma\Pi_m\mathcal{S}_{\Y|\boldsymbol{X}}=(\Pi_m\Gamma)(\Pi_m\mathcal{S}_{\Y|\boldsymbol{X}})=\Gamma_m\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}.
\end{align}
On the other hand, one has $\overline{\mathrm{Im}}(\Pi_m M)=\overline{\mathrm{Im}}(\Pi_m M\Pi_m)$ by Lemma $\ref{lem: colPBP equal colPB operator}$. Since $\Pi_m M$ and $\Pi_m M\Pi_m$ are both of finite rank, one can further get
\begin{align*}
\mathrm{Im}(\Pi_m M)&=\overline{\mathrm{Im}}(\Pi_m M)=\overline{\mathrm{Im}}(\Pi_m M\Pi_m)=\mathrm{Im}(\Pi_mM\Pi_m).
\end{align*}
Then according to Theorem $\ref{theorem, MDDO and conditional mean independence}$(ii), one has
\begin{align}
\mathrm{Im}(\Pi_m M)=\mathrm{Im}(\Pi_mM\Pi_m)=\mathrm{Im}(M_m).\label{eq:Pim span M}
\end{align}
Combining \eqref{eq:Pim Gamma S}, \eqref{eq:Pim span M} with \eqref{eq: corollary, MDDO and central subspace}, one has $\Gamma_m\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}=\mathrm{Im}\{M_m\}$.
Finally, one can get $ \Gamma_m^\dagger\mathrm{Im}\{M_m\}=\Gamma_m^\dagger\Gamma_m\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}=\Pi_m\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}=\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}$.
\end{proof}

\section{Wely Inequality for a Self-adjoint and Compact Operator}\label{ap:Wely inequality for self-adjoint and compact operators}
First, we show the following three results in standard functional analysis textbook.
\begin{lemma}[Spectral theorem]\label{thm: Spectral theorem}Let $\wt{\mathcal{H}}$ be a Hilbert space and $A:\wt{\mc{H}}\to\wt{\mc{H}}$ be a compact, self-adjoint operator. There is an at most countable orthonormal basis $\{\wt e_j\}_{j\in J}$ ($J=\{1,\cdots,n\}$ or $\mathbb{Z}_{\geqslant1}$) of $\wt{\mathcal{H}}$ and eigenvalues $\{\wt\lambda_j\}_{j\in J}$ with $|\wt\lambda_1|\geqslant|\wt\lambda_2|\geqslant\cdots\geqslant0$ converging to zero, such that
\begin{align*}
x=\sum_{j\in J}\langle x,\wt e_j\rangle \wt e_j;\qquad Ax=\sum_{j\in J}\wt\lambda_j\langle x,\wt e_j\rangle \wt e_j,\qquad x \in\wt{\mathcal{H}}.
\end{align*}
\end{lemma}

\begin{lemma}[Rayleigh's principle]\label{lem:Rayleigh operator}Let $A$ be a compact, self-adjoint operator. If $\{\wt e_j\}_{j\in J}$ and $\{\wt\lambda_j\}_{j\in J}$ are eigenvectors and eigenvalues define in Lemma $\ref{thm: Spectral theorem}$ respectively. Then
\[|\wt\lambda_1|=\mathop{\sup\limits_{\|u\|=1}}|\langle Au,u\rangle|;\qquad|\wt\lambda_n|=\mathop{\sup\limits_{\|u\|=1}}_{u\in\{\wt e_1,\cdots,\wt e_{n-1}\}^\perp}|\langle Au,u\rangle|~(n\geqslant 2).\]
\end{lemma}
\begin{lemma}[Minimax theorem]\label{lem:minimax operator}
Assume that $A$ is a positive semi-definite and compact operator with its eigenvalues $\{\wt\lambda_i\}$ ordered as $\wt\lambda_1\geqslant\dots\geqslant \wt\lambda_n\geqslant\dots\geqslant 0$, then
$$
\wt\lambda_n=\inf_{E_{n-1}}\sup_{x\in E_{n-1}^\perp,\|x\|=1}\langle Ax,x\rangle
$$
where $E_{n-1}$ with dimension $n-1$ is a closed linear subspace of $\wt{\mc H}$.
\end{lemma}
Then we give the Wely inequality for a self-adjoint and compact operator.
\begin{proposition}\label{prop: wely operator}
Let $M=N+R$ where $M$, $N$ and $R$ are three self-adjoint and compact operators defined on a Hilbert space $\wt{\mc H}$. Also, $M$ and $N$ are positive semi-definite with their respective eigenvalues $\{\mu_i\},\{\nu_i\}$ ordered as follows
\begin{align*}
M:\mu_1\geqslant\dots\geqslant \mu_n\geqslant\dots\geqslant 0;\qquad
N:\nu_1\geqslant\dots\geqslant \nu_n\geqslant\dots\geqslant 0,
\end{align*}
while $R$'s eigenvalues are $\{\rho_i\}$ ordered as follows:
\[R:|\rho_1|\geqslant\dots\geqslant |\rho_n|\geqslant\dots\geqslant 0.\]
Then the following inequalities hold: $|\mu_k-\nu_k|\leqslant|\rho_1|=\|R\| $, $k\geqslant1$.
\end{proposition}
\begin{proof}
From Lemma $\ref{lem:minimax operator}$, we have:
\[\mu_n=\inf_{E_{n-1}}\sup_{x\in E_{n-1}^\perp,\|x\|=1}\langle Mx,x\rangle;\qquad\nu_n=\inf_{E_{n-1}}\sup_{x\in E_{n-1}^\perp,\|x\|=1}\langle Nx,x\rangle,\]
where $E_{n-1}$ with dimension $n-1$ is a closed linear subspace of $\wt{\mc H}$.
By Lemma $\ref{lem:Rayleigh operator}$, we have:
$$
\sup_{\|u\|=1}|\langle Ru,u\rangle|=|\rho_1|.
$$
Since $\langle Mu,u\rangle=\langle Nu,u\rangle+\langle Ru,u\rangle$, for any $\|u\|=1$, we have:
$$
\langle Nu,u\rangle-|\rho_1|\leqslant\langle Mu,u\rangle \leqslant \langle Nu,u\rangle+|\rho_1|.
$$
Then for any given $n-1$ dimensional closed linear subspace of $\wt{\mc H}$, we conclude
\begin{equation}\label{eq:max ineq}
\sup_{u\in E_{n-1}^\perp,\|u\|=1}\langle Nu,u\rangle-|\rho_1|\leqslant\sup_{u\in E_{n-1}^\perp,\|u\|=1}\langle Mu,u\rangle\leqslant \sup_{u\in E_{n-1}^\perp,\|u\|=1}\langle Nu,u\rangle+|\rho_1|.
\end{equation}
Take the infimum with respective to $E_{n-1}$ in \eqref{eq:max ineq}, we have
\[\nu_n-|\rho_1|\leqslant\mu_n\leqslant \nu_n+|\rho_1|\]
by Lemma $\ref{lem:minimax operator}$.
\end{proof}
The next result is a direct corollary of Proposition $\ref{prop: wely operator}$.
\begin{corollary}\label{coro:wely ineq operator}
Let $M$ and $N$ be two self-adjoint, positive semi-definite and compact operators defined on a Hilbert space $\wt{\mc H}$ with their respective eigenvalues $\{\mu_i\},\{\nu_i\}$ ordered as follows
\begin{align*}
M:\mu_1\geqslant\dots\geqslant \mu_n\geqslant\dots\geqslant 0\quad\text{and}\quad
N:\nu_1\geqslant\dots\geqslant \nu_n\geqslant\dots\geqslant 0.
\end{align*}
Then the following inequalities hold: $|\mu_k-\nu_k|\leqslant\|M-N\| $, $ k\geqslant1$.
\end{corollary}




\section{Proof of Proposition \ref{prop:bound hatMmd Mm}}
Before proving Proposition $\ref{prop:bound hatMmd Mm}$, we give the following conclusion, whose proof is deferred to the end of this section.
\begin{proposition}\label{proposition, concentration of MDDO}
Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{assumption: sub-Gaussian}$, for all $\gamma\in(0,1/2)$, there exist positive constants $D_0=D_0(\gamma,\sigma_0,\sigma_1)$, $D_1=D_1(\sigma_1)$, $D_2=D_2(\sigma_0,\sigma_1)$ and $n_0=n_0(\gamma,\sigma_0,\sigma_1)$ such that for all $n\geqslant n_0$ and
\[C\in \l D_0n^{\frac{2\gamma}{5}}-\ln\l D_1m^2n \r,D_2 n^{\frac{1}{5}}-\ln\l D_1m^2n \r \rmi,\]
we have
\begin{equation*}
\mathbb{P}\l\left\|\wh M_m- M_m\right\| <\l \frac{C+\ln( D_1m^2n)}{D_2}\r^{\frac52}\frac{12m}{\sqrt n}\r\geqslant 1-\exp(- C).
\end{equation*}
\end{proposition}
\paragraph{Proof of Proposition $\ref{prop:bound hatMmd Mm}$}
\begin{proof}




Using Corollary $\ref{coro:wely ineq operator}$, one can get
$
\lambda_i\l\wh M_m\r\leqslant \lno\wh M_m-M_m\rno +\lambda_i\l M_m\r
$. 
Since $\rank(M_m)=d$, one can get $\lambda_i(M_m)=0,~i\geqslant d+1$. Thus by Proposition $\ref{proposition, concentration of MDDO}$, one has
\begin{align}\label{eq:lambdai hat Mm upper bound}
\mathbb{P}\l\lambda_{d+1}(\wh M_m)<\l \frac{C+\ln\l D_1m^2n\r}{D_2}\r^{\frac52}\frac{12m}{\sqrt n}\r\geqslant 1-\exp(- C)\qquad(i\geq d+1). 
\end{align}
Notice that 
\begin{align*}\lno\wh M_m^d- M_m\rno &\leqslant\lno M_m-\wh M_m\rno +\lno\wh M_m-\wh M_m^d\rno ;\\
\lno\wh M_m-\wh M_m^d\rno &=\left\|\sum_{i=d+1}^\infty\wh\mu_i\wh\gamma_i\otimes \wh\gamma_i\right\| =\widehat{\lambda}_{d+1}=\lambda_{d+1}(\widehat{M}_m)
\end{align*}
by \eqref{wh M_m spectral decomposition}.
Then combing Proposition $\ref{proposition, concentration of MDDO}$ with \eqref{eq:lambdai hat Mm upper bound} can complete the proof.
\end{proof}


\paragraph{Proof of Proposition \ref{proposition, concentration of MDDO}}
\begin{proof}
Note that $\boldsymbol{X}^{(m)}=\sum\limits_{j=1}^m\langle \boldsymbol{X},\phi_j\rangle\phi_j$, then a simple calculation leads to
\begin{align*}
M_m&=-\sum_{i,j=1}^m\mathbb E\big[\langle \boldsymbol{X},\phi_i\rangle\langle \boldsymbol{X}',\phi_{j}\rangle\|\Y-\Y'\|\big]\phi_i\otimes\phi_j;\\
\widehat{M}_m&=-\sum_{i,j=1}^m\frac1{n^2}\sum_{k,\ell=1}^n\langle \boldsymbol{X}_k,\phi_i\rangle\langle \boldsymbol{X}_\ell,\phi_j\rangle\|\Y_k-\Y_\ell\|\phi_i\otimes\phi_j.
\end{align*}

For a operator $\Gamma'$ that can be expanded as $\Gamma':=\sum\limits_{i,j=1}^\infty a_{ij}\phi_i\otimes\phi_{j}$, let us define its maximal norm as $\|\Gamma'\|_{\mathrm{max}}=\sup\limits_{i,j}|a_{ij}|$.



\begin{lemma}\cite[Theorem 1]{mai2021slicing}\label{lemma, concentration of MDDOnm}
Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{assumption: sub-Gaussian}$, for all
$\gamma\in(0,1/2)$, there exist positive
constants $C_0=C_0(\gamma,\sigma_0,\sigma_1)$, $C_1=C_1(\sigma_1)$, $C_2 = C_2(\sigma_0;\sigma_1)$ and $n_0 = n_0(\gamma,\sigma_0,\sigma_1)$
such that for all $n\geqslant n_0$ and $\varepsilon\in(C_0 n^{-(1/2-\gamma)},1]$, we have
\begin{equation*}
\mathbb{P}\l\lno \widehat{M}_m-M_m\rno_{\max}>12\varepsilon\r\leqslant C_1 m^2n\exp\l- C_2\l\varepsilon^2 n\r^{1/5}\r.
\end{equation*}
\end{lemma}
\noindent Since $\lno\widehat{M}_m-M_m\rno \leqslant m\lno\widehat{M}_m-M_m\rno_{\mathrm{max}}$, one has
\begin{equation*}
\mathbb{P}\l\lno\widehat{M}_m-M_m\rno >12m\varepsilon\r\leqslant C_1 m^2n\exp\l-C_2\l\varepsilon^2 n\r^{1/5}\r.
\end{equation*}
Let $C=C_2\l\ve^2n\r^{1/5}-\ln\l C_1m^2n\r$ satisfying 
\begin{align*}
C\in\l C_2C_0^{2/5}n^{2\gamma/5}-\ln\l C_1m^2n\r,C_2n^{1/5}-\ln\l C_1m^2n\r\rmi,
\end{align*}
then one has
\begin{equation*}
\mathbb{P}\l\lno\widehat{M}_m-M_m\rno \leqslant\l \frac{C+\ln\l C_1m^2n\r}{C_2}\r^{\frac52}\frac{12m}{\sqrt{n}}\r>1- \exp(- C).
\end{equation*}
Then in order to complete the proof, one only need to choose $D_0$, $D_1$ and $D_2$ to be $C_2C_0^{2/5}$, $C_1$ and $C_2$ respectively. 
\end{proof}





\section{Properties of Sub-Gaussian Random Vectors}
We first review the definition of sub-Gaussian random variables.
\begin{definition}[Sub-Gaussian random variable and its upper-exponentially bounded constant]\label{def:sub gaussian variable}
A random variable $X$ is called a sub-Gaussian random variable if $X$ satisfies one of the following equivalent properties:
\begin{itemize}
 \item[1).] Tails. $\P(|X|>t)\leqslant \exp(1-t^{2}/K^{2}_{1})$ for any $t>0$;
 \item[2).] Moments. $\E[|X|^{p}]^{1/p}\leqslant K_{2}\sqrt{p}$ for any $p\geqslant 1$;
 \item[3).]Super-exponential moment: $\E[\exp(X^{2}/K^{2}_{3})]\leqslant \mr{e}$.

\noindent Moreover, if $\E[X]=0$, then the properties $1)-3)$ are also equivalent to the following one:
\item[4).] Moment generating function: $\E[\exp(tX)]\leqslant \exp(t^{2}K^{2}_{4})$ for all $t\in\R$.
\end{itemize}
Here $K_1$, $K_2$, $K_3$ and $K_4$ are four constants.
$K$ is called an upper-exponentially bounded constant of $X$ if 
$K\geqslant \max\{K_{1},K_{2},K_{3},K_{4}\}$.
\end{definition}
\begin{definition}[Sub-Gaussian random vector and its upper-exponentially bounded constant]\label{def,sub-Gaussian random vector,upper-exponentially bounded constant}
 ${X}\in\R^m$ is called a sub-Gaussian random vector if for all $x\in\R^m$, one-dimensional marginal $\langle{X},x\rangle$ is sub-Gaussian random variable. $K$ is called an upper-exponentially bounded constant of $X$ if $K$ satisfies:
 \begin{align*}
K\geqslant \sup_{x\in\mb{S}^{m-1}}K(\langle X,x\rangle) 
 \end{align*}
 where $K(\langle X,x\rangle)$ denotes an upper-exponentially bounded constant of $\langle X,x\rangle$.
Moreover, $K$ is called a uniform (about $m$) upper-exponentially bounded constant of $X$ if $K$ satisfies:
 \begin{align*}
K\geqslant \sup_m\sup_{x\in\mb{S}^{m-1}}K\l \langle X,x\rangle\r.
 \end{align*}
Furthermore, $X$ is called a uniform (about $m$) sun-Gaussian random vector.
 \end{definition}
The following is an application of sub-Gaussian random vectors.
\begin{lemma}[\citealt{vershynin2010introduction}]\label{lem:esgrm}
 Let $\M=[\bs m_1~\cdots~\bs m_n]$ be an $m\times n$ matrix ($n>m$) whose columns $\m_{i}$ are 
 independent centered sub-Gaussian random vectors with 
 covariance matrix $\mathbf{I}_{m}$. Let $\sigma^{+}_{\min}(\M)$ and $\sigma_{\max}(\M)$ be the infimum and supremum of positive singular values of $\M$ respectively. Then, for any $t>0$, with probability at least $1-2\exp(- C^{\prime}t^{2})$, we have
 \begin{equation*}
 \sqrt{n}-C_0\sqrt{m}-t\leqslant \sigma^{+}_{\min}(\M)\leqslant \sigma_{\max}(\M)\leqslant \sqrt{n}+C_0\sqrt{m}+t
 \end{equation*}
 where $C'$ and $C_0$ are two positive constants depending only on $K(\bs m_1)$:
 the upper-exponentially bounded constant of $\bs m_1$.
\end{lemma}
\noindent Let $t=\sqrt m$, then one can easily get
\begin{align}\label{equation, min max eval}
\begin{split}
\lambda_{\max}\left(\frac1n \M\M^\top\right)\leqslant \left(1+\frac{(C_0+1)\sqrt m}{\sqrt n}\right)^2;\\
\lambda_{\min}^+\left(\frac1n \M\M^\top\right)\geqslant \left(1-\frac{(C_0+1)\sqrt m}{\sqrt n}\right)^2, 
\end{split}
\end{align}
with probability at least $1-2\exp(- C'm)$ where $\lambda^{+}_{\min}(\cdot)$ and $\lambda_{\max}(\cdot)$ stands for the infimum and supremum of the positive spectrum respectively.



\begin{lemma}\label{lemma, estiamtion error of inverse sample cov}
Assume that $\x_1,\x_2,...,\x_n$ are $n$ i.i.d. samples from an $m$-dimensional centered sub-Gaussian vector with an invertible covariance matrix $\Sigma$. Let $\wh\Sigma:=\frac1n\sum_i \x_i\x_i^\top$.
Then there exists a positive constant $n_1'=n_1'(K(\bs m_1),c_1)$ ($c_1$ is defined in \eqref{eq: m n relationship}), such that when $n\geqslant n_1'$, we have
\begin{align*}
\lno\wh{\Sigma}-\Sigma\rno\hspace{-1.5mm}&\leqslant (C_0+2)^2\lambda_{\max}(\Sigma)\sqrt{\frac mn}~~\text{and}~~ \lno\wh{\Sigma}^{-1}-\Sigma^{-1}\rno\hspace{-1.5mm}\leqslant \frac{4(C_0+2)^2}{\lambda_{\min}(\Sigma)}\sqrt{\frac mn},
 \end{align*}
 with probability at least $1-2\exp(- C'm)$, where $C_0$ is defined in Lemma $\ref{lem:esgrm}$.
\end{lemma}
\begin{proof}
Let $\x_i=\Sigma^{\frac12}\m_i$ and $\bs{M}=[\bs m_1~\cdots~\bs m_n]$ where $\m_i$ is a centered sub-Gaussian random vector with covariance $\mathbf I_m$. Then one has 
\begin{align*}
\lno\wh\Sigma-\Sigma\rno&\leqslant\lno\Sigma^{\frac12}\rno\cdot\left\|\frac1n \M\M^\top-\mathbf I\right\|\cdot\lno\Sigma^{\frac12}\rno\\
&= \lambda_{\max}(\Sigma)\cdot\left[\lambda_{\max}\left(\frac1n \M\M^\top\right)-1\right]
\end{align*}
and 
\begin{align*}
\lno\wh{\Sigma}^{- 1}-\Sigma^{- 1}\rno
&\leqslant \lno\Sigma^{-\frac12}\rno\cdot\left\|\frac1n \M\M^\top-\mathbf I\right\|\cdot\lno\l\frac1n \M\M^\top\r^{-1}\rno\cdot\lno\Sigma^{-\frac12}\rno\\
&=\frac{1}{\lambda_{\min}(\Sigma)}\left[\lambda_{\max}\left(\frac1n \M\M^\top\right)-1\right]\cdot\lambda_{\min}\left(\frac1n \M\M^\top\right)^{-1}.
\end{align*}
By \eqref{equation, min max eval}, it is easy to check that
\begin{align*}&\lambda_{\max}\left(\frac1n \M\M^\top\right)-1\leqslant\left(1+\frac{(C_0+1)\sqrt m}{\sqrt n}\right)^2-1\leqslant\frac{(C_0+2)^2\sqrt m}{\sqrt n};\\
&\lambda_{\min}\left(\frac1n \M\M^\top\right)\geqslant \left(1-\frac{(C_0+1)\sqrt m}{\sqrt n}\right)^2\geqslant \frac14~\text{for}~n\geqslant [2(C_0+1)]^{\frac2{1-c_1}},
\end{align*}
with probability at least $1-2\exp(- C'm)$. Thus the proof is completed by choosing $n_1'(C_0,c_1):=[2(C_0+1)]^{\frac{2}{1-c_1}}$. 
\end{proof}

\section{Proof of Proposition \ref{prop:concentration Gammam dag Mmd}}\label{ap:concentration inequality}
We first give the following lemma whose proof is deferred to the end of this section.
\begin{lemma}\label{lem:PimTPimtoT}If $T$ is of finite rank, then we have $\lim\limits_{m\to \infty}\|\Pi_m T\Pi_m-T\| =0$.
\end{lemma}
A direct corollary of this lemma is as follows.
\begin{corollary}\label{lemma, M go to Mm}
%For any $\varepsilon>0$, one has $\|M-M_m\| <\varepsilon$ when $m$ is sufficiently large.
Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{as:Linearity condition and Coverage condition}$, we have $\lim\limits_{m\to\infty}\|M-M_m\| =0$.
\end{corollary}
\noindent We denote by $m_M(\varepsilon)$ the minimal integer $m_M$ satisfying $\|M-M_m\| \leqslant \varepsilon$ for all $m\geqslant m_M$.

Proposition $\ref{prop:concentration Gammam dag Mmd}$ is a direct corollary of the following Proposition.
\begin{proposition}
\label{prop:bound of finite estimate}
 Suppose that Assumptions $\ref{as:joint distribution assumption}$ to $\ref{assumption: rate-type condition}$ hold, then $\forall \gamma\in(0,1/2)$, there exist positive constants
 \begin{align*}
 n_1=n_1(\gamma,\sigma_0,\sigma_1,\bs K,m_M(1),c_1),\quad D_3=D_3(\|M\| ,\wt C,\bs K) 
 \end{align*}
and $C'=C'(\bs K)$
, such that when $n\geqslant n_1$, we have
\begin{equation*}
\begin{aligned}
\mb P\l \lno\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\rno  \leqslant \left[\frac{C+\ln(D_1m^2n)}{D_2}\right]^{\frac52}\frac{24m^{\alpha_1+1}}{\wt C\sqrt n}+D_3\frac{m^{(2\alpha_1+1)/2}}{n^{1/2}} \r&\\
\geqslant 1-\exp(- C)-2\exp(- C'm).&
\end{aligned}
\end{equation*}
Here $D_1,D_2$ and $C$ are defined in Proposition $\ref{prop:bound hatMmd Mm}$ and $\bs K$ is the uniform upper-exponentially bounded constant of $(\sqrt{\lambda_1}w_1,\dots,\sqrt{\lambda_m}w_m)$. 
\end{proposition}
\begin{proof}
By triangle inequality, one has
\begin{align*}
&\lno\widehat{\Gamma}_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\rno 
=\lno\widehat\Gamma_m^\dagger \widehat M_m^d-\wh\Gamma_m^\dagger M_m+\wh\Gamma_m^\dagger M_m-\Gamma_m^\dagger M_m\rno 
\\&\qquad\leqslant\lno\Gamma_m^\dagger\rno \cdot \lno\widehat M_m^d-M_m\rno +\lno\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\rno \cdot \lno M_m\rno .
\end{align*}
Thus one can bound $\lno\Gamma_m^{\dag}M_m-\widehat\Gamma_m^{\dag}\widehat M_m^d\rno $ by bound $\lno\Gamma_m^\dagger\rno $, $\lno\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\rno $, $\lno\widehat M_m^d-M_m\rno $ and $\lno M_m\rno $ respectively.
\begin{itemize}
 \item\textbf{Bound of $\lno\Gamma_m^\dagger\rno $}: By Assumption $\ref{assumption: rate-type condition}$, one has 
\begin{align}\label{eq:bound Gammam dagger}
\lambda_j\geqslant \wt C j^{-\alpha_1}\Rightarrow\lno\Gamma_m^\dagger\rno =\lambda_m^{-1}\leqslant \wt{C}^{-1} m^{\alpha_1}. 
\end{align} 
 \item\textbf{Bound of $\lno\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\rno $}:
 Let us define $\mc H_m:=\mathrm{span}\{\phi_1,\dots,\phi_m\}$ where $\{\phi_i\}$ is introduced in Equation $\eqref{eq:X expansion}$. It is easy to check that
$\lno\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\rno =\lno(\widehat\Gamma_m^\dagger-\Gamma_m^\dagger)|_{\mc H_m}\rno $ since $\l\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\r{\bs{\beta}}=0$ for any ${\bs{\beta}}\in\mc{H}_m^\perp$. 
Because $\l\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\r|_{\mc H_m}$ can be represented by matrix $\widehat{\Sigma}^{-1}-\Sigma^{-1}$ defined in Lemma $\ref{lemma, estiamtion error of inverse sample cov}$ under orthonormal basis $\{\phi_i\}_{i=1}^m$, one can get $\lno\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\rno =\|\widehat{\Sigma}^{-1}-\Sigma^{-1}\|$.
Similarly, one can also get $\lno\Gamma_m^\dagger\rno =\lno\Sigma^{-1}\rno=\lambda_{\min}^{-1}(\Sigma)$. Thus, by Lemma $\ref{lemma, estiamtion error of inverse sample cov}$ one has
\[\mb P\l\lno\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\rno \leqslant {4(C_0+2)^2}\lno\Gamma^{\dag}_m\rno \sqrt{\frac mn}\r\geqslant 1-2\exp(- C'm)\]
for sufficiently large $n\geqslant n_1'(\bs K,c_1)$
. Combing with $\lno\Gamma_m^\dagger\rno \hspace{-1mm}\leqslant \wt{C}^{-1} m^{\alpha_1}$, one can get
\begin{equation}\label{eq: distance hat gamma m dagger hat gamma m dagger}
\mb P\l\lno\widehat\Gamma_m^\dagger-\Gamma_m^\dagger\rno \leqslant \frac{4(C_0+2)^2m^{(2\alpha_1+1)/2}}{\wt Cn^{1/2}}\r\geqslant 1-2\exp(- C'm)
\end{equation}
for sufficiently large $n\geqslant n_1'(\bs K,c_1)$.
 \item\textbf{Bound of $\lno\widehat M_m^d-M_m\rno $}:
 See Proposition $\ref{prop:bound hatMmd Mm}$.
 \item \textbf{Bound of $\lno M_m\rno $}: By Corollary $\ref{lemma, M go to Mm}$, $\|M-M_m\| \leqslant 1$ for sufficiently large $m\geqslant m_M(1)$. Then by triangle inequality, one can get
\[\|M_m\| -\|M\| \leqslant \|M-M_m\| \leqslant 1.\]
Hence,
\begin{align}\label{eq:Mm leq M C}
\|M_m\| \leqslant \|M\| +1.
\end{align}
\end{itemize}
Combing \eqref{eq:bound Gammam dagger}, \eqref{eq: distance hat gamma m dagger hat gamma m dagger}, Proposition $\ref{prop:bound hatMmd Mm}$ with \eqref{eq:Mm leq M C}, one can choose $D_3$ and $n_1$ to be $\frac{4(C_0+2)^2(\|M\| +1)}{\wt C}$ and $\max\{n_0,n_1'(\bs K,c_1),m_M(1)^{1/c_1}\}$ respectively to
complete the proof where $n_0$ is defined in Proposition $\ref{prop:bound hatMmd Mm}$.
\end{proof}

\paragraph{Proof of Lemma \ref{lem:PimTPimtoT}}
\begin{proof}By the triangle inequality and compatibility of operator norm, one has
\begin{align*}
\|\Pi_m T\Pi_m-T\| &\leqslant\|\Pi_mT\Pi_m-\Pi_mT\| +\|\Pi_mT-T\| \\
&\leqslant\|(\Pi_m-I)T^*\| +\|(\Pi_m-I)T\| 
\end{align*}
where $I=\sum\limits_{i=1}^\infty\phi_i\otimes\phi_i$ for $\{\phi_i\}_{i\in\mb{Z}_{\geqslant 1}}$ defined in \eqref{eq:X expansion} being an orthonormal basis of $\mc H$. 
% Since the adjoint of $M(\Pi_m-I)$ is $(\Pi_m-I)M$, we have
% \begin{align*}&\|M(\Pi_m-I)\| +\|(\Pi_m-I)M\| \\
% =&
% \end{align*}

Since $T$ is of finite rank, let us assume that $\{e_i\}_{i=1}^k$ is an orthonormal basis of $\mathrm{Im}(T)$ where $k=\mr{rank}(T)$. For any ${\bs{\beta}}\in\mathcal{H}$ such that $\|{\bs{\beta}}\|=1$, one has $\|T{\bs{\beta}}\|\leqslant\|T\| \|{\bs{\beta}}\|=\|T\| $, so one can assume that $T{\bs{\beta}}\in\mathrm{Im}(T)$ admits the following expansion under basis $\{e_i\}_{i=1}^k$:
\[T{\bs{\beta}}=\sum_{i=1}^k b_ie_i,\quad \sum_{i=1}^k b^2_i\leqslant\|T\| ^2<\infty.\]
Thus
\[\|(I-\Pi_m)T{\bs{\beta}}\|=\left\|\sum_{i=1}^k(I-\Pi_m) b_ie_i\right\|\leqslant\sum_{i=1}^k |b_i|\cdot\|(I-\Pi_m) e_i\|.\]
Clearly, $\|(\Pi_m-I)\alpha\|~(\forall\alpha\in\H)$ tends to $0$ as $m\to\infty$ since 
\[(I-\Pi_m)\alpha=\left(\sum_{i={m+1}}^\infty\phi_i\otimes\phi_i\right)\left(\sum\limits_{i=1}^\infty c_i\phi_i\right)=\sum_{i=m+1}^\infty c_i\phi_i\xrightarrow{m\to\infty} 0\]
where we have assumed that $\alpha=\sum\limits_{i=1}^\infty c_i\phi_i$ .

Thus $\forall\varepsilon>0$, there exists some $N_i>0$ such that $\forall m> N_i$ one has $\|(\Pi_m-I)e_i\|<\varepsilon$, $(\forall i=1,...,k)$. Let $N=\max\{N_1,\cdots,N_k\}$, then $\forall m>N$ one has
\[\|(I-\Pi_m)T{\bs{\beta}}\|\leqslant\sum_{i=1}^k |b_i|\cdot\|(I-\Pi_m) e_i\|\leqslant\sum_{i=1}^k |b_i|\varepsilon\leqslant k\varepsilon\|T\| ,\]
which means that $\forall m>N$, one has
\begin{align*}
\|(\Pi_m-I)T\| &=\sup_{\|{\bs{\beta}}\|=1}\|(\Pi_m-I)T{\bs{\beta}}\|\leqslant k\varepsilon\|T\| . 
\end{align*}
Thus $\lim\limits_{m\to\infty}\|(\Pi_m-I)T\| =0$. 

Similarly, one can also get $\lim\limits_{m\to\infty}\|(\Pi_m-I)T^*\| =0$. Then the proof of Lemma $\ref{lem:PimTPimtoT}$ is completed.
\end{proof}
% \section{Sin Theta Theorem}\label{ap:Sin Theta theorem}
% \subsection{Sin Theta Theorem for Self-adjoint Operators}
% \begin{lemma}[Proposition 2.3 in \cite{seelmann2014notes}]\label{lemma, sin theta of infinite dimension operator}
% Let $B$ be a self-adjoint operator on a separable Hilbert space $\widetilde{\mathcal{H}}$, and let ${V}\in\mathcal{L}(\widetilde{\mathcal{H}})$ be another self-adjoint operator where $\mathcal{L}\l\widetilde{\mc H}\r$ stands for the space of bounded linear operators from a Hilbert space $\widetilde{\mc H}$ to $\widetilde{\mc H}$.
% Write \[\mathrm{spec}( B)=\sigma\cup\Sigma\quad\text{and}\quad \mathrm{spec}( B+ V)=\omega\cup\Omega
% \]
% with $\sigma\cap\Sigma=\varnothing=\omega\cap\Omega$, and suppose that there is $\widehat d>0$ such that
% \[\mathrm{dist}(\sigma,\Omega)\geqslant \widehat d\quad\text{and}\quad\mathrm{dist}(\Sigma,\omega)\geqslant \wh d\]
% where $\mathrm dist(\sigma,\Sigma):=\min\{|a-b|:a\in\sigma,b\in\Omega\}$.
% Then, the operator angle $\Theta=\Theta(P_{ B}(\sigma),P_{ B+ V}(\omega))$ satisfies the bound
% \[\|\sin\Theta\|:=\|P_{{B}}(\sigma)-P_{{B}+{V}}(\omega)\| \leqslant\frac\pi2\frac{\| V\| }{\wh d}\]
% where $P_{ B}(\sigma)$ denotes the spectral projection for $ B$ associated with $\sigma$, i.e., 
% \[P_{B}(\sigma):=\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}\frac{\mathrm{d}z}{z-B},\]
% where $\gamma$ is a contour on $\mathbb{C}$ that encloses $\sigma$ but no other elements of $\mathrm{spec}( B)$.
% \end{lemma}
% \begin{remark}
% We note that, 
% if further $ B$ is compact, 
% the spectral projection coincide with projection operator onto the closure of the space spanned by the eigenfunctions associated with the eigenvalues in $\sigma$.

% If $B$ is compact, by the spectral decomposition theorem one has
% \[B=\sum_{i=1}^\infty\mu_ie_i\otimes e_i\quad\text{and}\quad(z- B)^{-1}=\sum_{i=1}^\infty(z-\mu_i)^{-1}e_i\otimes e_i,\]
% where $\mr{spec}(B):=\{\mu_i\}_{i=1}^\infty$ satisfies $|\mu_i|\xrightarrow{i\to\infty} 0$.
% Then $\forall v\in \mathcal{H}$,
% \begin{align*}P_{B}(\sigma)v&=\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}({z-B})^{-1}v~{\mathrm{d}z}=\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}\sum_{i=1}^\infty(z-\mu_i)^{- 1}\langle e_i,v\rangle e_i~{\mathrm{d}z}\\
% &=\sum_{i=1}^\infty\left[\left(\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}(z-\mu_i)^{-1}~{\mathrm{d}z}\right)\langle e_i,v\rangle e_i\right]=\sum_{i\in\{i:\mu_i\in\sigma\}}\langle e_i,v\rangle e_i.
% \end{align*}
% Especially, if $\sigma=\mr{spec}(B)\backslash\{0\}$, then $P_{B}(\sigma)$ is the projection operator onto the $\overline{\mathrm{Im}}(B)$.
% \end{remark}
% Splitting eigenvalues into nonzero part and zero part yields the following useful corollary.
% \begin{corollary}\label{cor: sin theta self adjoint}
% Let $B$ and $B'$ be two positive semi-definite {and compact} operators with finite rank on a separable Hilbert space $\widetilde{\mathcal{H}}$. Let $\lambda_{\min}^+( B)$ and $\lambda_{\min}^+(B')$ be the infimum of the positive eigenvalues of ${B}$ and ${B}'$ respectively. Then we have
% \[\left\|P_{ B}-P_{ B'}\right\| \leqslant\frac\pi2\frac{\| B- B'\| }{\min\{\lambda_{\min}^+( B),\lambda_{\min}^+( B')\}}.\]
% \end{corollary}
% \subsection{Sin Theta Theorem for General Operators}
% When ${B}$ and ${V}$ in Lemma $\ref{lemma, sin theta of infinite dimension operator}$ are not self-adjoint, we use the symmetrization trick, which mainly depends on the following Lemma.
% \begin{lemma}\label{lem:projection equality}
% $P_A=P_{AA^*}$ for any bounded linear operator $A$ from a Hilbert space $\wt\H$ to $\wt\H$. Especially, $P_A=P_{AA^{\top}}$ for any matrix $A$.
% \end{lemma}
% \begin{proof}This lemma is a direct corollary of Lemma $\ref{lem: colPBP equal colPB operator}$.
% \end{proof}

% Then we have the following Sin Theta theorem for general operator.
% \begin{lemma}\label{lemma, sin theta of nonadjoint operator}
% Let $ B,B'\in\mathcal{L}(\widetilde{\mathcal{H}})$ be two compact operators (not necessarily self-adjoint) with finite rank.
% Then we have
% \begin{align*}
% \left\|P_{ B}-P_{ B'}\right\| &\leqslant\frac\pi2\frac{\| B B^*- B'B'^*\| }{\min\lb\sigma_{\min}^+( B)^2,\sigma_{\min}^+(B')^2\rb}\\
% &\leqslant \frac\pi2\frac{\| B- B'\| ^2+2\| B- B'\| \| B'\| }{\min\lb\sigma_{\min}^+( B)^2,\sigma_{\min}^+( B')^2\rb}.
% \end{align*}
% \end{lemma}
% \begin{proof}By Lemma $\ref{lem:projection equality}$, one can get $\left\|P_{ B}-P_{ B'}\right\| =\left\|P_{ B B^*}-P_{ B' B'^*}\right\| $.
% Since $ BB^*, B'B'^*$ are both self-adjoint and compact, by Lemma $\ref{cor: sin theta self adjoint}$, one has
% \begin{align*}
% \left\|P_{ B B^*}-P_{ B' B'^*}\right\| \leqslant \frac{\pi}{2}\frac{\| B B^*- B' B'^*\| }{\min\lb\lambda_{\min}^+\l B B^*\r,\lambda_{\min}^+\l B' B'^*\r\rb}.
% \end{align*}
% Then the proof is completed in view of the following inequality:
% % of $\| B B^*- B' B'^*\| $:
% \begin{align}
% \lno B B^*- B' B'^*\rno &= \|( B- B')( B- B')^*\hspace{-0.5mm}+\hspace{-0.5mm}( B-B')(B')^*\hspace{-0.5mm}+\hspace{-0.5mm} B'( B- B')^*\| \nonumber\\
% &\leqslant \| B- B'\| ^2+2\| B- B'\| \| B'\| . \label{eq:sy ineq}
% \end{align}
% \end{proof}


\section{Sin Theta Theorem}\label{ap:Sin Theta theorem}
\subsection{Sin Theta Theorem for Self-adjoint Operators}
\begin{lemma}[Proposition 2.3 in \cite{seelmann2014notes}]\label{lemma, sin theta of infinite dimension operator}
Let $B$ be a self-adjoint operator on a separable Hilbert space $\widetilde{\mathcal{H}}$, and let ${V}\in\mathcal{L}(\widetilde{\mathcal{H}})$ be another self-adjoint operator where $\mathcal{L}\left(\widetilde{\mc H}\right)$ stands for the space of bounded linear operators from a Hilbert space $\widetilde{\mc H}$ to $\widetilde{\mc H}$.
Write the spectra of $B$ and $B+V$ as \[\mathrm{spec}( B)=\sigma\cup\Sigma\quad\text{and}\quad \mathrm{spec}( B+ V)=\omega\cup\Omega
\]
with $\sigma\cap\Sigma=\varnothing=\omega\cap\Omega$, and suppose that there is $\widehat d>0$ such that
\[\mathrm{dist}(\sigma,\Omega)\geqslant \widehat d\quad\text{and}\quad\mathrm{dist}(\Sigma,\omega)\geqslant \wh d\]
where $\mathrm dist(\sigma,\Sigma):=\min\{|a-b|:a\in\sigma,b\in\Omega\}$.
Then it holds that
\[\|P_{{B}}(\sigma)-P_{{B}+{V}}(\omega)\| \leqslant\frac\pi2\frac{\| V\| }{\wh d}\]
where $P_{ B}(\sigma)$ denotes the spectral projection for $ B$ associated with $\sigma$, i.e., 
\[P_{B}(\sigma):=\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}\frac{\mathrm{d}z}{z-B},\]
where $\gamma$ is a contour on $\mathbb{C}$ that encloses $\sigma$ but no other elements of $\mathrm{spec}( B)$.
\end{lemma}
\begin{remark}
We note that, 
if further $ B$ is compact, 
the spectral projection coincide with projection operator onto the closure of the space spanned by the eigenfunctions associated with the eigenvalues in $\sigma$. 
% For more details, see, e.g., Remark 1 in \cite{chen2023optimality}.

Specifically, if $B$ is compact, by the spectral decomposition theorem one has
\[B=\sum_{i=1}^\infty\mu_ie_i\otimes e_i\quad\text{and}\quad(z- B)^{-1}=\sum_{i=1}^\infty(z-\mu_i)^{-1}e_i\otimes e_i,\]
where $\mr{spec}(B):=\{\mu_i\}_{i=1}^\infty$ satisfies $|\mu_i|\xrightarrow{i\to\infty} 0$.
Then $\forall v\in \mathcal{H}$, it holds that
\begin{align*}P_{B}(\sigma)v&=\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}({z-B})^{-1}v~{\mathrm{d}z}=\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}\sum_{i=1}^\infty(z-\mu_i)^{- 1}\langle e_i,v\rangle e_i~{\mathrm{d}z}\\
&=\sum_{i=1}^\infty\left[\left(\frac{1}{2\pi\mathrm{i}}\oint_{\gamma}(z-\mu_i)^{-1}~{\mathrm{d}z}\right)\langle e_i,v\rangle e_i\right]=\sum_{i\in\{i:\mu_i\in\sigma\}}\langle e_i,v\rangle e_i.
\end{align*}
In particular, if $\sigma=\mr{spec}(B)\backslash\{0\}$, then $P_{B}(\sigma)$ is the projection operator onto the $\overline{\mathrm{Im}}(B)$.
\end{remark}

Splitting eigenvalues into nonzero part and zero part yields the following useful corollary.
\begin{corollary}\label{cor: sin theta self adjoint}
Let $B$ and $B'$ be two positive semi-definite {and compact} operators with finite rank on a separable Hilbert space $\widetilde{\mathcal{H}}$. Let $\lambda_{\min}^+( B)$ and $\lambda_{\min}^+(B')$ be the infimum of the positive eigenvalues of ${B}$ and ${B}'$ respectively. Then we have
\[\left\|P_{ B}-P_{ B'}\right\| \leqslant\frac\pi2\frac{\| B- B'\| }{\min\{\lambda_{\min}^+( B),\lambda_{\min}^+( B')\}}.\]
\end{corollary}
\subsection{Sin Theta Theorem for General Operators}
When ${B}$ and ${V}$ in Lemma $\ref{lemma, sin theta of infinite dimension operator}$ are not self-adjoint, we use the symmetrization trick, which mainly depends on the following Lemma.
\begin{lemma}\label{lem:projection equality}
$P_A=P_{AA^*}$ for any bounded linear operator $A$ from a Hilbert space $\wt\H$ to $\wt\H$. Especially, $P_A=P_{AA^{\top}}$ for any matrix $A$.
\end{lemma}
\begin{proof}First we show that the null space of  $A^*$ is the same as the null space of $AA^*$.
On the one hand, 
\[x\in\mathrm{null}(A^*)\Longrightarrow
A^*x=0\Longrightarrow AA^*x=0\Longrightarrow x\in\mathrm{null}(AA^*); 
\]
One the other hand,
\begin{align*}x\in\mathrm{null}(AA^*)&\Longrightarrow
AA^*x=0\Longrightarrow \langle x,AA^*x\rangle=\langle A^*x,A^*x\rangle=\|A^*x\|^2=0\\
&\Longrightarrow A^*x=0\Longrightarrow x\in\mathrm{null}(A^*).
\end{align*}
Hence, we have $\mathrm{null}(A^*)=\mathrm{null}(AA^*)$. Take the orthogonal complement of the both sides of this equality, we can get
\[\mathrm{null}(A^*)^{\perp}=\mathrm{null}(AA^*)^{\perp}\Longrightarrow {\mathrm{Im}(A)}={\mathrm{Im}(AA^*)}.\]
\end{proof}
Then we have the following Sin Theta theorem for general operator.
\begin{lemma}\label{lemma, sin theta of nonadjoint operator}
Let $ B,B'\in\mathcal{L}(\widetilde{\mathcal{H}})$ be two compact operators (not necessarily self-adjoint) with finite rank.
Then we have
\begin{align*}
\left\|P_{ B}-P_{ B'}\right\| &\leqslant\frac\pi2\frac{\| B B^*- B'B'^*\| }{\min\left\{\sigma_{\min}^+( B)^2,\sigma_{\min}^+(B')^2\right\}}\\
&\leqslant \frac\pi2\frac{\| B- B'\| ^2+2\| B- B'\| \| B'\| }{\min\left\{\sigma_{\min}^+( B)^2,\sigma_{\min}^+( B')^2\right\}}.
\end{align*}
\end{lemma}
\begin{proof}By Lemma $\ref{lem:projection equality}$, one can get $\left\|P_{ B}-P_{ B'}\right\| =\left\|P_{ B B^*}-P_{ B' B'^*}\right\| $.
Since $ BB^*, B'B'^*$ are both self-adjoint and compact, by Lemma $\ref{cor: sin theta self adjoint}$, one has
\begin{align*}
\left\|P_{ B B^*}-P_{ B' B'^*}\right\| \leqslant \frac{\pi}{2}\frac{\| B B^*- B' B'^*\| }{\min\left\{\lambda_{\min}^+\left( B B^*\right),\lambda_{\min}^+\left( B' B'^*\right)\right\}}.
\end{align*}
Then the proof is completed in view of the following inequality:
% of $\| B B^*- B' B'^*\| $:
\begin{align}
\left\| B B^*- B' B'^*\right\| &= \|( B- B')( B- B')^*\hspace{-0.5mm}+\hspace{-0.5mm}( B-B')(B')^*\hspace{-0.5mm}+\hspace{-0.5mm} B'( B- B')^*\| \nonumber\\
&\leqslant \| B- B'\| ^2+2\| B- B'\| \| B'\| . \label{eq:sy ineq}
\end{align}
\end{proof}


\section{Proof of Theorem \ref{theorem, total convergence rate}}
Thanks to the triangle inequality, one can bound the subspace estimation error by bounding the error term (i): $\mathbf{ Loss}_1:=\left\|P_{\mc S_{\Y|\X}^{(m)}}-P_{ \widehat {\mc S}_{\Y|\X}^{(m)}}\right\| $ and error term (ii): $\mathbf{ Loss}_2:= \left\|P_{\mathcal S_{\Y|\boldsymbol{X}}}-P_{\mathcal S_{\Y|\boldsymbol{X}}^{(m)}}\right\| $ respectively.
\subsection{Upper bound of error term (i)}
We first give the following lemmas, whose proofs are all deferred to the end of this section.
\begin{lemma}\label{lem:Gammam dagger Mm uniformly bounded}
% Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{as:Linearity condition and Coverage condition}$,
% $\{\|\Gamma_m^\dagger M_m\| \}_{m=1}^\infty$ is uniformly (about $m$) bounded by $\|\Gamma^{-1}M\| $.
Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{as:Linearity condition and Coverage condition}$, it holds that $\|\Gamma_m^\dagger M_m\| \leq \|\Gamma^{-1}M\| (\forall m).$
% \begin{align*}
% \|\Gamma_m^\dagger M_m\| \leq \|\Gamma^{-1}M\| \quad\forall m.
% \end{align*}
% $\{\|\Gamma_m^\dagger M_m\| \}_{m=1}^\infty$ is uniformly (about $m$) bounded by $\|\Gamma^{-1}M\| $.
\end{lemma}
\begin{lemma}\label{lem: Gamma inverse M to Gammam dagger Mm}Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{as:Linearity condition and Coverage condition}$, we have \[\lim\limits_{m\to\infty}\lno\Gamma^{-1}M-\Gamma_m^\dagger M_m\rno =0.\]
\end{lemma}
\noindent We denote by $m_T(\varepsilon)$ the minimal integer $m_T$ satisfying $\lno\Gamma^{- 1}M-\Gamma_m^\dagger M_m\rno \hspace{-1mm}\leqslant \varepsilon$ for all $m\geqslant m_T$ and define an event 
$$\ttE:=\lb \left\|\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\right\|  \leqslant\hspace{-0.5mm}\left(\tfrac{D_0+1}{D_2}\right)^{\frac52}\tfrac{24}{\wt C}n^{c_1(\alpha_1+1)+\gamma-\frac{1}{2}}+D_3n^{\frac{c_1(2\alpha_1+1)-1}{2}}\rb.$$
Then by taking $C$ to be $(D_0+1)n^{\frac{2\gamma}{5}}-\ln\l D_1m^2n \r$ in  Proposition \ref{prop:bound of finite estimate}, one has: for $n\geqslant \l\frac{D_0+1}{D_2}\r^{\frac{5}{1-2\gamma}}$,
$$\P(\ttE)\geq 1-D_1m^2n\exp\left[-(D_0+1)n^{\frac{2\gamma}{5}}\right] -2\exp(- C'm).$$
\begin{lemma}\label{lem:lower bound sigma min total}
Introducing $
\bigtriangleup :=\max\lb \frac{\sigma_d(\Gamma^{-1} M)}{2},\frac{\sigma_d(\Gamma^{-1} M)^2}{4\|\Gamma^{-1}M\| } \rb$.
Suppose that Assumptions $\ref{as:joint distribution assumption}$ to $\ref{assumption: rate-type condition}$ hold, $c_1(2\alpha_1+1)-1<0$ and $2(c_1(\alpha_1+1)+\gamma)-1<0$. Then there exists a positive constant
\begin{align*}
n_2'=n_2'\l\sigma_d(\Gamma^{-1}M),\|\Gamma^{-1}M\| , \gamma,\sigma_0,\sigma_1,\bs K,m_M(1),c_1,m_T\l \tfrac{\bigtriangleup}{2}\r,\wt C,\alpha_1\r
\end{align*}
such that when $n\geqslant n_2'$, we have
\begin{align}
\sigma_{\min}^+(\Gamma_m^{\dagger} M_m)^2\geqslant \tfrac{\sigma_d(\Gamma^{-1}M)^2}{2} \label{eq: lower bound of sigma min}. 
\end{align}
Furthermore, Conditioning on $\ttE$, we have
\begin{align}\label{eq: lower bound of sigma min hat}
&\sigma_{\min}^+(\wh\Gamma_m^{\dagger}\wh M_m^d)^2\geqslant \tfrac{\sigma_d(\Gamma^{-1}M)^2}{2}.
\end{align}
\end{lemma}
The following proposition is an upper bound of error term (i):
\begin{proposition}\label{proposition, estimation error}
Positive constants $D_1$, $D_2$ and $C'$  as in Proposition $\ref{prop:bound of finite estimate}$,
suppose that Assumptions $\ref{as:joint distribution assumption}$ to $\ref{assumption: rate-type condition}$ hold, then $\forall \gamma\in(0,1/2)$, if $c_1$ satisfies $2c_1(\alpha_1+1)+2\gamma-1<0$ and $c_1(2\alpha_1+1)-1<0$, there exists a positive constant $C_1:=C_1\l \|\Gamma^{-1}M\| ,\sigma_d(\Gamma^{-1}M) ,\wt C,\gamma,\sigma_0,\sigma_1\r$ such that
\begin{align*}
\P\l
\lno P_{\mc{S}_{\Y|\X}^{(m)}}-P_{ \widehat{\mc{S}}_{\Y|\X}^{(m)}}\rno \leqslant C_1\frac{m^{\alpha_1+1}}{n^{1/2-\gamma}}\r\geqslant1-2\exp(- C'm)&\\
- D_1m^2n\exp\l -(D_0+1)n^{\frac{2\gamma}{5}} \r&,
\end{align*}
when 
\begin{align*}
n\geqslant\max\Bigg\{ n_1,\l\tfrac{D_0+1}{D_2}\r^{\frac{5}{1-2\gamma}},\left[\tfrac{\|\Gamma^{-1}M\|  \wt C}{48}\l\tfrac{D_2}{D_0+1}\r^{\frac52}\right]^{\frac{2}{2(c_1(\alpha_1+1)+\gamma)-1}}&,\\
\l \tfrac{\|\Gamma^{-1}M\| }{2D_3}\r^{\frac{2}{c_1(2\alpha_1+1)-1}},n_2',\left[ \tfrac{D_3\wt C}{24}\l \tfrac{D_2}{D_0+1} \r^{\frac52} \right]^{\frac2{2\gamma+c_1}}&\Bigg\}
\end{align*}
where $n_2'$ is defined in Lemma $\ref{lem:lower bound sigma min total}$.
\end{proposition}
\begin{proof}
By Lemma $\ref{lemma, way of estimate truncate central subspace}$, $\eqref{def: estimator central subspace}$ and Lemma $\ref{lemma, sin theta of nonadjoint operator}$, one has
\begin{align}
&\left\|P_{\mc S_{\Y|\vX}^{(m)}}-P_{\wh{\mc{S}}_{\Y|\vX}^{(m)}}\right\| =\left\|P_{\Gamma_m^{\dagger}M_m}-P_{\wh\Gamma_m^{\dagger}\wh M_m^d}\right\| \nonumber\\
&\qquad\leqslant\frac{\pi}{2}\frac{\lno\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\rno ^2+\lno\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\rno \lno\Gamma_m^\dagger M_m\rno }{\min\lb\sigma_{\min}^+\l\wh\Gamma_m^\dagger \wh M_m^d\r^2,\sigma_{\min}^+\l\Gamma_m^\dagger M_m\r^2\rb}\label{eq: PS minus P hat S norm}.
% &\leqslant C_5\|\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\|\\
% &=\widetilde O_{\mathbb{P}}\l\frac{m^{\alpha_1+1}}{n^{1/2}}\r,
\end{align}
% with probability at least $1-\exp(- C)-2\exp(- C'm)$.
Because of $c_1(2\alpha_1+1)-1<0$ and $2(c_1(\alpha_1+1)+\gamma)-1<0$, it is easy to check that when
\[n\geqslant\max\lb\left[\tfrac{\|\Gamma^{-1}M\|  \wt C}{48}\l\tfrac{D_2}{D_0+1}\r^{\frac52}\right]^{\frac{2}{2(c_1(\alpha_1+1)+\gamma)-1}},\l \tfrac{\|\Gamma^{-1}M\| }{2D_3}\r^{\frac{2}{c_1(2\alpha_1+1)-1}}\rb,\]
both $\l\tfrac{D_0+1}{D_2}\r^{\frac52}\tfrac{24}{\wt C}n^{c_1(\alpha_1+1)+\gamma-\frac{1}{2}}$ and $D_3n^{\frac{c_1(2\alpha_1+1)-1}{2}}$ are less than or equal to $\frac{\|\Gamma^{-1}M\| }{2}$. Thus, on the event $\ttE$,
\begin{align}\label{eq: high prob upper bound is Gamma minus 1 M}
\lno\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\rno \leqslant \lno\Gamma^{-1}M\rno .
\end{align}
By Lemma $\ref{lem:Gammam dagger Mm uniformly bounded}$, inserting \eqref{eq: high prob upper bound is Gamma minus 1 M} into \eqref{eq: PS minus P hat S norm} leads to
$$
\lno P_{\mc{S}_{\Y|\X}^{(m)}}-P_{ \widehat{\mc{S}}_{\Y|\X}^{(m)}}\rno
\leqslant \frac{\pi\lno\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\rno \lno\Gamma^{-1}M\rno }{\min\lb\sigma_{\min}^+\l\wh\Gamma_m^\dagger \wh M_m^d\r^2,\sigma_{\min}^+\l\Gamma_m^\dagger M_m\r^2\rb},
$$
on the event $\ttE$.
Furthermore, when $n\geqslant \left[ \frac{D_3\wt C}{24}\l \frac{D_2}{D_0+1} \r^{\frac52} \right]^{\frac2{2\gamma+c_1}}$ and $n\geq n_2'$, one can get
$\l \tfrac{D_0+1}{D_2}\r^{\frac52}\tfrac{24m^{\alpha_1+1}}{\wt C n^{1/2-\gamma}}$ is greater than or equal to $D_3\tfrac{m^{(2\alpha_1+1)/2}}{n^{1/2}}$
and then on the event $\ttE$,
\begin{align*}
\lno P_{\mc{S}_{\Y|\X}^{(m)}}-P_{ \widehat{\mc{S}}_{\Y|\X}^{(m)}}\rno \leqslant \tfrac{96\pi\|\Gamma^{-1}M\| }{\sigma_d(\Gamma^{-1}M)^2}\l \tfrac{D_0+1}{D_2}\r^{\frac52}\tfrac{m^{\alpha_1+1}}{\wt C n^{1/2-\gamma}}.
\end{align*}
 by
Lemma $\ref{lem:lower bound sigma min total}$.
Then choosing $C_1=\tfrac{96\pi\|\Gamma^{-1}M\| }{\wt C\sigma_d(\Gamma^{-1}M)^2}\l \tfrac{D_0+1}{D_2}\r^{\frac52}$ can complete the proof.
\end{proof}



\paragraph{Proof of Lemma \ref{lem:Gammam dagger Mm uniformly bounded}}
\begin{proof}
First, it is easy to check that:
\begin{align}
\Gamma^\dag_m=\Pi_m\Gamma^{-1}\Pi_m=\Pi_m\Gamma^{-1}=\Gamma^{-1}\Pi_m=\sum\limits_{i=1}^m\lambda_i^{-1}\phi_i\otimes\phi_i.\label{eq: Gamma m dag def}
\end{align}
According to \eqref{eq: Gamma m dag def} and $M_m=\Pi_mM\Pi_m$, it is easy to check that $\Gamma_m^\dagger M_m=\Pi_m \Gamma^{- 1}M\Pi_m$. Then by the compatibility of operator norm, one can get
\begin{align*}
\lno\Gamma_m^\dagger M_m\rno =\lno\Pi_m \Gamma^{-1}M\Pi_m\rno \leqslant \lno\Pi_m\rno  \lno\Gamma^{-1}M\rno \lno\Pi_m\rno =\lno\Gamma^{-1}M\rno .
\end{align*}
Note that $\Gamma^{-1}M$ is bounded since $\Gamma^{-1}M$ is of finite rank by Corollary $\ref{corollary, MDDO and central subspace}$. Thus the proof is completed. 
\end{proof}


\paragraph{Proof of Lemma \ref{lem: Gamma inverse M to Gammam dagger Mm}}
\begin{proof}
It is easy to check that
$\Gamma_m^\dagger M_m=\Pi_m\Gamma^{-1}M\Pi_m$ and $\Gamma^{-1}M$ is of finite rank by Corollary $\ref{corollary, MDDO and central subspace}$.
Thus the proof is completed by Lemma $\ref{lem:PimTPimtoT}$.
\end{proof}
\paragraph{Proof of Lemma \ref{lem:lower bound sigma min total}}
\begin{proof}
We first prove \eqref{eq: lower bound of sigma min}.
By Corollary $\ref{corollary, MDDO and central subspace}$ and Lemma $\ref{lem:projection equality}$, one has $\rank(\Gamma^{- 1}M)=\rank\l\Gamma^{- 1}M(\Gamma^{- 1}M)^*\r=d$. Thus
\begin{align*}
\sigma_{\min}^+(\Gamma^{-1}M)^2=\lambda_{\min}^+\l\Gamma^{-1}M(\Gamma^{-1}M)^*\r=\lambda_d\l \Gamma^{-1}M(\Gamma^{-1}M)^*\r. 
\end{align*}
 It is easy to see $\rank(\Gamma_m^\dagger M_m)=\rank\l \Gamma_m^\dagger M_m(\Gamma_m^\dagger M_m)^*\r\leqslant d$ by $\Gamma_m^\dagger M_m=\Pi_m \Gamma^{-1} M \Pi_m$ and Lemma $\ref{lem:projection equality}$, thus one can assume that 
 \begin{align*}
\sigma_{\min}^+(\Gamma^\dagger_m M_m)^2=\lambda_{\min}^+\l\Gamma_m^\dagger M_m(\Gamma_m^\dagger M_m)^*\r=\lambda_j\l \Gamma_m^\dagger M_m(\Gamma_m^\dagger M_m)^*\r
\end{align*}
for some $j\leqslant d$.
By Corollary $\ref{coro:wely ineq operator}$, $\eqref{eq:sy ineq}$ and
% (Notice that $M_m$ and $M$ are both compact and self-adjoint)
Lemma $\ref{lem: Gamma inverse M to Gammam dagger Mm}$
%and Lemma \ref{lem:Gammam dagger Mm uniformly bounded}
, one has
\begin{align*}
&\left|\sigma_{\min}^+(\Gamma^\dagger_m M_m)^2\hspace{-0.5mm}-\hspace{-0.5mm}\sigma_j(\Gamma^{-1} M)^2\right|\hspace{-0.5mm}=\hspace{-0.5mm}\left|\lambda_{j}\hspace{-1mm}\l\Gamma^\dagger_m M_m(\Gamma^\dagger_m M_m)^{*}\hspace{-0.5mm}\r\hspace{-0.5mm}-\hspace{-0.5mm}\lambda_j\hspace{-1mm}\l \Gamma^{-1} M(\Gamma^{-1} M)^*\hspace{-0.5mm}\r\right|\\
&\qquad\leqslant
\|\Gamma^{-1} M(\Gamma^{-1} M)^*- \Gamma_m^\dagger M_m(\Gamma_m^\dagger M_m)^*\| \\
&\qquad\leqslant \|\Gamma^{-1} M- \Gamma_m^\dagger M_m\| ^2+
\|\Gamma^{-1} M- \Gamma_m^\dagger M_m\| \cdot\|\Gamma^{-1} M\| \xrightarrow{m\to\infty} 0. 
% &\leqslant\|\Gamma^{-1} M- \Gamma_m^\dagger M_m\|\cdot3\|\Gamma^{-1} M\|
\end{align*}
Thus for 
$
n\geqslant m_T(\bigtriangleup)^{\frac1{c_1}}=m_T\l\max\lb\frac{\sigma_d(\Gamma^{-1} M)}{2},\frac{\sigma_d(\Gamma^{-1} M)^2}{4\|\Gamma^{-1}M\| }\rb\r^{\frac1{c_1}}, 
$
one has $\|\Gamma^{-1} M- \Gamma_m^\dagger M_m\| ^2$ and $\|\Gamma^{-1} M- \Gamma_m^\dagger M_m\| \cdot\|\Gamma^{-1} M\| $ are both less than or equal to $\frac{1}{4}\sigma_d(\Gamma^{-1} M)^2$. Hence one can get
$\left|\sigma_{\min}^+(\Gamma^\dagger_m M_m)^2-\sigma_j(\Gamma^{-1} M)^2\right|\leqslant\frac{1}{2}\sigma_d(\Gamma^{-1} M)^2$
% \begin{align*}\label{eq:sigma min Mm}
% \left|\sigma_{\min}^+(\Gamma^\dagger_m M_m)^2-\sigma_j(\Gamma^{-1} M)^2\right|\leqslant\frac{1}{2}\sigma_d(\Gamma^{-1} M)^2
% \|
% \lambda_j\l \Gamma_m^\dagger M_m\l\Gamma_m^\dagger M_m\r^*\r\geqslant \lambda_j\l \Gamma^{-1} M\l\Gamma^{-1} M\r^*\r-\frac{\lambda_d\l \Gamma^{-1} M\l\Gamma^{-1} M\r^*\r}{2}
% \geqslant\frac{\lambda_d\l \Gamma^{-1} M\l\Gamma^{-1} M\r^*\r}{2}. 
% \end{align*}
and
\begin{equation}
\sigma_{\min}^+(\Gamma^\dagger M_m)^2\geqslant \sigma_j(\Gamma^{-1} M)^2-\frac{1}{2}\sigma_d(\Gamma^{-1} M)^2\geqslant\frac{1}{2}\sigma_d(\Gamma^{-1} M)^2
\end{equation}
for sufficiently large $n$. This completes the proof of \eqref{eq: lower bound of sigma min}.





Next we prove $\eqref{eq: lower bound of sigma min hat}$. Combining Proposition $\ref{prop:bound of finite estimate}$ with Lemma $\ref{lem: Gamma inverse M to Gammam dagger Mm}$ leads to that on the event $\ttE$, 
$$
\lno\wh \Gamma_m^\dag\wh M^d_m- \Gamma^{-1}M\rno \leqslant\ve+\l\tfrac{D_0+1}{D_2}\r^{\frac52}\tfrac{24}{\wt C}n^{c_1(\alpha_1+1)+\gamma-\frac{1}{2}}+D_3n^{\frac{c_1(2\alpha_1+1)-1}{2}}
$$
for  $n\geqslant \max\{n_1,m_T( \ve)^{1/c_1}\}$.
Assuming that $c_1(2\alpha_1+1)-1<0$ and $2(c_1(\alpha_1+1)+\gamma)-1<0$, it is easy to check that when $$n\geqslant\max\lb\left[\frac{\bigtriangleup \wt C}{96}\l\frac{D_2}{D_0+1}\r^{\frac52}\right]^{\frac{2}{2(c_1(\alpha_1+1)+\gamma)-1}},\l \frac{\bigtriangleup}{4D_3}\r^{\frac{2}{c_1(2\alpha_1+1)-1}}\rb$$, both $\l\tfrac{D_0+1}{D_2}\r^{\frac52}\tfrac{24}{\wt C}n^{c_1(\alpha_1+1)+\gamma-\frac{1}{2}}$ and $D_3n^{\frac{c_1(2\alpha_1+1)-1}{2}}$ are less than or equal to $\frac{\bigtriangleup}{4}$. Letting $\varepsilon=\frac12\bigtriangleup$, one can get on the event $\ttE$,
when
\begin{align*}
n&\geqslant n_2'=n_2'\hspace{-0.5mm}\l\hspace{-0.5mm}\sigma_d(\Gamma^{-1}M),\|\Gamma^{-1}M\| , \gamma,\sigma_0,\sigma_1,\bs K,m_M(1),c_1,m_T\l \tfrac{\bigtriangleup}{2}\r,\wt C,\alpha_1\hspace{-0.5mm}\r\\
&:=\max\bigg\{ n_1,m_T\l \tfrac{\bigtriangleup}{2}\r^{1/c_1}, \left[\tfrac{\bigtriangleup \wt C}{96}\l\tfrac{D_2}{D_0+1}\r^{\frac52}\right]^{\frac{2}{2(c_1(\alpha_1+1)+\gamma)-1}},\l \tfrac{\bigtriangleup}{4D_3}\r^{\frac{2}{c_1(2\alpha_1+1)-1}}\bigg\},
\end{align*}
one has $\lno\wh \Gamma_m^\dag\wh M^d_m- \Gamma^{-1}M\rno \leqslant\bigtriangleup$ and further
$\sigma_{\min}^+(\wh\Gamma^\dagger \wh M^d_m)^2\hspace{-1mm}\geqslant\hspace{-1mm} \tfrac{\sigma_d(\Gamma^{-1} M)^2}{2}$ by the same argument as the proof of \eqref{eq: lower bound of sigma min}.
 This completes the proof of \eqref{eq: lower bound of sigma min hat}.
Considering that $m_T(\bigtriangleup)\leqslant m_{T}\l\frac\bigtriangleup2\r$, one can also get $\eqref{eq: lower bound of sigma min}$ when $n\geqslant n_2'$. Thus the proof is completed.
\end{proof}
\subsection{Upper bound of error term (ii)}\label{ap, subs, truncation error}
\begin{proposition}\label{proposition, truncation error}
Under Assumption $\ref{assumption: rate-type condition}$, there exists a positive constant $C_2:=C_2\l d,\wt C,\lambda_d(\mc{B}),\alpha_2\r$ where $\mc{B}:=\sum\limits_{i=1}^d {\bs{\beta}}_i\otimes{\bs{\beta}}_i$ for ${\bs{\beta}}_i$ defined in \eqref{def: central subspace}, such that when $n\geqslant \l \frac{\lambda_d({\mc{B}})}{4d\wt C^2}\sqrt{\frac{2\alpha_2-1}{\zeta(2\alpha_2)}}\r^{\frac{2}{c_1(1-2\alpha_2)}}$, we have
\begin{equation}\label{equation, truncation error}
 \left\|P_{\mathcal S_{\Y|\boldsymbol{X}}}-P_{\mathcal S_{\Y|\boldsymbol{X}}^{(m)}}\right\| \leqslant C_2m^{-\frac{2\alpha_2-1}{2}},
\end{equation}
where $\zeta(\cdot)$ is Riemann $\zeta$ function.
% \begin{equation}
% \|P_{\mathcal S_{Y|\boldsymbol{X}}}-P_{\mathcal S_{Y|\boldsymbol{X}}^{(m)}}\|\leqslant O_{\mathbb{P}}(dn^{-(\alpha_2-1)/(2\alpha_1+\alpha_2)}) 
% \end{equation}
\end{proposition}
\begin{proof}
Let ${\mc{B}^{(m)}}:=\sum\limits_{i=1}^d {\bs{\beta}}_i^{(m)}\otimes{\bs{\beta}}_i^{(m)}$ for ${\bs{\beta}}_i^{(m)}$ defined in \eqref{def: truncated central subspace}.
Combing with Equation $\eqref{def: central subspace}$, it is easy to check that $\left\|P_{\mathcal S_{\Y|\boldsymbol{X}}}-P_{\mathcal S_{\Y|\boldsymbol{X}}^{(m)}}\right\| =\|P_{\mc{B}}-P_{\mc{B}^{(m)}}\| $. By Corollary $\ref{cor: sin theta self adjoint}$, we have
\begin{align}\label{eq:sin theta for B Bm}
\|P_{\mc{B}}-P_{\mc{B}^{(m)}}\| \leqslant \frac{\pi}{2}\frac{\|{\mc{B}}-{\mc{B}^{(m)}}\| }{\min\{\lambda_{\min}^+({\mc{B}}),\lambda_{\min}^+({\mc{B}^{(m)}})\}}.
\end{align}

Note that ${\mc{B}}-{\mc{B}^{(m)}}$ is self-adjoint, then
\begin{align*}
&\lno{\mc{B}}-{\mc{B}^{(m)}}\rno =\sup_{{\bs{\beta}}\in\mathbb{S}_{ \mathcal H}}|\langle ({\mc{B}}-{\mc{B}^{(m)}})({\bs{\beta}}),{\bs{\beta}}\rangle|=\sup_{{\bs{\beta}}\in\mathbb{S}_{\mathcal H}}|\langle {\mc{B}}{\bs{\beta}},{\bs{\beta}}\rangle-\langle {\mc{B}^{(m)}}{\bs{\beta}},{\bs{\beta}}\rangle|\\
&~~=\sup_{{\bs{\beta}}\in\mathbb{S}_{\mathcal H}}\hspace{-0.9mm}\left|\sum_{i=1}^d\hspace{-0.9mm}\left[\langle{\bs{\beta}}_i,{\bs{\beta}}\rangle^2-\langle{\bs{\beta}}_i^{(m)},{\bs{\beta}}\rangle^2\right]\right|=\sup_{{\bs{\beta}}\in\mathbb{S}_{\mathcal H}}\hspace{-0.9mm}\left| \sum_{i=1}^d\langle{\bs{\beta}}_i-{\bs{\beta}}_i^{(m)},{\bs{\beta}}\rangle\langle{\bs{\beta}}_i+{\bs{\beta}}_i^{(m)},{\bs{\beta}}\rangle\right|\\
&~~\leqslant\sup_{{\bs{\beta}}\in\mathbb{S}_{\mathcal H}}\sum_{i=1}^d\left| \langle{\bs{\beta}}_i-{\bs{\beta}}_i^{(m)},{\bs{\beta}}\rangle\langle{\bs{\beta}}_i+{\bs{\beta}}_i^{(m)},{\bs{\beta}}\rangle\right|
\leqslant\sum_{i=1}^d\left\|{\bs{\beta}}_i-{\bs{\beta}}_i^{(m)}\right\|\left\|{\bs{\beta}}_i+{\bs{\beta}}_i^{(m)}\right\|,
\end{align*}
where the first inequality comes from the triangle inequality, and the 
second inequality comes from the Cauchy-Schwarz inequality and $\|{\bs{\beta}}\|=1$. 
 Then one has ${\bs{\beta}}_i=\sum\limits_{j=1}^\infty b_{ij}\phi_j$ and 
\[{\bs{\beta}}^{(m)}_i=\Pi_m{\bs{\beta}}_i=\sum_{j'=1}^m\phi_{j'}\otimes\phi_{j'}\sum_{j=1}^\infty b_{ij}\phi_j=\sum_{j'=1}^m\sum_{j=1}^\infty\langle\phi_{j'},\phi_j\rangle b_{ij}\phi_{j'}=\sum_{j=1}^mb_{ij}\phi_j.\]
According to Assumption $\ref{assumption: rate-type condition}$, one can get
\begin{align*}
\left\|{\bs{\beta}}_i-{\bs{\beta}}_i^{(m)}\right\|&=\left\|\sum_{j=m+1}^\infty b_{ij}\phi_j\right\|=\sqrt{\sum_{j=m+1}^\infty b_{ij}^2}\leqslant \wt C\sqrt{\sum_{j=m+1}^\infty j^{-2\alpha_2}};\\
\left\|{\bs{\beta}}_i+{\bs{\beta}}_i^{(m)}\right\|&\leqslant\|{\bs{\beta}}_i\|+\lno{\bs{\beta}}_i^{(m)}\rno\leqslant2\|{\bs{\beta}}_i\|=2\sqrt{\sum_{j=1}^\infty b_{ij}^2}\leqslant 2\wt C\sqrt{\sum_{j=1}^\infty j^{- 2\alpha_2}}.
\end{align*}
Because $\alpha_2>1/2$, one has
\[\sum\limits_{j=m+1}^\infty \frac{1}{j^{2\alpha_2}}\leqslant \frac{1}{2\alpha_2-1}\frac{1}{m^{2\alpha_2-1}};\qquad \sum_{j=1}^\infty \frac 1{j^{2\alpha_2}}=\zeta(2\alpha_2)\text{ is convergent},\]
where $\zeta(\cdot)$ is Riemann $\zeta$ function. Thus, one can get
\begin{equation}\label{eq: upper bound of operator norm of A minus B}
\lno{\mc{B}}-{\mc{B}^{(m)}}\rno \leqslant 2d\wt C^2\sqrt{\frac{\zeta(2\alpha_2)}{2\alpha_2-1}}m^{-\frac{2\alpha_2-1}{2}}.
\end{equation}

Furthermore, 
{since $\mr{rank}(\mc{B})=d$, one can get that $\lambda_{\min}^+(\mc{B})=\lambda_{d}(\mc{B})$. It is easy to see $\rank(\mc{B}^{(m)})\leqslant d$ by $\mc{B}^{(m)}=\Pi_m \mc{B} \Pi_m$, thus one can assume that $\lambda_{\min}^+(\mc{B}^{(m)})=\lambda_j( \mc{B}^{(m)})$ for some $j\leqslant d$.
By Corollary $\ref{coro:wely ineq operator}$
% (Notice that $M_m$ and $M$ are both compact and self-adjoint)
and \eqref{eq: upper bound of operator norm of A minus B}, one has:
$$
|\lambda_j( \mc{B}^{(m)})-\lambda_j\l \mc{B}\r|\leqslant\lno \mc{B}-\mc{B}^{(m)}\rno \leqslant 2d\wt C^2\sqrt{\frac{\zeta(2\alpha_2)}{2\alpha_2-1}}m^{-\frac{2\alpha_2-1}{2}}.
$$
Thus for sufficiently large {$n\geqslant \l \frac{\lambda_d({\mc{B}})}{4d\wt C^2}\sqrt{\frac{2\alpha_2-1}{\zeta(2\alpha_2)}} \r^{\frac{2}{c_1(1-2\alpha_2)}}$}, one has
\begin{align}
&\lambda_j\l \mc{B}^{(m)}\r\geqslant \lambda_j\l \mc{B}\r-\frac{\lambda_d\l \mc{B}\r}{2}
\geqslant\frac{\lambda_d\l \mc{B}\r}{2}\nonumber\\
&\qquad\Longrightarrow \min\{\lambda_{\min}^+({\mc{B}}),\lambda_{\min}^+({\mc{B}^{(m)}})\}\geqslant \frac{\lambda_d({\mc{B}})}{2}. \label{eq:lower bound lambda min plus B Bm}
\end{align}}
Inserting \eqref{eq: upper bound of operator norm of A minus B} and \eqref{eq:lower bound lambda min plus B Bm} into \eqref{eq:sin theta for B Bm} leads to
\begin{align*}
\left\|P_{\mathcal S_{\Y|\boldsymbol{X}}}-P_{\mathcal S_{\Y|\boldsymbol{X}}^{(m)}}\right\| \leqslant \frac{2\pi d\wt C^2}{\lambda_{d}(\mc{B})}\sqrt{\frac{\zeta(2\alpha_2)}{2\alpha_2-1}}m^{-\frac{2\alpha_2-1}{2}}.
\end{align*}
Then choosing $C_2:=\frac{2\pi d\wt C^2}{\lambda_d({\mc{B}})}\sqrt{\frac{\zeta(2\alpha_2)}{2\alpha_2-1}}$ can complete the proof.
\end{proof}



\subsection{Proof of Theorem \ref{theorem, total convergence rate}}
\begin{proof}
Note that
\begin{equation}
\begin{aligned}
\left\|P_{\mc{S}_{\Y|\X}}-P_{\widehat{\mc{S}}_{\Y|\X}^{(m)}}\right\| 
&\leqslant \left\|P_{\mc{S}_{\Y|\X}}-P_{\mc{S}_{\Y|\X}^{(m)}}\right\| +\left\|P_{\mc{S}_{\Y|\X}^{(m)}}-P_{ \widehat{\mc{S}}_{\Y|\X}^{(m)}}\right\| .\\
\end{aligned}
\end{equation}
Next we select $m$ to be $n^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}$, i.e.,  $c_1:=\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}$. And it is easy to check that $c_1$ satisfies $2c_1(\alpha_1+1)+2\gamma-1=-\frac{(1-2\gamma)(2\alpha_2-1)}{2\alpha_1+2\alpha_2+1}<0$ and $c_1(2\alpha_1+1)-1=-\frac{2[\gamma(2\alpha_1+1)+\alpha_2]}{2\alpha_1+2\alpha_2+1}<0$.
Then combining Proposition $\ref{proposition, estimation error}$ with Proposition $\ref{proposition, truncation error}$ leads to
\begin{align*}
\P\left[\left\|P_{\mc S_{\Y|\X}}-P_{\widehat{\mc{S}}_{\Y|\X}^{(m)}}\right\| \leqslant\hspace{-0.5mm} (C_1+C_2)n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2(2\alpha_1+2\alpha_2+1)}}\right]\hspace{-1mm}\geqslant\hspace{-1mm} 1-2\exp\hspace{-0.5mm}\l\hspace{-1mm}- C'n^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}\r&\\
-\exp\left[\ln\l D_1n^{\frac{2\alpha_1+2\alpha_2+3-4\gamma}{2\alpha_1+2\alpha_2+1}} \r-(D_0+1)n^{\frac{2\gamma}{5}}\right]&
\end{align*}
when $n\geqslant n_3'$, where
\begin{align*}
n_3'=\max\Bigg\{n_1,n_2',\left[\tfrac{\|\Gamma^{-1}M\|  \wt C}{48}\l\tfrac{D_2}{D_0+1}\r^{\frac52}\right]^{\frac{2}{2(c_1(\alpha_1+1)+\gamma)-1}}\hspace{-0.9mm},\l \tfrac{\|\Gamma^{-1}M\| }{2D_3}\r^{\frac{2}{c_1(2\alpha_1+1)-1}}\hspace{-0.9mm},\\
\l\tfrac{D_0+1}{D_2}\r^{\frac{5}{1-2\gamma}},\left[ \tfrac{D_3\wt C}{24}\l \tfrac{D_2}{D_0+1} \r^{\frac52} \right]^{\frac2{2\gamma+c_1}},\l\tfrac{\lambda_d(\mc{B})}{4d\wt C^2}\sqrt{\tfrac{{2\alpha_2-1}}{\zeta(2\alpha_2)}} \r^{\frac{2}{c_1(1-2\alpha_2)}}\Bigg\}
\end{align*}

It is easy to check that as long as $\frac{2\gamma}{5}<\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}\Longrightarrow\gamma<\frac{5}{4(\alpha_1+\alpha_2+3)}$, 
there exists a constant $n_3''=n_3''\l \gamma,\alpha_1,\alpha_2,D_0,D_1,C'\r$ such that when $n\geqslant n_3'$ further, we have 
\begin{align*}
\P\l\left\|P_{\mc S_{\Y|\X}}-P_{\widehat{\mc{S}}_{\Y|\X}^{(m)}}\right\| \leqslant (C_1+C_2)n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2(2\alpha_1+2\alpha_2+1)}} \r
\geqslant1-2\exp\l-\tfrac{D_0+1}{2}n^{\frac{2\gamma}{5}} \r.
\end{align*}
Thus one can choose $n_3=\max\{n_3',n_3''\}$ to get the following conclusion.
\begin{proposition}
Under Assumptions $\ref{as:joint distribution assumption}$ to $\ref{assumption: rate-type condition}$, for any $\gamma\in\l0,\tfrac{5}{4(\alpha_1+\alpha_2+3)}\r$, choosing 
$m=n^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}$ (i.e.,  $c_1=\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}$) yields a positive constant
\begin{align*}
D_4:=D_4\l \|\Gamma^{-1}M\| ,\sigma_d(\Gamma^{-1}M) ,\gamma,\sigma_0,\sigma_1,d,\wt C,\lambda_d\l\sum\limits_{i=1}^d {\bs{\beta}}_i\otimes{\bs{\beta}}_i\r,\alpha_2\r 
\end{align*}
such that when $n$ is sufficiently large, we have:
\begin{align*}
\P\l\left\|P_{\mc{S}_{\Y|\X}}-P_{\widehat{\mc{S}}_{\Y|\X}^{(m)}}\right\| \leqslant D_4n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2(2\alpha_1+2\alpha_2+1)}} \r
\geqslant1-2\exp\l -\tfrac{D_0+1}{2}n^{\frac{2\gamma}{5}} \r,
\end{align*}
where $D_0$ and $D_1$ are defined in Proposition $\ref{prop:bound hatMmd Mm}$.
\end{proposition}
\noindent
% Theorem $\ref{theorem, total convergence rate}$ is a direct corollary of above proposition.
Define 
$$\mathtt F:=\left\{\left\|P_{\mc{S}_{\Y|\X}}-P_{\widehat{\mc{S}}_{\Y|\X}^{(m)}}\right\| \leqslant D_4n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2(2\alpha_1+2\alpha_2+1)}}\right\}.$$
Then 
\begin{align*}
 \mb E\left[\left\|P_{\mc{S}_{\Y|\X}}-P_{\widehat{ \mc{S}}_{\Y|\X}^{(m)}}\right\|^2\right] =&
  \mb E\left[\left\|P_{\mc{S}_{\Y|\X}}-P_{\widehat{ \mc{S}}_{\Y|\X}^{(m)}}\right\|^21_{\mathtt{F}}\right] +
   \mb E\left[\left\|P_{\mc{S}_{\Y|\X}}-P_{\widehat{ \mc{S}}_{\Y|\X}^{(m)}}\right\|^21_{\mathtt{F}^c}\right]\\ 
 \leqslant &
 D_4^2n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2\alpha_1+2\alpha_2+1}}+4\mb P\left( \mathtt F^c\right)\\
 \lesssim&n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2\alpha_1+2\alpha_2+1}}+\exp\l -\tfrac{D_0+1}{2}n^{\frac{2\gamma}{5}} \r\\
\lesssim&n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2\alpha_1+2\alpha_2+1}}.
\end{align*}
This completes the proof of  Theorem \ref{theorem, total convergence rate}.
\end{proof}







\section{Additional Simulation Results of Section \ref{sec:Synthetic}}
This section contains the additional  simulation results  of Sections \ref{sec:Synthetic}  when $\varepsilon\sim N(0,1)$.



We show the average $\mc D(\bs B;\bs{\wh B})$ with different $m$ or $\rho$ for three methods under $\mc M_1$ to $\mc M_3$ in Figure \ref{fig:error 3models,noise1},
where we mark minimal error in each model with red `$\times$'. The shaded areas represent the standard error associated with these estimates and all of them are less than  $0.01$. For FSFIR, the  minimal errors for $\mc M_1-\mc M_3$ are  $0.08,0.02,0.01$ respectively.
For TFSIR, the  minimal errors are  $0.08,0.02,0.01$ and for regularized FSIR,  the  minimal errors are $0.13,0.06,0.01$.  

% Figure environment removed


Figure \ref{fig:error 3models,noise1} shows that FSFIR attains the best performance among  all models. 
Moreover, FSFIR is easier to practice as it does not need a slice number $H$ in advance. 




