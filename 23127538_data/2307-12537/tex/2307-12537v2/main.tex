\documentclass[12pt]{article} %***
\usepackage[sectionbib]{natbib}
\usepackage{array,epsfig,fancyheadings,rotating}
\usepackage{algorithm} 
%\usepackage{algorithmic} 
\usepackage{algorithmicx,setspace}
\usepackage{algpseudocode} 
\usepackage[]{hyperref} %<----modified by Ivan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{sectsty, secdot}
%\sectionfont{\fontsize{12}{15}\selectfont}
\sectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
\renewcommand{\theequation}{\thesection\arabic{equation}}
\subsectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{ulem}
\textwidth=31.9pc
\textheight=46.5pc
\oddsidemargin=1pc
\evensidemargin=1pc
\headsep=15pt
%\headheight=.2cm
\topmargin=.6cm
\parindent=1.7pc
\parskip=0pt
\def\P{\mathbb P}
\def\Im{\mathrm{Im}}
\def\R{\mathbb R}
\def\u{\boldsymbol{u}}
\def\E{\mathbb E}
\def\vX{\boldsymbol{X}}
\def\mnull{\mathrm{null}}
\def\X{\boldsymbol X}
\def\s{\boldsymbol{s}}
\def\l{\left(}
\def\r{\right)}
\def\lmi{\left[}
\def\rmi{\right]}
\def\li{\left\langle}
\def\ri{\right\rangle}
\def\lno{\left\|}
\def\rno{\right\|}
\def\bbeta{\boldsymbol{\beta}}
\def\lb{\left\{}
\def\rb{\right\}}
\def\var{\mathrm{var}}
\def\M{\boldsymbol M}
\def\x{\boldsymbol x}
\def\Z{\mathbb{Z}}
\def\m{\boldsymbol m}
\def\H{\mathcal H}
\def\S{\mathcal{S}_{Y|\bs{X}}}
\def\Sm{\mathcal{S}_{Y|\bs{X}}^{(m)}}
\def\hSm{\wh{\mathcal{S}}_{Y|\bs{X}}^{(m)}}
\def\rank{\mathrm{rank}}
\def\mspan{\mathrm{span}}
\def\mi{\mathrm{i}}
\newcommand{\mb}{\mathbb}
\newcommand{\mc}{\mathcal}
\newcommand{\bs}{\boldsymbol}
\newcommand{\wh}{\widehat}
\newcommand{\mr}{\mathrm}
\newcommand{\ol}{\overline}
\newcommand{\ti}{\tilde}
\newcommand{\wt}{\widetilde}
\newcommand{\ve}{\varepsilon}
\newcommand{\idi}{\boldsymbol{\dot\iota}}
\newcommand{\ttE}{\mathtt{E}}
\newcommand{\Y}{\bs{Y}}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath,color}
\usepackage{algorithmicx,algorithm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{amsthm}
\numberwithin{equation}{section}
\setcounter{page}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\usepackage{hyperref}
\usepackage{cleveref}
\newcommand{\myvec}[1]{\stackrel{\raisebox{-2pt}[0pt][0pt]{\small$\rightharpoonup$}}{#1}}
\pagestyle{fancy}

\usepackage{mleftright}
\newcommand{\poly}{\mr{poly}}
\newcommand{\NTK}{K^{\mr{NT}}}

\newcommand{\widesim}{\mathrel{\scalebox{2.2}[1]{\hbox{$\sim$}}}}
\newcommand{\iid}{\mathrel{\stackrel{\text{\raisebox{-0.5ex}{i.i.d.}}}{\widesim}}}

% 定义一些括号
\newcommand{\pt}[1]{\left(#1\right)}
\newcommand{\bk}[1]{\left[#1\right]}
\newcommand{\cl}[1]{\left\{#1\right\}}
\newcommand{\ang}[1]{\left\langle#1\right\rangle}
\newcommand{\fl}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ce}[1]{\left\lceil#1\right\rceil}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}

\newcommand{\mpt}[1]{\mleft(#1\mright)}
\newcommand{\mbk}[1]{\mleft[#1\mright]}
\newcommand{\mcl}[1]{\mleft\{#1\mright\}}


\pagestyle{fancy}
\def\n{\noindent}
\lhead[\fancyplain{} \leftmark]{}
\chead[]{}
\rhead[]{\fancyplain{}\rightmark}
\cfoot{\thepage}
\usepackage{geometry}
\usepackage{authblk}
\geometry{left=2.54cm,right=2.54cm,top=2.54cm,bottom=2.54cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Functional Slicing-free Inverse Regression via Martingale Difference Divergence Operator}
% \begin{aug}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%Only one address is permitted per author. %%
% %%Only division, organization and e-mail is %%
% %%included in the address.                  %%
% %%Additional information can be included in %%
% %%the Acknowledgments section if necessary. %%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % \author[1]{ }
% % \affil[1]{Department of Statistics \& Applied Probability, National University of Singapore}

% % \author[2]{ }
% % \author[3]{Lin Qian}
% % \affil[2,3]{Center for Statistical Sciences, Tsinghua University}
% \author[a]{Rui Chen}\thanks{Center for Statistical Science, Department of Industrial Engineering, Tsinghua University, $<$chenrui@mail.tsinghua.edu.cn$>$}
% \author[b]{Songtao Tian}\thanks{Co-first author. Department of Mathematical Sciences, Tsinghua University, $<$tst20@mails.tsinghua.edu.cn$>$}
% \author[c]{Qian Lin}\thanks{Center for Statistical Science, Department of Industrial Engineering, Tsinghua University,$<$qianlin@tsinghua.edu.cn$>$}
% \author[d]{Jun S. Liu}\thanks{Corresponding author. Department of Statistics, Harvard University, $<$jliu@stat.harvard.edu$>$}


% \author[a]{Songtao Tian}
% \author[b]{Zixiong Yu\thanks{Co-first author.}}
% \author[c]{Rui Chen}
% \author[d]{Qian Lin\thanks{Corresponding author.}}
% \affil[a]{Department of Mathematical Sciences, Tsinghua University, $<$tst20@mails.tsinghua.edu.cn$>$}
% \affil[b]{Department of Mathematical Sciences, Tsinghua University, $<$
% yuzx19@mails.tsinghua.edu.cn$>$}
% \affil[c]{Center for Statistical Science, Department of Industrial Engineering, Tsinghua University, $<$chenrui@mail.tsinghua.edu.cn$>$}

% \affil[d]{Center for Statistical Science, Department of Industrial Engineering, Tsinghua University;~Beijing Academy of Artificial Intelligence, Beijing, 100084, China,$<$qianlin@tsinghua.edu.cn$>$}
\author[a]{Songtao Tian}
\author[a]{Zixiong Yu\thanks{Co-first author.}}
\author[b]{Rui Chen\thanks{Corresponding author.}}
\affil[a]{Department of Mathematical Sciences, Tsinghua University}
% \affil[b]{Department of Mathematical Sciences, Tsinghua University}
\affil[b]{Center for Statistical Science, Department of Industrial Engineering, Tsinghua University}

\renewcommand*{\Affilfont}{\small\it} % 修改机构名称的字体与大小
\renewcommand\Authands{ and } % 去掉 and 前的逗号
\date{} % 去掉日期
\begin{document}
\maketitle
\renewcommand{\baselinestretch}{2}
\begin{quotation}
\begin{spacing}{2}
\noindent
\small Abstract:~
Functional sliced inverse regression (FSIR) is one of the most popular algorithms for functional sufficient dimension reduction (FSDR). However, the choice of slice scheme in FSIR is critical but challenging.
In this paper, we propose a new method called functional slicing-free inverse regression (FSFIR) to estimate the central subspace in FSDR. FSFIR is based on the martingale difference divergence operator, which is a novel metric introduced to characterize the conditional mean independence of a functional predictor on a multivariate response. We also provide a specific convergence rate for the FSFIR estimator.
Compared with existing functional sliced inverse regression methods, FSFIR does not require the selection of a slice number. Simulations demonstrate the efficiency and convenience of FSFIR.
\end{spacing}
\vspace{9pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \pagestyle{fancy}
% \def\n{\noindent}
% \lhead[\fancyplain{} \leftmark]{}
% \chead[]{}
% \rhead[]{\fancyplain{}\rightmark}
% \cfoot{\thepage}

% % -------------------------允许算法跨页-------------
% \makeatletter
% \newenvironment{breakablealgorithm}
%  {% \begin{breakablealgorithm}
%  \begin{center}
%  \refstepcounter{algorithm}% New algorithm
%  \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
%  \renewcommand{\caption}[2][\relax]{% Make a new \caption
%  {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
%  \ifx\relax##1\relax % #1 is \relax
%  \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
%  \else % #1 is not \relax
%  \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
%  \fi
%  \kern2pt\hrule\kern2pt
%  }
%  }{% \end{breakablealgorithm}
%  \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
%  \end{center}
%  }
% \makeatother

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{document}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \renewcommand{\baselinestretch}{2}

% \markright{ \hbox{\footnotesize\rm Statistica Sinica
% %{\footnotesize\bf 24} (201?), 000-000
% }\hfill\\[-13pt]
% \hbox{\footnotesize\rm
% %\href{http://dx.doi.org/10.5705/ss.20??.???}{doi:http://dx.doi.org/10.5705/ss.20??.???}
% }\hfill }

% \markboth{\hfill{\footnotesize\rm FIRSTNAME1 LASTNAME1 AND FIRSTNAME2 LASTNAME2} \hfill}
% {\hfill {\footnotesize\rm FILL IN A SHORT RUNNING TITLE} \hfill}

% \renewcommand{\thefootnote}{}
% $\ $\par

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \fontsize{12}{14pt plus.8pt minus .6pt}\selectfont \vspace{0.8pc}
% % \centerline{\large\bf Martingale difference divergence operator and its application to }
% \centerline{\large\bf Functional Slicing-free Inverse Regression via}
% \vspace{2pt} 

% \centerline{\large\bf Martingale Difference Divergence Operator}
% \vspace{.4cm} 
% \centerline{Songtao Tian, Zixiong Yu, Rui Chen, Qian Lin} 
% \vspace{.4cm} 
% \centerline{\it Tsinghua University}
%  \vspace{.55cm} \fontsize{9}{11.5pt plus.8pt minus.6pt}\selectfont

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{quotation}
% \noindent {\it Abstract:}
% In this paper, we propose a method which we call functional slicing-free inverse regression (FSFIR) to estimate the central subspace in functional
% sufficient dimension reduction. FSFIR is based on the martingale difference divergence operator (MDDO)which is a new metric that we introduce to
% characterize the conditional mean independence of a functional predictor on a scalar response.
% We give a specific convergence rate of FSFIR estimator. Compared with existing functional sliced inverse regression (FSIR) methods, FSFIR does not involve any slice number. Simulations demonstrate the efficiency of FSFIR. Our FSFIR can be regarded as a generalization of slicing-free inverse regression studied by Mai-Shao-Wang-Zhang (2021) in the high-dimensional data.

% \vspace{9pt}
\noindent {\it Key words and phrases:}
Functional sliced inverse regression, Functional slicing-free inverse regression, Martingale difference divergence, Sufficient dimension reduction. 
\par
\end{quotation}\par
	
	
	
	\def\thefigure{\arabic{figure}}
	\def\thetable{\arabic{table}}
	
	\renewcommand{\theequation}{\thesection.\arabic{equation}}
	
	
	\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont
	
% \tableofcontents
\newpage
\section{Introduction}




Classical statistical methods often failed in the high-dimensional data where the number of features $p$ is comparable to or even larger than the number of observed samples $n$. Sufficient dimension reduction (SDR)
is often the first step in dealing with high-dimensional problems. SDR aims to finding the minimal low-rank projection, 
of a predictor $\X\in\R^p$, which contains all the information of a response $Y\in\R$ without estimating the unknown link function. 
Indeed, SDR gives the intersection of all spaces $\mc S\subset \R^p$ satisfying $Y\perp \!\!\! \perp \bs X|P_{\mathcal S} \bs X$ where $\perp \!\!\! \perp$ denotes independence and
$P_{\mc S}$ denotes the projection onto $\mc S$. The said intersection is known as the \textit{central subspace} and denoted by $\mc S_{Y|\X}$. 

There are various well-known SDR methods: \textit{sliced inverse regression} (SIR, \citealt{li1991sliced}), 
 \textit{sliced average variance estimation} (SAVE, \citealt{cook1991sliced}), \textit{principal hessian directions} (PHD, \citealt{li1992principal}), \textit{directional regression} (DR, \cite{li2007directional}), \textit{minimum average variance estimation} (MAVE, \cite{xia2009adaptive}) and many others.
Among these methods that deal with the SDR problems,
SIR is particularly popular  due to its simplicity and efficiency. 
Under linearity and coverage conditions (See Assumption \ref{as:Linearity condition and Coverage condition}), SIR relates the central subspace $\S$ with the eigen-space of the covariance of conditional mean, i.e.,   $\var\mpt{\E[\X|Y]}$. Then SIR estimates $\var\mpt{\E[\X|Y]}$ by dividing the samples into several equally-sized slices according to the order statistics of response and averaging the sample covariance within each slice. 

However, the consistency and convergence rate of SIR, together with other slice-based SDR methods, involve the choice of a suitable slice number (denoted by $H$). 
\cite{hsing1992asymptotic}
 proved that SIR gives a root $n$ consistent estimation when each slice contains two observations (i.e.,  $H=n/2$), whereas \cite{zhu1995asymptotics}
argued that a smaller number of observations in each slice could yield a bigger covariance matrix of asymptotically multinormal distribution of estimators. In addition, by considering the case where the number of samples in each slice, denoted by $c$, goes to infinity with increasing sample size $n$,
 \cite{zhu1995asymptotics} showed that the optimal $c$ satisfies $c=c_0n^\alpha$ where $c_0>0$ is a constant and $\alpha\in\pt{0,1/2}$ depends on the distribution of the predictor $\X$ and the central curve $m(y):=\E[\bs X|Y=y]$.  Nevertheless, 
determining the constants $c_0$ and $\alpha$ in practice is usually challenging.


Notably, in the multivariate-valued predictor case, several approaches have been proposed to address the issue of selecting a suitable slice number $H$. For instance, 
\cite{zhu2010dimension} suggested
 \textit{cumulative slicing estimation} by considering all estimations with two slices for the central subspace $\S$. They proposed \textit{cumulative mean estimation}, \textit{cumulative variance estimation} and \textit{cumulative directional regression } parallel to SIR, SAVE and DR respectively.
\cite{cook2014fused} also proposed a \textit{fusing} method to relieve the burden of choosing a suitable $H$.
However, this method is not entirely slicing-free because it still requires a predefined collection of quantile slicing schemes. In order to provide an adaptive slicing scheme, \cite{wang2019dimension} implemented a regularization criteria by transforming the eigen-decomposition problem into a trace-optimization problem.
\cite{mai2021slicing} proposed a \textit{slicing-free inverse regression} method
 with the help of the \textit{martingale difference divergence matrix} (MDDM) which measures the conditional mean independence of a high-dimensional predictor on a multivariate response. 

It is an important trend to study  functional data analysis in SDR and many significant achievements have been made in this area. For example, 
\cite{ferre2003functional} first applied SIR to functional-valued data where $\bs X$ is in $\mathcal L_2([0,1])$ (the separable Hilbert space of square-integrable curves on $[0,1]$) and the response $Y$ is in $\mb R$. Based on this work, various developments in FSIR have been made (e.g., \cite{Forzanicook2007note,lian2014Sefsdr,lian2015functional,wang2020functional,chen2023optimality}). 


 Again, the choice of the slicing number $H$  in FSIR remains an issue. In addition, although the central space can be defined for both univariate and multivariate responses, most existing SDR methods, such as FSIR , primarily focus on the case of univariate response. Extending these methods to handle multivariate response is a non-trivial task, and this limitation is evident in FSIR.


Therefore, it is natural to seek for a \textit{functional slicing-free} method for  multivariate response to avoid the difficulty in choosing a suitable $H$.
 In this paper, we propose a new method we call functional slicing-free inverse regression (FSFIR) to estimate the central subspace $\mc S_{\Y|\bs X}$ for $(\vX,\Y)\in\mathcal L_2([0,1])\times\R^q$ without specifying any $H$.  
 
\subsection{Major contributions}
The FSFIR method is our solution to the aforementioned goals. Around this core innovation, the main clues and results of our article are explained as follows.

 First, we introduce a new metric: martingale difference divergence operator
 (MDDO), which generalizes MDDM in \cite{lee2018martingale}. It turns out that MDDO enjoys a lot of properties similar to MDDM: 
 \begin{itemize}
 \item[$\mathrm{(i)}$]$\mathrm{MDDO}(\boldsymbol{X}|\Y)=0\Longleftrightarrow \mathbb E[\boldsymbol{X}|\Y]=0$\quad a.s.;
\item[$\mathrm{(ii)}$] $\mathrm{MDDO}(T^*{\boldsymbol{X}}|\Y)=T^*\mathrm{MDDO}({\boldsymbol{X}}|\Y)T,\quad\forall T:\mc H\to\mc H $.
 \end{itemize}
 From (i) we see that MDDO quantifies the conditional mean independence of a functional predictor $\bs X$ on a multivariate response $\Y$.
 It further implies that under the linearity and coverage conditions, the central subspace $\mc S_{\Y|\vX}$ and the image of MDDO are closely related:
 \begin{equation}\label{Eqt:GammaSMDDO}
 \Gamma \mc S_{\Y|\vX}=\mathrm{Im}\{\mathrm{MDDO}(\boldsymbol{X}|\Y)\}.
 \end{equation}
 
 Second, we propose the FSFIR by estimating $\mathrm{MDDO}(\boldsymbol{X}|\Y)$ without specifying any $H$.  Our method is based on the above \eqref{Eqt:GammaSMDDO} and inspired by  \cite{mai2021slicing}.
 To tackle the issue that $\Gamma^{-1}$ may be unbounded, we adopt a truncation on the predictor $\X$.

 
 
 Finally, we derive a specific convergence rate of FSFIR for estimating the central subspace $\mc S_{\Y|\vX}$. To compare FSFIR with classical FSIR methods including truncated FSIR \citep{ferre2003functional,chen2023optimality} and regularized FSIR \citep{lian2015functional}, we conduct simulations contains the 
subspace estimation error performance of FSFIR on both synthetic data  and real data. The results demonstrate the good performance and convenience of our FSFIR compared with   FSIR methods. 



The rest of this paper is organized as follows. In Section $\ref{subsection, review}$, we review MDDM briefly. Detailed definition and properties of MDDO are in Section $\ref{subsection, MDDO}$. 
 Section $\ref{section, Slicing-free IR via MDDO}$ establishes 
the medium to estimate the central subspace $\mc S_{\Y|\vX}$ in terms of MDDO, i.e.,  Equation \eqref{Eqt:GammaSMDDO}. In Section $\ref{subsection, estimation method}$, we propose the FSFIR for estimating the central subspace $\mc S_{\Y|\vX}$ and then design a detailed algorithm for FSFIR. The specific convergence rate of FSFIR is given in Section $\ref{subsection, convergence rate}$. 
Section $\ref{section, experiments}$
contains our experiments. We make some concluding remarks in  Section $\ref{section, Discussion}$.

\subsection*{Notations}
Let $\mathcal H:=\mathcal L_2([0,1])$ be the separable Hilbert space of square-integrable curves on $[0,1]$ with the inner product $\ang{f,g}=\displaystyle{\int_{0}^1}f(u)g(u)\,\mathrm{d}u$ and norm $\norm{f}:=\sqrt{\ang{f,f}}$ for $f,g\in\mathcal H$.


Given any
operator $T$ on $\mathcal H$, we use $\mathrm{Im}(T)$ and $\mathrm{null}(T)$
to denote  the closure of  image of $T$, and the null space of $T$ respectively. Besides,  we use $P_T$ to denote the projection operator from $\mc H$ to ${\mathrm{Im}}(T)$,  $T^*$ the adjoint operator of $T$ (a bounded linear operator).
We use $\norm{T}$ to denote the operator norm  with respect to $\ang{\cdot,\cdot}$ of $T$:
\begin{align*}
\norm{T} :={\sup_{\bs{\beta}\in\mathbb{S}_\mathcal{H}}}\|T(\bs{\beta})\|
\end{align*}
where $\mathbb{S}_{\mathcal{H}}=\cl{\bs{\beta}\in\mathcal{H}:\norm{\bs{\beta}}=1}$.
If $T$ is further compact, then we use $\sigma_j(T)$ to denote the $j$-th singular value of $T$. 
 When $T$ is  positive semi-definite and compact, 
  $T^\dagger$  denotes the  Moore-Penrose pseudo-inverse of $T$ and 
 $\lambda_j(T)$ denotes the $j$-th eigenvalue of $T$.
Abusing notations, we also denote by $P_S$ the projection operator onto a closed space $S\subseteq \mathcal{H}$.
% We use $\mathrm{spec}(T)$ to denote the set of all eigenvalues of the operator $T$. 
 For any $x,y\in\H$, their tensor product $x\otimes y:\H\to\H$ is defined to be the linear operator: $(x\otimes y)(z)=\ang{x,z}y$ for all $z\in\H$.
 For any random element $\boldsymbol{X}=\bs X_t\in \H$, its mean function is defined as $(\mb E\bs X)_t=\mb E[\bs X_t]$.  
%  the mean $\E\boldsymbol{X}$ is defined as the unique element in $\H$ such that for all $z\in \H$, 
% $\langle \E\boldsymbol{X}, z \rangle=\E\langle \boldsymbol{X}, z \rangle$. 
  For any random operator $T$ on $\H$, the mean $\E[T]$ is defined as the unique operator on $\H$ such that for all $z\in \H$, 
$ (\E[T])(z)=\E[T(z)]$. Specifically,
% The covariance operator of $\boldsymbol{X}$, $\var(\boldsymbol{X})$, is defined as $\var(\boldsymbol{X})(z)=\E\left( \langle \boldsymbol{X}, z \rangle \boldsymbol{X} \right) - \langle\E\boldsymbol{X}, z \rangle \E\boldsymbol{X}$. 
we denote by $\Gamma$ the covariance operator of $\X$, i.e.,  
\begin{align*}
\Gamma:=\var(\bs X)=\E[(\X-\E\X)\otimes (\X-\E\X)] 
\end{align*}
satisfying  $\var(\boldsymbol{X})(z)=\E\mpt{ \ang{\boldsymbol{X}, z} \boldsymbol{X}}- \ang{\E\boldsymbol{X}, z}\E\boldsymbol{X}$.


Throughout the paper, $C_i$ stands for a generic 
constant, $i\geqslant 0$ being an integer. Note that $C_i$ depends on the context. 
For a random sequence $X_n$, we denote by $X_n=O_{\mathbb{P}}(a_n)$ that $\forall\varepsilon>0$, there exists a constant $C_\varepsilon>0$, such that
$\sup_n\mathbb{P}(|X_n|\geqslant C_\varepsilon a_n)\leqslant\varepsilon$.  
For two sequences $a_n$ and $b_n$, we denote $a_n\lesssim  b_n$  if there
exists  a  positive constant $C$ such that $a_n\leqslant Cb_n$. Let $[k]$ denote $\{1,2,\dots,k\}$ for some positive integer $k\geqslant1$.
% We hide the log factors of $n$ in $\widetilde O_{\mathbb{P}}$.
% We denoted by $X_n=o_p(1)$ that $\{X_n\}$ converges in probability to $0$.


\section{MDDO for Functional Data}\label{section, MDDO for functional data}


In this section, we introduce a new metric which we call  martingale difference divergence operator (MDDO) to measure the conditional mean independence
of a functional-valued predictor on  a multivariate response. We are mainly motivated by the work \cite{lee2018martingale} that introduced the notion of  martingale difference divergence matrix (MDDM).
\subsection{Review of the MDDM}\label{subsection, review}
To characterize the conditional mean independence of $\bs V:=(V_1,...,V_p)^\top\in\mb R^p$ on $\bs U=(U_1,...,U_q)^\top\in\mb R^q$, \cite{lee2018martingale}
define the MDDM, which we now recall.
\begin{definition} [\citealt{lee2018martingale}]
\label{def: MDDM}
For $\bs V\in\mb R^p$ and $\bs U\in\mb R^q$, let $$H(\bs s):=(H_1(\bs s),...,H_p(\bs s))^\top\in \mb R^p\qquad~\text{for}~\bs s\in\mb R^q$$ where $H_j(\bs s)$ is defined by $H_j(\bs s)=\mathrm{cov}(V_j,e^{\mi\langle \bs s,\bs U\rangle})$, $\mi=\sqrt{-1}$. Let $H^*(\bs s)$ be the conjugate-transpose of $H(\bs s)$. Then the following matrix is called the Martingale Difference Divergence Matrix (MDDM):
\begin{equation*}
\mathrm{MDDM}(\bs V|\bs U):=\frac{1}{c_q}\int_{\mb R^q}\frac{H(\bs s)H^*(\bs s)}{\|\bs s\|^{1+q}}~\mathrm{d}\bs s,
\end{equation*}
where $c_q$ stands for the constant $\frac{\pi^{(q+1)/2}}{\varGamma\l (q+1)/2\r}$ with $\varGamma(\cdot)$ being the Gamma function. 
\end{definition}
\begin{remark}
The integration on $\mb{R}^q$ is taken in the sense of principal value, i.e.,  $\displaystyle{\int_{\mb{R}^q}}=\displaystyle{\lim\limits_{\varepsilon\to0^+}\int_{D_\varepsilon}}$, where $D_\varepsilon=\{\bs x\in\mb{R}^q:\varepsilon\leqslant\|\bs x\|\leqslant \varepsilon^{-1}\}$.
\end{remark}
Clearly, $\mathrm{MDDM}(\bs V|\bs U)$ is a positive semi-definite matrix. Suppose that $\mb E[\|\bs V\|^2+\|\bs U\|^2]<\infty$, then there is a simpler expression for MDDM:
\begin{equation*}
\mathrm{MDDM}(\bs V|\bs U)=- \mb E[(\bs V-\E\bs V)(\bs V'-\E\bs V')^\top\|\bs U-\bs U'\|],
\end{equation*}
where $(\bs V',\bs U')$ is another independent identical distributed (i.i.d.) copy of $(\bs V,\bs U)$. MDDM enjoys some properties:
\begin{proposition}[\citealt{lee2018martingale}]
\label{proposition, MDDM basic peoperties}~
\begin{itemize}
\item [$\mathrm{(i)}$]
For $\bs V\in\mb R^p$ and $\bs U\in\mb R^q$, $\bs V$ is conditional mean independent on $\bs U$ almost surely (a.s.) if and only if $\mathrm{MDDM}(\bs V|\bs U)$ vanishes, i.e., 
\[\mb E[\bs V|\bs U]=\mb E[\bs V]~\text{a.s.}\Longleftrightarrow \mathrm{MDDM}(\bs V|\bs U)=\bs{O}\]
where $\bs{O}$ stands for the zero matrix;
\item [$\mathrm{(ii)}$] For any $\bs A\in\mb R^{p\times d}$, we have $\mathrm{MDDM}(\bs A^\top\bs V|\bs U)=\bs A^\top\mathrm{MDDM}(\bs V|\bs U)\bs A$.
\end{itemize}
\end{proposition}
MDDM generalizes the \textit{Martingale Difference Divergence} (MDD) and the normalized MDD, namely \textit{Martingale Difference correlation} (MDC). For more about MDD and MDC, see \cite{shao2014martingale}. All these statistics can be used to measure the conditional mean independence of $\bs V$ on $\bs U$. Specifically, we have
\begin{align*}
\mb E[\bs V|\bs U]=\mb E[\bs V]~\text{a.s.}&\Longleftrightarrow\mathrm{MDD}(\bs V|\bs U)=\mathrm{MDC}(\bs V|\bs U)=0\\
&\Longleftrightarrow\mathrm{MDDM}(\bs V|\bs U)=\bs O.
\end{align*}
Meanwhile, we can estimate the MDDM in the following way: assume that we have observed $n$ i.i.d. samples $\{(\bs V_k,\bs U_k)\}_{k=1}^n$ from the same joint distribution as $(\bs V,\bs U)$. Then the finite sample counterpart of MDDM can be defined as 
\begin{equation*}
\mathrm{MDDM}_n(\bs V|\bs U):=- \frac{1}{n^2}\sum_{h,l=1}^n(\bs V_h-\overline{\bs V}_n)(\bs V_l-\overline {\bs V}_n)^\top\|\bs U_h-\bs U_l\|,
\end{equation*}
where $\overline {\bs V}_n:=n^{-1} \sum_{i=1}^n \bs V_i$ is the sample mean of $\{\bs V_i\}_{i=1}^n$.
\cite{mai2021slicing} related the eigenspace (i.e.,  the space spanned by eigenfunctions corresponding to the nonzero eigenvalues) of MDDM with the central subspace in SDR and then proposed a slicing-free inverse regression estimator. Furthermore, they proved a key large deviation inequality
between $\mathrm{MDDM}_n(\bs V|\bs U)$ and $\mathrm{MDDM}(\bs V|\bs U)$ and then established the consistency of this estimator.
\subsection{MDDO}\label{subsection, MDDO}
In this section, we will generalize the MDDM in Definition $\ref{def: MDDM}$ to the MDDO. Roughly speaking, we replace the vector-valued $(\bs V,\bs U)\in\R^{p}\times\R^q$ with a functional-valued $(\bs X,\Y)\in\mc H\times \R^{q}$. Accordingly, we need the tensor product to replace the matrix product. 

Without loss of generality, we assume that $\vX\in\mc H$ satisfies $\mathbb{E}[\boldsymbol{X}]=0$ throughout the paper. As is usually done in functional data analysis \citep{ferre2003functional,lian2014Sefsdr,lian2015functional,chen2023optimality}, we assume  that $\mathbb{E}[\|\boldsymbol{X}\|^4]<\infty$, 
which implies
that $\Gamma$ is a trace class \citep{hsing2015theoretical} and $\boldsymbol{X}$ possesses the following Karhunen--Lo\'{e}ve expansion: 
 there exists a pairwise uncorrelated random sequence $\{\omega_j\}_{j\in\Z_{\geqslant1}}$ with $\mb E[\omega_j]=0$ and $\mathrm{var}(\omega_j)=1$ for all $j$, such that 
\begin{equation}\label{eq:X expansion}
\bs X=\sum_{j=1}^\infty \sqrt{\lambda_j}\omega_j\phi_j,
\end{equation}
where $\{\lambda_j\}_{j\in\Z_{\geqslant1}}$ and $\{\phi_j\}_{j\in\Z_{\geqslant1}}$ are eigenvalues (with descent order) and associated  eigenfunctions of $\Gamma$.
In addition, we assume that $\Gamma$ is non-singular (i.e., $\lambda_i>0, \forall i$) as the literature on functional data analysis usually does. Since $\Gamma$ is compact ($\Gamma$ is a trace class), by spectral decomposition theorem of compact operators, we know that $\{\phi_{i}\}^{\infty}_{i=1}$ forms a complete basis of $\mc H$. 


 


% \begin{equation}\label{eq:X expansion}
% \bs X=\sum_{j=1}^\infty \sqrt{\lambda_j}\omega_j\phi_j,
% \end{equation}
% where $\xi_{i}$'s  are random variables satisfying   $\E[\xi^{2}_{i}]=\lambda_{i}$  and $\E[\xi_{i}\xi_{j}]=0$ for $i\neq j$ and $\{\phi_{i}\}^{\infty}_{i=1}$ are the eigenfunctions of  $\Gamma$ associated with  the decreasing eigenvalues sequence  $\{\lambda_{i}\}^{\infty}_{i=1}$. 
% In addition, we assume that $\Gamma$ is non-singular (i.e., $\lambda_i>0, \forall i$) as the literature on functional data analysis usually does. Since $\Gamma$ is compact ($\Gamma$ is a trace class), by spectral decomposition theorem of compact operators, we know that $\{\phi_{i}\}^{\infty}_{i=1}$ forms a complete basis of $\mc H$. 
% Without loss of generality, we assume that $\mathbb{E}[\boldsymbol{X}]=0$ throughout the paper. As is usually done in functional data analysis \citep{ferre2003functional,lian2014Sefsdr,lian2015functional,chen2023optimality}, we  assume that $\mathbb{E}[\|\boldsymbol{X}\|^4]<\infty$, which implies that $\Gamma$ is a trace class and $\boldsymbol{X}$ possesses the following Karhunen–Lo\'{e}ve expansion:  there exists a pairwise uncorrelated random sequence $\{\omega_j\}_{j\in\Z_{\geqslant1}}$ with $\mb E[\omega_j]=0$ and $\mathrm{var}(\omega_j)=1$ for all $j$, such that 
% \begin{equation}\label{eq:X expansion}
% \bs X=\sum_{j=1}^\infty \sqrt{\lambda_j}\omega_j\phi_j,
% \end{equation}
% where $\{\lambda_j\}_{j\in\Z_{\geqslant1}}$ and $\{\phi_j\}_{j\in\Z_{\geqslant1}}$ are eigenvalues (with descent order) and associated  eigenfunctions of $\Gamma$.




Now, we can state our definition of MDDO. 
% \begin{definition}\label{def: MDDO}
% For $(\bs X,Y)\in\mc H\times \mb R$, we define the Martingale Difference Divergence Operator (MDDO) by 
% \begin{equation*}
% \mathrm{MDDO}(\bs X|Y):=\frac{1}{\pi}\int_{\mb R}\frac{G_{s}\otimes \overline G_{s}}{s^{2}}~\mathrm{d} s,
% \end{equation*}
% where $G_{s}\in\mc H$ is defined as $G_{s}(t)=\mathrm{cov}\hspace{-0.9mm}\left(\bs X(t),e^{\mi sY}\right)$ for any $t\in[0,1]$ and $\overline G_{s}$ is the complex
% conjugate of $G_{s}$.
% \end{definition}

\begin{definition}\label{def: MDDO}
For $\bs X\in\mc H$ and $\Y\in\R^q$, we define the Martingale Difference Divergence Operator (MDDO) by 
\begin{equation*}
\mathrm{MDDO}(\bs X|\Y):=\frac{1}{c_q}\int_{\mb R^q}\frac{G_{\bs s}\otimes \overline G_{\bs s}}{\|\bs s\|^{1+q}}~\mathrm{d}\bs s,
\end{equation*}
where $G_{\bs s}\in\mc H$ for $\bs s\in\R^q$ is defined as $G_{\bs s}(t)=\mathrm{cov}\hspace{-0.9mm}\left(\bs X(t),e^{\mi \langle\bs s,\Y\rangle}\right)$ for any $t\in[0,1]$ and $\overline G_{\bs s}$ is the complex
conjugate of $G_{\bs s}$.
\end{definition}


\begin{remark}
Again, the integration on $\mb{R}^q$ is taken in the sense of principal value.
\end{remark}

Clearly, $\mathrm{MDDO}(\bs X|\Y)$ is a positive semi-definite operator from $\mc H$ to itself. Next we characterize MDDO in terms of expectation, 
which is easier to compute.
% i.e.,  eigenvalues of $\mathrm{MDDO}(\bs X|Y)$ are all real and non-negative. 
In order to achieve this, we need a global
assumption:
\begin{assumption}\label{as:joint distribution assumption}~
 The joint distribution of $(\boldsymbol{X}, \Y)\in\mathcal H\times \mathbb{R}^q$ satisfies the second-order-moment condition, i.e.,  
$\mb E[\|\bs X\|^2+\|\Y\|^2]<\infty$.
\end{assumption}
\begin{lemma}\label{lemma, equivalence of two def of MDDO}Under Assumption $\ref{as:joint distribution assumption}$, we have,
\begin{equation*}
\mathrm{MDDO}(\bs X|\Y)=- \mb E[\bs X\otimes\bs X'\|\Y-\Y'\|],
\end{equation*}
where $(\bs X',\Y')$ is an i.i.d. copy of $(\bs X,\Y)$.
\end{lemma}
Similar  
simple expressions in terms of expectation have been established for MDD in \cite{shao2014martingale}, and MDDM in \cite{lee2018martingale}. 
Next, we show some properties of MDDO which will be used in Sections \ref{section, Slicing-free IR via MDDO} and \ref{section, error}.
\begin{theorem}\label{theorem, MDDO and conditional mean independence}
Under Assumption $\ref{as:joint distribution assumption}$, the following facts hold:
\begin{itemize}
\item[$\mathrm{(i)}$]$\mathrm{MDDO}(\boldsymbol{X}|\Y)=0\Longleftrightarrow \mathbb E[\boldsymbol{X}|\Y]=0$\quad a.s.;
\item[$\mathrm{(ii)}$] $\mathrm{MDDO}(T^*{\boldsymbol{X}}|\Y)=T^*\mathrm{MDDO}({\boldsymbol{X}}|\Y)T,\quad\forall T:\mc H\to\mc H $.
\end{itemize}
\end{theorem}
These properties generalize Proposition $\ref{proposition, MDDM basic peoperties}$.
Property (i) in Theorem \ref{theorem, MDDO and conditional mean independence} means MDDO can characterize the conditional mean independence of a functional-valued predictor on a multivariate response. Based on (i), we relate the central subspace $\mc S_{\Y|\vX}$ with the image of $\mathrm{MDDO}(\bs X|\Y)$, see the next Section $\ref{section, Slicing-free IR via MDDO}$.
\section{FSFIR via MDDO}\label{section, Slicing-free IR via MDDO}
\subsection{Review of  FSIR}\label{subsection, FSIR}


% {\color{blue}Most existing SIR
% literature  considers the following
%  multiple-index model with univariate response:}
% \begin{align}\label{eq:multiple index model}
% Y=f(\langle\bs{\beta}_1,\bs X\rangle,\dots,\langle\bs{\beta}_d,\bs X\rangle,\varepsilon), 
% \end{align}
% where $\bs X\in\mc H,\bs{\beta}_i\in\H~(i\in[d])$, 
% $f:\R^{d+1}\to \R$  is an unknown link function and $\varepsilon$ is a random noise independent of $\X$. Inspired by the advanced work of \cite{li1991sliced}, \cite{ferre2003functional} proposed the FSIR to estimate the central subspace
% \begin{align}\label{def: central subspace}\mc S_{Y|\bs{X}}=\mathrm{span}\{\bs{\beta}_1,...,\bs{\beta}_d\}.
% \end{align}
% Although $\bs{\beta}_i$'s are not individually identifiable  in view of the existence of unknown link function $f$, estimation of  $\{\bs{\beta}_1,...,\bs{\beta}_d\}$ is possible.


Functional SDR aims to  give the intersection of all spaces $\mc S\subset\mc H$ satisfying $\Y\perp \!\!\! \perp \bs X|P_{\mathcal S} \bs X$ for 
$(\bs X,\Y)\in\mc H\times \R^{q}$. The said intersection is known as the   \textit{functional central subspace} and denoted by $\mc S_{\Y|\X}$. To estimate $\mc S_{\Y|\X}$, people often need some mild conditions on $(\X,\Y)$ explained below. 

Assume that $\mc S_{\Y|\X}$ has a basis as follows:
\begin{align}\label{def: central subspace}\mc S_{\Y|\bs{X}}=\mathrm{span}\{\bs{\beta}_1,...,\bs{\beta}_d\}.
\end{align}
\begin{assumption}\label{as:Linearity condition and Coverage condition}
The joint distribution of $(\X,\Y)\in\mc H\times \R^{q}$ satisfies
\begin{itemize}
 \item[\textbf{i)}] \textit{Linearity condition:}
 The conditional expectation $\mathbb E\left[\langle \bs b,{\boldsymbol{X}}\rangle|\langle\bs{\beta}_1,{\boldsymbol{X}}\rangle,\dots,\langle\bs{\beta}_d,{\boldsymbol{X}}\rangle\right]$ is linear in $\langle \bs{\beta}_1,{\boldsymbol{X}}\rangle,...,\langle\bs{\beta}_d,{\boldsymbol{X}}\rangle$ for any $\bs b\in\mathcal H$.
 \item[\textbf{ii)}]\textit{Coverage condition:} $\mathrm{Rank}\l\mathrm{var}(\mb E[\bs X|\Y])\r=d$.
\end{itemize}
\end{assumption}
Both these conditions generalize multivariate ones in SDR literature \citep{li1991sliced,zhu2006sliced,lin2018consistency,lin2021optimality,mai2021slicing,huang2023sliced}. 

The next Lemma $\ref{as:Linearity condition and Coverage condition}$ is the basis of many functional SDR methods such as FSIR. 
\begin{lemma}[\citealt{ferre2003functional}]\label{lemma: SE=GammaS} 
Define
\begin{align}\label{equation, IRS}
\mathcal S_{\mathbb E(\boldsymbol{X}|\Y)}:=\mathrm{span}\{\mathbb E(\boldsymbol{X}|\Y=\bs y)\mid \bs y\in\mb R^q\}.
\end{align}
Then under Assumption $\ref{as:Linearity condition and Coverage condition}$, one has
\begin{equation*}
\Gamma \mc S_{\Y|\bs X}=\mc S_{\mathbb E(\boldsymbol{X}|\Y)}=\mathrm{Im}\{\mathrm{var}(\mb E(\bs X|\Y))\},
\end{equation*}
where $\Gamma \mc S_{\Y|\bs X}:=\mathrm{span}\{\Gamma\bs{\beta}_1,...,\Gamma\bs{\beta}_d\}.$
\end{lemma}
Based on this, one can estimate the central subspace $\mc S_{\Y|\vX}$ by estimating the eigenspace of $\Gamma^{-1} \mathrm{var}(\mb E[\bs X|\Y])$. 
While the central space is well-defined for both univariate and multivariate responses, most existing SDR methods, such as FSIR \citep{ferre2003functional}, mainly focus on the case of univariate response. 
Extending these methods to handle multivariate response is a challenging task.
The FSIR  procedures for estimating  $\mathrm{var}(\mb E[\bs X|Y])$ with univariate response can be briefly summarized as follows. Given $n$ i.i.d. samples $\{(\bs X_i,Y_{i})\}_{i\in[n]}$, 
% with the same distribution as $(\X,Y)$ from the multiple-index model \eqref{eq:multiple index model}
 FSIR divides the samples into $H$ equally-sized slices according to the order statistics $Y_{(i)}$ and estimates $\mathrm{var}[\mb E(\bs X|Y)]$ by
\begin{equation}
 \dfrac1H\sum\limits_{h=1}^H\overline{\bs {X}}_{h,\cdot}\otimes\overline{\bs {X}}_{h,\cdot},
\end{equation}
where $\overline{\bs {X}}_{h,\cdot}$ is the sample mean of the $h$-th slice. 

In general, the consistency and convergence rate
of SIR depend on the slice number $H$. Choosing a suitable slice number $H$ in SIR is a difficulty. 
 On one hand, too small $H$ yields too much samples in each slice. Then SIR can not fully characterize the dependence of the predictor on the response, which will cause a large bias. On the other hand, a small amount of samples in each slice yields a large variance of the estimation \citep{zhu1995asymptotics}.
To avoid this difficulty, we propose the method of FSFIR for multivariate response  in the following section (see also section $\ref{subsection, estimation method}$).
\subsection{Principle of FSFIR}\label{subsection, MDDO in SDR}
In this section, we lay the foundation of FSFIR.
The following conclusion about MDDO is needed.
\begin{theorem}\label{theorem, MDDO and IRS}
Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{as:Linearity condition and Coverage condition}$, we have
\[
{\mathcal{S}_{\mathbb E(\boldsymbol{X}|\Y)}}={\mathrm{Im}}\{\mathrm{MDDO}(\boldsymbol{X}|\Y)\}.
\]
\end{theorem}
This result generalizes \cite[Proposition 2]{mai2021slicing}.
Combining Theorem $\ref{theorem, MDDO and IRS}$ with Lemma $\ref{lemma: SE=GammaS}$, we can derive that:
\begin{corollary}
\label{corollary, MDDO and central subspace}
Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{as:Linearity condition and Coverage condition}$,
we have
\[\Gamma \mc S_{\Y|\vX}={\mathcal{S}_{\mathbb E(\boldsymbol{X}|\Y)}}=\mathrm{Im}\{\mathrm{MDDO}(\boldsymbol{X}|\Y)\}.\]
\end{corollary}
\section{FSFIR Algorithm and Corresponding Asymptotic Properties}\label{section, error}
Based on the above Corollary $\ref{corollary, MDDO and central subspace}$, we can estimate the central subspace $\mc S_{\Y|\vX}$ by estimating the eigen-space of $\Gamma^{- 1}\mathrm{MDDO}(\bs X|\Y)$ without specifying any slice scheme. So in this part, we develop the procedures of FSFIR in Section $\ref{subsection, estimation method}$ where a truncation scheme is adopted due to the unboundness of $\Gamma^{-1}$. 
Then in Section $\ref{subsection, convergence rate}$ we give the specific convergence rate of FSFIR for estimating $\mc S_{\Y|\vX}$. 
\subsection{FSFIR algorithm}\label{subsection, estimation method}
% In general, it is impossible to estimate $\Gamma^{-1}$ via $\wh\Gamma^{\dagger}$, the pseudo-inverse of the sample covariance operator $\widehat{\Gamma}:=\frac{1}{n}\sum^{n}_{i=1}\boldsymbol{X}_{i}\otimes \boldsymbol{X}_{i}$, because $\Gamma$ is compact ($\Gamma$ is a trace class) and $\Gamma^{-1}$ is unbounded. 
In general, it is not possible to estimate $\Gamma^{-1}$ directly using $\wh\Gamma^{\dagger}$, which is the pseudo-inverse of the sample covariance operator $\widehat{\Gamma}:=\frac{1}{n}\sum^{n}_{i=1}\boldsymbol{X}_{i}\otimes \boldsymbol{X}_{i}$. This is because $\Gamma$ is a compact operator (trace class) and $\Gamma^{-1}$ is unbounded.
% To tackle this issue, people adopted some measures 
To address this issue, various approaches have been proposed, such as truncation on the covariance operator \citep{ferre2003functional,chen2023optimality} and ridge-type regularization \citep{lian2015functional}. 
Our way to overcome this issue is to do truncation on the predictor (see also \cite{li2010deciding}). 





% Before we elaborate on it, we need an assumption about decomposition of the random function $\bs X$:
% \begin{assumption}\label{assumption: decomposition} There exists a pairwise uncorrelated random sequence $\{\omega_j\}_{j\in\Z_{\geqslant1}}$ with $\mb E[\omega_j]=0$ and $\mathrm{var}(\omega_j)=1$ for all $j$, such that 
% \begin{equation}\label{eq:X expansion}
% \bs X=\sum_{j=1}^\infty \sqrt{\lambda_j}\omega_j\phi_j,
% \end{equation}
% where $\{\lambda_j\}_{j\in\Z_{\geqslant1}}$ is a descending sequence of real numbers and $\{\phi_j\}_{j\in\Z_{\geqslant1}}$ is an orthonormal basis of $\mc H$. 
% \end{assumption}
% {\color{red}
% \begin{remark}
% In fact, when $\bs X$ is a square integrable function with continuous covariance, Assumption $\ref{assumption: decomposition}$ coincides with the Karhunen–Lo\.{e}ve decomposition \citep{hsing2015theoretical}. 
% Furthermore,
% if $\boldsymbol{X}$ is a centered Gaussian process, Assumption $\ref{assumption: decomposition}$ holds with $\{\omega_j\}_{j\in\Z_{\geqslant1}}$ independent and standard normal random variables \citep{kac1947explicit,deheuvels2008karhunen}.
% \end{remark}}
Now we state our truncation scheme. For a smoothing parameter $m$ satisfying 
\begin{align}\label{eq: m n relationship}
m=n^{c_1},\quad c_1\in(0,1), 
\end{align}
we define the truncated predictor as follows:
\begin{align*}
\X^{(m)}:=\Pi_m \bs X=\sum\limits_{j=1}^m\sqrt{\lambda_j}\omega_j\phi_j
\end{align*}
where $\Pi_m:=\sum\limits_{i=1}^m\phi_{i}\otimes\phi_i$. 
Accordingly, the truncated covariance operator $\Gamma_m:=\mathrm{var}(\bs X^{(m)})$ and the pseudo-inverse of $\Gamma_m$ satisfy:
\begin{align*}
\Gamma_m&=\sum\limits_{i=1}^m\lambda_i\phi_i\otimes\phi_i\quad\text{and}\quad \Gamma^\dag_m=\sum\limits_{i=1}^m\lambda_i^{-1}\phi_i\otimes\phi_i
\end{align*}
respectively. 
% {\color{red}Clearly, $\|\Gamma-\Gamma_m\|$ tends to $0$ as $m\to\infty$. Thus we can approximate $\Gamma^{-1}$ by $\wh\Gamma^\dag_m$:
% \begin{align*}
% \widehat{\Gamma}_m:=\frac{1}{n}\sum_{i=1}^n \bs X_i^{(m)}\otimes \bs X_i^{(m)}
% \end{align*}
% where $\bs X_i^{(m)}:=\Pi_m\bs X_i,i=1,...,n$. }
In accordance with the truncation on the predictor $\X$, we turn to estimate the \textit{truncated central subspace} $\mc S_{\Y|\bs X}^{(m)}$:
\begin{align}\label{def: truncated central subspace}\mc S_{\Y|\bs X}^{(m)}=\Pi_m \mc S_{\Y|\bs X}=\mathrm{span}\{\bs{\beta}_1^{(m)},\dots,\bs{\beta}_d^{(m)}\},\end{align}
 where $\bs{\beta}_{k}^{(m)}:=\Pi_m(\bs{\beta}_k)~\text{for}~k=1,\dots,d$.
To estimate $\mc S_{\Y|\bs X}^{(m)}$, we need a fundamental lemma.
\begin{lemma}\label{lemma, way of estimate truncate central subspace}
Under Assumption $\ref{as:joint distribution assumption}$, we have:
\begin{align*}
\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}=\Gamma_m^\dagger\mathrm{Im}\{\mathrm{MDDO}(\boldsymbol{X}^{(m)}|\Y)\}.
\end{align*}
\end{lemma}
Consequently, it amounts to estimating $\Gamma_m^\dagger\mathrm{Im}\{\mathrm{MDDO}(\boldsymbol{X}^{(m)}|\Y)\}$. To this end, we estimate both $\Gamma_m^\dagger$ and $\mathrm{Im}\{\mathrm{MDDO}(\boldsymbol{X}^{(m)}|\Y)\}$. 
From now on, we abbreviate
$\mathrm{MDDO}(\boldsymbol{X}|\Y)$, $\mathrm{MDDO}(\boldsymbol{X}^{(m)}|\Y)$ and $\widehat{\mathrm{MDDO}}(\boldsymbol{X}^{(m)}|\Y)$ to $M$, $M_m$ and $\widehat M_m$ respectively. 

We are in a position to state the estimation procedures of $\mathcal{S}^{(m)}_{{\Y|\boldsymbol{X}}}$.
Define the estimator of $\Gamma_m$ and $M_m$ as follows:
\begin{align*}
\widehat{\Gamma}_m:=\frac{1}{n}\sum_{i=1}^n \bs X_i^{(m)}\otimes \bs X_i^{(m)};\quad\wh M_m:=- \frac{1}{n^2}\sum_{j,k=1}^n\bs X_j^{(m)}\otimes \bs X_k^{(m)}\|\Y_j-\Y_k\|
\end{align*}
where $\bs X_i^{(m)}:=\Pi_m\bs X_i,i=1,...,n$.
Then the estimator of $\mc S_{\Y|\bs X}^{(m)}$ can be defined by
\begin{align}\label{eq:def of hat Sm}
\wh{\mc S}_{\Y|\bs X}^{(m)}:=\widehat{\Gamma}_m^\dagger\mathrm{Im}^d\lb\widehat M_m\rb, 
\end{align}
where $\mathrm{Im}^d\lb\widehat M_m\rb$ denotes the space spanned by the top $d$ eigenfunctions of $\wh M_m$.

An equivalent definition of $\wh{\mc S}_{Y|\bs X}^{(m)}$ can be derived as follows: define $\wh M_m^d$ by:
\begin{align}\label{wh M_m spectral decomposition}\wh M_m=\sum_{i=1}^\infty\wh\mu_i\wh\gamma_i\otimes\wh\gamma_i\quad\text{and}\quad \wh M_m^d:=\sum_{i=1}^d\wh\mu_i\wh\gamma_i\otimes\wh\gamma_i,\end{align}
where $\lb\wh\mu_i\rb_{i\in\Z_{\geq1}}$ are eigenvalues with descending order of $\wh M_m$, and $\wh\gamma_i$'s are eigenfunctions of $\wh M_m$ associated with $\wh\mu_i$. 
Then
\begin{align}\label{def: estimator central subspace}
\wh{\mc S}_{\bs{Y}|\bs X}^{(m)}= \mathrm{Im}\lb\widehat{\Gamma}_m^\dagger\widehat M_m^d\rb.\end{align}









Based on \eqref{eq:def of hat Sm}, we introduce detailed FSFIR procedures in Algorithm $\ref{alg:slicing-free}$.
\begin{algorithm}[H]
\setstretch{1.5}
\begin{algorithmic}
\caption {FSFIR. }\label{alg:slicing-free} 
\State 
\begin{enumerate}
\item Standardize $\{\X_i,1\leqslant i\leqslant n\}$, i.e., $\bs Z_i=\bs X_i-n^{- 1}\sum_{i=1}^n \bs X_{i};$
\item Do truncation: 
% for any $\gamma\in\l0,\tfrac{5}{4(\alpha_1+\alpha_2+3)}\r$, choosing $m\propto n^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}$ 
choose some $m$ and then obtain
 $\bs Z^{(m)}_{i}=\Pi_{m}\bs Z_{i}$;
\item Form the estimator $\widehat M_m$ and $\widehat\Gamma_m$ according to 
 \begin{equation*}
\widehat M_m=-\frac{1}{n^{2}}\sum_{j,k=1}^n\bs Z_j^{(m)}\otimes \bs Z_k^{(m)}\|\Y_j-\Y_k\|\quad\text{and}\quad \widehat\Gamma_m=\frac1n\sum_{i=1}^n \bs Z_i^{(m)} \otimes \bs Z_i^{(m)}
 \end{equation*}
 respectively;
\item Find the top $d$ eigenfunctions of $ \widehat{M}_{m}$ and denote them by $\widehat{\gamma}_{k} (k=1,\ldots,d)$;
\item Figure out $\widehat{\bs{\beta}}_{k}=\widehat{\Gamma}^{\dagger}_{m}\widehat{\gamma}_{k}(k=1,\dots,d$);
\end{enumerate}
\State  Return $\widehat{\mc S}_{Y|\bs X}^{(m)}=\mathrm{span}\lb\widehat{\bs{\beta}}_1,...,\widehat{\bs{\beta}}_d\rb$.
\end{algorithmic}
\end{algorithm}




% \begin{breakablealgorithm}
% \caption {FSFIR. ($\alpha_1,\alpha_2$ as in Assumption $\ref{assumption: rate-type condition}$)}
% \label{alg:slicing-free} 
% \noindent Standardize $\{\X_i,1\leqslant i\leqslant n\}$,i.e.,  $\bs Z_i=\bs X_i-n^{- 1}\sum_{i=1}^n \bs X_{i};$
% \begin{enumerate}
% \item Do truncation: for any $\gamma\in\l0,\tfrac{5}{4(\alpha_1+\alpha_2+3)}\r$, choosing $m\propto n^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}$ 
%  and then obtain
%  $\bs Z^{(m)}_{i}=\Pi_{m}\bs Z_{i}$;
% \item Form the estimator $\widehat M_m$ and $\widehat\Gamma_m$ according to 
%  \begin{equation*}
% \widehat M_m=-\frac{1}{n^{2}}\sum_{j,k=1}^n\bs Z_j^{(m)}\otimes \bs Z_k^{(m)}|Y_j-Y_k|\quad\text{and}\quad \widehat\Gamma_m=\frac1n\sum_{i=1}^n \bs Z_i^{(m)} \otimes \bs Z_i^{(m)}
%  \end{equation*}
%  respectively;
% \item Find the top $d$ eigenfunctions of $ \widehat{M}_{m}$ and denote them by $\widehat\eta_{k} (k=1,\ldots,d)$;
% \item Figure out $\widehat{\bs{\beta}}_{k}=\widehat{\Gamma}^{\dagger}_{m}\widehat\eta_{k}(k=1,\dots,d$);
% \end{enumerate}
% \noindent Return $\widehat{\mc S}_{Y|\bs X}^{(m)}=\mathrm{span}\lb\widehat\bs{\beta}_1,...,\widehat\bs{\beta}_d\rb$.
% \end{breakablealgorithm}

Our FSFIR algorithm provides an optimal selection criterion for $m$, 
precisely $m\propto n^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}$  for arbitrary $\gamma\in\l0,\tfrac{5}{4(\alpha_1+\alpha_2+3)}\r$, $\alpha_1$ and $\alpha_2$ are defined in Assumption $\ref{assumption: rate-type condition}$.
In practice, it should be better to choose $m=tn^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}$ for some $t\in[1/\log(n),\log(n)]$ which may be determined by cross-validation.


\begin{remark}
Note that the estimation method in Algorithm $\ref{alg:slicing-free}$ can be realized without specifying any slice number $H$. For the effectiveness of FSFIR, see the next Section $\ref{subsection, convergence rate}$.
\end{remark}

\subsection{Convergence rate of FSFIR estimator}\label{subsection, convergence rate}
% Now that we have established the estimator of the $\widehat{\Gamma}_m^\dagger\widehat M_m$ in order to estimate the central subspace, 
 Before stating our main result, we need a uniform sub-Gaussian assumption.
\begin{assumption}\label{assumption: sub-Gaussian}
There exist two positive constants $\sigma_0$ and $\sigma_1$ such that\begin{equation}\label{eq:uniform subgaussian bound}
 \sup_m\max_{1\leqslant j\leqslant m}\mathbb E\left[\exp\big(2\sigma_0\langle \boldsymbol{X},\phi_j\rangle^2\big)\right]\leqslant \sigma_1\quad\text{and} \quad \mathbb E\left[\exp\big(2\sigma_0\|\Y\|^2\big)\right]\leqslant \sigma_1. 
 \end{equation}
 \end{assumption}
These inequalities  generalize \cite[Condition (C1)]{mai2021slicing}.
Similar sub-Gaussian type conditions are commonly used in SIR literature \citep{lin2018consistency,lin2019sparse,lin2021optimality,huang2023sliced}.
We now derive a large deviation inequality between $\widehat M_m^d$ and $M_m$.
\begin{proposition}\label{prop:bound hatMmd Mm}
Under Assumptions $\ref{as:joint distribution assumption}$ and $\ref{assumption: sub-Gaussian}$, for all $\gamma\in(0,1/2)$, there exist positive constants $D_0=D_0(\gamma,\sigma_0,\sigma_1)$, $D_1=D_1(\sigma_1)$, $D_2=D_2(\sigma_0,\sigma_1)$ and $n_0=n_0(\gamma,\sigma_0,\sigma_1)$ such that for all $n\geqslant n_0$ and
$$C\in \l D_0n^{\frac{2\gamma}{5}}-\ln\l D_1m^2n \r,D_2 n^{\frac{1}{5}}-\ln\l D_1m^2n \r \rmi,$$ 
we have
\begin{equation*}
\mathbb{P}\l\|\wh M_m^d- M_m\| <\l \frac{C+\ln( D_1m^2n)}{D_2}\r^{\frac52}\frac{24m}{\sqrt n}\r\geqslant 1-\exp(- C).
\end{equation*}
\end{proposition}

Thanks to this proposition, we can derive a concentration inequality for $\wh\Gamma_m^\dagger\wh M_m^d$ around its population counterpart $\Gamma_m^\dagger M_m$. Before we get a hand on this, we recall the following rate-type condition which is fundamental in functional data analysis \citep{ferre2003functional, ferre2005smoothed,Hall2007mcflr,lei2014adaptive,lian2015functional,chen2023optimality}. 
\begin{assumption}[Rate-type condition]\label{assumption: rate-type condition}
There exist positive constants $\alpha_1$, $\alpha_2$, $\wt C$ and $\wt C'$ such that
\begin{align*}
\alpha_2>1/2,\quad\lambda_j\geqslant \wt Cj^{- \alpha_1}\quad\text{and}\quad |b_{ij}|\leqslant \wt C'j^{- \alpha_2}, \quad(\forall i\in[d],j\in\mb Z_{+})
\end{align*}
 where $b_{ij}:=\langle  \bs{\beta}_i,\phi_j\rangle$ for $\bs\beta_i$ defined in \eqref{def: central subspace}.
\end{assumption}
\begin{remark}
 The assumption about the eigenvalues $\lambda_{j}$ of $\Gamma$ ensures that the estimation of eigenfunctions of $\Gamma$ is accurate. The assumption about the coefficients $b_{ij}$ implies that they do not decrease too quickly concerning $j$ uniformly for all $i$. This assumption also implies that any basis $\{\wt\bbeta_i\}_{i=1}^d$ of $\mc S_{\Y|\vX}$ that satisfies $\wt\bbeta_i=\sum_{j=1}^\infty\wt{b}_{ij}\phi_j$ has coefficients $|\wt b_{ij}|\lesssim j^{-\alpha_2}$.
% The decay rate type assumptions such as Assumption $\ref{assumption: rate-type condition}$ or its variant are generally required when studying the coverage rate of FSIR \citep{ferre2003functional, ferre2005smoothed,Hall2007mcflr,lei2014adaptive,lian2015functional,chen2023optimality}.
 \end{remark}
\begin{proposition}
\label{prop:concentration Gammam dag Mmd}
 Suppose that Assumptions $\ref{as:joint distribution assumption}$ to $\ref{assumption: rate-type condition}$ hold, then $\forall \gamma\in(0,1/2)$, we have
\begin{equation*}
\begin{aligned}
 \lno\widehat\Gamma_m^\dagger \widehat M_m^d-\Gamma_m^\dagger M_m\rno =O_{\mb{P}}\l \frac{m^{\alpha_1+1}}{n^{1/2-\gamma}} \r.
\end{aligned}
\end{equation*}
\end{proposition}
This proposition is used to give an error bound of the subspace estimation error in the subsequent Theorem $\ref{theorem, total convergence rate}$.
Now we state the convergence rate of our FSFIR estimator:
\begin{theorem}\label{theorem, total convergence rate}
Assume Assumptions $\ref{as:joint distribution assumption}$ to $\ref{assumption: rate-type condition}$ hold. Then for any $\gamma\in\l0,\tfrac{5}{4(\alpha_1+\alpha_2+3)}\r$, by choosing 
$m=n^{\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}}$ (i.e.,  $c_1=\frac{1-2\gamma}{2\alpha_1+2\alpha_2+1}$), we can get that
% \begin{align*}
%  \left\|P_{\mc{S}_{Y|\X}}-P_{\widehat{ \mc{S}}_{Y|\X}^{(m)}}\right\| =O_{\mb{P}}\l n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2(2\alpha_1+2\alpha_2+1)}}\r. 
% \end{align*}
\begin{align*}
 \mb E\left[\left\|P_{\mc{S}_{\Y|\X}}-P_{\widehat{ \mc{S}}_{\Y|\X}^{(m)}}\right\|^2\right] \lesssim n^{-\frac{(2\alpha_2-1)(1-2\gamma)}{2\alpha_1+2\alpha_2+1}}. 
\end{align*}
\end{theorem}

This specific convergence rate guarantees the effectiveness of FSFIR. To prove this convergence rate, we decompose the error into two parts: $\mathbf{Loss}_1$  caused by truncation which is easy to bound and $\mathbf{Loss}_2$ caused by estimating $\wh{\mc S}_{Y|\bs X}^{(m)}$ with finite samples. Our main job is to bound the latter one. To this end, we apply the generalized  Sin Theta theorem in \cite[Proposition 2.3]{seelmann2014notes} to non-symmetric operator. Then, $\mathbf{Loss}_2$ is bounded by 
combining this non-symmetric Sin Theta theorem with Proposition $\ref{prop:concentration Gammam dag Mmd}$.

\section{Numerical Performance of FSFIR}\label{section, experiments}

In this section, we study the numerical performance of FSFIR from several aspects. The first  experiments focuses on the empirical subspace estimation error performance of FSFIR for estimating the central subspace in  synthetic data.
The  experiment includes the comparison with some well-known FSIR methods  including the truncated FSIR \citep{ferre2003functional,chen2023optimality} and regularized FSIR \citep{lian2015functional}. The results reveal the  advantage and  convenience of FSFIR to practice.
Then we apply FSFIR algorithm to a real data: the bike sharing data to  demonstrate the efficiency of our algorithm.
\subsection{Synthetic experiments}\label{sec:Synthetic}
We first introduce the synthetic models we considered in this subsection. All synthetic models are of a functional-valued predictor.
Throughout this section, we set  $\varepsilon\sim N(0,0.25)$, i.e., a noise level of $0.25$. The  experimental results  corresponding to a higher noise level such as $1$ are presented in the Supplementary Materials.
\begin{example}\citep[Example 1]{lei2014adaptive,lee2020testing} 
First, let $\bar\beta_1=0.3$, $\bar\beta_j=4(- 1)^jj^{- 2}(j\geqslant2)$ and $\beta_j=\bar\beta_j/\|\bar\beta\|$ where $\bar\beta=\{\bar\beta_n\}_{n=1}^\infty$. Then we define $\bs{\beta}(t)=\sum\limits_{j=1}^{100}\beta_j\phi_j(t)$ where $\phi_1(t)=1$ together with $\phi_j(t)=\sqrt2 \cos[(j-1)\pi t](j\geqslant2)$ form an orthonormal basis. Then we define model $\mc M_1$ as follows:
\begin{align*}
\mc M_1:
&\quad~Y=\langle \bs X(t),\bs{\beta}(t)\rangle+\varepsilon;\\
&\bs X(t)=\sum_{j=1}^{100} j^{- 0.55}X_j\phi_j(t),t\in[0,1],
\end{align*}
where $X_j\overset{\mathrm{iid}}{\sim}N(0,1)$.
\end{example}

\begin{example}\citep[example M1]{lian2015functional}
Consider $\bs{\beta}_1(t)=\sqrt2\sin\left(\frac32\pi t\right)$ and $\bs{\beta}_2(t)=\sqrt2\sin\left(\frac52\pi t\right)$. Define model $\mc M_2$ as follows:
\[
\mc M_2: Y=\langle\bs{\beta}_1,\bs X\rangle+100\langle\bs{\beta}_2,\bs X\rangle^3+\varepsilon,
\]
where $\bs X$ is a standard Brown motion and we approximate it by the top 100 eigenfunctions of the Karhunen–Loève decomposition in practical implementation.
\end{example}

\begin{example}
    $Y=\exp(\langle \bs \beta,\boldsymbol{X}\rangle )+\epsilon$,
    where $\boldsymbol{X}$ is the standard Brownian motion on $[0,1]$, and $\bs\beta=\sqrt2\sin(\frac{3\pi t}{2})$.
\end{example}

% \begin{example}
% Let
% \begin{equation*}
% \beta_{1,j}=\left\{\begin{matrix}
% 8j^{- 2}, & j~\mathrm{is~even};\\
%  0 & j~\mathrm{is~odd},
% \end{matrix}\right.
% \qquad
% \beta_{2,j}=\left\{\begin{matrix}
% 8j^{- 2}, & j~ \mathrm{is~ odd};\\
%  0, & j~ \mathrm{is~ even}, 
% \end{matrix}\right.
% \end{equation*}
% and define:
% \begin{equation*}
% \beta_1(t)=\sum_{j=1}^{100}\beta_{1,j}\phi_j(t);\quad\beta_2(t)=\sum_{j=1}^{100}\beta_{2,j}\phi_j(t),
% \end{equation*}
% where $\phi_n(t):=\sqrt2\sin[(n-\frac12)\pi t](n\geqslant 1)$ is an orthonormal basis. Define model $\mc M_3$ as follows:
% \[
% \mc M_3:{Y}=100\langle\beta_1(t),\bs X(t)\rangle^3+\langle\beta_2(t),\bs X(t)\rangle+\varepsilon
% \]
% where $\bs X$ is also a standard Brown motion.
% \end{example}

In the following, we compare our FSFIR method with several slice-based methods using models $\mc M_1$ to $\mc M_3$.
Consider two slice-based methods --- one is truncated FSIR \citep{ferre2003functional,chen2023optimality}, which studies a truncation on the covariance operator, and the other is regularized FSIR \citep{lian2015functional}, which estimates $\S$ by applying a regularization tune parameter $\rho$ on $\Gamma$. In the following, we abbreviate these two methods as TFSIR and RFSIR respectively. 
% We abbreviate these two slice-based methods to \textit{truncated FSIR} and \textit{regularized FSIR} respectively.


In this experiment, we set the sample size $n=20000$. For slice-based methods, we set the slice number $H=10$, a popularly adopted slice number. To evaluate the performance of these methods, we choose the  subspace estimation error: $\mc D(\bs B;\bs{\wh{B}}):=\left\|P_{\bs B}-P_{\bs{\wh B}}\right\| $ where $\bs B:=(\bs\beta_1,...,\bs\beta_d):\R^d\to \mc H$, $\bs{\wh B}:=(\wh{\bs\beta}_1,...,\wh{\bs\beta}_d):\R^d\to \mc H$.
This metric takes value in $[0,1]$ and the smaller it is, the better the performance.

For each model, we choose  several $m$'s for FSFIR and TFSIR, among which one tends to have the best performance. 
% Following the same fashion, we select several $\rho$'s for RFSIR.
Specifically, 
$m$ ranges in $\{2,3,\dots,13,14,20,30,40\}$. Following the same fashion,  $\rho$ ranges in $0.01\times \{1,2,\cdots,9,10,15,20,25,30,40,\cdots,140,150\}$. Each trial is repeated 100 times for reliability.
We show the average $\mc D(\bs B;\bs{\wh B})$ with different $m$ or $\rho$ for three methods under $\mc M_1$ to $\mc M_3$ in Figure \ref{fig:error 3models},
where we mark minimal error in each model with red `$\times$'. The shaded areas represent the standard error associated with these estimates and all of them are less than  $0.009$. For FSFIR, the  minimal errors for $\mc M_1-\mc M_3$ are  $0.06,0.01,0.01$ respectively.
For TFSIR, the  minimal errors are  $0.06,0.02,0.01$ and for regularized FSIR,  the  minimal errors are $0.09,0.04,0.01$.  

% Figure environment removed
% and \textit{standard error} (SE) multiplied by 1000 in Table $\ref{table, Comparsion of different methods}$.

Figure \ref{fig:error 3models} shows that FSFIR attains the best performance among  all models. 
Moreover, FSFIR is easier to practice as it does not need a slice number $H$ in advance. 
% \begin{table}[htbp]
% \renewcommand\arraystretch{0.3}
% \centering
% \scalebox{0.9}{
% \begin{tabular}{ |c||c||c||c||c||c|}
%  \hline
%  \multicolumn{6}{ |c| }{Compare FSFIR with TFSIR and RFSIR}\\
% %  \multicolumn{6}{ |c| }{ and regularized FSIR} \\
%  \hline  Model&$m$&FSFIR&TFSIR&$\rho$& RFSIR\\
% %  \multirow{1}{*}{Model} & \multirow{1}{*}{$m$} &\multirow{1}{*}{FSFIR} & TFSIR& \multirow{1}{*}{$\rho$} & RFSIR \\ 
% %  & & & FSIR& & FSIR \\ 
%  \hline
%  \multirow{4}{*}{$\mc M_1$} 
%  &10& 0.058(0.3) & 0.061(2.9) & 0.01&0.059(4.2) \\
%  &20& 0.027(1.9) & 0.038(4.5)& 0.02&0.049(2.7) \\ 
%  &30& 0.028(3.4) & 0.046(6.1)&0.03& 0.049(3.3) \\
%  &40& 0.035(4.9)& 0.061(6.9)&0.04 & 0.054(2.9) \\
%  \hline \hline
%  \multirow{4}{*}{$\mc M_2$} 
%  &2& 0.707(0)
% % $3*10^{- 16}$

% % $3*10^{- 16}$
% & 0.707(0.01) &0.6& 0.025(6.5) \\
%  &3& 0.007(5.4) & 0.015(3.6)&0.65& 0.024(5.5) \\ 
%  &5& 0.044(24) & 0.040(18.8)&0.7 & 0.025(5.1) \\
%  &8& 0.102(34.8) & 0.086(27.6)& 0.75&0.027(7.4) \\
%  \hline \hline
%  \multirow{4}{*}{$\mc M_3$} 
%  &3& 0.206(9.6) & 0.204(8.2)&0.05 & 0.141(13.1) \\
%  &5& 0.168(42.6) & 0.149(36.2)&0.075& 0.132(9.9) \\ 
%  &8& 0.263(71.6) & 0.196(56.1)&0.1& 0.132(7.9) \\
%  &10& 0.333(81.4) & 0.267(66.8) &0.125& 0.135(7.5) \\
%  \hline \hline
%  \hline
% \end{tabular}}
% \caption{average error and SE multiplied by 1000. {\color{red}Reset this experiment to make FSFIR behave best.}}
% \label{table, Comparsion of different methods}
% \end{table}

% \subsection{Sensitivity to the truncation parameter $m$}
% In Figure $\ref{figure, comparsion of MDDO and FSIR}$, we show the boxplots of the regularized subspace estimation error $\mc D(\bs\beta;\bs{\wh\beta})$ for models $\mc M_1$ to $\mc M_3$ for both FSFIR and truncated FSIR based on $100$ replications. Aiming to finding the best $m$, we did experiments for $m$ ranging in
% $\{2,3,5,8,10,20,30,40,50\}$ and $n=20000$.
% Regarding the 
% % best performance (as has been shown in Table \ref{table, Comparsion of different methods}) and 
% the stability about $m$, it turns out that FSFIR and truncated FSIR are almost of the same level. Again, this result shows the convenience of FSFIR to practice.
% % Figure environment removed
\subsection{Application to real data}
In this section, we analyze the bike sharing data \citep{fanaee2014event,lee2022functional}, which includes hourly bike rental counts and weather information such as temperature, precipitation, wind speed, and humidity. The data was collected every day from January 1, 2011 to December 31, 2012 from the Capital Bike Share system in Washington, DC and can be found at \url{https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset}.

The main goal of this section is to investigate how temperature affects bike rentals on Saturdays. After removing data from three Saturdays with significant missing information, we plot hourly bike rental counts and hourly normalized temperature (values divided by 41, the maximum value) for 102 Saturdays in Figure \ref{figure, comparsion of MDDO and FSIR, real data}. In the following analysis, we use hourly normalized temperature and the logarithm of the daily average  bike rental counts as the predictor function and scalar response, respectively.
% Figure environment removed
% After reducing the dimension using FSFIR, we used Gaussian process regression to fit a nonparametric regression model. 


To evaluate the estimation error performance of  FSFIR for estimating the central space, we incorporate dimension reduction with FSFIR as an intermediate step in modeling the relationship between the predictor and response variables. Specifically, we apply FSFIR  to perform dimension reduction on a given training dataset $\{(\vX_i,Y_i)\}_{i=1}^n$. This yields a set of low-dimensional predictors $\bs x_i$ for each $i\in [n]$. Subsequently, we utilize Gaussian process regression to fit a nonparametric regression model using the samples $\{(\bs x_i,Y_i)\}_{i=1}^n$.
We randomly selected $90$ samples as the training data and used the remaining data to calculate the out-of-sample mean square error (MSE). We repeated this process $100$ times and calculated the mean and standard error. The results are presented in Table \ref{tab:real data error},  which suggests that FSFIR performs well in practical applications. 
It is noteworthy that the best
result of  FSFIR  is observed when $d=1$. 
This means  FSFIR  provides an  accurate and simpler (lower dimensional) model for the relationship between the response variable and the predictor. 
% As a result, FSFIR is highly effective in detecting the most significant associations between the response variable and the predictor, making it an efficient method for dimension reduction.
\begin{table}[H]
\renewcommand\arraystretch{0.3}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 & $m$   & $1$ & $3$ & $5$ & $7$ & $9$  & $11$ & $13$  \\
\hline
\hline
\multirow{5}{*}{FSFIR} 
& $d=1$  & \emph{\textbf{0.230}} & \textbf{0.259} & \textbf{0.265} & \textbf{0.247} & \textbf{ 0.280} & \textbf{ 0.319}  & \textbf{0.320} \\ 
& & {\footnotesize(0.0097)} & {\footnotesize(0.0126)} & {\footnotesize(0.0127)} & {\footnotesize(0.0122)} & {\footnotesize(0.0128)} & {\footnotesize(0.0143)} & {\footnotesize(0.0160)} 
\\
& $d=2$  &  & {\textbf{0.356}} & \textbf{0.276} & \textbf{0.372} & \textbf{0.334} & \textbf{ 0.396} & \textbf{0.329} \\ 
& & & {\footnotesize(0.0409)} & {\footnotesize(0.0170)} & {\footnotesize(0.0460)} & {\footnotesize(0.0312)} & {\footnotesize(0.0226)}  & {\footnotesize(0.0170)} 
\\ 
& $d=3$  &  & \textbf{0.358} & \textbf{0.461} & \textbf{0.370} & \textbf{0.420} & \textbf{ 0.625}& \textbf{0.396} \\ 
& &  & {\footnotesize(0.0303)} & {\footnotesize(0.0507)} & {\footnotesize(0.0304)} & {\footnotesize(0.0390)} & {\footnotesize(0.0795)}  & {\footnotesize(0.0365)} 
\\
& $d=4$  &  &  & \textbf{0.699} & \textbf{0.473} & \textbf{0.726} & \textbf{ 0.831}& \textbf{0.460} \\ 
& &  &  & {\footnotesize(0.0544)} & {\footnotesize(0.0584)} & {\footnotesize(0.0854)} & {\footnotesize(0.1008)}   & {\footnotesize(0.0374)} 
\\
& $d=5$  & &  & \textbf{1.052} & \textbf{0.876} & \textbf{1.131} & \textbf{0.883} & \textbf{0.936}\\ 
& &  &  & {\footnotesize(0.0710)} & {\footnotesize(0.0682)} & {\footnotesize(0.0930)} & {\footnotesize(0.0942)}    & {\footnotesize(0.0875)} 
\\[1pt]
\hline
\end{tabular}
\caption{The empirical mean (standard error) of MSE.}\label{tab:real data error}
\end{table}











% \begin{table}[H]
% \renewcommand\arraystretch{0.3}
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
% \hline
% $H=10$ & $m$   & $1$ & $3$ & $5$ & $7$ & $9$  & $11$ & $13$  \\
% \hline
% \hline
% \multirow{5}{*}{FSFIR} 
% & $d=1$  & \textbf{0.182} & \emph{\textbf{0.176}} & \textbf{0.183} & \textbf{0.178} & \textbf{ 0.193} & \textbf{ 0.200}  & \textbf{0.267} \\ 
% & & {\footnotesize(0.0083)} & {\footnotesize(0.0069)} & {\footnotesize(0.0077)} & {\footnotesize(0.0070)} & {\footnotesize(0.0079)} & {\footnotesize(0.0068)} & {\footnotesize(0.0111)} 
% \\
% & $d=2$  &  & {\textbf{0.247}} & \textbf{0.183} & \textbf{0.231} & \textbf{0.232} & \textbf{ 0.232} & \textbf{0.326} \\ 
% & & & {\footnotesize(0.0185)} & {\footnotesize(0.0068)} & {\footnotesize(0.0257)} & {\footnotesize(0.0241)} & {\footnotesize(0.0146)}  & {\footnotesize(0.0263)} 
% \\ 
% & $d=3$  &  & \textbf{0.812} & \textbf{0.442} & \textbf{0.358} & \textbf{0.357} & \textbf{ 0.267}& \textbf{0.426} \\ 
% & &  & {\footnotesize(0.0745)} & {\footnotesize(0.0660)} & {\footnotesize(0.0669)} & {\footnotesize(0.0450)} & {\footnotesize(0.0175)}  & {\footnotesize(0.0319)} 
% \\
% & $d=4$  &  &  & \textbf{1.213} & \textbf{0.700} & \textbf{0.585} & \textbf{ 0.455}& \textbf{0.536} \\ 
% & &  &  & {\footnotesize(0.1274)} & {\footnotesize(0.0650)} & {\footnotesize(0.0600)} & {\footnotesize(0.0488)}   & {\footnotesize(0.0460)} 
% \\
% & $d=5$  & &  & \textbf{2.445} & \textbf{1.206} & \textbf{1.008} & \textbf{0.856} & \textbf{0.765}\\ 
% & &  &  & {\footnotesize(0.1483)} & {\footnotesize(0.1022)} & {\footnotesize(0.0915)} & {\footnotesize(0.0715)}    & {\footnotesize(0.0700)} 
% \\[1pt]
% \hline
% \multirow{5}{*}{TFSIR}
% & $d=1$  & \textbf{0.199} & \textbf{0.183} & \textbf{0.192} & \textbf{0.183} & \textbf{ 0.187} & \emph{\textbf{0.177}}   & \textbf{0.188} \\ 
% &  &{\footnotesize(0.0082)} & {\footnotesize(0.0080)} & {\footnotesize(0.0084)} & {\footnotesize(0.0087)} & {\footnotesize(0.0095)} & {\footnotesize(0.0083)}  & {\footnotesize(0.0085)} 
% \\
% & $d=2$  &  & \textbf{0.186} & \textbf{0.182} & \textbf{0.183} & \textbf{0.190} & \textbf{0.182} & \textbf{0.213}\\ 
% & &  & {\footnotesize(0.0084)} & {\footnotesize(0.0084)} & {\footnotesize(0.0083)} & {\footnotesize(0.0088)} & {\footnotesize(0.0096)}  & {\footnotesize(0.0133)} 
% \\ 
% & $d=3$  &  & \textbf{0.178 }& \textbf{0.181} & \textbf{0.184} & \textbf{0.181} & \textbf{ 0.219} & \textbf{0.248}\\ 
% & &  & {\footnotesize(0.0079)} & {\footnotesize(0.0078)} & {\footnotesize(0.0078)} & {\footnotesize(0.0087)} & {\footnotesize(0.0109)}  & {\footnotesize(0.0312)} 
% \\
% & $d=4$  &  &  & \textbf{0.178 }& \textbf{0.193} & \textbf{0.218} & \textbf{ 0.198}  & \textbf{0.265}  \\ 
% & &  &  & {\footnotesize(0.0080)} & {\footnotesize(0.0118)} & {\footnotesize(0.0104)} & {\footnotesize(0.0093)}  & {\footnotesize(0.0216)} 
% \\
% & $d=5$  &  & & \textbf{ 0.179} & \textbf{ 0.246} & \textbf{0.247} & \textbf{ 0.236}  & \textbf{0.325}\\ 
% & & &  & {\footnotesize(0.0070)} & {\footnotesize(0.0116)} & {\footnotesize(0.0157)} & {\footnotesize(0.0209)}   & {\footnotesize(0.0298)} 
% \\[1pt]
% \hline
%  & $\rho$    & $e^{-6}$ & $e^{-5}$ & $e^{-4}$ & $e^{-3}$  & $e^{-2}$ & $e^{-1}$  & $e^{0}$ \\
% \hline
% \multirow{5}{*}{RFSIR} 
% & $d=1$   & \textbf{0.187} & \textbf{0.187} & \textbf{0.195} & \textbf{ 0.206} & \textbf{0.210} & \textbf{0.196}& \textbf{0.226} \\ 
% &  & {\footnotesize(0.0087)} & {\footnotesize(0.0075)} & {\footnotesize(0.0079)} & {\footnotesize(0.0077)} & {\footnotesize(0.0099)} & {\footnotesize(0.0093)}   & {\footnotesize(0.0088)} 
% \\
% & $d=2$  & \textbf{0.174} & \textbf{0.191} & \textbf{0.174} & \textbf{0.184} & \textbf{ 0.196} & \textbf{0.199}& \textbf{0.233}  \\ 
% & &  {\footnotesize(0.0073)} & {\footnotesize(0.0081)} & {\footnotesize(0.0086)} & {\footnotesize(0.0081)} & {\footnotesize(0.0077)} & {\footnotesize(0.0091)}& {\footnotesize(0.0096)} 
% \\ 
% & $d=3$   & \textbf{0.177} & \textbf{0.179} & \textbf{0.197} & \textbf{ 0.180} & \textbf{ 0.183} & \textbf{0.194} & \textbf{0.233} \\ 
% &  & {\footnotesize(0.0081)} & {\footnotesize(0.0078)} & {\footnotesize(0.0089)} & {\footnotesize(0.0078)} & {\footnotesize(0.0083)} & {\footnotesize(0.0072)} & {\footnotesize(0.0095)} 
% \\
% & $d=4$  & \textbf{0.188} & \emph{\textbf{0.164}} & \textbf{0.184} & \textbf{0.195} & \textbf{ 0.181} & \textbf{0.193}& \textbf{0.217}  \\ 
% &  & {\footnotesize(0.0077)} & {\footnotesize(0.0071)} & {\footnotesize(0.0091)} & {\footnotesize(0.0079)} & {\footnotesize(0.0080)} & {\footnotesize(0.0086)}  & {\footnotesize(0.0098)} 
% \\
% & $d=5$  & \textbf{0.181} & \textbf{0.175} & \textbf{0.183} & \textbf{0.173} & \textbf{0.183} & \textbf{0.196}& \textbf{0.221} \\ 
% &  & {\footnotesize(0.0086)} & {\footnotesize(0.0064)} & {\footnotesize(0.0072)} & {\footnotesize(0.0077)} & {\footnotesize(0.0078)} & {\footnotesize(0.0091)} & {\footnotesize(0.0117)}
% \\[1pt]
% \hline
% \end{tabular}
% \caption{The empirical mean (standard error) of MSE.{\color{red}Under modification}}\label{tab:real data error}
% \end{table}
\section{Concluding Remarks}\label{section, Discussion}
In summary, we introduce two novel objects, the statistics MDDO and the method FSFIR.
MDDO serves to measure the conditional mean independence of a functional-valued predictor on a multivariate response. And based on MDDO, FSFIR aims to estimating the central subspace $\mc S_{\Y|\vX}$. 




Besides MDDO, there are other ways to examine the conditional mean
independence in functional-data cases, such as the \textit{functional martingale
difference divergence} (FMDD, \citealt{lee2020testing}). However, we would like to point out an extra feature of our MDDO --- Under certain circumstances, a low rank projection of $\bs X$ is conditional mean independent of $Y$ even if $\X$ is not. In other words, for some $e\in\mc H$, $\E[\langle e,\X\rangle|Y]=\E[\langle e,\X\rangle]$ but $\E[\X|Y]$ is not equal to $\E[\X]$. 
In this case, we can use MDDO to
separate out a part that is conditional mean independent of $Y$ in view of Theorem $\ref{theorem, MDDO and conditional mean independence}$ (ii). This property of MDDO makes it a tool for slicing-free estimation (see the proof of Theorem $\ref{theorem, MDDO and IRS}$).

However, there are still several open problems that remain to be solved. For example, a study of FSFIR   from a decision theoretic point of view  is interesting. Additionally, it would be interesting to extend these methods to cases with high-dimensional  or functional-valued response. We plan to explore these topics in future research.

\section*{Supplementary Materials}
Supplement to ``Functional Slicing-free Inverse Regression via Martingale Difference Divergence Operator''. The supplementary material includes
the proofs for all the theoretical results in the paper.



 
\bibliography{reference}
\bibliographystyle{chicago}

\bigskip
\vskip .65cm

\noindent
Songtao Tian, Department of Mathematical Sciences, Tsinghua University
\vskip 2pt
\noindent
E-mail: tst20@mails.tsinghua.edu.cn


\noindent
Zixiong Yu, Department of Mathematical Sciences, Tsinghua University
\vskip 2pt
\noindent
E-mail: yuzx19@mails.tsinghua.edu.cn




\noindent
Rui Chen, Center for Statistical Science, Department of Industrial Engineering, Tsinghua University
\vskip 2pt
\noindent
E-mail: chenrui@mail.tsinghua.edu.cn
\vskip 2pt




\noindent
Qian Lin, Center for Statistical Science, Department of Industrial Engineering, Tsinghua University;~Beijing Academy of Artificial Intelligence, Beijing, 100084, China
\vskip 2pt
\noindent
E-mail: qianlin@tsinghua.edu.cn


\input{supplement.tex}












\end{document}
