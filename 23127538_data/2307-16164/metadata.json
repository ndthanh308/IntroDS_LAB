{
  "title": "Adaptive learning of density ratios in RKHS",
  "authors": [
    "Werner Zellinger",
    "Stefan Kindermann",
    "Sergei V. Pereverzyev"
  ],
  "submission_date": "2023-07-30T08:18:39+00:00",
  "revised_dates": [
    "2023-12-12T08:48:57+00:00",
    "2024-01-02T09:32:23+00:00"
  ],
  "abstract": "Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.",
  "categories": [
    "cs.LG",
    "math.ST",
    "stat.ML"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": "Journal of Machine Learning Research 24 (395), 1-28, 2023",
  "arxiv_id": "2307.16164",
  "pdf_url": "https://arxiv.org/pdf/2307.16164v3",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 6272593,
  "size_after_bytes": 534785
}