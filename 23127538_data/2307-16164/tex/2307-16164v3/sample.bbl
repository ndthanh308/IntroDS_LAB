\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bach(2010)]{bach2010self}
Francis Bach.
\newblock {Self-concordant analysis for logistic regression}.
\newblock \emph{Electronic Journal of Statistics}, 4\penalty0 (none):\penalty0 384 -- 414, 2010.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzev, and Rosasco]{bauer2007regularization}
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of Complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.

\bibitem[Bickel et~al.(2009)Bickel, Br{\"u}ckner, and Scheffer]{bickel2009discriminative}
Steffen Bickel, Michael Br{\"u}ckner, and Tobias Scheffer.
\newblock Discriminative learning under covariate shift.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0 (9), 2009.

\bibitem[Birg{\'e}(2001)]{birge2001alternative}
Lucien Birg{\'e}.
\newblock An alternative point of view on lepski's method.
\newblock \emph{Lecture Notes-Monograph Series}, pages 113--133, 2001.

\bibitem[Blanchard and M{\"u}cke(2018)]{blanchard2018optimal}
Gilles Blanchard and Nicole M{\"u}cke.
\newblock Optimal rates for regularization of statistical inverse learning problems.
\newblock \emph{Foundations of Computational Mathematics}, 18:\penalty0 971--1013, 2018.

\bibitem[Blanchard et~al.(2019)Blanchard, Math{\'e}, and M{\"u}cke]{blanchard2019lepskii}
Gilles Blanchard, Peter Math{\'e}, and Nicole M{\"u}cke.
\newblock {L}epskii principle in supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.10764}, 2019.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0 (3):\penalty0 331--368, 2007.

\bibitem[Caponnetto and Yao(2010)]{caponnetto2010cross}
Andrea Caponnetto and Yuan Yao.
\newblock Cross-validation based adaptation for regularization operators in learning theory.
\newblock \emph{Analysis and Applications}, 8\penalty0 (02):\penalty0 161--183, 2010.

\bibitem[Caponnetto et~al.(2005)Caponnetto, Rosasco, De~Vito, and Verri]{caponnetto2005empirical}
Andrea Caponnetto, Lorenzo Rosasco, Ernesto De~Vito, and Alessandro Verri.
\newblock Empirical effective dimension and optimal rates for regularized least squares algorithm.
\newblock Technical report, Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT, 2005.

\bibitem[Cucker and Smale(2001)]{Cucker2001OnTM}
F.~Cucker and S.~Smale.
\newblock On the mathematical foundations of learning.
\newblock \emph{Bulletin of the American Mathematical Society}, 39:\penalty0 1--49, 2001.

\bibitem[De~Vito et~al.(2010)De~Vito, Pereverzyev, and Rosasco]{de2010adaptive}
Ernesto De~Vito, Sergei~V Pereverzyev, and Lorenzo Rosasco.
\newblock Adaptive kernel methods using the balancing principle.
\newblock \emph{Foundations of Computational Mathematics}, 10\penalty0 (4):\penalty0 455--479, 2010.

\bibitem[Dinu et~al.(2023)Dinu, Holzleitner, Beck, Nguyen, Huber, Eghbal-zadeh, Moser, Pereverzyev, Hochreiter, and Zellinger]{dinu2022aggregation}
M.-C. Dinu, M.~Holzleitner, M.~Beck, D.~H. Nguyen, A.~Huber, H.~Eghbal-zadeh, B.~A. Moser, S.~V. Pereverzyev, S.~Hochreiter, and W.~Zellinger.
\newblock Addressing parameter choice issues in unsupervised domain adaptation by aggregation.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Dudley(2002)]{dudley2018real}
Richard~M Dudley.
\newblock \emph{Cambridge studies in advanced mathematics: {R}eal analysis and probability}.
\newblock 74. Cambridge University Press, 2nd edition, 2002.

\bibitem[Engl et~al.(1996)Engl, Hanke, and Neubauer]{engl1996regularization}
Heinz~Werner Engl, Martin Hanke, and Andreas Neubauer.
\newblock \emph{Regularization of inverse problems}, volume 375.
\newblock Springer Science \& Business Media, 1996.

\bibitem[Gizewski et~al.(2022)Gizewski, Mayer, Moser, Nguyen, Pereverzyev~Jr, Pereverzyev, Shepeleva, and Zellinger]{gizewski2022regularization}
Elke~R Gizewski, Lukas Mayer, Bernhard~A Moser, Duc~Hoan Nguyen, Sergiy Pereverzyev~Jr, Sergei~V Pereverzyev, Natalia Shepeleva, and Werner Zellinger.
\newblock On a regularization of unsupervised domain adaptation in {RKHS}.
\newblock \emph{Applied and Computational Harmonic Analysis}, 57:\penalty0 201--227, 2022.

\bibitem[Goldenshluger and Pereverzev(2000)]{goldenshluger2000adaptive}
Alexander Goldenshluger and Sergei~V Pereverzev.
\newblock Adaptive estimation of linear functionals in {H}ilbert scales from indirect white noise observations.
\newblock \emph{Probability Theory and Related Fields}, 118\penalty0 (2):\penalty0 169--186, 2000.

\bibitem[Guo et~al.(2022)Guo, Guo, and Shi]{guo2022capacity}
Xin Guo, Zheng-Chu Guo, and Lei Shi.
\newblock Capacity dependent analysis for functional online learning algorithms.
\newblock \emph{arXiv preprint arXiv:2209.12198}, 2022.

\bibitem[Hido et~al.(2011)Hido, Tsuboi, Kashima, Sugiyama, and Kanamori]{hido2011statistical}
Shohei Hido, Yuta Tsuboi, Hisashi Kashima, Masashi Sugiyama, and Takafumi Kanamori.
\newblock Statistical outlier detection using direct density ratio estimation.
\newblock \emph{Knowledge and Information Systems}, 26:\penalty0 309--336, 2011.

\bibitem[Kanamori et~al.(2009)Kanamori, Hido, and Sugiyama]{kanamori2009least}
Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama.
\newblock A least-squares approach to direct importance estimation.
\newblock \emph{The Journal of Machine Learning Research}, 10:\penalty0 1391--1445, 2009.

\bibitem[Kanamori et~al.(2011)Kanamori, Suzuki, and Sugiyama]{kanamori2011f}
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama.
\newblock $ f $-divergence estimation and two-sample homogeneity test under semiparametric density-ratio models.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0 (2):\penalty0 708--720, 2011.

\bibitem[Kanamori et~al.(2012)Kanamori, Suzuki, and Sugiyama]{kanamori2012statistical}
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama.
\newblock Statistical analysis of kernel-based least-squares density-ratio estimation.
\newblock \emph{Machine Learning}, 86\penalty0 (3):\penalty0 335--367, 2012.

\bibitem[Kato et~al.(2019)Kato, Teshima, and Honda]{kato2019learning}
Masahiro Kato, Takeshi Teshima, and Junya Honda.
\newblock Learning from positive and unlabeled data with a selection bias.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Keziou and Leoni-Aubin(2005)]{keziou2005test}
Amor Keziou and Samuela Leoni-Aubin.
\newblock Test of homogeneity in semiparametric two-sample density ratio models.
\newblock \emph{Comptes Rendus Math{\'e}matique}, 340\penalty0 (12):\penalty0 905--910, 2005.

\bibitem[Lehmann and Casella(2006)]{lehmann2006theory}
Erich~L Lehmann and George Casella.
\newblock \emph{Theory of point estimation}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[{L}epskii(1991)]{lepskii1991problem}
OV~{L}epskii.
\newblock On a problem of adaptive estimation in gaussian white noise.
\newblock \emph{Theory of Probability \& Its Applications}, 35\penalty0 (3):\penalty0 454--466, 1991.

\bibitem[Lu and Pereverzev(2013)]{lu2013regularization}
Shuai Lu and Sergei~V Pereverzev.
\newblock Regularization theory for ill-posed problems.
\newblock In \emph{Regularization Theory for Ill-posed Problems}. de Gruyter, 2013.

\bibitem[Lu et~al.(2020)Lu, Math{\'e}, and Pereverzev]{lu2020balancing}
Shuai Lu, Peter Math{\'e}, and Sergei~V Pereverzev.
\newblock Balancing principle in supervised learning for a general regularization scheme.
\newblock \emph{Applied and Computational Harmonic Analysis}, 48\penalty0 (1):\penalty0 123--148, 2020.

\bibitem[Marteau-Ferey et~al.(2019)Marteau-Ferey, Ostrovskii, Bach, and Rudi]{marteau2019beyond}
Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, and Alessandro Rudi.
\newblock Beyond least-squares: Fast rates for regularized empirical risk minimization through self-concordance.
\newblock In \emph{Conference on Learning Theory}, pages 2294--2340. PMLR, 2019.

\bibitem[Math{\'e}(2006)]{mathe2006lepskii}
Peter Math{\'e}.
\newblock The {L}epskii principle revisited.
\newblock \emph{Inverse problems}, 22\penalty0 (3):\penalty0 L11, 2006.

\bibitem[Menon and Ong(2016)]{menon2016linking}
Aditya Menon and Cheng~Soon Ong.
\newblock Linking losses for density ratio and class-probability estimation.
\newblock In \emph{International Conference on Machine Learning}, pages 304--313. PMLR, 2016.

\bibitem[Mohamed and Lakshminarayanan(2016)]{mohamed2016learning}
Shakir Mohamed and Balaji Lakshminarayanan.
\newblock Learning in implicit generative models.
\newblock \emph{arXiv preprint arXiv:1610.03483}, 2016.

\bibitem[M{\"u}cke(2018)]{mucke2018adaptivity}
Nicole M{\"u}cke.
\newblock Adaptivity for regularized kernel methods by {L}epskii's principle.
\newblock \emph{arXiv preprint arXiv:1804.05433}, 2018.

\bibitem[Nesterov and Nemirovskii(1994)]{nesterov1994interior}
Yurii Nesterov and Arkadii Nemirovskii.
\newblock \emph{Interior-point polynomial algorithms in convex programming}.
\newblock SIAM, 1994.

\bibitem[Nguyen et~al.(2023)Nguyen, Zellinger, and Pereverzyev]{nguyen2023regularized}
DH~Nguyen, W~Zellinger, and S~Pereverzyev.
\newblock On regularized {R}adon-{N}ikodym differentiation.
\newblock \emph{RICAM-Report 2023-13}, 2023.

\bibitem[Nguyen et~al.(2007)Nguyen, Wainwright, and Jordan]{nguyen2007estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 20, 2007.

\bibitem[Nguyen et~al.(2010)Nguyen, Wainwright, and Jordan]{nguyen2010estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael~I Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex risk minimization.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0 (11):\penalty0 5847--5861, 2010.

\bibitem[Que and Belkin(2013)]{que2013inverse}
Qichao Que and Mikhail Belkin.
\newblock Inverse density as an inverse problem: The {F}redholm equation approach.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Rudi and Rosasco(2017)]{rudi2017generalization}
Alessandro Rudi and Lorenzo Rosasco.
\newblock Generalization properties of learning with random features.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Herbrich, and Smola]{scholkopf2001generalized}
Bernhard Sch{\"o}lkopf, Ralf Herbrich, and Alex~J Smola.
\newblock A generalized representer theorem.
\newblock In \emph{International Conference on Computational Learning Theory}, pages 416--426. Springer, 2001.

\bibitem[Schuster et~al.(2020)Schuster, Mollenhauer, Klus, and Muandet]{schuster2020kernel}
Ingmar Schuster, Mattes Mollenhauer, Stefan Klus, and Krikamol Muandet.
\newblock Kernel conditional density operators.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 993--1004. PMLR, 2020.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the log-likelihood function.
\newblock \emph{Journal of Statistical Planning and Inference}, 90\penalty0 (2):\penalty0 227--244, 2000.

\bibitem[Smola et~al.(2009)Smola, Song, and Teo]{smola2009relative}
Alex Smola, Le~Song, and Choon~Hui Teo.
\newblock Relative novelty detection.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 536--543. PMLR, 2009.

\bibitem[Steinwart et~al.(2009)Steinwart, Hush, Scovel, et~al.]{steinwart2009optimal}
Ingo Steinwart, Don~R Hush, Clint Scovel, et~al.
\newblock Optimal rates for regularized least squares regression.
\newblock In \emph{Conference on Learning Theory}, pages 79--93, 2009.

\bibitem[Sugiyama et~al.(2007)Sugiyama, Krauledat, and M{\"u}ller]{sugiyama2007covariate}
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M{\"u}ller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (5), 2007.

\bibitem[Sugiyama et~al.(2012{\natexlab{a}})Sugiyama, Suzuki, and Kanamori]{sugiyama2012density}
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori.
\newblock \emph{Density ratio estimation in machine learning}.
\newblock Cambridge University Press, 2012{\natexlab{a}}.

\bibitem[Sugiyama et~al.(2012{\natexlab{b}})Sugiyama, Suzuki, and Kanamori]{sugiyama2012densitybregman}
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori.
\newblock Density-ratio matching under the bregman divergence: a unified framework of density-ratio estimation.
\newblock \emph{Annals of the Institute of Statistical Mathematics}, 64\penalty0 (5):\penalty0 1009--1044, 2012{\natexlab{b}}.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Wahba(1990)]{wahba1990spline}
Grace Wahba.
\newblock \emph{Spline models for observational data}.
\newblock SIAM, 1990.

\bibitem[Zellinger et~al.(2021)Zellinger, Shepeleva, Dinu, Eghbal-zadeh, Nguyen, Nessler, Pereverzyev, and Moser]{zellinger2021balancing}
Werner Zellinger, Natalia Shepeleva, Marius-Constantin Dinu, Hamid Eghbal-zadeh, Hoan~Duc Nguyen, Bernhard Nessler, Sergei Pereverzyev, and Bernhard~A Moser.
\newblock The balancing principle for parameter choice in distance-regularized domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhang(2002)]{zhang2002effective}
Tong Zhang.
\newblock Effective dimension and generalization of kernel learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 15, 2002.

\end{thebibliography}
