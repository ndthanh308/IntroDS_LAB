\begin{thebibliography}{10}

\bibitem{sugiyama2012density}
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori.
\newblock {\em Density ratio estimation in machine learning}.
\newblock Cambridge University Press, 2012.

\bibitem{smola2009relative}
Alex Smola, Le~Song, and Choon~Hui Teo.
\newblock Relative novelty detection.
\newblock In {\em Artificial Intelligence and Statistics}, pages 536--543.
  PMLR, 2009.

\bibitem{hido2011statistical}
Shohei Hido, Yuta Tsuboi, Hisashi Kashima, Masashi Sugiyama, and Takafumi
  Kanamori.
\newblock Statistical outlier detection using direct density ratio estimation.
\newblock {\em Knowledge and information systems}, 26:309--336, 2011.

\bibitem{keziou2005test}
Amor Keziou and Samuela Leoni-Aubin.
\newblock Test of homogeneity in semiparametric two-sample density ratio
  models.
\newblock {\em Comptes Rendus Math{\'e}matique}, 340(12):905--910, 2005.

\bibitem{kanamori2011f}
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama.
\newblock $ f $-divergence estimation and two-sample homogeneity test under
  semiparametric density-ratio models.
\newblock {\em IEEE Transactions on Information Theory}, 58(2):708--720, 2011.

\bibitem{nguyen2007estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by
  penalized convex risk minimization.
\newblock {\em Advances in neural information processing systems}, 20, 2007.

\bibitem{nguyen2010estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael~I Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock {\em IEEE Transactions on Information Theory}, 56(11):5847--5861,
  2010.

\bibitem{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock {\em Journal of statistical planning and inference}, 90(2):227--244,
  2000.

\bibitem{gizewski2022regularization}
Elke~R Gizewski, Lukas Mayer, Bernhard~A Moser, Duc~Hoan Nguyen, Sergiy
  Pereverzyev~Jr, Sergei~V Pereverzyev, Natalia Shepeleva, and Werner
  Zellinger.
\newblock On a regularization of unsupervised domain adaptation in rkhs.
\newblock {\em Applied and Computational Harmonic Analysis}, 57:201--227, 2022.

\bibitem{dinu2022aggregation}
M.-C. Dinu, M.~Holzleitner, M.~Beck, D.~H. Nguyen, A.~Huber, H.~Eghbal-zadeh,
  B.~A. Moser, S.~V. Pereverzyev, S.~Hochreiter, and W.~Zellinger.
\newblock Addressing parameter choice issues in unsupervised domain adaptation
  by aggregation.
\newblock {\em International Conference on Learning Representations}, 2023.

\bibitem{mohamed2016learning}
Shakir Mohamed and Balaji Lakshminarayanan.
\newblock Learning in implicit generative models.
\newblock {\em arXiv preprint arXiv:1610.03483}, 2016.

\bibitem{schuster2020kernel}
Ingmar Schuster, Mattes Mollenhauer, Stefan Klus, and Krikamol Muandet.
\newblock Kernel conditional density operators.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 993--1004. PMLR, 2020.

\bibitem{kato2019learning}
Masahiro Kato, Takeshi Teshima, and Junya Honda.
\newblock Learning from positive and unlabeled data with a selection bias.
\newblock In {\em International conference on learning representations}, 2019.

\bibitem{Cucker2001OnTM}
F.~Cucker and S.~Smale.
\newblock On the mathematical foundations of learning.
\newblock {\em Bulletin of the American Mathematical Society}, 39:1--49, 2001.

\bibitem{sugiyama2012densitybregman}
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori.
\newblock Density-ratio matching under the bregman divergence: a unified
  framework of density-ratio estimation.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  64(5):1009--1044, 2012.

\bibitem{kanamori2009least}
Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama.
\newblock A least-squares approach to direct importance estimation.
\newblock {\em The Journal of Machine Learning Research}, 10:1391--1445, 2009.

\bibitem{kanamori2012statistical}
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama.
\newblock Statistical analysis of kernel-based least-squares density-ratio
  estimation.
\newblock {\em Machine Learning}, 86(3):335--367, 2012.

\bibitem{sugiyama2008directest}
Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von
  B{\"u}nau, and Motoaki Kawanabe.
\newblock Direct importance estimation for covariate shift adaptation.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  60(4):699--746, 2008.

\bibitem{bickel2009discriminative}
Steffen Bickel, Michael Br{\"u}ckner, and Tobias Scheffer.
\newblock Discriminative learning under covariate shift.
\newblock {\em Journal of Machine Learning Research}, 10(9), 2009.

\bibitem{menon2016linking}
Aditya Menon and Cheng~Soon Ong.
\newblock Linking losses for density ratio and class-probability estimation.
\newblock In {\em International Conference on Machine Learning}, pages
  304--313. PMLR, 2016.

\bibitem{nguyen2023regularized}
DH~Nguyen, W~Zellinger, and S~Pereverzyev.
\newblock On regularized {R}adon-{N}ikodym differentiation.
\newblock {\em RICAM-Report 2023-13}, 2023.

\bibitem{marteau2019beyond}
Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, and Alessandro Rudi.
\newblock Beyond least-squares: Fast rates for regularized empirical risk
  minimization through self-concordance.
\newblock In {\em Conference on learning theory}, pages 2294--2340. PMLR, 2019.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{blanchard2018optimal}
Gilles Blanchard and Nicole M{\"u}cke.
\newblock Optimal rates for regularization of statistical inverse learning
  problems.
\newblock {\em Foundations of Computational Mathematics}, 18:971--1013, 2018.

\bibitem{dudley2018real}
Richard~M Dudley.
\newblock {\em Cambridge studies in advanced mathematics: {R}eal analysis and
  probability}.
\newblock 74. Cambridge University Press, 2nd edition, 2002.

\bibitem{nesterov1994interior}
Yurii Nesterov and Arkadii Nemirovskii.
\newblock {\em Interior-point polynomial algorithms in convex programming}.
\newblock SIAM, 1994.

\bibitem{10.1214/09-EJS521}
Francis Bach.
\newblock {Self-concordant analysis for logistic regression}.
\newblock {\em Electronic Journal of Statistics}, 4(none):384 -- 414, 2010.

\bibitem{steinwart2009optimal}
Ingo Steinwart, Don~R Hush, Clint Scovel, et~al.
\newblock Optimal rates for regularized least squares regression.
\newblock In {\em Conference on Learning Theory}, pages 79--93, 2009.

\bibitem{engl1996regularization}
Heinz~Werner Engl, Martin Hanke, and Andreas Neubauer.
\newblock {\em Regularization of inverse problems}, volume 375.
\newblock Springer Science \& Business Media, 1996.

\bibitem{bauer2007regularization}
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock {\em Journal of complexity}, 23(1):52--72, 2007.

\bibitem{rudi2017generalization}
Alessandro Rudi and Lorenzo Rosasco.
\newblock Generalization properties of learning with random features.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{zhang2002effective}
Tong Zhang.
\newblock Effective dimension and generalization of kernel learning.
\newblock {\em Advances in Neural Information Processing Systems}, 15, 2002.

\bibitem{caponnetto2005empirical}
Andrea Caponnetto, Lorenzo Rosasco, Ernesto De~Vito, and Alessandro Verri.
\newblock Empirical effective dimension and optimal rates for regularized least
  squares algorithm.
\newblock Technical report, Computer Science and Artificial Intelligence
  Laboratory (CSAIL), MIT, 2005.

\bibitem{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock {\em Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem{lehmann2006theory}
Erich~L Lehmann and George Casella.
\newblock {\em Theory of point estimation}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{wahba1990spline}
Grace Wahba.
\newblock {\em Spline models for observational data}.
\newblock SIAM, 1990.

\bibitem{guo2022capacity}
Xin Guo, Zheng-Chu Guo, and Lei Shi.
\newblock Capacity dependent analysis for functional online learning
  algorithms.
\newblock {\em arXiv preprint arXiv:2209.12198}, 2022.

\bibitem{bach2010self}
Francis Bach.
\newblock {Self-concordant analysis for logistic regression}.
\newblock {\em Electronic Journal of Statistics}, 4(none):384 -- 414, 2010.

\bibitem{lepskii1991problem}
OV~Lepskii.
\newblock On a problem of adaptive estimation in gaussian white noise.
\newblock {\em Theory of Probability \& Its Applications}, 35(3):454--466,
  1991.

\bibitem{goldenshluger2000adaptive}
Alexander Goldenshluger and Sergei~V Pereverzev.
\newblock Adaptive estimation of linear functionals in hilbert scales from
  indirect white noise observations.
\newblock {\em Probability Theory and Related Fields}, 118(2):169--186, 2000.

\bibitem{birge2001alternative}
Lucien Birg{\'e}.
\newblock An alternative point of view on lepski's method.
\newblock {\em Lecture Notes-Monograph Series}, pages 113--133, 2001.

\bibitem{mathe2006lepskii}
Peter Math{\'e}.
\newblock The lepskii principle revisited.
\newblock {\em Inverse problems}, 22(3):L11, 2006.

\bibitem{de2010adaptive}
Ernesto De~Vito, Sergei~V Pereverzyev, and Lorenzo Rosasco.
\newblock Adaptive kernel methods using the balancing principle.
\newblock {\em Foundations of Computational Mathematics}, 10(4):455--479, 2010.

\bibitem{mucke2018adaptivity}
Nicole M{\"u}cke.
\newblock Adaptivity for regularized kernel methods by lepskii's principle.
\newblock {\em arXiv preprint arXiv:1804.05433}, 2018.

\bibitem{blanchard2019lepskii}
Gilles Blanchard, Peter Math{\'e}, and Nicole M{\"u}cke.
\newblock Lepskii principle in supervised learning.
\newblock {\em arXiv preprint arXiv:1905.10764}, 2019.

\bibitem{lu2020balancing}
Shuai Lu, Peter Math{\'e}, and Sergei~V Pereverzev.
\newblock Balancing principle in supervised learning for a general
  regularization scheme.
\newblock {\em Applied and Computational Harmonic Analysis}, 48(1):123--148,
  2020.

\bibitem{zellinger2021balancing}
Werner Zellinger, Natalia Shepeleva, Marius-Constantin Dinu, Hamid
  Eghbal-zadeh, Hoan~Duc Nguyen, Bernhard Nessler, Sergei Pereverzyev, and
  Bernhard~A Moser.
\newblock The balancing principle for parameter choice in distance-regularized
  domain adaptation.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{caponnetto2010cross}
Andrea Caponnetto and Yuan Yao.
\newblock Cross-validation based adaptation for regularization operators in
  learning theory.
\newblock {\em Analysis and Applications}, 8(02):161--183, 2010.

\bibitem{sugiyama2007covariate}
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M{\"u}ller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock {\em Journal of Machine Learning Research}, 8(5), 2007.

\bibitem{scholkopf2001generalized}
Bernhard Sch{\"o}lkopf, Ralf Herbrich, and Alex~J Smola.
\newblock A generalized representer theorem.
\newblock In {\em International conference on computational learning theory},
  pages 416--426. Springer, 2001.

\bibitem{lu2013regularization}
Shuai Lu and Sergei~V Pereverzev.
\newblock Regularization theory for ill-posed problems.
\newblock In {\em Regularization Theory for Ill-posed Problems}. de Gruyter,
  2013.

\end{thebibliography}
