\section{Decision Programming} \label{sec:decision_programming}

Decision programming relies on influence diagrams, which are graphical representations of decision problems. In influence diagrams, nodes represent chance events, decisions and consequences. Specifically, let $G(N,A)$ be an acyclic graph formed by nodes in $N = C \cup D \cup V$, where $C$ is a subset of chance nodes, $D$ a subset of decision nodes, and $V$ a subset of value nodes. Value nodes represent consequences incurred from decisions made at nodes $D$ and chance events observed at nodes $C$. Each decision and chance node $j \in C \cup D$ can assume a state $s_j$ from a discrete and finite set of states $S_j$. For a decision node $j \in D$, $S_j$ represents the decision alternatives. For a chance node $j \in C$, $S_j$ is the set of possible outcomes. 

In the diagram, arcs represent interdependency among decisions and chance events. Set $A = \{(i,j) \mid i,j \in N\}$ contains the arcs $(i,j)$, which represent the influence between nodes $i$ and $j$. This influence is propagated in the diagram in the form of \emph{information}. That is, an arc $(i,j)$ that points to a decision node $j \in D$ indicates that the decision at $j \in D$ is made \emph{knowing} the realisation (i.e., uncertainty outcome or decision made) of state $s_i \in S_i$, with $i \in C \cup D$. On the other hand, an arc that points to a chance node $j \in C$ indicates that the realisation $s_j \in S_j$ is dependent (or conditional) on realisation $s_i \in S_i$ of node $i \in C \cup D$.

The \emph{information set} $I(j) = \{i \in N \mid (i,j) \in A\}$ comprises all immediate predecessors (or 
%
parents) 
%
of a given node $j \in N$. Despite being a less common terminology, we opt for the term ``information set'' to highlight the role of information in the modelling of the decision process. The decisions $s_j \in S_j$ made in each decision node $j \in D$ depend on their \emph{information state} $s_{I(j)} \in S_{I(j)}$, where $S_{I(j)} = \prod_{i \in I(j)} S_i$ is the set of all possible information states for node $j$. Analogously, the possible realisations $s_j \in S_j$ for each chance node $j \in C$ and their associated probabilities also depend on their information state $s_{I(j)} \in S_{I(j)}$.

Let us define $X_j \in S_j$ as the realised state at a chance node $j \in C$. For a decision node $j \in D$, let $Z_j: S_{I(j)} \to S_j$ be a mapping between each information state $s_{I(j)} \in S_{I(j)}$ and decision $s_j \in S_j$. That is, $Z_j(s_{I(j)})$ defines a local decision strategy, which represents the choice of some $s_j \in S_j $ in $j \in D$, given the information $s_{I(j)}$. Such a mapping can be represented by an indicator function $\mathbb{I}: S_{I(j)} \times S_j \to \{0,1\}$ defined so that
%
\begin{align*}
    \mathbb{I}(s_{I(j)}, s_j) = \begin{cases} 1, &\text{ if } Z_j \text{ maps } s_{I(j)} \text{ to } s_j \text{, i.e., } Z_j(s_{I(j)}) = s_j; \\ 0, &\text{ otherwise.} \end{cases}
\end{align*}
%
A (global) \emph{decision strategy} is the collection of local decision strategies in all decision nodes: $Z = (Z_j)_{j \in D}$, selected from the set of all possible strategies $\mathbb{Z}$.

A \emph{path} is a sequence of states $s = (s_i)_{i=1,\dots,n}$, with $n = |C| + |D|$ and 
\begin{equation}
    S = \{(s_i)_{i=1,\dots,n} \mid s_i \in S_i, i =1, \dots, n\}\label{eq:paths}    
\end{equation} 
is the set of all possible paths. We assume that the nodes $C \cup D$ are numbered from 1 to $n$ such that for each arc $(i,j) \in A$, $i<j$. Moreover, we say that a strategy $Z$ is compatible with a path $s \in S$ if $Z_j(s_{I(j)}) = s_j$ for all $j \in D$. We denote as $S(Z) \subseteq S$ the subset of all paths that are compatible with a strategy $Z$. 

Using the notion of information states, the conditional probability of observing a given state $s_j$ for $j \in C$ is $\mathbb{P}(X_j = s_j \mid X_{I(j)} = s_{I(j)})$. The probability associated with a path $s \in S$ being observed given a strategy $Z$ can then be expressed as
%
\begin{align}
    \mathbb{P}(s \mid Z) = \left(\prod_{j \in C}\mathbb{P}(X_j = s_j \mid X_{I(j)} = s_{I(j)})\right)\left(\prod_{j \in D} \mathbb{I}(s_{I(j)},s_j)\right) \label{eq:path_probability}.
\end{align}
%
Notice that the term $\prod_{j \in D} \mathbb{I}(s_{I(j)},s_j)$ in equation \eqref{eq:path_probability} takes value one if the strategy $Z$ is compatible with the path $s \in S$, being zero otherwise. Furthermore, notice that one can pre-calculate the probability 
%
\begin{equation}
    p(s) = \left(\prod_{j \in C}\mathbb{P}(X_j = s_j \mid X_{I(j)} = s_{I(j)})\right) \label{eq:p-def}
\end{equation}
%
of a path $s \in S$ being observed, in case a compatible strategy is chosen.

At the value node $v \in V$, a real-valued utility function $U_v : S_{I(v)} \to \mathbb{R}$ maps the information state $s_{I(v)}$ to a utility value $U_v(s_{I(v)})$. We usually assume the utility value of a path $s$ to be the sum of individual value nodes' utilities: $U(s) = \sum_{v \in V} U_v(s_{I(v)})$. The default objective is to choose a strategy $Z \in \mathbb{Z}$ maximising the expected utility, which can be expressed as
%
\begin{equation} 
\underset{Z \in \mathbb{Z}}{\text{max }} \sum_{s \in S}  \mathbb{P}(s \mid Z) U(s). \label{eq:orig-obj}
\end{equation}
%
Notice that other objective functions can also be modelled. For example, \citet{salo2022} discuss the use of the conditional value-at-risk.

To formulate this into a mathematical optimisation problem, we start by representing the local strategies $Z_j$ using binary variables $z(s_j \mid s_{I(j)})$ that take value one if $\mathbb{I}(s_{I(j)},s_j) = 1$, and 0 otherwise. We then observe that using \eqref{eq:path_probability} and \eqref{eq:p-def}, the objective function \eqref{eq:orig-obj} becomes 
%
\begin{equation*} 
\underset{z}{\text{max }} \sum_{s \in S}  p(s) U(s) \prod_{j \in D} z(s_j \mid s_{I(j)}). \label{eq:nonlin-obj}
\end{equation*}
%
This function is nonlinear and is used only for illustrating the nature of the formulations. The usefulness of this construction becomes more obvious in Section \ref{sec:formulations}. \citet{salo2022} instead replace the conditional path probability $\mathbb{P}(s \mid Z)$ in \eqref{eq:orig-obj} with a continuous decision variable $\pi(s)$, enforcing the correct behaviour of this variable using affine constraints.

With these building blocks, the problem can be formulated as a mixed-integer linear programming (MILP) model, which allows for employing off-the-shelf mathematical programming solvers. The MILP problem presented in \citet{salo2022} can be stated as \eqref{eq:dp_obj}-\eqref{eq:dp_z_bin}.
%
\begin{align}
    \underset{Z \in \mathbb{Z}}{\text{max }}  &\sum_{s \in S} \pi(s) U(s)\label{eq:dp_obj}\\
    \text{subject to }  &\sum_{s_j \in S_j} z(s_j \mid s_{I(j)}) = 1, &&\forall j \in D, s_{I(j)} \in S_{I(j)}, \label{eq:dp_z_sum}\\
    &0 \le \pi(s) \le p(s), &&\forall s \in S, \label{eq:dp_pi_lim}\\
    &\pi(s) \le z(s_j \mid s_{I(j)}), &&\forall j \in D, s \in S, \label{eq:dp_pi_upper}\\
    &\pi(s) \ge p(s) + \sum_{j \in D} z(s_j \mid s_{I(j)}) \, - | D |, &&\forall s \in S, \label{eq:dp_pi_lower} \\
    &z(s_j \mid s_{I(j)}) \in \{0,1\}, &&\forall j \in D, s_j \in S_j, s_{I(j)} \in S_{I(j)}. \label{eq:dp_z_bin}
\end{align}

Variables $\pi(s)$ are nonnegative continuous variables representing the conditional path probability in equation \eqref{eq:path_probability}. They take the value of the path probability $p(s)$ in case the selected strategy $Z$ is compatible with the path $s \in S$ and zero otherwise. Notice that this compatibility is equivalent to observing $z(s_j \mid s_{I(j)}) = 1$ for all $s_j \in S$ such that $j \in D$. 

The objective function \eqref{eq:dp_obj} defines the expected utility value, which is calculated considering only the paths that are compatible with the strategy. Constraint \eqref{eq:dp_z_sum} enforces the one-to-one nature of the mapping $\mathbb{I}(s_{I(j)}, s_j)$, represented by the $z$-variables. The correct behaviour of variables $\pi(s)$ is guaranteed by constraints \eqref{eq:dp_pi_lim}-\eqref{eq:dp_pi_lower}, which enforce that $\pi(s) = p(s)$ if $z(s_j \mid s_{I(j)}) = 1$ for all $s_j \in S$ such that $j \in D$. The term $| D |$ in \eqref{eq:dp_pi_lower} represents the cardinality of the set $D$, that is, the number of decision nodes in the diagram. Notice that the domain of $\pi(s)$ is defined in \eqref{eq:dp_pi_lim}.

