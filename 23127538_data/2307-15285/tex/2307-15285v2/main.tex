\pdfoutput=1
\documentclass{article}
%\openup6pt
\usepackage{arxiv}
\usepackage[toc,page]{appendix}

\usepackage[usenames]{xcolor}

\usepackage{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple UR typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{cite}
%\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm,algcompatible}
\usepackage[toc,page]{appendix}
\newcommand{\iu}{\mathrm{i}\mkern1mu}
\usepackage{lineno}
%\linenumbers

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\newcommand{\bibsection}{\section*{\refname}}

\numberwithin{equation}{section}

\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\lip}{Lip}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator*{\esssup}{ess\,sup}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}

%
\usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journal{Neural Networks}

%
\begin{document}
%\begin{frontmatter}

\title{Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks
}

%\titlerunning{Short form of title}        % if too long for running head

\author{Jonathan W. Siegel \\
 Department of Mathematics\\
 Texas A\&M University\\
 College Station, TX 77843 \\
 \texttt{jwsiegel@tamu.edu} \\
}

%\author[1]{Jonathan W. Siegel
%\fnref{fn1}}

%\author[2]{Jinchao Xu\fnref{fn2}}

%\fntext[fn1]{Department of Mathematics, Pennsylvania State University, State College, PA 16801}

% \author{Jonathan W. Siegel \\
%   Department of Mathematics\\
%   Pennsylvania State University\\
%   University Park, PA 16802 \\
%   \texttt{jus1949@psu.edu} \\
%   \And Jinchao Xu \\
%   Department of Mathematics\\
%   Pennsylvania State University\\
%   University Park, PA 16802 \\
%   \texttt{jxx1@psu.edu} \\
% }

%\authorrunning{Short form of author list} % if too long for running head

%\institute{Jonathan W. Siegel, Jinchao Xu \at
%              Pennsylvania State University \\
%              Mathematics Department \\
%              \email{jus1949@psu.edu, jxx1@psu.edu}          %  \\
%             \emph{Present address:} of F. Author  %  if needed
%}

%\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle

\begin{abstract}
    We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
\end{abstract}

\section{Introduction}
A (centered) zonotope in $\mathbb{R}^{d+1}$ (so that the sphere $S^d\subset \mathbb{R}^{d+1}$ is of dimension $d$) is a convex polytope $P$ which is the Minkowski sum of finitely many centered line segments, i.e. a body of the form
\begin{equation}\label{zonotope-P}
    P = \{x_1v_1 + \cdots + x_nv_n,~x_i\in[-1,1]\}
\end{equation}
for some collection of vectors $v_i\in \mathbb{R}^{d+1}$. The number $n$ is the number of summands of the zonotope. A zonoid is a convex body which is a limit of zonotopes in the Hausdorff metric. 

We consider the following problem: Given an arbitrary zonoid $Z$, how accurately can $Z$ be approximated by a polytope $P$ with $n$-summands? Here accuracy $\epsilon$ is taken to mean that $Z\subset P \subset (1+\epsilon)Z$. 

This problem has been studied by a variety of authors (see for instance \cite{bourgain1989approximation,bourgain1988distribution,linhart1989approximation,bourgain1993approximating,betke1983estimating,joos2023isoperimetric}). Of particular interest is the case when $Z = B^{d+1}$ is the Euclidean unit ball. In this case the problem has an equivalent formulation as (see \cite{betke1983estimating}): how many directions $v_1,...,v_n\in S^d$ are required to estimate the surface area of a convex body in $\mathbb{R}^{d+1}$ from the volumes of its $d$-dimensional projections orthogonal to each $v_i$?

In \cite{bourgain1989approximation}, it was shown using spherical harmonics that with $n$ summands and $Z = B^{d+1}$ the best error one can achieve is lower bounded by
\begin{equation}\label{bourgain-lower-bound}
    \epsilon(n) \geq c(d)n^{-\frac{1}{2}-\frac{3}{2d}}.
\end{equation}
When $d=2,3$, this bound was matched up to logarithmic factors in \cite{bourgain1988distribution}, specifically it was shown that for general zonoids $Z$, we have
\begin{equation}
    \epsilon(n) \leq C(d)\begin{cases}
        n^{-\frac{1}{2}-\frac{3}{2d}} \sqrt{\log(n)} & d = 2\\
        n^{-\frac{1}{2}-\frac{3}{2d}} \log(n)^{3/2} & d = 3.
    \end{cases}
\end{equation}
For larger values of $d$ the result in \cite{bourgain1988distribution} gives the worse upper bound of
\begin{equation}
    \epsilon(n) \leq C(d)n^{-\frac{1}{2}-\frac{1}{d-1}} \sqrt{\log(n)}.
\end{equation}
In \cite{bourgain1993approximating} (see also \cite{linhart1989approximation}) it was shown that these bounds can be attained using summands of equal length if $Z = B^{d+1}$. 

The picture was nearly completed in \cite{matouvsek1996improved} where it was shown that we have
\begin{equation}\label{matousek-result}
    \epsilon(n) \leq C(d)\begin{cases}
        n^{-\frac{1}{2}-\frac{3}{2d}} \sqrt{\log(n)} & d = 2,3\\
        n^{-\frac{1}{2}-\frac{3}{2d}} & d \geq 4.
    \end{cases}
\end{equation}
Moreover, it was shown that the upper bound when $d\geq 4$ can be achieved using summands of equal length for all zonoids $Z$.

In this work, we remove the logarithmic factors in \eqref{matousek-result} when $d=2,3$, i.e. we prove that
\begin{equation}\label{our-result-zonotope}
    \epsilon(n) \leq C(d) n^{-\frac{1}{2}-\frac{3}{2d}}
\end{equation}
for all $d$, and thus provide upper bounds exactly matching (up to a constant factor) the lower bound \eqref{bourgain-lower-bound}. To formulate these results, we pass to the dual setting (see \cite{bourgain1989approximation,matouvsek1996improved}). A symmetric convex body $Z$ is a zonoid iff
\begin{equation}
    \|x\|_{Z^*} := \sup_{z\in Z}x\cdot z = \int_{S^d} |x\cdot y|d\tau(y)
\end{equation}
for a positive measure $\tau$ on $S^d$. The body $Z$ is a zonotope with $n$-summands iff $\tau$ is supported on $n$ points. Since our error measure is scale invariant, we may assume that $\tau$ is a probability distribution. Given these considerations, the bound \eqref{our-result-zonotope} follows from the following result.

\begin{theorem}\label{optimal-zonoid-approximation-theorem}
    There exists a constant $C = C(d)$ such that for any probability measure $\tau$ on the sphere $S^d$, there exists a probability measure $\tau'$ on $S^d$ which is supported on $n$ points, such that
    \begin{equation}
        \sup_{x\in S^d} \left|\int_{S^d}|x\cdot y|d\tau(y) - \int_{S^d}|x\cdot y|d\tau'(y)\right| \leq Cn^{-\frac{1}{2}-\frac{3}{2d}}.
    \end{equation}
\end{theorem}
We remark that our method produces summands of unequal length (i.e. a non-uniform distribution $\tau'$) and we do not know whether this approximation can be achieved using summands of equal length (even for the ball $B^{d+1}$) when $d < 4$.

Recently, there has been renewed interest in the zonoid approximation problem due to its connection with approximation by shallow ReLU$^k$ neural networks \cite{bach2017breaking}. The ReLU$^k$ activation function (simply called ReLU when $k=1$) is defined by
\begin{equation}
    \sigma_k(x) = x_+^k :=  \begin{cases}
        x^k & x \geq 0\\
        0 & x < 0,
    \end{cases}
\end{equation}
where in the case $k=0$ we interpret $0^0=1$ (so that $\sigma_0$ is the Heaviside function). A shallow ReLU$^k$ neural network is a function on $\mathbb{R}^d$ of the form
\begin{equation}
    f_n(x) = \sum_{i=1}^n a_i\sigma_k(\omega_i\cdot x + b_i) = \sum_{i=1}^n a_i(\omega_i\cdot x + b_i)_+^k,
\end{equation}
where the $a_i\in \mathbb{R}$ are coefficients, the $\omega_i\in \mathbb{R}^d$ are directions, the $b_i\in \mathbb{R}$ are the offsets (or biases) and $n$ (the number of terms) is called the width of the network.

Shallow neural networks can be viewed as a special case of non-linear dictionary approximation. Given a Banach space $X$, let $\mathbb{D}\subset X$ be a bounded subset, i.e. $\|\mathbb{D}\| := \sup_{d\in \mathbb{D}} \|d\|_X < \infty$, which we call a dictionary. Non-linear dictionary approximation methods seek to approximate a target function $f$ by elements of the set
\begin{equation}
    \Sigma_n(\mathbb{D}) = \left\{\sum_{i=1}^n a_id_i,~a_i\in \mathbb{R},~d_i\in \mathbb{D}\right\}
\end{equation}
of $n$-term linear combinations of dictionary elements. Note that because the elements $d_i$ are not fixed, this a non-linear set of functions. It is often important to obtain some control on the coefficients $a_i$ in a non-linear dictionary expansion. For this reason, we introduce the set
\begin{equation}
    \Sigma^M_n(\mathbb{D}) = \left\{\sum_{i=1}^n a_id_i,~a_i\in \mathbb{R},~d_i\in \mathbb{D},~\sum_{i=1}^n |a_i| \leq M\right\}
\end{equation}
of non-linear dictionary expansions with $\ell^1$-bounded coefficients. 

We consider using shallow ReLU$^k$ neural networks to approximate functions and derivatives of order up to $k$ uniformly on a bounded domain $\Omega\subset \mathbb{R}^d$, so we take our Banach space $X$ to be the Sobolev space $W^k(L_\infty(\Omega))$ (see for instance \cite{evans2010partial}), with norm given by
\begin{equation}
    \|f\|_{W^{k}(L_\infty(\Omega))} := \sup_{|\alpha| \leq k} \|D^\alpha f\|_{L^\infty(\Omega)}.
\end{equation}
In our analysis, without loss of generality we will often take $\Omega = B^d := \{x:~|x| \leq 1\}$ to be the unit ball in $\mathbb{R}^d$ to simplify the presentation.

Shallow neural networks correspond to non-linear approximation with the dictionary
\begin{equation}
    \mathbb{D} = \mathbb{P}_k^d := \{\sigma_k(\omega_i\cdot x + b_i),~\omega_i\in S^{d-1},~b_i\in [a,b]\} \subset W^{k}(L_\infty(\Omega)),
\end{equation}
where by positive homogeneity we can take $\omega_i$ on the unit sphere, and the biases $b_i$ are restricted to an interval depending upon $\Omega$ to ensure both the boundedness and expressiveness of $\mathbb{P}_k^d$ (see for instance \cite{siegel2023characterization}). When $\Omega = B^d$ is the unit ball in $\mathbb{R}^d$, we can take $[a,b] = [-1,1]$ for example, since $$\sigma_k(\omega\cdot x + b) + (-1)^k\sigma_k(-\omega\cdot x - b)$$
for $\omega\in S^{d-1}$ and $b\in [-1,1]$ spans the space of polynomials of degree at most $k$ on $B^d$.
% Here we take the the underlying Banach space to be the Sobolev space $W^{k}(L_\infty(S^d))$, which we define as follows. We identify this space with the set of positively $k$-homogeneous functions $f:\mathbb{R}^{d+1}\rightarrow \mathbb{R}$, i.e. $f(\alpha x) = \alpha^k f(x)$ for $\alpha \geq 0$, with norm defined by

% The multi-index $\alpha = (\alpha_1,...,\alpha_{d+1})$ in this definition denotes a derivative in the ambient space $\mathbb{R}^{d+1}$. 

% Note that this is a slight variant on the usual definition of Sobolev spaces on the sphere, which is well suited to the dictionary $\mathbb{P}_k^d$ consisting of positively $k$-homogeneous function. We do this to avoid the technical complications of dealing with derivatives on the sphere, and instead deal only with derivatives in the ambient space.

% For functions $f\in W^{k}(L_\infty(S^d))$ we also define lower order Sobolev norms
% \begin{equation}
%     \|f\|_{W^{m}(L_\infty(S^d))} := \sup_{|\alpha| \leq m} \|D^\alpha f\|_{L^\infty(S^d)}
% \end{equation}
% for $m=0,...,k$.

A typical class of functions considered in the context of non-linear dictionary approximation is the variation space of the dictionary $\mathbb{D}$, defined as follows. Let
\begin{equation}\label{B-1-d-definition}
    B_1(\mathbb{D}) := \overline{\bigcup_{n=1}^\infty \Sigma^1_n(\mathbb{D})},
\end{equation}
denote the closed symmetric convex hull of the dictionary $\mathbb{D}$ and define the variation norm of a function $f\in X$ by
\begin{equation}
    \|f\|_{\mathcal{K}_1(\mathbb{D})} := \inf\{s > 0:~f\in sB_1(\mathbb{D})\}.
\end{equation}
This construction is also called the gauge of the set $B_1(\mathbb{D})$ (see for instance \cite{rockafellar1997convex}), and has the property that the unit ball of the $\mathcal{K}_1(\mathbb{D})$-norm is exactly the closed convex hull $B_1(\mathbb{D})$. We also write
\begin{equation}
    \mathcal{K}_1(\mathbb{D}) := \{f\in X;~\|f\|_{\mathcal{K}_1(\mathbb{D})} < \infty\}
\end{equation}
for the space of functions with finite $\mathcal{K}_1(\mathbb{D})$-norm.
The variation norm has been introduced in different forms in the literature and plays an important role in statistics, signal processing, non-linear approximation, and the theory of shallow neural networks (see for instance \cite{barron2008approximation,temlyakov2008greedy,devore1996some,devore1998nonlinear,barron1993universal,jones1992simple,siegel2020approximation,siegel2022high,ongie2019function,parhi2021banach}).

In the case corresponding to shallow ReLU$^k$ neural networks, $\mathbb{D} = \mathbb{P}_k^d$, the variation space can equivalently be defined via integral representations, which were studied for example in \cite{bach2017breaking,ma2022barron}. Specifically, we have $f\in \mathcal{K}_1(\mathbb{P}_k^d)$ iff there exists a (signed measure) $d\mu$ of bounded variation on $S^{d-1}\times [a,b]$ such that
\begin{equation}
    f(x) = \int_{S^{d-1}\times [a,b]} \sigma_k(\omega\cdot x + b)d\mu(\omega,b)
\end{equation}
pointwise almost everywhere. Moreover, the variation norm is given by
\begin{equation}
    \|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)} = \inf\left\{\int_{S^{d-1}\times [a,b]}d|\mu|(\omega,b),~f(x) = \int_{S^{d-1}\times [a,b]} \sigma_k(\omega\cdot x + b)d\mu(\omega,b)\right\},
\end{equation}
where the infimum above is taken over all measures with finite total variation giving such a representation of $f$. This is due to the fact that
\begin{equation}\label{eq-221}
    B_1(\mathbb{P}_k^d) = \left\{\int_{S^{d-1}\times [a,b]} \sigma_k(\omega\cdot x + b)d\mu(\omega,b),~\int_{S^{d-1}\times [a,b]}d|\mu|(\omega,b) \leq 1\right\},
\end{equation}
which follows from Lemma 3 in \cite{siegel2023characterization}, and an `empirical' discretization of the integral in \eqref{eq-221} using the fact that half-spaces have bounded VC-dimension \cite{vapnik1971uniform,vapnik1999nature} (note that the closure in \eqref{B-1-d-definition} is taken in the $X = W^k(L_\infty)$-norm). In fact, it follows easily from this that we get the same set $B_1(\mathbb{P}_k^d)$, and thus the same variation space $\mathcal{K}_1(\mathbb{P}_k^d)$, even if we take the closure in \eqref{B-1-d-definition} with respect to a weaker norm such as $L_2$.

%That the right hand size is contained in the left hand size follows by `empirically' discretizing the integral in \eqref{eq-221}, i.e. by sampling from the probability distribution $d|\mu|$. This results in an element in $\Sigma_n^1(\mathbb{P}_k^d)$ which converges to $f$ in the $W^{k}(L_\infty(S^d))$-norm due to the fact that half-spaces on the sphere have bounded VC-dimension \cite{vapnik1971uniform,vapnik1999nature}. The reverse inclusion follows since convergence in $W^{k}(L_\infty(S^d))$ implies convergence in $L_2(S^d)$. Then the existence of an integral representation follows from Lemma 3 in \cite{siegel2023characterization} due to the compactness of $\mathbb{P}_k^d$ in $L_2$.

One important question is how efficiently functions in the variation space $\mathcal{K}_1(\mathbb{D})$ can be approximated by non-linear dictionary expansions $\Sigma_n(\mathbb{D})$ with $n$ terms. When the space $X$ is a Hilbert space (or more generally a type-$2$ Banach space), we have the bound \cite{barron1993universal,jones1992simple,pisier1981remarques}
\begin{equation}\label{maurey-jones-barron-rate}
    \inf_{f_n\in \Sigma_n(\mathbb{D})} \|f - f_n\|_X \leq C\|f\|_{\mathcal{K}_1(\mathbb{D})}n^{-\frac{1}{2}}.
\end{equation}
The constant here depends only upon the norm of the dictionary $\|\mathbb{D}\|$ and the type-$2$ constant of the space $X$. Moreover, the norm of the coefficients $a_i$ can be controlled, so that if $f$ is in $B_1(\mathbb{D})$ (the unit ball of $\mathcal{K}_1(\mathbb{D})$), then $f_n$ can be taken in $\Sigma^1_n(\mathbb{D})$. This fact was first applied to neural network approximation by Jones and Barron \cite{barron1993universal,jones1992simple}, and forms the basis of the dimension independent approximation rates obtained for shallow neural networks.

For many dictionaries, for example the dictionaries $\mathbb{P}_k^d$ corresponding to shallow neural networks, the rate \eqref{maurey-jones-barron-rate} can be significantly improved (see for instance \cite{bach2017breaking,klusowski2018approximation,siegel2022sharp,makovoz1996random}). For instance, in the $L_2$-norm we get the rate
\begin{equation}
    \inf_{f_n\in \Sigma_n(\mathbb{P}_k^d)} \|f - f_n\|_{L_2(\Omega)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)}n^{-\frac{1}{2}-\frac{2k+1}{2d}},
\end{equation}
and this rate is optimal up to logarithmic factors if we require even mild control on the coefficients $a_i$ (for instance $|a_i| \leq C$ for a constant $C$) \cite{siegel2022sharp}.

In this work, we consider approximation rates for the dictionary $\mathbb{P}_k^d$ on the variation space $\mathcal{K}_1(\mathbb{P}_k^d)$ in the $W^{m}(L_\infty)$-norm for $m=0,...,k$, i.e. we consider uniform approximation of both $f$ and its derivatives up to order $m$. This is a much stronger error norm than the $L_2$-norm, and approximating derivatives is important for applications of shallow neural networks to scientific computing (see for instance \cite{siegel2023greedy,lu2022priori,xu2020finite}). For an arbitrary (bounded) dictionary $\mathbb{D}\subset W^{m}(L_\infty)$, no rate of approximation for $\mathcal{K}_1(\mathbb{D})$ by $\Sigma_n(\mathbb{D})$ can be obtained in general, i.e. there exist dictionaries for which the rate can be arbitrarily slow \cite{donahue1997rates}. Thus, our results rely upon the special structure of the $\mathbb{P}_k^d$ dictionary of ReLU$^k$ atoms.

This problem has previously been considered in the case $m=0$, i.e. in the $L_\infty$-norm (see for instance \cite{bach2017breaking,klusowski2018approximation,ma2022uniform,barron1992neural,cheang2000better,yukich1995sup}). In this case, when $k=0$ an approximation rate of
\begin{equation}
    \inf_{f_n\in \Sigma_n(\mathbb{P}_0^d)} \|f - f_n\|_{L_\infty(\Omega)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_0^d)}n^{-\frac{1}{2}-\frac{1}{2d}},
\end{equation}
was proved in \cite{ma2022uniform} using results from geometric discrepancy theory \cite{matouvsek1995tight}. For $k=1$, the aforementioned results on approximating zonoids by zonotopes \cite{matouvsek1996improved} were used in \cite{bach2017breaking} to get a rate of 
\begin{equation}
        \inf_{f_n\in \Sigma_n(\mathbb{P}_1^d)} \|f - f_n\|_{L_\infty(S^d)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_1^d)}\begin{cases}
            n^{-\frac{1}{2}-\frac{3}{2d}}\sqrt{\log{n}} & d = 2,3\\
            n^{-\frac{1}{2}-\frac{3}{2d}} & d \geq 4
        \end{cases}
\end{equation}
on the sphere $S^d$. Finally, when $k\geq 2$, the best known result is \cite{klusowski2018approximation}
\begin{equation}
    \inf_{f_n\in \Sigma_n(\mathbb{P}_k^d)} \|f - f_n\|_{L_\infty(\Omega)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)} n^{-\frac{1}{2}-\frac{1}{d}} \sqrt{\log{n}}.
\end{equation}
We remark that for all of these results, the coefficients of $f_n$ can be controlled. Specifically, if $f\in B_1(\mathbb{P}_k^d)$, then $f_n$ can be taken in $\Sigma_n^1(\mathbb{P}_k^d)$.

By refining the techniques of discrepancy theory used to obtain these $L_\infty$ bounds, we are able to prove the following result, which is essentially a generalization of Theorem \ref{optimal-zonoid-approximation-theorem}.
\begin{theorem}\label{shallow-network-approximation-dist-result}
    Let $k \geq 0$. For any probability distribution $\tau$ on $S^{d-1}\times [-1,1]$ there exists a probability distribution $\tau'$ supported on at most $n$ points such that for any multi-index $\alpha$ with $|\alpha| \leq k$ we have
    \begin{equation}
        \sup_{x\in B^d} \left|D_x^\alpha\left(\int_{S^d}\sigma_k(\omega\cdot x + b)d\tau(\omega,b) - \int_{S^d}\sigma_k(\omega\cdot x + b)d\tau'(\omega,b)\right)\right| \leq Cn^{-\frac{1}{2}-\frac{2(k - |\alpha|) + 1}{2d}},
    \end{equation}
    where $D_x^\alpha$ denotes the $\alpha$-th order derivative with respect to $x$. Here the constant $C = C(d,k)$ depends only upon $d$ and $k$. 
    %If $\Omega = B_1^d$ is the unit ball in $\mathbb{R}^d$ and $|a|,|b|\leq 2$, then $C = O(d^k)$ grows at most polynomially in the dimension $d$.
\end{theorem}
As a Corollary, we obtain the following approximation rate.
\begin{theorem}\label{shallow-network-approximation-theorem}
    Let $0\leq m\leq k$ and $\Omega = B^d$. Then we have the bound
    \begin{equation}
        \inf_{f_n\in \Sigma_n(\mathbb{P}_k^d)} \|f - f_n\|_{W^{m}(L_\infty(\Omega))} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)}n^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}},
    \end{equation}
    where $C = C(d,k)$ is a constant. 
    %If $\Omega = B_1^d$ is the unit ball, then $C(d,k,\Omega) = O(d^k)$ grows at most polynomially in $d$. 
    Moreover, the coefficients of $f_n$ can be controlled, so if $f\in B_1(\mathbb{P}_k^d)$, then $f_n$ can be taken in $\Sigma_n^1(\mathbb{P}_k^d)$.
\end{theorem}
 We remark that by scaling, Theorems \ref{shallow-network-approximation-dist-result} and \ref{shallow-network-approximation-theorem} can easily be extended to any bounded domain $\Omega\subset \mathbb{R}^d$. Theorem \ref{shallow-network-approximation-theorem} extends the approximation rates derived in \cite{siegel2022sharp} from the $L_2$-norm to the $L_\infty$-norm, which significantly improves upon existing results \cite{ma2022uniform,bach2017breaking,klusowski2018approximation} when $k \geq 1$. In addition, we obtain approximation rates in the $W^m(L_\infty)$-norm, which enables derivatives and function values to be uniformly approximated simultaneously. The approximation rates given in Theorem \ref{shallow-network-approximation-theorem} are an important building block in obtaining approximation rates for shallow ReLU$^k$ networks on Sobolev and Besov spaces \cite{yang2023optimal}.

Theorems \ref{optimal-zonoid-approximation-theorem} and \ref{shallow-network-approximation-dist-result} are proved using a modification of the geometric discrepancy argument used in \cite{matouvsek1996improved}, while Theorem \ref{shallow-network-approximation-theorem} is an easy Corollary of Theorem \ref{shallow-network-approximation-dist-result}. Theorem \ref{optimal-zonoid-approximation-theorem} follows essentially as a special case of Theorem \ref{shallow-network-approximation-dist-result} when $k=1$, except that the ReLU activation function is replaced by the absolute value function and the setting is changed to the sphere. For this reason, we only give the complete proof of Theorem \ref{shallow-network-approximation-dist-result}. The changes necessary to obtain Theorem \ref{optimal-zonoid-approximation-theorem} are relatively straightforward and left to the reader. We begin by collecting the necessary geometric and combinatorial facts in Section \ref{lemmas-section}. The proofs of Theorems \ref{shallow-network-approximation-dist-result} and \ref{shallow-network-approximation-theorem} are given in Section \ref{shallow-relu-network-approximation}.

\section{Geometric Lemmas}\label{lemmas-section}
In this section, we collect and prove the geometric Lemmas which are crucial to the proofs of Theorems \ref{optimal-zonoid-approximation-theorem} and \ref{shallow-network-approximation-dist-result}. In particular, in the proof of Theorem \ref{shallow-network-approximation-dist-result} we will need the following covering result.
\begin{lemma}\label{covering-set-ball-lemma}
    Let $P$ be an $N$ point subset of $S^{d-1}\times [-1,1]$ (i.e. a set of $n$ halfspaces) and $0 < \delta < 1$ be given. Then there exists a subset $\mathcal{N}\subset B^d$ of the unit ball (depending upon both $P$ and $\delta$) with $|\mathcal{N}| \leq (C/\delta)^{d}$ such that for any $x\in B^d$ there exists a $z\in \mathcal{N}$ with
    \begin{itemize}
        \item $|x - z| \leq C\delta\sqrt{d}.$
        \item $|P\cap \{(\omega,b)\in S^{d-1}\times [-1,1],~\sign{(\omega\cdot x + b)} \neq \sign{(\omega\cdot z + b)}\}| \leq \delta N.$
    \end{itemize}
    Here $C$ is an absolute constant.
\end{lemma}
A version of this Lemma on the sphere, which is required for the proof of Theorem \ref{optimal-zonoid-approximation-theorem} was proved by Matousek \cite{matouvsek1996improved}.
\begin{lemma}[Lemma 6 in \cite{matouvsek1996improved}]\label{covering-set-lemma}
    Let $P$ be an $N$-point subset of $S^d$ and $0 < \delta < 1$ be given. There exists a subset $\mathcal{N} \subset S^d$ (depending upon both $P$ and $\delta$) with $|\mathcal{N}| \leq C\delta^{-d}$ such that for any $x\in S^d$, there exists a $z\in \mathcal{N}$ with
    \begin{itemize}
        \item $|x - z| \leq \delta.$
        \item $|P\cap \{y\in S^d,~\sign{(y\cdot x)} \neq \sign{(y\cdot z)}\}| \leq \delta N.$
    \end{itemize}
    Here $C = C(d)$ is a constant independent of $N, P$ and $\delta$.
\end{lemma}
The proof of Lemma \ref{covering-set-ball-lemma} follows essentially the same ideas as the proof of Lemma \ref{covering-set-lemma} in \cite{matouvsek1996improved}. However, for completeness we give the proof here as well. The version given in Lemma \ref{covering-set-ball-lemma} explicitly tracks the dimension dependence of the constants and can be used to track the dimension dependence of the constants in Theorems \ref{optimal-zonoid-approximation-theorem} and \ref{shallow-network-approximation-dist-result}.

We begin by recalling the relevant combinatorial background (see for instance \cite{matousek1999geometric}, Chapter 5).

\begin{definition}
    A set system $(X,\mathcal{S})$ consists of a set $X$ and a collection of subsets $\mathcal{S}\subset 2^X$ of $X$.
\end{definition}
The particular set system which we will consider in the proof of Lemma \ref{covering-set-ball-lemma} is given by
\begin{equation}\label{ball-set-system}
    X = S^{d-1}\times [-1,1],~\mathcal{S} = \left\{\{(\omega,b):~\omega\cdot x + b \geq 0\},~x\in B^d\right\}.
\end{equation}
In other words, the elements are halfspaces and the sets consists of all halfspaces containing a given point $x$ in the unit ball.

Given a subset $Y\subset X$, we write $\mathcal{S}|_Y = \{Y\cap S,~S\in \mathcal{S}\}$ for the system $\mathcal{S}$ restricted to the set $Y$.
\begin{definition}[VC-dimension \cite{vapnik1971uniform}]
    A subset $Y\subset X$ is shattered by $\mathcal{S}$ if $\mathcal{S}|_Y = 2^Y$. The VC-dimension of the set system $(X,\mathcal{S})$ is the cardinality of the largest finite subset of $X$ which is shattered by $\mathcal{S}$.
\end{definition}
An important ingredient in the proof is the following bound on the VC-dimension of the set system given in \eqref{ball-set-system}.
\begin{lemma}[Lemma 3 in \cite{ma2022uniform}]\label{halfspace-VC-bound-lemma}
    The set system in \eqref{ball-set-system} has VC-dimension bounded by $d$.
\end{lemma}
Finally, we will need the following packing Lemma for set systems with bounded VC-dimension.
\begin{lemma}\label{packing-VC-dimension-lemma}
    Let $(X,\mathcal{S})$ be a set system and $\mu$ a probability measure on the set $X$. Define a distance $d_\mu$ on the collection of subsets $\mathcal{S}$ by
    \begin{equation}
        d_{\mu}(S_1,S_2) = \mu(S_1\Delta S_2).
    \end{equation}
    In other words, $d_\mu$ is the probability that a randomly chosen element from the measure $\mu$ will be in one set but not the other.

    Let $\epsilon > 0$ and suppose that $S_1,...,S_N\in \mathcal{S}$ are such that $d_{\mu}(S_i,S_j) \geq \epsilon$ for all $i\neq j$. Then, if $(X,\mathcal{S})$ has VC-dimension at most $d$, we have
    \begin{equation}
        N \leq \left(\frac{C}{\epsilon}\right)^d
    \end{equation}
    for an absolute constant $C$ (we can take for instance $C = 50$).
\end{lemma}
This was first proved by Haussler in the case where $X$ is a finite set and $\mu$ is the counting measure \cite{haussler1995sphere}, with a weaker result (losing a logarithmic factor) being obtained earlier by Dudley \cite{dudley1978central}. The generalization to arbitrary probability measures $\mu$ follows from this result in a relatively simple manner, and has been noted in the case of certain geometric set systems in \cite{matousek1991cutting}.

\begin{proof}[Proof of Lemma \ref{packing-VC-dimension-lemma}]
    Consider drawing $k$ independent random samples $x_1,...,x_k$ from the probability distribution $\mu$ with replacement. We consider the sets
    \begin{equation}
        \bar{S}_j = \{i:x_i\in S_j\} \subset \{1,...,k\}.
    \end{equation}
    It is clear that the VC-dimension of the set system $\bar{S}_1,...,\bar{S}_N$ viewed as a subsets of $\{1,...,k\}$ is also at most $d$. Applying Theorem 1 in \cite{haussler1995sphere}, we see that if each pair $\bar{S}_i$ and $\bar{S}_j$ differ in at least $\delta k$ elements for some $\delta > 0$, then
    \begin{equation}
        N \leq e(d+1)\left(\frac{2e}{\delta}\right)^d \leq \left(\frac{C}{\delta}\right)^d
    \end{equation}
    for $C = 25$ (for example).

    Finally, we observe that by choosing $k$ large enough we can guarantee that any two pairs $\bar{S}_i$ and $\bar{S}_j$ differ in at least $\delta k$ elements for $\delta = \epsilon/2$ with positive probability. Indeed, for each pair of sets the fraction of elements that they differ in is an average of Bernoulli random variables with expectation at least $\epsilon$, since $d_\mu(S_i,S_j) \geq \epsilon$. Using standard concentration inequalities combined with a union bound (for $k$ large enough, note that $N$ is fixed) completes the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{covering-set-ball-lemma}]
    Consider the set system given in \eqref{ball-set-system} and the probability measure $\mu$ defined by
    \begin{equation}\label{definition-of-mu}
        \mu = \frac{1}{2}\pi + \frac{1}{2}\pi_P,
    \end{equation}
    where $\pi$ is the uniform probability measure on $S^{d-1}\times [-1,1]$, and $\pi_P$ is the empirical measure associated to the set of halfspaces $P$, i.e.
    \begin{equation}\label{definition-of-pi-P}
        \pi_P = \frac{1}{|P|}\sum_{(\omega,b)\in P} \delta_{(\omega,b)}
    \end{equation}
    where $\delta_{(\omega,b)}$ denotes the Dirac measure at the point $(\omega,b)$. 
    
    Let $x_1,...,x_N\in B^d$ (viewed as elements of the set system $\mathcal{S}$) be a maximal set of points such that $d_{\mu}(x_i,x_j) \geq \delta / 2$. By Lemma \ref{packing-VC-dimension-lemma} and the VC-dimension bound in Lemma \ref{halfspace-VC-bound-lemma}, we have that
    \begin{equation}
        N \leq \left(\frac{2C}{\delta}\right)^d.
    \end{equation}
    Moreover, given an $z\in B^d$ there is an $x_i$ such that $d_\mu(x_i,z) < \delta / 2$ by the maximality of the set $x_1,...,x_N$. From the definition of $\mu$, this means that $d_\pi(x_i,z) < \delta$ and $d_{\pi_P}(x_i,z) < \delta$. Given the form \eqref{definition-of-pi-P} of $\pi_P$, $d_{\pi_P}(x_i,z) < \delta$ is equivalent to
    $$|P\cap \{(\omega,b)\in S^{d-1}\times [-1,1],~\sign{(\omega\cdot x_i + b)} \neq \sign{(\omega\cdot z + b)}\}| < \delta |P| = \delta N.$$
    On the other hand, $d_\mu(x_i,z) < \delta$ implies that
    \begin{equation}\label{expectation-bound-362}
        \mathbb{E}_{\omega\in S^{d-1}} \left[\mathbb{P}(x_i\cdot\omega < b <  z\cdot\omega~\text{or}~z\cdot\omega < b <  x_i\cdot\omega)\right] = \frac{1}{2}\mathbb{E}_{\omega\in S^{d-1}}|(x_i - z)\cdot \omega| < \delta,
    \end{equation}
    where the expectation denotes an average over the sphere $S^{d-1}$, and the probability is over a uniformly random $b\in [-1,1]$. It is well-known that for any fixed unit vector $w\in S^{d-1}$ we have
    \begin{equation}
        \mathbb{E}_{\omega\in S^{d-1}}|w\cdot \omega| \geq cd^{-1/2}
    \end{equation}
    for an absolute constant $c$. Together with \eqref{expectation-bound-362}, this implies that $|x_i - z| < C\delta\sqrt{d}$ as desired.
\end{proof}

\section{Approximation by Shallow ReLU$^k$ Neural Networks}\label{shallow-relu-network-approximation}
In this section, we give the proof of Theorem \ref{shallow-network-approximation-dist-result}, which 
%gives uniform approximation rates for shallow ReLU$^k$ neural networks. Theorem \ref{shallow-network-approximation-dist-result} 
follows easily from the following Proposition.
\begin{proposition}\label{term-decrease-by-a-factor-reluk-proposition}
    Fix an integer $k \geq 0$. Let $\tau$ be a probability distribution on the sphere $S^{d-1}\times [-1,1]$ which is supported on $N$ points for $N$ sufficiently large ($N\geq 6$ is sufficient). Then there exists a probability distribution $\tau'$ supported on at most $(1-c)N$ points such that for all multi-indices $\alpha$ with $|\alpha|\leq k$ we have
    \begin{equation}
        \sup_{x\in B^d} \left|D_x^\alpha\left(\int_{S^{d-1}\times [-1,1]}\sigma_k(\omega\cdot x + b)d\tau(\omega,b) - \int_{S^{d-1}\times [-1,1]}\sigma_k(\omega\cdot x + b)d\tau'(\omega,b)\right)\right| \leq CN^{-\frac{1}{2}-\frac{2(k - |\alpha|) + 1}{2d}},
    \end{equation}
    where $D_x^\alpha$ denotes the $\alpha$-th order derivative with respect to $x$.
    Here $C = C(d,k)$ and $c$ is an absolute constant.
\end{proposition}

\begin{proof}[Proof of Theorem \ref{shallow-network-approximation-dist-result}]
    We repeatedly apply Proposition \ref{term-decrease-by-a-factor-reluk-proposition} to the distribution $\tau$ until the size of the support set is at most $n/2$. By Proposition \ref{term-decrease-by-a-factor-reluk-proposition}, for any multi-index $\alpha$ the error incurred in the $\alpha$-th derivative is bounded by
    \begin{equation}
        Cn^{-\frac{1}{2}-\frac{2(k-|\alpha|)+1}{2d}}\left(\sum_{j=0}^\infty (1-c)^{j\left(\frac{1}{2}+\frac{2(k-|\alpha|)+1}{2d}\right)}\right) \lesssim n^{-\frac{1}{2}-\frac{2(k-|\alpha|) + 1}{2d}},
    \end{equation}
    since the errors in each step form a geometric series whose sum is bounded by a multiple of its largest term (which is the error made in the last step).
\end{proof}
\begin{proof}[Proof of Theorem \ref{shallow-network-approximation-theorem}]
    Suppose without loss of generality that $\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)} \leq 1$, i.e. that $f\in B_1(\mathbb{P}_k^d)$.
    
    By definition, this means that for any $\epsilon > 0$ there exist parameters $(\omega_1,b_1),...,(\omega_N,b_N)\in S^d\times [-1,1]$ and weights $a_1,...,a_N\in \mathbb{R}$ (for a sufficiently large $N$) such that
    \begin{equation}\label{f-approximation-273}
        \left\|f - \sum_{i=1}^N a_i\sigma_k(\omega_i\cdot x + b_i)\right\|_{W^{k}(L_\infty(S^d))} < \epsilon,
    \end{equation}
    and $\sum_{i=1}^N |a_i| \leq 1$. The next step is to approximate the sum in \eqref{f-approximation-273} by an element in $\Sigma_n^1(\mathbb{P}_k^d)$. To do this, we split the sum into its positive and negative parts, i.e. we write
    \begin{equation}\label{positive-negative-decomposition-277}
        \sum_{i=1}^N a_i\sigma_k(\omega_i\cdot x + b_i) = \sum_{a_i > 0} a_i\sigma_k(\omega_i\cdot x + b_i) - \sum_{a_i < 0} |a_i|\sigma_k(\omega_i\cdot x + b_i).
    \end{equation}
    By considering the positive and negative pieces separately, we essentially reduce to the case where all $a_i$ are positive. In this case, the sum can be written
    \begin{equation}
        \sum_{i=1}^N a_i\sigma_k(\omega_i\cdot x + b_i) = \int_{S^{d-1}\times [-1,1]}\sigma_k(\omega\cdot x + b)d\tau(\omega,b)
    \end{equation}
    for a probablity measure $\tau$ supported on at most $N$ points. 

    Applying Theorem \ref{shallow-network-approximation-dist-result} gives an $f_n\in \Sigma_{n/2}^1(\mathbb{P}_k^d)$ such that
    \begin{equation}
        \left\|f_n - \sum_{i=1}^N a_i\sigma_k(\omega_i\cdot x + b_i)\right\|_{W^{m}(L_\infty(S^d))} \leq Cn^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}},
    \end{equation}
    whenever $a_i \geq 0$ and $\sum_{i=1}^N a_i = 1$. Applying this to the positive and negative parts in \eqref{positive-negative-decomposition-277} and summing them gives an $f_n\in \Sigma_{n}^1(\mathbb{P}_k^d)$ such that
    \begin{equation}
        \left\|f - f_n\right\|_{W^{m}(L_\infty(S^d))} \leq Cn^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}} + \epsilon.
    \end{equation}
    Since $\epsilon > 0$ was arbitrary, this completes the proof.
\end{proof}

It remains to prove Proposition \ref{term-decrease-by-a-factor-reluk-proposition}. The proof utilizes the ideas of geometric discrepancy theory and borrows many ideas from the proof of Proposition 9 in \cite{matouvsek1996improved}. However, Proposition 9 in \cite{matouvsek1996improved} only deals with uniform distributions, and a few key modifications are required to deal with the case of `unbalanced' distributions $\tau$, which enables us to remove the logarithmic factors in all dimensions in Theorems \ref{optimal-zonoid-approximation-theorem}, \ref{shallow-network-approximation-dist-result}, and \ref{shallow-network-approximation-theorem}. In addition, dealing with the higher order smoothness of the ReLU$^k$ activation function introduces significant technical difficulties.

    We first introduce some notation. We will need to work with symmetric tensors in order to handle higher order derivatives of multivariate functions. Our tensors will be defined on the ambient space $\mathbb{R}^{d+1}$ containing the sphere $S^d$, so let $I = \{1,...,d+1\}$ denote the relevant indexing set. A (symmetric) tensor $X$ of order $m$ is an array of numbers indexed by a tuple $\textbf{i}\in I^m$, which satisfies 
    \begin{equation}
        X_\textbf{i} = X_{\pi(\textbf{i})}
    \end{equation}
    for any permutation $\pi$ of $\{1,...,m\}$. Here $\pi(\textbf{i})_{j} = \textbf{i}_{\pi(j)}$ for $j=1,...,m$. Note that vectors in $\mathbb{R}^{d+1}$ are symmetric tensors of order one. We adopt the $\ell^\infty$ norm on the space of symmetric tensors, i.e.
    \begin{equation}
        \|X\| := \max_{\textbf{i}\in I^m} |X_\textbf{i}|.
    \end{equation}  
    
    Given tensors $X$ and $Y$ of orders $m_1$ and $m_2$, their tensor product, which is a tensor of order $m_1 + m_2$, is defined in the standard way by
    \begin{equation}
        (X\otimes Y)_{\textbf{i}\textbf{j}} = X_{\textbf{i}}Y_{\textbf{j}},
    \end{equation}
    where $\textbf{i}\in I^{m_1}$, $\textbf{j}\in I^{m_2}$ and $\textbf{i}\textbf{j}$ denotes concatenation. We will also write $X^{\otimes r}$ for the $r$-fold tensor product of $X$ with itself.
    Supposing that $m_1 \geq m_2$, we define the contraction, which is a tensor of order $m_1 - m_2$ by
    \begin{equation}
        \langle X,Y\rangle_{\textbf{i}} = \sum_{\textbf{j}\in I^{m_2}} X_{\textbf{i}\textbf{j}}Y_{\textbf{j}}.
    \end{equation}
    Note that since we will be exclusively working with $\mathbb{R}^{d+1}$ with the standard inner product, to simplify the presentation we will not make the distinction between covariance and contravariance in the following. 
    
    We remark that repeated contraction can be written in terms of the tensor product in the following way
    \begin{equation}\label{contraction-tensor-product-identity}
        \langle \langle X,Y\rangle, Z\rangle = \langle X, Y\otimes Z\rangle,
    \end{equation}
    and also note the inequality
    \begin{equation}\label{contraction-l-infty-inequality}
        \left\|\langle X,Y\rangle\right\| \leq C\|X\|\|Y\|,
    \end{equation}
    where $C = C(d,k) = (d+1)^k$.
    
    Given an order $0 \leq m\leq k$, we denote the $m$-th derivative (tensor) of the ReLU$^k$ function (with $\omega$ and $b$ fixed) by
    \begin{equation}\label{sigma-k-m-definition}
        \sigma_k^{(m)}(x;\omega,b) = D_x^m[\sigma_k(\omega\cdot x + b)] = \begin{cases}
            \frac{k!}{(k-m)!}(\omega\cdot x + b)^{k-m}\omega^{\otimes m} & \omega\cdot x + b \geq 0\\
            0 & \omega\cdot x + b < 0.
        \end{cases}
    \end{equation}
    In order to deal with the additional smoothness of the activation function we will need to utilize higher-order Taylor polynomials. Given points $x_1,x_2\in S^d$, parameters $(\omega,b)\in S^{d-1}\times [-1,1]$, an order $0\leq m\leq k$, and a number of terms $0\leq r\leq k-m$, we denote by
    \begin{equation}\label{taylor-expansion-definition}
        \mathcal{T}^{m,r}_{x_1}(x_2;\omega,b) := \sum_{q=0}^r \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_1;\omega,b), (x_2 - x_1)^{\otimes q}\right\rangle
    \end{equation}
    the $r$-th order Taylor polynomials of $\sigma_k^{(m)}(x;\omega,b)$ around $x_1$ evaluated at $x_2$.

% Next, we recall a few crucial Lemmas from \cite{matouvsek1996improved} which we will need in our proof. The first Lemma shows that a finite subset of the sphere can be partitioned into pieces with `low crossing number.'
% \begin{lemma}[Lemma 5 in \cite{matouvsek1996improved}]\label{low-crossing-number lemma}
%     Let $s \geq 2$ be an integer and $P$ be an $N$-point subset of $S^d$ with $N \geq s$. Then there exist disjoint subsets $P_1,...,P_t\subset P$ with $|P_i| = s$ and $|\cup_{i=1}^t P_i| \geq N/2$ such that
%     \begin{itemize}
%         \item For every $y\in S^d$ we have
%         \begin{equation}
%             \left|\{i\in \{1,..,t\},~\text{s.t.}~\exists x_1,x_2\in P_i ~\text{with}~\sign{(y\cdot x_1)} \neq \sign{(y\cdot x_2)}\}\right| \leq CN^{1-1/d},
%         \end{equation}
%         i.e. the number of sets $P_i$ which are `crossed' by any given great circle is at most $CN^{1-1/d}$.
%         \item Each set $P_i$ has diameter at most $CN^{-1/d}$.
%     \end{itemize}
%     Here $C = C(d,s)$ is a constant independent of $N$.
% \end{lemma}

% The next Lemma shows that certain covering sets of the sphere can be constructed.

% Utilizing both of these Lemmas, we give the proof of Proposition \ref{term-decrease-by-a-factor-reluk-proposition}.
\begin{proof}[Proof of Proposition \ref{term-decrease-by-a-factor-reluk-proposition}]
    Note that since $\tau$ is supported on $N$ points 
    the integral which we are trying to approximate in Proposition \ref{term-decrease-by-a-factor-reluk-proposition} is given by
    \begin{equation}\label{sum-representation-146}
        \int_{S^{d-1}\times [-1,1]}\sigma_k(\omega\cdot x + b)d\tau(\omega,b) = \sum_{(\omega,b)\in S} a_{\omega,b}\sigma_k(\omega\cdot x + b),
    \end{equation}
    where $|S| = N$ and the coefficients $a_{\omega,b}$ satisfy $a_{\omega,b}\geq 0$ and  $\sum_{(\omega,b)\in S} a_{\omega,b} = 1$.
    
    Let $M$ denote the median of the coefficients $a_{\omega,b}$ and set
    $$
    S_- = \{(\omega,b)\in S:~a_{\omega,b} \leq M\},~~S_+ = \{(\omega,b)\in S:a_{\omega,b} > M\}.
    $$
    This gives a decomposition of the sum in \eqref{sum-representation-146} in terms of its large and small coefficients
    \begin{equation}
        \sum_{(\omega,b)\in S} a_{\omega,b}\sigma_k(\omega\cdot x + b) = \sum_{(\omega,b)\in S_-} a_{\omega,b}\sigma_k(\omega\cdot x + b) + \sum_{(\omega,b)\in S_+} a_{\omega,b}\sigma_k(\omega\cdot x + b).
    \end{equation}
    We will leave the second, i.e. the large, sum untouched and approximate the small sum by
    \begin{equation}\label{term-count-decrease-approximation}
        \sum_{(\omega,b)\in S_-} a_{\omega,b}\sigma_k(\omega\cdot x + b) \approx \sum_{(\omega,b)\in T} b_{\omega,b}\sigma_k(\omega\cdot x + b),
    \end{equation}
    where $T\subset S_-$ and $|T| \leq (1-c)|S_-|$, the new coefficients $b_{\omega,b} \geq 0$ and satisfy $$\sum_{(\omega,b)\in T}b_{\omega,b} = \sum_{(\omega,b)\in S_-} a_{\omega,b},$$ and the error of approximation satisfies (using the tensor norm we have introduced)
    \begin{equation}\label{error-bound-162}
        \sup_{x\in S^d}\left\|\sum_{(\omega,b)\in S_-} a_{\omega,b}\sigma_k^{(m)}(x;\omega,b) - \sum_{(\omega,b)\in T} b_{\omega,b}\sigma_k^{(m)}(x;\omega,b)\right\| \leq CN^{-\frac{1}{2} - \frac{2(k-m)+1}{2d}}
    \end{equation}
    for $m=0,...,k$. Setting
    \begin{equation}
        \tau' = \sum_{(\omega,b)\in T}b_{\omega,b} \delta_{\omega,b} + \sum_{(\omega,b)\in S_+} a_{\omega,b}\delta_{\omega,b}
    \end{equation}
    now completes the proof since $|S_-| \geq N/2$ (here $\delta_{\omega,b}$ denotes the Dirac delta distribution at $(\omega,b)$).

    We now turn to the heart of the proof, which is constructing an approximation \eqref{term-count-decrease-approximation} which satisfies \eqref{error-bound-162}. Note first that by construction, we have
    \begin{equation}\label{bound-on-coefficients-small-half}
        \max_{(\omega,b)\in S_-} a_{\omega,b} \leq M \leq \frac{2}{N},
    \end{equation}
    i.e. all of the coefficients in the small half are at most $2/N$. This holds since at least half (i.e. at least $N/2$) of the $a_{\omega,b}$ are at least as large as the median $M$ and $\sum_{(\omega,b)\in S} a_{\omega,b} = 1$.

    Next we construct a multi-scale covering of the ball using Lemma \ref{covering-set-ball-lemma}. For $l=1,...,L$ with $2^L > N$ we apply Lemma \ref{covering-set-ball-lemma} with $P = S_-$ and $\delta = 2^{-l}$ to obtain a sequence of sets $N_l\subset B^d$ with $|N_l| \leq (C2^{l})^d$ such that for any $x\in B^d$ there exists a $z\in N_l$ with $|x-z| \leq C2^{-l}\sqrt{d}$ and
    $$
        |\{y\in S_-:\sign{(y\cdot x)} \neq \sign{(y\cdot z)}\}| \leq 2^{-l} |S_-| \leq 2^{-l}N.
    $$
    Given a point $x\in S^d$, we denote by $\pi_l(x)$ the point $z\in N_l$ satisfying these properties (if this point is not unique we choose one arbitrarily for each $x$). 

    For each level $l=1,...,L$, each point $x\in N_l$, and each index $m=0,..,k$ we consider the function
    \begin{equation}\label{definition-of-phi-m}
        \phi_{x,l}^m(\omega,b) = \begin{cases}
            \sigma_k^{(m)}(x;\omega,b) - \mathcal{T}^{m,k-m}_{\pi_{l-1}(x)}(x;\omega,b) & l \geq 2\\
            \sigma_k^{(m)}(x;\omega,b) & l=1,
        \end{cases}
    \end{equation}
    where $\mathcal{T}^{m,k-m}_{\pi_{l-1}(x)}(x;\omega,b)$ is the $(k-m)$-th order Taylor polynomial of $\sigma_k^{(m)}(x;\omega,b)$ defined in \eqref{taylor-expansion-definition}.

    We note the following bounds on $\phi_{x,l}^m(\omega,b)$. First, if $\sign{(\omega\cdot x + b)} = \sign{(\omega\cdot \pi_{l-1}(x) + b)}$ (for $l \geq 2$), then
    \begin{equation}\label{phi-bound-zero}
        \phi_{x,l}^m(\omega,b) = 0.
    \end{equation}
    This holds since on the half space $\{x:\omega\cdot x + b\geq 0\}$ the function $\sigma_k^{(m)}(x;\omega,b)$ is a polynomial of degree $k-m$ in $x$. Thus on this half-space it is equal to its $(k-m)$-th order Taylor polynomial about any point. So if $x$ and $\pi_{l-1}(x)$ both lie in this half-space, then the difference in \eqref{definition-of-phi-m} vanishes. On the other hand, if $x$ and $\pi_{l-1}(x)$ both lie in the complement, then all terms in \eqref{definition-of-phi-m} are $0$.

    On the other hand, for any $x\in B^d$ and $(\omega,b)\in S^{d-1}\times [-1,1]$ we have the bound
    \begin{equation}\label{bound-on-phi-nonzero}
        \|\phi_{x,l}^m(\omega,b)\| \leq C2^{-l(k-m)},
    \end{equation}
    where $C = C(d,k)$. This holds since $\sigma_k^{(m)}(x;\omega,b)$ (as a function of $x$) has $(k-m)$-th order derivatives which are bounded by $C(k) = k!2^{k-m}$ for $x\in B^d$ by \eqref{sigma-k-m-definition}. Thus using Taylor's theorem the difference in \eqref{definition-of-phi-m} is bounded by
    \begin{equation}
        \left\|\sigma_k^{(m)}(x;\omega,b) - \mathcal{T}^{m,k-m}_{\pi_{l-1}(x)}(x;\omega,b)\right\| \leq C|x - \pi_{l-1}(x)|^{k-m} \leq C2^{-l(k-m)},
    \end{equation}
    for $C = C(d,k)$. When $l=1$ we also trivially obtain the bound \eqref{bound-on-phi-nonzero}.
    
    The next step is to decompose the functions $\sigma_k^{(m)}(x;\omega,b)$ with $(\omega,b)\in S_-$ in terms of the $\phi_{x,l}^m(\omega,b)$. This is captured in the following technical Lemma.
    \begin{lemma}\label{relu-k-representation-lemma}
    Let $\phi_{x,l}^m$ be defined by \eqref{definition-of-phi-m}. For $x\in S^d$ define $x_L = \pi_L(x)$ and $x_l = \pi_l(x_{l+1})$ for $l < L$. 
    
    Then for any $m=0,...,k$, $x\in S^d$ and $(\omega,b)\in S_-$ we have
    \begin{equation}\label{sigma-decomposition-reluk}
        \sigma_k^{(m)}(x;\omega,b) = \sum_{l=1}^L\phi^{m}_{x_j,j}(\omega,b) + \sum_{i=1}^{k-m}\sum_{l=1}^L  \left\langle\phi^{m+i}_{x_l,l}(\omega,b),\Gamma^{m}_{i,l}(x)\right\rangle,
    \end{equation}
    for a collection of tensors $\Gamma^{m}_{i,l}(x)$ depending upon $x$ which satisfy the bound
        \begin{equation}\label{Gamma-x-tensor-bound}
            \|\Gamma^{m}_{i,l}(x)\| \leq C2^{-il},
        \end{equation}
        for a constant $C(d,k)$.
    \end{lemma}
    The proof of this Lemma is a technical, but relatively straightforward calculation, so we postpone it until the end of this Section and complete the proof of Proposition \ref{term-decrease-by-a-factor-reluk-proposition} first.

    Utilizing the decomposition \eqref{sigma-decomposition-reluk} we write the LHS of \eqref{error-bound-162} as
    \begin{equation}
    \begin{split}
        \sup_{x\in S^d}&\left\|\sum_{l=1}^L\left[\sum_{(\omega,b)\in S_-} a_{\omega,b}\phi^{m}_{x_l,l}(\omega,b) - \sum_{(\omega,b)\in T} b_{\omega,b}c\phi^{m}_{x_l,l}(\omega,b)\right]\right.\\ &+ \left.\sum_{l=1}^L\sum_{i=1}^{k-m}\left\langle\sum_{(\omega,b)\in S_-} a_{\omega,b}\phi^{m+i}_{x_l,l}(\omega,b) - \sum_{(\omega,b)\in T} b_{\omega,b}\phi^{m+i}_{x_l,l}(\omega,b), \Gamma^{m}_{i,l}(x)\right\rangle\right\|.
    \end{split}
    \end{equation}
    Utilizing the triangle inequality, the bound \eqref{contraction-l-infty-inequality}, and the bound \eqref{Gamma-x-tensor-bound}, we see that it suffices to find the subset $T\subset S_-$ with $|T| \leq (1-c)|S_-|$, and new coefficients $b_{\omega,b}\geq 0$ satisfying $\sum_{(\omega,b)\in T}b_{\omega,b} = \sum_{(\omega,b)\in S_-} a_{\omega,b}$ such that for $m=0,...,k$ we have
    \begin{equation}\label{error-decomposition-241}
        \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il}\sup_{x\in N_l}\left\|\sum_{(\omega,b)\in S_-} a_{\omega,b}\phi^{m+i}_{x,l}(\omega,b) - \sum_{(\omega,b)\in T} b_{\omega,b}\phi^{m+i}_{x,l}(\omega,b)\right\| \leq CN^{-\frac{1}{2} - \frac{2(k-m) + 1}{2d}}
    \end{equation}
    for a constant $C = C(d,k)$.

    To find this set $T$ and new coefficients $b_{\omega,b}$, we divide the set $P = S_-$ into disjoint subsets $P_1,..,P_t$ of size $3$ with
    \begin{equation}\label{number-of-P-lower-bound}
        \left|\bigcup_{i=1}^t P_i\right| \geq |S_-|/2.
    \end{equation}
    (Note that here we need $|S_-| \geq 3$ which follows from $N \geq 6$.)
    We denote the three elements of each set $P_j$ by
    $$
        P_j = \{u_j,v_j,w_j\},
    $$
    which are ordered so that the coefficients satisfy $0 \leq a_{u_j}\leq a_{v_j}\leq a_{w_j}$. Note that  $S_-$ contains halfspaces and so each of the elements $u_j,v_j,w_j$ consist of an $(\omega,b)$ tuple.
    
    Based upon the partition $P_1,...,P_t$, we will use a modification of the partial coloring argument given in \cite{matouvsek1996improved} (the idea is originally due to Spencer \cite{spencer1985six} and Beck \cite{beck1984some}). The main difference is in how we use a partial coloring to reduce the number of terms in the sum over $S_-$.

    Given a `partial coloring' $\chi:\{1,...,t\}\rightarrow \{-1,0,1\}$, we transform the sum $\sum_{(\omega,b)\in S_-} a_{\omega,b} \sigma_k(\omega\cdot x + b)$ in the following way. 
    If $\chi(j) = 1$, we remove the term corresponding to $u_j$, double the coefficient $a_{v_j}$ of the term corresponding to $v_j$, and add the difference $a_{u_j} - a_{v_j}$ to the coefficient $a_{w_j}$ of the term corresponding to $w_j$. If $\chi(j) = -1$, we do the same but reverse the roles of $u_j$ and $v_j$. 
    
    This results in a transformed sum $\sum_{(\omega,b)\in T} b_{\omega,b} \sigma_k(\omega\cdot x + b)$ over a set $T\subset S_-$ and with coefficients $b_{\omega,b}$ for $(\omega,b)\in T$ described as follows. Let
    \begin{equation}
        R_j = \begin{cases}
            \emptyset & \chi(j) = 0\\
            \{u_j\} & \chi(j) = 1\\
            \{v_j\} & \chi(j) = -1,
        \end{cases}
    \end{equation}
    denote the removed set for each $P_j$.
    Then the set $T$ is given by
    \begin{equation}
        T = S_-\setminus \left(\bigcup_{j=1}^t R_j\right),
    \end{equation}
    and for $(\omega,b)\in T$ the coefficients $b_{\omega,b}$ are given by
    \begin{equation}
        b_{\omega,b} = \begin{cases}
            a_{\omega,b} & (\omega,b)\notin \bigcup_{j}P_j\\
            (1+\chi(j))a_{\omega,b} & (\omega,b) = v_j\\
            (1-\chi(j))a_{\omega,b} & (\omega,b) = u_j\\
            a_{\omega,b} + \chi(j)(a_{u_j} - a_{v_j}) & (\omega,b) = w_j.
        \end{cases}
    \end{equation}
    We have constructed this transformation so that $$\sum_{(\omega,b)\in T} b_{\omega,b} = \sum_{(\omega,b)\in S_-} a_{\omega,b},$$
    the $b_{\omega,b} \geq 0$ since $w_j$ has the largest coefficient among the halfspaces in $P_j$, and for any $x\in S^d$ the error in the $m$-th derivative is given by
    \begin{equation}
    \begin{split}
        \sum_{(\omega,b)\in S_-} a_{\omega,b}\sigma_k^{(m)}(x;\omega,b)& - \sum_{(\omega,b)\in T} b_{\omega,b}\sigma_k^{(m)}(x;\omega,b) =\\
        &\sum_{j=1}^t \chi(j)\left[-a_{u_j}\sigma_k^{(m)}(x;u_j) + a_{v_j}\sigma_k^{(m)}(x;v_j) + (a_{u_j} - a_{v_j})\sigma_k^{(m)}(x;w_j)\right].
    \end{split}
    \end{equation}
    Using the linearity of the derivative and the definition of the Taylor polynomial \eqref{taylor-expansion-definition}, this implies that for any $x\in S^d$, any level $l = 1,...,L$, and any $m=0,...,k$ we have
    \begin{equation}\label{error-formula-306}
        \sum_{(\omega,b)\in S_-} a_{\omega,b} \phi_{x,l}^m(\omega,b) - \sum_{(\omega,b)\in T} b_{\omega,b}\phi_{x,l}^m(\omega,b) = \sum_{j=1}^t \chi(j) \Psi^m_{x,l,j},
    \end{equation}
    where we have defined
    \begin{equation}\label{definition-of-psi}
    \begin{split}
        \Psi^m_{x,l,j} = -a_{u_j}\phi^m_{x,l}(u_j) + a_{v_j}\phi^m_{x,l}(v_j) + (a_{u_j} - a_{v_j})\phi^m_{x,l}(w_j).
    \end{split}
    \end{equation}
    Further, for any index $j$ such that $\chi(j) \neq 0$, we have eliminated one term (either $u_j$ or $v_j$) from the sum. Thus
    \begin{equation}\label{T-decrease-equation-317}
        |T| = |S| - |\{j:~\chi(j) \neq 0\}|.
    \end{equation}    
    
    We proceed to find a partial coloring $\chi:\{1,...,t\}\rightarrow \{-1,0,1\}$ with a positive fraction of non-zero entries, i.e. with $|\{j:~\chi(j) \neq 0\}| \geq ct$, such that for $m=0,...,k$
    \begin{equation}\label{partial-coloring-bound}
        \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il}\sup_{x\in N_l} \left\|\sum_{j=1}^t\chi(j)\Psi^{m+i}_{x,l,j}\right\| \leq CN^{-\frac{1}{2} - \frac{2(k-m)+1}{2d}},
    \end{equation}
    for a constant $C = C(d,k)$.

    By \eqref{error-formula-306} this will guarantee that the LHS in \eqref{error-decomposition-241} is sufficiently small, and by \eqref{T-decrease-equation-317} this will guarantee that the set $T$ is small enough, since by \eqref{number-of-P-lower-bound}
    \begin{equation}
        |T| = |S| - |\{j:~\chi(j) \neq 0\}| \leq |S| - ct \leq \left(1 - \frac{c}{6}\right)|S_-|.
    \end{equation}
    The existence of such a partial coloring $\chi$ follows from a well-known technique in discrepancy theory called the partial coloring method.

    Given a (total) coloring $\epsilon:\{1,...,t\}\rightarrow \{\pm 1\}$ we consider the quantities
    \begin{equation}\label{definition-of-E}
        E^m_{x,l}(\epsilon) := \sum_{j=1}^t\epsilon(j)\Psi^m_{x,l,j}
    \end{equation}
    for each $x\in N_l$. We would like to find a coloring $\epsilon$ such that $\|E^m_{x,l}(\epsilon)\| \leq \Delta^m_l$ for all $l=1,...,L$, $m=0,...,k$ and $x\in N_l$, where the $\Delta^m_l$ are suitable parameters chosen so that
    \begin{equation}\label{condition-on-delta-k}
        \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l \leq CN^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}},
    \end{equation}
    for $m=0,...,k$.
    
    One strategy would be to choose $\epsilon$ uniformly at random, bound the tail of the random variable $E^m_{x,l}(\epsilon)$, and use a union bound over $x\in N_l$. Unfortunately, this strategy will lose a factor $\sqrt{\log{N}}$. The ingenious method to get around this, due to Spencer \cite{spencer1985six} and Beck \cite{beck1984some}, is to show that instead there exist \textit{two} colorings $\epsilon_1$ and $\epsilon_2$ such that $\|E^m_{x,l}(\epsilon_1) - E^m_{x,l}(\epsilon_2)\| \leq \Delta^m_l$ for all $l=1,...,L$, $m=0,...,k$, and $x\in N_l$, and such that $\epsilon_1$ and $\epsilon_2$ differ in many indices, i.e.
    \begin{equation}
        |\{j:~\epsilon_1(j)\neq \epsilon_2(j)\}| \geq ct
    \end{equation}
    for an absolute constant $c$. Then $\chi = \frac{1}{2}(\epsilon_1 - \epsilon_2)$ gives the desired partial coloring.

    We will prove the existence of these two colorings $\epsilon_1$ and $\epsilon_2$ for suitably chosen parameters $\Delta^m_l$ satisfying \eqref{condition-on-delta-k}. To help organize this calculation, it is convenient to introduce the notion of the entropy of a discrete distribution (see for instance \cite{matousek1999geometric,alon2016probabilistic}). (Note that for simplicity all of the logarithms in the following are taken with base $2$.)
\begin{definition}
    Let $X$ be a discrete random variable, i.e. the range of $X$ is a countable set $\Lambda$. The entropy of $X$ is defined by
    \begin{equation}
        H(X) = -\sum_{\lambda\in \Lambda} p_\lambda\log(p_\lambda),
    \end{equation}
    where $p_\lambda = \mathbb{P}(X = \lambda)$ is the probability of the outcome $\lambda$.
\end{definition}
One important property of the entropy we will use is subadditivity, i.e. if $X = (X_1,...,X_r)$, then
\begin{equation}\label{subadditivity-entropy}
    H(X) \leq \sum_{j=1}^r H(X_j),
\end{equation}
where we have equality in the above bound when the components $X_j$ of $X$ are independent.

An important component of the calculation is the following Lemma from \cite{matouvsek1996improved} (see also \cite{matouvsek1996discrepancy,alon2016probabilistic}).
\begin{lemma}[Lemma 11 in \cite{matouvsek1996improved}]\label{entropy-partial-coloring-lemma}
    Let $\epsilon:\{1,...,t\}\rightarrow \{\pm 1\}$ be a uniformly random coloring. Let $b$ be a function of $\epsilon$ and suppose that the entropy satisfies $H(b(\epsilon)) \leq t/5$. Then there exist two colorings $\epsilon_1,\epsilon_2$ differing in at least $t/4$ components such that $b(\epsilon_1) = b(\epsilon_2)$.
\end{lemma}
We utilize this lemma in the following way. Take each entry of the (tensor-valued) random variable $E^m_{x,l}(\epsilon)$ defined in \eqref{definition-of-E} and round it to the nearest multiple of the (still undetermined) parameter $\Delta^m_l$. This results in a random variable
\begin{equation}
    b^m_{x,l}(\epsilon) = [(\Delta_l^m)^{-1}E^m_{x,l}(\epsilon)],
\end{equation}
where $[\cdot]$ denote the (component-wise) nearest integer function. Note that if $b^m_{x,l}(\epsilon_1) = b^m_{x,l}(\epsilon_2)$, then it follows that $\|E^m_{x,l}(\epsilon_1) - E^m_{x,l}(\epsilon_2)\| \leq \Delta^m_l$. Applying Lemma \ref{entropy-partial-coloring-lemma} and the subadditivity of the entropy \eqref{subadditivity-entropy}, we see that if
\begin{equation}\label{entropy-sum-bound-354}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq t/5
\end{equation}
for an appropriate choice of $\Delta^m_l$ satisfying \eqref{condition-on-delta-k}, then there exist two colorings $\epsilon_1$ and $\epsilon_2$ satisfying the desired condition with $c = 1/4$.

It remains to choose the parameters $\Delta^m_l$ satisfying \eqref{condition-on-delta-k} and to bound the sum in \eqref{entropy-sum-bound-354}. For this, we utilize the following Lemma from \cite{matouvsek1996improved}, which bounds the entropy of a `rounded' random variable in terms of the tails of the underyling real-valued random variable.
\begin{lemma}[Lemma 11 in \cite{matouvsek1996improved}]\label{entropy-tail-bound-lemma}
    Let $E$ be a real valued random variable satisfying the tail estimates
    \begin{equation}
        \mathbb{P}(E \geq \alpha M) \leq e^{-\alpha^2/2},~ \mathbb{P}(E \leq -\alpha M) \leq e^{-\alpha^2/2},
    \end{equation}
    for some parameter $M$. Let $b(E)$ denote the random variable obtained by rounding $E$ to the nearest multiple of $\Delta = 2\lambda M$. Then the entropy of $b$ satisfies
    \begin{equation}
        H(b) \leq G(\lambda) := C_0\begin{cases}
            e^{-\lambda^2/9} & \lambda \geq 10\\
            1 & .1 < \lambda < 10\\
            -\log(\lambda) & \lambda \leq .1
        \end{cases}
    \end{equation}
    for an absolute constant $C_0$.
\end{lemma}

To apply this Lemma, we bound the tails of the random variables $E^m_{x,l}(\epsilon)$. This follows using Bernstein's inequality as in \cite{matouvsek1996improved}. Fix an $l\geq 2$ and an $x\in N_l$. We call an index $j$ `good' if
\begin{equation}\label{good-index-definition-zonotope}
    \sign(\omega\cdot x + b) = \sign(\omega\cdot \pi_{l-1}(x) = b)
\end{equation}
for $(\omega,b) = u_j,v_j,$ and $w_j$,
and `bad' otherwise. Using \eqref{phi-bound-zero} we see that for the good indices we have
\begin{equation}\label{good-index-bound}
    \Psi^m_{x,l,j} = 0.
\end{equation}

For the bad indices, we utilize \ref{bound-on-phi-nonzero} to get
\begin{equation}\label{bad-indices-bound}
    \|\Psi^m_{x,l,j}\| \leq C2^{-(k-m)l}N^{-1},
\end{equation}
since $a_{u_j},a_{v_j} \leq 2/N$ by \eqref{bound-on-coefficients-small-half}. Next, we bound the number of bad indices. An index is bad if
\begin{equation}\label{eq-391}
    \sign(\omega\cdot x + b) \neq \sign(\omega\cdot \pi_{l-1}(x) + b),
\end{equation}
for $(\omega,b) = u_j,v_j$ or $w_j$. From the construction of $N_{l-1}$ using Lemma \ref{covering-set-lemma}, the number of $(\omega,b)$ for which \eqref{eq-391} occurs (and thus the number of bad indices) is bounded by $2^{-l}|S_-| \leq C2^{-l}N$.

Thus, Bernstein's inequality (applied only to the bad indices) gives the following bound on the components of the random variable $E^m_{x,l}$,
\begin{equation}\label{tail-bound-estimate-466}
    \mathbb{P}\left((E^m_{x,l})_\textbf{i} \geq \alpha M_l^m \right) \leq e^{-\alpha^2/2}
\end{equation}
for all $\textbf{i}\in I^m$, where
\begin{equation}
    M_l^m = C2^{-\left(k-m+\frac{1}{2}\right)l}N^{-1/2},
\end{equation}
for a constant $C = C(d,k)$. The same bound holds also for the negative tails. 

The proof is completed via the following calculation (see \cite{matouvsek1996improved,matouvsek1996discrepancy} for similar calculations). We let $\alpha,\beta > 0$ be parameters to be specified in a moment and define a function
\begin{equation}
    \Lambda_{\alpha,\beta}(x) = \begin{cases}
        2^{\alpha x} & x\geq 0\\
        2^{\beta x} & x\leq 0.
    \end{cases}
\end{equation}
Let $\kappa > 0$ be another parameter and set $\tau$ to be the largest integer satisfying $2^{d\tau} \leq \kappa N$. We set the discretization parameter to be
\begin{equation}\label{equation-426}
    \Delta^m_l = 2M_l^m\Lambda_{\alpha,\beta}(l - \tau).
\end{equation}

Fix an $m$ with $0\leq m\leq k$. We calculate 
\begin{equation}
\begin{split}
     \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l &\leq CN^{-1/2}\sum_{i=0}^{k-m} \sum_{l=-\infty}^\infty 2^{-il}2^{-\left(k-m-i+\frac{1}{2}\right)l} \Lambda_{\alpha,\beta}(l - \tau)\\
     &\leq CN^{-1/2} \sum_{l=-\infty}^\infty 2^{-\left(k-m+\frac{1}{2}\right)l} \Lambda_{\alpha,\beta}(l - \tau),
\end{split}
\end{equation}
since all of the terms in the sum over $i$ are the same.
Making a change of variables in the last sum, we get
\begin{equation}
    \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l \leq CN^{-1/2}2^{-\left(k-m+\frac{1}{2}\right)\tau}\sum_{l=-\infty}^\infty 2^{-\left(k-m+\frac{1}{2}\right)l} \Lambda_{\alpha,\beta}(l). 
\end{equation}
If we now choose $\alpha$ and $\beta$ such that the above sum over $l$ converges (this will happen as long as $\alpha < k-m+\frac{1}{2} < \beta$), then we get
\begin{equation}
     \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l \leq CN^{-\frac{1}{2} - \frac{2(k-m)+1}{2d}},
\end{equation}
since by construction $2^\tau \geq (1/2)(\kappa N)^{1/d}$ (note the constant $C$ depends upon the choice of $\kappa$ which will be made shortly). This verifies \eqref{condition-on-delta-k}.

To verify the entropy condition, we calculate
\begin{equation}\label{G-entropy-bound}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l}\sum_{\textbf{i}\in I^m} H(b^m_{x,l}(\epsilon)_{\textbf{i}})
\end{equation}
using subadditivity of the entropy. We now use Lemma \ref{entropy-tail-bound-lemma} combined with the tail bound estimate \eqref{tail-bound-estimate-466} to get
\begin{equation}
    H(b^m_{x,l}(\epsilon)_{\textbf{i}}) \leq G(\Delta_l^m/(2M_l^m)) = G(\Lambda_{\alpha,\beta}(l-\tau)).
\end{equation}
Using that $|I^m| \leq C = C(d,k)$ and $|N_l| \leq C2^{dl}$, we get that
\begin{equation}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq C\sum_{l=1}^L 2^{dl}G(\Lambda_{\alpha,\beta}(l-\tau)) \leq C\sum_{l=\infty}^\infty 2^{dl}G(\Lambda_{\alpha,\beta}(l-\tau)).
\end{equation}
Finally, making another change of variables, we get
\begin{equation}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq C2^{d\tau}\sum_{l=\infty}^\infty 2^{dl}G(\Lambda_{\alpha,\beta}(l)) \leq C\kappa N,
\end{equation}
since it is easy to verify that the above sum over $l$ converges for any $\alpha,\beta > 0$. Choosing $\kappa$ sufficiently small so that $C\kappa \leq 1/60$ will guarantee that the condition \eqref{entropy-sum-bound-354} is satisfied (since $t \geq N/12$ by \eqref{number-of-P-lower-bound}) and this completes the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{relu-k-representation-lemma}]
        We first prove that for $m = 0,...,k$, $(\omega,b)\in S^{d-1}\times [-1,1]$ and $l=1,...,L$ we have
        \begin{equation}\label{sigma-l-decomposition-reluk}
        \sigma_k^{(m)}(x_l;\omega,b) = \sum_{j=1}^l\phi_{x_j,j}^m(\omega,b) + \sum_{i=1}^{k-m}\sum_{j=1}^{l-1}\left\langle \phi_{x_j,j}^{m+i}(\omega,b), \Gamma_{i,j}^{m,l}(x)\right\rangle,
    \end{equation}
    where the tensors $\Gamma_{i,j}^{m,l}(x)$ satisfy the bound
    \begin{equation}\label{bound-on-gamma-tensors}
        \left\|\Gamma_{i,j}^{m,l}(x)\right\| \leq C2^{-ij},
    \end{equation}
    for a constant $C = C(d,k)$. 

        We prove this by (reverse) induction on $m$. Note if $m=k$ the equation \eqref{sigma-l-decomposition-reluk} holds since the definition of $\phi_{x,l}^{(m)}$ in \eqref{definition-of-phi-m} becomes 
        \begin{equation}
        \phi_{x,l}^m(\omega,b) = \begin{cases}
            \sigma_k^{(m)}(x;\omega,b) - \sigma_k^{(m)}(\pi_{l-1}(x);\omega,b) & l \geq 2\\
            \sigma_k^{(m)}(x;\omega,b) & l=1,
        \end{cases}
        \end{equation}
        Let $0\leq m \leq k$ and suppose that \eqref{sigma-l-decomposition-reluk} holds for $m+1,...,k$. We will show that it also holds for $m$. Expanding out the Taylor polynomial in the definition of $\phi_{x,l}^m$ for $x = x_l$ we see that
        \begin{equation}
        \begin{split}
            \sigma_k^{(m)}(x_l;\omega,b) &= \phi_{x_l,l}^m(\omega,b) + \mathcal{T}^{m,k-m}_{x_{l-1}}(x_l;\omega,b)\\ &= \phi_{x_l,l}^m(\omega,b) + \sum_{q=0}^{k-m} \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_{l-1};\omega,b), (x_l - x_{l-1})^{\otimes q}\right\rangle\\
            &= \phi_{x_l,l}^m(\omega,b) + \sigma_k^{(m)}(x_{l-1};\omega,b) + \sum_{q=1}^{k-m} \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_{l-1};\omega,b), (x_l - x_{l-1})^{\otimes q}\right\rangle.
        \end{split}            
        \end{equation}
        Applying this expansion recursively to $\sigma_k^{(m)}(x_{l-1};\omega,b)$, we get
        \begin{equation}
        \begin{split}
            \sigma_k^{(m)}(x_l;\omega,b) = \sum_{j=1}^l \phi_{x_j,j}^m(\omega,b) + \sum_{p=1}^{l-1}\sum_{q=1}^{k-m}\frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_{p};\omega,b), (x_{p+1} - x_{p})^{\otimes q}\right\rangle.
        \end{split}
        \end{equation}
        Now we use the inductive assumption to expand $\sigma_k^{(m+q)}(x_{p};\omega,b)$ using \eqref{sigma-l-decomposition-reluk} and apply the identity \eqref{contraction-tensor-product-identity} to get
        \begin{equation}
        \begin{split}
            \sigma_k^{(m)}(x_l;\omega,b) = \sum_{j=1}^l \phi_{x_j,j}^m(\omega,b) &+ \sum_{p=1}^{l-1}\sum_{q=1}^{k-m}\frac{1}{q!}\sum_{j=1}^{p}\left\langle \phi_{x_j,j}^{m+q}(\omega,b), (x_{p+1} - x_{p})^{\otimes q}\right\rangle\\
            & + \sum_{p=1}^{l-1}\sum_{q=1}^{k-m}\frac{1}{q!}\sum_{i'=1}^{k-m-q}\sum_{j=1}^{p-1} \left\langle \phi_{x_j,j}^{m+q+i'}(\omega,b),\Gamma_{i',j}^{m+q,l}(x) \otimes (x_{p+1} - x_p)^{\otimes q}\right\rangle.
        \end{split}
        \end{equation}
        Rearranging this sum, we obtain
        \begin{equation}
            \sigma_k^{(m)}(x_l;\omega,b) = \sum_{j=1}^l\phi_{x_j,j}^m(\omega,b) + \sum_{i=1}^{k-m}\sum_{j=1}^{l-1}\left\langle \phi_{x_j,j}^{m+i}(\omega,b), \Gamma_{i,j}^{m,l}(x)\right\rangle,
        \end{equation}
        where the tensors $\Gamma_{i,j}^{m,l}(x)$ are defined recursively by
        \begin{equation}
            \Gamma_{i,j}^{m,l}(x) = \frac{1}{i!}\sum_{p=j}^{l-1} (x_{p+1} - x_{p})^{\otimes i} + \sum_{p=j}^{l-1} \sum_{q=1}^{i-1}\frac{1}{q!} \Gamma_{i-q,j}^{m+q,l}(x)\otimes (x_{p+1} - x_{p})^{\otimes q}.
        \end{equation}
        Finally, we bound the norm $\|\Gamma_{i,j}^{m,l}(x)\|$. By construction, the points $x_p$ satisfy $|x_{p+1} - x_p| \leq C2^{-p}$ for a dimension dependent constant $C = C(d)$ (in the Euclidean norm which bounds the $\ell^\infty$-norm). This gives the bound
        \begin{equation}
            \|\Gamma_{i,j}^{m,l}(x)\| \leq C\left(\frac{1}{i!}\sum_{p=j}^{l-1}2^{-pj} + \sum_{p=j}^{l-1} \sum_{q=1}^{i-1}\frac{1}{q!} 2^{-pq}\|\Gamma_{i-q,j}^{m+q,l}\|\right).
        \end{equation}
        Utilizing the inductive assumption to bound $\|\Gamma_{i-q,j}^{m+q,l}\|$ we get
        \begin{equation}
            \begin{split}
                \|\Gamma_{i,j}^{m,l}\| &\leq C\left(\frac{1}{i!}\sum_{p=j}^{l-1}2^{-pj} + \sum_{p=j}^{l-1} \sum_{q=1}^{i-1}\frac{1}{q!} 2^{-pq}2^{-(i-q)j}\right)\\
                &\leq C\left(\frac{1}{i!}\sum_{p=j}^{\infty}2^{-pj} + 2^{-ij}\sum_{p=j}^{\infty} \sum_{q=1}^{\infty}\frac{1}{q!} 2^{-q(p-j)}\right)\leq C2^{-ij},
            \end{split}
        \end{equation}
        for a potentially different constant $C$. However, the induction is completed after a finite number (namely $k+1$) steps. Thus the constant $C = C(d,k)$ can be taken uniform in $m$. This proves \eqref{sigma-l-decomposition-reluk}.

        To prove \eqref{sigma-decomposition-reluk}, we write
        \begin{equation}\label{decomposition-x-610}
        \begin{split}
            \sigma_k^{(m)}(x;\omega,b) &= \left[\sigma_k^{(m)}(x;\omega,b) - \mathcal{T}^{m,k-m}_{x_L}(x;\omega,b)\right] + \mathcal{T}^{m,k-m}_{x_L}(x;\omega,b)\\ 
            &= \left[\sigma_k^{(m)}(x;\omega,b) - \mathcal{T}^{m,k-m}_{x_L}(x;\omega,b)\right] + \sum_{q=0}^{k-m} \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_L;\omega,b), (x - x_L)^{\otimes q}\right\rangle.
        \end{split}
        \end{equation}
        We claim that if $\omega,b\in S_-$, then the first term
        \begin{equation}\label{equation-364-difference-0}
        \sigma_k^{(m)}(x;\omega,b) - \mathcal{T}^{m,k-m}_{x_L}(x;\omega,b) = 0.
        \end{equation}
        This follows since by construction using Lemma \ref{covering-set-ball-lemma}, we have the bound
        \begin{equation}
            |S_-\cap \{(\omega,b)\in S^{d-1}\times [-1,1],~\sign{(\omega\cdot x + b)} \neq \sign{(\omega\cdot \pi_L(x) + b)}\}| \leq 2^{-L} |S_-| < 1,
        \end{equation}
        since $2^{-L} < N^{-1}$. Thus for all $(\omega,b)\in S_-$ and $x\in S^d$, we have $\sign{(\omega\cdot x + b)} = \sign{(\omega\cdot \pi_L(x) + b)}$, and the argument following \eqref{phi-bound-zero} implies that the difference in \eqref{equation-364-difference-0} vanishes. 
        
        Next, we expand each term in the sum in \eqref{decomposition-x-610} using \eqref{sigma-l-decomposition-reluk} with $l = L$ to get (using again the identity \eqref{contraction-tensor-product-identity})
        \begin{equation}
            \sigma_k^{(m)}(x;\omega,b) = \sum_{q=0}^{k-m}\frac{1}{q!}\sum_{j=1}^L\left\langle\phi_{x_j,j}^{m+q}(\omega,b), (x - x_L)^{\otimes q}\right\rangle + \sum_{q=0}^{k-m}\frac{1}{q!}\sum_{i'=1}^{k-m-q}\sum_{j=1}^{L-1}\left\langle \phi_{x_j,j}^{m+q+i'}(\omega,b), \Gamma_{i',j}^{m+q,L}\otimes (x-x_L)^{\otimes q}\right\rangle.
        \end{equation}
        Rewriting this, we get
        \begin{equation}
            \sigma_k^{(m)}(x;\omega,b) = \sum_{j=1}^L\phi^{m}_{x_j,j}(\omega,b) + \sum_{i=1}^{k-m}\sum_{j=1}^L  \left\langle\phi^{m+i}_{x_j,j}(\omega,b),\Gamma^{m}_{i,j}(x)\right\rangle,
        \end{equation}
        where the tensors $\Gamma^{m}_{i,j}(x)$ are given by
        \begin{equation}
            \Gamma^{m}_{i,j}(x) = \frac{1}{i!}(x - x_L)^{\otimes i} + \sum_{q=0}^{i-1}\Gamma_{i-q,j}^{m+q,L}\otimes (x-x_L)^{\otimes q}.
        \end{equation}
        Finally, we bound the norm $\|\Gamma^{m}_{i,j}(x)\|$. Utilizing that $|x - x_L| \leq C2^{-L}$ and the bound \eqref{bound-on-gamma-tensors} we get
        \begin{equation}
            \|\Gamma^{m}_{i,j}(x)\| \leq \frac{2^{-Li}}{i!} + C\sum_{q=0}^{i-1} 2^{-(i-q)j}2^{-Lq} \leq C2^{-ij},
        \end{equation}
        for a constant $C(d,k)$, since $j\leq L$ and $0\leq i \leq k$. Upon relabelling $j$ to $l$ this is exactly Lemma \ref{relu-k-representation-lemma}.
    \end{proof}

\section{Acknowledgements}
We would like to thank Ron DeVore, Rob Nowak, Jinchao Xu, and Rahul Parhi for helpful discussions. This work was supported by the National Science Foundation (DMS-2111387 and CCF-2205004).

\bibliographystyle{amsplain}
\bibliography{refs}
\end{document}
