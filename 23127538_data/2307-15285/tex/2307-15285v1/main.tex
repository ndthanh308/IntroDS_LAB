\documentclass{article}
%
\usepackage{arxiv}
\usepackage[toc,page]{appendix}

\usepackage[usenames]{xcolor}

\usepackage{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{cite}
%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm,algcompatible}
\usepackage[toc,page]{appendix}
\newcommand{\iu}{\mathrm{i}\mkern1mu}
\usepackage{lineno}
%

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\newcommand{\bibsection}{\section*{\refname}}

\numberwithin{equation}{section}

\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\lip}{Lip}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator*{\esssup}{ess\,sup}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}

%
\usepackage{mathptmx}      %
%
%
%
%
%
%
%
%
%
%

%
\begin{document}
%

\title{Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks
}

%

\author{Jonathan W. Siegel \\
 Department of Mathematics\\
 Texas A\&M University\\
 College Station, TX 77843 \\
 \texttt{jwsiegel@tamu.edu} \\
}

%
%

%

%

%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%

%
%

\maketitle

\begin{abstract}
    We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
\end{abstract}

\section{Introduction}
A (centered) zonotope in $\mathbb{R}^{d+1}$ (so that the sphere $S^d\subset \mathbb{R}^{d+1}$ is of dimension $d$) is a convex polytope $P$ which is the Minkowski sum of finitely many centered line segments, i.e. a body of the form
\begin{equation}\label{zonotope-P}
    P = \{x_1v_1 + \cdots + x_nv_n,~x_i\in[-1,1]\}
\end{equation}
for some collection of vectors $v_i\in \mathbb{R}^{d+1}$. The number $n$ is the number of summands of the zonotope. A zonoid is a convex body which is a limit of zonotopes in the Hausdorff metric. 

We consider the following problem: Given an arbitrary zonoid $Z$, how accurately can $Z$ be approximated by a polytope $P$ with $n$-summands? Here accuracy $\epsilon$ is taken to mean that $Z\subset P \subset (1+\epsilon)Z$. 

This problem has been studied by a variety of authors (see for instance \cite{bourgain1989approximation,bourgain1988distribution,linhart1989approximation,bourgain1993approximating,betke1983estimating,joos2023isoperimetric}). Of particular interest is the case when $Z = B^{d+1}$ is the Euclidean unit ball. In this case the problem has an equivalent formulation as (see \cite{betke1983estimating}): how many directions $v_1,...,v_n\in S^d$ are required to estimate the surface area of a convex body in $\mathbb{R}^{d+1}$ from the volumes of its $d$-dimensional projections orthogonal to each $v_i$?

In \cite{bourgain1989approximation}, it was shown using spherical harmonics that with $n$ summands and $Z = B^{d+1}$ the best error one can achieve is lower bounded by
\begin{equation}\label{bourgain-lower-bound}
    \epsilon(n) \geq c(d)n^{-\frac{1}{2}-\frac{3}{2d}}.
\end{equation}
When $d=2,3$, this bound was matched up to logarithmic factors in \cite{bourgain1988distribution}, specifically it was shown that for general zonoids $Z$, we have
\begin{equation}
    \epsilon(n) \leq C(d)\begin{cases}
        n^{-\frac{1}{2}-\frac{3}{2d}} \sqrt{\log(n)} & d = 2\\
        n^{-\frac{1}{2}-\frac{3}{2d}} \log(n)^{3/2} & d = 3.
    \end{cases}
\end{equation}
For larger values of $d$ the result in \cite{bourgain1988distribution} gives the worse upper bound of
\begin{equation}
    \epsilon(n) \leq C(d)n^{-\frac{1}{2}-\frac{1}{d-1}} \sqrt{\log(n)}.
\end{equation}
In \cite{bourgain1993approximating} (see also \cite{linhart1989approximation}) it was shown that these bounds can be attained using summands of equal length if $Z = B^{d+1}$. The picture was nearly completed in \cite{matouvsek1996improved} where it was shown that we have
\begin{equation}\label{matousek-result}
    \epsilon(n) \leq C(d)\begin{cases}
        n^{-\frac{1}{2}-\frac{3}{2d}} \sqrt{\log(n)} & d = 2,3\\
        n^{-\frac{1}{2}-\frac{3}{2d}} & d \geq 4.
    \end{cases}
\end{equation}
Moreover, it was shown that the upper bound when $d\geq 4$ can be achieved using summands of equal length for all zonoids $Z$.

In this work, we remove the logarithmic factors in \eqref{matousek-result} when $d=2,3$, i.e. we prove that
\begin{equation}\label{our-result-zonotope}
    \epsilon(n) \leq C(d) n^{-\frac{1}{2}-\frac{3}{2d}}
\end{equation}
for all $d$, and thus provide upper bounds exactly matching (up to a constant factor) the lower bound \eqref{bourgain-lower-bound}. To formulate these results, we pass to the dual setting (see \cite{bourgain1989approximation,matouvsek1996improved}). A symmetric convex body $Z$ is a zonoid iff
\begin{equation}
    \|x\|_{Z^*} := \sup_{z\in Z}x\cdot z = \int_{S^d} |x\cdot y|d\tau(y)
\end{equation}
for a positive measure $\tau$ on $S^d$. The body $Z$ is a zonotope with $n$-summands iff $\tau$ is supported on $n$ points. Since our error measure is scale invariant, we may assume that $\tau$ is a probability distribution. Given these considerations, the bound \eqref{our-result-zonotope} follows from the following result.

\begin{theorem}\label{optimal-zonoid-approximation-theorem}
    There exists a constant $C = C(d)$ such that for any probability measure $\tau$ on the sphere $S^d$, there exists a probability measure $\tau'$ on $S^d$ which is supported on $n$ points, such that
    \begin{equation}
        \sup_{x\in S^d} \left|\int_{S^d}|x\cdot y|d\tau(y) - \int_{S^d}|x\cdot y|d\tau'(y)\right| \leq Cn^{-\frac{1}{2}-\frac{3}{2d}}.
    \end{equation}
\end{theorem}
We remark that our method produces summands of unequal length (i.e. a non-uniform distribution $\tau'$) and we do not know whether this approximation can be achieved using summands of equal length (even for the ball $B^{d+1}$) when $d < 4$.

Recently, there has been renewed interest in the zonoid approximation problem due to its connection with approximation by shallow ReLU$^k$ neural networks \cite{bach2017breaking}. The ReLU$^k$ activation function (simply called ReLU when $k=1$) is defined by
\begin{equation}
    \sigma_k(x) = x_+^k :=  \begin{cases}
        x^k & x \geq 0\\
        0 & x < 0,
    \end{cases}
\end{equation}
where in the case $k=0$ we interpret $0^0=1$. A shallow ReLU$^k$ neural network on the sphere is a function of the form
\begin{equation}
    f_n(x) = \sum_{i=1}^n a_i\sigma_k(x\cdot y_i) = \sum_{i=1}^n a_i(x\cdot y_i)_+^k,
\end{equation}
where the $a_i\in \mathbb{R}$ are coefficients, the $y_i\in S^d$ are directions, and $n$ (the number of terms) is called the width of the network.

Shallow neural networks can be viewed as a special case of non-linear dictionary approximation. Given a Banach space $X$, let $\mathbb{D}\subset X$ be a bounded subset, i.e. $\|\mathbb{D}\| := \sup_{d\in \mathbb{D}} \|d\|_X < \infty$, which we call a dictionary. Non-linear dictionary approximation methods seek to approximate a target function $f$ by elements of the set
\begin{equation}
    \Sigma_n(\mathbb{D}) = \left\{\sum_{i=1}^n a_id_i,~a_i\in \mathbb{R},~d_i\in \mathbb{D}\right\}
\end{equation}
of $n$-term linear combinations of dictionary elements. Note that because the elements $d_i$ are not fixed, this a non-linear set of functions. It is often important to obtain some control on the coefficients $a_i$ in a non-linear dictionary expansion. For this reason, we introduce the set
\begin{equation}
    \Sigma^M_n(\mathbb{D}) = \left\{\sum_{i=1}^n a_id_i,~a_i\in \mathbb{R},~d_i\in \mathbb{D},~\sum_{i=1}^n |a_i| \leq M\right\}
\end{equation}
of non-linear dictionary expansions with $\ell^1$-bounded coefficients. 

Shallow neural networks correspond to taking
\begin{equation}
    \mathbb{D} = \mathbb{P}_k^d := \{\sigma_k(x\cdot y),~y\in S^d\} \subset W^{k}(L_\infty(S^d)).
\end{equation}
Here we take the the underlying Banach space to be the Sobolev space $W^{k}(L_\infty(S^d))$, which we define as follows. We identify this space with the set of positively $k$-homogeneous functions $f:\mathbb{R}^{d+1}\rightarrow \mathbb{R}$, i.e. $f(\alpha x) = \alpha^k f(x)$ for $\alpha \geq 0$, with norm defined by
\begin{equation}
    \|f\|_{W^{k}(L_\infty(S^d))} := \sup_{|\alpha| \leq k} \|D^\alpha f\|_{L^\infty(S^d)}.
\end{equation}
The multi-index $\alpha = (\alpha_1,...,\alpha_{d+1})$ in this definition denotes a derivative in the ambient space $\mathbb{R}^{d+1}$. 

Note that this is a slight variant on the usual definition of Sobolev spaces on the sphere, which is well suited to the dictionary $\mathbb{P}_k^d$ consisting of positively $k$-homogeneous function. We do this to avoid the technical complications of dealing with derivatives on the sphere, and instead deal only with derivatives in the ambient space.

For functions $f\in W^{k}(L_\infty(S^d))$ we also define lower order Sobolev norms
\begin{equation}
    \|f\|_{W^{m}(L_\infty(S^d))} := \sup_{|\alpha| \leq m} \|D^\alpha f\|_{L^\infty(S^d)}
\end{equation}
for $m=0,...,k$.

A typical class of functions considered in the context of non-linear dictionary approximation is the variation space of the dictionary $\mathbb{D}$, defined as follows. Let
\begin{equation}
    B_1(\mathbb{D}) := \overline{\bigcup_{n=1}^\infty \Sigma^1_n(\mathbb{D})},
\end{equation}
denote the closed symmetric convex hull of the dictionary $\mathbb{D}$ and define the variation norm of a function $f\in X$ by
\begin{equation}
    \|f\|_{\mathcal{K}_1(\mathbb{D})} := \inf\{s > 0:~f\in sB_1(\mathbb{D})\}.
\end{equation}
This construction is also called the gauge of the set $B_1(\mathbb{D})$ (see for instance \cite{rockafellar1997convex}), and has the property that the unit ball of $\mathcal{K}_1(\mathbb{D})$ is exactly the closed convex hull $B_1(\mathbb{D})$. The variation norm has been introduced in different forms in the literature and plays an important role in statistics, signal processing, non-linear approximation, and the theory of shallow neural networks (see for instance \cite{barron2008approximation,temlyakov2008greedy,devore1996some,devore1998nonlinear,barron1993universal,jones1992simple,siegel2020approximation,siegel2022high,ongie2019function,parhi2021banach}).

In the case where $\mathbb{D} = \mathbb{P}_k^d$ consists of the dictionary corresponding to shallow neural networks, the variation space can equivalently be defined via integral representations, which were studied for example in \cite{bach2017breaking,ma2022barron}. Specifically,
\begin{equation}
    \|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)} = \inf\left\{\int_{S^d}d|\mu|(y),~f(x) = \int_{S^d} \sigma_k(x\cdot y)d\mu(y)\right\},
\end{equation}
where the infimum above is taken over all measures with finite total variation. This follows from the fact that
\begin{equation}\label{eq-221}
    B_1(\mathbb{P}_k^d) = \left\{\int_{S^d} \sigma_k(x\cdot y)d\mu(y),~\int_{S^d}d|\mu|(y) \leq 1\right\}.
\end{equation}
That the right hand size is contained in the left hand size follows by `empirically' discretizing the integral in \eqref{eq-221}, i.e. by sampling from the probability distribution $d|\mu|$. This results in an element in $\Sigma_n^1(\mathbb{P}_k^d)$ which converges to $f$ in the $W^{k}(L_\infty(S^d))$-norm due to the fact that half-spaces on the sphere have bounded VC-dimension \cite{vapnik1971uniform,vapnik1999nature}. The reverse inclusion follows since convergence in $W^{k}(L_\infty(S^d))$ implies convergence in $L_2(S^d)$. Then the existence of an integral representation follows from Lemma 3 in \cite{siegel2023characterization} due to the compactness of $\mathbb{P}_k^d$ in $L_2$.

One important question is how efficiently functions in the variation space $\mathcal{K}_1(\mathbb{D})$ can be approximated by non-linear dictionary expansions $\Sigma_n(\mathbb{D})$ with $n$ terms. When the space $X$ is a Hilbert space (or more generally a type-$2$ Banach space), we have the bound \cite{barron1993universal,jones1992simple,pisier1981remarques}
\begin{equation}\label{maurey-jones-barron-rate}
    \inf_{f_n\in \Sigma_n(\mathbb{D})} \|f - f_n\|_X \leq C\|f\|_{\mathcal{K}_1(\mathbb{D})}n^{-\frac{1}{2}}.
\end{equation}
The constant here depends only upon the norm of the dictionary $\|\mathbb{D}\|$ and the type-$2$ constant of the space $X$. Moreover, the norm of the coefficients $a_i$ can be controlled, so that if $f$ is in $B_1(\mathbb{D})$ (the unit ball of $\mathcal{K}_1(\mathbb{D})$), then $f_n$ can be taken in $\Sigma^1_n(\mathbb{D})$. 

For many dictionaries, specifically the dictionaries $\mathbb{P}_k^d$ corresponding to shallow neural networks, the rate \eqref{maurey-jones-barron-rate} can be improved (see for instance \cite{bach2017breaking,klusowski2018approximation,siegel2022sharp,makovoz1996random}). For instance, in the $L_2(S^d)$-norm we get the rate
\begin{equation}
    \inf_{f_n\in \Sigma_n(\mathbb{P}_k^d)} \|f - f_n\|_{L_2(S^d)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)}n^{-\frac{1}{2}-\frac{2k+1}{2d}},
\end{equation}
and this rate is optimal up to logarithmic factors if we require even mild control on the coefficients $a_i$ (for instance $|a_i| \leq C$ for a constant $C$) \cite{siegel2022sharp}. Note that the results in \cite{makovoz1996random,klusowski2018approximation,siegel2022sharp} were stated and proved on a domain in $\mathbb{R}^d$ instead of on the sphere $S^d$. However, the techniques and bounds carry over to the sphere in a straightforward manner.

In this work, we consider approximation rates for the dictionary $\mathbb{P}_k^d$ on the variation space $\mathcal{K}_1(\mathbb{P}_k^d)$ in the $W^{m}(L_\infty(S^d))$-norm for $m=0,...,k$, i.e. we consider uniform approximation of both $f$ and its derivatives up to order $m$. This is a much stronger error norm than the $L_2$-norm, and approximating derivatives is important for applications of shallow neural networks to scientific computing (see for instance \cite{siegel2023greedy,lu2022priori,xu2020finite}).

This problem has previously been considered in the case $m=0$, i.e. in the $L_\infty(S^d)$-norm (see for instance \cite{bach2017breaking,klusowski2018approximation,ma2022uniform,barron1992neural,cheang2000better,yukich1995sup}. In this case, when $k=0$ an approximation rate of
\begin{equation}
    \inf_{f_n\in \Sigma_n(\mathbb{P}_0^d)} \|f - f_n\|_{L_\infty(S^d)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_0^d)}n^{-\frac{1}{2}-\frac{1}{2d}},
\end{equation}
was proved in \cite{ma2022uniform} using results from geometric discrepancy theory \cite{matouvsek1995tight}. For $k=1$, the aforementioned results on approximating zonoids by zonotopes \cite{matouvsek1996improved} were used to get a rate of \cite{bach2017breaking}
\begin{equation}
        \inf_{f_n\in \Sigma_n(\mathbb{P}_1^d)} \|f - f_n\|_{L_\infty(S^d)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_1^d)}\begin{cases}
            n^{-\frac{1}{2}-\frac{3}{2d}}\sqrt{\log{n}} & d = 2,3\\
            n^{-\frac{1}{2}-\frac{3}{2d}} & d \geq 4.
        \end{cases}
\end{equation}
Finally, when $k\geq 2$, the best known result is \cite{klusowski2018approximation}
\begin{equation}
    \inf_{f_n\in \Sigma_n(\mathbb{P}_k^d)} \|f - f_n\|_{L_\infty(S^d)} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)} n^{-\frac{1}{2}-\frac{1}{d}} \sqrt{\log{n}}.
\end{equation}
We remark that for all of these results, the coefficients of $f_n$ can be controlled. Specifically, if $f\in B_1(\mathbb{P}_k^d)$, then $f_n$ can be taken in $\Sigma_n^1(\mathbb{P}_k^d)$.

Utilizing our techniques, we obtain the following approximation rate, which significantly improves the existing upper bounds when $k\geq 1$ and also allows derivatives to be uniformly approximated.
\begin{theorem}\label{shallow-network-approximation-theorem}
    Let $0\leq m\leq k$. Then we have the bound
    \begin{equation}
        \inf_{f_n\in \Sigma_n(\mathbb{P}_k^d)} \|f - f_n\|_{W^{m}(L_\infty(S^d))} \leq C\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)}n^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}},
    \end{equation}
    where $C = C(d,k)$ is a constant. Moreover, the coefficients of $f_n$ can be controlled, so if $f\in B_1(\mathbb{P}_k^d)$, then $f_n$ can be taken in $\Sigma_n^1(\mathbb{P}_k^d)$.
\end{theorem}
Theorems \ref{optimal-zonoid-approximation-theorem} and \ref{shallow-network-approximation-theorem} are proved using a modification of the geometric discrepancy argument used in \cite{matouvsek1996improved}. Theorem \ref{optimal-zonoid-approximation-theorem} follows essentially as a special case of Theorem \ref{shallow-network-approximation-theorem} when $k=1$. The only difference is that the ReLU activation function is replaced by the absolute value function. This requires a trivial change in the proof of Theorem \ref{shallow-network-approximation-theorem}, and we omit it for the sake of brevity. The proof of Theorem \ref{shallow-network-approximation-theorem} is given in Section \ref{shallow-relu-network-approximation}.

We also remark that although our analysis is done on the sphere (mainly for simplicity of presentation), our techniques can be used to obtain the same results on a bounded domain.

\section{Approximation by Shallow ReLU$^k$ Neural Networks}\label{shallow-relu-network-approximation}
In this section, we give the proof of Theorem \ref{shallow-network-approximation-theorem}, which gives uniform approximation rates for shallow ReLU$^k$ neural networks. Theorem \ref{shallow-network-approximation-theorem} follows easily from the following Proposition.
\begin{proposition}\label{term-decrease-by-a-factor-reluk-proposition}
    Fix an integer $k \geq 0$. Let $\tau$ be a probability distribution on the sphere $S^d$ which is supported on $N$ points for $N$ sufficiently large ($N\geq 6$ is sufficient). Then there exists a probability distribution $\tau'$ supported on at most $(1-c)N$ points such that for all multi-indices $\alpha$ with $|\alpha|\leq k$ we have
    \begin{equation}
        \sup_{x\in S^d} \left|D_x^\alpha\left(\int_{S^d}\sigma_k(x\cdot y)d\tau(y) - \int_{S^d}\sigma_k(x\cdot y)d\tau'(y)\right)\right| \leq CN^{-\frac{1}{2}-\frac{2(k - |\alpha|) + 1}{2d}},
    \end{equation}
    where $D_x^\alpha$ denotes the $\alpha$-th order derivative with respect to $x$.
    Here $C = C(d,k)$ and $c$ is an absolute constant.
\end{proposition}
\begin{proof}[Proof of Theorem \ref{shallow-network-approximation-theorem}]
    Suppose without loss of generality that $\|f\|_{\mathcal{K}_1(\mathbb{P}_k^d)} \leq 1$, i.e. that $f\in B_1(\mathbb{P}_k^d)$.
    
    By definition, this means that for any $\epsilon > 0$ there exists points $y_1,...,y_N\in S^d$ and weights $a_1,...,a_N\in \mathbb{R}$ (for a sufficiently large $N$) such that
    \begin{equation}\label{f-approximation-273}
        \left\|f - \sum_{i=1}^N a_i\sigma_k(x\cdot y_i)\right\|_{W^{k}(L_\infty(S^d))} < \epsilon,
    \end{equation}
    and $\sum_{i=1}^N |a_i| \leq 1$. The next step is to approximate the sum in \eqref{f-approximation-273} by an element in $\Sigma_n^1(\mathbb{P}_k^d)$. To do this, we split the sum into its positive and negative parts, i.e. write
    \begin{equation}\label{positive-negative-decomposition-277}
        \sum_{i=1}^N a_i\sigma_k(x\cdot y_i) = \sum_{a_i > 0} a_i\sigma_k(x\cdot y_i) - \sum_{a_i < 0} |a_i|\sigma_k(x\cdot y_i).
    \end{equation}
    By considering the positive and negative pieces separately, we essentially reduce to the case where all $a_i$ are positive. In this case, the sum can be written
    \begin{equation}
        \sum_{i=1}^N a_i\sigma_k(x\cdot y_i) = \int_{S^d}\sigma_k(x\cdot y)d\tau(y)
    \end{equation}
    for a probablity measure $\tau$ supported on at most $N$ points. 
    
    We repeatedly apply Proposition \ref{term-decrease-by-a-factor-reluk-proposition} to the distribution $\tau$ until the size of the support set is at most $n/2$. By Proposition \ref{term-decrease-by-a-factor-reluk-proposition}, the $W^{m}(L_\infty)$-error incurred is bounded by
    \begin{equation}
        Cn^{-\frac{1}{2}-\frac{2(k-m)+1}{2d}}\left(\sum_{j=0}^\infty (1-c)^{j\left(\frac{1}{2}+\frac{2(k-m)+1}{2d}\right)}\right) \lesssim n^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}},
    \end{equation}
    since the errors in each step form a geometric series whose sum is bounded by a multiple of its largest term (which is the error made in the last step).

    This gives an $f_n\in \Sigma_{n/2}^1(\mathbb{P}_k^d)$ such that
    \begin{equation}
        \left\|f_n - \sum_{i=1}^N a_i\sigma_k(x\cdot y_i)\right\|_{W^{m}(L_\infty(S^d))} \leq Cn^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}},
    \end{equation}
    whenever $a_i \geq 0$ and $\sum_{i=1}^N a_i = 1$. Applying this to the positive and negative parts in \eqref{positive-negative-decomposition-277} gives an $f_n\in \Sigma_{n}^1(\mathbb{P}_k^d)$ such that
    \begin{equation}
        \left\|f - f_n\right\|_{W^{m}(L_\infty(S^d))} \leq Cn^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}} + \epsilon.
    \end{equation}
    Since $\epsilon > 0$ was arbitrary, this completes the proof.
\end{proof}

It remains to prove Proposition \ref{term-decrease-by-a-factor-reluk-proposition}. The proof utilizes the ideas of geometric discrepancy theory and borrows many ideas from the proof of Proposition 9 in \cite{matouvsek1996improved}. However, Proposition 9 in \cite{matouvsek1996improved} only deals with uniform distributions, and a few key modifications are required to deal with the case of `unbalanced' distributions $\tau$, which enables us to remove the logarithmic factors in all dimensions in Theorems \ref{optimal-zonoid-approximation-theorem} and \ref{shallow-network-approximation-theorem}. In addition, dealing with the higher order smoothness of the ReLU$^k$ activation function introduces significant technical difficulties.

    We first introduce some notation. We will need to work with symmetric tensors in order to handle higher order derivatives of multivariate functions. Our tensors will be defined on the ambient space $\mathbb{R}^{d+1}$ containing the sphere $S^d$, so let $I = \{1,...,d+1\}$ denote the relevant indexing set. A (symmetric) tensor $X$ of order $m$ is an array of numbers indexed by a tuple $\textbf{i}\in I^m$, which satisfies 
    \begin{equation}
        X_\textbf{i} = X_{\pi(\textbf{i})}
    \end{equation}
    for any permutation $\pi$ of $\{1,...,m\}$. Here $\pi(\textbf{i})_{j} = \textbf{i}_{\pi(j)}$ for $j=1,...,m$. Note that vectors in $\mathbb{R}^{d+1}$ are symmetric tensors of order one. We adopt the $\ell^\infty$ norm on the space of symmetric tensors, i.e.
    \begin{equation}
        \|X\| := \max_{\textbf{i}\in I^m} |X_\textbf{i}|.
    \end{equation}  
    
    Given tensors $X$ and $Y$ of orders $m_1$ and $m_2$, their tensor product, which is a tensor of order $m_1 + m_2$, is defined in the standard way by
    \begin{equation}
        (X\otimes Y)_{\textbf{i}\textbf{j}} = X_{\textbf{i}}Y_{\textbf{j}},
    \end{equation}
    where $\textbf{i}\in I^{m_1}$, $\textbf{j}\in I^{m_2}$ and $\textbf{i}\textbf{j}$ denotes concatenation. We will also write $X^{\otimes r}$ for the $r$-fold tensor product of $X$ with itself.
    Supposing that $m_1 \geq m_2$, we define the contraction, which is a tensor of order $m_1 - m_2$ by
    \begin{equation}
        \langle X,Y\rangle_{\textbf{i}} = \sum_{\textbf{j}\in I^{m_2}} X_{\textbf{i}\textbf{j}}Y_{\textbf{j}}.
    \end{equation}
    Note that since we will be exclusively working with $\mathbb{R}^{d+1}$ with the standard inner product, to simplify the presentation we will not make the distinction between covariance and contravariance in the following. 
    
    We remark that repeated contraction can be written in terms of the tensor product in the following way
    \begin{equation}\label{contraction-tensor-product-identity}
        \langle \langle X,Y\rangle, Z\rangle = \langle X, Y\otimes Z\rangle,
    \end{equation}
    and also note the inequality
    \begin{equation}\label{contraction-l-infty-inequality}
        \left\|\langle X,Y\rangle\right\| \leq C\|X\|\|Y\|,
    \end{equation}
    where $C = C(d,k) = (d+1)^k$.
    
    Let us write $\sigma_k(x,y) = (x\cdot y )^k_+$ for the ReLU$^k$ activation function. Given an order $0 \leq m\leq k$, we denote the $m$-th derivative (tensor) of the ReLU$^k$ function by
    \begin{equation}\label{sigma-k-m-definition}
        \sigma_k^{(m)}(x,y) = D_x^m[\sigma_k(x\cdot y)] = \begin{cases}
            \frac{k!}{(k-m)!}(x\cdot y)^{k-m}y^{\otimes m} & x\cdot y \geq 0\\
            0 & x\cdot y < 0.
        \end{cases}
    \end{equation}
    In order to deal with the additional smoothness of the activation function we will need to utilize higher-order Taylor polynomials. Given points $x_1,x_2,y\in S^d$, an order $0\leq m\leq k$, and a number of terms $0\leq r\leq k-m$, we denote by
    \begin{equation}\label{taylor-expansion-definition}
        \mathcal{T}^{m,r}_{x_1}(x_2,y) := \sum_{q=0}^r \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_1,y), (x_2 - x_1)^{\otimes q}\right\rangle
    \end{equation}
    the $r$-th order Taylor polynomials of $\sigma_k^{(m)}(x,y)$ around $x_1$ evaluated at $x_2$.

Next, we recall a few crucial Lemmas from \cite{matouvsek1996improved} which we will need in our proof. The first Lemma shows that a finite subset of the sphere can be partitioned into pieces with `low crossing number.'
\begin{lemma}[Lemma 5 in \cite{matouvsek1996improved}]\label{low-crossing-number lemma}
    Let $s \geq 2$ be an integer and $P$ be an $N$-point subset of $S^d$ with $N \geq s$. Then there exist disjoint subsets $P_1,...,P_t\subset P$ with $|P_i| = s$ and $|\cup_{i=1}^t P_i| \geq N/2$ such that
    \begin{itemize}
        \item For every $y\in S^d$ we have
        \begin{equation}
            \left|\{i\in \{1,..,t\},~\text{s.t.}~\exists x_1,x_2\in P_i ~\text{with}~\sign{(y\cdot x_1)} \neq \sign{(y\cdot x_2)}\}\right| \leq CN^{1-1/d},
        \end{equation}
        i.e. the number of sets $P_i$ which are `crossed' by any given great circle is at most $CN^{1-1/d}$.
        \item Each set $P_i$ has diameter at most $CN^{-1/d}$.
    \end{itemize}
    Here $C = C(d,s)$ is a constant independent of $N$.
\end{lemma}

The next Lemma shows that certain covering sets of the sphere can be constructed.
\begin{lemma}[Lemma 6 in \cite{matouvsek1996improved}]\label{covering-set-lemma}
    Let $P$ be an $N$-point subset of $S^d$ and $0 < \delta < 1$ be given. There exists a subset $\mathcal{N} \subset S^d$ (depending upon both $P$ and $\delta$) with $|\mathcal{N}| \leq C\delta^{-d}$ such that for any $x\in S^d$, there exists a $z\in \mathcal{N}$ with
    \begin{itemize}
        \item $|x - z| \leq \delta.$
        \item $|P\cap \{y\in S^d,~\sign{(y\cdot x)} \neq \sign{(y\cdot z)}\}| \leq \delta N.$
    \end{itemize}
    Here $C = C(d)$ is a constant independent of $N$ and $\delta$.
\end{lemma}
Utilizing both of these Lemmas, we give the proof of Proposition \ref{term-decrease-by-a-factor-reluk-proposition}.
\begin{proof}[Proof of Proposition \ref{term-decrease-by-a-factor-reluk-proposition}]
    Note that since $\tau$ is supported on $N$ points 
    the integral which we are trying to approximate in Proposition \ref{term-decrease-by-a-factor-reluk-proposition} is given by
    \begin{equation}\label{sum-representation-146}
        \int_{S^d}\sigma_k(x\cdot y)d\tau(y) = \sum_{y\in S} a_y\sigma_k(x\cdot y),
    \end{equation}
    where $|S| = N$ and the coefficients $a_y$ satisfy $a_y\geq 0$ and  $\sum_{y\in S} a_y = 1$.
    
    Let $M$ denote the median of the coefficients $a_y$ and set
    $$
    S_- = \{y\in S:~a_y \leq M\},~~S_+ = \{y\in S:a_y > M\}.
    $$
    This gives a decomposition of the sum in \eqref{sum-representation-146} in terms of its large and small coefficients
    \begin{equation}
        \sum_{y\in S} a_y\sigma_k(x\cdot y) = \sum_{y\in S_-} a_y\sigma_k(x\cdot y) + \sum_{y\in S_+} a_y\sigma_k(x\cdot y).
    \end{equation}
    We will leave the second, i.e. the large, sum untouched and approximate the small sum by
    \begin{equation}\label{term-count-decrease-approximation}
        \sum_{y\in S_-} a_y\sigma_k(x\cdot y) \approx \sum_{y\in T} b_y\sigma_k(x\cdot y),
    \end{equation}
    where $T\subset S_-$ and $|T| \leq (1-c)|S_-|$, the new coefficients $b_y \geq 0$ and satisfy $\sum_{y\in T}b_y = \sum_{y\in S_-} a_y$, and the error of approximation satisfies (using the tensor notation we have introduced)
    \begin{equation}\label{error-bound-162}
        \sup_{x\in S^d}\left\|\sum_{y\in S_-} a_y\sigma_k^{(m)}(x,y) - \sum_{y\in T} b_y\sigma_k^{(m)}(x,y)\right\| \leq CN^{-\frac{1}{2} - \frac{2(k-m)+1}{2d}}
    \end{equation}
    for $m=0,...,k$ with $C = C(d,k)$ a constant.
    Setting
    \begin{equation}
        \tau' = \sum_{y\in T}b_y \delta_y + \sum_{y\in S_+} a_y\delta_y
    \end{equation}
    now completes the proof since $|S_-| \geq N/2$.

    We now turn to the heart of the proof, which is constructing an approximation \eqref{term-count-decrease-approximation} which satisfies \eqref{error-bound-162}. Note first that by construction, we have
    \begin{equation}\label{bound-on-coefficients-small-half}
        \max_{y\in S_-} a_y \leq M \leq \frac{2}{N},
    \end{equation}
    i.e. all of the coefficients in the small half are at most $2/N$. This holds since at least half (i.e. at least $N/2$) of the $a_y$ are at least as large as the median $M$ and $\sum_{y\in S} a_y = 1$.

    Next we utilize the multiscale covering of the sphere constructed in \cite{matouvsek1996improved}. For $l=1,...,L$ with $2^L > N$ we apply Lemma \ref{covering-set-lemma} with $P = S_-$ and $\delta = 2^{-l}$ to obtain a sequence of sets $N_l\subset S^d$ with $|N_l| \leq C2^{dl}$ such that for any $x\in S^d$ there exists a $z\in N_l$ with $|x-z| \leq 2^{-l}$ and
    $$
        |\{y\in S_-:\sign{(y\cdot x)} \neq \sign{(y\cdot z)}\}| \leq 2^{-l} |S_-| \leq 2^{-l}N.
    $$
    Given a point $x\in S^d$, we denote by $\pi_l(x)$ the point $z\in N_l$ satisfying these properties (if this point is not unique we choose one arbitrarily for each $x$). 

    For each level $l=1,...,L$, each point $x\in N_l$, and each index $m=0,..,k$ we consider the function
    \begin{equation}\label{definition-of-phi-m}
        \phi_{x,l}^m(y) = \begin{cases}
            \sigma_k^{(m)}(x,y) - \mathcal{T}^{m,k-m}_{\pi_{l-1}(x)}(x,y) & l \geq 2\\
            \sigma_k^{(m)}(x,y) & l=1,
        \end{cases}
    \end{equation}
    where $\mathcal{T}^{m,k-m}_{\pi_{l-1}(x)}(x,y)$ is the $(k-m)$-th order Taylor polynomial of $\sigma_k^{(m)}(x,y)$ defined in \eqref{taylor-expansion-definition}.

    We note the following bounds on $\phi_{x,l}^m(y)$. First, if $\sign{(y\cdot x)} = \sign{(y\cdot \pi_{l-1}(x))}$ (for $l \geq 2$), then
    \begin{equation}\label{phi-bound-zero}
        \phi_{x,l}^m(y) = 0.
    \end{equation}
    This holds since on the half space $\{x:x\cdot y\geq 0\}$ the function $\sigma_k^{(m)}(x,y)$ is a polynomial of degree $k-m$ in $x$. Thus on this half-space it is equal to its $(k-m)$-th order Taylor polynomial about any point. So if $x$ and $\pi_{l-1}(x)$ both lie in this half-space, then the difference in \eqref{definition-of-phi-m} vanishes. On the other hand, if $x$ and $\pi_{l-1}(x)$ both lie in the complement, then all terms in \eqref{definition-of-phi-m} are $0$.

    On the other hand, for any $x,y\in S^d$ we have the bound
    \begin{equation}\label{bound-on-phi-nonzero}
        \|\phi_{x,l}^m(y)\| \leq C2^{-(k-m)l},
    \end{equation}
    where $C = C(d,k)$. This holds since $\sigma_k^{(m)}(x,y)$ (as a function of $x$) has bounded $(k-m)$-th order derivatives. Thus the difference in \eqref{definition-of-phi-m} is bounded by
    \begin{equation}
        \left\|\sigma_k^{(m)}(x,y) - \mathcal{T}^{m,k-m}_{\pi_{l-1}(x)}(x,y)\right\| \leq C|x - \pi_{l-1}(x)|^{k-m} \leq C2^{-(k-m)l}.
    \end{equation}
    When $l=1$ we also trivially obtain the bound \eqref{bound-on-phi-nonzero}.
    
    The next step is to decompose the functions $\sigma_k^{(m)}(x,y)$ with $y\in S_-$ in terms of the $\phi_{x,l}^m(y)$. This is captured in the following technical Lemma.
    \begin{lemma}\label{relu-k-representation-lemma}
    Let $\phi_{x,l}^m$ be defined by \eqref{definition-of-phi-m}. For $x\in S^d$ define $x_L = \pi_L(x)$ and $x_l = \pi_l(x_{l+1})$ for $l < L$. 
    
    Then 
     for any $m=0,...,k$, $x\in S^d$ and $y\in S_-$ we have
    \begin{equation}\label{sigma-decomposition-reluk}
        \sigma_k^{(m)}(x,y) = \sum_{l=1}^L\phi^{m}_{x_j,j}(y) + \sum_{i=1}^{k-m}\sum_{l=1}^L  \left\langle\phi^{m+i}_{x_l,l}(y),\Gamma^{m}_{i,l}(x)\right\rangle,
    \end{equation}
    where the tensors $\Gamma^{m}_{i,l}(x)$ satisfy the bound
        \begin{equation}\label{Gamma-x-tensor-bound}
            \|\Gamma^{m}_{i,l}(x)\| \leq C2^{-il},
        \end{equation}
        for a constant $C(d,k)$.
    \end{lemma}
    The proof of this Lemma is a technical, but relatively straightforward calculation, so we postpone it until the end of this Section and complete the proof of Proposition \ref{term-decrease-by-a-factor-reluk-proposition} first.

    Utilizing the decomposition \eqref{sigma-decomposition-reluk} we write the LHS of \eqref{error-bound-162} as
    \begin{equation}
        \sup_{x\in S^d}\left\|\sum_{l=1}^L\left[\sum_{y\in S_-} a_y\phi^{m}_{x_l,l}(y) - \sum_{y\in T} b_y\phi^{m}_{x_l,l}(y)\right] + \sum_{l=1}^L\sum_{i=1}^{k-m}\left\langle\sum_{y\in S_-} a_y\phi^{m+i}_{x_l,l}(y) - \sum_{y\in T} b_y\phi^{m+i}_{x_l,l}(y), \Gamma^{m}_{i,l}(x)\right\rangle\right\|.
    \end{equation}
    Utilizing the triangle inequality, the bound \eqref{contraction-l-infty-inequality}, and the bound \eqref{Gamma-x-tensor-bound}, we see that it suffices to find the subset $T\subset S_-$ with $|T| \leq (1-c)|S_-|$, and new coefficients $b_y\geq 0$ satisfying $\sum_{y\in T}b_y = \sum_{y\in S_-} a_y$ such that for $m=0,...,k$ we have
    \begin{equation}\label{error-decomposition-241}
        \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il}\sup_{x\in N_l}\left\|\sum_{y\in S_-} a_y\phi^{m+i}_{x,l}(y) - \sum_{y\in T} b_y\phi^{m+i}_{x,l}(y)\right\| \leq CN^{-\frac{1}{2} - \frac{2(k-m) + 1}{2d}}
    \end{equation}
    for a constant $C = C(d,k)$.

    To find this set $T$, we apply Lemma \ref{low-crossing-number lemma} with $s=3$ to the set $P = S_-$ (note that here we need $|S_-| \geq 3$ which follows from $N \geq 6$) . This gives disjoint subsets $P_1,..,P_t$ of size $3$ satisfying the conclusions of the Lemma with
    \begin{equation}\label{number-of-P-lower-bound}
        \left|\bigcup_{i=1}^t P_i\right| \geq |S_-|/2.
    \end{equation}
    We denote the three elements of each set $P_j$ by
    $$
        P_j = \{u_j,v_j,w_j\},
    $$
    which are ordered so that the coefficients satisfy $0 \leq a_{u_j}\leq a_{v_j}\leq a_{w_j}$. Based upon the partition $P_1,...,P_t$, we will use a modification of the partial coloring argument given in \cite{matouvsek1996improved} (the idea is originally due to Spencer \cite{spencer1985six} and Beck \cite{beck1984some}), which we describe in the following.

    Given a `partial coloring' $\chi:\{1,...,t\}\rightarrow \{-1,0,1\}$, we transform the sum $\sum_{y\in S_-} a_y \sigma_k(x\cdot y)$ as follows. If $\chi(j) = 1$, we remove the term corresponding to $u_j$, double the coefficient $a_{v_j}$ of the term corresponding to $v_j$, and add the difference $a_{u_j} - a_{v_j}$ to the coefficient $a_{w_j}$ of the term corresponding to $w_j$. If $\chi(j) = -1$, we do the same but reverse the roles of $u_j$ and $v_j$. This results in a transformed sum $\sum_{y\in T} b_y \sigma_k(x\cdot y)$ over a set $T\subset S_-$ and with coefficients $b_y$ for $y\in T$ described as follows. Let
    \begin{equation}
        R_j = \begin{cases}
            \emptyset & \chi(j) = 0\\
            \{u_j\} & \chi(j) = 1\\
            \{v_j\} & \chi(j) = -1,
        \end{cases}
    \end{equation}
    denote the removed set for each $P_j$.
    Then the set $T$ is given by
    \begin{equation}
        T = S_-\setminus \left(\bigcup_{j=1}^t R_j\right),
    \end{equation}
    and for $y\in T$ the coefficients $b_y$ are given by
    \begin{equation}
        b_y = \begin{cases}
            a_y & y\notin \bigcup_{j}P_j\\
            (1+\chi(j))a_{y} & y = v_j\\
            (1-\chi(j))a_y & y = u_j\\
            a_{y} + \chi(j)(a_{u_j} - a_{v_j}) & y = w_j.
        \end{cases}
    \end{equation}
    We have constructed this transformation so that $$\sum_{y\in T} b_y = \sum_{y\in S_-} a_y,$$
    the $b_y \geq 0$ since $w_j$ has the largest coefficient among the points in $P_j$, and for any $x\in S^d$ the error in the $m$-th derivative is given by
    \begin{equation}
        \sum_{y\in S_-} a_y\sigma_k^{(m)}(x,y) - \sum_{y\in T} b_y\sigma_k^{(m)}(x,y) = \sum_{j=1}^t \chi(j)\left[-a_{u_j}\sigma_k^{(m)}(x,u_j) + a_{v_j}\sigma_k^{(m)}(x,v_j) + (a_{u_j} - a_{v_j})\sigma_k^{(m)}(x,w_j)\right].
    \end{equation}
    Using the linearity of the derivative and the definition of the Taylor polynomial \eqref{taylor-expansion-definition}, this implies that for any $x\in S^d$, any level $l = 1,...,L$, and any $m=0,...,k$ we have
    \begin{equation}\label{error-formula-306}
        \sum_{y\in S_-} a_y \phi_{x,l}^m(y) - \sum_{y\in T} b_y\phi_{x,l}^m(y) = \sum_{j=1}^t \chi(j) \Psi^m_{x,l,j},
    \end{equation}
    where we have defined
    \begin{equation}\label{definition-of-psi}
    \begin{split}
        \Psi^m_{x,l,j} = -a_{u_j}\phi^m_{x,l}(u_j) + a_{v_j}\phi^m_{x,l}(v_j) + (a_{u_j} - a_{v_j})\phi^m_{x,l}(w_j).
    \end{split}
    \end{equation}
    Further, for any index $j$ such that $\chi(j) \neq 0$, we have eliminated one term (either $u_j$ or $v_j$) from the sum. Thus
    \begin{equation}\label{T-decrease-equation-317}
        |T| = |S| - |\{j:~\chi(j) \neq 0\}|.
    \end{equation}    
    
    We proceed to find a partial coloring $\chi:\{1,...,t\}\rightarrow \{-1,0,1\}$ with a positive fraction of non-zero entries, i.e. with $|\{j:~\chi(j) \neq 0\}| \geq ct$, such that for $m=0,...,k$
    \begin{equation}\label{partial-coloring-bound}
        \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il}\sup_{x\in N_l} \left\|\sum_{j=1}^t\chi(j)\Psi^{m+i}_{x,l,j}\right\| \leq CN^{-\frac{1}{2} - \frac{2(k-m)+1}{2d}}.
    \end{equation}

    By \eqref{error-formula-306} this will guarantee that the LHS in \eqref{error-decomposition-241} is sufficiently small, and by \eqref{T-decrease-equation-317} this will guarantee that the set $T$ is small enough, since by \eqref{number-of-P-lower-bound}
    \begin{equation}
        |T| = |S| - |\{j:~\chi(j) \neq 0\}| \leq |S| - ct \leq \left(1 - \frac{c}{6}\right)|S_-|.
    \end{equation}
    The existence of such a partial coloring $\chi$ follows from a relatively well-known technique in discrepancy theory called the partial coloring method.

    Given a (total) coloring $\epsilon:\{1,...,t\}\rightarrow \{\pm 1\}$ we consider the quantities
    \begin{equation}\label{definition-of-E}
        E^m_{x,l}(\epsilon) := \sum_{j=1}^t\epsilon(j)\Psi^m_{x,l,j}
    \end{equation}
    for each $x\in N_l$. We would like to find a coloring $\epsilon$ such that $\|E^m_{x,l}(\epsilon)\| \leq \Delta^m_l$ for all $l=1,...,L$, $m=0,...,k$ and $x\in N_l$, where the $\Delta^m_l$ are suitable parameters chosen so that
    \begin{equation}\label{condition-on-delta-k}
        \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l \leq CN^{-\frac{1}{2}-\frac{2(k-m) + 1}{2d}},
    \end{equation}
    for $m=0,...,k$.
    One strategy would be to choose $\epsilon$ uniformly at random, bound the tail of the random variable $E^m_{x,l}(\epsilon)$, and use a union bound over $x\in N_l$. Unfortunately, this strategy will lose a factor $\sqrt{\log{N}}$. The ingenious method to get around this, due to Spencer \cite{spencer1985six} and Beck \cite{beck1984some}, is to show that instead there exist \textit{two} colorings $\epsilon_1$ and $\epsilon_2$ such that $\|E^m_{x,l}(\epsilon_1) - E^m_{x,l}(\epsilon_2)\| \leq \Delta^m_l$ for all $l=1,...,L$, $m=0,...,k$, and $x\in N_l$, and such that $\epsilon_1$ and $\epsilon_2$ differ in many indices, i.e.
    \begin{equation}
        |\{j:~\epsilon_1(j)\neq \epsilon_2(j)\}| \geq ct
    \end{equation}
    for an absolute constant $c$. Then $\chi = \frac{1}{2}(\epsilon_1 - \epsilon_2)$ gives the desired partial coloring.

    Finally, we will prove the existence of these two colorings $\epsilon_1$ and $\epsilon_2$ for suitably chosen parameters $\Delta^m_l$ satisfying \eqref{condition-on-delta-k}. To help organize this calculation, it is convenient to introduce the notion of the entropy of a discrete distribution (see for instance \cite{matousek1999geometric,alon2016probabilistic}). (Note that for simplicity all of the logarithms in the following are taken with base $2$.)
\begin{definition}
    Let $X$ be a discrete random variable, i.e. the range of $X$ is a countable set $\Lambda$. The entropy of $X$ is defined by
    \begin{equation}
        H(X) = -\sum_{\lambda\in \Lambda} p_\lambda\log(p_\lambda),
    \end{equation}
    where $p_\lambda = \mathbb{P}(X = \lambda)$ is the probability of the outcome $\lambda$.
\end{definition}
One important property of the entropy we will use is subadditivity, i.e. if $X = (X_1,...,X_r)$, then
\begin{equation}\label{subadditivity-entropy}
    H(X) \leq \sum_{j=1}^r H(X_j),
\end{equation}
where we have equality in the above bound when the components $X_j$ of $X$ are independent.

An important component of the calculation is the following Lemma from \cite{matouvsek1996improved} (see also \cite{matouvsek1996discrepancy,alon2016probabilistic}).
\begin{lemma}[Lemma 11 in \cite{matouvsek1996improved}]\label{entropy-partial-coloring-lemma}
    Let $\epsilon:\{1,...,t\}\rightarrow \{\pm 1\}$ be a uniformly random coloring. Let $b$ be a function of $\epsilon$ and suppose that the entropy satisfies $H(b(\epsilon)) \leq t/5$. Then there exist two colorings $\epsilon_1,\epsilon_2$ differing in at least $t/4$ components such that $b(\epsilon_1) = b(\epsilon_2)$.
\end{lemma}
We utilize this lemma in the following way. Take each entry of the (tensor-valued) random variable $E^m_{x,l}(\epsilon)$ defined in \eqref{definition-of-E} and round it to the nearest multiple of the (still undetermined) parameter $\Delta^m_l$. This results in a random variable
\begin{equation}
    b^m_{x,l}(\epsilon) = [(\Delta_l^m)^{-1}E^m_{x,l}(\epsilon)],
\end{equation}
where $[\cdot]$ denote the (component-wise) nearest integer function. Note that if $b^m_{x,l}(\epsilon_1) = b^m_{x,l}(\epsilon_2)$, then it follows that $\|E^m_{x,l}(\epsilon_1) - E^m_{x,l}(\epsilon_2)\| \leq \Delta^m_l$. Applying Lemma \ref{entropy-partial-coloring-lemma} and the subadditivity of the entropy \eqref{subadditivity-entropy}, we see that if
\begin{equation}\label{entropy-sum-bound-354}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq t/5
\end{equation}
for an appropriate choice of $\Delta^m_l$ satisfying \eqref{condition-on-delta-k}, then there exist two colorings $\epsilon_1$ and $\epsilon_2$ satisfying the desired condition with $c = 1/4$.

It remains to choose the parameters $\Delta^m_l$ satisfying \eqref{condition-on-delta-k} and to bound the sum in \eqref{entropy-sum-bound-354}. For this, we utilize the following Lemma from \cite{matouvsek1996improved}, which bounds the entropy of a `rounded' random variable in terms of the tails of the underyling real-valued random variable.
\begin{lemma}[Lemma 11 in \cite{matouvsek1996improved}]\label{entropy-tail-bound-lemma}
    Let $E$ be a real valued random variable satisfying the tail estimates
    \begin{equation}
        \mathbb{P}(E \geq \alpha M) \leq e^{-\alpha^2/2},~ \mathbb{P}(E \leq -\alpha M) \leq e^{-\alpha^2/2},
    \end{equation}
    for some parameter $M$. Let $b(E)$ denote the random variable obtained by rounding $E$ to the nearest multiple of $\Delta = 2\lambda M$. Then the entropy of $b$ satisfies
    \begin{equation}
        H(b) \leq G(\lambda) := C_0\begin{cases}
            e^{-\lambda^2/9} & \lambda \geq 10\\
            1 & .1 < \lambda < 10\\
            -\log(\lambda) & \lambda \leq .1
        \end{cases}
    \end{equation}
    for an absolute constant $C_0$.
\end{lemma}

To apply this Lemma, we bound the tails of the random variables $E^m_{x,l}(\epsilon)$. This follows using Bernstein's inequality as in \cite{matouvsek1996improved}. Fix an $l\geq 2$ and an $x\in N_l$. We call an index $j$ `good' if
\begin{equation}\label{good-index-definition-zonotope}
    \sign(u_j\cdot x) = \sign(u_j\cdot \pi_{l-1}(x))~\text{and}~ \sign(v_j\cdot x) = \sign(v_j\cdot \pi_{l-1}(x)) ~\text{and}~ \sign(w_j\cdot x) = \sign(w_j\cdot \pi_{l-1}(x)),
\end{equation}
and `bad' otherwise. Using \eqref{phi-bound-zero} we see that for the good indices we have
\begin{equation}\label{good-index-bound}
    \Psi^m_{x,l,j} = 0.
\end{equation}

For the bad indices, we utilize \ref{bound-on-phi-nonzero} to get
\begin{equation}\label{bad-indices-bound}
    \|\Psi^m_{x,l,j}\| \leq C2^{-(k-m)l}N^{-1}
\end{equation}
since $a_{u_j},a_{v_j} \leq 2/N$ by \eqref{bound-on-coefficients-small-half}. Next, we bound the number of bad indices. An index is bad if
\begin{equation}\label{eq-391}
    \sign(y\cdot x) \neq \sign(y\cdot \pi_{l-1}(x)),
\end{equation}
for $y = u_j,v_j$ or $w_j$. From the construction of $N_{l-1}$ using Lemma \ref{covering-set-lemma}, the number of $y$ for which \eqref{eq-391} occurs (and thus the number of bad indices) is bounded by $C2^{-l}|S_-| \leq C2^{-l}N$.

Thus, Bernstein's inequality (applied only to the bad indices) gives the following bound on the components of the random variable $E^m_{x,l}$,
\begin{equation}\label{tail-bound-estimate-466}
    \mathbb{P}\left((E^m_{x,l})_\textbf{i} \geq \alpha M_l^m \right) \leq e^{-\alpha^2/2}
\end{equation}
for all $\textbf{i}\in I^m$, where
\begin{equation}
    M_l^m = C2^{-\left(k-m+\frac{1}{2}\right)l}N^{-1/2}.
\end{equation}
The same bound holds also for the negative tails. 

The proof is completed via the following calculation (see \cite{matouvsek1996improved,matouvsek1996discrepancy} for similar calculations). We let $\alpha,\beta > 0$ be parameters to be specified in a moment and define a function
\begin{equation}
    \Lambda_{\alpha,\beta}(x) = \begin{cases}
        2^{\alpha x} & x\geq 0\\
        2^{\beta x} & x\leq 0.
    \end{cases}
\end{equation}
Let $\kappa > 0$ be another parameter and set $\tau$ to be the largest integer satisfying $2^{d\tau} \leq \kappa N$. We set the discretization parameter to be
\begin{equation}\label{equation-426}
    \Delta^m_l = 2M_l^m\Lambda_{\alpha,\beta}(l - \tau).
\end{equation}

Fix an $m$ with $0\leq m\leq k$. We calculate 
\begin{equation}
\begin{split}
     \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l &\leq CN^{-1/2}\sum_{i=0}^{k-m} \sum_{l=-\infty}^\infty 2^{-il}2^{-\left(k-m-i+\frac{1}{2}\right)l} \Lambda_{\alpha,\beta}(l - \tau)\\
     &\leq CN^{-1/2} \sum_{l=-\infty}^\infty 2^{-\left(k-m+\frac{1}{2}\right)l} \Lambda_{\alpha,\beta}(l - \tau),
\end{split}
\end{equation}
since all of the terms in the sum over $i$ are the same.
Making a change of variables in the last sum, we get
\begin{equation}
    \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l \leq CN^{-1/2}2^{-\left(k-m+\frac{1}{2}\right)\tau}\sum_{l=-\infty}^\infty 2^{-\left(k-m+\frac{1}{2}\right)l} \Lambda_{\alpha,\beta}(l). 
\end{equation}
If we now choose $\alpha$ and $\beta$ such that the above sum over $l$ converges (this will happen as long as $\alpha < k-m+\frac{1}{2} < \beta$), then we get
\begin{equation}
     \sum_{l=1}^L \sum_{i=0}^{k-m} 2^{-il} \Delta^{m+i}_l \leq CN^{-\frac{1}{2} - \frac{2(k-m)+1}{2d}},
\end{equation}
since by construction $2^\tau \geq (1/2)(\kappa N)^{1/d}$ (note the constant $C$ depends upon the choice of $\kappa$ which will be made shortly). This verifies \eqref{condition-on-delta-k}.

To verify the entropy condition, we calculate
\begin{equation}\label{G-entropy-bound}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l}\sum_{\textbf{i}\in I^m} H(b^m_{x,l}(\epsilon)_{\textbf{i}})
\end{equation}
using subadditivity of the entropy. We now use Lemma \ref{entropy-tail-bound-lemma} combined with the tail bound estimate \eqref{tail-bound-estimate-466} to get
\begin{equation}
    H(b^m_{x,l}(\epsilon)_{\textbf{i}}) \leq G(\Delta_l^m/(2M_l^m)) = G(\Lambda_{\alpha,\beta}(l-\tau)).
\end{equation}
Using that $|I^m| \leq C = C(d,k)$ and $|N_l| \leq C2^{dl}$, we get that
\begin{equation}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq C\sum_{l=1}^L 2^{dl}G(\Lambda_{\alpha,\beta}(l-\tau)) \leq C\sum_{l=\infty}^\infty 2^{dl}G(\Lambda_{\alpha,\beta}(l-\tau)).
\end{equation}
Finally, making another change of variables, we get
\begin{equation}
    \sum_{l=1}^L\sum_{m=0}^k\sum_{x\in N_l} H(b^m_{x,l}(\epsilon)) \leq C2^{d\tau}\sum_{l=\infty}^\infty 2^{dl}G(\Lambda_{\alpha,\beta}(l)) \leq C\kappa N,
\end{equation}
since it is easy to verify that the above sum over $l$ converges for any $\alpha,\beta > 0$. Choosing $\kappa$ sufficiently small so that $C\kappa \leq 1/60$ will guarantee that the condition \eqref{entropy-sum-bound-354} is satisfied (since $t \geq N/12$ by \eqref{number-of-P-lower-bound}) and this completes the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{relu-k-representation-lemma}]
        We first prove that for $m = 0,...,k$, $y\in S^d$ and $l=1,...,L$ we have
        \begin{equation}\label{sigma-l-decomposition-reluk}
        \sigma_k^{(m)}(x_l,y) = \sum_{j=1}^l\phi_{x_j,j}^m(y) + \sum_{i=1}^{k-m}\sum_{j=1}^{l-1}\left\langle \phi_{x_j,j}^{m+i}(y), \Gamma_{i,j}^{m,l}\right\rangle,
    \end{equation}
    where the tensors $\Gamma_{i,j}^{m,l}$ satisfy the bound
    \begin{equation}\label{bound-on-gamma-tensors}
        \left\|\Gamma_{i,j}^{m,l}\right\| \leq C2^{-ij},
    \end{equation}
    for a constant $C = C(d,k)$. 

        We prove this by (reverse) induction on $m$. Let $0\leq m \leq k$ and suppose that \eqref{sigma-decomposition-reluk} holds for $m+1,...,k$ (note if $m=k$ there is no assumption here). We will show that it also holds for $m$. Expanding out the Taylor polynomial in the definition of $\phi_{x,l}^m$ for $x = x_l$ we see that
        \begin{equation}
        \begin{split}
            \sigma_k^{(m)}(x_l,y) &= \phi_{x_l,l}^m(y) + \mathcal{T}^{m,k-m}_{x_{l-1}}(x_l,y)\\ &= \phi_{x_l,l}^m(y) + \sum_{q=0}^{k-m} \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_{l-1},y), (x_l - x_{l-1})^{\otimes q}\right\rangle\\
            &= \phi_{x_l,l}^m(y) + \sigma_k^{(m)}(x_{l-1},y) + \sum_{q=1}^{k-m} \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_{l-1},y), (x_l - x_{l-1})^{\otimes q}\right\rangle.
        \end{split}            
        \end{equation}
        Applying this expansion recursively to $\sigma_k^{(m)}(x_{l-1},y)$, we get
        \begin{equation}
        \begin{split}
            \sigma_k^{(m)}(x_l,y) = \sum_{j=1}^l \phi_{x_j,j}^m(y) + \sum_{p=1}^{l-1}\sum_{q=1}^{k-m}\frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_{p},y), (x_{p+1} - x_{p})^{\otimes q}\right\rangle.
        \end{split}
        \end{equation}
        Now we use the inductive assumption to expand $\sigma_k^{(m+q)}(x_{p},y)$ using \eqref{sigma-l-decomposition-reluk} and apply the identity \eqref{contraction-tensor-product-identity} to get
        \begin{equation}
        \begin{split}
            \sigma_k^{(m)}(x_l,y) = \sum_{j=1}^l \phi_{x_j,j}^m(y) &+ \sum_{p=1}^{l-1}\sum_{q=1}^{k-m}\frac{1}{q!}\sum_{j=1}^{p}\left\langle \phi_{x_j,j}^{m+q}(y), (x_{p+1} - x_{p})^{\otimes q}\right\rangle\\
            & + \sum_{p=1}^{l-1}\sum_{q=1}^{k-m}\frac{1}{q!}\sum_{i'=1}^{k-m-q}\sum_{j=1}^{p-1} \left\langle \phi_{x_j,j}^{m+q+i'}(y),\Gamma_{i',j}^{m+q,l} \otimes (x_{p+1} - x_p)^{\otimes q}\right\rangle.
        \end{split}
        \end{equation}
        Rearranging this sum, we obtain
        \begin{equation}
            \sigma_k^{(m)}(x_l,y) = \sum_{j=1}^l\phi_{x_j,j}^m(y) + \sum_{i=1}^{k-m}\sum_{j=1}^{l-1}\left\langle \phi_{x_j,j}^{m+i}(y), \Gamma_{i,j}^{m,l}\right\rangle,
        \end{equation}
        where the tensors $\Gamma_{i,j}^{m,l}$ are defined recursively by
        \begin{equation}
            \Gamma_{i,j}^{m,l} = \frac{1}{i!}\sum_{p=j}^{l-1} (x_{p+1} - x_{p})^{\otimes i} + \sum_{p=j}^{l-1} \sum_{q=1}^{i-1}\frac{1}{q!} \Gamma_{i-q,j}^{m+q,l}\otimes (x_{p+1} - x_{p})^{\otimes q}.
        \end{equation}
        Finally, we bound the norm $\|\Gamma_{i,j}^{m,l}\|$. By construction, the points $x_p$ satisfy $|x_{p+1} - x_p| \leq 2^{-p}$ (in the Euclidean norm which bounds the $\ell^\infty$-norm). This gives the bound
        \begin{equation}
            \|\Gamma_{i,j}^{m,l}\| \leq \frac{1}{i!}\sum_{p=j}^{l-1}2^{-pj} + \sum_{p=j}^{l-1} \sum_{q=1}^{i-1}\frac{1}{q!} 2^{-pq}\|\Gamma_{i-q,j}^{m+q,l}\|.
        \end{equation}
        Utilizing the inductive assumption to bound $\|\Gamma_{i-q,j}^{m+q,l}\|$ we get
        \begin{equation}
            \begin{split}
                \|\Gamma_{i,j}^{m,l}\| &\leq \frac{1}{i!}\sum_{p=j}^{l-1}2^{-pj} + C\sum_{p=j}^{l-1} \sum_{q=1}^{i-1}\frac{1}{q!} 2^{-pq}2^{-(i-q)j}\\
                &\leq \frac{1}{i!}\sum_{p=j}^{\infty}2^{-pj} + C2^{-ij}\sum_{p=j}^{\infty} \sum_{q=1}^{\infty}\frac{1}{q!} 2^{-q(p-j)}\leq C2^{-ij},
            \end{split}
        \end{equation}
        for a potentially different constant $C$. However, the induction is completed after a finite number (namely $k+1$) steps. Thus the constant $C = C(d,k)$ can be taken uniform in $m$. This proves \eqref{sigma-l-decomposition-reluk}.

        To prove \eqref{sigma-decomposition-reluk}, we write
        \begin{equation}\label{decomposition-x-610}
        \begin{split}
            \sigma_k^{(m)}(x,y) &= \left[\sigma_k^{(m)}(x,y) - \mathcal{T}^{m,k-m}_{x_L}(x,y)\right] + \mathcal{T}^{m,k-m}_{x_L}(x,y)\\ 
            &= \left[\sigma_k^{(m)}(x,y) - \mathcal{T}^{m,k-m}_{x_L}(x,y)\right] + \sum_{q=0}^{k-m} \frac{1}{q!}\left\langle\sigma_k^{(m+q)}(x_L,y), (x - x_L)^{\otimes q}\right\rangle.
        \end{split}
        \end{equation}
        We claim that if $y\in S_-$, then the first term
        \begin{equation}\label{equation-364-difference-0}
        \sigma_k^{(m)}(x,y) - \mathcal{T}^{m,k-m}_{x_L}(x,y) = 0.
        \end{equation}
        This follows since by construction using Lemma \ref{covering-set-lemma}, we have the bound
        \begin{equation}
            |S_-\cap \{y\in S^d,~\sign{(y\cdot x)} \neq \sign{(y\cdot \pi_L(x))}\}| \leq 2^{-L} |S_-| < 1,
        \end{equation}
        since $2^{-L} < N^{-1}$. Thus for all $y\in S_-$ and $x\in S^d$, we have $\sign{(y\cdot x)} = \sign{(y\cdot \pi_L(x))}$, and the argument following \eqref{phi-bound-zero} implies that the difference in \eqref{equation-364-difference-0} vanishes. 
        
        Next, we expand each term in the sum in \eqref{decomposition-x-610} using \eqref{sigma-l-decomposition-reluk} with $l = L$ to get (using again the identity \eqref{contraction-tensor-product-identity})
        \begin{equation}
            \sigma_k^{(m)}(x,y) = \sum_{q=0}^{k-m}\frac{1}{q!}\sum_{j=1}^L\left\langle\phi_{x_j,j}^{m+q}(y), (x - x_L)^{\otimes q}\right\rangle + \sum_{q=0}^{k-m}\frac{1}{q!}\sum_{i'=1}^{k-m-q}\sum_{j=1}^{L-1}\left\langle \phi_{x_j,j}^{m+q+i'}(y), \Gamma_{i',j}^{m+q,L}\otimes (x-x_L)^{\otimes q}\right\rangle.
        \end{equation}
        Rewriting this, we get
        \begin{equation}
            \sigma_k^{(m)}(x,y) = \sum_{j=1}^L\phi^{m}_{x_j,j}(y) + \sum_{i=1}^{k-m}\sum_{j=1}^L  \left\langle\phi^{m+i}_{x_j,j}(y),\Gamma^{m}_{i,j}(x)\right\rangle,
        \end{equation}
        where the tensors $\Gamma^{m}_{i,j}(x)$ are given by
        \begin{equation}
            \Gamma^{m}_{i,j}(x) = \frac{1}{i!}(x - x_L)^{\otimes i} + \sum_{q=0}^{i-1}\Gamma_{i-q,j}^{m+q,L}\otimes (x-x_L)^{\otimes q}.
        \end{equation}
        Finally, we bound the norm $\|\Gamma^{m}_{i,j}(x)\|$. Utilizing that $|x - x_L| \leq 2^{-L}$ and the bound \eqref{bound-on-gamma-tensors} we get
        \begin{equation}
            \|\Gamma^{m}_{i,j}(x)\| \leq \frac{2^{-Li}}{i!} + C\sum_{q=0}^{i-1} 2^{-(i-q)j}2^{-Lq} \leq C2^{-ij},
        \end{equation}
        for a constant $C(d,k)$, since $j\leq L$ and $0\leq i \leq k$. Upon relabelling $j$ to $l$ this is exactly Lemma \ref{relu-k-representation-lemma}.
    \end{proof}

\section{Acknowledgements}
We would like to thank Ron DeVore, Rob Nowak, Jinchao Xu, and Rahul Parhi for helpful discussions. This work was supported by the National Science Foundation (DMS-2111387 and CCF-2205004).

\bibliographystyle{amsplain}
\bibliography{refs}
\end{document}
