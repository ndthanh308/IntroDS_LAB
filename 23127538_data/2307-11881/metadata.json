{
  "title": "Selecting the motion ground truth for loose-fitting wearables: benchmarking optical MoCap methods",
  "authors": [
    "Lala Shakti Swarup Ray",
    "Bo Zhou",
    "Sungho Suh",
    "Paul Lukowicz"
  ],
  "submission_date": "2023-07-21T19:37:23+00:00",
  "revised_dates": [
    "2023-07-26T00:13:47+00:00"
  ],
  "abstract": "To help smart wearable researchers choose the optimal ground truth methods for motion capturing (MoCap) for all types of loose garments, we present a benchmark, DrapeMoCapBench (DMCB), specifically designed to evaluate the performance of optical marker-based and marker-less MoCap. High-cost marker-based MoCap systems are well-known as precise golden standards. However, a less well-known caveat is that they require skin-tight fitting markers on bony areas to ensure the specified precision, making them questionable for loose garments. On the other hand, marker-less MoCap methods powered by computer vision models have matured over the years, which have meager costs as smartphone cameras would suffice. To this end, DMCB uses large real-world recorded MoCap datasets to perform parallel 3D physics simulations with a wide range of diversities: six levels of drape from skin-tight to extremely draped garments, three levels of motions and six body type - gender combinations to benchmark state-of-the-art optical marker-based and marker-less MoCap methods to identify the best-performing method in different scenarios. In assessing the performance of marker-based and low-cost marker-less MoCap for casual loose garments both approaches exhibit significant performance loss (>10cm), but for everyday activities involving basic and fast motions, marker-less MoCap slightly outperforms marker-based MoCap, making it a favorable and cost-effective choice for wearable studies.",
  "categories": [
    "cs.CV",
    "cs.GR"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": "ACM ISWC 2023",
  "arxiv_id": "2307.11881",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 10988768,
  "size_after_bytes": 734504
}