%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
%\documentclass[manuscript,screen,review]{acmart}
\documentclass[sigconf]{acmart}
% \documentclass[sigconf,anonymous,review]{acmart}
%\documentclass[sigconf,anonymous]{acmart}
% \usepackage[capitalise]{cleveref}
\usepackage[capitalise, noabbrev]{cleveref}

%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ISWC]{ISWC'23: ACM International Symposium on Wearable Computer}{ October 08--12,
  2023}{Cancun, Mexico}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
\acmBooktitle{ISWC'23: ACM International Symposium on Wearable Computers,
 October 08--12, 2023, Cancun, Mexico} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}



\acmSubmissionID{7420}
%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Selecting the motion ground truth for loose-fitting wearables: benchmarking optical MoCap methods}%Are low-cost marker-less vision MoCap methods sufficient as ground truth with loose garments?}
%Are expensive MoCap systems suitable for body pose ground truth with loose garments?} %Are low-cost marker-less MoCap methods sufficient for ground truth in loose garment?}
%DrapedMoCapBench: Benchmarking 3D Human Pose Estimation Methods on different Fitting Clothes using 3D Simulated Data

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz}
\affiliation{%
  \institution{German Reserch Center for Artificial Intelligence (DFKI) and University of Kaiserslautern, Germany}  
  \country{}
  \streetaddress{Trippstadterstr. 122}
  %\city{Kaiserslautern}
  \postcode{67663}
}
\email{{lala\_shakti\_swarup.ray, bo.zhou, sungho.suh, paul.lukowicz}@dfki.de}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Ray and Zhou, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

To help smart wearable researchers choose the optimal ground truth methods for motion capturing (MoCap) for all types of loose garments, 
we present a benchmark, DrapeMoCapBench (DMCB), specifically designed to evaluate the performance of optical marker-based and marker-less MoCap. 
High-cost marker-based MoCap systems are well-known as precise golden standards.
However, a less well-known caveat is that they require skin-tight fitting markers on bony areas to ensure the specified precision, making them questionable for loose garments.
On the other hand, marker-less MoCap methods powered by computer vision models have matured over the years, which have meager costs as smartphone cameras would suffice.
To this end, DMCB uses large real-world recorded MoCap datasets to perform parallel 3D physics simulations with a wide range of diversities: six levels of drape from skin-tight to extremely draped garments, three levels of motions and six body type - gender combinations to benchmark state-of-the-art optical marker-based and marker-less MoCap methods to identify the best-performing method in different scenarios.
%Then, state-of-the-art optical marker-based and marker-less MoCap methods were applied to the simulation to identify the best-performing method in different scenarios.
%Our evaluation shows that neither marker-based or marker-less methods achieve a minimum position error of <10cm.
In assessing the performance of marker-based and low-cost marker-less MoCap for casual loose garments both approaches exhibit significant performance loss (>10cm), but for everyday activities involving basic and fast motions, marker-less MoCap slightly outperforms marker-based MoCap, making it a favorable and cost-effective choice for wearable studies. The code is available at \texttt{github.com/lalasray/DMCB/}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010341.10010342.10010344</concept_id>
       <concept_desc>Computing methodologies~Model verification and validation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Model verification and validation}%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{motion capturing benchmark, physics-based cloth-motion simulation, quantitative characterization}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
 % % Figure removed
 % \caption{Seattle Mariners at Spring Training, 2010.}
 % \Description{Enjoying the baseball game from the third-base
 % seats. Ichiro Suzuki preparing to bat.}
 % \label{fig:teaser}
%\end{teaserfigure}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\vspace{-3pt}
\label{sec:intro}
Wearable sensing systems have gained a growing interest towards motion tracking, including IMU sensors \cite{gong2021robust, jiang2022transformer, yi2022physical}, RFIDs \cite{jin2018towards}, capacitive fabric sensors \cite{zhou2023mocapose}, computational fabrics \cite{liu2019reconstructing}, and multi-modalities \cite{Liu2020A}.
With continuous motion tracking, activity recognition in various scenarios is trivial downstream tasks \cite{behera2020deep, jansen20073d} along with the shared representation with other domains such as computer vision and large language models \cite{radford2021learning,moon2022imu2clip}.

However, the widely accepted golden standard for motion capture (MoCap) systems, such as Qualisys (Sweden), Vicon (USA), and OptiTrack (USA), relies on optical markers placed on the body \cite{jiang2022transformer, yi2022self}. These systems employ skin-tight marker placement on bony areas and rely on rigid biomechanical models to convert the surface points to inner joints \cite{skeletontracking, groen2012sensitivity}.
% However, the consensus golden standard ground truth MoCap systems, such as Qualisys (Sweden), Vicon (USA), and OptiTrack (USA), based on optical markers are specified with skin-tight marker placement and even on bony areas because they rely on rigid biomechanical models to convert the surface points to inner joints \cite{skeletontracking, groen2012sensitivity}.
Optical marker-based MoCap utilizes either active markers \cite{barca2006new,raskar2007prakash} with built-in light sources or passive markers \cite{lee2017low} with unique visual patterns or retro-reflective properties. 
These systems capture the surface marker positions using synchronized camera triangulation and infer the joint motion inside the human body using biomechanical models \cite{skeletontracking}. 
Optical MoCap systems are generally favored over inertial methods (that lack absolute positioning) due to their simplicity, accuracy, and robustness against external interference  \cite{fleron2019accuracy}. %and real-time tracking
However, when the markers are placed over a loose piece of garment, they are not able to follow the underlying body motion, which results in significant kinematic errors \cite{mcfadden2020sensitivity}. %as stated by McFadden et al.
The development of loose-fitting is crucial in wearable applications \cite{zhou2023mocapose, bello2021mocapaci, mcadams2011wearable} to improve user acceptance, comfort, accommodation of various body shapes, and mass adoption. 
Nevertheless, relying on marker-based MoCap to provide motion ground truth constraints further the development of loose garments.
% Constraints related to marker-based mocap limits further development of loose garments, 
%On one hand, this limits further development of loose garments, which is crucial for user acceptance; 
% On the other, studies with loose garments like casual apparel, where the optical markers need to be exposed on the outside of the garment, are in practice outside the specification of those systems.
Video-based marker-less MoCap deep learning algorithms map semantic information (e.g., body parts) to pose without explicit markers using deep learning \cite{sigal2021human, chatzis2020comprehensive,gamra2021review} have matured with the rapid advancement of artificial intelligence.
But there is a lack of comprehensive comparisons between marker-based and marker-less MoCap systems, especially considering loose garments.

% of how they compare to marker-based MoCap systems, especially in loose garments.
%Marker-less MoCap takes a different approach and utilizes DL models trained on annotated datasets to directly infer the 3D pose of individuals from input images or videos. By leveraging DL, the models learn to map semantic information to pose without explicit markers.
%Numerous marker-less DL MoCap models have been proposed for different types of motions, with marker-based MoCap systems used as the ground truth \cite{sigal2021human, chatzis2020comprehensive,gamra2021review}.
% There is extensive research on different marker-less DL MoCap models \cite{sigal2021human, chatzis2020comprehensive,gamra2021review} for various types of motions using marker-based MoCap as the ground truth.
% However, most of these studies do not consider that loose garments may compromise the pose from marker-based MoCap. 

Several studies comparing marker-based and marker-less MoCap in applications such as controlling an endoscopic instrument \cite{reilink20133d}, baseball pitching biomechanics \cite{fleisig2022comparison}, gait analysis \cite{kanko2021concurrent}, and clinical usability \cite{Ancans2021Wearable} have found that while marker-based MoCap generally exhibits slightly higher accuracy, marker-less systems have the potential to serve as a viable alternative, especially in clinical settings where patient comfort and ease of use are crucial factors \cite{nakano2020evaluation}. 

% Studies to compare marker-based and marker-less pose estimation has been conducted in the past using various application as the basis of comparison like controlling an endoscopic instrument\cite{reilink20133d}, baseball pitching biomechanics \cite{fleisig2022comparison}, for gait analysis \cite{kanko2021concurrent}, and clinical usability  \cite{Ancans2021Wearable}. 
% Figure environment removed
%Most of these studies have pointed out that although marker-based MoCap has slightly higher accuracy \cite{nakano2020evaluation}; marker-less systems have the potential to be a viable alternative, particularly for clinical settings where patient comfort and ease of use are essential factors. 
%Because of the absence of ground truth, these studies focus on complexity, ease of use, and overall performance rather than quantitative precision comparison. 
These studies prioritize complexity, ease of use, and overall performance rather than quantitative precision comparison due to the absence of anatomic motion reference, and no evaluation is done considering loose garments to the level of casual apparel.
However, to conduct such a comparison, especially with loose garments, there are two requirements that cannot be achieved in reality:
\begin{enumerate}
    \item Anatomic true motion under the garment and skin is required to quantitatively compare MoCap methods, which cannot be captured in reality through non-invasive methods. %, because even marker-based MoCap uses biomechanical approximation from surface markers.
    \item The exact motion sequences need to be reproduced precisely in scenarios with individuals of different body shapes wearing different garments.
\end{enumerate}
% (1) The anatomical true motion underneath the garment and skin is required to quantitatively compare different MoCap methods, which is unknown in the real world because even marker-based MoCap uses biomechanical approximation from surface markers.
% (2)  The exact motion sequences need to be reproduced precisely in multiple scenarios with persons of different body shapes wearing different garments.
Largely due to these challenges, existing quantitative reviews of marker-less methods use marker-based MoCap as reference \cite{wang2021deep}, which itself has substantial error from the anatomic joints due to the biomechanical approximation.
To address these issues, we leverage 3D physics-based simulation to benchmark the impossible.
We use real-world captured motion datasets to generate the inputs required for marker-based and marker-less MoCap methods, thus quantitatively comparing them to the common anatomic true motion. 
In particular, we make the following contributions: 
\begin{enumerate}
    \item A garment and soft body physics simulation evaluate marker-based and marker-less MoCap performances while persons with different body types repeat the same motions wearing different garments in terms of drape. Using real-world captured motion datasets to generate the input required for both MoCap methods, we quantitatively compare them to the common anatomical true motion.
    \item Our benchmark involves diverse varieties of motion types and garment drape levels which, together with a holistic comparison, can assist practitioners in choosing the optimal MoCap for wearable experiment ground truth for their specific applications balancing aspects such as garment designs, types of motion, cost and time overhead, and precision.
\end{enumerate}
% (1) A garment and soft body physics simulation evaluates marker-based and marker-less MoCap performances while persons with different body types repeat the same motions wearing different garments in terms of drape. Using real-world captured motion datasets to generate the input required for both MoCap methods, we quantitatively compare them to the common anatomical true motion. %, which is impossible through real-world captures. %. This solves the problem of needing to have ground truth pose in a real dataset. Reality check through 3D physics-based simulation: w

% (2) We quantitatively compared the MoCap methods in diverse varieties of motion types and garment drape levels. Together with a holistic comparison, our benchmark can assist practitioners in choosing the optimal MoCap for wearable experiment ground truth for their specific applications balancing aspects such as garment designs, types of motion, cost and time overhead, and precision.
% by three motion types (daily activity, fast, extreme joint) and six quantified drape levels. 
%The benchmark can be adapted to new motions and garments not included in our benchmark datasets. %We present a framework to translate MoCap data into videos of virtual persons wearing simulated clothes embedded with optical markers.
%\vspace{-8pt}



%\section{Background}
\vspace{-3pt}
% \vspace{-8pt}
\section{Proposed Method}
\vspace{-2pt}
The overall framework of our benchmark methodology is shown in \cref{fig:pipeline}.
By leveraging 3D physics simulation, we solve the reality challenge that the exact motion cannot be perfectly reproduced to establish quantitative comparisons of different scenarios.% with different MoCap methods.

\vspace{-8pt}
\subsection{DrapeMoCapBench Pipeline}

The simulation pipeline strictly adheres to reality, as the inputs to all MoCap methods are true to their specifications: 3D surface marker locations for marker-based kinematic methods and 1080p image sequences for marker-less vision models.

\vspace{-5pt}
\subsubsection{3D physics simulation}
With Blender3D \cite{blender}, motion sequences from \cref{sec:mocap_dataset} were converted to volumetric human bodies of different builds with the help of SMPL-X blender addon \cite{pavlakos2019expressive}, then dressed in garments described in \cref{sec:clothes}.
All simulated garments are assigned cloth properties equivalent to that of woven cotton (un-stretchable) with vertex mass of 0.05 kg, stiffness tension and compression of 15, stiffness and damping bending of 0.5, damping tension, compression, and shear 5, and stiffness shear of 10.
Doubled layered cloth mesh and improved body-cloth collision provided in Simplycloth \cite{simplycloth} along with soft tissue dynamics over captured skeletal motions using Mosh++\cite{loper2014mosh} enabled us to introduce realistic deformation of the garments over volumetric human models performing dynamic activities while having minimal artifacts.
Then inputs for optical MoCap were derived from the 3D scenes of parallel simulations of the same underlying motion as described in \cref{sec:mocap_methods}.

\vspace{-5pt}
\subsubsection{Motion Source Dataset}
\label{sec:mocap_dataset}
We used the AMASS framework \cite{mahmood2019amass} for converting MoCap data from various sources and formats to a standardized format based on the SMPL \cite{SMPL:2015} body model, a 3D model that accurately represents the human body. 
We considered three types of motion. First, the basic motions, such as walking and interaction, are derived from the HumanEva \cite{sigal2010humaneva} and TotalCapture \cite{trumble2017total} dataset containing 20 minutes of motion sequences. % 30 samples of total of 36210 frames. 
Second, the fast motions, including sports, dancing, etc., are derived from DanceDB \cite{dance_db} and Totalcapture containing 40 minutes of motion. %33 samples of a total of 70985 frames. 
Finally, the motions with extreme joint bending, like Yoga and gymnastics, are derived from PosePrior \cite{akhter2015pose}, and PresSim \cite{ray2023pressim} dataset containing 37 minutes of motion. % 36 samples having a total of 66560 frames.

% Figure environment removed


%\begin{table}
%\footnotesize
%\centering
%\caption{Drape class categorization for different cloth, gender, build combinations.}
%\vspace{-10pt}
%\label{tab:draping}
%\begin{tabular}{ccccc}
%\toprule
%\multicolumn{2}{c}{Drape Classes} & Cloth & \multicolumn{2}{c}{Drape} \\
%\cmidrule{4-5}
%Male & Female & &  Male & Female\\
%\midrule
%1-1-1 & 1-1-1 & Sleeveless & 0.10-0.08-0.07 & 0.09-0.08-0.07\\
%2-1-1 & 2-1-1 & T-Shirt & 0.12-0.09-0.07 & 0.11-0.10-0.07\\
%3-2-1 & 2-2-1 & Shorts & 0.21-0.14-0.10 & 0.17-0.13-0.09\\
%3-2-2 & 2-2-1 & Skirt & 0.24-0.17-0.12 & 0.19-0.14-0.10\\
%3-2-2 & 2-2-2 & Shirt & 0.25-0.21-0.18 & 0.19-0.18-0.16\\
%3-2-2 & 3-2-2 & Dress & 0.21-0.19-0.16 & 0.21-0.19-0.17\\
%3-3-3 & 3-3-3 &Trousers & 0.28-0.25-0.23 & 0.29-0.26-0.24\\
%3-3-3 & 4-3-3 & Jacket & 0.29-0.27-0.26 & 0.31-0.28-0.27\\
%4-3-3 & 4-4-3 & Hoodie & 0.35-0.29-0.26 & 0.36-0.31-0.28\\
%4-4-3 & 4-4-3 & Cardigan & 0.33-0.31-0.29 & 0.32-0.31-0.28\\
%5-4-4 & 5-4-4 & Cargo & 0.41-0.36-0.33 & 0.41-0.38-0.35\\
%6-5-5 & 5-5-5 & Robes & 0.51-0.46-0.43 & 0.48-0.45-0.44\\
%6-5-5 & 6-5-5 & Trench Coat & 0.53-0.50-0.48 & 0.53-0.50-0.47\\
%\bottomrule
%\multicolumn{5}{c}
%{*For each 'male' and 'female', we separate by small - average - heavy build.}\\
%\end{tabular}
%\vspace{-10pt}
%\end{table}
\vspace{-5pt}
\subsubsection{Quantifying drape of loose garments}
\label{sec:clothes}
We used 3D assets for a broad selection of apparel from commonly available categories for both genders using the Simplycloth\cite{simplycloth} plugin with garments ranging from skin-tight (minimal drape) to very loose (maximal drape). They are assigned into one of six classes based on the drape percentage, as depicted in \cref{fig:drape}. 
Drape amount is calculated as the percentage of the extra volume occupied by the garment as compared to another garment that fits the body perfectly and has the volume underlying the covered body as visualized in \cref{fig:overview}:
$
Drape = \frac{{Volume_{\text{garment}} - Volume_{\text{CoveredBody}}}}{{Volume_{\text{CoveredBody}}}}
$.
%It calculates the percentage of extra volume required by a garment as compared to another garment that fits the body perfectly.
For all garments except uni-cloths, a piece for the upper body and lower body for a particular build sharing combined drape class from 1 to 6 is selected to dress the SMPL body mesh.

\vspace{-5pt}
\subsection{MoCap methods}
\vspace{-3pt}
\label{sec:mocap_methods}
State-of-the-art marker-based and marker-less MoCap methods were implemented with the benchmark pipeline.
\vspace{-3pt}
\subsubsection{Marker-based}
%Although its very hard to say whether optical MoCap systems or inertial ones would perform better over loose garments, we choose to benchmark the former purely based on the advantage of better accuracy and possibility of absolute positioning. Inertial sensors get affected by noise and drift over time, which is rampant when used over loose garments.
Regardless of the marker principle, they return a 3D coordinate.
  % We assumed our markers to be active with an expected triangulation accuracy range of 5mm
A marker set of 24 marker pairs (48 markers) associated with the 24 joints from the SMPL skeleton is either attached over the garments or the skin, depending on whether their original position is covered by the garment in T-pose. 
%From each marker pairs $(x_1, y_1, z_1)$ and $(x_2, y_2, z_2)$  to calculate the position of each joint, we used
%$ Joint =  \frac{{x_1 + x_2}}{2}, \frac{{y_1 + y_2}}{2}, \frac{{z_1 + z_2}}{2} $.
This represents the optimal real-world marker placement, and 24 marker pairs are sufficient, as the simulation retrieves the coordinates directly without accounting for occlusion and triangulation from multiple cameras.
5 mm error was added as Gaussian noise to the coordinate according to the best-performing solutions \cite{merriaux2017study,maletsky2007accuracy,van2018accuracy}.
%The extracted 3D marker position is then normalized to have a range from -0.1 to 0.1.
%The origin of the skeleton is moved to the pelvic joint to match the output of the marker-less MoCap before being translated back 
The surface markers are converted to Bio-Vision Hierarchy (BVH) MoCap files having SMPL skeleton hierarchy using forward kinematics to approximate joints' absolute position and angle.
\vspace{-5pt}
\subsubsection{Marker-less}
Two marker-less models were considered: a temporal semi-supervised 3d pose estimation model VideoPose3D \cite{pavllo20193d} and a lightweight real-time 3d pose estimation model BlazePose3D \cite{bazarevsky2020blazepose}.
Videos (1920$\times$1080) were rendered from the simulation scene, then fed into Detectron2\cite{wu2019detectron2} + VideoPose3D or BlazePose3D to extract multi-joint poses relative to the video frame. They are then rescaled to the original size of the body (170cm height) and converted to BVH files.
%to extract the 17-joint 3D poses, which are mapped back to 24 joint SMPL poses using correspondence mapping and transformation estimation and then converted to BVH MoCap files containing SMPL skeleton hierarchy. % but having a range from -0.1 to 0.1 
%Similarly, we also calculated the BVH MoCap files from the 33-joint skeleton of BlazePose3D.



% Figure environment removed


\vspace{-6pt}
\subsection{Evaluation Metrics}
\vspace{-3pt}
We consider two frequently used metrics in the MoCap field: absolute Mean Per Joint Position Error (MPJPE), which provides a quantitative measure of the accuracy of 3D joint positions and is more often used in animation, and Circular Root Mean Squared Error (CRMSE), that assesses the performance of pose joint angle estimation used primarily in sports and medical research on joint angles.
They are defined as follows: 
$ MPJPE = \frac{1}{n}\sum_{i=1}^{n} || \mathbf{P}_i - \mathbf{\hat{P}}_i || $
where $n$ is the number of joints, $\mathbf{P}_i$ is the ground truth position of the $i$-th joint, $\mathbf{\hat{P}}_i$ is the estimated position of the $i$-th joint, and $|| \cdot ||$ denotes the Euclidean distance. 
$
CRMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \left(1 - \cos(\theta_i - \hat{\theta_i})\right)}
$
Here, N represents the total number of joint angles, \(\theta_i\) represents the ground truth angle for the $i$-th joint, and \(\hat{\theta_i}\) represents the corresponding predicted angle.  
Due to the availability of comprehensive measurements about human models and garments obtained from the simulation, it is straightforward to calculate MPJPE using the 3D estimated joints in Euler space. On the other hand, the CRMSE involves estimating joint angles by applying forward kinematics and then computing the error.



%\def\originaltable{0}
%\if\originaltable1
%\begin{table}[!t]
%\footnotesize
%\centering
%\caption{Benchmark results of marker-based and marker-less MoCap methods in different drape and motion combinations.}
%  \vspace{-10pt}
%  \label{tab:benchresult}
%\begin{tabular}{ccc}
%\toprule
% Drape class & \multicolumn{2}{c}{Marker-based (error +-5 mm) / VideoPose3D / BlazePose3D} \\
%\cmidrule{2-3}
% & MPJPE ($\downarrow$ norm. dist. (in cm)) & CRMSE ($\downarrow$ angle (in degree)) \\
%\midrule
%\multicolumn{3}{c}{Basic motions}\\
%\midrule
%1 & \textbf{0.134} / 0.188 / 0.210 & \textbf{4.3} / 4.4 / 4.4\\
%2 & \textbf{0.188} / 0.285 / 0.325 & \textbf{6.8} / 7.0 / \textbf{6.8}\\
%3 & \textbf{0.361} / 0.402 / 0.503 & 9.8 / 9.8 / \textbf{9.7}\\
%4 & \textbf{0.510} / 0.543 / 0.622 & 13.6 / \textbf{13.1}/13.4\\
%5 & \textbf{0.698} / 0.718 / 0.820 & 21.9 / \textbf{18.9} / 20.2\\
%6 & 0.919 / \textbf{0.912} / 1.121 & 35.7 / \textbf{25.8} / 28.1\\
%\midrule
%\multicolumn{3}{c}{Fast motions}\\
%\midrule
%1 & \textbf{0.131} / 0.192 / 0.207 & 4.7 / 4.5 / \textbf{4.3}\\
%2 & \textbf{0.210} / 0.290 / 0.346 & 7.1 / \textbf{6.6} / 6.8\\
%3 & 0.412 / \textbf{0.408} / 0.544 & 11.4 / \textbf{10.8} / 11.2\\
%4 & 0.651 / \textbf{0.566} / 0.697 & 18.2 / \textbf{16.4} / 16.2\\
%5 & 0.915 / \textbf{0.811} / 0.901 & 26.5 / \textbf{21.6} / 24.0\\
%6 & 1.140 / \textbf{1.006} / 1.208 & 43.6 / \textbf{29.2} / 32.1\\
%\midrule
%\multicolumn{3}{c}{Extreme joint motions}\\
%\midrule
%1 & \textbf{0.138} / 0.263 / 0.301 & \textbf{4.2} / 8.3 / 9.7\\
%2 & \textbf{0.192} / 0.401 / 0.525 & \textbf{7.1} / 11.1 / 14.5\\
%3 & \textbf{0.341} / 0.601 / 0.725 & \textbf{10.4} / 17.8 / 20.2\\
%4 & \textbf{0.496} / 0.712 / 0.829 & \textbf{14.5} / 20.8 / 24.1\\
%5 & \textbf{0.651} / 0.901 / 1.004 & \textbf{23.6} / 29.2 / 32.8\\
%6 & \textbf{0.892} / 1.108 / 1.190 & 38.3 / \textbf{36.9} / 42.1\\
%\bottomrule
%\end{tabular}
%\end{table}

%\else
%\begin{table}[!t]
%\footnotesize
%\centering
%\caption{Benchmark results of marker-based and marker-less MoCap methods in different drape and motion combinations.}
  %\vspace{-12pt}
 % \label{tab:benchresult}
%\begin{tabular}{c|c|c|c|c|c|c}
%\toprule
 %Drape& \multicolumn{2}{c}{Marker-based} & \multicolumn{4}{c}{Markerless}\\
%\cmidrule{4-7}
%class& \multicolumn{2}{c}{(error $\pm$ 5 mm)} & \multicolumn{2}{c}{VideoPose3D} & \multicolumn{2}{c}{Blazepose}\\
%\midrule
%& MPJPE & CRMSE & MPJPE & CRMSE & MPJPE & CRMSE\\
%& cm($\downarrow$) & degree($\downarrow$) & cm($\downarrow$) & degree($\downarrow$) & cm($\downarrow$) & degree($\downarrow$)\\
%\midrule
%\multicolumn{7}{c}{1. Basic motions}\\
%\midrule
%1 & \textbf{0.134} & \textbf{4.3} & 0.188 & 4.4 & 0.210 & 4.4\\
%2 & \textbf{0.188} & \textbf{6.8} & 0.285 & 7.0 & 0.325 & \textbf{6.8} \\
%3 & \textbf{0.361} & 9.8 & 0.402 & 9.8 & 0.503 & \textbf{9.7} \\
%4 & \textbf{0.510} & 13.6 & 0.543 & \textbf{13.1} & 0.622 & 13.4 \\
%5 & \textbf{0.698} & 21.9 & 0.718 & \textbf{18.9} & 0.820 & 20.2 \\
%6 & 0.919 & 35.7 & \textbf{0.912} & \textbf{25.8} & 1.121 & 28.1 \\
%\midrule
%\multicolumn{7}{c}{2. Fast motions}\\
%\midrule
%1 & \textbf{0.131} & 4.7 & 0.192 & 4.5 & 0.207 & \textbf{4.3}\\
%2 & \textbf{0.210} & 7.1 & 0.290 & \textbf{6.6} & 0.346 & 6.8\\
%3 & 0.412 & 11.4 & \textbf{0.408} & \textbf{10.8} & 0.544 & 11.2\\
%4 & 0.651 & 18.2 & \textbf{0.566} & \textbf{16.4} & 0.697 & 16.2\\
%5 & 0.915 & 26.5 & \textbf{0.811} & \textbf{21.6} & 0.901 & 24.0\\
%6 & 1.140 & 43.6 & \textbf{1.006} & \textbf{29.2} & 1.208 & 32.1\\
%\midrule
%\multicolumn{7}{c}{3. Extreme angle motions}\\
%\midrule
%1 & \textbf{0.138} & \textbf{4.2} & 0.263 & 8.3 & 0.301 & 9.7\\
%2 & \textbf{0.192} & \textbf{7.1} & 0.401 & 11.1 & 0.525 & 14.5\\
%3 & \textbf{0.341} & \textbf{10.4} & 0.601 & 17.8 & 0.725 & 20.2\\
%4 & \textbf{0.496} & \textbf{14.5} & 0.712 & 20.8 & 0.829 & 24.1\\
%5 & \textbf{0.651} & \textbf{23.6} & 0.901 & 29.2 & 1.004 & 32.8\\
%6 & \textbf{0.892} & 38.3 & 1.108 & \textbf{36.9} & 1.190 & 42.1\\
%\bottomrule
%\end{tabular}
%\vspace{-15pt}
%\end{table}
\section{Results and Discussion}
\vspace{-3pt}
With the unclothed body, the MPJPE between marker-based and marker-less implementations of basic motion tested on the TotalCapture dataset is 4.7 cm, and extreme angle motions tested on the PosePrior dataset is 8.2 cm.
These results quantitatively align with the literature comparing marker-less models with marker-based mocap as reference \cite{kanazawa2018end, qiu2019cross, wang2021deep, ostrek2019existing}. 
The MPJPE and CRMSE between different MoCap implementations and the anatomic joints are detailed in \cref{fig:benchmark}. We calculated MPJPE for both markers placed on cloth only as well as the entire marker set for marker-based MoCap method. 
%Among the 18 combinations of 3 motion types and six drape levels, we can expect that in 16 cases, both marker-based and marker-less methods will provide less than 1cm joint position error. 
%The differences between marker-based and marker-less methods are generally within 0.1cm MPJPE.
%Exceptions are the extremely loose (drape class 6) with fast or extreme joint motions, which still see a maximum 1.1cm error from both approaches. 
The minimum joint-position error is unsurprisingly from drape class 1 with marker-based methods, which is still >10 cm.
Such comparison has only been possible before with our simulation pipeline, as the anatomic joint coordinates cannot be derived in reality with non-invasive superficial methods like surface markers or video analysis as explained in \cref{sec:intro}.
% Figure environment removed
The term `looseness' is highly subjective, and its interpretation depends on the relative volume of the garment as compared to the wearer. 
%For instance, a shirt that is normally sized may be perceived as loose if worn by a slender individual. 
To account for this variability, we employed a quantitative measurement of drape and organized our findings into drape classes. 
Everyday loose garments that effectively follow the wearer's body motion typically belong to drape class 2 or 3. 
In this range, either marker-based or marker-less gives 15cm to 35cm MPJPE and 6° to 11° CRMSE.
Absolute MPJPE is susceptible to joint hierarchy alignment, including shifting and rotation errors, while CRMSE is affected by the relative angle of bone in terms of its parent. As expected bio-mechanical constraints the MPJPE overall marker set as compared to where it is calculated for markers placed only over the garment in marker-based MoCap.
However, there are extreme cases, such as robes and trench coats, where the garments exhibit limited adherence to the wearer's motion.
All methods see gradually increasing errors with more drape of the garment.%; however, the rate of degradation of marker-less methods is slightly less for basic motions and fast motions that can be visualized by calculating the difference to skin-tight baselines of each method to quantify the degradation of MoCap accuracy over draping which shows slightly less range of variation for marker-less methods irrespective of motion type.
Marker-less methods (especially VideoPose3D) are more stable as the drape increases, which might be attributed to the fact that they work on detecting semantic segments of body parts.
DL-based marker-less models are expected to perform better on basic and fast motions than on extreme joint angles since they are primarily trained on datasets consisting of the first two.
On the other hand, the marker-based method does not have such limitations, as it uses forward kinematics with biomechanical constraints from the markers. % rather than a trained DL model.
Apart from the quantitative benchmark results, we also compare both methods holistically as listed in \cref{fig:comp}.

In our simulated benchmark, we aimed to maintain realism as much as possible; however, there are certain limitations that could be addressed in future research. 
For the MoCap implementations, the marker-based method excluded occlusions in real-world triangulation and human setup errors as marker positions are retrieved directly from the simulation; and marker-less methods did not consider optical confusions, especially between the subject and background, which may lead to sub-optimal image quality.
However, these aspects are also challenges in real-world experiments and are often actively eliminated by practitioners.
Our simulation only considered one standard cloth material; the influence of different materials on how the garment and markers react to various motions could be included. Draping is significantly affected by cloth material as it determines the weight, stretch, and rigidity which influences how the fabric hangs, folds, and holds a shape when draped over a 3D form. 
To further enhance realism, future work should incorporate support for external collisions, such as garments interacting with the floor. The variety of clothing options and motion types could also be expanded. 
It is also crucial to introduce support for other modalities in our benchmarking multi-camera and RGBD marker-less MoCap methods, which could provide even better precision with still less cost than marker-based MoCap.

% Figure environment removed
\vspace{-3pt}
\section{Conclusions and Outlook}
\vspace{-3pt}
We propose the DrapeMoCapBench as a benchmark methodology based on 3D physics simulations to compare marker-based and marker-less MoCap systems, mainly when dealing with loose garments. 
The simulation allows us to benchmark what is impossible in reality - quantitatively comparing different scenarios of precisely reproduced motions against the anatomic true motion under draped garments and body skin.
It incorporates physics-based simulation of the human body and garment models with real-world motion datasets, generating input data for marker-based and marker-less MoCap methods. 
Through the benchmark, we provide a comprehensive comparison of the MoCap methods with a benchmark table that quantifies precision for different motion types and levels of garment drape and holistic considerations. 
Our findings indicate that, while in line with the literature for skin-tight clothing, both marker-based and marker-less MoCap suffer significant performance loss in casual loose garments like shirts.
For daily activities (basic and fast motions) involving casual loose garments, marker-less MoCap slightly outperforms marker-based MoCap.
Considering the low cost of marker-less methods, they could be a preferable choice of reference for wearable studies.
DMCB can be a valuable resource for wearable practitioners seeking to select the most suitable MoCap method for their own applications, considering scenario-specific precision and holistic factors. 
The marker-less methods are closer to the marker-based MoCap than the anatomic joints in terms of MPJPE, which could be explained by the fact that the DL models were trained mostly using marker-based MoCap as the ground truth.
DMCB can also be used in future work as a data augmentation tool to improve vision-based pose estimation models.
Wearable developers can consider the specific errors we identified for each motion and garment type rather than relying solely on marker tracking errors, which through our findings, do not accurately represent the pose estimation error often not specified explicitly by the MoCap system.
Furthermore, the DMCB pipeline can be adapted to evaluate the expected MoCap performance for new garment designs, allowing for tailored assessments of different garment pieces.
\newpage
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
%To Robert, for the bagels and explaining CMYK and color spaces.
The research reported in this paper was supported by the BMBF (German Federal Ministry of Education and Research) in the project VidGenSense (01IW21003). It was also funded by Carl-Zeiss Stiftung under the Sustainable Embedded AI project (P2021-02-009).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix
\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
