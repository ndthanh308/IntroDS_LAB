%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript, screen, nonacm]{acmart}
%\documentclass[acmsmall]{acmart}



%%%OUR PACKAGES
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{XXXX}
% \acmYear{XXXX}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a JOURNAL article.
% \acmJournal{JACM}
% \acmVolume{37}
% \acmNumber{4}
% \acmArticle{111}
% \acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}


%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}




%%ADDED
\usepackage{interval}
\usepackage{subfiles}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{booktabs}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title[Robustness of SRSs Against Training Perturbations]{Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study}
%OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
%OR BETTER Testing robustness of sequential recommender systems: an empirical study
%OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
%%If your work has a subtitle, define it with the {\verb|subtitle|} command.
%\title[short title]{full title}


\author{Filippo Betello}
    \orcid{0009-0006-0945-9688}
    %\authornote{Both authors contributed equally to this research.}
    %\authornotemark[1]
    \affiliation{
      \institution{Sapienza University of Rome}
      %\streetaddress{...}
      \city{Rome}
      %\state{State}
      \country{Italy}
      %\postcode{....}
      }
    \email{betello.1835108@studenti.uniroma1.it}

\author{Federico Siciliano}
    \orcid{0000-0003-1339-6983}
    \affiliation{
      \institution{Sapienza University of Rome}
       \city{Rome}
      %\state{State}
      \country{Italy}
      }
    \email{federico.siciliano@uniroma1.it}

\author{Pushkar Mishra}
    \orcid{0000-0002-1653-6198}
    \affiliation{
      \institution{Meta AI}
        \city{London}
      \country{United Kingdom}
      }
    \email{pushkarmishra@meta.com}

\author{Fabrizio Silvestri}
    \orcid{0000-0001-7669-9055}
    \affiliation{
      \institution{Sapienza University of Rome}
       \city{Rome}
      %\state{State}
      \country{Italy}
      }
    \email{fsilvestri@diag.uniroma1.it}

%\renewcommand{\shortauthors}{Betello et al.}


\begin{abstract}
Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs.
\end{abstract}



% The ACM Computing Classification System ---
% \url{https://www.acm.org/publications/class-2012} --- is a set of
% classifiers and concepts that describe the computing
% discipline. Authors can select entries from this classification
% system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
% commands to be included in the \LaTeX\ source.
%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003350</concept_id>
       <concept_desc>Information systems~Recommender systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003350</concept_id>
<concept_desc>Information systems~Recommender systems</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003359</concept_id>
<concept_desc>Information systems~Evaluation of retrieval results</concept_desc>
<concept_significance>100</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Neural networks</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Information systems~Recommender systems}
\ccsdesc[100]{Information systems~Evaluation of retrieval results}
\ccsdesc[300]{Computing methodologies~Neural networks}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Recommender systems, Sequential recommender systems, Robustness}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%%\begin{teaserfigure}
%%  % Figure removed
%%  \caption{Seattle Mariners at Spring Training, 2010.}
%%  \Description{Enjoying the baseball game from the third-base
%%  seats. Ichiro Suzuki preparing to bat.}
%%  \label{fig:teaser}
%%\end{teaserfigure}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}


\maketitle

\section{Introduction}
Recommender systems have become ubiquitous in our daily lives \cite{adomavicius2005toward}, playing a key role in helping users navigate the vast amounts of information available online.
Thanks to the global spread of e-commerce services, social media and streaming platforms, recommender systems have become increasingly important for personalized content delivery and user engagement \cite{zhang2019deep}.
Typically, users' current interests are influenced by their past interactions and are therefore often dynamic and evolving \cite{wang2019sequential}.
%For example, the recommendation system should recommend watching The Hobbit trilogy to a user who has watched The Lord of the Rings trilogy.
In recent years, Sequential Recommender Systems (SRSs) have emerged as a popular approach to modeling user behavior over time \cite{quadrana2018sequence}, leveraging the temporal dependencies in users' interaction sequences to make more accurate predictions.

To improve the effectiveness of SRSs, various techniques have been developed, including Recurrent Neural Networks (RNNs) \cite{wu2017recurrent}, Attention mechanisms \cite{kang2018self}, and graph-based models \cite{wang2019neural}. These approaches have shown promising results in capturing complex user behavior patterns and providing personalized recommendations. However, despite their success, the robustness of SRSs against perturbations in the training data remains an open research question\cite{Li_2021}.

\looseness -1 In this study, we investigate the impact of items removal from users' interaction sequences on the performance of SRSs. We conduct an empirical analysis to understand the correlation between the degree and location of perturbation in the training data, and the resulting performance degradation. Specifically, we temporally order the sequences of interactions for each user and remove a variable number of items, ranging from 1 to 10, from the beginning, middle and end of the sequence.
%with $n\in\interval{1}{10}$.

Experiments can be envisioned in a real-world scenario where disruptions may occur due to users using different services for the same purpose. In such cases, service providers may have only part of the user's data, while the rest is owned by their competitors. Nevertheless, the provider needs to train its recommender system with this incomplete data set, and it wants the model to be resilient in the face of such perturbations.

We test two widely used SRSs, SASRec \cite{kang2018self} and GRU4Rec \cite{hidasi2016sessionbased}, on four popular benchmark datasets: MovieLens 100K \cite{harpermovielens}, MovieLens 1M \cite{harpermovielens}, Foursquare \cite{yang2016participatory} in both Tokyo and New York versions.

We use the RecBole \cite{zhao2021recbole} library to evaluate the performance of the models, employing metrics such as Normalized Discounted Cumulative Gain (NDCG), Precision, Recall and Mean Reciprocal Rank (MRR), and to ensure reproducibility of the results. Moreover, in order to better evaluate the resulting ranking lists, we make use of the Rank List Sensitivity metrics \cite{oh2022rank}.

We can summarise our contributions as follows:
\begin{itemize}
    \item Our investigation shows that the most recent items in a user's interaction sequence are critical for accurate recommendation performance: when these items are removed, there is a significant drop in all the performance metrics for both the models and across all the datasets.
    \item  Conversely, when the oldest or middle items are removed, the performance degradation is not as pronounced. \item We observe that in some cases the drop in performance is proportional to the number of items removed.
\end{itemize}

%The rest of the paper is organized as follows. Section \ref{sec:related} shows recent works in the current literature. The datasets, architectures, metrics and experiment settings used are explained in Section \ref{sec:methods}. Section \ref{sec:results} present our findings. In Section \ref{sec:conclusion} we conclude with pointers to future ideas.


\section{Related works} \label{sec:related}
\subsection{Sequential Recommender Systems}
SRSs are algorithms that leverage a user's past interactions with items to make personalized recommendations over time. Compared to traditional recommender systems, SRSs consider the order and timing of these interactions, allowing for more accurate predictions of a user's preferences and behaviors \cite{wang2019sequential}. SRSs have been widely used in various applications, including e-commerce \cite{schafer2001commerce,hwangbo2018recommendation}, social media \cite{guy2010social,amato2017recommendation}, and music streaming platforms \cite{schedl2015music,schedl2018current,afchar2022explainability}.

Various techniques have been proposed to implement SRSs.
%facilitate sequential recommendations based on previous user interactions, allowing for the modeling of sequential dynamics in user behavior.
Initially, Markov Chain models were employed in sequential recommendation \cite{fouss2005novel,fouss2005web}; they are probabilistic models that assume the future state of a sequence only depends on the current state. However, these models may struggle to capture complex dependencies in long-term sequences, and hence, have limitations.
In recent years, Recurrent Neural Networks (RNNs) have emerged as one of the most promising approach in this field \cite{donkers2017sequential, hidasi2016sessionbased, quadrana2017personalizing}: these methods encode the users' historical preferences into a vector that gets updated at each time step, and it is used to predict the next item in the sequence. Despite their success, RNNs may face challenges in dealing with long-term dependencies and generating diverse recommendations.
Another approach comes from the use of the attention mechanism \cite{vaswani2017attention}: two different examples are SASRec \cite{kang2018self} and BERT4Rec \cite{sun2019bert4rec} architectures. This method dynamically weighs the importance of different parts of the sequence, to better capture the important features and improve the prediction accuracy.
Recently, Graph Neural Networks have become popular in the field of recommendations system \cite{wu2022graph, purificato2023sheaf}, especially in the sequential domain \cite{chang2021sequential,fan2021continuous}.

\subsection{Robustness in Sequential Recommender Systems}
%Robustness in computer science is studied in every field: for example %\citeauthor{calzavara2019treant,calzavaratrees2019}
%Calzavara \emph{et al.}
%\cite{calzavara2019treant, calzavaratrees2019} analyzed two different types of adversarial training in decision trees.

\looseness -1 Robustness is an important aspect of SRSs as they are vulnerable to noisy and incomplete data.
Surveys on the robustness of recommender systems \cite{hurley2011robustness,burke2015robust} discussed the challenges in developing robust recommender systems and presented various techniques for improving the robustness of recommender systems.
%\cite{mobasher2007toward} studied the robustness of SRSs against various attack models, such as data poisoning attacks and profile injection attacks.
Robustness can be measured by Fairness and Diversity.
Fairness is one of the most critical aspects of recommender systems: Singh \emph{et al.} \cite{Singh_2018} proposes a framework for defining Fairness constraints on rankings, along with the effective algorithms needed to compute utility-maximizing rankings under such Fairness constraints. Conversely, \cite{Li_2021} analyzes this problem from the user's point of view, differentiates users into advantaged and disadvantaged groups, and proposes a re-ranking method to mitigate the bias.
On the other hand, Diversity is widely discussed in \cite{KUNAVER2017154}: the authors present a survey on the various techniques that are been developed in the literature.

\looseness -1 Many tools have been developed to test the robustness of different algorithms. \cite{o2004collaborative}, provided both a formalisation of the topic and a framework; the same was done more recently by \cite{ovaisi2022rgrecsys}, who developed a toolkit called RGRecSys. The toolkit provides a unified framework for evaluating the robustness of SRSs and can be used to compare different algorithms under different scenarios.
In Natural Language Processing (NLP), robustness can be tested using the Robustness Gym library \cite{goel2021robustness}: it unifies four standard evaluation paradigms, namely subpopulations, transformations, evaluation sets, and adversarial attack. Inspired by this work, the authors of \cite{ovaisi_2022} present a library (\textit{RGRecSys}) where it is possible to compare different recommender models under different scenarios (a subset of interest, under distributional shift, under transformation on user or item features, under Cross-Site Request Forgery (CSRF) \cite{barthcsrf} and under sparse data).

Recent work has focused on studying the problem and trying to increase robustness in different ways.
\cite{tang2023improving} focuses on robustness in the sense of training stability. Their method attempts to improve gradient descent by combining gradient clipping and normalization.
\cite{oh2022rank} instead investigated the Rank List Sensitivity of SRSs to interaction perturbations. They showed that even small perturbations in user-item interactions can lead to significant changes in the recommendations, and proposed a metric to measure the rank list sensitivity of recommender systems.
%This work highlights the importance of robustness in SRSs and the need for developing algorithms that can handle noisy and incomplete data.

Our work expands on this, going on to analyze not only other models and datasets, but above all making a more accurate investigation of the effect that different types of perturbation can have on the models' performance. In fact, while \cite{oh2022rank} perturbs a single interaction in the whole dataset, we perturb the sequences of all users and analyze the performance as the number of perturbations changes.

%\section{Motivations} \label{motivations}
%The purpose of this section is to have an easy understanding of the explanation of the phenomenon we have demonstrated in our experiments. As we will see, the performance of the models is greatly affected, especially when the items removed are the last ones (in the temporal sequence). Our motivation is that they are more responsible for the prediction of the $i_{n+1}$ item: this is because the items in the end of the sequence have more impact on the performance of the models. More often than not, this impact is proportional to the number of items removed.

\section{Methodology} \label{sec:methods}
%In this section, we introduce the settings, datasets, architectures and metrics that we have used in this work.

\subsection{Setting}
In a Sequential Recommendation setting, each user $u$ is represented by a temporally ordered sequence of items $S_u$ it has interacted with: $S_u = [I_1, I_2, ..., I_j, ..., I_{L-1}, I_{L}]$, where $L$ is the length of the sequence for user $u$. 
For simplicity of notation, we consider only one user, so we use $L$, whereas it would be more correct to use $L_u$, as the number of interactions varies between users.

For the training data perturbations, we have considered three different scenarios which we will refer to in Section \ref{sec:results}:
\begin{itemize}
    \item Remove $n$ items at the \textbf{beginning} of the sequence: $[I_{n+1}, I_{n+2}, ..., I_{L-1}]$.
    \item Remove $n$ elements in the \textbf{middle} of the sequence: $[I_1, I_2, ...,I_{\left\lfloor\frac{L-1-n}{2}\right\rfloor},I_{\left\lfloor\frac{L-1+n}{2}\right\rfloor} ..., I_{L-1}]$.
    \item Remove $n$ elements from the \textbf{end} of the sequence: $[I_1, I_2, ..., I_{L-1-N}]$.
\end{itemize}

with $n \in \{1,2,...,10\}$.
In practice, the data is first separated into training, validation and test set. Subsequently, only the training data are perturbed, with a methodology dependent on the scenario considered and the model is then trained on these. The models, trained on data perturbed in a different manner, are therefore always tested on the same data.

The reason why $I_L$ does not appear in the formulation is that, before the training phase, $I_L$ is separated from the training data and assigned to the test data. Therefore, we exclude the last element $I_L$ from our removal method as it is reserved for testing the model's performance.

\subsection{Real-world setting}

The lack of user-object interactions in real-world scenarios is a common event, as user interactions are spread across multiple services. For example, in the case of movies or TV shows, a particular user may watch them through broadcast TV, in cinemas or multiple streaming services.

In our case, the three considered scenarios correspond to three specific real-world events:
\begin{itemize}
    \item Removing $n$ items at the \textbf{beginning} of the sequence: This represents a user who signs up for a service without having used it before, so all their past interactions, i.e., those at the beginning of the complete sequence, were performed on other services.
    \item Removing $n$ elements in the \textbf{middle} of the sequence: This represents a user who takes a break from using the service for a certain period and then resumes using it, but any interactions they had during the considered period are not available to the service provider in question.
    \item Removing $n$ elements from the \textbf{end} of the sequence: This represents a user who has stopped using the service, so the service provider loses all interactions the user has from that point onward. The service provider still has an interest in winning the user back through their own platform or other means such as advertising, and thus, it is essential to have a robust model to continue providing relevant items to the user.
\end{itemize}

We might assume that removing the latest interactions will lead to the most significant decrease in performance, as those reflect the users' current interests most accurately. Furthermore, it is likely that items at the beginning of a long sequence are less relevant. Removing items in the middle of the sequence can have an impact on performance in a less easily predictable manner. However, given a long enough sequence, the effect is expected to be minimal.

\subsection{Datasets}

\begin{table}[t]
    \caption{Dataset statistics after preprocessing}
    \begin{tabular}{l||ccc|cc}
        \toprule
        Dataset & Users & Items & Interactions & Average $\frac{\mathrm{Actions}}{\mathrm{User}}$ & Median $\frac{\mathrm{Actions}}{\mathrm{User}}$ \\
        \midrule
        \midrule
        MovieLens 1M  & 6040  & 3952  & 1M  & 165  & 96   \\
        \midrule
        MovieLens 100K & 943 & 1682 & 100K & 106 & 65  \\
        \midrule
        Foursquare Tokyo & 2293 & 61858 & 537703 & 250 & 153       \\
        \midrule
        Foursquare New York & 1083 & 38333 & 227428 & 210 & 173       \\
        \bottomrule         
    \end{tabular}
    \label{tab:stats}
\end{table}


% The selection of datasets is guided by the fact that the following constraints has to be satisfied:
% \begin{equation}
%     (\mathrm{number\_of\_users}*10 < \mathrm{total\_interactions}) \quad \textrm{AND} \quad (\mathrm{number\_items\_per\_user} > 10)
% \end{equation}
% This is because we remove $n$ items from the beginning, middle, and end of the sequence, with $k\in\interval{1}{10}$. If the first constraint is not satisfied, we are trying to remove more elements than exist in the datasets. On the other hand, if the second condition is not met, we delete all items for a particular user who has less than 10 interactions. 

%Because of that,
We use four different datasets:

\textbf{MovieLens} \cite{harpermovielens} $\rightarrow$ This benchmark dataset is often used to test recommender systems. In this work, we use the \textit{MovieLens} 100K version, which contains 100,000 interactions for 943 users and 1682 items, and \textit{MovieLens} 1M version, which contains one million interactions for 6040 users and 3952 items. The average number of interactions for each user in the MovieLens 100K dataset is 106, while for MovieLens 1M dataset is approximately 165. The ratings in MovieLens 1M and MovieLens 100K datasets are made on a 5-star scale.

\textbf{Foursquare} \cite{yang2014modeling} $\rightarrow$ This dataset contains check-ins from New York City and Tokyo collected over a period of approximately ten months (from April 12 2012 to February 16 2013). It contains 227,428 New York City check-ins made by more than 1080 users with about 38,000 items, and 573,703 Tokyo check-ins made by 2293 users with approximately 62,000 items. The average number of interactions for each user in the New York City dataset is approximately 210, while for the Tokyo dataset is approximately 250. In this case, the interaction type is check-ins. 

The statistics for all the datasets are shown in Table \ref{tab:stats}.

We select datasets widely used in the literature and with a high number of interactions per user.
The limitation in dataset selection arises from our intention to assess the robustness against the removal of up to 10 elements.
Therefore, the dataset must satisfy the following constraint:
% \begin{equation}
%     (\text{number\_of\_users}*10 < \text{total\_interactions}) \land (\text{number\_items\_per\_user} > 10)
% \end{equation}

\begin{equation}
    L_u > 10 \quad \forall u \in U
\end{equation}

where $L_u$ is the number of interactions of user $u$, i.e. the length of the sequence $S_u$ of interactions, and $U$ is the set of all users in the dataset.
If the condition is not met, we would delete all items for a particular user who has less than 10 interactions. Therefore, we could not train the model on that particular user.

\subsection{Architectures}
In our study, we use two different architectures to validate the results.

\textbf{SASRec} \cite{kang2018self} $\rightarrow$ In order to comprehend the user's preferences, it uses self-attention processes to determine the importance of each interaction between the user and the item. 
We use the model implemented in RecBole \cite{zhao2021recbole}, with the default hyperparameters of the library.

\textbf{GRU4Rec} \cite{hidasi2016sessionbased} $\rightarrow$ It is a recurrent neural network architecture that uses gated recurrent units (GRUs) \cite{cho2014learning} to improve the accuracy of the prediction of the click-through rate (CTR).

\looseness -1 We choose to use these two models because both have demonstrated excellent performance in several benchmarks and have been widely cited in the literature.
Furthermore, as one model employs attention while the other utilizes RNN, their network functioning differs, which makes evaluating their behavior under training perturbations particularly interesting.
We use the models' implementation provided by the RecBole Python library \cite{zhao2021recbole}. The default hyperparameters are kept for each model as configured in the library.

\subsection{Metrics}
To evaluate the performance of the models, we use the traditional evaluation metrics used for Sequential Recommendation: Precision, Recall, MRR and NDCG. All the metrics are calculated with a cut-off $K$ of 20.

To mitigate excessive computation, a typical practice involves randomly selecting 100 negative items for each user and subsequently comparing the ranking of these items with the positive one.
The term \textit{positive item} is used to refer to the next item in the sequence, which is the one to be predicted. On the other hand, the term \textit{negative item} is used to describe any item that is not present in the user's sequence and is thus considered irrelevant to the user.

% We use four evaluation metrics that have been widely used in previous works:
% \textbf{NDCG@K} $\rightarrow$ It measures the quality of the recommendation list based on the relevance and the position of the recommended items.
% \textbf{Recall@K} $\rightarrow$ It is the percentage of successfully recommended products that are relevant to the user.
% \textbf{Precision@K} $\rightarrow$ It is the fraction of recommended items that are relevant to the user. 
% \textbf{MRR@K} $\rightarrow$ It measures the average of the reciprocal ranks of the first relevant item found in the recommendation list for each user.

\looseness -1 Moreover, we decide to further investigate the stability of the recommendation models. We borrow from \cite{oh2022rank} the Rank Sensitivity List (RLS) metric: it compares two recommendation lists, one coming from the model which is trained without perturbations (i.e. without removing any items) with the one coming from the model that has been trained in adversarial conditions (i.e. with removing items).
RLS employs a similarity measure, which can be chosen from two possible options:
%Specifically, there are two measures for RLS:
\begin{itemize}
    \item \textbf{Rank Bias Overlap (RBO)} \cite{webberrbo} measures the similarity of orderings between two rank lists. Higher values indicate that the items in the two lists are arranged similarly;
    \item \textbf{Jaccard Similarity (JAC)} \cite{jaccard1912distribution} is a normalized measure of the similarity of the contents of two sets. A model is stable if its Jaccard score is close to 1.
\end{itemize}
Having a model $M$, two ranked lists $X$ and $Y$, and a similarity function $sim$ between the two ranked lists, we can formalize the RLS metric as:
\begin{equation}
    \boldsymbol{\mathrm{RLS}} = \!\frac{1}{|X|} \! \sum_{\forall X_k \in X} \! \! \! \! \text{sim}(R_M^{X_k}, R_M^{X_k}) \quad
    \boldsymbol{\mathrm{RBO(X,Y)}} \!= \!(1-p)  \! \sum_{d=1}^{|I|}p^{d-1}\frac{|X[1:d] \cap Y[1:d]|}{d} \quad
    \boldsymbol{\mathrm{JAC(X,Y)}} = \! \frac{|X \cap Y|}{|X \cup Y|}
\end{equation}

\subsection{Experiments} \label{experiments}
\looseness -1 All the experiments are performed on a single NVIDIA RTX 3090 with 10496 CUDA cores and 24 GB of VRAM. The batch size is fixed to 4096. Adam optimizer is used with a fixed learning rate of $5*10^{-4}$. The number of epochs is set to 300, but in order to avoid overfitting, we early stop the training when the NDCG@20 does not improve for 50 epochs. 

The RecBole library was utilized for conducting all the experiments, encompassing data preprocessing, model configuration, training, and testing. This comprehensive library ensures the reproducibility of the entire process.


\section{Results} \label{sec:results}
%For the sake of brevity, we show some of the results just two model-dataset pairs. The complete results can be found in this repository: \url{}.

\subsection{Intrinsic Models Instability}
\begin{table}[ht]
\caption{\textbf{Variation of metrics between two seeds.} Metrics for the 4 datasets considered for GRU4Rec and SASRec. For Precision (Prec.), Recall, MRR and NDCG, it is shown the percentage variation between the obtained performance using two different initialization seeds for the models. For RLS using RBO (RBO) and RLS using Jaccard (JAC), the value of the metric is shown without leading zeroes. All metrics are computed with a cut-off of 20. For each metric, the value corresponding to the dataset where a model is less robust is highlighted in \textbf{bold}. For each dataset, the value corresponding to the model that is the least robust of the two given a metric is \underline{underlined}.}
%are affected by the removal of the last element, while the other types of removal have no great impact.}
\centering
\begin{tabular}{l||cccc|cc||cccc|cc}
\toprule
& \multicolumn{6}{c||}{SASRec} & \multicolumn{6}{c}{GRU4Rec} \\
\midrule
 & Prec. & Recall & MRR & NDCG & RBO & JAC & Prec. & Recall & MRR & NDCG & RBO & JAC\\
\midrule\midrule
ML 100k & 0.5\% & 0.5\% & 0.5\% & \underline{0.3\%} & .466 & .489 & \underline{\textbf{1.7\%}} & \underline{\textbf{1.7\%}} & \underline{1.2\%} & 0.1\% & \underline{.337} & \underline{.413}\\ 
\midrule
ML 1M & \underline{0.3\%} & \underline{0.4\%} & \textbf{0.5\%} & \textbf{0.5\%} & .549 & .569 & 0.2\% & 0.2\% & \underline{3.5\%} & \underline{2.5\%} & \underline{.311} & \underline{.347}\\
\midrule
FS NYC & \underline{0.9\%} & \underline{0.9\%} & 0.0\% & \underline{0.3\%} & \textbf{.398} & \textbf{.273} & 0.1\% & 0.1\% & \underline{0.6\%} & 0.1\% & \underline{\textbf{.110}} & \underline{\textbf{.083}}\\
\midrule
FS TKY & \underline{\textbf{1.4\%}} & \underline{\textbf{1.4\%}} & 0.4\% & 0.02\% & .418 & .267 & 0.8\% & 0.8\% & \underline{\textbf{4.7\%}} & \underline{\textbf{3.4\%}} & \underline{.210} & \underline{.165}\\
\bottomrule
\end{tabular}
\label{tab:baseline_metrics}
\end{table}

To measure the inherent robustness of the models, i.e. in the baseline case (without removal of items), we train the model twice using different initialization seeds and compare the rankings obtained using the percentage variation between the two rankings. The results are shown in Table \ref{tab:baseline_metrics}.

The percentage discrepancy (in absolute value) between the Precision, Recall, MRR and NDCG values obtained using two different initialization seeds is shown. It can be seen that in general the discrepancy is negligible: almost always less than 1\%.
On the other hand, the two RLS metrics, calculated using RBO and Jaccard respectively, show us the similarity between the two rankings produced with different initialization seeds.
%Recalling that a robust model should obtain a value close to 1, where the values are far from it, here the values are far from it.
It is worth recalling that a robust model should yield a RLS score that approaches 1. However, in this case, the values deviate significantly from this ideal threshold.
These combined results indicate to us that the models converge to an adequate performance beyond the initialization seed, but that the actual rankings produced are heavily influenced by it.

The \textbf{bold} values show for each metric, the dataset where the least robust result was obtained. None of the datasets stands out more than the others, but we can mention that Foursquare Tokyo seems to give more problems regarding standard evaluation metrics (Precision, Recall, MRR and NDCG), while for Foursquare New York City it seems more difficult to produce stable rankings (see RLS-RBO and RLS-JAC).

The \underline{underlined} values instead compare, for each metric and dataset, which of the two models is the least robust. If we consider metrics that do not take into account the position of the positive item in the ranking, i.e. Precision and Recall, GRU4Rec seems more robust than SASRec. If, on the other hand, we look at metrics that penalize relevant items in positions too low in the ranking, we see that the opposite happens. This suggests to us that GRU4Rec is able to return a better set of results, but in a less relevant order than SASRec does.
As proof of this, if we check the RLS metrics, we see that GRU4Rec is always the least robust model as the initialization seed changes.

%I risultati riscontrati sono in linea con quanto atteso e presentato nella 

\subsection{Comparison of the position of removal}
\begin{table}[ht]
\caption{Metrics for the 3 scenarios considered for GRU4Rec on MovieLens 1M and SASRec on Foursquare Tokyo. For Precision (Prec.), Recall, MRR and NDCG, it is shown the percentage variation between removing ten items and the reference value. For RLS using RBO (RBO) and RLS using Jaccard (JAC), the value of the metric is shown without leading zeroes. All metrics are computed with a cut-off of 20. For each metric, in \textbf{bold} it is highlighted the value representing the less robust model.}
%are affected by the removal of the last element, while the other types of removal have no great impact.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l||cccc|cc||cccc|cc}
\toprule
& \multicolumn{6}{c||}{SASRec - Foursquare Tokyo} & \multicolumn{6}{c}{GRU4Rec - MovieLens 1M} \\
\midrule
 & Prec. & Recall & MRR & NDCG & RBO & JAC & Prec. & Recall & MRR & NDCG & RBO & JAC\\
\midrule\midrule
Beginning & -1.33\% & -1.35\% & -0.09\% & -1.10\% & .421 & .267 & -0.22\% & -0.23\% & -2.44\% & -1.61\% & .222 & .258\\
\midrule
Middle & -1.29\% & -1.07\% & -0.03\% & -0.62\% & .430 & .269 & -0.47\% & -0.48\% & -4.99\% & -3.37\% & .219 & .254\\
\midrule
End & \textbf{-4.89\%} & \textbf{-4.9\%} & \textbf{-6.99\%} & \textbf{-3.33\%} &\textbf{.223} & \textbf{.174} & \textbf{-17.4\%} & \textbf{-17.4\%} & \textbf{-85.5\%} & \textbf{-56.0\%} & \textbf{.061} & \textbf{.089}\\
\bottomrule
\end{tabular}}
\label{tab:comparison_removal_position}
\end{table}

The percentage variation between removing ten items and removing none (reference value) in the training set is displayed in Table \ref{tab:comparison_removal_position}. We report this value for the Precision (Prec.), Recall, MRR and NDCG metrics, all calculated with a cut-off of 20, for the three scenarios considered.
From Table \ref{tab:comparison_removal_position} it can be seen that the removal of items from the beginning or middle of the sequence does not greatly hinder the performance of the model; only a small decrease is observed, which may simply be due to the slightly reduced amount of total training data.
Instead, it can be observed how removing items from the end of the sequence leads to a drastic reduction in metrics: in the case of GRU4Rec applied to the MovieLens 1M dataset, the NDCG more than halves.

Finally, we can see how the difference between the three settings, although maintaining the same trend, is less marked for SASRec applied to the Foursquare Tokyo dataset. This may be due to the generally higher performance of SASRec \cite{kang2018self}. A more in-depth analysis is presented in Section \ref{res:dataset_diff}.

Table \ref{tab:comparison_removal_position} also shows the two RLS metrics, computed using RBO and Jaccard similarity, on the same model-dataset pairs
%of Table \ref{tab:comparison_removal_position}.
They are computed by comparing the ranking obtained when removing 10 items with those of the baseline.
%Recalling that a RLS is a measure of similarity, 1 indicates identical rankings, whereas the lower the value, the more different the two rankings are.
\looseness -1 Table \ref{tab:comparison_removal_position} shows us how removing items at the end of the sequence leads to considerable variation in the rankings produced by the models. Especially in the case of GRU4Rec, the values approach 0, meaning that the produced rankings share almost no items.

Our results are in contrast to those of \cite{oh2022rank}, which instead claim that an initial perturbation of a user sequence leads to a higher impact on the RLS. However, their experimental setting is different than ours, as explained in \ref{sec:related}.

%The main result presented in this subsection is that all metrics are impacted when the removed items are the most recent ones, while removal in the other two cases does not affect performance.


\subsection{Effect of the number of elements removed}
%In this subsection, we show that the performance of the last item removal in chronological order is affected by the number of items removed.

% Figure environment removed
%As can be noted, the overall performance is not affected  when removing elements from the beginning and middle (red and blue lines respectively) compared to the reference (green line), but removing elements from the end affects the overall performance for both models.

\looseness -1 As we discussed in the previous section, removing elements that are at the beginning or in the middle of the temporally ordered sequence has no effect on performance. This is also confirmed by Figure \ref{fig:number_removal_comparison}, where we can also see, however, that for the above-mentioned cases there is no variation as the number of removed elements increases.
The deviation of the RLS displayed in Figure \ref{fig:number_removal_comparison_rbo_gru_ml-1m} will be analyzed in more detail in Section \ref{res:dataset_diff}.
For the remaining setting, the one where we remove items at the end of the sequence, the effect of the number of items removed is evident: the metrics drop drastically as the number of items removed increases. This result holds true for both models considered and for all four datasets tested.

\subsection{Differences between the datasets}\label{res:dataset_diff}
%In the previous two subsections, we emphasized that performance is affected by both the location and the number of items removed. We will now focus on the difference between the two types of datasets used.

% Figure environment removed

Figure \ref{fig:datasets_comparison} shows the performance for the three different settings for the SASRec model applied to all datasets. From Figures \ref{fig:datasets_comparison_ndcg_ml-100k}, \ref{fig:datasets_comparison_ndcg_ml-1m}, \ref{fig:datasets_comparison_rbo_ml-100k}, \ref{fig:datasets_comparison_rbo_ml-1m} we see that the downward trend of the metric when removing items at the end of the sequence is a characteristic of the MovieLens dataset: both NDCG@20 and RLS-RBO@20 show a decrease when increasing the number of removed items.
We hypothesize that this is happening because the average number of actions per user and the number of items (see Table \ref{tab:stats}) are not that large compared to the number of items removed.
\looseness -1 On the other hand, the Foursquare datasets (\ref{fig:datasets_comparison_ndcg_tky}, \ref{fig:datasets_comparison_ndcg_nyc}, \ref{fig:datasets_comparison_rbo_tky}, \ref{fig:datasets_comparison_rbo_nyc})  do not suffer major performance degradation, probably due to the higher average number of actions per user and the number of items (see Table \ref{tab:stats}) than MovieLens. In addition to this, and probably for the same motivation, the degradation of the RLS metrics is lower with respect to that displayed in the MovieLens datasets.
Finally, it is interesting to note that on MovieLens 1M, there is a consistent performance degradation even when elements at the beginning and in the middle of the temporally ordered sequence are removed.
This can be observed as a small decrease in the NDCG@20 (Figure \ref{fig:datasets_comparison_ndcg_ml-1m}), but a sharp decrease in the value of the RLS-RBO (Figures \ref{fig:number_removal_comparison_rbo_gru_ml-1m}, \ref{fig:datasets_comparison_rbo_ml-1m}). This means that even if the model performs approximately the same, the rankings produced vary greatly.
%On the other hand, other datasets do not suffer from this phenomenon, as can be seen in Figure \ref{fig_dataset_comparison}.
The cause may be the fact that the MovieLens 1M dataset, among those considered, has the largest number of users and interactions.

\section{Conclusion} \label{sec:conclusion}
In this work, we have analyzed the importance of the position of items in a temporally ordered sequence for training SRSs. Our results demonstrate the importance of the most recent elements in users' interactions sequence: when these items are removed from the training data, there is a significant drop in all evaluation metrics for all case studies investigated. Furthermore, this reduction is proportional to the number of elements removed.
Conversely, the performance degradation is not as pronounced when the oldest elements (towards the beginning) or the middle elements in the sequence are removed.
We validated our hypothesis using four different datasets, MovieLens, both 100K and 1M versions, and Foursquare, both New York and Tokyo versions, and two different models, SASRec and GRU4Rec. We used traditional evaluation metrics such as NDCG and Recall, but also RLS, a metric specifically designed to measure Sensitivity. Future work in this direction could first extend our results to more models and more datasets, and then investigate a way to make the models robust to the removal of training data. We hypothesize that the solution may lie in using different training strategies, like random masking, Robust loss functions \cite{bucarelli2023leveraging, wani2023combining}, or different optimization algorithms.
While we focused on Sequential Recommender Systems, our investigation would be relevant for other approaches as well.

\begin{acks}
This work was partially supported by projects FAIR (PE0000013) and SERICS (PE00000014) under the MUR National Recovery and Resilience Plan funded by the European Union - NextGenerationEU, by Sapienza's project FedSSL: Federated Self-supervised
Learning with applications to Automatic Health Diagnosis and by ERC Starting Grant No. 802554 (SPECGEO) and PRIN 2020 project n.2020TA3K9N "LEGO.AI". Supported also by the ERC Advanced Grant 788893 AMDROMA, EC H2020RIA project “SoBigData++” (871042), PNRR MUR project IR0000013-SoBigData.it.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{citations}

\end{document}
