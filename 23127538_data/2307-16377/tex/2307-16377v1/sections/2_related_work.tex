\section{Related Work}


Based on the incorporation of a human body model~\cite{SMPL,SMPL-X:2019}, Deep Neural Network-based 3D Human Mesh Recovery methods~\cite{i2L_meshNet,HMR,SPIN,3dCrwodNet,BEV,CVPE_2020_OOH,zhang2021pymaf,ROMP,PARE,jiang2020multiperson,metro_lin,GraphCMR,mesh_graphormer,choi2020pose2mesh,dwivedi2021learning,guan2021bilevel,kissos2020beyond,kocabas2020vibe,li2021hybrik,tripathi2020posenet3d,OCHMR} can be divided into two categories. The first, SMPL-based approaches~\cite{HMR,SPIN,PARE,zhang2021pymaf,ROMP,BEV,kocabas2020vibe,jiang2020multiperson}, maps input pixels to SMPL parameters~\cite{SMPL} such as pose and shape and reconstructs meshes by SMPL models, while the second, SMPL-free methods~\cite{metro_lin,GraphCMR,mesh_graphormer}, directly maps raw pixels to 3D mesh vertices  without the assistance of SMPL models. 
In this paper, we mainly consider the first method as the implementation approach.


\noindent\textbf{Human Mesh Recovery.} 
Usually, human mesh recovery methods estimate 3D human mesh of a single person within a person bounding box, which is scaled to the same size. This allows us to assume that the distance between each individual and the camera is roughly equivalent in the cropped image patch. 
Early works~\cite{HMR,SPIN} employ spatial averaging on CNN features for obtaining global features and utilize Multi-Layer Perceptrons (MLPs) to regress SMPL parameters. 
However, global pooling is not suitable for achieving pixel-aligned results, leading to subpar performance in real-world scenarios.
PARE~\cite{PARE} proposes using part segmentation masks to enhance pixel alignments.
Zhang~\etal~\cite{CVPE_2020_OOH} make use of occlusion segmentation masks to allow the model to attend to the visible human body parts, which also helps to reconstruct complete human mesh.
OCHMR~\cite{OCHMR} employs load-global center maps to make the model regress the mesh of the referred person.
While these methods make progress in occluded human mesh recovery by enhancing the ability to represent 2D information, they overlook the 3D structural information.
3DCrowdNet~\cite{3dCrwodNet} and BEV~\cite{BEV} introduce 3D representations to locate human joints in 3D space.
However, these approaches also have limitations since they extract 2D CNN features in the corresponding region of located 3D joints, thereby overlooking the full potential of 3D representations.
Therefore, we design a fusion framework to integrate 2D and 3D features for mutual complementation.

\noindent\textbf{Multi-Modality Transformers.} 
Multi-Modality transformers~\cite{lei2021detecting, li2020hero, kamath2021mdetr, li2021referring, zhang2022eatformer, zhang2023rethinking, zhang2021analogous} are capable of processing input data from multiple modalities, such as text, image, audio, or video, in a single model. 
The attention mechanism is a key component of transformers, which enables them to selectively focus on relevant parts of the input sequence when generating the output.
Returning to the present task, it is worth noting that although there is only one visual modality, two distinct representations (\ie, 2D and 3D representations) are available.
Thus, we propose using transformers with attention mechanisms to integrate multi-representation features, rather than relying on global average pooling or joint feature sampling in CNN features.

\noindent\textbf{Contrastive Learning.} Contrastive learning~\cite{chen2020simple,he2020momentum,grill2020bootstrap,chen2020big} is a type of unsupervised learning that aims to learn a similarity metric between data samples. The goal of contrastive learning is to bring similar examples closer together in feature space while pushing dissimilar examples farther apart.
With regards to the current objective, in 3D space, human body joints occupy a relatively small proportion, with the majority of voxels in the space being occupied by other objects (\eg, another person's body, background elements, and empty elements).
This poses a significant challenge in learning a similarity metric between data samples in 3D space.
To address this issue, we propose a novel joint-based contrastive learning strategy inspired by the recent success of pixel contrastive learning in semantic segmentation~\cite{zhong2021pixel, wang2021exploring, alonso2021semi, hu2021region}, which enables the network to learn a clear similarity metric in 3D space.
