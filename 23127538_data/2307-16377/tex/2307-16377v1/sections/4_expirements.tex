\input{tables/pc_oc_crowd_sota}
\input{tables/3dpw_cmu}
\input{tables/ablation_tables.tex}

\section{Experiments}
\label{sec:experiments}


\noindent\textbf{Implementation Detail.}
This proposed \methodname is validated using an end-to-end training approach on the ResNet-50~\cite{resnet} backbone. Following 3DCrowdNet~\cite{3dCrwodNet}, we initialize ResNet from Xiao~\etal~\cite{xiao2018simple} for fast convergence. We use AdamW optimizer~\cite{AdamW} with a batch size of 256 and weight decay of $10^{-4}$. The initial learning rate is  $10^{-4}$. 
The ResNet-50 backbone takes a $256 \times 256$ image as input and produces image features with size of $2048 \times 8 \times 8 $. We build the 3D features with size of $256 \times 8 \times 8 \times 8$ and 2D features with size of $256 \times 8 \times 8$. 
For joint-to-non-joint contrast, we sample $50$ anchor joints in each mini-batch, which are paired with $1024$ positive and $2048$  and negative samples. For joint-to-joint contrast, we sample $50$ anchor joints in each mini-batch, which are paired with $128$ positive and $512$ and negative samples. Both contrastive losses are set to a temperature of $0.07$.
More details can be found in the supplementary material. 

\noindent\textbf{Training.} Following the settings of previous work~\cite{HMR,SPIN,3dCrwodNet}, our approach is trained on a mixture of data from several datasets with 3D and 2D annotations, including Human3.6M~\cite{ionescu_h36m}, MuCo-3DHP~\cite{muco}, MSCOCO~\cite{coco},  and CrowdPose~\cite{li2018crowdpose}. Only the training sets are used, following the standard split protocols. For the 2D datasets, we also utilize their pseudo ground-truth SMPL parameters~\cite{Moon_2022_CVPRW_NeuralAnnot} for training. 

\noindent\textbf{Evaluation.} The 3DPW~\cite{3dpw} test split, 3DOH~\cite{CVPE_2020_OOH} test split, 3DPW-PC~\cite{ROMP,3dpw}, 3DPW-OC~\cite{CVPE_2020_OOH,3dpw}, 3DPW-Crowd~\cite{3dCrwodNet,3dpw} and CMU-Panoptic~\cite{CMUpanoptic} datasets are used for evaluation.
3DPW-PC and 3DPW-Crowd are the \textit{person-person} occlusion subset of 3DPW, 3DPW-OC is the \textit{person-object} occlusion subset of 3DPW and 3DOH is another \textit{person-object} occlusion specific dataset.
We adopt per-vertex error (PVE) in mm to evaluate the 3D mesh error. 
We employ Procrustes-aligned mean per joint position error (PA-MPJPE) in mm and mean per joint position error (MPJPE) in mm to evaluate the 3D pose accuracy. As for CMU-Panoptic, we only report  mean per joint position error (MPJPE) in mm following previous work~\cite{jiang2020multiperson, 3dCrwodNet, ROMP}.

\subsection{Comparison to the State-of-the-Art on Occlusion Benchmark}
\noindent\textbf{3DPW-OC~\cite{3dpw,CVPE_2020_OOH}} is a person-object occlusion subset of 3DPW and contains 20243 persons.~\cref{table:oc_sota} shows our method  achieve a new state-of-the-art performance on 3DPW-OC.

\noindent\textbf{3DOH~\cite{CVPE_2020_OOH}} is a person-object occlusion-specific dataset and contains 1290 persons in testing set, which incorporates a greater extent of occlusions than 3DPW-OC. 
For a fair comparison, we initialize PARE with weights that are not trained on the 3DOH training set, resulting in different performances from the results reported in~\cite{PARE}.
\cref{table:oc_sota} shows our method surpasses all the competitors with $59.3$  (PA-MPJPE). 

\noindent\textbf{3DPW-PC~\cite{3dpw,ROMP}} is a multi-person subset of 3DPW and contains 2218 persons' annotations under person-person occlusion.~\cref{table:oc_sota} shows our method surpasses all the competitors with $58.8$  (PA-MPJPE).

\noindent\textbf{3DPW-Crowd~\cite{3dpw, 3dCrwodNet}} is a person crowded  subset of 3DPW and contains 1923 persons.  We slightly surpass previous state-of-the-art as shown in~\cref{table:oc_sota}.
 
\noindent\textbf{CMU-Panoptic~\cite{CMUpanoptic}} is a dataset with multi-person indoor scenes. We follow previous methods~\cite{3dCrwodNet,jiang2020multiperson} applying 4 scenes for evaluation without using any data from training set. 
\cref{table:cmu}, shows that our method outperforms previous 3D human pose estimation methods on CMU-Panoptic, which means our model also works well for indoor and daily life scenes.
\vspace{-2mm}
\subsection{Comparison to the State-of-the-Art on Standard Benchmark}
\vspace{-2mm}
\noindent\textbf{3DPW~\cite{3dpw}} is the latest large-scale benchmark for 3D human mesh recovery. We do not use the training set and report performance on its test split which contains 60 videos and 3D annotations of 35515 persons. As shown in~\cref{table:3dpw}, Our method achieves  state-of-the-art  results among previous approaches.
 The results demonstrate the robustness of \methodname to a variety of in-the-wild scenarios.
\vspace{-2mm}
\subsection{Analysis.}
\vspace{-2mm}
In this section, we analyze the main components of \methodname and evaluate their impact on the mesh recovery performance.
More details and ablation studies can be found in the supplementary material.

\noindent\textbf{Utilization of 2D and 3D features:}
\cref{tab:2d_3d} demonstrates that the incorporation of 3D features is beneficial for mesh recovery performance. For the utilization of 2D features, flatting shows better performance than sampling, which supports our hypothesis that sampling joint features in obscured regions could have a negative impact.
For 3D features, we do not conduct experiments for flatting 3D features due to memory limitations. Moreover, we believe that 3D joint feature sampling is adequate for alleviating occlusion problems by attending to the accurate depth.
\cref{fig:_weight} shows the attention weights in the last refining layer. The query tokens significantly pay more attention to 3D features, which validates the usefulness of our fusion framework.

\noindent\textbf{Validation of coarse-to-fine regression:}
We validate the accuracy of intermediate predictions of fusion transformer in~\cref{tab:coarse_to_fine}, which shows the coarse-to-fine regression process in \methodname.

\noindent\textbf{Decoupling SMPL query:}
\methodname performance improvement is observed in~\cref{tab:smpl_token} by decoupling SMPL query from joint query. In the experiment without decoupling, we employ mean pooling on the decoder's output and regress SMPL parameters through MPLs. Decoupling SMPL query is presumed to enhance performance by reducing interference in executing other tasks (\eg, joint localization) during SMPL parameter regression.


% Figure environment removed
% Figure environment removed

% Figure environment removed


\noindent\textbf{3D Joint contrastive learning:}
The impact of 3D joint contrastive learning on the performance of \methodname is presented in~\cref{tab:contrastive}. Both joint-to-non-joint and joint-to-joint contrastive losses result in improved performance, with the former being more effective as it incorporates global supervision for the entire 3D space. 
Our contrastive losses also lead to more compact and well-separated 
learned joint embeddings, as shown in~\cref{fig:joint_contrast}. 
This indicates that our network can generate more discriminative 3D features, producing semantically clear 3D spaces and promising results. 

