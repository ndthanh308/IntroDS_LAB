\vspace{-6mm}
\section{Introduction}
\label{sec:introduction}

The estimation of 3D human meshes from single RGB images is an active area of research in computer vision with a broad range of applications in robotics, AR/VR, and human behavior analysis.
In contrast to estimating the pose of general objects~\cite{yang2021dsc}, human mesh recovery is more challenging due to the complex and deformable structure of the human body. Nevertheless, enhancing human-centric tasks can be achieved by combining visual features and prior knowledge about human anatomy through constructing multi-knowledge representations~\cite{mkr}.
Generally, the human mesh recovery task takes a single image as input and regresses human model parameters such as SMPL~\cite{SMPL} as output.

Driven by deep neural networks, this task has achieved rapid progress~\cite{HMR,SPIN,3dCrwodNet,BEV,CVPE_2020_OOH,zhang2021pymaf,ROMP,PARE,OCHMR,jiang2020multiperson,metro_lin,GraphCMR,mesh_graphormer}. 
Recent studies have focused on regressing accurate human meshes despite occlusions.
To achieve this, most of them employ 2D prior knowledge (\eg, UV maps~\cite{CVPE_2020_OOH}, part segmentation masks~\cite{PARE} and 2D human key points~\cite{OCHMR}) to focus the model on visible human body parts for enhancing the 2D alignment of the predicted mesh. 
Additionally, some methods~\cite{3dCrwodNet,BEV} introduce 3D representations to locate 3D joints and extract 2D features from the corresponding regions of the 2D image.


Even though the above methods have achieved significant progress in occluded human mesh recovery, they still remain constrained to these two aspects: the pursuit of \textbf{2D alignment} and \textbf{local supervision} for 3D joints.
(\romannumeral1) As shown in~\cref{fig:fig_1}{\color{red}a}, the above methods employing 2D prior knowledge mainly focus on \textbf{2D alignment} technologies, including spatial averaging and 2D joint sampling.
However, in crowded or occluded scenarios, solely focusing on 2D alignment may acquire ambiguous features for the entire mesh due to the lack of estimation of hidden parts.
Accordingly, the invisible human body parts would be aligned based on prior knowledge of the standard SMPL template, resulting in misalignment with visible parts and leading to inaccurate 3D reconstructions.
(\romannumeral2) Furthermore, creating a comprehensive and precise 3D representation from a single RGB image is an ill-posed problem as the inherently limited information.
As illustrated in~\cref{fig:fig_1}{\color{red}b}, some methods that use 3D representations rely on localized 3D joints as \textbf{local supervision},
ignoring the rich semantic relations between voxels across different scenes. 
These \enquote{local} contents (\ie, human joints) occupy only a small portion of the 3D space, while most voxels are often occupied by occlusions and background.
Consequently, the lack of explicit supervision for the entire 3D space makes it difficult to differentiate target humans from other semantically similar voxels, resulting in ambiguous 3D representations.

Therefore, to improve occluded human mesh recovery, we consider investigating a fusion framework that integrates 2D and 3D features for \textbf{2D$\&$3D alignment}, along with a \textbf{global supervision} strategy to obtain a semantically clear 3D feature space. 
By leveraging the complementary information from both 2D and 3D representations, the network could overcome the limitations of using only a single 2D representation, enabling obscured human parts to be detected in 3D representations and achieving 2D$\&$3D alignment. 
Given a global supervision strategy, we could explicitly supervise the entire 3D space to highlight the representation of target humans and distinguish them from other semantically similar voxels, resulting in a semantically clear 3D feature space.


Based on the above motivation, this paper proposes  a novel framework, 3D JOint contrastive learning with TRansformers (JOTR), for recovering occluded human mesh using a fusion of multiple representations as shown in~\cref{fig:fig_1}{\color{red}a}.
Unlike existing methods such as 3DCrowdNet~\cite{3dCrwodNet} and BEV~\cite{BEV} that employ 3D-aware 2D sampling techniques, JOTR integrates 2D and 3D features through transformers ~\cite{transformer} with attention mechanisms.
Specifically, \methodname utilizes an encoder-decoder transformer architecture to combine 3D local features (\ie, sampled 3D joint features) and 2D global features (\ie, flatten 2D features), enhancing both 2D and 3D alignment.
Besides, to obtain semantically clear 3D representations, the main objective is to strengthen and highlight the human representation while minimizing the impact of irrelevant features (\eg, occlusions and background). 
Accordingly, we propose a new approach, 3D joint contrastive learning (in~\cref{fig:fig_1}{\color{red}b}), that provides global and explicit supervision for 3D space to improve the similarity of semantically similar voxels (\ie, human joints), while maintaining discrimination from other voxels (\eg, occlusions). 
By carefully designing 3D joint contrast for 3D representations, \methodname can mitigate the effects of occlusion and acquire semantically meaningful 3D representations, resulting in accurate localization of 3D human joints and acquisition of meaningful 3D joint features.


We conduct extensive experiments on both standard  3DPW benchmark~\cite{3dpw} and occlusion benchmarks such as 3DPW-PC~\cite{3dpw,ROMP}, 3DPW-OC~\cite{CVPE_2020_OOH,3dpw}, 3DPW-Crowd~\cite{3dpw,3dCrwodNet}, 3DOH~\cite{CVPE_2020_OOH} and CMU Panoptic~\cite{CMUpanoptic}, and \methodname achieves state-of-the-art performance on these datasets.
Especially, \methodname outperforms the prior state-of-the-art method 3DCrowdNet~\cite{3dCrwodNet} by \textbf{6.1}  (PA-MPJPE), \textbf{4.9} (PA-MPJPE), and \textbf{5.3} (MPJPE)  on 3DPW-PC, 3DPW-OC, and 3DPW  respectively.
Moreover, we carry out comprehensive ablation experiments to demonstrate the effectiveness of our framework and 3D joint contrastive learning strategy.
Our contributions are summarized as follows:
\begin{itemize}
	\vspace{-2mm}
	\item We propose \methodname, a novel method for recovering occluded human mesh using a fusion of 2D global and 3D local features, which overcomes limitations caused by person-person and person-object occlusions and achieves 2D$\&$3D aligned results. \methodname achieves state-of-the-art results on both standard and occluded datasets, including 3DPW, 3DPW-PC, 3DPW-OC, 3DOH, CMU Panoptic, and 3DPW-Crowd.
	\vspace{-1mm}
	\item We develop a 3D joint contrastive learning strategy that  supervises the 3D space explicitly and globally to obtain semantically clear 3D representations, minimizing the impact of occlusions and adapting to more challenging scenarios with the help of cross-image contrast. 
\end{itemize}