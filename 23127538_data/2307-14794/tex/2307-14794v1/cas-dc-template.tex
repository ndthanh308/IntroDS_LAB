%% 
%% Copyright 2019-2020 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
\documentclass[a4paper,fleqn]{cas-dc}



%\usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[authoryear,longnamesfirst]{natbib}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[mathlines,pagewise]{lineno}% Enable numbering of text and display math
\usepackage{afterpage}
\usepackage{etoolbox}
\usepackage{subcaption}


%
%CUSTOM PACK
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}
\usetikzlibrary{calc,math}
\usetikzlibrary{calc,math}
\usepackage{amssymb}
\usepackage{float} 


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\usepackage[table]{xcolor}
\renewcommand{\arraystretch}{1.0}
\usepackage{diagbox, xcolor}
%\usepackage{amsmath}
\setlength{\arrayrulewidth}{0.5mm}
\setlength{\tabcolsep}{5pt}
%\usepackage[
%    backend=biber,
%    style=numeric,
%  ]{biblatex}
%\addbibresource{biblio.bib}
%%%%%%%%%%%%%% NEWCOMMAND
\newcommand{\tL}{\tilde{L}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfxx}{\boldsymbol{x}}
\newcommand{\bfXX}{\boldsymbol{X}}

\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfW}{\mathbf{W}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\bfTheta}{{\Theta}}
\setlength{\linenumbersep}{4pt}
%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%

% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

% Short title
\shorttitle{D. Mickaël / Ocean Modelling}

% Short author
\shortauthors{Mickaël Delcey et~al.}

% Main title of the paper
\title [mode = title]{Enhancing Gravity Currents Analysis through Physics-Informed Neural Networks: Insights from Experimental Observations}                      

% Title footnote mark
% eg: \tnotemark[1]


% Title footnote 1.
% eg: \tnotetext[1]{Title footnote text}
% \tnotetext[<tnote number>]{<tnote text>} 



% First author
%
% Options: Use if required
% eg: \author[1,3]{Author Name}[type=editor,
%       style=chinese,
%       auid=000,
%       bioid=1,
%       prefix=Sir,
%       orcid=0000-0000-0000-0000,
%       facebook=<facebook id>,
%       twitter=<twitter id>,
%       linkedin=<linkedin id>,
%       gplus=<gplus id>]
\author[1]{Mickaël Delcey}[orcid = 0000-0003-2225-8531]

% Corresponding author indication


% Footnote of the first author


% Email id of the first author


% URL of the first author


%  Credit authorship


% Address/affiliation


\affiliation[1]{organization={LEMTA, Université de Lorraine, CNRS},
    addressline={2, Avenue de la Forêt de Haye, B.P. 160}, 
    city={Vandœuvre-lès-Nancy},
    % citysep={}, % Uncomment if no comma needed between city and postcode
    postcode={54504}, 
    country={France}}
% Second author
\author[1]{Yoann Cheny}
\cormark[1]
% Third author
\author[1]{Jean Schneider}[%
   ]


\author[1]{Simon Becker}[]

\author[1]{Yvan Dossmann}



\cortext[cor1]{ yoann.cheny@univ-lorraine.fr}


% Address/affiliation
\affiliation[2]{Institut Universitaire de France (IUF)}

% Fourth author
\author%
[1,2]
{Sébastien Kiesgen De Richter}




% Corresponding author text




% For a title note without a number/mark


% Here goes the abstract
\begin{abstract}
Gravity currents in oceanic flows require simultaneous measurements of pressure and velocity to assess energy flux, which is crucial for predicting fluid circulation, mixing, and overall energy budget. In this paper, we apply Physics Informed Neural Networks (PINNs) to infer velocity and pressure field from Light Attenuation Technique (LAT) measurements for gravity current induced by lock-exchange. In a PINN model, physical laws are embedded in the loss function of a neural network, such that the model fits the training data but is also constrained to reduce the residuals of the governing equations. PINNs are able to solve ill-posed inverse problems training on sparse and noisy data, and therefore can be applied to real engineering applications. 
The noise robustness of PINNs and the model parameters are investigated in a 2 dimensions toy case on a lock-exchange configuration , employing synthetic data. Then  we train a PINN with experimental LAT measurements and quantitatively compare the velocity fields inferred to PIV measurements performed simultaneously on the same experiment. Finally, we study the energy flux field $J=p \boldsymbol{u}$ derived from the model. The results state that accurate and useful quantities can be derived from a PINN model trained on real experimental data which is encouraging for a better description of gravity currents and improve models of ocean circulation.


\end{abstract}

% Use if graphical abstract is present
% \begin{graphicalabstract}
% % Figure removed
% \end{graphicalabstract}

% Research highlights
%\begin{highlights}
%\item Physics Informed Neural Networks (PINNs) are robust to noisy and gappy data
%\item PINNs are able to infer velocity and pressure field from experimental data
%\item Computation of energy fluxes allow energy budgets for Lock-Exchange configuration 
%\item Modifying the continuity equation to take in account 3D effects improve the results
%\end{highlights}

% Keywords
% Each keyword is seperated by \sep
\begin{keywords}
Physics Informed Neural Networks \sep Gravity Currents \sep Lock-Exchange \sep Deep Learning
\end{keywords}


\maketitle


\section*{Introduction}
 Gravity currents are common in oceanic flows like ocean overflows, or hydrothermal vents for example. It is crucial to determine energy transport in such density varying flows to predict fluid circulation, mixing and overall budget. For this, it is necessary to evaluate the energy flux fields $J=p\boldsymbol{u}$ which requires simultaneous measurements of the pressure $p$ and velocity $\boldsymbol{u}$ simultaneously in $3$ dimensions ($3D$) in order to obtain an upper bound on the energy available for dissipation and turbulent mixing.  
%\noindent The lock-exchange flow  \cite{shin} has long served as a paradigm configuration for studying the spatio-temporal evolution of gravity currents \cite{ungarish2009introduction}.In  this experiment, fluids of different densities are separated by a vertical barrier in a tank. When the gate
%is removed, differences in the hydrostatic pressure cause the denser fluid to flow in
%one direction along the bottom boundary of the tank, while the lighter fluid flows in
%the opposite direction along the top boundary of the tank.  \\
In recent years, Particule Image Velocimetry (PIV) (\cite{perez2018piv}), Planar Laser Induced Fluorescence (PLIF) (\cite{crimaldi2008planar}) and Light Attenuation Technique (LAT) \cite{dossmann2016experiments} have been widely used and combined to get simultaneous measurements of velocity and density fields in 2D (\cite{balasubramanian2018entrainment}). 3D measurements have also been performed by using 3D-LIF (\cite{tian20033d}), Tomo-BOS (\cite{BOS}) and PIV-3D (\cite{elsinga2006tomographic}, \cite{partridge2019versatile}) techniques. 
Although well established, these techniques are very costly and difficult to implement in practice in a simultaneous manner in the field. Furthermore, it is necessary to use complex inverse methods to extract the pressure field from the density measurements. Thus, achieving a complete reconstruction of the hydrodynamic field remains a significant challenge.
In their work, \cite{nash2005estimating} proposed a method to extract the pressure field in the case of a linear perturbation of the latter by the density field. It succeeds to predict the time average energy flux but assumes that there is no contribution of the dynamic pressure. \cite{clarke} managed to overcome this limitation by using the Boussinesq polarization relations for calculating the pressure but requires nearly periodic perturbations of the density field. These methods provide leading order approximations for the averaged energy flux from measurements but cannot capture transient features contrary to Green function methods recently proposed by \cite{allshouse2016internal} but which remains valid only in the case of linear or very weakly non-linear perturbations of the density field. To account for non-linear perturbations of the density field, and achieve a more realistic reconstruction, data assimilation techniques can be employed  (\cite{da}, \cite{Mons_2022}) but this requires to run multiple CFD simulations which is prohibitively time consuming.

Recent advances in deep learning (\cite{Goodfellow}) and the universal approximation nature of Neural Networks offer new possibilities for reconstruction problems in fluid mechanics (\cite{brunton2020machine}) even for non linear perturbations of the hydrodynamic fields. However, the chaotic nature of the flows investigated makes the reconstruction problem most of the time out of the reach of purely data driven algorithms. In that context, Raissi et al. (\cite{raissi2020science}) recently proposed to use Physics-Informed Neural Network (PINNs) for solving forward and inverse partial differential equation problems which overcomes the low data availability in physical systems. It consists of adding prior knowledge about physical laws by constraining a neural network to respect a set of partial differential equations (PDEs). This method has quickly proven itself on various non-linear forward (\cite{mao2020physics}) , and inverse (\cite{cai2021physics}) problems for synthetic data (see \cite{cuomo2022scientific} for a complete review). Since PINNs can handle noisy and gappy data, as well as ill-posed problems, it seems to be the ideal candidate to tackle inverse problems with experimental data, however few applications on this subject are available. In \cite{esspresso2021flow} Kaniadakis demonstrated the ability of PINNs to solve inverse problems using experimental Background-riented schlieren (BOS) data, however, velocity fields inferred have only been validated by a qualitative comparison on PIV  measurements in a companion experiment. Recently, PINNs have also been applied BOS data to reconstruct high speed flows fields (\cite{exp1}), but no validation of the velocity field is available.

This paper has two main objectives, the first is to provide for the first time a quantitative validation of the velocity fields obtained by PINNs. To do this, we combine LAT and PIV measurements on the same lock-exchange experiments, then we build a Physics Informed Neural Network model to infer velocity and pressure from density measurements only, finally, we compare the velocity field inferred with PIV measurements kept for validation. 
The second objective is to demonstrate that this method can be used to directly calculate the energy flux $J=pu$ and to draw up energy budgets for strongly non-linear perturbations of the density field. 


The principles of Physics informed Neural Networks and the governing equations of Gravity Currents flow will be discussed in section \ref{section1} and \ref{section2} respectively.
In section \ref{section3} we study the noise robustness of PINNs on synthetical data for the lock exchange configuration.
We apply PINNs model on LAT experimental measurements from lock-exchange experiment in section \ref{section4}, velocity fields inferred are quantitatively compared to PIV measurements performed simultaneously on the same experience.
Finally, we discuss the energy flux and vorticity derived from the inferred pressure and velocity fields.



\section{Physics Informed Neural Networks.}\label{section1}

Artificial Neural Networks (ANNs) are known as universal approximators (\cite{Cybenko}) so they can in theory learn any measurable function from observed data.
A neural network consists of an input layer, one or more hidden layers, and an output layer. The layers are composed of nodes, also known as neurons, and each neuron is associated with a set of learnable parameters called weights and biases which are used to transform the input signals and compute the output of the model.  %are optimized to minimize an objective function called the loss function. The loss function penalizes the mismatch between the network predictions and  a set of observed data.%\\
A network of $N$  layers performs a series of computations, known as forward propagation, that transform the input vector $x$ through each layer of the network until a final prediction $a^{[N]}$ is obtained. :
\begin{subequations}
\begin{align}
& a^{[1]} = x \in \mathbb{R}^{n_{1}}, \\
& a^{[k]} = \sigma \left( W^{[k]}a^{[k-1]}+b^{[k]} \right) ,~ 2 \leq k \leq N ,\\
& a^{[N]} =  W^{[N]}a^{[N-1]}+b^{[N]},~ ~k = N ,
\end{align}
\end{subequations} Here $W^{[k]}\in \mathcal{M}_{n_{k}n_{k-1}}(\mathbb{R})$, $b^{[k]} \in \mathbb{R}^{n_{k}}$ are the weights and biases of the $k^{th}$-layers and $\sigma$ is a non linear transformation called activation function. $n_k$ is the number of neurons of the layer $k$.\\
\\ The learnable parameters of the networks : $\boldsymbol{\theta} = (W_{k},b_{k})_{k = 1\dots N}$ are adjusted through the training process until they reach an optimal value $\boldsymbol{\theta^{*}} = (W_{k}^*,b_{k}^*)_{k = 1\dots N}$ that minimizes a loss function $\mathcal{L}$  which penalizes the mismatch between the network predictions and observed data :
\begin{center}
\begin{align}
\label{argmin}
 ~~~~~~ ~~~~~~  ~~~~~~ \huge \theta^{*}= \underset{\theta}{\argmin}  ~\mathcal{L}(\theta)
\end{align}
\end{center} 
most of the time, for regression tasks, the loss function $\mathcal{L}$ is the \emph{Mean Squared Error} between the observations $\{a_{i}\}_{i = 1 \dots N_{\textrm{data}}}$ and the corresponding predictions of the network $\{a^{[N]}(x_{i})\}_{i = 1 \dots N_{\textrm{data}}}$ where $N_\textrm{data}$ is the size of the observed dataset.\\
Weights are updated at each training step, using the gradient descent :
\begin{center}
\begin{align}
\label{gradient}
~~~~~~ ~~~~~~  ~~~~~~  \theta_{n+1} = \theta_{n} - \eta \nabla_{\theta} \mathcal{L}(\theta),
\end{align}
\end{center}

where $\eta$ is called the learning rate.\\ The key point in the success of neural networks is \textit{backpropagation}. Backpropagation (\cite{higham2019deep}) is an algorithm for computing the gradients of the loss function with respect to the learnable parameters in a neural network. 


 Nowadays deep learning libraries such as \textit{Tensorflow/Keras} (\cite{geron2019hands}) and \textit{Pytorch} (\cite{pytorch}) include automatic differentiation (AD) tools (\cite{griewank1989automatic}) that allow an efficient computation of these gradients. 
\newline
 \newline
In the context of \emph{physics-informed neural networks} (PINNs) (\cite{raissi2020science}), a neural network $\boldsymbol{v}_{\mathcal{N}}$ is used to approximate the solution $\boldsymbol{v}$ of partial differential equations like :

\begin{subequations}
\label{eq:PINN}
\begin{align}
     &\partial_{t}\boldsymbol{v} + \mathcal{D}_{\textrm{\textbf{x}}}[\boldsymbol{v}],~~\textrm{\textbf{x}} \in \Omega,~~t\in [0,T], \label{eq:PINN:1}\\
     &\boldsymbol{v}(\textrm{\textbf{x}},0) = h(x), x \in \Omega , \\
     &\boldsymbol{v}(\textrm{\textbf{x}},t) = g(x,t),~~ \textrm{\textbf{x}}\in \partial \Omega, t\in [0,T],
\end{align}
\end{subequations}


where $\mathcal{D}_{\textrm{\textbf{x}}}$ is a linear or non linear differential operator , $ \Omega,~ \partial \Omega, [0,T]$ are respectively the spatial domain and his frontier, and the temporal domain. $h|_{\Omega}, g|_{\partial\Omega,[0,T]}$ are the initial and boundary conditions.  \\
We define $f$ that represents the left-hand-side of \ref{eq:PINN:1}:
\begin{center}
    \begin{align}
    ~~~~~~~~~~~~~~~f:=\partial_{t}\boldsymbol{v}+ \mathcal{D}_{\textrm{\textbf{x}}}[\boldsymbol{v}]
    \end{align}
\end{center}
$v_{\mathcal{N}}$ takes as input $(\textrm{\textbf{x}},t)$ and AD enables the computation of the derivatives of $\boldsymbol{v}_{\mathcal{N}}$ with respect to $\textrm{\textbf{x}}$ and $t$, thus we can get an explicit expression of $f(\textrm{\textbf{x}},t),~\forall (\textrm{\textbf{x}},t)$.
This stage is the key instance of PINNs because automatic differentiation does not introduce truncation errors due to numerical approximations as with conventional derivation methods.
The objective function to minimize is expressed as :
\begin{center}
\begin{subequations}

\begin{align}
    & \mathcal{L} = \lambda_{1}\mathcal{L}_{f}+\lambda_{2}\mathcal{L}_{b}+\lambda_{3}\mathcal{L}_{0}+\lambda_{4}\mathcal{L}_{\textrm{data}}, \\
    & \mathcal{L}_{f} = \dfrac{1}{N_f}\sum_{i=1}^{N_{f}}||f(\textrm{\textbf{x}}^{f}_{i},t^{f}_{i})||^{2}, \\
    & \mathcal{L}_{b} = \dfrac{1}{N_b}\sum_{i=1}^{N_{b}} ||\boldsymbol{v}_{\mathcal{N}}(\textrm{\textbf{x}}^{b}_{i},t^{b}_{i})-g(\textrm{\textbf{x}}^{b}_{i},t^{b}_{i})||^{2}, \\
    & \mathcal{L}_{0} = \dfrac{1}{N_0}\sum_{i=1}^{N_{0}} ||\boldsymbol{v}_{\mathcal{N}}(\textrm{\textbf{x}}^{0}_{i},0)-h(\textrm{\textbf{x}}^{0}_{i})||^{2}, \\
    & \mathcal{L}_{\textrm{data}} = \dfrac{1}{N_\textrm{data}}\sum_{i=1}^{N_{\textrm{data}}} ||\boldsymbol{v}_{\mathcal{N}}(\textrm{\textbf{x}}^{\textrm{data}}_{i},t^{\textrm{data}}_{i})-\boldsymbol{v}(\textrm{\textbf{x}}^{\textrm{data}}_{i},t^{\textrm{data}}_{i})||^{2}.
\end{align}    
\end{subequations}
\end{center}


Here $\mathcal{L}_{\textrm{data}}$ is due to additional observations inside the domain, and $\mathcal{L}_{f},~\mathcal{L}_{b},~\mathcal{L}_{0}$ penalize the residual equations, the boundary, and the initial conditions. \\
 The parameters $\lambda_{1\dots 4} \in \mathbb{R}_{+}$ are non trainable weights set manually to manage the ratio between all loss terms.\\
Collocation points for data, residuals and boundary conditions : $\{\textrm{\textbf{x}}^{*}_{i},t^{*}_{i} \}_{i=1 \dots N_{*}}$ are often selected on a discrete Cartesian grid. 
In most of the cases the algorithm selected to optimize the parameter $\theta$ is ADAM (\cite{kingma2014adam}), a first-order gradient-based optimization algorithm for stochastic objective functions .\\

In the next section, we will apply PINNs to gravity currents generated by lock exchange, to infer the density, velocity and pressure field with synthetical data from 2D simulation. We can see a schematic representation of PINN for this problem Fig. \ref{fig:sketchPINN}.




\definecolor{op}{gray}{0.9}
% Figure environment removed

\section{Governing equations of Lock-Exchange gravity currents}\label{section2}



In the lock exchange flow (\cite{shin}), fluids of different densities initially at
rest are separated by a vertical membrane, in a tank. The fluids in the left and right hand compartments have the respective densities $\tilde{\rho_1}$ and $\tilde{\rho_2}$ with  $\tilde{\rho_1} > \tilde{\rho_2}$, the density gradient
is caused for instance by salinity difference. Once the membrane is withdrawn the density difference $\Delta\rho = \tilde{\rho_1} - \tilde{\rho_2}$ between the two compartments induces the generation and propagation of a gravity current at the tank bottom. The intensity of turbulent structures and induced mixing in the current is controlled by the Reynolds number $Re$. The flow is governed by Navier-Stokes, mass transport and incompressibility equations. Moreover, in this paper, we study Gravity Currents induced by small difference in density, so that Boussinesq Aproximation (\cite{gray1976validity}) is  effective. 


\definecolor{op}{gray}{0.9}
% Figure environment removed

%\subsection{Governing equations}
In order to render the equations dimensionless (see ~\cite{hartel2000analysis}), we use $\tL_y$ as the characteristic
length scale and the characteristic velocity is the buoyancy velocity defined as~:
\begin{equation}\label{eq:buyvel}
    \tilde{u}_b=\sqrt{g\frac{\tilde{\rho_1}-\tilde{\rho_2}}{\tilde{\rho_a}}\tilde{L}_y}, \quad \text{with}\quad\tilde{\rho}_a=\frac{\tilde{\rho_1}+\tilde{\rho_2}}{2}.
\end{equation}
We consider small density difference $\tilde{\rho}_1 - \tilde{\rho}_2\ll \tilde{\rho}_1$ for which the Boussinesq approximation can be adopted, thus the governing equations read in dimensionless form~:



% We demonstrate the validity of the model with ideal numerical data generated by CFD for the case of gravity currents produced by lock-exchange, with Re = 5500. We use the spectral element code Nek5000 over the 3D dimensionless spatial and time domain $\Omega = [0,6.0] \times [0,1.0] \times [0,0.4]$ and $T = [0,9.7]$. We assume symmetry condition at $z=0.1$, boundary conditions at the bottom $z=0$, down  $y = 0$, left and right $x=0$ and $x=6.0$. The governing set of equations on this case are the dimensionless incompressible Navier-stokes equations and the mass transport equation expressed below :
% In dimensionless form the governing equations thus read :
\begin{subequations}
\label{eq:NS}
\begin{align}
    & \dfrac{\partial \boldsymbol{u}}{\partial t} + (\boldsymbol{u} \cdot \nabla )~ \boldsymbol{u}  +\dfrac{\partial p}{\partial x} - \dfrac{1}{Re}\nabla ^{2} \boldsymbol{u}  \label{eq:momx} + \rho ~ \boldsymbol{e_{y}}=0,\\
    & \dfrac{\partial \rho}{\partial t}+(\boldsymbol{ u} \cdot \nabla )~ \rho - \dfrac{1}{Re\,Sc}\nabla^{2}\rho \label{eq:mass}=0,\\
    & \nabla \cdot \boldsymbol{ u} = 0\label{eq:cont},  
\end{align}
\end{subequations}
where  $\boldsymbol{u} = (u,v)^\intercal$ denotes the dimensionless velocity vector. The non-dimensional pressure $p$ and density $\rho$ are given by~:
\begin{equation}\label{eq:prhoadim}
    p=\frac{\tilde{p}}{\tilde{\rho}_a\tilde{u}_b^2},\quad \rho=\frac{\tilde{\rho}-\tilde{\rho}_2}{\tilde{\rho_1}-\tilde{\rho_2}}.
\end{equation}
The Reynolds number $Re$ and the Schmidt number $Sc$ arising in the dimensionless equations \eqref{eq:NS} are defined by~:
\begin{equation}\label{eq:prhoadim}
    Re=\frac{\tilde{u}_b\tL_y}{\tilde{\nu}},\quad Sc=\frac{\tilde{\nu}}{\tilde{\kappa}},
\end{equation}
where $\tilde{\nu}$ is the kinematic viscosity and $\tilde{\kappa}$ denotes the molecular diffusivity of the chemical specie producing the density difference. 




\section{Application of PINNs on synthetical data}\label{section3}

\subsection{synthetic data for the lock-exchange problem}

An accurate representation of the interface between the miscible fluids is crucial in gravity currents simulations, which requires high-order numerical methods to compute steep gradients in the vicinity of the interface. To this end, the governing equations~\eqref{eq:NS} are solved with the code Nek5000 (\cite{nek5000-web-page}) that has been for example successfully employed in numerical investigations by ~\cite{ozgokmen2004three}. The discretization scheme in Nek5000 is based on the spectral element method proposed by ~\cite{patera1984spectral} with exponential convergence in space and $3^\text{rd}$-order timestepping scheme. 

We consider the dimensionless computational domain $\Omega = [0,6.0] \times [0,1.0]$ and $T = [0,10]$ with $Re=5500$. While for liquids such as salt water $Sc\simeq 700$, we consider here $Sc=1$. This assumption is commonly made to reduce computational costs and has low influence on the gravity current front as shown by ~\cite{marshall2021effect}.
\noindent A free-slip condition (no normal flow) is applied at $y=1.0$ and no slip-conditions are applied on the remaining domain boundaries. Grid independent results were obtain on a computational mesh of $90\times45$ elements where unknowns are represented by $7^\text{th}$-order Lagrange interpolating polynomials, and with a fixed timestep $\Delta t = 2.10^{-2}$ giving a Courant–Friedrichs–Lewy number less than $0.5$. We show in Fig.~\ref{fig:evolutionrho2D} the time evolution of the density field obtain with Nek5000.



% Figure environment removed



We build from this simulation a training dataset $\mathcal{D}$ where the collocation points are selected in the sub domain : $\Omega_{obs} \times T_{obs}= [0.0, 4.5] \times [0,1.0] \times [4.0,7.5] \subset \Omega \times T $,  which is discretized into a uniform Cartesian grid of $N_x \times N_y \times N_t$ points , where $N_{x}=750$, $N_{y}= 150$ and $N_{t} = 400$.

Finally, to mimic experimental measurements of LAT, we extract from this simulation the observed quantity of the density $\rho$ on each collocation points, and build our training dataset $\mathcal{D} = (\textbf{\textrm{x}}_i,t_i,\rho_i)_{i=1 \dots N_xN_yN_t}$ while the other fields $u,~v,~p$ are used for validation purpose.


\subsection{Noise robustness}

In this section, noises of varying magnitudes were added to $\mathcal{D}$.
We investigate 4 different cases : 
\begin{itemize}
    \item \textbf{Case 1 : } Original dataset $\mathcal{D}$.
    \item \textbf{Case 2 : } We add a gaussian noise to $\rho$ field with standard deviation $\sigma = 5\%$ (experimental precision of LAT).
    \item \textbf{Case 3 : } same as Case 2 but with $\sigma =25\%$.
    \item \textbf{Case 4 : } Same as case 3 but in addition, we remove a part of the information in all the snapshots. (we draw 200 squares of $10 \times 10$ pixels randomly from Case 3 dataset 
    ).
\end{itemize}
A visualization of training data for $\rho$ for cases 1-4 is available Fig. \ref{fig:vis}

% Figure environment removed


For all cases, the PINN loss function is expressed as:


\begin{subequations}
\label{eq:Loss2D1}
\begin{align}
    & \mathcal{L} = \lambda\mathcal{L}_{\textrm{data}} + \mathcal{L}_{f}\\
    & \mathcal{L}_{f} = \dfrac{1}{N_f}\sum_{i=1}^{N_{f}}||f(\textrm{\textbf{x}}^{f}_{i},t^{f}_{i})||^{2} \\
    & \mathcal{L}_{\textrm{data}} = \dfrac{1}{N_{\textrm{data}}} \sum_{i=1}^{N_{\textrm{data}}} ||\rho_{\mathcal{N}}(\textrm{\textbf{x}}^{\textrm{data}}_{i},t^{\textrm{data}}_{i})-\rho(\textrm{\textbf{x}}^{\textrm{data}}_{i},t^{\textrm{data}}_{i})||^{2}  \label{eq:Loss2D1:data}
\end{align}    
\end{subequations}

The neural networks $\mathcal{N}$ take as input $(\textbf{\textrm{x}},t) = (x,y,t)$ and outputs the prediction $\boldsymbol{v}_{\mathcal{N}}(\textbf{\textrm{x}},t) = (\rho_{\mathcal{N}},u_{\mathcal{N}},v_{\mathcal{N}},p_{\mathcal{N}})$. The term $\mathcal{L}_{f}$ penalizes the governing equations ~\eqref{eq:NS} and $\mathcal{L}_{\textrm{data}}$ penalizes mismatch between network prediction for $\rho_{\mathcal{N}}$ and observed $\rho$.  The parameter $\lambda$ is a weighting  coefficient used to reinforce the influence of \ref{eq:Loss2D1:data}, in practice we set $\lambda = 500$.

The neural network is composed of 8 layers of 190 neurons each. The stochastic Adam optimizer \cite{kingma2014adam} is employed for solving \ref{argmin} : Glorot normal initializer \cite{glorot2010understanding} is employed to initialize the biases and the weights that are computed iteratively with a gradient update \ref{gradient} on a subset $\mathcal{B} \subset \mathcal{D}$ called a batch. One training round over $\mathcal{D}$ is called an epoch.  Training is done for 100 epochs with a batch size of 4096. We use a decreasing learning rate schedule. The learning rate starts at 1e-3 and ends at 1e-5. swish  activation function \cite{swishramachandran2017searching} is selected for all layers except the last one.  Finally, training is done with two GPU A6000 in parallel.
The choice of these hyperparameters provides a goodsatisfactory compromise between quality of the results and computation time.

\subsection{Results on synthetical data}
For quantitative results, we define the following relative L2-norm :

\begin{equation}\label{eq:L2gene}
\epsilon_V =\frac{100}{\sup_{\bfx \in  \Omega_{obs} \times T_{obs}} \left| V_\bfx \right|}\sqrt{\frac{\sum_{\bfx\in\bfX}	\left|V_{\mathcal{N}}(\bfx) - V_\bfx\right|^{2}}{\left|\bfX\right|}}
\end{equation}
where $V \in (\rho,u,v,w,p)$ and $\bfX \subseteq \Omega_{obs} \times T_{obs}  $ defines the domain where the error is computed. We choose $\sup_{\bfx \in \Omega_{obs} \times T_{obs}} \left| V_\bfx \right|$ as reference value to avoid the division near to zero issue reported in \cite{pof}. 



In Fig. \ref{fig:profil1}, we can see that the models for cases 1,2 and 3 are very efficient and accurate for the inference of all fields. Case 4 also performs great mostly everywhere, but we can see a big drop in performance near $x = 1.5$ which may be due to a lack of training data in this area. For all the cases, the results for $u$ and $v$ close to $x = 4.49$ are less accurate, but the steep variations near the front are well inferred.


Fig. \ref{fig:profil2}  shows the evolution in time of the error in $\Omega_{obs}$ for the fields $\rho,~u,~v$. We can see the $L_2$-errors for the density and pressure field are almost constant in time except for density in Case 4. For all cases, the accuracy of the predictions $u$ and $v$ improves with time. Since gravity currents propagate from left to right, at the initial stages of the flow, a large portion of the area covered by the flow contains zero values that might not be useful for training the neural network, making it more challenging to effectively learn and approach the velocity. However, as the gravity current progresses and reaches the end of the flow, the entire current is present in $\Omega_{obs}$, and more useful values such as non zero variations of density can be utilized by the neural network, possibly leading to improved performance.


Table 1 illustrates the global error for Cases 1 to 4 between the density, velocity and pressure fields predicted by PINN, and the corresponding ground thruth. The first three cases have highly accurate predictions , indicating that the model is robust to noise. Despite the higher errors in the fourth case, the predictions still fall within an acceptable range, suggesting that the model can handle incomplete data with high noise level.





These results are very promising and the robustness of PINNs to noise in the 2D lock exchange configuration suggests the potential of this framework for real experimental data.


In the following section, we apply the PINN model to experimental data obtained by LAT.

\clearpage


% Figure environment removed
    



% Figure environment removed


\begin{table}[ht!]
\centering
\begin{tabular}{ p{1.9cm}p{1.1cm}p{1.1cm}p{1.1cm}c  }

Case & $\epsilon_\rho$ & $\epsilon_u$  & $\epsilon_v$  &  $\epsilon_p$  \\
 \hline
Case $n^{o}$1 & 0.035\% & 1.17\%  & 1.81\%  & 1.03\% \\
Case $n^{o}$2 & 0.085\% & 1.29\%  &  2.41\%  & 1.07\% \\
Case $n^{o}$3& 0.53\% & 2.65\% & 5.30\% &2.48\%\\
Case $n^{o}$4 & 7.98\% & 4.93\% & 5.12\% & 4.48\% \\
\end{tabular}
\label{table:numericalL2relative}
\caption{Relative $L_2$-norm errors of the fields reconstructed by PINN computed on the spatio-temporal domain $\Omega_{obs} \times T_{obs}$.}
\end{table}








\section{Application of PINNs to LAT experimental measurements }\label{section4}


\subsection{Experimental setup}
The experimental apparatus is summarized in Fig.  \ref{fig:apparatus}. Experiments are conducted in a $150$~cm long, $10$~cm wide and $35$~cm high plexiglas tank. After the tank is filled with freshwater to an height $25$ cm, a gate with insulating sides is inserted at 20 cm from the left end of the tank. 

Salt is dissolved in the lock to reach a homogeneous density $\tilde{\rho}_1= 1001.2~\textrm{kg.m}^{-3}$ slightly larger than the freshwater density $\tilde{\rho}_2=999.8~\textrm{kg.m}^{-3}$ which gives us $\Delta \rho = 1.4~\textrm{kg.m}^{-3}$, $Re= 14000$ and $Sc=1000$ according to the definitions in section (\ref{eq:prhoadim}).


Measurements are performed using two cameras and two light sources.
The PIV and LAT cameras are respectively mounted with a $180$ mm f2-8 and $25$ mm lens. 
The cameras are mounted on tripods and placed at a distance of $250$ cm from the tank. Images are recorded at a frame rate of $10$ fps and the spatial resolution of the raw images is 8Mpx for both cameras.

The PIV (particle image velocimetry \cite{raffel2007particle}) camera is synchronized with a vertical LASER sheet that illuminates the central section of the tank, providing two-dimensional vertical velocity field measurements as commonly performed in stratified flows.

The LAT (light attenuation technique) camera is synchronized with a white LED panel place behind the tank. The latter technique allows to measure high resolution cross-averaged density fields by adding a known concentration of tracer in the dense compartment. In the present experiment, a red food dye is used as a tracer for salinity and hence density. The reader may refer to \cite{dossmann2016experiments} for a detailed description of the LAT protocol.

The two cameras and light sources are computer\\-controlled, which permits to synchronize velocity and density measurements. As the LED panel interferes with the PIV measurements, it is required to trigger the PIV and LAT sources in a slightly phase-shifted manner. Measurements are latter recombined on an identical timestep during the PIV post-process. A common spatial origin is used to match the recorded velocity and density field. 












 








 
  % Figure environment removed



%We conduct the experiment with $\Tilde{\rho}_{2}- \Tilde{\rho}_{1}=  7kg.m^{-3}$ which gives us $Re= 14000$ and $Sc=1000$, 

As the studied flow evolves in 3D, using the canonical PINN model would imply forcing a neural network to impose the Navier-Stokes equations in 3D and having available measurements of the density field in the whole tank which is prohibitive in our case. 


So in the following, we assume that the flow is homogeneous in the z-direction, this assumption is commonly made for the lock release configuration \cite{inbook}. As a consequence, we reduce the problem size and consider only the 2D Navier-Stokes equations \ref{eq:NS} and, under this assumption, the spanwise-averaged density $\overline{\rho}$ measured by LAT is assimilated to 2D density measurements $\rho$.
In Fig. \ref{fig:exppivlat}, we can see a sample of the fields measured by the combination of LAT and PIV. 

\subsection{Training data}

The data are acquired on the spatial and temporal dimensionless domain $\Omega_{exp} \times T_{exp} = [3.0,4.65] \times [0.05,0.5] \times [19.5,21.5]$.

this domain is discretized into a uniform grid of $N_x \times N_y \times N_t$ points , where $N_{x}=1050$, $N_{y}= 300$ and $N_{t} = 90$.



% Figure environment removed


%As a consequence of the homogeneity assumption, LAT density measurements $\overline{\rho}$ become $\rho$ due to invariance of density in depth.\\
Our objective is to infer the hydrodynamic field in the middle plane of the domain given LAT measurements of density. We keep the available PIV measurements for validation purpose. 

We apply canonical PINN method with LAT experimental measures as observation data in the domain $\Omega_{exp} \times T_{exp}$ with Navier-Stokes equation \ref{eq:NS} and loss fonction \ref{eq:Loss2D1}. 

The neural network is composed of 8 layers of 250 neurons each which, in our case is the structures which gives the best results. Training is done for 150 epochs with a batch size of 4096. We use Adam optimizer with a decreasing learning rate schedule. The learning rate starts at 5e-4 and ends at 1e-5. Finally we use $swish$ activation function on each layers transition, except for the last layers where no activation function is selected. 

% Figure environment removed




\subsection{Results on experimental data}
In Fig \ref{fig:profil2div}, we can see a comparison between predicted $\rho$ $u$ and $v$ fields and the corresponding measured fields by LAT/PIV. Since the model is trained on density, it is not surprising that the predicted density values are highly accurate, but we can notice that PINN has a smoothing effect on the density field. What is the most interesting here is the comparison on the velocity fields. Indeed, the NN has succeeded without prior information on $u$ and $v$ to reproduce the right amplitudes of values as well as the location of the current front. On the other hand, there is a bias in the values for $u$ and the small variations above the current (blue profile) are poorly captured. Moreover, for $v$, the values on the right of the front seem to be outliers. 


After a thorough study of the problem parameters, we came to the conclusion that the residual related to the continuity equation (\ref{eq:cont}) in 2D degraded the results. So, to account for the spanwise variations of the flow, we propose a modification of the continuity equation (\ref{eq:cont}) :

\begin{equation}
  \label{eq:cont1}
     ~~~~~~~~~~~~~~~~~~~~\frac{\partial u}{\partial x} + \frac{\partial v}{\partial y} + \xi = 0 
\end{equation}


where $\xi$ which depends on $(x,y,t) \in \Omega_{exp} \times T_{exp}$ is a new output of the neural network. 
Thus, the new model $\mathcal{M}$ take as input $(\textbf{\textrm{x}},t) = (x,y,t)$ and outputs the prediction $\boldsymbol{v}_{\mathcal{M}}(\textbf{\textrm{x}},t) = (\rho_{\mathcal{M}},u_{\mathcal{M}},v_{\mathcal{M}},p_{\mathcal{M}},\xi_{\mathcal{M}})$.
Since the 3D Navier-Stokes equations involve $\dfrac{\partial w}{\partial z}$ in the continuity equation, we expect $\xi_{\mathcal{M}}$ to converge during training to the restriction of $\dfrac{\partial w}{\partial z}$ to the PIV plane. 
In the following, we denote by $\mathcal{N}$ or $\mathcal{M}$ either if the model was trained with the equation (\ref{eq:cont}) or with (\ref{eq:cont1}).

% Figure environment removed


The modification made to the equation has led to significant improvements in the accuracy of the predictions.
The results Fig. \ref{fig:profilw_z} show the efficiency of this new formulation of the problem. 
 For the $u$ component of velocity, although a small bias can still be observed on the values above the stream, the predicted quantities inside the stream and near the blue zone are suitable. The predicted  profiles for the $v$ component of velocity in the blue and green zones are now accurate, and the model is able to capture steep variations, such as the large peak at $x=4.3$, with good accuracy. 

\begin{table}[h]
\centering
\begin{tabular}{ p{1.9cm}p{1.1cm}p{1.1cm}c  }
Model & $\epsilon_\rho$ & $\epsilon_u$  & $\epsilon_v$  \\
 \hline
$\mathcal{N}$ & 1.72\% & 20.90\%  &  31.5\%   \\
$\mathcal{M}$   & 1.35\% & 14.6\% & 12.65\% \\
\end{tabular}
\caption{Relative $L_2$-norm errors on the spatio-temporal domain $\Omega_{exp} \times T_{exp}$ between reconstructed and measured fields. model name is $\mathcal{N}$ or $\mathcal{M}$ either if training was done with standard continuity equation (\ref{eq:cont}) or variation (\ref{eq:cont1})}
\label{table:expL2relative}
\end{table}

The table \ref{table:expL2relative} shows the overall improvement in predictions brought by the simple change in the continuity equation \ref{eq:cont1}. The model achieve great results on the global space $\Omega_{exp} \times T_{exp}$.

Model $\mathcal{M}$ doesn't take into account all the 3D effects, but correction (\ref{eq:cont1}) makes it more precise. 
The surrogate model is fully differentiable and enables the computation of various quantities of interest, including vorticity, pressure, and energy fluxes and budgets.
In the following, we examine the pressure field inferred by analyzing energy fluxes and the residual of the energy conservation equation given by :

\begin{equation}
\label{eq:energy}
\begin{aligned}
     & \left| e_{5} \right|= \bigg| \frac{\partial}{\partial t}\left( \frac{|\boldsymbol{u}|^{2}}{2}\right)  + \nabla \cdot \left[ \left( \dfrac{|\boldsymbol{u}|^{2}}{2} + p\right)\boldsymbol{u}\right]  \\
     & -\nabla \cdot (\dfrac{1}{\textrm{Re}} \nabla \boldsymbol{u} \cdot \boldsymbol{u} ) + \dfrac{1}{\textrm{Re}} \nabla\boldsymbol{u} :  \nabla\boldsymbol{u} + \rho\boldsymbol{u} \cdot \boldsymbol{e_y} \bigg|.
\end{aligned}
\end{equation}


In Fig. \ref{fig:stream}, we represent the quantity $J = p \boldsymbol{u}$ where $\boldsymbol{u} = (u,v)^{T}$, and respectively the isovalues of the separated components $pv$, $pu$ given by $\mathcal{M}$ at three different times during the flow. These quantities provide information about the energy flux within the gravity current.

We clearly evidence a zone where the current transfers energy to the surrounding fluid, which are the zone where $pv$ reach its maximum values in the right of the current front. In the same way, one identifies a zone where the current captures energy from the ambient fluid which are the zone of circulation above the current where $ pu $ reach its minimum values. We can notice a very sharp transition between these two zones (red dotted line) where $pv$ changes sign. This criterion is consistent through time and represents a quantitative criterium to divide the density field in two parts, the front and the head of the current. In the last raw of Fig. \ref{fig:stream} vorticity isovalues inferred $\omega$ are plotted and superposed to the energy flux vector field. We observe that the transition between the two previously identified zones is also associated with the appearance of re-circulation rolls where the mixing becomes efficient. 

 Although we have no direct experimental measurement of the pressure field for validation, it is interesting to discuss its relevance in regards to the energy conservation equation (see eq (\ref{eq:energy})).
For that, we plot in Fig. \ref{fig:e5} the magnitude of energy residual $\lvert e_{5} \rvert$ defined as:



 We find that $\lvert e_{5} \rvert$ is around $3 \times 10^{-2}$. This error is small from a physical point of view, especially considering the experimental errors of measurements for such unsteady flows but is significant from a mathematical point of view. This can be explained by the nature of the physical model used to model the flow here.
  Indeed, we use the 2D equations to train the neural network. This approximation is justified in view of the mainly two-dimensional character of the flow but the small fluctuations of the transverse component of the velocity can carry a non-negligible fraction of the energy which would explain a significantly high value of the residuals $e_{5}$. An improvement in the future would be to develop the same type of approach but using 3D flow reconstruction methods from 2D measurements (see \cite{pof}). The implementation of the 3D equations would then allow to significantly reduce the error on the energy transport equation. 






% Figure environment removed



% Figure environment removed

\clearpage


\section{Conclusion}

In this work, we quantitatively demonstrate that PINNs are able to solve inverse problems from experimental data. We first investigated the robustness of this model to noisy and gappy data. This study was made from synthetic data obtained by the CFD solver Nek5000 for the lock-exchange configuration, and allowed us to study the parametric aspect of the model. Then we trained a physics informed model from LAT measurements acquired on the lock-exchange configuration, the inferred velocity fields have been compared to PIV measurements made simultaneously on the same experiment. Despite the assumption of homogeneity in z-direction, the results are convincing and the model succeeds in predicting the pressure field. Based on the energy flux computed from the results, we discuss the structure of the gravity current and show the potential of Physics Informed Neural Networks for ocean modeling.




\section{Bibliography}






\printcredits

%% Loading bibliography style file
\bibliographystyle{model1-num-names}
%\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{biblio}






\end{document}

