\documentclass[12pt]{article}

\usepackage{arxiv}
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{amssymb,amsmath,amsfonts,eurosym, geometry,ulem,graphicx, caption, color,setspace, comment, footmisc,caption, pdflscape, subfigure, hyperref,array, algpseudocode}

\usepackage[ruled]{algorithm2e}
\usepackage[numbers]{natbib}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{caption}
\usepackage{amsthm}
\usepackage{lipsum}
\usepackage{etoolbox}
\usepackage{enumitem}
\usepackage{float}
\usepackage{physics}
\allowdisplaybreaks[3]
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}

\renewcommand{\shorttitle}{A Quantitative Analysis of OSS Quality: Insights from Metric Distributions}

% \geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}

\title{\textbf{\Large A Quantitative Analysis of Open Source Software Code Quality: Insights from Metric Distributions \\}}

\author{Siyuan Jin$^{1,2}$, Mianmian Zhang$^{1}$, Yekai Guo$^{3}$, Yuejiang He$^{1}$, \\
\textbf{Ziyuan Li$^{4,1,*}$, Bichao Chen$^{1,*}$, Bing Zhu$^{1,*}$, and Yong Xia$^{1,*}$}\\
	\normalsize $^{1}$ HSBC Laboratory, Guangzhou, China\\
	\normalsize $^{2}$ School of Business and Management, Hong Kong University of Science and Technology, Hong Kong, China\\
	\normalsize $^{3}$ School of Data Science, Fudan University, Shanghai, China\\
 	\normalsize $^{4}$ School of Physics, Sun Yat-sen University, Guangzhou, China\\
	\normalsize liziyuan3@mail.sysu.edu.cn, bichao.chen@hsbc.com, bing1.zhu@hsbc.com, yong.xia@hsbc.com\\
	\normalsize *corresponding author
}

\newcolumntype{C}{>{\centering\arraybackslash}X}
\bibliographystyle{unsrt}

\begin{document}
\maketitle
\begin{abstract}
Code quality is a construct in open-source software (OSS) with three dimensions: maintainability, reliability, and functionality. We identify 20 distinct metrics and categorize them into two types: 1) monotonic metrics that consistently influence code quality; and 2) non-monotonic metrics that lack a consistent relationship for evaluation. We propose a distribution-based method to evaluate both types, which demonstrates great explainability of OSS adoption. Our empirical analysis includes more than 36,460 OSS repositories and their raw metrics from SonarQube\footnote{https://www.sonarsource.com} and CK\footnote{https://github.com/mauricioaniche/ck}. Our work contributes to the multi-dimensional construct of code quality and its metric measurements.
\end{abstract}
\vspace{1.5ex}
\keywords{Open source software, Code quality, Construct dimensions, Measurements}

\onehalfspacing
\section{Introduction}
Code quality positively influences software adoption \cite{crowston2003defining}. Precise code quality measurement can improve software products, increase user satisfaction, and save costs of IT systems \cite{kekre1995drivers}, which influences the success and adoption of software \cite{levine2010quality}. Figure \ref{fig: Construct} shows that code quality is a multi-dimensional construct that includes dimensions \cite{edwards2001multidimensional}: maintainability, reliability, and functionality. We identified 20 distinct metrics from previous literature to measure these dimensions. However, the literature remains a gap in the methodologies for evaluating them. We propose a distribution-based method to provide scores for each metric, which shows great explainability on OSS adoption.

\textbf{RQ.~1: How to evaluate code quality metrics?}
We divide metrics into monotonic metrics and non-monotonic metrics. Monotonic metrics consistently impact code quality, while non-monotonic metrics lack a consistent monotonic relationship for evaluation (Figure \ref{fig:metric_dist}). We evaluated their scores by analyzing their probability distributions among high-star OSS. For monotonic metrics, we fit an exponential distribution and use the distance from 0 in their cumulative distribution functions (CDFs) as their scores. For non-monotonic metrics, we fit an asymmetric Gaussian distribution and use the distance away from the central point in their CDFs as their scores. Our evaluation method provides scores within the range of $0\sim100$ for each metric.

Our empirical analysis covers 36,460 high-star repositories. The selection of high-star repositories provides a more critical evaluation because they generally have lower bugs, resulting in sharper distributions. 

\textbf{RQ.~2: How to use the evaluated scores to explain OSS adoption?}
We investigated the explainability of our code quality metric scores on OSS GitHub stars. The number of Github stars reflects OSS quality and adoption \cite{medappa2019does}. With standard machine learning approaches, we use R2 and accuracy to assess their explanatory power. The results show our code quality scores can explain the number of OSS stars well. Our methodology can be applied to different target variables, providing a flexible strategy for evaluating code quality in various contexts.

% Figure environment removed

This work has the following contributions. Prior literature has discussed diverse code quality metrics \cite{mccabe1976complexity, stamelos_code_2002, shin2010evaluating, bianchi_organizational_2012}. We extend them by dividing code quality metrics into two types and evaluating them with a novel distribution-based method. We evaluate code quality metric scores for over 36,460 GitHub OSS repositories.
We then use our scores to explain the GitHub stars \cite{lee2009measuring}, generating insights into how code quality may influence the widespread adoption of OSS. Our study advances the understanding of code quality with two different types of code quality metrics and contributes to better quality control standards and practices.

\section{Literature Review} \label{sec: lr} 
OSS enhance code quality \cite{ljungberg2000open, von2006promise} and innovation \cite{levine2014open, vitharana2010impact,kumar2011competitive}. Many firms utilize and contribute to OSS \cite{mehra2011firms} and developers reuse OSS to lower their search cost \cite{haefliger2008code}, which requires high code quality. 

Many research studied performance evaluations for software. Inappropriate performance measurements have been identified as a major cause of IT systems failing \cite{kekre1995drivers, fitoussi2012outsourcing}. As new technologies and techniques emerge \cite{jin2022cev}, more precise measurements of software quality and fit are needed. Our approach sets itself apart from past studies by taking into account the distribution of high-quality software and delivering accurate scores for each software.

Code quality has dimensions \cite{polites2012conceptualizing}. The IEEE standard defines code quality as the collective features and characteristics of software that meet given needs \cite{fitzpatrick1996software}. Later on, user-friendliness and useful functionalities are included in the definition of code quality \cite{lee2009measuring}, echoing the three dimensions in the ISO/IEC 25010 standard \cite{klima2022selected}: maintainability, reliability, and functionality \cite{athanasiou2014test}. Similarly, other studies have similar dimensions: maintainability \cite{motogna2023empirical}, readability \cite{gonzalez2023reliability}, and functionality \cite{shen2020api}. We define the construct and dimensions in Table \ref{tb: construct}.

\begin{table*}[]
\footnotesize
\centering
\caption{Construct Definition \label{tb: construct}}
\begin{tabular}{@{}p{1.5cm}p{4cm}p{2.2cm}p{7.5cm}@{}}
\toprule
\textbf{Construct} & \textbf{Definition} & \textbf{Dimensions} & \textbf{Definition} \\ 
\midrule
\multirow{3}{5cm}{Code Quality} & \multirow{3}{4.5cm}{How well-written the code is, including maintainability, reliability, and functionality. \cite{lee2009measuring}} & Maintainability & The code is easy to understand, enhance, or correct. \cite{deligiannis2003empirical} \\
& & Reliability & The code is user-friendly and stable. \cite{lee2009measuring} \\
& & Functionality & The code has useful functions. \cite{lee2009measuring} \\ 
\bottomrule
\end{tabular}
\end{table*}

Code quality has metric measurements \cite{bianchi_organizational_2012}, including the size of components \cite{stamelos_code_2002}, code complexity \cite{mccabe1976complexity,shin2010evaluating}, and so on. However, most existing metric identifications have focused on monotonic areas rather than non-monotonic metrics, simplifying the analysis. Our paper considers both and proposes a uniform solution for them.

Some measurements can reflect the level of code quality, such as the number of stars \cite{medappa2019does}. Although adoption and OSS activities are determined by many factors, such as commitment \cite{maruping2019developer}, transparency \cite{shaikh2016folding}, and leader resources \cite{dong2021project}, OSS adoption can partially reflect OSS code quality. Lee et al. \cite{lee2009measuring} highlight the impact of code quality on user satisfaction and adoption. Therefore, we use GitHub stars as a reflective measure of code quality. 


\section{Methodologies} \label{sec: method}
% To effectively evaluate code quality metrics, we provide a distribution-based metric evaluation method on high-star GitHub repositories.

To quantitatively evaluate code quality metrics, we first map out the distribution of each metric in high-star OSS repositories and then score them according to their corresponding metric CDFs.

% Figure environment removed

Table \ref{tab:metric_definition} presents two different types of metrics distributions: monotonic and non-monotonic metrics. We fit exponential distributions to monotonic metrics, the probability distribution function (PDF) of which reads as:
\begin{equation}\label{eq:exp_pdf}
f_1(x;c,\lambda) = \begin{cases}
0 & \text{if } x \leq c  \\
\lambda \exp\left[-\lambda {(x-c)}\right] & \text{if } x > c 
\end{cases} \
\end{equation}
where $\lambda$ and $c$ are the fitting parameters. The corresponding score function based on the CDF of Eq. \eqref{eq:exp_pdf} reads as
\begin{equation}\label{eq:exp_score}
\small
\begin{split}
& M_1(x; c, \lambda) =  100 \times \begin{cases}
1 & \text{if } x \leq c \\
\exp\left[-\lambda {(x-c)}\right]  & \text{if } x > c
\end{cases} \
\end{split}
\end{equation}
% \begin{equation}\label{eq:4}
% \begin{split}
% & M_1(x, \theta, c) =   \\
% & 100 \times \begin{cases}
% 1 & \text{if } c \leq 1e-6, \\
% 1 -  \lambda \exp\left[-\lambda {(x-c)}\right]  & \text{if } c > 1e-6.
% \end{cases}
% \end{split}
% \end{equation}
The score falls into the range of $0\sim100$ and it peaks at $c$ and decays exponentially for $x>c$.

The non-monotonic metrics follow an asymmetric Gaussian distribution (see the left of Fig. \ref{fig:metric_dist}), the PDF of which reads as
\begin{equation}\label{eq:agass_pdf}
\begin{split}
& f_2(x;\mu, \sigma_1, \sigma_2) = \\
&
\begin{cases}
\frac{1}{\sqrt{2\pi}}\frac{2}{{\sigma_1} + {\sigma_2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma_1^2}\right) & \text{if } 0\leq x < \mu \\
\frac{1}{\sqrt{2\pi}}\frac{2}{{\sigma_1} + {\sigma_2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma_2^2}\right) & \text{if }  x \geq \mu 
\end{cases} \
\end{split} 
\end{equation}
where $\mu, c, \sigma_1,\sigma_2$ are fitting parameters representing the peak position, peak height on the right, and peak widths on each side, respectively. The corresponding score function is
\begin{equation}\label{eq:agass_score}
\small
\begin{split}
&M_2(x, \mu, \sigma_1, \sigma_2) = \\
& 100 \times
\begin{cases}
1 - \operatorname{erf}\left(\frac{x-\mu}{\sigma_1 \sqrt{2}}\right) & \text{if }  0\leq x < \mu\\
1 - \operatorname{erf}\left(\frac{x-\mu}{\sigma_2 \sqrt{2}}\right) & \text{if } x \geq \mu
\end{cases}\
\end{split} 
\end{equation}
where the score falls into the range of $0\sim100$, peaks at $\mu$, and decays according to the Z-score of the Gaussian function on each side.


To obtain an overall score, we assign weights to individual scores. The overall score for a given repository, denoted by $k$, can be computed as follows:
\begin{equation} \label{eq: overall_score}
\small
Q^{overall}_{k} = \sum_{i}{{\omega_{i}} \cdot Q^{metric}_{i,k}} \, ,
\text{subject to:} \sum_i{\omega_i} = 1 .
\end{equation}
The weights $\omega_i$ are derived from the importance values from supervised learning models for metric scores to a target variable such as repository stars. 

%We describe an application scenario where the target variable is the number of GitHub stars, as depicted in Figure \ref{fig: Workflow} as follows.
% Figure environment removed




\section{Empirical Analysis} \label{sec: application}

\subsection{Data Sources}
\begin{table*}
\renewcommand{\arraystretch}{1.1}
\footnotesize
\centering
\caption{Statistical Summary}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Programming Language} &  \textbf{Max Number of Stars} & \textbf{Min Number of Stars} & \textbf{Number of Filtered Repositories} \\ \midrule
Java                                        & 50k                     & 100                     & 1,645                           \\
Python                                      & 228k                    & 260                     & 16,096                          \\
JavaScript                                & 107k                    & 270                     & 7,722                           \\
TypeScript                                  & 202k                    & 60                      & 10,997                          \\ \bottomrule
\end{tabular}
\label{tab:repo-statistics}
\end{table*}

GitHub is the largest OSS management platform that has more than 39 million public repositories (As of June 2023). We selected a subset of repositories with Java, Python, JavaScript, and TypeScript as the main programming languages and sorted them by the number of stars. We collected code from the top $\sim20,000$ repositories for each programming language. The number of GitHub stars is a measure of OSS adoption \cite{medappa2019does}. We removed non-engineering repositories by pattern matching, such as a guide for Java interviews in \href{https://github.com/Snailclimb/JavaGuide}{JavaGuide}. 

We used code scanners to obtain metrics. Scripting language repositories (Python, Javascript, TypeScript) can be directly imported, while non-scripting Java repositories need to be compiled first. Compiling Java repositories is challenging due to their different JDK, maven, or Gradle versions. Therefore, we only chose repositories with GitHub releases for compilation, which led to 36,460 repositories and over 600 million lines of code. Table ~\ref{tab:repo-statistics} reports the statistics of the cloned repositories.

\subsection{Metrics Overview}
We used SonarQube and CK to extract metrics from OSS repositories. For Java repositories, we generated over 100 metrics and selected 20 based on the ISO/IEC 25010 international standard \cite{klima2022selected}. For scripting language repositories, we only extracted 12 metrics. Table~\ref{tab:metric_definition} shows the 20 metrics with their corresponding ISO/IEC 25010 characteristics. 

\begin{table*}
\centering
\footnotesize
\renewcommand{\arraystretch}{1.1}
\begin{threeparttable}
\caption{Definition of 20 Metrics for Code Quality Evaluation}
\begin{tabular}{@{}p{2cm}p{4cm}p{8cm}@{}}
\toprule
\textbf{Dimension}                         & \textbf{Metric}                                                             & \textbf{Definition}                                                                                                                    \\ \midrule
\multirow{12}{*}{Maintainability} & Cyclomatic Complexity\tnote{a}                    & Number of independent paths through code.                                                                                     \\
                                  & File Complexity\tnote{b} & Cyclomatic complexity averaged by files.                                                                                      \\
                                  & Cognitive Complexity\tnote{a}                     & Combination of cyclomatic complexity and human assessment.                                                                    \\
                                  & Code Smells\tnote{a}                              & Number of code smell issues.                                                                                                  \\
                                  & Coupling Between Objects                                           & Number of classes that are coupled to a particular class.                                                                     \\
                                  & Fan-in                                                             & Number of input dependencies a class has.                                                                                     \\
                                  & Fan-out                                                            & Number of output dependencies a class has.                                                                                    \\
                                  & Depth Inheritance Tree                                             & Number of "fathers" a class has.                                                                                    \\
                                  & Number of Children                                                 & Number of immediate subclasses that a particular class has.                                                                   \\
                                  & Lack of Cohesion of Methods                                        & Degree to which class methods are coupled.                                                                                    \\
                                  & Tight Class Cohesion                                               & Ratio of the number of pairs of directly related methods in a class to the maximum number of possible methods in the class.   \\
                                  & Loose Class Cohesion                                               & Ratio of the number of directly or indirectly related method pairs in a class to the maximum number of possible method pairs. \\ \midrule
\multirow{3}{*}{Reliability}      & Total Violations\tnote{a}                         & Number of issues including all severity levels.                                      \\
                                  & Critical Violations\tnote{a}                      & Number of issues of the critical severity.                                                                                    \\
                                  & Info Violations\tnote{a}                          & Number of issues of the info severity.                                                                                        \\ \midrule
\multirow{5}{*}{Functionality}    & Line to Cover\tnote{a}                            & Lines to be covered by unit tests.                                                                                                  \\
                                  & Comment Lines\tnote{a}                            & Number of comment lines.                                                                                                      \\
                                  & Duplicated Blocks\tnote{c}                        & Number of duplicated blocks of line.                                                                                          \\
                                  & Duplicated Files\tnote{b}                         & Number of files involved in duplicated blocks.                                                                                \\
                                  & Duplicated Lines\tnote{a}                         & Number of lines involved in duplicated blocks.                                                                                \\ \bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item[a] Normalized by Non-comment Line of Codes.
\item[b] Normalized by Number of Functions.
\item[c] Normalized by Number of Statements.
\end{tablenotes}
\label{tab:metric_definition}
\end{threeparttable}
\end{table*}

We normalized metrics to ensure score fairness. Cyclomatic complexity, cognitive complexity, code smells, line to cover, and violations-related metrics are normalized by non-comment lines of code, duplicated lines are normalized by lines of code, and comment lines are normalized by the sum of non-comment lines and comment lines, to account for repository size. File complexity and duplicated files are normalized by the number of files, and duplicated blocks are normalized by the number of statements to adjust for differences across repositories. This normalization process results in a more unbiased score for the metrics across different OSS.

\subsection{Importance Weights}
We use standard machine-learning approaches to derive weights for different metric scores and calculate a repository's overall code quality score. Our model can explain OSS adoption (Github stars) using evaluated scores.

Figure~\ref{fig: Workflow} illustrates the entire process from data collection to final scores. We use custom data filters to ensure genuine engineering repositories are retained. We extract code quality metrics using a metric scanner and generate metric scores using the distribution-based method in Section~\ref{sec: method}, with each programming language having its distribution for each metric. We implement a Gradient Boosting Classifier (GBC) model with 0-1 labels as dependent variables based on the number of GitHub stars. We label the top and bottom quintiles (20\%) of the OSS repository stars as 1 and 0, respectively. The model generates importance values as weights for each metric. Finally, we obtain a weighted average code quality score according to Eq. \eqref{eq: overall_score}.

\begin{algorithm}
\footnotesize
  \SetAlgoLined
  \KwIn{Training dataset $\mathcal{D} = \{(\mathbf{m}_i, c_i)\}_{i=1}^N$, number of iterations $T$}
  \KwOut{Ensemble model $F(\mathbf{m})$}
  
  Initialize model $F_0(\mathbf{m}) = 0$\;
  
  \For{$t=1$ to $T$}{
    Compute the negative gradient: $r_{it} = -\frac{\partial L(c_i, F(\mathbf{m}_i))}{\partial F(\mathbf{m}_i)} \bigg|_{F(\mathbf{m}) = F_{t-1}(\mathbf{m})}$\;
    
    Fit a base learner $h_t(\mathbf{m})$ to the negative gradient: $h_t(\mathbf{m}) = \arg\min_h \sum_{i=1}^N L(c_i, F_{t-1}(\mathbf{m}_i) + h(\mathbf{m}_i))$\;
    
    Update the ensemble model: $F_t(\mathbf{m}) = F_{t-1}(\mathbf{m}) + \eta h_t(\mathbf{m})$, where $\eta$ is the learning rate\;
  }
  \caption{GBC in Our Context}
  \label{alg:gbc}
\end{algorithm}

The GBC algorithm is presented in Algorithm \ref{alg:gbc}, where each data point contains a metric score $\mathbf{m}_i$ and its corresponding classification $c_i$ according to its GitHub star. We divide the whole dataset into a training ($\mathcal{D}$) and a validation set by a ratio of 4:1. The GBC algorithm works with an ensemble model $F_0(\mathbf{m})$ and we fine-tune it by fitting base learners $h_t(\mathbf{m})$ to the loss function's negative gradient. The learning rate $\eta$ determines the base learners' contribution, resulting in the final ensemble model $F(\mathbf{m})$ providing the aggregate prediction.

\section{Results} \label{sec: result}

\subsection{Metric Distributions}

Table~\ref{tab:metric-parameters1} and Table~\ref{tab:metric-parameters2} present the fitted parameters for the asymmetric Gaussian [Eq. \eqref{eq:agass_pdf}] and Exponential [Eq. \eqref{eq:exp_pdf}] distributions, respectively. Java repositories have 8 more maintainability metrics describing cohesion and coupling in the codes, which are absent for other programming languages due to a lack of proper metric scanners. 

\begin{table*}
\scriptsize
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Parameters of the Fitted Asymmetric Gaussian Distributions ($\boldsymbol{\mu}$, $\boldsymbol{\sigma_1}$, $\boldsymbol{\sigma_2}$)}
\begin{tabular}{@{}ccccc@{}}
\toprule
{\textbf{Metric}} & Java$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & JavaScript$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & Python$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & TypeScript$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ \\ \midrule
Cyclomatic Complexity            & (155.228,50.947,40.902)                                                      & (166.692,88.415,78.289)                                                       & (162.321,53.497,52.789)                                                    & (127.273,51.616,66.733)                                                        \\

Cognitive Complexity            &(50.870,40.120,75.664)                                                                         &(33.238,32.586,121.541)                                                    &(170.042,33.546,0.000)                                                    & (29.619,22.964,81.617)                                                      \\

Comment Lines            &(15.841,11.451,137.269)                                                                        &(0.007,6.575,96.312)                                                        &(91.730,64.805,148.192)                                                    & (0.002,9.300,72.443)                                                      \\
Fan-in                                   & (1.101,0.463,1.217)                                & /                                                & /                                            & /                                                \\
Fan-out                          & (5.181,2.043,4.639)                                                      & /                                                                              & /                                                                          & /                                                                              \\
Loose Class Cohesion             & (0.329,0.149,0.176)                                                      & /                                                                              & /                                                                          & /                                                                              \\
Tight Class Cohesion             & (0.228,0.100,0.128)                                                      & /                                                                              & /                                                                          & /                                                                              \\
Coupling Between Objects         & (7.055,2.580,5.086)                                                      & /                                                                              & /                                                                          & /                                                                              \\ \bottomrule
\end{tabular}
\label{tab:metric-parameters1}
\end{table*}

\begin{table*}
\centering
\footnotesize
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.2}
\caption{Parameters of the Fitted Exponential Distributions ($\boldsymbol{c}$, $\boldsymbol{\lambda}$)}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Metric}                                   & Java$(\boldsymbol{c},\boldsymbol{\lambda})$ & JavaScript$(\boldsymbol{c},\boldsymbol{\lambda})$ & Python$(\boldsymbol{c},\boldsymbol{\lambda})$ & TypeScript$(\boldsymbol{c},\boldsymbol{\lambda})$ \\ \midrule
%Cyclomatic Complexity                    & /                                          & /                                                & /                                            & /                                                \\
File Complexity & (0,0.485)                                  & (0,0.884)                                       & (0,0.917)                                   & (0,0.492)                                       \\
%Cognitive Complexity                     & /                                 &/                                    & /                                 & /                                     \\
Code Smells                              & (1.123,50.731)                                 & (0.036,60.260)                                       & (0.004,37.177)                                   & (0.017,16.530)                                       \\

Depth Inheritance Tree                   & (1.003,0.502)                                  & /                                                & /                                            & /                                                \\
Number of Children                       & (0.002,0.137)                                  & /                                                & /                                            & /                                                \\
Lack of Cohesion of Methods              & (0.053,80.004)                                  & /                                                & /                                            & /                                                \\
Total Violations                         & (1.160,54.376)                                  & (0.054,63.313)                                       & (0.004,387.551177)                                   & (0.021,18.168)                                       \\
Critical Violations                      & (0.019,9.872)                                  & (0.020,48.811)                                       & (0.007,9.443)                                    & (0.005,5.497)                                        \\
Info Violations                          & (0.019,1.934)                                  & (0.001,1.436)                                        & (0.002,1.401)                                    & (0.003,1.535)                                        \\
Line to Cover                            & (0,0.000)                                  & (0,0.000)                                        & (0,0.000)                                    & (0,0.000)                                        \\
Duplicated Blocks                        & (0,0.015)                                  & (0.001,0.021)                                      & (0,0.010)                                    & (0,0.021)                                       \\
Duplicated Files                         & (0.003,0.135)                                  & (0.001,0.203)                                        & (0,0.222)                                    & (0,0.116)                                        \\
Duplicated Lines                         & (0.439,63.284)                                  & (0.145,163.258)                                      & (0.081,124.342)                                  & (0.085, 102.796)                                      \\ \bottomrule
\end{tabular}
\label{tab:metric-parameters2}
\end{table*}


Monotonic metrics, such as 'Code Smells', exhibit an exponential distribution pattern, as represented in Fig. \ref{fig:metric_dist} and Table \ref{tab:metric-parameters2}. This distribution aligns with our understanding that superior code quality is associated with fewer bugs, verifying the effectiveness of our method. Furthermore, the threshold parameter $c$ reflects the tolerance value for full scores. In the probability density function (Eq. \eqref{eq:exp_pdf}) except 'Code Smells', 'Depth Inheritance Tree', and 'Total Violations' where $c$ approximates 1. The fitted exponential decay parameter, $\lambda$, reflects the sensitivity of metrics to scores.  Particularly, a $\lambda\lesssim1$ is observed for metrics such as 'File Complexity', 'Depth Inheritance Tree', 'Number of Children', 'Duplicated Blocks', and 'Duplicated Files', which implies a low sensitivity to metric variations of the order of 1. Conversely, the $\lambda$ value for total violation is high, which reflects the high sensitivity of the number of violations.

% Info violation has the lowest $\theta$ as info violation shows the preliminary errors for which managers have the lowest tolerance. 

Non-monotonic metrics, such as the 'Cyclomatic Complexity', follow an asymmetric Gaussian distribution. According to Eq. \eqref{eq:agass_score}, repositories with metric values close to the Gaussian center get higher scores since they fall into the range where high-quality OSS are mostly located. In Table \ref{tab:metric-parameters1}, the Gaussian centers $\mu$ are large ($\gg1$) for the metrics of 'Cyclomatic Complexity', 'Cognitive Complexity', and 'Comment Lines' in most cases except for the 'Comment Lines' of the Javascript and Typescript languages. The latter two distributions are almost monotonic ($\mu=0$), potentially because these two languages are generally easy to understand and do not require additional command lines. The fitted widths $\sigma_{1,2}$ are large and have asymmetric sensitivity; i.e. relatively long tails are observed on the right of the asymmetric Gaussian distributions. For "command line" in Python, increasing command lines before the center point has high sensitivity, while it becomes less sensitive after the center point.



\subsection{Importance Weights}

Table~\ref{tab:importance_values} shows the feature importance values from the GBC model in Section \ref{sec: application}, which we use as metric weights in Eq. \eqref{eq: overall_score} within the three dimensions: maintainability, reliability, and functionality. The relative importance values are listed in Table~\ref{tab:importance_values}. We normalized the importance values for each dimension to get relative weights within dimensions. 

\begin{table*}
\footnotesize
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Importance Values for Metric Scores}
\begin{tabular}{@{}p{2cm}p{4cm}cccc@{}}
\toprule
\multirow{2}{2cm}{\textbf{Dimension}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{4}{c}{\textbf{Importance}}                                    \\
  & \multicolumn{1}{c}{}                        & \textbf{Java}           & \textbf{JavaScript}     & \textbf{Python}     & \textbf{TypeScript} \\ \midrule
\multirow{12}{*}{Maintainability}             & Cyclomatic Complexity                       & 0.110 (0.083) & 0.190 (0.082) & 0.250 (0.120) & 0.223 (0.081) \\
  & File Complexity    & 0.220 (0.165) & 0.396 (0.171) & 0.449 (0.215) & 0.402 (0.146) \\
  & Cognitive Complexity                        & 0.086 (0.065) & 0.289 (0.125) & 0.119 (0.057) & 0.215 (0.078) \\
  & Code Smells                                 & 0.066 (0.049) & 0.125 (0.054) & 0.182 (0.087) & 0.160 (0.058) \\
  & Coupling Between Objects                    & 0.096 (0.072) & /              & /              & /              \\
  & Fan-in                                      & 0.108 (0.081) & /              & /              & /              \\
  & Fan-out                                     & 0.057 (0.043) & /              & /              & /              \\
  & Depth Inheritance Tree                      & 0.075 (0.057) & /              & /              & /              \\
  & Number of Children                          & 0.026 (0.020) & /              & /              & /              \\
  & Lack of Cohesion of Methods                 & 0.078 (0.058) & /              & /              & /              \\
  & Tight Class Cohesion                        & 0.010 (0.008) & /              & /              & /              \\
  & Loose Class Cohesion                        & 0.068 (0.051) & /              & /              & /              \\
  & \textbf{Sum}   & 1 (0.752) & 1 (0.432) & 1 (0.479) & 1 (0.363) \\
\midrule
\multirow{3}{*}{Reliability}                  & Total Violations                            & 0.474 (0.056) & 0.288 (0.070) & 0.293 (0.068) & 0.228 (0.065) \\
  & Critical Violations                         & 0.272 (0.032) & 0.420 (0.102) & 0.410 (0.095) & 0.414(0.118) \\
  & Info Violations                             & 0.254 (0.030) & 0.292 (0.071) & 0.297 (0.069) & 0.358 (0.102) \\
  & \textbf{Sum}   & 1 (0.118) & 1 (0.243) & 1 (0.232) & 1 (0.285) \\
\midrule
\multirow{5}{*}{Functionality}                & Line to Cover                               & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
  & Comment Lines                               & 0.454 (0.059) & 0.317 (0.103) & 0.370 (0.107) & 0.318 (0.112) \\
  & Duplicated Blocks                           & 0.162 (0.021) & 0.286 (0.093)   & 0.197 (0.057) & 0.148 (0.052) \\
  & Duplicated Files                            & 0.190 (0.025) & 0.120 (0.039) & 0.166 (0.048) & 0.179 (0.063) \\
  & Duplicated Lines                            & 0.194 (0.025) & 0.277 (0.090) & 0.267 (0.077) & 0.355 (0.125) \\ 
  & \textbf{Sum}   & 1 (0.130) & 1 (0.325) & 1 (0.289) & 1 (0.352) \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
    \item  The parenthesis values are original importance values, while the values outside parenthesis are normalized in the dimension level. 
\end{tablenotes}
\label{tab:importance_values}
\end{table*}

In the maintainability dimension, 'File Complexity' has the largest weight across four programming languages, followed by 'Cognitive Complexity' 'Cyclomatic Complexity', and 'Code Smells'. These metrics contribute more to the maintainability scores. For Java repositories, all the coupling and cohesion metrics show similar contributions $\lesssim0.1$, reflecting their weak contribution to OSS adoption.

In the reliability dimension, 'Total Violations' contributes mostly to Java, while 'Critical Violations' contributes mostly to the other three languages, which suggests varying priorities of solving violations for different languages. 

In the functionality dimension, the 'Comment Lines' metric contributes more to Java, potentially because Java is less intuitive to understand, which requires code comments for better understanding. The 'Comment Lines' metric also contributes significantly to the other three scripting languages. We note that zero 'Line to Cover' metric values were obtained in our raw data, either caused by problems in obtaining this metric or because codes in OSS repositories are rarely tested. This gap can be closed when applying our methodology in specific companies where values of 'Line to Cover' are obtained for their close-source repositories.

\subsection{Scores}

% Figure environment removed

After obtaining metric distributions, we score those metrics of each OSS repository based on their respective distribution locations.

We present the overall scores of included OSS repositories in Fig.~\ref{fig:scores_all} and assess the explanatory power of our metric scores on the OSS stars using Table~\ref{Explainability}. We observe that Java code metric scores show higher explanatory power for the OSS repository's stars compared to the other languages, which suggests that code quality can better determine the success of Java-based OSS repositories in terms of stars received, which may be attributed to the greater availability of metrics for Java or the nature of repositories developed using Java for large-scale platforms and systems.

In contrast, JavaScript, Python, and TypeScript exhibit relatively lower explanatory power of metric scores, indicating their code quality might be less critical in determining their OSS adoption, possibly because of their primary use in data analytics or other domains where their adoption is less influenced by code quality.

\begin{table}
\footnotesize
\centering
\caption{Metric Scores Explanatory Power \label{Explainability}}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Language} & \textbf{Java} & \textbf{JavaScript} & \textbf{Python} & \textbf{TypeScript} \\ \midrule
Accuracy          & 0.947         & 0.826               & 0.808           & 0.817               \\
Precision         & 0.971         & 0.838               & 0.831           & 0.834              \\
Recall            & 0.917         & 0.803               & 0.771           & 0.784               \\
F1                & 0.943         & 0.820               & 0.800           & 0.808               \\
AUC\_ROC          & 0.946         & 0.826               & 0.815           & 0.817               \\
R2                & 0.787    & 0.274 	& 0.186 	& 0.247 \\ 
\bottomrule
\end{tabular}
\end{table}


\section{Conclusion} \label{sec: dis}

Our research focuses on code quality with three dimensions: maintainability, reliability, and functionality. We evaluate metrics based on their distributions. Our study advances the understanding of code quality and contributes to better quality control standards and practices, ultimately supporting the success and sustainability of software. 

Although our study provides valuable insights, it has some limitations that need to be acknowledged. We have not yet systematically validated the effectiveness of the method. Moving forward, it would be beneficial to incorporate validation techniques, such as sensitivity tests, to ensure the accuracy and reliability of the distribution fitting. Additionally, the parameters of the fitted distribution are sensitive to data distribution, making it necessary to incorporate more data for determining them. 

\section*{Acknowledgment}
Y. Xia is partly supported by the "Pioneering Innovator" award from the Guangzhou Tianhe District government. Z. Li is partly supported by the Guangdong Basic and Applied Basic Research Foundation (2021A1515012039). We would like to acknowledge useful discussions and support from Mianmian Zhang and other colleagues at the HSBC Lab.

\singlespacing

\bibliography{main}

\end{document}