\documentclass[12pt]{article}

\usepackage{arxiv}
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{amssymb,amsmath,amsfonts,eurosym, geometry,ulem,graphicx, caption, color,setspace, comment, footmisc,caption, pdflscape, subfigure, hyperref,array, algpseudocode}

\usepackage[ruled]{algorithm2e}
\usepackage[numbers]{natbib}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{caption}
\usepackage{amsthm}
\usepackage{lipsum}
\usepackage{etoolbox}
\usepackage{enumitem}
\usepackage{float}
\usepackage{physics}
\allowdisplaybreaks[3]
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}

\renewcommand{\shorttitle}{A Quantitative Analysis of OSS Quality: Insights from Metric Distributions}

% \geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}

\title{\textbf{\Large A Quantitative Analysis of Open Source Software Code Quality: Insights from Metric Distributions \\}}

\author{Siyuan Jin$^{1,2}$, Mianmian Zhang$^{1}$, Yekai Guo$^{3}$, Yuejiang He$^{1}$, \\
\textbf{Ziyuan Li$^{4,1,*}$, Bichao Chen$^{1,*}$, Bing Zhu$^{1,*}$, and Yong Xia$^{1,*}$}\\
	\normalsize $^{1}$ HSBC Laboratory, Guangzhou, China\\
	\normalsize $^{2}$ School of Business and Management, Hong Kong University of Science and Technology, Hong Kong, China\\
	\normalsize $^{3}$ School of Data Science, Fudan University, Shanghai, China\\
 	\normalsize $^{4}$ School of Physics, Sun Yat-sen University, Guangzhou, China\\
	\normalsize liziyuan3@mail.sysu.edu.cn, bichao.chen@hsbc.com, bing1.zhu@hsbc.com, yong.xia@hsbc.com\\
	\normalsize *corresponding author
}

\newcolumntype{C}{>{\centering\arraybackslash}X}
\bibliographystyle{unsrt}

\begin{document}
\maketitle
\begin{abstract}
Code quality is a crucial construct in open-source software (OSS) with three dimensions: maintainability, reliability, and functionality. To accurately measure them, we divide 20 distinct metrics into two types: 1) threshold-type metrics that influence code quality in a monotonic manner; 2) non-threshold-type metrics that lack a monotonic relationship to evaluate. We propose a distribution-based method to provide scores for metrics, which demonstrates great explainability on OSS adoption. Our empirical analysis includes more than 36,460 OSS projects and their raw metrics from SonarQube\footnote{https://www.sonarsource.com} and CK\footnote{https://github.com/mauricioaniche/ck}. Our work contributes to the understanding of the multi-dimensional construct of code quality and its metric measurements.
\end{abstract}
\vspace{1.5ex}
\keywords{Open Source Software, Code Quality, Construct Dimensions, Adoption}
\section{Introduction}
Code quality positively influences user acceptance and adoption rates of software \cite{crowston2003defining,alenezi2021internal, silva2023factors}. Figure \ref{fig: Construct} shows that code quality is a multi-dimensional construct that includes dimensions \cite{edwards2001multidimensional}: maintainability, reliability, and functionality. Based on the literature and common standards, We identify 20 distinct metrics to measure these dimensions. Despite the literature that has discussed these metrics, a gap remains in the methodologies for evaluating them.  We propose a distribution-based methodology to provide scores for each metric, which shows great explainability on OSS adoption.

\textbf{RQ.~1: How to evaluate code quality metrics?}
We divide metrics into two types: threshold-type and non-threshold-type. threshold-type metrics have a monotonic relationship with overall code quality, while non-threshold-type metrics lack a monotonic relationship to evaluate (see Figure \ref{fig:metric_dist}). We evaluate their scores by analyzing their probability distributions among high-star OSS projects. For threshold-type metrics, we fit an exponential distribution and use the location of the data in their cumulative distribution functions (CDFs) as their scores. For non-threshold-type metrics, we fit an asymmetric Gaussian distribution and use the location of the data away from the central point in their CDFs as their scores.

We evaluate code quality metrics of high-star projects from 36,460 GitHub OSS using SonarQube and CK packages. We choose high-star projects to make our evaluation more critical. High-star projects generally have lower bugs, resulting in sharper distributions and additional bugs will make scores decrease dramatically. Our evaluation method provides scores within the range of $0\sim100$ for each metric.

\textbf{RQ.~2: How to integrate the evaluated metric scores for explaining an OSS project's star?}
We investigate the significance of individual code quality scores for certain applications. Our study focuses on the explainability of our code quality metric scores on the project's GitHub stars. We use a machine learning approach to determine the correlation between our metric scores and GitHub stars, using R2 and accuracy measures to assess the explanatory power of our metric evaluated scores. Our methodology can be applied to different target variables, providing a flexible strategy for evaluating code quality in various contexts.

% Figure environment removed

The key contributions of our paper include:
\begin{itemize}
\item Distribution-based evaluation: Based on the literature on code quality \cite{alenezi2021internal,mccabe1976complexity, stamelos_code_2002, shin2010evaluating, bianchi_organizational_2012}, we identify and evaluate the dimensions of code quality, thus refining our understanding of the construct. 
\item Construction of an explanatory model for GitHub stars: In contrast to the studies \cite{silva2023factors, altuwaijri2022factors}, we develop a model to evaluate code quality metric scores for over 36,460 GitHub OSS projects. We then use their scores to explain the GitHub stars \cite{lee2009measuring, zerouali2019diversity,zhao2021evaluation}, generating insights into how code quality may influence the widespread adoption of a project.
\end{itemize}
\section{Literature Review} \label{sec: lr} 
OSS collects efforts to enhance code quality \cite{ljungberg2000open, von2006promise,heinemann2011extent} and innovation \cite{levine2014open, vitharana2010impact}. Developers have been extensively reusing OSS code to lower their search cost \cite{haefliger2008code}, which makes code quality important for better reuse.

Code quality has dimensions \cite{polites2012conceptualizing}. Hines \cite{hines1970software} explores the concept of software quality but does not provide a clear definition of code quality. The IEEE standard defines code quality as the collective features and characteristics of software that meet given needs \cite{fitzpatrick1996software}. However, user-friendliness and useful functionalities are included in the definition of code quality \cite{lee2009measuring}, echoing the three dimensions in the ISO/IEC 25010 standard \cite{klima2022selected}: maintainability, reliability, and functionality \cite{athanasiou2014test}. We define the construct and dimensions in Table \ref{tb: construct}.

\begin{table*}[]
\footnotesize
\centering
\caption{Construct Definition \label{tb: construct}}
\begin{tabular}{@{}p{1.5cm}p{4cm}p{2.2cm}p{6.5cm}@{}}
\toprule
\textbf{Construct} & \textbf{Definition} & \textbf{Dimensions} & \textbf{Definition} \\ 
\midrule
\multirow{3}{5cm}{Code Quality} & \multirow{3}{4.5cm}{How well-written the code is, including maintainability, reliability, and functionality. \cite{lee2009measuring}} & Maintainability & The code is easy to understand, enhance, or correct. \cite{deligiannis2003empirical} \\
& & Reliability & The code is user-friendly and stable. \cite{lee2009measuring} \\
& & Functionality & The code has useful functions. \cite{lee2009measuring} \\ 
\bottomrule
\end{tabular}
\end{table*}

Code quality has quantitative measurements. Alenezi et al. \cite{alenezi2021internal} discuss quality attributes like maintainability \cite{motogna2023empirical}, readability \cite{gonzalez2023reliability}, and functionality \cite{shen2020api}. Others have used metrics for code quality measurement \cite{stamelos_code_2002, bianchi_organizational_2012} and code complexity \cite{mccabe1976complexity,shin2010evaluating, hassan_predicting_2009}. However, most existing metric identifications have focused on threshold-type areas rather than non-threshold-type areas. Our paper considers both and proposes a uniform solution for them.

OSS code quality has reflective measures that have monotonic relationships with code quality. Silva et al. \cite{silva2023factors} identify a significant correlation between OSS adoption and quality. Saini et al. use committed rate, representing contribution activity, which may reflect code quality \cite{saini2016fuzzy}. Although adoption and OSS activities are determined by many factors, such as commitment \cite{maruping2019developer}, transparency \cite{shaikh2016folding}, and leader resources \cite{dong2021project}, OSS adoption can partially reflect OSS code quality. Lee et al. \cite{lee2009measuring} identify five determinants for OSS success, highlighting the impact of code quality on user satisfaction and adoption. Other studies \cite{zerouali2019diversity, zhao2021evaluation} reaffirm this association. Therefore, we use GitHub stars as a reflective measure of OSS code quality. 

\section{Data} \label{sec: data}

\subsection{Data Sources}
\begin{table*}
\renewcommand{\arraystretch}{1.1}
\footnotesize
\centering
\caption{Statistical Summary of OSS Projects}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Language} & \textbf{Number of Projects} & \textbf{Max Number of Stars} & \textbf{Min Number of Stars} & \textbf{Number of Filtered Projects} \\ \midrule
Java                 & 21,695                       & 50k                     & 100                     & 1,645                           \\
Python               & 20,280                       & 228k                    & 260                     & 16,096                          \\
JavaScript           & 20,596                       & 107k                    & 270                     & 7,722                           \\
TypeScript           & 20,449                       & 202k                    & 60                      & 10,997                          \\ \bottomrule
\end{tabular}
\label{tab:repo-statistics}
\end{table*}

GitHub is the largest OSS management platform that has more than 39 million public repositories (As of June 2023). To save time and storage, we select a subset of repositories with Java, Python, JavaScript, and TypeScript as the main programming languages and sorted them by the number of stars. We collect code from the top $\sim20,000$ projects for each programming language. The number of GitHub stars is a measure of a project's adoption \cite{jarczyk2018surgical}. We remove non-engineering projects by pattern matching, such as a guide for Java interviews in \href{https://github.com/Snailclimb/JavaGuide}{JavaGuide}. Table ~\ref{tab:repo-statistics} reports the statistics of the cloned projects before the filter. 

We use code scanners to obtain metrics. Scripting language projects (Python, Javascript, TypeScript) can be directly imported, while non-scripting Java projects need to be compiled first. Compiling Java projects is challenging due to their different JDK, maven, or Gradle versions. Therefore, we only chose projects with GitHub releases for compilation, which leads to 36,460 projects and over 600 million lines of code.

\subsection{Metrics Overview}
We use SonarQube and CK to extract metrics from OSS projects. For Java projects, we generate over 100 metrics and select 20 based on the ISO/IEC 25010 international standard \cite{klima2022selected}, which defines and classifies the OSS code metrics. For scripting language projects, we only extract 12 metrics. Table~\ref{tab:metric_definition} shows the 20 metrics with their corresponding ISO/IEC 25010 characteristics. 

\begin{table*}
\centering
\footnotesize
\renewcommand{\arraystretch}{1.1}
\begin{threeparttable}
\caption{Definition of 20 Metrics for Code Quality Evaluation}
\begin{tabular}{@{}p{2cm}p{5cm}p{7.5cm}@{}}
\toprule
\textbf{Dimension}                         & \textbf{Metric}                                                             & \textbf{Definition}                                                                                                                    \\ \midrule
\multirow{12}{*}{Maintainability} & Cyclomatic Complexity\tnote{a}                    & Number of independent paths through code.                                                                                     \\
                                  & File Complexity\tnote{b} & Cyclomatic complexity averaged by files.                                                                                      \\
                                  & Cognitive Complexity\tnote{a}                     & Combination of cyclomatic complexity and human assessment.                                                                    \\
                                  & Code Smells\tnote{a}                              & Number of code smell issues.                                                                                                  \\
                                  & Coupling Between Objects                                           & Number of classes that are coupled to a particular class.                                                                     \\
                                  & Fan-in                                                             & Number of input dependencies a class has.                                                                                     \\
                                  & Fan-out                                                            & Number of output dependencies a class has.                                                                                    \\
                                  & Depth Inheritance Tree                                             & Number of output dependencies a class has.                                                                                    \\
                                  & Number of Children                                                 & Number of immediate subclasses that a particular class has.                                                                   \\
                                  & Lack of Cohesion of Methods                                        & Degree to which class methods are coupled.                                                                                    \\
                                  & Tight Class Cohesion                                               & Ratio of the number of pairs of directly related methods in a class to the maximum number of possible methods in the class.   \\
                                  & Loose Class Cohesion                                               & Ratio of the number of directly or indirectly related method pairs in a class to the maximum number of possible method pairs. \\ \midrule
\multirow{3}{*}{Reliability}      & Total Violations\tnote{a}                         & Number of issues including all severity levels (blocker, critical, major, minor, info).                                       \\
                                  & Critical Violations\tnote{a}                      & Number of issues of the critical severity.                                                                                    \\
                                  & Info Violations\tnote{a}                          & Number of issues of the info severity.                                                                                        \\ \midrule
\multirow{5}{*}{Functionality}    & Line to Cover\tnote{a}                            & Lines to be covered by unit tests.                                                                                                  \\
                                  & Comment Lines\tnote{a}                            & Number of comment lines.                                                                                                      \\
                                  & Duplicated Blocks\tnote{c}                        & Number of duplicated blocks of line.                                                                                          \\
                                  & Duplicated Files\tnote{b}                         & Number of files involved in duplicated blocks.                                                                                \\
                                  & Duplicated Lines\tnote{a}                         & Number of lines involved in duplicated blocks.                                                                                \\ \bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item[a] Normalized by Non-comment Line of Codes.
\item[b] Normalized by Number of Functions.
\item[c] Normalized by Number of Statements.
\end{tablenotes}
\label{tab:metric_definition}
\end{threeparttable}
\end{table*}

We normalize metrics to ensure score fairness. Cyclomatic complexity, cognitive complexity, code smells, line to cover and violations-related metrics are normalized by non-comment lines of code, duplicated lines are normalized by lines of code, and comment lines are normalized by the sum of non-comment lines and comment lines, to account for project size. File complexity and duplicated files are normalized by the number of files, and duplicated blocks are normalized by the number of statements to adjust for differences across projects. This normalization process results in a more accurate and unbiased score for the metrics across different OSS projects.

\section{Methodologies} \label{sec: method}
% To effectively evaluate code quality metrics, we provide a distribution-based metric evaluation method on high-star GitHub projects.

To score the obtained code quality metrics for quantitatively understanding these values, we utilize a statistical method. In general, we first map out the distribution of each metric in high-star OSS projects and then score each project according to the corresponding CDF of its metric value.

% Figure environment removed

Two different types of distributions are observed for those metrics in Table \ref{tab:metric_definition}: threshold-type and non-threshold-type metrics. For the threshold-type ones, we observe a monotonic distribution, as seen on the right side of Fig. \ref{fig:metric_dist}. We fit an exponential distribution to the threshold-type-metric data
\begin{equation}\label{eq:exp_pdf}
f_1(x;c,\lambda) = \begin{cases}
0 & \text{if } x \leq c  \\
\lambda \exp\left[-\lambda {(x-c)}\right] & \text{if } x > c 
\end{cases} \
\end{equation}
where $\lambda$ and $c$ are the fitting parameters. The corresponding score function based on the CDF of Eq. \eqref{eq:exp_pdf} reads as
\begin{equation}\label{eq:exp_score}
\begin{split}
& M_1(x; c, \lambda) =   \\
& 100 \times \begin{cases}
1 & \text{if } x \leq c \\
\exp\left[-\lambda {(x-c)}\right]  & \text{if } x > c
\end{cases} \
\end{split}
\end{equation}
% \begin{equation}\label{eq:4}
% \begin{split}
% & M_1(x, \theta, c) =   \\
% & 100 \times \begin{cases}
% 1 & \text{if } c \leq 1e-6, \\
% 1 -  \lambda \exp\left[-\lambda {(x-c)}\right]  & \text{if } c > 1e-6.
% \end{cases}
% \end{split}
% \end{equation}
The score falls into the range of $0\sim100$ and it peaks at $c$ and decays exponentially for $x>c$.

The non-threshold-type metrics follow an asymmetric Gaussian distribution (see examples on the left of Fig. \ref{fig:metric_dist}), the PDF of which reads as
\begin{equation}\label{eq:agass_pdf}
\begin{split}
& f_2(x;\mu, \sigma_1, \sigma_2) = \\
&
% \begin{cases}
% y_0+\frac{2}{\sqrt{2\pi}}\frac{c}{{\sigma_1} + {\sigma_2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma_1^2}\right) & \text{if } x \geq \mu, \\
% \frac{2}{\sqrt{2\pi}}\frac{y_0+c}{{\sigma_1} + {\sigma_2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma_2^2}\right) & \text{if } 0\leq x < \mu.
% \end{cases}
\begin{cases}
\frac{1}{\sqrt{2\pi}}\frac{2}{{\sigma_1} + {\sigma_2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma_1^2}\right) & \text{if } 0\leq x < \mu \\
\frac{1}{\sqrt{2\pi}}\frac{2}{{\sigma_1} + {\sigma_2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma_2^2}\right) & \text{if }  x \geq \mu 
\end{cases} \
\end{split} 
\end{equation}
where $\mu, c, \sigma_1,\sigma_2$ are fitting parameters representing the peak position, peak height on the right, and peak widths on each side, respectively. The corresponding score function is
\begin{equation}\label{eq:agass_score}
\begin{split}
&M_2(x, \mu, \sigma_1, \sigma_2) = \\
& 100 \times
\begin{cases}
1 - \operatorname{erf}\left(\frac{x-\mu}{\sigma_1 \sqrt{2}}\right) & \text{if }  0\leq x < \mu\\
1 - \operatorname{erf}\left(\frac{x-\mu}{\sigma_2 \sqrt{2}}\right) & \text{if } x \geq \mu
\end{cases}\
\end{split} 
\end{equation}
where the score falls into the range of $0\sim100$, peaks at $\mu$, and decays according to the Z-score of the Gaussian function with the corresponding width on each side.


% \begin{small}
% \begin{equation}\label{eq:1}
% f_1(x;c,\theta) = \lambda \exp\left[-\lambda {(x-c)}\right]
% \end{equation}

% \begin{equation}\label{eq:2}
% \begin{split}
% & f_2(x;\mu, y_0, c, \sigma_1,\sigma_2) = \\
% &
% % \begin{cases}
% % y_0+\frac{2}{\sqrt{2\pi}}\frac{c}{{\sigma_1} + {\sigma_2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma_1^2}\right) & \text{if } x \geq \mu, \\
% % \frac{2}{\sqrt{2\pi}}\frac{y_0+c}{{\sigma_1} + {\sigma_2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma_2^2}\right) & \text{if } 0\leq x < \mu.
% % \end{cases}
% \begin{cases}
% y_0+c\exp\left(-\frac{(x-\mu)^2}{2\sigma_1^2}\right) & \text{if } x \geq \mu, \\
% (y_0+c)\exp\left(-\frac{(x-\mu)^2}{2\sigma_2^2}\right) & \text{if } 0\leq x < \mu.
% \end{cases}
% \end{split}
% \end{equation}

% \begin{equation}\label{eq:4}
% \begin{split}
% & M(x, \theta, c) =   \\
% & 100 \times \begin{cases}
% 1 & \text{if } c \leq 1e-6, \\
% 1 -  \lambda \exp\left[-\lambda {(x-c)}\right]  & \text{if } c > 1e-6.
% \end{cases}
% \end{split}
% \end{equation}

% \begin{equation}\label{eq:5}
% \begin{split}
% &M(x, \mu, \sigma_1, \sigma_2) = \\
% & 200 \times
% \begin{cases}
% 1 - \frac{1}{2}\left[1+\operatorname{erf}\left(\frac{x-\mu}{\sigma_1 \sqrt{2}}\right)\right] & \text{if } x \geq \mu, \\
% 1 - \frac{1}{2}\left[1+\operatorname{erf}\left(\frac{x-\mu}{\sigma_2 \sqrt{2}}\right)\right] & \text{if } x < \mu.
% \end{cases}
% \end{split}
% \end{equation}
% \end{small}
% where $c$ and $\theta$ are the parameters of the Exponential distribution, $\mu$, $\sigma_1$, and $\sigma_2$ are the parameters of the asymmetric Gaussian distribution. $C$ denotes a normalization factor, and $I$ is an indicator function.

% For threshold-type metrics, like code smells, we use exponential distribution to fit them. For non-threshold-type metrics, like code command lines, we use asymmetry Gaussian distribution to fit them.

% We select high-star projects as a reflective measure of code quality. The resulting distribution corresponds to the probability of high-adopted projects.

To obtain a unified score from the 20 metric scores in Table \ref{tab:metric_definition}, we assign weights to each of them, so the overall code quality score for a given project, denoted by $k$, can be computed as follows:
\begin{equation} \label{eq: overall_score}
\small
Q^{overall}_{k} = \sum_{i}{{\omega_{i}} \cdot Q^{metric}_{i,k}} \, ,
\text{subject to:} \sum_i{\omega_i} = 1 .
\end{equation}
We assign weights $\omega_i$ by calculating the importance of each metric score to a target variable such as project stars, commit frequencies, and project folk numbers, or their combination. In the following, we describe an application scenario where the target variable is the number of GitHub stars, as depicted in Figure \ref{fig: Workflow}.

% Figure environment removed

\section{Application} \label{sec: application}
We use machine learning techniques to derive weights for different metric scores and calculate a project's overall code quality score. Our model aims to explain OSS adoption using evaluated scores, with GitHub stars as a measure of adoption. 

Figure~\ref{fig: Workflow} illustrates the entire process from data collection to final scores. As described in Section~\ref{sec: data}, we use custom data filters to ensure genuine engineering projects are retained.  We extract code quality metrics using a metric scanner and generate metric scores using the distribution-based method in Section~\ref{sec: method}, with each programming language having its distribution for each metric. We implement a Gradient Boosting Classifier (GBC) model with 0-1 labels as dependent variables based on the number of GitHub stars. We label the top and bottom quintiles (20\%) of the OSS project stars as 1 and 0, respectively. The model generates importance values as weights for each metric. Finally, we obtain a weighted average code quality score according to Eq. \eqref{eq: overall_score}.

\begin{algorithm}
\small
  \SetAlgoLined
  \KwIn{Training dataset $\mathcal{D} = \{(\mathbf{m}_i, c_i)\}_{i=1}^N$, number of iterations $T$}
  \KwOut{Ensemble model $F(\mathbf{m})$}
  
  Initialize model $F_0(\mathbf{m}) = 0$\;
  
  \For{$t=1$ to $T$}{
    Compute the negative gradient: $r_{it} = -\frac{\partial L(c_i, F(\mathbf{m}_i))}{\partial F(\mathbf{m}_i)} \bigg|_{F(\mathbf{m}) = F_{t-1}(\mathbf{m})}$\;
    
    Fit a base learner $h_t(\mathbf{m})$ to the negative gradient: $h_t(\mathbf{m}) = \arg\min_h \sum_{i=1}^N L(c_i, F_{t-1}(\mathbf{m}_i) + h(\mathbf{m}_i))$\;
    
    Update the ensemble model: $F_t(\mathbf{m}) = F_{t-1}(\mathbf{m}) + \eta h_t(\mathbf{m})$, where $\eta$ is the learning rate\;
  }
  \caption{GBC in Our Context}
  \label{alg:gbc}
\end{algorithm}

The GBC algorithm is presented in Algorithm \ref{alg:gbc}, where each data point contains a metric score $\mathbf{m}_i$ and its corresponding classification $c_i$ according to its GitHub star. We divide the whole dataset into a training ($\mathcal{D}$) and a validation set by a ratio of 4:1. The GBC algorithm works with an ensemble model $F_0(\mathbf{m})$ and we fine-tune it by fitting base learners $h_t(\mathbf{m})$ to the loss function's negative gradient. The learning rate $\eta$ determines the base learners' contribution, resulting in the final ensemble model $F(\mathbf{m})$ providing the aggregate prediction.

\section{Results} \label{sec: result}

\subsection{Metric Distributions}

Table~\ref{tab:metric-parameters1} and Table~\ref{tab:metric-parameters2} present the fitted parameters for the asymmetric Gaussian [Eq. \eqref{eq:agass_pdf}] and Exponential [Eq. \eqref{eq:exp_pdf}] distributions, respectively. Java projects have 8 more maintainability metrics describing cohesion and coupling in the codes, which are absent for other programming languages due to a lack of proper metric scanners. After obtaining appropriate probability distributions of these metrics by fitting, we score the corresponding metrics of each OSS project based on their respective distributions.


\begin{table*}
\scriptsize
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Parameters of the Fitted Asymmetric Gaussian Distributions ($\boldsymbol{\mu}$, $\boldsymbol{\sigma_1}$, $\boldsymbol{\sigma_2}$)}
\begin{tabular}{@{}ccccc@{}}
\toprule
{\textbf{Metric}} & Java$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & JavaScript$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & Python$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & TypeScript$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ \\ \midrule
Cyclomatic Complexity            & (155.228,50.947,40.902)                                                      & (166.692,88.415,78.289)                                                       & (162.321,53.497,52.789)                                                    & (127.273,51.616,66.733)                                                        \\

Cognitive Complexity            &(50.870,40.120,75.664)                                                                         &(33.238,32.586,121.541)                                                    &(170.042,33.546,0.000)                                                    & (29.619,22.964,81.617)                                                      \\

Comment Lines            &(15.841,11.451,137.269)                                                                        &(0.007,6.575,96.312)                                                        &(91.730,64.805,148.192)                                                    & (0.002,9.300,72.443)                                                      \\
Fan-in                                   & (1.101,0.463,1.217)                                & /                                                & /                                            & /                                                \\
Fan-out                          & (5.181,2.043,4.639)                                                      & /                                                                              & /                                                                          & /                                                                              \\
Loose Class Cohesion             & (0.329,0.149,0.176)                                                      & /                                                                              & /                                                                          & /                                                                              \\
Tight Class Cohesion             & (0.228,0.100,0.128)                                                      & /                                                                              & /                                                                          & /                                                                              \\
Coupling Between Objects         & (7.055,2.580,5.086)                                                      & /                                                                              & /                                                                          & /                                                                              \\ \bottomrule
\end{tabular}
\label{tab:metric-parameters1}
\end{table*}

\begin{table*}
\centering
\footnotesize
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.2}
\caption{Parameters of the Fitted Exponential Distributions ($\boldsymbol{c}$, $\boldsymbol{\lambda}$)}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Metric}                                   & Java$(\boldsymbol{c},\boldsymbol{\lambda})$ & JavaScript$(\boldsymbol{c},\boldsymbol{\lambda})$ & Python$(\boldsymbol{c},\boldsymbol{\lambda})$ & TypeScript$(\boldsymbol{c},\boldsymbol{\lambda})$ \\ \midrule
%Cyclomatic Complexity                    & /                                          & /                                                & /                                            & /                                                \\
File Complexity & (0,0.485)                                  & (0,0.884)                                       & (0,0.917)                                   & (0,0.492)                                       \\
%Cognitive Complexity                     & /                                 &/                                    & /                                 & /                                     \\
Code Smells                              & (1.123,50.731)                                 & (0.036,60.260)                                       & (0.004,37.177)                                   & (0.017,16.530)                                       \\

Depth Inheritance Tree                   & (1.003,0.502)                                  & /                                                & /                                            & /                                                \\
Number of Children                       & (0.002,0.137)                                  & /                                                & /                                            & /                                                \\
Lack of Cohesion of Methods              & (0.053,80.004)                                  & /                                                & /                                            & /                                                \\
Total Violations                         & (1.160,54.376)                                  & (0.054,63.313)                                       & (0.004,387.551177)                                   & (0.021,18.168)                                       \\
Critical Violations                      & (0.019,9.872)                                  & (0.020,48.811)                                       & (0.007,9.443)                                    & (0.005,5.497)                                        \\
Info Violations                          & (0.019,1.934)                                  & (0.001,1.436)                                        & (0.002,1.401)                                    & (0.003,1.535)                                        \\
Line to Cover                            & (0,0.000)                                  & (0,0.000)                                        & (0,0.000)                                    & (0,0.000)                                        \\
Duplicated Blocks                        & (0,0.015)                                  & (0.001,0.021)                                      & (0,0.010)                                    & (0,0.021)                                       \\
Duplicated Files                         & (0.003,0.135)                                  & (0.001,0.203)                                        & (0,0.222)                                    & (0,0.116)                                        \\
Duplicated Lines                         & (0.439,63.284)                                  & (0.145,163.258)                                      & (0.081,124.342)                                  & (0.085, 102.796)                                      \\ \bottomrule
\end{tabular}
\label{tab:metric-parameters2}
\end{table*}



% \begin{table*}
% \scriptsize
% \centering
% \renewcommand{\arraystretch}{1.2}
% \caption{Parameters of the Fitted Asymmetric Gaussian Distributions ($\boldsymbol{\mu}$, $\boldsymbol{\sigma_1}$, $\boldsymbol{\sigma_2}$)}
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% {\textbf{Metric}} & Java$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & JavaScript$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & Python$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ & TypeScript$(\boldsymbol{\mu}$,$\boldsymbol{\sigma_1}$,$\boldsymbol{\sigma_2})$ \\ \midrule
% Cyclomatic Complexity            & [153(2), 44(3), 41(2))]  & [155(4), 53(5), 87(3))] & [158(1), 41(2), 54(1))] &[121(2), 36(2), 70(2))] \\

% Cognitive Complexity            & [53(5), 47(7), 77(4))]  &[34(4), 32(6), 118(4))] & [127(3), 67(3), 117(3))] & [31(2), 24(3), 80(2))] \\

% Comment Lines            & [57(5), 113(4), 113(4))]   &[0$^*$, 95(2), 95(2))] & [99(3), 78(3), 147(2))]  & [0$^*$, 73(2), 73(2))] \\
% Coupling Between Objects         & [7.0(0.3), 2.5(0.2), 5.0(0.3))]                                                      & /                                                                              & /                                                                          & /                                                                              \\ 
% Fan-in                                   & [1.07(0.03), 0.28(0.03), 1.15(0.03))]                                & /                                                & /                                            & /                                                \\
% Fan-out                          & [5.0(0.1), 1.9(0.1), 4.6(0.1))]                                                      & /                                                                              & /                                                                          & /                                                                              \\
% Loose Class Cohesion             & [0.33(0.01), 0.15(0.01), 0.18(0.01))]                                                      & /                                                                              & /                                                                          & /                                                                              \\
% Tight Class Cohesion             & [0.226(0.008), 0.097(0.007), 0.128(0.008))]                                                      & /                                                                              & /                                                                          & /                                                                              \\
% \bottomrule
% \end{tabular}
% \raggedright $*$ represents that the parameter is fixed to a constant during fitting.
% \label{tab:metric-parameters1}
% \end{table*}

% \begin{table*}
% \centering
% \footnotesize
% \setlength{\tabcolsep}{10pt}
% \renewcommand{\arraystretch}{1.2}
% \caption{Parameters of the Fitted Exponential Distributions ($\boldsymbol{c}$, $\boldsymbol{\lambda}$)}
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% \textbf{Metric}                                   & Java$(\boldsymbol{c},\boldsymbol{\lambda})$ & JavaScript$(\boldsymbol{c},\boldsymbol{\lambda})$ & Python$(\boldsymbol{c},\boldsymbol{\lambda})$ & TypeScript$(\boldsymbol{c},\boldsymbol{\lambda})$ \\ \midrule
% %Cyclomatic Complexity by Number of Files
% File Complexity & [0$^*$, 3.8(0.06)]   & [0$^*$, 3.1(0.1)]                                       & [0$^*$, 2.28(0.07)]                                  & [0$^*$, 6.7(0.1)]                                       \\
% Code Smells                              & [0$^*$, 0.016(0.002)]                                 & [0$^*$, 0.034(0.003)]                                       & [0$^*$, 0.029(0.002)]                                   & [0$^*$, 0.082(0.002)]                                       \\

% Depth Inheritance Tree                   & [1$^*$, 2.37(0.05)]                                  & /                                                & /                                            & /                                                \\
% Number of Children                       & [0$^*$, 10.4(0.2)]                                  & /                                                & /                                            & /                                                \\
% Lack of Cohesion of Methods              & [0$^*$, 0.03(0.001)]                                  & /                                                & /                                            & /                                                \\
% Total Violations                         & [0$^*$, 0.014(0.002)]   & [0$^*$, 0.026(0.002)] & [0$^*$, 0.014(0.002)] & [0$^*$, 0.07(0.002)] \\
% Critical Violations                      & [0$^*$, 0.1(0.01)] & [0$^*$, 0.219(0.008)] & [0$^*$, 0.112(0.008)] & [0$^*$, 0.392(0.006)] \\
% Info Violations                          & [0$^*$, 0.74(0.02)] & [0$^*$, 1.06(0.03)] & [0$^*$, 0.95(0.03)]  & [0$^*$, 0.96(0.02)] \\
% Line to Cover                            & (0, 0)$^\#$  & (0, 0)$^\#$ & (0, 0)$^\#$ & (0, 0)$^\#$ \\
% Duplicated Blocks                        & [0$^*$, 83(3)]  & [0$^*$, 74(1)]  & [0$^*$, 158(3)]  & [0$^*$, 74(2)]\\
% Duplicated Files                         & [0$^*$, 6.3(0.6)] & [0$^*$, 4.7(0.2)] & [0$^*$, 4.4(0.1)]  & [0$^*$, 9(0.3)] \\
% Duplicated Lines                         & [0$^*$, 0.0179(0.0004)]  & [0$^*$, 0.01(0.0005)] & [0$^*$, 0.0162(0.0008)] & [0$^*$, 0.0142(0.0003)] \\ 
% \bottomrule
% \end{tabular}
% \raggedright $*$ represents that the parameter is fixed to a constant during fitting. \\
% $\#$ indicates that the data is problematic.
% \label{tab:metric-parameters2}
% \end{table*}

%Threshold-type metrics, such as the number of 'Code smells', follow an Exponential distribution, as shown on the right of Fig. \ref{fig:metric_dist} and in Table \ref{tab:metric-parameters2}. Projects of higher code quality generally have lower values of these metrics, as expected, corroborating the efficacy of our scoring system. The thresholds $c$ in Eq. \eqref{eq:exp_pdf} are close to 0 except the 'Code Smells', 'Depth Inheritance Tree' and 'Total Violations', which have $c \sim 1$.
%The thresholds $c$ in Eq. \eqref{eq:exp_pdf} are fixed for all fittings; i.e. $c=0$ for all threshold-type metrics except the Depth Inheritance Tree, for which $c=1$. 
%For each of these metrics, the fitted $\lambda$ share the same order of magnitude across the four programming language, indicating common distribution patterns. We observe $\lambda\lesssim1$ for metrics like the 'File Complexity', 'Depth Inheritance Tree', 'Number of Children', 'Duplicated Blocks' and 'Duplicated Files', implying that the scoring system will be less sensitive to changes of metrics on the order of 1 for these metrics. 

Threshold-type metrics, such as 'Code Smells', exhibits an exponential distribution pattern, as represented in Fig. \ref{fig:metric_dist} and corroborated by the values in Table \ref{tab:metric-parameters2}. This distribution trend aligns with the fundamental premise that superior code quality is typically associated with lower metrics, providing a verification for the effectiveness of our applied scoring system. Furthermore, the threshold parameter $c$ in the probability density function (Eq. \eqref{eq:exp_pdf}) hovers near 0, with the exception of 'Code Smells', 'Depth Inheritance Tree', and 'Total Violations' where $c$ approximates 1. The fitted exponential decay parameter, $\lambda$, across the four programming languages presents a similar order of magnitude for each metric, suggesting a shared distribution pattern. Particularly, a $\lambda\lesssim1$ is observed for metrics such as 'File Complexity', 'Depth Inheritance Tree', 'Number of Children', 'Duplicated Blocks', and 'Duplicated Files', which implies a diminished sensitivity of the scoring system to metric variations of the order of 1 for these specified categories.

% Info violation has the lowest $\theta$ as info violation shows the preliminary errors for which managers have the lowest tolerance. 

Non-threshold-type metrics, such as the 'Cyclomatic Complexity', which follow an asymmetric Gaussian distribution, show distinct characteristics from the threshold-type ones. According to Eq. \eqref{eq:agass_score}, projects with metric values close to the Gaussian center get higher scores since they fall into the range where high-quality projects mostly locate. In Table \ref{tab:metric-parameters1}, the Gaussian centers $\mu$ are large ($\gg1$) for the metrics of 'Cyclomatic Complexity', 'Cognitive Complexity' and 'Comment Lines' in most cases except for the 'Comment Lines' of the Javascript and Typescript languages. The later two distributions are almost monotonic ($\mu=0$), the cause of which is not clear to us now. The fitted widths $\sigma_{1,2}$ are large as well and mostly $\sigma_1<\sigma_2$; i.e. relatively long tails are observed on the right of the asymmetric Gaussian distributions. For Java projects, the non-threshold-type coupling metrics, 'Coupling Between Objects', 'Fan-in', and 'Fan-out', exhibit $\mu, \sigma_{1,2} \sim1$ and $\sigma_1 < \sigma_2$, while the cohesion ones ('Loose Class Cohesion' and 'Tight Class Cohesion') show $\mu, \sigma_{1,2} <1$ and $\sigma_1 \sim\sigma_2$.

% High-quality projects exhibit cyclomatic complexity values that are primarily centered around the mean of the distribution, indicating well-structured, maintainable code.

\subsection{Importance Weights}

Table~\ref{tab:importance_values} showcases the feature importance values from the GBC model described in Section \ref{sec: application}, which function as weights in Eq. \eqref{eq: overall_score} for each metric within the three key dimensions: Maintainability, Reliability, and Functionality. The relative importance of each metric in its corresponding dimension for each programming language is listed in the table, while its overall importance value from the GBC model is shown in the brackets. The importance of the three dimensions is obtained by summing overall importance values of their composite metrics, which is shown as 'Sum' in Table \ref{tab:importance_values}.

% \begin{table*}
% \footnotesize
% \centering
% \renewcommand{\arraystretch}{1.2}
% \caption{Importance Values for Metric Scores. For each dimension in each language, the sum of contributions from all metrics is 1. The numbers in the brackets are weights of each metric and their sums are 1 for each programming language.}
% \begin{tabular}{@{}p{2cm}p{4cm}cccc@{}}
% \toprule
% \multirow{2}{2cm}{\textbf{Dimension}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{4}{c}{\textbf{Importance}}                                    \\
%   & \multicolumn{1}{c}{}                        & \textbf{Java}           & \textbf{JavaScript}     & \textbf{Python}     & \textbf{TypeScript} \\ \midrule
% \multirow{12}{*}{Maintainability}             & Cyclomatic Complexity                       & 0.100 (0.0746) & 0.190 (0.0821) & 0.251 (0.1203) & 0.223 (0.0806) \\
%   & File Complexity    & 0.213 (0.1596) & 0.396 (0.1713) & 0.448 (0.2147) & 0.403 (0.1461) \\
%   & Cognitive Complexity                        & 0.071 (0.0533) & 0.289 (0.1251) & 0.120 (0.0575) & 0.215 (0.0779) \\
%   & Code Smells                                 & 0.101 (0.0756) & 0.125 (0.0542) & 0.181 (0.0869) & 0.159 (0.0577) \\
%   & Coupling Between Objects                    & 0.086 (0.0642) & /              & /              & /              \\
%   & Fan-in                                      & 0.101 (0.0755) & /              & /              & /              \\
%   & Fan-out                                     & 0.039 (0.0289) & /              & /              & /              \\
%   & Depth Inheritance Tree                      & 0.071 (0.0535) & /              & /              & /              \\
%   & Number of Children                          & 0.028 (0.0212) & /              & /              & /              \\
%   & Lack of Cohesion of Methods                 & 0.076 (0.0567) & /              & /              & /              \\
%   & Tight Class Cohesion                        & 0.022 (0.0163) & /              & /              & /              \\
%   & Loose Class Cohesion                        & 0.092 (0.0691) & /              & /              & /              \\
%   & \textbf{Sum}   & 1 (0.7485) & 1 (0.4327) & 1 (0.4794) & 1 (0.3623) \\
% \midrule
% \multirow{3}{*}{Reliability}                  & Total Violations                            & 0.468 (0.0578) & 0.289 (0.0700) & 0.294 (0.0683) & 0.228 (0.0651) \\
%   & Critical Violations                         & 0.245 (0.0302) & 0.419 (0.1016) & 0.408 (0.0946) & 0.413(0.1180) \\
%   & Info Violations                             & 0.287 (0.0354) & 0.292 (0.0707) & 0.298 (0.0691) & 0.359 (0.1025) \\
%   & \textbf{Sum}   & 1 (0.1234) & 1 (0.2423) & 1 (0.232) & 1 (0.2856) \\
% \midrule
% \multirow{5}{*}{Functionality}                & Line to Cover                               & 0.000 (0.0000) & 0.000 (0.0000) & 0.000 (0.0000) & 0.000 (0.0000) \\
%   & Comment Lines                               & 0.469 (0.0601) & 0.316 (0.1026) & 0.371 (0.1071) & 0.318 (0.1118) \\
%   & Duplicated Blocks                           & 0.187 (0.0240) & 0.286 (0.0931)   & 0.196 (0.0566) & 0.149 (0.0525) \\
%   & Duplicated Files                            & 0.160 (0.0204) & 0.120 (0.0390) & 0.167 (0.0481) & 0.178 (0.0626) \\
%   & Duplicated Lines                            & 0.184 (0.0235) & 0.278 (0.0904) & 0.266 (0.0766) & 0.356 (0.1252) \\ 
%   & \textbf{Sum}   & 1 (0.1281) & 1 (0.3251) & 1 (0.2884) & 1 (0.3521) \\
%   \bottomrule
% \end{tabular}
% % \begin{tablenotes}
% %     \item  The numbers in parentheses are the original values.
% % \end{tablenotes}
% \label{tab:importance_values}
% \end{table*}

\begin{table*}
\footnotesize
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Importance Values for Metric Scores. For each dimension in each language, the sum of contributions from all metrics is 1. The numbers in the brackets are weights of each metric and their sums are 1 for each programming language.}
\begin{tabular}{@{}p{2cm}p{4cm}cccc@{}}
\toprule
\multirow{2}{2cm}{\textbf{Dimension}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{4}{c}{\textbf{Importance}}                                    \\
  & \multicolumn{1}{c}{}                        & \textbf{Java}           & \textbf{JavaScript}     & \textbf{Python}     & \textbf{TypeScript} \\ \midrule
\multirow{12}{*}{Maintainability}             & Cyclomatic Complexity                       & 0.110 (0.083) & 0.190 (0.082) & 0.250 (0.120) & 0.223 (0.081) \\
  & File Complexity    & 0.220 (0.165) & 0.396 (0.171) & 0.449 (0.215) & 0.402 (0.146) \\
  & Cognitive Complexity                        & 0.086 (0.065) & 0.289 (0.125) & 0.119 (0.057) & 0.215 (0.078) \\
  & Code Smells                                 & 0.066 (0.049) & 0.125 (0.054) & 0.182 (0.087) & 0.160 (0.058) \\
  & Coupling Between Objects                    & 0.096 (0.072) & /              & /              & /              \\
  & Fan-in                                      & 0.108 (0.081) & /              & /              & /              \\
  & Fan-out                                     & 0.057 (0.043) & /              & /              & /              \\
  & Depth Inheritance Tree                      & 0.075 (0.057) & /              & /              & /              \\
  & Number of Children                          & 0.026 (0.020) & /              & /              & /              \\
  & Lack of Cohesion of Methods                 & 0.078 (0.058) & /              & /              & /              \\
  & Tight Class Cohesion                        & 0.010 (0.008) & /              & /              & /              \\
  & Loose Class Cohesion                        & 0.068 (0.051) & /              & /              & /              \\
  & \textbf{Sum}   & 1 (0.752) & 1 (0.432) & 1 (0.479) & 1 (0.363) \\
\midrule
\multirow{3}{*}{Reliability}                  & Total Violations                            & 0.474 (0.056) & 0.288 (0.070) & 0.293 (0.068) & 0.228 (0.065) \\
  & Critical Violations                         & 0.272 (0.032) & 0.420 (0.102) & 0.410 (0.095) & 0.414(0.118) \\
  & Info Violations                             & 0.254 (0.030) & 0.292 (0.071) & 0.297 (0.069) & 0.358 (0.102) \\
  & \textbf{Sum}   & 1 (0.118) & 1 (0.243) & 1 (0.232) & 1 (0.285) \\
\midrule
\multirow{5}{*}{Functionality}                & Line to Cover                               & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
  & Comment Lines                               & 0.454 (0.059) & 0.317 (0.103) & 0.370 (0.107) & 0.318 (0.112) \\
  & Duplicated Blocks                           & 0.162 (0.021) & 0.286 (0.093)   & 0.197 (0.057) & 0.148 (0.052) \\
  & Duplicated Files                            & 0.190 (0.025) & 0.120 (0.039) & 0.166 (0.048) & 0.179 (0.063) \\
  & Duplicated Lines                            & 0.194 (0.025) & 0.277 (0.090) & 0.267 (0.077) & 0.355 (0.125) \\ 
  & \textbf{Sum}   & 1 (0.130) & 1 (0.325) & 1 (0.289) & 1 (0.352) \\
  \bottomrule
\end{tabular}
% \begin{tablenotes}
%     \item  The numbers in parentheses are the original values.
% \end{tablenotes}
\label{tab:importance_values}
\end{table*}

% In the Maintainability dimension, 'File Complexity' held the greatest weight for Java and TypeScript, suggesting its significance. For JavaScript and Python, the 'Cognitive Complexity' and 'Cyclomatic Complexity' metrics respectively demonstrated the highest importance values, indicating their crucial role in the maintainability of software written in these languages.

In the Maintainability dimension, 'File Complexity' holds the largest contribution in all four programming languages, indicating its crucial role in the maintainability of highly-adopted software written in these languages. The other two code complexity measures, 'Cognitive Complexity' and 'Cyclomatic Complexity', as well as 'Code Smells', also contribute significantly. For Java projects, all the coupling and cohesion metrics show similar contributions $\lesssim0.1$. 

In the Reliability dimension, the 'Total Violations' metric contributes mostly for Java and the 'Critical Violations' one is the most important for the other three languages. This suggests that mitigating all violations can help enhance GitHub stars of Java projects, while solving critical violations other than other violations is more important for the three scripting languages. For each programming language, the two less contributed metrics have almost equal importance values. 

In the Functionality dimension, the 'Comment Lines' metric shows the dominant importance in Java, implying its influential role in popularizing Java OSS projects, which may be attributed to the fact that Java might be less intuitive to understand, thereby making code comments essential for understanding Java codes. The 'Comment Lines' metric contributes also significantly for the other three scripting languages, while the rest Functionality metrics are important as well. We note that zero 'Line to Cover' metric values were obtained in our raw data, either caused by problems in obtaining this metric or due to the fact that codes in OSS projects are rarely tested. However, this metric is essential in many close-source software developments. This gap can be closed when applying our methodology in specific companies where values of 'Line to Cover' are obtained for their own close-source projects.

\subsection{Scores}

% Figure environment removed

We present the overall scores of all included OSS projects in Fig.~\ref{fig:scores_all} and assess the explanatory power of our metric scores using Table~\ref{Explainability}. The first five measurements are generated from the GBC model, and R2 is generated from the Gradient Boosting Regression model. Our analysis indicates that Java code metric scores have higher explanatory power than other languages as Java language has more obtained metrics. Another hypothesis is Java is typically used for large-scale platforms and systems development while scripting languages are typically used for data analytics, which may contribute to the different importance of code quality. Future work can be done to verify it more robustly.


\begin{table}
\footnotesize
\centering
\caption{Metric Scores Explanatory Power \label{Explainability}}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Language} & \textbf{Java} & \textbf{JavaScript} & \textbf{Python} & \textbf{TypeScript} \\ \midrule
Accuracy          & 0.947         & 0.826               & 0.808           & 0.817               \\
Precision         & 0.971         & 0.838               & 0.831           & 0.834              \\
Recall            & 0.917         & 0.803               & 0.771           & 0.784               \\
F1                & 0.943         & 0.820               & 0.800           & 0.808               \\
AUC\_ROC          & 0.946         & 0.826               & 0.815           & 0.817               \\
R2                & 0.787    & 0.274 	& 0.186 	& 0.247 \\ 
\bottomrule
\end{tabular}
\end{table}


\section{Conclusion} \label{sec: dis}

Our research focuses on code quality within OSS with the three dimensions of maintainability, reliability, and functionality. We propose methods to assess metric scores based on their distributions. Overall, our study advances the understanding of OSS code quality and contributes to better quality control standards and practices, ultimately supporting the success and sustainability of OSS projects. 

However, our study has limitations. We have not validated the effectiveness of the method systematically, leaving room for future research. We use GitHub stars as an adoption metric, which may not be entirely accurate. Future work can incorporate more metrics, such as the number of releases, download counts, repository contributor activity, and pull request frequency.

\section*{Acknowledgment}
Y. Xia is partly supported by the "Pioneering Innovator" award from the Guangzhou Tianhe District government. Z. Li is partly supported by the Guangdong Basic and Applied Basic Research Foundation (2021A1515012039). We would like to acknowledge useful discussions and support from our colleagues at the HSBC Lab.

\bibliography{template}
\end{document}

\end{document}
