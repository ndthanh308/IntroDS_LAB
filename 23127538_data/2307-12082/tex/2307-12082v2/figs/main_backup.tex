\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage{amssymb,amsmath,amsfonts,eurosym,geometry,ulem,graphicx,caption,color,setspace,sectsty,comment,footmisc,caption,natbib,pdflscape,subfigure,array,hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance} % For balanced columns on the last page
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{algorithm,algcompatible}% http://ctan.org/pkg/{algorithms,algorithmicx}
\usepackage{caption}
\usepackage{amsthm}
\usepackage{lipsum}
\usepackage{etoolbox}
\usepackage{enumitem}
\usepackage{float}
\usepackage{physics}
\allowdisplaybreaks[3]
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{threeparttable}
\doublespacing

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\bibliographystyle{apalike}

\geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}

\begin{document}

\title{Driving Popularity of Open-Source Projects: A Quantitative Examination of Code Quality Metrics}
\author{Siyuan Jin, Mianmian Zhang, Yekai Guo, Yuejiang He, \\ Bichao Chen, Ziyuan Li, Bing Zhu, Yong Xia}

\maketitle

\begin{abstract}
\noindent The rapid expansion of Open Source Software (OSS) underscores the pivotal role of quality assurance in software development. Our research concentrates on the impact of code quality on the popularity of OSS projects. We propose an innovative machine learning-based tool to automate the measurement of code quality, using models like Support Vector Machines (SVM), sequence-to-sequence networks, and BERT. Our tool leverages key GitHub popularity indicators, linking high-quality code directly to project popularity. It quantitatively examines and rates code based on Maintainability, Reliability, and Functional Suitability. The user-friendly rating system provides actionable insights for developers and project managers, serving to increase the popularity of their open-source projects by focusing on the enhancement of code quality.
\end{abstract}

\section{Introduction}
\begin{itemize}
    \item First paragraph: why oss projects want to become popular?
    \item Second paragraph: Are popular projects of high-quality?
    \item Third paragraph: What problems we focus on discussing the relationship between popularity and code quality: large-scale statistical study, programming language senstitive (mention this in the second paragraph)
    \item Fourth paragraph: what we have done and implications for the research community
\end{itemize}

Open Source Software (OSS) has become a pivotal asset in software development, largely due to its cost-effectiveness and practicality. However, the OSS sphere grapples with significant quality disparities in the code, a challenge further amplified by the inability of traditional quality assurance methods such as comprehensive peer reviews to efficiently scale amidst the distributed nature of open source development \citep{sadowski_modern_2018}. OSS projects frequently resort to the collective efforts of numerous contributors to iteratively refine code quality over time \citep{bianchi_organizational_2012}. Still, the intricate task of comprehending and incorporating pre-existing code often daunts contributors, necessitating robust, automated code review tools powered by machine learning models \citep{di_nucci_detecting_2018, ray_large_2014, huq_review4repair_2022, uddin_software_2022}.

Emphasizing the importance of in-depth code review for OSS contributors, we underscore the pivotal role of compatibility, coding standards, and functionality in the evaluation process. We argue that low-quality, poorly written, or inconsistent code can severely undermine project progression. This issue can be addressed with an automated code review tool that not only enhances code readability and usability but also optimizes code contributions.

From the standpoint of project management, the integration of an automated code review tool is crucial for proficient software quality control. This tool bolsters coding standards, ensures code uniformity, and improves readability. It swiftly identifies and rectifies bugs, vulnerabilities, and performance issues, thereby reducing debugging time and enhancing overall code quality. Additionally, this tool verifies the correct usage of APIs and preserves architectural coherence within the codebase. The provision of insightful feedback and useful metrics further equips project managers to make informed decisions about code improvements, refactoring initiatives, and technical debt management, thereby enriching software quality, boosting team productivity, and effectively overseeing the technical facets of the project \citep{kemerer_impact_2009}.

With the emergence of large language models (LLM), coding paradigms are undergoing significant transformation, underscoring the urgency for stringent code quality management measures. Despite the automated code generation capabilities offered by LLMs, exclusive reliance on these outputs could engender inconsistencies and subpar code. Thus, a thorough code quality management system is crucial to enforce coding norms, detect and correct errors, and enhance code readability \citep{shin_evaluating_2011}. This system ensures compatibility with existing codebases, eases the comprehension and integration of generated code, and provides guidelines for effective usage.

Code quality measurements, historically divided into several categories, play an essential role in assessing software functionality \citep{kochhar_empirical_2013}. Our research focuses on building a comprehensive code quality measurement tool, incorporating all relevant categories and leveraging machine learning to automate the review process. We demonstrate the efficacy and speed of our method, marking a significant stride in the automated review of code. Then we focus on what features make a Github project popular from the code quality perspectives.

\section{Literature Review}
% Code quality has been the focus of research for several decades. \citet{stamelos_code_2002} used 10 metrics to measure code quality in a structure analysis. \citet{bianchi_organizational_2012} examined the influence of programming languages on software quality, concluding that there is no significant difference among various programming languages. However, they noted heterogeneity across different programming languages. Consequently, it may be necessary to apply language-specific standards when assessing code quality. A few papers specifcially study a few components of code quality, such as code complexity \citep{mccabe_complexity_1976, shin_evaluating_2011, hassan_predicting_2009}, detection of code smell \citep{dogan_towards_2022,ferreira_detecting_2023}, process maturity \citep{harter_effects_2000},
% \citet{kemerer_impact_2009} investigate the impact of review rate on software quality, while controlling for a comprehensive set of factors that may affect the analysis. The paper provides empirical evidence that the review rate is a significant factor affecting defect removal effectiveness, even after accounting for developer ability and other significant process variables
% Many research uses SonarQube and CheckMarx as benchmark for evaluting code quality \citep{zou_mvulpreter_2022, zhang_automatic_2023}.
% \citet{hijazi_quality_2023} proposes a novel approach to evaluate the quality of modern code reviews using biometric information collected from the reviewer during the review process. The approach utilizes Artificial Intelligence techniques to predict the cognitive load from the extracted biomarkers and classify each code region according to a set of features.
% \citet{buse_learning_2010} explores the concept of code readability and its relation to software quality. The authors derive associations between local code features and human notions of readability to construct an automated readability measure that correlates strongly with measures of software quality.
% \citet{shin_evaluating_2011} Identifying three categories of metrics that can be used to predict vulnerable code locations: complexity, code churn, and developer activity metrics. It shows that these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects compared to a random selection of files for inspection and testing.
% \citet{kochhar_empirical_2013} investigates the adoption of software testing in open source projects by studying more than 20,000 non-trivial software projects and exploring the correlation of test cases with various project development characteristics. The aim is to understand the extent to which testing is used in practice and how it can improve the quality of software systems.

The importance of code quality has been extensively studied in the literature. \citet{stamelos_code_2002} utilized 10 metrics to measure code quality through structural analysis. \citet{bianchi_organizational_2012} investigated the influence of programming languages on software quality and found that while there were no significant differences among languages, heterogeneity existed across different languages, suggesting the need for language-specific standards. Code complexity has also been a focal point in code quality research, with studies conducted by \citet{mccabe_complexity_1976}, \citet{shin_evaluating_2011}, and \citet{hassan_predicting_2009} examining various aspects of complexity and its impact. Detection of code smells has also been explored by \citet{dogan_towards_2022} and \citet{ferreira_detecting_2023}.

\citet{kemerer_impact_2009} examined the impact of review rate on software quality, controlling for various factors. The study revealed that review rate significantly influenced defect removal effectiveness, even after considering developer ability and other process variables. SonarQube and CheckMarx have commonly been used as benchmarks for evaluating code quality, as noted in the works of \citet{zou_mvulpreter_2022} and \citet{zhang_automatic_2023}.

\citet{hijazi_quality_2023} proposed a novel approach to assess the quality of modern code reviews using biometric information collected from reviewers during the review process. The approach employed Artificial Intelligence techniques to predict cognitive load and classify code regions based on extracted features. \citet{buse_learning_2010} explored code readability and its relationship with software quality. They developed an automated readability measure based on associations between code features and human perceptions, which showed a strong correlation with software quality measures.

\citet{shin_evaluating_2011} identified three categories of metrics, including complexity, code churn, and developer activity, to predict vulnerable code locations. By applying these models, they achieved significant reductions in the number of files and lines of code to inspect or test compared to random selection. \citet{kochhar_empirical_2013} investigated the adoption of software testing in open source projects, analyzing over 20,000 projects to explore the correlation of test cases with development characteristics. The study aimed to understand testing practices and how they contribute to software quality improvement.

\citet{chen2021evaluating} have bring the CodeX, a large language model fine-tuned on publicly available code from GitHub, help developers write code more efficiently and accurately, reducing the time and effort required to build and maintain software applications. Later than that, a large number of people (\citet{fried2022incoder, christopoulou2022pangu, li2023starcoder}) working on the responsible development of Large Language Models for Code. However, \citet{ji2023survey} has point out the hallucination problem in the generation content, which degrades the system performance and fails to meet user expectations in many real-world scenarios. AI generated code will be risky if the code generated by LLMs introduces security weaknesses that can be later exploited by hackers. We need a way to validate that code generated by an LLM is correct and does not introduce any security vulnerabilities, or find other ways to address this issue to ensure LLM code generation does not result in less secure software.

Overall, this literature review demonstrates the wide range of topics studied within the field of code quality, highlighting the importance of considering various metrics and approaches to assess and enhance software quality.

\subsection{Technology Acceptance Model}
\citet{davis1985technology} proposes technology acceptance model to show perceived usefulness and perceived ease of use determine the intention of adoption. In our context, we regard the popularity of open-source project (e.g. stars, folks) as one potential intention of adoption. Furthermore, \citet{davis1989perceived} clearly defined perceived usefulness as "the degree to which a person believes that using a particular system would enhance his or her job performance". Perceived ease of use is "the degree to which a person believes that using a particular system would be free of effort". \citet{szajna1994software} shows reliability, construct validity an self-report are key factors on usefulness and ease of use. However, the work does not go deep into software code and measure the quality of these perspectives. We extend prior study by leveraging machine-learning based algorithms to review code and provide more detailed analysis of subcategories. Some other recent studies \citep{riemenschneider2001explaining} work on specific software tool acceptance.  \citet{venkatesh2000determinants} consider perceived ease of use from system perspective, and include control, instrisic motivation and emotion on the model. Our work differs from these studies by providing a more comprehensive view from the code level rather than system level.

\section{Data}
\subsection{Data Sources}
We started by collecting a total of 36,625 projects with more than 50 stars from github, including four different programming languages Java, JavaScript, Python and TypeScript, totaling over 600 million lines of code. The Java projects were then moved into two code audit platforms, SonarQube\footnote{https://www.sonarsource.com} and CheckMarx\footnote{https://checkmarx.com}, which together generated more than 100 metrics for each project. We selected 26 of all metrics based on the way codes are evaluated in ISO/IEC 25010, an international standard that defines and classifies the metrics that need to be considered when evaluating the quality of software code. The remaining projects were just put into SonarQube, and we used 18 metrics. Table~\ref{tab:metric_definition} shows the 26 metrics we used and their corresponding ISO/IEC 25010 characteristic and Figure~\ref{fig:metrics_distribution} gives a preliminary look at our data. Note that the last 8 maintainability metrics are used only in Java projects.  

\begin{table}[htbp]
\centering
\small
\begin{threeparttable}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{2.5cm}|p{5cm}|p{8cm}|}
\hline
\textbf{ISO/IEC 25010 characteristic} & \textbf{Metric} & \textbf{Definition} \\
\hline
Maintainability & Cyclomatic Complexity\tnote{a} & Number of independent paths through code. \\
& Cyclomatic Complexity by Number of Files\tnote{b} & Cyclomatic complexity averaged by files. \\
& Cognitive Complexity\tnote{a} & Combination of cyclomatic complexity and human assessment. \\
& Code Smells\tnote{a} & Number of code smell issues. \\
& Coupling Between Objects & Number of classes that are coupled to a particular class. \\
& Fan-in & Number of input dependencies a class has. \\
& Fan-out & Number of output dependencies a class has. \\
& Depth Inheritance Tree & Number of output dependencies a class has. \\
& Number of Children & Number of immediate subclasses that a particular class has. \\
& Lack of Cohesion of Methods & Degree to which class methods are coupled. \\
& Tight Class Cohesion & Ratio of the number of pairs of directly related methods in a class to the maximum number of possible methods in the class. \\
& Loose Class Cohesion & Ratio of the number of directly or indirectly related method pairs in a class to the maximum number of possible method pairs. \\
\hline
Reliability & Total Violations\tnote{a} & Number of issues including all severity levels (blocker, critical, major, minor, info). \\
& Critical Violations\tnote{a} & Number of issues of the critical severity. \\
& Info Violations\tnote{a} & Number of issues of the info severity. \\
\hline
Functional Suitability & Line Coverage\tnote{a} & Lines covered by unit tests. \\
& Comment Lines\tnote{a} & Number of comment lines. \\
& Duplicated Blocks\tnote{c} & Number of duplicated blocks of line. \\
& Duplicated Files\tnote{b} & Number of files involved in duplicated blocks. \\
& Duplicated Lines\tnote{a} & Number of lines involved in duplicated blocks. \\
\hline
Sized\tnote{d} & Number of Functions & Number of functions. \\
& Number of Statements & Number of statements. \\
& Number of Classes & Number of classes. \\
& Number of Files & Number of files. \\
& Non-comment Line of Codes & Number of lines of code that are not blank or comment ones. \\
& Line to Cover & Total lines in the code that should have tests for. \\
\hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item[a] Normalized by Non-comment Line of Codes.
\item[b] Normalized by Number of Functions.
\item[c] Normalized by Number of Statements.
\item[d] Not included in the original ISO/IEC 25010 characteristics, but we also regard it as an important metric.
\end{tablenotes}
\caption{Metric definition for all metrics.}
\label{tab:metric_definition}
\end{threeparttable}
\end{table}

% Figure environment removed

\subsection{Metrics Overview}
\section{Methodologies}
\subsection{Feature Distribution}
We fitted an exponential distribution or an asymmetric Gaussian distribution to each indicator, except for size, based on Figure~\ref{fig:metrics_distribution}. The distribution parameters were estimated, and subsequently, we used these distributions to score each project. The probability density functions of the exponential distribution and the asymmetric Gaussian distribution are given by:

\begin{equation}
f(x;c,\theta) = \frac{1}{\theta} \exp\left[-\frac{(x-c)}{\theta}\right] I_{x\geq c}
\end{equation}

\begin{equation}
f(x;\mu,\sigma_1,\sigma_2) = C\left[\exp\left(-\frac{(x-\mu)^2}{2\sigma_1^2}\right) I_{x<\mu} + \exp\left(-\frac{(x-\mu)^2}{2\sigma_2^2}\right) I_{x\geq\mu}\right]
\end{equation}

Here, $c$ and $\theta$ are the parameters for fitting the exponential distribution, and $\mu$, $\sigma_1$, and $\sigma_2$ are the parameters for fitting the asymmetric Gaussian distribution. $C$ is the normalization coefficient, and $I(\cdot)$ is the indicator function.

The fitted parameters for the Java projects are presented in Table~\ref{tab:metric-parameters1} and Table~\ref{tab:metric-parameters2}:
\begin{table}[ht]
\centering
\small
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Metric} & $\boldsymbol{\mu}$ & $\boldsymbol{\sigma_1}$ & $\boldsymbol{\sigma_2}$ \\
\hline
Cyclomatic Complexity & 0.157 & 0.053 & 0.040 \\
Fan-out & 5.194 & 2.106 & 4.662 \\
Loose Class Cohesion & 0.347 & 0.186 & 0.166 \\
Tight Class Cohesion & 0.242 & 0.126 & 0.119 \\
Coupling Between Objects & 7.635 & 3.349 & 5.267 \\
\hline
\end{tabular}
\caption{Metric parameters for scoring ($\boldsymbol{\mu}$, $\boldsymbol{\sigma_1}$, $\boldsymbol{\sigma_2}$).}
\label{tab:metric-parameters1}
\end{table}

\begin{table}[ht]
\centering
\small
\begin{tabular}{|c|c|c|}
\hline
\textbf{Metric} & $\boldsymbol{c}$ & $\boldsymbol{\theta}$ \\
\hline
Cyclomatic Complexity by Number of Files & 0 & 0.009 \\
Cognitive Complexity & 0 & 0.091 \\
Code Smells & 0 & 51.08 \\
Fan-in & 0 & 2.056 \\
Depth Inheritance Tree & 1 & 0.441 \\
Number of Children & 0 & 0.052 \\
Lack of Cohesion of Methods & 0 & 470.8 \\
Total Violations & 0 & 54.81 \\
Critical Violations & 0 & 9.088 \\
Info Violations & 0 & 1.252 \\
Line Coverage & 0 & 0.000 \\
Comment Lines & 0 & 0.116 \\
Duplicated Blocks & 0 & 0.029 \\
Duplicated Files & 0 & 0.001 \\
Duplicated Lines & 0 & 0.060 \\
\hline
\end{tabular}
\caption{Metric parameters for scoring ($\boldsymbol{c}$, $\boldsymbol{\theta}$).}
\label{tab:metric-parameters2}
\end{table}

\subsection{Weighting}
Please note that you may need to adjust the table formatting according to your requirements.

After obtaining the parameters, we assign scores to each project for each metric. Considering a specific metric $m_i$, we examine the values $v_{ij}$ for all projects $j=1,\ldots,N_ja$, where $N_ja$ represents the total number of Java projects. Using the fitted distributions mentioned above, we calculate the standardized distance $z_{ij}$ between $v_{ij}$ and the mode of the corresponding distribution for $m_i$.

For the asymmetric Gaussian distribution:
\begin{equation}
z_{ij} = \frac{{(\mu_i - v_{ij})}}{{\sigma_{1i}}} \cdot I_{v_{ij} < \mu_i} + \frac{{(v_{ij} - \mu_i)}}{{\sigma_{2i}}} \cdot I_{v_{ij} \geq \mu_i}
\end{equation}

For the exponential distribution:
\begin{equation}
z_{ij} = \frac{{(v_{ij} - c_i)}}{{\theta_i}}
\end{equation}

Subsequently, the scores are calculated using $z_{ij}$:
\begin{equation}
\text{score}_{ij} = \frac{{\phi(z_{ij})}}{{\phi(0)}} \cdot 100
\end{equation}
where $\phi(x) = \frac{1}{{\sqrt{2\pi}}} \exp\left(-\frac{{x^2}}{{2\sigma^2}}\right)$ is the density function of the standard normal distribution.


% Figure environment removed

These equations allow us to calculate the scores for each project on each metric based on the corresponding $z_{ij}$ values.

To obtain the score on Maintainability, Reliability and Functional Suitability, we designed a weight calculation method to obtain the weight of each metric by the feature importance of the classification model in machine learning.

First of all, we selected the 20\% projects with the highest and lowest number of stars from all java projects. Then we divided the training set and the test set with a ratio of 4:1. On the training set, we built the classification model by gradient boosting tree and xgboost respectively, and evaluated the accuracy of the classification model on the test set. Figure\ref{fig:signal_background_distribution} shows the prediction probabilities of two classification models on Java projects. 

% Figure environment removed

Finally, the feature importance normalized by the softmax function is used as the weight, which is shown in Table~\ref{tab:importance_values} and the scores are weighted and summed to obtain the Maintainability, Reliability and Functional Suitability scores

\begin{table}[ht]
\small
\centering
\begin{tabular}{|p{3cm}|p{5cm}|c|c|}
\hline
\textbf{ISO/IEC 25010 characteristic} & \textbf{Metric} & \textbf{GBC importance} & \textbf{XGB importance} \\
\hline
\multirow{11}{*}{Maintainability} & Cyclomatic Complexity & 0.0836 (0.0620) & 0.0826 (0.0473) \\
& Cyclomatic Complexity by Number of Files & 0.0942 (0.1815) & 0.0887 (0.1184) \\
& Cognitive Complexity & 0.0813 (0.0342) & 0.0825 (0.0464) \\
& Code Smells & 0.0836 (0.0624) & 0.0836 (0.0602) \\
& Coupling Between Objects & 0.0818 (0.0399) & 0.0822 (0.0427) \\
& Fan-in & 0.0850 (0.0788) & 0.0839 (0.0631) \\
& Fan-out & 0.0814 (0.0351) & 0.0824 (0.0447) \\
& Depth Inheritance Tree & 0.0816 (0.0373) & 0.0829 (0.0510) \\
& Number of Children & 0.0815 (0.0371) & 0.0828 (0.0493) \\
& Lack of Cohesion of Methods & 0.0823 (0.0460) & 0.0822 (0.0418) \\
& Tight Class Cohesion & 0.0812 (0.0324) & 0.0837 (0.0606) \\
& Loose Class Cohesion & 0.0825 (0.0490) & 0.0822 (0.0421) \\
\hline
\multirow{3}{*}{Reliability} & Total Violations & 0.3323 (0.0410) & 0.3331 (0.0339) \\
& Critical Violations & 0.3398 (0.0635) & 0.3369 (0.0563) \\
& Info Violations & 0.3279 (0.0277) & 0.3300 (0.0356) \\
\hline
\multirow{5}{*}{Functional Suitability} & Line Coverage & 0.1931 (0.0000) & 0.1923 (0.0000) \\
& Comment Lines & 0.2133 (0.9918) & 0.2031 (0.0547) \\
& Duplicated Blocks & 0.1980 (0.0251) & 0.1982 (0.0305) \\
& Duplicated Files & 0.1986 (0.0282) & 0.2037 (0.0577) \\
& Duplicated Lines & 0.1969 (0.0196) & 0.2027 (0.0527) \\
\hline
\end{tabular}
\caption{Importance values for metrics in ISO/IEC 25010 characteristics. The numbers in parentheses are the original values.}
\label{tab:importance_values}
\end{table}


\section{Results}
\subsection{XGBoost}

\subsection{Code Quality Platform}


\section{Discussion and Outlook }

\clearpage
% \bibliographystyle{alpha}
\bibliography{literature}

\end{document}