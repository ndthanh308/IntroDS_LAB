\section{Methodology}

\subsection{Threat Model}
Following existing studies on attacking pre-trained encoders~\cite{jia2022badencoder,saha2022backdoor,liu2022poisonedencoder}, we assume the attacker has access to the pre-trained encoders (\eg, through purchasing or directly downloading from publicly available websites), but has no knowledge of the pre-training datasets and the downstream tasks. 
The goal of the attacker is  to conduct non-targeted adversarial attacks  to disable the downstream tasks or damage their accuracy. Specifically, the attacker uses the pre-trained encoder to design a downstream agnostic universal adversarial perturbation or patch  that applies to various kinds of the input images from different datasets.
Then the adversarial example can mislead all the downstream classifiers that inherit the victim pre-trained encoder.
We also assume that the downstream task undertaker (called user hereinafter) is able to fine-tune the linear layer or the pre-trained encoder for their cause and the model provider can adopt common defenses like adversarial training to purify the encoder.

% Figure environment removed

 % Figure environment removed

\subsection{Problem Definition}
Given an input $x \in \mathcal{D}_{p}$ to a pre-trained  encoder $ g_{\theta} (\cdot )$ that returns a feature vector $v \in \mathcal{V}$, a downstream classifier $f_{\theta^{'} } (\cdot )$ gives predictions based on the similarity of the feature vectors, where $\theta$ and $\theta^{'}$ denote the parameters of the pre-trained encoder and the downstream classifier, $x$ indicates any image in the dataset, $\mathcal{D}_{p}$ and $\mathcal{V}$ refer to the pre-training dataset and feature space, respectively.
The attacker uses an attacker's surrogate dataset $\mathcal{D}_{a}$, which is irrelevant to the pre-training dataset $\mathcal{D}_{p}$ and the downstream dataset  $\mathcal{D}_{d}$, to generate a universal adversarial noise against the pre-trained encoder. 
Additionally, the universal adversarial noise $\delta$ should be suffciently small, and modeled through an upper-bound $\epsilon$ on the $l_{p}$-norm.
This problem can be formulated as:
\begin{equation} \label{eq:1}
 g_{\theta}\left (  x + \delta  \right )  \ne g_{\theta}\left (  x  \right ), \quad  s.t.\left \| \delta  \right \| _{p}\le \epsilon
\end{equation}



The attacker's goal is to implement a universal non-target attack to fool the downstream classifier $f_{\theta^{'}}$.
When a universal adversarial noise $\delta$ is attached to the downstream dataset sample $x \in \mathcal{D}_{d}$ , it leads to misclassification. Therefore, the attacker's goal can be formalized as:
\begin{equation} \label{eq:2}
f_{\theta^{'} }( g_{\theta}\left (  x + \delta  \right )) \ne  f_{\theta^{'} }(g_{\theta}\left (  x  \right ) ), \quad s.t.  \left \| \delta  \right \|_{ p} \le \epsilon
\end{equation}






\subsection{Intuition Behind AdvEncoder}
The pre-trained encoder outputs similar feature vectors for similar images, which are close together in the feature space and far away from the images of other categories. 
The  downstream tasks will output decisions based on the these feature vectors, thus the attacker needs to push the adversarial example away from its initial position as much as possible in the feature space.
In order to realize downstream-agnostic adversarial attack, there are two challenges ahead.


\textbf{Challenge I: Lack of supervised signals in pre-trained encoder.}
When the attacker feeds the image to the pre-trained encoder, it only obtains the corresponding feature vector instead of the label.
It is infeasible to effectively attack the pre-trained encoder with the traditional approaches of adversarial examples in supervised learning.
An intuitive idea is to add a large budget perturbation to the sample to make the pre-trained encoder misclassify it. 
However,  as seen from~\cref{fig:tsne}(a), a large budget perturbation will not necessarily achieve the above goal, but may simply be an internal movement within the same class, rather than in a direction away from that class.
Recent works~\cite{wang2020high,madry2017towards,delac2005effects} have revealed that surface-statistical content with high frequency property is essential for DNNs and adversarial perturbations also have this property. 
Therefore, we propose using a universal adversarial noise to change the high frequency component of the image, \ie, the texture information, to influence the output of the pre-trained encoder. It plays the role of  label guidance in the supervised learning, and it is easier to push the target samples out of the original decision boundaries from the perspective of directly altering the semantics of the image itself.


\textbf{Challenge II: Lack of information about the downstream tasks.}
In the pre-trained encoder to downstream task paradigm, where fine-tuning affects original feature boundaries of the model, the above approach that simply fools the pre-trained encoder can barely influence downstream task decisions.
As seen in~\cref{fig:tsne}(b), the adversarial example that have left the original class are again correctly classified by the downstream model after the change of decision boundaries caused by fine-tuning.
We thus hope to make the adversarial examples far enough away from the original class by a universal adversarial noise under a small perturbation bound, as depicted in~\cref{fig:tsne}(c).
Consequently, the downstream classifier will be misled based on the apparent similarity of the feature vectors.
Given the remarkable capability of generative networks at generating features with fixed patterns, we further design a generative attack framework to improve the generalization of universal adversarial noise. As shown in~\cref{fig:tsne}(d),  all target samples will be clustered together in the feature space and get away from all the normal samples, making it difficult for downstream tasks to correctly classify target samples.


\subsection{Frequency-based Generative Attack Framework}
In this section, we present AdvEncoder, a novel generative attack against pre-trained encoder in self-supervised learning. 
The pipeline of AdvEncoder is depicted in~\cref{fig:pipeline}. It consists of an adversarial generator $\mathcal{G}$, a high frequency filter $\mathcal{H}$ and a victim encoder $\mathcal{E}$. 
Specifically, we design a frequency-based  generative attack framework to generate a universal adversarial noise. 
By feeding a fixed noise $z$ into the adversarial generator, we obtain a universal adversarial noise and paste it onto target image of the attacker's surrogate dataset $\mathcal{D}_{a}$ to get an adversarial example $x^{adv}$. 

The objective function of the adversarial generator $\mathcal{G}$ is:
\begin{equation}  \label{eq:3}
 \mathcal{L}_{\mathcal{G}} =\alpha  \mathcal{L}_{adv} + \beta  \mathcal{L}_{hfc} +\lambda \mathcal{L}_{q}
\end{equation}
where  $\mathcal{L}_{adv}$ is the adversarial loss function, $\mathcal{L}_{hfc}$  is the high frequency component loss function, $\mathcal{L}_{q}$ is the quality loss function, $\alpha$, $\beta$, $\lambda$ are pre-defined hyper-parameters. 


$\mathcal{L}_{adv}$ enhances the attack strength of universal adversarial noise by maximizing the feature vector distance between the normal and adversarial samples of the encoder output.
We adopt InfoNCE \cite{oord2018representation} loss to measure the similarity between the output feature vectors of the pre-trained encoder $g (\cdot )$. Specifically, we treat the benign sample $x_{i} \in \mathcal{D}_{a}$ and the adversarial sample $x_{i}^{adv}$ as negative pairs, pulling away their feature distance. Thus $\mathcal{L}_{adv}$  is expressed as:

\begin{equation}  \label{eq:4}
\mathcal{L}_{adv} = log\left [ \frac{exp\left ( S\left ( g_{\theta}(x_{i}^{adv}), g_{\theta}(x_{i} ) \right )  \right /\tau ) }{ {\textstyle \sum_{j=0}^{K}} exp\left (  S\left ( g_{\theta}(x_{i}^{adv}), g_{\theta}(x_{j}) \right / \tau)  \right )  }  \right ]
\end{equation}
where 
$S \left ( \cdot \right ) $ denotes the cosine similarity measure function, $j$ is not equal to $i$, and $\tau$ indicates a temperature parameter.



Due to the lack of the guidance of label  information, pushing away the locations of output embeddings in the feature space  by adding noises alone requires large  perturbation budget.
$\mathcal{L}_{hfc}$ changes the original semantic features of the image by modifying the high frequency components
to further separate the  location of the target sample.

We can obtain the HFC of an image through the high frequency component filter $\mathcal{H}$.
The high frequency component loss $\mathcal{L}_{hfc}$  can be formalized as:

\begin{equation} \label{eq:5}
\mathcal{L}_{hfc} = - {\left \|\mathcal{H}(x^{adv})-\mathcal{H}(x)  \right \| } _{2}
\end{equation}

To achieve better stealthiness, we use $\mathcal{L}_{q}$ to control the magnitude of the adversarial noises output by the generator and crop $\delta$ after each optimisation to ensure it meets the constraints $\varepsilon$. Formally, we have:
\begin{equation} \label{eq:6}
\mathcal{L}_{q} = {\left \|x^{adv}- x  \right \| } _{2}
\end{equation}



Without changing the framework of AdvEncoder, we can convert a universal adversarial noise into two common forms of attacks, universal adversarial perturbation (AdvEncoder-Perturbation, abbreviated as Adv-PER) and universal adversarial patch (AdvEncoder-Patch, abbreviated as Adv-PAT).


\noindent\textbf{Adv-PER.} The attacker  directly adds the universal adversarial perturbation generated by the   generator  to the image, which has better stealthiness.
The perturbation-based adversarial example can be represented as:
\begin{equation} \label{eq:7}
x^{adv} = x +  \mathcal{G}(z)
\end{equation}


\noindent\textbf{Adv-PAT.} 
The attacker can apply the adversarial patch to the image with a randomly chosen hidden location to obtain the adversarial example. It is easier to be realized in the physical world.
The patch-based adversarial example can be represented as:
\begin{equation} \label{eq:8}
x^{adv}  = x  \odot (1- m) + \mathcal{G}(z)  \odot m
\end{equation}
where $ \odot$ denotes the element-wise product, $m$ is a binary matrix that contains the position information of the universal adversarial patch.

