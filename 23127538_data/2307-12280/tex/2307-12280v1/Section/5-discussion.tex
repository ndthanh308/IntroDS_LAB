\section{Discussion and Limitation}
In this paper, we mainly study non-targeted attacks due to downstream knowledge limitations. 
But we can still achieve targeted attacks through other perspectives.
Firstly, the result from Figure~\ref{fig:tsne}(d) shows that the adversarial examples produced by AdvEncoder form a class of their own in the feature space of downstream model.
We can choose the labels where the adversarial examples are classified as the target labels.
Secondly, the attacker can also use some reference samples as targets for optimization and use their labels in downstream tasks as target labels.

Existing works~\cite{peng2022fingerprinting,madry2017towards, chen2021towards} suggest that the parameters and structure of the target model changes can greatly affect the performance of adversarial examples. 
Modifications to the parameters of the pre-trained encoder in downstream tasks affect the attack performance of the adversarial perturbations, but have slight effect on the adversarial patchs. At the same time, it also puts a higher demand on the user's computational resources and labeled data.

