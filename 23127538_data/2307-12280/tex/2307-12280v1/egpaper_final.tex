\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{booktabs}
\usepackage{amsmath, amsthm, amssymb}

\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}}
% \renewcommand{\algorithmicensure}{\textbf{Parameter:}}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{bbding}
\usepackage{url}
\usepackage{threeparttable}

\usepackage{caption}
\usepackage{balance}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{multirow}
% Include other packages here, before hyperref.

\usepackage{color}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\def\ie{\textit{i.e.}}
\def\eg{\textit{e.g.}}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage{lipsum}
% \usepackage[symbol]{footmisc}
% \renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\usepackage{authblk}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Downstream-agnostic Adversarial Examples}


\author{Ziqi Zhou\textsuperscript{\rm $*$}\textsuperscript{1 2 3 4}, Shengshan Hu\textsuperscript{\rm $*$}\textsuperscript{1 2 3 4},  Ruizhi Zhao\textsuperscript{\rm $*$}\textsuperscript{1 2 3 4}\\ \vspace{-0.5em} Qian Wang\textsuperscript{\rm $\ddagger$}, Leo Yu Zhang\textsuperscript{\rm $\mathsection$},   Junhui Hou\textsuperscript{\rm $\mathparagraph$}, Hai Jin\textsuperscript{\rm $\dagger$}\textsuperscript{1 2 5}\\\vspace{0.5em}

\textsuperscript{\rm $*$}School of Cyber Science and Engineering, Huazhong University of Science and Technology\\
\textsuperscript{\rm $\dagger$}School of Computer Science and Technology, Huazhong University of Science and Technology\\
\textsuperscript{\rm $\ddagger$}School of Cyber Science and Engineering, Wuhan University\\
\textsuperscript{\rm $\mathsection$}School of Information and Communication Technology, Griffith University\quad\\
\textsuperscript{\rm $\mathparagraph$}Department of Computer Science, City University of Hong Kong \\
{\tt\small \{zhouziqi, hushengshan, zhaoruizhi, hjin\}@hust.edu.cn}\\ 
{\tt\small qianwang@whu.edu.cn}, 
{\tt\small leo.zhang@griffith.edu.au}, 
{\tt\small jh.hou@cityu.edu.hk}
}


\maketitle
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream  users only need to perform fine-tuning operations to enjoy the benefit of ``large model". Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use. 
\vspace{0.2em}
\blfootnote{
\textsuperscript{1}National Engineering Research Center for Big Data Technology and System
\textsuperscript{2}Services Computing Technology and System Lab \
\textsuperscript{3}Hubei Key Laboratory of Distributed System Security \ 
\textsuperscript{4}Hubei Engineering Research Center on Big Data Security \ \textsuperscript{5}Cluster and Grid Computing Lab
}

In this paper, we propose AdvEncoder, the first framework for generating downstream-agnostic universal adversarial examples based on the pre-trained encoder. 
AdvEncoder aims to construct a universal adversarial perturbation or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained encoder. 
Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high frequency component information of the image to guide the generation of adversarial examples. 
Then we design a generative attack framework to construct adversarial perturbations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability.
Our results show that an attacker can successfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset.
We also tailor four defenses for pre-trained encoders, the results of which further prove the attack ability of AdvEncoder.
Our codes are available at: \url{https://github.com/CGCL-codes/AdvEnocder}.

\end{abstract}
\vspace{-0.6em}



%%%%%%%%% BODY TEXT


\input{AdvEncoder/Section/1-introduction}
\input{AdvEncoder/Section/2-relatedwork}
\input{AdvEncoder/Section/3-methodology}
\input{AdvEncoder/Section/4-experiments}
\input{AdvEncoder/Section/6-conclusion}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{AdvEncoder/Section/0-appendix}
\end{document}
