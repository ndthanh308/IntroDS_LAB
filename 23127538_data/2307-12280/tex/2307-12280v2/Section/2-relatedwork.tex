\section{Background and Related Work}
\subsection{Self-supervised Learning}
Self-supervised learning seeks to utilize the oversight signals within the unlabeled data itself to pre-train encoders that can convert complex inputs into generic representations. The pre-trained encoder that learned generally valuable domain knowledge can be used as a universal feature extractor to transfer knowledge to solve different specific downstream tasks. In this paper, we concentrate on image encoders.

Based on~\cite{fini2022self, tao2022exploring},  self-supervised learning schemes can be divided into the following categories: \emph{(1) contrastive learning methods} (\eg, MoCo~\cite{chen2020improved, chen2021empirical}, SimCLR~\cite{chen2020simple}) train representations such that dissimilar negative pairs are widely apart and comparable positive pairs are shown to be near to one another. \emph{(2) negative-free methods} (\eg, BYOL~\cite{grill2020bootstrap}, Sim-Siam~\cite{chen2021exploring}, and ReSSL~\cite{zheng2021ressl}) achieve better representation without the use of negative samples by maintaining the consistency between positive samples and ignoring negative ones. \emph{(3) clustering-based methods} (\eg, SwAV~\cite{caron2020unsupervised}, DeepCluster v2~\cite{caron2020unsupervised}, and DINO~\cite{caron2021emerging}) group  similar samples into the same class using conventional clustering methods. \emph{(4) redundancy reduction-based methods} (\eg, Barlow Twins~\cite{zbontar2021barlow}, W-MSE~\cite{ermolov2021whitening}, VICReg~\cite{bardes2021vicreg}, and VIbCReg~\cite{lee2021vibcreg}) enhance the connection in the same dimension of the representation while attempting to decoupling in distinct dimensions. Concurrently, the use of nearest-neighbor retrieval has been investigated in NNCLR~\cite{dwibedi2021little}. These approaches start from different motivations, design different loss functions, and use different network structures and tricks, which also make them have different defense abilities against adversarial attacks.

\subsection{Attacks on Pre-trained Encoders}

Recently, a growing number of works began to investigate the privacy and security issues of the pre-trained encoders in self-supervised learning. 
Some efforts investigated privacy risks against pre-trained encoders, such as membership inference attacks~\cite{he2021quantifying, liu2021encodermi}, model extraction ~\cite{dziedzic2022difficulty, liu2022stolenencoder,sha2022can}.
At the same time, backdoor attacks and poisoning attacks, the common security threats that usually occur in the training phase, have been shown to be deleterious to pre-trained encoders~\cite{carlini2021poisoning,jia2022badencoder,liu2022poisonedencoder,saha2022backdoor}. 
In contrast, \textit{adversarial examples}, which appear during the testing phase and pose great threat against neural networks, have not been thoroughly investigated yet. 
A concurrent work, PAP~\cite{ban2022pre}, produced a pre-trained perturbation by lifting the feature activations of low-level layers, but the generated adversarial examples lack semantics and rely heavily on the pre-training dataset.
On the contrary, our work aims to achieve effective attacks from the perspective of directly changing the intrinsic texture features of the samples under more demanding conditions that better reflect 
realistic scenarios.


\subsection{Universal Adversarial Examples}
It is well known that deep neural networks are vulnerable to adversarial examples, where an attacker can fool the model by adding minor noise to the image, usually in the form of perturbation~\cite{carlini2017towards, goodfellow2014explaining, hu2022protecting, hu2023pointca} and patch \cite{hu2021advhash, liu2020bias, yang2020design}.
Universal adversarial attack \cite{moosavi2017universal} was proposed to fool the target model by imposing a single adversarial noise vector on all the images.
Existing works can be divided into optimization-based universal adversarial attacks~\cite{moosavi2017universal, mopuri2018generalizable, mopuri2017fast} and generative universal adversarial attacks~\cite{hayes2018learning, mopuri2018nag, mopuri2018ask}.
Compared with  optimization-based solution, generative universal adversarial attacks can generate more generalized and natural-looking adversarial examples by learning the distribution of samples.
% UAP ~\cite{moosavi2017universal}, UAPEPGD~\cite{deng2020universal}, SSP~\cite{naseer2020self}.
However, existing generative universal adversarial attacks in supervised learning can only fool a single model and require the label  information of the model output. 
Since pre-trained encoders can only output the feature vector corresponding to the image, exiting attacks cannot be directly applied  to  the pre-trained encoders, let alone having no knowledge about the downstream tasks.
Some works also proposed different defenses against adversarial examples, such as data pre-processing, adversarial training~\cite{madry2017towards,tramer2019adversarial}, pruning~\cite{ zhu2017prune}, and fine-tuning~\cite{peng2022fingerprinting}. These methods can defend against adversarial samples at different phases.
