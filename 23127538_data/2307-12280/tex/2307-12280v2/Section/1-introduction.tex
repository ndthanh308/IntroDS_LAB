\section{Introduction}
 % Figure environment removed


\emph{Self-supervised learning}~\cite{chen2020simple, chen2020improved} (SSL) is an emerging machine learning paradigm that seeks to overcome the restrictions of labeled data. It usually uses a large volume of unlabeled data to pre-train a general-purpose encoder, which can be used as a feature extractor for various downstream tasks like image classification, image retrieval, object detection, etc. 
%where only a small training data is required for fine-tuning. 
As a result, any resource-constrained user can enjoy the advantages of ``large model" without performing the expensive training from scratch, where only light-weight fine-tuning operations are needed at its request. 
Driven by this promising prospect, pre-training encoders become popular in industry and many service providers
publicly release their pre-trained encoders  (\eg, SimCLR by Google~\cite{chen2020simple, chen2020big}, MoCo by Meta~\cite{chen2021empirical, he2020momentum}) or deploy them as a commercial service (\eg, OpenAI~\cite{openaiapi},  Clarifai~\cite{clarifaiapi}).

%In this work, we paid attention to the security of pre-trained encoder, which received much less consideration in the literature. 

Meanwhile, it is well known that \emph{deep neural networks} (DNNs) are vulnerable to various adversarial attacks~\cite{goodfellow2014explaining, moosavi2017universal, yang2020design, zhou2023advclip}, which will make pre-trained encoder fragile as well. However, the security of pre-trained encoder has received much less consideration in the literature.  
Although some recent works studied security threats on pre-trained encoders including backdoor attack~\cite{hu2022badhash, jia2022badencoder}, poisoning attack~\cite{liu2022poisonedencoder}, and privacy risks~\cite{cong2022sslguard, liu2021encodermi}, 
none of them 
paid attention to adversarial examples, another kind of prevalent and destructive attack on DNNs. 
Constructing adversarial examples against pre-trained encoders is quite different  from its traditional attack route due to the fact that the attacker has no knowledge of the downstream tasks. 
In other words, the attacker needs to attack a DNN  without knowing its task type, the pre-training dataset, and the downstream dataset,  even when the whole model will get fine-tuned.
\textit{To the best of our knowledge, how to realize adversarial example attack in the practical scenario of pre-training still remains challenging and unresolved.}

In this work, we take a big step towards bridging the gap between adversarial examples and pre-trained encoders. 
We consider both adversarial perturbation~\cite{carlini2017towards, goodfellow2014explaining, kurakin2018adversarial, moosavi2016deepfool}  and  patch~\cite{brown2017adversarial,hu2021advhash, liu2020bias, yang2020design}. The former one has a high imperceptibility, while the latter one is visible but confined to a small area of the image and more readily applicable in the physical world.
Furthermore, without the knowledge of downstream data,
% Moreover, \red{due to the limitation of unknown input samples,} 
we aim to realize universal adversarial attacks~\cite{hayes2018learning,moosavi2017universal,zhang2024whydoes} where  one adversarial perturbation or patch applies to a set of natural images and can cause model misclassification.

Specifically, we propose AdvEncoder, a novel attack framework for generating downstream-agnostic  universal adversarial examples.
% \red{from the perspective of directly altering the semantic features of the image itself.}
% The most challenging job lies in \red{addressing the limitations of lacks of labels in the pre-trained encoder, the information about the downstream tasks, and model robustness variation}. 
The most challenging job lies in addressing the limitations and lacking supervised signals and the information about the downstream tasks.
Inspired by the fact that deep neural networks are biased towards texture features of images~\cite{jo2017measuring, wang2020high}, the change of texture information, 
\ie, the \emph{high  frequency components} (HFC) of the image,  is very likely to cause the model decision change. 
We first exploit a high frequency component filter to get the HFC of benign and adversarial samples, and pull away their Euclidean distance as much as possible to influence the model's decision. 
We then design a  generative attack framework to construct adversarial perturbations or patches with high attack success rates and transferability by learning the distribution of the data, with a fixed random noise as input. 
Our main contributions are summarized as follows:

\begin{itemize}
\item We propose AdvEncoder, the first attack framework to construct downstream-agnostic universal adversarial examples in self-supervised learning. We reveal that the pre-trained encoder incurs severe security risks for the downstream tasks.
\item We design a frequency-based generative network to generate universal adversarial examples by directly alearting  the texture  features of the image itself. It is a flexible framework that  can generate both adversarial perturbations and patches.
\item Our extensive experiments on fourteen  self-supervised training methods and four image datasets show that our AdvEncoder achieves high attack success rates and transferability against different downstream tasks. 
\item We tailor four popular defenses to mitigate AdvEncoder. The
results further prove the attack ability of AdvEncoder and highlight the needs of new defense mechanism to defend pre-trained encoders. 

\end{itemize}







