@misc{2204.02311,
Author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
Title = {PaLM: Scaling Language Modeling with Pathways},
Year = {2022},
Eprint = {arXiv:2204.02311},
}

@misc{2210.15424,
Author = {Teven Le Scao and Thomas Wang and Daniel Hesslow and Lucile Saulnier and Stas Bekman and M Saiful Bari and Stella Biderman and Hady Elsahar and Niklas Muennighoff and Jason Phang and Ofir Press and Colin Raffel and Victor Sanh and Sheng Shen and Lintang Sutawika and Jaesung Tae and Zheng Xin Yong and Julien Launay and Iz Beltagy},
Title = {What Language Model to Train if You Have One Million GPU Hours?},
Year = {2022},
Eprint = {arXiv:2210.15424},
}
@article{kalamkar2019study,
  title={A study of BFLOAT16 for deep learning training},
  author={Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and others},
  journal={arXiv preprint arXiv:1905.12322},
  year={2019}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{zhao2023pytorch,
  title={Pytorch FSDP: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}

@inproceedings{swintransformer,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{wmt,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}

@inproceedings{vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6836--6846},
  year={2021}
}


@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{huang-etal-2020-improve,
    title = "Improve Transformer Models with Better Relative Position Embeddings",
    author = "Huang, Zhiheng  and
      Liang, Davis  and
      Xu, Peng  and
      Xiang, Bing",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.298",
    doi = "10.18653/v1/2020.findings-emnlp.298",
    pages = "3327--3335",
    abstract = "The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.",
}

@inproceedings{mao-2022-fine,
    title = "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
    author = "Mao, Huanru Henry",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.697",
    pages = "10236--10242",
    abstract = "Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O(1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative - decaying fast weights - that runs fast on GPU, outperforms prior methods, and retains 99{\%} of attention{'}s performance for GPT-2. We also show competitive performance on WikiText-103 against more complex attention substitutes.",
}


@inproceedings{
    zhen2022cosformer,
    title={cosFormer: Rethinking Softmax In Attention},
    author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=Bl8CQrx2Up4}
}

@book{Advanced.Algebra,
    title={Advanced Algebra},
    author={Musheng Yao and Advanced Algebra},
    year={2015},
    publisher={Fudan University Press}
}

@article{chen2021permuteformer,
  title={Permuteformer: Efficient relative position encoding for long sequences},
  author={Chen, Peng},
  journal={arXiv preprint arXiv:2109.02377},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{
    choromanski2021rethinking,
    title={Rethinking Attention with Performers},
    author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}

@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  howpublished={\url{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}},
  year={2018}
}

@misc{
    liu2020roberta,
    title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2020},
    url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}

@book{bracewell1986fourier,
  title={The Fourier transform and its applications},
  author={Bracewell, Ronald Newbold and Bracewell, Ronald N},
  volume={31999},
  year={1986},
  publisher={McGraw-hill New York}
}

@article{hua2022transformer,
  title={Transformer Quality in Linear Time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc V},
  journal={arXiv preprint arXiv:2202.10447},
  year={2022}
}

@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{merity2017pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={5th International Conference on Learning Representations, {ICLR},
               Toulon, France},
  year={2017}
}

@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}


@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{liutkus2021relative,
  title={Relative positional encoding for transformers with linear complexity},
  author={Liutkus, Antoine and Cífka, Ondřej and Wu, Shih-Lun and Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael},
  booktitle={International Conference on Machine Learning},
  pages={7067--7079},
  year={2021},
  organization={PMLR}
}

@article{horn2021translational,
  title={Translational equivariance in kernelizable attention},
  author={Horn, Max and Shridhar, Kumar and Groenewald, Elrich and Baumann, Philipp FM},
  journal={arXiv preprint arXiv:2102.07680},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{
    dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{gulati20_interspeech,
  author={Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
  title={{Conformer: Convolution-augmented Transformer for Speech Recognition}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={5036--5040},
  doi={10.21437/Interspeech.2020-3015}
}
@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}


@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@inproceedings{
    ke2021rethinking,
    title={Rethinking Positional Encoding in Language Pre-training},
    author={Guolin Ke and Di He and Tie-Yan Liu},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=09-528y2Fgf}
}

@article{raffel2019exploring, 
    title={Exploring the limits of transfer learning with a unified text-to-text transformer}, 
    author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J}, 
    journal={arXiv preprint arXiv:1910.10683}, 
    year={2019} 
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press}
}

@article{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@article{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21665--21674},
  year={2020}
}

@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={AAAI},
  year={2021}
}

@article{islam2020much,
  title={How much position information do convolutional neural networks encode?},
  author={Islam, Md Amirul and Jia, Sen and Bruce, Neil DB},
  journal={arXiv preprint arXiv:2001.08248},
  year={2020}
}

@inproceedings{metaformer,
  author       = {Weihao Yu and
                  Mi Luo and
                  Pan Zhou and
                  Chenyang Si and
                  Yichen Zhou and
                  Xinchao Wang and
                  Jiashi Feng and
                  Shuicheng Yan},
  title        = {MetaFormer is Actually What You Need for Vision},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages        = {10809--10819},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/CVPR52688.2022.01055},
  doi          = {10.1109/CVPR52688.2022.01055},
  timestamp    = {Wed, 05 Oct 2022 16:31:19 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/YuLZSZWFY22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lru,
  author       = {Antonio Orvieto and
                  Samuel L. Smith and
                  Albert Gu and
                  Anushan Fernando and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  Razvan Pascanu and
                  Soham De},
  title        = {Resurrecting Recurrent Neural Networks for Long Sequences},
  journal      = {CoRR},
  volume       = {abs/2303.06349},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.06349},
  doi          = {10.48550/arXiv.2303.06349},
  eprinttype    = {arXiv},
  eprint       = {2303.06349},
  timestamp    = {Thu, 16 Mar 2023 16:04:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-06349.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{iclr18,
  author       = {Eric Martin and
                  Chris Cundy},
  title        = {Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=HyUNwulC-},
  timestamp    = {Thu, 25 Jul 2019 14:25:41 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MartinC18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{s4,
  author       = {Albert Gu and
                  Karan Goel and
                  Christopher R{\'{e}}},
  title        = {Efficiently Modeling Long Sequences with Structured State Spaces},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=uYLFoz1vlAC},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/GuGR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{unified,
  author       = {Hongyu He and
                  Marko Kabic},
  title        = {A Unified View of Long-Sequence Models towards Modeling Million-Scale
                  Dependencies},
  journal      = {CoRR},
  volume       = {abs/2302.06218},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.06218},
  doi          = {10.48550/arXiv.2302.06218},
  eprinttype    = {arXiv},
  eprint       = {2302.06218},
  timestamp    = {Mon, 20 Feb 2023 14:27:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-06218.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hyena,
  author       = {Michael Poli and
                  Stefano Massaroli and
                  Eric Nguyen and
                  Daniel Y. Fu and
                  Tri Dao and
                  Stephen Baccus and
                  Yoshua Bengio and
                  Stefano Ermon and
                  Christopher R{\'{e}}},
  title        = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.10866},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.10866},
  doi          = {10.48550/arXiv.2302.10866},
  eprinttype    = {arXiv},
  eprint       = {2302.10866},
  timestamp    = {Fri, 24 Feb 2023 11:55:23 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-10866.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{h3,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Khaled Kamal Saab and
                  Armin W. Thomas and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  journal      = {CoRR},
  volume       = {abs/2212.14052},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.14052},
  doi          = {10.48550/arXiv.2212.14052},
  eprinttype    = {arXiv},
  eprint       = {2212.14052},
  timestamp    = {Sun, 08 Jan 2023 14:16:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-14052.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{glu,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu-improving,
  author       = {Albert Gu and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  Thomas Paine and
                  Matt Hoffman and
                  Razvan Pascanu},
  title        = {Improving the Gating Mechanism of Recurrent Neural Networks},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {3800--3809},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/gu20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:18 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/GuGP0P20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}


@inproceedings{s4d,
  author       = {Albert Gu and
                  Karan Goel and
                  Ankit Gupta and
                  Christopher R{\'{e}}},
  title        = {On the Parameterization and Initialization of Diagonal State Space
                  Models},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html},
  timestamp    = {Fri, 05 May 2023 16:00:57 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/GuG0R22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Elfwing2017SigmoidWeightedLU,
  title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2017},
  volume={107},
  pages={
          3-11
        }
}

@article{Zhou2016MinimalGU,
  title={Minimal gated unit for recurrent neural networks},
  author={Guoxiang Zhou and Jianxin Wu and Chen-Lin Zhang and Zhi-Hua Zhou},
  journal={International Journal of Automation and Computing},
  year={2016},
  volume={13},
  pages={226-234}
}
@article{Greff2015LSTMAS,
  title={LSTM: A Search Space Odyssey},
  author={Klaus Greff and Rupesh Kumar Srivastava and Jan Koutn{\'i}k and Bas R. Steunebrink and J{\"u}rgen Schmidhuber},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2015},
  volume={28},
  pages={2222-2232}
}

@inproceedings{lei-etal-2018-simple,
    title = "Simple Recurrent Units for Highly Parallelizable Recurrence",
    author = "Lei, Tao  and
      Zhang, Yu  and
      Wang, Sida I.  and
      Dai, Hui  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1477",
    doi = "10.18653/v1/D18-1477",
    pages = "4470--4481",
    abstract = "Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5{---}9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation by incorporating SRU into the architecture.",
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@misc{wang2020linformer,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{s4aslinearrnn,
  author       = {Ankit Gupta and
                  Harsh Mehta and
                  Jonathan Berant},
  title        = {Simplifying and Understanding State Space Models with Diagonal Linear
                  RNNs},
  journal      = {CoRR},
  volume       = {abs/2212.00768},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.00768},
  doi          = {10.48550/arXiv.2212.00768},
  eprinttype    = {arXiv},
  eprint       = {2212.00768},
  timestamp    = {Thu, 08 Dec 2022 15:26:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-00768.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{s44mt,
  author       = {Ali Vardasbi and
                  Telmo Pessoa Pires and
                  Robin M. Schmidt and
                  Stephan Peitz},
  title        = {State Spaces Aren't Enough: Machine Translation Needs Attention},
  journal      = {CoRR},
  volume       = {abs/2304.12776},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.12776},
  doi          = {10.48550/arXiv.2304.12776},
  eprinttype    = {arXiv},
  eprint       = {2304.12776},
  timestamp    = {Wed, 03 May 2023 14:12:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-12776.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{peng2023rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Xiangru Tang and Bolun Wang and Johan S. Wind and Stansilaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{lra,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Samira Abnar and
                  Yikang Shen and
                  Dara Bahri and
                  Philip Pham and
                  Jinfeng Rao and
                  Liu Yang and
                  Sebastian Ruder and
                  Donald Metzler},
  title        = {Long Range Arena : {A} Benchmark for Efficient Transformers},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=qVyeW-grC2k},
  timestamp    = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Tay0ASBPRYRM21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tcn,
  author       = {Colin Lea and
                  Michael D. Flynn and
                  Ren{\'{e}} Vidal and
                  Austin Reiter and
                  Gregory D. Hager},
  title        = {Temporal Convolutional Networks for Action Segmentation and Detection},
  booktitle    = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages        = {1003--1012},
  publisher    = {{IEEE} Computer Society},
  year         = {2017},
  url          = {https://doi.org/10.1109/CVPR.2017.113},
  doi          = {10.1109/CVPR.2017.113},
  timestamp    = {Fri, 24 Mar 2023 00:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/LeaFVRH17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wavenet,
  author       = {A{\"{a}}ron van den Oord and
                  Sander Dieleman and
                  Heiga Zen and
                  Karen Simonyan and
                  Oriol Vinyals and
                  Alex Graves and
                  Nal Kalchbrenner and
                  Andrew W. Senior and
                  Koray Kavukcuoglu},
  title        = {WaveNet: {A} Generative Model for Raw Audio},
  booktitle    = {The 9th {ISCA} Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15
                  September 2016},
  pages        = {125},
  publisher    = {{ISCA}},
  year         = {2016},
  url          = {http://www.isca-speech.org/archive/SSW\_2016/abstracts/ssw9\_DS-4\_van\_den\_Oord.html},
  timestamp    = {Tue, 16 Nov 2021 11:36:20 +0100},
  biburl       = {https://dblp.org/rec/conf/ssw/OordDZSVGKSK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{qrnn,
  author       = {James Bradbury and
                  Stephen Merity and
                  Caiming Xiong and
                  Richard Socher},
  title        = {Quasi-Recurrent Neural Networks},
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2017},
  url          = {https://openreview.net/forum?id=H1zJ-v5xl},
  timestamp    = {Thu, 25 Jul 2019 14:25:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/0002MXS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{strongtypedrnn,
  author       = {David Balduzzi and
                  Muhammad Ghifary},
  editor       = {Maria{-}Florina Balcan and
                  Kilian Q. Weinberger},
  title        = {Strongly-Typed Recurrent Neural Networks},
  booktitle    = {Proceedings of the 33nd International Conference on Machine Learning,
                  {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  series       = {{JMLR} Workshop and Conference Proceedings},
  volume       = {48},
  pages        = {1292--1300},
  publisher    = {JMLR.org},
  year         = {2016},
  url          = {http://proceedings.mlr.press/v48/balduzzi16.html},
  timestamp    = {Wed, 29 May 2019 08:41:46 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/BalduzziG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mega,
  author       = {Xuezhe Ma and
                  Chunting Zhou and
                  Xiang Kong and
                  Junxian He and
                  Liangke Gui and
                  Graham Neubig and
                  Jonathan May and
                  Luke Zettlemoyer},
  title        = {Mega: Moving Average Equipped Gated Attention},
  journal      = {CoRR},
  volume       = {abs/2209.10655},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2209.10655},
  doi          = {10.48550/arXiv.2209.10655},
  eprinttype    = {arXiv},
  eprint       = {2209.10655},
  timestamp    = {Wed, 28 Sep 2022 15:17:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2209-10655.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{pretrainingwoattn,
  author       = {Junxiong Wang and
                  Jing Nathan Yan and
                  Albert Gu and
                  Alexander M. Rush},
  title        = {Pretraining Without Attention},
  journal      = {CoRR},
  volume       = {abs/2212.10544},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.10544},
  doi          = {10.48550/arXiv.2212.10544},
  eprinttype    = {arXiv},
  eprint       = {2212.10544},
  timestamp    = {Wed, 04 Jan 2023 16:01:37 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-10544.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zaheer2020big,
  title={Big Bird: Transformers for Longer Sequences.},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{gss,
  author       = {Harsh Mehta and
                  Ankit Gupta and
                  Ashok Cutkosky and
                  Behnam Neyshabur},
  title        = {Long Range Language Modeling via Gated State Spaces},
  journal      = {CoRR},
  volume       = {abs/2206.13947},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2206.13947},
  doi          = {10.48550/arXiv.2206.13947},
  eprinttype    = {arXiv},
  eprint       = {2206.13947},
  timestamp    = {Fri, 09 Dec 2022 09:06:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2206-13947.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{gupta2022DSS,
  author       = {Ankit Gupta and
                  Albert Gu and
                  Jonathan Berant},
  title        = {Diagonal State Spaces are as Effective as Structured State Spaces},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html},
  timestamp    = {Fri, 05 May 2023 16:00:57 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/0001GB22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Choromanski2020RethinkingAW,
  title={Rethinking Attention with Performers},
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tam{\'a}s Sarl{\'o}s and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy J. Colwell and Adrian Weller},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.14794}
}

@article{Li2022WhatMC,
  title={What Makes Convolutional Models Great on Long Sequence Modeling?},
  author={Yuhong Li and Tianle Cai and Yi Zhang and De-huai Chen and Debadeepta Dey},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.09298}
}

@inproceedings{Schlag2021LinearTA,
  title={Linear Transformers Are Secretly Fast Weight Programmers},
  author={Imanol Schlag and Kazuki Irie and J{\"u}rgen Schmidhuber},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{Schmidhuber1992LearningTC,
  title={Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks},
  author={J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1992},
  volume={4},
  pages={131-139}
}

@inproceedings{rfa,
  author       = {Hao Peng and
                  Nikolaos Pappas and
                  Dani Yogatama and
                  Roy Schwartz and
                  Noah A. Smith and
                  Lingpeng Kong},
  title        = {Random Feature Attention},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=QtTKTdVrFBB},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Peng0Y0SK21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ckconv,
  author       = {David W. Romero and
                  Anna Kuzina and
                  Erik J. Bekkers and
                  Jakub Mikolaj Tomczak and
                  Mark Hoogendoorn},
  title        = {CKConv: Continuous Kernel Convolution For Sequential Data},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=8FhxBtXSl0},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/RomeroKBTH22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
qin2023toeplitz,
title={Toeplitz Neural Network for Sequence Modeling},
author={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=IxmWsm4xrua}
}

@inproceedings{swintransformer,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{wmt,
  title={Findings of the 2014 workshop on statistical machine translation},
  author={Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and others},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={12--58},
  year={2014}
}




@inproceedings{huang-etal-2020-improve,
    title = "Improve Transformer Models with Better Relative Position Embeddings",
    author = "Huang, Zhiheng  and
      Liang, Davis  and
      Xu, Peng  and
      Xiang, Bing",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.298",
    doi = "10.18653/v1/2020.findings-emnlp.298",
    pages = "3327--3335",
    abstract = "The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.",
}



@book{Advanced.Algebra,
    title={Advanced Algebra},
    author={Musheng Yao and Advanced Algebra},
    year={2015},
    publisher={Fudan University Press}
}

@article{chen2021permuteformer,
  title={Permuteformer: Efficient relative position encoding for long sequences},
  author={Chen, Peng},
  journal={arXiv preprint arXiv:2109.02377},
  year={2021}
}





@article{s5,
  author       = {Jimmy T. H. Smith and
                  Andrew Warrington and
                  Scott W. Linderman},
  title        = {Simplified State Space Layers for Sequence Modeling},
  journal      = {CoRR},
  volume       = {abs/2208.04933},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2208.04933},
  doi          = {10.48550/arXiv.2208.04933},
  eprinttype    = {arXiv},
  eprint       = {2208.04933},
  timestamp    = {Tue, 16 Aug 2022 16:44:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2208-04933.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}


@misc{
    liu2020roberta,
    title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2020},
    url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}

@book{bracewell1986fourier,
  title={The Fourier transform and its applications},
  author={Bracewell, Ronald Newbold and Bracewell, Ronald N},
  volume={31999},
  year={1986},
  publisher={McGraw-hill New York}
}



@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}



@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}


@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{liutkus2021relative,
  title={Relative positional encoding for transformers with linear complexity},
  author={Liutkus, Antoine and Cífka, Ondřej and Wu, Shih-Lun and Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael},
  booktitle={International Conference on Machine Learning},
  pages={7067--7079},
  year={2021},
  organization={PMLR}
}

@article{horn2021translational,
  title={Translational equivariance in kernelizable attention},
  author={Horn, Max and Shridhar, Kumar and Groenewald, Elrich and Baumann, Philipp FM},
  journal={arXiv preprint arXiv:2102.07680},
  year={2021}
}


@inproceedings{
    dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{gulati20_interspeech,
  author={Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
  title={{Conformer: Convolution-augmented Transformer for Speech Recognition}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={5036--5040},
  doi={10.21437/Interspeech.2020-3015}
}

@inproceedings{
    peng2021random,
    title={Random Feature Attention},
    author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=QtTKTdVrFBB}
}



@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}



@article{raffel2019exploring, 
    title={Exploring the limits of transfer learning with a unified text-to-text transformer}, 
    author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J}, 
    journal={arXiv preprint arXiv:1910.10683}, 
    year={2019} 
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}



@inproceedings{tay2020sparse,
  title={Sparse sinkhorn attention},
  author={Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle={International Conference on Machine Learning},
  pages={9438--9447},
  year={2020},
  organization={PMLR}
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press}
}

@article{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21665--21674},
  year={2020}
}


@article{islam2020much,
  title={How much position information do convolutional neural networks encode?},
  author={Islam, Md Amirul and Jia, Sen and Bruce, Neil DB},
  journal={arXiv preprint arXiv:2001.08248},
  year={2020}
}

@inproceedings{
zhu2021longshort,
title={Long-Short Transformer: Efficient Transformers for Language and Vision},
author={Chen Zhu and Wei Ping and Chaowei Xiao and Mohammad Shoeybi and Tom Goldstein and Anima Anandkumar and Bryan Catanzaro},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=M_lkFOwVdYc}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{liu2021pay, title={Pay attention to mlps}, author={Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V}, journal={Advances in Neural Information Processing Systems}, volume={34}, pages={9204--9215}, year={2021} }

@inproceedings{tay2021synthesizer,
  title={Synthesizer: Rethinking self-attention for transformer models},
  author={Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
  booktitle={International conference on machine learning},
  pages={10183--10192},
  year={2021},
  organization={PMLR}
}

@article{simplelongconv,
  author       = {Daniel Y. Fu and
                  Elliot L. Epstein and
                  Eric Nguyen and
                  Armin W. Thomas and
                  Michael Zhang and
                  Tri Dao and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Simple Hardware-Efficient Long Convolutions for Sequence Modeling},
  journal      = {CoRR},
  volume       = {abs/2302.06646},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.06646},
  doi          = {10.48550/arXiv.2302.06646},
  eprinttype    = {arXiv},
  eprint       = {2302.06646},
  timestamp    = {Mon, 20 Feb 2023 14:27:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-06646.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{onlstm,
  author       = {Yikang Shen and
                  Shawn Tan and
                  Alessandro Sordoni and
                  Aaron C. Courville},
  title        = {Ordered Neurons: Integrating Tree Structures into Recurrent Neural
                  Networks},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=B1l6qiR5F7},
  timestamp    = {Thu, 25 Jul 2019 13:03:16 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ShenTSC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{rae-razavi-2020-transformers,
    title = "Do Transformers Need Deep Long-Range Memory?",
    author = "Rae, Jack  and
      Razavi, Ali",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.672",
    doi = "10.18653/v1/2020.acl-main.672",
    pages = "7524--7529",
    abstract = "Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL {---} a Transformer augmented with a long-range memory of past activations {---} has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.",
}


@inproceedings{swin,
  author       = {Ze Liu and
                  Yutong Lin and
                  Yue Cao and
                  Han Hu and
                  Yixuan Wei and
                  Zheng Zhang and
                  Stephen Lin and
                  Baining Guo},
  title        = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  booktitle    = {2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
                  2021, Montreal, QC, Canada, October 10-17, 2021},
  pages        = {9992--10002},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICCV48922.2021.00986},
  doi          = {10.1109/ICCV48922.2021.00986},
  timestamp    = {Thu, 19 May 2022 16:00:58 +0200},
  biburl       = {https://dblp.org/rec/conf/iccv/LiuL00W0LG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lee-thorp-etal-2022-fnet,
    title = "{FN}et: Mixing Tokens with {F}ourier Transforms",
    author = "Lee-Thorp, James  and
      Ainslie, Joshua  and
      Eckstein, Ilya  and
      Ontanon, Santiago",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.319",
    doi = "10.18653/v1/2022.naacl-main.319",
    pages = "4296--4313",
    abstract = "We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that {``}mix{''} input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97{\%} of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80{\%} faster on GPUs and 70{\%} faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the {``}efficient Transformers{''} on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.",
}

@inproceedings{rao2021global,
  title={Global Filter Networks for Image Classification},
  author={Rao, Yongming and Zhao, Wenliang and Zhu, Zheng and Lu, Jiwen and Zhou, Jie},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}

@inproceedings{guibas2021efficient,
  title={Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators},
  author={Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{qin-etal-2022-devil,
    title = "The Devil in Linear Transformer",
    author = "Qin, Zhen  and
      Han, Xiaodong  and
      Sun, Weixuan  and
      Li, Dongxu  and
      Kong, Lingpeng  and
      Barnes, Nick  and
      Zhong, Yiran",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.473",
    pages = "7025--7041",
    abstract = "Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .",
}
Creative Commons License


@inproceedings{gu2022efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}

@misc{dss,
Author = {Ankit Gupta and Albert Gu and Jonathan Berant},
Title = {Diagonal State Spaces are as Effective as Structured State Spaces},
Year = {2022},
Eprint = {arXiv:2203.14343},
}

@inproceedings{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@misc{qin2023linearized,
      title={Linearized Relative Positional Encoding}, 
      author={Zhen Qin and Weixuan Sun and Kaiyue Lu and Hui Deng and Dongxu Li and Xiaodong Han and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
      year={2023},
      eprint={2307.09270},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Skyformer,
    title={Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\"om Method}, 
    author={Yifan Chen and 
            Qi Zeng and 
            Heng Ji and 
            Yun Yang},
    booktitle={Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
    year={2021}
}

@inproceedings{DBLP:conf/iclr/LiHEL21,
  author       = {Zhong Li and
                  Jiequn Han and
                  Weinan E and
                  Qianxiao Li},
  title        = {On the Curse of Memory in Recurrent Neural Networks: Approximation
                  and Optimization Analysis},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=8Sqhl-nF50},
  timestamp    = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LiHEL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{indrnn,
  author       = {Shuai Li and
                  Wanqing Li and
                  Chris Cook and
                  Ce Zhu and
                  Yanbo Gao},
  title        = {Independently Recurrent Neural Network (IndRNN): Building a Longer
                  and Deeper {RNN}},
  booktitle    = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages        = {5457--5466},
  publisher    = {Computer Vision Foundation / {IEEE} Computer Society},
  year         = {2018},
  url          = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Li\_Independently\_Recurrent\_Neural\_CVPR\_2018\_paper.html},
  doi          = {10.1109/CVPR.2018.00572},
  timestamp    = {Fri, 24 Mar 2023 00:02:54 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/0005LCZG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
huang2023encoding,
title={Encoding Recurrence into Transformers},
author={Feiqing Huang and Kexin Lu and Yuxi CAI and Zhen Qin and Yanwen Fang and Guangjian Tian and Guodong Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=7YfHla7IxBJ}
}

@inproceedings{linearxfmrsparallelscan,
  author       = {Valerii Likhosherstov and
                  Krzysztof Marcin Choromanski and
                  Jared Quincy Davis and
                  Xingyou Song and
                  Adrian Weller},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Sub-Linear Memory: How to Make Performers SLiM},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {6707--6719},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/35309226eb45ec366ca86a4329a2b7c3-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:47 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/LikhosherstovCD21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xfmrsarernns,
  author       = {Angelos Katharopoulos and
                  Apoorv Vyas and
                  Nikolaos Pappas and
                  Fran{\c{c}}ois Fleuret},
  title        = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
                  Attention},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {5156--5165},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/katharopoulos20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/KatharopoulosV020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v139-touvron21a,
  title =     {Training data-efficient image transformers \&amp; distillation through attention},
  author =    {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {International Conference on Machine Learning},
  pages =     {10347--10357},
  year =      {2021},
  volume =    {139},
  month =     {July}
}

@inproceedings{DBLP:conf/iclr/MahtoVTH21,
  author       = {Shivangi Mahto and
                  Vy Ai Vo and
                  Javier S. Turek and
                  Alexander Huth},
  title        = {Multi-timescale Representation Learning in {LSTM} Language Models},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=9ITXiTrAoT},
  timestamp    = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MahtoVTH21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/NeilPL16,
  author       = {Daniel Neil and
                  Michael Pfeiffer and
                  Shih{-}Chii Liu},
  editor       = {Daniel D. Lee and
                  Masashi Sugiyama and
                  Ulrike von Luxburg and
                  Isabelle Guyon and
                  Roman Garnett},
  title        = {Phased {LSTM:} Accelerating Recurrent Network Training for Long or
                  Event-based Sequences},
  booktitle    = {Advances in Neural Information Processing Systems 29: Annual Conference
                  on Neural Information Processing Systems 2016, December 5-10, 2016,
                  Barcelona, Spain},
  pages        = {3882--3890},
  year         = {2016},
  url          = {https://proceedings.neurips.cc/paper/2016/hash/5bce843dd76db8c939d5323dd3e54ec9-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/NeilPL16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/TallecO18a,
  author       = {Corentin Tallec and
                  Yann Ollivier},
  title        = {Can recurrent neural networks warp time?},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=SJcKhk-Ab},
  timestamp    = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/TallecO18a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@article{forgetgate,
  author       = {Jos van der Westhuizen and
                  Joan Lasenby},
  title        = {The unreasonable effectiveness of the forget gate},
  journal      = {CoRR},
  volume       = {abs/1804.04849},
  year         = {2018},
  url          = {http://arxiv.org/abs/1804.04849},
  eprinttype    = {arXiv},
  eprint       = {1804.04849},
  timestamp    = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1804-04849.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gray2006toeplitz,
  title={Toeplitz and circulant matrices: A review},
  author={Gray, Robert M and others},
  journal={Foundations and Trends{\textregistered} in Communications and Information Theory},
  volume={2},
  number={3},
  pages={155--239},
  year={2006},
  publisher={Now Publishers, Inc.}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{yu2022metaformer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10819--10829},
  year={2022}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{
alibi,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}


@article{titsias2016one,
  title={One-vs-each approximation to softmax for scalable estimation of probabilities},
  author={Titsias, Michalis K},
  journal={arXiv preprint arXiv:1609.07410},
  year={2016}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@article{gao2017properties,
  title={On the properties of the softmax function with application in game theory and reinforcement learning},
  author={Gao, Bolin and Pavel, Lacra},
  journal={arXiv preprint arXiv:1704.00805},
  year={2017}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@article{Laurent2016ARN,
  title={A recurrent neural network without chaos},
  author={Thomas Laurent and James H. von Brecht},
  journal={ArXiv},
  year={2016},
  volume={abs/1612.06212}
}

@article{minimalrnn,
  author       = {Minmin Chen},
  title        = {MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural
                  Networks},
  journal      = {CoRR},
  volume       = {abs/1711.06788},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.06788},
  eprinttype    = {arXiv},
  eprint       = {1711.06788},
  timestamp    = {Mon, 13 Aug 2018 16:46:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-06788.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{mildenhall2021nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{mehta2022long,
  title={Long range language modeling via gated state spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2206.13947},
  year={2022}
}



@inproceedings{salman2015weather,
  title={Weather forecasting using deep learning techniques},
  author={Salman, Afan Galih and Kanigoro, Bayu and Heryadi, Yaya},
  booktitle={2015 international conference on advanced computer science and information systems (ICACSIS)},
  pages={281--285},
  year={2015},
  organization={Ieee}
}

@inproceedings{selvin2017stock,
  title={Stock price prediction using LSTM, RNN and CNN-sliding window model},
  author={Selvin, Sreelekshmy and Vinayakumar, R and Gopalakrishnan, EA and Menon, Vijay Krishna and Soman, KP},
  booktitle={2017 international conference on advances in computing, communications and informatics (icacci)},
  pages={1643--1647},
  year={2017},
  organization={IEEE}
}

@inproceedings{miao2015eesen,
  title={EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding},
  author={Miao, Yajie and Gowayyed, Mohammad and Metze, Florian},
  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  pages={167--174},
  year={2015},
  organization={IEEE}
}

@inproceedings{
martin2018parallelizing,
title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
author={Eric Martin and Chris Cundy},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HyUNwulC-},
}

@inproceedings{gong21b_interspeech,
  author={Yuan Gong and Yu-An Chung and James Glass},
  title={{AST: Audio Spectrogram Transformer}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={571--575},
  doi={10.21437/Interspeech.2021-698}
}

@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={arXiv preprint arXiv:2104.11178},
  year={2021}
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{du2022glm,
      title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling}, 
      author={Zhengxiao Du and Yujie Qian and Xiao Liu and Ming Ding and Jiezhong Qiu and Zhilin Yang and Jie Tang},
      year={2022},
      eprint={2103.10360},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{taylor2022galactica,
      title={Galactica: A Large Language Model for Science}, 
      author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
      year={2022},
      eprint={2211.09085},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{workshop2023bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
      year={2023},
      eprint={2211.05100},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{penedo2023refinedweb,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      eprint={2306.01116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{zheng2022linear,
  title={Linear complexity randomized self-attention mechanism},
  author={Lin Zheng and Chong Wang and Lingpeng Kong},
  booktitle={International Conference on Machine Learning},
  pages={27011--27041},
  year={2022},
  organization={PMLR}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Tillet2019TritonAI,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Philippe Tillet and Hsiang-Tsung Kung and David D. Cox},
  journal={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  year={2019}
}

@inproceedings{zheng2023efficient,
  title={Efficient Attention via Control Variates},
  author={Lin Zheng and Jianbo Yuan and Chong Wang and Lingpeng Kong},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=G-uNfHKrj46}
}

@misc{1904.10509,
Author = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
Title = {Generating Long Sequences with Sparse Transformers},
Year = {2019},
Eprint = {arXiv:1904.10509},
}

@misc{2008.07669,
Author = {Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
Title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
Year = {2020},
Eprint = {arXiv:2008.07669},
}

@misc{2307.10156,
Author = {Zhen Qin and Yiran Zhong and Hui Deng},
Title = {Exploring Transformer Extrapolation},
Year = {2023},
Eprint = {arXiv:2307.10156},
}

@misc{2307.09270,
Author = {Zhen Qin and Weixuan Sun and Kaiyue Lu and Hui Deng and Dongxu Li and Xiaodong Han and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
Title = {Linearized Relative Positional Encoding},
Year = {2023},
Eprint = {arXiv:2307.09270},
}

@article{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  year={2023}
}

@article{liu2022neural,
  title={Neural architecture search on efficient transformers and beyond},
  author={Liu, Zexiang and Li, Dong and Lu, Kaiyue and Qin, Zhen and Sun, Weixuan and Xu, Jiacheng and Zhong, Yiran},
  journal={arXiv preprint arXiv:2207.13955},
  year={2022}
}