\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{beltagy2020longformer}
Iz Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer, 2020.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
  O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai
  Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der
  Wal.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling, 2023.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{1904.10509}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers, 2019.

\bibitem{choromanski2021rethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J Colwell, and Adrian
  Weller.
\newblock Rethinking attention with performers.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{2204.02311}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem{dao2023flashattention2}
Tri Dao.
\newblock Flash{A}ttention-2: Faster attention with better parallelism and work
  partitioning.
\newblock 2023.

\bibitem{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with
  {IO}-awareness.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{h3}
Tri Dao, Daniel~Y. Fu, Khaled~Kamal Saab, Armin~W. Thomas, Atri Rudra, and
  Christopher R{\'{e}}.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock {\em CoRR}, abs/2212.14052, 2022.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling, 2022.

\bibitem{simplelongconv}
Daniel~Y. Fu, Elliot~L. Epstein, Eric Nguyen, Armin~W. Thomas, Michael Zhang,
  Tri Dao, Atri Rudra, and Christopher R{\'{e}}.
\newblock Simple hardware-efficient long convolutions for sequence modeling.
\newblock {\em CoRR}, abs/2302.06646, 2023.

\bibitem{2008.07669}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re.
\newblock Hippo: Recurrent memory with optimal polynomial projections, 2020.

\bibitem{s4d}
Albert Gu, Karan Goel, Ankit Gupta, and Christopher R{\'{e}}.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock In {\em NeurIPS}, 2022.

\bibitem{s4}
Albert Gu, Karan Goel, and Christopher R{\'{e}}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In {\em The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.

\bibitem{gupta2022DSS}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In {\em NeurIPS}, 2022.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de Las~Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den
  Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
  Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models, 2022.

\bibitem{hua2022transformer}
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc~V Le.
\newblock Transformer quality in linear time.
\newblock {\em arXiv preprint arXiv:2202.10447}, 2022.

\bibitem{kalamkar2019study}
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal
  Banerjee, Sasikanth Avancha, Dharma~Teja Vooturi, Nataraj Jammalamadaka,
  Jianyu Huang, Hector Yuen, et~al.
\newblock A study of bfloat16 for deep learning training.
\newblock {\em arXiv preprint arXiv:1905.12322}, 2019.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.

\bibitem{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem{ke2021rethinking}
Guolin Ke, Di He, and Tie-Yan Liu.
\newblock Rethinking positional encoding in language pre-training.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.

\bibitem{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension, 2019.

\bibitem{liu2022neural}
Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran
  Zhong.
\newblock Neural architecture search on efficient transformers and beyond.
\newblock {\em arXiv preprint arXiv:2207.13955}, 2022.

\bibitem{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, et~al.
\newblock Mixed precision training.
\newblock {\em arXiv preprint arXiv:1710.03740}, 2017.

\bibitem{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  8024--8035. Curran Associates, Inc., 2019.

\bibitem{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
  Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
  and Julien Launay.
\newblock The refinedweb dataset for falcon llm: Outperforming curated corpora
  with web data, and web data only, 2023.

\bibitem{peng2023rwkv}
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi
  Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, Xuzheng He,
  Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra,
  Hayden Lau, Krishna Sri~Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru
  Tang, Bolun Wang, Johan~S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan
  Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.
\newblock Rwkv: Reinventing rnns for the transformer era, 2023.

\bibitem{alibi}
Ofir Press, Noah Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input
  length extrapolation.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{qin2023toeplitz}
Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai,
  Lingpeng Kong, and Yiran Zhong.
\newblock Toeplitz neural network for sequence modeling.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{qin-etal-2022-devil}
Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and
  Yiran Zhong.
\newblock The devil in linear transformer.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 7025--7041, Abu Dhabi, United Arab
  Emirates, Dec. 2022. Association for Computational Linguistics.

\bibitem{zhen2022cosformer}
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie
  Yan, Lingpeng Kong, and Yiran Zhong.
\newblock cosformer: Rethinking softmax in attention.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{qin2023linearized}
Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao
  Dai, Lingpeng Kong, and Yiran Zhong.
\newblock Linearized relative positional encoding, 2023.

\bibitem{2307.10156}
Zhen Qin, Yiran Zhong, and Hui Deng.
\newblock Exploring transformer extrapolation, 2023.

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock
  \url{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf},
  2018.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rae2022scaling}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
  Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
  Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de
  Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,
  Aidan Clark, Diego de Las~Casas, Aurelia Guy, Chris Jones, James Bradbury,
  Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William
  Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals,
  Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray
  Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher, 2022.

\bibitem{2210.15424}
Teven~Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman,
  M~Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason
  Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika,
  Jaesung Tae, Zheng~Xin Yong, Julien Launay, and Iz Beltagy.
\newblock What language model to train if you have one million gpu hours?,
  2022.

\bibitem{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock {\em arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{taylor2022galactica}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
  Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
\newblock Galactica: A large language model for science, 2022.

\bibitem{Tillet2019TritonAI}
Philippe Tillet, Hsiang-Tsung Kung, and David~D. Cox.
\newblock Triton: an intermediate language and compiler for tiled neural
  network computations.
\newblock {\em Proceedings of the 3rd ACM SIGPLAN International Workshop on
  Machine Learning and Programming Languages}, 2019.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{workshop2023bloom}
BigScience Workshop, :, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie
  Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra~Sasha
  Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander~M. Rush,
  Stella Biderman, Albert Webson, Pawan~Sasanka Ammanamanchi, Thomas Wang,
  Benoît Sagot, Niklas Muennighoff, Albert~Villanova del Moral, Olatunji
  Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu
  Nguyen, Lucile Saulnier, Samson Tan, Pedro~Ortiz Suarez, Victor Sanh, Hugo
  Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel,
  Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham~Fikri Aji, Amit Alfassy, Anna
  Rogers, Ariel~Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue,
  Christopher Klamm, Colin Leong, Daniel van Strien, David~Ifeoluwa Adelani,
  Dragomir Radev, Eduardo~González Ponferrada, Efrat Levkovizh, Ethan Kim,
  Eyal~Bar Natan, Francesco~De Toni, Gérard Dupont, Germán Kruszewski, Giada
  Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin,
  Isaac Johnson, Itziar Gonzalez-Dios, Javier de~la Rosa, Jenny Chim, Jesse
  Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep
  Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro~Von Werra, Leon
  Weber, Long Phan, Loubna~Ben allal, Ludovic Tanguy, Manan Dey, Manuel~Romero
  Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin
  Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh~Chien Vu, Mohammad~A.
  Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis,
  Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson,
  Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi
  Bommasani, Roberto~Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,
  Sebastian Nagel, Shamik Bose, Shamsuddeen~Hassan Muhammad, Shanya Sharma,
  Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney
  Zink, Tiago~Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev,
  Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid
  Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
  Davut~Emre Taşar, Elizabeth Salesky, Sabrina~J. Mielke, Wilson~Y. Lee,
  Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti
  Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik
  Strobelt, Jason~Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M~Saiful
  Bari, Maged~S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel
  Albanie, Sheng Shen, Srulik Ben-David, Stephen~H. Bach, Taewoon Kim, Tali
  Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru
  Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh,
  Adam Roberts, Hyung~Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong
  Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max
  Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette,
  Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre
  Cornette, Pierre~François Lavallée, Rémi Lacroix, Samyam Rajbhandari,
  Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers,
  Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun
  Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak
  Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli
  Bogdanov, Genta~Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo,
  Jekaterina Novikova, Jessica~Zosa Forde, Jordan Clive, Jungo Kasai, Ken
  Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton
  Cheng, Oleg Serikov, Omer Antverg, Oskar van~der Wal, Rui Zhang, Ruochen
  Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina,
  Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,
  Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger,
  Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy
  Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
  Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin
  Ajibade, Bharat Saxena, Carlos~Muñoz Ferrandis, Daniel McDuff, Danish
  Contractor, David Lansky, Davis David, Douwe Kiela, Duong~A. Nguyen, Edward
  Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib
  Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
  Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio~Bonis Sanz, Livia Dutra,
  Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha
  Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis
  Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An,
  Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav
  Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach
  Nguyen, Abhinav~Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima
  Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang,
  Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier,
  Daniel~León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio
  Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
  Helena~U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas
  Golde, Jose~David Posada, Karthik~Rangasai Sivaraman, Lokesh Bulchandani, Lu
  Liu, Luisa Shinzato, Madeleine~Hahn de Bykhovetz, Maiko Takeuchi, Marc
  Pàmies, Maria~A Castillo, Marianna Nezhurina, Mario Sänger, Matthias
  Samwald, Michael Cullan, Michael Weinberg, Michiel~De Wolf, Mina Mihaljcic,
  Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg,
  Nicholas~Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya
  Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su,
  Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok~S Deshmukh, Shubhanshu
  Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan
  Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech
  Kusa, Yanis Labrak, Yash~Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin
  Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and
  Thomas Wolf.
\newblock Bloom: A 176b-parameter open-access multilingual language model,
  2023.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.

\bibitem{zhao2023pytorch}
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less
  Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et~al.
\newblock Pytorch fsdp: experiences on scaling fully sharded data parallel.
\newblock {\em arXiv preprint arXiv:2304.11277}, 2023.

\bibitem{zheng2022linear}
Lin Zheng, Chong Wang, and Lingpeng Kong.
\newblock Linear complexity randomized self-attention mechanism.
\newblock In {\em International Conference on Machine Learning}, pages
  27011--27041. PMLR, 2022.

\bibitem{zheng2023efficient}
Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong.
\newblock Efficient attention via control variates.
\newblock In {\em International Conference on Learning Representations}, 2023.

\end{thebibliography}
