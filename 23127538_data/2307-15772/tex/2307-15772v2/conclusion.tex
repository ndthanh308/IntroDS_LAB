\section{Concluding Remarks}
\label{S:concluding}

The main theme of this  paper was to  introduce, for a bounded domain $\Omega\subset \R^d$,  new model
 classes $\cV_w := \cV_w(\cD(\Omega)$), called {\it weighted variation spaces}, and to prove bounds on how well functions in these classes can be
 approximated by a linear combination of $n$ terms of the ReLU dictionary $\cD(\Omega)$. That is, we provided bounds on the error $E_n(f)_{L_2(\Omega)}$
 in approximating $f\in \cV_w(\Omega)$  in the $L_2(\Omega)$ norm by the elements of $\Sigma_n:=\Sigma_n(\cD(\Omega))$ where $\Sigma_n$ is the nonlinear manifold
 of functions $g$ that are a linear combination of $n$ elements of the ReLU dictionary. We showed that for certain choices of the weight $w$ (dependent on $\Omega$)
 the functions in these new model classes have the same approximation rate as those in the classical variation spaces
 $\cV(\Omega)$.  Since $\cV_w$ is strictly  larger than the classical variation spaces $\cV:=\cV(\Omega)$, this gives stronger results
 on $n$-term ReLU approximation than those in the literature.  Thus, these new model classes $\cV_w$ are important in trying to understand which functions are well approximated by $\Sigma_n$.

A natural follow-up question would be to then consider the problem of learning from data generated from from the samples of a function from $\cV_w$, in both the noiseless and noisy settings. In the literature, the former is referred to as \emph{optimal recovery} and the latter is referred to as \emph{minimax estimation}. For the classical variation spaces, the minimax estimation rates have been determined~\cite{PNMinimax}. On the other hand, the optimal recovery rates are currently unknown. Once the data sites are fixed, it is well-known that the procedure for optimal recovery takes the form of solving a regularized least-squares problem over the model class~\cite{binev2024optimal}. Theorem~\ref{representer} below motivates a numerical method (posed as a neural network training problem) to investigate the problem of optimal recovery (as well as for minimax estimation for $\cV_w$).

Assume that $d\ge 2$ and $\Omega = B^d$ in the sequel. Then, $\bar Z(\Omega)=S^{d-1}\times [-1,1]$. Let $w$ be any admissible weight function in the sense of \eqref{wa}  
and let $\cD_w$ be the weighted dictionary defined in \eref{newdictionary}.  We use the results and notation of \S \ref{S:variation}  in going forward.  In particular, the functions in $\cV_w:=\cV_w(\Omega)$ all take the form (see \eref{weightrep})
\be 
\label{form}
\tilde f_\mu:=\int_{\bar Z(\Omega)} \tilde \phi(\cdot;\xi,t)\, d\mu\quad {\rm and} \quad \|\tilde f_\mu\|_{\cV_w(\Omega)}= \|\mu\|_{\cM}.
\ee

Consider the following data-fitting problem.  Suppose that $x_i$, $i=1,\dots,m$, are points from the interior of $\Omega$
and $y_i$, $i=1,\dots, m$, are real numbers.  The data-fitting problem
\begin{equation}
    \inf_{f \in \cV_w(\Omega)} \sum_{i=1}^m |y_i - f(x_i)|^2 + \lambda \norm{f}_{\cV_w},
    \label{eq:opt-Vw}
\end{equation}
with $\lambda > 0$ is equivalent to the data-fitting problem
\begin{equation}
    \inf_{\mu \in \cM(\bar Z(\Omega))} \sum_{i=1}^m \left|y_i -  f_\mu(x_i) \right|^2 + \lambda \norm{\mu}_{\cM},
    \label{eq:opt-measures}
\end{equation}
in the sense that their infimal values are the same and if $\mu^\star$ is a minimizer of \eqref{eq:opt-measures}, then
$f_{\mu^*}$
is a minimizer of \eqref{eq:opt-Vw}. Note that the minimization problem \eref{eq:opt-Vw} does not depend on the ambient space $L_2(\Omega)$ in which we measure error of performance for the approximation problem. An important property of the weighted variation spaces is that solutions to data-fitting problems over this model class admit finite-parameter representations as neural networks. This is summarized in the next theorem.

\begin{theorem} \label{representer}
    Suppose that $w$ is an admissible weight function and  the  $\{x_i\}_{i=1}^m$ lie in the interior of $\Omega$. Then, there exists a solution $f^*$ to \eqref{eq:opt-Vw} that takes the form of a shallow ReLU network
    \begin{equation}
        f^\star(x) = \sum_{j=1}^n a_j \phi(x; \xi_j, t_j) = \sum_{j=1}^n a_j (\xi_j \cdot x - t_j)_+,
    \end{equation}
    where the number of atoms satisfies $n \leq m$, $\{a_j\}_{j=1}^n \subset \R \setminus \{0\}$, and $\{(\xi_j, t_j)\}_{j=1}^n \subset \bar Z(\Omega)$ are data-dependent and not known \emph{a priori}. Furthermore, the regularization cost is $\norm{f^\star}_{\cV_w} = \sum_{j=1}^n w(\xi_j, t_j) \abs{a_j}$.
\end{theorem}
\vskip .1in
\noindent
{\bf Proof:}
Let $C(\bar Z(\Omega))$ denote the space of real  valued functions   on $\bar Z(\Omega)$. This is a Banach space when equipped with the  $L_\infty$-norm. By the Riesz--Markov--Kakutani representation theorem~\cite[Chapter~7]{FollandRA}, the dual of $C(\bar Z(\Omega))$ can be identified with the space of signed Radon measures $\cM:=\cM(\bar Z(\Omega))$. It is well-known that the extreme points of the unit ball
\be
    \{\mu \in \cM : \norm{\mu}_{\cM} \leq 1\}
\ee
are the Dirac measures $\pm \delta_{(\xi, t)}$, $(\xi, t) \in \bar Z(\Omega)$ (see, e.g.,~\cite[Proposition~4.1]{BC}).

Next, for $i = 1, \ldots, m$, we introduce the functions
\be 
\label{defhi}
h_i(\xi, t) := \tilde \phi(x_i;\xi,t)=\frac{\phi(x_i; \xi, t)}{w(\xi, t)},\quad (\xi,t)\in \bar Z(\Omega). 
\ee 
We can rewrite \eqref{eq:opt-measures} as
\be
    \inf_{\mu \in \cM } \sum_{i=1}^m |y_i - \langle \mu, h_i \rangle |^2 + \lambda \norm{\mu}_{\cM},
    \label{eq:opt-measures2}
\ee
where $\langle \cdot, \cdot \rangle$ denotes the duality pairing between $C(\bar Z(\Omega))$ and $\cM$. Since the functions $h_i $, $i=1,\dots,m$, are in $ C(\bar Z(\Omega))$,   the mappings $\mu \mapsto \langle \mu, h_i \rangle$ are weak$^*$ continuous~\cite[Theorem~IV.20, p. 114]{RSBook}. This shows that the hypothesis of the abstract representer theorem~\cite{BCDDDW,BC,U} are satisfied.  That theorem shows  that there exists a solution to \eqref{eq:opt-measures2} that takes the form of a linear combination of the extreme points of the unit regularization ball. Thus, there exists a solution that takes the form
\be
    \mu^\star = \sum_{j=1}^n c_j \delta_{(\xi_j, t_j)},
\ee
where the number of atoms satisfies $n \leq m$, $\{c_j\}_{j=1}^n \subset \R \setminus \{0\}$, and $\{(\xi_j, t_j)\}_{j=1}^n \subset \bar Z(\Omega)$ are distinct, data dependent, and not known \emph{a priori}. Clearly $\norm{\mu^\star}_{\cM} = \sum_{j=1}^n \abs{c_j}$.

From the equivalence between \eqref{eq:opt-Vw} and \eqref{eq:opt-measures2}, we see that the function 
\begin{equation}
\label{solution}
    f_{\mu^\star} =  \int_{\bar Z(\Omega)}  \tilde \phi(\cdot; \xi, t)  d\mu^\star(\xi, t) = \sum_{j=1}^n \frac{c_j}{w(\xi_j, t_j)} \phi(x; \xi_j, t_j)
\end{equation}
is a minimizer of \eqref{eq:opt-Vw}. The theorem follows by the substitution $a_j := c_j / w(\xi_j, t_j)$.
\hfill $\Box$

We have not indicated the fact that the solution \eref{solution} to the data-fitting problem depends on $\lambda$.  If we let
$\lambda$ tend to zero then the solutions converge to a minimum-norm interpolant $f^{\#}$ of the data
\be 
\label{minnorm}
f^{\#} \in \argmin \{\|f\|_{\cV_w}: f(x_i)=y_i,\ i=1,\dots,m\}.
\ee 
In which case, there always exists an $f^{\#}$ that has a representation 
\be 
\label{minnormrep} 
f^{\#} =   \sum_{j=1}^n  a_j^{\#}  \phi(x; \xi_j^{\#}, t_j^{\#}),
\ee 
with $n\le m$.
 
The theorem statement also holds when the first term in the objective in \eqref{eq:opt-Vw} is replaced by any loss function $\cL(\cdot, \cdot)$ which is lower semi-continuous (see~\cite[Proof~of~Theorem~3.2]{PNDeep}). In neural network parlance, the $\xi_j$ are referred to as the \emph{input weights}, the $a_j$ are referred to as the \emph{output weights} and the $t_j$ are referred to as the \emph{biases}. Observe that the norm of a single neuron $\phi(\cdot; \xi, t)$, where $\xi \in \R^d$ and $t \in \R$, takes the form
\begin{equation}
    \norm{\phi(\cdot; \xi, t)}_{\cV_w} = \norm{\xi} w\left(\frac{\xi}{\norm{\xi}}, \frac{t}{\norm{\xi}}\right),
\end{equation}
where we took advantage of the fact that the ReLU is positively homogeneous of degree $1$. In this form,  the input weights are not restricted to be unit norm. Theorem~\ref{representer} then implies that a solution to the variational problem in \eqref{eq:opt-Vw} can be found by training a sufficiently wide (fixed width $n \geq m$) neural network to a global minimizer with an appropriate regularization term. This follows, in particular, by finding a solution to the neural network training problem
\begin{equation}
    \min_{\theta} \sum_{i=1}^m |y_i - f_\theta(x_i)|^2 + \lambda \sum_{j=1}^n |a_j| \norm{\xi_j} w\left(\frac{\xi_j}{\norm{\xi_j}}, \frac{t_j}{\norm{\xi_j}}\right), \label{eq:nn-prob}
\end{equation}
where
\begin{equation}
    f_\theta(x) = \sum_{j=1}^n a_j \phi(x; \xi_j, t_j) = \sum_{j=1}^n a_j (\xi_j \cdot x - t_j)_+
\end{equation}
is a shallow ReLU neural network and $\theta = (a_j, \xi_j, t_j)_{j=1}^n$ denotes the neural network parameters and $n \geq m$.
When $\lambda$ is chosen to be sufficiently small, the estimator $f_{\widetilde{\theta}}$ achieves the optimal recovery rate for the model class $\cV_w$, where $\widetilde{\theta}$ is any minimizer of \eqref{eq:nn-prob}~\cite{binev2024optimal}. 

When $w$ is the weight specified in \eqref{defweightd} (which satisfies the hypotheses of Theorem~\ref{representer}), the resulting regularizer takes the form
\begin{equation}
    \sum_{j=1}^n |a_j| \norm{\xi_j} \left(1 - \frac{t_j}{\norm{\xi_j}}\right)^{\frac{1}{2} + \frac{d}{4}} \label{eq:new-reg}
\end{equation}
This is a new regularizer for training neural networks which directly penalizes the biases. If we assume the data sites $\{x_i\}_{i=1}^m$ are drawn i.i.d.\ uniformly on $B^d$, then this penalization reflects the volume of the subset of $B^d$ where the neuron is ``active''  (nonzero output). This suggests a new, data-adaptive regularization scheme in which the the penalty on a given neuron is proportional to the number of data in its support. This regularizer should be contrasted with the unweighted case in which the regularizer takes the form
\begin{equation}
    \sum_{j=1}^n |a_j| \norm{\xi_j},
\end{equation}
which is sometimes referred to as the \emph{path-norm}~\cite{NSS} of the neural network. Remarkably, path-norm regularization is equivalent to the common procedure of training a neural network with \emph{weight decay}~\cite{KH} which corresponds to a regularizer of the form
\begin{equation}
    \frac{1}{2}\sum_{j=1}^n |a_j|^2 + \norm{\xi_j}^2.
\end{equation}
We refer the reader to~\cite{PNSurvey} for more details about this equivalence. The new regularizer in \eqref{eq:new-reg} requires further study in both theory and practice.

\subsection{Open Problems}
The results presented in this paper open the door to several new research directions.
\begin{enumerate}
    \item We have shown that the classical variation space $\cV(\Omega)$ is not the approximation space $\cA^\alpha = \cA^\alpha(L_2(\Omega))$, $\alpha = \frac{1}{2} + \frac{3}{2d}$, since the (strictly larger) weighted variation space $\cV_w(\Omega)$ admits the same $n$-term approximation rate with shallow ReLU networks. Thus, the results of this paper bring us one step closer to characterizing the approximation space $\cA^\alpha$, $\alpha = \frac{1}{2} + \frac{3}{2d}$. Future work will be devoted to finding a characterization of this approximation space.

    \item The results of this paper only consider $L_2$-approximation. We conjecture that the same rates hold for weighted variation spaces for all $L_p$, $1 \leq p \leq \infty$, where now the admissibility condition on the weights will depend on $p$. That is to say, for each $1 \leq p \leq \infty$, there exists a weight function $w^*_p$ such that the the optimal rate  $n^{-\frac{1}{2}-\frac{3}{2d}}$ is achieved.

    \item 
    The determination of the optimal recovery rates and minimax estimation rates for $\cV_w$ is a natural follow-up research direction. Theorem~\ref{representer} and \eqref{eq:nn-prob} provide a numerical method (posed as a neural network training problem) whose solutions are known to achieve the optimal recovery rate. A characterization of this rate is has not been determined, even in the unweighted scenario.
     
    \item The weighted variation spaces motivates a new form of data-adaptive regularization for neural networks. Theoretical and experimental comparisons of this new form of regularization compared with more conventional regularization techniques is a direction of future work. Furthermore, extensions of this regularizer to deep neural networks is also a direction of future work.
\end{enumerate}
  
