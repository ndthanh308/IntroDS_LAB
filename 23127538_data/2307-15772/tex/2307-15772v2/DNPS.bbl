\begin{thebibliography}{10}

\bibitem{Bach}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em Journal of Machine Learning Research}, 18(1):629--681, 2017.

\bibitem{B}
Andrew~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information Theory}, 39(3):930--945, 1993.

\bibitem{BCDD}
Andrew~R. Barron, Albert Cohen, Wolfgang Dahmen, and Ronald~A. DeVore.
\newblock Approximation and learning by greedy algorithms.
\newblock {\em Annals of Statistics}, 36(1):64--94, 2008.

\bibitem{binev2024optimal}
Peter Binev, Andrea Bonito, Ronald DeVore, and Guergana Petrova.
\newblock Optimal learning.
\newblock {\em Calcolo}, 61(1):15, 2024.

\bibitem{BGKP}
Helmut Bolcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen.
\newblock Optimal approximation with sparsely connected deep neural networks.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 1(1):8--45, 2019.

\bibitem{bourgain1988distribution}
Jean Bourgain and Joram Lindenstrauss.
\newblock Distribution of points on spheres and approximation by zonotopes.
\newblock {\em Israel Journal of Mathematics}, 64:25--31, 1988.

\bibitem{bourgain1989approximation}
Jean Bourgain, Joram Lindenstrauss, and Vitali Milman.
\newblock Approximation of zonoids by zonotopes.
\newblock {\em Acta Mathematica}, 162(1):73--141, 1989.

\bibitem{BCDDDW}
Claire Boyer, Antonin Chambolle, Yohann De~Castro, Vincent Duval,
  Fr\'{e}d\'{e}ric de~Gournay, and Pierre Weiss.
\newblock On representer theorems and convex regularization.
\newblock {\em SIAM Journal on Optimization}, 29(2):1260--1281, 2019.

\bibitem{BC}
Kristian Bredies and Marcello Carioni.
\newblock Sparsity of solutions for variational inverse problems with
  finite-dimensional data.
\newblock {\em Calculus of Variations and Partial Differential Equations},
  59(1):Paper No. 14, 26, 2020.

\bibitem{CDPW}
Albert Cohen, Ronald DeVore, Guergana Petrova, and Przemyslaw Wojtaszczyk.
\newblock Optimal stable nonlinear approximation.
\newblock {\em Foundations of Computational Mathematics}, 22(3):607--648, 2022.

\bibitem{DHP}
Ronald DeVore, Boris Hanin, and Guergana Petrova.
\newblock Neural network approximation.
\newblock {\em Acta Numerica}, 30:327--444, 2021.

\bibitem{DNonlinear}
Ronald~A. DeVore.
\newblock Nonlinear approximation.
\newblock {\em Acta Numerica}, 7:51--150, 1998.

\bibitem{DLbook}
Ronald~A. DeVore and George~G. Lorentz.
\newblock {\em Constructive Approximation}.
\newblock Grundlehren der mathematischen Wissenschaften. Springer Berlin
  Heidelberg, 1993.

\bibitem{DP}
Ronald~A. DeVore and Vasil~A. Popov.
\newblock Interpolation spaces and non-linear approximation.
\newblock In {\em Function Spaces and Applications: Proceedings of the
  US-Swedish Seminar held in Lund, Sweden, June 15--21, 1986}, pages 191--205.
  Springer, 2006.

\bibitem{ma2022barron}
Weinan E, Chao Ma, and Lei Wu.
\newblock The {B}arron space and the flow-induced function spaces for neural
  network models.
\newblock {\em Constructive Approximation}, 55(1):369--406, 2022.

\bibitem{FollandRA}
Gerald~B. Folland.
\newblock {\em Real analysis: Modern techniques and their applications}.
\newblock Pure and Applied Mathematics (New York). John Wiley \& Sons, Inc.,
  New York, second edition, 1999.

\bibitem{GK}
R{\'e}mi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender.
\newblock Approximation spaces of deep neural networks.
\newblock {\em Constructive Approximation}, 55(1):259--367, 2022.

\bibitem{Jones}
Lee~K. Jones.
\newblock A simple lemma on greedy approximation in {H}ilbert space and
  convergence rates for projection pursuit regression and neural network
  training.
\newblock {\em The Annals of Statistics}, pages 608--613, 1992.

\bibitem{KB}
Jason~M. Klusowski and Andrew~R. Barron.
\newblock Approximation by combinations of {ReLU} and squared {ReLU} ridge
  functions with {$\ell^1$} and {$\ell^0$} controls.
\newblock {\em IEEE Transactions on Information Theory}, 64(12):7649--7656,
  2018.

\bibitem{korolev2022two}
Yury Korolev.
\newblock Two-layer neural networks with values in a {B}anach space.
\newblock {\em SIAM Journal on Mathematical Analysis}, 54(6):6358--6389, 2022.

\bibitem{KH}
Anders Krogh and John Hertz.
\newblock A simple weight decay can improve generalization.
\newblock {\em Advances in neural information processing systems}, 4, 1991.

\bibitem{kurkova2001bounds}
V{\v{e}}ra Kůrková and Marcello Sanguineti.
\newblock Bounds on rates of variable-basis and neural-network approximation.
\newblock {\em IEEE Transactions on Information Theory}, 47(6):2659--2665,
  2001.

\bibitem{lu2021deep}
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network approximation for smooth functions.
\newblock {\em SIAM Journal on Mathematical Analysis}, 53(5):5465--5506, 2021.

\bibitem{Makovoz}
Yuly Makovoz.
\newblock Uniform approximation by neural networks.
\newblock {\em Journal of Approximation Theory}, 95(2):215--228, 1998.

\bibitem{matouvsek1996improved}
Ji{\v{r}}{\'\i} Matou{\v{s}}ek.
\newblock Improved upper bounds for approximation by zonotopes.
\newblock {\em Acta Mathematica}, 177(1):55--73, 1996.

\bibitem{mhaskar2004tractability}
Hrushikesh~N. Mhaskar.
\newblock On the tractability of multivariate integration and approximation by
  neural networks.
\newblock {\em Journal of Complexity}, 20(4):561--590, 2004.

\bibitem{mhaskar2020dimension}
Hrushikesh~N. Mhaskar.
\newblock Dimension independent bounds for general shallow networks.
\newblock {\em Neural Networks}, 123:142--152, 2020.

\bibitem{NSS}
Behnam Neyshabur, Russ~R. Salakhutdinov, and Nati Srebro.
\newblock Path-{SGD}: Path-normalized optimization in deep neural networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{O+}
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro.
\newblock A function space view of bounded norm infinite width {ReLU} nets: The
  multivariate case.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{PN}
Rahul Parhi and Robert~D. Nowak.
\newblock Banach space representer theorems for neural networks and ridge
  splines.
\newblock {\em Journal of Machine Learning Research}, 22(43):1--40, 2021.

\bibitem{PNDeep}
Rahul Parhi and Robert~D. Nowak.
\newblock What kinds of functions do deep neural networks learn? {I}nsights
  from variational spline theory.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 4(2):464--489,
  2022.

\bibitem{PNSurvey}
Rahul Parhi and Robert~D. Nowak.
\newblock Deep learning meets sparse regularization: A signal processing
  perspective.
\newblock {\em IEEE Signal Processing Magazine}, 40(6):63--74, 2023.

\bibitem{PNMinimax}
Rahul Parhi and Robert~D. Nowak.
\newblock Near-minimax optimal estimation with shallow {ReLU} neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 69(2):1125--1140,
  2023.

\bibitem{P}
Allan Pinkus.
\newblock Approximation theory of the {MLP} model in neural networks.
\newblock {\em Acta Numerica}, 8:143--195, 1999.

\bibitem{M}
Gilles Pisier.
\newblock Remarques sur un r{\'e}sultat non publi{\'e} de {B}. {M}aurey.
\newblock {\em S{\'e}minaire d'Analyse Fonctionnelle (dit
  ``Maurey-Schwartz'')}, pages 1--12, April 1981.

\bibitem{RSBook}
Michael Reed and Barry Simon.
\newblock {\em Methods of Modern Mathematical Physics {I}: Functional
  analysis}.
\newblock Academic Press, 1972.

\bibitem{ZX}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Optimal approximation rate of relu networks in terms of width and
  depth.
\newblock {\em Journal de Math{\'e}matiques Pures et Appliqu{\'e}es},
  157:101--135, 2022.

\bibitem{siegel2023optimal}
Jonathan~W. Siegel.
\newblock Optimal approximation of zonoids and uniform approximation by shallow
  neural networks.
\newblock {\em arXiv preprint arXiv:2307.15285}, 2023.

\bibitem{JX}
Jonathan~W. Siegel.
\newblock Optimal approximation rates for deep {ReLU} neural networks on
  {S}obolev and {B}esov spaces.
\newblock {\em Journal of Machine Learning Research}, 24(357):1--52, 2023.

\bibitem{SXSharp}
Jonathan~W. Siegel and Jinchao Xu.
\newblock Sharp bounds on the approximation rates, metric entropy, and
  {$n$}-widths of shallow neural networks.
\newblock {\em Foundations of Computational Mathematics}, pages 1--57, 2022.

\bibitem{SXDict}
Jonathan~W. Siegel and Jinchao Xu.
\newblock Characterization of the variation spaces corresponding to shallow
  neural networks.
\newblock {\em Constructive Approximation}, pages 1--24, 2023.

\bibitem{U}
Michael Unser.
\newblock A unifying representer theorem for inverse problems and machine
  learning.
\newblock {\em Foundations of Computational Mathematics}, 21(4):941--960, 2021.

\bibitem{Y}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\end{thebibliography}
