\begin{thebibliography}{10}

\bibitem{Bach}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em Journal of Machine Learning Research}, 18(1):629--681, 2017.

\bibitem{B}
Andrew~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information Theory}, 39(3):930--945, 1993.

\bibitem{BGKP}
Helmut Bolcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen.
\newblock Optimal approximation with sparsely connected deep neural networks.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 1(1):8--45, 2019.

\bibitem{BCDDDW}
Claire Boyer, Antonin Chambolle, Yohann De~Castro, Vincent Duval,
  Fr\'{e}d\'{e}ric de~Gournay, and Pierre Weiss.
\newblock On representer theorems and convex regularization.
\newblock {\em SIAM Journal on Optimization}, 29(2):1260--1281, 2019.

\bibitem{BC}
Kristian Bredies and Marcello Carioni.
\newblock Sparsity of solutions for variational inverse problems with
  finite-dimensional data.
\newblock {\em Calculus of Variations and Partial Differential Equations},
  59(1):Paper No. 14, 26, 2020.

\bibitem{CDPW}
Albert Cohen, Ronald DeVore, Guergana Petrova, and Przemyslaw Wojtaszczyk.
\newblock Optimal stable nonlinear approximation.
\newblock {\em Foundations of Computational Mathematics}, 22(3):607--648, 2022.

\bibitem{DHP}
Ronald DeVore, Boris Hanin, and Guergana Petrova.
\newblock Neural network approximation.
\newblock {\em Acta Numerica}, 30:327--444, 2021.

\bibitem{DNonlinear}
Ronald~A. DeVore.
\newblock Nonlinear approximation.
\newblock {\em Acta Numerica}, 7:51--150, 1998.

\bibitem{FollandRA}
Gerald~B. Folland.
\newblock {\em Real analysis: Modern techniques and their applications}.
\newblock Pure and Applied Mathematics (New York). John Wiley \& Sons, Inc.,
  New York, second edition, 1999.

\bibitem{GK}
R{\'e}mi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender.
\newblock Approximation spaces of deep neural networks.
\newblock {\em Constructive Approximation}, 55(1):259--367, 2022.

\bibitem{Jones}
Lee~K. Jones.
\newblock A simple lemma on greedy approximation in {H}ilbert space and
  convergence rates for projection pursuit regression and neural network
  training.
\newblock {\em The Annals of Statistics}, pages 608--613, 1992.

\bibitem{KB}
Jason~M. Klusowski and Andrew~R. Barron.
\newblock Approximation by combinations of {ReLU} and squared {ReLU} ridge
  functions with {$\ell^1$} and {$\ell^0$} controls.
\newblock {\em IEEE Transactions on Information Theory}, 64(12):7649--7656,
  2018.

\bibitem{KH}
Anders Krogh and John Hertz.
\newblock A simple weight decay can improve generalization.
\newblock {\em Advances in neural information processing systems}, 4, 1991.

\bibitem{kurkova2001bounds}
Vera Kurkov{\'a} and Marcello Sanguineti.
\newblock Bounds on rates of variable-basis and neural-network approximation.
\newblock {\em IEEE Transactions on Information Theory}, 47(6):2659--2665,
  2001.

\bibitem{lu2021deep}
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network approximation for smooth functions.
\newblock {\em SIAM Journal on Mathematical Analysis}, 53(5):5465--5506, 2021.

\bibitem{ma2022barron}
Chao Ma, Lei Wu, et~al.
\newblock The barron space and the flow-induced function spaces for neural
  network models.
\newblock {\em Constructive Approximation}, 55(1):369--406, 2022.

\bibitem{Makovoz}
Yuly Makovoz.
\newblock Uniform approximation by neural networks.
\newblock {\em Journal of Approximation Theory}, 95(2):215--228, 1998.

\bibitem{mhaskar2004tractability}
Hrushikesh~Narhar Mhaskar.
\newblock On the tractability of multivariate integration and approximation by
  neural networks.
\newblock {\em Journal of Complexity}, 20(4):561--590, 2004.

\bibitem{NSS}
Behnam Neyshabur, Russ~R. Salakhutdinov, and Nati Srebro.
\newblock Path-{SGD}: Path-normalized optimization in deep neural networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{O+}
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro.
\newblock A function space view of bounded norm infinite width {ReLU} nets: The
  multivariate case.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{PN}
Rahul Parhi and Robert~D. Nowak.
\newblock Banach space representer theorems for neural networks and ridge
  splines.
\newblock {\em Journal of Machine Learning Research}, 22(43):1--40, 2021.

\bibitem{PNDeep}
Rahul Parhi and Robert~D. Nowak.
\newblock What kinds of functions do deep neural networks learn? {I}nsights
  from variational spline theory.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 4(2):464--489,
  2022.

\bibitem{PNSurvey}
Rahul Parhi and Robert~D. Nowak.
\newblock Deep learning meets sparse regularization: A signal processing
  perspective.
\newblock {\em arXiv preprint arXiv:2301.09554}, 2023.

\bibitem{PNMinimax}
Rahul Parhi and Robert~D. Nowak.
\newblock Near-minimax optimal estimation with shallow {ReLU} neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 69(2):1125--1140,
  2023.

\bibitem{P}
Allan Pinkus.
\newblock Approximation theory of the {MLP} model in neural networks.
\newblock {\em Acta Numerica}, 8:143--195, 1999.

\bibitem{M}
Gilles Pisier.
\newblock Remarques sur un r{\'e}sultat non publi{\'e} de {B}. {M}aurey.
\newblock {\em S{\'e}minaire d'Analyse Fonctionnelle (dit
  ``Maurey-Schwartz'')}, pages 1--12, April 1981.

\bibitem{RSBook}
Michael Reed and Barry Simon.
\newblock {\em Methods of Modern Mathematical Physics {I}: Functional
  analysis}.
\newblock Academic Press, 1972.

\bibitem{ZX}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Optimal approximation rate of relu networks in terms of width and
  depth.
\newblock {\em Journal de Math{\'e}matiques Pures et Appliqu{\'e}es},
  157:101--135, 2022.

\bibitem{JX}
Jonathan~W. Siegel.
\newblock Optimal approximation rates for deep {ReLU} neural networks on
  {S}obolev spaces.
\newblock {\em arXiv preprint arXiv:2211.14400}, 2022.

\bibitem{SXSharp}
Jonathan~W. Siegel and Jinchao Xu.
\newblock Sharp bounds on the approximation rates, metric entropy, and
  {$n$}-widths of shallow neural networks.
\newblock {\em Foundations of Computational Mathematics}, pages 1--57, 2022.

\bibitem{SXDict}
Jonathan~W. Siegel and Jinchao Xu.
\newblock Characterization of the variation spaces corresponding to shallow
  neural networks.
\newblock {\em Constructive Approximation}, pages 1--24, 2023.

\bibitem{U}
Michael Unser.
\newblock A unifying representer theorem for inverse problems and machine
  learning.
\newblock {\em Foundations of Computational Mathematics}, 21(4):941--960, 2021.

\bibitem{Y}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\end{thebibliography}
