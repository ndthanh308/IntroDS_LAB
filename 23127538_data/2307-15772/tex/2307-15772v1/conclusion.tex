\section{Concluding remarks and  implications to neural network training}
\label{S:concluding}



 
The main theme of this  paper was to  introduce, for a bounded domain $\Omega\subset \R^d$,  new model
 classes $\cV_w := \cV_w(\cD(\Omega)$), called {\it weighted variation spaces}, and to prove bounds on how well functions in these classes can be
 approximated by a linear combination of $n$ terms of the ReLU dictionary $\cD(\Omega)$. That is, we provided bounds on the error $E_n(f)_{L_2(\Omega)}$
 in approximating $f\in \cV_w(\Omega)$  in the $L_2(\Omega)$ norm by the elements of $\Sigma_n:=\Sigma_n(\cD(\Omega))$ where $\Sigma_n$ is the nonlinear parameterized manifold
 of functions $g$ that are a linear combination of $n$ elements of the ReLU dictionary. We showed that for certain choices of the weight $w$ (dependent on $\Omega$)
 the functions in these new model classes have the same approximation rate as those in the classical variation spaces
 $\cV(\Omega)$.  Since $\cV_w$ is strictly  larger than the classical variation spaces $\cV:=\cV(\Omega)$, this gives stronger results
 on $n$-term ReLU approximation than those in the literature.  Thus, these new model classes $\cV_w$ are important in trying to understand which functions are well approximated by $\Sigma_n$.

These approximation theoretic results may also have  implications for training and regularization when using shallow neural networks for learning an unknown function from data observations. We outline these implications in the remainder of this section. We assume that $d\ge 2$ and $\Omega = B^d$ throughout this section.  Therefore, $\bar Z(\Omega)=S^{d-1}\times [-1,1]$. Let $w$ be any admissible weight function in the sense of \eqref{wa}  
and let $\cD_w$ be the weighted dictionary defined in \eref{newdictionary}.  We use the results and notation of \S \ref{S:variation}  in going forward.  In particular, the functions in $\cV_w:=\cV_w(\Omega)$ all take the form (see \eref{weightrep})
%
\be 
\label{form}
\tilde f_\mu:=\int_{\bar Z(\Omega)} \tilde \phi(\cdot;\xi,t)\, d\mu\quad {\rm and} \quad \|\tilde f_\mu\|_{\cV_w(\Omega)}= \|\mu\|_{\cM}.
\ee

We consider the following data-fitting problem.  Suppose that $x_i$, $i=1,\dots,m$, are points from the interior of $\Omega$
and $y_i$, $i=1,\dots, m$, are real numbers.  The data-fitting problem
\begin{equation}
    \inf_{f \in \cV_w(\Omega)} \sum_{i=1}^m |y_i - f(x_i)|^2 + \lambda \norm{f}_{\cV_w},
    \label{eq:opt-Vw}
\end{equation}
with $\lambda > 0$ is equivalent to the data-fitting problem
\begin{equation}
    \inf_{\mu \in \cM(\bar Z(\Omega))} \sum_{i=1}^m \left|y_i -  f_\mu(x_i) \right|^2 + \lambda \norm{\mu}_{\cM},
    \label{eq:opt-measures}
\end{equation}
in the sense that their infimal values are the same and if $\mu^\star$ is a minimizer of \eqref{eq:opt-measures}, then
$f_{\mu^*}$
%\begin{equation}
   % f_{\mu^\star} = x \mapsto \int_{S^{d-1} \times [-1, 1]} \frac{\phi(x; \xi, t)}{w(\xi, t)} d\mu^\star(\xi, t)
%\end{equation}
is a minimizer of \eqref{eq:opt-Vw}. Note that the minimization problem \eref{eq:opt-Vw} does not depend on the ambient space $L_2(\Omega)$ in which we measure error of performance for the approximation problem. An important property of the weighted variation spaces is the following \emph{representer theorem}.

\begin{theorem} \label{representer}
    Suppose that $w$ is an admissible weight function and  the  $\{x_i\}_{i=1}^m$ lie in the interior of $\Omega$. Then, there exists a solution $f^*$ to \eqref{eq:opt-Vw} that takes the form of a shallow ReLU network
    \begin{equation}
        f^\star(x) = \sum_{j=1}^n a_j \phi(x; \xi_j, t_j) = \sum_{j=1}^n a_j (\xi_j \cdot x - t_j)_+,
    \end{equation}
    where the number of atoms satisfies $n \leq m$, $\{a_j\}_{j=1}^n \subset \R \setminus \{0\}$, and $\{(\xi_j, t_j)\}_{j=1}^n \subset \bar Z(\Omega)$ are data-dependent and not known \emph{a priori}. Furthermore, the regularization cost is $\norm{f^\star}_{\cV_w} = \sum_{j=1}^n w(\xi_j, t_j) \abs{a_j}$.
\end{theorem}
\vskip .1in
\noindent
{\bf Proof:}
Let $C(\bar Z(\Omega))$ denote the space of real  valued functions   on $\bar Z(\Omega)$. This is a Banach space when equipped with the  $L_\infty$-norm. By the Riesz--Markov--Kakutani representation theorem~\cite[Chapter~7]{FollandRA}, the dual of $C(\bar Z(\Omega))$ can be identified with the space of signed Radon measures $\cM:=\cM(\bar Z(\Omega))$. It is well-known that the extreme points of the unit ball
\be
    \{\mu \in \cM : \norm{\mu}_{\cM} \leq 1\}
\ee
are the Dirac measures $\pm \delta_{(\xi, t)}$, $(\xi, t) \in \bar Z(\Omega)$ (see, e.g.,~\cite[Proposition~4.1]{BC}).

Next, for $i = 1, \ldots, m$, we introduce the functions
%
\be 
\label{defhi}
h_i(\xi, t) := \tilde \phi(x_i;\xi,t)=\frac{\phi(x_i; \xi, t)}{w(\xi, t)},\quad (\xi,t)\in \bar Z(\Omega). 
\ee 
We can rewrite \eqref{eq:opt-measures} as
\be
    \inf_{\mu \in \cM } \sum_{i=1}^m |y_i - \langle \mu, h_i \rangle |^2 + \lambda \norm{\mu}_{\cM},
    \label{eq:opt-measures2}
\ee
where $\langle \cdot, \cdot \rangle$ denotes the duality pairing between $C(\bar Z(\Omega))$ and $\cM$. Since the functions $h_i $, $i=1,\dots,m$, are in $ C(\bar Z(\Omega))$,   the mappings $\mu \mapsto \langle \mu, h_i \rangle$ are weak$^*$ continuous~\cite[Theorem~IV.20, p. 114]{RSBook}. This shows that the hypothesis of  the abstract representer theorem of~\cite[Theorems~2~and~3]{U} (see also~\cite{BCDDDW,BC}) are satisfied.  That theorem shows  that there exists a solution to \eqref{eq:opt-measures2} that takes the form of a linear combination of the extreme points of the unit regularization ball. Thus, there exists a solution that takes the form
\be
    \mu^\star = \sum_{j=1}^n c_j \delta_{(\xi_j, t_j)},
\ee
where the number of atoms satisfies $n \leq m$, $\{c_j\}_{j=1}^n \subset \R \setminus \{0\}$, and $\{(\xi_j, t_j)\}_{j=1}^n \subset \bar Z(\Omega)$ are distinct, data-dependent, and not known \emph{a priori}. Clearly $\norm{\mu^\star}_{\cM} = \sum_{j=1}^n \abs{c_j}$.

From the equivalence between \eqref{eq:opt-Vw} and \eqref{eq:opt-measures2}, we see that the function 
\begin{equation}
\label{solution}
    f_{\mu^\star} =  \int_{\bar Z(\Omega)}  \tilde \phi(\cdot; \xi, t)  d\mu^\star(\xi, t) = \sum_{j=1}^n \frac{c_j}{w(\xi_j, t_j)} \phi(x; \xi_j, t_j)
\end{equation}
is a minimizer of \eqref{eq:opt-Vw}. The theorem follows by the substitution $a_j := c_j / w(\xi_j, t_j)$.
\hfill $\Box$

We have not indicated the fact that the solution \eref{solution} to the data fitting problem depends on $\lambda$.  If we let
$\lambda$ tend to zero then the solutions converge to a minimum-norm interpolant $f^{\#}$ of the data
%
\be 
\label{minnorm}
f^{\#} \in \argmin \{\|f\|_{\cV_w}: f(x_i)=y_i,\ i=1,\dots,m\}.
\ee 
In which case, there always exists an $f^{\#}$ that has a representation 
\be 
\label{minnormrep} 
f^{\#} =   \sum_{j=1}^n  a_j^{\#}  \phi(x; \xi_j^{\#}, t_j^{\#}),
\ee 
with $n\le m$.
 



The theorem statement also holds when the first term in the objective in \eqref{eq:opt-Vw} is replaced by any loss function $\cL(\cdot, \cdot)$ which is lower semi-continuous (see~\cite[Proof~of~Theorem~3.2]{PNDeep}). In neural network parlance, the $\xi_j$ are referred to as the \emph{input weights}, the $a_j$ are referred to as the \emph{output weights} and the $t_j$ are referred to as the \emph{biases}. 

This representer theorem provides insight into new forms of regularization for neural networks. Indeed, first notice that the norm of a single neuron $\phi(\cdot; \xi, t)$, where $\xi \in \R^d$ and $t \in \R$, takes the form
\begin{equation}
    \norm{\phi(\cdot; \xi, t)}_{\cV_w} = \norm{\xi} w\left(\frac{\xi}{\norm{\xi}}, \frac{t}{\norm{\xi}}\right),
\end{equation}
where we took advantage of the fact that the ReLU is positively homogeneous of degree $1$. In this form,  the input weights are not restricted to be unit norm. The representer theorem (Theorem~\ref{representer}) implies that a solution to the variational problem in \eqref{eq:opt-Vw} can be found by training a sufficiently wide (fixed width $n \geq m$) neural network to a global minimizer. In particular, by finding a solution to the neural network training problem
\begin{equation}
    \min_{\theta} \sum_{i=1}^m |y_i - f_\theta(x_i)|^2 + \lambda \sum_{j=1}^n |a_j| \norm{\xi_j} w\left(\frac{\xi_j}{\norm{\xi_j}}, \frac{t_j}{\norm{\xi_j}}\right),
\end{equation}
where
\begin{equation}
    f_\theta(x) = \sum_{j=1}^n a_j \phi(x; \xi_j, t_j) = \sum_{j=1}^n a_j (\xi_j \cdot x - t_j)_+
\end{equation}
is a shallow ReLU neural network and $\theta = (a_j, \xi_j, t_j)_{j=1}^n$ denotes the neural network parameters.

When $w$ is the weight specified in \eqref{defweightd} (which satisfies the hypotheses of Theorem~\ref{representer}), the resulting regularizer takes the form
\begin{equation}
    \sum_{j=1}^n |a_j| \norm{\xi_j} \left(1 - \frac{t_j}{\norm{\xi_j}}\right)^{\frac{1}{2} + \frac{d}{4}} \label{eq:new-reg}
\end{equation}
This is a new regularizer for training neural networks which directly penalizes the biases. If we assume the data sites $\{x_i\}_{i=1}^m$ are drawn i.i.d.\ uniformly on $B^d$, then this penalization reflects the volume of the subset of $B^d$ where the neuron is ``active"  (nonzero output). This suggests a new, data-adaptive regularization scheme in which the the penalty on a given neuron is proportional to the number of data in its support. This regularizer should be contrasted with the unweighted case in which the regularizer takes the form
\begin{equation}
    \sum_{j=1}^n |a_j| \norm{\xi_j},
\end{equation}
which is sometimes referred to as the \emph{path-norm}~\cite{NSS} of the neural network. Remarkably, path-norm regularization is equivalent to the common procedure of training a neural network with \emph{weight decay}~\cite{KH} which corresponds to a regularizer of the form
\begin{equation}
    \frac{1}{2}\sum_{j=1}^n |a_j|^2 + \norm{\xi_j}^2.
\end{equation}
We refer the reader to~\cite{PNSurvey} for more details about this equivalence. The new regularizer in \eqref{eq:new-reg} requires further study in both theory and practice.

\subsection{Open Problems}
The results presented in this paper open the door to several new research directions.
\begin{enumerate}
    \item We have shown that the classical variation space $\cV(\Omega)$ is not the approximation space $\cA^\alpha = \cA^\alpha(L_2(\Omega))$, $\alpha = \frac{1}{2} + \frac{3}{2d}$, since the (strictly larger) weighted variation space $\cV_w(\Omega)$ admits the same $n$-term approximation rate with shallow ReLU networks. Thus, the results of this paper bring us one step closer to characterizing the approximation space $\cA^\alpha$, $\alpha = \frac{1}{2} + \frac{3}{2d}$. Future work will be devoted to finding a characterization of this approximation space.

    \item The results of this paper only consider $L_2$-approximation. We conjecture that the same rates hold for weighted variation spaces for all $L_p$, $1 \leq p \leq \infty$, where now the admissibility condition on the weights will depend on $p$. That is to say, for each $1 \leq p \leq \infty$, there exists a weight function $w^*_p$ such that the the optimal rate  $n^{-\frac{1}{2}-\frac{3}{2d}}$ is achieved.
     
    \item The weighted variation spaces lead to a new form of data-adaptive regularization for neural networks. Theoretical and experimental comparisons of this new form of regularization compared with more conventional regularization techniques is a direction of future work. Furthermore, extensions of this regularizer to deep neural networks is also a direction of future work.
\end{enumerate}
  
