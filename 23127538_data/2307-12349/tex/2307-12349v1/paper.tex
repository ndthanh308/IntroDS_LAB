\documentclass[10pt,journal,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
\fi

\usepackage{amsmath}

\ifCLASSOPTIONcompsoc
 \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
 \usepackage[caption=false,font=footnotesize]{subfig}
\fi

\usepackage{stfloats}

\usepackage{url}

\usepackage{amssymb}

\usepackage{booktabs}       %
\usepackage[hypcap=false]{caption}
\usepackage[table]{xcolor}         %
\usepackage{multirow}
\usepackage{enumitem} %
\usepackage{colortbl} %
\definecolor{tabletitle}{gray}{.8}
\definecolor{ours}{gray}{.95}
\definecolor{ggray}{RGB}{127,127,127}
\definecolor{reda}{RGB}{202,0,0}
\definecolor{redb}{RGB}{217,148,143}
\definecolor{myyellow}{RGB}{190,144,0}
\definecolor{mygreen}{RGB}{0,136,51}
\definecolor{myblue}{RGB}{0,102,204}
\newcolumntype{B}{!{\vrule width 1pt}}
\usepackage{pifont}
\newcommand{\yes}{\text{\ding{51}}}  %
\newcommand{\no}{\text{\ding{55}}}  %
\newcommand{\blank}{â€”}

\newcommand{\parahead}[1]{\noindent\textbf{#1}}
\newcommand{\best}[1]{\textcolor{reda}{\textbf{#1}}}
\newcommand{\second}[1]{\textcolor{mygreen}{\textbf{#1}}}
\newcommand{\third}[1]{\textcolor{myblue}{\textbf{#1}}}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\title{ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer}

\author{Youwei~Pang,~
  Xiaoqi~Zhao,~
  Lihe~Zhang,~
  and~Huchuan~Lu%
  \IEEEcompsocitemizethanks{
    \IEEEcompsocthanksitem All the authors are with Dalian University of Technology, Dalian, China.
    \IEEEcompsocthanksitem L. Zhang is the corresponding author.
  }%
}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}

\IEEEtitleabstractindextext{%
  \begin{abstract}
    Deep learning (DL) has advanced the field of dense prediction, while gradually dissolving the inherent barriers between different tasks.
    However, most existing works focus on designing architectures and constructing visual cues only for the specific task, which ignores the potential uniformity introduced by the DL paradigm.
    In this paper, we attempt to construct a novel \underline{ComP}lementary \underline{tr}ansformer, \textbf{ComPtr}, for diverse bi-source dense prediction tasks.
    Specifically, unlike existing methods that over-specialize in a single task or a subset of tasks, ComPtr starts from the more general concept of bi-source dense prediction.
    Based on the basic dependence on information complementarity, we propose consistency enhancement and difference awareness components with which ComPtr can evacuate and collect important visual semantic cues from different image sources for diverse tasks, respectively.
    ComPtr treats different inputs equally and builds an efficient dense interaction model in the form of sequence-to-sequence on top of the transformer.
    This task-generic design provides a smooth foundation for constructing the unified model that can simultaneously deal with various bi-source information.
    In extensive experiments across several representative vision tasks, i.e. remote sensing change detection, RGB-T crowd counting, RGB-D/T salient object detection, and RGB-D semantic segmentation, the proposed method consistently obtains favorable performance.
    The code will be available at \url{https://github.com/lartpang/ComPtr}.
  \end{abstract}

  \begin{IEEEkeywords}
    Bi-source Dense Prediction Tasks, Bi-source Transformer, Complementary Transformer, Task-generic Architecture.
  \end{IEEEkeywords}}

\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{W}{ith} the development of vision transformer~\cite{ViT,Swin}, recent works achieve stunning performance on many important benchmarks.
The task-generic characteristic of the sequence-to-sequence interaction architecture prompts the gaps among different tasks to be further eliminated.

Dense prediction is an important and fundamental problem of computer vision.
It often requires laborious and dense annotations of various objects of interest.
Typical tasks include remote sensing image segmentation~\cite{changedetection-LEVIR,changedetection-SYSU}, crowd counting (density map estimation)~\cite{rgbtcc-TAFNet-RGBTCC}, salient object detection~\cite{RGBDSODSurveyZhou,CMMSODSurvey,WWGSODSurvey}, and scene image semantic segmentation~\cite{NYUDv2-40classes}.
Existing schemes have achieved impressive performance in their respective tasks.
Among them, the bi-source input paradigm is a common style due to its practicability and effectiveness.
For example, the exploration of scene monitoring in a large time span and scene understanding in complex environments rely on more auxiliary data for joint information mining.
However, because of the fragmented definitions, these approaches usually focus on a single task.
This task-specific design brings repetitive exploration, redundant parameters, and resource overhead.
In this work, we explore the potential commonalities of architecture design and visual semantics among various tasks.

% Figure environment removed

\begin{table*}[!t]
  \centering
  \resizebox{\linewidth}{!}{%
    \input{tables/task.tex}
  }
  \vspace{-1em}
  \caption{Attributes of several typical bi-source dense prediction tasks.  }
  \label{tab:task}
  \vspace{-1em}
\end{table*}

% Figure environment removed

The complementarity between two inputs can be decomposed into consistency and difference.
Based on it, we design a simple yet effective framework ComPtr, as shown in Fig.~\ref{fig:net}, to address the general bi-source vision modeling.
Specifically, we design a complementary transformer, which contains consistency enhancement blocks and difference awareness blocks, {\em i.e.}, CEM and DAM.
The ``consistency'' tends to highlight the commonality between multi-source data.
The CEM blocks are equipped in the encoder to strengthen the object-related representation.
While the ``difference'' depicts the specificity of each stream.
We build the DAM-based decoder, which continuously endows the enhanced common representation with scene details from different sources to achieve accurate predictions.

The global dense interaction based on the transformer plays an important role.
However, its computational complexity increases quadratically with the length of the input token sequence.
This inherent property in the structure of the standard transformer leads to a severe computational burden when dealing with high-resolution features.
To alleviate this problem, we design the aggregation-diffusion attention inspired by the K-means algorithm.
Specifically, several specific proxy prototypes are inserted into the feature interaction path.
It provides an alternative of linear complexity for global information propagation, of which the schematic diagram is shown in Fig.~\ref{fig:agg-diff}.

The main contributions of this paper can be summarized as follows:

\begin{enumerate}
  \item To the best of our knowledge, it is the first attempt to accomplish the unification of the network architecture for diverse bi-source dense prediction tasks.
  \item We build a complementary transformer, ComPtr, by deconstructing the complementarity property of tasks into two aspects: consistency and difference, which effectively enhances global feature interaction and narrows the gaps among multiple tasks.
  \item A novel proxy prototype bridging strategy is proposed to construct the aggregation-diffusion attention mechanism, which helps to improve and simplify the global information propagation process.
  \item ComPtr achieves state-of-the-art performance on several challenging dataset benchmarks of five representative tasks.\footnote{It is necessary to emphasize that \textbf{our core objective is to design a simple yet efficient unified architecture for diverse bi-source dense prediction tasks, rather than building an optimal solution for a specific task}. More details can be seen in Sec.~\ref{sec:bisourcetask} and Tab.~\ref{tab:task}.}
\end{enumerate}

\section{Related Work}

\subsection{Bi-source Dense Prediction}\label{sec:bisourcetask}

The dense prediction field has a large number of branches, and they have a variety of data forms and task objectives.
The bi-source dense prediction tasks can be summarized based on task attributes as listed in Tab.~\ref{tab:task}.
From the forms of inputs, there are two categories: \textit{single-modal data source} and \textit{multi-modal data source}.
According to the attributes of objects, they can be divided into three aspects: \textit{class-agnostic}, \textit{single-class}, and \textit{multi-class} tasks.
In addition, their outputs are \textit{complete region segmentation} ({\em i.e.}, confidence or binary map) or \textit{rough point location} ({\em i.e.}, density estimation).
Motivated by these considerations, we select several representative bi-source tasks to explore the general modeling in this work.
As shown in Tab.~\ref{tab:task}, they cover all the aforementioned task attributes.

\subsubsection{Task-specific Methods}

Task-specific methods dominate different tasks, and they use the characteristics and prior information of the task itself as a guide for model design.
In \textit{\textbf{remote sensing change detection}} (RSCD) that aims to explore the dissimilarity of the specific content of bi-temporal images, the exploration of task-inspired difference information has been the main concern of related works.
The pioneering work~\cite{changedetection-FCNN} proposes a fully convolutional siamese neural network to perform change detection and this classical architecture is inherited by many follow-up methods.
And, single-temporal image segmentation~\cite{changedetection-FC-EF-Res,changedetection-DTCDSCN} and edge detection~\cite{changedetection-EGRCNN}, which are closely related to RSCD, are introduced as auxiliaries to construct multi-task learning.
To enhance the mining process of difference information, recent works~\cite{changedetection-IFN,changedetection-BIT,changedetection-ICIF-Net,changedetection-DMINet} begin to incorporate the transformer structure and attention mechanism to model contexts within spatial and temporal domains.
In \textit{\textbf{RGB-T crowd counting}}, the thermal modality plays an important role in poor illumination conditions and is further highlighted because of its sensitivity to the human body.
Existing two-stream methods~\cite{rgbtcc-CMCRL,rgbtcc-DEFNet} and three-stream~\cite{rgbtcc-TAFNet-RGBTCC} aggregate complementary information by designing complex cross-modal interaction structures.
Recent work \cite{rgbtcc-MAT} introduces the mutual attention transformer to leverage the cross-modal information.
In \textit{\textbf{RGB-D/T salient object detection}}, the two-stream architecture is the mainstream.
\cite{HDFNet} uses the dynamic convolution module with depth information to guide feature decoding and achieves more flexible and efficient multi-scale cross-modal feature processing.
\cite{ECFFNet-RGBTSOD} designs an effective cross-modal fusion and enables the salient boundary via the bilateral fusion strategy.
Recently, by introducing the transformer-based dense interaction component, \cite{TriTransNet-RGBDSOD,rgbdsod-VST,rgbdtsod-SwinNet,caver-tip} achieve superior results.
In \textit{\textbf{RGB-D semantic segmentation}}, the geometric prior of depth information is fully depicted and utilized.
\cite{rgbdss-3DN-Conv,rgbdss-SGNet} use the depth modality to help the convolution layer adjust the receptive field and adapt to geometric transformations.
\cite{rgbdss-ShapeConv} designs a depth-specific convolution layer and pays more attention to shape information by re-weighting the shape component and the base component of the depth feature.
And \cite{rgbdss-DCANet} proposes a pixel differential convolution attention module to capture long-range dependency for RGB data and local geometric consistency for depth data.
\cite{rgbdss-3DGNN} builds a k-nearest neighbor graph based on position and depth information.
All the above-mentioned schemes, without exception, focus on task-specific information mining.
They make full use of task-prior knowledge to obtain more inductive bias.

% Figure environment removed

\subsubsection{Task-generic Methods}

The task-generic design can improve the efficiency of data utilization and take a solid step towards a more intelligent algorithm by integrating knowledge of various tasks.
The existing task-generic models can be roughly classified into three categories: the large-scale pre-training task-independent architecture~\cite{BERT,GPT-3}, the multi-task joint learning framework~\cite{generalmodel-Unicorn} and the transferable general single-task architecture~\cite{rgbdtsod-SwinNet,caver-tip}.
This paper belongs to the last one.
Although several recent approaches~\cite{rgbdtsod-SwinNet,caver-tip} have attempted to explore the cross-task unified architecture, they only focus on a single field (SOD).
\textbf{There is a large room in the extension of task concepts, data forms, and object attributes, which is exactly what we focus on in this work.}

\subsection{Transformer-based Methods}

Long-range context information is crucial for accurate scene understanding, which has been verified in many existing works~\cite{DilatedConvolution,Deeplab,LargeKernel}.
Due to the built-in long-range dependence modeling without induction bias, the non-local operation like the non-local block~\cite{NonLocalNet} and self-attention in transformer~\cite{ViT,ViT-Adapter,Swin}, has become an important component for capturing the global context cues.
Recently, several transformer-based approaches have been proposed for bi-source dense prediction tasks.
Most of those methods~\cite{changedetection-DMINet,rgbdsod-VST,rgbdtsod-SwinNet,caver-tip,rgbtcc-RGBTMMCC,rgbtcc-MAT} follow the design pattern of using independent encoders for different sources.
They apply vision transformers to construct hierarchical representations and ignore the complementarity characterization in the encoder stage.
Moreover, their interaction units based on standard attention also limit the scalability of their structure~\cite{changedetection-DMINet,rgbdsod-VST,rgbtcc-MAT,caver-tip}.
Different from them, we use the proxy prototype mechanism inspired by the clustering algorithm to transform the global interaction of complementary information into two sequential steps of aggregation and diffusion.
This design effectively improves the scalability of the proposed architecture for different tasks and plays a positive role in realizing a general model for diverse bi-source dense prediction tasks.

\section{Methodology}

In this section, we first present the overall structure of the proposed ComPtr and then introduce the details of the different components in sequence.

\subsection{Overall Architecture}\label{sec:overallarchitecture}

Following the two-branch encoder-decoder architecture which is commonly used in diverse bi-source dense prediction tasks~\cite{changedetection-FCNN,TriTransNet-RGBDSOD,caver-tip,ECFFNet-RGBTSOD,VT5000-ADF}, we design a \textbf{comp}lementary \textbf{tr}ansformer (\textbf{ComPtr}) framework.
Considering that complementarity plays an important role in the bi-source dense prediction task, this concept is decomposed into two parts, {\em i.e.}, consistency and difference, in the proposed model.
And they lead the improvement and design of the encoder and decoder respectively.
As shown in Fig.~\ref{fig:net}, ComPtr is composed of these basic components: a \textit{parameter-shared feature extractor}, several \textit{consistency enhancement and difference awareness blocks}, and a \textit{light task-specific predictor}.

% Figure environment removed

\subsubsection{Consistency-Enhanced Encoder}

We directly choose the relatively frequently-used Swin Transformer~\cite{Swin} to realize the function of feature extraction which consists of several \textit{non-overlap patch embedding layers} and \textit{swin transformer blocks} built on the shifted window based self-attention.
\textbf{The siamese design with shared parameters is also introduced here} like~\cite{changedetection-FCNN} to simplify the model design and reduce the number of parameters.
Besides, the proposed consistency enhancement component is inserted into the basic feature extractor to build the consistency enhanced encoder as depicted on the left of Fig.~\ref{fig:net}, which allows each stream to be strengthened with specific important cues while maintaining its own specificity.

\subsubsection{Difference-Aware Decoder}

In the decoder, the deepest two-stream features are merged by a simple linear fusion layer only containing normalization and linear layers.
The result is delivered to a cascaded difference awareness block and intersects with the shallow encoder features, which absorbs the difference information at the specific scale between the two streams.
Together with the subsequent stacked interpolation operations, difference awareness blocks, and task-specific predictor, they form the difference-aware decoder as shown on the right of Fig~\ref{fig:net}.
On the basis of features enhanced by consistency-related patterns, the model continuously absorbs inter-source specificity information at different scales to achieve accurate task-specific decoding.

\subsubsection{Light Task-specific predictor}

To adapt to different types of tasks, we introduce several lightweight task-specific predictors by simply combining interpolation, normalization, activation, and linear layers.

\subsection{Complementary Transformer}\label{sec:complementarytransoformer}

The consistency enhancement and difference awareness blocks are all based on a novel and efficient global aggregation-diffusion attention mechanism (ADA), but there are obvious differences in the details.

% Figure environment removed

\subsubsection{Aggregation-Diffusion Attention}\label{sec:aggdffattn}

In order to build a more effective dense interaction between the features from the two sources to promote the mining and dissemination of global complementary information, we introduce the transformer architecture.
Although it has powerful information modeling and interaction capabilities, its global dense interaction form also causes a non-negligible quadratic computational complexity with respect to the length of the input sequence.
Inspired by the K-means clustering algorithm, we introduce the proxy prototype as the mediator for global information propagation.
And the straight-through dense interaction of the standard attention is decoupled and reconstructed into two different stages: the forward global aggregation and the backward information diffusion as depicted in Fig.~\ref{fig:attention}.

In the \textbf{aggregation stage}, the attribute-specific key and value $K^{fw}, V^{fw} \in \mathbb{R}^{L \times D} (L = H \times W)$ are generated from two-stream source feature set $S=\{F^i\}^2_{i=1}, F^i \in \mathbb{R}^{L \times C}$ by the complementarity operation $\mathtt{CompOps}$.
And $K$ learnable $D$-dimensional proxy prototype vectors $P = \{p_i\}_{i=1}^{K}$ are also transformed to the same channel dimension $D$ by a linear projection weight $W^{fw}_{Q}$.
The above process can be formulated as:
\begin{gather}
  K^{fw}, V^{fw} = \mathtt{CompOps}(F^1, F^2), \label{equ:compops} \\
  Q^{fw} = P W^{fw}_{Q}, \quad W^{fw}_{Q} \in \mathbb{R}^{D \times D}.
\end{gather}
Inspired by the similarity-based information aggregation in K-means, the pair-wise $\mathtt{cosine}$ similarity between the source embedding and the proxy prototype $P$ is served as the basis for assigning the global information in the aggregation process, which can be expressed mathematically as:
\begin{equation}
  \begin{gathered}
    A^{fw} = \mathtt{cosine}(Q^{fw}, K^{fw}).
  \end{gathered}
\end{equation}
The next steps are similar to the centralization process in K-means.
Different from the hard sample selection mechanism ($\mathtt{argmax}$) in K-means, $\mathtt{softmax}$ is chosen to soften the process and ensure the differentiability of the calculation.
So we can get the normalized similarity as follows:
\begin{equation}
  \begin{gathered}
    \tilde{A}^{fw} = \mathtt{softmax}(A^{fw}).
  \end{gathered}
\end{equation}
And a similarity-based transformation is introduced to integrate the more important cues from the transformed input $V^{fw}$ into the proxy prototype $P$.
The updated proxy prototype $\tilde{P} \in \mathbb{R}^{K \times D}$ can be obtained as follows:
\begin{equation}
  \begin{gathered}
    \tilde{P} = \mathtt{FFN}((\tilde{A}^{fw} V^{fw}) W^{fw}_{O}), \quad W^{fw}_{O} \in \mathbb{R}^{D \times D},
  \end{gathered}
\end{equation}
where $\mathtt{FFN}$ is a forward feed network containing normalization, linear and activation layers.

And in the \textbf{diffusion stage}, the proxy prototype $\tilde{P}$, acting as a mediator, diffuse these global semantics back into the original image space, and this procedure can also be considered as reversed K-means.
At this time, the image feature $F \in \mathbb{R}^{L' \times C}$, {\em i.e.}, the diffusion slot, can supplement the complement information from the prototype $\tilde{P}$ based on the similarity relationship.
Specifically, after the specific linear transformation by $W^{bw}_{Q}$ and $W^{bw}_{K}$, both $F$ and $\tilde{P}$ are involved in the calculation of the correlation, which can generate the similarity $A^{bw}$ after the $\mathtt{cosine}$ operation as following:
\begin{equation}
  \begin{gathered}
    Q^{bw} = F W^{bw}_{Q}, \quad W^{bw}_{Q} \in \mathbb{R}^{C \times D}, \\
    K^{bw} = \tilde{P} W^{bw}_{K}, \quad W^{bw}_{K} \in \mathbb{R}^{D \times D}, \\
    A^{bw} = \mathtt{cosine}(Q^{bw}, K^{bw}).
  \end{gathered}
\end{equation}
The $\mathtt{softmax}$ function is imposed on $A^{bw}$ to normalize the importance of different prototypes and modulate the subsequent feature reconstruction based on $\tilde{P}$.
And it can be mathematically expressed as follows:
\begin{equation}
  \begin{gathered}
    \tilde{A}^{bw} = \mathtt{softmax}(A^{bw}), \\
    V^{bw} = \tilde{P} W^{bw}_{V}, \quad W^{bw}_{V} \in \mathbb{R}^{D \times D}, \\
    Z^{bw} = \tilde{A}^{bw} V^{bw} W^{bw}_{O}, \quad W^{bw}_{O} \in \mathbb{R}^{D \times C}.
  \end{gathered}
\end{equation}
And the residual connection and learnable vector $\gamma \in \mathbb{R}^{D}$ are introduced to balance the attention output $Z^{bw}$ and the input slot feature $F$, where $\gamma$ is initialized with the zero value to ensure the original feature distribution in the early stage of training as~\cite{ViT-Adapter}:
\begin{equation}
  \begin{gathered}
    \tilde{F} = F + \mathtt{FFN}(F + \gamma \odot Z^{bw}),
  \end{gathered}
\end{equation}
where $\odot$ denotes the element-wise multiplication.
The successive aggregation and diffusion phases realize the global interaction based on the proxy prototype.

\subsubsection{Comparison with standard attention}\label{sec:compwithstdattn}

For a $D$-dimensional image token sequence of length $L = H \times W$, the computational complexity and the memory footprint of the standard attention are approximately $O(L^{2} \times D)$ and $O(L^{2})$.
Compared with the standard attention, the proposed proxy prototype bridging strategy effectively reduces the computation and memory of dense global interaction to linear complexity ($O(L \times K \times D + L \times K \times D)$ and $O(L \times K + L \times K)$), where $K$ is the number of proxy prototypes.
Considering that $K$ is a small constant, {\em i.e.} $K \ll L$, the proposed aggregation-diffusion attention is more efficient than the standard form.
In order to visualize the difference in inference efficiency, we show the comparison of the proposed ADA with the standard attention for different resolutions of the input in Fig.~\ref{fig:efficiency}, and more details and analyses can be found in Sec.~\ref{sec:num_k}.

\subsubsection{Consistency Enhancement Block (CEB)}\label{sec:consistencyblock}
By introducing the consistency operation into the proposed aggregation-diffusion attention, we design the CEB to strengthen the common visual cues and help the model obtain a more powerful data representation.
And the consistency-specialized key and value embeddings $K_{con}$ and $V_{con}$ can be obtained by the multi-granularity element-wise multiplication of the two image source feature inputs $F^1$ and $F^2$ from different streams as shown on the top right-hand corner of Fig.~\ref{fig:attention}.
They are used as $K^{fw}$ and $V^{fw}$ in Equ.~\ref{equ:compops}.
Specifically, the consistency operation is composed of several normalization, linear, and pooling layers.
With the aid of two parameter-free average pooling layers, we can obtain products $\{F^i_{con}\}_{i=1}^3$ from three different scales.
After concatenating and transformed along the channel, a complete multi-scale consistency representation is generated from the inputs.
It is split into $K_{con}$ and $V_{con}$ along the channel which participate in the update process of the proxy prototype $P$.
In the diffusion stage, the slot feature $F$ with $L' = 2HW$ is the concatenation of the image features $F^1$ and $F^2$ along the flatten spatial dimension.
It is reconstructed independently to $\tilde{F}$ based on the updated proxy prototype $\tilde{P}$.
And these enhanced features $\tilde{F}^1$ and $\tilde{F}^2$ stored in the slot $\tilde{F}$ replace original $F^1$ and $F^2$ as the input of the next swin transformer block.

\subsubsection{Difference Awareness Block (DAB)}\label{sec:differenceblock}

This block differs from the CEB in two aspects, one is the generation of embeddings $K_{diff}$ and $V_{diff}$ based on the source features, and the other is the global diffusion way of the assigned information.
As shown on the bottom right-hand corner of Fig.~\ref{fig:attention}, a hierarchical difference operation containing normalization, linear and pooling layers are embedded here to obtain the multi-granularity absolute difference representation $K_{diff}$ and $V_{diff}$, which are used as $K^{fw}$ and $V^{fw}$ in Equ.~\ref{equ:compops}.
Besides, the slot feature $F$ with $L' = HW$ in the diffusion stage is a compact hybrid representation, which comes from the initial fusion of the encoder features $F^1$ and $F^2$, and the deeper DAB feature $\tilde{F}'$ through several stacked linear layers.
The output slot feature $\tilde{F}$ of the block is fed into the subsequent module and involved in the decoding process of the high-resolution prediction.

\subsubsection{Loss Function}

For a fair comparison with existing approaches, we use the commonly used loss functions for each task.
In remote sensing change detection, RGB-D/T salient object detection, and RGB-D semantic segmentation tasks, the binary or multi-class cross-entropy loss is used to supervise the training of the model.
In RGB-T crowd counting, we follow the loss form of~\cite{rgbtcc-RGBTMMCC} containing the distribution matching loss proposed in~\cite{cc-dm} for density map estimation and an L1 loss for counting regression.

\section{Experiments}

\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{%
    \rowcolors{2}{gray!10}{white}
    \input{tables/rscd.tex}
  }
  \caption{Comparison on the testing datasets of LEVIR-CD and SYSU-CD for RSCD.
    \textbf{Colors \textcolor{reda}{red}, \textcolor{mygreen}{green} and \textcolor{myblue}{blue} in all tables represent the first, second and third ranked results.}
  }
  \label{tab:rscd}
\end{table}

\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{%
    \rowcolors{2}{gray!10}{white}
    \input{tables/rgbtcc.tex}
  }
  \caption{Comparison on RGBT-CC~\cite{rgbtcc-TAFNet-RGBTCC} testing dataset.
    The top six single-modal methods are retrained by~\cite{rgbtcc-RGBTMMCC} and the others are bi-modal methods.}
  \label{tab:rgbtcc}
\end{table}

\begin{table*}[!t]
  \centering
  \resizebox{\linewidth}{!}{%
    \rowcolors{2}{gray!10}{white}
    \input{tables/rgbdsod.tex}
  }
  \caption{Experiments on five RGB-D SOD benchmarks.
    ``$\star$'': Using the multi-scale training strategy.
  }
  \label{tab:rgbdsod}
\end{table*}

\begin{table*}[!t]
  \centering
  \resizebox{\linewidth}{!}{%
    \rowcolors{2}{gray!10}{white}
    \input{tables/rgbtsod-full.tex}
  }
  \caption{Comparison on three RGB-T SOD benchmarks.
    ``\blank'': Traditional methods based on hand-crafted features.
  }
  \label{tab:rgbtsod}
  \vspace{0em}
\end{table*}

\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{%
    \rowcolors{2}{gray!10}{white}
    \input{tables/rgbdss.tex}
  }
  \caption{Comparison on NYU-Depth V2 (40 classes)~\cite{NYUDv2-40classes} benchmark for RGB-D SS.
    ``\blank'': Not available.
  }
  \label{tab:rgbdss}
\end{table}

To verify the generality, transferability and effectiveness of the proposed architecture, we select four types of important bi-source tasks with large diversity: remote sensing change detection (RSCD), RGB-T crowd counting (RGB-T CC), RGB-D/T salient object detection (RGB-D/T SOD), and RGB-D semantic segmentation (RGB-D SS).
Specific experimental details are presented in the following sections.

\subsection{Basic Training Settings}

In all experiments, we use Swin Transformer~\cite{Swin} pre-trained from ImageNet~\cite{ImageNet} as the initial parameters of the feature extractor for all tasks, while other parts are randomly initialized by PyTorch.
The three-channel image is normalized, while the single-channel one only replicates three times along the channel after dividing by 255.
The AdamW~\cite{AdamW} optimizer is consistently used due to its suitability for the transformer architecture.
The settings of other hyper-parameters for different tasks mostly follow those of the existing algorithms~\cite{changedetection-DMINet,rgbtcc-RGBTMMCC,caver-tip,rgbdtsod-SwinNet,SPNet-RGBDSOD-journal,ECFFNet-RGBTSOD,rgbdss-SA-Gate} to achieve a more fair comparison, and more task-specific details are shown in the corresponding sections.

\subsection{Remote Sensing Change Detection (RSCD)}\label{sec:rscd-setting}

\noindent\textbf{Datasets.}
Two commonly used large-scale data benchmarks are used to evaluate performance, namely LEarning, VIsion, and Remote sensing (LEVIR-CD)~\cite{changedetection-LEVIR} and Sun Yat-Sen University Dataset (SYSU-CD)~\cite{changedetection-SYSU}.
\textbf{LEVIR-CD} is a large-scale and very-high-resolution (VHR) RSCD dataset collected via Google Earth API.
It contains 637 pairs of real bi-temporal RGB image patches with a time span of 5-14 years, a size of $1024 \times 1024$, a spatial resolution of 0.5 m/pixel and a total of 31333 individual change buildings.
The dataset is split into training/validation/testing sets in a 7:1:2 ratio and each sample is cropped to 16 small patches with a size of $256 \times 256$ for decreasing the computational cost.
The recently proposed \textbf{SYSU-CD} is another important and challenging benchmark containing 20000 pairs of 0.5 m/pixel aerial images taken between the years 2007 and 2014 in Hong Kong.
These images with a shape of $256 \times 256$ are divided into training/validation/testing sets with 12000/4000/4000 image pairs.
The model is trained, validated, and tested independently on each dataset.
Following the existing methods~\cite{changedetection-DMINet,changedetection-ICIF-Net,changedetection-BIT}, image patches with a shape of $256 \times 256$ and a batch size of 16 are augmented using random scaling, affine transformation, flipping, and color jitter during training.
For the experiments on the LEVIR-CD dataset, we train the model for 200 epochs and use the cosine scheduler with an initial learning rate of 0.0002 and a weight decay of 0.0005.
On SYSU-CD, to alleviate the over-fitting problem, we introduced the one-cycle scheduler~\cite{onecycle} with a large learning rate of 0.001 and a weight decay of 0.0005, which can play a role of regularization, and the number of training epochs is reduced to a quarter of the one on LEVIR-CD.

\noindent\textbf{Metrics.}
For the comparison between the change prediction map and the ground truth, we introduce five common binary image similarity metrics, including \textit{precision} (Pre.), \textit{recall} (Rec.), \textit{F1-score} (F1), \textit{intersection over union} (IOU) and \textit{overall accuracy} (OA).
And they can be mathematically formalized as follows:
\begin{gather}
  \mathrm{Pre.} = \frac{TP}{TP + FP}, \\
  \mathrm{Rec.} = \frac{TP}{TP + FN}, \\
  \mathrm{F1}   = \frac{2TP}{2TP + FN + FP}, \\ %
  \mathrm{IOU}  = \frac{TP}{TP + FP + FN}, \\ %
  \mathrm{OA}   = \frac{TP + TN}{TP + TN + FP + FN},
\end{gather}
where TP, TN, FP, and FN are the number of true positive, true negative, false positive, and false negative samples in the binary prediction, respectively.
Since F1 and IOU implicitly consider both Pre. and Rec., they can reflect the performance of the model more generally.

% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{RGB-T Crowd Counting (RGB-T CC)}\label{sec:rgbtcc-setting}

\noindent\textbf{Datasets.}
In the pioneering work~\cite{rgbtcc-TAFNet-RGBTCC}, a new large-scale point-annotated dataset benchmark \textbf{RGBT-CC} is carefully collected and manually aligned by an optical-thermal camera and contains 2030 RGB-T pairs with 138389 point-annotated people.
All samples have a standard resolution of $640 \times 480$.
1013 pairs are captured in the light and 1017 pairs are taken from some dark scenes.
The number of the training, validation and testing sets is 1030, 200 and 800, respectively.
The model is trained, validated, and tested independently on these datasets like the existing methods~\cite{changedetection-DMINet,rgbtcc-RGBTMMCC}.
All settings are aligned with the recent best practice~\cite{rgbtcc-RGBTMMCC}, where the batch size, learning rate, weight decay, epoch, and crop size are set to 16, 0.00001, 0.0001, 500, $224 \times 224$.
And no data augmentation other than random cropping is used for this task.

\noindent\textbf{Metrics.}
For RGB-T CC, five regression metrics are utilized to evaluate the regression error, which include four levels of \textit{grid average mean absolute error}~\cite{cc-GAME} (GAME$_i$, $i \in \{0, 1, 2, 3\}$) and \textit{root mean square error} (RMSE).
To calculate GAME, the density prediction and the ground truth are grided into $4^i$ ($i \in \{0, 1, 2, 3\}$) parts.
\begin{gather}
  \mathrm{GAME}_{i} = \frac{1}{N} \sum^{N}_{n=1} \sum^{4^i}_{j=1} | P^j_n - G^j_n |,
\end{gather}
where $P^j_n$ and $G^j_n$ are the count prediction and ground truth in $j^{th}$ part of $n^{th}$ test image pairs.
RMSE is mathematically defined as:
\begin{gather}
  \mathrm{RMSE} = \sqrt{\frac{1}{N} \sum^{N}_{n=1} (P_n - G_n)^2},
\end{gather}
where $P_n$ and $G_n$ are the predicted count and ground truth of $n^{th}$ test image pairs.

\subsection{RGB-D/T Salient Object Detection (RGB-D SOD)}\label{sec:rgbdtsod-setting}

\noindent\textbf{Datasets.}
For RGB-D SOD, we introduce five datasets NJUD~\cite{NJUD}, NLPR~\cite{NLPR}, STEREO1000~\cite{STEREO}, SIP~\cite{SIP}, and DUTLF-Depth~\cite{DUTRGBD}.
\textbf{NJUD} consists of 1985 image pairs involving a lot of complex scenarios.
In \textbf{NLPR}, 1000 RGB-D image pairs are collected from diverse indoor and outdoor scenes.
\textbf{STEREO1000} is composed of 1000 RGB-D image pairs from Flickr, NVIDIA 3D Vision Live and Stereoscopic Image Gallery.
\textbf{SIP} containing 929 pairs of high-resolution person images is captured in an outdoor scene.
Complex lighting conditions, extreme contrast, and variable pose of people all increase the difficulty of this dataset.
\textbf{DUTLF-Depth} is larger in scale and its content is also much richer.
It collects 800 pairs of indoor and 400 pairs of outdoor RGB-D images, which contain a large number of challenging objects and scenes.
Continuing the setup of existing methods~\cite{caver-tip,HAINet-RGBDSOD,TriTransNet-RGBDSOD}, we directly use the same 2985 images from NJUD, NLPR, and DUTLF-Depth to train the model and evaluate it on the remaining data.
And for RGB-T SOD, VT821~\cite{VT821-MTMR}, VT1000~\cite{VT1000-SDGL}, and VT5000~\cite{VT5000-ADF} are introduced as the benchmark.
\textbf{VT821} is a pioneering RGB-T SOD datasets, which includes 821 RGB-Thermal-GT image pairs.
\textbf{VT1000} containing 1000 pairs of images, increases the scale of the RGB-T SOD dataset and covers more than 400 kinds of objects in more than 10 types of scenes with diverse lighting conditions.
\textbf{VT5000} contains 5000 pairs of densely annotated RGB-T images, which greatly enriches the diversity and complexity of the task.
And this dataset is divided into training and testing sets, each of which has 2500 image pairs.
Except for the training set containing 2500 image pairs of VT5000 for training, the rest of the data are used for testing as~\cite{rgbdtsod-SwinNet,caver-tip}.
On RGB-D/T SOD, the batch size, learning rate, and input shape are consistently set to 16, 0.0002, and $256 \times 256$ for ComPtr-T as~\cite{caver-tip} ($384 \times 384$ for ComPtr-B as~\cite{rgbdtsod-SwinNet}).
Following~\cite{SPNet-RGBDSOD-journal,ECFFNet-RGBTSOD,rgbdtsod-SwinNet,caver-tip}, random affine transformation, flipping, color jittering are introduced as the data augmentation and our models are trained for 100 epochs in RGB-T SOD and 200 epochs in RGB-D SOD with the cosine scheduler.

\noindent\textbf{Metrics.}
For RGB-D SOD, all methods are evaluated based on five gray-scale image metrics,
\textit{S-measure}~\cite{Smeasure} ($S_m$),
\textit{maximum F-measure}~\cite{Fmeasure} ($F_{\beta}$),
\textit{maximum E-measure}~\cite{Emeasure} ($E_{m}$),
\textit{weighted F-measure}~\cite{wFmeasure} ($F^{\omega}_{\beta}$),
and \textit{MAE}.
$S_m$ focuses on region-aware and object-aware structural similarities $S_r$ and $S_o$, between the saliency map and the ground truth. It can be expressed as: $S_m = \alpha S_o + (1 - \alpha) S_r$, where $\alpha$ is set to $0.5$.
$F_{\beta}$ is a region-based similarity metric based on precision and recall. The mathematical form is: $F_{\beta} = \frac{(1 + \beta^2) precision \times recall}{\beta^2 precision + recall}$, where $\beta^2$ is generally set to $0.3$ to emphasize more on the precision.
$E_m$ is composed of local pixel values and the image-level mean value to jointly evaluate the similarity between the prediction and the ground truth.
$F^{\omega}_{\beta}$ improves the existing metric $F_{\beta}$ by using a weighted precision for measuring exactness and a weighted recall for measuring completeness.
MAE indicates the average absolute pixel error.
It is worth noting that the evaluation of the SOD task is based on the gray-scale prediction map.
The value of the predicted result indicates the probability of the corresponding position belonging to the salient object region.
So, both the calculation process and the formula details of $F_{\beta}$ and $F1$ mentioned in Sec.~\ref{sec:rscd-setting} are different, which serves the specific task requirements.

% Figure environment removed

% Figure environment removed

\subsection{RGB-D Semantic Segmentation (RGB-D SS)}\label{sec:rgbdss-setting}

\noindent\textbf{Datasets.}
We also conduct extended experiments and analysis on the popular indoor \textbf{RGB-D SS} benchmark, NYU-Depth V2 dataset with 40 classes~\cite{NYUDv2-40classes}.
It contains 1449 pairs of RGB-D images with the shape of $480 \times 640$, which is captured in the indoor scene.
It is a representative and challenging RGB-D semantic segmentation dataset and includes complex and diverse indoor categories such as scene structures like walls and floors, furniture items like beds and chairs, and objects like lamps and bags.
There are 795 image pairs for training and 654 image pairs for testing as~\cite{rgbdss-SA-Gate,rgbdss-ShapeConv}.
Following the practices and strategies of the existing works~\cite{rgbdss-PDCNet,rgbdss-DCANet,rgbdss-ShapeConv,rgbdss-NANet,rgbdss-SGNet,rgbdss-SA-Gate}, we employ some general data augmentation strategies as~\cite{rgbdss-SA-Gate,rgbdss-SGNet}, including random flipping, scaling and cropping, and set the number of epochs as 400, batch size as 8, initial learning rate as 0.00001, weight decay as 0.0001, and input size as $480 \times 480$, respectively.
The poly learning rate scheduler~\cite{poly} with a warm-up stage and a factor of 0.9 is imposed on the AdamW optimizer~\cite{AdamW}.

\noindent\textbf{Metrics.}
For RGB-D SS, three commonly used metrics, \textit{pixel accuracy} (Pixel Acc.), \textit{mean accuracy} (Mean Acc.), and \textit{mean region intersection over union} (Mean IOU) are reported here.
And they are defined as follows:
\begin{gather}
  \mathrm{Pixel\ Acc.} = \frac{\sum_i N_{i \rightarrow i}}{\sum^{N_{cls}}_{j} \sum_k N_{j \rightarrow k}}, \\
  \mathrm{Mean\ Acc.}  = \frac{1}{N_{cls}} \frac{\sum_i N_{i \rightarrow i}}{\sum_k N_{i \rightarrow k}}, \\
  \mathrm{Mean\ IOU}   = \frac{1}{N_{cls}} \frac{\sum_i N_{i \rightarrow i}}{\sum_k N_{i \rightarrow k} + \sum_k N_{k \rightarrow i} - N_{i \rightarrow i}},
\end{gather}
where $N_{i \rightarrow j}$ represents the number of pixels of class $i$ classified as class $j$ by the model and $N_{cls}$ is the number of classes in the dataset.

\subsection{Comparison with State-of-the-Art Methods}

We evaluate the competitiveness of our ComPtr by carefully comparing it with the state-of-the-art on different tasks.
There are 12 models and variants from~\cite{changedetection-FCNN,changedetection-FC-EF-Res,changedetection-IFN,changedetection-DTCDSCN,changedetection-SNUNet,changedetection-BIT,changedetection-EGRCNN,changedetection-MSPSNet,changedetection-DTCDSCN,changedetection-DMINet} for RSCD,
11 methods for RGB-T CC which contains 6 single-modal methods~\cite{cc-CSRNet,cc-BL,cc-dm,cc-P2PNet,cc-MARUNet-CFANet,cc-MAN} retrained by~\cite{rgbtcc-RGBTMMCC} and 5 bi-modal methods~\cite{rgbtcc-CMCRL,rgbtcc-TAFNet-RGBTCC,rgbtcc-MAT,rgbtcc-DEFNet,rgbtcc-RGBTMMCC},
15 methods~\cite{JLDCF-RGBDSOD-journal,HDFNet,SIP,RD3D-RGBDSOD,TriTransNet-RGBDSOD,DCF-RGBDSOD,DSA2F-RGBDSOD,COME15K-CMINet,SPNet-RGBDSOD-journal,rgbdsod-VST,HAINet-RGBDSOD,CCAFNet-RGBDSOD,DCMF-RGBDSOD,rgbdtsod-SwinNet,caver-tip} for RGB-D SOD,
10 methods~\cite{VT821-MTMR,M3S-NIR-RGBTSOD,VT5000-ADF,VT1000-SDGL,CGFNet-RGBTSOD,CSRNet-RGBTSOD,ECFFNet-RGBTSOD,MIDD-RGBTSOD,rgbdtsod-SwinNet,caver-tip} for RGB-T SOD,
and 13 methods~\cite{rgbdss-LSD-GF,rgbdss-MMAFNet,rgbdss-ACNet,rgbdss-RDFNet,rgbdss-3DGNN,rgbdss-SA-Gate,rgbdss-Malleable25D,rgbdss-SGNet,rgbdss-NANet,rgbdss-ShapeConv,rgbdss-InverseForm,rgbdss-DCANet,rgbdss-PDCNet} for RGB-D SS.

\begin{table*}[!t]
  \centering
  \resizebox{\linewidth}{!}{%
    \rowcolors{2}{gray!10}{white}
    \input{tables/num_k.tex}
  }
  \caption{Ablation on the number $K$ of the proxy prototypes in each attention layer.
    ``$K=4$'' is our final choice.
    ``$\infty$'': With the same number of proxy prototypes as that of the input tokens.
    ``Std.'': Dense interactive form of standard attention.
  }
  \label{tab:num_k}
\end{table*}

\begin{table*}[!t]
  \centering
  \resizebox{\linewidth}{!}{%
    \rowcolors{2}{gray!10}{white}
    \input{tables/ablation-cc-cd-sod.tex}
  }
  \caption{Ablation on the proposed components in the complementary transformer.
    ``$-$CEB'': Without the consistency enhancement block.
    ``$-$DAB'': Without the difference awareness block.
    ``$-$Both'': Baseline model which is a variant without the proposed blocks.
    ``$-$CompOps'': ComPtr variant without the complementarity operation.
  }
  \label{tab:ablation}
\end{table*}

\subsubsection{Quantitative Comparison}

All comparisons are listed in Tab.~\ref{tab:rscd}, Tab.~\ref{tab:rgbtcc}, Tab.~\ref{tab:rgbdsod}, Tab.~\ref{tab:rgbtsod}, and Tab.~\ref{tab:rgbdss}.
From the comparison in Tab.~\ref{tab:rscd}, our method does not obtain the best results on Pre. and Rec., but achieves significant leading performance on F1 and IOU in the RSCD task.
Compared with DMINet~\cite{changedetection-DMINet}, ``-T'' version achieves a relative improvement of 1.55\% F1 and 2.89\% IOU on LEVIR-CD, and ``-B'' version yields performance gains of 1.05\% F1 and 1.81\% IOU on SYSU-CD.
In RGB-T CC, ComPtr achieves an improvement of 0.38 GAME$_0$ and 0.31 RMSE over the recent~\cite{rgbtcc-RGBTMMCC} as shown in Tab.~\ref{tab:rgbtcc}.
With respect to two bi-modal SOD methods, CAVER~\cite{caver-tip} and SwinNet~\cite{rgbdtsod-SwinNet}, our performance is still superior, especially on DUTLF-Depth, SIP and STEREO1000 for RGB-D SOD in Tab.~\ref{tab:rgbdsod}, and VT821 and VT1000 for RGB-T SOD in Tab.~\ref{tab:rgbtsod}.
Besides, for more complex multi-class RGB-D SS in Tab.~\ref{tab:rgbdss}, our ComPtr-B achieves the best 55.5\% mean IOU on NYU-Depth V2.
The two versions, ``-T'' and ``-B '', perform differently on different tasks.
\textbf{\textit{There may be two underlying factors.
    On the one hand, it may be related to the different requirements of hyper-parameters during training for backbone with different volumes~\cite{dl-ScaleModel}.
    On the other hand, the different characteristics of these datasets result in different fitness for the swin architecture.}}
Actually, to avoid over-engineering, we do not validate more parameter combinations and structural variants.
And current results have demonstrated the generality of the proposed architecture and its adaptability to diverse task concepts, data forms, and object attributes.

\subsubsection{Qualitative Comparison}

In Fig.~\ref{fig:rscd-cmp}, Fig.~\ref{fig:rgbtcc-cmp}, Fig.~\ref{fig:rgbdsod-cmp}, Fig.~\ref{fig:rgbtsod-cmp}, and Fig.~\ref{fig:rgbdss-cmp}, we visualize the prediction maps of the proposed model and several state-of-the-art methods.
In addition to the results of our method, those of some recent methods with publicly available weight parameters or predictions are also listed.
Visually, the results of our algorithm are much closer to the ground truth, and also adapt well to the complex examples shown here.
For example, in the face of dense small buildings and independent large buildings in Fig.~\ref{fig:rscd-cmp}, and strong background interference in Fig.~\ref{fig:rgbdsod-cmp} and Fig.~\ref{fig:rgbtsod-cmp}, our algorithm performs more robustly.
For the prediction results of the RGB-T CC task in Fig.~\ref{fig:rgbtcc-cmp}, ours has a smaller absolute error compared to MMCC~\cite{rgbtcc-RGBTMMCC}'s.
It is also worth noting that our method still shows good crowd counting performance even in low-light scenarios due to the assistance of thermal infrared images.
Besides, as shown in Fig.~\ref{fig:rgbdss-cmp} for RGB-D SS, the predictions of our approaches also exhibit better intra-class consistency and inter-class differentiation.
These effects can be attributed to the generality of the concept of complementarity for these tasks and the effectiveness of component design based on consistency and difference.

\subsection{Ablation Studies}

% Figure environment removed

A series of ablation studies are presented in this section to reveal how the proposed components affect the final performance of the model.
These experiments are constructed on three representative tasks, RGB-T CC, RSCD and RGB-D SOD, with large-scale datasets of various forms and attributes, where we take Swin-T~\cite{Swin} as the backbone.

\subsubsection{Number of Proxy Prototypes}\label{sec:num_k}

The number of proxy prototypes is an important hyper-parameter of the proposed ADA.
To simplify the design process, the current version of the model uses the same number of prototypes in both types of embedded components.
We set five small constant values $\{1, 2, 4, 8, 16\}$ for $K$ as candidates and record the performance of the five variants in Tab.~\ref{tab:num_k} on three different tasks, RGB-T CC, RSCD and RGB-D SOD, respectively.
As we can see from the results, the sensitivity to $K$ varies across tasks.
\textit{RSCD and RGB-D SOD tasks are relatively more robust to the value of $K$.}
Considering the balance of performance across tasks, we choose $K=4$ as the final setting.
Besides, we also supplement two special cases, namely ``$K=\infty$'' where the value of $K$ is consistent with the number of input tokens and ``Std.'' where the proposed ADA is replaced with the full standard attention that also integrates CompOps for a reasonable comparison.
Notably, the version $K=4$ surpasses the two benchmark versions, reflecting the potential of more compact representations for consistency and difference modelling.
From the complexity statistics of different variants in Fig.~\ref{fig:efficiency}, we can see that the proposed strategy based on the information aggregation mediated by a small number of prototypes can effectively save the computational budget of the inference process, which actually holds true for the training phase as well, and improve the parallel efficiency of the model.

\subsubsection{Effectiveness of Proposed Components}

To verify the effectiveness of the proposed components, we remove all complementarity operations, CEBs and DABs, respectively, and evaluate these variants on the three datasets from different tasks, RGB-T CC, RSCD, and RGB-D SOD.
The results in Tab.~\ref{tab:ablation} demonstrate that these components provide a positive gain for the performance of the final model.
Besides, there is an interesting observation that removing the difference component causes significant performance degradation.
This highlights the importance of difference information for bi-source tasks. %
At the same time, the consistency component also reflects the positive effect.

\subsubsection{Feature Analysis}

To better understand the behavior of the proposed components, the analysis is carried out in dense multi-object scenario and sparse single-object scenario.
And the corresponding source features are visualized in Fig.~\ref{fig:feature}.
It is clear that the two operations play complementary roles in different stages.
In the shallow layer (Stage-1\&2), the detailed information from the difference operation and the object cues from the consistency operation are paid special attention.
And in the deep layer (Stage-3\&4), the two operations converge to the location of the object of interest, which is more obvious in the RGB-D SOD task.
Besides, in RSCD, it can be seen that the difference maps in the shallow features almost cover the whole image, which reflects the dependence of this task on the attribute.

\section{Conclusion}

In this paper, we design a simple yet effective task-generic framework, ComPtr, from the concept of complementarity.
It is common in bi-source information interaction for diverse dense prediction tasks with significantly different data patterns.
From the perspective of consistency and difference, we design the consistency enhancement block and the difference awareness block respectively, which meet the needs of feature extraction and prediction decoding for complementary information.
At the same time, we design an optimization strategy mediated by the proxy prototype, which effectively reduces the computational complexity of the global complementary information propagation process.
Based on it, we construct a novel clustering-inspired aggregation-diffusion attention mechanism to improve the flexibility and scalability of the model.
These simple yet effective designs make the proposed model no longer depend on task-specific data properties or forms, achieving a more general architecture.
Extensive experiments on five diverse dense prediction tasks verify the effectiveness of the proposed method with superior performance to existing state-of-the-art competitors.

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bib_lite}

\end{document}
