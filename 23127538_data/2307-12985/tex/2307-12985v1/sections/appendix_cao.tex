\section{Implementation details for Section~\ref{section_realData_nb}}
\label{appendix_cao}

In Section~\ref{section_realData_nb}, we analyze a publicly-available dataset that is associated with \cite{cao2020human}, which can be downloaded from \url{https://descartes.brotmanbaty.org/}. For the main cell type analysis, we used all cells from this dataset that were collected from the kidney. For the cell subtype analysis, we used all of the kidney cells that were labeled as metanephric cells in the original analysis by \cite{cao2020human}. For both analyses, we filtered to genes with non-zero counts in at least 10 cells. After this subsetting, the kidney dataset has dimension $178,603 \times 34,714$ and the metanephric dataset has dimension $90,876 \times 31,385$. 

To carry out Step 1 of Algorithm~\ref{alg_cao_intra} on our two datasets, we used the \texttt{Monocle3} package. To preprocess each dataset, we generated a 50-dimensional principal components embedding of each dataset and subsequently a 2-dimensional UMAP embedding using the default settings of the preprocessing functions in the \texttt{Monocle3} package. 
Next, we performed Leiden clustering using the the \texttt{Monocle3} clustering function. We chose a resolution parameter that gave a similar number of clusters to those obtained in the original paper. More specifically, we set the resolution parameter to $1 \times 10^{-6}$ for the full kidney dataset and $1 \times 10^{-5}$ for the metanephric cell subset. To carry out Step 2 of Algorithm~\ref{alg_cao_intra}, we split the 50-dimensional principal components embedding of the full count data from kidney and metanephric cells into five folds containing equal numbers of cells. We used the cluster labels, inferred as described above (on the full counts), as the ``true" cluster labels. For each of the five folds, we then trained a linear SVM model to predict the cluster assignment from 80\% of the embedded expression data. We generated a confusion matrix by comparing the ``true" labels to this trained model's predictions on the held-out subset. We note that this SVM is slightly different from that of \cite{cao2020human}, who trained their SVM using the whole transcriptome rather than the reduced-dimension embedding. 

To carry out the ``assume Poisson" version of Algorithm~\ref{alg_cs_intra} on our two datasets, we performed Poisson count splitting (Algorithm~\ref{alg_cs}, or, equivalently, Algorithm~\ref{alg_gcs} with $b'_j = \infty$) with $M = 2$ folds and $\epsilon_1 = \epsilon_2 = 0.5$  
on each dataset to obtain an $X^{(1)}$ and an $X^{(2)}$ for each of the datasets. We then followed the preprocessing and clustering procedures outlined above on each fold for each dataset, to obtain two clusterings for each dataset. To produce Figures~\ref{fig_realData_confusion}(b) and \ref{fig_realData_confusion}(f), we re-ordered the test set labels on the $y$-axis to make the confusion matrix as diagonal as possible (since the clusters are invariant to re-labeling).

Finally, to carry out the negative binomial version of Algorithm~\ref{alg_cs_intra} on our two datasets, we applied the \texttt{sctranstorm} package in \texttt{R} with its default parameters to estimate overdispersion values for each gene (see Appendix~\ref{appendix_sct}). We then performed negative binomial count splitting (Algorithm~\ref{alg_gcs}) with $M = 2$ folds and $\epsilon_1 = \epsilon_2 = 0.5$  on each dataset to obtain an $X^{(1)}$ and an $X^{(2)}$ for each of the datasets. We then proceeded as in the Poisson case. 
