\section{Negative binomial count splitting}
\label{section_gcs}

In this section, we consolidate some theoretical results from \cite{neufeld2022inference} and \cite{neufeld2023data} to facilitate their immediate application to scRNA-seq data. Results in this section are proven in Appendix~\ref{appendix_proofs}.
 
\subsection{Algorithm and main result}

We now introduce negative binomial count splitting and state the key result. 

\begin{algorithm}[Negative binomial count splitting]
\label{alg_gcs}
Let $X \in \mathbb{Z}_{\geq 0}^{n \times p}$. For a chosen $M \in \mathbb{Z}^{+}$, $b_j' \geq 0$ for $j=1,\ldots,p$, and $\epsilon_1,\ldots,\epsilon_M \in (0,1)$ such that $\sum_{m=1}^M \epsilon_m = 1$, draw \\ $\left(\bx_{ij}^{(1)}, \ldots, \bx_{ij}^{(M)}\right) \mid \bx_{ij}=X_{ij} \sim \mathrm{DirichletMultinomial}\left(X_{ij}, \epsilon_1 b_j', \ldots, \epsilon_M b_j' \right)$. 
\end{algorithm}

The marginals of a Dirichlet-multinomial distribution are beta-binomial, i.e. $\bxm_{ij} \mid \bx_{ij} = X_{ij} \sim \text{BetaBinomial}(X_{ij}, \epsilon_m b_j', (1-\epsilon_m) b_j')$. While binomial thinning has appeared in numerous papers as a way to construct training and test sets from count-valued data \citep{neufeld2022inference, sarkar2021separating, leiner2021data}, to our knowledge beta-binomial thinning has only been used for this purpose by \cite{neufeld2023data}. The beta-binomial thinning operator has appeared in the time series literature as far back as \cite{mckenzie1986autoregressive} for constructing autoregressive processes with negative binomial marginals. The following result, which appeared for $M=2$ in the context of autoregressive processes in \cite{joe1996time}, tells us what happens when $\bx_{ij} \sim \mathrm{NB}\left(\mu_{ij}, b_j\right)$ and we apply Algorithm~\ref{alg_gcs} with $b_j'=b_j$. 


\begin{theorem}
\label{theorem_nbthin}	
Let $X \in \mathbb{Z}_{\geq 0}^{n \times p}$ be a dataset such that the entries $X_{ij}$ are realizations of $\bx_{ij} \overset{\mathrm{ind.}}{\sim} \NB\left(\mu_{ij}, b_j \right)$. If we apply Algorithm~\ref{alg_gcs} to each element $X_{ij}$ of this data using $b_j'=b_j$,
then (1) $\bxm_{ij} \overset{\mathrm{ind.}}{\sim} \NB(\epsilon_m \mu_{ij}, \epsilon_m b_{j})$ for $i=1,\ldots, n$, $j=1,\ldots, p$, and (2) the folds $\xo, \ldots, \xM$ are mutually independent.
\end{theorem}

Drawing $(\bold{X}_{ij}^{(1)}, \ldots, \bold{X}_{ij}^{(M)}) \mid \bold{X}_{ij}=X_{ij}$ from a Dirichlet-multinomial distribution, as in Algorithm~\ref{alg_gcs}, is the same as first drawing $\left( \boldsymbol{\epsilon}_1, \ldots, \boldsymbol{\epsilon}_M \right)$ from a $\mathrm{Dirichlet}(\epsilon_1 b_j', \ldots, \epsilon_m b_j')$ distribution and then letting $(\bold{X}_{ij}^{(1)}, \ldots, \bold{X}_{ij}^{(M)})  \mid \bold{X}_{ij}=X_{ij} \sim \mathrm{Multinomial}(X_{ij}, \boldsymbol{\epsilon}_1, \ldots, \boldsymbol{\epsilon}_M)$. Thus, to accommodate the overdispersion in $\bold{X}_{ij}$ relative to the Poisson distribution, we add additional randomness to the sampling process by making the parameters $(\epsilon_1,\ldots, \epsilon_M)$ from Algorithm~\ref{alg_cs} random variables. 

Theorem~\ref{theorem_nbthin} implies that $\bxm$ is independent of $\bxmm : = \bx - \bxm$ for $m=1,\ldots,M$. To address Examples~\ref{ex:denoise}, \ref{ex:diffExp}, and \ref{ex:stable} from Section~\ref{section_intro_nb}, we will use $\bxmm$ as a training set and $\bxm$ as a test set. The bottom line is that if we believe that $\bold{X}_{ij} \overset{\mathrm{ind.}}{\sim} \NB\left(\mu_{ij}, b_j\right)$ and we know the true values $b_j$, then a direct extension of Poisson count splitting is available. 


\subsection{The role of the parameter $b_j'$}

Theorem~\ref{theorem_nbthin} requires that we apply Algorithm~\ref{alg_gcs} with the correct value of the overdispersion parameter; i.e. that $\bx_{ij} \sim \mathrm{NB}(\mu_{ij}, b_j)$ and we choose $b_j' = b_j$. In this section, we consider what happens when $b_j' \neq b_j$.

When $b'_j = \infty$, drawing $\left(\bx_{ij}^{(1)}, \ldots, \bx_{ij}^{(M)}\right) \mid \bx_{ij}=X_{ij} \sim \mathrm{DirichletMultinomial}\left(X_{ij}, \epsilon_1 b_j', \ldots, \epsilon_M b_j' \right)$ is equivalent to  drawing $\left(\bx_{ij}^{(1)},\ldots, \bx_{ij}^{(M)}\right) \mid \bx_{ij}=X_{ij} \sim \mathrm{Multinomial}\left(X_{ij}, {\epsilon}_1, \ldots, {\epsilon}_M \right)$, and so Algorithm~\ref{alg_gcs} reduces to Algorithm~\ref{alg_cs}. 

\begin{theorem}[\cite{neufeld2022inference}]
\label{theorem_nb_binom_thin}	
If $\bx_{ij} \overset{\mathrm{ind.}}{\sim} \NB\left(\mu_{ij}, b_j\right)$ and we apply Algorithm~\ref{alg_gcs} with $b_j' = \infty$ to $\bx_{ij}$, then (1) $\bxm_{ij} \overset{\mathrm{ind.}}{\sim} \NB(\epsilon_m \mu_{ij},  b_j)$ and (2) $\mathrm{Cor}(\bxm_{ij}, \bxmm_{ij}) = \frac{\sqrt{\epsilon_m(1-\epsilon_m)}}{\sqrt{\frac{b_j^2}{\mu_{ij}^2} + \frac{b_j}{\mu_{ij}} + \epsilon_m(1-\epsilon_m)}}$, where $\bx^{(-m)} := \bx - \bx^{(m)}$. 
\end{theorem}

Theorem~\ref{theorem_nb_binom_thin} says that while applying Poisson count splitting (or negative binomial count splitting with $b_j'=\infty$) on data from a negative binomial distribution yields training and test sets that follow the same model as the full data up to a parameter scaling, these datasets are positively correlated. The positive correlation increases as the true value of $b_j$ decreases, and decreases to $0$ as $b_j \rightarrow \infty$. Moreover, we see from Theorem~\ref{theorem_nb_binom_thin} that the  overdispersion parameters (and thus the variances) of $\bxm_{ij}$ are too small relative to Theorem~\ref{theorem_nbthin}. Thus, by failing to put enough noise into our sampling process, applying Poisson count splitting to negative binomial data results in training and test sets that are not as noisy as they should be, leading to positive correlation between them. 

We now consider the more general case of finite $b_j'$. The following result is included (under a different parameterization) in \cite{neufeld2023data}. 

\begin{theorem}
\label{theorem_generalcase}
If $\bx_{ij} \sim \NB(\mu_{ij}, b_j)$ and we apply Algorithm~\ref{alg_gcs} with parameter $b_j'$:
\begin{enumerate}
\item $\E[\bxm_{ij}] = \epsilon_m \mu_{ij}$,
\item $\Var(\bxm_{ij}) = \epsilon_m \Var(\bx_{ij}) + \epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j} \left( 
\frac{b_j+1}{b_j'+1} - 1\right)
$,
\item $\mathrm{Cov}(\bxm_{ij}, \bxmm_{ij}) = \epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j} \left( 
1- \frac{b_j+1}{b_j'+1} \right)$, where $\bx^{(-m)} := \bx - \bx^{(m)}$. 
\end{enumerate}
\end{theorem}

Unlike in Theorem~\ref{theorem_nb_binom_thin}, the folds of data that result from applying Algorithm~\ref{alg_gcs} with arbitrary values for $b_j'$ do not necessarily follow negative binomial distributions. The first statement of Theorem~\ref{theorem_generalcase} says that, regardless of the value of $b'_j$ used, the expected value of $\bxm$ and $\bxmm$ are scaled by $\eps_m$ and $(1-\eps_m)$ compared to $\E[\bold{X}]$ in \eqref{eq_meanmodel}. Thus, we can estimate the latent space using the training set $\bxmm$, and validate these estimates using the held out set $\xm$. The second statement of Theorem~\ref{theorem_generalcase} tells us that using the wrong value for $b_j'$ affects the variance of $\bxm$. The third statement says that if $b'_j \neq b_j$, then the training and test sets are correlated, and the magnitude of the correlation grows with the magnitude of the discrepancy between $b_j$ and $b_j'$. The result is displayed and empirically confirmed in Figure~\ref{fig_corvar}. In order to use $\bxm$ to validate a model fit to $\bxmm$ or to do valid inference on latent variables fit to $\bxmm$, we need independence between $\bxm$ and $\bxmm$. Thus, the major takeaway from Theorem~\ref{theorem_generalcase} is that, when the true $b_j$ are unknown, it is important to estimate them well. In Section~\ref{section_simulation_nb}, we use the well-known R package \texttt{sctransform} \citep{hafemeister2019normalization} to estimate each $b_j$. 


% Figure environment removed




\subsection{The role of the parameters $\epsilon_1,\ldots,\epsilon_M$}

In this section, we consider the case where we used the ``correct" value of $b_j'$ and thus the results of Theorem~\ref{theorem_nbthin} hold. In this setting, it is simple to show that, for a given fold $m$, the parameter $\epsilon_m$ governs a tradeoff between the amount of information in the training set $\bxmm$ and in the test set $\bxm$. This result is summarized in the following theorem.

\begin{theorem}[Information tradeoff as we vary $\eps$]
\label{theorem_fisher}
If $\bold{X}_{ij} \sim \NB(\mu_{ij}, b_j)$, then the Fisher information contained in a single datapoint $X_{ij}$ for the parameter $\mu_{ij}$ is $I_{\mu_{ij}}(\bx_{ij}) = \frac{b_j}{(b_j + \mu_{ij})\mu_{ij}}$. If we apply Algorithm~\ref{alg_gcs} with $b'_j=b_j$, then for $m=1,\ldots,M$, the Fisher information contained in $\bold{X}^{(m)}_{ij}$ for the parameter $\mu_{ij}$ is $\epsilon_m I_{\mu_{ij}}(\bx_{ij})$, and the corresponding Fisher information contained in $\bold{X}^{(-m)}_{ij}$ is $(1-\epsilon_m) I_{\mu_{ij}}(\bx_{ij})$. 
\end{theorem}

We will see in Section~\ref{section_simulation_nb} that the ideal choice of $\epsilon_1,\ldots,\epsilon_M$ depends on the application.





