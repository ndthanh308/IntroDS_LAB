\section{Proofs for Section~\ref{section_gcs}}
\label{appendix_proofs}

\subsection{Proof of Theorem~\ref{theorem_nbthin}}
\label{appendix_mainproof}

This result follows from applying Theorem 3 from \cite{neufeld2023data} to each individual element $X_{ij}$ in the matrix $X$, in the specific case where the $X_{ij}$ are independent negative binomial random variables. We note that \cite{neufeld2023data} use a different parameterization of the negative binomial: for $\by \sim \NB(r,p)$, they have $\E[\by] =  r \frac{1-p}{p}$ and $\Var(\by) =   r \frac{1-p}{p^2}$. This parameterization of the negative binomial is convolution-closed in the parameter $r$ if $p$ is held fixed, and corresponds to our parameterization if $\mu =r \frac{1-p}{p}$ and $b = r$. 

\subsection{Proof of Theorem~\ref{theorem_nb_binom_thin}}
\label{appendix_infproof}

We note that, when $b_j = \infty$, for $m = 1,\ldots,M$, $\bx_{ij}^{(m)} \mid \bx_{ij} = X_{ij} \sim \mathrm{Binomial}(X_{ij}, \epsilon_m)$ and $X^{(-m)} \mid \bx_{ij} = X_{ij} \sim \mathrm{Binomial}(X_{ij}, 1-\epsilon_m)$. Armed with these two facts, the first statement of Theorem~\ref{theorem_nb_binom_thin} is proved in \cite{harremoes2010thinning} and \cite{leiner2021data}, and the second statement of Theorem~\ref{theorem_nb_binom_thin} is proved in \cite{neufeld2022inference}. 

\subsection{Proof of Theorem~\ref{theorem_generalcase}}
\label{appendix_generalproof}

While parts of this theorem were proved in \cite{neufeld2022inference}, we prove this theorem in full directly here so that the notation matches that of this paper. 

The first statement of Theorem~\ref{theorem_generalcase} follows directly from the law of total expectation. Regardless of the value of $b'_j$, 
\begin{align*}
\E\left[\bxm_{ij}\right] &= \E\left[\E\left[\bxm_{ij} \mid \bx=X_{ij}\right]\right] = \E\left[\epsilon_m \bx_{ij}\right] = \epsilon_m \mu_{ij}, \\
\E\left[\bxmm_{ij}\right] &= \E\left[\E\left[\bxmm_{ij} \mid \bx=X_{ij}\right]\right] = \E\left[(1-\epsilon_m) \bx_{ij}\right] = (1-\epsilon_m) \mu_{ij}.
\end{align*}
This is because the parameters $b_j'$ do not affect the expected values of the \\ $\mathrm{DirichletMultinomial}\left(X_{ij}, \epsilon_1 b_j',\ldots, \epsilon_M b_j' \right)$ distribution, only the variance. \\
\\
The second statement of Theorem~\ref{theorem_generalcase} uses the law of total variance. We start by deriving the marginal variance of $\Var\left(\bxm_{ij}\right)$, as follows:
\begin{align*}
	\Var\left(\bxm_{ij}\right) &= \E\left[\Var\left(\bxm_{ij} \mid \bx_{ij} = X_{ij}\right)\right] + \Var\left( \E\left[ \bxm_{ij} \mid \bx_{ij} = X_{ij} \right]\right).
	\end{align*}
As $\bxm_{ij} \mid \bx_{ij} = X_{ij} \sim \bb\left(X_{ij}, \epsilon_m b_j', (1-\epsilon_m) b_j' \right)$, we plug in the (known) mean and variance of the beta-binomial distribution. 
\begin{align*}
\Var\left(\bxm_{ij}\right) &= \E\left[ \frac{\bx_{ij} \epsilon_m (1-\epsilon_m)  (b_j'+\bx)}{(b_j'+1)}\right] + \Var\left( \epsilon_m \bx_{ij} \right) \\
&= \frac{ \epsilon_m (1-\epsilon_m)  b_j' }{(b_j'+1)} \E\left[ \bx \right] + \frac{ \epsilon_m (1-\epsilon_m)}{(b_j'+1)} \E\left[ \bx^2 \right] + \epsilon_m^2 \Var\left( \bx_{ij} \right).
\end{align*}
We next plug in the known mean and variance of $\bx_{ij} \sim \NB(\mu_{ij},b_j)$.
\begin{align*}
\Var\left(\bxm_{ij}\right) &= \frac{ \epsilon_m (1-\epsilon_m)  b_j' \mu_{ij} }{(b'+1)} + \frac{ \epsilon_m (1-\epsilon_m)}{(b_j'+1)} \left(\mu_{ij} + \frac{\mu_{ij}^2}{b_j} + \mu_{ij}^2 \right) + \epsilon_m^2 \left(\mu_{ij} + \frac{\mu_{ij}^2}{b_j} \right) \\
%&= \frac{ \epsilon_m (1-\epsilon_m) \mu_{ij}}{(b'+1)} \left( b_j'  + 1 + \frac{\mu_{ij}}{b_j} + \mu_{ij} \right) + \epsilon_m^2 \left(\mu_{ij} + \frac{\mu_{ij}^2}{b_j} \right) \\
%&= \epsilon (1-\epsilon_m) \mu_{ij} + \frac{ \epsilon_m (1-\epsilon_m) \mu_{ij}}{(b_j'+1)} \left(\frac{\mu_{ij}}{b_j} + \mu_{ij} \right) + \epsilon_m^2 \left(\mu_{ij} + \frac{\mu_{ij}^2}{b_j} \right) \\
%&= \epsilon_m \mu_{ij} - \epsilon_m^2 \mu_{ij} + \frac{ \epsilon_m (1-\epsilon_m) \mu_{ij}^2}{(b_j'+1)} \left(\frac{1}{b_j} + 1 \right) + \epsilon^2 \mu_{ij} + \frac{\epsilon_{m}^2 \mu^2}{b_j}\\
&= \epsilon_{m} \mu_{ij} + \frac{\epsilon_{m}^2 \mu_{ij}^2}{b_j} 
+ \frac{ \epsilon_{m} (1-\epsilon_{m}) \mu^2}{(b_j'+1)} \left(\frac{1}{b_j} + 1 \right).
\end{align*}
To compare the magnitude of this variance to 
$\epsilon_m \Var(\bx)$, we add and subtract $\epsilon_m \frac{\mu_{ij}^2}{b_j}$.
\begin{align*}
\Var\left(\bxm_{ij}\right) &= \left[\epsilon_m \mu_{ij} + \epsilon_m \frac{\mu_{ij}^2}{b_j}\right] - \left[ \epsilon_m \frac{\mu_{ij}^2}{b_j} - \epsilon_m^2 \frac{\mu_{ij}^2}{b_j}\right] + \frac{ \epsilon_m (1-\epsilon_m) \mu_{ij}^2}{(b_j'+1)} \left(\frac{1}{b_j} + 1 \right) \\
&= \epsilon_m \Var(\bx) - 
\left[\epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j} \right] + 
\frac{ \epsilon_m (1-\epsilon_m) \mu_{ij}^2}{(b_j'+1)} \left(\frac{1}{b_j} + 1\right) \\
&= \epsilon_m \Var(\bx) + \epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j}\left( 
\frac{b_j+1}{b_j'+1} - 1\right),
\end{align*}
as claimed in Theorem~\ref{theorem_generalcase}. We omit the derivation of $\Var\left( \bxmm_{ij}\right)$, as it is identical to the derivation above after noting that $\Var(\bxmm_{ij} \mid \bold{X}_{ij} = X_{ij}) = \Var(\bx_{ij} - \bxm_{ij} \mid \bold{X}_{ij} = X_{ij}) = \Var(\bxm_{ij} \mid \bold{X}_{ij} = X_{ij})$.  \\
\\
Finally, for the third statement of Theorem~\ref{theorem_generalcase}, we use the fact that
$$
2 \times \mathrm{Cov}(\bxmm_{ij}, \bxm_{ij}) = \Var(\bold{X}_{ij})-\Var(\bxmm_{ij})-\Var\left(\bxm_{ij}\right).
$$
We then plug in the known values of these variances, and 
simplify, as follows:
\begin{align*}
2 \times \mathrm{Cov}(\bxmm_{ij}, \bxm_{ij}) &= \Var(\bold{X}_{ij}) - \epsilon_m \Var(\bold{X}_{ij}) - \epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j} \left( 
\frac{b_j+1}{b_j'+1} - 1\right)\\
& \ \ \ \ \ - (1-\epsilon_m) \Var(\bold{X}_{ij}) - \epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j} \left( 
\frac{b_j+1}{b_j'+1} - 1\right)
\\
&=  - 2 \epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j} \left( 
\frac{b_j+1}{b_j'+1} - 1\right).
\end{align*}
Thus, $
\mathrm{Cov}(\bxmm_{ij}, \bxm_{ij}) = \epsilon_m (1-\epsilon_m) \frac{\mu_{ij}^2}{b_j} \left( 
1- \frac{b_j+1}{b_j'+1} \right)$. 


\subsection{Proof of Theorem~\ref{theorem_fisher}}
\label{appendix_fisherproof}

We first derive the Fisher information contained in $X_{ij}$ for the parameter $\mu_{ij}$:
\begin{align*}
I_{\mu_{ij}}\left(\bold{X}_{ij}\right)	&= -\E \left[ \frac{d^2}{d \mu_{ij}^2} \log \left( f\left( \bold{X}_{ij} \mid \mu_{ij}, b_j \right) \right) \right] \\
&= -\E \left[ \frac{d^2}{d \mu_{ij}^2} \left( \log \left(\frac{\Gamma(\bold{X}_{ij}+b_j)}{\Gamma(b_j)\bold{X}_{ij}!} \right) + \bold{X}_{ij}  \log \left(\frac{\mu_{ij}}{\mu_{ij}+b_j}\right) + b_j \log \left(\frac{b_j}{\mu_{ij}+b_j}\right) \right)  \right] \\
&= -\E \left[ \bold{X}_{ij}  \left( \frac{-1}{\mu_{ij}^2} + \frac{1}{(\mu_{ij}+b_j)^2} \right) + b_j \left( \frac{1}{(\mu_{ij}+b_j)^2} \right) \right] \\
&=  \frac{1}{\mu_{ij}} - \frac{\mu_{ij}+b_j}{(\mu_{ij}+b_j)^2} 
=  \frac{ b_j }{\mu_{ij}(\mu_{ij}+b_j)}.
\end{align*}
Now we derive the Fisher information contained in $\bxm_{ij}$ for the parameter $\mu_{ij}$:
\begin{align*}
I_{\mu_{ij}}\left(\bxm_{ij} \right)	&= -\E \left[ \frac{d^2}{d \mu_{ij}^2} \log \left( f\left( \bxm_{ij} \mid \eps \mu_{ij}, \eps b_j \right) \right) \right] \\
&= -\E \left[ \frac{d^2}{d \mu_{ij}^2} \left( \log \left(\frac{\Gamma(\bold{X}_{ij}^{(m)}+\epsilon_m b_j)}{\Gamma(\epsilon_m b_j)(\bold{X}_{ij}^{(m)})!} \right) + \bold{X}^{(m)}_{ij}  \log \left(\frac{\epsilon_m  \mu_{ij}}{\epsilon_m \mu_{ij}+\epsilon_m b_j}\right) + \epsilon_m  b_j \log \left(\frac{\epsilon_m  b_j}{\epsilon_m \mu_{ij}+\epsilon_m b_j}\right) \right)  \right] \\
%&= -\E \left[ \frac{d^2}{d \mu_{ij}^2} X_{ij}  \left( \log (\mu_{ij}) - \log \left( \mu_{ij}+b_j \right)\right) + \epsilon_m b_j \left( \log(b_j) - \log(\mu_{ij}+b_j) \right) \right] \\
%&= -\E \left[ \frac{d}{d \mu_{ij}} X_{ij}  \left( \frac{1}{\mu_{ij}} - \frac{1}{\mu_{ij}+b_j} \right) - \epsilon_m b_j \left( \frac{1}{ \mu_{ij}+b_j} \right) \right] \\
&= -\E \left[ \bold{X}_{ij}^{(m)}  \left( \frac{-1}{\mu_{ij}^2} + \frac{1}{(\mu_{ij}+b_j)^2} \right) + \epsilon_m b_j \left( \frac{1}{(\mu_{ij}+b_j)^2} \right) \right] \\
&=  \frac{\epsilon_m}{\mu_{ij}} - \frac{\epsilon_m \mu_{ij}+\epsilon_m b_j}{(\mu_{ij}+b_j)^2} 
%&= \epsilon_m \left( \frac{1}{\mu_{ij}} - \frac{1}{(\mu_{ij}+b_j)} \right) \\
=  \epsilon_m I_{\mu_{ij}}\left(\bold{X}_{ij}\right).
\end{align*}

Theorem~\ref{theorem_nbthin} guarantees independence between $\bxm_{ij} $ and $\bxmm_{ij}$, and therefore  $I_{\mu_{ij}}\left(\bxmm_{ij} \right)	+ I_{\mu_{ij}}\left(\bxm_{ij} \right) = I_{\mu_{ij}}\left(\bold{X}_{ij} \right)$. It follows that $I_{\mu_{ij}}\left(\bxmm_{ij} \right) =  (1-\epsilon_m) I_{\mu_{ij}}\left(\bold{X}_{ij}\right)$. 
