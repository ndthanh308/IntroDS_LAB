\section{Implementation details for Figure~\ref{fig_intro}}
\label{appendix_naive_sampsplit}

To generate Figure~\ref{fig_intro}, we first generate a toy dataset $X \in \mathbb{Z}_{\geq 0}^{100 \times 2}$, where each element $X_{ij}$ is drawn independently from the $\mathrm{NB}(5, 5)$ distribution, which has mean $5$ and variance $10$ (see Section~\ref{subsec_nb} for parameterization details). In panels (c) and (d), the naive method that uses the data twice  and our proposed method are implemented as described in Section~\ref{section_simulation_nb}. Here, we provide details about the implementation of sample splitting used in Figure~\ref{fig_intro}. 

We begin by splitting the cells such that the first $n/2$ cells belong to the training set, and the remaining cells belong to the test set, where $n=100$. 

We first describe Figure~\ref{fig_intro}(c). For values of $k$ ranging from $1$ to $10$, we cluster the first $n/2$ rows of the matrix $\log(X+1)$ using k-means clustering with $K$ clusters. This yields estimated cluster assignments $\hat{c}_i$ only for cells $i = 1,\ldots,n/2$. We then apply $3$-nearest neighbors classification to label each cell in the test set with the majority-label from its three nearest neighbors in the training set (using Euclidean distance on the log-transformed data).  We then compute an estimated mean $\hat{\mu}_{ij}$ for each datapoint $X_{ij}$ as
\begin{equation}
\label{eq_sampsplit_hatmu}
\hat{\mu}_{ij} = \frac{1}{\sum_{i'=1}^{n/2} \bold{1}\{ \hat{c}_i' = \hat{c}_i\}} \sum_{i'=1}^{n/2} X_{i',j}  \bold{1}\{ \hat{c}_i' = \hat{c}_i\},
\end{equation}
which is the sample mean of the training set data points belonging to this cluster. Finally, we compute the within-cluster mean-squared error as
\begin{equation}
\label{seq_sampsplit_mse}
\frac{1}{n/2 \times 2} \sum_{i = n/2+1}^n \sum_{j=1}^{2} \left( \log(X_{ij}+1) - \log(\hat{\mu}_{ij}+1) \right)^2.
\end{equation}

Sample splitting is implemented in a similar way in Figure~\ref{fig_intro}(d). We run $k$-means clustering with $k=2$ on the logged training data. This yields cluster assignments $\hat{c}_i$ only for $i=1,\ldots,n/2$. We once again obtain cluster assignments $\hat{c}_i$ for $i=n/2+1,\ldots,n$ by applying $3$-nearest neighbors. Finally, for $j=1$ and for $j=2$, we fit a negative binomial generalized linear model where the response is $X_{ij}$ and the covariate is $\hat{c}_i$, for $i=n/2+1, \ldots, n$. 

In both cases, sample splitting fails because $\hat{c}_i$ for $i=n/2+1, \ldots,n$ is obtained using the data $X_{ij}$ for $i=n/2+1, \ldots,n$, via the $3$-nearest neighbors classification step. Thus, the test set is not truly ``held out" in computing $\hat{c}_{i}$ for $i = n/2+1,\ldots,n$. 


To create panels (e) and (f) of Figure~\ref{fig_intro}, we apply Algorithms~\ref{alg_cao_intra} and \ref{alg_cs_intra} to the single realization of toy data shown in Figure~\ref{fig_intro}. In both algorithms, we estimate $k=5$ clusters by running $k$-means on the log-transformed data. For Algorithm~\ref{alg_cao_intra}, we use a support vector machine (SVM) with a linear kernel as the classifier. 