\section{Introduction}
\label{section_intro_nb}

A single-cell RNA sequencing (scRNA-seq) dataset involving $n$ cells and $p$ genes can be written as a matrix $X \in \mathbb{Z}_{\geq 0}^{n \times p}$, where entry $X_{ij}$ is the number of unique molecular identifiers from the $i$th cell that map to the $j$th gene. It is common to model $X$ as a realization from a random variable $\bold{X}$, and assume that
\begin{equation}
\label{eq_meanmodel}
\E\left[ \bx_{ij} \right] = \gamma_i \Lambda_{ij}, \text{ with }g(\Lambda) = L \beta^\top \text{ for } L \in \mathbb{R}^{n \times K^*}, \beta \in \mathbb{R}^{p \times K^*},
\end{equation}
for some link function $g(\cdot)$ \citep{sarkar2021separating}. In \eqref{eq_meanmodel}, $\gamma=(\gamma_1,\ldots,\gamma_n)^T$ stores cell-specific \emph{size factors}, which reflect technical variation in sequencing depth between cells. The matrix $\Lambda$ represents the biological variation of interest, which, after applying a link function $g(\cdot)$, has rank $K^*$ for some $K^* \leq \min(n,p)$. 

Fitting the model \eqref{eq_meanmodel}--that is, obtaining estimates $\hat{L}(X)$ and $\hat{\beta}(X)$ of $L$ and $\beta$--may be of interest for a number of reasons. For example, we may wish to denoise the data by replacing the size-factor normalized dataset $\mathrm{diag}(\gamma)^{-1} X$ with its low-rank estimate $g^{-1}\left(\hat{L}(X)\hat{\beta}(X)^T \right)$ \citep{eraslan2019single, townes2019feature}, or we may wish to interpret $\hat{L}(X)$ as a measure of an unobserved aspect of cell state, e.g. cell type or position along a developmental trajectory \citep{aizarani2019human, van2020trajectory}. After fitting the model \eqref{eq_meanmodel}, we typically want to perform some type of model validation or inference. 

\begin{example}
\label{ex:denoise}
We want to assess the quality of our low-rank approximation $\mathrm{diag}(\gamma)^{-1} X \approx g^{-1}\left( \hat{L}(X)\hat{\beta}(X)^T\right)$ \citep{sarkar2021separating, batson2019molecular}. 
\end{example}

\begin{example}
\label{ex:diffExp}
We want to identify genes that are associated with $\hat{L}(X)$ \citep{aizarani2019human, van2020trajectory, zhang2019valid}. 
\end{example}

\begin{example}
\label{ex:stable}
We want to know whether we would obtain a similar estimate  $\hat{L}(X)$ on an independent realization of $\bold{X}$ drawn from the same distribution \citep{cao2020human, lange2004stability, ullmann2022validation}. We refer to this as ``reproducibility."
\end{example}

In Example~\ref{ex:denoise}, \emph{because we estimated $L$ and $\beta$ on the data $X$, we cannot re-use $X$ to assess model fit} \citep{hastie2009elements}. In Example~\ref{ex:diffExp}, \emph{because we estimated $L$ on the data $X$, we cannot re-use $X$ to test for association} \citep{gao2022selective}. In Example~\ref{ex:stable}, \emph{we only have access to one dataset}, so it is unclear how to proceed. While very specialized approaches are available to overcome these challenges in specific instantiations of Example~\ref{ex:denoise} \citep{fu2020estimating, grabski2022significance}, Example~\ref{ex:diffExp} \citep{gao2022selective, chen2022selective, zhang2019valid, chung2015statistical}, and Example~\ref{ex:stable} \citep{tibshirani2005cluster, lange2004stability}, in this paper we will provide a much more flexible framework for model validation or inference after fitting \eqref{eq_meanmodel}, which will be applicable to all three examples. 

To illustrate the problem, we generate a toy data matrix $X \in \mathbb{Z}_{\geq 0}^{100 \times 2}$ with elements drawn independently from a negative binomial distribution with mean $5$ and variance $10$. This is a special case of \eqref{eq_meanmodel} with $\gamma_1 = \ldots = \gamma_n = 1$, $K^*=1$, $L = \bold{1}_{100}$, and $\beta = [5, 5]^T$. The data are shown in Figure~\ref{fig_intro}(a). To illustrate Example~\ref{ex:denoise}, we apply
$k$-means clustering to the data to estimate $K$ clusters for a range of values of $K$. 
Though there is one true cluster in this example (all 100 cells are homogenous), the mean squared error (MSE; defined in \eqref{eq_mse} in Section 4) computed on the same data used for clustering is monotone decreasing in $K$ (Figure~\ref{fig_intro}(c)), incorrectly suggesting that a larger value of $K$ always leads to a better fit. To illustrate Example~\ref{ex:diffExp}, we fit a negative binomial generalized linear model (GLM) to test whether the expected expression of the first gene is associated with the cluster labels when we estimate $K=2$ clusters. When we perform this test on the same data used for clustering, we obtain p-values that are much smaller than the $\mathrm{Unif}(0,1)$ distribution (Figure~\ref{fig_intro}(d)), and thus do not control the Type 1 error rate (recall that no true clusters are present, and thus there is no true association between the first gene and the estimated clusters). 

The solution here might seem obvious: to split our 100 cells into a training set, used to fit \eqref{eq_meanmodel}, and a test set, used for model validation. Unfortunately, this \emph{sample splitting} approach does not work. The issue is that fitting \eqref{eq_meanmodel} using the cells in the training set yields latent variable coordinates for cells in the training set only. To use the test set for validation or inference, we must obtain latent variable coordinates of the cells in the test set. This step involves using the test set data itself, which invalidates downstream evaluation or inference. 
In our toy example in Figure~\ref{fig_intro}, we apply $k$-means clustering to the cells in the training set, and then assign cluster labels to the cells in the test set using 3-nearest neighbor classification. We see in Figure 1(c) that, over 1000 simulated datasets, the within-cluster MSE computed on the test set (see Appendix~\ref{appendix_naive_sampsplit} for details) decreases monotonically with the number of clusters, because we used the test set both to compute latent variable coordinates for the cells in the test set and to compute the within-cluster MSE. Similarly, Figure~\ref{fig_intro}(d) shows that, over 1,000 simulated datasets, the p-values from a negative binomial GLM that regresses the first gene from the test set onto the test set cluster assignments do not control the Type 1 error rate. We refer the reader to \cite{owen2009bi} for more discussion of the inadequacy of sample splitting in the setting of Example~\ref{ex:denoise}, and \cite{gao2022selective}, \cite{chen2022selective}, and \cite{neufeld2022inference} for a related discussion in the setting of Example~\ref{ex:diffExp}.

In the setting of Example~\ref{ex:stable}, \cite{cao2020human} implement a procedure that they call ``intradataset cross-validation" (see Algorithm~\ref{alg_cao_intra}). Inspired by the general framework of \cite{abdelaal2019comparison},  their procedure involves estimating clusters using all of the data and then performing 5-fold cross-validation to assess the accuracy of a classifier fit to predict these clusters. Low cross-validation error is treated as evidence of cluster reproducibility, because it means that a given cell's cluster assignment can be reproduced by a classifier, even when that cell itself was not used to train the classifier. Figure~\ref{fig_intro}(e) shows a confusion matrix comparing the cell types estimated via clustering (with $K=5$) to those predicted using cross-validation (with five folds and a support vector machine classifier) for the cells in the toy dataset from Figure~\ref{fig_intro}(a). Despite the fact that all cells are homogenous in this dataset (and thus the estimated clusters are driven by random noise), 95\% of the cells fall on the diagonal of the confusion matrix, falsely suggesting reproducibility of the clusters. The issue is that, since all of the data from all of the cells was used for clustering, any downstream model evaluation is compromised, even if the downstream task makes use of cross-validation. 

% Figure environment removed

Now, suppose that we were able to sequence the same set of cells twice to obtain two independent datasets $\xtr \in \mathbb{Z}_{\geq 0}^{n \times p}$ and $\xte \in \mathbb{Z}_{\geq 0}^{n \times p}$ generated from $\bold{X}$ in \eqref{eq_meanmodel} (with the same true underlying $L$ and $\beta$ matrices). We could estimate $L$ and/or $\beta$ using only $\xtr$, and could then validate the results or conduct inference using $\xte$. Thus, the challenges associated with Examples 1 and 2 displayed in Figure 1(c) and 1(d) would be entirely avoided. Similarly, we could estimate one set of clusters on  $\xtr$ and another set of clusters on  $\xte$ and compare the two clusterings using a metric such as the adjusted Rand Index \citep{hubert1985comparing}, entirely avoiding the challenge of Example 3. 

In practice, we cannot sequence the same set of cells twice. Instead, we propose using our single dataset $X$ to reverse engineer two datasets $\xtr$ and $\xte$ that function like independent sequencing experiments performed on the same sets of cells. Our proposal is an extension of the ideas of \cite{batson2019molecular}, \cite{sarkar2021separating}, and \cite{neufeld2022inference}, who perform this reverse engineering under the assumption that $X_{ij} \overset{\mathrm{ind.}}{\sim} \mathrm{Poisson}(\Lambda_{ij})$. However, scRNA-seq data are typically overdispersed relative to the Poisson or the binomial distribution, and so the \emph{Poisson count splitting} procedure developed in these earlier papers will fail to produce independent training and test sets. %in practice.  

In recent work, \cite{neufeld2023data} and \cite{dharamshi2023generalized} developed \emph{data thinning}, a vast generalization of Poisson count splitting that enables us to split a random variable drawn from a number of well-known distributional families into two or more independent components. In this paper, we focus on the special case of data thinning for negative binomial random variables, which allows us to obtain independent matrices $\xtr$ and $\xte$ under the assumption that the elements of $X$ are independent draws from negative binomial distributions. We refer to this procedure as \emph{negative binomial count splitting}. Critically, $\xtr$ and $\xte$ are drawn from the same distribution as $X$, up to a parameter scaling. 

Figure~\ref{fig_intro}(c) and Figure~\ref{fig_intro}(d) show that, in our toy example, negative binomial count splitting correctly determines that $K^*=1$ and controls the Type 1 error rate. Figure~\ref{fig_intro}(f) shows that a modified version of ``intradataset cross-validation" that makes use of negative binomial count splitting yields a confusion matrix that accurately reflects the absence of signal. 

Negative binomial count splitting requires a negative binomial assumption to ensure independence between the training and test sets. However, once the data has been split, we are free to use any latent variable estimation method or inferential technique. %For instance, one could apply negative binomial count splitting to obtain training and test sets, and then subsequently fit Poisson generalized linear models. 

In Section~\ref{section_background_nb}, we review the Poisson count splitting procedure of \cite{neufeld2022inference} and its generalization to data thinning \citep{neufeld2023data, dharamshi2023generalized}. In Section~\ref{section_gcs}, we introduce negative binomial count splitting and provide some theoretical results. In Section~\ref{section_simulation_nb}, we apply negative binomial count splitting to Example~\ref{ex:denoise} and Example~\ref{ex:diffExp} on simulated data. In Section~\ref{section_realData_nb}, we revisit the intradataset cross-validation procedure of \cite{cao2020human}, and assess the reproducibility (Example~\ref{ex:stable}) of cell types and subtypes from their human cell atlas using negative binomial count splitting. We close with a brief discussion in Section~\ref{sec_disc}. 

