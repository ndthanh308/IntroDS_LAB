\section{Simulation Study}
\label{section_simulation_nb}

In this section, we apply negative binomial count splitting to Example 1 and Example 2 from Section~\ref{section_intro_nb}, instantiated to the setting where there are $K^*$ true discrete latent variables which represent cell types that we wish to estimate using $k$-means clustering. More specifically, we consider the following settings:

\begin{list}{}{}
\item{\textbf{Example 1, instantiated to clustering:}} We fit models with $K$ clusters for $K=1,\ldots,10$, evaluate each model, then let the best value of $K$ be our estimate of $K^*$. 
\item{\textbf{Example 2, instantiated to clustering:}} We cluster the data into $K=2$ estimated cell types and test each gene for differential expression across the two estimated clusters.
\end{list}

After introducing our simulation setup in Section~\ref{subsec_datagen}, we show that negative binomial count splitting can be easily applied to Example 1 (Section~\ref{subsec_estK}) and Example 2 (Section~\ref{subsec_difExp}). 

\subsection{Data generating mechanism}
\label{subsec_datagen}

We generate datasets with $n$ cells, $p$ genes, and $K^*$ true cell types. Each gene has a baseline expression level $\exp(\beta_{j0})$ where $\beta_{j0} \overset{\mathrm{i.i.d.}}{\sim} N(0,1)$ for $j=1,\ldots,p$. 

For each dataset, we assign each cell to one of the $K^*$ clusters with equal probability. The first column of the latent variable matrix $L \in \mathbb{R}^{n \times K^*}$ contains ones, and the remaining columns are indicators for clusters $k=2,\ldots,K^*$. The matrix $\beta \in \mathbb{R}^{p \times K^*}$ stores $\beta_{10},\ldots,\beta_{p0}$ in the first column. When $K^* >1$, $\beta_{j2} = \beta^*$ for the first $5\%$ of the genes, and the rest of the entries in the second column of $\beta$ are $0$. If $K^*>2$, then $\beta_{j3} = \beta^*$ for the next $5\%$ of the genes, and all other entries are $0$. We continue filling in the $\beta$ matrix in this manner until all $K^*$ columns have been filled. We consider different values of $\beta^*$ for different datasets, but within a dataset we always use the same value of $\beta^*$ such that all $K^*$ clusters are equally easy to detect. Finally, we let $\log\left(\Lambda\right) = L \beta^\top$. 

We let the overdispersion parameter $b_j$ for the $j$th gene be a function of the average expression $\bar{\Lambda}_j = \frac{1}{n} \sum_{i=1}^n \Lambda_{ij}$ for that gene \citep{choudhary2022comparison, hafemeister2019normalization, love2014moderated}. More specifically, we set $b_j = \frac{\bar{\Lambda}_j}{\tau}$ for either $\tau=1$ (``mild overdispersion") or $\tau=5$ (``severe overdispersion"). We then let $\bx_{ij} \sim \mathrm{NB}(\Lambda_{ij}, b_j)$, such that $\Var(\bold{X}_{ij}) = \Lambda_{ij}\left( 1 + \frac{\Lambda_{ij}}{b_j}\right) \approx \Lambda_{ij}\left( 1 + \tau\right)$. We omit size factors from this simulation study, as they are not the focus of this paper. 

\subsection{Selecting the number of clusters}
\label{subsec_estK}

\subsubsection{Methods}
\label{subsubsec_numclustmethods}

We now introduce the general algorithm used in this section. 

\begin{algorithm}[Estimating the number of clusters] 
Start with datasets $\xtr \in \mathbb{Z}_{\geq 0}^{n \times p}$ and $\xte \in \mathbb{Z}_{\geq 0}^{n \times p}$ and parameter $\eps \in (0,1)$, where $\E\left[ \xte \right] = \frac{1-\eps}{\eps} \E[\xtr]$.  For $K=1,\ldots,10$:
\begin{enumerate}
\item Run k-means clustering to estimate $K$ clusters on $\log(\xtr+1)$. This yields a cluster assignment $\hat{c}_i \in \{1,\ldots,K\}$ for $i = 1,\ldots, n$.
\item For $i=1,\ldots,n$ and $j=1,\ldots,p$, estimate $\E[\bold{X}_{ij}^\mathrm{train}]$ with the sample mean of the training set points assigned to the same cluster:  
$\hat{\mu}^{\mathrm{train}}_{ij} = \frac{1}{\sum_{i'=1}^n \bold{1}\{\hat{c}_{i'} = \hat{c}_i\}} \sum_{i'  = 1}^n X^\mathrm{train}_{i'j} \bold{1}\{\hat{c}_{i'} = \hat{c}_i\}$. 
\item Estimate $\E[\bold{X}_{ij}^\mathrm{test}]$ as follows:
$\hat{\mu}^{\mathrm{test}}_{ij} = \frac{1-\eps}{\eps} \hat{\mu}^{\mathrm{train}}_{ij}$. 
\item Compute the within-cluster mean squared error on the test set as follows: 
\begin{equation}
\label{eq_mse}
MSE(K) = \frac{1}{n \times p} \sum_{i=1}^n \sum_{j=1}^p \left( \log\left(X^\mathrm{test}_{ij}+1\right) - \log\left( \hat{\mu}^{\mathrm{test}}_{ij} +1 \right) \right)^2. 
\end{equation}
\end{enumerate}
\label{alg_estK}
\end{algorithm}

We apply Algorithm~\ref{alg_estK} as follows.  

\begin{list}{}{}
\item{\textbf{Naive method:}} Run Algorithm~\ref{alg_estK} with $\xtr = \xte = X$ and $\eps = 0.5$. 
\item{\textbf{Poisson count splitting (PCS):}} Apply Algorithm~\ref{alg_cs} to $X$ with $M=2$ and some $(\epsilon_1, \epsilon_2)$ to obtain $\xo$ and $\xt$. Run Algorithm~\ref{alg_estK} with $\xtr = \xo, \xte = \xt$, and $\epsilon = \epsilon_1$.  
\item{\textbf{Negative binomial count splitting, known $b$ (NBCS-known):}} Apply Algorithm~\ref{alg_gcs}  to $X$ with $M=2$, some choice of $(\epsilon_1, \epsilon_2)$, and $(b_1', \ldots, b_p')=(b_1,\ldots, b_p)$ to obtain $\xo$ and $\xt$. Run Algorithm~\ref{alg_estK} using $\xtr = \xo, \xte = \xt$, and $\epsilon = \epsilon_1$.

\item{\textbf{Negative binomial count splitting, estimated $b$ (NBCS-estimated):}} First, use the \texttt{R} package \texttt{sctransform} \citep{hafemeister2019normalization} to obtain estimates $\hat{b}_1, \ldots, \hat{b}_p$ of $b_1,\ldots,b_p$. Details are given in Appendix~\ref{appendix_sct}. Then run Algorithm~\ref{alg_gcs} on matrix $X$ with $M=2$, some choice of $(\epsilon_1, \epsilon_2)$, and $(b_1', \ldots, b_p')=(\hat{b}_1,\ldots, \hat{b}_p)$ to obtain $\xo$ and $\xt$. Then run Algorithm~\ref{alg_estK} using $\xtr = \xo, \xte = \xt$, and $\epsilon = \epsilon_1$.
\end{list}{}{}
We now extend both versions of NBCS to perform cross-validation.
\begin{list}{}{}
\item{\textbf{Negative binomial cross-validation, known $b$ (NBCV-known):}} Obtain $(X^{(1)}, \ldots, X^{(M)})$ by running Algorithm~\ref{alg_gcs} on $X$ with $M=10$, $\epsilon_m = \frac{1}{M}$ for $m = 1,\ldots, M$, and $(b_1', \ldots, b_p')=(b_1,\ldots, b_p)$. For $m = 1,\ldots,M$, apply Algorithm~\ref{alg_estK} with 
$\xtr=\xmm$, $\xte=\xm$, and $\eps = \frac{M-1}{M}$. For each value of $K$, record the total MSE summed across the $M$ folds. 
\item{\textbf{Negative binomial cross-validation, estimated $b$ (NBCV-estimated): }} First obtain estimates $\hat{b}_1,\ldots,\hat{b}_p$ of $b_1,\ldots,b_p$ using \texttt{sctransform} on the full data $X$. Then proceed as in NBCV-known, but apply Algorithm~\ref{alg_gcs} with $(b_1', \ldots, b_p') = (\hat{b}_1,\ldots,\hat{b}_p)$.
\end{list}

We already showed in Section~\ref{section_intro_nb} that the naive method fails to provide a viable solution to Examples 1 and 2; nonetheless we include it here for the sake of comparison. We do not consider sample splitting in this section, as we already saw in Section~\ref{section_intro_nb} that it fails to provide a viable solution to our problems of interest. 

\subsubsection{Results}
\label{subsubsec_numclustres}

We first generate 1,000 datasets with $n = 1000$ and $p = 1000$ and  $\beta^* = 1.5$ for $K^* \in \{1,3,5\}$ and the two overdispersion settings described in Section~\ref{subsec_datagen}. For each dataset, we consider the naive method, Poisson count splitting, NBCS-known, and NBCS-estimated (all with $\eps = 0.5$). We plot the average MSE over the 1000 datasets, defined in \eqref{eq_mse}, as a function of $K$. To facilitate comparisons between methods, the y-axis of Figure~\ref{fig_mse} has been scaled such that the MSE for each method ranges from $0$ to $1$.

Figure~\ref{fig_mse} shows that, regardless of the true value of $K^*$ or the amount of overdispersion, the MSE for the naive method decreases monotonically with $K$. Thus, we cannot simply select the value of $K$ that minimizes the loss function. We must instead search for a bend or an ``elbow" in the MSE plot, which fails to provide a clear answer for the number of clusters we should select. We see that PCS performs well (the loss function is minimized when $K^*=K$) under mild overdispersion, but under severe overdispersion its performance approaches that of the naive method. This is as expected from Theorem~\ref{theorem_nb_binom_thin}, since the correlation between $\xtr = X^{(1)}$ and $\xte = X^{(-1)}$ under PCS increases as the amount of overdispersion increases. Both versions of NBCS have loss functions that are minimized when $K=K^*$, regardless of the true value of $K^*$ or the amount of overdispersion in the data. Thus, we can select the number of clusters by selecting the value of $K$ that minimizes the loss function. As the naive method and PCS do not lead to loss functions that are minimized at the true number of clusters, we do not consider them for the remainder of this section. 

% Figure environment removed

We next explore the role of the parameter $\epsilon$ in NBCS. We generate 2,000 datasets with mild overdispersion with $n= 500$ and $p=40$ for $K=5$ and for $\beta^*$ values ranging from $2$ to $6$. For values of $\epsilon$ ranging from $0$ to $1$, we perform NBCS-known. For each dataset and each value of $\epsilon$, we consider three metrics. The left panel of Figure~\ref{fig_roleEps} displays the adjusted Rand index (ARI) between the true clusters and those estimated using $\xtr$ when $K=K^*$, as a function of $\epsilon$. For a given signal strength, the average ARI increases with $\epsilon$ because a large value of $\eps$ means that we use more of the information in our data in the cluster estimation phase of Algorithm~\ref{alg_estK} (see Theorem~\ref{theorem_fisher}). The center panel of Figure~\ref{fig_roleEps} shows the proportion of times that the MSE is minimized at $K=K^*$ (i.e. that we select the correct value of $K$), given that the ARI between the true clusters and the estimated clusters when $K=K^*$ exceeds $0.8$. This metric decreases with $\epsilon$, as large values of $\eps$ leave less information in the test set for us to validate the estimated clusters. The right panel of Figure~\ref{fig_roleEps} shows the overall proportion of datasets for which the loss function is minimized at $K=K^*$. The optimal value of $\eps$ depends on the true signal strength, but it always involves a tradeoff between choosing $\eps$ large enough to estimate good clusters on $\xtr$, but not so large that we cannot accurately validate the clusters. 

% Figure environment removed

Finally, we compare NBCS-known with $\eps = 0.9$ to NBCV-known with 10 folds. We generate 2,000 datasets where $n=500$ and $p=40$ for values of $\beta^*$ ranging from $1$ to $6$ and $K^* = 1,3,5$. As both methods use 90\% of the information in the data for training and 10\% for testing, these methods have the same average MSE curves over many datasets. However, for a given dataset, NBCV-known selects the correct value of $K$ more often than NBCS-known, because averaging the MSE over 10 folds reduces the variance in our validation step (Figure~\ref{fig_propCor}). 

% Figure environment removed


\subsection{Testing for differential expression}
\label{subsec_difExp}

\subsubsection{Methods}

In this section we let $K^*=2$ and we always estimate two clusters on the data. Our focus is no longer on estimating the number of clusters, but rather on studying differential expression across a given set of estimated clusters. We use the following algorithm. 

\begin{algorithm}[Testing for differential expression.] Start with datasets $\xtr \in \mathbb{Z}_{\geq 0}^{n \times p}$ and $\xte \in \mathbb{Z}_{\geq 0}^{n \times p}$. 
\begin{enumerate}	
\item Apply $k$-means clustering with $K=2$ to estimate clusters on $\log(\xtr+1)$. This yields a cluster assignment $\hat{c}_i \in \{0,1\}$ for $i=1,\ldots,n$. 
\item For $j=1,\ldots,p$, fit a negative binomial GLM of $X^\mathrm{test}_j$ on $\hat{c}$. Report the Wald p-value for the slope coefficient. 
\end{enumerate}
\label{alg_diffExp}
\end{algorithm}

We consider the following ways to obtain $\xtr$ and $\xte$ in Algorithm~\ref{alg_diffExp}. 

\begin{list}{}{}
\item{\textbf{Naive method:}} Let $\xtr = \xte = X$.
\item{\textbf{Poisson count splitting (PCS):}} Obtain $\xtr = X^{(1)}$ and $\xte = X^{(2)}$ by running Algorithm~\ref{alg_cs} on the data $X$ with $M=2$ and $(\epsilon_1, \epsilon_2) = (\epsilon, 1-\epsilon)$. 
\item{\textbf{Negative binomial count splitting, known $b$ (NBCS-known):}} Obtain $\xtr = \xo$ and $\xte= X^{(2)}$ by running Algorithm~\ref{alg_gcs} on $X$ with $M=2$, $(\epsilon_1, \epsilon_2) = (\epsilon, 1-\epsilon)$, and $(b_1', \ldots, b_p')=(b_1,\ldots, b_p)$. 
\item{\textbf{Negative binomial count splitting, estimated $b$ (NBCS-estimated:)}} Use the \texttt{R} package \texttt{sctransform} \citep{hafemeister2019normalization} to obtain estimates $\hat{b}_1, \ldots, \hat{b}_p$ of $b_1,\ldots,b_p$ using the full dataset $X$. Then apply Algorithm~\ref{alg_gcs} on matrix $X$ with $M=2$, $(\epsilon_1, \epsilon_2) = (\epsilon, 1-\epsilon)$, and $(b_1', \ldots, b_p')=(\hat{b}_1,\ldots, \hat{b}_p)$. 
\end{list}

Unlike in Section~\ref{subsec_estK}, we do not consider splitting with $M>2$ folds and we do not aggregate results across folds by interchanging the roles of the train and test sets. We leave the possibility of aggregating differential expression test statistics across multiple folds to future work. Once again, we do not consider sample splitting because we already saw in Figure~\ref{fig_intro} that it fails to provide a viable solution to in Example~\ref{ex:diffExp}. 

\subsubsection{Results}

We generate datasets using the mechanism described in Section~\ref{subsec_datagen} with $K^*=2$, $n=500$, and $p=40$. Under this mechanism, the first two genes are differentially expressed across the two true clusters. We refer to the remaining $38$ genes as null genes because they have the same expected expression across all cells. Figure~\ref{fig_QQ} shows uniform QQ plots of the p-values obtained from the four variations of Algorithm~\ref{alg_diffExp} for the null genes, aggregated across 1000 datasets for each of 16 $\beta^*$ values. We see from Figure~\ref{fig_QQ} that both the naive method and Poisson count splitting fail to control the Type 1 error rate. Poisson count splitting performs worse when overdispersion is severe. On the other hand, both versions of NBCS control the Type 1 error rate. 

% Figure environment removed

Finally, we explore the role of $\eps$ in this setting. We generate $1000$ datasets where $n=1000$ and $p=1000$ for each of 16 values of $\beta^*$ ranging from $0$ to $3$. For each dataset, we consider the adjusted Rand index between the true clusters and those estimated on the training set. As in Section~\ref{subsec_estK}, we expect that larger values of $\epsilon$ will lead to higher adjusted Rand indices, on average, for a given signal strength $\beta^*$. This is confirmed in the left panel of Figure~\ref{fig_detectPower}. On the other hand, given the clusters that we estimated on the training set, smaller values of $\epsilon$ leave us with more power to detect differential expression on the test set. We define $\hat{\beta}_j^*$ to be the estimated GLM coefficient for a gene $X_j$ if we regress its mean vector $\Lambda_j$ onto the clusters estimated on the training set; note that $\hat{\beta}_j^* = \beta^*$ only if the estimated clusters are exactly equal to the true clusters. The right panel of Figure~\ref{fig_detectPower} plots the proportion of times that the differential expression p-value for a non-null gene was less than $0.05$, as a function of $\hat{\beta}_j^*$. We see that, for a given value of $\hat{\beta}_j^*$, the proportion of null hypotheses rejected is highest when $\epsilon$ is small, because smaller values of $\epsilon$ leave more information in the test set. 

% Figure environment removed







