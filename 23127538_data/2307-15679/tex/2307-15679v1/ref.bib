@article{whitney1984kalman,
  title={The Kalman filter family tree- A survey of state-of-the-art analysis methods based on the Kalman filter},
  author={WHITNEY, DA},
  journal={NAECON 1984},
  pages={426--432},
  year={1984}
}

@article{jaeger2002adaptive,
  title={Adaptive nonlinear system identification with echo state networks},
  author={Jaeger, Herbert},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}


@inproceedings{GRU,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Dzmitry Bahdanau and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  editor    = {Alessandro Moschitti and
               Bo Pang and
               Walter Daelemans},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
               {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  pages     = {1724--1734},
  publisher = {{ACL}},
  year      = {2014},
  url       = {https://doi.org/10.3115/v1/d14-1179},
  doi       = {10.3115/v1/d14-1179},
  timestamp = {Fri, 06 Aug 2021 00:40:23 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/ChoMGBBSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu2020improving,
  title={Improving the gating mechanism of recurrent neural networks},
  author={Gu, Albert and Gulcehre, Caglar and Paine, Thomas and Hoffman, Matt and Pascanu, Razvan},
  booktitle={International Conference on Machine Learning},
  pages={3800--3809},
  year={2020},
  organization={PMLR}
}


@InProceedings{gatebias,
  title = 	 {An Empirical Exploration of Recurrent Network Architectures},
  author = 	 {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2342--2350},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/jozefowicz15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/jozefowicz15.html},
  abstract = 	 {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM’s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM’s forget gate closes the gap between the LSTM and the GRU.}
}

@article{NTM,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}
@InProceedings{RNNEM,
author="Peng, Baolin
and Yao, Kaisheng
and Jing, Li
and Wong, Kam-Fai",
editor="Li, Juanzi
and Ji, Heng
and Zhao, Dongyan
and Feng, Yansong",
title="Recurrent Neural Networks with External Memory for Spoken Language Understanding",
booktitle="Natural Language Processing and Chinese Computing",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="25--35",
abstract="Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorise long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorisation capability of RNNs. Experiments on the ATIS dataset demonstrated that the proposed model was able to achieve state-of-the-art results. Detailed analysis may provide insights for future research.",
isbn="978-3-319-25207-0"
}
@article{DNC,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{NeuralStack,
  title={Learning to transduce with unbounded memory},
  author={Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{DMN,
  title={Ask me anything: Dynamic memory networks for natural language processing},
  author={Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  booktitle={International conference on machine learning},
  pages={1378--1387},
  year={2016},
  organization={PMLR}
}

@article{MemN2N,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{chang2019antisymmetricrnn,
  title={AntisymmetricRNN: A dynamical system view on recurrent neural networks},
  author={Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H},
  journal={arXiv preprint arXiv:1902.09689},
  year={2019}
}
@article{lim2021noisy,
  title={Noisy recurrent neural networks},
  author={Lim, Soon Hoe and Erichson, N Benjamin and Hodgkinson, Liam and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5124--5137},
  year={2021}
}
@inproceedings{ma2020particle,
  title={Particle filter recurrent neural networks},
  author={Ma, Xiao and Karkus, Peter and Hsu, David and Lee, Wee Sun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5101--5108},
  year={2020}
}

@inproceedings{rotman2021shuffling,
  title={Shuffling recurrent neural networks},
  author={Rotman, Michael and Wolf, Lior},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9428--9435},
  year={2021}
}
@inproceedings{ma2020temporal,
  title={Temporal pyramid recurrent neural network},
  author={Ma, Qianli and Lin, Zhenxi and Chen, Enhuan and Cottrell, Garrison},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5061--5068},
  year={2020}
}
@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1120--1128},
  year={2016},
  organization={PMLR}
}

@inproceedings{stateregularized,
  title={State-regularized recurrent neural networks},
  author={Wang, Cheng and Niepert, Mathias},
  booktitle={International Conference on Machine Learning},
  pages={6596--6606},
  year={2019},
  organization={PMLR}
}

@article{allen2019convergence,
  title={On the convergence rate of training recurrent neural networks},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zhang2021sbo,
  title={SBO-RNN: Reformulating Recurrent Neural Networks via Stochastic Bilevel Optimization},
  author={Zhang, Ziming and Yue, Yun and Wu, Guojun and Li, Yanhua and Zhang, Haichong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25839--25851},
  year={2021}
}
@inproceedings{massart2022coordinate,
  title={Coordinate descent on the orthogonal group for recurrent neural network training},
  author={Massart, Estelle and Abrol, Vinayak},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7744--7751},
  year={2022}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{le2015simple,
  title={A simple way to initialize recurrent networks of rectified linear units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}

@article{talathi2015improving,
  title={Improving performance of recurrent neural network with relu nonlinearity},
  author={Talathi, Sachin S and Vartak, Aniket},
  journal={arXiv preprint arXiv:1511.03771},
  year={2015}
}

@inproceedings{tomita:cogsci82,
  added-at = {2008-02-26T11:58:58.000+0100},
  address = {Ann Arbor, Michigan},
  author = {Tomita, M.},
  biburl = {https://www.bibsonomy.org/bibtex/2f58f1f713c87f73c452d9d57032bc3af/schaul},
  booktitle = {{P}roceedings of the Fourth Annual Conference of the Cognitive Science Society},
  citeulike-article-id = {2380316},
  description = {idsia},
  interhash = {1637072fd0f3a2ea005ae9c3c616c5f6},
  intrahash = {f58f1f713c87f73c452d9d57032bc3af},
  keywords = {inaki},
  pages = {105--108},
  priority = {2},
  timestamp = {2008-02-26T12:01:08.000+0100},
  title = {Dynamic Construction of Finite Automata from examples using Hill-climbing},
  year = 1982
}

@article{DBLP:journals/corr/ElliottFSS16,
  author    = {Desmond Elliott and
               Stella Frank and
               Khalil Sima'an and
               Lucia Specia},
  title     = {Multi30K: Multilingual English-German Image Descriptions},
  journal   = {CoRR},
  volume    = {abs/1605.00459},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.00459},
  eprinttype = {arXiv},
  eprint    = {1605.00459},
  timestamp = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ElliottFSS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

