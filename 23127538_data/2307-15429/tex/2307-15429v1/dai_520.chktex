dai_520.tex:160:79: Error: Solo `)' found. Compared with single-task learning (STL), MTL has two remarkable advantages: 1) the model typically has a smaller size and higher learning efficiency by sharing parameters across tasks \citep{misra2016cross, yang2020multi, vandenhende2021multi}, and 2) the performance on some tasks can be further improved due to the correlation between different tasks \citep{swersky2013multi}.   
dai_520.tex:160:252: Error: Solo `)' found. Compared with single-task learning (STL), MTL has two remarkable advantages: 1) the model typically has a smaller size and higher learning efficiency by sharing parameters across tasks \citep{misra2016cross, yang2020multi, vandenhende2021multi}, and 2) the performance on some tasks can be further improved due to the correlation between different tasks \citep{swersky2013multi}.   
dai_520.tex:323:2: Error: Solo `)' found. 1) The actor-critic structure \citep{peters2008reinforcement} allows the SAC model to be updated at each step, enabling IGBv2 to assign the current optimal weights in time;  
dai_520.tex:324:2: Error: Solo `)' found. 2) The replay buffer \citep{mnih2013playing} in SAC is a commonly used off-policy mechanism, which can improve sample efficiency by training the SAC model on random historical data. Because the MTL model can only be trained once, the training data available for our SAC model is much less than that for typical DRL applications, making sample efficiency crucial;  
dai_520.tex:325:2: Error: Solo `)' found. 3) The maximum entropy mechanism \citep{ziebart2008maximum} can increase the randomness of the action, thereby increasing the likelihood of discovering the global optimum of the MTL model.  
