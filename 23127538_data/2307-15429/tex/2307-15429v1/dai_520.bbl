\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arulkumaran et~al.(2017)Arulkumaran, Deisenroth, Brundage, and
  Bharath]{arulkumaran2017deep}
Kai Arulkumaran, Marc~Peter Deisenroth, Miles Brundage, and Anil~Anthony
  Bharath.
\newblock Deep reinforcement learning: A brief survey.
\newblock \emph{IEEE Signal Processing Magazine}, 34\penalty0 (6):\penalty0
  26--38, 2017.

\bibitem[Badrinarayanan et~al.(2017)Badrinarayanan, Kendall, and
  Cipolla]{badrinarayanan2017segnet}
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
\newblock Segnet: A deep convolutional encoder-decoder architecture for image
  segmentation.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 39\penalty0 (12):\penalty0 2481--2495, 2017.

\bibitem[Caruana(1998)]{caruana1998multitask}
Rich Caruana.
\newblock \emph{Multitask learning}.
\newblock Springer, 1998.

\bibitem[Chen et~al.(2021)Chen, Proietti, Liu, and Yoo]{chen2021multi}
Xiaoliang Chen, Roberto Proietti, Che-Yu Liu, and SJ~Ben Yoo.
\newblock A multi-task-learning-based transfer deep reinforcement learning
  design for autonomic optical networks.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 39\penalty0
  (9):\penalty0 2878--2889, 2021.

\bibitem[Chen et~al.(2018)Chen, Badrinarayanan, Lee, and
  Rabinovich]{chen2018gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  794--803. PMLR, 2018.

\bibitem[Chen et~al.(2020)Chen, Ngiam, Huang, Luong, Kretzschmar, Chai, and
  Anguelov]{chen2020just}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2039--2050, 2020.

\bibitem[Chowdhuri et~al.(2019)Chowdhuri, Pankaj, and
  Zipser]{chowdhuri2019multinet}
Sauhaarda Chowdhuri, Tushar Pankaj, and Karl Zipser.
\newblock Multinet: Multi-modal multi-task learning for autonomous driving.
\newblock In \emph{IEEE Winter Conference on Applications of Computer Vision},
  pages 1496--1504. IEEE, 2019.

\bibitem[Fey and Lenssen(2019)]{fey2019fast}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with pytorch geometric.
\newblock \emph{ArXiv Preprint ArXiv:1903.02428}, 2019.

\bibitem[Gasteiger et~al.(2020)Gasteiger, Gro{\ss}, and
  G{\"u}nnemann]{gasteiger2020directional}
Johannes Gasteiger, Janek Gro{\ss}, and Stephan G{\"u}nnemann.
\newblock Directional message passing for molecular graphs.
\newblock \emph{ArXiv Preprint ArXiv:2003.03123}, 2020.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Justin Gilmer, Samuel~S Schoenholz, Patrick~F Riley, Oriol Vinyals, and
  George~E Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1263--1272. PMLR, 2017.

\bibitem[Guo et~al.(2018)Guo, Haque, Huang, Yeung, and Fei-Fei]{guo2018dynamic}
Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li~Fei-Fei.
\newblock Dynamic task prioritization for multitask learning.
\newblock In \emph{European Conference on Computer Vision}, pages 270--287,
  2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and
  Moore]{kaelbling1996reinforcement}
Leslie~Pack Kaelbling, Michael~L Littman, and Andrew~W Moore.
\newblock Reinforcement learning: A survey.
\newblock \emph{Journal of Artificial Intelligence Research}, 4:\penalty0
  237--285, 1996.

\bibitem[Kendall et~al.(2018)Kendall, Gal, and Cipolla]{kendall2018multi}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 7482--7491, 2018.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ArXiv Preprint ArXiv:1412.6980}, 2014.

\bibitem[Kurin et~al.(2022)Kurin, De~Palma, Kostrikov, Whiteson, and
  Kumar]{kurin2022defense}
Vitaly Kurin, Alessandro De~Palma, Ilya Kostrikov, Shimon Whiteson, and M~Pawan
  Kumar.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock \emph{ArXiv Preprint ArXiv:2201.04122}, 2022.

\bibitem[Lin et~al.(2021)Lin, Ye, and Zhang]{lin2021closer}
Baijiong Lin, Feiyang Ye, and Yu~Zhang.
\newblock A closer look at loss weighting in multi-task learning.
\newblock \emph{ArXiv Preprint ArXiv:2111.10603}, 2021.

\bibitem[Lin et~al.(2022)Lin, Feiyang, Zhang, and Tsang]{lin2022reasonable}
Baijiong Lin, YE~Feiyang, Yu~Zhang, and Ivor Tsang.
\newblock Reasonable effectiveness of random weighting: A litmus test for
  multi-task learning.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Liu, Jin, Stone, and
  Liu]{liu2021conflict}
Bo~Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu.
\newblock Conflict-averse gradient descent for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18878--18890, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Li, Kuang, Xue, Chen, Yang, Liao,
  and Zhang]{liu2021towards}
Liyang Liu, Yi~Li, Zhanghui Kuang, J~Xue, Yimin Chen, Wenming Yang, Qingmin
  Liao, and Wayne Zhang.
\newblock Towards impartial multi-task learning.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Johns, and Davison]{liu2019end}
Shikun Liu, Edward Johns, and Andrew~J Davison.
\newblock End-to-end multi-task learning with attention.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 1871--1880, 2019.

\bibitem[Liu et~al.(2022)Liu, James, Davison, and Johns]{liu2022auto}
Shikun Liu, Stephen James, Andrew~J Davison, and Edward Johns.
\newblock Auto-lambda: Disentangling dynamic task relationships.
\newblock \emph{ArXiv Preprint ArXiv:2202.03091}, 2022.

\bibitem[Ma et~al.(2018)Ma, Zhao, Yi, Chen, Hong, and Chi]{ma2018modeling}
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed~H Chi.
\newblock Modeling task relationships in multi-task learning with multi-gate
  mixture-of-experts.
\newblock In \emph{ACM SIGKDD Conference on Knowledge Discovery and Data
  Mining}, pages 1930--1939, 2018.

\bibitem[Maron et~al.(2019)Maron, Ben-Hamu, Serviansky, and
  Lipman]{maron2019provably}
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman.
\newblock Provably powerful graph networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Misra et~al.(2016)Misra, Shrivastava, Gupta, and
  Hebert]{misra2016cross}
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 3994--4003, 2016.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{ArXiv Preprint ArXiv:1312.5602}, 2013.

\bibitem[Navon et~al.(2022)Navon, Shamsian, Achituve, Maron, Kawaguchi,
  Chechik, and Fetaya]{navon2022multi}
Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal
  Chechik, and Ethan Fetaya.
\newblock Multi-task learning as a bargaining game.
\newblock In \emph{International Conference on Machine Learning}, pages
  16428--16446. PMLR, 2022.

\bibitem[Peters and Schaal(2008)]{peters2008reinforcement}
Jan Peters and Stefan Schaal.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural Networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Ramakrishnan et~al.(2014)Ramakrishnan, Dral, Rupp, and
  Von~Lilienfeld]{ramakrishnan2014quantum}
Raghunathan Ramakrishnan, Pavlo~O Dral, Matthias Rupp, and O~Anatole
  Von~Lilienfeld.
\newblock Quantum chemistry structures and properties of 134 kilo molecules.
\newblock \emph{Scientific Data}, 1\penalty0 (1):\penalty0 1--7, 2014.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{ArXiv Preprint ArXiv:1706.05098}, 2017.

\bibitem[Sener and Koltun(2018)]{sener2018multi}
Ozan Sener and Vladlen Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Silberman et~al.(2012)Silberman, Hoiem, Kohli, and
  Fergus]{silberman2012indoor}
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock \emph{European Conference on Computer Vision}, 7576:\penalty0
  746--760, 2012.

\bibitem[Swersky et~al.(2013)Swersky, Snoek, and Adams]{swersky2013multi}
Kevin Swersky, Jasper Snoek, and Ryan~P Adams.
\newblock Multi-task bayesian optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Tang et~al.(2020)Tang, Liu, Zhao, and Gong]{tang2020progressive}
Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong.
\newblock Progressive layered extraction (ple): A novel multi-task learning
  (mtl) model for personalized recommendations.
\newblock In \emph{ACM Conference on Recommender Systems}, pages 269--278,
  2020.

\bibitem[Vandenhende et~al.(2021)Vandenhende, Georgoulis, Van~Gansbeke,
  Proesmans, Dai, and Van~Gool]{vandenhende2021multi}
Simon Vandenhende, Stamatios Georgoulis, Wouter Van~Gansbeke, Marc Proesmans,
  Dengxin Dai, and Luc Van~Gool.
\newblock Multi-task learning for dense prediction tasks: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 44\penalty0 (7):\penalty0 3614--3633, 2021.

\bibitem[Vinyals et~al.(2015)Vinyals, Bengio, and Kudlur]{vinyals2015order}
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.
\newblock Order matters: Sequence to sequence for sets.
\newblock \emph{ArXiv Preprint ArXiv:1511.06391}, 2015.

\bibitem[Watkins(1989)]{watkins1989learning}
Christopher John Cornish~Hellaby Watkins.
\newblock \emph{Learning from delayed rewards}.
\newblock King's College, Cambridge United Kingdom, 1989.

\bibitem[Wu et~al.(2020)Wu, Pan, Chen, Long, Zhang, and
  Philip]{wu2020comprehensive}
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S~Yu
  Philip.
\newblock A comprehensive survey on graph neural networks.
\newblock \emph{IEEE transactions on Neural Networks and Learning Systems},
  32\penalty0 (1):\penalty0 4--24, 2020.

\bibitem[Yang et~al.(2020)Yang, Xu, Wu, and Wang]{yang2020multi}
Ruihan Yang, Huazhe Xu, Yi~Wu, and Xiaolong Wang.
\newblock Multi-task reinforcement learning with soft modularization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4767--4777, 2020.

\bibitem[Ye et~al.(2021)Ye, Lin, Yue, Guo, Xiao, and Zhang]{ye2021multi}
Feiyang Ye, Baijiong Lin, Zhixiong Yue, Pengxin Guo, Qiao Xiao, and Yu~Zhang.
\newblock Multi-objective meta learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 21338--21351, 2021.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5824--5836, 2020.

\bibitem[Zhang and Yang(2021)]{zhang2021survey}
Yu~Zhang and Qiang Yang.
\newblock A survey on multi-task learning.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  34\penalty0 (12):\penalty0 5586--5609, 2021.

\bibitem[Zhao et~al.(2019)Zhao, Hong, Wei, Chen, Nath, Andrews, Kumthekar,
  Sathiamoorthy, Yi, and Chi]{zhao2019recommending}
Zhe Zhao, Lichan Hong, Li~Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee
  Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed~Chi.
\newblock Recommending what video to watch next: a multitask ranking system.
\newblock In \emph{ACM Conference on Recommender Systems}, pages 43--51, 2019.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey,
  et~al.]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, Anind~K Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~8, pages
  1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
