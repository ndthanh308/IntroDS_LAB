\section{Introduction}\label{sec:intro}
The great success of self-supervised pretraining with powerful transformer-style architectures \cite{devlin2018bert,deberta,DBLP:conf/icml/KumarIOIBGZPS16,fu2021violet,yang2021just,zellers2021merlot} has significantly boosted the performance of answering simple questions (\eg, ``what is the man doing'') on short videos (\eg, 3$\sim$15s) \cite{jang2017tgif,DBLP:conf/mm/XuZX0Z0Z17}. The advances thus point towards complex video question answering (VideoQA), \lyc{that features long video containing multiple objects and events} \cite{next-qa,causalvid,zhong2022video}. Compared with simple VideoQA, complex VideoQA poses several unique challenges: 


% Figure environment removed



1) \textbf{Longer videos with multiple objects interacting differently at different time.} The long video and rich visual content indispensably bring more background scenes that include  massive question-irrelevant video moments and objects. For example, to answer the question in \cref{fig:1a}, only the interaction between object ``person'' and ``bike'' on the last three frames encloses the answer information, leaving the massive rest as background. These backgrounds, if not filtered properly, will overwhelm the critical scene and interfere with answer prediction. 
% answer prediction, especially when there are hard negative answers presented in the candidate answer list, \eg, ``ride the bike'' \vs ``push the bike'' in the example shown in~\cref{fig:1a}. The distractor ``ride the bike'', though irrelevant to the question,  corresponds to a large part of the video.
%
2) \textbf{Harder negative answer as distractors.}
% Complex VideoQA is often set as multiple-choice QA where the negative answers are tailored for each video-question instance. Such a setting, together with the massive video content, provides an ideal foundation to build hard negative candidates as distractors. The hard negatives are usually very similar to the correct answer but correspond to a different video moment or object. For example, the answer candidate ``A. ride the bike'' of \cref{fig:1a}, though incorrect with respect to the question, is related to a large part of the video. As a result, these distractors can seriously derail the prediction if not properly modeled.
% \lyc{While negative answer in simple VideoQA contains little semantic information and is easy to rule out. 
Negative answers in complex VideoQA are typically tailored for each video instance. Due to the massive video content, the vast question-irrelevant scene provides an ideal foundation to build a hard negative candidate as distractor.
The hard negatives are very similar to the correct answer but correspond to a different video moment or object.
For example, the answer candidate ``A.ride the bike'' of \cref{fig:1a}, though irrelevant to the question,  corresponds to a large part of the video. 
As a result, these distractors can seriously derail the prediction if not properly modeled.


In light of the challenges, current methods (both pretrained and task-specific architectures) hardly perform well on complex VideoQA. 
% As shown by our preliminary studies (refer to \cref{fig:1b,fig:1c}), 
In \cref{fig:1b,fig:1c}, we use the video length and number of objects\footnote{We acquire the object number using annotation of video relation detection dataset \cite{vidvrd}, which shares same source video as NExT-QA.} to indicate the complexity of the video questions. We can see that current methods suffer a drastic performance drop when video length increases or more objects are involved. The reason can be provided from two aspects:
% 1) On the one hand, the pretaining methods typically embrace datasets with simple questions and short videos, 
% where the answer can be easily captured via a static frame without the video dynamics. This severely hindered their transferability on complex VideoQA. 
% %
% On the other hand, the task-specific methods also suffer from limited generalization ability, since they present a video as a fixed number of frames and objects that cannot adapt to different visual content, which rigidity undermines their performance on diverse VideoQA tasks.
%
% \textbf{First}, confronting long video and multiple objects, pretrained methods suffer from a domain gap, since they are typically pretrained with static images or sparse frames \cite{DBLP:conf/icml/KumarIOIBGZPS16, lei2021less} for single event understanding, without the awareness of performing fine-grained reasoning over long videos with multiple objects and events. 
%
\textbf{First}, confronting long video and multiple objects, pretrained methods suffer from a domain gap. Because they are typically pretrained with short videos and simple questions, \cite{DBLP:conf/icml/KumarIOIBGZPS16, lei2021less} 
where the answer can be easily captured via a static frame, without fine-grained reasoning over multiple objects in a long video. 
% 
While recent task-specific methods exploit object-level representation for fine-grained reasoning \cite{hostr,pgat,hqga,VGT}, they exhibit limited generalization ability, as they handle different videos with only a fixed number of frames and objects, and cannot adapt to lengthy and varied visual content,
% This rigidity undermines their performance in generalizing to a wide range of video question-answering content.
which rigidity undermines their adaptability to a wide range of video content.
%%
\textbf{Second}, to model the answer candidate, prevailing designs \cite{jang2017tgif,gao2018motionappearance,fan2019heterogeneous,hga,hqga} append the candiate to the question and treat the formed question-answer sequence as a whole for cross-modal learning. However, this makes the answer candidate directly interact with the whole video content, which gives rise to a strong spurious correlation between the hard negative candidates (\eg ``A. ride the bike" in \cref{fig:1a}) and the question-irrelevant scenes (\eg `` riding'' scene in first three frames), leading to a false positive prediction. 
% without inspecting the critical scenes.
%
% To model the answer candidates, prevailing designs tend to append it to the question and treat the textual sequence as a whole for cross-modal encoding, which makes answer candidates directly fuse with video. 
% However, this approach can establish a strong spurious correlation between a negative candidate (\ie ``A.ride the bike" in \cref{fig:1a}) and question-irrelevant scenes (\ie `` riding'' scene in first three frames), leading to a false prediction without inspecting gold ``B.push the bike'' answer on the critical scene. 

In this regard, we propose TranSTR, a Transformer-style VideoQA architecture that coordinates a Spatio-Temporal Rationalization (STR) with a more reasonable video-text interaction pipeline for candidate answer modeling.
%
% Concretely, STR works as a differentiable selection module that adaptively collects question-critical frames and objects.
%
% STR is a differentiable selection module that adaptively rules out the vast background content which might derail the prediction during answer decoding. 
%
STR first temporally collects the critical frames from a long video, followed by a spatial selection of objects on the identified frames. By further fusing the selected visual objects and frames via light-weight reasoning module, we derive spatial and temporal rationales that exclusively support answering. 
% deocoder
% In addition, to circumvent the spurious correlation in the current modeling of answer candidates. We formulate a novel reasoning pipeline by separating the question-and-answer candidates. Specifically, during multi-modal encoding, only the question is fused with the video to filter out the background via STR, and the answer candidates are introduced afterward via a transformer-style decoder with a set of learnable answer queries. 
%
% However, 
% % stacking with the 
% current way of modeling answer candidates still makes STR suffer from the candidate-background correlation, leading to the question-irrelevant frame and object being identified as critical.
% To circumvent this issue, we formulate a more reasonable video-text interaction pipeline, where the question and answer candidates are separately (instead of being appended as a whole) fed to the model at different stages.
In addition to STR, we circumvent the spurious correlation in the current modeling of answer candidates by formulating a more reasonable video-text interaction pipeline, where the question and answer candidates are separately (instead of being appended as a whole) fed to the model at different stages. 
% 
Specifically, before rationale selection, only the question is interacted with the video to filter out the massive background content while keeping question-critical frames and objects. After that, a transformer-style answer decoder introduces the answer candidates to these critical elements to determine the correct answer.
% Specifically, before rationale selection, only the question is interacted with the video to filter out the massive background content. After identifing the question-critical frames and objects, the answer candidates are introduced to interact with these critical elements to determine the correct answer. 
Such a strategy prevents the interaction between the hard negative answers and the massive background scenes, thus enabling our STR to perform better on complex VideoQA (see TranSTR in \cref{fig:1b,fig:1c}). 
\lyc{It is worth noting that STR and the answering modeling are reciprocal. Without STR's selection, all visual content will still be exposed to answer candidates. Without our answer modeling, STR could identify the question-irrelevant frame and object as critical.
Thus, the success of TranSTR is attributed to the integration of both.}




















Our contributions are summarized as follows:

% \begin{itemize}
%     \item We analyze the necessity and challenge of complex VideoQA. We then identify the crucial importance of discovering spatio-temporal rationales and preventing spurious correlation in modeling candidate answers. 
%     \item We propose TranSTR, a Transformer-style instantiation of our solution, which features a spatio-temporal rationalization (STR) module together with a more reasonable candidate answer modeling strategy. The answer modeling strategy is independently verified to be effective in boosting other existing VideoQA models.
%     \item We achieve new SoTA performances on complex VideoQA benchmarks, surpassing the previous SoTA by a large margin (\eg, NExT-QA \cite{next-qa} +5.8\%, CausalVid-QA\cite{causalvid} +6.8\%). Additionally, our method also outperforms the task-specific architectures and shows competitive results to the pretrained methods on traditional simple VideoQA benchmarks. 
%     % (\eg, MSVD-QA \cite{DBLP:conf/mm/XuZX0Z0Z17} +3.5\%, MSRVTT\cite{DBLP:conf/mm/XuZX0Z0Z17}  +3.4\%).
% \end{itemize}

% \begin{itemize}[leftmargin=*]
%     \item We analyze the necessity and challenge of complex VideoQA task. As a key solution, we identify the crucial importance of discovering spatio-temporal rationales along with a more reasonable candidate answer modeling strategy.
%     \item We identify the spurious correlation in the current use of answer candidates, and propose a novel candidate modeling strategy via a query-based answer decoder, which can boost any off-the-shelf Video QA model.
%     \item We perform extensive experiments and achieve SoTA performance on four popular benchmarks, especially for complex VideoQA. (NExT-QA \cite{next-qa} +5.8\%, CausalVid-QA \cite{causalvid} +6.8\%)
% \end{itemize}

\begin{itemize}[leftmargin=*]
    \item We analyze the necessity and challenge of complex VideoQA. To solve the task, we identify the importance of discovering spatio-temporal rationales and preventing spurious correlation in modeling candidate answers.
    \item We propose TranSTR that features a spatio-temporal rationalization (STR) module together with a more reasonable candidate answer modeling strategy. The answer modeling strategy is independently verified to be effective in boosting other existing VideoQA models.
    \item We perform extensive experiments and achieve SoTA performance on four popular benchmarks, especially for ones that features complex VideoQA (NExT-QA \cite{next-qa} +5.8\%, CausalVid-QA \cite{causalvid} +6.8\%)
\end{itemize}


