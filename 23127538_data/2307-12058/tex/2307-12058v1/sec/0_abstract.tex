\begin{abstract}
This paper strives to solve complex video question answering (VideoQA) which features long video containing multiple objects and events at different time. To tackle the challenge, we highlight the importance of identifying question-critical temporal moments and spatial objects from the vast amount of video content. Towards this, we propose a \textbf{S}patio-\textbf{T}emporal \textbf{R}ationalization (STR), a differentiable selection module that adaptively collects question-critical moments and objects using cross-modal interaction.
The discovered video moments and objects are then served as grounded rationales to support answer reasoning. Based on STR, we further propose TranSTR, a Transformer-style neural network architecture that takes STR as the core and additionally underscores a novel answer interaction mechanism to coordinate STR for answer decoding. Experiments on four datasets show that TranSTR achieves new state-of-the-art (SoTA). Especially, on NExT-QA and Causal-VidQA which feature complex VideoQA, it significantly surpasses the previous SoTA by 5.8\% and 6.8\%, respectively. We then conduct extensive studies to verify the importance of STR as well as the proposed answer interaction mechanism. With the success of TranSTR and our comprehensive analysis, we hope this work can spark more future efforts in complex VideoQA.  
Code will be released at \url{https://github.com/yl3800/TranSTR}.


% 修改原因：老板说abs没有体现出TranSTR 为啥好的原因

% In this paper, we formulate the challenge of the complex VideoQA, in which the question typically refers to multiple objects in a long video. This inevitably carries massive question-irrelevant visual content, which not only brings visual redundancy but more importantly, establishes a strong spurious correlation to a tailored negative answer.  

% \lyc{In this paper, we formulate the challenge of the complex VideoQA, in which the massive video content with tailored answer candidates not only introduces vast question-irrelevant visual content as redundancy, but more importantly, arises a strong spurious correlation between a hard negative answer and background, thus misleading the prediction.
% %
% To tackle this, we first propose a \textbf{S}patio-\textbf{T}emporal \textbf{R}ationalizer (STR) to adaptively trim off question-irrelevant scenes, then coordinate it with a novel model-agnostic answer decoding scheme to alleviate negative candidate-background correlation in the existing modeling of answer candidates.
% %
% Instantiating this pipeline with a transformer achitecture, we show our method, TranSTR, achieves significant improvements over SoTAs, especially on NExT-QA (+5.8\%) and Causal-VidQA (+6.8\%) that feature complex VideoQA.
% We further conduct extensive studies to verify the importance of STR as well as the proposed answer interaction mechanism. With the success of TranSTR and our comprehensive analysis, we hope to spark future efforts in complex VideoQA.} All results are fully reproducible at \url{https://anonymous.4open.science/r/TranSTR/}

% TranSTR achieves new state-of-the-art (SoTA). Especially, on NExT-QA and Causal-VidQAwhich feature complex VideoQA, it significantly surpasses the previous SoTA by 5.8\% and 6.8\%, respectively. We then conduct extensive studies to verify the importance of STR as well as the proposed answer interaction mechanism. With the success of TranSTR and our comprehensive analysis, we hope this work can spark future efforts in complex VideoQA. All results are fully reproducible at \url{https://anonymous.4open.science/r/TranSTR/}

% Worse still, the talior-made hard negative answer candidates establishs a suprious correlation bettenhave crippled existing methods, due to their inability to handle visual redundancy and inappropriate answer modeling. 
% To tackle the challenge, we highlight the importance of identifying question-critical temporal moments and spatial objects from the vast amount of video content.

% We then propose STR to adaptively trim off question-irrelevant scenes, while coordinating it with a novel model-agnostic answer decoding scheme to alleviate distractor-background correlation in the existing use of answer candidates.
% Instantiating on this pipeline, we show our method, TranSTR, achieves significant improvements over SoTAs, especially on complex VideoQA benchmarks.
\vspace{-0.5cm}
\end{abstract}