\section{Related Works}\label{sec:related}

\noindent\textbf{Video Question Answering (VideoQA).}
Substantiated as a fundamental extension of ImageQA, VideoQA has enlarged its definition by adding a temporal extension. 
% 
According to the pre-extracted feature granularity, existing methods either use the frame-level features or incorporate object features for fine-grain reasoning. 
% Aiming to reveal how the machine reflects the physical world, Video Question Answering VideoQA has gained drastic progress over the last few years. This advancement stems, in part from the recent surge of video-language pre-training models, and in part from the continuing enthusiasm for task-specific architecture design.
% %
% In pre-train methods, 
In task-specific designs, Focus on simple questions and short videos, earlier efforts tend to model the video sequence as a visual graph using purely frame features. As the pioneer of graph-based structure, \cite{hga} and \cite{park2021bridge} build their typologies based on the heterogeneity of input modality,  while \cite{mspan} enables progressive relational reasoning between multi-scale graphs. 
%
Recently, the emergence of complex VideoQA benchmarks \cite{next-qa, causalvid} has prompted studies on long video with multiple visual entities. 
In this regard, Another line of research has prevailed by processing video as multi-level hierarchy. 
\cite{hcrn} first build a bottom-up pathway by assembling information first from frame-level, then merging to clip-level. The following works \cite{hostr, hqga} extend the hierarchy into the object-level, where a modular network is designed to connect objects on the same frame. Most recently, \cite{VGT} establish its improvement by enabling relation reasoning in a sense of object dynamics via temporal graph transformer.
%
Despite effectiveness, the current designs unanimously rely on a fixed number of frames and objects, which severely compromise their transferability across diverse video instances.
In sharp contrast, our method works in a fully adaptive manner to explicitly select frames and objects for reasoning over different circumstances, which demonstrates superior generalization ability.

\vspace{5pt}
\noindent\textbf{Rationalization.}
In pursuit of  explainability, the recent development of DNN is encouraged to reveal the intuitive evidence of their prediction, \ie the rationales. 
As one of the prevailing practices, the rationalization has been extended from the NLP community\cite{DBLP:conf/kdd/Ribeiro0G16} to the Graph \cite{DIR} and Vision field \cite{DBLP:conf/cvpr/ZhangYMW19}. 
Recently, this development also stems from the multi-modal community.
\cite{DBLP:conf/cvpr/ParkHARSDR18} and \cite{DBLP:conf/cvpr/DuaKB21} proposes ImageQA-based tasks that inquire about additional textual evidence, \cite{causalvid} brings this idea to the videoQA.
Despite the progress, the recent solution focus on the rationale only at frame level, and they either require a rationale finder with heavy computation overhead \cite{IGV} or needs to be trained in a data-hungry contrastive manner \cite{EIGV}. TranSTR, however, identifies both critical frames and objects from an efficient cross-modal view. 
% 
Also, distinct from the token reduction method in transformer literature, which trades accuracy for efficiency. Rationalization intends to improve performance \cite{rationalization-robustness}. The intuition behind is that, if a model can find the causal part, they have the potential to ignore the noise. 
% In VideoQA, only a small proportion of critical interactions are responsive to answering, leaving the rest as redundancy; this makes reasoning of VideoQA suffer a highly sparse target signal.

% \vspace{5pt}
% \noindent\textbf{Differentiable Selection.}


