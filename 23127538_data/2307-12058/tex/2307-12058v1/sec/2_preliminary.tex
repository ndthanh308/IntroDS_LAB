\section{Preliminaries}
\label{sec:preliminaries}
% Here, we provide a formal definition of VideoQA. 
% Throughout the paper, we use upper-cased (\eg $A$) and lower-cased (\eg $a$) letters to denote a variable and its deterministic value, respectively.

\vspace{5pt}
\noindent \textbf{Modeling.}
Given the video $V$ and the question $Q$, the VideoQA model $\phi(V,Q)$ aims to encapsulate the visual content and linguistic semantics and choose the predictive answer $\hat{A}$ from the answer candidates.
% knowledge $\hat{M}$ that sufficiently enclosed answer information, 
%
Typically, an entropy-based risk function $\mathcal{L}(\phi(V,Q), A)$ is applied to approach the ground-truth answer ${A}$.
% to abridge the predictive answer $\hat{A}$ with the ground-truth answer ${A}$. 
% % \begin{gather}\label{equ:erm-loss}
% %     \min_{f}\mathcal{L}(f(V,Q), A).
% % \end{gather}

\vspace{5pt}
\noindent \textbf{Data representation.}
We uniformly sample $T$ clips and keep the middle frame of each clip to represent a video.
Then, for each frame, we extract a frame feature $\Mat{f}_t$ via a pretrained image recognition backbone and $S$ object features $\Mat{o}_{t,s}$ using pretrained object detector, where $t,s$ denotes the $s$-th object on the $t$-th frame.
To represent the text, we encode the question as a sequence of $L$ tokens using a pretrained language model and obtain a textual representation $\Mat{q}_l$ for each of them. The visual backbones are frozen during training while the language backbone is finetuned end-to-end as in \cite{VGT}.
% To extract features from the video, we first uniformly sample $K$ frames from the video and extract frame features $\Mat{f}_k$ using a pretrained image backbone. We also extract $N$ object features $\Mat{o}{k,n}$ on each frame using a pretrained object detector, where $k$ denotes the $k$-th frame and $n$ denotes the $n$-th object on that frame. For the text input, we encode the question as a sequence of $L$ tokens using a pretrained language model and obtain a textual representation $\Mat{q}$.
To project the representations into a common $d$-dimensional space, we apply a three linear mappings on $\Mat{f}_t$, $\Mat{o}_{t,s}$, and $\Mat{q}_l$, respectively, and thus acquire 
$\Mat{F}\!=\!\left\{ \Mat{f}_t \right\}_{t=1}^{T} \!\in\!\Space{R}^{T\times d}$, 
$\Mat{O}\!=\!\left\{ \Mat{o}_{t,s}\right\}_{t=1,s=1}^{T,S}\!\in\!\Space{R}^{T\times S \times d}$, 
and 
$\Mat{Q}\!=\!\left\{ \Mat{q}_l \right\}_{l=1}^{L}\!\in\!\Space{R}^{L\times d}$ to denote the frame, object, and question features, respectively.



