\section{Experiments}\label{sec:exp}
\input{tab/dataset}
% \input{tab/main}
% \input{tab/causalvid}
% \input{tab/ablation}

\noindent\textbf{Datasets:}
% Experiments are conducted on four benchmarks to challenge TranSTR from different aspects. 
% Specifically, the prevailing \textbf{NExT-QA} \cite{next-qa} and \textbf{Causal-VidQA} \cite{causalvid} features the complex VideoQA task by adopting Multi-Choice setting, which aims to challenge the model's temporal reasoning ability with complex causal and commonsense relations. 
% \textbf{MSVD-QA} \cite{DBLP:conf/mm/XuZX0Z0Z17} and \textbf{MSRVTT-QA} \cite{DBLP:conf/mm/XuZX0Z0Z17} employ the Open-Ended setting, and they mainly emphasize the description of the video objects, activities, as well as their attributes. Detailed statistics of the datasets are listed in \cref{tab:dataset}.
%
Experiments were conducted on four benchmarks to evaluate TranSTR's performance from different aspects. Specifically, the recent NExT-QA \cite{next-qa} and Causal-VidQA \cite{causalvid} datasets challenge models with complex VideoQA tasks using a Multi-Choice setting, which aims to test their temporal reasoning ability with complex causal and commonsense relations. In addition, MSVD-QA \cite{DBLP:conf/mm/XuZX0Z0Z17} and MSRVTT-QA \cite{DBLP:conf/mm/XuZX0Z0Z17} employ an Open-Ended setting and emphasize the description of video objects, activities, and their attributes. Their statistics are presented in Table \ref{tab:dataset}.

\vspace{5pt}
\noindent\textbf{Implementation Details:}
% Follow the convention \cite{VGT}, we sample a video as a sequence of $T$=16 frames where each frame is encoded by a ViT-L \cite{vit} pre-trained on ImageNet-21k. Then, we use Faster-RCNN \cite{faster-rcnn} pre-trained on the Visual Genome as an object detector to acquire $S$=20 objects on each frame.
% As for the textual embedding, pretrained a Deberta model \cite{deberta} is adopted as language model an to encode the question and answer.  In training, we use an Adam optimizer with a learning rate of 1e-5, and the hidden dimension $d$ is set to 768. For the hyper-parameters, we set $K_f$=5 and $K_o$=12 for all datasets.
%
Following the convention established in \cite{VGT}, we sample each video as a sequence of $T$=16 frames, where each frame is encoded by a ViT-L \cite{vit} model that pre-trained on ImageNet-21k. To extract object-level features, we employ a Faster-RCNN \cite{faster-rcnn} model that pre-trained on the Visual Genome and detect $S$=20 objects in each frame.
%
For the textual encoding, we adopt a pretrained Deberta-base model \cite{deberta} to encode the question and answer. During training, we optimize the model using an Adam optimizer with a learning rate of 1e-5, and set the hidden dimension $d$ to 768. For the hyper-parameters, we set $K_f$=5 and $K_o$=12 for all datasets.

\vspace{5pt}
\noindent \textbf{Next}, we show our experimental results to answer the following questions:
\begin{itemize}[leftmargin=*]
\setlength\itemsep{-.20em}
    \item \textbf{Q1:} How is TranSTR compared with the SoTA?
    \item \textbf{Q2:} How effective are the proposed components?
    \item \textbf{Q3:} What learning pattern does the rationlizer capture?
    % \vspace{-15pt}
\end{itemize}

\subsection{Main Result (Q1)}
In \cref{tab:main,tab:causal_vid}, we show that TranSTR outperforms SoTAs on all question types. Our observations are as follows:

% 不同dataset的表现差异， 上更好，因为mc的视频和问题都更长，更难。
% \begin{itemize}[leftmargin=*]
% \setlength\itemsep{-.20em}
    % C 和basline 差不多
    % \item 
\textbf{QA setting.}
Comparing the performance of TranSTR with state-of-the-art methods on four datasets, we observe that TranSTR achieves a greater improvement on Multi-Choice (NExT +5.8\% and Causal-Vid +6.8\%) compared to Open-End QA (MSVD +3.5\% and MSRVTT +3.4\%). This can be explained from two aspects:
%
(1) Unlike Open-Ended datasets that contain simple questions and short videos, Multi-Choice datasets (NExT and Causal-Vid) focus on complex VideoQA, in which composite question sentences with long videos and multiple objects (see \cref{tab:dataset}) makes the identifying and inspecting of critical scenes necessary. This aligns with TranSTR's design philosophy of removing redundancy and explicitly exposing critical elements. Therefore, TranSTR achieves a larger gain on complex VideoQA.
%
(2) In multi-choice QA, SoTA methods often append candidate answers to the question during encoding, which can create a spurious correlation between the negative answer and the question-irrelevant scene, leading to a false prediction. However, such an issue is less significant in open-ended QA, where each answer candidate is treated as a one-hot category without semantic meaning. Thus, the decoder of TranSTR brings extra benefits to the multi-choice setting, and result in larger gains compared to open-ended QA.

% \vspace{-3pt}
% \item 
\textbf{Question-type.}
Based on the analysis of Multi-Choice datasets, we observe that the improvement in overall performance of TranSTR is largely due to the enhancement in answering composite questions (including Acc@C and Acc@T in NExT-QA, Acc@E and Acc@P and Acc@C in Causal-VidQA) that require deeper understanding such as causal relations and counterfactual thinking, compared to the descriptive question type (Acc@D:+1.8$\sim$2.7\%). This demonstrates TranSTR's outstanding reasoning ability for complex VideoQA tasks.
%
In particular, for Causal-VidQA, TranSTR shows a significant improvement in answering reason-based questions (Acc@P:AR +10.5\%, Acc@C:AR +8.3\%). Because questions of this type require the model to justify its prediction by selecting the correct evidence, which aligns with the concept of rationalization in TranSTR's design philosophy. Therefore, TranSTR's rationalization mechanism enables it to perform optimally in answering reason-based questions.
% \end{itemize}
\input{tab/main}
\input{tab/causalvid}

\subsection{In-Depth Study (Q2)}
\subsubsection{Ablative Results} 
\input{tab/ablation}
We validate the key components of TranSTR by performing model ablation and discussing other implementation alternatives.
% \begin{itemize}[leftmargin=*]
% \setlength\itemsep{-.20em}
As shown in \cref{tab:ablation-loss}, we first study the effectiveness of TranSTR by removing both STR and decoder (``w/o STR \& decoder''), which induces a severe performance decline on every question type. 
%
Then, we conduct experiments to study STR. As a detailed breakdown test, we notice that reasoning with all frames without temporal rationalization (w/o ``TR'') will cause a performance drop.
A similar declination is also observed when spatial rationalization is erased (w/o ``SR''), that is, all objects on the selected frame are used for reasoning. 
Such performance drops are expected, because a large proportion of frames only contain a question-irrelevant scene, and the pretrained object detector will inevitably introduce noisy objects. These question-irrelevant contents, if not properly ruled out, will make the background overwhelm the causal information, due to its spurious correlation with the answer distractor. 
As a result, we witness a more significant performance drop when both temporal and spatial rationalization are removed (w/o ``STR'').
%
Next, we validate the effectiveness of our decoder design by adopting a conventional implementation that concatenates each answer candidate with the question before feeding it to the model. This variant, remarked as ``w/o decoder'', also caused a substantial performance drop, which highlights the importance of our video-text interaction pipeline in eliminating the background-distractor correlation.
%
Comparing the performance of (w/o ``STA'') and (w/o ``deocder'') to (``w/o STR \& decoder''), we show that removing both STR and decoder induce a more severe decline, which demonstrates that STR can coordinate well with the proposed decoder and their benefits are mutually reinforcing.
%
We also evaluate the importance of our MGR module by replacing it with average pooling. This variant, denoted as ``w/o MGR'', uses an average pooling to gather all objects on each frame and adds the pooled representation to the corresponding critical frames. We observed a significant performance drop when compared to the original TranSTR, which confirms the necessity of MGR in aggregating the multi-grain evidence for reasoning.
To validate that the STR indeed learns to focus on the critical elements instead of making random choices, we replace our differentiable top-K module, with a random K selection. As a result, the performance of ``Random K" drops drastically, which verifies the proposed STR is fully trainable to capture the answer information.
In addition, we also verify our choice of the differentiable module by replacing our perturbed maximum method with the ``SinkHorn Top-K'' \cite{sinkhorn}, and the results validate our implementation.




% First, we notice that reasoning with all frames without temporal selection (w/o TA) will cause a drop in TranSTR's performance.
% A similar declination is also observed when spatial selection is erased (w/o SA), that is, all objects on the selected frame are used for reasoning. 
% Such performance drops are expected, because a large proportion of frames only contain a question-irrelevant scene, and the pretrained object detector will inevitably introduce noisy objects. This question-irrelevant information, if not properly ruled out, will make the redundancy overwhelm the causal information. 
% As a result, we witness a more significant performance drop when both temporal and spatial selection are removed (w/o STR)
% Further, to validate that the rationalizer indeed learned to focus on the critical elements instead of making random choices, we replace our differentiable topK module, with a random K selection. As a result, the performance of ``Random" drops drastically, which verifies the proposed rationalizer is fully trainable to capture the answer information.
% In addition, we also verify our choice of the differentiable module by replacing our perturbed maximum method with the SinkHorn TopK \cite{sinkhorn} (denoted as ``SinkHorn''), and the results validate our implementation.

% % \item In the bottom part, we first verified the superiority of MGR by replacing the  aggregation module with average pooling. Concretely, instead of leveraging the cross-attention \cref{eq:mga}, we use an average pooling to gather all objects on each frame and add the pooled representation to the corresponding critical frames. We denote this variant as w/o MGR, and we a severe drop when compared to the original TranSTR, which verifies the necessity of MGR.
% % In the last raw,  we verify the superiority of our decoder design, where the variant (w/o decode) is realized by following the conventional implementation that concatenates each answer candidate with the question before feeding to the model.  As expected, such implementation causes a substantial performance drop.

% % \item 
% In the bottom part of \cref{tab:ablation-loss}, we first evaluated the importance of our MGR module by replacing it with average pooling. This variant, denoted as ``w/o MGR'', uses an average pooling to gather all objects on each frame and adds the pooled representation to the corresponding critical frames. We observed a significant performance drop when compared to the original TranSTR, which confirms the necessity of MGR in aggregating the multi-grain evidence for reasoning.
% %
% Next, we validated the effectiveness of our decoder design by comparing it with a conventional implementation that concatenates each answer candidate with the question before feeding it to the model. This variant, remarked as ``w/o decoder'', also caused a substantial performance drop, which highlights the importance of our reasoning framework in eliminating the spurious correlation between negative answers and environmental scenes.
% Last, we show that removing both STR and decoder induce a more severe decline than variants without either of them, which demonstrates that STR can coordinate well with our reasoning framework and their benefits are mutually reinforcing.

% % \end{itemize}

\subsubsection{Analysis on Complex VideoQA}
\input{tab/vt_obj}
\input{tab/decoder}
In \cref{tab:vt_obj }, we compare the results of TranSTR on the simple and complex VideoQA, where the test set of NExT-QA \cite{next-qa} is split by the length and object number of the source video, respectively. 
By calculating the accuracy within each group, we notice that all existing methods, as well as the TranSTR baseline (TranSTR without STR and proposed decoder), suffer from a performance drop when the video length exceeds 80 seconds (diff: -1.3\%$\sim$-2\%) or the video contains more than 5 objects (diff: -0.5\%$\sim$-1.9\%)). 
In contrast, TranSTR has alleviated this issue by explicitly ruling out redundant frames and noisy objects, thus resulting in even better performance on complex samples.

Similar to \cref{fig:1b}, we also investigate how the TranSTR performs on samples with different video lengths. In \cref{fig:vid_len}, we sort all test samples based on their video length and use a subset with the top percentage of longest videos to calculate accuracy. For example, 10\% on the x-axis denotes the accuracy of samples with the top 10\% longest videos, and 100\% denotes all samples considered.
Although the performance of TranSTR and baseline are initially comparable, the advantage of TranSTR becomes more pronounced as videos become longer. As a result, when the subset narrows down to the 10\% longest videos, we observe a difference in the accuracy of over 4\%.

\vspace{-5pt}
\subsubsection{Study of Decoder}
Our video-text interaction pipeline is able to cater to any SoTA methods without compromising their structure. Thus, we apply the proposed decoder to three VideoQA backbones by separating the answer candidates from the question and feeding them to our answer decoder. As shown in \cref{tab:decoder}, our decoder is able to consistently improve the performance of all backbones, validating the assumption that isolating the answer candidates from the video-text encoding can eliminate the spurious correlation between a negative answer and background scenes, resulting in favorable gains for the backbone models.
%
Moreover, our decoder design is also more efficient compared to the conventional implementation. In the traditional approach, a question needs to be fused with the video multiple times, with each time concatenating a different answer candidate. In contrast, our design only forwards the question once, as the answer candidates are introduced only after the video-question fusion, resulting in a much more efficient architecture.



% Figure environment removed



% Figure environment removed

\vspace{-5pt}
\subsubsection{Study of Hyper-parameter}
To validate the sensitivity of TranSTR to the number of collected interactions, we conduct experiments with variations of $K_f$ and $K_o$ on NExT-QA. Without loss of generality, we tune $K_f$ ($K_o$) while setting $K_o$ = 12 ($K_f$ = 5). According to \cref{fig:k}, we observe that the performance of TranSTR varied mostly in the range of 61\% to 61.5\% under different combinations of hyperparameters, which demonstrated the effectiveness of TranSTR's adaptive design. However, we also notice a significant drop in some corner cases. When $K_f$ ($K_o$) is too small, the number of critical frames (objects) is limited, which hinders the performance as some visual evidence for answering is missing. Similarly, when $K_f$ is larger than 10, it introduces too much background, thus hurting the performance.



\subsection{Study of Critical Frames and Objects (Q3)}
\vspace{-3pt}
\noindent \textbf{Quantitative study.} 
To grasp the learning insight of TranSTR, we inspect the number of frames and objects that are collected as critical by the adaptive rationalization. 
Concretely, we draw the number of frames $C$ and non-critical frames $T$-$C$ in \cref{fig:k}. For the visual objects, since the number of critical objects $C_t$ varies according to frame content, we take the average of $C_t$ overall all critical frames, while leaving the rest objects as non-critical.
As a result, TranSTR can pinpoint a small group of tokens as critical tokens while leaving the rest as redundancy, which manifests the mass of question-irrelevant content in the original video, thus pointing the necessity of rationalization. 

\vspace{2pt}
\noindent \textbf{Qualitative study.}
To capture the learning pattern of TranSTR, we present some prediction results in \cref{fig:case_study} along with the identified frames and objects.
In general, TranSTR can locate very few indicative elements as visual evidence. 
In question 1, we show the effectiveness of the STR. In temporal selection, it rules out the environment scene and targets the first four "swing" frames as critical. Next, in the spatial selection, it excludes non-causal objects (\ie "dog") and focuses on the relation between question-relevant objects (\ie "man" and "baby"). By aggregating the critical elements in frames and objects, TranSTR successfully reaches the gold answer. As a comparison, when the STR is removed, the massive background overwhelms the salient reasoning pattern and leads to a false prediction.
%
Question 2 demonstrates the effect of our answer decoder. We can see that TranSTR targets three critical frames that encompass the question-referred ``person", while selecting ``camera" and ``girl" as critical objects to correctly infer the person's intention. However, when the decoder is removed, the prediction falls into negative answer ``E.seeing the pictures". This is because implementation without our answer decoder inevitably suffers from a spurious correlation between the negative answer ``E.seeing the pictures" and frames where the girl is actually seeing pictures, even though these frames are irrelevant to the question.
%
Lastly, we present a failure case in question 3, where TranSTR fails to capture the subtle difference between ``feed'' and ``touch'', although the critical visual elements are located, which leads to a false prediction of the girl's intention.



