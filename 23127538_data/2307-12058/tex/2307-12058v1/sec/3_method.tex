\section{Method}\label{sec:method}
% pipeline

% % Figure environment removed



% Figure environment removed

As shown in \cref{fig:2a} TranSTR consists of three main components: Spatio-Temporal Rationalization (STR), Multi-Grain Reasoning (MGR), and Answer Decoder. 
%
First, STR follows a two-step selection process, where Adaptive Temporal Rationalization (TR) is first  performed for frame selection, followed by object selection via Adaptive Spatial Rationalization (SR).
%
Next, the selected frames and objects are then combined through MGR, which generates multi-modal representations by fusing with the question.  
%
Finally, based on the multi-modal representations, the answer decoder takes the combination of answer candidates as queries and predicts an answer.
%
In this section, we provide a detailed illustration of each module. 

\subsection{Spatio-Temporal Rationalization (STR)}

STR aims to find the question-critical frames and objects in a fully adaptive and differentiable manner, which comprises two components: an Adaptive Temporal-Rationalization (TR) that identifies the critical frames and an Adaptive Spatial-Rationalization (SR) that pinpoints the critical objects on the identified frames.

\subsubsection{Adaptive Temporal Rationalization (TR)}
To identify the critical frames from the video, TR takes the encoded video frames $\Mat{F}\!\in\!\space{R}^{T\times d}$ as input and adaptively selects question-critical frames from a cross-modal view.
%
As shown in \cref{fig:2b}, a self-attention layer is first adopted to contextualize $\Mat{F}$. Then, a cross-attention is applied by taking contextualized frame feature $\Mat{F}'$ as query and question embedding $\Mat{Q}$ as key and value, which yields the frame tokens $\Mat{F}''\!\in\!\space{R}^{T\times d}$ and the cross-attention map $\Mat{Z}\!\in\!\space{R}^{T\times L}$.
\begin{gather}
    \Mat{F}'=\text{Self-Attention}(\Mat{F}) + \Mat{F}, \label{eq:1}\\
    \Mat{F}'', \Mat{Z}=\text{Cross-Attention}(\Mat{F}', \Mat{Q}) + \Mat{F}'. \label{eq:2}
\end{gather}
For brevity, we omit the superscript in $\Mat{F}''$ and use $\Mat{F}$ to denote the resulting frame tokens. 

Naturally, each value in the cross-attention map indicates the cross-modal interaction activeness between a frame and a question token.
%
To enable an adaptive frame selection that caters to different videos, we collect the $K_f$ interactions of the highest attention score from the cross-attention map $\Mat{Z}$, then gather their corresponding frame tokens $\tilde{\Mat{F}} \!\in\!\space{R}^{C\times d}$ as a subset of $\Mat{F}$, where $C$ is the number of critical frames. This process is formally given by:
\begin{gather} \label{eq:3}
    \tilde{\Mat{F}}=\text{Adaptive-Selection}_K(\Mat{F}, \Mat{Z}) \quad \text{s.t.}\,K=K_f.
\end{gather}
Notably, by gathering 1-D tokens from the 2-D interaction view, we enable an adaptive collection of $\tilde{\Mat{F}}$ with much fewer tokens being selected as critical frames (\ie $C \!\ll\! T$ and $C \!\ll\! K_f$).
%
It is worth noting that the interaction selection via vanilla hard Top-K produces a discrete selection, making it inapplicable for end-to-end training. 
%
We address this issue by adopting a differentiable Top-K using the perturbed maximum method \cite{pertub}, 
which has empirically shown advantages over other differentiable technique (\cf \cref{tab:ablation-loss})

\vspace{-0.6cm}
\subsubsection{Adaptive Spatial Rationalization (SR)}
Given the embedding of the selected frames $\tilde{\Mat{F}}$, SR aims to pinpoint the question-critical objects in each frame. 
%
To achieve that, SR enable an adaptive object selection similar to \cref{eq:1,eq:2,eq:3}.
%
Specifically, for the $t$-th critical frame $\tilde{\Mat{f}}_t$, we first feed SR with fixed $S$ object features detected on that frame, then collect top $K_o$ interactions from its cross-modal attention map with question embedding. Finally, by gathering their corresponding object tokens, we obtain the critical object feature $\tilde{\Mat{o}_t} \!\in\!\space{R}^{C_t\times d}$, where $C_t$ denotes the number of critical objects on $t$-th critical frame.
% where $C_t$ is critical object number on $t$-th critical frame.
%
It is worth noting that, SR is applied independently to each frame, thus, different frames can adapt to different numbers of the critical objects $C_t$, even if we keep $K_o$ constant for all frames.

%
\subsection{Multi-Granularity Reasoning (MGR)}
MGR aims to enhance the frame-level representation with fine-grained object embedding, while modeling the video dynamic together with question semantics.
%
% % we design a frame-wise feature aggregation module MGA. 
% MGA first refines the representation of a critical frame $\Mat{f}_t$ by gathering the information of its nested critical objects $\Mat{o}_{m,s} s\in {}$.
%
% then it models the temporal dynamic across all enhanced critical frames while taking the question semantic into account. 
%
As shown in \cref{fig:2c}, MGR first applies intra-frame aggregation via a cross-attention, which takes the frame feature of the $t$-th critical frame $\tilde{\Mat{f}}_{t} \!\ \in\! \space{R}^{1 \times d}$ as query, and all critical objects in $t$-th frame $\tilde{\Mat{o}}_{t} \! \in\! \space{R}^{{C_{t}} \times d} $ as key and value to generate an object-enhanced representation $\mathring{\Mat{f}}_{t}\! \in \! \space{R}^{1 \times d}$ for the $t$-th frame:
\begin{gather} \label{eq:mga}
    \mathring{\Mat{f}}_{t}=\text{Cross-Attention}(\tilde{\Mat{f}}_{t}, \tilde{\Mat{o}}_{t}) + \tilde{\Mat{f}}_{t}.
\end{gather}
By doing so to all $C$ critical frames, we acquire $\mathring{\Mat{F}} \! \in \space{R}^{C \times d}$ as the object-enhanced frame representation for all critical frames.
% Naturally, we acquire $\mathring{\Mat{F}} \! \in \space{R}^{C \times d}$ as the object-enhanced frame representation for all $C$ critical frames. 
%
Next, a transformer layer is adopted to establish cross-frame dynamics, which takes in the concatenation of $\mathring{\Mat{F}}$ and question tokens $\Mat{Q}$, and yields multi-modal representations $\Mat{M} \!\in\!\space{R}^{(C+L) \times d}$ for answer decoding:
\begin{gather}
    \Mat{M}=\text{Transformer-Layer}([\mathring{\Mat{F}};\Mat{Q}]),
\end{gather}
where $[;]$ denotes concatenation operation.

\subsection{Answer Decoding}
Existing methods \cite{VGT,IGV,EIGV} concatenate a question with answer candidates. As analyzed in Sec.~\ref{sec:intro}, these methods suffer from a spurious correlation between negative candidates and question-irrelevant video scenes. To circumvent this issue \lyc{in spatio-temporal rationalization}, we employ a transformer-style decoder that takes as input the question-critical multi-modal representations $\Mat{M}$ and the representations of the candidate answers to determine the correct answer. We detail our implementations for multi-choice QA and open-ended QA in the next sub-sections. 
% \jbnote{be more specific..basically, you need to clearly describe all the learnable modules.}

\vspace{-7pt}
\subsubsection{Multi-Choice QA} 
In Multi-Choice QA, answer candidates are given as $\left| A_{mc} \right|$ sentences or short phrases that are tailored for each video-question pair. Therefore, reasoning on Multi-Choice QA typically requires fine-grain inspection of the video content as well as the interaction between the video and the candidate answers.
% Thus, MCQA has become an ideal setting for complex VideoQA. 
To this end, we first prepend a $\left[ \text{CLS} \right]$ token to each answer candidate and feed the sequences to the same language model used for question encoding. Then, we gather the output of the `[CLS]' tokens for all encoded answer candidates, and form the answer query $\Mat{U}_{mc} \in \Space{R}^{\left| A_{mc} \right| \times d}$.
%
During decoding, we feed a transformer decoder with $\Mat{U}_{mc}$ as query to interact with the multi-modal representation $\Mat{M}$, which yields the decoded representation $\Mat{H}_{mc} \in \Space{R}^{\left| A_{mc} \right| \times d}$ as:
\begin{gather} \label{eq:decoder-mc}
    \Mat{H}_{mc} = \text{Transformer-Decoder}(\Mat{U}_{mc}, \Mat{M}).
\end{gather}
Notably, since the correctness of a answer candidate is invariant to its position, answer query is free of position encoding. 
%
Finally, we apply a linear projection on $\Mat{H}_{mc}$ to get the answer prediction $\hat{A}_{mc} \in \Space{R}^{\left| A_{mc} \right|}$,
\begin{gather}
    \hat{A}_{mc}=\text{Linear}(\Mat{H}_{mc}).
\end{gather}

\vspace{-15pt}
\subsubsection{Open-Ended QA}
Open-Ended setting provides $\left| A_{oe} \right|$ simple-form answer candidates (typically a single word) that are shared among all question instances, which makes the whole candidates set it too large to be processed as Multi-Choice setting. (\ie $\left| A_{oe} \right| \gg  \left| A_{mc} \right|$).
% Thus, we follow the convention \cite{IGV, EIGV} and treated each answer candidate as a one-hot category without semantic meaning.
Instead, we take inspiration from DETR \cite{detr}, and initialize a single learnable embedding $\Mat{U}_{oe} \in \Space{R}^{d}$ as answer query.
Analogous to Multi-Choice setting, we feed $\Mat{U}_{oe}$ to the transformer decoder together with $\Mat{M}$ and acquire the decoded representation $\Mat{H}_{oe} \in \Space{R}^{d}$ similar to \cref{eq:decoder-mc}.
As a result, we obtain the prediction $\hat{A}_{oe} \in \Space{R}^{\left|A_{oe}\right|}$ by projecting $\Mat{H}_{oe}$ to the answer space $\Space{R}^{ \left|A_{oe}\right|}$ via a linear layer:
\begin{gather}
    \vspace{-2pt}
    \hat{A}_{oe}=\text{Linear}(\Mat{H}_{oe}).
        \vspace{-2pt}
\end{gather}

During training, we establish our objective on a cross-entropy loss.  For inference, the differentiable Top-K is replaced with vanilla hard Top-K for better efficiency. 
% better efficiency and determinacy.