\section{Introduction} \label{sec:intro}


%1 任务意义， 最近popular的原因 1 pertain 2, transformer-based Artech design
% Aiming to reveal how the machine reflects the physical world, Video Question Answering (VideoQA) has been formed as answering natural language questions in the context of videos, which has gained drastic progress over the last few years. This advancement stems, in part from the recent surge of video-language pre-training models, and in part from the continuing enthusiasm for task-specific architecture design. 
% chatpgt
Video Question Answering (VideoQA), which aims to explore how machines understand and interpret the physical world, has made significant progress in recent years. This advancement stems, in part, from the recent surge of video-language pre-training models, and partly from the enthusiasm for designing task-specific architectures.

%2 % 从数据的角度 simple-complex question 有神么区别
% Recent progress in answering simple questions with short videos has encouraged the community to explore complex VideoQA, which arise several unique challenges: 
% chatpgt
Recent great success in answering simple questions with short videos has prompted the community to explore complex video question answering, which poses several unique challenges:
% 
% Compared to short videos that tend to exhibit ``static frame bias'' (\ie only one frame is informative enough to describe the whole video \cite{atp}), long videos cover composite visual dynamics, which require comprehensive causal reasoning to handle the long-term temporal dependencies.
%
1) \textit{Longer videos} often present complex visual dynamics that require comprehensive temporal reasoning to handle the intricate causal relationships between objects and events. In contrast, short videos typically exhibit ``static frame bias'', in which only a single frame provides meaningful information about the entire video, making it easier to overlook the nuances of the temporal dynamics.
% 
% 2) Fine-grain inspection. In simple cases, questions typically focus on the attribute of the objects that appear simultaneously, which is easy to capture even from a frame-level inspection.
% However, complex questions involve cross-frame relations or events among multiple objects, which requires a fine-grained inspection of the visual entities and their transition. 
%
2) \textit{Fine-grain inspection} is crucial in complex VideoQA scenarios, which involve cross-frame relations or events among multiple objects. While simple cases may focus on individual object attributes, which are easy to capture even at the frame level, answering more complex questions requires a fine-grained inspection of the visual entities and their transitions over time.

%3.1 从模型的角度， 为什么目前的方法解决不好complex question
% Here we elaborate on why pre-trained models and task-specific designs, hardly perform well on complex questions. 
% \begin{itemize}[leftmargin=*]
%     \item 
% \end{itemize}

Although pre-trained models demonstrate remarkable performance on simple VideoQA tasks, their transferability on complex ones suffers a suboptimal performance. 
%
This is because the pre-training data are comprised of simple questions and short videos with a highly sparse sampling rate, where ``static frame bias'' makes the answering pattern of simple questions easily captured by fitting dataset correlation.  
%
However, such correlation does not hold in complex scenarios, as long videos hardly exhibit ``static frame bias'', and the complex questions tend to inquire about the temporal relations that require a detailed inspection of the video dynamics.   

%3.2 task specific de deisgn 也不行。。。
% Recently, some task-specific architectures are proposed to address the complex VideoQA. 
%
% Specifically, they incorporate the object region feature for fine-grain inspection and process the long video as a multi-level hierarchy, 
% % where each level is constructed by fix number of frames or objects.
% where each level is constructed by a group of adjacent frames or objects within the same frame.
% %
% However, the number of frames or objects in each level of the hierarchy is fixed with no adaptation ability to different videos, which seriously undermines their generalization ability.
% % 太多， 太少。。。折线图train VS val

Recently, some task-specific architectures are proposed to address the complex VideoQA.
Specifically, they leverage object region features for fine-grained inspection and handle long videos as a multi-level hierarchy, with each level comprising a group of adjacent frames or objects in the same frame. 
However, these architectures suffer from limited generalization ability since the number of frames or objects in each level of the hierarchy is fixed and cannot adapt to different videos, which rigidity undermines their performance on diverse VideoQA tasks.


%我们提出一种
To resolve these limitations, we propose a novel spatio-temporal rationalizer, a differentiable rationalization module that adaptively collects question-critical frames and objects from a cross-modal interaction view. 
% as rationales that exclusively support reasoning. 
% It adaptively selects question-critical frames and objects as rationales that exclusively support reasoning. 
% 
Concretely, it collects the most critical frames from a long video through temporal rationalization, followed by spatial selection of objects on each frame.
%
By aggregating the selected visual elements, we derive rationales that exclusively support reasoning.
%
To make rationalization differentiable and adaptive to different video content, we adopt differentiable Top-K algorithm \cite{pertub} to select the visual elements based on their cross-modal interaction activeness with the question. 
%
%%%%%%% decoder
% In addition to the rationalizer, we find the prevailing designs unanimously concatenate the answer candidates and question as a whole before fusing with the video feature. 
% However, this can cause a negative answer easily prevails, 
% as a negative answer can hold a strong spurious correlation with the question-irrelevant scenes. 
% That is, there are more instances of women``riding bike" than ``pushing bike" in the training set. Thus, once fusing video with answer candidates, the first two ``riding bike" clips, although question irelevent will induce the model to predict ``A.riding bike" without inspecting the question-critical scene.
% We circumvent this issue by separating question and answer candidates.
% Concretely, only the question is fused with video in multi-modal encoding, while answer candidates are introduced afterward via a transformer decoder with a set of learnable answer queries.
%
% chatgpt
In addition to the rationalizer, we have identified a potential issue with the prevailing designs that concatenate the answer candidates and question as a whole before fusing with the video feature. 
This approach can lead to a negative answer being incorrectly predicted when there is a strong spurious correlation with question-irrelevant scenes. 
For example, there may be more instances of women "riding bike" than "pushing bike" in the training set, which can cause the model to predict "A.riding bike" based on irrelevant scenes.
%
We circumvent this issue by separating the question and answer candidates. Specifically, only the question is fused with the video in a multi-modal encoding, and the answer candidates are introduced later through a transformer decoder with a set of learnable answer queries. This approach ensures that the model focuses only on the question-critical scenes, leading to a more accurate prediction.
%%%%%%%%%%%

By combining the rationalizer and answer decoder, we show our framework, \underline{S}patio-\underline{t}emporal R\underline{a}tionalized T\underline{r}ansformer (STAR), out-performs the current state-of-the-art (SoTA) models across several popular benchmarks.
%
Our contributions are summarized as follows:

\begin{itemize}[leftmargin=*]
    % \item For the first time, we analyze the necessity and challenge of complex VideoQA task, and highlight spatio-temporal rationalization is the key to address the complex VideoQA.
    % \item We highlight rationalization as the key to mitigating the current inability toward the complex VideoQA, and propose a differentiable selection module that adaptively collects critical visual elements in different granularity.
    \item We analyze the necessity and challenge of complex VideoQA tasks and identify spatio-temporal rationalization as a key solution, then propose a differentiable selection module that adaptively collects critical visual elements in different granularity.
    \item We identify the spurious correlation in current use of answer candidates, and propose a query-based answer decoder that can boost any off-the-shelf Video QA model.
    \item We perform extensive experiments on four benchmark datasets (NExT-QA \cite{next-qa} +5.5\%, CausalVid-QA\cite{causalvid} +10.4\%, MSVD-QA \cite{DBLP:conf/mm/XuZX0Z0Z17} +1.1\%, MSRVTT\cite{DBLP:conf/mm/XuZX0Z0Z17}) to demonstrate the effectiveness of STAR.
\end{itemize}

% a transformer-style answer decoder that fully exploits the rationales to approach the gold answer. 

% Moreover, to achieve the strong rationale exploitation while maintaining the interaction-aware decoding, we resort to a transformer-style decoder by employing a learnable answer query that conditions on answer candidates. 
 
% Distinct from existing works, we circumvents the \lyc{aforementioned} early exposure issue of answer candidates by isolating them from the encoding. Since only rationales are fed to the decoder, the redundant interaction between the negative answer and the environmental scene is naturally ruled out from reasoning. 

%%%%%%%%%%%%%%%%%%%%%%
% 任务是啥，现有的pretrain 已经把simple question做的很好了
% 但是面对复杂问题，pertain 有gap
% 另一方面， ask specific videoQA deisgn, fix obj, 也不能generalize
% 我们提出一种 。。

% % Figure environment removed


% Figure environment removed%

