\section{Introduction} \label{sec:intro}

% Figure environment removed


The great success of self-supervised pretraining with powerful transformer-style architectures \cite{devlin2018bert,deberta,DBLP:conf/icml/KumarIOIBGZPS16,fu2021violet,yang2021just,zellers2021merlot} have significantly boosted the performance of answering simple questions (\eg, ``what is the man doing'') about short videos (\eg, 3$\sim$15s) \cite{jang2017tgif,DBLP:conf/mm/XuZX0Z0Z17}. The advances thus point a research trend towards complex video question answering (VideoQA) \cite{next-qa,causalvid,zhong2022video}. Compared with simple VideoQA, complex VideoQA poses several unique challenges:

1) \textbf{The videos are relatively longer and often carry multiple objects and activities or events.} The long video and rich visual content 
indispensably bring more background scenes that include question-irrelevant video moments and visual objects. The background scenes, if not filtered properly, will derail answer prediction, especially when there are hard negative answers presented in the candidate answer list, \eg, ``ride the bike'' \vs ``push the bike'' in the example shown in~\cref{fig:1a}. The distractor ``ride the bike'', though irrelevant to the question,  corresponds to a large part of the video.

2) \textbf{The questions demand the understanding of rich video dynamics that emphasize causal and temporal relations for answer.} While simple VideoQA may focus on individual object attributes or group activities which are easy to capture at the static frame-level \cite{lei2021less,atp}, answering more complex questions requires a fine-grained inspection of the visual entities and their space-time interactions.

In light of the challenges, we propose SoTRFormer, a transformer-style architecture that highlights a spatio-temporal rationalizer (SoTR) along with a coordinated answer decoder for complex question answering. Concretely, SoTR collects the question-critical moments (instantiated by key frames in this work) from a long video via a temporal rationalization module, followed by spatial selection of objects on the identified key frames. The select key frames and objects are then served as rationales to support answer reasoning. For actual reasoning, we employ light-weight spatio-temporal transformer layers. Then to decode the answer, we 

% \jbnote{clarify how your method capture the temporal dynamics since you highlight this challenge.}


%
% By aggregating the selected visual elements, we derive rationales that exclusively support reasoning.
%
% To make rationalization differentiable and adaptive to different video content, we adopt differentiable Top-K algorithm \cite{pertub} to select the visual elements based on their cross-modal interaction activeness with the question. 
%
%%%%%%% decoder
% In addition to the rationalizer, we find the prevailing designs unanimously concatenate the answer candidates and question as a whole before fusing with the video feature. 
% However, this can cause a negative answer easily prevails, 
% as a negative answer can hold a strong spurious correlation with the question-irrelevant scenes. 
% That is, there are more instances of women``riding bike" than ``pushing bike" in the training set. Thus, once fusing video with answer candidates, the first two ``riding bike" clips, although question irelevent will induce the model to predict ``A.riding bike" without inspecting the question-critical scene.
% We circumvent this issue by separating question and answer candidates.
% Concretely, only the question is fused with video in multi-modal encoding, while answer candidates are introduced afterward via a transformer decoder with a set of learnable answer queries.
%
% chatgpt
% In addition to the rationalizer, we have identified a potential issue with the prevailing designs that concatenate the answer candidates and question as a whole before fusing with the video feature. 
% This approach can lead to a negative answer being incorrectly predicted when there is a strong spurious correlation with question-irrelevant scenes. 
% For example, there may be more instances of women "riding bike" than "pushing bike" in the training set, which can cause the model to predict "A.riding bike" based on irrelevant scenes.
% %
% We circumvent this issue by separating the question and answer candidates. Specifically, only the question is fused with the video in a multi-modal encoding, and the answer candidates are introduced later through a transformer decoder with a set of learnable answer queries. This approach ensures that the model focuses only on the question-critical scenes, leading to a more accurate prediction.
% %%%%%%%%%%%

% By combining the rationalizer and answer decoder, we show our framework, \underline{S}patio-\underline{t}emporal R\underline{a}tionalized T\underline{r}ansformer (STAR), out-performs the current state-of-the-art (SoTA) models across several popular benchmarks.
%
Our contributions are summarized as follows:

\begin{itemize}[leftmargin=*]
    % \item For the first time, we analyze the necessity and challenge of complex VideoQA task, and highlight spatio-temporal rationalization is the key to address the complex VideoQA.
    % \item We highlight rationalization as the key to mitigating the current inability toward the complex VideoQA, and propose a differentiable selection module that adaptively collects critical visual elements in different granularity.
    \item We analyze the necessity and challenge of complex VideoQA tasks and identify spatio-temporal rationalization as a key solution, then propose TranSTR.
    \item We propose 
    identify the spurious correlation in current use of answer candidates, and propose a query-based answer decoder that can boost any off-the-shelf Video QA model.
    \item We perform extensive experiments on four benchmark datasets (NExT-QA \cite{next-qa} +5.8\%, CausalVid-QA\cite{causalvid} +6.8\%, MSVD-QA \cite{DBLP:conf/mm/XuZX0Z0Z17} +3.5\%, MSRVTT\cite{DBLP:conf/mm/XuZX0Z0Z17}  +3.4\%) to demonstrate the effectiveness of STAR.
\end{itemize}

% a transformer-style answer decoder that fully exploits the rationales to approach the gold answer. 

% Moreover, to achieve the strong rationale exploitation while maintaining the interaction-aware decoding, we resort to a transformer-style decoder by employing a learnable answer query that conditions on answer candidates. 
 
% Distinct from existing works, we circumvents the \lyc{aforementioned} early exposure issue of answer candidates by isolating them from the encoding. Since only rationales are fed to the decoder, the redundant interaction between the negative answer and the environmental scene is naturally ruled out from reasoning. 

%%%%%%%%%%%%%%%%%%%%%%
% 任务是啥，现有的pretrain 已经把simple question做的很好了
% 但是面对复杂问题，pertain 有gap
% 另一方面， ask specific videoQA deisgn, fix obj, 也不能generalize
% 我们提出一种 。。

% % Figure environment removed



