\begin{table*}[ht!]
\small
\centering
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cc|cccc|cc|cc}
\multicolumn{2}{c|}{} & \multicolumn{4}{c|}{\textbf{Models}} & \multicolumn{2}{c|}{\textbf{Baselines}} & \multicolumn{2}{c}{\textbf{Aggregate}}\\
\toprule
\multicolumn{2}{c|}{\textbf{Task/Lang.}} & \textbf{T0} & \textbf{Flan-T5} & \textbf{Flan-UL2} & \textbf{Tk-Instruct} %\textbf{Alpaca} 
& \textbf{Most Freq} &  \textbf{Random} &  \textbf{Majority} & \textbf{MACE} \\
\midrule
\multirow{3}{*}{SA} 
& EN & 0.453$^{\star}$ & 0.532$^{\star}$ & 0.482$^{\star}$ & \textbf{0.553}$^{\star}$ & 0.167 & 0.334 & 0.503$^{\star}$ & 0.514$^{\star}$  
\\
& DE & 0.469$^{\star}$ & 0.495$^{\star}$ & 0.433$^{\star}$ & \textbf{0.517}$^{\star}$ &  0.167 & 0.331 & 0.480$^{\star}$ & 0.484$^{\star}$ 
\\
& FR & 0.460$^{\star}$ & 0.518$^{\star}$ & 0.445$^{\star}$ & \textbf{0.528}$^{\star}$ & 0.167 & 0.337 & 0.486$^{\star}$ & 0.490$^{\star}$ 
\\
\midrule
\multirow{3}{*}{AC-Gender} 
& EN & 0.516 & 0.594$^{\star}$ & \textbf{0.624}$^{\star}$ & 0.541$^{\star}$ & %0.384 &
0.337 & 0.501 & 0.617$^{\star}$ & 0.623$^{\star}$ 
\\
& DE & 0.456 & 0.437 & 0.447 & 0.431 & 0.334 & \textbf{0.497} & 0.458 & 0.485 
\\
& FR & 0.428 & 0.573$^{\star}$ & 0.566$^{\star}$ & 0.563$^{\star}$ & 0.335 & 0.503 & 0.577$^{\star}$ & \textbf{0.579}$^{\star}$ 
\\
\midrule
\multirow{3}{*}{AC-Age} 
& EN & 0.495 & 0.442 & 0.516$^{\star}$ & 0.397 & %0.478 &
0.336 & 0.497 & 0.569$^{\star}$ & \textbf{0.572}$^{\star}$ 
\\
& DE & 0.458 & 0.366 & \textbf{0.503} & 0.344 & 0.334 & 0.500 & 0.422 & 0.499
\\
& FR & 0.497 & 0.375 & \textbf{0.550}$^{\star}$ & 0.343 & %0.505 &
0.335 & 0.500 &  0.443 & 0.542$^{\star}$
\\
\midrule
\multirow{3}{*}{TD} 
& EN & 0.558$^{\star}$ & 0.579$^{\star}$ & 0.588$^{\star}$ & 0.567$^{\star}$ & 0.085 & 0.195 & 0.588$^{\star}$ & \textbf{0.596}$^{\star}$ 
\\
& DE & 0.506$^{\star}$ & 0.514$^{\star}$ & 0.513$^{\star}$ & 0.493$^{\star}$ & 0.105 & 0.193 & 0.516$^{\star}$ & \textbf{0.520}$^{\star}$ \\
& FR & \textbf{0.314}$^{\star}$ & 0.271$^{\star}$ & 0.264$^{\star}$ & 0.257$^{\star}$ & 0.096 & 0.193 & 0.281$^{\star}$ & 0.293$^{\star}$  \\
\midrule
\midrule
\multirow{3}{*}{Mean} 
& EN & 0.506 & 0.537 & 0.553 & 0.515 & 0.231 & 0.382 & 0.569 & \textbf{0.576} \\
& DE & 0.472 & 0.453 & 0.474 & 0.446 & 0.235 & 0.380 & 0.469 & \textbf{0.497} \\
& FR & 0.425 & 0.434 & 0.456 & 0.423 & 0.233 & 0.383 & 0.447 & \textbf{0.476} \\
\bottomrule
\midrule
\multirow{2}{*}{HS} 
& EN & 0.621$^{\star}$ & 0.726$^{\star}$ & 0.670$^{\star}$ & 0.579$^{\star}$ & %0.428 &
0.367 & 0.490 & 0.717$^{\star}$  & \textbf{0.726}$^{\star}$  
\\
& ES & 0.601$^{\star}$ & 0.532$^{\star}$ & 0.519 & 0.449 & %0.394 &
0.370 & 0.492 & 0.533$^{\star}$ & \textbf{0.603}$^{\star}$
\\
\bottomrule

\end{tabular}
}
\caption{Zero-shot Macro-F1 results obtained by the LLMs on the Trustpilot and HatEval tasks, the baselines and the aggregation methods. Best result per language and task is shown in bold. Significant improvement over Random baseline ($^{\star}: p \leq 0.01$) with bootstrap sampling. For non-English languages, we use the multilingual mT0 model.} \label{tab:zs_results}
\end{table*}