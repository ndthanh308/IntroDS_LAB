\pdfoutput=1

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{amsmath}

\usepackage{tacl2021v1}
% \setlength\titlebox{10cm} % <- for Option 2 below

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{multicol}
\usepackage[compact]{titlesec}

%\usepackage{draftwatermark}
%\SetWatermarkText{xConfidential}
%\SetWatermarkScale{1}

% Display authors
\taclpubformattrue

\newcommand{\final}[1]{\textcolor{magenta}{#1}}

\newcommand{\promptbox}[3]{%
  \begin{tcolorbox}[colback=#3!10,colframe=black!50,title=#1] #2
  \end{tcolorbox}%
}

\title{Leveraging Label Variation of Large Language Models\\Improves Zero-Shot Performance}

\title{Leveraging Label Variation in Large Language Models for Zero-Shot Text Classification}

\author{\bf {Flor Miriam} {Plaza-del-Arco}, \bf {Debora Nozza}, \bf Dirk Hovy\\
  Bocconi University \\
  Via Sarfatti 25 \\
  Milan, Italy \\
  \texttt{\{flor.plaza, debora.nozza, dirk.hovy\}@unibocconi.it}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

The zero-shot learning capabilities of large language models (LLMs) make them ideal for text classification without annotation or supervised training.  Many studies have shown impressive results across multiple tasks. While tasks, data, and results differ widely, their similarities to human annotation can aid us in tackling new tasks with minimal expenses.
We evaluate using 5 state-of-the-art LLMs as ``annotators'' on 5 different tasks (age, gender, topic, sentiment prediction, and hate speech detection), across 4 languages: English, French, German, and Spanish. 
No single model excels at all tasks, across languages, or across all labels within a task. However, aggregation techniques designed for human annotators perform substantially better than any one individual model.
Overall, though, LLMs do not rival even simple supervised models, so they do not (yet) replace the need for human annotation.
We also discuss the tradeoffs between speed, accuracy, cost, and bias when it comes to aggregated model labeling versus human annotation.


%Large language models (LLMs) have generated a lot of excitement due to their remarkable zero- and few-shot capabilities. This has made them a good choice for text classification without the need for annotation or supervised training. Many studies have shown impressive results across multiple tasks. However, it is worth mentioning that the training data for these tasks were often included in the model's training data. We evaluate 5 state-of-the-art LLMs with diverse architectures on 5 challenging ``unseen''  datasets for different tasks (age, gender, topic, sentiment prediction, and hate speech detection), across 4 languages: English, French, German, and Spanish. Results are varied. Although some models perform well on certain tasks, no model can excel at all tasks or across all labels of a given task. We also find that using aggregation techniques designed for human annotators is more effective than relying on individual models. Leveraging their diversity, we derive aggregate labels that are more accurate than any individual model's predictions, as well as the majority label.Our discussion includes a thorough analysis of the tradeoffs between speed, accuracy, cost, and bias when it comes to aggregated model labeling versus human annotation.LLMs may not fully replace the need for human annotation, but they do possess enough similarities to human annotation to aid us in tackling new tasks with minimal expenses.

% The zero- and few-shot capabilities of large language models (LLMs) have led to excitement about using them to classify text directly, without the need for annotation or supervised training. But while various papers have found good performance on several tasks, it turns out that those tasks' training data was typically included in the model's training data. 
% In this paper, we test 5 LLMs (with various architectures) on 5 ``unseen'' tasks (age, gender, topic, and sentiment prediction, and hate speech detection) on 4 languages (English, French, German, and Spanish). We find that results are rather mixed. While some models generalize well, none excel at all tasks or across all labels of a given task. 
% However, we find that instead of using individual models, we can apply methods for human annotators. Leveraging their variation, we find aggregate labels that are better than any of the individual models' predictions, or the majority label.
% We also discuss the tradeoffs in speed, accuracy, cost, and bias for aggregated model labeling vs.\ human annotation.
% So while LLMs do not (yet) replace the need for human annotation, they do share many aspects of human annotation that can help us tackle new tasks with minimal costs.
\end{abstract}


\section{Introduction}

Large Language Models (LLMs) have revolutionized many aspects of Natural Language Processing (NLP). \newcite{brown2020language} showed early on that LLMs have few-shot and even zero-shot learning (ZSL) capabilities due to their inherent ability to ``understand'' language.\footnote{We are using this anthropomorphism as a verbal shortcut. The models do not actually understand anything \cite{bender-koller-2020-climbing}.}
Subsequent iterations have improved these capabilities further. Those improvements have seemingly obviated one of the most time and labor-intensive aspects of NLP: annotating enough data to train a supervised language model.
Instead, we can use LLMs to directly predict the labels via prompting. Indeed, various papers have tested this hypothesis and found good performance on a wide range of NLP tasks \cite{zhao2023survey,su2022selective,wei2022emergent,brown2020language}.

However, upon closer inspection, these claims require some caveats: different models excel at different tasks, data sets, and formulations \cite{gilardi2023chatgpt,tornberg2023chatgpt}. 
% most of the tasks on which LLMs perform well are so common that their training data was included in the LLMs' training as well. That explains their performance, but requires a revision of what ``zero-shot'' really means here.
The ultimate promise of LLMs, though, is that their language capability lets them generalize to \textit{any} text classification task. 
% \footnote{It is also unclear how interesting that set of tasks is to non-NLP users of LLMs, like industry stakeholders and social scientists.}
What if the answer is not to wait for one model to rule them all, but to treat their variation similar to the disagreement among human annotators? Not as individual flaws, but as specializations we can exploit.

To test this, we use 5 state-of-the-art LLMs as ``annotators'' and evaluate them on 5 prediction datasets in 4 languages. We prompt the models in a ZSL setting with instructions similar to the ones we would give to a human annotator. 

The results suggest we still need human annotation. The LLMs perform much less well on those tasks than on the ones previously reported in the literature. In some cases, they do worse than a dummy classifier using the most frequent label, or random assignment.
We also find that there are no clear winners and losers: most models excel on some tasks, but not on others. Some models do very well predicting certain labels in a given task, but perform poorly on other labels.

This kind of variation is also common to human annotation, where different annotators have different strengths (or levels of reliability).
Recent work in annotation \cite{basile-etal-2021-need,plank-2022-problem} has suggested leveraging this human label variation to our advantage. We argue that the same applies to LLMs if we treat them as ``annotators''.

In the simplest case, this could be achieved by majority voting: for each instance, use the label that most LLMs suggested. However, the majority can still be wrong.
Instead, we can use Bayesian models of annotation \cite{passonneau-carpenter-2014-benefits, paun-etal-2018-comparing} to weigh the answers based on the inferred reliability of the annotators. This approach is similar to Bayesian classifier combination, but does not require gold labels to assign the scores. That distinction is crucial, as we want to work with un-annotated data.

In the vast majority of cases, aggregated labels outperform even the best individual LLM. On average across all tasks, aggregated labels perform substantially better than the average LLM (by 8.3 points F1) and better than majority voting.

However, even the best ZSL performance is usually still well below that of even simple supervised models trained on the same data, and substantially lower than Transformer-based supervised models (by over 10 F1 points on average).

We discuss what these results mean for the role of human annotation and supervised learning in NLP, with respect to performance, but also time, cost, and bias, and ethics.

\paragraph{Contributions} 1) We evaluate the effectiveness of using 5 state-of-the-art LLMs as annotators via ZSL with prompting on 5 different tasks 2) We conduct a comparison across 4 languages 3) We analyze the robustness of label aggregation. 4) We discuss the technical, moral, and ethical ramifications of this development for NLP and annotation.


\section{Data}

For our experiments, we use two datasets: Trustpilot \cite{hovy2015user} and HatEval \cite{basile-etal-2019-semeval}. Note that for most models, these datasets are ``unseen'', i.e., the data that has not been part of the LLMs' training. The one exception is HatEval in EN, which is included in Flan-T5 and Flan-UL2 models.

\textbf{Trustpilot} \cite{hovy2015user} is a multilingual dataset with demographic user information. It uses reviews from the user review website Trustpilot. The dataset includes reviews from various countries with different languages. 
To test a variety of languages commonly found in LLMs, we select the subsets with English from the United States, German from Germany, and French from France for our experiments.
The data includes labels for sentiment, the topic of the review, and two demographic dimensions of the review authors: self-declared gender and age (these two are not available for all data points). We use the same data splits as \newcite{hovy-2015-demographic} to ensure comparability and consistency. Given our ZSL setup, we use only their evaluation sets for each language, which consists of the joint development and test sets. 

\textbf{HatEval} \cite{basile-etal-2019-semeval} is a multilingual dataset for hate speech detection against immigrants and women on Twitter, part of a SemEval 2019 shared task. The dataset contains tweets manually annotated for hate speech via crowdsourcing. %To collect the tweets, they follow three different strategies: (1) monitoring potential victims of hate accounts, (2) downloading the history of identified haters, and (3) filtering Twitter streams with keywords, i.e., words, hashtags, and stems. 
The corpus has data in English and Spanish. We use the benchmark test set provided by the HatEval competition for both languages.


\subsection{Tasks}
We evaluate the performance of LLMs as annotators on five text classification tasks: four from the Trustpilot dataset and one from the HatEval corpus. These tasks (see Table \ref{table:tasks}) involve sentiment analysis, topic detection, and predicting demographic attributes (gender and age), performing. These two \textbf{attribute classification (AC)} task are binary sub: the \textbf{gender} of the text author (\textit{male} or \textit{female})\footnote{The data does not allow a more fine-grained classification of gender identities, as the original website only provided users with those two options. See Ethical Implications (Section \ref{sec:ethics}) for more discussion.} and the \textbf{age} of the text author (\textit{under 35} or \textit{above 45} years old). In the \textbf{sentiment analysis (SA)} task, reviews are classified into \textit{negative}, \textit{neutral}, and \textit{positive} sentiments based on 1, 3, and 5-star ratings, respectively. The \textbf{topic detection (TD)} task uses the review categories of the texts to classify them into one of five topics. For this task, the exact topics vary across languages.
% \begin{itemize}
%     \item 
EN: \textit{Car lights}, \textit{fashion accessories}, \textit{pets}, \textit{domestic appliances}, and \textit{hotels}.
    % \item 
    DE: \textit{Wine}, \textit{car rental}, \textit{drugs} and \textit{pharmacy}, \textit{flowers}, and \textit{hotels}. 
    % \item 
    FR: \textit{Clothes and fashion}, \textit{fashion accessories}, \textit{pets}, \textit{computer and accessories}, and \textit{food and beverage}.
% \end{itemize}

Finally, for the \textbf{hate speech detection (HS)}, the task is to classify a tweet as either hate speech or non-hate speech.

\input{dataset}


\section{Models}

We experiment with five state-of-the-art instruction fine-tuned LLMs from two different model families: \textit{i)} T5 with encoder-decoder \cite{raffel2020exploring} and \textit{ii)} LLaMA with single decoder architecture \cite{touvron2023llama}. We specifically select these five models due to their fine-tuning on a diverse range of instructions. They use intuitive explanations of the downstream task to respond to natural language prompts. This aspect is vital when using prompt-based methods. Moreover, all of these models are accessible as open-source downloads.
Our selection represents a realistic pool of LLMs as annotators for a current NLP practitioner. Note that as models evolve rapidly, this selection is likely to change, but the results of a diverse pool of LLM annotators should hold regardless.

In particular, we use the following models:
\begin{enumerate}
    \item \textbf{FLAN-T5}\footnote{\url{https://huggingface.co/google/flan-t5-xl}} \cite{chung2022scaling} 
    
    \item \textbf{Flan-UL2}\footnote{\url{https://huggingface.co/google/flan-ul2}} \cite{tay2023ul2} (these are the Flan versions of the T5 \cite{raffel2020exploring} and UL2 models).
    
    \item \textbf{T0} \footnote{\url{https://huggingface.co/bigscience/T0}}  \cite{sanh2022multitask} and the multilingual \textbf{mT0}\footnote{\url{https://huggingface.co/bigscience/mT0-xxl}} \cite{muennighoff-etal-2023-mteb}. For the non-English languages, we use mT0 since T0 has been trained on English texts.
    
    \item \textbf{Tk-Instruct}\footnote{\url{https://huggingface.co/allenai/tk-instruct-3b-def}} \cite{wang-etal-2022-super} 
    \item \textbf{Alpaca}\footnote{\url{https://github.com/tatsu-lab/stanford_alpaca}} \cite{alpaca}.
\end{enumerate}

FLAN-T5 \cite{chung2022scaling} has been pre-trained on a mixture of unsupervised and supervised tasks, where each task is converted into a text-to-text format. It has been fine-tuned on over 1000 tasks and covers 60 languages.

Similar to FLAN-T5, FLAN-UL2 \cite{tay2023ul2} has a similar architecture to T5 \cite{raffel2020exploring} with an upgraded pre-training procedure known as UL2 \cite{tay2023ul2}. T0 \cite{sanh2022multitask} is an encoder-decoder model based on T5 that is trained on a multi-task mixture of NLP datasets partitioned into different tasks.

mT0 is based on Google's mT5 \cite{xue-etal-2021-mt5} and has been fine-tuned on xP3\footnote{\url{https://huggingface.co/datasets/bigscience/xP3}}, which covers 13 training tasks across 46 languages with English prompts.
 
Tk-Instruct \cite{wang-etal-2022-super} is a generative model for transforming task inputs given declarative in-context instructions, like ``\textit{Given an utterance and the past 3 utterances, output ‘Yes’ if the utterance contains the small-talk strategy, otherwise output ‘No’. Small-talk is
a cooperative negotiation strategy...}'' \citep[adapted from][]{wang-etal-2022-super}. 
It also uses the T5 model as a basis but trains it on all task instructions in a multi-task setup. It is fine-tuned on the \texttt{SUPER-NATURALINSTRUCTIONS} dataset \cite{triantafillou2020metadataset}, a large benchmark of 1,616 NLP tasks and their natural language instructions. It covers 76 broad task types spanning 55 different languages. 

Finally, Alpaca \cite{alpaca} is an instruction-following language model released by Stanford, fine-tuned from Meta's LLaMA model \cite{touvron2023llama}. Alpaca\footnote{The authors note that this model is still under development and is not yet fine-tuned to be safe and harmless.} is trained on 52K instruction-following demonstrations \cite{wang2023selfinstruct} generated in the style of self-instruct using \textit{text-davinci-003}. 
Most of the models are sourced from the Hugging Face repository \cite{wolf2020huggingfaces}. For Alpaca, we rely on the model weights provided directly by the official repository.


\subsection{ZSL Prompting}
A prompt is a special input or instruction provided to an LLM to generate a desired response. It can be a sentence, phrase, or even an entire paragraph, serving as the starting point for the model to generate text. The prompt guides the language model's comprehension and influences its output. 

Figure \ref{fig:prompts} depicts the task formulations (prompt instructions) we give to the LLMs, who act as our annotators, for every considered text classification task. These prompt instructions are designed to resemble the instructions we would give a human annotator for a particular task. We add ``Answer'' to mark the output field after the instruction to improve the LLMs' understanding and output format. 
For the TD tasks, the list of five topics varies by language. For instance, the prompt for the English TD task is: ``I love the earrings I bought,'' ``Is this review about ``car lights,'' ``fashion accessories,'' ``pets,'' ``domestic appliances,'' or ``hotels''~?'' <\texttt{Answer}>: \{LM answer\}.

Tk-Instruct and Alpaca require prompts to be constructed with specific fields: ``definition''\footnote{The Alpaca model uses ``instruction'' instead}, ``input,'' and ``output.''
The ``definition'' is used to specify the instruction or guidance, the ``input'' contains the instance to be classified, and the ``output'' is the output indicator. 
For instance, the prompt for the HS task is the following: <\texttt{Definition}>
Is this tweet expressing ``hate speech'' or ``non-hate speech?'' <\texttt{Input}> ``I hate you''
<\texttt{Response}>: \{LM answer\}.

For the zero-shot prediction, we provide each model with a task-specific prompt and evaluate the model's performance based on the resulting outputs. We used the default parameters for each of the models. We did not optimize any hyperparameters.

If the produced output does not correspond to a valid class, we assign the most common class for that task.
These out-of-label (OOL) predictions vary enormously between tasks and models.
Binary or ternary classification tasks (AC, SA, HS) exhibit a very low OOL percentage (<1\%). In contrast, TD shows a significantly higher percentage (13\%) due to the larger number of classes and the more descriptive nature of these classes (e.g., ``fashion accessories'').
At the model level, Flan models have a very low percentage (1\%), T0 and Tk-Instruct have a low OOL percentage ($\sim$2\%), while Alpaca has a higher percentage (12\%). For TD, Alpaca has an average of 50\% OOL predictions, with a peak of 91\% in French.

\paragraph{Computing Infrastructure} We run all experiments on a server with three NVIDIA RTX A6000 and 48GB of RAM.

\input{prompts}

\subsection{Baselines}

To assess the validity of the LLMs across our five tasks and to establish reference points, we use two baselines: the most frequent class and random choice.

The most-frequent-class baseline does not require any model. It always picks the most frequent label for a task as final prediction. This standard baseline method can be very strong in unbalanced data sets.
The random-choice method randomly picks a label from the set of labels associated with the specific task. It provides a baseline to evaluate the performance of LLMs against a random decision-making process.


\subsection{Aggregation of labels}

We use two different methods to aggregate the five labels provided by the LLMs to reach a consensus: majority voting, and a Bayesian model of annotation, Multi-Annotator Competence Estimation \citep[MACE,][]{hovy-etal-2013-learning}. These methods use different approaches to combine the five labels from the LLMs into one, allowing for a comprehensive analysis and ensuring a more accurate aggregation outcome. 

The majority-vote approach tallies up the number of votes each label received from the five models and selects the one with the highest count as the final aggregated label. In case of a tie, it randomly chooses among the top candidates. This approach is common in many annotation projects, but has the drawback that the majority can still be wrong.

MACE computes the competence of each LLM (i.e., the probability it chooses the correct label instead of guessing it) and uses these weights to select the most likely label, usually leading to a more accurate aggregation outcome than majority voting \cite{paun-etal-2018-comparing}.
Competence scores tend to correlate with annotators' actual expertise \cite{hovy-etal-2013-learning}, and can therefore be used to directly compare annotators in the absence of gold labels.


\section{Results}
We compare five models on five tasks and four languages. The relevant guiding questions here are: 
\textit{How much do models agree with each other?} (as this suggests their ability to find a correct answer), 
\textit{How reliable is each model?} (as this suggests whether they can be aggregated)
and \textit{How good are the predictions of the individual models and their aggregations} (as this determines whether ZSL is a viable alternative to supervised learning)?


\subsection{Inter-model Agreement}
To get insights into the consistency and reliability of the LLMs as annotators, we evaluate the level of agreement among the different models. We use four common inter-annotator-agreement metrics: Cohen's $\kappa$ \cite{Cohen1960ACO}, Fleiss' $\kappa$ \cite{Fleiss1971MeasuringNS}, and Krippendorff's $\alpha$ \cite{krippendorff1computing} (which all correct the observed agreement for expected agreement), as well as the unweighted raw agreement (i.e., the uncorrected level of agreement between LLMs). The results are shown in Table \ref{tab:agreement_trustpilot}.
Note that raw agreement is normally much higher than corrected inter-annotator agreement measures.

\input{inter_model_agg_trustpilot}

The results show a wide range of agreement values, but a few take-aways become apparent. 

1) Some tasks are easier than others: SA has relatively high agreement scores, TD much less so, and gender, age, and HS prediction have little to no actual agreement. One explanation is that the tasks with low agreement scores are inherently harder. Note that the number of labels does not factor into agreement: SA has three labels, TD five, whereas AC-Age, AC-Gender, and HS have two.

2) Language does not factor into the differences. The models we test are all multi-lingual, and the languages we test are generally high-resource. The agreement difference between the different languages on the same task is negligible.

3) The scores suggest that the different models specialize on different tasks and labels. As we will see in the performance  and reliability analysis, some models perform better on some tasks than others. This specialization of the models suggests that aggregation is not only feasible, but also beneficial (as the aggregation hopefully benefits from differing expertise).

\input{competence_trustpilot}
% \input{competence_hateval}


\subsection{Reliability}
When aggregating results, the main question is ``Whom to trust?''
We use the competence scores from the MACE model to assess the reliability of each model on each task and language. These scores represent the probability that a model chooses the correct label (rather than just guessing it).
Table \ref{tab:tab_competence_trustpilot} 
% and \ref{tab:tab_competence_hateval} 
shows the competence scores.

As hypothesized before, the competence scores suggest specializations of the different models on different languages and tasks. None of the models is dominant in all settings, though the two Flan models tend to have higher competence scores than the other models (reflected in their higher mean competence scores in Table \ref{tab:tab_competence_trustpilot}).
T0 and Flan-T5 seem to be better for French data.
Alpaca labels seem be discounted for most tasks by MACE.


\subsection{Performance of Models and Robustness of Label Aggregation}
Ultimately, we care about the predictive performance of zero-shot models. Table \ref{tab:results_trustpilot} 
% and \ref{tab:results_hateval} 
shows the macro-F1 scores of the LLMs on all tasks and languages.
We compute the statistical difference of the individual results over the random-choice baseline, using a bootstrap sampling test with the \textit{bootsa}\footnote{\url{https://github.com/fornaciari/boostsa}} Python package. We use 1000 bootstrap samples, a sample size of \%20, and $p \leq 0.01$. 

Most models clearly and significantly outperform the most-frequent-label and random-choice baselines. The one outlier is Alpaca, which often performs worse than the random-choice baseline. This result is possibly due to the nascent state of Alpaca, which is described as ``still under development'' \cite{alpaca}. 
Note also that Flan-T5 and Flan-UL2 were the only models that included the HatEval dataset in their training. Consequently, they perform substantially better than the other models (with Flan-T5 receiving a very high competence score from MACE).

\paragraph{Aggregation}
When aggregating different annotations into a single label, we implicitly assume that a) there is a single correct answer and b) that the wisdom of the crowd will find it. The first assumption is up for debate \cite{basile-etal-2021-need}, but the latter is clearly borne out by the results here.
In most cases, both majority voting and MACE aggregation result in predictions that are better than the majority of models, and 50\% of the time (7 out of 14 tasks) the best model.
Note that for SA, Tk-Instruct performs better than the aggregation methods in all languages. For AC-Gender in English, Flan-UL2 is better, and in German, no method outperforms random choice (though MACE aggregation is close).

Overall, though, the two aggregation methods are substantially more robust than any one individual model (see the Mean results) across all languages and data sets. Presumably, they suffer less from the variance across tasks and languages and instead are able to draw upon the specialization of each model as a source of information. 
The MACE competence score does correlate (though not strongly) with the actual performance of the models: 0.45 Spearman $\rho$ and 0.43 Pearson $\rho$. 
This correlation suggests that MACE identified some of the specializations of the models correctly. However, a custom weighting of each model's prediction (for example based on the actual performance) might perform even better. Though in practice, this weighting would of course be unknown. 
\input{results_trustpilot}
% \input{results_hateval}

\paragraph{Comparison to supervised learning}
ZSL holds a lot of promise for quick predictions, but to assess its worth, we need to compare it to supervised models.
For the Trustpilot data, we compare our best ZSL result for each task and language (see Table \ref{tab:results_trustpilot}) to two types of supervised models. 1) a simple Logistic Regression model \citep[the baseline ``agnostic'' results reported in][]{hovy-2015-demographic} and a recent Transformer-based model \citep[the best results from][]{hung-etal-2023-demographic}.
Similarly, for HatEval, we compare with 1) a simple linear Support Vector Machine based on a TF-IDF representation \citep[the baseline results reported in][]{basile-etal-2019-semeval} and a fine-tuned multilingual Transformer model \cite{nozza-2021-exposing}. Table \ref{tab:results_trustpilot_comp} shows the results. 
The two methods approximate an upper and lower bound on supervised learning for these datasets.
\input{results_comp_supervised}

Except for 4 cases (AC-Gender and AC-Age in French, AC-Gender in English, HS in English), even the simple ML models beat the best ZSL result we achieved, be it from an LLM or aggregation method. 
%These supervised models were trained on word embeddings. While they have access to large amounts of data, the model itself is simple.
Comparing the results on TrustPilot to the upper bounds from \newcite{hung-etal-2023-demographic}, we see an average performance gap of 10.2 F1 points.
An interesting result is the one obtained for HS in English, where ZSL achieves better results, presumably because it is not affected by overfitting issues that arise during training \cite{nozza-2021-exposing}.

These results show that while ZSL might be a fast approximation for prediction tasks, it is stil far from competitive with supervised learning.


\section{Related Work}

Generating human-annotated data is time-consuming and expensive, especially for complex or specialized tasks with limited available data. Instead, one possible solution is to rely on automatic annotation models to generate labeled data for specific tasks, often leveraging a small subset of labeled data \cite{smit-etal-2020-combining,rosenthal-etal-2021-solid}. Supervised learning has emerged as the dominant method, with the widespread adoption of first conventional machine learning models like Support Vector Machines (SVM) and Logistic Regression, and, more recently, the current state-of-the-art Transformer-based models like BERT \cite{devlin-etal-2019-bert}.

More recently, LLMs have shown zero-shot and few-shot learning capabilities \cite{brown2020language}, and researchers have further advanced these models to incorporate natural language instructions \cite{chung2022scaling, wang-etal-2022-super,alpaca}. They have led to innovative techniques like prompting \cite{liu2023pre} that use zero-shot and few-shot learning paradigms without the need to train a supervised model. Recent works have explored these new techniques and found good performance on various NLP tasks \cite{brown2020language,su2022selective}. However, using LLMs with prompting for data annotation remains largely unexplored, and only a few studies exist. For instance, \cite{lee2023large} evaluate the performance and alignment between LLMs and humans. Their findings revealed that these models not only fall short in performing Natural Language Inference (NLI) tasks compared to humans but also struggle to capture the distribution of human disagreements accurately. Some studies have used ChatGPT as an annotator. While some have seen excellent capabilities \cite{Huang_2023,gilardi2023chatgpt,tornberg2023chatgpt}, \newcite{kuzman2023chatgpt} claim that the performance drops significantly when considering less-resourced languages, showing the current limitations of ChatGPT.

For annotation, it remains to be seen whether different LLMs (not only ChatGPT, which is not open source) can generalize to \textit{any} text classification task, especially if these tasks are not well-represented in the training data. In addition, recent studies have shown the importance of considering human label variation \cite{basile-etal-2021-need, plank-2022-problem}, i.e., the disagreement between human annotators, as a source of information rather than a problem. 
%It is an open question to determine whether the same applies to LLMs if we consider them annotators.


\section{Discussion}
Our results indicate that treating LLMs as annotators and aggregating their responses works. At least in the sense that it is cost-effective and produces a result quickly. However, we also find that the overall performance is still well below that of even simple supervised models.

Human annotation thus still has an important role to play if we are focusing on performance. As LLMs become more capable, this edge might diminish, to the point where LLM annotation is equivalent to human annotation. As an aside: even though all models are likely to improve across the board, we still expect there to be specialization effects, meaning an aggregation approach is likely going to stay relevant for the foreseeable future.

But what about bias? Human label variation is not only due to different levels of expertise or diligence \cite{snow-etal-2008-cheap}, it can also vary due to differing opinions, definitions, and biases \cite{shah-etal-2020-predictive}.
Certain tasks are subjective by nature \cite{basile-etal-2019-semeval,rottger-etal-2022-two}, but even seemingly objective tasks like part-of-speech tagging can have different interpretations \cite{plank-etal-2014-linguistically}.
The current discussion around the moral and ethical alignment of LLMs \cite{liu-etal-2022-aligning} should make us cautious about using these models as annotators in subjective or sensitive tasks. Aggregation might be able to overcome the biases of any one particular model, but it cannot safeguard against widespread or common biases.
Lastly, annotation might be exploratory \citep[the ``descriptive'' paradigm in][]{rottger-etal-2022-two}, where the goal is to find the range of human responses.

However, replacing human annotators with LLMs has ramifications beyond mere performance and bias issues. 
While crowdsourced annotations can be problematic in terms of worker exploitation \cite{fort-etal-2011-last}, it does often provide low to moderate income earners with a way to supplement their living. Replacing this option with LLMs is a real-life example of automation making human jobs obsolete.

Given the performance gap and the issues around moral judgment, NLP practitioners might want to strongly consider the option of human annotation for the time being.


\section{Conclusion}
We use zero-shot prompting to compare five current LLMs as annotators on five tasks in four languages. We find that the models' performances vary widely across tasks and labels.
We propose to leverage this label variation similarly to human label variation by aggregating their predictions into a single label. We find that this approach is, on average, substantially better than any individual model. So given that we cannot measure performance in truly zero-shot settings, using label aggregation guarantees better average performance than selecting any one LLM.
With the rapid development of LLMs, it is hard to predict what will happen next. Increasing the parameter size of the models might improve their generalization capabilities to unseen tasks. Aggregation is still likely to improve performance.

Our findings suggest that practitioners who want to label a large amount of data at minimal cost (both financially and time-wise) would benefit from the aggregation approach we outlined here.
However, we also find that even the best models cannot compete with ``traditional'' supervised classification approaches.
Furthermore, human annotation allows practitioners to encode a specific view or approach in a prescriptive manner, or explore the range of responses in a descriptive way \cite{rottger-etal-2022-two}. Relying on LLMs while alignment and bias still need to be solved \cite{mokander2023auditing} makes this approach not suitable for sensitive applications.



\section*{Ethical Considerations}
\label{sec:ethics}

The data we use for AC-gender classification only makes a binary distinction (the Trustpilot website allowed users only to choose from two options). We do not assume this to be representative of gender-identities and only use this data to test our hypotheses.

The languages we evaluate all come from the Indo-European branch of languages. The selection was due to data availability and our knowledge of languages. While we do not expect results to systematically differ from other languages, we do note that this is conjecture. 

% Add information regarding the HatEval data

%\section*{Acknowledgments}

\bibliography{tacl2021,anthology}
\bibliographystyle{acl_natbib}

%\pagebreak

%Note that we use the latest open-source language models for our experiments, we did not explore other recent language models, such as the GPT family, primarily because they are not open and reasonably reproducible\footnote{\url{https://hackingsemantics.xyz/2023/closed-baselines/}}, and therefore the community may encounter challenges in replicating our results. 

\end{document}


