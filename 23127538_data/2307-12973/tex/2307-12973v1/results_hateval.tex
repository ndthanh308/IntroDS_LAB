\begin{table*}[ht!]
\small
\centering
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cc|ccccc|cc|cc}
\multicolumn{2}{c|}{} & \multicolumn{5}{c|}{\textbf{Models}} & \multicolumn{2}{c|}{\textbf{Baselines}} & \multicolumn{2}{c}{\textbf{Aggregate}}\\
\toprule
\multicolumn{2}{c|}{\textbf{Task/Lang.}} & \textbf{mT0} & \textbf{Flan-T5} & \textbf{Flan-UL2} & \textbf{Tk-Instruct} & \textbf{Alpaca} & \textbf{Most Freq.} &  \textbf{Random} &  \textbf{Majority} & \textbf{MACE} \\
\midrule
\multirow{2}{*}{HS} 
& EN & 0.621$^{\star}$ & 0.726$^{\star}$ & 0.670$^{\star}$ & 0.579$^{\star}$ & 0.428 & 0.367 & 0.490 & \textbf{0.732}$^{\star}$  & 0.726$^{\star}$  
\\
& ES & 0.601$^{\star}$ & 0.532$^{\star}$ & 0.519 & 0.449 & 0.394 & 0.370 & 0.492 & 0.476 & \textbf{0.603}$^{\star}$
\\
\bottomrule
\end{tabular}
}
\caption{Macro-F1 results obtained by the LLMs on the HatEval dataset, the baselines and the aggregation methods. Best result per language and task is shown in bold. Significant improvement over Random baseline ($^{\star}: p \leq 0.01$) with bootstrap sampling. For  non-English languages, we use the multilingual mT0 model.}\label{tab:results_hateval}
\end{table*}