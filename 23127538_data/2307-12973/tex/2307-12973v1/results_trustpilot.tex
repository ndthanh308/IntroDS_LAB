\begin{table*}[ht!]
\small
\centering
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cc|ccccc|cc|cc}
\multicolumn{2}{c|}{} & \multicolumn{5}{c|}{\textbf{Models}} & \multicolumn{2}{c|}{\textbf{Baselines}} & \multicolumn{2}{c}{\textbf{Aggregate}}\\
\toprule
\multicolumn{2}{c|}{\textbf{Task/Lang.}} & \textbf{T0} & \textbf{Flan-T5} & \textbf{Flan-UL2} & \textbf{Tk-Instruct} & \textbf{Alpaca} & \textbf{Most Freq} &  \textbf{Random} &  \textbf{Majority} & \textbf{MACE} \\
\midrule
\multirow{3}{*}{SA} 
& EN & 0.453$^{\star}$ & 0.532$^{\star}$ & 0.482$^{\star}$ & \textbf{0.553}$^{\star}$ & 0.467$^{\star}$ & 0.167 & 0.334 & 0.489$^{\star}$ & 0.495$^{\star}$  
\\
& DE & 0.469$^{\star}$ & 0.495$^{\star}$ & 0.433$^{\star}$ & \textbf{0.517}$^{\star}$ & 0.427$^{\star}$ & 0.167 & 0.331 & 0.466$^{\star}$ & 0.477$^{\star}$ 
\\
& FR & 0.460$^{\star}$ & 0.518$^{\star}$ & 0.445$^{\star}$ & \textbf{0.528}$^{\star}$ & 0.434$^{\star}$ & 0.167 & 0.337 & 0.469$^{\star}$ & 0.476$^{\star}$ 
\\
\midrule
\multirow{3}{*}{AC-Gender} 
& EN & 0.516 & 0.594$^{\star}$ & \textbf{0.624}$^{\star}$ & 0.541$^{\star}$ & 0.384 & 0.337 & 0.501 & 0.600$^{\star}$ & 0.622$^{\star}$ 
\\
& DE & 0.456 & 0.437 & 0.447 & 0.431 & 0.394 & 0.334 & \textbf{0.497} & 0.435 & 0.496%$^{\star}$ 
\\
& FR & 0.428 & 0.573$^{\star}$ & 0.566$^{\star}$ & 0.563$^{\star}$ & 0.451 & 0.335 & 0.503 & \textbf{0.576}$^{\star}$ & \textbf{0.576}$^{\star}$ 
\\
\midrule
\multirow{3}{*}{AC-Age} 
& EN & 0.495 & 0.442 & 0.516$^{\star}$ & 0.397 & 0.478 & 0.336 & 0.497 & \textbf{0.590}$^{\star}$ & \textbf{0.590}$^{\star}$ 
\\
& DE & 0.458 & 0.366 & 0.503 & 0.344 & 0.501 & 0.334 & 0.500 & 0.448 & \textbf{0.528}$^{\star}$
\\
& FR & 0.497 & 0.375 & 0.550$^{\star}$ & 0.343 & 0.505 & 0.335 & 0.500 &  0.473 & \textbf{0.545}$^{\star}$
\\
\midrule
\multirow{3}{*}{TD} 
& EN & 0.558$^{\star}$ & 0.579$^{\star}$ & 0.588$^{\star}$ & 0.567$^{\star}$ & 0.174 & 0.085 & 0.195 & 0.587$^{\star}$ & \textbf{0.596}$^{\star}$ 
\\
& DE & 0.506$^{\star}$ & 0.514$^{\star}$ & 0.513$^{\star}$ & 0.493$^{\star}$ & 0.189 & 0.105 & 0.193 & 0.516$^{\star}$ & \textbf{0.520}$^{\star}$ 
\\
& FR & \textbf{0.314}$^{\star}$ & 0.271$^{\star}$ & 0.264$^{\star}$ & 0.257$^{\star}$ & 0.096 & 0.096 & 0.193 & 0.271$^{\star}$ & 0.293$^{\star}$  \\
\midrule
\midrule
\multirow{3}{*}{Mean} 
& EN & 0.506 & 0.537 & 0.553 & 0.515 & 0.376 & 0.231 & 0.382 & 0.567 & \textbf{0.576} \\
& DE & 0.472 & 0.453 & 0.474 & 0.446 & 0.378 & 0.235 & 0.380 & 0.466 &	\textbf{0.505} \\
& FR & 0.425 & 0.434 & 0.456 & 0.423 & 0.372 & 0.233 & 0.383 &	0.447 & \textbf{0.473} \\
\bottomrule
\midrule
\multirow{2}{*}{HS} 
& EN & 0.621$^{\star}$ & 0.726$^{\star}$ & 0.670$^{\star}$ & 0.579$^{\star}$ & 0.428 & 0.367 & 0.490 & \textbf{0.732}$^{\star}$  & 0.726$^{\star}$  
\\
& ES & 0.601$^{\star}$ & 0.532$^{\star}$ & 0.519 & 0.449 & 0.394 & 0.370 & 0.492 & 0.476 & \textbf{0.603}$^{\star}$
\\
\bottomrule

\end{tabular}
}
\caption{Macro-F1 results obtained by the LLMs on the Trustpilot and HatEval tasks, the baselines and the aggregation methods. Best result per language and task is shown in bold. Significant improvement over Random baseline ($^{\star}: p \leq 0.01$) with bootstrap sampling. For non-English languages, we use the multilingual mT0 model.} \label{tab:results_trustpilot}
\end{table*}