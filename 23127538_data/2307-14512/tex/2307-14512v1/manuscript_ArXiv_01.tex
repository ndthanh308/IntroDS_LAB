% \begin{filecontents*}{example.eps}
% %!PS-Adobe-3.0 EPSF-3.0
% %%BoundingBox: 19 19 221 221
% %%CreationDate: Mon Sep 29 1997
% %%Creator: programmed by hand (JK)
% %%EndComments
% gsave
% newpath
%   20 20 moveto
%   20 220 lineto
%   220 220 lineto
%   220 20 lineto
% closepath
% 2 setlinewidth
% gsave
%   .4 setgray fill
% grestore
% stroke
% grestore
% \end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}


% ////////////////////////// Added by @Mehdi
% \usepackage{subcaption}
\usepackage{soul}
\usepackage{color, colortbl}
\usepackage{tikz}
\usepackage{multirow}
\usepackage[english=american]{csquotes}
\usepackage{subfig}
\usepackage{tcolorbox}
\usepackage{graphics}
\usepackage{hyperref}
\usepackage{url}
% \usepackage{lipsum}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{threeparttable}
%\usepackage{booktabs}
\usepackage{adjustbox}

\newcommand{\Foutse}[1]{\textcolor{purple}{{\it [Foutse: #1]}}}
\newcommand{\Amin}[1]{\textcolor{red}{{\it [Amin: #1]}}}
\newcommand{\Mehdi}[1]{\textcolor{teal}{{\it [Mehdi: #1]}}}
\newcommand{\jack}[1]{\textcolor{orange}{{\it [Jack: #1]}}}
\newcommand{\Florian}[1]{\textcolor{brown}{{\it [Florian: #1]}}}


\newcommand{\ReplicationPackage}{\texttt{https://github.com/ML-Bugs-2022/Replication-Package}}
% ////////////////////////// 

%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Bug Characterization in Machine Learning-based Systems\thanks{This work was supported by: Fonds de Recherche du Québec (FRQ), the Canadian Institute for Advanced Research (CIFAR) as well as the DEEL project CRDPJ 537462-18 funded by the National Science and Engineering Research Council of Canada (NSERC) and the Consortium for Research and Innovation in Aerospace in Québec (CRIAQ), together with its industrial partners Thales Canada inc, Bell Textron Canada Limited, CAE inc and Bombardier inc.}%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Mohammad Mehdi Morovati \and
        Amin Nikanjam \and 
        Florian Tambon \and
        Foutse Khomh \and Zhen Ming (Jack) Jiang
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Mohammad Mehdi Morovati
           \and
           Amin Nikanjam
              \and
              Florian Tambon \and
            Foutse Khomh \at
            SWAT Lab., Polytechnique Montréal, Montréal, Canada\\
            \email{\{mehdi.morovati,amin.nikanjam,florian.tambon,foutse.khomh\}@polymtl.ca}\\
           Zhen Ming (Jack) Jiang \at York University, Toronto, Canada \\
           \email{zmjiang@cse.yorku.ca}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}

% \textcolor{blue}{
Rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Since corrective maintenance, i.e. identifying and resolving systems bugs, is a key task in the software development process to deliver reliable software components, it is necessary to investigate the usage of ML components, from the software maintenance perspective. Understanding the bugs characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 \textit{GitHub} repositories that used one of the three most popular ML frameworks, i.e., \textit{TensorFlow}, \textit{Keras}, and \textit{PyTorch}. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspection of 386 sampled reported issues in the identified ML-based systems to indicate whether they affect ML components or not. Our analysis shows that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. Next, we thoroughly examined 109 identified ML bugs to identify their root causes, symptoms, and calculate their required fixing time. The results also revealed that ML bugs have significantly different characteristics compared to non-ML bugs, in terms of the complexity of bug-fixing (number of commits, changed files, and changed lines of code). Based on our results, fixing ML bugs are more costly and ML components are more error-prone, compared to non-ML bugs and non-ML components respectively. Hence, paying a significant attention to the reliability of the ML components is crucial in ML-based systems. These results deepen the understanding of ML bugs and we hope that our findings help shed light on opportunities for designing effective tools for testing and debugging ML-based systems.
% }

% Rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the value of having reliable ML components, i.e., a software component operating based on ML. Since corrective maintenance is a key task to deliver reliable software components, it is necessary to investigate the employment of ML components, from the software maintenance perspective. Understanding ML bugs can help developers to determine where to focus development and testing efforts. Hence, in this paper, we study how employing ML components affects software maintenance in ML-based software systems. We extracted 447,948 \textit{GitHub} repositories that used one of the three most popular ML frameworks, i.e., \textit{TensorFlow}, \textit{Keras}, and \textit{PyTorch}. After multiple steps of filtering, we check the top 100 repositories with the highest number of closed issues. We carefully inspect them to make sure that an ML component is utilized to interact with other software components in the system. Then, we manually inspect 386 sampled issues raised in the selected ML-based systems to indicate whether they are bugs related to ML components or not. Next, we review 109 identified ML bugs to determine their root cause, symptom, and effort-to-fix. We show that nearly half of the issues are related to ML components. The results also revealed that ML bugs have significantly different characteristics compared to non-ML ones, in terms of the complexity of bug-fixing (number of commits, changed files, and changed lines of code). We also show that fixing ML bugs consumes as many resources (time and expertise level) as non-ML bugs. Thus, paying significant attention to the reliability of the ML components is crucial since ML issues are more costly and their impacts can be propagated to other software components. These results deepen the understanding of ML bugs and we hope that our findings help shed light on opportunities to design effective tools for testing and debugging ML-based systems.
\keywords{Software Bug \and Software Testing \and ML-based Systems \and ML Bug \and Deep Learning \and Software Maintenance \and Empirical Study}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
% \textcolor{blue}{
Machine Learning (ML) is a branch of Artificial Intelligence (AI) that has been applied in a large number of practical applications such as computer Vision~\cite{liu2020small}, and Natural Language Processing (NLP)~\cite{aithal2021automatic}. ML-based systems refer to software systems having at least one ML component~\cite{morovati2023bugs}. A software component is a module or unit of software which is distinct logically or functionally from other modules~\cite{ieee5733835:2010}. Accordingly, an ML component is a software component that operates based on ML. An ML component has several elements affecting its functionality~\cite{morovati2023bugs}.
% }
% Machine Learning (ML) is a flourishing branch of Artificial Intelligence (AI) that works successfully in a large number of practical applications such as computer vision~\cite{liu2020small}, and Natural Language Processing (NLP)~\cite{aithal2021automatic}. 
% ML-based systems refer to software systems having at least one ML component. 
% \textcolor{blue}{
Figure~\ref{fig:ml_based_system} shows a sample ML-based system; i.e., a transcription service where the ML component provides the key functionality which is converting uploaded voice to text. However, to be able to use the ML model in the production environment, several additional software components, referred to as non-ML components, are necessary. For instance, a user interface is required for enabling users to create a user account, upload the audio file(s), make payment of service expenses, and view the resulting transcript generated from the uploaded audio. Additionally, database and processing components are required to manage related tasks for converting uploaded audio to text, saving transcripts, and a monitoring component to ensure that operational system metrics meet the specified requirements. It is worth mentioning that the ML component itself comprises various elements (from configuration to monitoring) and ML code represents only a small part of the entire ML component~\cite{sculley2015hidden}. 
% }
% (e.g., Figure~\ref{fig:ml_based_system}). 
% An ML component is a software component that operates based on ML. An ML component has several elements 
% affecting its functionality~\cite{google_MLOps}.
% The explosive growth of applying ML and Deep Learning (DL) in safety-critical areas (such as autonomous driving ~\cite{choi2022sensor,jebamikyous2022autonomous}, health-care~\cite{lei2022predicting,kaul2022deep}, and aviation systems~\cite{alkhamisi2020ensemble,zhang2019ensemble}) led to Software Engineering community experiencing a surge in demand for reliable ML-based systems. 

% Figure environment removed

% Software Reliability Engineering (SRE) is an integral step in the software engineering process that centers around software reliability~\cite{pham2003handbook}. Almost all engineering fields consider reliability as the most important factor in quality~\cite{lyu2007software}. In software engineering, reliability refers to the likelihood that software operates without any failure in a specific period of time and environment~\cite{ieee5733835:2010}. 
% With regards to the fact that software failure originally stems from a software bug, characterizing software bugs plays a key role in SRE~\cite{galin2004software,lyu2007software}. Therefore, a clear understanding of bugs in ML-based systems is one of the essential requirements for providing effective SRE methods for these systems.
% \textcolor{blue}{
Software maintenance is the last part of the software development process known for its high required effort and cost~\cite{grubb2003software}. Corrective maintenance, a type of software maintenance focusing on detecting and fixing bugs, is considered as one of the main tasks to deliver reliable software systems~\cite{ieee5733835:2010}. Thus, a clear understanding of software bugs as the core concept of corrective maintenance can assist developers to implement reliable software systems more efficiently. 
% Fundamental differences between traditional and ML software components exist: developers of traditional software systems write a program that directly encodes the logic to meet the functional requirements, while ML developers implement an ML application that encodes the ML model like a neural network structure and train it using a dataset~\cite{zhang2020machine}. This ML model is an integral part of an ML component which will need to interact with other non-ML and ML components. Therefore, we are facing new types of bugs, either ML bugs affecting the ML components or non-ML bugs affecting non-ML components, in ML-based systems and new challenges in characterizing them~\cite{riccio2020testing}.
% Like any other type of software system, ML-based software systems are also error-prone and need
% SRE to meet required reliability~\cite{IEEE:reliability:7827907}.
% Corrective maintenance that focuses on fixing bugs is also one of the main tasks to deliver reliable software systems~\cite{ieee5733835:2010}.
% Because of fundamental differences between traditional and ML software components, we are facing new types of bugs in ML-based systems and new challenges to characterize them~\cite{riccio2020testing}. 
There are fundamental differences between traditional software components and ML software components. In traditional software components, developers implement a program that directly encodes the logic to fulfill the functional requirements. However, to develop ML components, one needs to implement an ML model, such as a neural network, and then train it using a dataset~\cite{zhang2020machine}. In other words, ML algorithms learn a complex function from the given dataset and subsequently apply the learned function to the unseen data points. The trained model is then used in the ML components for inference and generating desired results. 
Due to the central role of the trained ML model in the functionality of ML components, we are facing new types of bugs in ML components, referring to the malfunctioning of learned functions for unseen data points, and new challenges in characterizing them~\cite{zhang2018empirical}. 
Accordingly, we can categorize bugs in ML-based systems into two main categories: ML bugs that affect the functionality of ML components and non-ML bugs which affect the functionality of other (non-ML) components. It is important to note that ML models serve as an integral element within ML components that requires interaction with other non-ML and ML components. 
% }

% As an example, developers of traditional software systems write a program that encodes the model to meet the functional requirements. But ML developers implement an ML application that encodes the ML model as a network structure and train it using a dataset matching the target problem~\cite{zhang2018empirical}. 
% \textcolor{blue}{
Although a number of studies investigated the bugs in ML programs~\cite{shen2021comprehensive,yan2021exposing}, differences between bug types in ML programs~\cite{humbatova2020taxonomy}, and their symptoms and root causes~\cite{islam2019comprehensive,zhang2018empirical}, none of them provides detailed information on the characteristics of bugs from the software maintenance viewpoint, in comparison with non-ML bugs. This paper aims to fill this gap in the literature by providing insight into the characterization of ML and non-ML bugs in ML-based systems. We also examine the impact of employing ML on the maintenance of software systems. We answer the following research questions:
% }
% Although a number of studies investigated the bugs in ML programs \cite{shen2021comprehensive,yan2021exposing}, differences between bug types in ML programs \cite{humbatova2020taxonomy}, and their symptoms and root causes \cite{islam2019comprehensive,zhang2018empirical}, none of them provides detailed information on their characteristics from the software maintenance viewpoint, in comparison with non-ML bugs. 
% Thus, we aim to provide insight into the characterization of ML and non-ML bugs in ML-based systems and identify the impact of employing ML
% on the maintenance of software systems. The final selection of repositories, post filtering, focuses on the most popular and publicly available (open source) repositories over GitHub using Python as a programming language. We formulate the goals of the study with the following research questions:
\begin{itemize}
    \item[\textit{\textbf{RQ1:}}]What are the distributions of the ML and non-ML bugs in ML-based software systems?
    \item[\textit{\textbf{RQ2:}}]What are the differences between the complexity level of fixing ML and non-ML bugs in ML-based systems? 
    \item[\textit{\textbf{RQ3:}}]What is the difference between needed resources (time-to-fix and developer expertise) for fixing ML and non-ML bugs?
\end{itemize}

To answer these questions, we analyzed 386 closed issues gathered from 40 open-source ML-based software systems using the three most popular ML frameworks including \textit{TensorFlow} \cite{abadi2016tensorflow}, \textit{Keras} \cite{chollet2018keras}, and \textit{PyTorch} \cite{paszke2019pytorch}. We summarize the contribution of this study as follows:
% \textcolor{blue}{
\begin{itemize}
    \item We present the first comprehensive empirical study on the characteristics of the bug in ML-based systems, from the software maintenance point of view. 
    \item We provide insight into the cost and complexity of fixing ML and non-ML bugs in ML-based systems.
    \item We indicate and discuss the root causes and symptoms of studied ML bugs.
   \item  We make the dataset used in this study publicly available online~\cite{Replication-Package} for other researchers/practitioners to replicate our results or build on our work.
    % \item We make the dataset used in this study publicly available online \cite{Replication-Package} for other researchers/practitioners to replicate our results or build on our work.
\end{itemize}
% }

\textbf{The rest of the paper is organized as follows.} We present the background of the study in Section~\ref{sec:background}. Next, we explain the methodology we follow to answer research questions in Section~\ref{sec:methodology}. Section \ref{sec:result} describes the results and findings of the study. 
Then, we discuss the related works and threats to validity of this study in Section~\ref{sec:related_work} and Section~\ref{sec:validity}. Finally, we present the conclusion and future works in Section~\ref{sec:conclusion}.

\section{Background}
\label{sec:background}

\subsection{ML-based systems}

A software component is a self-contained part of software or application that is able to work independently. The reason behind dividing software systems into components is to provide smaller parts with less complexity and more manageability~\cite{lau2018introduction}. The ML community defines ML components as software components that work based on ML algorithms and aim to provide intelligent behavior. ML-based systems are software systems containing at least one ML component~\cite{martinez2021software}.

DL, as a branch of ML, includes neural networks with numerous layers that results in large (deep) models. Recent DL outstanding successes in decision-making and human-competitive tasks (e.g. stock trading~\cite{carta2021multi}) have made it a part of several state-of-the-art software systems ~\cite{humbatova2021deepcrime}. DL-based systems are software systems where the DL model is implemented, trained with a large dataset, and integrated into the software systems~\cite{chen2020comprehensive}. Nowadays, there is an increasing demand for providing reliable DL-based systems, because of the increasing adoption of DL in different software areas, and especially safety-critical areas where any single bug may result in catastrophe (e.g., Tesla autopilot accident~\cite{vlasic2016self}). 

Several frameworks such as \textit{TensorFlow}, \textit{Keras}, \textit{PyTorch}, and \textit{scikit-learn} \cite{scikit-learn} have been provided to help users to design, and implement ML models, and integrate them into the software systems. They provide high-level APIs to simplify the development of ML components for non-expert ML programmers such as software engineers and domain experts~\cite{schoop2021umlaut}. 

\subsection{Bugs in ML-based systems}
Software error is an incorrect part of a source code, which can be a grammatical, logical, or any other type of mistake that a software developer may make \cite{galin2004software}. An ML error is any mistake that occurred in any part of the code of an ML component (e.g. data collection, feature extraction, and ML training code) \cite{zhang2020machine}. A Software system containing erroneous code can produce an abnormal behavior, i.e., a fault (software bug). A software bug is a problem causing a disparity between defined software requirements and developed functionality \cite{ieee5733835:2010}. Accordingly, an ML bug refers to the differences between the existing and required behavior of an ML component \cite{zhang2020machine}. 



% \textcolor{blue}{
Based on the location of the bugs (in the code) and their impact, bugs in ML-based systems can be classified into ML and non-ML bugs. We define ML bugs as issues that affect the functionality of ML components. In contrast, a non-ML bug is an issue that deteriorates the functionality of non-ML components. Similar to research conducted by Riccio et al.~\cite{riccio2020testing}, we categorize bugs in ML-based systems into three main classes based on the location of the code where the bugs occur (Figure~\ref{fig:ml_bug}):
% }

% Figure environment removed

% \textcolor{blue}{
\begin{itemize}
    \item \textit{\textbf{Bugs in ML components}}: In this class, we investigate only bugs inside ML components in isolation. As an example, issue~\cite{issue_sample_01} is related to a problem in training logs, hence it is related to an issue inside the ML component. 
    \item \textit{\textbf{Bugs in non-ML components, but affecting ML components}}: This class considers bugs inside any component affecting ML components' functionality. This category includes all bugs classified into the previous category (Bugs in ML components) as well. For example, issue~\cite{issue_sample_02} explains a problem in the output server channel causing a bug in interactive learning. Although this issue is not inside the ML component, it results in failure in the inference of the ML component.
    \item \textit{\textbf{Bugs anywhere in the system}}: This class contains any bugs in all the system's components, related to ML components or not. An issue of the Rasa project (\#4142)~\cite{issue_sample_03} is an example of a non-ML bug referring to a problem in the presentation of the output.
\end{itemize}
% }
% Based on the place of bugs (in the code) and their effect, bugs in ML-based systems can be classified into ML and non-ML bugs. 
% In this paper, using the three ML-based systems testing levels introduced by Riccio et al. \cite{riccio2020testing} (model, integration, and system testing), 
% we categorize bugs in ML-based systems
% into three main classes. This classification relies on the place in the code where the bugs occur:
% \begin{itemize}
%     \item \textit{\textbf{Bugs in ML components}:} In this class, we investigate only bugs inside ML components, in isolation.
%     \item \textit{\textbf{Bugs anywhere in the system affecting ML components}:} This class considers bugs inside any component affecting ML components' functionality. We refer to this class as \textbf{\textit{ML bugs}}. 
%     \item \textit{\textbf{Bugs 
%     anywhere in the system}:} This class contains any bugs in all the system's components, including \textit{non-ML bugs}. 
% \end{itemize}


% \textcolor{blue}{
ML bugs can emerge from three main sources: program level (ML program written to build ML model), production level (i.e., using the ML model for inference), and infrastructure level (ML frameworks) (see Figure \ref{fig:bug_dl}). Bugs at any level may also affect the overall quality of the ML component and accordingly, the quality of the ML-based system. In this study, we focus on the ML bugs at the program level. 
% }

% ML bugs can emerge from three main sources: program level (ML program written to build ML model), production level (i.e., ML model), and infrastructure level (ML frameworks) (see Figure \ref{fig:bug_dl}). Bugs at any level may also affect the overall quality of the ML component and accordingly, the quality of the ML-based system. Therefore, guaranteeing quality at each level is an important necessity in SRE of ML-based systems \cite{yan2021exposing}.  



When a user tries to use a buggy section of the software, it triggers the bug which results in a system failure. System failure refers to the inability of the software system to perform its identified functionality. In addition to the general concept of system failure, ML bugs may also result in a bad performance, crash, data corruption, hang, and memory out of band which are considered ML failures as well~\cite{islam2019comprehensive}. 

% Figure environment removed

\subsection{SRE in ML-based systems}
SRE is the study of the functional behavior of the software systems in conformance with the user requirements, in terms of reliability~\cite{lyu2007software,IEEE:reliability:7827907}. Software reliability implies the ability of the software system or component to perform required functionality during a specific period of time. Essential differences between the paradigms of traditional and ML-based software systems generate new challenges in SRE of ML-based systems. Fast changes in new versions of ML frameworks~\cite{islam2020repairing}, unportable code~\cite{lenarduzzi2021software}, bug reproducibility~\cite{zhang2018empirical}, and lack of detailed information about bugs~\cite{wardat2021deeplocalize} are the most significant challenges in SRE of ML-based systems. Thus, SRE methods working effectively for traditional software systems operate unsatisfactory for ML-based systems. As such, SRE techniques need to be adapted from the ones originally developed for traditional software systems to ML-based systems.  

One of the most significant SRE tasks to enhance the reliability of the software system and stop the recurrence of software failures is software maintenance~\cite{wang2006reliability}. Software maintenance is the process of modifying software systems after delivering them to the end user, in order to fix discovered bugs, improve software performance, or add new features to adapt the software to new requirements. Software maintenance is classified into four basic types including adaptive, corrective, perfective, and preventive~\cite{ieee5733835:2010}. Corrective maintenance is an essential part of SRE that aims at fixing bugs after delivering the software systems to the users~\cite{IEEE:reliability:7827907}. 
% \textcolor{blue}{
Concerning that characterizing bugs is the first step toward bug fixing~\cite{ni2020analyzing}, bug characterization is considered a necessary stage in the maintenance process.
% }
Thus, we are going to shed light on the way that ML and non-ML bugs affect the corrective maintenance of ML-based software systems.
% \textcolor{blue}{
Besides, understanding bug characterizations potentially improves developers' expertise in debugging and development practices. In other words, our results would help ML-based software developers to identify best practices that lead to fewer bugs. Moreover, the provided results could foster the implementation of automated testing tools for bug detection, bug localization, and debugging.
% }


\section{Methodology}
\label{sec:methodology}

% Figure environment removed

In this section, we describe our methodology to investigate bugs in ML-based software systems answering our RQs. The general process consists of three steps: (1) Collecting repositories, (2) Manual inspection of repositories, and (3) Manual labeling of bugs. Figure~\ref{fig:method} illustrates the methodology that we followed in our study. 

\subsection{Collecting repositories}
To identify relevant ML issues, this study focuses on projects developed using the three most popular ML frameworks: \textit{TensorFlow}, \textit{Keras}, and \textit{PyTorch}. We select them based on their popularity metrics in \textit{GitHub} as shown in Table~\ref{tbl:ml_frameworks}, e.g., the number of stars and forks.

\begin{table}
\small
\caption{Details of the selected ML frameworks.}
    \centering
    \begin{tabular}{p{3.5cm} r r r}
        \hline
            \textbf{ML Framework} & \textbf{\#stars} & \textbf{\#forks} & \textbf{\#subscribers} \\
            \hline
            \rowcolor{gray!40}
            TensorFlow & $174 k$ & $88.3 k$ & $3.4 k$\\
            % \hline
            \rowcolor{gray!40}
            Keras & $58.3 k$ & $19.3 k$ & $2 k$\\
            % \hline
            \rowcolor{gray!40}
            PyTorch & $66.7 k$ & $18.3 k$ & $1.7 k$ \\
            Caffe & $ 33.3 k$ & $19 k$ & $269$ \\
            Jax & $ 23.1 k$ & $2.2 k$ & $504$ \\
            MXNet & $ 20.4 k$ & $6.9 k$ & $875$ \\
            CNTK & $ 17.4 k$ & $4.4 k$ & $201$ \\
            sonnet & $ 9.6 k$ & $1.4 k$ & $51$ \\
        \hline
    \end{tabular}
        \label{tbl:ml_frameworks}
        \vspace{-1em}
\end{table}

We use \textit{GitHub} as the main source to extract the needed data for our analysis. As of December 2021, \textit{GitHub} \cite{github-website} has more than 73 million registered users and over 200 million repositories. Computer society also considers \textit{GitHub} as the most important resource of open source software systems~\cite{li2020exploratory}. \textit{GitHub} provides APIs to facilitate extracting detailed data about repositories. We use \textit{GitHub} search API~\cite{github_api_v3} to extract repositories using identified ML frameworks. \textit{GitHub} search API receives a query as input and returns a list of repositories that match the query.


Since \textit{Python} is the most popular programming language for ML~\cite{voskoglou2017best,Gupta:MLLangugae}, and \textit{TensorFlow}, \textit{PyTorch}, and \textit{Keras} provide \textit{Python} APIs, we also narrow down the search to the repositories developed using \textit{Python} by adding `\texttt{language:python}' parameter to the search queries. In \textit{Python}, when a developer wants to use a library, she adds the library to the script using the `\texttt{import}' command before its usage. For example, developers add `\texttt{import keras}' to the script to use \textit{Keras} in their application. So, we run search queries with `\texttt{import <ML framework>}' (\textit{TensorFlow}, \textit{Keras}, and \textit{Pytorch}) to fetch all repositories using these frameworks. Since \textit{Keras} may be imported using `\texttt{import tensorflow.keras}', we also search for this command as well.
\textit{GitHub} limits the users to access only the first 1000 results of the search API. To overcome this issue, we manage to perform different search queries by adding a parameter to fetch files with the size inside the indicated range (i.e., \texttt{size:<min>..<max>}). We define the range for each query in such a way that achieves less than 1000 results. Because our query returns an empty list for the repositories using the files greater than 500 \textit{MB}, we divide the whole range between 1 \textit{KB} and 500 \textit{MB} into ranges of 10 \textit{KB}. So, we raise 150,000 search API calls in total, 50,000 for each framework to fetch repositories. 

\begin{table}
\small
\caption{Detailed information about the number of remaining repositories after each filtering out step.}
    \centering
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{p{6cm} r r r}
        \hline
            \multirow{2}{6em}{} & \multicolumn{3}{ c }{ML frameworks} \\
             \cline{2-4}
            & \multicolumn{1}{c}{\textbf{\textit{TensorFlow}}} & \multicolumn{1}{c}{\textbf{\textit{Keras}}} & \multicolumn{1}{c}{\textbf{\textit{PyTorch}}} \\ 
            \hline
            All extracted & 144415  & 98462 & 205071 \\
            % \hline
            After removing unpopular & 10480  & 4108  & 12556\\
            % \hline
            After removing inactive & 4393 & 2989 & 7171\\
            % \hline
            After removing repos with trivial history & 1633  & 738 & 2189 \\
            % \hline
            After removing personal & 1445 & 661  & 1950 \\
            % \hline
            Repos with ML keywords & 156 & 488 & 250\\
            % \hline
            After manual checking top 100 & 12 &  15 & 13\\
            
            \hline
            \hline
            \textbf{Total extracted repos} & \multicolumn{3}{r}{\textbf{447948}} \\
            \textbf{Total remained repos} & \multicolumn{3}{r}{\textbf{4057}} \\
            \hline
            %\textbf{Manually checked repos} & \multicolumn{3}{r}{\textbf{300}} \\
            \textbf{Identified ML-based repos} & \multicolumn{3}{r}{\textbf{40}} \\
            \hline
    \end{tabular}
    % }
        \label{tbl:repo_filter}
        \vspace{-1em}
\end{table}

Next, we conduct four filtering steps based on repositories' metadata to remove personal, inactive, and unpopular repositories, and ones with trivial history (Table~\ref{tbl:repo_filter}). For mining repositories' metadata such as the number of open/closed issues, and the number of commits, we use \textit{GitHub} GraphQL API~\cite{github_GraphQL_API} that outperforms API V3 for extracting such information. In other words, we replace several \textit{GitHub} API V3 calls with one GraphQL API call. We use the following exclusion criteria that were employed successfully in previous studies~\cite{humbatova2020taxonomy,krishna2018connection,hata2021same}: 
\begin{itemize}
    \item \textbf{unpopular repositories}: repositories with less than 10 stars or 10 forks. 
    \item \textbf{personal repositories}: repositories with 1 collaborator.
    \item \textbf{inactive repositories}: repositories without any activity during last year. 
    \item \textbf{repositories with trivial history}: repositories with less than 100 commits.
\end{itemize}

We are left with 4,057 repositories after this step.

\subsection{Manual inspection of repositories}
% \textcolor{blue}{
After collecting repositories and filtering out irrelevant ones, we proceed with manual inspection of them. This is necessary to identify true ML-based systems. In the first step, we selected 30 repositories randomly (10 for each framework) with the highest number of closed issues and PRs similar to the methodology followed by Humbatova et al.~\cite{humbatova2020taxonomy}. We then manually check all 30 repositories to ensure that they are ML-based systems and have at least one ML component. We establish the following exclusion criteria to investigate collected repositories and remove non-ML-based repositories:
% }
% After collecting repositories and filtering out irrelevant ones, we proceed with manual inspection of them. This step is necessary to identify true ML-based systems. 
% % \textcolor{blue}{
% In the first manual checking step, we selected a sample of 100 repositories for each framework. Instead of selecting repositories randomly, the top 100 repositories with the highest number of closed issues and PRs have been selected for each ML framework (the same methodology done by Humbatova et al.~\cite{humbatova2020taxonomy}). 
% % }
% In this step, we manually check all 300 repositories (100 for each framework) to ensure that they are ML-based systems and have at least one ML component. Thus, we establish the following exclusion criteria to investigate collected repositories and remove non-ML-based repositories:
% \textcolor{blue}{
\begin{itemize}
    \item Repositories that use a language other than English to describe their application, such as~\cite{github_repo_chinese},
    \item Tutorial and training repositories which are a set of sample codes or implemented examples of books, such as~\cite{github_repo_training},
    \item Repositories using ML framework, but not for implementing the functionality of components based on ML algorithms (Figure \ref{fig:bug_dl}), such as \cite{github_repo_data_preprocess} that uses \textit{Keras} for data preprocessing,
    \item Repositories using ML frameworks for testing their functionality or providing examples of their application usage, such as~\cite{github_repo_test_func},
    \item Repositories that implement a wrapper for the ML frameworks to extend their functionality, such as~\cite{github_repo_wrapper}.
\end{itemize}
% }
% \textcolor{blue}{
To proceed, the first three authors went through 30 randomly selected repositories and checked them separately, to categorize them into ML-based and non-ML-based, using an open coding procedure~\cite{seaman1999qualitative}. After a meeting to discuss the results, they concluded that only 3 out of the 30 manually checked repositories are ML-based systems. Hence, we added another filtering criterion to decrease the number of false positives. We investigate repositories’ scripts for keywords related to defining, training, and evaluating ML models and exclude repositories without any of such keywords. For instance, in \textit{Keras}, \textit{`Sequential'} and \textit{`Model'} APIs are used to define the model, and \textit{`fit'}, \textit{`compile'}, \textit{`evaluate'}, \textit{`predict'}, \textit{`train\_on\_batch'}, \textit{`test\_on\_batch'}, \textit{`predict\_on\_batch'}, and \textit{`run\_eagerly'} APIs for training and evaluating models~\cite{keras_doc}. We followed the same approach for \textit{TensorFlow} and \textit{PyTorch}, and reported details in our replication package~\cite{Replication-Package}.
By running this filtering step, we retained 894 repositories.
% }

% To this end, the first three authors went through 30 randomly selected repositories and checked them separately, to categorize them into ML-based and non-ML-based, using an open coding procedure~\cite{seaman1999qualitative}. After a meeting to discuss the results, they concluded that only 3 out of the 30 manually checked repositories are ML-based systems. Hence, we added another filtering criterion to decrease the number of false positives. We investigate repositories’ scripts for keywords related to defining, training, and evaluating ML models and exclude repositories without any of such keywords. 
% For instance, in \textit{Keras}, \textit{`Sequential'} and \textit{`Model'} APIs are used to define the model, and \textit{`fit'}, \textit{`compile'}, \textit{`evaluate'}, \textit{`predict'}, \textit{`train\_on\_batch'}, \textit{`test\_on\_batch'}, \textit{`predict\_on\_batch'}, and \textit{`run\_eagerly'} APIs for training and evaluating models~\cite{keras_doc}. We followed the same approach for \textit{TensorFlow} and \textit{PyTorch}, and reported details in our replication package~\cite{Replication-Package}.
% By running this filtering step, we retained 894 repositories. 
% \textcolor{blue}{
We then selected a sample of 100 repositories for each framework, in total 300. The top 100 repositories with the highest number of closed issues and PRs have been selected for each ML framework, similar to the methodology followed by Humbatova et al.~\cite{humbatova2020taxonomy} to be checked manually. To label repositories into ML-based or non-ML-based, the first three authors classified the first 15 repositories of each ML framework (45 in total) independently. To assess inter-rater agreement among them, we used Fleiss’ kappa~\cite{falotico2015fleiss} like in similar works~\cite{yang2022mining,quach2021empirical}, and obtained an inter-rater agreement of about 33\%. Next, the three authors meet to discuss the conflicts and resolve them. Afterward, we repeated the labeling process for 45 repositories and the agreement rate reached 89.9\%, which is acceptable to keep going through the rest of the repositories (Fleiss’ kappa agreement greater than 81\% is interpreted as almost perfect agreement~\cite{hartling2012validity}). So, we proceeded to label the rest of the repositories, and the three authors labeled all 300 repositories and achieved 87.7\% agreement. In the end, 40 repositories were identified as ML-based. The list of repositories is available in our replication package~\cite{Replication-Package}. The list encompasses projects such as AirSim\footnote{\url{https://github.com/microsoft/AirSim}} a simulator for drones and cars, FaceSwap\footnote{\url{https://github.com/deepfakes/faceswap}} a deep learning tool to swap faces on videos or DeepSpeech\footnote{\url{https://github.com/mozilla/DeepSpeech}} a speech-to-text engine.
% }
% Then, we selected the top 100 repositories with the highest number of closed issues for each framework to be checked manually. To label repositories into ML-based or non-ML-based, the first three authors classified the first 15 repositories of each ML framework (45 in total) independently. To assess inter-rater agreement among them, we used Fleiss' kappa~\cite{falotico2015fleiss} like in similar works~\cite{yang2022mining,quach2021empirical}, and obtained an inter-rater agreement of about 33\%. Next, the three authors meet to discuss the conflicts and resolve them. Afterward, we repeated the labeling process for 45 repositories and the agreement rate reached 89.9\%, which is acceptable to keep going through the rest of the repositories (Fleiss' kappa agreement greater than 81\% is interpreted as \emph{almost perfect agreement} \cite{hartling2012validity}). So, we proceeded to label the rest of the repositories, and the three authors labeled all \textbf{300} repositories and achieved \textbf{87.7\%} agreement. In the end, \textbf{40} repositories were identified as ML-based. The list of repositories is available in our replication package \cite{Replication-Package}. The list encompasses projects such as AirSim\footnote{\url{https://github.com/microsoft/AirSim}} a simulator for drones and cars, FaceSwap\footnote{\url{https://github.com/deepfakes/faceswap}} a deep learning tool to swap faces on videos or DeepSpeech\footnote{\url{https://github.com/mozilla/DeepSpeech}} a speech-to-text engine.

% \textcolor{blue}{
It is worth noting that we do not consider any exclusion criterion to filter out any ML algorithm. However, after manual inspection of repositories, all of the remaining repositories are DL-based software systems. Besides, with respect to the fact that we aim at comparing characteristics of bugs in ML-based vs non-ML-based systems in this study, we do not make any distinctions between various types of ML algorithms (CNN, Transformer, RNN, etc.).
% }

\subsection{Manual labeling of bugs}
Since our goal in this study is to characterize bugs in ML-based systems, following the existing work \cite{shen2021comprehensive,nikanjam2022faults}, we mine the 40 identified ML-based repositories and extract closed issues and merged Pull Requests (PR) showing a bug-fix. We choose such PRs because 1) bugs mentioned in such PRs have been already accepted and then got fixed and 2) these PRs usually have more comprehensive information about the fixed bugs (e.g., code changes, links to related issues, and discussions among developers), which facilitates understanding the bugs. To identify PRs with the purpose of bug-fixing from repositories, following the existing study \cite{shen2021comprehensive,garcia2020comprehensive}, we collect PRs whose tags/titles include at least one bug-relevant keywords (i.e., fix, defect, error, bug, issue, mistake, incorrect, fault, and flaw). Similarly for closed issues, as users raise \textit{GitHub} issues for several purposes (e.g. asking questions, enhancement, feature request, and reporting bugs) and assign some tags to represent their goal, closed issues with at least one of the mentioned keywords in tags or titles were extracted.

We initially extracted a total of \textbf{44,342} closed issues and merged PRs. Out of them, we achieved \textbf{8,057} with the bug-relevant keywords. Following previous works \cite{chen2020comprehensive,zhang2019}, to ensure a 95\% confidence level and a 5\% confidence interval, we randomly sample \textbf{367} issues/PRs. With respect to the different number of raised issues/PRs in each ML-based repository of our dataset, we use the Stochastic Universal Sampling (SUS)~\cite{wirsansky2020hands} method to obtain a fair set of sampled artifacts and prevent any bias against the repositories with the small number of issues/PRs. In other words, we extract bugs from each repository, based on the ratio of each repository’s issues/PRs to the total number of extracted ones. We round calculated values to the closest greater integer number, and that left us with \textbf{386} issues/PRs at the end.


In the next step, we inspect the extracted issues/PRs manually to identify their type as \textit{ML} or \textit{non-ML}, their root causes and symptoms following an open coding procedure \cite{seaman1999qualitative}. To this end, we use a two steps manual labeling: 1) to categorize the issues/PRs into ML and non-ML and, 2) to classify the ML ones based on their root causes and symptoms. To categorize the issues/PRs into ML and non-ML, each of the first three authors went independently through the first 40 issues/PRs (almost 10\% of all sampled issues) as a pilot analysis, and we achieved a 44.8\% inter-rater agreement Fleiss' kappa. To identify the main reasons for disagreements and resolve them, two meetings were held and a clear criterion was agreed upon for each class. Generally, we label issues/PRs affecting the quality of ML components as ML bugs and the rest as non-ML bugs. During the manual labeling, several collected closed issues turned out not to be real issues. As an example, some issues are generated automatically by the repository's bot~\cite{github_issue_bot_generate} or closed by it, because nobody replied to the issue~\cite{github_issue_bot_close}. So, we added an additional group as '\textit{not real bug}' referring to the issues which can be classified as neither ML nor non-ML, satisfying one of the following rules:

\begin{itemize}
    \item issues generated automatically by a bot.
    \item issues which are end-user questions (users do not know how to handle the problems) or user's mistakes.
    \item issues that are closed without fixing (not having enough information from the issue report or having a rejected PR).
    \item issues that are not reproducible.
    \item issues that are unclear or without explanation.
\end{itemize}

We also filter out the PRs that are found to be irrelevant to bug fixing (labeled as \textit{no real issue}). Moreover, some PRs fixed multiple bugs, and we, therefore, labeled each of them as an individual bug where applicable, similar to the existing work \cite{garcia2020comprehensive,shen2021comprehensive}.
 
Then we labeled those 40 issues/PRs again and achieved an 89.6\% agreement (interpreted as almost perfect agreement~\cite{hartling2012validity}), which is reasonable to keep labeling the rest of the issues/PRs. To label the remaining issues/PRs, we label every 100 bugs, have a meeting to discuss the results, explore the main reasons for disagreements, and resolve them for the next parts. Eventually, we achieve \textbf{89.7\%} agreement, resulting in \textbf{109} ML bugs (28.2\%), \textbf{119} non-ML bugs (30.8\%) and \textbf{158} not real bugs (40.9\%). Table~\ref{tbl:issues_detailed} illustrates the detailed information regarding manual labeling of issues as ML bugs or non-ML bugs.  

\begin{table}
\footnotesize
\caption{Detailed information about labeled bugs.}
    % \resizebox{\columnwidth}{!}{
    \centering
    \begin{tabular}{p{2cm} r r | r r r}
        \hline
            \textbf{Framework} & \multicolumn{1}{c}{\textbf{\textit{Issue}}} & 
            \textbf{\textit{PR}} &
            \textbf{\textit{ML bug}} &
            \multicolumn{1}{c}{\textbf{\textit{non-ML bug}}} & 
            \multicolumn{1}{c}{\textbf{\textit{Not real bug}}}\\
            \hline
            % \hline
            TensorFlow & 88 & 32 & 18 & 41 & 61 \\
            % \hline
            Keras & 76 & 11  & 8 & 46 & 33\\
            % \hline
            PyTorch & 118  & 61 & 83  & 32 & 64\\
            \hline
            \textbf{Total} & \textbf{282}  & \textbf{104} & \textbf{109} & \textbf{119} & \textbf{158}\\
        \hline
    \end{tabular}
    % }
        \label{tbl:issues_detailed}
        \vspace{-2em}
\end{table}

Then, we carry out the second part of bug labeling to indicate their \textbf{root cause} and \textbf{symptoms}. Since many of our selected repositories used DL models, we employ the recent classification of root causes and symptoms of bugs in DL systems introduced in \cite{islam2019comprehensive} which extended a former study \cite{zhang2018empirical}. For root causes, we have: \textit{absence of inter API compatibility} (AIAPIC), \textit{absence of type checking} (AOTC), \textit{API change} (APIC), \textit{API misuse} (APIM), \textit{confusion with computation model} (CWCM), \textit{incorrect model parameter or structure} (IMPS), \textit{structure inefficiency} (SI), \textit{unaligned tensor} (UT), and \textit{wrong documentation} (WD). 
% \textcolor{blue}{
Table~\ref{tab:bug_root_cause} represents the detailed description of root causes of ML bugs in ML-based systems.
% }
The types of symptoms are as follows: \textit{bad performance}, \textit{crash}, \textit{data corruption}, \textit{hang}, \textit{incorrect functionality}, and \textit{memory out of bound}. We mainly rely on the definitions presented in their original sources \cite{zhang2018empirical,islam2019comprehensive}.
To label the bugs, we first selected 20 ML bugs out of the 109 ML bugs and labeled them as a pilot step, achieving 30\% and 84.9\% agreements based on Fleiss' kappa for root causes and symptoms, respectively. We again meet two times to identify the major reasons for disagreements and reach a consensus on the labels' concepts and definitions. Then, labeling the first 20 ML bugs again resulted in an 86.5\% inter-rater agreement for root causes and 89.7\% for symptoms. For the rest of the bugs, we inspected them in three rounds. In each round, we label around 1/3 of the bugs, then meet to discuss the disagreements and resolve them. In the end, we achieved \textbf{88.4\%} and \textbf{97.2\%} agreement for root causes and symptoms of ML bugs.

\begin{table}[]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{p{3cm} p{10cm}}
    \hline
     \rowcolor{gray}
     \textbf{Bug root cause}& \textbf{Description} \\
     \hline
     Absence of Inter API Compatibility (AIAPIC) & Inconsistency of two different types of libraries. For example, issue!\cite{issue_sample_08} emerges from a problem in using \textit{`pytest'} in the application developed using PyTorch.\\
     \rowcolor{lightgray}
     Absence of Type Checking (AOTC) & Data type mismatch in calling API methods. As an example, issue~\cite{issue_sample_09} stems from the absence of type checking of “trainer.logger” in different places. \\
     API Change (APIC) & Releasing new versions of ML frameworks that their APIs are not compatible with their previous versions. Issue~\cite{issue_sample_10} that is raised because of API changes in versions 1.2 and the older ones of pytorch lightning is considered as an example of APIC. \\
     \rowcolor{lightgray}
     API Misuse (APIM) & Trying to use an API developed by an ML framework, without a sound understanding about it. \\
     Confusion with Computation Model (CWCM) & Confusion about the function of an API of the ML framework leading to the misuse of the assumed computation model by the ML framework. \\
     \rowcolor{lightgray}
     Incorrect Model Parameter or Structure (IMPS) & Constructing the ML model such as creating an ML model with  incorrect structure or using inappropriate parameters. For instance, issue~\cite{issue_sample_11} raises because of ignoring parameters’ default value of the early stopping method. \\
     Structure Inefficiency (SI) & Problems in the modeling step of ML component/software. Although this category is similar to IMPS, their symptoms are different. In fact, IMPS leads to system crashes, however SI results in system bad performance. Issue~\cite{issue_sample_12} is an example that its root cause is using incorrect training parameters resulting in system bad performance. \\
     \rowcolor{lightgray}
     Unaligned Tensor (UT) & Trying to build a computation graph in an ML process without providing input data satisfying input specification of the ML framework API. For instance, issue~\cite{issue_sample_13} stems from the shape mismatch of the intermediate tensors. \\
     Wrong Documentation (WD) & Incorrect information presented in the formal documentation of the library. As an example, issue~\cite{issue_sample_14} is related to the unclear explanation in the document of the lithening\-AI library for using early stopping API. \\
     \hline
    \end{tabular}
    }
    \caption{
    % \textcolor{blue}{
    The identified root causes of ML bugs in open-source ML-based software systems.
    }
    % }
    \label{tab:bug_root_cause}
\end{table}

\section{Empirical Results}
\label{sec:result}
In this section, we provide and discuss empirical results for each RQ. The materials and all collected data used for this study are publicly available in our replication package \cite{Replication-Package}. 



\subsection{RQ1: Distribution of bugs in ML-based Systems}\label{sec:rq1}

% \textcolor{blue}{
Our goal in RQ1 is to evaluate the distribution of ML and non-ML bugs in ML-based software systems. Results are presented in Figure~\ref{fig:dist}. Manual labeling of a selected sample (with a 95\% confidence interval) of potential bugs reveals that 40.9\% of them are not real bugs. This finding suggests that almost half of the issues/PRs reported in the collected repositories are not real bugs. Some of these issues are identified as users' questions/mistakes while others lack sufficient description to be considered as a bug (e.g., not reproducible or have unclear description/not enough information). This finding aligns with a previous research conducted by Anvik et al.~\cite{10.1145/1117696.1117704} which found that only 58\% of the issues raised for Eclipse and 44\% of the issues raised for Firefox have been either fixed or considered for fixing. On the other hand, Long et al.~\cite{Long22} carried out research on the performance and accuracy bugs within ML frameworks (e.g. TensorFlow, Keras, PyTorch, etc) and reported only 3\% of the raised issues as bugs without sufficient information or non-reproducible. This discrepancy can be explained by the fact that our study encompasses a broader range of bug types in ML-based systems, beyond just accuracy and performance issues, which seem to be more easily identifiable (and likely fixed) than other types of bugs. Furthermore, we focus on the bugs within ML-based systems rather than inside the ML framework itself. Moreover, it is worth noting that localizing and identifying the root causes of bugs inside ML-based systems are likely more challenging than bugs inside ML frameworks, mainly due to the utilization of various libraries in the implementation of ML-based systems. Among the remaining bugs, our results demonstrate that 47.8\% of the real bugs in ML-based systems are dealing with ML components (Figure \ref{fig:bug_distribution}). Although ML components of software take just about 10\% of development time~\cite{menzies2019five}, it represents roughly half of the real issues encountered in a typical ML-based system. These findings emphasize the critical need for automatic testing tools for ML components and ML-based software systems.
% }



% Our goal in RQ1 is to evaluate the distribution of ML and non-ML bugs in ML-based software systems. Results are presented in Figure~\ref{fig:dist}. Manual labeling of the selected potential bugs revealed that \textbf{40.9\%} of them are not real bugs, with a 95\% confidence interval. In this way, we can conclude that almost half of the issues/PRs of the collected repositories are \emph{\textit{not real bugs}}. Some of them are identified as users' questions/mistakes while others have too little information to be considered as a bug (e.g., not reproducible or with unclear description/not enough information). This finding is in accordance with previous research conducted by Anvik et al.~\cite{anvik2005coping} mentioned that only 58\% of the raised issues for Eclipse and 44\% for Firefox have been fixed or are to be fixed. On the other hand, this observation is in contrast with results reported by~\cite{Long22} about small proportions of insufficient information in accuracy/performance bug reports inside DL frameworks, as a branch of ML. This can be explained by the fact that we did not limit our study to only accuracy/performance bugs which seem to be more easily identified or described (and likely fixed) than other types of bugs. Also, we considered bugs inside ML-based systems rather than the framework itself (which are more likely to be hard to root out because of extra nested libraries). Out of all the remaining bugs, the results show that 47.8\% of the "real" problems in ML-based systems are dealing with ML components (Figure \ref{fig:bug_distribution}). Although ML components of software take just about 10\% of development time~\cite{menzies2019five}, it represents roughly half of the real issues that a typical ML-based system will face. This actually emphasizes the need for automatic testing tools for ML components and ML-based software systems.


\begin{tcolorbox}
\textbf{Finding 1. }A large number of issues in ML-based systems either turn out to be users' questions/mistakes or did not provide enough information to decide whether it is a bug or to help fix it. Moreover, although almost 10\% of development time is consumed for ML components, nearly half of the "real" bugs in ML-based systems are related to ML components. 
\end{tcolorbox}

% Figure environment removed

As mentioned in Section \ref{sec:methodology}, to classify the root cause and symptom of our bugs, we employed the categories introduced by Islam et al. ~\cite{islam2019comprehensive}. Figure \ref{fig:bug_rootCause} presents the indicated root causes. The most important observation is that 72.5\% of the identified ML bugs could not be classified in any of the 9 root causes defined in \cite{islam2019comprehensive}, so we classified them as \textit{Other} (we do not report this category in Figure \ref{fig:bug_rootCause}). The reason is that the defined category focused on root causes related to APIs, models (structure and parameter), and documentation. Therefore, for further investigation of root causes, we went through the bugs categorized as \textit{Other} using the 11 labels provided by \cite{jia2021symptoms} for root causes of bugs in DL libraries. We followed a similar methodology to what is done initially for root causes. We found the following results: Dimension mismatch (1.3\%), Processing (15.2\%), Algorithm (15.2\%), Corner case (2.5\%), Logic error (36.7\%), Configuration error (19.0\%), Concurrency (10.1\%) while we found no occurrence (0\%) for Type confusion, Inconsistency, Referenced types error, and Memory. We refer the reader to \cite{jia2021symptoms} for a full description of the categories. The root cause of the majority of bugs is identified as Logic error, as it covers bugs that occur in the logic of  the code like incorrect program flow or wrong order of actions. This also includes bugs that lead to wrong calculations of gradients. Since ML code is highly dependent on configurations (like hyperparameters), the next largest portion of root causes is  Configuration error which represents bugs caused by wrong configurations. However, both classifications overlooked problems raised during training and using trained models. It looks like that, by increasing the development and usage of ML-based software systems, we need a new category of bug’s root causes for such systems.

%In this way, they ignored problems raised during training and using trained models: for example, there is no root cause that covers wrong hyperparameter settings \Amin{can't we continue like this by mention major categories we found?}. In other words, their classification identifies root causes mainly associated with early stages of ML-based systems life cycle and missed root causes of issues that happen in systems with non-trivial history (advanced training and deployment). Therefore, the introduced classification of root causes by Islam et al. ~\cite{islam2019comprehensive} has become stale now. It looks that by increasing development and usage of ML-based software systems, we need a new category of bug’s root causes for such systems.

Categorizing bugs based on their symptoms, as reported in Figure \ref{fig:bug_symptoms}, shows that crash with 52.2\% is the most prominent symptom which is in accordance with previous studies \cite{islam2019comprehensive}. However, the ratio of `incorrect functionality' rises to 39.4\% compared to 12\% in the previous report \cite{islam2019comprehensive}. The reason can be an increase in the application of ML components in software systems from 2019 (the previous study \cite{islam2019comprehensive}) to 2022.

% \textcolor{blue}{
Then, we categorized ML bugs based on the taxonomy of faults in DL systems introduced by Humbatova et al.~\cite{humbatova2020taxonomy}. Their taxonomy consists of 5 top-level categories that we used to label ML bugs: “Tensors\&Inputs”, “Model”, “Training”, “API”, and “GPU Usage”. The results are reported in Figure~\ref{fig:ml_bug_taxonomy}. Overall, the “Training” category is the most frequent type of bug with 52.7\% which is similar to the original study~\cite{humbatova2020taxonomy} where authors reported 52.5\% of issues related to the training. It also echoes~\cite{Long22} where accuracy/performance bugs are mainly concerned with the training phase (although it concerns bugs inside DL frameworks). The next prevalent type of bug is Tensors\&Inputs which deals with problems related to the wrong shape and type/format of the data. Similar to root causes, here, we encountered 9 bugs that did not fit into any of the 5 categories. So, we have added the ‘other’ category to cover them. These bugs include issues happening during the loading/saving of a trained model, deployment of the trained model, model inference, checkpoints, and monitoring of a trained model. Figure~\ref{fig:ml_bug_other} shows the distribution of bugs in the “other” category. The “loading/saving of a trained model” category refers to the issues which may occur when developers try to save a trained model to be able to use it in another program or load a saved model which is trained in the previous steps (e.g., issue~\cite{issue_sample_04}). Model deployment issues are related to the possible problems in deploying a trained ML model on a specific platform.  For instance, issue~\cite{issue_sample_05} tries to fix an architectural problem of the model to be able to deploy on ONNX. Problems categorized as inference issues refer to the errors occurring in the prediction step of the ML component. Issue~\cite{issue_sample_06} is an example of an inference issue explaining a problem in the implementation of the prediction step of the trained model. Checkpoint issues are explained as the problems that developers faced while trying to save the ML model during its training process to be able to continue the training process from checkpoints, in case of any crash/stop in the model training process. Issue~\cite{issue_sample_07} is considered an example of the model checkpoint issue. Monitoring issues are related to the problems of monitoring a deployed model. 
% }

% Figure environment removed

% Then, we categorized ML bugs based on the taxonomy of faults in DL systems introduced by Humbatova et al.~\cite{humbatova2020taxonomy}. Their taxonomy consists of 5 top-level categories that we used to label ML bugs: \textit{Tensors\&Inputs}, \textit{Model}, \textit{Training}, \textit{API}, and \textit{GPU Usage}. The results are reported in Figure \ref{fig:taxonomy}. Overall, \textit{Training} category is the most frequent type of bug with 52.7\% which is similar to the original study \cite{humbatova2020taxonomy} where authors reported 52.5\% of issues related to the training. It also echoes~\cite{Long22} where accuracy/performance bugs are mainly concerned with the training phase (although it concerns bugs inside DL frameworks). The next prevalent type of bug is \textit{Tensors\&Inputs} which deals with problems related to the wrong shape and type/format of the data. 
% % Similar to root causes, here, we encountered bugs that did not fit into any of the 5 categories. So, we added the \textit{Other} category containing the bugs that could not be categorized based on the provided taxonomy's classes. 
% % \textcolor{blue}{
% % Similar to root causes of bugs, 
% Similar to root causes, here, we encountered 9 bugs that did not fit into any of the 5 categories. So, we have added the \textit{Other} category to cover them. These bugs include issues happening during the loading/saving of a trained model, deployment of the trained model, model inference, checkpoints, and monitoring of a trained model. Hence, the community may need to revise the existing taxonomy of bug types to cover issues related to all stages of development, deployment, and maintenance of ML-based software systems, not only pre-processing, training, and testing.
% that could not be categorized based on the provided taxonomy's classes. Bugs in this category is 
% }
% such as bugs related to the load/save of the trained models. 
% \textcolor{blue}{
% To label bugs that fall into \textit{Other} category, we use an open coding methodology using an online document (to support accessing all raters anytime) created to classify the bugs and solve possible conflicts among raters. Each rater labels the issues independently by descriptive labels. In the next step, raters have a meeting to review created labels without considering the issues and reach a consensus on the final list of labels. Next, they checked their labeled issues and revise the labels based on the finalized list of labels. The agreement level after labeling was XXX. 
% }

% \textcolor{blue}{
% Description of newly created categories. 
% }

% Figure \ref{fig:taxonomy_other} illustrates such types of bugs and their ratio. 



\begin{tcolorbox}
\textbf{Finding 2. } Because of the increasing usage of ML in different domains and the increasing maturity of ML-based software systems over time, we need to revise existing ML bug classifications and add new possible categories to be able to classify root causes/symptoms of ML bugs. Moreover, most of the issues of ML components seem to originate from the training phase.
\end{tcolorbox}

%\begin{table}
%\caption{The best accuracy of models in predicting types of bugs in ML-based systems (using 10-fold cross validation).}
%    \centering
%    \resizebox{\columnwidth}{!}{
%    \begin{tabular}{p{1cm} r r l}
%        \hline
%            \multirow{2}{*}{\textbf{Model}} & 
%           \multicolumn{2}{c}{\textbf{Accuracy}} &
%            \multirow{2}{*}{\textbf{ Features}} \\
%            \cline{2-3}
%            & \textbf{Mean} & \textbf{Std} & \\
%            \hline\hline
%            \multirow{1}*{RF} & 72.68 & 10.79  & 
%            Issue title, Issue message, Issue comments
%            \\
%            \hline
%            SVM & 73.58 & 9.05 & 
%            Issue title, Issue message, Issue comments
%            \\
%            \hline
%            NB & 75.30 & 9.26 & 
%            Issue title, Issue message 
%            \\
%        \hline
%    \end{tabular}
%    }
%        \label{tbl:bug_prediction_models}
%\vspace{-1em}
%\end{table}

% \textbf{Bug Prediction in ML-based Systems.} To investigate the possibility of predicting different types of bugs in ML-based systems, we train three different ML models including Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). We used our 386 labeled bugs as the training dataset. The input features of the models include issue title, issue message, issue comments and the result of their sentiment analysis. We use \texttt{Sentiment\-Intensity\-Analyzer} of \texttt{nltk 3.7} as sentiment analyzer and \texttt{scikit-learn 1.0.2} for three mentioned models with default parameters. Features were chosen to match what reviewers did to decide whether it was an ML bug or not (in a more simplistic fashion). Each model was tasked with predicting one of the two classes (ML bug and  non-ML bug). Text features are first processed using classical natural language preprocessing (stopwords, ...) and then a Bag-Of-Word model is applied. A $10$-fold cross-validation is used to train the models with various combinations of features in order to evaluate the impact of training data on the performance of the system.
 
% Results show that all models have an average of $70$\% of accuracy, with NB model slightly outperforming the two other models (Table.~\ref{tbl:bug_prediction_models}). However, the high standard deviation over our cross-validation shows that the model's performance is highly dependent on the training data used. There is little surprise given the small samples of data accessible, as well as the possible not discriminative enough features. Since the goal of this RQ was to evaluate empirically the distribution of bugs in ML-based systems, we chose to use simple models with the data we manually labeled to extract some preliminary results from simple baseline models. We leave to future work deriving a bigger dataset along with more complex models. 
% % Figure environment removed



% % Figure environment removed

% % Figure environment removed


% % Figure environment removed

\subsection{RQ2: Complexity of bug fixes in ML-based systems}\label{sec:rq2}
To compare the complexity of bug fixes for different types of bugs in ML-based systems, we used metrics that are utilized successfully by previous studies~\cite{kononenko2018studying,chaturvedi2014predicting} including the number of commits to fix the bug, number of changed files, number of changed Lines of Code (LOC), entropy of changes, number of comments on the issue, number of comments on the bug-fix related to the issue, number of users who discussed the issue, and number of users who discussed and collaborated on bug-fix. Figure~\ref{fig:fix_complexity_level} reports the results. %\Amin{Are there specific metrics for ML components to measure their complexity that are not captured by the studied metrics? I'm afraid no!}

% \textcolor{blue}{
To ensure a statistically significant difference between metrics for ML and non-ML bugs, we also run statistical tests. As we make no assumptions on the distribution of data and since the variance of data is different, “Mann–Whitney U test” is the recommended test~\cite{arcuri2014hitchhiker}. We used the two-sided test of the \textit{Mann–Whitney U} test which tests the null hypothesis that the probability of a given metric of ML bugs being greater than non-ML bugs is equal to the probability of a given metric of non-ML bugs being greater than ML bugs. Finally, as reporting simply p-value might be misleading~\cite{Kampenes07}, we also computed effect size to assess the magnitude of the differences~\cite{Cliff93}. Moreover, “Cliff delta” test is a recommended test for calculating effect size for data with non-normal distribution~\cite{macbeth2011cliff}. For both tests, we used their official implementation in the R statistical environment (with “effsize” library for Cliff's delta). Results are presented in Table~\ref{tbl:statistical_test}.
% }
 
% To ensure a statistically significant difference between metrics for ML and non-ML bugs, we also run statistical tests. Since we do not assume our data to be distributed normally, we selected \textit{Mann–Whitney U} test~\cite{arcuri2014hitchhiker}. Reporting simply $p$-value might be misleading~\cite{Kampenes07}, so we also tested \textit{effect size} using \textit{Cliff's delta} test~\cite{Cliff93} to add \textit{practical} significance on top of statistical one. For both tests, we used their official implementation in the R statistical environment\footnote{\url{https://www.r-project.org/}} (with \texttt{effsize} library for Cliff's delta). Results are presented in Table \ref{tbl:statistical_test}. 

\begin{table*}
\centering
\caption{Detailed result of \textit{Mann–Whitney U} and \textit{Cliff's delta} test.}
%\begin{adjustbox}{max width=\textwidth}
\begin{threeparttable}
   \begin{tabular}{p{5cm} r r}
        \hline
            \textbf{Metric} & \multicolumn{1}{c}{\textbf{\textit{p-value}}} & 
            \textbf{\textit{effect size (d)}}\\
            \hline
             \# commits & \textbf{$<$ 1e-3} & \textbf{0.337}\tnote{M}\\
             \# changed files & 0.080 & 0.133\tnote{N}\\
             \# changed LOC & \textbf{0.033} & \textbf{0.164}\tnote{S}\\
             \# comments on issue & 0.347 & -0.072\tnote{N}\\
             \# comments on fix & 0.441 & -0.058\tnote{N}\\
             \# collaborators on issue & 0.904 & -0.009\tnote{N}\\
             \# collaborator on fix & \textbf{$<$ 1e-3} & \textbf{0.272}\tnote{S}\\
             \# comment/collaborator on issue & 0.086 & -0.131\tnote{N}\\
             \# comment/collaborator on fix & \textbf{0.014} & \textbf{-0.192}\tnote{S}\\
             entropy & \textbf{0.008} & \textbf{0.204}\tnote{S}\\
        \hline
    \end{tabular}
    \begin{tablenotes}
       \item [M] medium effect 
       \item [S] small effect
       \item [N] negligible effect
     \end{tablenotes}
\end{threeparttable}
%\end{adjustbox}
\label{tbl:statistical_test}
\end{table*}

% Figure environment removed

As we can see in Table \ref{tbl:statistical_test}, regarding $p$-value, using the widely accepted $0.05$ significance threshold, the results show that differences in the number of commits, number of changed LOC, entropy of changes, number of collaboration on fix, 
% \textcolor{blue}{
and number of comment/collaborator on fixes
% } 
are significant while the number of changed files, number of collaborators on issues, number of comments on both issues and fixes, 
% \textcolor{blue}{
and number of comment/collaborator on issues
% } 
are not significant. To interpret the effect size, we use the traditional scale associated with Cliff's delta~\cite{Romano06} that is $\lvert d \rvert = 0.147$ is small, $\lvert d \rvert = 0.33$ is medium and $\lvert d \rvert = 0.474$ is large. All \textit{statistical} significant tests previously described are also \textit{practically} significant with a \textit{small} effect size, except \textit{number of commits} which results in a \textit{medium} effect size. Moreover, the number of commits, changed LOC, number of collaborators on a fix and entropy features have a positive effect size which indicates that this feature is more prevalent in ML bugs than non-ML bugs. 
% \textcolor{blue}{
Besides, the negative effect size of the number of comments/collaborators signifies that this feature tends to be higher in non-ML bugs compared to ML ones.
% dominates it for ML bugs. 
% }
According to obtained results, ML bugs tend to require more commits as well as more changed LOC compared to non-ML bugs. They also require more collaborators to be fixed as well. Yet, the presence of numerous outliers in commits for non-ML bugs (Figure~\ref{fig:commits}) shows that some non-ML bugs can prove harder to address. Regarding the number of changed LOC, there seem to be multiple outliers in both cases (Figure~\ref{fig:LOC}), which can imply that this feature is independent of the nature of the bug, and is basically bug-dependent, although there is a significant difference.

% \textcolor{blue}{
To give a picture of the number of comments on issues and fixes, we provide the average number of comments per collaborator (Figures~\ref{fig:issue_comment_per_collab} and \ref{fig:fix_comment_per_collab}). Results reveal that collaborators put more comments on average on non-ML issues, compared to ML ones. As Table~\ref{tbl:statistical_test} represents, the difference between the number of comments/collaborators on issues in ML and non-ML bugs is not significant, but it is significant for fixes with small effect size ( $\lvert d \rvert = -0.192$).
% \Amin{mention the value!}. 
One possible reason behind this discrepancy may be the fact that developers who put comments on ML bugs collaborate passively. In other words, collaborators just put a few comments on fixes and then stop collaborating on fixes actively, which may be because of their limited knowledge of ML. 
For instance, there are 8 different collaborators on a PR (\href{https://github.com/Lightning-AI/lightning/pull/4309}{\#4309}) of \textit{lightning-AI} project which is the fix of an issue (\href{https://github.com/Lightning-AI/lightning/issues/3660}{\#3660}). However, according to activities, only 2-3 of them are collaborating on fixing the issue. 
%\Foutse{can we give an example of such a phenomenon? it will make the argument stronger if you can quote some comments here! }
% \Amin{are these findings consistent with what we mention later?}
% } 
% \Mehdi{in the previous subsections, we just mentioned that ML-bugs need more collaborators!}\Amin{ok, just to make sure that we are consistent all over}
 
For the number of issue's collaborators (Figure~\ref{fig:issue_collab}), there is no statistical difference in our case which signifies that while ML bugs require more developers to be fixed (Figure~\ref{fig:fix_collab}), there is a similar number of collaborators discussing on the issue. As such, ML bugs tend not to require significantly more collaborators (experts) to be identified and diagnosed in ML-based systems. Regarding the number of comments on different issues (Figure~\ref{fig:issue_comment} and Figure~\ref{fig:fix_comment}), while not being statistically significant, the distribution of data reveals that the number of comments in non-ML bugs is on average higher than ML ones. The reason behind it can be that a higher number of developers are more skilled in general programming than in ML. However, there seems to be a high number of outliers, which means that while on average ML bugs receive fewer comments than non-ML ones, some bugs require extensive discussions, potentially harder to pinpoint the root cause of the problem for instance.
Finally, ML bugs tend to significantly have higher entropy than non-ML bugs (Figure~\ref{fig:entropy}).
% \Amin{@Mehdi: I revised. have a look please!}
%, since fewer files \Foutse{the entropy typically captures the spread of changed lines across files...no?, so having few files could affect entropy but not always...}are changed in ML bugs compared to non-ML ones.
This means, in general, ML bugs happen in some specific files whereas non-ML bugs tend to be more scattered across the project’s files. 

%\Florian{About LDA, missing citations and some more explanations, I don't remember everything Mehdi did. Plus, we use the PR for the LDA, but the reviewer was talking about commits, so not exactly the same.}
% \textcolor{blue}{
% To understand the correlation among the studied metrics, we used the \textit{spearman} method from \textit{pandas} Python library. Results represent that entropy (considered as a candidate to show the complexity of changes~\cite{chaturvedi2014predicting}) has a very strong positive correlation~\cite{schober2018correlation} with the number of changed files (with 0.96), number of commits (with 0.86), and number of changed LOC (with 0.85) as one could expect.
% % \Amin{what do you mean by implicitly?} \Mehdi{I meant it is as usual and does not need more explanation}\Amin{so, obvious?}. 
% Besides, entropy has a moderate positive correlation with the number of people who collaborate on the bug-fix discussion, which could also be expected because of the fact that entropy is calculated based on the fix's changes. \Foutse{you mean that the different people who collaborated may have been editing different files?} Other correlations are either negligible (such as the correlation between the number of commits and the number of comments on the issue measured as 0.11), or as expected 
% % \Florian{I would avoid saying something is "obviously understandable". Maybe understandable on its own is enough}
% (e.g. correlation between the number of commits and the number of changed LOC calculated as 0.78 \Foutse{why is this an expected correlation actually?}). \Foutse{then so what? what do you conclude from this correlation analysis?}
% % With respect to the fact that entropy is calculated based on fix's changes, 
% }
To further investigate why ML bugs need more change iterations (i.e., more commits), we performed a thematic analysis of the bug-fixing PRs 
% \Foutse{did you define this acronym?} 
using Latent Dirichlet Allocation (LDA) from the Gensim library \cite{lda-gensim}. We removed any kind of text known as PR template (repetitive text in all PRs)
% \Amin{irrelevant text to the content of the PR} \Foutse{template?} 
which could bias the model in our 109 ML bugs' PRs. We then applied LDA for topic modeling based on Mallet \cite{mallet} to obtain the relevant number of topics. The algorithm returned the best coherence score (in parenthesis) for a number of topics of 7 (0.6326), 26 (0.6297), and 20 (0.6218). We chose the case of the 7 topics, as it yielded the best score and a higher number of topics might mean the model started grouping issues based on the application rather than the proper cause of the issue (for instance, in the 20 topics proposed, one of them is based on the keywords \enquote{state}, \enquote{action}, \enquote{intent} or \enquote{slot} which are keywords heavily used in one of the applications we collected). Reviewers then analyzed the keywords of the 7 topics and the issues labeled with the given topic to come up with topic labels. The final topics are: Test related (15.8\%), Requirements and Dependencies (19.3\%), Training Accelerator and Model checkpoint (18.4\%), Scripting Bugs (14.9\%), Model Bugs (16.7\%), Training Bugs (7.9\%), Documentation (7.0\%). The topics that are the most represented are commits linked to Requirements and Dependencies and Training Accelerator and Model checkpoint with over \textbf{18\%} each, while the least represented topics are the Documentation and Training bugs with around \textbf{7\%}. Interestingly, the largest part of the issues for ML bugs do not relate directly to ML (Requirements/Dependencies), nonetheless, such bugs are expected in ML systems as they often rely on multiple libraries and frameworks to work.

\begin{tcolorbox}
\textbf{Finding 3.} ML bugs have significantly different characteristics from non-ML ones, in ML-based systems: they need more commits, a higher number of LOC, and more collaborators involved. Analysis of the ML-related commits shows bugs based on Requirements/Dependencies are the most widespread while Documentation is the least represented.
\end{tcolorbox}


\subsection{RQ3: Needed resources for fixing bugs in ML-based systems}\label{sec:rq3}

% \Foutse{did you check bug re-opening rates? very often some bugs are closed as fixed, and then re-opened later...this is also used as a proxy to capture the challenge posed by the bug!!!} 
% \Foutse{you could also check if there is explicit co-review activities when ML bugs are fixed, vs other bugs...and who reviews the patches!}
To measure the number of resources spent for fixing bugs in ML-based systems, we consider two metrics: time-to-fix and expertise level of developers who fix the bugs (which were used in previous studies as well~\cite{kononenko2018studying,romano2021empirical}). %Results are reported in Figure~\ref{fig:expertise_level}. 
To indicate the time-to-fix, we used the time range between the issue opening and closing date~\cite{bosu2014impact}. For the expertise level of developers, we consider the number of commits done in all other repositories by the developer who fixed the bug, exactly before fixing each issue of our dataset. Therefore, in our list, some users appeared multiple times with different numbers of commits depending on the date of the bug-fix. We kept only the highest expertise level observed (that is, the latest update of the number of commits). We added all users gathered that way whether they only were present on ML, non-ML, or both types of bugs in our dataset.
% or on both. 
Similar to RQ2 (Section \ref{sec:rq2}), we use \textit{Mann-Whitney U} and \textit{Cliff's delta} test. %We present the results in Table \ref{tbl:statistical_test_2}.


Our results reveal that fixing time is not statistically significant ($p$-value = 0.423, d = 0.061 which is negligible), that is fixing ML bugs requires no more time compared to non-ML ones. %(Figure~\ref{fig:fix_duration}). 
This does not necessarily mean that ML bugs are as "easy to fix” as non-ML ones since other metrics we calculated such as the number of commits or the number of collaborators working on a fix were significantly higher for ML bugs. As we focused on repositories containing ML components, this non-significant difference might mean that developers are experts in solving both types of issues, so all issues take almost the same amount of time. This observation can be further emphasized when comparing the expertise level of users.

In all cases, the tests are not statistically significant which means that the number of prior commits of developers working on ML or non-ML issues is similar. This reflects on the observation made previously: as developers working in those repositories are used to both ML and non-ML issues, they are likely to be experts at the same level, even if in our dataset we flagged them as working on only one type of bug. Yet, some users worked on both types of issues. To assess the impact they have, we removed them from ML/non-ML issues groups and retested our data again, which resulted in still no significance even if the metric decreased. This means that users having some expertise in both types of bugs, in our dataset, do not change the outcome, implying that other users working on a single type of issue take as much time as the other group. Similarly, removing outliers (that is, users that have an overly high number of commits) does not affect the results. 

Finding 3 seems to suggest that ML bugs are more complex in terms of bug fixing than non-ML bugs. Nonetheless, the fact that we find no evidence of statistically different expertise levels or time-to-fix between ML and non-ML bugs in this study would seem to contradict Finding 3. As we do not have access to the \textit{effective time} spent on the fix, it can be the case that ML bugs were given priority and the actual time spent on fixing the non-ML issues is lower than the time-to-fix we have access to (date of fix and date of beginning). 
% As such, based on our results, we can only say there seems not to be any statistical difference between ML and non-ML on expertise levels and time-to-fix. \Foutse{i don't get the purpose of this sentence...are we trying to say that using that metric is a bad idea?} 
While there might be some threats to the validity of considered criteria used to measure needed resources for fixing ML and non-ML bugs (expertise level of developers and time-to-fix),
% \Foutse{which criteria?}, at least time-to-fix,
% might be biased because of how it is defined \Foutse{what kind of bias exactly?}, 
no other information was available to compute such a metric.

% \textcolor{blue}{
% \Foutse{what is the motivation for doing this correlation analysis? we need to first explain why it is important to do it} 
We examined the correlation between entropy and time-to-fix using the \textit{Spearman} correlation method from \textit{pandas} Python library.
%approach to calculate correlation among metrics employed to measure the needed resources 
% \Amin{resource?} \Mehdi{I don't understand what your comment is about}\Amin{like, is time-to-fix a resource?} 
%(time-to-fix and expertise level of users) and other metrics....
% and 
Results reveal that time-to-fix has a moderate positive correlation (based on correlation formal definition~\cite{schober2018correlation}) with entropy (0.42). With respect to the fact that entropy is considered a measure to show the complexity of changes~\cite{chaturvedi2014predicting}, it can imply that time-to-fix is a good candidate to show the needed effort for fixing issues. 
% It is also worth mentioning the correlations between the expertise level of users who fix issues and other criteria are negligible ($<0.1$). 
% \Foutse{then so what? what do you conclude from this correlation analysis?}
% }

% \textcolor{blue}{
Because of the negative impact of bug reopening on software quality \cite{tagra2022revisiting}, we examined the reopening rate of ML and non-ML bugs from the studied system. We aim to understand if developers experience higher bug reopening rates when dealing with ML bugs than they do for non-ML bugs. Results show that the reopening rate of ML issues (5.45\%) is a bit higher than that of non-ML bugs (4.2\%). We assessed the statistical significance of this result using the \textit{Chi-squared} test. We leveraged the \textit{scipy} Python library to run this test and obtained a $p-$value of $0.894$. Meaning that there is no significant difference between the reopening rate of ML and non-ML bugs. Besides, the rate of issue reopening in ML-based systems with 4.8\% is a bit less than the one reported for traditional software systems
% \Amin{non-ML or general/traditional systems?} systems 
(between 6\% and 10\%) by Zimmermann et al. ~\cite{zimmermann2012characterizing}.
% }

% \textcolor{blue}{
Considering the major impact of code reviews on the software quality~\cite{bosu2016process}, we examined the rate of developers' invitations to review PRs fixing ML and non-ML bugs. We found that in 49.05\% of ML bugs, developers were invited to review PRs compared to only  23.36\% for non-ML-bugs. This finding suggests that higher attention is given to the quality of bug-fix changes when dealing with ML bugs. We explain this result by the fact that fixing ML bugs requires ML knowledge which may not be mastered by all of the developers on the team. We also investigated the number of PRs' reviewers in ML and non-ML issues since it has a positive correlation with the time-to-fix~\cite{maddila2019predicting}. As is shown in Figure~\ref{fig:pr_review}, the number of developers who reviewed bug-fixing PRs for ML bugs is higher than the number of developers who reviewed bug-fixing PRs for non-ML bugs. Furthermore, results of \textit{Mann–Whitney U} and \textit{Cliff's delta} tests (similar to \ref{sec:rq2}) indicate that the difference between the number of reviewers involved in fixing ML and non-ML issues is significant ($p$-value $<$ 1e-6, d = 0.378 which is interpreted as medium effect size).
% }

% \textcolor{blue}{
Moreover, we checked the number of bug-fix PRs merged without review in our collected bugs. We found that 37.8\% of merging bug-fix PRs for non-ML bugs have been accomplished without conducting any review compared to 13.7\% for ML bugs. This lower number of non-reviewed bug-fixing PRs may suggest that development teams are more cautious when dealing with PRs related to ML bugs (reviewing 87.3\% of them), which may also explain the lower rate of bug reopening observed for ML bugs. % because of more required resources and more complexity in comparison to non-ML ones as shown by our results, which likely leads to decrease the risk of introducing new bugs in ML components in the future~\cite{macleod2017code}.
% }
% , and higher negative effects of ML bugs on the whole ML-based systems
%\Foutse{why?? what is the logical connection here? please discuss this first with Amin!!! the logic of the argument here doesnt make much sense!!!}
%\Foutse{i dont understand the connection with checking the number of PR without code review, i would instead expect to see you scrutinising those PR that are not reviewed to see if they are trivial or not...anyway, event if a change is trivial it is not a justification for not reviewing it...i just dont understand your logic here!!! what exactly are you investigating and why is that important!!!!}
%One may interpret this as the fact that fixing ML issues are more complicated than non-ML ones and require more effort. Another potential interpretation is that ML issues have been addressed more carefully than non-ML ones by development teams. 
%On the other hand, thanks to the fact that merging PRs without review increases the risk of introducing new bugs in the future, based on the results we can also conclude that
%because PR that are merged without code review may be implementing only trivial changes
%demand for making sure about fixing ML bugs.
% PRs which are trying to fix ML bugs 
% \Amin{do you mean: bug-fixing PRs?} 
% \Amin{I don’t understand what is computed here? This is the percentage of what? Merged PRs?\Mehdi{merging PRs without review}}
% more complicated fixes related to ML issues
% \Amin{fixing ML issues are more complicated?}. 
% fixing ML issues efficiently has higher priority for development teams, in comparison with non-ML ones. 
% \Amin{how to add this: So with the risk of introducing bugs in the future, they merged them without reviewing them.


\textcolor{blue}{
% Figure environment removed
}
%for different types of bugs check how often developers have been invited to review different types of PRs. Results indicate that fixing ML-bugs (with 
% \Foutse{what? i don't quite get what is computed...please clarify!} 
% \Foutse{what is this percentage representing???}
% more requests for reviewing specific purpose \Amin{I don't understand  reviewing specific purpose}
% PRs by particular users 
% \Amin{what do you mean particular here? Were the reviewers the same?}. 
% In other words, there is 
% compared to non-ML ones. 

% \Foutse{fine, but are there PRs that are not reviewed? or that are hastily reviewed (i.e., too fast? previous work has shown that commits that are reviewed too fast are more likely to contain faults than other commits...so may be we can have a richer discussion with this angle...etc } % project administrators may prefer to invite particular developers (i.e., with ML expertise) for reviewing ML-related PRs more commonly.  % Moreover, as ML-bugs have more serious impact on the quality of the software systems, 


%% Figure environment removed


\begin{tcolorbox}
\textbf{Finding 4.} There is no statistically significant difference between ML and non-ML  on expertise level and time-to-fix when fixing bugs in ML-based systems. However, since the time-to-fix an issue does not capture all the effort that went into fixing the issue because it doesn't necessarily account for preparatory work done before starting editing the code, future work should investigate the cost of fixing ML bugs in more detail. % workequal effective time spent working on the issue.
\end{tcolorbox}

%\begin{table}
%\caption{Result of \textit{Mann–Whitney U} and \textit{Cliff's delta} test on expertise level.
%}
%    \resizebox{\columnwidth}{!}{
%    \centering
%    \small
%    \begin{tabular}{p{4.5cm} r r}
%        \hline
%            \textbf{Metric} & \multicolumn{1}{c}{\textbf{\textit{p-value}}} & 
%            \textbf{\textit{effect size (d)}}\\
%            \hline
%             All users & 0.346 & -0.090 (negligible)\\
%             After removing users with both ML and non-ML expertise& 0.201 & %-0.135 (negligible)\\
%             All users (without outliers) & 0.932 & -0.009 (negligible)\\
%             After removing users with both ML and non-ML expertise (without outliers) & 0.838 & 0.025 (negligible)\\
%        \hline
%    \end{tabular}
%    }    \item We believe there might be some specific metrics (compared to the studied metrics) to measure the complexity of ML components. For example, the number/type of layers in a neural network or how software systems employ (output of) the trained models.

%        \label{tbl:statistical_test_2}
%        \vspace{-1em}
%\end{table}

\subsection{Discussions}
% \textcolor{blue}{
We found that a large number of issues in ML-based systems either are users’ questions/mistakes or do not come with enough information to decide whether it is a bug or to help fix it. Moreover, although almost 10\% of development time is spent on ML components, nearly half of the “real” bugs in ML-based systems are related to ML components. Given the increasing usage and maturity of ML-based software systems over time, we believe that the software engineering community needs to revise existing ML bug classifications and add new possible categories to be able to classify root causes/symptoms of ML bugs. Our results revealed that ML bugs have significantly different characteristics from non-ML bugs, in ML-based systems: they need more number of commits, a higher number of LOC, and more collaborators involved, i.e. they seem to need more revision on average before the bug is fixed. The analysis of ML-related commits shows that bugs related to Requirements/Dependencies are the most widespread while Documentation bugs are the least represented. We did not find any statistically significant difference between ML and non-ML on the level of expertise of developers involved in their correction and the time taken to fix them in ML-based systems. However, the time-to-fix of an issue does not capture all the effort that went into fixing the issue because it does not necessarily account for preparatory work done before starting to edit the code.
% }

% \textcolor{blue}{
Our work can help in planning and forecasting quality assurance activities for ML-based systems. Based on our findings, the maintenance team can allocate appropriate resources for detection and correction of bugs. Our findings show where and how ML-based system developers should invest their efforts to have the most efficient software maintenance (lowering the cost of maintenance). Similar to previous research works (e.g., compiler bugs in DL systems~\cite{shen2021comprehensive}), we shed light on the root causes and symptoms of bugs in open-source ML-based systems. These findings can be used to facilitate the development of (automatic) debugging tools for ML-based software systems~\cite{tan2014bug}. Furthermore, our findings indicate the necessity of adopting novel techniques for the prediction and detection of ML bugs, given their distinctive characteristics and distribution of ML bugs in comparison to non-ML bugs, as shown in this study.
% }

% \textcolor{blue}{
It should be taken into consideration that applying ML in software systems results in generating new types of bugs which do not exist in traditional software systems. On the other hand, although developing ML components take just about 10\% of the ML-based systems development time~\cite{menzies2019five}, it represents roughly half of the real issues that a typical ML-based system will face. That is, ML components are more error-prone causing more serious suffering in comparison with non-ML ones. From the software maintenance viewpoint, it is obvious that developing ML components places a heavier financial burden on the software development process, with respect to their higher proneness to the bugs. As a software maintenance task, this observation emphasizes the need for automatic testing tools for ML components and ML-based software systems. A number of studies on the automatic bug detection~\cite{nikanjam2021automatic,zhang2021autotrainer}, bug localization~\cite{wardat2021deeplocalize}, debugging~\cite{wardat2022deepdiagnosis}, and bug repair~\cite{islam2020repairing} in ML-based systems that have been carried out during the last few years showed that there is an increasing need for automatic testing tools for ML-based systems.
% }

% \textcolor{blue}{
Based on our dataset of ML bugs, one can develop an automatic approach to predict the type of bugs in ML-based systems, as the most recent taxonomy~\cite{humbatova2020taxonomy} does not cover deployment issues. By increasing development and usage of ML-based software systems in various new areas, developers may face new categories of bugs which raises demand for revising existing bug’s taxonomy and adding new discovered bugs to them. In other words, a novel taxonomy of bugs that covers issues related to all stages of development, deployment and maintenance of ML-based software systems is necessary. Zhang et al.~\cite{zhang2018empirical} is considered as one of the first studies that provided a list of root causes and symptoms of various bugs in ML-based systems. Shen et al.~\cite{shen2021comprehensive} also studied bugs related to ML-based systems from ML compiler point of view.  In fact, they studied bugs in ML compilers (such as TVM\footnote{https://tvm.apache.org/}, Glow\footnote{https://ai.meta.com/tools/glow/}, and nGraph\footnote{https://www.intel.ca/content/www/ca/en/artificial-intelligence/ngraph.html}), not in the ML-based systems. Tomban et al.~\cite{tambon2021silent} also generated a list of bugs which may occur inside ML frameworks. Therefore, it is obvious that understanding about bugs in ML-based systems is still progressing. Accordingly, we would be required to update ML bug’s root causes, based on the newly added categories of ML bugs. 
% }

% \textcolor{blue}{
We believe there might be some specific metrics (compared to the studied metrics) to measure the complexity of ML components. There exist some studies reviewing complexity of the ML components from different viewpoints. As an example, Hu et al.~\cite{hu2021model} studied the complexity of DL models as the complexity of the problem that the DL model can express. They explained that model complexity can be measured based on various factors including model framework (feedforward neural network, convolutional neural network , etc), model size (number of parameters, number of hidden layers, etc), model optimization process (objective function’s form, learning algorithm, hyperparameters, etc), and the complexity of the data used for model training (data distribution, data dimensionality, information volume, etc). Zhang et al.~\cite{zhang2020machine} also represents model relevance as an ML testing property that checks the complexity of the model and tries to prevent model from overfitting. When the complexity of the model is higher than the problem, the model overfits to the training data and can not generalize. On the other hand, Yao et al.~\cite{yao2017complexity} carried out an empirical study to show the relationship between model complexity and model performance. They introduced complexity as the ability to control data preprocessing, feature selection, classifier selection and parameter tuning. Their results showed that models with higher complexity (more dimension to be controlled) gain better performance and classifier selection plays the most effective role in achieving higher performance. Therefore, ML engineers should be careful about the model complexity and the type of classifiers that they use.
% }

% \textcolor{blue}{
With respect to the fact that about 70\% of software development costs belong to software maintenance~\cite{grubb2003software}, automatic bug detection, and debugging tools improve the quality and speed of software development leading to a decrease in the software maintenance cost and accordingly software development, significantly~\cite{bennett2000software}. 
% }
% Based on our findings and observations in this study, in this section, we discuss future directions of research on bugs in ML software systems:\\
% \begin{enumerate}
%     \item Although ML components of a software takes just about 10\% of development time \cite{menzies2019five}, it represents roughly half of the real issues that a typical ML-based system will face. As a software maintenance task, this observation emphasizes the need for automatic testing tools for ML components and ML-based software systems.
%     \item Based on our dataset of ML bugs, one can examine an automatic approach to predict the type of bugs in ML-based systems, as the most recent taxonomy \cite{humbatova2020taxonomy} does not cover deployment issues.
%     \item By increasing development and usage of ML-based software systems, we need a new category of bug’s root causes for such systems. Similarly, a novel taxonomy of bug types is necessary that should cover issues related to all stages of development, deployment and maintenance of ML-based software systems.
%     \item We believe there might be some specific metrics (compared to the studied metrics) to measure the complexity of ML components. For example, the number/type of layers in a neural network or how software systems employ (output of) the trained models.
% \end{enumerate}
%\Amin{don't we need a summary/discussion here? how results may help with maintenance, ...? how our results help other researchers, on reliability engineering of MLs}\\
%\Amin{highlight observations and recommend or conduct how to understand if this was in fact the trend.}\\

\section{Related Works}
\label{sec:related_work}


Multiple studies analyzed bugs inside DL programs built on top of DL frameworks. Zhang et al.~\cite{zhang2018empirical} studied the characteristics of bugs in DL programs implemented based on \textit{TensorFlow}. They collected 175 bugs and reported their symptoms, root causes, and challenges to detect and localize them. Islam et al.~\cite{islam2019comprehensive} worked by studying bugs in DL programs implemented using \textit{Caffe}, \textit{Keras}, \textit{TensorFlow}, \textit{Theano}, \textit{Torch}. Humbatova et al.~\cite{humbatova2020taxonomy} manually investigated 1981 bug-fix commits and 1,392 issues/PRs from \textit{GitHub}, and 2,653 posts from \textit{Stack Overflow} related to the programs using \textit{TensorFlow}, \textit{Keras}, and \textit{PyTorch}, providing a taxonomy of bugs in DL programs by combining the result of manual checking and interviews of DL practitioners and researchers. Cao et al.\cite{cao2021characterizing} studied performance bugs in DL programs using \textit{TensorFlow} and \textit{Keras} from 225 \textit{Stack Overflow} posts. Finally, Liu et al.~\cite{liu2021detecting} performed an empirical study on the 12,289 failed \textit{TensorFlow} jobs and they provided a tool named \textit{ShapeTracer} to detect shape-related bugs in \textit{TensorFlow} programs. Although all of the mentioned studies conducted empirical studies on ML bugs, none of them studied the characteristics of ML bugs from the software maintenance perspective and the differences between ML and non-ML bugs in ML-based systems.  

Some other studies ~\cite{rivera2021challenge,tambon2021silent,jia2021symptoms} also investigated DL bugs, but inside the DL frameworks. For instance,
Jia et al. \cite{jia2021symptoms} studied root causes and symptoms of bugs affecting Tensorflow frameworks and provided a taxonomy. As such, since the bugs are not in the codes developed using those frameworks but rather in the framework itself, their study scope is different from ours.

 

\section{Threats to Validity}
\label{sec:validity}

\textit{Construct validity:} Limitations of our approach could come from: 1) keywords used for extracting repositories, and 2) choice of exclusion criteria for repositories and issues. The rest of the approach (bug filtering, issues classification of symptoms/root cause, statistical sampling...) is based on existing works. For keywords, we used `\texttt{import <ML framework>}' and `\texttt{language: python}' which is sufficiently general to catch a lot of repositories and is more likely to generate False Positive (repositories that we would end up discarding) rather than False Negative. Exclusion criteria were chosen after some preliminary discussions among authors to weed out irrelevant repositories. Raters further agreed to flag issues/repositories they were not sure about, to allow for careful discussion in order to avoid incorrect labeling.


\textit{Internal Validity:} The first source of internal validity is the manual checking of repositories and  issues. To alleviate this threat, the first three authors 
% \textcolor{blue}{
(one Ph.D. and two Ph.D. candidates who are practitioners of ML-based systems)
% } 
selected 300 repositories, and 386 issues, and then labeled them. After several meetings, we reached an agreement on the labeling process and the rules to differentiate between bug types. Another internal threat to validity is using sampling for gathering issues from the extracted repositories. To mitigate the sampling bias, we did the sampling with 95\% and 5\% of confidence levels and confidence intervals, respectively, which are the most common sampling in the Software Engineering community. The next threat is the fixes that we consider for each issue. In general, it is supposed that each PR works on one specific issue. But in some cases, developers use one PR to fix more than one issue. So, in a few cases, we could not separate the changes that are exactly related to the studied issues which may impact our results.

\textit{External Validity:} The main threat to external validity is the selected ML frameworks. We studied repositories using \textit{TensorFlow}, \textit{Keras}, and \textit{PyTorch}, because these frameworks are the most popular ML frameworks in GitHub, with the highest number of stars/forks among all ML frameworks (such as \textit{MXNet} \cite{chen2015mxnet}, and \textit{Caffe} \cite{jia2014caffe}). While we focused on \textit{Python} code inside ML repositories, as it is the most used language for this task \cite{voskoglou2017best,Gupta:MLLangugae}, we believe that similar observations could be made about ML issues for other programming languages, since the type of issue will likely remain the same. In the sampling process as well, we focus on repositories with the highest number of stars, forks, commits, and issues to study the most mature repositories and mitigate the generalization threat.
% \textcolor{blue}{
Another threat to the external validity of this study would be the generalization of the results, because of using GitHub as the main source for data collection. We selected GitHub because it is the most important platform for hosting open-source projects. %However, we can not claim that our provided results are surely valid for all communities.\Florian{Not sure last sentence is needed}
% }


\textit{Conclusion Validity:} Conclusion limitations can be potentially wrongly classified, missing issues/repository type, and the replicability of the study. We manually inspected 4,057 repositories out of over 400,000 repositories scraped from GitHub. Similarly, for issues, we extracted $44,342$ issues from the ML-based repositories and sampled $386$ using a well-known statistical procedure to ensure the credibility of our results. In both cases, we believe that the sample data is big enough to be representative, and not mislead us in our conclusions. Similarly, the labeling of both repositories and issues was done independently by three raters and then discussed to mitigate potential errors. At last, we provided a replication package \cite{Replication-Package} to allow for the reproducibility of our results, and also for other researchers to build on our study.

\section{Conclusion and Future Works}
\label{sec:conclusion}
Since ML-based systems are increasingly being used in various domains, including safety-critical systems. Their reliability has become paramount. Corrective maintenance is one of the main tasks in reliability engineering and bug diagnostic plays a key role in this task. In this paper, we aimed to characterize different types of bugs in ML-based systems, from the software maintenance perspective. Therefore, we manually checked 386 selected issues raised in ML-based systems developed using the three most popular ML frameworks (\textit{TensorFlow}, \textit{Keras}, and \textit{PyTorch}). Firstly, we showed that the prevalence of ML bugs is almost the same as non-ML bugs. Furthermore, the results showed that the previous classifications on ML-related bug types, their root cause, and symptoms are becoming stale and should be revised. We also observed that ML bugs are mostly more complicated than non-ML bugs. Moreover, the required resources (time-to-fix and developer expertise level) for fixing ML bugs are almost similar to non-ML ones. 
% \textcolor{blue}{
Besides, we represented that development teams fix ML issues 
more cautiously compared to
% has a higher priority than 
non-ML ones,
% for development teams, 
with respect to the percentage of bug-fix PRs which are merged without review. 
% }
% \textcolor{blue}{
In our future works, our first objective is to illustrate the characteristics of the bugs in various types of ML algorithms such as Computer Vision, NLP, etc. Additionally, we aim to expand the current study to comprehensively characterize different types of ML bugs (and their root causes/symptoms) and make a comparison among the characterization of different types of ML bugs. Furthermore, we plan to revise the existing taxonomy of bug types to encompass issues encountered throughout the whole ML application development pipeline including development, deployment, and maintenance, not only pre-processing, training, and testing. Finally, following previous studies~\cite{hanam2016discovering} ML bug characteristics can be used as a base to identify bug patterns in ML-based systems which help ML software maintainers to detect/predict ML bugs more easily. 
% }
% In future works, we plan to extend the current study to comprehensively characterize different types of ML bugs (and their root causes/symptoms) and make a comparison among the characterization of different types of ML bugs. 

\section{Data Availability Statement}
The dataset generated during the current study is available in the replication package, which is accessible via 
% this \href{https://github.com/ML-Bugs-2022/Replication-Package}{link}
\cite{Replication-Package}.


\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
% end of file template.tex


