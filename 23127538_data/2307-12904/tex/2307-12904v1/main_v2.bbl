\begin{thebibliography}{10}

\bibitem{balestriero2021learning}
R.~Balestriero, J.~Pesenti, and Y.~LeCun.
\newblock Learning in high dimension always amounts to extrapolation.
\newblock Preprint available at
  \href{https://arxiv.org/abs/2110.09485}{arXiv:2110.09485}, 2021.

\bibitem{Barron1992}
A.~R. Barron.
\newblock Neural net approximation.
\newblock In {\em Yale Workshop on Adaptive and Learning Systems}, volume~1,
  pages 69--72, 1992.

\bibitem{Barron1993}
A.~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information Theory}, 39(3):930--945, 1993.

\bibitem{Barron1994ApproximationAE}
A.~R. Barron.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock {\em Machine Learning}, 14:115--133, 1994.

\bibitem{Boelcskei2019}
H.~Bölcskei, P.~Grohs, G.~Kutyniok, and P.~Petersen.
\newblock Optimal approximation with sparsely connected deep neural networks.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 1(1):8--45, 2019.

\bibitem{Chen2019}
J.~Chen and H.~Nurdin.
\newblock Learning nonlinear input–output maps with dissipative quantum
  systems.
\newblock {\em Quantum Information Processing}, 18, 05 2019.

\bibitem{Chenetal2020}
J.~Chen, H.~I. Nurdin, and N.~Yamamoto.
\newblock Temporal information processing on noisy quantum computers.
\newblock {\em Phys. Rev. Appl.}, 14:024065, Aug 2020.

\bibitem{cordier2022biology}
B.~A. Cordier, N.~P. Sawaya, G.~G. Guerreschi, and S.~K. McWeeney.
\newblock Biology and medicine in the landscape of quantum advantages.
\newblock {\em Journal of the Royal Society Interface}, 19(196):20220541, 2022.

\bibitem{cybenko1989approximation}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of Control, Signals and Systems}, 2(4):303--314,
  1989.

\bibitem{Dasgupta2020}
S.~Dasgupta, K.~Hamilton, P.~Lougovski, and A.~Banerjee.
\newblock Designing a nisq reservoir with maximal memory capacity for
  volatility forecasting.
\newblock 04 2020.

\bibitem{Folland1995}
G.~Folland.
\newblock {\em Introduction to Partial Differential Equations, 2nd Edition}.
\newblock Princeton University Press, 1995.

\bibitem{FujiiNakajima2017}
K.~Fujii and K.~Nakajima.
\newblock Harnessing disordered-ensemble quantum dynamics for machine learning.
\newblock {\em Phys. Rev. Appl.}, 8:024030, Aug 2017.

\bibitem{Ghoshetal2021}
S.~Ghosh, K.~Nakajima, T.~Krisnanda, K.~Fujii, and T.~C.~H. Liew.
\newblock Quantum neuromorphic computing with reservoir computing networks.
\newblock {\em Advanced Quantum Technologies}, 4(9):2100053, 2021.

\bibitem{Gonon2021}
L.~Gonon.
\newblock Random feature neural networks learn {B}lack-{S}choles type {PDEs}
  without curse of dimensionality.
\newblock {\em Preprint, arXiv 2106.08900}, 2021.

\bibitem{RC12}
L.~Gonon, L.~Grigoryeva, and J.-P. Ortega.
\newblock {Approximation error estimates for random neural networks and
  reservoir systems}.
\newblock {\em The Annals of Applied Probability}, 33(1):28--69, 2023.

\bibitem{RC8}
L.~Gonon and J.-P. Ortega.
\newblock {Reservoir computing universality with stochastic inputs}.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  31(1):100--112, 2020.

\bibitem{RC20}
L.~Gonon and J.-P. Ortega.
\newblock {Fading memory echo state networks are universal}.
\newblock {\em Neural Networks}, 138:10--13, 2021.

\bibitem{Goto2021}
T.~Goto, Q.~H. Tran, and K.~Nakajima.
\newblock Universal approximation property of quantum machine learning models
  in quantum-enhanced feature spaces.
\newblock {\em Phys. Rev. Lett.}, 127:090506, Aug 2021.

\bibitem{GrigOrtega2018}
L.~Grigoryeva and J.-P. Ortega.
\newblock {Echo state networks are universal}.
\newblock {\em Neural Networks}, 108:495--508, 2018.

\bibitem{GrigoryevaOrtega2018}
L.~Grigoryeva and J.-P. Ortega.
\newblock {Universal discrete-time reservoir computers with stochastic inputs
  and linear readouts using non-homogeneous state-affine systems}.
\newblock {\em Journal of Machine Learning Research}, 19(24):1--40, 2018.

\bibitem{gripenberg2003approximation}
G.~Gripenberg.
\newblock Approximation by neural networks with a bounded number of nodes at
  each level.
\newblock {\em Journal of Approximation Theory}, 122(2):260--266, 2003.

\bibitem{Guehring2020}
I.~G\"{u}hring, G.~Kutyniok, and P.~Petersen.
\newblock Error bounds for approximations with deep {R}e{LU} neural networks in
  {$W^{s,p}$} norms.
\newblock {\em Anal. Appl. (Singap.)}, 18(5):803--859, 2020.

\bibitem{Guehring2023}
I.~G\"{u}hring, M.~Raslan, and G.~Kutyniok.
\newblock Expressivity of deep neural networks.
\newblock In {\em Mathematical aspects of deep learning}, pages 149--199.
  Cambridge University Press, 2023.

\bibitem{hornik1991}
K.~Hornik.
\newblock {Approximation capabilities of multilayer feedforward networks}.
\newblock {\em Neural Networks}, 4(1989):251--257, 1991.

\bibitem{hornik1989multilayer}
K.~Hornik, M.~Stinchcombe, and H.~White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock {\em Neural Networks}, 2(5):359--366, 1989.

\bibitem{HCS2006}
G.-B. Huang, L.~Chen, and C.-K. Siew.
\newblock Universal approximation using incremental constructive feedforward
  networks with random hidden nodes.
\newblock {\em Trans. Neur. Netw.}, 17(4):879–892, July 2006.

\bibitem{Jaeger2001}
H.~Jaeger.
\newblock The "echo state" approach to analysing and training recurrent neural
  networks-with an erratum note'.
\newblock {\em Bonn, Germany: German National Research Center for Information
  Technology GMD Technical Report}, 148, 01 2001.

\bibitem{Kallenberg2002}
O.~Kallenberg.
\newblock {\em {Foundations of Modern Probability, 2nd Edition}}.
\newblock Probability and Its Applications. Springer New York, 2002.

\bibitem{kidger2020universal}
P.~Kidger and T.~Lyons.
\newblock Universal approximation with deep narrow networks.
\newblock In {\em Conference on Learning Theory}, pages 2306--2327. PMLR, 2020.

\bibitem{Ledoux2013}
M.~Ledoux and M.~Talagrand.
\newblock {\em {Probability in Banach Spaces}}.
\newblock Springer Berlin Heidelberg, 2013.

\bibitem{leshno1993multilayer}
M.~Leshno, V.~Y. Lin, A.~Pinkus, and S.~Schocken.
\newblock Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
\newblock {\em Neural Networks}, 6(6):861--867, 1993.

\bibitem{LukoseviciusJaeger2009}
M.~Lukoševičius and H.~Jaeger.
\newblock Jaeger, h.: Reservoir computing approaches to recurrent neural
  network training. computer science review 3, 127-149.
\newblock {\em Computer Science Review}, 3:127--149, 08 2009.

\bibitem{MPOrtega2022}
R.~Martínez-Peña and J.-P. Ortega.
\newblock Quantum reservoir computing in finite dimensions.
\newblock {\em Physical Review E}, 107, 12 2022.

\bibitem{MMM21}
S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Generalization error of random features and kernel methods:
  hypercontractivity and kernel matrix concentration.
\newblock {\em Preprint, arXiv 2101.10588}, 2021.

\bibitem{Mhaskar1996}
H.~N. Mhaskar.
\newblock {Neural networks for optimal approximation of smooth and analytic
  functions}.
\newblock {\em Neural computation}, 8(1):164--177, 1996.

\bibitem{MOLTENI2023}
R.~Molteni, C.~Destri, and E.~Prati.
\newblock Optimization of the memory reset rate of a quantum echo-state network
  for time sequential tasks.
\newblock {\em Physics Letters A}, 465:128713, 2023.

\bibitem{Mujaletal2021}
P.~Mujal, R.~Martínez-Peña, J.~Nokkala, J.~García-Beni, G.~L. Giorgi, M.~C.
  Soriano, and R.~Zambrini.
\newblock Opportunities in quantum reservoir computing and extreme learning
  machines.
\newblock {\em Advanced Quantum Technologies}, 4(8):2100027, 2021.

\bibitem{Nokkala2021}
J.~Nokkala, R.~Martínez-Peña, G.~Giorgi, V.~Parigi, M.~Soriano, and
  R.~Zambrini.
\newblock Gaussian states of continuous-variable quantum systems provide
  universal and versatile reservoir computing.
\newblock {\em Communications Physics}, 4, 03 2021.

\bibitem{PerezSalinas2020datareuploading}
A.~P{\'{e}}rez-Salinas, A.~Cervera-Lierta, E.~Gil-Fuster, and J.~I. Latorre.
\newblock Data re-uploading for a universal quantum classifier.
\newblock {\em {Quantum}}, 4:226, Feb. 2020.

\bibitem{perez2021one}
A.~P{\'e}rez-Salinas, D.~L{\'o}pez-N{\'u}{\~n}ez, A.~Garc{\'\i}a-S{\'a}ez,
  P.~Forn-D{\'\i}az, and J.~I. Latorre.
\newblock One qubit as a universal approximant.
\newblock {\em Physical Review A}, 104(1):012405, 2021.

\bibitem{preskill2012quantum}
J.~Preskill.
\newblock Quantum computing and the entanglement frontier.
\newblock Preprint available at
  \href{https://arxiv.org/abs/1203.5813}{arXiv:1203.5813}, 2012.

\bibitem{RahimiRecht2008a}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1177--1184, 2008.

\bibitem{RahimiRecht2008}
A.~Rahimi and B.~Recht.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In D.~Koller, D.~Schuurmans, Y.~Bengio, and L.~Bottou, editors, {\em
  Advances in Neural Information Processing Systems}, volume~21, pages
  1313--1320. Curran Associates, Inc., 2009.

\bibitem{RudiRosasco2017}
A.~Rudi and L.~Rosasco.
\newblock Generalization properties of learning with random features.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3215--3225, 2017.

\bibitem{Rudin1987}
W.~Rudin.
\newblock {\em Real \& Complex Analysis}.
\newblock McGraw-Hill; 3rd edition, 1987.

\bibitem{Schuld2021}
M.~Schuld, R.~Sweke, and J.~J. Meyer.
\newblock Effect of data encoding on the expressive power of variational
  quantum-machine-learning models.
\newblock {\em Physical Review A}, 103:032430, Mar 2021.

\bibitem{siegel2022parallel}
A.~Siegel.
\newblock A parallel algorithm for understanding design spaces and performing
  convex hull computations.
\newblock {\em Journal of Computational Mathematics and Data Science},
  2:100021, 2022.

\bibitem{stamatopoulos2022towards}
N.~Stamatopoulos, G.~Mazzola, S.~Woerner, and W.~J. Zeng.
\newblock Towards quantum advantage in financial market risk using quantum
  gradient algorithms.
\newblock {\em Quantum}, 6:770, 2022.

\bibitem{Suzukietal2021}
Y.~Suzuki, Q.~Gao, K.~Pradel, K.~Yasuoka, and N.~Yamamoto.
\newblock Natural quantum reservoir computing for temporal information
  processing.
\newblock {\em Scientific reports}, 12, 2022.

\bibitem{TANAKA2019100}
G.~Tanaka, T.~Yamane, J.~B. Héroux, R.~Nakane, N.~Kanazawa, S.~Takeda,
  H.~Numata, D.~Nakano, and A.~Hirose.
\newblock Recent advances in physical reservoir computing: A review.
\newblock {\em Neural Networks}, 115:100--123, 2019.

\bibitem{xu2020neural}
K.~Xu, M.~Zhang, J.~Li, S.~S. Du, K.-i. Kawarabayashi, and S.~Jegelka.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks.
\newblock Preprint available at
  \href{https://arxiv.org/abs/2009.11848}{arXiv:2009.11848}, 2020.

\bibitem{yarotsky2017error}
D.~Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\end{thebibliography}
