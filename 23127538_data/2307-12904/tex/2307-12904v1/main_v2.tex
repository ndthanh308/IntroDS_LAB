\documentclass[reqno]{amsart}
\usepackage{bbm,amsmath, amsfonts, amsthm, amssymb}
\usepackage[margin=1.5in]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{graphicx}

\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\usepackage[braket, qm]{qcircuit}
\graphicspath{{Figures/}}

\setcounter{tocdepth}{3}% to get subsubsections in toc
\let\oldtocsection=\tocsection
\let\oldtocsubsection=\tocsubsection
\let\oldtocsubsubsection=\tocsubsubsection
\renewcommand{\tocsection}[2]{\hspace{0em}\oldtocsection{#1}{#2}}
\renewcommand{\tocsubsection}[2]{\hspace{1em}\oldtocsubsection{#1}{#2}}
\renewcommand{\tocsubsubsection}[2]{\hspace{2em}\oldtocsubsubsection{#1}{#2}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[textsize=small]{todonotes}

\newcommand{\jack}[2][]
{\todo[color=yellow, #1]{#2}}
\newcommand{\lukas}[2][]
{\todo[color=magenta, #1]{#2}}

\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\newcommand{\Ofi}[2][]
{\textcolor{cadmiumgreen}{#2}}

\usepackage{braket}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\CC}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\PPb}{\overline{\mathbb{P}}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\eps}{\varepsilon}
\newcommand{\D}{\mathrm{d}}
\newcommand{\E}{\mathrm{e}}
\newcommand{\I}{\mathrm{i}}
\newcommand{\VIX}{\mathrm{VIX}}
\newcommand{\ds}{\mathrm{ds}}
\newcommand{\dr}{\mathrm{dr}}
\newcommand{\dt}{\mathrm{dt}}
\newcommand{\Df}{\mathrm{D}}
\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Ff}{\mathcal{F}}
\newcommand{\Ffov}{\overline{\mathcal{F}}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Hht}{\widetilde{\mathcal{H}}}
\newcommand{\Ii}{\mathcal{I}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Nn}{\mathcal{N}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\Vf}{\mathfrak{V}}
\newcommand{\nq}{\mathfrak{n}}
\newcommand{\nqb}{\overline{\mathfrak{n}}}
\newcommand{\pf}{\mathfrak{p}}
\newcommand{\LOnefhat}{L^1[\widehat{f}]}
\newcommand{\simp}{\sigma_{\mathrm{imp}}}
%\newcommand{\C}{\mathrm{C}}
\newcommand{\BS}{\mathrm{BS}}
\newcommand{\Sso}{\mathcal{S}^{o}}
\newcommand{\Ssi}{\mathcal{S}^{i}}
\newcommand{\half}{\frac{1}{2}}


\newcommand{\pds}{\partial_{\sigma}}
\newcommand{\pdk}{\partial_{k}}
\newcommand{\pdx}{\partial_{x}}
\newcommand{\IBS}{\BS^{-1}}
\newcommand{\one}{\mathbbm{1}}

\newcommand{\ww}{\boldsymbol{w}}
\newcommand{\xx}{\boldsymbol{x}}
\newcommand{\bx}{\boldsymbol{\xi}}
\newcommand{\ab}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\gb}{\boldsymbol{\gamma}}
\newcommand{\Af}{\boldsymbol{\mathbf{A}}}
\newcommand{\Bf}{\boldsymbol{\mathbf{B}}}
\newcommand{\Wf}{\boldsymbol{W}}
\newcommand{\ttheta}{\boldsymbol{\theta}}
\newcommand{\TTheta}{\boldsymbol{\Theta}}
\newcommand{\ftrn}{f_{n,\ttheta}^{R}}
\newcommand{\gtrn}{g_{n,\ttheta}^{R}}
\newcommand{\Lbarf}{\overline{L}_{2}[f]}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% Quantum Commands %%%%%%%%%%%%%%%%%%%%%
\newcommand{\Ug}{\mathtt{U}}
\newcommand{\Ig}{\mathtt{I}}
\newcommand{\Cg}{\mathtt{C}}
\newcommand{\Xg}{\mathtt{X}}
\newcommand{\Yg}{\mathtt{Y}}
\newcommand{\Zg}{\mathtt{Z}}
\newcommand{\Hg}{\mathtt{H}}
\newcommand{\Rg}{\mathtt{R}}
\newcommand{\Rgx}{\mathtt{R}_{\mathrm{x}}}
\newcommand{\Rgy}{\mathtt{R}_{\mathrm{y}}}
\newcommand{\Rgz}{\mathtt{R}_{\mathrm{z}}}
\newcommand{\Pg}{\mathtt{P}}
\newcommand{\Vg}{\mathtt{V}}
\newcommand{\Wg}{\mathtt{W}}
\newcommand{\Mg}{\mathtt{M}}

\newcommand{\kz}{\ket{0}}
\newcommand{\ko}{\ket{1}}


\newcommand{\R}{\mathbb{R}}
\newcommand{\Rl}{\mathrm{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\Pol}{\mathrm{Pol}}
\newcommand{\Trig}{\mathrm{Trig}}
\newcommand{\Hess}{\ensuremath{\operatorname{Hess}}}
\newcommand{\Trace}{\ensuremath{\operatorname{Trace}}}

\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Hc}{\mathcal{H}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\notthis}[1]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}
\newtheorem{assumption}[theorem]{Assumption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{ocean}{rgb}{0,0.1,0.6}
\def\ocean#1{\textcolor{ocean}{#1}}
\def\red#1{\textcolor{red}{#1}}
\def\blue#1{\textcolor{blue}{#1}}

\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Universal Approximation Theorem for Quantum Neural Networks]{Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs}
\author{Lukas Gonon}
\address{Department of Mathematics, Imperial College London}
\email{l.gonon@imperial.ac.uk}
\author{Antoine Jacquier}
\address{Department of Mathematics, Imperial College London, and the Alan Turing Institute}
\email{a.jacquier@imperial.ac.uk}
\keywords{Universal Approximation Theorem, quantum neural networks, quantum reservoir computing}
\subjclass[2010]{68Q12, 68T07, 65D15}

\thanks{AJ is supported by the EPSRC grants EP/W032643/1 and  EP/T032146/1.}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Universal approximation theorems are the foundations of classical neural networks,
providing theoretical guarantees that the latter are able to approximate maps of interest.
Recent results have shown that this can also be achieved in a quantum setting,
whereby classical functions can be approximated by parameterised quantum circuits.
We provide here precise error bounds for specific classes of functions
and extend these results to the interesting new setup of randomised quantum circuits,
mimicking classical reservoir neural networks. Our results show in particular that a quantum neural network with $\mathcal{O}(\varepsilon^{-2})$ weights and  $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve  accuracy $\varepsilon>0$ when approximating  functions with integrable Fourier transform.
\end{abstract}

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Artificial neural networks were devised decades ago to approximate arbitrary functions by some algorithmically generated composition of maps.
With an increasing level of generality, Cybenko~\cite{cybenko1989approximation}, Hornik~\cite{hornik1991}, Hornik, Stinchcombe, White~\cite{hornik1989multilayer}, Leshno, Lin, Pinkus, Schocken~\cite{leshno1993multilayer} proved that spaces of artificial neural networks (with arbitrary width) are in fact dense within some spaces of functions, 
giving rise to the notion of \emph{universal approximations}.
The case of arbitrary depth and fixed width was studied later by 
Gripenberg~\cite{gripenberg2003approximation} and Kidger and Lyons~\cite{kidger2020universal} among many others. Bounds on the approximation errors have been obtained by Barron~\cite{Barron1992, Barron1993, Barron1994ApproximationAE}, Mhaskar~\cite{Mhaskar1996} and in many other works, for instance in~\cite{Boelcskei2019, Guehring2020, yarotsky2017error} and the review in~\cite{Guehring2023}.
These results---and many subsequent papers refining them---provide theoretical grounds for the use of neural networks in various applications.
While the generated function spaces are not constructive, they nevertheless guarantee that complicated and seemingly intractable functions (possibly in non-Euclidean or even infinite-dimensional spaces) can be well approximated by easy-to-implement neural networks, the simplest of them being feedforward neural networks.

Over the past few years, the rise of quantum computing capabilities, while still in their infancy, 
has open the gates to (small-scale) applications, in particular optimisation 
(through quantum annealing, as developed by \texttt{D-Wave}) and machine learning.
There has been wide interest in searching for so-called \emph{quantum supremacy}, 
or at least (and more realistically) \emph{quantum advantage} 
(a term coined by Preskill in~\cite{preskill2012quantum}) and the many empirical results available at the moment, 
in biology~\cite{cordier2022biology} or in finance~\cite{stamatopoulos2022towards}, for example, 
indicate that more research is needed to provide clear benefits for real-life applications.
In the context of machine learning specifically, 
quantum computing essentially replaces layers of feedforward neural networks by parameterised quantum circuits; the similarities end up here as the non-linearity of classical activation functions (such as sigmoid, $\tanh$, ReLu) find no specific equivalent in quantum neural networks, 
and a different approach is thus required. 
The fundamental difference of outputs 
(a vector on the real line for a classical neural network and a discrete probability distribution for its quantum version) also requires a different thought process.

The present paper focuses on the theoretical aspects of quantum neural networks.
In~\cite{PerezSalinas2020datareuploading,perez2021one} the authors proved a universal approximation theorem for quantum neural networks
by constructing a one-qubit quantum circuit able to arbitrarily approximate any continuous complex-valued function.
A similar approach was carried out in~\cite{Schuld2021}, who showed that data encoding can be approximated by infinitely (akin to infinite-width classical neural networks) repeating simple encoding schemes based on Pauli gates.
Both papers rely on Fourier series representations of the function to be approximated,
a very natural path because of their trigonometric interpretations, similar to the actions of (quantum) rotation matrices. 

The goal of the present article is to move one step further and to prove precise error bounds for these approximations.
Our first result, in Section~\ref{sec:variational},
consists of error bounds for a universal approximation theorem for continuous functions, bounded in~$L^{1}$ with mild constraints on their Fourier transforms. 
To do so, we explicitly build a parameterised quantum circuit and prove that a set of hyperparameters (or rotation angles) can achieve accurate estimation. More precisely, we show that a quantum neural network with $\mathcal{O}(\varepsilon^{-2})$ weights and  $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve  accuracy $\varepsilon>0$ when approximating  functions with integrable Fourier transform.
One potential drawback---standard in classical machine learning---is the obvious (very high) dimension of the hyperparameter space. 
To palliate this, in Section~\ref{sec:ReservoirQ}, we prove a similar result where the original quantum circuit is replaced by a reservoir quantum circuit, where all the unitary operators apart from the last one, are randomised and frozen.
This gives the advantage, similar to classical random features \cite{RahimiRecht2008a, RahimiRecht2008}, extreme learning machines~\cite{HCS2006} and reservoir computing~\cite{Jaeger2001, LukoseviciusJaeger2009, TANAKA2019100}), of reducing the dimension of the optimisation problem, thereby being more amenable to applications.
In Section~\ref{sec:variationalInfty}, we further improve the results of Section~\ref{sec:variational} to the $L^\infty$ case, thus providing uniform bounds.

Quantum reservoir computing~\cite{FujiiNakajima2017} has been extensively studied in the past years from a theoretical and from an empirical perspective, in~\cite{Dasgupta2020, MPOrtega2022, MOLTENI2023, Suzukietal2021} and in the review papers~\cite{Ghoshetal2021, Mujaletal2021}. 
Universal approximation properties for quantum reservoir computing have been studied in~\cite{Chen2019, Chenetal2020, Nokkala2021}, 
analogous to universality results for classical reservoir computing~\cite{RC8, RC20, GrigOrtega2018, GrigoryevaOrtega2018}. 
In quantum reservoir computing there is usually a dynamical aspect, whereby the inputs that need to be processed are sequences. Here we work in a static setting, but still use the term \textit{reservoir quantum circuit} to emphasise that the parameters within the variational quantum circuits are randomly generated and then fixed. Alternatively, we could also use the terminology \textit{quantum extreme learning machine} or \textit{quantum random feature network} in analogy to the classical terminology in the static case. 

The approximation bounds we obtain for these quantum reservoirs are akin to the approximation bounds available for classical random feature neural networks in~\cite{Gonon2021, RC12}. 
As in these, we consider here functions whose Fourier transforms satisfy certain integrability conditions. We prove that these functions can be approximated by quantum reservoir networks up to an approximation error of order $n^{-\frac{1}{2}}$, where~$n$ is a parameter we may choose, proportional to the number of randomly generated weights. 
Put differently, our result shows that an approximation accuracy $\varepsilon>0$ can be achieved by using $\mathcal{O}(\varepsilon^{-2})$ randomly generated weights and a circuit with $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits. 

In classical machine learning, random feature methods  are closely linked to kernel methods, as in~\cite{RahimiRecht2008a, RahimiRecht2008} and in the more recent works~\cite{MMM21, RudiRosasco2017}. Similarly, quantum reservoir networks are related to quantum feature maps as studied for example in~\cite{Goto2021}.
There, the authors prove a universal approximation result for quantum feature maps. A collection of basis functions is built by applying multiple observables to a quantum circuit. Linear combinations of these basis functions are then shown to be universal approximators. The proof is based on polynomial approximations, which also yields error bounds for these quantum feature maps. These results are valid for all Lipschitz functions and require $\mathcal{O}(\varepsilon^{-1})$ qubits and $\mathcal{O}(\varepsilon^{-d})$ measurement basis functions 
(where~$d$ is the data dimension) to achieve accuracy $\varepsilon>0$. In contrast, here we consider a single quantum circuit  and linearly combine probabilities obtained from a single measurement of this circuit.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approximation error bounds for variational quantum circuits}
\label{sec:variational}

\subsection{Construction of a universal variational quantum circuit}
\label{subsec:circuit}
%Consider the following variational quantum circuit: 
For $n \in \N$, 
weights $\ab = (\ab^{1},\ldots,\ab^{n}) \in (\RR^d)^n$, $\bb = (b^{1},\ldots,b^{n}) \in \R^n$, 
$\gb = (\gamma^{1},\ldots,\gamma^{n}) \in [0,2\pi]^n$ and an input $\xx=(x_1,\ldots,x_d) \in \RR^d$
define the following gates acting on a single qubit:
%\jack[inline]{The input is $x\in\RR^d$ or the sequence $x_1,\ldots, x_n$, also in $\RR^d$?}
%\lukas[inline]{It's $x\in\RR^d$, there was a typo in the next equation. Does it make sense now?}
\begin{equation*}
\begin{array}{rcll}
\Ug^{(i)}_1 & := & \Ug^{(i)}_1\left(\ab^{i},b^{i}, \xx\right)
& := \Hg \, \Rgz\left(-b^{i}\right) \Rgz\left(-a^{i}_d x_d\right) \cdots \Rgz\left(-a^{i}_1 x_1\right) \Hg,
\\
\Ug^{(i)}_2 & := & \Ug^{(i)}_2 \left(\gamma^{i}\right)
& := \Rgy\left(\gamma^{i}\right),
\end{array}
\end{equation*}
with~$\Hg$ the Hadamard gate and~$\Rgy$, $\Rgz$ the rotations around the $\mathrm{Y}$-and the $\mathrm{Z}$-axis respectively: 
$$
\Rgy(\gamma)
:= \begin{pmatrix} \cos\left(\frac{\gamma}{2}\right) & - \sin\left(\frac{\gamma}{2}\right) \\ \sin\left(\frac{\gamma}{2}\right) & \cos\left(\frac{\gamma}{2}\right) \end{pmatrix}
\qquad\text{and}\qquad
\Rgz(\alpha) := \begin{pmatrix} \E^{-\I \frac{\alpha}{2}} & 0 \\ 0 & \E^{\I \frac{\alpha}{2}} \end{pmatrix},
$$
for $\alpha\in\RR$ and $\gamma \in [0,2\pi]$.
Now write $\ttheta=(\ab^{(i)},b^{(i)},\gamma^{(i)})_{i=1,\ldots,n}
\in \TTheta := (\RR^d\times\RR\times[0,2\pi])^n$
and define the block matrix

\[
\Ug := \Ug(\ttheta,\xx) :=  \begin{bmatrix}
\Ug^{(1)}_1 \otimes \Ug^{(1)}_2 & \mathbf{0}_{4 \times 4}  & \mathbf{0}_{4 \times 4} & \cdots &  \mathbf{0}_{4 \times 4} & \mathbf{0}_{4 \times n_0} \\
\mathbf{0}_{4 \times 4} & \Ug^{(2)}_1 \otimes \Ug^{(2)}_2 & \mathbf{0}_{4 \times 4} & \cdots &  \mathbf{0}_{4 \times 4} & \vdots  \\
\vdots &  & \ddots &  & \vdots & \vdots \\
\mathbf{0}_{4 \times 4} & \cdots &  \mathbf{0}_{4 \times 4} &  \Ug^{(n-1)}_1 \otimes \Ug^{(n-1)}_2 & \mathbf{0}_{4 \times 4} & \vdots 
 \\
\mathbf{0}_{4 \times 4} & \cdots &  \cdots & \mathbf{0}_{4 \times 4} & \Ug^{(n)}_1 \otimes \Ug^{(n)}_2 & \mathbf{0}_{4 \times n_0}
\\
{\bf 0}_{n_0 \times 4} & \cdots & \cdots & \cdots & {\bf 0}_{n_0 \times 4} & {\bf 1}_{n_0 \times n_0}
\end{bmatrix},
\]
with $n_0$ the integer in~$\N_0$ such that $\log_2(4n+n_0) \in \N$.  
Then $\Ug \in \C^{(4n+n_0) \times (4n+n_0)}$ is unitary and can be viewed as a gate operating on $\nq := \lceil \log_2(4n) \rceil $ qubits. Let $N=4n+n_0=2^{\nq}$ and $\Vg \in \C^{N \times N}$ any unitary matrix that maps 
$\ket{0}^{\otimes \nq}$ to the state $\ket{\psi} = \frac{1}{\sqrt{n}} \sum_{i=0}^{n-1} \ket{4i}$. 

\begin{example}
To clarify the somewhat abstract construction above, we provide some examples to make it more explicit. 
It is indeed trivial to generate all the columns for any integer $n\in\NN$:
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
$n$ & $n_0$ & $\nq$ & $\ket{\psi}$\\
\hline
1 & 0 & 2 & $\ket{00}$\\
2 & 0 & 3 & $\displaystyle \frac{1}{\sqrt{2}}\left(\ket{000}+\ket{100}\right)$\\
3 & 4 & 4 & $\displaystyle \frac{1}{\sqrt{3}}\left(\ket{0000}+\ket{0100}+\ket{1000}\right)$\\
4 & 0 & 4 & $\displaystyle \frac{1}{\sqrt{4}}\left(
\ket{0000}+\ket{0100}+\ket{1000}+ \ket{1100}\right)$\\
5 & 12 & 5 & $\displaystyle \frac{1}{\sqrt{5}}\left(
\ket{00000}+\ket{00100}+\ket{01000}+\ket{01100}+\ket{10000}
\right)$\\
\hline
\end{tabular}
\end{center}
\end{example}
\begin{remark}
Since $\Vg\ket{0}^{\otimes\nq} = \ket{\psi}$, then the first column of~$\Vg$ (seen as a matrix in $\CC^{N\times N}$) corresponds exactly to~$\ket{\psi}$ (seen as a vector in $\CC^{N}$).
We can in fact give an explicit (but not necessarily unique) construction for such a unitary matrix~$\Vg$
given this constraint on the first column. 
Indeed, set 
$\Vg := 2\ket{\varphi}\bra{\varphi} - \Ig$,
with 
$$
\ket{\varphi} := \frac{\ket{0}+\ket{\psi}}{\sqrt{2\left(1+\braket{0|\psi}\right)}},
$$
where we write $\ket{0}$ in place of $\ket{0}^{\otimes\nq}$ for brevity here.
In this case, immediate computations show that~$\Vg$ is unitary since 
$\Vg^\dagger := 2\ket{\varphi}\bra{\varphi} - \Ig = \Vg$
%\qquad\text{and}\qquad
and
$\Vg\Vg^\dagger = \Vg^\dagger\Vg = \Ig$.
Furthermore,
\begin{align*}
\Vg\ket{0} & = \left(2\ket{\varphi}\bra{\varphi} - \Ig\right)\ket{0}\\
 & = \left(2\frac{\ket{0}+\ket{\psi}}{\sqrt{2\left(1+\braket{0|\psi}\right)}}\frac{\bra{0}+\bra{\psi}}{\sqrt{2\left(1+\braket{0|\psi}\right)}} - \Ig\right)\ket{0}\\
 & = \frac{\ket{0}\bra{0}+\ket{0}\bra{\psi}+\ket{\psi}\bra{0}+\ket{\psi}\bra{\psi}}{1+\braket{0|\psi}}
 \ket{0} - \ket{0}\\
 & = \frac{\ket{0}+\ket{0}\braket{\psi|0}+\ket{\psi}+\ket{\psi}\braket{\psi|0}}{1+\braket{0|\psi}} - \ket{0}\\ 
  & = \frac{\ket{0}\left(1+\braket{\psi|0}\right)+\ket{\psi}\left(1+\braket{\psi|0}\right)}{1+\braket{0|\psi}} - \ket{0}
  = \ket{\psi},
\end{align*}
where we used the fact that $\braket{0|\psi} \in \RR$ in the third line.

The property $\Vg\ket{0}^{\otimes\nq} = \ket{\psi}$ is in fact the only property of $\Vg$ that is required, otherwise  $\Vg$  does not have any impact on the scheme or the error bounds. In case there are several alternative choices for $\Vg$  one may thus select the one that is most suitable from the perspective of hardware requirements and limitations.
\end{remark}
We can now measure the state of the $\nq$-qubit system after applying the gates $\Vg$ and $\Ug$. 
The possible states that we could measure are ${0,\ldots,N-1}$. 
For $m \in \{0,1,2,3\}$ we denote by~$\P_{m}$ the probability that the measured state is in  $\{m,4+m,\ldots,4(n-1)+m\}$.
By running the circuit~$S$ times we may estimate these probabilities by
\[
\widehat{\P}_{m}^{n} := \frac{1}{S} \sum_{s=1}^S \mathbbm{1}_{\{m,4+m,\ldots,4(n-1)+m\}}(i^{(s)}),
\]
with $i^{(s)}$ the measured state in the $i$-th shot. 
Here we will consider~$S$ to be large enough and will consider $\P_{m}^{n} = \P_{m}^{n}(\ttheta,\xx)$ as the output of the quantum circuit. 
We summarise this construction in the following formal definition:

\begin{definition}
For $n \in \NN$ and $\ttheta \in\TTheta$, 
we define the unitary operator $\Cg_{\nq}(\ttheta,\xx) := \Ug(\ttheta,\xx)\Vg$ acting on $\nq = \lceil \log_2(4n) \rceil $ qubits, namely the realisation of the variational quantum circuit

\begin{minipage}{5cm}
\begin{center}
	\Qcircuit{
			&\lstick{\ket{0}^{\otimes \nq}} & \gate{\Vg} & \gate{\Ug(\ttheta,\xx)} & \meter 
		}
	\end{center}
\end{minipage}

 With a slight abuse of notation, we may also refer to this circuit as~$\Cg_{\nq}(\ttheta,\xx)$.
 Furthermore, for any $m \in \{0,1,2,3\}$, we let
 $\P_m^{n} := \PP\left("\Cg_{\nq}(\ttheta,\xx)\ket{0}^{\otimes \nq} \in \{m,4+m,\ldots,4(n-1)+m\}"\right)$.
\end{definition}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Approximation error bound}
We now consider learning based on the variational quantum circuit constructed above. 
For a continuous and integrable (in~$\Cc(\RR^d)\times L^{1}(\RR^d)$) function $f \colon \RR^d \to \R$, 
we denote by $\widehat{f}(\bx) := \int_{\RR^d} \E^{-2\pi\I \boldsymbol{y} \cdot \bx} f(\boldsymbol{y}) \D \boldsymbol{y}$, for $\bx \in \RR^d$, 
its Fourier transform and define
$$
\LOnefhat :=\int_{\RR^d} |\widehat{f}(\bx)|\D \bx,
$$
which may or may not be finite.


Given the operator $\Cg_{\nq}(\ttheta,\xx)$
and $R>0$,
introduce the map $\ftrn:\RR^d \to\RR$ by
$$
\ftrn(\cdot)
:= R-2R[\P_1^{n}(\ttheta,\cdot)
+ \P_2^{n}(\ttheta,\cdot)].
$$

In order to avoid repetitions later and to compare the different results more clearly, introduce the spaces of functions
\begin{align}
\Ff & := \Big\{f:\RR^d\to\RR: f\in \Cc\left(\RR^d\right)\cap L^{1}\left(\RR^d\right)\Big\},\label{eq:Space1}\\
\Ff_{R} & := \left\{f\in\Ff,
\text{ with }\LOnefhat \leq R\right\},
\qquad\text{for any }R>0.\label{eq:Space1R}
\end{align}

Before stating the first theorem, we fix a probability measure~$\mu$ on~$\RR^d$,
which shall be used throughout to measure the approximation error. 
This measure can be chosen arbitrarily and changes how the approximation quality is weighted in different regions of~$\RR^d$. 
For example, if~$\mu$ is the uniform measure on a hypercube $[-M,M]^d$ (for some $M>0$), then the approximation quality is weighted equally over $[-M,M]^d$ and no weight is put on the approximation quality outside $[-M,M]^d$.

The following result shows that the outputs of~$\Cg_{\nq}(\ttheta,\xx)$ can be used to approximate such  functions--further satisfying $\widehat{f} \in L^{1}(\RR^d)$--up to an error of size $n^{-\frac{1}{2}}$. 
In particular, no curse of dimensionality occurs and the number of qubits is logarithmic in~$n$.  

\begin{theorem}\label{thm:Approx}
For any $R>0$, $f \in \Ff_{R}$ and
$n\in\NN$, there exists~$\ttheta\in\TTheta$ such that 
$$
\left(\int_{\RR^d} 
 \left|f(\xx) - \ftrn(\xx)\right|^2 \mu(\D \xx)\right)^{1/2} \leq  \frac{\LOnefhat}{\sqrt{n}}.
$$
%	with $C_f = \int_{\RR^d} |\widehat{f}(\bx)| \D\bx$. 
\end{theorem}

\begin{remark}
We would like to emphasise that the quantum circuit above depends on~$\ttheta$ (a quantum analogue of the vector of hyperparameters in classical neural networks) but also on the input data~$\xx$.
While this may not sound ideal at first, 
this is how classical analyses are performed, for example in~\cite{perez2021one, Schuld2021}.
Now, the theorem states that the optimal vector~$\ttheta$ does not itself depend on~$\xx$, which is a very desirable feature indeed.
Assume for example that the measure~$\mu$ is a sum of Dirac masses, namely
$$
\mu(\D\xx) = \sum_{i=1}^{M}\mu_{i}\delta_{\xx^{i}},
$$
for some positive integer~$M$ so that the expression in Theorem~\ref{thm:Approx} reads
$$
\int_{\RR^d} 
 \left|f(\xx) - \ftrn(\xx)\right|^2 \mu(\D \xx)
 = \sum_{i=1}^{M}\mu_i
 \left|f(\xx^i) - \ftrn(\xx^i)\right|^2,
$$
implying that the quantum neural network can approximate the function~$f$ over any compact hypercube 
$[\min\{\xx^i_1\}_{i=1,\ldots,M}, \max\{\xx^i_1\}_{i=1,\ldots,M}]
\times\cdots\times[\min\{\xx^i_d\}_{i=1,\ldots,M}, \max\{\xx^i_d\}_{i=1,\ldots,M}]$
with a single vector of parameters~$\ttheta$. More generally,
fix a weighting function $w \colon \R^d \to [0,\infty)$ that models the weight $w(\xx)$ we assign to the error at each point $\xx \in \R^d$. 
Provided that $w \in L^1(\R^d)$, we can use it to define a probability measure $\mu$ with Lebesgue-density $w/\|w\|_1$ for which the error reads
$$
\int_{\RR^d} 
\left|f(\xx) - \ftrn(\xx)\right|^2 \mu(\D \xx)
= \frac{1}{\|w\|_1} \int_{\RR^d} w(\xx)
\left|f(\xx) - \ftrn(\xx)\right|^2 \D \xx,
$$
so that the quantum neural network is able to approximate the function $f$ on all of $\R^d$ in an average sense with a single vector of parameters $\ttheta$. 
Handling or adding constraints to take care of potential erratic behaviour outside the (necessarily) compact set of training data points has recently been tackled for classical neural networks~\cite{balestriero2021learning,siegel2022parallel,xu2020neural} and we leave it to future research in the quantum case.
\end{remark}


For standard neural networks, the condition $\int_{\RR^d} \|\bx\| |\widehat{f}(\bx)|\D \bx < \infty$ guarantees that~$f$ can be approximated by neural networks without the curse of dimensionality, as established in~\cite{Barron1993}. 
For $f \in L^1(\RR^d)$, this condition is stronger than the requirement $\LOnefhat < \infty$ imposed in Theorem~\ref{thm:Approx}. 

A sufficient condition for $\LOnefhat < \infty$ is the requirement that $f \in \Hh^{s}(\RR^d)$ for $s>\frac{d}{2}$, 
where~$\Hh^{s}$ is the Sobolev space of order~$s$, see \cite[Lemma~6.5]{Folland1995} applied with $k=0$ and its proof. 
In dimension $d=1$, for example, a sufficient condition to apply Theorem~\ref{thm:Approx} is thus $f \in L^1(\R)\cap L^2(\R) \cap \Cc(\R)$ ($=L^1(\R) \cap \Cc(\R) $), $f' \in L^2(\R)$. 
%\jack[inline]{In \cite[Lemma~6.5]{Folland1995}, what is called the "Sobolev Lemma", it seems to require $s>k+\frac{d}{2}$, to have the finite bound and $\Hh^{s}\subset \Cc^k$. Do you assume/consider $k=0$ here?}
%\lukas[inline]{Yes, precisely, we apply it with $k=0$. Actually the key point $f \in \Hh^{s}(\RR^d) \rightarrow \int_{\RR^d} |\widehat{f}(\bx)|\D\bx < \infty$ is in the proof of \cite[Lemma~6.5]{Folland1995}. I added that we consider $k=0$ here, what do you think?}
The proof of Theorem~\ref{thm:Approx} is constructive and consists of two steps. 
First, in Proposition~\ref{prop:circuitOutput}, 
we show that for any choice of weights $\ttheta=(\ab^{i},b^{i},\gamma^{i})_{i=1,\ldots,n}$ the function 
\begin{equation}
	\label{eq:fct}
\gtrn:\xx \longmapsto \frac{1}{n}\sum_{i=1}^{n} R\cos\left(\gamma^{i}\right) \cos\left( l^{i}(\xx)\right),
\qquad\text{with}\qquad
l^{i}(\xx) := b^{i}+\ab^{i} \cdot \xx,
\end{equation}
can be realised as the output of~$\Cg_{\nq}(\ttheta,\xx)$, namely that it is equal to~$\ftrn$. 
Then, in Proposition~\ref{prop:FourierApprox}, 
we use a probabilistic argument to show that functions of type~\eqref{eq:fct} are able to approximate continuous, integrable functions $f$ with integrable Fourier transform up to an error of size $\LOnefhat n^{-\frac{1}{2}}$,
thus yielding Theorem~\ref{thm:Approx}. 


\begin{remark}
Instead of the quantum circuit in Theorem~\ref{thm:Approx} one could alternatively use a circuit with $2n$ qubits in which~$\Ug$ has a tensor product structure. More specifically, $\Ug$ could be chosen as the sequence of~$n$ two-qubit operations
\[
\Ug = \prod_{i=1}^n (\mathbbm{1}\otimes \mathbbm{1})^{\otimes i-1}  \otimes  
\left(\Ug^{(i)}_1 \otimes \Ug^{(i)}_2\right)
\otimes (\mathbbm{1}\otimes \mathbbm{1})^{\otimes n-i},
\]
and 
$\Vg \in \C^{2^{2n} \times 2^{2n}}$ could be chosen as any unitary matrix that maps 
$\ket{0}^{\otimes 2n}$ to the state 
$\ket{\psi} = \frac{1}{\sqrt{n}} \sum_{i=0}^{n-1} (\ket{1} \otimes \ket{1} )^{\otimes i} \otimes (\ket{0} \otimes \ket{0} )^{\otimes n-i}$. 
Choosing $\P_m^{n}$ ($m\in \{0,1,2,3\}$) as the probability that the measured state is in 
$\{ (\ket{1} \otimes \ket{1} )^{\otimes i} \otimes (\ket{x} \otimes \ket{y} ) \otimes (\ket{0} \otimes \ket{0} )^{\otimes n-i-1} \,: \, i \in \{0,\ldots,n-1\}\}$ where $x,y \in \{0,1\}$ are the coefficients in the binary representation $m=2x+y$,
the proof of Proposition~\ref{prop:circuitOutput} below carries over analogously. 
\end{remark}

\subsection{Proof of Theorem~\ref{thm:Approx}}
In this section we prove two propositions, the combination of which directly implies Theorem~\ref{thm:Approx}.

\begin{proposition} \label{prop:circuitOutput}
For any $n\in\NN$, $\ttheta\in\TTheta$, 
the representation
$\ftrn = \gtrn$ holds over~$\RR^d$.
\end{proposition}
\begin{proof}
Let us first calculate $\P_0^{n}$. To do this, we first compute 
$$
\Ug \Vg \ket{0}^{\otimes \nq}
= \Ug \ket{\psi}  =  \frac{1}{\sqrt{n}} \sum_{j=0}^{n-1} \Ug \ket{4j}  
 =
 \frac{1}{\sqrt{n}} \sum_{j=0}^{n-1} \sum_{k=0}^3\left[\Ug^{(j+1)}_1 \otimes \Ug^{(j+1)}_2\right]_{k+1,1} \ket{4j+k},
$$
and consequently
\begin{align*}
\P_0^{n}  = \sum_{i=0}^{n-1} \left|\bra{4i} \Ug \Vg \ket{0}^{\otimes \nq} \right|^2
 & =
 \sum_{i=0}^{n-1} \left|\bra{4i} \frac{1}{\sqrt{n}} \sum_{j=0}^{n-1} \sum_{k=0}^3\left[\Ug^{(j+1)}_1 \otimes \Ug^{(j+1)}_2\right]_{k+1,1} \ket{4j+k}  \right|^2
  \\ & =
 \frac{1}{n}\sum_{i=0}^{n-1} \left| \left[\Ug^{(i+1)}_1 \otimes \Ug^{(i+1)}_2\right]_{1,1}  \right|^2.
\end{align*}
Computing
\begin{align*}
[\Ug^{(i)}_1 \otimes \Ug^{(i)}_2]_{1,1}
& = [\Ug^{(i)}_1]_{1,1} [\Ug^{(i)}_2]_{1,1} = \cos\left(\frac{\gamma^{i}}{2}\right) 
\left[\Hg \begin{pmatrix} \E^{\frac{\I}{2} l^{i}(\xx)}
& 0 \\
0 & \E^{-\frac{\I}{2} l^{i}(\xx)} \end{pmatrix}  \Hg \right]_{1,1}\\
& =\frac{1}{\sqrt{2}}\cos\left(\frac{\gamma^{i}}{2}\right) \left[\Hg \begin{pmatrix} \E^{\frac{\I}{2} l^{i}(\xx)}
& \E^{\frac{\I}{2} l^{i}(\xx)} \\ \E^{-\frac{\I}{2} l^{i}(\xx)}
& -\E^{-\frac{\I}{2} l^{i}(\xx)} \end{pmatrix}   \right]_{1,1}\\
& = \frac{1}{2}\cos\left(\frac{\gamma^{i}}{2}\right) \left(\E^{\frac{\I}{2} l^{i}(\xx)} + \E^{-\frac{\I}{2} l^{i}(\xx)}\right)\\
& = \cos\left(\frac{\gamma^{i}}{2}\right)
\cos\left(\frac{l^{i}(\xx)}{2}\right),
\end{align*} 
then
$
\P_0^{n} =
\frac{1}{n}\sum_{i=1}^{n} \cos\left(\frac{\gamma^{i}}{2}\right)^2
\cos\left(\frac{l^{i}(\xx)}{2}\right)^2$, 
which simplifies, using $\cos(y)^2=\frac{\cos(2y)+1}{2}$, to
\begin{align*}
\P_0^{n} & =
\frac{1}{n}\sum_{i=1}^{n} 
\frac{1}{4}\left(\cos\left(\gamma^{i}\right)+1\right)
\left(\cos\left(l^{i}(\xx)\right)+1\right)\\
& = \frac{1}{4} + \frac{1}{4n}\sum_{i=1}^{n} \cos\left(\gamma^{i}\right)
\cos\left( l^{i}(\xx)\right)
+ \frac{1}{4n}\sum_{i=1}^{n} \cos\left(\gamma^{i}\right)
+ \frac{1}{4n}
\sum_{i=1}^{n} \cos\left(l^{i}(\xx)\right).
\end{align*}


Next, for $m \in \{1,2,3\}$, we have
%denote by $\P_l$ the probability that the measured state is in  $\{l,4+l,\ldots,4(n-1)+l\}$. Then
\begin{align*}
\P_m^{n} = \sum_{i=0}^{n-1} \left|\bra{4i+m} \Ug \Vg \ket{0}^{\otimes \nq} \right|^2
& =
\sum_{i=0}^{n-1} \left|\bra{4i+m} \frac{1}{\sqrt{n}} \sum_{j=0}^{n-1} \sum_{k=0}^{3}
\left[\Ug^{(j+1)}_1 \otimes \Ug^{(j+1)}_2\right]_{k+1,1} \ket{4j+k} \right|^2
\\ & =
\frac{1}{n}\sum_{i=0}^{n-1} 
\left| \left[\Ug^{(i+1)}_1 \otimes \Ug^{(i+1)}_2\right]_{m+1,1}  \right|^2.
\end{align*}
Computing as above 
\begin{align*}
[\Ug^{(i)}_1 \otimes \Ug^{(i)}_2]_{2,1} & = [\Ug^{(i)}_1]_{1,1} [\Ug^{(i)}_2]_{2,1} = \sin\left(\frac{\gamma^{i}}{2}\right)
\cos\left(\frac{l^{i}(\xx)}{2}\right),\\
[\Ug^{(i)}_1 \otimes \Ug^{(i)}_2]_{3,1} & = [\Ug^{(i)}_1]_{2,1} [\Ug^{(i)}_2]_{1,1} = 
\I \cos\left(\frac{\gamma^{i}}{2}\right)
\sin\left(\frac{l^{i}(\xx)}{2}\right),\\
[\Ug^{(i)}_1 \otimes \Ug^{(i)}_2]_{4,1} & = [\Ug^{(i)}_1]_{2,1} [\Ug^{(i)}_2]_{2,1} = 
\I \sin\left(\frac{\gamma^{i}}{2}\right)
\sin\left(\frac{l^{i}(\xx)}{2}\right),
\end{align*}
thus yields
\begin{align*}
\P_1^{n} & = \frac{1}{n}\sum_{i=1}^{n} \sin\left(\frac{\gamma^{i}}{2}\right)^2
\cos\left(\frac{l^{i}(\xx)}{2}\right)^2
\\
\P_2^{n} & = \frac{1}{n}\sum_{i=1}^{n} \cos\left(\frac{\gamma^{i}}{2}\right)^2
\sin\left(\frac{l^{i}(\xx)}{2}\right)^2
\\ 
\P_3^{n} & = \frac{1}{n}\sum_{i=1}^{n} \sin\left(\frac{\gamma^{i}}{2}\right)^2
\sin\left(\frac{l^{i}(\xx)}{2}\right)^2.
\end{align*}
Therefore,
\begin{align*}
\P_0^{n} + \P_1^{n} & = \frac{1}{n}\sum_{i=1}^{n}
\cos\left(\frac{l^{i}(\xx)}{2}\right)^2
= \frac{1}{2} + \frac{1}{2n}\sum_{i=1}^{n} \cos\left(l^{i}(\xx)\right),\\
\P_0^{n} + \P_2^{n} & = \frac{1}{n}\sum_{i=1}^{n} \cos\left(\frac{\gamma^{i}}{2}\right)^2 = \frac{1}{2} + \frac{1}{2n}\sum_{i=1}^{n} \cos\left(\gamma^{i}\right).
\end{align*}
Putting it all together we obtain, for any given $R>0$, that
\[
R - 2R\left[\P_1^{n}+\P_2^{n}\right]
= R\left[1 + 4\P_0^{n} - 2\left(\P_0^{n} + \P_1^{n}\right) - 2\left(\P_0^{n} + \P_2^{n}\right)\right]
= \frac{1}{n}\sum_{i=1}^{n} R\cos\left(\gamma^{i}\right) \cos\left(l^{i}(\xx)\right).
\]
\end{proof}


\begin{proposition} \label{prop:FourierApprox} 
 Let $R>0$ and $f\in\Ff_{R}$.
 For any $n \in \N$, there exists $\ttheta\in\TTheta$ such that
$$
\left(\int_{\RR^d} \left|\ftrn(\xx) - f(\xx)\right|^2 \mu(\D \xx)\right)^{1/2} \leq \frac{\LOnefhat}{\sqrt{n}}.
$$
%where $C_f = \int_{\RR^d} |\widehat{f}(\bx)| \D\bx$.
\end{proposition}

\begin{proof}
Since $\widehat{f}\in L^{1}(\RR^d)$, the Fourier inversion theorem states that for all $\xx\in \RR^d$,
\[
f(\xx) = \int_{\RR^d} \E^{2\pi \I\xx \bx}\widehat{f}(\bx) \D \bx.
\]
Since~$f$ is real-valued, we may then write, for any $\xx\in\RR^d$,
\begin{align}\label{eq:auxEq2}
f(\xx) = \int_{\RR^d} \E^{2\pi \I\xx \bx}\widehat{f}(\bx) \D \bx
& = \int_{\RR^d}\left\{ \cos{(2\pi \xx \cdot \bx)}\mathrm{Re}[\widehat{f}(\bx)] - \sin{(2\pi \xx \cdot \bx)}\mathrm{Im}[\widehat{f}(\bx)] \right\}\D \bx 
\\
& = 
\int_{\RR^d}\left\{ \cos{(2\pi \xx \cdot \bx)}\mathrm{Re}[\widehat{f}(\bx)] + \cos\left(2\pi \xx \cdot \bx+\frac{\pi}{2}\right)\mathrm{Im}[\widehat{f}(\bx)] \right\}\D \bx.\nonumber
\end{align}
Let $\pf := \LOnefhat^{-1}\int_{\RR^d} |\mathrm{Re}[\widehat{f}(\bx)]| \D \bx \in [0,1]$ and
$Z_1,\ldots,Z_n$ be i.i.d.\ random variables with a $\pf$-Bernoulli distribution, so that
$\P(Z_i=1) = \pf$ and $\P(Z_i=0) = 1-\pf = \LOnefhat^{-1}\int_{\RR^d} |\mathrm{Im}[\widehat{f}(\bx)]| \D\bx$. 
If $\int_{\RR^d} |\mathrm{Re}[\widehat{f}(\bx)]| \D\bx \neq 0 $, let $\nu_1$ be the probability measure on $\RR^d$ with density $\frac{|\mathrm{Re}[\widehat{f}]|}{\int_{\RR^d} |\mathrm{Re}[\widehat{f}(\bx)]| \D\bx }$, otherwise $\nu_1$ is an arbitrary probability measure on $\RR^d$.
Analogously, if $\int_{\RR^d} |\mathrm{Im}[\widehat{f}(\bx)]| \D\bx \neq 0$, let $\nu_0$ be the probability measure on $\RR^d$ with density $\frac{|\mathrm{Im}[\widehat{f}]|}{\int_{\RR^d} |\mathrm{Im}[\widehat{f}(\bx)]| \D\bx }$, otherwise $\nu_0$ is an arbitrary probability measure on $\RR^d$.
Let $\bU_1,\ldots,\bU_n$ be i.i.d.\ random variables with distribution~$\nu_1$, 
and $\bV_1,\ldots,\bV_n$ be i.i.d.\ random variables with distribution~$\nu_0$ and assume that $\bU_1,\ldots,\bU_n$, $\bV_1,\ldots,\bV_n$ and $Z_1,\ldots,Z_n$ are independent. 
Set now
\begin{align*}
\mathbf{A}_i & := 2 \pi (Z_i \bU_i+(1-Z_i)\bV_i),
\qquad
B_i := \frac{\pi}{2}(1-Z_i),\\
\quad
W_i & := L^{1}[\widehat{f}]\left[ \frac{\mathrm{Re}[\widehat{f}](\bU_i)}{|\mathrm{Re}[\widehat{f}](\bU_i)|} Z_i + \frac{\mathrm{Im}[\widehat{f}](\bV_i)}{|\mathrm{Im}[\widehat{f}](\bV_i)|}(1-Z_i) \right],
\end{align*}
with the quotient set to zero when the denominator is null, and consider the random function 
$$
F(\xx) := \frac{1}{n}\sum_{i=1}^n W_i \cos(B_i+\mathbf{A}_i\cdot \xx).
$$
Then we calculate
\[
\begin{aligned}
\EE[F(\xx)] & = \EE[W_1 \cos(B_1+\mathbf{A}_1\cdot \xx)]
\\ & =  \LOnefhat \left(\pf\EE\left[\frac{\mathrm{Re}[\widehat{f}](\bU_1)}{|\mathrm{Re}[\widehat{f}](\bU_1)|}  \cos(2\pi \bU_1\cdot \xx)\right] + (1-\pf)\EE\left[\frac{\mathrm{Im}[\widehat{f}](\bV_1)}{|\mathrm{Im}[\widehat{f}](\bV_1)|}
\cos\left(\frac{\pi}{2}+2 \pi \bV_1\cdot\xx\right)\right]\right)
\\ & =  \LOnefhat \left(
\pf\int_{\RR^d} \frac{\mathrm{Re}[\widehat{f}](\bx)}{|\mathrm{Re}[\widehat{f}](\bx)|}  \cos(2\pi \bx \cdot \xx) \nu_1(\D\bx) + (1-\pf)\int_{\RR^d} \frac{\mathrm{Im}[\widehat{f}](\bx)}{|\mathrm{Im}[\widehat{f}](\bx)|}
\cos\left(\frac{\pi}{2}+2 \pi \bx \cdot \xx\right)\nu_0(\D\bx)]\right)
\\ & = \LOnefhat \left(\pf\int_{\RR^d} \frac{\mathrm{Re}[\widehat{f}](\bx)}{\int_{\RR^d} |\mathrm{Re}[\widehat{f}(z)]| \D z}  \cos(2\pi \bx \cdot \xx) \D\bx  + (1-\pf)\int_{\RR^d} \frac{\mathrm{Im}[\widehat{f}](\bx)}{\int_{\RR^d}|\mathrm{Im}[\widehat{f}](z)| \D z} \cos\left(\frac{\pi}{2}+2 \pi \bx \cdot \xx\right) \D\bx\right)
\\ & = \int_{\RR^d} {\mathrm{Re}[\widehat{f}](\bx)}  \cos(2\pi \bx \cdot \xx) \D\bx  + \int_{\RR^d} {\mathrm{Im}[\widehat{f}](\bx)} \cos\left(\frac{\pi}{2}+2 \pi \bx \cdot \xx\right) \D\bx 
 = f(\xx). 
\end{aligned}
\]
In particular, using the i.i.d.\ assumption and Fubini, we obtain
\begin{equation}\label{eq:L2estimate}\begin{aligned}
\EE\left[ \int_{\RR^d} |f(\xx) - F(\xx)|^2 \mu(\D \xx) \right]
%& = \int_{\RR^d} \EE\left[  |f(\xx)-F(\xx)|^2 \right] \mu(\D\xx)\\
& = \int_{\RR^d} \VV[F(\xx)] \mu(\D \xx) 
\\ & = \frac{1}{n^2} \int_{\RR^d} \VV\left[\sum_{i=1}^n W_i \cos(B_i + \mathbf{A}_i\cdot \xx)\right] \mu(\D\xx)
\\ & = \frac{1}{n} \int_{\RR^d} \VV\left[ W_1 \cos(B_1 + \mathbf{A}_1\cdot \xx)\right] \mu(\D\xx)
\\ & \leq \frac{1}{n} \int_{\RR^d} \EE\left[\left(W_1 \cos(B_1 + \mathbf{A}_1\cdot \xx)\right)^2\right] \mu(\D\xx)
\\ & \leq \frac{1}{n}  \EE\left[W_1^2\right] 
 \leq  \frac{1}{n} \left(\int_{\RR^d} |\widehat{f}(\bx)| \D\bx\right)^2
= \frac{1}{n} \LOnefhat^2.
\end{aligned}
\end{equation}
Since $\P(Z\leq B)>0$ for any non-negative random variable~$Z$ with $\EE[Z]\leq B$ for $B>0$, then
%Indeed, if  $\tilde{Z}\geq 0$ is a random variable with $\EE[\tilde{Z}]= 0$, 
%then necessarily $\tilde{Z}=0$ $\P$-a.s. 
%Thus, in our case $\P(Z\leq B)=0$ would imply that
% $\tilde{Z}=Z-B$ satisfies $\tilde{Z}> 0$ $\P$-a.s. and thus $\EE[Z]-B=\EE[\tilde{Z}]> 0$.  
there exists $\omega \in \Omega$ such that 
$F_\omega(\xx) = \frac{1}{n}\sum_{i=1}^n W_i(\omega) \cos(B_i(\omega)+\mathbf{A}_i(\omega)\cdot\xx)$ satisfies
$$
\int_{\RR^d} |f(\xx)-F_\omega(\xx)|^2 \mu(\D\xx) \leq  \frac{1}{n} \left(\int_{\RR^d} |\widehat{f}(\bx)| \D\bx\right)^2.
$$
It remains to be shown that $F_\omega=f_{\ttheta}$ for a suitable choice of weights
$\ttheta=(\ab^{i},b^{i},\gamma^{i})_{i=1,\ldots,n}$.
But this follows directly by choosing $b^{i}=B_i(\omega)$, 
$\ab^{i}=\mathbf{A}_i(\omega)$ and $\gamma^{i} = \arccos(\frac{W_i(\omega)}{R})$ (so that $R\cos\left(\gamma^{i}\right)= W_i(\omega) $), which is well defined because 
$W_i(\omega) = \LOnefhat \phi_i$ for some $\phi_i \in \{-1,1\}$ and thus $|\frac{W_i(\omega)}{R}|\leq 1$ given the constraint $\LOnefhat\leq R$. Therefore, 
$$
\left(\int_{\RR^d} |f(\xx)-f_{\ttheta}(\xx)|^2 \mu(\D\xx)\right)^{1/2} \leq  \frac{\LOnefhat}{\sqrt{n}}.
$$
\end{proof}

\subsection{Universal approximation by variational quantum circuits}

As a corollary of Theorem~\ref{thm:Approx} we obtain the following universal approximation result:

\begin{corollary} \label{cor:universality}
Let $\mu$ be a probability measure on $\RR^d$ and let $f \in L^2(\RR^d,\mu)$. 
Then for any $\varepsilon >0$ there exist $n \in \N$, $R>0$ and $\ttheta\in\TTheta$ such that 
$\Cg_{\nq}(\ttheta,\xx)$ outputs a function $f_{\ttheta}$ with 
\begin{equation}\label{eq:universality}
\left(\int_{\RR^d} |f(\xx)-f_{\ttheta}(\xx)|^2 \mu(\D\xx)\right)^{1/2} \leq  \varepsilon.
\end{equation}
\end{corollary}
\begin{proof}
We first show that~$f$ can be approximated up to error $\frac{\varepsilon}{2}$ by a function in 
$\Cc_c^\infty(\RR^d)$. This follows by standard arguments, which we give for completeness. 
By \cite[Lemma~1.35]{Kallenberg2002} there exists a bounded continuous function $g \colon \RR^d \to \R$ with $\left|\int_{\RR^d} |f(\xx)-g(\xx)|^2 \mu(\D\xx)\right|^{\half} \leq  \frac{\varepsilon}{6}$. Denote by $\chi_m \colon \RR^d \to [0,1]$ a continuous function with $\chi_m(\xx)=1$ for $\xx \in [-m,m]^d$ and $\chi_m(\xx)=0$ for $\xx \in \RR^d \setminus [-m-1,m+1]^d$. Then 
$\left(\int_{\RR^d} |g(\xx)-\chi_m(\xx)g(\xx)|^2 \mu(\D\xx)\right)^{\half} \leq \sup_{x \in \RR^d } |g(\xx)| \mu(\{\RR^d \setminus [-m,m]^d\}) \leq \frac{\varepsilon}{6}$ for $m$ chosen sufficiently large. 
Finally, since $\Cc_c^\infty(\RR^d)$ is dense in $\Cc_c(\RR^d)$ in the supremum norm, 
there exists $h \in \Cc_c^\infty(\RR^d)$ with 
\begin{equation}\label{eq:auxEq1}
\left(\int_{\RR^d} |f(\xx)-h(\xx)|^2 \mu(\D\xx)\right)^{\half} \leq \|f-g\|_{L^2(\mu)} + \|g-\chi_m g \|_{L^2(\mu)} + \|\chi_m g - h \|_{L^2(\mu)} \leq \frac{\varepsilon}{2}.
\end{equation}

Now we show how to apply Theorem~\ref{thm:Approx} to~$h$. 
Since~$h$ is a Schwartz function, its Fourier transform $\widehat{h}$ is one as well and in particular~$h$ 
and~$\widehat{h}$ are both integrable. Choosing $R = L^{1}[\widehat{h}]$ and $n=\lceil (2C_h \varepsilon^{-1})^2 \rceil $, Theorem~\ref{thm:Approx} yields the existence of $\ttheta\in\TTheta$ such that 
\[
\left(\int_{\RR^d} |h(\xx)-f_{\ttheta}(\xx)|^2 \mu(\D\xx)\right)^{\half} \leq  \frac{ L^{1}[\widehat{h}]}{\sqrt{n}} \leq \frac{\varepsilon}{2}. 
\]
This estimate together with~\eqref{eq:auxEq1} then imply~\eqref{eq:universality}, as claimed. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variational quantum circuits with randomly generated weights}\label{sec:ReservoirQ}

In this section we  construct a quantum circuit in the spirit of reservoir computing: the parameters inside the circuit are randomly generated and only a final layer of weights after measuring the quantum state is trainable. 
This has the advantage that training such a circuit only requires us to solve a linear regression problem, as opposed to generic quantum circuits which are typically trained using gradient-based methods. 

In the notation above, we could consider a circuit in which the weights $(b^{i},\ab^{i})_{i=1,\ldots,n}$ are randomly sampled and then fixed, and only the remaining weights $(\gamma^{i})_{i=1,\ldots,n}$ are trained over. 
Here we consider an even simpler circuit which does not depend on $\gamma^{1},\ldots,\gamma^{(n)}$, but instead on some trainable ``readout weights'' appearing only after measurement. 


\subsection{Approximation error bounds for a random universal quantum circuit}
\label{subsec:reservoirCircuit}

Let $n \in \N$,
$\Bf=(B^{i})_{i=1,\ldots,n}$ i.i.d.\ with $\half$-Bernoulli distribution and 
$\Af=(\Af^{i})_{i=1,\ldots,n}$ i.i.d.\ with density~$\pi_a$, and~$\Af$ and~$\Bf$ independent,
and $\Af^i = (A^i_j)_{1\leq j \leq d}$ for each $i=1,\ldots, n$.
%We now build a quantum circuit using these random variables. 
For an input $\xx \in \RR^d$ we consider,
%the gate $\Ug^{(i)}_1$ with parameters $(2\pi A^{(i)},\frac{\pi}{2} B^{(i)})$, i.e., 
for $i=1,\ldots, n$,
$$
\overline{\Ug}^{(i)}_1(\xx)
 := \Ug^{(i)}_1\left(2\pi \mathbf{A}^{i},\frac{\pi}{2} B^{i},\xx\right)
 = \Hg \, \Rgz\left(-B^{i}\frac{\pi}{2}\right) \Rgz\left(-2\pi {A}^{i}_d x_d\right) \cdots 
 \Rgz\left(-2 \pi {A}^{i}_1 x_1\right) \Hg.
$$
Similarly to Section~\ref{subsec:circuit} we now use these gates to build the (random) block matrix 
\[
\overline{\Ug} := \overline{\Ug}(\xx) := 
\begin{bmatrix}
	\overline{\Ug}^{(1)}_1(\xx) & \mathbf{0}_{2 \times 2}  & \mathbf{0}_{2 \times 2} & \cdots &  \mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times \overline{n}_0} \\
	\mathbf{0}_{2 \times 2} & \overline{\Ug}^{(2)}_1(\xx) & \mathbf{0}_{2 \times 2} & \cdots &  \mathbf{0}_{2 \times 2} & \vdots  \\
	\vdots &  & \ddots &  & \vdots & \vdots \\
	\mathbf{0}_{2 \times 2} & \cdots &  \mathbf{0}_{2 \times 2} &  \overline{\Ug}^{(n-1)}_1(\xx) & \mathbf{0}_{2 \times 2} & \vdots 
	\\
	\mathbf{0}_{2 \times 2} & \cdots &  \cdots & \mathbf{0}_{2 \times 2} & \overline{\Ug}^{(n)}_1(\xx) & \mathbf{0}_{n \times \overline{n}_0}
	\\
	{\bf 0}_{\overline{n}_0 \times 2} & \cdots & \cdots & \cdots & {\bf 0}_{\overline{n}_0 \times 2} & {\bf 1}_{\overline{n}_0 \times \overline{n}_0}
\end{bmatrix},
\]
with $\overline{n}_0 \in \N_0$ the smallest positive integer such that $\log_2(2n+\overline{n}_0) \in \N$.  
Then $\overline{\Ug} \in \C^{(2n+\overline{n}_0) \times (2n+\overline{n}_0)}$ is a (random) unitary matrix and can be viewed as a gate operating on $\nqb := \lceil \log_2(2n) \rceil $ qubits. Let $\overline{N} := 2n+\overline{n}_0=2^{\nqb}$ and similarly to Section~\ref{subsec:circuit} let $\overline{\Vg} \in \C^{\overline{N} \times \overline{N}}$ be any unitary matrix that maps 
$\ket{0}^{\otimes \nqb}$ to the state $\ket{\overline{\psi}} = \frac{1}{\sqrt{n}} \sum_{i=0}^{n-1} \ket{2i}$. 

Consider now the circuit, which we call~$\overline{\Cg}_{\nqb}(\ttheta)$,
\begin{center}
	\Qcircuit{
		&\lstick{\ket{0}^{\otimes \nqb}} & \gate{\overline{\Vg}} & \gate{\overline{\Ug}(\xx)} & \meter 
	}
\end{center}
The possible measurement outcomes are $0,\ldots,\overline{N}-1$,
and we denote by $\PPb_k := \PPb_k(\xx)$ the probability that the measured state is equal to 
$k \in \{0,\ldots,\overline{N}-1\}$,
 which can be estimated by running the circuit, 
 as explained in Section~\ref{subsec:circuit}.
In contrast to Section~\ref{sec:variational}, here no parameter is trained/adjusted within the quantum circuit. 
The matrix parameters~$\Af$ and~$\Bf$ are randomly generated and then fixed, 
and  the subsequent inputs~$\xx$ are mapped through the fixed circuit to output probabilities 
$\PPb_k = \PPb_k(\xx) = \PPb_k^{\Af,\Bf}(\xx)$, for $k=0,\ldots,\overline{N}-1$. 

Mimicking the previous section, we introduce the space
\begin{equation}\label{eq:Space1Bar}
\Ffov := \Big\{f:\RR^d\to\RR: f \in \Cc(\RR^d) \cap L^1(\RR^d), \widehat{f}\in L^1(\RR^d), |\widehat{f}|\ll \pi_a, 
\int_{\RR^d} \frac{|\widehat{f}(\bx)|^2}{\pi_a(\bx)} \D\bx<\infty\Big\},
\end{equation}
where we write $\nu\ll \mu$ if $\mu(A) = 0$ implies $\nu(A) = 0$ to denote that a measure~$\nu$ is  absolutely continuous with respect to the measure~$\mu$,
and where, for a function $f\in\Ffov$, we denote
$$
\Lbarf := \left(2\int_{\RR^d} \frac{|\widehat{f}(\bx)|^2}{\pi_a(\bx)} \D\bx\right)^{1/2}.
$$

Given the operator $\overline{\Cg}_{\nqb}(\ttheta)$
and $\ww\in\RR^n$, introduce the map $F_{\ww}:\RR^d \to\RR$ by
\begin{equation}\label{eq:randomF}
F_{\ww}(\xx) := \sum_{j=0}^{n-1} w_j \left(2\PPb_{2j}(\xx)-\frac{1}{n}\right).
\end{equation}
The following theorem provides an approximation error bound for such a circuit. 
The approximation bound proved here is a quantum analogue  to the bounds obtained for classical random feature neural networks in \cite{Gonon2021, RC12}.

\begin{theorem}\label{thm:random}
For any $n \in \N$, $f\in\Ffov$,
 there exists an $\RR^n$-valued $\sigma(\Af,\Bf)$-measurable random vector~$\Wf$ such that 
$$
\EE\left[\int_{\RR^d} \left|F_{\Wf}(\xx)-f(\xx)\right|^2 \mu(\D\xx)\right]^{1/2} \leq \frac{\Lbarf}{\sqrt{n}}.
$$
\end{theorem}

The proof of Theorem~\ref{thm:random} is quite similar to that of Theorem~\ref{thm:Approx}, it follows by combining Propositions~\ref{prop:FourierApproxRandom} and \ref{prop:circuitOutputRandom} below.

\begin{example}
As an example, consider $d=1$ and let~$\pi_a$ be the density of a~$t_1$-distribution. 
Then the condition 
$\Lbarf < \infty$ translates to $\int_{\RR} (1+|\bx|^2)|\widehat{f}(\bx)|^2 \D\bx < \infty$, namely $f \in \Hh^1(\R)$. 
Thus~$f$ satisfies the conditions of Proposition~\ref{prop:FourierApproxRandom} if it has a weak derivative~$f'$ and if both~$f$ and~$f'$ are square-integrable. 
	
More generally, for $d \in \N$ and~$\pi_a$ the density of a $t_{\nu}(0,\mathbbm{1}_d)$-distribution (with $\nu>0$),
the condition $\Lbarf < \infty$ is equivalent to 
$\int_{\RR^d} |\widehat{f}(\bx)|^2(1+\|\bx\|^2)^{(\nu+d)/2} \D\bx < \infty$, 
which means that~$f$ needs to belong to the Sobolev space $\Hh^{\frac{\nu+d}{2}}(\RR^d)$.
\end{example}

The next result shows that in the case where $f \in \Hh^{s}(\RR^d)$ for some $s>\frac{d}{2}$ we may in fact generate~$\mathbf{A}^{i}$ from a distribution arbitrary close to any given  density, at the expense of a possibly large constant in the error bound. 
\begin{corollary} 
Fix $s>\frac{d}{2}$.
Let $\varphi$ be an arbitrary probability density, 
$\nu = 2(s-\frac{d}{2})$ and~$t_\nu$ the density of a  $t_{\nu}(0,\mathbbm{1}_d)$-distribution. 
For $\delta \in (0,1)$, set $\pi_a := \delta t_\nu + (1-\delta) \varphi$. 
Let $n \in \N$ and let $F_w$  be the function realised by the quantum circuit in Theorem~\ref{thm:random} with weights~$\Af^i$ distributed according to~$\pi_a$ and $B^{i}$ distributed according to a $\half$-Bernoulli distribution. 
%\lukas[inline]{added here distribution of $B^i$}
	Then for any $f \in \Hh^{s}(\RR^d) \cap L^1(\RR^d)$
	there exists a $\sigma(\Af,\Bf)$-measurable random vector~$\Wf$ and a constant $C_{f,\delta}$ independent of~$n$ such that 
	\[
	\EE\left[\int_{\RR^d} \left|F_{\Wf}(\xx)-f(\xx)\right|^2 \mu(\D\xx)\right]^{1/2} \leq \frac{C_{f,\delta}}{\sqrt{n}}.
	\]
\end{corollary}
\begin{proof}
	The assumption on $f$ guarantees that $f \in \Cc(\RR^d)$ and  $\widehat{f} \in L^1(\RR^d)$. In addition, $\pi_a$ is strictly positive and so  $|\widehat{f}|\ll \pi_a$ holds, because $\pi_a(\xx)\neq 0$ for all $\xx \in \RR^d$.  %\jack{not sure I understand} \lukas{does it make sense now? $\pi_a(x)\neq 0$ for all $x$ means that $\pi_a(x)= 0$ never happens (and so, if $\pi_a(x)= 0$ then also $|\widehat{f}|(x)=0$, because $\pi_a(x)= 0$ never happens. Do you see a way of writing this more clearly?}
	Finally, $\pi_a \geq \delta t_\nu$ and thus inserting $t_\nu(\bx) = \frac{\Gamma((\nu+d)/2)}{\Gamma(\nu/2) \nu^{d/2} \pi^{d/2}} (1 + \nu^{-1} \|\bx\|^2)^{-(\nu+d)/2}$ yields
	\begin{equation} \label{eq:auxEq4} \begin{aligned}
			\int_{\RR^d} \frac{|\widehat{f}(\bx)|^2}{\pi_a(\bx)} \D\bx & \leq 
			\int_{\RR^d} \frac{|\widehat{f}(\bx)|^2}{\delta t_\nu(\bx)} \D\bx 
			\\ & \leq 
			\frac{1}{\delta} \frac{\Gamma(\frac{\nu}{2}) \nu^{d/2} \pi^{d/2}}{\Gamma(\frac{\nu+d}{2})}		\int_{\RR^d} |\widehat{f}(\bx)|^2(1 + \nu^{-1} \|\bx\|^2)^{(\nu+d)/2} \D\bx 
			\\ & \leq 
			\frac{1}{\delta} \frac{\Gamma(\frac{\nu}{2}) \nu^{d/2} \pi^{d/2}}{\Gamma(\frac{\nu+d}{2})} \max(1,\nu^{-1})^{(\nu+d)/2}	\int_{\RR^d} |\widehat{f}(\bx)|^2(1 + \|\bx\|^2)^{(\nu+d)/2} \D\bx,
	\end{aligned}	
 \end{equation}
	which is finite since $f \in \Hh^s(\RR^d)$ and $s =  (\nu+d)/2$. 
	The hypotheses of Theorem~\ref{thm:random} are thus satisfied and so the claim follows from Theorem~\ref{thm:random} and the bound~\eqref{eq:auxEq4}. 
\end{proof}


\subsection{Proof of Theorem~\ref{thm:random}}
Propositions~\ref{prop:FourierApproxRandom} and~\ref{prop:circuitOutputRandom} are the essential tools, directly implying Theorem~\ref{thm:random}.
For $\ww \in\RR^n$, mimicking~\eqref{eq:fct},
introduce the (random) map $G_{\ww}:\RR^n\to\RR$ and,
for each $i=1,\ldots, n$, the linear map $L^i$ by

\begin{equation}\label{eq:randomG}
G_{\ww}(\xx) := \frac{1}{n}\sum_{i=1}^{n} W_i \cos\left(L^i(\xx)\right),
\qquad\text{with}\qquad
L^i(\xx) := \frac{\pi}{2} B^{i}+2 \pi \Af^{i} \cdot \xx.
\end{equation}

\begin{proposition}\label{prop:FourierApproxRandom}
%Let $n \in \N$, $\Bf=(B^{(i)})_{i=1,\ldots,n}$ i.i.d.\ with $\half$-Bernoulli distribution and $\Af=(A^{(i)})_{i=1,\ldots,n}$ i.i.d.\ with density~$\pi_a$, and~$\Af$ and~$\Bf$ independent.
 For any $f \in\Ffov$, there exist a $\sigma(\Af,\Bf)$-measurable random vector~$\Wf$ such that the random function~$G_{\Wf}$ in~\eqref{eq:randomG} satisfies
$$
\EE\left[\int_{\RR^d} \left|G_{\Wf}(\xx)-f(\xx)\right|^2 \mu(\D \xx)\right]^{1/2} \leq \frac{\Lbarf}{\sqrt{n}}.
$$
\end{proposition}
\begin{proof}
First, note that, for each $i\in\{1,\ldots, n\}$, the random variable
\[
W_i := \frac{2}{\pi_a(\mathbf{A}^{i})} 
\left\{\left(1-B^{i}\right) \mathrm{Re}[\widehat{f}](\mathbf{A}^{i})
+ B^{i}\mathrm{Im}[\widehat{f}](\mathbf{A}^{i}) \right\},
\]
is well defined since $|\widehat{f}|\ll \pi_a$. 
The independence and the i.i.d.\ assumptions then yield
\[
\begin{aligned}
\EE\left[G_{\Wf}(\xx)\right]
& = \EE\left[W_1 \cos\left(L^1(\xx)\right)\right]
 =  \int_{\RR^d} 
\left\{\mathrm{Re}[\widehat{f}](\bx) \cos(2 \pi \bx \cdot \xx)  + \mathrm{Im}[\widehat{f}](\bx) \cos\left(\frac{\pi}{2}+2 \pi \bx \cdot \xx\right)\right\} \D\bx
 = f(\xx),
\end{aligned}
\]
where the last step follows by~\eqref{eq:auxEq2}. 
Proceeding as in the proof of Proposition~\ref{prop:FourierApprox}, we obtain
\[\begin{aligned}
\EE\left[ \int_{\RR^d} \left|f(\xx) - G_{\Wf}(\xx)\right|^2 \mu(\D\xx) \right] & = \int_{\RR^d} \VV\left[G_{\Wf}(\xx)\right] \mu(\D\xx) 
	\\ & = \frac{1}{n^2} \int_{\RR^d} \VV\left[\sum_{i=1}^n W_i \cos\left(L^i(\xx)\right)\right] \mu(\D\xx)
	\\ & = \frac{1}{n} \int_{\RR^d} \VV\left[ W_1 \cos\left(L^1(\xx)\right)\right] \mu(\D\xx)
	\\ & \leq \frac{1}{n} \int_{\RR^d} \EE\left[\left(W_1 \cos\left(L^1(\xx)\right)\right)^2\right] \mu(\D\xx)
	\\ & \leq \frac{1}{n}  \EE\left[W_1^2\right]
	  =  \frac{2}{n} \int_{\RR^d} 
	\frac{1}{\pi_a(\bx)}  \left( \mathrm{Re}[\widehat{f}](\bx)^2 + \mathrm{Im}[\widehat{f}](\bx)^2  \right) \D\bx,
\end{aligned}
\]
which implies the claimed bound.
\end{proof}

\begin{remark} As mentioned earlier, the proof above yields an analogous result for the circuit constructed in Section~\ref{sec:variational}. 
Indeed, choosing $R=\max(W_1,\ldots,W_n)$, 
then
$\Gamma^{i} = \arccos(\frac{W_i}{R})$ is well defined as a $\sigma(\Af,\Bf)$-measurable random variable, and so is~$R$, so that the random function
$\overline{F}(\xx) :=  \frac{1}{n}\sum_{i=1}^{n} R\cos\left(\Gamma^{i}\right) \cos(\frac{\pi}{2} B^{i}+2 \pi \Af^{i} \cdot \xx)$
satisfies
\[
\EE\left[\int_{\RR^d} \left|\overline{F}(\xx)-f(\xx)\right|^2 \mu(\D\xx)\right]^{1/2} \leq \frac{\Lbarf}{\sqrt{n}}.
\]
\end{remark}

The next result shows that the functions~$G_{\ww}$ employed in Proposition~\ref{prop:FourierApproxRandom} can indeed be realised by the considered quantum circuit. 


\begin{proposition}\label{prop:circuitOutputRandom}
For $n \in \N$, given~$\overline{\Cg}_{\nqb}(\ttheta)$, and $\ww \in \RR^n$,
the identity $F_{\ww}\equiv G_{\ww}$ holds on~$\RR^n$.
%\xx) := \displaystyle \sum_{j=0}^{n-1} w_j \left[2\PP_{2j}(\xx)-\frac{1}{n}\right]$. Then
%\[
%F_{\ww}(\xx) = \frac{1}{n} \sum_{i=1}^{n} w_i \cos\left( \frac{\pi}{2} B^{(i)}+2\pi a^{(i)}\xx\right).
%\]
\end{proposition}
\begin{proof}
The proof requires the computation of the probabilities~$\PPb_k$ for~$k$ even. 
First,
$$
\overline{\Ug}\, \overline{\Vg} \ket{0}^{\otimes \nqb}  = \overline{\Ug} \ket{\overline{\psi}}  =  \frac{1}{\sqrt{n}} \sum_{j=0}^{n-1} \overline{\Ug} \ket{2j}  
 =
\frac{1}{\sqrt{n}} \sum_{j=0}^{n-1}  \left\{\left[\overline{\Ug}^{(j+1)}_1\right]_{1,1} \ket{2j} +  \left[\overline{\Ug}^{(j+1)}_1\right]_{2,1} \ket{2j+1}\right\}.
$$
Therefore, for $k=2j+m$, $j \in \{0,\ldots,n-1\}, m \in \{0,1\}$, 
\begin{equation}\label{eq:auxEq3} 
\PPb_k  =  \left|\bra{k} \overline{\Ug}\, \overline{\Vg} \ket{0}^{\otimes \nqb}\right|^2
 =
 \frac{1}{n} 
 \left| \left[\overline{\Ug}^{(j+1)}_1\right]_{m+1,1}  \right|^2.
\end{equation}
Similarly as in the proof of Proposition~\ref{prop:circuitOutput} we now obtain for $i=j+1$ that
\[ \begin{aligned}
\left[\overline{\Ug}^{(i)}_1\right]_{1,1} & = \left[\Hg \begin{pmatrix} \E^{\frac{\I}{2} L^i(\xx)} & 0 \\ 0 & \E^{-\frac{\I}{2} L^i(\xx)} \end{pmatrix}  \Hg \right]_{1,1}
	 =\frac{1}{\sqrt{2}} \left[\Hg \begin{pmatrix} \E^{\frac{\I}{2} L^i(\xx)} & \E^{\frac{\I}{2} L^i(\xx)} \\ \E^{-\frac{\I}{2} L^i(\xx)} & -\E^{-\frac{\I}{2} L^i(\xx)} \end{pmatrix}   \right]_{1,1}
	 = \cos\left(\frac{L^i(\xx)}{2}\right).
%\\
%	[\overline{\Ug}^{(i)}_1]_{2,1} &  = \I \sin(\frac{1}{2}( B^{(i)}+a^{(i)} x))
\end{aligned} 
\]
By plugging this into~\eqref{eq:auxEq3} and using the double-angle formula we then obtain
$$
\PPb_{2(i-1)}
=  \frac{1}{n} \left|\cos\left(\frac{1}{2}L^i(\xx)\right)\right|^2
=  \frac{1}{2n} \cos\left(L^i(\xx)\right) + \frac{1}{2n}.
$$
Inserting this expression into $F_{\ww}$ in~\eqref{eq:randomF} then yields the claimed representation. 
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$L^\infty$-error bounds for variational quantum circuits}
\label{sec:variationalInfty}
In this section we prove approximation error bounds for variational quantum circuits in the case when the error is measured with respect to the uniform norm on compacts. 

\subsection{$L^\infty$-error bounds for trainable variational quantum circuits}

For $R>0$, let~$\Ff_{R}$ be as above in~\eqref{eq:Space1R}; 
for $f \in \Ff_{R}$ let
\begin{equation}
\|f\|_{\mathcal{B}_2} := \left(\int_{\RR^d} \|\bx\|^2 |\widehat{f}(\bx)|\D \bx\right)^{\frac{1}{2}},
\qquad\text{and}\qquad
\mathcal{B}_R := \{f \in \Ff_{R} \, \colon \, \|f\|_{\mathcal{B}_2} < \infty  \}
\end{equation}
be a subset of $\Ff_{R}$ with further integrability properties of the Fourier transform. For functions in $\mathcal{B}_R$ we can complement the $L^2(\R^d,\mu)$-error bound in Theorem~\ref{thm:Approx} by a uniform error bound on compact sets.
\begin{theorem}\label{thm:ApproxUniform} 
	For any $R,M>0$, $f \in \mathcal{B}_{R}$ and
	$n\in\NN$, there exists~$\ttheta\in\TTheta$ such that 
	$$
	\sup_{\xx \in [-M,M]^d} 
	\left|f(\xx) - \ftrn(\xx)\right| \leq  \frac{2 (\pi+1) L^{1}[\widehat{f}] 
		+ 8 \pi  M d^{\frac{1}{2}}
		\LOnefhat^{\frac{1}{2}}\|f\|_{\mathcal{B}_2}}{\sqrt{n}}.
	$$
	%	with $C_f = \int_{\RR^d} |\widehat{f}(\bx)| \D\bx$. 
\end{theorem}
\begin{proof} The proof is identical to that of Theorem~\ref{thm:Approx}, except that the $L^2$-bound in \eqref{eq:L2estimate} is replaced by an $L^\infty$-bound, and we use the same notation as there. 
Define first for all $\xx \in [-M,M]^d$, $i=1,\ldots,n$ the random variables  $U_{i,\xx} = W_i \cos(B_i+\mathbf{A}_i\cdot \xx)$. Then the $L^\infty$-error that we aim to bound can be rewritten as 
\[\begin{aligned}
	\EE\left[ \sup_{x \in [-M,M]^d} |f(\xx) - F(\xx)| \right]
	& = \EE\left[ \sup_{x \in [-M,M]^d} \left| 
 \frac{1}{n}\sum_{i=1}^n \left(U_{i,\xx}-\EE[U_{i,\xx}]\right)\right| \right] .
\end{aligned}
\]
Let $\varepsilon_1,\ldots,\varepsilon_d$ be i.i.d.\ Rademacher random variables independent of $\mathbf{A}=(\mathbf{A}_1,\ldots,\mathbf{A}_n)$ and $\mathbf{B}=(B_1,\ldots,B_n)$. Using symmetrisation, we then obtain 
\[\begin{aligned}
	\EE\left[ \sup_{x \in [-M,M]^d} |f(\xx) - F(\xx)| \right]
	& \leq 2 \EE\left[ \sup_{x \in [-M,M]^d} \left| \frac{1}{n}\sum_{i=1}^n \varepsilon_i U_{i,\xx}\right| \right] .
\end{aligned}
\]
For any $\ab=(\ab_1,\ldots,\ab_n) \in \R^d \times \cdots \times \R^d$, $\mathbf{b}=(b_1,\ldots,b_n) \in \R^n$ consider the set 
\[T_{\ab,\mathbf{b}} 
:= \{
(l^{i}(\xx))_{i=1,\ldots,n} \,\colon \, \xx \in [-M,M]^d 
\},
\]
with the function~$l^i(\xx) = b_i+\ab_i\cdot \xx$,
and for $\mathbf{w}=(w_1,\ldots,w_n) \in \R^n$ with $|w_i|\leq L^{1}[\widehat{f}]$  define the maps $\varrho_{w_i} \colon \R \to \R$ via $\varrho_{w_i}(x) = \frac{w_i}{L^{1}[\widehat{f}]} (\cos(x)-1)$. 
Then independence yields that
\[\begin{aligned}
\EE\left[ \sup_{\xx \in [-M,M]^d} \left| \frac{1}{n}\sum_{i=1}^n \varepsilon_i U_{i,\xx}\right| \right]  & = \EE\left[ \left. \EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \cos(l^{i}(\xx))  \right| \right]  \right|_{(\mathbf{w},\ab,\mathbf{b})=(\mathbf{W},\mathbf{A},\mathbf{B})} \right].
\end{aligned}\]
and with the above definitions and Jensen's inequality we obtain  
\[\begin{aligned}
\EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \cos(l^{i}(\xx))  \right| \right]  & \leq L^{1}[\widehat{f}]  \EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i \varrho_{w_i} (l^{i}(\xx)) \right| \right] 
 + \EE\left[ \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \right| \right] 
\\ & \leq  L^{1}[\widehat{f}]  \EE\left[ \sup_{\mathbf{t} \in T_{\ab,b}} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i \varrho_{w_i} (t_i) \right| \right] + \VV\left[\frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \right]^{\frac{1}{2}}.
\end{aligned}
\]
The fact that $\varrho_{w_i}(0)=0$ and that $\varrho_{w_i}$ is $1$-Lipschitz allows us to apply the comparison theorem \cite[Theorem~4.12]{Ledoux2013}
\[
\EE\left[ \sup_{\mathbf{t} \in T_{\ab,b}} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i \varrho_{w_i} (t_i) \right| \right] \leq 2\EE\left[ \sup_{\mathbf{t} \in T_{\ab,b}} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i t_i \right| \right].
\] 
Using this, the fact that $\varepsilon_1,\ldots,\varepsilon_n$ are i.i.d.\ Rademacher random variables we thus obtain
\[\begin{aligned}
	& \EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \cos(b_i+\ab_i\cdot \xx)  \right| \right]  
	\\ & \leq 2 L^{1}[\widehat{f}]  \EE\left[ \sup_{\mathbf{t} \in T_{\ab,b}} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i t_i \right| \right] + \VV\left[\frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \right]^{\frac{1}{2}}
	\\ & = 2 L^{1}[\widehat{f}]  \EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i (l^{i}(\xx)) \right| \right] + \frac{1}{n} \left(\sum_{i=1}^n\VV\left[ \varepsilon_i w_i \right]\right)^{\frac{1}{2}}
	\\ & \leq 2 L^{1}[\widehat{f}]  \left( \EE\left[\left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i b_i \right| \right]
	+
\EE\left[ \sup_{\xx \in [-M,M]^d} \left| \xx \cdot \frac{1}{n}\sum_{i=1}^n \varepsilon_i \ab_i  \right| \right]
	\right)  + \frac{\left\| \mathbf{w} \right\|}{n}.
\end{aligned}
\]
We already estimated the first term (with~$b_i$ instead of~$w_i$) as
\[
\EE\left[\left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i b_i \right| \right] \leq  \frac{\left\| \mathbf{b} \right\|}{n}.
\]
For the second term, we use the fact that $\EE[\varepsilon_i \varepsilon_j]=\delta_{i,j}$ to obtain
\[ 
\begin{aligned}
\EE\left[ \sup_{\xx \in [-M,M]^d} \left| \xx \cdot \frac{1}{n}\sum_{i=1}^n \varepsilon_i \ab_i  \right| \right]  \leq \sup_{\xx \in [-M,M]^d} \| \xx \|_{\infty}
\EE\left[  \left\|\frac{1}{n}\sum_{i=1}^n \varepsilon_i \ab_i  \right\|_1 \right] 
& 
=
\frac{M}{n}
 \sum_{j=1}^d \EE\left[ \left|\sum_{i=1}^n  \varepsilon_i a_{i,j} \right| \right]
 \\ & \leq \frac{M}{n}
\sum_{j=1}^d \left(\sum_{i=1}^n a_{i,j}^2 \right)^{\frac{1}{2}}
\end{aligned}
\] 
and thus 
\[\begin{aligned}
	& \EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \cos(l^{i}(\xx))  \right| \right]  
 \leq 2 L^{1}[\widehat{f}]  \left( \frac{\left\| \mathbf{b} \right\|}{n}
	+
\frac{M}{n}
\sum_{j=1}^d \left(\sum_{i=1}^n a_{i,j}^2 \right)^{\frac{1}{2}}
	\right)  + \frac{\left\| \mathbf{w} \right\|}{n}.
\end{aligned}
\]
Combining the obtained estimates and again using Jensen's inequality yields
\[\begin{aligned}
	\EE& \left[ \sup_{x \in [-M,M]^d} \left| \frac{1}{n}\sum_{i=1}^n \varepsilon_i U_{i,\xx}\right| \right]   = \EE\left[ \left. \EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i w_i \cos(l^{i}(\xx))  \right| \right]  \right|_{(\mathbf{w},\ab,\mathbf{b})=(\mathbf{W},\mathbf{A},\mathbf{B})} \right]
	\\  & \leq  2 L^{1}[\widehat{f}]  \left( \frac{\EE\left[ \left\| \mathbf{B} \right\|\right]}{n}
	+
	\frac{M }{n}
\sum_{j=1}^d \EE\left[ \left(\sum_{i=1}^n| A_{i,j}|^2 \right)^{\frac{1}{2}}\right]
	\right)  + \frac{\EE\left[\left\| \mathbf{W} \right\|\right]}{n} 
	\\ & \leq 2 L^{1}[\widehat{f}]  \left(\frac{1}{\sqrt{n}} \EE[B_1^2]^{\frac{1}{2}}
	+ \frac{M}{\sqrt{n}}
\sum_{j=1}^d \EE\left[| A_{1,j}|^2 \right]^{\frac{1}{2}}
	\right)  + \frac{ \EE\left[ \left| W_1 \right|^2 \right]^{\frac{1}{2}}}{\sqrt{n}} 
	\\ & \leq 2 L^{1}[\widehat{f}]  \left({\frac{\pi}{2}} \frac{1}{\sqrt{n}} 
	+ \frac{M d^{\frac{1}{2}}}{\sqrt{n}}
	\EE[\|\mathbf{A}_1\|^2]^{\frac{1}{2}}
	\right)  + \frac{L^{1}[\widehat{f}]}{\sqrt{n}}.
\end{aligned}
\]
Inserting the definition of $\mathbf{A}_1$ yields
\[\begin{aligned}
	\EE[\|\mathbf{A}_1\|^2]^{\frac{1}{2}} & = 	2 \pi \EE[\| Z_1 \bU_1+(1-Z_1)\bV_1\|^2]^{\frac{1}{2}}
\\ & = 	2 \pi \left(\pf \int_{\RR^d} \| \bx\|^2 \nu_1(\D \bx)  +(1-\pf)\int_{\RR^d} \| \bx\|^2 \nu_0(\D \bx) \right)^{\frac{1}{2}}
\\ & = 	2 \pi \left( \LOnefhat^{-1} \int_{\RR^d} \| \bx\|^2 |\mathrm{Re}[\widehat{f}](\bx)|\D \bx + \LOnefhat^{-1} \int_{\RR^d} \| \bx\|^2 |\mathrm{Im}[\widehat{f}](\bx)|\D \bx \right)^{\frac{1}{2}}
\\ & = 	2 \pi \LOnefhat^{-1/2 }\left( \int_{\RR^d} \| \bx\|^2 |\widehat{f}(\bx)|\D \bx \right)^{\frac{1}{2}}.
\end{aligned} 
\]
Overall, we obtain the bound
\[\begin{aligned}
\EE\left[ \sup_{x \in [-M,M]^d} |f(\xx) - F(\xx)| \right]
& \leq 4 L^{1}[\widehat{f}]  \left({\frac{\pi}{2}} \frac{1}{\sqrt{n}} 
+ \frac{M d^{\frac{1}{2}}}{\sqrt{n}}
\EE[\|\mathbf{A}_1\|^2]^{\frac{1}{2}}
\right)  + \frac{2 L^{1}[\widehat{f}]}{\sqrt{n}}
\\ & \leq \frac{2 (\pi+1) L^{1}[\widehat{f}] 
	+ 8 \pi  M d^{\frac{1}{2}}
	\LOnefhat^{\frac{1}{2}}\|f\|_{\mathcal{B}_2}}{\sqrt{n}}.
\end{aligned}
\]


%\[\begin{aligned}
%	\EE& \left[ \sup_{x \in [-M,M]^d} \left| \frac{1}{n}\sum_{i=1}^n \varepsilon_i U_{i,\xx}\right| \right]  
%	\\  & \leq  2 L^{1}[\widehat{f}] \EE\left[ \left. \EE\left[ \sup_{t \in T_{a,b}} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i t_i \right| \right]  \right|_{(w,a,b)=(W,A,B)} \right] + \frac{1}{n} \VV\left[\sum_{i=1}^n \varepsilon_i W_i \right]^{\frac{1}{2}}
%	\\ & = 2 L^{1}[\widehat{f}]  \EE\left[ \sup_{\xx \in [-M,M]^d} \left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i (B_i+\mathbf{A}_i \cdot \xx ) \right| \right]  + \frac{1}{n}\left(\sum_{i=1}^n \VV[\varepsilon_i W_i] \right)^{\frac{1}{2}}
%	\\ & \leq 2 L^{1}[\widehat{f}]  \left( \EE\left[\left|  \frac{1}{n}\sum_{i=1}^n \varepsilon_i B_i \right| \right]
%	+
%	\EE\left[ \left. \EE\left[ \sup_{\xx \in [-M,M]^d} \left| \xx \cdot \frac{1}{n}\sum_{i=1}^n \varepsilon_i \ab_i  \right| \right] \right|_{a=A} \right]
%	 \right)  + \frac{ \EE\left[ \left| W_1 \right|^2 \right]^{\frac{1}{2}}}{\sqrt{n}} 
%		\\ & \leq 2 L^{1}[\widehat{f}]  \left( \VV\left[\frac{1}{n}\sum_{i=1}^n \varepsilon_i B_i \right]^{\frac{1}{2}}
%	+ \sup_{\xx \in [-M,M]^d} \| \xx \|
%	\EE\left[ \left. \EE\left[ \left\|\frac{1}{n}\sum_{i=1}^n \varepsilon_i \ab_i  \right\|^2 \right]^{\frac{1}{2}} \right|_{a=A} \right]
%	\right)  + \frac{ \EE\left[ \left| W_1 \right|^2 \right]^{\frac{1}{2}}}{\sqrt{n}} 
%\\ & \leq 2 L^{1}[\widehat{f}]  \left(\frac{1}{n}\left(\sum_{i=1}^n \VV[\varepsilon_i B_i] \right)^{\frac{1}{2}}
%+ \frac{M d^{\frac{1}{2}}}{n}
%\EE\left[\left. \left(\sum_{i=1}^n \sum_{j=1}^n \EE[\varepsilon_i \varepsilon_j \ab_i \cdot \ab_j] \right)^{\frac{1}{2}} \right|_{a=A}\right]
%\right)  + \frac{ \EE\left[ \left| W_1 \right|^2 \right]^{\frac{1}{2}}}{\sqrt{n}} 
%\\ & \leq 2 L^{1}[\widehat{f}]  \left(\frac{1}{\sqrt{n}} \EE[B_1^2]^{\frac{1}{2}}
%+ \frac{M d^{\frac{1}{2}}}{\sqrt{n}}
%\EE[\|\mathbf{A}_1\|]
%\right)  + \frac{ \EE\left[ \left| W_1 \right|^2 \right]^{\frac{1}{2}}}{\sqrt{n}} 
%\\ & \leq 2 L^{1}[\widehat{f}]  \left({\frac{\pi}{2}} \frac{1}{\sqrt{n}} 
%+ \frac{M d^{\frac{1}{2}}}{\sqrt{n}}
%\EE[\|\mathbf{A}_1\|]
%\right)  + \frac{L^{1}[\widehat{f}]}{\sqrt{n}} 
%\end{aligned}
%\]
\end{proof}

\subsection{$L^\infty$-universal approximation theorem}
As a corollary to Theorem~\ref{thm:ApproxUniform} we obtain a universal approximation theorem for continuous functions on compact subsets of $\R^d$ and with error measured with respect to the uniform norm. 
\begin{corollary}
Let $\mathcal{X} \subset \RR^d$ be compact and $f \in \Cc(\mathcal{X},\RR)$.	Then for any $\varepsilon >0$ there exist $n \in \N$, $R>0$ and $\ttheta\in\TTheta$ such that $\Cg_{\nq}(\ttheta,\xx)$ outputs a function $f_{\ttheta}$ with 
\begin{equation}\label{eq:Linftyuniversality}
\sup_{\xx \in \mathcal{X}} |f(\xx)-f_{\ttheta}(\xx)|  \leq \varepsilon.
\end{equation}
\end{corollary}
\begin{proof} 
The proof proceeds similarly as in Corollary~\ref{cor:universality}. 
We first show that~$f$ can be approximated on $\mathcal{X}$ up to error $\frac{\varepsilon}{2}$ by a function in 	$\Cc_c^\infty(\RR^d)$.  Choose $M>0$ such that $\mathcal{X} \subset [-M,M]^d$. 
By Tietze's extension theorem~\cite[Theorem~20.4]{Rudin1987} there exists a bounded continuous function $F\colon \R^d \to \R$ which coincides with~$f$ on $\mathcal{X}$. Now let $\chi \colon \RR^d \to [0,1]$ a continuous function with $\chi(\xx)=1$ for $\xx \in [-M,M]^d$ and $\chi(\xx)=0$ for $\xx \in \RR^d \setminus [-M-1,M+1]^d$. Then, since $\Cc_c^\infty(\RR^d)$ is dense in $\Cc_c(\RR^d)$ in the supremum norm, there exists $h \in \Cc_c^\infty(\RR^d)$ with 
\begin{equation}\label{eq:auxEqCor}
	\sup_{x \in \RR^d} |F(\xx)\chi(\xx)-h(\xx)|  \leq \frac{\varepsilon}{2}.
\end{equation}
Now we show how to apply Theorem~\ref{thm:ApproxUniform} to~$h$. 
Since~$h$ is a Schwartz function, its Fourier transform $\widehat{h}$ is as well. In particular~$h$ 
and~$\widehat{h}$ are both integrable and also $\bx \mapsto \|\bx\|^2 |\widehat{h}(\bx)|$ is integrable over $\RR^d$. 
Choosing $R = L^{1}[\widehat{h}]$ and $n=\lceil (2(2 (\pi+1) L^{1}[\widehat{h}] 
	+ 8 \pi  M d^{\frac{1}{2}}
	L^{1}[\widehat{h}] ^{\frac{1}{2}}\|h\|_{\mathcal{B}_2}) \varepsilon^{-1})^2 \rceil $, Theorem~\ref{thm:ApproxUniform} yields the existence of $\ttheta\in\TTheta$ such that 
\[
\sup_{\xx \in [-M,M]^d} |h(\xx)-f_{\ttheta}(\xx)| \leq \frac{\varepsilon}{2}. 
\]
This estimate together with~\eqref{eq:auxEqCor} then imply
\begin{equation*}
	\sup_{\xx \in \mathcal{X}} |f(\xx)-f_{\ttheta}(\xx)|\leq 	\sup_{\xx \in \mathcal{X}} |F(\xx)\chi(\xx)-h(\xx)| + \sup_{\xx \in \mathcal{X}} |h(\xx)-f_{\ttheta}(\xx)|  \leq  \varepsilon.
\end{equation*}
\end{proof}


\subsection{$L^\infty$-error bounds for reservoir quantum circuits}

We can also obtain a uniform approximation result for a subset of  $\Ffov_b \subset \Ffov$. 
Let~$\Ffov_b$ be the set of functions in~$\Ffov$ for which $\widehat{f}/\pi_a$ is bounded.
% by using the quantum circuit constructed in Section~\ref{sec:ReservoirQ} and the function $
%F_{\ww}$
%with a specific choice of initial distribution $\pi_a$. 
In particular, given an arbitrary compact set, the result below guarantees existence of readout weights for the reservoir quantum circuit which achieve an arbitrarily small uniform approximation error.
Suppose that
\[
\EE\left[\|\mathbf{A}_1\|^2\right]^{\frac{1}{2}}
= \left(\int_{\RR^d} \|\bx\|^2 \pi_a(\bx) \D \bx \right)^{\frac{1}{2}} < \infty.
\]
	 
\begin{theorem}\label{thm:ApproxUniformStronger} 
	For any $M>0$, $f \in \Ffov_b$ and
	$n\in\NN$, there exists an 
 $\RR^n$-valued $\sigma(\Af,\Bf)$-measurable random vector~$\Wf$ such that 
$$
\EE\left[\sup_{\xx \in [-M,M]^d}  \left|F_{\Wf}(\xx)-f(\xx)\right|\right] \leq \frac{1}{\sqrt{n}} \left(8 \left\|\frac{\widehat{f}}{\pi_a}\right\|_\infty  \left(\frac{\pi}{2^{3/2}}
+ 2 \pi M d^{\frac{1}{2}}
\EE[\|\mathbf{A}_1\|^2]^{\frac{1}{2}}
\right)  +  \Lbarf \right).
$$
\end{theorem}
\begin{proof}
The proof follows by combining the arguments from the proofs of Theorem~\ref{thm:random} and Theorem~\ref{thm:ApproxUniform}. Analogously to the proof of the latter, we need to derive an $L^\infty$-error bound instead of the $L^2$-error bound in Proposition~\ref{prop:FourierApproxRandom}.
%Let us choose $\pi_a = \frac{|\widehat{f}|}{\LOnefhat}$, which is well-defined since  $f \in \Ff_{R}$.  For this choice of $\pi_a$ we have $\Lbarf = \sqrt{2} \LOnefhat < \infty$ and 
We may use $\EE[G_{\Wf}(\xx)]=f(\xx)$ to proceed precisely as in the proof of   Theorem~\ref{thm:ApproxUniform} and obtain
\[
\begin{aligned}
& \EE\left[\sup_{\xx \in [-M,M]^d} \left|G_{\Wf}(\xx)-f(\xx)\right| \right]
  \leq 8 \left\|\frac{\widehat{f}}{\pi_a}\right\|_\infty  \left(\frac{\pi}{2\sqrt{n}} \EE[B_1^2]^{\frac{1}{2}}
 + \frac{2 \pi M d^{\frac{1}{2}}}{\sqrt{n}}
\EE[\|\mathbf{A}_1\|^2]^{\frac{1}{2}}
\right)  + \frac{ \EE\left[ \left| W_1 \right|^2 \right]^{\frac{1}{2}}}{\sqrt{n}},
\end{aligned}
\]
where we used the boundedness of $\widehat{f}/\pi_a$ to guarantee that $w_i \leq 2 \|\frac{\widehat{f}}{\pi_a}\|_\infty$ and the functions~$\varrho_{w_i}$ can be chosen $1$-Lipschitz and the comparison theorem \cite[Theorem~4.12]{Ledoux2013} can be applied. 
We recall from the proof of Proposition~\ref{prop:FourierApproxRandom} that $\EE[|W_1|^2]^{\frac{1}{2}} =\Lbarf$ and calculate
 $\EE[B_1^2]=\frac{1}{2}$. Inserting these expressions, we obtain
 \[
 \begin{aligned}
 & \EE\left[\sup_{\xx \in [-M,M]^d} \left|G_{\Wf}(\xx)-f(\xx)\right| \right]
  \leq 8 \left\|\frac{\widehat{f}}{\pi_a}\right\|_\infty  \left(\frac{\pi}{2^{3/2}\sqrt{n}}
 + \frac{2 \pi M d^{\frac{1}{2}}}{\sqrt{n}}
 \EE[\|\mathbf{A}_1\|^2]^{\frac{1}{2}}
 \right)  + \frac{ \Lbarf}{\sqrt{n}}.
 \end{aligned}
 \]
\end{proof} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{abbrv}
\bibliography{references}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
