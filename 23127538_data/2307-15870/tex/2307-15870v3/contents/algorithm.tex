In this section, we propose a greedy algorithm for \InvReg frequency adaptation, which is an important component of our \method system.
We first analyze the convergence bound after $H$ rounds \textit{w.r.t.} both the \InvReg frequency and the \Reg frequency. 
Then we would present our algorithm design and explain how it contributes to model convergence.


\subsection{Convergence Analysis}
\label{analysis-section}
\vspace{-0.1cm}
Since the training is performed in an alternate manner, the convergence of the loss function relates to the training status in both the \srvUp and \locReg stages. 
For the sake of analysis, we make the following assumptions as suggested in \cite{li2019convergence, yang2021achieving, haddadpour2019convergence, ajalloeian2020convergence}:
\begin{assumption}
\label{assump1}
\vspace{-0.2cm}
    (Lipschiz Continuous Gradient) The loss function $F(\cdot)$ and loss components $f_s(\cdot)$ and $f_{u,i}(\cdot)$ of the entire model are L-smooth such that:
    \begin{equation}
    \begin{split}
        & \Vert \nabla F(\boldsymbol{x}) - \nabla F(\boldsymbol{y}) \Vert \leq L \Vert \boldsymbol{x} - \boldsymbol{y} \Vert, \forall \boldsymbol{x}, \forall\boldsymbol{y};\\
        & \Vert \nabla f_s(\boldsymbol{x}) - \nabla f_s(\boldsymbol{y}) \Vert \leq L \Vert \boldsymbol{x} - \boldsymbol{y} \Vert, \forall \boldsymbol{x}, \forall\boldsymbol{y};\\
        & \Vert \nabla f_{u,i}(\boldsymbol{x}) - \nabla f_{u,i}(\boldsymbol{y}) \Vert \leq L \Vert \boldsymbol{x} - \boldsymbol{y} \Vert, \forall \boldsymbol{x}, \forall\boldsymbol{y}. \\
        \nonumber
    \end{split}
    \end{equation}
\end{assumption}

\begin{assumption}
\label{assump2}
\vspace{-0.6cm}
    \bluenote{(Bounded Second Moments \cite{han2021accelerating, yang2021achieving})} There exist constants $G_s$ and $G_u$, such that the second moments of the stochastic gradients of the unsupervised loss and supervised loss on any data sample are upper bounded by:
    \begin{equation}
    \begin{split}
        & \Vert \nabla \ell_{u}(x, \boldsymbol{w}) \Vert^2 \leq G_u^2, \forall x, \forall \boldsymbol{w}; \\
        & \Vert \nabla \ell_{s}(x, \boldsymbol{w}) \Vert^2 \leq G_s^2, \forall x, \forall \boldsymbol{w}. \\  
        \nonumber
    \end{split}
    \end{equation}
\end{assumption}

\begin{assumption}
\label{assump3}
    \vspace{-0.6cm}
    (Unbiased Gradient Estimator) Let $\nabla f_s(\boldsymbol{w})$ and $\nabla f_{u,i}(\boldsymbol{w}_i)$ separately denote the gradients derived from the labeled data on the PS and from the unlabeled data of client $i$. The gradient estimators are unbiased as:
    \begin{equation}
    \begin{split}
        & \mathbb{E} \Vert \tilde{\nabla} f_{s}(\boldsymbol{w}) \Vert = \mathbb{E} \Vert \nabla f_{s}(\boldsymbol{w}) \Vert, \forall \boldsymbol{w};\\ 
        & \mathbb{E} \Vert \tilde{\nabla} f_{u, i}(\boldsymbol{w}) \Vert = \mathbb{E} \Vert \nabla f_{u, i}(\boldsymbol{w}) \Vert, \forall \boldsymbol{w}
        \nonumber.    
    \end{split}
    \end{equation}
\end{assumption}

Based on those assumptions, we have the following results given an initialized model $\boldsymbol{w}^0$.
% Based on those assumptions, we have the following results given an initialized model $\boldsymbol{w}^0$ (\bluenote{the detailed proof is presented in supplementary materials}):

\begin{theorem} 
\label{theorem1}
\vspace{-0.2cm}
The sequence of outputs $\{\boldsymbol{w}^{h, k}\}$ generated by supervised training and global aggregation satisfies:
\begin{equation}
 \underset{h \in [H], k \in \{0, \cdots, K_s\}}{\min} {\mathbb{E} \Vert \nabla F(\boldsymbol{w}^{h, k}) \Vert^2} 
    \leq \frac{2(F(\boldsymbol{w}^0) - F(\boldsymbol{w}^*))}{\eta_1 HK_s} 
    +\Phi.
    \nonumber
\end{equation}
where $\Phi \triangleq (\frac{L^2(K_u-1)(2K_u-1)\eta_1^2}{3K_s} + \frac{LK_u^2 \eta_1^3}{K_s} + 1) G_u^2 + (L \eta_1 + \frac{2 K_u}{K_s}) G_s^2$  (\bluenote{the detailed proof is presented in Appendix}).
\end{theorem}


Theorem \ref{theorem1} suggests that the convergence of the model $\boldsymbol{w}$, obtained through alternating training, is intricately tied to $G_s^2$ and $G_u^2$. 
Specifically, as training goes on, continuous model updating either on \srvUp or \locReg stage ensures that the corresponding second-order moments, $G_s^2$ or $G_u^2$, trend towards zero. 
This indicates that model convergence is guaranteed if the updating frequency on either training stage nears zero. 
However, updating frequencies for both stages are often set the same in practice, resulting in a seesaw relationship between $G_s^2$ and $G_u^2$.
Thus, convergence is impeded due to fluctuations in $\Phi$.

% To provide evidence for this, we conduct experiments on a customized 4-layer CNN model, pre-trained on 1,000 labeled samples from the SVHN dataset for 1,000 iterations. 
% At the beginning, the model undergoes 1,000 continuous iterations on the unlabeled dataset with pseudo labels predicted by a teacher model. 
% Subsequently, the model undergoes an extra 1,000 iterations on the labeled dataset. 
% Fig. \ref{fig-alg-lossv} records the variations in $f_s$ and $f_u$. 
% The results demonstrate that minimizing either loss ($f_s$ or $f_u$) negatively impacts the other. 
% Hence, convergence can only be achieved when positive effects consistently surpass negative ones.
% After sufficient training rounds, the influences of both stages counterbalance each other, and a dynamic equilibrium is reached (we term it convergence). 
% During this process, the global updating frequency $K_s$ and the \locReg updating frequency $K_u$ are pivotal to the equilibrium.

% \bluenote{
% Next, we delve into a more detailed analysis based on
% the experimental observations.
% Theorem \ref{theorem1} highlights the complexity of jointly optimizing $K_u$ and $K_s$ to reduce the expected gradient of the loss function. 
% Particularly, $K_u$ presents a multifaceted challenge as it impacts both the speed at which the model converges and the volume of features/gradients exchanged between clients and the PS per round, thereby affecting communication costs.
% For the sake of simplification, we opt to keep $K_u$ constant and concentrate our efforts on exploring the effects of adjusting $K_s$.
% We simulate a simple environment comprising 10 clients and 1 PS with $K_u=50$ and adjust $K_s$. 
% We record the number of aggregation rounds needed to reach equilibrium  alongside the accuracy for each round. 
% Equilibrium is defined by a variance in total loss falling below 0.01 across 50 consecutive rounds.
% Echoing Theorem \ref{theorem1}, an increased global updating frequency facilitates a quicker transition of the model into a dynamic equilibrium, \ie, the expected gradient approaches to $G_u^2$ rapidly. 
% This premise receives empirical support in Fig. \ref{fig-alg-sfreq}(a), where higher values of $K_s$ lead to convergence in fewer aggregation rounds.}

% \bluenote{Additionally, within EC systems, where stringent time constraints prevail, the ideal global updating frequency might shift depending on available time budgets.
% Fig. \ref{fig-alg-sfreq}(b) suggests that a higher $K_s$ accelerates the model's convergence to a higher accuracy state earlier on. 
% On the flip side, a lower $K_s$ has the potential to yield greater accuracy during the latter training phases by nudging the equilibrium state closer to global optima (thereby reducing $\Phi$), albeit at the cost of more aggregation rounds, \ie, more resource consumption.
% As such, we face a dilemma in determining the global updating frequency.}

% % Figure environment removed


% % Figure environment removed

\subsection{Global Updating Frequency Adaptation}
\vspace{-0.1cm}
\bluenote{
In practical applications, it is impractical to determine an optimal global updating frequency in advance of training. 
To dynamically adjust the global updating frequency, we focus on analyzing the training dynamics. 
Generally, the model exhibits significant differences in training status between the initial and later training phases. 
Initially, the model parameters are initialized randomly and the supervised loss decreases rapidly, which denotes a large $\Delta f_s(\boldsymbol{w})$. 
Given the potential bias in pseudo labels, it is imperative to initialize a higher global updating frequency to prioritize the supervised training phase for keeping the model focused on correct targets. 
As training progresses, the supervised loss stabilizes, which indicates that the model has sufficiently assimilated the information from the labeled data. 
Then, the model should shift towards the semi-supervised training stage to minimize interference from semi-supervised training and expedite convergence towards the global optimal. 
Therefore, the crux lies in accurately seizing the moment to adjust the global updating frequency for balanced training.}

\bluenote{
To delve deeper into this matter, we conduct additional experiments on the CNN model described earlier.
In each round, we monitor both the supervised loss, $f_s(\boldsymbol{w}^h)$, and the semi-supervised loss, $f_u(\boldsymbol{w}^h)$, hereafter referred to simply as $f_s^h$ and $f_u^h$.
Following this, we calculate the mean loss across each discrete observational period---comprising 10 rounds---and label these as $\bar{f}_s^n$ and $\bar{f}_u^n$ (with $n$ extending from 1 to $\lceil \frac{H}{10} \rceil$).
Subsequently, we compute the change in average loss between successive observation periods, denoted as $\Delta \bar{f}_s^n$ and $\Delta \bar{f}_u^n$, which will be further examined over spans of 100 rounds.
For simplification, we define an indicator function as:
\vspace{-0.2cm}
\begin{equation}
  I_n = 
  \begin{cases} 
   1 & \text{if } \Delta \bar{f}_u^n > \Delta \bar{f}_s^n; \\
   0 & \text{otherwise}.
  \end{cases}
  \vspace{-0.2cm}
\end{equation}
}

% Figure environment removed

% Figure environment removed

\bluenote{
As depicted in Fig. \ref{fig-alg-lossr}, during the initial training phase, the model is dominated by supervised loss ($I_n = 0$). 
As training progresses and output confidence grows, semi-supervised loss becomes more pronounced, evidenced by the proportion in every 100 rounds where $\Delta \bar{f}_u^n$ surpasses $\Delta \bar{f}_s^n$, represented by $R_h = \frac{1}{10}\sum_{n=1}^{10} I_n$. 
Notably, Fig. \ref{fig-alg-lossr} (b) unveils a significant phenomenon that when $K_s=100$, the initial $R_h$ is substantially higher compared to the scenario when $K_s=20$.
However, as training proceeds, this gap diminishes and may even invert.
This implies that semi-supervised training is negatively impacted sooner with a higher global updating frequency.
This insight, aligning with the pattern presented in Fig. \ref{fig-alg-train} (b), establishes $R_h$ as a crucial metric for evaluating the training progress.
Based on these observations, we propose a greedy algorithm for adaptively adjusting the global updating frequency with the following rule:
\vspace{-0.1cm}
\begin{equation}
\label{eq-fdecay}
K_{s}^{h+1}=
  \begin{cases}
    \max(\lfloor \frac{K_{s}^{h}}{\alpha}\rfloor, K_{min}) & \text{if } R_h \geq 0.5; \\
    K_{s}^{h} & \text{otherwise}.
  \end{cases}
  \vspace{-0.1cm}
\end{equation}
where $\alpha \in R^+$ is a decaying factor with $\alpha \geq 1$.}

\begin{algorithm}[t]
\caption{Training process on the PS}
\begin{algorithmic}[1]
    \STATE Initialize global model $\boldsymbol{w} = (\boldsymbol{w}_c, \boldsymbol{w}_s)$, teacher model $\tilde{\boldsymbol{w}} = \boldsymbol{w}$, $K_s^0 = K_s$
    \FOR{$h=0$ to $H-1$}
        % \STATE \textbf{Processing at the Parameter Server}
        \FOR{$k=0$ to $K_s^h-1$}
            \STATE Update $\boldsymbol{w}^{h, k+1} = \boldsymbol{w}^{h, k} - \eta_h\tilde{\nabla} f_s(\boldsymbol{w}^{h, k})$ 
            \STATE Update $\tilde{\boldsymbol{w}}^{h, k+1} = \gamma \tilde{\boldsymbol{w}}^{h, k} + (1-\gamma) \boldsymbol{w}^{h , k+1}$
        \ENDFOR
        \STATE Send $\boldsymbol{w}_c^{h+}$ and $\tilde{\boldsymbol{w}}_c^{h+}$ to all clients
        \STATE Set $\boldsymbol{w}_s^{h+,0} = \boldsymbol{w}_s^{h+}$
        \FOR{$k=1$ to $K_u$}
            \FOR{$i$ in $V$}
                \STATE Get ($\boldsymbol{z}_i$, $\tilde{\boldsymbol{z}}_i$) from client $i$
                \STATE Forward propagation with $\boldsymbol{z}_i$ and $\tilde{\boldsymbol{z}}_i$ on $\boldsymbol{w}_s^{h+,k}$ and  $\tilde{\boldsymbol{w}}_s^{h+,k}$
                \STATE Backward propagation, calculate $\tilde{\nabla}_s f_{u,i}(\boldsymbol{w}_s^{h+,k})$
                \STATE Send the gradients of $\boldsymbol{z}_i$ to client $i$
            \ENDFOR
            \STATE Update top models as Eq. (\ref{eq-split-psupdate})
        \ENDFOR
        \FOR{$i$ in $V$}
            \STATE Get $\boldsymbol{w}_{c,i}^{h+, K_u}$ from client $i$
        \ENDFOR
        \STATE Set $\boldsymbol{w}_c^{h+1} = \frac{1}{N}\sum_{i \in [N]}\boldsymbol{w}_{c,i}^{h+, K_u}$
        \STATE Update $\Delta \bar{f}_s^h$ and $\Delta \bar{f}_u^h$ and calculate $R_h$
        \STATE Update $K_s^{h+1}$ as Eq. (\ref{eq-fdecay})
    \ENDFOR
\end{algorithmic}
\label{alg-process}
\end{algorithm}

\bluenote{
Next, we will explain our algorithm design.
Specifically, we start the training with a high global updating frequency ($K_s=100$) to encourage swift convergence. 
When the semi-supervised loss declines faster than the supervised loss, we adjust the global updating frequency downwards.
This allows the model to focus more on semi-supervised training in the later training phases by minimizing interference from supervised training. 
To mitigate the influence of random variations, adjustments to the global updating frequency are made only when more than half of the last 10 observation periods exhibit $\Delta \bar{f}_u^n > \Delta \bar{f}_s^n$.
Additionally, to prevent the model from overfitting to unlabeled data, we set a lower bound for the global updating frequency $K_{min}$ proportional to the ratio of labeled data available at the PS to the entire dataset, \ie, $K_{min} = \lfloor \beta\frac{|\mathcal{D}l|} {|\mathcal{D}|} K_u \rfloor$.
Finally, confirmatory experiments are conducted under the aforementioned system to validate the efficacy of our algorithm. 
Upon detecting $R_h \geq 0.5$, the algorithm proportionately reduces the global update frequency ($\alpha=2$), until it reaches the predefined minimum $K_{min}$($\beta=4$). 
These adjustments lead to a consistent reduction in overall loss, as depicted in Fig. \ref{fig-alg-train} (a), ensuring our achieves the best possible model performance across varying computational constraints (Fig. \ref{fig-alg-train} (b)). 
In essence, our algorithm maintains a favorable balance between alternate training stages, thereby steering the model towards peak performance.
}


We present the overall training process on the PS, including the adaptation algorithm, in Alg. \ref{alg-process}. 
The training starts with supervised training, and the teacher model is updated batch-wise at the PS in each round for $K_s^h$ iterations (Lines 4-5 in Alg. \ref{alg-process}). 
The PS distributes the global bottom model and teacher bottom model to each client (Line 7 in Alg. \ref{alg-process}). 
The \locReg on PS is performed for each client in turn using the top model.
Then, the calculated gradients for student features (features generated by student bottom models) are sent to the corresponding clients (Lines 11-14 in Alg. \ref{alg-process}). 
After $K_u$ steps of \locReg, the bottom models are aggregated (Lines 18-21 in Alg. \ref{alg-process}). 
Finally, the PS updates $K_s$ based on the loss variation at the end of each round (Lines 22-23 in Alg. \ref{alg-process}) and starts the next round. 
The client training process is identical to what we have described in Section \ref{system-design}, which includes conducting local forward propagation, uploading features, and backward propagation based on the downloaded gradients subsequently.