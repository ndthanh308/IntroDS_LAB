% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{konevcny2016federated}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, D.~Ramage, and P.~Richt{\'a}rik,
  ``Federated optimization: Distributed machine learning for on-device
  intelligence,'' \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{li2019edge}
E.~Li, L.~Zeng, Z.~Zhou, and X.~Chen, ``Edge ai: On-demand accelerating deep
  neural network inference via edge computing,'' \emph{IEEE Transactions on
  Wireless Communications}, vol.~19, no.~1, pp. 447--457, 2019.

\bibitem{xu2022adaptive}
Y.~Xu, Y.~Liao, H.~Xu, Z.~Ma, L.~Wang, and J.~Liu, ``Adaptive control of local
  updating and model compression for efficient federated learning,'' \emph{IEEE
  Transactions on Mobile Computing}, 2022.

\bibitem{liang2022new}
B.~Liang, J.~Cai, and H.~Yang, ``A new cell group clustering algorithm based on
  validation \& correction mechanism,'' \emph{Expert Systems with
  Applications}, vol. 193, p. 116410, 2022.

\bibitem{wen2023survey}
J.~Wen, Z.~Zhang, Y.~Lan, Z.~Cui, J.~Cai, and W.~Zhang, ``A survey on federated
  learning: challenges and applications,'' \emph{International Journal of
  Machine Learning and Cybernetics}, vol.~14, no.~2, pp. 513--535, 2023.

\bibitem{li2020review}
L.~Li, Y.~Fan, M.~Tse, and K.-Y. Lin, ``A review of applications in federated
  learning,'' \emph{Computers \& Industrial Engineering}, vol. 149, p. 106854,
  2020.

\bibitem{liao2023accelerating}
Y.~Liao, Y.~Xu, H.~Xu, Z.~Yao, L.~Wang, and C.~Qiao, ``Accelerating federated
  learning with data and model parallelism in edge computing,'' \emph{IEEE/ACM
  Transactions on Networking}, 2023.

\bibitem{han2021accelerating}
D.-J. Han, H.~I. Bhatti, J.~Lee, and J.~Moon, ``Accelerating federated learning
  with split learning on locally generated losses,'' in \emph{ICML 2021
  Workshop on Federated Learning for User Privacy and Data Confidentiality.
  ICML Board}, 2021.

\bibitem{thapa2022splitfed}
C.~Thapa, P.~C.~M. Arachchige, S.~Camtepe, and L.~Sun, ``Splitfed: When
  federated learning meets split learning,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~36, no.~8, 2022, pp. 8485--8493.

\bibitem{liao2023mergesfl}
Y.~Liao, Y.~Xu, H.~Xu, L.~Wang, Z.~Yao, and C.~Qiao, ``Mergesfl: Split
  federated learning with feature merging and batch size regulation,''
  \emph{arXiv preprint arXiv:2311.13348}, 2023.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{zhao2018federated}
Y.~Zhao, M.~Li, L.~Lai, N.~Suda, D.~Civin, and V.~Chandra, ``Federated learning
  with non-iid data,'' \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem{wang2023distribution}
Y.~Wang, Y.~Tong, Z.~Zhou, R.~Zhang, S.~J. Pan, L.~Fan, and Q.~Yang,
  ``Distribution-regularized federated learning on non-iid data,'' in
  \emph{2023 IEEE 39th International Conference on Data Engineering
  (ICDE)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 2113--2125.

\bibitem{liao2023decentralized}
Y.~Liao, Y.~Xu, H.~Xu, L.~Wang, C.~Qian, and C.~Qiao, ``Decentralized federated
  learning with adaptive configuration for heterogeneous participants,''
  \emph{IEEE Transactions on Mobile Computing}, 2023.

\bibitem{gui2023sk}
J.~Gui, Y.~Song, Z.~Wang, C.~He, and Q.~Huang, ``Sk-gradient: Efficient
  communication for distributed machine learning with data sketch,'' in
  \emph{2023 IEEE 39th International Conference on Data Engineering
  (ICDE)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 2372--2385.

\bibitem{lin2021semifed}
H.~Lin, J.~Lou, L.~Xiong, and C.~Shahabi, ``Semifed: Semi-supervised federated
  learning with consistency and pseudo-labeling,'' \emph{arXiv preprint
  arXiv:2108.09412}, 2021.

\bibitem{kim2022federated}
W.~Kim, K.~Park, K.~Sohn, R.~Shu, and H.-S. Kim, ``Federated semi-supervised
  learning with prototypical networks,'' \emph{arXiv preprint
  arXiv:2205.13921}, 2022.

\bibitem{wang2022enhancing}
L.~Wang, Y.~Xu, H.~Xu, J.~Liu, Z.~Wang, and L.~Huang, ``Enhancing federated
  learning with in-cloud unlabeled data,'' in \emph{2022 IEEE 38th
  International Conference on Data Engineering (ICDE)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2022, pp. 136--149.

\bibitem{lin2020ensemble}
T.~Lin, L.~Kong, S.~U. Stich, and M.~Jaggi, ``Ensemble distillation for robust
  model fusion in federated learning,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~33, pp. 2351--2363, 2020.

\bibitem{albaseer2020exploiting}
A.~Albaseer, B.~S. Ciftler, M.~Abdallah, and A.~Al-Fuqaha, ``Exploiting
  unlabeled data in smart cities using federated edge learning,'' in \emph{2020
  International Wireless Communications and Mobile Computing (IWCMC)}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 1666--1671.

\bibitem{zhang2021improving}
Z.~Zhang, Y.~Yang, Z.~Yao, Y.~Yan, J.~E. Gonzalez, K.~Ramchandran, and M.~W.
  Mahoney, ``Improving semi-supervised federated learning by reducing the
  gradient diversity of models,'' in \emph{2021 IEEE International Conference
  on Big Data (Big Data)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021,
  pp. 1214--1225.

\bibitem{diao2021semifl}
E.~Diao, J.~Ding, and V.~Tarokh, ``Semifl: Communication efficient
  semi-supervised federated learning with unlabeled clients,'' \emph{arXiv
  preprint arXiv:2106.01432}, 2021.

\bibitem{jeong2021federated}
W.~Jeong, J.~Yoon, E.~Yang, and S.~J. Hwang, ``Federated semi-supervised
  learning with inter-client consistency \& disjoint learning,'' in \emph{9th
  International Conference on Learning Representations, ICLR 2021}.\hskip 1em
  plus 0.5em minus 0.4em\relax International Conference on Learning
  Representations, ICLR, 2021.

\bibitem{long2020fedsiam}
Z.~Long, L.~Che, Y.~Wang, M.~Ye, J.~Luo, J.~Wu, H.~Xiao, and F.~Ma, ``Fedsiam:
  Towards adaptive federated semi-supervised learning,'' \emph{arXiv preprint
  arXiv:2012.03292}, 2020.

\bibitem{zhao2023does}
J.~Zhao, S.~Ghosh, A.~Bharadwaj, and C.-Y. Ma, ``When does the student surpass
  the teacher? federated semi-supervised learning with teacher-student ema,''
  \emph{arXiv preprint arXiv:2301.10114}, 2023.

\bibitem{wang2023knowledge}
J.~Wang, S.~Zeng, Z.~Long, Y.~Wang, H.~Xiao, and F.~Ma, ``Knowledge-enhanced
  semi-supervised federated learning for aggregating heterogeneous lightweight
  clients in iot,'' in \emph{Proceedings of the 2023 SIAM International
  Conference on Data Mining (SDM)}.\hskip 1em plus 0.5em minus 0.4em\relax
  SIAM, 2023, pp. 496--504.

\bibitem{li2021fedbn}
X.~Li, M.~Jiang, X.~Zhang, M.~Kamp, and Q.~Dou, ``Fedbn: Federated learning on
  non-iid features via local batch normalization,'' \emph{arXiv preprint
  arXiv:2102.07623}, 2021.

\bibitem{gupta2018distributed}
O.~Gupta and R.~Raskar, ``Distributed learning of deep neural network over
  multiple agents,'' \emph{Journal of Network and Computer Applications}, vol.
  116, pp. 1--8, 2018.

\bibitem{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, N.~Carlini, Z.~Zhang, H.~Zhang, C.~A. Raffel, E.~D.
  Cubuk, A.~Kurakin, and C.-L. Li, ``Fixmatch: Simplifying semi-supervised
  learning with consistency and confidence,'' \emph{Advances in neural
  information processing systems}, vol.~33, pp. 596--608, 2020.

\bibitem{li2021model}
Q.~Li, B.~He, and D.~Song, ``Model-contrastive federated learning,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 10\,713--10\,722.

\bibitem{lee2022contrastive}
D.~Lee, S.~Kim, I.~Kim, Y.~Cheon, M.~Cho, and W.-S. Han, ``Contrastive
  regularization for semi-supervised learning,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  3911--3920.

\bibitem{khosla2020supervised}
P.~Khosla, P.~Teterwak, C.~Wang, A.~Sarna, Y.~Tian, P.~Isola, A.~Maschinot,
  C.~Liu, and D.~Krishnan, ``Supervised contrastive learning,'' \emph{Advances
  in Neural Information Processing Systems}, vol.~33, pp. 18\,661--18\,673,
  2020.

\bibitem{gupta2022understanding}
K.~Gupta, T.~Ajanthan, A.~v.~d. Hengel, and S.~Gould, ``Understanding and
  improving the role of projection head in self-supervised learning,''
  \emph{arXiv preprint arXiv:2212.11491}, 2022.

\bibitem{cubuk2020randaugment}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le, ``Randaugment: Practical
  automated data augmentation with a reduced search space,'' in
  \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern
  recognition workshops}, 2020, pp. 702--703.

\bibitem{li2019convergence}
X.~Li, K.~Huang, W.~Yang, S.~Wang, and Z.~Zhang, ``On the convergence of fedavg
  on non-iid data,'' \emph{arXiv preprint arXiv:1907.02189}, 2019.

\bibitem{yang2021achieving}
H.~Yang, M.~Fang, and J.~Liu, ``Achieving linear speedup with partial worker
  participation in non-iid federated learning,'' \emph{arXiv preprint
  arXiv:2101.11203}, 2021.

\bibitem{haddadpour2019convergence}
F.~Haddadpour and M.~Mahdavi, ``On the convergence of local descent methods in
  federated learning,'' \emph{arXiv preprint arXiv:1910.14425}, 2019.

\bibitem{ajalloeian2020convergence}
A.~Ajalloeian and S.~U. Stich, ``On the convergence of sgd with biased
  gradients,'' \emph{arXiv preprint arXiv:2008.00051}, 2020.

\bibitem{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng, ``Reading
  digits in natural images with unsupervised feature learning,'' 2011.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton \emph{et~al.}, ``Learning multiple layers of features
  from tiny images,'' 2009.

\bibitem{coates2011analysis}
A.~Coates, A.~Ng, and H.~Lee, ``An analysis of single-layer networks in
  unsupervised feature learning,'' in \emph{Proceedings of the fourteenth
  international conference on artificial intelligence and statistics}.\hskip
  1em plus 0.5em minus 0.4em\relax JMLR Workshop and Conference Proceedings,
  2011, pp. 215--223.

\bibitem{diao2022semifl}
E.~Diao, J.~Ding, and V.~Tarokh, ``Semifl: Semi-supervised federated learning
  for unlabeled clients with alternate training,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~35, pp. 17\,871--17\,884, 2022.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein \emph{et~al.}, ``Imagenet large scale
  visual recognition challenge,'' \emph{International journal of computer
  vision}, vol. 115, pp. 211--252, 2015.

\bibitem{krizhevsky2017imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' \emph{Communications of the ACM},
  vol.~60, no.~6, pp. 84--90, 2017.

\bibitem{berthelot2019mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~A.
  Raffel, ``Mixmatch: A holistic approach to semi-supervised learning,''
  \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{xu2024overcoming}
Y.~Xu, Y.~Liao, L.~Wang, H.~Xu, Z.~Jiang, and W.~Zhang, ``Overcoming noisy
  labels and non-iid data in edge federated learning,'' \emph{IEEE Transactions
  on Mobile Computing}, 2024.

\bibitem{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter, ``Sgdr: Stochastic gradient descent with warm
  restarts,'' \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{hsu2019measuring}
T.-M.~H. Hsu, H.~Qi, and M.~Brown, ``Measuring the effects of non-identical
  data distribution for federated visual classification,'' \emph{arXiv preprint
  arXiv:1909.06335}, 2019.

\bibitem{tarvainen2017mean}
A.~Tarvainen and H.~Valpola, ``Mean teachers are better role models:
  Weight-averaged consistency targets improve semi-supervised deep learning
  results,'' \emph{Advances in neural information processing systems}, vol.~30,
  2017.

\bibitem{zhang2021flexmatch}
B.~Zhang, Y.~Wang, W.~Hou, H.~Wu, J.~Wang, M.~Okumura, and T.~Shinozaki,
  ``Flexmatch: Boosting semi-supervised learning with curriculum pseudo
  labeling,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 18\,408--18\,419, 2021.

\bibitem{wang2022freematch}
Y.~Wang, H.~Chen, Q.~Heng, W.~Hou, Y.~Fan, Z.~Wu, J.~Wang, M.~Savvides,
  T.~Shinozaki, B.~Raj \emph{et~al.}, ``Freematch: Self-adaptive thresholding
  for semi-supervised learning,'' \emph{arXiv preprint arXiv:2205.07246}, 2022.

\bibitem{liao2023adaptive}
Y.~Liao, Y.~Xu, H.~Xu, L.~Wang, and C.~Qian, ``Adaptive configuration for
  heterogeneous participants in decentralized federated learning,'' in
  \emph{IEEE INFOCOM 2023-IEEE Conference on Computer Communications}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 1--10.

\bibitem{liu2022enhancing}
J.~Liu, Y.~Xu, H.~Xu, Y.~Liao, Z.~Wang, and H.~Huang, ``Enhancing federated
  learning with intelligent model migration in heterogeneous edge computing,''
  in \emph{2022 IEEE 38th International Conference on Data Engineering
  (ICDE)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 1586--1597.

\bibitem{li2020federated}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith,
  ``Federated optimization in heterogeneous networks,'' \emph{Proceedings of
  Machine learning and systems}, vol.~2, pp. 429--450, 2020.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in
  context,'' in \emph{Computer Vision--ECCV 2014: 13th European Conference,
  Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2014, pp. 740--755.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{oh2022locfedmix}
S.~Oh, J.~Park, P.~Vepakomma, S.~Baek, R.~Raskar, M.~Bennis, and S.-L. Kim,
  ``Locfedmix-sl: Localize, federate, and mix for improved scalability,
  convergence, and latency in split learning,'' in \emph{Proceedings of the ACM
  Web Conference 2022}, 2022, pp. 3347--3357.

\bibitem{han2023splitgp}
D.-J. Han, D.-Y. Kim, M.~Choi, C.~G. Brinton, and J.~Moon, ``Splitgp: Achieving
  both generalization and personalization in federated learning,'' in
  \emph{IEEE INFOCOM 2023-IEEE Conference on Computer Communications}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 1--10.

\end{thebibliography}
