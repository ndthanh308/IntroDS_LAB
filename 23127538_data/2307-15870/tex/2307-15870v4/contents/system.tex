% In this section, we delve into the detailed workflow of our system.
% \subsection{System Workflow}

% In this section, we delve into the detailed workflow of our system. 
In \method, \textit{cross-entity semi-supervised training} always follows the \textit{server-side \srvUp}.
Such alternate training procedure will be conducted for $H$ rounds. 
Fig. \ref{workflow-fig} illustrates the workflow of \method, which consists of the following five processes in round $h$:
% In round $h$, the workflow of \method consists of the following five processes:

(1)-(2) \textbf{\textit{Supervised Training} and \textit{Bottom Model Broadcast}.} 
Initially, the PS trains a global model for $K_s$ iterations using labeled data.
The global model, denoted as $\boldsymbol{w}=(\boldsymbol{w}_c, \boldsymbol{w}_s)$, consists of a bottom model $\boldsymbol{w}_c$ and a top model $\boldsymbol{w}_s$.
The model is originally designed to be trained with cross-entropy loss upon model outputs and ground-truth labels. 
Besides, a supervised-contrastive loss \cite{khosla2020supervised} is also employed as the loss function, which enables the model to learn similar or dissimilar representations with labels of the same or different classes. 
% To facilitate the training process, an additional projection head $\boldsymbol{w}_h$ \cite{gupta2022understanding} is juxtaposed as a top model alongside $\boldsymbol{w}_s$, receiving the output from $\boldsymbol{w}_c$.
To facilitate the training process, an additional projection head $\boldsymbol{w}_h$ \cite{gupta2022understanding} is juxtaposed alongside $\boldsymbol{w}_s$, receiving the output from $\boldsymbol{w}_c$.
% For consistency, the projection head is trained simultaneously with other sub-models \cite{khosla2020supervised}.
Here, we define $g_{\boldsymbol{w}}(\cdot)$ as the feed-forward function for model $\boldsymbol{w}$. 
Let $\mathcal{B}_l \in \mathcal{D}_l$ represent any mini-batch from the labeled dataset. The loss function is defined as follows:
\begin{equation}
\label{supcon-loss}
 \mathcal{T}(x_j, \boldsymbol{w}) = \frac{-1}{|P(j)|} \sum\limits_{p \in P(j)}{\log \frac{\exp (z_j \cdot z_p / \kappa)}{ \sum\limits_{a \in A(j)}{\exp (z_j \cdot z_a / \kappa)}}}  
\end{equation}
where $z_j = g_{\boldsymbol{w}_p}(g_{\boldsymbol{w}_c}(x_j))$, $A(j)$ is the set of indexes in all reference samples except $j$, $P(j) \triangleq \{p \in A(j), y_p = y_j\}$ is the set of positive samples of image $x_j$, $\kappa \in \mathcal{R}^+$ is a scalar temperature parameter. 
In our formulation, the similar pairs consist of samples with the same label, while the dissimilar pairs consist of samples with different labels. 
The loss for supervised training is the summation over Eq. \eqref{supcon-loss} and cross-entropy loss $\mathcal{H}$:
\begin{equation}
 \ell_{s} = \mathcal{H} + \mathcal{T}
\end{equation}


% During training, an EMA model $\tilde{\boldsymbol{w}}=(\tilde{\boldsymbol{w}}_c, \tilde{\boldsymbol{w}}_s, \tilde{\boldsymbol{w}}_p)$ 
% % (hereinafter referred to as EMA model for short) 
% composed of a bottom model $\tilde{\boldsymbol{w}}_c$, juxtaposed top model $\tilde{\boldsymbol{w}}_s$ and projection head $\tilde{\boldsymbol{w}}_p$ calculated as $\tilde{\boldsymbol{w}} = \gamma \tilde{\boldsymbol{w}} + (1-\gamma) \boldsymbol{w}, \gamma \in (0, 1]$ is employed as the teacher model.
During training, an EMA model $\tilde{\boldsymbol{w}}=(\tilde{\boldsymbol{w}}_c, \tilde{\boldsymbol{w}}_s, \tilde{\boldsymbol{w}}_p)$ calculated as $\tilde{\boldsymbol{w}} = \gamma \tilde{\boldsymbol{w}} + (1-\gamma) \boldsymbol{w}, \gamma \in (0, 1]$ is employed as the teacher model.
The teacher model shares the same model architecture as $\boldsymbol{w}$, with its parameters being computed as a moving average of the global models trained in previous steps.
It is utilized throughout the process to provide both pseudo labels and teacher features. 
To allow for loss calculation on abundant reference samples, we maintain a two-level memory queue $\mathcal{Q}$ on the fly that caches the most recent features generated from the teacher model. 
The gradients of the supervised-contrastive loss be only applied to the current mini-batch. 

When global training is done in round $h$, a global model $\srvmodh = \boldsymbol{w}^{h, K_s}$ is obtained. 
Part of its components including the top model $\srvmodh_s$, the projection head $\srvmodh_p$ and their teacher counterparts are kept on the server side, while the bottom model as $\srvmodh_c$ and its teacher counterpart $\tilde{\boldsymbol{w}}_c^{h+\frac{1}{2}}$ are broadcast to each client. 

(3) \textbf{\textit{Forward Propagation}.} 
Each \locReg round consists of $K_u$ iterations, where $K_u$ equals the \Reg frequency. 
For client $i$, the bottom model in round $h$ is initialized as $\boldsymbol{w}_{c,i}^{h + \frac{1}{2}, 1} = \boldsymbol{w}_{c}^{h + \frac{1}{2}}$. Before the $k$-th iteration, the bottom model and its teacher counterpart are notated as $\boldsymbol{w}_{c,i}^{h + \frac{1}{2},k}$ and $\tilde{\boldsymbol{w}}_{c,i}^{h + \frac{1}{2},k}$, respectively.

% % Figure environment removed

During the $k$-th iteration, client $i$ performs \locReg on a batch size of $d_i^{h + \frac{1}{2},k}$.
% The process of loss calculation is illustrated in Fig. \ref{arch-fig}.
Considering the feed-forward process, any data sample $x$ in a mini-batch $\mathcal{B}_{u, i} \subset \mathcal{D}_{u,i}$, $|\mathcal{B}_{u, i}| = d_i^{h + \frac{1}{2},k}$ first undergoes weak augmentation $a_w(\cdot)$, including random horizontal flipping and cropping, and strong augmentation $a_s(\cdot)$, exemplified by RandAugment \cite{cubuk2020randaugment} in our system. 
The productions are denoted as $a_w(x)$ and $a_s(x)$, respectively. 
They are then fed-forward to the bottom model $\boldsymbol{w}_{c,i}^{k,\iota}$ and teacher bottom model $\tilde{\boldsymbol{w}}_{c,i}^{k,\iota}$ in parallel, generating student features $\boldsymbol{e}_i=(e_{i,1}, \cdots, e_{i, |\mathcal{B}_{u,i}|})$ and teacher features $\tilde{\boldsymbol{e}}_i=(\tilde{e}_{i,1}, \cdots, \tilde{e}_{i, |\mathcal{B}_{u,i}|})$, where $e_{i,j} = g_{ \boldsymbol{w}_{c,i}^{h, k}}(a_s(x_{i,j}))$, $\tilde{e}_{i,j}= g_{ \tilde{\boldsymbol{w}}_{c,i}^{h, k}}(a_w(x_{i,j})), x_{i,j} \in \mathcal{B}_{u,i}$. 
The student features $\boldsymbol{e}_i$ and teacher features $\tilde{\boldsymbol{e}_i}$, each containing $d_i^{h + \frac{1}{2},k}$ samples, are then sent to the PS. 
On the PS, both the student and teacher features arriving at a synchronization barrier are then fed forward server-side models and their teacher counterparts in parallel. 

% Figure environment removed

(4) \textbf{\textit{Clustering Regularization \& Backward Propagation}.} 
As stated in section \ref{description-section}, a cross-entropy loss is applied to minimize the prediction of noisy input, \ie, a strongly-augmented sample and the pseudo-label. 
Additionally, to minimize the distance of model predictions of the same class in embedding space, we extend the contrastive loss to a multi-client setting. 
Based on notations of Eq. (\ref{supcon-loss}), we define the \textit{Clustering Regularization} loss as:
\vspace{-0.2cm}
\begin{equation}
\label{cr-eq}
     \mathcal{C}(x_j, \boldsymbol{w}) = \frac{-1}{|\hat{P}(j)|} \sum\limits_{p \in \hat{P}(j)}{\log \frac{\exp (z_j \cdot \tilde{z}_{p} / \kappa)}{ \sum\limits_{a \in [\mathcal{Q}]}{\exp (z_j \cdot \tilde{z}_a / \kappa)}}}  
     \vspace{-0.2cm}
\end{equation}
where $z_j = g_{\boldsymbol{w}_p}(e_j)$, $\tilde{z}_p = g_{\tilde{\boldsymbol{w}}_p}(\tilde{e_p})$ are projected student/teacher features, $\hat{P}(j) \triangleq \{p \in [\mathcal{Q}],  \max_m{(\tilde{q}_{p,m})} > \tau, \tilde{q}_{p} = q_j\}$ is the set of the indexes of weakly-augmented samples that have the same pseudo-label with $x_j$ and their confidences come up to $\tau$. 
% As shown in Fig. \ref{arch-fig}, we follow the designation of contrastive regularization \cite{lee2022contrastive}, while in our system EMA models $\tilde{\boldsymbol{w}}_c$ and $\tilde{\boldsymbol{w}}_p$ are instructed to guide the process.
We follow the designation of contrastive regularization \cite{lee2022contrastive}, while in our system the teacher model is instructed to guide the process. 
Specifically, the reference samples used for contrastive regularization are derived from the teacher features stored in the globally shared memory queue $\mathcal{Q}$, where features from prior supervised training are dequeued at a lower frequency.
Our total loss over unlabeled data is composed of the consistency regularization term and clustering regularization term: 
\vspace{-0.1cm}
\begin{equation}
    \ell_{u} = \mathcal{H} + \mathcal{C}
    \vspace{-0.1cm}
\end{equation}

By performing backward propagation, the estimated gradients for client $i$ computed as $\tilde{\nabla}_s f_{u, i}(\boldsymbol{w}_{s}^{h + \frac{1}{2}, k}) = \frac{1}{|\mathcal{B}_{u,i}|}\sum_{x_{i,j} \in \mathcal{B}_{u,i}} {\nabla \ell_{ce}(x_{i,j}, \boldsymbol{w}_{s}^{h + \frac{1}{2},k})}$ and $\tilde{\nabla}_p f_{u, i}(\boldsymbol{w}_{p}^{h + \frac{1}{2},k}) = \frac{1}{|\mathcal{B}_{u,i}|}\sum_{x_{i,j} \in \mathcal{B}_{u,i}} {\nabla \ell_{c}(x_{i,j}, \boldsymbol{w}_{p}^{h + \frac{1}{2},k})}$ is kept until getting through all student features received before. 
Subsequently, the PS updates its server-side models with learning rate $\eta_h$ as:
% \vspace{-0.2cm}
\begin{equation}
\begin{split}
    & \boldsymbol{w}_{s}^{h + \frac{1}{2},k+1} = \boldsymbol{w}_{s}^{h + \frac{1}{2},k} - \eta_h \frac{1}{N} \sum\limits_{i \in [N]} \tilde{\nabla}_S f_{u, i}(\boldsymbol{w}_{s}^{h + \frac{1}{2}, k}) \\
    & \boldsymbol{w}_{p}^{h + \frac{1}{2}, k+1} = \boldsymbol{w}_{p}^{h + \frac{1}{2}, k} - \eta_h \frac{1}{N} \sum\limits_{i \in [N]} \tilde{\nabla}_P f_{u, i}(\boldsymbol{w}_{p}^{h + \frac{1}{2},k})
\end{split}
\label{server_split_update_eq}
\end{equation}

Along with that, the gradients of student features $\boldsymbol{e}_i$ computed as $d \boldsymbol{e}_i = \{ \nabla \ell_{u}(e_{i,j \in [\mathcal{B}_{u,i}]}, \boldsymbol{w}_{c, i}^{h + \frac{1}{2},k}) \}$ with a batch size of $d_i^{h + \frac{1}{2},k}$ are sent to the corresponding client $i$. 
Then each client continues to perform backward propagation $\tilde{\nabla}_c f_{u, i}(\boldsymbol{w}_{c, i}^{h + \frac{1}{2},k}) = \frac{1}{|\mathcal{B}_{u,i}|}\sum\limits_{j \in [\mathcal{B}_{u,i}]} {\nabla \ell_{u}(e_{i,j}, \boldsymbol{w}_{c, i}^{h + \frac{1}{2},k})}$ and separately updates its student/teacher bottom model as:
\begin{equation}
\begin{split}
    & \boldsymbol{w}_{c,i}^{h + \frac{1}{2},k+1} = \boldsymbol{w}_{c,i}^{h + \frac{1}{2},k} - \eta_h \tilde{\nabla}_c f_{u, i}(\boldsymbol{w}_{c, i}^{h + \frac{1}{2},k}) \\
    & \tilde{\boldsymbol{w}}_{c,i}^{h + \frac{1}{2},k+1} = \gamma \tilde{\boldsymbol{w}}_{c,i}^{h + \frac{1}{2},k} + (1-\gamma) \boldsymbol{w}_{c,i}^{h + \frac{1}{2},k+1}
\end{split}
\label{client_split_update_eq}
\end{equation}

% \bluenote{
% The EMA model remains constant at this stage to avoid extra computation and to prevent it from being biased.
% }

(5) \textbf{\textit{Bottom Model Aggregation}.} 
After total $K_u$ iterations, clients upload their bottom models to the PS at a synchronization barrier, while the teacher bottom models, which are obtained from \srvUp, are excluded from model uploading. 
The server-side models are set as $\boldsymbol{w}_{s}^{h+1} = \boldsymbol{w}_{s}^{h + \frac{1}{2}, K_u + 1}$ and $\boldsymbol{w}_{p}^{h+1} = \boldsymbol{w}_{p}^{h + \frac{1}{2}, K_u + 1}$. 
The PS aggregates the bottom models uploaded by clients to obtain a global bottom model $\boldsymbol{w}_{c}^{h+1} = \frac{1}{N} \sum_{i \in [V]}{ \boldsymbol{w}_{c,i}^{h + \frac{1}{2}, K_u + 1}}$, which, together with the modules residing at the PS, is assembled for further \srvUp.