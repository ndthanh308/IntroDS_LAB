\subsection{Federated Learning}
% Federated Learning (FL) is a distributed machine learning paradigm that allows training models on decentralized data.
% It has been proposed to address the challenges of privacy and security in centralized machine learning \cite{konevcny2016federated, mcmahan2017communication, kairouz2021advances}. 
Federated Learning (FL) has been proposed to address the challenges of privacy and security in centralized machine learning \cite{konevcny2016federated, mcmahan2017communication, kairouz2021advances}. 
In FL, data heterogeneity is a crucial factor in the performance, both in terms of quality and efficiency. 
For that, Zhao et al. \cite{zhao2018federated} demonstrated that the accuracy reduction can be explained by weight divergence, which they addressed by sharing a small subset of data between edge devices. 
Li et al. \cite{li2020federated} proposed adding a proximal term to the objective to improve convergence stability, which is adversely affected by statistical heterogeneity. 
Recently, Li et al. \cite{li2021model} addressed the non-IID issue from a model representation perspective by correcting local updates with the representation of the global model. 
% Several FL algorithms have been proposed to explore adjusting local updating frequencies \cite{wang2018edge, li2019communication} to reduce network traffic consumption and improve training efficiency. 
% Other works \cite{liu2019accelerate, tyagi2020taming} have explored assigning various batch sizes to clients dynamically to address the efficiency problem in heterogeneous systems.

\subsection{Semi-Supervised Learning}
Semi-Supervised Learning (SSL) has been proposed to leverages both labeled and unlabeled data for training, since acquiring labels are often challenging and time-consuming. 
% SSL algorithms aim to learn a better model by leveraging the information present in the unlabeled data. 
Popular SSL methods include pseudo-labeling \cite{lee2013pseudo} and consistency regularization \cite{laine2016temporal, berthelot2019mixmatch}, which train the model over unlabeled data. 
Xie \etal \cite{xie2020unsupervised} show that strong data augmentation can improve performance, and Sohn \etal \cite{sohn2020fixmatch} propose FixMatch that combines pseudo-labeling and consistency regularization to achieve favorable performance. 
Recently, Lee \etal \cite{lee2022contrastive} improves the efficiency and accuracy of consistency regularization by using well-clustered features of unlabeled data greatly, which greatly inspires our work.

\subsection{Semi-Supervised Federated Learning}
Semi-Supervised Federated Learning (Semi-FL) aims to leverage both labeled and unlabeled data in FL. 
Lin \etal \cite{lin2021semifed} study the scenario when clients have both labeled and unlabeled data and propose SemiFed, where the pseudo-label is retained only if models from different clients agree on the same label. 
Other studies also explore leveraging in-cloud unlabeled data using techniques like distillation \cite{lin2020ensemble} or consistency regularization \cite{wang2022enhancing}. 
Concurrently, several approaches have been proposed for labeled-data-on-server scenarios. 
% Kim \etal \cite{kim2022federated} enforce prediction consistency between clients and decompose model parameters for supervised and unsupervised tasks. 
Zhang \etal \cite{zhang2021improving} propose aggregating models group-wise to mitigate the issue of increased gradient diversity caused by non-IID data.
Diao \etal \cite{diao2021semifl} propose SemiFL, which trains the model alternately on the PS and clients and constructs a Mixup dataset for local training. 
Despite these promising studies, the impact of data non-IIDness on model performance and training efficiency remains a significant challenge.