% In this section, we delve into the detailed workflow of our system.
% \subsection{System Workflow}

In this section, we delve into the detailed workflow of our system. 
% We propose to train our model over unlabeled data in a split manner and perform updating on the PS alternately. 
In a nutshell, the training process over local unlabeled data $\mathcal{D}_u$, which consists of \locReg, always follows the \srvUp over labeled dataset $\mathcal{D}_l$. 
Such alternate training procedure will be conducted for $H$ rounds. 
Fig. \ref{workflow-fig} illustrates the workflow of our system (EMA models are omitted for simplicity). 
In round $h$, our system workflow consists of the following four steps:

(1) \textbf{Supervised Training and Bottom Model Broadcast.} 
Initially, the PS trains a global model for $K_s$ iterations using labeled data. 
This global model, denoted as $\boldsymbol{w}=(\boldsymbol{w}_c, \boldsymbol{w}_s)$, consists of a bottom model $\boldsymbol{w}_c$ and a top model $\boldsymbol{w}_s$.
Supervised-contrastive loss \cite{khosla2020supervised} is employed as the loss function, which enables the model to learn similar or dissimilar representations with labels of the same or different classes. 
The model is originally designed to be trained with cross-entropy loss upon model outputs and ground-truth labels. 
To facilitate the contrastive training process \cite{khosla2020supervised}, an additional projection head $\boldsymbol{w}_h$ is juxtaposed as a top model alongside $\boldsymbol{w}_s$, accepting the output from $\boldsymbol{w}_c$.
% For consistency, the projection head is trained simultaneously with other sub-models \cite{khosla2020supervised}.
Here, we define $g_{\boldsymbol{w}}(\cdot)$ as the feed-forward function for model $\boldsymbol{w}$. Let $\mathcal{B}_l \in \mathcal{D}_l$ represent any mini-batch from the labeled dataset. The loss function is defined as follows:
\begin{equation}
\label{supcon-loss}
 \mathcal{T}(x_j, \boldsymbol{w}) = \frac{-1}{|P(j)|} \sum\limits_{p \in P(j)}{\log \frac{\exp (z_j \cdot z_p / \kappa)}{ \sum\limits_{a \in A(j)}{\exp (z_j \cdot z_a / \kappa)}}}  
\end{equation}
where $z_j = g_{\boldsymbol{w}_p}(g_{\boldsymbol{w}_c}(x_j))$, $A(j)$ is the set of indexes in all reference samples except $j$, $P(j) \triangleq \{p \in A(j), y_p = y_j\}$ is the set of positives samples of image $x_j$, $\kappa \in \mathcal{R}^+$ is a scalar temperature parameter. 
In our formulation, the similar pairs consist of samples with the same label, while the dissimilar pairs consist of samples with different labels. 
The loss for supervised training is the summation over Eq. \eqref{supcon-loss} and cross-entropy loss $\mathcal{H}$:
\begin{equation}
 \ell_{s} = \mathcal{H} + \mathcal{T}
\end{equation}

During training, a teacher model $\tilde{\boldsymbol{w}}=(\tilde{\boldsymbol{w}}_c, \tilde{\boldsymbol{w}}_s, \tilde{\boldsymbol{w}}_p)$ composed of a bottom model $\tilde{\boldsymbol{w}}_c$ and juxtaposed top models $\tilde{\boldsymbol{w}}_s$ and $\tilde{\boldsymbol{w}}_p$ is calculated as $\tilde{\boldsymbol{w}} = \gamma \tilde{\boldsymbol{w}} + (1-\gamma) \boldsymbol{w}, \gamma \in (0, 1]$.
The teacher model shares the same model architecture as $\boldsymbol{w}$, with its parameters being computed as a moving average of models globally trained in previous steps.
It is utilized throughout the process to provide both pseudo labels and teacher features. 
To allow for loss calculation on abundant reference samples, we maintain a memory queue $\mathcal{Q}$ on-the-fly that caches the most recent features generated from the average model \cite{he2020momentum}. 
The gradients of the supervised-contrastive loss only impose on the current mini-batch. 

When global training is done in round $h$, a global model $\boldsymbol{w}^{h'} = \boldsymbol{w}^{h', K_s}$ is obtained. 
Part of its components including top models $\boldsymbol{w}_s^{h'}$ and $\boldsymbol{w}_p^{h'}$ and their teacher counterparts are kept at the server side, while the bottom model as $\boldsymbol{w}_c^{h'}$ and its teacher counterpart $\tilde{\boldsymbol{w}}_c^{h'}$ are distributed to each client. 

(2) \textbf{Forward Propagation.} 
Each \locReg round consists of $K_u$ iterations, where $K_u$ equals to the communication period in the later context. 
% Note that $\frac{K_s}{K_u}$ denotes the \InvReg frequency, which will be experimentally investigated in Section \ref{analysis-section}. 
For client $i$, the client-side model in round $h$ is initialized as $\boldsymbol{w}_{c,i}^{h,1} = \boldsymbol{w}_{c}^{h'}$. In the $k$-th iteration, the model is notated as $\boldsymbol{w}_{c,i}^{h,k}$ and the corresponding teacher model is notated as $\tilde{\boldsymbol{w}}_{c,i}^{h,k}$.

% Figure environment removed

During the $k$-th iteration, client $i$ performs \locReg on a batch size of $d_i^{h,k}$.
The process of loss calculation is illustrated in Fig. \ref{arch-fig}.
Considering the feed-forward process, any data sample $x$ in a mini-batch $\mathcal{B}_{u, i} \subset \mathcal{D}_{u,i}$, $|\mathcal{B}_{u, i}| = d_i^{h,k}$ first undergoes weak and strong augmentation, $a_w(\cdot)$ and $a_s(\cdot)$, to produce weakly-augmented version $a_w(x)$ and strong-augmented version $a_s(x)$, respectively. 
They are then fed-forward to the bottom model $\boldsymbol{w}_{c,i}^{k,\iota}$ and teacher model $\tilde{\boldsymbol{w}}_{c,i}^{k,\iota}$ in parallel. 
The generated smashed data is expressed as $\boldsymbol{e}_i=(e_{i,1}, \cdots, e_{i, |\mathcal{B}_{u,i}|})$ and $\tilde{\boldsymbol{e}}_i=(\tilde{e}_{i,1}, \cdots, \tilde{e}_{i, |\mathcal{B}_{u,i}|})$, where $e_{i,j} = g_{ \boldsymbol{w}_{c,i}^{h, k}}(a_s(x_{i,j}))$, $\tilde{e}_{i,j}= g_{ \tilde{\boldsymbol{w}}_{c,i}^{h, k}}(a_w(x_{i,j})), x_{i,j} \in \mathcal{B}_{u,i}$. 
The smashed data $\boldsymbol{e}_i$ and $\tilde{\boldsymbol{e}_i}$, each containing $d_i^{h,k}$ samples, are then sent to the PS. 
On the PS, the smashed data arrived at a synchronization barrier is then fed forward to stop models and their teacher counterparts by turns, respectively. 

(3) \textbf{Backward Propagation.} 
As stated in section \ref{description-section}, a cross-entropy loss is applied to minimize the prediction of noisy input, \ie, a strongly-augmented sample and the pseudo-label. 
Additionally, to minimize the distance of model predictions of the same class in embedding space, we extend the contrastive loss to a multi-client setting. 
Based on notations of Eq. (\ref{supcon-loss}), we define the \textit{Clustering Regularization} loss as:
\begin{equation}
\label{cr-eq}
     \mathcal{C}(x_j, \boldsymbol{w}) = \frac{-1}{|\hat{P}(j)|} \sum\limits_{p \in \hat{P}(j)}{\log \frac{\exp (z_j \cdot \tilde{z}_{p} / \kappa)}{ \sum\limits_{a \in [\mathcal{Q}]}{\exp (z_j \cdot \tilde{z}_a / \kappa)}}}  
 \end{equation}
where $z_j = g_{\boldsymbol{w}_p}(e_j)$, $\tilde{z}_p = g_{\tilde{\boldsymbol{w}}_p}(\tilde{e_p})$, $\hat{P}(j) \triangleq \{p \in [\mathcal{Q}],  \max_m{(\tilde{q}_{p,m})} > \tau, \tilde{q}_{p} = q_j\}$ is the set of the indexes of weakly-augmented samples that have the same pseudo-label with $x_j$ and their confidences come up to $\tau$. 
As shown in Fig. \ref{arch-fig}, we follow the designation of contrastive regularization \cite{lee2022contrastive}, while in our system EMA models $\tilde{\boldsymbol{w}}_c$ and $\tilde{\boldsymbol{w}}_p$ are instructed to guide the process. 
Specifically, the reference samples used for contrastive regularization are derived from the teacher features stored in the globally shared memory queue $\mathcal{Q}$. 
Our total loss over unlabeled data is composed of the consistency regularization term and clustering regularization term: 
\begin{equation}
    \ell_{u} = \mathcal{H} + \mathcal{C}
\end{equation}

By performing back propagation, the estimated gradient for client $i$ computed as $\tilde{\nabla}_s f_{u, i}(\boldsymbol{w}_{s}^{h, k}) = \frac{1}{|\mathcal{B}_{u,i}|}\sum_{x_{i,j} \in \mathcal{B}_{u,i}} {\nabla \ell_{ce}(x_{i,j}, \boldsymbol{w}_{s}^{h,k})}$ and $\tilde{\nabla}_p f_{u, i}(\boldsymbol{w}_{p}^{h,k}) = \frac{1}{|\mathcal{B}_{u,i}|}\sum_{x_{i,j} \in \mathcal{B}_{u,i}} {\nabla \ell_{c}(x_{i,j}, \boldsymbol{w}_{p}^{h,k})}$ is kept until getting through all smashed data received before. 
Subsequently, the PS updates its server-side top models with learning rate $\eta_h$, which is expressed as:
\begin{equation}
\begin{split}
    & \boldsymbol{w}_{s}^{h,k+1} = \boldsymbol{w}_{s}^{h,k} - \eta_h \frac{1}{N} \sum\limits_{i \in [N]} \tilde{\nabla}_S f_{u, i}(\boldsymbol{w}_{s}^{h,k}) \\
    & \boldsymbol{w}_{p}^{h,k+1} = \boldsymbol{w}_{p}^{h,k} - \eta_h \frac{1}{N} \sum\limits_{i \in [N]} \tilde{\nabla}_P f_{u, i}(\boldsymbol{w}_{p}^{h,k})
\end{split}
\label{server_split_update_eq}
\end{equation}

Along with that, the gradients of smashed data $\boldsymbol{e}_i$ computed as $d \boldsymbol{e}_i = \{ \nabla \ell_{u}(e_{i,j \in [\mathcal{B}_{u,i}]}, \boldsymbol{w}_{c, i}^{h,k}) \}$ with a batch size of $d_i^{h,k}$ are sent to the corresponding client $i$. 
Then each client continues to perform backward propagation $\tilde{\nabla}_c f_{u, i}(\boldsymbol{w}_{c, i}^{h,k}) = \frac{1}{|\mathcal{B}_{u,i}|}\sum\limits_{j \in [\mathcal{B}_{u,i}]} {\nabla \ell_{u}(e_{i,j}, \boldsymbol{w}_{c, i}^{h,k})}$ and updates its bottom model respectively, which is expressed as:
\begin{equation}
    \boldsymbol{w}_{c,i}^{h,k+1} = \boldsymbol{w}_{c,i}^{h,k} - \eta_h \tilde{\nabla}_c f_{u, i}(\boldsymbol{w}_{c, i}^{h,k})
\label{client_split_update_eq}
\end{equation}
    
(4) \textbf{Bottom Model Uploading and Global Aggregation.} 
After total $K_u$ iterations, clients upload their bottom models to the PS at a synchronization barrier, while the bottom EMA models, which are obtained from \srvUp, is excluded from model uploading. 
The top models on the PS are set as $\boldsymbol{w}_{s}^{h+1} = \boldsymbol{w}_{s}^{h, K_u}$ and $\boldsymbol{w}_{p}^{h+1} = \boldsymbol{w}_{p}^{h, K_u}$. 
The PS aggregates the bottom models uploaded by clients to obtain a global bottom model $\boldsymbol{w}_{c}^{h+1} = \frac{1}{N} \sum_{i \in [V]}{ \boldsymbol{w}_{c,i}^{h}}$, which, together with the modules residing at the PS, is assembled for further \srvUp.