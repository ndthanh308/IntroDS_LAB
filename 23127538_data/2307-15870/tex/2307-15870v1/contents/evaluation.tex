In this section, we begin by providing a list of the datasets and models, as well as a description of the devices utilized in the experiments. 
We then introduce the adopted baselines and metrics for performance comparison. 
Finally, we present our evaluation results and analyze the superiority of our proposed system under various scenarios.

\subsection{Datasets and Models}
\textbf{Datasets:} We conduct experiments on three commonly used real-world datasets for semi-supervised learning: SVHN \cite{netzer2011reading}, CIFAR-10 \cite{krizhevsky2009learning}, and IMAGE-100. 
The SVHN dataset contains 73,257 digits for training and 26,032 digits for testing, which are labeled in 10 classes. 
By default, 1,000 labeled digits are allocated for the PS. 
The remaining digits in the training dataset are distributed to clients as unlabeled data.
The CIFAR-10 dataset is an image dataset consisting of 60,000 32$\times$32 color images (50,000 for training and 10,000 for testing) in 10 categories, and 4,000 images are set as labeled data on the PS. 
To evaluate our proposed system on a more challenging task, we create the IMAGE-100 dataset, which is a subset of ImageNet \cite{russakovsky2015imagenet} and contains 100 out of 1,000 categories. 
Each sample in IMAGE-100 is resized to the shape of 144$\times$144$\times$3, and 5,000 labeled images are allocated for the PS.

\textbf{Models:} Three models with different types and structures are adopted on the above three real-world datasets for performance evaluation: (i) CNN on SVHN, (ii) AlexNet \cite{krizhevsky2017imagenet} on CIFAR-10, (iii) VGG16 \cite{simonyan2014very} on IMAGE-100. 
For SVHN, we train a customized CNN model with size of 3.35MB, which has two 5$\times$5 convolutional layers, a fully-connected layer with 512 units, and a softmax output layer with 10 units. 
Besides, for CIFAR-10, we train the AlexNet model composed of three 3$\times$3 convolutional layers, one 7$\times$7 convolutional layer, one 11$\times$11 convolutional layer, two fully-connected hidden layers, and one softmax output layer. 
Finally, the VGG16 model, which consists of 13 convolutional layers with kernel size of 3$\times$3, two fully-connected layers and a softmax output layer, is trained for image classification of IMAGE-100.

\subsection{Baselines and Metrics}
\textbf{Baselines:} We compare our proposed system with the following four baselines.
\begin{itemize}
\item[\textbullet] \textbf{Supervised-only}: 
Supervised-only refers to using only the labeled dataset available on the PS for supervised training. 
This represents the lower bound of what can be achieved with a limited amount of labeled data.
\item[\textbullet] \textbf{FedRGD} \cite{zhang2021improving}: Given labeled data on the PS and unlabeled data on clients, FedRGD performs supervised and semi-supervised training over the labeled and unlabeled data in parallel. 
To aggregate models, FedRGD divides the clients into groups randomly and performs group-wise averaging (with the latest global model from the PS), and generates the group models for data labeling and further training.
In order to ensure model convergence, the initial global model is trained with the labeled data on the PS for the first 50 rounds.
% This approach is employed to demonstrate the limitations of \rednote{its} non-IID solution.
\item[\textbullet] \textbf{SemiFL} \cite{diao2021semifl}: SemiFL is a state-of-the-art approach for semi-supervised FL and involves a similar two-stage training procedure as ours, except for the steps of semi-supervised training.
Concretely, at the beginning of each communication round, SemiFL generates pseudo-labels for the local data of each client upon the latest global model.
\bluenote{While in a specific iteration, each client applies the Mixup \cite{berthelot2019mixmatch} technique on the data batch to augment the data and performs training using a specialized "mix" loss}.
% This approach is claimed to be state-of-the-art for the scenario where labeled data reside on the PS.
% \cite{berthelot2019mixmatch}
\item[\textbullet] \textbf{FedEMA}: It is a replication of our system without incorporating clustering regularization. This serves as an ablation study to demonstrate the effectiveness of our clustering regularization under various scenarios.
\end{itemize}

\textbf{Metrics:} We employ the following metrics to evaluate the performance of different systems.
\begin{itemize}
\item[\textbullet] \textbf{Test accuracy}: In each round, we measure the accuracy of the global model on the test set with different baselines and record it. For FedEMA and our system, we use the global EMA model for testing.
\item[\textbullet] \textbf{Time cost}: We record the total time taken to achieve target accuracy on different systems, which includes the time for computation and synchronization.
\item[\textbullet] \textbf{Communication cost}: The communication cost for transmitting models and smashed data batches (if any) between clients and the PS to achieve the target accuracy is recorded.
\end{itemize}

% Figure environment removed

% Figure environment removed

\subsection{Experimental Setup}
We evaluate the performance of our proposed system on an AMAX deep learning workstation equipped with an Intel(R) Xeon(R) Gold 5218R CPU, 8 NVIDIA GeForce RTX 3090 GPUs, and 256 GB RAM. 
We simulate one PS and 30 clients on this workstation. 
To reflect the heterogeneity in the computation capabilities of the clients, we introduce variations in the computation time of client devices. 
Specifically, we simulate each client's computation time to range from 2$\times$-4$\times$ the actual computation time for performing one-round training on the workstation.
% For simulating wireless links between the clients and the PS, we vary the communication time of client devices. 
As the distances between clients and the PS may vary in the real-world scenario, for simulating the network dynamics of wireless links between clients and the PS, the outbound bandwidths are implemented to fluctuate between 2Mbps and 8Mbps, and the inbound bandwidths vary between 10Mbps and 20Mbps.
We use PyTorch deep learning framework for our software implementation of model training and build up the connections between clients and the PS using the socket library.

By default, each experiment is run for 1,000 aggregation rounds on SVHN, CIFAR-10, and IMAGE-100 to ensure convergence. 
During each round, $N_h$ = 10 clients are randomly selected to participate in the training process. 
We use an SGD-momentum optimizer with a momentum of 0.9 to optimize the models. 
A cosine learning rate decay schedule \cite{loshchilov2016sgdr} is adopted, with the learning rate initialized at 0.02 and theminimum learning rate set to 0.005. 
Additionally, we use weight decay regularization with a coefficient of 5e-4. 
As for the adapation algorithm, we set $\varrho_1 = 0.01$ and $\varrho_2 = 0.05$. 
Unless otherwise specified, the number of labeled data samples on the PS is 1,000, 4,000, and 5,000 for SVHN, CIFAR-10, and IMAGE-100, respectively, and the unlabeled data samples are distributed uniformly across clients.

\subsection{Experiment Results}

\subsubsection{Overall Effectiveness}
In this section, we compare our system with baselines. 
The final test accuracies on SVHN, CIFAR-10, and IMAGE-100 are presented in Table \ref{test_acc_table}. \bluenote{Supervised-only is omitted in the later sections for training efficiency comparison since clients are not involved in it.}

\begin{table}[t]
    \centering
    \caption{Overall test accuracy (\%).}
    \label{test_acc_table}
    \begin{tblr}{
      row{2} = {c},
      cell{1}{1} = {r=2}{},
      cell{1}{2} = {c=3}{c},
      cell{3}{2} = {c},
      cell{3}{3} = {c},
      cell{3}{4} = {c},
      cell{4}{2} = {c},
      cell{4}{3} = {c},
      cell{4}{4} = {c},
      cell{5}{2} = {c},
      cell{5}{3} = {c},
      cell{5}{4} = {c},
      cell{6}{2} = {c},
      cell{6}{3} = {c},
      cell{6}{4} = {c},
      cell{7}{2} = {c},
      cell{7}{3} = {c},
      cell{7}{4} = {c},
      hline{1,3,8} = {-}{},
      hline{2} = {2-4}{},
    }
    Baseline        & Dataset       &               &           \\
                    & SVHN          & CIFAR-10      & IMAGE-100 \\
    Supervised-only & 73.7          & 75.0          & 26.1      \\
    FedRGD          & 88.1          & 83.8          & 16.3      \\
    SemiFL          & 88.2          & 86.4          & 65.4      \\
    FedEMA          & 88.6          & 88.2          & 60.3      \\
    \textbf{Ours}   & \textbf{91.1} & \textbf{89.6} & \textbf{66.7}      
    \end{tblr}
\end{table}

In terms of accuracy, our proposed system consistently achieves the highest accuracy compared to state-of-the-art approaches. For instance, on the SVHN dataset, our system achieves 91.1\% accuracy, which is 2.9\% higher than SemiFL and 17.4\% higher than Supervised-only. 
On CIFAR-10, our system achieves 89.6\% accuracy, which is 3.2\% higher than SemiFL and 14.6\% higher than Supervised-only. On IMAGE-100, our system achieves 66.7\% accuracy, which is 1.3\% higher than SemiFL and 40.6\% higher than Supervised-only. 
Our system also improves accuracy from 1.4\% to 6.4\% compared to FedEMA across the three datasets, highlighting the effectiveness of our proposed clustering regularization beyond simply maintaining an EMA model. 

It is clear to see that FedRGD performs almost the poorest among the baselines. 
That is because training with pseudo-labels does not always promote model performance, and the knowledge learned under supervision may become corrupted after model aggregation.
Thus its grouping algorithm helps stabilize the training process in a way of assigning a larger aggregation weight for the model trained in supervised manner. 
Although its grouping algorithm stabilizes the training process by assigning greater aggregation weights to models trained under supervision, the test accuracy on IMAGE-100 still indicates its ineffectiveness.
It is worth noting that the accuracy gap on the IMAGE-100 dataset between our system and the model trained only on the labeled data is over 40\%. 
This result strongly indicates the importance and necessity of our system in leveraging unlabeled data, and its superiority remains consistent across various models and datasets.

We also present the time cost required to achieve different test accuracies in Fig. \ref{time_cost-fig}. 
FedRGD is excluded from some of the comparison plots because it fails to achieve the target accuracy. 
When training small-scale models, our system fails to outperform some of the baselines on the SVHN dataset in term of the time cost, as shown in Fig. \ref{time_cost-fig}(a).
This happens when the cost of transmitting the smashed data outweighs that of the entire model.
However, our system benefits from scaling the size of the model. 
In most cases, our system achieves the lowest time cost for reaching the target accuracy.
For instance, for training AlexNet, to achieve 80\% accuracy on CIFAR-10, it takes 163.4min, 481.7min, and 823.3min for FedEMA, SemiFL, and FedRGD, respectively, while our system only takes 130.9min. 
This translates to speed-ups of 1.2-6.3$\times$. Moreover, for a higher target accuracy such as 85\%, it takes 463.3min for FedEMA and 938.3min for SemiFL, which is even out of reach for FedRGD. 
In contrast, our system only takes 328.3min. 

\begin{table}[t]
    \centering
    % \caption{Overall comp./sync. time (x100min).}
    \caption{Overall computation/synchronization time ($\times$100min).} 
    \label{comp_time_perc_tab}
    \begin{tblr}{
      row{2} = {c},
      cell{1}{1} = {r=2}{},
      cell{1}{2} = {c=3}{c},
      cell{3}{2} = {c},
      cell{3}{3} = {c},
      cell{3}{4} = {c},
      cell{4}{2} = {c},
      cell{4}{3} = {c},
      cell{4}{4} = {c},
      cell{5}{2} = {c},
      cell{5}{3} = {c},
      cell{5}{4} = {c},
      cell{6}{2} = {c},
      cell{6}{3} = {c},
      cell{6}{4} = {c},
      hline{1,3,7} = {-}{},
      hline{2} = {2-4}{},
    }
    Baseline        & Dataset &          &           \\
                    & SVHN    & CIFAR-10 & IMAGE-100 \\
    % FedRGD          & 84.8    & 27.7     & 32.0      \\
    % SemiFL          & \textbf{94.3}    & 29.1     & 28.9      \\
    % FedEMA          & 19.3    & 16.7     & 39.8      \\
    % Ours            & 66.4    & \textbf{38.2}     & \textbf{56.1}  

    FedRGD          & 1.4/0.3    & 4.3/11.4     & 13.2/28.0      \\
    SemiFL          & 3.3/0.2    & 5.1/11.9     & 15.2/29.3      \\
    FedEMA          & 1.1/2.8    & 2.1/9.4     & 13.3/19.2      \\
    Ours            & 2.2/2.9    & 2.4/9.5     & 14.0/18.9 
    \end{tblr}
\end{table}

Additionally, our system speeds up training by 3.0-3.3$\times$ for reaching 60\% test accuracy with VGG16 on the IMAGE-100 dataset compared to FedEMA and SemiFL, respectively. 
To investigate the secret of our training efficiency, we record the computing time (includes \srvUp and \locReg) and synchronization time (mainly includes communication) for all baselines in Table \ref{comp_time_perc_tab}. 
The results show that most of the time spent on training large-scaled models is not on computation, but on communication, where the advantages of our system lie. 
We reduced much of the time for communication with the help of the SFL framework, but we do not stop there: the clustering regularization promotes the model convergence with little extra cost. 
This results in competitive time cost performance against SemiFL and FedRGD, in which performing complete training and model transmission has became a burden for clients.

% Figure environment removed

% Figure environment removed

\subsubsection{Effect of Communication Cost}
We demonstrate the communication efficiency of our proposed system through Fig. \ref{comm_cost-fig}, which tracks the overall network traffic consumption until the target accuracy is reached. 
Our choice of split layers, as elaborated in Section \ref{split-layer-section}, played a pivotal role in achieving this efficiency. 
Our system exhibits the least amount of network traffic consumption in most cases. 
For example, in Fig. \ref{comm_cost-fig}(b), our system consumes only 85.1GB to achieve 80\% accuracy on CIFAR-10, while FedEMA, SemiFL, and FedRGD consume 98.6GB, 780.3GB, and 1,494.1GB, respectively. 
This translates to reductions of 13.7\%, 89.1\%, and 94.3\% in communication costs. 
Similarly, Fig. \ref{comm_cost-fig}(c) demonstrates that our system reduces communication cost by 70.2\% and 80.1\% compared to FedEMA and SemiFL, respectively, when reaching 60\% test accuracy on IMAGE-100. 
The low communication cost of both FedEMA and our system implies the training efficiency of the SFL framework since the communication time is a significant contributor to the overall training time. 
However, in certain cases, such as training a customized CNN model, the transmission cost of smashed data might outweigh the benefits of SFL.
Fig. \ref{comm_cost-fig}(a) shows that our system consumes approximately 66.6GB to achieve 88\% accuracy, which saves 21.9\% of that of FedEMA, though still 3.4$\times$ higher than SemiFL.

\subsubsection{Adaptability to Data Distribution}

\begin{table}[t]
    \centering
    \caption{Test accuracy (\%) of models on different datasets with $\alpha=0.1$.}
    \label{noniid_ds_acc_table}
    \begin{tblr}{
      row{2} = {c},
      cell{1}{1} = {r=2}{},
      cell{1}{2} = {c=3}{c},
      cell{3}{2} = {c},
      cell{3}{3} = {c},
      cell{3}{4} = {c},
      cell{4}{2} = {c},
      cell{4}{3} = {c},
      cell{4}{4} = {c},
      cell{5}{2} = {c},
      cell{5}{3} = {c},
      cell{5}{4} = {c},
      cell{6}{2} = {c},
      cell{6}{3} = {c},
      cell{6}{4} = {c},
      hline{1,3,7} = {-}{},
      hline{2} = {2-4}{},
    }
    Baseline        & Dataset &          &           \\
                    & SVHN    & CIFAR-10 & IMAGE-100 \\
    FedRGD          & 81.7    & 78.1     & 16.3      \\
    SemiFL          & 80.1    & 79.1     & 50.8      \\
    FedEMA          & 83.3    & 80.2     & 49.3      \\
    \textbf{Ours}           & \textbf{87.0}    & \textbf{82.8}     & \textbf{56.6}      
    \end{tblr}
\end{table}

We compare our system with baselines on various datasets dealing with data non-IIDness through sampling data from a Dirichlet distribution $Dir(\alpha)$ \cite{hsu2019measuring} for each client. 
The results are presented in Table \ref{noniid_ds_acc_table}, where different categories of data on each client follow a distribution of $Dir(0.1)$. 
From the results, our method achieves a consistent improvement over SemiFL and FedEMA on all three datasets. 
Concretely, our system outperforms SemiFL by 6.9 \%, 3.7\%, and 5.8\% on the three datasets, respectively. 
Also, compared to FedEMA, we improve the test accuracy by 3.7\%, 2.6\%, and 7.3\%, respectively. 
It demonstrates the effectiveness of our design in mitigating data non-IIDness, which is getting more significant on datasets with a wider variety of categories.

\begin{table}[t]
    \centering
    \caption{Test accuracy (\%) of models on CIFAR-10 with different data distributions.}
    \label{noniid_cifar10_acc_table}
    \begin{tblr}{
      column{2} = {c},
      column{3} = {c},
      column{4} = {c},
      column{5} = {c},
      cell{1}{1} = {r=2}{},
      cell{1}{2} = {c=4}{c},
      hline{1,3,7} = {-}{},
      hline{2} = {2-5}{},
    }
    Baseline & Data distribution &      &      &      \\
             & Dir(1.0)  & Dir(0.5)  & Dir(0.1)  & Dir(0.01) \\
    FedRGD   & 81.3      & 76.7     & 78.1      & 74.2 \\
    SemiFL   & 83.8      & 82.9     & 79.1      & 75.7 \\
    FedEMA   & 86.5      & 84.9     & 80.2      & 77.9 \\
    \textbf{Ours}     & \textbf{87.0}  & \textbf{86.7} & \textbf{82.8} & \textbf{81.2} 
    \end{tblr}
\end{table}

% Figure environment removed

Moreover, we investigate the adaptability of our method on different data skewness on CIFAR-10. 
The data distributions among clients in our settings are shown in Fig. \ref{dist_hm-fig} and we present the training process on these distributions in Fig. \ref{data_dist-fig}. 
Notably, the performance of FedRGD fluctuates dramatically around 100minutes when its group-wise aggregation is performed, highlighting the detrimental effect of aggregating both global and local models on model convergence.
Under such circumstance, our system still achieves promising performance.
Table \ref{noniid_cifar10_acc_table} provides a direct comparison of the final performance for different levels of non-IIDness. 
In some extreme cases, FedRGD fails to outperform Supervised-only, suggesting that the model parameters may be misled by the highly skewed unlabeled dataset. 
A common phenomenon is that the more the data is skewed, the more the model performance deteriorates. 
In contrast, our system consistently achieves the highest accuracy across all levels of data skewness, showcasing its adaptability to diverse data distributions. 
Under extreme non-IID scenarios such as $Dir(0.05)$, the test accuracy of our system improves by 3.3\% and 5.5\%, compared to FedEMA and SemiFL. 
The experimental results align with our original intention well with the assistance of pseudo clusters.
And the stable performance over various data distributions is also attributed to the solid foundation built in the \srvUp stage on strongly-augmented data, which is unfortunately often overlooked in current works.


\subsubsection{Impact of the Scale of Labeled Dataset}

% Figure environment removed

We conduct experiments on CIFAR-10, where we place 4,000 labeled data samples on the PS. 
However, in real-world scenarios, the amount of labeled data on the PS varies. 
To explore the impact of different scales of labeled datasets on our method, we conduct experiments by changing the amount of labeled data on the entire dataset. 
The results are presented in Fig. \ref{nlabeled-fig}. 
We observe that the test accuracy gradually decreased from 88.2\% to 64.2\% when the amount of labeled data varied from 4,000 to 250 in FedEMA. 
To investigate why the performances of baselines drop a lot when the amount of labeled data decreased from 500 to 250, we record the mask rate (the number of examples that are masked out) and data impurity (the error rate of unlabeled data that falls above the threshold) of FedEMA as an example in Fig. \ref{nlabels_compare-fig}(b). 
We infer that this issue is attributed to the increase in both the mask rate and data impurity, causing the data to become unreliable. 

% Figure environment removed

% % Figure environment removed

By contrast, our system predominates the quality of predictions, as shown in Fig. \ref{nlabels_compare-fig}(a), indicating the advantages of our proposed clustering regularization. 
Our method utilizes data samples with the largest class probability that fall beneath $\tau$, which is considered invalid in consistency regularization. 
Moreover, \srvUp on strong-augmented data provides a solid foundation for \locReg by enhancing model robustness, thus reducing impurities of unlabeled data. 
In addition, our results suggest that with only a few labeled data (perhaps less than 2\% of the overall dataset) is enough to satisfy some accuracy requirement, 80\% for example.

\subsubsection{Impacts of Split Layer}
\label{split-layer-section}

Our choice of split layers for each model is primarily based on the cost of communication and computation on the client side.
Fig. \ref{split_sizes-fig} illustrates the impacts of split layer on communication cost, with a communication period set to 50 and a batch size set to 64 on various datasets, except for IMAGE-100, which is set to 32 due to GPU memory constraints.
The communication cost during each round is revealed as the sum of the size of smashed data and size of client-side model. It is commonly known that the computation cost (floating-point operations) increases with the number of layers.  
However, this law does not hold true for communication costs, as the size of the full model concentrates on the top full-connected layers, but the size of smashed data is much larger in the first few layers than in the later ones. 
In fact, theminimum communication cost may exist in the hidden layer, as shown in Fig. \ref{split_sizes-fig}. 
Another factor to consider is the possible scaling of communication period, which would also scale the size of smashed data. 
For that, we prioritize to the split layer with smashed data being small-sized while ensuring it dose not compromise the effectiveness of clustering regularization.
% Finally, since the clustering regularization we proposed is expected to play the most of its role, the split layer should preferably not be the last two. 
Based on these considerations, the indices of the split layer we select are 2, 5 and 13 for CNN, AlexNet and VGG16, respectively. 

% \subsubsection{Effect of Communication Period}

% The superiority of our system in terms of time and communication cost can largely be attributed to the relatively short communication period (P) we have selected as shown in Fig. \ref{comm_period-fig}(b). Unlike baselines such as SemiFL and FedRGD that transmit the entire model, the communication cost of our system increases much less as the communication period decreases. For instance, when the communication period is reduced to 1/8 (from 200 to 50), the cost of our system and FedEMA increases by less than 2x, while SemiFL and FedRGD increase by 8x (from 4.2GB to 32.8GB) To validate the reasonableness of our choice of communication period (50), we conducted several experiments with different communication periods, including 25, 100, and 200, and compared them with our default setting with AlexNet on CIFAR-10.

% Fig. \ref{comm_period-fig}(a) shows that when the communication period is very small, for example, 25, the time cost for the model to achieve the target accuracy of 85\% is the longest, taking 195.2min. That is because to reach the same number of iterations, the smaller the communication period, the more time is spent on model transmission, as illustrated in Fig. \ref{comm_period-fig}(b). However, with a larger communication period, such as 200, the model performance does not improve faster as expected. It takes 140.4min to achieve 85\%, compared to when it is set to 100 that takes 133.2min. We hypothesize that this is due to the staleness of pseudo-labels, which can restrain local training as the parameters of the teacher model are frozen during local training. Updating the pseudo-labels requires frequent aggregation of knowledge through model averaging from other clients. Therefore, the communication period being fixed to 50 is a relatively sound setting. And as the communication period equals to $K_u$ in our system, the result shows the rationality of fixing the regularizing iteration numbers that we mentioned in Section \ref{intro}.


\subsubsection{The Impact of Global Updating Frequency Adaptation}
After conducting extensive experiments, we evaluate the impact of \InvReg frequency adaptation algorithm on the performance of our approach. 
We present the comparison in Fig. \ref{opt_alg-fig} between our system with (w/) and without (w/o) the algorithm in terms of accuracy and training loss. 
% Figure environment removed
The result shows that our adaptive adjustment of \InvReg frequency helps the model training reach an equilibrium closer to the global optima.
This is consistent with our expectation in Section \ref{analysis-section}, with a noticeable increase in final accuracy. 
This effect is particularly evident when dealing with smaller amounts of unlabeled data, as the average \InvReg frequency adapts to a lower value. 
For instance, with 250 labeled data on the PS, the adaptation strategy improves accuracy by 8.7\%, while the average \InvReg frequency is reduced to 0.65. 
Moreover, as seen in Fig. \ref{opt_alg-fig}(b), adapting the \InvReg frequency brings greater reduction in training loss, and the effect is especially obvious at several milestones that the \InvReg frequency decays. 
In contrast, with a constant \InvReg frequency (=50), the loss descent is hindered to a large extent because of the negative effect between our two training stages. 
Notably, the increase in the training loss during initial rounds is due to the increase of the amount of confident unlabeled data when the model performance is constantly improved with \srvUp. 
In summary, our adaptation algorithm serves as a good complement to our proposed method.