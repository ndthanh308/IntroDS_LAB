
Recently, Mobile and Internet of Things (IoT) devices generate vast amounts of data with great potential for improving the performance of various applications. 
To process these data locally from both privacy and economic perspectives, Federated Learning (FL) \cite{konevcny2016federated} has emerged.
It allows clients to collaboratively train Machine Learning (ML) models without sharing their private data, and further aggregates local model updates at the server into an improved global model \cite{mcmahan2017communication}. 
FL has found applications in diverse domains, including next-word-prediction, mobile edge computing and smart home \cite{li2020review}.

In many AI applications, large models with more parameters are always preferred due to their great capacity to capture complex patterns and relationships in the data, leading to better generalization and higher accuracy in various tasks \cite{arpit2017closer}. 
However, hardware limitations of edge devices and network bandwidth constraints between devices and remote parameter servers pose restrictions on typical FL.
This motivates Split Federated Learning (SFL) \cite{thapa2022splitfed}, a promising solution for efficiently training large-scale models. 
In SFL, a full model is split into two submodels. 
The output side, termed the top (sub-)model, is located on the server, while the input side, termed the bottom (sub-)model, resides on each client.
During training, clients transmit activations (\ie, \textit{smashed data}) of the split layer to the server, and the server separately processes the forward and back-propagation on the top model with each clientâ€™s smashed data in parallel. 
The model splitting reduces the computation burden on clients and the communication cost for model transmission, since a client only needs to download and upload the bottom model rather than the full model. 
For example, given an 8-layer AlexNet with size of 137 MB, when splitting the model at the fifth layer, the sizes of the bottom model and smashed data are about 9MB and 1MB, respectively. 
Considering that a client only holds the bottom model and the on-device raw data are processed locally, the privacy of both data and model is effectively safeguarded.

Despite the above benefits of SFL, two critical issues have to be addressed in SFL. 
Firstly, it is usually impractical that clients always have access to fully labeled data for model training.
% Besides, acquiring labels is a challenging and time-consuming task, resulting in a significant amount of untapped unlabeled data.
Considering that data annotation is time-consuming and probably acquires domain-specific expert knowledge, it is challenging to obtain sufficient labels, which makes large amounts of valuable unlabeled data untapped.
Secondly, SFL is sensitive to the non-Independent and Identically Distributed (non-IID) data among clients, which will result in varying performance degradation, %depending on the data skewness, 
as model weights trained on the non-IID data of different clients exhibit varying divergences \cite{zhao2018federated}.

To utilize the unlabeled data, prior works, such as \cite{lin2021semifed, kim2022federated}, incorporate the techniques of semi-supervised learning in FL, and assume that both the labeled and the unlabeled data are uniformly distributed across clients.
However, these works are not applicable in the scenarios where clients possess only the unlabeled data. 
Recent approaches \cite{jeong2020federated, zhang2021improving, diao2021semifl} in Semi-supervised FL (termed Semi-FL) address the local training predicament by generating pseudo-labels for unlabeled data using a global \textit{Teacher} model trained with labeled data on the server. 
Moreover, the works \cite{long2020fedsiam, wang2022enhancing, zhao2023does} enhance the reliability of pseudo-labels by updating the Teacher model with Exponential Moving Average (EMA) of previous model updates, thereby contributing to better performance.
Yet, when training on non-IID unlabeled data, the above methods either suffer significant performance degradation \cite{diao2021semifl} or sacrifice computation and communication efficiency \cite{jeong2020federated, zhao2023does}.
%if specifically optimized for such scenarios,

To this end, we aim to enhance model performance in a cost-efficient manner, and propose a novel approach called \textit{Clustering Regularization}.
The core idea is to leverage the strengths of multiple clients by directing smashed data towards shared \textit{pseudo} clusters, each composed of confident samples with the same pseudo-label.
Inherent to Semi-SFL, where the server collects smashed data for pseudo-labeling, we can seamlessly incorporate clustering regularization into the training process with direct access to the smashed data.
This yields an efficient system for Semi-supervised SFL (Semi-SFL) tailored to heterogeneous participants, termed as Pseudo-Clustering Semi-SFL.
However, there is still one pivotal factor that influences our regularization effectiveness.
In our system, training a model goes through two distinct and alternate steps: \srvUp, where the server updates the full model with labeled data, and \locReg, where clustering regularization is the training objective.
% where SFL on unlabeled data is performed with clustering regularization being the training objective.
Such inconsistency between the two steps is termed training inconsistency, and is validated to have an impact on model convergence in Section \ref{analysis-section}.

% , and it necessitates investigation into the mutual impact and control mechanisms to guarantee model convergence.

To address the inconsistency issue, we focus on measuring and controlling the effect of clustering regularization.
Typically, according to our theoretical analysis (Section \ref{analysis-section}), a relatively higher \InvReg frequency will lead to fewer training rounds for model convergence, but restrict the improvement of model performance.
We thus propose to adapt the \InvReg frequency for the dilemma.
Though adjusting the communication period will directly affect the effectiveness of clustering Regularization, it leads to erratic model divergence \cite{zhang2021client}, making the problem more complex.
As a result, we opt for adjusting the number of iterations during the \srvUp stage to indirectly control the effect of clustering rRegularization, termed \InvReg frequency adaptation.
Our system contributes to achieving the trade-off between training at a rather low computation or communication cost and mitigating performance decline on non-IID data. 
%While previous works has struggled with the trade-off between training at a rather low computing or communicating cost and mitigating performance decline on non-IID data, our system perfectly balance both. 
The main contributions of this paper is summarized as follows:
\vspace{-0.1cm}
\begin{itemize}
    \item We propose \textit{Pseudo-Clustering Semi-SFL}, which is the first to address the semi-supervised training problem in the SFL setting to the best of our knowledge. 
    The \textit{Clustering Regularization} integrates with SFL and effectively leverages unlabeled data even when it is non-IID.
    \item We analyse the influence of \InvReg frequency on model convergence and experimentally investigate its impact on the effectiveness of clustering regularization. 
    Building on this findings, we adapt the \InvReg frequency in response to expected changes in supervised loss to ensure the effectiveness of our method.
    \item We conduct experiments to evaluate the performance of our system. The experimental results show our system provides a 3.3$\times$ speed-up in training time by reducing 80.1\% of the communication cost while reaching the target accuracy. 
    Additionally, we are able to achieve accuracy improvements up to 6.9\% under non-IID scenarios compared to the state-of-the-art.
\end{itemize}