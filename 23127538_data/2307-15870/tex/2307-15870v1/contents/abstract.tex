Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. 
However, training and deploying large models for broader applications is challenging in resource-constrained environments. 
Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients.
SFL often assumes labeled data for local training on clients, however, it is not the case in practice.
Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. 
Herein, we propose \textit{Pseudo-Clustering Semi-SFL}, a novel system for training models in scenarios where labeled data reside on the server. 
By introducing \textit{Clustering Regularization}, model performance under data non-IIDness can be improved. 
Besides, our theoretical and experimental investigations into model convergence reveal that the inconsistent training processes on labeled and unlabeled data impact the effectiveness of clustering regularization.
Upon this, we develop a control algorithm for \InvReg frequency adaptation, which dynamically adjusts the number of \srvUp iterations to mitigate the training inconsistency.
%In doing so, the training inconsistency is effectively mitigated, ultimately prioritizing \textit{Clustering Regularization}.
Extensive experiments on benchmark models and datasets show that our system provides a 3.3$\times$ speed-up in training time and reduces the communication cost by about 80.1\% while reaching the target accuracy, and achieves up to 6.9\% improvement in accuracy under non-IID scenarios compared to the state-of-the-art.