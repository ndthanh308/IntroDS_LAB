% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{konevcny2016federated}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, D.~Ramage, and P.~Richt{\'a}rik,
  ``Federated optimization: Distributed machine learning for on-device
  intelligence,'' \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas,
  ``Communication-efficient learning of deep networks from decentralized
  data,'' in \emph{Artificial intelligence and statistics}.\hskip 1em plus
  0.5em minus 0.4em\relax PMLR, 2017, pp. 1273--1282.

\bibitem{li2020review}
L.~Li, Y.~Fan, M.~Tse, and K.-Y. Lin, ``A review of applications in federated
  learning,'' \emph{Computers \& Industrial Engineering}, vol. 149, p. 106854,
  2020.

\bibitem{arpit2017closer}
D.~Arpit, S.~Jastrz{\k{e}}bski, N.~Ballas, D.~Krueger, E.~Bengio, M.~S. Kanwal,
  T.~Maharaj, A.~Fischer, A.~Courville, Y.~Bengio \emph{et~al.}, ``A closer
  look at memorization in deep networks,'' in \emph{International conference on
  machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp.
  233--242.

\bibitem{thapa2022splitfed}
C.~Thapa, P.~C.~M. Arachchige, S.~Camtepe, and L.~Sun, ``Splitfed: When
  federated learning meets split learning,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~36, no.~8, 2022, pp. 8485--8493.

\bibitem{zhao2018federated}
Y.~Zhao, M.~Li, L.~Lai, N.~Suda, D.~Civin, and V.~Chandra, ``Federated learning
  with non-iid data,'' \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem{lin2021semifed}
H.~Lin, J.~Lou, L.~Xiong, and C.~Shahabi, ``Semifed: Semi-supervised federated
  learning with consistency and pseudo-labeling,'' \emph{arXiv preprint
  arXiv:2108.09412}, 2021.

\bibitem{kim2022federated}
W.~Kim, K.~Park, K.~Sohn, R.~Shu, and H.-S. Kim, ``Federated semi-supervised
  learning with prototypical networks,'' \emph{arXiv preprint
  arXiv:2205.13921}, 2022.

\bibitem{jeong2020federated}
W.~Jeong, J.~Yoon, E.~Yang, and S.~J. Hwang, ``Federated semi-supervised
  learning with inter-client consistency \& disjoint learning,'' \emph{arXiv
  preprint arXiv:2006.12097}, 2020.

\bibitem{zhang2021improving}
Z.~Zhang, Y.~Yang, Z.~Yao, Y.~Yan, J.~E. Gonzalez, K.~Ramchandran, and M.~W.
  Mahoney, ``Improving semi-supervised federated learning by reducing the
  gradient diversity of models,'' in \emph{2021 IEEE International Conference
  on Big Data (Big Data)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021,
  pp. 1214--1225.

\bibitem{diao2021semifl}
E.~Diao, J.~Ding, and V.~Tarokh, ``Semifl: Communication efficient
  semi-supervised federated learning with unlabeled clients,'' \emph{arXiv
  preprint arXiv:2106.01432}, 2021.

\bibitem{long2020fedsiam}
Z.~Long, L.~Che, Y.~Wang, M.~Ye, J.~Luo, J.~Wu, H.~Xiao, and F.~Ma, ``Fedsiam:
  Towards adaptive federated semi-supervised learning,'' \emph{arXiv preprint
  arXiv:2012.03292}, 2020.

\bibitem{wang2022enhancing}
L.~Wang, Y.~Xu, H.~Xu, J.~Liu, Z.~Wang, and L.~Huang, ``Enhancing federated
  learning with in-cloud unlabeled data,'' in \emph{2022 IEEE 38th
  International Conference on Data Engineering (ICDE)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2022, pp. 136--149.

\bibitem{zhao2023does}
J.~Zhao, S.~Ghosh, A.~Bharadwaj, and C.-Y. Ma, ``When does the student surpass
  the teacher? federated semi-supervised learning with teacher-student ema,''
  \emph{arXiv preprint arXiv:2301.10114}, 2023.

\bibitem{zhang2021client}
W.~Zhang, X.~Wang, P.~Zhou, W.~Wu, and X.~Zhang, ``Client selection for
  federated learning with non-iid data in mobile edge computing,'' \emph{IEEE
  Access}, vol.~9, pp. 24\,462--24\,474, 2021.

\bibitem{gupta2018distributed}
O.~Gupta and R.~Raskar, ``Distributed learning of deep neural network over
  multiple agents,'' \emph{Journal of Network and Computer Applications}, vol.
  116, pp. 1--8, 2018.

\bibitem{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, N.~Carlini, Z.~Zhang, H.~Zhang, C.~A. Raffel, E.~D.
  Cubuk, A.~Kurakin, and C.-L. Li, ``Fixmatch: Simplifying semi-supervised
  learning with consistency and confidence,'' \emph{Advances in neural
  information processing systems}, vol.~33, pp. 596--608, 2020.

\bibitem{li2021model}
Q.~Li, B.~He, and D.~Song, ``Model-contrastive federated learning,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 10\,713--10\,722.

\bibitem{lee2022contrastive}
D.~Lee, S.~Kim, I.~Kim, Y.~Cheon, M.~Cho, and W.-S. Han, ``Contrastive
  regularization for semi-supervised learning,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  3911--3920.

\bibitem{khosla2020supervised}
P.~Khosla, P.~Teterwak, C.~Wang, A.~Sarna, Y.~Tian, P.~Isola, A.~Maschinot,
  C.~Liu, and D.~Krishnan, ``Supervised contrastive learning,'' \emph{Advances
  in Neural Information Processing Systems}, vol.~33, pp. 18\,661--18\,673,
  2020.

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick, ``Momentum contrast for
  unsupervised visual representation learning,'' in \emph{Proceedings of the
  IEEE/CVF conference on computer vision and pattern recognition}, 2020, pp.
  9729--9738.

\bibitem{li2019convergence}
X.~Li, K.~Huang, W.~Yang, S.~Wang, and Z.~Zhang, ``On the convergence of fedavg
  on non-iid data,'' \emph{arXiv preprint arXiv:1907.02189}, 2019.

\bibitem{yang2021achieving}
H.~Yang, M.~Fang, and J.~Liu, ``Achieving linear speedup with partial worker
  participation in non-iid federated learning,'' \emph{arXiv preprint
  arXiv:2101.11203}, 2021.

\bibitem{haddadpour2019convergence}
F.~Haddadpour and M.~Mahdavi, ``On the convergence of local descent methods in
  federated learning,'' \emph{arXiv preprint arXiv:1910.14425}, 2019.

\bibitem{ajalloeian2020convergence}
A.~Ajalloeian and S.~U. Stich, ``On the convergence of sgd with biased
  gradients,'' \emph{arXiv preprint arXiv:2008.00051}, 2020.

\bibitem{arazo2020pseudo}
E.~Arazo, D.~Ortego, P.~Albert, N.~E. Oâ€™Connor, and K.~McGuinness,
  ``Pseudo-labeling and confirmation bias in deep semi-supervised learning,''
  in \emph{2020 International Joint Conference on Neural Networks
  (IJCNN)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 1--8.

\bibitem{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng, ``Reading
  digits in natural images with unsupervised feature learning,'' 2011.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton \emph{et~al.}, ``Learning multiple layers of features
  from tiny images,'' 2009.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein \emph{et~al.}, ``Imagenet large scale
  visual recognition challenge,'' \emph{International journal of computer
  vision}, vol. 115, pp. 211--252, 2015.

\bibitem{krizhevsky2017imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' \emph{Communications of the ACM},
  vol.~60, no.~6, pp. 84--90, 2017.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{berthelot2019mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~A.
  Raffel, ``Mixmatch: A holistic approach to semi-supervised learning,''
  \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter, ``Sgdr: Stochastic gradient descent with warm
  restarts,'' \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{hsu2019measuring}
T.-M.~H. Hsu, H.~Qi, and M.~Brown, ``Measuring the effects of non-identical
  data distribution for federated visual classification,'' \emph{arXiv preprint
  arXiv:1909.06335}, 2019.

\bibitem{kairouz2021advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings \emph{et~al.}, ``Advances
  and open problems in federated learning,'' \emph{Foundations and
  Trends{\textregistered} in Machine Learning}, vol.~14, no. 1--2, pp. 1--210,
  2021.

\bibitem{li2020federated}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith,
  ``Federated optimization in heterogeneous networks,'' \emph{Proceedings of
  Machine learning and systems}, vol.~2, pp. 429--450, 2020.

\bibitem{lee2013pseudo}
D.-H. Lee \emph{et~al.}, ``Pseudo-label: The simple and efficient
  semi-supervised learning method for deep neural networks,'' in \emph{Workshop
  on challenges in representation learning, ICML}, vol.~3, no.~2, 2013, p. 896.

\bibitem{laine2016temporal}
S.~Laine and T.~Aila, ``Temporal ensembling for semi-supervised learning,''
  \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem{xie2020unsupervised}
Q.~Xie, Z.~Dai, E.~Hovy, T.~Luong, and Q.~Le, ``Unsupervised data augmentation
  for consistency training,'' \emph{Advances in neural information processing
  systems}, vol.~33, pp. 6256--6268, 2020.

\bibitem{lin2020ensemble}
T.~Lin, L.~Kong, S.~U. Stich, and M.~Jaggi, ``Ensemble distillation for robust
  model fusion in federated learning,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~33, pp. 2351--2363, 2020.

\end{thebibliography}
