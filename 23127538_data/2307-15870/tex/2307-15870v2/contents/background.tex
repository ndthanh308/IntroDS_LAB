
In this section, we provide a description of semi-supervised SFL, followed by an exploration of the underlying motivation behind our system design.
For ease of expression, we list some key notations in Table \ref{notation-table}.

% % Figure environment removed

\subsection{Semi-supervised Split Federated Learning} \label{description-section}
Split Federated Learning (SFL) \cite{thapa2022splitfed} is a combination of Federated Learning (FL) \cite{konevcny2016federated} and Split Learning (SL) \cite{gupta2018distributed}. 
FL is designed to train ML models on distributed edge devices with privacy-preserving, while SL aims at reducing the computation cost of participants with limited resources. 
Therefore, SFL inherits the advantages of both FL and SL. 

Similar to FL, a Parameter Server (PS) and $N$ clients collaboratively train an ML model (denoted as $\boldsymbol{w}$) in SFL. 
The model is split into two submodels as $\boldsymbol{w} = (\boldsymbol{w}_s, \boldsymbol{w}_c)$. 
The bottom model $\boldsymbol{w}_c$ and the top model $\boldsymbol{w}_s$ separately reside on clients and the PS. 
In general, the PS randomly selects a subset $V_h \in V$, consisting of $N_h = |V_h|$ ($N_h \leq N$) active clients at round $h$. 
% duplicated
The selected clients with bottom models are instructed to send features \cite{thapa2022splitfed} and ground-truth labels $y_{i}$ ($i\in[V_h]$) to the PS. 
The PS then calculates the gradients of features for each client in parallel and sends them to clients while accumulating the gradients $\Delta \boldsymbol{w}_s$ of the top model. 
Afterward, the clients perform backpropagation to update their bottom models.
At the end of each round, the PS aggregates bottom models to obtain a global model for further training.

However, considering the lack of sufficient expert knowledge or labor on clients, it is usually practical that most or even all data on clients are unlabeled while the PS possesses some labeled data annotated by domain experts \cite{diao2021semifl,zhao2023does}.
Thus, it is appealing to develop Semi-supervised SFL (Semi-SFL),
% In Semi-supervised SFL (Semi-SFL), clients have access to unlabeled data, while the PS possesses the labeled dataset annotated by domain experts.
where the complete dataset $\mathcal{D}$ consists of labeled dataset $\mathcal{D}_l$ and unlabeled dataset $\mathcal{D}_u$, and $\mathcal{D}_u \triangleq \mathcal{D}_{u, 1} \cup \mathcal{D}_{u, 2} \cup \cdots \cup \mathcal{D}_{u, N}$ ($\mathcal{D}_{u, i}$ is the dataset of client $i$). 
The loss function over a labeled data sample $(x, y)$ and the model parameter $\boldsymbol{w} \in \mathbb{R}^d$ with $d$ dimensions is defined as $\ell_{s}(x, y, \boldsymbol{w})$.
Thus, considering the supervised training on the labeled dataset $\mathcal{D}_l$, the loss function is $ \mathbb{E}_{x \in \mathcal{D}_l} \ell_s(x, y, \boldsymbol{w})$.

To leverage the vast amounts of unlabeled data on clients, recent studies have achieved promising results by enforcing model predictions on augmented data that deviate significantly from the data distribution. 
The objective is to ensure the alignment between these predictions with their corresponding pseudo-labels \cite{sohn2020fixmatch}. 
In other words, for a given unlabeled data sample $x$, the model's prediction of its weakly-augmented version is represented as a vector $q=(q_{1}, \cdots, q_{M}) \in [0, 1]^M$, where $\sum_{m=1}^M{q_{m}} = 1$, and $M$ is the number of classes. 
The pseudo-label for $x$ is then defined as $\hat{q} =\argmax_m {q_{m}}$, and is retained only if $\max_m {q_{m}}$ falls above the pre-defined confidence threshold $\tau$. 

Let $\mathcal{H}$ denote the cross-entropy loss function, which measures the discrepancy between the predicted and true labels. 
The unsupervised training loss with consistency regularization can be represented as:
\begin{equation}
     \ell_{u}(x, \boldsymbol{w}) = \mathbbm{1}(\underset{m}{\max} {(q_{m})} > \tau) \mathcal{H}(x, \hat{q}, \boldsymbol{w})
\end{equation}
Then the total training objective is expressed as:
\begin{equation}
   F(\boldsymbol{w}^*) \triangleq \underset{\boldsymbol{w} \in \mathbb{R}^d}{\min \; } \left[\mathbb{E}_{x \in \mathcal{D}_l} \ell_{s}(x, y, \boldsymbol{w}) + \mathbb{E}_{x \in \mathcal{D}_u} \ell_{u}(x, \boldsymbol{w}) \right]
\end{equation}

\begin{table}[tb]
\centering
\caption{Key notations.}
\begin{tabular}{cp{6cm}}
    \toprule  % top lines
    \textbf{Notation} & \makecell[c]{\textbf{Semantics}} \\ 
    \midrule
    % $N$ & number of clients \\
    $N_h$ & number of active clients in round $h$ \\
    
    % $V$ & set of clients \\
    $V_h$ & set of active clients in round $h$ \\
    $\mathcal{D}_s$ & labeled dataset on the PS \\
    $\mathcal{D}_u$ & unlabeled dataset across clients \\
    $\boldsymbol{w}_s$ & top model on the PS\\
    $\boldsymbol{w}_{c,i}$ & bottom model on client $i$ \\
    $\boldsymbol{w}_p$ & projection head on the PS\\
    $\tilde{\boldsymbol{w}}$ & teacher model  \\
    $\boldsymbol{w}^h$ & model at the beginning of round $h$  \\
    $\srvmodh$ & model after supervised training in round $h$  \\
    $F(\boldsymbol{w})$ & loss function of the entire model \\
    $e_i$ & student features by client $i$ \\
    $\tilde{e}_i$ & teachers features by client $i$ \\
    $f_s(\cdot)$   & loss function for supervised training \\
    $f_u(\cdot)$   & loss function for semi-supervised training \\
    % $\mathcal{Q}$ & memory queue on the PS \\
    $H$     & \# of aggregation rounds \\
    $K_s$   & \# of iterations for server-side supervised training, \ie, the global updating frequency \\
    $K_u$   & \# of iterations for cross-entity semi-supervised training, \ie, the cross-entity updating frequency \\
    % $P$     & aggregation period \\
    $\eta_h$ & learning rate in round $h$ \\
    \bottomrule  % bottom lines
\end{tabular}
\label{notation-table}
\vspace{-0.2cm}
\end{table}

\subsection{Motivation for System Design} \label{motivation-section}
In Semi-FL, the consistency regularization loss encounters a major hurdle when the unlabeled data is non-IID among clients, as local models tend to favor locally abundant categories while potentially disregarding minorities. 
Although each client might train an expert model, the model divergence among clients is exacerbated when the pre-defined confidence threshold ($\tau$) blocks out data samples belonging to minority categories, especially in the early stages of training. 
Consequently, such imbalance in training leads to biased features \cite{li2021model}, resulting in logits with low confidence which are more likely to be excluded from consistency regularization.
% hampers overall performance after aggregation.  
Therefore, our objective is to effectively utilize the unlabeled data by exploring inter-client knowledge while ensuring efficient communication and computation.
% , and further alleviating the non-IID problem.

Fortunately, the nature of SFL provides inspiration for a solution.
In Semi-SFL, pseudo-labeling is performed on the PS where the top model resides. 
As shown in Fig. \ref{motiv-fig} (left), each client transmits two types of features for loss calculation: teacher features from the teacher bottom model (used for pseudo-labeling), and student features from the local bottom model (used for computing gradients).
It is worth noting that the features collected on the PS show less skewness and higher abundance compared to those on each client. 
The fact that features are mutually accessible on the PS allows for leveraging the knowledge contained in features from multiple clients to mitigate the negative impact of non-IID data.

To this end, we propose to let the model learn more potential knowledge from features, enabling it to make confident predictions by identifying patterns and commonalities in the input.
We modify a contrastive loss \cite{lee2022contrastive} for our purpose and propose \textit{Clustering Regularization}.
Concretely, for efficient processing, the features are first projected into lower-dimension vectors with the help of a two-layer linear network, \ie, projection head.
Subsequently, the (low-dimension) teacher features form different clusters (called \textit{pseudo clusters}) regarding the pseudo-labels in the feature space, as illustrated in Fig. \ref{motiv-fig} (left). 
The contrastive loss then regularizes the projected student features of unlabeled samples to move towards the pseudo clusters of the same classes, as depicted in Fig. \ref{motiv-fig} (right). 
After clustering regularization, student features with the same pseudo-labels get closer to each other, which generates useful guideline information for model updating.
Owing to the computation and communication-friendly framework of SFL, the execution of clustering regularization for all participating clients is completed on the PS, relieving clients from the corresponding computational and aggregation overhead.

% Figure environment removed

Powered by clustering regularization, we can fully exploit the potential of unlabeled data, including the samples that are filtered by the confidence threshold and further excluded from consistency regularization. 
By leveraging the knowledge encoded in the pseudo clusters, even data samples belonging to minority classes can contribute to model training.
% which effectively propagates to the bottom models on clients.
From another perspective, feature clustering assists and accelerates the process of consistency regularization, since well-clustered features provide data samples a higher chance to be credited with high confidence by the top model. 
Importantly, clustering regularization is not directly applied to the top model, which avoids introducing confirmation bias \cite{lee2022contrastive} and ensures that the model is always focused on the correct goal.
% Importantly, we preserve the original pseudo-labels for training the top model.
% This avoids introducing confirmation bias \cite{lee2022contrastive}, ensuring the model remains focused on its correct objective.
% Thus though all the student features are identified by pseudo labels that might be error, our approach avoids introducing confirmation bias \cite{lee2022contrastive}, ensuring the model stays focused on its correct goal.
 
