
% Recently, \bluenote{vast amounts of data generated by mobile and Internet of Things (IoT) devices have exhibited great potential for improving the performance of various applications. 
% To process these data from both privacy and economic perspectives, Federated Learning (FL) \cite{konevcny2016federated} has emerged.
% In FL, clients (\eg, end devices) are encouraged to collaboratively train Machine Learning (ML) models locally without sharing their private data, and the local model updates are further aggregated by the server into an improved global model \cite{mcmahan2017communication}.}
% \bluenote{FL has been extensively applied in diverse domains, including intelligent medical \cite{liang2022new}, finance \cite{wen2023survey} and smart home \cite{li2020review}}.

Recently, \bluenote{vast amounts of data generated by mobile and Internet of Things (IoT) devices have exhibited great potential for improving the performance of various applications. 
To process these data from both privacy and economic perspectives, Federated Learning (FL) \cite{konevcny2016federated} has emerged and been extensively applied in many AI applications, including intelligent medical \cite{liang2022new}, finance \cite{wen2023survey} and smart home \cite{li2020review}.
In order to capture complex patterns and relationships in massive data, large-scale models
% , like ViT \cite{dosovitskiy2020image}, 
have achieved better generalization and higher accuracy in various tasks \cite{arpit2017closer}.}
However, hardware limitations of clients and network bandwidth constraints between clients and remote servers pose restrictions on typical FL.
This motivates Split Federated Learning (SFL) \cite{han2021accelerating, thapa2022splitfed, liao2023accelerating}, a promising solution for efficiently training large-scale models. 

\bluenote{In SFL, a full model is divided into two submodels, \ie, a top model (close to the output layer) and a bottom model (close to the input layer), at an intermediate layer (called \textit{split layer}). 
The top model is updated on the server, while the bottom model is trained on each client.
During training, clients transmit activations (also called \textit{features}) of the split layer to the server, which further performs forward and backward propagation on the top model with the features and returns the corresponding gradients. 
For example, given an 8-layer AlexNet with the size of 137 MB, when splitting the model at the 5th layer, the sizes of its bottom model and features are about 9 MB and 1 MB, respectively.
Accordingly, model splitting contributes to reducing computation burden and communication costs for clients.
Besides, since clients have access to only the bottom models, the privacy of the model's complete structure is protected.}
% Furthermore, a client only holds the bottom model, and its raw data are processed locally, thus the privacy of both the data and the model is effectively safeguarded.}

Despite the above benefits of SFL, two critical issues have to be addressed in SFL. 
Firstly, it is usually impractical that SFL always has access to fully labeled data for model training.
% Besides, acquiring labels is a challenging and time-consuming task, resulting in a significant amount of untapped unlabeled data.
Considering that data annotation is time-consuming and probably requires domain-specific expert knowledge, it is challenging to obtain sufficient labels, which makes large amounts of valuable unlabeled data untapped.
\bluenote{Secondly, the clients usually belong to different individuals and/or work under different circumstances, thus data generated by different clients probably are non-Independent and Identically Distributed (non-IID).
% since SFL is sensitive to the non-Independent and Identically Distributed (non-IID) data among clients, 
Models trained on non-IID data across different clients usually exhibit varying divergences \cite{zhao2018federated},
which will result in performance degradation. %depending on the data skewness, 
}

\bluenote{ 
To our best knowledge, the above two issues have not been addressed in SFL literature up to now.
We are encouraged to find some inspiration in previous FL works.
To make full use of unlabeled data, many FL works \cite{lin2021semifed, kim2022federated} integrate the techniques of semi-supervised learning and propose Semi-supervised FL (termed Semi-FL). 
Some Semi-FL works assume clients possess labeled data while the server has unlabeled data \cite{wang2022enhancing, lin2020ensemble}.
However, considering the lack of sufficient expert knowledge and/or labor, clients probably have more unlabeled data.
Herein, we focus on a more practical scenario, where labeled data are maintained by the server and unlabeled data reside on the clients \cite{albaseer2020exploiting, zhang2021improving}.
In this case, Semi-FL first performs server-side supervised training on the labeled data, and then conducts client-side semi-supervised training on the unlabeled data using techniques like pseudo-labeling \cite{diao2021semifl,jeong2021federated,long2020fedsiam,zhao2023does}.
For example, Diao \etal \cite{diao2021semifl} directly distribute and employ the up-to-date global model to generate pseudo-labels for the unlabeled data, and further train the local models (replicas of the global model) on clients in parallel.
As in typical FL, the non-IID issue inevitably reduces the generalization ability of trained models in \cite{diao2021semifl}.
% predicament by generating pseudo-labels for the unlabeled data with a global model in advance. 
% Following that, clients continue their training on the local model by enforcing consistency between stochastic outputs and pseudo-labels.
}

% \bluenote{
% To deal with the non-IID issue in Semi-FL, researchers propose to build up a  regarding its impairment on model performance.
% Acknowledge this, FedSiam \cite{long2020fedsiam} employs an Exponential Moving Average (EMA) teacher, retaining long-term information from previous training on non-IID data. 
% FedSwitch \cite{zhao2023does} dynamically switches between EMA teacher and student models, contributing to lower communication cost and better generalization performance than FedSiam.
% Furthermore, to fully harness the knowledge from multiple models trained on non-IID data, FedMatch \cite{jeong2021federated} enforces consistency between predictions across these models. 
% Recent advancement pFedKnow \cite{wang2023knowledge} equips the server with a pretrained model that serves a dual purpose: initializing the lightweight global model and guiding the aggregation of personalized client models. 
% Despite its effectiveness in addressing the computational inefficiency of FedMatch, pFedKnow relies on prior knowledge (a pre-trained model) that may be inaccessible for new tasks or datasets.
% }
\bluenote{
For the non-IID issue in Semi-FL, many researchers have made efforts to improve the quality of pseudo-labels.
For example, each client in \cite{jeong2021federated} is provided with multiple helpers (\ie, models of other clients with similar predictions given the same input) to generate high-quality pseudo-labels, but at the expense of increased communication costs.
To limit the communication costs, an Exponential Moving Average (EMA) model (regarded as a \textit{teacher} model) is constructed and widely utilized to guide the updating of each client's local model (regarded as a \textit{student} model) through pseudo-labeling \cite{long2020fedsiam,zhao2023does}.
As the teacher models retain not only the generalized knowledge from historical teacher models but also the personalized knowledge from recent global models, they can help to promote the semi-supervised training on clients well. 
Besides the teacher models, some literature \cite{wang2023knowledge} employs a pre-trained model to initialize the global model and guide the aggregation of student models, so as to further improve the training performance.
However, all the existing Semi-FL arts necessitate full models for semi-supervised learning on clients, which is not applicable in SFL, since clients possess only the bottom models. 
}

\bluenote{
To advance SFL in utilizing unlabeled data, it is intuitive to incorporate semi-supervised learning in SFL.
In a naive Semi-supervised SFL (Semi-SFL) solution, both the global model and the teacher model are split and their bottom models are allocated to the clients.
It mainly involves two alternate phases in a training round: i) server-side supervised training, where the server updates the global and teacher models with generally IID labeled data, and ii) cross-entity semi-supervised training, where client-side teacher/student bottom models produce the teacher/student features with non-IID data, which are separately sent and fed to the server-side top model for pseudo-labeling and model training.
% and ii) cross-entity semi-supervised training, where the clients with non-IID data calculate the teacher/student features using teacher/student bottom models, and separately feed the teacher and student features to the server-side top model for pseudo-labeling and model training.
However, this solution only adopts the output logits and pseudo-labels for loss calculation, failing to adequately tackle the non-IID issue (refer to Section \ref{evaluation}).
% neglecting the rich information contained in the features.
}

% Consequently, in non-IID scenarios, the biased features is more likely to be excluded from loss calculation due to low confidence, leading to degraded model performance.
% Hence, there is an urgent need for a novel and compatible approach for SFL.}

% \bluenote{
% To this end, we review the distinct properties of SFL and propose a novel Semi-supervised SFL (Semi-SFL) approach that incorporates \textit{Clustering Regularization} to utilize unlabeled and non-IID data on clients.
% Building on the insight that \textit{the features collected on the server demonstrates reduced skewness compared to individual instances}, clustering regularization aims to fully exploit inter-client knowledge from features.
% This is implemented by rectifying imbalanced loss calculation by steering non-IID features towards balanced clusters, discerned by pseudo labels.}
% Inherent to SFL, the lightweight nature of clients is perfectly preserved.
% % where the server collects features for pseudo-labeling, the lightweight nature of clients is preserved.
% However, there is still one pivotal factor that influences our regularization effectiveness.
% In our system, the training objective in the \textit{x-entity semi-supervised training} step is distinct from the \textit{server-side supervised training} step.
% Such inconsistency between the two steps is termed training inconsistency, and we have validated its negative impact on model convergence in Section \ref{analysis-section}.
% \bluenote{
\bluenote{To this end, we review the distinct properties of SFL and propose a novel Semi-SFL system that incorporates \textit{Clustering Regularization} to perform SFL with unlabeled data.
% and non-IID client data.
% In SFL-CR, the server has access to both features and pseudo-labels, thus the potential knowledge contained in features from different clients can be extracted and employed for semi-supervised training.
% Under the non-IID setting, the student features from student bottom models of different clients are observed to diverge from the features (called teacher features) produced by teacher bottom models in the feature space, since the student bottom models may more likely overfit local non-IID data than the teacher bottom models, resulting in logits with low confidence by the server-side top model.
% Therefore, we can regularize the student bottom models by steering the student features towards balanced clusters comprising all the accessible teacher features, which motivates the design of clustering regularization.
% Under the non-IID setting, 
In Semi-SFL, the teacher bottom models inherit the advantages (\eg, high generalization ability) of teacher models.
Considering that the student bottom models are gradually trained on the non-IID client data as training progresses, they are more likely to overfit the client data compared to the teacher bottom models.
% Considering that the student bottom models are gradually trained on the non-IID client data as training progresses, they are more likely to overfit the client data compared to the teacher bottom models which inherit the advantages (\eg, high generalization ability) of teacher models.
Thus, the student features usually suffer from non-negligible \textit{feature shift} \cite{li2021fedbn} and reduced representative ability. 
% \ie, features diverge from their correct and expected positions in the feature space.
% the features (including teacher and student features) from bottom models may diverge from their correct positions in the feature space, and the student features probably suffer from more severe disturbance, since the student bottom models are more likely to overfit local non-IID data than the teacher bottom models.
% Fortunately, 
% owing to the reliable representative ability of teacher features,
% the teacher features, preserving long-term information from previous training, mitigate that shift, 
Fortunately, the clustering of teacher features provides a comprehensive view of the feature distribution, which is robust to data skewness.
As a result, the clustering results can be utilized to regularize the student bottom models, so as to improve their generalization ability, which motivates the design of clustering regularization.}
% Therefore, we can regularize the student bottom models based on the clustering results, so as to improve the generalization ability of student bottom models, which motivates the design of clustering regularization.
% Therefore, we can regularize the student bottom models by steering the student features towards the cluster centers of teacher features, so as to improve the generalization ability of student bottom models, which motivates the design of clustering regularization.
\bluenote{Furthermore, according to our theoretical and experimental analysis in Section \ref{analysis-section}, the global updating frequency (\ie, the number of times when performing server-side supervised training in a round) has a complicated influence on model convergence, and it is quite challenging to adaptively adjust the global updating frequency to achieve satisfied training performance.}
% To address the inconsistency issue, we focus on measuring and controlling the effect of clustering regularization.
% According to our theoretical analysis in Section \ref{analysis-section}, a relatively higher \InvReg frequency will lead to fewer training rounds until model convergence, but restrict the improvement of model performance.
% We thus propose to adapt the \InvReg frequency for the dilemma.
% Though adjusting the communication period will directly affect the effectiveness of clustering regularization, it leads to erratic model divergence \cite{zhang2021client}, making the problem more complex.
% As a result, we opt for adjusting the number of iterations during the \srvUp stage for indirect control, termed \InvReg frequency adaptation.
% \bluenote{
% Outlined in Table \ref{prop-cmpr}, our system contributes to achieving a delicate balance between maintaining lightweight clients and alleviating performance decline on non-IID data, all without requiring any prior knowledge.
% }

Our main contributions are summarized as follows:
% \vspace{-0.1cm}
\begin{itemize}
    \item \bluenote{We develop a novel Semi-SFL system incorporating clustering regularization, which to the best of our knowledge is the first solution to simultaneously address the issues of unlabeled and non-IID data in SFL.}
    \item \bluenote{We theoretically and experimentally investigate the impact of global updating frequency on model convergence.
    Building on these findings, we dynamically and adaptively regulate the global updating frequency in response to expected changes in supervised
    loss to guarantee the effectiveness and efficiency of our proposed system.}
    \item We conduct extensive experiments to evaluate the performance of our system. 
    The experimental results show our system provides a 3.0$\times$ speed-up in training time by reducing 70.3\% of the communication cost while reaching the target accuracy. 
    Besides, it achieves accuracy improvements up to 5.1\% under different non-IID settings compared to the state-of-the-art baselines.
\end{itemize}


% \begin{table}
% \centering
% \caption{\bluenote{Comparison between existing Semi-FL approaches and ours.}}
% \begin{tabular}{c|cccc}
% \toprule
% \multirow{2}{*}{\textbf{Approach}}  & Model     & Data          & Client    & Prior \\
%                                     & Privacy   & Heterogeneity & FLOPS     & Free \\
% \midrule
% SemiFL\cite{diao2021semifl}         & \ding{55} & \ding{55} & \textit{High} & \ding{51} \\
% FedMatch\cite{jeong2021federated}   & \ding{55} & \ding{51} & \textit{High} & \ding{51} \\
% FedSiam\cite{long2020fedsiam}       & \ding{55} & \ding{51} & \textit{High} & \ding{51} \\
% FedSwitch\cite{zhao2023does}        & \ding{55} & \ding{51} & \textit{High} & \ding{55} \\
% pFedKnow\cite{wang2023knowledge}    & \ding{55} & \ding{51} & \textit{Low}  & \ding{55} \\
% % \rowcolor{red!15}
% \textbf{Ours}                       & \ding{51} & \ding{51} & \textit{Low}  & \ding{51} \\      
% \bottomrule
% \end{tabular}
% \label{prop-cmpr}
% \end{table}