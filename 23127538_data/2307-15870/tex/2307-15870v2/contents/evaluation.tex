In this section, we begin by providing a list of the datasets and models, as well as a description of the devices utilized in the experiments. 
We then introduce the adopted baselines and metrics for performance comparison. 
Finally, we present our evaluation results and analyze the superiority of our proposed system under various scenarios.

\subsection{Datasets and Models}
\textbf{Datasets:} We conduct experiments on three commonly used real-world datasets for semi-supervised learning: SVHN \cite{netzer2011reading}, CIFAR-10 \cite{krizhevsky2009learning}, and IMAGE-100. 
The SVHN dataset contains 73,257 digits for training and 26,032 digits for testing, which are labeled in 10 classes. 
As suggested in \cite{diao2021semifl}, 1,000 labeled digits in the training set are allocated for the PS, while the remaining training data are distributed to clients as unlabeled data.
The CIFAR-10 dataset is an image dataset consisting of 60,000 32$\times$32 color images (50,000 for training and 10,000 for testing) in 10 categories, and 4,000 images are set as labeled data on the PS. 
To evaluate our proposed system on a more challenging task, we create the IMAGE-100 dataset, which is a subset of ImageNet \cite{russakovsky2015imagenet} and contains 100 out of 1,000 categories. 
Each sample in IMAGE-100 is resized to the shape of 144$\times$144$\times$3, and 5,000 labeled images are allocated for the PS.

\textbf{Models:} Three models with different types and structures are adopted on the above three real-world datasets for performance evaluation: (i) CNN on SVHN, (ii) AlexNet \cite{krizhevsky2017imagenet} on CIFAR-10, (iii) VGG16 \cite{simonyan2014very} on IMAGE-100. 
% (iii) VGG16 \cite{simonyan2014very} on IMAGE-100. 
For SVHN, we train a customized CNN model with size of 3.35MB, which has two 5$\times$5 convolutional layers, a fully-connected layer with 512 units, and a softmax output layer with 10 units. 
Besides, for CIFAR-10, we train the AlexNet model composed of three 3$\times$3 convolutional layers, one 7$\times$7 convolutional layer, one 11$\times$11 convolutional layer, two fully-connected hidden layers, and one softmax output layer. 
Finally, the VGG16 model, which consists of 13 convolutional layers with kernel size of 3$\times$3, two fully-connected layers and a softmax output layer, is trained for image classification of IMAGE-100.
% \bluenote{Finally, the LeViT model, which consists of 4 convolutional layers with kernel size of 3$\times$3, 3 stages with 384, 512, 768 channels, each containing 4 pairs of Attention and MLP blocks, and two fully-connected layers and a softmax output layer, is trained on IMAGE-100.}

\subsection{Baselines and Metrics}
\textbf{Baselines:} We compare our proposed system with the following four baselines.
\begin{itemize}
\item[\textbullet] \textbf{Supervised-only}: 
Supervised-only refers to using only the labeled dataset available on the PS for supervised training. 
This represents the lower bound that can be achieved with a limited amount of labeled data.
% \item[\textbullet] \textbf{FedRGD} \cite{zhang2021improving}: Given labeled data on the PS and unlabeled data on clients, FedRGD performs supervised and semi-supervised training over the labeled and unlabeled data in parallel. 
% To aggregate models, FedRGD divides the clients into groups randomly and performs group-wise averaging (with the latest global model from the PS), and generates the group models for data labeling and further training.
% To ensure model convergence, the initial global model is trained with the labeled data on the PS for the first 50 rounds.

% \item[\textbullet] \textbf{FedEMA}: It is a simplified version of our proposed system without the operation of incorporating clustering regularization. 
% FedEMA serves as an ablation study to demonstrate the effectiveness of our clustering regularization under various scenarios.

\item[\textbullet] \textbf{SemiFL} \cite{diao2021semifl}: 
SemiFL is the first approach for semi-supervised FL with accuracy on par with standalone training.
% At the beginning of each communication round, 
In SemiFL, pseudo-labels for the local data of each client are generated upon the latest global model.
While in a certain iteration, each client applies the Mixup \cite{berthelot2019mixmatch} technique on each data batch to augment the data and then perform forward and backward propagation using a specialized ``mix'' loss.

\item[\textbullet] \bluenote{
\textbf{FedMatch} \cite{jeong2021federated}: 
% To address Semi-FL problems across clients with both IID and non-IID data, FedMatch focuses on learning inter-client consistency by mutual sharing of client models. 
FedMatch learns inter-client consistency by mutual sharing of client models. 
% FedMatch focuses on learning inter-client consistency by mutual sharing of client models. 
To mitigate interference between supervised and semi-supervised tasks, FedMatch decomposes model parameters into two variables, which are updated in an alternate way.
% preserving one while updating the other.
}

\item[\textbullet] \bluenote{
\textbf{FedSwitch} \cite{zhao2023does}: 
FedSwitch is the state-of-the-art approach that leverages an EMA model, \ie, a teacher model, to ensure the quality of pseudo-labels. 
Additionally, it adaptively switches between the teacher and student model for pseudo-labeling, both to enhance the quality of pseudo-labels in non-IID settings and reduce communication costs.
}

\item[\textbullet] \bluenote{
\textbf{FedSwitch-SL}: An extension version of FedSwitch that incorporates the technique of Split Learning (SL). It serves as an ablation study to demonstrate the effectiveness of our clustering regularization.
}
\end{itemize}

\textbf{Metrics:} We employ the following metrics to evaluate the performance of different systems.
\begin{itemize}
\item[\textbullet] \textbf{Test accuracy}: In each round, we measure the accuracy of the global model on the test set with different baselines. 
For FedSwitch(-SL) and our system, we use the global teacher model for testing.
\item[\textbullet] \textbf{Time cost}: We record the total time taken to achieve target test accuracy on different systems, which includes the time for computation and communication.
% \item[\textbullet] \textbf{Communication cost}: The communication cost for transmitting models and feature batches (if any) between clients and the PS to achieve the target accuracy is also recorded.
\item[\textbullet] \textbf{Communication cost}: The communication cost for transmitting models and feature batches (if any) to between entities achieve the target accuracy is also recorded.
\end{itemize}

% Figure environment removed

% Figure environment removed

\subsection{Experimental Setup}
We evaluate the performance of our proposed system on an AMAX deep learning workstation equipped with an Intel(R) Xeon(R) Gold 5218R CPU, 8 NVIDIA GeForce RTX 3090 GPUs, and 256 GB RAM. 
We simulate one PS and 30 clients on this workstation. 
To reflect the heterogeneity in the computation capabilities of clients, we assume that the computing time of one local iteration on each simulated client follows a Gaussian distribution. 
The mean and variance of the Gaussian distribution are derived from time records of performing one local iteration on various commercial edge devices, such as NVIDIA Jetson NX and AGX.
% Specifically, we simulate each client's computation time to range from 2$\times$-4$\times$ the actual computation time for performing one-round training on the workstation.
Additionally, for simulating the network dynamics of wireless links between clients and the PS, the outbound bandwidths are implemented to fluctuate between 2Mbps and 8Mbps, and the inbound bandwidths vary between 10Mbps and 20Mbps, as suggested in \cite{xu2022adaptive}.
We use the PyTorch deep learning framework for our software implementation of model training and build up the connections between clients and the PS using the socket library.
The source code is available at \url{https://github.com/littlefishe/Capsule}.

\bluenote{We run 10 trials for all benchmark models and datasets with different random seeds. 
The standard errors are presented in the tables and the figures.}
% by error bars or shadows.
By default, each experiment is run for 1,000 aggregation rounds on SVHN, CIFAR-10, and IMAGE-100 to ensure convergence. 
During each round, $N_h$ = 10 clients are randomly selected to participate in the training process. 
Similar to \cite{diao2021semifl}, we use an SGD optimizer to optimize the models and adopt the cosine learning rate decay schedule \cite{loshchilov2016sgdr}. 
Besides, we set the same hyperparameters as in \cite{diao2021semifl}, where the optimizer momentum $\beta_l = 0.9$, the initial learning rate $\eta = 0.02$, and the confidence threshold $\tau = 0.95$.
As for the adaptation algorithm, we set $\varrho_1 = 0.01$ and $\varrho_2 = 0.05$. 
Unless otherwise specified, the number of labeled data samples on the PS is 1,000, 4,000, and 20,000 for SVHN, CIFAR-10, and IMAGE-100, respectively, and the unlabeled data samples are distributed uniformly across clients.

\begin{table}[t]
\centering
\caption{\bluenote{Overall test accuracy (\%).}}
\label{test_acc_table}
\begin{tblr}{
  row{2} = {c},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{c},
  cell{3}{2} = {c},
  cell{3}{3} = {c},
  cell{3}{4} = {c},
  cell{4}{2} = {c},
  cell{4}{3} = {c},
  cell{4}{4} = {c},
  cell{5}{2} = {c},
  cell{5}{3} = {c},
  cell{5}{4} = {c},
  cell{6}{2} = {c},
  cell{6}{3} = {c},
  cell{6}{4} = {c},
  cell{7}{2} = {c},
  cell{7}{3} = {c},
  cell{7}{4} = {c},
  cell{8}{2} = {c},
  cell{8}{3} = {c},
  cell{8}{4} = {c},
  hline{1,3,9} = {-}{},
  hline{2} = {2-4}{},
}
Baseline            & Dataset       &               &           \\
                    & SVHN          & CIFAR-10      & IMAGE-100 \\
% Supervised-only     & 73.7          & 75.0          & 26.1      \\
% SemiFL              & 88.1 $\pm$ 0.72   & 86.8 $\pm$ 1.23   & 65.4      \\
% FedMatch            & 88.4 $\pm$ 0.56   & 86.1 $\pm$ 1.06   & 65.4      \\
% FedSwitch           & 89.0 $\pm$ 0.46   & 87.9 $\pm$ 1.04   & 60.3      \\
% FedSwitch-SFL       & 89.1 $\pm$ 0.36   & 87.5 $\pm$ 0.79   & 60.3      \\
% Supervised-only     & 73.6(0.24)            & 75.1(0.52)            & 28.5(0.73)     \\
% SemiFL              & 88.1(0.72)            & 86.8(1.23)            & 56.4(2.02)      \\
% FedMatch            & 88.4(0.56)            & 86.1(1.06)            & ?            \\
% FedSwitch           & 89.0(0.46)            & 87.9(1.04)            & 46.9(1.51)      \\
% FedSwitch-SL        & 89.1(0.36)            & 87.5(0.79)            & 47.1(1.43)      \\
% Ours       & \textbf{91.4(0.49)}   & \textbf{88.7(1.13)}   & 61.1(1.79)      
Supervised-only     & 73.6(0.24)            & 75.1(0.52)            & 26.1(0.73)     \\
SemiFL              & 88.1(0.72)            & 86.8(1.23)            & 65.4(2.02)      \\
FedMatch            & 88.4(0.56)            & 86.1(1.06)            & 29.6(1.70)            \\
FedSwitch           & 89.0(0.46)            & 87.9(1.04)            & 60.3(1.51)      \\
FedSwitch-SL        & 89.1(0.36)            & 87.5(0.79)            & 61.1(1.43)      \\
Ours                & \textbf{91.4(0.49)}   & \textbf{88.7(1.13)}   & \textbf{66.5(1.79)}   
\end{tblr}
\end{table}

\subsection{Experiment Results}

\subsubsection{Overall Effectiveness} 
The final test accuracies on SVHN, CIFAR-10, and IMAGE-100 for our system and baselines are presented in Table \ref{test_acc_table}. 
Supervised-only is omitted in the later sections for the comparison of training efficiency since the clients are not involved in this approach.

In terms of accuracy, our proposed system consistently achieves the highest accuracy compared to state-of-the-art approaches. 
\bluenote{
For instance, on the SVHN dataset, our system achieves 91.4\% accuracy, which is 2.3\% higher than that in FedSwitch-SL. 
Similarly, our system achieves 88.7\% accuracy on CIFAR-10, surpassing SemiFL by 1.2\%.
Moreover, on IMAGE-100, our system achieves 61.1\% accuracy, which exhibits a 5.4\% improvement over FedSwitch-SL.
Our system also improves accuracy from 2.6\% to 36.9\% compared to FedMatch across the three datasets, highlighting the effectiveness of our proposed clustering regularization.}

% In addition, the results indicate that FedRGD performs almost the poorest among the baselines. 
% That is because training with pseudo-labels does not always promote model performance, and the knowledge learned under supervision may become corrupted after model aggregation.
% Thus its grouping algorithm helps stabilize the training process in a way of assigning a larger aggregation weight for the model trained under supervision. 
% Although its grouping algorithm stabilizes the training process by assigning greater aggregation weights to models trained under supervision, the test accuracy on IMAGE-100 still indicates its ineffectiveness.
Moreover, the results indicate a naive integration of Semi-FL techniques, such as combining FedSwitch with Split Learning, does not yield substantial improvements in overall accuracy.
\bluenote{
Additionally, FedMatch, which is specifically crafted to address the non-IID issue within the context of CIFAR-10, fall short in achieving superior performance on more complicated tasks, like the IMAGE-100 classification. 
That can be attributed to its model parameter decomposition strategy, which leads to ineffective batch normalization over separate hidden input features.}
It is worth noting that the accuracy gap between our model and the model trained only with labeled data is over 40\% on IMAGE-100. 
This result strongly indicates the importance and necessity of our system in leveraging unlabeled data, and its superiority remains consistent across various models and datasets.

We also present the time cost required to achieve different test accuracies in Fig. \ref{time_cost-fig}. 
\bluenote{FedMatch is excluded from the comparison plots on IMAGE-100 because it fails to achieve the target accuracy.}
When training small-scale models, our system fails to outperform all of the baselines on the SVHN dataset in terms of the time cost, as shown in Fig. \ref{time_cost-fig}(a).
This happens when the cost of transmitting the features outweighs that of the entire model, as evidenced by the high overhead of the FedSwitch-SL.
However, both FedSwitch-SL and our system benefit from scaling the size of the model. 
% In most cases, our system achieves the lowest time cost for reaching the target accuracy.
\bluenote{
For instance, to achieve 80\% accuracy on CIFAR-10, it takes 208.4 min and 217.9 min on average for our system and FedSwitch-SL to train an AlexNet, respectively, and 284.4-1308.0 min for other baselines on FL, which indicates speed-ups of 1.4-6.3$\times$ on the baselines.
Moreover, for a higher target accuracy such as 85\%, it takes 534.9 min for FedSwitch-SL, while other Semi-FL baselines span from 652.3 to 2230.0 min.
In contrast, our system only takes 406.1 min.
}

\begin{table}[t]
\centering
% \caption{Overall comp./sync. time (x100min).}
\caption{\bluenote{Computation/Communication time ($\times$100min).}} 
\label{comp_time_perc_tab}
\begin{tblr}{
  row{2} = {c},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{c},
  cell{3}{2} = {c},
  cell{3}{3} = {c},
  cell{3}{4} = {c},
  cell{4}{2} = {c},
  cell{4}{3} = {c},
  cell{4}{4} = {c},
  cell{5}{2} = {c},
  cell{5}{3} = {c},
  cell{5}{4} = {c},
  cell{6}{2} = {c},
  cell{6}{3} = {c},
  cell{6}{4} = {c},
  cell{7}{2} = {c},
  cell{7}{3} = {c},
  cell{7}{4} = {c},
  hline{1,3,8} = {-}{},
  hline{2} = {2-4}{},
}
Baseline        & Dataset &          &           \\
                & SVHN    & CIFAR-10 & IMAGE-100 \\
% FedRGD          & 84.8    & 27.7     & 32.0      \\
% SemiFL          & \textbf{94.3}    & 29.1     & 28.9      \\
% FedEMA          & 19.3    & 16.7     & 39.8      \\
% Ours            & 66.4    & \textbf{38.2}     & \textbf{56.1}  

SemiFL          & 3.3/0.2       & 5.8/11.9      & 16.2/29.3      \\
FedMatch        & 4.7/0.3       & 10.3/13.7     & 21.7/24.0      \\
FedSwitch       & 3.1/0.3       & 5.0/12.6      & 15.9/36.3      \\
FedSwitch-SFL   & 1.2/2.6       & 3.2/9.1       & 13.7/18.2      \\
Ours            & 2.2/3.1       & 3.0/9.5       & 14.2/19.0 
\end{tblr}
\end{table}

\bluenote{
Additionally, our system speeds up training by 3.0-5.3$\times$ for reaching 60\% test accuracy with VGG16 on the IMAGE-100 dataset compared to FedSwitch-SL and Semi-FL baselines, respectively.}
To investigate the secret of our training efficiency, we record the computing time 
% (includes \srvUp and \locReg)
on two alternate training phases and synchronization time (mainly includes communication) for all baselines in Table \ref{comp_time_perc_tab}. 
The results show that most of the time spent on training large-scale models is not on computation, but on communication, where the advantages of our system lie. 
We reduced much of the time for communication with the help of the SFL framework, but we do not stop there: the clustering regularization promotes the model convergence with little extra cost. 
This results in competitive time cost performance against Semi-FL works, in which performing complete training and model transmission has become a burden for clients.

% Figure environment removed

% Figure environment removed

\subsubsection{Effect of Communication Cost}
We demonstrate the communication efficiency of our proposed system through Fig. \ref{comm_cost-fig}, which tracks the overall network traffic consumption until the target accuracy is reached. 
Our choice of split layers, as elaborated in Section \ref{split-layer-section}, played a pivotal role in achieving this efficiency. 
Our system exhibits the least amount of network traffic consumption in most cases. 
\bluenote{
For example, in Fig. \ref{comm_cost-fig}(b), our system consumes only 319.9GB to achieve 80\% accuracy on CIFAR-10, while FedSwitch consumes 15.2GB higher and Semi-FL baselines consume from 454.1 to 1351.7GB.
This translates to reductions of 4.5\%-78.4\% in communication costs. 
Similarly, Fig. \ref{comm_cost-fig}(c) demonstrates that our system reduces communication cost by 70.3\%-90.8\% compared to all baselines, respectively, when reaching 60\% test accuracy on IMAGE-100. 
The low communication cost of both FedSwitch-SL and our system implies the training efficiency of the SFL framework since the communication time is a significant contributor to the overall training time. 
However, in certain cases, such as training a customized CNN model, the transmission cost of features might outweigh the benefits of SFL.
Fig. \ref{comm_cost-fig}(a) shows that our system consumes approximately 10.7GB to achieve 85\% accuracy is still 27.4\% higher than FedSwitch.}
% which saves 50.0\% of that of FedSwitch-SL, though still 27.4\% higher than FedSwitch.}

\subsubsection{Adaptability to Data Distribution}

\begin{table}[t]
\centering
\caption{\bluenote{Test accuracy (\%) on different datasets ($\alpha=0.1$).}}
\label{noniid_ds_acc_table}
\begin{tblr}{
  row{2} = {c},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{c},
  cell{3}{2} = {c},
  cell{3}{3} = {c},
  cell{3}{4} = {c},
  cell{4}{2} = {c},
  cell{4}{3} = {c},
  cell{4}{4} = {c},
  cell{5}{2} = {c},
  cell{5}{3} = {c},
  cell{5}{4} = {c},
  cell{6}{2} = {c},
  cell{6}{3} = {c},
  cell{6}{4} = {c},
  cell{7}{2} = {c},
  cell{7}{3} = {c},
  cell{7}{4} = {c},
  hline{1,3,8} = {-}{},
  hline{2} = {2-4}{},
}
Baseline        & Dataset               &                       &           \\
                & SVHN                  & CIFAR-10              & IMAGE-100 \\
% SemiFL          & 80.5 $\pm$ 0.66           & 78,9 $\pm$ 0.90           & 50.8      \\
% FedMatch        & 84.1 $\pm$ 0.56           & 79.7 $\pm$ 0.96           & 16.3      \\
% FedSwitch       & 82.9 $\pm$ 0.50           & 79.4 $\pm$ 0.61           & 49.3      \\
% FedSwitch-SL    & 83.0 $\pm$ 0.65            & 79.2 $\pm$ 0.67           & 49.3      \\
% \textbf{Ours}   & \textbf{86.3 $\pm$ 0.68}  & \textbf{83.4 $\pm$ 0.66}  & \textbf{56.6}
% SemiFL          & 80.5(0.66)            & 78.9(0.90)            & 47.8(1.69)      \\
% FedMatch        & 84.1(0.56)            & 79.7(0.96)            & ?      \\
% FedSwitch       & 82.9(0.50)            & 79.4(0.61)            & 34.0(1.83)      \\
% FedSwitch-SL    & 83.0(0.65)            & 79.2(0.67)            & 34.6(1.58)      \\
% Ours   & \textbf{86.3(0.68)}   & \textbf{83.4(0.66)}   & \textbf{54.1(1.33)} 
SemiFL          & 80.5(0.66)            & 78.9(0.90)            & 50.8(1.69)      \\
FedMatch        & 84.1(0.56)            & 79.7(0.96)            & 29.1(1.70)               \\
FedSwitch       & 82.9(0.50)            & 79.4(0.61)            & 48.9(1.83)      \\
FedSwitch-SL    & 83.0(0.65)            & 79.2(0.67)            & 49.3(1.58)      \\
Ours   & \textbf{86.3(0.68)}   & \textbf{83.4(0.66)}   & \textbf{56.6(1.33)} 
\end{tblr}
\end{table}

We compare our system with baselines dealing with data non-IIDness. This is implemented through sampling data from a Dirichlet distribution $Dir(\alpha)$ \cite{hsu2019measuring} for each client. 
The results are presented in Table \ref{noniid_ds_acc_table}, where different categories of data on each client follow a distribution of $Dir(0.1)$. 
\bluenote{
From the results, our method achieves a consistent improvement over SemiFL and FedSwitch-SL on all three datasets. 
Concretely, our system outperforms SemiFL by 5.8\%, 4.5\%, and 5.8\% on the three datasets, respectively. 
Besides, compared to FedSwitch-SL, we improve the test accuracy by 3.3\%, 4.2\%, and 7.3\%, respectively. 
It demonstrates the effectiveness of our design in mitigating data non-IIDness, which is getting more significant on datasets with a wider variety of categories.
}

\begin{table}[t]
\centering
\caption{\bluenote{Test accuracy (\%) of models on CIFAR-10 with different data distributions.}}
\label{noniid_cifar10_acc_table}
\begin{tblr}{
  column{2} = {c},
  column{3} = {c},
  column{4} = {c},
  column{5} = {c},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=4}{c},
  hline{1,3,8} = {-}{},
  hline{2} = {2-5}{},
}
Baseline        & Data distribution     &                       &                       &    \\
                & Dir(1.0)              & Dir(0.5)              & Dir(0.1)              & Dir(0.05) \\
% SemiFL          & 83.1 $\pm$ 1.30       & 78.9 $\pm$ 0.90       & 76.5 $\pm$ 1.05 \\
% FedMatch        & 84.4 $\pm$ 1.10       & 79.7 $\pm$ 0.96       & 75.2 $\pm$ 1.42 \\
% FedSwitch       & 83.8 $\pm$ 1.14       & 79.4 $\pm$ 0.61       & 77.3 $\pm$ 0.99 \\
% FedSwitch-SL    & 84.1 $\pm$ 0.65       & 79.2 $\pm$ 0.67       & 77.2 $\pm$ 1.07 \\
% \textbf{Ours}   & \textbf{86.2 $\pm$ 1.29} & \textbf{83.4 $\pm$ 0.66} & \textbf{82.4 $\pm$ 0.89} 
SemiFL          & 84.9(0.92)            & 83.1(1.30)            & 78.9(0.90)            & 76.5(1.05) \\
FedMatch        & 85.9(1.03)            & 84.4(1.10)            & 79.7(0.96)            & 76.6(1.42) \\
FedSwitch       & 86.2(1.00)            & 83.8(1.14)            & 79.4(0.61)            & 77.3(0.99) \\
FedSwitch-SL    & 86.4(0.98)            & 84.1(0.65)            & 79.2(0.67)            & 77.2(1.07) \\
Ours   & \textbf{87.9(0.95)}   & \textbf{86.2(1.29)}   & \textbf{83.4(0.66)}   & \textbf{82.4(0.89)}
\end{tblr}
\end{table}

% Figure environment removed

% Figure environment removed

% Figure environment removed

Moreover, we investigate the adaptability of our method on different data skewness on CIFAR-10. 
The data distributions among clients in our settings are shown in Fig. \ref{dist_hm-fig} and we present the training process on these distributions in Fig. \ref{data_dist-fig}. 
% Notably, the performance of FedRGD fluctuates dramatically around 100 minutes when its group-wise aggregation is performed, highlighting the detrimental effect of aggregating both global and local models on model convergence.
\bluenote{
Notably, the performance of FedMatch fluctuates dramatically for around 400 minutes since it uses decomposed model parameters for supervised training and semi-supervised training, highlighting the detrimental effect of such disjoint learning on model convergence.}
Table \ref{noniid_cifar10_acc_table} provides a direct comparison of the final performance for different levels of non-IIDness. 
In some extreme cases, FedMatch performs similarly to Supervised-only, suggesting that even with multiple helpers for pseudo-labeling, the model parameters can still be misled by the highly skewed unlabeled dataset. 
A common phenomenon is that the more the data is skewed, the more the model performance deteriorates. 
In contrast, our system consistently achieves the highest accuracy across all levels of data skewness, showcasing its adaptability to diverse data distributions. 
Under extreme non-IID scenarios such as $Dir(0.05)$, the test accuracy of our system improves by 5.1\%-5.9\%, compared to all baselines. 
The experimental results align with our original intention well with the assistance of the clustering of teacher features.
\bluenote{
Notably, the results of SVHN and IMAGE-100 are in good agreement with those observed on CIFAR-10. Due to the limited space, we omit the experimental results here.}
% Considering constraints in space, the results on SVHN and IMAGE-100, which closely align with those observed with CIFAR-10 are not explicitly showcased in our experiments.
% And the stable performance over various data distributions is also attributed to the solid foundation built in the \srvUp stage on strongly-augmented data, which is unfortunately often overlooked in current works.

\subsubsection{Impact of the Scale of Labeled Dataset}

We conduct experiments on CIFAR-10, where we place 4,000 labeled data samples on the PS. 
However, in real-world scenarios, the amount of labeled data on the PS varies. 
To explore the impact of different scales of labeled datasets on our method, we conduct experiments by changing the amount of labeled data on the entire dataset. 
The results are presented in Fig. \ref{nlabeled-fig}. 
\bluenote{
We observe that the test accuracy gradually decreased from 87.9\% to 63.0\% when the amount of labeled data varied from 4,000 to 250 in FedSwitch-SL.}
To investigate the reason for the significant performance decline when the amount of labeled data decreased from 500 to 250, we record the mask rate (the number of examples that are masked out) and data impurity (the error rate of unlabeled data that falls above the threshold) of FedSwitch as an example in Fig. \ref{nlabels_compare-fig}(b). 
We infer that this issue is attributed to the increase in both the mask rate and data impurity, causing the data to become unreliable. 


% % Figure environment removed

By contrast, our system predominates the quality of predictions, as shown in Fig. \ref{nlabels_compare-fig}(a), indicating the advantages of our proposed clustering regularization. 
Our method utilizes data samples with the largest class probability that fall beneath $\tau$, which is considered invalid in consistency regularization. 
Moreover, \srvUp on strong-augmented data provides a solid foundation for semi-supervised learning by enhancing model robustness, thus reducing impurities of unlabeled data. 
In addition, our results suggest that only a few labeled data (less than 2\% of the overall dataset) is enough to satisfy some accuracy requirements, 80\% for example.

\subsubsection{Impact of Split Layer}
\label{split-layer-section}

Our choice of split layers for each model is primarily based on the cost of communication and computation on the client.
Fig. \ref{split_sizes-fig} illustrates the impacts of split layer on communication cost, with $K_u = 50$ and batch size set to 64 on various datasets, except for IMAGE-100, which is set to 32 due to GPU memory constraints.
The communication cost during each round is revealed as the sum of the size of features and the size of the bottom model. It is commonly known that the computation cost (floating-point operations) increases with the number of layers.  
However, this law does not hold for communication costs, as the size of the full model concentrates on the top full-connected layers, but the size of features is much larger in the first few layers than in the later ones. 
In fact, the minimum communication cost may exist in the hidden layer, as shown in Fig. \ref{split_sizes-fig}. 
% Another factor to consider is the possible scaling of $K_u$, which would also scale the size of features. 
For that, we prioritize the split layer with small-size features while ensuring it does not compromise the effectiveness of clustering regularization.
Based on these considerations, the indices of the split layer we select are 2, 5, and 13 for CNN, AlexNet, and VGG16, respectively. 

\subsubsection{Impact of Global Updating Frequency Adaptation}
We evaluate the impact of the \InvReg frequency adaptation algorithm on the performance of our approach. 
We present the comparison in Fig. \ref{opt_alg-fig} between our system with (w/) and without (w/o) the algorithm in terms of accuracy and training loss. 
% Figure environment removed
The result shows that our adaptive adjustment of \InvReg frequency helps the model training reach an equilibrium closer to the global optima.
This is consistent with our expectation in Section \ref{analysis-section}, with a noticeable increase in final accuracy. 
This effect is particularly evident when dealing with smaller amounts of unlabeled data.
% as the average \InvReg frequency adapts to a lower value. 
For instance, with 250 labeled data on the PS, the adaptation strategy improves accuracy by 8.7\%, while the average \InvReg frequency is reduced to 32.5. 
Moreover, as seen in Fig. \ref{opt_alg-fig}(b), adapting the \InvReg frequency brings a greater reduction in training loss, and the effect is especially obvious at several milestones that the \InvReg frequency decays. 
In contrast, with a constant \InvReg frequency (=50), the loss descent is hindered to a large extent because of the negative effect between our two training stages. 
Notably, the increase in the training loss during initial rounds is due to the increase in the amount of confident unlabeled data when the model performance is constantly improved with \srvUp. 
In summary, our adaptation algorithm serves as a good complement to our proposed method.