\subsection{Federated Learning}
% Federated Learning (FL) is a distributed machine learning paradigm that allows training models on decentralized data.
% It has been proposed to address the challenges of privacy and security in centralized machine learning \cite{konevcny2016federated, mcmahan2017communication, kairouz2021advances}. 
Federated Learning (FL) has been proposed to perform privacy-preserving distributed model training \cite{konevcny2016federated, liu2022enhancing, jiang2022fedmp}. 
% address the challenges of privacy and security in centralized machine learning \cite{konevcny2016federated, liu2022enhancing, jiang2022fedmp}. 
In FL, data heterogeneity is a crucial factor for training performance, both in terms of accuracy and efficiency. 
Zhao \etal \cite{zhao2018federated} demonstrate that the issue of accuracy reduction caused by non-IID data can be explained by weight divergence, and they address this issue by sharing a small subset of data between clients for local training, which raises concerns about data privacy.
Li \etal \cite{li2020federated} propose adding a proximal term to the objective to improve convergence stability, but it introduces non-negligible computation burdens on clients.
% , which is adversely affected by statistical heterogeneity. 
Recently, Li \etal \cite{li2021model} address the non-IID issue from a model representation perspective by correcting local updates with the representation of the global model. 
% However, the potential assistance from other clients is often overlooked in the FL literature.
However, typical FL generally perform model training on labeled data, which is impractical in many real-world applications \cite{lin2014microsoft, radford2019language}.


% \subsection{Semi-Supervised Learning}
% Semi-Supervised Learning (SSL) is targeted to leverage both labeled and unlabeled data for training models in a centralized manner.
% Popular SSL methods address model training on unlabeled data through either pseudo-labeling \cite{lee2013pseudo} or consistency regularization \cite{laine2016temporal, berthelot2019mixmatch}.
% Xie et al. \cite{xie2020unsupervised} demonstrate the efficacy of strong data augmentation, while Sohn et al. \cite{sohn2020fixmatch} introduce FixMatch, a method combining pseudo-labeling and consistency regularization for enhanced performance. 
% Notably, Lee et al. \cite{lee2022contrastive} advance the efficiency and accuracy of consistency regularization by leveraging well-clustered features from unlabeled data, serving as a key inspiration for our work.
\subsection{Semi-Supervised Federated Learning}
\bluenote{
To utilize unlabeled data in FL, Semi-supervised Federated Learning (Semi-FL) has gained significant attention.
It shares the objective of FL and extends to the semi-supervised learning setting. 
Some studies assume clients possess both labeled and unlabeled data \cite{lin2021semifed, kim2022federated, wang2022enhancing} for their loss calculation.
To relax the assumption of labeled data on clients, several approaches focus on more common scenarios where labeled data reside on the server.
% Diao \etal \cite{diao2021semifl} present SemiFL that achieves accuracy comparable with standalone training.
% They apply Mixup \cite{berthelot2019mixmatch} technique to augment the local dataset on clients and pioneer the alternate training phases widely adopted in later literature.
Diao \etal \cite{diao2021semifl} present SemiFL, which applies Mixup technique \cite{berthelot2019mixmatch} to augment the local dataset on clients and pioneer the alternate training phases widely adopted in later literature. 
Moreover, SemiFL achieves accuracy comparable with standalone training. 
Jeong \etal \cite{jeong2021federated} introduce FedMatch, which enforces prediction consistency between clients to address non-IID challenges.
To mitigate the high communication cost in sharing client models, Wang \etal \cite{wang2023knowledge} conduct knowledge distillation on the server, which requires a pre-trained model for the process.
Concurrently, Zhao \etal \cite{zhao2023does} achieve state-of-the-art results by adaptively switching between teacher and student models for pseudo-labeling.
Nonetheless, this method relies on an IIDness hyper-parameter for system control, which might be inaccessible for new tasks or datasets.}


\subsection{Split Federated Learning}
\bluenote{
Split Federated Learning (SFL) \cite{gupta2018distributed} focuses on training large-scale models on resource-constrained end devices by incorporating the advantages of federated learning \cite{konevcny2016federated} and split learning. 
For instance, Thapa \etal \cite{thapa2022splitfed} pioneer feasible SFL, which protects model privacy through model splitting and enables client-side model updates through exchanging of features/gradients.
To reduce the traffic consumption, Han \etal \cite{han2021accelerating} introduce an auxiliary network to update client-side models, without requiring gradients from the server.
Then, Oh \etal \cite {oh2022locfedmix} propose LocFedMix-SL, which uploads mixup-augmented features to the server for faster convergence and employs an auxiliary network to regularize the client-side models for better performance.
To address system heterogeneity, Liao \etal \cite{liao2023accelerating} accelerates SFL on resource-constrained and heterogeneous clients by determining diverse batch sizes for different clients and adapting local updating frequency in each aggregation round.
Recently, Han \etal \cite{han2023splitgp} propose SplitGP to ensure both personalization and generalization capabilities by allocating different tasks for all clients.
Despite these notable advancements, none of the existing SFL works have yet explored utilizing features to address the non-IID issue.}
% (known as smashed data in some SFL literature) to address the non-IID problem.}

