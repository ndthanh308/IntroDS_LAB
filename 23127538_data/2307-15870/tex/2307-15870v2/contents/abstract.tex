Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. 
% However, training and deploying large-scale models for broader applications is challenging in resource-constrained environments. 
However, training and deploying large-scale models on resource-constrained clients is challenging. 
Fortunately, Split Federated Learning (SFL) offers a feasible solution by alleviating the computation and/or communication burden on clients.
However, existing SFL works often assume sufficient labeled data on clients, which is usually impractical.
\bluenote{Besides, data non-IIDness across clients poses another challenge to ensure efficient model training.
% Previous FL works have adopted techniques like semi-supervised learning to leverage unlabeled data, but data non-IIDness poses another challenge to ensure training performance. 
To our best knowledge, the above two issues have not been simultaneously addressed in SFL.
Herein, we propose a novel Semi-SFL system, which incorporates clustering regularization to perform SFL under the more practical scenario with unlabeled and non-IID client data.}
% a novel Semi-SFL system with clustering regularization for training models in the label-at-server scenario, effectively improving model performance regardless of data non-IIDness.
Moreover, our theoretical and experimental investigations into model convergence reveal that the inconsistent training processes on labeled and unlabeled data have an influence on the effectiveness of clustering regularization.
To this end, we develop a control algorithm for dynamically adjusting the global updating frequency, so as to mitigate the training inconsistency and improve training performance.
% adaptation, which dynamically adjusts the number of iterations server-side supervised training to mitigate the training inconsistency.
%In doing so, the training inconsistency is effectively mitigated, ultimately prioritizing \textit{Clustering Regularization}.
Extensive experiments on benchmark models and datasets show that our system provides a 3.0$\times$ speed-up in training time and reduces the communication cost by about 70.3\% while reaching the target accuracy, and achieves up to 5.1\% improvement in accuracy under non-IID scenarios compared to the state-of-the-art baselines.