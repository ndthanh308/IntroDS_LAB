In this section, we propose a greedy algorithm for \InvReg frequency adaptation, which is an important component of our system.
We first analyze the convergence bound after $H$ rounds \textit{w.r.t.} both the \InvReg frequency and the \Reg frequency. 
Then we will introduce our algorithm design and explain how it contributes to model convergence.

\subsection{Convergence Analysis}
\label{analysis-section}
Since the training is performed in an alternate manner, the convergence of the loss function relates to the training status in both the \srvUp and \locReg stages. 
For the sake of analysis, we make the following assumptions as suggested in \cite{li2019convergence, yang2021achieving, haddadpour2019convergence, ajalloeian2020convergence}:

\begin{assumption}
\label{assump1}
    (Lipschiz Continuous Gradient) The loss function $F(\cdot)$ and loss components $f_s(\cdot)$ and $f_{u,i}(\cdot)$ of the entire model are L-smooth such that:
    \begin{equation}
    \begin{split}
        & \Vert \nabla F(\boldsymbol{x}) - \nabla F(\boldsymbol{y}) \Vert \leq L \Vert \boldsymbol{x} - \boldsymbol{y} \Vert, \forall \boldsymbol{x}, \forall\boldsymbol{y}\\
        & \Vert \nabla f_s(\boldsymbol{x}) - \nabla f_s(\boldsymbol{y}) \Vert \leq L \Vert \boldsymbol{x} - \boldsymbol{y} \Vert, \forall \boldsymbol{x}, \forall\boldsymbol{y}\\
        & \Vert \nabla f_{u,i}(\boldsymbol{x}) - \nabla f_{u,i}(\boldsymbol{y}) \Vert \leq L \Vert \boldsymbol{x} - \boldsymbol{y}, \Vert, \forall \boldsymbol{x}, \forall\boldsymbol{y} \\
        \nonumber
    \end{split}
    \end{equation}
\end{assumption}

\begin{assumption}
\label{assump2}
    (Bounded Second Moments) There exist constants $G_s$ and $G_u$, such that the second moments of the stochastic gradients of the unsupervised loss and supervised loss on any data sample are upper bounded by:
    \begin{equation}
    \begin{split}
        & \Vert \nabla \ell_{u}(x, \boldsymbol{w}) \Vert^2 \leq G_u^2, \forall x, \forall \boldsymbol{w} \\
        & \Vert \nabla \ell_{s}(x, \boldsymbol{w}) \Vert^2 \leq G_s^2, \forall x, \forall \boldsymbol{w} \\  
        \nonumber
    \end{split}
    \end{equation}
\end{assumption}

\begin{assumption}
\label{assump3}
    (Unbiased Gradient Estimator) Let $\nabla f_s(\boldsymbol{w})$ denote the gradients derived from the labeled data on the PS and $\nabla f_{u,i}(\boldsymbol{w}_i)$ denote the gradients derived from the unlabeled data of client $i$, the gradient estimators are unbiased as:
    \begin{equation}
    \begin{split}
        & \mathbb{E} \Vert \tilde{\nabla} f_{s}(\boldsymbol{w}) \Vert = \mathbb{E} \Vert \nabla f_{s}(\boldsymbol{w}) \Vert, \forall \boldsymbol{w}\\ 
        & \mathbb{E} \Vert \tilde{\nabla} f_{u, i}(\boldsymbol{w}) \Vert = \mathbb{E} \Vert \nabla f_{u, i}(\boldsymbol{w}) \Vert, \forall \boldsymbol{w}
        \nonumber    
    \end{split}
    \end{equation}
\end{assumption}

Based on those assumptions, we have the following results given an initialized model $\boldsymbol{w}^0$ (\bluenote{complete proof is presented in the Appendix}):
% Based on those assumptions, we have the following results given an initialized model $\boldsymbol{w}^0$ (\bluenote{the detailed proof is presented in supplementary materials}):

\begin{theorem} 
\label{theorem1}
The sequence of outputs $\{\boldsymbol{w}^{h, k}\}$ generated by supervised training and global aggregation satisfies:
\begin{equation}
\begin{split}
    & \underset{h \in [H], k \in [K_s + 1]}{\min} {\mathbb{E} \Vert \nabla F(\boldsymbol{w}^{h, k}) \Vert^2} 
    \leq \frac{2(F(\boldsymbol{w}^0) - F(\boldsymbol{w}^*))}{\eta HK_s} 
    +\Phi
    \nonumber
\end{split}   
\end{equation}
where $\Phi \triangleq (\frac{L^2(K_u-1)(2K_u-1)\eta^2}{3K_s} + \frac{LK_u^2 \eta^3}{K_s} + 1) G_u^2 + (L \eta + \frac{2 K_u}{K_s}) G_s^2$.
\end{theorem}

Theorem \ref{theorem1} suggests that the convergence of model training in an alternating manner is dependent on $G_s^2$ and $G_u^2$. 
In fact, when $\eta$ approaches 0, consecutive training on either \srvUp or \locReg stage will lead to either $G_s^2$ or $G_u^2$ approaches 0.
It implies that convergence can be ensured when the updating frequency on the other stage approaches 0. 
However, due to the alternate training, $G_s^2$ and $G_u^2$ might affect each other, resulting in fluctuations in $\Phi$ that hinder the convergence.

To provide evidence for this, an experiment is conducted on AlexNet, which is pre-trained on 4,000 labeled data samples from the CIFAR-10 dataset for 5,000 iterations.
Initially, the model is trained over the unlabeled dataset using the pseudo labels predicted by the teacher model for 20,000 iterations.
Then, the training is resumed over the labeled dataset for another 20,000 iterations. 
The variations of both $f_s$ and $f_u$ are recorded in Fig. \ref{loss_adv-fig}. 
The results demonstrate that minimizing one of the losses ($f_s$ or $f_u$) has negative effects on the other.
The convergence can only be achieved if the positive effect always outweighs the negative effect. 
More generally, an equilibrium will be attained after sufficient training rounds if the effects of the two stages cancel each other out, and both the \InvReg frequency $K_s$ and the \Reg frequency $K_u$ are the key factors that influence the equilibrium.

% Figure environment removed


% Figure environment removed

Next, we delve into a more detailed analysis based on the experimental observations.
From Theorem \ref{theorem1}, determining an optimal $K_u$ to minimize the expected gradients is non-trivial. 
To simplify the problem, considering a fixed $K_u$, a larger \InvReg frequency is preferred to reach equilibrium faster (the expected gradients move faster towards $G_u^2$).
% On the contrary, a too small \InvReg frequency will ignore the importance of \srvUp and be trapped in a loop of long training and aggregations to an acceptable model, followed by being misled by sporadic and potentially erroneous labels, and starting from scratch. 
However, as observed in Fig. \ref{loss_adv-fig}, \locReg has a smaller impact on the supervised loss compared to vice versa. 
Consequently, a lower \InvReg frequency might contribute more to convergence ($G_u^2$ closer to 0).
To observe the impact of \InvReg frequency, we conduct experiments by training AlexNets on the CIFAR-10 dataset using our proposed system with 10 clients.
We set $K_u=50$ and vary $K_s$.
We record the aggregation rounds and test accuracy until the loss averaged over the last 10 rounds is reduced to less than a certain scalar (0.01), indicating reaching equilibrium. 
Fig. \ref{sup_freq-fig} shows that the model trained with a relatively smaller \InvReg frequency yields higher model accuracy, at the expense of more aggregation rounds, \ie, more resource consumption. 
As such, we face a dilemma in determining the \InvReg frequency.

\subsection{Global Updating Frequency Adaptation}
In the previous section, it is highlighted that obtaining a constant optimal \InvReg frequency in advance during training rounds is not feasible. 
To address this challenge, we present an algorithm for \InvReg frequency adaptation. 
Specifically, in each round, we estimate the supervised loss $\ell_s^{h+1}$ for the next round with a moving average. 
The PS then calculates the difference between two 10-round estimated loss averages as $\Delta \ell_s$. 
We enable the system to adjust the \InvReg frequency on several training milestones $\mathcal{M}$ as:
\begin{equation}
K_{s}^{h+1}=
    \begin{cases}
        \frac{K_{s}^{h}}{2} & \text{if } | \Delta \ell_s^h | < \varrho_1 \text{ and } h \in \mathcal{M} \\
        2 K_{s}^{h} & \text{if } | \Delta \ell_s^h | > \varrho_2 \text{ and } h \in \mathcal{M} \\
        K_{s}^{h} & \text{otherwise}\\
    \end{cases}
\label{up-freq_decay_eq}
\end{equation}
where $0 < \varrho_1 < \varrho_2$ are priors. 

Next, we will delve into our algorithm design.
Initially, the model parameters are randomly initialized.
At this stage, the model barely generates confident pseudo labels and mainly focuses on supervised training, resulting in a rapid reduction of the loss, \ie, a high value of $|\Delta \ell_s^h|$. 
Since the few pseudo labels at the beginning may be biased, the model requires more attention on supervised training to update towards the correct goal. 
Therefore, we assign our system a larger \InvReg frequency to prioritize supervised training.
As training progresses and the loss over the labeled data stabilizes, it signifies that sufficient attention has been given to the labeled data. 
Consequently, we instruct a lower \InvReg frequency to avoid hindering the \locReg stage.
This ensures that the positive effects consistently outweigh the negative effects during the alternate training phases, enabling the model to achieve optimal performance.
Moreover, our approach prevents over-reduction of the \InvReg frequency, which could otherwise increase confirmation bias \cite{arazo2020pseudo}, by using the high value of $|\Delta \ell_s^h|$ as a signal to adjust the \InvReg frequency.
% We have also accounted for cases where the task is particularly difficult, resulting in a large $\Delta\ell_s^h$, by keeping the \InvReg frequency unchanged or even increasing it. 
The results presented in the subsequent sections will demonstrate the significance of our adaptation algorithm.

\begin{algorithm}[htbp]
\caption{Training process on the PS}
\begin{algorithmic}[1]
    \STATE Initialize global model $\boldsymbol{w} = (\boldsymbol{w}_c, \boldsymbol{w}_s, \boldsymbol{w}_p)$, average model, $\tilde{\boldsymbol{w}} = \boldsymbol{w}$, $K_s^1 = K_s$
    \FOR{$h=1$ to $H$}
        % \STATE \textbf{Processing at the Parameter Server}
        \FOR{$k=1$ to $K_s^h$}
            \STATE Update $\boldsymbol{w}^{h, k+1} = \boldsymbol{w}^{h, k} - \eta_h\tilde{\nabla} f_s(\boldsymbol{w}^{h, k})$ 
            \STATE Update $\tilde{\boldsymbol{w}}^{h, k+1} = \gamma \tilde{\boldsymbol{w}}^{h, k} + (1-\gamma) \boldsymbol{w}^{h , k+1}$
        \ENDFOR
        \STATE Send $\boldsymbol{w}_c^{h + \frac{1}{2}}$ and $\tilde{\boldsymbol{w}}_c^{h + \frac{1}{2}}$ to all workers
        and set $\boldsymbol{w}_s^{h  + \frac{1}{2}, 1} = \boldsymbol{w}_s^{h + \frac{1}{2}}$, $\boldsymbol{w}_p^{h + \frac{1}{2}, 1} = \boldsymbol{w}_p^{h + \frac{1}{2}}$
        \FOR{$k=1$ to $K_u$}
            \FOR{$i$ in $V$}
                \STATE Get ($\boldsymbol{e}_i$, $\tilde{\boldsymbol{e}}_i$) from client $i$
                \STATE Forward propagation with $\boldsymbol{e}_i$ and $\tilde{\boldsymbol{e}}_i$ on ($\boldsymbol{w}_s^{h + \frac{1}{2},k}$, $\boldsymbol{w}_p^{h,k}$) and  ($\tilde{\boldsymbol{w}}_s^{h + \frac{1}{2},k}$,$\tilde{\boldsymbol{w}}_p^{h + \frac{1}{2},k}$)
                \STATE Backward propagation, calculate $\tilde{\nabla}_s f_{u,i}(\boldsymbol{w}_s^{h + \frac{1}{2},k})$ and $\tilde{\nabla}_p f_{u,i}(\boldsymbol{w}_p^{h + \frac{1}{2},k})$
                \STATE Send the gradients of $\boldsymbol{e}_i$ to client $i$
            \ENDFOR
            \STATE Update top models as Eq. (\ref{server_split_update_eq})
        \ENDFOR
        \FOR{$i$ in $V$}
            \STATE Get $\boldsymbol{w}_{c,i}^{h + \frac{1}{2}, K_u + 1}$ from client $i$
        \ENDFOR
        \STATE Set $\boldsymbol{w}_c^{h+1} = \frac{1}{N}\sum_{i \in [N]}\boldsymbol{w}_{c,i}^{h + \frac{1}{2}, K_u + 1}$
        \STATE Estimate $\ell_s^{h+1}$ and Update $\Delta \ell_s^{h}$
        \STATE Update $K_s^{h+1}$ as Eq. (\ref{up-freq_decay_eq})
    \ENDFOR
\end{algorithmic}
\label{process-alg}
\end{algorithm}

We present the overall training process on the PS, including the adaptation algorithm, in Alg. \ref{process-alg}. 
The training starts with supervised training, and the teacher model is updated batch-wise at the PS in each round for $K_s^h$ iterations (Lines 4-5 in Alg. \ref{process-alg}). 
The PS distributes the global bottom model and teacher bottom model to each client (Line 7 in Alg. \ref{process-alg}). 
The cross-entity semi-supervised training on PS is performed for each client in turn using the top model.
Then, the calculated gradients for student features are sent to the corresponding clients (Lines 10-13 in Alg. \ref{process-alg}). 
After $K_u$ steps of \locReg, the bottom models are aggregated (Lines 19-22 in Alg. \ref{process-alg}). 
Finally, the PS updates $K_s$ based on estimating the loss variation at the end of each round (Lines 23-24 in Alg. \ref{process-alg}) and starts the next round. 
The client training process is identical to what we have described in Section \ref{system-design}, which includes conducting local forward propagation, uploading features, and backward propagation based on the downloaded gradients subsequently.