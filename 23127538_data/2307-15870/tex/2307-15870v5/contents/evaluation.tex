In this section, we begin by providing a list of the datasets and models, as well as a description of the devices utilized in the experiments. 
We then introduce the adopted baselines and metrics for performance comparison. 
Finally, we present our evaluation results and analyze the superiority of \method under various scenarios.

\vspace{-0.1cm}
\subsection{Datasets and Models}
\textbf{Datasets:} We conduct experiments on four commonly used real-world datasets for semi-supervised learning: SVHN \cite{netzer2011reading}, CIFAR-10 \cite{krizhevsky2009learning}, STL-10 \cite{coates2011analysis} and IMAGE-100. 
The SVHN dataset contains 73,257 digits for training and 26,032 digits for testing, which are labeled in 10 classes. 
As suggested in \cite{diao2022semifl}, 1,000 labeled digits in the training set are allocated for the PS, while the remaining training data are distributed to clients as unlabeled data.
The CIFAR-10 dataset is an image dataset consisting of 60,000 32$\times$32 color images (50,000 for training and 10,000 for testing) in 10 categories, and 4,000 images are set as labeled data on the PS. 
The STL-10 dataset contains images from 10 different classes.
It includes 5,000 labeled images for training and 8,000 images for testing, with each image of size 96$\times$96. An additional 100,000 unlabeled images are provided for unsupervised learning, making this dataset particularly suitable for semi-supervised learning scenarios.
To evaluate \method on a more challenging task, we create the IMAGE-100 dataset, which is a subset of ImageNet \cite{russakovsky2015imagenet} and contains 100 out of 1,000 categories. 
Each sample in IMAGE-100 is resized to the shape of 144$\times$144$\times$3, and 5,000 labeled images are allocated for the PS.

\textbf{Models:} Four models with different types and structures are adopted on the above three real-world datasets for performance evaluation: (i) CNN on SVHN, (ii) AlexNet \cite{krizhevsky2017imagenet} on CIFAR-10, (iii) VGG13 \cite{simonyan2014very} on STL-10, (iv) VGG16 \cite{simonyan2014very} on IMAGE-100. 
For SVHN, we train a customized CNN model, which has two 5$\times$5 convolutional layers, a fully-connected layer with 512 units, and a softmax output layer with 10 units. 
% with size of 3.35MB
For CIFAR-10, we train the AlexNet model with size of 127MB, composed of three 3$\times$3 convolutional layers, one 7$\times$7 convolutional layer, one 11$\times$11 convolutional layer, two fully-connected hidden layers, and one softmax output layer. 
For STL-10, the VGG-13 model with size of 508MB is adopted, which features 10 convolutional layers with 3$\times$3 kernel sizes, two fully-connected layers, and a softmax output layer for classifying images into 10 categories.
Lastly, the VGG16 model with 0.13B parameters and size of 528MB, which consists of 13 convolutional layers with kernel size of 3$\times$3, two fully-connected layers and a softmax output layer, is trained for image classification of IMAGE-100.
% \bluenote{Finally, the LeViT model, which consists of 4 convolutional layers with kernel size of 3$\times$3, 3 stages with 384, 512, 768 channels, each containing 4 pairs of Attention and MLP blocks, and two fully-connected layers and a softmax output layer, is trained on IMAGE-100.}

% Figure environment removed

% \vspace{-0.4cm}
\subsection{Baselines and Metrics}
\textbf{Baselines:}  We measure the effectiveness of \method through a comparison with five baselines.
\begin{itemize}
\item[\textbullet] \textbf{Supervised-only}: 
Supervised-only refers to using only the labeled dataset available on the PS for supervised training. 
This represents the lower bound that can be achieved with a limited amount of labeled data.
% \item[\textbullet] \textbf{FedRGD} \cite{zhang2021improving}: Given labeled data on the PS and unlabeled data on clients, FedRGD performs supervised and semi-supervised training over the labeled and unlabeled data in parallel. 
% To aggregate models, FedRGD divides the clients into groups randomly and performs group-wise averaging (with the latest global model from the PS), and generates the group models for data labeling and further training.
% To ensure model convergence, the initial global model is trained with the labeled data on the PS for the first 50 rounds.

% \item[\textbullet] \textbf{FedEMA}: It is a simplified version of our proposed system without the operation of incorporating clustering regularization. 
% FedEMA serves as an ablation study to demonstrate the effectiveness of our clustering regularization under various scenarios.

\item[\textbullet] \textbf{SemiFL} \cite{diao2022semifl}: 
SemiFL is the first approach for semi-supervised FL with accuracy on par with standalone training, in which pseudo-labels for the local data of each client are generated upon the latest global model.
% At the beginning of each communication round, 
% In SemiFL, pseudo-labels for the local data of each client are generated upon the latest global model.
In a certain iteration, each client applies the Mixup \cite{berthelot2019mixmatch} technique on each data batch to augment the data and then perform forward and backward propagation using a specialized ``mix'' loss.

\item[\textbullet] 
\textbf{FedMatch} \cite{jeong2021federated}: 
% To address Semi-FL problems across clients with both IID and non-IID data, FedMatch focuses on learning inter-client consistency by mutual sharing of client models. 
% FedMatch focuses on learning inter-client consistency by mutual sharing of client models. 
FedMatch learns inter-client consistency by mutual sharing of client models. 
To mitigate interference between supervised and semi-supervised tasks, FedMatch decomposes model parameters into two variables, which are updated in an alternate way.

\item[\textbullet] 
\textbf{FedSwitch} \cite{zhao2023does}: 
FedSwitch is the state-of-the-art approach that leverages an EMA model, \ie, a teacher model, to ensure the quality of pseudo-labels. 
Additionally, it adaptively switches between the teacher and student model for pseudo-labeling, both to enhance the quality of pseudo-labels in non-IID settings and reduce communication costs.


\item[\textbullet]
\textbf{FedSwitch-SL}: An extension version of FedSwitch that incorporates the technique of Split Learning (SL). It serves as an ablation study to demonstrate the effectiveness of our clustering regularization.

\end{itemize}

\textbf{Metrics:} We employ the following metrics to evaluate the performance of different approaches.
\begin{itemize}
\item[\textbullet] \textbf{Test accuracy}: In each round, we measure the accuracy of the global model on the test set with different baselines. 
For FedSwitch(-SL) and \method, we use the global teacher model for testing.
\item[\textbullet] \textbf{Time cost}: We record the total time taken to achieve target test accuracy on different approaches, which includes the time for computation and communication.
% \item[\textbullet] \textbf{Communication cost}: The communication cost for transmitting models and feature batches (if any) between clients and the PS to achieve the target accuracy is also recorded.
\item[\textbullet] \textbf{Communication cost}: The communication cost for transmitting models and feature batches (if any) between entities to achieve the target accuracy is also recorded.
\end{itemize}

\subsection{Experimental Setup}
% To reflect the heterogeneity in the computation capabilities of clients, we assume that the computing time of one local iteration on each simulated client follows a Gaussian distribution. 
% The mean and variance of the Gaussian distribution are derived from time records of performing one local iteration on various commercial edge devices, such as NVIDIA Jetson NX and AGX.
% We simulate one PS and 30 clients on this workstation. 
% The computation time for each simulated client is generated by applying relative computation times compared to the performance metrics of the workstation.
% To represent the heterogeneity among clients, we derive speed-up ratios from the performance characteristics of the NVIDIA Jetson AGX Xavier\footnote{For more details about NVIDIA Jetson AGX Xavier, visit \url{https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-agx-xavier/}}. 
% These characteristics are measured under various operating modes, with different configurations of active CPUs and varying CPU/GPU frequencies \cite{liao2023accelerating, jiang2023computation, xu2024overcoming}. 
% \bluenote{
We evaluate the performance of \method on a hardware prototype system, which consists of an AMAX deep learning workstation equipped with an Intel(R) Xeon(R) Platinum 8358P CPU, 8 NVIDIA GeForce RTX A6000 GPUs, and 512 GB RAM, along with 80 Nvidia Jetson devices.
% \footnote{For more details about NVIDIA Jetson AGX Xavier, visit \url{https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-agx-xavier/}}. 
To represent the heterogeneity among clients, the NVIDIA Jetson devices operate under different modes, with various configurations of active CPUs and varying CPU/GPU frequencies \cite{liao2023accelerating, xu2024overcoming}.
% Detailed technical specifications of the Jetson AGX Xavier are provided in Table \ref{tab-device}.
Note that the algorithms and theoretical analyses presented in this paper are applicable to a variety of edge devices with diverse computing and communication capabilities, beyond the specific devices mentioned.
% }

% \begin{table}[t]
% \centering
% \caption{Different computing modes for Jetson AGX.\label{tab-device}}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Mode ID} & \textbf{CPU Frequency} & \textbf{GPU Type} & \textbf{Memory} \\
% \midrule
% Mode 0 & 2.3 GHz & 1.4 GHz & 2.1 GHz \\
% Mode 2 & 1.2 GHz & 0.7 GHz & 1.3 GHz \\
% Mode 5 & 1.8 GHz & 0.9 GHz & 1.6 GHz \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[ht]
% \centering
% \caption{Relative computation times of different computing modes compared to the workstation.\label{tab-device}}
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Device} & \textbf{CNN} & \textbf{AlexNet} & \textbf{VGG13} & \textbf{VGG16} \\
% \midrule
% Workstation         & 1.0$\times$ & 1.0$\times$ & 1.0$\times$ & 1.0$\times$ \\
% Jetson AGX (Mode 0) & 3.2$\times$ & 7.9$\times$ & 10.4$\times$ & 10.7$\times$ \\
% Jetson AGX (Mode 2) & 7.5$\times$ & 14.8$\times$ & 18.8$\times$ & 19.4$\times$ \\
% Jetson AGX (Mode 5) & 5.1$\times$ & 11.0$\times$ & 14.3$\times$ & 14.8$\times$ \\
% \bottomrule
% \end{tabular}
% \end{table}

% Additionally, for simulating the network dynamics of wireless links between clients and the PS, the outbound bandwidths are implemented to fluctuate between 0.8Mbps and 8Mbps, and the inbound bandwidths vary between 10Mbps and 20Mbps, as suggested in \cite{liao2023accelerating}.

% \bluenote{
Additionally, the clients and the PS are connected via wireless links. 
Due to random channel noise and varying distances between devices and the router, the outbound bandwidth fluctuates between 0.8 Mbps and 8 Mbps, and the inbound bandwidth varies between 10 Mbps and 20 Mbps.
We use the PyTorch deep learning framework for our software implementation of model training and build up the connections between clients and the PS using the socket library.
The source code is available at \url{https://github.com/littlefishe/Capsule}.
% }

We run 10 trials for all benchmark models and datasets with different random seeds. 
The standard errors are presented in the tables and the figures.
% by error bars or shadows.
By default, each experiment is run for 1,000 aggregation rounds on SVHN, CIFAR-10, STL-10, and IMAGE-100 to ensure convergence. 
% During each round, $N_h$ = 30 clients are randomly selected to participate in the training process.
Similar to \cite{diao2022semifl}, we use an SGD optimizer to optimize the models and adopt the cosine learning rate decay schedule \cite{loshchilov2016sgdr}. 
Besides, we set the same hyperparameters as in \cite{diao2022semifl}, where the optimizer momentum $\beta_l = 0.9$, the initial learning rate $\eta = 0.02$, and the confidence threshold $\tau = 0.95$.
As for the adaptation algorithm, we set $\alpha = 1.5$ and $\beta = 8$. 
For FedSwitch-SL and \method, the indices of the split layer we select are 2, 5, 10, and 13 for CNN, AlexNet, VGG13, and VGG16, respectively. 
Unless otherwise specified, the number of labeled data samples on the PS is 1,000, 4,000, 5,000, and 5,000 for SVHN, CIFAR-10, STL-10, and IMAGE-100, respectively, and the unlabeled data samples are distributed uniformly across clients.

% % Figure environment removed


% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{table}[t]
\caption{Overall test accuracy (\%).}
\label{tab-ovacc}
\renewcommand{\arraystretch}{1.3} % Adjust the row height
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Baselines}} & \multicolumn{4}{c}{Dataset}                \\ \cline{2-5} 
\multicolumn{1}{c}{}                           & SVHN      & CIFAR-10  & STL-10 & IMAGE-100 \\
\midrule
Supervised-only                                & 73.6(0.2) & 75.1(0.5) & 81.9(0.2) & 26.1(0.7) \\
SemiFL                                         & 88.1(0.7) & 86.8(1.2) & 90.7(2.5) & 65.4(2.0) \\
FedMatch                                       & 88.4(0.6) & 86.1(1.1) & 77.5(1.1) & 29.6(1.7) \\
FedSwitch                                      & 89.0(0.5) & 87.9(1.0) & 91.5(0.3)    & 60.3(1.5) \\
FedSwitch-SL                                   & 89.1(0.4) & 87.5(0.8) & 90.1(0.4)    & 61.1(1.4) \\
\method                                          & \textbf{91.4(0.4)} & \textbf{88.6(0.2)} & \textbf{92.8(0.3)} & \textbf{66.6(0.3)} \\
\bottomrule
\end{tabular}
\vspace{-0.4cm}
\end{table}


\subsection{Experiment Results}

% Figure environment removed

% Figure environment removed

% \subsubsection{Impact of Split Layer}
% \label{split-layer-section}

% Our choice of split layers for each model is primarily based on the cost of communication and computation on the client.
% Fig. \ref{fig-split} illustrates the impacts of split layer on communication cost, with $K_u = 50$ and a batch size of 64 for SVHN and CIFAR-10. 
% For STL-10 and IMAGE-100, the batch size is set to 32 due to GPU memory constraints.
% The communication cost during each round is revealed as the sum of the size of features and the size of the bottom model. 
% It is commonly known that the computation cost (floating-point operations) increases with the number of layers.  
% However, this law does not hold for communication costs, as the size of the full model concentrates on the top full-connected layers, but the size of features is much larger in the first few layers than in the later ones. 
% In fact, the minimum communication cost may exist in the hidden layer, as shown in Fig. \ref{fig-split}. 
% For that, we prioritize the split layer with small-size features while ensuring it does not compromise the effectiveness of clustering regularization.
% Based on these considerations, the indices of the split layer we select are 2, 5, 10, and 13 for CNN, AlexNet, VGG13, and VGG16, respectively. 

\subsubsection{Overall Effectiveness} 
The final test accuracies on SVHN, CIFAR-10, STL-10, and IMAGE-100 for \method and baselines are presented in Table \ref{tab-ovacc}. 
Supervised-only is omitted in the later sections for the comparison of training efficiency since the clients are not involved in this approach.
In terms of accuracy, \method consistently achieves the highest accuracy compared to state-of-the-art approaches. 
For instance, on the SVHN dataset, \method can achieve 91.4\% accuracy, which is 2.3\% higher than that in FedSwitch-SL. 
Similarly, \method achieves 88.6\% and 92.8\% accuracy on CIFAR-10 and STL-10, respectively, surpassing SemiFL by 1.8\% and 2.7\%.
Moreover, on IMAGE-100, \method can achieve 66.6\% accuracy, which exhibits a 5.5\% improvement over FedSwitch-SL.
\method also improves accuracy from 2.5\% to 37.0\% compared to FedMatch across the four datasets, highlighting the effectiveness of the clustering regularization.

% Moreover, the results indicate a naive integration of Semi-FL techniques, such as combining FedSwitch with Split Learning, does not yield substantial improvements in overall accuracy.
% Additionally, FedMatch, which is specifically crafted to address the non-IID issue within the context of CIFAR-10, falls short in achieving superior performance on tasks like STL-10 and IMAGE-100 classification. 
% That can be attributed to its model parameter decomposition strategy, which leads to ineffective batch normalization over separate hidden input features.
% It is worth noting that the accuracy gap between our model and the model trained only with labeled data is over 40\% on IMAGE-100. 
% This result strongly indicates the importance and necessity of our system in leveraging unlabeled data, and its superiority remains consistent across various models and datasets.

We also present the time cost required to achieve different test accuracies in Fig. \ref{fig-timecost}. 
FedMatch is excluded from the comparison plots on IMAGE-100 and from some bars in the STL-10 plots because it fails to achieve the target accuracy.
When training small-scale models, \method fails to outperform all of the baselines on the SVHN dataset in terms of the time cost, as shown in Fig. \ref{fig-timecost}(a).
This happens when the cost of transmitting the features outweighs that of the entire model, as evidenced by the high overhead of the FedSwitch-SL.
However, both FedSwitch-SL and \method benefit from scaling the size of the model. 
% In most cases, \method achieves the lowest time cost for reaching the target accuracy.
For instance, to achieve 80\% accuracy on CIFAR-10, \method requires 86.5 min on average to train an AlexNet, compared to 113.6 min for FedSwitch-SL and 279.9-702.4 min for other FL baselines, indicating speed-ups of 1.3-8.1$\times$ over the baselines.
For a higher target accuracy of 85\%, \method only takes 322.8 min, while FedSwitch-SL requires 330.9 min, and other Semi-FL baselines span from 591.9 to 1211.9 min. 
% In contrast, \method only takes 322.8 min.
Additionally, \method speeds up training by 3.6-19.5$\times$ to reach 85\% accuracy with VGG13 on STL-10 and by 3.8-7.0$\times$ to reach 60\% accuracy with VGG16 on IMAGE-100 compared to FedSwitch-SL and other Semi-FL baselines.


% Figure environment removed

\subsubsection{Effect of Communication Cost}
We demonstrate the communication efficiency of \method through Fig. \ref{fig-commcost}, which tracks the overall network traffic consumption until the target accuracy is reached. 
% Our choice of split layers, as elaborated in Section \ref{split-layer-section}, played a pivotal role in achieving this efficiency. 
% Our system exhibits the least amount of network traffic consumption in most cases. 
Our choice of split layers played a pivotal role in achieving this efficiency. 
For example, in Fig. \ref{fig-commcost}(b), \method consumes only 181.95GB to achieve 80\% accuracy on CIFAR-10, while FedSwitch-FL consumes 59.7GB higher and FL baselines consume from 441.9 to 560.5GB.
This translates to reductions of 24.7\%-83.4\% in communication costs. 
Similarly, Fig. \ref{fig-commcost}(c) shows a 72.9\%-92.9\% reduction in communication costs for \method to reach 85\% accuracy on STL-10, and Fig. \ref{fig-commcost}(d) shows a 73.6\%-83.5\% reduction to reach 60\% accuracy on IMAGE-100.
The low communication cost of both FedSwitch-SL and \method implies the training efficiency of the SFL framework since the communication time is a significant contributor to the overall training time.
However, in certain cases, such as training a customized CNN model, the transmission cost of features might outweigh the benefits of SFL.
Fig. \ref{fig-commcost}(a) shows that \method consumes approximately 74.4GB to achieve 85\% accuracy, which is 6.3 times higher than FedSwitch.
% which saves 50.0\% of that of FedSwitch-SL, though still 27.4\% higher than FedSwitch.}

\subsubsection{Adaptability to Data Distribution\label{sec-exp-dir}}

\begin{table}[t]
\centering
\vspace{0.2cm}
\caption{Test accuracy (\%) on different datasets ($\alpha=0.1$).}
\label{tab-noniid}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Baselines}} & \multicolumn{3}{c}{Dataset}                \\ \cline{2-4} 
\multicolumn{1}{c}{}                           & SVHN      & CIFAR-10           & IMAGE-100 \\ \midrule
SemiFL                                         & 80.5(0.7) & 78.9(0.9)          & 50.8(1.7) \\
FedMatch                                       & 84.1(0.6) & 79.7(1.0)          & 29.1(1.7) \\
FedSwitch                                      & 82.9(0.5) & 79.4(0.6)          & 48.9(1.8) \\
FedSwitch-SL                                   & 83.0(0.7) & 79.2(0.7)          & 49.3(1.6) \\
\method                                          & \textbf{87.2(0.2)} & \textbf{83.8(0.2)} & \textbf{56.1(0.4)}  \\ \bottomrule
\end{tabular}
\vspace{-0.2cm}
\end{table}

\begin{table}[t]
\centering
\caption{Test accuracy (\%) of models on CIFAR-10 with different data distributions.}
\label{tab-cifarnoniid}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Baselines}} & \multicolumn{4}{c}{Data Distribution}         \\ \cline{2-5} 
\multicolumn{1}{c}{}                           & Dir(1.0)  & Dir(0.5)  & Dir(0.1)  & Dir(0.05) \\ \midrule
SemiFL                                         & 84.9(0.9) & 83.1(1.3) & 78.9(0.9) & 76.5(1.1) \\
FedMatch                                       & 85.9(1.0) & 84.4(1.1) & 79.7(1.0) & 76.6(1.4) \\
FedSwitch                                      & 86.2(1.0) & 83.8(1.1) & 79.4(0.6) & 77.3(1.0) \\
FedSwitch-SL                                   & 86.4(1.0) & 84.1(0.7) & 79.2(0.7) & 77.2(1.1) \\
\method                                          & \textbf{87.9(0.2)} & \textbf{86.8(0.2)} & \textbf{83.8(0.2)} & \textbf{82.3(0.1)} \\ \bottomrule
\end{tabular}
\vspace{-0.3cm}
\end{table}

We compare \method with baselines in dealing with data non-IIDness. This is implemented through sampling data from a Dirichlet distribution $Dir(\alpha)$ \cite{hsu2019measuring} for each client.
STL-10 is excluded from this experiment because the necessary ground truth label information for sampling from certain distributions is not available for unlabeled data.
The results are presented in Table \ref{tab-noniid}, where different categories of data on each client follow a distribution of $Dir(0.1)$. 
From the results, \method achieves a consistent improvement over baselines on all three datasets. 
Concretely, \method outperforms FedSwitch-SL by 4.2\%, 4.6\%, and 6.8\% on the three datasets, respectively. 
% Besides, compared to FedSwitch-SL, we improve the test accuracy by 3.3\%, 4.2\%, and 7.3\%, respectively. 
These results demonstrate the effectiveness of \method in mitigating data non-IIDness, which is getting more significant on datasets with a wider variety of categories.

Moreover, we investigate the adaptability of \method on different data skewness on CIFAR-10. 
The data distributions among clients in our settings are shown in Fig. \ref{fig-hm} and we present the training process on these distributions in Fig. \ref{fig-noniid}. 
% Notably, the performance of FedRGD fluctuates dramatically around 100 minutes when its group-wise aggregation is performed, highlighting the detrimental effect of aggregating both global and local models on model convergence.
Notably, the performance of FedMatch fluctuates dramatically for around 200 min since it uses decomposed model parameters for supervised training and semi-supervised training, highlighting the detrimental effect of such disjoint learning on model convergence.
Table \ref{tab-cifarnoniid} provides a direct comparison of the final performance for different levels of non-IIDness. 
In some extreme cases, FedMatch performs similarly to Supervised-only, suggesting that even with multiple helpers for pseudo-labeling, the model parameters can still be misled by the highly skewed unlabeled dataset. 
A common phenomenon is that the more the data is skewed, the more the model performance deteriorates. 
In contrast, \method consistently achieves the highest accuracy across all levels of data skewness, showcasing its adaptability to diverse data distributions. 
Under extreme non-IID scenarios such as $Dir(0.05)$, the test accuracy of \method improves by 5.0\%-5.8\%, compared to all baselines. 
The experimental results align with our original intention well with the assistance of the clustering of teacher features (features generated from teacher bottom models).
The stable performance over various data distributions is also attributed to the solid foundation built in the \srvUp stage on strongly-augmented data, which is unfortunately often overlooked in current works.
Notably, the results of SVHN and IMAGE-100 are in good agreement with those observed on CIFAR-10. 
Due to the limited space, we omit the experimental results here.
% Considering constraints in space, the results on SVHN and IMAGE-100, which closely align with those observed with CIFAR-10 are not explicitly showcased in our experiments.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}


% % Figure environment removed

% Figure environment removed

\subsubsection{Impact of the Scale of Labeled Dataset}

We conduct experiments on CIFAR-10, where we place 4,000 labeled data samples on the PS. 
However, in real-world scenarios, the amount of labeled data on the PS varies. 
To explore the impact of different scales of labeled datasets on \method, we conduct experiments by changing the amount of labeled data on the entire dataset. 
The results are presented in Fig. \ref{fig-nlabel}. 
We observe that the test accuracy gradually decreased from 87.9\% to 63.0\% when the amount of labeled data varied from 4,000 to 250 in FedSwitch-SL.
To investigate the reason for the significant performance decline when the amount of labeled data decreased from 500 to 250, we record the mask rate (the number of examples that are masked out) and data impurity (the error rate of unlabeled data that falls above the threshold) of FedSwitch as an example in Fig. \ref{fig-nlabel}(b). 
We infer that this issue is attributed to the increase in both the mask rate and data impurity, causing the data to become unreliable. 

By contrast, \method predominates the quality of predictions, as shown in Fig. \ref{fig-nlabel}(a), indicating the advantages of our proposed clustering regularization. 
\method utilizes data samples with the largest class probability that fall beneath $\tau$, which is considered invalid in consistency regularization. 
Moreover, \srvUp on strong-augmented data provides a solid foundation for semi-supervised learning by enhancing model robustness, thus reducing impurities of unlabeled data. 
In addition, our results suggest that only a few labeled data (less than 2\% of the overall dataset) is enough to satisfy some accuracy requirements, 80\% for example.

% \subsubsection{Adaptation to Labeled Data on Client}

% To evaluate the robustness and efficacy of \method under conditions with varying amounts of labeled data, we conduct a series of experiments on the STL-10 dataset. 
% We established four distinct label coverage scenarios, where the total labels on clients ranged from 1,000 to 4,000 labels. 
% The distribution of labels across clients and servers is illustrated in Fig. \ref{fig-clabel}(a).
% As depicted in Fig. \ref{fig-clabel}(b), the results demonstrate that \method consistently maintains stable and effective performance, achieving test accuracies between 91.1\% and 92.7\% across all scenarios. 
% Notably, \method with client labels achieves a significant improvement in accuracy, up to 7.6\% compared to the models that do not leverage labeled data.
% A decrease in test accuracy is observed with fewer labels on the PS, which can be attributed to the diminished effectiveness of the teacher model. 
% This model relies mainly on server-side supervised training, leading to higher impurity in pseudo-labels and slightly reduced overall performance.

% % Figure environment removed


\subsection{Ablation Study}
\subsubsection{ Impact of Global Updating Frequency Adaptation}

% Figure environment removed

We conduct ablation experiments to assess the impact of our adaptive \InvReg frequency adaptation algorithm on the performance of \method. 
Fig. \ref{fig-algexp} contrasts the test accuracies and distribution of \InvReg frequencies with (w/) and without (w/o) the algorithm. 
As shown in Fig. \ref{fig-algexp}(a), the adaptive algorithm significantly boosts the modelâ€™s final accuracy, which validates our analytical results in Section \ref{analysis-section} that the algorithm effectively guides the model towards the global optimum.
The benefits of the algorithm are especially notable in scenarios with scarce labeled data, where rapid saturation of supervised training negatively impacts model performance.
As depicted in Fig. \ref{fig-algexp}(b), in scenarios with fewer labels, the \InvReg frequency tends to stabilize at lower levels. 
For instance, with only 250 labeled samples on PS, the adaptive algorithm improves the accuracy by 10.8\%, with \InvReg frequencies mainly below 30. 
These results demonstrate that the adaptive algorithm substantially enhances training efficiency and model performance in environments with label scarcity by intelligently adjusting updating frequencies. 
Overall, the adaptive algorithm is a vital component in ensuring the training efficiency of \method.

% Figure environment removed

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.3} % Adjust the row height
\caption{Test accuracy (\%) under various data distributions with different projection head designs.\label{tab-ph}}
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Ablation}} & \multicolumn{4}{c}{Data Distribution}                         \\ \cline{2-5} 
\multicolumn{1}{c}{}    & Dir(1.0)      & Dir(0.5)      & Dir(1.0)      & Dir(1.0)      \\ \midrule
No Proj Head            & 86.6          & 82.3          & 82.8          & 80.9          \\
Linear Proj Head        & 87.2          & 86.0          & 82.9          & 81.7          \\
MLP Proj Head           & \textbf{87.9} & \textbf{86.8} & \textbf{83.8} & \textbf{82.3} \\ \bottomrule
\end{tabular}
\vspace{-0.6cm}
\end{table}



\begin{table*}[t]
\centering
\caption{Influence of parameters $\alpha$ and $\beta$ on algorithm performance.\label{tab-algab}}
\resizebox{\textwidth}{!}{% Resize table to fit within the text width
\begin{minipage}{\textwidth}
\centering
\subfloat[SVHN]{
\begin{tabular}{cccc}
\toprule
\textbf{Accuracy(\%)} & $\beta = 4$ & $\beta = 8$ & $\beta = 12$ \\ \midrule
$\alpha = 1.5$ & 91.8 & 91.1 & 91.3 \\
$\alpha = 2.0$ & 91.6 & 91.4 & 91.2 \\
$\alpha = 3.0$ & \textbf{91.8} & 91.0 & 91.4 \\
$\alpha = 4.0$ & 91.4 & 91.6 & 91.1 \\
\bottomrule
\end{tabular}
}
\hfill
\subfloat[CIFAR-10 (4000 labels)]{
\begin{tabular}{cccc}
\toprule
\textbf{Accuracy(\%)} & $\beta = 4$ & $\beta = 8$ & $\beta = 12$ \\ \midrule
$\alpha = 1.5$ & 88.1 & 89.1 & 88.1 \\
$\alpha = 2.0$ & 89.6 & 88.6 & 88.4 \\
$\alpha = 3.0$ & \textbf{89.7} & 88.3 & 88.6 \\
$\alpha = 4.0$ & 88.8 & 88.7 & 87.9 \\
\bottomrule
\end{tabular}
}
\hfill
\subfloat[CIFAR-10 (250 labels)]{
\begin{tabular}{cccc}
\toprule
\textbf{Accuracy(\%)} & $\beta = 4$ & $\beta = 8$ & $\beta = 12$ \\ \midrule
$\alpha = 1.5$ & 81.7 & \textbf{82.9} & 82.6 \\
$\alpha = 2.0$ & 81.3 & 81.9 & 80.7 \\
$\alpha = 3.0$ & 77.8 & 80.3 & 78.1 \\
$\alpha = 4.0$ & 73.6 & 80.0 & 75.3 \\
\bottomrule
\end{tabular}
}
\end{minipage}
}
\vspace{-0.3cm}
\end{table*}


\subsubsection{ Impact of Projection Head}
We investigate the impact of various projection head designs on the effectiveness of clustering regularization in \method under diverse non-IID scenarios. 
Experiments are conducted on the CIFAR-10 dataset, which is adjusted to exhibit varying levels of data skewness as detailed in Section \ref{sec-exp-dir}. 
We test several projection head configurations: one without any projection head (effectively serving as a baseline with just an identity connection), one with a single linear layer, and another featuring a multi-layer perceptron (MLP) with two linear layers separated by a ReLU activation function. 
As illustrated in Table \ref{tab-ph}, performance improves progressively from no projection head, to a single linear layer, and peaks with the MLP projection head. 
These findings affirm that projection heads enhance the efficacy of clustering regularization on addressing data non-IIDness issues by acting as a critical, parameterized component of the regularization process. 
Moreover, the simple MLP projection head strikes an optimal balance by significantly boosting performance without imposing excessive computational demands.


\subsubsection{ Ablation Study on Hyperparameters}
In \method, we set the hyperparameters $\gamma = 0.99$, $\tau=0.95$, $\alpha=1.5$, and $\beta=8$ consistently throughout training. 
We conduct a comprehensive study to assess the impact of these parameters on model performance, especially using the CIFAR-10 dataset configured with 250 labels.
Firstly, we explore the effect of the EMA decay factor $\gamma$, crucial for balancing the influence of historical data on the current teacher model's updates.
Typically, $\gamma$ is set close to 1 to ensure a robust smoothing effect in the model's updates.
As shown in Fig. \ref{fig-abs}(a), settings between 0.9 and 0.99 yield the best performance, while the accuracy declines at 0.999 due to the delay in the integration of new information from unlabeled data since the teacher model is only updated on the PS. 
Therefore, we set $\gamma$ at 0.99, consistent with standard practices and comparative baselines that utilize an EMA approach \cite{tarvainen2017mean}.
%
Next, we evaluate the confidence threshold $\tau$, pivotal for determining the utilization rate and quality of unlabeled data. 
A higher $\tau$ reduces the number of unlabeled data included in consistency regularization while enhancing the reliability of pseudo labels in forming pseudo clusters, as detailed in Section \label{motivation-section}. 
Fig. \ref{fig-abs}(b) reveals that a $\tau$ setting of 0.95 maximizes test accuracy. 
Although our study maintains a fixed $\tau$, it's worth noting that recent research \cite{zhang2021flexmatch, wang2022freematch} dynamically adjusts $\tau$, which diverges from our focus.
%
Lastly, we assess the parameters within our global updating frequency adaptation algorithm, $\alpha$ and $\beta$. 
Through systematic testing across datasets such as SVHN and CIFAR-10 with variable label counts, we analyze the impacts of different $\alpha$ and $\beta$ settings. 
Results are summarized in Table \ref{tab-algab}, showing that no single configuration consistently excels.
However, $\alpha=1.5$ and $\beta=8$ are found to be reliably effective, providing a stable and efficient balance for model training.
