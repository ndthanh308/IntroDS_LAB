\section{Algorithm for Pricing Loss}
\label{sec:pricing}

\begin{algorithm2e}[htb]
\caption{Learning with corrupted binary signal under pricing loss with uniform discretization}
\label{algo:pricing}
Set parameter $\eta_0 = T^{-1/(d+1)}$, agnostic check schedule $\tau_0$\;
Learner uniformly discretizes input space into hypercubes with lengths no larger than $\eta_0$ each\;
For each hypercube $I_j$, learner maintains an associated range $Y_j$ (initialized as $[0, L]$), query count $c_j$ (initialized as $0$)\;
% \State Set $\eta = 10 L \cdot T^{1/(d+1)}$
\For {$t = 1, 2, ..., T$} {
    Learner receives context $x_t$\;
    Learner finds hypercube $I_j$ such that $x_t \in I_j$\;
    Let $Y_j$ be the associated range of $I_j$\;
    \If {$\len (Y_j) < 10 L \cdot \eta$} { %\Comment{The interval is pricing-ready}
        $c_j := c_j + 1$\;
        \If { $c_j > \tau_0$ } {
            Query $\max(Y_j)$\;
            Set $c_j := 0$\;
        }
        \Else { 
            Query $\min(Y_j)$\;
        }
        \If {Learner is surprised}{  %\LineComment{Learner is surprised if queried $\max(Y_j)$ and $\sigma_t = 0$, or queried $\min(Y_j)$ and $\sigma_t = 1$}
            Set $Y_j := [0,L]$\;%\Comment{Reset the range of $Y_j$}\;
            Set $c_j := 0$\;
        }
    }
    % \ElsIf {Exists an endpoint in $S_j$ not queried} %\label{algo:queryendpt_beg}
    %     \State Query an unqueried endpoint of $S_j$
    %     \If {Queried $\min(S_j)$ and $\sigma_t = 1$, or queried $\max(S_j)$ and $\sigma_t = 0$} 
    %     \State Mark $I_j$ as unsafe \Comment{Contamination found in the current interval}
    %     \EndIf
    %     \If {Both endpoints of $S_j$ have been queried}
    %         \If {$I_j$ marked unsafe}
    %             \State Set range $Y_j := [0,1]$ \label{algoline:marked_rangeset}
    %         \Else
    %             \State Set range $Y_j = [\min(S_j) - L\cdot \len(I_j)), \max(S_j) + L\cdot \len(I_j))] \cap [0,1]$ \label{algoline:unmarked_rangeset}
    %         \EndIf
    %     \EndIf %\label{algo:queryendpt_end}
    \Else {%{$I_t$ not marked unsafe} \label{algo:notmarked_beg}
        $Y_j := \algmq(I_j, Y_j)$\;
        % \If { $\len(Y_j) < 4 L\cdot \len(I_j)$}  \Comment{associated range $Y_j$ has shrunk enough}
        %     \State Bisect each side of $I_j$ to form $2^d$ new hypercubes, each with length $...$
        % \EndIf %\label{algo:notmarked_end}
    }
    %\Else \Comment{$I_t$ is marked dishonest} \label{algo:dishonest_begin}
        %\State Start from scratch until converge, takes $\log T$ steps \label{algo:test}
        
        %\State Let $\set{[], [], []}$ be the sequence of feasible region (endpoints? ) on the path to $I_t$. 
        %\State Perform binary search on this path and find the first interval [] that contains $f(I_t)$. \Comment{Takes $\log\log T$ steps}, set feasible region $I_t = []$
        %\State From this point perform binary search and shrink feasible region \Comment{Takes $c$ steps, where $c$ is number of corruptions before reaching a correction interval? }
    %\EndIf %\label{algo:dishonest_end}
}
\end{algorithm2e}


This section discusses the new ideas needed to design corruption-robust algorithm for the pricing loss. The description shall be given in the context of dynamic pricing, and the learner shall be referred to as the seller in this section. The main algorithm is summarized in~\cref{algo:pricing}. Note that for the pricing loss, the case with $d =1$ and $d > 1$ are treated together. 

Extending~\cref{algo:1dabsolute} for the symmetric loss to pricing loss is not straightforward, since agnostic checks will overprice and the seller necessarily incurs a large loss whenever she overprices. The learner did not have this problem with symmetric loss, since the symmetric loss is continuous. 

\subsection{Algorithm for $C = 0$}
I first give the description of an algorithm for $C = 0$. The algorithm starts with a uniform discretization of the input space into hypercubes with length $\eta_0 = T^{-1/(d+1)}$. When a context appears in some hypercube $I_j$, one of two things can happen: 
\begin{enumerate}
    \item If the associated range is larger than $10L\cdot \eta_0$, the learner performs $\algmq$ and updates the associated range. There can be at most ${O}(\eta_0^d \log T)$ rounds of this type, since for each hypercube the associated range will shrink below $L\cdot O(\eta_0)$ after $O(\log T)$ queries. 
    \item Otherwise, the associated range is less than $10L\cdot \eta_0$, and the learner can directly set the lower end of the interval as the price. The total loss from these rounds can be bounded as $L\cdot O(T\eta_0)$. 
\end{enumerate}
The total loss is then $L (\eta_0^d \log T) + L(T\eta_0) = L\cdot {O}(T^{d/(d+1)} \log T)$. 

\szdelete{
The learner searches for the associated range $Y_j$ of a hypercube using the midpoint query procedure $\algmq$ (these shall be termed \emph{searching rounds} or \emph{searching queries}) and stops the search process when the associated range $Y_j$ becomes small enough (specifically, when the length of $Y_j$ drops below $10L\cdot \eta_0$). The hypercube is said to become \emph{pricing-ready} when this happens. When there are no adversarial corruptions, the learner now has a good estimate of the optimal price and sets the lower end of $Y_j$ as the price for any context in this hypercube. These shall be termed \emph{pricing rounds}. At \emph{pricing rounds}, the seller expects the buyer to purchase the item, and assuming the range $Y_j$ is accurate (which is the case when $C = 0$), the revenue loss should be no larger than $10 L \cdot \eta$. The total loss from \emph{pricing rounds} can be bounded by $L\cdot O(T\cdot \eta_0)$. The total loss from searching rounds can be bounded by $L\cdot  \widetilde{O}(\eta_0^{-d})$ since there are $O(\eta_0^{-d})$ hypercubes, and for each hypercube, there can be at most $O(\log T)$ queries before the hypercube becomes \emph{pricing-ready}. By the choice of parameter $\eta_0 = T^{-1/(d+1)}$, the regret bound is $L\cdot \widetilde{O} (T^{d/(d+1)})$. }

\szdelete{
It should be noted that~\cite{mao2018contextual} proposed an algorithm based on adaptive discretization whereas the above algorithm is based on uniform discretization. Using adaptive discretization improves the regret bound by a $O(\log T)$ factor, the presentation for the corruption-robust algorithm for pricing loss is based on uniform discretization, as it highlights the new algorithmic ideas more clearly. Nevertheless, the algorithm can be combined with adaptive discretization by using ideas from the previous section. }

\szcomment{TODO. Compare with adaptive discretization. }\szcomment{DONE}

\subsection{Corruption-Robust Pricing with Scheduled Agnostic Checks}

The seller could potentially run into issues when there are adversarial corruptions. The adversary can manipulate the seller into underpricing by a large margin by only corrupting a small number of signals. Consider the following example. The buyer is willing to pay $0.5$ for an item with context $x$. The adversary can manipulate the signals in the $\algmq$ procedure so that the seller's associated range for $x$ is $[0, 10L\cdot \eta]$, which does not contain and is well below the optimal price $0.5$ for this item. The seller then posts a price of $0$ for item $x$. Even though the buyer purchases the item at a price of $0$, the seller is losing 0.5 revenue per round, and this happens without the adversary corrupting only $O(\log T)$ rounds. \szcomment{TODO, rephrase}

\szcomment{ $Y_j$ is not a valid pricing range, and even though she prices the item at the lower end of $Y_j$ and the buyer is purchasing the item, the true valuation could be much higher and the seller is losing a large amount of revenue. }

To combat adversarial corruptions, the learner performs agnostic check queries. These queries differ from agnostic checks for the symmetric loss in the following two aspects. First, whereas for the symmetric loss, the learner performs agnostic check queries on both ends of the associated range $Y_j$, for the pricing loss the seller only performs agnostic checks on the upper end of $Y_j$. This ensures the seller is not underpricing the product by a large margin. Second, the seller only performs agnostic check queries when the associated range of the hypercube is sufficiently small (below $L\cdot O(\eta_0)$). The seller expects the buyer not the purchase the item during agnostic check queries. 

The learner will set a schedule for performing agnostic checks. At a high level, performing agnostic checks too often incurs large regret from overpricing, while performing agnostic checks too few may not detect corruptions effectively. The schedule for agnostic checks serves as a mean to balance the two. This schedule informs the learner how often to perform agnostic check so as not to incur large regret when overpricing, and at the same time control the loss incurred from corrupted signals. Specifically, the learner keeps track of how many rounds the context vectors arrived in each hypercube. The learner then performs an agnostic check every $\tau_0$ rounds. Here, $\tau_0$ will be a paramter to be chosen later. 

Some notions are introduced. 
\begin{definition} [Pricing-ready hypercubes]
For hypercube $I_j$, if the length of the associated range is below $10L\cdot \eta_0$, then the hypercube is \emph{pricing-ready}. 
\end{definition}

\begin{definition} [Pricing round, checking round, searching round]
If a hypercube is \emph{pricing-ready}, then setting the price as the lower end of the associated range is termed \emph{pricing round}, and setting the price as the upper end of the associated range is termed \emph{checking round}. If a hypercube is not \emph{pricing-ready}, then performing a $\algmq$ is termed \emph{searching round}. 
\end{definition}

\begin{definition}[Surprises]
    The seller is said to become \emph{surprised} when the signal she receives is inconsistent with her current knowledge. Specifically, the seller becomes surprised when either: she performs a checking round but observes an underprice signal $(\sigma_t = 0)$; or she performs a pricing round but observes an overprice signal $(\sigma_t = 1)$. 
\end{definition}

\begin{definition} [Runs]
Fix some hypercube $I_j$, the set of queries that occurs on $I_j$ before the learner becomes \emph{surprised} (or before the algorithm terminates) called a \emph{run}. 
\end{definition}
When a run ends, the learner resets the associated range and a new run begins. Note there can be multiple runs on the same hypercube. 


The algorithm gives the following cumulative loss bound. A detailed analysis can be found in~\cref{app:pricing}. 
\begin{theorem}
\label{thm:pricing}
Using~\cref{algo:pricing}, the learner incurs a total loss
\[
L\cdot O(C\log T + T^{d/(d+1)}\log T + C\tau_0 + T/\tau_0). 
\]
Specifically, setting $\tau = T^{1/(d+1)}$, the learner incurs total loss
\[
L\cdot O(T^{d/(d+1)}\log T + C\log T + C\cdot T^{1/(d+1)}) = L\cdot \widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)} ). 
\]
\end{theorem}

\begin{proof}(Proof Sketch. )
The proof consists of several steps. 
\paragraph{Step 1.} The seller can become only become surprised when at least one corruption occurs on the current `run' of some hypercube. Thus the seller can become surprised at most $C$ times. 
\paragraph{Step 2. }For searching rounds, the total loss can be bounded as $L\cdot O( (C + \eta_0^{-d}) \log T)$. There can be at most $O(T/\tau_0)$ checking rounds, contributing loss $L\cdot O(T /\tau_0)$. 
\paragraph{Step 3. } For pricing rounds, the total loss can be bounded as $L\cdot O(C \tau_0 + T \eta_0)$. At a very high level, if the pricing interval is accurate (meaning the lower endpoint is just below the true price by a margin of $L\cdot O(\tau_0)$), then these rounds contribute loss at most $O(T \eta_0)$. Otherwise, the seller can detect if the pricing interval is inaccurate by performing agnostic checks, and the loss from these rounds can be bound by $O(C \tau_0)$. 

Putting everything together completes the proof. 
\end{proof}

\begin{remark}
The loss is sublinear as long as $C = o(T^{d/(d+1)})$. In~\cite{mao2018contextual}, it was shown the optimal regret is $\Omega(T^{d/(d+1)})$ for $C = 0$. Hence, the above theorem is optimal when $C = O(T^{ (d-1)/(d+1) })$. 
\end{remark}


If the asymptotic order of $C$ is known, the learner can set $\tau_0$ to balance the terms and achieve a sharper regret bound. 
\begin{corollary}
Assume the asymptotic order of $C$ satisfies $C = O(g(T))$ and is known to the learner. Here, $g$ is some sublinear function. Using~\cref{algo:pricing} with $\tau_0 = \sqrt{T/g(T)}$, the learner incurs total loss
\[
L\cdot \widetilde{O} (T^{d/(d+1)} + \sqrt{TC}). 
\]
\end{corollary}
\begin{remark}
When $C = O(T^{(d-1)/(d+1)})$, the loss simplifies to $L\cdot \widetilde{O} (T^{d/(d+1)})$. 
When $C = \Omega(T^{(d-1)/(d+1)})$, the loss simplifies to $L\cdot \widetilde{O} (\sqrt{TC})$. 
\end{remark}

