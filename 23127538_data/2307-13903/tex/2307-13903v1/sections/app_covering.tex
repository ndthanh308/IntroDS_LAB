\section{Covering Dimension Based}

\begin{algorithm}[htbp]
\caption{Covering Dimension Based}
\label{algo:covering}
\begin{algorithmic}
\State Learner start with uniform density over $\cF(1/T^2)$, the $1/T^2$ covering of $\cF$
\For{$t = 1, 2, ..., T$}
    \State Learner finds $m_t$ such that $\Pr[f(x_t) \ge m_t] \ge 1/2$, $\Pr[f(x_t) \le m_t] \ge 1/2$
    \State Learner guesses $q_t$ uniformly chosen from $[m_t - 1/T, m_t + 1/T]$
    \State $w(f) = w(f) * (1 + c)$ if $\sigma(q_t - f(x_t)) = \sigma_t$
    \State $w(f) = w(f) * (1 - c)$ if $\sigma(q_t - f(x_t)) = 1-\sigma_t$
    \State Normalize
\EndFor
\end{algorithmic}
\end{algorithm}

%In addition, $f(x_t) = q_t$ happens with $0$ probability for any $f\in \cF(1/T^2)$. 
\begin{lemma}
    With probability $1 - \frac{1}{2T}$, $f^*(x_t)$ and $f_0(x_t)$ are on the same side of $q_t$.
\end{lemma}
\begin{proof}
    Fix any $x_t, m_t$. Note that $\abs{f_0(x_t) - f^*(x_t)} < \frac{1}{T^2}$. With probability $1 - \frac{1}{2T}$, $f_0(x_t)$ and $f^*(x_t)$ will be on the same side of a randomly drawn point in the interval $[m_t - 1/T, m_t + 1/T]$. 
\end{proof}

In accordance with the possibly corrupted signal $\sigma_t$, let $W_t^-$ be the total density of $f(x_t)$ not on the same side of $q_t$, and let $W_t^+$ be the total density of $f(x_t)$ on the same side of $q_t$. 
\begin{align*}
    W_t^- &= w ( \set{ f: \sigma( q_t - f(x_t)) = \sigma_t} ) \\
    W_t^+ &= w ( \set{ f: \sigma( q_t - f(x_t)) = 1 - \sigma_t} ) 
\end{align*}
Further define $\Delta^+ = W_t^+ - 1/2$, then $\Delta^+ \in [-1/2, 1/2]$. Intuitively, the algorithm makes more progress when $\Delta^+$ is closer to 0. 

Assume the signal was uncorrupted at round $t$, then for any $f(x_t)$ on the same side as $q_t$, the density update follows 
\begin{align*}
w(f) &= w(f) \cdot (1 + c) / [(1+c) W^+ + (1-c) W^-] \\
&= w(f) \cdot (1 + c) / (1 + 2c\Delta^+)
\end{align*}
If $f(x_t)$ is on different side as $q_t$,
\begin{align*}
w(f) &= w(f) \cdot (1 - c) / [(1+c) W^+ + (1-c) W^-] \\
&= w(f) \cdot (1 - c) / (1 + 2c\Delta^+). 
\end{align*}

\begin{lemma}
If the signal is uncorrupted and $\abs{f^*(x_t) - m_t} > 2/T$, then weight of $f_0$ grows. 
\end{lemma}
\begin{proof}
Recall $m_t$ is the median, and $q_t$ is the perturbed version. If $f^*$ and $m_t$ are at least $2/T$ apart, then $f^*$ and $f_0$ will be on same side of $q_t$. With probability $1/2$, the learner has $W^- \ge 1/2$, and consequently $w_{t+1}(f_0) \ge w_t(f_0) \cdot (1 + c)$. With the remaining probability $w_{t+1}(f_0) \ge w_t(f_0)$. 

Hence the learner has the following:
\begin{align*}
    \Ex[w_{t+1}(f_0)] \ge \frac{2+c}{2} w_t(f_0)
\end{align*}
\begin{align*}
    \Ex \left[ \frac{1} {w_{t+1}(f_0)} \right] \le \frac{2+c}{2 + 2c} \cdot \frac{1}{w_t(f_0)}
\end{align*}
\end{proof}

\begin{lemma}
    If the signal is corrupted, then
    \begin{align*}
        w_{t+1}(f_0) \ge w_t(f_0)\cdot (1 - c) / (1 + c)
    \end{align*}
    \begin{align*}
        \frac{1}{w_{t+1}(f_0)} \le \frac{1+c}{1-c} \cdot \frac{1}{w_t(f_0)}
    \end{align*}
\end{lemma}
\begin{proof}
Follows from update rule. 
\end{proof}

\begin{lemma}
If the signal is uncorrupted, then
\begin{align*}
    ...
\end{align*}
\end{lemma}
\begin{proof}
    With $1 - \nicefrac{1}{2T}$ probability, $f^*$ and $f_0$ are on same side of $q_t$ and $w_{t+1}(f_0) \ge w_t(f_0)$. Otherwise with the remaining $\nicefrac{1}{2T}$ probability, 
    \begin{align*}
        w_{t+1}(f_0) \ge w_t(f)\cdot \frac{1-c}{1+c}
    \end{align*}
    Hence
    \begin{align*}
        \Ex[w_{t+1}(f_0)] &\ge (1 - \frac{1}{2T}) w_t(f) + \frac{1}{2T} \cdot \frac{1-c}{1+c} \cdot w_t(f) \\
        &\ge w_t(f) [ 1 - \frac{1}{2T}\cdot \frac{2c}{1 + c} ]
    \end{align*}
    \begin{align*}
        \Ex[\frac{1}{w_{t+1}(f_0)}] &\le (1 - \frac{1}{2T}) \cdot \frac{1}{w_t(f)} + \frac{1}{2T} \cdot \frac{1+c}{1-c} \cdot \frac{1}{w_t(f)} \\
        &\le \frac{1}{w_t(f)} \left( 1 + \frac{1}{2T}\cdot \frac{2c}{1 - c} \right)
    \end{align*}
\end{proof}

\begin{theorem}
The main theorem ...
\end{theorem}
\begin{proof}
    Define the variables:
    $s_1 = 1/\abs{N}$. 
    \begin{align*}
        s_t &= \frac{1+c}{1-c} \text{ if } \text{corrupted} \\
        s_t &= \frac{2+c}{2 + 2c} \cdot s_{t-1}\text{ if } \abs{} > 2/T, \text{uncorrupted} \\
        s_t &= 1 + \frac{1}{2T}\cdot\frac{2c}{1-c} \text{if } \abs{} < 2/T, \text{uncorrupted}
    \end{align*}

    Define the stochastic process
    \[
    Y_t = \frac{1}{s_t\cdot w_t(f_0)}
    \]
    Then $\Ex[Y_{t+1}] \le Y_{t}$. 

    Let $C$ be the total number of corrupted rounds, let $C'$ be the total number of uncorrupted rounds with $\abs{...} < 2/T$. Then $s_T$ can be upper bounded: 
    \begin{align*}
        s_T \le \frac{1}{N} \cdot  (\frac{1+c}{1-c})^{C} \cdot (\frac{2+c}{2+2c})^{C'} \cdot (1 + \frac{1}{2T})^T \le O(1/T)
    \end{align*}

    Hence
    \[
    Y_T = \frac{1}{s_T w_T} \ge \frac{1}{s_T} \ge \Omega(T)
    \]
\end{proof}

\szcomment{TODO remove log T factor??}
\subsection{Removing the $\log T$ factor}
Try using a multi-scale argument. 
Show when guess is $\epsilon$-far, then all $\epsilon$-close hypothesis gets multiplied by a constant factor. This is because any $\epsilon$-close hypothesis must be on the same side of the true hypothesis $f^*$. 

Existing results. 
\begin{enumerate}
    \item $O(d^2)$ loss using multiscale discretization. Deterministic signals, no corruption. No density. Select the midpoint from suitable layer. Then keep all hypothesis in layer $i$ that is $z_i$-consistent to make sure true hypothesis never eliminated. 
    \item $O(d\log T)$ loss using single scale discretization. Noisy signals. Use density. Compute the midpoint. Then the weight of $f_0$ multiplied by constant factor whenever guess is $2/T$ far. Also need to make sure when guess is $2/T$ close, the weight of $f_0$ does not decrease too much, to achieve this add perturbation to guess so that they are on same side. 
\end{enumerate}

To recap, the argument for $\log T$ version with density update goes as follows. When guess $2/T$-far, then $f_0$ must be on same side as $f$, and will get multiplied by constant factor. This can happen at most $O(d\log T)$ times. 

The argument for regret constant in $T$ uses a multiscale argument. 

Let $N_z(\cF)$ be any $z$-covering of the hypothesis class $\cF$. Define the $\gamma$-window median to be the point $q$ such that
\[
w( > q + \gamma) > 1/4, w( < q - \gamma) > 1/4
\]
If there is no such $q$, then the density does not have a $\gamma$-window median. 

The learner keeps a density $w_t$ for each layer of discretization $F^{z_i}$, here $z_i = 3^-d$ so that layer $i$ provides a $z_i$-covering. The density $w^i_1$ is initialized as the uniform density. 

Ideally the algorithm should satisfy the following:
\begin{enumerate}
    \item If the guess was $2z_i$-far, then $f_i \ge f_i \cdot \frac{1.5}{1.25}$. 
    \item If uncorrupted, the weight does not decrese. 
    \item If corrupted, weight decrease by at most $1/3$. 
\end{enumerate}

The analysis focuses on the density of $f_1, f_2, ...$, the density of $z_i$-close approximations. 

The algorithm is summarized as follows. 

\begin{algorithm}
\caption{Covering Dimension based}

\begin{algorithmic}[1]

\State Learner maintains density for all layers

\For {$t = 1, 2, ..., T$} 
    \State Find the smallest index $i$ such that there exists a $z_i$-window median, let the median be $q$. 
    \State Learner queries $q$ and receives possibly corrupted signal $\sigma_t$
    \For {$i = 1, 2, \dots$} 
        \State Update the density of all $f$ in layer $i$ as follows: 
        \begin{align*}
            w(f) &= w(f) * 1.5 \text{ if } f(x) > q - z_i \\
            w(f) &= w(f) * 0.5 \text{ otherwise }
        \end{align*}
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Let $f_i$ be the function in layer $i$ such that 
\[
\norm{f_i - f^*}_\infty < z_i
\]

\begin{lemma}
    Fix a round $t$. Assume the guess was $2z_i$-far. Then $f_{i}$ is multiplied by a constant factor for any $i$. 
\end{lemma}
\begin{proof}
    We show that there exists a point $q$ such that $q$ is the $\gamma$-window median for any $\gamma \le z_i$. 
\end{proof}

