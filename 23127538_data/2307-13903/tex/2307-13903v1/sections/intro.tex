\section{Introduction}
Consider the dynamic pricing problem, where a seller (she) attempts to sell a product to a buyer each day without any prior knowledge of the maximum price $u$ the buyer is willing to pay (i.e. the buyer's private valuation for the product). \szcomment{Each day, a seller (she) attempts to sell a product to a buyer. The seller does not know the maximum price the buyer is willing to pay (i.e. the buyer's private valuation for the item) and must learn this price as she observes the buyer's behavior. } The seller must learn this price by observing the buyer's behavior, which takes the form of a binary signal: a purchased or an unpurchased item. An unpurchased item indicates an overpriced product, leading to a possible loss of the entire revenue $u$; a purchased item indicates an underpriced product, leading to a possible loss of surplus (the difference between $u$ and the posted price). However, the seller does not directly observe the revenue loss. She \lbcomment{I think you need to mention 'she' at the beginning} only observes the binary signal of the buyer. The seller adjusts the posted price according to the signal and gradually converges to the optimal price. An early important work~\cite{kleinberg2003value} studied dynamic pricing from a regret-minimization perspective and fully characterize the optimal regret of the seller for the most basic form of this problem. 

\szcomment{An early important result in dynamic pricing from a regret-minimization perspective appeared in~\cite{kleinberg2003value}, and this work characterized the optimal regret of the seller for the most basic form of this problem. }\lbcomment{An early important result in dynamic pricing, established in~\cite{kleinberg2003value}, from a regret-minimization perspective, and this work characterized the optimal regret of the seller for the most basic form of this dynamic pricing problem. }

\szcomment{Alternate version. Consider the dynamic pricing problem, where a seller attempts to sell a product to a buyer each day without any prior knowledge of the maximum price the buyer is willing to pay, which referred to as the buyer's private valuation. The seller must learn this price by observing buyer's behavior, which takes the form of a binary signal - a purchased or unpurchased item. A purchased item indicates an underpriced product, leading to potential loss of surplus; an unpurchased item indicates an overpriced product, leading to a possible loss of revenue. However, the seller lacks insight of the revenue loss and only has access to the binary signal. As such, the seller adjusts the price according to the observed signal and gradually converges to the optimal price. A seminal study~\cite{kleinberg2003value} characterizes the optimal regret of the seller. (I think the last sentence doesn't fit here, which may go to the next paragraph for complete literature survey. Or make this sentence more complete to add more details to fit the theme) }


The \emph{contextual} search (or contextual pricing) problem~\cite{liu2021optimal, leme2022contextual, mao2018contextual} is motivated by the fact that buyers have different valuations for differentiated products with different features. Henceforth the features of products are referred to as \emph{context} vectors. A common assumption is that the buyer's valuation is a linear function on the context vector~\cite{liu2021optimal, leme2022contextual}. In this setting, Liu et al.~\cite{liu2021optimal} almost fully characterize the optimal regret. Another assumption that appeared in the literature is Lipschitzness: the valuation function only requires Lipschitzness in the context vectors without assuming an explicit parametric form~\cite{mao2018contextual}. 

\szcomment{Alternate version. The contextual search problem comes from the idea that various product features influence a buyer's valuation. These product features are referred to as context vectors. A common assumption is that the buyer's valuation is a linear function on the context vectors. Liu et al.~\cite{liu2021optimal} have almost completely characterized the optimal regret in this setting. Lipschitzness is another approach: the valuation function only requires Lipschitzness in the context vectors without assuming an explicit parametric form~\cite{mao2018contextual}. }

In practice, it is unreasonable to assume the signals the seller receives are perfect. For example, buyers can act irrationally, or in extreme cases may even exhibit malicious behaviors, resulting in faulty signals. This motivates the study of designing contextual search algorithms robust to corruptions in the binary signals that the learner observes. \szdelete{The corruptions are modeled as generated by an adversary who has the power to corrupt the binary signals observed by the learner.}\lbcomment{This sounds weird grammatically} Previous works~\cite{krishnamurthy2022contextual, leme2022corruption} studied linear contextual search with corruptions, in which an adversary has the power to corrupt the binary signals. This work studies Lipschitz contextual search with corruptions and proposes corruption-robust algorithms. 

\szcomment{This work considers an adversary model where the adversary has the power to directly corrupt the binary signal after observing the learner's guess. This adversary model is much stronger than the recent works~\cite{krishnamurthy2022contextual, leme2022corruption} on corruption-robust linear contextual search. }\szcomment{studied linear contextual search in an \emph{adversarial corruption} model. In their model, the adversary commits to a corruption $z_t$ to be added to the function value \emph{before} observing the guess by the learner. In the current work, \lbcomment{In the current work -> In this work} the adversary is able to directly decide whether to corrupt the signal, hence the power of the adversary in this work is significantly stronger. }

\paragraph{Loss Functions and Applications. } The literature on contextual search usually considers two loss functions, the \emph{pricing loss} and the \emph{symmetric loss} (synonymous with the \emph{absolute loss} in some work). The pricing loss is defined by
\[
\ell(q, f(x)) = f(x) - q\cdot \ind{q\le f(x)}. 
\]
The pricing loss captures the dynamic pricing setting described above. The optimal price for a product with context $x$ is $f(x)$. If the seller overprices $q > f(x)$, she loses the entire revenue $f(x)$; if the seller underprices $q \le f(x)$, she loses the potential surplus $f(x) - q$. The cumulative pricing loss then measures the regret of the seller had she known the valuation function $f$. It should be noted the actual loss of each round is never revealed to the learner, and only the binary signal is revealed. 

The symmetric loss is defined as
\[
\ell(q, f(x)) = \abs{q - f(x)}. 
\]
An application of symmetric loss is personalized medicine. Consider a healthcare provider experimenting with a new drug. Assume the optimal dosage for a patient with context feature $x$ is $f(x)$, and the given dosage is $q$. The symmetric loss then measures the distance between the injected dosage and the optimal dosage. The learner (healthcare provider) is not able to directly observe this loss but can observe a binary signal informing whether she overdosed or underdosed. 

\subsection{Contributions}
In this work, I design corruption-robust algorithms for learning Lipschitz functions with binary signals. The learner tries to learn a Lipschitz function $f$ chosen by an adversary. At each round, the adversary presents the learner with an adversarially chosen context vector $x_t$, and the learner submits a guess $q_t$ to the value of $f(x_t)$ and observes a binary signal indicating whether the guess was low or high. A total of $C$ signals may be corrupted, though the value of $C$ is unknown to the learner. The goal of the learner will be to incur a small cumulative loss that degrades gracefully as $C$ increases. In the adversary model in this work, the adversary has the power to directly corrupt the binary signal after observing the guess submitted by the learner, and this model is much stronger than previous works on corruption-robust linear contextual search~\cite{krishnamurthy2022contextual, leme2022corruption}. 

I design corruption-robust algorithms under two loss functions, symmetric loss and pricing loss. The corruption-robust algorithm for symmetric loss is given in~\cref{sec:symmetric}, and the algorithm for pricing loss is given in~\cref{sec:pricing}. The main result are presented below and summarized in~\cref{table:results}. 

\lbcomment{Alternate version: Assuming the optimal dosage for a patient with context feature $x$ is $f(x)$, and the administered dosage is $y$, the symmetric loss measures the deviation between the given dosage and the optimal dosage. The learner (healthcare provider) cannot directly observe this loss but can access a binary signal indicating if an overdose or underdose has occurred. I present a corruption-robust algorithm for symmetric loss in~\cref{sec:symmetric}, with the main result outlined as follows.
}

\begin{theorem*}[\Cref{thm:symm1D} restated]
For symmetric loss with $d=1$, there exists an algorithm that achieves regret $L\cdot O(C\log T)$. 
\end{theorem*}

\begin{theorem*}[\Cref{thm:symmMD} restated]
For symmetric loss with $d\ge 2$, there exists an algorithm that achieves regret $L\cdot O_d(C\log T + T^{(d-1)/d})$. 
\end{theorem*}

\szdelete{The pricing loss is defined as
\[
\ell(q, f(x)) = f(x) - q\cdot \ind{q \le f(x)}. 
\]
The natural application is dynamic pricing, as introduced previously. If the seller overprices ($q > f(x)$), she loses the entire revenue $f(x)$; if the seller underprices ($q \le f(x)$), she loses the potential surplus $f(x) - q$. I give corruption-robust algorithms for pricing loss in~\cref{sec:pricing}, a simplified version of the main result is as follows. }

\lbcomment{Alternate version. One of its application is the dynamic pricing problem. If seller overprices ($y > f(x)$), the entire revenue $f(x)$ is lost; whereas, if the seller underprices ($y \le f(x)$), the potential surplus $f(x) - y$ is forfeited.  I give corruption-robust algorithms for pricing loss in~\cref{sec:pricing}, a simplified version of the main result is as follows. }
\begin{theorem*}[\Cref{thm:pricing} restated]
    For pricing loss, there exists an algorithm that achieves regret $L\cdot \widetilde{O}(T^{d/(d+1)} + C \cdot T^{1/(d+1)})$. More generally, for any $\tau_0 \in (2,T)$, there exists an algorithm that achieves regret $L\cdot \widetilde{O} (T/\tau_0 + C\tau_0 + T^{d/(d+1)})$. 
\end{theorem*}

This work \lbcomment{The current work -> this work; make the whole paper consistent of using ``this work'' to refer as your work, or readers maybe confused} introduces the new technique of \emph{sanity check}. \lbcomment{I'd say you should italicize every occurrence of 'sanity check' -> \textit{sanity check}, so the readers know you are referring to the same word, and it makes your paragraph much cleaner} \szcomment{I italized the first occurrences of this word (in every new section)}This new component is incorporated into the adaptive discretization or uniform discretization procedure that is usually used in learning Lipschitz functions. At a very high level, \emph{sanity checks} insures the estimated function values in the discretized input space are accurate, despite adversarial corruptions. The analysis is much more intricate, since \emph{sanity checks} can also be corrupted. 

\renewcommand{\arraystretch}{1.5}
\begin{table}[ht]
\centering
\caption{Regret for Lipschitz contextual search}
\label{table:results}
\begin{tabular}{|l|l|l|}
\hline
                    & No corruption~\cite{mao2018contextual} & Corrupted Signals (\textbf{this work})                                     \\ \hline
Symmetric $(d = 1)$ & $O(\log T)$         & $O(C\log T)$                                  \\ \hline
Symmetric $(d > 1)$ & $O(T^{(d-1)/d})$    & $O_d(C\log T + T^{(d-1)/d})$                    \\ \hline
Pricing Loss        & $O(T^{d/(d+1)})$    & $\widetilde{O}(C\cdot T^{1/(d+1)} + T^{d/(d+1)})$ \\ \hline
\end{tabular}
\end{table}
\szcomment{Change height.. }

\subsection{Related Work}
\lbcomment{consider making this title a section or a bold paragraph. You only have 1.1 but not 1.2 or 1.3 followed by this section, which looks awkward to me.}
The two most related threads to this work are contextual search and corruption-robust learning. The linear contextual search problem was studied in~\cite{liu2021optimal, leme2022contextual}, and the recent work by Liu et al.~\cite{liu2021optimal} achieves state-of-the-art regret bounds. For the Lipschitz contextual search problem, Mao et al.~\cite{mao2018contextual} propose algorithms based on adaptive zooming and binary search. They also showed their algorithm to be nearly optimal. The contextual search problem is also closely related to the dynamic pricing problem, for an overview see~\cite{den2015dynamic}. \szcomment{TODO}

Recently, designing learning algorithms robust to corruption and data-poisoning attacks has received much attention. Leme et al.~\cite{leme2022corruption} gave corruption-robust algorithms for the linear contextual problem based on density updates. The study of adversarial attack and the design of corruption-robust algorithms has also appeared in bandit learning~\cite{lykouris2018stochastic, gupta2019better, jun2018adversarial, zuo2020near, garcelon2020adversarial, bogunovic2021stochastic} and reinforcement learning~\cite{lykouris2021corruption, zhang2022corruption, chen2021improved}, to name a few. \szcomment{TODO, add cite. }

This work is also related to the literature on contextual bandits and bandits in metric spaces~\cite{kleinberg2008multi, kleinberg2010sharp, cesa2017algorithmic, slivkins2011contextual}. The notable work by Slivkins~\cite{slivkins2011contextual} studied a setting where the learner picks an arm after observing the contextual information each round, and subsequently the context-arm dependent reward (or equivalently loss) is revealed. \szcomment{In this work \lbcomment{In the current work -> In this work}, the loss is not revealed to the learner, and only a binary signal is observed, hence the feedback structure is fundamentally different. \lbcomment{You may want to add a couple of sentences to explain why the difference matter, and perhaps why it matters should go to the end of this paragraph or go to the contribution section to emphasize your novelty; review other people's work first and explain what is unique in your work} }\szcomment{TODO. add cite}

Another interesting direction peripheral to this work is learning with strategic agents. In this setting, the learner is not interacting with an adversary, but with a strategic agent whose goal is to maximize his long-term utility. Amin et al.~\cite{amin2013learning} studied designing auctions with strategic buyers; Amin et al.~\cite{amin2014repeated} and Drusta~\cite{drutsa2020optimal} incorporate contextual information and design contextual auctions with strategic buyers. 
