\subsection{Symmetric Loss with $d > 1$}
\label[appendix]{app:sym-highd}



\begin{algorithm}[h]
\caption{Learning with corrupted binary signal under symmetric loss with $d > 1$}
\label{algo:highDsymm}
\begin{algorithmic}[1]
\State Learner maintains a partition of hypercubes $I_j$ of the input space $[0,1]^d$ throughout learning process
\State For each hypercube $I_j$, learner stores a sanity check interval $S_j$ and maintains an associated range $Y_j$
\State The partition is initialized as a single hypercube with sanity check interval $S_j$ set to $[0, 8L]$
\For {round $t = 1, 2, ..., T$}
    \State Learner receives context $x_t$
    \State Learner finds hypercube $I_j$ such that $x_t \in I_j$
    \State Let $Y_j$ be the feasible interval of $I_j$
    \If {Exists an endpoint of $S_j$ not queried} %\label{algo:queryendpt_beg}
        \State Query an unqueried endpoint of $S_j$
        \If {Queried $\min(S_j)$ and $\sigma_t = 1$, or queried $\max(S_j)$ and $\sigma_t = 0$} 
        \State Mark $I_j$ as unsafe \Comment{Contamination found in the current interval}
        \EndIf
        \If {Both endpoints of $S_j$ have been queried}
            \If {$I_j$ marked unsafe}
                \State Set range $Y_j := [0, 8L]$ \label{algoline:highd_marked_rangeset}
            \Else
                \State Set range $Y_j = [\min(S_j) - L\cdot \len(I_j)), \max(S_j) + L\cdot \len(I_j))] \cap [0,1]$ \label{algoline:highd_unmarked_rangeset}
            \EndIf
        \EndIf %\label{algo:queryendpt_end}
    \Else %{$I_t$ not marked unsafe} \label{algo:notmarked_beg}
        \State $Y_j := \algmq(I_j, Y_j)$
        \If { $\len(Y_j) < \max (4 L\cdot \len(I_j), 4L / T )$ }  \Comment{associated range $Y_j$ has shrunk enough}
            \State Bisect each side of $I_j$ to form $2^d$ new hypercubes each with length $\len(I_j) / 2$
            \State For each new hypercube $I_{ji}$ ($1\le i \le 2^d$) set $S_{ji} = Y_j$
        \EndIf %\label{algo:notmarked_end}
    %\Else \Comment{$I_t$ is marked dishonest} \label{algo:dishonest_begin}
        %\State Start from scratch until converge, takes $\log T$ steps \label{algo:test}
        
        %\State Let $\set{[], [], []}$ be the sequence of feasible region (endpoints? ) on the path to $I_t$. 
        %\State Perform binary search on this path and find the first interval [] that contains $f(I_t)$. \Comment{Takes $\log\log T$ steps}, set feasible region $I_t = []$
        %\State From this point perform binary search and shrink feasible region \Comment{Takes $c$ steps, where $c$ is number of corruptions before reaching a correction interval? }
    \EndIf %\label{algo:dishonest_end}
\EndFor
\end{algorithmic}
\end{algorithm}

The corruption-robust algorithm for learning a Lipschitz function from $\cR^d \rightarrow \cR (d > 1)$ under symmetric loss is summarized in~\cref{algo:highDsymm}. The proposed algorithm initializes the input space as a single hypercube with $S_j$ initialized to $[0, 8L]$ (instead of $8^d$ hypercubes with $S_j$ initialized as $[0,L]$). This is purely for ease of exposition: the number of hypercubes at depth $h$ is simplified from $8^d \cdot O(2^{hd})$ to $O(2^{hd})$. \szcomment{TODO here}
%The analysis makes the assumption that $L = 1/8$, so that there is exactly one root interval (line?? in algorithm ??). If $L < 1/8$, the learner can simply scale range of 

\begin{theorem}[\Cref{thm:symmMD} restated]
\Cref{algo:highDsymm} achieves cumulative symmetric loss $L\cdot O(T^{\nicefrac{(d-1)}{d}} + C\cdot \log T + C\cdot 2^d)$ for $d \ge 2$. 
\end{theorem}
The analysis will be similar to that of~\cref{algo:1dabsolute} and~\cref{thm:symm1D}. 

\begin{proof}
There are $O(C)$ corrupted intervals, contributing $L\cdot O(C\log T)$ loss. There are $O(C\cdot 2^d)$ amending intervals, contributing $ L\cdot O( C\cdot 2^d)$ loss. For safe intervals, there are at most $O(2^{hd})$ intervals at depth $h$. Throughout $T$ rounds, a loss with magnitude $O(2^{-h})$ can be charged at most $O(2^{hd})$ times, and the total loss of all safe intervals is then at most $L\cdot O(T^{(d-1) / d})$, since the loss coming from safe intervals can be upper bounded by
\begin{align*}
    L\cdot \sum_{h=0}^{\frac{\log T}{d}} O( 2^{(d-1)h} ) = L\cdot O(T^{\nicefrac{(d-1)}{d}}). 
\end{align*}
Putting the loss of all three types of intervals together completes the proof. 
\end{proof}


