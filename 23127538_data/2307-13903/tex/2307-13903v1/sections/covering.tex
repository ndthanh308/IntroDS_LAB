\section{Covering Dimension Based Bounds}
This sections gives a corrupted robust algorithm for learning general hypothesis classes with finite covering dimension. The problem setup remains the same with the Lipschitz function class being replaced by a general hypothesis class. It is shown here that the density update algorithm in~\cite{liu2021optimal} achieves loss $O(Cd\log T)$. 

Let $\cF$ be a hypothesis class and $\cX$ be the input space. For any function $f\in\cF$, $f$ maps elements in $\cX$ to $\cR$. An adversary selects a ground truth hypothesis $f^*$. At each round, the adversary selects an input $x_t$, and the learner submits a guess $q_t$ upon observing the input. The adversary then gives the learner a binary feedback $\sigma_t$, and the learner suffers the symmetric loss $\ell_t = \abs{f^*(x_t) - q_t}$. 

The algorithm works as follows. The learner takes a $\frac{1}{T^2}$-covering of the hypothesis space $\cF$.  Denote the covering set by $C_\cF$, then there exists a element $f_0$ in the covering set such that $\norm{f_0 - f^*}_\infty < 1/T^2$. 

At initialization, the learner sets a uniform density in the covering set. Subsequently in each round, the learner finds the number $m_t$ such that $y_t$ is the median and queries $q_t$, a perturbed version of $m_t$. 

The analysis focuses on the density of $f_0$. Assume in some round, the query point is far from $f^*(x_t)$ with a distance at least $2/T$ and that the signal was uncorrupted, then density of $f_0$ will be multiplied by a constant. For any corrupted rounds, the density of $f_0$ is shrinks by at most a constant factor. 

\begin{theorem}
    Main theorem. 
\end{theorem}

\szcomment{Moved the proof to appendix. }

