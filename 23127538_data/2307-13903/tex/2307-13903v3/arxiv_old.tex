\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

\show\abs

\usepackage{fullpage,amssymb,graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{bbm}
%\usepackage{commath}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{nicefrac}
\usepackage{mathtools}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{appendix}

\usepackage{thmtools,thm-restate}


\usepackage[suppress]{color-edits}

\addauthor[sz]{sz}{red}
\addauthor[sz]{zz}{red}

\addauthor[draft]{draft}{blue}


\input{notations.tex}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Corruption-robust Lipschitz contextual pricing}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Corruption-Robust Lipschitz Contextual Pricing
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: 
%\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Shiliang Zuo \\
  University of Illinois Urbana-Champaign \\
  \texttt{szuo3@illinois.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
I consider the problem of learning a Lipschitz function with corrupted binary feedback. An adversary selects a $L$-Lipschitz function $f$ at the beginning of the game. Then each round, the adversary selects a context vector $x_t$ in the input space, then the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds the signal may be corrupted, though the value of $C$ is unknown to the learner. I present a natural yet powerful technique `sanity check', which proves useful in designing corruption-robust algorithms. I design algorithms which: for the symmetric loss, the learner achieves $O(C\log T)$ loss with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ loss with $d > 1$; for the pricing loss with a learner's choice of parameter $u \in (0, 1)$, the learner achieves $\Tilde{O}(T^{\frac{(1-u)d + u}{(1-u)d + 1} } + T^u C^{1-u} + C^{1/u} )$ loss. 
\end{abstract}


% keywords can be removed
\keywords{Online Learning \and Dynamic Pricing \and Robust Algorithm}


\section{Introduction}

Consider a seller (she) who at each day attempts to sell a product to a buyer. The seller does not know the maximum price the buyer is willing to pay (i.e. the buyer's private valuation for the item), and must learn this price as she observes the purchasing behavior of the buyer. To this end, the seller sets a price each day, and observes a binary signal from the buyer: the buyer either purchased or did not purchase the item. An unpurchased item indicates an overprice on the seller's side and results in possible loss of revenue; a purchased item indicates an underprice and results in possible loss of surplus. The seller does not directly observe how much revenue she actually lost, but only observes the binary purchasing behavior of the buyer. The seller adjusts the posted price according to the signal, and gradually converges the the optimal price. Such is the dynamic pricing problem in its most basic form.  An early important result appeared in~\cite{kleinberg2003value}, characterizing the optimal regret of the seller. 

The contextual search problem~\cite{liu2021optimal, leme2022contextual, mao2018contextual} comes from the motivation that products may come differentiated, and the buyer's valuation will be a function of the features of the products. Henceforth the features of products will be referred to as context vectors. A common assumption is that the buyer's valuation is a linear function on the context vector. In this setting, Liu et al.~\cite{liu2021optimal} almost fully characterizes the optimal regret. Another assumption is lipschitzness: the valuation function does not assume explicit parametric form, but only requires lipschitzness in the context vector~\cite{mao2018contextual}. 

In practice, it is unreasonable to assume the signals the sellers receives are perfect. For example, agents can act irrational, or malicious entities may interfere with buyer's behaviors, resulting in faulty signals. This motivates the study of contextual pricing with corrupted binary signals. Leme et al. studies the linear contextual search problem with corrupted binary signals~\cite{leme2022corruption}, and proposes a low-regret algorithm based on density updates. 

In this work, I design corruption-robust algorithms for learning Lipschitz functions with binary signals under two loss functions, the symmetric loss and pricing loss. The symmetric loss is defined as
\[
\ell(y, f(x)) = \abs{y - f(x)}. 
\]
A potential application where symmetric loss applies is personalized medicine. Assume the optimal dosage for a patient with context feature $x$ is $f(x)$, and the injected dosage is $y$. The symmetric loss then measures the distance of injected dosage to optimal dosage. The learner (healthcare provider) is not able to directly observe this loss, but can observe a binary signal informing whether she overdosed or underdosed. I give a corruption-robust algorithm for symmetric loss in~\cref{sec:symmetric}, the main result is as follows. 
\begin{theorem*}[\Cref{thm:symm1D} restated]
For $d=1$, there exists an algorithm that achieves cumulative symmetric loss $L\cdot O(C\log T)$. 
\end{theorem*}

\begin{theorem*}[\Cref{thm:symmMD} restated]
For $d>1$, there exists an algorithm that achieves cumulative symmetric loss $L\cdot O_d(C\log T + T^{(d-1)/d})$. 
\end{theorem*}

The pricing loss is defined as
\[
\ell(y, f(x)) = f(x) - y\cdot \ind{y \le f(x)}. 
\]
The natural application is dynamic pricing, as introduced previously. If the seller overprice ($y > f(x)$), she loses the entire revenue $f(x)$; if the seller underprice ($y \le f(x)$), she loses the potential surplus $f(x) - y$. I give corruption-robust algorithms for pricing loss in~\cref{sec:pricing}, a simplified version of the main result is as follows. 
\begin{theorem*}[\Cref{thm:pricing} simplified]
    Fix $u\in(0,1)$. There exists an algorithm that achieves cumulative pricing loss $L\cdot \Tilde{O}(T^{\frac{(1-u)d + u}{(1-u)d + 1} } + T^u C^{1-u} + C^{1/u})$. 
\end{theorem*}
\szcomment{TODO}

\subsection{Related Work}
\szcomment{TODO}


% The problem of learning a lipschitz function through binary feedback was initiated by~\cite{mao2018contextual}. I consider the problem when the binary signals are corrupted by an adaptive adversary, and give corruption robust algorithms for this problem. For $d=1$ under symmetric loss, I give a algorithm with loss bound $L\cdot O(C\log T)$. Here $C$ is the number of rounds with corrupted signals. Critically, the value of $C$ is \emph{unknown} to the learner. I also extend the algorithm to higher dimensions with $d \ge 2$. 

\section{Problem Formulation}
\label{sec:formulation}
\begin{definition}
Let $f: \cR^d \rightarrow \cR$. If $f$ satisfies:
\[
\abs{f(x) - f(y)} \le L\norm{x - y}
\]
for any $x, y$, then $f$ is $L$-Lipschitz with respect to the norm $\norm{\cdot}$. If the norm is the $\norm{\cdot}_\infty$ norm, then $f$ is simply $L$-Lipschitz, or that $f$ has Lipschitz constant $L$.  
\end{definition}

An adversary selects an $L$-Lipschitz function $f:\cR^d \rightarrow [0,L]$. The function $f$ is initially unknown to the learner. At each round $t$, the adversary selects a \textit{context} $x_t$ in the input space $\cX$, and the learner submits a guess $q_t$ to the value $f(x_t)$. 

After the learner submits a guess, she observe a binary signal $\sigma_t \in \set{0,1}$. The binary signal $\sigma_t$ at round $t$ may be uncorrupted or corrupted. If the signal is uncorrupted, then
\[
\sigma_t := \sigma(q_t - f(x_t)). 
\]
If the signal is corrupted, then
\[
\sigma_t := 1 - \sigma(q_t - f(x_t))
\]
Here, $\sigma(u)$ is the step function that takes value 1 if $u > 0$ and 0 if $u \le 0$. Hence an uncorrupted signal of $\sigma_t = 1$ indicates a guess too high, and a signal of $\sigma_t = 0$ indicates a guess too low. If $f(x_t) = q_t$, then the feedback can be arbitrary in $\set{0,1}$. However, the learner can add infinitesimal perturbation to $q_t$, so that $f(x_t) = q_t$ happens with 0 probability, thus subsequent analysis will not consider the case $f(x_t) = q_t$. It is assumed that at most $C$ rounds are corrupted. Critically, the value $C$ is \emph{unknown} to the learner, though it is assumed $C$ is sublinear in $T$. 

After the guess is submitted each round, the learner suffers a loss $\ell_t = \ell(q_t, f(x_t))$. Note that the loss function is known to the learner but the loss value is never revealed to the learner. The goal of the learner will be to incur as little cumulative loss as possible, where the cumulative loss is defined as
\[ 
\sum_{t=1}^T \ell_t = \sum_{t=1}^T \ell(q_t, f(x_t)). 
\]

\begin{remark}
It should be noted there is a slight difference in the range of $f$ compared with the formulation in~\cite{mao2018contextual}. In~\cite{mao2018contextual}, the authors assumed the range of $f$ to be $[0,1]$ regardless the value of $L$, where this work assume the range of $f$ to be $[0,L]$. The author believe the assumption in this paper is the more natural one, as the range of an $L$-Lipschitz function on $[0,1]^d$ is $L$. Assuming the range to be $[0,L]$ also avoids some of the unnecessary complications (in the author's opinion) that arises from the scaling of $L$ (see~\cite{mao2018contextual} appendix A). If one replaces the range $[0,L]$ to $[0,1]$, the proposed algorithms in this work leads to sharper regret bounds (see details in following sections). 
\end{remark}

\input{sections/symmetricloss}


\input{sections/pricing.tex}

%\section{Conclusion}
% I studied the problem of learning a Lipschitz function with corrupted binary feedback, and proposed corruption-robust algorithms. 
%\input{sections/covering.tex}


% \section{Tight Bound for 1d loss}
% Idea. Express loss in recursive fashion. A large loss implies small loss in future rounds. While a small loss implies potential larger loss in future rounds. Though the loss is never revealed, so this adds difficulty. 

% The midpoint algorithm by ??[][] have $\log T$ loss when querying dyadic numbers. Short proof: assume the function was actually constant. Each dyadic number presented twice... 



%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  



\newpage
\begin{appendices}

%\input{sections/app_Leffect}

\input{sections/app_proof1d}

\input{sections/app_highd.tex}

\input{sections/app_pricing}

%\input{sections/app_covering}

%\input{sections/misc_improve}

\end{appendices}



\end{document}


