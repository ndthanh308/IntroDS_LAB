\section{Algorithm for Absolute Loss}
\label{sec:symmetric}

This section gives a corruption-robust algorithm for the absolute loss, defined by
\[
\ell(q_t, f(x_t)) = \abs{f(x_t) - q_t}. 
\] 
I will first design an algorithm for $d = 1$ in this section (\cref{algo:1dabsolute}), then extend the algorithm to $d \ge 2$. 

\begin{algorithm2e}
\caption{Learning with corrupted binary signals under absolute loss for $d=1$}
\label{algo:1dabsolute}
Learner maintains a partition of intervals $I_j$ of the input space throughout the learning process\;
For each interval $I_j$ in the partition, learner stores a checking interval $S_j$, and maintains an associated range $Y_j$\;
The partition is initialized as $8$ intervals $I_j$ with length $1/8$ each, and each $I_j$ has associated range $Y_j$ and checking interval $S_j$ set to $[0,L]$\;
\For {$t = 1, 2, ..., T$} {
    Learner receives context $x_t$\;
    Learner finds interval $I_j$ such that $x_t \in I_j$\;
    Let $Y_j$ be the associated range of $I_j$\;
    \If {Exists an endpoint of $S_j$ not yet queried} { %\label{algo:queryendpt_beg}
        Learner selects an unqueried endpoint of $S_j$ as guess \;
        \If {Learner guessed $\min(S_j)$ and $\sigma_t = 1$, or guessed $\max(S_j)$ and $\sigma_t = 0$} {
            Mark $I_j$ as unsafe\;
        }
        %\Comment{Contamination found in the current interval}
        \If {Both endpoints of checking interval $S_j$ have been queried} {
            \If {$I_j$ marked unsafe} {
                Set range $Y_j := [0,L]$\;\label{algoline:marked_rangeset}
            }
            \Else {
                Set range $Y_j = [\min(S_j) - L\cdot \len(I_j)), \max(S_j) + L\cdot \len(I_j))] \cap [0,L]$\;\label{algoline:unmarked_rangeset}
            }
        } %\label{algo:queryendpt_end}
    % \Else { %{$I_t$ not marked unsafe} \label{algo:notmarked_beg}
    %     $Y_j := \algmq(I_j, Y_j)$\;
    %     \If { $\len(Y_j) < \max( 4 L\cdot \len(I_j), 4 L / T )$ }  {
    %         Bisect $I_j$ into $I_{j1}, I_{j2}$, set $S_{j1} = Y_j, S_{j2} = Y_j$\;
    %     }%\label{algo:notmarked_end}
    % }
    %\Else \Comment{$I_t$ is marked dishonest} \label{algo:dishonest_begin}
        %\State Start from scratch until converge, takes $\log T$ steps \label{algo:test}
        
        %\State Let $\set{[], [], []}$ be the sequence of feasible region (endpoints? ) on the path to $I_t$. 
        %\State Perform binary search on this path and find the first interval [] that contains $f(I_t)$. \Comment{Takes $\log\log T$ steps}, set feasible region $I_t = []$
        %\State From this point perform binary search and shrink feasible region \Comment{Takes $c$ steps, where $c$ is number of corruptions before reaching a correction interval? }
    }
    \Else { %{$I_t$ not marked unsafe} \label{algo:notmarked_beg}
        $Y_j := \algmq(I_j, Y_j)$\;
        \If { $\len(Y_j) < \max( 4 L\cdot \len(I_j), 4 L / T )$ }  {
            Bisect $I_j$ into $I_{j1}, I_{j2}$, set $S_{j1} = Y_j, S_{j2} = Y_j$\;
        }%\label{algo:notmarked_end}
    }
}
\end{algorithm2e}

\begin{algorithm2e}
\caption{Midpoint query procedure: $\algmq(I_j, Y_j)$}
\label{algo:midpt_query}
    Input: Interval $I_j$, associated range $Y_j$\;
    Let $q$ be midpoint of $Y_j$\;
    Learner queries $q$\;
    \If {$\sigma_t$ = 1}{
        % \State Set $Y'_j := [\min(Y_j), q_t + L\cdot\len(I_j)]$
        Set $Y'_j := [0, q_t + L\cdot\len(I_j)] \cap Y_j$\;
        \szcomment{This and above should be equivalent}
    }
    \Else {
        % \State Set $Y'_j := [q_t - L\cdot\len(I_j), \max(Y_j)]$
        Set $Y'_j := [q_t - L\cdot\len(I_j), 1] \cap Y_j$\;
        \szcomment{This and above should be equivalent}
    }
    Return $Y'_j$\;
\end{algorithm2e}

\subsection{Algorithm for $C=0$}

It will be helpful to first give a brief description of an algorithm that appeared in~\cite{mao2018contextual}. This algorithm works for $d=1$ and when there are no adversarial corruptions. At each point in time, the learner maintains a partition of the input space into intervals. For each interval $I_j$ in the partition, the learner also maintains an associated range $Y_j$, which serves as an estimate of the image of $I_j$. In particular, the algorithm ensures the following is true: $f(I_j)\subset Y_j, \len(Y_j) = L\cdot O(\len(I_j))$. When a context appears in $I_j$, the learner selects the midpoint of $Y_j$ as the query point, and the associated range $Y_j$ shrinks and gets refined over time. When $Y_j$ reaches a point where significant refinement is no longer possible, the learner zooms in on $I_j$ by bisecting it. This corresponds roughly to the subprocedure summarized in~\cref{algo:midpt_query}, which is termed the midpoint query procedure $\algmq$. The midpoint query procedure shall be used as a subprocedure in the corruption-robust algorithms that this work proposes. 

\subsection{Corruption-Robust Search with Agnostic Checks}

The main algorithm for absolute loss with $d = 1$ is summarized in~\cref{algo:1dabsolute}. This algorithm is corruption-robust and agnostic to the corruption level $C$. Below I give the key ideas in this algorithm. 

When there are adversarial corruptions, it will generally be impossible to tell for certain whether the associated range $Y_j$ contains the image of the interval $I_j$. That is, the learner will not know with complete certainty whether $f(I_j) \subset Y_j$ holds. The analysis divides intervals into three types: \textit{safe intervals}, \textit{correcting intervals}, and \textit{corrupted intervals}. 

\begin{definition} [Corrupted, Correcting, and Safe Intervals]
Intervals are divided into three types. Consider some interval $I_j$. 
\begin{itemize}
\item $I_j$ is a \emph{corrupted interval} if, there exists some $t$ where $x_t\in I_j$ and the signal $\sigma_t$ is corrupted. %for any context that appears in the interval, the signal is corrupted gives a corrupted signal. 
\item $I_j$ is a \emph{correcting interval} if its parent interval is a \emph{corrupted interval} and for every round $t$ where $x_t \in I_j$, the signal $\sigma_t$ is uncorrupted. 
\item An interval is a \emph{safe interval} if its parent interval is safe or correcting (or itself is a root interval), and for any round $t$ where $x_t\in Y_j$, the signal $\sigma_t$ is uncorrupted. %A root interval with no corruption is also \emph{safe}. 
\end{itemize}
\end{definition}

I introduce the \textit{agnostic checking} measure to combat adversarial corruptions. Consider a new interval $I_j$ that has been formed by bisecting its parent interval, and that a context $x_t$ appears in this interval $I_j$. The learner will first query the two endpoints of $Y_j$ and test whether $f(I_j)\subset Y_j$ holds (according to the possibly corrupted signals). The interval passes the agnostic check if according to the (possibly corrupted) signals $f(I_j) \subset Y_j$. If the interval does not pass the agnostic check, it is marked as \emph{unsafe}, and the learner resets $Y_j$ to $[0, L]$ and effectively searches from scratch for the associated range. 

The main theorem is stated below. 

\begin{theorem}
\label{thm:symm1D}
\Cref{algo:1dabsolute} incurs $L\cdot O(C \cdot \log T)$ cumulative absolute loss for $d = 1$. 
\end{theorem}

\begin{proof}[Proof Sketch]
The proof consists of several steps. 
\paragraph{Step 1.} An interval not marked unsafe bisects in $O(1)$ rounds. This is because the associated range is shrinking by a constant after each query and after $O(1)$ rounds the associated range will have shrunk enough and meet the criteria for bisecting the interval. By a similar logic, any interval marked unsafe bisects in $O(\log T)$ rounds. 
\paragraph{Step 2.} Consider any \emph{correcting} interval. After the agnostic checking steps, the associated range must contain the true range of the function on the interval. The associated range will then be accurate when the interval is bisected. Then consider any \emph{safe} interval. Its parent must be safe or correcting, and by induction, a \emph{safe} interval will not be marked \emph{unsafe}. 
\paragraph{Step 3.} There can be at most $O(C)$ \emph{corrupted} intervals. Since any \emph{correcting} interval has a \emph{corrupted} interval as a parent, there can be at most $O(C)$ correcting intervals. A total of $O(\log T)$ queries can occur on corrupted and correcting intervals, contributing loss $O(C\log T)$ (actually one can show correcting intervals contribute loss at most $O(C)$). For \emph{safe} intervals, a loss of magnitude $O(2^{-h})$ can occur at most $O(2^h)$ times (since there are $O(2^h)$ intervals at depth $h$), thus the total loss from safe intervals can be bounded by $O(\log T)$. 

Putting everything together gives the total $L\cdot O(C\cdot \log T)$ regret bound. 
\end{proof}

\begin{remark}
The regret is tight up to $\log T$ factors. To see this, consider the first $C$ rounds, where the adversary corrupts the signal with probability $1/2$ each round. The learner essentially receives no information during this period, and each round incurs regret $\Omega(L)$. 
\end{remark}

% A detailed analysis appears in~\cref{app:proof1d}. The main components of the proof include the following. 
% \begin{enumerate}
%     \item Since any corrupted interval is bisected in $O(\log T)$ rounds, the total loss from corrupted interval can be bounded as $O(C\log T)$. 
%     \item There are at most $O(C)$ amending intervals, and each amending interval contribute $O(1)$ loss. 
%     \item A safe internval at depth $h$ contribute loss $O(2^{-h})$. There are at most $O(2^h)$ intervals at depth $h$, hence a loss with $O(2^{-h})$ can be charged at most $O(2^h)$ times. 
% \end{enumerate}


\begin{remark}
    An interesting aspect of this algorithm is that the learner remains agnostic to the type of each interval. In other words, during the run of the algorithm, the learner will not know for certain which type an interval belongs to. The analysis makes use of the three types of the interval, not the algorithm itself. 
\end{remark}


% If the signal at these two endpoints tells the learner $f(I_j) \notin Y_j$, then the learner will know either the signal at these two endpoints are corrupted, or that corruptions in its parent interval lead to an inaccurate $Y_j$ that failed to contain the image of $I_j$. In either case, the learner marks the interval $I_j$ as an unsafe interval, resets the associated range $Y_j$ to $[0,1]$, and thus effectively start from scratch and searches for the associated range $Y_j$ of the interval $I_j$. 

% If the signal at these two endpoints tells the learner that $Y_j$ is indeed an accurate upper bound for $f(I_j)$, then the interval is not marked unsafe. However, it should be noted that, even if the interval is not marked unsafe, the learner still cannot necessarily ensure that $Y_j$ contains the image $f(I_j)$, since the signal at these two endpoints may be corrupted. The learner has no way of verifying whether the feedback was accurate, and will act as if $Y_j$ is indeed an upper bound on the image of $f(I_j)$, and apply the midpoint querying strategy to refine the estimate $Y_j$. If $Y_j$ was indeed an accurate upper bound on the $f(I_j)$ and signals on the interval $I_j$ were accurate, then the learner will incur small loss on this interval. Otherwise if $Y_j$ was not an accurate upper bound on $f(I_j)$, then it must mean that the signal when querying the endpoints were corrupted, and hence the learner will incur regret as a consequence of the corrupted signals. However we shall see that the total loss incured as a result of the corrupted signals can be bounded above by $\tilde{O}(C)$, where recall $C$ is the number of corruptions. 

%The learner starts with a coarse partition of the input space into intervals, and for each interval $I_j$ maintains a interval $Y_j$ that contains the image of this interval. As the feasible interval $Y_j$ shrinks enough, the interval $I_j$ is split in half, thus allowing for finer estimates of the image of the child intervals. 


% \subsection{Analysis}
% A detailed analysis of algorithm~\cref{algo:1dabsolute} is given. Some definitions are first introduced that will be helpful in the analysis. 

% \begin{definition}[Depth]
%     The intervals at initialization has depth 1, and each interval has depth increased by 1 when split. 
% \end{definition}
% The depth of an interval measures how many splits happened before reaching the current interval. 

% Next, three types of intervals are introduced. 
% \begin{definition}[Corrupted Interval]
%     An interval is called a corrupted interval if any round inside the interval is corrupted. 
% \end{definition}

% \begin{definition}[Correcting Interval]
%     An interval is called a correcting interval if any round inside the interval is uncorrupted and the parent interval is corrupted. 
% \end{definition}

% \begin{definition}[Honest Interval]
%     An interval is called a honest interval if any round inside the interval is uncorrupted and the parent interval is either a correction interval or a honest interval. As a special case, intervals at depth 1 are honest if they are not a corrupted interval. 
% \end{definition}

% It can be seen that these three types of intervals are disjoint, and their union is the set of all intervals reached by the learner in the learning process. 

% We begin with the following lemmas. 

% \begin{restatable}{lemma}{markedAreNotHonest}
%     If interval marked dishonest, then must be correcting or corrupted. 
% \end{restatable}

% \begin{restatable}{lemma}{markedSplits}
%     If an interval marked dishonest, then splits in $O(\log T)$ rounds. If interval not marked dishonest, then splits in $O(1)$ rounds. 
% \end{restatable}

% \begin{restatable}{lemma}{correctionCorrects}
%     Let $I_j$ be a correction interval and $Y_j$ be its feasible interval before $I_j$ is split. Then $f(I_j) \in Y_j$. 
% \end{restatable}

% \begin{restatable}{lemma}{corruptedLoss}
%     Corrupted interval contribute $O(C \log T)$ loss. 
% \end{restatable}

% \begin{restatable}{lemma}{correctingLoss}
%     Correcting interval contribute $O(C)$ loss. 
% \end{restatable}


% \begin{restatable}{lemma}{honestLoss}
% Consider an honest interval in depth $h$. This interval is split within $O(1)$ rounds, and each round incur $O(2^{-h})$ regret. 
% \end{restatable}


\szcomment{Optimal 1d lipschitz? }
\szcomment{1/2, 1/2, 3/4, 3/4, 15/16, 15/16, 35/32, 35/32, 315/256}

\subsection{Extending to $d > 1$}
\Cref{algo:1dabsolute} can be extended in a straightforward way to accommodate the case $d > 1$. The full algorithm and analysis is given in~\cref{app:sym-highd}. The only difference is that the learner maintains $d$-dimensional hypercubes instead of intervals. The main theoretical result is as follows. 
\begin{theorem}
\label{thm:symmMD}
    There exists an algorithm that incurs $L\cdot O_d(C\log T +  T^{(d-1) / d)})$ cumulative absolute loss for $d\ge 2$. 
\end{theorem}

\begin{remark}
In~\cite{mao2018contextual}, it was shown the optimal regret when $C=0$ is $\Omega(T^{(d-1)/(d)})$. Hence, the dependence on $C$ and $T$ are both optimal in the above theorem (up to $\log T$ factors). 
\end{remark}
