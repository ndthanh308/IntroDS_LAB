\section{Algorithm for Pricing Loss}
\label{sec:pricing}

\begin{algorithm2e}[htb]
\caption{Learning with corrupted binary signal under pricing loss with uniform discretization}
\label{algo:pricing}
Set parameter $\eta_0 = T^{-1/(d+1)}$, agnostic check schedule $\tau_0$\;
Learner uniformly discretizes input space into hypercubes with lengths no larger than $\eta_0$ each\;
For each hypercube $I_j$, learner maintains an associated range $Y_j$ (initialized as $[0, L]$), query count $c_j$ (initialized as $0$)\;
% \State Set $\eta = 10 L \cdot T^{1/(d+1)}$
\For {$t = 1, 2, ..., T$} {
    Learner receives context $x_t$\;
    Learner finds hypercube $I_j$ such that $x_t \in I_j$\;
    Let $Y_j$ be the associated range of $I_j$\;
    \If {$\len (Y_j) < 10 L \cdot \eta$} { %\Comment{The interval is pricing-ready}
        $c_j := c_j + 1$\;
        \If { $c_j > \tau_0$ } {
            Query $\max(Y_j)$\;
            Set $c_j := 0$\;
        }
        \Else { 
            Query $\min(Y_j)$\;
        }
        \If {Learner is surprised (see Definition \ref{def:surprises})}{  %\LineComment{Learner is surprised if queried $\max(Y_j)$ and $\sigma_t = 0$, or queried $\min(Y_j)$ and $\sigma_t = 1$}
            Set $Y_j := [0,L]$\;%\Comment{Reset the range of $Y_j$}\;
            Set $c_j := 0$\;
        }
    }
    % \ElsIf {Exists an endpoint in $S_j$ not queried} %\label{algo:queryendpt_beg}
    %     \State Query an unqueried endpoint of $S_j$
    %     \If {Queried $\min(S_j)$ and $\sigma_t = 1$, or queried $\max(S_j)$ and $\sigma_t = 0$} 
    %     \State Mark $I_j$ as unsafe \Comment{Contamination found in the current interval}
    %     \EndIf
    %     \If {Both endpoints of $S_j$ have been queried}
    %         \If {$I_j$ marked unsafe}
    %             \State Set range $Y_j := [0,1]$ \label{algoline:marked_rangeset}
    %         \Else
    %             \State Set range $Y_j = [\min(S_j) - L\cdot \len(I_j)), \max(S_j) + L\cdot \len(I_j))] \cap [0,1]$ \label{algoline:unmarked_rangeset}
    %         \EndIf
    %     \EndIf %\label{algo:queryendpt_end}
    \Else {%{$I_t$ not marked unsafe} \label{algo:notmarked_beg}
        $Y_j := \algmq(I_j, Y_j)$\;
        % \If { $\len(Y_j) < 4 L\cdot \len(I_j)$}  \Comment{associated range $Y_j$ has shrunk enough}
        %     \State Bisect each side of $I_j$ to form $2^d$ new hypercubes, each with length $...$
        % \EndIf %\label{algo:notmarked_end}
    }
    %\Else \Comment{$I_t$ is marked dishonest} \label{algo:dishonest_begin}
        %\State Start from scratch until converge, takes $\log T$ steps \label{algo:test}
        
        %\State Let $\set{[], [], []}$ be the sequence of feasible region (endpoints? ) on the path to $I_t$. 
        %\State Perform binary search on this path and find the first interval [] that contains $f(I_t)$. \Comment{Takes $\log\log T$ steps}, set feasible region $I_t = []$
        %\State From this point perform binary search and shrink feasible region \Comment{Takes $c$ steps, where $c$ is number of corruptions before reaching a correction interval? }
    %\EndIf %\label{algo:dishonest_end}
}
\end{algorithm2e}


This section discusses the new ideas needed to design corruption-robust algorithm for the pricing loss. The description shall be given in the context of dynamic pricing, and the learner shall be referred to as the seller in this section. The main algorithm is summarized in~\cref{algo:pricing}. Note that for the pricing loss, the case with $d =1$ and $d > 1$ are treated together. 

Extending~\cref{algo:1dabsolute} for the absolute loss to pricing loss is not straightforward, since agnostic checks will overprice and the seller necessarily incurs a large loss whenever she overprices. The learner did not have this problem with absolute loss, since the absolute loss is continuous. 

\subsection{Algorithm for $C = 0$}
I first give the description of an algorithm for $C = 0$. The algorithm is adapted from \cite{mao2018contextual} but uses uniform discretization. The input space is discretized into hypercubes with length $\eta_0 = T^{-1/(d+1)}$. When a context appears in some hypercube $I_j$, one of two things can happen: 
\begin{enumerate}
    \item If the associated range is larger than $10L\cdot \eta_0$, the learner performs $\algmq$ and updates the associated range. There can be at most ${O}(\eta_0^d \log T)$ rounds of this type, since for each hypercube the associated range will shrink below $L\cdot O(\eta_0)$ after $O(\log T)$ queries. 
    \item Otherwise, the associated range is less than $10L\cdot \eta_0$, and the learner can directly set the lower end of the interval as the price. The total loss from these rounds can be bounded as $L\cdot O(T\eta_0)$. 
\end{enumerate}
The total loss is then $L (\eta_0^d \log T) + L(T\eta_0) = L\cdot {O}(T^{d/(d+1)} \log T)$. 

\szdelete{
The learner searches for the associated range $Y_j$ of a hypercube using the midpoint query procedure $\algmq$ (these shall be termed \emph{searching rounds} or \emph{searching queries}) and stops the search process when the associated range $Y_j$ becomes small enough (specifically, when the length of $Y_j$ drops below $10L\cdot \eta_0$). The hypercube is said to become \emph{pricing-ready} when this happens. When there are no adversarial corruptions, the learner now has a good estimate of the optimal price and sets the lower end of $Y_j$ as the price for any context in this hypercube. These shall be termed \emph{pricing rounds}. At \emph{pricing rounds}, the seller expects the buyer to purchase the item, and assuming the range $Y_j$ is accurate (which is the case when $C = 0$), the revenue loss should be no larger than $10 L \cdot \eta$. The total loss from \emph{pricing rounds} can be bounded by $L\cdot O(T\cdot \eta_0)$. The total loss from searching rounds can be bounded by $L\cdot  \widetilde{O}(\eta_0^{-d})$ since there are $O(\eta_0^{-d})$ hypercubes, and for each hypercube, there can be at most $O(\log T)$ queries before the hypercube becomes \emph{pricing-ready}. By the choice of parameter $\eta_0 = T^{-1/(d+1)}$, the regret bound is $L\cdot \widetilde{O} (T^{d/(d+1)})$. }

\szdelete{
It should be noted that~\cite{mao2018contextual} proposed an algorithm based on adaptive discretization whereas the above algorithm is based on uniform discretization. Using adaptive discretization improves the regret bound by a $O(\log T)$ factor, the presentation for the corruption-robust algorithm for pricing loss is based on uniform discretization, as it highlights the new algorithmic ideas more clearly. Nevertheless, the algorithm can be combined with adaptive discretization by using ideas from the previous section. }

\szcomment{TODO. Compare with adaptive discretization. }\szcomment{DONE}

\subsection{Corruption-Robust Pricing with Scheduled Agnostic Checks}

The seller could potentially run into issues when there are adversarial corruptions. The adversary can manipulate the seller into underpricing by a large margin by only corrupting a small number of signals. Consider the following example. The buyer is willing to pay $0.5$ for an item with context $x$. The adversary can manipulate the signals in the $\algmq$ procedure so that the seller's associated range for $x$ is $[0, 10L\cdot \eta]$, which does not contain and is well below the optimal price $0.5$ for this item. The seller then posts a price of $0$ for item $x$. Even though the buyer purchases the item at a price of $0$, the seller is losing 0.5 revenue per round, and this happens with the adversary corrupting only $O(\log T)$ rounds. \szcomment{TODO, rephrase}

\szcomment{ $Y_j$ is not a valid pricing range, and even though she prices the item at the lower end of $Y_j$ and the buyer is purchasing the item, the true valuation could be much higher and the seller is losing a large amount of revenue. }

To combat adversarial corruptions, the learner performs agnostic check queries. These queries differ from agnostic checks for the absolute loss in the following two aspects. First, whereas for the absolute loss, the learner performs agnostic check queries on both ends of the associated range $Y_j$, for the pricing loss the seller only performs agnostic checks on the upper end of $Y_j$. This ensures the seller is not underpricing the product by a large margin. Second, the seller only performs agnostic check queries when the associated range of the hypercube is sufficiently small (below $L\cdot O(\eta_0)$). The seller expects the buyer not the purchase the item during agnostic check queries. 

The learner will set a schedule for performing agnostic checks. At a high level, performing agnostic checks too often incurs large regret from overpricing, while performing agnostic checks too few may not detect corruptions effectively. The schedule for agnostic checks serves as a mean to balance the two. This schedule informs the learner how often to perform agnostic check so as not to incur large regret when overpricing, and at the same time control the loss incurred from corrupted signals. Specifically, the learner keeps track of how many rounds the context vectors arrived in each hypercube. The learner then performs an agnostic check every $\tau_0$ rounds. Here, $\tau_0$ will be a paramter to be chosen later. 

Some notions are introduced. 
\begin{definition} [Pricing-ready hypercubes]
For hypercube $I_j$, if the length of the associated range is below $10L\cdot \eta_0$, then the hypercube is \emph{pricing-ready}. 
\end{definition}

\begin{definition} [Pricing round, checking round, searching round]
If a hypercube is \emph{pricing-ready}, then setting the price as the lower end of the associated range is termed \emph{pricing round}, and setting the price as the upper end of the associated range is termed \emph{checking round}. If a hypercube is not \emph{pricing-ready}, then performing a $\algmq$ is termed \emph{searching round}. 
\end{definition}

\begin{definition}[Surprises]
\label{def:surprises}
    The seller is said to become \emph{surprised} when the signal she receives is inconsistent with her current knowledge. Specifically, the seller becomes surprised when either: she performs a checking round but observes an underprice signal $(\sigma_t = 0)$; or she performs a pricing round but observes an overprice signal $(\sigma_t = 1)$. 
\end{definition}

\begin{definition} [Runs]
Fix some hypercube $I_j$, the set of queries that occurs on $I_j$ before the learner becomes \emph{surprised} (or before the algorithm terminates) called a \emph{run}. 
\end{definition}
When a run ends, the learner resets the associated range and a new run begins. Note there can be multiple runs on the same hypercube. 


The algorithm gives the following cumulative loss bound. A detailed analysis can be found in~\cref{app:pricing}. 
\begin{theorem}
\label{thm:pricing}
Using~\cref{algo:pricing}, the learner incurs a total loss
\[
L\cdot O(C\log T + T^{d/(d+1)}\log T + C\tau_0 + T/\tau_0)
\]
for any unknown corruption budget $C$. 
Specifically, setting $\tau_0 = T^{1/(d+1)}$, the learner incurs total loss
\[
L\cdot O(T^{d/(d+1)}\log T + C\log T + C\cdot T^{1/(d+1)}) = L\cdot \widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)} ). 
\]
\end{theorem}

\begin{proof}[Proof Sketch]
The proof consists of several steps. 
\paragraph{Step 1.} The seller can only become surprised when at least one corruption occurs on the current `run' of some hypercube. Thus the seller can become surprised at most $C$ times. 
\paragraph{Step 2. }For searching rounds, the total loss can be bounded as $L\cdot O( (C + \eta_0^{-d}) \log T)$. There can be at most $O(T/\tau_0)$ checking rounds, contributing loss $L\cdot O(T /\tau_0)$. 
\paragraph{Step 3. } For pricing rounds, the total loss can be bounded as $L\cdot O(C \tau_0 + T \eta_0)$. At a very high level, if the pricing interval is accurate (meaning the lower endpoint is just below the true price by a margin of $L\cdot O(\tau_0)$), then these rounds contribute loss at most $O(T \eta_0)$. Otherwise, the seller can detect if the pricing interval is inaccurate by performing agnostic checks, and the loss from these rounds can be bound by $O(C \tau_0)$. 

Putting everything together completes the proof. 
\end{proof}

% \begin{remark}
% The loss is sublinear as long as $C = o(T^{d/(d+1)})$. In~\cite{mao2018contextual}, it was shown the optimal regret is $\Omega(T^{d/(d+1)})$ for $C = 0$. Hence, the above theorem is optimal when $C = O(T^{ (d-1)/(d+1) })$. 
% \end{remark}


If the corruption budget $C$ is known, the learner can set $\tau_0$ to balance the terms and achieve a sharper regret bound. 

% \begin{corollary}
%     Choose some parameter $C_0$ and set $\tau_0 = ??$. If $C < C_0$, the algorithms incurs total loss, if $C > C_0$, the algorithm incurs total loss $O(T)$. Specifically, setting $C_0 = T^{(d-2)/(d+1)}$, and the algorithm achieves xx loss when. 
% \end{corollary}

\begin{corollary}
\label{cor:pricingKnownC}
Assume the corruption budget $C$ is known to the learner. Using~\cref{algo:pricing} with $\tau_0 = \sqrt{T/C}$, the learner incurs total loss
\[
L\cdot \widetilde{O} (T^{d/(d+1)} + \sqrt{TC}). 
\]
\end{corollary}

\subsection{Lower Bounds}

This subsection shows two lower bounds for either known or unknown $C$, showing the upper bounds in the previous subsection are essentially tight. Define an environment as the tuple $(f, C, \mathcal{S})$, representing the function, corruption budget, and corruption strategy of the adversary respectively. In the following assume $L = 1$, the scaling to other $L$ is straightforward. 

\begin{theorem}
Let $A$ be any algorithm to which the corruption budget $C$ is unknown. Suppose $A$ achieves a cumulative pricing loss $R(T) = o(T)$ when $C = 0$. Then, there exists some corruption strategy with $C = 2R(T)$, such that the algorithm suffers $\Omega(T)$ regret. 
\end{theorem}
\begin{proof}
Let us consider two environments. In the first environment, the adversary chooses $f(x)\equiv 0.5$ and never corrupts the signal. In the second environment, the adversary chooses $f(x)\equiv 1$ and corrupts the signal whenever the query is above 0.5. 

Now, since the algorithm achieves regret $R(T)$ when $C=0$, then when facing the first environment, the seller can only query values above $0.5$ for at most $2R(T)$ times. However, by choosing $C = 2R(T)$, the adversary can make the two environments indistinguishable from the seller. Hence the seller necessarily incurs $\Omega(T)$ regret in the second environment. 
\end{proof}
\begin{remark}
The above lower bound shows one cannot hope to achieve a regret bound of the form $O(T^{(d/(d+1))}) + C\cdot o(T^{1/(d+1)})$ when $C$ is unknown. First note any algorithm must achieve $\Omega(T^{d/(d+1)})$ when $C=0$ by the lower bound in \cite{mao2018contextual}. Then, suppose the algorithm could achieve $O(T^{d/(d+1)})$ when $C=0$, the algorithm must achieve $\Omega(T)$ regret for some $C = \Theta(T^{d/(d+1)})$. 
\end{remark}

%effectively manipulating the seller into believing the true price is 0.5. 

Next, we will prove a lower bound when corruption budget $C$ is {known}. The lower bound will be against a randomized adversary, who draws an environment from some probability distribution $\cD$, and the corruption budget is defined as the expected value of $C$ under distribution $\cD$. Note that the upper bounds (\Cref{thm:pricing}, Corollary \ref{cor:pricingKnownC}) also hold for randomized adversaries. 
\begin{theorem}
There exists some $\cD$ where any algorithm incurs expected regret $\Omega(\sqrt{CT})$, even with complete knowledge of $\cD$. 
\end{theorem}
\begin{proof}
The adversary uses two environments. In the first environment, the function $f(x) \equiv 0.5$ and the corruption budget is 0. In the second environment, the function $f(x) \equiv 1$, the corruption budget is $C_0$, and the adversary corrupts any query above 0.5 until the budget is depleted. The adversary chooses the first environment with probability $1-p$ and the second environment with probability $p$. 

If the learner's algorithm queried more than $C_0$ rounds above $0.5$, then the learner incurs regret $0.5C_0$ in the first environment, giving expected regret $\Omega((1-p) C_0)$. If the algorithm did not query more than $C_0$ rounds above $0.5$, then the algorithm incurs regret $0.5(T - C_0)$ in the second environment, giving expected regret $\Omega((T-C_0)p)$. Choosing $p = \sqrt{C/T}, C_0 = \sqrt{CT}$ completes the proof. 
% Consider the following corruption strategy. The space is discretized into $\sqrt{T/C}$ hypercubes of equal length. The context that the adversary selects will be the center of each hypercube, and the value at each context will be 0.5 except for one context which has value $0.5 + \eps$. Each hypercube will be selected by the adversary for $\sqrt{CT}$ rounds. The adversary will corrupt the first $C$ queries in the hypercube which has value $0.5+\eps$. At a high level, the learner can only detect the hypercube with value $0.5 + \eps$ if he performs checking rounds in every hypercube for at least $C$ rounds. Alternatively, if the learner does not perform checking query, he will incur regret $\eps \sqrt{CT}$. 
% Can we choose there to be $1/\eps = \sqrt{T/C}^{1/d}$ hypercubes that has value $0.5+\eps$? 
\end{proof}



% \subsection{Lower Bound}
% \szcomment{Comment: Different approach to state the upper bound. }

% It is natural to wonder whether the dependence on $C$ in the above theorem is tight. That is, whether one can achieve a bound of the form:
% \[
% \widetilde{O}(T^{d/(d+1)} + C)
% \]

% The below lower bound shows this is not possible. 

% \begin{theorem}
% Assume $L = 1$. Let $A$ be any algorithm to which the corruption budget $C$ is unknown. Suppose $A$ achieves a cumulative pricing loss $R(T) = o(T)$ when $C = 0$. Then, there exists some $C = o(T)$, such that the algorithm suffers $\Omega(T)$ regret. 
% \end{theorem}

% \begin{proof}
% Let us consider two environments. In the first environment, the adversary chooses $f(x)=0.5$. In the second environment, the adversary chooses $f(x)=1$. 
	
% The adversary adopts the following strategy. In the first environment where $f(x) = 0.5$, the adversary never corrupts the signal. In the second environment where $f(x) = 1$, the adversary corrupts the signal whenever the seller queries a point above $0.5$. Hence, the adversary manipulates the seller into thinking the true price is 0.5. 

% Now we know the algorithm achieves regret $R(T)$ when $C=0$. Then, when $f(x) = 0.5$, the seller can only query values above $0.5$ for at most $2R(T)$ times. However, by choosing $C = 2R(T)$, the adversary can make the two environments indistinguishable to the seller. Hence the seller necessarily incurs $\Omega(T)$ regret. 
% \end{proof}

