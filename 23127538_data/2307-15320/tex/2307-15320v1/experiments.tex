\section{Experimental Results}
\vspace{-.1cm}
\label{sec:expr}

\input{figures/robotic_setup}


\subsection{Experimental Setup}
\label{sec:experimental_setup}

Our robotic platform is a UR5 6-DoF robotic arm equipped with a two-finger Robotiq RG6 gripper, see Figure~\ref{fig:experimental_setup}.
Input RGB images are captured with two Intel RealSense D435 cameras located in the front and the left side of the robotic arm.
We perform control and obtain camera measurements together with proprioceptive information at the frequency of 10 Hz. 
We use the MuJoCo engine \cite{todorov12} to perform physics simulation and rendering of the robotic environment. To align real and simulated images, we carry out extrinsic camera calibration using AprilTag \cite{olson11} and we use the intrinsic parameters provided by the RealSense camera. 
Images are resized and cropped into $180 \times 240$ pixels. 
We define the gripper workspace as a volume of $40\times40\times20 \text{ cm}^{3}$. The gripper initial position is sampled from the set of possible positions inside this workspace while the initial object positions are sampled randomly inside this same workspace and on  the robot table.

\noindent \textbf{Evaluation under real-world variations.}
We evaluate the robustness of our models to natural variations in visual appearance caused by changes in surface textures, lighting and camera poses.  
We consider five scenarios by: (i) adding a textured table cloth, (ii) decreasing the intensity of the room lighting, (iii) adding a variable color lighting with two Olafus $50 \text{W}$ RGB LED flood lights, (iv) offseting object colors and (v) adding noise to the camera pose. Figure~\ref{fig:robustness_setup} illustrates the default scenario as well as the five variations. 


\subsection{Implementation details}

\noindent\textbf{Cube localization proxy task.} To evaluate the precision of cube localization, we use three cubes with side length of $5\text{cm}$ and colored green, red and yellow respectively. We collect a real dataset for the proxy task where we sample random positions for the cubes and the gripper inside the environment workspace. 
We repeat this 250 times and vary the viewing conditions, such as background texture, lightening and camera viewpoint, see Figure~\ref{fig:robustness_setup}. 
While in simulation we know the cubes and gripper position, on the real robot we do not have direct access to this information. 
To overcome this limitation, we manually place the cubes in known positions, and the gripper moves each cube to a random position. Finally, the gripper moves to a random position, and we use the robot forward kinematics at each step to create the ground truth. 
We train our model for $400,000$ gradient steps using the AdamW optimizer \cite{loshchilov18}. We use a cosine scheduler with initial learning rate of $3 \times 10^{-4}$ and the minimum learning rate of $1 \times 10^{-6}$.


% 

\noindent\textbf{Policy learning.} 
We optimize robotic manipulation policies using a batch size of $32$. 
For the stacking, pushing, pushing-to-pick and box-retrieving tasks we perform $400,000$ gradient steps while the more difficult tasks of assembling, rope-shaping and sweeping are optimized for $1\text{M}$ steps. 
We evaluate the performance of our models on $250$ episodes in simulation with domain randomization disabled, and run $20$ trials for each model and task when performing real robot evaluation. 
We experimentally set the value of $\lambda$ in (\ref{eqn:loss}) to $0.8$ during training and train our policy models with the AdamW optimizer~\cite{loshchilov18} and a cosine scheduler.

\input{figures/real_data_setups}

\input{tables/policy_ablation_complete}

\input{figures/dr_param_opt.tex}


\subsection{Policy learning in simulation}
\label{sec:policy_sim}

In this section, we evaluate the design choices of our policy network in simulation for all seven tasks, see 
Table~\ref{table:policy_ablation}. We train our models on 2000 (4000) demonstrations in simulation using DR with the parameters defined in Section~\ref{sec:dr_ablation} and evaluate them in simulation without any augmentation.
The baseline takes as input a single viewpoint and a single frame and reaches 44.97\% success rate on average.
Adding a second viewpoint improves the performance to $65.43\%$ on average and results in $50.4\%$ absolute gain for the assembling task which requires high precision. 
Adding a short-term history by taking as input three frames results in a further improvement of $28.51\%$ on average. 
Finally, we add proprioceptive information to the input, which further improves the performance by $4.4\%$ and leads to the best performance of $98.34\%$ success rate.
Propioceptive information is essential for tasks such as sweeping (+$18\%$) where a high precision of the gripper position is required. 

\subsection{Domain randomization ablation with a proxy task}
\label{sec:dr_ablation}


In this section, we first determine the best domain randomization (DR) parameters for each augmentation type on the proxy localization task.
Figure~\ref{fig:texture_study} compares the two different texture types and shows that ambientCG textures are much more effective than the procedurally generated ones. As illustrated in Figure~\ref{fig:textures_comp}, ambientGC textures are of high quality and have richer variations than procedurally generated textures which could explain this difference in error.
In the following, we always use ambientCG texture randomization.
For lighting randomization, Figure~\ref{fig:light_study} shows that performance is not very sensitive to different randomization ranges for light ambiance, diffuse and specular coefficients. Hence, we select an intermediate value randomizing the previous three coefficients in the range $[0, 0.6]$. 
For the object colors, HSV color offset has a significant impact as shown in Figure~\ref{fig:color_study}. 
A large offset significantly deteriorates the performance as it confuses colors of different objects. If the offset is too small, the model is not robust enough. We use an offset around the object nominal color of $\phi_o=(0.05, 0.1, 0.1)$ that achieves the best performance.
Finally, as shown in Figure~\ref{fig:cam_study} a small randomization of camera parameters is important due to possible calibration errors and too much variation hurts the performance. We offset the values for camera position by $[-10, 10]\text{ cm}$, for camera angle by $[-0.05, 0.05]\text{ rad}$ and for field of view by $[-1.0, 1.0]^{\circ}$.

\input{tables/dr_ablation}


\input{tables/sim_real_tasks}



Table \ref{table:dr_ablation} compares combinations of different DR types using the selected parameters. We evaluate both the error on the proxy localization task for the default scene appearance and the average error of all the varied appearance scenes, see Figure~\ref{fig:robustness_setup}.
In addition, we compare DR with the standard 2D image augmentation; we follow \cite{lee21} and modify synthetic images using a random offset $[-0.12, 0.12]$ for brightness, $[-0.05, 0.05]$ for hue, $[0.5, 1.5]$ for saturation, $[0.5, 1.5]$ for contrast and $[-4, 4]$ pixels for horizontal and vertical image translation. 

The first row shows the results for training with 20k synthetic images. We can observe that by adding image augmentations the performance improves only very slightly. 
Texture randomization significantly improves the performance and decrease the error from $7.55$ to $2.52 \text{ cm}$.
Adding lighting randomization does not improve performance, i.e., the error remains similar. 
With the variation of object colors, we further improve the error by $1\text{ cm}$ and achieve an error of $1.62 \text{ cm}$.
Varying the camera parameters additionally improves performance to $1.33 \text{ cm}$. 
Further adding 2D image augmentations has a minor impact on the error and decreases it to $0.95 \text{ cm}$. 

Since we are using synthetic data for training, we can easily generate additional data.  We show a large boost in performance when increasing the dataset from $20,000$ to $100,000$ images, resulting in an error of $0.48 \text{ cm}$. Notably, our model trained with DR and simulation-only data outperforms the model trained on real but limited data, and is also more robust to variations in the scene appearance. The error for such data with variations (last column of Table~\ref{table:dr_ablation} is $1.39 \text{ cm}$ 
 for synthetic training data with DR in contrast to $3.08 \text{ cm}$ obtained when training with real data.
Finally, we show that additional fine-tuning of our model on real data is beneficial, i.e., the error decreases to  $0.14 \text{ cm}$.

\subsection{Ablation of domain randomization for policy learning}
\label{sec:policy_real}


In this section, we measure the impact of domain randomization (DR) on policy learning beyond the proxy task.
We train policies with our DR approach on a suite of seven manipulation tasks described in Section~\ref{sec:tasks}.
The same DR parameters selected for the cube localization proxy task are used for all the manipulation tasks.

Table~\ref{table:policy_success} presents the results. We compare our full DR approach with three variations of Table~\ref{table:dr_ablation}. The first variation uses only 2D image augmentation; the second one adds ambientCG texture randomization to the 2D image augmentation; and the third one includes lighting and object color randomization on top of the previous variation. 
Using only 2D image augmentation is not enough to succeed in any task on the real robot, i.e., the success rate is 0. 
Adding ACG texture randomization helps transferring the policy with an average success rate of $11.3/20$. Additionally adding light and object color randomization improves the performance further and results in an average success rate of $14.0/20$. 
Adding small camera variations to the training further improves the performance, i.e.,  our full DR setup obtains a success rate of $18.6/20$ on average. 
When comparing results in Tables~\ref{table:dr_ablation} and~\ref{table:policy_success}, we can see that \emph{the performance of the proxy task is well aligned with the performance of policies for all considered manipulation tasks on the real robot}. 
This validates the effectiveness of our proxy task for choosing DR parameters for the visual sim-to-real policy transfer. We illustrate successful runs of our policies on the real robot in Figure~\ref{fig:tasks_real}. 
Additional qualitative results for all seven manipulation tasks are presented in the supplementary video. 

\input{figures/tasks_real.tex}
\input{figures/tasks_real_failure.tex}



\noindent \textbf{Failure cases analysis.}
 Figure~\ref{fig:tasks_real_failure} illustrates two type of failure cases. The first type of failure (top row) is due to unseen states. During pushing the cube rotates by 90 degrees, a state never observed in training. This causes the gripper to leave the pushing surface and continue the pushing to the target place without the cube.
The second one originates from the physical sim-to-real gap (bottom row). During assembling the gripper grasps the nut by the end of the handle and the nut rotates due to its weight and the lack of sufficient friction between the gripper fingers and the nut. This issue does not occur in simulation due to the insufficient realism in friction modeling. 




\subsection{Robustness of policies to visual variations}

In this section, we evaluate the robustness to visual variations for the stacking task on the real robot. Table~\ref{table:robustness_policy} compares  our policy trained with DR, a policy trained on real data and a policy trained on DR and fine-tuned on real data for the different variations described in Section~\ref{sec:experimental_setup}. 
We use $2,000$ demonstrations in simulation and $150$ real robot demonstrations. While simulation allows us to gather $2000$ demonstrations in minutes, gathering $150$ demonstrations on the real robot is time-consuming (approx.~4.5 hours).

\input{tables/robustness}


In the default setting without visual appearance variation of real scenes, all the policies achieve $100\%$ success rate. However, our DR policy exhibits much stronger robustness to changes in the table texture and achieves $100\%$ success rate compared to $5.0\%$ success rate obtained by the real data policy. 
Similarly, variations of the lighting conditions caused by lowering light intensity and changing light color result in larger decrease of performance for policies trained on the real data compared to our policies. 
When we modify cube colors, we can see that both the DR and real data policy perform well achieving a success rate of $90.0 \%$ and $95.0 \%$ respectively.
Finally, the variation of camera parameters results in a large degradation of both policies.
Overall we conclude that policies trained on synthetic data with DR are more robust compared to policies trained on the real data, when such data is available in limited amounts. Additionally, using the real data to fine-tune the policies trained with DR results in an additional increase of the average success rate of $+4.5 \%$.


\vspace{-.1cm}