\section{Approach}
Our goal is to train robust visuomotor policies in simulation and to deploy them  to manipulation tasks on a real robot.
We first describe our visual policies in Section~\ref{sec:bc} and introduce domain randomization for visual sim-to-real transfer in Section~\ref{sec:dr}. We then present our approach for selecting parameters of domain randomization using a proxy task in Section~\ref{sec:proxy_task}.

\input{figures/model_arch}

\vspace{-.1cm}

\subsection{Visuomotor policies for robotic manipulation}
\label{sec:bc}

\noindent\textbf{Problem formulation.}
We learn a policy $\pi_{\theta}(\mbf{a}_{t}\,|\,\mbf{o}_{t})$ that maps environment observation $\mbf{o}_{t}$ to a robot action $\mbf{a}_{t}$ at  timestep $t$, see Figure~\ref{fig:model_arch}.
The action $\mbf{a}_{t}=(\mbf{v}_{t}, \boldsymbol{\omega}_{t}, g_{t})$ is composed of the linear velocity $\mbf{v}_{t} \in \mathbb{R}^{3}$, the angular velocity $\boldsymbol{\omega}_{t} \in  \mathbb{R}^{3}$ of the robot end-effector and the gripper openness state $g_{t} \in \{0, 1\}$. 
The policy is executed in a closed-loop setting to perform a manipulation task. 

\noindent\textbf{Model architecture.}
Our model takes input from two RGB cameras as frames $\mbf{I}_{t} = \{\mbf{I}^1_{t}, \mbf{I}^2_t \}$ at each timestep $t$ where $\mbf{I}^{*}_{t} \in \mathbb{R}^{H \times W \times 3}$.
The cameras are placed on a wide baseline with 90 degrees relative angle to alleviate depth ambiguities and occlusions.  
The observation $o_t$ includes the last three RGB frames $(\mbf{I}_{t-2}, \mbf{I}_{t-1}, \mbf{I}_t)$ and the last three gripper proprioceptive values $\mbf{P}_{t}=[\text{pos}_t, \sin(\phi_t), \cos(\phi_t)]$, where $\text{pos}_t$ is the gripper position with respect to the robot base, 
and $\phi_t$ is the rotation angle of the gripper around the end-effector axis. 
We stack frames from each viewpoint in the channel dimension and use \mbox{ResNet-18} network~\cite{he16} to generate a feature vector of dimension $512$ for each viewpoint. 
We then concatenate feature vectors from the two viewpoints together with the proprioceptive information $\mbf{P}_{t}$.
The action $\mbf{a}_{t}$ is finally predicted by a multi-layer perceptron (MLP) composed of two layers with $512$ hidden units and a ReLU activation function each.

%\smallskip
\noindent\textbf{Training with behavior cloning.}
Given a dataset of observation-action pairs $\{(\mbf{o}_{t}, \mbf{a}_{t})\}_{t}$ obtained from expert trajectories, 
we randomly initialize our policy network
and train it using a combination of the mean-squared error (MSE) loss for velocity control $\mbf{v}_{t}$, $\boldsymbol{\omega}_{t}$, and the binary cross entropy (BCE) loss for gripper state probability $g_{t}$ defined as:
\begin{equation}
  L = \lambda L_{MSE}((\hat{\mbf{v}}_{t}, \hat{\boldsymbol{\omega}}_{t}), (\mbf{v}_{t}, \boldsymbol{\omega}_{t})) + (1-\lambda)L_{BCE}(\hat{g}_{t}, g_{t})
  \label{eqn:loss}
\end{equation}
where $\lambda$ is a hyper-parameter to balance the MSE and BCE terms, $(\mbf{v}_{t}, \boldsymbol{\omega}_{t}, g_{t})$ is the expert action and $\pi_{\theta}(o_{t}) = (\hat{\mbf{v}}_{t}, \hat{\boldsymbol{\omega}}_{t}, \hat{g}_{t})$ is the predicted action for observation $o_t$.
\vspace{-.1cm}

\subsection{Domain Randomization}
\label{sec:dr}
In order to improve sim-to-real transfer, we augment synthetic images with domain randomization (DR) for policy learning.
We investigate different visual DR components, including textures, lightning conditions, object colors and camera parameters as described below.

\noindent\textbf{Texture randomization.} 
To obtain robustness to scene appearance, we randomize the textures of the robot, table, wall and floor. 
As the ability to distinguish object colors matters in our tasks, we do not randomize the object textures. 
We compare two types of textures as illustrated in Figure~\ref{fig:textures_comp}.
The first type is procedural textures \cite{tobin17} (Figure~\ref{fig:textures_comp} top), which have random colors and follow one of the four patterns - checkers, gradient, noise and a plain color. 
The second type is a set of $1,203$ high quality and realistic textures from ambientCG assets \cite{ambientCG} (Figure~\ref{fig:textures_comp} bottom). 

\input{figures/textures_comp}

\noindent\textbf{Lighting randomization.} To achieve robustness to lighting conditions, we sample the light position uniformly in a portion of a sphere for rendering images. 
We define the sphere in spherical coordinates with a distance of the light source to the workspace center in the range $[1.0, 3.0]$ meter, azimuthal angle and polar angle in the range $[0, \pi/2]$ and $[\pi/10, 4\pi/10]$ radians. This range of parameters allows us to sample realistic light source positions around the table, e.g., we avoid sampling lights inside the table or under it.
Besides light positions, we randomize the light properties with diffuse, specular and ambient coefficients around a nominal value set to $0.3$ by adding an offset sampled from the range $[-\psi_{l}, \psi_{l}]$ where $\psi_{l}$ is a parameter to be optimized.

\noindent\textbf{Variation of object colors.} 
We treat object colors in a special way since object manipulation in some of our tasks depends on the object color.
We therefore randomize object colors by sampling the Hue, Saturation and Value (Brightness) (HSV) around their nominal value. 
We sample an offset from range $[-\phi_o, \phi_o]$ and add it to the object color expressed using HSV coordinates in $[0, 1]$. The range $\phi_o$ is selected such that it is sufficiently large to cover possible color discrepancies between real and simulated scenes, and not too large to avoid confusion between different objects with initially different colors.

\noindent\textbf{Variation of camera parameters.} Camera calibration is often imprecise especially in terms of extrinsic parameters. 
To improve robustness to slight viewpoint changes, we randomize camera positions in simulation by sampling camera angles, locations and the field of view (FOV) around default values.

\input{figures/overview}
\vspace{-.2cm}
\subsection{Localization as a proxy task}
\vspace{-.1cm}
\label{sec:proxy_task}

DR parameters could be selected for each task by optimizing the accuracy of the learned policies for a real robot scenario. 
Such an approach, however, is highly time-consuming as it requires policy retraining and real-world policy evaluation for each set of DR parameters.
In this work we demonstrate that DR parameters can be efficiently chosen off-line using real images recorded for a proxy task. 
For this purpose we propose the following task of cube localization.

Given RGB images of a scene with three cubes of different colors, our proxy task aims to predict 3D locations of each cube relative to the gripper, see Figures~\ref{fig:textures_comp} and  \ref{fig:robustness_setup}. 

We chose this proxy task because it requires to distinguish different colors and localize multiple objects, which enables to study DR parameters for color-dependent and multi-object manipulation policies.

To predict 3D locations of the cubes, we use a similar neural network architecture as in Section~\ref{sec:bc} but with RGB frames $\mbf{I}_{t} = \{\mbf{I}^1_{t}, \mbf{I}^2_t \}$ for one time step instead of three and no proprioceptive information at the input.
We train the model by minimizing the distance between the predicted and the ground truth positions of the cubes. 

To evaluate sim-to-real transfer for the proxy task, we train the cube localization network in simulation using different DR parameters and measure localization precision on pre-recorded images of real scenes.
Our empirical experiments in Section~\ref{sec:expr} show that the performance of this proxy task is well aligned with the real-world accuracy of diverse manipulation policies.
\vspace{-.2cm}