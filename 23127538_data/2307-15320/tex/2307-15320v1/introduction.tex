\section{Introduction}
\vspace{-.1cm}

Data-driven policy learning allows to acquire reactive robotic skills and has recently shown
impressive results both in simulation and in the real world for tasks such as dexterous manipulation \cite{akkaya19, allshire21, handa22, chen22}, robotic arm manipulation~\cite{levine16, lee21, florence21}, quadruped locomotion \cite{miki22,frey22,agarwal22,aractingi22} and navigation \cite{chaplot2020object,chen2021history,chen2022think,khandelwal2022simple}. 
Despite much progress in recent years, deploying simulator-trained policies in the real world remains challenging due to the visual and physical sim-to-real gap \cite{akkaya19,handa22,miki22,bousmalis18,agarwal22}. 
As result, current methods often rely on real-robot training data~\cite{lee21,florence21,bousmalis18,florence20,zhang18} and are hard to scale due to the prohibitive cost of large-scale real data collection. 


To reduce the visual sim-to-real gap, existing methods typically apply domain adaptation (DA)~\cite{bousmalis18, rao20, ho21} or domain randomization (DR)~\cite{handa22,lee21,tobin17,strudel20} techniques.
While DA attempts to create more realistic synthetic images, DR merely randomizes scene parameters such as textures and lighting, and is more common due to its simplicity and good performance in practice.
Nevertheless, most existing DR methods study sim-to-real transfer for disembodied tasks such as pose estimation~\cite{tobin17, alghonaim21} and object detection~\cite{valtchev20} as they are easier to evaluate than real robot tasks.
Hence, it remains unclear
(i)~whether DR generalizes well to diverse and challenging robotic tasks,
(ii)~how to select optimal DR parameters for learning robotic policies, and 
(iii) how robotic policies trained with DR in simulation compare to policies trained on real data when tested in real scenes with varying visual appearance.

In this work we address the above questions and systematically study visual domain randomization on a suite of seven challenging robotic manipulation tasks, see Figure~\ref{fig:overview}. 
Our initial experiments show that policies trained in simulation without adaptation fail and achieve zero accuracy when transferred to a real robot.
To improve sim-to-real transfer, we propose a cube localization proxy task and use it for off-line optimization of DR parameters such as textures, lighting, object colors and camera parameters.
We then use DR with the obtained set of parameters and train policies for seven robotic manipulation tasks, namely stacking, box-retrieving, assembling, pushing, pushing-to-pick, sweeping and rope-shaping, see examples in Figure~\ref{fig:teaser}.

As one of our main contributions, we demonstrate that improvements on our proxy task consistently transfer to all considered manipulation policies under the same DR parametrization.
Our policies trained on simulation-only data using off-line optimized DR parameters achieve 93\% average success rate on seven real-world manipulation tasks.
Moreover, we show that our simulation-trained policies demonstrate robustness to appearance variations of real scenes and outperform policies trained on real but limited data.

In summary, our contributions are three-fold: 
(i)~We propose a proxy task and demonstrate that DR parameters optimized on this task generalize to a rich set of manipulation policies.
(ii)~We present a diverse set of seven manipulation tasks and use them to benchmark sim-to-real policy transfer.
(iii)~We ablate and show high success rate for policies trained only in simulation across a diverse set of manipulation tasks on a real robot. 
We also demonstrate our approach to outperform policies trained on real but limited data.
