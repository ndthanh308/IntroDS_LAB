\section{Related Work}

The discrepancies between simulation and the real world mainly come from the visual appearance of the scene \cite{denninger20,sadeghi18,qin22} and the 
physical dynamics \cite{xue18,valassakis20,mehta20,tsai21}. Here, we focus on reducing the visual sim-to-real gap and review the two dominant approaches, namely domain adaptation and domain randomization.

\noindent\textbf{Domain adaptation} methods aim to map images rendered in simulation to realistic images, for example using Generative adversarial networks (GANs) \cite{bousmalis18, rao20, ho21}.
Alternatively, such methods attempt to map simulated and real data to a common domain either in the image space~\cite{james19} or feature space~\cite{ganin16}.
Other methods \cite{hansen21} perform adaptation by finetuning the model during deployment using self-supervision.
While GANs are expressive models, they often generate artifacts~\cite{bousmalis18} such as spurious objects and scene structures. 
To preserve the 3D structure of scenes in generated images, domain adaptation methods often resort to manually-defined constraints such as segmentation masks~\cite{bousmalis18,ho21}.

\noindent\textbf{Domain randomization} methods, also known as domain augmentation, focus on adding variation to the simulated data to enforce robustness of the learned model to image perturbations. 
Tobin \textit{et al.} \cite{tobin17} change the visual appearance of the simulation by randomizing the material textures, objects pose, shape and color, background, lighting properties and camera parameters. 
While this method can successfully transfer robotic policies trained in simulation to real robots~\cite{james17,andrychowicz20,matas18,sadeghi18}, it requires a significant amount of manual tuning and expensive iterations of simulation-based policy training followed by real-robot validation.  
For this reason, systematic and efficient domain randomization is still an open question~\cite{hofer20}. In this work, we address this question by examining how domain randomization parameters should be chosen to make them usable across different tasks and scene conditions.
Alghonaim and Johns~\cite{alghonaim21} study a set of design choices for visual sim-to-real. However, the evaluation is limited to one off-line task, object pose estimation. Our work goes beyond this task studying design choices for a  varied set of on-line robotic control tasks. 
Pashevich et al. \cite{pashevich19} exploit object localization as a proxy task to optimize 2D image augmentations for depth images with Monte-Carlo tree search (MCTS). 
In contrast, we address sim-to-real transfer for RGB images where our experimental results in Table~\ref{table:dr_ablation} confirm that 3D domain randomization is by far more effective than 2D image augmentations. The cost of 3D rendering prohibits MCTS optimization. We empirically show that a simple greedy search strategy on a proxy task enables successful sim-to-real transfer of visuomotor policies for diverse and challenging manipulation tasks.
