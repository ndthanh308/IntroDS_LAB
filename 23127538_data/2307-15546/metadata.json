{
  "title": "On the Trade-off Between Efficiency and Precision of Neural Abstraction",
  "authors": [
    "Alec Edwards",
    "Mirco Giacobbe",
    "Alessandro Abate"
  ],
  "submission_date": "2023-07-28T13:22:32+00:00",
  "revised_dates": [
    "2023-10-02T09:56:53+00:00"
  ],
  "abstract": "Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions that result in dynamical models with these semantics. Empirically, we demonstrate the trade-off that these different neural abstraction templates have vis-a-vis their precision and synthesis time, as well as the time required for their safety verification (done via reachability computation). We improve existing synthesis techniques to enable abstraction of higher-dimensional models, and additionally discuss the abstraction of complex neural ODEs to improve the efficiency of reachability analysis for these models.",
  "categories": [
    "cs.LO",
    "cs.LG",
    "eess.SY"
  ],
  "primary_category": "cs.LO",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15546",
  "pdf_url": "https://arxiv.org/pdf/2307.15546v2",
  "comment": "Appeared at QEST 2023. Added codebase link; corrected Eq. 11",
  "num_versions": null,
  "size_before_bytes": 1203969,
  "size_after_bytes": 535857
}