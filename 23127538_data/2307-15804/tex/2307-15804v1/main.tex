\documentclass[12pt]{amsart}

\usepackage{Style_files/Packages}
\usepackage{Style_files/New_commands}
\usepackage{Style_files/Style}
\usepackage{comment}
\usepackage{thmtools}
\usepackage{thm-restate}
%\usepackage[page,header]{appendix}
%\usepackage{titletoc}
%\usepackage{thmtools}

\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata, positioning}
%\pgfplotsset{compat=1.18}

%\newcommand{\R}{\mathbb{R}}
\newcommand{\LPG}{\mathsf{L}\mathsf{P}\mathsf{G}}
\newcommand{\C}{\mathsf{C}}
\renewcommand{\S}{\mathcal{S}_{d-1}}
\renewcommand{\P}{\mathbb{P}}
%\newcommand{\N}{\mathbb{N}}
\newcommand{\1}{\mathbbm{1}}
%\newcommand{\E}{\mathbb{E}}

\newcommand{\noteL}[1]{{\color{teal}[{\color{teal} \bfseries L: } #1]}}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.4em}

\usepackage{accents}
\newcommand{\dbtilde}[1]{\accentset{\approx}{#1}}

% \usepackage{wrapfig}
% \usepackage[ruled, linesnumbered]{algorithm2e}

\title{On Single Index Models beyond Gaussian Data}
\author{Joan Bruna, Loucas Pillaud-Vivien and Aaron Zweig}

\begin{document}

\maketitle


\begin{abstract}
Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. 
Amongst those functions, the simplest are single-index models $f(x) = \phi( x \cdot \theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\phi$ applied to an unknown one-dimensional projection $\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the planted setting where $\phi$ is known, our main results establish that Stochastic Gradient Descent can efficiently recover the unknown direction $\theta^*$ in the high-dimensional regime, under assumptions that extend previous works ~\cite{yehudai2020learning,wu2022learning}. 
\end{abstract}


\vfill 

\setcounter{tocdepth}{1}
\tableofcontents

\vfill 

\clearpage


\section{Introduction}
Over the past years, there has been sustained effort to enlarge our mathematical understanding of high-dimensional learning, particularly when using neural networks trained with gradient-descent methods --- highlighting the interplay between algorithmic, statistical and approximation questions. An essential, distinctive aspect of such models is their ability to perform \emph{feature learning}, or to extract useful low-dimensional features out of high-dimensional observations. 

An appealing framework to rigorously analyze this behavior are sparse functions of the form $f(x) = \phi( \Theta_*^\top x)$, where the labels are generated by a generic non-linear, low-dimensional function $\phi: \mathbb{R}^k \to \mathbb{R}$ of linear features $\Theta^\top_* x$, with $\Theta_* \in \mathbb{R}^{d \times k}$ with $k \ll d$. While the statistical and approximation aspects of such function classes are by now well-understood \cite{barron1993universal,bach2017breaking}, the outstanding challenge remains computational, in particular in understanding the ability of gradient-descent methods to succeed. Even in the simplest setting of \emph{single-index models} ($k=1$), and assuming that $\phi$ is known,  the success of gradient-based learning depends on an intricate interaction between the data distribution $x \sim \nu$ and the `link' function $\phi$; and in fact computational lower bounds are known for certain such choices \cite{yehudai2020learning,song2021cryptographic,goel2020superpolynomial,diakonikolas2017statistical,shamir2018distribution}. 

Positive results thus require to make specific assumptions, either about the data, or about the link function, or both. On one end, there is a long literature, starting at least with \cite{kalai2009isotron,shalev2010learning,kakade2011efficient}, that exploits certain properties of $\phi$, such as invertibility or monotonicity, under generic data distributions satisfying mild anti-concentration properties \cite{soltanolkotabi2017learning,frei2020agnostic,yehudai2020learning,wu2022learning}. On the other end, by focusing on canonical high-dimensional measures such as the Gaussian distribution, the seminal works \cite{arous2020online,dudeja2018learning} built a harmonic analysis framework of SGD, resulting in a fairly complete picture of the sample complexity required to learn generic link functions $\phi$, and revealing a rich asymptotic landscape beyond the proportional regime $n \asymp d$, characterized by the number of vanishing moments, or \emph{information exponent} $s$ of $\phi$, 
whereby $n \asymp d^{s-1}$ samples are needed for recovery. Since then, several authors have built and enriched this setting to multi-index models~\cite{abbe2022merged,abbe2023sgd,damian2022neural,arnaboldi2023high}, addressing the semi-parametric learning of the link function~\cite{biettilearning2022}, as well as exploring SGD-variants~\cite{arous2022high,barak2022hidden,berthier2023learning,chen2023learning}. This harmonic analysis framework relies on two key properties of the Gaussian measure and their interplay with SGD: its spherical symmetry and its stability by linear projection. Together, they provide an optimization landscape that is well-behaved in the limit of infinite data, and enable SGD to escape the `mediocrity' of initialisation, where the initial direction $\theta_0$, in the high-dimensional setting, has vanishingly small correlation $| \theta_0 \cdot \theta^*| \simeq 1/\sqrt{d}$ with the planted direction $\theta^*$. 

In this work, we study to what extent the `Gaussian picture' is robust to perturbations, focusing on the planted setting where $\phi$ is known. Our motivation comes from the fact that real data is rarely Gaussian, yet amenable to being approximately Gaussian via CLT-type arguments.
We establish novel positive results along two main directions: (i) when spherical symmetry is preserved but stability is lost, and (ii) when spherical symmetry is lost altogether. In the former, we show that spherical harmonics can be leveraged to provide a benign optimization landscape for SGD under mild regularity assumptions, for initialisations that can be reached with constant probability with the same sample complexity as in the Gaussian case. 
In the latter, we quantify the lack of symmetry with robust projected Wasserstein distances, and show that for `quasi-symmetric' measures with small distance to the Gaussian reference, SGD efficiently succeeds for link functions with information exponent $s \leq 2$. Finally, using Stein's method, we address substantially `non-symmetric' distributions, demonstrating the strength and versatility of the harmonic analysis framework. 

% provided the initialization satisfies $| \theta_0 \cdot \theta^*| \simeq \sqrt{s/d}$, where $s$ is the information exponent of $\phi$. Since such initializations can be obtained with constant probability $\simeq e^{-s}$ independent of dimension

% \begin{itemize}
%     \item Interaction between data distribution and link function assumptions. 
%     \item Specific data and general link: framework of information exponent from Ben Arous et al. 
%     \item Specific link and general data: much less is known, Yehudai and shamir. Anti-concentration as important tool.
%     \item Our main contribution: reconcile these two streams of work and establish general results in both fronts (data and link), using simple stochastic gradient descent. 
%     \item Focus on the planted setting of a single neuron. $\phi:\mathbb{R} \to \mathbb{R}$ known link function, $f(x) = \phi(\langle x, \theta^* \rangle)$ target function. 
%     For simplicity, let us write $\phi_\theta(x) := \phi( \langle x, \theta \rangle)$. 
%     Loss is 
%     $L(\theta) = \mathbb{E}_{\nu} [| \phi_\theta - \phi_{\theta^*}|^2]$. Empirical loss defined analogously. 
%     \item We study two settings: (i) spherical symmetry with subgaussian tails, where the link function is arbitrary (TBC), and (ii) general distribution, where the link function is Lipschitz and bounded and with information exponent $s=2$ (TBC).
%     \item Key observation: lack of Gaussianity can be viewed as an additional perturbative term in the loss, pretty much as the empirical fluctuations. While empirical fluctuations can be controlled by increasing the number of training points, non-Gaussian perturbations are shown to be concentrated in the equatorial band of width $O(d^{-1/2})$ in the case (i), and  of order $O(d^{-1/2})$, uniformly over the unit sphere in case (ii). 
%     \item As a result, random initialization will overcome these perturbations with positive probability, \emph{independent of dimension}.  
% \end{itemize}

%  \paragraph{Additional Related Works.}
%  Literature on single-index models
%  Shamir Yehudai, Ben Arous. 
% {\color{red} TODO fill}

% \clearpage



\section{Preliminaries and Problem Setup}
\label{sec:preliminaries}

The focus of this work is to understand regression problems with input/output data $(x, y) \in \R^d \times \R $ generated by \textit{single-index} models. This is a class of problems where the data labels are produced by a non-linear map of a one-dimensional projection of the input, that is
%
\begin{align}
\label{eq:y_model}
    y = \phi(x \cdot \theta^*), 
\end{align}
%
where $\phi:\mathbb{R} \to \mathbb{R}$ is also known as the \textit{link function}, and $\theta^* \in \S$, the sphere of $\R^d$, is the \textit{hidden direction} that the models wants to learn. Quite naturally, the learning is made through the family of generalized linear predictors $\mathcal{H} = \{\phi_{\theta} : x \to \phi( x \cdot \theta), \ \text{for }\, \theta\! \in \S \}$, built upon the link function (which is assumed known) and parametrized by the sphere.
%
\paragraph{Loss function.}
We assume that the input data is distributed according to a probability $\nu \in \mathcal{P}(\R^d)$. Equation~\eqref{eq:y_model} implies that the target function that produces the labels, $\phi_{\theta^*},$ lies in this parametric class. The overall loss classically corresponds to the average over all the data of the square penalisation $l(\theta, x) := (\phi_\theta(x) - \phi_{\theta^*}(x))^2$ so that the population loss writes 
%
\begin{align}
\label{eq:mainsetup}
    L(\theta) := \mathbb{E}_{\nu} \left[\big(\phi(x \cdot \theta) - \phi(x \cdot \theta^*)\big)^2\right] = \left\|\phi_{\theta} - \phi_{\theta^*}\right\|^2_{L^2_\nu},
\end{align}
%
where we used the notation $\|f\|^p_{L^p_\nu} = \E_\nu[|f|^p]$, valid for all $p \in \N^*$. Let us put emphasis on the fact that the loss $L$ is a non-convex function of the parameter $\theta$, hence it is not \textit{a priori} guaranteed that gradient-based method are able to retrieve the ground-truth $\theta^*$. This often requires a precise analysis of the \textit{loss landscape}, and where the high-dimensionality can play a role of paramount importance: we place ourselves in this high-dimensional setting for which the dimension is fixed but considered very large $d \gg 1$. Finally, we assume throughout the article that $\phi_\theta$ belongs to the weighted Sobolev space $W^{1,4}_\nu:=\{ \phi, \text{ such that } 
\sup_{\theta \in \S} \left[\|\phi_\theta\|_{L^4_\nu} + \|\phi_\theta'\|_{L^4_\nu}\right] <  \infty \}$.

\paragraph{Stochastic gradient descent.}
To recover the signal given by $\theta^* \in \S$, we run \textit{online stochastic gradient descent} (SGD) on the sphere $\S$. This corresponds to having at each iteration $t \in \mathbb{N}^*$ a \textit{fresh sample} $x_t$ drawn from $\nu$ and independent of the filtration $\mathcal{F}_t = \sigma(x_1, \dots, x_{t-1})$ and performing a spherical gradient step, with step-size $\delta>0$, with respect to $\theta \to l(\theta, x_t)$:
%
\begin{align}
    \label{eq:SGD}
    \theta_{t+1} &= \frac{\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)}{\left|\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)\right|}, \qquad \text{ with initialization } \theta_{0} \sim \mathrm{Unif}(\S),
\end{align}
%
%the uniform distribution on the sphere. 
An important scalar function that enables to track the progress of the SGD iterates is the correlation with the signal $m_\theta:=\theta \cdot \theta^* \in [-1,1]$. We will drop the subscript in case there is no ambiguity. Note that, due to the high-dimensionality of the setup, we have the following lemma:
%
%{\color{blue} Est-ce qu on peut utiliser le lemma aussi pour donner un lower bound, comme ca on a tout ensemble?}
\begin{lemma}
\label{lem:initialization}
For all $a > 0$, we have $ \P_{\theta_0}(m_{\theta_0} \geq a/\sqrt{d}) \leq a^{-1}e^{-a^2/4}  $. Additionally, for any~$\delta>0$ such that $ \max\{a, \delta\} \leq \sqrt{d}/4$, we have the lower bound:
    $\P_{\theta_0}(m_{\theta_0} \geq a/\sqrt{d}) \geq \frac{\delta}{4} e^ {- (a + \delta)^2} $.
\end{lemma}

%\begin{lemma}
%\label{lem:initialization}
%    For all $a > 0$, we have $ \P_{\theta_0}(m_{\theta_0} \geq a) \leq e^{-d a^2/2} $. %Additionally, for $b = \sqrt{\tau/d}$, we have 
%    $\P_{\theta_0}(m_{\theta_0} \geq b) \geq \max(d^{\Theta(1)} (1- \frac{\tau}{d})^{d/2}, e^{-%\tau/2} - O(1/\sqrt{d})) \simeq \max(d^{\Theta(1)}, 1-  O(1/\sqrt{d})) \exp( -\tau / 2)$.
   % $\P_{\theta_0}(m_{\theta_0} \geq b) \geq C e^{-\tau} -O(1/\sqrt{d})$ for a universal constant $C>0$. 
%\end{lemma}
%
This fact implies that, when running the algorithm in practice, it is initialized with high probability near the equator of $\S$, or at least in a band of typical size $1/\sqrt{d}$ (see Figure~\ref{fig:nice_tikz} for a schematic illustration of this fact). Finally, we use the notation $\nabla_\theta^\mathcal{S}$ to denote the spherical gradient, that is $ \nabla_\theta^\mathcal{S} l(\theta, x) = \nabla_\theta  l(\theta, x) - (\nabla_\theta  l(\theta, x) \cdot \theta) \theta$. As $\nabla_\theta^\mathcal{S}  l(\cdot, x_t)$ is an unbiased estimate of $\nabla_\theta^\mathcal{S}  L$, it is expected that the latter gradient field rules how the SGD iterates travel across the loss landscape.


\paragraph{Loss landscape in the Gaussian case.} As stressed in the introduction, this set-up has been studied by \cite{dudeja2018learning,arous2020online} in the case where $\nu$ is the standard Gaussian, noted as $\gamma$ here to avoid any confusion for later. Let us comment a bit this case to understand what can be the typical landscape of this single-index problem. 
Thanks to the spherical symmetry, the loss admits a scalar summary statistic, given precisely by the correlation $m_\theta$. Moreover, the loss admits an explicit
representation in terms of the Hermite decomposition of the link function $\phi$: if $\{h_j\}_j$ denotes the orthonormal basis of Hermite polynomials of $L^2_\gamma$, then $L(\theta) = 2\sum_j |\langle \phi, h_j\rangle|^2(1 - m^j) := \bar{\ell}(m)$. As a result, 
%Remarkably, it can be shown in this case that the loss depends only of the correlation $m$, and that 
the gradient field projected along the signal is a (locally simple) \textit{positive function} of the correlation that behaves similarly to
%
\begin{align}
    \label{eq:loss_property_gaussian}
    - \nabla_\theta^\mathcal{S}  L(\theta) \cdot \theta^* \simeq C m^{s-1} (1 - m),
\end{align}
%
where $s \in \N^*$ is the index of the first non-zero of the Hermite coefficients $\{\langle \phi, h_j\rangle\}_j$.
This has at least three important consequences for the gradient flow: (i) if initialized positively, the correlation is an increasing function along the dynamics and there is no bad local minima in the loss landscape,  (ii) the parameter $s \in \N^*$ controls the flatness of the loss landscape near the origin and therefore controls the optimization speed of SGD in this region (iii) as soon as $m$ is large enough, the contractive term $1-m$ makes the dynamics converge exponentially fast.
%
\paragraph{Loss landscape in general cases.} Obviously for general distributions $\nu$, the calculation presented in Eq.\eqref{eq:loss_property_gaussian} is no-longer valid. However, the crux of the present paper is that properties (i)-(ii)-(iii) are robust to the change of distribution and can be shown to be preserved under small adaptations. More precisely, we have the following definition.
%
\begin{definition}[Local Polynomial Growth]
\label{def:growth}
We say that $L$ has the local polynomial growth of order $k \in \N^*$ and scale $b \geq 0$, if there exists $C > 0$ such that for all $m_\theta \geq b$, 
%
\begin{equation}
    - \nabla_\theta^\mathcal{S}  L(\theta) \cdot \theta^* \geq C (1 - m_\theta) \left(m_\theta - b\right)^{k-1}~.
\end{equation}
    In such a case we say that $L$ satisfies $\LPG(k,b)$.
\end{definition}
%
In this definition, and as showed later in specific examples given in Section~\ref{sec:population_landscape}, the scale parameter $b$ should be thought as a small parameter  proportional to $1/\sqrt{d}$. 
%
%\begin{definition}[Contraction]
%\label{def:contraction}
%We say that $L$ has the contraction property of order $\alpha >0$, if there %exists $C > 0$ such that for all $ m_\theta \geq \frac12$, 
%%
%\begin{equation}
%    - \nabla_\theta^\mathcal{S}  L(\theta) \cdot \theta^* \geq C \left(1 - %m_\theta\right)^\alpha~.
%\end{equation}
%In such a case we say that $L$ satisfies $\C(\alpha)$.
%\end{definition}
%
If $\nu$ is Gaussian, we can rewrite Eq.\eqref{eq:loss_property_gaussian} and show that $L$ verifies $\LPG(s,0)$ for $s \in \N^*$, referred to as the \textit{information exponent} of the problem in~\cite{arous2020online}.
An important consequence of satisfying $\LPG(k,b)$ is that the the population landscape is free of bad local minima outside the equatorial band $\Sigma_b:= \{ \theta \in \S \ , \   m_\theta \leq b\}$. Therefore, when $b$ is of scale $1/\sqrt{d}$, Lemma \ref{lem:initialization} indicates that one can efficiently produce initializations that avoid it.  
Hence, this property is the fundamental ingredient that enables the description of the path taken by SGD. We derive it thoroughly in next Section. Section~\ref{sec:population_landscape} is devoted to showcasing generic examples when this property is satisfied.  
%
%\begin{fact}
%Assume that $L(\theta)$ is such that $\nabla L(\theta)$ has polynomial growth for %some $k\geq 0$ at scale $b$. 
%Then $L(\theta)$ has no bad local minima in the spherical cap $\{ \theta ;  m > %\frac{b}{\sqrt{d}} \}$. 
%\end{fact}
%



\section{Stochastic Gradient Descent under \texorpdfstring{$\LPG$}{LPG}}
\label{sec:sgd}

In this section, we derive the main results on the trajectory of the stochastic gradient descent. They state that property $\LPG(s,b/\sqrt{d})$ is in fact \textit{sufficient} to recover the same quantitative guarantees as the one depicted in \cite{arous2020online}, despite the lack of Gaussianity of the distribution $\nu$. Recall that the recursion satisfied by the SGD iterates is given by Eq.\eqref{eq:SGD}. To describe their movement, let us introduce the following notations: for all $t \in \mathbb{N}^*$, we denote the normalization by $r_t := \left|\theta_t - \delta \nabla_\theta^\mathcal{S}  \ell(\theta_t, x_t)\right|$ and the martingale induced by the stochastic gradient descent as $M_t := l(\theta_t, x_t) - \E_\nu [ l(\theta_t, x)]$.

\paragraph{Tracking the correlation.} Recall that the relevant signature of the dynamics is the one-dimensional correlation $m_{t} = \theta_t \cdot \theta^*$. For infinitesimal step-sizes $\delta \to 0$, it is expected that $\theta_t$ follow the spherical gradient flow $\dot{\theta}_t = - \nabla_\theta^\mathcal{S} L(\theta_t)$, that translates naturally on the summary statistics $m_t$ as the following time evolution
%
\begin{align}
\label{eq:idealized_ODE}
\dot{m_t} = - \nabla_\theta^\mathcal{S} L(\theta_t) \cdot \theta^*.
\end{align}
%
The main idea behind the result of this section is to show that, even if the energy landscape near $m = m_0$ is rough at scale $1/\sqrt{d}$, the noise induced by SGD does not prevent $m$ to \textit{grow as the idealized dynamics described by the ODE \eqref{eq:idealized_ODE}}. Let us write the iterative recursion followed by $(m_t)_{t \geq 0}$: with the notation recalled above, for $t \in \mathbb{N}^*$, we have 
%
\begin{align}
\label{eq:dynamics_main_m}
m_{t+1} =  \frac{1}{r_t}\left( m_t - \delta\nabla_\theta^S L(\theta_t) \cdot \theta^* - \delta\nabla_\theta^S M_t \cdot \theta^*  \right).
\end{align}
%
With this dynamics at hand, the proof consists in controlling both the discretization error through $r_t$ and the directional martingale induced by the term $\delta\nabla_\theta^S M_t \cdot \theta^*$.

\paragraph*{Weak recovery.} As it is the case for the gradient flow, most of the time spent by the SGD dynamics is near the equator, or more precisely in a band of the type $\Sigma_{b,c} = \{ \theta \in \S, \ b/\sqrt{d} \leq m_\theta \leq c/\sqrt{d} \}$, where $b<c$ are constants independent of the dimension. Hence, the real first step of the dynamics is to go out any of these bands. This is the reason why it is natural to define $S_a := \{\theta \in \S, \, m_\theta \geq a\}$, the spherical cap of level $a \in (0,1)$ as well as the hitting time
%
\begin{align}
    \tau^+_a := \inf\{ t \geq 0, \, m_{\theta_t} \geq a  \},
\end{align}
%
which corresponds to the first time $(\theta_t)_{t \geq 0}$ enters in $S_a$. We arbitrarily choose a numerical constant independent of dimension, say $a = 1/2$, and refer to the related hitting time $\tau^+_{1/2}$ as the \textit{weak recovery time} of the algorithm.
%
\begin{theorem}[Weak Recovery]
\label{thm:weak_recovery}
    Let $(\theta_t)_{t \geq 0}$ follow the SGD dynamics of Eq.\eqref{eq:SGD} and let $L$ satisfy $\LPG(s,b/\sqrt{d})$, with~$b >0 $ and $s \in \N^*$, then, conditionally on the fact that $m_0 \geq 4b/\sqrt{d} $, for any $\varepsilon > 0$, we have
    %
    \begin{align}
       \quad \tau^+_{1/2} \leq \left\{
       \begin{array}{llll}
         d \cdot C/\varepsilon & \text{ when $s = 1,\ \ \ $ and with the choice} & \delta = \varepsilon/d \\
         d \log^2(d) \cdot C/\varepsilon & \text{ when $s = 2,\ \ \ $ and with the choice} & \delta = \varepsilon/(d \log d) \\
         d^{s-1} \cdot C/\varepsilon & \text{ when $s \geq 3,\ \ \ $ and with the choice} & \delta = \varepsilon d^{-s/2}
    \end{array} 
    \right. ~,
    \end{align}
    with probability at least $1 - C\varepsilon$, for some $C >0$ that depends solely on the link function $\phi$.
\end{theorem}
%
Let us comment on this result. It says that that the integer $s$ coming from the growth condition controls the hardness of exiting the equator of the sphere. Indeed, as can be seen in $\LPG(s,b/\sqrt{d})$, the larger the $s$, the smaller the gradient projection is and hence the less information the SGD dynamics has to move from the initialization. This result can be seen as an extension of \cite[Theorem 1.3]{arous2020online} valid only in the Gaussian case ($b = 0$). Furthermore, the Gaussian case shows that Theorem~\ref{thm:weak_recovery} is tight up to $\log(d)$ factors. Finally, note that the result is conditional to the fact that the initialization is larger that some constant factor of $1/\sqrt{d}$, which has at least constant probability to happen in virtue of Lemma~\ref{lem:initialization}. This probability can be lowered by any constant factor by sampling offline a constant factor (independent of $d$) of i.i.d. initializations and keeping the one that maximizes its correlation with $\theta^*$.
%{\color{blue} $\max_{k=1\ldots K} |\langle \theta_0^{(k)}, \theta^* \rangle| \simeq \sqrt{\log K/d}$ Yes, il fait donc $e^{b^2}$ initialisatioin } 
Finally, note that in all the cases covered by the analysis, it is possible to keep track of the constant $C$, and show that overall it depends only (i) on the property of $\nu$ w.r.t. the Gaussian on the one hand and (ii) on the Sobolev norm of the link function $\|\phi\|_{W_\nu^{1,4}}$ on the other hand. 

\paragraph*{Strong recovery.} We place ourselves \textit{after} the weak recovery time described in the previous question and want to understand if the $(\theta_t)_{t \geq 0}$ dynamics goes to $\theta^*$ and if yes, how fast it does so. This is what we refer to as \textit{the strong recovery} question, captured by the fact that the one-dimensional summary statistics  $m$ go towards $1$. Thanks to the Markovian property of the SGD dynamics, we have the equality between all time $s > 0$ marginal laws of
%
\begin{align*}
\left(\theta_{\tau^+_{\nicefrac{1}{2}} + s}\  \bigg| \  \tau^+_{1/2},\, \theta_{\tau^+_{1/2}}\right) \overset{\text{Law}}{=} \left(\theta_{s} \  \bigg| \  \theta_{s} = \theta_{\tau^+_{1/2}} \right),  
\end{align*}
%
and hence the strong recovery question is equivalent to study the dynamics with initialization that has already weakly recovered the signal, i.e. such that $m_\theta  = 1/2$. We show that this part of the loss landscape is very different that the equator band in which the dynamics spends most of its times: in all the cases, we can choose stepsizes independent of the dimension and show that the time to reach the vicinity of $\theta^*$ will be independent of $d$.
%
\begin{theorem}[Strong Recovery]
\label{thm:strong_recovery}
    Let $(\theta_t)_{t \geq 0}$ follow the SGD dynamics of Eq.\eqref{eq:SGD} and let $L$ satisfy $\LPG(s,b/\sqrt{d})$, with~$b >0 $ and $s \in \N^*$, then, for any $\varepsilon > 0$, taking $\delta = \varepsilon \log(1/\varepsilon)^{-1}$,   we have that there exists a time $T > 0$, such that 
    %
    \begin{align}
       |1 - m_T| \leq \varepsilon, \text{ and }\quad |T - \tau^+_{1/2}| \leq C \log(1/\varepsilon)\varepsilon^{-1} 
    \end{align}
    with probability at least $1 - C\varepsilon$, for some $C >0$ that depends solely of the link function $\phi$.
\end{theorem}
%
As introduced above, the important messages of this theorem are that (i) there is no difference between the different parameters setups captured by the information exponent $s$, and (ii) the time it takes to reach an $\varepsilon$-vicinity of $\theta^*$ is \textit{independent of the dimension}. We remark that we decided to present Theorem~\ref{thm:strong_recovery} resetting the step-size $\delta$ to put emphasis on the intrinsic difference between the two phases. Yet, we could have kept the same stepsize as in the weak recovery case: this obviously would slow down unnecessarily the second phase.


% Figure environment removed





\section{Typical cases of loss landscape with \texorpdfstring{$\LPG$}{property} property}
\label{sec:population_landscape}

In this section, we showcase two prototypical cases where the $\LPG$ holds true: the section \ref{subsec:sym} deals with the spherically symmetric setting, whereas the section \ref{subsec:nonsym} describes a perturbative regime where the distribution is \textit{approximately} Gaussian is a quantitative sense. 


\subsection{The symmetric case}
\label{subsec:sym}
We start our analysis with the spherically symmetric setting. We show that a spherical harmonic decomposition provides a valid extension of the Hermite decomposition in the Gaussian case, leading to essentially the same quantitative performance up to constant (in dimension) factors. 

\paragraph*{Spherical Harmonic Representation of the Population Loss.}
 We express the data distribution $\nu$ as a mixture of uniform measures $\nu = \int_0^\infty \tau_{r,d}\, \rho(dr) $, where $\tau_{r,d}=\mathrm{Unif}(r \S)$. 
Let $\tau_d = \tau_{1,d}$ and $u_d\in \mathcal{P}([-1,1])$ be the projection of $\tau_d$ onto one direction, with density given in close form by $u_d(dt) = Z^{-1}(1-t^2)^{(d-3)/2} \mathbf{1}(|t|\leq 1) dt $, where $Z$ is a normarlizing factor. 
%
Let $\{P_{j,d}\}_{j \in \mathbb{N}}$ be the orthogonal basis of Gegenbauer polynomials  of $L^2_{u_d}([-1,1])$, normalized such that $P_{j,d}(1) = 1$ for all~$j, d$. For each $r>0$, consider 
%
    $$l_r(\theta) := \langle\phi_\theta, \phi_{\theta^*}  \rangle_{\tau_{r,d}}  =   \langle \phi^{(r)}_\theta, \phi^{(r)}_{\theta^*}\rangle_{\tau_{d}}~,$$
    %
where we define $\phi^{(r)}: [-1,1] \to \mathbb{R}$ such that $\phi^{(r)}(t):= \phi( r t)$. We write its decomposition in~$L^2_{u_d}([-1,1])$ as 
    $\phi^{(r)} = \sum_{j} \alpha_{j,r,d} P_{j,d}~,~\text{with }~\alpha_{j,r,d} = \frac{\langle \phi^{(r)}, P_{j,d} \rangle}{\| P_{j,d} \|^2}~.$
%
    Let $\Omega_{d}$ be the Lebesgue measure of $\S$, and $N(j,d) = \frac{2d+j-2}{d}\binom{d+j-3}{d-1}$ the so-called \textit{dimension} on the spherical harmonics of degree $j$ in dimension $d$. 
  From the Hecke-Funk representation formula \cite[Lemma 4.23]{frye2012spherical} and the chosen normalization $P_{j, d} (1) = 1$, we have $\| P_{j,d} \|_2 = \left(\frac{\Omega_{d-1}}{\Omega_{d-2} N(j,d)} \right)^{1/2}$ \cite[Proposition 4.15]{frye2012spherical} and obtain finally
  \begin{align}
      l_r(\theta) &=  
      %\langle\phi^{(r)}_\theta, \phi^{(r)}_{\theta^*}\rangle_{\tau_{d}} = \frac{\Omega_{d-2}}{\Omega_{d-1}}\sum_j \frac{\Omega_{d-1}}{\Omega_{d-2}} N(j,d)^{-1} \alpha_{j,r,d}^2 P_{j,d}( \theta \cdot \theta^*) \\
      %&:=
      \sum\nolimits_{j} \bar{\alpha}_{j,r,d}^2 P_{j,d}( \theta \cdot \theta^*)~,
  \end{align}
  %
where we defined for convenience $\bar{\alpha}_{j,r,d} = \alpha_{j,r,d} / \sqrt{N(j,d)}$. As a result, it follows that the overall loss writes as solely the correlaton $m_\theta = \theta \cdot \theta^*$ as  
%s
    \begin{align}
    \label{eq:lossmain}
        L(\theta) 
        %&= \mathbb{E}_\nu [ |\phi_\theta - \phi_{\theta^*}|^2] \nonumber 
        %&= C_\phi - 2 \langle \phi_\theta, \phi_{\theta^*} \rangle_{\nu} \nonumber = C_\phi - 2\int_0^\infty \langle \phi_\theta, \phi_{\theta^*} \rangle_{\tau_{r,d}} \rho(dr) \nonumber  \\
        %&= 2 \mathbb{E}_\eta [\phi^2] - 2 \sum_{j} \left(\int_0^\infty \bar{\alpha}_{j,r,d}^2 \rho(dr) \right) P_{j,d}(\theta \cdot \theta^*) 
        = 2\|\phi\|^2_{L^2_\nu} -2 \sum\nolimits_{j} {\beta}_{j,d} P_{j,d}(m_\theta)~
        := \ell(m) ~,
    \end{align}
    %s
    where ${\beta}_{j,d} = \int_0^\infty \bar{\alpha}_{j,r,d}^2 \rho(dr) \geq 0$. 
%s
Unsurprisingly, we observe that, thanks to the spherical symmetry and analogous to the Gaussian case, the loss still admits~$m=\theta \cdot \theta^*$ as a summary statistics. Yet, it is represented in terms of Gegenbauer polynomials, rather than monomials as in the Gaussian case. The monomial representation is a consequence of the stability of the Gaussian measure, as seen by the fact that 
$\langle \phi_\theta, \phi_{\tilde{\theta}} \rangle_{\gamma_d} = \langle \phi, \mathsf{A}_{\theta \cdot \tilde{\theta}} \phi \rangle_{\gamma}~,$
where $(\mathsf{A}_m f)(t) = \mathbb{E}_{z \sim \gamma} [f ( mt + \sqrt{1-m^2}z)]$ has a \emph{semi-group} structure (it is even known in fact as the Ornstein-Ulhenbeck semi-group). 

Let $\eta \in \mathcal{P}(\mathbb{R})$ be the marginal of $\nu$ along any direction. The following proposition gives a closed form formula of the coefficients $\beta_{j,d}$, represented as integrals over the radial distribution $\rho$ and projections of the link function $\phi$:
%
\begin{proposition}[Loss representation]
\label{prop:basicdec}
The $\beta_{j,d}$ defined in \eqref{eq:lossmain} have the integral representation
    \begin{equation}
        \beta_{j,d} = \langle \phi, \mathcal{K}_j \phi \rangle_{L^2_\eta} ~,
    \end{equation}
    where $\mathcal{K}_j$ is a positive semi-definite integral operator of $L^2_\eta$ that depends solely on $\rho$ and $\phi$.     
    %with kernel 
%\begin{align}
%\mathcal{K}_j(t,t') &= \frac{\Omega_{d-2}N(j,d)}{\Omega_{d-1}}  %\int_0^\infty  P_j(r^{-1} t) P_j(r^{-1} t') \bar{u}_d(r^{-1} t) %\bar{u}_d(r^{-1} t') \rho(dr)~,
%\end{align}
%where we defined the conditional density 
%$$\bar{u}_d(r^{-1}t) = \frac{r^{-1} {u}_d(r^{-1} t)}{\int_0^\infty %(r')^{-1} {u}_d((r')^{-1} t) \rho(dr')}~. $$
%Moreover, we have 
%\begin{equation}
%    \mathbb{E}_\eta [\phi^2] = \frac{\Omega_{d-2}}{\Omega_{d-%1}}\sum_j \beta_{j,d} = \frac{\Gamma((d-2)/2)}%{\sqrt{\pi}\Gamma((d-1)/2)} \sum_j \beta_{j,d}~.
%\end{equation}
\end{proposition}
%
Note that a closed form expression of $\mathcal{K}_j$ can be found in Appendix \ref{secapp:basicdec}. The above proposition is in fact the stepping stone to calculate properly the \textit{information exponent} that plays a crucial role in the property $\LPG$. This is given through the link between the spectrum $\beta_{j,d}$ and the decomposition of $\phi$ in the $L^2_\eta$ orthogonal basis of polynomials, that we denote by $\{ q_j\}_j$. 
%
\begin{proposition}
\label{prop:infoexpo_sphere}
    Let $s = \inf\{ j; \beta_{j,d} > 0\}$ and $\tilde{s} = \inf\{ j; \langle \phi, q_j\rangle_{\eta} \neq 0\}$. Then $s \leq \tilde{s}$. 
\end{proposition}
%
Thus, the number of vanishing moments of $\phi$ with respect to the data marginal $\eta$ provides an upper bound on the `effective' information exponent of the problem $s$, as we will see next. 
%This gives an \textit{a priori} computable upper bound on the \textit{information exponent} of the problem directly related to intrinsic properties of the model. 
% {\color{red} TODO 
% question to clarify: if $q_j$ is the orthogonal polynomial basis wrt $\eta$, do we have a link between $\beta_j$ and $\langle \phi, q_j\rangle_\eta$, besides conservation of norm, $\sum_j \beta_j = \sum_i \langle \phi, q_i \rangle_\eta^2$ ? If $\phi$ has $s-1$ vanishing moments, ie $\langle \phi, q_i \rangle_\eta = 0$ for $i < s $, with $\langle \phi, q_s \rangle \neq 0$, what does this imply in terms of $\beta_j$ ?}

\paragraph{Local Polynomial Growth.} 
From (\ref{eq:lossmain}), and as
$\nabla_\theta^{\mathcal{S}} L(\theta) = \ell'(m) (\theta^* - m \theta)~,$ we directly obtain
%
\begin{equation}
-\nabla_\theta^{\mathcal{S}} L(\theta) \cdot \theta^* = -(1 - m^2) \ell'(m) =  2 (1 - m^2) \sum\nolimits_{j} \beta_{j,d} P'_{j,d}(m)~,    
\end{equation}
% and therefore 
% \begin{equation}
%     \dot{m} = \ell'(m) (1 - m^2)~.
% \end{equation}
%
which is the quantity we want to understand to exhibit the property $\LPG$ in this case. Hence, we now turn into the question of obtaining sufficient guarantees on the coefficients $(\beta_{j,d})_j$ that ensure local polynomial growth.
Since the typical scale of initialization for $m$ is $\Theta(1/\sqrt{d})$, 
our goal is to characterize sufficient conditions of local polynomial growth with $b = O(1/\sqrt{d})$. 

For that purpose, let us define two key quantities of Gegenbauer polynomials:
\begin{align}
    \upsilon_{j,d} & := -\min_{t \in (0,1)} P_{j,d}(t)~, ~~&\text{(smallest value)}\\
    z_{j,d} & := \arg\max\left\{ t \in (0,1); P_{j,d}(t)=0 \right\}~.~~&\text{(largest root)} 
\end{align}
% Later we will give tight bounds on $\zeta_{j,d}$ and $\upsilon_{j,d}$. 
% But first, let us verify how these quantities help us characterize local polynomial growth, thanks to the following fact: 
% \begin{fact}[Derivative of Gegenbauer polynomial]
% \begin{align}
%     P_{j,d}'  &= \frac{j (j+d-2)}{d-1} P_{j-1,d+2}~.
% \end{align}
% \end{fact}
%By assembling this fact with the definitions of $\zeta_{j,d}$ and $\upsilon_{j,d}$, 
We have the following sufficient condition based on the spectrum $(\beta_{j,d})_j$:
\begin{restatable}[Spectral characterization of $\LPG$]{proposition}{sufficientcond}
\label{prop:sufficientcond}
    Suppose there exist constants $K,C>0$ and $s \in \mathbb{N}$ such that we both have $\beta_{s,d} \geq C$ and $\sum_{j > s} \beta_{j,d} j (j+d-2)  \upsilon_{j-1,d+2} \leq K d^{(3-s)/2}$~.
    %
    Then, taking $s^*$ as the infimum of such $s$, $L$ has the property $\LPG(s^*-1,z_{s^*,d})$. In particular, whenever $s^*\ll d$, we have $z_{s^*,d} \leq 2\sqrt{s^*/d}$.     
\end{restatable}
%\begin{proposition}[Spectral characterization of $\LPG$]
%\end{proposition}
This proposition thus establishes that, modulo a mild regularity assumption expressed though the decay of the coefficients $\beta_{j,d}$, the spherically symmetric non-Gaussian setting has the same geometry as the Gaussian setting, for correlations \emph{slightly} above the equator. Crucially, the required amount of correlation to `feel' the local polynomial growth is a $O(\sqrt{s})$ factor from the typical initialization, and can be thus obtained with probability $\simeq e^{-s}$ over the initialization, according to Lemma~\ref{lem:initialization}, a lower bound which is \emph{independent of $d$}. 

% In Section \ref{fact:asympups}, we verify that $j (j+d-2) \upsilon_{j,d} \simeq \kappa^{-d} j^2 \min(1,j/d)^{-\varepsilon_d}$ with $\kappa>1$ and $\varepsilon_d \geq C> 0$ uniformly in $d$. As a result, the sufficient condition for $\beta_j$ becomes 
% %
% \begin{equation}
% \label{eq:simpledecaycond}
% \sum_{j >s}^d j^{2-\varepsilon_d} \beta_{j,d}  + \sum_{j>d} j^2 \beta_{j,d}  \leq K d^{(3-s)/2} \kappa^d~.
% \end{equation}

The sufficient condition for $\beta_{j,d}$ appearing in Proposition \ref{prop:sufficientcond} involves the minimum values $\upsilon_{j,d}$ of Gegenbauer polynomials $P_{j,d}$, as well as sums of the form $\sum_j j^2 \beta_{j,d}$. 
In order to obtain a more user-friendly condition, we now provide an explicit control of $\upsilon_{j,d}$, and leverage mild regularity of $\phi$ to control $\sum_j j^2 \beta_{j,d}$. This motivates the following assumption on $\phi$ and $\nu$:
\begin{assumption}
\label{ass:phi_symmetric_assumptions}
    The link function $\phi$ satisfies $\phi \in L^2_\eta$ and $\phi' \in L^4_\eta$, and the radial distribution $\rho$ has finite fourth moment $\mathbb{E}_\rho [r^4] < \infty$. 
\end{assumption}
\begin{theorem}[$\LPG$ for symmetric distributions]
\label{coro:symmcase}
    Assume that $\phi$ and $\nu$ satisfy Assumption \ref{ass:phi_symmetric_assumptions}, and let $s^* = \inf\{j; \langle \phi, q_j \rangle_\eta \neq 0 \}$. Then $L$ has the property $\LPG(s^*-1,2\sqrt{s^*/d})$. 
\end{theorem}
The proof is provided in Appendix \ref{sec:proofthmsym}. At a technical level, the main challenge in proving Theorem \ref{coro:symmcase} is to achieve a uniform control of $\upsilon_{j,d}$ in $j$, a result which may be of independent interest. We address it by combining state-of-the-art bounds on the roots of the Gegenbauer polynomials, allowing us to cover the regime where $j$ is small or comparable to $d$, together with integral representations via the Cauchy integral formula, providing control in the regime of large $j$. On the other hand, we relate the sum $\sum_j j^2 \beta_{j,d}$ to a norm of $\phi'$ using a Cauchy-Schwartz argument, where we leverage the fourth moments from Assumption \ref{ass:phi_symmetric_assumptions}. 
\begin{remark}
Since we are in a setting where $\phi$ is known, an alternative to the original recovery problem from Eq (\ref{eq:mainsetup}) is to consider a pure Gegenbauer `student' link function of the form $\tilde{\phi} = P_{s,d}$, where $s$ is the information exponent from Proposition \ref{prop:infoexpo_sphere}. Indeed, the resulting population loss 
$\tilde{L}(\theta) = \mathbb{E} [ (\tilde{\phi}(x \cdot \theta) - \tilde{\phi}( x \cdot \theta^*) )^2]$ satisfies the $\LPG$ property, as easily shown in Fact \ref{fact:gegenwellbeh}.
\end{remark}


% In Section \ref{fact:asympups}, we verify that $\upsilon_{j,d} \leq \min( C_j d^{-j/2}, \tilde{C}_d j^{-3d/4})$, with constants $C_j$ and $\tilde{C}_d$ such that 
% $C_j d^{-j/2} \leq \tilde{C}_d j^{-3d/4}$ whenever $j \leq A d$ for a constant $A$. 
% As a result, the sufficient condition for $\beta_j$ becomes 
% %
% \begin{equation}
% \label{eq:simpledecaycond}
% \sum_{j >s}^{A d} \beta_{j,d} j (j+d-2) C_j d^{-(j-s)/2}  + \tilde{C}_d d^{s/2} \sum_{j>A d} \beta_{j,d} (j+d-2) j^{1-3d/4}  \leq K d^{3/2} ~.
% \end{equation}
% %
% In words, this condition imposes very mild decay on the coefficients $\beta_{j,d}$ beyond their integrability $\sum_j \beta_{j,d} = \| \phi\|^2_{\eta} < \infty$. 
% In particular, as $\phi' \in L^2_\eta$, we verify in Fact \ref{fact:betas_derivative} that 
% $\sum_j j^2 \beta_{j,d} =  O(\| \phi'\|_{L^4_\eta}^2)~,$
% which implies the above condition (\ref{eq:simpledecaycond}). 

For the sake of completeness, we describe more precisely two concrete case studies below.

% \textit{Gaussian Case:}
% When $\nu$ is Gaussian, the population loss admits a simple representation in terms of the Hermite decomposition of $\phi$ \cite{arous2020online}: we have 
% \begin{equation}
% \label{eq:gaussianloss}
% L_{\mathrm{Gauss}}(\theta) := \bar{\ell}(m_\theta) = \|\alpha\|^2 - 2\sum_j \alpha_j^2 m^j_\theta  ~,  
% \end{equation}
%  with $\alpha_j = \langle \phi, h_j \rangle_{\gamma}$, where $h_j$ is the $j$-th Hermite polynomial. In that case we verify directly that $\nabla L(\theta)$ satisfies $\LPG(s-1,0)$, where $s=\inf\{ j; \alpha_j \neq 0\}$ is the so-called information exponent of $\phi$. 
% % $\ell'(m)$ has $(s,0,s-1)$ growth, where $s$ is the so-called information exponent of $\phi$, or number of vanishing moments, as established already in \cite{arous2020online}. 

\begin{example}[Uniform Measure on the Sphere]
When $\nu = \text{Unif}(\sqrt{d} \S)$, we have $\rho = \delta_{\sqrt{d}}$, and therefore $\beta_{j,d} = \bar{\alpha}^2_{j, \sqrt{d},d}$. 
In that case, the orthogonal polynomial basis $\{q_j(t)\}_j$ of $L^2_\eta$ coincides with the rescaled Gegenbauer polynomials, $q_j(t) = P_{j,d}(t/\sqrt{d}) $.
Consider now a link function 
$\phi$ with $s-1$ vanishing moments with respect to $L^2_\eta$, i.e. such that 
$\bar{\alpha}_{j,d}=\langle \phi, q_j \rangle_\eta = 0$ for $j < s$ and $\bar{\alpha}_{s,d}=\langle \phi, q_s \rangle_\eta \neq 0$; and with sufficient decay in the higher harmonics as to satisfy the bound on the sum presented in Proposition~\ref{prop:sufficientcond} (for example, $\phi(t) = q_s(t)$ trivially satisfies this condition). 
Then Proposition \ref{prop:sufficientcond} applies and we conclude that the resulting population landscape satisfies $\LPG(s-1, O(\sqrt{s/d}))$.
\end{example}

%has local polynomial growth of order $s-1$ at scale $O(1/\sqrt{d})$. 


In \cite{yehudai2020learning,wu2022learning} it is shown that monotonically increasing link functions\footnote{or link functions where their monotonic behavior dominates; see \cite{wu2022learning}.} lead to a benign population landscape, provided the data distribution $\nu$ satisfies mild anti-concentration properties. 
We verify that in our framework.
%
\begin{example}[Non-decreasing $\phi$]
    Indeed, Proposition \ref{prop:sufficientcond} is verified with $s=1$, provided $\phi' \in L^2_\eta$. Indeed, if $\phi\neq 0$ is monotonic, then we have
%$\mathbb{E}_\mu[ t \phi(t) ]\neq 0$
$\beta_1 = \langle \phi, \mathcal{K}_1 \phi \rangle_\eta = C_d \left(\mathbb{E}_{\eta} [t \phi(t)] \right)^2 \neq 0~,$
since we can assume without loss of generality that $\phi(t)\geq 0$ for $t\geq 0$ and $\phi(t) \leq 0$ for $t \leq 0$. 
\end{example}
%
We emphasize that the results of \cite{yehudai2020learning,wu2022learning} extend beyond the spherically symmetric setting, which is precisely the focus of next section. 

%


\subsection{Non-Spherically Symmetric Case}
\label{subsec:nonsym}
% {\color{blue}
% \begin{itemize}
%     \item Technical tool for non-symmetric case: the driving quantity is the approximate gaussianity of two-dimensional marginals. Consider a centered and  isometric data distribution $\nu \in \mathcal{P}_2(\mathbb{R}^d)$, ie such that $\mathbb{E}_\nu x=0$ and $\mathbb{E}_\nu [x x^\top] = I_d$. We consider the $2$-dimensional Wassertein distance \cite[definition 1]{niles2022estimation} 
%     \begin{equation}
%         \widetilde{W}_{1,2}(\nu, \mu) = \sup_{B \in \mathrm{Gr}(2, d)} W_1( \nu_B, \mu_B)~,
%     \end{equation}
%     where the sup runs for any two-dimensional subspace $B \in \mathrm{Gr}(2, d)$, and $\nu_B$ is the projection (or marginal) of $\nu$ onto $B$.  It is related to sliced Wassertein distance.
%   \item If
%     \begin{equation}
%     \label{eq:bill}
%         %\Delta(\nu) := \sqrt{d} \sup_{B \in \mathrm{Gr}(2, d)} W_1( \nu_B, \gamma_2)~.
%         \widetilde{W}_{1,2}( \nu, \gamma_d) \leq \frac{\Delta_\nu}{\sqrt{d}}~, 
%     \end{equation}
%     with $\Delta(\nu) = O_d(1)$, then we show 
%     that $\ell'(m)$ has local growth $(\Delta(\nu)/\sqrt{d}, 1)$.  
%     \item The $2$-dimensional Wasserstein distance between a measure and its empirical counterpart in fact also work for us. Rates are in ${1/\sqrt{d}}$. \cite[Proposition 8]{niles2022estimation}.
%     \item Which distributions satisfy (\ref{eq:bill})? If $\nu$ is exchangeable? Can we overcome the barrier $s=2$ ?
% \end{itemize}
% }
We now turn to the setting where $\nu$ is no longer assumed to have spherical symmetry. 
By making further regularity assumptions on $\phi$, our main insight is that distributions that are \emph{approximately} symmetric (defined in an appropriate sense) still benefit from a well-behaved optimization landscape. 

\paragraph*{Two-dimensional Wasserstein Distance.}
When $\nu$ is not spherically symmetric, the machinery of spherical harmonics does not apply, and we thus need to rely on another structural property. Consider a centered and  isometric data distribution $\nu \in \mathcal{P}_2(\mathbb{R}^d)$, i.e. such that $\mathbb{E}_\nu x=0$ and $\Sigma_\nu=\mathbb{E}_\nu [x x^\top] = I_d$. 
We consider the two-dimensional $1$-Wassertein distance \cite[Definition 1]{niles2022estimation} --see also \cite{paty2019subspace}-- between a pair of distributions $\nu_a, \nu_b \in \mathbb{P}(\mathbb{R}^d)$, defined as 
    \begin{equation}
        \widetilde{W}_{1,2}(\nu_a, \nu_b) := \sup_{P \in \mathrm{Gr}(2, d)} W_1( P_\# \nu_a, P_\# \nu_b)~,
    \end{equation}
    where the supremum runs for any two-dimensional subspace $P \in \mathrm{Gr}(2, d)$, and $P_\# \nu \in \mathcal{P}(\mathbb{R}^2)$ is the projection (or marginal) of $\nu$ onto the span of $P$. 
$\widetilde{W}_{1,2}$ is a distance (\cite[Proposition 1]{paty2019subspace}) and measures the largest $1$-Wasserstein distance between any two-dimensional marginals. 

We are in particular interested in the setting where $\nu_a = \nu$ is our data distribution, and $\nu_b$ is a reference symmetric measure --  for instance the standard Gaussian measure $\gamma_d$. Consider the fluctuations 
%
\begin{align}
\Delta_L(\theta) & := |L(\theta) - \bar{\ell}(m_\theta)| \text{ and }~,\\
\Delta_{\nabla L}(\theta) &:= | \nabla^{\mathcal{S}}_\theta L(\theta) \cdot \theta^* - \bar{\ell}'(m_\theta) (1 - m_\theta^2) |~,
\end{align}
%
where $\bar{\ell}(m)$ is the Gaussian loss defined in Section \ref{sec:preliminaries} and $L(\theta) = \mathbb{E}_\nu [ | \phi(  x \cdot \theta ) - \phi(  x \cdot \theta^* )|^2]$.
%= \sum_j \alpha_j^2 m^j$ with $\alpha_j = \langle \phi, h_j\rangle_\gamma$, where $h_j$ is the $j$th Hermite polynomial.
$\Delta_L$ and $\Delta_{\nabla L}$ thus measure respectively the fluctuations of the population loss and the relevant (spherical) gradient direction. 
By making additional mild regularity assumptions on the link function $\phi$, we can obtain a uniform control of the population loss geometry using the dual representation of the $1$-Wasserstein distance. 

\begin{restatable}[Regularity of link function]{assumption}{asslipreg}
%\begin{assumption}[Regularity of link function]
\label{ass:lipreg}
    We assume that $\phi, \phi'$ are both $B$-Lipschitz, 
    and that $\phi''(t) = O(1/t)$. 
    %{\color{red} TODO FIX: missing linear term. Need bounded support and not compatible with $\sqrt{d}$ scaling: double check. }
\end{restatable}
\begin{restatable}[Subgaussianity]{assumption}{asssubgaussian}
\label{ass:subgaussian}
    The data distribution $\nu$ is $M$-subgaussian: for any~$v \in \S$, we have $\|  x\cdot v  \|_{\psi_2} \leq M $, where $\|z\|_{\psi_2}:= \inf\{ t>0; \, \mathbb{E}[\exp(z^2/t^2)\leq 2\}$ is the Orlitz-2 norm.        
\end{restatable}

%Indeed, $\widetilde{W}_{1,2}(\nu, \gamma)$ controls the uniform deviations of 
\begin{restatable}[Uniform gradient approximation]{proposition}{gradwassers}
    \label{prop:gradwassers}
    Under Assumptions \ref{ass:lipreg} and \ref{ass:subgaussian}, for all~$\theta \in \S$, 
%    Something there to prevent the compilation error.
\begin{equation}
    %\sup_{\theta \in \S} | \langle \nabla^{\mathbb{S}}_\theta L(\theta), \theta^* \rangle - \bar{\ell}'(m) (1 - m^2) | \leq 2(L'B + L^2) \widetilde{W}_{1,2}(\nu, \gamma)~,
%    \sup_{\theta \in \S} | \langle \nabla^{\mathbb{S}}_\theta L(\theta), \theta^* \rangle - \bar{\ell}'(m) (1 - m^2) | \leq \tilde{C} \widetilde{W}_{1,2}(\nu, \gamma) \left(\log(\widetilde{W}_{1,2}(\nu, \gamma)^{-1})\right)^2~,
%      | \langle \nabla^{\mathbb{S}}_\theta L(\theta), \theta^* \rangle - \bar{\ell}'(m) (1 - m^2) | \leq \sqrt{1-m^2} \tilde{C} \widetilde{W}_{1,2}(\nu, \gamma) \left(\log(\widetilde{W}_{1,2}(\nu, \gamma)^{-1})\right)^2~,
     \Delta_{\nabla L}(\theta) = (1-m^2) O\left( \widetilde{W}_{1,2}(\nu, \gamma) \log ( \widetilde{W}_{1,2}(\nu, \gamma)^{-1} ) \right)
     %(1-m^2)^{3/2} \widetilde{W}_{1,2}(\nu, \gamma) \left|\log\left(\left((1-m^2) \widetilde{W}_{1,2}(\nu, \gamma)\right)^{-1/C} \right)\right|~,
\end{equation}
where the $O(\cdot)$ notation only hides constants appearing in Assumptions \ref{ass:lipreg} and \ref{ass:subgaussian}.   
\end{restatable}
% \begin{proposition}[Uniform gradient approximation]

% \end{proposition}
In words, the population gradient under $\nu$ is viewed as a perturbation of the population
gradient under $\gamma$, which has the well-behaved geometry already described in Section~\ref{sec:preliminaries}. These perturbations 
can be uniformly controlled by the projected 1-Wasserstein distance, thanks to the subgaussian tails of $\nu$. 

Our focus will be in situations where $\widetilde{W}_{1,2}(\nu, \gamma)=O(1/\sqrt{d})$. This happens to be the `natural' optimistic scale for this metric in the class of isotropic distributions $\Sigma_\nu = I_d$, as can be seen for instance when $\nu = \mathrm{Unif}(\sqrt{d} \S)$. Under such conditions, it turns out that link functions with information exponent $s\leq 2$ can be recovered with simple gradient-based methods, by paying an additional polynomial (in $d$) cost in time complexity. 

\begin{assumption}
\label{ass:smallinfoexponent}
    The Gaussian information exponent of $\phi$, $s:=\arg\min\{ j; \langle \phi, H_j \rangle  \neq 0\}$ satisfies $s \leq 2$. 
\end{assumption}
\begin{assumption}
\label{ass:wassersteinsmall} The projected Wasserstein distance satisfies
    $\widetilde{W}_{1,2}(\nu, \gamma) \leq M'/\sqrt{d}$.
\end{assumption}
\begin{restatable}[$\LPG$, non-symmetric setting]{proposition}{propgrad_expo2}
\label{prop:grad_expo2}
    Under Assumptions \ref{ass:lipreg}, \ref{ass:subgaussian}, \ref{ass:smallinfoexponent} and \ref{ass:wassersteinsmall},~$L$ verifies $\LPG\left(1,O\left(\sqrt{\frac{(\log d^\kappa)}{d}}\right)\right)$, where $\kappa$ depends only on $B,M,M'$. %and the contraction property.
\end{restatable}
This proposition illustrates the cost of breaking spherical symmetry in two aspects: (i) it requires additional regularity on $\phi$, and notably restricts its (Gaussian) information exponent to $s=2$, and (ii) the scale to reach $\LPG$ is now no longer dimension-free, but has a polynomial dependency on dimension, since from Lemma \ref{lem:initialization}, picking $\delta$ any positive constant we have 
$$\P_{\theta_0}\left(m_{\theta_0} \geq \sqrt{\log d^\kappa}/\sqrt{d}\right) \geq \frac{\delta}{4}  e^ {- (\sqrt{\log d^\kappa} + \delta)^2} \geq \Omega\left( d^{-(1+o(1))\kappa}\right)~. $$
%$\mathbb{P}\left( m_0 \geq \sqrt{\frac{(\log d^\kappa)}{d}}\right) \geq \log d^{\kappa} e^{-\log d^\kappa } = d^{\Theta(1)-\kappa}$. 
At present, we are not able to rule this out as a limitation of our proof;  establishing whether this polynomial dependency on dimension is an inherent cost of the symmetry breaking is an interesting question for future work. 

While assumptions \ref{ass:lipreg}, \ref{ass:subgaussian} and \ref{ass:smallinfoexponent} are transparent and impose only mild conditions on the link function and tails of $\nu$, the `real' assumption of Proposition \ref{prop:grad_expo2} is the concentration of $\widetilde{W}_{1,2}(\nu, \gamma)$ (Assumption \ref{ass:wassersteinsmall}). The ball $\{ \nu; \widetilde{W}_{1,2}(\nu, \gamma) = O(1/\sqrt{d}) \}$ contains many non-symmetric measures, for instance empirical measures sampled from $\gamma$ with $n = \omega(d^2)$ \cite[Proposition 8]{niles2022estimation}, and we suspect it contains many other examples, such as convolutions of the form $\nu \ast \gamma_{\sigma}$ arising for instance in diffusion models. That said, one should \emph{not} expect the distance $\widetilde{W}_{1,2}(\nu, \gamma)$ to be of order $1/\sqrt{d}$ for generic `nice' distributions $\nu$; for instance, log-concave distributions are expected to satisfy $W_1( P_\# \nu, \gamma) \simeq 1/\sqrt{d}$ for \emph{most} subspaces $P$, as captured in Klartag's CLT for convex bodies \cite{klartag2007central}. In summary, many situations of interest fall outside this regime, which motivates us to relax the uniform Wasserstein criterion. 

% {\color{red} TODO discussion on {\bf
% Which non-symmetric distributions $\nu$ satisfy $\widetilde{W}_{1,2}(\nu, \gamma) = O(1/\sqrt{d})$? }
% Replace $\gamma$ by the \emph{symmetrisation} of $\nu$. Discuss how this would affect the answer. }
%While Proposition \ref{prop:grad_expo2} captures non-symmetric data distributions, it relies on the strong assumption that $W_1( P_\#\nu, \gamma_2) = O(1/\sqrt{d})$ \emph{uniformly} over all subspaces; in other words, the data distribution should look approximately Gaussian (at scale $O(1/\sqrt{d})$) when projected onto \emph{any} two-dimensional subspace. 


\paragraph{Localized Growth via Stein's method.}
To illustrate the mileage of the previous techniques beyond this `quasi-symmetric' case, we consider now an idealised setting where the data is drawn from a product measure $\nu = \eta^{\otimes d}$, with $\eta \in \mathcal{P}(\mathbb{R})$ and $W_1( \eta, \gamma_1) = \Theta(1)$. In other words, $x=(x_1, \ldots, x_d) \sim \nu$ if $x_i \sim \eta$ are i.i.d. In this setting, the distances $W_1( P_\# \nu, \gamma)$ reflect a CLT phenomena, which requires the subspace $P$ to `mix' across independent variables. Consequently, one may expect the expression of the hidden direction $\theta^*$ in the canonical basis to play a certain role.
For that purpose, we make the following additional regularity assumption on the tails of $\phi$ to simplify the quantitative bounds: 
\begin{restatable}[Additional Regularity in third derivatives] {assumption}{assthird}
\label{ass:extralip}
    $\phi$ admits four derivatives bounded by $L$, with $|\phi^{(3)}(t)| = O(1/t)$ and $|\phi^{(4)}(t)| = O(1/t^2)$. Moreover, the third moment of the data distribution is finite:
    $\tau_3=\mathbb{E}_{t \sim \eta}[t^3] < \infty$.    
\end{restatable}
Stein's method provides a powerful control on $\Delta_L(\theta)$ and 
$\Delta_{\nabla L}(\theta)$, as shown by the following result:
%by leveraging the product measure structure of $\nu$:
\begin{restatable}[Stein's method for product measure]{proposition}{propstein}
\label{prop:stein}
    Let $\chi(\theta, \theta^*):= \| \theta\|_4^2 + \| \theta^*\|_4^2$. 
    Under Assumptions \ref{ass:lipreg}, \ref{ass:subgaussian} and \ref{ass:extralip}, there exists a universal constant $C=C(M, B, \tau_3)$ such that %and $\tilde{C}=\tilde{C}(M, L, L', L'',\tau_3)$ such that  
        \begin{equation}
        \label{eq:loss_conc}
        \Delta_L(\theta) \leq C \chi(\theta, \theta^*)~,\text{and }~\Delta_{\nabla L}(\theta) \leq C \sqrt{1-m^2} \chi(\theta, \theta^*)~.
%        \max(\Delta_{L}(\theta), \Delta_{\nabla L}(\theta)) \leq C (1-m_\theta^2) O\left(\chi(\theta, \theta^*) \log( \chi(\theta, \theta^*)^{-1} ) \right)~.        
     %   \left(\log \chi(\theta, \theta^*)(1-m_\theta^2) \right)^2~.
%        \left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right) \left(\log\left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right)\right)^2~.%\left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right) ~,
    \end{equation}    
\end{restatable}
% \begin{proposition}[Stein's method for product measure]
% \label{prop:stein}
%     Let $\chi(\theta, \theta^*):= \| \theta\|_4^2 + \| \theta^*\|_4^2$. 
%     Under Assumptions \ref{ass:lipreg}, \ref{ass:subgaussian} and \ref{ass:extralip}, there exists a universal constant $C=C(M, L, \tau_3)$ such that %and $\tilde{C}=\tilde{C}(M, L, L', L'',\tau_3)$ such that  
%         \begin{equation}
%         \label{eq:loss_conc}
%         \max(\Delta_{L}(\theta), \Delta_{\nabla L}(\theta)) \leq C (1-m_\theta^2) O\left(\chi(\theta, \theta^*) \log( \chi(\theta, \theta^*)^{-1} ) \right)~.        
%      %   \left(\log \chi(\theta, \theta^*)(1-m_\theta^2) \right)^2~.
% %        \left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right) \left(\log\left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right)\right)^2~.%\left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right) ~,
%     \end{equation}
%     % and
%     % \begin{equation}
%     % \label{eq:grad_conc}
%     %     \Delta_{\nabla L}(\theta) \leq \tilde{C} \left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right) \left(\log\left( \|\theta\|_3^3 + \| \theta\|_4^2 + \|\theta^*\|_3^3 + \| \theta^*\|_4^2 \right)\right)^2~.
%     % \end{equation}
% \end{proposition}
The proof is based on the Stein's method for multivariate variables \cite[Theorem 3.1]{rollin2013stein} with independent entries, which provides a quantitative CLT bound. Contrary to the quasi-symmetric case, here the concentration is not uniform over the sphere, but crucially depends on the sparsity of \emph{both} $\theta$ and $\theta^*$, measured via the $\ell_4$ norms $\| \theta \|_4$, $\| \theta^* \|_4$: for incoherent, non-sparse directions, we have $\|\theta \|_4^2 \simeq 1/\sqrt{d}$, recovering the concentration rate that led to Proposition \ref{prop:grad_expo2},  
while for sparse directions we have $\| \theta\|_4^2 = \Theta(1)$, indicating an absence of concentration to the Gaussian landscape.

Therefore, the natural conclusion is to assume a planted model where $\theta^*$ is \emph{incoherent} with the data distribution, i.e. $\| \theta^* \|_4 = O(d^{1/4})$. While the $\LPG$ property does not directly apply in this setting, we outline an argument that suggests that the single-index model can still be efficiently solved using gradient-based methods. 
For that purpose, we assume that $\theta^*$ is drawn uniformly in $\S$, which implies that its squared-$L^4$ norm $\| \theta^*\|_4^2$ is of order $d^{-1/2}$ with high probability:
\begin{fact}
    Assume $\theta^* \sim \mathrm{Unif}(\S)$. Then $\mathbb{P}( \| \theta^*\|_4^2 \leq C /\sqrt{d}) \geq 1 - C'\exp(-C)$. 
\end{fact}

Because $\theta_0$ is also drawn uniformly on the sphere, the typical value of $\chi(\theta, \theta^*)$ is of order $d^{-1/2}$. For Gaussian information exponent $s=2$, the population gradient under Gaussian data satisfies $-\nabla_\theta^{\S}L_\gamma(\theta) \cdot \theta^* \geq C m_\theta$. As a consequence, by Proposition \ref{prop:stein}, whenever $|\theta_0 \cdot \theta^*| > c \sqrt{d}$ (which happens with constant probability lower bounded by $e^{-c}$), we enter a `local' $\LPG$ region where $-\nabla_\theta^{\S}L(\theta) \cdot \theta^* \geq C (m - c/\sqrt{d}) > 0$. 
While this condition is sufficient in the quasi-symmetric setting to start accumulating correlation (Theorem \ref{thm:weak_recovery}), now this event is conditional on $\theta$ being dense, ie so that $\chi(\theta, \theta^*) = O(1/\sqrt{d})$. 

Since the typical value of $\chi(\theta, \theta^*)$ is of scale $1/\sqrt{d}$, one would expect that SGD will rarely visit sparse points where $\chi(\theta, \theta^*)\gg O(1/\sqrt{d})$, and thus that the local $\LPG$ property will be valid for \emph{most} times during the entropic phase of weak recovery --- 
and therefore that the correlation $m_\theta$ will pile-up as in the quasi-symmetric setting. 

We summarise this property in the following conjecture:
\begin{conjecture}[SGD avoids sparse points]
\label{conj:SGDparse}
    Assume $\theta^*, \theta_0$ are drawn from the uniform measure, and let $\theta_t$ be the $t$-th iterate of SGD with $\delta \simeq 1/(d \log d)$.  
    There exists a universal constant $C$ such that for any $\xi>0$, we have %the probability that the first $T$ iterations of SGD satisfies
    \begin{equation}
        \mathbb{P}\left( \sup_{t \leq T} \| \theta_t\|_4^2 \geq \sqrt{\frac{{\xi} \log T }{{d}}} \right) \leq C \exp(-\xi^2 d)~.
    \end{equation}
\end{conjecture}
Since the time to escape mediocrity in the case $s=2$ is $T \simeq d \log(d)^2$, this conjecture would imply that SGD does not effectively `see' any sparse points, and thus escapes mediocrity. If one assumed that in this phase the dynamics is purely noisy, now pretending that $\theta_i$ were drawn independently from the uniform measure, and that $\|\theta\|_4^4$ is approximately Gaussian with mean $d^{-1}$ and variance $d^{-3}$, the result follows by simple concentration. The challenging aspect of Conjecture \ref{conj:SGDparse} is precisely to handle the dependencies across iterates, as well as the spherical projection steps. 





% \subsection{Case Studies}
% \label{sec:casestudies_nonsym}



% Berry-Esseeen for Wasserstein distance. 

% CLT

% Mean-Field Models

% Weakly-interacting Gibbs models (mention it future work)

\section{Experiments}

In order to validate our theory, and inspect the degree to which our bounds may be pessimistic, we consider empirical evaluation of the training process in our two primary settings.  Specifically, we consider random initialization on the half-sphere (with the sign chosen to induce positive correlation as in \cite{arous2021online}), and investigate how often strong recovery occurs relative to the information exponent of the link function.

% Figure environment removed

% Figure environment removed


\paragraph*{Symmetric Case}

For the spherically symmetric setting, we experiment with the input distribution that is uniform on the sphere.  We are primarily interested in verifying that, unlike the Gaussian case, strong recovery depends on whether the initial correlation is sufficiently high to avoid local minima and benefit from the $\LPG$ guarantee.  This is not evident in the 2nd degree Gegenbauer case, which is monotonic and quickly reaches strong recovery, but it is clear from the 4th degree Gegenbauer link function.

In the infinite sample setting, Figure~\ref{fig:gegen} exactly characterizes the loss landscape when learning the 4th degree Gegenbauer under inputs uniform on $\S$ for different values of $d$.  Note that the largest zero for $d = 50$ occurs at $\approx \pm 0.31$, and the loss is monotonic for $m$ values initialized outside that region.  This phenomenon persists for higher dimensions, and one may observe that $d$ increases, the critical points become smaller in magnitude, according to the scaling $\simeq \sqrt{1/d}$.

The bottom right subplot in Figure~\ref{fig:runs} indicates training runs in this setting, where red lines are initialized uniformly on the sphere, and blue lines are initialized uniformly conditioned on $m = 0.4$, which is slightly past the last zero of the polynomial.  We observe that random initialization infrequently exceeds the threshold necessary for strong recovery, but planting the initialization above this threshold gives a high probability of recovery.

\paragraph*{Non-Symmetric Case}


For the non-spherically symmetric setting, we compare the performance of Gaussian inputs with inputs that are approximately Gaussian under a two-dimensional projection.  For simplicity, we loosen our assumptions slightly, and consider the input distribution as the $d$ dimensional product distribution of uniform random variables (rescaled to have unit variance), and allow for a non-Lipschitz link function.  Here, we are primarily interested in whether Assumption~\ref{ass:smallinfoexponent} is tight and $s \leq 2$ is necessary for recovery, as well as whether Conjecture~\ref{conj:SGDparse} holds in practice.

To evaluate, we compare strong recovery rates when training on a "tricky" function with $s = 2$ (chosen to be $\frac{1}{2} \left(h_2 - h_3 - h_4 + h_5 \right)$) versus a function with $s = 3$ (simply the degree three hermite polynomial $h_3$).  We make this choice for the $s = 2$ function in order to produce a function which is not monotonic, for which learning is easy under many distributions, as discussed in~\cite{yehudai2020learning}.

In Figure~\ref{fig:runs} we observe that strong recovery reliably occurs for both the Gaussian and hypercube input distributions when $s = 2$.  There is more variance in the Gaussian runs, likely because the magnitude of the gradients will be larger due to the inclusion of high degree terms.  But for the $s = 3$ case, the Gaussian distribution converges quickly while the hypercube distribution frequently cannot escape the equator.

\section{Conclusion and Perspectives}

In this work, we have asked whether the remarkable properties of high-dimensional Gaussian SGD regression of single-index models are preserved as one loses some key aspects that make Gaussian distributions so special (and so appealing for theorists). 
Our results are mostly positive, indicating a robustness of the Gaussian theory, especially within the class of spherically symmetric distributions, where a rich spherical harmonic structure is still available.  As one loses spherical symmetry, the situation becomes more dire, motivating a perturbative analysis that we have shown is effective via projected Wasserstein and Stein couplings. 

That said, there are several open and relevant avenues that our work has barely touched upon, such as  understanding whether the robustness can be transferred to other algorithms beyond SGD, or addressing the semi-parametric problem when the link function is unknown, along the lines of \cite{biettilearning2022,abbe2023sgd,damian2022neural,berthier2023learning}. A particularly interesting direction of future work is to extend the analysis of product measures to `weakly dependent' distributions, motivated by natural images where locality in pixels captures most (but not all) of the statistical dependencies. Stein's method appears to be a powerful framework that can accommodate such weak dependencies, and deserves future investigation. 


\bibliographystyle{apalike}
\bibliography{references}


\clearpage


\appendix
%\appendixpage

$ $

{\Huge \textsc{Appendix}}

\vspace{0.5cm}

We gather in the appendix the proofs of the theorems, propositions and lemmas stated in the main text. In Section~\ref{sec:proof_lemma_1}, the reader will find a short proof of Lemma \ref{lem:initialization}. In Section~\ref{sec:sgdapp}, we prove Theorems~\ref{thm:weak_recovery} and \ref{thm:strong_recovery} on the SGD dynamics. Sections~\ref{sec:symapp} and~\ref{sec:nonsymapp} are respectively devoted to prove that the $\LPG$ property holds in some spherical symmetric case and under some perturbative regime.

% \vspace{0.5cm}

% \par\noindent\rule{\textwidth}{.5pt}
% \rule[.8\baselineskip]{\textwidth}{.5pt}
% \vspace{-1cm}

% \startcontents[sections]
% \printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}

% \par\noindent\rule{\textwidth}{.5pt}
% \rule[.8\baselineskip]{\textwidth}{.5pt}


\input{appendix}



\end{document}