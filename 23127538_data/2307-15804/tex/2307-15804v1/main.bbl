\begin{thebibliography}{}

\bibitem[Abbe et~al., 2022]{abbe2022merged}
Abbe, E., Boix-Adsera, E., and Misiakiewicz, T. (2022).
\newblock The merged-staircase property: a necessary and nearly sufficient
  condition for sgd learning of sparse functions on two-layer neural networks.
\newblock {\em arXiv preprint arXiv:2202.08658}.

\bibitem[Abbe et~al., 2023]{abbe2023sgd}
Abbe, E., Boix-Adsera, E., and Misiakiewicz, T. (2023).
\newblock Sgd learning on neural networks: leap complexity and saddle-to-saddle
  dynamics.
\newblock {\em arXiv preprint arXiv:2302.11055}.

\bibitem[Area et~al., 2004]{Area2004ZerosOG}
Area, I., Dimitrov, D.~K., Godoy, E., and Ronveaux, A. (2004).
\newblock Zeros of gegenbauer and hermite polynomials and connection
  coefficients.
\newblock {\em Math. Comput.}, 73:1937--1951.

\bibitem[Arnaboldi et~al., 2023]{arnaboldi2023high}
Arnaboldi, L., Stephan, L., Krzakala, F., and Loureiro, B. (2023).
\newblock From high-dimensional \& mean-field dynamics to dimensionless odes: A
  unifying approach to sgd in two-layers networks.
\newblock {\em arXiv preprint arXiv:2302.05882}.

\bibitem[Arous et~al., 2021]{arous2021online}
Arous, G.~B., Gheissari, R., and Jagannath, A. (2021).
\newblock Online stochastic gradient descent on non-convex losses from
  high-dimensional inference.
\newblock {\em The Journal of Machine Learning Research}, 22(1):4788--4838.

\bibitem[Bach, 2017]{bach2017breaking}
Bach, F. (2017).
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):629--681.

\bibitem[Barak et~al., 2022]{barak2022hidden}
Barak, B., Edelman, B., Goel, S., Kakade, S., Malach, E., and Zhang, C. (2022).
\newblock Hidden progress in deep learning: Sgd learns parities near the
  computational limit.
\newblock {\em Advances in Neural Information Processing Systems},
  35:21750--21764.

\bibitem[Barron, 1993]{barron1993universal}
Barron, A.~R. (1993).
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information theory}, 39(3):930--945.

\bibitem[Ben~Arous et~al., 2021]{arous2020online}
Ben~Arous, G., Gheissari, R., and Jagannath, A. (2021).
\newblock Online stochastic gradient descent on non-convex losses from
  high-dimensional inference.
\newblock {\em Journal of Machine Learning Research (JMLR)}, 22:106--1.

\bibitem[Ben~Arous et~al., 2022]{arous2022high}
Ben~Arous, G., Gheissari, R., and Jagannath, A. (2022).
\newblock High-dimensional limit theorems for sgd: Effective dynamics and
  critical scaling.
\newblock {\em arXiv preprint arXiv:2206.04030}.

\bibitem[Berthier et~al., 2023]{berthier2023learning}
Berthier, R., Montanari, A., and Zhou, K. (2023).
\newblock Learning time-scales in two-layers neural networks.
\newblock {\em arXiv preprint arXiv:2303.00055}.

\bibitem[Bietti et~al., 2022]{biettilearning2022}
Bietti, A., Bruna, J., Sanford, C., and Song, M.~J. (2022).
\newblock Learning single-index models with shallow neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Chen et~al., 2023]{chen2023learning}
Chen, S., Dou, Z., Goel, S., Klivans, A.~R., and Meka, R. (2023).
\newblock Learning narrow one-hidden-layer relu networks.
\newblock {\em arXiv preprint arXiv:2304.10524}.

\bibitem[Damian et~al., 2022]{damian2022neural}
Damian, A., Lee, J., and Soltanolkotabi, M. (2022).
\newblock Neural networks can learn representations with gradient descent.
\newblock In {\em Conference on Learning Theory}.

\bibitem[De~Carli, 2008]{de2008local}
De~Carli, L. (2008).
\newblock Local lp inequalities for gegenbauer polynomials.
\newblock In {\em Topics in classical analysis and applications in honor of
  Daniel Waterman}, pages 73--87. World Scientific.

\bibitem[Diakonikolas et~al., 2017]{diakonikolas2017statistical}
Diakonikolas, I., Kane, D.~M., and Stewart, A. (2017).
\newblock Statistical query lower bounds for robust estimation of
  high-dimensional gaussians and gaussian mixtures.
\newblock In {\em 2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 73--84. IEEE.

\bibitem[Dimitrov and Nikolov, 2010]{DIMITROV20101793}
Dimitrov, D.~K. and Nikolov, G.~P. (2010).
\newblock Sharp bounds for the extreme zeros of classical orthogonal
  polynomials.
\newblock {\em Journal of Approximation Theory}, 162(10):1793--1804.
\newblock Special Issue dedicated to the memory of Borislav Bojanov.

\bibitem[{\relax DLMF}, 2022]{NIST:DLMF}
{\relax DLMF} (2022).
\newblock {\it NIST Digital Library of Mathematical Functions}.
\newblock \url{https://dlmf.nist.gov/}, Release 1.1.10 of 2023-06-15.
\newblock F.~W.~J. Olver, A.~B. {Olde Daalhuis}, D.~W. Lozier, B.~I. Schneider,
  R.~F. Boisvert, C.~W. Clark, B.~R. Miller, B.~V. Saunders, H.~S. Cohl, and
  M.~A. McClain, eds.

\bibitem[Driver and Jordaan, 2012]{DRIVER20121200}
Driver, K. and Jordaan, K. (2012).
\newblock Bounds for extreme zeros of some classical orthogonal polynomials.
\newblock {\em Journal of Approximation Theory}, 164(9):1200--1204.

\bibitem[Dudeja and Hsu, 2018]{dudeja2018learning}
Dudeja, R. and Hsu, D. (2018).
\newblock Learning single-index models in gaussian space.
\newblock In Bubeck, S., Perchet, V., and Rigollet, P., editors, {\em
  Proceedings of the 31st Conference On Learning Theory}, volume~75 of {\em
  Proceedings of Machine Learning Research}, pages 1887--1930. PMLR.

\bibitem[Frei et~al., 2020]{frei2020agnostic}
Frei, S., Cao, Y., and Gu, Q. (2020).
\newblock Agnostic learning of a single neuron with gradient descent.
\newblock {\em Advances in Neural Information Processing Systems},
  33:5417--5428.

\bibitem[Frye and Efthimiou, 2012]{frye2012spherical}
Frye, C. and Efthimiou, C.~J. (2012).
\newblock Spherical harmonics in p dimensions.
\newblock {\em arXiv preprint arXiv:1205.3548}.

\bibitem[Gautschi, 1959]{gautschi1959some}
Gautschi, W. (1959).
\newblock Some elementary inequalities relating to the gamma and incomplete
  gamma function.
\newblock {\em J. Math. Phys}, 38(1):77--81.

\bibitem[Goel et~al., 2020]{goel2020superpolynomial}
Goel, S., Gollakota, A., Jin, Z., Karmalkar, S., and Klivans, A. (2020).
\newblock Superpolynomial lower bounds for learning one-layer neural networks
  using gradient descent.
\newblock In {\em International Conference on Machine Learning}, pages
  3587--3596. PMLR.

\bibitem[Kakade et~al., 2011]{kakade2011efficient}
Kakade, S.~M., Kanade, V., Shamir, O., and Kalai, A. (2011).
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock {\em Advances in Neural Information Processing Systems}, 24.

\bibitem[Kalai and Sastry, 2009]{kalai2009isotron}
Kalai, A.~T. and Sastry, R. (2009).
\newblock The isotron algorithm: High-dimensional isotonic regression.
\newblock In {\em COLT}.

\bibitem[Klartag, 2007]{klartag2007central}
Klartag, B. (2007).
\newblock A central limit theorem for convex sets.
\newblock {\em Inventiones mathematicae}, 168(1):91--131.

\bibitem[Laforgia and Natalini, 2013]{laforgia2013some}
Laforgia, A. and Natalini, P. (2013).
\newblock On some inequalities for the gamma function.
\newblock {\em Advances in Dynamical Systems and Applications}, 8(2):261--267.

\bibitem[Niles-Weed and Rigollet, 2022]{niles2022estimation}
Niles-Weed, J. and Rigollet, P. (2022).
\newblock Estimation of wasserstein distances in the spiked transport model.
\newblock {\em Bernoulli}, 28(4):2663--2688.

\bibitem[Paty and Cuturi, 2019]{paty2019subspace}
Paty, F.-P. and Cuturi, M. (2019).
\newblock Subspace robust wasserstein distances.
\newblock In {\em International conference on machine learning}, pages
  5072--5081. PMLR.

\bibitem[R{\"o}llin, 2013]{rollin2013stein}
R{\"o}llin, A. (2013).
\newblock Stein's method in high dimensions with applications.
\newblock In {\em Annales de l'IHP Probabilit{\'e}s et statistiques},
  volume~49, pages 529--549.

\bibitem[Shalev-Shwartz et~al., 2010]{shalev2010learning}
Shalev-Shwartz, S., Shamir, O., and Sridharan, K. (2010).
\newblock Learning kernel-based halfspaces with the zero-one loss.
\newblock {\em arXiv preprint arXiv:1005.3681}.

\bibitem[Shamir, 2018]{shamir2018distribution}
Shamir, O. (2018).
\newblock Distribution-specific hardness of learning neural networks.
\newblock {\em The Journal of Machine Learning Research}, 19(1):1135--1163.

\bibitem[Soltanolkotabi, 2017]{soltanolkotabi2017learning}
Soltanolkotabi, M. (2017).
\newblock Learning relus via gradient descent.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Song et~al., 2021]{song2021cryptographic}
Song, M.~J., Zadik, I., and Bruna, J. (2021).
\newblock On the cryptographic hardness of learning single periodic neurons.
\newblock {\em Advances in Neural Processing Systems (NeurIPS)}.

\bibitem[Stam, 1982]{stam1982limit}
Stam, A.~J. (1982).
\newblock Limit theorems for uniform distributions on spheres in
  high-dimensional euclidean spaces.
\newblock {\em Journal of Applied probability}, 19(1):221--228.

\bibitem[Szego, 1939]{szego1939orthogonal}
Szego, G. (1939).
\newblock {\em Orthogonal polynomials}, volume~23.
\newblock American Mathematical Soc.

\bibitem[Ursell, 2007]{ursell2007integrals}
Ursell, F. (2007).
\newblock Integrals with nearly coincident branch points: Gegenbauer
  polynomials of large degree.
\newblock {\em Proceedings of the Royal Society A: Mathematical, Physical and
  Engineering Sciences}, 463(2079):697--710.

\bibitem[Watson, 1922]{watson1922treatise}
Watson, G.~N. (1922).
\newblock {\em A treatise on the theory of Bessel functions}, volume~3.
\newblock The University Press.

\bibitem[Wu, 2022]{wu2022learning}
Wu, L. (2022).
\newblock Learning a single neuron for non-monotonic activation functions.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4178--4197. PMLR.

\bibitem[Yehudai and Shamir, 2020]{yehudai2020learning}
Yehudai, G. and Shamir, O. (2020).
\newblock Learning a single neuron with gradient methods.
\newblock In {\em Conference on Learning Theory}, pages 3756--3786. PMLR.

\end{thebibliography}
