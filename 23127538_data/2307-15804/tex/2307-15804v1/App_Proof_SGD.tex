\section{Proofs on the SGD dynamics: Section \ref{sec:sgd}}
\label{sec:sgdapp}

We first recall the notations useful to fully describe the dynamics. In Section~\ref{subsec:weak_recovery}, we prove Theorem~\ref{thm:weak_recovery} about weak recovery. In Section~\ref{subsec:strong_recovery}, we prove Theorem~\ref{thm:strong_recovery} about strong recovery. Finally, 

\subsection{Recalling the dynamics}

For the sake of clarity, let us recall the notations and facts developed in the main text. The overall loss classically corresponds to the average over all the \textit{data} of a square penalisation $l(\theta, x) = (\phi_\theta(x) - \phi_{\theta^*}(x))^2$ so that 
%
$$L(\theta) = \mathbb{E}_{\nu} [(\phi_\theta(x) - \phi_{\theta^*}(x))^2].$$
%
To recover the signal given by $\theta^*$, we run \textit{online stochastic gradient descent} on the sphere $\S$. This corresponds to have at each iteration $t \in \mathbb{N}^*$ a \textit{fresh sample} $x_t$ independent of the filtration $\mathcal{F}_t = \sigma(x_1, \dots, x_{t-1})$ and perform a spherical gradient step, with step-size $\delta>0$, with respect to $\theta \to l(\theta, x_t)$:
%
\begin{align}
    \label{eq:SGD_app}
    \theta_{t+1} &= \frac{\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)}{\left|\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)\right|},
\end{align}
%
initialized at $\theta_0$ uniformly on the sphere: $\theta_{0} \sim \mathrm{Unif}(\S)$. Recall that we use the notation $\nabla_\theta^\mathcal{S}$ to denote the spherical gradient, that is $$ \nabla_\theta^\mathcal{S} l(\theta, x) =  \nabla_\theta  l(\theta, x) - (\nabla_\theta  l(\theta, x) \cdot \theta) \theta.$$
%
Let us introduce the following frequently used notations: for all $t \in \mathbb{N}^*$, we denote the normalization by $r_t := r(\theta_t, x_t) = \left|\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)\right|$ and the martingale induced by the stochastic gradient descent as $M_t = M(\theta_t, x_t) = l(\theta_t, x_t) - \E_\nu [ l(\theta_t, x)]$.

\subsection{Tracking the correlation.} Recall that the relevant signature of the dynamics is the one-dimensional correlation: $m_t = \theta_t \cdot \theta^*$. Let us re-write the iterative recursion followed by $(m_t)_{t \geq 0}$, with the notation recalled above, for $t \in \mathbb{N}^*$, 
%
\begin{align}
\label{eq:dynamics_main_m_app}
m_{t+1} = \frac{1}{r_t}\left( m_t - \delta \nabla^S l(\theta_t, x_t) \cdot \theta^*  \right) =  \frac{1}{r_t}\left( m_t - \delta\nabla^S L(\theta_t) \cdot \theta^* - \delta\nabla^S M_t \cdot \theta^*  \right).
\end{align}
%
We want to lower bound the right hand side of~\eqref{eq:dynamics_main_m_app}. We begin by a lower bound on~$1/r_t$. 

%
\begin{lemma}[Bound on $r_t$]
    For all $t \in \N^*$, we have $1/r_t \geq 1 - 2 \delta^2 (K + \overline{M}_t)$, where $\overline{M}_t = \left|\nabla_\theta M_t \right|^2 $.    
\end{lemma}
%
\begin{proof}
    For all $t \in \N^*$, we have, by orthogonality of as $\theta_t$ and $\nabla_\theta^\mathcal{S}  l(\theta_t, x_t)$, that
    %
    \begin{align*}
        r_t^2 = \left|\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)\right|^2  = 1 + \delta^2 \left|\nabla_\theta^\mathcal{S} l(\theta_t, x_t)\right|^2 \leq 1 + 2 \delta^2 \left(\left|\nabla_\theta L \right|^2 + \left|\nabla_\theta M_t \right|^2\right).
    \end{align*}
    %
    Furthermore, $\left|\nabla_\theta L \right|^2 \leq K $ by Lemma~\ref{lem:tech_bounds}. Hence, we have $r_t^2 \leq 1 + 2 \delta^2 (K + \overline{M}_t)$, and we conclude as thanks to the inequality $ (1 + u)^{-1/2} \geq 1 - u $ for all $ u > 0$.
\end{proof}
%
Thanks the fact that $L$ satisfies $\LPG(s,b/\sqrt{d})$, ie $-\nabla^\mathcal{S}  L(\theta) \cdot \theta^* \geq C (1 - m)(m - b/\sqrt{d})^{s-1}$, we have that the dynamics satisfies the following inequality between iterates:
%
\begin{align}
\label{eq:dynamics_lower_m}
    m_{t+1} &\geq m_t + C \delta (1 - m_t) (m_t - b/\sqrt{d})^{s-1} - \delta\nabla^S M_t \cdot \theta^* - \delta^2 \xi_t, 
\end{align}
%
where $\xi_t = 2 (K + \overline{M}_t)|m_t| + 2 \delta (K + \overline{M}_t) (|\nabla^\mathcal{S}  L(\theta) \cdot \theta^*| + |\nabla^S M_t \cdot \theta^*|)$. All the terms of the inequality have a natural origin: the second term is the ideal term coming from the gradient flow and the growth condition, the third term corresponds to the martingale increments coming form the noise induced by SGD and the final term is simply a discretization error term scaling in $O(\delta^2)$. Moreover with the same notations and summing all these terms until time $T \in \N^*$, we can write 
%
\begin{align}
\label{eq:dynamics_lowersum_m}
    m_{T} &\geq m_0 + C \delta \sum_{t = 0}^{T-1} (1 - m_t) (m_t - b/\sqrt{d})^{s-1} - \delta \sum_{t = 0}^{T-1} \nabla^S M_t \cdot \theta^* - \delta^2 \sum_{t = 0}^{T-1} \xi_t.
\end{align}
%
The following section show how to control these terms in a quantitative way. This leads to the weak recovery result.

\subsection{Weak recovery}
\label{subsec:weak_recovery}

\paragraph{Good initialization.}\textit{During all this section, we condition on the event $\{m_0 \geq 4 b /\sqrt{d}\}$}.

Before stating these lemmas, let us introduce some new notations. As already introduce, we recall that we denote $S_\eta := \{\theta \in \S, \, m_\theta \geq \eta\}$, the spherical cap of level $\eta \in (0,1)$. Moreover for $\alpha \in (-1, 1) $, similarly to what is done in~\cite{arous2020online}, we define the following stopping times $\tau^+_\alpha := \inf\{ t \geq 0, \, m_{\theta_t} \geq \alpha  \}$ and $\tau^-_\alpha := \inf\{ t \geq 0, \, m_{\theta_t} \leq \alpha  \}$ reciprocally as the first time when $(\theta_t)_{t \geq 0}$ enters in $S_\alpha$ or leaves $S_\alpha$.

\subsubsection{Proof of Theorem~\ref{thm:weak_recovery}}

Thanks to Lemmas~\ref{lem:ODE_term}, \ref{lem:Martingale_term} and \ref{lem:Discretization_term}, there exists $K$ constants that depend solely on the model such that  we have the following lower bound: for all $\lambda_1, \lambda_2 > 0$, conditionally to the event on the events $\{T \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\,$,
%
\begin{align*}
    m_{T} &\geq m_0 + \frac{C \delta}{2} \sum_{t = 0}^{T-1} \left(m_t - \frac{b}{\sqrt{d}}\right)^{s-1} - \lambda_1 - \lambda_2,
\end{align*}
%
with probability larger that $1 - (\frac{K T \delta^2}{\lambda_1^2} +\frac{K T \delta^2}{\lambda_2})$. Now we choose $\lambda_1 = \lambda_2 = b /\sqrt{d}$, so that 
%
%
\begin{align*}
    m_{T} - \frac{b}{\sqrt{d}} &\geq \frac{b}{\sqrt{d}} + \delta \frac{C}{2} \sum_{t = 0}^{T-1} \left(m_t - \frac{b}{\sqrt{d}}\right)^{s-1},
\end{align*}
%
with probability at least $1 - K T \delta^2 ( d/b^2 + \sqrt{d}/b) \geq 1 - \bar{K} T \delta^2 d$, for some  $\bar{K}$ that depends still solely on the model. For the sake of simplicity, we abuse slightly of notation and remove the bar on $K$ as it is still a model constant. We will use this abuse during all the proof of the theorem to keep simple notations. Let $\varepsilon > 0$, we divide the proof into the three case $s = 1,\, s=  2, \, s \geq 3$.


{\bfseries Case} $s = 1$, $\delta = \varepsilon/d$. In this case, we have that with probability $1 - K T \delta^2 d$, 
%
\begin{align*}
    m_{T} &\geq \frac{2b}{\sqrt{d}} + \frac{C \delta}{2} T, 
\end{align*}
%
And the right and side is slarger than $1/2$ as soon as $\delta T \geq 1/C$. From this we have that with probability at least $1 - K \delta d$, the hitting time is upper bounded by
%
$$ \tau^+_{1/2} \leq \frac{1}{ C \delta}.$$
%
Now, taking $\delta = \varepsilon d^{-1}$, we have that with probability at least $\displaystyle 1 - K \varepsilon$, we have 
%
$$ \tau^+_{1/2} \leq \frac{1}{ \varepsilon C } d.$$

{\bfseries Case} $s = 2$, $\delta = \varepsilon/(d\log d)$. Now by a discrete version of Grönwall inequality, recalled in Lemma~\ref{lem:gron_bihari}, we have with probability at least $1 - K T \delta^2 d^2$,
%
\begin{align*}
    m_{T} - \frac{b}{\sqrt{d}} &\geq \frac{b}{\sqrt{d}} \left(  1 + \delta \frac{C}{2}\right)^{T} \geq \frac{b}{\sqrt{d}} e^{C \delta T}, 
\end{align*}
%
for $d$ large enough. And as the right hand side is larger than $1/2 + b / \sqrt{d}$ whenever,  $$\delta T \geq \frac{1}{C} \log (\sqrt{d}/4b),$$
for $d$ large enough compared to $b$. Then taking such a $T$, with probability at least $1 - K \delta d \log d$ , the hitting time is upper bounded by 
%
$$ \tau^+_{1/2} \leq \frac{2}{ C \delta} \log \left(d\right).$$
%
Now, taking $\delta = \varepsilon d^{-1}(\log d)^{-1}$, we have that with probability at least $\displaystyle 1 - K \varepsilon$, we have 
%
$$ \tau^+_{1/2} \leq \frac{2}{ \varepsilon C } d \log(d)^2 .$$

{\bfseries Case} $s \geq 3$, $\delta = \varepsilon d^{-s/2}$. Now by the discrete version of Bihari-LaSalle inequality, recalled in Lemma~\ref{lem:gron_bihari}, we have with probability at least $1 - K T \delta^2 d$, 
%
\begin{align*}
    m_{T} - \frac{b}{\sqrt{d}} &\geq \frac{b}{\sqrt{d}} \left(  1 - \delta \frac{C (s-2)}{2} \left(\frac{b}{\sqrt{d}}\right)^{ s - 2} T \right)^{-\frac{1}{s-2}}. 
\end{align*}
%
And as the right hand side is larger than $1/2 + b / \sqrt{d}$ whenever,  $$\delta T \geq \frac{d^{(s-2)/2}}{C (s - 2) b^{s-2}},$$
for $d$ large enough compare to $b$. Then taking such a $T$, with probability at least $1 - K d^{\frac{s-2}{2}} \delta d$, the hitting time is upper bounded by 
%
$$ \tau^+_{1/2} \leq \frac{1}{ C b^{s-2}} \frac{d^{\frac{s-2}{2}}}{\delta}.$$
%
Now, taking $\delta = \varepsilon d^{-s/2}$, we have that with probability at least $\displaystyle 1 - K \varepsilon$, we have 
%
$$ \tau^+_{1/2} \leq \frac{4}{ \varepsilon a b^{s-2}} d^{s-1} .$$

\subsubsection{Technical intermediate result to lower bound each term of Eq.~\texorpdfstring{\eqref{eq:dynamics_lowersum_m}}{lol}}

\begin{lemma}[ODE term]
\label{lem:ODE_term}
   Conditioned to the event  $\{T \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\,$, we have the inequality 
    %
    \begin{align*}
    \sum_{t = 0}^{T-1} (1 - m_t) (m_t - b/\sqrt{d})^{s-1} \geq \frac{1}{2} \sum_{t = 0}^{T-1} (m_t - b/\sqrt{d})^{s-1}.
    \end{align*}
    %
\end{lemma}

\begin{proof}
    This simply results from the fact that For all $t \leq T - 1$, we have $\{t \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\, \subset \{T \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\,$, and summing these terms until $T - 1$ gives the proof of the lemma.
\end{proof}

\begin{lemma}[Martingale term]
\label{lem:Martingale_term}
    For all $\lambda_1 > 0$, we have that 
    %
    \begin{align}
        \P \left( \sup_{t \leq T}\, \delta \left|\sum_{k = 0}^{t-1} \nabla^S M_k \cdot \theta^*\right|  \geq \lambda_1  \right) \leq \frac{K T \delta^2}{\lambda_1^2}, 
    \end{align}
    where $K > 0$, that depends solely on the model through $f, \nu$.
\end{lemma}

\begin{proof}
    This is a consequence of Doob's maximal inequality for (sub)martingale. Indeed, for $t \leq T $, let $H_{t-1} = \sum_{k = 0}^{t-1} \nabla^S M_k \cdot \theta^*$. We have that $H_t$ is a $\mathcal{F}_t$-adapted martingale and we have the following upper bounded on its variance:
    %
\begin{align*}
     \E[H_{t-1}^2] = \E\left[\left(\sum_{k = 0}^{t-1} \nabla^S M_k \cdot \theta^* \right)^2\right] = \E\left[\sum_{k = 0}^{t-1} \left(\nabla^S M_k \cdot \theta^* \right)^2\right] \leq  t \sup_\theta \E_x\left[ \left|\nabla M(x, \theta)\right|^2\right] \leq K t,
\end{align*}
%
where the last inequality comes from the Lemma~\ref{lem:tech_bounds}. Now, thanks to Doob's maximal inequality, we have for all $\lambda_1 > 0$,
%
\begin{align*}
    \P \left( \sup_{t \leq T}\, \delta |H_{t-1}|  \geq \lambda_1  \right) \leq \frac{ \E[H_{T-1}^2] \delta^2}{\lambda_1^2} \leq \frac{K T \delta^2}{\lambda_1^2},
\end{align*}
%
and this concludes the proof of the lemma.
\end{proof}

\begin{lemma}[Discretization term]
\label{lem:Discretization_term}
    For all $\lambda_2 > 0$, we have that 
    %
    \begin{align}
        \P \left( \sup_{t \leq T}\, \delta^2 \sum_{k = 0}^{t-1} \xi_k  \geq \lambda_2  \right) \leq \frac{K T \delta^2}{\lambda_2}, 
    \end{align}
    where $K > 0$ depends solely on the model through $f, \nu$.
    %
\end{lemma}
%
\begin{proof}
    The bound follows from an application of Markov's inequality. Indeed, since all the terms of the sum are positive, the supremum is attained in $t = T-1$, and we shall only consider this case. For $\lambda > 0$, 
    \begin{align*}
        \P \left( \delta^2 \sum_{t = 0}^{T-1} \xi_t  \geq \lambda  \right) \leq \frac{\delta^2}{\lambda}\E\left( \sum_{t = 0}^{T-1} \xi_t \right) \leq \frac{T \delta^2}{\lambda}\sup_\theta \E_x[\xi(x, \theta)] \leq \frac{K T \delta^2}{\lambda}, 
    \end{align*}
    %
    where the last inequality comes from Lemma~\ref{lem:tech_bounds}. 
\end{proof}

\subsection{Strong recovery}
\label{subsec:strong_recovery}


The reasoning is almost identical to the one of the previous section, except from the fact that instead of tracking the growing movement on $(m_t)_{t \geq 0}$, we will track the decaying movement of $(1 - m_t)_{t \geq 0}$. 

\subsubsection{Upper bound on the residual}

As said in the main text, we place ourselves \textit{after} the weak recovery time. Thanks to the Markovian property of the SGD dynamics, we have the equality between all time $s > 0$ marginal laws of
%
\begin{align*}
\left(\theta_{\tau^+_{\nicefrac{1}{2}} + s}\  \bigg| \  \tau^+_{1/2},\, \theta_{\tau^+_{1/2}}\right) \overset{\text{Law}}{=} \left(\theta_{s} \  \bigg| \  \theta_{s} = \theta_{\tau^+_{1/2}} \right),  
\end{align*}
%
and hence the strong recovery question is equivalent to study the dynamics with initialization such that $m_\theta  = 1/2$. As demonstrated before we have that $\P(\tau_{1/2}^+ < \infty) \geq 1 - C \varepsilon$ so that up to $\varepsilon$ terms, this conditioning does not hurt the probability of the later events. In fact this conditioning seems even artificial as it seems provable that $\tau_{1/2}^+$ is almost surely finite. Yet, we leave this more precise study for another time. Let us define for all $t \in \N $, the residual $u_t = 1 - m_{t+\tau^+_{1/2}} > 0$, and thanks to the lower bound given by Eq.~\eqref{eq:dynamics_lower_m}, we have
%
\begin{align*}
    u_{t+1} \leq u_t - C \delta u_t (m_t - b/\sqrt{d})^{s-1} + \delta\nabla^S M_t \cdot \theta^* + \delta^2 \xi_t.
\end{align*}
%
From there, we will similarly as for the proof of Theorem 

\subsubsection{Proof of Theorem~\ref{thm:strong_recovery}}
%
Let us fix a small number $\varepsilon>0$. As previously, thanks to Lemmas~\ref{lem:ODE_term},\ref{lem:Martingale_term} and \ref{lem:Discretization_term}, there exists $K>0$ that depends solely on the model such that we have the following upper bound: for all $\lambda_1, \lambda_2 > 0$, and $ t \leq \tau_{1/3}^- \wedge \tau_{1-\varepsilon}^+ $ summing between times $0$ and $t$, 
%
\begin{align*}
    u_{t} &\leq u_0 - \frac{C \delta}{4^{s-1}} \sum_{k =0}^{t-1} u_k + \lambda_1 + \lambda_2,
\end{align*}
%
with probability larger that $1 - (\frac{K t \delta^2}{\lambda_1^2} +\frac{K t \delta^2}{\lambda_2})$ and $d$ large enough. Realizing that $u_{0} \leq 1/2$, and choosing $\lambda_1 = \lambda_2 = 1/8$, so that 
%
\begin{align*}
    u_{t} &\leq \frac34 - \frac{C \delta}{4^{s-1}} \sum_{k =0}^{t-1} u_k~,
\end{align*}
%
with probability at least $1 - K t \delta^2$. We have by Grönwall inequality~(Lemma~\ref{lem:gron_bihari})
%
\begin{align*}
    u_{t} &\leq \frac34 \left(1 - \frac{C \delta}{4^{s-1}} \right)^{t} \leq \frac34 e^{- \frac{C \delta}{4^{s-1}}t }.
\end{align*}
%
Hence, as the right end side is smaller than $\varepsilon$ for the time $$t \delta \geq \frac{4^{s-1}}{C} \log(1/\varepsilon),$$
%
we choose such a $t$, so that with probability at least $1 - K \delta \log(1/\varepsilon)$, the delayed hitting time $\overline{\tau}^+_{1-\varepsilon} := \inf\{ t \geq 0, \, u_{t} \leq \varepsilon  \}$ satisfies $$\overline{\tau}^+_{1-\varepsilon} \leq \frac{4^{s-1}}{C \delta} \log(1/\varepsilon),$$ and taking $\delta = \varepsilon/\log(1/\varepsilon)$ gives that with a probability $1 - K \varepsilon$, we have $$ \overline{\tau}^+_{1-\varepsilon} \leq \frac{4^{s-1}}{C \varepsilon} \log(1/\varepsilon)^2, $$ and this concludes the proof of Theorem~\ref{thm:strong_recovery}.


\subsection{Some technical bounds}
\label{subsec:tech_SGD_bounds}

We end this section by providing (i) some necessary technical technical bound on the quantities appearing in the SGD controls (ii) some discrete versions of Grönwall-type lemmas.

\subsubsection{Technical bounds on models expectations}

\begin{lemma}[Technical bounds]
\label{lem:tech_bounds}
 We have that there exists a constant $\overline{K} > 0$ solely depending on the function $\phi$ and the distribution $\nu$ such that:
 %
 \begin{align}
     \max \left\{ \sup_\theta \left|\nabla_\theta L \right|^2,\, \sup_\theta \E_x\left[ \left|\nabla M(x, \theta)\right|^2\right],\, \sup_\theta \E_x[\xi(x, \theta)]\right\} &\leq \overline{K}, 
 \end{align}
 %
where we recall that  $M(x, \theta) = l(x, \theta) - \E_x[l(x, \theta)]$ and  $\xi(x, \theta) = 2 (K + \left|\nabla M(x, \theta)\right|^2)|m_\theta| + 2 \delta (K + \left|\nabla M(x, \theta)\right|^2) (|\nabla^\mathcal{S}  L(\theta) \cdot \theta^*| + |\nabla^S M(x, \theta) \cdot \theta^*|)$.
\end{lemma}

%
\begin{proof}
In all the following proof we consider any $\theta \in \S$. We treat the three bounds separately. An important quantity that we define here is: $$ \mathsf{K}:= \sup_\theta \E_\nu \left[|x|^2  \phi'^2(x \cdot \theta)\phi^2(x \cdot \theta)\right] \vee K \vee 1,$$
%
which is a constant that solely depends on the model through $\phi$ and $\nu$.

\textit{First term.} We have that 
%
    \begin{align*}
        \left|\nabla_\theta L \right|^2 &= 4 \left\|\E_\nu\left[x \phi'(x \cdot \theta)\left(\phi(x \cdot \theta) - \phi(x \cdot \theta^*)\right) \right]\right\|^2 \\
        &\leq 4 \E_\nu \left[|x|^2  \phi'^2(x \cdot \theta)\left(\phi(x \cdot \theta) - \phi(x \cdot \theta^*)\right)^2 \right] \\
        &\leq 8 \E_\nu \left[|x|^2  \phi'^2(x \cdot \theta)\left(\phi^2(x \cdot \theta) + \phi^2(x \cdot \theta^*) \right)\right].  
    \end{align*}
Hence taking the supremum over $\theta \in \S$, we have
%
\begin{align*}
        \sup_\theta \left|\nabla_\theta L \right|^2 &\leq 16 \mathsf{K}.  
    \end{align*}
    %
    %
\textit{Second term.} We have that for all $x \in \R^d$,   
    %
    \begin{align*}
        M(x, \theta) &= l(x, \theta) - \E_\nu[l(x, \theta)],
    \end{align*}
    %
hence 
    %
    \begin{align*}
        \nabla_\theta M(x, \theta) &=  \nabla_\theta l(x, \theta) - \E_\nu[ \nabla_\theta l(x, \theta)],
    \end{align*}
    %
    and finally, 
    %
        %
    \begin{align*}
        \E_\nu \left|\nabla_\theta M(x, \theta)\right|^2 &=   \E_\nu \left|\nabla_\theta l(x, \theta)\right|^2 - \left|\E_\nu[ \nabla_\theta l(x, \theta)]\right|^2 \\
        &\leq  \E_\nu \left|\nabla_\theta l(x, \theta)\right|^2 \\
        &\leq \E_\nu \left[|x|^2  \phi'^2(x \cdot \theta)\left(\phi(x \cdot \theta) - \phi(x \cdot \theta^*)\right)^2 \right].
    \end{align*}
    %
Hence taking the supremum over $\theta \in \S$, we have
%
\begin{align*}
        \sup_\theta \E_\nu \left|\nabla_\theta M(x, \theta)\right|^2 &\leq 2 \mathsf{K}.  
    \end{align*}
    %
\textit{Third term.} In the expression of $\xi$, the first part $ 2 (K + \left|\nabla M(x, \theta)\right|^2)|m_\theta|$, is bounded by the quantity controlling the first two terms (see above). This is also the case for the part that is multiplied by the $|\nabla^\mathcal{S}  L(\theta) \cdot \theta^*|$ term that is bounded now by the square of the aforementioned quantity. The only term that requires an analysis comes from the multiplication last part. More precisely, using that $a \leq a^2 + 1/2$ for all $a \in \R$, 
%
\begin{align*}
    \xi(x, \theta) &= 2 (K + \left|\nabla M(x, \theta)\right|^2)|m_\theta| + 2 \delta (K + \left|\nabla M(x, \theta)\right|^2) (|\nabla^\mathcal{S}  L(\theta) \cdot \theta^*| + |\nabla^S M(x, \theta) \cdot \theta^*|) \\
    &\leq 2 (K + \left|\nabla M(x, \theta)\right|^2) + 2 \delta (K + \left|\nabla M(x, \theta)\right|^2) (|\nabla L(\theta)|^2 + |\nabla M(x, \theta)|^2 + 1),
\end{align*}
%
so that we can write
    %
    \begin{align*}
        \sup_\theta \E_\nu [\xi(x, \theta)] &\leq 2 (K + \mathsf{K}) + 2 \delta K ( 16 \mathsf{K} + 2 \mathsf{K} + 1 )  + 2 \delta  ( \mathsf{K}^2 + \sup_\theta \E_\nu [|\nabla M|^4]  +  \mathsf{K} ) \\
        &\leq 4 \mathsf{K} + 32 \delta \mathsf{K}^2 + 2 \delta \sup_\theta \E_\nu [|\nabla M|^4].
    \end{align*}
    It hence remains to bound this latter quantity. We have
%
\begin{align*}
    \E_\nu [|\nabla M|^4] &= \E_\nu\left[ |\nabla l|^2 + | \E \nabla l |^2 - 2 \nabla l \cdot \E \nabla l   \right]^2 \\
    &\leq 2 \E_\nu\left[ |\nabla l|^4 + | \E \nabla l |^4 + 2 (\nabla l \cdot \E \nabla l)^2   \right] \\
    &\leq 8 \E_\nu\left[ |\nabla l|^4\right],
\end{align*}
%
    where the last equality stands by Cauchy-Schwartz and Jensen inequality. Finally, we have, by the 
    %
\begin{align*}
    \sup_\theta \E_\nu [|\nabla M|^4] &\leq \sup_\theta \E_\nu \left[|x|^4  \phi'^4(x \cdot \theta)\left(\phi(x \cdot \theta) - \phi(x \cdot \theta^*)\right)^4 \right] \\
    &\leq 8 \sup_\theta \E_\nu \left[|x|^4  \phi'^4(x \cdot \theta)\phi^4(x \cdot \theta) \right], 
\end{align*}
%
which is a contant that depends solely on the model. 
\end{proof}

\subsubsection{Discrete Grönwall and Bihari-Lasalle bounds}

We now turn to stating classical comparison Lemma for recursive inequalities.



\begin{lemma}[Grönwall and  Bihari-Lasalle]
\label{lem:gron_bihari}
We have the bounds for the recursive inequalities:

{\bfseries Case $s = 2$.} Suppose $(m_t)_{t \in \N}$ satisfies for $s \geq 3$, and positives numbers $a,b>0$, and $b< a/2 \wedge 1$,
%
\begin{align}
    m_t &\geq a + b \sum_{k = 0}^{t - 1} m_k, \qquad \text{ then, } \ \ m_t \geq a \left( 1 + b \right)^{t} \\
    m_t &\leq a - b \sum_{k = 0}^{t - 1} m_k, \qquad \text{ then, } \ \ m_t \leq a \left( 1 - b \right)^{t}.
\end{align}


{\bfseries Case $s \geq 3$.} Suppose $(m_t)_{t \in \N}$ satisfies for $s \geq 3$, and positives numbers $a,b>0$:
%
\begin{align}
    m_t \geq a + b \sum_{k = 0}^{t - 1} m_k^{s-1}, \qquad \text{ then, } \ \ m_t \geq a \left( 1 - (s-2)b a^{s-2} t\right)^{-\frac{1}{s-2}}.
\end{align}
%
\end{lemma}
%
\begin{proof}
The case $s = 2$ is known to be the discrete version of the Grönwall lemma and is treated in all standard textbooks, the case $s \geq 3$ referred to as the Bihari-Lasalle inequality is for example proven in Appendix C of \cite{arous2021online}. 
\end{proof}