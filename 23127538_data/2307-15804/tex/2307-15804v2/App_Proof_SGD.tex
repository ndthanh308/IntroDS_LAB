\section{Proofs on the SGD dynamics: Section \ref{sec:sgd}}
\label{sec:sgdapp}

We first recall the notations useful to fully describe the dynamics. In Section~\ref{subsec:weak_recovery}, we prove Theorem~\ref{thm:weak_recovery} about weak recovery. In Section~\ref{subsec:strong_recovery}, we prove Theorem~\ref{thm:strong_recovery} about strong recovery. Finally, 

\subsection{Recalling the dynamics}

For the sake of clarity, let us recall the notations and facts developed in the main text. The overall loss classically corresponds to the average over all the \textit{data} of a square penalisation $l(\theta, x) = (\phi_\theta(x) - \phi_{\theta^*}(x))^2$ so that 
%
$$L(\theta) = \mathbb{E}_{\nu} [(\phi_\theta(x) - \phi_{\theta^*}(x))^2].$$
%
To recover the signal given by $\theta^*$, we run \textit{online stochastic gradient descent} on the sphere $\S$. This corresponds to have at each iteration $t \in \mathbb{N}^*$ a \textit{fresh sample} $x_t$ independent of the filtration $\mathcal{F}_t = \sigma(x_1, \dots, x_{t-1})$ and perform a spherical gradient step, with step-size $\delta>0$, with respect to $\theta \to l(\theta, x_t)$:
%
\begin{align}
    \label{eq:SGD_app}
    \theta_{t+1} &= \frac{\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)}{\left|\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)\right|},
\end{align}
%
initialized at $\theta_0$ uniformly on the sphere: $\theta_{0} \sim \mathrm{Unif}(\S)$. Recall that we use the notation $\nabla_\theta^\mathcal{S}$ to denote the spherical gradient, that is $$ \nabla_\theta^\mathcal{S} l(\theta, x) =  \nabla_\theta  l(\theta, x) - (\nabla_\theta  l(\theta, x) \cdot \theta) \theta.$$
%
Let us introduce the following frequently used notations: for all $t \in \mathbb{N}^*$, we denote the normalization by $r_t := r(\theta_t, x_t) = \left|\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)\right|$ and the martingale induced by the stochastic gradient descent as $M_t = M(\theta_t, x_t) = l(\theta_t, x_t) - \E_\nu [ l(\theta_t, x)]$.

\subsection{Tracking the correlation.} Recall that the relevant signature of the dynamics is the one-dimensional correlation: $m_t = \theta_t \cdot \theta^*$. Let us re-write the iterative recursion followed by $(m_t)_{t \geq 0}$, with the notation recalled above, for $t \in \mathbb{N}^*$, 
%
\begin{align}
\label{eq:dynamics_main_m_app}
m_{t+1} = \frac{1}{r_t}\left( m_t - \delta \nabla^S l(\theta_t, x_t) \cdot \theta^*  \right) =  \frac{1}{r_t}\left( m_t - \delta\nabla^S L(\theta_t) \cdot \theta^* - \delta\nabla^S M_t \cdot \theta^*  \right).
\end{align}
%
We want to lower bound the right hand side of~\eqref{eq:dynamics_main_m_app}. We begin by a lower bound on~$1/r_t$. 

%
\begin{lemma}[Bound on $r_t$]
    For all $t \in \N^*$, we have $1/r_t \geq 1 - \delta^2 \left|\nabla_\theta l(\theta_t, x_t)\right|^2$.    
\end{lemma}
%
\begin{proof}
    For all $t \in \N^*$, we have, by orthogonality of as $\theta_t$ and $\nabla_\theta^\mathcal{S}  l(\theta_t, x_t)$, that
    %
    \begin{align*}
        r_t^2 = \left|\theta_t - \delta \nabla_\theta^\mathcal{S}  l(\theta_t, x_t)\right|^2  = 1 + \delta^2 \left|\nabla_\theta^\mathcal{S} l(\theta_t, x_t)\right|^2 \leq 1 + \delta^2 \left|\nabla_\theta l(\theta_t, x_t)\right|^2.
    \end{align*}
    %
    Hence, from the inequality $ (1 + u)^{-1/2} \geq 1 - u $ for all $ u > 0$, we conclude the proof.
\end{proof}
%
Thanks the fact that $L$ satisfies $\LPG(s,b/\sqrt{d})$, ie $-\nabla^\mathcal{S}  L(\theta) \cdot \theta^* \geq C (1 - m)(m - b/\sqrt{d})^{s-1}$, we have that the dynamics satisfies the following inequality between iterates:
%
\begin{align}
\label{eq:dynamics_lower_m}
    m_{t+1} &\geq m_t + C \delta \left(1 - m_t\right) \left(m_t - \frac{b}{\sqrt{d}}\right)^{s-1} - \delta\nabla^S M_t \cdot \theta^* - \delta^2  |m_t| \left|\nabla_\theta l(\theta_t, x_t)\right|^2 -  \delta^3 \xi_t, 
\end{align}
%
where $\xi_t = \left|\nabla_\theta l(\theta_t, x_t)\right|^2 |\nabla^S l(\theta_t, x_t) \cdot \theta^* |$. All the terms of the inequality have a natural origin: the second term is the ideal term coming from the gradient flow and the growth condition, the third term corresponds to the martingale increments coming form the noise induced by SGD and the two final terms are simply discretization errors coming from discrete nature of the procedure and the projection step. 

However, to have a tight dependency with respect to the dimension, we need to be extra careful. This is why, following \cite{arous2021online}, we decompose this term introducing a threshold $\MM > 0$, to be fixed later, such that: 
%
\begin{align*}
    |m_t| \left|\nabla_\theta l(\theta_t, x_t)\right|^2 & =|m_t| \left|\nabla_\theta l(\theta_t, x_t)\right|^2 \mathds{1}_{\{\left|\nabla_\theta l(\theta_t, x_t)\right|^2 \leq \MM\}} + |m_t| \left|\nabla_\theta l(\theta_t, x_t)\right|^2\mathds{1}_{\{\left|\nabla_\theta l(\theta_t, x_t)\right|^2 > \MM\}}
\end{align*}
%
With the same notations and summing all these terms until time $T \in \N^*$, we can write 
%
\begin{align*}
    m_{T} \geq m_0 &+ C \delta \sum_{t = 0}^{T-1} (1 - m_t) (m_t - b/\sqrt{d})^{s-1} - \delta \sum_{t = 0}^{T-1} \nabla^S M_t \cdot \theta^* - \delta^2 \sum_{t = 0}^{T-1} |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}} \\
    &- \delta^2 \sum_{t = 0}^{T-1} |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 > \MM\}}  - \delta^3 \sum_{t = 0}^{T-1} \xi_t, \nonumber
\end{align*}
%
where we use, for the sake of compactness, the shortcut notation $l_t = l(x_t, \theta_t)$. The strategy of the proof is the following: the first term is the drift term that makes the correlation grow, the second term is simply a martingale term that we deal with via standard martingale inequality, and the forth and fifth term are discretization error that we will bound loosely. The difficulty comes from the third term: the proof is based on the fact that we use a ``part'' of the drift term (say half) to control it. This is why we decide to rewrite finally our inequality as,    
%
%
\begin{align}
\label{eq:dynamics_lowersum_m}
    m_{T} \geq m_0 &+ \delta \frac{C}{2} \sum_{t = 0}^{T-1} (1 - m_t) (m_t - b/\sqrt{d})^{s-1} - \delta \sum_{t = 0}^{T-1} \nabla^S M_t \cdot \theta^* - \delta \sum_{t = 0}^{T-1} D_t \\
    &- \delta^2 \sum_{t = 0}^{T-1} |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 > \MM\}}  - \delta^3 \sum_{t = 0}^{T-1} \xi_t, \nonumber
\end{align}
%
where we have defined $D_t := \frac{C}{2} (1 - m_t) (m_t - b/\sqrt{d})^{s-1} - \delta |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}}$.
%
The following section show how to control these five terms in a quantitative way.

\subsection{Weak recovery}
\label{subsec:weak_recovery}

\paragraph{Good initialization.}\textit{During all this section, we condition on the event $\{m_0 \geq 5 b /\sqrt{d}\}$}.

Before stating these lemmas, let us introduce some new notations. As already introduce, we recall that we denote $S_\eta := \{\theta \in \S, \, m_\theta \geq \eta\}$, the spherical cap of level $\eta \in (0,1)$. Moreover for $\alpha \in (-1, 1) $, similarly to what is done in~\cite{arous2020online}, we define the following stopping times $\tau^+_\alpha := \inf\{ t \geq 0, \, m_{\theta_t} \geq \alpha  \}$ and $\tau^-_\alpha := \inf\{ t \geq 0, \, m_{\theta_t} \leq \alpha  \}$ reciprocally as the first time when $(\theta_t)_{t \geq 0}$ enters in $S_\alpha$ or leaves $S_\alpha$.

\subsubsection{Proof of Theorem~\ref{thm:weak_recovery}}

Thanks to Lemmas~\ref{lem:ODE_term}, \ref{lem:first_artingale_term}, \ref{lem:submartingale_term}, \ref{lem:first_discretization_term} and \ref{lem:second_discretization_term}, that serve bounding all the terms in the $m_T$ inequality, there exists a constant $K$ that depend solely on the model such that  we have the following lower bound: for all $\lambda > 0$, conditionally to the event on the events $\{T \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\,$,
%
\begin{align*}
    m_{T} &\geq m_0 + \frac{C}{2^{s+1}} \delta \sum_{t = 0}^{T-1} m_t^{s-1} - 4 \lambda,
\end{align*}
%
with probability larger that $\displaystyle 1 - \left(\frac{K T \delta^2}{\lambda^2} + \exp\left( -\frac{\lambda^2}{2 K^2 \delta^2 T + \lambda \delta (C + \delta \MM)}\right) +\frac{K T d^2 \delta^2}{\lambda \MM} +\frac{K T d \delta^3}{\lambda} \right)$. Now we choose $\lambda = b /\sqrt{d}$ and $\MM = d^{3/2}$ so that
%
%
\begin{align*}
    m_{T} &\geq \frac{b}{\sqrt{d}} + \frac{C}{2^{s+1}} \delta \sum_{t = 0}^{T-1} m_t^{s-1},
\end{align*}
%
with probability at least $1 - p_{\delta, M}(T)$, where we defined naturally
%
$$p_{\delta, M}(T) := \left(\frac{K T d \delta^2}{b^2} + \exp\left( -\frac{b^2}{2 K^2 d \delta^2 T + b \sqrt{d} \delta (C + \delta \MM)}\right) +\frac{K T d^{5/2} \delta^2}{b \MM} +\frac{K T d^{3/2} \delta^3}{b}  
 \right).$$ 
 %
Let us upper bound the probability $p_{\delta, M}(T)$. Let us set $\varepsilon > 0$ a small constant. First, in the exponential term, the term $b \sqrt{d} \delta (C + \delta d^{3/2})$ is negligible in virtue of the fact that in any of the cases of Theorem~\ref{thm:weak_recovery}, we have $\delta \leq \varepsilon/d$. Moreover, for the sake of clarity, we gather all constant $K, C, b$ as one constant generic $\mathsf{K}$, as these depend only on the data distribution and the link function. Hence, for $d$ large enough,
%
\begin{align*}
p_{\delta, M}(T) &\leq \mathsf{K} \left(d T \delta^2 + \exp\left( -\frac{1}{d  T \delta^2 } \right) + d T \delta^2 + d^{3/2} T \delta^3 \right),
\end{align*}
and as $d^{3/2} T \delta^3 \lesssim d T \delta^2 $ for the range of $\delta$ we choose, we have $p_{\delta, M}(T)  \leq \mathsf{K} \left(d T \delta^2 + \exp\left( -\frac{1}{d  T \delta^2 } \right) \right)$, and considering that we will take in any case $d T \delta^2 \leq 1$, as we have the inequality $\exp\left( -\frac{1}{d  T \delta^2 }\right) \leq d T \delta^2$, so that finally
\begin{align*}
p_{\delta, M}(T)  \leq \mathsf{K} d T \delta^2
\end{align*}
%
%In all the three cases the analysis covers, we set $\delta = \mathcal{O}(1/d)$, so that there exists a constant independent of dimension, $\overline{K}>0$, such that  the aforementioned probability satisfies: $$ 1 - K T \delta^2 ( d/b^2 + \delta^2 d^2/b^2) \geq 1 - \overline{K} T \delta^2 d. $$ We will keep track of $\overline{K}$ as a generic constant independent of the dimension so that, for the sake of clarity, we do not keep track of multiplication by other constant parameters. 
We divide the proof into the three cases $s = 1,\, s=  2, \, s \geq 3$.


{\bfseries Case} $s = 1$, $\delta = \varepsilon/d$. In this case, we have that with probability $1 - p_{\delta, M}(T)$, 
%
\begin{align*}
    m_{T} &\geq \frac{b}{\sqrt{d}} + \frac{C \delta}{2^{s}} T. 
\end{align*}
%
The right and side is larger than $1/2$ as soon as $\delta T \geq 2^{s}/C$. From this we have that with probability at least $1 - p_{\delta, M}(T)$, the hitting time is upper bounded by
%
$$ \tau^+_{1/2} \leq \frac{2^s}{ C \delta}.$$
%
Now, taking $\delta = \varepsilon d^{-1}$, we can check that for $\varepsilon$  small enough, $d T \delta^2 \leq 2^s \varepsilon/C =\varepsilon \mathcal{O}(1) $ so that we have that with probability at least $\displaystyle 1 - \mathsf{K} \varepsilon$, we have 
%
$$ \tau^+_{1/2} \leq \frac{\mathsf{K}}{ \varepsilon } d.$$

{\bfseries Case} $s = 2$, $\delta = \varepsilon/(d\log d)$. Now by a discrete version of Grönwall inequality, recalled in Lemma~\ref{lem:gron_bihari}, we have with probability at least $1 - p_{\delta, M}(T)$,
%
\begin{align*}
    m_{T} - \frac{b}{\sqrt{d}} &\geq \frac{b}{\sqrt{d}} \left(  1 + \delta \frac{C}{2}\right)^{T} \geq \frac{b}{\sqrt{d}} e^{C \delta T}, 
\end{align*}
%
for $d$ large enough. And as the right hand side is larger than $1/2 + b / \sqrt{d}$ whenever,  $$\delta T \geq \frac{1}{C} \log (\sqrt{d}/4b),$$
for $d$ large enough compared to $b$. Then taking such a $T$, with probability at least $1 - p_{\delta, M}(T)$ , the hitting time is upper bounded by 
%
$$ \tau^+_{1/2} \leq \frac{2}{ C \delta} \log \left(d\right).$$
%
Now, taking $\delta = \varepsilon d^{-1}(\log d)^{-1}$, we can check that for $\varepsilon$  small enough, $d T \delta^2 \leq \frac{ \varepsilon }{C } =\varepsilon \mathcal{O}(1)$ so that we have that with probability at least $\displaystyle 1 - \mathsf{K} \varepsilon$, we have
%
$$ \tau^+_{1/2} \leq \frac{\mathsf{K}}{ \varepsilon } d \log(d)^2 .$$

{\bfseries Case} $s \geq 3$, $\delta = \varepsilon d^{-s/2}$. Now by the discrete version of Bihari-LaSalle inequality, recalled in Lemma~\ref{lem:gron_bihari}, we have with probability at least $1 - p_{\delta, M}(T)$, 
%
\begin{align*}
    m_{T} - \frac{b}{\sqrt{d}} &\geq \frac{b}{\sqrt{d}} \left(  1 - \delta \frac{C (s-2)}{2} \left(\frac{b}{\sqrt{d}}\right)^{ s - 2} T \right)^{-\frac{1}{s-2}}. 
\end{align*}
%
And as the right hand side is larger than $1/2 + b / \sqrt{d}$ whenever,  $$\delta T \geq \frac{d^{(s-2)/2}}{C (s - 2) b^{s-2}},$$
for $d$ large enough compare to $b$. Then taking such a $T$, with probability at least $1 - p_{\delta, M}(T)$, the hitting time is upper bounded by 
%
$$ \tau^+_{1/2} \leq \frac{1}{ C b^{s-2}} \frac{d^{\frac{s-2}{2}}}{\delta}.$$
%
Now, taking $\delta = \varepsilon d^{-s/2}$, we can check that for $\varepsilon$  small enough, $d T \delta^2 \leq \frac{ \varepsilon }{C (s - 2) b^{s-2} } =\varepsilon \mathcal{O}(1)$ so that we have that with probability at least $\displaystyle 1 - \mathsf{K} \varepsilon$, we have
%
$$ \tau^+_{1/2} \leq \frac{\mathsf{K}}{ \varepsilon} d^{s-1} .$$

\subsubsection{Technical intermediate result to lower bound each term of Eq.~\texorpdfstring{\eqref{eq:dynamics_lowersum_m}}{lol}}

\begin{lemma}[ODE term]
\label{lem:ODE_term}
   Conditioned to the event  $\{T \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\,$, we have the inequality 
    %
    \begin{align*}
    \sum_{t = 0}^{T-1} (1 - m_t) (m_t - b/\sqrt{d})^{s-1} \geq \frac{1}{2^s} \sum_{t = 0}^{T-1} m_t^{s-1}.
    \end{align*}
    %
\end{lemma}

\begin{proof}
    This simply results from the fact that for all $t \leq T - 1$, we have $\{t \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\, \subset \{T \leq \tau_{1/2}^+ \, \wedge \tau^-_{2b/\sqrt{d}}\ \}\,$, so that we can use the inequalities $1 - m \geq 1/2$ and $m - b/\sqrt{d} \geq m/2$. Summing these terms until $T - 1$ gives the proof of the lemma.
\end{proof}

\begin{lemma}[First martingale term]
\label{lem:first_artingale_term}
    For all $\lambda > 0$, we have that 
    %
    \begin{align}
        \P \left( \sup_{t \leq T}\, \delta \left|\sum_{k = 0}^{t-1} \nabla^S M_k \cdot \theta^*\right|  \geq \lambda  \right) \leq \frac{K T \delta^2}{\lambda^2}, 
    \end{align}
    where $K > 0$, that depends solely on the model through $f, \nu$.
\end{lemma}

\begin{proof}
    This is a consequence of Doob's maximal inequality for (sub)martingale. Indeed, for $t \leq T $, let $H_{t-1} = \sum_{k = 0}^{t-1} \nabla^S M_k \cdot \theta^*$. We have that $H_t$ is a $\mathcal{F}_t$-adapted martingale and we have the following upper bounded on its variance:
    %
\begin{align*}
     \E[H_{t-1}^2] &= \E\left[\left(\sum_{k = 0}^{t-1} \nabla^S M_k \cdot \theta^* \right)^2\right] \\
     &= \E\left[\sum_{k = 0}^{t-1} \left(\nabla^S M_k \cdot \theta^* \right)^2\right]  \\
     &\leq  t \sup_\theta \E_x\left[ \left(\nabla^S M_k \cdot \theta^* \right)^2\right] \\
     &\leq K t,
\end{align*}
%
where the last inequality comes from the Lemma~\ref{lem:tech_bounds}. Now, thanks to Doob's maximal inequality, we have for all $\lambda > 0$,
%
\begin{align*}
    \P \left( \sup_{t \leq T}\, \delta |H_{t-1}|  \geq \lambda  \right) \leq \frac{ \E[H_{T-1}^2] \delta^2}{\lambda^2} \leq \frac{K T \delta^2}{\lambda^2},
\end{align*}
%
and this concludes the proof of the lemma.
\end{proof}

\begin{lemma}[Submartingale term]
\label{lem:submartingale_term}
    For all $\lambda > 0$, if for all $t \leq T$, $m_t \in [2b/\sqrt{d}, 1/2]$, and $\delta$ is such that $\delta \leq \varepsilon / d$, with a small enough constant $\varepsilon>0$, we have that 
    %
    \begin{align}
         \P\left( \delta\sum_{t=0}^{T-1} D_t \leq - \lambda \right)  \leq \exp \left( - \frac{\lambda^2}{ 2 K^2 \delta^2 T + \lambda \delta (C + \delta \MM)}\right)
    \end{align}
    where $K > 0$, that depends solely on the model through $f, \nu$.
\end{lemma}

\begin{proof}
    First, recall that we have defined $D_t = \frac{C}{2} (1 - m_t) (m_t - b/\sqrt{d})^{s-1} - \delta |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}}$. Let us notice that if $m_t \in [2b/\sqrt{d}, 1/2]$, then $1-m_t \geq 1/2$ and $(m_t - b/\sqrt{d})^{s-1} \geq m_t^{s-1} / 2^{s-1}$. Hence, if $m_t$ lies in such an interval, 
    %
    \begin{align*}
        D_t &\geq  \frac{C}{2^{s + 1}} m_t^{s-1} - \delta |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}} \\
        &\geq  \frac{C}{2^{s + 1}} m_t^{s-1} \left(1 -  2^{s+1} \delta \frac{ \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}}}{C m_t^{s-2}} \right).
    \end{align*}
    %
    Now, for $\delta$ such that $\E\left[ 1 -  2^{s+1} \delta \frac{ \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}}}{C m_t^{s-2}}\, | \, \mathcal{F}_{t-1}\right] \geq 0$, $\left(\sum_{k=1}^t D_k\right)_{t \geq 0}$ is a submartingale, which is true as soon as
    %
    \begin{align*}
        \delta \leq \frac{C m_t^{s-2}}{2^{s+1}\sup_\theta \E\left[ \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}}\,  |\, \mathcal{F}_{t-1}  \right]  }, 
    \end{align*}
    %
    which is itself true if
    %
        %
    \begin{align*}
        \delta \leq \frac{C}{4^{s}\sup_\theta \E\left[ \left|\nabla_\theta l_t\right|^2\right] }, 
    \end{align*}
    %
    which is implied by the condition required in the lemma given the upper bound on~$\E[ \left|\nabla_\theta l_t\right|^2]$ provided in Lemma~\ref{lem:tech_bounds}.
    %
    In order to apply Freedman tail inequality for this submartingale, let us provide upper bound on the increments as well as their variance. Indeed, we have, for all $t \geq 0$,  
    %
    \begin{align*}
       |D_t| &\leq \frac{C |1 - m_t| |m_t - b \sqrt{d}|^{s-1}}{2} + \delta |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}}  \\
       &\leq  \frac{C + \delta \MM}{2}~,
    \end{align*}
       and in virtue of the inequality $(a+b)^2 \leq 2(a^2 + b^2)$, we have
    \begin{align*}
    \E\left[D_t^2\,|\, \mathcal{F}_{t-1}\right] &\leq 2 \left( \frac{C^2 |1 - m_t|^2 |m_t - b \sqrt{d}|^{2(s-1)}}{4} + \delta^2 |m_t|^2 \E \left[\left|\nabla_\theta l_t\right|^4 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 \leq \MM\}} \right]  \right) \\
     &\leq \frac{C^2 + \delta^2 \E\left[ \left|\nabla l\right|^4\right]}{2}  \\
     & \leq \frac{C^2 + K \delta^2 d^2}{2} \\
     & \leq K^2. 
    \end{align*}
    %
    %
    Hence, by the Freedman tail inequality recalled in Theorem~\ref{thm:freedman}, for all $\lambda > 0$, 
    %
     \begin{align*}
    \P\left( \delta\sum_{t=0}^{T-1} D_t \leq - \lambda \right)  \leq \exp \left( - \frac{\lambda^2}{ 2 K^2 \delta^2 T + \lambda \delta (C + \delta \MM)}\right)~,
    \end{align*}
    %
    which concludes the proof of the Lemma.
\end{proof}


\begin{lemma}[First discretization term]
\label{lem:first_discretization_term}
    We have that, almost surely 
    %
    \begin{align}
         \P \left( \sup_{t \leq T}\, \delta^2 \sum_{t = 0}^{T-1} |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 > \MM\}} \geq \lambda \right) \leq \frac{K T \delta^2 d^2}{\lambda \MM}, 
    \end{align}
    where $K > 0$ depends solely on the model through $f, \nu$.
    %
\end{lemma}
%
\begin{proof}
This term is handled via a combination of Markov and Cauchy-Schwartz inequalities. First, notice that,
%
\begin{align*}
    \sup_{t \leq T}\, \delta^2 \sum_{t = 0}^{T-1} |m_t| \left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 > \MM\}} \leq T \delta^2 \sup_{t \leq T} \left\{ |m_t|\left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 > \MM\}} \right\}.
\end{align*}
%
Furthermore, for all $t \leq T$, all $\lambda > 0$, via Markov inequality, then Cauchy-Schwartz inequality,
\begin{align*}
\P\left(|m_t|\left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 > \MM\}} \geq \lambda \right) &\leq \frac{\E\left[|m_t|\left|\nabla_\theta l_t\right|^2 \mathds{1}_{\{\left|\nabla_\theta l_t\right|^2 > \MM\}}\right]}{\lambda} \\
&  \leq \frac{\sqrt{\E\left[\left|\nabla_\theta l_t\right|^4\right]}\sqrt{ \P\left(\left|\nabla_\theta l_t\right|^2 > \MM\right)}}{\lambda} \\
&  \leq \frac{\sqrt{\E\left[\left|\nabla_\theta l_t\right|^4\right]}\sqrt{ \E\left[\left|\nabla_\theta l_t\right|^4\right] / \MM^2 }}{\lambda} \\
&  \leq \frac{\E\left[\left|\nabla_\theta l_t\right|^4\right]}{\lambda \MM} \\
&  \leq \frac{K d^2}{\lambda \MM}~,
\end{align*}
where the last inequality is due to Lemma~\ref{lem:tech_bounds}. Multiplying this bound by $T \delta^2$ ends the proof the lemma.
\end{proof}

\begin{lemma}[Second discretization term]
\label{lem:second_discretization_term}
    For all $\lambda > 0$, we have that 
    %
    \begin{align}
        \P \left( \sup_{t \leq T}\, \delta^3 \sum_{k = 0}^{t-1} \xi_k  \geq \lambda  \right) \leq \frac{K T d \delta^3}{\lambda}, 
    \end{align}
    where $K > 0$ depends solely on the model through $f, \nu$.
    %
\end{lemma}
%
\begin{proof}
   Recall that $\xi_k = \left|\nabla_\theta l(\theta_k, x_k)\right|^2 |\nabla^S l(\theta_k, x_k) \cdot \theta^* |$. The bound follows from an application of Markov's inequality. Indeed, since all the terms of the sum are positive, the supremum is attained in $t = T-1$, and we shall only consider this case. For $\lambda > 0$, 
    \begin{align*}
        \P \left( \delta^3 \sum_{t = 0}^{T-1} \xi_t  \geq \lambda  \right) &\leq \frac{\delta^3}{\lambda}\E\left[ \sum_{t = 0}^{T-1} \xi_t \right]  \\
        &\leq \frac{T \delta^3}{\lambda}\sup_\theta \left\{  
        \E_x[\left|\nabla_\theta l(\theta, x)\right|^2 |\nabla^S l(\theta, x) \cdot \theta^* |]\right\} \\ 
        &\leq \frac{T \delta^3}{\lambda}\sup_\theta \left\{ 
        \sqrt{\E_x\left[[\left|\nabla_\theta l(\theta, x)\right|^4\right]} \sqrt{ \E_x \left[ |\nabla^S l(\theta, x) \cdot \theta^* |^2 \right]} \right\} \\  
        &\leq \frac{T \delta^3}{\lambda} 
        \sqrt{\sup_\theta \E_x\left[[\left|\nabla_\theta l(\theta, x)\right|^4\right]} \sqrt{ \sup_\theta \E_x \left[ |\nabla^S l(\theta, x) \cdot \theta^* |^2 \right]}  \\  
        &\leq \frac{T \delta^3}{\lambda} 
        \sqrt{\sup_\theta \E_x\left[[\left|\nabla_\theta l(\theta, x)\right|^4\right]} \sqrt{ \sup_\theta \E_x \left[ |\nabla^S l(\theta, x) \cdot \theta^* |^2 \right]}  \\ 
        &\leq \frac{T \delta^3}{\lambda} 
        \sqrt{K d^2} \sqrt{ K }  \\ 
        &\leq \frac{K T d \delta^3}{\lambda},
    \end{align*}
    %
    where the penultimate inequality comes from Lemma~\ref{lem:tech_bounds}. 
\end{proof}

\subsection{Strong recovery}
\label{subsec:strong_recovery}


The reasoning is almost identical to the one of the previous section, except from the fact that instead of tracking the growing movement on $(m_t)_{t \geq 0}$, we will track the decaying movement of $(1 - m_t)_{t \geq 0}$. 

\subsubsection{Upper bound on the residual}

As said in the main text, we place ourselves \textit{after} the weak recovery time. Thanks to the Markovian property of the SGD dynamics, we have the equality between all time $s > 0$ marginal laws of
%
\begin{align*}
\left(\theta_{\tau^+_{\nicefrac{1}{2}} + s}\  \bigg| \  \tau^+_{1/2},\, \theta_{\tau^+_{1/2}}\right) \overset{\text{Law}}{=} \left(\theta_{s} \  \bigg| \  \theta_{s} = \theta_{\tau^+_{1/2}} \right),  
\end{align*}
%
and hence the strong recovery question is equivalent to study the dynamics with initialization such that $m_\theta  = 1/2$. As demonstrated before we have that $\P(\tau_{1/2}^+ < \infty) \geq 1 - \mathsf{K} \varepsilon$ so that up to $\varepsilon$ terms, this conditioning does not hurt the probability of the later events. In fact this conditioning seems even artificial as it seems provable that $\tau_{1/2}^+$ is almost surely finite. Yet, we leave this more precise study for another time. 

\subsubsection{A (slightly) different decomposition}

Let us define for all $t \in \N $, the residual $u_t = 1 - m_{t+\tau^+_{1/2}} > 0$, and thanks to the lower bound given by Eq.~\eqref{eq:dynamics_lower_m}, we have
%
\begin{align*}
    u_{t+1} \leq u_t - C \delta u_t (m_t - b/\sqrt{d})^{s-1} + \delta\nabla^S M_t \cdot \theta^* + \delta^2 |m_t| |\nabla l(x_t, \theta_t)|^2  +  \delta^3 \xi_t,
\end{align*}
%
From there, the proof is similar to the weak recovery case, except that the extra-care we used for the term $\delta^2 |m_t| |\nabla l(x_t, \theta_t)|^2$ is not necessary. We use simply the decomposition of this term in a second martingale term $$N_t = |m_t| |\nabla l(x_t, \theta_t)|^2 - \E\left[|m_t| |\nabla l(x_t, \theta_t)|^2 | \mathcal{F}_{t-1}\right]$$ and the drift that we directly upper bound as $\E\left[|m_t| |\nabla l(x_t, \theta_t)|^2 | \mathcal{F}_{t-1}\right] \leq K d $. Now similarly to Lemma~\ref{lem:first_artingale_term}, we have the upper bound:
%
\begin{lemma}[New martingale term]
\label{lem:second_artingale_term}
    For all $\lambda > 0$, we have that 
    %
    \begin{align}
        \P \left( \sup_{t \leq T}\, \delta^2 \left|\sum_{k = 0}^{t-1} N_k \right|  \geq \lambda  \right) \leq \frac{K d^2 T \delta^4}{\lambda^2}, 
    \end{align}
    where $K > 0$, that depends solely on the model through $f, \nu$.
\end{lemma}

\begin{proof}
 This is a consequence of Doob's maximal inequality for the martingale. Indeed, for $t \leq T $, let $H_{t-1} = \sum_{k = 0}^{t-1} N_k$. We have that $N_t$ is a $\mathcal{F}_t$-adapted martingale and we have the following upper bounded on its variance:
    %
\begin{align*}
     \E[N_{t-1}^2] &= \E\left[\left(\sum_{k = 0}^{t-1} \nabla^S M_k \cdot \theta^* \right)^2\right] \\
     &= \E\left[\sum_{k = 0}^{t-1} N_k ^2\right]  \\
     &\leq  t \sup_\theta \E_x \left(N_k \right)^2 \\
     &\leq K d^2 t,
\end{align*}
%
where the last inequality comes from the Lemma~\ref{lem:tech_bounds}. Now, thanks to Doob's maximal inequality, we have for all $\lambda > 0$,
%
\begin{align*}
    \P \left( \sup_{t \leq T}\, \delta^2 |H_{t-1}|  \geq \lambda  \right) \leq \frac{ \E[H_{T-1}^2] \delta^4}{\lambda^2} \leq \frac{K d^2 T \delta^4}{\lambda^2},
\end{align*}
%
and this concludes the proof of the lemma.
\end{proof}
%
Now, everything is in order to prove the Theorem~\ref{thm:strong_recovery}.
%
\subsubsection{Proof of Theorem~\ref{thm:strong_recovery}}
%
Let us fix a small number $\varepsilon>0$. As previously, thanks to Lemmas~\ref{lem:first_artingale_term}, \ref{lem:second_discretization_term},  \ref{lem:second_artingale_term}, there exists $K>0$ that depends solely on the model such that we have the following upper bound: for all $\lambda$, and $ t \leq \tau_{1/3}^- \wedge \tau_{1-\varepsilon}^+ $ summing between times $0$ and $t$, 
%
\begin{align*}
    u_{t} &\leq u_0 - \frac{C \delta}{4^{s-1}} \sum_{k =0}^{t-1} u_k + K \delta^2 d + 3\lambda,
\end{align*}
%
with probability larger that $\displaystyle 1 - \left(\frac{K t \delta^2}{\lambda^2} + \frac{K d^2 t \delta^4}{\lambda^2} +\frac{K t d \delta^3}{\lambda}\right)$ and $d$ large enough. Let us choose $\lambda = 1/16$ and $\delta$ small enough so that $K\delta^2 d \leq \lambda$. Hence, realizing that $u_{0} \leq 1/2$, we have 
%
\begin{align*}
    u_{t} &\leq \frac34 - \frac{C \delta}{4^{s-1}} \sum_{k =0}^{t-1} u_k~,
\end{align*}
%
with probability at least $1 - \mathsf{K} t \delta^2 ( 1  + d^2 \delta^2 + d \delta) \gtrsim 1 - \mathsf{K} t \delta^2$, as we choose in any case $\delta = \varepsilon \mathcal{O}(1)$. Note that we used the same convention as in the weak recovery case that $\mathsf{K}$ denotes \textit{any} constant that simply depend on the model. We have by Grönwall inequality~(Lemma~\ref{lem:gron_bihari})
%
\begin{align*}
    u_{t} &\leq \frac34 \left(1 - \frac{C \delta}{4^{s-1}} \right)^{t} \leq \frac34 e^{- \frac{C \delta}{4^{s-1}}t }.
\end{align*}
%
Hence, as the right end side is smaller than $\varepsilon$ for the time $$t \delta \geq \frac{4^{s-1}}{C} \log(1/\varepsilon),$$
%
we choose such a $t$, so that with probability at least $1 - \mathsf{K} \delta \log(1/\varepsilon)$, the delayed hitting time $\overline{\tau}^+_{1-\varepsilon} := \inf\{ t \geq 0, \, u_{t} \leq \varepsilon  \}$ satisfies
%
$$\overline{\tau}^+_{1-\varepsilon} \leq \frac{4^{s-1}}{C \delta} \log(1/\varepsilon),$$ 
%
and taking $\delta = \varepsilon / d$ gives that with a probability at least $1 - \mathsf{K} \varepsilon \log(1/\varepsilon) / d$, we have 
%
$$ \overline{\tau}^+_{1-\varepsilon} \leq \frac{4^{s-1}}{C \varepsilon} d \log(1/\varepsilon). $$ 
%
Considering that $d$ is large and $\varepsilon$ is simply a constant we get that $1 - \mathsf{K} 
  \varepsilon \log(1/\varepsilon) / d \geq 1 - \mathsf{K}\varepsilon $  and and this concludes the proof of Theorem~\ref{thm:strong_recovery}.


\subsection{Some technical bounds}
\label{subsec:tech_SGD_bounds}

We end this section by providing (i) some necessary technical technical bound on the quantities appearing in the SGD controls (ii) some discrete versions of Grönwall-type lemmas.

\subsubsection{Technical bounds on models expectations}

\begin{lemma}[Technical bounds]
\label{lem:tech_bounds}
 We have that there exists a constant $K > 0$ solely depending on the function $\phi$ and the distribution $\nu$ such that:
 %
 \begin{align}
    \ \sup_{\theta \in \mathcal{S}_{d-1}} \E_x\left[ \langle \nabla_\theta^{\mathcal{S}} M(x, \theta), \theta_*\rangle^2\right] & \leq K~, \quad \text{and } \quad  \sup_\theta \E_x \left[ |\nabla^S l(\theta, x) \cdot \theta^* |^2 \right]] \leq K  \\
    \sup_{\theta \in \mathcal{S}_{d-1}} \E_x[\left|\nabla_\theta l(\theta, x)\right|^2] &\leq K d, \\
    \sup_{\theta \in \mathcal{S}_{d-1}} \E_x[\left|\nabla_\theta l(\theta, x)\right|^4] &\leq K d^2.
 \end{align}
%
\end{lemma}

%
\begin{proof}


In all the following proof we consider any $\theta \in \S$. Notice that we have the following calculation that is common to all the bounds we cover
%
\begin{align*}
    \nabla l (\theta, x) = x \phi'(x \cdot \theta)  \phi(x \cdot \theta_*)
\end{align*}
%

We treat the three bounds separately. 
%An important quantity that we define here is: $$ \mathsf{K}:= \sup_\theta \E_\nu \left[|x|^2  \phi'^2(x \cdot \theta)\phi^2(x \cdot \theta)\right] \vee K \vee 1,$$
%
%which is a constant that solely depends on the model through $\phi$ and $\nu$.

\textit{First terms.} We have that for all $x \in \R^d$,   
    %
    \begin{align*}
        M(x, \theta) &= l(x, \theta) - \E_\nu[l(x, \theta)],
    \end{align*}
    %
hence 
    %
    \begin{align*}
        \nabla^{\mathcal{S}}_\theta M(x, \theta) &=  \nabla^{\mathcal{S}}_\theta l(x, \theta) - \E_\nu[ \nabla^{\mathcal{S}}_\theta l(x, \theta)] \\
        &=  \nabla_\theta l(x, \theta) - \E_\nu[ \nabla_\theta l(x, \theta)] - (\theta \cdot \nabla_\theta l(x, \theta)) \theta + \E_\nu[ (\theta \cdot \nabla_\theta l(x, \theta)) \theta]
        ,
    \end{align*}
    %
    and finally, 
    %
        %
    \begin{align*}
        \nabla^{\mathcal{S}}_\theta M(x, \theta) \cdot \theta_* &=     \nabla_\theta l(x, \theta)\cdot \theta_* - \E_\nu[ \nabla_\theta l(x, \theta)\cdot \theta_*] - (\theta \cdot \nabla_\theta l(x, \theta)) m + \E_\nu[ (\theta \cdot \nabla_\theta l(x, \theta)) m].
    \end{align*}
    %
    hence thanks to applying the inequality $(a + b)^2 \leq 2 a^2 + 2 b^2$, this amounts to bound first
    %
      \begin{align*}
        \E_x \left(\nabla_\theta l(x, \theta)\cdot \theta_* \right)^2 &=   \E_x \left[ \left( x \cdot \theta_*\right)^2 \phi'^2(x \cdot \theta)  \phi^2(x \cdot \theta_*) \right] \leq K,
    \end{align*}
    %
    and second
      \begin{align*}
        \E_x \left((\nabla_\theta l(x, \theta)\cdot \theta) m \right)^2 &\leq   \E_x \left[ \left( x \cdot \theta\right)^2 \phi'^2(x \cdot \theta)  \phi^2(x \cdot \theta_*) \right] \leq K.
    \end{align*}
    

    \textit{Second term.} We have 
        \begin{align*}
        \E_x \left|\nabla_\theta l(x, \theta)\right|^2 &=  \E_x \left[|x|^2 \phi'^2(x \cdot \theta)  \phi^2(x \cdot \theta_*) \right] \leq Kd~.
    \end{align*}
        \textit{Third term.} We have similarly
        \begin{align*}
        \E_x \left|\nabla_\theta l(x, \theta)\right|^4 &=  \E_x \left[|x|^4 \phi'^2(x \cdot \theta)  \phi^2(x \cdot \theta_*) \right] \leq Kd^2~.
    \end{align*}

\end{proof}


\subsubsection{Standard tail probabilities for submartingales}

We recall here a theorem on submartingales from Freedman. This is an adaptation from Theorem 4.1 stated in~\cite{freedman1975tail}.
%
\begin{theorem}[Submartinagle tail bound]
\label{thm:freedman}
    Suppose that $(X_t)_{t \in \N}$ is random sequence adapted to a filtration $(\mathcal{F}_t)_{t \in \N}$. For $T \geq 1$, suppose there exist $a, b > 0$ such that $\E[X_t\, |\, \mathcal{F}_{t-1}] \geq 0$, the almost sure upper-bound $\sup_{t\leq T} |X_t| \leq a $ as well as $\sup_{t\leq T} \E[X_t^2\, |\, \mathcal{F}_{t-1}] \leq b$, then for all $\lambda > 0$, 
    %
    \begin{align}
    \P\left( \sum_{k=1}^T X_k \leq - \lambda \right)  \leq \exp \left( - \frac{\lambda^2}{2(T b + \lambda a)}\right)
    \end{align}
    %
\end{theorem}

\subsubsection{Discrete Grönwall and Bihari-Lasalle bounds}

We now turn to stating a classical comparison lemma for recursive inequalities.



\begin{lemma}[Grönwall and  Bihari-Lasalle]
\label{lem:gron_bihari}
We have the bounds for the recursive inequalities:

{\bfseries Case $s = 2$.} Suppose $(m_t)_{t \in \N}$ satisfies for $s \geq 3$, and positives numbers $a,b>0$, and $b< a/2 \wedge 1$,
%
\begin{align}
    m_t &\geq a + b \sum_{k = 0}^{t - 1} m_k, \qquad \text{ then, } \ \ m_t \geq a \left( 1 + b \right)^{t} \\
    m_t &\leq a - b \sum_{k = 0}^{t - 1} m_k, \qquad \text{ then, } \ \ m_t \leq a \left( 1 - b \right)^{t}.
\end{align}


{\bfseries Case $s \geq 3$.} Suppose $(m_t)_{t \in \N}$ satisfies for $s \geq 3$, and positives numbers $a,b>0$:
%
\begin{align}
    m_t \geq a + b \sum_{k = 0}^{t - 1} m_k^{s-1}, \qquad \text{ then, } \ \ m_t \geq a \left( 1 - (s-2)b a^{s-2} t\right)^{-\frac{1}{s-2}}.
\end{align}
%
\end{lemma}
%
\begin{proof}
The case $s = 2$ is known to be the discrete version of the Grönwall lemma and is treated in all standard textbooks, the case $s \geq 3$ referred to as the Bihari-Lasalle inequality is for example proven in Appendix C of \cite{arous2021online}. 
\end{proof}