\section{Comparison of Computational Costs}\label{subsec:computational-costs}
In this section we compare the overall computational costs during the offline and the online phase of the three methods considered in this contribution: Solving the linear system from~\cref{equ:linear-system-for-optimal-final-time-adjoint} for the optimal final time adjoint~$\varphi_\mu^*(T)$, computing the approximation~$\tilde{\varphi}_\mu^N$ using~\Cref{alg:online-greedy}, and computing the machine learning surrogate solution~$\hat{\varphi}_\mu^N$ as described in~\Cref{sec:reduced-order-machine-learning}. Additionally, we also consider the computational costs for evaluating the error estimator~$\eta_\mu(p)$ for a given approximate final time adjoint~$p\in X$. The error estimator can be applied to both reduced models provided by~\Cref{alg:online-greedy,alg:online-machine-learning} and might be helpful to certify the results a posteriori (see also~\Cref{rem:adaptive-model-hierarchy}). It should as well be stressed at the beginning of the section that all costs are stated for the finite-dimensional setting, i.e.~we assume here~$n \coloneqq \dim(X) < \infty$.
\par
The overall assumption is that the reduced basis constructed by the greedy algorithm is small compared to the dimension of the state space, i.e.~that it holds~$N\ll n$. In this case, we will see below that the reduced models considered in this contribution can greatly reduce the computational effort compared to the exact solution of the optimal control problem. Due to the error estimates presented in~\Cref{subsec:error-analysis-greedy-algorithm,subsec:online-computations,subsec:error-estimation-ml}, the reduced models can achieve accurate approximations at the same time as well.
\par
Below we first consider general assumptions on the discretization of the optimal control system and the costs for computing certain matrix decompositions that help reducing the overall computational effort. Afterwards, we continue by discussing the online costs and the costs for the a posteriori error estimator, because some of these algorithms appear as steps in the offline procedures as well. Finally, we state the costs for the offline computation of the considered reduced models.

\subsection{Preprocessing Costs}
In the following, we assume that a temporal discretization consisting of~$n_t\in\setN$ time steps is used for each ordinary differential equation that arises in the algorithms. This means that we use a time step size of~$\Delta t=T/n_t$. Furthermore, we use implicit time stepping methods, such as the well-known Crank-Nicolson scheme~\cite{crank1947practical}, which require the solution of linear systems of equations in each time step. Since the system matrices arising in our setting are assumed to be independent of time, all linear systems of equations involve the same fixed matrices in each time step (for a fixed parameter~$\mu\in\params$). We can thus first compute the lowerâ€“upper (LU)~decomposition~\cite[Chapter~3.2]{golub1996matrix} of~$A_\mu$ and afterwards solve linear systems involving~$A_\mu$ by forward and backward elimination. Similarly, since the matrix~$R$ is symmetric and positive-definite, it possesses a Cholesky decomposition~\cite[Chapter~4.2.3]{golub1996matrix}, i.e.~we can write~$R=\tilde{R}\tilde{R}^\top$ where~$\tilde{R}\in\setR^{m\times m}$ is a lower triangular matrix. Using the Cholesky decomposition, one can replace the costly solution of a dense linear system by forward and backward elimination, using that the Cholesky factor~$\tilde{R}$ is a lower triangular matrix. Hence, the Cholesky decomposition of~$R$ can be used to speedup the computation of~$u_\mu$, which involves solving multiple linear systems of equations with~$R$ as system matrix (see~\cref{equ:optimality-system-odes}). Certainly, in the case of a parameter-dependent weighting matrix~$R_\mu$, it is generally impossible to precompute the Cholesky factor of the weighting matrix since it changes with the parameter.
\par
Computing the LU~decomposition of~$A_\mu\in\setR^{n\times n}$ requires~$\mathcal{O}(n^3)$ operations in general. For the Cholesky decomposition of~$R\in\setR^{m\times m}$, in total~$\mathcal{O}(m^3)$ operations are needed. The preprocessing costs therefore amount to
\begin{align}\label{equ:preprocessing-cost}
    \mathcal{O}\big(n^3+m^3\big)
\end{align}
operations. To solve an ordinary differential equation of the form as in~\cref{equ:parametrized-control-system} or in~\cref{equ:optimality-system-odes}, we need~$\mathcal{O}\big(n_tn(n+m)\big)$ operations under the assumption that the respective LU~decompositions and Cholesky decompositions have already been computed before. For systems with sparse system matrices, for instance those stemming from finite element discretizations of PDEs, it might nevertheless be beneficial to use iterative solvers instead of LU~decompositions.

\subsection{Costs for Calculating the (approximate) Optimal Control}
In the next three subsections we discuss the costs for computing the exact optimal control, its approximation using~\Cref{alg:online-greedy} and the machine learning based approximation from~\Cref{alg:online-machine-learning}. We already emphasize at this point that the costs stated below will contain the costs for computing the (approximate) optimal adjoint datum as well as the costs for deriving the (approximate) optimal control from the adjoint datum. Furthermore, we note that in typical applications it holds~$n_t\geq n$ and~$m\ll n$.
\subsubsection{Exact Optimal Control}\label{subsubsec:costs-exact-optimal-control}
In order to compute the exact optimal final time adjoint~$\varphi_\mu^*(T)$ (up to a prescribed numerical tolerance), one has to solve the linear system stated in~\Cref{lem:linear-system}. Recall that~$M$ and~$\Gramian$ are symmetric and positive semi-definite matrices. If they commute, also~$I+M\Gramian$ is symmetric and positive semi-definite. Consequently, the linear system of equations in~\eqref{equ:linear-system-for-optimal-final-time-adjoint} can be solved efficiently for~$\varphi_\mu^*(T)$ using the conjugate gradient~(CG) method~\cite[Chapter~10.2]{golub1996matrix}. The~CG~method only requires applications of the (symmetric and positive-definite) system matrix to certain vectors. As described in~\Cref{rem:applying-gramian-to-vectors}, this can be done in our case without explicitly constructing the Gramian matrix~$\Gramian$. However, it is still required to solve two initial value problems (one for the adjoint state and one for the primal state) to compute a single product~$(I+M\Gramian)p$ for a vector~$p\in X$. This step can therefore become costly for large systems and needs to be performed multiple times in each iteration of the~CG~algorithm. We also emphasize that the system matrix~$I+M\Gramian$ depends on the parameter~$\mu\in\params$ and can thus not be efficiently precomputed (even under assumptions such as affine parameter-dependence of~$A_\mu$ and~$B_\mu$, due to the matrix exponential in~\cref{equ:definition-gramian-matrix}). The~CG~algorithm requires to solve the optimality system in~\cref{equ:optimality-system-main} multiple times for different terminal conditions for the adjoint state. The number of iterations of the~CG~algorithm depends on the condition number of the system matrix at hand. To enable a comparison with the reduced models below, let us denote by~$n_\mathrm{CG}\in\setN$, $n_\mathrm{CG}\leq n$, the number of iterations needed in the~CG~algorithm until the norm of the residual drops below a prescribed tolerance (we omit at this point an explicit dependence of the number of iterations on the parameter~$\mu$, but instead assume that the number of iterations is similar for all parameters~$\mu\in\params$). Computation of the optimal control~$u_\mu^*$ for a parameter~$\mu\in\params$ requires
\[
    \mathcal{O}\big(n^3+m^3+n_\mathrm{CG}n_tn(n+m)\big)
\]
operations, where also the preprocessing costs stated in~\cref{equ:preprocessing-cost} are considered.
\begin{remark}
    If~$M\Gramian$ is not symmetric and positive semi-definite (or if no guarantees on it are given a priori), the linear system in~\Cref{lem:linear-system} can be solved by other iterative methods. For instance, gradient descent can be used on the least-squares problem. Again, the computation of the full Gramian is not required, but only its application on vectors. The cost per each iteration is then the cost of the resolution of a forward and a backward dynamical system in dimension~$n$ with~$n_t$ time steps. Moreover, for a given tolerance, the number of iterations needed to achieve that precision is given by the convergence rates of the method. In what follows, for simplicity and due to the clearness of the complexity of the~CG~method, we stick to the case in which the~CG~method can be applied.
\end{remark}
\subsubsection{Reduced Optimal Control based on the Greedy Procedure}\label{subsubsec:costs-greedy-online}
In order to compute the reduced optimal adjoint~$\tilde{\varphi}_\mu^N$ for a given parameter~$\mu\in\params$ according to~\Cref{alg:online-greedy}, we first compute the decompositions of~$R$ and~$A_\mu$ as already described above. Furthermore, Line~\ref{lst:online-compute-final-time-states} in~\Cref{alg:online-greedy} requires the solution of~$2N$ evolution systems (the primal and the adjoint for each vector of the reduced basis), each of the cost~$\mathcal{O}(n_tn(n+m))$. Assembling the matrix~$\bar{X}_\mu^\top\bar{X}_\mu\in\setR^{N\times N}$ requires~$\mathcal{O}(N^2n)$ operations, while solving the corresponding linear system of equations for the reduced coefficients in~\cref{equ:linear-system-for-reduced-coefficients} is of complexity~$\mathcal{O}(N^3)$. Computing the linear combination of the basis functions according to~\cref{equ:definition-approximate-adjoint} requires~$\mathcal{O}(Nn)$ operations and is hence negligible compared to the complexity for setting up the linear system. Altogether, taking into account the preprocessing costs~\eqref{equ:preprocessing-cost}, the costs for computing the reduced optimal control using~\Cref{alg:online-greedy} is of complexity
\[
    \mathcal{O}\big(n^3+m^3+Nn_tn(n+m)+N^2n+N^3\big).
\]
Consequently, the online phase of the greedy reduced order model provides a speedup compared to computing the exact optimal control as long as it holds~$N<n_\mathrm{CG}$.
\subsubsection{Reduced Optimal Control based on the Machine Learning Greedy Procedure}\label{subsubsec:costs-ml-procedure}
Typically, the evaluation of a machine learning surrogate~$\hat{\pi}_N\colon\params\subset\setR^p\to\setR^N$ requires~$\mathcal{O}\big(\chi(p+N)\big)$ operations, where~$\chi\colon\setN\to\setN$ is a polynomial of moderate degree. For instance in the case of deep neural networks, see~\Cref{subsec:deep-neural-networks}, the numbers of neurons in each layer of the network usually scale with~$p+N$ and therefore the computational effort for a forward pass through the neural network is of complexity~$\mathcal{O}\big((p+N)^2\big)$ due to the required matrix-vector multiplications. The constants in the bound depend on the number of layers in the network. Together with the reconstruction step of the approximate final time adjoint from the coefficients, which requires~$\mathcal{O}(Nn)$ operations, see~\cref{equ:definition-approximate-adjoint-ml}, the total costs for the online phase in the case of the machine learning reduced model amount to
\[
    \mathcal{O}\big(n^3+m^3+n_tn(n+m)+\chi(p+N)+Nn\big).
\]
We emphasize at this point that the costs for computing the approximate control using the machine learning surrogate is dominated by the computation of the control for the given approximate final time adjoint, which requires~$\mathcal{O}\big(n^3+m^3+n_tn(n+m)\big)$ operations. The computation of the approximate final time adjoint itself is typically much faster and of complexity~$\mathcal{O}\big(\chi(p+N)+Nn\big)$ when applying the machine learning surrogate.
\par
In comparison to the computational costs for the reduced model based on the greedy procedure, see~\Cref{subsubsec:costs-greedy-online}, the complexity of evaluating the machine learning based reduced model (under the assumptions that~$p\ll n$, $N\ll n$, $m\ll n$, and~$n_t\geq n$) is given by~$\mathcal{O}\big(n_tn(n+m)\big)$ while the reduced model from~\Cref{subsec:online-computations} requires~$O\big(Nn_tn(n+m)\big)$ operations. As we will see in the numerical experiments in~\Cref{sec:numerical-experiments}, this reduction in the computational complexity will result in a serious speedup when using the machine learning procedure.

\subsection{Evaluation of the a Posteriori Error Estimator}\label{subsec:costs-a-posteriori-estimator}
Given a parameter~$\mu\in\params$ and an approximate final time adjoint~$p\in X$, evaluating the error estimate~$\eta_\mu(p)$ defined in~\cref{equ:definition-error-estimator} mainly requires the same steps as evaluating the reduced model in~\Cref{alg:online-greedy}, except that the system in~\cref{equ:optimality-system-main} has to be solved only once and not~$N$~times. Therefore, the dominating costs for the error estimator are of the order
\[
    \mathcal{O}\big(n^3+m^3+n_tn(n+m)\big).
\]
We observe in particular that the evaluation of the error estimator is of the same complexity as computing the reduced optimal control using the machine learning model, see~\Cref{subsubsec:costs-ml-procedure}.
\par
If the approximate final time adjoint~$p\in X$ has been computed using the online procedure of the~\RBROM{} from~\Cref{alg:online-greedy}, i.e.~if it holds~$p=\tilde{\varphi}_\mu^N\in X^N$, we can use that
\begin{align}\label{equ:simplified-computation-perturbed-adjoint}
    (I+M\Gramian)\tilde{\varphi}_\mu^N = \sum\limits_{i=1}^{N}\alpha_i^\mu(I+M\Gramian)\varphi_i = \sum\limits_{i=1}^{N}\alpha_i^\mu x_i^\mu.
\end{align}
Therefore, we can reuse the states~$x_i^\mu$ that have already been computed in~Line~\ref{lst:online-compute-final-time-states} and the right hand side of the system given by~$M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)$ computed in~Line~\ref{lst:online-solve-system-coefficients} of~\Cref{alg:online-greedy} to reduce the computational costs of evaluating the error estimator~$\eta_\mu(\tilde{\varphi}_\mu^N)$ to~$\mathcal{O}\big(Nn\big)$, which is required for computing~$(I+M\Gramian)\tilde{\varphi}_\mu^N$ according to~\eqref{equ:simplified-computation-perturbed-adjoint} and the norm of the residual. All other components of the residual have already been computed in~\Cref{alg:online-greedy}. This simplification can also be used in the implementation of the offline greedy procedure in~\Cref{alg:offline-greedy}.

\subsection{Costs for the Offline Computations}
Subsequently we state the costs for performing the greedy procedure in~\Cref{alg:offline-greedy} and the machine learning greedy procedure from~\Cref{alg:offline-machine-learning}. In the discussion we make use of the results on the costs for the online computations obtained above.
\subsubsection{Greedy Procedure}
If the storage capabilities allow, we can precompute the LU~decomposition of~$A_\mu$ for all parameters~$\mu\in\paramstrain$, which requires~$\mathcal{O}(n_\mathrm{train}n^3)$ operations. Afterwards, the \texttt{while}-loop in~\Cref{alg:offline-greedy} runs for~$N$ iterations (where~$N$ is certainly unknown before executing the algorithm). In each iteration, the exact optimal adjoint datum for one training parameter has to be computed (which needs~$\mathcal{O}(n_\mathrm{CG}n_tn(n+m))$ operations, see~\Cref{subsubsec:costs-exact-optimal-control}). Furthermore, for each of the~$n_\mathrm{train}$ training parameters the reduced optimal adjoint has to be computed, which is of complexity~$\mathcal{O}(Nn_tn(n+m)+N^2n+N^3)$ (since the iteration counter~$k$ is increased up to a value of~$N$), and the a posteriori error estimator is evaluated (which costs only~$\mathcal{O}(n)$ operations, see the discussion in~\Cref{subsec:costs-a-posteriori-estimator}). Altogether, the complexity of running~\Cref{alg:offline-greedy} can be estimated as
\[
    \mathcal{O}\left(n_\mathrm{train}n^3+m^3+N\left(n_\mathrm{CG}n_tn(n+m)+n_\mathrm{train}\big(Nn_tn(n+m)+N^2n+N^3\big)\right)\right).
\]
\subsubsection{Machine Learning Greedy Procedure}
To build a machine learning surrogate using the machine learning greedy procedure from~\Cref{alg:offline-machine-learning}, the greedy procedure in~\Cref{alg:offline-greedy} is performed as the first step. Afterwards, the machine learning algorithm is trained using the collected training data~$D_\mathrm{train}$. If we denote by~$C_\mathrm{train}$ the costs of training the machine learning surrogate (which typically depends on the size~$n_\mathrm{train}$ of the training set, the dimension~$p$ of the parameter space, the reduced dimension~$N$, and the polynomial~$\chi$ introduced in~\Cref{subsubsec:costs-ml-procedure}), the overall costs for~\Cref{alg:offline-machine-learning} are of the complexity
\[
    \mathcal{O}\left(n_\mathrm{train}n^3+m^3+N\left(n_\mathrm{CG}n_tn(n+m)+n_\mathrm{train}\big(Nn_tn(n+m)+N^2n+N^3\big)\right)+C_\mathrm{train}\right).
\]