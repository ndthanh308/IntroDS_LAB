\section{Conclusion and Outlook}\label{sec:conclusion-outlook}
In this contribution we considered parameterized optimal control problems where the objective functional penalizes a (weighted) deviation from a target state and a large control energy. To solve such optimal control problems fast for several values of the parameter, for instance in a real-time application or many-query scenario, we first extended the greedy control procedure, previously developed in the controllability context, either the exact~\cite{lazar2016greedy}, or the approximate one~\cite{lazar2023greedy}, to this setting. In contrast to the penalisation approach applied in this work,  the desired target state is considered as a constraint in the cited papers. However, both approaches allow for an application of the Hilbert uniqueness method. In particular, the optimal control, i.e.~the one that minimizes the objective functional, can be completely described by the optimal final state of the adjoint problem. For this reason, the  greedy algorithm builds a reduced basis for the manifold of optimal final time adjoint states over the parameter set, and not for the manifold of optimal controls which is usually more complex and difficult to describe.
\par
Secondly, we applied machine learning approaches, such as deep neural networks, kernel methods, and Gaussian process regression, to learn the reduced coefficients as a function of the parameter. We derived error bounds for the greedy approximation and proved that the proposed algorithm is indeed a weak greedy algorithm. Furthermore, we also showed how to bound the error of the machine learning with respect to the greedy error and the error in approximating the coefficients. A comparison of the computational costs reveals the enormous potential in reducing the computational effort by applying machine learning in our scenario. The numerical experiments exhibit that the machine learning surrogates can accurately approximate the reduced coefficients while providing a massive speedup compared to the exact solution of the optimal control problem. In addition, due to a posteriori error estimates we derived, it is possible to obtain a reliable bound on the error of the machine learning results in a cheap manner without computing the exact solution.
\par
The numerical experiments in this paper are run for control systems whose underlying dynamics (governed by the heat and the damped wave operator) are dissipative, which is crucial for the efficiency of the greedy algorithm. Namely, a conservative system supports large, non-linear variations of optimal final time adjoints with respect to the parameter~\cite{greif2019decay}, which requires a relatively big reduced basis to obtain a sufficiently accurate reduced model (cf.~the discussion on the damping constant in~\Cref{subsec:wave-equation-experiment}). In this way, the developed methods can be similarly (and successfully) applied to other dissipative systems, such as advection-diffusion-reaction ones. Although our examples are based on PDEs in one space dimension, the same procedure can be applied in a higher dimensions cases as well. Here we do not expect difficulties, apart from those conditioned by the efficiency of standard numerical methods that have to be employed in the offline phase.
\par
As discussed extensively in~\Cref{sec:reduced-order-machine-learning}, many different machine learning algorithms can be integrate in the approach. The only requirement on the machine learning surrogate is that it approximates vector-valued functions and can be trained in a supervised manner, i.e.~by providing samples of the function to approximate. A theoretical investigation of the machine learning models in terms of approximation quality of the parameter to solution map could be a topic of future research. For instance for the kernel methods applied above, theoretical results on the approximation properties and bounds for the errors exist (e.g.~\cite{santin2017convergence}) that allow for a rigorous a priori error bound that involves only the $P$-greedy tolerance~$\varepsilon_P$ and the norm of the parameter to solution map in the reproducing kernel Hilbert space (under the assumption that this mapping is indeed contained in the reproducing kernel Hilbert space).
\par
To further speedup the computations of the reduced models during the online phase (in particular Lines~\ref{lst:online-compute-final-time-states}, \ref{lst:online-time-dependent-adjoint}, and~\ref{lst:online-greedy-control} in~\Cref{alg:online-greedy} and Lines~\ref{lst:online-ml-adjoint-equation} and~\ref{lst:online-ml-control} in~\Cref{alg:online-machine-learning}), an additional hyper-reduction, that accelerates solving the equation for the time-dependent adjoint variable, may be applied. Similarly, the error estimator could be approximated by replacing the system in~\cref{equ:optimality-system-odes} by a reduced system. To guarantee a reliable and efficient error estimator, theoretical investigations of such an approach would be indispensable. However, we should mention that replacing the optimality system in~\cref{equ:optimality-system-main} by a surrogate would also speedup the exact computation of the optimal final time adjoint. Several approaches considering parametrized model order reduction of control systems have been suggested in the last decades, see~\cite{benner2015survey} for a survey of methods.
\par
An application of the adaptive model hierarchy from~\cite{haasdonk2023certified} as described in~\Cref{rem:adaptive-model-hierarchy} to the parametrized optimal control setting could be an interesting extension and combination of the approaches. The error estimators and reduced models developed in this paper would serve as the main ingredients to an adaptive model hierarchy with certified results for parametrized optimal control problems.
\par
Finally, the approaches discussed in this paper can be extended to other classes of optimal control problems. As an example, one might  explore a generalization of the method to linear time-variant systems with time-dependent parameters, objective functionals that are not necessarily quadratic with respect to the state and the control, or to problems with additional constraints such as bounds on the control variables. Instead of a target state, it may also be of interest to include an output quantity and an objective functional measuring a deviation from a target output. Similarly, one could penalise a deviation of the whole trajectory from some desired one. This would require the study of the coupled optimality system, and the optimal feedback controls obtained by solving the corresponding Riccati equations should be explored. In such a setting, the greedy approach should probably be accompanied by POD, which is common when constructing a reduced basis for a manifold of time-dependent functions~\cite{hesthaven2016certified}.