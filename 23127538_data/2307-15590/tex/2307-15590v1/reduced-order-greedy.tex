\section{Reduced Order Modeling by a Greedy Algorithm}\label{sec:reduced-order-modeling-greedy}
In the previous section, we derived a linear system for the optimal final time adjoint state~$\varphi_\mu^*(T)$ and observed that the optimal control is already completely determined by~$\varphi_\mu^*(T)$. The main goal of this section is to extend the greedy control algorithm introduced in~\cite{lazar2016greedy} to the setting of parametrized optimal control problems of the form~\eqref{equ:parametrized-optimal-control-problem}.

\subsection{Error Estimation by considering the Residual}\label{subsec:residual-based-error-estimator}
Given a parameter~$\mu\in\params$ and an arbitrary vector~$p\in\X$, we define the error estimator~$\eta_\mu\colon\X\to\setR$ as
\begin{align}\label{equ:definition-error-estimator}
    \eta_\mu(p) \coloneqq \norm{M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right) - (I+M\Gramian)p},
\end{align}
where the term inside the norm corresponds to the residual of equation~\eqref{equ:linear-system-for-optimal-final-time-adjoint}.
\par
We obtain the following theorem which states that~$\eta_\mu$ is an efficient and reliable error estimator for the final time adjoint state, where we use in the proof that the operator~$M\Gramian$ is positive (see~\Cref{as:symmetric-product}).
\begin{theorem}[Efficient and reliable error estimator for the distance from the adjoint]\label{thm:error-estimator-adjoint}
    Let~$\mu\in\params$ be a parameter, $\varphi_\mu^*(T)\in\X$ the corresponding optimal final time adjoint solving equation~\eqref{equ:linear-system-for-optimal-final-time-adjoint}, and let~$p\in\X$ be an approximate final time adjoint. Then it holds
    \begin{align}\label{equ:error-estimator-bounds}
        \norm{\varphi_\mu^*(T)-p} \quad \leq \quad \eta_\mu(p) \quad \leq \quad \norm{I+M\Gramian}_{\mathcal{L}(\X,\X)} \ \norm{\varphi_\mu^*(T)-p}.
    \end{align}
\end{theorem}
\begin{proof}
    For the optimal final time adjoint ~$\varphi_\mu^*(T)\in\X$, according to~\Cref{lem:linear-system},  it holds
    \[
        M(e^{A_\mu T}x_\mu^0-x_\mu^T)=(I+M\Gramian)\varphi_\mu^*(T).
    \]
     Plugging this equation into the definition of the error estimator, we obtain
    \begin{align*}
        \eta_\mu(p)^2 &= \norm{(I+M\Gramian)(\varphi_\mu^*(T)-p)}^2 \\
        &=\norm{\varphi_\mu^*(T)-p)}^2+\norm{M\Gramian(\varphi_\mu^*(T)-p)}^2+2\langle M\Gramian(\varphi_\mu^*(T)-p),(\varphi_\mu^*(T)-p)\rangle \\
        &\geq \norm{\varphi_\mu^*(T)-p)}^2,
    \end{align*}
    where we used that $M\Gramian$ is positive. From here the lower bound in~\eqref{equ:error-estimator-bounds} follows directly. The upper bound comes simply from the definition of the operator norm.
\end{proof}
The error estimator~$\eta_\mu(p)$ can be used to quantify the quality of an approximation of the optimal final time adjoint~$\varphi_\mu^*(T)$ by an arbitrary~$p\in\X$ without ever computing~$\varphi_\mu^*(T)$. Instead, it is only necessary to solve the linear initial value problem~\eqref{equ:optimality-system-main} twice: first with~$x_\mu(0)=0$, $\varphi_\mu(T)=p$ (in order to compute~$-\Gramian p$) and then with~$x_\mu(0)=x_\mu^0$, $\varphi_\mu(T)=0$ (i.e.~without control of the state equation~\eqref{equ:parametrized-control-system}) in order to determine~$e^{A_\mu T}x_\mu^0$ (the term~$e^{A_\mu T}x_\mu^0$ corresponds to the free dynamics). The error estimator~$\eta_\mu$ can therefore be evaluated much more efficiently than solving the full optimal control problem.
\par
In the following section, we describe an algorithm that constructs a low-dimensional subspace of~$\X$ in which an approximation of the optimal final time adjoint is searched. The basis of this subspace is chosen by a greedy algorithm that uses the error estimator~$\eta_\mu$ to determine the next parameter and in particular the associated final time adjoint that is added to the basis.

\subsection{Greedy Procedure for constructing a Reduced Basis}\label{subsec:greedy-for-reduced-basis}
Let~$\varepsilon>0$ be a prescribed tolerance. We aim at constructing a reduced subspace~$\X^N\subset\X$ of low dimension~$\dim\X^N=N$, such that~$\X^N$ approximates the manifold
\begin{align}\label{equ:definition-solution-manifold}
    \mathcal{M} \coloneqq \{\varphi_\mu^*(T):\mu\in\params\} \subset\X
\end{align}
of all possible optimal final time adjoints up to a tolerance of~$\varepsilon$. Since~$\params$ is compact and the mapping~$\params\ni\mu\mapsto\varphi_\mu^*(T)\in\X$ is Lipschitz continuous (which follows from~\Cref{as:continuity-parameter-to-system-matrices} and the characterization of~$\varphi_\mu^*(T)$ as solution of the linear equation in~\eqref{equ:linear-system-for-optimal-final-time-adjoint}), the set~$\mathcal{M}$ is also compact. The distance~$\operatorname{dist}(Y,Z)$ of a subspace~$Y\subset\X$ to a compact set~$Z\subset\X$ is defined as
\[
    \operatorname{dist}(Y,Z) \coloneqq \sup\limits_{z\in Z}\inf\limits_{y\in Y} \norm{y-z}.
\]
We thus search for a reduced space~$\X^N\subset\X$ such that~$\operatorname{dist}(\X^N,\mathcal{M})\leq\varepsilon$. To this end, greedy algorithms construct a sequence of reduced bases~$\Phi^0,\Phi^1,\dots$, starting with~$\Phi^0=\emptyset\subset\X$, and corresponding reduced spaces~$\X^0=\Span{\Phi^0}=\{0\},\X^1=\Span{\Phi^1},\dots$ by successively adding new basis functions. These functions are selected as snapshots, i.e.~optimal final time adjoints for certain parameters. Assume a reduced basis~$\Phi^k=\{\varphi_1,\dots,\varphi_k\}\subset\X$ of dimension~$k$ is given with associated reduced space~$\X^k=\Span{\Phi^k}$. The next basis function~$\varphi_{k+1}\in\X$ is chosen as~$\varphi_{k+1}=\varphi_{\mu_{k+1}}^*(T)$, where the parameter~$\mu_{k+1}\in\params$ is determined such that the distance~$\operatorname{dist}(\X^k,\{\varphi_{\mu_{k+1}}^*(T)\})$ of~$\varphi_{\mu_{k+1}}^*(T)$ to~$\X^k$ (or a suitable estimate of this distance that is cheaper to compute) is the largest among all parameters~$\mu\in\params$, i.e.~it holds
\[
    \mu_{k+1} \in \argmax\limits_{\mu\in\params}\,\operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\}).
\]
After choosing the parameter~$\mu_{k+1}$, the optimal final time adjoint~$\varphi_{\mu_{k+1}}^*(T)$ is computed by solving~\cref{equ:linear-system-for-optimal-final-time-adjoint} and added to the reduced basis, i.e.~the new basis~$\Phi^{k+1}$ is defined as~$\Phi^{k+1}=\Phi^k\cup\{\varphi_{k+1}\}$ for~$\varphi_{k+1}=\varphi_{\mu_{k+1}}^*(T)$. After updating the reduced basis, the error for the parameter~$\mu_{k+1}$ vanishes, since the optimal final time adjoint for~$\mu_{k+1}$ is now contained in the reduced space~$\X^{k+1}$, i.e.~$\varphi_{\mu_{k+1}}^*(T)\in\Phi^{k+1}\subset\X^{k+1}$.
\par
To make the construction of~$\Phi^N$ computationally feasible one usually restricts the search for a new parameter to a finite set~$\paramstrain\subset\params$ of~$n_\mathrm{train}\coloneqq\lvert\paramstrain\rvert<\infty$ training parameters. This set~$\paramstrain$ of training parameters is chosen to be dense in the overall set~$\params$ of parameters (in the sense that the union of balls with certain radius around the training parameters covers the whole parameter set), see also the pseudocode of the procedure provided in~\Cref{alg:offline-greedy} below. Furthermore, instead of computing the true distance~$\operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\})$ of a subspace~$\X^k$ to~$\varphi_\mu^*(T)$ for all training parameters~$\mu\in\paramstrain$, an estimate of this distance is used that does not require the computation of~$\varphi_\mu^*(T)$. In our setting, we will estimate the distance using the error estimator from~\cref{equ:definition-error-estimator} and a suitably chosen approximate final time adjoint.
\par
Once having constructed a reduced basis~$\Phi^k$, it is used to obtain an approximate final time adjoint for an arbitrary parameter~$\mu\in\params$. To this end, we first compute~$x_i^\mu=(I+M\Gramian)\varphi_i\in\X$ for all~$i=1,\dots,k$. The state~$-\Gramian\varphi_i\in\X$ for~$i\in\{1,\dots,k\}$ corresponds to the final time state~$x_\mu(T)$ of the system~\eqref{equ:optimality-system-odes} with zero initial datum and the control determined by the basis function~$\varphi_i\in\Phi^k$. Thus, $x_i^\mu$ can be seen as  the perturbation of the final state~$-\Gramian\varphi_i\in\X$ by the operator~$-\left((\Gramian)^{-1} + M\right)$, i.e.~it holds~$-\left((\Gramian)^{-1} + M\right)\left(-\Gramian\varphi_i\right)=(I+M\Gramian)\varphi_i=x_i^\mu$. Hence, the space~$\redSpaceTransformed^k\coloneqq\Span{x_1^\mu,\dots,x_k^\mu}=(I+M\Gramian)\X^k\subset\X$ contains all possible perturbed final time states that can be generated from the final time adjoints in the reduced basis~$\Phi^k$ for the system determined by the parameter~$\mu$ and starting from zero initial conditions. This motivates, together with the linear equation for the optimal final time adjoint from~\Cref{lem:linear-system}, to consider the projection of~$M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)$ onto~$\redSpaceTransformed^k$ and therefore the distance~$\operatorname{dist}(\redSpaceTransformed^k,\{M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)\})$, see also~\Cref{fig:proof-error-estimator-reduced-space}. Let us denote by~$P_Y(x)\in Y$ the orthogonal projection of a vector~$x\in\X$ onto a subspace~$Y\subset\X$. Then it holds
\[
    \operatorname{dist}\left(\redSpaceTransformed^k,\left\{M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)\right\}\right) = \norm{M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right) - P_{\redSpaceTransformed^k}\Big(M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)\Big)}.
\]
We can express the projection in the basis of~$\redSpaceTransformed^k$ given by the vectors~$x_1^\mu,\dots,x_k^\mu$ (which form a linearly independent set since the corresponding parameters~$\mu_1,\dots,\mu_k\in\paramstrain$ were selected by the greedy procedure). Let us write
\begin{align}\label{equ:projection-onto-parameter-dependent-state-space}
    P_{\redSpaceTransformed^k}\Big(M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)\Big) = \sum\limits_{i=1}^{k} \alpha_i^\mu x_i^\mu \in \redSpaceTransformed^k
\end{align}
for some coefficients~$\alpha_1^\mu,\dots,\alpha_k^\mu\in\setR$. Then we choose the approximate final time adjoint~$\tilde{\varphi}_\mu^k\in\X^k$ as
\begin{align}\label{equ:definition-approximate-adjoint}
    \tilde{\varphi}_\mu^k = \sum\limits_{i=1}^{k} \alpha_i^\mu \varphi_i.
\end{align}
With these definitions, we have that
\[
    (I+M\Gramian)\tilde{\varphi}_\mu^k = \sum\limits_{i=1}^{k} \alpha_i^\mu x_i^\mu = P_{\redSpaceTransformed^k}\Big(M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)\Big),
\]
and in particular
\[
    \eta_\mu(\tilde{\varphi}_\mu^k)=\operatorname{dist}\left(\redSpaceTransformed^k,\left\{M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)\right\}\right).
\]
This allows us to estimate the efficiency of the reduced space~$\X^k$ through the error estimator~$\eta_\mu$, i.e.
\[
    \operatorname{dist}\left(\X^k,\left\{\varphi_\mu^*(T)\right\}\right) \approx \eta_\mu(\tilde{\varphi}_\mu^k).
\]
More precisely, the following theorem holds.
\begin{theorem}[Efficient and reliable error estimator for a reduced space]\label{thm:error-estimator-reduced-space}
    Let~$\mu\in\params$ be a parameter and~$\varphi_\mu^*(T)\in\X$ the optimal final time adjoint solving the optimality system in~\cref{equ:optimality-system-main}. Then, for the error estimator~$\eta_\mu(\tilde{\varphi}_\mu^k)$ with~$\eta_\mu$ introduced in~\cref{equ:definition-error-estimator} and~$\tilde{\varphi}_\mu^k$ from~\cref{equ:definition-approximate-adjoint}, it holds
    \begin{equation}\label{equ:RS-error}
        \operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\}) \quad \leq \quad \eta_\mu(\tilde{\varphi}_\mu^k) \quad \leq \quad \norm{I+M\Gramian}_{\mathcal{L}(\X,\X)}\cdot\operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\}).
    \end{equation}
    
\end{theorem}
\begin{proof}
    Due to orthogonality of the projection and the Pythagorean theorem, it holds
    \[
        \norm{\varphi_\mu^*(T)-\tilde{\varphi}_\mu^k}^2 = \norm{\varphi_\mu^*(T)-P_{\X^k}(\varphi_\mu^*(T))}^2 + \norm{\tilde{\varphi}_\mu^k-P_{\X^k}(\varphi_\mu^*(T))}^2
    \]
    and therefore
    \begin{align*}
        \operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\})^2 &= \norm{\varphi_\mu^*(T)-P_{\X^k}(\varphi_\mu^*(T))}^2 \\
        &= \norm{\varphi_\mu^*(T)-\tilde{\varphi}_\mu^k}^2 - \norm{\tilde{\varphi}_\mu^k-P_{\X^k}(\varphi_\mu^*(T))}^2 \\
        &\leq \eta_\mu(\tilde{\varphi}_\mu^k)^2
    \end{align*}
    by~\Cref{thm:error-estimator-adjoint}. This implies the lower bound in~\cref{equ:RS-error}. 
    \par
    In order to obtain the upper one, we estimate
    \begin{align*}
        \eta_\mu(\tilde{\varphi}_\mu^k) &= \norm{M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right) - (I+M\Gramian)\tilde{\varphi}_\mu^k} \\
        &= \norm{(I+M\Gramian)\varphi_\mu^*(T)-P_{\redSpaceTransformed^k}\Big((I+M\Gramian)\varphi_\mu^*(T)\Big)} \\
        &\leq \norm{(I+M\Gramian)\varphi_\mu^*(T)-(I+M\Gramian)P_{\X^k}(\varphi_\mu^*(T))} \\
        &\leq \norm{I+M\Gramian}_{\mathcal{L}(\X,\X)}\norm{\varphi_\mu^*(T)-P_{\X^k}(\varphi_\mu^*(T))} \\
        &= \norm{I+M\Gramian}_{\mathcal{L}(\X,\X)}\operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\}).
    \end{align*}
    Here we used that~$(I+M\Gramian)P_{\X^k}(\varphi_\mu^*(T))\in\redSpaceTransformed^k$, which follows from the fact that~$P_{\X^k}(\varphi_\mu^*(T))\in\X^k$ and~$\redSpaceTransformed^k = (I+M\Gramian)\X^k$.
\end{proof}

For a visualization of the quantities involved in the proof of~\Cref{thm:error-estimator-reduced-space} and their relationships, see~\Cref{fig:proof-error-estimator-reduced-space}.
% Figure environment removed

\begin{remark}[On the efficiency bound in~\Cref{thm:error-estimator-reduced-space}]
    We emphasize at this point that the first bound in~\Cref{thm:error-estimator-reduced-space} holds for an arbitrary choice~$p\in\X^k$ for the approximate final time adjoint, i.e.~we have~$\operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\})\leq\eta_\mu(p)$ for all~$p\in\X^k$. However, the efficiency of the error estimator~$\eta_\mu(\tilde{\varphi}_\mu^k)$, i.e.~the second inequality in~\Cref{thm:error-estimator-reduced-space}, heavily relies on the choice of the approximate final time adjoint~$\tilde{\varphi}_\mu^k$. The efficiency bound is required to maintain the convergence of the weak greedy algorithm, see~\Cref{subsec:error-analysis-greedy-algorithm}. 
\end{remark}

To formulate the greedy algorithm in pseudo-code, we observe that the following properties hold, which will allow for an error analysis of the greedy algorithm in~\Cref{subsec:error-analysis-greedy-algorithm}: The parameter to solution map, given as
\[
    \params\ni\mu\mapsto\varphi_\mu^*(T)=\left(I+M\Gramian\right)^{-1}M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)\in\X,
\]
is Lipschitz continuous (due to the Lipschitz continuity in~\Cref{as:continuity-parameter-to-system-matrices}, and since the function that maps an operator to its inverse is also Lipschitz continuous), as already mentioned above. The constant~$C_{\varphi^*}>0$ is defined as the Lipschitz constant of this mapping, i.e.~it holds
\[
    \norm{\varphi_\mu^*(T)-\varphi_{\tilde{\mu}}^*(T)} \leq C_{\varphi^*}\norm{\mu-\tilde{\mu}}\qquad\text{for all }\mu,\tilde{\mu}\in\params.
\]
Moreover, the norm~$\norm{I+M\Gramian}$ is uniformly bounded over the parameter domain. The constant~$C_\Lambda>0$ is defined as
\[
    C_\Lambda \coloneqq \sup_{\mu\in\params} \norm{I+M\Gramian}_{\mathcal{L}(\X,\X)} < \infty.
\]
Both of these statements are direct consequences of~\Cref{as:continuity-parameter-to-system-matrices} and the compactness of the parameter set~$\params$. We further remark that it holds~$C_\Lambda\geq 1$, which immediately follows from~\cref{equ:error-estimator-bounds}, and that~$C_{\varphi^*}\geq 0$. In addition to the constants above, we define~$0<\gamma\leq 1$ as
\begin{align}\label{equ:definition-greedy-constant-gamma}
    \gamma \coloneqq \frac{1}{C_{\varphi^*}+C_\Lambda}.
\end{align}
The constant~$\gamma$ will play a crucial role in the error analysis and in the performance of the greedy algorithm.
\par
The procedure of the greedy algorithm is summarized in~\Cref{alg:offline-greedy}.
\begin{algorithm}[H]
    \caption{Offline phase of the greedy procedure}\label{alg:offline-greedy}
    \begin{algorithmic}[1]
        \Require Tolerance~$\varepsilon>0$
        \Ensure Reduced basis~$\Phi^N\subset\X$, reduced space~$\X^N=\Span{\Phi^N}\subset\X$, training parameters and associated reduced coefficients collected in a set~$D_\mathrm{train}\subset\paramstrain\times\setR^N$
        \Procedure{Greedy}{$\varepsilon$}
        \State $\tilde{\varepsilon} \gets C_\Lambda\gamma\varepsilon$,\quad$\delta \gets \frac{\tilde{\varepsilon}}{C_\Lambda}$
        \State define a training set~$\paramstrain\subset\params$ such that for all~$\mu\in\params$ there exists~$\tilde{\mu}\in\paramstrain$ with~$\norm{\mu-\tilde{\mu}}\leq\delta$
        \State $k \gets 0$,\quad$\Phi^0 \gets \emptyset$,\quad$\X^0 \gets \{0\}\subset\X$\label{lst:start-greedy-without-tolerance}
        \State $\tilde{\varphi}_\mu^0 \gets 0$ for all~$\mu\in\paramstrain$
        \State select next parameter~$\mu_{1} \gets \argmax_{\mu\in\paramstrain} \eta_\mu(\tilde{\varphi}_\mu^0)$
        \While{$\eta_{\mu_{k+1}}(\tilde{\varphi}_{\mu_{k+1}}^k) > \tilde{\varepsilon}$}\label{lst:offline-termination-criterion}
            \State compute optimal final time adjoint~$\varphi_{k+1} \gets \varphi_{\mu_{k+1}}^*(T)$ by solving~\cref{equ:linear-system-for-optimal-final-time-adjoint}
            \State $\Phi^{k+1} \gets \Phi^k\cup\{\varphi_{k+1}\}$\qquad (apply orthonormalization using Gram-Schmidt algorithm if desired)
            \State $\X^{k+1} \gets \Span{\Phi^{k+1}}$
            \State $k \gets k+1$
            \For{$\mu\in\paramstrain$}
                \State compute~$x_i^\mu \gets (I+M\Gramian)\varphi_i$ for~$i=1,\dots,k$
                \State assemble operator~$\bar{\X}_\mu \gets [x_1^\mu\ \cdots\ x_k^\mu]\in \mathcal{L}(\setR^k,\X)$ for projection onto~$\redSpaceTransformed^k$
                \State compute coefficients~$\alpha^\mu=(\alpha_1^\mu,\dots,\alpha_k^\mu)^\top\in\setR^k$ as solution of the~$k\times k$~linear system (see~\cref{equ:projection-onto-parameter-dependent-state-space})\label{lst:computation-coefficients-offline-greedy}
                    \[
                        \bar{\X}_\mu^*\bar{\X}_\mu\alpha^\mu=\bar{\X}_\mu^* M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)
                        \vspace{.5em}
                    \]
                \State compute final time adjoint~$\tilde{\varphi}_\mu^k \gets \sum_{i=1}^{k} \alpha_i^\mu \varphi_i$
            \EndFor
            \State select next parameter~$\mu_{k+1} \gets \argmax_{\mu\in\paramstrain} \eta_\mu(\tilde{\varphi}_\mu^k)$
        \EndWhile
        \Return{$\Phi^N \gets \Phi^k$, $\X^N \gets\X^k$, $D_\mathrm{train} \gets \{(\mu,\alpha^\mu):\mu\in\paramstrain\}$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

In the greedy procedure in~\Cref{alg:offline-greedy} we also collect and return all training parameters and the corresponding reduced coefficients in the set~$D_\mathrm{train}$. This will be convenient and useful for the machine learning training in~\Cref{sec:reduced-order-machine-learning}, see in particular~\Cref{alg:offline-machine-learning}.

\begin{remark}[Orthonormalization of the reduced basis]
    To improve numerical stability and to simplify computations of projections, new basis functions can be orthonormalized against the previous ones before adding them to the reduced basis. For that purpose, for instance the Gram-Schmidt algorithm can be used which results in an orthonormal reduced basis~$\Phi^{N}$.
\end{remark}

\begin{remark}[Construction of a reduced basis without using the greedy procedure]
    Instead of applying the greedy algorithm described above, it is also possible to determine a reduced basis using methods such as the POD method~\cite{graessle2021model}. This algorithm requires a set of given training snapshots~$\varphi_{\mu_1}^*(T),\dots,\varphi_{\mu_{n_\mathrm{train}}}^*(T)\in\X$ for several training parameters~$\mu_1,\dots,\mu_{n_\mathrm{train}}\in\params$ that has to be computed beforehand. However, solving the optimal control problem many times is very costly. Since an efficient error estimator is available in our setting, it might therefore be beneficial to select the reduced basis functions in a greedy manner. Nevertheless, if enough training data is already available, the proper orthogonal decomposition might be a plausible alternative to the greedy procedure.
\end{remark}

\subsection{Error Analysis of the Greedy Algorithm}\label{subsec:error-analysis-greedy-algorithm}
In this section, we will investigate the error of the greedy algorithm and prove that~\Cref{alg:offline-greedy} indeed provides a reduced basis~$\Phi^N$ and corresponding reduced space~$\X^N\subset\X$ such that each optimal final time adjoint~$\varphi_\mu^*(T)\in\X$ can be approximated by an error of at most~$\varepsilon$ in the reduced space~$\X^N$ (see~\Cref{thm:weak-greedy-and-approximation-error}). Before doing so, we give a short general introduction to (weak) greedy algorithms and recall a result by DeVore et al.~\cite{devore2013greedy} (see~\Cref{thm:devore-greedy}) stating that weak greedy algorithms produce approximating sequences of reduced spaces with the same convergence rates as the optimal spaces in the sense of Kolmogorov.
\par
Let~$V$ be a Hilbert space and~$K\subset V$ a compact subset of~$V$. Greedy algorithms aim at constructing a sequence of subspaces~$V_N\subset V$ of dimension~$N\in\setN$ that approximate the set~$K$. Similarly to~\Cref{alg:offline-greedy} described in~\Cref{subsec:greedy-for-reduced-basis}, the spaces~$V_N$ are constructed by selecting new basis functions in a ``greedy'' fashion, i.e.~by selecting elements from~$K$ that are currently poorly approximated by elements from~$V_N$. The general procedure is stated in the following pseudocode:
\begin{algorithm}[H]
    \caption{General (weak) greedy algorithm}\label{alg:general-weak-greedy}
    \begin{algorithmic}[1]
        \Require Greedy constant~$\gamma\in(0,1]$, tolerance~$\varepsilon>0$
        \Ensure Reduced basis~$\Psi^N\subset V$, reduced space~$V^N=\Span{\Psi^N}\subset V$
        \State $N\gets 0$,\quad$\Psi^N=\emptyset$,\quad$V^0=\{0\}\subset V$
        \While{$\operatorname{dist}(V^N,K)>\varepsilon$}
            \State choose next element~$v_{N+1}\in K$ such that it holds
            \begin{equation}\label{equ:greedy-choice-condition}
                \hspace{2em}\operatorname{dist}(V^N,\{v_{N+1}\}) \geq \gamma\cdot\operatorname{dist}(V^N,K)
                \vspace{.75em}
            \end{equation}
            \State $\Psi^{N+1} \gets \Psi^N\cup\{v_{N+1}\}$
            \State $V^{N+1} \gets \Span{\Psi^{N+1}}$
            \State $N \gets N+1$
        \EndWhile
        \Return{$\Psi^N$, $V^N$}
    \end{algorithmic}
\end{algorithm}
For a constant~$\gamma<1$ the algorithm above is called \emph{weak} greedy algorithm, while for~$\gamma=1$ the procedure is also known as \emph{strong} greedy algorithm. By considering the optimal reduced subspace of dimension~$N\in\setN$, we can measure the quality of the reduced space obtained by the weak greedy algorithm. We denote by
\begin{equation}
\label{d_n}
    d_N(K) \coloneqq \inf\limits_{\substack{W\subset V,\\\dim W=N}}\operatorname{dist}(W,K)    
\end{equation}
the so-called \emph{Kolmogorov~$N$-width}, where~$W$ denotes an~$N$-dimensional linear subspace of~$V$. In this way, $d_N(K)$ measures the performance of the optimal approximation of~$K$ by a linear space of fixed dimension~$N$.
\par
In contrast, by~$\sigma_N(K)$ we denote the error of the reduced space~$V^N\subset V$ constructed by the greedy algorithm and defined as
\begin{equation}
\label{sigma_n}
    \sigma_N(K) \coloneqq \operatorname{dist}(V^N, K).
\end{equation}
The importance and efficiency of (weak) greedy algorithms is ensured by the following theorem that gives a connection between the decay of the Kolmogorov~$N$-width~$d_N(K)$ with respect to~$N$ and the decay of the error of the reduced space~$\sigma_N(K)$.
\begin{theorem}[DeVore et al.~\cite{devore2013greedy}, Corollary 3.3 (ii) and (iii)]\label{thm:devore-greedy}
    For the weak greedy algorithm with constant~$\gamma$ in a Hilbert space~$V$ and a compact set~$K\subset V$, we have the following:
    \begin{enumerate}
        \item[(i)] If~$d_N(K)\leq C_0N^{-\alpha}$, $N=1,2,\dots$, then~$\sigma_N(K)\leq C_1N^{-\alpha}$, $N=1,2,\dots$, with~$C_1\coloneqq 2^{5\alpha+1}\gamma^{-2}C_0$.
        \item[(ii)] If~$d_N(K)\leq C_0e^{-c_0N^\alpha}$, $N=1,2,\dots$, then~$\sigma_N(K)\leq\sqrt{2C_0}\gamma^{-1}e^{-c_1N^\alpha}$, $N=1,2,\dots$, where~$c_1\coloneqq 2^{-1-2\alpha}c_0$.
    \end{enumerate}
\end{theorem}
This theorem implies that weak greedy algorithms preserve the decay rates of the Kolmogorov~$N$-widths, in such a way providing the optimal approximation errors, up to a multiplicative constant.
\par
In our setting, we consider the Hilbert space~$V=\X$ and aim at approximating the set~$K=\mathcal{M}$ of optimal final time adjoint states over the parameter domain~$\params$ as defined in~\cref{equ:definition-solution-manifold}. We will prove in the following that~\Cref{alg:offline-greedy} is indeed a weak greedy algorithm as introduced in~\Cref{alg:general-weak-greedy}, in particular that the resulting reduced spaces~$\X^N$ satisfy relation~\eqref{equ:greedy-choice-condition}. For this reason, according to~\Cref{thm:devore-greedy}, we can transfer convergence properties of the Kolmogorov~$N$-width of~$\mathcal{M}$ to the error decay of the spaces~$\X^N$ constructed by Algorithm 1.
\begin{theorem}[Weak greedy algorithm and approximation error]\label{thm:weak-greedy-and-approximation-error}
    The procedure presented in~\Cref{alg:offline-greedy} is a weak greedy algorithm with the constant~$\gamma$ defined in~\cref{equ:definition-greedy-constant-gamma}. Furthermore, for the resulting reduced space~$\X^N\subset\X$ it holds for all parameters~$\mu\in\params$ the approximation error estimate
    \begin{align}\label{equ:approximation-error-estimate}
        \operatorname{dist}(\X^N,\{\varphi_\mu^*(T)\}) \quad \leq \quad \varepsilon.
    \end{align}
\end{theorem}
\begin{proof}
    Let~$k\in\{1,\dots,N-1\}$ be an arbitrary iteration of~\Cref{alg:offline-greedy} such that the termination criterion is not fulfilled. Let further be the corresponding reduced basis~$\Phi^k$ and reduced space~$\X^k=\Span{\Phi^k}$ given. We have to prove that it holds
    \begin{align}\label{equ:distance-greedy-inequality}
        \operatorname{dist}(\X^k,\{\varphi_{\mu_{k+1}}^*(T)\}) \geq \gamma\cdot\max\limits_{\mu\in\params}\,\operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\}) = \gamma\cdot\operatorname{dist}(\X^k,\mathcal{M}),
    \end{align}
    with a constant~$\gamma$ independent of~$k$ (see also~\cref{equ:greedy-choice-condition}). To this end, let~$\mu\in\params$ be an arbitrary parameter. Then there exists a parameter~$\tilde{\mu}\in\paramstrain$, such that~$\norm{\mu-\tilde{\mu}}\leq\delta$. Due to the choice of the parameter~$\mu_{k+1}\in\paramstrain$, it holds
    \[
        \eta_{\mu_{k+1}}(\tilde{\varphi}_{\mu_{k+1}}^k) \geq \eta_{\tilde{\mu}}(\tilde{\varphi}_{\tilde{\mu}}^k)
    \]
    for all~$\tilde{\mu}\in\paramstrain$. We then have the estimate
    \begin{align*}
        \operatorname{dist}(\X^k,\{\varphi_\mu^*(T)\}) &\leq \norm{\varphi_\mu^*(T)-\varphi_{\tilde{\mu}}^*(T)} + \operatorname{dist}(\X^k,\{\varphi_{\tilde{\mu}}^*(T)\}) \\
        &\leq C_{\varphi^*}\delta + \operatorname{dist}(\X^k,\{\varphi_{\tilde{\mu}}^*(T)\}) \\
        &\leq C_{\varphi^*}\frac{\tilde{\varepsilon}}{C_\Lambda} + \eta_{\tilde{\mu}}(\tilde{\varphi}_{\tilde{\mu}}^k) \\
        &\leq C_{\varphi^*}\frac{\tilde{\varepsilon}}{C_\Lambda} + \eta_{\mu_{k+1}}(\tilde{\varphi}_{\mu_{k+1}}^k) \\
        &\leq \left(\frac{C_{\varphi^*}}{C_\Lambda}+1\right)\eta_{\mu_{k+1}}(\tilde{\varphi}_{\mu_{k+1}}^k) \\
        &\leq \left(\frac{C_{\varphi^*}}{C_\Lambda}+1\right)C_\Lambda\cdot\operatorname{dist}(\X^k,\{\varphi_{\mu_{k+1}}^*(T)\}) \\
        &= (C_{\varphi^*}+C_\Lambda)\cdot\operatorname{dist}(\X^k,\{\varphi_{\mu_{k+1}}^*(T)\}) \\
        &= \frac{1}{\gamma}\cdot\operatorname{dist}(\X^k,\{\varphi_{\mu_{k+1}}^*(T)\}).
    \end{align*}
    Here, we used that~$\tilde{\varepsilon} < \eta_{\mu_{k+1}}(\tilde{\varphi}_{\mu_{k+1}}^k)$ since the termination criterion is not fulfilled. This proves the inequality in~\eqref{equ:distance-greedy-inequality} by multiplying both sides by~$\gamma>0$, since the parameter~$\mu\in\params$ was arbitrary. Consequently, \Cref{alg:offline-greedy} is a weak greedy algorithm.
    \par
    Regarding the approximation error estimate, we obtain similarly as above that for all~$\mu\in\params$ it holds
    \begin{align*}
        \operatorname{dist}(\X^N,\{\varphi_\mu^*(T)\}) &\leq \frac{C_{\varphi^*}}{C_\Lambda}\tilde{\varepsilon} + \eta_{\mu_{N+1}}(\tilde{\varphi}_{\mu_{N+1}}^N) \\
        &\leq \left(\frac{C_{\varphi^*}}{C_\Lambda}+1\right)\tilde{\varepsilon} \\
        &= \frac{\tilde{\varepsilon}}{C_\Lambda\gamma} \\
        &= \varepsilon,
    \end{align*}
    where we used that~$\eta_{\mu_{N+1}}(\tilde{\varphi}_{\mu_{N+1}}^k) \leq \tilde{\varepsilon}$, i.e.~that the termination criterion is fulfilled by the last computed final time adjoint. This proves the estimate in~\eqref{equ:approximation-error-estimate}.
\end{proof}

Based on the last two results, the introduced~\Cref{alg:offline-greedy} preserves the optimal decay rates, expressed through the Kolmogorov~$N$-widths, of the optimal final time adjoints manifold~$\mathcal{M}$. More precisely, the sequences~$(d_n(\mathcal{M}))_{n\in\setN}$ and~$(\sigma_n(\mathcal{M}))_{n\in\setN}$, defined by~\eqref{d_n} and~\eqref{sigma_n}, respectively, exhibit the same asymptotic behaviour, up to a multiplicative constant. As the solution manifold~$\mathcal{M}$ is usually beyond our disposal, it is hard to estimate its Kolmogorov~$N$-widths directly. However, these can be related to the Kolmogorov widths of the set of admissible parameters~$\params$, which can often be easily determined, even in the case of infinite dimensional parameters (cf.~\cite[\S 4.3]{cohen2015approximation}). More precisely the following result holds~\cite{cohen2016kolmogorov}, under an additional holomorphic assumption.
\begin{theorem}[Cohen and DeVore~\cite{cohen2016kolmogorov}, Theorem 1.1]\label{thm:greedy-hol}
    For a pair of complex Banach spaces~$X$ and~$V$ assume that~$u\colon O\to V$ is a holomorphic map from an open set~$O\subset X$ into~$V$ with uniform bound. If~$K\subset O$ is a compact subset of~$X$ then for any~$\alpha>1$ and~$C_0>0$ it holds
    \[
        d_n(K)\leq C_0 n^{-\alpha} \quad \Longrightarrow \quad d_n(u(K)) \leq C_1 n^{-\beta} \qquad\text{for all }n\in\setN,
    \]
    for any~$\beta<\alpha-1$ and a constant~$C_1$ depending on~$C_0$, $\alpha$ and the mapping~$u$.
\end{theorem}
The proof of the last theorem also provides an explicit estimate of the constant~$C_1$ in dependence on~$C_0$, $\alpha$ and the mapping~$u$. Combining this result with~\Cref{thm:devore-greedy,thm:weak-greedy-and-approximation-error}, one is able to a priori estimate the number of snapshots, i.e. the dimension of the reduced basis space constructed by the offline part of the greedy algorithm, required to approximate the solution manifold within a given error. However, the price of  such an estimate  is the holomorphic dependence of the solutions on the parameters, which is unlikely to occur in real-world problems. Moreover, such estimates tend to be conservative and are not often used in practice. For this reason we do not explore~\Cref{thm:greedy-hol} and do not require additional smoothness of our parameter mappings, other than the one postulated by~\Cref{as:continuity-parameter-to-system-matrices}. We rather run the offline part of the greedy algorithm and observe the dimension of the reduced basis a posteriori.
\par
Summarizing, we have seen in this section that~\Cref{alg:offline-greedy} is a weak greedy algorithm which enables the construction of reduced spaces with theoretically guaranteed convergence properties and error bounds.

\subsection{Efficient and Reduced Online Computations}\label{subsec:online-computations}
After constructing a reduced basis~$\Phi^N\subset\X$ (where the dimension~$N=|\Phi^N|$ of the reduced space depends on the prescribed error tolerance~$\varepsilon>0$) during the offline phase, the reduced order model can be used during the online phase to replace the costly solution of the original optimal control problem. For a new given parameter~$\mu\in\params$, the approximate final time adjoint~$\tilde{\varphi}_\mu^N\in\X^N=\Span{\Phi^N}$ is computed similarly as described in~\Cref{subsec:greedy-for-reduced-basis}, see in particular~\cref{equ:definition-approximate-adjoint}. For completeness, we state the online procedure also in form of a pseudocode, see~\Cref{alg:online-greedy}.
\begin{algorithm}[H]
    \caption{Online evaluation of the approximate control based on the greedy procedure}\label{alg:online-greedy}
    \begin{algorithmic}[1]
        \Require Parameter~$\mu\in\params$, reduced basis~$\Phi^N=\{\varphi_1,\dots,\varphi_N\}$ constructed by~\Cref{alg:offline-greedy}
        \Ensure Approximate final time adjoint~$\tilde{\varphi}_\mu^N\in\X^N$, approximate optimal control~$\tilde{u}_\mu^N\in G$
        \State compute~$x_i^\mu \gets (I+M\Gramian)\varphi_i$ for~$i=1,\dots,N$ \label{lst:online-compute-final-time-states}
        \State assemble operator~$\bar{\X}_\mu \gets [x_1^\mu\ \cdots\ x_N^\mu]\in\mathcal{L}(\setR^N,\X)$ for projection onto~$\redSpaceTransformed^N$
        \State compute coefficients~$\alpha^\mu=(\alpha_1^\mu,\dots,\alpha_N^\mu)^\top\in\setR^N$ as solution of the~$N \times N$~linear system (see~\cref{equ:projection-onto-parameter-dependent-state-space}) \label{lst:online-solve-system-coefficients}
        \begin{equation}\label{equ:linear-system-for-reduced-coefficients}
            \bar{\X}_\mu^*\bar{\X}_\mu\alpha^\mu=\bar{\X}_\mu^* M\left(e^{A_\mu T}x_\mu^0 - x_\mu^T\right)
            \vspace{.5em}
        \end{equation}
        \State compute final time adjoint~$\tilde{\varphi}_\mu^N \gets \sum_{i=1}^{N} \alpha_i^\mu \varphi_i$\qquad (see~\cref{equ:definition-approximate-adjoint})
        \State solve~$-\dot{\tilde{\varphi}}_\mu(t)=A_\mu^*\tilde{\varphi}_\mu(t)$ for~$t\in[0,T]$,\quad $\tilde{\varphi}_\mu(T)=\tilde{\varphi}_\mu^N$ backwards in time\qquad (see~\cref{fromdualtoprimal}) \label{lst:online-time-dependent-adjoint}
        \State compute associated control~$\tilde{u}_\mu^N(t) \gets -R^{-1}B_\mu^*\tilde{\varphi}_\mu(t)$\qquad (see~\cref{control})\label{lst:online-greedy-control}
        \Return{$\tilde{\varphi}_\mu^N$, $\tilde{u}_\mu^N$}
    \end{algorithmic}
\end{algorithm}

By means of the error estimator~$\eta_\mu$ defined in~\cref{equ:definition-error-estimator}, it is possible to obtain an estimation of the error during the online phase as well. By~\Cref{thm:error-estimator-adjoint}, it holds
\[
    \norm{\varphi_\mu^*(T) - \tilde{\varphi}_\mu^N} \leq \eta_\mu(\tilde{\varphi}_\mu^N).
\]
Additionally, if the reduced basis~$\Phi^N$ is constructed using the greedy algorithm introduced in~\Cref{subsec:greedy-for-reduced-basis} for a tolerance~$\varepsilon>0$, we obtain by combining~\Cref{thm:error-estimator-reduced-space} and~\Cref{thm:weak-greedy-and-approximation-error} the estimate
\[
    \eta_\mu(\tilde{\varphi}_\mu^N) \leq C_\Lambda\varepsilon\qquad\text{for all }\mu\in\params,
\]
and hence it holds
\begin{align}\label{equ:error-bound-online-phase}
    \norm{\varphi_\mu^*(T) - \tilde{\varphi}_\mu^N} \leq C_\Lambda\varepsilon
\end{align}
for all parameters~$\mu\in\params$. We can therefore also bound the error in the online phase given that the reduced space was created with a certain error tolerance.
\par
Since we are typically interested in an approximation of the control~$u_\mu^*$, we also give an error bound for the control. To this end, let us define for~$t\in[0,T]$ the constant
\[
    \zeta_{\mu,t} \coloneqq \norm{R^{-1}B_\mu^*e^{A_\mu^*(T-t)}}_{\mathcal{L}(\X,\U)} < \infty.
\]
Then it holds for all~$t\in[0,T]$ that
\[
    \norm{u_\mu^*(t)-\tilde{u}_\mu^N(t)}_\U \leq \zeta_{\mu,t}\norm{\varphi_\mu^*(T)-\tilde{\varphi}_\mu^N}_\X.
\]
Similar to the bound on the error in the final time adjoint, we can deduce that if the reduced basis~$\Phi^N$ is constructed with a greedy tolerance of~$\varepsilon>0$, the error in the control can be bounded by
\[
    \norm{u_\mu^*(t)-\tilde{u}_\mu^N(t)}_\U \leq \zeta_{\mu,t}C_\Lambda\varepsilon.
\]
The constant~$\zeta_{\mu,t}$ depends in particular on the parameter~$\mu$ and properties of the system matrices, such as stability of the system. Certainly, due to~\Cref{as:continuity-parameter-to-system-matrices} and the compactness of~$\params$, there is also a finite uniform upper bound of the constants~$\zeta_{\mu,t}$ over the parameter set~$\params$.
\par
In our numerical experiments below, we will refer to the reduced order model from~\Cref{alg:online-greedy} as~\RBROM{} (greedy reduced order model).