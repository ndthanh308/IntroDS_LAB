\section{Acceleration of the Online Phase using Machine Learning}\label{sec:reduced-order-machine-learning}
The reduced order model introduced in~\Cref{sec:reduced-order-modeling-greedy} already gives a speedup in solving the optimal control problem compared to solving the exact problem from~\cref{equ:optimality-system-main}, see also the numerical experiments in~\Cref{sec:numerical-experiments}. However, the computational costs during the online phase still involve high-dimensional computations. If for instance the state space is finite-dimensional, $\X=\setR^n$, the costs scale with the dimension~$n$. To be more specific, for a new parameter~$\mu\in\params$, one has to compute~$x_i^\mu=(I+M\Gramian)\varphi_i$ for~$i=1,\dots,N$ (with~$N$ denoting the dimension of the reduced space) by solving the system from~\cref{equ:linear-system-for-optimal-final-time-adjoint}, see also~\Cref{rem:applying-gramian-to-vectors}. In order to circumvent these steps, we propose to apply machine learning algorithms to directly approximate the map from the parameters to the reduced coefficients. Training data for the machine learning process can be generated using the~\RBROM{} introduced in~\Cref{sec:reduced-order-modeling-greedy}. The idea is motivated by a similar approach that has been introduced in the context of parametrized PDEs in~\cite{hesthaven2018nonintrusive} and was applied to instationary problems in~\cite{wang2019nonintrusive}. In the following section, we first describe the main ideas of our approach in detail. Afterwards, we discuss how the error estimator introduced in~\Cref{subsec:greedy-for-reduced-basis} can be applied to also evaluate the error of the machine learning approximation. We finally introduce specific machine learning algorithms used in our numerical experiments below in~\Cref{sec:numerical-experiments}.

\subsection{Learning the Map from Parameters to Coefficients}
During the online phase, each evaluation of the reduced model as shown in~\Cref{subsec:online-computations} requires the computation of~$N$ final time states~$x_i^\mu\in \X$ in Line~\ref{lst:online-compute-final-time-states} of~\Cref{alg:online-greedy}. Each of these computations amounts in solving a decoupled system of ordinary differential equations, one forward and one backward, as in~\cref{fromdualtoprimal} and \cref{state}. However, this is only required to setup a (small) linear system of equations to compute the reduced coefficients~$\alpha^\mu\in\setR^N$ in Line~\ref{lst:online-solve-system-coefficients} of~\Cref{alg:online-greedy}. These coefficients determine the projection of the ``target''~$M(e^{A_\mu T}x_\mu^0-x_\mu^T)$ to the space of final states, perturbed by the operator~$-\left((\Gramian)^{-1}+M\right)$, reachable from the reduced space~$\X^N$. Afterwards, in order to compute the approximate control, one only has to solve a backward evolutionary equation (Line~\ref{lst:online-time-dependent-adjoint} in~\Cref{alg:online-greedy}) and apply the operator~$-R^{-1}B^*_{\mu}$. The main computational effort in~\Cref{alg:online-greedy} is spent in Line~\ref{lst:online-compute-final-time-states}. The idea to accelerate the online computations is to replace steps~\ref{lst:online-compute-final-time-states}--\ref{lst:online-solve-system-coefficients} by applying a cheaply to evaluate machine learning surrogate that provides, given a parameter~$\mu\in\params$, an approximation~$\hat{\alpha}^\mu\in\setR^N$ of the reduced coefficients~$\alpha^\mu\in\setR^N$ for the parameter~$\mu$. Similarly to the remaining steps of~\Cref{alg:online-greedy}, this results in an approximation~$\hat{\varphi}_\mu^N=\sum_{i=1}^{N}\hat{\alpha}_i^\mu\varphi_i\in\X^N$ of the final time adjoint and associated control~$\hat{u}_\mu^N\in G$. In other words, we define the mapping~$\pi_N\colon\params\to\setR^N$ for a parameter~$\mu\in\params$ as~$\pi_N(\mu)\coloneqq\alpha^\mu$, where~$\alpha^\mu\in\setR^N$ solves the linear system from~\cref{equ:linear-system-for-reduced-coefficients}. Furthermore, we construct an approximation~$\hat{\pi}_N\colon\params\to\setR^N$ of~$\pi_N$ by training a machine learning algorithm and define the machine learning final time adjoint~$\hat{\varphi}_\mu^N\approx\tilde{\varphi}_\mu^N$ as
\begin{align}\label{equ:definition-approximate-adjoint-ml}
    \hat{\varphi}_\mu^N \coloneqq \sum\limits_{i=1}^{N}\big[\hat{\pi}_N(\mu)\big]_i\varphi_i = \sum\limits_{i=1}^{N}\hat{\alpha}_i^\mu\varphi_i.
\end{align}
In our algorithm we first run the greedy algorithm to construct a reduced basis~$\Phi^N$. Additionally, the greedy algorithm already returns the pairs~$(\mu,\pi_N(\mu))\in\paramstrain\times\setR^N$ for all~$\mu\in\paramstrain$. Any supervised machine learning algorithm that builds an approximation by using training data, consisting of inputs and corresponding outputs of the function to approximate, can be used in our setting and trained on the data set~$D_\mathrm{train}=\{(\mu,\pi_N(\mu)):\mu\in\paramstrain\}$. In particular, the approach is not restricted to the machine learning methods applied in our numerical experiments. Furthermore, the error estimates presented in the next section are also independent of the exact implementation of the machine learning surrogate.
\begin{remark}[Smoothness of the parameter to coefficient map]
    The ability of machine learning algorithms to provide a satisfactory approximation of a function is typically linked to the smoothness of the function. Usually, one can observe that the smoother the function to approximate, the better the machine learning approximation. In our setting, the smoothness of the map~$\pi_N$ depends on the smoothness of the maps~$\mu\mapsto A_\mu$, $\mu\mapsto B_\mu$, $\mu\mapsto x_\mu^0$, and~$\mu\mapsto x_\mu^T$, i.e.~all parameter-dependent parts of the optimal control problem. Due to~\Cref{as:continuity-parameter-to-system-matrices}, we can also expect a smooth parameter to coefficients map~$\pi_N$ that is amenable to approximation by machine learning surrogates.
\end{remark}
For completeness and later reference, we summarize the offline and the online procedure for the machine learning reduced model in the following two pseudocodes:
\begin{algorithm}[H]
    \caption{Offline phase of the machine learning greedy procedure}\label{alg:offline-machine-learning}
    \begin{algorithmic}[1]
        \Require Greedy tolerance~$\varepsilon>0$
        \Ensure Reduced basis~$\Phi^N\subset \X$, approximation~$\hat{\pi}_N$ of the parameter to coefficients map
        \State $\Phi^N,\X^N,D_\mathrm{train} \gets \Call{Greedy}{\varepsilon}$\qquad (see~\Cref{alg:offline-greedy})
        \State train a machine learning algorithm using the data~$D_\mathrm{train}$ to obtain a surrogate~$\hat{\pi}_N$
        \Return{$\Phi^N$, $\hat{\pi}_N$}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
    \caption{Online evaluation of the approximate control based on the machine learning greedy procedure}\label{alg:online-machine-learning}
    \begin{algorithmic}[1]
        \Require Parameter~$\mu\in\params$, reduced basis~$\Phi^N$ of size~$N=|\Phi^N|$, machine learning approximation~$\hat{\pi}_N$ of the parameter to coefficients map
        \Ensure Approximate final time adjoint~$\hat{\varphi}_\mu^N\in \X^N$, approximate optimal control~$\hat{u}_\mu^N\in G$
        \State evaluate surrogate~$\hat{\pi}_N$ to obtain approximate coefficients~$\hat{\alpha}^\mu \gets \hat{\pi}_N(\mu)$
        \State compute final time adjoint~$\hat{\varphi}_\mu^N \gets \sum_{i=1}^{N} \hat{\alpha}_i^\mu \varphi_i$\qquad (see~\cref{equ:definition-approximate-adjoint-ml})
        \State solve~$-\dot{\hat{\varphi}}_\mu(t)=A_\mu^*\hat{\varphi}_\mu(t)$ for~$t\in[0,T]$,\quad $\hat{\varphi}_\mu(T)=\hat{\varphi}_\mu^N$ backwards in time\qquad (see~\cref{fromdualtoprimal})\label{lst:online-ml-adjoint-equation}
        \State compute associated control~$\hat{u}_\mu^N(t) \gets -R^{-1}B_\mu^*\hat{\varphi}_\mu(t)$\qquad (see~\cref{control})\label{lst:online-ml-control}
        \Return{$\hat{\varphi}_\mu^N$, $\hat{u}_\mu^N$}
    \end{algorithmic}
\end{algorithm}

\begin{remark}[Choice of training parameters for machine learning]
    It is also possible to add more training samples, i.e.~pairs of the form~$(\mu,\pi_N(\mu))$ for~$\mu\in\params$, to the set of training data for the machine learning algorithm. This additional training data can be generated cheaply by running the online algorithm of the~\RBROM{}, see~\Cref{alg:online-greedy}. In particular, such an enrichment of the training set can be performed adaptively as well, depending on the accuracy of the machine learning prediction. To this end, the a posteriori error estimator introduced in~\Cref{subsec:residual-based-error-estimator} and described in~\Cref{subsec:error-estimation-ml} for the machine learning surrogate might be used to evaluate the machine learning performance, see also~\Cref{rem:adaptive-model-hierarchy}.
\end{remark}
Instead of learning the optimal adjoint datum (or control) directly as functions of parameter (and time), our approach requires learning a mapping between the (typically low-dimensional) spaces~$\params$ and~$\setR^N$ (where usually also $N$ is small). This will allow, as we shall see below, for a significant reduction of the computational costs, while at the same time providing reliable and accurate approximations.
\par
The training data for the machine learning surrogate is generated for free in the offline phase (see~Line~\ref{lst:computation-coefficients-offline-greedy} of~\Cref{alg:offline-greedy}). Solving the exact optimal control problem to generate training data, for instance for learning the controls directly, would be prohibitively expensive both in computational time and in memory. Furthermore, as we will see below, using the greedy algorithm and the reduced basis as an intermediate step results in a priori and a posteriori error estimates for the machine learning results. These might not be available when directly learning the controls or adjoints.

\subsection{A Priori and a Posteriori Error Estimation of Machine Learning Results}\label{subsec:error-estimation-ml}
We can derive a simple a priori bound for the error in the approximation of the optimal final time adjoint that consists of the online greedy error in~\cref{equ:error-bound-online-phase} and the machine learning error in approximating the parameter to coefficients mapping:
\begin{lemma}[Error bound for the machine learning approximation]\label{lem:error-bound-machine-learning}
    Let~$\Phi^N=\{\varphi_1,\dots,\varphi_N\}\subset \X$ be a reduced basis constructed using the greedy algorithm from~\Cref{subsec:greedy-for-reduced-basis} for an error tolerance~$\varepsilon>0$. Further, let~$\bar{\Phi}^N\in\mathcal{L}(\setR^{N},\X)$ be the operator given for the~$i$-th unit vector~$e_i\in\setR^N$ as~$\bar{\Phi}^N e_i=\varphi_i$, $i=1,\dots,N$. Let~$\hat{\pi}_N\colon\params\to\setR^N$ be an approximation of the exact parameter to coefficients map~$\pi_N\colon\params\to\setR^N$. Then it holds
    \[
        \norm{\varphi_\mu^*(T)-\hat{\varphi}_\mu^N} \quad \leq \quad C_\Lambda\varepsilon + \norm{\bar{\Phi}^N}_{\mathcal{L}(\setR^{N},\X)}\norm{\pi_N(\mu)-\hat{\pi}_N(\mu)}
    \]
    for all parameters~$\mu\in\params$.
\end{lemma}
\begin{proof}
    Using the bound in~\cref{equ:error-bound-online-phase} and the definitions of the operator~$\bar{\Phi}^N$ as well as the approximate final time adjoints~$\tilde{\varphi}_\mu^N$ and~$\hat{\varphi}_\mu^N$, it holds for all~$\mu\in\params$ that
    \begin{align*}
        \norm{\varphi_\mu^*(T)-\hat{\varphi}_\mu^N} &\leq \norm{\varphi_\mu^*(T)-\tilde{\varphi}_\mu^N} + \norm{\tilde{\varphi}_\mu^N - \hat{\varphi}_\mu^N} \\
        &\leq C_\Lambda\varepsilon + \norm{\bar{\Phi}^N\alpha^\mu - \bar{\Phi}^N\hat{\alpha}^\mu} \\
        &= C_\Lambda\varepsilon + \norm{\bar{\Phi}^N\big(\pi_N(\mu)-\hat{\pi}_N(\mu)\big)} \\
        &\leq C_\Lambda\varepsilon + \norm{\bar{\Phi}^N}_{\mathcal{L}(\setR^{N},\X)}\norm{\pi_N(\mu)-\hat{\pi}_N(\mu)}.
    \end{align*}
\end{proof}
The first term in the error bound of~\Cref{lem:error-bound-machine-learning} corresponds to the greedy approximation error in the offline phase and can be adjusted by the choice of the greedy tolerance~$\varepsilon$. The second term measures the error of the machine learning in approximating the map~$\pi_N$. If one uses orthonormalization during the greedy procedure, it even holds~$\norm{\bar{\Phi}^N}_{\mathcal{L}(\setR^{N},\X)}=1$, which further simplifies the error estimate.
\par
The error of the approximate final time adjoint~$\hat{\varphi}_\mu^N\in \X^N$ can also be estimated in an a posteriori manner by means of the error estimator~$\eta_\mu$ defined in~\cref{equ:definition-error-estimator}. This results, due to~\Cref{thm:error-estimator-adjoint}, in an efficient and reliable error estimator even for the machine learning results, i.e.~it holds
\[
    \norm{\varphi_\mu^*(T)-\hat{\varphi}_\mu^N} \quad \leq \quad \eta_\mu(\hat{\varphi}_\mu^N) \quad \leq \quad \norm{I+M\Gramian}_{\mathcal{L}(\X,\X)}\norm{\varphi_\mu^*(T)-\hat{\varphi}_\mu^N}
\]
for all~$\mu\in\params$. The error certification and the guarantees obtained by the greedy procedure, see also~\Cref{thm:weak-greedy-and-approximation-error} and~\cref{equ:error-bound-online-phase}, are the main advantages of the presented approach compared to learning the optimal control directly as a function of the parameter.

\begin{remark}[Adaptive and certified surrogate model hierarchy from~\cite{haasdonk2023certified}]\label{rem:adaptive-model-hierarchy}
    Recently, a model hierarchy consisting of a full-order model (similar to the exact solution of the parametrized optimal control problem in our setting), a reduced order model (similar to the~\RBROM{} created by the greedy algorithm in~\Cref{sec:reduced-order-modeling-greedy}), and a machine learning surrogate (similar to the one proposed in this section) was introduced in~\cite{haasdonk2023certified} and further tested with deep kernel models as machine learning surrogates in~\cite{wenzel2023application}. Due to the error estimation described above for the~\RBROM{} as well as the machine learning surrogate, the adaptive model hierarchy is also applicable in the setting of parametrized optimal control problems as considered in this contribution. The reduced model in~\cite{haasdonk2023certified} is also constructed by an adaptive procedure depending on the error estimate but without selecting the training parameters a priorily. Instead, the adaptive model is queried for different parameters and the reduced model (and hence also the machine learning surrogate) is automatically built and updated if necessary, i.e.~if the desired error tolerance is not fulfilled. The adaptive model construction from~\cite{haasdonk2023certified} can therefore be seen as a greedy procedure incorporated into the online phase, and thus shows similarities to the greedy construction of the reduced models considered in this paper.
\end{remark}

\subsection{Machine Learning Approaches}
In this subsection we shortly introduce the machine learning approaches applied in the numerical experiments in~\Cref{sec:numerical-experiments}. As already highlighted above, the whole algorithm is not restricted to these specific choices of methods. We start by defining (deep) neural networks in~\Cref{subsec:deep-neural-networks} and afterwards describe kernel methods in~\Cref{subsec:kernel-methods} and Gaussian process regression in~\Cref{subsec:gaussian-process-regression}. More technical details on the implementation and exact choices of certain parts of the machine learning methods can be found in~\Cref{subsec:implementational-details}. For practical implementations, we assume that the parameter set~$\params$ is finite-dimensional, i.e.~$\params\subset\setR^p$ for some~$p\in\setN$.

\subsubsection{Deep Neural Networks}\label{subsec:deep-neural-networks}
Nowadays, deep neural networks are one of the most popular and widespread machine learning algorithms~\cite{lecun2015deep}. They have also been applied to model order reduction~\cite{hesthaven2018nonintrusive,wang2019nonintrusive,haasdonk2023certified,wenzel2023application} and optimal control~\cite{kmet2011neural}. In this work, we restrict our attention to a standard class of neural networks, so-called \emph{feedforward neural networks}. This class of neural networks applies an alternating sequence of affine transformations and element-wise nonlinear activation functions to the input~\cite{petersen2018optimal}. The function defined by the neural network can be tailored to the training data by adjusting the entries of the matrices and vectors in the affine transformations, the so-called \emph{weights} and \emph{biases} of the neural network.
\par
To be more precise, we describe the notion of a feedforward neural network following a formal definition from~\cite{petersen2018optimal}, see also~\cite[Section~4]{keil2022adaptive}: Let~$L\in\setN$ denote the number of layers of the neural network and~$p=N_0,N_1,\dots,N_{L-1},N_L=N\in\setN$ the numbers of neurons in each layer. Here, we emphasize that the number of neurons in the input layer~$N_0=p$ and the number of neurons in the output layer~$N_L=N$ are chosen such that the corresponding neural network defines a mapping~$\setR^p\to\setR^N$ which is required in our application of approximating the parameter to coefficients map. Furthermore, we consider for~$i=1,\dots,L$ the weight matrices~$W_i\in\setR^{N_i\times N_{i-1}}$ and bias vectors~$b_i\in\setR^{N_i}$ which are collected in the tuple~$\theta=\big((W_1,b_1),\dots,(W_L,b_L)\big)$. To introduce nonlinearity, an activation function~$\rho\colon\setR\to\setR$ is applied component-wise between the affine layers. Let~$\rho_n\colon\setR^n\to\setR^n$ denote the component-wise application of~$\rho$ to~$n$-dimensional vectors. With these ingredients at hand, we define the neural network~$\Phi_\theta\colon\setR^p\to\setR^N$ associated to the weights and biases~$\theta$ for an input~$x\in\setR^p$ as
\[
    \Phi_\theta(x) \coloneqq r_L(x),
\]
where~$r_L\colon\setR^p\to\setR^N$ is defined recursively by
\begin{align*}
    r_L(x) &\coloneqq W_Lr_{L-1}(x)+b_L, \\
    r_i(x) &\coloneqq \rho_{N_i}(W_ir_{i-1}(x)+b_i)\qquad\text{for }i=1,\dots,L-1, \\
    r_0(x) &\coloneqq x.
\end{align*}
\par
For a set of training data~$(x_i,y_i)\in\setR^p\times\setR^N$, $i=1,\dots,n_\mathrm{train}$, the weights and biases~$\theta$ of the neural network are optimized such that the loss function
\[
    \mathcal{L}_\mathrm{dnn}(\theta) \coloneqq \frac{1}{n_\mathrm{train}}\sum\limits_{i=1}^{n_\mathrm{train}}\norm{\Phi_\theta(x_i)-y_i}_2^2
\]
is minimized, i.e.~the mean squared error of the neural network in predicting the outputs~$y_i$ given the inputs~$x_i$ for~$i=1,\dots,n_\mathrm{train}$. Hence, the approximate mapping is chosen as~$\hat{\pi}_N\coloneqq\Phi_{\theta^*}$ with~$\theta^*\in\argmin_{\theta} \mathcal{L}_\mathrm{dnn}(\theta)$. The loss function~$\mathcal{L}_\mathrm{dnn}$ is typically optimized using gradient based methods. In our numerical experiments, we apply the quasi-Newton method L-BFGS, see~\cite{liu1989limited}. The gradient of~$\mathcal{L}_\mathrm{dnn}$ with respect to~$\theta$ can be efficiently computed via backpropagation~\cite{rumelhart1986learning}. To avoid overfitting of the training data, one usually also evaluates the loss function for a validation set, that is chosen distinct from the training set, in each iteration of the optimization algorithm. If the loss on the validation set starts to increase over a couple of consecutive iterations, the optimization is cancelled. This procedure is known as \emph{early stopping}, see~\cite{prechelt1997early} for a discussion of different early stopping approaches and~\cite{molinari2021iterative} for theoretical guarantees in a simplified setting.
\par
The outputs in our application are the coefficients with respect to the reduced basis of final time adjoints. Due to the construction of the reduced basis by the greedy algorithm from~\Cref{subsec:greedy-for-reduced-basis}, the basis functions are sorted by importance (similar to the sorting of singular vectors in a singular value decomposition). Therefore, coefficients associated to different basis functions vary quite heavily in their magnitude. To ensure a sufficiently accurate approximation of all coefficients, we thus apply a simple scaling to the coefficients before training the neural network. The scaling is an affine transformation taking into account the minimum and maximum of the respective coefficient over the training set such that each coefficient is mapped to the interval~$[0,1]$ (on the training set).
\par
A feedforward neural network with~$L\geq 3$ layers is typically called \emph{deep neural network} (DNN). The reduced order model that uses deep neural networks for the coefficient prediction will be called~\DNNROM{} in the remainder of this paper.

\subsubsection{Kernel Methods}\label{subsec:kernel-methods}
Approximations using kernel functions have been applied in the context of surrogate modeling quite successfully in the last years, see for instance~\cite{santin2021kernel,haasdonk2023certified,wenzel2023application}. Very recently, kernel methods were also used for solving optimal control problems, see~\cite{ehring2023hermite}. We refer for instance to~\cite{wendland2005scattered} for a general introduction to kernel methods.
\par
Essentially, these methods build around the notion of (scalar) positive-definite kernels which are mappings~$k\colon\setR^p\times\setR^p\to\setR$ such that the kernel matrix~$[k(x_i,x_j)]_{i,j=1}^n$ is symmetric and positive-definite for all~$n\in\setN$ and distinct~$x_i\in\setR^p$, $i=1,\dots,n$. Associated to every positive-definite kernel~$k$ is a reproducing kernel Hilbert space~$H_{k}$ defined as follows, see the Moore-Aronszajn theorem~\cite{aronszajn1950theory}: Let~$H_{k}^0\coloneqq\Span{\left\{k(\cdot,x):x\in\setR^p\right\}}$, then we can define an inner product~$\langle\cdot,\cdot\rangle_{H_{k}}$ on~$H_{k}^0$ by
\[
    \left\langle\sum_{i=1}^{n}\alpha_i k(\cdot,x_i),\sum_{j=1}^{m}\beta_j k(\cdot,z_j)\right\rangle_{H_{k}} \coloneqq \sum_{i=1}^{n}\sum_{j=1}^{m}\alpha_i\beta_j k(x_i,z_j)
\]
for~$n,m\in\setN$, $\alpha_i,\beta_j\in\setR$ and~$x_i,z_j\in\setR^p$ for~$i=1,\dots,n$ and~$j=1,\dots,m$. The space~$H_{k}$ is now given as the completion of~$H_{k}^0$ with respect to the inner product~$\langle\cdot,\cdot\rangle_{H_{k}}$, i.e.~it holds~$H_{k}=\overline{H_{k}^0}^{\langle\cdot,\cdot\rangle_{H_{k}}}$. Furthermore, the space~$H_{k}$ has the function~$k$ as reproducing kernel, i.e. it holds~$\Phi(x)=\langle\Phi,k(\cdot,x)\rangle_{H_{k}}$ for all~$\Phi\in H_{k}$ and~$x\in\setR^p$. Given a set of data points~$(x_i,y_i)\in\setR^p\times\setR$, $i=1,\dots,n_\mathrm{train}$, kernel methods typically try to minimize a loss function~$\mathcal{L}_\mathrm{kernel}\colon H_{k}\to\setR$ (similar to the loss function~$\mathcal{L}_\mathrm{dnn}$ introduced above for the training of neural networks) of the form
\[
    \mathcal{L}_\mathrm{kernel}(\Phi) \coloneqq \frac{1}{n_\mathrm{train}}\sum\limits_{i=1}^{n_\mathrm{train}}\lvert\Phi(x_i)-y_i\rvert^2 + \lambda\norm{\Phi}_{H_{k}}^2
\]
for~$\Phi\in H_{k}$, where~$\lambda\geq 0$ denotes a regularization parameter and~$\norm{\,\cdot\,}_{H_{k}}\coloneqq\sqrt{\langle\cdot,\cdot\rangle_{H_{k}}}$ is the norm induced by the inner product~$\langle\cdot,\cdot\rangle_{H_{k}}$ on the reproducing kernel Hilbert space. A well-known representer theorem, see for instance~\cite{bohn2019representer}, reveals that there are coefficients~$\alpha_i\in\setR$, $i=1,\dots,n_\mathrm{train}$, such that it holds
\[
    \Phi^* = \sum\limits_{i=1}^{n_\mathrm{train}}\alpha_i k(\cdot,x_i),
\]
where~$\Phi^*=\argmin_{\Phi\in H_{k}}\mathcal{L}_\mathrm{kernel}(\Phi)$ minimizes the loss function. In particular, the minimizer of the loss function can be represented as a linear combination of the kernel~$k$ evaluated only at the data points~$x_i$, $i=1,\dots,n_\mathrm{train}$.
\par
The setting introduced above can be extended to vector-valued outputs~$y_i\in\setR^{N}$ as required in our use case of approximating the coefficients of the reduced representation of the optimal final time adjoint. To this end, one considers matrix-valued kernels~$k_N\colon\setR^p\times\setR^p\to\setR^{N\times N}$ defined as~$k_N=k\cdot I_N$ and vector-valued coefficients~$\alpha_i\in\setR^N$, where~$I_N\in\setR^{N\times N}$ denotes the~$N\times N$~identity matrix. Furthermore, to obtain a surrogate that can be evaluated fast, it is beneficial to obtain a sparse approximation of~$\Phi^*$. To be more precise, one aims to select an appropriately chosen subset~$\Xi\subset\{1,\dots,n_\mathrm{train}\}$ of the training set such that~$\lvert\Xi\rvert\ll n_\mathrm{train}$ and approximate~$\Phi^*$ as
\begin{align}\label{equ:sparse-kernel-approximation}
    \Phi^* \approx \hat{\Phi} \coloneqq \sum\limits_{i\in\Xi}\alpha_i k_N(\cdot,x_i)
\end{align}
with certain coefficients~$\alpha_i\in\setR^N$ for~$i\in\Xi$. The selection of the training inputs used in the approximation, i.e.~of the set~$\Xi$, can for instance be done using a greedy algorithm similar to those described in~\Cref{sec:reduced-order-modeling-greedy}. One very popular example of such an algorithm is the \emph{vectorial kernel orthogonal greedy algorithm} (VKOGA), see~\cite{santin2021kernel,wenzel2021novel}, that is used for our numerical experiments below.
\par
We will refer to the surrogate model using~$\hat{\pi}_N \coloneqq \hat{\Phi}$ built by the VKOGA algorithm as~\VKOGAROM{}.

\subsubsection{Gaussian Process Regression}\label{subsec:gaussian-process-regression}
Applied to regression and probabilistic classification tasks, Gaussian processes are an indispensable tool in the machine learning landscape. In particular in the context of regression problems, thus known as \emph{Gaussian process regression} (GPR), these methods have proven to provide satisfactory results in many applications. Gaussian process regression has recently been applied to reduced order modeling in~\cite{guo2018reduced}, and also to (stochastic) optimal control in~\cite{mayer2019stochastic}. For a general introduction to Gaussian processes in the context of machine learning and in particular to Gaussian process regression, see~\cite{rasmussen2006gaussian}. There are strong connections between kernel methods and Gaussian process regression, see for instance~\cite{kanagawa2018gaussian} for a review concerning this matter. The same topic is also discussed in~\cite[Chapter~2.2]{rasmussen2006gaussian}. Our presentation in this section follows~\cite{rasmussen2006gaussian}.
\par
Gaussian process regression starts from a parametrized model function whose parameters are determined such that given training data are likely to occur as results of the model and that the chosen parameters also have a high probability. To make this precise, let us denote by~$\theta$ the parameters, also known as \emph{weights}, of our model function. A probability distribution is assigned to these parameters that determines how likely certain values of the parameters are to occur in the model. This distribution is called \emph{prior} and denoted as~$P(\theta)$. Furthermore, we collect the inputs of our training set in a matrix~$X\in\setR^{n_\mathrm{train}\times p}$ and the corresponding outputs in a matrix~$Y\in\setR^{n_\mathrm{train}\times N}$. Furthermore, we denote by~$P(Y|X,\theta)$ the so-called \emph{likelihood} which corresponds to the probability of the outputs~$Y$ given the weights~$\theta$. We should emphasize at this point that the output~$Y$ is assumed to be affected by random noise and hence it is given as a random disturbance of the true model with unknown weights~$\theta$. The \emph{posterior} distribution of the weights~$P(\theta|Y,X)$ can now be computed using Bayes' rule as
\[
    P(\theta|Y,X) = \frac{P(Y|X,\theta)P(\theta)}{P(Y|X)}.
\]
Given a new input~$x\in\setR^p$, the prediction of the model is a probability distribution~$P(y|x,X,Y)$ which for a possible output~$y\in\setR^N$ is calculated as
\[
    P(y|x,X,Y) = \int P(y|x,\theta)P(\theta|X,Y)\d{\theta}.
\]
It provides the average of the likelihood for the individual sample~$(x,y)\in\setR^p\times\setR^N$ over the possible parameters weighted by the posterior distribution given the weights~$\theta$. As the final prediction of the model, the mean~$\mathbb{E}_y[P(y|x,X,Y)]$ of the distribution~$P(y|x,X,Y)$ is taken. Additional properties of the distribution~$P(y|x,X,Y)$, such as for instance its variance or higher order moments, can be used to further characterize the result and obtain confidence intervals for the prediction. Typically, the prior is defined by a covariance function in form of a kernel similar to the ones introduced in the previous section on kernel methods. The hyper-parameters of the kernel are fitted during the construction of the surrogate, see~\cite{rasmussen2006gaussian} for more details.
\par
The reduced model based on Gaussian process regression, which considers the approximate mapping from parameter to coefficients defined as~$\hat{\pi}_N(x) \coloneqq \mathbb{E}_y[P(y|x,X,Y)]$, will be called~\GPRROM{} in the following.