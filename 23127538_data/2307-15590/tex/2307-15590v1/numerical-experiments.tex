\section{Numerical Experiments}\label{sec:numerical-experiments}
In the following section we investigate the proposed algorithms in two numerical examples. We first consider a parametrized version of the heat equation where the parameter influences the conductivity as well as the target state. The control acts on the two boundaries of the one-dimensional computational domain. As a second example, we examine a damped wave equation with boundary control where the parameter changes the wave propagation speed.
\par
Before describing the numerical experiments in detail, we give an overview of some implementational aspects and discuss the software used to perform the test cases.

\subsection{Implementational Details}\label{subsec:implementational-details}
The temporal and spatial discretizations of the considered equations are specified below individually for the respective examples. In this section we only state general details that are the same for both experiments.
\par
As discussed in~\Cref{subsec:optimality-system}, we apply the~CG~algorithm to solve the optimality system for the exact optimal final time adjoint. We choose a tolerance of~$\varepsilon_\mathrm{CG}=10^{-12}$ for the~CG~method in all experiments, i.e.~the algorithm is stopped once the Euclidean norm of the residual drops below~$\varepsilon_\mathrm{CG}$.
\par
All numerical test cases are implemented in the \texttt{Python} programming language. Computations dealing with vectors and matrices, in particular computations of matrix decompositions, use the \texttt{numpy}~\cite{harris2020array} and \texttt{scipy}~\cite{virtanen2020SciPy} packages. The neural networks are implemented using \texttt{PyTorch}~\cite{paszke2019PyTorch} and are adapted from the code used in the model order reduction software \texttt{pyMOR}~\cite{milk2016pyMOR}. Furthermore, we apply the \texttt{VKOGA} library\footnote{The \texttt{VKOGA} library is available at \url{https://github.com/GabrieleSantin/VKOGA}}~\cite{santin2021kernel} for implementing the kernel methods, and use the implementation of Gaussian process regression available in \texttt{scikit-learn}~\cite{scikit-learn}.
\par
We use a fixed neural network architecture throughout the experiments that consists of three hidden layers with~$50$~neurons in each layer, i.e.~$L=4$, $N_0=p$, $N_1=N_2=N_3=50$, and~$N_4=N$. This architecture is chosen sufficiently large for our application and no special adaptation was necessary in our experiments. As activation function we apply the well-known hyperbolic tangent~$\rho=\tanh$. As described in~\Cref{subsec:deep-neural-networks} we use the L-BFGS algorithm implemented in~\texttt{PyTorch} for optimization and perform~$10$~restarts of the training using random initial weights and biases. The neural network obtaining the smallest loss is finally returned by the training procedure and used for the prediction. For validation of the results during the optimization,~$10\%$~of the training parameters are randomly selected as a validation set. The validation data is used to evaluate the early stopping criterion which checks whether the loss decreased within the last~$10$ optimization steps. If that is not the case, the training is stopped.
\par
For the ~\VKOGAROM{} we apply the Gaussian radial basis function kernel~$k\colon\setR^p\times\setR^p\to\setR$ defined for~$x,y\in\setR^p$ as
\[
    k(x,y) \coloneqq \exp\left(-\big(\beta\norm{x-y}_2\big)^2\right),
\]
where the constant~$\beta>0$ is defined for the heat equation experiment in~\Cref{subsec:heat-equation-experiment} as~$\beta=0.1$ and for the wave equation example in~\Cref{subsec:wave-equation-experiment} as~$\beta=1$. Further, in order to select the subset~$\Xi$ of training parameters used for the sparse representation of the kernel interpolant in~\cref{equ:sparse-kernel-approximation}, we use the~$P$-greedy algorithm (see~\cite{santin2021kernel} for more details) with a tolerance of~$\varepsilon_P=10^{-10}$. The regularization parameter~$\lambda$ is set to~$\lambda=0$, i.e.~no additional regularization is incorporated in the loss function.
\par
For the Gaussian process regression surrogate, we apply the prior defined by the covariance function given through the kernel~$k$ defined as
\[
    k(x,y) \coloneqq c\cdot\exp\left(-\frac{\norm{x-y}_2^2}{2l^2}\right)
\]
for inputs~$x,y$ with the hyper-parameters~$c\in[0.1,1000]$ and~$l\in[0.001,1000]$. The optimization process for fitting the hyper-parameters is restarted~$10$~times with different initial guesses to maximize the $\log$-marginal likelihood. To ensure that the matrix appearing during the fitting process is positive-definite, 
we add a constant of~$\alpha=0.001$ to the diagonal of the kernel matrix. Furthermore, the outputs are normalized to zero mean and unit variance which fits to the chosen prior with the same properties.
\par
Regarding the constants~$C_\Lambda$, $C_{\varphi^*}$, and~$\gamma$ introduced in~\Cref{subsec:greedy-for-reduced-basis}, we remark that there typically do not exist sharp and easy to compute estimates for these constants. A naive estimate using submultiplicativity of the matrix norm and the triangle inequality results in pessimistic estimates that are impractical in numerical experiments. We therefore directly prescribe the tolerance~$\tilde{\varepsilon}$ and select an appropriate training set~$\paramstrain\subset\params$ (instead of specifying the error tolerance~$\varepsilon$), which is sufficient to run the greedy procedure from~\Cref{alg:offline-greedy} starting in~Line~\ref{lst:start-greedy-without-tolerance}.
\par
The numerical experiments have been carried out on a dual socket compute server with two Intel(R) Xeon(R) Gold 6254 CPUs running at 3.10GHz and 36~cores in each CPU.
\par
We also provide the source code for our numerical experiments~\cite{sourcecode}, which can be used to reproduce the numerical results stated below\footnote{The corresponding \texttt{GitHub}-repository containing the source code is available at~\url{https://github.com/HenKlei/ML-OPT-CONTROL}}.

\subsection{Test Case 1: Heat Equation}\label{subsec:heat-equation-experiment}
As a first example we consider the heat equation in one spatial dimension with a two-dimensional parameter~$\mu\in\params\coloneqq[1,2]\times[0.5,1.5]\subset\setR^2$. The first component~$\mu_1$ of the parameter~$\mu=[\mu_1,\mu_2]\in\params$ determines the conductivity in the equation whereas the second component~$\mu_2$ determines the target state~$v_\mu^T$. The problem of interest is given as
\begin{alignat*}{2}
    \partial_t v_\mu(t,y) - \mu_1\Delta v_\mu(t,y) &= 0 && \text{for }t\in[0,T],y\in\Omega, \\
    v_\mu(t,0) &= u_{\mu,1}(t) && \text{for }t\in[0,T], \\
    v_\mu(t,1) &= u_{\mu,2}(t) && \text{for }t\in[0,T], \\
    v_\mu(0,y) &= v_\mu^0(y) = \sin(\pi y) \qquad && \text{for }y\in\Omega.
\end{alignat*}
Here, we denote by~$u_\mu(t)=\big[u_{\mu,1}(t),u_{\mu,2}(t)\big]^\top\in\setR^2$ for~$t\in[0,T]$ the (two-dimensional) control that influences the boundary conditions on both ends of the spatial domain~$\Omega=[0,1]$. The target state is given as~$v_\mu^T(y)=\mu_2y$ for~$y\in\Omega$. Furthermore, we consider the dynamics until the final time~$T=0.1$ is reached. The system above is discretized in space using a second-order central finite difference scheme on a uniform spatial grid consisting of~$n_y=100$ inner points with a spatial grid size of~$h=1/(n_y+1)$. In such a way the above system can be written in the form of~\eqref{equ:parametrized-control-system} with the system matrices given as
\begin{align}\label{equ:system-matrices-heat-equation}
    A_\mu = \frac{\mu_1}{h^2}\bar{A}
    \qquad\text{with}\qquad
    \bar{A}=\begin{bmatrix}
        -2 & 1      &        &        &   \\
        1 & -2      & 1      &        &   \\
          &  \ddots & \ddots & \ddots &   \\
          &         & 1      & -2     & 1 \\
          &         &        & 1      & -2
    \end{bmatrix}\in\setR^{n\times n}
    \qquad\text{and}\qquad
    B_\mu = \frac{\mu_1}{h^2}\begin{bmatrix}
        1 & 0 \\
        0 & 0 \\
        \vdots & \vdots \\
        0 & 0 \\
        0 & 1
    \end{bmatrix}\in\setR^{n\times 2},
\end{align}
where the system dimension is~$n=n_y$. This system fulfills the well-known \emph{Kalman rank condition} for all parameters~$\mu\in\params$ and is therefore controllable. Furthermore, the continuity requirements stated in~\Cref{as:continuity-parameter-to-system-matrices} are also obviously fulfilled due to the choice of the system matrices~$A_\mu$ and~$B_\mu$ and the target state~$v_\mu^T$.
\par
For discretization of the time derivative we apply the Crank-Nicolson method, see~\cite{crank1947practical}, for~$n_t=30\cdot n_y$ uniform time steps with a time step size of~$\Delta t=1/n_t$. 
\par
In the context of discretized PDEs, the standard Euclidean inner product might not be well-suited. We therefore choose a weighted version of the Euclidean inner product for the state space that takes into account the spatial discretization size~$h$, see~\cite{haasdonk2011efficient}. To be more precise, we equip the state space~$\X=\setR^n$ with the inner product~$\langle\cdot,\cdot\rangle_h$ defined for~$x,y\in\X$ as
\begin{align}\label{equ:inner-product-state-space-example}
    \langle x,y\rangle_h = h\langle x,y\rangle_2,
\end{align}
where~$\langle\cdot,\cdot\rangle_2$ denotes the standard Euclidean inner product on~$\setR^n$. For the space of controls~$\U=\setR^2$ we use the standard Euclidean inner product. However, on the time-discretized control space, i.e.~the discretized version of the space~$G$, we consider the norm~$\norm{\cdot}_{\Delta t}$ defined for~$u=[u_{i,j}]_{i=1,\dots,n_t;j=1,2}\in\setR^{n_t\times 2}$ as
\begin{align}\label{equ:norm-control-space-example}
    \norm{u}_{\Delta t} \coloneqq \left(\Delta t\sum\limits_{i=1}^{n_t} \norm{u_{i}}_2^2\right)^{\frac{1}{2}},
\end{align}
which is only used to compare optimal and approximate controls. Due to the definition of the cost functional in~\cref{equ:cost-functional}, the inner product~$\langle\cdot,\cdot\rangle_h$ also enters the optimal control problem.
\par
The weighting matrices~$M\in\setR^{n\times n}$ and~$R\in\setR^{2\times 2}$ are chosen as
\[
    M = I\qquad\text{and}\qquad R = \begin{bmatrix}0.125 & 0 \\ 0 & 0.25\end{bmatrix},
\]
which in particular fulfill~\Cref{as:symmetric-product}. The different weights for the different components of the control have the effect that the size of the second component~$u_{\mu,2}$ is penalized stronger than the size of the first one~$u_{\mu,1}$, i.e.~controlling the right boundary value of the state is considered to be more costly than controlling the left boundary value. An example of the optimal final time adjoint as well as the associated controls and states for the parametrized heat equation is shown in~\Cref{fig:heat-equation-example-results} for the parameter~$\mu=(1.5,0.75)\in\params$. We denote the discretized states by~$x_\mu$ to use the same notation as in the previous sections. In the top left plot of the figure we notice that the initial state is driven close to the target state over time with the control acting on the boundary points. This can also be seen in the bottom left plot of the figure where the initial state, target state and optimal state at final time are compared. Since we are solving an optimal control problem where a deviation from the target state is penalized but the system is not forced to hit the target state exactly, we see a slight deviation from the target state in the optimal final time state. Furthermore, we observe in the top right plot of~\Cref{fig:heat-equation-example-results} that the optimal final time adjoint is very smooth, which facilitates the approximation of optimal final time adjoints by lower-dimensional subspaces. The bottom right plot of~\Cref{fig:heat-equation-example-results} shows how the two components of the control change over time. In particular at the final time~$T=0.1$, the first component~$u_{\mu,1}^*$ is close to~$0$ while the second component~$u_{\mu,2}^*$ is close to the value~$0.75$, as expected due to the choice of the target state~$v_\mu^T(y)=\mu_2y$, the parameter value~$\mu_2=0.75$, and because the controls act on the boundary values of the solution.
\par
% Figure environment removed
We apply the greedy procedure from~\Cref{subsec:greedy-for-reduced-basis} to construct a reduced basis using~$n_\mathrm{train}=64$ samples on a uniform~$8\times 8$~grid in the parameter space~$\params$. At this point, we emphasize once again that the full optimal control problem is not solved for all parameters in the training set~$\paramstrain$ during~\Cref{alg:offline-greedy}. Instead, the exact solution is only computed for those parameters that are selected by the greedy procedure. It is therefore possible to consider a relatively large training set for the greedy algorithm since the error estimator is rather cheap to evaluate.
\par
The results of the greedy algorithm are depicted in~\Cref{fig:heat-equation-greedy-results}. The greedy tolerance was set to~$\tilde{\varepsilon}=10^{-6}$ which is reached for a reduced basis size of~$N=8$, see~\Cref{fig:heat-equation-greedy-results}. Since no prior knowledge of the constants~$C_\Lambda$ and~$\gamma$ is available for this example, we directly set the tolerance~$\tilde{\varepsilon}$ appearing in the termination criterion in~Line~\ref{lst:offline-termination-criterion} of~\Cref{alg:offline-greedy} instead of the greedy tolerance~$\varepsilon$. We also observe that the overestimation of the true error by the error estimator becomes smaller with a larger reduced basis size and that the estimated maximum error is close to the true maximum error. In~\Cref{fig:heat-equation-singular-values}, the singular values of the optimal final time adjoints associated to the~$n_\mathrm{train}=64$ training parameters are shown in decreasing order. To be more precise, we solved for the optimal final time adjoint for all training parameters, collected these vectors as columns in a matrix, and computed the singular value decomposition of that matrix. Due to equivalences of norms, the respective singular values show (approximately) the order of the decay of the Kolmogorov~$N$-width of the solution manifold~$\mathcal{M}$ of optimal final time adjoints (at least on the training set~$\paramstrain$). One can observe an exponential decay in the singular values until machine precision is reached, see also the blue line marked by~\raisebox{2pt}{\addlegendimageintext{thick, color6}} in the figure. This suggests that the manifold~$\mathcal{M}$ is amenable for approximation by low-dimensional linear subspaces. Hence, the greedy algorithm can be applied in this setting and finds a suitable reduced space according to~\Cref{thm:devore-greedy,thm:weak-greedy-and-approximation-error}.
\par
% Figure environment removed
Afterwards, the same set of training parameters as for the greedy algorithm has been used for training the different machine learning surrogates. Once again, we can use the relatively large training set consisting of~$n_\mathrm{train}=64$ parameters, because the computation of the training snapshots is already performed during the greedy algorithm and the exact optimal control problem does not have to be solved for all parameters in~$\paramstrain$. All reduced models, the~\RBROM{}, the~\DNNROM{}, the~\VKOGAROM{}, and the~\GPRROM{} have been tested on a set of~$100$ randomly chosen parameters that have not been part of the training set. The results are summarized in~\Cref{tab:heat-equation-results} and presented in more detail in~\Cref{fig:heat-equation-box-plot,fig:heat-equation-errors-parameters}. Here we address two types of errors: the errors in the optimal final time adjoints, i.e.~the deviation of~$\varphi_\mu^*(T)$ from its approximate value, and similarly the errors in the optimal controls. Both, the averaged and maximal errors in the final time adjoint, are well below~$10^{-4}$ for all considered methods. We emphasize at this point that the tolerance~$\tilde{\varepsilon}=10^{-6}$ prescribed for the greedy construction of the reduced basis is only reached by the~\RBROM{}. The approximation of the reduced coefficients by machine learning introduces an additional error that is reflected in a larger error in the final time adjoint. Nevertheless, the machine learning algorithms provide good approximations in particular in terms of the average error. Further, the average errors in the control for all reduced models are of order~$10^{-6}$ or even smaller and therefore the approximate controls are close to the optimal ones. The last two columns of~\Cref{tab:heat-equation-results} show average runtimes of the considered algorithms for computing the (approximate) control and speedups of the reduced models compared to the computation of the exact solution. Here we see a speedup of about~$2$ for the~\RBROM{} while all machine learning surrogates reach speedups of around~$40$. Therefore, the machine learning reduced models outperform the~\RBROM{} in terms of computational efficiency during the online phase. In this example, in particular the~\GPRROM{} constitutes a serious alternative to the~\RBROM{} also with respect to approximation accuracy. We omit a detailed listing of the training times for the machine learning models since they are performed completely offline and amount to a couple of seconds at most. They are negligible compared to the time required for building the reduced basis using the greedy procedure. In~\Cref{fig:heat-equation-errors-parameters} we present the true errors and the estimated errors for the various reduced models over the enumerated test parameter set. The estimated errors are about an order of magnitude larger for the machine learning models compared to the true errors. Accordingly, the efficiency constant of the error estimator~$\eta_\mu$ seems to be relatively moderate, which results in a good error estimate for the machine learning models. It is also visible that the estimator indeed provides a reliable estimate, i.e.~the estimated error is an upper bound for the actual error. For the~\RBROM{} we in particular observe that both, the true and the estimated error in the final time adjoint, are below the prescribed tolerance of~$10^{-6}$ over the whole parameter test set.
\begin{table}[ht]
	\centering
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{l c c c c d{1.4} d{2.2}}
			\toprule
			{Method} & {Max.~error adjoint} & {Avg.~error adjoint} & {Max.~error control} & {Avg.~error control} & \mc{Avg.~runtime (s)} & \mc{Avg.~speedup} \\ \midrule \midrule
			
			Exact solution & {$-$} & {$-$} & {$-$} & {$-$} & 6.2760 & \mc{$-$} \\
			
			\RBROM{} & {$2.3\cdot 10^{-7}$} & {$5.3\cdot 10^{-8}$} & {$2.3\cdot 10^{-8}$} & {$5.4\cdot 10^{-9}$} & 2.6526 & 2.37 \\
			
            \DNNROM{} & {$2.2\cdot 10^{-5}$} & {$5.8\cdot 10^{-6}$} & {$9.1\cdot 10^{-6}$} & {$2.0\cdot 10^{-6}$} & 0.1623 & 40.33 \\

            \VKOGAROM{} & {$7.0\cdot 10^{-5}$} & {$1.8\cdot 10^{-5}$} & {$2.5\cdot 10^{-5}$} & {$6.9\cdot 10^{-6}$} & 0.1580 & 41.03 \\

            \GPRROM{} & {$1.4\cdot 10^{-5}$} & {$2.2\cdot 10^{-6}$} & {$4.2\cdot 10^{-6}$} & {$7.6\cdot 10^{-7}$} & 0.1572 & 41.40 \\
			\bottomrule
	\end{tabular}}
	\caption{Results of the numerical experiments for the heat equation using the~\RBROM{}, \DNNROM{}, \VKOGAROM{} and~\GPRROM{}.}
	\label{tab:heat-equation-results}
\end{table}

% Figure environment removed

\subsection{Test Case 2: Damped Wave Equation}\label{subsec:wave-equation-experiment}
In our second numerical example we consider a parametrized damped wave equation with a Dirichlet control acting on the right boundary of the one-dimensional spatial domain. The problem of interest reads as
\begin{alignat*}{2}
    \partial_{tt} v_\mu(t,y) + \nu\partial_t v_\mu(t,y) - \mu\Delta v_\mu(t,y) &= 0 && \text{for }t\in[0,T],y\in\Omega, \\
    v_\mu(t,0) &= 0 && \text{for }t\in[0,T], \\
    v_\mu(t,1) &= u_{\mu}(t) && \text{for }t\in[0,T], \\
    v_\mu(0,y) &= v_\mu^0(y) = \sin(\pi y) \qquad && \text{for }y\in\Omega, \\
    \partial_t v_\mu(0,y) &= 0 && \text{for }y\in\Omega.
\end{alignat*}
The damping constant is chosen as~$\nu=10$, see the discussion below as well. Further, we choose as final time~$T=1$, the spatial domain~$\Omega=[0,1]$, while the parameter~$\mu$, determining the velocity, ranges within the interval~$\params=[3,10]\subset\setR$. Similarly as in the example of the heat equation we discretize the spatial derivative by means of second-order central finite differences on a grid with~$n_y=100$ inner grid points. For time discretization we apply the Crank-Nicolson scheme with~$n_t=10\cdot n_y$ time steps. We rewrite the discretized damped wave equation as a first order system of dimension~$n=2\cdot n_y=200$. The resulting system matrices are given as
\[
    A_\mu = \begin{bmatrix}[ccc|ccc]
        \hphantom{0}&\hphantom{0}&\hphantom{0}&\hphantom{0}&\hphantom{0}&\hphantom{0} \\
        \multicolumn{3}{c|}{0} & \multicolumn{3}{c}{I} \\
        &&&&& \\
        \cmidrule(lr){1-6}
        &&&&& \\
        \multicolumn{3}{c|}{\frac{\mu}{h^2}\bar{A}} & \multicolumn{3}{c}{-\nu I} \\
        &&&&&
    \end{bmatrix}\in\setR^{n\times n}
    \qquad\text{and}\qquad
    B_\mu = \frac{\mu}{h^2}\begin{bmatrix}
        0 \\
        \vdots \\
        0 \\
        1
    \end{bmatrix}\in\setR^{n\times 1},
\]
where the matrix~$\bar{A}\in\setR^{n_y\times n_y}$ was defined above for the heat equation example in~\cref{equ:system-matrices-heat-equation}, and~$I\in\setR^{n_y\times n_y}$ denotes the identity matrix. Similar to the heat equation example, the discretized system is controllable, i.e.~the Kalman rank condition is fulfilled. Furthermore, the parameter to system matrices mappings are Lipschitz continuous and therefore~\Cref{as:continuity-parameter-to-system-matrices} holds as well.
\par
We prescribe the target~$v_\mu^T(y)=y$ and~$\partial_tv_\mu^T(y)=0$ for all~$y\in\Omega$ and~$\mu\in\params$. Further, we choose the same inner products and norms as in the previous example of the heat equation, see~\cref{equ:inner-product-state-space-example,equ:norm-control-space-example}. In this example we set~$M=10\cdot I\in\setR^{n\times n}$ and~$R=0.1\cdot I\in\setR^{1\times 1}$ for the weighting matrices associated to the different terms in the objective functional. Hence, also~\Cref{as:symmetric-product} is fulfilled in this case.
\par
\Cref{fig:wave-equation-example-results} depicts the optimal final time adjoint, optimal control and corresponding optimal states (as space-time plot and at final time~$T$) for the damped wave equation with parameter~$\mu=5\in\params$. The initial state is driven towards the target state as can be seen in the plots on the left hand side of the figure. As in the heat equation example, the target state is not hit exactly but a deviation is allowed. The velocity component of the states is not shown in the plot since the target and initial velocity are zero and the velocity of the optimal state at final time is of order~$10^{-3}$ or smaller. The top right plot shows the optimal final time adjoint divided into its two components corresponding to the state and the velocity. One can see that the final time adjoint state is quite smooth for both components and that in particular the second component, associated to the velocity, is relatively close to zero. This facilitates a low-dimensional approximation of the final time adjoint. The bottom right plot of~\Cref{fig:wave-equation-example-results} shows the optimal control which exhibits the typical oscillations patterned at the end of the time interval. These oscillations are much more pronounced than the oscillations in the optimal final time adjoint. Hence, it seems advantageous to approximate the adjoint datum instead of the control.
\par
% Figure environment removed
The plots in~\Cref{fig:wave-equation-example-results} depict the solution for a single parameter value~$\mu=5$. In order to deal with the full range of parameter dependent problems, we apply the greedy algorithm~\cref{alg:offline-greedy} with a training set consisting of~$n_\mathrm{train}=50$ training samples from a uniform grid on the parameter domain~$\params$. The greedy tolerance is set to~$\tilde{\varepsilon}=10^{-2}$. Similar to the previous example we have no prior knowledge of the constants~$C_\Lambda$ and~$\gamma$ and therefore directly select~$\tilde{\varepsilon}$ instead of~$\varepsilon$.
\par
\Cref{fig:wave-equation-greedy-results} shows the estimated maximum and true maximum errors over the greedy steps until the greedy tolerance~$\tilde{\varepsilon}$ is reached for a reduced basis size of~$N=18$. The parameters selected by the greedy algorithm are also shown in~\Cref{fig:wave-equation-errors-parameters} (marked as crosses~\raisebox{.24ex}{\addlegendimageintext{mark=x, only marks, color1}}) and are distributed almost uniformly across the parameter range with a slight clustering at the smaller parameter values. One can observe in~\Cref{fig:wave-equation-greedy-results} that the estimator is more pessimistic in this example than in the heat equation problem and overestimates the true error by about two orders of magnitude. Nevertheless, the error estimator still provides a reliable estimate of the error, i.e.~the estimated error is an upper bound for the true error. However, the greedy procedure is not stopped with a reduced size of~$5$ although the true error would have been already below the prescribed tolerance for such a basis size. This behavior reveals the fact that the constant~$C_\Lambda$, which determines the efficiency of the error estimator, is larger for this example. This is to be expected, as the dissipation rate is weaker in this case than it was for the heat equation example.
\par
In order to explore how the dissipation rate influences the problem, we analyze its dependence on the damping constant~$\nu$. \Cref{fig:wave-equation-singular-values} shows the singular values for the optimal final time adjoints over the training set for different values of the damping constant~$\nu$. The singular value decay is rather slow in the case of the undamped wave equation, i.e.~for~$\nu=0$, see also~\cite{greif2019decay}. The same behavior emerges for a small value of the damping constant like~$\nu=5$, which means that also the decay of the Kolmogorov~$N$-width of the manifold of optimal final time adjoints is slow in these cases. Thus, a relatively large reduced space would be necessary to obtain a sufficiently accurate reduced model. In that regard, the restriction to linear subspaces (constructed by the greedy algorithm in this case) can be seen as a limitation of the general methodology since it might not be applicable to all control systems. The introduction of a larger damping constant such as~$\nu=10$ or even~$\nu=100$, however, leads to a faster decay of the singular values (see the curves marked as~\addlegendimageintext{thick, color1, mark=diamond*, mark size=3, mark options={solid, fill opacity=0.5}} and~\addlegendimageintext{thick, color4, mark=triangle*, mark size=3, mark options={solid, fill opacity=0.5}} in~\Cref{fig:wave-equation-singular-values}) and thus makes the solution manifold amenable for approximation by linear subspaces of small dimension. For a large damping constant of~$\nu=100$ we observe an exponential decay of the singular values (similar to the heat equation example) up to a reduced basis size of around~$30$. For a moderate damping constant of~$\nu=10$, the decay is also exponential at least for the first couple of modes. We should nevertheless mention that also in the case of the damped wave equation with~$\nu=10$, the decay of the singular values slows down quite rapidly after about~$7$ modes. Hence, to reach very small approximation errors, still a large reduced basis would be required. Since we are not interested in an example where the dissipativity, resulting from a large damping constant, dominates the solution behavior, we choose a value of~$\nu=10$ and the larger greedy tolerance of~$\tilde{\varepsilon}=10^{-2}$.
\par
% Figure environment removed
After constructing a reduced basis using the greedy procedure from~\Cref{subsec:greedy-for-reduced-basis}, the machine learning reduced models described in~\Cref{sec:reduced-order-machine-learning} are trained on the same~$n_\mathrm{train}=50$ training parameters that were already used in the greedy algorithm. For testing purposes, we draw~$100$ random values from the parameter range~$\params$ that were not contained in the training set and evaluate the performance of the reduced models on the test parameter set. The results of the online phase are presented in~\Cref{tab:wave-equation-results}. Furthermore, the error statistics for the reduced models are summarized in~\Cref{fig:wave-equation-box-plot}, whereas the errors of the reduced models with respect to the test parameters are shown in detail in~\Cref{fig:wave-equation-errors-parameters}. First of all, we observe an enormous speedup reached by all machine learning models compared to the exact solution of the optimal control and also compared to the~\RBROM{}. The average speedup of the latter model is about~$21$, while the average speedup of the three considered machine learning surrogates is around~$730$. In total, the average runtime could be reduced from about~$229$ seconds for the exact solution to roughly~$0.32$ seconds using the combined machine learning/reduced basis surrogates. In terms of approximation accuracy, we see that the average error in the final time adjoint is about one order of magnitude larger for the machine learning reduced models compared to the~\RBROM{}. For the control we observe that the~\DNNROM{}, the~\VKOGAROM{}, and the~\GPRROM{} all reach average errors of the order~$10^{-4}$. The box plot in~\Cref{fig:wave-equation-box-plot} and also the true errors in~\Cref{fig:wave-equation-errors-parameters} reveal that the~\VKOGAROM{} and the~\GPRROM{} actually suffer from several outliers, in particular at the boundaries of the parameter domain, which limit their average behavior. However, on most parts of the parameter domain the error of those two methods is close to the error of the~\RBROM{}. The~\DNNROM{} on the other hand produces errors that are about one order of magnitude larger on the whole parameter domain but with less outliers. It might be possible to reduce these outliers and further improve the performance of the machine learning models by tuning their hyper-parameters. However, such an optimization of the machine learning models is beyond the scope of this article and not the focus of this work. To adaptively adjust for the outliers, we remark that the error estimator also captures the outliers well and might therefore help to detect areas in the parameter domain where additional training data should be generated to improve the machine learning models. At this point, we also would like to advert to the relatively large overestimation of the error that can be observed in~\Cref{fig:wave-equation-errors-parameters} and was already present in~\Cref{fig:wave-equation-greedy-results}. Also for the most accurate reduced model, i.e.~the~\RBROM{}, the estimated error is about two orders of magnitude larger than the true error. Nevertheless, the curves of estimated errors follow the true errors quite closely in terms of error variation with respect to the parameter, although being two orders of magnitude larger.
\begin{table}[ht]
	\centering
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{l c c c c d{3.4} d{3.2}}
			\toprule
			{Method} & {Max.~error adjoint} & {Avg.~error adjoint} & {Max.~error control} & {Avg.~error control}& \mc{Avg.~runtime (s)} & \mc{Avg.~speedup} \\ \midrule \midrule
			
			Exact solution & {$-$} & {$-$} & {$-$} & {$-$} & 228.8106 & \mc{$-$} \\
			
			\RBROM{} & {$3.0\cdot 10^{-4}$} & {$4.7\cdot 10^{-5}$} & {$1.3\cdot 10^{-4}$} & {$2.3\cdot 10^{-5}$} & 10.8503 & 21.10 \\
			
            \DNNROM{} & {$3.8\cdot 10^{-3}$} & {$3.8\cdot 10^{-4}$} & {$4.6\cdot 10^{-3}$} & {$7.0\cdot 10^{-4}$} & 0.3230 & 731.48 \\

            \VKOGAROM{} & {$2.0\cdot 10^{-2}$} & {$5.7\cdot 10^{-4}$} & {$5.7\cdot 10^{-3}$} & {$2.0\cdot 10^{-4}$} & 0.3226 & 733.80 \\

            \GPRROM{} & {$8.9\cdot 10^{-3}$} & {$3.9\cdot 10^{-4}$} & {$1.1\cdot 10^{-2}$} & {$5.3\cdot 10^{-4}$} & 0.3359 & 708.55 \\
			\bottomrule
	\end{tabular}}
	\caption{Results of the numerical experiments for the damped wave equation using the~\RBROM{}, \DNNROM{}, \VKOGAROM{} and~\GPRROM{}.}
	\label{tab:wave-equation-results}
\end{table}

% Figure environment removed