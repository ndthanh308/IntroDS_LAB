@article{cohen2016kolmogorov,
    title   = {Kolmogorov widths under holomorphic mappings},
    journal = {IMA Journal of Numerical Analysis},
    volume  = {36},
    number  = {1},
    pages   = {1-12},
    year    = {2016},
    doi     = {10.1093/imanum/dru066},
    author  = {Albert Cohen and Ronald DeVore}
}

@article{cohen2015approximation,
    author  = {Albert Cohen and Ronald DeVore},
    title   = {Approximation of high-dimensional parametric PDEs},
    journal = {Acta Numerica},
    volume  = {24},
    number  = {1},
    pages   = {1-159},
    year    = {2015},
    doi     = {10.1017/S0962492915000033}
}

@article{dalsanto2020data,
    title = {Data driven approximation of parametrized PDEs by reduced basis and neural networks},
    author = {Dal Santo, Niccolo and Deparis, Simone and Pegolotti, Luca},
    publisher = {ACADEMIC PRESS INC ELSEVIER SCIENCE},
    journal = {Journal Of Computational Physics},
    address = {San Diego},
    volume = {416},
    pages = {109550},
    year = {2020},
    doi = {10.1016/j.jcp.2020.109550}
}

@inproceedings{molinari2021iterative,
  title={Iterative regularization for convex regularizers},
  author={Molinari, Cesare and Massias, Mathurin and Rosasco, Lorenzo and Villa, Silvia},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1684--1692},
  year={2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
}

@book{peypouquet2015convex,
  title={Convex optimization in normed spaces: theory, methods and examples},
  doi = {10.1007/978-3-319-13710-0},
  year = {2015},
  publisher = {Springer International Publishing},
  author = {Juan Peypouquet}
}

@incollection{ballarin2022spacetime,
    title = {Chapter 9 - Space-time POD-Galerkin approach for parametric flow control},
    editor = {Emmanuel Trélat and Enrique Zuazua},
    series = {Handbook of Numerical Analysis},
    publisher = {Elsevier},
    volume = {23},
    pages = {307-338},
    year = {2022},
    booktitle = {Numerical Control: Part A},
    issn = {1570-8659},
    doi = {10.1016/bs.hna.2021.12.009},
    author = {Francesco Ballarin and Gianluigi Rozza and Maria Strazzullo}
}

@article{daniel2020model,
	Author = {Daniel, Thomas and Casenave, Fabien and Akkari, Nissrine and Ryckelynck, David},
	Journal = {Advanced Modeling and Simulation in Engineering Sciences},
	Number = {1},
	Pages = {16},
	Title = {Model order reduction assisted by deep neural networks ({ROM}-net)},
	Volume = {7},
	Year = {2020},
    doi = {10.1186/s40323-020-00153-6},
}

@article{lazar2023greedy,
    title = {Greedy search of optimal approximate solutions},
    journal = {Pure and Applied Functional Analysis},
    volume = {8},
    number = {2},
    pages = {547-564},
    year = {2023},
    author = {Martin Lazar and Enrique Zuazua}
}

@incollection{lazar2022control,
    title = {Chapter 8 - Control of parameter dependent systems},
    editor = {Emmanuel Trélat and Enrique Zuazua},
    series = {Handbook of Numerical Analysis},
    publisher = {Elsevier},
    volume = {23},
    pages = {265-306},
    year = {2022},
    booktitle = {Numerical Control: Part A},
    issn = {1570-8659},
    doi = {10.1016/bs.hna.2021.12.008},
    author = {Martin Lazar and Jérôme Lohéac}
}

@article{lazar2016greedy,
    title = {Greedy controllability of finite dimensional linear systems},
    journal = {Automatica},
    volume = {74},
    pages = {327-340},
    year = {2016},
    doi = {10.1016/j.automatica.2016.08.010},
    author = {Martin Lazar and Enrique Zuazua},
    keywords = {Parametrised ODEs and PDEs, Greedy control, Weak-greedy, Heat equation, Wave equation, Finite-differences},
    abstract = {We analyse the problem of controllability for parameter dependent linear finite-dimensional systems. The goal is to identify the most distinguished realisations of those parameters so to better describe or approximate the whole range of controls. We adapt recent results on greedy and weak greedy algorithms for parameter dependent PDEs or, more generally, abstract equations in Banach spaces. Our results lead to optimal approximation procedures that, in particular, perform better than simply sampling the parameter-space to compute the controls for each of the parameter values. We apply these results for the approximate control of finite-difference approximations of the heat and the wave equation. The numerical experiments confirm the efficiency of the methods and show that the number of weak-greedy samplings that are required is particularly low when dealing with heat-like equations, because of the intrinsic dissipativity that the model introduces for high frequencies.}
}

@article{hesthaven2018nonintrusive,
    AUTHOR = {Hesthaven, Jan S. and Ubbiali, Stefano},
    TITLE = {Non-intrusive reduced order modeling of nonlinear problems using neural networks},
    JOURNAL = {Journal of Computational Physics},
    VOLUME = {363},
    YEAR = {2018},
    PAGES = {55--78},
    MRCLASS = {65N30 (65N22)},
    MRNUMBER = {3784416},
    DOI = {10.1016/j.jcp.2018.02.037},
}

@article{wang2019nonintrusive,
    author   = {Wang, Qian and Hesthaven, Jan S. and Ray, Deep},
    title    = {Non-intrusive reduced order modeling of unsteady flows using artificial neural networks with application to a combustion problem},
    year     = {2019},
    volume   = {384},
    pages    = {289--307},
    doi      = {10.1016/j.jcp.2019.01.031},
    journal = {Journal of Computational Physics},
    mrclass  = {80A25 (65M99)},
    mrnumber = {3920924},
}

@article{devore2013greedy,
    author={DeVore, Ronald and Petrova, Guergana and Wojtaszczyk, Przemyslaw},
    title={Greedy Algorithms for Reduced Bases in Banach Spaces},
    journal={Constructive Approximation},
    year={2013},
    volume={37},
    number={3},
    pages={455-466},
    doi={10.1007/s00365-013-9186-2},
}

@article{haasdonk2023certified,
    author = {Haasdonk, Bernard and Kleikamp, Hendrik and Ohlberger, Mario and Schindler, Felix and Wenzel, Tizian},
    title = {A New Certified Hierarchical and Adaptive RB-ML-ROM Surrogate Model for Parametrized PDEs},
    journal = {SIAM Journal on Scientific Computing},
    volume = {45},
    number = {3},
    pages = {A1039-A1065},
    year = {2023},
    doi = {10.1137/22M1493318},
    abstract = {Abstract. We present a new surrogate modeling technique for efficient approximation of input-output maps governed by parametrized PDEs. The model is hierarchical as it is built on a full order model, reduced order model (ROM), and machine learning (ML) model chain. The model is adaptive in the sense that the ROM and ML model are adapted on the fly during a sequence of parametric requests to the model. To allow for a certification of the model hierarchy, as well as to control the adaptation process, we employ rigorous a posteriori error estimates for the ROM and ML models. In particular, we provide an example of an ML-based model that allows for rigorous analytical quality statements. We demonstrate the efficiency of the modeling chain on a Monte Carlo and a parameter-optimization example. Here, the ROM is instantiated by Reduced Basis methods, and the ML model is given by a neural network or by a kernel model using vectorial kernel orthogonal greedy algorithms.}
}

@article{wenzel2023application,
      title={Application of Deep Kernel Models for Certified and Adaptive RB-ML-ROM Surrogate Modeling}, 
      author={Tizian Wenzel and Bernard Haasdonk and Hendrik Kleikamp and Mario Ohlberger and Felix Schindler},
      year={2023},
      eprint={2302.14526},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@inproceedings{kmet2011neural,
    author={Kmet, Tibor},
    editor={Honkela, Timo
    and Duch, W{\l}odzis{\l}aw
    and Girolami, Mark
    and Kaski, Samuel},
    title={Neural Network Solution of Optimal Control Problem with Control and State Constraints},
    booktitle={Artificial Neural Networks and Machine Learning -- ICANN 2011},
    year={2011},
    publisher={Springer Berlin Heidelberg},
    address={Berlin, Heidelberg},
    pages={261--268},
    doi={10.1007/978-3-642-21738-8_34},
    abstract={A neural network based optimal control synthesis is presented for solving optimal control problems with control and state constraints. The optimal control problem is transcribed into nonlinear programming problem which is implemented with adaptive critic neural network. The proposed simulation methods is illustrated by the optimal control problem of feeding adaptation of filter feeders of Daphnia. Results show that adaptive critic based systematic approach holds promise for obtaining the optimal control with control and state constraints.},
    isbn={978-3-642-21738-8}
}

@article{dede2012reduced,
    author={Ded{\`e}, Luca},
    title={Reduced Basis Method and Error Estimation for Parametrized Optimal Control Problems with Control Constraints},
    journal={SIAM Journal of Scientific Computing},
    year={2012},
    volume={50},
    number={2},
    pages={287-305},
    abstract={We propose a Reduced Basis method for the solution of parametrized optimal control problems with control constraints for which we extend the method proposed in Ded{\`e}, L. (SIAM J. Sci. Comput. 32:997, 2010) for the unconstrained problem. The case of a linear-quadratic optimal control problem is considered with the primal equation represented by a linear parabolic partial differential equation. The standard offline--online decomposition of the Reduced Basis method is employed with the Finite Element approximation as the ``truth'' one for the offline step. An error estimate is derived and an heuristic indicator is proposed to evaluate the Reduced Basis error on the optimal control problem at the online step; also, the indicator is used at the offline step in a Greedy algorithm to build the Reduced Basis space. We solve numerical tests in the two-dimensional case with applications to heat conduction and environmental optimal control problems.},
    doi={10.1007/s10915-011-9483-5},
}

@article{ehring2023hermite,
      title={Hermite kernel surrogates for the value function of high-dimensio\-nal nonlinear optimal control problems}, 
      author={Tobias Ehring and Bernard Haasdonk},
      year={2023},
      eprint={2305.06122},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@book{wendland2005scattered,
    title = {Scattered {D}ata {A}pproximation},
    publisher = {Cambridge University Press},
    year = {2005},
    author = {Wendland, Holger},
    volume = {17},
    series = {Cambridge Monographs on Applied and Computational Mathematics},
    address = {Cambridge},
    isbn = {978-0521-84335-5},
    doi = {10.1017/CBO9780511617539},
}

@book{hesthaven2016certified,
    title = {Certified Reduced Basis Methods for Pa\-ra\-me\-trized Partial Differential Equations},
    publisher = {Springer Cham},
    year = {2016},
    author = {Jan S. Hesthaven and Gianluigi Rozza and Benjamin Stamm},
    address={New York},
    series = {SpringerBriefs in Mathematics},
    isbn = {978-3-319-22470-1},
    doi = {10.1007/978-3-319-22470-1},
}

@incollection{santin2021kernel,
    author       = {Santin, Gabriele and Haasdonk, Bernard},
    title        = {Kernel Methods for Surrogate Modeling},
    booktitle    = {Model Order Reduction},
    year         = {2021},
    editor       = {Benner, Peter and Grivet-Talocia, Stefano and Quarteroni, Alfio and Rozza, Gianluigi and Schilders, Wil and Silveira, Luís Miguel},
    booksubtitle = {System- and Data-Driven Methods and Algorithms},
    volume       = {2},
    publisher    = {De Gruyter},
    doi          = {10.1515/9783110498967-009},
}

@article{wenzel2021novel,
    title = {A novel class of stabilized greedy kernel approximation algorithms: Convergence, stability and uniform point distribution},
    journal = {Journal of Approximation Theory},
    volume = {262},
    pages = {105508},
    year = {2021},
    doi = {10.1016/j.jat.2020.105508},
    author = {Tizian Wenzel and Gabriele Santin and Bernard Haasdonk},
    keywords = {Kernel methods, Greedy algorithms, Convergence rates},
    abstract = {Kernel based methods provide a way to reconstruct potentially high-dimensional functions from meshfree samples, i.e., sampling points and corresponding target values. A crucial ingredient for this to be successful is the distribution of the sampling points. Since the computation of an optimal selection of sampling points may be an infeasible task, one promising option is to use greedy methods. Although these methods may be very effective, depending on the specific greedy criterion the chosen points might quickly lead to instabilities in the computation. To circumvent this problem, we introduce and investigate a new class of stabilized greedy kernel algorithms, which can be used to create a scale of new selection strategies. We analyze these algorithms, and in particular we prove convergence results and quantify in a precise way the distribution of the selected points. These results allow to prove, in the case of certain Sobolev kernels, that the algorithms have optimal stability and optimal convergence rates, including for functions outside the native space of the kernel. The results also apply to the case of the usual P-greedy algorithm, significantly improving state-of-the-art results available in the literature. Illustrative experiments are presented that support the theoretical findings and show improvements of the stabilized algorithms in terms of accuracy due to improved stability.}
}

@article{santin2017convergence,
    title = {Convergence rate of the data-independent P-greedy algorithm in kernel-based approximation},
    volume = {10},
    DOI = {10.14658/pupj-drna-2017-Special_Issue-9},
    number = {6},
    journal = {Dolomites Research Notes on Approximation},
    publisher = {Padova University Press},
    author = {Santin, Gabriele and Haasdonk, Bernard},
    year = {2017},
    pages = {68–78}
}

@article{bohn2019representer,
    author = {Bohn, Bastian and Rieger, Christian and Griebel, Michael},
    title = {A Representer Theorem for Deep Kernel Learning},
    year = {2019},
    publisher = {JMLR.org},
    volume = {20},
    number = {1},
    abstract = {In this paper we provide a finite-sample and an infinite-sample representer theorem for the concatenation of (linear combinations of) kernel functions of reproducing kernel Hilbert spaces. These results serve as mathematical foundation for the analysis of machine learning algorithms based on compositions of functions. As a direct consequence in the finite-sample case, the corresponding infinite-dimensional minimization problems can be recast into (nonlinear) finite-dimensional minimization problems, which can be tackled with nonlinear optimization algorithms. Moreover, we show how concatenated machine learning problems can be reformulated as neural networks and how our representer theorem applies to a broad class of state-of-the-art deep learning methods.},
    journal = {Journal of Machine Learning Research},
    pages = {2302–2333},
    numpages = {32},
    keywords = {deep kernel learning, multilayer kernel, regularized least-squares regression, representer theorem, artificial neural networks},
    doi = {https://dl.acm.org/doi/10.5555/3322706.3362005},
}

@article{harris2020array,
    title = {Array programming with {NumPy}},
    author = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
    year = {2020},
    journal = {Nature},
    volume = {585},
    number = {7825},
    pages = {357--362},
    doi = {10.1038/s41586-020-2649-2},
    publisher = {Springer Science and Business Media {LLC}},
}

@article{virtanen2020SciPy,
    author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
    title = {{{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}},
    journal = {Nature Methods},
    year = {2020},
    volume = {17},
    pages = {261--272},
    doi = {10.1038/s41592-019-0686-2},
}

@article{milk2016pyMOR,
    author    = {Milk, Ren\'{e} and Rave, Stephan and Schindler, Felix},
    title     = {{pyMOR} -- Generic Algorithms and Interfaces for Model Order Reduction},
    journal   = {SIAM Journal of Scientific Computing},
    year      = {2016},
    volume    = {38},
    number    = {5},
    pages     = {S194--S216},
    doi       = {10.1137/15m1026614},
    groups    = {RB, software},
    keywords  = {model order reduction, reduced basis method, empirical interpolation, scientific computing, software, Python},
    publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@incollection{paszke2019PyTorch,
    author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    booktitle = {Advances in Neural Information Processing Systems 32},
    publisher = {Curran Associates, Inc.},
    year      = {2019},
    editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d' Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages     = {8024--8035},
    doi = {https://dl.acm.org/doi/10.5555/3454287.3455008},
}

@article{petersen2018optimal,
    author   = {Philipp Petersen and Felix Voigtlaender},
    title    = {Optimal approximation of piecewise smooth functions using deep {ReLU} neural networks},
    journal  = {Neural Networks},
    year     = {2018},
    volume   = {108},
    pages    = {296 - 330},
    abstract = {We study the necessary and sufficient complexity of ReLU neural networks – in terms of depth and number of weights – which is required for approximating classifier functions in an Lp-sense. As a model class, we consider the set Eβ(Rd) of possibly discontinuous piecewise Cβ functions f:[−12,12]d→R, where the different “smooth regions” of f are separated by Cβ hypersurfaces. For given dimension d≥2, regularity β>0, and accuracy ε>0, we construct artificial neural networks with ReLU activation function that approximate functions from Eβ(Rd) up to an L2 error of ε. The constructed networks have a fixed number of layers, depending only on d and β, and they have O(ε−2(d−1)∕β) many nonzero weights, which we prove to be optimal. For the proof of optimality, we establish a lower bound on the description complexity of the class Eβ(Rd). By showing that a family of approximating neural networks gives rise to an encoder for Eβ(Rd), we then prove that one cannot approximate a general function f∈Eβ(Rd) using neural networks that are less complex than those produced by our construction. In addition to the optimality in terms of the number of weights, we show that in order to achieve this optimal approximation rate, one needs ReLU networks of a certain minimal depth. Precisely, for piecewise Cβ(Rd) functions, this minimal depth is given – up to a multiplicative constant – by β∕d. Up to a log factor, our constructed networks match this bound. This partly explains the benefits of depth for ReLU networks by showing that deep networks are necessary to achieve efficient approximation of (piecewise) smooth functions. Finally, we analyze approximation in high-dimensional spaces where the function f to be approximated can be factorized into a smooth dimension reducing feature map τ and classifier function g – defined on a low-dimensional feature space – as f=g∘τ. We show that in this case the approximation rate depends only on the dimension of the feature space and not the input dimension.},
    doi      = {10.1016/j.neunet.2018.08.019},
    keywords = {Deep neural networks, Piecewise smooth functions, Function approximation, Sparse connectivity, Metric entropy, Curse of dimension},
}

@article{rumelhart1986learning,
    author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    title     = {{Learning representations by back-propagating errors}},
    journal   = {Nature},
    year      = {1986},
    volume    = {323},
    number    = {6088},
    pages     = {533--536},
    doi       = {10.1038/323533a0},
    abstract  = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
    keywords  = {algorithm learning modeling neuralnetworks},
}

@article{liu1989limited,
    author  = {Dong C. Liu and Jorge Nocedal},
    title   = {On the Limited Memory {BFGS} Method for Large Scale Optimization},
    journal = {Mathematical Programming},
    year    = {1989},
    volume  = {45},
    pages   = {503--528},
    doi     = {10.1007/bf01589116},
}

@inproceedings{prechelt1997early,
    author    = {Lutz Prechelt},
    title     = {Early Stopping - but when?},
    booktitle = {Neural Networks: Tricks of the Trade, volume 1524 of LNCS, chapter 2},
    year      = {1997},
    pages     = {55--69},
    publisher = {Springer-Verlag},
    doi       = {10.1007/978-3-642-35289-8_5},
}

@article{scikit-learn,
    title={Scikit-learn: Machine Learning in {P}ython},
    author={Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011},
    doi={https://dl.acm.org/doi/10.5555/1953048.2078195},
}

@article {guo2018reduced,
    AUTHOR = {Guo, Mengwu and Hesthaven, Jan S.},
    TITLE = {Reduced order modeling for nonlinear structural analysis using {G}aussian process regression},
    JOURNAL = {Computer Methods in Applied Mechanics and Engineering},
    VOLUME = {341},
    YEAR = {2018},
    PAGES = {807--826},
    MRCLASS = {65N30 (65N99)},
    MRNUMBER = {3845646},
    DOI = {10.1016/j.cma.2018.07.017},
}

@article{lecun2015deep,
    author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    title={Deep learning},
    journal={Nature},
    year={2015},
    day={01},
    volume={521},
    number={7553},
    pages={436-444},
    abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
    doi={10.1038/nature14539},
}

@article{crank1947practical,
    title={A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type},
    volume={43},
    DOI={10.1017/S0305004100023197},
    number={1},
    journal={Mathematical Proceedings of the Cambridge Philosophical Society},
    publisher={Cambridge University Press},
    author={Crank, John and Nicolson, Phyllis},
    year={1947},
    pages={50–67}
}

@book{golub1996matrix,
    title = {Matrix Computations},
    author = {Golub, Gene H. and Van Loan, Charles F.},
    publisher = {The Johns Hopkins University Press},
    year = {1996},
    edition = {Third Edition},
    doi = {https://dl.acm.org/doi/10.5555/248979},
    isbn = {0801854148},
}

@inbook{graessle2021model,
    title = {Model order reduction by proper orthogonal decomposition},
    booktitle = {Volume 2 Snapshot-Based Methods and Algorithms},
    booktitle = {Volume 2: Snapshot-Based Methods and Algorithms},
    author = {Carmen Gräßle and Michael Hinze and Stefan Volkwein},
    editor = {Peter Benner and Stefano Grivet-Talocia and Alfio Quarteroni and Gianluigi Rozza and Wil Schilders and Luís Miguel Silveira},
    publisher = {De Gruyter},
    address = {Berlin, Boston},
    pages = {47--96},
    doi = {10.1515/9783110671490-002},
    isbn = {9783110671490},
    year = {2021},
}

@book{rasmussen2006gaussian,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    isbn = {026218253X},
    keywords = {dblp},
    pages = {I-XVIII, 1-248},
    publisher = {MIT Press},
    series = {Adaptive computation and machine learning},
    title = {Gaussian processes for machine learning},
    year = {2006},
    doi = {10.7551/mitpress/3206.001.0001},
}

@inproceedings{mayer2019stochastic,
    author={Mayer, Jana and Dolgov, Maxim and Stickling, Tobias and Özgen, Selim and Rosenthal, Florian and Hanebeck, Uwe D.},
    booktitle={2019 American Control Conference (ACC)},
    title={Stochastic Optimal Control Using Gaussian Process Regression over Probability Distributions},
    year={2019},
    volume={},
    number={},
    pages={4847-4853},
    doi={10.23919/ACC.2019.8814658}
}

@article{benner2015survey,
    doi = {10.1137/130932715},
    year = {2015},
    publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
    volume = {57},
    number = {4},
    pages = {483--531},
    author = {Peter Benner and Serkan Gugercin and Karen Willcox},
    title = {A Survey of Projection-Based Model Reduction Methods for Parametric Dynamical Systems},
    journal = {{SIAM} Review}
}

@article{kanagawa2018gaussian,
    title={Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences}, 
    author={Motonobu Kanagawa and Philipp Hennig and Dino Sejdinovic and Bharath K Sriperumbudur},
    year={2018},
    eprint={1807.02582},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{haasdonk2011efficient,
    doi = {10.1080/13873954.2010.514703},
    year = {2011},
    publisher = {Informa {UK} Limited},
    volume = {17},
    number = {2},
    pages = {145--161},
    author = {Bernard Haasdonk and Mario Ohlberger},
    title = {Efficient reduced models and a posteriori error estimation for parametrized dynamical systems by offline/online decomposition},
    journal = {Mathematical and Computer Modelling of Dynamical Systems}
}

@article{aronszajn1950theory,
    doi = {10.1090/s0002-9947-1950-0051437-7},
    year = {1950},
    publisher = {American Mathematical Society ({AMS})},
    volume = {68},
    number = {3},
    pages = {337--404},
    author = {Nachman Aronszajn},
    title = {Theory of reproducing kernels},
    journal = {Transactions of the American Mathematical Society}
}

@book{hale1980ordinary,
    year = {1980},
    author = {Jack K. Hale},
    title = {Ordinary Differential Equations},
    edition = {2},
    publisher = {Robert E. Krieger Publishing Company},
    isbn = {0-89874-011-8}
}

@article{greif2019decay,
	title = {Decay of the {Kolmogorov} {N}-width for wave problems},
	journal = {Applied Mathematics Letters},
	volume = {96},
	pages = {216-222},
	year = {2019},
	issn = {0893-9659},
	doi = {10.1016/j.aml.2019.05.013},
	author = {Constantin Greif and Karsten Urban},
	keywords = {Kolmogorov-width, Wave equation},
	abstract = {The Kolmogorov N-width dN(M) describes the rate of the worst-case error (w.r.t. a subset M⊂H of a normed space H) arising from a projection onto the best-possible linear subspace of H of dimension N∈N. Thus, dN(M) sets a limit to any projection-based approximation such as determined by the reduced basis method. While it is known that dN(M) decays exponentially fast for many linear coercive parameterized partial differential equations, i.e., dN(M)=O(e−βN), we show in this note, that only dN(M)=O(N−1∕2) for initial–boundary-value problems of the hyperbolic wave equation with discontinuous initial conditions. This is aligned with the known slow decay of dN(M) for the linear transport problem.}
}

@misc{sourcecode,
    doi = {10.5281/ZENODO.8188417},
    url = {https://zenodo.org/record/8188417},
    author = {Kleikamp, Hendrik and Lazar, Martin and Molinari, Cesare},
    title = {Software for ``Be greedy and learn: efficient and certified algorithms for parametrized optimal control problems''},
    publisher = {Zenodo},
    year = {2023},
    copyright = {Open Access}
}

@Article{keil2022adaptive,
    author={Keil, Tim and Kleikamp, Hendrik and Lorentzen, Rolf J. and Oguntola, Micheal B. and Ohlberger, Mario},
    title={Adaptive machine learning-based surrogate modeling to accelerate PDE-constrained optimization in enhanced oil recovery},
    journal={Advances in Computational Mathematics},
    year={2022},
    volume={48},
    number={6},
    pages={73},
    abstract={In this contribution, we develop an efficient surrogate modeling framework for simulation-based optimization of enhanced oil recovery, where we particularly focus on polymer flooding. The computational approach is based on an adaptive training procedure of a neural network that directly approximates an input-output map of the underlying PDE-constrained optimization problem. The training process thereby focuses on the construction of an accurate surrogate model solely related to the optimization path of an outer iterative optimization loop. True evaluations of the objective function are used to finally obtain certified results. Numerical experiments are given to evaluate the accuracy and efficiency of the approach for a heterogeneous five-spot benchmark problem.},
    doi={10.1007/s10444-022-09981-z},
}