\section{Results \& Analysis} 
\label{sec:eval}
This section presents the user study results.
Unless otherwise indicated, results are based on the full set of participants.


\subsection{Solving times}
\label{sec:eval_solving_time}
This subsection addresses \textbf{RQ1:} \emph{How long do human users take to solve different types of \captchas?}
Figure~\ref{fig:captcha_solving_time} shows the the distribution of solving times for each \captcha type.
We observed a small number of extreme outliers where the participant likely switched to another task before returning to the study.
We therefore filtered out the highest $50$ solving times per \captcha type, out of $1,000$ total. %

For reCAPTCHA, the selection between image- or click-based tasks is made dynamically by Google.
Whilst we know that $85\%$ and $71\%$ of participants (easy and hard setting) were shown a click-based \captcha, the exact task-to-participant mapping is not revealed to website operators.
We therefore assume that the slowest solving times correspond to image-based tasks.
\todo{This seems to be entirely conjecture. Can you clarify this process. A comment on HotCRP is fine.}
After disambiguation, click-based reCAPTCHA had the lowest median solving time at $3.7$ seconds.
Curiously, there was little difference between easy and difficult settings.

The next lowest median solving times were for distorted text \captchas.
As expected, simple distorted text \captchas were solved the fastest.
Masked and moving versions had very similar solving times.
For hCAPTCHA, there is a clear distinction between easy and difficult settings.
The latter consistently served either a harder image-based task or increased the number of rounds.
However, for both hCAPTCHA settings, the fastest solving times are similar to those of reCAPTCHA and distorted text.
Finally, the game-based and slider-based \captchas generally yielded higher median solving times, though some participants still solved these relatively quickly (e.g., $< 10$~seconds).

With the exception of reCAPTCHA (click) and distorted text, we observed that solving times for other types have a relatively high variance. 
Some variance is expected, especially since these results encompass all input modalities across both direct and contextualized settings. 
However, \emph{relative differences in variances} indicate that, while some types of \captchas are consistently solved quickly, most have a range of solving times across the user population.
The full statistical analysis of our solving time results is presented in Appendix~\ref{sec:eval_statistical}.

\todo{I think a violin plot would be preferable to the box plots.}

% Figure environment removed

% Figure environment removed

\subsection{Preferences analysis}
\label{sec:eval_preferences}
This subsection addresses \textbf{RQ2:} \emph{What \Captcha types do users prefer?}
Figure~\ref{fig:pref_res} shows participants' \captcha preference responses after completing the solving tasks.
The \captcha types are sorted from most to least preferred by overall preference score, which is calculated by summing the numeric scores.
Since easy and difficult settings of hCAPTCHA are visually indistinguishable, we could only ask participants for one preference.

As expected, participants tend to prefer \captchas with lower solving times.
For example, reCAPTCHA (click) has the lowest median solving time and the highest user preference.
However, surprisingly, this trend does not seem to hold for game-based and slider-based \captchas, since these received some of the highest preference scores, even though they typically took longer than other types.
\todo{If there is room, it would be interesting to see a linear regression of these items per captcha.}
This suggests that factors beyond solving time could be contributing to participants' preference scores.
Notably, no single \captcha type is either universally liked or disliked.
For example, even the top-rated click-based reCAPTCHA, was rated 1 or 2 by $18.9\%$ of participants.
Similarly, over $31.0\%$ rated hCAPTCHA 4 or 5, although it had the lowest overall preference score.



% Figure environment removed



\subsection{Direct vs.\ contextualized setting}
\label{sec:eval_context}
This subsection addresses \textbf{RQ3:} \emph{Does experimental context affect solving time?}
Figure~\ref{fig:b_v_ub} shows histograms of \captcha solving times for participants in the direct vs.\ contextualized settings.
In every case except one, the mean solving time is lower in the direct setting.
In most cases, the distribution from the contextualized setting has more participants with longer solving times, i.e., a longer tail.

The largest statistically significant difference is in reCAPTCHA (easy click), where the mean solving time grows by $1.8$ seconds ($57.5\%$).
Second is Arkose (rotation), where it grows by $10$ seconds ($56.1\%$).
Across all \captcha types, the average increase from direct to contextualized is $26.7\%$.
Similarly, the mean solving time for reCAPTCHA (easy image) increased by $63.6\%$ in the contextualized setting showing the largest increase. 
However this was not statistically significant.
This is likely due to the skew of participants in direct and contextualized versions receiving image-challenges, which is controlled by Google.
Easy images were shown to 8.9\% of contextualized and to 17.2\% of direct setting participants, while hard images were shown to $25.5\%$ and $30\%$ respectively, resulting in different population sizes.

On the other hand, hCAPTCHA (difficult), which has the highest median solving time overall, showed no significant difference in mean solving time between direct and contextualized settings.
This may be attributable to the difficulty of solving this type of \captcha, regardless of the setting.

Results of Kruskal-Wallis tests confirm that there are statistically significant differences in mean solving times for all \captcha types ($p < 0.001$) except Geetest, reCAPTCHA (image) and hCAPTCHA (difficult).
While there were several potential confounding factors in our study, these results suggest that experimental context can have a significant impact on participants' \captcha solving times, and must therefore be taken into account in the design of future user studies.

\subsection{Effects of demographics}
\label{sec:eval_dem}
This subsection addresses \textbf{RQ4:} \emph{Do demographics affect solving time?}
We analyzed how demographic characteristics in our study correlate with \captcha solving times.
For some characteristics, such as education and gender, we did not observe large differences in \captcha solving times (see Figures~\ref{fig:gender_vs_time} and~\ref{fig:education_vs_time} in Appendix~\ref{sec:dem_appendix}).


\subsubsection{Effects of age}
\label{sec:eval_age}

Figure~\ref{fig:age_vs_time} shows the effect of participants' age on solving time.
The green line is the average solving time for each age, and the red line is a linear fit minimizing mean square error.
For all types, except reCAPTCHA (easy image), there is a trend of younger participants having lower average solving times.
This agrees with prior results~\cite{Bursztein} and is especially noticeable in hCAPTCHA, Arkose (selection), and Geetest.

\subsubsection{Effects of device type}
\label{sec:eval_device_type}
% Figure environment removed


Figure~\ref{fig:device_type_vs_time} shows the effect of device type.
Although there are some differences in median between device types for certain \captcha types, the Kruskal-Wallis test shows that the differences in means are mostly not statistically significant.
The only statistically significant differences are in distorted text \captchas ($p < 0.02$) and reCAPTCHA (hard click) ($p < 0.01$), where participants who used computers had a lower mean solving time compared to those using phones.
Interestingly, we found a statistically significant difference between participants who used physical keyboards and those who used touch input for the simple and masked distorted text \captchas ($p < 0.02$), as well as reCAPTCHA (hard click) ($p < .001$), reCAPTCHA (easy click) ($p < .05$), and Arkose (selection) ($p < .003$).
We found no statistically significant difference in mean solving times for moving distorted text. %

\subsubsection{Effects of typical Internet use}
% Figure environment removed

Figure~\ref{fig:internet_use_vs_time} shows the relationship between participants' self-reported dominant Internet usage patterns and their \captcha solving times. 
The Kruskal-Wallis test shows some initial evidence for statistically significant differences between participants who use the Internet primarily for work and those who use it for other purposes ($p < 0.05$).
The former were typically slower than the latter in 8 out of 12 \captchas. %
However, some categories do not have a sufficient number of participants, thus further investigation is recommended.


\subsection{Accuracy of \Captchas}
\label{sec:eval_accuracy}
Table~\ref{tab:bots_vs_humans} contrasts our measured human solving times and accuracy against those of automated bots reported in the literature.
Interestingly, these results suggest that bots \emph{can} outperform humans, both in terms of solving time and accuracy, across all these \captcha types.
As mentioned in Section~\ref{sec:study_limitations}, our decision to use unmodified real-world \captchas means we only have accuracy results for a subset of \captcha types (e.g., neither Geetest nor Arkose provide accuracy information).
For the same reason, our accuracy results also include participants who only partially completed the study. 


\taggedpara{reCAPTCHA:} The accuracy of image classification was 81\% and 81.7\% on the easy and hard settings respectively.
Surprisingly, the difficulty appeared not to impact accuracy.

\taggedpara{hCAPTCHA:} The accuracy was 81.4\% and 70.6\% on the easy and hard settings respectively.
This shows that, unlike reCAPTCHA, the difficulty has a direct impact on accuracy.

\taggedpara{Distorted Text:} We evaluated \emph{agreement} among participants as a proxy for accuracy.
As each individual \captcha was served to three separate participants, we measured agreement between any two or more participants.
We also observed that agreement increases dramatically (20\% on average) if responses are treated as case insensitive, as shown in Table~\ref{tab:accuracy}.


\begin{table}[h!]
\centering
\footnotesize
\caption{Humans vs.\ bot solving time (seconds) and accuracy (percentage) for different \captcha types.}
\label{tab:bots_vs_humans}
\begin{tabularx}{\columnwidth}{l l l l l}
\toprule
      &       \multicolumn{2}{c}{\textbf{Human}}                    & \multicolumn{2}{c}{\textbf{Bot}}  \\\cmidrule(lr){2-3}\cmidrule(lr){4-5}
\textbf{\Captcha Type}      &  Time                  &   Accuracy       &  Time                  &  Accuracy        \\ \midrule
reCAPTCHA (click) &   3.1-4.9                             &  71-85\%               & 1.4 \cite{Sivakorn2016}      & 100\% \cite{Sivakorn2016}              \\ \midrule
Geetest           &  28-30                            &  N/A                   & 5.3~\cite{Haiqin2019}            & 96\%  ~\cite{Haiqin2019}               \\ \midrule
Arkose            &  18-42                            &  N/A                   & N/A                              & N/A                                    \\ \midrule
Distorted Text    &  9-15.3                            &  50-84\%               & \textless{}1 ~\cite{Zi20}       & 99.8\% \cite{Goodfellow}              \\ \midrule
reCAPTCHA (image) &  15-26                              &  81\%                  & 17.5  \cite{Hossen2020}        & 85\% \cite{Hossen2020}                 \\ \midrule
hCAPTCHA          &  18-32                            &  71-81\%               & 14.9 \cite{Hossen2021}           & 98\% \cite{Hossen2021}                 \\ \bottomrule
\end{tabularx}

\vspace{\floatsep}
\caption{Agreement for distorted text \captchas.}
\label{tab:accuracy}
\begin{tabularx}{\columnwidth}{X c c}
\toprule
      & Average Agreement & Average Agreement (case insensitive) \\
	   \midrule
Simple & 84\%             & 93\%                                                                      \\
Masked & 50\%             & 73\%                                                                      \\
Moving & 62\%             & 90\%                                                                      \\
\midrule
Total  & 65\%             & 85\%                                                                      \\
\bottomrule
\end{tabularx}
\end{table}


\section{Measuring User Abandonment}
\label{sec:eval_abandonment}
This subsection addresses \textbf{RQ5:} \emph{Does experimental context influence abandonment?}
Upon completion, we observed that the number of \captchas solved during our study exceeded what would be expected based on the number of participants who completed the study.
We hypothesized that this was due to participants starting but not completing the study.
To measure this behavior, we conducted a second user study that collected timestamps between \captchas, regardless of whether the entire study was completed.
We measured: (1) how many participants started the task; (2) how many abandoned the task when solving a \captcha; and (3) if so, at which task and \captcha.

This abandonment-focused study consisted of four groups, each with $100$ unique participants.
Two groups were presented with the  direct setting and the other two with the contextualized setting (see Section~\ref{sec:B_vs_UB_SD}).
We hypothesized that the amount of compensation might also impact abandonment, so we doubled the compensation for one of the groups in each setting.
The studies were run sequentially to avoid prospective participants simply picking the higher-paying study.

We summarize the key findings below, and present the full results in Tables~\ref{tab:unbiased_75},~\ref{tab:unbiased_150},~\ref{tab:biased_30}, and~\ref{tab:biased_60} in Appendix~\ref{sec:appendix-abandonment}.
Out of a total of $574$ participants who started the study, $174$ abandoned prior to completion (i.e., $30\%$ abandonment rate).
Several observations can be made:
First, in the direct setting, $25\%$ of the participants who ultimately abandoned the study did so before solving the first \captcha, but this rose to nearly $50\%$ in the contextualized setting.
Second, doubling the pay halved the abandonment rate for the contextualized setting (as expected), but increased it by $50\%$ in the direct setting.
Third, participants in the contextualized setting were $120\%$ more likely to abandon than those in the direct setting.
Fourth, in the contextualized setting, participants at the higher compensation level solved \captchas faster than those at the lower compensation level ($21.5\%$ decrease in average solving time across all \captcha types).
Interestingly, in the direct setting, participants at the higher compensation level solved \captchas \emph{slower} than those at the lower compensation level ($27.4\%$ \emph{increase} in average solving time across all \captcha types).
Finally, some \captcha types (e.g., Geetest) exhibited higher rates of abandonment than others.

This initial investigation strongly motivates the need for further exploration of \captcha-induced abandonment.
Although we studied the impact of compensation and experimental context, there may be other reasons behind abandonment, such as: \captcha type, \captcha difficulty, and expected duration of study.
Nevertheless, the trend of average users' unwillingness to solve a \captcha during account creation (even for monetary compensation) is a relevant finding for websites that choose to protect account creation (and/or account access) using \captchas.
