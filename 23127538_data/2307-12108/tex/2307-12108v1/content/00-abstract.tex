For nearly two decades, \captchas have been widely used as a means of protection against bots. 
Throughout the years, as their use grew, techniques to defeat or bypass \captchas have continued to improve.
Meanwhile, \captchas have also evolved in terms of sophistication and diversity, becoming increasingly difficult to solve for both bots (machines) and humans. 
Given this long-standing and still-ongoing arms race, it is critical to investigate how long it takes legitimate users to solve modern \captchas, and how they are perceived by those users.

In this work, we explore \captchas \emph{in the wild} by evaluating users' solving performance and perceptions of \emph{unmodified currently-deployed} \captchas.
We obtain this data through manual inspection of popular websites and user studies in which $1,400$ participants collectively solved $14,000$ \captchas.
Results show significant differences between the most popular types of \captchas: surprisingly, solving time and user perception are not always correlated.
We performed a comparative study to investigate the effect of \emph{experimental context} -- specifically the difference between solving \captchas directly versus solving them as part of a more natural task, such as account creation.
Whilst there were several potential confounding factors, our results show that experimental context could have an impact on this task, and must be taken into account in future \captcha studies.
Finally, we investigate \captcha-induced user task \emph{abandonment} by analyzing participants who start and do not complete the task.
