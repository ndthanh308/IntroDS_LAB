\section{Introduction}
\label{sec:intro}

Automated bots pose a significant challenge for, and danger to, many website operators and providers.
Masquerading as legitimate human users, these bots are often programmed to scrape content, create accounts, post fake comments or reviews, consume scarce resources, or generally (ab)use other website functionality intended for human use~\cite{imperva2022, forrester2021}.
If left unchecked, bots can perform these nefarious actions at scale.
\Captchas are a widely-deployed defense mechanism that aims to prevent bots from interacting with websites by forcing each user to perform a task, such as solving a challenge~\cite{cap_usage}.
Ideally, the task should be straightforward for humans, yet difficult for machines~\cite{vonAhn}.

The earliest \captchas asked users to transcribe random distorted text from an image.
However, advances in computer vision and machine learning have dramatically increased the ability of bots to recognize distorted text~\cite{yan2008low,gao2012divide,hernandez2010pitfalls}, and by 2014, automated tools achieved over 99\% accuracy~\cite{Goodfellow,Shet}.
Alternatively, bots often outsource solving to \emph{\captcha farms} -- sweatshop-like operations where humans are paid to solve \captchas~\cite{Motoyama}.
In light of this, \captchas have changed and evolved significantly over the years. 
Popular \captcha tasks currently include object recognition (e.g., ``select squares with...''), parsing distorted text, puzzle solving (e.g., ``slide the block...''), and user behavior analysis~\cite{Goodfellow,Shet}. 
It is therefore critical to understand and quantify how long it takes legitimate users to solve current \captchas, and how these \captchas are perceived by users.

Several prior research efforts have explored \captcha solving times, e.g.,~\cite{Bursztein, Bigham, Gao, Ross, Uzun, senCAP}.
For example, over a decade ago, Bursztein et al.~\cite{Bursztein} performed a large-scale user study, using over $1,100$ unique participants from Amazon Mechanical Turk (MTurk)~\cite{mturk} as well as \captcha farms.
Their results showed that \captchas were often more difficult or took longer to solve than was expected.
There was a loose correlation between time-to-annoyance and abandonment, with higher abandonment rates observed for \captchas that took longer to solve.
The same study also showed several demographic trends, e.g., users outside the US typically took longer to solve English-language \captcha schemes.
However, since this study, the \captcha ecosystem has changed substantially: new \captcha types emerged, input methods evolved, and Web use boomed.

More recently, Feng et al.~\cite{senCAP} used a similar methodology, with $202$ participants, to study the usability of their newly-proposed sen\captcha in comparison to text, audio, image, and video-based \captchas.
They found that sen\captcha outperformed the alternatives, both in terms of solving time and user preference.
They used Securimage~\cite{securimage}, a free open-source PHP script, to generate text and audio \captchas, and they implemented their own image and video \captchas.


Building upon and complementing prior work, this paper evaluates \captchas \emph{in the wild} -- specifically, the solving times and user perceptions of \emph{unmodified} (i.e., not re-implemented) \emph{currently-deployed} \captcha types.
We first performed a manual inspection of 200 popular websites, based on the Alexa Top websites list~\cite{alexa}, to ascertain: (1) \emph{how many} websites use \captchas, and (2) \emph{what types} of \captchas they use. 
Next, we conducted a $1,000$-participant user study using Amazon MTurk, wherein each participant was required to solve $10$ different types of \captchas.
We collected information about participants' \captcha solving times, relative preferences for \captcha types, types of devices used, and various demographic information.


One notable aspect of our user study is that we attempted to measure the impact of experimental context on participants' \captcha solving times.
Half of the participants were directly asked to solve \captchas, whilst the other half were asked to create accounts, which involved solving \captchas as part of the task.
The latter setting was designed to measure \captcha solving times \emph{in the context} of a typical web activity.  

One inherent limitation of any user study, especially when using MTurk, is that we cannot ensure that all participants who begin the study will complete it.
All of our results should therefore be interpreted as referring to \emph{users who are willing to solve \captchas}, rather than users in general.

Indeed, having noted that some participants began but did not complete our main study, we conducted a secondary MTurk study specifically designed to quantify how many users abandon their intended web activity when confronted with different types of \captchas. 
We believe that \captcha-induced \emph{user abandonment} is an important -- yet understudied -- consideration, since every abandoned task (e.g., purchase, account creation) represents a potential loss for the website.

To facilitate reproducibility and enable further analysis, we provide the entire anonymized data-set collected during our user studies, along with our analysis code.\footnote{\url{https://github.com/sprout-uci/captcha-study}}



