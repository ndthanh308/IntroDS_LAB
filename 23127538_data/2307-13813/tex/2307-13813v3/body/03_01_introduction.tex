\section{Introduction}
\label{sec:introduction}

With data and models becoming progressively larger
\citep{DBLP:conf/nips/ChenKSNH20,DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2108-07258,DBLP:journals/corr/abs-2206-04615}, 
the ability to reduce training wall--clock time is a requirement for practical \gls{ml} at scale.
Optimizer scaling rules allow us to find faster learning procedures that produce similar results.
For example, the \emph{linear scaling rule} for \gls{sgd}
\citep{DBLP:journals/corr/Krizhevsky14,DBLP:journals/corr/GoyalDGNWKTJH17},
states that the learning rate should be scaled linearly with the batch size.
This optimizer scaling works \emph{both ways}.
Access to larger computational resources means one can train equivalent models in reduced wall-clock time.
Alternatively, with access to limited computational resources, larger distributed computations can be replicated at increased wall-clock time.

Many \gls{ml} algorithms rely on a \emph{model EMA},
a functional copy of a \emph{target model}\footnote{The target model usually undergoes gradient-based optimization, but this does not have to be the case.}, whose 
parameters move towards those of its target model according to an \gls{ema} (\Cref{def:emaUpdateDefinition}) at a rate parameterized by a momentum hyperparameter $\rho$.  
\begin{restatable}[EMA Update]{definition}{firstema}
\label{def:emaUpdateDefinition}
     The \gls{ema} update for the model \gls{ema} parameters $\rvzeta_t$ following target model parameters  $\rvtheta_t$ at iteration $t$ with momentum $\rho\equiv1-\beta_\rho$ is
    \label{def:ema}
    \begin{equation}
    \rvzeta_{t+1}
    =
    \rho \,\rvzeta_t + (1-\rho)\,\rvtheta_t
    \equiv
    (1-\beta_\rho) \,\rvzeta_t +\beta_\rho\,\rvtheta_t.
    \end{equation}
\end{restatable}
The \emph{model \gls{ema}} has a number of desirable properties:
i) 
the model \gls{ema} inhabits wider minima than the target model, reducing overfitting and improving generalization
\citep{Ruppert1988EfficientEF,Polyak92,DBLP:conf/iclr/HuangLP0HW17,DBLP:conf/uai/IzmailovPGVW18,DBLP:conf/cvpr/HeCXLDG22};
ii)
compared to the target model, the model \gls{ema} moves slowly, 
making it useful as a stabilizer for networks governing Bellman updates in 
reinforcement learning,
\citep{DBLP:journals/corr/LillicrapHPHETS15};
and iii)
the model \gls{ema} is relatively cheap to compute, whilst providing a valid model but \emph{different} to the target model.
This third property has made the model \gls{ema} a common choice for the \emph{teacher} in many distillation setups, 
from semi-supervised learning \citep{DBLP:conf/nips/TarvainenV17,sohn2020fixmatch,manohar2021kaizen,higuchi2022momentum},
to \gls{ssl} methods like 
\gls{byol} \citep{DBLP:conf/nips/GrillSATRBDPGAP20},
DINO \citep{DBLP:journals/corr/abs-2104-14294},
and data2vec \citep{baevski2022data2vec,DBLP:journals/corr/abs-2212-07525}.

Despite its significant role in optimization, a recipe for adapting the \gls{ema} Update (\Cref{def:emaUpdateDefinition}) when changing batch size has,
to the best of our knowledge, been absent.
To address this, we derive an \gls{ema} Scaling Rule (\Cref{def:ema-sr})
which states how the \gls{ema} momentum $\rho$ hyperparameter \emph{should} be modified\footnote{We stress that the study of momentum in gradient-based optimizers is not the focus of this work.
We refer to \citet{DBLP:conf/iclr/SmithL18,li2019stochastic} for a discussion on scaling rules for these methods.
}.
\begin{restatable}[\gls{ema} Scaling Rule]{definition}{firstemascaling}
\label{def:emaScalingRuleExponentialVersion}
    When computing the \gls{ema} update (\Cref{def:ema}) of a model undergoing stochastic optimization with batch size $\hat B=\kappa B$,
    use a momentum $\hat\rho=\rho^\kappa$ and scale other optimizers according to their own scaling rules.
    \label{def:ema-sr}
\end{restatable}
In \Cref{def:ema-sr}, the momentum $\rho$, which is defined at batch size $B$, typically corresponds to a ``good hyperparameter choice'', although this does not need to be the case in general.
In this paper, we make the following contributions.
\begin{enumerate}[leftmargin=0.75cm]
    \item With the assumptions of \citet{DBLP:journals/corr/GoyalDGNWKTJH17}, we derive an \gls{ema} Scaling Rule: the \gls{ema} update \emph{momentum} should be scaled \emph{exponentially} with the batch size (\Cref{def:ema-sr}).
    \item To validate this EMA Scaling Rule theoretically, we propose \gls{sde} approximations of optimization in the presence of a model \gls{ema} (\Cref{subsec:ema-sdes}). 
    This model \gls{ema} contributes to the loss, covering semi-supervised learning and \gls{ssl}.
    We prove that these approximations are first order weak approximations, and that our \gls{ema} Scaling Rule is correct in the \gls{sde} limit under realistic gradient assumptions (\Cref{cor:validity-scaling-rule}).
    \item We empirically validate the \gls{ema} Scaling Rule in synthetic settings (\Cref{subsec:toy-experiment})
    and real-world settings where the model \gls{ema} plays an increasingly significant role in optimization: 
    i) 
    where the model \gls{ema} is used during inference instead of the target model (\Cref{subsec:supervised-polyakking}); 
    ii) 
    pseudo-labeling, 
    where the model \gls{ema} (\emph{teacher}) follows the target model (\emph{student}), and the \emph{student} is optimized 
    on a mixture of a) labeled data and b) data without labels, whose pseudo-labels are produced by the \emph{teacher} (\Cref{subsec:semi-supervised}); 
    and iii)
    self-supervised learning, which is the same as the semi-supervised case, except there is no labeled data (\Cref{subsec:self-supervised}).
    \item We observe that pseudo-labeling and \gls{ssl} training dynamics during optimizer warm-up are not always able to be replicated at large batch sizes using \emph{only} the \gls{ema} Scaling Rule.
    We propose and verify practical methods to overcome this limitation, enabling us to scale to a batch size of 24,576 with BYOL \glspl{vit}, reducing wall-clock training by 6$\times$ under idealized hardware scenarios while maintaining performance of the batch size 4096 baseline.
\end{enumerate}
Finally, to aid practitioners looking to scale, in \Cref{app:scaling-toolbox} we provide a \emph{Scaling Toolbox}, which gives practical advice on how to scale systematically, collecting known scaling rules, and explaining how to think about the \gls{sde} perspective of optimization.
