
\FloatBarrier

\subsection{ResNet-18 hyperparameter sensitivity analysis}
\label{subsec:byol-sensitivity-analysis}

To demonstrate that the \gls{ema} Scaling Rule works for a broad range of optimization hyperparameters (i.e. \emph{beyond} those presented in 
\Cref{fig:r18-byol} and \Cref{subsec:self-supervised}), we provide a sensitivity analysis for base teacher momentum $\rho_B$ and base learning rate $\eta_B$ in the challenging setting of \gls{byol}.

\paragraph{Base teacher momentum}

In \Cref{fig:momentum-ablation} we show the effect of changing the base teacher momentum $\rho_B$, defined at batch size 1024.
The \gls{ema} Scaling Rule is robust to modifications of momentum down to $\rho_B\approx 0.946$ in this particular setting.
Below $\rho_B\approx 0.946$, matching is poor, 
although the smallest momentum in this setting corresponds to $0.841^4\approx 0.5$, which is a particularly small teacher momentum, and is unlike to provide utility over the using the target model (see \Cref{app:asymptoticAnalysis}).

\FloatBarrier

% Figure environment removed

\newpage
\paragraph{Base learning rate}
In \Cref{fig:lr-ablation} we show the effect of changing the base learning rate $\eta_B$, defined at batch size 1024.
The \gls{ema} Scaling Rule is robust over a wide range of learning rates.
At the largest learning rate $\eta_B=0.5$ matching starts to become poor at scaling $\kappa=4$.

% Figure environment removed

\FloatBarrier

\subsection{ResNet-18 additional scaling analysis}
\label{subsec:byol-scaling-analysis}
To demonstrate that the \gls{ema} Scaling Rule works for a broad range of scalings $\kappa$ (i.e. \emph{beyond} those presented in 
\Cref{fig:r18-byol} and \Cref{subsec:self-supervised}), we investigate scaling down to $\kappa=1/8$ in \Cref{fig:scaling-ablation-2}.
We see that the \gls{ema} Scaling Rule works well down to the small batch size of 128, although matching is not perfect.
We suspect this is due to the presence of Batch Normalization layers in the ResNet-18 architecture, which underperform at small batch sizes \citep{DBLP:conf/icml/IoffeS15}.
The synthetic analysis of \Cref{subsec:toy-experiment} instead demonstrated the \gls{ema} Scaling Rule holding for scalings spanning factors of $\kappa$ that differ by 1024, with scaling error insensitive to the value of $\kappa$ for sufficiently low $\kappa$ (see \Cref{fig:curve-approximation-error}).

% Figure environment removed

\FloatBarrier

\subsection{Scaling a ResNet-50 BYOL using LARS and Progressive Scaling}
\label{subsec:byol-additional}

Here we investigate whether Progressive Scaling and the \gls{ema} Scaling Rule can be used in practice where there is no known optimizer \gls{sde} approximation.
We use the default 300 epoch configuration for \gls{byol} \citep{DBLP:conf/nips/GrillSATRBDPGAP20} in \Cref{fig:r50-byol}.
We see that although trajectories during training do not match, we are able to match or surpass the linear probe performance of the \gls{byol} baseline at the larger batch size if 32,768.
\emph{This indicates that the contributions of our work have practical utility beyond the theoretical constraints.}


\ifthenelse{\equal{\anonymous}{0}}{
\input{body/05_06_ssl_byol_lars_compute}
}{\compute}


% Figure environment removed
