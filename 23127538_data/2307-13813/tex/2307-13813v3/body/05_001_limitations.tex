\section{Limitations}
\label{app:limitations}

The \gls{ema} Scaling Rule provides a recipe for producing training dynamics independent of the batch size used in stochastic optimization.
The technology underpinning it will not \emph{always} give the desired behavior, however.

The first issue occurs with the wording present in the \gls{ema} Scaling Rule: \emph{[...] and scale other optimizers according to their own scaling rules} (\Cref{def:ema-sr}):
\begin{enumerate}[leftmargin=0.75cm]
    \item This statement requires that the given \gls{sde} approximation we are using for the model optimizer is itself providing well-behaved scaling, that is, that in the \emph{absence} of a model \gls{ema}, the model optimization trajectories at the batch sizes $B$ and $\kappa B$, with optimizer hyperparameters appropriately scaled, are close.
    In general we know this is not true.
    First, we know that the \gls{sde} approximation for \gls{sgd} breaks at a given $\kappa$ due to discretization error \citep{DBLP:conf/nips/LiMA21}.
    Second, we know that if the gradient noise is not sufficiently large, the \gls{sde} approximation for Adam does not exist \citep{DBLP:conf/nips/MalladiLPA22}, i.e. an \gls{sde} motivated scaling rule has no meaning.
    \item This statement requires knowledge of how to scale the corresponding model optimizer.
    We have principled ways to achieve this for \gls{sgd} \citep{DBLP:conf/nips/LiMA21}, and for the adaptive optimization methods RMSProp and Adam \citep{DBLP:conf/nips/MalladiLPA22}.
    Empirically, a square-root scaling law for LAMB \citep{DBLP:conf/iclr/YouLRHKBSDKH20} has been observed, however, it has not been derived formally.
    Problematically, there is no known hyperparameter scaling law or \gls{sde} approximation known for LARS \citep{DBLP:journals/corr/abs-1708-03888}, which has been used in \gls{byol} \citep{DBLP:conf/nips/GrillSATRBDPGAP20} and many other large-scale training procedures for convolution-based architectures. 
    Despite this, we are able to demonstrate in \Cref{subsec:byol-additional} that a combination of the \gls{ema} Scaling Rule and progressive scaling can match, or surpass baseline \gls{byol} performance at a batch size of 32,768 using LARS, indicating that although the theoretical guarantees may not hold, there is still practical utility in the tools we provide in this work.
    \item It may be the case that the optimal performance attainable by a given model setup exists at a level of discretization/gradient noise where no \gls{sde} exists. 
    In this case, \gls{sde}-derived scaling rules can never be valid, and no scaling of this dynamics can be achieved with known tools.
\end{enumerate}
The second issue is related to the case when the optimizer scaling rule is valid.
In this case, the error for the \gls{ema} Scaling Rule at finite learning rate $\eta$ at large $\kappa$ can be considerable.
In cases where the model \gls{ema} plays a role in the overall optimization, the error introduced by the \gls{ema} Scaling Rule can break the preservation of model dynamics.

Put another way, an optimizer scaling rule and the \gls{ema} Scaling Rule each introduce their own discretization errors. 
In the case where \gls{ema} plays a role in optimization, as soon as the discretization error of \emph{either} the optimizer scaling rule \emph{or} the \gls{ema} Scaling Rule is large, the error for the joint optimization procedure is large.
This is \emph{at least} as bad as cases that \emph{do not} use a model \gls{ema} during the optimization process.
