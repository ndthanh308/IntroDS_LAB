\section{Additional details on numerical stability}

A general analysis of overflow and underflow of the \gls{ema} Update (\Cref{def:emaUpdateDefinition}) or \gls{ema} Scaling Rule
(\Cref{def:emaScalingRuleExponentialVersion})
for different momenta $\rho$, particularly for IEE-754 floating point values, is beyond the scope of this work due to non-linearity from mechanisms like gradual underflow \citep{iee}.

In our setting, do not suffer from practical overflow or underflow issues through exponentiation when applying the \gls{ema} Scaling Rule, as \texttt{FP32} precision allows a maximum $\rho=1-\varepsilon$, or minimum $\rho=\varepsilon$ with $\varepsilon\approx 1.2\times 10^{-7}$.
Take self-supervised image representation learning (\Cref{subsec:self-supervised}) as a baseline, with $\kappa=1$ corresponding to batch size $B=4096$ with momentum $\rho_B=0.996$.
The maximum value of $\rho$ corresponds to scaling
$\kappa=\log(\rho_B)/\log(\varepsilon)\approx 1/(32K)$,
give a batch size less than one, while 
the minimum value of $\rho$ corresponds to scaling
$\kappa=\log(\rho_B)/\log(1-\varepsilon)\approx 4K$, giving a batch size $B\approx 8M$ which is beyond current hardware feasibility, and beyond the breakdown of known optimizer scaling rules \citep{DBLP:conf/nips/LiMA21}.

To examine how momentum may induce numerical errors in practice during training, we train a linear regression model with a Polyak-Ruppert average \Cref{def:polyak-ruppert-average}, and
and track the difference between \texttt{FP32} model weights and weights in i) \texttt{BF16}; ii) \texttt{FP16}; and iii) a second \texttt{FP32} run, which act as a proxy for overflow and underflow.
In \Cref{fig:numerics} we plot these differences using the maximum absolute difference between model parameters, where the maximum is taken over individual weights
\begin{equation}
    \text{MaxAbsDiff}(\texttt{dtype})=\max_{i=1}^P\left|\theta_i^{\texttt{FP32}}-\theta_i^{\texttt{dtype}}\right|,
\end{equation}
where $P$ is the number of parameters in the model.
We observe that when model weights and \gls{ema} weights are \texttt{FP16} (never done in practice), an increasing variance happens for \texttt{FP16} as the value of the momentum $\rho$ approaches 0.99999, whereas \texttt{BF16} and \texttt{FP32} are stable. 
We stress that all experiments presented in the paper store weights for target model \emph{and} \gls{ema} in \texttt{FP32} and use automatic-mixed precision to cast them to \texttt{BF16} during training, and so do not encounter momentum-induced overflow or underflow.
% Figure environment removed
