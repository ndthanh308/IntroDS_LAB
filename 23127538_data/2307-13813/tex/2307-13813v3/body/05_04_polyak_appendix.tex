\section{Additional details and results for Polyak-Ruppert averaging}
\label{app:polyak}

\paragraph{Additional background} Polyak-Ruppert averaging (\Cref{def:polyak-ruppert-average}) is a simplification of \gls{swa} \citep{DBLP:conf/uai/IzmailovPGVW18} which uses a more complex multi-cycle schedule based weighting of the model parameters. Both \Cref{def:polyak-ruppert-average} and \gls{swa} present similar favorable properties like wider minima and better generalization \citep{DBLP:conf/uai/IzmailovPGVW18}. 
For example, \citet{DBLP:conf/cvpr/HeCXLDG22} observed that a supervised ViT-H/14 overfits on ImageNet1k 
\citep{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} without a model \gls{ema}, achieving an accuracy of 80.9\%.
Equipping a Polyak-Ruppert average ($\rho=0.9999$) alleviated overfitting and gave a 83.1\% accuracy.

\paragraph{Organization} In this appendix, we look at additional momenta for one-dimensional noisy parabola, as well as extensions to $D$-dimensions (\Cref{app:noisy-parabola}), provide a more detailed view of the results of \Cref{subsec:supervised-polyakking} (\Cref{app:subsec:polyak-image-classification}), and investigate the scenario where the \gls{ema} Scaling Rule (\Cref{def:ema-sr}) is applied to batch normalization \citep{DBLP:conf/icml/IoffeS15} coefficients (\Cref{subsec:polyak-bn}).

\subsection{Noisy parabola}
\label{app:noisy-parabola}

\paragraph{Additional one-dimensional examples}

First we consider additional one-dimensional examples, investigating the effect of modifying the base momentum $\rho_B$.
We present $\rho_B=0.99$ in \Cref{fig:app-toy-experiment-1d-0-99}, and $\rho_B=0.999$ in \Cref{fig:app-toy-experiment-1d-0-999}.
The results for $\rho_B=0.9999$ are presented in main text in \Cref{fig:toy-experiment}.

% Figure environment removed

% Figure environment removed

As described by the scaling error term in \Cref{eq:scaling-error}, the approximation error at a given $\kappa$ is higher for lower momenta $\rho$.
For a large range of scalings $\kappa$, the \gls{ema} Scaling Rule and the optimal momenta $\rho^*$ are consistent.
In summary, we see the synthetic experiments validate the results of \Cref{subsec:toy-experiment} for a range of momenta $\rho$.

\paragraph{Examples in higher dimensions}
Our final use of the synthetic \emph{noisy} parabola will consider an extension to $D$ dimensions.
Consider the optimization of $\rvtheta\in\R^D$ in a \emph{noisy parabola} at the origin:
\begin{align}
    \Ls(\rvtheta)
    &=\frac a2\,\rvtheta^\intercal\rvtheta,
    &\rvtheta_{k+1} &= \rvtheta_{k} - \eta \,\rvg_k, & \rvg_k&=a\,\rvtheta_k + \rvepsilon_k,
    & \rvepsilon_k\sim \mathcal{N}\left(\mathbf 0, \tfrac{b \,\rvg_k^2 + c}\kappa \right),
\end{align}
for curvature $a>0$,
scaled additive $b>0$,
and additive $c>0$ noise coefficients.
The scaling factor $\kappa$ in the covariance denominator implements the reduction in gradient noise as the scaling (i.e., the batch size) increases \citep{DBLP:journals/corr/abs-1711-04623}.
Let $\rvtheta\in\R^D$ be optimized with \gls{sgd} (\Cref{def:sgd}) and let there be a Polyak-Ruppert average (\Cref{def:polyak-ruppert-average}) $\rvzeta\in\R^D$ with momentum $\rho=1-\beta$ for $\rvtheta$.
We consider dimensionalities $D=2$ (\Cref{fig:app-toy-experiment-2d-0-9999}), $D=16$ (\Cref{fig:app-toy-experiment-16d-0-9999}), and $D=100$ (\Cref{fig:app-toy-experiment-100d-0-9999}).
We observe no significant differences in the \gls{ema} scaling behavior as we vary dimensions.

% Figure environment removed

% Figure environment removed

% Figure environment removed

\paragraph{Compute} The compute usage for the noisy parabola experiments is relatively small, with each run taking less than one minute on a single CPU, and so we do not detail this compute usage as we do in the other experimental sections. 

\clearpage

\subsection{Image Classification}
\label{app:subsec:polyak-image-classification}

\paragraph{Hyperparameters} We present the base hyperparameters for our image experiments in \Cref{tab:sup-r50-recipe}.

\paragraph{Data} For large scale vision evaluation, we use the ImageNet1k dataset \citep{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}, 
a widely used dataset containing approximately 1.2 million labeled images, distributed almost uniformly across 1000 different object classes, like animals, plants, and vehicles.

The images in ImageNet1k are are not consistent in resolution.
To handle this, they are resized and cropped to a standard size (in our case, $224\times 224$), before further processing.
This is part of the standard ImageNet augmentation stack for convolutional networks mentioned in 
\Cref{tab:sup-r50-recipe}.

\begin{table}[t]
  \caption{Supervised ResNetv2-50 hyperparameters used in Polyak-Ruppert Averaging experiments.}
  \label{tab:sup-r50-recipe}
  \centering
  \small
  \begin{tabular}{lc}
    \toprule
    & Supervised ResNetv2-50 \\
    \midrule
    ImageNet1k Test Top-1 & $76.27 \pm 0.10 \%$ \\
    ImageNet1k EMA Test Top-1 & $76.55 \pm 0.07 \%$ \\
    \midrule
    Weight initialization & \texttt{kaiming\_normal(relu)}  \\
    Backbone normalization    & BatchNorm  \\
    Synchronized BatchNorm over replicas & No \\ 
    Learning rate schedule & Multi step: $\times 0.1$ at $[30, 60, 80]$ epochs \\    
    Learning rate warmup (epochs) & 5 \\    
    Learning rate minimum value & $1\times 10^{-6}$  \\    
    Training duration (epochs) & 90 \\
    Optimizer & SGD + Momentum \\    
    SGD momentum & 0.9 \\    
    Optimizer scaling rule & Linear \\
    Base learning rate & 0.4  \\
    Base batch size & 1024  \\
    Base Polyak momentum & 0.9999 \\    
    Weight decay & $1\times 10^{-4}$ \\
    Weight decay scaling rule & None \\
    Weight decay skip bias & Yes \\
    Numerical precision & \texttt{bf16} \\
    Augmentation stack & ImageNet \\   
    Label smoothing rate & 0.1 \\
    \bottomrule
  \end{tabular}
\end{table}


\ifthenelse{\equal{\anonymous}{0}}{
\input{body/05_04_polyak_compute}
}{\compute}


\paragraph{Additional results}

In \Cref{fig:r50-polyak-full} we present a more detailed view of the results in \Cref{subsec:supervised-polyakking}.
First, we see that for all train metrics, model trajectories match,
and that a learning rate step schedule after warmup is present.
As discussed in \Cref{fig:r50-polyak-full}, a gap in \gls{ema} Test Top-1 trajectories begins at scaling $\kappa=4$, with a more pronounced effect visible at $\kappa=8$.
From \Cref{fig:r50-polyak-full} it is clear that the (non-\gls{ema}) Test Top-1 performance trajectory is no longer matching at these scalings, demonstrating that the problem is not due to a breakdown of the \gls{ema} Scaling Rule, but rather, that the model is overfitting at larger batch sizes due to batch normalization \citep{DBLP:conf/icml/IoffeS15}.

% Figure environment removed

\FloatBarrier

\subsection{Applying the EMA Scaling Rule to Batch Normalization}
\label{subsec:polyak-bn}

In 
\Cref{subsec:supervised-polyakking} and 
\Cref{app:subsec:polyak-image-classification},
we investigated a range of scalings $\kappa$, \emph{with} and \emph{without} applying the \gls{ema} Scaling Rule to the Polyak momentum.
In those experiments, we maintained Batch Normalization \citep{DBLP:conf/icml/IoffeS15} coefficients of $\rho_{\text{BN}}=0.9$ throughout\footnote{In many \gls{ml} frameworks, this value is defined using $\beta_\rho=1-\rho$, i.e. the default is $0.1$ and corresponds to $\beta_{\text{BN}}$ rather than $0.9$ corresponding to $\rho_{\text{BN}}$. We use $\rho_{\text{BN}}$ to maintain consistency across this work.}, i.e. the \gls{ema} Scaling Rule is not applied.
The running statistics of Batch Normalization \emph{are} an \gls{ema} with values determined by $\rho_{\text{BN}}$ and so it is reasonable to suspect we should apply the \gls{ema} Scaling Rule to $\rho_{\text{BN}}$ also.

In \Cref{fig:r50-polyak-full-bn} we investigate the effect of applying the \gls{ema} Scaling Rule to Batch Normalization coefficients, using $\hat\rho_{\text{BN}}=\rho_{\text{BN}}^\kappa$.
We observe that the Test Top-1 trajectories \emph{with} the \gls{ema} Scaling Rule applied are slightly closer to the reference trajectories for scalings $\kappa\geq 2$ than those trajectories \emph{without} the \gls{ema} Scaling Rule.
As the effect is not particularly large, at least in this setup, we do pursue further ablating applications of the \gls{ema} Scaling Rule to batch normalization coefficients, and always use $\rho_{\text{BN}}=0.1$ for Batch Normalization, independent of $\kappa$.

% Figure environment removed

