\begin{table}[t!]
  \caption{The role of the model \gls{ema} $\rvzeta$ in the optimization of $(\rvtheta, \rvzeta)$ given a target model $\rvtheta$ for different techniques, ordered by increasing influence of the \gls{ema} model.
  All statements assume a momentum $0\leq\rho<1$ and that the target model $\rvtheta$ is subject to stochastic optimization at a batch size $B$. }
  \label{tab:different-ema}
  \centering
  \small
  \begin{tabular}{p{0.22\textwidth}p{0.71\textwidth}}
    \toprule
    \textsc{Technique} & \textsc{Role of Model \gls{ema}} \\
    \midrule
    \textsc{Polyak-Ruppert averaging, Sec. \ref{subsec:supervised-polyakking}} & 
    $\rvtheta$ undergoes optimization and is tracked by
    $\rvzeta$, which does not affect $\rvtheta$. 
    $\rvzeta$ is an estimate of $\rvtheta$ with a time horizon and variance determined by $B$ and $\rho$.
     \\ \midrule
    \textsc{Continuous pseudo-labeling, Sec.~\ref{subsec:semi-supervised}} &
    \emph{Pre-Training} is as above in Polyak-Ruppert Averaging.
    \emph{After Pre-Training}, $\rvzeta$ (\emph{teacher}) produces targets for $\rvtheta$ (\emph{student}) from unlabeled data, which is combined with labeled data.
    The optimization endpoint is dependent on $B$ and $\rho$.\\ \midrule
    \textsc{Self-supervised learning, Sec.~\ref{subsec:self-supervised}} & 
    As above in \emph{After Pre-Training}, except there is no labeled data.
    The optimization endpoint is dependent on $B$ and $\rho$.
    \\
    \bottomrule
  \end{tabular}
  \vspace{-0.3cm}
\end{table}
