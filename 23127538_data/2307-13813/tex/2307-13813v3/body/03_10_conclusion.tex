\section{Conclusion}
\label{sec:conclusion}






We provide an \gls{ema} Scaling Rule: when changing the batch size by a factor of $\kappa$,  exponentiate the momentum of the \gls{ema} update to the power of $\kappa$.
This scaling rule should be applied in addition to optimizer scaling rules
(for example, linearly scaling the SGD learning rate),
and
enables the scaling of methods which rely on \gls{ema} and are sensitive to the choice of \gls{ema} momentum.

We prove the validity of the \gls{ema} Scaling Rule by deriving 
first-order \gls{sde} approximations of discrete model optimization when a model \gls{ema} is present and can contribute to the model objective.
We demonstrate empirical support 
for a variety of uses of \gls{ema}, ordered by increasing influence of the role of \gls{ema} on the optimization procedure: supervised model tracking (i.e. Polyak-Ruppert averaging) in speech and vision domains, pseudo-labeling in speech, and self-supervised image representation learning.
In almost all scenarios, using the \gls{ema} Scaling Rule
enables matching of training dynamics under batch size modification, whereas not using it results in significant differences in optimization trajectories. 
For example, we can scale the \gls{byol} self-supervised method to a batch size of 24,576 without any performance loss \emph{only} when using the \gls{ema} Scaling Rule.

While learning rate scaling rules are relatively commonplace in \gls{ml}, 
the role of \gls{ema} has been overlooked.
With this work, 
we highlight the importance of scaling the \gls{ema} momentum,
and hope that future works will use the \gls{ema} Scaling Rule to scale the \gls{ema} momentum correctly, in the same way that learning rates and other optimizer hyperparameters are scaled.
