\section{Related work}
\label{sec:related-work}


{\bf Optimizer scaling rules from \glspl{sde}}~
The \gls{sde} perspective has uncovered optimizer scaling rules and allowed an understanding of their limitations.
\citet{DBLP:conf/iclr/SmithL18} used \glspl{sde} to uncover the \gls{sgd} Scaling Rule, while 
\citep{DBLP:conf/nips/LiMA21} used \glspl{sde} to explain that rule's breakdown in terms of discretization error.
The \gls{sde} analysis was extended to adaptive optimization by
\citep{DBLP:conf/nips/MalladiLPA22}, producing an Adam Scaling Rule (\Cref{def:adam-sr}),
indicating that along with the learning rate, the $\beta_{1,2}$ and $\epsilon$ parameters transform.
The $\beta_{1,2}$ transformation is consistent with the \gls{ema} Scaling Rule in the \gls{sde} limit.
Our work differs as it considers a model EMA that alters the objective.

{\bf Varying the batch size during training}~
\citet{DBLP:conf/iclr/SmithKYL18}
investigated the benefits of scheduling the batch size at a fixed learning rate as an alternative to scheduling the learning rate at a fixed batch size.
These two are equivalent through the \gls{sgd} Scaling Rule.
The authors \emph{do not} scale the optimizer hyperparameters during this procedure, as they are intentionally replicating the training dynamics of a learning rate schedule.
This is in contrast with \emph{Progressive Scaling} (\Cref{def:progressive-scaling}) which scales the hyperparameters to \emph{maintain} the optimization process at different levels of discretization.

{\bf Large batch training of SSL distillation methods} 
\gls{ssl} methods learn representations without labels, meaning they can take advantage of web-scale data.
Large batch optimization is required to make use of this data in a reasonable amount of time.
\citet{DBLP:conf/nips/GrillSATRBDPGAP20} demonstrated algorithmic robustness when \emph{reducing} the batch size through gradient accumulation and EMA update skipping, which implements an approximation of our \gls{ema} Scaling Rule for $\kappa<1$.
Our work provides a recipe to scale down \emph{and up} in $\kappa$. 
MoCo-v3 \citep{DBLP:conf/iccv/ChenXH21} enables contrastively distilled \gls{vit}s up to a batch size of 6144, where the model drops in performance. 
More recently, methods like DINO \citep{DBLP:conf/nips/CaronMMGBJ20} present a worse scenario, and are unable to scale beyond batch size 1024 \citep{DBLP:journals/corr/abs-2209-15589}.
In contrast, our work presents practical tools to scale to large batch sizes in the presence of an \gls{ema}, enabling practical training of these \gls{ssl} methods on large scale data.



