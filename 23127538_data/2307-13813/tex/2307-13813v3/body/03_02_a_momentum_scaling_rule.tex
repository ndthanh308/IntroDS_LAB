\section{The EMA Scaling Rule}
\label{sec:a-momentum-scaling-rule}

We begin with an informal discussion of scaling rules and motivate the existence of an exponential scaling rule for the momentum parameter controlling the update of the model \gls{ema}.

\subsection{Background and an informal discussion of scaling rules}
\label{sec:a-momentum-scaling-rule-background}

Consider a model with parameters $\rvtheta_t$ at iteration $t$
updated with \gls{sgd} (\Cref{def:sgd}).
\begin{definition}[SGD Update]
     The \gls{sgd} update for a model with parameters $\rvtheta_t$ at iteration $t$ given a minibatch $\sB=\{x^{(b)}\sim P_{\rvx}:b=1,2,\ldots,B\}$ of $B=|\sB|$ samples with learning rate $\eta$ is
    \begin{equation}
    \rvtheta_{t+1}
    =
    \rvtheta_t - \eta \times \frac1B
    \sum_{x\in\sB} \nabla_{\rvtheta} \Ls(x;\rvtheta_{t}),
    \end{equation}
    where $\Ls$ is the loss function, 
    $\nabla_\theta \Ls(x;\theta_t)$ is the parameter gradient for the sample $x$ at iteration $t$, and the $x\in \sB$ are \gls{iid} from $P_{\rvx}$.
    \label{def:sgd}
\end{definition}
Iterating over a sequence of independent minibatches 
$\sB_0, \sB_1, \ldots, \sB_{\kappa-1}$
produces model parameters
\begin{align}
    \rvtheta_{t+\kappa}
    =
    \rvtheta_t - \eta \times \frac1B
    \sum_{j=0}^{\kappa-1}
    \sum_{x\in\sB_j} \nabla_{\rvtheta} \Ls(x;\rvtheta_{t+j}).
    \label{eq:sgd}
\end{align}
If gradients vary slowly 
$\nabla_{\rvtheta}\Ls(x;\theta_{t+j})\approx \nabla_{\rvtheta}\Ls(x;\theta_{t})$, $j=0,\ldots,\kappa-1$,
\emph{one} \gls{sgd} step with
$\hat\eta=\kappa\eta$ on a batch $\widehat\sB=\cup_i\sB_i$ of size $\hat B=\kappa B$
results in $\hat\rvtheta_{t+1}\approx\rvtheta_{t+k}$,
yielding the \gls{sgd} Scaling Rule (\Cref{def:lsr}).
\begin{definition}[\gls{sgd} Scaling Rule]
    When running \gls{sgd} (\Cref{def:sgd}) with batch size $\hat B=\kappa B$,
    use a learning rate $\hat\eta=\kappa\eta$ \citep{DBLP:journals/corr/Krizhevsky14,DBLP:journals/corr/GoyalDGNWKTJH17}.
    \label{def:lsr}
\end{definition}
For clarity in this work, we adopt the naming convention \emph{[Algorithm Name] Scaling Rule},
which means all parameters of those algorithms are appropriately scaled from batch size $B$ to $\kappa B$.

As discussed in \citet{DBLP:journals/corr/GoyalDGNWKTJH17},
although the assumption of slowly changing gradients is strong, if it is true, then
$\rvtheta_{t+k}\approx \hat\rvtheta_{t+1}$
\emph{only} if $\hat\eta=\kappa\eta$.
The validity of the \gls{sgd} Scaling Rule has been formally studied.
In particular, there was ambiguity regarding whether the scaling should be a square-root or linear \citep{DBLP:journals/corr/Krizhevsky14}.
\gls{sde} approaches have resolved this ambiguity, and have been used to estimate the scaling $\kappa$ when the \gls{sgd} Scaling Rule is no longer guaranteed to hold \citep{DBLP:conf/nips/LiMA21}.

To address model parameter \glspl{ema}, we first restate the \gls{ema} Update (\Cref{def:ema}).
\firstema*
The model \gls{ema} parameters $\rvzeta$ do not typically receive gradient information, we take the convention that $\rho$ is close to one, and the $\beta_\rho$ subscript will be omitted where it is clear from the context.

Assuming again that gradients change slowly $\nabla_{\rvtheta}\Ls(x;\rvtheta_{t+j},\rvzeta_{t+j})\approx \nabla_{\rvtheta}\Ls(x;\rvtheta_{t},\rvzeta_{t})\approx \rvg$, for gradient $\rvg$,
iterating over $\kappa$ independent minibatches produces model states (see \Cref{app:matrix-calculations} for derivation)
\begin{align}
\label{eq:scalingRuleSummaryEquation}
\begin{bmatrix}
\rvtheta_{t+\kappa}
\\
\rvzeta_{t+\kappa}
\\
\rvg
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & -\eta \\
(1-\rho) & \rho & 0\\
0 & 0 & 1
\end{bmatrix}^\kappa
\cdot 
\begin{bmatrix}
\rvtheta_{t}
\\
\rvzeta_{t}
\\
\rvg
\end{bmatrix}
=
\begin{bmatrix}
\rvtheta_{t}-\eta\,\kappa \,\rvg
\\
\rho^\kappa \, \rvzeta_{t}
+(1-\rho^\kappa) \, \rvtheta_t
+\mathcal O\left(\eta\times \beta_\rho\right)
\\
\rvg
\end{bmatrix}.
\end{align}
The first row is the \gls{sgd} Scaling Rule (\Cref{def:lsr}). The third row 
implements the \emph{slowly changing gradients} assumption for the first row.
The second row is equivalent to a single \gls{ema} update (\Cref{def:ema}) with momentum $\hat\rho=\rho^\kappa$; we can take a \emph{single} \gls{sgd} update with batch size $\hat B=\kappa B$ and learning rate $\hat\eta=\kappa\eta$, and a 
\emph{single} \gls{ema} update with momentum $\hat\rho=\rho^\kappa$, and we get $(\hat\rvtheta_{t+1},\hat\rvzeta_{t+1})\approx(\rvtheta_{t+\kappa},\rvzeta_{t+\kappa})$ up to terms $\mathcal O(\eta \times \mathcal \beta_\rho)$.
This yields the \gls{ema} Scaling Rule (\Cref{def:ema-sr}).
\firstemascaling*

The \gls{ema} Scaling Rule was derived for \gls{sgd}, and is extended to other optimizers in the following way. 
An optimizer scaling rule ensures $\hat\rvtheta_{t+1}=\rvtheta_{t+\kappa}$,
satisfying identification for the first row.
Next, the zeroth order term in $\eta\times\beta_{\rho}$ in the second row in \Cref{eq:scalingRuleSummaryEquation} is optimizer-independent, and therefore unchanged.
Finally, the first order terms in $\eta\times\beta_{\rho}$ in the second row, corresponding to the scaling rule error, are an \gls{ema} accumulation of target model $\rvtheta$ updates under optimization, which is still 
$\mathcal{O}(\eta \times \mathcal  \beta_\rho)$, although its functional form may be different for different optimizers.

The above discussion is intended to give an intuition for why the \gls{ema} momentum should be scaled exponentially.
As we have used the same slow-moving gradient assumption as the original \gls{sgd} Scaling Rule,
this may cast doubt on whether our rule is correct.
To remove this ambiguity, we will follow 
\citet{DBLP:conf/iclr/SmithL18,DBLP:conf/nips/LiMA21,DBLP:conf/nips/MalladiLPA22}, and show that the \gls{ema} Scaling Rule (\Cref{def:ema-sr}) is correct in the \gls{sde} limit under more realistic gradient assumptions. 

\subsection{The EMA Scaling Rule through the lens of stochastic differential equations}
\label{subsec:ema-sdes}

\glspl{sde} are a tool typically used to obtain scaling rules from first principles~\citep{DBLP:conf/nips/LiMA21,DBLP:conf/nips/MalladiLPA22}.
In the following, we use \glspl{sde} to obtain strong theoretical guarantees for the 
\gls{ema} Scaling Rule found in \Cref{sec:a-momentum-scaling-rule-background}.
We consider the following discrete dynamics for \gls{ema}:
\begin{align}
    \label{eq:iterations}
    \begin{split}
    \rvtheta_{k+1} &= \rvtheta_{k} - \eta\, \rvg_k,
    \enspace \text{with }
    \rvg_k=\nabla f(\rvtheta_k, \rvzeta_k) + \sigma \, \rvepsilon_k, 
    \text{ and }
    \rvepsilon_k \sim \mathcal{E}_\sigma(\rvtheta_k, \rvzeta_k),\\
    \rvzeta_{k+1} &= \rho \, \rvzeta_k + (1-\rho) \,\rvtheta_k,
    \end{split}
\end{align}
where $\sigma>0$ is the noise scale, 
$\mathcal{E}_\sigma(\rvtheta_k,  \rvzeta_k)$
is the gradient noise distribution, assumed to be zero-mean and variance 
$\mSigma(\rvtheta_k,  \rvzeta_k)$ 
independent of $\sigma$, and 
$\nabla f(\rvtheta_k, \rvzeta_k)\equiv\nabla_{\rvtheta} f(\rvtheta_k, \rvzeta_k)$.
We posit a dependency of the loss $f$ on the EMA $\rvzeta$ in order to cover semi-supervised (\Cref{subsec:semi-supervised}) and \gls{ssl} (\Cref{subsec:self-supervised}).
The case of Polyak-Ruppert averaging (\Cref{subsec:supervised-polyakking}), is covered by letting $f$ be independent of $\rvzeta$.

We aim to obtain an \gls{sde} approximation of \Cref{eq:iterations} as $\eta$ goes to zero.
The scaling rule for iterations of $\rvtheta$ is well known~\citep{DBLP:conf/nips/LiMA21}: we let $\sigma_0 = \sigma \sqrt{\eta}$.
The analysis of \Cref{sec:a-momentum-scaling-rule-background}
gives the scaling rule $\hat{\eta} = \eta \kappa$ and $\hat{\rho} = \rho^{\kappa}$.
Linearizing this rule near 
$\eta = 0$ 
gives 
$\hat{\rho} = 1 - \kappa\times(1 - \rho)$, which is a linear relationship between $1 -\rho$ and $\eta$. 
We therefore let $\beta_0=(1 - \rho) / \eta$ and consider the SDE 
\begin{align}
    \label{eq:sde-sgd}
    \begin{split}
        d\Theta_t &= - \nabla f(\Theta_t, Z_t)\,dt 
        +
        \sigma_0\,\mSigma(\Theta_t, Z_t)^{\frac12}\,dW_t,
        \enspace \text{with }
        W_t \text{ a Wiener process},\\
        dZ_t &= \beta_0(\Theta_t - Z_t)dt,
    \end{split}
\end{align}
where $\Theta_t$ and $Z_t$ are \gls{sde} variables relating to model and \gls{ema} parameters respectively.
The SDE in \Cref{eq:sde-sgd} approximates the discrete iterations of \Cref{eq:iterations} when the learning rate $\eta$ goes to zero.
One way to see this is that an Euler-Maruyama discretization of the SDE with learning rate $\eta$ exactly recovers the discrete iterations.
More formally, we have \Cref{thm:sde-for-sgd-ema}, which is in the same spirit as those found in~\cite{DBLP:conf/nips/LiMA21,DBLP:conf/nips/MalladiLPA22}. In the theorem, $G^\alpha$ is the set of functions with derivatives up to order $\alpha$ that have at most polynomial growth (see~\Cref{def:polynomial-growth}).
\begin{theorem}[SDE for SGD + EMA; informal see~\Cref{thm:app:sde}]
     Assume that $f$ is continuously differentiable, with $f\in G^3$.
     Let 
     $\Theta_t,Z_t$ 
     be solutions of  \Cref{eq:sde-sgd},
     and $\rvtheta_k,\rvzeta_k$ iterations of \Cref{eq:iterations}
     with
     $\mSigma^{\frac12}\in G^2$. 
     Then, for any time horizon $T >0$ and function $g\in G^2$, there exists a constant $c>0$ independent of $\eta$ such that 
    \begin{equation}
        \max_{k=0,\,\dots\,,\,\lfloor T /\eta \rfloor} |\mathbb{E}[g(\Theta_{\eta k}, Z_{\eta k})] - \mathbb{E}[g(\rvtheta_k, \rvzeta_k)]| \leq c\times  \eta .
    \end{equation}
    \label{thm:sde-for-sgd-ema}
    \vspace{-0.5cm}
\end{theorem}
\Cref{thm:sde-for-sgd-ema} formalizes the intuition that the SDE is an accurate approximation of the discrete iterations. In turn, it allows validating the scaling rule in the same spirit as in~\citet{DBLP:conf/nips/MalladiLPA22}.
\begin{corollary}[Validity of the EMA Scaling Rule]
     Assume that $f$ is continuously differentiable, with $f\in G^3$ and $\mSigma^{\frac12}\in G^2$. 
     Let $\rvtheta_k^{(B)}, \rvzeta_k^{(B)}$ be iterations of \Cref{eq:iterations} with batch size $B$ and hyperparameters $\eta, \rho$. 
     Let $\rvtheta_k^{(\kappa B)}, \rvzeta_k^{(\kappa B)}$ be iterates with batch size $\kappa B$, and $\hat{\eta}$ determined by the \gls{sgd} Scaling Rule (\Cref{def:lsr}) and $\hat{\rho}$ determined by the \gls{ema} Scaling Rule (\Cref{def:ema-sr}). 
     Then, for any time horizon $T >0$ and function $g\in G^2$, there exists a constant $c>0$ independent of $\eta$ such that 
    \begin{equation}
        \max_{k=0,\,\dots\,,\, \lfloor T /\eta \rfloor} |\mathbb{E}[g(\rvtheta_{\lfloor k / \kappa \rfloor}^{(\kappa B)}, \rvzeta_{\lfloor k / \kappa \rfloor}^{(\kappa B)})] - \mathbb{E}[g(\rvtheta_k^{(B)}, \rvzeta_k^{(B)})]| \leq c\times  \eta .
    \end{equation}
    \label{cor:validity-scaling-rule}
    \vspace{-0.5cm}
\end{corollary}
\Cref{cor:validity-scaling-rule} shows that two trajectories with different batch sizes are close in the limit of small learning rate, demonstrating the validity of \Cref{def:ema-sr}.
A natural follow-up question is 
\emph{what happens when an adaptive optimizer is used instead of SGD?}
\citet{DBLP:conf/nips/MalladiLPA22} study this without an \gls{ema} and characterize how hyperparameters change with the noise scale.
In particular, they show that under a high gradient noise hypothesis, there exists a limiting SDE. 
In \Cref{app:ema-approximation-theorem}, we derive the limiting SDEs for RMSProp and Adam with an EMA.
Although a formal proof of closeness between the iterations and these SDEs is beyond the scope of this work, these \glspl{sde} indicate that the EMA Scaling Rule holds for adaptive algorithms. 
We demonstrate this empirically in \Cref{sec:experiments}.

