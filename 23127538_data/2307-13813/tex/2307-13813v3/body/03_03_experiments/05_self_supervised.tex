\subsection{Self-supervised image representation learning}
\label{subsec:self-supervised}

Finally, we turn our attention to  distillation based
\acrfull{ssl}.
where the model \gls{ema} is the \emph{teacher}
\citep{DBLP:conf/nips/GrillSATRBDPGAP20,DBLP:journals/taslp/NiizumiTOHK23,DBLP:journals/corr/abs-2104-14294,DBLP:journals/corr/abs-2304-07193}.

We will use \gls{byol}
(\cite{DBLP:conf/nips/GrillSATRBDPGAP20}, \Cref{def:emaUpdateDefinition})\footnote{
The \gls{byol} \gls{ema} update (\Cref{eq:byol-ema-update}) uses $\rvtheta_{t+1}$ instead of our analyzed $\rvtheta_{t}$ (\Cref{eq:scalingRuleSummaryEquation}).
The effect upon the overall \gls{ema} update is $\mathcal{O}(\eta\times\beta_\rho)$ and so is captured by the \gls{ema} Scaling Rule (\Cref{def:ema-sr}).
}
for our investigation into scaling as it is well-studied \citep{DBLP:conf/icml/TianCG21,DBLP:journals/corr/abs-2302-04817}, relatively simple to implement due to minimal hyper-parameters, and obtains competitive results \citep{DBLP:conf/nips/GrillSATRBDPGAP20,DBLP:journals/corr/abs-2209-15589}.
Since \gls{byol} learns through self-referential distillation, momentum plays a significant role in optimization. 
We analyze: i) a ResNet-18 \citep{DBLP:conf/cvpr/HeZRS16} on CIFAR10 \citep{CIFAR10} (\Cref{fig:r18-byol}) using SGD (\Cref{def:sgd}); and ii) a \gls{vit}-B/16 \citep{DBLP:conf/iclr/DosovitskiyB0WZ21} on ImageNet1k using AdamW \citep{DBLP:conf/iclr/LoshchilovH19}.
A recipe for \gls{byol} using \glspl{vit} is provided in \Cref{app:byol-vit}. 


% Figure environment removed

% Figure environment removed

{\bf ResNet-18 on CIFAR-10}~
We begin with a ResNet-18 model and short training duration to enable quick iteration,
and an \gls{sgd} optimizer as it has as \emph{known} scaling rule. 
This allows us to probe the \gls{ema} Scaling Rule without potential confounders like poor gradient-based optimizer scaling\footnote{For competitive performance with the reference \gls{byol}
\citep{DBLP:conf/nips/GrillSATRBDPGAP20}
using a ResNet-50, adaptive optimization, and longer training duration, see 
\Cref{subsec:byol-additional}
and
\Cref{fig:r50-byol}.}. 

We observe that \emph{without} the \gls{ema} Scaling Rule, there is a  drop in test top-1 linear probe (\Cref{def:linear-probe}) performance compared to the baseline, whereas \emph{with} the \gls{ema} Scaling Rule, we closely match the baseline model until batch size 4096.
We show that this result is consistent for a range of base learning rates $\eta_B$ and momenta $\rho_B$ in \Cref{subsec:byol-sensitivity-analysis}.
At batch size 8192, we see a performance gap between the scaled model using the \gls{ema} Scaling Rule and the baseline.
We speculate that this is due to dynamics early in the \gls{byol} training process that are challenging to replicate at larger batch sizes.
To test, and potentially circumvent this, we introduce \emph{Progressive Scaling} (\Cref{def:progressive-scaling}).
\begin{definition}[Progressive Scaling, informal; see \Cref{subsec:dynamic-batch-scaling}] 
    Given batch size $B$ and hyperparameters at $B$, 
    slowly increase the batch size to the desired largest batch size during training.
    At any intermediate batch size $\hat B=\kappa B$, all hyperparameters are scaled according to their scaling rules.
    \label{def:progressive-scaling}
\end{definition}
We see that transitioning to the higher batch size \emph{during} the warmup period results in a model optimization trajectory that diverges from the baseline, whereas transitioning \emph{after} warmup results in matching final trajectories of the scaled and baseline models.
In summary, \emph{progressive scaling} allows us to match \gls{byol} dynamics at large batch sizes, provided we transition after the warmup period.
This observation is consistent with our hypothesis regarding \gls{byol} dynamics during warmup.


{\bf Vision Transformers on ImageNet1k}~
\label{subsec:vit_byol}
Progressive Scaling coupled with the \gls{ema} Scaling Rule is required when scaling \gls{byol} \glspl{vit} (\Cref{fig:vitb-byol}),
enabling baseline loss tracking to a batch size of 24,576. 
Perfect scaling fails at batch size 32,768,
 consistent with observations in supervised learning 
\citep{DBLP:journals/corr/GoyalDGNWKTJH17,DBLP:conf/aaai/HuoGH21}.
Despite the breakdown, there is only a small drop in 1.6\% probe performance when using the \gls{ema} Scaling Rule, compared to as 44.56\% drop \emph{without} it.
We also observe that it is sometimes possible to match test model performance using \emph{only} Progressive Scaling and \emph{not} the \gls{ema} Scaling Rule, although this still induces a training loss mismatch.
We stress that such an approach is \emph{not} guaranteed to work and discuss when this approach succeeds and fails in \Cref{app:byol-progressive-scaling-regimes} and \Cref{fig:robustness-cartoon}.


At the transition point between batch sizes, 
an impulse perturbation\footnote{Instead of a single large batch transition as in \Cref{fig:vitb-byol} we perform a sequential transition in \Cref{app:byol-waterfall}. We find that a slow increase in batch size minimizes the magnitude of the perturbation and leads to a final model with higher effective linear probe top-1 than the reference by approximately $1.17\%$.} is measured at the student, visible from the training loss. 
This 
is recovered from by the learning process, 
and the new model matches the reference batch size. 
This perturbation happens in both the AdamW and \gls{sgd} settings, 
leading us to 
suspect
this is due to the \gls{byol} learning process, rather than an artifact of optimizer or momentum scaling. However, since this is not directly related to the EMA Scaling Rule proposed in this work, we defer this analysis to future investigation.

















