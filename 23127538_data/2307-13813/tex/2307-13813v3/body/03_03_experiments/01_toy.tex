\subsection{Polyak-Ruppert averaging in a simple setting}
\label{subsec:toy-experiment}

At inference, it is typical to use a model \gls{ema}, known as Polyak-Ruppert Averaging (\Cref{def:polyak-ruppert-average}).

\begin{definition}[Polyak-Ruppert Average]
    When optimizing model parameters $\rvtheta$, compute their \gls{ema} $\rvzeta$ (\Cref{def:ema}).
    Use $\rvzeta$ instead of $\rvtheta$ at inference \citep{Polyak92,Ruppert1988EfficientEF}.
    \label{def:polyak-ruppert-average}
\end{definition}

We begin by showing the \gls{ema} Scaling Rule is \emph{required} to match parameter trajectories in a simple setting.
Consider the optimization of $\rtheta$ in a \emph{noisy parabola}  whose loss $\Ls(\rtheta)$ is parameterized by coefficients for curvature $a>0$,
scaled additive noise $b\geq0$,
and additive noise $c\geq0$:
\begin{align}
    \Ls(\rtheta)
    &=\frac a2\,\rtheta^2,
    &\rtheta_{k+1} &= \rtheta_{k} - \eta \,\rg_k, & \rg_k&=a\,\rtheta_k + \repsilon_k,
    & \repsilon_k\sim \mathcal{N}\left(0, \tfrac{b \,\rg_k^2 + c}\kappa \right).
\end{align}
The scaling factor $\kappa$ in the covariance denominator implements gradient noise reduction as scaling (i.e. batch size) increases \citep{DBLP:journals/corr/abs-1711-04623}.
Let $\rtheta\in\R$ be optimized with \gls{sgd} (\Cref{def:sgd}) and $\rzeta\in\R$ be a Polyak-Ruppert average (\Cref{def:polyak-ruppert-average}) for $\rtheta$ with momentum $\rho=1-\beta$ .
At scaling $\kappa=1$, we use $\beta_B=\eta_B=10^{-4}$ 
and $I_B=10^4$ iterations, to yield a total time $T=I_B\times \eta_B=1$.
To keep gradients $\mathcal O(1)$ and gradient noise non-negligible, we take
$a=1$, $b=0.5$, and $c=0$.
% Figure environment removed

First, we observe the effect of scaling on a single run (\Cref{fig:parabola-single-runs}) by tracking the position of the model \gls{ema}.
We see that at scaling $\kappa=8$ or $\kappa=256$, the runs using the \gls{ema} Scaling Rule match the baseline trajectory,
whereas the runs using the baseline momentum do not, with a greater deviation induced by greater scaling $\kappa$.
Even at $\kappa=8$, there is a significant difference between scaled and unscaled trajectories, despite the seemingly small numerical difference of their momenta\footnote{Momentum enters optimization exponentially; small changes can lead to very different updates.}.

Second, we consider whether the \gls{ema} Scaling Rule is optimal.
To do this, inspired by the \gls{sde} analysis (\Cref{subsec:ema-sdes}), 
we define the approximation error, $\text{Err}(\rho,\kappa,g)$, of a test function $g$ for a given scaling $\kappa$ using momentum $\rho$, and the value of the momentum $\rho^*(\kappa,g)$ that minimizes this error:
\begin{align}
    \rho^*(\kappa,g)=
    &
    \argmin_\rho
    \text{Err}(\rho,\kappa,g),
    &
    \text{Err}(\rho,\kappa,g)
    &\equiv
    \max_{k=0,\ldots,T/\eta}
    \left|
    \E \,g(\rvzeta_k)
    -
    \E \,g(\rvzeta^{(\kappa,\rho)}_{k/\kappa})
    \right|.
    \label{eq:optimal-momentum}
\end{align}
For scalings $\kappa\in\{1,2,4,\ldots,1024\}$, we determine the optimal momentum $\rho^*$ and compare it to the \gls{ema} Scaling Rule (\Cref{fig:curve-approximation-error}, left).
The scaling rule tracks the $\rho^*$ until $\kappa=256$, when
the $\rho^*$ become systematically higher.
We see target model error increase at $\kappa=256$ (\Cref{fig:curve-approximation-error}, right). 
As the target model error is \gls{ema}-independent, this indicates that the \gls{sgd} Scaling Rule is breaking.
At the lower scaling $\kappa=64$, there is an inflection point in the \gls{ema} Scaling Rule approximation error, before the model error grows.
This difference indicates the $\mathcal{O}(\eta\times \beta_\rho)$ terms of \Cref{eq:scalingRuleSummaryEquation} are beginning to influence the \gls{ema} update.
Finally, 
these observations are true in $D=100$ dimensions, (\Cref{app:noisy-parabola}), and
we stress that \emph{not} changing the momentum at every scaling $\kappa$ induces large approximation error, indicating there is merit to using the \gls{ema} Scaling Rule.
