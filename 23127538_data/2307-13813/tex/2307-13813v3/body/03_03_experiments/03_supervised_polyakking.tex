\subsection{Supervised learning on real data with Polyak-Ruppert averaging}
\label{subsec:supervised-polyakking}

We now turn to real-world classification where
the target model $\rvtheta$ optimizes a parametric log-likelihood
$\max_{\rvtheta} \log p(\rvy | \rvx; \rvtheta)$ 
with inputs and labels $(\vx,\vy)$ drawn from a joint distribution $p(\rvy, \rvx)$.

{\bf Image Classification}~~~
We consider a variant of the original \gls{sgd} Scaling Rule result \citep{DBLP:journals/corr/GoyalDGNWKTJH17} and train a ResNetv2 \citep{DBLP:conf/eccv/HeZRS16}
on ImageNet1k \citep{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} (\Cref{fig:r50-polyak}) using a three step learning rate schedule. 
The base momentum $\rho_B=0.9999$ at batch size 1024 was found by hyperparameter optimizing for \gls{ema} test performance, and we seek to achieve this optimized performance at different batch sizes.
% Figure environment removed
We \emph{do not} apply the \gls{ema} Scaling Rule on the Batch Normalization \citep{DBLP:conf/icml/IoffeS15} statistics\footnote{Since Batch Normalization statistics use an \gls{ema} update, it is reasonable to ask whether the \gls{ema} Scaling Rule should be applied.
We investigate this in \Cref{subsec:polyak-bn}.
We find one \emph{should} apply the scaling rule, however, the effect is less significant than the application of the \gls{ema} Scaling Rule to model parameters.}. 
We observe that \emph{without} the EMA Scaling Rule, there is a significant drop in model \gls{ema} test performance, whereas \emph{with} the \gls{ema} Scaling Rule, we can approximate the baseline model \gls{ema} test top-1 performance across all batch sizes.
We match baseline \gls{ema} statistics across the full trajectory batch size 2048, where the test \gls{ema} performance diverges.
This is due to non-\gls{ema} test performance dropping for high $\kappa$ (see \Cref{app:subsec:polyak-image-classification}).
We observe that model \gls{ema} top-1 is approximately 0.2\% to 0.3\% higher than the target model.



{\bf \gls{asr}}~~~ 
We train a transformer \citep{DBLP:conf/nips/VaswaniSPUJGKP17}
using the \gls{ctc} loss~\citep{graves2006connectionist} and Adam optimizer on the \tco{} subset (100h) of LibriSpeech~\citep{panayotov2015librispeech} (for details see \Cref{app:speech}).
We apply the Adam Scaling Rule (\citet{DBLP:conf/nips/MalladiLPA22}, \Cref{def:adam-sr}) and use dynamic batching (minibatch size $\times$ sequence length $=\text{const}=290s$,
and $s$ indicates audio duration in seconds).

\emph{Without} the EMA Scaling Rule, there is a significant difference in model \gls{ema} test \gls{wer} trajectories compared to the baseline, whereas \emph{with} the \gls{ema} Scaling Rule, trajectories match, as is shown in \Cref{fig:speech-polyak}. 
We note that compared to image classification, in \gls{asr}, the model \gls{ema} converges to similar final performance irrespective of use of the scaling rule. 
This convergence is due to the longer training time compared to the \gls{ema} horizon as discussed in \Cref{tab:different-ema} (see \Cref{app:asymptoticAnalysis} for a proof sketch).
Although in this specific case one can achieve similar \emph{final performance} without the \gls{ema} Scaling Rule, it is \emph{necessary} to use the \gls{ema} Scaling Rule in order to replicate the full training trajectory, which gives \emph{guarantees} on properties like final performance (see \Cref{cor:validity-scaling-rule}).
We also observe
a growing gap between the baseline and \gls{ema}-scaled trajectories as we increase~$\kappa$. 
Inspecting the train loss and non-EMA test WER, which \emph{do not} depend on the \gls{ema} update (see \Cref{fig:app-speech-polyak}, \Cref{subsec:speech-detailed}), indicates this is due to a breakdown of the Adam Scaling Rule.
\emph{In summary, evaluation on ASR shows that the EMA Scaling Rule holds in practice for sequential data with dynamic batch sizes, as well as when using adaptive optimization.}

% Figure environment removed

