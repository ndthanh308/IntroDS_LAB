\contentsline {section}{\numberline {1}Introduction}{1}{section.1}%
\contentsline {section}{\numberline {2}The EMA Scaling Rule}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Background and an informal discussion of scaling rules}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}The EMA Scaling Rule through the lens of stochastic differential equations}{4}{subsection.2.2}%
\contentsline {section}{\numberline {3}Experiments}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Polyak-Ruppert averaging in a simple setting}{5}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Supervised learning on real data with Polyak-Ruppert averaging}{6}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Semi-supervised speech recognition via pseudo-labeling}{7}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Self-supervised image representation learning}{8}{subsection.3.4}%
\contentsline {section}{\numberline {4}Related work}{10}{section.4}%
\contentsline {section}{\numberline {5}Conclusion}{10}{section.5}%
\contentsline {section}{\numberline {6}Acknowledgements}{11}{section.6}%
\ttl@starttoc {sections@1}
\contentsline {section}{\numberline {A}Broader impact}{18}{appendix.A}%
\contentsline {section}{\numberline {B}Limitations}{18}{appendix.B}%
\contentsline {section}{\numberline {C}The scaling toolbox: practical methods for enabling systematic scaling}{19}{appendix.C}%
\contentsline {subsection}{\numberline {C.1}The continuous time/SDE perspective}{19}{subsection.C.1}%
\contentsline {subsection}{\numberline {C.2}Scaling rules for optimization}{20}{subsection.C.2}%
\contentsline {subsection}{\numberline {C.3}Commonly used values of hyperparameters at different batch sizes}{21}{subsection.C.3}%
\contentsline {subsection}{\numberline {C.4}Progressive scaling}{21}{subsection.C.4}%
\contentsline {section}{\numberline {D}EMA approximation theorems with SDEs}{22}{appendix.D}%
\contentsline {subsection}{\numberline {D.1}SGD with model EMA}{22}{subsection.D.1}%
\contentsline {subsection}{\numberline {D.2}Adaptive gradient methods with model EMA}{24}{subsection.D.2}%
\contentsline {section}{\numberline {E}Additional proofs}{25}{appendix.E}%
\contentsline {subsection}{\numberline {E.1}Iterations of SGD + EMA}{25}{subsection.E.1}%
\contentsline {subsection}{\numberline {E.2}Limiting behavior of Polyak-Ruppert averaging}{26}{subsection.E.2}%
\contentsline {section}{\numberline {F}Additional details and results for Polyak-Ruppert averaging}{27}{appendix.F}%
\contentsline {paragraph}{Additional background}{27}{section*.12}%
\contentsline {paragraph}{Organization}{28}{section*.13}%
\contentsline {subsection}{\numberline {F.1}Noisy parabola}{28}{subsection.F.1}%
\contentsline {paragraph}{Additional one-dimensional examples}{28}{section*.14}%
\contentsline {paragraph}{Examples in higher dimensions}{28}{section*.17}%
\contentsline {paragraph}{Compute}{29}{section*.21}%
\contentsline {subsection}{\numberline {F.2}Image Classification}{30}{subsection.F.2}%
\contentsline {paragraph}{Hyperparameters}{30}{section*.22}%
\contentsline {paragraph}{Data}{30}{section*.23}%
\contentsline {paragraph}{Compute usage}{30}{section*.25}%
\contentsline {paragraph}{Additional results}{30}{section*.27}%
\contentsline {subsection}{\numberline {F.3}Applying the EMA Scaling Rule to Batch Normalization}{31}{subsection.F.3}%
\contentsline {section}{\numberline {G}Additional details and results for Automatic Speech Recognition (ASR)}{32}{appendix.G}%
\contentsline {paragraph}{Data}{32}{section*.30}%
\contentsline {paragraph}{Acoustic model}{32}{section*.31}%
\contentsline {paragraph}{Training}{33}{section*.33}%
\contentsline {paragraph}{Pseudo-Labeling}{33}{section*.34}%
\contentsline {subsection}{\numberline {G.1}Additional experimental settings and detailed metrics}{34}{subsection.G.1}%
\contentsline {subsection}{\numberline {G.2}Scaling to $\kappa =16$ with Progressive Scaling}{37}{subsection.G.2}%
\contentsline {section}{\numberline {H}Additional details and results for self-supervised image representation learning}{38}{appendix.H}%
\contentsline {paragraph}{Organization}{38}{section*.42}%
\contentsline {subsection}{\numberline {H.1}Components of self-supervised learning}{38}{subsection.H.1}%
\contentsline {subsection}{\numberline {H.2}A ResNet-18 recipe for BYOL}{39}{subsection.H.2}%
\contentsline {paragraph}{Hyperparameters}{39}{section*.43}%
\contentsline {subsection}{\numberline {H.3}A Vision Transformer recipe for BYOL}{39}{subsection.H.3}%
\contentsline {paragraph}{Hyperparameters}{39}{section*.45}%
\contentsline {paragraph}{Additional background}{39}{section*.47}%
\contentsline {subsection}{\numberline {H.4}The role of Batch Normalization and Layer Normalization in BYOL with ViTs}{40}{subsection.H.4}%
\contentsline {subsection}{\numberline {H.5}Longer training duration with incremental Progressive Scaling}{41}{subsection.H.5}%
\contentsline {subsection}{\numberline {H.6}Building intuition around Progressive Scaling and momentum sensitivity}{41}{subsection.H.6}%
\contentsline {subsection}{\numberline {H.7}Compute usage for ViT BYOL investigation}{42}{subsection.H.7}%
\contentsline {subsection}{\numberline {H.8}ResNet-18 hyperparameter sensitivity analysis}{43}{subsection.H.8}%
\contentsline {paragraph}{Base teacher momentum}{43}{section*.56}%
\contentsline {paragraph}{Base learning rate}{44}{section*.58}%
\contentsline {subsection}{\numberline {H.9}ResNet-18 additional scaling analysis}{45}{subsection.H.9}%
\contentsline {subsection}{\numberline {H.10}Scaling a ResNet-50 BYOL using LARS and Progressive Scaling}{46}{subsection.H.10}%
\contentsline {subsection}{\numberline {H.11}Preventing collapse phenomena in DINO at scale}{47}{subsection.H.11}%
\contentsline {paragraph}{Hyperparameters}{48}{section*.63}%
\contentsline {paragraph}{Results}{48}{section*.65}%
\contentsline {paragraph}{Compute}{51}{section*.71}%
\contentsline {section}{\numberline {I}Additional details on numerical stability}{51}{appendix.I}%
\contentsline {section}{\numberline {J}Contributions}{52}{appendix.J}%
\contentsline {paragraph}{Preliminary work}{52}{section*.74}%
\contentsline {paragraph}{EMA scaling rules for constant gradients}{52}{section*.75}%
\contentsline {paragraph}{EMA approximation theorems with SDEs}{52}{section*.76}%
\contentsline {paragraph}{Polyak-Ruppert averaging in a simple setting}{52}{section*.77}%
\contentsline {paragraph}{Polyak-Ruppert averaging on image classification}{52}{section*.78}%
\contentsline {paragraph}{Automatic speech recognition}{52}{section*.79}%
\contentsline {paragraph}{Self-supervised image representation learning}{52}{section*.80}%
\contentsline {paragraph}{Progressive Scaling}{53}{section*.81}%
\contentsline {paragraph}{Limiting behavior of Polyak-Ruppert averaging}{53}{section*.82}%
\contentsline {paragraph}{Numerical stability analysis}{53}{section*.83}%
\contentsline {paragraph}{Implementation details}{53}{section*.84}%
\contentsfinish 
