\subsection{Semi-supervised speech recognition via pseudo-labeling}
\label{subsec:semi-supervised}

We continue using the same \gls{asr} model and training pipeline of~\Cref{subsec:supervised-polyakking}.
However, we consider semi-supervised learning via continuous pseudo-labeling where labeled (\tco{}, 100h) and unlabeled (the rest of LibriSpeech, 860h) data are given during training,
and the model \gls{ema} is involved in the overall optimization~\citep{likhomanenko2020slimipl,likhomanenko2022continuous, manohar2021kaizen,higuchi2022momentum}. 
We first pre-train a target model (\emph{student}) on a limited labeled set for a short period (e.g. 20k steps of $B=8\times 290s$\footnote{Note that number of steps is batch size dependent and should be scaled by $1/\kappa$ (see \Cref{app:scaling-toolbox}).}).
Concurrently, the student updates a model \gls{ema} (\emph{teacher}).
After pre-training,
we continue training the student with both labeled and unlabeled data,
with the teacher first transcribing unlabeled data from the batch producing
\glspl{pl}.
These \glspl{pl} are treated by the student as ground-truth transcriptions, and standard supervised optimization is performed.

Compared to Polyak-Ruppert Averaging (\Cref{subsec:supervised-polyakking}), where the model \gls{ema} plays no role in the joint optimization, 
we observe that in \gls{pl} it is \emph{essential} to employ the \gls{ema} Scaling Rule in order to match the model trajectories at scaled batch sizes.
When the \gls{ema} Scaling Rule is not used, \Cref{fig:speech-pl-9999} reveals a significant difference in  \gls{pl} quality trajectory, leading to a higher test \gls{wer}.

For $\kappa > 2$, we found the Adam Scaling Rule does not match perfectly the reference trajectory in the pre-training phase.
This results in a significantly different \gls{pl} quality
at the start of pseudo-labeling (20k steps of $B=8\times 290s$), which affects the training dynamics~\citep{berrebbi2023continuous}.
To alleviate the Adam Scaling Rule mismatch effect for $\kappa > 2$, we postpone the pseudo-labeling until pre-training on labeled data gives similar validation \gls{wer}, see Appendix~\ref{app:speech}. 
With this heuristic, we can match the baseline trajectory with the \gls{ema} Scaling Rule up to $\kappa=8$ (\Cref{fig:speech-pl-9999}).


\emph{In summary, (a) model EMA affects the optimization process of pseudo-labeling in ASR resulting in
the necessity of EMA Scaling Rule to be applied while scaling the batch size; (b) an optimizer scaling rule breakdown results in the EMA Scaling Rule breakdown but this effect can be alleviated by longer pre-training on labeled data having similar PLs quality at the start across different scalings.}

% Figure environment removed

% Figure environment removed
