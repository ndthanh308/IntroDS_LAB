\subsection{Progressive scaling}
\label{subsec:dynamic-batch-scaling}

In \Cref{subsec:self-supervised} we introduced Progressive Scaling (\Cref{def:progressive-scaling}) to test our hypothesis that early in the \gls{byol} training procedure, there are dynamics that are challenging to replicate at larger batch sizes.
To remove ambiguity, in \Cref{alg:progressive-scaling} we provide pseudo-code for how to use Progressive Scaling.

\begin{algorithm}[t!]
\caption{Stochastic Gradient Descent with Progressive Scaling}\label{alg:progressive-scaling}
\begin{algorithmic}
\Require Base learning rate $\eta$, base momentum $\rho$ for base batch size $B$
\Require Initial target model parameters $\rvtheta$ and model \gls{ema} parameters $\rvzeta$
\Require Epochs $E$ and schedule of batch sizes $\mathcal B=B_1,B_2,\ldots,B_{E}$
\Require Loss function $\Ls$
\For{$e$ in $1,2\ldots,E$}
    \State $\hat B \gets \mathcal B[e]$ \Comment{Get current batch size}
    \State $\kappa \gets \hat B/B$      \Comment{Compute scaling factor}
    \State $\hat\eta \gets \kappa \eta$ \Comment{Get scaled learning rate}
    \State $\hat\rho \gets \rho^\kappa$ \Comment{Get scaled momentum}
    \For{$b$ in $1,2\ldots,\text{floor}(E/\hat B)$}
        \State Sample a minibatch of $\hat B$ samples $\mathcal X=\{\vx^{(1)},\ldots,\vx^{(\hat B)}\}$
        \State $\rvtheta \gets \rvtheta - 
        (\hat \eta / \hat B)
        \sum_{x\in\mathcal X} \nabla_{\rvtheta} \Ls(x;\rvtheta,\rvzeta)$ \Comment{SGD Update}
        \State $\rvzeta \gets \hat\rho \,\rvzeta+(1-\hat\rho)\,\rvtheta$ \Comment{EMA Update}
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

In \Cref{alg:progressive-scaling}, the prefactor of the \gls{sgd} update could also have been written $\eta/B$, although an equivalent use of the base momentum is not possible.

Finally, we outline how to extend \Cref{alg:progressive-scaling} to more complex setups, like those presented in 
\Cref{subsec:self-supervised}:
\begin{enumerate}[leftmargin=0.75cm]
    \item Optimizer scaling rules are used appropriately, for example the Adam scaling rule in case of using the Adam optimizer to update parameters $\rvtheta$.
    \item Schedules for hyperparameters are computed using the base hyperparameters, and are then modified by application of the scaling law in epoch (outer) loop.
    \item Schedules for hyperparameters at the \emph{step} rather than epoch level can be achieved in practice through recomputing the schedule and updating the notion of minibatch index appropriately throughout training.
\end{enumerate}
All of the above techniques are used in \Cref{subsec:self-supervised}.
In addition, scheduling batch sizes within epoch is possible, providing one maintains a notion of computation within some fixed continuous time $T_{\text{fixed}}$. 
We did not investigate this scenario.
