\FloatBarrier
\section{Contributions}
\label{sec:attribution}

All authors contributed to writing this paper, designing the experiments, discussing results at each stage of the project.

\paragraph{Preliminary work} 
Derivation of the \gls{ema} Scaling Rule with learning rate $\eta=0$, initial synthetic and self-supervised ImageNet1k experiments done by Dan Busbridge.

\paragraph{EMA scaling rules for constant gradients} Original proof of 
\Cref{eq:scalingRuleSummaryEquation}
and the form of $\delta(\eta,\rho,\kappa)$ in \Cref{eq:scaling-error} done by Eeshan Gunesh Dhekane.
Final proof presented in 
\Cref{app:matrix-calculations}
done by Dan Busbridge, verified by Eeshan Gunesh Dhekane and Pierre Ablin.

\paragraph{EMA approximation theorems with SDEs}
Proofs of validity of EMA Scaling Rule in the SDE limit presented in 
\Cref{subsec:ema-sdes} and 
\Cref{app:ema-approximation-theorem}
done by Pierre Ablin.

\paragraph{Polyak-Ruppert averaging in a simple setting}
Design of noisy parabola setting of \Cref{subsec:toy-experiment} and initial experiments done by Russ Webb.
Design of $\rho^*$-optimality search (\Cref{eq:optimal-momentum}), final experiments and analysis of \Cref{subsec:toy-experiment} and \Cref{app:noisy-parabola} done by Dan Busbridge.

\paragraph{Polyak-Ruppert averaging on image classification}
ResNet-v2 50 reproduction (\Cref{tab:sup-r50-recipe}) and baseline momentum identification done by Jason Ramapuram.
Final ImageNet1k experiments and analysis of \Cref{subsec:supervised-polyakking} and \Cref{app:subsec:polyak-image-classification,subsec:polyak-bn} done by Dan Busbridge.

\paragraph{Automatic speech recognition}
Experiments and analysis of automatic speech recognition using Polyak-Ruppert averaging (\Cref{subsec:supervised-polyakking}) and continuous pseudo-labeling (\Cref{subsec:semi-supervised} and \Cref{app:speech}), as well as design choice of a seed model to start pseudo-labeling (aligning quality of the seed models for different batch size settings before pseudo-labeling process) done by Tatiana Likhomanenko.

\paragraph{Self-supervised image representation learning}
BYOL ResNet 18 recipe (\Cref{tab:byol-r18-recipe}) and experiments on CIFAR10 using \gls{sgd} (\Cref{fig:r18-byol}), and BYOL ResNet 50 experiments using LARS (\Cref{subsec:byol-additional}) done by Dan Busbridge. BYOL ResNet 50 baseline implementation and BYOL \gls{vit} recipe (\Cref{tab:byol-recipe}) done by Jason Ramapuram.
BYOL \gls{vit} exploratory ablations done by Eeshan Gunesh Dhekane and Jason Ramapuram.
All final BYOL \gls{vit} experiments and analysis 
(\Cref{fig:vitb-byol} and 
\Cref{app:byol-progressive-scaling-regimes,app:byol-waterfall,app:byol-vit-ln-vs-bn}) done by Jason Ramapuram. Baseline DINO reproduction done by Dan Busbridge. 
DINO experiments and analysis (\Cref{subsec:dino}) done by Xavier Suau Cuadros.

\paragraph{Progressive Scaling}
Progressive Scaling (\Cref{def:progressive-scaling} and \Cref{alg:progressive-scaling}) 
is proposed by Dan Busbridge based on discussions with Xavier Suau Cuadros, Tatiana Likhomanenko, Jason Ramapuram, Russ Webb, and the authors of \citet{DBLP:conf/nips/MalladiLPA22}. 
Adaptation of progressive scaling to semi-supervised learning in  automatic speech recognition (\Cref{app:speech-progressive}) done by Tatiana Likhomanenko, and to self-supervised learning in vision done by Dan Busbridge and Jason Ramapuram for BYOL (\Cref{fig:r18-byol,fig:vitb-byol} and \Cref{app:byol-waterfall,app:byol-vit-ln-vs-bn,subsec:byol-additional}) and Xavier Suau Cuadros for DINO (\Cref{subsec:dino}).

\paragraph{Limiting behavior of Polyak-Ruppert averaging} Original proof of limiting behavior of Polyak-Ruppert averaging done by Eeshan Gunesh Dhekane.
Final proof presented in 
\Cref{app:asymptoticAnalysis}
done by Dan Busbridge, verified by Eeshan Gunesh Dhekane.

\paragraph{Implementation details} 
Investigations carried out in two distributed, scalable frameworks: Jax for automatic speech recognition experiments, done by Tatiana Likhomanenko; and PyTorch for all remaining investigations, done by 
Dan Busbridge, Xavier Suau Cuadros, Eeshan Gunesh Dhekane, Jason Ramapuram and Russ Webb. 
Initial implementation of progressive scaling experiments for incremental-style strategies (e.g. \Cref{app:byol-waterfall}) showing feasibility done by Jason Ramapuram, and subsequent progressive scaling implementations for smooth strategies (e.g. \Cref{subsec:byol-additional,subsec:dino}) done by Dan Busbridge and Xavier Suau Cuadros.
