\FloatBarrier
\subsection{Preventing collapse phenomena in DINO at scale}
\label{subsec:dino}

Until now, our representatives \gls{ssl} method has been \gls{byol} for reasons discussed in \Cref{subsec:self-supervised}.
Here, we will turn our attention to \gls{dino} \citep{DBLP:journals/corr/abs-2104-14294},
which has the update rule presented in \Cref{def:dino}.
\begin{definition}[DINO Update] 
     \gls{dino} learns unsupervised features by 
     matching predictions over emergent pseudo-labels of a student backbone and head $f(\,\cdot\,;\rvtheta)$ 
     to those of an \gls{ema} teacher $f(\,\cdot\,;\rvzeta)$ through a 
     cross-entropy guided distillation procedure.
    \gls{dino} has a additional centering procedure, which is a form of batch normalization with momentum $\rho_c=0.9$ which we do not scale using the \gls{ema} Scaling Rule. 
    The update for the parameters of \gls{dino} is
    \begin{align}
    \rvtheta_{t+1}
    &=
    \rvtheta_t - \eta \times \frac1B
    \sum_{x\in\sB} \nabla_{\rvtheta} \Ls(x;\rvtheta_{t},\rvzeta_{t}, \rvc_t)
    \\
    \rvzeta_{t+1}
    &=
    \rho \,\rvzeta_t + (1-\rho)\,\rvtheta_{t+1}
    \\
    \rvc_{t+1}
    &= 
    \rho_{c} \, \rvc_t + (1 - \rho_{c}) \,\E_{x^\prime} \rvzeta(x^\prime) \\
    \text{with} \;\; \Ls(x;\rvtheta_{t},\rvzeta_{t}, \rvc_t)
    &=
    H\big( f(x_1, \rvtheta_{t}), f(x_2, \rvzeta_{t}) - \rvc_{t} \big)  + (x_1\leftrightarrow x_2),
    \end{align}
    where $H(\va,\vb)\equiv - \sum_{m=1}^M p_m(\va)\,\log p_m(\vb)$
    is the cross-entropy between categorical distributions over $M$ (emergent pseudo-)classes given logits $\va,\vb\in\R^{M}$, 
    $x_1$ and $x_2$ are two views of a single variate $x$, often produced by augmentations,
    and $x_1\leftrightarrow x_2$ denotes symmetrization over $x_1$ and $x_2$.
    \label{def:dino}
\end{definition}
In practice, \gls{dino} employs multi-crop \citep{DBLP:journals/corr/abs-2104-14294}.
We omit this detail for clarity of presentation, although we \emph{do} use multi-crop in the experiments that follow.

Our interest \gls{dino} is due to the  difficulty in its optimization\footnote{For an example, see
\url{https://github.com/facebookresearch/dino/issues/43\#issuecomment-881453515}.}, and in particular, preventing collapse phenomena in \gls{dino} at batch sizes above 1024, which is an open research problem.
In this section, we will show that a combination of the \gls{ema} Scaling Rule (\Cref{def:ema-sr}) and 
Progressive Scaling (\Cref{def:progressive-scaling}) enable training of \gls{dino} beyond batch size 1024 without sacrificing performance.

\paragraph{Hyperparameters} Base hyperparameters are presented in \Cref{tab:dino-hp}.

\compute

\begin{table}[t]
  \caption{\gls{dino} ViT-B/16 Training hyperparameters.}
  \label{tab:dino-hp}
  \centering
  \small
  \begin{tabular}{lcccc}
    \toprule
    & \gls{dino} ViT-B/16 \\
    \midrule
    CIFAR10 Linear Probe Top-1 ($\rho_B=0.996$) & 85.38\%  \\
    CIFAR10 Linear Probe Top-1 ($\rho_B=0.992$) & 86.96\%  \\
    \midrule
    Weight initialization & \texttt{trunc\_normal(.02)}  \\
    Normalization    & Layer Norm  \\
    Learning rate schedule & Single Cycle Cosine \\    
    Learning rate warmup (epochs) & 50 \\    
    Learning rate minimum value & $1\times10^{-6}$ \\        
    Training duration (epochs) & 280 \\    
    Optimizer & AdamW \\    
    Optimizer scaling rule & Adam \\
    Base ($\beta_1$, $\beta_2$) & (0.9, 0.95) \\
    Base learning rate & $3\times 10^{-4}$  \\
    Base batch size ($B$) & 1024  \\
    Base teacher momentum ($\rho_B$) & 0.992 or 0.996  \\
    Base weight decay & 0.04 \\
    Weight decay scaling rule & Linear \\
    Weight decay skip bias & Yes \\
    Center Momentum & 0.9 \\
    Center Momentum Scaling Rule & None \\
    Precision & \texttt{bf16} \\
    Augmentation stack & \texttt{DINO multi-crop} \citep{DBLP:conf/nips/CaronMMGBJ20} \\   
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Results}
In \Cref{fig:dino-cifar10-0.996,fig:dino-cifar10-0.992} we show the results obtained training \gls{dino} on CIFAR-10 with $\rho_B=0.996$ and $\rho_B=0.992$ respectively at the reference batch size of 1024. 
We employ smooth Progressive Scaling (\Cref{def:progressive-scaling}) between epochs 120 and 180.

At batch size 2048, the training loss matches the reference \emph{only} when the \gls{ema} Scaling Rule is applied, whereas the run \emph{without} the scaling rule diverges from the reference. 
The impact of this divergence is emphasized as we consider the larger batch size of 4096.
Here. there is also a gap \emph{with} the \gls{ema} Scaling Rule, however is approximately three times smaller than the gap \emph{without} the \gls{ema} Scaling Rule.

Additionally, we observe that using $\rho_B=0.992$ yields higher Top-1 accuracy over $\rho_B=0.996$, and in our experiments, using the \gls{ema} Scaling Rule \emph{always} performs better in terms of linear probe performance than not using the scaling rule.

% Figure environment removed

% Figure environment removed

\FloatBarrier

In \Cref{fig:dino-cifar10-0.996-hp} we show how the hyperparameters $\rho$, $B$ and learning rate change with the progressive scaling in \Cref{def:progressive-scaling}.

% Figure environment removed

\FloatBarrier
We also attempted to use a sharp batch size transition (\Cref{fig:dino-cifar10-0.992-step,fig:dino-cifar10-0.992-hp-step}), which leads to the collapse pheonomena observed in prior work. 
This collapse happens with and without the \gls{ema} Scaling Rule.
We suspect this is due to dynamics specific to \gls{dino}'s early phase that are even more challenging to replicate under discretization than those of \gls{byol}.

% Figure environment removed

% Figure environment removed

\ifthenelse{\equal{\anonymous}{0}}{
\input{body/05_06_ssl_dino_compute}
}{\compute}


Our results in this section show it is possible to scale \gls{dino} to large batch sizes \emph{without} sacrificing performance by using \emph{both} the \gls{ema} Scaling Rule and Progressive Scaling, providing the batch size schedule of Progressive Scaling is not sudden. 
