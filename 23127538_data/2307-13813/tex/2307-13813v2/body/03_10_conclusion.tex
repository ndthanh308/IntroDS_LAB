\section{Conclusion}
\label{sec:conclusion}






We have extended the \gls{sde} approximations to cover scenarios where a model \gls{ema} is present and can contribute to the model objective.
From this approximation we derive an \gls{ema} Scaling Rule: one should exponentiate the momentum of the \gls{ema} update to the power of $\kappa$ when changing the batch size by a factor of $\kappa$, as well as scaling other optimizer hyperparameters appropriately.
Furthermore, the EMA Scaling Rule allows the extension of scaling rules to semi/self-supervised methods which rely on \gls{ema} and are sensitive to the choice of the momentum.
We have conducted a wide range of empirical verifications of our scaling rule, from supervised model tracking (i.e. Polyak-Ruppert averaging) in speech and vision domains, pseudo-labeling in speech, and image representation learning.
In almost all scenarios, using the \gls{ema} Scaling Rule
enabled matching of training dynamics under batch size modification, whereas not using it resulted in significant differences in optimization trajectories. For example, we were able to scale \gls{byol} \gls{ssl} to a batch size of 24,576 without any performance loss \emph{only} when by the \gls{ema} Scaling Rule.

While learning rate scaling rules are relatively commonplace in practical \gls{ml} use, the treatment of \gls{ema} is typically decoupled from optimization.
With this work, we aim to bring the role of the \gls{ema} to the forefront when considering the dynamics of optimzation, and that going forward, the momentum will be treated and scaled correctly, in the same way that the learning rates (and other optimizer hyperparameters) are scaled.
