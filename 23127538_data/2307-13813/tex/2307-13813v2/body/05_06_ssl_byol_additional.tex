
\FloatBarrier
\subsection{Scaling a ResNet-50 BYOL using LARS and Progressive Scaling}
\label{subsec:byol-additional}

Here we investigate whether Progressive Scaling and the \gls{ema} Scaling Rule can be used in practice where there is no known optimizer \gls{sde} approximation.
We use the default 300 epoch configuration for \gls{byol} \citep{DBLP:conf/nips/GrillSATRBDPGAP20} in \Cref{fig:r50-byol}.
We see that although trajectories during training do not match, we are able to match or surpass the linear probe performance of the \gls{byol} baseline at the larger batch size if 32,768.
\emph{This indicates that the contributions of our work have practical utility beyond the theoretical constraints.}

% Figure environment removed

\ifthenelse{\equal{\anonymous}{0}}{
\input{body/05_06_ssl_byol_lars_compute}
}{\compute}
