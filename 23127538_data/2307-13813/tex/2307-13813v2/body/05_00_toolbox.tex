\section{The scaling toolbox: practical methods for enabling systematic scaling}
\label{app:scaling-toolbox}

There are many different components involved in preserving optimization dynamics at different batch sizes. 
In this appendix we collect into a single place the different concepts and values that we found useful in practice, in an attempt to make the practice of scaling as accessible as possible.

\subsection{The continuous time/SDE perspective}
\label{sec:app-sde-perspective}

Here we discuss the mindset difference required when trying to preserve training dynamics.
In \gls{ml} we typically use stochastic optimization, leading us to think of the optimization in terms of \emph{performing updates}, or \emph{stepping the optimizer}.
This notion has become more common in the era of large datasets, where it may be the case that we only see a fraction of the dataset during optimization.

For dynamics preservation under scaling, we suggest that it is simpler to consider the \emph{amount of data} seen by the training process, or alternatively, the amount of \emph{continuous time} in the discretization of \glspl{sde} view.
The reason is the following.
The \gls{sde} scaling rule results 
(\Cref{def:ema-sr}, \cite{li2019stochastic,DBLP:conf/nips/LiMA21,DBLP:conf/nips/MalladiLPA22}) follow from showing that different discretizations of the \gls{sde} are close to that \gls{sde}, providing we appropriately scale hyperparameters (see \Cref{subsec:ema-sdes}).
Each of these discretizations shares the \emph{total continuous time} $T=\hat\eta\times \widehat {N}_{\text{iter}}$\footnote{This is in the case of \gls{sgd}, for RMSProp and Adam one should use $T=\hat{\eta}^2\times \widehat {N}_{\text{iter}}$ \citep{DBLP:conf/nips/MalladiLPA22}.} of the underlying \gls{sde}, but each discretization has a \emph{different} number of iterations $\widehat {N}_{\text{iter}}={N}_{\text{iter}}/\kappa $.

This perspective is already adopted, perhaps by accident in some domains.
For example, in \gls{cv}, 
it is typical to compare model performance after optimization on ImageNet1k after a \emph{number of epochs},
whilst also specifing a learning rate warmup after a \emph{number of epochs}.
This transforms the schedule into the form \emph{wait until the process meets [condition]}, where here \emph{[condition]} is \emph{when the process has seen sufficiently many samples.}

More generally, we can specify any \emph{condition} that is not a property of the discretization procedure itself.
Instead, the discretization procedure should be viewed as a numerical approximation method for the \gls{sde} we are evolving, and the properties of that discretization process (like \emph{number of steps}) are not \emph{of specific interest} in the world view where we do decouple optimization from the batch size.
A specific example of this more general case is present in \Cref{subsec:semi-supervised}, where for scaling $\kappa>2$ we wait until the pre-training \gls{wer} is sufficiently low.

There may be cases where one is working with a setup that is explicitly defined in terms of quantities related to the discretization process.
Indeed, the optimizer hyperparameters are examples of these, and need to be scaled accordingly with $\kappa$.
The other typical example of this is conditions based on the \emph{number of optimizer steps}, rather than the number of epochs.
In this case, these quantities should be scaled to achieve the desired condition in the same amount of time, i.e. as above $\widehat {N}_{\text{iter}}={N}_{\text{iter}}/\kappa$, where ${N}_{\text{iter}}$ is the number of iterations specified at the base batch size $B$. 
Concretely, if training is specified in a number of steps, then doubling the batch size implies you should train for half the number of steps.

\subsection{Scaling rules for optimization}

For ease of reference, we collect all the scaling rules related to batch size modification we are aware of.
We begin with the most well-known, the \gls{sgd} Scaling Rule (\Cref{def:lsr,def:app-sgd-scaling-rule}).
\begin{definition}[\gls{sgd} Scaling Rule]
    When running \gls{sgd} (\Cref{def:sgd}) with batch size $\hat B=\kappa B$,
    use a learning rate $\hat\eta=\kappa\eta$ \citep{DBLP:journals/corr/Krizhevsky14,DBLP:journals/corr/GoyalDGNWKTJH17}. 
    \label{def:app-sgd-scaling-rule}
\end{definition}
The \gls{sgd} Scaling Rule is also known as the Linear Scaling Rule (LSR), although for clarity, this work adopts the naming convention \emph{[Algorithm Name] Scaling Rule},
which means all parameters of those algorithms are appropriately scaled from batch size $B$ to $\kappa B$.

Next, we give the two scaling rules known for the adapative optimizers RMSProp \citep{rmsprop} and Adam \citep{DBLP:journals/corr/KingmaB14} in \Cref{def:rmpsprop-sr} and \Cref{def:adam-sr} respectively.

\begin{definition}[RMSProp Scaling Rule]
    When running RMSProp  \citep{rmsprop} with batch size $\hat B=\kappa B$,
    use a learning rate $\hat\eta=\sqrt\kappa\eta$,
    beta coefficient $\hat\beta=1-\kappa\times(1-\beta)$, and adaptivity parameter
    $\hat\epsilon=\frac\epsilon{\sqrt\kappa}$
    \citep{DBLP:conf/nips/MalladiLPA22}.
    \label{def:rmpsprop-sr}
\end{definition}

\begin{definition}[Adam Scaling Rule]
    When running Adam \citep{DBLP:journals/corr/KingmaB14} with batch size $\hat B=\kappa B$,
    use a learning rate $\hat\eta=\sqrt\kappa\eta$,
    beta coefficients 
    $\hat\beta_1=1-\kappa\times(1-\beta_1)$,
    $\hat\beta_2=1-\kappa\times(1-\beta_2)$, and adaptivity parameter
    $\hat\epsilon=\frac\epsilon{\sqrt\kappa}$
    \citep{DBLP:conf/nips/MalladiLPA22}.
    \label{def:adam-sr}
\end{definition}

Next, we present a contribution of this work, the \gls{ema} Scaling Rule
(\Cref{def:ema-sr,def:app-ema-scaling-rule}), which extends the above scaling rules to allow the presence of a model \gls{ema} which is able to contribute to the overall optimization (see \Cref{app:ema-approximation-theorem,app:matrix-calculations} for derivations).

\begin{definition}[\gls{ema} Scaling Rule]
    When computing the \gls{ema} update (\Cref{def:ema}) of a model undergoing stochastic optimization with batch size $\hat B=\kappa B$,
    use a momentum $\hat\rho=\rho^\kappa$ and scale other optimizers according to their own scaling rules.
\label{def:app-ema-scaling-rule}
\end{definition}

Concretely, if we are using \gls{sgd} in the presence of a model \gls{ema},
\Cref{def:app-sgd-scaling-rule,def:app-ema-scaling-rule} state that we should take 
$\hat\eta=\kappa\eta$ \emph{and} $\hat\rho=\rho^\kappa$ when scaling by $\kappa=\hat B/B$.

The final scaling rule is for weight decay, and follows from the scaling logic discussed in \Cref{sec:app-sde-perspective} and \citet{DBLP:journals/corr/Krizhevsky14}.
If we take the weight decay regularization penalty $\lambda$ defined at batch size $B$, what should the weight decay $\hat\lambda$ be for batch size $\hat B=\kappa B$?
For simplicity, consider $\kappa$ updates of optimization of parameters $\rvtheta_t$ in the presence of weight decay only
\begin{equation}
    \rvtheta_{t+\kappa}
    =\rvtheta_{t+\kappa-1}-\eta\,\lambda\,\rvtheta_{t+\kappa-1}
    =(1 - \eta\,\lambda)\,\rvtheta_{t+\kappa-1}
    =(1 - \eta\,\lambda)^\kappa\,\rvtheta_{t}.
\end{equation}
Therefore, to match the effect of weight decay with a single iteration step, we need to match
\begin{equation}
    1-\hat\eta \,\hat \lambda = (1 - \eta\,\lambda)^\kappa.
\end{equation}
Solving for $\hat\lambda$ and expanding around $\eta\approx 0$ gives
\begin{equation}
    \hat\lambda
    =\frac{1-(1 - \eta\,\lambda)^\kappa}{\hat\eta}
    \approx 
    \frac{\eta}{\hat \eta}\times 
    \kappa \,\lambda
    +\mathcal{O}(\eta).
\end{equation}
This leads to the Weight Decay Scaling Rule (\Cref{def:wd-sr}).

\begin{definition}[Weight Decay Scaling Rule]
    When using weight decay with batch size $\hat B=\kappa B$,
    use a penalty term $\hat\lambda=(\kappa \hat\eta / \eta)\,\lambda$,
    where $\hat\eta$ and $\eta$ represent the scaled and unscaled learning rates of the corresponding optimizer
    \citep{DBLP:journals/corr/Krizhevsky14,DBLP:conf/nips/Li0TSG18,DBLP:conf/iclr/LoshchilovH19}.
    \label{def:wd-sr}
\end{definition}

The Weight Decay Scaling Rule implies that using \emph{linear} scaling for the learning rate $\eta$ then the weight decay penalty is automatically scaled,
and when using \emph{square-root} scaling for the learning rate $\eta$ (e.g. in the case of the Adam Scaling Rule (\Cref{def:adam-sr})) then the weight decay penalty should also be scaled with a \emph{square-root} as is proposed in \citet{DBLP:conf/iclr/LoshchilovH19}.

Finally, we see that if the implementation of weight decay does not have an update scaled by the learning rate, i.e. the update is $\rvtheta_{t+1}=(1-\lambda)\,\rvtheta_t$, then the scaling rule is optimizer-independent, and becomes linear for small weight decay, i.e. $\hat\lambda=\kappa\lambda$,
and for arbitrary $\lambda$ takes the form $\hat\lambda=1-(1-\lambda)^\kappa$.

\subsection{Commonly used values of hyperparameters at different batch sizes}

In the literature it is common to give a base learning rate $\eta$ defined at batch size 256, implicitly using the \gls{sgd} Scaling Rule, even when using the Adam optimizer.
Because the scaling of other optimization hyperparameters was not understood until recently, it is also common to just present these \emph{for the experiment}, e.g. the Adam betas and epsilon, and the \gls{ema} momentum, implicitly defined at the scale of the experiment, for example at batch size 4096.
One way to deal with this in practice is to define a single reference batch size $B$ at which \emph{all} hyperparameters are defined, and then scale from there.
In this case, it is easiest to compute \emph{using linear scaling} the learning rate at the redefined base batch size 
$\eta = \tilde\kappa\,\eta_{\text{orig}}$,
where $\tilde\kappa=B/B_{\text{orig}}$,
and then scale this new reference $\eta$ as $\hat\eta=\kappa\eta$, $\kappa=\hat B/B$, along with e.g. the momentum defined at $B$.

As this process can be slightly frustrating, we have provided tables of typical learning rates in \Cref{tab:common-hparams-lr} and momenta in \Cref{tab:common-hparams-momenta}.

\input{body/05_00_toolbox_hparam_table}

\input{body/05_00_toolbox_progressive_scaling}
