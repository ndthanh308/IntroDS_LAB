\section{Additional proofs}

\subsection{Iterations of SGD + EMA}
\label{app:matrix-calculations}

Here we derive a critical component of the \gls{ema} Scaling Rule, 
the matrix equation of \Cref{eq:scalingRuleSummaryEquation} from which the \gls{ema} Scaling Rule (\Cref{def:ema-sr}) follows.
\begin{theorem}[Iterations of SGD + EMA]
\label{thm:app:sgd-ema-iterations}
Assuming that gradients change slowly 
over iterations of \gls{sgd} (\Cref{def:sgd}) and \gls{ema} (\Cref{def:ema}):
$\nabla_{\rvtheta}\Ls(x;\rvtheta_{t+j},\rvzeta_{t+j})\approx \nabla_{\rvtheta}\Ls(x;\rvtheta_{t},\rvzeta_{t})\approx \rvg$, for 
$j=1,2,\ldots,\kappa$ and
representative gradient $\rvg$,
iterating over $\kappa$ independent minibatches produces model states
\begin{align}
\begin{bmatrix}
\rvtheta_{t+\kappa}
\\
\rvzeta_{t+\kappa}
\\
\rvg
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & -\eta \\
1-\rho & \rho & 0\\
0 & 0 & 1
\end{bmatrix}^\kappa
\cdot 
\begin{bmatrix}
\rvtheta_{t}
\\
\rvzeta_{t}
\\
\rvg
\end{bmatrix}
=
\begin{bmatrix}
\rvtheta_{t}-\eta\,\kappa \,\rvg
\\
\rho^\kappa \, \rvzeta_{t}
+(1-\rho^\kappa) \, \rvtheta_t
+\mathcal O\left(\eta\times \beta_\rho\right)
\\
\rvg
\end{bmatrix}.
\end{align}
\end{theorem}

\begin{proof}
First note that
for matrices of the form
\begin{equation}
    \mA
    =
    \begin{bmatrix}
    1 & 0 & a_{0,2} \\
    1-a_{1,1} & a_{1,1} & 0\\
    0 & 0 & 1
    \end{bmatrix},
\end{equation}
their multiplication follows
\begin{align}
    \mA\,\mB
    &=
    \begin{bmatrix}
    1 & 0 & a_{0,2} \\
    1-a_{1,1} & a_{1,1} & 0\\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 0 & b_{0,2} \\
    1-b_{1,1} & b_{1,1} & 0\\
    0 & 0 & 1
    \end{bmatrix}\nonumber\\
    &=
    \begin{bmatrix}
    1 & 0 & a_{0,2}+b_{0,2} \\
    1 - a_{1,1}\,b_{1,1} & a_{1,1}\,b_{1,1} & (1-a_{1,1})\,b_{0,2}\\
    0 & 0 & 1
    \end{bmatrix},
\end{align}
and
\begin{align}
    \mA\,\mB\,\mC
    &=
    \begin{bmatrix}
    1 & 0 & a_{0,2}+b_{0,2} \\
    1 - a_{1,1}\,b_{1,1} & a_{1,1}\,b_{1,1} & (1-a_{1,1})\,b_{0,2}\\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 0 & c_{0,2} \\
    1-c_{1,1} & c_{1,1} & 0\\
    0 & 0 & 1
    \end{bmatrix}\nonumber\\
    &=
    \begin{bmatrix}
    1 & 0 & a_{0,2}+b_{0,2}+c_{0,2} \\
    1 - a_{1,1}\,b_{1,1}\,c_{1,1} & a_{1,1}\,b_{1,1}\,c_{1,1} & 
    (1-a_{1,1})\,b_{0,2}+(1-a_{1,1}\,b_{1,1})\,c_{0,2}\\
    0 & 0 & 1
    \end{bmatrix}.
\end{align}
By induction
\begin{align}
    \mA^\kappa=
    \begin{bmatrix}
    1 & 0 & \kappa \times a_{0,2} \\
    1 - a_{1,1}^\kappa & a_{1,1}^\kappa & 
    \delta(a_{0,2}, a_{1,1},\kappa)\\
    0 & 0 & 1
    \end{bmatrix},
\end{align}
where
\begin{align}
\delta(a_{0,2}, a_{1,1},\kappa)
=a_{0,2}\,\sum_{i=1}^{\kappa - 1}\left(1-a_{1,1}^i\right)
&=a_{0,2}\left(\kappa-\frac{1-a_{1,1}^\kappa}{1-a_{1,1}}\right), \quad \text{for} \, a_{1,1}\neq 1.
\end{align}
It follows that
\begin{align}
\begin{bmatrix}
1 & 0 & -\eta \\
1-\rho & \rho & 0\\
0 & 0 & 1
\end{bmatrix}^\kappa
=
\begin{bmatrix}
1 & 0 & -\kappa\,\eta \\
1-\rho^\kappa & \rho^\kappa & \delta(\eta,\rho,\kappa)\\
0 & 0 & 1
\end{bmatrix}
\end{align}
where the \gls{ema} Scaling Rule error
\begin{align}
    \delta(\eta,\rho,\kappa)
    &=
    \inParentheses{-\eta}\left(\kappa-\frac{1-\rho^\kappa}{1-\rho}\right)
    \approx
    \inParentheses{-\eta}\left(\kappa-\kappa +\mathcal{O}(\beta_\rho)\right)=0+\mathcal{O}(\eta\times\beta_\rho),
    \label{eq:scaling-error}
\end{align}
where $\beta_\rho\equiv 1 - \rho$ and the approximation is around $\rho=1$.
\end{proof}
