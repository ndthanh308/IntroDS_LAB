\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{natbib}
\usepackage{setspace}
\usepackage[a4paper,includeheadfoot,margin=1in]{geometry}

\def\b{\boldsymbol}
\def\bs{\boldsymbol}
\def\vec{\mathsf{vec}\,}
\def\argmax{\operatorname*{\mathsf{arg\,max}}}
\def\argmin{\operatorname*{\mathsf{arg\,min}}}
\def\sup{\operatorname*{\mathsf{sup}}}
\def\diag{\mathsf{diag}\,}
\def\E{\mathbb{E}}
\def\R{\mathbb{R}}

\newcommand{\mn}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}

\newcommand{\y}{{\bf y}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bU}{{\bf U}}
\newcommand{\x}{{\bf x}}
\newcommand{\X}{{\bf X}}
\newcommand{\Z}{{\bf Z}}
\newcommand{\z}{{\bf z}}
\newcommand{\bb}{{\bf b}}
\newcommand{\beps}{{\bs{\varepsilon}}}
\newcommand{\F}{\mathcal{F}}


\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{definition}{Definition}
%\newtheorem*{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
%\newtheorem{example}{Example}
%\newtheorem*{example*}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}
%\newtheorem{proposition}{Proposition}

%\newtheoremstyle{named}{}{}{\itshape}{}{\bfseries}{.}{.5em}{\thmnote{#3's }#1}

\title{Concentration for high-dimensional linear processes with dependent innovations}
\author{Eduardo F. Mendes\thanks{corresponding author: \texttt{eduardo.mendes@fgv.br}}~\thanks{School of Economics of São Paulo, Fundação Getulio Vargas, Brazil} \and Fellipe Lopes\thanks{School of Applied Mathematics, Fundação Getulio Vargas, Brazil}}
\date{}                     %% if you don't need date to appear
%\email{eduardo.mendes@fgv.br}
\begin{document}
\maketitle

\begin{abstract}
    We develop concentration inequalities for the $l_\infty$ norm of a vector linear processes on mixingale sequences with sub-Weibull tails. These inequalities make use of the Beveridge-Nelson decomposition, which reduces the problem to concentration for sup-norm of a vector-mixingale or its weighted sum. This inequality is used to obtain a concentration bound for the maximum entrywise norm of the lag-$h$ autocovariance matrices of linear processes. These results are useful for estimation bounds for high-dimensional vector-autoregressive processes estimated using $l_1$ regularisation, high-dimensional Gaussian bootstrap for time series, and long-run covariance matrix estimation. 
\newline
\noindent
\newline
\textbf{JEL}: C32, C55, C58.
\newline\noindent
\newline
\textbf{Keywords}: high-dimensional time series, linear process, mixingale, sub-Weibull, autocovariance.
\end{abstract}
\maketitle
\onehalfspace

\section{Introduction}
In high-dimensional statistics, the dimension of the model is typically much larger than the available sample size. In these settings, the models are estimated by optimising a regularised loss function. The popular LASSO estimates the parameters in linear regression by minimising the square loss under $l_1$ restriction on the parameter vector. As the number of observations is much smaller than the number of variables, non-asymptotic results play a central role in the area. Concentration inequalities are, then, used to derive non-asymptotic error bounds for parameter estimates in sparse settings, i.e., when the dimension of the parameter vector is larger than the number of observations, but the number of non-zero parameters is smaller. \citet{pBsG2011} and \citet{mw2019hdbook} present an overview of high-dimensional statistical methods and the role of concentration inequalities in the area.

Hoeffding, Bernstein, and Hanson-Wright (-type) inequalities are among of the mostly used tools, and they are valid for independent random variables with sub-Gaussian or subexponential tails. Naturally, these inequalities have been extended to dependent settings, such as the Azuma-Hoeffding inequality for martingale differences with bounded variation. Further refinements are also available in the literature. For instance, \cite{fMmPeR2009} extends Bernstein inequality for strong mixing processes; \cite{dn2007} derive Bernstein and Rosenthal type inequalities for a class of weak dependent dependent processes, and \cite{a2015} derive a Hanson-Wright concentration for correlated sub-Gaussian processes, just to cite a few. More recently, attention has turned to relaxing the sub-Gaussian and subexponential tails to a weaker condition, denoted the sub-Weibull tail \citep{gss2021,wlt2020,kc2022}. These variables do not admit moment generating functions and appear naturally when involving products or powers of sub-Gaussian and subexponential random variables. A challenge in high-dimensional time series modelling is to obtain concentration inequalities for dependent processes under weak dependence conditions and heavy tails.  

time series analysis has extensively explored linear processes due to their relevance in representing many models. An example is the VARMA model, which admits a linear representation under certain conditions on the innovation process, and stationary nonlinear models through the Wold representation theorem \citep{tsay2013tsbook}. Typically, innovations are assumed to be independent, allowing for well-understood asymptotic properties \citep{hh1980,ps1992,hl2006}. As an alternative to independence, one has to define the dependence structure of the process.

For example, \cite{wlg2001} shows the weak convergence of partial sums for martingale difference innovations, while \cite{wm2005} establishes the CLT and invariance principle, and \cite{dmp2011} presents maximal inequalities and functional CLT, both for innovations in a class of weak dependent processes. For a comprehensive treatment of multivariate time series, linear representation of non-linear processes, and illustrative examples, please refer to \cite{tsay2013tsbook,hl2006} and \cite{bd2009tsbook}.

In this work, we derive a concentration bound for the $l_\infty$ norm of high-dimensional linear processes, where the innovation process has a mixingale dependence and sub-Weibull tails. This concentration is then used to bound the maximum entry-wise error norm of the lag-$h$ sample autocovariance matrices. Note that we do not require stationarity of the innovation process.

These concentration bounds are used in the derivation of nonasymptotic estimation bounds in high-dimension time series models, such as VARMA models \citep{wbbm2021}, misspecified VAR($p$) models under $l_1$ penalty \citep{wlt2020, mmm2022}, $l_1$ penalised Yule-Walker estimation \citet{hll2015,wt2021}. These inequalities are also fundamental in the derivation of statistical properties of methods for inference on high-dimensional time series: desparsified inference in VAR($p$) models \citep{asw2023}, multiplier bootstrap in high-dimensional time series \citep{kkp2021,asw2023boot}. Finally, the concentration bound for autocovariances is also used in the estimation of long-run covariance matrices \citet{zw2017,bgs2022}. Naturally, all of these papers consider distinct dependence settings, and their proof strategy may vary. 

The paper is organised as follows. In Section \ref{s:basic} we derive a concentration bound for the sup-norm of vector-valued mixingale sequences with sub-Weibull tails. Then, in Section \ref{s:lp}, we define the multivariate linear process and derive the respective concentration bound. We use a algebraic representation of infinite sums, denoted the Beveridge-Nelson decomposition, which allows us to write the linear processes as a sum of individual innovations that can be handled using the concentration inequality in Section \eqref{s:basic}, and a term of smaller order with a sub-Weibull tail. In Section \ref{s:autocov} we derive a concentration for lag-$h$ autocovariances of linear processes. We close the paper in Section \ref{s:discussion}, where we point out some directions for applications and further development.

\subsection{Notation}
Throughout the paper, we use the following notation. For any $x\in\R$ , $(x)_+ = x\vee 0$. For a vector $\bb = (b_1, ..., b_k)'\in\R^k$ and $p\in[1,\infty]$, $ |\bb|_p$ denotes its $\ell_p$ norm, i.e., $|\bb|_p = (\sum_{i=1}^k|b_i|^p)^{1/p}$ for $p\in[1,\infty)$ and $|\bb|_\infty = \max_{1\leq i\leq k} |b_i|$. We also define $|\bb|_0 = \sum_{i=1}^kI(b_i\ne 0)$. For a random variable $X$, $\|X\|_p = (\E|X|^p)^{1/p}$ for $p\in[1,\infty)$  and $\|X\|_\infty = \sup\{a\in\R:\Pr(|X|\ge a)=0\}$.
For a $m\times n$ matrix $\bs{A}$ with elements $a_{ij}$, we denote $\mn{\bs{A}}_1 = \max_{1\le j\le n}\sum_{i=1}^m|a_{ij}|$, $\mn{\bs{A}}_\infty = \max_{1\le i\le m}\sum_{j=1}^n|a_{ij}|$, the induced $l_\infty$ and $l_1$ norms respectively, and the maximum elementwise norm $\mn{\bs{A}}_{\max} = \max_{i,j} |a_{ij}|$. Also $\Lambda_{\min}(\b A)$ and $\Lambda_{\max}(\b A)$ denote the minimum and maximum eigenvalues of the square matrix $\b A$, respectively. We shall use $c_1$, $c_2$, ... as generic constants that may change values each time they appear. Vector $\bs{e}_i$ is a canonical basis vector and may change dimension every time it appears.

\section{Basic concepts and first concentration inequality}\label{s:basic}
In this section, we present the dependence structure and tail conditions used in the paper. We first characterise the sub-Weibull random variables and the sub-Weibull norm of the maximum of $n$ random variables. Then, given a stochastic process in $\R^n$ with a finite $L_p$ norm, we characterise a form of dependence denoted mixingale, in which the $L_p$ distance between marginal and conditional expectations decreases as we condition further in the past. Finally, we develop a concentration inequality for the $\ell_\infty$ vector-norm (sup norm) of the sums of dependent random variables based on \citet{jiang2009}. 

\subsection{Sub-Weibull random variables}
Sub-Weibull random variables have previously been studied in a systematic way in \cite{wlt2020, vgna2020, gss2021, zw2022, kc2022}, and has also appeared in the context of entries of a random matrix in \cite[Condition $C0$]{tv2013}. Sub-Weibull random variables are characterised by a wide range of tails, including heavy tails for which the moment generating function does not exist. By focussing on this class of random variables, we can extend our analysis to a wider range of applications and settings.

A sub-Weibull random variable is traditionally defined in terms of its tails;
\begin{definition}[Sub-Weibull($\alpha$) random variable]
    Let $\alpha>0$. A sub-Weibull($\alpha$) random variable $X$ satisfies
    \[
    \Pr(|X|>x)\le 2\exp\{(x/K)^\alpha\},    
    \]
    for all $x>0$ and some $K>0$.
\end{definition}
Nevertheless, there are equivalent definitions in terms of moments and moment generating functions of a $|X|^\alpha$ and Orlicz (quasi-) norms. Let $\psi_\alpha(\cdot) = \exp(x^\alpha)-1$ for $\alpha>0$ and denote the (quasi-)norm 
\[
    \|\cdot\|_{\psi_\alpha} := \inf\{c>0:\E\psi_\alpha(|\cdot|/c)\le 1\}.
\]
It follows that $K=\|X\|_{\psi_\alpha}$ in the definition above. This definition is shown to be equivalent to requiring that $\sup_{p>0}p^{-1/\alpha}\|X\|_p < K$ for some constant $K$. In particular, the following equivalence will be useful \citep[Lemma A.2]{gss2021}. Let $d_\alpha := (e\alpha)^{1/\alpha}/2$ and $D_\alpha := (2e(\alpha\wedge 1))^{1/\alpha}$:
\begin{equation}
    d_\alpha \sup_{p>0}p^{-1/\alpha}\|X\|_p \le \|X\|_{\psi_\alpha}\le D_\alpha \sup_{p>0}p^{-1/\alpha}\|X\|_p.   
\end{equation}
The (quasi-)norm $\|\cdot\|_{\psi_\alpha}$ is an example of an Orlicz (quasi-)norm. For $\alpha\ge 1$, $\|\cdot\|_{\psi_\alpha}$ is a norm, but $\psi_\alpha(\cdot)$ is not convex for $\alpha<1$. Nevertheless, some properties hold for all $\alpha$:
\begin{enumerate}
    \item $\|X\|_{\psi_\alpha} = 0$ if and only if $\Pr(X=0)=1$;
    \item for any constant $c$, $\|cX\|_{\psi_\alpha} = |c|\|X\|_{\psi_\alpha}$;
    \item $\|X+Y\|_{\psi_\alpha} \le C_\alpha \left(\|X\|_{\psi_\alpha} +  \|Y\|_{\psi_\alpha}\right)$, where $C_\alpha := 2^{1/\alpha}$ if $\alpha<1$ and $C_\alpha:= 1$ if $\alpha\ge 1$ \citep[Lemma A.3]{gss2021}.
\end{enumerate}
A direct consequence of this result is that $\|\cdot\|_{\psi_\alpha}$ is \textit{nearly} convex in a sense that
\[
    \|a X + (1-a) Y\||_{\psi_\alpha} \le C_\alpha \left[a\|X\|_{\psi_\alpha}+(1-a)\|Y\|_{\psi_\alpha}\right].
\]
Another consequence is the concentration that follows after a simple modification of \citet[Lemma 2.2.2][]{vVjW1996book}. Let $X_1,...,X_n$ be sub-Weibull random variables, the maximum of these random variables is also sub-Weibull($\alpha$) with:
\begin{equation}\label{eq:maxbndmod}
\|\max_i |X_i|\|_{\psi_\alpha} \le c_1 \psi_\alpha^{-1}(2n)\max_i \|X_i\|_{\psi_\alpha},   
\end{equation}
where $c_1 = 2/\log(1.5)$. An important tail bound that will be used throughout the article follows after Markov's inequality:
\begin{equation}\label{eq:orlicz-tail}
    \Pr\left(\max_{1\le i\le n}|X_i| > x\right) \le \exp\left(-\frac{x^\alpha}{(c_1\max_{1\le i\le n}\|X_i\|_{\psi_\alpha})^\alpha \log(1+2n)}\right).
\end{equation}

This definition extends to vector-valued random variables.
\begin{definition}[Sub-Weibull($\alpha$) random vector]
    Let $\alpha>0$. A random vector $\X\in\R^n$ is a sub-Weibull($\alpha$) random vector if all its scalar projections are sub-Weibull($\alpha$), i.e., $\|\X\|_{\psi_\alpha}:=\sup_{\bb'\bb\le 1}\|\bb'\X\|_{\psi_\alpha}<\infty$.
\end{definition}
It follows from the definition that the individual elements of a sub-Weibull random vector are also sub-Weibull of the same order.

\subsection{Mixingales}
We characterize the dependence in the process using moments and conditional moments of the series. This type of dependence is very weak in the sense that it only requires the conditional moments to converge to the marginals in $L_p$ as we condition further in the past. Some classical measures of dependence, such as strong mixing, imply mixingale dependence.
\begin{definition}[Mixingale]
    Let $\{X_t\}$ be a causal stochastic process and $\{\mathcal{F}_t\}$ be an increasing sequence of $\sigma$-fields in such a way that $X_t$ is $\mathcal{F}_t$ measurable. The process $\{X_t\}$ is an $L_p$-mixingale process with respect to $\{\mathcal{F}_t\}$ if there exists a decreasing sequence $\{\rho_m\}$ and a constant $c_t$ satisfying  
    \[
    \left\|\E[X_t|\mathcal{F}_{t-m}]-E[X_t]\right\|_p \le c_t\rho_m.    
    \]
\end{definition}
Let $\{\F_t\}$ denote an increasing sequence of $\sigma$-algebras and $\{\X_t\}$ denote a stochastic process taking values on $\R^n$ such that $\X_t$ is $\F_t$-measurable for each $t$. The power of mixingales comes from the telescoping sum representation. In simple terms, we can write any random variable as a sum of martingale difference terms and a conditional expectation. Using a telescoping sum representation we obtain \citep[Section 16.2]{jD1994},
\begin{align}
    \X_t -\E[\X_t] &= \sum_{i=1}^{m}(\E[\X_t|\mathcal{F}_{t-i+1}] - \E[\X_t|\F_{t-i}] ) + (\E[\X_t|\F_{t-m}]) - \E[\X_t]) \nonumber \\
    &= \sum_{i=1}^m V_{i,t} + (\E[\X_t|\F_{t-m}]) - \E[\X_t]),\label{eq:telescoping}
\end{align}
where $V_{i,t} = \E[\X_t|\mathcal{F}_{t-i+1}] - \E[\X_t|\F_{t-i}]$ is a martingale difference process, by definition. Hence, 
\begin{equation}\label{eq:sumXt}
    \sum_{t=1}^T \left(\X_t-\E[\X_t]\right) = \sum_{i=1}^m \left(\sum_{t=1}^T V_{i,t}\right) + \sum_{t=1}^T \E[\X_t - \E(\X_t)|\F_{t-m}].
\end{equation}
In the following lemma we bound the sup norm of the last term in \eqref{eq:sumXt} using the mixingale property.
\begin{lemma} Let $\{\X_t = (X_{1t},...,X_{nt})'\}$ be a causal stochastic process taking values on $\R^n$ and $\{\F_t\}$ an increasing sequence of $\sigma$-fields in such a way that $\X_t$ is $\F_t$ measurable. Suppose that each $\{X_{jt},\F_t\}$ is $L_p$-mixingale with constants $c_{jt}$ and $\{\rho_{jm}\}$ and let $\rho_m = \max_{1\le j\le n}\rho_{jm}$. Then, 
    \[
        \Pr\left(\left|\sum_{t=1}^T \E[\X_t - \E(\X_t)|\F_{t-m}]\right|_\infty>\frac{Tx}{2}\right) \le \frac{2^p}{x^p}n\rho_m^p\bar{c}_T^p,
    \] 
    where $\bar{c}_T = \max_{1\le j\le n}T^{-1}\sum_{t=1}^Tc_{jt}^p$.
    \label{l:bnddepmixingale}
\end{lemma}

\subsection{Concentration inequality}
We are interested in bounding the $\ell_\infty$ vector norm of $\sum_{t=1}^T (\X_t-\E[\X_t])$. Then, follows from equation \eqref{eq:sumXt} and simple probability manipulation:
\begin{equation}
    \begin{split}
    \Pr\left(\left|\sum_{t=1}^T\left( \X_t-\E[\X_t]\right)\right|_\infty > Tx\right) 
    &\le \sum_{i=1}^m\Pr\left(\left|\sum_{t=1}^T V_{i,t}\right|_\infty > \frac{Tx}{2m}\right) \\
    & + \Pr\left(\left|\sum_{t=1}^T \E[\X_t - \E(\X)|\F_{t-m}]\right|_\infty>\frac{Tx}{2}\right).
    \end{split}
    \label{eq:probbnd}
\end{equation}
The first term requires bounding a martingale process whereas the last term is handle by the mixingale concentration in Lemma \eqref{l:bnddepmixingale}.

\citet{mmm2022} derive a concentration bound for the sup norm of vector-valued martingale processes which we adapt to the sub-Weibull case. The proof of this result is found in the Appendix.
\begin{lemma}[Concentration bounds for high dimensional martingales]\label{l:bndmartingale}
    Let $\{\b\xi_t\}_{t=1,...,T}$ denote a multivariate martingale difference process with respect to the filtration $\mathcal{F}_{t}$ taking values on $\R^n$ and assume that $\max_{i,t}\|\xi_{it}\|_{\psi_\alpha}<c_{\psi_{\alpha}}$. Then,
    \[
    \Pr\left(\left|\sum_{t=1}^T\b\xi_t\right|_\infty > T x\right) \le  2n\exp\left(-\frac{Tx^2}{2M^2+xM}\right) + 4\exp\left(-\frac{M^\alpha}{c_1\log(3nT)}\right),
    \]
    for all $M>0$ and $c_1 = (2c_{\psi_{\alpha}}/\log(1.5))^\alpha$ .
\end{lemma}

We are finally in a position to introduce the first concentration inequality. It is called a \emph{triplex} inequality \citep{jiang2009} as it has three terms: the first one is a Bernstein-type bound, the second one handles the tail, and the third one the dependence. 
\begin{theorem}[First triplex inequality]\label{t:triplex}
    Let $\{\X_t = (X_{1t},...,X_{nt})'\}$ be a causal stochastic process that takes values in $\R^n$ and $\{\F_t\}$ an increasing sequence of $\sigma$-algebras such that $\X_t$ is $\F_t$ measurable. 
    Suppose that each $\{X_{jt},\F_t\}$ is $L_p$-mixingale with constants $c_{jt}$ and $\{\rho_{jm}\}$, and let $\rho_m = \max_{1\le j\le n}\rho_{jm}$ and $\bar{c}_T = \max_{1\le j\le n}T^{-1}\sum_{t=1}^Tc_{jt}$.  Furthermore, suppose that $\max_{i,t}\|X_{it}\|_{\psi_\alpha} < c_{\psi_\alpha} <\infty$.
    Then, for any natural $m$ and scalar $M>0$: 
    \begin{equation}
        \label{eq:triplex}
        \begin{split}
        \Pr\left(\left|\sum_{t=1}^T \X_t-\E[\X_t]\right|_\infty > Tx\right) &\le 2mn\exp\left(-\frac{Tx^2}{8(Mm)^2+2x Mm}\right)\\
        & + 4m\exp\left(-\frac{M^\alpha}{c_1\log(3nT)}\right)+\frac{2^p}{x^p}n\rho_m^p\bar{c}_T^p,
        \end{split}
    \end{equation}
    where $c_1 := (2c_{\psi_{\alpha}}/\log(1.5))^{\alpha}$.
\end{theorem}
\begin{proof}
    The proof follows after the direct application of Lemma \ref{l:bnddepmixingale} and Lemma \ref{l:bndmartingale} to Equation \eqref{eq:probbnd}.
\end{proof}

A simple corollary aims to remove the dependence of the bound on $M$ and $m$. We shall also impose a \emph{sub-Weibull-type} decay to $\rho_m$.
\begin{corollary}[Sub-Weibull Concentration]\label{c:bndsw}
    Under assumptions of Theorem \ref{t:triplex}, suppose that $\rho_m \le e^{-m^\gamma/(pc_\rho)}$ for some $\gamma>0$. Then, for any $\tau>0$ and all $T\ge \log(n)+\tau$:
    \begin{equation}
        \begin{split}
        \Pr\left(\left|\sum_{t=1}^T \X_t-\E[\X_t]\right|_\infty > Tx\right) 
        &\le 2c_\rho^{\frac{1}{\gamma}} (\log(n) + \tau)^{\frac{1}{\gamma}}e^{-\tau}+(2\bar{c}_T)^px^{-p}e^{-\tau}\\
        & + 4c_\rho^{\frac{1}{\gamma}}(\log(n) + \tau)^{\frac{1}{\gamma}} e^{-\frac{(x\sqrt{T})^\alpha}{ c_1\log(3nT)(\log(n)+\tau)^{\frac{\alpha}{2}+\frac{\alpha}{\gamma}} } },
        \end{split}
    \end{equation}
    where $c_1:=(8c_{\psi_{\alpha}}c_\rho^{1/\gamma}/\log(1.5))^{\alpha}$.
\end{corollary}
\begin{proof}
    The proof follows by setting $m := c_\rho^{\frac{1}{\gamma}}(\log(n)+\tau)^{\frac{1}{\gamma}}$ and $mM := x\sqrt{T}/4\sqrt{\log(n)+\tau}$.
\end{proof}
In Corollary \ref{c:bndsw}, $\alpha$ controls the tail weight and $\gamma$ the strength of the dependence. As $\gamma$ increases, we expect the series to be less dependent, and as $\alpha$ increases, it will have lighter tails. In particular, when $\gamma\rightarrow\infty$ and $\alpha = 2$ we recover a sub-Gaussian inequality in rate.

The constants that appear in the inequality have not been optimised. A simpler bound for high-dimensional vectors, i.e. large $n$, follows by setting $\tau=\log(n)$. Let $c_1 = 2(2c_\rho)^{1/\gamma}$, $c_2:=\beta 2^{\frac{7}{2}\alpha+\frac{\alpha}{\gamma}}(c_{\psi_{\alpha}}c_\rho^{1/\gamma}/\log(1.5))^{\alpha}$ and suppose that $T>2\log(n)$ and $T\le n^{\beta-1}/3$ for some $\beta>1$. Then,
\begin{equation}\label{eq:bndsw-simple}
    \Pr\left(\left|\sum_{t=1}^T \X_t-\E[\X_t]\right|_\infty > Tx\right) 
    \le c_1\frac{\log(n)^{1/\gamma}}{n} + \frac{2^p\bar{c}_T}{nx^p} + 2c_1\log(n)^{1/\gamma} e^\frac{(x\sqrt{T})^\alpha}{c_2\log(n)^{1+\frac{\alpha}{2}+\frac{\alpha}{\gamma}}}.
\end{equation}

This inequality sheds some light on the rate of increase in $n$ we can expect so that the right-hand side converges to zero. If $\alpha = 2$, the sub-Gaussian case, we have $\log(n) = o(T^{\gamma/(2\gamma+2)})$, in the sub-Exponential case, $\alpha=1$, $\log(n) = o(T^{\gamma/(3\gamma+2)})$, and in the case with a heavy tail with $\alpha = 0.5$, we have $\log(n) = o(T^{\gamma/(5\gamma+2)})$. In all cases, a strong dependence will guide the rate. If $\gamma$ is very large, the rates are, respectively, close to $o(T^{1/2})$, $o(T^{1/3})$, and $o(T^{1/5})$, which are the same rates obtained in Lemma \ref{l:bndmartingale}, shown to be tight in \cite{xFiGqL2012}. This concentration will be used in the next sections to handle more complex stochastic processes. 



%-----------------------------------------------------------------------------------------
% Linear Processes
%-----------------------------------------------------------------------------------------
\section{Linear processes with dependent innovations}\label{s:lp}
% define the multivariate liner process
In this section, we extend the concentration inequality in Theorem \ref{t:triplex} to linear stochastic processes with sub-Weibull tails. We first define the multivariate linear process followed by a useful decomposition, denoted the Beveridge-Nelson (BN) decomposition \citep{bn1981}. The BN decomposition is an algebraic decomposition of the linear filter that has been widely used in statistics to reduce time series asymptotics to simpler processes like i.i.d., martingale difference, and mixingale processes \citep{ps1992}. Similarly, it allows us to analyse the sup-norm of linear processes using tools and techniques that have been developed for simpler processes.

Let $\{C_j\}$ denote the $n\times n$ matrices and $\{\X_t\}$ denote a centred stochastic process taking values in $\R^n$. The linear process $\Y_t$ is
\begin{equation}\label{eq:lp}
    \Y_t = \sum_{j=0}^\infty C_j\X_{t-j} = C(L)\X_t,
\end{equation}
where $C(L) = \sum_{j=0}^\infty C_jL^j$ is a lag polynomial and $L$ is the lag operator ($L\X_t  =\X_{t-1}$). 

The tail properties $\Y_t$ are not directly inherited by $\X_t$, unless conditions on $\{C_j\}$ are satisfied:
\begin{lemma}
    \label{l:moments}
    Let $\{\b a_j\}$ denote a sequence of elements in $\R^n$ each with finite $l_1$ norm, $\{\Z_t\}$ a sequence of random vectors satisfying $\sup_{|\b a|_1\le 1}\|\b a'\Z_t\|_\psi\le c_t <\infty$ where $\|\cdot\|_\psi$ is some norm and $c_t$ positive constants, and let $W_t = \sum_{j=0}^\infty \b a_j'Z_{t-j}$. Then, $\|W_t\|_\psi \le \sum_{j=0}^\infty |\b a_j|_1c_{t-j}$, provided $\sum_{j=0}^\infty|\b a_j|_1<\infty$.
\end{lemma}

The Beveridge-Nelson (BN) decomposition to expresses the matrix polynomial $C(z)$ as follows:
\begin{equation}\label{eq:bn}
    C(z) = C(1) - (1-z) \widetilde{C}(z),
\end{equation}
where $\widetilde{C}(z) = \sum_{j=0}^\infty \widetilde{C}_jz^j$ and $\widetilde{C}_j = \sum_{k=j+1}^\infty C_k$. we rewrite the linear process as: 
\begin{equation}
    \Y_t = C(L)\X_t = C(1)\X_t - \widetilde\X_t + \widetilde\X_{t-1},
\end{equation}
where $\widetilde\X_t = \widetilde{C}(L)\X_t$. 

If $\X_t$ is sub-Weibull($\alpha$) then each component of $\widetilde\X_t$ is also sub-Weibull($\alpha$), provided $\sum_{j=1}^\infty j|\b e_i'C_j|_1 <\infty$. Let $\{\b e_i = (0,...,0,1,0,...,0)', i=1,...,n\}$ denote the canonical basis vectors for $\R^n$. When applying the lemma \ref{l:moments} to $\max_{i\le n}\|\b e_i'\widetilde\X_t\|_{\psi_\alpha}$, we substitute $\b a_j = \b e_i'\widetilde{C}_j$, which means that we require $\sum_{j=1}^\infty j|\b e_i'C_j|_1 <\infty$ for all $1\le i\le n$, and $\sup_{|\bb|\le 1}\|\bb'\X_t\|_\psi < \infty$. 

The BN decomposition allows us to decompose the linear process into simpler components that we can then analyse using existing techniques and tools. In particular, we will use Theorem \ref{t:triplex} to bound the first term and equation \eqref{eq:orlicz-tail} to bound the remaining terms. 
\begin{theorem}[Concentration inequality for Linear Processes]\label{t:bndlp}
    Let $\{\X_t = (X_{1t},...,X_{nt})' \}$ be a centred sub-Weibull($\alpha$) causal process taking values in $\R^n$, with sub-Weibull constant $c_{\psi_{\alpha}}$, and let $\{\F_t\}$ be an increasing sequence of $\sigma$-algebras such that $\X_t$ is $\F_t$ measurable. Assume that, for each $i=1,...,n$, $\{X_{it},\F_t\}$ is $L_p$-mixingale with positive constants $\{c_{it}\}$ and decreasing sequence $\{\rho_{im}\}$ and write $\bar{c}_T = \max_{1\le i\le n} T^{-1}\sum_{t=1}^Tc_{jt}$ and $\rho_m = \max_{1\le i\le n}\rho_{im}$.

    Write the linear process $\Y_t = C(L)\X_t$, where $\{C_j\}$ is a sequence of square matrices that satisfy $\max_{1\le i\le n}\sum_{j=1}^\infty j|\b{e}_i'C_j|_1\le \tilde{c}_\infty<\infty$ and denote $c_\infty = \mn{C(1)}_\infty$.

    Then, for any $0<a<1$, $T>0$, $M>0$ and $m=1,2,...$, we have 
    \begin{equation}
        \begin{split}
            \Pr\left(\left|\sum_{t=1}^T\Y_t\right|_\infty \ge Tx\right)
            &\le 2mn\exp\left(-\frac{T(ax)^2}{8c_\infty^2(Mm)^2+2c_\infty ax Mm}\right)\\
            & + 4m\exp\left(-\frac{M^\alpha}{c_1\log(3nT)}\right)+\frac{(2c_\infty)^p}{(ax)^p}n\rho_m^p\bar{c}_T^p\\
            & + \exp\left(-\frac{((1-a)Tx)^\alpha}{c_2 \log(1+2n)}\right)
        \end{split}
    \end{equation}
    where $c_1:=(2 c_{\psi_{\alpha}}/\log(1.5))^\alpha$ and $c_2:= (2 c_{\psi_{\alpha}} \tilde{c}_\infty/\log(1.5))^\alpha$
\end{theorem}
\begin{proof}
    Using BN decomposition, $\Y_t = C(1)\X_t + \widetilde\X_{t-1}-\widetilde\X_t$, which means that
    \[
        \sum_{t=1}^T\Y_t = C(1)\sum_{t=1}^T\X_t + \widetilde\X_0 - \widetilde\X_T.
    \]
    Hence, 
    \begin{align*}
        \Pr\left(\big|\sum_{t=1}^T\Y_t\big|_\infty > Tx\right) 
        &=  \Pr\left(\big|C(1)\sum_{t=1}^T\X_t + \widetilde\X_0 - \widetilde\X_T\big|_\infty > Tx\right)\\
        &\le  \Pr\left(\big| C(1)\sum_{t=1}^T\X_t|_\infty > \alpha Tx\right) +  \Pr\left(\big|\widetilde\X_0 - \widetilde\X_T\big|_\infty > (1-\alpha) Tx\right)\\
        &:= A+B
    \end{align*}

    We bound $A$ using the triplex inequality for mixingales and we bound $B$ using the ``Orlicz bound'' in equation \eqref{eq:orlicz-tail}.

    It follows directly that $\left|C(1)\sum_t\X_t\right|_\infty\le \mn{C(1)}_\infty|\sum_t\X_t|_\infty\le c_\infty |\sum_t\X_t|_\infty$. Then, we apply the Theorem \ref{t:triplex}:
    \begin{align*} 
        \Pr\left(\big|\ C(1)\sum_{t=1}^T\Y_t|_\infty > a Tx\right) 
        &\le  \Pr\left(\big|\sum_{t=1}^T\X_t|_\infty > a c_\infty^{-1}Tx\right)\\
        &\le 2mn\exp\left(-\frac{T(ax)^2}{8c_\infty^2(Mm)^2+2c_\infty ax Mm}\right)\\
        & + 4m\exp\left(-\frac{M^\alpha}{c_1\log(3nT)}\right)+\frac{(2c_\infty)^p}{(ax)^p}n\rho_m^p\bar{c}_T^p,
    \end{align*}
     In $B$ we start by writing $\widetilde\X_0-\widetilde\X_T = \widetilde{C}(L)(\X_0-\X_T)$, so that $|\widetilde\X_0-\widetilde\X_T|_\infty = \max_j|\b e_j'\widetilde{C}(L)(\X_0-\X_T)|$. It follows after triangle inequality and Lemma \ref{l:moments} that $\|\b e_j'\widetilde{C}(L)(\X_0-\X_T)\|_{\psi_\alpha} \le 2c_{\psi_{\alpha}}\max_j\sum_{k=1}^\infty|\b e_j'\widetilde{C}_j|\le 2c_{\psi_{\alpha}} \tilde{c}_\infty$ which is bounded by assumption. Hence, we have from equation \eqref{eq:orlicz-tail} that
     \[
        \Pr\left(\big|\widetilde\X_0 - \widetilde\X_T\big|_\infty > (1-a) Tx\right) <  \exp\left(-\frac{((1-a)Tx)^\alpha}{c_2 \log(1+2n)}\right),
    \] 
    with $c_2:=(4c_{\psi_{\alpha}}\tilde{c}_\infty/\log(1.5))^\alpha$
\end{proof}
Closer to our development, the papers \cite{wlt2020} and \cite{mmm2022} both study regularized estimation of high-dimensional vector-autoregressive models. In \cite{wlt2020}, the authors study the estimation of VAR models on sub-Weibull stochastic processes with a strong mixing dependence, while in \cite{mmm2022}, the authors consider a broader class of stochastic processes. The following corollary removes the dependence on $M$, $m$, and $a$.
\begin{corollary}\label{c:bndlp}
    Under assumptions of Theorem \ref{t:bndlp}, let $\rho_m\le \exp({-m^\gamma/(pc_\rho)})$ for some $\gamma>0$. Then, for any $\tau>0$ and $T>\log(n)+\tau$,
    \begin{equation}
        \begin{split}
            \Pr\left(\left|\sum_{t=1}^T\Y_t\right|_\infty \ge Tx\right)
            &\le c_1(\log(n)+\tau)^{1/\gamma}e^{-\tau} + e^{-\frac{(xT)^{\alpha}}{c_2\log(1+2n)}}+(4c_\infty)^p\bar{c}_Tx^{-p}e^{-\tau}\\
            & + 2c_1(\log(n)+\tau)^{1/\gamma}e^{-\frac{(x\sqrt{T})^\alpha}{c_3\log(3nT)(\log(n)+\tau)^{\frac{\alpha}{2}+\frac{\alpha}{\gamma}}}},
        \end{split}
    \end{equation}
    where $c_1:=2(c_\rho)^{1/\gamma}$, $c_2:=(4c_{\psi_\alpha}\tilde{c}_\infty/\log(1.5))^\alpha$, and $c_3:= (8c_{\psi_\alpha}c_\infty c_\rho^{1/\gamma}/\log(1.5))^\alpha$.
\end{corollary}
\begin{proof}
    The result follows after replacing $c_\infty mM = ax\sqrt{T}/4\sqrt{\log(n)+\tau}$, $m = (c_\rho(\log(n)+\tau))^{1/\gamma}$, and $a=1/2$. The lower bound on $T$ requires $(\log(n)+\tau)/T<1$.
\end{proof}

Let $\tau = \log(n)$ and assume that $T\le n^{\beta-1}/3$ for some $\beta>1$:
\begin{equation}\label{eq:bndlp-simple}
    \begin{split}
        \Pr\left(\left|\sum_{t=1}^T\Y_t\right|_\infty \ge Tx\right) 
        &\le c_1\frac{\log(n)^{1/\gamma}}{n}+ e^{-\frac{(xT)^{\alpha}}{c_2\log(1+2n)}}+\frac{(4c_\infty)^p\bar{c}_T}{n x^p} \\
        &\quad + 2c_1\log(n)^{1/\gamma}e^{-\frac{(x\sqrt{T})^\alpha}{c_3 \log(n)^{1+\frac{\alpha}{2}+\frac{\alpha}{\gamma}}}}
    \end{split},    
\end{equation}
where $c_1 :=  2^{1+1/\gamma}(c_\rho)^{1/\gamma}$,  $c_2:=(4c_{\psi_\alpha}\tilde{c}_\infty/\log(1.5))^\alpha$, and $c_3:=\beta 2^{\frac{7\alpha}{2}+\frac{\alpha}{\gamma}}(c_{\psi_\alpha}c_\infty c_\rho^{1/\gamma}/\log(1.5))^\alpha$. Therefore, the same rates as discussed in the previous section are recovered. 

\begin{remark}
    Some comments on the previous results are in order:
    \begin{enumerate}
        \item In Theorems \ref{t:triplex} and \ref{t:bndlp}, the stochastic process $\{\X_t\}$ is not assumed stationary, only that a certain tail property and dependence hold;
        \item if $X_t$ is not centered, we must work with $\Z_t = \X_t-\E[\X_t]$, in which case $\Y_t = \E[Y_t] + \sum_{j=0}^{\infty}C_j \Z_t$;
        \item in Corollary \ref{c:bndlp}, $\bar{c}_T$ is not necessarily bounded and may increase as $T\rightarrow\infty$. In such case, conclusions about the growth rate of $n$ may change;
        \item similarly, constants $c_\infty$ and $\tilde{c}_\infty$ may depend on $n$, in which case the \emph{constant} factor $c_2$ in Theorem \ref{t:bndlp} is not be constant anymore and has to be taken into account;
        \item if $\bar{c}_T<\infty$ for all $T$, then $\frac{\log(n)^{1/2+1/\alpha+1/\gamma}}{\sqrt{T}}\left|{\sum_{t=1}^T\Y_t}\right|_\infty \le O_p(1)$.
    \end{enumerate}
\end{remark}        


%-------------------------------------------------------------------------
% inequality for martingales
%-------------------------------------------------------------------------
A simpler but interesting case occurs when the process $\{\X_t,\F_{t-1}\}$ is a martingale difference. In this case, we may work directly with Lemma \ref{l:bndmartingale}, instead of Theorem \ref{t:triplex}. Equivalently, we take $m=1$ and $\rho_m=0$ in Theorem \ref{t:bndlp}. As we have discussed in the previous section, it is equivalent to taking $\gamma\rightarrow\infty$ which yields better rates for $n$ and a tighter bound. 

\begin{corollary}[Concentration for linear processes on martingale differences]\label{c:bndlp_dm}
    Let $\{\X_t = (X_{1t},...,X_{nt})' \}$ be a centred sub-Weibull($\alpha$) causal process taking values in $\R^n$, with sub-Weibull constant $c_{\psi_{\alpha}}$, and let $\{\F_t\}$ be an increasing sequence of $\sigma$-algebras such that $\X_t$ is $\F_t$ measurable, and assume that $\{\X_t,\F_{t-1}\}$ is a martingale difference process.

    Write the linear process $\Y_t = C(L)\X_t$, where $\{C_j\}$ is a sequence of square matrices that satisfy $\max_{1\le i\le n}\sum_{j=1}^\infty j|\b{e}_i'C_j|_1\le \tilde{c}_\infty<\infty$ and denote $c_\infty = \mn{C(1)}_\infty$.

    Then, for any $T>0$ and $M>0$,
    \begin{equation}
        \begin{split}
            \Pr\left(\left|\sum_{t=1}^T\Y_t\right|_\infty \ge Tx\right)
            &\le 2n\exp\left(-\frac{T(ax)^2}{8c_\infty^2M^2+2c_\infty ax M}\right)\\
            & + 4\exp\left(-\frac{M^\alpha}{c_1\log(3nT)}\right) + \exp\left(-\frac{((1-a)Tx)^\alpha}{c_2 \log(1+2n)}\right)
        \end{split}
    \end{equation}
    where $c_1:=(2 c_{\psi_{\alpha}}/\log(1.5))^\alpha$ and $c_2:= (2 c_{\psi_{\alpha}} \tilde{c}_\infty/\log(1.5))^\alpha$
\end{corollary}
\begin{proof}
    We take $m=1$ and $\rho_m = 0$ in Theorem \ref{t:bndlp}.
\end{proof}

Similarly to Corollary \ref{c:bndlp}, we obtain the following simpler bound:
\begin{equation}
    \begin{split}
        \Pr\left(\left|\sum_{t=1}^T\Y_t\right|_\infty \ge Tx\right)
        &\le 2e^{-\tau} + e^{-\frac{(xT)^{\alpha}}{c_1\log(1+2n)}}+ 4e^{-\frac{(x\sqrt{T})^\alpha}{c_2\log(3nT)(\log(n)+\tau)^\frac{\alpha}{2}}},
    \end{split}
\end{equation}
where $c_1:=(4c_{\psi_\alpha}\tilde{c}_\infty/\log(1.5))^\alpha$, and $c_2:= (8c_{\psi_\alpha}c_\infty/\log(1.5))^\alpha$. Replace $\tau = \log(n)$ and assume that $T\le n^{\beta-1}/3$ for some $\beta>1$
\begin{equation}\label{eq:bndlp-simple_md}
    \begin{split}
        \Pr\left(\left|\sum_{t=1}^T\Y_t\right|_\infty \ge Tx\right)
        &\le \frac{2}{n}+ e^{-\frac{(xT)^{\alpha}}{c_1\log(1+2n)}}+ 4e^{-\frac{(x\sqrt{T})^\alpha}{2^{1+\frac{\alpha}{2}}\beta c_2\log(n)^{1+\frac{\alpha}{2}}}}.
    \end{split}
\end{equation}

%------------------------------------------------------------------------------------------
% covariance of linear processes
%------------------------------------------------------------------------------------------
\section{Empirical lag-$h$ autocovariance matrices}\label{s:autocov}
In this section, we will examine the concentration properties of the empirical lag $h$ autocovariance matrix of sub-Weibull linear processes under the maximum entry-wise norm. This concentration property is important in the LASSO estimation of large vector auto-regressive models and in the use of the multiplier bootstrap. By studying the concentration of the empirical autocovariance matrix, we can better understand the behaviour of these statistical methods in high-dimensional settings. 

Let $\{\X_t\}$ denote a centred, sub-Weibull, causal stochastic process taking values on $\R^n$, and $\Y_t = C(L)\X_t$, $t=1,\cdots,T$, a dependent sequence of random vectors.  Let
\begin{equation}\label{eq:cov}
    \widehat{\Sigma}_T(h) := \frac{1}{T}\sum_{t=h+1}^T\Y_t\Y_{t-h}'\quad\mbox{and}\quad\Sigma(h)_T := \E[\widehat{\Sigma}_T(h)] = \frac{1}{T}\sum_{t=h+1}^T\E[\Y_t\Y_{t-h}'].
\end{equation}  
Our goal is to find a bound for 
\begin{equation}\label{eq:delta}
    \Delta_T(h) = \mn{\widehat{\Sigma}_T(h) - {\Sigma}_T(h)}_{\max} = \left|\vec(\widehat{\Sigma}_T(h) - {\Sigma}_T(h))\right|_\infty,
\end{equation}
which is the maximum element in absolute value of the matrix.

The strategy is to rewrite the autocovariance matrix as sums of linear and sub-Weibull processes and use the above inequalities. Let $C_j=0$ for all $j<0$ and write the outer product of linear processes
\begin{align*}
    \Y_t\Y_{t+h}' 
    & = C(L)\X_t\X_{t+h}'C(L)'\\
    & = \left(\sum_{j=0}^\infty C_{j-h}\X_{t+h-j}\right)\left(\sum_{j=0}^\infty C_j\X_{t+h-j}\right)'\\
    & = \sum_{j=0}^\infty C_j\X_{t-j}\X_{t-j}C_{j+h}'\\
    &\quad + \sum_{k=1}^\infty\sum_{j=0}^\infty C_j\X_{t-j}\X_{t-k-j}'C_{j+h+k}'\\
    &\quad + \sum_{k=1}^\infty\sum_{j=0}^\infty C_{j-h+k}\X_{t+h-k-j}\X_{t+h-j}'C_j'.
\end{align*}
Recall that for 3 matrices $A$, $B$ and $C$, $\vec(ABC') = (C\otimes A)~\vec(B)$, where $C\otimes A$ is the Kronecker product of matrices $A$ and $C$. Furthermore, there is a commutation matrix $\bs{P}$ such that for any matrix $W$, we have $\vec(W') = \bs{P}\vec(W)$.

Let $\bs{\eta}_t(k) = \vec(\X_t\X_{t-k}')$ and $F_{j,k} = C_j\otimes C_k$, then
\begin{align*}
    \Z_{t,k}(h) 
    &:= \vec\left(\sum_{j=0}^\infty C_j\X_{t-j}\X_{t-k-j}'C_{j+h+k}'\right)\\
    & = \sum_{j=0}^\infty (C_{j+h+k}\otimes C_j)~\vec(\X_{t-j}\X_{t-k-j})\\
    & = \sum_{j=0}^\infty F_{j+h+k,j}~L^j\bs{\eta}_t(k)\\
    & = F_{k+h}(L)\bs{\eta}_t(k).
\end{align*}
Now, for some commutation matrix $\bs{P}$
\begin{align*}
    \Z^*_{t+h,k}(-h) 
    &:= \vec\left(\sum_{j=0}^\infty C_{j-h+k}\X_{t+h-k-j}\X_{t+h-j}'C_j'\right)\\
    & = \bs{P} \vec\left[\left(\sum_{j=0}^\infty C_{j-h+k}\X_{t+h-k-j}\X_{t+h-j}'C_j'\right)'\right] \\
    & = \bs{P} \vec\left(\sum_{j=0}^\infty C_j\X_{t+h-j}\X_{t+h-k-j}'C_{j-h+k}'\right) \\
    & = \sum_{j=0}^\infty \bs{P} F_{j-h+k,j}~L^j\bs{\eta}_{t+h}(k)\\
    & = \bs{P} F_{k-h}(L)\bs{\eta}_{t+h}(k)\\
    & = \bs{P} \Z_{t+h,k}(-h)
\end{align*}
Therefore,
\begin{equation} \label{eq:vec}
    \vec(\Y_t\Y_{t-h}') = \Z_{t,0}(h) + \sum_{k=1}^\infty \left[ \Z_{t,k}(h) + \Z^*_{t-h,k}(-h)\right].
\end{equation}

Using the BN decomposition on $\Z_{t,k}(h)$ and $\Z^*_{t,k}(h) = \bs{P}\Z_{t,k}(h)$:
\[
  \Z_{t,k}(h) = F_{k+h}(L)\bs{\eta}_t(k) = F_{k+h}(1)\bs{\eta}_t(k) - (1-L) \tilde{F}_{k+h}(L)\bs{\eta}_t(k),
\]
where
\[
    \tilde{F}_{k+h,j} = \sum_{s=j+1}^\infty F_{k+h+s,s}\quad\mbox{and}\quad\tilde{F}_{k+h}(L) = \sum_{j=0}^\infty\tilde{F}_{k+h,j}L^j.
\] 
Similarly,
\[
    \Z^*_{t-h,k}(h) = \bs{P}\,F_{k-h}(1)\bs{\eta}_{t+h}(k) - (1-L) \bs{P}\,\tilde{F}_{k-h}(L)\bs{\eta}_{t+h}(k) .
\]
Bringing everything together:
\begin{align*}
    \vec(\Y_t\Y_{t+h}') 
    & = F_h(1)\bs{\eta}_t(0) + \sum_{k=1}^\infty F_{k+h}(1)\bs\eta_t(k) + F_{k-h}(1)\bs\eta_{t+h}(k)\\
    &\quad -(1-L)\left\{\tilde{F}_h(L)\bs\eta_t(0)+\sum_{k=1}^\infty\tilde{F}_{k+h}(L)\bs\eta_t(k)\right\}\\
    &\quad -(1-L)\sum_{k=1}^\infty\bs{P}\,\tilde{F}_{k-h}(L)\bs\eta_{t+h}(k).
\end{align*}
Set $\bar{\bs\eta}_{t}(h) = \bs\eta_{t}(h) - \E\bs\eta_{t}(h)$. The centered empirical autocovariances can be written as
\begin{align*}
    \vec(\widehat{\Sigma}_T(h) - \Sigma_T(h)) & =  \frac{1}{T}\sum_{t=1}^{T-h}\vec(\Y_t\Y_{t+h}' - \E\Y_t\Y_{t+h}')\\
    & = \frac{1}{T}\sum_{t=1}^{T-h} \sum_{k=0}^\infty F_{k+h}(1)\tilde{\bs\eta}_t(k)\\
    & \quad + \frac{1}{T}\bs{P}\sum_{t=1}^{T-h} \sum_{k=1}^\infty F_{k-h}(1)\tilde{\bs\eta}_{t+h}(k)\\
    & \quad + \frac{1}{T}\sum_{k=0}^\infty \tilde{F}_{k+h}(L)\tilde{\bs\eta}_0(k) - \frac{1}{T}\sum_{k=0}^\infty \tilde{F}_{k+h}(L)\tilde{\bs\eta}_{T-h}(k) \\
    & \quad + \frac{1}{T}\bs{P}\,\sum_{k=1}^\infty\tilde{F}_{k-h}(L)\tilde{\bs\eta}_h(k) - \frac{1}{T}\bs{P}\,\sum_{k=0}^\infty \tilde{F}_{k-h}(L)\tilde{\bs\eta}_{T}(k).
\end{align*}
The BN decomposition transformed the average of a bilinear process $T^{-1}\sum_t\Y_t\Y_{t+h}'$ into the sum of the averages of the linear process and sub-Weibull random vectors. Hence, the concentration bounds for the empirical covariance matrix can be derived from concentration bounds for linear process and sub-Weibull tail bounds. We write
\begin{align}
    \Pr(\Delta_T(h) > x ) 
    & \le \Pr\left(\left|F_{h}(1)\sum_{t=1}^{T-h}\tilde{\bs\eta}_t(0)\right|_\infty > \frac{Tx}{4}\right)\label{eq:pl0}\\
    &\quad + \Pr\left(\left|\sum_{t=1}^{T-h} \sum_{k=1}^\infty F_{k+h}(1)\tilde{\bs\eta}_t(k)\right|_\infty > \frac{Tx}{4}\right)\label{eq:pl1}\\
    &\quad + \Pr\left(\left|\sum_{t=1}^{T-h} \sum_{k=1}^\infty F_{k-h}(1)\tilde{\bs\eta}_{t+h}(k)\right|_\infty > \frac{Tx}{4}\right)\label{eq:pl2}\\
    &\quad + \Pr\left(\left|\sum_{k=0}^\infty \tilde{F}_{k+h}(L)\tilde{\bs\eta}_0(k)-\sum_{k=0}^\infty \tilde{F}_{k+h}(L)\tilde{\bs\eta}_{T-h}(k)\right|_\infty > \frac{Tx}{8}\right)\label{eq:sw1}\\
    %&\quad + \Pr\left(\left|\sum_{k=0}^\infty \tilde{F}_{k+h}(L)\tilde{\bs\eta}_{T-h}(k)\right|_\infty > \frac{Tx}{12}\right)\\
    &\quad + \Pr\left(\left|\sum_{k=1}^\infty\tilde{F}_{k-h}(L)\tilde{\bs\eta}_h(k)-\sum_{k=1}^\infty \tilde{F}_{k-h}(L)\tilde{\bs\eta}_{T}(k)\right|_\infty > \frac{Tx}{8}\right).\label{eq:sw2}
    %&\quad + \Pr\left(\left|\sum_{k=1}^\infty \tilde{F}_{k-h}(L)\tilde{\bs\eta}_{T-h}(k)\right|_\infty > \frac{Tx}{12}\right).
\end{align}
Let 
\[
    \bs{W}_t = \sum_{k=1}^\infty\tilde{F}_{k-h}(L)\bs{V}_t(k),
\]
where $\{\bs{V}_t(k)\}$ is sub-Weibull($\alpha$) and $\|\delta'\bs{V}_t(k)\|_{\psi_\alpha} <c_{\psi_{\alpha}}$ for every $\delta'\delta = 1$. 
\begin{align*}
    \|\delta'\bs{W}_t\|_{\psi_\alpha}
    &= \left\|\sum_{k=1}^\infty\sum_{j=0}^\infty\delta'\tilde{F}_{k-h,j}\bs{V}_{t-j}(k)\right\|_{\psi_\alpha}\\
    &\le C_\alpha \sum_{k=1}^\infty\sum_{j=0}^\infty\left\|\delta'\tilde{F}_{k-h,j}\bs{V}_{t-j}(k)\right\|_{\psi_\alpha}\\
    &\le C_\alpha  \sum_{k=1}^\infty\sum_{j=0}^\infty\sum_{s=j+1}^\infty\left\|\delta'F_{k-h+s,s}\bs{V}_{t-j}(k)\right\|_{\psi_\alpha}\\
    &\le C_\alpha  \sum_{k=1}^\infty\sum_{j=0}^\infty\sum_{s=j+1}^\infty\mn{C_{k-h+s}\otimes C_s}\sup_{\delta'\delta=1}\left\|\delta'\bs{V}_{t-j}(k)\right\|_{\psi_\alpha}\\
    &\le C_\alpha c_{\psi_{\alpha}} \,\sum_{k=1}^\infty\sum_{j=0}^\infty\sum_{s=j+1}^\infty\mn{C_{k-h+s}}\mn{C_s}\\
    &\le C_\alpha c_{\psi_{\alpha}} \,\sum_{k=1}^\infty\left(\sum_{s=(k-h+1)_+ }^\infty\mn{C_s}^2\right)^{1/2} \cdot \sum_{j=0}^\infty\left(\sum_{s=j+1}^\infty\mn{C_s}^2\right)^{1/2}\\ 
    &\le C_\alpha c_{\psi_{\alpha}} \, \left(\sum_{k=(-h)_+}^\infty (k-h)_+\mn{C_k}^2\right)^{1/2} \cdot \left(\sum_{j=0}^\infty j\mn{C_j}^2\right)^{1/2}\\
    &\le C_\alpha c_{\psi_{\alpha}} \sum_{j=0}^\infty j\mn{C_j}^2.
\end{align*}
Therefore, if $\tilde{c}_{2,\infty}:=\sum_{j=1}^\infty j \mn{C_j}^2<\infty$, $\bs{W}_t$ is sub-Weibull($\alpha$), just like $\bs{V}_t$. It follows that if $\delta'V_t(k)$ is sub-Weibull($\alpha$), then $(\delta'V_t(k))^2$ is sub-Weibull($\alpha/2$). 

We first apply the sub-Weibull tail bound in equation \eqref{eq:orlicz-tail}, for bounding \eqref{eq:sw1} and \eqref{eq:sw2}. Let $\bs{V}_t(k) = \tilde{\bs\eta}_s(k) - \tilde{\bs\eta}_t(k)$:
\[
\|\delta'(\tilde{\bs\eta}_s - \tilde{\bs\eta}_t)|_{\psi_\alpha}\le 2 \|\delta'\tilde{\bs\eta}_s\|_{\psi_\alpha}\le 2 \max_t\|(\bs{b}'\bs\epsilon_t)^2\|_{\psi_\alpha},
\] 
for some $\bs{b}'\bs{b}=1$. Hence:
\begin{equation}\label{eq:bnd-sw1}
    %\Pr\left(\left|\sum_{k=0}^\infty \tilde{F}_{k+h}(L)\tilde{\bs\eta}_0(k)-\sum_{k=0}^\infty \tilde{F}_{k+h}(L)\tilde{\bs\eta}_{T-h}(k)\right|_\infty > \frac{Tx}{8}\right) 
    \mbox{\eqref{eq:sw1}} \le \exp\left(-\frac{(Tx)^{\alpha/2}}{c_2 \log(\sqrt{3}n)}\right),\\
\end{equation}
and, similarly, 
\begin{equation}\label{eq:bnd-sw2}
    %\Pr\left(\left|\sum_{k=1}^\infty\tilde{F}_{k-h}(L)\tilde{\bs\eta}_h(k)-\sum_{k=1}^\infty \tilde{F}_{k-h}(L)\tilde{\bs\eta}_{T}(k)\right|_\infty > \frac{Tx}{8}\right) 
    \mbox{\eqref{eq:sw2}} \le \exp\left(-\frac{(Tx)^{\alpha/2}}{c_2 \log(\sqrt{3}n)}\right),
\end{equation}
where $c_2 := 2(32/\log(1.5) \cdot C_\alpha c_{\psi_{\alpha/2}} \tilde{c}_{2,\infty})^{\alpha/2}$. 

We have to make assumptions about the dependence of the series in order to use the inequalities developed for linear processes. Let $\{\bs\eta_t(k) = (\eta_{1t}(k),...,\eta_{n^2t}(k))'\}$, where $\bs\eta_t(k) = \vec(\X_t\X_{t-k}')$, for $k=0,1,...$, and $\{\F_t\}$ be an increasing sequence of $\sigma$-algebras such that $\X_t\X_{t-k}'$ is $\F_t$ measurable. The processes $\{\eta_{jt}(k),\F_t\}$ are $L_1$ mixingale with constants $c_{jt}$ and decreasing sequences $\rho_{jm}$, for each $k=1,2,...$ and $j=1,...,n^2$. The following bound holds for $k=0,1,2,...$:
\[
    \|\E[\eta_{jt}(k)-\E[\eta_{jt}(k)]|\F_{t-m}]\|_1\le c_{jt}\rho_{jm}.
\]
Suppose that the above mixingale condition holds for $k=0$, $\rho_m := \max_j\rho_{jm} \le e^{-m^\gamma/c_\rho}$, and recall that for a square matrix $A$ and vector $b$, $|Ab|_\infty\le\mn{A}|b|_\infty$. Using \eqref{eq:bndsw-simple} (Corollary \ref{c:bndsw}) on \eqref{eq:pl0}, where $\alpha$ is replaced by $\alpha/2$ and $n$ by $n^2$:
\begin{equation}\label{eq:bnd-pl0}
    \mbox{\eqref{eq:pl0}} \le c_1\frac{\log(n)^{1/\gamma}}{n^2} + \frac{8c_h\bar{c}_T}{n^2x}+2c_1\log(n)^{1/\gamma}e^{-\frac{(x\sqrt{T})^{\alpha/2}}{c_2\log(n)^{1+\alpha/4+\alpha/2\gamma}}},
\end{equation}
where $c_1 := 2(4c_\rho)^{1/\gamma}$, $c_h := \mn{F_h(0)}_\infty = \mn{\sum_{j=0}^\infty C_{j+h}\otimes C_j}_\infty$, and for some $\beta>1$ define below, $c_2 := \beta 2^{1+3\alpha+\alpha/\gamma}(c_{\psi_{\alpha/2}}c_\rho^{1/\gamma}/\log(1.5))^{\alpha/2}$.

Suppose now that the above mixingale condition holds for $k= 1,2,...$, $\rho_m := \max_j\rho_{jm} \le e^{-m^\gamma/c_\rho}$. Using \eqref{eq:bndlp-simple} (Corollary \ref{c:bndlp}) on \eqref{eq:pl1}, where $\alpha$ is replaced by $\alpha/2$, $n$ by $n^2$, $c_\infty:= \max_{1\le k\le h}\mn{\sum_{i=0}^\infty(\sum_{j=i+k}^\infty C_j)\otimes C_i}_\infty$, $\tilde{c}_\infty := \max_{1\le i\le n^2}\sum_{j=1}^\infty j\left|\sum_{k=0}^\infty\bs{e}_i'(C_{j+k}\otimes C_k)\right|_1$:
\begin{equation}\label{eq:bnd-pl1}
    \mbox{\eqref{eq:pl1}} \le c_1\frac{\log(n)^{1/\gamma}}{n^2} + \frac{16c_\infty\bar{c}_T}{n^2x}+2c_1
    \log(n)^{1/\gamma}e^{-\frac{(x\sqrt{T})^{\alpha/2}}{c_2\log(n)^{1+\frac{\alpha}{4}+\frac{\alpha}{2\gamma}}}}+e^{-\frac{(xT)^{\alpha/2}}{c_3\log(n)}}.
\end{equation}
where the above constants are $c_1 := 2^{1+2/\gamma}c_\rho^{1/\gamma}$, $c_2 :=  \beta 2^{1+3\alpha+\alpha/\gamma}(c_{\psi_{\alpha/2}}c_\infty c_\rho^{1/\gamma}/\log(1.5))^{\alpha/2}$, and $c_3:= 3\cdot 2^{2\alpha}(c_{\psi_{\alpha/2}}\tilde{c}_\infty)^{\alpha/2} $. In both cases, the bound holds for all $n$ and $T$, where $T<n^{\beta-1}/3$ for some $\beta>1$, and $T\ge 4\log(n)$. Naturally, we could use Theorem \ref{t:triplex} and Theorem \ref{t:bndlp} to obtain a probability bound that would hold for all $T$ and $n$. A similar bound holds for \eqref{eq:pl2}. 

The following theorem summarises the main result in this section.
\begin{theorem}\label{t:acovmatrix}
    Let $\{\X_t = (X_{1t},...,X_{nt})' \}$ be a centred sub-Weibull($\alpha$) causal process taking values in $\R^n$, and let $\{\F_t\}$ be an increasing sequence of $\sigma$-algebras such that $\X_t$ is $\F_t$ measurable. 

    Write $\bs\eta_t(k) = \vec(\X_t\X_{t-k}')$ and the stochastic process $\{\bs\eta_t(k) = (\eta_{1t}(k),...,\eta_{n^2t}(k))'\}$ for $k=0,1,...$. The processes $\{\eta_{it}(k),\F_t\}$ are $L_1$ mixingale with constants $c_{it}$ and decreasing sequences $\rho_{im}$, for each $k=0,1,...$ and $i=1,...,n^2$. Let $\bar{c}_T = \max_{1\le i\le n^2} T^{-1}\sum_{t=1}^Tc_{it}$ and $\rho_m = \max_{1\le i\le n}\rho_{im}\le e^{-m^\gamma/c_\rho}$.
    
    Finally, let the linear process $\Y_t = C(L)\X_t$, where $\{C_j\}$ is a sequence of square matrices and define the following finite constants:
    \begin{enumerate}
        \item[a. ] $\tilde{c}_{2,\infty}:= \sum_{j=1}^\infty j \mn{C_j}^2$;
        \item[b. ] $c_h := \max_{1\le k\le h}\mn{\sum_{j=0}^\infty C_{j+k}\otimes C_j}_\infty$;
        \item[c. ] $c_\infty:= \max_{1\le k\le h}\mn{\sum_{i=0}^\infty(\sum_{j=i+k}^\infty C_j)\otimes C_i}_\infty$;
        \item[d. ] $\tilde{c}_\infty := \max_{1\le i\le n^2}\sum_{j=1}^\infty j\left|\sum_{k=0}^\infty\bs{e}_i'(C_{j+k}\otimes C_k)\right|_1$.
    \end{enumerate}
    Let $\Delta_T(h)$ be as defined in equations \eqref{eq:cov} - \eqref{eq:delta}. Then, for each $n$ and $T$ that satisfies $T>4\log(n)$ and $3T <n^{\beta-1}$ for some $\beta>1$: 
    \begin{equation}
        \begin{split}
        \Pr(\Delta_T(h) > x) 
        &\le c_1\frac{\log(n)^{1/\gamma}}{n^2} + \frac{c_2\bar{c}_T}{n^2x} + c_3\log(n)^{1/\gamma}e^{-\frac{(x\sqrt{T})^{\alpha/2}}{c_4\log(n)^{1+\frac{\alpha}{4}+ \frac{\alpha}{2\gamma}}}} + 4e^{-\frac{(xT)^{\alpha/2}}{c_5\log(n)}} 
        \end{split},
    \end{equation}
    where the constants $c_1,...,c_5$ depend only on $\beta$, $c_\rho, c_{\psi_{\alpha/2}}, \tilde{c}_{2,\infty}, c_h, c_\infty, \tilde{c}_\infty$, but not on $n$ or $T$. 
\end{theorem}
\begin{proof}
    This result is derived along the Section \ref{s:autocov}. The final bound is found by adding \eqref{eq:sw1} -- \eqref{eq:bnd-pl1} and the equivalent bound for \eqref{eq:pl2}. The constants can be calculated tracing back its values in the text.
\end{proof}

There are two sets of conditions that require further explanation: the conditions in $\{C_j\}$ and the $L_1$ mixingale condition on $\X_t\X_{t-k}$. 

After simple algebraic manipulation, we obtain sufficient conditions for boundedness of (a) -- (d), above. Condition (a) is already in a straightforward form and requires that the singular values of $C_j$ decrease sufficiently fast. Condition (b) is equivalent to $\max_{k\le h,\,1\le i\le n^2} |\bs{e}_i'(\sum_{j=0}^\infty C_{j+k}\otimes C_j)|_1$, which means that if (d) is bounded, so is (b). As for condition (c), let $S_k = \sum_{j=k}^\infty C_j$ and observe that $|\bs{e}_i'(\sum_{l=0}^\infty S_{j=l+k}\otimes C_l)|_1 \le \max_{k,1\le i\le n}|\bs{e}_i'S_k|_1^2 = \max_{k\ge 0}\mn{S_k}_\infty^2$. Finally, let $l$ and $m$ denote natural numbers such that $n\cdot(l-1)+m = i$, $C_j^{r,l}$ denote the $(r,l)$ element of matrix $C_j$ and $(S_{2,j}^{l,r})^2 = \sum_{k=j}^\infty(C_k^{l,r})^2$. The term $|\bs{e}_i'(\sum_{k=0}^\infty C_{j+k}\otimes C_k)|_1 = \sum_{1\le r,s\le n}|\sum_{k=0}^\infty C_{j+k}^{l,r}C_k^{m,s}|$ is bounded by above by $ \sum_{r=1}^nS_{2,j}^{l,r}\cdot\sum_{s=1}^nS_{2,0}^{m,s}$. Therefore, $\max_{1\le i\le n^2}\sum_{j=1}^\infty j\left|\sum_{k=0}^\infty\bs{e}_i'(C_{j+k}\otimes C_k)\right|_1$ is upper bound by $\max_{1\le m\le n}(\sum_{s=1}^nS_{2,0}^{m,s})\cdot \max_{1\le l\le n}(\sum_{r=1}^n\sum_{j=1}^\infty j S_{2,j}^{l,r})$.

Regarding the mixingale condition, suppose that $\{\X_t\}$ is centred, uncorrelated in time, and $L_2$ mixingale. We must consider two situations: $k\le m$ and $k>m$. If $k\le m$, for any two elements $i$ and $j$, $\|\E[X_{it}|\F_{t-m}]X_{j,t-k}\|_1\le \| \E[X_{it}|\F_{t-m}]\|_2\|X_{j,t-k}\|_2$, which means that $\|\E[X_{it}|\F_{t-m}]X_{j,t-k}\|_1\le \|X_{j,t-k}\|_2c_{it}\rho_m$. Assume now that $k>m$. Let $\F = \F_{t}$, $\mathcal{G} = \F_{t-k-m}$, $X = x_{it}$, and $Y=X_{j,t-k}$ in Lemma \ref{l:mixingale_prod} in the Appendix. Then, $\rho_\F = c_{it}\rho_m$ and $\rho_\mathcal{G} = c_{jt}\rho_m$, where $\rho_m \le e^{-m^\gamma/c_\rho}$. It follows after Cauchy-Schwarz and Markov's inequalities:
\begin{align*}
    \||XY|I(|Y-\E[Y|\mathcal{G}]|>M)\|_1
    &\le\|XY\|_2\Pr(|Y-\E[Y|\mathcal{G}]|>M)^{1/2}\\
    &\le \|XY\|_2 M^{-1}\|Y-\E[Y|\mathcal{G}]\|_2\\
    &\le \|XY\|_2\|Y\|_2M^{-1},
\end{align*}
or, equivalently,
\[
    \||X_{i,t}X_{j,t-k}|I(|X_{j,t-k}-\E[X_{j,t-k}|\F_{t-m-k}]|>M)\|_1 \le \|X_{i,t}X_{j,t-k}\|_2\|X_{j,t-k}\|_2 M^{-1}.
\]
Select $M = \rho_m^{-1/2}$ to obtain the bounds
\[
    \|\E[X_{it}X_{j,t-k}|\F_{t-m}]\|_1 \le (c_{it}+c_{it}c_{j,t-k}\rho_m^{3/2}+\|X_{i,t}X_{j,t-k}\|_2\|X_{j,t-k}\|_2) e^{-m^\gamma/2c_\rho}.   
\] 

Therefore, for an adequate choice of constants $c_t$, and any $k>0$ and $m$, $ \|\E[X_{it}X_{j,t-k}|\F_{t-m}]\|_1 \le c_te^{-m^\gamma/2c_\rho}$, which means that $\{\eta_{it}(k),\F_t\}$ is $L_1$-mixingale with the same ``sub-Weibull'' rate. If $k=0$ we directly assume that $\{X_{it}X_{jt},\F_t\}$ is $L_1$ mixingale. This condition was discussed in more detail in \citet{mmm2022}, which also presents examples of processes satisfying it.

If the process $\X_t,\F_{t-1}$ is a martingale difference process, it means that $\E[X_{it}X_{j,t-k}|\F_{t-m}]=0$ for $k=\pm 1, \pm 2, ...$, which is equivalent to taking $\gamma\rightarrow\infty$. However, in such cases, the probability bounds for \eqref{eq:pl1} and \eqref{eq:pl2} are tighter, and follow from Corollary \ref{c:bndlp_dm} and equation \eqref{eq:bndlp-simple_md}:
\begin{equation*}
    \mbox{\eqref{eq:pl1}} \le \frac{2}{n^2} +c_1e^{-\frac{(x\sqrt{T})^{\alpha/2}}{c_2\log(n)^{1+\frac{\alpha}{4}}}}+e^{-\frac{(xT)^{\alpha/2}}{c_3\log(n)}},
\end{equation*}
for constants $c_1$, $c_2$ and $c_3$ that depend only on $\beta$, $c_\infty$, $\tilde{c}_\infty$, and $c_{\psi_{\alpha/2}}$. A similar bound holds for \eqref{eq:pl2} and yields a slightly tighter version of Theorem \ref{t:acovmatrix}. Nevertheless, the process $\{\widetilde{\bs\eta}_t(0),\F_{t-1}\}$ is not martingale difference, and we still need the $L_1$-mixingale condition on the squared process. 

\section{Discussion}\label{s:discussion}

In this paper, we discuss concentration bounds for the sup-norm of vector-valued linear processes. We show that, under summability conditions on the weights, the rates obtained for sums of mixingales are extended for sums of linear processes. This result is interesting in its own right, but also noteworthy in that it does not require stationarity and that the \emph{constants} depending on the weights or the mixingale term may grow with the dimension $n$ of the process or the sample size $T$. Furthermore, the mixingale condition is a familiar condition used in the derivation of asymptotic properties of time series estimators, as well as in the analysis of properties of non-linear models. \citet[Section 3]{mmm2022} shows some examples of processes that satisfy mixingale and tail conditions.

High-dimensional linear processes have appeared in the literature of regularised time series estimation. This article generalises the concentration results in \citet{wlt2020} that requires $\Y_t$ to be $\beta$-mixing, \citet{mmm2022} that requires $Y_t$ to be approximately VAR($p$) process, and \citet{bs2021} that imposes i.i.d. innovations. \citet{mmm2022} could be extended to misspecified linear models, in the sense that the mean is not a approximately sparse VAR model. \citet{bs2021} shows that factor models and generalised factor models admit a linear process representation. We could extend this result to processes with stochastic variance (see \citet[Section 3]{mmm2022}). \citet{asw2023boot,kkp2021} shows validity and how to use the multiplier bootstrap in high-dimensional VAR(p) models.

\citet{kc2022} show that the concentration inequalities derived in this paper can be applied in many statistical settings. The bound on the covariance matrix in Section \ref{s:autocov} can be naturally extended to the case where $\Y_t$ is replaced by $\Y_t-\bar{\Y}$, where $\bar\Y = (1/T)S_T$. These inequalities can be used to derive estimation bounds for covariance estimation in the maximum $k$-sub matrix operator norm and the restricted isometry property. Finally, we could verify the restricted eigenvalue condition and restricted eigenvalue condition for linear time series models. 

We apply our concentration inequality to the estimation of lag-$h$ autocovariances. Nevertheless, we could consider concentration for more general U-statistics. In this case, we replace $\X_{t-i}\X_{t-j}$ by $K(\X_{t-i},\X{t-j})$ where $K(\cdot,\cdot)$ is a symmetric kernel satisfying the measurability, tail and mixingale conditions.

\newpage
\bibliographystyle{apalike}
\bibliography{mybib}
\newpage
\appendix
\section{Auxiliary results and proofs}
\begin{proof}[Derivation of equation \eqref{eq:maxbndmod}]
    This equation follows after a small modification to the proof of \citet[Lemma 2.2.2][]{vVjW1996book}. We will replicate the proof for the sake of completeness.

    We have to show that for $x,y\ge K>0$, $\psi_\alpha(x)\psi_\alpha(y)\le \psi_\alpha(cxy)$ for some $c>0$:
    \begin{align*}
        (e^{x^\alpha}-1)(e^{y^\alpha}-1) 
        &= e^{x^\alpha+y^\alpha}+1-e^{x^\alpha}-e^{y^\alpha}\\
        &\le e^{(1/x^\alpha+1/y^\alpha)\,(xy)^\alpha}-1\\
        &\le \psi\left(\frac{2}{(\log K)^\alpha}\,xy\right),
    \end{align*}
    for all $\alpha >0$ and $K>0$, and with $c = 2/K^\alpha$. It follows that for all $x,y\ge K$, $\psi_\alpha(x/y)\le\psi_\alpha(cx)/\psi_\alpha(y)$. So, for any $C>0$ and $y\ge K$
    \begin{align*}
        \max_i\psi_\alpha\left(\frac{|X_i|}{Cy}\right) 
        &\le \max_i\left[ \frac{\psi_\alpha(c|X_i|/C)}{\psi_\alpha(y)} + \psi_\alpha\left(\frac{|X_i|}{Cy}\right)I\left(\left|\frac{|X_i|}{C}\right|\le K\right)\right]\\
        &\le\sum_{i=1}^n\frac{\psi_\alpha(c|X_i|/C)}{\psi_\alpha(y)} + \psi_\alpha(K).
    \end{align*}
    Let $C=c\max_i\|X_i\|_{\psi_\alpha}$, $y=\psi_\alpha^{-1}(2n)$, $K=(\log 1.5)^{1/\alpha}$ and take expectation on both sides:
    \[
    \E\left[\max_i\psi_\alpha\left(\frac{|X_i|}{Cy}\right)\right] \le \frac{n}{\psi_\alpha(y)}+\psi_\alpha(K) = 1.
    \]
    Therefore, 
    \[
    \|\max_i|X_i|\|_{\psi_\alpha}\le \frac{2}{\log(1.5)} \psi_\alpha^{-1}(2n)\max_i\|X_i\|_{\psi_\alpha}.
    \]
    % The second step is to require that $\psi_\alpha(K)=0.5$, which means that $K=(\log 1.5)^{1/\alpha}$. Now, the only step in the proof that convexity is required is to bound $\psi^{-1}(2m)\le 2\psi^{-1}(m)$, which we leave as is.
\end{proof}
\begin{proof}[Proof of Lemma \ref{l:bnddepmixingale}]
    It follows from the union bound and Markov's inequality that
    \[
        \Pr\left(\left|\sum_{t=1}^T \E[\X_t - \E(\X_t)|\F_{t-m}]\right|_\infty>\frac{Tx}{2}\right) \le n\max_{1\le j\le n}\frac{2^p}{x^p T^p}\E\left|\sum_{t=1}^T \E[X_{jt} - \E(X_{jt})|\F_{t-m}]\right|^p.
    \]
    Applying Loeve's $c_r$ inequality
    \[
        \E\left|\sum_{t=1}^T \E[X_{jt} - \E(X_{j1})|\F_{t-m}]\right|^p \le T^{p-1} \sum_{t=1}^T \E\left|\E[X_{jt} - \E(X_{jt})|\F_{t-m}]\right|^p\le T^p \rho_{jm}^p \bar{c}_T.
    \]
    Result follows directly.
\end{proof}
\begin{proof}[Proof of Lemma \ref{l:bndmartingale}]
    The proof of this lemma follows \cite[Lemma 5]{mmm2022}. Write $\b\xi_t= (\xi_{1t},...,\xi_{nt})'$. The proof follows after applying \cite[Corollary 2.3]{xFiGqL2012}.

    Write $V_k^2(M) = \max_{1\le i\le n}\sum_{t=1}^k\E[\xi_{it}^2I(\xi_{it}<M)|\mathcal{F}_t]$, $X_{ik} = \sum_{t=1}^k\xi_{it}$ and $X_{ik}'(M) = \sum_{t=1}^k \xi_{it}I(\xi_{it}\le M)$. It follows that for $v>0$ and $x>0$,
    \begin{align*}
        \Pr(|\X_T|_\infty>x) &\le \Pr(\exists i,k:\,X_{ik} > x \cap V_k^2(M)\le v^2) + \Pr(V_T^2(M)>v^2)\\
        &\le \Pr(\exists i,k:\,X_{ik}'(M) > x \cap V_k^2(M)\le v^2) + \Pr(V_T^2(M)>v^2)\\
        &\quad + \Pr\left( \max_{1\le i\le n}\sum_{t=1}^k \xi_{it}I(\xi_{it}> M) >0\right)\\
        &\overset{(1)}{\le} n\exp\left(-\frac{(Tx/M)^2}{2((v/M)^2+\frac{T}{3}x/M)}\right) + \Pr(V_n^2(M)>v^2) \\
        &\quad+ \Pr\left(\max_{1\le t\le T} |\b\xi_t|_\infty > M\right)\\
        &\overset{(2)}{\le} n\exp\left(-\frac{Tx^2}{2M^2+Mx}\right) + 2 \Pr\left(\max_{1\le t\le T} |\b\xi_t|_\infty > M\right) .
    \end{align*}
    In $(1)$ we use union bound and \cite[Theorem 2.1]{xFiGqL2012} and in $(2)$ we set $v^2 = T(M^2+\frac{1}{6T}Mx)$ and the following:
    \begin{align*}
        \Pr(V_T^2(M)>v^2)
        &\le \Pr\left(\max_{1\le i\le n}\sum_{t=1}^T\E[\xi_{it}^2I(|\xi_{it}| \le M)|\mathcal{F}_t]\ge v^2\right)\\
        &\quad  + \Pr\left(\max_{1\le i\le n}\sum_{t=1}^T\E[\xi_{it}^2I(\xi_{it}<-M)|\mathcal{F}_t]>0\right)\\
        &\le \Pr\left(\max_{1\le i\le n}\sum_{t=1}^T\E[\xi_{it}^2I(|\xi_{it}| \le M)|\mathcal{F}_t]\ge T(M^2+\frac{1}{6T}Mx)\right)\\
        &\quad + \Pr\left(\max_{1\le t\le T}|\b\xi_t|_\infty>M\right)\\
        &\le \Pr\left(\max_{1\le t\le T}|\b\xi_t|_\infty>M\right),
    \end{align*}
    where in the last line we note that $\sum_{t=1}^T\E[\xi_{it}^2I(|\xi_{it}| \le M)|\mathcal{F}_t] \le TM^2$.

    Now, write $\Pr(|\X_T|_\infty\ge Tx) = \Pr(\max_{i\le n} X_{iT}\ge Tx)+\Pr(\max_{i\le n}(-X_{iT})\ge Tx)$ and apply the above development in both terms.

    Finally, use equation \eqref{eq:orlicz-tail} to conclude the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{l:moments}]
    The proof follows after the convexity of the norms and $\sum_{j=0}^\infty |\b a_j|_1 <\infty$.
\begin{align*}
    \|W_t\|_\psi
        &= \|\sum_{j=0}^\infty \b a_j'\Z_{t-j}\|_\psi \\
       % &= \left\|\sum_{j=0}^\infty |\alpha_j| \frac{\alpha_j'Z_{t-j}}{|\alpha_j|_1}\right\|_\psi\\
        &=\sum_{j=0}^\infty |\b a_j|_1\,\left\|\sum_{j=0}^\infty\frac{|\b a_j|_1}{\sum_{j=0}^\infty |\b a_j|_1}\frac{\b a_j'\Z_{t-j}}{|\b a_j|_1}\right\|_\psi\\
        &\le \sum_{j=0}^\infty |\b a_j|_1\,\sum_{j=0}^\infty\frac{|\b a_j|_1}{\sum_{i=0}^\infty |\b a_i|_1}\left\|\frac{\b a_j}{|\b a_j|_1}'\Z_{t-j}\right\|_\psi\\
        &\le \sum_{j=0}^\infty|\b a_j|_1\sup_{|\b a|_1\le 1}\left\|\b a'\Z_{t-j}\right\|_\psi\\
        &\le \sum_{j=0}^\infty |\b a_j|_1 c_{t-j}
\end{align*}
\end{proof}

\begin{lemma}\label{l:mixingale_prod}
    Let $X$ and $Y$ be uncorrelated, centred random variables and suppose that $\mathcal{G}\subset\F$ are two $\sigma$-algebras of events. Let $\|\E[X|\F]\|_2=\rho_\F$ and $\|\E[Y|\mathcal{G}]\|=\rho_\mathcal{G}$, with the natural extension that $\E[Y|\emptyset] = \E[Y] = 0$. Then, for any $\F$-measurable $M>0$,
    \[
    \|\E[XY|\F]\|_1 \le \|M\|_2\rho_\F + \rho_F\rho_\mathcal{G}+\||XY|I(|Y-\E[Y|\mathcal{G}]|>M)\|_1.
    \]
\end{lemma}
\begin{proof}
    Let $W = YI(|Y-\E[Y|\mathcal{G}]|\le M)$ and $Z = Y-W = YI(|Y-\E[Y|\mathcal{G}]|>M)$. Then, $\E[Y|\mathcal{G}]-M<W<\E[Y|\mathcal{G}]+M$. Multiplying by $X$ and taking $\E[\cdot|\F]$, 
    \[
        \E[X|\F]\E[Y|\mathcal{G}]-\E[X|\F]M<\E[XW|\F]<\E[X|\F]\E[Y|\mathcal{G}]+\E[X|\F]M,
    \]
    where we use the fact that $\E[X\E[Y|\mathcal{G}]|\F] = \E[X|\F]\E[Y|\mathcal{G}]$.
    Therefore, $\E|\E[XW|\F] - \E[X|\F]\E[Y|\mathcal{G}]| < \E|M\E[X|\F]|$. Now:
    \begin{align*}
    \|\E[XY|\F]\|_1
        &\le \|\E[XW|\F]\|_1 + \|\E[XZ|\F]\|_1\\ 
        &\le \|\E[XW|\F] - \E[X|\F]\E[Y|\mathcal{G}]\|_1 + \|\E[X|\F]\E[Y|\mathcal{G}]\|_1 + \|XZ\|_1\\
        &\le \|M\|_2\|\E[X|\F]\|_2 + \|\E[X|\F]\|_2\|\E[Y|\mathcal{G}]\|_2 + \|X\|_p\|Z\|_q.
    \end{align*}
    The last line follows after Hölder's inequality.
\end{proof}
\end{document} 