% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

%通过多模态基础模型实现通用人工智能
@article{fei2022towards,
  title={Towards artificial general intelligence via a multimodal foundation model},
  author={Fei, Nanyi and Lu, Zhiwu and Gao, Yizhao and Yang, Guoxing and Huo, Yuqi and Wen, Jingyuan and Lu, Haoyu and Song, Ruihua and Gao, Xin and Xiang, Tao and others},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={1--13},
  year={2022},
  publisher={Nature Publishing Group}
}

%与音乐数据相关图像生成
@inproceedings{qiu2018image,
  title={Image generation associated with music data},
  author={Qiu, Yue and Kataoka, Hirokatsu},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={2510--2513},
  year={2018}
}

@inproceedings{vegas,
  title={Visual to sound: Generating natural sound for videos in the wild},
  author={Zhou, Yipin and Wang, Zhaowen and Fang, Chen and Bui, Trung and Berg, Tamara L},
  booktitle="CVPR",
  year={2018}
}

%潘洗尘
@inproceedings{pan-etal-2022-leveraging,
    title = "Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition",
    author = "Pan, Xichen  and
      Chen, Peiyu  and
      Gong, Yichen  and
      Zhou, Helong  and
      Wang, Xinbing  and
      Lin, Zhouhan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.308",
    doi = "10.18653/v1/2022.acl-long.308",
    pages = "4491--4503",
    abstract = "Training Transformer-based models demands a large amount of data, while obtaining aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled unimodal data. On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains underexplored. In this work, we successfully leverage unimodal self-supervised learning to promote the multimodal AVSR. In particular, audio and visual front-ends are trained on large-scale unimodal datasets, then we integrate components of both front-ends into a larger multimodal framework which learns to recognize parallel audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from unimodal self-supervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning. Our model is experimentally validated on both word-level and sentence-level tasks. Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30{\%}.",
}

%不引用
%从音频生成图像
@article{lingenerating,
  title={Generating Images from Audio},
  author={Lin, Chih Wen and Su, Ting-Wei}
}

%在语义一致下从音频生成图像(损害修复)
@article{zhao2022generating,
  title={Generating images from audio under semantic consistency},
  author={Zhao, Pengcheng and Chen, Yanxiang and Zhao, Lulu and Wu, Guang and Zhou, Xi},
  journal={Neurocomputing},
  volume={490},
  pages={93--103},
  year={2022},
  publisher={Elsevier}
}

%Wav2音频指挥
@inproceedings{wu2022wav2clip,
  title={Wav2clip: Learning robust audio representations from clip},
  author={Wu, Ho-Hsiang and Seetharaman, Prem and Kumar, Kundan and Bello, Juan Pablo},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4563--4567},
  year={2022},
  organization={IEEE}
}

%学习细粒度的深度表示
@inproceedings{reed2016learning,
  title={Learning deep representations of fine-grained visual descriptions},
  author={Reed, Scott and Akata, Zeynep and Lee, Honglak and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={49--58},
  year={2016}
}

%多模态语音处理，基线任务结果
@inproceedings{chen2022first,
  title={The first multimodal information based speech processing (misp) challenge: Data, tasks, baselines and results},
  author={Chen, Hang and Zhou, Hengshun and Du, Jun and Lee, Chin-Hui and Chen, Jingdong and Watanabe, Shinji and Siniscalchi, Sabato Marco and Scharenborg, Odette and Liu, Di-Yuan and Yin, Bao-Cai and others},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={9266--9270},
  year={2022},
  organization={IEEE}
}

%视听语音深度多模态学习
@inproceedings{mroueh2015deep,
  title={Deep multimodal learning for audio-visual speech recognition},
  author={Mroueh, Youssef and Marcheret, Etienne and Goel, Vaibhava},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2130--2134},
  year={2015},
  organization={IEEE}
}

%端到端视听场景，基于多模态视频特征
@inproceedings{hori2019end,
  title={End-to-end audio visual scene-aware dialog using multimodal attention-based video features},
  author={Hori, Chiori and Alamri, Huda and Wang, Jue and Wichern, Gordon and Hori, Takaaki and Cherian, Anoop and Marks, Tim K and Cartillier, Vincent and Lopes, Raphael Gontijo and Das, Abhishek and others},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2352--2356},
  year={2019},
  organization={IEEE}
}

%微软BEiT3全领域优异多模态模型
@article{wang2022image,
  title={Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}

%音视频跨模态表征对比
@article{chung2022cross,
  title={Cross-Modal Contrastive Representation Learning for Audio-to-Image Generation},
  author={Chung, HaeChun and Shim, JooYong and Kim, Jong-Kook},
  journal={arXiv preprint arXiv:2207.12121},
  year={2022}
}

%uniformer:用于高效时空表示学习的统一转换器
@article{li2022uniformer,
  title={Uniformer: Unified transformer for efficient spatiotemporal representation learning},
  author={Li, Kunchang and Wang, Yali and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2201.04676},
  year={2022}
}

%BERT:深度双向转换语言理解的预训练
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

%WavLM：用于全栈语音处理的大规模自监督预训练
@article{chen2022wavlm,
  title={Wavlm: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2022},
  publisher={IEEE}
}

%HuBERT：通过隐藏单元的掩蔽预测进行自我监督语音表示学习
@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

%Transformer
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

%不引用
%chinese-bert-wwm-ext Pre-Training With Whole Word Masking for Chinese BERT
@article{cui2021pre,
  title={Pre-training with whole word masking for chinese bert},
  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3504--3514},
  year={2021},
  publisher={IEEE}
}

%audio albert音频Bert表示
@inproceedings{chi2021audio,
  title={Audio albert: A lite bert for self-supervised learning of audio representation},
  author={Chi, Po-Han and Chung, Pei-Hung and Wu, Tsung-Han and Hsieh, Chun-Cheng and Chen, Yen-Hao and Li, Shang-Wen and Lee, Hung-yi},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={344--350},
  year={2021},
  organization={IEEE}
}

%使用 BERT 和 Wav2vec 2.0 进行音频-文本融合的跨模态蒸馏，用于细粒度情感分类
@article{kim2022cross,
  title={Cross-modal distillation with audio--text fusion for fine-grained emotion classification using BERT and Wav2vec 2.0},
  author={Kim, Donghwa and Kang, Pilsung},
  journal={Neurocomputing},
  volume={506},
  pages={168--183},
  year={2022},
  publisher={Elsevier}
}

%Audio DistilBERT：用于语音表示学习的蒸馏音频
@inproceedings{yu2021audio,
  title={Audio DistilBERT: A Distilled Audio BERT for Speech Representation Learning},
  author={Yu, Fan and Guo, Jiawei and Xi, Wei and Yang, Zhao and Jiang, Rui and Zhang, Chao},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

%VATT: Transformers for Video, Audio and Text
@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24206--24221},
  year={2021}
}

%CLIP
@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and others},
  journal={ICML},
  year={2021}
}

%B7
@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

%VGG-Sound
@inproceedings{chen2020vggsound,
  title={Vggsound: A large-scale audio-visual dataset},
  author={Chen, Honglie and Xie, Weidi and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={ICASSP},
  pages={721--725},
  year={2020},
  organization={IEEE}
}

%AudioSet
@inproceedings{45857,
title	= {Audio Set: An ontology and human-labeled dataset for audio events},
author	= {Jort F. Gemmeke and Daniel P. W. Ellis and Dylan Freedman and Aren Jansen and Wade Lawrence and R. Channing Moore and Manoj Plakal and Marvin Ritter},
year	= {2017},
booktitle	= {Proc. IEEE ICASSP 2017},
address	= {New Orleans, LA}
}

%深度视听学习综述
@article{zhu2021deep,
  title={Deep audio-visual learning: A survey},
  author={Zhu, Hao and Luo, Man-Di and Wang, Rui and Zheng, Ai-Hua and He, Ran},
  journal={International Journal of Automation and Computing},
  volume={18},
  number={3},
  pages={351--376},
  year={2021},
  publisher={Springer}
}

%MOCO假的
@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

%MOCO
@INPROCEEDINGS{9157636,
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Momentum Contrast for Unsupervised Visual Representation Learning}, 
  year={2020},
  volume={},
  number={},
  pages={9726-9735},
  doi={10.1109/CVPR42600.2020.00975}}

%openai基于神经网络的语言模型
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

%Wav2Vec
@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

%SpeechLM
@article{zhang2022speechlm,
  title={Speechlm: Enhanced speech pre-training with unpaired textual data},
  author={Zhang, Ziqiang and Chen, Sanyuan and Zhou, Long and Wu, Yu and Ren, Shuo and Liu, Shujie and Yao, Zhuoyuan and Gong, Xun and Dai, Lirong and Li, Jinyu and others},
  journal={arXiv preprint arXiv:2209.15329},
  year={2022}
}

%扩散模型
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

%VQGAN
@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}

%dalle
@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

%google brivl
@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}











%语音非转录大规模表示学习
@inproceedings{ilharco-etal-2019-large,
    title = "Large-Scale Representation Learning from Visually Grounded Untranscribed Speech",
    author = "Ilharco, Gabriel  and
      Zhang, Yuan  and
      Baldridge, Jason",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1006",
    doi = "10.18653/v1/K19-1006",
    pages = "55--65",
    abstract = "Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results{---}improving recall in the top 10 from 29.6{\%} to 49.5{\%}. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results.",
}

%参考表格的实验方法
@INPROCEEDINGS{8682383,
  author={Wan, Chia-Hung and Chuang, Shun-Po and Lee, Hung-Yi},
  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Towards Audio to Scene Image Synthesis Using Generative Adversarial Network}, 
  year={2019},
  volume={},
  number={},
  pages={496-500},
  doi={10.1109/ICASSP.2019.8682383}}



%启发的引用工作,数据集
@article{chrupala2022visually,
  title={Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques},
  author={Chrupa{\l}a, Grzegorz},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={673--707},
  year={2022}
}

%启发的引用工作, 定性评估
@inproceedings{hessel2021clipscore,
  title={CLIPScore: A Reference-free Evaluation Metric for Image Captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7514--7528},
  year={2021}
}

%启发的引用工作, 文本引导扩散模型生成
@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36479--36494},
  year={2022}
}

%NERF
@inproceedings{mildenhall2020nerf,
 title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
 author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
 year={2020},
 booktitle={ECCV},
}

%音频生成图像的特殊gan
@inproceedings{xu2018attngan,
  title={Attngan: Fine-grained text to image generation with attentional generative adversarial networks},
  author={Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1316--1324},
  year={2018}
}

%具有深度语言理解的逼真文本到图像扩散模型
@inproceedings{NEURIPS2022_ec795aea,
 author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {36479--36494},
 publisher = {Curran Associates, Inc.},
 title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

%一致性模型
@misc{song2023consistency,
      title={Consistency Models}, 
      author={Yang Song and Prafulla Dhariwal and Mark Chen and Ilya Sutskever},
      year={2023},
      eprint={2303.01469},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%VQVAE
@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{sungbin2023sound,
      title={Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment}, 
      author={Kim Sung-Bin and Arda Senocak and Hyunwoo Ha and Andrew Owens and Tae-Hyun Oh},
      year={2023},
      eprint={2303.17490},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{ma2015using,
  title={Using Word2Vec to process big text data},
  author={Ma, Long and Zhang, Yanqing},
  booktitle={2015 IEEE International Conference on Big Data (Big Data)},
  pages={2895--2897},
  year={2015},
  organization={IEEE}
}

@inproceedings{vqvae,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  booktitle=NIPS,
  year={2017}
}
%审稿人推荐的一些工作
@misc{girdhar2023imagebind,
      title={ImageBind: One Embedding Space To Bind Them All}, 
      author={Rohit Girdhar and Alaaeldin El-Nouby and Zhuang Liu and Mannat Singh and Kalyan Vasudev Alwala and Armand Joulin and Ishan Misra},
      year={2023},
      eprint={2305.05665},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
%SD diffu
@InProceedings{Rombach_2022_CVPR,
    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10684-10695}
}
%SpeechLM
@misc{zhang2023speechlm,
      title={SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data}, 
      author={Ziqiang Zhang and Sanyuan Chen and Long Zhou and Yu Wu and Shuo Ren and Shujie Liu and Zhuoyuan Yao and Xun Gong and Lirong Dai and Jinyu Li and Furu Wei},
      year={2023},
      eprint={2209.15329},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}