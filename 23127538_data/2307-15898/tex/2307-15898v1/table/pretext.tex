\subsection{Different pretext tasks}

% Figure environment removed

From Figure \ref{fig:train_num}, we see results of amount of training data to train downstream classifiers on VGGSound, compared among Supervise, OpenL3, YamNet, and Wav2CLIP. Both YamNet and Wav2CLIP only require $\sim$1/10 amount of data to reach similar performance as Supervise counterpart in the smaller labeled data region, and eventually all others outperform OpenL3 when enough labels are available.

YamNet and Wav2CLIP yield very similar trend throughout all our comparisons (Table \ref{tab:all_tasks} and Figure \ref{fig:train_num}), except for FSD50K and TAU, where we hypothesize that the match between pretext and downstream tasks does affect the performance. However, this is still surprising that they perform very similarly on all other classification tasks, as the pretext tasks for pre-training are very different, while YamNet requires large amount of labeled data, Wav2CLIP is pre-trained without human annotations, and is therefore easier to scale.