@inproceedings{avrahami2022blended,
  title={Blended diffusion for text-driven editing of natural images},
  author={Avrahami, Omri and Lischinski, Dani and Fried, Ohad},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18208--18218},
  year={2022}
}

@article{aneja2019real,
  title={Real-time lip sync for live 2d animation},
  author={Aneja, Deepali and Li, Wilmot},
  journal={arXiv preprint arXiv:1910.08685},
  year={2019}
}

@article{batzolis2021conditional,
  title={Conditional image generation with score-based diffusion models},
  author={Batzolis, Georgios and Stanczuk, Jan and Sch{\"o}nlieb, Carola-Bibiane and Etmann, Christian},
  journal={arXiv preprint arXiv:2111.13606},
  year={2021}
}

@inproceedings{biswas2021realistic,
  title={Realistic talking face animation with speech-induced head motion},
  author={Biswas, Sandika and Sinha, Sanjana and Das, Dipanjan and Bhowmick, Brojeshwar},
  booktitle={Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
  pages={1--9},
  year={2021}
}

@inproceedings{chen2020talking,
  title={Talking-head generation with rhythmic head motion},
  author={Chen, Lele and Cui, Guofeng and Liu, Celong and Li, Zhong and Kou, Ziyi and Xu, Yi and Xu, Chenliang},
  booktitle={European Conference on Computer Vision},
  pages={35--51},
  year={2020},
  organization={Springer}
}

@inproceedings{chen2018lip,
  title={Lip movements generation at a glance},
  author={Chen, Lele and Li, Zhiheng and Maddox, Ross K and Duan, Zhiyao and Xu, Chenliang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={520--535},
  year={2018}
}

@article{pedersoli2022estimating,
  title={Estimating Visual Information From Audio Through Manifold Learning},
  author={Pedersoli, Fabrizio and Wiebe, Dryden and Banitalebi, Amin and Zhang, Yong and Yi, Kwang Moo},
  journal={arXiv preprint arXiv:2208.02337},
  year={2022}
}

@article{s2i,
  title={Sound-to-Imagination: Unsupervised Crossmodal Translation Using Deep Dense Network Architecture},
  author={Fanzeres, Leonardo A and Nadeu, Climent},
  journal={arXiv preprint arXiv:2106.01266},
  year={2021}
}

@inproceedings{chen2019hierarchical,
  title={Hierarchical cross-modal talking face generation with dynamic pixel-wise loss},
  author={Chen, Lele and Maddox, Ross K and Duan, Zhiyao and Xu, Chenliang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7832--7841},
  year={2019}
}

@article{chen2020wavegrad,
  title={WaveGrad: Estimating gradients for waveform generation},
  author={Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J and Norouzi, Mohammad and Chan, William},
  journal={arXiv preprint arXiv:2009.00713},
  year={2020}
}

@article{chen2022talking,
  title={Talking Head Generation Driven by Speech-Related Facial Action Units and Audio-Based on Multimodal Representation Fusion},
  author={Chen, Sen and Liu, Zhilei and Liu, Jiaxing and Wang, Longbiao},
  journal={arXiv preprint arXiv:2204.12756},
  year={2022}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}
@article{chen2022does,
  title={Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?},
  author={Chen, Sanyuan and Wu, Yu and Wang, Chengyi and Liu, Shujie and Chen, Zhuo and Wang, Peidong and Liu, Gang and Li, Jinyu and Wu, Jian and Yu, Xiangzhan and others},
  journal={arXiv preprint arXiv:2204.12765},
  year={2022}
}
@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{bapna2021slam,
  title={SLAM: A unified encoder for speech and language modeling via speech-text joint pre-training},
  author={Bapna, Ankur and Chung, Yu-an and Wu, Nan and Gulati, Anmol and Jia, Ye and Clark, Jonathan H and Johnson, Melvin and Riesa, Jason and Conneau, Alexis and Zhang, Yu},
  journal={arXiv preprint arXiv:2110.10329},
  year={2021}
}

@article{bapna2022mslam,
  title={mSLAM: Massively multilingual joint pre-training for speech and text},
  author={Bapna, Ankur and Cherry, Colin and Zhang, Yu and Jia, Ye and Johnson, Melvin and Cheng, Yong and Khanuja, Simran and Riesa, Jason and Conneau, Alexis},
  journal={arXiv preprint arXiv:2202.01374},
  year={2022}
}

@inproceedings{chung2020splat,
    title = "{SPLAT}: Speech-Language Joint Pre-Training for Spoken Language Understanding",
    author = "Chung, Yu-An  and
      Zhu, Chenguang  and
      Zeng, Michael",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    doi = "10.18653/v1/2021.naacl-main.152",
    pages = "1897--1907",
}

@inproceedings{kim2021st,
  title={St-Bert: Cross-Modal Language Model Pre-Training for End-to-End Spoken Language Understanding},
  author={Kim, Minjeong and Kim, Gyuwan and Lee, Sang-Woo and Ha, Jung-Woo},
  booktitle={Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={7478--7482},
  year={2021},
  doi={10.1109/ICASSP39728.2021.9414558}
}

@inproceedings{qian2021speech,
  title={Speech-language pre-training for end-to-end spoken language understanding},
  author={Qian, Yao and Bianv, Ximo and Shi, Yu and Kanda, Naoyuki and Shen, Leo and Xiao, Zhen and Zeng, Michael},
  booktitle={Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={7458--7462},
  year={2021},
  doi={10.1109/ICASSP39728.2021.9414900}
}

@inproceedings{ao2021speecht5,
    title = "{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing",
    author = "Ao, Junyi  and
      Wang, Rui  and
      Zhou, Long  and
      Wang, Chengyi  and
      Ren, Shuo  and
      Wu, Yu  and
      Liu, Shujie  and
      Ko, Tom  and
      Li, Qing  and
      Zhang, Yu  and
      Wei, Zhihua  and
      Qian, Yao  and
      Li, Jinyu  and
      Wei, Furu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.393",
    doi = "10.18653/v1/2022.acl-long.393",
    pages = "5723--5738",
}
 
 @article{tremblay2016broca,
  title={Broca and Wernicke are dead, or moving past the classic model of language neurobiology},
  author={Tremblay, Pascale and Dick, Anthony Steven},
  journal={Brain and language},
  volume={162},
  pages={60--71},
  year={2016},
  publisher={Elsevier}
}

@article{schneider2019wav2vec,
  title={wav2vec: Unsupervised pre-training for speech recognition},
  author={Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  journal={arXiv preprint arXiv:1904.05862},
  year={2019}
}


@inproceedings{dong2019unified,
 author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
 booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems},
 pages = {13063--13075},
 title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
 url = {https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems},
  pages={6000--6010},
  volume={30},
  url={https://dl.acm.org/doi/pdf/10.5555/3295222.3295349},
  year={2017}
}


@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    booktitle = "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
}

@inproceedings{Graves10.1145CTC,
author = {Graves, Alex and Fern\'{a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143891},
doi = {10.1145/1143844.1143891},
pages = {369–376},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}
@inproceedings{wang2021unispeech,
  title={Unispeech: Unified speech representation learning with labeled and unlabeled data},
  author={Wang, Chengyi and Wu, Yu and Qian, Yao and Kumatani, Kenichi and Liu, Shujie and Wei, Furu and Zeng, Michael and Huang, Xuedong},
  booktitle={International Conference on Machine Learning},
  pages={10937--10947},
  year={2021},
  organization={PMLR}
}
@inproceedings{DBLP:conf/icassp/Lee021,
  author    = {Jaesong Lee and
               Shinji Watanabe},
  title     = {Intermediate Loss Regularization for CTC-Based Speech Recognition},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages     = {6224--6228},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICASSP39728.2021.9414594},
  doi       = {10.1109/ICASSP39728.2021.9414594},
  timestamp = {Thu, 23 Jun 2022 19:54:44 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/Lee021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2207-04177,
  author    = {Jicheng Zhang and
               Yizhou Peng and
               Haihua Xu and
               Yi He and
               Eng Siong Chng and
               Hao Huang},
  title     = {Intermediate-layer output Regularization for Attention-based Speech
               Recognition with Shared Decoder},
  journal   = {CoRR},
  volume    = {abs/2207.04177},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2207.04177},
  doi       = {10.48550/arXiv.2207.04177},
  eprinttype = {arXiv},
  eprint    = {2207.04177},
  timestamp = {Thu, 14 Jul 2022 15:34:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2207-04177.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/slt/Liu0LKSZ21,
  author    = {Chunxi Liu and
               Frank Zhang and
               Duc Le and
               Suyoun Kim and
               Yatharth Saraf and
               Geoffrey Zweig},
  title     = {Improving {RNN} Transducer Based {ASR} with Auxiliary Tasks},
  booktitle = {{IEEE} Spoken Language Technology Workshop, {SLT} 2021, Shenzhen,
               China, January 19-22, 2021},
  pages     = {172--179},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/SLT48900.2021.9383548},
  doi       = {10.1109/SLT48900.2021.9383548},
  timestamp = {Thu, 08 Apr 2021 15:38:28 +0200},
  biburl    = {https://dblp.org/rec/conf/slt/Liu0LKSZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wang2021self,
  author    = {Chengyi Wang and
               Yu Wu and
               Sanyuan Chen and
               Shujie Liu and
               Jinyu Li and
               Yao Qian and
               Zhenglu Yang},
  title     = {Improving Self-Supervised Learning for Speech Recognition with Intermediate
               Layer Supervision},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022},
  pages     = {7092--7096},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/ICASSP43922.2022.9747022},
  doi       = {10.1109/ICASSP43922.2022.9747022},
  timestamp = {Fri, 15 Jul 2022 08:46:32 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/WangWCLLQY22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2022supervision,
  title={Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training},
  author={Wang, Chengyi and Wang, Yiming and Wu, Yu and Chen, Sanyuan and Li, Jinyu and Liu, Shujie and Wei, Furu},
  journal={arXiv preprint arXiv:2206.10125},
  year={2022}
}

@inproceedings{tang2022unified,
    title = "Unified Speech-Text Pre-training for Speech Translation and Recognition",
    author = "Tang, Yun  and
      Gong, Hongyu  and
      Dong, Ning  and
      Wang, Changhan  and
      Hsu, Wei-Ning  and
      Gu, Jiatao  and
      Baevski, Alexei  and
      Li, Xian  and
      Mohamed, Abdelrahman  and
      Auli, Michael  and
      Pino, Juan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.105",
    doi = "10.18653/v1/2022.acl-long.105",
    pages = "1488--1499",
}

@article{lample2019cross,
  title={Cross-lingual language model pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@INPROCEEDINGS{Xu9414641Self,
  author={Xu, Qiantong and Baevski, Alexei and Likhomanenko, Tatiana and Tomasello, Paden and Conneau, Alexis and Collobert, Ronan and Synnaeve, Gabriel and Auli, Michael},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Self-Training and Pre-Training are Complementary for Speech Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={3030-3034},
  doi={10.1109/ICASSP39728.2021.9414641}
}

@inproceedings{duan2022multi,
  title={Multi-modal alignment using representation codebook},
  author={Duan, Jiali and Chen, Liqun and Tran, Son and Yang, Jinyu and Xu, Yi and Zeng, Belinda and Chilimbi, Trishul},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15651--15660},
  year={2022}
}

@inproceedings{oord2017neural,
  title={Neural discrete representation learning},
  author={Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 31st Conference on Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{yang2021superb,
  title={Superb: Speech processing universal performance benchmark},
  author={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y and Liu, Andy T and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and others},
  journal={arXiv preprint arXiv:2105.01051},
  year={2021}
}

@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={5206--5210},
  year={2015},
  doi={10.1109/ICASSP.2015.7178964},
  organization={IEEE}
}

@inproceedings{pase+,
  title={Multi-task self-supervised learning for robust speech recognition},
  author={Ravanelli, Mirco and Zhong, Jianyuan and Pascual, Santiago and Swietojanski, Pawel and Monteiro, Joao and Trmal, Jan and Bengio, Yoshua},
  booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6989--6993},
  year={2020},
  organization={IEEE}
}


@article{cpc,
    author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
    title     = {Representation Learning with Contrastive Predictive Coding},
    journal   = {CoRR},
    volume    = {abs/1807.03748},
    year      = {2018},
    archivePrefix = {arXiv},
    eprint    = {1807.03748},
    timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{modified_cpc,
    title={Unsupervised pretraining transfers well across languages},
    author={Rivi{\`e}re, Morgane and Joulin, Armand and Mazar{\'e}, Pierre-Emmanuel and Dupoux, Emmanuel},
    booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    pages={7414--7418},
    year={2020},
  organization={IEEE}
}




@inproceedings{vq_wav2vec,
    title={vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations},
    author={Alexei Baevski and Steffen Schneider and Michael Auli},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2020},
}

@inproceedings{vq_wav2vec_ft,
    author={A. {Baevski} and A. {Mohamed}},
    booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    title={Effectiveness of Self-Supervised Pre-Training for ASR},
    year={2020},  volume={},  number={},  pages={7694-7698},  doi={10.1109/ICASSP40776.2020.9054224},
  organization={IEEE}
}

@inproceedings{wav2vec2,
    author    = {Alexei Baevski and
               Yuhao Zhou and
               Abdelrahman Mohamed and
               Michael Auli},
    title     = {wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech Representations},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    year      = {2020},
}

@inproceedings{pase,
  title={Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks},
  author={Pascual, Santiago and Ravanelli, Mirco and Serr{\`a}, Joan and Bonafonte, Antonio and Bengio, Yoshua},
  booktitle={Interspeech},
  pages={161--165},
  year={2019}
}

@inproceedings{apc1,
    author={Yu-An Chung and Wei-Ning Hsu and Hao Tang and James Glass},
    title={{An Unsupervised Autoregressive Model for Speech Representation Learning}},
    year=2019,
    booktitle={Interspeech},
    pages={146--150}
}

@inproceedings{apc2,
  author={Y. {Chung} and J. {Glass}},
  booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Generative Pre-Training for Speech with Autoregressive Predictive Coding}, 
  year={2020},
  pages={3497-3501},
  doi={10.1109/ICASSP40776.2020.9054438},
  organization={IEEE}
}

@inproceedings{improved_apc,
    title = "Improved Speech Representations with Multi-Target Autoregressive Predictive Coding",
    author = "Chung, Yu-An  and
      Glass, James",
    booktitle = "Association for Computational Linguistics (ACL)",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.acl-main.213",
    pages = "2353--2358"
}

@inproceedings{vq_apc,
    title={Vector-Quantized Autoregressive Predictive Coding},
    author={Chung, Yu-An and Tang, Hao and Glass, James},
    booktitle={Interspeech},
    pages={3760--3764},
    year={2020}
}

@article{npc,
  title={Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies},
  author={Liu, Alexander H and Chung, Yu-An and Glass, James},
  journal={arXiv preprint arXiv:2011.00406},
  year={2020}
}

@inproceedings{mockingjay,
   title={Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders},
   booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   author={Liu, Andy T. and Yang, Shu-wen and Chi, Po-Han and Hsu, Po-chun and Lee, Hung-yi},
   year={2020},
  organization={IEEE}
}

@article{tera,
  title={Tera: Self-supervised learning of transformer encoder representation for speech},
  author={Liu, Andy T and Li, Shang-Wen and Lee, Hung-yi},
  journal={arXiv preprint arXiv:2007.06028},
  year={2020}
}
@article{decoar2,
  title={{DeCoAR} 2.0: Deep Contextualized Acoustic Representations with Vector Quantization},
  author={Ling, Shaoshi and Liu, Yuzong},
  journal={arXiv preprint arXiv:2012.06659},
  year={2020}
}

@article{wavlm,
  title={Wavlm: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2022},
  publisher={IEEE}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}



@article{baevski2022data2vec,
  title={Data2vec: A general framework for self-supervised learning in speech, vision and language},
  author={Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  journal={arXiv preprint arXiv:2202.03555},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{rousseau2012ted,
  title={TED-LIUM: an Automatic Speech Recognition dedicated corpus.},
  author={Rousseau, Anthony and Del{\'e}glise, Paul and Esteve, Yannick},
  booktitle={LREC},
  pages={125--129},
  year={2012}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@article{mohri2002weighted,
  title={Weighted finite-state transducers in speech recognition},
  author={Mohri, Mehryar and Pereira, Fernando and Riley, Michael},
  journal={Computer Speech \& Language},
  volume={16},
  number={1},
  pages={69--88},
  year={2002},
  publisher={Elsevier}
}

@article{ren2019fastspeech,
  title={Fastspeech: Fast, robust and controllable text to speech},
  author={Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{kahn2020librilight,
  title={Libri-light: A benchmark for asr with limited or no supervision},
  author={Kahn, Jacob and Rivi{\`e}re, Morgane and Zheng, Weiyi and Kharitonov, Evgeny and Xu, Qiantong and Mazar{\'e}, Pierre-Emmanuel and Karadayi, Julien and Liptchinsky, Vitaliy and Collobert, Ronan and Fuegen, Christian and others},
  booktitle={Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={7669--7673},
  year={2020},
  doi={10.1109/ICASSP40776.2020.9052942},
  organization={IEEE}
}

@InProceedings{Hernandez2018tedlium3,
author="Hernandez, Fran{\c{c}}ois
and Nguyen, Vincent
and Ghannay, Sahar
and Tomashenko, Natalia
and Est{\`e}ve, Yannick",
editor="Karpov, Alexey
and Jokisch, Oliver
and Potapova, Rodmonga",
title="TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation",
booktitle="Speech and Computer",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="198--208",
isbn="978-3-319-99579-3"
}

@InProceedings{Carletta2005AMI,
author="Carletta, Jean
and Ashby, Simone
and Bourban, Sebastien
and Flynn, Mike
and Guillemot, Mael
and Hain, Thomas
and Kadlec, Jaroslav
and Karaiskos, Vasilis
and Kraaij, Wessel
and Kronenthal, Melissa
and Lathoud, Guillaume
and Lincoln, Mike
and Lisowska, Agnes
and McCowan, Iain
and Post, Wilfried
and Reidsma, Dennis
and Wellner, Pierre",
editor="Renals, Steve
and Bengio, Samy",
title="The AMI Meeting Corpus: A Pre-announcement",
booktitle="Machine Learning for Multimodal Interaction",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="28--39",
isbn="978-3-540-32550-5"
}


@article{wang2020covost,
  title={Covost 2 and massively multilingual speech-to-text translation},
  author={Wang, Changhan and Wu, Anne and Pino, Juan},
  journal={arXiv preprint arXiv:2007.10310},
  year={2020}
}

@article{li2022recent,
  title={Recent advances in end-to-end automatic speech recognition},
  author={Li, Jinyu and others},
  journal={APSIPA Transactions on Signal and Information Processing},
  volume={11},
  number={1},
  year={2022},
  publisher={Now Publishers, Inc.}
}

@inproceedings{li2019neural,
  title={Neural speech synthesis with transformer network},
  author={Li, Naihan and Liu, Shujie and Liu, Yanqing and Zhao, Sheng and Liu, Ming},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6706--6713},
  year={2019}
}

@article{chen2022maestro,
  title={MAESTRO: Matched Speech Text Representations through Modality Matching},
  author={Chen, Zhehuai and Zhang, Yu and Rosenberg, Andrew and Ramabhadran, Bhuvana and Moreno, Pedro and Bapna, Ankur and Zen, Heiga},
  journal={arXiv preprint arXiv:2204.03409},
  year={2022}
}

@article{Changhan2021Large,
  publtype={informal},
  author={Changhan Wang and Anne Wu and Juan Miguel Pino and Alexei Baevski and Michael Auli and Alexis Conneau},
  title={Large-Scale Self- and Semi-Supervised Learning for Speech Translation},
  year={2021},
  cdate={1609459200000},
  journal={CoRR},
  volume={abs/2104.06678},
  url={https://arxiv.org/abs/2104.06678}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}


@inproceedings{chung2021w2v,
  title={W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training},
  author={Chung, Yu-An and Zhang, Yu and Han, Wei and Chiu, Chung-Cheng and Qin, James and Pang, Ruoming and Wu, Yonghui},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={244--250},
  year={2021},
  organization={IEEE}
}

@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{NEURIPS2021_w2vu,
 author = {Baevski, Alexei and Hsu, Wei-Ning and CONNEAU, Alexis and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27826--27839},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Speech Recognition},
 url = {https://proceedings.neurips.cc/paper/2021/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf},
 volume = {34},
 year = {2021}
}


@INPROCEEDINGS{tts4pretrain2,
  author={Chen, Zhehuai and Zhang, Yu and Rosenberg, Andrew and Ramabhadran, Bhuvana and Moreno, Pedro and Wang, Gary},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Tts4pretrain 2.0: Advancing the use of Text and Speech in ASR Pretraining with Consistency and Contrastive Losses}, 
  year={2022},
  volume={},
  number={},
  pages={7677-7681},
  doi={10.1109/ICASSP43922.2022.9746475}
}

 @inproceedings{kaldi,
      title = {The Kaldi Speech Recognition Toolkit},
      author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and  Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and  Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and  Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and  Vesely, Karel},
      publisher = {IEEE Signal Processing Society},
      year = {2011},
      note = {IEEE Catalog No.: CFP11SRW-USB},
      abstract = {We describe the design of Kaldi, a free, open-source  toolkit for speech recognition research. Kaldi provides a  speech recognition system based on finite-state transducers  (using the freely available OpenFst), together with  detailed documentation and scripts for building complete  recognition systems. Kaldi is written is C++, and the core  library supports modeling of arbitrary phonetic-context  sizes, acoustic modeling with subspace Gaussian mixture  models (SGMM) as well as standard Gaussian mixture models,  together with all commonly used linear and affine  transforms. Kaldi is released under the Apache License  v2.0, which is highly nonrestrictive, making it suitable  for a wide community of users.},
      url = {http://infoscience.epfl.ch/record/192584},
}


@article{jamaludin2019you,
  title={You said that?: Synthesising talking faces from audio},
  author={Jamaludin, Amir and Chung, Joon Son and Zisserman, Andrew},
  journal={International Journal of Computer Vision},
  volume={127},
  number={11},
  pages={1767--1779},
  year={2019},
  publisher={Springer}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}

}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{son2017lip,
  title={Lip reading sentences in the wild},
  author={Son Chung, Joon and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6447--6456},
  year={2017}
}

@article{cooke2006audio,
  title={An audio-visual corpus for speech perception and automatic speech recognition},
  author={Cooke, Martin and Barker, Jon and Cunningham, Stuart and Shao, Xu},
  journal={The Journal of the Acoustical Society of America},
  volume={120},
  number={5},
  pages={2421--2424},
  year={2006},
  publisher={Acoustical Society of America}
}

@inproceedings{cudeiro2019capture,
  title={Capture, learning, and synthesis of 3D speaking styles},
  author={Cudeiro, Daniel and Bolkart, Timo and Laidlaw, Cassidy and Ranjan, Anurag and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10101--10111},
  year={2019}
}

@inproceedings{das2020speech,
  title={Speech-driven facial animation using cascaded gans for learning of motion and texture},
  author={Das, Dipanjan and Biswas, Sandika and Sinha, Sanjana and Bhowmick, Brojeshwar},
  booktitle={European conference on computer vision},
  pages={408--424},
  year={2020},
  organization={Springer}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@inproceedings{eskimez2018generating,
  title={Generating talking face landmarks from speech},
  author={Eskimez, Sefik Emre and Maddox, Ross K and Xu, Chenliang and Duan, Zhiyao},
  booktitle={International Conference on Latent Variable Analysis and Signal Separation},
  pages={372--381},
  year={2018},
  organization={Springer}
}

@inproceedings{eskimez2020end,
  title={End-to-end generation of talking faces from noisy speech},
  author={Eskimez, Sefik Emre and Maddox, Ross K and Xu, Chenliang and Duan, Zhiyao},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1948--1952},
  year={2020},
  organization={IEEE}
}

@article{fan2022frido,
  title={Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis},
  author={Fan, Wan-Cyuan and Chen, Yen-Chun and Chen, DongDong and Cheng, Yu and Yuan, Lu and Wang, Yu-Chiang Frank},
  journal={arXiv preprint arXiv:2208.13753},
  year={2022}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{gu2022vector,
  title={Vector quantized diffusion model for text-to-image synthesis},
  author={Gu, Shuyang and Chen, Dong and Bao, Jianmin and Wen, Fang and Zhang, Bo and Chen, Dongdong and Yuan, Lu and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10696--10706},
  year={2022}
}

@article{harvey2022flexible,
  title={Flexible Diffusion Modeling of Long Videos},
  author={Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  journal={arXiv preprint arXiv:2205.11495},
  year={2022}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{ho2022cascaded,
  title={Cascaded Diffusion Models for High Fidelity Image Generation.},
  author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
  journal={J. Mach. Learn. Res.},
  volume={23},
  pages={47--1},
  year={2022}
}

@article{ho2022video,
  title={Video diffusion models},
  author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
  journal={arXiv preprint arXiv:2204.03458},
  year={2022}
}

@inproceedings{huang2022prodiff,
  title={Prodiff: Progressive fast diffusion model for high-quality text-to-speech},
  author={Huang, Rongjie and Zhao, Zhou and Liu, Huadai and Liu, Jinglin and Cui, Chenye and Ren, Yi},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={2595--2605},
  year={2022}
}

@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

@inproceedings{ji2021audio,
  title={Audio-driven emotional video portraits},
  author={Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Wayne and Loy, Chen Change and Cao, Xun and Xu, Feng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14080--14089},
  year={2021}
}

@article{karras2017audio,
  title={Audio-driven facial animation by joint end-to-end learning of pose and emotion},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Herva, Antti and Lehtinen, Jaakko},
  journal={ACM Transactions on Graphics (TOG)},
  volume={36},
  number={4},
  pages={1--12},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{kim2022guided,
  title={Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data},
  author={Kim, Sungwon and Kim, Heeseung and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2205.15370},
  year={2022}
}

@article{kong2020diffwave,
  title={Diffwave: A versatile diffusion model for audio synthesis},
  author={Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2009.09761},
  year={2020}
}

@inproceedings{kumar2020robust,
  title={Robust one shot audio to video generation},
  author={Kumar, Neeraj and Goel, Srishti and Narang, Ankur and Hasan, Mujtaba},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={770--771},
  year={2020}
}

@inproceedings{lahiri2021lipsync3d,
  title={Lipsync3d: Data-efficient learning of personalized 3d talking faces from video using pose and lighting normalization},
  author={Lahiri, Avisek and Kwatra, Vivek and Frueh, Christian and Lewis, John and Bregler, Chris},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2755--2764},
  year={2021}
}

@article{levkovitch2022zero,
  title={Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models},
  author={Levkovitch, Alon and Nachmani, Eliya and Wolf, Lior},
  journal={arXiv preprint arXiv:2206.02246},
  year={2022}
}

@article{lu2021live,
  title={Live speech portraits: real-time photorealistic talking-head animation},
  author={Lu, Yuanxun and Chai, Jinxiang and Cao, Xun},
  journal={ACM Transactions on Graphics (TOG)},
  volume={40},
  number={6},
  pages={1--17},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{lugaresi2019mediapipe,
  title={Mediapipe: A framework for building perception pipelines},
  author={Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and others},
  journal={arXiv preprint arXiv:1906.08172},
  year={2019}
}

@inproceedings{lugmayr2022repaint,
  title={Repaint: Inpainting using denoising diffusion probabilistic models},
  author={Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11461--11471},
  year={2022}
}

@article{meng2021sdedit,
  title={Sdedit: Image synthesis and editing with stochastic differential equations},
  author={Meng, Chenlin and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  journal={arXiv preprint arXiv:2108.01073},
  year={2021}
}

@inproceedings{mittal2020animating,
  title={Animating face using disentangled audio representations},
  author={Mittal, Gaurav and Wang, Baoyuan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3290--3298},
  year={2020}
}

@article{nguyen2019deep,
  title={Deep learning for deepfakes creation and detection},
  author={Nguyen, Thanh Thi and Nguyen, Cuong M and Nguyen, Dung Tien and Nguyen, Duc Thanh and Nahavandi, Saeid},
  journal={arXiv preprint arXiv:1909.11573},
  volume={1},
  pages={2},
  year={2019}
}

@article{nichol2021glide,
  title={Glide: Towards photorealistic image generation and editing with text-guided diffusion models},
  author={Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  journal={arXiv preprint arXiv:2112.10741},
  year={2021}
}

@inproceedings{nichol2021improved,
  title={Improved denoising diffusion probabilistic models},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle={International Conference on Machine Learning},
  pages={8162--8171},
  year={2021},
  organization={PMLR}
}

@inproceedings{popov2021grad,
  title={Grad-tts: A diffusion probabilistic model for text-to-speech},
  author={Popov, Vadim and Vovk, Ivan and Gogoryan, Vladimir and Sadekova, Tasnima and Kudinov, Mikhail},
  booktitle={International Conference on Machine Learning},
  pages={8599--8608},
  year={2021},
  organization={PMLR}
}

@inproceedings{prajwal2020lip,
  title={A lip sync expert is all you need for speech to lip generation in the wild},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={484--492},
  year={2020}
}

@inproceedings{preechakul2022diffusion,
  title={Diffusion autoencoders: Toward a meaningful and decodable representation},
  author={Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10619--10629},
  year={2022}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@inproceedings{richard2021meshtalk,
  title={Meshtalk: 3d face animation from speech using cross-modality disentanglement},
  author={Richard, Alexander and Zollh{\"o}fer, Michael and Wen, Yandong and De la Torre, Fernando and Sheikh, Yaser},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1173--1182},
  year={2021}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10684--10695},
  year={2022}
}

@article{ruiz2022dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  journal={arXiv preprint arXiv:2208.12242},
  year={2022}
}

@article{sadoughi2019speech,
  title={Speech-driven expressive talking lips with conditional sequential generative adversarial networks},
  author={Sadoughi, Najmeh and Busso, Carlos},
  journal={IEEE Transactions on Affective Computing},
  volume={12},
  number={4},
  pages={1031--1044},
  year={2019},
  publisher={IEEE}
}

@inproceedings{saharia2022palette,
  title={Palette: Image-to-image diffusion models},
  author={Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris and Ho, Jonathan and Salimans, Tim and Fleet, David and Norouzi, Mohammad},
  booktitle={ACM SIGGRAPH 2022 Conference Proceedings},
  pages={1--10},
  year={2022}
}

@article{saharia2022image,
  title={Image super-resolution via iterative refinement},
  author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J and Norouzi, Mohammad},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@inproceedings{song2021tacr,
  title={TACR-Net: Editing on Deep Video and Voice Portraits},
  author={Song, Luchuan and Liu, Bin and Yin, Guojun and Dong, Xiaoyi and Zhang, Yufei and Bai, Jia-Xuan},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={478--486},
  year={2021}
}

@article{song2022everybody,
  title={Everybody’s talkin’: Let me talk as you want},
  author={Song, Linsen and Wu, Wayne and Qian, Chen and He, Ran and Loy, Chen Change},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={17},
  pages={585--598},
  year={2022},
  publisher={IEEE}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{song2018talking,
  title={Talking face generation by conditional recurrent adversarial network},
  author={Song, Yang and Zhu, Jingwen and Li, Dawei and Wang, Xiaolong and Qi, Hairong},
  journal={arXiv preprint arXiv:1804.04786},
  year={2018}
}

@article{suwajanakorn2017synthesizing,
  title={Synthesizing obama: learning lip sync from audio},
  author={Suwajanakorn, Supasorn and Seitz, Steven M and Kemelmacher-Shlizerman, Ira},
  journal={ACM Transactions on Graphics (ToG)},
  volume={36},
  number={4},
  pages={1--13},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{tae2021editts,
  title={EdiTTS: Score-based Editing for Controllable Text-to-Speech},
  author={Tae, Jaesung and Kim, Hyeongju and Kim, Taesu},
  journal={arXiv preprint arXiv:2110.02584},
  year={2021}
}

@article{taylor2017deep,
  title={A deep learning approach for generalized speech animation},
  author={Taylor, Sarah and Kim, Taehwan and Yue, Yisong and Mahler, Moshe and Krahe, James and Rodriguez, Anastasio Garcia and Hodgins, Jessica and Matthews, Iain},
  journal={ACM Transactions on Graphics (TOG)},
  volume={36},
  number={4},
  pages={1--11},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{thies2020neural,
  title={Neural voice puppetry: Audio-driven facial reenactment},
  author={Thies, Justus and Elgharib, Mohamed and Tewari, Ayush and Theobalt, Christian and Nie{\ss}ner, Matthias},
  booktitle={European conference on computer vision},
  pages={716--731},
  year={2020},
  organization={Springer}
}

@article{tolosana2020deepfakes,
  title={Deepfakes and beyond: A survey of face manipulation and fake detection},
  author={Tolosana, Ruben and Vera-Rodriguez, Ruben and Fierrez, Julian and Morales, Aythami and Ortega-Garcia, Javier},
  journal={Information Fusion},
  volume={64},
  pages={131--148},
  year={2020},
  publisher={Elsevier}
}

@article{vougioukas2020realistic,
  title={Realistic speech-driven facial animation with gans},
  author={Vougioukas, Konstantinos and Petridis, Stavros and Pantic, Maja},
  journal={International Journal of Computer Vision},
  volume={128},
  number={5},
  pages={1398--1413},
  year={2020},
  publisher={Springer}
}

@article{wang2021audio2head,
  title={Audio2head: Audio-driven one-shot talking-head generation with natural head motion},
  author={Wang, Suzhen and Li, Lincheng and Ding, Yu and Fan, Changjie and Yu, Xin},
  journal={arXiv preprint arXiv:2107.09293},
  year={2021}
}

@inproceedings{wang2020speech,
  title={Speech Driven Talking Head Generation via Attentional Landmarks Based Representation.},
  author={Wang, Wentao and Wang, Yan and Sun, Jianqing and Liu, Qingsong and Liang, Jiaen and Li, Teng},
  year={2020}
}

@article{wen2020photorealistic,
  title={Photorealistic audio-driven video portraits},
  author={Wen, Xin and Wang, Miao and Richardt, Christian and Chen, Ze-Yin and Hu, Shi-Min},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={26},
  number={12},
  pages={3457--3466},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wu2021imitating,
  title={Imitating arbitrary talking style for realistic audio-driven talking face synthesis},
  author={Wu, Haozhe and Jia, Jia and Wang, Haoyu and Dou, Yishun and Duan, Chao and Deng, Qingshan},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={1478--1486},
  year={2021}
}

@article{xiao2021tackling,
  title={Tackling the generative learning trilemma with denoising diffusion gans},
  author={Xiao, Zhisheng and Kreis, Karsten and Vahdat, Arash},
  journal={arXiv preprint arXiv:2112.07804},
  year={2021}
}

@inproceedings{xie2021towards,
  title={Towards realistic visual dubbing with heterogeneous sources},
  author={Xie, Tianyi and Liao, Liucheng and Bi, Cheng and Tang, Benlai and Yin, Xiang and Yang, Jianfei and Wang, Mingjie and Yao, Jiali and Zhang, Yang and Ma, Zejun},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={1739--1747},
  year={2021}
}

@article{yang2022diffsound,
  title={Diffsound: Discrete diffusion model for text-to-sound generation},
  author={Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
  journal={arXiv preprint arXiv:2207.09983},
  year={2022}
}

@article{yang2022diffusion,
  title={Diffusion models: A comprehensive survey of methods and applications},
  author={Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  journal={arXiv preprint arXiv:2209.00796},
  year={2022}
}

@article{yang2022VIDEO,
  title={Diffusion probabilistic modeling for video generation},
  author={Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
  journal={arXiv preprint arXiv:2203.09481},
  year={2022}
}

@article{yi2020audio,
  title={Audio-driven talking face video generation with learning-based personalized head pose},
  author={Yi, Ran and Ye, Zipeng and Zhang, Juyong and Bao, Hujun and Liu, Yong-Jin},
  journal={arXiv preprint arXiv:2002.10137},
  year={2020}
}

@article{zhang20213d,
  title={3d talking face with personalized pose dynamics},
  author={Zhang, Chenxu and Ni, Saifeng and Fan, Zhipeng and Li, Hongbo and Zeng, Ming and Budagavi, Madhukar and Guo, Xiaohu},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2021},
  publisher={IEEE}
}

@inproceedings{zhang2021facial,
  title={Facial: Synthesizing dynamic talking face with implicit attribute learning},
  author={Zhang, Chenxu and Zhao, Yifan and Huang, Yifei and Zeng, Ming and Ni, Saifeng and Budagavi, Madhukar and Guo, Xiaohu},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3867--3876},
  year={2021}
}

@article{zhang2022motiondiffuse,
  title={Motiondiffuse: Text-driven human motion generation with diffusion model},
  author={Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
  journal={arXiv preprint arXiv:2208.15001},
  year={2022}
}

@inproceedings{zhang2021flow,
  title={Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset},
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3661--3670},
  year={2021}
}

@inproceedings{zhao2021sparse,
  title={Sparse to dense motion transfer for face image animation},
  author={Zhao, Ruiqi and Wu, Tianyi and Guo, Guodong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1991--2000},
  year={2021}
}

@inproceedings{zhou2019talking,
  title={Talking face generation by adversarially disentangled audio-visual representation},
  author={Zhou, Hang and Liu, Yu and Liu, Ziwei and Luo, Ping and Wang, Xiaogang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={9299--9306},
  year={2019}
}

@inproceedings{zhou2021pose,
  title={Pose-controllable talking face generation by implicitly modularized audio-visual representation},
  author={Zhou, Hang and Sun, Yasheng and Wu, Wayne and Loy, Chen Change and Wang, Xiaogang and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4176--4186},
  year={2021}
}

@article{zhou2020makelttalk,
  title={Makelttalk: speaker-aware talking-head animation},
  author={Zhou, Yang and Han, Xintong and Shechtman, Eli and Echevarria, Jose and Kalogerakis, Evangelos and Li, Dingzeyu},
  journal={ACM Transactions on Graphics (TOG)},
  volume={39},
  number={6},
  pages={1--15},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{zhu2021arbitrary,
  title={Arbitrary talking face generation via attentional audio-visual coherence learning},
  author={Zhu, Hao and Huang, Huaibo and Li, Yi and Zheng, Aihua and He, Ran},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={2362--2368},
  year={2021}
}

@inproceedings{zhu2017unpaired,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2223--2232},
  year={2017}
}

% Speech2Vid -
@InProceedings{Chung17b,
  author       = "Joon~Son Chung and Amir Jamaludin and Andrew Zisserman",
  title        = "You said that?",
  booktitle    = "British Machine Vision Conference",
  year         = "2017",
}
% RSDGAN -
@article{Vougioukas2018EndtoEndSF,
  title={End-to-End Speech-Driven Facial Animation with Temporal GANs},
  author={Konstantinos Vougioukas and Stavros Petridis and Maja Pantic},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.09313}
}
% CRAN -
@inproceedings{song2019talking,
  title={Talking face generation by conditional recurrent adversarial network},
  author={Song, Yang and Zhu, Jingwen and Li, Dawei and Wang, Andy and Qi, Hairong},
  booktitle={Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  pages={919--925},
  year={2019}
}
% chen et al.
@article{chen2021talking,
  title={Talking head generation with audio and speech related facial action units},
  author={Chen, Sen and Liu, Zhilei and Liu, Jiaxing and Yan, Zhengxiang and Wang, Longbiao},
  journal={arXiv preprint arXiv:2110.09951},
  year={2021}
}
% lipnet
@article{assael2016lipnet,
  title={LipNet: End-to-End Sentence-level Lipreading},
  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and de Freitas, Nando},
  journal={GPU Technology Conference},
  year={2017}
}
% openface Facial Behavior Analysis Toolkit
@INPROCEEDINGS{openface,
  author={Baltrusaitis, Tadas and Zadeh, Amir and Lim, Yao Chong and Morency, Louis-Philippe},
  booktitle={2018 13th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2018)}, 
  title={OpenFace 2.0: Facial Behavior Analysis Toolkit}, 
  year={2018},
  volume={},
  number={},
  pages={59-66},
  doi={10.1109/FG.2018.00019}}

% openface FR
@techreport{amos2016openface,
  title={OpenFace: A general-purpose face recognition
    library with mobile applications},
  author={Amos, Brandon and Bartosz Ludwiczuk and Satyanarayanan, Mahadev},
  year={2016},
  institution={CMU-CS-16-118, CMU School of Computer Science},
}

% CPBD
@article{narvekar2011no,
  title={A no-reference image blur metric based on the cumulative probability of blur detection (CPBD)},
  author={Narvekar, Niranjan D and Karam, Lina J},
  journal={IEEE Transactions on Image Processing},
  volume={20},
  number={9},
  pages={2678--2683},
  year={2011},
  publisher={IEEE}
}
% ACD
@inproceedings{tulyakov2018mocogan,
  title={Mocogan: Decomposing motion and content for video generation},
  author={Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1526--1535},
  year={2018}
}
% Facial AUs
@inproceedings{Ekman1978FacialAC,
  title={Facial action coding system: a technique for the measurement of facial movement},
  author={Paul Ekman and Wallace V. Friesen},
  year={1978}
}

%Wav2CLIP的引用

@article{bommasani2021opportunities,
  title={On the Opportunities and Risks of Foundation Models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{tzinis2020into,
  title={Into the wild with audioscope: Unsupervised audio-visual separation of on-screen sounds},
  author={Tzinis, Efthymios and Wisdom, Scott and Jansen, Aren and Hershey, Shawn and Remez, Tal and Ellis, Daniel PW and Hershey, John R},
  journal={ICLR},
  year={2021}
}

@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12873--12883},
  year={2021}
}

@article{alwassel2019self,
  title={Self-supervised learning by cross-modal audio-video clustering},
  author={Alwassel, Humam and Mahajan, Dhruv and Korbar, Bruno and Torresani, Lorenzo and Ghanem, Bernard and others},
  journal={NeurIPS},
  year={2020}
}

@article{recasens2021broaden,
  title={Broaden Your Views for Self-Supervised Video Learning},
  author={Recasens, Adri{\`a} and Luc, Pauline and Alayrac, Jean-Baptiste and Wang, Luyu and Strub, Florian and Tallec, Corentin and Malinowski, Mateusz and Patraucean, Viorica and Altch{\'e}, Florent and others},
  journal={ICCV},
  year={2021}
}

@inproceedings{zhou2018visual,
  title={Visual to sound: Generating natural sound for videos in the wild},
  author={Zhou, Yipin and Wang, Zhaowen and Fang, Chen and Bui, Trung and Berg, Tamara L},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3550--3558},
  year={2018}
}

@article{alayrac2020self,
  title={Self-Supervised MultiModal Versatile Networks.},
  author={Alayrac, Jean-Baptiste and Recasens, Adria and Schneider, Rosalia and Arandjelovic, Relja and Ramapuram, Jason and De Fauw, Jeffrey and Smaira, Lucas and Dieleman, Sander and Zisserman, Andrew},
  journal={NeurIPS},
  volume={},
  number={},
  pages={},
  year={2020}
}

@inproceedings{kolesnikov2020big,
  title={Big transfer (bit): General visual representation learning},
  author={Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  booktitle={ECCV},
  pages={491--507},
  year={2020},
  organization={Springer}
}


@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={ECCV},
  pages={104--120},
  year={2020},
  organization={Springer}
}

@article{chen2020big,
  title={Big self-supervised models are strong semi-supervised learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2006.10029},
  year={2020}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={CVPR},
  pages={7464},
  year={2019}
}

@inproceedings{thoker2019cross,
  title={Cross-modal knowledge distillation for action recognition},
  author={Thoker, Fida Mohammad and Gall, Juergen},
  booktitle={ICIP},
  pages={},
  year={2019},
  organization={IEEE}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={ICML},
  pages={1597},
  year={2020},
  organization={PMLR}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent: A new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and others},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{gemmeke2017audio,
  title={Audio set: An ontology and human-labeled dataset for audio events},
  author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={ICASSP},
  pages={776--780},
  year={2017},
  organization={IEEE}
}

@article{schneider2019wav2vec,
  title={wav2vec: Unsupervised pre-training for speech recognition},
  author={Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  journal={Interspeech},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={NeurIPS},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@inproceedings{gutmann2010noise,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={AISTATS},
  pages={297--304},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{zbontar2021barlow,
  title={Barlow twins: Self-supervised learning via redundancy reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  journal={arXiv preprint arXiv:2103.03230},
  year={2021}
}

@techreport{wuyusong2020_t6,
    Author = "Wu, Yusong and Chen, Kun and Wang, Ziyue and Zhang, Xuan and Nian, Fudong and Li, Shengchen and Shao, Xi",
    title = "Audio Captioning Based on Transformer and Pre-Training for 2020 {DCASE} Audio Captioning Challenge",
    institution = "DCASE Challenge",
    year = "2020",
    month = "",
    abstract = "This report proposes an automated audio captioning model for the 2020 DCASE audio captioning challenge. In this challenge, a model is required to be trained from scratch to generate natural language descriptions of a given audio signal. However, as limited data available and restrictions on using pre-trained models trained by external data, training directly from scratch can result in poor performance where acoustic events and language are poorly modeled. For better acoustic event and language modeling, a sequence-to-sequence model is proposed which consists of a CNN encoder and a Transformer decoder. In the proposed model, the encoder and word embedding are firstly pre-trained. Regulations and data augmentations are applied during training, while fine-tuning is applied after training. Experiments show that the proposed model can achieve a SPIDEr score of 0.227 on audio captioning performance."
}

@inproceedings{piczak2015dataset,
  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},
  author = {Piczak, Karol J.},
  booktitle = {ACM Multimedia},
  date = {2015-10-13},
  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},
  doi = {10.1145/2733373.2806390},
  location = {{Brisbane, Australia}},
  isbn = {978-1-4503-3459-4},
  publisher = {{ACM Press}},
  pages = {1015},
  year = {2015}
}

@article{spijkervet2021contrastive,
  title={Contrastive Learning of Musical Representations},
  author={Spijkervet, Janne and Burgoyne, John Ashley},
  journal={ISMIR},
  year={2021}
}

@inproceedings{Salamon:UrbanSound:ACMMM:14,
    Address = {Orlando, FL, USA},
    Author = {Salamon, J. and Jacoby, C. and Bello, J. P.},
    Booktitle = {ACM Multimedia},
    Month = {Nov.},
    Pages = {1041--1044},
    Title = {A Dataset and Taxonomy for Urban Sound Research},
    Year = {2014}}


@InProceedings{ramesh2021zero,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {PMLR},
  pages = 	 {8821--8831},
  year = 	 {2021},
  volume = 	 {139},
  month = 	 {18--24 Jul},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={NeurIPS Deep Learning Workshop},
  year={2014}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@inproceedings{wu2021multi,
  title={Multi-Task Self-Supervised Pre-Training for Music Classification},
  author={Wu, Ho-Hsiang and Kao, Chieh-Chi and Tang, Qingming and Sun, Ming and McFee, Brian and Bello, Juan Pablo and Wang, Chao},
  booktitle={ICASSP},
  pages={556--560},
  year={2021},
  organization={IEEE}
}

@inproceedings{ravanelli2020multi,
  title={Multi-task self-supervised learning for robust speech recognition},
  author={Ravanelli, Mirco and Zhong, Jianyuan and Pascual, Santiago and Swietojanski, Pawel and Monteiro, Joao and Trmal, Jan and Bengio, Yoshua},
  booktitle={ICASSP},
  pages={6989--6993},
  year={2020},
  organization={IEEE}
}

@article{gong2021psla,
  title={PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation},
  author={Gong, Yuan and Chung, Yu-An and Glass, James},
  journal={arXiv preprint arXiv:2102.01243},
  year={2021}
}

@techreport{Fedorishin2021,
    Author = "Fedorishin, Dennis and Sankaran, Nishant and Mohan, Deen and Birgiolas, Justas and Schneider, Philip and Setlur, Srirangaraj and Govindaraju, Venu",
    title = "Investigating Waveform and Spectrogram Feature Fusion for Audio Classification",
    institution = "DCASE2021 Challenge",
    year = "2021",
    month = "June",
    abstract = "This technical report presents our submitted system for the DCASE 2021 Challenge Task1B: Audio-Visual Scene Classifica- tion. Focusing on the audio modality only, we investigate the use of two common feature representations within the audio understand- ing domain, the raw waveform and Mel-spectrogram, and measure their degree of complementarity when using both representations in a fusion setting. We introduce a new model paradigm for audio clas- sification by fusing features learned from Mel-spectrograms and the raw waveform from separate feature extraction branches. Our ex- perimental results show that our proposed fusion model has a 4.5\% increase in validation accuracy and a reduction of .14 in validation loss over the Task 1B baseline audio-only subnetwork. We further show that learned features of raw waveforms and Mel-spectrograms are indeed complementary to each other and that there is a consis- tent classification performance improvement over models trained on Mel-spectrograms alone."
}

@article{guzhov2021audioclip,
  title={AudioCLIP: Extending CLIP to Image, Text and Audio},
  author={Guzhov, Andrey and Raue, Federico and Hees, J{\"o}rn and Dengel, Andreas},
  journal={arXiv preprint arXiv:2106.13043},
  year={2021}
}

@INPROCEEDINGS{Kazakos2021SlowFastAuditory,
  author={Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  booktitle={ICASSP}, 
  title={Slow-Fast Auditory Streams for Audio Recognition}, 
  year={2021},
  pages={855-859},
  doi={10.1109/ICASSP39728.2021.9413376}}

@article{fonseca2020fsd50k,
  title={FSD50k: an open dataset of human-labeled sound events},
  author={Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier},
  journal={arXiv preprint arXiv:2010.00475},
  year={2020}
}

@misc{musdb18-hq,
  author       = {Rafii, Zafar and
                  Liutkus, Antoine and
                  Stöter, Fabian-Robert and
                  Mimilakis, Stylianos Ioannis and
                  Bittner, Rachel},
  title        = {MUSDB18-HQ - an uncompressed version of MUSDB18},
  month        = aug,
  year         = 2019,
  doi          = {10.5281/zenodo.3338373},
  url          = {https://doi.org/10.5281/zenodo.3338373}
}

@INPROCEEDINGS{9415085,
  author={Wang, Shanshan and Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  booktitle={ICASSP}, 
  title={A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis}, 
  year={2021},
  volume={},
  number={},
  pages={626-630},
  doi={10.1109/ICASSP39728.2021.9415085}}

@article{aytar2016soundnet,
  title={Soundnet: Learning sound representations from unlabeled video},
  author={Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  journal={NeurIPS},
  volume={29},
  pages={892--900},
  year={2016}
}

@inproceedings{arandjelovic2017look,
  title={Look, listen and learn},
  author={Arandjelovic, Relja and Zisserman, Andrew},
  booktitle={ICCV},
  pages={609--617},
  year={2017},
  organization={IEEE}
}

@inproceedings{cramer2019look,
  title={Look, listen, and learn more: Design choices for deep audio embeddings},
  author={Cramer, Jason and Wu, Ho-Hsiang and Salamon, Justin and Bello, Juan Pablo},
  booktitle={ICASSP},
  pages={3852--3856},
  year={2019},
  organization={IEEE}
}

@inproceedings{Turpault2019_DCASE,
    Author = "Turpault, Nicolas and Serizel, Romain and Parag Shah, Ankit and Salamon, Justin",
    title = "{Sound event detection in domestic environments with weakly labeled data and soundscape synthesis}",
    booktitle = "{DCASE}",
    address = "New York City, United States",
    year = "2019",
    month = "October",
    keywords = "Sound event detection ; Weakly labeled data ; Semi-supervised learning ; Synthetic data",
    abstract = "This paper presents Task 4 of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and provides a first analysis of the challenge results. The task is a followup to Task 4 of DCASE 2018, and involves training systems for large-scale detection of sound events using a combination of weakly labeled data, i.e. training labels without time boundaries, and strongly-labeled synthesized data. The paper introduces Domestic Environment Sound Event Detection (DESED) dataset mixing a part of last year dataset and an additional synthetic, strongly labeled, dataset provided this year that we’ll describe more in detail. We also report the performance of the submitted systems on the official evaluation (test) and development sets as well as several additional datasets. The best systems from this year outperform last year’s winning system by about 10\% points in terms of F-measure.",
    hal_id = "hal-02160855",
    hal_version = "v2",
    url = "https://hal.inria.fr/hal-02160855"
}

@inproceedings{Drossos_2020_icassp,
    Author = "Drossos, Konstantinos and Lipping, Samuel and Virtanen, Tuomas",
    title = "Clotho: {An} Audio Captioning Dataset",
    booktitle = "ICASSP",
    address = "",
    year = "2020",
    month = "May",
    abstract = "Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online (https://zenodo.org/record/3490684).",
    url = "https://arxiv.org/abs/1910.09387"
}