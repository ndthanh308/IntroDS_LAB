\section{Experimental setup}
\label{sec:setup}
\vskip -0.5ex
In this section we provide the reader with some insights regarding the evaluation setup used to assess the performances of the proposed detector.
First, we describe the datasets considered for training and testing the system.
Then, we illustrate the training procedure used to train the \gls{fc} networks.
Finally, we define the anti-forensics attacks used to test the robustness of the model. 

\vspace{.5em}
\noindent
\textbf{Considered datasets.}
During all the experiments, we considered several datasets $\mathcal{D}$ to train and test the proposed synthetic speech detector. These include both real and deepfake speech samples for a total of more than \num{175000} audio tracks.
We used the following datasets to test the robustness of the proposed system in real-world conditions and verify that it is not overfitting to one set or domain.

\begin{itemize}[leftmargin=*]

    \item{\emph{ASVspoof 2019}} \cite{todisco2019asvspoof} is a speech audio dataset created to develop antispoofing techniques for automatic speaker verification. We consider its \gls{la} partition of the dataset. It contains both real and deepfake speech data generated with several techniques, including both \gls{tts} and \gls{vc} systems. In particular, the \textit{train} and \textit{dev} partitions include authentic signals along with speech samples generated with \num{6} different algorithms (named A01, A02, ..., A06), while the \textit{eval} partition comprises real signals and samples from \num{13} other algorithms (A07, ..., A19).
   
    \item{\emph{LJSpeech}} (LJS) \cite{ljspeech} contains audio clips of a single speaker reciting pieces from non-fiction books.

    \item{\emph{LibriSpeech}} (LS) \cite{panayotov2015librispeech} is an open-source dataset containing audio clips of authentic speech. From this corpus we considered the subset \textit{train-clean-100}.
    
    \item{\emph{Cloud2019}} corresponds to the dataset proposed in \cite{Lieto2019}. It includes tracks from different speakers generated considering several \gls{tts} cloud services: Amazon AWS Polly (PO), Google Cloud Standard (GS), Google Cloud WaveNet (GW), Microsoft Azure (AZ), and IBM Watson (WA).
   
    \item{\emph{VidTIMIT}} \cite{sanderson2009multi} comprises audio-video recordings of \num{43} people reciting short sentences and recorded in an office environment. We consider only the audio signals of these data since the videos are not related to the task at hand.


\end{itemize}

\vspace{.5em}
\noindent
\textbf{Training setup.}
We train the complete model in an end-to-end fashion, considering the whole system as a larger network, allowing it to identify the best configuration of parameters for the final classification.
Alternatively, we could train the single models individually for the speech deepfake detection task, performing a \textit{Late Fusion}~\cite{salvi2023robust} between the extracted embeddings.
However, we experienced that this second method led to poorer performance, so that all the results reported in the next section consider the end-to-end training strategy.

We trained the model for \num{100} epochs by monitoring the value of the validation loss, computed considering the Cross-Entropy function.
We assumed \num{10} epochs as early stopping, a batch size of \num{128}, and a learning rate $lr=$\num{e-4} appropriately reduced on plateaux.
Finally, the features we input to the model are normalized linearly between \num{0} and \num{1}.

Depending on the experiments, we trained the models on different datasets. 
In the first one, we only considered ASVspoof 2019 both in training and test. We did so to obtain results comparable with those of the papers which introduced the feature sets we are using \cite{mari2022sound, borrelli2021synthetic, albadawy2019detecting}.
We assumed the detectors of the single papers as baselines to benchmark our performance.
In this case, due to the high imbalance in the number of real and fake tracks contained in the ASVspoof dataset, we trained the models making sure we had the same number of samples of the two classes in each batch.

In the following experiments, on the other hand, we increased the number of real samples contained in the training set, considering both ASVspoof 2019 and LibriSpeech as training datasets.
This was done to balance the two classes and provide a broader distribution of real samples, improving the system's detection capabilities on unseen data.
In this case, however, we experienced unstable training due to the different distribution of the features extracted from the real samples of ASVspoof and LibriSpeech.
To address this issue, we decided to train the network by weighting the cost of each sample with weight	$$w(D, L) = \frac{1}{\lvert S(D, L)\rvert},$$
where $\lvert S(D, L)\rvert$ is the cardinality of the set of samples with label $L \in \{0, 1\}$ in dataset $D \in \{\text{ASVspoof }train, \text{Librispeech}\}$.
This proved effective in stabilizing the descent of the validation loss as it forced the classifier to learn meaningful features to discern real and fake samples instead of separating the distributions of the ASVspoof and Librispeech data.

To summarize, we have considered two different dataset configurations in training. In the first one we trained only on ASVspoof \textit{train} and \textit{dev} with batch balancing. We used this setup to benchmark the performances of our system with those of the reference papers~\cite{mari2022sound, borrelli2021synthetic, albadawy2019detecting}, which have been trained on these data.
In the second we trained the system ASVspoof and LibriSpeech with sample weighting. We considered this setup for the more challenging experiments, such as dataset generalization and anti-forensics attacks.

\vspace{.5em}
\noindent
\textbf{Anti-forensics attacks.}
The detection performance achieved by anti-deepfake systems may not be entirely significant in a real-world scenario. In fact, the quality of the ``in-the-wild'' signals degrades because of editing and formatting operations, thus compromising the accuracy of deepfake detection systems. This is the case of speech tracks found on the web or social platforms, often subject to compression and post-processing operations.

In this work, we test the robustness of the proposed detector for two of these operations, namely Gaussian noise injection and MP3 compression.
We do so since the presence of white noise could hide some artifacts introduced by synthetic audio generators, while compression is an operation often present in real-world scenarios.
We performed all post-processing operations using the Python \textit{audiomentations}~\cite{audiomentations}
library, 
considering three different noise levels with amplitude \textit{std} = [0.1, 0.01, 0.001] corresponding to \textit{SNR} = [2, 22, 42], and two different compression values (\textit{bitrate} [\si{kbit/s}] = [128, 32]).
