\section{Proposed system}
\label{sec:method}
\vskip -0.5ex
The proposed detector processes an audio signal $\mathbf{x}$
containing speech and classifies it with a label $y \in \{\text{REAL}, \text{FAKE}\}$, where REAL identifies authentic speech and FAKE indicates synthetically generated signals.
To do so, we consider three different feature sets proposed in the literature for synthetic speech detection and fuse them in a new system that performs the same task.
In particular, these correspond to the \gls{fd} features proposed in~\cite{mari2022sound}, the \gls{stlt} features presented in~\cite{borrelli2021synthetic} and the bicoherence features introduced in~\cite{albadawy2019detecting}.
We refer to these features as $\mathbf{f}_{\text{FD}}$, $\mathbf{f}_{\text{STLT}}$, and $\mathbf{f}_{\text{B}}$, respectively.
Regarding the \gls{fd} features $\mathbf{f}_{\text{FD}}$, we compute them only on the silenced parts of the signal under analysis, as it proved to be equally discriminative with respect to calculating them on the whole audio track~\cite{mari2022sound}.

We computed the feature sets $\mathbf{f}_{\text{FD}}$, $\mathbf{f}_{\text{STLT}}$ and $\mathbf{f}_{\text{B}}$ as proposed in the respective papers~\cite{mari2022sound, borrelli2021synthetic, albadawy2019detecting},  and we used them as inputs of the proposed models.
The dimensions of the three vectors are respectively $N_{\text{FD}}=$\num{416}, $N_{\text{STLT}}=$\num{800} and $N_{\text{B}}=$\num{8}. This could be a problem as, when using deep learning models, a high imbalance in the feature size can give the classifier a hard time learning how to assign the correct weight to its inputs.
To solve this issue, we projected the feature sets into different feature spaces using embedding extractors.
The resulting embeddings, that we call $\mathbf{e}_{\text{FD}}$, $\mathbf{e}_{\text{STLT}}$ and $\mathbf{e}_{\text{B}}$, contain \num{32}, \num{64} and \num{16} features respectively so that they have much more comparable size facilitating their integration.

The proposed system consists of four different blocks that need to be trained. Three are the \gls{fc} networks used to generate the embeddings $\mathbf{e}_{\text{FD}}$, $\mathbf{e}_{\text{STLT}}$, $\mathbf{e}_{\text{B}}$ from the input features, and one is the final model that performs the detection from the concatenation of the embeddings $\mathbf{e}_{\text{ALL}} = [\mathbf{e}_{\text{FD}}; \; \mathbf{e}_{\text{STLT}}; \; \mathbf{e}_{\text{B}} ].$
Table~\ref{tab:models} shows the architectures of the considered models, where the \gls{fc} layers indicated in bold are those used to extract the embeddings.
In all the networks we considered LeakyReLU as activation function and Softmax as output.

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{12pt}
\caption{Architectures of the considered models. The \gls{fc} layers indicated in bold are those used to extract the embeddings.}
\label{tab:models}
\resizebox{.7\columnwidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{\gls{fd} model} & \textbf{\gls{stlt} model} & \textbf{Bicoh. model} & \textbf{Fusion model} \\ \midrule \midrule
$\mathbf{f}_{\text{FD}}$: (416, 1) & $\mathbf{f}_{\text{STLT}}$: (800, 1) & $\mathbf{f}_{\text{B}}$: (8, 1) & $\mathbf{f}_{\text{ALL}}$: (112, 1) \\ \midrule
FC (416, 128) & FC (800, 512) & FC (8, 32) & FC (112, 32)    \\
Dropout (0.25) & Dropout (0.25) & Dropout (0.25) & Dropout (0.25)  \\
BatchNorm1D & BatchNorm1D & BatchNorm1D & BatchNorm1D \\ \midrule
FC (128, 64) & \textbf{FC (512, 64)} & \textbf{FC (32, 16)}     & FC (32, 2) \\
Dropout (0.25) & Dropout (0.25) & Dropout (0.25) &  \\
BatchNorm1D & BatchNorm1D & BatchNorm1D & \\ \midrule
\textbf{FC (64, 32)}     & FC (64, 2)               & FC (16, 2)               &                 \\
Dropout (0.25)           &                          &                 \\
BatchNorm1D              &                          &                          &                 \\ \midrule
FC (32, 2)               &                          &                          &                 \\ \midrule \midrule
$\mathbf{e}_{\text{FD}}$: (32, 1) & $\mathbf{e}_{\text{STLT}}$: (64, 1) & $\mathbf{e}_{\text{B}}$: (16, 1) & //    \\ \bottomrule
\end{tabular}}
\vspace{-1.5em}
\end{table}

To summarize, the complete pipeline of the detection process is structured in three steps, as shown in Figure~\ref{fig:pipeline}.
First, the feature vectors $\mathbf{f}_{\text{FD}}$, $\mathbf{f}_{\text{STLT}}$, and $\mathbf{f}_{\text{B}}$ are extracted from the input speech signal under analysis $\mathbf{x}$.
Then, the three sets of features are given as input to the three separate \gls{fc} neural networks to perform dimensionality reduction and extract the embeddings $\mathbf{e}_{\text{FD}}$, $\mathbf{e}_{\text{STLT}}$ and $\mathbf{e}_{\text{B}}$.
Finally, a \gls{fc} network takes as input the concatenation $\mathbf{e}_{\text{ALL}}$ of the three generated embeddings and performs synthetic speech detection. The output of this model is $\hat{y}$, corresponding to the estimation of the class $y$ of the signal $\mathbf{x}$.

We decided to adopt a deep-learning-based approach to leverage most of the consistencies and inconsistencies within the analyzed feature sets.
The first three \gls{fc} models are used to perform a dimensionality reduction of the features, which helps create embeddings $\mathbf{e}$ that contain only the most crucial information of the corresponding feature vector $\mathbf{f}$.
In doing so, when we concatenate them, the features included in $\mathbf{e}_{\text{ALL}}$ are very informative for the task at hand and improve the detection capabilities of the system.

% Figure environment removed

