\section{Results}
\label{sec:results}
\vspace{ -0.5ex}
In this section we analyze the features considered during the speech deepfake detection task and assess the performances of the proposed detector in different scenarios. 
We express these in terms of \gls{roc} curves, \gls{auc} and balanced accuracy.
Optimal performances are reached when \gls{auc} and balanced accuracy are equal to one.

\vspace{.2em}
\noindent
\textbf{Feature analysis.}
As a first experiment, we analyze the characteristics of each of the considered feature sets. 
We want to investigate how much the information they provide is correlated to avoid the computation of redundant data.
Since the proposed system performs a fusion of the three different sets of features, we want to ensure the content of the three is orthogonal so that we increase the amount of information fed to the model.
As previously mentioned, we use these features as they analyze three different aspects of a speech signal. $\mathbf{f}_{\text{FD}}$ contains information about silences, $\mathbf{f}_{\text{STLT}}$ models speech, while $\mathbf{f}_{\text{B}}$ exploits bispectral information.

To measure the correlation between the three sets, we compute the Pearson coefficient for each pair of elements of the feature vectors.
The resulting matrix describes cross-correlations between different features and auto-correlations of each vector.
Figure~\ref{fig:correlation matrix} shows the absolute values of the results of this analysis computed on the ASVspoof 2019 \textit{train} dataset.
There, we can identify different rectangular regions for each feature vector. The bicoherence features are not clearly visible since they are much less numerous.
Although the single feature vectors present a quite high degree of internal correlation, the cross coefficients between them are low. This means that the three feature vectors do not strongly correlate and do not share much information.
This motivates the joint use of these features, increasing the model's detection accuracy and robustness and the use of \gls{fc} networks to perform dimensionality reduction and drop the redundant information within each feature set.

% Figure environment removed

\iffalse
% Figure environment removed
\fi

\vspace{.2em}
\noindent
\textbf{Detection results.}
In this experiment we train and validate the proposed system respectively on the \textit{train} and \textit{dev} partitions of the ASVspoof 2019 dataset and test it on the \textit{eval} set.
We do the same with the single models that consider just one feature set at a time to verify that the fusion of the three actually improves the detection capabilities of the model.
Figure~\ref{fig:roc} shows the result of this analysis, where we can see that the fused model outperforms all the single ones and leads to better performances than the considered baselines.


Furthermore, we have also tried to see if a majority voting strategy between the predictions of the single classifiers was more effective than the proposed approach.
The end-to-end architecture shows superior performances that justify its use, with the balanced accuracy that increases by \num{6}\%.
This is probably because the end-to-end model can choose how to use and aggregate the information provided by the three feature sets, leading to better results.

% Figure environment removed


\vspace{.5em}
\noindent
\textbf{Generalization results.}
We now assess the generalization capabilities of the proposed model by testing it on unseen data during training. 
This is an important aspect in multimedia forensics, where the developed detectors are often tested in conditions other than those considered during training and must be able to provide reliable predictions.
The datasets we consider in this experiment are Cloud19~\cite{Lieto2019}, LJSpeech~\cite{ljspeech}, and VidTIMIT~\cite{sanderson2009multi}. 
To improve the system performance in this scenario, we train it on both ASVspoof 2019 and LibriSpeech, following the same approach proposed in~\cite{conti2022deepfake}.
During training, we considered the weight-based strategy presented in the previous section.

Figure~\ref{fig:barplot} 
shows the results of this analysis, where the detector proves to have good generalization capabilities by scoring high accuracy values.
Furthermore, the joint training on ASVspoof and LibriSpeech has improved the accuracy values achieved on datasets never seen before.
As a drawback, the same strategy lowers the performance on ASVspoof 2019 \textit{eval}, with a balanced accuracy value that goes from \num{83.3} to \num{78.2} on the same dataset.
This is probably due to the fact that the system improves its generalization performance and becomes less prone to overfitting on ASVspoof.

% Figure environment removed

\iffalse 

% Figure environment removed

\fi

\vspace{.5em}
\noindent
\textbf{Anti-forensics attacks.}
In this experiment we tested the robustness of the developed system to anti-forensics attacks.
Synthetic speech signals usually contain traces and artifacts left by the generators, which can be leveraged to discriminate them.
However, post-processing operations can corrupt or remove these, making the detection performance more challenging.
This is the case of media content circulating online, where the low quality and the post-processing applied to the signals make it challenging to classify them.
For this reason, being able to perform the deepfake detection task even on processed data is crucial.

We consider two anti-forensics attacks, Gaussian noise injection and MP3 compression, and evaluate how the detector performance is affected. As in the previous test, we consider the model trained on both ASVspoof 2019 and LibriSpeech.
A small change to the system must be implemented when we consider Gaussian noise injection.
Since the noise increases the power of the signal, it is unfeasible to detect the silenced regions (silence threshold needs to be automatically estimated as the analyst does not know the amount of added noise) and compute the \gls{fd} features on them. For this reason, we will consider \gls{fd} features computed on the whole signal. This should not affect the system final performance, as~\cite{mari2022sound} showed that the detector achieves equivalent scores when the \gls{fd} features are computed on the whole sample or the silenced parts only.

Figure~\ref{fig:attacks} 
shows the balanced accuracy values achieved on ASVspoof \textit{eval} in the considered cases.
These show that the proposed method is robust to MP3 compression since the accuracy does not seem to decrease even if we compress the sample considerably.
On the other hand, Gaussian noise appears to be more problematic as the prediction becomes almost random when injecting noise with a SNR value equal to \num{2}.


% Figure environment removed

\iffalse
\begin{table}[!t]
    \centering
    \caption{Accuracies on the test sets and balanced accuracies in the ASVSpoof-eval dataset with anti-forensic attacks}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        & ASVspoof \textit{eval} & Cloud19 & LJSpeech & VidTimit\\
        \hline
        Datasets & 78.2 & 85.2 & 90.7 & 100.0\\
        \hline
        & Reference & rate=128 & rate=32 & -\\
        \hline
        MP3 & 78.2 & 78.2 & 77.0 & -\\
        \hline
        & Reference & SNR=42 & SNR=22 & SNR=2\\
        \hline
        Noise & 79.9 & 74.9 & 73.0 & 50.0\\
        \hline
    \end{tabular}
    \label{tab:anti}
\end{table}

\fi
