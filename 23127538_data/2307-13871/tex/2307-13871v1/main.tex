\pdfoutput=1
\pdfoptionpdfminorversion=7

\documentclass[pre,onecolumn,12pt,aps,superscriptaddress,citeautoscript,tightenlines,nofootinbib]{revtex4-2}
% longbibliography only for PRR
% \documentclass[aps,pre,superscriptaddress,nofootinbib,preprint]{revtex4-2}

\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{footmisc}
\usepackage{bbold,soul}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{gensymb}
\usepackage{watermark}
\usepackage{mathtools}

% \watermark{
%     {\color{red} 
%         \textbf{PRE-RELEASE DRAFT: DO NOT DISTRIBUTE}
%     }
% }


\usepackage{mc}
\let\vec\mathbf


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % IF COMPILING FOR THE ARXIV, COMMENT THIS OUT
% % ALSO: include the SI explicitly at the end of this document

% %----Helper code for dealing with external references----
% % (by cyberSingularity at http://tex.stackexchange.com/a/69832/226)

% \usepackage{xr}
% \makeatletter

% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
% \typeout{(#1)}% latexmk will find this if $recorder=0
% % however, in that case, it will ignore #1 if it is a .aux or 
% % .pdf file etc and it exists! If it doesn't exist, it will appear 
% % in the list of dependents regardless)
% %
% % Write the following if you want it to appear in \listfiles 
% % --- although not really necessary and latexmk doesn't use this
% %
% \@addtofilelist{#1}
% %
% % latexmk will find this message if #1 doesn't exist (yet)
% \IfFileExists{#1}{}{\typeout{No file #1.}}
% }\makeatother

% \newcommand*{\myexternaldocument}[1]{%
% \externaldocument{#1}%
% \addFileDependency{#1.tex}%
% \addFileDependency{#1.aux}%
% }
% %------------End of helper code--------------

% % put all the external documents here!
% \myexternaldocument{section_SI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\title{Emulating Expert Insight: \\ A Robust Strategy for Optimal Experimental Design}
% Title Ideas % 
% A surrogate for generic scientific value
% A surrogate for generic scientific value drives epistemic research/measurement
% Driving epistemic research with a model-free surrogate for [scientific] value
% A model free surrogate for scientific value
% A model free surrogate for the [scientific] value of [multidimensional] observations
% Title Ideas %

\author{Matthew R.~Carbone}
\email{mcarbone@bnl.gov}
\affiliation{Computational Science Initiative, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Hyeong Jin Kim}
\affiliation{Center for Functional Nanomaterials, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Chandima Fernando}
\affiliation{National Synchroton Light Source II, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Shinjae Yoo}
\affiliation{Computational Science Initiative, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Daniel Olds}
\affiliation{National Synchroton Light Source II, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Howie Joress}
\thanks{orcid.org/0000-0002-6552-2972}
\affiliation{Materials Measurement Science Division, National Institute of Standards and Technology, Gaithersburg, MD 20899, USA}

\author{Brian DeCost}
\thanks{orcid.org/0000-0002-3459-5888}
\affiliation{Materials Measurement Science Division, National Institute of Standards and Technology, Gaithersburg, MD 20899, USA}

\author{Bruce Ravel}
\thanks{orcid.org/0000-0002-4126-872X}
\affiliation{Materials Measurement Science Division, National Institute of Standards and Technology, Gaithersburg, MD 20899, USA}

\author{Yugang Zhang}
\email{yuzhang@bnl.gov}
\affiliation{Center for Functional Nanomaterials, Brookhaven National Laboratory, Upton, NY 11973, USA}

\author{Phillip M.~Maffettone}
\email{pmaffetto@bnl.gov}
\affiliation{National Synchroton Light Source II, Brookhaven National Laboratory, Upton, NY 11973, USA}

\date{\today}


\begin{abstract}
%The fundamental task in optimal design of experiments is determining which experiment(s) to run next given observed data. In Bayesian Optimization (BO), this question can be answered when framed through the context of exploring a space or maximizing some observable, i.e., balancing the exploration/exploitation tradeoff. But what happens when we lack such an explicit objective, and simply want to explore ``interesting" regions, where arbitrary scalar or vector observables (such as spectra, or diffraction patterns) are the only information available to the experimenter? It is not obvious how to address this question when data is observed sequentially and is available only in limited quantities. In this work, we propose a heuristic solution to this question. We call this protocol for sequentially choosing the next experiment(s) the Scientific Value Agent (SVA). The SVA procedure derives a scalar quantity for the available set of input conditions and observables, called the scientific value, which roughly measures where observables change significantly, and is a proxy for interesting regions of the input space, such as phase boundaries. Our method is seamlessly compatible with the existing tools of BO, multi-modal and multi-fidelity experiments, can be implemented in only a few extra lines of code, and depends only on a simple, easy-to-implement heuristic.
The challenge of optimal design of experiments (DOE) pervades materials science, physics, chemistry, and biology. 
Bayesian optimization has been used to address this challenge in vast sample spaces, although it requires framing experimental campaigns through the lens of maximizing some observable.
This framing is insufficient for epistemic research goals that seek to comprehensively analyze a sample space, without an explicit scalar objective (e.g., the characterization of a wafer or sample library). 
In this work, we propose a flexible formulation of scientific value that recasts a dataset of input conditions and higher-dimensional observable data into a continuous, scalar  metric.
Intuitively, the scientific value function  measures where observables change significantly, emulating the perspective of experts driving an experiment, and can be used in collaborative analysis tools or as an objective for optimization techniques.
We demonstrate this technique by exploring simulated phase boundaries from different observables, autonomously driving a variable temperature measurement of a ferroelectric material, and providing feedback from a nanoparticle synthesis campaign. 
The method is seamlessly compatible with existing optimization tools, can be extended to multi-modal and multi-fidelity experiments, and can integrate existing models of an experimental system. 
Because of its flexibility, it can be deployed in a range of experimental settings for autonomous or accelerated experiments. 
\end{abstract}

\maketitle

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% Phil's Proposed Intro Outline %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% This is a mix of Whitesides' approach, and some sci comms lessons I've picked up. 
% I like to use: "Hook"--"Volley"--"Set"--"Spike". Mix in some "And"--"But"--"Therefore". Wrapped up in an "inverse pyramid" of general to specific. 
% Firstly a hook that demonstrates that the problem is important. The hook should have a strong "And"--"But"--"Therefore" structure. The pieces of this are already present, but I have some suggestions. 
% Secondly a volley/set. Set the probelm up for yourself, refining the scope downward, explain what is done and what is lacking/needed in the area. This naturally has a weak "And"--"But"--"Therefore" structure ("Done"--"Lacking"--"Needed").
% Lastly, go in for the kill with a "Spike". This is your thesis. It should mirror the volley/set completely, almost sounding redundant but with a different tone. This announces that all of the problems you laid out previously are solved in the attached work. 
% You can almost think of this as a truncated "Hero's Journey": A hook calls to adventure, the volley world builds, the set lays out the challenges to the point of a dire abyss, and the thesis is the transformation and victory. 

%%%%% Hook - Self driving labs (SDLs) are the why. %%%%%
% SDLs are big, scaling, and hit every area of impact your readership cares about (citation deluge of applications)
% AND: Federations of agents can be used, with many excellent examples of agents that can optimize a target property or agents that can optimize resource allocation for unrelated samples (citation deluge of agents)
% BUT: When target is system understanding, interpretation/interpretability, and/or complete characterization of space, we're up shit creek
% THERFORE: We need something better (sign posting the what is needed from the Volley/Set)
%%%%% Hook - Self driving labs (SDLs) are the why. %%%%%
The combination of  automation and artificial intelligence (AI) to create closed-loop self driving, autonomous laboratories---or human-interfaced acceleration platforms---has begun revolutionizing scientific research across chemistry~\cite{hase2019next, escalate}, biology~\cite{senior2020improved, Narayanan_2021}, and materials science~\cite{Stach_2021, campbell2021outlook,barbour2022advancing,konstantinova2022machine,seifrid2022reaching}.
These contemporary platforms mostly use single AI agents, but can also leverage the added value of multiple agents working in tandem~\cite{maffettone2023self}.
To date, most efforts in agent development have focused on designing algorithms that optimize a target value~\cite{Noack_2021} or resource allocation~\cite{gamification,mcdannald2022fly}.
Unfortunately, these approaches to agent design do not encompass the research settings where the objective is more epistemic; that is, the research pertains to comprehensive understanding or interpretation of an experimental space~\cite{mcdannald2022reproducible}, and not the optimization of a target. 
Examples of epistemic objectives are ubiquitous in characterization~\cite{MaffettoneMASS}, user facilities, and ``science as a service'' platforms~\cite{li2020autonomous}.
These bring forth a new challenge in how to leverage AI advancements for  optimal experiment design.
\par
%%%%% Volley/Setup  %%%%
% When it matters (in no particular order)
    % Limited priors. Model free
    % Fixed sample space
    % Fixed reaction space -> polymorph curiosity (Cronin lab NMR stuff)
    % Cannot cast as a multi arm bandid
    % Large plateus of steadyness and sharp minimum or changes (phase diagrams, protein folding has the right shape)
    % Device mapping
    % 3d mapping (tomography) <- really any mapping problem 
% What is done/lacking
    % Design of experiments covers robustness/stats of a known space
    % RL hits on limited resources (Bad seed paper)
    % Model dependence/specificity (Kinetics is a decent example for DesOfExp, exploring ordinate with bayes opt (expected information gain), diversification of abscissa)
    % BayesOpt attacks limited resources with a good target
% What is needed (anything in the thesis should be sign posted here)
    % Gives the scientist the optimal set of data to understand the problem space on revisiting
    % Model free ordinate 
    % Optimizable target that fits nicely with the advancements made above
%%%%% Volley/Setup  %%%%
Research motivated by comprehensive understanding of a system is common across disciplines. 
It appears  in problems involving spatial characterization or fixed sample spaces, such as device mapping~\cite{Hua_2021, Olds_2020}, tomography~\cite{barbour2022advancing}, or phase mapping~\cite{Kusne_2020,joress2020high}. 
It also is recurrent when searching large plateaus of space for sharp changes, such as in searching for reactivity~\cite{Caramelli_2021} or phase changes
~\cite{Maffettone_XCA, joress2020high}.
Nonetheless, when research questions are directed more by understanding a system than by optimizing the system for a single property, certain measurements will still prove more valuable than others. 
% But
Traditional DOE approaches the exploration of a known space to explain the variation of a response function in that space, albeit it is not adaptive and expects each input parameter to impact the response function~\cite{StatisticsForExperimenters}.
In the case of allocating limited resources over independent samples, reinforcement learning (RL) has been used to extract maximal value~\cite{gamification}.
When a model for the system is available Bayesian inference can be used to query data that will best mitigate the uncertainty of that model \cite{konstantinova2022_PRR,koplik2022topological,Kusne_2023_Matter}.
Bayesian optimization over expected information gain~\cite{balandat2020botorch} can be used to explore the experimental space; however, without a conversion between an observable and a finite objective, Bayesian optimization cannot be effectively leveraged. 
\par
%Therefore
% Made the 'therefore' a separate paragraph to break up the run-on. It stands alone with its own thesis sentence. 
With an epistemic goal, an optimal agent will therefore yield an experimental design that produces the best dataset for understanding the experimental space.
This understanding would be derived from expert interpretation, modeling, AI, or some combination of techniques. 
Furthermore, the agent should be able to operate with or without a model of space or the observable. 
It must also be robust to the ``cold start" problem~\cite{swersky2013multi}, operating efficiently under initially extremely data- and information-limited conditions.
Lastly, it would be beneficial for any agent to make use of contemporary advancements in optimization methods. 
\par
%%%%% Spike - Thesis %%%%
% In the following we a forumlation of scientific value that solves the needs. 
% We demonstrate this on X, Y, Z. 
% This creates opportunitites/impact in A, B, C. 
%%%%% Spike - Thesis %%%%
Herein, we propose a generic scientific value function (SVF) that recasts a dataset of observables into scalar measures of ``value" by mirroring the perspective and actions of human experts.
While natively model-free, the SVF can incorporate models of the experimental or observable space.
Crucially, it can be used as an optimization target in other procedures, such as Bayesian or Monte Carlo optimization~\cite{balandat2020botorch}.
We demonstrate the application of the SVF through: i) a simulated X-ray diffraction (XRD) phase mapping of first- and  second-order transitions; ii) a simulated absorption spectroscopy study of a periodic phase boundary; iii) a variable temperature X-ray total scattering study of BaTiO$_3$; and iv) an ultraviolet-visible (UV-vis) absorption spectroscopic analysis of nanoparticle synthesis conditions. 
The adaptive applications of the SVF are accomplished by optimizing the next measurement value over experimental space using a Gaussian process~\cite{Rasmussen2006GP} surrogate model and Bayesian optimization~\cite{shahriari2015taking,frazier2018tutorial,balandat2020botorch}. 
This work creates opportunities for optimal dataset creation and research acceleration without a pre-existing optimization target, and will find broad applicability across scientific disciplines. % Better to have specifics here...


\section{Results \& Discussion}

\subsection{A surrogate function for scientific value} \label{subsec:a surrogate function for scientific value}
% What niche is it trying to fill?
% Mimic the decisions of a scientist exploring a sample space without a model
% Prototypical eg of phase mapping a wafer under time constraints
% A normal scientific goal would be to measure every unique phase at least once, and then focus on regions of change
% At the start, the measurement of any location has the same value
% As the experiment progresses, regions where nothing is changing are less valuable than regions where things are changing
% Thus we define a value function U(x|D) that is dataset dependent and represents this intuition.
We set out to construct a surrogate function for scientific value that would emulate the judgement of an expert scientist without a model for their experimental system. 
Consider a prototypical example of mapping the phase diagram of a material over multiple dimensions.
A rational scientific goal would be to measure every unique phase at least once, and measure with greater resolution across phase boundaries. 
At the start of a campaign, the measurement of every location has the same potential value. 
As the campaign progresses, measuring the regions where the observable is not changing with the ordinate becomes less valuable than measuring regions of rapid change. We defined the Scientific Value Function (SVF), $U,$ to capture this intuition.

\par
% The big mathematical formalism paragraph
First, we consider an input space $\mathcal{X},$ where queries of $\vec x_i \in \mathcal{X}$ comprise a dataset $\mathcal{D}_N \coloneqq \{ (\vec x_1, \vec y_1), ... (\vec x_N, \vec y_N)\}$,
where $\vec y_i$ are noisy, multi-dimensional observations of some function, $\vecf(\vec x_i$), such as a diffraction image.
We further define two correlation functions for both the input space and the observation space, $h(\vec x_i, \vec x_j)$ and $g(\vec y_i, \vec y_j)$, respectively.
The default correlation function used in this work for both $h$ and $g$ is the Euclidean distance, or $L_2$ norm, $\norm{\cdot}_{L_2}$.
Thus, we define the dataset-dependent Scientific Value Function (SVF) as
\begin{equation}\label{eq:U(x,D)}
  U(\vecx_i, \vecy_i | \mathcal{D}_N) = \sum_{(\vec x_j, \vec y_j) \in \mathcal{D}_N}
  g(\vec y_i, \vec y_j)
  \exp{ - \frac{1}{2}\frac{h(\vec x_i, \vec x_j)^2}{
      h_{\mathrm{min}}(\vec x_i | \mathcal{D}_N)^2}
  },
\end{equation}
where $h_\mathrm{min}(\vecx_i | \mathcal{D}_N)$ is the distance between $\vecx_i$ and its nearest neighbor in $\mathcal{D}_N.$ Using Eq.~\eqref{eq:U(x,D)}, the scientific value can be computed for all inputs in $\mathcal{D}_N.$ 
\par

% Dive into the advantages/features of the formalism
% model free
% incorperate any distance metric: topological distance, levenshtine for discrete, latent distance 
% optimizable 
The SVF considers the individual value of a new datum with respect to each existing datum and sums over all members of the dataset for a net value. 
The first term considers where the observable is distinct from those contained in the dataset, and thus valuable. 
The second term decays that value with respect to how far the data are in the input space.
In order to avoid overestimating the value of local regions, the second term is regularized by the nearest neighbors of points in input space.
We considered other forms of the SVF that would use these correlation functions (\emph{e.g.,} proportionate or derivative-like functions); however, chose the form of the second term such that it would be limited between 1 and 0, and had options for regularization. 
\par

This formalism offers a few key features and advantages.
Firstly, it adequately reflects the intuition of researchers in practice.
It also reduces the dimensionality of the observable space to a scalar objective function that can be readily optimized.
While the approach is natively model free, the correlation functions $g$ and $h$ are flexible and can readily incorporate models of the system.
For instance, a discrete input space could easily use the Levenshtein or Manhattan distance.
With knowledge of the observable space, the distance in a latent space from a variational autoencoder has been used in early implementations of the SVF.
Even without a model of the observable space, more involved functions could be considered such as those from time resolved pair-correlation functions~\cite{konstantinova2022_PRR} or topological data analysis~\cite{koplik2022topological}. 

In the following we made use of Bayesian optimization over a Gaussian process (GP) surrogate model of SVF. 
The GP used a Matern kernel with homoskedastic noise to construct a probabilistic model of $U$ in all input space,  including regions where there are no observations. 
When conditioning the GP, we scaled the value of $U$ to $U\in [0, 1]$. 
We used the Expected Improvement (EI)~\cite{mockus1978application} and Upper Confidence Bound (UCB)~\cite{srinivas2009gaussian} acquisition functions for Bayesian optimization.
The UCB functions presented use a weighting for variance of $\beta$=10, although similar results were obtained for values of $\beta$ ranging from 10 to 100. 
\par

We benchmarked this against a common experimental design of measuring over an optimal grid given allotted resources (e.g., time or number of measurements), described here as grid search.
We note that despite the specific choices used in this work, the extensibility and flexibility of the SVF allows it to be used with any optimization protocol or probabilistic model that can approximate it. 
Herein, we call the SVF modeling procedure used in tandem with the tools of Bayesian optimization (or optimization in general) the Scientific Value Agent (SVA).

% %%%%%%%%%%%%%%%%%%%%%%%%%%Previous Matt %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The primary result of this work is called the Scientific Value Function (SVF) which, when used in tandem with the standard tools of Bayesian optimization (BO) provides a powerful auxiliary quantity for autonomous experimental design under data-limited conditions, where there is minimal (or no) prior knowledge and no explicit objective. The SVF is a representation of how much ``scientific value" can be gained by running an experiment at some experimental condition (which could be a position on a wafer, concentrations of reactants in a flow reactor experiment, etc.). When maximized at each iteration, it acts as a sampling protocol that produces a dataset for best understanding the experimental input space.

% To define scientific value, we consider the set of available experimental inputs, $X = \{\vecx_i\}_{i=1}^N,$ and their corresponding observations $\{\vecf(\vecx_i)\}_{i=1}^N$ (which can be vector or scalar-valued). If, for example, two points $\vecx_i$ and $\vecx_j,$ are close together and their observations are also similar, sampling another point in that neighborhood likely will not yield much additional information. Conversely, if their observations are significantly different, then sampling more points in that region is necessary in order to develop a complete understanding of that local neighborhood. These guiding principles were designed to be commensurate with experimental intuition, where the entire space is sampled sufficiently, but experiments are not wasted on regions where observables remain relatively constant.

% The SVF encodes these principles, and is henceforth defined as
% \begin{equation} \label{eq: U | X}
%     U(\vecx, X) = \sum_{\vecx' \in X} d\left(\vecf(\vecx), \vecf(\vecx')\right) w(\vecx, \vecx'),
%     %\exp{-\frac{1}{2} \frac{d^2(\vecx, \vecx')}{d^2(\vecx, \nearestNeighbor(\vecx, X) )}},
% \end{equation} % \nearestNeighbor(\vecx, X) = \mathrm{argmin}_{\vecx' \in X \backslash \vecx} d(\vecx, \vecx')
% where $d$ is some measure of distance, such as the Euclidean norm (which we use in this work) and $w$ is a weight function, which determines the contributions of the difference between the observations to the SVF. The distance function can also be used to encode prior knowledge, or can utilize a distance in some reduced dimensional space. Before every experiment, $U$ can be approximated by a probabilistic model (in this work, we use a GP), and the standard tools from BO can be used to attempt to maximize it in iterative fashion. Unlike traditional BO, $U$ is dynamic and depends on the current dataset; it is not a static source of truth.

% ... We provide the details of the SVA (the SVF in a BO autonomous experimentation loop) in SI, Section~\ref{SI-section:SVA+BO}.

% The remainder of this work is dedicated to presenting various examples in which the SVA procedure is shownd to \textit{significantly} outperform the gold standard of grid-based sampling.
% %%%%%%%%%%%%%%%%%%%%%%%%%%Previous Matt %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Characterizing a one-dimensional phase space with simulated X-ray diffraction} \label{subsec:xrd1dim}

We first tested the SVA \emph{in silico} using the simulated X-ray diffraction measurement of a library that contained linear mixtures of four phases [Fig.~\ref{fig:xrd1dim}]. 
This sampling of a one-dimensional space is common in studying phase behavior over composition or state variables \cite{maffettone2023self, Hua_2021, Olds_2017}.
The four XRD patterns corresponding to the phases were defined by a series of randomly placed Gaussian peaks over a constant background.
Normally distributed noise was introduced to the observation at each query [Fig.~S1]. 
%We begin with a realistic example to demonstrate the SVA procedure, called ``XRD1dim": a one-dimensional scan of a two-dimensional sample, where additional statistics are introduced by jittering the position along the orthogonal direction. Mock X-ray diffraction (XRD) patterns are treated as pure phases, and combined linearly in the proportions as shown in Fig.~\ref{fig:xrd1dim}(a). 
In order to simulate reasonable and challenging types of phase changes, we chose functional forms to represent first and second order transitions: sigmoidal to approximate a discontinuous first order transition, and linear and quadratic for different rates of second order.
These are highlighted in the three shaded regions of Figure~\ref{fig:xrd1dim}(a). 
In an optimal measurement of this compositional library, sampling density should be correlated to the rates of change of the phases.
\par

To quantify sampling performance, we tracked the mean squared error (MSE) between the true observation space (phase fractions) and the observation space that could be reconstructed by the sampled queries (assuming oracle knowledge of phase fractions given the observable). 
The reconstructed dataset is produced by linearly interpolating observations between measured points. 
As shown in Figure~\ref{fig:xrd1dim}(c), this metric will decay as more observations are made, with a smaller error corresponding to more robust sampling. 

% Figure environment removed

Figure~\ref{fig:xrd1dim}(b) shows the average sampling histogram of the SVA over 300 independent experiments. 
Even at small $N,$ we see that the three phase boundary regions were sampled in proportions commensurate with the rate of change of the phases in those regions. 
The linear change region was only sparsely sampled (but still sampled more compared to regions of no change), whereas the quadratic region was sampled much more densely. 
Nonetheless, the region of near-instantaneous change was sampled most densely and earliest, as the algorithm discovered  this  very sharp boundary, and therefore required more samples to produce an accurate representation of the observable in that region.


Both choices of acquisition function outperformed the optimal grid design  by roughly an order of magnitude. 
This performance was apparent in the low-$N$ and limiting cases. 
Not only did the SVA procedures propose experiments in relevant regions of space, they also modeled regions of significant change more efficiently than conventional methods.
\par

We also considered using Bayesian inference as a DOE benchmark. This approach first clustered the data, then trained a probabilistic regressor to predict the cluster labels, and finally queried new points where the uncertainty was maximized \cite{Kusne_2023_Matter}.  
We found this methodology to be strongly dependent on the chosen number of clusters, not necessarily more performant than a grid search in the limiting case, and less performant on low N [Fig.~S5]. 
While Bayesian inference can be a powerful tool \cite{Kusne_2023_Matter}, it is intrinsically model dependent, increasing in potency when a more accurate model for the system is available. 
In Bayesian inference, a label assignment is necessary (here accomplished by K-means clustering).  SVA doesn't require a model or labels, but can incorporate one through the correlation function, $g(\vec y_i, \vec y_j)$.
Considering this comparison and the prevalence of grid searches at actual beamlines, we chose the grid search technique as a our benchmark.



\subsection{Characterizing a two-dimensional space with a periodic interface} \label{subsec:sine2phase}
We completed a second \emph{in silico} test, which sought to characterize a two-dimensional library of sample compositions defined by coordinates $\vecx \in \mathbb{R}^2.$ 
In this case, the library contained only two phases, separated by a sharp periodic boundary [Fig.~\ref{fig:sine2phase}(a)]. 
The observation of phases and their mixtures was characterized by a spectrum, simulated using noisy Gaussian functions centered at two different locations in space [Fig.~S2]. 
As above, phases were linearly mixed, with the proportion of the phases given by a sigmoid function of the position on the wafer,
\begin{equation} \label{eq-prop-sinusoid}
    \begin{split}
    \quad b(x_1) = \frac{1}{2} + \frac{1}{4} \sin(2\pi x_1),
    \\
    p(\vecx) = \frac{1}{1 + \exp{-50 [x_2 - b(x_1)]}}.
    \end{split}
\end{equation}
Designed to be a drastic and challenging test, the resultant phase-dependence on position can be see in Figure~\ref{fig:sine2phase}(a).
\par


We compared the performance of the SVA against conventional methods using the same metrics from above. 
Again it was clear that an active learning approach coupled with SVF outperforms conventional measurement techniques [Fig.~\ref{fig:sine2phase}(c)]. 
Additionally, we examined how the SVA queried the space around the phase boundary.
Even in data-limited conditions, the approach successfully mapped out regions of significant change, while still sufficiently sampling relatively constant regions of phase space.
As shown in Figure~\ref{fig:sine2phase}(b), the sampling focused on the most information rich region,  highlighted around the curve  $b(x_1).$ 

% Figure environment removed

Compared to the conventional grid search benchmark, the SVA outperformed this baseline by roughly an order of magnitude.
The results of a single SVA experiment using the UCB acquisition function with a total of 250 samples show a dense sampling of the interface, without under-sampling the surrounding area.
Both UCB and EI behaved comparably and outperformed the baseline, with UCB being more exploitative of the narrow transition region.
Although trade-offs with acquisition functions are expected, optimizing over the SVF was robust regardless of acquisition function choice. 


\subsection{Characterizing the subtle phase transitions of Barium Titanate}
The final example we present in the active setting highlights the case where naive data-driven approaches fail~\cite{maffettone2021constrained}.
Furthermore, we used this to demonstrate the capacity to integrate more physics aware correlation functions into the SVF to improve the expressiveness of the surrogate modeling.
We emulated a continuous valued experiment in which total scattering data of BaTiO$_3$ were measured as a function of temperature, by interpolating a dataset measured over 5\textdegree\,C intervals at the Pair Distribution Function beamline at the National Synchrotron Lightsource II [Fig.~S3].
These data contains incredibly subtle transitions between four distinct crystallographic phases (rhombohedral, orthorhombic, tetrahedral, and cubic)
%($Pm\overline{3}m,$ $P4mm,$ $Amm2,$ and $R3m$) 
that are difficult to distinguish using data-driven approaches~\cite{maffettone2021constrained}.
Using established methods, we trained an ensemble of convolutional neural networks to predict these phases from simulated diffraction patterns, and used the trained models to create an encoding of the noisy experimental data~\cite{Maffettone_XCA}.
We used the SVA procedure to created a surrogate model for a SVF that used these encodings to calculate the observation space correlation function, $g(\vec y_i, \vec y_j)$.
\par

Following the same procedure as the previous examples, we showcase the results of the sampling as a function of the number of queries in Figure~\ref{fig:bto}(b).
The SVA correctly identified and attended to the phase transitions extracted by data refinement.
We compared approaches by considering the ability of the resultant dataset to construct the Rietveld refined compositions [Fig.~\ref{fig:bto}(a)].
Without the inclusion of a deep learned embedding, the SVA produced datasets on par with conventional methods; however, by combining the flexibility of the SVF with a deep learned embedding, it autonomously up-sampled the phase changes of BaTiO$_3$ [Fig.~S4].

% Figure environment removed


Because total scattering is a measure of bulk state, it captures more phase coexistence than is present locally throughout the sample, and the first-order phase transitions in BaTiO$_3$ appear gradual and continuous.
Subsequently, a grid search approach could reconstruct the bulk compositions from Rietveld refinements as well as, if not better than, the SVA approach  [Fig.~\ref{fig:bto}(c)].
Nonetheless, grid search methods failed to focus on the unique physical behavior of the transitions, which would be exposed through pair distribution function or spectroscopic analysis.   
This highlights the potential for the SVA to suggest clarifying experiments in the multifidelity or multimodal setting~\cite{maffettone2023self}.


\subsection{In-line analysis of nanoparticle synthesis}
% Channel 1: 2mM of HAuCl4 (HA)
% Channel 2: 16mM of NaCit (CN)
% Channel 3: 10mM HCl (pH)
% Channel 4: Buffer- 10mM PBS, pH 7.4 & 0.01% Tween
% Channel 5: 10mM NaOH (pH)
% Channel 6: Methylene Blue dye
% Channel 10: Silicone Oil

In the previous three examples, we demonstrated the use of the SVF in an adaptive setting across diverse problems relevant to materials science. 
We acknowledge that a variety of algorithms---or none at all---may be preferable for driving an experiment.
Therefore, we highlighted the breadth of the approach by applying the SVF in a passive analysis setting.
We deployed the SVF to visualize a spectroscopy dataset produced  during an automated nanoparticle synthesis experiment at the Center for Functional Nanomaterials at Brookhaven National Laboratory.
The  dataset consisted of $N=375$ experimental flow reactor conditions ($\vecx_i$) and the corresponding UV-vis absorption spectrum ($\vecy_i$). 
The nanoparticle synthesis experiments were performed in a flow reactor by varying 4 experimental parameters: the volume of sodium citrate (NaCit, 16~mmol/L), Chloroauric acid (HAuCl$_4,$ 2~mmol/L), hydrochloric acid (HCl, 10~mmol/L), and sodium hydroxide (NaOH, 10~mmol/L). 
The total volume of liquid in any experiment is always equal to 40 $\mu$L, (the size of the droplet in the flow reactor). 
This reduces the number of degrees of freedom to 3, wherein HCl and NaOH are used to drive the reaction pH.
UV-vis absorption spectra were then taken of the final reaction products. 
%Detailed methodology of the flow reactor can be found in Ref.~\onlinecite{}.
\par

The experiments were performed via domain experts using a grid search with manual intervention. 
To assist in processing the large dataset of measurements, $\mathcal{D}_N,$, we computed $U(\vecx_i, \vecy_i, \mathcal{D}_N)$ for all $(\vecx_i, \vecy_i) \in \mathcal{D}_N,$.
This offered the scientists a visual representation of scientific value [Fig.~\ref{fig:uv}(a)].
\par



% Figure environment removed

The analysis provided by the SVF equipped the scientist with an actionable visualization tool that keeps the human in the loop of an otherwise automated experiment.
The visualization highlighted regions of high scientific value, as the dataset was growing, by linking the positions in phase space to their individual spectra.
By cross referencing the regions of high value with their respective spectra, the scientist was able to examine the SVF analysis and validate it with their own insight. 
We expect the combination of this advancement with user interface engineering will create a potent tool that impacts a variety of analysis techniques. 

\section{Conclusion}

% Invert introduction, start specific and grow general. 
In this work, we presented the Scientific Value Function, which replicates the judgement of human experts, and recasts a dataset of higher dimensional observables into scalar measures of value. 
By quantifying value, the SVF creates an epistemic research objective that can be optimized without the need for feature engineering or a scalar observable.
We demonstrated the deployment of the SVF in an adaptive learning context (SVA), how it can be complemented by ML or data reduction techniques, and how it can be used in a streaming analysis deployment for visualizations that accelerate decision making by the human expert. 
Because the approach provides a flexible, experiment-agnostic path for building autonomous workflows, it has far reaching implications for accelerated science across physics, chemistry, materials, and biology. 



\begin{acknowledgements}
This research is supported in part by Brookhaven National Laboratory (BNL), Laboratory Directed Research and Development (LDRD) Grants: No. 22-059, ``Precision synthesis of multiscale nanomaterials through AI-guided robotics for advanced catalysts,â€ and No 23-039, ``Extensible robotic beamline scientist for self-driving total scattering studies." This research also used resources of the Center for Functional Nanomaterials, the PDF (28-ID-1) Beamline and resources of the National Synchrotron Light Source II, which are U.S. Department of Energy Office of Science User Facilities, at Brookhaven National Laboratory under Contract No. DE-SC0012704.
\end{acknowledgements}


% \section*{Author contributions}
% M.R.C. developed the algorithm, led the software development, and performed the \textit{in silico} experiments.
% H.J.K. and Y.Z. performed the flow reactor experiments.
% S.Y. provided expertise and guidance in the algorithm development. 
% D.O. performed  XRD measurements. D.O., H.J., B.D., B.R., and P.M.M. provided insight on applications at user facilities. 
% P.M.M. conceived the project with M.R.C., and advised the derivation and development. 
% The manuscript was prepared by P.M.M. and M.R.C., and edited by all authors. 
% \begin{itemize}
%     \item M.R.C. is a co-principal investigator (PI) on the primary funding source of this work (BNL LDRD 22-059), developed the SVA algorithm, performed all \textit{in silico} experiments, wrote most of the relevant software, and wrote the majority of the manuscript.
%     \item H.J.K. and Y.Z. performed all flow reactor experiments.
%     \item Y.Z. is the lead PI on the primary funding source of this work. % this sort of note doesn't belong here. 
%     \item S.Y. provided invaluable expertise and guidance on the algorithmic components of this work.
%     \item D.O., H.J., B.D., B.R., and P.M.M. performed experimental XRD measurements and provided key insight on the application of the SVA methodology to experiments at synchroton light sources.
%     \item P.M.M. served as the primary intellectual driver of the project and wrote a significant portion of the manuscript.
% \end{itemize}

\section*{Declaration of interests}
The authors declare no competing interests, financial or otherwise.


\section*{Data statement}

All software and data used to generate the results in this manuscript can be found open source under a BSD-3-clause license~\href{https://github.com/matthewcarbone/ScientificValueAgent}{github.com/matthewcarbone/ScientificValueAgent}. Tarballs containing all of the data used to generate our results are hosted open access at \href{https://doi.org/10.5281/zenodo.8184819}{doi.org/10.5281/zenodo.8184819}. All figures presented in this manuscript can be regenerated by using these data, and the notebooks stored at the GitHub link above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IF COMPILING FOR THE ARXIV, UNCOMMENT THIS
% \include{section_SI.tex}
% \clearpage
% \pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{bibliography.bib}

\end{document}