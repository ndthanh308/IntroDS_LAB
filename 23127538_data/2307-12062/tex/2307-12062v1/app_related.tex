\section{Additional Related Work}
\label{sec:add_related}

\subsection{Game-Theoretic Reinforcement Learning}

Superhuman performance in two-player games usually involves two components: the first focuses on finding a model-free blueprint strategy, which is the setting we focus on in this paper. The second component improves this blueprint online via model-based subgame solving and search~\citep{burch2014solving, moravcik2016refining, brown2018depth, brown2020combining, brown2017safe, schmid2021player}. This combination of blueprint strategies with subgame solving has led to state-of the art performance in Go~\citep{silver2017mastering}, Poker~\citep{brown2017libratus, brown2018superhuman, moravvcik2017deepstack}, Diplomacy~\citep{gray2020human}, and The Resistance: Avalon~\citep{serrino2019finding}. Methods that only use a blueprint have achieved state-of-the-art performance on Starcraft~\citep{alphastar}, Gran Turismo~\citep{wurman2022outracing}, DouDizhu~\citep{zha2021douzero}, Mahjohng~\citep{li2020suphx}, and Stratego~\citep{mcaleer2020pipeline, perolat2022mastering}. In the rest of this section we focus on other model-free methods for finding blueprints.      

Deep CFR~\citep{deep_cfr, steinberger2019single} is a general method that trains a neural network on a buffer of counterfactual values. However, Deep CFR uses external sampling, which may be impractical for games with a large branching factor, such as Stratego and Barrage Stratego. DREAM~\citep{steinberger2020dream} and ARMAC~\citep{gruslys2020advantage} are model-free regret-based deep learning approaches. ReCFR~\citep{liu2022model} propose a bootstrap method for estimating cumulative regrets with neural networks. ESCHER~\citep{mcaleer2022escher} remove the importance sampling term of Deep CFR and show that doing so allows scaling to large games.  

Neural Fictitious Self-Play (NFSP)~\citep{nfsp} approximates fictitious play by progressively training a best response against an average of all past opponent policies using reinforcement learning. The average policy converges to an approximate Nash equilibrium in two-player zero-sum games.   
%but has slower convergence bounds than CFR.  

There is an emerging literature connecting reinforcement learning to game theory. QPG~\citep{srinivasan2018actor} shows that state-conditioned $Q$-values are related to counterfactual values by a reach weighted term summed over all histories in an infostate and proposes an actor-critic algorithm that empirically converges to a NE when the learning rate is annealed. NeuRD~\citep{hennes2020neural}, and F-FoReL~\citep{perolat2021poincare} approximate replicator dynamics and follow the regularized leader, respectively, with policy gradients. Actor Critic Hedge (ACH)~\citep{ach} is similar to NeuRD but uses an information set based value function. All of these policy-gradient methods do not have theory proving that they converge with high probability in extensive form games when sampling trajectories from the policy. In practice, they often perform worse than NFSP and DREAM on small games but remain promising approaches for scaling to large games \citep{perolat2022mastering}. 