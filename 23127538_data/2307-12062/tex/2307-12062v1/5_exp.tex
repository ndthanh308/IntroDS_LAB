\section{Experiments}
\label{sec:exp}
\vspace{-0.5em}
This paper introduces a novel concept called temporally coupled attacks, which distinguishes itself from standard adversarial attacks by incorporating temporal coupling. Previous research has primarily focused on attackers with different functionalities, specifically targeting either the state space or the action space. In our experimental setup, we investigate three types of attackers: those specialized in perturbing the state space, those focused on perturbing the action space, and those capable of adaptably targeting both spaces. Within each attack domain, we consider both the standard attackers without temporal coupling and the temporally-coupled attackers. By employing this diverse set of attackers, we conduct a comprehensive evaluation of the state-of-the-art robustness of our proposed method, \ours, in comparison to existing robust baselines. This evaluation sheds light on the effectiveness of \ours across a wide range of attack scenarios and highlights its robustness against different types of adversaries.

\textbf{Experiment setup.} 
Our experiments are conducted on five various and challenging MuJoCo environments: Hopper, Walker2d, Halfcheetah, Ant, and Humanoid, all using the v2 version of MuJoCo. We use the Proximal Policy Optimization (PPO) algorithm as the policy optimizer and a Long Short-Term Memory (LSTM) network as the policy network for all of the robust training methods we evaluate. To maintain methodological consistency and minimize potential discrepancies arising from different PPO implementations across methods, we ensure highly similar benchmark results. For attack constraint $\epsilon$, we use the commonly adopted values $\epsilon$ for each environment. For the temporally-coupled constraint $\bar{\epsilon}$, we set the optimal maximum $\bar{\epsilon}$ as $\epsilon/5$ (with minor adjustments in some environments). Other choices of $\bar{\epsilon}$ will be further discussed in the ablation studies.

In terms of evaluation metrics, we report the average test episodic rewards both under no attack and against the strongest adversarial attacks to reflect both the natural performance and robustness of trained agents, by training adversaries targeting the trained agents from scratch. For reproducibility, we train each agent configuration with 10 seeds and report the one with the median robust performance, rather than the best one. More implementation details are in Appendix~\ref{app:exp:imp}.

\textbf{Baselines.} 
We compare our approach \ours with other robust RL baselines in this paper. Robust training frameworks can be categorized into two types. The first type requires training with a specified adversary during training, such as the alternating training framework (ATLA~\citep{zhang2021robust}) and \ours. The second type does not require training with an adversary, such as WocaR-PPO~\citep{liang2022efficient} and AR-PPO (PPO variant of AR-DDPG~\citep{tessler2019action}). The baselines we chose demonstrate state-of-the-art or great robustness in prior works. The first type of approaches require training agents with adversaries targeting specific attack domains and the second type of baselines can be evaluated directly for their robustness without the need for additional adversary training.

\textbf{Case I: Against attacks on state space}

In this experiment, our focus is on evaluating the robustness of our methods against state adversaries that perturb the states received by the agent. Among the ATLA~\citep{zhang2021robust, sun2021strongest} methods, PA-ATLA-PPO is the most robust, which trains with the standard strongest PA-AD attacker. As a modification, we train PA-ATLA-PPO* with a temporally-coupled PA-AD attacker. For a more intuitive and fair comparison, in Table~\ref{tab:table_state} we only present the rewards of the best-performing ATLA agents under the type of attacks they were trained with. Our \ours method utilizes the temporally-coupled PA-AD attacker for training. We report the lowest rewards achieved under both standard and temporally-coupled state attacks among the six existing strongest adversaries to present the robustness of robust models.

In the absence of any attacks, \ours maintains a competitive natural reward, which indicates that the agent's performance does not degrade significantly in the environment where is no adversary after approaching an approximate Nash equilibrium with the adversary. Even without training with regular attackers, our method demonstrates significantly better robustness under the non-temporally-coupled type of attack, particularly in the highest-dimensional and challenging environment, Humanoid, where it outperforms other methods by a large margin. Under our proposed temporally-coupled attacks, the average performance of our approach surpasses the state-of-the-art by up to 45\%, highlighting the strong robustness of the policies learned by \ours against all types of state adversarial attacks.
\input{tables/table_state}

\textbf{Case II: Against attacks on action space}

In addition to state attacks, we assess the robustness of our methods against action adversaries that perturb the actions taken by the agent. Action perturbations have been extensively studied in the context of model uncertainty in control. In this attack domain, we are the first to train an RL-based action adversary using the trajectory outlined in Algorithm~\ref{alg:ours}, aiming to showcase the worst-case performance of our robust agents under action perturbations.

Among our baselines, we include AR-PPO, although it is not robust against strong action adversaries and performs well only under random noise. Another modification we made is AC-ATLA-PPO, where we train the agent alternately with the aforementioned action adversary. Similar to PA-ATLA-PPO*, we also train AC-ATLA-PPO* agents with a temporally-coupled action adversary, which is also utilized to train our \ours agents. Since RL-based action adversaries lead to a more significant drop in rewards compared to action noise, we report the best action attack rewards achieved by the robust agents under the strongest trained action attacks for robustness evaluation.

In general, while action perturbation may not cause as strong of a "damage" as state perturbation, our \ours method still achieves superior robustness. In terms of natural reward, \ours performs comparably with other baselines. While the advantage of \ours may not be apparent or significant under standard action attacks in less challenging environments, it surpasses other methods by more than 10\% on Ant and Humanoid. Under temporally-coupled action attacks, \ours consistently outperforms the most robust baseline by an average of over 20\%, particularly exhibiting exceptional robustness on Humanoid. These results demonstrate the effective defense of \ours against different types of adversarial attacks in the action space.
\input{tables/table_action}

\textbf{Case III: Against attacks on either state or action spaces}

In prior works, adversarial attacks typically focus on perturbing either the agent's observations or introducing noise to the action space. However, in real-world scenarios, agents may encounter both types of attacks. To address this challenge, we propose an adversary called the State or Action Adversary (SA-AD), which allows the adversary to choose between attacking the agent's state or action at each time step, integrating this choice into the adversary's action space. The SA-AD attacker, inspired by the PA-AD attacker~\cite{sun2021strongest}, only needs to learn the best policy perturbation directions that can be transformed into state or action perturbations according to the adversary's choices while maintaining manageable training complexity. For further details on the SA-AD attacker, please refer to Appendix~\ref{app:exp:alg}. Similar to the previous experiments, We train SA-ATLA-PPO with SA regular attacker, while SA-ATLA-PPO* and \ours are trained with temporally-coupled SA attackers.

Our experimental results demonstrate that \ours obtains similar natural rewards compared to the ATLA baselines, which is consistent with the findings from previous experiments. To summarize the results under SA attacks, our findings indicate that the combination of two different forms of attacks can effectively target robust agents in most scenarios, providing strong evidence of their robustness. In the case of regular SA attackers, \ours outperforms other methods in all five environments, with a margin of over 20\% in the Humanoid environment. Moreover, when defending against temporally-coupled attacks, \ours significantly enhances robustness by more than 30\% in multiple environments, with a minimum improvement of 10\%. These results clearly demonstrate the robustness of \ours against attackers that can target different domains.
\input{tables/table_adaptable}

\textbf{Summary.} We calculated the average normalized rewards for each evaluation metric and each robust agent in all the environments as in Figure~\ref{fig:exp}. This visualization vividly showcases that \ours demonstrates notably superior robustness under both standard and temporally-coupled attacks, in comparison to other approaches. Overall, these findings emphasize our empirical potential and contributions of \ours and provide intuitive insights into improving the robustness of agents through a novel and convincing evaluation framework for robust RL.

\input{figures/exp}

\textbf{Ablation studies for temporally-coupled constraint $\bar{\epsilon}$.} 
As defined in our framework, the temporally-coupled constraint $\bar{\epsilon}$ limits the perturbations within a range that varies between timesteps. When $\bar{\epsilon}$ is set too large, the constraint becomes ineffective, resembling a standard attacker. 
\begin{wrapfigure}{r}{0.35\textwidth}
% \vspace{-1em}
    \centering
    \input{figures/eps_}
    \vspace{-0.5em}
    \caption{Ablated studies for $\bar{\epsilon}$.
    % $\pi_\theta$ is the acting policy being trained by $\loss_{\pi_\theta}$ defined in Equation~\eqref{loss:policy}, including the original loss of the base DRL algorithm $\lossrl$, a regularization term $\lossreg$, as well as $\lossworst$, a term for improving the \worstqname based on $\worstcritic$. Here $\worstcritic$ which estimates the \worstqname of $\pi_\theta$ is updated by $\loss_{\worstcritic}=\lossest$ depending on $\pi_\theta$.  
    }
    \label{fig:eps_}
% \vspace{-1em}
\end{wrapfigure}
Conversely, setting $\bar{\epsilon}$ close to zero overly restricts perturbations, leading to a decline in attack performance. An appropriate value for $\bar{\epsilon}$ is critical for effective temporally-coupled attacks. Figure~\ref{fig:eps_} illustrates the performance of robust models against temporally-coupled state attackers trained with different maximum $\bar{\epsilon}$. For WocaR-PPO, the temporally-coupled attacker achieves optimal attack performance when the values of $\bar{\epsilon}$ are set to 0.02. As the $\bar{\epsilon}$ values increase and the temporally-coupled constraint weakens, the agent's performance improves, indicating a decrease in the adversary's attack effectiveness. In the case of \ours agents, they consistently maintain robust performance as the $\bar{\epsilon}$ values become larger. This observation highlights the impact of temporal coupling on the vulnerability of robust baselines to such attacks. In contrast, \ours agents consistently demonstrate robustness against these attacks.






