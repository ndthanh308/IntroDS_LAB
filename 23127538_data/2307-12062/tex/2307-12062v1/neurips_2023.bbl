\begin{thebibliography}{10}

\bibitem{bechtle2020curious}
Sarah Bechtle, Yixin Lin, Akshara Rai, Ludovic Righetti, and Franziska Meier.
\newblock Curious ilqr: Resolving uncertainty in model-based rl.
\newblock In Leslie~Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors,
  {\em Proceedings of the Conference on Robot Learning}, volume 100 of {\em
  Proceedings of Machine Learning Research}, pages 162--171. PMLR, 30 Oct--01
  Nov 2020.

\bibitem{behzadan2017whatever}
Vahid Behzadan and Arslan Munir.
\newblock Whatever does not kill deep reinforcement learning, makes it
  stronger.
\newblock {\em CoRR}, abs/1712.09344, 2017.

\bibitem{brown2020combining}
Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong.
\newblock Combining deep reinforcement learning and search for
  imperfect-information games.
\newblock {\em Advances in Neural Information Processing Systems},
  33:17057--17069, 2020.

\bibitem{brown2019deep}
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm.
\newblock Deep counterfactual regret minimization.
\newblock In {\em International conference on machine learning}, pages
  793--802. PMLR, 2019.

\bibitem{deep_cfr}
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm.
\newblock Deep counterfactual regret minimization.
\newblock In {\em International Conference on Machine Learning}, pages
  793--802, 2019.

\bibitem{brown2017libratus}
Noam Brown and Tuomas Sandholm.
\newblock Libratus: The superhuman {AI} for no-limit poker.
\newblock In {\em IJCAI}, pages 5226--5228, 2017.

\bibitem{brown2017safe}
Noam Brown and Tuomas Sandholm.
\newblock Safe and nested subgame solving for imperfect-information games.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{brown2018superhuman}
Noam Brown and Tuomas Sandholm.
\newblock Superhuman {AI} for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock {\em Science}, 359(6374):418--424, 2018.

\bibitem{brown2018depth}
Noam Brown, Tuomas Sandholm, and Brandon Amos.
\newblock Depth-limited solving for imperfect-information games.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{burch2014solving}
Neil Burch, Michael Johanson, and Michael Bowling.
\newblock Solving imperfect information games using decomposition.
\newblock In {\em Twenty-eighth AAAI conference on artificial intelligence},
  2014.

\bibitem{feng2021discovering}
Xidong Feng, Oliver Slumbers, Yaodong Yang, Ziyu Wan, Bo~Liu, Stephen McAleer,
  Ying Wen, and Jun Wang.
\newblock Discovering multi-agent auto-curricula in two-player zero-sum games.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{fischer2019online}
Marc Fischer, Matthew Mirman, Steven Stalder, and Martin~T. Vechev.
\newblock Online robustness training for deep reinforcement learning.
\newblock {\em CoRR}, abs/1911.00887, 2019.

\bibitem{franzmeyer2022illusionary}
Tim Franzmeyer, Stephen McAleer, Jo{\~a}o~F Henriques, Jakob~N Foerster,
  Philip~HS Torr, Adel Bibi, and Christian~Schroeder de~Witt.
\newblock Illusory attacks: Detectability matters in adversarial attacks on
  sequential decision-makers.
\newblock {\em arXiv preprint arXiv:2207.10170v2}, 2022.

\bibitem{ach}
Haobo Fu, Weiming Liu, Shuang Wu, Yijia Wang, Tao Yang, Kai Li, Junliang Xing,
  Bin Li, Bo~Ma, Qiang Fu, and Yang Wei.
\newblock Actor-critic policy optimization in a large-scale
  imperfect-information game.
\newblock In {\em Proceedings of the Tenth International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem{garcia2015comprehensive}
Javier Garc{\i}a and Fernando Fern{\'a}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 16(1):1437--1480, 2015.

\bibitem{gaskett2003reinforcement}
Chris Gaskett.
\newblock Reinforcement learning under circumstances beyond its control.
\newblock In {\em International Conference on Computational Intelligence for
  Modelling Control and Automation}, 2003.

\bibitem{gleave2019adversarial}
Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart
  Russell.
\newblock Adversarial policies: Attacking deep reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{goyal2023robust}
Vineet Goyal and Julien Grand-Clement.
\newblock Robust markov decision processes: Beyond rectangularity.
\newblock {\em Mathematics of Operations Research}, 48(1):203--226, 2023.

\bibitem{gray2020human}
Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown.
\newblock Human-level performance in no-press diplomacy via equilibrium search.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{gruslys2020advantage}
Audr{\=u}nas Gruslys, Marc Lanctot, R{\'e}mi Munos, Finbarr Timbers, Martin
  Schmid, Julien Perolat, Dustin Morrill, Vinicius Zambaldi, Jean-Baptiste
  Lespiau, John Schultz, et~al.
\newblock The advantage regret-matching actor-critic.
\newblock {\em arXiv preprint arXiv:2008.12234}, 2020.

\bibitem{Heger1994ConsiderationOR}
Matthias Heger.
\newblock Consideration of risk in reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 1994.

\bibitem{nfsp}
Johannes Heinrich and David Silver.
\newblock Deep reinforcement learning from self-play in imperfect-information
  games.
\newblock {\em arXiv preprint arXiv:1603.01121}, 2016.

\bibitem{hennes2020neural}
Daniel Hennes, Dustin Morrill, Shayegan Omidshafiei, R{\'e}mi Munos, Julien
  Perolat, Marc Lanctot, Audrunas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas,
  Edgar Du{\'e}{\~n}ez-Guzm{\'a}n, et~al.
\newblock Neural replicator dynamics: Multiagent learning via hedging policy
  gradients.
\newblock In {\em Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 492--501, 2020.

\bibitem{huang2017adversarial}
Sandy~H. Huang, Nicolas Papernot, Ian~J. Goodfellow, Yan Duan, and Pieter
  Abbeel.
\newblock Adversarial attacks on neural network policies.
\newblock In {\em International Conference on Learning
  Representations(Workshop)}, 2017.

\bibitem{korkmaz2021investigating}
Ezgi Korkmaz.
\newblock Investigating vulnerabilities of deep neural policies.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1661--1670.
  PMLR, 2021.

\bibitem{kos2017delving}
Jernej Kos and Dawn Song.
\newblock Delving into adversarial attacks on deep policies.
\newblock In {\em International Conference on Learning
  Representations(Workshop)}, 2017.

\bibitem{kumar2021policy}
Aounon Kumar, Alexander Levine, and Soheil Feizi.
\newblock Policy smoothing for provably robust reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{psro}
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl
  Tuyls, Julien P{\'e}rolat, David Silver, and Thore Graepel.
\newblock A unified game-theoretic approach to multiagent reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2017.

\bibitem{lanier2022feasible}
JB~Lanier, Stephen McAleer, Pierre Baldi, and Roy Fox.
\newblock Feasible adversarial robust reinforcement learning for underspecified
  environments.
\newblock {\em arXiv preprint arXiv:2207.09597}, 2022.

\bibitem{lee2020query}
Xian~Yeow Lee, Yasaman Esfandiari, Kai~Liang Tan, and Soumik Sarkar.
\newblock Query-based targeted action-space adversarial policies on deep
  reinforcement learning agents.
\newblock In {\em Proceedings of the ACM/IEEE 12th International Conference on
  Cyber-Physical Systems}, ICCPS '21, page 87â€“97, New York, NY, USA, 2021.
  Association for Computing Machinery.

\bibitem{li2020suphx}
Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang,
  Li~Zhao, Tao Qin, Tie-Yan Liu, and Hsiao-Wuen Hon.
\newblock Suphx: Mastering mahjong with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:2003.13590}, 2020.

\bibitem{liang2022efficient}
Yongyuan Liang, Yanchao Sun, Ruijie Zheng, and Furong Huang.
\newblock Efficient adversarial training without attacking: Worst-case-aware
  robust reinforcement learning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{lim2013reinforcement}
Shiau~Hong Lim, Huan Xu, and Shie Mannor.
\newblock Reinforcement learning in robust markov decision processes.
\newblock {\em Advances in Neural Information Processing Systems}, 26:701--709,
  2013.

\bibitem{liu2022model}
Weiming Liu, Bin Li, and Julian Togelius.
\newblock Model-free neural counterfactual regret minimization with bootstrap
  learning.
\newblock {\em IEEE Transactions on Games}, 2022.

\bibitem{lutjens2020certified}
Bj{\"o}rn L{\"u}tjens, Michael Everett, and Jonathan~P How.
\newblock Certified adversarial robustness for deep reinforcement learning.
\newblock In {\em Conference on Robot Learning}, pages 1328--1337. PMLR, 2020.

\bibitem{mandlekar2017adversarially}
Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li~Fei-Fei, and Silvio Savarese.
\newblock Adversarially robust policy learning: Active construction of
  physically-plausible perturbations.
\newblock In {\em 2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 3932--3939. IEEE, 2017.

\bibitem{Mankowitz2020Robust}
Daniel~J. Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost~Tobias
  Springenberg, Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy Mann, and Martin
  Riedmiller.
\newblock Robust reinforcement learning for continuous control with model
  misspecification.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{mannor2012lightning}
Shie Mannor, Ofir Mebel, and Huan Xu.
\newblock Lightning does not strike twice: Robust mdps with coupled
  uncertainty.
\newblock {\em arXiv preprint arXiv:1206.4643}, 2012.

\bibitem{mannor2016robust}
Shie Mannor, Ofir Mebel, and Huan Xu.
\newblock Robust mdps with k-rectangular uncertainty.
\newblock {\em Mathematics of Operations Research}, 41(4):1484--1509, 2016.

\bibitem{mcaleer2022escher}
Stephen McAleer, Gabriele Farina, Marc Lanctot, and Tuomas Sandholm.
\newblock Escher: Eschewing importance sampling in games by computing a history
  value function to estimate regret.
\newblock {\em International Conference on Learning Representations}, 2023.

\bibitem{mcaleer2022self}
Stephen McAleer, JB~Lanier, Kevin Wang, Pierre Baldi, Roy Fox, and Tuomas
  Sandholm.
\newblock Self-play psro: Toward optimal populations in two-player zero-sum
  games.
\newblock {\em arXiv preprint arXiv:2207.06541}, 2022.

\bibitem{mcaleer2021xdo}
Stephen McAleer, John Lanier, Pierre Baldi, and Roy Fox.
\newblock {XDO}: A double oracle algorithm for extensive-form games.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{mcaleer2020pipeline}
Stephen McAleer, John Lanier, Roy Fox, and Pierre Baldi.
\newblock Pipeline {PSRO}: A scalable approach for finding approximate {Nash}
  equilibria in large games.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{mcaleer2022anytime}
Stephen McAleer, Kevin Wang, Marc Lanctot, John Lanier, Pierre Baldi, and Roy
  Fox.
\newblock Anytime optimal psro for two-player zero-sum games.
\newblock {\em arXiv preprint arXiv:2201.07700}, 2022.

\bibitem{double_oracle}
H~Brendan McMahan, Geoffrey~J Gordon, and Avrim Blum.
\newblock Planning in the presence of cost functions controlled by an
  adversary.
\newblock {\em Proceedings of the 20th International Conference on Machine
  Learning (ICML)}, 2003.

\bibitem{moravvcik2017deepstack}
Matej Morav{\v{c}}{\'\i}k, Martin Schmid, Neil Burch, Viliam Lis{\`y}, Dustin
  Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael
  Bowling.
\newblock Deepstack: Expert-level artificial intelligence in heads-up no-limit
  poker.
\newblock {\em Science}, 356(6337):508--513, 2017.

\bibitem{moravcik2016refining}
Matej Moravcik, Martin Schmid, Karel Ha, Milan Hladik, and Stephen Gaukrodger.
\newblock Refining subgames in large imperfect information games.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem{muller2019generalized}
Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Perolat,
  Siqi Liu, Daniel Hennes, Luke Marris, Marc Lanctot, Edward Hughes, et~al.
\newblock A generalized training approach for multiagent learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{oikarinen2020robust}
Tuomas Oikarinen, Wang Zhang, Alexandre Megretski, Luca Daniel, and Tsui-Wei
  Weng.
\newblock Robust deep reinforcement learning through adversarial loss.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{xiao2019characterizing}
Xinlei Pan, Chaowei Xiao, Warren He, Shuang Yang, Jian Peng, Mingjie Sun,
  Mingyan Liu, Bo~Li, and Dawn Song.
\newblock Characterizing attacks on deep reinforcement learning.
\newblock In {\em {AAMAS}}, pages 1010--1018. International Foundation for
  Autonomous Agents and Multiagent Systems {(IFAAMAS)}, 2022.

\bibitem{pattanaik2017robust}
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish
  Chowdhary.
\newblock Robust deep reinforcement learning with adversarial attacks.
\newblock In {\em {AAMAS}}, pages 2040--2042. International Foundation for
  Autonomous Agents and Multiagent Systems Richland, SC, {USA} / {ACM}, 2018.

\bibitem{perolat2022mastering}
Julien Perolat, Bart de~Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub,
  Vincent de~Boer, Paul Muller, Jerome~T Connor, Neil Burch, Thomas Anthony,
  et~al.
\newblock Mastering the game of stratego with model-free multiagent
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2206.15378}, 2022.

\bibitem{perolat2021poincare}
Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark
  Rowland, Pedro Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart
  De~Vylder, et~al.
\newblock From {P}oincar{\'e} recurrence to convergence in imperfect
  information games: Finding equilibrium via regularization.
\newblock In {\em International Conference on Machine Learning}, pages
  8525--8535. PMLR, 2021.

\bibitem{pinto2017robust}
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta.
\newblock Robust adversarial reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2817--2826. PMLR, 2017.

\bibitem{schmid2021player}
Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin
  Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, et~al.
\newblock Player of games.
\newblock {\em arXiv preprint arXiv:2112.03178}, 2021.

\bibitem{serrino2019finding}
Jack Serrino, Max Kleiman-Weiner, David~C Parkes, and Josh Tenenbaum.
\newblock Finding friend and foe in multi-agent games.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{shen2020deep}
Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao.
\newblock Deep reinforcement learning with robust and smooth policy.
\newblock In {\em International Conference on Machine Learning}, pages
  8707--8718. PMLR, 2020.

\bibitem{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em nature}, 550(7676):354--359, 2017.

\bibitem{sokota2022unified}
Samuel Sokota, Ryan D'Orazio, J~Zico Kolter, Nicolas Loizou, Marc Lanctot,
  Ioannis Mitliagkas, Noam Brown, and Christian Kroer.
\newblock A unified approach to reinforcement learning, quantal response
  equilibria, and two-player zero-sum games.
\newblock {\em arXiv preprint arXiv:2206.05825}, 2022.

\bibitem{srinivasan2018actor}
Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P{\'e}rolat, Karl
  Tuyls, R{\'e}mi Munos, and Michael Bowling.
\newblock Actor-critic policy optimization in partially observable multiagent
  environments.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{steinberger2019single}
Eric Steinberger.
\newblock Single deep counterfactual regret minimization.
\newblock {\em arXiv preprint arXiv:1901.07621}, 2019.

\bibitem{steinberger2020dream}
Eric Steinberger, Adam Lerer, and Noam Brown.
\newblock {DREAM}: Deep regret minimization with advantage baselines and
  model-free learning.
\newblock {\em arXiv preprint arXiv:2006.10410}, 2020.

\bibitem{sun2023certifiably}
Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, Yongyuan Liang, Soheil Feizi,
  Sumitra Ganesh, and Furong Huang.
\newblock Certifiably robust policy learning against adversarial multi-agent
  communication.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{sun2021strongest}
Yanchao Sun, Ruijie Zheng, Yongyuan Liang, and Furong Huang.
\newblock Who is the strongest enemy? towards optimal and efficient evasion
  attacks in deep {RL}.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{tan2020robustifying}
Kai~Liang Tan, Yasaman Esfandiari, Xian~Yeow Lee, Soumik Sarkar, et~al.
\newblock Robustifying reinforcement learning agents via action space
  adversarial training.
\newblock In {\em 2020 American control conference (ACC)}, pages 3959--3964.
  IEEE, 2020.

\bibitem{tessler2019action}
Chen Tessler, Yonathan Efroni, and Shie Mannor.
\newblock Action robust reinforcement learning and applications in continuous
  control.
\newblock In {\em International Conference on Machine Learning}, pages
  6215--6224. PMLR, 2019.

\bibitem{thomas2021safe}
Garrett Thomas, Yuping Luo, and Tengyu Ma.
\newblock Safe reinforcement learning by imagining the near future.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{vinitsky2020robust}
Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and
  Alexandre Bayen.
\newblock Robust reinforcement learning using adversarial populations.
\newblock {\em arXiv preprint arXiv:2008.01825}, 2020.

\bibitem{alphastar}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in {StarCraft II} using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354, 2019.

\bibitem{wu2021crop}
Fan Wu, Linyi Li, Zijian Huang, Yevgeniy Vorobeychik, Ding Zhao, and Bo~Li.
\newblock {CROP:} certifying robust policies for reinforcement learning through
  functional smoothing.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{wurman2022outracing}
Peter~R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik
  Subramanian, Thomas~J Walsh, Roberto Capobianco, Alisa Devlic, Franziska
  Eckert, Florian Fuchs, et~al.
\newblock Outracing champion gran turismo drivers with deep reinforcement
  learning.
\newblock {\em Nature}, 602(7896):223--228, 2022.

\bibitem{zha2021douzero}
Daochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, Xiangru Lian, Xia Hu, and
  Ji~Liu.
\newblock Douzero: Mastering doudizhu with self-play deep reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  12333--12344. PMLR, 2021.

\bibitem{zhang2021robust}
Huan Zhang, Hongge Chen, Duane~S Boning, and Cho-Jui Hsieh.
\newblock Robust reinforcement learning on state observations with learned
  optimal adversary.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhang2020robust}
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo~Li, Mingyan Liu, Duane Boning, and
  Cho-Jui Hsieh.
\newblock Robust deep reinforcement learning against adversarial perturbations
  on state observations.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 21024--21037. Curran Associates, Inc., 2020.

\end{thebibliography}
