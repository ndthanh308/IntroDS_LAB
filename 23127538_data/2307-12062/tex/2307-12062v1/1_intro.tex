\section{Introduction}
\label{sec:intro}
In recent years, reinforcement learning (RL) has demonstrated remarkable success in tackling complex decision-making problems in various domains. However, the vulnerability of deep RL algorithms to test-time changes in the environment or adversarial attacks has raised significant concerns for real-world applications. 
% In particular, attackers may perturb the state observation received by the agent or the action executed by the agent to compromise its performance. 
Developing robust RL algorithms that can defend against these adversarial attacks is crucial for the safety, reliability and effectiveness of RL-based systems.% in practical scenarios.

In most existing research on robust RL~\citep{huang2017adversarial,liang2022efficient,sun2021strongest,tessler2019action,zhang2020robust}, the adversary is able to perturb the observation or action every timestep under a static constraint. Specifically, the adversary's perturbations are constrained within a predefined space, such as an $L_p$ norm, which remains unchanged from one timestep to the next. This \textit{standard} assumption in the robust RL literature can be referred to as a \textit{non-temporally-coupled} assumption. This static constraint, however, can result in much different way of perturbation at every consecutive time steps. For example, the attacker may be able to blow the wind hard southeast at time $t$ but northwest at time $t+1$ 
% to achieve optimal perturbations 
within this $L_p$ norm under this static constraint. In contrast, in the realm of real-world settings, the adversary may not have complete flexibility to perturb the environment differently across timesteps. For example, it is unlikely for the wind to move in one direction in one second, then in the opposite direction in the next second. In these \textit{temporally-coupled} settings, employing 
a robust policy learning technique designed for the static attack strategy
% a standard attack strategy for learning a robust policy 
would result in an excessively conservative policy. However, by formulating the robust RL problem as a partially-observable two-player game, we introduce a game-theoretic algorithm which lets the agent automatically adapt to the adversary under any attack constraints, either standard or temporally-coupled.
% introduce a strategic interaction between the agent and the temporally-coupled adversary, capturing the dynamics of adversarial attacks in a more realistic and effective manner. 
% \fh{ here it is not very convincing why. We should motivate on ``How does this game consider the temporal-couple?}

In this paper, we propose a novel approach: \oursfull (\ours) that leverages Policy Space Response Oracles (PSRO)~\citep{psro} for robust training in the \textit{temporally-coupled} setting. Our method aims to enhance the agent's resilience against the most powerful adversary in both state and action spaces. We model the interaction between the agent and the temporally-coupled adversary as a two-player zero-sum game and employ PSRO to ensure the agent's best response against the learned adversary and find an approximate equilibrium. This game-theoretic framework empowers our approach to effectively maximize the agent's worst-case rewards by adapting to the strongest adversarial strategies.

Our contributions are three-fold: First, we propose a novel class of temporally-coupled adversarial attacks to identify the realistic pitfalls of prior threat models and propose a challenge for existing robust RL methods which overlook the strength of temporally-coupled adversaries. Secondly, we introduce a game-theoretic response approach, referred to as \ours, for robust training with a temporally-coupled adversary. We elaborate the theoretical advantages of our approach compared to existing robust RL methods. Lastly, we provide extensive empirical results that demonstrate the effectiveness of our approach in defending against both temporally-coupled attacks and standard (non-temporally coupled) attacks. Our evaluations span across various continuous control tasks, considering perturbations in both state and action spaces. Figure~\ref{fig:beh} shows interpretable phenomenons of \ours agent and robust baselines under different types of attacks in Humanoid.
% Figure environment removed


%The remainder of this paper is organized as follows: in Section~\ref{sec:relate} we introduce related works in robust RL and game-theoretic RL. Then, Section~\ref{sec:pre} presents the necessary background and notations. In Section~\ref{sec:method} we outline our methodology, which includes incorporating temporal coupling in adversarial attacks and presenting our game-theoretic response approach for robust RL training. Section~\ref{sec:exp} is dedicated to discussing the experimental insights and key findings. Finally, we conclude the paper and provide directions for future research.\ys{I feel that this paragraph is not necessary given the short paper length.}