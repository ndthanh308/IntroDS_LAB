\vspace{-0.5em}
\section{Methodology}
\vspace{-0.5em}
\label{sec:method}
In this section, we first formally define temporally-coupled attacks. Then, we introduce our algorithm, a game-theoretic response approach for adversarial defense against the proposed attacks.
\vspace{-0.5em}
\subsection{Temporally-coupled Attack}

In adversarial RL, it is common and reasonable to impose restrictions on the power of an adversary. To achieve this, we introduce the concept of \textit{standard} admissible perturbations, as defined in Definition~\ref{def:con1}, which limits the adversary's ability to perturb a state $s_t$ or an action $a_t$ to a predefined set.
\begin{definition}[$\epsilon$-Admissible Adversary Perturbations]
An adversarial perturbation $p_t$ is considered admissible in the context of a state adversary if, for a given state $s_t$ at timestep $t$, the perturbed state $\tilde{s}_t$ defined as $\tilde{s}_t = s_t + p_t$ satisfies $\left\|s_{t}-\tilde{s}_{t}\right\|\leq\epsilon$, where $\epsilon$ is the state budget constraint. Similarly, if $p_t$ is generated by an action adversary, the perturbed action $\tilde{a}_t$ defined as $\tilde{a}_t = a_t + p_t$ should be under the action constraint of $\left\|a_t-\tilde{a}_{t}\right\|\leq\epsilon$. 
\label{def:con1}
\end{definition}

\begin{wrapfigure}{r}{0.45\textwidth}
    \vspace{-2em}
    \centering
    % Figure removed
    \vspace{-1.5em}
    \caption{\small{\textit{Standard} perturbations and \textit{Temporally-coupled} perturbations} in a 2d example.}
    % $\pi_\theta$ is the acting policy being trained by $\loss_{\pi_\theta}$ defined in Equation~\eqref{loss:policy}, including the original loss of the base DRL algorithm $\lossrl$, a regularization term $\lossreg$, as well as $\lossworst$, a term for improving the \worstqname based on $\worstcritic$. Here $\worstcritic$ which estimates the \worstqname of $\pi_\theta$ is updated by $\loss_{\worstcritic}=\lossest$ depending on $\pi_\theta$.  
    \label{fig:dia}
\vspace{-1em}
\end{wrapfigure}
While the budget constraint $\epsilon$ is commonly applied in prior adversarial attacks, it may not be applicable in many real-world scenarios where the attacker needs to consider the past perturbations when determining the current perturbations. Specifically, in the temporal dimension, perturbations exhibit a certain degree of correlation. To capture this characteristic, we introduce the concept of temporally-coupled attackers. We propose a temporally-coupled constraint as defined in Definition~\ref{def:con2}, which sets specific limitations on the perturbation at the current timestep based on the previous timestep's perturbation. 

\begin{definition}[$\bar{\epsilon}$-Temporally-coupled Perturbations]
\label{def:con2}
A temporally-coupled state perturbation $p_t$ is deemed acceptable if it satisfies the temporally-coupled constraint $\bar{\epsilon}$: $\left\|s_{t}-\tilde{s}_t-(s_{t+1}-\tilde{s}_{t+1}) \right\|_{a}\leq\bar{\epsilon}$ 
where $\tilde{s}_t$ and $\tilde{s}_{t+1}$ are the perturbed states obtained by adding $p_t$ and $p_{t+1}$ to $s_t$ and $s_{t+1}$, respectively. For action adversaries, the temporally-coupled constraint $\bar{\epsilon}$ is similarly denoted as $\left\|a_{t}-\tilde{a}_t-(a_{t+1}-\tilde{a}_{t+1}) \right\|\leq\bar{\epsilon}$, where $\tilde{a}_t$ and $\tilde{a}_{t+1}$ are the perturbed actions. 
\end{definition}

When an adversary is subjected to both of these constraints, it is referred to as a temporally-coupled adversary in this paper. For a temporally-coupled adversary, each timestep's perturbation is restricted within a certain range $\epsilon$, similar to other regular adversarial attacks. However, it is further confined within a smaller range $\bar{\epsilon}$ based on the previous timestep's perturbation. This design offers two significant benefits.

Firstly, it enables the adversary to consider the temporal coupling between perturbations over time. By constraining the perturbations to a smaller range and discouraging drastic changes in direction, the adversary can launch continuous and stronger attacks while preserving a certain degree of stability. Intuitively, if the adversary consistently attacks in one direction, it can be more challenging for the victim to preserve balance and defend effectively compared to when the perturbations alternate between the left and right directions.

Then, the temporally-coupled constraint also enables the adversary to efficiently discover the optimal attack strategy by narrowing down the range of choices for each timestep's perturbation. Reducing the search space does not necessarily weaken the adversary; in fact, it can potentially make the adversary stronger if the optimal attack lies within the temporally-determined search space, which is supported by our empirical results. By constraining the adversary to a more focused exploration of attack strategies, the temporally-coupled constraint facilitates the discovery and exploitation of more effective and targeted adversarial tactics that exhibit less variation at consecutive timesteps. This characteristic enhances the adversary's ability to launch consistent and potent attacks.

Practically, it is crucial to carefully determine $\bar{\epsilon}$ to guarantee that this additional temporally-coupled constraint does not impede the performance of attacks but rather amplifies their effectiveness. The effectiveness of different choices for $\bar{\epsilon}$ was empirically evaluated in our empirical studies, highlighting the benefits it brings to adversarial learning. By leveraging such a temporally-coupled adversary, we propose a novel approach for robust training that enhances the agent's robustness. The detailed advantages of this approach will be elaborated in the following section.
\vspace{-0.5em}
\subsection{\ours: \oursfull}
Existing works primarily focus on the non-temporally-coupled assumption and thus may not be suitable in many real-world scenarios, but by treating the game-theoretic framework with a temporally-coupled adversary, our robust RL approach offers a more generalized solution that covers both temporally-coupled and standard non-temporally-coupled settings.

In our \oursfull(\ours) framework as a modification of PSRO~\citep{psro}, an agent and a temporally-coupled adversary are trained as part of a two-player game. They play against each other and update their policies in response to each other's policies. The adversary is modeled as a separate agent who attempts to maximize the impact of attacks on the original agent's performance and whose action space is constrained by both $\epsilon$ and $\bar{\epsilon}$. 
% \fh{Consequently, \ours naturally considers the new temporally-coupled constraint when calculating the best response, while existing robust RL approaches such as \cite{liang2022efficient} cannot adapt to these additional temporally-coupled constraints and instead rely on assumption that the adversary is not constrained \fhc{not constrained can cause confusion (they may think it doesn't have epsilon bound), maybe just delete this part.}.}
Note that existing robust RL approaches such as \cite{liang2022efficient} heavily rely on the $\epsilon$-budget assumption, while the temporally-coupled constraints or other types of attack constraints are not considered or addressed.
In contrast, \ours naturally considers both the traditional $\epsilon$-budget constraint and the new temporally-coupled constraint when calculating the best response.
% \fhc{Talk about how the time-couple constraint is taken care of by GRAD?}
% \fh{\ours constructs the policy set of each agent in the zero-sum game through computing the best response to the opponent's current NE policy. Therefore, it naturally considers the new temporally-coupled constraint when calculating the best response.}
Meanwhile, the original agent's objective function is based on the reward obtained from the environment, taking into account the perturbations imposed by the adversary. The process continues until an approximate equilibrium is reached, at which point the original agent is considered to be robust to the attacks learned by the adversary. We show our full algorithm in Algorithm~\ref{alg:ours}.

For different types of attackers, the agent generates different trajectories while training against a fixed attacker. If the attacker only targets the state, then the agent's training data will consist of the altered state $\hat{s}$ after adding the perturbations from the fixed attacker. If the attacker targets the agent's action, the agent's policy output $a$ will be altered as $\hat{a}$ by the attacker, even if the agent receives the correct state $s$ during training. However, this action alteration may not be detectable in the trajectories collected by the agent. As for the adversary's training, after defining the adversary's attack method and policy model, the adversary applies attacks to the fixed agent and collects the original state, along with the negative of the agent's reward $-r$, to train the adversary.

While both \ours and ATLA~\citep{zhang2021robust} require training an adversary alongside the agent using RL, there is a key difference in their training approaches. In \ours, both the agent and the adversary have two policy sets. During each training epoch, the agent aims to find an approximate best response to the fixed adversary, and vice versa for the adversary. This iterative process promotes the emergence of stable and robust policies. After each epoch, the trained policies are added to the respective policy sets. \ours has the capability to continuously explore and learn new policies that are not present in the current policy set, thereby enabling ongoing improvement for both the agent and the adversary, which allows for a more thorough exploration of the policy space. In contrast, ATLA employs a limited number of iterations to train each agent in each round, which is not sufficient to allow the agent and adversary to find each other's best response within the policy space.
It is also worth noting that the original ATLA utilizes standard attack methods to train the adversary. However, several experimental observations indicate that agents trained with non-temporally-coupled adversaries tend to exhibit a conservative and overfitted behavior towards specific types of adversaries. 
In the next section, we empirically demonstrate that our approach exhibits superior and comprehensive robustness, which is capable of adapting to various attack scenarios and effectively countering different types of adversaries. 
\input{algorithms/GRAD}

