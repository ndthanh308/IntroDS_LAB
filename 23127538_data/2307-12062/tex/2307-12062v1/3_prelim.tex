\vspace{-0.5em}
\section{Preliminaries}
\vspace{-0.5em}
\label{sec:pre}

% \begin{algorithm}
% \SetAlgoLined

% \DontPrintSemicolon
% \KwResult{Nash Equilibrium}
%  Input: initial population $\Pi^0$\;
%  \While{Not terminated}{
%   Solve game restricted to policies in $\Pi^t$ to get meta-distribution $\pi^r$\;
%   \For{$i \in \{1,2\}$}{
%       Find a best response $\mathbb{BR}_i(\pi^r_{-i})$\;
%       $\Pi^{t+1}_i = \Pi^t_i \cup \mathbb{BR}_i(\pi^r_{-i})$\;
%   } 
%   \If{No novel best response exists for both players}{
% %   \If{$gv(G(\Pi_0, all possible p1 strategies)) = gv(G(all possible p0 strategies, \Pi_1))$}
%     Return $\pi^r$\;
%   }
%  }
%  \caption{Double Oracle}
% \end{algorithm}

\textbf{Notations and Background.} 
A Markov decision process (MDP) can be defined as a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ and $\mathcal{A}$ represent the state space and the action space, $\mathcal{R}$ is the reward function: $\mathcal{R}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})$ represents the set of probability distributions over the state space $\mathcal{S}$ and $\gamma\in(0,1)$ is the discount factor. The agent selects actions based on its policy, $\pi:\mathcal{S} \to \Delta(\mathcal{A})$, which is represented by a function approximator (e.g. a neural network) that is updated during training and fixed during testing. The value function is denoted by $V^{\pi}(s):=\mathbb{E}_{P,\pi}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})\mid s_{0}=s]$, which measures the expected cumulative discounted reward that an agent can obtain from state $s\in\mathcal{S}$ by following policy $\pi$.

\textbf{State Adversaries.} State adversary is a type of test-time attacker that perturbs the agent's state observation returned by the environment at each time step and aims to reduce the expected episode reward gained by the agent. While the input to the agent's policy is perturbed, the underlying state in the environment remains unchanged. State adversaries, such as those presented in ~\citep{zhang2020robust, zhang2021robust, sun2021strongest}, typically consider perturbations on a continuous state space under a certain attack budget $\epsilon$. The attacker perturbs a state $s$ into $\tilde{s} \in \mathcal{B}_{\epsilon}(s)$, where $\mathcal{B}_{\epsilon}(s)$ is a $\ell_p$ norm ball centered at $s$ with radius $\epsilon$. 

\textbf{Action Adversaries.}
Action adversaries' goal is to manipulate the behavior of the agent by directly perturbing the action $a$ executed by the agent to $\tilde{a}$ before the environment receives it (altering the output of the agent's policy), causing it to deviate from the optimal policy. In addition to directly perturbing actions, recent work ~\citep{tessler2019action} has also considered the setting where the action adversary selects a different, adversarial action with the probability $\alpha$ as an uncertainty constraint. In this paper, we focus solely on continuous-space perturbations and employ an admissible action perturbation budget as a commonly used $\ell_p$ threat model, similar to the state perturbation. 

\textbf{Zero-sum Game.}
We model the game between the agent and the adversary as a two-player zero-sum game that is a tuple $\langle \mathcal{S}, \Pi_a, \Pi_v, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\Pi_a$ and $\Pi_v$ denote the sets of policies for the agent and the adversary, respectively. In this framework, both the transition kernels $\mathcal{P}$ and the reward function $\mathcal{R}$ of the victim agent depend on not only its own policy $\pi_a \in \Pi_a$, but also the adversary's policy $\pi_v\in \Pi_v$. The adversary's reward $R(s_t, \bar{a}_t)$ is defined as the negative of the victim agent's reward $R(s_t, a_t)$, reflecting the zero-sum nature of the game. 

\begin{algorithm}[tb]
  \caption{Policy Space Response Oracles \cite{psro}}
  \label{psro}
\begin{algorithmic}
 \STATE {\bfseries Result:} Nash Equilibrium
  \STATE {\bfseries Input:} Initial population $\Pi^0$
  \REPEAT[for $t=0,1,\ldots$]
  \STATE $\pi^r \gets$ NE in game restricted to strategies in $\Pi^t$
  \FOR{$i \in \{1,2\}$}
  \STATE Find a best response $\beta_i \gets \mathbb{BR}_i(\pi^r_{-i})$
  \STATE $\Pi^{t+1}_i \gets \Pi^t_i \cup \{\beta_i\}$
  \ENDFOR
  \UNTIL{Approximate exploitability is less than or equal to zero}
  \STATE {\bfseries Return:} $\pi^r$
\end{algorithmic}
\end{algorithm}
% \textcolor{red}{Connection between zero-sum game and PSRO? I added this in the related work, should we instead move it here?}

% \textbf{Policy-space response oracles (PSRO)} PSRO~\cite{psro} approximates the DO algorithm in extensive-form games. The restricted-game NE is computed on the empirical game matrix $U^\Pi$, generated by having each policy in the population $\Pi$ play each opponent policy and tracking average utility in a $\Pi_1 \times \Pi_2$ payoff matrix \citep{wellman2006methods}. In each iteration, an approximate best response to the current restricted NE over the policies is computed via any RL algorithm. 
\textbf{Double Oracle Algorithm (DO) and Policy Space Response Oracles (PSRO).}
Double oracle~\citep{double_oracle} is an algorithm for finding a NE in normal-form games. The algorithm operates by keeping a population of strategies $\Pi^t$ at time $t$. Each iteration, a NE $\pi^{*,t}$ is computed for the game restricted to strategies in $\Pi^t$. Then, a best response $\mathbb{BR}_i(\pi^{*,t}_{-i})$ to this NE is computed for each player $i$ and added to the population, $\Pi_i^{t+1} = \Pi_i^t \cup \{\mathbb{BR}_i(\pi^{*,t}_{-i}) \}$ for $i \in \{1, 2\}$. 
% The DO algorithm is described in Algorithm \ref{double_oracle}. 
Although in the worst case DO must expand all pure strategies before $\pi^{*,t}$ converges to a NE in the original game, in many games DO terminates early and outperforms alternative methods. An interesting open problem is characterizing games where DO will outperform other methods.

Policy Space Response Oracles (PSRO)~\citep{psro, muller2019generalized, feng2021discovering, mcaleer2022anytime, mcaleer2022self}, shown in Algorithm~\ref{psro} are a method for approximately solving very large games. PSRO maintains a population of reinforcement learning policies and iteratively trains a best response to a mixture of the opponent's population. PSRO is a fundamentally different method than the previously described methods in that in certain games it can be much faster but in other games it can take exponentially long in the worst case. Neural Extensive Form Double Oracle (NXDO)~\citep{mcaleer2021xdo} combines PSRO with extensive-form game solvers and can be used to converge faster that PSRO. 