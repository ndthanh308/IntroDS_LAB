\vspace{-0.5em}
\section{Related Work}
\vspace{-0.5em}
\label{sec:relate}
% There are a number of related deep reinforcement learning methods for two-player zero-sum games. CFR-based techniques such as Deep CFR~\citep{brown2019deep}, DREAM~\citep{steinberger2020dream}, and ESCHER~\citep{mcaleer2022escher}, use deep reinforcement learning to approximate CFR. Policy-gradient techniques such as RPG~\citep{srinivasan2018actor}, NeuRD~\citep{hennes2020neural}, Friction-FoReL~\citep{perolat2021poincare, perolat2022mastering}, and MMD~\citep{sokota2022unified}, approximate Nash equilibrium via modified actor-critic algorithms. In this section we mainly focus on double oracle techniques such as PSRO~\citep{psro}. PSRO-based algorithms have been shown to outperform the previously-mentioned algorithms in certain games~\citep{mcaleer2021xdo}.   
% \subsection{Robust Reinforcement Learning(RL)}

\textbf{Robust RL against perturbations of state observations.}
% There are many existing adversarial defense approaches developed for RL agents against adversarial perturbations on state observations. 
\textit{Regularization-based methods}~\citep{zhang2020robust, shen2020deep,oikarinen2020robust} enforce the policy to have similar outputs under similar inputs, which achieves certifiable performance for DQN in some Atari games. But in continuous control tasks, these methods may not reliably improve the worst-case performance. A recent work by Korkmaz~\citep{korkmaz2021investigating} points out that these adversarially trained models may still be sensible to new perturbations.
\textit{Attack-driven methods} train DRL agents with adversarial examples. Some early works~\citep{kos2017delving, behzadan2017whatever, mandlekar2017adversarially, pattanaik2017robust, franzmeyer2022illusionary, vinitsky2020robust} apply weak or strong gradient-based attacks on state observations to train RL agents against adversarial perturbations. 
Zhang et al.~\citep{zhang2021robust} and Sun et al.~\citep{sun2021strongest} propose to alternately train an RL agent and a strong RL adversary, namely ATLA, which significantly improves the policy robustness against rectangle state perturbations. 
A recent work by Liang et al.~\citep{liang2022efficient} introduce a more principled adversarial training framework which does not explicitly learn the adversary, and both the efficiency and robustness of RL agents are boosted. 
There is also a line of work studying \textit{theoretical guarantees} of adversarial defenses in RL~\citep{lutjens2020certified, oikarinen2020robust,fischer2019online,kumar2021policy, wu2021crop,sun2023certifiably} in various settings. 

%action robust
\textbf{Robust RL against action perturbations.} 
Besides observation perturbations, attacks can happen in many other scenarios. For example, the agent's executed actions can be perturbed~\citep{xiao2019characterizing,tan2020robustifying, tessler2019action,lee2020query, lanier2022feasible}. Moreover, in a multi-agent game, an agent's behavior can create adversarial perturbations to a victim agent~\citep{gleave2019adversarial}.
Pinto et al.~\citep{pinto2017robust} model the competition between the agent and the attacker as a zero-sum two-player game, and train the agent under a learned attacker to tolerate both environment shifts and adversarial disturbances.

\textbf{Robust Markov decision process and safe RL.} 
There are several lines of work that study RL under safety/risk constraints~\citep{Heger1994ConsiderationOR,gaskett2003reinforcement,garcia2015comprehensive,bechtle2020curious,thomas2021safe} or under intrinsic uncertainty of environment dynamics~\citep{lim2013reinforcement,Mankowitz2020Robust}.
In particular, there are several works discussing coupled or non-rectangular uncertainty sets, which allow less conservative and more efficient robust policy learning by incorporating realistic conditions that naturally arise in practice. 
Mannor et al.~\citep{mannor2012lightning} propose to model coupled uncertain parameters based on the intuition that the total number of states with deviated parameters will be small.
Mannor et al.~\citep{mannor2016robust} identify ``k-rectangular'' uncertainty sets defined by the cardinality of possible conditional projections of uncertainty sets, which can lead to more tractable solutions. 
Another recent work by Goyal et al.~\citep{goyal2023robust} propose to model the environment uncertainty with factor matrix uncertainty sets, which can efficiently compute an optimal robust policy. 

\textbf{Two-player zero-sum games.}
There are a number of related deep reinforcement learning methods for two-player zero-sum games. CFR-based techniques such as Deep CFR~\citep{brown2019deep}, DREAM~\citep{steinberger2020dream}, and ESCHER~\citep{mcaleer2022escher}, use deep reinforcement learning to approximate CFR. Policy-gradient techniques such as RPG~\citep{srinivasan2018actor}, NeuRD~\citep{hennes2020neural}, Friction-FoReL~\citep{perolat2021poincare, perolat2022mastering}, and MMD~\citep{sokota2022unified}, approximate Nash equilibrium via modified actor-critic algorithms. Our robust RL approach takes the double oracle techniques such as PSRO~\citep{psro} as the backbone. PSRO-based algorithms have been shown to outperform the previously-mentioned algorithms in certain games~\citep{mcaleer2021xdo}. 
More related work on game-theoretic RL is discussed in Appendix~\ref{sec:add_related}.


