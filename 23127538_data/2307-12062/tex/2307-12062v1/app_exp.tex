\section{Experiment Details and Additional Results}
\label{app:exp}

\subsection{Implementation details}
\label{app:exp:imp}
We provide detailed implementation information for our proposed method (\ours) and baselines.

\textbf{Training Steps}\quad
For \ours, we specify the number of training steps required for different environments. In the Hopper, Walker2d, and Halfcheetah environments, we train for 10 million steps. In the Ant and Humanoid environments, we extend the training duration to 20 million steps. For the ATLA baselines, we train for 2 million steps and 10 million steps in environments of varying difficulty.

\textbf{Network Structure}\quad
Our algorithm (\ours) adopts the same PPO network structure as the baselines to maintain consistency. The network comprises a single-layer LSTM with 64 hidden neurons. Additionally, an input embedding layer is employed to project the state dimension to 64, and an output layer is used to project 64 to the output dimension. Both the agents and the adversaries use the same policy and value networks to facilitate training and evaluation. Furthermore, the network architecture for the best response and meta Nash remains consistent with the aforementioned configuration.

\textbf{Schedule of $\epsilon$ and $\bar{\epsilon}$}\quad
During the training process, we gradually increase the values of $\epsilon$ and $\bar{\epsilon}$ from 0 to their respective target maximum values. This incremental adjustment occurs over the first half of the training steps. We reference the attack budget $\epsilon$ used in other baselines for the corresponding environments. This ensures consistency and allows for a fair comparison with existing methods. The target value of $\bar{\epsilon}$ is determined based on the adversary's training results, which is set as $\epsilon/5$. In some smaller dimensional environments, $\bar{\epsilon}$ can be set to $\epsilon/10$. We have observed that the final performance of the trained robust models does not differ by more than 5\% when using these values for $\bar{\epsilon}$.

\textbf{Training Time}\quad
The training time for \ours varies based on the specific environment and its associated difficulty. On a single V100 GPU, training \ours typically requires over 20 hours for the Hopper, Walker2d, and Halfcheetah environments. For the more complex Ant and Humanoid environments, the training duration extends to approximately 40 hours. The training time required for defense against state adversaries or action adversaries is relatively similar.

\textbf{Observation and Reward Normalization}\quad
To ensure consistency with PPO implementation and maintain comparability across different codebases, we apply observation and reward normalization. Normalization helps to standardize the input observations and rewards, enhancing the stability and convergence of the training process. We have verified the performance of vanilla PPO on different implementations, and the results align closely with our implementation of \ours based on Ray rllib.

\textbf{Hyperparameter Selection}\quad
Hyperparameters such as learning rate, entropy bonus coefficient, and other PPO-specific parameters are crucial for achieving optimal performance. Referring to the results obtained from vanilla PPO and the ATLA baselines as references, a small-scale grid search is conducted to fine-tune the hyperparameters specific to \ours. Because of the significant training time and cost associated with \ours, we initially perform a simplified parameter selection using the Inverted Pendulum as a test environment.

\subsection{Adversaries in experiments}
\textbf{State Adversaries}\quad
Aimed to introduce the attack methods utilized during training and testing in our experiments. When it comes to state adversaries, PA-AD as Alogrithm~\ref{alg:pa-ad} stands out as the strongest attack compared to other state attacks. Therefore, we report the best state attack rewards under PA-AD attacks.

\textbf{Action Adversaries}\quad
In terms of action adversaries, an RL-based action adversary as Alogrithm~\ref{alg:ac-ad} can inflict more severe damage on agents' rewards compared to OU noise and parameter noise in~\citep{tessler2019action}.

\textbf{Adaptable Adversaries}\quad
For adaptable adversaries capable of perturbing both state and action spaces, considering the attack budget and cost, we prefer not to allow the adversary to perturb both spaces simultaneously at one timestep. Hence, it is necessary for the adversary to decide which space to perturb for each timestep. In {alg:sa-ad}, we introduce an additional dimension $\theta_t \in [-1, 1]$ in the adversary's action space to determine the attack domain. If $\theta_t 0$, the adversary perturbs the observation state $s_t$ to $\tilde{s}_t$; otherwise, it attacks the agent's policy output action $a_t$ to $\tilde{a}_t$. Building upon PA-AD~\citep{sun2021strongest}, the adversary director only needs to learn $\hat{a}_t$, which is composed of the policy perturbation $\hat{d}_t$ concatenated with $\theta$. Depending on the adversary's choice $\theta$, different actors will craft state or action perturbations for a given policy perturbation direction $\hat{d}$. This means that the SA-AD attacker only requires an additional dimension for domain choice compared to the PA-AD attacker, without significantly increasing the complexity of adversary training, thereby minimizing the impact on adversary performance. We show the adaptable attack method in Algorithm~\ref{alg:sa-ad}.

\label{app:exp:alg}
\input{algorithms/PA-AD}
\input{algorithms/AC-AD}
\input{algorithms/SA-AD}

\subsection{Attack budgets}
\label{app:exp:eps}
In Figure~\ref{fig:eps}, we report the performance of baselines and \ours under different attack budget $\epsilon$. As the value of $\epsilon$ increases, the rewards of robust agents under different types of attacks decrease accordingly. However, our approach consistently demonstrates superior robustness as the attack budget changes.
\input{figures/fig_eps}

\subsection{Temporally-coupled constraints}
We also investigate the impact of temporally-coupled constraints $\bar{\epsilon}$ on attack performance, as we explained in our experiment section.
\input{figures/fig_bar_eps}
