\vspace{-0.5em}
\begin{algorithm}[tb]
  \caption{\oursfull (\ours)}
  \label{alg:ours}
\begin{algorithmic}
  \STATE \textbf{Input:} Initial policy sets for the agent and adversary $\Pi:\{\Pi_a, \Pi_v\}$
  \STATE Compute expected utilities as empirical payoff matrix $U^{\Pi}$ for each joint $\pi:\{\pi_a, \pi_v\} \in \Pi$
  \STATE Compute meta-Nash equilibrium $\sigma_a$ and $\sigma_v$ over policy sets ($\Pi_a, \Pi_v$)
  \FOR {epoch in $\{1,2,\ldots\}$}
  \FOR {many iterations $N_{\pi_a}$} 
  \STATE Sample the adversary policy $\pi_v\sim \sigma_v$
  \STATE Train $\pi_a'$ with trajectories against the fixed adversary $\pi_v$: $\mathcal{D}_{\pi_a'}:=\{(\hat{s}_{t}^{k},a_{t}^{k},r_{t}^{k},\hat{s}_{t+1}^{k})\}\big|_{k=1}^{B}$ \\(when the fixed adversary only attacks the action space, $\hat{s}_t = s_t$.)
  \ENDFOR
  \STATE$\Pi_a = \Pi_a\cup\{\pi_a'\}$
  \FOR {many iterations $N_{\pi_v}$}
  \STATE Sample the agent policy $\pi_a \sim \sigma_a$
  \STATE Train the adversary policy $\pi_v'$ with trajectories: $\mathcal{D}_{\pi_v'}:=\{(s_{t}^{k},\bar{a}_{t}^{k},-r_{t}^{k},s_{t+1}^{k})\}\big|_{k=1}^{B}$\\
  ($\pi_v'$ applies attacks to the fixed victim agent $\pi_a$ based on $\bar{a}_{t}$ using different methods)
  \ENDFOR
  \STATE$\Pi_v = \Pi_v\cup\{\pi_v'\}$
  \STATE Compute missing entries in $U^{\Pi}$ from $\Pi$
  \STATE Compute new meta strategies $\sigma_a$ and $\sigma_v$ from $U^{\Pi}$
  \ENDFOR
  \STATE \textbf{Return:} current meta Nash equilibrium on whole population $\sigma_a$ and $\sigma_v$
\end{algorithmic}
\end{algorithm}
%\vspace{-0.5em}