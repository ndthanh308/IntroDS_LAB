\begin{algorithm}[!hbt]
  \caption{State or Action Adversary (SA-AD)}
  \label{alg:sa-ad}
  \begin{algorithmic}
  \STATE \textbf{Input:} Initialization of adversary directorâ€™s policy $v$; victim policy $\pi$, the actor function $g_s$ for the state space $\mathcal{S}$ and $g_a$ for the action space $\mathcal{A}$, initial state $s_0$
  \FOR {$t = 0, 1, 2, \ldots$}
  \STATE \textit{Director} $v$ samples a policy perturbing direction and perturbed choice, $\widehat{a}_{t}\sim\nu(\cdot|s_{t})$, where $\widehat{a}_{t} = (\widehat{d}_{t}, \theta_t), \theta_t \in [-1, 1]$
  \IF{$\theta_t \geq 0$}
  \STATE \textit{Actor} perturbs $s_t$ to $\tilde{s}_{t}=g_s(\widehat{d}_{t},s_{t})$
  \STATE Victim takes action $a_{t}\sim\pi(\cdot|\tilde{s}_{t})$, proceeds to $s_{t+1}$, receives $r_t$
  \ELSE 
  \STATE victim policy outputs action $a_{t}\sim\pi(\cdot|s_{t})$
  \STATE \textit{Actor} perturbs $a_t$ to $\tilde{a}_{t}=g_a(\widehat{d}_{t},a_{t})$
  \STATE The environment receives $\tilde{a}_{t}$, returns $s_{t+1}$ and $r_t$
  \ENDIF
  \STATE \textit{Director} saves $(s_t, \widehat{a}_t,-r_t,s_{t+1})$ to the adversary buffer
  \STATE \textit{Director} updates its policy $v$ using any RL algorithms
  \ENDFOR
  \end{algorithmic}
\end{algorithm}
\vspace{-0.5em}