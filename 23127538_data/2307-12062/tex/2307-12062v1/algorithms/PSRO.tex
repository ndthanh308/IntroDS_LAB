\begin{algorithm}[tb]
  \caption{Pipeline Policy-Space Response Oracles (P2SRO) \cite{mcaleer2020pipeline}}
  \label{p2sro}
\begin{algorithmic}
  \STATE \textbf{Input:} Initial policy sets for all players $\Pi^f$
  \STATE Compute expected utilities as empirical payoff matrix $U^{\Pi}$ for each joint $\pi \in \Pi^f$
  \STATE Compute meta-Nash equilibrium $\sigma^{*,j}$ over fixed policies ($\Pi^f$)
  \FOR {epoch in $\{1,2,\ldots\}$}
  \FOR{\textbf{all} $\pi^{j}\in\Pi^a$ (active policies) in parallel}
  \FOR {player $i\in \{1,2\}$} 
  \STATE Sample $\pi_{-i}\sim\sigma_{-i}^{*,j}$
  \STATE Train $\pi_i^j$ against $\pi_{-i}$
  \ENDFOR
  \IF{$\pi^j$ plateaus and $\pi^j$ is the lowest active policy}
  \STATE $\Pi^{f}=\Pi^f\cup\{\pi^j\}$
  \STATE Initialize new active policy at a higher level than all existing active policies
  \STATE Compute missing entries in $U^{\Pi}$ from $\Pi$
  \STATE Compute meta Nash equilibrium for each active policy
  \ENDIF
  \ENDFOR
  \STATE Periodically compute meta Nash equilibrium for each active policy
  \ENDFOR
  \STATE \textbf{Return:} current meta Nash equilibrium on whole population $\sigma^{*}$
\end{algorithmic}
\end{algorithm}
\vspace{-0.5em}