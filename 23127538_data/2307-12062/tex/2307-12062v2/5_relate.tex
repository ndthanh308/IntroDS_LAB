% \vspace{-0.5em}
\section{Related Work}
% \vspace{-0.5em}
\label{sec:relate}
\textbf{Robust RL against adversarial perturbations.}
Existing defense approaches for RL agents are primarily designed to counter adversarial perturbations in state observations. These methods encompass a wide range of strategies, including regularization techniques~\citep{zhang2020robust, shen2020deep, oikarinen2020robust}, attack-driven approaches involving weak or strong gradient-based attacks~\citep{kos2017delving, behzadan2017whatever, mandlekar2017adversarially, pattanaik2017robust, franzmeyer2022illusionary, vinitsky2020robust}, RL-based alternating training methods~\citep{zhang2021robust, sun2021strongest}, and worst-case motivated methods~\citep{liang2022efficient}. Furthermore, there is a line of research that delves into providing \textit{theoretical guarantees} for adversarial defenses in RL~\citep{lutjens2020certified, oikarinen2020robust, fischer2019online, kumar2021policy, wu2021crop, sun2023certifiably}, exploring a variety of settings and scenarios where these defenses can be effectively applied.Aversarial attacks can take various forms. For instance, perturbations can affect the actions executed by the agent~\citep{xiao2019characterizing,  tessler2019action, lanier2022feasible, lee2020spatiotemporally}. Additionally, the study of adversarial multi-agent games has also received attention~\citep{gleave2019adversarial, pinto2017robust}. 

\textbf{Robust Markov decision process and safe RL.} 
There are several lines of work that study RL under safety/risk constraints~\citep{Heger1994ConsiderationOR,gaskett2003reinforcement,garcia2015comprehensive,bechtle2020curious,thomas2021safe} or under intrinsic uncertainty of environment dynamics~\citep{lim2013reinforcement,Mankowitz2020Robust}.
In particular, several works discuss coupled or non-rectangular uncertainty sets, which allow less conservative and more efficient robust policy learning by incorporating realistic conditions that naturally arise in practice. \citet{mannor2012lightning} propose to model coupled uncertain parameters based on the intuition that the total number of states with deviated parameters will be small. \citet{mannor2016robust} identify ``k-rectangular'' uncertainty sets defined by the cardinality of possible conditional projections of uncertainty sets, which can lead to more tractable solutions. Another recent work~\citep{goyal2023robust} proposes to model the environment uncertainty with factor matrix uncertainty sets, which can efficiently compute a robust policies. 

\textbf{Two-player zero-sum games.}
There are a number of related deep reinforcement learning methods for two-player zero-sum games. CFR-based techniques such as Deep CFR~\citep{deep_cfr}, DREAM~\citep{steinberger2020dream}, and ESCHER~\citep{mcaleer2022escher}, use deep reinforcement learning to approximate CFR. Policy-gradient techniques such as NeuRD~\citep{hennes2020neural}, Friction-FoReL~\citep{perolat2021poincare, perolat2022mastering}, and MMD~\citep{sokota2022unified}, approximate Nash equilibrium via modified actor-critic algorithms. Our robust RL approach takes the double oracle techniques such as PSRO~\citep{psro} as the backbone. PSRO-based algorithms have been shown to outperform the previously-mentioned algorithms in certain games~\citep{mcaleer2021xdo}. 

A more detailed discussion of related works in robust RL and game-theoretic RL are in Appendix~\ref{app:related}.


