\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bechtle et~al.(2020)Bechtle, Lin, Rai, Righetti, and
  Meier]{bechtle2020curious}
Sarah Bechtle, Yixin Lin, Akshara Rai, Ludovic Righetti, and Franziska Meier.
\newblock Curious ilqr: Resolving uncertainty in model-based rl.
\newblock In Leslie~Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.),
  \emph{Proceedings of the Conference on Robot Learning}, volume 100 of
  \emph{Proceedings of Machine Learning Research}, pp.\  162--171. PMLR, 30
  Oct--01 Nov 2020.
\newblock URL \url{https://proceedings.mlr.press/v100/bechtle20a.html}.

\bibitem[Behzadan \& Munir(2017)Behzadan and Munir]{behzadan2017whatever}
Vahid Behzadan and Arslan Munir.
\newblock Whatever does not kill deep reinforcement learning, makes it
  stronger.
\newblock \emph{CoRR}, abs/1712.09344, 2017.

\bibitem[Brown \& Sandholm(2017{\natexlab{a}})Brown and
  Sandholm]{brown2017libratus}
Noam Brown and Tuomas Sandholm.
\newblock Libratus: The superhuman {AI} for no-limit poker.
\newblock In \emph{IJCAI}, pp.\  5226--5228, 2017{\natexlab{a}}.

\bibitem[Brown \& Sandholm(2017{\natexlab{b}})Brown and
  Sandholm]{brown2017safe}
Noam Brown and Tuomas Sandholm.
\newblock Safe and nested subgame solving for imperfect-information games.
\newblock \emph{Advances in neural information processing systems}, 30,
  2017{\natexlab{b}}.

\bibitem[Brown \& Sandholm(2018)Brown and Sandholm]{brown2018superhuman}
Noam Brown and Tuomas Sandholm.
\newblock Superhuman {AI} for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock \emph{Science}, 359\penalty0 (6374):\penalty0 418--424, 2018.

\bibitem[Brown et~al.(2018)Brown, Sandholm, and Amos]{brown2018depth}
Noam Brown, Tuomas Sandholm, and Brandon Amos.
\newblock Depth-limited solving for imperfect-information games.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Brown et~al.(2019)Brown, Lerer, Gross, and Sandholm]{deep_cfr}
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm.
\newblock Deep counterfactual regret minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  793--802, 2019.

\bibitem[Brown et~al.(2020)Brown, Bakhtin, Lerer, and Gong]{brown2020combining}
Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong.
\newblock Combining deep reinforcement learning and search for
  imperfect-information games.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17057--17069, 2020.

\bibitem[Burch et~al.(2014)Burch, Johanson, and Bowling]{burch2014solving}
Neil Burch, Michael Johanson, and Michael Bowling.
\newblock Solving imperfect information games using decomposition.
\newblock In \emph{Twenty-eighth AAAI conference on artificial intelligence},
  2014.

\bibitem[Efroni et~al.(2022)Efroni, Jin, Krishnamurthy, and
  Miryoosefi]{efroni22provable}
Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi.
\newblock Provable reinforcement learning with a short-term memory.
\newblock In \emph{{ICML}}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  5832--5850. {PMLR}, 2022.

\bibitem[Feng et~al.(2021)Feng, Slumbers, Yang, Wan, Liu, McAleer, Wen, and
  Wang]{feng2021discovering}
Xidong Feng, Oliver Slumbers, Yaodong Yang, Ziyu Wan, Bo~Liu, Stephen McAleer,
  Ying Wen, and Jun Wang.
\newblock Discovering multi-agent auto-curricula in two-player zero-sum games.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Fischer et~al.(2019)Fischer, Mirman, Stalder, and
  Vechev]{fischer2019online}
Marc Fischer, Matthew Mirman, Steven Stalder, and Martin~T. Vechev.
\newblock Online robustness training for deep reinforcement learning.
\newblock \emph{CoRR}, abs/1911.00887, 2019.

\bibitem[Franzmeyer et~al.(2022)Franzmeyer, McAleer, Henriques, Foerster, Torr,
  Bibi, and de~Witt]{franzmeyer2022illusionary}
Tim Franzmeyer, Stephen McAleer, Jo{\~a}o~F Henriques, Jakob~N Foerster,
  Philip~HS Torr, Adel Bibi, and Christian~Schroeder de~Witt.
\newblock Illusory attacks: Detectability matters in adversarial attacks on
  sequential decision-makers.
\newblock \emph{arXiv preprint arXiv:2207.10170v2}, 2022.

\bibitem[Fu et~al.(2022)Fu, Liu, Wu, Wang, Yang, Li, Xing, Li, Ma, Fu, and
  Wei]{ach}
Haobo Fu, Weiming Liu, Shuang Wu, Yijia Wang, Tao Yang, Kai Li, Junliang Xing,
  Bin Li, Bo~Ma, Qiang Fu, and Yang Wei.
\newblock Actor-critic policy optimization in a large-scale
  imperfect-information game.
\newblock In \emph{Proceedings of the Tenth International Conference on
  Learning Representations (ICLR)}, 2022.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and
  Fern{\'a}ndez]{garcia2015comprehensive}
Javier Garc{\i}a and Fernando Fern{\'a}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Gaskett(2003)]{gaskett2003reinforcement}
Chris Gaskett.
\newblock Reinforcement learning under circumstances beyond its control.
\newblock In \emph{International Conference on Computational Intelligence for
  Modelling Control and Automation}, 2003.

\bibitem[Gleave et~al.(2020)Gleave, Dennis, Wild, Kant, Levine, and
  Russell]{gleave2019adversarial}
Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart
  Russell.
\newblock Adversarial policies: Attacking deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Goyal \& Grand-Clement(2023)Goyal and Grand-Clement]{goyal2023robust}
Vineet Goyal and Julien Grand-Clement.
\newblock Robust markov decision processes: Beyond rectangularity.
\newblock \emph{Mathematics of Operations Research}, 48\penalty0 (1):\penalty0
  203--226, 2023.

\bibitem[Gray et~al.(2020)Gray, Lerer, Bakhtin, and Brown]{gray2020human}
Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown.
\newblock Human-level performance in no-press diplomacy via equilibrium search.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Gruslys et~al.(2020)Gruslys, Lanctot, Munos, Timbers, Schmid, Perolat,
  Morrill, Zambaldi, Lespiau, Schultz, et~al.]{gruslys2020advantage}
Audr{\=u}nas Gruslys, Marc Lanctot, R{\'e}mi Munos, Finbarr Timbers, Martin
  Schmid, Julien Perolat, Dustin Morrill, Vinicius Zambaldi, Jean-Baptiste
  Lespiau, John Schultz, et~al.
\newblock The advantage regret-matching actor-critic.
\newblock \emph{arXiv preprint arXiv:2008.12234}, 2020.

\bibitem[Heger(1994)]{Heger1994ConsiderationOR}
Matthias Heger.
\newblock Consideration of risk in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 1994.

\bibitem[Heinrich \& Silver(2016)Heinrich and Silver]{nfsp}
Johannes Heinrich and David Silver.
\newblock Deep reinforcement learning from self-play in imperfect-information
  games.
\newblock \emph{arXiv preprint arXiv:1603.01121}, 2016.

\bibitem[Hennes et~al.(2020)Hennes, Morrill, Omidshafiei, Munos, Perolat,
  Lanctot, Gruslys, Lespiau, Parmas, Du{\'e}{\~n}ez-Guzm{\'a}n,
  et~al.]{hennes2020neural}
Daniel Hennes, Dustin Morrill, Shayegan Omidshafiei, R{\'e}mi Munos, Julien
  Perolat, Marc Lanctot, Audrunas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas,
  Edgar Du{\'e}{\~n}ez-Guzm{\'a}n, et~al.
\newblock Neural replicator dynamics: Multiagent learning via hedging policy
  gradients.
\newblock In \emph{Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  492--501, 2020.

\bibitem[Huang et~al.(2022)Huang, Xu, Fang, and Zhao]{xu2022robust}
Peide Huang, Mengdi Xu, Fei Fang, and Ding Zhao.
\newblock Robust reinforcement learning as a stackelberg game via
  adaptively-regularized adversarial training.
\newblock In \emph{{IJCAI}}, pp.\  3099--3106. ijcai.org, 2022.

\bibitem[Huang et~al.(2017)Huang, Papernot, Goodfellow, Duan, and
  Abbeel]{huang2017adversarial}
Sandy~H. Huang, Nicolas Papernot, Ian~J. Goodfellow, Yan Duan, and Pieter
  Abbeel.
\newblock Adversarial attacks on neural network policies.
\newblock In \emph{International Conference on Learning
  Representations(Workshop)}, 2017.

\bibitem[Iyengar(2005)]{iyengar05robust}
Garud~N. Iyengar.
\newblock Robust dynamic programming.
\newblock \emph{Math. Oper. Res.}, 30\penalty0 (2):\penalty0 257--280, 2005.

\bibitem[Korkmaz(2021)]{korkmaz2021investigating}
Ezgi Korkmaz.
\newblock Investigating vulnerabilities of deep neural policies.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1661--1670.
  PMLR, 2021.

\bibitem[Kos \& Song(2017)Kos and Song]{kos2017delving}
Jernej Kos and Dawn Song.
\newblock Delving into adversarial attacks on deep policies.
\newblock In \emph{International Conference on Learning
  Representations(Workshop)}, 2017.

\bibitem[Kumar et~al.(2022)Kumar, Levine, and Feizi]{kumar2021policy}
Aounon Kumar, Alexander Levine, and Soheil Feizi.
\newblock Policy smoothing for provably robust reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Lanctot et~al.(2017)Lanctot, Zambaldi, Gruslys, Lazaridou, Tuyls,
  P{\'e}rolat, Silver, and Graepel]{psro}
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl
  Tuyls, Julien P{\'e}rolat, David Silver, and Thore Graepel.
\newblock A unified game-theoretic approach to multiagent reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Lanier et~al.(2022)Lanier, McAleer, Baldi, and
  Fox]{lanier2022feasible}
JB~Lanier, Stephen McAleer, Pierre Baldi, and Roy Fox.
\newblock Feasible adversarial robust reinforcement learning for underspecified
  environments.
\newblock \emph{arXiv preprint arXiv:2207.09597}, 2022.

\bibitem[Lee et~al.(2020)Lee, Ghadai, Tan, Hegde, and
  Sarkar]{lee2020spatiotemporally}
Xian~Yeow Lee, Sambit Ghadai, Kai~Liang Tan, Chinmay Hegde, and Soumik Sarkar.
\newblock Spatiotemporally constrained action space attacks on deep
  reinforcement learning agents.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pp.\  4577--4584, 2020.

\bibitem[Li et~al.(2020)Li, Koyamada, Ye, Liu, Wang, Yang, Zhao, Qin, Liu, and
  Hon]{li2020suphx}
Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang,
  Li~Zhao, Tao Qin, Tie-Yan Liu, and Hsiao-Wuen Hon.
\newblock Suphx: Mastering mahjong with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2003.13590}, 2020.

\bibitem[Liang et~al.(2022)Liang, Sun, Zheng, and Huang]{liang2022efficient}
Yongyuan Liang, Yanchao Sun, Ruijie Zheng, and Furong Huang.
\newblock Efficient adversarial training without attacking: Worst-case-aware
  robust reinforcement learning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=y-E1htoQl-n}.

\bibitem[Lim et~al.(2013)Lim, Xu, and Mannor]{lim2013reinforcement}
Shiau~Hong Lim, Huan Xu, and Shie Mannor.
\newblock Reinforcement learning in robust markov decision processes.
\newblock \emph{Advances in Neural Information Processing Systems},
  26:\penalty0 701--709, 2013.

\bibitem[Liu et~al.(2022)Liu, Li, and Togelius]{liu2022model}
Weiming Liu, Bin Li, and Julian Togelius.
\newblock Model-free neural counterfactual regret minimization with bootstrap
  learning.
\newblock \emph{IEEE Transactions on Games}, 2022.

\bibitem[L{\"u}tjens et~al.(2020)L{\"u}tjens, Everett, and
  How]{lutjens2020certified}
Bj{\"o}rn L{\"u}tjens, Michael Everett, and Jonathan~P How.
\newblock Certified adversarial robustness for deep reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1328--1337. PMLR, 2020.

\bibitem[Mandlekar et~al.(2017)Mandlekar, Zhu, Garg, Fei-Fei, and
  Savarese]{mandlekar2017adversarially}
Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li~Fei-Fei, and Silvio Savarese.
\newblock Adversarially robust policy learning: Active construction of
  physically-plausible perturbations.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  3932--3939. IEEE, 2017.

\bibitem[Mankowitz et~al.(2020)Mankowitz, Levine, Jeong, Abdolmaleki,
  Springenberg, Shi, Kay, Hester, Mann, and Riedmiller]{Mankowitz2020Robust}
Daniel~J. Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost~Tobias
  Springenberg, Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy Mann, and Martin
  Riedmiller.
\newblock Robust reinforcement learning for continuous control with model
  misspecification.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJgC60EtwB}.

\bibitem[Mannor et~al.(2012)Mannor, Mebel, and Xu]{mannor2012lightning}
Shie Mannor, Ofir Mebel, and Huan Xu.
\newblock Lightning does not strike twice: Robust mdps with coupled
  uncertainty.
\newblock \emph{arXiv preprint arXiv:1206.4643}, 2012.

\bibitem[Mannor et~al.(2016)Mannor, Mebel, and Xu]{mannor2016robust}
Shie Mannor, Ofir Mebel, and Huan Xu.
\newblock Robust mdps with k-rectangular uncertainty.
\newblock \emph{Mathematics of Operations Research}, 41\penalty0 (4):\penalty0
  1484--1509, 2016.

\bibitem[McAleer et~al.(2020)McAleer, Lanier, Fox, and
  Baldi]{mcaleer2020pipeline}
Stephen McAleer, John Lanier, Roy Fox, and Pierre Baldi.
\newblock Pipeline {PSRO}: A scalable approach for finding approximate {Nash}
  equilibria in large games.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[McAleer et~al.(2021)McAleer, Lanier, Baldi, and Fox]{mcaleer2021xdo}
Stephen McAleer, John Lanier, Pierre Baldi, and Roy Fox.
\newblock {XDO}: A double oracle algorithm for extensive-form games.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[McAleer et~al.(2022{\natexlab{a}})McAleer, Lanier, Wang, Baldi, Fox,
  and Sandholm]{mcaleer2022self}
Stephen McAleer, JB~Lanier, Kevin Wang, Pierre Baldi, Roy Fox, and Tuomas
  Sandholm.
\newblock Self-play psro: Toward optimal populations in two-player zero-sum
  games.
\newblock \emph{arXiv preprint arXiv:2207.06541}, 2022{\natexlab{a}}.

\bibitem[McAleer et~al.(2022{\natexlab{b}})McAleer, Wang, Lanctot, Lanier,
  Baldi, and Fox]{mcaleer2022anytime}
Stephen McAleer, Kevin Wang, Marc Lanctot, John Lanier, Pierre Baldi, and Roy
  Fox.
\newblock Anytime optimal psro for two-player zero-sum games.
\newblock \emph{arXiv preprint arXiv:2201.07700}, 2022{\natexlab{b}}.

\bibitem[McAleer et~al.(2023)McAleer, Farina, Lanctot, and
  Sandholm]{mcaleer2022escher}
Stephen McAleer, Gabriele Farina, Marc Lanctot, and Tuomas Sandholm.
\newblock Escher: Eschewing importance sampling in games by computing a history
  value function to estimate regret.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[McMahan et~al.(2003)McMahan, Gordon, and Blum]{double_oracle}
H~Brendan McMahan, Geoffrey~J Gordon, and Avrim Blum.
\newblock Planning in the presence of cost functions controlled by an
  adversary.
\newblock \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML)}, 2003.

\bibitem[Moravcik et~al.(2016)Moravcik, Schmid, Ha, Hladik, and
  Gaukrodger]{moravcik2016refining}
Matej Moravcik, Martin Schmid, Karel Ha, Milan Hladik, and Stephen Gaukrodger.
\newblock Refining subgames in large imperfect information games.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Morav{\v{c}}{\'\i}k et~al.(2017)Morav{\v{c}}{\'\i}k, Schmid, Burch,
  Lis{\`y}, Morrill, Bard, Davis, Waugh, Johanson, and
  Bowling]{moravvcik2017deepstack}
Matej Morav{\v{c}}{\'\i}k, Martin Schmid, Neil Burch, Viliam Lis{\`y}, Dustin
  Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael
  Bowling.
\newblock Deepstack: Expert-level artificial intelligence in heads-up no-limit
  poker.
\newblock \emph{Science}, 356\penalty0 (6337):\penalty0 508--513, 2017.

\bibitem[Muller et~al.(2019)Muller, Omidshafiei, Rowland, Tuyls, Perolat, Liu,
  Hennes, Marris, Lanctot, Hughes, et~al.]{muller2019generalized}
Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Perolat,
  Siqi Liu, Daniel Hennes, Luke Marris, Marc Lanctot, Edward Hughes, et~al.
\newblock A generalized training approach for multiagent learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Nilim \& Ghaoui(2005)Nilim and Ghaoui]{nilim05robust}
Arnab Nilim and Laurent~El Ghaoui.
\newblock Robust control of markov decision processes with uncertain transition
  matrices.
\newblock \emph{Oper. Res.}, 53\penalty0 (5):\penalty0 780--798, 2005.

\bibitem[Oikarinen et~al.(2021)Oikarinen, Zhang, Megretski, Daniel, and
  Weng]{oikarinen2020robust}
Tuomas Oikarinen, Wang Zhang, Alexandre Megretski, Luca Daniel, and Tsui-Wei
  Weng.
\newblock Robust deep reinforcement learning through adversarial loss.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=eaAM_bdW0Q}.

\bibitem[Pan et~al.(2022)Pan, Xiao, He, Yang, Peng, Sun, Liu, Li, and
  Song]{xiao2019characterizing}
Xinlei Pan, Chaowei Xiao, Warren He, Shuang Yang, Jian Peng, Mingjie Sun,
  Mingyan Liu, Bo~Li, and Dawn Song.
\newblock Characterizing attacks on deep reinforcement learning.
\newblock In \emph{{AAMAS}}, pp.\  1010--1018. International Foundation for
  Autonomous Agents and Multiagent Systems {(IFAAMAS)}, 2022.

\bibitem[Pattanaik et~al.(2018)Pattanaik, Tang, Liu, Bommannan, and
  Chowdhary]{pattanaik2017robust}
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish
  Chowdhary.
\newblock Robust deep reinforcement learning with adversarial attacks.
\newblock In \emph{{AAMAS}}, pp.\  2040--2042. International Foundation for
  Autonomous Agents and Multiagent Systems Richland, SC, {USA} / {ACM}, 2018.

\bibitem[Perolat et~al.(2021)Perolat, Munos, Lespiau, Omidshafiei, Rowland,
  Ortega, Burch, Anthony, Balduzzi, De~Vylder, et~al.]{perolat2021poincare}
Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark
  Rowland, Pedro Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart
  De~Vylder, et~al.
\newblock From {P}oincar{\'e} recurrence to convergence in imperfect
  information games: Finding equilibrium via regularization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8525--8535. PMLR, 2021.

\bibitem[Perolat et~al.(2022)Perolat, de~Vylder, Hennes, Tarassov, Strub,
  de~Boer, Muller, Connor, Burch, Anthony, et~al.]{perolat2022mastering}
Julien Perolat, Bart de~Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub,
  Vincent de~Boer, Paul Muller, Jerome~T Connor, Neil Burch, Thomas Anthony,
  et~al.
\newblock Mastering the game of stratego with model-free multiagent
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2206.15378}, 2022.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2817--2826. PMLR, 2017.

\bibitem[Rahimian \& Mehrotra(2019)Rahimian and
  Mehrotra]{rahimian2019distributionally}
Hamed Rahimian and Sanjay Mehrotra.
\newblock Distributionally robust optimization: A review.
\newblock \emph{arXiv preprint arXiv:1908.05659}, 2019.

\bibitem[Schmid et~al.(2021)Schmid, Moravcik, Burch, Kadlec, Davidson, Waugh,
  Bard, Timbers, Lanctot, Holland, et~al.]{schmid2021player}
Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin
  Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, et~al.
\newblock Player of games.
\newblock \emph{arXiv preprint arXiv:2112.03178}, 2021.

\bibitem[Serrino et~al.(2019)Serrino, Kleiman-Weiner, Parkes, and
  Tenenbaum]{serrino2019finding}
Jack Serrino, Max Kleiman-Weiner, David~C Parkes, and Josh Tenenbaum.
\newblock Finding friend and foe in multi-agent games.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Shen et~al.(2020)Shen, Li, Jiang, Wang, and Zhao]{shen2020deep}
Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao.
\newblock Deep reinforcement learning with robust and smooth policy.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8707--8718. PMLR, 2020.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sokota et~al.(2022)Sokota, D'Orazio, Kolter, Loizou, Lanctot,
  Mitliagkas, Brown, and Kroer]{sokota2022unified}
Samuel Sokota, Ryan D'Orazio, J~Zico Kolter, Nicolas Loizou, Marc Lanctot,
  Ioannis Mitliagkas, Noam Brown, and Christian Kroer.
\newblock A unified approach to reinforcement learning, quantal response
  equilibria, and two-player zero-sum games.
\newblock \emph{arXiv preprint arXiv:2206.05825}, 2022.

\bibitem[Srinivasan et~al.(2018)Srinivasan, Lanctot, Zambaldi, P{\'e}rolat,
  Tuyls, Munos, and Bowling]{srinivasan2018actor}
Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P{\'e}rolat, Karl
  Tuyls, R{\'e}mi Munos, and Michael Bowling.
\newblock Actor-critic policy optimization in partially observable multiagent
  environments.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Steinberger(2019)]{steinberger2019single}
Eric Steinberger.
\newblock Single deep counterfactual regret minimization.
\newblock \emph{arXiv preprint arXiv:1901.07621}, 2019.

\bibitem[Steinberger et~al.(2020)Steinberger, Lerer, and
  Brown]{steinberger2020dream}
Eric Steinberger, Adam Lerer, and Noam Brown.
\newblock {DREAM}: Deep regret minimization with advantage baselines and
  model-free learning.
\newblock \emph{arXiv preprint arXiv:2006.10410}, 2020.

\bibitem[Sun et~al.(2022)Sun, Zheng, Liang, and Huang]{sun2021strongest}
Yanchao Sun, Ruijie Zheng, Yongyuan Liang, and Furong Huang.
\newblock Who is the strongest enemy? towards optimal and efficient evasion
  attacks in deep {RL}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Sun et~al.(2023)Sun, Zheng, Hassanzadeh, Liang, Feizi, Ganesh, and
  Huang]{sun2023certifiably}
Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, Yongyuan Liang, Soheil Feizi,
  Sumitra Ganesh, and Furong Huang.
\newblock Certifiably robust policy learning against adversarial multi-agent
  communication.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=dCOL0inGl3e}.

\bibitem[Tessler et~al.(2019)Tessler, Efroni, and Mannor]{tessler2019action}
Chen Tessler, Yonathan Efroni, and Shie Mannor.
\newblock Action robust reinforcement learning and applications in continuous
  control.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6215--6224. PMLR, 2019.

\bibitem[Thomas et~al.(2021)Thomas, Luo, and Ma]{thomas2021safe}
Garrett Thomas, Yuping Luo, and Tengyu Ma.
\newblock Safe reinforcement learning by imagining the near future.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=vIDBSGl3vzl}.

\bibitem[Vinitsky et~al.(2020)Vinitsky, Du, Parvate, Jang, Abbeel, and
  Bayen]{vinitsky2020robust}
Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and
  Alexandre Bayen.
\newblock Robust reinforcement learning using adversarial populations.
\newblock \emph{arXiv preprint arXiv:2008.01825}, 2020.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{alphastar}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in {StarCraft II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wu et~al.(2022)Wu, Li, Huang, Vorobeychik, Zhao, and Li]{wu2021crop}
Fan Wu, Linyi Li, Zijian Huang, Yevgeniy Vorobeychik, Ding Zhao, and Bo~Li.
\newblock {CROP:} certifying robust policies for reinforcement learning through
  functional smoothing.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wurman et~al.(2022)Wurman, Barrett, Kawamoto, MacGlashan, Subramanian,
  Walsh, Capobianco, Devlic, Eckert, Fuchs, et~al.]{wurman2022outracing}
Peter~R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik
  Subramanian, Thomas~J Walsh, Roberto Capobianco, Alisa Devlic, Franziska
  Eckert, Florian Fuchs, et~al.
\newblock Outracing champion gran turismo drivers with deep reinforcement
  learning.
\newblock \emph{Nature}, 602\penalty0 (7896):\penalty0 223--228, 2022.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Zheng, Liang, Wang, Yuan, Ji, Luo,
  Liu, Yuan, Hua, et~al.]{xu2023drm}
Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying
  Ji, Yu~Luo, Xiaoyu Liu, Jiaxin Yuan, Pu~Hua, et~al.
\newblock Drm: Mastering visual reinforcement learning through dormant ratio
  minimization.
\newblock \emph{arXiv preprint arXiv:2310.19668}, 2023{\natexlab{a}}.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Panaganti, and
  Kalathil]{xu2023improved}
Zaiyan Xu, Kishan Panaganti, and Dileep Kalathil.
\newblock Improved sample complexity bounds for distributionally robust
  reinforcement learning.
\newblock In \emph{{AISTATS}}, volume 206 of \emph{Proceedings of Machine
  Learning Research}, pp.\  9728--9754. {PMLR}, 2023{\natexlab{b}}.

\bibitem[Zha et~al.(2021)Zha, Xie, Ma, Zhang, Lian, Hu, and
  Liu]{zha2021douzero}
Daochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, Xiangru Lian, Xia Hu, and
  Ji~Liu.
\newblock Douzero: Mastering doudizhu with self-play deep reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12333--12344. PMLR, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Chen, Xiao, Li, Liu, Boning, and
  Hsieh]{zhang2020robust}
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo~Li, Mingyan Liu, Duane Boning, and
  Cho-Jui Hsieh.
\newblock Robust deep reinforcement learning against adversarial perturbations
  on state observations.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  21024--21037. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/f0eb6568ea114ba6e293f903c34d7488-Paper.pdf}.

\bibitem[Zhang et~al.(2021)Zhang, Chen, Boning, and Hsieh]{zhang2021robust}
Huan Zhang, Hongge Chen, Duane~S Boning, and Cho-Jui Hsieh.
\newblock Robust reinforcement learning on state observations with learned
  optimal adversary.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=sCZbhBvqQaU}.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Cheng, Kalathil, Kumar, and
  Tian]{zhou2023natural}
Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, P.~R. Kumar, and Chao Tian.
\newblock Natural actor-critic for robust reinforcement learning with function
  approximation, 2023.

\end{thebibliography}
