% \vspace{-0.5em}
\section{Preliminaries}
% \vspace{-0.5em}
\label{sec:pre}

% \begin{algorithm}
% \SetAlgoLined

% \DontPrintSemicolon
% \KwResult{Nash Equilibrium}
%  Input: initial population $\Pi^0$\;
%  \While{Not terminated}{
%   Solve game restricted to policies in $\Pi^t$ to get meta-distribution $\pi^r$\;
%   \For{$i \in \{1,2\}$}{
%       Find a best response $\mathbb{BR}_i(\pi^r_{-i})$\;
%       $\Pi^{t+1}_i = \Pi^t_i \cup \mathbb{BR}_i(\pi^r_{-i})$\;
%   } 
%   \If{No novel best response exists for both players}{
% %   \If{$gv(G(\Pi_0, all possible p1 strategies)) = gv(G(all possible p0 strategies, \Pi_1))$}
%     Return $\pi^r$\;
%   }
%  }
%  \caption{Double Oracle}
% \end{algorithm}

\textbf{Notations and Background.} 
A Markov decision process (MDP) can be defined as a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ and $\mathcal{A}$ represent the state space and the action space, $\mathcal{R}$ is the reward function: $\mathcal{R}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})$ represents the set of probability distributions over the state space $\mathcal{S}$ and $\gamma\in(0,1)$ is the discount factor. The agent selects actions based on its policy, $\pi:\mathcal{S} \to \Delta(\mathcal{A})$, which is represented by a function approximator (e.g. a neural network) that is updated during training and fixed during testing. The value function is denoted by $U^{\pi}(s):=\mathbb{E}_{P,\pi}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})\mid s_{0}=s]$, which measures the expected cumulative discounted reward that an agent can obtain from state $s\in\mathcal{S}$ by following policy $\pi$.

\textbf{State or Action Adversaries.} State adversary is a type of test-time attacker that perturbs the agent's state observation returned by the environment at each time step and aims to reduce the expected episode reward gained by the agent. While the input to the agent's policy is perturbed, the underlying state in the environment remains unchanged. State adversaries, such as those presented in ~\citep{zhang2020robust, zhang2021robust, sun2021strongest}, typically consider perturbations on a continuous state space under a certain attack budget $\epsilon$. The attacker perturbs a state $s$ into $\tilde{s} \in \mathcal{B}_{\epsilon}(s)$, where $\mathcal{B}_{\epsilon}(s)$ is a $\ell_p$ norm ball centered at $s$ with radius $\epsilon$. 
Moreover, Action adversaries' goal is to manipulate the behavior of the agent by directly perturbing the action $a$ executed by the agent to $\tilde{a}$ with the probability $\alpha$ as an uncertainty constraint before the environment receives it (altering the output of the agent's policy), causing it to deviate from the optimal policy~\citep{tessler2019action}. In this paper, we focus solely on continuous-space perturbations and employ an admissible action perturbation budget as a commonly used $\ell_p$ threat model.

\textbf{Problem formulations as a zero-sum game.}
We model the game between the agent and the adversary as a two-player zero-sum game that is a tuple $\langle \mathcal{S}, \Pi_a, \Pi_v, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\Pi_a$ and $\Pi_v$ denote the sets of policies for the agent and the adversary, respectively. In this framework, both the transition kernels $\mathcal{P}$ and the reward function $\mathcal{R}$ of the victim agent depend on not only its own policy $\pi_a \in \Pi_a$, but also the adversary's policy $\pi_v\in \Pi_v$. The adversary's reward $R(s_t, \bar{a}_t)$ is defined as the negative of the victim agent's reward $R(s_t, a_t)$, reflecting the zero-sum nature of the game. The expected value $u_a^{\pi_a}(h)$ for the agent is the expected sum of future rewards in history $h$ and the robuat RL problem as a two-player zero-sum game has $u_v^{\pi_v}(h) + u_a^{\pi_a}(h) = 0$ for all agent and adversary strategies. A Nash equilibrium (NE) is a strategy profile such that, if all players played their NE strategy, no player could achieve higher value by deviating from it. Formally, $\pi^{*}_a$ is a NE if $u_a(\pi^{*}_a)=\max_{\pi_a}u_a(\pi_a,\pi_{v}^*)$. A best response $\mathbb{BR}$ strategy $\mathbb{BR}_a(\pi_{v})$ for the agent $a$ to a strategy $\pi_{v}$ is a strategy that maximally exploits $\pi_v: \mathbb{BR}_a(\pi_{v})$ = $\arg \max _{\pi_a}u_a( \pi_a, \pi_v)$. In this paper, our goal is to converge to the approximate NE for the zero-sum game. 


% Figure environment removed
% \textcolor{red}{Connection between zero-sum game and PSRO? I added this in the related work, should we instead move it here?}

% \textbf{Policy-space response oracles (PSRO)} PSRO~\cite{psro} approximates the DO algorithm in extensive-form games. The restricted-game NE is computed on the empirical game matrix $U^\Pi$, generated by having each policy in the population $\Pi$ play each opponent policy and tracking average utility in a $\Pi_1 \times \Pi_2$ payoff matrix \citep{wellman2006methods}. In each iteration, an approximate best response to the current restricted NE over the policies is computed via any RL algorithm. 
\textbf{Double Oracle Algorithm (DO) and Policy Space Response Oracles (PSRO).}
Double oracle~\citep{double_oracle} is an algorithm for finding a Nash Equilibrium (NE) in normal-form games. The algorithm operates by keeping a population of strategies $\Pi^t$ at time $t$. Each iteration, a NE $\pi^{*,t}$ is computed for the game restricted to strategies in $\Pi^t$. Then, a best response $\mathbb{BR}_i(\pi^{*,t}_{-i})$ to this NE is computed for each player $i$ and added to the population, $\Pi_i^{t+1} = \Pi_i^t \cup \{\mathbb{BR}_i(\pi^{*,t}_{-i}) \}$ for $i \in \{1, 2\}$. 
% The DO algorithm is described in Algorithm \ref{double_oracle}. 
Although in the worst case DO must expand all pure strategies before $\pi^{*,t}$ converges to an NE in the original game, in many games DO terminates early and outperforms alternative methods. An interesting open problem is characterizing games where DO will outperform other methods.

Policy Space Response Oracles (PSRO)~\citep{psro, muller2019generalized, feng2021discovering, mcaleer2022anytime, mcaleer2022self}, shown in Algorithm~\ref{psro} are a method for approximately solving very large games. PSRO maintains a population of reinforcement learning policies and iteratively trains the best response to a mixture of the opponent's population. PSRO is a fundamentally different method than the previously described methods in that in certain games it can be much faster but in other games it can take exponentially long in the worst case. 
%Neural Extensive Form Double Oracle (NXDO)~\citep{mcaleer2021xdo} combines PSRO with extensive-form game solvers and can be used to converge faster than PSRO. 