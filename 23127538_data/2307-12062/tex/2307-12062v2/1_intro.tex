\section{Introduction}
\label{sec:intro}
In recent years, reinforcement learning (RL) has demonstrated success in tackling complex decision-making problems in various domains. However, the vulnerability of deep RL algorithms to test-time changes in the environment or adversarial attacks has raised concerns for real-world applications. 
% In particular, attackers may perturb the state observation received by the agent or the action executed by the agent to compromise its performance. 
Developing robust RL algorithms that can defend against these adversarial attacks is crucial for the safety, reliability and effectiveness of RL-based systems.% in practical scenarios.

In most existing research on robust RL~\citep{huang2017adversarial,liang2022efficient,sun2021strongest,tessler2019action,zhang2020robust}, the adversary is able to perturb the observation or action every timestep under a static constraint. Specifically, the adversary's perturbations are constrained within a predefined space, such as an $L_p$ norm, which remains unchanged from one timestep to the next. This \textit{standard} assumption in the robust RL literature can be referred to as a \textit{non-temporally-coupled} assumption. However, this static constraint can lead to unrealistic perturbations: for example, the attacker may be able to blow the wind hard southeast at time $t$ but northwest at time $t+1$.
Providing robustness against such an perturbations may result in an overly conservative policy.

% to achieve optimal perturbations 
% within this $L_p$ norm under this static constraint.
However, the set of perturbations faced in the real world are typically \textit{temporally-coupled}: if the wind blows in one direction at one time step, it will likely blow in a similar directly at the next step.
% In contrast, in real-world settings the adversary may not have complete flexibility to perturb the environment differently across timesteps. For example, it is unlikely for the wind to move in one direction in one second, then in the opposite direction in the next second. In \textit{temporally-coupled} settings, employing 
% a robust policy learning technique designed for the static attack strategy
% a standard attack strategy for learning a robust policy 
% would result in an excessively conservative policy.
In this paper, we will treat the robust RL problem as a partially-observable two-player game and use tools from game theory to acquire robust policies, both for the non-temporally-coupled and the temporally-coupled settings. \looseness=-1
%we introduce a game-theoretic algorithm that lets the agent automatically adapt to the adversary under any attack constraints, either standard or temporally-coupled. 

In this paper, we propose a novel approach: \oursfull (\ours) that leverages Policy Space Response Oracles (PSRO)~\citep{psro} for robust training. \ours is more general than prior adversarial defenses in the sense that it does not target certain adversarial scenarios and converges to the approximate equilibrum training with an adversary policy set. While prior methods often assume the worst case and aim to improve against them, they lack adaptability to specific attacks such as these adversaries under temporally-coupled constraints.
We formulate the interaction between the agent and the temporally-coupled adversary as a two-player zero-sum game and employ PSRO to ensure the agent's best response against the learned adversary and find an approximate equilibrium. This game-theoretic framework empowers our approach to effectively maximize the agent's performance by adapting to the adversary's strategies.

Our contributions are three-fold. \textit{First}, we propose a novel class of temporally-coupled adversarial attacks to identify the realistic pitfalls of prior threat models as a challenge for existing robust RL methods. \textit{Second}, we introduce a game-theoretic response approach, referred to as \ours. We highlight the significant advantages of GRAD in terms of convergence and policy exploitability.
Notably, GRAD demonstrates adaptability to adversaries in both temporally-coupled and non-temporally-coupled settings. Furthermore, GRAD serves as a versatile and flexible solution for adversarial RL, enhancing robustness against diverse types of adversaries.

\textit{Third}, we provide empirical results that demonstrate the effectiveness of our approach in defending against both temporally-coupled and non-temporally coupled adversaries on various attack domains. Figure~\ref{fig:beh} illustrates how a robustness to temporally-coupled perturbations induces different behavior than robustness to standard perturbations.
% shows interpretable phenomenons of \ours agent and robust baselines under different types of attacks in Humanoid.
% Figure environment removed

