%\vspace{-0.5em}
\section{Conclusion and Discussion}
% \vspace{-0.5em}
% In this paper, we have introduced an attack model which follows a proposed temporally-coupled assumption. 
Motivated by the perturbations that arise in real world scenarios, we introduce a new attack model for studying deep RL models.
Since existing robust RL methods usually focus on a traditional threat model that perturbs state observations or actions arbitrarily within an $L_p$ norm ball, they become too conservative and can fail to perform a good defense under the temporally-coupled attacks. 
In contrast, we propose a game-theoretical response approach \ours, which finds the best response against attacks with various constraints including temporally-coupled ones. 
% \ours is based on the PSRO paradigm, which is shown to be effective and theoretically grounded in finding Nash equilibrium in two-player zero-sum games. 
Experiments across a range of continuous control tasks underscore the good performance of our approach over previous robust RL methods for both non-temporally-coupled attacks and temporally-coupled attacks across diverse attack domains.
% , highlighting the generalized robustness of \ours.

\textbf{Limitations.}\quad
The current PSRO-based approach may require several iterations to converge to the best response, which can pose limitations when computational resources are constrained. We leverage distributed RL tools to expedite the training of RL agents within \ours, enabling efficient learning of the best response. Detailed computational cost analysis can be found in Appendix~\ref{app:efficient}.

Regarding scalability concerns, we have demonstrated the \ours in addressing robust RL problems on high-dimensional tasks. In principle, alternative game-theoretic algorithms~\citep{perolat2022mastering}, known for their practical efficiency, can be considered for defense in different game scenarios. As part of our future research directions, we plan to explore methods to further enhance the scalability of \ours. This exploration may involve harnessing parallel training techniques and drawing insights from other scalable PSRO approaches~\citep{mcaleer2020pipeline, psro}. Additionally, we aim to extend the applicability of our method to pixel-based RL scenarios and real-world situations with increased practicality and complexity.




