\section{Additional Related Work}
\label{app:related}
\textbf{Robust RL against adversarial perturbations.}
% There are many existing adversarial defense approaches developed for RL agents against adversarial perturbations on state observations. 
\textit{Regularization-based methods}~\citep{zhang2020robust, shen2020deep,oikarinen2020robust} enforce the policy to have similar outputs under similar inputs, which can achieve certifiable performance for visual-input RL~\citep{xu2023drm} on Atari games. However, in continuous control tasks, these methods may not reliably improve the worst-case performance. Recent work by \cite{korkmaz2021investigating} points out that these adversarially trained models may still be sensible to new perturbations.
\textit{Attack-driven methods} train DRL agents with adversarial examples. Some early works~\citep{kos2017delving, behzadan2017whatever, mandlekar2017adversarially, pattanaik2017robust, franzmeyer2022illusionary, vinitsky2020robust} apply weak or strong gradient-based attacks on state observations to train RL agents against adversarial perturbations. 
\cite{zhang2021robust} and \cite{sun2021strongest} propose to alternately train an RL agent and a strong RL adversary, namely ATLA, which significantly improves the policy robustness against rectangle state perturbations. 
A recent work by ~\cite{liang2022efficient} introduces a more principled adversarial training framework that does not explicitly learn the adversary, and both the efficiency and robustness of RL agents are boosted. 

Additionally, a significant body of research has delved into providing \textit{theoretical guarantees} for adversarial defenses in RL~\citep{lutjens2020certified, oikarinen2020robust, fischer2019online, kumar2021policy, wu2021crop, sun2023certifiably}, exploring various settings and scenarios. Robust RL faces challenges under model uncertainty in prior works~\citep{iyengar05robust, nilim05robust, xu2023improved}. The main goal of \ours is to address the adversarial RL problem with an adversary that is adaptive to the agent's policy. Works like \cite{tessler2019action} on action perturbations and \cite{zhou2023natural} on model mismatch uncertainty, are hard to defend against the strongest adversarial perturbations and only empirically evaluated on uncertainty sets. This vulnerability arises due to the inherent difficulty in estimating the long-term worst-case value under adaptive adversaries. Distributional robust optimization (DRO)\citep{rahimian2019distributionally} is also challenging to apply to this challenging problem, especially against state adversaries in the high-dimensional state space. At the same time, adversarial training methods also struggle to effectively deal with model uncertainty or model mismatch problems. However, \ours, leveraging game-theoretic methods, demonstrates robustness against adversarial perturbations and model uncertainty, as a more effective and general solution for high-dimensional tasks.

\textbf{Robust RL formulated as a zero-shot game.} 
Considering robust RL formulated as zero-sum games, several notable contributions have emerged. \cite{pinto2017robust} proposed robust adversarial reinforcement learning (RARL), introducing the concept of training an agent in the presence of a destabilizing adversary through a zero-sum minimax objective function. \cite{xu2022robust} extended this paradigm with RRL-Stack, a hierarchical formulation of robust RL using a general-sum Stackelberg game model. \cite{tessler2019action} addressed action uncertainty by framing the action perturbation problem as a zero-sum game. \ours distinguishes itself from conventional robust RL approaches by achieving approximate equilibriums on an adversary policy set. While existing works often focus on specific adversaries or worst-case scenarios, limiting their adaptability, GRAD does not specifically target certain adversaries, which makes it adaptable to various adversaries, including both temporally-coupled and non-temporally-coupled adversarial settings. Moreover, GRAD's broad applicability extends to diverse attack domains, presenting a more practical and scalable solution for robust RL compared to prior works.

\textbf{Game-Theoretic Reinforcement Learning.}
Superhuman performance in two-player games usually involves two components: the first focuses on finding a model-free blueprint strategy, which is the setting we focus on in this paper. The second component improves this blueprint online via model-based subgame solving and search~\citep{burch2014solving, moravcik2016refining, brown2018depth, brown2020combining, brown2017safe, schmid2021player}. This combination of blueprint strategies with subgame solving has led to state-of-the-art performance in Go~\citep{silver2017mastering}, Poker~\citep{brown2017libratus, brown2018superhuman, moravvcik2017deepstack}, Diplomacy~\citep{gray2020human}, and The Resistance: Avalon~\citep{serrino2019finding}. Methods that only use a blueprint have achieved state-of-the-art performance on Starcraft~\citep{alphastar}, Gran Turismo~\citep{wurman2022outracing}, DouDizhu~\citep{zha2021douzero}, Mahjohng~\citep{li2020suphx}, and Stratego~\citep{mcaleer2020pipeline, perolat2022mastering}. In the rest of this section we focus on other model-free methods for finding blueprints.      

Deep CFR~\citep{deep_cfr, steinberger2019single} is a general method that trains a neural network on a buffer of counterfactual values. However, Deep CFR uses external sampling, which may be impractical for games with a large branching factor, such as Stratego and Barrage Stratego. DREAM~\citep{steinberger2020dream} and ARMAC~\citep{gruslys2020advantage} are model-free regret-based deep learning approaches. ReCFR~\citep{liu2022model} proposes a bootstrap method for estimating cumulative regrets with neural networks. ESCHER~\citep{mcaleer2022escher} removes the importance sampling term of Deep CFR and shows that doing so allows scaling to large games.  

Neural Fictitious Self-Play (NFSP)~\citep{nfsp} approximates fictitious play by progressively training the best response against an average of all past opponent policies using reinforcement learning. The average policy converges to an approximate Nash equilibrium in two-player zero-sum games.   
%but has slower convergence bounds than CFR.  

There is an emerging literature connecting reinforcement learning to game theory. QPG~\citep{srinivasan2018actor} shows that state-conditioned $Q$-values are related to counterfactual values by a reach weighted term summed over all histories in an infostate and proposes an actor-critic algorithm that empirically converges to an NE when the learning rate is annealed. NeuRD~\citep{hennes2020neural}, and F-FoReL~\citep{perolat2021poincare} approximate replicator dynamics and follow the regularized leader, respectively, with policy gradients. Actor Critic Hedge (ACH)~\citep{ach} is similar to NeuRD but uses an information set based value function. All of these policy-gradient methods do not have a theory proving that they converge with high probability in extensive form games when sampling trajectories from the policy. In practice, they often perform worse than NFSP and DREAM on small games but remain promising approaches for scaling to large games \citep{perolat2022mastering}. 