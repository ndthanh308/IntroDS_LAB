% \vspace{-0.5em}
\section{Robustness to Temporally-Coupled Attacks}
% \vspace{-0.5em}
\label{sec:method}
In this section, we first formally define temporally-coupled attacks. Then, we introduce our algorithm, a game-theoretic response approach for adversarial defense against the proposed attacks.
% \vspace{-0.5em}

\subsection{Temporally-coupled Attack}

Robust and adversarial RL methods restrict the power of the adversarial by defining a set of   admissible perturbations: %, as defined in Definition~\ref{def:con1}, which limits the adversary's ability to perturb a state $s_t$ or an action $a_t$ to a predefined set.\looseness=-1
\begin{definition}[$\epsilon$-Admissible Adversary Perturbations]
An adversarial perturbation $p_t$ is considered admissible in the context of a state adversary if, for a given state $s_t$ at timestep $t$, the perturbed state $\tilde{s}_t$ defined as $\tilde{s}_t = s_t + p_t$ satisfies $\left\|s_{t}-\tilde{s}_{t}\right\|\leq\epsilon$, where $\epsilon$ is the state budget constraint. Similarly, if $p_t$ is generated by an action adversary, the perturbed action $\tilde{a}_t$ defined as $\tilde{a}_t = a_t + p_t$ should be under the action constraint of $\left\|a_t-\tilde{a}_{t}\right\|\leq\epsilon$. 
\label{def:con1}
\end{definition}


While the budget constraint $\epsilon$ is commonly applied in prior adversarial attacks, it may not be applicable in many real-world scenarios where the attacker needs to consider the past perturbations when determining the current perturbations. Specifically, in the temporal dimension, perturbations exhibit a certain degree of correlation. To capture this characteristic, we introduce the concept of temporally-coupled attackers. We propose a temporally-coupled constraint as defined in Definition~\ref{def:con2}, which sets specific limitations on the perturbation at the current timestep based on the previous timestep's perturbation. 

\begin{definition}[$\bar{\epsilon}$-Temporally-coupled Perturbations]
\label{def:con2}
A temporally-coupled state perturbation $p_t$ is deemed acceptable if it satisfies the temporally-coupled constraint $\bar{\epsilon}$: $\left\|s_{t}-\tilde{s}_t-(s_{t+1}-\tilde{s}_{t+1}) \right\|\leq\bar{\epsilon}$ 
where $\tilde{s}_t$ and $\tilde{s}_{t+1}$ are the perturbed states obtained by adding $p_t$ and $p_{t+1}$ to $s_t$ and $s_{t+1}$, respectively. For action adversaries, the temporally-coupled constraint $\bar{\epsilon}$ is similarly denoted as $\left\|a_{t}-\tilde{a}_t-(a_{t+1}-\tilde{a}_{t+1}) \right\|\leq\bar{\epsilon}$, where $\tilde{a}_t$ and $\tilde{a}_{t+1}$ are the perturbed actions. 
\end{definition}

\begin{wrapfigure}{r}{0.45\textwidth}
    \vspace{-2em}
    \centering
    % Figure removed
    \vspace{-1.5em}
    \caption{\small{\textit{Standard} perturbations and \textit{temporally-coupled} perturbations} in a 2d example.}
    % $\pi_\theta$ is the acting policy being trained by $\loss_{\pi_\theta}$ defined in Equation~\eqref{loss:policy}, including the original loss of the base DRL algorithm $\lossrl$, a regularization term $\lossreg$, as well as $\lossworst$, a term for improving the \worstqname based on $\worstcritic$. Here $\worstcritic$ which estimates the \worstqname of $\pi_\theta$ is updated by $\loss_{\worstcritic}=\lossest$ depending on $\pi_\theta$.  
    \label{fig:dia}
\vspace{-1em}
\end{wrapfigure}
We illustrate this definition in Fig.~\ref{fig:dia}.
When an adversary is subjected to both of these constraints, it is referred to as a temporally-coupled adversary in this paper. For a temporally-coupled adversary, each timestep's perturbation is restricted within a certain range $\epsilon$, similar to other regular adversarial attacks. However, it is further confined within a smaller range $\bar{\epsilon}$ based on the previous timestep's perturbation. This design offers two significant benefits.

Firstly, it enables the adversary to consider the temporal coupling between perturbations over time. By constraining the perturbations to a smaller range and discouraging drastic changes in direction, the adversary can launch continuous and stronger attacks while preserving a certain degree of stability. Intuitively, if the adversary consistently attacks in one direction, it can be more challenging for the victim to preserve balance and defend effectively compared to when the perturbations alternate between the left and right directions.

Then, the temporally-coupled constraint also enables the adversary to efficiently discover the optimal attack strategy by narrowing down the range of choices for each timestep's perturbation. Reducing the search space does not necessarily weaken the adversary; in fact, it can potentially make the adversary stronger if the optimal attack lies within the temporally-determined search space, which is supported by our empirical results. By constraining the adversary to a more focused exploration of attack strategies, the temporally-coupled constraint facilitates the discovery and exploitation of more effective and targeted adversarial tactics that exhibit less variation at consecutive timesteps. This characteristic enhances the adversary's ability to launch consistent and potent attacks.

Practically, it is crucial to carefully determine $\bar{\epsilon}$ to guarantee that this additional temporally-coupled constraint does not impede the performance of attacks but rather amplifies their effectiveness. The effectiveness of different choices for $\bar{\epsilon}$ was empirically evaluated in our empirical studies, highlighting the benefits it brings to adversarial learning. Temporally-coupled perturbations represent a novel case to challenge the existing methods. Our empirical experiments demonstrate that, even with the introduction of temporally-coupled constraints, these perturbations can have a notable impact on existing robust models, showcasing the need for addressing such scenarios in robust RL.

% \vspace{-0.5em}
\subsection{GRAD: Game-Theoretic Approach for Adversarial Defense}
% Existing works primarily focus on the non-temporally-coupled assumption and thus may not be suitable in many real-world scenarios, but
Building upon prior robust RL methods, we develop a robust RL algorithm for the temporally-coupled setting. Our resulting method uses tools from game theory to enhance robustness against adversaries with different settings including non-temporally-coupled and temporally-coupled constraints.

In our \oursfull(\ours) framework as a modification of PSRO~\citep{psro}, an agent and a temporally-coupled adversary are trained as part of a two-player game. They play against each other and update their policies in response to each other's policies. The adversary is modeled as a separate agent who attempts to maximize the impact of attacks on the original agent's performance and whose action space is constrained by both $\epsilon$ and $\bar{\epsilon}$. 
% \fh{Consequently, \ours naturally considers the new temporally-coupled constraint when calculating the best response, while existing robust RL approaches such as \cite{liang2022efficient} cannot adapt to these additional temporally-coupled constraints and instead rely on assumption that the adversary is not constrained \fhc{not constrained can cause confusion (they may think it doesn't have epsilon bound), maybe just delete this part.}.}
% Note that existing robust RL approaches such as \cite{liang2022efficient} heavily rely on the $\epsilon$-budget assumption, while the temporally-coupled constraints or other types of attack constraints are not considered or addressed.
Our method adapts the $\epsilon$-budget assumption from prior work~\citep{liang2022efficient} to handle temporally-coupled constraints.
% In contrast, \ours considers both the traditional $\epsilon$-budget constraint and the new temporally-coupled constraint when calculating the best response.
% \fhc{Talk about how the time-couple constraint is taken care of by GRAD?}
% \fh{\ours constructs the policy set of each agent in the zero-sum game through computing the best response to the opponent's current NE policy. Therefore, it naturally considers the new temporally-coupled constraint when calculating the best response.}
Meanwhile, the original agent's objective function is based on the reward obtained from the environment, taking into account the perturbations imposed by the adversary. The process continues until an approximate equilibrium is reached, at which point the original agent is considered to be robust to the attacks learned by the adversary. We show our full algorithm in Algorithm~\ref{alg:ours}. 


Under some assumptions (see Appendix~\ref{app:proof} for details), \ours convergence to an approximate Nash Equilibrium (NE):
\begin{proposition}
For a finite-horizon MDP with a fixed number of discrete actions, \ours converges to an approximate Nash Equilibrium (NE) of the two-player zero-sum adversarial game.
\label{nash}
\end{proposition}

In \ours, both the agent and the adversary have two policy sets. During each training epoch, the agent aims to find an approximate best response to the fixed adversary, and vice versa for the adversary. This iterative process promotes the emergence of stable and robust policies. After each epoch, the new trained policies are added to the respective policy sets, which allows for a more thorough exploration of the policy space. 
% \ours has the capability to continuously explore and learn new policies that are not present in the current policy set, thereby enabling ongoing improvement for both the agent and the adversary, which allows for a more thorough exploration of the policy space. 

For different types of attackers, the agent generates different trajectories while training against a fixed attacker. If the attacker only targets the state, then the agent's training data will consist of the altered state $\hat{s}$ after adding the perturbations from the fixed attacker. If the attacker targets the agent's action, the agent's policy output $a$ will be altered as $\hat{a}$ by the attacker, even if the agent receives the correct state $s$ during training. As for the adversary's training, after defining the adversary's attack method and policy model, the adversary applies attacks to the fixed agent and collects the trajectory data and the negative reward to train the adversary.
The novelty of GRAD lies in its scalability and adaptability in robust RL which converges to the approximate equilibrium without only considering certain adversarial scenarios. Our work formulates the robust RL objective as a zero-sum game and demonstrates the efficacy of game-theoretic RL in tackling this objective, rather than being solely reliant on the specific game-theoretic RL algorithm of PSRO or focusing on defense against specific types of attackers.\ours provides a more versatile and adaptive solution.

As in Definition~\ref{def:con2}, which is actually a generalization of the original attack space. We have $\|s_t-\tilde{s_t}-(s_{t+1}-\tilde{s}_{t+1})\|\leq  \|s_t-\tilde{s}_t\| + \|s_{t+1}-\tilde{s}_{t+1}\| \leq 2\epsilon$, so when $\bar{\epsilon} > 2\epsilon$, it converges to the non-coupled attack scenario. Consequently, our defense strategy is not specific to a narrow attack set
In the next section, we empirically demonstrate that our approach exhibits superior and comprehensive robustness, which is capable of adapting to various attack scenarios and effectively countering different types of adversaries on continuous control tasks. 
\input{algorithms/GRAD}

