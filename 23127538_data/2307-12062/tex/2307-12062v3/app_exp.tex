\section{Experiment Details and Additional Results}
\label{app:exp}

\subsection{Implementation details}
\label{app:exp:imp}
We provide detailed implementation information for our proposed method (\ours) and baselines.

\textbf{Training Steps}\quad
For \ours, we specify the number of training steps required for different environments. In the Hopper, Walker2d, and Halfcheetah environments, we train for 10 million steps. In the Ant and Humanoid environments, we extend the training duration to 20 million steps. For the ATLA baselines, we train for 2 million steps and 10 million steps in environments of varying difficulty.

\textbf{Network Structure}\quad
Our algorithm (\ours) adopts the same PPO network structure as the ATLA baselines to maintain consistency. The network comprises a single-layer LSTM with 64 hidden neurons. Additionally, an input embedding layer is employed to project the state dimension to 64, and an output layer is used to project 64 to the output dimension. Both the agents and the adversaries use the same policy and value networks to facilitate training and evaluation. Furthermore, the network architecture for the best response and meta Nash remains consistent with the aforementioned configuration.

\textbf{Schedule of $\epsilon$ and $\bar{\epsilon}$}\quad
During the training process, we gradually increase the values of $\epsilon$ and $\bar{\epsilon}$ from 0 to their respective target maximum values. This incremental adjustment occurs over the first half of the training steps. We reference the attack budget $\epsilon$ used in other baselines for the corresponding environments. This ensures consistency and allows for a fair comparison with existing methods. The target value of $\bar{\epsilon}$ is determined based on the adversary's training results, which is set as $\epsilon/5$. In some smaller dimensional environments, $\bar{\epsilon}$ can be set to $\epsilon/10$. We have observed that the final performance of the trained robust models does not differ by more than 5\% when using these values for $\bar{\epsilon}$.

\textbf{Observation and Reward Normalization}\quad
To ensure consistency with PPO implementation and maintain comparability across different codebases, we apply observation and reward normalization. Normalization helps to standardize the input observations and rewards, enhancing the stability and convergence of the training process. We have verified the performance of vanilla PPO on different implementations, and the results align closely with our implementation of \ours based on Ray rllib.

\textbf{Hyperparameter Selection}\quad
Hyperparameters such as learning rate, entropy bonus coefficient, and other PPO-specific parameters are crucial for achieving optimal performance. Referring to the results obtained from vanilla PPO and the ATLA baselines as references, a small-scale grid search is conducted to fine-tune the hyperparameters specific to \ours. Because of the significant training time and cost associated with \ours, we initially perform a simplified parameter selection using the Inverted Pendulum as a test environment.

\subsection{Adversaries in experiments}
\textbf{State Adversaries}\quad
Aimed to introduce the attack methods utilized during training and testing in our experiments. When it comes to state adversaries, PA-AD as Alogrithm~\ref{alg:pa-ad} stands out as the strongest attack compared to other state attacks. Therefore, we report the best state attack rewards under PA-AD attacks.

\textbf{Action Adversaries}\quad
In terms of action adversaries, an RL-based action adversary as Alogrithm~\ref{alg:ac-ad} can inflict more severe damage on agents' rewards compared to OU noise and parameter noise in~\citep{tessler2019action}.

\input{algorithms/AC-AD}

\textbf{Mixed Adversaries}\quad
When dealing with mixed adversaries capable of perturbing both state and action spaces, it becomes crucial to design the action space for the adversary. In Algorithm~\ref{alg:mixed-ad}, we extend the idea of PA-AD~\citep{sun2021strongest}, which learns a policy perturbation direction to generate perturbations. In our case, the mixed adversary director only needs to learn the policy perturbation direction $\hat{d}_t$. For various attack domains, the actor functions then translate the direction $\hat{d}_t$ into state or action perturbations. This design approach ensures that our mixed adversary doesn't increase the complexity of adversary training, as it deploys mixed perturbations using different actor functions as required by distinct attack domains.
\label{app:exp:alg}
\input{algorithms/PA-AD}
\input{algorithms/mixed-AD}

\vspace{1em}
\textbf{Transition Adversaries. }
In addition to addressing adversarial perturbations, we extend the evaluation of \ours to consider transition uncertainty, mitigating the mismatch problem between the training simulator and the testing environment. Robustness under transition uncertainty is crucial for real-world applicability. To assess this aspect, experiments are conducted on perturbed MuJoCo environments (Hopper, Walker2d, and HalfCheetah) by modifying their physical parameters ('leg\_joint\_stiffness' value: 30, 'foot\_joint\_stiffness' value: 30, and bound on 'back\_actuator\_range': 0.5) following the protocol established by \cite{zhou2023natural}. Comparative evaluations are performed against robust natural actor-critic (RNAC)\citep{zhou2023natural} trained with Double-Sampling (DS) and Inaccurate Parameter Models (IPM) uncertainty. The results presented in Table~\ref{tab:transition} consistently demonstrate that \ours achieves competitive or superior performance compared to baseline methods in each perturbed environment, showcasing its effectiveness in robustly handling transition uncertainty.

\begin{table}[!t]
\vspace{-0.5em}
\centering
\renewcommand{\arraystretch}{1.3}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{4pt}
  \centering
  \begin{tabular}{p{2.5cm}<{\centering} p{3.5cm}<{\centering} p{2.5cm}<{\centering} p{2.5cm}<{\centering} p{2.5cm}<{\centering}}
    \toprule
    Perturbed Environments & & RNAC-PPO (DS) & RNAC-PPO (IPM) & \textbf{GRAD} \\
    \midrule
    \multirow{2}{*}{Hopper} & Natural reward& \textbf{$3502 \pm 256$} & $3254 \pm 138$ & $3482 \pm 209$ \\
     & 'leg\_joint\_stiffness' & $2359 \pm 182$ & $2289 \pm 124$ & \textbf{$2692 \pm 236$} \\
    \midrule
    \multirow{2}{*}{Walker} & Natural reward & $4322 \pm 289$ & $4248 \pm 89$ & \textbf{$4359 \pm 141$} \\
    & 'foot\_joint\_stiffness' & $4078 \pm 297$ & $4129 \pm 78$ & \textbf{$4204 \pm 132$} \\
    \midrule
    \multirow{2}{*}{Halfcheetah} & Natural reward  & $5524 \pm 178$ & $5569 \pm 232$ & \textbf{$6047 \pm 241$} \\
    & 'back\_actuator\_range'& $768 \pm 102$ & $1143 \pm 45$ & \textbf{$1369 \pm 117$} \\
    \bottomrule
  \end{tabular}}
\caption{Comparison of cumulative reward in Perturbed Environments with changed physical parameters.}
\vspace{-0.5em}
\label{tab:transition}
\end{table}


\textbf{Short-term Memorized Temporall-coupled Attacks. }
While our temporally-coupled setting considering perturbation from the last time step aligns with the common practice of state adversaries, which typically perturb the current state without explicitly attacking short-term memory, we recognized the importance of exploring a more general scenario akin to a general partially observable MDP~\citep{efroni22provable}. We introduced a short-term memorized temporally-coupled attacker by calculating the mean of perturbations from the past 10 steps and applying the temporally-coupled constraint to this mean.
The results in Table~\ref{tab:memorized} from these additional experiments against short-term memorized temporally-coupled attacks underscore the efficacy of GRAD under this extended setting. GRAD consistently demonstrates heightened robustness compared to other robust baselines when confronted with a memorized temporally-coupled adversary. These findings provide valuable insights into the temporal scope of perturbations, contributing to a more comprehensive understanding of GRAD's capabilities in handling diverse adversarial scenarios.

\begin{table}[!t]
\vspace{-0.5em}
\centering
\renewcommand{\arraystretch}{1.4}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{4pt}
\begin{tabular}{p{4cm}<{\centering} p{2.5cm}<{\centering} p{2.5cm}<{\centering} p{2.5cm}<{\centering} p{2.5cm}<{\centering} p{2.5cm}<{\centering}}
\toprule
Short-term Memorized Temporally-Coupled Attacks & Hopper & Walker2d & Halfcheetah & Ant & Humanoid \\
\midrule
PA-ATLA-PPO & 2334 $\pm$ 249 & 2137 $\pm$ 258 & 3669 $\pm$ 312 & 2689 $\pm$ 189 & 1573 $\pm$ 232 \\
WocaR-PPO & 2256 $\pm$ 332 & 2619 $\pm$ 198 & 4228 $\pm$ 283 & 3229 $\pm$ 178 & 2017 $\pm$ 213 \\
\textbf{GRAD} &  \textbf{2869 $\pm$ 228} & \textbf{3134 $\pm$ 251} & \textbf{4439 $\pm$ 287} & \textbf{3617 $\pm$ 188} & \textbf{2736 $\pm$ 269} \\
\bottomrule
\end{tabular}}
\caption{Performance Comparison under Memorized Temporally-Coupled Attacks}
\label{tab:memorized}
\vspace{-0.5em}

\end{table}







\subsection{Attack budgets}
\label{app:exp:eps}
In Figure~\ref{fig:eps}, we report the performance of baselines and \ours under different attack budget $\epsilon$. As the value of $\epsilon$ increases, the rewards of robust agents under different types of attacks decrease accordingly. However, our approach consistently demonstrates superior robustness as the attack budget changes.
\input{figures/fig_eps}

\subsection{Temporally-coupled constraints}
We also investigate the impact of temporally-coupled constraints $\bar{\epsilon}$ on attack performance, as we explained in our experiment section.
\input{figures/fig_bar_eps}

\subsection{Natural reward vs. Robustness}
We presents the natural performance comparison of \ours and action robust baselines in Figure~\ref{fig:action_natural}.
\label{app:natural}
% Figure environment removed

\subsection{Computational Cost}
\label{app:efficient}
The training time for \ours can vary depending on the specific environment and its associated difficulty. Typically, on a single V100 GPU, training \ours takes around 20 hours for environments like Hopper, Walker2d, and Halfcheetah. However, for more complex environments like Ant and Humanoid, the training duration extends to approximately 40 hours. It's worth noting that the training time required for defense against state adversaries or action adversaries is relatively similar.