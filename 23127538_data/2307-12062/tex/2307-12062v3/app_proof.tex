\section{Proof of Proposition~\ref{nash}}
\label{app:proof}

\begin{proof}
The \emph{exploitability} \( e(\pi) \) of a strategy profile \( \pi \) is defined as 
\[ e(\pi) = \sum_{i \in \mathcal{N}} \max_{\pi'_i}v_i(\pi'_i, \pi_{-i}). \]
A \emph{best response (BR)} strategy \( \mathbb{BR}_i(\pi_{-i}) \) for player \( i \) to a strategy \( \pi_{-i} \) is a strategy that maximally exploits \( \pi_{-i} \): 
\[ \mathbb{BR}_i(\pi_{-i}) = \arg\max_{\pi_i}v_i(\pi_i, \pi_{-i}). \]
% An \emph{\boldmath$\epsilon$\unboldmath-best response (\boldmath$\epsilon$\unboldmath-BR)} strategy \( \mathbb{BR}^\epsilon_i(\pi_{-i}) \) for player \( i \) to a strategy \( \pi_{-i} \) is a strategy that is at most \( \epsilon \) worse for player \( i \) than the best response:
% \[ v_i(\mathbb{BR}^\epsilon_i(\pi_{-i}), \pi_{-i}) \ge v_i(\mathbb{BR}_i(\pi_{-i}), \pi_{-i}) - \epsilon. \]
An \emph{\boldmath$\epsilon$\unboldmath-Nash equilibrium (\boldmath$\epsilon$\unboldmath-NE)} is a strategy profile \( \pi \) in which, for each player \( i \), \( \pi_i \) is an \( \epsilon \)-BR to \( \pi_{-i} \).

We can define the \emph{approximate exploitability} of the pair of meta-Nash equilibrium strategies for the agent and the adversary \( (\sigma_a, \sigma_v) \) as the sum of the expected reward their opponent approximate best responses achieve against them:
\[ \hat{e}(\sigma_a, \sigma_v) = v_a(\pi_a', \sigma_v) + v_v(\pi_v', \sigma_u), \]
where \( v_i(\pi_i', \sigma_{-i}) \) denotes the expected value of a player's approximate best response vs. the opponent meta-NE. 

Now assume that each approximate best response is within \( \frac{\epsilon}{4} \) of the optimal best response. Then 
\[ v_i(\pi_i', \sigma_{-i}) \ge v_i(\mathbb{BR}_i(\sigma_{-i}), \sigma_{-i}) - \frac{\epsilon}{4}. \]
As a result, upon convergence, when the approximate exploitability $\hat{e}(\sigma_a, \sigma_v)$ is less than $\frac{\epsilon}{2}$, 
then the exploitability of the pair of meta-Nash equilibrium strategies for the agent and the adversary \( (\sigma_a, \sigma_v) \) is less than $\epsilon$, and the pair of strategies are in an $\epsilon$-approximate Nash equilibrium. 

Every epoch where \ours does not converge to an approximate equilibrium, it must add a unique deterministic policy to the population for either the agent or the adversary because if both players added policies already included in their populations, those policies would not be approximate best responses. Given that the MDP has a finite horizon and operates in a discrete action space, there exists only a finite set of deterministic policies that can be added to the populations \( \Pi_a \) and \( \Pi_v \). Since the meta-Nash equilibrium over all possible deterministic policies is equivalent to the Nash equilibrium of the original game, in the worst case where all possible deterministic policies are added, the algorithm will terminate at an approximate Nash equilibrium. \qed
\end{proof}