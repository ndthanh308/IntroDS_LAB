% \vspace{-0.5em}
\section{Experiments}
\label{sec:exp}
% \vspace{-0.5em}
% This paper introduces a novel concept called temporally coupled attacks, which distinguishes itself from standard adversarial attacks by incorporating temporal coupling. Previous research has primarily focused on attackers with different functionalities, specifically targeting either the state space or the action space.
In our experiments, we investigate various types of attackers on different attack domains including state perturbations, action perturbations, model uncertainty and mixed perturbations. We will study a diverse set of attack and compare with state-of-the-art baselines.
% This evaluation sheds light on the effectiveness of \ours across a wide range of attack scenarios against different types of adversaries.

\noindent\textbf{Experiment setup.} \quad
Our experiments are conducted on five various and challenging MuJoCo environments: Hopper, Walker2d, Halfcheetah, Ant, and Humanoid, all using the v2 version of MuJoCo. We use the Proximal Policy Optimization (PPO) algorithm as the policy optimizer for \ours training. For attack constraint $\epsilon$, we use the commonly adopted values $\epsilon$ for each environment. We set the  temporally-coupled constraint $\bar{\epsilon} = \epsilon/5$ (with minor adjustments in some environments). Ablation experiments study the choice ofOther choices of $\bar{\epsilon}$ will be further discussed in the ablation studies.
Our experiments are conducted on five various and challenging MuJoCo environments: Hopper, Walker2d, Halfcheetah, Ant, and Humanoid, all using the v2 version of MuJoCo. We use the Proximal Policy Optimization (PPO) algorithm as the policy optimizer for \ours training. For attack constraint $\epsilon$, we use the commonly adopted values $\epsilon$ for each environment. We set the  temporally-coupled constraint $\bar{\epsilon} = \epsilon/5$ (with minor adjustments in some environments). Ablation experiments study the choice of $\bar{\epsilon}$.

We report the average test episodic rewards both under no attack and against the strongest adversarial attacks to reflect both the natural performance and robustness of trained agents, by training adversaries targeting the trained agents from scratch. For reproducibility, we train each agent configuration with 10 seeds and report the one with the median robust performance, rather than the best one. More implementation details are in Appendix~\ref{app:exp:imp}.
% Figure environment removed
% \vspace{-0.5em}
\paragraph{Case I: Robustness against state perturbations.}
In this experiment, our focus is on evaluating the robustness of our methods against state adversaries that perturb the states received by the agent. Among the alternating training~\citep{zhang2021robust, sun2021strongest} methods, PA-ATLA-PPO is the most robust, which trains with the standard strongest PA-AD attacker. As a modification, we train PA-ATLA-PPO* with a temporally-coupled PA-AD attacker. WocaR-PPO~\citep{liang2022efficient} is the state-of-the-art defense method against state adversaries. Our \ours method utilizes the temporally-coupled PA-AD attacker for training. Figure~\ref{fig:state_attacks} presents the performance of baseline and \ours under both non-temporally-coupled and temporally-coupled state perturbations. 

Despite being trained to handle temporally-coupled adversaries, our method also demonstrates strong performance in the non-robust (``natural'') setting, expecially on the high-dimensional Humanoid task.
% Even without training with a non-temporally-coupled state adversary, our method demonstrates better robustness under the non-temporally-coupled type of attack, particularly in the highest-dimensional and challenging environment, Humanoid, where it outperforms other methods by a large margin.
Under our temporally-coupled attacks, the average performance of \ours is 45\% higher than the strongest baseline.
% \ours shows the best robustness against all types of state adversarial attacks.
% Figure environment removed

% \vspace{-0.5em}
\paragraph{Case II: Robustness against action uncertainty.}
Beyond assessing the susceptibility of \ours to state attacks, we also investigate its robustness against action uncertainty, where the agent intends to execute an action but ultimately takes a different action than anticipated. We scrutinize two specific forms of action uncertainty, as outlined in prior work~\citep{tessler2019action}. The first one is action perturbations, introduced by an action adversary, which strategically adds noise to the agent's intended action. The second scenario revolves around model uncertainty, where, with a probability denoted as $\alpha$, an alternative action replaces the originally planned action output by the agent. These scenarios closely parallel real-world control situations, such as dealing with mass uncertainty (e.g., when a robot's weight changes) or facing sudden, substantial external forces (e.g., when an external force unexpectedly pushes a robot).

In our baseline comparisons, we include PR-MDP and NR-MDP~\citep{tessler2019action}, which are robust to action noise and model uncertainty. We also incorporate WocaR-PPO into our baseline evaluations. We train \ours using a temporally-coupled action adversary and evaluate its robustness in both action perturbation and model uncertainty scenarios.

\noindent\textbf{Action Perturbations.}\quad
To obtain a stronger evasion action perturbation rather than OU noise and parameter noise, we are the first to train an RL-based action adversary following the trajectory outlined in Algorithm~\ref{alg:ours}. This strategy aims to showcase the worst-case performance of our robust agents under action perturbations. For evaluation, we train both temporally-coupled and non-temporally-coupled action adversaries for each robust model. In Figure~\ref{fig:action_attacks}, we present the exceptional performance of \ours against standard and temporally-coupling action perturbations. \ours  demonstrates a high degree of robustness. For example, on the Humanoid task it outperforms the baselines by a 17\% margin for standard attacks and by a 40\% advantage against temporally-coupling action attacks. 
% Across other tasks, \ours  outperforms other methods, especially when confronted with temporally-coupling action attacks, where it exhibits a significant advantage.
These results provide evidence of \ours's defense mechanism against various types of adversarial attacks in the action space.

% Figure environment removed
\noindent\textbf{Model Uncertainty.}\quad
To evaluate robustness under model uncertainty, we consider a range of noise probabilities denoted as $\alpha$ in the range of [0, 0.05, 0.1, 0.15, 0.2]. These values represent the probability of a randomly generated noise replacing the action selected by the victim agent. As depicted in Figure~\ref{fig:uncertainty}, \ours exhibits superior robustness compared to action-robust baselines across a spectrum of $\alpha$ uncertainty value without explicit exposure to model uncertainty noises during training.
% Figure environment removed
\vspace{-0.5em}
\paragraph{Case III: Robustness against mixed adversaries.}
In prior works, adversarial attacks typically focused on perturbing either the agent's observations or introducing noise to the action space. However, in real-world scenarios, agents may encounter both types of attacks simultaneously. To address this challenge, we propose a mixed adversary, which allows the adversary to perturb the agent's state and action at each time step. We employ alternating training to create a baseline as Mixed-ATLA using this mixed adversary type. Our \ours model and Mixed-ATLA are trained with temporally-coupled mixed attackers. The detailed algorithm for the mixed adversary is provided in Appendix~\ref{alg:mixed-ad}.
% We believe that a well-trained mixed adversary not only holds practical significance but also provides a more comprehensive validation of the effectiveness of \ours in enhancing robustness.

Our results in Figure~\ref{fig:mixed_attacks} indicate that the combination of two different forms of attacks can  target robust agents in most scenarios, providing strong evidence of their robustness. \ours outperforms other methods in all five environments against non-temporally-coupled mixed adversaries, with a margin of over 20\% in the Humanoid environment. Moreover, when defending against temporally-coupled mixed attacks, \ours outperforms baselines by  30\% in multiple environments, with a minimum improvement of 10\%.
% These results clearly demonstrate the robustness of \ours against attackers that can target across domains.

\noindent\textbf{Natural Performance.}\quad
We also evaluate the natural performance of \ours and the baselines, as shown in Figure~\ref{fig:state_natural}, which compares natural rewards vs. rewards under the strongest temporally-coupled attacks. It is evident that while achieving robustness, \ours maintains a comparable natural performance with the baselines; the agent's performance does not degrade significantly in environments without adversaries. The natural performance comparing \ours with action-robust models can be found in Appendix~\ref{app:natural}.
% Figure environment removed

%\textbf{Summary.} We calculated the average normalized rewards for each evaluation metric and each robust agent in all the environments as in Figure~\ref{fig:exp}. This visualization vividly showcases that \ours demonstrates notably superior robustness under both standard and temporally-coupled attacks, in comparison to other approaches. Overall, these findings emphasize our empirical potential and contributions of \ours and provide intuitive insights into improving the robustness of agents through a novel and convincing evaluation framework for robust RL.

\begin{wrapfigure}{r}{0.35\textwidth}
% \vspace{-1em}
    \centering
    % Figure removed
    % \vspace{-0.5em}
    \caption{Ablated studies for $\bar{\epsilon}$.
    % $\pi_\theta$ is the acting policy being trained by $\loss_{\pi_\theta}$ defined in Equation~\eqref{loss:policy}, including the original loss of the base DRL algorithm $\lossrl$, a regularization term $\lossreg$, as well as $\lossworst$, a term for improving the \worstqname based on $\worstcritic$. Here $\worstcritic$ which estimates the \worstqname of $\pi_\theta$ is updated by $\loss_{\worstcritic}=\lossest$ depending on $\pi_\theta$.  
    }
    \label{fig:eps_}
% \vspace{-1em}
\end{wrapfigure}
\noindent\textbf{Ablation studies for temporally-coupled constraint $\bar{\epsilon}$.} \quad
As defined in our framework, the temporally-coupled constraint $\bar{\epsilon}$ limits the perturbations within a range that varies between timesteps. When $\bar{\epsilon}$ is set too large, the constraint becomes ineffective, resembling a standard attacker. 
Conversely, setting $\bar{\epsilon}$ close to zero overly restricts perturbations, leading to a decline in attack performance. An appropriate value for $\bar{\epsilon}$ is critical for effective temporally-coupled attacks. Figure~\ref{fig:eps_} illustrates the performance of robust models against temporally-coupled state attackers trained with different maximum $\bar{\epsilon}$. For WocaR-PPO, the temporally-coupled attacker achieves good performance when the values of $\bar{\epsilon}$ are set to 0.02. As the $\bar{\epsilon}$ values increase and the temporally-coupled constraint weakens, the agent's performance improves, indicating a decrease in the adversary's attack effectiveness. In the case of \ours agents, they consistently maintain robust performance as the $\bar{\epsilon}$ values become larger. This observation highlights the impact of temporal coupling on the vulnerability of robust baselines to such attacks. In contrast, \ours agents consistently demonstrate robustness against these attacks. \looseness=-1





