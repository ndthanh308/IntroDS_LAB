
\noindent{\Large \textbf{{Appendix}}}

\section{More Implentation Details}
\begin{algorithm}[h] 
	\caption{Visual Concept Translator} 
	\label{alg::conjugateGradient}
	\begin{algorithmic}[1] 
		\Require 
		$x^{src}$, $x^{ref}$: source and reference image.
            \Require 
            $\alpha$, $\beta$: learning rates; $S_{m}$, $S_{p}$: training steps.
		%\Ensure 
		%optimal $x^{*}$ 
		\State \textbf{Initial} content embedding $v^{src}$ and concept embedding $v^{ref}$.
            \State $\triangleright$ Multi-concept inversion
            \For{step=1,...,$S_{m}$}
            \State Compute $L_{ldm}$ and $L_{rec}$ in Eq.\;(14) and Eq.\;(15);
            \State Update concept embedding with gradient descent: $v^{ref}\longleftarrow v^{ref}-\alpha\bigtriangledown_{v^{ref}}(L_{ldm}+L_{rec})$.
            \EndFor
            \State $\triangleright$ Pivotal turning inversion
            \State Compute source latent $z^{src}=\mathcal{E}(x^{src})$ with encoder $\mathcal{E}$;
            \State Compute $z_{T}=$DDIM-Inversion$(z^{src})$ with Eq.\;(4)
            \State Compute unconditional embedding $v_\varnothing = \tau (\varnothing)$ with tokenizer $\tau$;
            \For{t=T,...,1};
                \For{step=1,...,$S_{p}$}
                \State Compute $L=z_0- \hat{z}_0\left(z_t,v_t^{src}\right)$ in Eq.\;(12);
                \State Update content embedding with gradient descent: $v_t^{src}\longleftarrow v_t^{src}-\alpha\bigtriangledown_{v_t^{src}}L$
                \EndFor
                \State $\hat{\epsilon} \leftarrow \tilde{\varepsilon}_\theta\left(z_t, t, v^{src}_t,v^{\varnothing}\right)$ in Eq.\;(10);
                \State $z_{t-1}\leftarrow \text{DDIM-sample}(z_t,\hat{\epsilon},t)$
            \EndFor
            \State $\triangleright$ Content-concept fusion
            
            \For{t=T,...,1};
            \State Compute noise prediction $\epsilon^{src}$ and  $\epsilon^{ref}$ in Eq.\;(7);
             \State $M_t,\hat{\epsilon}\leftarrow \tilde{\varepsilon}_\theta\left(z_t, t, v^{src}_t,v^{ref}\right)$ in Eq.\;(8);
             \State  $M_t^*,\hat{\epsilon}^*\leftarrow \tilde{\varepsilon}_\theta\left(z^{*}_t, t, v^{src}_t,v^{\varnothing}\right)$ in Eq.\;(10);

            \State $\widehat{M}_t \leftarrow \textit{AC}\left(M_t,M_t^*,t\right)$ in Eq.\;(11)
            \State $\hat{\epsilon}=\tilde{\varepsilon}_\theta\left(z_t, t, v^{src}_t,v^{ref}\right)\{M \leftarrow  \widehat{M}_t\}$
            \State $z_{t-1}\leftarrow \text{DDIM-sample}(z_t,\hat{\epsilon},t)$
            \State $z^*_{t-1}\leftarrow \text{DDIM-sample}(z^*_t,\hat{\epsilon}^*,t)$
            \EndFor
        \State Compute target image $x^{tgt}=\mathcal{D}(z^{0})$ with decoder $\mathcal{D}$; 
    
	\end{algorithmic} 
 \label{algorithm}
\end{algorithm}

Our full algorithm is shown in Algorithm \ref{algorithm}. For multi-concept inversion, we empirically found that 200 training steps are enough for convergence, and this process only takes about 150 seconds. Furthermore, for 
pivotal turning inversion, our found optimal training step is 1000, which takes about 60 seconds. The learning rate is $5\times10^{-4}$ for multi-concept inversion. For pivotal turning inversion, we reduce the learning rate when step increases, as
\begin{equation}
\begin{aligned}
lr = 1\times 10^{-2} \times s / 5000,
\end{aligned}
\end{equation}
where $s$ is the current step numbers.
The algorithm also includes the unconditional embedding $v_\varnothing$, which is extracted by putting empty text to the BERT tokenizer. The Adam optimizer is used for both inversion processes.


\section{More Results of General Image-to-image Translation}
To further verify the model performance in the general image-to-image translation tasks, we make more experiments with different reference images, as shown in Fig.\;\ref{fig:appendix_animal_face}. 

% Figure environment removed

It's noted that there is a trade-off between structural preservation and semantic changes. As shown in Fig.\;\ref{fig:result_balance}, the injection ratio of cross-attention and self-attention affects the result a lot. To obtain the ideal results, we can adjust the attention injection ratio to the optimal value. Empirically, we adopt a low cross-attention injection ratio of about $20\%$, and adjust the self-attention injection ratio to achieve different preservation results.

% Figure environment removed



\section{More Results of Style Transfer}
To further verify the model performance in the style transfer task, we make more experiments with different reference styles, as shown in Fig.\;\ref{fig:style_transfer}. In the figure, the first column contains the reference images, and the first row contains the content images. The model outputs show the excellent performance of the proposed method with content preserved and style transferred.



% Figure environment removed

As a type of style transfer, portrait style transfer tries to substitute the input face with another stylized face. The proposed algorithm shows excellent performance in the task of portrait style as Fig.\;\ref{fig:face_swap}. Given the one-shot input, our model can substitute the face in the reference style with the face in the content image with high quality.

% Figure environment removed


\section{More Ablation Study}
We evaluate the influence of the number of multi-concept embeddings, as described in part of multi-concept inversion (Section 3.4) in the main paper. Given a reference image, we visualize the model performance with different concept embeddings as shown in Fig.\;\ref{fig:ablation_study}. From the figure,  a small embedding number cannot well translate the concepts in the reference image, as in columns 2-3. A too-large embedding number still leads to poor performance with translation failures, as in columns 5-6. We empirically found that using 3 concept embeddings is the best choice, as in column 4.



% Figure environment removed