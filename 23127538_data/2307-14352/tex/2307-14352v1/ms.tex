\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{algorithm,algpseudocode}
\usepackage{algorithmicx}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12241} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{General Image-to-Image Translation with\\ One-Shot Image Guidance}

\author{Bin Cheng\footnotemark[1], Zuhao Liu\footnotemark[1], Yunbo Peng, Yue Lin\\
NetEase Games AI Lab\\
{\tt\small \{chengbin04, liuzuhao, gzpengyunbo, gzlinyue\}@crop.netease.com}}


\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    
    %\captionsetup{type=figure}
    \captionsetup{type=figure}
    % Figure removed
    \captionof{figure}{The image-to-image translation exhibition of the proposed visual concept translator (VCT). For each image group, the upper-left part is the source image, the lower-left part is the reference image, and the right part is the model output (target image).}
    
\end{center}
}]

\ificcvfinal\thispagestyle{empty}\fi

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{The first two authors contributed equally to this work.}


%%%%%%%%% ABSTRACT
\begin{abstract}
Large-scale text-to-image models pre-trained on massive text-image pairs show excellent performance in image synthesis recently. However, image can provide more intuitive visual concepts than plain text. People may ask: how can we integrate the desired visual concept into an existing image, such as our portrait? Current methods are inadequate in meeting this demand as they lack the ability to preserve content or translate visual concepts effectively. Inspired by this, we propose a novel framework named visual concept translator (VCT) with the ability to preserve content in the source image and translate the visual concepts guided by a single reference image. The proposed VCT contains a content-concept inversion (CCI) process to extract contents and concepts, and a content-concept fusion (CCF) process to gather the extracted information to obtain the target image. Given only one reference image, the proposed VCT can complete a wide range of general image-to-image translation tasks with excellent results. Extensive experiments are conducted to prove the superiority and effectiveness of the proposed methods. Codes are available at https://github.com/CrystalNeuro/visual-concept-translator.




   
\end{abstract}


\section{Introduction}



Image-to-image translation (I2I) task aims to learn a conditional generation function that translates images from source to target domain with source content preserved and target concept transferred\cite{richardson2021encoding, wang2018high}. General I2I can complete a wide range of applications without dedicated model design or training from scratch \cite{wang2022pretraining}. 
%High-quality image synthesis has been developed rapidly in recent years due to the application of large-scale image synthesis models \cite{rombach2022high, saharia2022photorealistic, ramesh2022hierarchical}
Traditionally, generative adversarial networks (GAN) or normalizing flow \cite{grover2020alignflow} are mainly applied to I2I tasks \cite{karras2019style, karras2019style, richardson2021encoding, collins2020editing}. However, these methods suffer from the problem of lacking adaptability \cite{su2022dual}. The model trained in one source-target dataset cannot adapt to another one, so they fail to work in the scenario of general I2I. 

Diffusion-based image synthesis has been developed rapidly in recent years due to the application of large-scale models \cite{rombach2022high, saharia2022photorealistic, ramesh2022hierarchical}. Their strength is using a large number of image-text pairs for model training, so diverse images can be generated by sampling in the latent space guided by a specific text prompt. 
However, in our daily life, we accept massive visual signals containing abundant visual concepts. These visual concepts are difficult to describe in plain text just as the adage "A picture is worth a thousand words".
In addition, I2I guided by reference images has wide applications including game production, artistic creation, and virtual reality. Therefore, research on image-guided I2I contains great potential in the computer vision community. 

Several works try to extract visual information from images with the desired concepts. Specifically, \cite{gal2022image} proposes a technique named textual inversion (TI) which freezes the model and learns a text embedding to represent the visual concepts. On the basis of TI, DreamBooth \cite{ruiz2022dreambooth} and Imagic \cite{kawar2022imagic} are proposed to alleviate overfitting caused by model fine-tuning.
The above methods are under the few-shot setting but sometimes collecting several related images containing the same concept is difficult. To address this problem, \cite{dong2022dreamartist} proposes to use both positive and negative text embedding to fit the one-shot setting. However, these methods cannot be directly used in I2I tasks because they cannot preserve the content in the source image. 

In order to preserve the source contents, the recent proposed DDIM inversion \cite{dhariwal2021diffusion, song2020denoising} finds out the deterministic noise along the reverse direction of the diffusion backward process. Then, some studies\cite{mokady2022null, hertz2022prompt} further apply and improve the DDIM inversion to text-guided image editing. However, these methods are text-conditional so they fail to understand the visual concepts from reference images.
Alternately, some works \cite{yang2022paint, su2022dual} try to connect the source and target domain with image condition, but their models are task-specific so they cannot be used in general I2I.

In this paper, to complete the general I2I tasks guided by reference images, we propose a novel framework named visual concept translator (VCT) with the ability to preserve content in the source image and translate the visual concepts with a single reference image. The proposed VCT solves the image-guided I2I by two processes named content-concept inversion (CCI) and content-concept fusion (CCF). The CCI process extracts contents and concepts from source  and reference images through pivot turning inversion and multi-concept inversion, The CCF process employs a dual-stream denoising architecture to gather the extracted information to obtain the target image. Given only one reference image, the proposed VCT can complete a wide range of general image-to-image translation tasks with excellent results.
Extensive experiments including massive tasks of general I2I and style transfer are conducted for model evaluation.

In summary, our contributions are as follow

(1) We propose a novel framework named visual concept translator (VCT). Given only a single reference image, VCT can complete the general I2I tasks with the ability to preserve content in the source image and translate the visual concepts.

(2) We propose a content-concept inversion (CCI) to extract contents and concepts with pivot turning inversion and multi-concept inversion. We also propose a content-concept fusion (CCF) process to gather the extracted information with a dual-stream denoising architecture.

(3) Extensive experiments including massive tasks of general I2I and style transfer are conducted for model evaluation. The generation results show high superiority and effectiveness of the proposed methods.


% Figure environment removed



%%%%%%%%% BODY TEXT
\section{Related Works}
\subsection{Image-to-image Translation}
 The I2I aims to translate an image from the source domain to the target domain. The current I2I paradigms are mostly based on GANs \cite{baek2021rethinking, liu2021smoothing, gabbay2021scaling, zhang2020cross, zheng2021spatially, yang2022unsupervised, zhu2017unpaired}. 
 However, these methods suffer from the problem of lacking adaptability \cite{su2022dual}. The model trained in one source-target dataset cannot adapt to another one. In addition, large training images are always required for these methods.

 The TuiGAN proposed by Lin et al. \cite{lin2020tuigan} can achieve translation with only one image pair, but their method necessitates retraining the whole network for each input pair, which is very time-consuming. 

 One specific type of I2I named image style transfer tries to transform the image style from source to target.
The seminal work of Gatys et al. \cite{gatys2015neural} shows that artistic images can be generated by separating content and style with deep neural network. Then, to realize real-time style transfer, Johnson et al. \cite{johnson2016perceptual} train a feed-forward network to handle the optimization problem mentioned by Gatys et al. Many works \cite{wu2018direction, ulyanov2016texture, ulyanov2017improved, li2016precomputed, jing2018stroke, kotovenko2019content} are categorized into per-style-per-model where the trained model can only fit one specific style. In order to increase the model flexibility, arbitrary style transfer is realized by many studies \cite{huang2017arbitrary, park2019arbitrary, jing2020dynamic, deng2020arbitrary, liu2021adaattn, sheng2018avatar, wu2020efanet} where only single forward pass is needed for any input style image. However, these methods fail to generalize to general I2I tasks such as face swap because they lacks the ability to process fine-grained information.


\subsection{Diffusion-based Image Synthesis}
Large-scale diffusion models conditioned on the plain text have shown good performance in high-resolution image syntheses recently, such as Stable Diffusion \cite{rombach2022high}, Imagen \cite{saharia2022photorealistic} and DALL-E 2 \cite{ramesh2022hierarchical}. The large text-image models \cite{devlin2018bert, radford2021learning} are used by these methods to achieve text-guided synthesis.
However, the text used to generate the target images is sometimes unavailable, so the inversion technique is used by many works \cite{gal2022image, ruiz2022dreambooth, kawar2022imagic} to learn a text embedding to guide the pre-trained large-scale diffusion models. To achieve translation of images from the source to the target domain, DDIM inversion \cite{dhariwal2021diffusion, song2020denoising} finds out the deterministic noise vector with text condition along the reverse direction of the backward process, but the method is guided by text only. Our proposed method tries to handle the above drawbacks and fuses the abundant visual concepts from image to complete the general I2I tasks.


\section{Methods}



\subsection{Preliminaries}

\noindent {\bf Latent Diffusion Models}. Diffusion models are probabilistic generative models in which an image $x_0$ is generated by progressively removing noise from an initial Gaussian noise image $x_T \sim \mathcal{N}\left(0,\mathbf{I}\right)$ in the sequence of  $x_T,x_{T-1},...,x_1,x_0$.

With the remarkable capacity of image generation, the
Latent Diffusion Model (LDM) \cite{rombach2022high} is utilized as our model backbone. Different from the conventional diffusion models that perform denoising operations directly in the image space, LDM conducts the process in the latent space with an autoencoder.

    Specifically, an input image x is encoded into the latent space by the autoencoder $z=\mathcal{E}(x), \hat{x}=\mathcal{D}(z)$ (with an encoder $\mathcal{E}$ and a decoder $\mathcal{D}$) pre-trained with a large number of images. Then, the denoising process is achieved by training a neural network $\epsilon_{\theta}\left(z_t,t,v\right)$ that predicts the added noise, following the objective:
\begin{equation}
\min _\theta E_{z_0, \epsilon \sim \mathcal{N}(0, I), t \sim \operatorname{U}(1, T)}\left\|\epsilon-\varepsilon_\theta\left(z_t, t, v\right)\right\|_2^2.
\end{equation}
Note that $v$ is the text embedding generated from the text condition and $z_t$ is the noisy latent in timestamp $t$. 
$z_t$ is generated by adding noise to the sampled data $z_0$ as
\begin{equation}
z_t = \sqrt{\alpha _t} z_0 + \sqrt{1-\alpha _t} \epsilon,
\label{eq:add_noise}
\end{equation}
with $0=\alpha _t<\alpha _{t-1}<...<\alpha _0 =1$, which are hyperparameters of the diffusion schedule, and $\epsilon \sim \mathcal{N} \left(0, I\right)$.

The text embedding $v$ is obtained by $v=\tau\left(y\right)$ where $\tau$ is a BERT \cite{devlin2018bert} tokenizer and $y$ is a text prompt.
The tokenizer $\tau$ converts each word or sub-word in an input string to a token, which is an index in specific pre-defined dictionary. Each token is then linked to a unique embedding vector that can be retrieved through an index-based lookup.

\noindent {\bf Texture inversion}. Textual Inversion (TI) \cite{gal2022image} is a novel technique for introducing a new concept in a pre-trained text conditional generative model by learning an embedding $e^*$  as pseudo-words $S^*$. With a small collection of images $X$, TI do so by solving the following optimization problem:
\begin{equation}
\min _e E_{x~\sim\mathcal{U}_X}E_{z_t\sim q\left(z_t \mid x\right)}\left\|\epsilon-\hat\varepsilon_\theta\left(z_t, t, \tau\left(y,S^*\right)\right)\right\|_2^2.
\end{equation}
As such, it motivates the learned embedding $e^*$ to capture fine visual details unique to the concept in a coarse level.

\noindent {\bf DDIM inversion}. Inversion entails finding a noise
map $z_t$ that reconstructs the input latent code $z_0$ upon sampling. A simple inversion technique was suggested for the DDIM sampling \cite{dhariwal2021diffusion,song2020denoising}, based on the
assumption that the ODE process can be reversed in the limit of small steps:
\begin{equation}
z_{t+1}=\sqrt{\bar{\alpha}_{t+1}} f_\theta\left(z_t, t, v\right)+\sqrt{1-\bar{\alpha}_{t+1}} \varepsilon_\theta\left(z_t, t, v\right).
\label{eq:ddiminversion}
\end{equation}
where $z_{t}$ is noised latent code at timestep $t$, $\bar{\alpha}_{t+1}$ is noise scaling factor as defined in DDIM\cite{dhariwal2021diffusion}, and $f_\theta\left(z_t, t, v\right)$ predicts the final denoised latent code $z_0$.
\begin{equation}
f_\theta\left(x_t, t, c\right)=\frac{x_t-\sqrt{1-\bar{\alpha}_t} \epsilon_\theta\left(x_t, t, c\right)}{\sqrt{\bar{\alpha}_t}}
\end{equation}
In other words, the diffusion process is performed in the reverse direction, that is $z_0 \rightarrow z_T$ instead of $z_T \rightarrow z_0$, where $z_0$ is set to be the encoding of the given real image.

\noindent {\bf Classifier-free guidance}.  The diffusion model may ignore the conditional input and produce results uncorrelated with this input. One way to address this is the classifier-free guidance\cite{ho2021classifier}. During denoising step, with a guidance scale $w \geq 1$, the classifier-free guidance prediction is defined by:
\begin{equation}
\tilde{\varepsilon}_\theta\left(z_t, t, v\right)=w \cdot \varepsilon_\theta\left(z_t, t, v\right)+(1-w) \cdot \varepsilon_\theta\left(z_t, t, v_\varnothing\right) .
\end{equation}
where $v_\varnothing$ represents the the embedding of a null text.
\subsection{Overall Framework}

Given a source image $x^{src}$ and a reference image $x^{ref}$, the goal of VCT is to generate a new image $x^{tgt}$ that complies with $x^{ref}$ while preserves the structure and semantic layout of $x^{src}$. 

Fig. \ref{fig:overall_framework} shows the overall framework of the proposed VCT including a content-concept inversion (CCI) process and a content-concept fusion (CCF) process. As shown in Fig. \ref{fig:overall_framework} (a), the CCI process extracts contents and concepts from source image $x^{src}$ and reference image $x^{ref}$ into learnable embeddings. Then in Fig. \ref{fig:overall_framework} (b), the CCF process employs a dual-stream denoising architecture including a main branch $\mathcal{B}$ and a content matching branch $\mathcal{B^*}$, and both branches starts from the same initial noise inverted by $x^{src}$. The content matching branch reconstructs the source image and extracts the attention maps to guide the main process by the attention control mechanism. Then, the main branch gathers all the information to obtain a target image $x^{tgt}$. For better understanding, we first explain the CCF process in Section \ref{sec:fusion}, then we describe the CCI process in Section \ref{sec:inversion}.

% Figure environment removed

% Figure environment removed

\subsection{Content-concept Fusion}
\label{sec:fusion}
\noindent {\bf $\epsilon$ Space Fusion}. Given two different text embedding $v^{src}$ and  $v^{ref}$, they can be guided separately and yield two different noise prediction $\epsilon^{src}$ and $\epsilon^{ref}$:
\begin{equation}
\begin{aligned}
\epsilon^{src}=\varepsilon_\theta\left(z_t, t, v^{src}\right),\epsilon^{ref}=\varepsilon_\theta\left(z_t, t, v^{ref}\right)
.
\end{aligned}
\end{equation}
We call this space \textit{$\epsilon$ space}.

According to the conclusion stated by classifier
guidance\cite{dhariwal2021diffusion} and classifier-free guidance\cite{ho2021classifier}, the noise prediction in $\epsilon$ space in each diffusion step can be interpreted as score estimation function $\varepsilon_\theta\left(z_t, t, v\right) \approx -\sigma_t \nabla_{z_t} \log p\left({z}_t\mid{v}\right)$, where $\nabla_{z_t} \log p\left({z}_t\mid{v}\right)$ represents the gradient of log likelihood of an implicit classifier $p\left({v}\mid{z}_t\right)\propto p\left({z}_t \mid v\right) / p\left({z}_t\right).$

Under the score estimation function view of $\epsilon$ space, the independent feature  $v^{src}$ and $v^{ref}$ can be fused in the $\epsilon$ space to generate images containing certain attributes from both the source image and the reference image:
\begin{equation}
\tilde{\varepsilon}_\theta\left(z_t, t, v^{src},v^{ref}\right)=w \cdot \epsilon^{src} +(1-w) \cdot \epsilon^{ref} .
\label{eq:epsilon_fusion}
\end{equation}
$w$ is the hyperparameter that balances the two terms. It's noted that the classifier-free guidance is a special case of Eq. \ref{eq:epsilon_fusion}.
\\
\noindent {\bf Dual stream denoising architecture}. Based on the $\epsilon$ fusion mechanism, we now turn to the image translation task. As shown in Fig.\ref{fig:content_matching_branch}, let $x^T$ be the initial noise, obtained by inverting $x^{src}$ using DDIM inversion with Eq. \ref{eq:ddiminversion}, where we set $v=v_\varnothing$.
Starting from the same initial noise $x^T$, we employ a dual-stream denoising architecture for I2I, denoted as a main branch $\mathcal{B}$ and a content matching branch $\mathcal{B^*}$.

The content matching branch $\mathcal{B^*}$ is a denoising process that perfectly reconstructs the source image $x^{src}$ (with $z^{src}$ perfectly reconstructed in latent space for LDMs), and the main branch $\mathcal{B}$ is the denoising process that finally serves for the I2I tasks. 
\begin{equation}
\begin{aligned}
\mathcal{B^*}&:z_T \rightarrow z_{T-1}^* \rightarrow... \rightarrow z_1^* \rightarrow z^{src} \\
\mathcal{B}&:z_T \rightarrow z_{T-1} \rightarrow... \rightarrow z_1\rightarrow z^{tgt} .
\end{aligned}
\end{equation}

At each denoising step $t$, the content matching branch $\mathcal{B^*}$ aims to extract the text embedding $v_t^{src}$ and the attention map $M_t^*$, which would serve for the parallel denoising step in the main branch. With $\mathcal{B^*}$,  we obtain meaningful embedding and generated structure of the source image.

To better inject the information of the source image $x^{src}$, the dual stream diffusion processes have almost the same computation pipelines, except for the reference embeddings used in $\epsilon$ space fusion. We perform  $\epsilon$ space fusion in the content matching branch as the main branch by:
\begin{equation}
\tilde{\varepsilon}_\theta\left(z_t, t, v^{src},v_\varnothing\right)=w \cdot \epsilon^{src} +(1-w) \cdot \epsilon^{\varnothing} .
\label{eq:cfg_fusion}
\end{equation}
The above sampling procedure reduces to the classifier-free guidance. And we should ensure that Eq. \ref{eq:epsilon_fusion} and Eq. \ref{eq:cfg_fusion} have a same $w$ for the dual-stream diffusion architecture.


\noindent {\bf Attention control}. Recent large-scale diffusion models \cite{rombach2022high,saharia2022photorealistic,ramesh2022hierarchical} incorporate conditioning by augmenting the denoising network $\varepsilon_{\theta}$ with self-attension layer and cross-attention layer \cite{bahdanau2014neural,vaswani2017attention}. Of particular interest is the \textit{cross-attention map} and \textit{self-attention map}, donated as $M$ in total, which is observed to have a tight relation with the structure of the image \cite{hertz2022prompt}. To this end, Amir at al.\cite{hertz2022prompt} pursue \textit{prompt-to-prompt} editing framework for text-guided image translation task, which controls the attention maps of the edited image by injecting the attention maps of the original image along the diffusion process. 

In our case, we employ soft attention control as described in \textit{prompt-to-prompt} \cite{hertz2022prompt}. Let $M_t^*$ be the attention map of a single step $t$ of the content matching branch, and $M_t$ be the attention map of the main branch. The soft attention control is defined as:
\begin{equation}
\textit{AC}\left(M_t,M_t^*,t\right) = \begin{cases}M_t^* & \text { if } t<\tau \\ M_t & \text { otherwise }\end{cases}
\end{equation}
where $\tau$ is a timestamp parameter that determines until which step the attention map replacement is applied. We denoted by $\tilde{\varepsilon}_\theta\left(z_t, t, v^{src},v^{ref}\right)\{M \leftarrow  \widehat{M}\}$ where we override the attention map $M$ in $\tilde{\varepsilon}$ with additional given map $\widehat{M}$.

\subsection{Content-concept Inversion}
\label{sec:inversion}

\noindent {\bf Pivotal turning inversion} is proposed to generate the content embedding to guide the CCF process.  We start by studying the DDIM inversion\cite{dhariwal2021diffusion,song2020denoising}. In pratice , a slight error is incorporated in every step. For unconditional diffusion models, the accumulated error is negligible and the DDIM inversion succeeds. However, recall that meaningful editing using the Stable Diffusion model \cite{rombach2022high} requires applying classifier-free guidance with a guidance scale $w$. Ron et al.\cite{mokady2022null} have present that such a guidance scale amplifies the accumulated error. 

To this end, Ron et al. \cite{mokady2022null} introduce null-text inversion technology to reconstruct the image and further for text-guided image translation task. Null-text inversion modifys the unconditional embedding in each timestamp $t$ that is used for classifier-free guidance to matching the initial conditional DDIM inversion trajectory. 

In our image-guided case, we do not know the exact text prompt of the source image $x^{src}$. So, inspired by \cite{mokady2022null}, we implement unconditional DDIM inversion, and optimize the source embedding $v_t^{src}$ in each timestamp $t$ for accurately matching the source image $x^{src}$, instead of the DDIM inversion trajectory.

In each timestamp $t$, we optimize the $v_t^{src}$ by:
\begin{equation}
\min _{v_t^{src}} \left \| z_0- \hat{z}_0\left(z_t,v_t^{src}\right) \right\|^2_2
\end{equation}
where $\hat{z}_0\left(z_t,v_t^{src}\right)$ refers to the estimated clean latent $\hat{z}_0$ given $z_t$ and $v_t^{src}$, using the Tweedie’s formula \cite{kim2021noise2score}. We rewrite it as:
\begin{equation}
\hat{z}_0\left(z_t,v_t^{src}\right)=\frac{z_t}{\sqrt{\bar{\alpha}_t}}-\frac{\sqrt{1-\bar{\alpha}_t}}{\sqrt{\bar{\alpha}_t}} \tilde{\varepsilon}_\theta\left(z_t, t, v^{src},v_\varnothing\right)
\label{eq:Tweedie}
\end{equation}
where $\tilde{\varepsilon}_\theta\left(z_t, t, v^{src},v_\varnothing\right)$ is defined in Eq. \ref{eq:cfg_fusion}. 

Note that for every $t<T$ , the optimization should start from the endpoint of the previous step $t + 1$ optimization, which computes a constant $z_t$ using the optimized $v_{t+1}^{src}$ and $z_{t+1}$. Otherwise, the learning embedding would not hold at inference.



\noindent {\bf Multi-concept inversion} is proposed to represent the complex visual concepts by generating the concept embedding. Lastly, we should learn a reference embedding $v^{ref}$ from the reference image $x^{ref}$. The methodological approach is related to Textual Inversion \cite{gal2022image} and DreamArtist \cite{dong2022dreamartist}. 

To represent the concepts in the input images, Textual Inversion \cite{gal2022image} learns an embedding as pseudo-words $S_*$ from few-shot images. DreamArtist \cite{dong2022dreamartist} improves Textual Inversion, which learns a paired positive and negative multi-concept embeddings($S_*^p$ and $S_*^n$) from one-shot image and proposes reconstruction constraint for detail enhancemen. In our case, we apply a similar strategy as DreamArtist, yet our method offers two improvements:

Firstly, we find that the multi-concept embeddings are useful for mining semantics information from the images, while the negative embeddings are optional. And in our pipeline, the negative embeddings is in conflict with the source embedding $x^{src}$. Thus, we use a single positive multi-concept embeddings for learning the reference text embedding $v^{ref}$. We freeze the parameters of the generative diffusion model $\varepsilon_\theta$, and optimize the $v^{ref}$ using the denoising diffusion objective\cite{ho2020denoising}:
\begin{equation}
\mathcal{L}_{ldm}= E_{\epsilon,t } \left[\left\|\epsilon-\varepsilon_\theta\left(z_t^{ref}, t, v^{ref}\right)\right\|_2^2\right].
\end{equation}
where $v^{ref}$ is a multi-concept embeddings, $z_t^{ref}$ is a noisy version of $z^{ref}_0$ (the latent code of the reference image $x^{ref}$) obtained using Eq. \ref{eq:add_noise}, $\epsilon \sim \mathcal{N}(0, I)$ and $t \sim \operatorname{U}(1, T) $.

Secondly, we improve the reconstruction constraint for detail enhancement mechanism in DreamArtist. DreamArtist applys reconstruction constraint in $x$ space, which can be denoted as $\mathcal{D}\left(\hat{z}_{t-1}\left(z_t,S^*\right)\right) \leftrightarrow x_0$.
For one thing, optimization in $x$ space suffers from huge resource consumption due to the gradient back propagation inside decoder $\mathcal{D}$. For another thing, there is a gap between the estimating $z_{t-1}$ and $z_0$, especially in the early stage of the denoising processes.

Formally, we implement reconstruction constraint in $z$ space. The reconstruction loss can be written as:
\begin{equation}
\mathcal{L}_{rec}= E_{\epsilon,t } \left[\left\|z_0^{ref}-\hat{z}_0\left(z_t^{ref},v_t^{ref}\right)\right\|_2^2\right].
\end{equation}
where $\hat{z}_0\left(z_t^{ref},v_t^{ref}\right)$ refers to the estimated clean latent $\hat{z}_0^{ref}$ given $z_t^{ref}$ and $v_t^{ref}$, using Eq. \ref{eq:Tweedie}.


\section{Experiments}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

\subsection{Implementation details}
Putting all components together, our full algorithm is presented in our supplementary material. The core training processes consist of two parts: pivotal tuning inversion with $x^{src}$ and multi-concept inversion with $x^{ref}$, which can be implemented independently. For more details please refer to our supplementary material.

Our experiments were conducted using a single A100 GPU. We use Adam\cite{Kingma2014AdamAM} optimizer for both training processes. We collect the evaluation images from the large-scale LAION 5B dataset \cite{schuhmann2021laion} containing 5 billion images.

\subsection{Comparison to Prior/Concurrent Work}

{\bf General I2I Tasks}. Here, we evaluate the performance of the proposed framework in general I2I tasks including leopard$\rightarrow$dog, face swap and mountain$\rightarrow$snow mountain, as shown in Fig. \ref{fig:general_I2I}. We compare the proposed method with TuiGAN \cite{lin2020tuigan}, PhotoWCT \cite{li2018closed}, stable diffusion (SD) \cite{rombach2022high}, textual inversion (TI) \cite{gal2022image} and prompt-to-prompt (Ptp) \cite{hertz2022prompt}. For text-to-image models without learned embedding input including SD and Ptp, we use BLIP image caption model \cite{li2022blip} to extract text description as input of diffusion model.
From Fig. \ref{fig:general_I2I}, for GAN-based translation methods TuiGAN and PhotoWCT cannot well translate the concept with only one image input with poor generation quality. 
For example, from columns 3-4 of Fig. \ref{fig:general_I2I}, GAN-based methods only translate part of texture features from the reference image in leopard$\rightarrow$dog and face swap task, and the image quality is poor in the mountain$\rightarrow$snow mountain task. Therefore, the GAN-based methods cannot achieve satisfactory results in the one-shot setting.
For diffusion-based methods SD and TI, the concepts of the reference image can be well preserved, but the information in the content image cannot be extracted. As shown in the column 7 of Fig. \ref{fig:general_I2I}, Ptp can well preserve content but the concepts in the reference images cannot be fused. By tackling all weaknesses of the above methods, the proposed VCT can generate the best results with concepts learned and content preserved.

Furthermore, to evaluate the strong concept translation ability of the proposed VCT, we keep the content image fixed and change different reference images in Fig. \ref{fig:I2I_fine_grade}. The generation results of different reference images show satisfactory content preservation and concept translation ability. More results can be found in the supplementary material.

As shown in Fig. \ref{fig:concurrent}, we further make comparisons to concurrent one-shot baselines: Paint-by-example\cite{yang2022paint} and ControlNet\cite{zhang2023adding}. These methods use additional condition for controlling the generated image, while our method obtains better performance.

% Figure environment removed


{\bf Image Style Transfer}. In addition to general I2I, the proposed method also achieves excellent results in tasks of image style transfer. We compare our method with recent SOTAs in style transfer tasks with different art styles. As shown in Fig. \ref{fig:style_transfer}, we totally compare with three GAN-based methods including TuiGAN \cite{lin2020tuigan}, PhotoWCT \cite{li2018closed} and ArtFlow \cite{chen2021artistic}, and three diffusion-based methods including SD \cite{rombach2022high}, TI \cite{gal2022image} and Ptp \cite{hertz2022prompt}. Following the setting of general I2I, we use BLIP image caption model to extract text descriptions for text-to-image model SD and Ptp. From the results in Fig. \ref{fig:style_transfer}, large defects existing for results generated by GAN-based methods, especially for TuiGAN and ArtFlow as columns 3 and 5 in Fig. \ref{fig:style_transfer}. The same problem exists in diffusion-based methods SD and TI as general I2I that contents cannot be preserved. For Ptp, although the contents are preserved, the concept in the reference images cannot be well translated. The proposed method can also generate the most satisfactory images, as shown in column 9 of Fig. \ref{fig:style_transfer}.

We also evaluate the model performance by keeping the reference image fixed and changing content image, and vice versa. The results are shown in Fig. \ref{fig:style_transfer_fine_grade}. The excellent translation results prove the generalization of the proposed method.


 \begin{table}
 \caption{Quantitative comparison of style transfer results. Our
model outperforms baseline methods in image quality(BRISQUE), content preservation(LPIPS) and human preference(sPre.)}.
 \footnotesize
 \centering
% \vspace{-0.25cm}
\vspace{0.25cm}
\begin{tabular}{cccc}
\toprule

method & BRISQUE $\downarrow$ &  LPIPS $\downarrow$ & sPre. $\uparrow$\\
\midrule

InST \cite{zhang2023inversion}  & 21.09 & 0.487 &       1.22    \\
ptp \cite{hertz2022prompt}  & 10.11 & 0.298 &       1.89     \\
ours  &  {\bf 9.44} &  {\bf 0.242}   &  {\bf 2.63}  \\


\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:quantitative}
 \end{table}


\noindent {\bf{Quantitative Comparision}}. Due to the absence of ground truth for style transfer and the domain gap between the two domains, quantitative evaluation remains an open challenge. Recall that our goal is to create a new image from a source image and a reference image. Thus, we use the following three metrics to evaluate the generated images. 1) Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE), which is a no-reference image quality score for quality assessment. 2) Learned Perceptual Image Patch Similarity (LPIPS), which evaluates the distance between the source image and target image to verify the content preservation performance of the different models. 3) Human preference score (Pre.), where we invited participants to vote on the generated results of the different methods through a user study. Our model outperforms in overall evaluation metric. Further experimental settings can be found in the supplementary material.


\subsection{Ablation Study}

Finally, we ablate each component of our method and show its effectiveness, including multi-concept inversion (MCI), pivotal turning inversion (PTI) and attention control (AC).  

See visual ablation studies in Fig.\ref{fig:ablation_study_method}.  (a) By removing MCI, where we use a word 'dog' to generate the reference embedding $v^{ref}$ in our pipeline, the generated result is not the specific dog in the reference image. (b) Without using PTI, the content matching branch cannot reconstruct the content image, due to the inconsistent ddim sampling trajectory. (c) By removing AC, the result can not retain the structure of the content image. 

Overall, we can obtain the best generation outputs by using all of our proposed components, which better preserves the structure and semantic layout of the content image, while complying with the reference image. Further ablations can be found in the supplementary material.

\section{Conclusion}
In this work, motivated by the importance of visual concepts in our daily life,
we complete the general I2I with image guidance by proposing a novel framework named VCT. It can preserve the content in the source image and translate visual concepts guided by a single reference image. We evaluate the proposed model on a wide range of general image-to-image translation tasks with excellent results. 

\newpage

{\small
\begin{thebibliography}{10}\itemsep=-1pt


\bibitem{baek2021rethinking}
Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Hyunjung Shim.
\newblock Rethinking the truly unsupervised image-to-image translation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 14154--14163, 2021.

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{chen2021artistic}
Haibo Chen, Zhizhong Wang, Huiming Zhang, Zhiwen Zuo, Ailin Li, Wei Xing,
  Dongming Lu, et~al.
\newblock Artistic style transfer with internal-external learning and
  contrastive learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:26561--26573, 2021.

\bibitem{collins2020editing}
Edo Collins, Raja Bala, Bob Price, and Sabine Susstrunk.
\newblock Editing in style: Uncovering the local semantics of gans.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5771--5780, 2020.

\bibitem{deng2020arbitrary}
Yingying Deng, Fan Tang, Weiming Dong, Wen Sun, Feiyue Huang, and Changsheng
  Xu.
\newblock Arbitrary style transfer via multi-adaptation network.
\newblock In {\em Proceedings of the 28th ACM international conference on
  multimedia}, pages 2719--2727, 2020.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems},
  34:8780--8794, 2021.

\bibitem{dong2022dreamartist}
Ziyi Dong, Pengxu Wei, and Liang Lin.
\newblock Dreamartist: Towards controllable one-shot text-to-image generation
  via contrastive prompt-tuning.
\newblock {\em arXiv preprint arXiv:2211.11337}, 2022.

\bibitem{gabbay2021scaling}
Aviv Gabbay and Yedid Hoshen.
\newblock Scaling-up disentanglement for image translation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6783--6792, 2021.

\bibitem{gal2022image}
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit~H Bermano, Gal
  Chechik, and Daniel Cohen-Or.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion.
\newblock {\em arXiv preprint arXiv:2208.01618}, 2022.

\bibitem{gatys2015neural}
Leon~A Gatys, Alexander~S Ecker, and Matthias Bethge.
\newblock A neural algorithm of artistic style.
\newblock {\em arXiv preprint arXiv:1508.06576}, 2015.

\bibitem{grover2020alignflow}
Aditya Grover, Christopher Chute, Rui Shu, Zhangjie Cao, and Stefano Ermon.
\newblock Alignflow: Cycle consistent learning from multiple domains via
  normalizing flows.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4028--4035, 2020.

\bibitem{hertz2022prompt}
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel
  Cohen-Or.
\newblock Prompt-to-prompt image editing with cross attention control.
\newblock {\em arXiv preprint arXiv:2208.01626}, 2022.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{ho2021classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock In {\em NeurIPS 2021 Workshop on Deep Generative Models and
  Downstream Applications}.

\bibitem{huang2017arbitrary}
Xun Huang and Serge Belongie.
\newblock Arbitrary style transfer in real-time with adaptive instance
  normalization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1501--1510, 2017.

\bibitem{jing2020dynamic}
Yongcheng Jing, Xiao Liu, Yukang Ding, Xinchao Wang, Errui Ding, Mingli Song,
  and Shilei Wen.
\newblock Dynamic instance normalization for arbitrary style transfer.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4369--4376, 2020.

\bibitem{jing2018stroke}
Yongcheng Jing, Yang Liu, Yezhou Yang, Zunlei Feng, Yizhou Yu, Dacheng Tao, and
  Mingli Song.
\newblock Stroke controllable fast style transfer with adaptive receptive
  fields.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 238--254, 2018.

\bibitem{johnson2016perceptual}
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
\newblock Perceptual losses for real-time style transfer and super-resolution.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages 694--711. Springer, 2016.

\bibitem{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 4401--4410, 2019.

\bibitem{kawar2022imagic}
Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar
  Mosseri, and Michal Irani.
\newblock Imagic: Text-based real image editing with diffusion models.
\newblock {\em arXiv preprint arXiv:2210.09276}, 2022.

\bibitem{kim2021noise2score}
Kwanyoung Kim and Jong~Chul Ye.
\newblock Noise2score: tweedie’s approach to self-supervised image denoising
  without clean images.
\newblock {\em Advances in Neural Information Processing Systems}, 34:864--874,
  2021.

\bibitem{Kingma2014AdamAM}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{kotovenko2019content}
Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, and Bjorn Ommer.
\newblock Content and style disentanglement for artistic style transfer.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 4422--4431, 2019.

\bibitem{li2016precomputed}
Chuan Li and Michael Wand.
\newblock Precomputed real-time texture synthesis with markovian generative
  adversarial networks.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14},
  pages 702--716. Springer, 2016.

\bibitem{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em International Conference on Machine Learning}, pages
  12888--12900. PMLR, 2022.

\bibitem{li2018closed}
Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and Jan Kautz.
\newblock A closed-form solution to photorealistic image stylization.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 453--468, 2018.

\bibitem{lin2020tuigan}
Jianxin Lin, Yingxue Pang, Yingce Xia, Zhibo Chen, and Jiebo Luo.
\newblock Tuigan: Learning versatile image-to-image translation with two
  unpaired images.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part IV 16}, pages 18--35.
  Springer, 2020.

\bibitem{liu2021adaattn}
Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling Wang, Xin Li, Zhengxing
  Sun, Qian Li, and Errui Ding.
\newblock Adaattn: Revisit attention mechanism in arbitrary neural style
  transfer.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6649--6658, 2021.

\bibitem{liu2021smoothing}
Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe,
  Bruno Lepri, Wei Wang, and Marco De~Nadai.
\newblock Smoothing the disentangled latent style space for unsupervised
  image-to-image translation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10785--10794, 2021.

\bibitem{mokady2022null}
Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
\newblock Null-text inversion for editing real images using guided diffusion
  models.
\newblock {\em arXiv preprint arXiv:2211.09794}, 2022.

\bibitem{park2019arbitrary}
Dae~Young Park and Kwang~Hee Lee.
\newblock Arbitrary style transfer with style-attentional networks.
\newblock In {\em proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 5880--5888, 2019.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{richardson2021encoding}
Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav
  Shapiro, and Daniel Cohen-Or.
\newblock Encoding in style: a stylegan encoder for image-to-image translation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 2287--2296, 2021.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{ruiz2022dreambooth}
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
  Kfir Aberman.
\newblock Dreambooth: Fine tuning text-to-image diffusion models for
  subject-driven generation.
\newblock {\em arXiv preprint arXiv:2208.12242}, 2022.

\bibitem{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S~Sara Mahdavi,
  Rapha~Gontijo Lopes, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em arXiv preprint arXiv:2205.11487}, 2022.

\bibitem{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{sheng2018avatar}
Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang.
\newblock Avatar-net: Multi-scale zero-shot style transfer by feature
  decoration.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8242--8250, 2018.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{su2022dual}
Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Dual diffusion implicit bridges for image-to-image translation.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{ulyanov2016texture}
Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky.
\newblock Texture networks: Feed-forward synthesis of textures and stylized
  images.
\newblock {\em arXiv preprint arXiv:1603.03417}, 2016.

\bibitem{ulyanov2017improved}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Improved texture networks: Maximizing quality and diversity in
  feed-forward stylization and texture synthesis.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6924--6932, 2017.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2022pretraining}
Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and
  Fang Wen.
\newblock Pretraining is all you need for image-to-image translation.
\newblock {\em arXiv preprint arXiv:2205.12952}, 2022.

\bibitem{wang2018high}
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan
  Catanzaro.
\newblock High-resolution image synthesis and semantic manipulation with
  conditional gans.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8798--8807, 2018.

\bibitem{wu2018direction}
Hao Wu, Zhengxing Sun, and Weihang Yuan.
\newblock Direction-aware neural style transfer.
\newblock In {\em Proceedings of the 26th ACM international conference on
  Multimedia}, pages 1163--1171, 2018.

\bibitem{wu2020efanet}
Zhijie Wu, Chunjin Song, Yang Zhou, Minglun Gong, and Hui Huang.
\newblock Efanet: Exchangeable feature alignment network for arbitrary style
  transfer.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 12305--12312, 2020.

\bibitem{yang2022paint}
Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong
  Chen, and Fang Wen.
\newblock Paint by example: Exemplar-based image editing with diffusion models.
\newblock {\em arXiv preprint arXiv:2211.13227}, 2022.

\bibitem{yang2022unsupervised}
Shuai Yang, Liming Jiang, Ziwei Liu, and Chen~Change Loy.
\newblock Unsupervised image-to-image translation with generative prior.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 18332--18341, 2022.


\bibitem{zhang2023inversion}
Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu.
\newblock Inversion-based style transfer with diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10146--10156, 2023.
  

\bibitem{zhang2023adding}
Lvmin Zhang and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models, 2023.

\bibitem{zhang2020cross}
Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen.
\newblock Cross-domain correspondence learning for exemplar-based image
  translation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5143--5153, 2020.

\bibitem{zheng2021spatially}
Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai.
\newblock The spatially-correlative loss for various image translation tasks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 16407--16417, 2021.

\bibitem{zhu2017unpaired}
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei~A Efros.
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2223--2232, 2017.

\end{thebibliography}
}


\raggedbottom
\pagebreak
\clearpage

\appendix

\input{appendix}



\end{document}