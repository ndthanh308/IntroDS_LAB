\clearpage

\appendix
\appendixpage

\section{RobustFill and DeepCoder DSLs}
\label{app:dsls}

\autoref{fig:robustfill_dsl} contains the DSL for the RobustFill experiments. See \citet{ROBUSTFILL} for a description of what the operations do. We add the operations \code{Substitute}, \code{SubstituteAll}, which replace the $i^\text{th}$ occurrence (or all occurrences) of a regex $r$ with character $c$, and \code{Remove}, \code{RemoveAll}, which remove the $i^\text{th}$ occurrence (or all occurrences) of a regex $r$, to increase the expressivity of our DSL.

\autoref{fig:deepcoder_dsl} shows the DSL used for the DeepCoder experiments. The operations are exactly as described in \citet{DEEPCODER}.

% Figure environment removed

% Figure environment removed

\clearpage

\section{RobustFill and DeepCoder Benchmark Details}
\label{app:datasets}
\textbf{RobustFill.}\quad
To create a task for our RobustFill datasets, we first sample random input strings up to 20 characters, one string for each of 4 examples. We then sample a program according to the train or test distribution for the generalization task (as described below), such that the program executes successfully on the inputs to form the example outputs. Due to the concatenation structure of RobustFill programs, we treat each concatenated expression as a subprogram, and recall that we denote the \emph{length} of a program to be the number of subprograms.

For \LengthGen, we train on programs of length 1 to 6 inclusive and test on programs of length 7 to 10. For \ComposeDiffConcepts, we group together all of the substring operations into a \emph{substring concept} and all of the modification operations plus constant strings as a \emph{non-substring concept} (the \code{Compose} operation is omitted), using programs of length 2-6 for both train and test. We use the same lengths and concepts for \SwitchConceptOrder, where training tasks use only the substring concept for the first half of the parts and only the non-substring concept for the latter half, and test tasks have the ordering reversed.
For \ComposeNewOp, 25\% of training tasks are length 1 programs containing only a \code{Compose} operation, the remainder of the training tasks are length 2-6 programs without \code{Compose}, and we test on length 2-6 programs that use \code{Compose}. For \AddOpFunctionality, all tasks are length 1-6 programs, we train on those where a substring operation is \emph{not} used within a \code{Compose} operation, and we test on programs where a substring operation \emph{is} used within a \code{Compose} operation.

\textbf{DeepCoder.}\quad
For DeepCoder, we treat each non-input line as a subprogram, so the example program above has length 2.
We use 3 I/O examples, at most 2 inputs, lists with at most 5 elements, and integers between $-50$ and $50$ inclusive.
For \LengthGen, we train on programs of length 1 to 4 and test on length 5.
For \ComposeDiffConcepts\ and \SwitchConceptOrder, we use programs of length 1 to 4 and split the operations into a concept containing all first-order operations plus the \code{Map} operation, and another concept containing all remaining higher-order operations.
For \ComposeNewOp, 25\% of training tasks are length 1 programs containing only a \code{Scanl1} operation, the remainder of training tasks are length 2-4 programs without \code{Scanl1}, and we test on length 2-4 programs that use \code{Scanl1}. For \AddOpFunctionality, all tasks are length 1-4 programs, \code{Scanl1} is only used with the lambdas \code{(-)} and \code{(min)} during training, and we test on tasks where \code{Scanl1} is used with the other lambdas \code{(+)}, \code{(*)}, and \code{(max)}.

The program sampling procedure that we used for RobustFill ensures that the ground-truth program for a test task is within the test distribution over programs. However, the task might actually have a different solution within the \emph{train} distribution. Then, solving this test task is not necessarily a signal of generalization.\footnote{This issue is far less prevalent in the RobustFill dataset due to the concatenation program structure reducing the number of different programs that solve a task.}
To address this, we construct the DeepCoder dataset more carefully as follows.
We sample random inputs as before. Then, we perform an exhaustive enumerative search of all programs in the train distribution up to a maximum length, and similarly for programs in the test distribution. As a result, we can identify all minimal-length solution programs for a given task (if it is solvable by any program up to the maximum enumerated length). Finally, we sample training tasks among those where there exists a minimal-length solution program in the train distribution, and test tasks among those where \emph{all} minimal-length solutions are in the test distribution.\footnote{Note that it is still possible for a test task to be solved with a \emph{longer} program in the train distribution, but detecting these cases in general is extremely difficult.
}

\section{Beam Search during ExeDec}
\label{app:beam_search}
\autoref{sec:method} and \autoref{alg:exedec} describe a single synthesis attempt, but we design a beam search process to run multiple synthesis attempts efficiently in parallel.

Our goal is the same as in traditional beam search~\citep{BEAMSEARCH}, which is to output $k$ sequences that maximize a score function. Formally, a candidate sequence $C$ at step $t$ is a sequence of execution subgoals and subprograms $C = [\{S_i^{(1)}\}, P^{(1)}, \hdots, \{S_i^{(t)}\}, P^{(t)}]$. 
As notation, we let $C^{(j)}$ denote the prefix of $C$ up to and including the $j$-th subprogram $P^{(j)}$.
The score for a candidate sequence $C$ is given by
\begin{align}
    \mathsf{Score}(C) = \sum_{j = 1}^t \mathsf{Score}(\{S_i^{(j)}\}) + \mathsf{Score}(P^{(j)}) + \mathsf{Valid}(C^{(j)})\,.
    \label{eq:beam_score}
\end{align}
Here, since both SubgoalModel and SynthesizerModel in \exedec{} are autoregressive sequence models (see \autoref{sec:method} for more details), they can produce token sequences (either set of subgoals or a subprogram) with their corresponding log-probabilities which we use as the score function, i.e., $\mathsf{Score}(\{S_i^{(j)}\}) = \log p(S_i^{(j)} \mid \{(I_i^{(j)}, O_i^{(j)})\})$ and $\mathsf{Score}(P^{(j)}) = \log p(P^{(j)} \mid \{(I_i^{(j)}, S_i^{(j)})\})$.
By default, $\mathsf{Valid}(C^{(j)}) = 0$ unless $C^{(j)}$ fails to meet some required conditions. First, we set $\mathsf{Valid}(C^{(j)}) = -\infty$ if any subgoal or subprogram in $C^{(j)}$ is a token sequence that does not parse, or if any subprogram does not execute successfully.
We also check whether $C^{(j)}$ leads to unique program functionality within the beam of $k$ candidate sequences.
Note that different candidate sequences can result in the same subprograms even if the subgoals are different, and furthermore, combining different subprograms might result in full programs with the same functionality that are practically equivalent for future steps.
Therefore, we set $\mathsf{Valid}(C^{(j)}) = -\infty$ for all candidate sequences in the beam with corresponding program functionality equivalent to that of another higher-scoring candidate sequence in the beam.
Finally, domain-specific checks may determine that the candidate sequence $C^{(j)}$ is unlikely to result in a successful program in later steps, or that some computation limit such as a maximum number of steps is reached. In any case, an ``invalid'' beam element will drop out of the beam at the next step, freeing space in the beam for expansions of other surviving beam elements.

Note that we do not perform separate beam searches for each call to a model, since an isolated beam search would start with a single empty beam element with score 0. Instead, we perform a single long beam search throughout \autoref{alg:exedec} where each beam element is a candidate sequence $C$ with score $\mathsf{Score}(C)$. Model calls can be seen as ``extending'' the ongoing beam search, adding more tokens to the existing candidate sequences while maintaining their scores.

\section{Aligned Relative Attention for the SubgoalModel}
\label{app:ara}
In the Specification-Transformer (\autoref{sec:method}),
we use \emph{relative attention}~\citep{Shaw2018Relative}, i.e., representations of relative positions (distances between positions) instead of absolute positions of tokens.
This improves Transformer performance, particularly in length generalization~\citep{Shaw2018Relative,Csordas2021Devil}.
However, unlike the other models, our SubgoalModel actually predicts a sequence of sequences, i.e., a subgoal for each I/O example, concatenated together.
Thus, the relative position between
subgoal $S_i$ and I/O encoding $\phi_i$ is not consistent for every example index $i$,
so a naive relative position embedding would not represent consistent information across examples.
To remedy this, we propose \emph{aligned relative attention} (ARA), which alters the ``positions'' of subgoal tokens in the relative distance calculation.
Specifically, we increment the current position by $1$ for each token during subgoal prediction as usual, but when starting a new subgoal $S_i$ we set the current position to the position of $\phi_i$. This ensures that the relative position between the beginnings of $S_i$ and $\phi_i$ is always $0$.
ARA only applies to the SubgoalModel to help it perform the sequence-of-sequences prediction, and it is not intended to address compositional generalization directly.

\section{Training Details}
\label{app:training}
For the experiments involving Transformer models trained from scratch (\autoref{subsec:scratch}), we performed a small hyperparameter search varying the learning rate and model size, choosing a single setting that performed well on training metrics across generalization tasks and model types. We used an embedding dimension of 512, hidden dimension of 1024, 3 layers, and 4 attention heads. For relative attention, we use 32 buckets for relative position embeddings, with bucket boundaries placed logarithmically given the maximum relative distance which is computed based on the lengths of the input and output sequences.  We train with the Adam optimizer with a learning rate of $2 \times 10^{-4}$ with linear warmup for 16,000 steps and square root decay, with a batch size of 128 and 500K training steps, using fresh synthetic training data without repeating examples.
Training took about 1 day for RobustFill (or about 5 hours for DeepCoder) with 8 TPU v2 accelerators per model.

In our experiments, some models have different sizes (with the other hyperparameters held constant):
\begin{itemize}[leftmargin=1.2em,itemsep=0em]
\item \exedec{}, the no-subgoal ablation, and the Transformer baseline all use an embedding dimension of 512 and hidden dimension of 1024, as selected from the hyperparameter search mentioned above.
\item \exedec-Small uses an embedding dimension of 360 and hidden dimension of 720, such that the total number of parameters between the SubgoalModel and SynthesizerModel is approximately equal to the number of parameters in the no-subgoal ablation or in the baseline, which only use one model each. \exedec{}'s code solution only comes from the SynthesizerModel, while the SubgoalModel's predictions do not show up anywhere in the solution and only encourage the SynthesizerModel to think about the next subgoal instead of the end goal. Thus, compared to the ablation and baseline, \exedec{} holds the SynthesizerModel size constant to compare the two modes of thinking, while \exedec{}-Small holds the total number of parameters constant to verify that the improvements are not only due to extra parameters in the SubgoalModel.
\item Latent Programmer uses a smaller embedding dimension of 256 and hidden dimension of 512 because we ran into out-of-memory issues when using the Latent Programmer's training script which trains two models at once. Our trained Latent Programmer models are still larger than those reported in the Latent Programmer paper leading to a corresponding performance increase, reaching 76\% accuracy on no-generalization RobustFill compared to 57\% reported in the Latent Programmer paper.
\end{itemize}

Additionally, for Latent Programmer, we performed a separate hyperparameter search varying the learning rate and number of pretraining steps. We chose a learning rate of $5 \times 10^{-4}$ for RobustFill and $2 \times 10^{-4}$ for DeepCoder, and 20,000 pretraining steps for both datasets. We use a beam size of 10 and a latent beam size of 3, as in the Latent Programmer paper. (For experiments with beam size of 1, the latent beam size is also 1.) All other settings were kept the same as for the other models.

For DeepCoder, the ground-truth training programs have variable names chosen randomly among a fixed set of possible names. If the variable names were used in a canonical order instead, the models would be unable to predict subprograms using variable names unseen during training, as is required for length generalization. When predicting a subprogram, the model only predicts the right hand side of the assignment, which at test time is assigned to a new variable name using a canonical ordering.

\section{Beam Size 1 Results for Models Trained from Scratch}
\label{app:beam_size_1}

\autoref{fig:experiments} in \autoref{subsec:scratch} shows detailed results for all generalization tasks, using beam size 10. We also performed experiments with beam size 1, summarized below in \autoref{tab:beam_size_1}.

\setlength{\tabcolsep}{5.5pt}
\begin{table}[H]
\centering
\small
\caption{Results for beam size 1 and beam size 10 (end-to-end test accuracy as percentages). ``NoGen'' refers to the case where no compositional generalization is required, while ``GenAvg'' refers to the average across the 5 compositional generalization tasks.}
\label{tab:beam_size_1}
\begin{tabular}{l|cccc|cccc}
\toprule
 & \multicolumn{4}{c|}{RobustFill} & \multicolumn{4}{c}{DeepCoder} \\
 & \multicolumn{2}{c}{Beam Size 1} & \multicolumn{2}{c|}{Beam Size 10} & \multicolumn{2}{c}{Beam Size 1} & \multicolumn{2}{c}{Beam Size 10} \\
Approach & NoGen & GenAvg & NoGen & GenAvg & NoGen & GenAvg & NoGen & GenAvg \\
\midrule
\exedec{}         & 81.4 & \textbf{73.1} & \textbf{95.5} & \textbf{87.0} & 39.1 &  \textbf{8.4} & 61.2 & \textbf{23.1} \\
\exedec{}-Small   & 78.7 & 71.2 & 94.4 & 85.9 & 36.1 &  7.7 & 58.4 & 21.4 \\
Ablation          & 79.5 & 63.8 & 94.9 & 80.2 & 39.9 &  5.6 & \textbf{64.7} & 18.1 \\
Transformer       & \textbf{83.7} & 33.9 & 90.9 & 42.4 & \textbf{50.7} &  4.9 & 53.6 &  \phantom{0}5.7 \\
Latent Programmer & 66.0 & 26.3 & 76.1 & 33.7 & 40.5 &  2.4 & 46.5 &  \phantom{0}3.5 \\
\bottomrule
\end{tabular}
\end{table}

We observe that \exedec{} achieves the highest generalization average, for both RobustFill and DeepCoder and for both beam size 1 and 10. Additionally, for both no-generalization and average generalization, the baseline Transformer's performance does not improve that much going from beam size 1 to 10. On the other hand, \exedec{} improves significantly more from beam size 1 to 10, showing the effectiveness of our modified beam search (\autoref{app:beam_search}) that enables exploring different solution routes by planning in the execution space.

\section{Single-Step Accuracy}
\label{app:single_step_accuracy}

To gain more insight into the step-by-step synthesis process, we measured the ``single-step accuracy'' for \exedec{}'s SubgoalModel and SynthesizerModel, and the CombinedModel from the no-subgoal ablation. More specifically, for a single step in a trajectory that matches the ground-truth so far, how often does the SubgoalModel predict subgoals exactly matching the ground-truth ones (for all I/O examples) with a single greedy decoding? And, given a ground-truth trajectory so far, how often does the SynthesizerModel or CombinedModel predict a program whose behavior matches the ground-truth subprogram? The single-step accuracy results are in \autoref{tab:single_step_accuracy}.

\setlength{\tabcolsep}{5pt}
\begin{table}[H]
\centering
\small
\caption{Single-step accuracy percentages.}
\label{tab:single_step_accuracy}
\begin{tabular}{l|ccc|ccc}
\toprule
 & \multicolumn{3}{c|}{RobustFill} & \multicolumn{3}{c}{DeepCoder} \\
Generalization Task & Subgoal & Synthesizer & Combined & Subgoal & Synthesizer & Combined \\
\midrule
No Generalization       & 97.7 & 97.1 & 94.9 & 55.8 & 98.9 & 64.4 \\
\LengthGen{}            & 97.2 & 96.4 & 94.0 & 31.6 & 96.4 & 41.9 \\
\ComposeDiffConcepts{}  & 95.3 & 98.9 & 90.5 & 38.0 & 94.3 & 40.0 \\
\SwitchConceptOrder{}   & 90.8 & 98.9 & 87.4 & 16.4 & 77.8 & 17.9 \\
\ComposeNewOp{}         & 93.9 & 94.0 & 84.4 & 40.8 & 91.3 & 44.4 \\
\AddOpFunctionality{}   & 94.0 & 84.4 & 82.3 & 40.3 & 60.0 & 41.5 \\
\bottomrule
\end{tabular}
\end{table}

Note that the single-step accuracy metric is particularly low for the SubgoalModel and CombinedModel on DeepCoder because there are potentially many correct ways of solving the problem, and this metric only considers the single ground-truth solution. In fact, the SubgoalModel does not need to have super high accuracy in order for \exedec{} to achieve good end-to-end results, because the SynthesizerModel can ignore slight errors in the subgoals and still produce a program that behaves as closely as possible while using only 1 DSL operation. We have seen many concrete cases of the SynthesizerModel being given a slightly incorrect subgoal and then producing the correct subprogram anyway, which disagrees with the subgoal but correctly makes progress overall.

\section{Intuition of Spurious Patterns}
\label{app:spurious_patterns}

In \autoref{fig:experiments}, \exedec{} sometimes performs worse than the no-subgoal ablation in \LengthGen{} and \SwitchConceptOrder{}, while \exedec{} performs much better in \ComposeDiffConcepts{} and \ComposeNewOp{}.
For \AddOpFunctionality{}, \exedec{} and the no-subgoal ablation have a much smaller improvement over the Transformer baseline, especially in RobustFill.
These observations may be explained by analyzing the different kinds of ``spurious patterns'' that arise in the various compositional generalization splits:
\begin{itemize}[leftmargin=1.2em,itemsep=0em]

    \item For \LengthGen{} and \SwitchConceptOrder{}, the index of the current subprogram carries major implications in the training distribution, for example, ``the problem should be almost solved by now'' or ``we must use this category of operation''. However, these implications are drastically changed in the test distribution, leading to \emph{spurious patterns} that may lead to poor performance on the test split. The Transformer baseline is aware of the length of the prediction so far, so it can be easily confused by this distribution shift --- and indeed, it performs particularly poorly on these tasks. On the other hand, both \exedec{} and the no-subgoal ablation are less aware of the current index of the subprogram, since the prior subprograms are only indirectly provided to the models through the program state. For these tasks, \exedec{} and the no-subgoal ablation have relatively similar performance but greatly outperform the Transformer baseline.

    \item For \ComposeDiffConcepts{} and \ComposeNewOp{}, the spurious patterns arise from comparison to what work needs to be done outside the current subprogram, for example, ``this subprogram uses the same category of operation as the other subprograms'' or ``this subprogram can use operation $X$ only if there are no other subprograms''. Again, these patterns are changed between the train and test distributions. The no-subgoal ablation is susceptible to overfitting on these patterns because it sees the I/O specification for the current subprogram composed with all future subprograms. On the other hand, \exedec{} is more shielded from these spurious patterns because its SynthesizerModel only sees the I/O specification for the current subprogram (provided that the SubgoalModel predicts the correct subgoals). This difference may explain why \exedec{} has a larger performance improvement over the no-subgoal ablation for these generalization tasks.

    \item For \AddOpFunctionality{}, the spurious pattern is actually within an individual subprogram, i.e., some subprograms in test problems are outside the distribution of subprograms seen during training. None of the compared approaches are well-shielded from this form of spurious pattern, leading to the relatively low performance of our methods on this generalization task for RobustFill. (For DeepCoder, the trend is less clear since some problems may be solved using longer programs outside the test distribution.)
\end{itemize}


\section{Algorithm Comparison Case Study}
\label{app:case_study}

\paragraph{RobustFill.}
\autoref{fig:comparison_1} and \autoref{fig:comparison_2} compare how \exedec{}, the no-subgoal ablation, and the Transformer baseline perform on two example problems from the RobustFill DSL, under the \ComposeNewOp{} generalization task where models were not trained on composed programs (other than length-1 programs) that use the \code{Compose} operation. A beam size of 1 (greedy decoding) is used for simplicity. In the figures, subprograms and execution results are colored red if they are incorrect. Portions of the final programs are colored yellow if they do execute correctly but implement the desired functionality suboptimally, i.e., with more subprograms than necessary.

For both example problems, \exedec{} successfully solves the problem using a minimal-length program that is within the test distribution of the compositional generalization task, showing successful generalization. However, the no-subgoal ablation does not perform as well. In \autoref{fig:comparison_1}, it uses the \code{Compose} operation incorrectly because, due to the \ComposeNewOp{} task, it has never been trained to use \code{Compose} to produce a \emph{prefix} of the output. This is not an issue for \exedec{} because the prefixes are provided by the SubgoalModel. In \autoref{fig:comparison_2}, the ablation predicts a correct but suboptimal program in the training distribution, replacing a \code{Compose} operation with three separate steps. The Transformer baseline is comparatively the worst, predicting incorrect programs in the training distribution for both problems, showing a clear inability to compositionally generalize.

\paragraph{DeepCoder.}
\autoref{fig:comparison_3} shows a similar comparison between the three approaches, this time on a DeepCoder list manipulation problem under the \ComposeDiffConcepts{} generalization task. For this generalization task, we partition the DSL operations into two groups or ``concepts'' -- one concept contains the higher-order operations \code{Scanl1}, \code{Filter}, \code{Count}, and \code{ZipWith}, while the other concept contains all of the first-order operations\footnote{The concept with first-order operations also contains the higher-order operation \code{Map}, to better balance the number of different program functionalities obtainable within each concept.}. The training problems have minimal-length ground-truth programs that only compose operations within the same concept, while test problems require mixing operations from different concepts to achieve a minimal-length solution program.

The problem in \autoref{fig:comparison_3} is solved using operations from both concepts: \code{Scanl1} is higher-order, while \code{Take} and \code{Sort} are first-order operations.
The Transformer baseline fails to solve this problem. It correctly uses \code{Scanl1} as the first step but incorrectly continues to use higher-order operations. This behavior makes sense because the model was trained on programs showing a similar pattern, and it does not deviate from the pattern because this approach is not compositionally general.

Similarly, the no-subgoal ablation correctly uses \code{Scanl1} in the first step but also continues to use higher-order operations in subsequent steps until eventually the program fails to typecheck. We observe that, after successfully using \code{Scanl1} in the first subprogram, the updated specification actually describes a problem with a solution program within the training distribution (using first-order operations \code{Take} and \code{Sort} from the same concept). So, why is the ablation unable to solve the updated specification, even though the SynthesizerModel is not directly conditioned on the previous subprograms like the Transformer baseline is? We hypothesize that the SynthesizerModel recognizes from the specification that \code{x2} was computed by applying a \code{Scanl1} operation to \code{x0}, and thus according to patterns seen during training, the model is inclined to continue using higher-order operations. This hypothesis is supported by the fact that the ablation, as well as \exedec{} and the baseline, all correctly solve a simplified version of this problem where the input \code{x0} is replaced with the result of the first subprogram, \code{x2 = Scanl1 (+) x0}, such that the \code{Scanl1} portion is ``already computed'' but not in a way visible from the specification. In other words, the ablation succeeds for the simplified problem because it is now unable to refer to previous subprograms.

In contrast, \exedec{} successfully solves the problem with a minimal-length solution that switches between concepts. Note that its SynthesizerModel cannot directly or indirectly reference previous subprograms. The SubgoalModel \emph{can} indirectly refer to previous subprograms by examining the specification, but it is generally less susceptible to spurious compositional patterns. For example, the ablation's SynthesizerModel might learn the pattern ``predict \emph{an operation} in the same concept as previously'' which is very easy for a neural model to recognize and implement, whereas it is more difficult for \exedec{}'s SubgoalModel to learn the pattern ``predict \emph{a subgoal that is implementable with an operation} in the same concept as previously''. This provides some intuition for why \exedec{} is more compositionally general overall.

% Figure environment removed

% Figure environment removed

% Figure environment removed

\section{Programs and Prompts for LLM Experiments}
\label{app:llm}

\subsection{DSL Programs as Python Functions}

For the LLM experiments, we transform the DSL programs into Python functions that call a hypothetical \code{dsl} library, enabling the LLM to use its understanding of Python programming while we measure how well it generalizes to new functionality it has not been trained on.

The RobustFill program \code{GetFrom(\textquotesingle\ \textquotesingle) | Const(\textquotesingle .\textquotesingle) | Compose(ToCase(PROPER), GetToken(WORD, 1))} transforms the input string \str{TURING, Alan} into the output string \str{Alan.Turing}. For the LLM experiments, it is written as a Python function as follows:

\begin{lstlisting}
def program(x):
  parts = [
      dsl.GetFrom(x, ' '),
      dsl.Const('.'),
      dsl.ToCase(dsl.GetToken(x, dsl.Type.WORD, 1), dsl.Case.PROPER),
  ]
  return ''.join(parts)
\end{lstlisting}

The DeepCoder program \code{x0 = INPUT | x1 = Map (**2) x0 | x2 = Sort x1} transforms the input list \spec{$[5, 3, -4]$} into the output list \spec{$[9, 16, 25]$}. As a Python function, this would be:

\begin{lstlisting}
def program(x0):
  x1 = dsl.Map(dsl.SQUARE, x0)
  x2 = dsl.Sort(x1)
  return x2
\end{lstlisting}

We also experiment with a ``Pythonic'' form of DeepCoder programs, for example:

\begin{lstlisting}
def program(x0):
  x1 = [x ** 2 for x in x0]
  x2 = sorted(x1)
  return x2
\end{lstlisting}

\subsection{LLM Prompts}
\label{app:llm_prompts}

We programmatically create prompts for 3 LLM approaches (baseline, ablation, and \exedec{}), for 3 kinds of datasets (RobustFill, DeepCoder, and DeepCoder-Pythonic). We provide prompts for a few example combinations in figures:
\begin{itemize}
    \item \autoref{fig:baseline_prompt_robustfill} has a Baseline-style prompt for RobustFill,
    \item \autoref{fig:ablation_prompt_deepcoder} has an Ablation-style prompt for DeepCoder,
    \item \autoref{fig:exedec_prompt_robustfill} has an \exedec{}-style prompt for RobustFill, and
    \item \autoref{fig:exedec_prompt_deepcoder_pythonic} has an \exedec{}-style prompt for DeepCoder-Pythonic.
\end{itemize}
In each figure, the prompt contains only 1 few-shot example for brevity, but our experiments used 4 few-shot examples for all prompts. Line wrapping is denoted with the \mbox{\space\textcolor{red}{$\hookrightarrow$}\space} symbol.

Observe that the information contained in the \exedec{}-style prompt is the same as in the Ablation-style prompt, just with different ordering of the code for a step and its corresponding execution results. Although this difference may seem slight, it leads to the improved performance for the \exedec{}-style approach.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

\clearpage
\section{Failure Modes in LLM Experiments}
\label{app:llm_failures}
In the LLM experiments, \exedec{}'s incorrect solution programs encounter a variety of errors. \autoref{tab:llm_errors} lists the specific errors, the number of programs encountering an error considering all generalization tasks (including no generalization), and the proportion of that error among all incorrect programs for that dataset.

\setlength{\tabcolsep}{10pt}
\begin{table}[H]
\centering
\small
\caption{Error analysis for LLM experiments for \exedec{} @ 1 (greedy decoding).}
\label{tab:llm_errors}
\begin{tabular}{l|cc|cc|cc}
\toprule
 & \multicolumn{2}{c|}{RobustFill} & \multicolumn{2}{c|}{DeepCoder} & \multicolumn{2}{c}{DeepCoder-Pythonic} \\
\midrule
  (Correct)                  &  \phantom{0}33 &             ---  &  \phantom{0}66 &             ---  & \phantom{00}88 &             ---  \\
  \texttt{AssertionError}    &  \phantom{00}2 & \phantom{0}0.2\% &  \phantom{00}0 & \phantom{0}0.0\% & \phantom{000}0 & \phantom{0}0.0\% \\
  \texttt{AttributeError}    &  \phantom{00}6 & \phantom{0}0.5\% &  \phantom{0}63 & \phantom{0}5.6\% & \phantom{000}0 & \phantom{0}0.0\% \\
  \texttt{IndexError}        &  \phantom{00}0 & \phantom{0}0.0\% &  \phantom{00}0 & \phantom{0}0.0\% & \phantom{00}18 & \phantom{0}1.6\% \\
  \texttt{NameError}         &  \phantom{00}0 & \phantom{0}0.0\% &  \phantom{00}0 & \phantom{0}0.0\% & \phantom{000}9 & \phantom{0}0.8\% \\
  \texttt{SyntaxError}       &  \phantom{00}1 & \phantom{0}0.1\% &  \phantom{00}1 & \phantom{0}0.1\% & \phantom{000}0 & \phantom{0}0.0\% \\
  \texttt{TypeError}         &  234           &           20.1\% &            150 &           13.2\% & \phantom{000}9 & \phantom{0}0.8\% \\
  \texttt{ValueError}        &  \phantom{00}0 & \phantom{0}0.0\% &  \phantom{00}0 & \phantom{0}0.0\% & \phantom{000}3 & \phantom{0}0.3\% \\
  \texttt{ZeroDivisionError} &  \phantom{00}0 & \phantom{0}0.0\% &  \phantom{00}0 & \phantom{0}0.0\% & \phantom{00}15 & \phantom{0}1.3\% \\
  Wrong behavior             &  924           &           79.2\% &            920 &           81.1\% &           1058 &           95.1\% \\
\bottomrule
\end{tabular}
\end{table}

For RobustFill, about 20\% of failures are \texttt{TypeError}s caused by using the DSL incorrectly, while about 79\% of failures do not encounter a runtime error but simply have behavior inconsistent with the I/O specification.

For DeepCoder, \texttt{AttributeError} accounts for about 5.6\% of failures (when the predicted program attempts to use a hallucinated function or constant in the DSL), \texttt{TypeError} accounts for about 13\% of failures, and about 81\% of failures are due to wrong behavior.
For DeepCoder-Pythonic, about 5\% of failures are from various runtime errors, while about 95\% are due to wrong behavior.

Overall, the vast majority of failures are due to wrong behavior. Although LLMs have seen much success in predicting programs from natural language specifications, they still perform poorly in programming-by-example without natural language hints, especially for synthetic problems.

\section{Hierarchical \exedec{}}
\label{app:hierarchical}

The following is Python-like pseudocode describing how \exedec{} might be extended to enable hierarchical decompositions. We leave exploration of this extension to future work.

\begin{lstlisting}
def ExeDecHierarchical(inputs, outputs):
  difficulty = DifficultyModel(inputs, outputs)  # outputs easy/hard
  if difficulty == 'easy':  # base case
    return SynthesizerModel(inputs, outputs)

  # recursive case
  subprograms = []
  while True:
    subgoals = SubgoalModel(inputs, outputs)
    subprogram = ExeDecHierarchical(inputs, subgoals)  # recurse
    subprograms.append(subprogram)
    execution_result = Execute(subprogram, inputs)
    if execution_result == outputs:
      return CombineProgramParts(subprograms)
    (inputs, outputs) = UpdateSpecification(inputs, outputs,
                                            execution_result)
\end{lstlisting}