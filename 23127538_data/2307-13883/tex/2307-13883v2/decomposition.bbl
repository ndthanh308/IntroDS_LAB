\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"{u}}rek et~al.(2021)Aky{\"{u}}rek, Aky{\"{u}}rek, and
  Andreas]{Akyrek2021Learning}
Ekin Aky{\"{u}}rek, Afra~Feyza Aky{\"{u}}rek, and Jacob Andreas.
\newblock Learning to recombine and resample data for compositional
  generalization.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2021.

\bibitem[Andreas(2020)]{Andreas2020GoodEnough}
Jacob Andreas.
\newblock Good-enough compositional data augmentation.
\newblock In \emph{Assocation for Computational Linguistics ({ACL})}, 2020.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, and Sutton]{Austin2021Program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles
  Sutton.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Bahdanau et~al.(2016)Bahdanau, Cho, and Bengio]{ATTENTION}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2016.

\bibitem[Bahdanau et~al.(2019)Bahdanau, de~Vries, O'Donnell, Murty, Beaudoin,
  Bengio, and Courville]{CLOSURE}
Dzmitry Bahdanau, Harm de~Vries, Timothy~J. O'Donnell, Shikhar Murty, Philippe
  Beaudoin, Yoshua Bengio, and Aaron~C. Courville.
\newblock {CLOSURE}: Assessing systematic generalization of {CLEVR} models.
\newblock \emph{arXiv preprint arXiv:1912.05783}, 2019.

\bibitem[Balog et~al.(2017)Balog, Gaunt, Brockschmidt, Nowozin, and
  Tarlow]{DEEPCODER}
Matej Balog, Alexander~L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and
  Daniel Tarlow.
\newblock {{DeepCoder}: Learning to write programs}.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2017.

\bibitem[Barke et~al.(2020)Barke, Peleg, and Polikarpova]{PROBE}
Shraddha Barke, Hila Peleg, and Nadia Polikarpova.
\newblock Just-in-time learning for bottom-up enumerative synthesis.
\newblock In \emph{Object-oriented Programming, Systems, Languages, and
  Applications ({{OOPSLA}})}, 2020.

\bibitem[Bieber et~al.(2020)Bieber, Sutton, Larochelle, and Tarlow]{IPAGNN}
David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow.
\newblock Learning to execute programs with instruction pointer attention graph
  neural networks.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan,
  de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri,
  Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power,
  Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes,
  Herbert{-}Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain,
  Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight,
  Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and
  Zaremba]{Chen2021Evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Pond{\'{e}}
  de~Oliveira~Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
  Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
  Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick
  Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
  Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings,
  Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert{-}Voss,
  William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
  Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
  Hesse, Andrew~N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
  Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
  Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
  Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2019)Chen, Liu, and Song]{Chen2019ExecutionGuided}
Xinyun Chen, Chang Liu, and Dawn Song.
\newblock Execution-guided neural program synthesis.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Chen et~al.(2020)Chen, Liang, Yu, Song, and
  Zhou]{Chen2020NeuralSymbolic}
Xinyun Chen, Chen Liang, Adams~Wei Yu, D.~Song, and Denny Zhou.
\newblock Compositional generalization via neural-symbolic stack machines.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Song, and Tian]{Chen2021Latent}
Xinyun Chen, Dawn Song, and Yuandong Tian.
\newblock Latent execution for neural program synthesis beyond domain-specific
  languages.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})},
  2021{\natexlab{b}}.

\bibitem[Chomsky(1957)]{Chomsky1957Syntactic}
Noam Chomsky.
\newblock \emph{Syntactic Structures}.
\newblock Mouton de Gruyter, 1957.

\bibitem[Conklin et~al.(2021)Conklin, Wang, Smith, and
  Titov]{Conklin2021MetaLearning}
Henry Conklin, Bailin Wang, Kenny Smith, and Ivan Titov.
\newblock Meta-learning to compositionally generalize.
\newblock In \emph{Association for Computational Linguistics and International
  Joint Conference on Natural Language Processing ({ACL/IJCNLP})}, 2021.

\bibitem[Csord{\'{a}}s et~al.(2021)Csord{\'{a}}s, Irie, and
  Schmidhuber]{Csordas2021Devil}
R{\'{o}}bert Csord{\'{a}}s, Kazuki Irie, and J{\"{u}}rgen Schmidhuber.
\newblock The devil is in the detail: Simple tricks improve systematic
  generalization of transformers.
\newblock In \emph{Empirical Methods in Natural Language Processing ({EMNLP})},
  2021.

\bibitem[Devlin et~al.(2017)Devlin, Uesato, Bhupatiraju, Singh, Mohamed, and
  Kohli]{ROBUSTFILL}
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel{-}rahman
  Mohamed, and Pushmeet Kohli.
\newblock {RobustFill}: Neural program learning under noisy {I/O}.
\newblock In \emph{International Conference on Machine Learning ({ICML})},
  2017.

\bibitem[Drozdov et~al.(2023)Drozdov, Sch{\"{a}}rli, Aky{\"{u}}rek, Scales,
  Song, Chen, Bousquet, and Zhou]{Drozdov2023Compositional}
Andrew Drozdov, Nathanael Sch{\"{a}}rli, Ekin Aky{\"{u}}rek, Nathan Scales,
  Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou.
\newblock Compositional semantic parsing with large language models.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2023.

\bibitem[Ellis et~al.(2019)Ellis, Nye, Pu, Sosa, Tenenbaum, and
  Solar{-}Lezama]{REPL}
Kevin Ellis, Maxwell~I. Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando
  Solar{-}Lezama.
\newblock Write, execute, assess: Program synthesis with a {REPL}.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Ellis et~al.(2021)Ellis, Wong, Nye, Sabl{\'{e}}{-}Meyer, Morales,
  Hewitt, Cary, Solar{-}Lezama, and Tenenbaum]{DREAMCODER}
Kevin Ellis, Catherine Wong, Maxwell~I. Nye, Mathias Sabl{\'{e}}{-}Meyer, Lucas
  Morales, Luke~B. Hewitt, Luc Cary, Armando Solar{-}Lezama, and Joshua~B.
  Tenenbaum.
\newblock {DreamCoder}: bootstrapping inductive program synthesis with
  wake-sleep library learning.
\newblock In \emph{Programming Language Design and Implementation ({PLDI})},
  2021.

\bibitem[Finegan-Dollak et~al.(2018)Finegan-Dollak, Kummerfeld, Zhang,
  Ramanathan, Sadasivam, Zhang, and Radev]{Finegan18Improving}
Catherine Finegan-Dollak, Jonathan~K. Kummerfeld, Li~Zhang, Karthik Ramanathan,
  Sesh Sadasivam, Rui Zhang, and Dragomir Radev.
\newblock Improving text-to-{SQL} evaluation methodology.
\newblock In \emph{Assocation for Computational Linguistics ({ACL})}, 2018.

\bibitem[Furrer et~al.(2020)Furrer, van Zee, Scales, and
  Sch{\"{a}}rli]{Furrer2020Compositional}
Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch{\"{a}}rli.
\newblock Compositional generalization in semantic parsing: Pre-training vs.
  specialized architectures.
\newblock \emph{arXiv preprint arXiv:2007.08970}, 2020.

\bibitem[Google et~al.(2023)Google, Anil, Dai, Firat, Johnson, Lepikhin,
  Passos, Shakeri, Taropa, Bailey, Chen, Chu, Clark, Shafey, Huang,
  Meier-Hellstern, Mishra, Moreira, Omernick, Robinson, Ruder, Tay, Xiao, Xu,
  Zhang, Abrego, Ahn, Austin, Barham, Botha, Bradbury, Brahma, Brooks, Catasta,
  Cheng, Cherry, Choquette-Choo, Chowdhery, Crepy, Dave, Dehghani, Dev, Devlin,
  Díaz, Du, Dyer, Feinberg, Feng, Fienber, Freitag, Garcia, Gehrmann,
  Gonzalez, Gur-Ari, Hand, Hashemi, Hou, Howland, Hu, Hui, Hurwitz, Isard,
  Ittycheriah, Jagielski, Jia, Kenealy, Krikun, Kudugunta, Lan, Lee, Lee, Li,
  Li, Li, Li, Li, Lim, Lin, Liu, Liu, Maggioni, Mahendru, Maynez, Misra,
  Moussalem, Nado, Nham, Ni, Nystrom, Parrish, Pellat, Polacek, Polozov, Pope,
  Qiao, Reif, Richter, Riley, Ros, Roy, Saeta, Samuel, Shelby, Slone, Smilkov,
  So, Sohn, Tokumine, Valter, Vasudevan, Vodrahalli, Wang, Wang, Wang, Wang,
  Wieting, Wu, Xu, Xu, Xue, Yin, Yu, Zhang, Zheng, Zheng, Zhou, Zhou, Petrov,
  and Wu]{PALM2}
Google, Rohan Anil, Andrew~M. Dai, Orhan Firat, Melvin Johnson, Dmitry
  Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey,
  Zhifeng Chen, Eric Chu, Jonathan~H. Clark, Laurent~El Shafey, Yanping Huang,
  Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin
  Robinson, Sebastian Ruder, Yi~Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
  Gustavo~Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,
  James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng,
  Colin Cherry, Christopher~A. Choquette-Choo, Aakanksha Chowdhery, Clément
  Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
  Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus
  Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,
  Steven Hand, Hadi Hashemi, Le~Hou, Joshua Howland, Andrea Hu, Jeffrey Hui,
  Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
  Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine
  Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek
  Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma
  Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John
  Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek,
  Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker
  Riley, Alex~Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee
  Shelby, Ambrose Slone, Daniel Smilkov, David~R. So, Daniel Sohn, Simon
  Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,
  Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan
  Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
  Ce~Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.
\newblock {PaLM} 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Gu et~al.(2021)Gu, Kase, Vanni, Sadler, Liang, Yan, and
  Su]{Gu2021Beyond}
Yu~Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and
  Yu~Su.
\newblock Beyond {IID}: three levels of generalization for question answering
  on knowledge bases.
\newblock In \emph{The Web Conference ({WWW})}, 2021.

\bibitem[Gulwani(2011)]{FLASHFILL}
Sumit Gulwani.
\newblock Automating string processing in spreadsheets using input-output
  examples.
\newblock In \emph{Principles of Programming Languages ({POPL})}, 2011.

\bibitem[Gulwani et~al.(2017)Gulwani, Polozov, and Singh]{RISHABHSURVEY}
Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh.
\newblock \emph{Program Synthesis}, volume~4 of \emph{Foundations and
  Trends{\textregistered} in Programming Languages}.
\newblock 2017.

\bibitem[Herzig \& Berant(2020)Herzig and Berant]{Herzig2020SpanBased}
Jonathan Herzig and Jonathan Berant.
\newblock Span-based semantic parsing for compositional generalization.
\newblock In \emph{Empirical Methods in Natural Language Processing ({EMNLP})},
  2020.

\bibitem[Herzig et~al.(2021)Herzig, Shaw, Chang, Guu, Pasupat, and
  Zhang]{Herzig2021Unlocking}
Jonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin Guu, Panupong Pasupat, and
  Yuan Zhang.
\newblock Unlocking compositional generalization in pre-trained models using
  intermediate representations.
\newblock \emph{arXiv preprint arXiv:2104.07478}, 2021.

\bibitem[Hong et~al.(2021)Hong, Dohan, Singh, Sutton, and
  Zaheer]{LATENTPROGRAMMER}
Joey Hong, David Dohan, Rishabh Singh, Charles Sutton, and Manzil Zaheer.
\newblock Latent programmer: Discrete latent codes for program synthesis.
\newblock In \emph{International Conference on Machine Learning ({ICML})},
  2021.

\bibitem[Keysers et~al.(2020)Keysers, Sch{\"a}rli, Scales, Buisman, Furrer,
  Kashubin, Momchev, Sinopalnikov, Stafiniak, Tihon, Tsarkov, Wang, van Zee,
  and Bousquet]{Keysers2020Measuring}
Daniel Keysers, Nathanael Sch{\"a}rli, Nathan Scales, H.~Buisman, Daniel
  Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz
  Stafiniak, Tibor Tihon, D.~Tsarkov, Xiao Wang, Marc van Zee, and O.~Bousquet.
\newblock Measuring compositional generalization: A comprehensive method on
  realistic data.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Kim \& Linzen(2020)Kim and Linzen]{COGS}
Najoung Kim and Tal Linzen.
\newblock {COGS}: A compositional generalization challenge based on semantic
  interpretation.
\newblock In \emph{Empirical Methods in Natural Language Processing ({EMNLP})},
  2020.

\bibitem[Lake(2019)]{Lake2019Compositional}
Brenden~M. Lake.
\newblock Compositional generalization through meta sequence-to-sequence
  learning.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2019.

\bibitem[Lake \& Baroni(2018)Lake and Baroni]{SCAN}
Brenden~M. Lake and Marco Baroni.
\newblock Generalization without systematicity: On the compositional skills of
  sequence-to-sequence recurrent networks.
\newblock In \emph{International Conference on Machine Learning ({ICML})},
  2018.

\bibitem[Lee et~al.(2018)Lee, Heo, Alur, and Naik]{EUPHONY}
Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik.
\newblock Accelerating search-based program synthesis using learned
  probabilistic models.
\newblock In \emph{Programming Language Design and Implementation ({PLDI})},
  2018.

\bibitem[Li et~al.(2019)Li, Zhao, Wang, and Hestness]{Li2019Compositional}
Yuanpeng Li, Liang Zhao, JianYu Wang, and Joel Hestness.
\newblock Compositional generalization for primitive substitutions.
\newblock In \emph{Empirical Methods in Natural Language Processing and
  International Joint Conference on Natural Language Processing
  ({EMNLP-IJCNLP})}, 2019.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond,
  Eccles, Keeling, Gimeno, Lago, Hubert, Choy, de~Masson~d'Autume, Babuschkin,
  Chen, Huang, Welbl, Gowal, Cherepanov, Molloy, Mankowitz, Robson, Kohli,
  de~Freitas, Kavukcuoglu, and Vinyals]{ALPHACODE}
Yujia Li, David~H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'{e}}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin~Dal
  Lago, Thomas Hubert, Peter Choy, Cyprien de~Masson~d'Autume, Igor Babuschkin,
  Xinyun Chen, Po{-}Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov,
  James Molloy, Daniel~J. Mankowitz, Esme~Sutherland Robson, Pushmeet Kohli,
  Nando de~Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
\newblock Competition-level code generation with {AlphaCode}.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092--1097, 2022.

\bibitem[Liu et~al.(2020)Liu, An, Lou, Chen, Lin, Gao, Zhou, Zheng, and
  Zhang]{Liu2020Compositional}
Qian Liu, Shengnan An, Jianguang Lou, B.~Chen, Zeqi Lin, Yan Gao, Bin Zhou,
  Nanning Zheng, and Dongmei Zhang.
\newblock Compositional generalization by learning analytical expressions.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2020.

\bibitem[Murali et~al.(2018)Murali, Qi, Chaudhuri, and Jermaine]{BAYOU}
Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine.
\newblock Neural sketch learning for conditional program generation.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2018.

\bibitem[Nye et~al.(2019)Nye, Hewitt, Tenenbaum, and
  Solar{-}Lezama]{SKETCHADAPT}
Maxwell Nye, Luke Hewitt, Joshua~B. Tenenbaum, and Armando Solar{-}Lezama.
\newblock Learning to infer program sketches.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Nye et~al.(2021)Nye, Pu, Bowers, Andreas, Tenenbaum, and
  Solar-Lezama]{Nye2021Blended}
Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua~B. Tenenbaum, and
  Armando Solar-Lezama.
\newblock Representing partial programs with blended abstract semantics.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2021.

\bibitem[Odena \& Sutton(2020)Odena and Sutton]{SIGNATURES}
Augustus Odena and Charles Sutton.
\newblock Learning to represent programs with property signatures.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Odena et~al.(2020)Odena, Shi, Bieber, Singh, Sutton, and Dai]{BUSTLE}
Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles Sutton, and
  Hanjun Dai.
\newblock {BUSTLE}: Bottom-up program synthesis through learning-guided
  exploration.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Oren et~al.(2021)Oren, Herzig, and Berant]{Oren2021Finding}
Inbar Oren, Jonathan Herzig, and Jonathan Berant.
\newblock Finding needles in a haystack: Sampling structurally-diverse training
  sets from synthetic data for compositional generalization.
\newblock In \emph{Empirical Methods in Natural Language Processing ({EMNLP})},
  2021.

\bibitem[Parisotto et~al.(2017)Parisotto, Mohamed, Singh, Li, Zhou, and
  Kohli]{Parisotto2017NeuroSymbolic}
Emilio Parisotto, Abdel{-}rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong
  Zhou, and Pushmeet Kohli.
\newblock Neuro-symbolic program synthesis.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2017.

\bibitem[Parlante(2022)]{cs106a}
Nick Parlante.
\newblock {CS106A}: Programming methodologies.
\newblock Course at Stanford University, 2022.

\bibitem[Qiu et~al.(2022)Qiu, Shaw, Pasupat, Nowak, Linzen, Sha, and
  Toutanova]{Qiu2022Improving}
Linlu Qiu, Peter Shaw, Panupong Pasupat, Pawel~Krzysztof Nowak, Tal Linzen, Fei
  Sha, and Kristina Toutanova.
\newblock Improving compositional generalization with latent structure and data
  augmentation.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies ({NAACL-HLT})}, 2022.

\bibitem[Russin et~al.(2019)Russin, Jo, O'Reilly, and
  Bengio]{Russin2019Compositional}
Jake Russin, Jason Jo, Randall O'Reilly, and Yoshua Bengio.
\newblock Compositional generalization in a deep seq2seq model by separating
  syntax and semantics.
\newblock \emph{arXiv preprint arXiv:1904.09708}, 2019.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{Shaw2018Relative}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics ({NAACL})}, 2018.

\bibitem[Shaw et~al.(2021)Shaw, Chang, Pasupat, and
  Toutanova]{Shaw2021Compositional}
Peter Shaw, Ming{-}Wei Chang, Panupong Pasupat, and Kristina Toutanova.
\newblock Compositional generalization and natural language variation: Can a
  semantic parsing approach handle both?
\newblock In \emph{Association for Computational Linguistics and International
  Joint Conference on Natural Language Processing ({ACL/IJCNLP})}, 2021.

\bibitem[Shi et~al.(2019)Shi, Steinhardt, and Liang]{FRANGEL}
Kensen Shi, Jacob Steinhardt, and Percy Liang.
\newblock {FrAngel}: Component-based synthesis with control structures.
\newblock \emph{Proceedings of the ACM on Programming Languages (PACMPL)},
  3\penalty0 (POPL):\penalty0 1--29, 2019.

\bibitem[Shi et~al.(2022{\natexlab{a}})Shi, Bieber, and Singh]{TFCODER}
Kensen Shi, David Bieber, and Rishabh Singh.
\newblock {TF-Coder}: Program synthesis for tensor manipulations.
\newblock \emph{ACM Transactions on Programming Languages and Systems
  (TOPLAS)}, 44\penalty0 (2):\penalty0 1--36, 2022{\natexlab{a}}.

\bibitem[Shi et~al.(2022{\natexlab{b}})Shi, Dai, Ellis, and Sutton]{CROSSBEAM}
Kensen Shi, Hanjun Dai, Kevin Ellis, and Charles Sutton.
\newblock {CrossBeam}: Learning to search in bottom-up program synthesis.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2022{\natexlab{b}}.

\bibitem[Shin et~al.(2018)Shin, Polosukhin, and Song]{Shin2018Improving}
Richard Shin, Illia Polosukhin, and Dawn Song.
\newblock Improving neural program synthesis with inferred execution traces.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2018.

\bibitem[Shrivastava et~al.(2021)Shrivastava, Larochelle, and Tarlow]{PEPS}
Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
\newblock Learning to combine per-example solutions for neural program
  synthesis.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2021.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and Le]{BEAMSEARCH}
Ilya Sutskever, Oriol Vinyals, and Quoc~V. Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{TRANSFORMER}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Neural Information Processing Systems ({NeurIPS})}, 2017.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Lapata, and
  Titov]{Wang2020MetaLearning}
Bailin Wang, Mirella Lapata, and Ivan Titov.
\newblock Meta-learning for domain generalization in semantic parsing.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies ({NAACL-HLT})}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Yin, Lin, and
  Xiong]{Wang2021Learning}
Bailin Wang, Wenpeng Yin, Xi~Victoria Lin, and Caiming Xiong.
\newblock Learning to synthesize data for semantic parsing.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies ({NAACL-HLT})}, 2021{\natexlab{b}}.

\bibitem[Yin \& Neubig(2017)Yin and Neubig]{Yin2017Syntactic}
Pengcheng Yin and Graham Neubig.
\newblock A syntactic neural model for general-purpose code generation.
\newblock In \emph{Assocation for Computational Linguistics ({ACL})}, 2017.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Shen, Ding, Tenenbaum, and
  Gan]{Zhang2023Planning}
Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua~B. Tenenbaum, and
  Chuang Gan.
\newblock Planning with large language models for code generation.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Sch{\"{a}}rli, Hou, Wei, Scales, Wang,
  Schuurmans, Cui, Bousquet, Le, and Chi]{Zhou2022LeasttoMost}
Denny Zhou, Nathanael Sch{\"{a}}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi
  Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc~V. Le, and Ed~H.
  Chi.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2023.

\bibitem[Zohar \& Wolf(2018)Zohar and Wolf]{GARBAGECOLLECTOR}
Amit Zohar and Lior Wolf.
\newblock Automatic program synthesis of long programs with a learned garbage
  collector.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\end{thebibliography}
