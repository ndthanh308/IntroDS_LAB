\clearpage

\appendix
\appendixpage

\section{RobustFill and DeepCoder DSLs}
\label{app:dsls}

\autoref{fig:robustfill_dsl} contains the DSL for the RobustFill experiments. See \citet{Devlin2017Robustfill} for a description of what the operations do. We add the operations \code{Substitute}, \code{SubstituteAll}, which replace the $i^\text{th}$ occurrence (or all occurrences) of a regex $r$ with character $c$, and \code{Remove}, \code{RemoveAll}, which remove the $i^\text{th}$ occurrence (or all occurrences) of a regex $r$, to enhance the expressivity of our DSL.

\autoref{fig:deepcoder_dsl} shows the DSL used for the DeepCoder experiments. The operations are exactly as described in \citet{Balog2017Deepcoder}.

% Figure environment removed

% Figure environment removed

\clearpage

\section{Algorithm Comparison Case Study}
\label{app:case_study}

\paragraph{RobustFill.}
\autoref{fig:comparison_1} and \autoref{fig:comparison_2} compare how \exedec{}, the no-subgoal ablation, and the Transformer baseline perform on two example problems from the RobustFill DSL, under the \ComposeNewOp{} generalization task where models were not trained on composed programs (other than length-1 programs) that use the \code{Compose} operation. A beam size of 1 (greedy decoding) is used for simplicity. In the figures, subprograms and execution results are colored red if they are incorrect. Portions of the final programs are colored yellow if they do execute correctly but implement the desired functionality suboptimally, i.e., with more subprograms than necessary.

For both example problems, \exedec{} successfully solves the problem using a minimal-length program that is within the test distribution of the compositional generalization task, showing successful generalization. However, the no-subgoal ablation does not perform as well. In \autoref{fig:comparison_1}, it uses the \code{Compose} operation incorrectly because, due to the \ComposeNewOp{} task, it has never been trained to use \code{Compose} to produce a \emph{prefix} of the output. This is not an issue for \exedec{} because the prefixes are provided by the SubgoalModel. In \autoref{fig:comparison_2}, the ablation predicts a correct but suboptimal program in the training distribution, replacing a \code{Compose} operation with three separate steps. The Transformer baseline is comparatively the worst, predicting incorrect programs in the training distribution for both problems, showing a clear inability to compositionally generalize.

\paragraph{DeepCoder.}
\autoref{fig:comparison_3} shows a similar comparison between the three approaches, this time on a DeepCoder list manipulation problem under the \ComposeDiffConcepts{} generalization task. For this generalization task, we partition the DSL operations into two groups or ``concepts'' -- one concept contains the higher-order operations \code{Scanl1}, \code{Filter}, \code{Count}, and \code{ZipWith}, while the other concept contains all of the first-order operations\footnote{The concept with first-order operations also contains the higher-order operation \code{Map}, to better balance the number of different program functionalities obtainable within each concept.}. The training problems have minimal-length ground-truth programs that only compose operations within the same concept, while test problems require mixing operations from different concepts to achieve a minimal-length solution program.

The problem in \autoref{fig:comparison_3} is solved using operations from both concepts: \code{Scanl1} is higher-order, while \code{Take} and \code{Sort} are first-order operations.
The Transformer baseline fails to solve this problem. It correctly uses \code{Scanl1} as the first step but incorrectly continues to use higher-order operations. This behavior makes sense because the model was trained on programs showing a similar pattern, and it does not deviate from the pattern because this approach is not compositionally general.

Similarly, the no-subgoal ablation correctly uses \code{Scanl1} in the first step but also continues to use higher-order operations in subsequent steps until eventually the program fails to typecheck. We observe that, after successfully using \code{Scanl1} in the first subprogram, the updated specification actually describes a problem with a solution program within the training distribution (using first-order operations \code{Take} and \code{Sort} from the same concept). So, why is the ablation unable to solve the updated specification, even though the SynthesizerModel is not directly conditioned on the previous subprograms like the Transformer baseline is? We hypothesize that the SynthesizerModel recognizes from the specification that \code{x2} was computed by applying a \code{Scanl1} operation to \code{x0}, and thus according to patterns seen during training, the model is inclined to continue using higher-order operations. This hypothesis is supported by the fact that the ablation, as well as \exedec{} and the baseline, all correctly solve a simplified version of this problem where the input \code{x0} is replaced with the result of the first subprogram, \code{x2 = Scanl1 (+) x0}, such that the \code{Scanl1} portion is ``already computed'' but not in a way visible from the specification. In other words, the ablation succeeds for the simplified problem because it is now unable to refer to previous subprograms.

In contrast, \exedec{} successfully solves the problem with a minimal-length solution that switches between concepts. Note that its SynthesizerModel cannot directly or indirectly reference previous subprograms. The SubgoalModel \emph{can} indirectly refer to previous subprograms by examining the specification, but it is generally less susceptible to spurious compositional patterns. For example, the ablation's SynthesizerModel might learn the pattern ``predict \emph{an operation} in the same concept as previously'' which is very easy for a neural model to recognize and implement, whereas it is more difficult for \exedec{}'s SubgoalModel to learn the pattern ``predict \emph{a subgoal that is implementable with an operation} in the same concept as previously''. This provides some intuition for why \exedec{} is more compositionally general overall.

% Figure environment removed

% Figure environment removed

% Figure environment removed
