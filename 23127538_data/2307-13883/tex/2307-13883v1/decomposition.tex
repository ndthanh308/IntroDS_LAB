\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{physics}
\usepackage{arydshln}
\usepackage{textcomp}
\usepackage{url}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{appendix}
\usepackage{enumitem}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{subcaption}

% Autoref tweaks
\renewcommand{\sectionautorefname}{Section}
\let\subsectionautorefname\sectionautorefname
\let\subsubsectionautorefname\sectionautorefname
\newcommand{\algorithmautorefname}{Algorithm}

% Paper-specific custom things
\newcommand\std[1]{\ensuremath{\color{gray} \scriptstyle \pm #1}}
\renewcommand{\tt}[1]{\fontfamily{cmtt}\selectfont #1}
\def\programs{{\mathcal{P}}}
\def\specs{{\mathcal{X}}}
\newcommand{\logicalOR}{\; | \;}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\exedec}{ExeDec}

% Colored text
\usepackage{xcolor}
\colorlet{codecolor}{blue!60!black}
\colorlet{speccolor}{green!40!black}
\colorlet{taskcolor}{red!40!black}
\newcommand{\code}[1]{\texttt{\small \color{codecolor} #1}}
\newcommand{\str}[1]{{\color{speccolor}``#1"}}
\newcommand{\spec}[1]{{\color{speccolor}#1}}
\newcommand{\LengthGen}{{\color{taskcolor}\emph{Length-Generalization}}}
\newcommand{\ComposeDiffConcepts}{{\color{taskcolor}\emph{Compose-Different-Concepts}}}
\newcommand{\SwitchConceptOrder}{{\color{taskcolor}\emph{Switch-Concept-Order}}}
\newcommand{\ComposeNewOp}{{\color{taskcolor}\emph{Compose-New-Operation}}}
\newcommand{\AddOpFunctionality}{{\color{taskcolor}\emph{Add-Operation-Functionality}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Begin Algorithm stuff (mostly copied from old papers)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx}
\usepackage{float}
\newfloat{algorithm}{t}{}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\Input{\item[\algorithmicinput]}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\Output{\item[\algorithmicoutput]}
\algnewcommand\algorithmicdata{\textbf{Auxiliary Data:}}
\algnewcommand\Data{\item[\algorithmicdata]}
\algblockdefx{RepeatUntilTimeout}{EndRepeat}{\textbf{repeat until timeout}}{}
\algblockdefx{RepeatUntil}{EndRepeat}{\textbf{repeat until }}{}
\algtext*{EndRepeat}
\makeatletter
\algnewcommand{\LineComment}[1]{\Statex \hskip\ALG@thistlm \(\triangleright\) #1}
\makeatother

\usepackage{etoolbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\usetikzlibrary{calc}

% begin vertical rule patch for algorithmicx (http://tex.stackexchange.com/questions/144840)
% note that some of the packages above are also needed
\newcommand{\ALGtikzmarkcolor}{lightgray}% customise this, if you want
\newcommand{\ALGtikzmarkextraindent}{4pt}% customise this, if you want
\newcommand{\ALGtikzmarkverticaloffsetstart}{-.7ex}% customise this, if you want
\newcommand{\ALGtikzmarkverticaloffsetend}{-.5ex}% customise this, if you want
\makeatletter
\newcounter{ALG@tikzmark@tempcnta}

\newcommand\ALG@tikzmark@start{%
    \global\let\ALG@tikzmark@last\ALG@tikzmark@starttext%
    \expandafter\edef\csname ALG@tikzmark@\theALG@nested\endcsname{\theALG@tikzmark@tempcnta}%
    \tikzmark{ALG@tikzmark@start@\csname ALG@tikzmark@\theALG@nested\endcsname}%
    \addtocounter{ALG@tikzmark@tempcnta}{1}%
}

\def\ALG@tikzmark@starttext{start}
\newcommand\ALG@tikzmark@end{%
    \ifx\ALG@tikzmark@last\ALG@tikzmark@starttext
        % ignore this, the block was opened then closed directly without any other blocks in between (so just a \State basically)
        % don't draw a vertical line here
    \else
        \tikzmark{ALG@tikzmark@end@\csname ALG@tikzmark@\theALG@nested\endcsname}%
        \tikz[overlay,remember picture] \draw[\ALGtikzmarkcolor] let \p{S}=($(pic cs:ALG@tikzmark@start@\csname ALG@tikzmark@\theALG@nested\endcsname)+(\ALGtikzmarkextraindent,\ALGtikzmarkverticaloffsetstart)$), \p{E}=($(pic cs:ALG@tikzmark@end@\csname ALG@tikzmark@\theALG@nested\endcsname)+(\ALGtikzmarkextraindent,\ALGtikzmarkverticaloffsetend)$) in (\x{S},\y{S})--(\x{S},\y{E});%
    \fi
    \gdef\ALG@tikzmark@last{end}%
}

% the following line injects our new tikzmarking code
\apptocmd{\ALG@beginblock}{\ALG@tikzmark@start}{}{\errmessage{failed to patch}}
\pretocmd{\ALG@endblock}{\ALG@tikzmark@end}{}{\errmessage{failed to patch}}
\makeatother
% end vertical rule patch for algorithmicx

% Left-justified comments
\algnewcommand{\LeftComment}[1]{\Statex \(\triangleright\) #1}

% Change indentation
\algrenewcommand\algorithmicindent{1em}

% Keep black line numbers, even if we change line colors
\algrenewcommand{\alglinenumber}[1]{\color{black}\footnotesize#1:}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% END Algorithm stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TLDR sentence: We describe different forms of compositional generalization that are desirable in program synthesis, and present a decomposition-based approach to synthesis achieving higher compositional generalization on two domains compared to prior approaches.

\title{\exedec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis}

\author{%
Kensen Shi \\
Google DeepMind \\
\small\texttt{kshi@google.com}
\And
Joey Hong \\
UC Berkeley \\
\small\texttt{joey\_hong@berkeley.edu}
\And
Manzil Zaheer \\
Google DeepMind \\
\small\texttt{manzilzaheer@google.com}
\And
Pengcheng Yin \\
Google DeepMind \\
\small\texttt{pcyin@google.com}
\And
Charles Sutton \\
Google DeepMind \\
\small\texttt{charlessutton@google.com}
}

\begin{document}


\maketitle

\begin{abstract}
When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose \exedec{}, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. \exedec{} has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\emph{Program synthesis} aims to assist programmers by automatically producing code according to a user's specification of what the code should do~\citep{gulwani2017program}.
Program synthesis systems, such as programming by example (PBE) systems, have been effective 
for tasks such as  string manipulation~\citep{FLASHFILL,Devlin2017Robustfill,CROSSBEAM}, 
writing short Java functions~\citep{FRANGEL},
and tensor manipulation~\citep{TFCODER}.
Neural program synthesizers, especially those based on large language models~\citep{chen2021evaluating,Austin2021-fu,ALPHACODE}, have been particularly successful at
generating code functions and blocks across a variety of general-purpose programming languages.

An essential capability of human programmers is their ability to generalize by recombining parts of prior knowledge to solve new tasks. 
For example, a capable programmer can quickly adapt to new concepts and APIs, and
compose different code idioms in unseen ways to solve novel problems. 
These skills are instances of \emph{compositional generalization}, which is the ability to generalize to test examples consisting of different compositions of components individually seen during training~\citep{nic2004compositionality}.
To build neural synthesis systems that are better at compositional generalization,
we propose designing systems that learn to
\emph{decompose} a complex task into a list of simpler subtasks.
Each subtask is defined by a goal, so the process of decomposing a task is essentially planning.
Indeed, decomposition is a skill so fundamental to software engineering that the first programming course at Stanford University begins teaching decomposition within the first week of class, immediately after introducing basic syntax, the concept of functions, and simple control flow~\citep{cs106a}. 
This can enable
compositional generalization because subtasks used for solving the
training set can be combined in different ways on the test set.

Based on this intuition, we propose \exedec{}, a novel search method
for neural program synthesis that performs decomposition within the \emph{execution space}.
A PBE task defines a program by pairs of program inputs with their desired outputs.
Thus, it is natural to describe a subgoal by the desired \emph{intermediate state}, i.e.,
values of local variables, for the next subtask. To describe the intuition in another way,
we imagine that a human programmer does not decide on what code to write one token at a time,
but rather thinks about what the result of the next code block should be, and then writes code
to accomplish that.
Specifically,
\exedec{} uses two neural models, a \emph{subgoal model} that predicts the desired program state
for the next part of the program, and a \emph{synthesizer model} that attempts to generate
a program that reaches that subgoal from the prior state. We interleave neural prediction with program execution within a beam search that enables exploring different predicted decompositions. 

To evaluate this approach, we introduce a new meta-benchmark 
for measuring the compositional generalization abilities of program synthesizers.
Given a standard program synthesis benchmark containing a domain-specific language 
and a distribution over target programs, our meta-benchmark describes train-test splits for 5 different types of compositional generalization, such as length generalization or composing API functions in different combinations in the training and test sets.
Interestingly, while \exedec{} has slightly better performance over a Transformer baseline in the i.i.d.\ setting, \exedec{} also achieves a $2\times$ to $4\times$ accuracy increase in the compositional generalization setting.
Additionally, \exedec{} improves upon an ablation that does not explicitly propose subgoals, showing the importance of reasoning about execution subgoals instead of directly predicting code.

\section{Compositional Generalization in Programming}
\label{sec:generalization}

The goal in program synthesis is to find a program in a given language that is consistent with a specification.
Formally, we are given a domain specific language (DSL) which defines a space $\programs$ of programs. Elements in the DSL include functions, which we call \emph{operations}, identifiers, constants, and so on.
The task is described by a specification $X \in \specs$ and is solved by an unknown program $P^* \in \programs$.
More specifically, in programming by example (PBE), a specification is a set of input/output (I/O) examples denoted $X = \{(I_1, O_1), \hdots (I_n, O_n)\}$. Then, solving specification $X$ means finding a program $P$ (not necessarily $P^*$) that correctly solves all of the examples: $P(I_i) = O_i, \ \forall i$.

Program synthesizers are more robust and more broadly applicable if they generalize well. In this section we discuss how to test different generalization capabilities that are desirable for synthesizers to have.
At a high level, these capabilities can be grouped into three broad categories of generalization.
(1) \emph{Length generalization}: produce longer code than appeared in the training set, but without a prohibitive increase in computational cost as would occur for combinatorial approaches.
(2) \emph{Mix and match concepts}: compose code idioms in novel ways to solve novel problems, but without combinatorially many training examples covering all combinations.
(3) \emph{Apply general principles}: quickly adapt to new, updated, or custom APIs by drawing on knowledge of other similar APIs.

These kinds of generalization are different instances of \emph{compositional generalization}, which revolves around understanding how basic building blocks can be composed in different ways to create larger structures. Prior work in natural language processing has studied compositionality in natural language~\citep{chomsky2002syntactic,Lake2018Scan,keysers20cfq,gu2021beyond}.
We want to do the same in the context of program synthesis, focusing on how problem-solving (in the form of programs) is compositional. Regardless of the programming language or DSL, programs nearly always consist of compositions of smaller parts. In general, a ``subprogram'' could mean a line of code, a block of statements, a function, or some other natural DSL-specific portion of code, where multiple such portions can be combined into a full program. 

From the three broad categories of generalization above, we devise five concrete compositional generalization tasks applicable to program synthesis. For each generalization task, we describe a way of creating training and test sets with disjoint distributions. This forms a \emph{meta-benchmark}: with these descriptions, one may construct compositional generalization benchmarks using existing program synthesis datasets or DSLs.
\autoref{fig:generalization_tasks} illustrates the tasks, described in more detail below:

% Figure environment removed

\begin{enumerate}[leftmargin=1.2em,itemsep=0em]
\item \textbf{\LengthGen}:
Can a model \emph{produce longer code} than seen in training, when necessary?
Here, ``length'' counts the number of subprograms and not the number of tokens, so there is more emphasis on generalizing to more complex compositional patterns. For this task, we train on problems of lengths 1 to $n$ and test on lengths $n+1$ to $m$ (where $m > n$).

\item \textbf{\ComposeDiffConcepts} (a form of ``mix and match concepts''):
Can a model \emph{use concepts in different combinations} than seen in training?
We partition the DSL operations into multiple groups or \emph{concepts},\footnote{Ideally, operations within a group should have meaningful commonalities that form one concept, and each concept should have roughly equal semantic complexity, but these are not strictly required.} train on compositions of operations from the same concept, and test on compositions from different concepts. For example, if two concepts consist of operations $\{A_1, A_2, \ldots\}$ and $\{B_1, B_2, \dots\}$ respectively, then this generalization task involves training on programs of the forms $A_i \,\circ\, A_j$ and $B_i \,\circ\, B_j$, and testing on the forms $A_i \,\circ\, B_j$ and $B_i \,\circ\, A_j$ (and similarly for compositions of 3 or more operations). As a real-world example, this task is similar to training on scripts containing only TensorFlow or only NumPy, but synthesizing code using both libraries.

\item \textbf{\SwitchConceptOrder} (a form of ``mix and match concepts''):
Can a model \emph{compose concepts in different orders} than seen in training?
We again partition the DSL operations into multiple concepts. We train on compositions of operations drawn from one sequence of concepts and test on a different sequence of concepts, e.g., train on $A_i \,\circ\, B_j$ and test on $B_i \,\circ\, A_j$. As a real-world example, in the training data a function might be validating inputs at the beginning of the code, but we want to use the function in a different context, e.g., to validate results at the end.

\item \textbf{\ComposeNewOp} (a form of ``apply general principles''):
Can a model learn to \emph{use a new isolated operation within a larger composition}?
In this task, we train on the isolated operation and compositions without the operation, and test on compositions using the operation.
A real-world example of this kind of generalization would be composing a new function with others in a larger solution, after seeing examples of the function used in isolation.

\item \textbf{\AddOpFunctionality} (a form of ``apply general principles''):
Can a model \emph{extend its understanding of an operation by drawing on parallels} to other operations?
We omit from the training data some functionality of an operation that could be inferred from other context, and test on programs using that functionality.
This task can occur when a library function is upgraded with a new parameter whose behavior can be inferred from analogous parameters in other functions.

\end{enumerate}

\section{Benchmark Creation}
\label{sec:benchmarks}

While \autoref{sec:generalization} focused on the meta-benchmark framework for five forms of compositional generalization, this section describes our instantiation of those tasks into compositional generalization benchmarks for two popular program synthesis domains, RobustFill~\citep{Devlin2017Robustfill} and DeepCoder~\citep{DEEPCODER}.

\textbf{RobustFill.}\quad
In the RobustFill domain, the objective is to synthesize a sequence of string manipulation operations from I/O examples, where each example's input is a single string. A RobustFill program is a concatenation of expressions. There are 4 categories of expressions: an operation that extracts a substring from the input (e.g., \code{GetUpto(\emph{regex})}), an operation that returns a modified version of the input (e.g., \code{ToCase(\emph{case})}), a special \code{Compose} operation (applying a modification operation to the result of another operation),
or a constant string character. As an example, the program \code{GetToken(WORD, -1) | Const(\textquotesingle\ \textquotesingle) | Compose(ToCase(PROPER), GetUpto(\textquotesingle,\textquotesingle))} is a concatenation of 3 expressions and transforms the input string \str{TURING, Alan} into the output string \str{Alan Turing}. See \autoref{app:dsls} for the full RobustFill DSL, which we extended from the original RobustFill paper~\cite{Devlin2017Robustfill} by adding more operations.

To create a task for our compositional generalization datasets, we first sample random input strings up to 20 characters, one string for each of 4 examples. We then sample a program according to the train or test distribution for the generalization task (as described below), such that the program executes successfully on the inputs to form the example outputs. Due to the concatenation structure of RobustFill programs, we treat each concatenated expression as a subprogram, and recall that we denote the \emph{length} of a program to be the number of subprograms.

For \LengthGen, we train on programs of length 1 to 6 inclusive and test on programs of length 7 to 10. For \ComposeDiffConcepts, we group together all of the substring operations into a \emph{substring concept} and all of the modification operations plus constant strings as a \emph{non-substring concept} (the \code{Compose} operation is omitted), using programs of length 2-6 for both train and test. We use the same lengths and concepts for \SwitchConceptOrder, where training tasks use only the substring concept for the first half of the parts and only the non-substring concept for the latter half, and test tasks have the ordering reversed.
For \ComposeNewOp, 25\% of training tasks are length 1 programs containing only a \code{Compose} operation, the remainder of the training tasks are length 2-6 programs without \code{Compose}, and we test on length 2-6 programs that use \code{Compose}. For \AddOpFunctionality, all tasks are length 1-6 programs, we train on those where a substring operation is \emph{not} used within a \code{Compose} operation, and we test on programs where a substring operation \emph{is} used within a \code{Compose} operation.


\textbf{DeepCoder.}\quad
The DeepCoder domain involves manipulation of integer lists in a line-by-line programming style. Tasks have one or more inputs which may be integers or integer lists. Each line of a DeepCoder program applies one DSL operation to inputs or previous variables and assigns the result to a new variable. The result of the last line is the program's output. Operations include first-order list operations (\code{Sort}, \code{Reverse}, and various forms of indexing, slicing, and aggregating) and higher-order operations (Haskell-inspired \code{Map}, \code{Filter}, \code{Count}, \code{ZipWith}, and \code{Scanl1}) which manipulate lists using one of several hardcoded lambda functions. As an example, the program \code{x1 = INPUT | x2 = Map (**2) x1 | x3 = Sort x2} (where ``\code{|}'' denotes a new line) transforms the input list \spec{$[5, 3, -4]$} into the output list \spec{$[9, 16, 25]$}. See \autoref{app:dsls} for the full DeepCoder DSL.

For DeepCoder, we treat each non-input line as a subprogram, so the example program above has length 2.
We use 3 I/O examples, at most 2 inputs, lists with at most 5 elements, and integers between $-50$ and $50$ inclusive.
For \LengthGen, we train on programs of length 1 to 4 and test on length 5.
For \ComposeDiffConcepts\ and \SwitchConceptOrder, we use programs of length 1 to 4 and split the operations into a concept containing all first-order operations plus the \code{Map} operation, and another concept containing all remaining higher-order operations.
For \ComposeNewOp, 25\% of training tasks are length 1 programs containing only a \code{Scanl1} operation, the remainder of training tasks are length 2-4 programs without \code{Scanl1}, and we test on length 2-4 programs that use \code{Scanl1}. For \AddOpFunctionality, all tasks are length 1-4 programs, \code{Scanl1} is only used with the lambdas \code{(-)} and \code{(min)} during training, and we test on tasks where \code{Scanl1} is used with the other lambdas \code{(+)}, \code{(*)}, and \code{(max)}.

The program sampling procedure that we used for RobustFill ensures that the ground-truth program for a test task is within the test distribution over programs. However, the task might actually have a different solution within the \emph{train} distribution. Then, solving this test task is not necessarily a signal of generalization.\footnote{This issue is far less prevalent in the RobustFill dataset due to the concatenation program structure reducing the number of different programs that solve a task.}
To address this, we construct the DeepCoder dataset more carefully as follows.
We sample random inputs as before. Then, we perform an exhaustive enumerative search of all programs in the train distribution up to a maximum length, and similarly for programs in the test distribution. As a result, we can identify all minimal-length solution programs for a given task (if it is solvable by any program up to the maximum enumerated length). Finally, we sample training tasks among those where there exists a minimal-length solution program in the train distribution, and test tasks among those where \emph{all} minimal-length solutions are in the test distribution.\footnote{Note that it is still possible for a test task to be solved with a \emph{longer} program in the train distribution, but detecting these cases in general is extremely difficult.
}

\textbf{Choice of Domains.}\quad
Both domains allow us to generate a large amount of synthetic training data with ground-truth decompositions into subprograms. For more realistic code in general-purpose programming languages, such data collection requires more effort, especially if ``natural'' decompositions are desired.
Beyond the difference in string versus list manipulation, RobustFill and DeepCoder are quite different in other important ways, allowing us to study the compositional generalization of various approaches in different scenarios. First, RobustFill gradually builds an output by combining results of subprograms that are mostly independent, while DeepCoder applies operations repeatedly to the same few objects until the output is reached. In this sense, RobustFill is closer to inverse CAD~\cite{REPL}, instantiating complex objects with many fields like dataclasses, or other tasks involving several independent analyses, while DeepCoder is closer to tensor manipulation~\cite{TFCODER}, dynamic programming, or other tasks involving sequences of manipulations or updates applied to the same objects. Second, RobustFill uses the same input for each subprogram while DeepCoder involves program states that change due to the new variable bindings on each line, making DeepCoder more complex and closer to realistic programs with execution states changing over time.

\section{Program Synthesis via Decomposition}
\label{sec:method}

In this section we describe our proposed program synthesis method based on execution decomposition, where the model predicts step-by-step execution subgoals given I/O examples and synthesizes subprograms for each step.

\begin{algorithm}[t]
    \caption{\exedec: synthesis via decomposition in the execution space. \\ Note, $\{x_i\}$ is short for $[x_1, \dots, x_n]$ throughout, where $n$ is the number of I/O examples.}
    \label{alg:exedec}
    \begin{algorithmic}[1]
        \Function{\exedec}{$\{(I_i, O_i)\}$}
            \State $t \gets 1$
            \State $(I_i^{(1)}, O_i^{(1)}) \gets (I_i, O_i),\ \forall i$
            \While{True}
                \State $\{S_i^{(t)}\} \gets \Call{SubgoalModel}{\{(I_i^{(t)}, O_i^{(t)})\}}$ \Comment{Predict the next execution subgoals}
                \State $P^{(t)} \gets \Call{SynthesizerModel}{\{(I_i^{(t)}, S_i^{(t)})\}}$ \Comment{Predict the next subprogram}
                \State $E_i^{(t)} \gets \Call{Execute}{P^{(t)}, I_i^{(t)}},\ \forall i$
                \If{$\forall i.\ E_i^{(t)} = O_i^{(t)}$} \Comment{Is this the last subprogram?}
                    \State \Return $\Call{CombineProgramParts}{P^{(1)}, \dots, P^{(t)}}$
                \EndIf
                \LineComment{Update $\{(I_i^{(t)}, O_i^{(t)})\}$ to represent work that is left to be done (domain-specific).}
                \State $(I_i^{(t+1)}, O_i^{(t+1)}) \gets \Call{UpdateSpecification}{I_i^{(t)}, O_i^{(t)}, E_i^{(t)}},\ \forall i$
                \State $t \gets t + 1$
            \EndWhile
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Execution Decomposition}
\label{subsec:exedec}
The Execution Decomposition (\exedec{}) strategy outlined in \autoref{alg:exedec} aims to reason about the step-by-step execution behavior of a program rather than the code tokens. As in \autoref{sec:generalization}, we assume that the program is a sequence of one or more \emph{subprograms} that may be combined later. At each step, to synthesize the next subprogram, we first call a \emph{SubgoalModel} that takes I/O examples and predicts the next execution subgoals, i.e., the output of the next subprogram for each example.
Because the subgoal is the desired output at this step, predicting the
next subprogram is itself a programming by example task.
Thus, we provide the inputs and subgoals to a \emph{SynthesizerModel} which predicts the corresponding subprogram.
Finally, we execute the predicted subprogram and compute an \emph{updated specification} that describes the work that remains to be done 
by the rest of the program.

This updated specification is maintained throughout the step-by-step synthesis process. Because the overall program is specified
by I/O examples, we  use I/O examples for the updated specification as well. Intuitively, the inputs in the updated specification will be the current program state, and the outputs will be the output of the overall
task, but the details are slightly different because of specifics of the DSLs.
We begin with the original I/O examples for the overall synthesis task, and we update them in a domain-specific way as subprograms are synthesized (line 10). For instance, in RobustFill the input for each subprogram is the same as the original input, while the output becomes smaller as we remove already-synthesized prefixes of the output: $(I_i^{(t+1)}, O_i^{(t+1)}) \gets (I_i^{(t)}, \Call{RemovePrefix}{O_i^{(t)}, E_i^{(t)}})$; this is because the top level operation in RobustFill programs is always concatenation.\footnote{If the synthesized subprogram does not execute to a prefix of the current output for all examples, this synthesis attempt cannot succeed due to RobustFill's concatenation of subprograms. Such ``invalid'' subprograms are detected and handled during a beam search.} For DeepCoder, the input is the full program state (i.e., the set of variables and their values for each example) which is expanded with new variables as subprograms are synthesized, while the output remains constant for each example: $(I_i^{(t+1)}, O_i^{(t+1)}) \gets (I_i^{(t)}\cup E_i^{(t)}, O_i^{(t)})$. If \exedec{} synthesizes a subprogram that executes to the entire remaining output, there are no more subprograms to synthesize, so the subprograms are combined to form the full synthesized program.

\textbf{Beam Search.}\quad
\autoref{alg:exedec} describes a single synthesis attempt, but we can actually perform a \emph{search} comprising multiple synthesis attempts running efficiently in parallel using a modified beam search where each beam state is a partial rollout of the step-by-step synthesis algorithm. Our goal is the same as in traditional beam search~\cite{BEAMSEARCH}, which is to output $k$ sequences that maximize a score function. Formally, a candidate sequence $C$ at step $t$ is a sequence of execution subgoals and subprograms $C = [\{S_i^{(1)}\}, P^{(1)}, \hdots, \{S_i^{(t)}\}, P^{(t)}]$. 
As notation, we let $C^{(j)}$ denote the prefix of $C$ up to and including the $j$-th subprogram $P^{(j)}$.
The score for a candidate sequence $C$ is given by
\begin{align}
    \mathsf{Score}(C) = \sum_{j = 1}^t \mathsf{Score}(\{S_i^{(j)}\}) + \mathsf{Score}(P^{(j)}) + \mathsf{Valid}(C^{(j)})\,.
    \label{eq:beam_score}
\end{align}
Here, since both SubgoalModel and SynthesizerModel in \exedec{} are autoregressive sequence models (see \autoref{subsec:models} for more details), they can produce token sequences (either set of subgoals or a subprogram) with their corresponding log-probabilities which we use as the score function, i.e., $\mathsf{Score}(\{S_i^{(j)}\}) = \log p(S_i^{(j)} \mid \{(I_i^{(j)}, O_i^{(j)})\})$ and $\mathsf{Score}(P^{(j)}) = \log p(P^{(j)} \mid \{(I_i^{(j)}, S_i^{(j)})\})$.
By default, $\mathsf{Valid}(C^{(j)}) = 0$ unless $C^{(j)}$ fails to meet some required conditions. First, we set $\mathsf{Valid}(C^{(j)}) = -\infty$ if any subgoal or subprogram in $C^{(j)}$ is a token sequence that does not parse, or if any subprogram does not execute successfully.
We also check whether $C^{(j)}$ leads to unique program functionality within the beam of $k$ candidate sequences.
Note that different candidate sequences can result in the same subprograms even if the subgoals are different, and furthermore, combining different subprograms might result in full programs with the same functionality that are practically equivalent for future steps.
Therefore, we set $\mathsf{Valid}(C^{(j)}) = -\infty$ for all candidate sequences in the beam with corresponding program functionality equivalent to that of another higher-scoring candidate sequence in the beam.
Finally, domain-specific checks may determine that the candidate sequence $C^{(j)}$ is unlikely to result in a successful program in later steps, or that some computation limit such as a maximum number of steps is reached. In any case, an ``invalid'' beam element will drop out of the beam at the next step, freeing space in the beam for expansions of other surviving beam elements.

Note that we do not perform separate beam searches for each call to a model, since an isolated beam search would start with a single empty beam element with score 0. Instead, we perform a single long beam search throughout \autoref{alg:exedec} where each beam element is a candidate sequence $C$ with score $\mathsf{Score}(C)$. Model calls can be seen as ``extending'' the ongoing beam search, adding more tokens to the existing candidate sequences while maintaining their scores.

\subsection{Model Architecture}
\label{subsec:models}
Recall from \autoref{alg:exedec} that our approach \exedec{} relies on two models, SubgoalModel and SynthesizerModel.
We let both be sequence-to-sequence (seq2seq) models, which have been shown to be successful on various natural language~\citep{Attention,Transformer} and program synthesis tasks~\citep{parisotto2017neuro,Devlin2017Robustfill}.
We choose our seq2seq model to be a Transformer due to its impressive performance on natural language tasks over traditional RNNs~\citep{Transformer}.
We modify the baseline Transformer architecture to account for the fact that we operate on sets of inputs due to having multiple I/O examples. 
We call our model a Specification-Transformer, which we describe in further detail below.

\paragraph{Specification-Transformer.}
For consistent notation for the two models, we let $\{X_i\}$ be the multi-example input to the transformer and $Y$ its output. 
Formally, $X_i = (I_i, O_i)$ for SubgoalModel and $(I_i, S_i)$ for SynthesizerModel, and $Y = [S_1, \mathrm{Sep}, S_2, \mathrm{Sep}, \hdots, S_n]$ for SubgoalModel and $Y = P$ for SynthesizerModel, where $\mathrm{Sep}$ is a new token added to our vocabulary to partition the subgoals across examples.
Note that subgoals $S_i$ and subprogram $P$ are sequences of tokens.

Our Specification-Transformer consists of two modules. 
A Transformer encoder receives the specification $\{X_i\}$ and produces an encoding $\phi$.
Following \citet{Devlin2017Robustfill}, our encoder performs \emph{double attention} on the specification. 
That is, for each example $X_i$, the encoder performs the operation $\phi_i \leftarrow \mathrm{TransformerEncoder}(X_i)$, where the encoder performs self-attention on input $I_i$ followed by cross-attention from the output (either $O_i$ or $S_i$) to $I_i$.
Then, the encoding $\phi$ is simply the concatenation across examples $\phi \leftarrow \mathrm{Concat}(\{\phi_i\})$.
Next, a Transformer decoder takes the encoding and autoregressively generates the output token-by-token.
Formally, let $Y_{\ell-1} = [y_1, y_2, \hdots, y_{\ell-1}]$ be the output (subgoals or subprogram) generated so far. The decoder predicts the next output token as
$y_\ell \leftarrow \mathrm{TransformerDecoder}(Y_{\ell-1}, \phi)$.
As described by~\citet{Transformer}, the Transformer encoder and decoder both apply a stack of self-attention and feed-forward units.


\paragraph{Aligned Relative Attention.}
We use \emph{relative attention}~\citep{shaw2018relative}, i.e., representations of relative positions (distances between positions) instead of absolute positions of tokens.
This improves Transformer performance, particularly in length generalization~\citep{shaw2018relative,Csordas2021-ey}.
However, because the SubgoalModel predicts a concatenation of subgoals across examples, the relative position between
subgoal $S_i$ and I/O encoding $\phi_i$ is not consistent for every example index $i$,
so a naive relative position embedding would not represent consistent information across examples.
To remedy this, we propose \emph{aligned relative attention} (ARA), which alters the ``positions'' of subgoal tokens in the relative distance calculation.
Specifically, we increment the current position by $1$ for each token during subgoal prediction as usual, but when starting a new subgoal $S_i$ we set the current position to the position of $\phi_i$. This ensures that the relative position between the beginnings of $S_i$ and $\phi_i$ is always $0$.

\subsection{No-Subgoal Ablation}
We also experiment with an ablation of \exedec{} that performs step-by-step decomposition but without predicting execution subgoals first, instead directly predicting the next subprogram from the I/O examples. In \autoref{alg:exedec}, this ablation is achieved by replacing lines 5 and 6 with a single line, $P^{(t)} \gets \Call{CombinedModel}{\{(I_i^{(t)}, O_i^{(t)})\}}$, thus skipping the step of predicting execution subgoals. This ablation
uses the same model architecture (without ARA) and an analogous beam search.

\subsection{Model Training}
\label{subsec:training}
We generate training problems as described in \autoref{sec:benchmarks}. We train the models in \autoref{alg:exedec} using \emph{decomposed} data, that is, based on teacher forcing using \autoref{alg:exedec}. Specifically, for each subprogram in the ground-truth solution, we collect (A) the updated specification based on executing the previous ground-truth subprograms, (B) the subprogram's execution result on all examples, and (C) the subprogram itself. Then, we train the SubgoalModel to predict (B) given (A), the SynthesizerModel to predict (C) given (B) and the example inputs from (A), and the CombinedModel to predict (C) given (A). Each model type is trained separately for each generalization task.

For DeepCoder, the ground-truth training programs have variable names chosen randomly among a fixed set of possible names. If the variable names were used in a canonical order instead, the models would be unable to predict subprograms using variable names unseen during training, as is required for length generalization. When predicting a subprogram, the model only predicts the right hand side of the assignment, which at test time is assigned to a new variable name using a canonical ordering.

We performed a small hyperparameter search varying the learning rate and model size, choosing a single setting that performed well on training metrics across generalization tasks and model types. We used a learning rate of $2 \times 10^{-4}$, embedding dimension of 512, hidden dimension of 1024, batch size of 128, and 500K training steps, using enough synthetic training data to avoid repeating examples,
and training ${\sim} 1$ day for RobustFill (${\sim} 5$ hours for DeepCoder) with 8 TPU v2 accelerators per model.

\section{Experiments}
\label{sec:experiments}

Our experiments compare \exedec{}, an ablation without aligned relative attention, the no-subgoal ablation, and a Transformer baseline without any decomposition. All models including the baseline have the same hyperparameters and architecture (except that ARA only applies to the SubgoalModel), differing only in the training data used and corresponding maximum lengths of model inputs and outputs.\footnote{Because the baseline Transformer is trained on entire programs instead of subprograms, but the number of training examples is held constant, the baseline actually sees more subprograms during training than our models.}
Using the compositional generalization datasets constructed as described in \autoref{sec:benchmarks} and models trained as in \autoref{subsec:training}, we ran the different approaches with a beam size of 10 and measure the overall success rate on 1000 test examples per generalization task. We repeated the experiments using 5 different random initializations for model training. \autoref{fig:experiments} shows the results.

% Figure environment removed

\textbf{Discussion.}\quad
On both domains, \exedec{} significantly outperforms the Transformer baseline on every generalization task and in the i.i.d.\ setting (testing on the training distribution without any compositional generalization).
Specifically, \exedec{} achieves $+44\%$ higher average compositional generalization than the Transformer baseline on RobustFill and $+18\%$ on DeepCoder, a \emph{4.4$\times$ higher} success rate. But despite the notable improvements, DeepCoder in particular remains a difficult domain with deeply nested operation compositions that obscure the intended computation, while RobustFill has a more flat compositional structure that is easier to learn.

Our step-by-step decomposition approach introduces important inductive biases into the approach. By training models on the decomposed data, we teach the models that subprograms can be reasoned about separately, regardless of the compositional patterns present in other subprograms. The SubgoalModel does not see any code tokens and is only affected by compositional generalization patterns indirectly (since the distribution over programs affects the distribution over execution traces), and the SynthesizerModel only sees code tokens for the current subprogram and cannot reference any compositional patterns that appear when comparing to other subprograms. In contrast, the Transformer baseline sees all compositional patterns in the full programs, making it more likely to overfit to those patterns. The decomposition strategy also encourages our models to understand intermediate program states while the Transformer baseline is not trained with such execution information.

Compared to the no-subgoal ablation, \exedec{}
achieves higher compositional generalization performance on a majority of generalization tasks across the two domains, averaging $+7\%$ improvement on RobustFill and $+5\%$ on DeepCoder. This supports our hypothesis that predicting execution states is more robust than predicting code in the compositional generalization setting.
We also note that our aligned relative attention technique for the SubgoalModel is essential in DeepCoder but has little effect in RobustFill. A DeepCoder subgoal is an integer or a list of length at most 5, represented with fewer tokens in general compared to RobustFill subgoals (strings with length up to 20). Thus, without ARA, the DeepCoder subgoals across examples have closer positions to each other and are more easily confused in terms of relative distances to the encoded I/O examples.

Even though \exedec{} performs the best in most situations, the no-subgoal variation is slightly better on DeepCoder's training distribution and \LengthGen. In theory, one could combine the two decomposition variations in an ensemble to get the best of both approaches on unknown test distributions. Finally, we observe that in most cases \exedec{} has smaller variance across random initializations than the no-subgoal variation, i.e., \exedec{} might be more consistent in practice.

As a case study, we compare \exedec{}, the no-subgoal ablation, and the Transformer baseline on example RobustFill and DeepCoder problems in \autoref{app:case_study}. Through these examples, we discuss some behaviors and observations that clarify the advantages to \exedec{}'s approach.

\textbf{Limitations.}\quad
One limitation of \exedec{} is its need for a training dataset with ground-truth decompositions. For more general programs, we could use a heuristic such as line-by-line decomposition, but it is unclear whether the models would learn better if given \emph{natural} decompositions. Furthermore, programmers often think about decomposition in a hierarchical manner, and incorporating a recursive decomposition strategy is an interesting direction for future work.
Lastly, our SubgoalModel takes the simplistic approach of predicting tokenizations of objects, but to handle more complex objects, a more general form of the SubgoalModel might instead predict \emph{abstractions} of objects.

\section{Related Work}

\textbf{Compositional Generalization.}\quad
Compositional generalization is well-studied in NLP literature. Many works propose benchmarks to evaluate the understanding of natural language sentences with compositionally novel structures, such as SCAN~\citep{Lake2018Scan}.
They are either constructed by synthesizing examples based on a schema of generalization patterns (similar to what we do)~\citep{Bahdanau2019CLOSUREAS,keysers20cfq,Kim2020COGSAC}, or by repartitioning existing datasets with \textit{i.i.d.}~samples into splits with disjoint compositional structures~\citep{finegan18improving,shaw-etal-2021-compositional}.
Our benchmark takes inspiration from COGS~\citep{Kim2020COGSAC}, which defines a taxonomy of compositional structures in English for natural language understanding.
While many generalization concepts are similar to those proposed in \autoref{sec:generalization},
we focus on measuring compositional generalization of computer programs, which exhibit widely different compositional structures than natural language.

In addition, many works aim to measure and improve the compositional generalization capabilities of large language models (LLMs). Early works found that language models often fail to compositionally generalize on benchmarks such as SCAN and CFQ~\citep{keysers20cfq}, with follow-up works seeing improved performance through small modifications to the training of LLMs such as weight sharing~\citep{Csordas2021-ey,ontanon2022making}. 
Recently, papers have proposed specialized neural architectures with improved generalization performance~\citep{russin19separate,li19primitive,liu20analytical,chen20neuralsymbolic,herzig20spanparsing}, but these architectures are typically domain-specific.
Finally, more generalized approaches have been proposed including meta-learning~\citep{lake19compositionalmetalearning,wang2020meta,Conklin2021MetaLearningTC}, data augmentation~\citep{andreas20goodenough,oren-etal-2021-finding,Akyrek2021LearningTR,Wang2021LearningTS,qiu2021improving}, representation learning~\citep{Furrer2020CompositionalGI,Herzig2021UnlockingCG}, and in-context learning~\citep{Zhou2022LeasttoMostPE,Drozdov2022CompositionalSP}.

\textbf{Programming by Example.}\quad 
Various techniques have been applied to program synthesis~\citep{ARMANDOCOURSE,RISHABHSURVEY}, and recently
much attention has focused on machine learning for programming by example~\citep{Devlin2017Robustfill,bunel2018leveraging,parisotto2017neuro,DREAMCODER}.
Many methods incorporate learning to guide the search over programs, 
such as using learned premise selection~\citep{DEEPCODER,SIGNATURES},
syntax-guided search~\citep{Yin2017-my,EUPHONY},
bottom-up search~\citep{TFCODER, PROBE},
two-level search~\citep{INFERSKETCHES},
per-example search~\citep{PEPS},
and execution-guided synthesis methods~\citep{GARBAGECOLLECTOR,REPL,ChenLS19,BUSTLE,CROSSBEAM}.
More generally, there is less work on systematic generalization for machine learning for code,
although \citet{IPAGNN} studies length generalization in the learning-to-execute task~\citep{learning-to-execute}.


\textbf{Multi-step Program Synthesis.}\quad
\exedec{} is an instance of multi-step program synthesis, which broadly refers to methods involving multiple calls to (potentially different) models.
\emph{Execution-guided synthesis} is a popular form of this, iteratively generating and refining partial programs using execution information~\citep{GARBAGECOLLECTOR,REPL,ChenLS19,PEPS}, and some approaches do this with latent representations of the program state~\citep{chen2021latent} or execution traces~\citep{NEURIPS2018_7776e88b}. \emph{Planning} is another form of multi-step synthesis that first generates high-level plans of what the program should do~\citep{SKETCHADAPT,Bayou,Zhang2023PlanningWL}, sometimes with latent representations of plans~\citep{LATENTPROGRAMMER,zhong2023hierarchical}. 
Our method, \exedec{}, draws ideas from both avenues of multi-step synthesis, making plans by predicting subgoals and using step-by-step program execution to guide the search.

\section{Conclusion}
We explored the important aspect of compositional generalization in neural program synthesis. The ability to decompose complex tasks into smaller subtasks is a fundamental skill employed by human programmers, and measuring whether neural program synthesis methods exhibit similar capabilities is crucial for assessing their potential.
We introduced a meta-benchmark that characterizes 5 forms of compositional generalization in program synthesis, and we instantiated these generalization tasks in the RobustFill and DeepCoder domains.
The findings demonstrate that predicting decompositions of program execution, rather than solely focusing on program syntax, leads to significantly improved compositional generalization (up to $4.4\times$). 
This suggests that incorporating information about the step-by-step decomposition and leveraging it in the synthesis of programs can enhance the ability of neural models to tackle more complex tasks.
It would be an interesting future direction to apply these techniques to general purpose programming languages and to natural language to code tasks.

\bibliography{decomposition}
\bibliographystyle{plainnat}

\input{appendix}

\end{document}
