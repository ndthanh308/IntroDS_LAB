\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2021)Aky{\"u}rek, Akyurek, and
  Andreas]{Akyrek2021LearningTR}
Ekin Aky{\"u}rek, Afra~Feyza Akyurek, and Jacob Andreas.
\newblock Learning to recombine and resample data for compositional
  generalization.
\newblock \emph{ArXiv}, abs/2010.03706, 2021.

\bibitem[Andreas(2020)]{andreas20goodenough}
Jacob Andreas.
\newblock Good-enough compositional data augmentation.
\newblock In \emph{Proceedings of ACL}, 2020.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, and Sutton]{Austin2021-fu}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles
  Sutton.
\newblock Program synthesis with large language models.
\newblock August 2021.

\bibitem[Bahdanau et~al.(2016)Bahdanau, Cho, and Bengio]{Attention}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Bahdanau et~al.(2019)Bahdanau, de~Vries, O'Donnell, Murty, Beaudoin,
  Bengio, and Courville]{Bahdanau2019CLOSUREAS}
Dzmitry Bahdanau, Harm de~Vries, Timothy~J. O'Donnell, Shikhar Murty, Philippe
  Beaudoin, Yoshua Bengio, and Aaron~C. Courville.
\newblock Closure: Assessing systematic generalization of clevr models.
\newblock \emph{ArXiv}, abs/1912.05783, 2019.

\bibitem[Balog et~al.(2017{\natexlab{a}})Balog, Gaunt, Brockschmidt, Nowozin,
  and Tarlow]{Balog2017Deepcoder}
Matej Balog, Alexander~L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and
  Daniel Tarlow.
\newblock {DeepCoder}: Learning to write programs.
\newblock \emph{ICLR}, 2017{\natexlab{a}}.

\bibitem[Balog et~al.(2017{\natexlab{b}})Balog, Gaunt, Brockschmidt, Nowozin,
  and Tarlow]{DEEPCODER}
Matej Balog, Alexander~L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and
  Daniel Tarlow.
\newblock {DeepCoder: Learning to write programs}.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2017{\natexlab{b}}.

\bibitem[Barke et~al.(2020)Barke, Peleg, and Polikarpova]{PROBE}
Shraddha Barke, Hila Peleg, and Nadia Polikarpova.
\newblock {Just-in-Time} learning for {Bottom-Up} enumerative synthesis.
\newblock In \emph{Object-oriented Programming, Systems, Languages, and
  Applications ({{OOPSLA}})}, 2020.

\bibitem[Bieber et~al.(2020)Bieber, Sutton, Larochelle, and Tarlow]{IPAGNN}
David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow.
\newblock Learning to execute programs with instruction pointer attention graph
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  ({NeurIPS})}, 2020.

\bibitem[Bunel et~al.(2018)Bunel, Hausknecht, Devlin, Singh, and
  Kohli]{bunel2018leveraging}
Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet
  Kohli.
\newblock Leveraging grammar and reinforcement learning for neural program
  synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan, Pinto, Kaplan,
  Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2019)Chen, Liu, and Song]{ChenLS19}
Xinyun Chen, Chang Liu, and Dawn Song.
\newblock Execution-guided neural program synthesis.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Chen et~al.(2020)Chen, Liang, Yu, Song, and
  Zhou]{chen20neuralsymbolic}
Xinyun Chen, Chen Liang, Adams~Wei Yu, D.~Song, and Denny Zhou.
\newblock Compositional generalization via neural-symbolic stack machines.
\newblock In \emph{Proceedings of NeurIPS}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Song, and Tian]{chen2021latent}
Xinyun Chen, Dawn Song, and Yuandong Tian.
\newblock Latent execution for neural program synthesis beyond domain-specific
  languages.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22196--22208, 2021{\natexlab{b}}.

\bibitem[Chomsky and Lightfoot(2002)]{chomsky2002syntactic}
N.~Chomsky and D.W. Lightfoot.
\newblock \emph{Syntactic Structures}.
\newblock De Gruyter Reference Global. Mouton de Gruyter, 2002.
\newblock ISBN 9783110172799.
\newblock URL \url{https://books.google.com/books?id=a6a\_b-CXYAkC}.

\bibitem[Conklin et~al.(2021)Conklin, Wang, Smith, and
  Titov]{Conklin2021MetaLearningTC}
Henry Conklin, Bailin Wang, Kenny Smith, and Ivan Titov.
\newblock Meta-learning to compositionally generalize.
\newblock \emph{ArXiv}, abs/2106.04252, 2021.

\bibitem[Csord{\'a}s et~al.(2021)Csord{\'a}s, Irie, and
  Schmidhuber]{Csordas2021-ey}
R{\'o}bert Csord{\'a}s, Kazuki Irie, and J{\"u}rgen Schmidhuber.
\newblock The devil is in the detail: Simple tricks improve systematic
  generalization of transformers.
\newblock August 2021.

\bibitem[Damnjanovic(2004)]{nic2004compositionality}
Nic Damnjanovic.
\newblock The compositionality papers.
\newblock \emph{Australasian Journal of Philosophy}, 82:\penalty0 366--367, 06
  2004.
\newblock \doi{10.1080/713659827}.

\bibitem[Devlin et~al.(2017)Devlin, Uesato, Bhupatiraju, Singh, Mohamed, and
  Kohli]{Devlin2017Robustfill}
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel{-}rahman
  Mohamed, and Pushmeet Kohli.
\newblock {RobustFill}: Neural program learning under noisy {I/O}.
\newblock \emph{ICML}, 2017.

\bibitem[Drozdov et~al.(2022)Drozdov, Scharli, Akyuurek, Scales, Song, Chen,
  Bousquet, and Zhou]{Drozdov2022CompositionalSP}
Andrew Drozdov, Nathanael Scharli, Ekin Akyuurek, Nathan Scales, Xinying Song,
  Xinyun Chen, Olivier Bousquet, and Denny Zhou.
\newblock Compositional semantic parsing with large language models.
\newblock \emph{ArXiv}, abs/2209.15003, 2022.

\bibitem[Ellis et~al.(2019)Ellis, Nye, Pu, Sosa, Tenenbaum, and
  Solar{-}Lezama]{REPL}
Kevin Ellis, Maxwell~I. Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando
  Solar{-}Lezama.
\newblock Write, execute, assess: Program synthesis with a {REPL}.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Ellis et~al.(2020)Ellis, Wong, Nye, Sable-Meyer, Cary, Morales,
  Hewitt, Solar-Lezama, and Tenenbaum]{DREAMCODER}
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas
  Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua~B. Tenenbaum.
\newblock Dreamcoder: Growing generalizable, interpretable knowledge with
  wake-sleep bayesian program learning.
\newblock \emph{CoRR}, abs/2006.08381, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.08381}.

\bibitem[Finegan-Dollak et~al.(2018)Finegan-Dollak, Kummerfeld, Zhang,
  Ramanathan, Sadasivam, Zhang, and Radev]{finegan18improving}
Catherine Finegan-Dollak, Jonathan~K. Kummerfeld, Li~Zhang, Karthik Ramanathan,
  Sesh Sadasivam, Rui Zhang, and Dragomir Radev.
\newblock Improving text-to-{SQL} evaluation methodology.
\newblock In \emph{Proceedings of ACL}, 2018.

\bibitem[Furrer et~al.(2020)Furrer, van Zee, Scales, and
  Scharli]{Furrer2020CompositionalGI}
Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Scharli.
\newblock Compositional generalization in semantic parsing: Pre-training vs.
  specialized architectures.
\newblock \emph{ArXiv}, abs/2007.08970, 2020.

\bibitem[Gu et~al.(2021)Gu, Kase, Vanni, Sadler, Liang, Yan, and
  Su]{gu2021beyond}
Yu~Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and
  Yu~Su.
\newblock Beyond iid: three levels of generalization for question answering on
  knowledge bases.
\newblock In \emph{Proceedings of the Web Conference 2021}, pages 3477--3488,
  2021.

\bibitem[Gulwani et~al.(2017{\natexlab{a}})Gulwani, Polozov, and
  Singh]{gulwani2017program}
S.~Gulwani, O.~Polozov, and R.~Singh.
\newblock \emph{Program Synthesis}.
\newblock Foundations and Trends(r) in Programming Languages Series. Now
  Publishers, 2017{\natexlab{a}}.
\newblock ISBN 9781680832921.
\newblock URL \url{https://books.google.com/books?id=mK5ctAEACAAJ}.

\bibitem[Gulwani(2011)]{FLASHFILL}
Sumit Gulwani.
\newblock Automating string processing in spreadsheets using input-output
  examples.
\newblock In \emph{PoPL'11, January 26-28, 2011, Austin, Texas, USA}, 2011.

\bibitem[Gulwani et~al.(2017{\natexlab{b}})Gulwani, Polozov, Singh,
  et~al.]{RISHABHSURVEY}
Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et~al.
\newblock Program synthesis.
\newblock \emph{Foundations and Trends{\textregistered} in Programming
  Languages}, 4\penalty0 (1-2):\penalty0 1--119, 2017{\natexlab{b}}.

\bibitem[Herzig and Berant(2020)]{herzig20spanparsing}
Jonathan Herzig and Jonathan Berant.
\newblock Span-based semantic parsing for compositional generalization.
\newblock In \emph{Proceedings of EMNLP}, 2020.

\bibitem[Herzig et~al.(2021)Herzig, Shaw, Chang, Guu, Pasupat, and
  Zhang]{Herzig2021UnlockingCG}
Jonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin Guu, Panupong Pasupat, and
  Yuan Zhang.
\newblock Unlocking compositional generalization in pre-trained models using
  intermediate representations.
\newblock \emph{ArXiv}, abs/2104.07478, 2021.

\bibitem[Hong et~al.(2021)Hong, Dohan, Singh, Sutton, and
  Zaheer]{LATENTPROGRAMMER}
Joey Hong, David Dohan, Rishabh Singh, Charles Sutton, and Manzil Zaheer.
\newblock Latent programmer: Discrete latent codes for program synthesis.
\newblock In \emph{International Conference on Machine Learning ({ICML})},
  2021.

\bibitem[Keysers et~al.(2020)Keysers, Sch{\"a}rli, Scales, Buisman, Furrer,
  Kashubin, Momchev, Sinopalnikov, Stafiniak, Tihon, Tsarkov, Wang, van Zee,
  and Bousquet]{keysers20cfq}
Daniel Keysers, Nathanael Sch{\"a}rli, Nathan Scales, H.~Buisman, Daniel
  Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz
  Stafiniak, Tibor Tihon, D.~Tsarkov, Xiao Wang, Marc van Zee, and O.~Bousquet.
\newblock Measuring compositional generalization: A comprehensive method on
  realistic data.
\newblock In \emph{Proceedings of ICLR}, 2020.

\bibitem[Kim and Linzen(2020)]{Kim2020COGSAC}
Najoung Kim and Tal Linzen.
\newblock Cogs: A compositional generalization challenge based on semantic
  interpretation.
\newblock \emph{ArXiv}, abs/2010.05465, 2020.

\bibitem[Lake(2019)]{lake19compositionalmetalearning}
Brenden~M Lake.
\newblock Compositional generalization through meta sequence-to-sequence
  learning.
\newblock In \emph{Proceedings of NeurIPS}, 2019.

\bibitem[Lake and Baroni(2018)]{Lake2018Scan}
Brenden~M. Lake and Marco Baroni.
\newblock Generalization without systematicity: On the compositional skills of
  sequence-to-sequence recurrent networks.
\newblock \emph{ICML}, 2018.

\bibitem[Lee et~al.(2018)Lee, Heo, Alur, and Naik]{EUPHONY}
Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik.
\newblock Accelerating search-based program synthesis using learned
  probabilistic models.
\newblock In \emph{Conference on Programming Language Design and Implementation
  (PLDI)}, pages 436--449, June 2018.

\bibitem[Li et~al.(2019)Li, Zhao, Wang, and Hestness]{li19primitive}
Yuanpeng Li, Liang Zhao, JianYu Wang, and Joel Hestness.
\newblock Compositional generalization for primitive substitutions.
\newblock In \emph{Proceedings of EMNLP/IJCNLP}, 2019.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond,
  Eccles, Keeling, Gimeno, Dal~Lago, Hubert, Choy, De~Masson~D'autume,
  Babuschkin, Chen, Huang, Welbl, Gowal, Cherepanov, Molloy, Mankowitz, Robson,
  Kohli, De~Freitas, Kavukcuoglu, and Vinyals]{ALPHACODE}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago,
  Thomas Hubert, Peter Choy, Cyprien De~Masson~D'autume, Igor Babuschkin,
  Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov,
  James Molloy, Daniel~J Mankowitz, Esme~Sutherland Robson, Pushmeet Kohli,
  Nando De~Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
\newblock {Competition-Level} code generation with {AlphaCode}.
\newblock
  \url{https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf},
  2022.
\newblock Accessed: 2022-2-26.

\bibitem[Liu et~al.(2020)Liu, An, Lou, Chen, Lin, Gao, Zhou, Zheng, and
  Zhang]{liu20analytical}
Qian Liu, Shengnan An, Jianguang Lou, B.~Chen, Zeqi Lin, Yan Gao, Bin Zhou,
  Nanning Zheng, and Dongmei Zhang.
\newblock Compositional generalization by learning analytical expressions.
\newblock In \emph{Proceedings of NeurIPS}, 2020.

\bibitem[Murali et~al.(2018)Murali, Qi, Chaudhuri, and Jermaine]{Bayou}
Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine.
\newblock Neural sketch learning for conditional program generation.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2018.

\bibitem[Nye et~al.(2019{\natexlab{a}})Nye, Hewitt, Tenenbaum, and
  Solar{-}Lezama]{INFERSKETCHES}
Maxwell~I. Nye, Luke~B. Hewitt, Joshua~B. Tenenbaum, and Armando
  Solar{-}Lezama.
\newblock Learning to infer program sketches.
\newblock In \emph{International Conference on Machine Learning {(ICML)}},
  pages 4861--4870, 2019{\natexlab{a}}.
\newblock URL \url{http://proceedings.mlr.press/v97/nye19a.html}.

\bibitem[Nye et~al.(2019{\natexlab{b}})Nye, Hewitt, Tenenbaum, and
  Solar{-}Lezama]{SKETCHADAPT}
Maxwell~I. Nye, Luke~B. Hewitt, Joshua~B. Tenenbaum, and Armando
  Solar{-}Lezama.
\newblock Learning to infer program sketches.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2019{\natexlab{b}}.

\bibitem[Odena and Sutton(2020)]{SIGNATURES}
Augustus Odena and Charles Sutton.
\newblock Learning to represent programs with property signatures.
\newblock In \emph{International Conference on Learning Representations
  {(ICLR)}}, 2020.

\bibitem[Odena et~al.(2020)Odena, Shi, Bieber, Singh, Sutton, and Dai]{BUSTLE}
Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles Sutton, and
  Hanjun Dai.
\newblock {BUSTLE}: {Bottom-Up} program synthesis through learning-guided
  exploration.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, September 2020.

\bibitem[Ontañón et~al.(2022)Ontañón, Ainslie, Cvicek, and
  Fisher]{ontanon2022making}
Santiago Ontañón, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher.
\newblock Making transformers solve compositional tasks, 2022.

\bibitem[Oren et~al.(2021)Oren, Herzig, and Berant]{oren-etal-2021-finding}
Inbar Oren, Jonathan Herzig, and Jonathan Berant.
\newblock Finding needles in a haystack: Sampling structurally-diverse training
  sets from synthetic data for compositional generalization.
\newblock In \emph{Proceedings of EMNLP}, 2021.

\bibitem[Parisotto et~al.(2017)Parisotto, Mohamed, Singh, Li, Zhou, and
  Kohli]{parisotto2017neuro}
Emilio Parisotto, Abdel{-}rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong
  Zhou, and Pushmeet Kohli.
\newblock Neuro-symbolic program synthesis.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Parlante(2022)]{cs106a}
Nick Parlante.
\newblock {CS106A}: Programming methodologies.
\newblock Course at Stanford University, 2022.

\bibitem[Qiu et~al.(2021)Qiu, Shaw, Pasupat, Nowak, Linzen, Sha, and
  Toutanova]{qiu2021improving}
Linlu Qiu, Peter Shaw, Panupong Pasupat, Pawe{\l}~Krzysztof Nowak, Tal Linzen,
  Fei Sha, and Kristina Toutanova.
\newblock Improving compositional generalization with latent structure and data
  augmentation.
\newblock \emph{arXiv preprint arXiv:2112.07610}, 2021.

\bibitem[Russin et~al.(2019)Russin, Jo, O'Reilly, and Bengio]{russin19separate}
Jake Russin, Jason Jo, R.~O'Reilly, and Yoshua Bengio.
\newblock Compositional generalization in a deep seq2seq model by separating
  syntax and semantics.
\newblock \emph{ArXiv}, abs/1904.09708, 2019.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018relative}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics {(NAACL)}}, 2018.

\bibitem[Shaw et~al.(2021)Shaw, Chang, Pasupat, and
  Toutanova]{shaw-etal-2021-compositional}
Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova.
\newblock Compositional generalization and natural language variation: Can a
  semantic parsing approach handle both?
\newblock In \emph{Proceedings of ACL}, 2021.

\bibitem[Shi et~al.(2019)Shi, Steinhardt, and Liang]{FRANGEL}
Kensen Shi, Jacob Steinhardt, and Percy Liang.
\newblock {FrAngel}: Component-based synthesis with control structures.
\newblock \emph{Proceedings of the ACM on Programming Languages (PACMPL)},
  3\penalty0 (POPL):\penalty0 1--29, 2019.

\bibitem[Shi et~al.(2022{\natexlab{a}})Shi, Bieber, and Singh]{TFCODER}
Kensen Shi, David Bieber, and Rishabh Singh.
\newblock {TF-Coder}: Program synthesis for tensor manipulations.
\newblock \emph{ACM Transactions on Programming Languages and Systems
  (TOPLAS)}, 44\penalty0 (2):\penalty0 1--36, 2022{\natexlab{a}}.

\bibitem[Shi et~al.(2022{\natexlab{b}})Shi, Dai, Ellis, and Sutton]{CROSSBEAM}
Kensen Shi, Hanjun Dai, Kevin Ellis, and Charles Sutton.
\newblock {CrossBeam}: Learning to search in bottom-up program synthesis.
\newblock In \emph{International Conference on Learning Representations
  ({ICLR})}, 2022{\natexlab{b}}.

\bibitem[Shin et~al.(2018)Shin, Polosukhin, and Song]{NEURIPS2018_7776e88b}
Eui~Chul Shin, Illia Polosukhin, and Dawn Song.
\newblock Improving neural program synthesis with inferred execution traces.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/7776e88b0c189539098176589250bcba-Paper.pdf}.

\bibitem[Shrivastava et~al.(2021)Shrivastava, Larochelle, and Tarlow]{PEPS}
Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
\newblock Learning to combine per-example solutions for neural program
  synthesis.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Solar-Lezama(2018)]{ARMANDOCOURSE}
Armando Solar-Lezama.
\newblock Introduction to program synthesis.
\newblock \url{https://people.csail.mit.edu/asolar/SynthesisCourse/TOC.htma},
  2018.
\newblock Accessed: 2018-09-17.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and Le]{BEAMSEARCH}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Wang et~al.(2020)Wang, Lapata, and Titov]{wang2020meta}
Bailin Wang, Mirella Lapata, and Ivan Titov.
\newblock Meta-learning for domain generalization in semantic parsing.
\newblock \emph{arXiv:2010.11988}, 2020.

\bibitem[Wang et~al.(2021)Wang, Yin, Lin, and Xiong]{Wang2021LearningTS}
Bailin Wang, Wenpeng Yin, Xi~Victoria Lin, and Caiming Xiong.
\newblock Learning to synthesize data for semantic parsing.
\newblock \emph{ArXiv}, abs/2104.05827, 2021.

\bibitem[Yin and Neubig(2017)]{Yin2017-my}
Pengcheng Yin and Graham Neubig.
\newblock A syntactic neural model for {General-Purpose} code generation.
\newblock In \emph{Assocation for Computational Linguistics {(ACL)}}, 2017.

\bibitem[Zaremba and Sutskever(2014)]{learning-to-execute}
Wojciech Zaremba and Ilya Sutskever.
\newblock Learning to execute, 2014.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Shen, Ding, Tenenbaum, and
  Gan]{Zhang2023PlanningWL}
Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua~B. Tenenbaum, and
  Chuang Gan.
\newblock Planning with large language models for code generation.
\newblock \emph{ArXiv}, abs/2303.05510, 2023.

\bibitem[Zhong et~al.(2023)Zhong, Lindeborg, Zhang, Lim, and
  Sun]{zhong2023hierarchical}
Linghan Zhong, Ryan Lindeborg, Jesse Zhang, Joseph~J Lim, and Shao-Hua Sun.
\newblock Hierarchical neural program synthesis.
\newblock \emph{arXiv preprint arXiv:2303.06018}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Scharli, Hou, Wei, Scales, Wang, Schuurmans,
  Bousquet, Le, and hsin Chi]{Zhou2022LeasttoMostPE}
Denny Zhou, Nathanael Scharli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
  Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed~Huai hsin Chi.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock \emph{ArXiv}, abs/2205.10625, 2022.

\bibitem[Zohar and Wolf(2018)]{GARBAGECOLLECTOR}
Amit Zohar and Lior Wolf.
\newblock Automatic program synthesis of long programs with a learned garbage
  collector.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\end{thebibliography}
