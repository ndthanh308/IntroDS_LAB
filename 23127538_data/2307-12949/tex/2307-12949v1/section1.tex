\section{Introduction}

Automatic Speech Recognition (ASR) is a key component in processing audio materials such as audio translation and voice assistant \cite{federmann-lewis-2016-microsoft}, and speech information extraction \cite{cho-etal-2021-streamhover}. Typical ASR systems produce chunks of transcription without any text structures such as sentence and phrase boundaries \cite{jones2003measuring}. As a result, it lowers the readability of the generated ASR texts \cite{jones2003measuring} and severely affects the performance of systems for downstream tasks over this type of text, e.g., information extraction \cite{alam2015comparing}. To address this issue, the Punctuation Restoration (PR) task has been added to the ASR systems \cite{tilk2015lstm} to improve the text readability and the performance of downstream tasks for ASR-generated texts such as question answering \cite{pouran-ben-veyseh-etal-2022-behanceqa}, chitchat detection \cite{lai-etal-2022-behancecc}, and tutorial recommendation \cite{Veyseh2022TutorialRF}. The most recent successful work for PR was all built on top of transformer-based PLMs such as BERT \cite{devlin-etal-2019-bert} and ELECTRA \cite{clark2020electra}.

%together with other post-processing tasks such as true-casing \cite{lita-etal-2003-truecasing}.

% Modern PR systems model the PR task as a word-level sequence labeling task, in which each word is labeled with either a punctuation mark or a NULL label. These models depend on  

% Due to its importance, many studies have been conducted for PR in recent years. 
%Early work employed various combinations of both text-feature and audio features \cite{}. More recent studies mainly develop advanced neural network architectures \cite{}, and integrate external knowledge \cite{}. 
Despite such progress, lacking domain-specific training data is still a major obstacle that hinders the research and development of PR systems for real-world applications \cite{lai-etal-2022-behancepr}. We identify two factors accounting for this issue. First, speech topics involve a unique set of keywords as well as slang in spoken languages. The ASR system and PR system without topic knowledge can be severely affected by the shift of topics in the source audio. Second, unlike other tasks where the unlabeled data is created by humans, the input of PR is generated by an ASR system. This creates a unique dependency that must be addressed by the PR model. Consequently, creating cost-effective datasets for a wide range of domains for PR is highly challenging.

Moreover, naive adoption of available punctuated data is problematic. While large-scale punctuated texts corpora are available, they are mostly written texts (REF texts), which are substantially well-punctuated. In contrast, ASR-generated texts (ASR texts) inherit a substantial amount of noise from both spoken language (e.g., verbal pauses) and the transcription process (e.g., word errors). Accordingly, prior studies have shown that a PR model that was trained on REF texts performed poorly on real-world ASR texts \cite{alam-etal-2020-punctuation}. In other words, directly using readily available written texts does not help to improve the PR model.


To overcome these issues, we introduce a novel data generation method to automatically generate large-scale, high-quality labeled data for PR. In particular, instead of manual annotation, we employ a pre-trained language model, namely GPT2 \cite{radford2019language}, to create synthetic labeled data for PR because generative models like GPT2 can generate punctuated texts that can be converted to labeled data for PR easily. Since the GPT2 model was trained on written texts across diverse topics, this leads to two issues that need to be addressed. 

First, the topics in the generated texts are unconstrained, which is suboptimal for some specific applications, such as gaming livestreaming. As such, we propose a method to control the topic of the generated texts. Instead of unconditional text generation, we feed the GPT2 model with an in-topic seed text, which was sampled from an in-topic unsupervised source. Hence, we encourage the GPT2 model to generate more texts within the initial topic. As a result, we can leverage GPT2's knowledge to obtain unlimited in-topic labeled texts for PR.

Second, the disconnection of the GPT2 model and the target PR model might cause a discrepancy between GPT2-generated texts and the target PR text. Therefore, to improve the quality of the  GPT2-generated data for PR, we propose to further finetune the GPT2 model in parallel with the training of the PR model to generate optimal customized texts for PR. Particularly, we propose a meta-learning framework to consider the GPT2 model as a meta-parameter for the training of the PR model, in which the GPT2 model will be fine-tuned based on the performance of the PR model on the development set. A trivial solution is reinforcement learning, where the reward can be calculated directly from the evaluation metrics of the PR model on the development set, e.g., the F1-score. However, obtaining a reliable, fast reward is challenging due to either the small scale of the evaluation or the computational cost of the evaluation that has to be done at every single iteration. To alleviate this issue, we propose a novel reward function that relies on the gradients of the PR model obtained from the generated texts and the development set. Intuitively, a generated sample should have a higher reward if the PR model's gradients derived from the sample follows the PR expected gradients derived from the development set. Toward this end, in each iteration, after generating synthetic PR data, we compute an average gradient of the PR model over the generated data for each training example. Then, we compute another average gradient of the PR model over a sampled subset of the development set. Finally, the reward for each generated sample is computed using the cosine similarity score between the two gradients. We evaluate the effectiveness of the proposed methods on two benchmark datasets for PR. The experiments show that our model outperforms the strongest baseline on both datasets.






% The GPT2 model is fed with a small chunk of in-topic text to generate more in-topic texts for PR.

% To overcome these issues, instead of annotating more data for some particular domains, we introduce a novel domain-agnostic data generation method to generate labeled PR data automatically. The generated data can inherit a wide range of keywords from the PLM, while the PLM-generated text is also customized to the domain of interest. In particular, we propose to employ a generative PLM named GPT2 to create synthetic data for PR. The GPT2 model is trained on the augmented in-topic data, and then the fine-tuned GPT2 is asked to generate a large amount of labeled semi-in-domain data. This data is then combined with the real in-domain PR data to train a PR model. Moreover, this method allows us to control the seed text that is fed to the GPT2 model, hence controlling the topic in the PLM-generated texts. Hence, this method brings the topic gap between the PLM-generated data and the target PR data.

%  As such, it is necessary that the GPT2 and the PR models can interact with each other so that the PR model can guide the GPT2 model to generate spoken texts instead of written texts. In particular, we proposed a reinforcement learning method to allow the PR model to give feedback to the GPT2 model. So, the GPT2 model can generate texts to optimize the PR model's performance.
