\section{Related Work}

Early PR studies employed syntactic features and prosodic features\cite{szaszak2019leveraging} to train graphical models such as HMM and CRF \cite{tilk2015lstm}. Recent models for PR employed artificial neural networks to model the PR problem as a sequence-to-sequence problem using various network architectures such as convolutional neural network \cite{che-etal-2016-punctuation}, recurrent neural network \cite{tilk2015lstm,kim2019deep}, and transformer \cite{alam-etal-2020-punctuation}. Pretrained language models stand at the core of the recent PR models. There have been variants of pre-trained language models used for PR such as BERT \cite{fu-etal-2021-improving}, RoBERTa \cite{alam-etal-2020-punctuation,courtland-etal-2020-efficient}, ELECTRA \cite{hentschel2021making,chen2021discriminative}, XLM-RoBERTa \cite{chordia-2021-punktuator}, and funnel-transformer \cite{shi2021incorporating}. Recent advance in training and preprocessing leads to many training techniques such as data augmentation \cite{alam-etal-2020-punctuation}, adversarial training \cite{yi2020adversarial}, multitask learning \cite{lin2020joint,hentschel2021making}, self-training \cite{chen2021discriminative}, two-stage training \cite{fu-etal-2021-improving}, and contrastive learning \cite{huang2021token}. External knowledge was also incorporated into the PR model including external punctuated data \cite{fu-etal-2021-improving}, syntactic features \cite{shi2021incorporating} and acoustic features \cite{zhu2022unified}.