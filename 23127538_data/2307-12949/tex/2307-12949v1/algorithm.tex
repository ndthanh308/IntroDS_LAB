
\begin{algorithm}[t]
\caption{Reinforcement Learning for PR}\label{alg:cap}
\begin{algorithmic}

\Require $\modelgen, \modelpr, f_{\theta}, f_{\omega}$
\Require $\dunsup$
\Require $\dtrainpr, \ddevpr$


\For{$t$ $<$ max\_iteration}
    
    \State{$\bseed \gets sample(\dunsup)$}
    \State{$\bgenpr \gets \modelgen_{t-1}(\bseed)$} \Comment{Generate data}
    
    \State{$\btrainpr \gets sample(\dtrainpr)$}
    % \State{$\thetapr \gets train(\thetapr, \bgenpr \cup \btrainpr)$} \Comment{Train PR model}
    \State{$\thetapr_t \gets update(\thetapr_{t-1}, \nabla f_{\thetapr}(\bgenpr \cup \btrainpr)$} \Comment{Update PR model}
    
    \State{$\bdevpr \gets sample(\ddevpr)$}
    \State{$grad^{dev} \gets \nabla f_{\thetapr}(\bdevpr)$}
    \State{$grad^{gen} \gets \nabla f_{\thetapr}(\bgenpr)$}
    
    \State{$r=grad^{dev} \times grad^{gen}$} \Comment{Compute reward}
    

    \State{$\nabla_{\omega}= \sum_{b_i \in \bgenpr}r_i\nabla f_{\thetagen}(b_i)$}
    
    \State{$\thetagen_t \gets update(\thetagen_{t-1}, \nabla_{\omega})$} \Comment{Update GPT2 model}
\EndFor

\end{algorithmic}
\end{algorithm}