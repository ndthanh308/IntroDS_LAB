\newcommand{\dunsup}[0]{\mathcal{D}^{unsup}}
% \newcommand{\dtraingen}[0]{\mathcal{D}^{gen}_{train}}
% \newcommand{\ddevgen}[0]{\mathcal{D}^{gen}_{dev}}
\newcommand{\dtrainpr}[0]{\mathcal{D}^{train}}
\newcommand{\ddevpr}[0]{\mathcal{D}^{dev}}
\newcommand{\dtestpr}[0]{\mathcal{D}^{test}}


\newcommand{\bseed}[0]{\mathcal{B}^{seed}}
\newcommand{\bgenpr}[0]{\mathcal{B}^{gen}}
\newcommand{\btrainpr}[0]{\mathcal{B}^{train}}
\newcommand{\bdevpr}[0]{\mathcal{B}^{dev}}
\newcommand{\btestpr}[0]{\mathcal{B}^{test}}


\newcommand{\thetapr}[0]{\theta}
\newcommand{\thetagen}[0]{\omega}

\newcommand{\modelpr}[0]{\mathcal{M}^\theta}
\newcommand{\modelgen}[0]{\mathcal{M}^\omega}

\section{Proposed Approach}

\subsection{Problem Setting}

Similar to prior studies, we model the PR task as a word-level sequence labeling problem. Given a text input sequence $X=\{w_1, w_2, \cdots, w_N\}$ where $N$ is the number of words in the whole sequence, the input $X$ is encoded into vector space using a large language model, parameterized as $f_{\theta}$, as $H=\{h_1, h_2, \cdots, h_N\}$. The ground truth corresponding to the input sequence is $Y=\{y_1,y_2, \cdots, y_N\}$ where $y_i$ belongs to a predefined list of punctuation marks. The model's prediction is formalized as $\hat{Y}=\{\hat{y}_1,\hat{y}_2, \cdots, \hat{y}_N\}$. The PR model, parameterized as $\modelpr$,  is trained using cross-entropy loss: 

$$\mathcal{L}_{CE}=-\frac{1}{N}y_i\text{log}\hat{y}_i$$

We propose a reinforcement learning framework to leverage a generative PLM to adaptively generate PR data for training the PR model. In particular, the learning process involves two models: a PR model $\modelpr$  and a GPT2 model $\modelgen$. It first generates in-topic punctuated text from some seed texts derived from the in-topic unsupervised text. Then, the PR model $\modelpr$ is trained on both the generated data and human-annotated data. Afterward, the GPT2 model $\modelgen$ is finetuned based on the feedback from the PR model to further improve the effectiveness of the generated data. This is achieved by a reinforcement learning algorithm that exploits the agreement between generated data and the development set of the human-annotated data to form the reward function. Algorithm \ref{alg:cap} presents the detail of the proposed method. 


\input{table-iwlts}

\subsection{Data Augmentation}
Training/testing data discrepancy is a crucial problem in the punctuation restoration task. The training data that are obtained from written text, however, does not reflect the noise in the actual spoken text that is transcribed by an ASR system. As such, to introduce noise to the text, we augmented the input text using three strategies: \textit{duplication, alternation, deletion}  with an augmentation probability of $\alpha_1, \alpha_2, \alpha_3$ similar to prior work \cite{alam-etal-2020-punctuation}. To fit the very long input sequence into a large language model, the input sequence must be split into shorter segments of the same size. Due to the randomness of the chunking, the predictions of the edge tokens (head and tail of the chunk) might be severely affected due to the lack of preceding or following contexts. To overcome this, we feed additional preceding and following words of a chunk to help the large language model better encodes the sequence for the PR task, especially for predicting the chunk's beginning and ending words. In particular, we concatenate $C$ preceding and $C$ following words to the input sequence, if they are available, resulting in the input sequence $X_C=\{C, X, C\}$ fed to the PR model. We do not predict the labels for these additional tokens to avoid prediction conflict with the preceding and tailing chunks, as well as to prevent the lack of context to recur.
\input{algorithm}



\subsection{Data Generation} 
Due to the limited annotated in-topic data for PR, we proposed a more feasible method to generate an unlimited amount of data for PR using a generative language model, named GPT2. As GPT2 was trained on a massive amount of unsupervised learning text across many topics, it can generate a long piece of text given just a short seed prompt, which controls the topic of the generated text through the seed text given to GPT2. To do that, we obtained the transcripts of the TED-talk from 2013 to 2017 (separated from the IWSLT corpus which covers talks before 2012); then we used this as our unsupervised in-topic corpus for text generation. For the BehancePR corpus, we use the unsupervised text in the development set as the in-topic seed.

In particular, in each iteration, a batch of semi-annotated data $\bgenpr$ is generated by the GPT2 model $\modelgen_{t-1}$ using an in-topic seed $\bseed$. Another batch $\btrainpr$ is sampled from the original PR training data $\dtrainpr$. Finally, the PR model $\modelpr$ is trained on the combined batch of these two batches.

\subsection{Reinforcement Learning} The GPT2 model is helpful in generating well-punctuated in-topic data. However, as the generation is done independently from the PR model, the generated data inherits the written language style from the GPT2 model's memory. As a result, the generated data is not optimal for the PR task, as the ultimate goal of PR is to be used for spoken language. As such, it is necessary for the PR model to give feedback to the GPT2 model so that the GPT2 model can be finetuned in parallel with the training of the PR model. Expectedly, the guidance from the PR model can make the GPT2 model generate more relevant text.


One trivial way to measure the effectiveness of the generated data is the performance of the PR model (e.g., overall F1-score) over the development set. However, as the label in a PR dataset is highly imbalanced, using a discrete measure like F1-score might lead to a high variance reward, hence, inaccurate estimation. Moreover, we aim to train the GPT2 such that the model can learn to generate a sample $\bgenpr$ that resembles the language style in the development set $\ddevpr$. Intuitively, the generated text should be similar to the spoken human language if the gradient updates of the model trained on $\bgenpr$ and $\ddevpr$ are aligned. Formally, the reward $r_i$ for each batch of generated texts $\bgenpr$  is computed as follows: 
\begin{equation}
    r_i = \nabla_{\theta}\mathcal{L}(\bgenpr_i; \theta_{t-1}) \cdot
    \sum_{\mathcal{B}_j \in \ddevpr}\frac{\nabla_{\theta}\mathcal{L}(\mathcal{B}_j; \theta_{t-1}))}{|\ddevpr|}
\end{equation}
where $\mathcal{L}(\mathcal{B}; \theta_{t-1})$ is the cross-entropy of training the PR model $\modelpr_{t-1}$ on the sample $\mathcal{B}$ and $\cdot$ denotes dot product.

Finally, the GPT2 model is trained to maximize negative log-likelihood:
\begin{equation}
\mathcal{L}_G=-\sum_{\mathcal{B}_i \in \bgenpr}r_ilog P(B_i)
\end{equation}

