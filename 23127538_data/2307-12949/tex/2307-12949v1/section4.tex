\section{Experiments}

\input{table-behance}



\textbf{Settings}: In this paper, we evaluate our proposed model on two available English datasets that have been used in previous studies. 
\textbf{IWSLT} is the benchmark dataset for the PR task in English. It annotates three prominent punctuation marks: \textit{PERIOD, COMMA, QUESTION}. The IWSLT corpus contains texts derived from TED Talks, which are mainly monologues. The testing set of this corpus contains both reference text (REF), which is well-written text, and transcribed text (ASR) with manually inserted punctuation. Whereas the training set consists of only REF text. The training, development, and test sets contain approximately 2.1M, 300K, and 12K words, respectively. 
\textbf{BehancePR} is a human-annotated dataset for livestreaming videos. It features multiple speakers as well as interaction with a large number of audiences. BehancePR corpus contains only ASR text. The training/development/testing sets contain approximately 1.2M, 34K, and 44K words, respectively. 
The models are evaluated using the standard precision, recall, and F1-score (micro).


\textbf{Hyperparameters}: In this paper, each input word is tokenized using the word-piece tokenizer provided in the PLM. The representation of the first word-piece is collected as the input of the classifier head, which is a fully connected layer, to predict the punctuation.
We employed the DeBERTa-large PLM \cite{he2021deberta} as the encoder of the PR model. The hidden states of the top 8 layers are used as the representation of a token, searched from a pool of \{1,4,8,12\} layers. The GPT2-medium is used to generate the text. The seed texts for the GPT2 model contain 64 consecutive words randomly sampled from these pools. Both models are trained using the Adam optimizer with a learning rate in \{2e-5, 5e-5\}. The augmentation ratios $\alpha_1,\alpha_2,\alpha_3$ are set to 5\%, similar to \cite{alam-etal-2020-punctuation}. We concatenate $C=20$ context words to the head and tail of each chunk. Due to the high cost of evaluating the PR model on the whole development set, in each iteration, we only sample a subset $|B_j|=16$ chunks from $\mathcal{D}_{dev}$ to compute the reward.



\subsection{IWSLT corpus}

\textbf{Baselines}: We compared our model with the state-of-the-art PR models: \textbf{RoBERTa-large+Augmentation} model employs a RoBERTa-large PLM \cite{alam-etal-2020-punctuation}. The input data is augmented using three augmentation strategies: insertion, substitution, and deletion. \textbf{ELECTRA-base+Multitask} \cite{hentschel2021making} is finetuned using additional augmentation detection loss and knowledge distillation loss. \textbf{ELECTRA-large+Discriminative Self-Training} \cite{chen2021discriminative} was self-trained with a discriminator to detect human-annotated data and pseudo-machine-labeled data. \textbf{Funnel-transformer-xlarge+POSFusion} \cite{shi2021incorporating} incorporates additional part-of-speech features from an external neural-network based POS tagger.

\textbf{Results}: Table \ref{table-result-ted} compares the examined models' performance on both the REF test set and the ASR test set. The performance on the REF test set shows us the performance in case the ASR text is close to the written text, while the ASR test shows the actual performance on ASR text.

On the REF test set, ELECTRA-large is the best model among the five examined PLMs with an F1 score of 84.4\%, and it is closely followed by DeBERTa-large (0.3\%  lower). These models leave a large margin to the smaller models such as ELECTRA-base (approx. 3\% lower). Comparing the full models, our DeBERTA-large + RL model gains 1\% over the DeBERTa-large model, achieving 85.1\%. This performance is on par with the ELECTRA-large + Discriminative Self-Training model with a mere margin of 0.1\%. 


% On the REF test set, ELECTRA-large is the best model among the five examined PLMs with an F1 score of 84.4\%, and it is closely followed by DeBERTa-large (0.3\%  lower) and funnel-transformer-xlarge (0.7\% lower). This is reasonable given their similar sizes and architectures \cite{clark2020electra,he2021deberta}. These models leave a large margin to the smaller models: RoBERTa-large (approx. 2\% lower) and ELECTRA-base (approx. 3\% lower). Second, comparing the full models, our DeBERTA-large + RL model gains 1\% over the DeBERTa-large model, achieving 85.1\%. This performance is on par with the ELECTRA-large + Discriminative Self-Training model with a mere margin of 0.1\%. Third, comparing the performances for each punctuation, even though the DeBERTa-large + RL loses on QUESTION with substantially lower performance compared to ELECTRA-base and ELECTRA-large models, it yields an identical F1 score to the F1 score of the ELECTRA-large+Discriminative Self-Training model on both COMMA and PERIOD, which account for more than 90\% of the dataset. This experiment shows the effectiveness of the RL training process in providing helpful examples for training the PR model on reference data.


For ASR text, comparing the full models, our DeBERTa-large + RL model (77\% in terms of overall F1) outperforms all the other models at a large margin of 3\% to the highest competitor, RoBERTa-large + Augmentation,  with $p<0.01$. Moreover, without additional training signals or external features, the DeBERTa-large model yields similar performance to other PLMs (e.g., RoBERTa-large and funnel-transformer-xlarge). Furthermore, our proposed model outperforms the other models on all three punctuation marks with a consistently large margin ranging from 1.8\% to 5.1\%, compared to the next highest. These results clearly show the robustness of our proposed RL method to boost the performance of real-world ASR data significantly. The improvement suggests that the RL method has provided helpful training examples to help the model bridge the gap between the REF text and the ASR text in the training and testing data, respectively.


\subsection{BehancePR corpus}

\textbf{Baselines}: We compare our models with the state-of-the-art models that have been evaluated on this corpus. These models include the \textbf{RoBERTa-large} model and its variants with \textbf{Data Augmentation} and \textbf{Conditional Random Field} \cite{alam-etal-2020-punctuation}.

\textbf{Results}: First, we found that data augmentation does not improve the performance of the model trained on the BehancePR dataset. The reason is that the BehancePR dataset's training and testing data are all ASR texts, which is different from the IWSLT corpus in which the training texts are REF texts, and the testing texts are ASR texts. As such, introducing data augmentation skewed the distribution of training and testing data in the BehancePR corpus. Hence, hurting the model's performance. Table \ref{table-result-behance} presents the overall performance of our proposed models on the BehancePR corpus. The DeBERTa-large outperforms the current state-of-the-art  RoBERTa-large+CRF model (62.2\% versus 62.9\%). Furthermore, the DeBERTa-large + RL improves the F1 score from 64.2\% to 65.2\% (+1.0) (statistically significant with $p<0.01$). This again shows the effectiveness of the proposed reinforcement learning methods.



\input{table-ablation}

\subsection{Ablation study}

We perform an ablation study to examine the contribution of each component of the model on the IWSLT ASR test set as shown in Table \ref{table-ablation} (Rows 1-7). Adding the augmentation to the DeBERTa-large model boosts the performance from 71.4\% to 75.0\% (\textbf{+3.6\%}), while \textit{GPT} improves the F1 score from 75.0\% to 75.6\% (\textbf{+0.6\%}). Finally, when we add \textit{RL}, the F1 score jumps from 75.6\% to 77.0\%. These demonstrate that all the proposed components contribute to the improvement. However, data augmentation and RL contribute largely to the performance gain on the IWSLT ASR test set. Finally, to further show the effectiveness of the \textit{RL}, we add it to the RoBERTa-large+Augmentation, resulting in an increase of 1\% in the F1 score. This experiment shows that our RL method is model-agnostic that can be applied to any PR model.

The PR model and the GPT2 model could be finetuned/pre-trained with different strategies. To examine whether finetuned or pre-trained model before the reinforcement learning could further improve the performance of the model. We used the configuration of the full model with GPT2 and RL. However, for the PR model, we trained the PR alone with the same training data for 1 and 2 epochs. Similarly, we pre-trained the GPT2 model on the unsupervised text derived from the training set for the same epochs. Table \ref{table-ablation} (Rows 8-12) reports the performance of these runs. As can be seen from the performance, training/finetuning the model using only PR or GPT2 data significantly hurts the performance of the model. In particular, pretraining a single epoch on PR or GPT2  reduced the performance by 0.4\% to 0.7\%, respectively. Further training the model for one more epoch decreased the performance by 0.4\% to 0.5\%, respectively.


% \input{tab:pretraining}

