{
  "title": "A faster and simpler algorithm for learning shallow networks",
  "authors": [
    "Sitan Chen",
    "Shyam Narayanan"
  ],
  "submission_date": "2023-07-24T03:04:10+00:00",
  "revised_dates": [],
  "abstract": "We revisit the well-studied problem of learning a linear combination of $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional Gaussian measure. Chen et al. [CDG+23] recently gave the first algorithm for this problem to run in $\\text{poly}(d,1/\\varepsilon)$ time when $k = O(1)$, where $\\varepsilon$ is the target error. More precisely, their algorithm runs in time $(d/\\varepsilon)^{\\mathrm{quasipoly}(k)}$ and learns over multiple stages. Here we show that a much simpler one-stage version of their algorithm suffices, and moreover its runtime is only $(d/\\varepsilon)^{O(k^2)}$.",
  "categories": [
    "cs.LG",
    "cs.DS",
    "stat.ML"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12496",
  "pdf_url": null,
  "comment": "14 pages",
  "num_versions": null,
  "size_before_bytes": 58612,
  "size_after_bytes": 59254
}