
% @misc{Authors14,
%  author = {Full Author Name},
%  title = {The Frobnicatable Foo Filter},
%  note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
%  year = 2014
% }

% @misc{Authors14b,
%  author = {Full Author Name},
%  title = {Frobnication Tutorial},
%  note = {Supplied as additional material {\tt tr.pdf}},
%  year = 2014
% }

% @article{Alpher02,
% author = {Alvin Alpher},
% title = {Frobnication},
% journal = {Journal of Foo},
% volume = 12, 
% number = 1, 
% pages = {234--778}, 
% year = 2002
% }

% @article{Alpher03,
% author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe},
% title = {Frobnication Revisited},
% journal = {Journal of Foo},
% volume = 13, 
% number = 1, 
% pages = {234--778}, 
% year = 2003
% }

% @article{Alpher04,
% author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe and Gavin Gamow},
% title = {Can a Machine Frobnicate?},
% journal = {Journal of Foo},
% volume = 14, 
% number = 1, 
% pages = {234--778}, 
% year = 2004
% }


@inproceedings{c1,
  author    = {Dan Hendrycks and
               Thomas G. Dietterich},
  title     = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=HJz6tiCqYm},
  timestamp = {Thu, 25 Jul 2019 14:25:46 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HendrycksD19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{c2,
  author    = {Benjamin Recht and
               Rebecca Roelofs and
               Ludwig Schmidt and
               Vaishaal Shankar},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Do ImageNet Classifiers Generalize to ImageNet?},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {5389--5400},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/recht19a.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/RechtRSS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{c3,
  author    = {Shiv Shankar and
               Vihari Piratla and
               Soumen Chakrabarti and
               Siddhartha Chaudhuri and
               Preethi Jyothi and
               Sunita Sarawagi},
  title     = {Generalizing Across Domains via Cross-Gradient Training},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=r1Dx7fbCW},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ShankarPCCJS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{c4,
  doi = {10.48550/ARXIV.1710.03077},
  
  url = {https://arxiv.org/abs/1710.03077},
  
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deeper, Broader and Artier Domain Generalization},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{c5,
 author = {Balaji, Yogesh and Sankaranarayanan, Swami and Chellappa, Rama},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {MetaReg: Towards Domain Generalization using Meta-Regularization},
 url = {https://proceedings.neurips.cc/paper/2018/file/647bba344396e7c8170902bcf2e15551-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{c6,
  title={Certifying some distributional robustness with principled adversarial training},
  author={Sinha, Aman and Namkoong, Hongseok and Volpi, Riccardo and Duchi, John},
  journal={arXiv preprint arXiv:1710.10571},
  year={2017}
}

@article{c7,
  title={Certified defenses against adversarial examples},
  author={Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
  journal={arXiv preprint arXiv:1801.09344},
  year={2018}
}

@inproceedings{c8,
  title={Provable defenses against adversarial examples via the convex outer adversarial polytope},
  author={Wong, Eric and Kolter, Zico},
  booktitle={International Conference on Machine Learning},
  pages={5286--5295},
  year={2018},
  organization={PMLR}
}

@inproceedings{c9,
  title={Adversarial discriminative domain adaptation},
  author={Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7167--7176},
  year={2017}
}

@article{c10,
  title={Adversarial deep averaging networks for cross-lingual sentiment classification},
  author={Chen, Xilun and Sun, Yu and Athiwaratkun, Ben and Cardie, Claire and Weinberger, Kilian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={557--570},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{c11,
  title={Cycada: Cycle-consistent adversarial domain adaptation},
  author={Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={1989--1998},
  year={2018},
  organization={Pmlr}
}


@InProceedings{ttt-rot,
  title = 	 {Test-Time Training with Self-Supervision for Generalization under Distribution Shifts},
  author =       {Sun, Yu and Wang, Xiaolong and Liu, Zhuang and Miller, John and Efros, Alexei and Hardt, Moritz},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9229--9248},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sun20b/sun20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sun20b.html},
  abstract = 	 {In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a single unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts.}
}


@inproceedings{c13,
title={Tent: Fully Test-Time Adaptation by Entropy Minimization},
author={Dequan Wang and Evan Shelhamer and Shaoteng Liu and Bruno Olshausen and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=uXl3bZLkr3c}
}

@ARTICLE{c14,
  author={Moorthy, Anush Krishna and Bovik, Alan Conrad},
  journal={IEEE Transactions on Image Processing}, 
  title={Blind Image Quality Assessment: From Natural Scene Statistics to Perceptual Quality}, 
  year={2011},
  volume={20},
  number={12},
  pages={3350-3364},
  doi={10.1109/TIP.2011.2147325}}

@article{c15,
title = {Free-energy principle inspired visual quality assessment: An overview},
journal = {Digital Signal Processing},
volume = {91},
pages = {11-20},
year = {2019},
note = {Quality Perception of Advanced Multimedia Systems},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S105120041930020X},
author = {Guangtao Zhai and Xiongkuo Min and Ning Liu},
keywords = {Free energy principle, Image quality assessment, Generative model, Visual quality, Image processing},
abstract = {The free energy principle was proposed several years ago as a unified justification for some brain theories. The process of human perception, cognition, action, and learning can be well explained using the free energy principle. The free energy principle suggests that the human perception and understanding of a given scene can be modeled as an active inference process, and the human brain tries to explain the scene using an internal generative model. The discrepancy between the given image or view and its best internal generative model explainable part is upper bounded by the free energy of the inference process. It was then conjectured that perceptual quality of the input image is closely related to free energy value of the process. Following this framework, dozens of visual quality assessment techniques have been proposed in the last few years and many have achieved state of the art performance. In this paper, we first give an overview of the free energy principle and then review the free energy principle inspired visual quality assessment metrics with a comparison in terms of algorithm design and performance.}
}

@ARTICLE{c16,
  author={Mittal, Anish and Moorthy, Anush Krishna and Bovik, Alan Conrad},
  journal={IEEE Transactions on Image Processing}, 
  title={No-Reference Image Quality Assessment in the Spatial Domain}, 
  year={2012},
  volume={21},
  number={12},
  pages={4695-4708},
  doi={10.1109/TIP.2012.2214050}}

@inproceedings{c17,
  title={Convolutional neural networks for no-reference image quality assessment},
  author={Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1733--1740},
  year={2014}
}

@article{c18,
  title={Free-energy principle inspired visual quality assessment: An overview},
  author={Zhai, Guangtao and Min, Xiongkuo and Liu, Ning},
  journal={Digital Signal Processing},
  volume={91},
  pages={11--20},
  year={2019},
  publisher={Elsevier}
}

@ARTICLE{c19,
  author={Liu, Yutao and Gu, Ke and Zhang, Yongbing and Li, Xiu and Zhai, Guangtao and Zhao, Debin and Gao, Wen},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Unsupervised Blind Image Quality Evaluation via Statistical Measurements of Structure, Naturalness, and Perception}, 
  year={2020},
  volume={30},
  number={4},
  pages={929-943},
  doi={10.1109/TCSVT.2019.2900472}}


@article{c20,
  title={The free-energy principle: a unified brain theory?},
  author={Friston, Karl},
  journal={Nature reviews neuroscience},
  volume={11},
  number={2},
  pages={127--138},
  year={2010},
  publisher={Nature publishing group}
}

@article{c21,
  title={A psychovisual quality metric in free-energy principle},
  author={Zhai, Guangtao and Wu, Xiaolin and Yang, Xiaokang and Lin, Weisi and Zhang, Wenjun},
  journal={IEEE Transactions on Image Processing},
  volume={21},
  number={1},
  pages={41--52},
  year={2011},
  publisher={IEEE}
}

@article{c22,
  title={No-reference screen content image quality assessment with unsupervised domain adaptation},
  author={Chen, Baoliang and Li, Haoliang and Fan, Hongfei and Wang, Shiqi},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={5463--5476},
  year={2021},
  publisher={IEEE}
}

@InProceedings{c23,
author = {Ying, Zhenqiang and Niu, Haoran and Gupta, Praful and Mahajan, Dhruv and Ghadiyaram, Deepti and Bovik, Alan},
title = {From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{c24,
author = {Hosu, Vlad and Lin, Hanhe and Sziranyi, Tamas and Saupe, Dietmar},
title = {KonIQ-10k: An Ecologically Valid Database for Deep Learning of Blind Image Quality Assessment},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {29},
issn = {1057-7149},
url = {https://doi.org/10.1109/TIP.2020.2967829},
doi = {10.1109/TIP.2020.2967829},
abstract = {Deep learning methods for image quality assessment (IQA) are limited due to the small size of existing datasets. Extensive datasets require substantial resources both for generating publishable content and annotating it accurately. We present a systematic and scalable approach to creating KonIQ-10k, the largest IQA dataset to date, consisting of 10,073 quality scored images. It is the first in-the-wild database aiming for ecological validity, concerning the authenticity of distortions, the diversity of content, and quality-related indicators. Through the use of crowdsourcing, we obtained 1.2 million reliable quality ratings from 1,459 crowd workers, paving the way for more general IQA models. We propose a novel, deep learning model (KonCept512), to show an excellent generalization beyond the test set (0.921 SROCC), to the current state-of-the-art database LIVE-in-the-Wild (0.825 SROCC). The model derives its core performance from the InceptionResNet architecture, being trained at a higher resolution than previous models ( $512times 384$ ). Correlation analysis shows that KonCept512 performs similar to having 9 subjective scores for each test image.},
journal = {Trans. Img. Proc.},
month = {jan},
pages = {4041–4056},
numpages = {16}
}

@InProceedings{c25,
author="Jinjin, Gu
and Haoming, Cai
and Haoyu, Chen
and Xiaoxing, Ye
and Ren, Jimmy S.
and Chao, Dong",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="PIPAL: A Large-Scale Image Quality Assessment Dataset for Perceptual Image Restoration",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="633--651",
abstract="Image quality assessment (IQA) is the key factor for the fast development of image restoration (IR) algorithms. The most recent IR methods based on Generative Adversarial Networks (GANs) have achieved significant improvement in visual performance, but also presented great challenges for quantitative evaluation. Notably, we observe an increasing inconsistency between perceptual quality and the evaluation results. Then we raise two questions: (1) Can existing IQA methods objectively evaluate recent IR algorithms? (2) When focus on beating current benchmarks, are we getting better IR algorithms? To answer these questions and promote the development of IQA methods, we contribute a large-scale IQA dataset, called Perceptual Image Processing Algorithms (PIPAL) dataset. Especially, this dataset includes the results of GAN-based methods, which are missing in previous datasets. We collect more than 1.13 million human judgments to assign subjective scores for PIPAL images using the more reliable ``Elo system''. Based on PIPAL, we present new benchmarks for both IQA and super-resolution methods. Our results indicate that existing IQA methods cannot fairly evaluate GAN-based IR algorithms. While using appropriate evaluation methods is important, IQA methods should also be updated along with the development of IR algorithms. At last, we improve the performance of IQA networks on GAN-based distortions by introducing anti-aliasing pooling. Experiments show the effectiveness of the proposed method.",
isbn="978-3-030-58621-8"
}


@ARTICLE{c26,
  author={Virtanen, Toni and Nuutinen, Mikko and Vaahteranoksa, Mikko and Oittinen, Pirkko and Häkkinen, Jukka},
  journal={IEEE Transactions on Image Processing}, 
  title={CID2013: A Database for Evaluating No-Reference Image Quality Assessment Algorithms}, 
  year={2015},
  volume={24},
  number={1},
  pages={390-402},
  doi={10.1109/TIP.2014.2378061}}

@article{c27,
  title={A survey of unsupervised deep domain adaptation},
  author={Wilson, Garrett and Cook, Diane J},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={11},
  number={5},
  pages={1--46},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{c28,
  title={An introduction to domain adaptation and transfer learning},
  author={Kouw, Wouter M and Loog, Marco},
  journal={arXiv preprint arXiv:1812.11806},
  year={2018}
}

@article{c29,
  title={Deep visual domain adaptation: A survey},
  author={Wang, Mei and Deng, Weihong},
  journal={Neurocomputing},
  volume={312},
  pages={135--153},
  year={2018},
  publisher={Elsevier}
}

@article{c30,
  title={A kernel method for the two-sample problem},
  author={Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte J and Scholkopf, Bernhard and Smola, Alexander J},
  journal={arXiv preprint arXiv:0805.2368},
  year={2008}
}

@inproceedings{c31,
  title={Return of frustratingly easy domain adaptation},
  author={Sun, Baochen and Feng, Jiashi and Saenko, Kate},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{c32,
  title={Contrastive adaptation network for unsupervised domain adaptation},
  author={Kang, Guoliang and Jiang, Lu and Yang, Yi and Hauptmann, Alexander G},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4893--4902},
  year={2019}
}

@article{c33,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@article{c34,
  title={Unsupervised domain adaptation in the wild via disentangling representation learning},
  author={Li, Haoliang and Wan, Renjie and Wang, Shiqi and Kot, Alex C},
  journal={International Journal of Computer Vision},
  volume={129},
  number={2},
  pages={267--283},
  year={2021},
  publisher={Springer}
}

@article{c35,
  title={Unpaired multi-domain image generation via regularized conditional gans},
  author={Mao, Xudong and Li, Qing},
  journal={arXiv preprint arXiv:1805.02456},
  year={2018}
}

@inproceedings{c36,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2223--2232},
  year={2017}
}

@inproceedings{c37,
 author = {Liu, Yuejiang and Kothari, Parth and van Delft, Bastien and Bellot-Gurlet, Baptiste and Mordan, Taylor and Alahi, Alexandre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21808--21820},
 publisher = {Curran Associates, Inc.},
 title = {TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?},
 url = {https://proceedings.neurips.cc/paper/2021/file/b618c3210e934362ac261db280128c22-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{simclr,
author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
title = {A Simple Framework for Contrastive Learning of Visual Representations},
year = {2020},
publisher = {JMLR.org},
abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {149},
numpages = {11},
series = {ICML'20}
}


@inproceedings{c39,
 author = {Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11539--11551},
 publisher = {Curran Associates, Inc.},
 title = {Improving robustness against common corruptions by covariate shift adaptation},
 url = {https://proceedings.neurips.cc/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{c40,
  title = 	 {Do We Really Need to Access the Source Data? {S}ource Hypothesis Transfer for Unsupervised Domain Adaptation},
  author =       {Liang, Jian and Hu, Dapeng and Feng, Jiashi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6028--6039},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/liang20a/liang20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/liang20a.html},
  abstract = 	 {Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.}
}


@article{c41,
  title={An improved evaluation framework for generative adversarial networks},
  author={Liu, Shaohui and Wei, Yi and Lu, Jiwen and Zhou, Jie},
  journal={arXiv preprint arXiv:1803.07474},
  year={2018}
}

@article{c42,
  title={Test-time batch statistics calibration for covariate shift},
  author={You, Fuming and Li, Jingjing and Zhao, Zhou},
  journal={arXiv preprint arXiv:2110.04065},
  year={2021}
}

@inproceedings{c43,
  title={Minimum class confusion for versatile domain adaptation},
  author={Jin, Ying and Wang, Ximei and Long, Mingsheng and Wang, Jianmin},
  booktitle={European Conference on Computer Vision},
  pages={464--480},
  year={2020},
  organization={Springer}
}

@inproceedings{c44,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013}
}

@InProceedings{c45,
author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
title = {Unsupervised Visual Representation Learning by Context Prediction},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@inproceedings{c46,
  title={Unsupervised learning of visual representations by solving jigsaw puzzles},
  author={Noroozi, Mehdi and Favaro, Paolo},
  booktitle={European conference on computer vision},
  pages={69--84},
  year={2016},
  organization={Springer}
}

@InProceedings{c47,
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
title = {Colorization as a Proxy Task for Visual Understanding},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@InProceedings{c48,
  title = 	 {Unsupervised Learning by Predicting Noise},
  author =       {Piotr Bojanowski and Armand Joulin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {517--526},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/bojanowski17a/bojanowski17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/bojanowski17a.html},
  abstract = 	 {Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision; this paper introduces a generic framework to train such networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of the features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with the state-of-the-arts among unsupervised methods on ImageNet and Pascal VOC.}
}

@InProceedings{c49,
author="Caron, Mathilde
and Bojanowski, Piotr
and Joulin, Armand
and Douze, Matthijs",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="Deep Clustering for Unsupervised Learning of Visual Features",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="139--156",
abstract="Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large-scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.",
isbn="978-3-030-01264-9"
}

@article{c50,
  title={Big self-supervised models are strong semi-supervised learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22243--22255},
  year={2020}
}

@inproceedings{c51,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008}
}

@inproceedings{c52,
  title={Unsupervised feature learning framework for no-reference image quality assessment},
  author={Ye, Peng and Kumar, Jayant and Kang, Le and Doermann, David},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={1098--1105},
  year={2012},
  organization={IEEE}
}

@ARTICLE{c53,
  author={Xu, Jingtao and Ye, Peng and Li, Qiaohong and Du, Haiqing and Liu, Yong and Doermann, David},
  journal={IEEE Transactions on Image Processing}, 
  title={Blind Image Quality Assessment Based on High Order Statistics Aggregation}, 
  year={2016},
  volume={25},
  number={9},
  pages={4444-4457},
  doi={10.1109/TIP.2016.2585880}}

% @article{c54,
%   title={Blind image quality assessment using a deep bilinear convolutional neural network},
%   author={Zhang, Weixia and Ma, Kede and Yan, Jia and Deng, Dexiang and Wang, Zhou},
%   journal={IEEE Transactions on Circuits and Systems for Video Technology},
%   volume={30},
%   number={1},
%   pages={36--47},
%   year={2018},
%   publisher={IEEE}
% }

@INPROCEEDINGS{c55,
  author={Ye, Peng and Kumar, Jayant and Kang, Le and Doermann, David},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Unsupervised feature learning framework for no-reference image quality assessment}, 
  year={2012},
  volume={},
  number={},
  pages={1098-1105},
  doi={10.1109/CVPR.2012.6247789}}

% @article{c56,
%   title={Sgd-net: Efficient model-based deep learning with theoretical guarantees},
%   author={Liu, Jiaming and Sun, Yu and Gan, Weijie and Xu, Xiaojian and Wohlberg, Brendt and Kamilov, Ulugbek S},
%   journal={IEEE Transactions on Computational Imaging},
%   volume={7},
%   pages={598--610},
%   year={2021},
%   publisher={IEEE}
% }


@ARTICLE{nrfr,
  author={Bosse, Sebastian and Maniry, Dominique and Müller, Klaus-Robert and Wiegand, Thomas and Samek, Wojciech},
  journal={IEEE Transactions on Image Processing}, 
  title={Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment}, 
  year={2018},
  volume={27},
  number={1},
  pages={206-219},
  doi={10.1109/TIP.2017.2760518}}

@InProceedings{c57,
author = {Liu, Xialei and van de Weijer, Joost and Bagdanov, Andrew D.},
title = {RankIQA: Learning From Rankings for No-Reference Image Quality Assessment},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@ARTICLE{c58,
  author={Ma, Kede and Liu, Wentao and Zhang, Kai and Duanmu, Zhengfang and Wang, Zhou and Zuo, Wangmeng},
  journal={IEEE Transactions on Image Processing}, 
  title={End-to-End Blind Image Quality Assessment Using Deep Neural Networks}, 
  year={2018},
  volume={27},
  number={3},
  pages={1202-1213},
  doi={10.1109/TIP.2017.2774045}}

% @article{c58,
%   title={Image quality assessment using contrastive learning},
%   author={Madhusudana, Pavan C and Birkbeck, Neil and Wang, Yilin and Adsumilli, Balu and Bovik, Alan C},
%   journal={IEEE Transactions on Image Processing},
%   volume={31},
%   pages={4149--4161},
%   year={2022},
%   publisher={IEEE}
% }

@inproceedings{c59,
  title={Learning disentangled feature representation for hybrid-distorted image restoration},
  author={Li, Xin and Jin, Xin and Lin, Jianxin and Liu, Sen and Wu, Yaojun and Yu, Tao and Zhou, Wei and Chen, Zhibo},
  booktitle={European Conference on Computer Vision},
  pages={313--329},
  year={2020},
  organization={Springer}
}

@inproceedings{c60,
  title={LIRA: Lifelong Image Restoration from Unknown Blended Distortions},
  author={Liu, Jianzhao and Lin, Jianxin and Li, Xin and Zhou, Wei and Liu, Sen and Chen, Zhibo},
  booktitle={European Conference on Computer Vision},
  pages={616--632},
  year={2020},
  organization={Springer}
}

@ARTICLE{c61,
  author={Zhang, Weixia and Ma, Kede and Yan, Jia and Deng, Dexiang and Wang, Zhou},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Blind Image Quality Assessment Using a Deep Bilinear Convolutional Neural Network}, 
  year={2020},
  volume={30},
  number={1},
  pages={36-47},
  doi={10.1109/TCSVT.2018.2886771}}

@inproceedings{c62,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@InProceedings{c63,
author = {Su, Shaolin and Yan, Qingsen and Zhu, Yu and Zhang, Cheng and Ge, Xin and Sun, Jinqiu and Zhang, Yanning},
title = {Blindly Assess Image Quality in the Wild Guided by a Self-Adaptive Hyper Network},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{c64,
  title={LIQA: Lifelong blind image quality assessment},
  author={Liu, Jianzhao and Zhou, Wei and Li, Xin and Xu, Jiahua and Chen, Zhibo},
  journal={IEEE Transactions on Multimedia},
  year={2022},
  publisher={IEEE}
}

@article{c65,
  title={dipIQ: Blind image quality assessment by learning-to-rank discriminable image pairs},
  author={Ma, Kede and Liu, Wentao and Liu, Tongliang and Wang, Zhou and Tao, Dacheng},
  journal={IEEE Transactions on Image Processing},
  volume={26},
  number={8},
  pages={3951--3964},
  year={2017},
  publisher={IEEE}
}

% @article{c6,
%   title={Using self-supervised learning can improve model robustness and uncertainty},
%   author={Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
%   journal={Advances in neural information processing systems},
%   volume={32},
%   year={2019}
% }

% @article{c7,
%   title={Using self-supervised learning can improve model robustness and uncertainty},
%   author={Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
%   journal={Advances in neural information processing systems},
%   volume={32},
%   year={2019}
% }


@InProceedings{tres,
  title={No-Reference Image Quality Assessment via Transformers, Relative Ranking, and Self-Consistency},
  author={Golestaneh, S Alireza and Dadsetan, Saba and Kitani, Kris M},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3209--3218},
  year={2022}
}

@InProceedings{c67,
    author    = {Ke, Junjie and Wang, Qifei and Wang, Yilin and Milanfar, Peyman and Yang, Feng},
    title     = {MUSIQ: Multi-Scale Image Quality Transformer},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {5148-5157}
}

@article{c68,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{c69,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{c70,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{c71,
  title={AVA: A large-scale database for aesthetic visual analysis},
  author={Murray, Naila and Marchesotti, Luca and Perronnin, Florent},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={2408--2415},
  year={2012},
  organization={IEEE}
}

@article{c72,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{c73,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{c74,
  title={Evaluating prediction-time batch normalization for robustness under covariate shift},
  author={Nado, Zachary and Padhy, Shreyas and Sculley, D and D'Amour, Alexander and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={arXiv preprint arXiv:2006.10963},
  year={2020}
}

@inproceedings{c75,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@inproceedings{c76,
  title={Group normalization},
  author={Wu, Yuxin and He, Kaiming},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}

@inproceedings{c77,
  TITLE = {{Unsupervised representation learning by predicting image rotations}},
  AUTHOR = {Komodakis, Nikos and Gidaris, Spyros},
  URL = {https://hal-enpc.archives-ouvertes.fr/hal-01832768},
  BOOKTITLE = {{International Conference on Learning Representations (ICLR)}},
  ADDRESS = {Vancouver, Canada},
  YEAR = {2018},
  MONTH = Apr,
  HAL_ID = {hal-01832768},
  HAL_VERSION = {v1},
}

@inproceedings{c78,
  added-at = {2008-06-17T16:01:02.000+0200},
  address = {New York, NY, USA},
  author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
  biburl = {https://www.bibsonomy.org/bibtex/2a00e5dd434027770eae311854b0bc88a/pprett},
  booktitle = {ICML '05: Proceedings of the 22nd international conference on Machine learning},
  citeulike-article-id = {1714577},
  comment = {Review on "Geeking with Greg": 
http://glinden.blogspot.com/2005/06/msn-search-and-learning-to-rank.html},
  doi = {10.1145/1102351.1102363},
  interhash = {700415055c8bc48d64de26303b25533c},
  intrahash = {a00e5dd434027770eae311854b0bc88a},
  isbn = {1595931805},
  keywords = {learning, machine, msr, networks, neural, ranking, ranknet, search, web},
  pages = {89--96},
  posted-at = {2007-10-01 10:27:11},
  priority = {0},
  publisher = {ACM Press},
  timestamp = {2008-06-17T16:02:06.000+0200},
  title = {Learning to rank using gradient descent},
  url = {http://portal.acm.org/citation.cfm?id=1102351.1102363},
  year = 2005
}

@InProceedings{meta,
author = {Zhu, Hancheng and Li, Leida and Wu, Jinjian and Dong, Weisheng and Shi, Guangming},
title = {MetaIQA: Deep Meta-Learning for No-Reference Image Quality Assessment},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{c80,
  doi = {10.48550/ARXIV.2203.13591},
  
  url = {https://arxiv.org/abs/2203.13591},
  
  author = {Wang, Qin and Fink, Olga and Van Gool, Luc and Dai, Dengxin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Continual Test-Time Domain Adaptation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{kadid,
  author={Lin, Hanhe and Hosu, Vlad and Saupe, Dietmar},
  booktitle={2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX)}, 
  title={KADID-10k: A Large-scale Artificially Distorted IQA Database}, 
  year={2019},
  volume={},
  number={},
  pages={1-3},
  doi={10.1109/QoMEX.2019.8743252}}

@article{tid,
title = {Image database TID2013: Peculiarities, results and perspectives},
journal = {Signal Processing: Image Communication},
volume = {30},
pages = {57-77},
year = {2015},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2014.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0923596514001490},
author = {Nikolay Ponomarenko and Lina Jin and Oleg Ieremeiev and Vladimir Lukin and Karen Egiazarian and Jaakko Astola and Benoit Vozel and Kacem Chehdi and Marco Carli and Federica Battisti and C.-C. {Jay Kuo}},
keywords = {Image visual quality metrics, Image denoising, Image lossy compression},
abstract = {This paper describes a recently created image database, TID2013, intended for evaluation of full-reference visual quality assessment metrics. With respect to TID2008, the new database contains a larger number (3000) of test images obtained from 25 reference images, 24 types of distortions for each reference image, and 5 levels for each type of distortion. Motivations for introducing 7 new types of distortions and one additional level of distortions are given; examples of distorted images are presented. Mean opinion scores (MOS) for the new database have been collected by performing 985 subjective experiments with volunteers (observers) from five countries (Finland, France, Italy, Ukraine, and USA). The availability of MOS allows the use of the designed database as a fundamental tool for assessing the effectiveness of visual quality. Furthermore, existing visual quality metrics have been tested with the proposed database and the collected results have been analyzed using rank order correlation coefficients between MOS and considered metrics. These correlation indices have been obtained both considering the full set of distorted images and specific image subsets, for highlighting advantages and drawbacks of existing, state of the art, quality metrics. Approaches to thorough performance analysis for a given metric are presented to detect practical situations or distortion types for which this metric is not adequate enough to human perception. The created image database and the collected MOS values are freely available for downloading and utilization for scientific purposes.}
}

@ARTICLE{live,
  author={Sheikh, H.R. and Sabir, M.F. and Bovik, A.C.},
  journal={IEEE Transactions on Image Processing}, 
  title={A Statistical Evaluation of Recent Full Reference Image Quality Assessment Algorithms}, 
  year={2006},
  volume={15},
  number={11},
  pages={3440-3451},
  doi={10.1109/TIP.2006.881959}}
  
@article{mae, 
        title={Test-Time Training with Masked Autoencoders},
        author={Gandelsman, Yossi and Sun, Yu and Chen, Xinlei and Efros, Alexei A.},
        year={2022},
        journal={arXiv preprint arXiv:2209.07522}
}

@article{c81,
  title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author={Dan Hendrycks and Thomas Dietterich},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

@misc{tta-conf,
  doi = {10.48550/ARXIV.2106.14999},
  
  url = {https://arxiv.org/abs/2106.14999},
  
  author = {Mummadi, Chaithanya Kumar and Hutmacher, Robin and Rambach, Kilian and Levinkov, Evgeny and Brox, Thomas and Metzen, Jan Hendrik},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Test-Time Adaptation to Distribution Shift by Confidence Maximization and Input Transformation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@ARTICLE{saad,
  author={Saad, Michele A. and Bovik, Alan C. and Charrier, Christophe},
  journal={IEEE Transactions on Image Processing}, 
  title={Blind Image Quality Assessment: A Natural Scene Statistics Approach in the DCT Domain}, 
  year={2012},
  volume={21},
  number={8},
  pages={3339-3352},
  doi={10.1109/TIP.2012.2191563}}

  @inproceedings{tta-proto,
 author = {Iwasawa, Yusuke and Matsuo, Yutaka},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {2427--2440},
 publisher = {Curran Associates, Inc.},
 title = {Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization},
 url = {https://proceedings.neurips.cc/paper/2021/file/1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{tta-tf,
  title     = {Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment},
  author    = {Kojima, Takeshi and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {1009--1016},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/141},
  url       = {https://doi.org/10.24963/ijcai.2022/141},
}

@article{contrique,
author = {Madhusudana, Pavan C. and Birkbeck, Neil and Wang, Yilin and Adsumilli, Balu and Bovik, Alan C.},
title = {Image Quality Assessment Using Contrastive Learning},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {31},
issn = {1057-7149},
url = {https://doi.org/10.1109/TIP.2022.3181496},
doi = {10.1109/TIP.2022.3181496},
abstract = {We consider the problem of obtaining image quality representations in a self-supervised manner. We use prediction of distortion type and degree as an auxiliary task to learn features from an unlabeled image dataset containing a mixture of synthetic and realistic distortions. We then train a deep Convolutional Neural Network (CNN) using a contrastive pairwise objective to solve the auxiliary problem. We refer to the proposed training framework and resulting deep IQA model as the CONTRastive Image QUality Evaluator (CONTRIQUE). During evaluation, the CNN weights are frozen and a linear regressor maps the learned representations to quality scores in a No-Reference (NR) setting. We show through extensive experiments that CONTRIQUE achieves competitive performance when compared to state-of-the-art NR image quality models, even without any additional fine-tuning of the CNN backbone. The learned representations are highly robust and generalize well across images afflicted by either synthetic or authentic distortions. Our results suggest that powerful quality representations with perceptual relevance can be obtained without requiring large labeled subjective image quality datasets. The implementations used in this paper are available at https://github.com/pavancm/CONTRIQUE.},
journal = {Trans. Img. Proc.},
month = {jan},
pages = {4149–4161},
numpages = {13}
}

