%&latex
% \section{Additional Implementation Details}
% \vspace{-2mm}
% The cross-attention and NeRF architecture is in line with ~\cite{kwon2021nhp}. 



\section{Progressive Rendering}
\input{./Tables/Appendix_Progressive}
\vspace{-2mm}
GP-NeRF~\cite{chen2022gpnerf} has proposed a progressive rendering strategy using the coarse geometry provided by the output 3D feature volume of SPC to reduce the number of rendering points. Although there is no SPC in our framework,  we find that simply using the fitted SMPL as the alternative works pretty well. Specifically, after sampling points on marched rays from the target view, we only render the points whose euclidean distance to the SMPL template is smaller than $0.1m$. Then, for these close points, we first get  the density values for all of them, and then only send part of them whose density value is larger than $0$ for the following color inference, which is in line with ~\cite{chen2022gpnerf}. The effectiveness of this strategy is illustrated in Table ~\ref{tab:progressive}. While without decreasing the performance, the inference time is reduced by around $70\%$. Notably, even without using such accelerating strategy, the inference is still over 2 times faster than NHP~\cite{kwon2021nhp} (56min \vs 1h55min, Table~\ref{tab:efficiency}). This strongly proves the efficiency of our method. 


\section{Performance on Training Frames}
\input{./Tables/Appendix_Fitting}
\vspace{-2mm}
Following previous 
methods~\cite{kwon2021nhp,chen2022gpnerf}, we report the fitting performance on the training set in Table~\ref{tab:fitting}.  We achieve the best fitting performance among the generalizable methods, which shows the superior capacity of our method. 

\section{Additional Ablation Studies}
\vspace{-2mm}
We provide more detailed ablation studies in this section. 

\subsection{Influence of Cluster Number $N_t$}
\input{./Tables/Appendix_Cluster_Number}
\vspace{-2mm}
We study the influence of cluster (token) number by varying it sequentially as $\{100, 300, 500, 1000\}$. As shown in Table~\ref{tab:cluster_number}, too large cluster number does not bring further improvement. As mentioned in \S~\ref{Sec_TransHE}, there exists misalignment between the fitted SMPL and the ground truth body. Larger cluster number may also include more misleading information, and we only intend to take the human representation as the coarse-level guidance, therefore we set $N_t=300$. 


\subsection{Influence of K-nearest Number $N_k$}
\input{./Tables/Appendix_K_Number}
\vspace{-2mm}
We show the influence of k-nearest number $N_k$ in Table~\ref{tab:k_number}. When using no k-nearest fields aggregation, \ie, $N_k=1$, the performance suffers a relatively significant drop in PSNR. This shows that using k-nearest fields aggregation can improve the robustness of human representation. When $N_k>1$, the performance tends to be more stable, and we choose $N_k$ as $7$ since it gives the best performance. 

\subsection{Influence of Perceptual Loss}
\input{./Tables/Appendix_Perceptual_Loss}
\vspace{-2mm}
In Table~\ref{tab:percep}, we demonstrate the influence of perceptual loss. Obviously, perceptual loss can largely improve the LPIPS, \ie, make the results visually pleasing, while shows less effect on PSNR and SSIM. Without perceptual loss, we still outperform previous methods by consistent margins in PSRN and SSIM. 


\subsection{Canonical K-means \vs Canonical Grid Voxelization }
\input{./Figures/kmeans_grid}
\input{./Tables/Appendix_kmeans_grid}
\vspace{-2mm}
In canonical body grouping, we employ the k-means clustering to get the grouping dictionary. Actually, using grid voxelization under the canonical space is also feasible. However, the uniform grid leads to the large variance of vertex number in each voxel considering the shape of human body, as shown in Fig.~\ref{fig:kmeans_grid}. Therefore, we use k-means instead for a more uniform split. As illustrated in Table~\ref{tab:kmeans_grid}, canonical k-means performs better than canonical grid voxelization. 


\section{Additional Visualization Examples}
\vspace{-2mm}
We provide more visualization examples in this section. 

\subsection{Ablation of FDI}
\input{./Figures/Ablation_FDI}
\vspace{-2mm}
To better illustrate the functional difference between human representation $\textbf{h}$ and appearance feature $\textbf{a}$ in FDI, we provide the ablation in Fig.~\ref{fig_abl_fdi}.  Obviously, the human representation $\textbf{h}$ contains geometry constraints from human priors with coarse color information, while $\textbf{a}$ shows more vivid colors with poor geometry. Hence, we propose to take the coarse human representation as the guidance for integrating proper fine-grained details from the appearance feature. 


\subsection{Comparisons with State-of-the-art}
\input{./Figures/Appendix_VIS_SOTA}
\vspace{-2mm}
We provide more comparison examples with previous state-of-the-art methods in Fig.~\ref{fig_appendix_vis_sota}. 

\section{Human Split}
\input{./Tables/Human_Split}
\vspace{-2mm}
 We list the detailed human split information in Table~\ref{tab:human_split}. We hope that it can serve as a standard split for the following researchers. The code will also be available upon acceptance. 