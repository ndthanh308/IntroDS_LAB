%&latex
\section{Experimental Results}
\input{./Tables/ZJU_MoCap_SOTA_V1}

% \input{./Tables/ZJU_MoCap_SOTA}
% \input{./Tables/H36M_SOTA}

\subsection{Experimental Settings}
\vspace{-2mm}
\noindent \textbf{Datasets.} We benchmark on ZJU-MoCap~\cite{peng2021neural} and H36M~\cite{ionescu2013human36m} for verifying the effectiveness of our TransHuman. 
% (i)\textit{ ZJU-MoCap.}

(i) ZJU-MoCap~\cite{peng2021neural} provides multi-view videos of $10$ human subjects with $23$ synchronized  cameras, together with the pre-fitted SMPL parameters and human masks. Each video spans between $1000$ to $2000$ frames and contains complicated motions like ``Taichi" and ``Twirl".  
Following~\cite{kwon2021nhp,chen2022gpnerf}, $10$ subjects are split into $7$ source subjects (ZJU-$7$) and $3$ target subjects (ZJU-$3$), and each subject is further divided into training and testing parts.
% and each subject is further divided into training and testing parts, \ie, ZJU-$7$-Train/Test, ZJU-$3$-Train/Test. 
% The detailed configuration of each setting is listed in Table ~\ref{tab:ZJU_SOTA}.
% The training is performed on ZJU-$7$-Train, and the evaluation can be further divided into $3$ settings based on the used test set:  \textbf{pose genralization} (ZJU-$7$-Test),  \textbf{identity generalization} (ZJU-$3$-Test), and \textbf{fitting} (ZJU-$7$-Train).
We strictly follow the officially released human split from \cite{kwon2021nhp} for training and testing. We refer the detailed split information to the appendix. To prove that our method can welly handle the incomplete painted SMPL, we additionally report the performance of the one-shot generalization setting, \ie, only 1 reference view is provided during inference. 
% The training is performed on ZJU-$7$-Train, and the evaluation can be further divided into $3$ settings based on the used test set:  \textbf{pose genralization} (ZJU-$7$-Test),  \textbf{identity generalization} (ZJU-$3$-Test), and \textbf{fitting} (ZJU-$7$-Train).
% We refer the detailed split information to the appendix. 
%TODO: Introduce pose generalization setting and identity generalization here.

(ii) H36M~\cite{ionescu2013human36m} records multi-view videos with 4 cameras and includes multiple subjects with complex motions. We use the preprocessed one by~\cite{peng2021animatable} which contains representative subjects S1, S5, S6, S7, S8, S9, S11, and their corresponding SMPL parameters and human masks. We verify the cross-dataset generalization ability with H36M, \ie, trained on ZJU-MoCap and then directly inference on H36M. The first 3 views are taken as the reference views, and the last one is used as the target view. 





% \noindent \textbf{Settings.}
% Following~\cite{kwon2021nhp,chen2022gpnerf}, $10$ subjects from ZJU-MoCap are split into $7$ source subjects and $3$ target subjects and each subject is further divided into train and test parts, \ie, ZJU-$7$-Train/Test, ZJU-$3$-Train/Test. The training is performed on ZJU-$7$-Train, and the evaluation can be further divided into $3$ settings based on the used test set:  \textbf{pose genralization} (ZJU-$7$-Test),  \textbf{identity generalization} (ZJU-$3$-Test), and  \textbf{fitting} (ZJU-$7$-Train).
% We further verify the \textbf{cross-dataset generalization} ability with H36M, \ie, trained on  ZJU-7-Train and then directly inference on H36M. The first 3 views are taken as the reference views, and the last one is used as the target view. 


% Following previous methods~\cite{kwon2021nhp,chen2022gpnerf}, ZJU-MoCap is split into $7$ source subjects (ZJU-$7$) and $3$ target subjects (ZJU-$3$). The training is performed on certain frames of source subjects (ZJU-$7$-Train), and the evaluation can be further divided into $3$ settings based on the different test set:
% (i) Pose generalization: The evaluation is performed on 
% (ii) Unseen poses of unseen subjects (identity generalization): 
% (iii) Seen poses of seen subjects (Overfiting): 
% (iv) Cross-dataset generalization: 

\noindent \textbf{Evaluation Metrics.}
For novel view synthesis, we report the commonly used Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM)~\cite{PSNR_wang2004image}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable} as the evaluation metrics. For 3D reconstruction, following~\cite{kwon2021nhp,chen2022gpnerf}, we only report the qualitative results since ground truth meshes are unavailable.  



\subsection{Implementation Details}
\vspace{-2mm}
In line with ~\cite{kwon2021nhp},  we take the ResNet-18~\cite{he2016Resnet} (only the first $3$ layers are used) as the CNN for extracting the deep features from reference images and set the multi-view number $N_v = 3$. The number of clusters (tokens) in human body grouping is set as $N_t = 300$, and the light-weight ViT-Tiny~\cite{dosovitskiy2020ViT} is employed as the transformer module. Each query point is assigned with $N_k = 7$ DPaRFs. Following ~\cite{kwon2021nhp,chen2022gpnerf}, we train on ZJU-MoCap with $512\times512$ 
resolutions, and for each ray we sample $64$ points by default during both the training and inference stages. 


 \input{./Figures/ZJU_SOTA}
\input{./Figures/3D_rec}


\subsection{Comparisons with State-of-the-art}
\vspace{-2mm}
\noindent \textbf{Baselines.}
Following ~\cite{kwon2021nhp,chen2022gpnerf}, we compare with both per-subject optimization methods~\cite{peng2021neural,neural_textures,wu2020NHR,lombardi2019neuralvolume} and generalizable methods~\cite{raj2021pva,yu2021pixelnerf,keypointNeRF,kwon2021nhp,chen2022gpnerf}. For per-subject optimization methods, an individual model is trained on the training part of each subject. Notably, previous state-of-the-art methods for generalizable neural human rendering~\cite{kwon2021nhp,chen2022gpnerf} actually use different human splits in their officially released code and are not in line with the one used in their papers (performance is not reproducible). Hence, for fair comparisons, we  \textbf{unify them under the released human split of NHP~\cite{kwon2021nhp}}. Specifically, we report the performance of NHP~\cite{kwon2021nhp} using the official checkpoint, and re-run the official code of GP-NeRF~\cite{chen2022gpnerf} under the unified human split. Note that, GP-NeRF has employed an overfitting trick which we think is unreasonable, \ie, they overfit the test reference views  instead of randomly sampling during the training stage. This trick leaks the test information to the training stage, therefore we remove it in our re-running. We also provide the comparisons under the released human split of GP-NeRF with the overfitting trick, where our method outperforms it consistently by large margins. 

\noindent \textbf{Novel View Synthesis.}
We compare the quantitative results with previous state-of-the-art methods in Table~\ref{tab:ZJU_SOTA}. Obviously, we outperform them by significant margins under all the settings. 
% Compared with the per-subject trained methods, our method outperforms NB~\cite{peng2021neural} by $3.46$ in PSNR under the pose generalization setting. 
Notably, for the identity generalization setting, the per-subject methods are directly trained on the target subjects while our method is only trained on the source subjects, 
% which is a significantly harder task, 
yet we still outperform them by large margins, \ie, $+3.27$ in PSNR. Compared with the recent SPC-based generalizable methods~\cite{kwon2021nhp,chen2022gpnerf}, our method also shows healthy margins, \ie, $+2.20$ PSNR and $-45\%$ LPIPS compared with the second-best under the pose generalization setting. 
For the more challenging cross-dataset generalization setting, we also outperform the baseline methods remarkably albeit these two datasets~\cite{peng2021neural,ionescu2013human36m} have significantly different distributions, which proves the superior generalization ability of our TransHuman. 


 The qualitative comparisons are illustrated in Fig. \ref{fig_zju_sota}, where our TransHuman gives significantly better details and body geometry. 
 We attribute this to the careful design of our framework, \ie, the global human representation brings more complete body geometry, the canonical learning scheme gives better generalization ability, and FDI further includes more fine-grained details like textures and lighting.
 
% \noindent \textbf{Qualitative Analysis of Novel View Synthesis.
% \noindent \textbf{Unseen Poses of Seen Subjects.}
% \noindent \textbf{Unseen Poses of Unseen Subjects.}
% \noindent \textbf{Seen Poses of Seen Subjects.}
% \noindent \textbf{Cross-dataset Generalization.}


\noindent \textbf{3D Reconstruction.}
The 3D reconstruction results are illustrated in Fig.~\ref{fig_3d}. Compared with NHP~\cite{kwon2021nhp} that uses the SPC-based human representation, our method achieves a more complete and fine-grained geometry with details like wrinkles. 

\subsection{Ablation Studies}
\vspace{-2mm}
Following~\cite{kwon2021nhp}, we perform ablation studies under the identity generalization setting. Due to the limited space, we refer more detailed ablation studies to the appendix. 

\noindent \textbf{Ablation of TransHE.}
We first study the effectiveness of canonical body grouping and canonical learning scheme in Table~\ref{tab:abl_transhe}. When performing the body grouping under the observation space with grid voxelization (``obs. body grouping"), the performance suffers a significant drop from $26.15$ to $25.28$ in PSNR. As introduced in \S~\ref{Sec_TransHE}, performing grouping under the observation space leads to the semantic ambiguity issue, therefore leading to worse performance. Then,  ``obs. PE" changes the position embedding of input tokens from the canonical positions $\hat{V}^{c}$ to observation positions $\hat{V}^{o}$, and also observes a significant decrease, \eg, $-0.35$ in PSNR. The canonical learning scheme eases the optimization and removes the pose misalignment between training and inference stages, therefore leading to better performance. 

\noindent \textbf{Ablation of DPaRF.}
We verify the effectiveness of DPaRF in Table~\ref{tab:abl_dparf}. ``w/o coordinate" represents removing the coordinate part from the human representation.
% , \ie, removing $\gamma_2(\overline{\mathbf{p}}_i)$ from Eq.~\ref{eq_humanrepresentation}. 
As expected, the performance drops by significant margins ($-0.35$ in PSNR). Coordinates contain the accurate position information of query point in each DPaRF, therefore is important. ``absolute coordinate'' indicates using the absolute coordinate of query point, \ie, $\textbf{p}$ instead of $\overline{\textbf{p}}$ in Eq.~\ref{eq_humanrepresentation}, and the performance does not show significant improvement compared with ``w/o coordinate". This further proves the importance of using the coordinate under the deformed coordinate systems. Finally, ``w/o k-nearest fields" shows that the k-nearest fields aggregation design can bring certain improvement on all the metrics. 


\input{./Tables/Ablation_TransHE}
\input{./Tables/Ablation_DPaRF}


% \input{./Figures/Ablation_FDI}



\noindent  \textbf{Ablation of FDI.}
% As to the verification of FDI, 
We first perform the ablation of FDI by individually removing the appearance feature part (``w/o $\textbf{a}$") or the human representation part (``w/o $\textbf{h}$"). As illustrated in Table~\ref{tab:abl_fdi}, merely using either of them gives an unsatisfactory performance. 
% We also provide the visualization of the ablation in Fig. \ref{fig_abl_fdi}. Obviously, $\textbf{h}$ is better at geometry with coarse color information while $\textbf{a}$ shows more vivid colors (albeit poor geometry). 
% This proves the importance of the FDI module which adaptively integrates the useful fine-grained details from  $\textbf{a}$ using $\textbf{h}$ as the guidance. 
Then, ``w/o RGB" shows that  the raw RGB features can further bring a measure of improvement. 

\input{./Tables/Ablation_FDI}
\input{./Tables/Ablation_SPC}


\noindent \textbf{Comparisons with SPC-based representation.}
To further verify the effectiveness of our proposed transformer-based human representation, we directly replace the TransHE and DPaRF modules with SPC and trilinear sampling in our code. We follow \cite{kwon2021nhp} to configure the SPC including the architecture and input resolution. As shown by Table \ref{tab:abl_spc}, our proposed transformer-based representation outperforms the SPC-based one by significant margins among all the metrics under a fair comparison setting. 



% \vspace{-1mm}
\subsection{Efficiency Analysis}
\vspace{-1mm}

\input{./Tables/Efficiency}

We compare the efficiency of our method with previous state-of-the-art methods in Table~\ref{tab:efficiency} under the identity generalization setting (438 frames). For a fair comparison with the previously fastest method GP-NeRF~\cite{chen2022gpnerf} under the same inference time, we provide a fast version of our method by reducing the sampling points per ray from $64$ to $16$ during inference (``Ours-16pts"). Obviously, with the same inference time, our method still outperforms GP-NeRF by $0.84$ in PSNR albeit using merely $64\%$ parameters, $55\%$ inference memory, and $71\%$ training memory, and the performance can be further significantly improved with acceptable additional cost. This proves that our TransHuman is both effective and efficient.  
\vspace{-1mm}
