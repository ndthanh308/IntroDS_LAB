\section{Think Before Speak prompting method}
We propose a three-step agent generation benchmark called ``Think Before Speak'' (TBS), which aims to guide Large Language Models (LLMs) in comprehending complex and lengthy contexts of interactions more accurately. Unlike simple template-based prompting approaches~\cite{llm_survey,zhao2023survey,huang2022towards}, our method takes into consideration the specific properties of Tabletop Role-Playing Games (TRPGs) and incorporates the principles of Chain of Thought (CoT)~\cite{wei2022chain,zero-shot-cot} in its prompting design.
In the generated check item, the answers consist of character names and corresponding skill names. However, directly expecting the models to produce accurate character and skill names is a challenging task. Using a single-step template prompting approach may result in LLMs generating characters that do not exist in the given contexts, characters with no relevant actions, mismatches between characters and their associated skills, or skills that are not defined within the game rules.
To address these challenges, our method guides LLMs through a three-step process. Firstly, the models are prompted to identify the characters present in the current game scenarios. Then, they are encouraged to consider the intentions of the characters and list those who are likely to take action or are engaged in ongoing movements. Finally, we provide the models with a comprehensive set of possible skills derived from the game rules, allowing them to select the most appropriate character-skill combinations that the GM may ask the players to check. This gradual guidance facilitates more accurate and context-aware responses from the LLMs.

%We introduce a three-step virtual GM generation benchmark named think before speak (TBS), which can gradually guide LLMs to more accurately understand the complex and long contexts. 
%Rather than straightforward template prompting~\cite{llm_survey,zero-shot-cot}, ours consider the properties of TRPG games and leverage CoT in prompting design. In the generated check item, the answers of skill check contains two components, which are the characters' names and corresponding skill names. Directly requiring the models to produce correct names and skills is challenging. With the single-step template prompting, LLMs may produce characters that not existed in the contexts, produce characters without any movements, produce mismatch between characters and corresponding skills, or produce non-existed skills in the rules. In our work, we guide LLMs first to figure out which characters are in current game scenarios, and then think about the intentions and list characters that may act next or perform some ongoing movements. Finally, we given all the possible skills from the rules and let the models to select most possible characters and corresponding skills that DM may ask the players to check. 

Specifically, in the first step of our prompting approach, we guide the language models by providing a prompt such as ``Based on the TRPG game record provided above, identify the characters or NPCs that exist in the current scenarios.'' This prompts the language model to recognize and understand the characters present in the given contexts.
In the second step, we prompt the language models with a question like ``Which character or NPC is expected to carry out activities next?'' This encourages the models to delve deeper into the semantics of the contexts and infer the intentions of the characters.
For the final step, we provide LLMs with all possible skills defined in the TRPG rules and guide them to generate character names that correspond to the potential skill checks. Our prompts for this step include phrases such as ``What skills are required for the mentioned characters to carry out their respective activities?''
Furthermore, to facilitate comparison with other benchmarks, we extend the TBS approach to also generate utterances to simulate a real-human GM. Given the predictions from the TBS model, LLMs are required to generate responses in the tone and style of a GM. We achieve this by using prompts such as  ``As a game master for a TRPG game, generate responses based on the provided character names and the corresponding skills.''


%Specifically, after given LLMs the contexts of the game records, at the first-step prompting, we use prompt like `Based on the above TRPG game record, which characters or NPC existed in current scenarios.' to lead the language model understanding existed characters in the scenarios. Then, we go further with the characters' intentions as `Which character or NPC will carry out activities next?'. This urge the language model further mine the semantics in contexts and infer intentions of characters. Finally, we provide all possible skills in rules of TRPG and guide the language model to produce character names corresponding with the possible skill checks by given prompts like `What kind of skills are needed for the mentioned characters to carry out corresponding activities?' Besides, to compare with other benchmarks, we further extend TBS to generate utterance of virtual GM. Given the predictions of TBS, we further require LLMs to produce the utterance in the tone of the GM, by prompts like `As a game master for a TRPG game, generate responses according the given characters names and the corresponding skill.' 

%As in TRPG games, most predictable responses of GM are about the check items for characters and the corresponding descriptions. Though GMs are also responsible to describe the background story, scenarios, responses from other NPCs, etc., these are hard to be inferred by the given contexts. Our benchmark excludes these unpredictable responses by given contexts and mainly following~\cite{zhu2023fireball}. 

%\textbf{Generation of GM utterance:}
%This part aims to generate vivid GM utterance that similar to real-human in TRPG games. According the check items and corresponding characters, we require the language models to produce descriptions to the players, For example, the prompt can be written as `Bill needs to check perception. Hill needs to check intelligence. Following above game records, write guidance for the above players to engage the game like a game master in DND rule.'. Toward more vivid responses, we also encourage the LLMs to generate response with second person pronouns. More detailed prompts will be presented in the supplementary. 














%According the dataset, we further propose skill check generation task as a novel testbed for LLMs researches, e.g., prompting learning. We also provide an example from MITD in Tab~\ref{tab.data_eg1}. The contexts are inputs for LLMs and we require the models to infer what kind of skill check is required for the corresponding characters in the next turn. As in the example, there are two challenges should be overcomes. 1. Understanding behaviours and inferring intentions of multiple characters. In Tab.~\ref{tab.data_eg1}, there are four characters in current game scenario, which are the brown bear, Bill, Elvis Zem, and Maurice. Though all the characters have movements and interact to others, only one player need to operate a skill check. Some characters are not involved in the fighting. Some others' movement have already operated and described by DM. For example, spell of Zem is performed in Turn4 and its effect has already described by DM in Turn10. Thus, the spell is a movement that already finished and should not be checked again in the next turn. 2. Understanding the game rules and matching to the characters' movements. In tab.~\ref{tab.data_eg1}, Maurice wants to escape from the attack of the bear. However, there is no an escape operation in skill check of DND rules. In this case, the bear use its strength to grip Maurice in the game and Maurice also need to check strength to make a contest with the bear. To answer this skill check, methods need to understand the intention and movements of characters. Then, according to the rules of the games, inferring corresponding check items for current turn like a real-human DM. 

%In details, our dataset possess three properties that completely different to the previous works. 

%1. MITD provides complex interactions with multiple characters. Due to the settings and rules in TRPG, different players need to act different characters. This induce the players need to understand various personalities and backgrounds of their own character and other characters (teammates and NPCs), understand the response and intention of other characters, and provide feedback to others. Then, DM will guide the players to make skill checks according the interactions. All these lead the game logs in our work possess diverse and complex interactions between multiple characters. 

%2. The interaction in MITD are with grounded language. Rather than play-by-post form, records used in MITD are from the instant chat log in games and recompiled by DM after game finished. This leads to two advantages that the previous play-by-post form can not provide. 1). the game records contains the grounded language interaction which are used in daily communication. All interactions are source from the immediate feedback of players. 2). The recompilation of DM can reduce irrelevant contents in records, e.g., out-of-character performances or thinking beyond the game characters. All these properties leads our works go further with the grounded language interactions and more focus on the understanding of complex semantics. 

%3. MITD is the fist Chinese dataset for TRPG logs. This provide a new testbed for the research in grounded language more than only with English. Besides, following the same rules in TRPG, players with different cultures should have the same skill check in the games. This indicates that even the context in MITD are translated to other languages, the answers for skill check would not be changed. Thus, MITD also can become a useful testbed for aligning multiple language in LLM studies. 