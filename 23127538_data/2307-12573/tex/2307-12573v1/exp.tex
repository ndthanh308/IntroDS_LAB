\section{Experimental Results}
In this section, we provide a detailed discussion, comprehensive evaluation, and analysis of our benchmark.

\textbf{Baseline Methods:} As our baseline, we employ LLMs with template prompting, which have been utilized in previous studies. We specifically adapt two popular LLMs, which are GPT-3.5 and GPT-4~\cite{brown2020language}. By incorporating different language models, we can thoroughly assess the performance of our prompting benchmark. Furthermore, recent researches~\cite{wei2022chain,zero-shot-cot,cot_self_consistency} have demonstrated the efficacy of Chain-of-Thought (CoT) methods in improving understanding capabilities. To compare with this approach, we include the zero-shot CoT (zcot) method~\cite{zero-shot-cot} in our evaluation.

Additionally, to demonstrate the ability to infer check items, we introduce a statistical predictor for check items. Given the predicted characters, we select the skills with the highest probability based on the statistical distribution observed in our dataset. This statistical predictor serves as a lower bound for generating check items and also reveals the impact of any biases present in our dataset.

%\textbf{Baselines: } We consider LLMs with simple template prompting as our baseline, which has been introduced and presented in previous methods. We adapt three popular LLMs, which are GPT-3.5, GPT-4, LLAMA, respectively. Different language models can comprehensively reflect the performances of our prompting benchmark. Meanwhile, many recent works show the effectiveness of Chain-of-Thought (CoT) methods in improving understanding ability. We also adapt zero-shot CoT (zcot) method~\cite{zero-shot-cot} in comparison. Moreover, to show the ability of inferring check items, we further introduce a statistical predictor for check items. Given predicted characters, we select the skills with the highest probability based on the statistical distribution of the dataset. This statistical predictor reflect the lowest boundary of generating check items and also reveal the influence of bias in our dataset.

\textbf{Evaluations:} To evaluate the effects of MOE and TBS frameworks on interaction understanding, we introduce the concept of a virtual Game Master (GM) in TRPGs. The virtual GM serves as a simulation of a real-human GM, possessing the ability to comprehend interactions, infer intentions, interact with players, and provide guidance for their actions. This role fulfills the criteria of our requirements for the agents that enable to understand complex interactions. By incorporating the virtual GM, we create a platform to assess the agents' understanding of complex interactions and their ability to navigate diverse scenarios.
%In order to assess the impact of MOE and TBS on the understanding of interaction, we propose to generate a virtual GM participate in the games. The virtual GM needs to simulate the response of real-human GM, who comprehend interactions, estimate intentions, interact with players and guide them to act. This role is just satisfy the requirements of our world model for the agents, which can indicate the agents whether they can successfully cooperate with others or interact with open objects. 
In detail, we generate GM utterances using both ground truth information from C2A and predictions from TBS. The generation process follows the methodology outlined in~\cite{zhu2023fireball,llm_survey}, which leverages LLMs, template prompts, and additional inputs for characters and skills.

Rather than relying on metrics based on captioning in previous works~\cite{gandalf,zhu2023fireball}, we employ subjective evaluation conducted by real-human players. Given the diversity of descriptions in grounded language, there is no definitive ground truth for evaluating the responses of GMs. Subjective evaluation provides more valuable insights into the degree of realism in the generated utterances. Following~\cite{gandalf,sub_eval1,sub_eval2,sub_eval3,liang2022seeg}, we invite volunteers to score the responses based on three factors: naturalness, groundedness, and factual correctness. Naturalness assesses the extent to which the generated responses resemble human-like language. Groundedness measures the degree to which the responses effectively employ grounded language similar to everyday communication. Lastly, factual correctness evaluates whether there are any factual errors or inconsistencies with the given contexts.


%\textbf{Virtual GM Evaluation: } To evaluate the effect of CSA and TBS to virtual GM generation, we further generate GM's utterance by given the ground truth in CSA and predictions from TBS. The generation method following~\cite{zhu2023fireball,llm_survey}, which utilize LLMs, template prompts, and additional inputs for characters and skills in ours. Moreover, rather than compare some metrics based on captioning, we adapt subjective evaluation by real-human players. Due to the diversity of descriptions in grounded language, there is no proper ground truth for the responses of GMs. The subjective evaluation can provide more valuable results for measuring the vivid degree of utterances. Following~\cite{gandalf,sub_eval1,sub_eval2,sub_eval3,liang2022seeg}, we ask the invited volunteers to score the responses in three factors, which are naturalness, groundness and factually correctness. The naturalness reflects how much the produced responses are similar to real-human. Groundness reflects what degree the generated responses flexibly use grounded language similar to the daily communication. Finally the factually correctness reflects whether there are some factual errors or inconsistency with the contexts. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
	\begin{center}
	\resizebox{0.88\columnwidth}{!}
            {
\begin{tabular}{|l|lllc|}
\hline
\multicolumn{1}{|c|}{\multirow{3}{*}{Prompting Method}} & \multicolumn{4}{c|}{LLMs}                                                                             \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                                  & \multicolumn{2}{c|}{GPT-3.5}                      & \multicolumn{2}{c|}{GPT-4}                        \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                                  & \multicolumn{1}{c|}{CF} & \multicolumn{1}{c|}{SF} & \multicolumn{1}{c|}{CF} & \multicolumn{1}{c|}{SF} \\ \hline \hline 
template prompt                                         & \multicolumn{1}{c|}{42.02}   & \multicolumn{1}{c|}{15.30}   & \multicolumn{1}{c|}{43.21}  &  15.93           \\ \hline
template prompt + zcot                                  & \multicolumn{1}{c|}{39.28}   & \multicolumn{1}{c|}{14.46}   & \multicolumn{1}{c|}{42.45}  &  16.25           \\ \hline \hline 
char prompt + skill prompt                              & \multicolumn{1}{c|}{50.43}   & \multicolumn{1}{c|}{14.78}   & \multicolumn{1}{c|}{53.55}  &  16.79           \\ \hline
pre-char prompt + char prompt + statistic predictor     & \multicolumn{1}{c|}{53.32}   & \multicolumn{1}{c|}{5.03}    & \multicolumn{1}{c|}{57.94}  &  5.03            \\ \hline
pre-char prompt + char prompt + skill prompt + zcot     & \multicolumn{1}{c|}{50.50}   & \multicolumn{1}{c|}{12.88}   & \multicolumn{1}{c|}{53.45}  &  17.39           \\ \hline
pre-char prompt + char prompt + skill prompt            & \multicolumn{1}{c|}{53.32}   & \multicolumn{1}{c|}{15.91}   & \multicolumn{1}{c|}{57.94}  &  20.02           \\ \hline
\end{tabular}
             }
        \end{center}
        \caption{Comparison of different prompting methods and LLMs. Results prove that our task is solvable but requires higher understanding ability for grounded and complex semantics. }
        \label{tab:p_r_metric}
\end{table}


\subsection{Objective Evaluation}

\textbf{Comparison of Prompting Methods:} We conduct a comparison between our proposed method and different prompting approaches. The results, as shown in Tab.~\ref{tab:p_r_metric}, reveal the effectiveness of our step-wise prompting approach compared to baselines such as zero-shot CoT and the statistical predictor.
The experimental results demonstrate that each step in our prompting process contributes significantly, leading to improved F-score for both characters and skills. This highlights the enhanced understanding capability of LLMs in comprehending the given contexts. Furthermore, due to the distribution bias present in our dataset, the statistical predictor proves to be useful, albeit with considerably lower performance compared to our proposed method and other prompting methods. This reveal the lower performance boundary in predicting skill labels.

Furthermore, in line with previous studies~\cite{zero-shot-cot,dong2022survey,wei2022chain}, the incorporation of zero-shot CoT has demonstrated improvements in the performance of LLMs across various tasks. However, when applied to the MOE task, the observed enhancements are not as substantial. Since MOE involves more grounded semantics and complex interactions, it presents a challenging scenario for existing prompting methods and remains an unsolved problem that requires further investigation.


%\textbf{Comparison of different prompting method: } We first compare our method with different prompting methods. As shown in Tab~\ref{tab:p_r_metric}, we experiment prompting methods with different steps and compare with baselines with zero-shot CoT and statistical predictor. The experimental results show that all step in our prompting is valuable, improve the precision and recalls in both characters and skills, and boost the understanding ability for LLMs to current contexts. Besides, since the distribution bias of our LGL, statistical predictor is also useful but its performances are far lower than ours and other prompting methods. This reveal a lower boundary of the performances of predicting skill labels. Finally, as presented in previous works~\cite{zero-shot-cot}, zero-shot CoT can improve the understanding ability of LLMs and lead the models to think gradually. This is also useful in our benchmark and introduce additional improvements in results. 

%\textbf{Comparison of different languages: }

\textbf{Comparison of different language models: } We further investigate the impact of different LLMs on the performance of our prompting methods. With advancements in LLM, the overall understanding and reasoning capabilities have significantly improved. As depicted in Tab.~\ref{tab:p_r_metric}, employing more advanced language models leads to higher performance in MOE task. In addition to the effectiveness of the prompting methods, the enhancements in LLMs themselves are also beneficial in comprehending the intricacies of complex and grounded interactions. The experimental results reveal that our task is solvable, yet there remains ample room for further exploration and improvement.

%\textbf{Comparison of different language models: } We also experiment the prompting methods with different LLMs. Since the developments from LLMs, the understanding and reasoning ability have been boosted. As in Tab.~\ref{tab:p_r_metric}, using better language model achieve higher performances in CSA. More than prompting methods, the improvements of LLMs are also useful in understanding the complex and grounded semantics. The experimental results reveal that our task is solvable but still have a large space to explore. 




\subsection{Subjective Evaluation}
We conducted a subjective evaluation by recruiting real-human players of TRPG as volunteers and collecting their responses through questionnaires. The average scores in different factors, which are naturalness, groundedness, and factual correctness, were computed following established guidelines~\cite{gandalf,sub_eval1,sub_eval2,sub_eval3,liang2022seeg}. The statistical results are presented in Fig.~\ref{fig:subjective}. Notably, methods that take into account the predictions or ground truth of MOE demonstrate higher performance across all evaluation factors. Generally, methods utilizing MOE labels outperform those using predicted labels. Moreover, when considering MOE predictions, the methods achieve superior performance in generating virtual GM responses. This observation confirms that a higher understanding ability for complex semantics leads to more vivid and human-like responses from the agents. Additionally, it underscores the strong correlation between MOE performance and virtual GM performance, highlighting the importance of MOE in the pursuit of improved agent generation.

%We conducted a subjective evaluation by enlisting real-human players of TRPG as volunteers and collecting their responses through questionnaires. The average scores in different factors, which are naturalness, groundedness, and factual correctness, were computed following established guidelines~\cite{gandalf,sub_eval1,sub_eval2,sub_eval3,liang2022seeg}. The statistical results are presented in Figure~\ref{fig:subjective}. To be noticed, methods considering the predictions or the ground truth of CSA show higher performances in all evaluating factors. Generally, methods with CSA labels outperform methods with predicted labels. Meanwhile, considering CSA predictions, methods also achieve higher performances in generating virtual GM responses. This prove that with higher understanding abbility for complex semantics, the agents can be more vivid and close to the real human. Meanwhile, this also reveal the high relevance of our CSA performances and virtual GM's performance. This reflect the significance of our CSA in pursuing better agent generation. 

Besides, our prompting method demonstrates superior performance in all evaluated factors. Specifically, our method exhibits significant improvements in factual correctness compared to the baseline methods. Furthermore, in terms of groundedness and naturalness, our method showcases comparable or even better performance than other methods. These results indicate that our method achieves enhanced understanding ability and is capable of generating improved utterances as GM descriptions. However, there is still ample room for improvement in terms of groundness and naturalness. The generated utterances may occasionally be overly verbose and lack the same level of vividness as those produced by real humans. This performance gap motivates further exploration of more effective methods for constructing advanced AI agents.


%\subsection{Subjective Evaluation}
%We invite real-human players of TRPG as volunteers and collect the questionnaires from them. We compute the average score in different factors, which are naturalness, groundness, and factually correctness, following~\cite{gandalf,sub_eval1,sub_eval2,sub_eval3,liang2022seeg}. The statistical results are presented as in Fig.~\ref{fig:subjective}. Our method show better performances in all factors. Specifically in factually correctness, our method show significant improvements than baseline methods. Meanwhile, for groundness and naturalness, our method also show comparable or better performances than other methods. The results indicate that our method achieve better understanding ability and also enable to produce better utterances as descriptions for GM. However, there are still large space to improve in groundness and naturalness, the generated utterance may tend to be wordy and still not as vivid as real-human. This performance gap also further encourages us to explore more effective methods to construct better AI agents. 



% Figure environment removed


