\documentclass{article}
% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_data_2023

% ready for submission

% to compile a preprint version, add the [preprint] option, e.g.:
\usepackage[preprint]{neurips_data_2023}
% This will indicate that the work is currently under review.

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_data_2023}

% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_data_2023}

% Submissions to the datasets and benchmarks are typically non anonymous,
% but anonymous submissions are allowed. If you feel that you must submit 
% anonymously, you can compile an anonymous version by adding the [anonymous] 
% option, e.g.:
%\usepackage[anonymous]{neurips_data_2023}
% This will hide all author names.

\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
%\usepackage{ctex}

\usepackage{times}
\usepackage{graphicx}
% for Chinese
\usepackage{CJKutf8}
% for table
\usepackage{tabularx}
\usepackage{float}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{longtable}
\usepackage{supertabular}
\usepackage{arydshln} 
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{amsmath}
%\usepackage{natbib}



\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{bbding}

%multi-row
\usepackage{multirow}


\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}


%\title{Boosting Agent Interactions by Introducing World Model: }
%\title{Towards Open WorldAgent Interaction: A Benchmark for LLM to Understand Multi-character and Novel Object Interaction}
%\title{Understanding Agent Interactions by Introducing World Model:  A Benchmark for Multi-character and Novel Object Interaction}
%\title{MOE: Understand Multi-character and Novel Object Interactions for LLMs}
%\title{xxx LLM: A xxx Benchmark for Understanding Multi-character and Novel Object Interactions}

%\title{Towards Multi-Character and Novel Object Interactions: A Benchmark for Virtual Game Master Integration for Agent World Models}
%\title{Towards Multi-Character and Novel Object Interactions: A Benchmark for Virtual Game Master}
\title{Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models}

%% Figure removed
%Enhancing Agent Understanding of Complex Interactions: A Benchmark for Intention Estimation

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


 
\author{\small Yuanzhi Liang$~\textsuperscript{\rm 1}$, Linchao Zhu$~\textsuperscript{\rm 2}$, Yi Yang$~\textsuperscript{\rm 2}$    \\ 
        \small{$~\textsuperscript{\rm 1}$ University of Technology Sydney}, 
	\small{$~\textsuperscript{\rm 2}$ Zhejiang University}
	\\{\tt\small {yuanzhi.Liang}@student.uts.edu.au} \vspace{-0.15cm}
        \\{\tt\small {zhulinchao7}@gmail.com} \vspace{-0.15cm}
        \\{\tt\small {yangyics}@zju.edu.cn}	
	%	\thanks{Corresponding Authors.}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds. However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects. Pre-defining all interactable objects in the agent's world model presents challenges, and conveying implicit intentions to multiple characters through complex interactions remains difficult. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.
%Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to engage in human-like interactions within virtual world. However, these interactions still suffer from limitations in complexity and flexibility, especially in scenarios involving multiple characters and novel objects. It is challenging to pre-define all interactable objects in the agent world model. Additionally, conveying implicit intentions to multiple characters through complex interactions remains a difficulty. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model to handle complex interactions, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate this exploration, we introduce a benchmark comprising the Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. The dataset captures log data from real-time communications during gameplay, providing diverse, grounded and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.
\end{abstract}

%Recent research has made strides in developing AI agents capable of human-like interactions using natural language and Large Language Models (LLMs). Agents can live in a virtual world, achieve feedback from the world model, and produce interactions with the environment and other agents. However, these interactions still suffer from limitations in complexity and flexibility, especially when it comes to more intricate interactions close to the real-human that may involve multiple characters and novel objects. Specifically, it is hard to pre-define all possible and interactable objects in the environment. Meanwhile, it is also hard to convey implicit intentions (e.g., persuasion, negotiation) to multiple characters by complex interactions similar to the real-time communications. To address this gap in designation of world models for agents, inspired by rules and settings in TRPG, we proposed to construct virtual GMs into the world model to handle complex interactions. As in the games, GM can oversee all information, estimate intentions of players, provide descriptions for environments, and provide feedback for agents after they operate actions. Integrating a virtual GM into agent world model may reduce limitation in complex interactions of agents. However, due to the absence of a proper benchmark with long and intricate contexts within multi-character and novel object interactions, previous methods are hard to construct a virtual GM that understands intentions and provide correct feedback according the complex interactions. In this paper, we propose a new benchmark consisting of a Multiple character and novel Object based interaction Estimation (MOE) task and a dataset to support the task. MOE challenges models to understand characters' intentions and accurately determine the next character's actions within long and intricate contexts within multi-character and novel object interactions. To support MOE task, we collect a dataset, which contains intricate contexts extracted from game logs obtained during real-time communications. This dataset serves as a valuable resource for MOE task and enables in-depth exploration and simulation of human behavior and interactions. Additionally, we present a simple prompting baseline and evaluate its performance alongside other prompting methods employing various LLMs. Through objective and subjective evaluations, we demonstrate the effectiveness of our baseline and underscore the significance of our dataset and task in advancing interaction understanding.
%We hope that our dataset and task will inspire the research community to deepen exploration of complex interactions with natural language and foster the development of enhanced AI agents.

%we propose a new benchmark consisting of a Multiple character and novel Object based interaction Estimation (MOE) task and a dataset for the task. MOE task challenges models to accurately determine the next character's actions and identify corresponding actions within long and intricate contexts within multi-character and novel object interactions. To support MOE task, we collect a dataset, which contains intricate contexts extracted from game logs obtained during real-time communications. This dataset serves as a valuable resource for MOE task and enables in-depth exploration and simulation of human behavior and interactions. Additionally, we present a simple prompting baseline and evaluate its performance alongside other prompting methods employing various LLMs. Through objective and subjective evaluations, we demonstrate the effectiveness of our baseline and underscore the significance of our dataset and task in advancing interaction understanding.
%We hope that our dataset and task will inspire the research community to deepen exploration of complex interactions with natural language and foster the development of enhanced AI agents.


%In the pursuit of developing AI agents that can engage in human-like interactions, recent research has explored the use of natural language and Large Language Models (LLMs). However, these interactions still face limitations in terms of complexity and flexibility, particularly for more complex interactions in the open world close to the reality, which considers multiple characters and novel objects interactions. However, before constructing better agents that enable to operate complex interactions, there is no proper benchmark for understanding the complex interactions close to the real-human. To encourage further developments, we propose a new benchmark, which comprises a task called Multiple character and novel Object based interaction Estimation (MOE) and a dataset named Multiple character based novel Object dataset (MOD). MOE task challenges understanding ability for complex interactions close to the real-human. It requires the model to accurately determine the next character to act and identify corresponding actions within long and intricate contexts of multi-character and novel object interactions. Moreover, we collect MOD dataset to support MOE task, which provides long and intricate contexts extracted from game logs which are captured from real-time communications. This dataset is a valuable resource for MOE task, facilitating deeper explorations into understanding and simulating human behavior and interactions.  Finally, We also present a simple prompting baseline, and evaluate its performance alongside other prompting methods utilizing different LLMs. Through objective and subjective evaluations, we demonstrate the effectiveness of our baseline and highlight the significance of our dataset and task in improving interaction understanding.

%Toward more capable agents close to the real human, some works let agent to interact by natural language, benefitting from the significant advancements of Large Language Models (LLMs). However, the agent interactions are still possess limited complexity and flexibly. The interactions are with limited length, limited instructions or limited interactable objects and characters in the virtual world. To overcome the limitation and enable AI agent to achieve more powerful language ability to interact, 
%we introduce a new benchmark for understanding interactions, named Complex Contexts based intention Answering (C2A), and a new dataset to support further exploratino in agent interactions, named  Long-context Multi-character Interaction (LMI) dataset. Specifically, in C2A, given lengthy and intricate contexts, the methods must accurately determine the character who will act next and identify the corresponding actions. This task provides explicit requirements for interaction understanding in agent generation, establishing a solid foundation for the development of factually accurate close to real human.
%Furthermore, we collect LMI dataset, which support C2A task. This dataset contains extended and intricate contexts extracted from game logs featuring real-time communication, offering a valuable resource for C2A and enabling further explorations in this domain. We also present a simple prompting baseline based on the dataset and task, and evaluate its performance alongside other prompting methods utilizing different LLMs. Through objective and subjective evaluations, we demonstrate the effectiveness of our prompting baseline. The results also show the significance of our dataset and task, particularly for improving interaction understanding and vivid response generation of agents.
%We hope that our dataset and task will inspire the research community to deepen their understanding of complex interactions with natural language and foster the development of enhanced AI agents.

%Designing an AI agent to fulfill the role of a game master (GM) in Tabletop Role-Playing Games (TRPGs) presents an intriguing and formidable challenge. TRPGs involve players engaging in immersive role-play, interacting with diverse characters, and receiving guidance from the GM to immerse themselves in fantastical worlds. While previous studies have employed Large Language Models (LLMs) to generate responses and guidance for adjacent game turns or simple player descriptions, the lack of data and benchmarks for complex interactions with grounded language limits their effectiveness in handling the intricate and contextually grounded semantics found in real-time communications that closely resemble human interaction.
%In this paper, we introduce a Character and Skill check Answering (CSA) task, which addresses the need to comprehend the complex and grounded semantics presented in game logs. In CSA, given lengthy and intricate contexts, the methods must accurately determine the character who will act next and identify the corresponding skill required by the game rules. This task provides explicit requirements for semantic understanding in agent generation, establishing a solid foundation for the development of factually accurate and immersive virtual GMs.
%Furthermore, we collect the Long-context Grounded-language TRPG Logs dataset (LGL) specifically for CSA task. This dataset contains extended and intricate contexts extracted from game logs featuring real-time communication, offering a valuable resource for CSA and enabling further explorations in this domain. We also present a simple prompting baseline based on the dataset and task, and evaluate its performance alongside other prompting methods utilizing different LLMs. Through objective and subjective evaluations, we demonstrate the effectiveness of our prompting baseline. The results also show the significance of our dataset and task, particularly for semantic understanding of game agent generation. We hope that our dataset and task will inspire the research community to deepen their understanding of complex grounded semantics and foster the development of enhanced AI agents.


%Designing the AI agent for game master (GM) in Tabletop Role-Playing Games (TRPGs) is an interesting but challenging task, due to the unique game mechanism. Within TRPGs, players engage in role-play, interacting with other characters and guided by the GM to immerse themselves in fantasy worlds. The interactions in the games contain complex and grounded semantics. Utilizing Large Language Models (LLMs), previous works generate better responses or guidance for contexts in adjacent game turns or simple descriptions of players. Meanwhile, the explorations are insufficient to handle complex and grounded semantics within real-time communications that close to the real-human. In this paper, we formalize a task for particularly understanding the complex game logs, named Character and Skill check Answering (CSA). Given a long and complex contexts with grounded language, similar to a real-human GM, the methods must correctly answer who will act next and what skill in game rules are required. Rather than directly generating responses, this task formulate an explicit testbed for the understanding of the contexts, which performs a valuable foundation for a factually accurate and vivid visual GM. Moreover, we construct a new dataset specifically for the task, named Long-context Grounded-language TRPG Logs dataset (LGL). This dataset provides extended and intricate contexts, excerped from game logs with real-time communication, which performs a valuable data source for further explorations. Moreover, based on the dataset and task, we further present a simple prompting baseline and evaluate our baseline and other prompting methods with different LLMs in CSA. Experimental results in both objective and subjective evaluations show the effectiveness of our prompting baseline, which also reveal the significance of our dataset and task. We wish our dataset and task can provide further inspiration for the community in understanding complex grounded semantics and generating better AI agents. 

%However, suffering from the forum-based data collections for game logs, adjacent game turns may contain responses separated by a long time and the reply are not as grounded as the real-time communications. This also induce previous works tend to use latest turn or simple descriptions as inputs and generate GM's utterance or intentions without explicit understanding of contexts, which are insufficient to explore complex and more grounded interactions close to the real-human. To encourage further developments, we construct a new dataset specifically for complex and grounded semantics in TRPG game logs, named Long-context Grounded-language TRPG Logs dataset (LGL). This dataset provides extended and intricate contexts, excerped from game logs with real-time communication. Taking advantages of LGL, we formalize a task for particularly understanding the complex game logs, named Character and Skill check Answering (CSA). Given a long and complex contexts with grounded language, similar to a real-human GM, the methods must correctly answer who will act next and what skill in game rules are required. Rather than directly generating responses, this task formulate an explicit testbed for the understanding of the contexts, which performs a valuable foundation for a factually accurate and vivid visual GM. Moreover, based on the dataset and task, we further present a simple prompting baseline to evaluate the performances of a virtual GM that explicitly considered the understanding of complex contexts in games. In experiment, we compare various prompting methods and LLMs. Results in both objective and subjective evaluations show the effectiveness of our prompting baseline, which also reveal the significance of our dataset and task. We wish our dataset and task can provide further inspiration for the community in understanding complex grounded semantics and generating better AI agents. 

%To construct an AI agent as virtual GM to guide players, previous works usually utilizing Large Language Models (LLMs), take latest game turns or simple descriptions for player actions to generate GM's utterance in game logs. In this setting, though proposed methods are effective, the performances of virtual GM are still have large space to improve to close to the real-human responses. 
%Many works are proposed and aim to generate better virtual GM to support players' playing process. However, current works limited on the forum-based data collections and end-to-end utterance generation, which are insufficient to complex and grounded interactions close to the real-time communication in daily life. To overcome the defeats, we introduce a new dataset specifically for complex and grounded semantics in TRPG game logs, named Long-context Grounded-language TRPG Logs dataset (LGL). This dataset provides extended and intricate contexts, excerped from game logs with real-time communication. Moreover, to push frontier of the generation of game agent in TRPG, we further propose a new benchmark, named think before speak (TBS) that provides two-step generation and jointly considers both understanding and generation. In this benchmark, we propose generation of game check and generation of GM utterance, which target to accurately understanding complex interactions and generate vivid utterance, respectively. During evaluation, we can separately evaluate two parts by objective evaluations like precision and recall and subjective evaluations with real-human volunteers. All experimental results indicate our benchmark offer the high-quality responses as a virtual GM for TRPG games. We also wish our dataset and method can provide further inspiration for the community in understanding complex grounded semantics and generating better AI agents. 

%The intricate and diverse grounded language interactions within TRPGs provide a valuable testbed for understanding and stimulating human reactions, e.g., command generation, state narrative. However, current works focus on abbreviated game contents and relies on forum-based communication, thereby limiting the complexity of contexts and the availability of immediate responses within grounded language.
%RTo encourage the development of understanding complex interactions and generating grounded language reactions, we propose the Long-context Grounded-language TRPG Logs dataset (LGL). This dataset provides extended and intricate contexts, excerped from game logs with real-time communication. Taking advantages of LGL, we further introduce a benchmark called skill check generation (SCG), designed to understand contexts and generate guidance akin to real-human GMs. We apply large language models (LLMs) and evaluate various prompting methods in this benchmark. While all methods demonstrate efficacy in understanding complex semantics and generating guidance, there remains a significant gap compared to real-human GMs. In all, by utilizing complex TRPG logs as a testbed for grounded language interactions, this work aims to stimulate further investigations towards a deeper understanding of grounded semantics and generation, facilitating the creation of vivid reactions from virtual humans.

%Though all methods are workable to understand complex semantics and produce guidance, there are still large gaps compared with the real-human DMs. By taking more complex TRPG logs as the testbed for grounded language interactions, this work aim to foster further investigations towards a deeper understanding for grounded semantics and generation for vivid reactions of virtual human.


%TRPG is a popular game form for entertainment. There are usually multiple players and a game master (GM) embark on adventures in a richly detailed fantasy world. The players operate role-play to act, interact with other characters and guided by GM to push on the game process. Due to the diverse and complex interaction within grounded language in the game, the game logs, recording communication details of players, can be a valuable data source for the exploration of stimulating and estimating real human reactions (e.g., command generation, state generation, etc.). However, current works focus on short contents with limited game turns and forum-based communication. These induce the contexts are not complex enough and lack of immediate response within more grounded language. To encourage further development in understanding complex interaction and grounded human communications, we propose Long-context Grounded-language TRPG Logs dataset (LGL), which offers long and complex contexts within more grounded language collected from game logs with real-time communication. Moreover, we construct a benchmark, skill check generation (SCG), which aims to produce the understanding and guidance of real-human GM. By introduce more complex context with grounded language and specifically designed benchmark, we wish to prompt further explorations for enabling the deeper understanding for vivid and grounded interactions.

%Understanding of grounded natural language interaction is a practical and challenging problem. It may involves multiple characters with different personalities, various scenarios, and diverse background information and contexts. Though it is easy for human being to imagine the interaction and behaviours of characters, models, e.g., large language models (LLMs) always struggling in understand the complex semantics, produce incorrect answers, or even generate gibberish. To push the frontier of this problem, we propose a novel task, SCG (Skill Check Generation in Tabletop Role-Playing Game). Specifically, we leverage the playing records of Tabletop Role-Playing Game, which is a role-playing game with multiple players and a host. All players will conduct various role-playing and complete an adventure together. The host will describe background information, behaviours and reactions of non-player characters, and guides the corresponding characters to perform skill check according their intention and interactions. All the information are contained in the playing records. In SCG, we focus on the skill check in playing records, which is provided by the host after multiple rounds of interactions and naturally labels who to act and what the action should be operated. This formulates a testbed for the understanding of grounded natural language. To generate the skill check, the language models are required to fully understand the complex interactions of various characters, abduct the intention of characters according the contexts, and also comprehend the background story in current scenarios. Moreover, we further propose a chain-of-thought (CoT) prompting method named character and motivation separated prompting (CMSP) to introduce LLMs to separately who and how to act in current round. The experimental results show the effectiveness of our CMSP in solving SCG. All dataset and benchmark will be available soon. 


\input{intro}

\input{rela}

\input{dataset}

\input{method}

\input{exp}

\section{Conclusion}
This paper proposes a new dataset, task, and benchmark to enhance the understanding ability of AI agents in dealing with complex interactions with multiple characters. The existing works in this field have limitations, particularly their reliance on forum-based data collections and do not consider complex and grounded semantics in the real-time communications. To overcome these limitations, we formalize a new task named Multiple character and Open instances based interaction Estimation (MOE), providing a testbed for the understanding ability of the agents and leading further improvements in agents' factual correctness. We also introduce a dataset to support MOE task, which is derived from real-time game logs in tabletop role-playing games (TRPGs) and provides a richer and more complex context capable of supporting MOE tasks.
Additionally, we introduce a prompting benchmark designed specifically to refine the interaction capabilities of AI agents in TRPGs. This benchmark focuses on understanding complex interactions and generating vibrant game master utterances. The three-stage generation process, which includes game check and GM utterance generation, has been evaluated both objectively and subjectively. The results clearly indicate that this approach significantly enhances the quality of AI responses within the TRPG context. We hope that this work will serve as inspiration for the AI community to further explore and enhance their understanding of complex grounded interactions and advance the interaction ability of AI agents.

%In this paper, we propose a new dataset, task and benchmark to enhance the understanding ability of AI agents for long and grounded semantics in Tabletop Role-Playing Games (TRPGs). Due to the limitations of current works, particularly their reliance on forum-based data collections and end-to-end utterance generation, we introduces the novel Long-context Grounded-language TRPG Logs dataset (LGL). This dataset, derived from real-time game logs, provides a richer, more complex context, which can support CSA. 
%We further introduce a prompting benchmark, designed to refine the generation capabilities of AI agents in TRPGs. It focuses on understanding complex interactions and generating vibrant game master utterances. The three-stage generation process, consisting of game check and GM utterance generation, has been evaluated both objectively and subjectively. The results indicate that this approach can significantly improve the quality of AI responses in a TRPG context. We hope this work will inspire the AI community to further explore and enhance the understanding of complex grounded semantics and the generation of AI agents.

\section{Limitations and Social Impacts}
While the use of an AI agent in a tabletop role-playing game (TRPG) could revolutionize the way these games are played, providing consistent and unbiased decisions, there are potential limitations and social impacts to consider. One key limitation is the AI's ability to simulate human creativity, empathy, and adaptability, which are all fundamental to the role of a game master. For instance, the AI may not fully comprehend nuanced player interactions or adapt the game based on the players' emotional state. Additionally, there could be social implications, such as the potential reduction in human interaction and shared storytelling, which are often crucial elements of TRPGs. For players, part of the joy of a TRPG is the shared human experience, the unpredictable responses, and the subtle non-verbal cues, which an AI might not replicate. The introduction of an AI game master could also result in job loss in professional game-mastering circles. Despite the AI's potential to provide a consistent and more accessible gaming experience, these human and social elements may be irreplaceable in a TRPG context.

%\bibliographystyle{neurips_data_2023}
%\bibliography{reference}
{
		\small
		\bibliographystyle{ieee_fullname}
		\bibliography{reference}
}

\end{document}
