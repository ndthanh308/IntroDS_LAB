\section{Introduction}
%\noindent\emph{A child use such primitive forms of language when he learns to task. Here the teaching of language is not explaining, but training.}
\noindent\emph{... the teaching of language is not explaining, but training.}

-- Ludwig Josef Johann Wittgenstei, Philosophical Investigations 

\iffalse 
Format Hardback | 592 pages
Dimensions 160 x 235 x 35mm | 972g
Publication date 13 Nov 2009
Publisher John Wiley and Sons Ltd
Imprint Wiley-Blackwell (an imprint of John Wiley & Sons Ltd)
Publication City/Country Chicester, United Kingdom
Language English
Edition Statement 4th Edition
ISBN10 1405159286
ISBN13 9781405159289
page 7 
\fi 



In recent years, there has been a growing interest in constructing AI agents capable of simulating and supporting human-like interactions across various domains. Notably, some agents have exhibited exceptional performance, surpassing human abilities in games like MOBA, Starcraft, poker, and Go. Building on the advancements in Large Language Models (LLMs), researchers have extended agent interactions to incorporate natural language. For instance, Park et al. \cite{park2023generative} have introduced generative agents that engage in free-form interactions using natural language, thereby creating virtual worlds where agents reside and even demonstrate spontaneous activities such as hosting parties. Similarly, Liu et al. \cite{liu2023training} have developed simulated societies in which LLM-powered agents engage in the virtual world and can support some discussions for social problems. These recent developments hold promise for advancing AI agents by leveraging natural language as an interactive tool, enabling them to exhibit more human-like behaviors. Furthermore, the exploration of phenomena resulting from endowing agents with more powerful language abilities for interaction can offer valuable insights. As discussed in the philosophical investigation, Ludwig Josef Johann Wittgenstein emphasized that teaching language is a form of training rather than mere explanation. General human communication is similar to engaging in a language game. Language serves as a fundamental tool for human interaction with the environment, facilitating the transmission of information, communication, negotiation, and cooperation within human groups, and contributing to the overall functioning of society. While the relationship between language and intelligence remains an open question, it is always worth exploring the potential evolution of more powerful and autonomous agents that can interact using natural language.


Going further with agent interactions, we have yet to fully empower the sufficient openness and freedom in the interactions between agents and the world. Existing approaches have often imposed constraints on agent interactions, leading to limited complexity and diversity in their capabilities. These constraints arise from the lack of interactions involving novel objects and multiple characters. While some prior research has explored language-based interaction abilities in generative agents~\cite{park2023generative}, their diversity remains restricted, focusing on a limited range of interactable objects. Additionally, previous works have primarily concentrated on two-character communication without considering implicit intentions through complex interactions. Such interactions fail to encompass nuanced behaviors (e.g., refusal, persuasion, group decision making, coalition building), akin to real-time communications involving multi-characters.


%Specifically, although some prior research has explored language-based interaction abilities in generative agents~\cite{park2023generative}, the diversity of these agents is still restricted, with a limited range of interactable objects considered. Previous works have primarily focused on two-character communication without the consideration of implicit intentions through complex interactions. Two agents present plain and simple interactions, which do not consider convey complex or implicit intentions like refusing, persuading, group decision making, coalition building, etc., close to the real-time communications with multi-characters. 



%failing to capture the complexity and realism of long and intricate multi-character interactions that are characteristic of real-time communication. 
%Furthermore, before we can develop better agent interactions, it is crucial to ensure that the agents first understand complex interactions. At present, we have not achieved the point where agents can autonomously engage in complex interactions themselves. Therefore, addressing the understanding challenge is a vital step towards empowering AI agents for more sophisticated and diverse interactions.
%Moreover, before empowering AI agents to engage in more complex and diverse interactions, we should first let the agents understand the complex interactions close to the real-human, which contains interactions with unlimited objects and characters. 


To address this challenge, we draw inspiration from tabletop role-playing games (TRPGs) and introduce a Game Master (GM) role into the agent's world model. TRPGs inherently offer highly complex and diverse interactions through natural language, involving multiple players in intricate and grounded multi-character scenarios. The GM oversees the game, provides scenario details, understands characters' intentions, and offers feedback on player actions, aligning with the requirements for a more comprehensive world model. Constructing and introducing a virtual GM capable of handling complex interactions with real humans could significantly enhance the feedback given to agents.
However, existing benchmarks in TRPG-related research lack the scope needed to develop a virtual GM that compensates for world model deficiencies. Current virtual GM works only explore short and simple interactions in limited rounds, lacking sufficient complexity. For instance, previous works have been derived from play-by-post forums~\cite{martin2018dungeons,callison-burch-etal-2022-dungeons}, where players contribute by writing and posting their responses on the forum. While, this asynchronous online communication introduces significant delays, with players often waiting for hours or even weeks to receive responses. As a result, data collected from such forums struggle to capture the vibrant and nuanced grounded semantics characteristic of real-time human interactions.
Moreover, the forum-based communication format tends to encourage players to respond to the immediate turn and provide formal written replies, thereby limiting the richness and groundedness of expressions that can be observed in real-time interactions with multi-characters. Consequently, previous works derived from forum data do not fully represent the diversity and complexity found in real-world multi-character interactions. More comprehensive and realistic benchmarks are needed to support the development of effective virtual GMs and address the deficiencies in agent world models.


%To address this challenge, inspired by the game settings in tabletop role-playing games (TRPGs), we introduce a role of Game Master (GM) into the world model for agents. In detail, TRPGs, such as Dungeons \& Dragons (DND)~\cite{gygax1974dungeons}, possess inherent characteristics that facilitate high levels of interaction complexity and diversity through natural language. In TRPGs, multiple players participate, leading to intricate and grounded multi-character interactions. The game logs in TRPGs present the realism interactions with high complexity and flexibility that the agents have yet to reach. Moreover, TRPGs feature a unique role known as the Game Master (GM), who oversees the game, describe the details for scenarios of the game, understands characters' intentions, and provides feedback for players that whether their actions can be successfully operated and what will happen if the interaction operated. While, these abilities just align the requirements for more comprehensive world model for agents and may partly compensate the deficiencies. If we can construct a virtual GM that enable to handle complex interactions and engage the games with real human, it would be very competent to provide correct and vivid feedback for the agents. However, existing benchmarks in TRPG-related research are inadequate for facilitating the exploration for constructing a virtual GM to compensate the deficiency of world model for agents. We expect the agents to generate complex multi-character interactions as in real-time communications. Current works for virtual GMs only explore in short and simple contexts with interactions in limited rounds, which are lack of sufficient complexity. Specifically, previous works have been derived from play-by-post forums~\cite{martin2018dungeons,callison-burch-etal-2022-dungeons}, where players contribute by writing and posting their responses on the forum. While, this asynchronous online communication introduces significant delays, with players often waiting for hours or even weeks to receive responses. As a result, data collected from such forums struggle to capture the vibrant and nuanced grounded semantics characteristic of real-time human interactions.
%Moreover, the forum-based communication format tends to encourage players to respond to the immediate turn and provide formal written replies, thereby limiting the richness and groundedness of expressions that can be observed in real-time interactions with multi-characters. As a consequence, existing benchmarks based on forum data do not fully reflect the diversity and complexity of real-world interactions involving multiple characters.






%our study identifies tabletop role-playing games (TRPGs) as a suitable testbed. TRPGs, such as Dungeons \& Dragons (DND)~\cite{gygax1974dungeons}, possess inherent characteristics that facilitate high levels of interaction complexity and diversity through natural language. In TRPGs, multiple players participate, leading to intricate and grounded multi-character interactions. Moreover, TRPGs feature a unique role known as the Game Master (GM), who oversees the game, narrates the unfolding story, understands the intentions of the characters, and assesses the success of character actions. This aligns with the requirements of abilities for agents to engage in complex interactions, making TRPGs a promising avenue for advancing agent interactions.
%However, existing benchmarks in TRPG-related research are inadequate for facilitating comprehensive exploration of agent interactions. These benchmarks have been derived from play-by-post forums~\cite{martin2018dungeons,callison-burch-etal-2022-dungeons}, where players contribute by writing and posting their responses on the forum. While, this asynchronous online communication introduces significant delays, with players often waiting for hours or even weeks to receive responses. As a result, data collected from such forums struggle to capture the vibrant and nuanced grounded semantics characteristic of real-time human interactions.
%Moreover, the forum-based communication format tends to encourage players to respond to the immediate turn and provide formal written replies, thereby limiting the richness and groundedness of expressions that can be observed in real-time interactions with multi-characters. As a consequence, existing benchmarks based on forum data do not fully reflect the diversity and complexity of real-world interactions involving multiple characters.


In this paper, we take the first step towards enhancing the world model for agents by integrating a virtual GM role capable of handling complex real-time interactions with multiple characters. We propose a benchmark, named Tachikuma, designed to encourage the designation of the virtual GM to effectively handle these complex interactions, infer characters' intentions, and provide accurate feedback to corresponding characters.
Our benchmark consists of two components: a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. In MOE, models are presented with intricate contexts extracted from TRPG log data, capturing real-time communications during gameplay. The objective is to infer character intentions and identify corresponding interactions, typically represented as skill checks, judged by a GM. The dataset supports the MOE task by providing long and intricate contexts from game logs, featuring interactions among multiple characters. The complexity of interactions among multiple characters, grounded in natural language, makes MOE a valuable testbed for evaluating abilities of virtual GMs. 



%In this paper, we aims to take the first step for complex real-time interactions with multiple characters. We present a benchmark to encourage LLMs to handle complex interactions, infer characters' intentions, and produce correct feedback to corresponding characters. Our benchmark comprises two components: the Multiple character and novel Object based interaction Estimation (MOE) task and a dataset to support the task. In MOE, models are presented with intricate contexts extracted from TRPG log data, capturing real-time communications during gameplay. The objective is to infer character intentions and identify corresponding interactions, typically represented as skill checks, judged by a Game Master (GM). The dataset supports the MOE task by providing long and intricate contexts from game logs, featuring interactions among multiple characters. The complexity of interactions among multiple characters, grounded in natural language, makes MOE a valuable testbed for evaluating abilities of virtual GMs. 


%In this paper, we present a benchmark for future exploration of open world complex interactions. Our benchmark consists of two components. First, we propose a novel task called Multiple character and novel Object based interaction Estimation (MOE), which focuses on understanding interactions within novel objects and involving multiple characters. Second, we introduce Multiple character and novel Object based Interaction dataset (MOD), a dataset designed to facilitate the exploration of complex interactions and support MOE task. In MOE task, models are presented with intricate contexts extracted from log data in TRPGs. These contexts capture grounded language from real-time communications during gameplay. The objective of the agents is to infer the intentions of the characters and identify their corresponding interactions. In TRPGs, character interactions are typically judged by a GM and often represented as skill checks. This game form naturally allows for the annotation of player intentions and interactions in the game logs, eliminating the need for additional manual annotations. Moreover, the active involvement of GMs as actual participants ensures the accuracy of intention judgments. Additionally, TRPGs commonly involve multiple characters who must communicate and collaborate to achieve their goals and complete adventures. As a result, the log data inherently captures complex interactions among multiple characters, characterized by grounded language. This aspect makes MOE task a valuable testbed for evaluating the understanding ability in handling intricate interactions. Overall, our benchmark targets for understanding complex interaction in the real world and establishes a solid foundation for constructing more realistic agents that simulate real-human interactions.

%It introduces diversity and complexity into agent interactions, enabling agents to interact with diverse instances in the world. Furthermore, the world model can guide agents in multi-character communications. By comprehending multi-character interactions, the world model can provide feedback to the agents, indicating whether they have successfully persuaded or cooperated with others, thereby introducing further complexity into the interactions.


Furthermore, in our dataset, we collect complex and long contexts with diverse real-human interactions from the game logs. Our dataset differs from conventional play-by-post forum data collection methods. Instead, we utilize data extracted from a Chinese TRPG forum\footnote{www.goddessfantasy.net}. These forum records, compiled by GMs after game ending, consist of voice recordings or real-time chat logs. This data source overcomes the limitations of play-by-post data collection, enabling us to extract long contexts with complex semantics similar to the real interactions. As these logs capture immediate communications, the interactions also exhibit higher groundedness, resulting in more vibrant and realistic responses akin to everyday conversations, as demonstrated in Fig.~\ref{fig:intro_moe}.
Moreover, our dataset encompasses not only the popular DND rules~\cite{gygax1974dungeons} but also a wide range of diverse game rules, including Call of Cthulhu (COC)~\cite{lovecraft2016call}, Pathfinder2 (PF2)~\cite{bulmahn2010pathfinder}, Savage Worlds (SW)~\cite{hensley2008savage}, etc. This diversity enhances the complexity and variety of our dataset. Building upon this dataset, we introduce MOE task, which consists of 1,003 context sections extracted from the game logs. Each section represents a complete adventure with multiple turns, showcasing intricate semantics. As shown in Tab.~\ref{tab.dataset_sta}, MOE includes an average of 32.12 turns per context excerpt, in contrast to previous works that typically involve only one turn.
The number of possible answers for characters and skills varies depending on the context, ranging from one to eleven. Additionally, specific game rules necessitate different skill categories for answers. For instance, considering the DND rule, there are 51 potential skills. These factors collectively contribute to MOE representing a challenging task for AI agents. The agent must demonstrate a comprehensive understanding of both the complex interactions, emulating human-like comprehension.
To provide a comprehensive assessment, we report the F-score as the final metric, separately for the predicted characters and overall intention answers. Evaluating character predictions reflects the accuracy of methods in inferring players' intentions. Simultaneously, evaluating overall answers offers insights into the understanding ability of both character intentions and the corresponding interactions. 


Finally, we present a three-step prompting baseline for constructing an agent capable of handling interactions like a real-human GM in TRPGs. Our simple baseline serves to demonstrate the value of our task and dataset in understanding complex interactions. Our method incorporates prompts specifically related to existing characters, their intentions, and the associated skill checks. By utilizing these prompts, we guide LLMs in gradually comprehending the intricate interactions that occur between players. We thoroughly evaluate our baseline method and compare its performance with other prompting methods utilizing various LLMs within MOE task. 
The experimental results indicate that MOE task is solvable but still possesses a large room for further improvement.
Furthermore, leveraging the answers obtained from MOE task, we employ LLMs to generate responses that simulate a real-human GM in the games. To evaluate the quality of these generated responses, we invite numerous volunteers to provide subjective evaluations. The experimental results demonstrate that incorporating the improved understanding ability of the agent leads to higher levels of factual correctness, naturalness, and groundedness in the generated responses, closely resembling real-human interactions. These results further underscore the significance of understanding ability in constructing proficient agents and highlight the importance of our benchmark.
We hope our dataset and benchmark as valuable resources that will inspire the research community to delve into the understanding of complex interactions and contribute to the development of more capable AI agents.



Our contributions can be summarized as follows:

1. We introduce a Multiple character and novel Object based interaction Estimation (MOE) task, specifically addressing challenges in handling complex interaction like a real-human GM. This task serves as a valuable testbed for evaluating the abilities of constructing virtual GMs and contributes to advancements in developing more realistic agents.

%We present a Multiple character and novel Object based interaction Estimation (MOE) task, which is designed to tackle the challenges associated with complex multi-character interaction understanding. This task serves as a valuable testbed for evaluating the agents' understanding abilities and contributes to the ongoing advancements in constructing more vivid agents.

%1. We introduce Multiple character and Open instances based interaction Estimation (MOE) task, specifically designed to address the challenges of complex multi-character interaction understanding. This task serves as a valuable testbed for assessing the understanding abilities of agents, which is valuable in further improvements of world model designation for agents' interactions.

2. We collect a dataset for MOE to address the limitations in exploring long contexts and intricate multi-character interactions in real-time communications. This dataset bridges a crucial gap in the current research, offering a comprehensive resource for analyzing and understanding these complex interactions.

%We introduce a Multiple character and novel Object based Interaction dataset (MOD) to overcome the existing limitations in investigating long contexts and intricate multi-character interactions within real-time communications. This dataset fills a crucial gap in the current research, providing a comprehensive resource for analyzing and understanding these complex interactions.

%2. We create a new dataset, Multiple character based Open Interaction dataset (MOD), to address the limitations in exploring long contexts and complex multi-character interactions in real-time communications. This dataset fills a gap in the current research by providing a comprehensive resource for analyzing such complex interactions.

3. We introduce a prompting baseline and conduct a comprehensive evaluation of different prompting methods using a range of Large Language Models (LLMs) within MOE task. The experimental results indicate that MOE task is solvable, yet there is ample room for further improvement. %These results highlight the potential for improving the performance of agents in complex multi-character interactions within MOE task.

4. We conduct subjective evaluations based on the answers obtained from MOE. These evaluations show that better performances in MOE lead to higher levels of factual correctness, naturalness, and groundedness in the generated responses, which are crucial factors for creating a vivid agents. These results further underscore the significance of our dataset and task in improving AI agents.  













%In recent years, the field of artificial intelligence (AI) has witnessed significant advancements, with researchers and developers exploring its potential in various domains. One area that has gained considerable attention is the design and development of AI agents for games. Many academic research~\cite{} and commercial applications~\cite{} have demonstrated the transformative power of AI agents, enhancing player experience, pushing the boundaries of game design and also offering constructive experiences for problems in the reality. Among various games, 


%Constructing AI agent raise more and more attention recently, which can simulate and support real human in many aspects in daily life. Many works are continously purse the more intelligent and more powerful agents. Especially in game agents, many works have already constructed AI agents that even outperform real-human performances in playing games like moba, starcraft, poker games, go game etc. Recently, with the significant advancements in LLMs, many research go further with the agents' interactions by using natural language. Park et al.~\cite{park2023generative} present the generative agents that can interact with freely by using natural language. Following one of a part of rule named DND from TRPGs, they produce a virtual world for agents and let them live in the environments. We can even whiteness the evolution of the behaviours of agents and find that they can spontaneously generate some actitivies like host a party. Liu et al.~\cite{liu2023training} further produce a simulated society by agents using LLMs and discuss some social problems. More than just learning and simulate human language, the recent works may show a more promising way toward better AI agent that react like a real human being. As discussed in discussion of Philosophical Investigation, language can be a kind of tool for human being to interact with environments. Language supports human to explain and deliver information to others, which formualte the fundation of communication, negotiation and coorperation of human group and prompt the functioning of society. Though the relational between language and intelligence is still an open question, it always worth to looking forward to the evolutions what if we endue more powerful ability for agents to interact with the virtual worlds and other agents. 


%In recent years, there has been increasing interest in constructing AI agents capable of simulating and supporting human-like interactions in various domains. Some agents have demonstrated remarkable performance, surpassing human capabilities in games such as MOBA, Starcraft, poker, and Go. Furthermore, leveraging the significant advancements in Language Models (LLMs), researchers have extended agent interactions to include natural language. For instance, Park et al.\cite{park2023generative} introduced generative agents that interact freely using natural language, creating virtual worlds in which agents reside and even exhibit spontaneous activities like hosting parties. Liu et al.\cite{liu2023training} developed simulated societies in which agents, powered by LLMs, engage in social problem discussions. These recent developments hold promise in advancing AI agents by leverage natural language as interactive tools, which lead to exhibit more human-like behaviors. Moreover, the interesting phenoemons after enduring more powerful language ability for agent to interact may contains more valuable insights. As discussed in the philosophical investigation, Ludwig Josef Johann Wittgenstei also pointed out teaching language is a kind of training, rather than explaining. The general communications of human being are similar to engage a game of using language. Language serves as a fundamental tool for human interaction with the environment. It enables the transmission of information, facilitates communication, negotiation, and cooperation within human groups, and promotes societal functioning. While the relationship between language and intelligence remains an open question, it is always worth exploring the potential evolution of more powerful and free agents that interact with natural language.




%Going further with agent interactions, we have yet to fully empower the sufficient openness and freedom in the interactions between agents and the world. Previous works have often constrained agent interactions, resulting in limited complexity and diversity. These limitations arise from the absence of a comprehensive virtual world model that allows the agents to interact with open instances and multiple characters. Specifically, although language-based interaction ability for agents has been explored in generative agents~\cite{park2023generative}, the diversity of these agents remains constrained, with a limited range of interactable objects. It is hard to list all possible instances in the world for agents. Moreover, previous works focused primarily on two-character communication, lacking the complexity and realism of long and intricate multi-character interactions found in real-time communication. For example, the agents are hard to react and cooperate as a team to solve particular problems. 


%Going further with agent interactions, we have yet to fully empower the sufficient openness and freedom in the interactions between agents and the world. Previous works have predominantly imposed restrictions on agent interactions, resulting in limited complexity and diversity. In our study, as mentioned in~\cite{ha2018world} as well, these deficiencies are source from the absence of a vivid virtual world model. In detail, the world model for agent interactions should involve a series of rules to interact with diverse objects and a series of strategies to interact with other characters. Specifically, though enduring language-based interaction ability for agents, the diversity of generative agents~\cite{park2023generative} remains constrained, often with a limited range of interactable objects. Besides, due to the limited interactions that only focus on communications of two characters, the interactions in previous works~\cite{park2023generative,gandalf} lack of long and complex multi-character interactions similar to the real-time communication in the reality. To overcome these limitations and enable AI agents to engage in more complex and diverse interactions, there is a clear need for a new testbed and benchmark that enable agents' interactions considering a virtual world model or world model, while also allowing for unlimited objects and characters. In light of this challenge, our study recognizes the potential of tabletop role-playing games (TRPGs) as a suitable testbed. TRPGs, exemplified by games like Dungeons \& Dragons (DND)~\cite{gygax1974dungeons}, offer inherent characteristics that facilitate high levels of interaction complexity and diversity through natural language. Generally, there are multiple players participate in the games, which induce complex and grounded multi-character interactions. Meanwhile, TRPGs contain an uniqe role named Game Mater (GM), who oversees the game, narrates the unfolding story, understands the intentions of characters, and judges whether the movements of characters can be successful operated. These completely satisfy the requirements of a virtual world models, which also lead the game can be a promising avenue for evaluating and advancing agent capabilities. 

%To overcome these limitations and enable AI agents to engage in more complex and diverse interactions, there is a clear need for a new testbed and benchmark that empowers agents with the ability to interact using natural language in long contexts, while also allowing for unlimited objects and characters. In light of this challenge, our study recognizes the potential of tabletop role-playing games (TRPGs) as a suitable testbed. TRPGs, exemplified by games like Dungeons \& Dragons (DND)~\cite{gygax1974dungeons}, offer inherent characteristics that facilitate high levels of interaction flexibility through grounded natural language, presenting a promising avenue for evaluating and advancing agent capabilities.

%Most approaches allow only limited-length interactions or responses, such as short contexts or brief instructions. Despite some attempts to enhance interaction abilities, such as incorporating natural language in generative agents~\cite{park2023generative}, the flexibility of these interactions remains constrained, often with a limited range of interactable objects and characters. 

%Going further with the interactions of agents, we still have not leveraged sufficient openness and freedom for the interactions between agents and the world. Agents in previous works are usually possess limited interactions. The limitations are mainly from two aspects, which are complexity and flexibility. Most works enable the interactions or responses of AI agent within limited length, e.g., short-contexts, brief instructions. Moreover, though some works empower more diverse interaction ability for the agents, e.g., natural language based interaction in generative agent, the flexibility of the interactions are still limited, e.g., limited number of interactable objects and characters. To exploit more complex and more flexible interaction for AI agent, a new testbed and benchmark for agent design are required, which should enable the interaction with natural language within long context and unlimited objects and characters. In our study, we found the requirements can be satisfied by TRPGs. Tabletop role-playing games (TRPGs)~\cite{gygax1974dungeons,chung2013table,daniau2016transformative,zagal2018definitions} possess high flexibility of interactions facilitated through grounded natural language~\cite{cover2014creation,daniau2016transformative,gandalf}, which can be the proper testbed for agents. 


%However, existing benchmarks in TRPGs related researches are inadequate for supporting comprehensive exploration of agent interactions. They have predominantly been collected from play-by-post forums~\cite{martin2018dungeons,callison-burch-etal-2022-dungeons}, where players participate by writing and posting their responses on the forum. However, this asynchronous online communication introduces significant delays, with players sometimes waiting for hours or even weeks to receive responses. Consequently, data collected from such forums struggle to capture the vivid and nuanced grounded semantics characteristic of real-time human interactions.


%Prior works, such as Park et al.~\cite{park2023generative}, introduce limited TRPG rules that impose constraints on the interactions between agents. The resulting agent communications often lack the enough diversity, logicality, and groundedness observed in real human interactions. This deficiency can be attributed to a lack of understanding of long and complex contexts, particularly in relation to the communication dynamics among multiple characters. Agents often exhibit simplistic responses without a deeper understanding of the intricate semantics and relationships between characters.

%However, current solutions and datasets are hard to support the exporation of agents' interaction. Park et al.~\cite{park2023generative} introduce limited TRPG rules to constraint interactions between agents. The communications between agents are also far from the real human with sufficient diversity, logicality, and groundedness. These is because the absence of understanding of long and complex contexts from the communication of multiple characters. The agents tend to simply response to others and without deeper consideration of the complex semantics and characters' relationships. Moreover, datasets for TRPG are collected from play-by-post forums~\cite{martin2018dungeons,callison-burch-etal-2022-dungeons}, where players participate in the game by writing their responses and posting them on the forum. However, this asynchronous online communication often results in extended waiting periods for players to receive responses, sometimes spanning hours or even weeks. Consequently, the data collected from such forums struggle to accurately capture the vivid and complex grounded semantics present in real-time human interactions. Moreover, the nature of forum-based communication tends to encourage players to respond to the nearest turn and provide formal written replies, thereby restricting the richness and flexibility of expressions.






%In this paper, we aim to push the frontier of interactions of agents through constructing a benchmark for a world model. This can be proposed in two folds. First, we propose a novel task, Multiple character and Open instances based interaction Estimation (MOE). which focuses on interaction within novel objects and involving multiple characters. Second, we introduce a new dataset, the Multiple character based Open Interaction dataset (MOD), specifically designed to facilitate the exploration of complex interactions and support MOE. In MOE task, the world models are presented with intricate contexts extracted from log data in TRPGs. These contexts capture grounded language from real-time communications during gameplay. The objective of the agents is to infer the intentions of the characters and identify their corresponding interaction. In TRPGs, the characters' interactions are judged by a GM and are often represented as skill checks. This game form enables the annotation of player intentions and interactions in the game logs inherently, avoiding the need for additional manual annotations. Additionally, the involvement of GMs as actual participants ensures the accuracy of intention judgments. Moreover, TRPGs typically involve multiple characters who must communicate and collaborate to achieve their goals and complete adventures. As a result, the log data naturally captures complex interactions between these characters, characterized by grounded language. These factors make MOE a valuable testbed for evaluating the world models' understanding ability in handling complex interactions. In all, our benchmark formulates a valuable foundation for constructing more vivid agents that simulate real-human interactions. This can introduce more diversity and complexity in agent interactions. The agents can interact with diverse instances in the world. Meanwhile, the world model can further guides the agents in multi-character communications. After comprehending multi-character interactions, the world model can indicate the agents whether they have persuaded or successfully coorperated with others and induce more complexity. 

%Moreover, MOE plays a crucial role in enhancing the quality of agent interactions. To assess the impact of C2A, we perform a comparative analysis of the generated utterances produced by various LLMs~\cite{brown2020language,zhao2023survey,huang2022towards}. We consider instances both with and without incorporating answers derived from C2A. To ensure a comprehensive evaluation, we rely on subjective assessments conducted by volunteers. Our analysis reveals a significant improvement in the performance of the generated responses when C2A is integrated. Specifically, we observe a remarkable 75\% increase in factual correctness, accompanied by notable 10\% improvements in terms of groundness and naturalness. These compelling results underscore the efficacy of C2A in propelling the advancement of superior AI agents.



%Complex Contexts based intention Answering (C2A), which focuses on agent interaction within long and complex contexts involving multiple characters. Second, we introduce a new dataset, the Long-context Multi-character Interaction (LMI) dataset, specifically designed to facilitate the exploration of complex interactions.
%In C2A task, agents are presented with extensive and intricate contexts extracted from log data in TRPGs. These contexts capture grounded language from real-time communications during gameplay. The objective of the agents is to infer the intentions of the characters and identify their corresponding intended actions, which are often represented as skill checks. In TRPGs, these skill checks are provided by a game master (GM). This game form enables the annotation of player intentions in the game logs inherently, avoiding the need for additional manual annotations. Additionally, the involvement of GMs as actual participants ensures the accuracy of intention judgments.
%Moreover, TRPGs typically involve multiple characters who must communicate and collaborate to achieve their goals and complete adventures. As a result, the log data naturally captures complex interactions between these characters, characterized by grounded language. These factors make C2A a valuable testbed for evaluating the AI agents' understanding and capability in handling complex interactions.
%Moreover, C2A plays a crucial role in enhancing the quality of agent interactions. To assess the impact of C2A, we perform a comparative analysis of the generated utterances produced by various LLMs~\cite{brown2020language,zhao2023survey,huang2022towards}. We consider instances both with and without incorporating answers derived from C2A. To ensure a comprehensive evaluation, we rely on subjective assessments conducted by volunteers. Our analysis reveals a significant improvement in the performance of the generated responses when C2A is integrated. Specifically, we observe a remarkable 75\% increase in factual correctness, accompanied by notable 10\% improvements in terms of groundness and naturalness. These compelling results underscore the efficacy of C2A in propelling the advancement of superior AI agents.

%Furthermore, C2A is valuable for improve the quality of interactions between agents. we conduct a comparative analysis of the generated utterances produced by different Language Models (LLMs)~\cite{brown2020language,zhao2023survey,huang2022towards}, considering both instances with and without incorporating answers derived from C2A. Through subjective evaluations conducted by volunteers, we observe a significant improvement in the performance of the generated responses when C2A is integrated. Specifically, the evaluations reveal a 75\% improvement in factual correctness, along with 10\% improvements in terms of groundness and naturalness. These findings underscore the efficacy of C2A in advancing the development of superior AI agents.


%In this paper, we aim to push the frontier of interactions of agents in two aspects: 1). providing a new task specific explore the interaction of agent within long and complex context within multi-characters, Complex Contexts based intention Answering (C2A); 2). introducing a new dataset to support the exploration of complex interactions, named Long-context Multi-character Interaction (LMI) dataset. Specifically, in C2A, the agent is presented with long and intricate contexts from log data in TRPGs, featuring grounded language extracted from game logs in real-time communication. The agent's objective is to determine which characters are intending to act and what is the corresponding intended actions. Benefitting from the setting of TRPGs, the intended actions can be simplified as the sill checks, which are provided by a game master (GM) who oversees the game and narrates the unfolding story. This indicates that the intentions of players are naturally annotated by a real-human GM in the game logs, which avoid additional annotations. Meanwhile, as an actual participator of the games, checks from GMs also ensure the accuracy of judging people's intentions. Moreover, there are always multiple characters in TRPGs. All characters should communicate and coorperate with each others to reach some goals and complete the adventures. This leads the log data of TRPGs naturally contains complex interactions of multiple characters with grounded language. All these factors induce C2A formulate a valuable testbed for understanding ability of AI agents for complex interactions. Furthermore, we conduct a comparative analysis of the generated utterances produced by LLMs~\cite{brown2020language,zhao2023survey,huang2022towards}, considering both instances with and without incorporating answers derived from C2A. Through subjective evaluations conducted by volunteers, we observe a significant improvement in the performance of generated responses when C2A is integrated. Specifically, the evaluations reveal a 75\% improvement in factual correctness, along with 10\% improvements in terms of groundness and naturalness. These results further emphasize the value of C2A in constructing superior AI agents.




%Furthermore, in LMI, we collect complex and long contexts with diverse real-human interactions from the game logs. Our dataset differs from conventional play-by-post forum data collection methods. Instead, we utilize data extracted from a Chinese TRPG forum\footnote{www.goddessfantasy.net}. These forum records, compiled by Game Masters (GMs) after game sessions, consist of voice recordings or real-time chat logs. This data source overcomes the limitations of play-by-post data collection, enabling us to extract long contexts with complex semantics from the game logs. As these logs capture immediate communications, the interactions exhibit higher groundedness, resulting in more vibrant and realistic responses akin to everyday conversations, as demonstrated in Table~\ref{tab.data_eg1}.
%Moreover, our dataset encompasses not only the popular DND rules~\cite{gygax1974dungeons} but also a wide range of diverse game rules, including Call of Cthulhu (COC)~\cite{lovecraft2016call}, Pathfinder2 (PF2)~\cite{bulmahn2010pathfinder}, Savage Worlds (SW)~\cite{hensley2008savage}, and others. This diversity enhances the complexity and variety of our dataset. Building upon this dataset, we introduce the C2A task, which consists of 1,003 context sections extracted from the game logs. Each section represents a complete adventure with multiple turns, showcasing intricate semantics. As shown in Table~\ref{tab.dataset_sta}, C2A includes an average of 32.12 turns per context excerpt, in contrast to previous works that typically involve only one turn.
%The number of possible answers for characters and skills varies depending on the context, ranging from one to eleven. Additionally, specific game rules necessitate different skill categories for answers. For instance, considering the DND rule, there are 51 potential skills. These factors collectively contribute to C2A representing a challenging task for AI agents. The agent must demonstrate a comprehensive understanding of both the complex interactions, emulating human-like comprehension.
%To provide a comprehensive assessment, we report the F-score as the final metric, separately for the predicted characters and overall intention answers. Evaluating character predictions reflects the accuracy of methods in inferring players' intentions. Simultaneously, evaluating overall answers offers insights into the understanding ability of both character intentions and the corresponding interactions. 



%Moreover, in our dataset, instead of relying on data collected from play-by-post forums, we leverage data sourced from a Chinese TRPG forum\footnote{www.goddessfantasy.net}. The forum records are typically compiled by the GM after the ending of games, comprising voice recordings or real-time chat logs. This distinct data source overcomes the limitations of play-by-post data collection, enabling us to extract long contexts with complex semantics from the game logs. Since these logs are derived from immediate communications, the interactions exhibit the higher groundedness and feature more vibrant responses akin to everyday conversations, as exemplified in Tab.~\ref{tab.data_eg1}.
%Moreover, our dataset encompasses not only Dungeons and Dragons (DND) rules~\cite{gygax1974dungeons} but also a wide range of diverse game rules such as COC~\cite{lovecraft2016call}, PF2~\cite{bulmahn2010pathfinder}, SW~\cite{hensley2008savage}, etc. This diversity further enriches the complexity and variety of our dataset. Building upon this dataset, we construct the C2A task, consisting of 1,003 sections of contexts extracted from the game logs. Each section encompasses a complete adventure and multiple turns, exhibiting intricate semantics. As in Tab.~\ref{tab.dataset_sta}, C2A comprises an average of 32.12 turns per excerpted context, while previous works typically contain only one turn. 
%The number of answers for characters and skills can vary from one to eleven, depending on the context. Additionally, since the answers must adhere to specific rules, contexts associated with different game rules necessitate different skill categories. For instance, considering the DND rule, there are 51 possible skills for the answers. These factors collectively contribute to CSA presenting a challenging task for AI agents in TRPG. The agent must fully comprehend both the complex contexts and the game rules, akin to a real-human understanding. For evaluation purposes, we first compute precision and recall separately for the predicted characters and overall intention answers. To provide a comprehensive evaluation, we report the F-score as the final metric, taking into account both precision and recall. Evaluating for the characters reflects the understanding accuracy of the methods in inferring players' intentions. Simultaneously, evaluating overall answers provides insights into the understanding ability concerning both character intentions and the rules of the games.










%Finally, we present a three-step prompting baseline for constructing an agent capable of simulating a GM in TRPGs and interacting with real-human players. Since GMs play the role of world models in the games, taking the real-human interactions as the agents in the games, the generation and evaluation of the virtual GM can also reflect the performance of a world model. Thus, our simple baseline can show the value of our task and dataset in understanding complex interactions. Our method incorporates prompts related to existing characters, their intentions, and the associated skill checks. By utilizing these prompts, we guide LLMs in gradually understanding the intricate interactions between players. We evaluate our baseline method and compare it with other prompting methods using various LLMs within MOE. The experimental results establish a strong correlation between the performance in the MOE task and the understanding capabilities of the agents.
%Besides, leveraging the answers obtained from MOE task, we employ LLMs to generate responses that simulate a real-human GM in the games. To assess the quality of these generated responses, we invite many volunteers to provide subjective evaluations. The experimental results demonstrate that incorporating the improved understanding ability of the agent leads to higher levels of factual correctness, naturalness, and groundedness in the generated responses, closely resembling real-human interactions. These results further underscore the significance of understanding ability in constructing proficient agents and highlight the importance of our benchmark.
%We hope our dataset and benchmark as valuable resources that will inspire the research community to delve into the understanding of complex interactions and contribute to the development of more capable AI agents.

%Finally, we present a three-step prompting baseline for constructing an agent that can react to real-human as a GM in TRPGs. The simple baseline further reveal the value of our task and dataset in improving the interactions of agents. Specifically, the prompts in our method including existing characters, their intentions, and the associated skill checks, respectively. By utilizing these prompts, we guide LLMs in gradually understanding the intricate interactions between players. We evaluate our baseline method and compare it with other prompting methods using different LLMs within the C2A framework. The experimental results highlight a strong correlation between the performance in the C2A task and the understanding capabilities of the methods. 
%Furthermore, leveraging the answers obtained from the C2A task, we employ LLMs to generate responses to simulate a real-human in the games. To assess the quality of these generated responses, we engage a panel of real-human volunteers to provide subjective evaluations. The experimental findings demonstrate that incorporating the improved understanding ability of the game agent results in higher levels of factual correctness, naturalness, and groundedness in the generated responses, closely resembling real-human interactions. These results further emphasize the significance of understanding ability in constructing proficient game agents and underscore the importance of the C2A task in advancing agents' interaction capabilities.
%We envision our dataset and benchmark as valuable resources that will inspire the research community to delve into complex interaction understanding and contribute to the development of more capable AI agents.



%Finally, we propose a three-step prompting baseline for constructing a virtual GM. This method takes into account the existing characters, their intentions, and the possible skill checks associated with them. Through these three prompts, we gradually guide LLMs to comprehend the intricate interactions between players. We evaluate our baseline and other prompting methods using various LLMs within the C2A framework, and the experimental results demonstrate a strong correlation between the performances in C2A and the understanding ability of the methods. Notably, with the adoption of multi-step prompting methods, LLMs exhibit enhanced understanding capabilities in capturing player interactions, resulting in superior performances in the C2A task.
%Furthermore, based on the answers obtained from C2A, we utilize LLMs to generate utterances for the virtual GM. To evaluate the quality of these generated responses, we engage numerous real-human players and GMs to provide subjective evaluations. The experimental results show that by considering the improved understanding ability of the game agent, the generated responses achieve higher levels of factual correctness, naturalness, and groundedness, closely resembling real-human interactions. This provides additional evidence reveals the significance of understanding ability in constructing an adept game agent and indicates the importance of the C2A task in advancing agent's interaction ability.
%We aspire for our dataset and benchmark to serve as valuable resources, inspiring the research community to explore complex semantic understanding and contribute to the development of better AI agents.





\iffalse 


Some works in philosophy and linguistics also discuss the relational between language and human intelligence.


Due to the unique and interesting form of various games, agent for games usually are required to interact with game environment and make decisions or strategies to achieve some goals in games. Many game agents provide unexpected solutions to inspire human players and can already outperform human players in many games, which show the potential of AI in applying various real scenarios. Meanwhile, more than achieving a better player for games, the game environment, where the game agents work with, also provide a valuable testbed to evaluate and explore the capacities of AI in a controllable and safe way. We can simulate and estimate the performance of AI agent in the virtual environment of games, which can guide the further application and research in the future works. 




We humans usually learn interact with others by natural language. 
As in discussion of Philosophical Investigation, language can be a kind of tool for human being to interact with environments. Language supports human to explain and deliver information to others, which formualte the fundation of communication, negotiation and coorperation of human group and prompt the functioning of society. Some works in philosophy and linguistics also discuss the relational between language and human intelligence. Meanwhile, in the exploration of artifical intelligence (AI), many works also show that with better learning of language or corresponding semantics, models' capacities can be boosted, be close to or even outperform human performances. Recent works in Large language models (LLMs) further reveal some unexpected emergent phenomenon in AI by learning large scale language data. Though significant advancements have already achieved, there are still some long-tanding problems that what the precise connections between AI and human language, and furthermore, how to purse more powerful AI by learning from human language. 

Moreover, to further explore and evaluate the ability of AI, games have been found as a valuable and controllable testbeds. We can test various ability (e.g., planning ability, react ability, perception ability, etc.), of AI by designing AI agent run in various game environments (e.g., RTS, platform jumping games, Moba, etc.). The agents in previous works usually follow the game rules and operate numerical calculation to reach particular goals. Recently, Park et al.~\cite{park2023generative} present the generative agents that can interact with freely by using natural language. Following one of a part of rule named DND from TRPGs, they produce a virtual world for agents and let them live in the environments. We can even whiteness the evolution of the behaviours of agents and find that they can spontaneously generate some actitivies like host a party. Then, Liu et al.~\cite{liu2023training} further produce a simulated society by agents using LLMs and explore the impacts of some factors. All the recent works illustrate interesting and valuable results for the exploration of designing agents that interacting each other with natural language. Considering the philosophical thought in~\cite{}, a question arises: can we further endue the natual langage as the interactive tools thoroughly, provide a more open enviroment for agents and witness further evolution of AI? 

In this paper, we aims to push the frontier of further enduing natural language as tool for interaction between AI agents. Agents in previous works are usually possess limited interactions. The limitations are mainly from two aspects, which are complexity and flexibility. Most works enable the interactions or responses of AI agent within limited length, e.g., short-contexts, brief instructions. Moreover, though some works empower more diverse interaction ability for the agents, e.g., freely moving and constructing in sandbox games like Minecraft, or natural language based interaction in generative agent, the flexibility of the interactions are still limited, e.g., limitied kinds of instructions in Minecraft, limited core rules of DND. We admire the significant advancements but expect to exploit more complex and more flexible interaction for AI agent. To this end, 
 
in our study, we find that TRPG is an valuable testbed for this exploration. 






interact with in short contexts and relative formal interactions. 



This is an interesting and valuable research that can reveal some clues intuitively. 

utilize LLMs to construct multiple virtual characters and let them interact with others freely. 

Recently, constructing AI agent has raise more and more attention. This challenge aims to construct an agent that close to the real human, which may participate in and assist people's work and life. Meanwhile, rather than applying and designing agent directly in the real environments, construct agent in the games seems to be more controllable and safe. 



open and persistent world 
nonlinear gameplay 



Constructing AI agent for games raise more and more attention recently. Due to the unique and interesting form of various games, agent for games usually are required to interact with game environment and make decisions or strategies to achieve some goals in games. Many game agents provide unexpected solutions to inspire human players and can already outperform human players in many games, which show the potential of AI in applying various real scenarios. Meanwhile, more than achieving a better player for games, the game environment, where the game agents work with, also provide a valuable testbed to evaluate and explore the capacities of AI in a controllable and safe way. We can simulate and estimate the performance of AI agent in the virtual environment of games, which can guide the further application and research in the future works. 

As mentioned above, we extract two properties for various AI agents, which are the way of interact with environments, and the setting of the game goals. Generally, most works focus on solving games by understanding and interact with the environments by specific instructions in the games. For example, the instructions in Minecraft can summarize some behaviour of character in the game system and many works use these instructions to complete various tasks in Minecraft. Recently, Benefiting from the significant advancement of large language models (LLMs),some works introduce a more intuitive interative way that close to the real human, which is interacting by natural language. Park et al.~\cite{park2023generative} utilize LLMs to construct multiple virtual characters and let them interact with others freely. Liu et al.~\cite{liu2023training} further produce a simulated society by LLMs and explore the impacts of some factors. Considering the high freedom and interactivity of natural language, these works show a better way to construct AI agent that close to the reality. Besides, in the another aspect of game goals, most works target to specific numeric based goals and guide the agent to fight with enemies or respond to the scenes, specifically target to particular games like moba, platform action games. Going beyond the dedicated AI, Park et al.~\cite{park2023generative} show another possibility that we can let the agnet to simulate real human and finish ambiguous and imprecise tasks, e.g., let agents host a party at their home. Similar to the real life, the most tasks in our daily life are also hard to be numeric and formalize to specific goals. For example, we may walk around the city just for fun. Carrying gifts for friends without any purposiveness. The open and diverse interactions by natual language and the uncertain and difficult to quantify goals may lead the designation of AI agent to the next stage and further close to our real life. 

However, finding a proper game or other environment for AI agent to explore the open interaction by natural language and unquantified tasks is still an open question. Generative agent~\cite{park2023generative} leverage the rule of DND to construct the virtual world, which is one of the rules in TRPGs. Though using very limited content in the game, they already expose the potiential of constructing vivid agent that close to real human by using TRPG data. Different from other games, Tabletop role-playing games (TRPGs)~\cite{gygax1974dungeons,chung2013table,daniau2016transformative,zagal2018definitions} contain diverse and unique rules, high player engagement, and the flexibility of interactions facilitated through grounded natural language~\cite{cover2014creation,daniau2016transformative,gandalf}. TRPGs offer a distinctive and dynamic form of entertainment by combining storytelling, strategic elements, and cooperative gameplay, etc. In these games, players act the roles of characters within intricately crafted and fantastical worlds, guided by a game master (GM) who oversees the game and narrates the unfolding story. All these leads TRPGs can be a valuable testbed for exploring AI agent. First, all players using grounded natual language to interact with other players, NPCs, and objects in the games. For example, players can only choose limited interctions with objects in Minecraft, e.g., pick up or using as a tool. Comparably, in TRPGs, the players can operate whatever interaction as they want. For example, a bard can even using the sward to perform a dance and encourage other teammate. The far more flexibility in TRPG induce it show more complexity and diversity that can simulate the real life, which is complete different from other games. Moreover, the main goals of TRPGs are usually finish an advanture. This indicates that there are numerous methods for players to overcome difficulties and complete the final goals. For example, in Moba game, the players must fight with enemies and destroy the enemy crystal. However, in TRPG, if encouter a enenmy, the players can persude, deception, enchant or just escape from the enemy, rather than only fight with the enemy. The high openness and freedom in TRPGs leads the goals ambiguous and uncertain. Every players can have their own solutions to the same problem and push the progress of the games. In all, the properties of TRPGs provide a higher requirements for game agents with more flexibility, complexity and diversity that close to the real life. 








The immense player base is exemplified by statistics~\cite{player_number} from 2020, revealing over 50 million players participating in the DND rule alone, which represents just one of the numerous rules in TRPGs. TRPGs offer a distinctive and dynamic form of entertainment by combining storytelling, strategic elements, and cooperative gameplay, etc. In these games, players act the roles of characters within intricately crafted and fantastical worlds, guided by a game master (GM) who oversees the game and narrates the unfolding story. The GM, akin to a host, assumes the primary responsibility of providing information to players (such as game settings, non-player character movements, and background stories) and adjudicating check items when players wish to undertake specific actions.



In this paper, we go further with 


To understand and support the real human behaviour, the agents 

Exploration of game agent provide a valuable testbed for 

Many works are proposed to design various agent for diverse tasks. 

The understanding ability of language models has dramatically improvements in recent years. Due to the significant advance in large languge models (LLMs), machines are already able to understand complex paragraphs and dialogues, summarize the contexts, and continue to write consequent contents. Many works show the powerful ability in various domains. Among them, generating AI agent is one of the hottest domain. Park et al.~\cite{park2023generative} utilize LLMs to construct multiple virtual characters and let them interact with others freely. Liu et al.~\cite{liu2023training} further produce a simulated society by LLMs and explore the impacts of some factors. Furthermore, Wang et al.~\cite{wang2023voyager} propose a lifelong learning agent based on LLMs in Minecrafft, which can continuously interact with the game environment, achieve skills and discover the world in game by itself. All these works present that there are large spaces to explore in AI agents with LLMs. The powerful LLMs can support the designation and implementation of many complex and vivid AI agent that assist human's life. Meanwhile, the applications of agent in reality also set higher requirements for the reliability and dependability of LLMs, which requires the languge models to avoid existed problems like misunderstanding human's intentions or generating hallucinations. This further requires LLMs to fully understand the human's intentions and react as a real human. 

However, previous works only reflect the understanding ability in particular domains, e.g., solving logic problems, rephrasing paragraphs, etc. The community still lack of the evaluating benchmark or language environment close to the complex interactions in reality. There are two deficiencies: 1. Lack of groundedness. Far from the real life senarios, many datasets and benchmarks are focus on formal materials 

It is a common situation that the human being work with multiple people and produce complex interactions with others using grounded language.  


However, the reliablity and dependablity of AI agent based on LLMs are still limited by some problems of LLMs, e.g.,  Toward better AI agent, we should ensure that LLMs are fully understand human's intentions and operate the correct instruction to support various requirements in the reality. To this end, 


Quantifying the understanding ability of large language models (LLMs) is a challenging problem. Though LLMs achieve dramatic progress and boost the performances of natual language processing (NLP) in many aspects, there are still large space to improve to close to a real human, since LLMs may misunderstand human's intentions or generate hallucinations. Many works have been conducted to explore the problems of LLMs and evaluate them in various aspects, e.g., solving logical problems, 

In recent years, the significant advance in large language models (LLMs) boost the performances of natual language processing (NLP) in many aspects. 

The machines are more powerful to understanding semantics and generate vivid responses. Though LLMs achieve dramatic progress, there are still large space to improve to close to a real human, since LLMs may misunderstand human's intentions or generate hallucinations. Many works have been conducted to discuss and overcome these problems. 

For example, LLMs may misunderstand semantics and human's intentions, generate unreliable information, or produce hallucinations, e.g., mistakes in factual knowledge or commonsense. These have been discussed in many works and many works are proposed to overcome these problems. More than solving the problems, another important but challenging line is how to evaluate LLMs. 

hinder the further application of LLMs in reality and have been explored in many works. 

Though at most time, these mistakes are tolerable and LLMs can support human being in various aspects, the slightly and possibly errors produced by LLMs always reduce the dependability of users, which hinders the further application in reality. 















Tabletop role-playing games (TRPGs)~\cite{gygax1974dungeons,chung2013table,daniau2016transformative,zagal2018definitions} have gained enduring popularity due to their unique rules, high player engagement, and the flexibility of interactions facilitated through grounded natural language~\cite{cover2014creation,daniau2016transformative,gandalf}. The immense player base is exemplified by statistics~\cite{player_number} from 2020, revealing over 50 million players participating in the DND rule alone, which represents just one of the numerous rules in TRPGs. TRPGs offer a distinctive and dynamic form of entertainment by combining storytelling, strategic elements, and cooperative gameplay, etc. In these games, players act the roles of characters within intricately crafted and fantastical worlds, guided by a game master (GM) who oversees the game and narrates the unfolding story. The GM, akin to a host, assumes the primary responsibility of providing information to players (such as game settings, non-player character movements, and background stories) and adjudicating check items when players wish to undertake specific actions. As an essential part of the games, the GM supports and guides the players, leading them through specific adventures outlined in the game models. GMs should possess a deep understanding of the game rules, comprehend players' intentions, and be adept at providing vivid descriptions. They play a pivotal role in the allure of TRPGs, as experienced GMs are capable of eliciting responses from players and ensuring the smooth and captivating progression of the game.

%Tabletop role-playing game (TRPG) is an enduring game, due to the high popularity, unique rules and its flexible interactions using grounded natural language. As a statistic~\cite{player_number} in 2020, there are over 50 million players in DND rule, which is just one of the numerous rules in TRPG. TRPG blends storytelling, strategy, and cooperative gameplay, offering a unique and dynamic form of entertainment. In the games, players assume the roles of characters within a richly detailed and fantastical world, guided by a game master (GM) who oversees the game and narrates the unfolding story. Then, GM, similar to the host of the games, mainly responsible to describe information for players (e.g., game settings, movements of non-player characters, background stories, etc.) and judge the check items for players if they want to operate some actions. As an essential part of the games, GM supports and guides the players to complete particular adventures following the scripts in the game models. GMs should be familiar with the game rules, considerate to players, and experienced in guiding players by the vivid descriptions. Generally, GMs are an important reason why TRPG is so charming. Experienced GMs can fully mobilize players' emotions and lead the game to proceed smoothly and interestingly. 

Moreover, with the advancements in artificial intelligence (AI), there has been increased attention on designing AI agents~\cite{chen2023next,perez2019general,iovino2022survey} for games. This development not only enhances the player experience but also pushes the boundaries of game design~\cite{guzdial2019friend,perez2019general,ye2020towards,ye2020mastering} while offering constructive solutions for real-world problems~\cite{park2023generative,grossmann2023ai,bail2023can,liu2023training}. Given the tremendous enthusiasm among a large number of players in TRPGs, many developers and researchers are striving to create improved game agents, e.g., dice robots, character builders, etc. Among these agents, constructing a virtual GM stands out as a particularly challenging and crucial area.
The introduction of a virtual GM allows players to conveniently engage in TRPGs anytime and anywhere, eliminating the need to find an experienced GM to host the games. Numerous works have been proposed to explore the generation of virtual GMs and have achieved notable advancements. These works can be broadly classified into two categories: 1) Rephrasing or simulating GM responses by directly learning from real-human GMs' interactions; and 2) Utilizing the latest turn or simple descriptions of players to generate guidance or commands.
Due to the improvements in Large Language Models (LLMs), both of these directions have witnessed significant progress. AI agents are now capable of generating vivid responses based on simple commands or comprehending the underlying semantics conveyed in game turns.

%Besides, benefiting from the developments of artificial intelligence (AI), designing AI agents for games has also raise more attention, which enhancing player experience, pushing the boundaries of game design and also offering constructive experiences for problems in the reality. Consider the enthusiasm of a large number of players in TRPG, many developers and researchers also attempt to create better game agents, e.g., dice robot, character builder, etc. Among the agents, constructing virtual GM is most challenging and important domain. With a virtual DM, players can conveniently play at anywhere and anytime without finding an experienced GM to host the games. Specifically, Many works are proposed to explore the virtual GM generation and achieve significant improvements. The previous works can be divided into two parts: 1. Rephrase or simulate GM's response by directly learning real-human GM's response. 2. Taking latest turn or simple descriptions to generate guidance or command. Benefit from the improvements of LLMs, both directions have dramatic improvements. The agent enable to rewrite vivid response from simple command or understand simple semantics from game turns. 

However, the existing approaches fall short of approximating the capabilities of a real-human GM, primarily due to their limited understanding of complex and grounded semantics in real-time communication. Within TRPGs, players engage in multifaceted interactions (e.g., with multiple characters and NPCs, with various props and scenarios, etc.) that are challenging to capture within a single game turn or encapsulate in simplistic descriptions. While LLMs demonstrate impressive semantic comprehension~\cite{huang2022towards,zhao2023survey}, the current explorations in virtual GMs and the understanding of game logs have yet to fully consider the complexities inherent in real-time communication, including more intricate and grounded contexts. 
This insufficiency primarily stems from the lack of a specific task and dataset. Many existing works rely on data obtained from play-by-post forums~\cite{martin2018dungeons,callison-burch-etal-2022-dungeons}, where players participate in the game by writing their responses and posting them on the forum. However, this asynchronous online communication often results in extended waiting periods for players to receive responses, sometimes spanning hours or even weeks. Consequently, the data collected from such forums struggle to accurately capture the vivid and complex grounded semantics present in real-time human interactions. Moreover, the nature of forum-based communication tends to encourage players to respond to the nearest turn and provide formal written replies, thereby restricting the richness and flexibility of expressions.
Given the reliance on forum-based communication in the source data, previous methods typically utilize inputs that focus on the latest turn or provide simple descriptions of actions. Unfortunately, these inputs fail to encompass the intricate semantics that unfold between players and the GM, as exemplified in Fig.~\ref{fig:intro}. While effective for handling simple and limited game turns, such approaches prove insufficient for addressing the complexity and groundedness of interactions that closely resemble real-time communications. Additionally, works such as G4C consider the intentions of GMs and merge these intentions with contexts to generate guidance. However, this approach still relies on the semantics directly expressed by the GMs in the games, rather than placing full reliance on a comprehensive understanding of players' interactions within preceding contexts, which aligns more closely with real-life situations.

%However, it is far from enough to close to a real-human GM, due to the insufficiency in the understanding of complex and grounded semantic close to the real-time communication. In the games, players usually have complex interactions between each other, which are hard to be included in one game turns or summarized in some simple descriptions. Though LLMs possess dramatic understanding ability for semantics, current explorations in virtual GM or understanding of game logs have not considered more complex and grounded contexts within real-time communications. 
%Moreover, the above insufficiency is mainly because the absence of particular task and dataset. Most works use data from play-by-post form, in which players play the game by writing their responses and posting to the forums. This kind of asynchronous online communication causes that the player have to wait response from others after several hours even weeks. This leads the data are hard to reflect vivid and complex grounded semantics as real-time human interactions. Meanwhile, due to the forum-based communication, most players tend to response the closest turn and provide formal written replies, which also limited the groundness and flexibility of expressions. Considering the forum-based communication in source data, previous methods usually utilize latest turn or simple descriptions of actions as inputs, which do not contains complex semantics between players and GM, as shown in Tab.~\ref{tab.data_eg1}. Though effective in the simple and limited game turns, These are insufficient to handle more complex and more grounded interactions close to the real-time communications. Besides, works like G4C consider the intention of GM specifically and merge intentions and contexts to produce better guidance. However, this depends on semantics directly expressed by the GMs in the games, rather than based the understanding of players' interactions in previous contexts, which is more close to the real-life situations. 

% Figure environment removed

To address the aforementioned limitations, we propose the Character and Skill check Answering (CSA) task, which represents a crucial step towards enhancing semantic understanding for virtual GMs. Within this task, the agent is presented with long and intricate contexts, featuring grounded language extracted from game logs in real-time communication. The agent's objective is to determine which characters are intending to act and which specific skills need to be checked, in accordance with the game rules. This task establishes a higher level of requirements specifically tailored to semantic understanding.
In detail, following the Theory of Mind~\cite{tom1,tom2,tom3}, CSA diverges from the conventional approach of directly generating utterances based on given contexts. Instead, it decouples the semantic understanding from the response generation and places particular emphasis on exploring the understanding phase. By explicitly requiring the agent to deduce the intentions of characters and their corresponding skills, CSA provides a dedicated testbed to evaluate the agent's understanding ability. Moreover, the improvements in understanding ability have a direct impact on enhancing the quality of generated responses by virtual GMs, promoting more effective communication between agents and real-human players.
We conduct a comparative analysis of the generated utterances produced by LLMs~\cite{brown2020language,zhao2023survey,huang2022towards}, considering both instances with and without incorporating answers derived from CSA. Through subjective evaluations conducted by volunteers, we observe a significant improvement in the performance of generated responses when CSA is integrated. Specifically, the evaluations reveal a 75\% improvement in factual correctness, along with 10\% improvements in terms of groundness and naturalness. These results further emphasize the value of CSA in constructing superior AI agents.

%To address the aforementioned limitations, we propose the Character and Skill check Answering (CSA) task, which represents a crucial step towards enhancing semantic understanding for virtual GMs. Within this task, the agent is presented with long and intricate contexts, featuring grounded language extracted from game logs in real-time communication. The agent's objective is to determine which characters are intending to act and which specific skills need to be checked, in accordance with the game rules. This task establishes a higher level of requirements specifically tailored to semantic understanding.
%In detail, drawing inspiration from the Theory of Mind~\cite{tom1,tom2,tom3}, rather than directly generating utterances based on the given contexts, CSA focuses on the understanding and decision-making processes. This can encourage the agent to think before speak, rather than solely produce utterances lack of factual correctness. CSA explicitly requires to infer the intentions of characters and skills, thereby providing a dedicated testbed for understanding ability. Meanwhile, the improvements for understanding ability can further improve the quality of generated responses by virtual GMs and facilitate better communication between agents and real-human players.
%We conduct a comparative analysis of the generated utterances produced by LLMs, both with and without considering answers derived from CSA. Through subjective evaluations conducted by volunteers, we observe a significant improvement in the performance of generated responses when CSA is incorporated. Specifically, the evaluations reveal a 75\% improvement in factual correctness, as well as 10\% improvements in terms of groundness and naturalness. These results further underscore the value of CSA in constructing superior agents.

%To overcome above defeats, we present the task of Character and Skill check Answering (CSA), which can be a critical step toward better semantic understanding for virtual GM. Given the long and complex contexts with grounded language in game logs, the agent must answer which characters are intent to act and what kinds of skill should be checked for the intentions corresponding to the game rules. This formulates the higher requirements specifically for semantic understanding. In detail, rather than directly generate utterances by given contexts, CSA particularly decouple the understanding and decision making part and provide a testbed for these. As mentioned in Theory of Mind~\cite{tom1,tom2,tom3}, by explicitly inferring the intentions of characters, CSA can improve the performances of generated responses of virtual GM, which leads a better communications between agents and the real-human players. Moreover, we perform a comparison of generated utterances by LLMs between considering answers in CSA and without answers in CSA. By subjective evaluation from volunteers, generated responses considering answers in CSA show significantly higher performances, which are 75\%, 10\% and 10\% improvements in factual correctness, groundness and naturalness respectively. This reveal the value of CSA in construct better agent empirically. 

Specifically, we collect a new dataset CSA, called the Long-context Grounded-language TRPG Log (LGL) dataset. Instead of relying on data collected from play-by-post forums, we leverage data sourced from a Chinese TRPG forum\footnote{www.goddessfantasy.net}. The forum records are typically compiled by the GM after the ending of games, comprising voice recordings or real-time chat logs. This distinct data source overcomes the limitations of play-by-post data collection, enabling us to extract long contexts with complex semantics from the game logs. Since these logs are derived from immediate communications, the interactions exhibit the higher groundedness and feature more vibrant responses akin to everyday conversations, as exemplified in Tab.~\ref{tab.data_eg1}.
Moreover, our dataset encompasses not only Dungeons and Dragons (DND) rules~\cite{gygax1974dungeons} but also a wide range of diverse game rules such as COC~\cite{lovecraft2016call}, PF2~\cite{bulmahn2010pathfinder}, SW~\cite{hensley2008savage}, etc. This diversity further enriches the complexity and variety of our dataset. Building upon this dataset, we construct the CSA task, consisting of 1,003 sections of contexts extracted from the game logs. Each section encompasses a complete adventure and multiple turns, exhibiting intricate semantics. As in Tab.~\ref{tab.dataset_sta}, CSA comprises an average of 32.12 turns per excerpted context, while previous works typically contain only one turn. In CSA, the agent is required to infer the appropriate check items based on the given long contexts, which are subsequently judged and guided by the GM. The predictions should comprise a list of tuples, with each tuple containing a character name and the corresponding skill name. The number of answers for characters and skills can vary from one to eleven, depending on the context. Additionally, since the answers must adhere to specific rules, contexts associated with different game rules necessitate different skill categories. For instance, considering the DND rule, there are 51 possible skills for the answers. These factors collectively contribute to CSA presenting a challenging task for AI agents in TRPG. The agent must fully comprehend both the complex contexts and the game rules, akin to a real-human GM.
For evaluation purposes, we first compute precision and recall separately for the predicted characters and overall answers. To provide a comprehensive evaluation, we report the F-score as the final metric, taking into account both precision and recall. Evaluating for the characters reflects the understanding accuracy of the methods in inferring players' intentions. Simultaneously, evaluating overall answers provides insights into the understanding ability concerning both character intentions and the rules of the games.

%Specifically, we collect a new dataset for this task, named Long-context Grounded-language TRPG Log (LGL) dataset. Rather than collecting data from play-by-post forums, we leverage the data from another Chinese TRPG forum (www.goddessfantasy.net). The records from this forum are typically compiled by the GM after the game ends, either through voice recordings or real-time chat logs. This makes the source data in our work overcome the defeats of play-by-post. We can excerpt long contexts with complex semantics from the game logs. Since the game logs are from immediate communications, the interactions are more grounded and contain more vivid responses closed to the daily communications as show in Tab.~\ref{tab.data_eg1}. Additionally, more than DND rules, our dataset also contains more diverse game rules like COC, PF2, SW, etc. This further enrich the diversity and complexity of our dataset. Based on this dataset, we construct the CSA task. We excerpt and label 1,003 sections of contexts from the game logs. All sections contains a complete adventure and multiple turns with complex semantics. As in Tab.~\ref{tab.dataset_sta}, CSA contains 32.12 turns in every excerpted context and previous works only contain one turn. CSA requires to infer what check items should be judged and guided by GM according the given long contexts. The predictions should be a list of multiple tuples and every turple contains a character name and corresponding skill name. Corresponding to the contexts, the number of answer for characters and skills are variable from one to eleven. Besides, since the answers should follow particular rules, contexts from different rules are required different categories of skills. Taking DND rule as an example, the number of possible skills for the answers is 51. All this factors induce CSA a challenge task for AI agent in TRPG. The agent has to fully understanding both complex contexts and games rules like real-human GM. For evaluation, we separately calculate the precision and recall of the predicted characters and overall answers. The metrics for characters can better reflect the understanding accuracy of methods for inferring the players' intentions. Meanwhile, the evaluation for overall answers further reflects the understanding ability for both character's intention and rules of the games. 

%Finally, we propose a three-step prompting baseline to construct a virtual GM. The proposed method specifically consider the existed characters, the intentions of characters, and possible skill check for charcters respectively. The three-step prompting gradually lead LLMs to comprehend complex interactions between players. We evaluate our baseline and other prompting methods using different LLMs within the CSA framework. The experimental results reveal a strong correlation between the performances in CSA and the understanding ability of the methods. With improved prompting methods, LLMs demonstrate enhanced understanding capabilities in capturing player interactions, leading to superior performances in the CSA task.
%Moreover, based on the answers obtained from CSA, we employ LLMs to generate utterances for the virtual GM. To evaluate the quality of these generated responses, we enlist the participation of numerous real-human players and GMs who perform subjective evaluations. The experimental results demonstrate that considering the improved understanding ability of the game agent, the generated responses exhibit higher levels of factual correctness, naturalness, and groundedness, closely resembling real-human interactions. This provides further evidence of the importance of understanding ability in constructing a proficient game agent and underscores the significance of the CSA task in advancing TRPG virtual GMs.
%We hope that our dataset and benchmark can serve as valuable resources, inspiring the research community to explore complex semantic understanding and facilitate the development of more sophisticated AI agents for games.

Finally, we propose a three-step prompting baseline for constructing a virtual GM. This method takes into account the existing characters, their intentions, and the possible skill checks associated with them. Through these three prompts, we gradually guide LLMs to comprehend the intricate interactions between players. We evaluate our baseline and other prompting methods using various LLMs within the CSA framework, and the experimental results demonstrate a strong correlation between the performances in CSA and the understanding ability of the methods. Notably, with the adoption of multi-step prompting methods, LLMs exhibit enhanced understanding capabilities in capturing player interactions, resulting in superior performances in the CSA task.
Furthermore, based on the answers obtained from CSA, we utilize LLMs to generate utterances for the virtual GM. To evaluate the quality of these generated responses, we engage numerous real-human players and GMs to provide subjective evaluations. The experimental results show that by considering the improved understanding ability of the game agent, the generated responses achieve higher levels of factual correctness, naturalness, and groundedness, closely resembling real-human interactions. This provides additional evidence reveals the significance of understanding ability in constructing an adept game agent and indicates the importance of the CSA task in advancing TRPG virtual GMs.
We aspire for our dataset and benchmark to serve as valuable resources, inspiring the research community to explore complex semantic understanding and contribute to the development of better AI agents for games.

%Finally, we further propose a prompting baseline to construct a virtual GM. Utilizing Large Language Models (LLMs), we evaluate our baseline and other prompting methods with different LLMs in CSA. The experimental results reveal that the relevance of performances in CSA and understanding ability of methods. With better prompting methods, LLMs show higher understanding ability for the interactions of players and achieve better performances in CSA task. Based on the answers of CSA, we further use LLMs to generate utterances of virtual GM. We invite many real-human players and GMs to perform subjective evaluation for the generated utterances. Experimental results show that with the consideration of better understanding ability of game agent, the generated responses show higher factually correctness, naturalness, and groundness close to the real human. This further prove the importance of the understanding ability in construct a game agent and the significance of CSA in TRPG virtual GM. We wish the dataset and benchmark can provide further inspiration for the community in exploring complex semantic understanding and better AI agent for games. 


Our contributions can be summarized as follows:

1. We introduce Character and Skill check Answering (CSA) task, specifically designed to address the challenges of grounded and complex semantic understanding in game logs. This task serves as a valuable testbed for assessing the understanding abilities of game agents.

2. We create a new dataset, Long-context Grounded-language TRPG Log (LGL), to address the limitations in exploring long contexts and complex grounded interactions in real-time communications. This dataset fills a gap in the current research by providing a comprehensive resource for analyzing such interactions.

3. We propose a prompting baseline and evaluate various prompting methods using different Large Language Models (LLMs) within CSA framework. The experimental results demonstrate a strong correlation between the understanding abilities of the methods and the performances achieved in CSA. This highlights the increased requirements presented by CSA task.

4. We conduct subjective evaluations based on the answers obtained from CSA. These evaluations show that better performances in CSA lead to higher levels of factual correctness, naturalness, and groundedness in the generated responses, which are crucial factors for creating a vivid virtual GM. These results further underscore the significance of our dataset and task in improving AI agents for TRPG games.



%Our main contributions are summarized as follows: 
%1. We construct a new task targeted for grounded and complex semantic understanding in game logs, named Character and skill check Answering (CSA), which formulate a valuable testbed for the understanding ability of game agents. 
%2. We present a new dataset for TRPG game logs, named Long-context Grounded-language TRPG Log (LGL). This compensates the ignorance of exploration of long context and complex grounded interactions from the real-time communications. 
%3. We demonstrate a prompting baseline and experiment some prompting methods with different LLMs based on CSA. The experimental results show the relevance between understanding ability of methods and the performances of CSA, which also reveal the higher requirements presented by CSA. Besides, based on answers of CSA, we provide subjective evaluations, which prove that the better performances on CSA lead to higher factually correctness, naturalness, and groundness of generated responses, which are important factors for a vivid virtual GM. All results prove the significance of our dataset and task in improve better AI agent for TRPG game. 



\fi 















\iffalse 

However, there are still large gap between the virtual GM and games with real-human participation. This is because by the absence of the understanding of complex and grounded semantic close to the real-time communication. 




The game master sets the stage, describing the setting, non-player characters, and challenges that the players will face. Players engage in grounded communication to describe their characters' actions, engage in dialogue, and interact with the game master (GM) and other players. 





As a critical step toward build a virtual GM with understanding ability, we present the task of Character and Skill check Answering (CSA). 
This paper introduce a 


However, previous works are insufficient in exploring complex and grounded semantics. 


, caused by the limitation of forum-based data collections. Most works use data from play-by-post form, in which players play the game by writing their responses and posting to the forums. This kind of asynchronous online communication causes that the player have to wait response from others after several hours even weeks. This leads the data are hard to reflect vivid and complex grounded semantics as real-time human interactions. Meanwhile, due to the forum-based communication, most players tend to response the closest turn and provide formal written replies, which also limited the groundness and flexibility of expressions. 

Moreover, more than describing some scenarios and background story predefined in game models, the most important duty for virtual GM is provide guidance for the players to operate skill checks. This requires GM to fully understand the semantic of players' interactions, understand the intentions of players, understand the rules of games, and provide vivid and reasonable guiding utterances. To this end, 


considering the forum-based communication in source data, previous methods usually utilize latest turn or simple descriptions of actions as inputs, which do not contains complex semantics between players and GM, as shown in Tab.~\ref{}. Though effective in the simple and limited game turns, These are insufficient to handle more complex and more grounded interactions close to the real-time communications. Besides, works like G4C consider the intention of GM specifically and merge intentions and contexts to produce better guidance. However, this depends on semantics directly expressed by the GMs in the games, rather than based the understanding of players' interactions in previous contexts, which is more close to the real-life situations. 

To overcome above defeats, we collect a new dataset, named Long-context Grounded-language TRPG Log (LGL) dataset. Rather than collecting data from play-by-post forums, we leverage the data from another Chinese TRPG forum (www.goddessfantasy.net). The records from this forum are typically compiled by the GM after the game ends, either through voice recordings or real-time chat logs. This makes the source data in our work overcome the defeats of play-by-post. We can excerpt long contexts with complex semantics from the game logs. Since the game logs are from immediate communications, the interactions are more grounded and contain more vivid responses closed to the daily communications as show in Tab.~\ref{}. Additionally, more than DND rules, our dataset also contains more diverse game rules like COC, PF2, SW, etc. This further enrich the diversity and complexity of our dataset. 

Moreover, to push frontier of the understanding for complex interactions and generation of game agent, we formalize a new task targeting at the understanding of complex game logs based on LGL, named Character and Skill check Answering (CSA). Specifically, we excerpt and label 1,003 sections of contexts from the game logs. All sections contains a complete adventure and multiple turns with complex semantics. As in Tab.~\ref{tab.dataset_sta}, CSA contains 32.12 turns in every excerpted context and previous works only contain one turn.
CSA requires to infer what check items should be judged and guided by GM according the given long contexts. The predictions should be a list of multiple tuples and every turple contains a character name and corresponding skill name. Corresponding to the contexts, the number of answer for characters and skills are variable from one to eleven. Besides, since the answers should follow particular rules, contexts from different rules are required different categories of skills. Taking DND rule as an example, the number of possible skills for the answers is 51. All this factors induce CSA a challenge task for AI agent in TRPG. The agent has to fully understanding both complex contexts and games rules like real-human GM. For evaluation, we separately calculate the precision and recall of the predicted characters and overall answers. The metrics for characters can better reflect the understanding accuracy of methods for inferring the players' intentions. Meanwhile, the evaluation for overall answers further reflects the understanding ability for both character's intention and rules of the games. We wish CSA can encourage the improvements in agent's factually correctness and exploration of generating more vivid agent responses. 

Finally, we further propose a prompting baseline to construct a virtual GM. Utilizing Large Language Models (LLMs), we evaluate our baseline in CSA. The experimental results reveal that the relevance of performances in CSA and understanding ability of methods. With better prompting methods, LLMs show higher understanding ability for the interactions of players and achieve better performances in CSA task. Based on the answers of CSA, we further use LLMs to generate utterances of virtual GM. We invite many real-human players and GMs to perform subjective evaluation for the generated utterances. Experimental results show that with the consideration of better understanding ability of game agent, the generated responses show higher factually correctness, naturalness, and groundness close to the real human. This further prove the importance of the understanding ability in construct a game agent and the significance of CSA in TRPG virtual GM. We wish the dataset and benchmark can provide further inspiration for the community in exploring complex semantic understanding and better AI agent for games. 

Our main contributions are summarized as follows: 

1. We present a new dataset for TRPG game logs, named Long-context Grounded-language TRPG Log (LGL). This compensates the ignorance of exploration of long context and complex grounded interactions from the real-time communications. 

2. We construct a new task targeted for grounded and complex semantic understanding in game logs, named Character and skill check Answering (CSA), which formulate a valuable testbed for the understanding ability of game agents. This also aims to encourage further development of factually correctness of virtual GM. 

3. We demonstrate a prompting baseline and experiment some prompting methods with different LLMs based on CSA. The experimental results show the relevance between understanding ability of methods and the performances of CSA, which also reveal the higher requirements presented by CSA. Besides, based on answers of CSA, we provide subjective evaluations, which prove that the better performances on CSA lead to higher factually correctness. Meanwhile, the higher factually correctness also leads to higher natualness and groundness of responses, which are important factors for a vivid virtual GM. All results prove the signficance of our dataset and task in improve better virtual GM for TRPG game. 























 

benchmark focus on the both understanding of semantics and the responses generation of virtual DM. Since GMs need to understand semantics and intention of players and then provide responses, we first disentangle the decision making and description of utterance as in Theory of Mind~\cite{}. Taking advantage of LGL, we introduce a new benchmark named think before speak (TBS), which involved a two-step generation: Generation of game Check and Generation of GM Utterance. Specifically, we first excerpt long and complex contexts from LGL and then label the check items according game rules and the guidance of GM in the logs. 
In generation of game check, we focus on understanding of the contexts and infer what check items should be judged and guided by GM. This aims to formulate a foundation for the generated responses that the responses should be produced based on the accurate understanding for the complex interactions and semantics of players. Then, for generation of GM utterance, we provide the check items and contexts to the language models, and encourage the methods to generate utterance that the GM may given for the players. This part targets to explore how to generate a more vivid game agent to guide the game process. During evaluation, we separately evaluate two parts. we calculate precision and recall of the predicted character and corresponding skills generated in the first step, respectively. Then, for the final utterance, we invite many real-human players and GMs to perform subjective evaluation for the generated utterances. Experimental results show the effectiveness our method in understanding complex interactions and generating vivid responses. We wish the dataset and benchmark can provide further inspiration for the community in exploring complex semantic understanding and better AI agent for games. 

Our main contributions are summarized as follows: 

1. We present a new dataset for TRPG game logs, named Long-context Grounded-language TRPG Log (LGL). This compensates the ignorance of exploration of long context and complex grounded interactions from the real-time communications. 

2. We construct a new benchmark for AI agent in TRPG games, named think before speak (TBS), which decouple the decision making and description of the game agent by introducing two-step generation, named Generation of game Check and Generation of GM Utterance. 

3. We demonstrate the effectiveness of our benchmark in understanding complex semantics and generating vivid responses.  



they depends on the game logs collected form the play-by-post forum. Though 

These interactions often involves a complex blend of natural language processing elements, including semantic understanding, grounded language understanding, reaction generation. By given the interactions, many works are proposed to produce AI agent that simulate GM's responses to players. 

Generally, previous works utilize game logs collected by the play-by-post forum. 


Though challenging, significant advancements are achieved in previous works and applications. 



Toward better game agents in TRPG, many works are proposed to generate utterance to simulate real-human GM. Generally, they apply language models~\cite{}, taking some descriptions or contexts in the latest turn as inputs, and produce the utterances that similar to real-human GM like guidance, responses, descriptions, etc. All the works show significant performances for simulating GM's responses and offers various interesting applications to serve the game players. 

However, previous explorations are not sufficient in understanding grounded language interactions in the games. In detail, the deficiency can be reflected in two aspects. 
1). Forum-based data collections and turn-based communication hinder the flexibility and grounded language expressions in contexts. Most works collect data sourced from the forum-based communication of players, named as play-by-post. In this form, players play the game by writing their responses and posting to the forums. This kind of asynchronous online communication causes that the player have to wait response from others after several hours even weeks. This leads the data are hard to reflect vivid and complex grounded semantics as real-time human interactions. 
2). End-to-end utterance generation neglects the deeper understanding for complex semantics. Due to the absence of long-context and complex interactions in previous corpus, the exploration in understanding complex semantics, e.g., multiple character interactions, various intentions, imagination for the scenarios, etc., are not sufficient. Previous works only utilize latest turn or simple descriptions of actions as inputs. The input contexts do not contains complex communications and discussions between players and GM, as shown in Tab.~\ref{}. Meanwhile, due to the forum-based communication, most players tend to response the closest turn, which also limited the complexity of semantics in previous works. Though the end-to-end generation is effective in previous simple contexts, it is still insufficient to handle more complex and more grounded interactions close to the real-time communications. Besides, works like G4C consider the intention of GM specifically and merge intentions and contexts to produce better guidance. However, this depends on semantics directly expressed by the GMs in the games, rather than focus the understanding of players, which is more close to the real-life situations. 


































Moreover, previous works usually operate end-to-end generation of DM utterances. Without the deeper understanding of semantics, it is hard for LLMs to generate vivid utterances as real-human DM. 

1). Previous corpus lack of grounded interactions under the real-time communications. Most works collect data sourced from the forum-based communication of players, named as play-by-post. In this form, players play the game by writing their responses and posting to the forums. This kind of asynchronous online communication causes that the player have to wait response from others after several hours even weeks. This leads the data are hard to reflect vivid and complex grounded semantics as real-time human interactions. 
2). More than generation, investigation for understanding long contexts and complex semantics is not enough. Previous works only utilize latest turn or simple descriptions of actions as inputs. The input contexts do not contains complex communications and discussions between players and GM, as shown in Tab.~\ref{}. Meanwhile, due to the forum-based communication, most players tend to response the closest turn, which also limited the complexity of semantics in previous works. Moreover, previous works usually operate end-to-end generation of DM utterances. Without the deeper understanding of semantics, it is hard for LLMs to generate vivid utterances as real-human DM. 







The diverse and complex interactions in the game with grounded language engage more and more attention in the explorations of enhancing the capabilities in real-world applications. For example, AI dungeon, a virtual DM that provide responses, descriptions, and guidance for real-human players, has already been online and commercially available for many years. 


The AI agent for games is a long-standing and valuable problem in academic and applications. 

Tabletop role-playing game (TRPG) is a popular game form in world wide. As a statistic~\cite{} in 2020, there are over 50 million players in DND rule, which is just one of the numerous rules in TRPG. TRPG blends storytelling, strategy, and cooperative gameplay, offering a unique and dynamic form of entertainment. In the games, players assume the roles of characters within a richly detailed and fantastical world, guided by a game master (GM) who oversees the game and narrates the unfolding story. The game master sets the stage, describing the setting, non-player characters, and challenges that the players will face. Players engage in grounded communication to describe their characters' actions, engage in dialogue, and interact with the game master (GM) and other players. This communication often involves a complex blend of natural language processing elements, including semantic understanding, grounded language understanding, reaction generation. The diverse and complex interactions in the game with grounded language engage more and more attention in the explorations of enhancing the capabilities in real-world applications. For example, AI dungeon, a virtual DM that provide responses, descriptions, and guidance for real-human players, has already been online and commercially available for many years. 

Toward better game agents in TRPG, many works are proposed to generate utterance to simulate real-human GM. Generally, they apply language models~\cite{}, taking some descriptions or contexts in the latest turn as inputs, and produce the utterances that similar to real-human GM like guidance, responses, descriptions, etc. All the works show significant performances for simulating GM's responses and offers various interesting applications to serve the game players. However, previous explorations are not sufficient in understanding grounded language interactions in the games. In detail, the deficiency can be reflected in two aspects. 
1). Previous corpus lack of grounded interactions under the real-time communications. Most works collect data sourced from the forum-based communication of players, named as play-by-post. In this form, players play the game by writing their responses and posting to the forums. This kind of asynchronous online communication causes that the player have to wait response from others after several hours even weeks. This leads the data are hard to reflect vivid and complex grounded semantics as real-time human interactions. 
2). More than generation, investigation for understanding long contexts and complex semantics is not enough. Previous works only utilize latest turn or simple descriptions of actions as inputs. The input contexts do not contains complex communications and discussions between players and GM, as shown in Tab.~\ref{}. Meanwhile, due to the forum-based communication, most players tend to response the closest turn, which also limited the complexity of semantics in previous works. Moreover, previous works usually operate end-to-end generation of DM utterances. Without the deeper understanding of semantics, it is hard for LLMs to generate vivid utterances as real-human DM. 


%Furthermore, toward construct better game agents, many research works are proposed and present some large-scale datasets of game logs, various task to generate different reactions, and various methods to understand the game logs better for players' intentions and grounded semantics. The significant advancements from these works should be admire. However, limited by the way of data collection, previous works neglect and also can not investigate more complex grounded semantics that close to real-human interactions. The defeats can be separated into two factors. 1). The complexity of game logs in previous works are relatively lower. Some works only focus on understanding one closest turn, which only contains a short contexts. The short contexts are hard to reflect complex interactions and semantics as real-human communications. 2). Data in previous works lack of grounded language with intermediate response that close to real-human communications. Most works collect data sourced from the forum-based communication of players, named as play-by-post. In this form, players play the game by writing their responses and posting to the forums. This kind of asynchronous online communication causes that the player have to wait response from others after several hours even weeks. This leads most players tend to response the closest turn and the communications do not contains long and grounded contexts. Moreover, some works focus on collect actions and behaviours of players. Though involved more turns, the data are hard to reflect vivid and complex grounded semantics as real-time human interactions. 

To overcome above defeats, we collect a new dataset, named Long-context Grounded-language TRPG Log (LGL) dataset. Rather than collecting data from play-by-post forums, we leverage the data from another Chinese TRPG forum (www.goddessfantasy.net). The records from this forum are typically compiled by the GM after the game ends, either through voice recordings or real-time chat logs. This makes the source data in our work overcome the defeats of play-by-post. We can excerpt long contexts with complex semantics from the game logs with the complete adventures. Since the game logs are from immediate communications, the interactions are more grounded and contain more vivid responses closed to the daily communications as show in Tab.~\ref{}. Specifically, as in Tab.~\ref{tab.dataset_sta}, our LGL contains 32.12 turns in every context and previous works only contain one turn. Additionally, more than DND rules, our dataset also contains more diverse game rules like COC, PF2, SW, etc. This further enrich the diversity and complexity of our dataset. 

Moreover, to push frontier of the understanding for complex interactions and generation of game agent, we disentangle the decision making and description of utterance of DMs. We leverage the dataset for two tasks: Generation of game Check (GenCheck) and Generation of GM Utterance (GenUtter). 
Specifically, we first excerpt long and complex contexts from LGL and then label the check items according game rules and the guidence of GM in the logs. In GenCheck, we encourage the methods to focus on understanding of the contexts and infer what check items should be judged and guided by GM. This aims to formulate a testbed for the understanding of complex grounded interactions in the game logs. During evaluation, we calculate precision and recall of the predicted character and cooresponding skills, respectively. Then, in GenUtter, we provide the check items and contexts to the language models, and encourage the methods to generate utterance that the GM may given for the players. This task targets to explore how to generate a more vivid game agent to guide the game process. During evaluation, we invite many real-human players and GMs to perform subjective evaluation for the generated utterances. 

Our main contributions are summarized as follows: 

1. We present a new dataset for TRPG game logs, named Long-context Grounded-language TRPG Log (LGL). This compensates the ignorance of exploration of long context and complex grounded interactions from the real-time communications. 

2. We decouple the decision making and description of the game agent by introducing two tasks, named Generation of game Check (GenCheck) and Generation of GM Utterance (GenUtter). 

3. We evaluate various LLMs and prompting methods in our benchmark. 


multiple turns with interactions of multiple players as the context. 


Then, we collect the check decision by GM and process the skill check to standardized answers in our dataset. In SCG, we require the methods to generate who will act in the next turn and what skill check are needed for the movements. To evaluate the task, we calculate the recall of predicted character names and skills to the original guidance by GMs for corresponding game records. Moreover, we also propose a character and motivation separate prompting (CMSP) method as a benchmark for our task. Rather than provide straightforward prompts to LLMs, CMSP take advantages of the chain of thought (CoT) prompting method and introduce two stage prompting for SCG. CMSP leads LLMs separately produce character proposals and skill proposals. This encourages the models to imagine the existed characters and possible movements and then infer the corresponding skill check for the movements. In experiments, CMSP show better performances and leads the language model better understand the complex interactions. All data and codes will be available soon. 

Game Check Generation and GM utterance generation. 

we propose a new benchmark for virtual DM generation, which involved two tasks: check generation and response generation. 


named Skill Check Generation in TRPG (SCG). Due to records contain skill checks guided by GM, this formulate a natural labels by real human to reflect the intention and semantics of players. Thus, we split multiple turns with interactions of multiple players as the context. Then, we collect the check decision by GM and process the skill check to standardized answers in our dataset. In SCG, we require the methods to generate who will act in the next turn and what skill check are needed for the movements. To evaluate the task, we calculate the recall of predicted character names and skills to the original guidance by GMs for corresponding game records. Moreover, we also propose a character and motivation separate prompting (CMSP) method as a benchmark for our task. Rather than provide straightforward prompts to LLMs, CMSP take advantages of the chain of thought (CoT) prompting method and introduce two stage prompting for SCG. CMSP leads LLMs separately produce character proposals and skill proposals. This encourages the models to imagine the existed characters and possible movements and then infer the corresponding skill check for the movements. In experiments, CMSP show better performances and leads the language model better understand the complex interactions. All data and codes will be available soon. 



similar to turn-based game rather than real-time roleplaying game. 

dataset contribution: 
turn-based v.s. real-time / long-context --> grounded language 

benchmark contribution: 
lack of understanding phrase? understanding + description v.s. end-to-end description 
show vivid descriptions for complex DM decisions. 




propose a new task and provide a novel benchmark method. In our work, we leverage the data from another Chinese TRPG forum (www.goddessfantasy.net). The records from this forum are typically compiled by the GM after the game ends, either through voice recordings or real-time chat logs. This makes the source data in our work become more comprehensive and complex. Besides, rather than just using one turn or combat turns in the previous works, we focus on the understanding of long contexts in the logs, leverage multiple turns of different players and push the exploration in grounded natural language interaction. Specifically, we propose a new task named Skill Check Generation in TRPG (SCG). Due to records contain skill checks guided by GM, this formulate a natural labels by real human to reflect the intention and semantics of players. Thus, we split multiple turns with interactions of multiple players as the context. Then, we collect the check decision by GM and process the skill check to standardized answers in our dataset. In SCG, we require the methods to generate who will act in the next turn and what skill check are needed for the movements. To evaluate the task, we calculate the recall of predicted character names and skills to the original guidance by GMs for corresponding game records.

Moreover, we also propose a character and motivation separate prompting (CMSP) method as a benchmark for our task. Rather than provide straightforward prompts to LLMs, CMSP take advantages of the chain of thought (CoT) prompting method and introduce two stage prompting for SCG. CMSP leads LLMs separately produce character proposals and skill proposals. This encourages the models to imagine the existed characters and possible movements and then infer the corresponding skill check for the movements. In experiments, CMSP show better performances and leads the language model better understand the complex interactions. All data and codes will be available soon. 






















Meanwhile, some other works with more turns mainly focus on the actions or 


Previous works only focus on the understanding of closest turns, which only contains short and limited contexts. 



The previous works rely on the data collect from `play-by-post' form, in which players play the game by writing their responses and posting to the forums. 


As players describe their characters' actions, motivations, and reactions to the game world, they generate vast amounts of textual data that can be analyzed and utilized for various research. Researchers can advance the development of sophisticated language models that can better comprehend, respond to, and engage with human language, ultimately enhancing the capabilities in real-world applications like virtual human generation. 


As a player, you have the freedom to shape your character's personality, abilities, and goals, allowing you to fully immerse yourself in their role. Through a combination of dice rolls, rule systems, and creative problem-solving, you and your fellow players navigate through a series of quests, battles, and mysteries, with the outcomes determined by your choices and the luck of the dice.

TRPGs foster collaboration and teamwork as players work together to overcome obstacles, solve puzzles, and defeat adversaries. Whether you're delving into ancient dungeons, exploring vast realms, or engaging in political intrigue, the possibilities are endless, limited only by the imaginations of those involved.

In a TRPG, players engage in verbal communication to describe their characters' actions, engage in dialogue, and interact with the game master (GM) and other players. This communication often involves a complex blend of natural language processing elements, including speech recognition, language understanding, context modeling, and language generation. As players describe their characters' actions, motivations, and reactions to the game world, they generate vast amounts of textual data that can be analyzed and utilized for NLP research.

TRPGs offer several unique features that make them valuable for NLP research. Firstly, they provide a controlled environment for studying dialogue systems and language understanding algorithms. The natural language interactions that occur during gameplay can be recorded, transcribed, and annotated, allowing researchers to analyze the nuances of human conversation and improve language models' ability to interpret and respond appropriately.

Secondly, TRPGs facilitate the study of context and context-awareness in language processing. The narratives in TRPGs are built upon a dynamic and evolving context, influenced by the players' actions, decisions, and the GM's storytelling. This context presents an excellent opportunity to investigate the challenges of maintaining coherent and meaningful dialogue within a changing environment, enhancing models' ability to comprehend and generate language in contextually rich scenarios.

Moreover, TRPGs offer a collaborative and interactive setting for studying multimodal communication. While verbal communication is central to TRPG gameplay, players also engage in non-verbal forms of expression, such as facial expressions, gestures, and body language. Exploring the integration of multimodal signals in NLP systems can help improve the accuracy and effectiveness of natural language understanding models, enabling them to capture and interpret a broader range of human communication cues.

In conclusion, TRPGs provide an exciting and valuable avenue for NLP research, offering a treasure trove of linguistic data and a dynamic environment to explore various aspects of natural language understanding, generation, and multimodal communication. By leveraging the unique characteristics of TRPG gameplay, researchers can advance the development of sophisticated language models that can better comprehend, respond to, and engage with human language, ultimately enhancing the capabilities of AI systems in real-world applications. So, grab your dice, gather your friends, and embark on an epic adventure where language meets imagination!



Recently, large language models (LLMs) have achieve significant progress in nature language processing and understanding. However, there is still a gap between real human and LLMs. Some abilities, e.g., theory of mind, helps the real human to easily understand complex interactions with multiple characters and precisely know the intentions of other people. The powerful ability in understanding grounded natural language interaction enable people to communicate and cooperate with others in daily life, which can be an essential bond to human society. Comparatively, LLMs are still struggling in understand grounded natural language interactions. LLMs usually make mistakes in understanding complex semantic, estimating the intentions of human being or even produce gibberish. 

To push frontier of the understanding ability of language models for complex interactions, we collect a new dataset, propose a new task and provide a novel benchmark method. To this end, we find that the playing record of Tabletop Role-Playing Game (TRPG) is a proper testbed for this work. In detail, TRPG is a form of interactive, narrative-driven role-playing game. Multiple players (usually four players) and one game master (GM) (or dungeon master (DM) in DND rule.). All players need to perform role-playing to act their character and work together to complete a mission or finish an adventure. GM need to describe scenarios, introduce background stories, provide responses of non-player characters (NPCs) according to the players' interactions and decide what kinds of check should be operated by which players. 

In this game, GM needs to understand the intention of other players and guide them to check their skill following the game rules. At this point, recent LLMs are hard to be competent to perform a GM. There are two challenges for the language models. 1). The interaction between multiple characters with diverse personalities are complex. Most games generally contains four players. Considering NPCs introduced and described by GM, there are usually more than four characters with different personalities, races, and backgrounds. This induces the interactions show more diversity and complexity than other NLP tasks. 2). Long and complex contexts are involved in the interaction. As a role-playing game, the players should consider the background of story, settings of the fantasy world, character background and the movements they performed in the past rounds. This leads to the interactions in TRPG contains multiple rounds and many additional descriptions by GM. All these properties of TRPG makes the records of the games becomes a valuable material to construct a new task for understanding grounded natural language interactions. 

The complexity of records in TRPG games have already raised attention in designing new NLP tasks. However, the previous works are based on the play-by-post data in a forum. The players write responses and post on the forum. Though convenient, all players are hard to obtain instant feedback and may only focus on recent interactions of other players and GM, which may reduce the complexity of interactions. This induces previous works only use data from the latest turn with one character to formulate the generation tasks. We admire their significant advancements but expect to provide a more complex understanding task for grounded natural language interactions with more characters, more contexts, and backgrounds. 

In our work, we leverage the data from another Chinese TRPG forum (www.goddessfantasy.net). The records from this forum are typically compiled by the GM after the game ends, either through voice recordings or real-time chat logs. This makes the source data in our work become more comprehensive and complex. Besides, rather than just using one turn or combat turns in the previous works, we focus on the understanding of long contexts in the logs, leverage multiple turns of different players and push the exploration in grounded natural language interaction. Specifically, we propose a new task named Skill Check Generation in TRPG (SCG). Due to records contain skill checks guided by GM, this formulate a natural labels by real human to reflect the intention and semantics of players. Thus, we split multiple turns with interactions of multiple players as the context. Then, we collect the check decision by GM and process the skill check to standardized answers in our dataset. In SCG, we require the methods to generate who will act in the next turn and what skill check are needed for the movements. To evaluate the task, we calculate the recall of predicted character names and skills to the original guidance by GMs for corresponding game records.

Moreover, we also propose a character and motivation separate prompting (CMSP) method as a benchmark for our task. Rather than provide straightforward prompts to LLMs, CMSP take advantages of the chain of thought (CoT) prompting method and introduce two stage prompting for SCG. CMSP leads LLMs separately produce character proposals and skill proposals. This encourages the models to imagine the existed characters and possible movements and then infer the corresponding skill check for the movements. In experiments, CMSP show better performances and leads the language model better understand the complex interactions. All data and codes will be available soon. 

%Then, all the interactions in the game are recorded and formulated as a report (game log or playing record), which are uploaded to some forums and shared by players. 




TRPG is a famous ... 
AI DM is a useful and practical application (AI Dungeons) , over 50 million people paly DND as statistic in 2020. 
Make works propose for TRPG log understanding and DM guidence generation 

1. lack of long-context understanding 
2. lack of grounded language with intermediate response (practical \& colloquial) 
3. lack of diversity in language and rules



\fi 