\section{On algorithmic details of ETHER}
\label{sec:ether-details}

In this section, we detail how ETHER is built over HIGhER from an algorithmic point of view.
We start by presenting in Algorithm~\ref{alg:HIGhER} an extended version of the pseudo-code for the HIGhER algorithm from \citet{cideron2020higher} with the following additions:

\begin{enumerate}
    \item (i) contrasting further the \textbf{learning} vs \textbf{behavioural} reward function concerns that we highlighted in Section~\ref{subsec:her-limitations}, 
    \item (ii) flagging the reliance on the \textbf{learning} reward function that depends on the predicate function, which is provided as an oracle in both HER and HIGhER.
\end{enumerate}

\begin{algorithm}[H]
\caption{Hindsight Generation for Experience Replay (HIGhER)}
\label{alg:HIGhER}
\SetKwInOut{Given}{Given}
\SetKwInOut{Initialize}{Initialize}
\SetKwRepeat{Do}{repeat}{until}
\Given{
\begin{itemize}
    \item an off-policy RL algorithm (e.g., DQN, R2D2) and its replay buffer $R$, 
    \item a behavioural policy $\pi_{behaviour}: \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$
    \item a \textbf{learning} reward function $r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{G} \rightarrow \mathbb{R}$ {\color{red}(oracle or learned - relying on the predicate function $f: \mathcal{S} \times \mathcal{G} \rightarrow \{0,1\}$)}, 
    \item a \textbf{behavioural} reward function $r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{G} \rightarrow \mathbb{R}$ (provided by the environment), 
    \item a language scoring function (e.g., parser accuracy, BLEU, etc.).
\end{itemize}
}
\Initialize{ 
\begin{itemize}
    %\item the behavioural policy $\pi_{behaviour}$, 
    %\item the replay buffer $R$, 
    \item the dataset $\mathcal{D}_{sup}$ of $(state, goal)$ pairs and a train-test split strategy to yield $\mathcal{D}_{sup/train}$ and $\mathcal{D}_{sup/val}$, 
    \item the Instruction Generator $m_{HIGhER}$.
\end{itemize}
}
\For{$episode = 1, M$}{
    Sample a goal $g$ and an initial state $s_0$\ from the environment\;
    $t = -1$\;
    \Do{(episode ends)}{
        $t = t + 1$\;
        Execute an action $a_t$ chosen from the behavioural policy $\pi_{behavioural}$\;
        Observe a new state $s_{t+1}$ and a {\color{purple}\textbf{behavioural} reward $r_t = r_{behavioural}(s_t, a_t, g)$}\;
        Store the transition $(s_t, a_t, r_t, s_{t+1}, g)$ in $R$\;
        Update Q-network parameters using the policy $\pi_{behavioural}$ and sampled minibatches from $R$\;
    }
    \If{{\color{blue} \textbf{learning} reward $r_{learning}(s_t, a_t, s_{t+1}, g) = f(s_{t+1}, g) = = 1$}}{
        Store the pair $(s_{t+1}, g)$ in $\mathcal{D}_{sup/train}$ or $\mathcal{D}_{sup/val}$\;
        Update $m_{HIGhER}$ parameters by sampling minibatches from $\mathcal{D}_{sup/train}$\;
    }
    \ElseIf{$m_{HIGhER}$ validation score is high enough \& $\mathcal{D}_{sup/val}$ is big enough}{
        %Use the \textbf{final} re-labelling strategy as follows...\;
        Duplicate the previous episode's transitions in $R$\;
        Sample $\hat{g}^0 = m_{HIGhER}(s_{t+1})$\;
        %Replace $g$ by $\hat{g}^0$ in {\color{red} \textbf{ all the duplicated transitions} of the last episode}\;
        Compute the {\color{blue}\textbf{learning} rewards $\forall t, \hat{r}^0_t = r_{learning}(s_t, a_t, s_{t+1}, \hat{g}^0) = f(s_{t+1}, \hat{g}^0)$}\;
        Replace $g$ by $\hat{g}^0$ and $r_t$ by $\hat{r}^0_t$ in {\color{red}\textbf{all the duplicated transitions} of the last episode}\;
    }
}
\end{algorithm}


Following the added nuances to the HIGhER algorithm, we can now show in greater and contrastive details the ETHER algorithm in Algorithm~\ref{alg:ETHER}, where we highlight the following:

\begin{enumerate}
    \item (i) the RG training can be done in parallel at any time, thus we present it in the most-inner loop of the algorithm,
    \item (ii) since ETHER trains its RG speaker and listener agents on the whole state space $\mathcal{S}$, the ability to perform either \textbf{final} or \textbf{future-k} re-labelling strategy is recovered. We present the case of the \textbf{future-k} re-labelling strategy below.
\end{enumerate}


\begin{algorithm}[H]
\caption{Emergent Textual Hindsight Experience Replay (ETHER)}
\label{alg:ETHER}
\SetKwInOut{Given}{Given}
\SetKwInOut{Initialize}{Initialize}
\SetKwRepeat{Do}{repeat}{until}
\Given{
\begin{itemize}
    \item an off-policy RL algorithm (e.g., DQN, R2D2) and its replay buffer $R$, 
    \item a behavioural policy $\pi_{behaviour}: \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$
    \item a descriptive, discriminative RG algorithm, with its dataset buffer $\mathcal{D}_{RG}$ and its listener and speaker agents\;
    \item a \textbf{learning} reward function $r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{G} \rightarrow \mathbb{R}$ {\color{green}( relying on the predicate function $f: \mathcal{S} \times \mathcal{G} \rightarrow \{0,1\}$ which is implemented via the RG's listener agent)}, 
    \item a \textbf{behavioural} reward function $r: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{G} \rightarrow \mathbb{R}$ (provided by the environment), 
    \item a language scoring function {\color{blue}(implemented via the RG's accuracy on the validation set)}.
\end{itemize}
}
\Initialize{ 
\begin{itemize}
    %\item the behavioural policy $\pi_{behaviour}$, 
    %\item the replay buffer $R$, 
    \item the dataset $\mathcal{D}_{sup}$ of $(state, goal)$ pairs and a train-test split strategy to yield $\mathcal{D}_{sup/train}$ and $\mathcal{D}_{sup/val}$, 
    \item the RG dataset $\mathcal{D}_{RG}$ of stimuli $state$ and a train-test split strategy to yield $\mathcal{D}_{RG/train}$ and $\mathcal{D}_{RG/val}$, 
    \item the Instruction Generator {\color{blue} $m_{ETHER}(\cdot)$, in the form of the RG's speaker agent}.
    \item {\color{blue} the learned predicate function $f_{ETHER}(\cdot)$, in the form of the RG's listener agent},
    \item {\color{blue} $K_{HER}\in\mathbb{N}$ specifying which re-labelling strategy to use (if $K_{HER}=0$ then \textbf{final}, otherwise \textbf{future-$K_{HER}$}).}
\end{itemize} 
}
\For{$episode = 1, M$}{
    Sample a goal $g$ and an initial state $s_0$\ from the environment\;
    $t = -1$\;
    \Do{episode ends}{
        $t = t + 1$\;
        Execute an action $a_t$ chosen from the behavioural policy $\pi_{behavioural}$\;
        Observe a new state $s_{t+1}$ and a {\color{purple}\textbf{behavioural} reward $r_t = r_{behavioural}(s_t, a_t, g)$}\;
        Store the transition $(s_t, a_t, r_t, s_{t+1}, g)$ in $R$\;
        Update Q-network parameters using the policy $\pi_{behavioural}$ and sampled minibatches from $R$\;
        {\color{blue} Store the stimulus $s_t$ in $\mathcal{D}_{RG/train}$ or $\mathcal{D}_{RG/val}$}\;
        {\color{blue} Update the RG's speaker and listener agents by playing $N_{RG}$ epochs of the RG, training on $\mathcal{D}_{RG/train}$ and performing evaluation on $\mathcal{D}_{RG/val}$}\;
    }
    \If{{\color{blue} \textbf{learning} reward $r_{learning}(s_t, a_t, s_{t+1}, g) = f_{ETHER}(s_{t+1}, g) = = 1$}}{
        Store the pair $(s_{t+1}, g)$ in $\mathcal{D}_{sup/train}$ or $\mathcal{D}_{sup/val}$\;
        Update {\color{blue}the RG's speaker agent parameters (ETHER) with supervised learning} by sampling minibatches from $\mathcal{D}_{sup/train}$\;
    }
    \ElseIf{\color{blue} RG validation accuracy on $\mathcal{D}_{RG/val}$ is high enough}{
        {\color{blue} Use the \textbf{future-$K_{HER}$} re-labelling strategy as follows...}\;
        $k = 0$, $T =$ last episode's length\;
        \Do{$k = = K_{HER}$}{
            Sample $T_{k}$ uniformly from $[1,T]$\; 
            Duplicate the previous episode's transitions in $R$, until sampled timestep $T_{k}$\;
            Sample $\hat{g}^0 = m_{ETHER}(s_{T_k})$\;
            %Replace $g$ by $\hat{g}^0$ in {\color{red} \textbf{ all the duplicated transitions} of the last episode}\;
            Compute the {\color{blue}\textbf{learning} rewards $\forall t, \hat{r}^0_t = r_{learning}(s_t, a_t, s_{t+1}, \hat{g}^0) = f_{ETHER}(s_{t+1}, \hat{g}^0)$}\;
            Replace $g$ by $\hat{g}^0$ and $r_t$ by $\hat{r}^0_t$ in {\color{red}\textbf{all the duplicated transitions} of the last episode}\;
            $k = k + 1$\;
        }
    }
}
\end{algorithm}