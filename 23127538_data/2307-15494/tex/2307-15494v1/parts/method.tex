\section{Method}

\subsection{Extending HIGhER's Applicability}
\label{sec:extending-higher}

%\todo[inline]{ETHER builds on HIGHER by adding three main novelties: (1) a semantic co-occurrence auxiliary task (2) an improvement on sample efficiency at learning a relabelling function based on contrastive learning and (3) the addition of a referential game pre-training to obtain a predicate function. Novelty (1) allows ETHER to extract communication-grounding learning signals from unsuccessful trajectories whereas (3) drops the assumption of a ground-truth predicate function which tells us if a state satisfies a given goal.}

As explained in Section~\ref{subsec:higher-limitations}, HIGhER, like HER, still relies on a predicate function being provided and queried as an oracle.
In the following, we propose to address this limitation in two ways.
We first propose to derive a partial predicate function from the re-labelling function in the Section \ref{subsubsec:deriving_predicate_function}.
Then, we show how to enhance the quality of the derived predicate function with a contrastive learning approach in Section \ref{subsubsec:enhancing-predicate-function-with-contrastive-learning}.

\subsubsection{Deriving a Predicate Function}
\label{subsubsec:deriving_predicate_function}

Because HIGhER only implements the \textbf{final} relabelling strategy, we remark that the predicate function is only necessary in order to compute the output of the \textit{learning reward function} when it is fed a state $s_t$ from an unsuccessful episode and the relabelling goal $g^\prime\in\mathcal{G}$, such that $g^\prime = m(s_{t_{final}})$. 
Notably, this new goal $g^\prime$ could have also been reached in previous steps of the same trajectory, and if so, those transitions should also feature a positive reward like the final state of the episode. 
This can be achieved by applying $m$ to all states in the trajectory and giving a positive reward \textit{if and only if} the new goal for a given state matches that of the relabelled goal for the last state $s_{last}$.
%, as per Equation~\ref{eq:deriving_predicate_function}:
%
%\begin{equation}
%r_t =
%    \begin{cases}
%    1 & \text{if } m(s_t) \equiv m(s_{t_{final}}) \\
%    0 & \text{otherwise} \\
%    \end{cases}
%    \label{eq:deriving_predicate_function}
%\end{equation}
%
This procedure derives a predicate function from a re-labelling function.
In the remainder of the paper, we will denote this extension as HIGhER$+$.
It is important to note that this procedure is not as sound as it could be because it makes the implicit and erroneous assumption that fulfilled goals are deterministic and unique for each state, whereas, firstly, the expressivity of natural language allows many different ways of expressing a similar semantic for a goal that would have been fulfilled in a given state (resulting in different theoretically-valid values for $m(s_t)$ and $m(s_{t_{final}})$) and, secondly, for any given state a distribution of fulfilled goals (with different semantics, not just synonymous expressions) could be defined.
For instance, whenever a ``pick up the blue ball'' can be identified as being fulfilled in a given state, then the goal ``pick up a blue object'', or ``pick up a ball'' are all as valid as the former. 

\subsubsection{Enhancing the Predicate Function with Contrastive Learning}
\label{subsubsec:enhancing-predicate-function-with-contrastive-learning}

Let us assume that we have access to a relabelling function, either learnt or given. 
Successful trajectories, those that yield a positive environment reward, are ground-truth indicators of a goal being satisfied on the last visited state. 
In contrast, the states in the trajectory leading up the goal-fulfilling state do not satisfy the same condition as the last state, as otherwise those transitions would receive a positive reward according to the environment's goal-conditioned reward signal. 
We exploit this structure to use contrastive learning methods. 

HIGhER learns an instruction relabelling function by making use of a dataset of (state, goal) pairs, as defined in Section~\ref{sec:background}. 
We further increase the accuracy of the learnt re-labelling function via contrastive learning where the positive examples are the same (state, goal) pairs in dataset $\mathcal{D}$ and the negative examples are defined as follows.
Let $T_{final}$ be the timestep of the final transition. 
Negative examples consist of pairs of states $S_{(T_{final} - i)}$ for $i \in [1, n]$ and their associated negative goal.
This negative goal is built contrastively to the true re-labelling goals as $G_{neg} = EoS$, i.e. using the End of Sentence (EoS) symbol. 
We use $EoS$ to trivially satisfy that the negative goal $G_{neg}$ differs from the goal of the positive example $g$. 

\subsection{Leveraging Unsuccessful Trajectories with Emergent Communication}
\label{sec:rg-as-unsupervised-rl-task}

\subsubsection{Learning a Predicate Function via Referential Games}
\label{sec:learning_predicate_function}

Taking a closer look at the listener agent of a visual discriminative RG, it takes as input $K+1$ stimuli and a message/linguistic description from the speaker agent to output confidence levels for each stimulus of the extent with which the message clearly describes them.
In the context of $K=0$, the listener agent outputs a likelihood for the message to be clearly describing some attributes wihtin the one and only stimulus provided.
This is analoguous to what a predicate function does.
Thus, the listener agent of any RG can be readily put in the place of the predicate function in the context of hindsight experience replay, provided that the RG's speaker agent is used as an Instruction Generator following HIGhER recipe.
Note that this extension already incorporates contrastive learning as a discriminative RG is literally asking the listener agent to contrast positive (the target) and negative (the distractors) stimuli.
Henceforth, we refer to this augmentation as the Emergent Textual Hindsight Experience Replay (ETHER) agent. 
We provide in Appendix~\ref{sec:ether-rg} further details about the instantiated RG.

\subsubsection{Semantic Co-occurrence Grounding}
\label{subsec:co-occurrence}

As mentioned before, a major shortcoming of HIGhER is that during training it learns a goal relabelling function, which is only capable of mapping states to goals that are satisfied in successful trajectories. This shortcoming is related to a seldom mentioned assumption in the design of goal-oriented language grounding environments: during training, the goal of an episode can \textit{always} be reached. Generally, these goals are represented through semantic descriptions of the necessary interactions between the agent and objects that are present in the environment (e.g., "pick up the green key", "open the door"). 
Presently, we hypothesise that if it is indeed the case that the goal can always be fulfilled, then upon specifiying a goal, agent observations will be biased to contain semantic components present in said goal. 
We test this hypothesis in the BabyAI environment in Figure~\ref{fig:co_occurrence} where we see that, indeed, the semantics of the goal are some of the most salient observed semantics in both expert trajectories and random walks. 
Given the similarity between semantics of observationns and goals in both successful and unsuccessful trajectories, which we refer to henceforth as \textbf{semantic co-occurrence}, we ask ourselves: How can this underlying environmental structure be leveraged to learn a semantic understanding of the goal trying to be achieved?
In the context of ETHER which is centered around the addition of RG agents, this translates as: How can this underlying environmental structure be leveraged to constraint the RG's emergent language to be aligned with the natural(-like) language used to describe goals in any instruction-following benchmark?

To answer this question, we introduce the \textbf{semantic co-occurrence grounding loss}, which aims to enhance an agent's language grounding ability during RG training by biasing the speaker agent's token embeddings towards parts of the agent's field of view containing semantic components that are emphasised by the current goal. 
Only the words/tokens present in the linguistic goal description provided are used, there is not need for any environment-private information. 
An example of this would be to bring the embedding of the token 'blue' closer to the visual embeddings of the part of the observation that contains a blue-coloured object.
We refer to this architecture as ETHER+.

%We acknowledge that there are other potential applications for our semantic co-occurrence auxiliary task, such as using this loss on a critic.
%\todo[inline]{Explain succintly (1 phrase) how this could be used in a critic}

% Figure environment removed
