\section{Implementation Details}
\label{sec:implementation-details}

\begin{wraptable}{R}{0.45\linewidth}%[h]
%\centering
\vspace{-50pt}
\caption{Hyper-parameter values relevant to R2D2 in the different architectures presented. All missing parameters follow the ones in Ape-X~\citep{horgan2018apex}.}
\label{table:hyperparams}
\begin{tabular}{@{} ll @{}} 
\toprule
\multicolumn{2}{c}{R2D2} \\ 
\midrule
\multicolumn{1}{l}{Number of actors} & \multicolumn{1}{c}{32} \\
\multicolumn{1}{l}{Actor update interval} & \multicolumn{1}{c}{1 env. step} \\
\multicolumn{1}{l}{Sequence unroll length} & \multicolumn{1}{c}{20} \\
\multicolumn{1}{l}{Sequence length overlap} & \multicolumn{1}{c}{10} \\
\multicolumn{1}{l}{Sequence burn-in length} & \multicolumn{1}{c}{10} \\
\multicolumn{1}{l}{N-steps return} & \multicolumn{1}{c}{3} \\
\multicolumn{1}{l}{Replay buffer size} & \multicolumn{1}{c}{$1\times10^4$ obs.} \\
\multicolumn{1}{l}{Priority exponent} & \multicolumn{1}{c}{0.9} \\
\multirow{2}{0.45\linewidth}{Importance sampling exponent} & \multicolumn{1}{c}{0.6} \\
& \\
\multicolumn{1}{l}{Discount $\gamma$} & \multicolumn{1}{c}{$0.98$} \\
\multicolumn{1}{l}{Minibatch size} & \multicolumn{1}{c}{64} \\
\multicolumn{1}{l}{Optimizer} & \multicolumn{1}{c}{Adam~\citep{kingma2014adam}} \\
\multicolumn{1}{l}{Learning rate} & \multicolumn{1}{c}{$6.25\times10^{-5}$} \\
\multicolumn{1}{l}{Adam $\epsilon$} & \multicolumn{1}{c}{$10^{-12}$} \\
\multirow{2}{0.35\linewidth}{Target network update interval} & \multicolumn{1}{c}{2500} \\
&  \multicolumn{1}{c}{updates} \\
%\multicolumn{2}{l}{Target network update interval} & \multicolumn{2}{c}{2500 updates} \\
\multicolumn{1}{l}{Value function rescaling} & \multicolumn{1}{c}{None} \\
%h(x) = sign(x)(âˆš|x|+ 1âˆ’1) +x, = 10âˆ’3
\bottomrule
\end{tabular}
\vspace{-45pt}
\end{wraptable}

Table~\ref{table:hyperparams} highlights the hyperparameters used for the off-policy RL algorithm, R2D2\citep{kapturowski2018recurrent}.
More details can be found, for reproducibility purposes, in our open-source implementation at HIDDEN-FOR-REVIEW-PURPOSES.

%Training was performed for each run on 1 NVIDIA GTX1080 Ti, and the average amount of training time for a run is 18 hours, on an observation budget of 5 million samples, with 32 actors.
Training was performed for each run on 1 NVIDIA GTX1080 Ti, and the amount of training time for a run is between 8 and 24 hours depending on the architecture. 
%, on an observation budget of 5 million samples, with 32 actors.

