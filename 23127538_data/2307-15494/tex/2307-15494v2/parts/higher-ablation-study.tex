\section{On HIGhER's Ablation Study}
\label{sec:higher-ablation-study}

Prior to the architectures described in the main part of the paper, we iterated over many designs and induction biases.
%Firstly, we experimented with R2D2's features such as the burn-in feature.
Notably we experimented with R2D2's burn-in feature.

Table~\ref{tab:PickUp-higher-no-burnin} shows the success ratios of HIGhER agents without burn-in feature against baseline R2D2 without burn-in feature on the modified (one-pick-up) PickUpDist-v0 task from the BabyAI benchmark at the end of the $200k$ observation budget.
The results show that the contrastive learning scheme for the predicate function is rather hurting performance compared to HIGhER+, while still being above baseline.
The burn-in feature provides the RL agent better sample-efficiency by stabilising the training of the recurrent network in the architecture.
While the instruction generator/speaker agent is being trained, the resulting goal re-labelled experiences that enters the replay buffer are presumably non-stationary.
Thus, we attribute the lower performance of the above architectures to the fact that they struggle to deal with the non-stationarity of the goal re-labelled experiences in the absence of the stabilising burn-in feature. 

\begin{table}[b]%{l}{0.6\textwidth}
\caption{Success ratios (mean and standard deviation) for agents without the burn-in feature of R2D2 after 200k steps in a modified version of the BabyAI PickUpDist-v0 task. 3 random seeds for each agent.}
\label{tab:PickUp-higher-no-burnin}
\centering
%    \renewcommand{\arraystretch}{1.5}
\begin{tabular}{@{}lccc@{}} 
%\begin{tabular}{c|c|c}
\toprule
    \textbf{Agent} & \textbf{Mean} & \\ \hline
    R2D2 (w/o Burn-In) & 13.02 $\pm$ 1.26 & \\ \hline
    \midrule
    HIGhER+ (w/o Burn-In) & 16.02 $\pm$ 1.79 & \\ \hline
    HIGhER++ (n=1) (w/o Burn-In) & 14.97 $\pm$ 1.19 & \\ \hline
    HIGhER++ (n=2) (w/o Burn-In) & 15.89 $\pm$ 0.60 & \\ \hline
    HIGhER++ (n=4) (w/o Burn-In) & 13.93 $\pm$ 2.29 & \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{comment} 
We also experimented with features related to our incremental improvements.
Most notably, we experimented with a version of HIGhER that does not share the observation encoder in the visual module.
We will refer to it as Agnostic-HIGhER. %\todo[]{replace values in the below table for agnostic higher with or withou burn-in, need to specify}

\begin{table}[t] %{L}{0.53\textwidth}
\caption{Success ratios (mean and standard deviation) for Agnostic agent variants with the burn-in feature of R2D2 after 200k steps in a modified version of the BabyAI PickUpDist-v0 task.}
\label{tab:PickUp-agnostic-higher}
\centering
%    \renewcommand{\arraystretch}{1.5}
\begin{tabular}{@{}lccc@{}} 
%\begin{tabular}{c|c|c}
\toprule
    \textbf{Agent} & \textbf{Mean} & \\ \hline
    R2D2 & 16.54 $\pm$ 1.37 & \\ \hline
    \midrule
    Agnostic-HIGhER+ & 0.1602 $\pm$ 0.0179 & \\ \hline
    Agnostic-HIGhER++ (n=1) & 0.1497 $\pm$ 0.01193 & \\ \hline
    Agnostic-HIGhER++ (n=2) & 0.1589 $\pm$ 0.00597 & \\ \hline
    Agnostic-HIGhER++ (n=4) & 0.1393 $\pm$ 0.02289 & \\
    \bottomrule
    \end{tabular}
\end{table}

%Table~\ref{tab:PickUp-agnostic-higher} shows the success ratios of HIGhER agents without burn-in feature against baseline R2D2 without burn-in feature either on the modified (one-pick-up) PickUpDist-v0 task from the BabyAI benchmark at the end of the $200k$ observation budget.
\todo[inline]{Analyse the results...}
%We interpret this as follows: without a shared observation encoder, the contrastive learning scheme does not provide any feedback to the RL agent, thus the performance should not increase in this context, but the fact that the performance are hurt could be solely due to \todo[inline]{the fact that the quality of the predicate function is not good enough yet to be used in the hindsight relabelling scheme.}
\end{comment}

