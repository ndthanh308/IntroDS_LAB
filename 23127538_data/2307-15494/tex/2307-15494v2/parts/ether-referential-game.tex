\section{On the Referential Game in ETHER}
\label{sec:ether-rg}

In the following, we detail further the referential game (RG) used in the ETHER architectures.

As highlighted in Section~\ref{subsec:emecom}, we follow the nomenclature proposed in \citet{DenamganaiAndWalker2020a} and focus on a \textit{descriptive object-centric (partially-observable) $2$-players/$L=10$-signal/$N=0$-round/$K=31$-distractor} RG variant, as illustrated in Figure~\ref{fig:ether-rg}.

The descriptiveness implies that the target stimulus may not be passed to the listener agent, but instead replaced with a descriptive distractor.
In effect, the listener agent's decision module therefore outpus a $K+2$-logit distribution where the $K+2$-th logit represents the meaning/prediction that none of the $K+1$ stimuli is the target stimulus that the speaker agent was `talking' about. 
The addition is made following \citet{Denamganai2023visual-COMPODIS} as a learnable logit value, $logit_{no-target}$, it is an extra parameter of the model. 
In this case the decision module output is no longer as specified in Equation~\ref{eq:discr-stgs}, but rather as follows:
\begin{equation}
\label{eq:descr-discr-stgs}
    p((d_i)_{i\in[0,K+1]} | (s_i)_{i\in[0,K]}; m) 
    = 
    Softmax \Bigl( ( h^l_L \cdot f(s_i)^T )_{i\in[0,K]} \cup \{\textcolor{red}{logit_{no-target}}\} 
    \Bigr).
\end{equation}

The descriptiveneness is ideal but not necessary in order to employ the listener agent as a predicate function for the hindsight experience replay scheme.
Thus, in the main results of the paper, we present the version without descriptiveness.

In the remainder of this section, we detail the STGS-LazImpa loss that we employed in our referential game, as illustrated in Figure~\ref{fig:ether-rg}.

%\todo[inline]{Linguistic Functions. Discuss \citet{Wu2021-entropy-decomposition}'s entropy decomposition trick to align with \citet{jakobson1960linguistics}'s functions of language, and how \citet{Lowe2019}'s concepts of positive signalling and positive listening factors in, in order to emphasise how our ETHER builds over each of those.}

%\todo[inline]{Discuss that ETHER allows using the future strategy from HER, whereas THER does not ; thanks to meaningful rewards along the way of multi-objective instructions, e.g. opening doors before picking up an object.}


\subsection{STGS-LazImpa Loss}
\label{subsec:stgs-lazimpa}

Emergent languages rarely bears the core properties of natural languages \citep{Kottur2017,Bouchacourt2018, Lazaridou2018, Chaabouni2020}, such as Zipf’s law of Abbreviation (ZLA). 
In the context of natural languages, this is an empirical law which states that the more frequent a word is, the shorter it tends to be~\citep{zipf2016human-zla, strauss2007word-zla}.
\citet{rita2020lazimpa} proposed LazImpa in order to make emergent languages follow ZLA.

To do so, Lazimpa adds to the speaker and listener agents some constraints to make the speaker lazy and the listener impatient.
Thus, denoting those constraints as $\mathcal{L}_{STGS-lazy}$ and $\mathcal{L}_{impatient}$, we obtain the STGS-LazImpa loss as follows:

\begin{equation}
\label{eq:stgs-lazimpa-loss}
    \mathcal{L}_{STGS-LazImpa} (m, (s_i)_{i\in[0,K]}) 
    =
    \mathcal{L}_{STGS-lazy}(m) 
    +
    \mathcal{L}_{impatient}(m, (s_i)_{i\in[0,K]}) .
\end{equation}

In the following, we detail those two constraints.

\textbf{Lazy Speaker.} The Lazy Speaker agent has the same architecture as common speakers. The ‘Laziness’ is originally implemented as a cost on the length of the message $m$ directly applied to the loss, of the following form:

\begin{equation}
\label{eq:lazy-loss}
\mathcal{L}_{lazy}( m ) = \alpha(acc)|m|
\end{equation}
where $acc$ represents the current accuracy estimates of the referential games being played, and $\alpha$ is a scheduling function, which is not differentiable. 
This is aimed to adaptively penalize depending on the message length.
Since the lazyness loss is not differentiable, they ought to employ a REINFORCE-based algorithm for the purpose of credit assignement of the speaker agent.

In this work, we use the STGS communication channel, which has been shown to be more sample-efficient than REINFORCE-based algorithms~\citep{Havrylov2017}, but it requires the loss functions to be differentiable.
Therefore, we modify the lazyness loss by taking inspiration from the variational autoencoders (VAE) literature~\citep{Kingma2013-VAE}.

The length of the speaker's message is controlled by the appearance of the EoS token, wherever it appears during the message generation process that is where the message is complete and its length is fixed.
Symbols of the message at each position are sampled from a distribution over all the tokens in the vocabulary that the listener agent outputs.
Let $(W_l)$ be this distribution over all tokens $w\in V$ at position $l\in [1,L]$, such that $\forall l\in[1,L],\, m_l \sim (W_l)$. 
We devise the lazyness loss as a Kullbach-Leibler divergence $D_{KL}(\cdot | \cdot)$ between these distribution and the distribution $(W_{EoS})$ which attributes all its weight on the EoS token.
Thus, we dissuade the listener agent from outputting distributions over tokens that deviate too much from the EoS-focused distribution $(W_{EoS})$, at each position $l$ with varying coefficients $\beta(l)$.
The coefficient function $\beta: [1,L] \rightarrow \mathbb{R}$ must be monotically increasing.
We obtain our STGS-lazyness loss as follows:
\begin{equation}
\label{eq:stgs-lazyness-loss}
    \mathcal{L}_{STGS-lazy}(m) 
    =
    \sum_{l\in[1,L]} 
    \beta(l)
    D_{KL} \Bigr( 
    (W_{EoS}) |
    (W_l)
    \Bigl)
\end{equation}

\textbf{Impatient Listener.} Our implementation of the Impatient
Listener agent follows the original work of \citet{rita2020lazimpa}: it is designed to guess the target stimulus as soon as possible, rather than solely upon reading the EoS token at the end of the speaker's message $m$. 
Thus, following Equation~\ref{eq:discr-stgs}, the Impatient Listener agent outputs a probability distribution over a set of $K+1$ stimuli $(s_0, ..., s_K)$ for all sub-parts/prefixes of the message $m=(m_1,...,m_l)_{l\in[1,L]}=(m_{\leq l})_{l\in[1,L]}$ :
\begin{equation}
\label{eq:impatient-discr-stgs}
    \forall l \in [1,L], \;\;
    p( \mathbf{(d^{\leq l}_i)_{i\in[0,K]}} | (s_i)_{i\in[0,K]} ; \mathbf{m^{\leq l}}) = 
    Softmax \Bigr( 
    ( \mathbf{h_{\leq l}} \cdot f(s_i)^T )_{i\in[0,K]} 
    \Bigl),
\end{equation}
where $\mathbf{h_{\leq l}}$ is the hidden state/output of the recurrent network in the language module (cf. Section~\ref{app:model-architecture}) after consuming tokens of the message from position $1$ to position $l$ included.

Thus, we obtain a sequence of $L$ probability distributions, which can each be contrasted, using the loss of the user's choice, against the target distribution $(D_{target})$ attributing all its weights on the decision $d_{target}$  where the target stimulus was presented to the listener agent.
Here, we employ \citet{Havrylov2017}'s Hinge loss.
Denoting it as $\mathbb{L}(\cdot)$, we obtain the impatient loss as follows:
\begin{equation}
\label{eq:impatient-loss}
    \mathcal{L}_{impatient/\mathbb{L}}( m, (s_i)_{i\in[0,K]}) 
    =
    \frac{1}{L}
    \sum_{l\in[1,L]}
    \mathbb{L}( (d^{\leq l}_{i\in[0,K]}, (D_{target}) ).
\end{equation}