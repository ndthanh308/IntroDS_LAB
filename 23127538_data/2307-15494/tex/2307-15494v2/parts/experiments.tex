\section{Experiments}
\label{sec:experiments}

\subsection{Experimental setup}

We perform all experiments in an altered version of the BabyAI environment 'BabyAI-PickUpDist-v0' \cite{Chevalier-Boisvert2018}. 
This environment rewards an agent at each episode for picking up a specifically coloured and shaped object among other distracting objects, depending on an observed, natural(-like) language instruction, e.g. ``Pick up the blue ball''. 
We altered the environment by adding a one-pickup-per-episode wrapper that makes it so that the episode ends when any object is picked up, meaning that there is no pick-up action happening in the rest of the episode but the very last experience, unless the episode times out after 40 available timesteps (similarly to \citet{cideron2020higher}). 
In other words, the result of an episode can either be successful (the agent picked up the target object, as specified by the instruction), unsuccessful (the agent picked up a wrong object), or timed out (i.e. no object was picked-up within the limit of the 40 timesteps).

\subsection{Agent Architecture}
\label{app:model-architecture}

The ETHER architecture is made up of three differentiable agents, the language-conditioned RL agent and the two RG agents (speaker and listener).
Similarly, the HIGhER architecture is built around a language-conditioned RL agent and an Instruction Generator, which plays the same role as the speaker agent in a RG thus we equate them in the following architectural details. 
Each agent consists of at least a language module and a visual module. 
The \textit{listener} agent additionally incorporates a third decision module that combines the outputs of the other two modules.
The RL agent similarly incorporates a third decision module with the addition that this third module contains a recurrent network, acting as core memory module for the agent. 
Using the Straight-Through Gumbel-Softmax (STGS) approach in the communication channel of the RG, the \textit{speaker} agent is prompted to produce the output string of symbols with a \textit{Start-of-Sentence} symbol and the visual module's output as an initial hidden state while the \textit{listener} agent consumes the string of symbols with the null vector as the initial hidden state. 
In the following subsections, we detail each module architecture in depth.

\textbf{Visual Module.} The visual module $f(\cdot)$ consists of the \textit{Shared Observation Encoder}, which is shared between all the different agents, followed by some agent-specific adaptation layers, as shown in Figure~\ref{fig:ether-arch}.
The former consists of three blocks of a $3\times3$ convolutional layer with stride $2$ followed by a $2$D batch normalization layer and a ReLU non-linear activation function. 
The two first convolutional layers have $32$ filters, whilst the last one has $64$. 
The bias parameters of the convolutional layers are not used, as it is common when using batch normalisation layers. 
Inputs are stimuli consisting of $4$ stacked consecutive frames of the environment resized to $64\times64$. 
The RL agent's adaptation layers consist of $2$ FiLM layers \citep{perez2018film} conditioned on the \textit{linguistic goal description} $g\in\mathcal{G}$ after it is processed by the RL agent's language module.
Please refer to the appendix or the open-sourced code for the details on the other adaptation layers which consists of fully-connected networks.

\textbf{Language Module.} The language module $g(\cdot)$ consists of some learned Embedding followed by either a one-layer GRU network~\citep{cho2014learning} in the case of the RL agent, or a one-layer LSTM network~\citep{hochreiter1997long} in the case of the RG agents. 
In the context of the \textit{listener} agent, the input message $m=(m_i)_{i\in[1,L]}$ (produced by the \textit{speaker} agent) is represented as a string of one-hot encoded vectors of dimension $|V|$ and embedded in an embedding space of dimension $64$ via a learned Embedding. The output of the \textit{listener} agent's language module, $g^l(\cdot)$, is the last hidden state of the RNN layer, $h^l_L = g^L(m_L, h^l_{L-1})$.
In the context of the \textit{speaker} agent's language module $g^S(\cdot)$, the output is the message $m=(m_i)_{i\in[1,L]}$ consisting of one-hot encoded vectors of dimension $|V|$, which are sampled using the STGS approach from a categorical distribution $Cat(p_i)$ where $p_i = Softmax(\nu(h^s_i))$, provided $\nu$ is an affine transformation and $h^s_i=g^s(m_{i-1}, h^s_{i-1})$. $h^s_0=f(s_t)$ is the output of the visual module, given the target stimulus $s_t$.

\textbf{Decision Module.} From the RL agent to the RG's listener agent, the decision module are very different since their outputs are either, respectively, in the action space $\mathcal{A}$ or the space of distributions over $K+1$ stimuli. 
For the RL agent, the decision module takes as input a concatenated vector comprising the output of the FiLM layers, after it has been procesed by a 3-layer fully-connected network with 256, 128 and 64 hidden units with ReLU non-linear activation functions, and some other information relevant to the RL context (e.g. previous reward and previous action selected, following the recipe in \citet{kapturowski2018recurrent}). 
The resulting concatenated vector is then fed to the core memory module, a one-layer LSTM network~\citep{hochreiter1997long} with $1024$ hidden units, which feeds into the advantage and value heads of a 1-layer dueling network \citep{wang2016dueling}.

In the case of the RG's listener agent, similarly to \citet{Havrylov2017}, the decision module builds a probability distribution over a set of $K+1$ stimuli/images $(s_0, ..., s_K)$, consisting of $K$ distractor stimuli and the target stimulus, provided in a random order (see Figure~\ref{fig:ether-rg}), given a message $m$ using the scalar product:
\begin{equation}
\label{eq:discr-stgs}
    p((d_i)_{i\in[0,K]}| (s_i)_{i\in[0,K]} ; m) = Softmax \Bigr( ( h^l_L \cdot f(s_i)^T )_{i\in[0,K]} \Bigl).
\end{equation}

\begin{comment}
\begin{wraptable}{R}{0.4\textwidth}
\vspace{-30pt}
\caption{\textbf{Top:} Success ratios (percentage of mean and standard deviation) for agents with burn-in feature of R2D2 after 200k observations.
\textbf{Bottom:} Accuracies (percentage of mean and standard deviation) for semantic grounding of the speaker agent after 200k observations. 
}
\label{tab:performance}
%\centering
%    \renewcommand{\arraystretch}{1.5}
\begin{tabular}{@{}lccc@{}} 
%\begin{tabular}{c|c|c}
\toprule
    \textbf{Agent} & \textbf{Success Ratio} & \\
    R2D2 & 16.54 $\pm$ 1.37 & \\ %1.372 
    \midrule
    HIGhER+ & 14.84 $\pm$ 1.40 & \\ %1.408 
    HIGhER++ (n=1) & 15.89 $\pm$ 1.19 & \\ %1.193
    HIGhER++ (n=2) & 16.80 $\pm$ 2.07 & \\ %2.067
    HIGhER++ (n=4) & 18.10 $\pm$ 2.54 & \\ %2.542
    \midrule 
    ETHER & 27.63 $\pm$ 1.20 & \\
    ETHER+ & 27.16 $\pm$ 2.57 & \\
    \bottomrule
%\caption{Accuracies (percentage of mean and standard deviation) for semantic grounding of the speaker agent after 200k observations.}
%\label{tab:alignement-accuracy}
    \toprule
     & \textbf{Any-Colour} & \\ %\textbf{Any-Shape} \\
    ETHER & 9.117 $\pm$ 1.676 & \\ %0.0 $\pm$ 0.0 \\
    ETHER+ & 32.75 $\pm$ 6.29 & \\ %0.0 $\pm$ 0.0 \\
    \bottomrule
\end{tabular}
\end{wraptable}

\end{comment}

\begin{wraptable}{R}{0.4\textwidth}
\vspace{-12pt}
\caption{Success ratios (percentage of mean and standard deviation) for agents with burn-in feature of R2D2 after 200k observations.
}
\label{tab:performance}
%\centering
%    \renewcommand{\arraystretch}{1.5}
\begin{tabular}{@{}lccc@{}} 
%\begin{tabular}{c|c|c}
\toprule
    \textbf{Agent} & \textbf{Success Ratio} & \\
    R2D2 & 16.54 $\pm$ 1.37 & \\ %1.372 
    \midrule
    HIGhER+ & 14.84 $\pm$ 1.40 & \\ %1.408 
    HIGhER++ (n=1) & 15.89 $\pm$ 1.19 & \\ %1.193
    HIGhER++ (n=2) & 16.80 $\pm$ 2.07 & \\ %2.067
    HIGhER++ (n=4) & 18.10 $\pm$ 2.54 & \\ %2.542
    \midrule 
    ETHER & 27.63 $\pm$ 1.20 & \\
    ETHER+ & 27.16 $\pm$ 2.57 & \\
    \bottomrule
\end{tabular}
\vspace{-10pt}
\end{wraptable}

% \begin{wraptable}{R}{0.43\textwidth}
% \caption{Accuracies (percentage of mean and standard deviation) for semantic grounding of the speaker agent after 200k observations.}
% \label{tab:alignement-accuracy}
% %\centering
% %    \renewcommand{\arraystretch}{1.5}
% \begin{tabular}{@{}lccc@{}} 
% %\begin{tabular}{c|c|c}
% \toprule
%     \textbf{Agent} & \textbf{Any-Colour} & \textbf{Any-Shape} \\
%     ETHER & 9.117 $\pm$ 1.676 & 0.0 $\pm$ 0.0 \\
%     ETHER+ & 32.75 $\pm$ 6.29 & 0.0 $\pm$ 0.0 \\
% \bottomrule
% \end{tabular}
% \end{wraptable}

\begin{comment}
\subsection{Results}

Table~\ref{tab:performance}(top) shows the success ratios for the different architectures.
We can see that while the HIGhER++ extensions provide marginal improvements, the ETHER approach outperforms all approaches, thus cementing the usage of visual discriminative referential games as viable unsupervised auxiliary task for RL.  
Table~\ref{tab:performance}(bottom) shows the extent to which the RG's speaker agent has been using any of the natural language colour words to describes stimuli containing said colour as a visual feature, as well as any of the natural language shape words to describes stimuli containing said shape as a visual feature. 
The results show that constraining of the RG agents using the semantic co-occurrence grounding loss in ETHER+ does start to provide alignement between the emergent language and the benchmark's natural-like language regarding the colour semantic, while the shape semantic remains ungrounded.
The latter possibly explains the current performance of the agents not exceeding $33\%$ of success ratio, which corresponds to the random success ratio if there was only the shape semantic to discriminate against. 
Indeed, the task proposes only 3 different shape semantics: the ball, the key, and the box.    
\end{comment}

\subsection{ETHER Improves Sample-Efficiency and Performance}

\textbf{Hypothesis.} Firstly, our extensions to HIGhER replace the oracle predicate function with a derived, deterministic predicate function which is not theoretically sound, thus we investigate here whether this can still be beneficial to the RL agent's learning by comparison to our R2D2 baseline \textbf{(H1)}.
Secondly, as ETHER instantiates hindsight learning and an unsupservised auxiliary task for RL in the form of the RG, we expect it to improve the sample-efficiency and the asymptotic performance compared to our R2D2 baseline \textbf{(H2)}.
Thirdly, since ETHER learns a principled predicate function (in the form of the listener agent of a RG) which is theoretically sound, as opposed to the predicate functions derived in our various HIGhER extensions, we hypothesise that ETHER should also improve the sample-efficiency and the asymptotic performance compared to HIGhER extensions \textbf{(H3)}.
Finally, as ETHER+ constraints the EL via semantic co-occurrence grounding, we ponder whether this constraint has any impact on the RL agent's performance \textbf{(H4)}. 

\textbf{Evaluation}. We evaluate both the sample-efficiency and performance by reporting on the success ratio of the RL agents over $256$ randomly-generated environments, after training has occured on a fixed sampling budget of $200$k observations.

\textbf{Results.} Table~\ref{tab:performance} shows the success ratios for the different algorithms and architectures.
We can see that our HIGhER extensions requires a great amount of negative examples in the contrastive training (n=4) in order to validate \textbf{(H1)}, and still it only provides marginal improvements over baseline.
Thus, our results shows that our derived predicate function is practically feasable but fairly limited.
On the otherhand, both ETHER approaches outperform all other approaches by almost doubling the final performance, thus validating both hypotheses \textbf{(H2)} and \textbf{(H3)}.
These results are cementing the usage of visual discriminative referential games as viable unsupervised auxiliary task for RL and they are showing that our principled, RG-learned predicate function is not only theoretically sound but also practical.
Finally, regarding \textbf{(H4)}, we observes similar mean asymptotic performance between ETHER and ETHER+, but ETHER+'s distribution has a greater standard deviation which goads us to think that the semantic co-occurrence grounding may exert some detrimental constraints onto the RL agent.
We bring the reader's attention onto the fact that the noise parameter $\epsilon_\text{noise}$ of the semantic co-occurrence grounding loss (cf. equation~\ref{eq:indicator-function}) may still be too strong and thus explain this detrimental effect on the RL performance.


\subsection{Semantic Co-Occurrence Grounding Improves Emergent Language Alignment}

\begin{wraptable}{R}{0.45\textwidth}
\vspace{-12pt}
\caption{Alignment accuracies (percentage of mean and standard deviation) between the emergent languages spoken by the RG's speaker agent and the benchmark's Natural Language, after training on 200k RL observations. 
}
\label{tab:performance-alignment}
%\centering
%    \renewcommand{\arraystretch}{1.5}
\begin{tabular}{@{}lccc@{}} 
%\begin{tabular}{c|c|c}
\toprule
\textbf{Agent} & \textbf{Any-Colour} & \textbf{Any-Shape} \\
ETHER & 9.117 $\pm$ 1.676 & 0.0 $\pm$ 0.0 \\
ETHER+ & 32.75 $\pm$ 6.29 & 0.0 $\pm$ 0.0 \\
\bottomrule
\end{tabular}
\vspace{-10pt}
\end{wraptable}

\textbf{Hypothesis.} Strong of the results showing similar RL performance betweem ETHER and ETHER+, we now investigate the alignment between the emergent languages wielded by the RG agents and the benchmark's natural-like language.
We hypothesise that only ETHER+ provides some linguistic alignment because ETHER has no incentives to do so \textbf{(H1)}.

\textbf{Evaluation.} We propose two metrics, referred to as 'Any-Colour' and 'Any-Shape' accuracies, to report on the alignment between the emergent languages and the benchmark's natural language. 
Each metric is consistent with a different attribute of the objects encountered by the RL agent in the environment, to wit 'colour' and 'shape'.
For the purpose of their computation, we use private information from the BabyAI benchmark that corresponds to the symbolic representation of the agent's field of view (as opposed to the pixel observation that the agents have as input state from the environment), here after referred to as symbolic image. 
The symbolic image describes the colour and shapes of the objects that are in the field of view of the agent using indices \footnote{cf. \url{https://minigrid.farama.org/api/wrappers/\#symbolic-obs}}. 
For each observation used in the RG training, we convert the indices of the corresponding symbolic image into colour and shape word tokens and check whether the RG's speaker agent use any of those word token in its emergent language description of the current observation.
Thus, the 'Any-Colour' accuracy metric registers high accuracy for the current observation is and only if \textbf{any} of the visible object's colour-related word tokens is used, and vice versa with shape-related word tokens for the 'Any-Shape' accuracy metric.
It is important to understand that these metrics only provides a lower bound to the true linguistic alignment between the emergent languages and the benchmark's natural language.
Indeed, they do not allow verification of whether each word token of a given colour or shape are related to their expected semantic with a one-to-one/bijective relationship.
Nevertheless, these metrics allow verification of whether the colour and shape information are being consistently used by the RG agents in ways that allows for interpretation of the emergent language utterances.


\textbf{Results.} Table~\ref{tab:performance-alignment} reports the 'Any-Colour' and 'Any-Shape' accuracies after $200$k RL observations, showing the extent to which the RG's speaker agent has been using any of the natural language colour and shape word tokens to describe stimuli containing said colour or shape as visual feature. 
The results show that constraining of the RG agents using the semantic co-occurrence grounding loss in ETHER+ does start to provide alignment between the emergent language and the benchmark's natural-like language regarding the colour semantic alone (roughly $32\%$ accuracy), as the shape semantic remains ungrounded ($0\%$ accuracy).
Compared to ETHER's $9\%$ of 'Any-Colour' accuracy, which is close to random performance on this metric, the results validate our hypothesis \textbf{(H1)}.

%The latter possibly explains the current performance of the agents not exceeding $33\%$ of success ratio, which corresponds to the random success ratio \textbf{if there was only the shape semantic to discriminate against}. 
%Indeed, the task proposes only 3 different shape semantics: the ball, the key, and the box.

While our proposed metrics here are limited, we highlight that we did also experiment with the conjunctions counterparts' metrics, the 'All-Colour' and 'All-Shape' metrics, as well as the 'Any-Object' and 'All-Object' metrics (checking whether any/all objects are mentioned, in terms of both their colour \textbf{and} their shape ; in order to disambiguate from the case where the emergent language description would make use of the colour-related token for a first visible object and the shape-related token to another visible object rather than the same as the colour-related token's one), but none of the architecture were able to perform better than $0\%$ on these metrics.
We plan to investigate in further details in future works how to make progress on these more difficult alignment metrics, as well as include metrics related to the structure of the language, such as compositionality as it has been shown to improve learning~\citep{Jiang2019-language-abstraction-hierarchical-rl}.

% \begin{wraptable}[R]{0.5\textwidth}
% \centering
%     \renewcommand{\arraystretch}{1.5}
%     \begin{tabular}{c|c|c}
%     \textbf{Agent} & \textbf{Success Ratio} & \\ \hline
%     ETHER & 27.63 $\pm$ 1.20 & \\ \hline
%     ETHER+ & 27.16 $\pm$ 2.57 & \\ \hline
%     \end{tabular}
% \caption{Success ratios ( percentage mean and standard deviation) for agents with burnin feature of R2D2 after 200k steps in a modified version of the BabyAI PickUpDist task.}
% \label{tab:Higher-PickUp}
% \end{wraptable}