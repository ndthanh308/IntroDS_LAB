\section{Introduction}

% Natural languages are powerful tools to describe reality as one can sense it, in all its complexity (e.g. in engineering terms), and even as one can imagine it (e.g. via the poetic function of languages, as defined by \citet{jakobson1960linguistics}). Linguistic properties, such as compositionality and recursivity, make natural languages flexible interfaces in so much so that they allow human beings to express infinitely many means via a finite set of tools. 
% Thus language-conditioned reinforcement learning (RL) promises more flexible and versatile artificial agents, able to follow a great variety of natural language instructions. \\

Since time immemorial, natural languages have been harnessed by humans as powerful tools to describe not only reality as one senses it, but also as one imagines it (e.g. via the poetic function of languages~\cite{jakobson1960linguistics}). Through properties such as compositionality and recursive syntax natural languages become flexible interfaces that allow humans to express arbitrarily complex meanings. Beyond being immensely useful for inter-human communication, natural languages can also be a fruitful means of communication between humans and AI models, as recently showed by the advent of large language models \cite{touvron2023llama}.
The use of natural languages to condition the behaviour of reinforcement learning (RL) agents remains an open question with an untapped potential~\cite{luketina2019survey}. 
In its most general form, how to train RL agents capable of achieving an arbitrary set of goals is the fundamental question within goal-conditioned RL. 
Language-conditioned RL addresses the challenge of training agents to attain a broad array of objectives, utilizing natural languages as an expressive and intuitive tool to define those goals.

To be able to describe goals in natural language and have language-conditioned RL agents learn well performing policies is already very useful. However, even optimal policies might not always be able to accomplish a task. For instance, a cleaning robot might not being able to wash the dishes in the sink if there is no soap left. In this scenario, to close the human-AI communication loop an agent could communicate that it succeeded at other goals such as \textit{pick up a plate} and \textit{turn water tap on}. This capability would greatly contribute towards agent explainability, yet it posits a hard question to answer: how may the agent learn in an unsupervised manner a communication protocol whose semantics align with the semantics of the language used to describe the goals it trains on? In other words, how may an agent learn to communicate whether it succeeded at \textit{pick a plate} when it initially has no notion of what a \textit{plate} is and what it means to \textit{pick}?

To tackle the challenge of language-conditioned RL and the learning of aligned emergent communication protocols we present Emerging Textual Hindsight Experience Replay (ETHER). Agents trained with ETHER learn a function mapping observed states to goals that the agents have reached, a missing piece in current language-conditioned RL, and further improves on the sample efficiency its state-of-the-art counterparts.

Our main contributions are threefold. 
Firstly, we extend HIGhER by enabling its deployment in any instruction-following task out-of-the-box without relying on any oracle. 
This is achieved by means of a learned, approximate predicate function, which we detail in Section~\ref{sec:extending-higher}.
Secondly, in order to further leverage unsuccessful trajectories, we propose the Emergent Textual Hindsight Experience Replay (ETHER) architecture, which builds on HIGhER and addresses both of its limitations, showing that a discriminative visual referential game is a viable unsupervised auxiliary task for RL~\citep{jaderberg2016UNREAL}. 
This is detailed in Section~\ref{sec:rg-as-unsupervised-rl-task}. 
Finally, facing the common problem of the emergent language shifting from natural languages, despite the instruction-following task making use of a natural-like language, we show that it is possible to align to some extent the emergent language with the natural language of the instruction-following benchmark by leveraging the semantic co-occurrence of visual and textual concepts.
%Then, the listener agent learns in an unsupervised fashion a predicate function that can be used in place of the oracle predicate function that HiGHER relied on.
Taken together, our work shows that emergent communication is a viable unsupervised auxiliary task for goal-conditioned RL in sparse reward settings and provides missing pieces to make HER more widely applicable.

We continue by reviewing necessary background and notation in Section~\ref{sec:background}. 
After delineating our methods in Section~\ref{sec:extending-higher}, we present experimental results on the PickUpDist instruction-following task of the BabyAI benchmark~\citep{Chevalier-Boisvert2018} in Section~\ref{sec:experiments}. 
Importantly, our results demonstrate that on a $200k$ observation budget our final agent method achieves almost twice the performance of the baseline HIGhER. 
Finally, %after having discussed some related works and the the relevant future works in Section~\ref{sec:discussion}, 
we conclude in Section~\ref{sec:conclusion}.
