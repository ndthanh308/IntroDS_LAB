\section{Background \& Notation}
\label{sec:background}

\subsection{Goal-Conditioned Reinforcement Learning}

In goal-conditioned RL, a goal-conditioned agent makes use of a policy $\pi : \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$ to interact at each time step $t$ with an environment to maximize its cumulative discounted reward over each episode $\sum_t \gamma^t r(s_t, a_t, s_{t+1}, g_t)$, where $\gamma \in [0, 1]$ is the discount factor, $r : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{G} \rightarrow \mathbb{R}$ is the environment-defined goal-conditioned reward function over the state space $\mathcal{S}$, action space $\mathcal{A}$, and goal space $\mathcal{G}$. 
It interacts by choosing an action $a_t \in \mathcal{A}$ based on the state $s_t \in \mathcal{S}$ it is in and a predefined goal $g\in\mathcal{G}$ sampled at the beginning of the episode. 
Along with the output of the reward function, the agent is provided at each interaction with the next state $s_{t+1}$ sampled from the transition distribution $T(s_{t+1}|s_t, a_t )$.  
We employ a goal-conditioned Q-function, i.e. Universal Value Function Approximator \citep{schaul2015universal}, defined by $Q_\pi (s,a, g) = \mathbb{E}_\pi [\sum_t \gamma^t r(s_t, a_t, s_{t+1}, g) | s_0 = s, a_0 = a, s_{t+1} \sim T]$ for  all $(s, a, g) \in \mathcal{S} \times \mathcal {A} \times \mathcal{G}$.
While previous works makes use of Deep Q-learning (DQN)\citep{DBLP:journals/corr/MnihKSGAWR13} to evaluate the Q-function with neural networks and perform off-policy updates by sampling transitions $(s_t, a_t, r_t, s_{t+1}, g)$ from a replay buffer, we employ Recurrent Replay with Distributed DQN (R2D2) \citep{kapturowski2018recurrent}.

\subsection{Hindsight Experience Replay and its Limitations}
\label{subsec:her-limitations}

In the context of goal-conditioned RL, rewards are inherently sparse for any given goal and this is exacerbated the larger the goal space $\mathcal{G}$ is.
In order to alleviate these issues, \citet{andrychowicz2017hindsight} proposed Hindsight Experience Replay (HER) which involves relabelling unsucessful (null-reward) trajectories where the agent failed to reach the sampled goal $g \in \mathcal{G}$, with a new goal $g^\prime \in \mathcal{G}$ that is actually found to be fulfilled in the final state of the relabelled episode. 
This approach improves the sample efficiency of off-policy RL algorithms %reduces the sparse reward problem 
by taking advantage of failed trajectories, repurposing them by reassigning them to the goals that were actually achieved.
%In doing so, it improves the sample efficiency of off-policy RL algorithms, such  as  DQN and R2D2. 

In effect, for each unsuccessful trajectory, the agent's memory/replay buffer is updated with one negative trajectory and an additional positive (relabelled) trajectory.
In order to do so, HER assumes the existence of a mapping/re-labelling function $m:S\rightarrow G$, which is an oracle (i.e., externally providing expert knowledge of the environment to the algorithm). It maps a state $s$ onto a goal $g$ that is achieved in this state. As their experiments deal with spatial goals, vanilla HER can extract the re-labelling goal from the achieved state (because $\mathcal{G}=\mathcal{S}$), but in the more general case, it cannot be applied without an external expert as this re-labelling oracle cannot be derived. HER's need for expert interventions drastically reduces its interest and range of applicable use cases. \\

HER also assumes the existence of a predicate function $f:\mathcal{S}\times \mathcal{G} \rightarrow \{0, 1\}$ which encodes whether the agent in a state $s$ satisfies a given goal $g$. This predicate function $f$ is used to define the \textbf{learning reward function} $r_{learning}(s_t, a_t, s_{t+1}, g) = f(s_{t+1}, g)$, that is used to infer the reward at each timestep of the re-labelled trajectories. 
Indeed, while at the beginning of an episode a goal $g$ is drawn from the space $\mathcal{G}$ of goals by the environment and, at each time step $t$, the transition $(s_t, a_t, r_t, s_{t+1}, g)$ is stored in the agent's memory/replay buffer with the rewards coming from what we will refer to as the \textbf{behavioral reward function}, i.e. the reward function instantiated by the environment, re-labelling involves using another reward function: at the end of an unsuccessful episode of length $T$, re-labelling and reward prediction occurs in order to store a seemingly-successful (relabelled) trajectory: an \textit{alternative} goal $\hat{g}^0$ and corresponding reward sequence $(r^0_t)_{t\in[0,T]}$ are inferred using the learning reward function (detailed below). 
New transitions $(s_t, a_t, r^0_t, s_{t+1}, \hat{g}^0)_{t\in[0,T]}$ are thus added to the replay buffer for each time step $t$. \\

HER offers two strategies to infer an alternative goal. 
Firstly, the \textbf{final} strategy infers an alternative goal using the re-labelling/mapping function on the \textbf{final} state of the unsuccessful trajectory of length $T$, $\hat{g}^0 = m(s_T)$, and the corresponding rewards are computed via the learning reward function using the predicate oracle $f$,  $\forall t \in [0,T-1], r^0_t = r_{learning}(s_t, a_t, s_{t+1}, \hat{g}^0) = f(s_{t+1}, \hat{g}^0)$. 
%DQN update rule remains identical to [29], transitions are sampled from the replay buffer, and the network is updated using one-step td-error minimization. \\
Or, any of the \textbf{future-k} strategies can be used, with $k\in\mathbb{N}$ being an hyperparameter. 
They consist of applying the \textbf{final} strategy to $k$ different, contiguous sub-parts of the main trajectory. 


\subsection{HIGhER and its Limitations.}
\label{subsec:higher-limitations}

HIGhER\citep{cideron2020higher} aims to expand the applicability of HER, and to do so it explores how to learn the re-labelling/mapping function (hereafter referred to as $m_{HIGhER}$ and Instruction Generator), rather than assuming it is provided or by using some form of external expert knowledge. 
Nevertheless, it still relies on a predicate function being provided and queried as an oracle.
%$f^\prime$ being \textit{partially} provided by the environment's reward signal upon termination of each episode, as we will see further below.
HIGhER investigates using hindsight experience replay in the instruction following setting from pixel-based observations, which brings some particularities as it differs from the robotic setting of HER. 
Firstly, the goal space and state space are no longer the same, hence the motivation towards learning a re-labelling/mapping function.
Secondly, there is no obvious mapping from stimuli (e.g. visual/pixel states) to the instructions that define the goals using a natural-like language. 
For instance, for a given state, due to the expressivity of natural languages, multiple goals may be defined as being fulfilled in this state. 

Despite the non-obvious mapping from stimuli to fulfilled goals, HIGhER still succeeds in learning a deterministic re-labelling/mapping function.
%HIGhER only has access to the predicate value of (state, goal) pairs involving goals as defined by the instruction following task, i.e. not all linguistic description goals $g$ can be evaluated by the predicate oracle function. 
%More specifically, some goal generated by the learned re-labeling/mapping function $m_{HIGhER}$ may not be valid as input to the partial predicate oracle function $f^\prime$.  \\
%On another hand, only a subset of the state space $\mathcal{S}$ may be corresponding to the actual goals defined by the current instruction following task, i.e. $m^{-1}(\mathcal{S}) \neq \mathcal{G}$.
%For instance, instruction following task usually only mention as goals the result of the overarching trajectories (e.g. 'pick up the red ball'), while leaving out relevant information such as all possible linguistic descriptions of the different steps that lead to the resulting goal of the task (e.g. 'walk up to the blue door', 'unlock the blue door', 'enter the room', 'walk right to the red ball').
%While many of the remaining states $s\in\mathcal{S}-m^{-1}(\mathcal{S})$, that are intermediary states to any trajectories, could be described using many \textbf{non-mutually exclusive} natural(/artificial) language utterances. Those ambuigities make the learning of $m_{HIGhER}$ as a predicate function ill-advised. 
%For those states that are not terminal to a successful trajectory and whose linguistic descriptions do not mention the overarching goals of the tasks, the HIGhER architecture does not have access to their predicate values.
%The HIGhER architecture does not even attempt to leverage the information contained in those kind of states and linguistic descriptions because it cannot generate those linguistic descriptions.
HIGhER learns an instruction generator by supervised learning on a dataset $\mathcal{D}_{sup}=\{(s,g) / f(s,g)=1\}$ consisting of state-goal pairs where the predicate value is know to be $1$. 
These pairs are harvested from successful trajectories of the RL agent which occur throughout the learning process (they could be provided as demonstrations, but this is not explored by the original work of \citet{cideron2020higher}) and correspond to final states $s$ of successful/positive-reward trajectories along with relevant linguistic instructions defining the fulfilled goals $g$. 
The ability to harvest successful trajectories from an RL agent in the process of being trained is capped by how likely is it that this RL agent will fulfill goals albeit while randomly/cluelessly exploring the environment.
Thus, one major limitation of HIGhER is that in the absence of initial (and therefore random - when harvested from the learning RL agent) successful trajectories, the dataset $\mathcal{D}_{sup}$ cannot be built, and it ensues that the hindsight experience replay scheme cannot be leveraged since the instruction generator/mapping function, $m_{HIGhER}$, cannot begin to learn.

Finally, it is important to note that HIGhER is constrained to using only the \textbf{final} re-labelling strategy.
Recall that the Instruction Generator is solely trained on episode's final state, and it is likely that the distribution of final states over the whole state space $\mathcal{S}$ is far from being uniform.
Thus, applying the re-labelling/mapping function of HIGhER on states encountered in the middle of an episode is tantamount to out-of-distribution application and would likely result in unpredictable re-labelling mistakes.
In the original work of \citet{cideron2020higher}, this particularity is not addressed, and only the \textbf{final} re-labelling strategy is experimented with.

%which only consist of achieved goal descriptions, i.e. the ones defined by the instruction following tasks.
%In other words, the expressivity of the re-labelling/mapping function is artificially capped by the design choices made by the creators of the instruction following tasks. 

% Figure environment removed


\subsection{Emergent Communication} 
\label{subsec:emecom}

Emergent Communication is at the interface of language grounding and language emergence. 
While language emergence raises the question of how to make artificial languages emerge, possibly with similar properties to natural languages, such as  compositionality \citep{Baroni2019, Guo2019, Li&Bowling2019, Ren2020}, language grounding is concerned with the ability to ground the meaning of (natural) language utterances into some sensory processes, e.g. the  visual modality. 
On one hand, emergent artificial languages' compositionality has been shown to further the learnability of said languages \citep{kirby2002learning, Smith2003, Brighton2002, Li&Bowling2019} and, on the other hand, natural languages' compositionality promises to increase the generalisation ability of the artificial agent that would be able to rely on them as a grounding signal, as it has been found to produce learned representations that generalise, when measured in terms of the data-efficiency of subsequent transfer and/or curriculum learning \citep{Higgins2017SCAN, Mordatch2017, MoritzHermann2017, Jiang2019-language-abstraction-hierarchical-rl}. 
%More in touch with the current context of this study, \citet{Chaabouni2020} showed that, when a specific kind of compositionality is found in the emerging languages (the kind that scores high on the positional disentanglement (posdis) metric for compositionality that they proposed), then it is a sufficient condition for systematicity to emerge.
Yet, emerging languages are far from being `natural-like' protolanguages \citep{Kottur2017,Chaabouni2019a,Chaabouni2019b}, and the questions of how to constraint them to a specific semantic or a specific syntax remain open problems. 
Nevertheless, some sufficient conditions can be found to further the emergence of compositional languages and generalising learned representations (e.g. ~\citet{Kottur2017, Lazaridou2018, Choi2018, Bogin2018, Guo2019, Korbak2019, Chaabouni2020, DenamganaiAndWalker2020b}). 
%Nevertheless, the ability of neural networks to generalise in a systematic fashion has been called into question, especially when it comes to language grounding in general~\cite{Hill2019-tm}, on relational reasoning tasks~\cite{Bahdanau2019}, or on the SCAN benchmark ~\citep{Lake&Baroni2018, Loula2018, Liska2018}, and more recently the gSCAN benchmark~\cite{Ruis2020-vj}. 
%Neural networks induction biases have been investigated towards finding necessary conditions that favour the emergence of systematicity~\citep{Hill2019-tm, Slowik2020, Korrel2019, Lake2019, Russin2019}. 

The backbone of the field rests on games that emphasise the functionality of languages, namely, the ability to efficiently communicate and coordinate between agents. The first instance of such an environment is the \textit{Signaling Game} or \textit{Referential Game (RG)} by \citet{lewis1969convention}, where a speaker agent is asked to send a message to the listener agent, based on the \textit{state/stimulus} of the world that it observed. 
The listener agent then acts upon the observation of the message by choosing one of the \textit{actions} available to it in order to perform the `best' \textit{action} given the observed \textit{state} depending on the notion of `best' \textit{action} being defined by the interests common to both players.
In RGs, typically, the listener action is to discriminate between a target stimulus, observed by the speaker and prompting its message generation, and some other distractor stimuli.
The listener must discriminate correctly while relying solely on the speaker's message.
The latter defined the discriminative variant, as opposed to the generative variant where the listener agent must reconstruct/generate the whole target stimulus (usually played with symbolic stimuli).
Visual (discriminative) RGs have been shown to be well-suited for unsupervised representation learning, either by competing with state-of-the-art self-supervised learning approaches on upstream classification tasks \citep{Dessi2021-emecom_as_ssl}, or because they have been found to further some forms of disentanglement~\cite{Higgins2018, Kim2018, Chen_2018-MIG, Locatello2020-cx} in learned representations \citep{Xu2022-COMPODIS,Denamganai2023visual-COMPODIS}. 
Such properties can enable ``better up-stream performance''\cite{Van_Steenkiste2019-xm}, greater sample-efficiency, and some form of (systematic) generalization~\cite{montero2021the, Higgins2017DARLA, Steenbrugge2018-sq}. 
Indeed, disentanglement is thought to reflect the compositional structure of the world, thus disentangled learned representations ought to enable an agent wielding them to generalize along those lines. 
%The work of \citet{Chaabouni2020} showed that, in the context of generative, symbolic (i.e. disentangled stimuli) referential games, the degree of compositionality of the emerging languages and the agents ability to generalize to zero-shot stimuli are not correlated, but (i) ``when a language is positionally disentangled (and, to a lesser extent, bag-of-symbols disentangled), it is very likely that the language will be able to generalize -- a guarantee we do not have from less informative topographic similarity'', and (ii) the data regime (e.g. low or high) is a better predictor for generalization (i.e. ``generalization emerges `naturally' if the input space if large'').
Thus, this paper aims to investigate visual discriminative RGs as auxiliary tasks for RL agents.

\textbf{Visual Discriminative Referential Game Setup.} Following the nomenclature proposed in \citet{DenamganaiAndWalker2020a}, we will focus primarily on a \textit{descriptive object-centric (partially-observable) $2$-players/$L=10$-signal/$N=0$-round/$K=31$-distractor} RG variant, as illustrated in Figure~\ref{fig:ether-rg}.

As an object-centric RG, as opposed to stimulus-centric, the listener and speaker agents are not being presented with the same exact target stimuli. 
Rather, they are being presented with different \textit{viewpoints} on the same target object shown in the target stimuli, where the word \textit{viewpoint} ought to be understood in a large sense. 
Indeed, object-centrism is implemented by applying data augmentation schemes such as gaussian blur, color jitter, and affine transformations, as proposed in \citet{Dessi2021-emecom_as_ssl}. 
Thus, the listener and speaker agents would be presented with different stimuli that nevertheless keeps the conceptual object being presented constant.
This aspect was introduced by \citet{Choi2018} (without it being of primary interest), where the pair of agents would literally be shown potentially the same 3D objects under different viewpoint, thus thinking of object-centrism as a \textit{viewpoint} shift is historically relevant.

Concerning the communication channel, it is parameterised with a Straight-Through Gumbel-Softmax (STGS) estimator following the work of \citet{Havrylov2017}. 
The vocabulary $V$ is fixed with $62$ ungrounded symbols, plus two grounded symbol accounting for the \textit{Start-of-Sentence} and \textit{End-of-Sentence} semantic, thus $|V|=64$. 
The maximum sentence length $L$ is always equal to $10$, thus placing our experiments in the context of an overcomplete communication channel whose capacity is far greater than the number of different meanings that the agents would encounter in our experiments~\citep{Kottur2018}.

In this paper, we will focus exclusively on STGS parameterisation, but many other could have been used (e.g. REINFORCE-based algorithms~\citep{williams1992simple}, quantization~\citep{carmeli2022quantizedEmeCom} and Obverter approaches~\citep{Choi2018,Bogin2018}).
Indeed, the STGS approach supposedly allows a richer signal towards solving the credit assignment problem that language emergence poses, since the gradient can be backpropagated from the listener agent to the speaker agent.
%, while, in comparison, it cannot be backpropagated when using more commonly adopted approaches based on. 
%This is due to the fact that the gradient can be backpropagated through the listener agent to the speaker agent, thus giving the speaker agent a gradient related to the listener agent's own inner 'reasoning'. 
%\todo[inline]{Having described the RG setup, the following section provides details on the architecture of the \textit{speaker} and \textit{listener} agents and the dataset used
%\footnote{For more details, please refer to our code released at: \url{https://github.com/Near32/ReferentialGym/tree/develop/zoo/referential-games\%2Bcompositionality\%2Bdisentanglement}.}.
%}
%\todo[inline]{Linguistic Functions. Discuss \citet{Wu2021-entropy-decomposition}'s entropy decomposition trick to align with \citet{jakobson1960linguistics}'s functions of language, and how \citet{Lowe2019}'s concepts of positive signalling and positive listening factors in, in order to emphasise how our ETHER builds over each of those.}
%\todo[inline]{Discuss that ETHER allows using the future strategy from HER, whereas THER does not ; thanks to meaningful rewards along the way of multi-objective instructions, e.g. opening doors before picking up an object.}
