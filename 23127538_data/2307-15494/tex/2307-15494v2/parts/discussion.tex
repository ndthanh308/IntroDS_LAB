\section{Discussion}
\label{sec:discussion}

%\todo[inline]{ \textbf{Emergent Communication in language-conditioned RL.} It can be beneficial especially in the context of HER, due to the gap that the speaker and the listener agent fills with respect to HER and extensions towards language-oriented HER, HIGhER\citep{cideron2020higher}.}

Despite the use of the semantic co-occurrence grounding loss, the issue of how to constrain the emergent language to be as close as possible to the benchmark's natural language broadly, remains. 

%\todo[inline, color=orange]{Hierarchical Reinforcement Learning.}

%\todo[inline, color=orange]{Hierarchical Latent Policy Learning. \citet{Sharma2021-sl3} and previous papers on latent language policy learning...}

% \textbf{Representation or Algorithm Learning.} 
% \citet{Co-Reyes2018-jl} proposes a meta-learner agent that learns to correct its behaviour based on corrections provided in the form of natural language utterances. 
% On the contrary to ours, their agent cannot be trained from scratch, as it relies on constraining assumptions, such as access to a correction function (taking a trajectory as input, it outputs a natural language correction utterance), which is reminiscent of the re-labelling/mapping function $m$ in HER, or access to an optimal policy for each meta-training tasks the agent is meta-trained on. 
% Nevertheless, our work similarly exploits a guidance signal, in the form of language utterances, for re-labelling purposes.
% A final discrepancy worth noting is that our guidance signal is used in an offline fashion, in-between RL episodes, towards building generalisable learned representations, whereas theirs is used in an online fashion, within RL episodes, in order to feed a cognitive-like learned algorithm.
% We mean to emphasise that their agent learns an algorithm (how to process corrections towards better performance), while our agent learns representations (enabling better performance).

%\todo[inline]{Mention future works: using our relabelling approach to learn an algorithm worth learning, e.g. how to debate options by attending and showing different evidence in past experiences}

We suggest that future work leverages a hierarchical RL paradigm.
Indeed, following the approaches in \citet{levy2018hierarchical} and \citet{MulBouchacourtBruni2019}, we plan to investigate the use of emergent language utterances as options to a hierarchical RL architecture where only the option-following part of the policy (conditioned on emergent language utterances) is trained with a hindsight experience replay scheme as a pre-training taking place before the option-selecting part of the policy (conditioned on the actual episode's instruction) is trained.

%\todo[inline]{Architecture is based off CNN with channel-wise output assumed to be sequences, which is a possible inductive bias among others, similar in nature to how Visual Transformers operate ; future works should investigate other possible biases.}

Furthermore, the current study did not investigate the systematic generalisation abilities of the resulting agent. 
This is an important task that will also be addressed in subsequent works.