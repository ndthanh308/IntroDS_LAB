\section{On the Semantic Co-Occurrence Grounding Loss}
\label{sec:co-occurrence-grounding}

In the following, we describe further how the semantic co-occurrence grounding loss is implemented in the ETHER+ architecture.
In Section~\ref{subsec:co-occurrence}, we introduced the \textbf{semantic co-occurrence grounding loss}, which aims to enhance an agent's language grounding ability during RG training. 
To do so, only the words/tokens present in the linguistic goal description provided are used as labels. 
Formally, let us define a linguistic goal description as a series of tokens, $g=(g_i)_{i\in [1,L]} \in \mathcal{G}$, where $L$ is the maximum sentence length hyperparameter, as defined in the RG setup.

Thus, for each of those token present in the goal $g$ of a given episode (out of all the tokens available in the vocabulary $V$, as defined in the RG setup), the semantic co-occurrence grounding loss will aim to bring a prior semantic-only embedding of the tokens closer to the visual embeddings of all the observations during the given episode.
We will denote by $(\lambda_w)_{w\in V}$ all the prior semantic-only embeddings for the vocabulary V.
And, on the other hand, it will also bring further away from the visual embeddings of all the observations during the episode the prior semantic-only embeddings of \textbf{all the tokens of the vocabulary that are not present in the current goal $g$}.

The semantic co-occurrence grounding loss is contrastive and inspired by \citet{radford2021CLIP}. 
More formally, as we defined $f(\cdot)$ as the visual module in Section~\ref{app:model-architecture}, we write the semantic co-occurrence grounding loss as follows:

\begin{equation}
\label{eq:semantic-co-occurrence-grounding-loss}
    \mathcal{L}^{sem.}_{co-occ.\, ground} ( g | (\lambda_w)_{w\in V} ) = \mathbb{E}_{s \sim \rho^\pi} \Biggl[ \sum_{w\in V} \mathcal{H}(w) \sum_{g_i \in g} \biggl( \mathbf{1}_{w}(g_i) - \frac{\lambda_w \cdot f(s)^T}{||\lambda_w ||_2 \cdot ||f(s)||_2} \biggr)^2 \Biggr],
\end{equation}

where $||\cdot||_2$ corresponds to the $L2$ norm, $\rho^\pi$ is the distribution over states in $s\in\mathcal{S}$ that is induced by using the policy $\pi$ to harvest the observations/stimuli, and ${1}_{w}(\cdot)$ is a noisy indicator function defined as follows:
\begin{equation}
\label{eq:indicator-function}
    \mathbf {1}_{w}(w^\prime):= (1-\epsilon_{noise}) \times \begin{cases}1~&{\text{ if }}~w^\prime = = w~,\\-1~&{\text{ if }}~w^\prime \neq w~.\end{cases}
\end{equation}
where $\epsilon_{noise}$ is some random noise uniformly sampled from $[0,0.2]$, following the noisy labels idea proposed in \citet{salimans2016improved-techniques-for-training-gans}.

As the loss is implemented over mini-batches of sampled stimuli, we also performing masking to reject tokens with null entropy over the mini-batch. For instance, in the proposed experiments performed on BabyAI~\citep{Chevalier-Boisvert2018}'s PickupDist-v0 task, the linguistic goal description always contains the prefix `pick up', therefore, when considering a mini-batch of stimuli (however they may come from different episodes), the likelihood of the tokens `pick' and `up' is maximal over the mini-batch and therefore their associated appearance distribution over the sampled stimuli will have null entropy. In Equation~\ref{eq:semantic-co-occurrence-grounding-loss}, $\mathcal{H}(w)$ denote the entropy of the appearance distribution of token $w\in V$.

\begin{comment}
\todo[inline]{address bootstrappig concerns of learning both f and lambdas at the same time}

\todo[inline]{talk about BYOL and self-supervised inspiration}
\end{comment}