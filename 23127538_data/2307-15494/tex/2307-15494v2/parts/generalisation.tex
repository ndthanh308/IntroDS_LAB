
\subsection{Generalisation Abilities}
\label{sec:generalisation}

While many studies have investigated the generalisation abilities of neural networks in the context of linguistic tasks (e.g. ~\citet{Kottur2017, Lake&Baroni2018, Resnick2019}), their assumed definition to the act of generalisation do not always coincide~\cite{Hupkes2019}. Following the work of ~\citet{Lake&Baroni2018} showing that recurrent neural networks (RNNs) fail to generalize systematically but are very successful at generalizing when enough supporting evidence is provided. It is worth emphasizing again, like many previous authors ~\citep{Bahdanau2019, Hupkes2019, Russin2019, Chollet2019}, that it is critical to clearly state what kind of generalisation abilities this study aims to evaluate. The remainder of this section proposes an in-depth examination of the range of possible generalisation abilities and suggests where this paper lies within those identified.

While some studies test for what is referred to as systematic generalisation abilities ~\cite{Kottur2017, Lake&Baroni2018, Liska2018, Korrel2019, Bahdanau2019, Russin2019}, others seem to test for (vanilla) generalisation abilities~\cite{Andreas2019, Hupkes2019, Chaabouni2020}. This work differentiates between the two in terms of the size of the pool of supporting evidence/samples, which the learning system is trained on. Note that pool size is assumed to vary the difficulty of the task. Thus, (hard) systematic generalisation being on the side of the spectrum where the pool size is the smallest, and (easy) (vanilla) generalisation being on the other side where it is the greatest. In this work, we focus on systematic generalisation, rather than vanilla, and we adopt ~\citet{Bahdanau2019}'s definition of systematic generalisation abilities (i.e. a model that has systematic generalisation abilities  ``should be able to reason about all possible object combinations despite being trained on a very small subset of them''). This definition is a continuation of our assumed definition for compositional abilities, particularly ``the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents''\citep{Fodor&Pylyshyn1988}. In other words, the kind of generalisation abilities we propose to test for could be referred as systematic combinatorial/compositional generalisation abilities.

%\subsection{Local vs. Broad Generalisation in Language Emergence \& Grounding}
%\label{sec:gen:local-vs-broad}

%Following the emphasis made by ~\cite{Chollet2019}, we acknowledge that \textit{local generalisation} --or ``robustness''-- is currently our main object of focus in the field of language emergence, and that we are only starting to attain a state-of-the-art approaching the highest part of that spectrum, although still far from the lowest part of the spectrum of \textit{broad generalisation} --or ``flexibility''--. Considering evaluation for systematic generalisation abilities, while the stimuli we consider in test sets are novel combinations of known components, we are still only testing for adaptability to ``known unknowns within a single task or well-defined set of tasks''~\citep{Chollet2019} (when taking into account the developer knowledge of the task and assuming this knowledge permeated into the design choices of the learning agents).

%Similar concerns can be seen at the interface between natural language grounding and reinforcement learning~\citep{Hill2019}, or even visual relational reasoning~\citep{Bahdanau2019}. Being concerned with both language emergence and grounding \textit{in the visual modality}, our work sits at a similar crossroad.

\subsection{Emergent Systematicity \& Prior Knowledge}
\label{sec:gen:emergent-systematicity}

The work of ~\citet{Hill2019} is particularly relevant to our current context, as it evaluates a specific kind of systematic generalisation referred to as ``emergent systematicity'' (~\citet{Hill2019} citing ~\citet{mcclelland2010letting}), ``because the architecture of our agent does not include components that are explicitly engineered to promote systematicity'' (i.e. systematic generalisation abilities). The concurrent work of ~\citet{Chaabouni2020} also tackles ``emergent'' systematicity/systematic generalisation. The learning agents will be equipped with CNNs and RNNs (see details in Appendix~\ref{app:model-architecture}), giving them some form of spatial/translational invariance prior, hierarchical/distributed visual feature prior, and sequence-computation-enabling prior. 

%\citet{Chollet2019} acknowledges that \textbf{prior knowledge} (e.g. explicitly engineered modules promoting a given aspect that the developers, who are aware of the tasks, would see fit to incorportate into the system) and \textbf{experience} (e.g. in the form of almost unlimited training data) ``allow experimenters to `buy' arbitrary levels of skills for a system, in a way that masks the system's own generalization power''. 
\citet{Chollet2019} emphasizes that \textbf{priors}, \textbf{experience}, and \textbf{generalisation difficulty} must be controlled in order to reliably evaluate broad generalisation abilities. Acknowledging the context of emergent systematicity in which this study lies can therefore be understood as a step towards controlling the prior knowledge baked into deep learning agents when evaluating their generalisation abilities. While the matter of controlling the generalisation difficulty is difficult to address in the current context, it is broached upon in Appendix~\ref{sec:gen-difficulty}. The matter of how to control each for the amount of experience available to the agent is discussed in Section~\ref{sec:evaluation-methods}.%, and provides a surface-level understanding of the generalisation difficulty involved in the different tasks described in Section~\ref{sec:gen-difficulty}.


\section{Evaluation Methodology}
\label{sec:evaluation-methods}

In the following experiments, learning agents will observe both symbolic and visual stimuli from particular train/test splits of the dSprites dataset~\citep{dsprites17,Higgins2016}. Symbolic representations are obtained by one-hot-encoding the latent representations that the dataset provides. Originally employed as a benchmark for disentangled representation learning, the dSprites dataset consists of visual representations for combinations of values along some generative factors/attributes/latent axes. There are $32$ possible values on each position axis, X and Y, $40$ possible values on the Orientation axis, $6$ possible values on the Scale axis, and $3$ possible values on the Shape axis. Note that in our experiments the value on the Shape attribute is always fixed to be the \textit{heart} shape in order to remove any orientation ambiguities (e.g. the square shape has four symmetries that prevents visual differentiation from, for instance, a rotation of $90$ degrees and that of $180$ degrees). The choice of the dSprites dataset is motivated by the availability of the generative factors (that are used as symbolic stimuli in their one-hot-encoded form), which permits the computation of topographic similarity~\citep{Brighton&Kirby2006} (following a similar approach to \citet{Lazaridou2018}) to assess the degree of compositionality in the emerging languages, as well as doing so separately from assessing generalisation abilities. We emphasise again how critical the separation of the two measures is given the related and concurrent work of \citet{Chaabouni2020} showing that learning agents are able to generalise in a referential game in spite of their utterances not being compositional.

%Most neural networks implement a mapping function between the inputs and outputs, which is close to a diffeomorphism. It is likely that the local topology in the input training space would play an important role in the ability of the neural network to perform well when tested in the most worthy parts of the generalisation space, whose axes could be understood as: (i) the amount of \textbf{prior knowledge} baked into the system by the developer, (ii) the amount of \textbf{prior experience} available to the system before being tested, and (iii) the \textbf{generalisation difficulty} of the testing phase compared to training phase.

%Therefore, we hypothesise and seek to validate in our experiments that testing generalisation by extrapolation does bear some generalisation difficulty compared to generalisation by interpolation, in spite of the latter currently being the norm in the field of language emergence and grounding. 

%The proposed experiments focus on emergent systematicity, thus controlling the amount of prior knowledge to be as minimal as possible. The learning agents will be equipped with CNNs and RNNs (see details in Appendix~\ref{app:model-architecture}), giving them some form of spatial/translational invariance prior, hierarchical/distributed visual feature prior, and sequence-computation-enabling prior. 

The amount of prior experience will be controlled in two ways. Firstly, by limiting the diversity in the training set, thus constraining our study to very small data set sizes. Secondly, by limiting the number of training epochs to fit to a sample budget of $480,000$ training samples, across the different settings. The number of gradient steps is the sample budget divided by the batch size assumed, which is detailed for each experiments in Section~\ref{sec:exp}.

%In this work, we will evaluate both emergent ``robustness'' and emergent ``flexibility'' towards language emergence and grounding, with different degrees of difficulty on the systematicity spectrum by varying the size of the pool of training examples.

%More specifically, during the following experiments, learning agents will observe visual stimuli/images from particular train/test splits of the dSprites dataset~\cite{dsprites17,Higgins2016} that allow us to evaluate their emergent systematic ``robustness'' and ``flexibility''. 

%TODO: provided transition from train test split strategy to this, as detail for the subsections.
%As emphasized by the literature, the level of structure in the observed meaning space~\citep{Kirby2014,Chaabouni2020} and the capacity of the communication channel~\citep{Kottur2017} have been identified as critical when studying language emergence and grounding. Therefore, these features will also be controlled in order to extend the external validity of the results. 

\subsection{Train-Test Split Strategy}
\label{sec:train-test-split-details}

%In Section~\ref{sec:exp:gen-difficutly}, the generalisation difficulty of different tasks is investigated. 
In the first experiment, we question whether the train-test split strategy matters when testing zero-shot compositional abilities. We arbitrarily propose to look at two simple train-test split strategies leading up to two different sets of tasks that we will refer to as: the interpolation tasks, $T_{inter}$, and the extrapolation tasks, $T_{extra}$. 
Considering that each stimulus can be described as a set of values taken from each attribute/generative factor/latent axis of the dSprites dataset, details of how the train-test splits are performed in each task can be described as follows. In the interpolation tasks, $T_{inter}$, the test sets consist of defining testing-purpose (and not test-only) values alternating with other values, on each attribute axis. In the extrapolation tasks, $T_{extra}$, the test sets consist of defining the testing-purpose (and not test-only) values as the first values (following the ordering of the original dataset) on each attribute axis. We highlight already, and will detail further below, that testing-purpose values are encountered by the agent both at training-time and testing-time, just not in combinations with all other possible values on the other latent axises of which a stimuli is composed.

Independently of the task, half of the possible values for each attribute axis are defined as testing-purpose values. The choice of using half of the possible values is motivated by the results of \citet{Bahdanau2019} when evaluating for emergent systematicity, i.e. a CNN+LSTM architecture similar to that of ours here, and showing that such a benchmark is already challenging enough.
Then, any sampled stimulus that combines \textbf{two or more} testing-purpose values for its different attributes is automatically retained for the test set. In other words, the testing-purpose values on each latent axis are presented to the agent at training-time while in combinations with sole non-testing-purpose values on the other latent axises, in order for the agent to familiarise itself with all the possible values on each latent axis while remaining unaware of a subset of all the combinations possible.  For an intuitive understanding, Figure~\ref{fig:extra-vs-inter} renders $2D$ projection of the symbolic $2$-attributes tasks, $T^{2}_{inter}$ and $T^{2}_{extra}$, and the results of performing t-SNE~\citep{maaten2008visualizing} on the symbolic $3$-attributes tasks, $T^{3}_{inter}$ and $T^{3}_{extra}$, thus highlighting their systematic and topological differences, and revealing the motivation for their naming as interpolation and extrapolation tasks. 
In a similar manner to \citet{Russin2019}, we acknowledge the intuitive difference of difficulty between the two flavours in which generalisation occurs~\cite{marcus2018deep}. These are:  \textit{interpolation}, where ``the train and test sets are independent and identically distributed (i.i.d)'', and \textit{extrapolation}, where the tested system is required to make ``an inferential leap about the entire structure of part of the distribution that they have not seen'', and so the test set is out-of-domain (o.o.d.) with regards to the train set. When testing for combinatorial generalisation abilities, following \citet{Chollet2019}'s framework, it is hypothesised (and further detail in Appendix~\ref{sec:gen-difficulty}) that tasks involving compositional generalisation by \textit{interpolation}, $T_{inter}$, have \textit{zero generalisation difficulty}, while tasks involving compositional generalisation by \textit{extrapolation}, $T_{extra}$, bear \textit{some generalisation difficulty}. We emphasise again that the naming convention adopted here for the tasks is solely motivated by the intuition provided by figure~\ref{fig:extra-vs-inter}, but the actual nature of the tasks are unknown, since we are dealing with stimuli represented as combinations of attributes. They cannot and should not be confounded with regular interpolation and extrapolation tasks, as far as we know.


\subsection{Control for the Level of Structure in the Meaning Space}

In the subsequent experiments, we will vary the level of structure in the meaning space by experimenting with subsets of the dSprites dataset, emphasizing either: $2$ attributes (position on the X-axis and Y-axis, yielding $48$ training examples and $16$ testing examples), $3$ attributes (similar to $2$ attributes plus Orientation, yielding $256$ examples in each train/test set), or $4$ attributes (similar to $3$ attributes plus Scale, yielding $960$ training examples and $2112$ testing examples). We leave it to future works to vary the level of structure by varying the number of possible values for each attribute.

The employed subsets subsample the original dSprites dataset, in order to reduce the number of samples available to the learning agents. This subsampling aims to control for the amount of prior experience, and thus design a test where the learning agents are not presented with oversized data sets from which to slowly build evidence ~\citep{Lake&Baroni2018, Loula2018, Liska2018}. With the exception of the Scale attribute that only originally contains $6$ possible values (and thus remains unchanged), each other attribute contains $8$ possible values that are sampled from the original data to be evenly spaced out (i.e. out of the $32$ possibles values on the X and Y attributes, we sample every $4$ values; out of the $40$ possibles values on the Orientation attribute, we sample every $5$ values). %From the relevant subsets of the dataset, we build training and testing sets by retaining samples that consist of combinations of specific values for different attributes, which we will refer to as testing-purpose values. In spite of their name, it is important to note that those values are present in the training set, except when combined with at least one other testing-purpose value any another latent axis, thus guaranteeing that the learning agent will be able to familiarise itself with all the possible values on each latent axis, while remaining unfamiliar to some retained combinations, as it is commonly done when testing for systematicity~\citep{Kottur2017,Choi2018,Lazaridou2018,Bahdanau2019}.

\subsection{Control for the Capacity of the Communication Channel}

Two different cases are considered in the following experiments. The \textit{complete} case, where there are exactly $8$ ungrounded symbols in the vocabulary $V$, plus a ninth grounded symbol, in order to account for the \textit{end of sentence} semantic, thus $|V|=9$. The maximum sentence length $L$ is always equal to the number of attributes in the subset on which the experiment takes place. On the other hand, the \textit{overcomplete} case consists of a vocabulary of size $|V|=100$, and maximum sentence length $L=20$.