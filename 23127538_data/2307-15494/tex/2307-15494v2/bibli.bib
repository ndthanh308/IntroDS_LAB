@article{Burda2018,
abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
archivePrefix = {arXiv},
arxivId = {1808.04355},
author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
eprint = {1808.04355},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning(3).pdf:pdf},
month = {aug},
title = {{Large-Scale Study of Curiosity-Driven Learning}},
url = {http://arxiv.org/abs/1808.04355},
year = {2018}
}

@article{Donahue2014,
abstract = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or "temporally deep", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are "doubly deep"' in that they can be compositional in spatial and temporal "layers". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.},
archivePrefix = {arXiv},
arxivId = {1411.4389},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
eprint = {1411.4389},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue et al. - 2014 - Long-term Recurrent Convolutional Networks for Visual Recognition and Description.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Entity Recognition,iGGi/Literature Review,iGGi/Literature Review/Natural Language Generation},
month = {nov},
title = {{Long-term Recurrent Convolutional Networks for Visual Recognition and Description}},
url = {http://arxiv.org/abs/1411.4389},
year = {2014}
}



@INPROCEEDINGS{6060676,
author={K. i. Yamada and N. Hara and K. Konishi},
booktitle={SICE Annual Conference 2011},
title={Circular motion generation for mobile robots using limit cycle systems #x2014; Application to a circular formation control},
year={2011},
pages={342-345},
keywords={limit cycles;mobile robots;motion control;multi-robot systems;circular formation control;circular motion generation;limit cycle systems;multiple mobile robots;Limit-cycles;Mobile robots;Numerical simulation;Sensors;Solids;Trajectory;circular formation;collective motion;mobile robots},
ISSN={pending},
month={Sept},}

@INPROCEEDINGS{5603026,
author={K. Yamada and N. Hara and H. Kokame and K. Konishi},
booktitle={Proceedings of SICE Annual Conference 2010},
title={Implementation and experimental validation of circular periodic motion generation for mobile robots using limit cycle systems},
year={2010},
pages={3391-3394},
keywords={limit cycles;mobile robots;motion control;circular periodic motion generation;e-puck;limit cycle systems;nonholonomic mobile robots;Clocks;Limit-cycles;Mobile robots;Simulation;Solid modeling;Trajectory;experimental validation;implementation;limit cycle;mobile robot;path following control},
month={Aug},}

@article{nakamura2016,
  title={Stability analysis of mobile robot formations based on synchronization of coupled oscillators},
  author={Nakamura, Tadashi and Tsukiji, Miyuki and Hara, Naoyuki and Konishi, Keiji},
  journal={IFAC-PapersOnLine},
  volume={49},
  number={22},
  pages={187--191},
  year={2016},
  publisher={Elsevier}
}


@inproceedings{Panagou2013MultiobjectiveCF,
  title={Multi-objective control for multi-agent systems using Lyapunov-like barrier functions},
  author={Dimitra Panagou and Dusan M. Stipanovic and Petros G. Voulgaris},
  booktitle={CDC},
  year={2013}
}

@inproceedings{Borrmann2015ControlBC,
  title={Control Barrier Certificates for Safe Swarm Behavior},
  author={Urs Borrmann and Li Wang and Aaron D Ames and Magnus Egerstedt},
  year={2015}
}


@article{spears2009,
doi = "10.1108/17563780911005827",
issn = "1756-378X",
title = "Distributed adaptive swarm for obstacle avoidance",
journal = "International Journal of Intelligent Computing and Cybernetics",
publisher = "Emerald",
number = "4",
volume = "2",
pages = "644-671",
author = "William M. Spears",
editor = "Suranga Hettiarachchi",
year = "2009",
month = "11",
day = "20"
}

@article{DBLP:journals/corr/LillicrapHPHETS15,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1509.02971},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.02971},
  timestamp = {Thu, 01 Oct 2015 14:28:48 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LillicrapHPHETS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{DBLP:journals/corr/NairSBAFMPSBPLM15,
  author    = {Arun Nair and
               Praveen Srinivasan and
               Sam Blackwell and
               Cagdas Alcicek and
               Rory Fearon and
               Alessandro De Maria and
               Vedavyas Panneershelvam and
               Mustafa Suleyman and
               Charles Beattie and
               Stig Petersen and
               Shane Legg and
               Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver},
  title     = {Massively Parallel Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1507.04296},
  year      = {2015},
  url       = {http://arxiv.org/abs/1507.04296},
  timestamp = {Sun, 02 Aug 2015 18:42:02 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/NairSBAFMPSBPLM15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{DBLP:journals/corr/MnihKSGAWR13,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  timestamp = {Wed, 01 Apr 2015 20:06:15 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/MnihBMGLHSK16,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  timestamp = {Tue, 01 Mar 2016 17:47:25 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MnihBMGLHSK16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@book{ACEBRON,
author={J.A.Acebron},
title={The Kuramoto Model},
year={2016},
publisher={Reviews of Modern Physics}
}

@book{GROS,
author={C.Gros},
title={Complex adaptative Dynamical Systems},
year={2010},
publisher={Springer}
}


@book{XIN,
title={Exact Solution For First-order Synchronization Transition In A Generalized Kuramoto Model},
author={Xin Hu , S. Boccaletti , Wenwen Huang , Xiyun Zhang , Zonghua Liu , Shuguang Guan, Choy-Heng Lai},
publisher={Scientific Reports},
year={2014},
}

@book{STROGATZ,
title={Stability of Incoherence in a Population of Coupled Oscillators},
author={Steven H. Strogatz, Renato E. Mirollo},
publisher={Journal of Statistical Physics, Vol. 63},
year={1991},
}


@book{AUFFRBD,
author={Helmut Garstenauer},
title={A Unified Framework for Rigid Body Dynamics},
year={2006}
}

@book{GPED,
author={Ian Millington},
title={Game Physics Engine Development},
year={2007}
}

@book{RTCD,
author={Christer Ericson},
title={Real-Time Collision Detection},
year={2004}
}

@book{IBDSORBS,
author={Brian Vincent Mirtich},
title={Impulse-based Dynamics Simulation of Rigid Body Systems},
year={1996}
}

@book{LSDSLAM,
author={J. Engel, T. Schöps, D. Cremers},
title={LSD-SLAM: Large-Scale Direct Monocular SLAM},
year={2014}
}

@book{SDVO,
author={Engel, Jakob AND Sturm, Jürgen AND Cremers, Daniel},
title={Semi-Dense Visual Odometry for a Monocular Camera},
year={2013}
}

@book{MVG,
author={Hartley, Richard AND Zisserman, Andrew},
title={Multiple View Geometry in Computer Vision, 2nd Edition},
publisher={Cambridge University Press},
year={2004}
}


@book{CV:A&A,
title={Computer Vision: Algorithms and Applications},
author={Szeliski, Richard},
year={2010},
publisher={Springer}
}

@book{3DRfMI,
title={3D reconstruction from multiple images},
author={T.Moons, M.Vergauwen, L.Van Gool},
year={2008},
}

@book{IDPMS,
title={Inverse Depth Parametrization for Monocular SLAM},
author={J.Civera, A.J.Davidson, J.M.Martinez Montiel},
year={2008},
}

@book{IDDCMS,
title={Inverse Depth to Depth Conversion for Monocular SLAM},
author={J.Civera, A.J.Davidson, J.M.Martinez Montiel},
year={2007},
}

@book{AMBS,
title={A Multiple-Baseline Stereo},
author={M.Okutomi, T.Kanade},
year={2007},
}

@book{KFBAEDIS,
title={Kalman Filter-based Algorithms for Estimating Depth from Image Sequences},
author={L.Matthies, T.Kanade},
year={2007},
}

@book{ORGBDCAQ,
title={Odometry from RGB-D Cameras for Autonomous Quadrocopters},
author={Christian Kerl},
year={2012},
}

@site{RPG,
title={Robotic \& Perception Group, University of Zurich. http://rpg.ifi.uzh.ch/},
}


@book{AFASSLAMS,
title={A Flexible and Scalable SLAM System with Full 3D Motion Estimation},
author={S.Kohlbrecher, O.von Stryk, J.Meyer, U.Klingauf},
year={2011},
}


@book{HECTORROS,
title={Hector Open Source Modules for Autonomous Mapping and Navigation with Rescue Robots},
author={S.Kohlbrecher, O.von Stryk, J.Meyer, U.Klingauf, T.Graber, K.Petersen},
year={2011},
}

@book{HAYAI,
title={High-speed laser localization for mobile robots},
author={K.Lingemann, A.Nüchter, J.Hertzberg, H.Surmann},
year={2005},
}

@book{HAYAI++,
title={Feature-Based Laser Scan Matching For Accurate and High Speed Mobile Robot Localization},
author={A.Aghamohammadi, H.Taghirad, A.Tamjidi, E.Mihankhah},
year={2006},
}

@book{WICP,
title={Weighted Range Sensor Matching Algorithms for Mobile Robot Displacement Estimation},
author={S.Pfister, K.Kriechbaum, S.Roumeliotis, J.Burdick},
year={2002},
}

@book{WIDL,
title={A Weighted Range Sensor Matching Algorithm for Mobile Robot Displacement Estimation, draft - 17 pages},
author={S.Pfister, K.Kriechbaum, S.Roumeliotis, J.Burdick},
year={2002},
}

@book{MATCHING,
title={Robot Pose Estimation in Unknown Environments by Matching 2D Range Scans},
author={F.Lu, E.Milios},
year={1995},
}

@book{ICP,
title={Efficient Variants of the ICP Algorithm},
author={S.Rusinkiewicz, M.Levoy},
year={2001},
}

@book{TMMTSDA,
title={Tracking Multiple Moving Targets with a Mobile Robot using Particle Filters and Statistical Data Association},
author={D.Schulz, W.Burgard, D.Fox, A.Cremers},
year={2000},
}

@book{SLAMwDATMO,
title={Online Simultaneous Localization And Mapping with Detection And Tracking of Moving Objects : Theory and Results from a Ground Vehicle in Crowded Urban Areas},
author={C.Wang, C.Thorpe, S.Thrun},
year={2002},
}

@book{ABCPinSLAM,
title={An Analysis of the Bias Correction Problem in Simultaneous Localization and Mapping},
author={W.S.Wijesoma, L.D.L.Perera, M.D.Adams, S.Challa},
year={2004},
}

@book{SLAMwJSBE,
title={SLAM with Joint Sensor Bias Estimation : Closed Form Solutions on Observability, Error Bounds and Convergence Rates},
author={L.D.L.Perera, W.S.Wijesoma, M.D.Adams},
year={2004},
}

@article{Jaderberg2015,
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02025v1},
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
doi = {10.1038/nbt.3343},
eprint = {arXiv:1506.02025v1},
isbn = {9781627480031},
issn = {1087-0156},
journal = {Nips},
pages = {1--14},
pmid = {26571099},
title = {{Spatial Transformer Network}},
year = {2015}
}

@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {arXiv:1506.01497v1},
isbn = {0162-8828 VO - PP},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}

@article{Dosovitskiy2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06852v2},
author = {Dosovitskiy, Alexey and Fischer, Philipp and Ilg, Eddy and H{\"{a}}usser, Philip and Hazirbas, Caner and Golkov, Vladimir and van der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
doi = {10.1109/ICCV.2015.316},
eprint = {arXiv:1504.06852v2},
file = {:home/kevin/Bureau/MASTER-OPU/MASTER OPU/MASTER REPORT/BIBLIO/NN/flownet-iccv-15.pdf:pdf},
isbn = {978-1-4673-8391-2},
journal = {{\{}IEEE{\}} International Conference on Computer Vision},
pages = {2758--2766},
title = {{{\{}FlowNet{\}}: {\{}L{\}}earning Optical Flow with Convolutional Networks}},
year = {2015}
}

@article{Imagenet,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}






@article{Wang2018,
author = {Wang, Jane X. and Kurth-Nelson, Zeb and Kumaran, Dharshan and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Hassabis, Demis and Botvinick, Matthew},
doi = {10.1038/s41593-018-0147-8},
file = {:home/kevin/Bureau/295964.full.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {jun},
number = {6},
pages = {860--868},
title = {{Prefrontal cortex as a meta-reinforcement learning system}},
url = {https://www.biorxiv.org/content/biorxiv/early/2018/04/13/295964.full.pdf},
volume = {21},
year = {2018}
}


@article{Gregor2016,
abstract = {We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
archivePrefix = {arXiv},
arxivId = {1604.08772},
author = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
eprint = {1604.08772},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gregor et al. - 2016 - Towards Conceptual Compression(4).pdf:pdf},
mendeley-groups = {iGGi/Literature Review},
month = {apr},
title = {{Towards Conceptual Compression}},
url = {http://arxiv.org/abs/1604.08772},
year = {2016}
}

@article{Zambaldi2018,
abstract = {We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games – surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.},
author = {Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and Shanahan, Murray and Langston, Victoria and Pascanu, Razvan and Botvinick, Matthew and Vinyals, Oriol and Battaglia, Peter and London, Deepmind},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zambaldi et al. - Unknown - Relational Deep Reinforcement Learning(2).pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Entity Recognition},
title = {{Relational Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1806.01830.pdf}
}

@inproceedings{Johnson2017,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2901--2910},
  year={2017}
}


@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
author = {Santoro, Adam and Raposo, David and Barrett, David G T and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy and London, Deepmind},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - Unknown - A simple neural network module for relational reasoning(3).pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Entity Recognition},
title = {{A simple neural network module for relational reasoning}},
url = {https://arxiv.org/pdf/1706.01427.pdf}
}


@techreport{Antoniou2018,
abstract = {Despite their impressive performance in many tasks, deep neural networks often struggle at relational reasoning. This has recently been remedied with the introduction of a plug-in relational module that considers relations between pairs of objects. Unfortunately, this is combinatorially expensive. In this extended abstract, we show that a DenseNet incorporating dilated convolutions excels at relational reasoning on the Sort-of-CLEVR dataset, allowing us to forgo this relational module and its associated expense.},
archivePrefix = {arXiv},
arxivId = {1811.00410v1},
author = {Antoniou, Antreas and S{\l}owik, Agnieszka and Crowley, Elliot J and Storkey, Amos},
eprint = {1811.00410v1},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Antoniou et al. - Unknown - Dilated DenseNets for Relational Reasoning.pdf:pdf},
title = {{Dilated DenseNets for Relational Reasoning}}
}



@techreport{Jahrens2018,
abstract = {Relational Networks (RN) as introduced by Santoro et al. (2017) [1] have demonstrated strong relational reasoning capabilities with a rather shallow architecture. Its single-layer design, however, only considers pairs of information objects, making it unsuitable for problems requiring reasoning across a higher number of facts. To overcome this limitation, we propose a multi-layer relation network architecture which enables successive refinements of relational information through multiple layers. We show that the increased depth allows for more complex relational reasoning by applying it to the bAbI 20 QA dataset, solving all 20 tasks with joint training and surpassing the state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1811.01838v1},
author = {Jahrens, Marius and Martinetz, Thomas},
eprint = {1811.01838v1},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jahrens, Martinetz - 2018 - Multi-layer Relation Networks.pdf:pdf},
title = {{Multi-layer Relation Networks}},
year = {2018}
}


@techreport{An2018,
abstract = {This paper proposes an attention module augmented relational network called SARN(Sequential Attention Relational Network) that can carry out relational reasoning by extracting reference objects and making efficient pairing between objects. SARN greatly reduces the computational and memory requirements of the relational network by [5], which computes all object pairs. It also shows high accuracy on the Sort-of-CLEVR dataset compared to other models, especially on relational questions.},
archivePrefix = {arXiv},
arxivId = {1811.00246v1},
author = {An, Jinwon and Lyu, Sungwon and Cho, Sungzoon},
eprint = {1811.00246v1},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/An, Lyu, Cho - Unknown - SARN Relational Reasoning through Sequential Attention.pdf:pdf},
title = {{SARN: Relational Reasoning through Sequential Attention}}
}


@article{Higgins2017DARLA,
abstract = {Domain adaptation is an important open prob-lem in deep reinforcement learning (RL). In many scenarios of interest data is hard to ob-tain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target do-main. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disen-tangled representation of the observed environ-ment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts -even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenar-ios, an effect that holds across a variety of RL en-vironments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).},
author = {Higgins, Irina and Pal, Arka and Rusu, Andrei and Matthey, Loic and Burgess, Christopher and Pritzel, Alexander and Botvinick, Matthew and Blundell, Charles and Lerchner, Alexander},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higgins et al. - Unknown - DARLA Improving Zero-Shot Transfer in Reinforcement Learning(2).pdf:pdf},
mendeley-groups = {iGGi/Literature Review},
title = {{DARLA: Improving Zero-Shot Transfer in Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.08475.pdf}
}

@article{Higgins2016,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  journal={Iclr},
  volume={2},
  number={5},
  pages={6},
  year={2017}
}



@article{Burgess2018,
abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in {\$}\backslashbeta{\$}-VAE, as training progresses. From these insights, we propose a modification to the training regime of {\$}\backslashbeta{\$}-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in {\$}\backslashbeta{\$}-VAE, without the previous trade-off in reconstruction accuracy.},
archivePrefix = {arXiv},
arxivId = {1804.03599},
author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
eprint = {1804.03599},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burgess et al. - 2018 - Understanding disentangling in {\$}beta{\$}-VAE(2).pdf:pdf},
month = {apr},
title = {{Understanding disentangling in $\beta$-VAE}},
url = {http://arxiv.org/abs/1804.03599},
year = {2018}
}

@article{Liu2017,
abstract = {Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit .},
archivePrefix = {arXiv},
arxivId = {1703.00848},
author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
eprint = {1703.00848},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Breuel, Kautz - 2017 - Unsupervised Image-to-Image Translation Networks.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Domain Adaptation},
month = {mar},
title = {{Unsupervised Image-to-Image Translation Networks}},
url = {http://arxiv.org/abs/1703.00848},
year = {2017}
}


@article{Bachman2016,
abstract = {We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.},
annote = {Efficient Information gathering:
1) reframe clues coming from different sources into a universal/complete representation.

2) Attention: "where to look?" ({\textless})--{\textgreater}"what is over there?" paradigm is being challenged with the following view: 
"what is happening?" --{\textgreater} "what would hapend if...?"
by treating attention focus as a decision/act that influences the ("perceived") environment ( that is being constructed as the universal representation).
Agent is not trying to ignore irrelevant information in a fully-knowledgeable state-observations, rather the agent aim to gather information in an incomplete knowledge state in order to diminish its uncertainties.

3) CLAIM: task-agnostic heuristics that further information gain improves task-specific performance.
Encourages to formulate hypothesise about the state of the environment and to ask question about the most uncertain hypotheses.
(curiosity, intrinsically-motived expl., auxiliary goals)

4) Devised an optimization objective that goads the agent to "ask question about x which most quickly allow it to makge good predictions about x and/or y", when (x,y) is a pair of observable/unobservable data pair.},
archivePrefix = {arXiv},
arxivId = {1612.02605},
author = {Bachman, Philip and Sordoni, Alessandro and Trischler, Adam},
eprint = {1612.02605},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bachman, Sordoni, Trischler - 2016 - Towards Information-Seeking Agents(3).pdf:pdf},
mendeley-groups = {to read - Novelty,to read - NLU,Article Gestion/3) SUMMARIZED --{\textgreater} TO IMPLEMENT},
month = {dec},
title = {{Towards Information-Seeking Agents}},
url = {http://arxiv.org/abs/1612.02605},
year = {2016}
}


@article{Huang2018,
abstract = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT},
archivePrefix = {arXiv},
arxivId = {1804.04732},
author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
eprint = {1804.04732},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2018 - Multimodal Unsupervised Image-to-Image Translation.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Domain Adaptation,iGGi/Literature Review/Multimodality},
month = {apr},
title = {{Multimodal Unsupervised Image-to-Image Translation}},
url = {http://arxiv.org/abs/1804.04732},
year = {2018}
}


@article{Hoffman2017,
abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.},
archivePrefix = {arXiv},
arxivId = {1711.03213},
author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei A. and Darrell, Trevor},
eprint = {1711.03213},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffman et al. - 2017 - CyCADA Cycle-Consistent Adversarial Domain Adaptation.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Domain Adaptation},
month = {nov},
title = {{CyCADA: Cycle-Consistent Adversarial Domain Adaptation}},
url = {http://arxiv.org/abs/1711.03213},
year = {2017}
}


@techreport{Schlag2018,
abstract = {We combine Recurrent Neural Networks with Tensor Product Representations to learn combinatorial representations of sequential data. This improves symbolic interpretation and systematic generalisation. Our architecture is trained end-to-end through gradient descent on a variety of simple natural language reasoning tasks, significantly outperforming the latest state-of-the-art models in single-task and all-tasks settings. We also augment a subset of the data such that training and test data exhibit large systematic differences and show that our approach generalises better than the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1811.12143v1},
author = {Schlag, Imanol and Schmidhuber, J{\"{u}}rgen},
eprint = {1811.12143v1},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schlag, Schmidhuber - Unknown - Learning to Reason with Third-Order Tensor Products.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Reasoning},
title = {{Learning to Reason with Third-Order Tensor Products}}
}


@article{Higgins2017SCAN,
abstract = {The natural world is infinitely diverse, yet this diversity arises from a relatively small set of coherent properties and rules, such as the laws of physics or chemistry. We conjecture that biological intelligent systems are able to survive within their diverse environments by discovering the regularities that arise from these rules primarily through unsupervised experiences, and representing this knowledge as abstract concepts. Such representations possess useful properties of compositionality and hierarchical organisation, which allow intelligent agents to recombine a finite set of conceptual building blocks into an exponentially large set of useful new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such concepts in the visual domain. We first use the previously published beta-VAE (Higgins et al., 2017a) architecture to learn a disentangled representation of the latent structure of the visual world, before training SCAN to extract abstract concepts grounded in such disentangled visual primitives through fast symbol association. Our approach requires very few pairings between symbols and images and makes no assumptions about the choice of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of compositional visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to invent and learn novel visual concepts through recombination of the few learnt concepts.},
archivePrefix = {arXiv},
arxivId = {1707.03389},
author = {Higgins, Irina and Sonnerat, Nicolas and Matthey, Loic and Pal, Arka and Burgess, Christopher P and Botvinick, Matthew and Hassabis, Demis and Lerchner, Alexander},
eprint = {1707.03389},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higgins et al. - 2017 - SCAN Learning Abstract Hierarchical Compositional Visual Concepts.pdf:pdf},
mendeley-groups = {iGGi/Literature Review},
month = {jul},
title = {{SCAN: Learning Abstract Hierarchical Compositional Visual Concepts}},
url = {http://arxiv.org/abs/1707.03389},
year = {2017}
}


@article{Kumar2018,
abstract = {Stochastic video prediction is usually framed as an extrapolation problem where the goal is to sample a sequence of consecutive future image frames conditioned on a sequence of observed past frames. For the most part, algorithms for this task generate future video frames sequentially in an autoregressive fashion, which is slow and requires the input and output to be consecutive. We introduce a model that overcomes these drawbacks-it learns to generate a global latent representation from an arbitrary set of frames within a video. This representation can then be used to simultaneously and efficiently sample any number of temporally consistent frames at arbitrary time-points in the video. We apply our model to synthetic video prediction tasks and achieve results that are comparable to state-of-the-art video prediction models. In addition, we demonstrate the flexibility of our model by applying it to 3D scene reconstruction where we condition on location instead of time. To the best of our knowledge, our model is the first to provide flexible and coherent prediction on stochastic video datasets, as well as consistent 3D scene samples. Please check the project website https://bit.ly/2jX7Vyu to view scene reconstructions and videos produced by our model.},
author = {Kumar, Ananya and London, Deepmind and Ali, S M and Deepmind, Eslami and N1c4ag, London and Rezende, Danilo J and Garnelo, Marta and Viola, Fabio and Lockhart, Edward and Shanahan, Murray},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar et al. - Unknown - Consistent Generative Query Networks.pdf:pdf},
mendeley-groups = {iGGi/Literature Review},
title = {{Consistent Generative Query Networks}}
}


@article{Hawkins2018Framework,
abstract = {How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.},
author = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
doi = {10.1101/442418},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hawkins et al. - 2018 - A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex.pdf:pdf},
journal = {bioRxiv},
mendeley-groups = {iGGi/Literature Review/Neuroscience},
month = {oct},
pages = {442418},
publisher = {Cold Spring Harbor Laboratory},
title = {{A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex}},
url = {https://www.biorxiv.org/content/early/2018/10/13/442418},
year = {2018}
}


@article{Liu2017YuXuan,
abstract = {Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, and embodiment. We term this kind of imitation learning as imitation-from-observation and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations and actions in the same environment, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show that our approach can perform imitation-from-observation for a variety of real-world robotic tasks modeled on common household chores, acquiring skills such as sweeping from videos of a human demonstrator. Videos can be found at https://sites.google.com/site/imitationfromobservation},
annote = {Imitation learning is depending on a supervised approach (behavioral cloning) or set in a given observation framework (inverse reinforcement learning), far from the reality of humans and animals imitation that account for the compensation of viewpoint, surrounding, and embodiment changes.


Imitation-from-observation:

What about imitation-from-observation where the observation is textual? It requires a shift in the modality, from pixels to text.},
archivePrefix = {arXiv},
arxivId = {1707.03374},
author = {Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
eprint = {1707.03374},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2017 - Imitation from Observation Learning to Imitate Behaviors from Raw Video via Context Translation(2).pdf:pdf;:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2017 - Imitation from Observation Learning to Imitate Behaviors from Raw Video via Context Translation.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ,Article Gestion/1) TO READ - Not PHD},
month = {jul},
title = {{Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation}},
url = {http://arxiv.org/abs/1707.03374},
year = {2017}
}


@article{Guez2018,
abstract = {Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimized to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines.},
archivePrefix = {arXiv},
arxivId = {1802.04697},
author = {Guez, Arthur and Weber, Th{\'{e}}ophane and Antonoglou, Ioannis and Simonyan, Karen and Vinyals, Oriol and Wierstra, Daan and Munos, R{\'{e}}mi and Silver, David},
eprint = {1802.04697},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guez et al. - 2018 - Learning to Search with MCTSnets.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,iGGi/Literature Review/Planning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE - Not PHD},
month = {feb},
title = {{Learning to Search with MCTSnets}},
url = {http://arxiv.org/abs/1802.04697},
year = {2018}
}

@article{Weber2017,
abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In con-trast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
author = {Weber, Th{\'{e}}ophane and Racani{\`{e}}re, S{\'{e}}bastien and Reichert, David P and Buesing, Lars and Guez, Arthur and Rezende, Danilo and Badia, Adria Puigdom{\`{e}}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Deepmind, Daan Wierstra},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weber et al. - 2017 - Imagination-Augmented Agents for Deep Reinforcement Learning.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ},
title = {{Imagination-Augmented Agents for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.06203.pdf}
}


@article{MoritzHermann2017,
abstract = {We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world.},
annote = {A combinaison of reinforcement and unsupervised learning yields an agent able to correlate embodied experience, visual representation and words and phrases altogether.

Generalization/zero-shot comprehension of novel instructions and situations is achieved.

RL framework:
state = stream of continuous visual input + textual instruction.

Learning 59 different words (2M episodes):
- A3C not learning.
- A3C + RewardPrediction + ValueReplay --{\textgreater} sufficient to learn.
- A3C +(RP+VR) + tAE / LP --{\textgreater} strong acceleration of learning.
- A3C +(RP+VR) +(tAE+LP) --{\textgreater} very strong acc.


Curriculum learning of words gives strong acceleration of the speed at which the whole lexical bank is learned.

Decoupling preliminary skills learning and inter-modal interaction learning is critical to yield quick learning. The main difficulty being in learning motor and visual skills.
Bootstrapping effect from existing semantic (and motor) knowledge prior acquisition to new semantic knowledge acquisition.


Generalization to never-before-seen bigrams was achieved from unigram and bigram training , as well as from bigram-only training.
Decomposition/Compositionality (lingual-visual disentanglement?) are learnable. It is critical since linguistic stimuli are usually sentences and rarely isolated words.


Investigate:

0) Ecological concern: training regime is different from a online intrinsically-motivated learning regime exhibited by a baby. Could all of this be learned through instrinsic motivation?

1) Is curriculum learning equivalent to a relevant (attention-guided?) exploration scheme, in terms of learning speed? {\textless}-- a multi-goal setting has to be put in place in order to alleviate ecological issues regarding the training regime that is different from an online intrinsically-motivated learning that a baby would exhibit.},
author = {{Moritz Hermann}, Karl and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and Wainwright, Marcus and Apps, Chris and Hassabis, Demis and Blunsom, Phil and London, Deepmind},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hermann et al. - 2017 - Grounded Language Learning in a Simulated 3D World(2).pdf:pdf},
mendeley-groups = {Article Gestion/3) SUMMARIZED --{\textgreater} TO IMPLEMENT,iGGi/Literature Review/Active Learning,iGGi/Literature Review/Compositionality,iGGi/Literature Review/Deep Reinforcement Learning,iGGi/Literature Review/Entity Recognition,iGGi/Literature Review/Interpretability,iGGi/Literature Review/Knowledge Representation,iGGi/Literature Review/Multimodality,iGGi/Literature Review/Natural Language Grounding,iGGi/Literature Review/Natural Language Understanding},
title = {{Grounded Language Learning in a Simulated 3D World}},
url = {https://arxiv.org/pdf/1706.06551.pdf}
}


@article{Lu201812,
abstract = {We propose a general formulation for addressing reinforcement learning (RL) problems in settings with observational data. That is, we consider the problem of learning good policies solely from historical data in which unobserved factors (confounders) affect both observed actions and rewards. Our formulation allows us to extend a representative RL algorithm, the Actor-Critic method, to its deconfounding variant, with the methodology for this extension being easily applied to other RL algorithms. In addition to this, we develop a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Using this benchmark, we demonstrate that the proposed algorithms are superior to traditional RL methods in confounded environments with observational data. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full RL problems with observational data. Code is available at https://github.com/CausalRL/DRL.},
annote = {Augmentation of the estimated reward function into a deconfounding reward function that relies on sampling a learned deconfounding model.

Observational distributions are opposed to Intervention distributions like free-will-based act observations are opposed to constrained act observations. Fortunately, knowing the causal diagram lets us use back-door and fron-door criteria to estimate the intervention distribution with do-calculus.

The deconfounding model consist of a recurrent generative model with latent(-confounding) variables.

Future Work/Limitations:
1) is there a link between pragmatic reasoning and deconfounding/counterfactual reasoning? 
Can the instanciation of deconfounding variables and their acknowledgement in adjustment formulas be yielding similar benefits and similar mechanisms when compared to pragmatics-focused reasoning that would rely on modular-representation-priored ensembles and re-scoring schemes (awfully similar to do-calculus?) ? It is hypothesized that each representation module (cf. "Modularity matters:...") would learn different causal models with diverse deconfounding (deterministic?) variables and diverse causality, and thus it would yield a feedforward counterfactual reasoning/re-scoring scheme that enables greater performances.

2) Assuming that modularity-priored ensembles are using deterministic deconfounding variables, it would stem to reason that augmenting modularity-priored ensembles with truely stochastic deconfounding variables could yield similar results than deconfounding/reasoning in a fashion more akin to the connectionnism paradigm.},
archivePrefix = {arXiv},
arxivId = {1812.10576},
author = {Lu, Chaochao and Sch{\"{o}}lkopf, Bernhard and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel},
eprint = {1812.10576},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, Sch{\"{o}}lkopf, Hern{\'{a}}ndez-Lobato - 2018 - Deconfounding Reinforcement Learning in Observational Settings.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/MultiAgents,Article Gestion/1) TO READ/Instructions,Article Gestion/1) TO READ/Curriculum / Active Learning,Article Gestion/1) TO READ/Reasoning},
month = {dec},
title = {{Deconfounding Reinforcement Learning in Observational Settings}},
url = {http://arxiv.org/abs/1812.10576},
year = {2018}
}

@techreport{Buesing201811,
abstract = {Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.},
annote = {Emulate human cognitive Capacity to reason about alternate, counterfactual outcomes of past experiences.

Problem: alleviate on mismatch between model and environment.

Generalize model knowledge through scenarios and infer through learned causal mechanisms in order to reduce the likelyhood of mismatch.

Advantage: under perfect model assumption, Model-Based Policy Evaluation ( MB-PE) and Couter Factual Policy Evaluation ( CF-PE) yield similarly unbiased queries. But, this assumption never holds, at the very least, only because of the parametric nature of the generative model.

Thus, without this assumption, C F-PE de-bias the queries thanks to its inclusion of information from the history.},
archivePrefix = {arXiv},
arxivId = {1811.06272v1},
author = {Buesing, Lars and Weber, Th{\'{e}}ophane and Zwols, Yori and Racan{\`{i}}, S{\'{e}}bastien and Guez, Arthur and Lespiau, Jean-Baptiste and Heess, Nicolas},
eprint = {1811.06272v1},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buesing et al. - Unknown - WOULDA, COULDA, SHOULDA COUNTERFACTUALLY-GUIDED POLICY SEARCH.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE},
title = {{WOULDA, COULDA, SHOULDA: COUNTERFACTUALLY-GUIDED POLICY SEARCH}}
}


@article{Jo201806,
annote = {Fully distributed representation prior is the mainspring behind the the 'good' features that CNNs compute for many different tasks. Yet, cognitive arguments would argue towards the use of a modular prior to learn 'better'/higher order invariant features. Comparison of the two priors on an invariant relational learning task shows a better data-efficiency and far better performance of the modularity prior.

Modularity prior solves the interference problem that may occur when homogeneous connectivity topology (topologies that implements the fully distributed representation prior) tries to learn different pattern that will interfere with each other.},
archivePrefix = {arXiv},
arxivId = {1806.06765},
author = {Jo, Jason and Verma, Vikas and Bengio, Yoshua},
eprint = {1806.06765},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jo, Verma, Bengio - 2018 - Modularity Matters Learning Invariant Relational Reasoning Tasks.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE},
month = {jun},
title = {{Modularity Matters: Learning Invariant Relational Reasoning Tasks}},
url = {https://arxiv.org/abs/1806.06765},
year = {2018}
}

@article{Andreas201604,
abstract = {We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural "listener" and "speaker" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated {\_}without{\_} demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81{\%} of the time, compared to a 69{\%} success rate using existing techniques.},
archivePrefix = {arXiv},
arxivId = {1604.00562},
author = {Andreas, Jacob and Klein, Dan},
eprint = {1604.00562},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas, Klein - 2016 - Reasoning About Pragmatics with Neural Listeners and Speakers.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Natural Language Grounding,Article Gestion/1) TO READ,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE},
month = {apr},
title = {{Reasoning About Pragmatics with Neural Listeners and Speakers}},
url = {http://arxiv.org/abs/1604.00562},
year = {2016}
}


@article{Fried201711,
abstract = {We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.},
annote = {Explicitly incorporating a procedure that lets the speaker reason about some embedded model of a listener and its context on top of a speaker-listener model enhances the pragmatism of both behaviours.

This induces counterfactual reasoning in the listener reasoning procedure.

Intractability of the optimization problems goads the authors to solved an approximate inference procedure rather.

(4-gram) BLEU score is a poor measure of the quality of the instructions, in terms of their ease to be followed/carried out by human instructee. (Also in Papineni et al. 2002)

In Tangram, the pragmatic speaker was able to produce easier-to-follow instructions than those produced by human instructor.

Future works:
1) Find a biologically-plausible structure that supports computational pragmatism and counterfactual reasoning the way this does(e.g. through ensemble and re-scoring / counterfactual modularity prior). Implement a neural world representation that encompasses some element agency (through a stochastic+deterministic transition model, following "Learning Latent Dynamics for Planning from Pixels"?).

2) Investigate the benefit of instruction generation by ensembles+re-scoring in the context of language acquisition in a multi-agent setting (cf. Emergence of grounded language literrature...).},
archivePrefix = {arXiv},
arxivId = {1711.04987},
author = {Fried, Daniel and Andreas, Jacob and Klein, Dan},
eprint = {1711.04987},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fried, Andreas, Klein - 2017 - Unified Pragmatic Models for Generating and Following Instructions.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning},
month = {nov},
title = {{Unified Pragmatic Models for Generating and Following Instructions}},
url = {http://arxiv.org/abs/1711.04987},
year = {2017}
}

@article{Li2016,
abstract = {A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interaction. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and demonstrate that the learner improves when asking questions. Finally, real experiments with Mechanical Turk validate the approach. Our work represents a first step in developing such end-to-end learned interactive dialogue agents.},
annote = {Learning from answers and asking questions in an end-to-end learning interactive dialogue approach.
--{\textgreater} student learns how to learn via dialogue
--{\textgreater} natural extension of the Forward Prediction learning scheme presented in "Dialog-based Language Learning" by Weston 2016.

Biblio:
1) Wittgenstein : language learning is best done through language use in language games.
2) Winograd language games + Wang's SHRDLURN
3) chit-chat conversational models
4) goal-oriented conversational models
5)QA from dialogues
--{\textgreater} everything learns from fixed supervised signals rather than interaction.


Ideas:
1) Different sources of mistakes made by a learner during dialogue.
2) "Learning to perform better in future dialogues" provides a grounding for the interaction in RL.
3) natural extension of the Forward Prediction learning scheme using QUESTIONNING AND INTERACTION.
4) Asking-Question and Question-Answering settings are implemented in all task sample, thus probing the systems on two accounts at once.
5) QUESTION CLARIFICATION: hard to understand user because of typo/spelling...
6) KNOWLEDGE OPERATION: reasoning steps are hard to identify, thus hints are asked for, or verification over a potential reasoning step is asked.
7) KNOWLEDGE ACQUISITION: incomplete KB so the agent asks for further details and the teacher gives it in the course of the dialogue, and then reasks the question later on. It is a difficult task given the memorization step that the agent has to perform. 
8) SUPERVISED LEARNING SCENARIO: 50{\%} question then answer + 50{\%} directly answers.
9) RL SCENARIO: the model learns to choise between answering directly and asking a question. It must trade carefully between gaining more information and remaining an interesting locutor.
10) UNKNOWN WORDS: need to be account for, and it seems that contextualization is good enough.

Future Work/Limitations:
1) Positive and negative feedbacks are templated by 6 categories.},
archivePrefix = {arXiv},
arxivId = {1612.04936},
author = {Li, Jiwei and Miller, Alexander H. and Chopra, Sumit and Ranzato, Marc'Aurelio and Weston, Jason},
eprint = {1612.04936},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Learning through Dialogue Interactions by Asking Questions.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Curriculum/Curiosity/Episodic/Imitation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/QA/Common Sense/Knowledge representation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Dialogue / Natural Language Generation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
month = {dec},
title = {{Learning through Dialogue Interactions by Asking Questions}},
url = {http://arxiv.org/abs/1612.04936},
year = {2016}
}


@article{Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
annote = {Encoder-decoder machine translation system suffers from a bottleneck in the instance of the encoding space fixed-length vector. 
This squashed information fixed-length vector does indeed remain fixed eventhoug the lenght of the encoded sentence might change.

It has been shown that increase of the sentences' length would see decrease in the performance.

The addition of a sequential attention model over the fixed-length bi-directionnal input word encoded vectors acts like a (soft)-attention that relaxes the information encoding constraint on the encoder, thus yielding better BLEU scored performance.

The importance of the soft-attention decoding is clearly seen in the annotation weights matrix.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
mendeley-groups = {iGGi/Literature Review,Article Gestion/1) TO READ/NLP},
month = {sep},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2014}
}



@article{Kendall2017,
abstract = {Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
archivePrefix = {arXiv},
arxivId = {1705.07115},
author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
eprint = {1705.07115},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall, Gal, Cipolla - 2017 - Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Deep Reinforcement Learning,iGGi/Literature Review/Active Learning,Article Gestion/1) TO READ/Reasoning,Article Gestion/1) TO READ/Safety,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE},
month = {may},
title = {{Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}},
url = {http://arxiv.org/abs/1705.07115},
year = {2017}
}



@article{Platanios2018,
abstract = {We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation. Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network). This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively. The rest of the model remains unchanged and is shared across all languages. We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation. We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.},
annote = {Leveraging shared structure (i.e. language factual knowledge) across different languages, using contextual parameter generation improves the data-efficiency, the zero-shot ability (skirting the error propagation that may occur with pivoting), and the never-ending ability of the system (learning new language on the fly...). 

Future works:
1) Considering adding an uncertainty modelization to increase the interpretability as well as the learning pace.},
archivePrefix = {arXiv},
arxivId = {1808.08493},
author = {Platanios, Emmanouil Antonios and Sachan, Mrinmaya and Neubig, Graham and Mitchell, Tom},
eprint = {1808.08493},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Platanios et al. - 2018 - Contextual Parameter Generation for Universal Neural Machine Translation(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ,Article Gestion/1) TO READ/NLP/Translation},
month = {aug},
title = {{Contextual Parameter Generation for Universal Neural Machine Translation}},
url = {http://arxiv.org/abs/1808.08493},
year = {2018}
}



@article{Savinov201810,
abstract = {Rewards are sparse in the real world and most today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself-thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward-making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory-which incorporates rich information about environment dynamics. This allows us to overcome the known "couch-potato" issues of prior work-when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in VizDoom, DMLab and MuJoCo. In navigational tasks from VizDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns loco-motion out of the first-person-view curiosity only.},
annote = {Re-define curiosity in order to prevent from the "couch-potato" issue:
Encouraging behaviours which produces observations that take some effort to reach because it would be outside of the already explored part of the environment.

Better convergence than ICM.
More Robust to spurious behaviours(?) than ICM.

Develop episodic curiosity (EC) module whose goal is to produce a bonus reward added to the environment reward.

Check against a memory of embedded observations for (backward) few-step reachability.

Immune to ICM's prediction error scenarios.
Learn at least 2 times faster than ICM on VizDoom.

Future work(s): 
1)test this episodic curiosity on continuous control tasks. Very Dense reward task --{\textgreater} watch out for performance degradation due to ECO module?
(+proprioceptive signal ? not from first-person-view?) --{\textgreater} scaling?

2)test for shared CNN weights between policy/value function and ECO module's embedding network --{\textgreater} watch out for accelerated training time?

3) extend this work to be compatible with demonstrations following (Savinov et al. 2018). --{\textgreater} application in a multi-agent setting for coordinated distributed/multi-agent exploration.},
archivePrefix = {arXiv},
arxivId = {1810.02274v2},
author = {Savinov, Nikolay and Raichuk, Anton and Marinier, Rapha{\"{e}}l and Vincent, Damien and Pollefeys, Marc and Lillicrap, Timothy and Gelly, Sylvain},
eprint = {1810.02274v2},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Savinov et al. - Unknown - EPISODIC CURIOSITY THROUGH REACHABILITY.02274:02274},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,iGGi/Literature Review/Active Learning},
title = {{Episodic Curiosity Through Reachability}}
}

@article{Andreas201711,
annote = {Using language in a pretraining phase to constrain en embedding parameter space subsequently improves the performance of the system in classification, text edition and RL.

Indeed, "the structure of natural language reflects the structure of the world". In conveys an "reusable" abstraction layer that is efficient for anything humans do.
Can we leverage natural language to learn more efficiently those kind of "reusable abstract structure" needed to imitate human beings efficiency?

Natural Language Latent parameterization space enables a direct (therefore easier) path towards discovering compositional concepts and understanding patterns that can be inferred from the data that have been experienced, while preventing the model to overfit to a small subset of it.

Embedding being done in a natural language latent space, the system becomes highly interpretable.

3 phased multitask/eta-learning framework: 
1) pretraining: many various dataset are used to yield a an input-to-language-description map (shared parameters) with relevant properties (compositionality, pattern inference, preventing overfitting...).

2) concept-learning: fine-tuning with task-specific parameters to efficiently ground the relevant patterns that the task exhibits.

3) evaluation:},
archivePrefix = {arXiv},
arxivId = {1711.00482},
author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
eprint = {1711.00482},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas, Klein, Levine - 2017 - Learning with Latent Language(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/NLP},
month = {nov},
title = {{Learning with Latent Language}},
url = {https://arxiv.org/abs/1711.00482},
year = {2017}
}

@article{Weston2016,
abstract = {A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.},
annote = {Supervision at the word level (tagging, parsing tasks) or sentence level ( QA, Machine Translation) are "not realistic of how human learn".
Language is learned as a means to communication (and then repurposed as an aim when it comes to art like literature...).

Predictive Lookadhead/Teacher (Pre-)Imitation enables QA without any reward signal.

Reward signal is poor compared to dialog feedback in terms of information.
Moreover, the dialog feedback yield an information about "how to use language in subsequent conversations".

Problem(s): 
1) most datasets in NLP are framed in a supervised way, as opposed to a dialog-based Language Learning way, so they propose a dataset with "natural feedback from teacher".
2) how to learn without reward signal: learn by predicting/imitating the teacher's replies.

Biblio:
1) Language Learning in human
2) RL with dialog

Future works/Limitations:
1) FP shows an issue with credit-assignement when the feedback is far from the question.
2) How come that FP works at all without any rewards? "predicting true answers [to the questions] leads to greater accuracy in forward prediction" of the teacher answers. To predict the teacher answer, one needs to know whether the question's answer is correct or wrong and it turns out that providing rights answers could enhance the ability of the agent to predict well. For instance, there could be so little possible variant of the teacher answers upon true answers compared to upon false answers. (One could yield compliments upon completion of the task the right way, and corrections upon failure, thus yielding a far more complex teacher's answers distribution in the case of failures than in the case of successes, thus implicitly goading the agent to yield true answer in order to have an easier FP task).

3) Keeping on testing until completion could see us setting up a negative rewards at each turn so that the agent is compelled to complete the task quickly, by yielding the true answers.

4) Language is actually learned through its grounding and with the aim to communicate, rather than through statistical tools. More can be done, maybe, by asking the agent to FP the whole conversation (thus enabling the agent to ask questions about the context/memory it has, firstly, as a premice for dialog-based counterfactual reasoning - some hierarchical / monte-carlo framework could help yield differently valuable questions (cf. MCTSnets and A0 value prediction network / A0CivManual's predicate prediction network) as well as the context/memory (for an actual counterfactual reasoning):
dialog-based language and world dynamical model learning, as the context/memory can truely be reasoned upon and queried ?},
archivePrefix = {arXiv},
arxivId = {1604.06045},
author = {Weston, Jason},
eprint = {1604.06045},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weston - 2016 - Dialog-based Language Learning.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/MultiAgent,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Dialogue / Natural Language Generation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/QA/Common Sense/Knowledge representation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Curriculum/Curiosity/Episodic/Imitation},
month = {apr},
title = {{Dialog-based Language Learning}},
url = {http://arxiv.org/abs/1604.06045},
year = {2016}
}


@article{Kirby199812,
annote = {NL presents peculiar linguistic structures that are assumed to enable children to learn it through observations of its usages by others. 

Authors argue towards the emergence those peculiar linguistic structures without the recourse to natural selection but rather with the recourse to some learning biases.


Biblio:
1) Compositionality definition
2) recursion definition
3) evolutionary linguistics
4) Pinker {\&} Bloom 1990: human language as bio adaptation to the need of communication via serial interface.
5) Mitchell 1997 : learner that can generalise and their prior learning biases.
6) Supporters, with different agenda, to the fact that learning processes affects language transmission/acquisition.
7) Hurford 1987: (Internal/)External Language : "utterances in the arena of use".


Ideas:
1) language = Learned, socially transmitted, system
2) need learners model in dynamic/social context.
3) Computational perspective in order to alleviate our failiable intuition -expressed in verbal theories-, or the difficult to mathematically formulate things, when it comes to dynamical systems.
4) I-/E-Language influence e.o. via the pr. of linguistic transmission.
5) the Pr. of Linguistic transmission requires some random process of invention to populate the grammar space with new rules to utter about novel(in the grammar sense?) meanings / meanings that challenges the internal grammar of the speaker?.
6) Learners learn from some (utterance / meanings) pairs.

7) Acquisition Algorithm: designed for efficiency and ease of analysis, rather than efficacy as practical grammar induction tool.
8) Seems to model two important processes though:
a. "rote learning of examples"
b. "induction of generalisations"

9) internal knowledge of the agent is represented as a context-free grammar.
10) Rule creation: incorporation and duplicate deletion are fundamentals mechanisms to create rules. But this inducer has no possibility to generalise. How can we let it generalise? By considering pairs of rules and trying to extract the least-general generalisation. In order to satisfy preservativity, we need not forget to define non-terminals that may appear during the process of rule subsumption.

11)At each simulation run, a new learner with a blank grammer is used.


11) Starting from tabular rasa, we need invention mechanisms:
a. introduction of new words for whole meaning, 
b. without the introduction of new syntactic structure (since it is the job of the rule creation/subsumption mechanisms).
--{\textgreater} These requirements are satisfied by looking for the closest meaning that the speaker know how to produce and replacing the novel parts with random seq. of symbols. Eventually, the speaker is exposed to that new {\textless}meaning,signal{\textgreater} pair in order to remain consistent with the subsequent remaining pairs that it will try to utter.

12) Infinite meaning space is created by adding predicates which can take other predicates as arguments.
--{\textgreater} during that simulation run, curriculum learning is used to make sure that degree-0 meanings (easy to learn from) are being learned from.


13) Why is comp. and rec. syntax inevitable, in this context? ease of transmission seems to further persistance.
14) I-language units / replicators compete with each other to persist (by being expressed). Each replicator's likelyhood of being expressed is equal, unless some replicators may be used to express multiple meaning. So, rather, the most likely expressed replicators are the more general ones.

15) Compositional recursive languages are the only on that can go through the transmission bottleneck without suffering any changes.

Future Works/Limitations:
1) the Pr. of Linguistic transmission requires some random process of invention utter about meanings that challenge the internal grammar of the speaker? 
--{\textgreater} How to efficiently come up with those process of invention?
--{\textgreater} How to invent in order to promote the language stability and at the same time the language expressivity, or some other linguistic properties like compositionality and recursivity, maybe? Can we measure those two efficiently and then make a decision in that space that would lead to a pareto solution?

2) The invention algorithm is rather very specific and might be a critical aspect of the whole system. It is a learning/exploration bias in itself and it ought to be investigated as a meta-parameter maybe?

3) The use of a new learning with a blank grammar at each new simulation iteration might be a critical point to investigate, as the new learner has no biases, like a newborn, in the next (horizontal transmission) generation.},
author = {Kirby, S},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirby - 2002 - Learning, bottlenecks and the evolution of recursive syntax.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience/Linguistic},
title = {{Learning, bottlenecks and the evolution of recursive syntax}},
url = {https://pdfs.semanticscholar.org/0def/76e16d1becfbb60e9dad80105926298e9686.pdf},
year = {1998}
}

@article{kirby2002learning,
  title={Learning, bottlenecks and the evolution of recursive syntax},
  author={Kirby, Simon},
  year={2002},
  publisher={na}
}




@article{krifka2001compositionality,
  title={Compositionality},
  author={Krifka, Manfred},
  journal={The MIT encyclopedia of the cognitive sciences},
  pages={152--153},
  year={2001},
  publisher={MIT Press Cambridge, MA}
}

@book{trask1993,
      title={A dictionary of grammatical terms in linguistics},
  author={Trask, Robert Lawrence},
  year={2013},
  publisher={Routledge},
  doi={https://doi.org/10.4324/9780203393369}
}

@article{pinker1990natural,
  title={Natural language and natural selection},
  author={Pinker, Steven and Bloom, Paul},
  journal={Behavioral and brain sciences},
  volume={13},
  number={4},
  pages={707--727},
  year={1990},
  publisher={Cambridge University Press}
}

@article{hurford1987language,
  title={Language and number: The emergence of a cognitive system},
  author={Hurford, James R},
  year={1987},
  publisher={Blackwell Pub}
}

@book{chomsky1986knowledge,
  title={Knowledge of language: Its nature, origin, and use},
  author={Chomsky, Noam},
  year={1986},
  publisher={Greenwood Publishing Group}
}

@article{Smith2003,
annote = {Language structure are better explained by cultural transmission, an iterated learning process, and the authors demonstrate here how compositionality arises because of stimulus poverty.

It stems from Chomsky's linguistic paradigm's impossibility to account for the process of primary linguistic data acquisition by children, without the Chomskyan genetically-encoded language organ of the mind that contains universal grammar.

Biblio:
1) innate "language instinct" that evolved for communication of info, desires, beliefs, or group cohesion.
2) individual linguistic capabilities vs cultural transmission dynamics as competing factors to understand linguistic behaviours. Jackendoff.
3) Noam Chomsky's Linguistic Paradigm defining language as a component of individual psychology.
4) Hurford's view of language as an essentially cultural phenomenon.

5) Internal vs Exernal (phonetics/sounds used) linguistic behaviours. External behaviours being viewed as "epiphenomenal".

6) Iterated Learning : Horizontal and Vertical form [1,16,22].
7) Recursive syntax explaination [14].
8) Word-order universalis [12].

9) High Semantic Complexity (/high degree of "meaning-space structure", F (number of dimensions in the meaning-space) and V (number of discrete values per dimension) ) leads to the emergence of a syntax.

Ideas:
1) Compositional system: "the meaning of a signal is a function of the meaning of its parts and the way they are put together." Krifka,2001 : Compositionality (in MIT encyclopedia of cognitive sciences)
2) Recursive syntax is also an important structural property of language as it allows for combinatorial recombinations, from a smal number of rules.
3) And compositionality is the key to understand the meaning of new utterances (created by recursion, for instance), given knowledge of the finite set of parts and what are the finite set of rules to recombine them.
4) Proto language can be evolved through iterated learning into compositional language.

5) Two key determinants of linguistic structure: STIMULUS POVERTY and STRUCTURE SEMANTIC REPRESENTATIONS.

6) "LANGUAGE AS A MAPPING BETWEEN MEANINGS AND SIGNALS"
--{\textgreater} homeomorphism : "preserves neighborhood relationships"


7) "HOW DOES THE COMPOSITIONALITY OF A LANGUAGE RELATES TO ITS STABILITY OVER CULTURAL TIME"?
8) Construction of holistic and compositional languages are detailed in a mathematical model.


9) Expressivity is related to language stability via the iterated learning model.

10 ) Metric: Relative stability of a compositional languages with respec to holistic languages.

11) Stimulus Poverty: number of R observations is very small compared to the number of objects in the environment N. --{\textgreater} severed transmisson bottleneck because of a very small object coverage b = R/N. 

12) Average inter-meaning Hamming Distance of objects that have been assigned meanings is minimized to create a "structured" environment, independantly of the structure of the meaning space.

13) MEASURE OF COMPOSITIONALITY: "degree of [Pearson product-moment] correlation between the distance between pairs of meanings and the distance between the corresponding pairs." Using Hamming distance in meaning space and Levenstein distance in signal space.


14) Transition from holistic to compositional language is faster in the context of structured environment because of the fact that objects with very different meanings associated to them will have a high probability of sharing feature values, thus making composiitonal language highly generalizable.


15) CULTURAL TRANSMISSION OF LANGUAGE: "highlights for the situatedness of language-using agents in an environments"


Future Works/Limitations:
1) Structure Semantic Representations seems to ask for the development of a online/few-shot/lifelong learning classifier in order to yield a semantically classified/structured world representation via sensory affordances, like vision.

2) (13) seem to be a relative measure of compositionality compared to the compositionality of the meaning space. Here, the meaning space is highly compositional, so the measure can be viewed as an absolute measure. In future works, where the meaning space is learned, is a relative measure good enough? How can we increase the compositionality of the (latent) meaning space that we would use? (assuming a VAE model to extract the meaning space of the pixel input)

3) Figure 6's result implies, in the absence of bottleneck (i.e. with a full-dataset sampling at each epoch, in the context of supervised learning) that iterated learning process (i.e. multi-epoch supervised learning) can only yield compositional language (i.e. compositional features) infrequently, with the condition that the environment (i.e high-complexity pixel inputs drawn) have to be structured (i.e. have to exhibit low "average inter-meaning Hamming distance" in the pixel space == it is never the case? --{\textgreater} How likely is it that distinct meanings would share feature values / patterns recognizable by a CNN, for instance ? ).
--{\textgreater} This is linked to Schoenemann's semantic complexity [19] in the sense that compositionality is used at its best "when language learners perceive the world as structured".
--{\textgreater} What about structuration by contrasting, thus bringing objects of different meaning (semantically) close to each other, especially when they share attributes/features (visually plausible/interchangeable). 
--{\textgreater} it seems that structuration might be achievable through the ordonnancement of objects with respect to their features/attributes' values, as a first step. Then, contrastive sequences can be sampled from this ordered sequence by alternating the meanings/classes. Let a,b,c,d be meanings instanciated in objects ai, bi, ci, di such as:
a1,a2, b2,c3,c1,b1,a3,b3,b5
Then a contrastive sequence coould be:
a1,b2,c1,b1,a3,b5
or
a2,b2,c3,b1,a3,b3


4) Cultural features are explained by the iterated learning model, but what about biological features? Before being cultural, language is biological as it relies on "a particular innate endowment", the biological endowment. Does "natural selection for a socially usefal language" play a role in linguistic structure?
cf. DEACON [8] ; brain and language co-evolution},
author = {Smith, K and Kirby, S and Life, H Brighton - Artificial and 2003, Undefined},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith et al. - Unknown - Iterated learning A framework for the emergence of language.pdf:pdf},
journal = {Artificial Life},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience/Linguistic,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
number = {4},
pages = {371--389},
title = {{Iterated learning: A framework for the emergence of language}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/106454603322694825},
volume = {9},
year = {2003}
}

@article{Brighton2002,
author = {Brighton, Henry},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/life, 2002 - Unknown - Compositional syntax from cultural transmission.pdf:pdf},
journal = {MIT Press},
mendeley-groups = {Article Gestion/1) TO READ/Linguistic},
title = {{Compositional syntax from cultural transmission}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/106454602753694756},
volume = {Artificial},
year = {2002}
}


@article{chomsky1980rules,
  title={Rules and representations},
  author={Chomsky, Noam},
  journal={Behavioral and brain sciences},
  volume={3},
  number={1},
  pages={1--15},
  year={1980},
  publisher={Cambridge University Press}
}

@article{chomsky1976reflections,
  title={Reflections on language},
  author={Chomsky, Noam A},
  year={1976}
}

@inproceedings{brighton2001survival,
  title={The survival of the smallest: Stability conditions for the cultural evolution of compositional language},
  author={Brighton, Henry and Kirby, Simon},
  booktitle={European Conference on Artificial Life},
  pages={592--601},
  year={2001},
  organization={Springer}
}

@article{lewis1969convention,
  title={Convention: A Philosophical Study},
  author={Lewis, David},
  year={1969}
}


@book{briscoe2002linguistic,
  title={Linguistic evolution through language acquisition},
  author={Briscoe, Ted},
  year={2002},
  publisher={Cambridge University Press}
}

@book{cangelosi2012simulating,
  title={Simulating the evolution of language},
  author={Cangelosi, Angelo and Parisi, Domenico},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{spike2017minimal,
  title={Minimal requirements for the emergence of learned signaling},
  author={Spike, Matthew and Stadler, Kevin and Kirby, Simon and Smith, Kenny},
  journal={Cognitive science},
  volume={41},
  number={3},
  pages={623--658},
  year={2017},
  publisher={Wiley Online Library}
}

@article{steels2012grounded,
  title={The grounded naming game},
  author={Steels, Luc and Loetzsch, Martin},
  journal={Experiments in cultural language evolution},
  volume={3},
  pages={41--59},
  year={2012},
  publisher={Citeseer}
}

@article{Lazaridou2016,
abstract = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.},
annote = {"Framework for language learning that relies on multi-agent communication"

Focus on the problem of refering to things/"coordinate in a referential game".

"How do we design environments that foster the development of a language that is portable to new situations and to new communication partners (in particular humans)?"

Biblio:
1) Motive for "handling natural-language-based communication [as] a key step toward the development of AI that can thrive in a world populated by other agents"

2) NN can "evolve communication in the context of games without a pre-coded protocol". [Sukhbaatar2016, Foerster2016]

3) Emerging language with human-in-the-loop from the very beginning: SHRLDU program of Winograd / [Wang, 2016].

4) "Wizard-of-Oz" environments where the interactions are heavily scripted [Mikolov 2015, Guiding Policies with Language via Meta-Learning].

6) Lewis' Signaling Game [1969].
7) "Cheap Talk" variant used in game theory.
--{\textgreater} questions the convergence of the "Cheap Talk" game in different languages that forms a Nash Equilibria. [Crawford 1982]




Ideas:
1) Start from the problem of REFERING TO THINGS
2) Is the language made up of associations between symbols and concepts like "broad object categories (as opposed to low-level visual properties)"?
--{\textgreater} Can the emerging language be made more interpretable by changing the environment?

3) Scalability issues of human-in-the-loop approach are acknowledge, while the Referential Game allow for human to be players while not having that scalability issue.


Results:
1) NN "can learn to coordinate in a referential game"
2) The language captures associations between symbols and "genral conceptual properties of the object [..], rather than low-level visual properties".
3) Temptative approach towards 


Future Works/Limitations:
1) Emergent communication that "stays close to human natural language".
2) Learning the "structural properties of language (e.g. lexical choice, syntax or style)"
3) Learning the "function-driven facets of language, such as how to hold a conversation".},
archivePrefix = {arXiv},
arxivId = {1612.07182},
author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
eprint = {1612.07182},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazaridou, Peysakhovich, Baroni - 2016 - Multi-Agent Cooperation and the Emergence of (Natural) Language(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/MultiAgents,Article Gestion/1) TO READ/NLP,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
month = {dec},
title = {{Multi-Agent Cooperation and the Emergence of (Natural) Language}},
url = {http://arxiv.org/abs/1612.07182},
year = {2016}
}


@article{crawford1982strategic,
  title={Strategic information transmission},
  author={Crawford, Vincent P and Sobel, Joel},
  journal={Econometrica: Journal of the Econometric Society},
  pages={1431--1451},
  year={1982},
  publisher={JSTOR}
}

@article{farrell1996cheap,
  title={Cheap talk},
  author={Farrell, Joseph and Rabin, Matthew},
  journal={Journal of Economic perspectives},
  volume={10},
  number={3},
  pages={103--118},
  year={1996}
}

@article{blume1998experimental,
  title={Experimental evidence on the evolution of meaning of messages in sender-receiver games},
  author={Blume, Andreas and DeJong, Douglas V and Kim, Yong-Gwan and Sprinkle, Geoffrey B},
  journal={The American Economic Review},
  volume={88},
  number={5},
  pages={1323--1340},
  year={1998},
  publisher={JSTOR}
}


@article{crawford1998survey,
  title={A survey of experiments on communication via cheap talk},
  author={Crawford, Vincent},
  journal={Journal of Economic theory},
  volume={78},
  number={2},
  pages={286--298},
  year={1998},
  publisher={Elsevier}
}


@article{Havrylov2017,
abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
annote = {"Variable-length strings of symbols"/"structured protocols" for richness-enabled communication optimized over straight-through Gumbel-softmax estimators (training-time end-to-end differentiation)

RL impossible because of :
1) number of possible messages proportional to size{\_}vocabulary{\^{}}sentence{\_}max{\_}length, i.e. way too many actions to consider.
2) non-stationnary environment created by the receiver trying to adapt to the sender.
3) How to learn the sender since there is a sampling operation in the middle of the computational graph: 
a. REINFORCE, but high variance due to big action space.
b. Gumbel-softmax estimator, makes things differentiable and thus allow for the optimization of the sender through backprop, but it uses real values at training time and discrete values at test time thus there is a loss of information that is perceived at test time as a loss of performance...
c. Straight-through Gumbel-softmax estimator, discretize the sampling with an argmax/winner-takes-all in the forward pass at training time but uses the softmax relaxation in the backward pass assuming that the two derivatives are somewhat equals, but it introduces a bias in the estimator.
--{\textgreater}High temperature should incur faster learning, but it is included as a learning parameter in its inverse form for better performances.




Idea: 
Supervision at the symbol-level within the strings can be efforced in order to goad the agents to invent AL close enough to NL. KL divergence with learned NL model on texts.


Results:
1) The longer the sequences of symbols allowed, the faster the protocol is learned.
2) Favouring protocols that resemble NL:
3) Are NL's statistical properties actually beneficial for communication?: 
4) Omission score analysis:
unregularized models has lower omission score thus arguing towards syllables-based nature, rather than a word with conten- and function-based nature like in NL. Regularization is at least valuable in promoting higher omission score, thus reflecting content-vs-function-based nature...


Biblio:
1) principles guiding ev. and em. of NL
2) communication-enabled collaborating agents
3) NN-based agent of the previous
4) compositionality
5) Language grounding in referential game, with indirect grounding of the AL into NL, framed as an instantiation of a VAE. Direct supervision is also discussed: Vinyals 2015.
6) emergent language biblio-related works
7) VAE-based approach to text summarization and semantic parsing.


Future Work/Limitations:
1) What could enable the RL framework to skirt the high-variance gradient problem create by the larger-than-life-sized action space? cf. dialog planning under uncertainties.
2) Would a non-stationnary-environment-purposed algorithm perform better?
3) Facing MUDs, in a SimonSays take on imitation learning, learning from the MUDs' text distribution with the KL divergence measure could be interesting in terms of emergence of an AL that is close enough to NL ?
4) Pre-trained CNN prevent the CNN to learn communication-relevant features, although we might argue that classification-relevant features should generalize towards communication-relevant features, right?},
archivePrefix = {arXiv},
arxivId = {1705.11192},
author = {Havrylov, Serhii and Titov, Ivan},
eprint = {1705.11192},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Havrylov, Titov - 2017 - Emergence of Language with Multi-agent Games Learning to Communicate with Sequences of Symbols.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/MultiAgent,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Visual Captionning},
month = {may},
title = {{Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols}},
url = {http://arxiv.org/abs/1705.11192},
year = {2017}
}


@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{maddison2017concrete,
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
  booktitle = {International Conference on Learning Representations},
  year = {2017}
}

@inproceedings{jang2017categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle = {International Conference on Learning Representations},
  year = {2017}
}

@article{zipf1949human,
  title={Human behavior and the principle of least effort.},
  author={Zipf, George Kingsley},
  year={1949},
  publisher={addison-wesley press}
}


@article{Choi2018,
abstract = {One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand- engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment.},
annote = {Show that the "agents can develop, out of raw image pixels, a language with compositional properties, given proper pressure from the environment".
--{\textgreater} reminiscent of ILM
--{\textgreater} "simulataneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols"
--{\textgreater} interesting metrics
In order to promote an high-order of abstraction, the task is framed so that the agents cannot use "piel-specific information, rather [they ought to rely on] object-related information to win the game".


Biblio:
1) deep learning in NLP
2) human learns to communicate from task-completion encouragements
3) [Mordatch2017, Kottur2017] show compositional languages but, with hand-engineered and disentangled input features... Can it be done from raw pixels ???

4) PIONEERING WORK : Batali1998
5) THEORY OF MIND: Premack and Woodruff1978
6) Human Language Development: Milligan 2007

7) In Batali1998 and Mordatch2017, "aside from using disentangled input,[...] the agents had access to the true intention of the speaker[...](meaning vector[...]) [...or...] each agent had an auxiliary task to predict the goals of all the other agents".

8) OBVERTER Technique: [Hurford1989, Oliphant {\&} Batali1997, Smith2001, Kirby {\&} Hurford2002]



Ideas:
1) Learning in supervised learning approach is limited. It is better to learn the way humans learn.
2) It has been shown that such approach works with [Mordatch2017, Kottur2017] with hand-engineered and disentangled input features... Can it be done from raw pixels ???
3) Oberter idea: "motivates an agent to search over messages and generate the ones that maximize their own understanding"
--{\textgreater} search algorithm
4) "preventing the agents from using piel-specific information, rather than object-related information to win the game."

5) On the opposite to Batali1998 and Mordatch2017, "our model uses no other signal than whether the listerner made a correct decision"
Results:
1)

Future works/Limitations:
1) Obverter idea: search algorithm can thus be handled by an MCTSNet !!
2) Sapir-Whoff hypothesis is broached upon given that they show to "learn disentangled compositional representations of visual scenes, without any supervision" apart from the "constraints imposed by their environment", through the language communication channel.
3)},
archivePrefix = {arXiv},
arxivId = {1804.02341},
author = {Choi, Edward and Lazaridou, Angeliki and de Freitas, Nando},
eprint = {1804.02341},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Lazaridou, de Freitas - 2018 - Compositional Obverter Communication Learning From Raw Visual Input(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/NLP/Visual Captioning,Article Gestion/1) TO READ/MultiAgents,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Curriculum/Active/Episodic learning},
month = {apr},
title = {{Compositional Obverter Communication Learning From Raw Visual Input}},
url = {http://arxiv.org/abs/1804.02341},
year = {2018}
}

@misc{Cao2018,
annote = {Assuming the necessity of a cooperative/coordinative task and communication protocols to see the emergence of communication, the author asks:

1) What task structures would further the emergence of communication? 

2) How protocols traits affect the fulfillment of the task?


Biblio:

1) Emergence of communication in referential games

2) Negotiation game : a model of non-cooperative games

3) complex behaviour without complex environment/data

4) self-play overfitting issue

5) Theory of Mind

On the contrary to selfish agents, Prosocial agents effectively use cheap talk (ungrounded language) to negotiate fairly, thus providing evidance that cooperation/other-agent-consideration is necessary for language emergence/grounding. 

First mover advantage, in case of a known upper limit on the number of turns, is induced through an ultimatum game on the last turn... unknown Randomly sampled upper limit solves the issue.

Use of an entropy regularization term to encourage exploration during training.

Results:

1) Selfish agent are enable to ground the cheap talk, because of agent interests being conflicting (cf. literature Crawford and Sobel 1982) 


Limitations/Questions:

1) Is there a limit on the number of symbol for each uttered message? Knowing that the work of Mordatch et al. showed that incitivising for few symbol usage helps with the compositionality of the learned language...

2) Non-iterated environments : no memory of previous interactions....

3) Could iterating the game give an incentive to develop verifiable cheap talk (impossible to lie if there is a threat of it being discovered and being punished for it...)?

3.5) Why even bother looking at multi-turn episodes without memory of the previous turns...?

4) Are both agent independent or is it a shared achitecture? Sharing the weights between the agents should make them more agreeable on a protocol, right?

--{\textgreater} Both agents being independant, they differentiate into speaker and listener eventhough the task is symmetric.

5) Zipfian distribution found in NL, reference?

6) Message content analysis protocol is quite interesting to learn about: "N-fold cross-validation" + "random baselines" where the item pool or the message transcripts are nullified.

7) bottom-up approach that learns "communicative behaviours directly from interaction with peers." --{\textgreater} "learning domain-specific reasoning capabilities from interaction, while having a general-purpose language layer at the top producing natural language."},
author = {Cao, Kris and Lazaridou, Angeliki and Lanctot, Marc and Leibo, Joel Z and Tuyls, Karl and Clark, Stephen},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2018 - Emergent Communication through Negotiation(2).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/MultiAgent,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning},
month = {feb},
title = {{Emergent Communication through Negotiation}},
url = {https://openreview.net/forum?id=Hk6WhagRW},
year = {2018}
}


@misc{cai2017making,
    title={Making Neural Programming Architectures Generalize via Recursion},
    author={Jonathon Cai and Richard Shin and Dawn Song},
    year={2017},
    eprint={1704.06611},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{luketina2019survey,
      title={A Survey of Reinforcement Learning Informed by Natural Language}, 
      author={Jelena Luketina and Nantas Nardelli and Gregory Farquhar and Jakob Foerster and Jacob Andreas and Edward Grefenstette and Shimon Whiteson and Tim Rocktäschel},
      year={2019},
      eprint={1906.03926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{chen2017towards,
  title={Towards synthesizing complex programs from input-output examples},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={arXiv preprint arXiv:1706.01284},
  year={2017}
}


@article{chang2018automatically,
  title={Automatically composing representation transformations as a means for generalization},
  author={Chang, Michael B and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:1807.04640},
  year={2018}
}

@article{saxton2019analysing,
  title={Analysing mathematical reasoning abilities of neural models},
  author={Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1904.01557},
  year={2019}
}

@article{xu2019can,
  title={What Can Neural Networks Reason About?},
  author={Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1905.13211},
  year={2019}
}


@techreport{Lazaridou2019,
abstract = {Good representations facilitate transfer learning and few-shot learning. Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity , we cast the representation learning problem in terms of learning to communicate. Our starting point sees traditional autoencoders as a single encoder with a fixed decoder partner that must learn to communicate. Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations. Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure.},
annote = {Casting the representation learning problem into a "learning to communicate" problem, in order to enhance the generalizability/reusability and the structure of the learned representation.

Motivation: 

Languages spoken by larger population exhibit simpler language with more structures and regularity. "inspired by the hypothesis that the size of a linguistic community has a causal effect on the structural properties of its language".

Metrics:

- number of idiosyncrasies.

- structure of the learned code.

Biblio:

1) Transfer learning and Few-shot

2) Disentanglement as a means towards "good" representation learning.

3) Regularization methods as a mean towards ||.

4)

5) Due to partner specificity, conceptual pacts arise and idiosyncratic utterances populate the dialogues. It can be seen as anoher approach to WITTGENSTEIN's Language Game, maybe?

6) Adversarial losses and gradient reversal layers in order to prevent co-adaptation in domain-adaptation neural frameworks. "penalize representations from retaining domain-specific information".

7) Representational Similarity Analysis (RSA) : topology comparison method, common in neuroscience.



Ideas:

1) Definition of a "good" representation, as one that is reusable / "induce the abstractions that capture the 'right' type of invariances", and thus "can allow for generalizing very quickly to a new task".

2) Work inspired by "abstraction mechanisms found in nature, to wit human language and communication", instead of disentanglement or regularization mechanisms.

3) "Test whether removing [conceptual pacts] between encoders and decoders [..] yield better generalization"? 

--{\textgreater} Reusability

--{\textgreater} structural properties.

4) Traditionnal (V)AE's loss does not incentivize towards "good representations".

5) NEURAL NETWORKS are so flexible that each encoder and decoders may very well be able to CO-ADAPT TO SEVERAL PARTNER at once. How to prevent this? 

--{\textgreater} identified by training a classifier to recognize the encoder from the latent code.

--{\textgreater} can be alleviated by introducing "adversarial losses or gradient reversal layers"

--{\textgreater} here, alleviated with an extra loss term: negative entropy of the classifier, thus incentivize towards the creation of a code that maximize the classifier's confusion.

6) Even though the pixel-level reconstruction loss is hindered, as it is only a self-supervision signal, it is not much of an issue, it is not the "goal" loss. 

7) METRICS:

a. "ease of learning": sample complexity of training new encoders and new decoders, alternatively, from a trained CbAE associated modules.

b. "transferability": sample complexity of training LINEAR classifiers on supervised learning tasks.

8) Both the original dropout and CbAE rely on the training of only a subset of the weights at a time:

--{\textgreater} dropout: "large expected overlaps between subsets"

--{\textgreater} CbAE: "fully mutually exclusive subsets"


Resulst:

a. CbAE furthers disentangled representations with a structure akin aligned with human perceptual data. 

b. Easier (sample complexity) to pick up representation...

c. the trade-off comes with the training speed that is lowered because the task is harder when the community is larger...

d. The larger the communities, the faster the transfer of the learned representation.

e. Comparing to original dropout, CbAE is the only one which provides "ease of learning" and "transferability" gains!!

f. As tested of VisA, "concepts representations are structured and disentangled". The larger the community, the higher the human-like structuration and disentanglement. 


Future works:

1) In multi-modal settings, temporally-consistent stimuli could be translated via the common latent space. Instead of a Community-based AE, we would go in the direction of a inner-self modality-based translation AE, no longer a model inspired by language but rather a model inspired by the multi-modal embedding of the way human beings learn representations about the world. 

What kind of properties could be derived of such a paradigm? Assuming a 

2) Conceptual pacts introduces idiosyncraties in the language in order to further the effectiveness of the communication. Maybe, the key to understand fully how human beings casually communicate is to be able to pick up quickly those idiosyncracies. Few-shot grounding or concepts acquisition seems to be the avenue for such a scheme.

3) The mechanism of adaptation from one encoder to its pair decoder that results in idiosyncracies could be alleviated in the human brain following the computational model where pyramidal neural layers try to encode and reconstruct their lower-level layer's activation schemes. Assuming multiple higher-level layer could tap into similar lower-level layers, the mechanism proposed in this paper could take place at the neural level, thus effectively regularizing the lower-level layer's learned representations? 

4) When it comes to the results on CIFAR-100, the community size effect is reversed (the larger the community, the smaller the sample complexity gain), the authors attribute that result to the fact that the models used are larger (thus the task is more complex). The community members are only seeing 1/K of the CbAE iterations and they have not been able to learn a relevant representation yet. More iterations would be necessary, then? But what would be the sufficient and necessary number?

5) The bit about the use of a batch normalization and its relation to the gains is not fully understood...??},
author = {Lazaridou, Angeliki},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazaridou - Unknown - SHAPING REPRESENTATIONS THROUGH COMMUNICATION.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Curriculum/Active/Episodic learning,Article Gestion/1) TO READ/Neuroscience/Language,Article Gestion/1) TO READ/NLP/Dialogue,Article Gestion/1) TO READ/Reasoning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Curriculum/Curiosity/Episodic/Imitation},
title = {{SHAPING REPRESENTATIONS THROUGH COMMUNICATION}}
}


@article{Higgins2018,
abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
annote = {How to learn representations in touch with the underlying data structure? in order to unlock:
-Data efficiency
-Robustness
-Generalisability
With the first principled definition of a disentangled representation.


Biblio:
1) representations that are invariant / equivariant to transformations.
2) actionable information: visual perception requires image acquisition to be an active process of information gathering. [69]
3) identification of relevant factors of variations of the data in an unsupervised way --{\textgreater} [67] Schmidhuber.
4) InfoGAN
5) beta-VAE literature




Ideas:
1) Natural transformations affects the world state in clear/delimited ways, either changing some aspects, or keeping others invariant.
--{\textgreater} SYMMETRY TRANSFORMATIONS

2) formal connection between symmetry groups and vector representations.

3) The study of symmetric transformations, and the Noether's theorem.

4) definition of a disentangled group actions from a set of symmetry transformations that are"generative and sufficient", acting on an abstract world state, as opposed to the observation produced from this abstract world state by a generative process.
The inference process maps the observation to the model's vector space representation.

5) Find a disentangled representation: let us find a mapping from the disentangled group actions (in the abstract state space) to the transformations in the vector space of representations, which conserves the structure of the former into the latter. 

6) disentangled representation learning in order to exhibit the desired compositionality property. for greater generalisation.

7) linear disentangled representation definition.

8) Some factors cannot be disentangled with respect to a particular decomposition of a symmetry group into subgroups. e.g. 3D rotations with respect to the X,Y,Z axis rotations subgroups decomposition that are not disentangled because those are not commutative transformations. 

9) Not all group decomposition are created equal with regards to their capacity to generalize and be useful in novel tasks. Some are clearly not desirable.

10) Active Perception is highlighted as the way towards empirically discovering useful group decompositions.
--{\textgreater} [69] "Steps Towards a Theory of Visual Information", Soatto (2010).
--{\textgreater} [55] "Challenging common assumptions in the unsupervised learning of disentangled representations." Locatello et al. (2018).
--{\textgreater} [72] "Interventional Robustness of Deep Latent Variable Models", Suter et al. (2018).
How to deal with finding the natural decompositions of the world?




-----------------

1) G-morphism == equivariant map == mapping f from the abstract world-state to the agent's representation s.t. there exists an action G onto Z , thus making the action(s) commute-able with f. EQUIVARIANCE CONDITION.

2) G-morphism require the existance of compatible action G onto Z.

3) Bijectivity and injectivity of f are good enough.
4) In theory, without any of those, we cannot invert f (e.g. occlusion...) , but in practice this can be alleviated via active sensing [69].









Limitations/Future Work:
1)},
archivePrefix = {arXiv},
arxivId = {1812.02230},
author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
eprint = {1812.02230},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higgins et al. - 2018 - Towards a Definition of Disentangled Representations.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning},
month = {dec},
title = {{Towards a Definition of Disentangled Representations}},
url = {http://arxiv.org/abs/1812.02230},
year = {2018}
}


@article{Andreas2019,
abstract = {Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.},
archivePrefix = {arXiv},
arxivId = {1902.07181},
author = {Andreas, Jacob},
eprint = {1902.07181},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas - 2019 - Measuring Compositionality in Representation Learning.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Neuroscience/Language,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Neuroscience/Multimodal fusion/Compositionality},
month = {feb},
title = {{Measuring Compositionality in Representation Learning}},
url = {http://arxiv.org/abs/1902.07181},
year = {2019}
}

@article{Cogswell2019,
abstract = {Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.},
annote = {"What are the conditions that lead to the emergence of a compositional language?"
Instantiation of a Cultural Transmission Bottleneck in the context of referential games.

Measure of the compositionality of the emerging language using interpolation.

Biblio:
1) Cultural Transmission in the context of the iterated learning model.
--{\textgreater} "language is directly but incompletely transmitted (taught) to one generation of agents from the previous generation."
--{\textgreater} "this is cast as a trade-off between expressivity and compressibility, where language must be expressive enough to differentiate between all possible meanings (e.g. objects) and compressible enought to be learned."

2) Supervised learning end of the language emergence spectrum:
--{\textgreater} machine translation, vqa, visual dialog [Das et al., 2017]

3) Of the difficulty of evaluating model on natural language because of OPEN-ENDEDNESS and STRONG PRIORS that confuses understanding metrics.

4) Let the environment and the interaction give the supervision signal: [Mikolov 2016, Gauthier 2016, Kiela 2016] + not cited [Mordatch 2018]
--{\textgreater} "If some of the agents in the environment already know a language like English then the other agents can indirectly learn that language."

5) Understanding emerging language and its difficulties: [Andreas, 2017]


IDEAS:

1) COOPERATIVE REFERENCE GAME [Kottur, 2017]

2) "we emulate cultural evolution instead of biological evolution"
--{\textgreater} Only the language changes from generation to generation, not the topology of the agent.
--{\textgreater} "Agents can directly benefit from evolutionary innovations throughout their lifetime instead of only at the beginning."

3) Implicit ILM: "different from iterated learning because our version of cultural transmission is implicit instead of explicit. Instead of teachers telling students exactly how to refer to the world, language is shared only to the extent doing so helps accomplish the goal."
--{\textgreater} periodic replacement in order to induce cultural transmission.

4) GOAL-DRIVEN NEURAL DIALOG: "Q-bot must report some attributes of an object seen only by A-bot. To accomplish this task, Q-bot must query A-bot for the information."
--{\textgreater} T-round/L-signal/object-centric referential game.

5) Policy gradient to maximize the reward: REINFORCE [Williams, 1992]

6) Training is done one pair at a time: for each new batch, an A bot is sampled, and another Q-bot is sampled. no definitive pairing!


Results:
1) method for implicit cultural transmission.
--{\textgreater} "Implicit cultural transmission does not use word-level super-vision, as opposed to explicit cultural transmission in which students are told which words refer to which objects. In implicit cultural transmission shared language emerges from shared goals."

--{\textgreater} replacement strategies in the context of a single agent A and Q: random or alternate reset.
--{\textgreater} replacement strategy: multi agent: reset of both a A bot and a Q bot: uniform random / epsilon greedy / OLDEST

2) Goal-driven Neural Dialog task


3) Compositionality is measured by generalization to held out compositions of attributes...

4) "the relationship between inter-round memory and compositionality is not clear."
--{\textgreater} "Memoryless + Overcomplete" setting is significantly hurt by the memoryless aspect.

5) Measure whether the implicit transmission of language occured or not: "assume that if two bots 'speak the same language' then that language was culturally transmitted."
--{\textgreater} Pairwise agent language (dis)similarity metric.
--{\textgreater} Clever (empirical) upper and lower bounds are computed for this metric.









Future work / limitations:
1) Stemming from biblio(4): using an obverter approach for this agent and some indirect grounding maybe then the

2) In the context of implicit cultural transmission that periodically re-initialize the agent, could we push the question further by only re-initializing the part of the agent concerned with language and letting its other modalities intact. How would the performance scale in that setting compared to the full-reset setting presented here?

3) Disentangled sensory stimuli : symbolic representation...
--{\textgreater} does it sill work with RGB images??

4) "the relationship between inter-round memory and compositionality is no clear" : investigate the use of memory by the agent in a multi-round setting...},
archivePrefix = {arXiv},
arxivId = {1904.09067},
author = {Cogswell, Michael and Lu, Jiasen and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
eprint = {1904.09067},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cogswell et al. - 2019 - Emergence of Compositional Language with Deep Generational Transmission(3).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/MultiAgent},
month = {apr},
title = {{Emergence of Compositional Language with Deep Generational Transmission}},
url = {http://arxiv.org/abs/1904.09067},
year = {2019}
}

@article{Evtimova2017,
abstract = {Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct modalities of an object, and their information exchange is bidirectional and of arbitrary duration. The multi-modal multi-step setting allows agents to develop an internal communication significantly closer to natural language, in that they share a single set of messages, and that the length of the conversation may vary according to the difficulty of the task. We examine these properties empirically using a dataset consisting of images and textual descriptions of mammals, where the agents are tasked with identifying the correct object. Our experiments indicate that a robust and efficient communication protocol emerges, where gradual information exchange informs better predictions and higher communication bandwidth improves generalization.},
annote = {Multi-modal, multi-step referential game is presented in an attempt to:

1) mimic human exchange, i.e. bidirectional and symmetric.

2) spur information exchange emergence. 

Biblio:

1) Communication as a "tool for sharing information" to "enhance learning in a difficult, sparse-reward environment".

[ Sukhbaatar(2016), Foerster (2016), Mordatch (2017), Andreas (2017)]

2) Mimic human conversation with "natural language dialogue based on a shared visual modality". [Das (2017), Strub (2017)]

3) "artificial communication can emerge through interaction with the world and/or other agents, which could then converge towards human language" [Gauthier (2016), Mikolov (2015), Lake (2016), Kiela(2016)]

4) Multi-round/step communication in the referential game context has been proposed in [Jorge (2016)] with an assymmetric communication channel though: the sender is limited to "single bit (yes/no) messages".

Ideas:
1) Multi-modality: the sender sees an image and communicate to the receiver symmetrically to it, while the receiver only have access to "textual descriptions".
WHY: it prevents the agents from "simply exchang[ing] the carbon copies of their modalities (e.g. communicating the value of an arbritrary pixel in an image) in order to solve the problem".

2) Multi step: "symmetric, high-bandwidth communication" is even further by the multi-modality that constrain the communication channel to be used at its fullest. It could be argue that multi-step is a necessity because of multi-modality: how does it scale with the length of the exchange?

3) SENDER: memory-less so that it can only focus on the current message's "question".
RECEIVER: The whole set of observation is provided to the receiver, which is similar to a full dataset discriminator strategy. It can only work better, I guess, following the insights from LABC?

4) Messages are d-dimensional vectors, thus preventing any syntax to appear. It does not have ordering, it is only a factorization. While this allow for a simple message consumption strategy with a simple RNN whose input is the message vector and output state is then used by subsequent modules, it cannot be dimmed an artificial language.

5) TRAINING SCHEME: sum of the reinforcement learning loss and the classification(cross-entropy) loss for each instance.
--{\textgreater} "negative entropies of sender's and receiver's message distributions" are regularized.
--{\textgreater} minimization of the "negative entropy of the receiver's termination distribution to encourage the conversation to be of length 1-(1/2){\^{}}Tmax on average".

6) Agents: ResNet-34 without fine-tuning.


Results:
1) agents develop the ability to manipulate the length of the conversation proportionaly to the difficulty of the task.

2) the receiver is asking progressively more specific questions as the sender's message distribution's entropy (sender's perplexity) is seen to increase (because "there are more ways to answer those highly specific questions" ?)

3) Zero-shot generalization is enhanced as the communication bandwidth is increased.

4) It is shown that the sender does shape the communication channel and this is important to enable the language to fit the needs of the task, as seen by an accelerated language grounding, yielding better data efficiency. Otherwise, the receiver is left having to ground the randomly initialized communication protocal, with less success.

5) Attention mechanism enhances the zero-shot generalization from 16{\%} to 27{\%}. It is assumed to be because attention lets the agent select well-known part of the images to make their predictions, and thus efficiently avoiding the transfer task...



Future works/ Limitations:
1) While multi modality does tackle the "random pixel carbon copy" message problem, it should be compared to [Choi (2018)]'s object-centric referential game. While one focuses on a different embodiement type, the other focuses on a symmetric/same-type but different embodiement in the 3D space. It shows that multi-modality can be thought at two different level: first, at the level of the TYPE of modality, secondly at the the level of the INSTANCE of the modality.

2) This multi-type of modality could be used to learn a translation model between the language used to textually describe the experiences and the symmetric communication channel the agents use.

3) Multi-type of modality could be refined into a cultural population where each agents would have their own specific "preferred"-type of modality, thus the emerging language would be shaped by the different aspects of each modalities.

4) Following (3), find a better way to make the sender focus on the current question more than on the others. Maybe the GPL architecture could work well here?

5) Following (4), given the success in this approach, I am interested to know if a two-timescale hierarchy of RNN (one that would consume the proper sentences and whose latest output state would be consumed as a summary of the sentence by the other rnn) would help with the appearance of a syntax and the use of proper sentences of vectors.},
archivePrefix = {arXiv},
arxivId = {1705.10369},
author = {Evtimova, Katrina and Drozdov, Andrew and Kiela, Douwe and Cho, Kyunghyun},
eprint = {1705.10369},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Evtimova et al. - 2017 - Emergent Communication in a Multi-Modal, Multi-Step Referential Game(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ,Article Gestion/1) TO READ/NLP,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
month = {may},
title = {{Emergent Communication in a Multi-Modal, Multi-Step Referential Game}},
url = {http://arxiv.org/abs/1705.10369},
year = {2017}
}


@article{Andreas201704,
abstract = {Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.},
annote = {"develop a translation model based on the insight that agent messages and natural lan- guage strings mean the same thing ifthey induce the same beliefabout the world in a listener."},
archivePrefix = {arXiv},
arxivId = {1704.06960},
author = {Andreas, Jacob and Dragan, Anca and Klein, Dan},
eprint = {1704.06960},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas, Dragan, Klein - 2017 - Translating Neuralese(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/NLP,Article Gestion/1) TO READ/Curriculum/Active/Episodic learning},
month = {apr},
title = {{Translating Neuralese}},
url = {http://arxiv.org/abs/1704.06960},
year = {2017}
}

@article{Andreas201706,
abstract = {We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a "syntax" with functional analogues to qualitative properties of natural language.},
annote = {Do "'language-like' structures" arise when training is not supervised but reinforced by an environment signal?
"Neural representations are capable of spontaneously developing a synta[, i.e. "negation, conjunction, and disjunction",] with functional analogues to qualitative properties of natural language".

--{\textgreater} Pragmatics: artificial languages utterances induce the same high-level behaviour as natural language utterances, over a set of alternative states.
--{\textgreater} Semantics: is the structure of the space of representations / the syntax of the artificial language similar to that of natural languages?

Biblio:
1) Comparison of AE model to referential game structures: "Encoder-decoder model [..] implementing an anloguous communication protocal". 
--{\textgreater} [Yu et al., 2016]

2) "COMPOSITIONALITY: the existence of operations like negation and coordination that can be applied to utterances with predictable effects on meaning".
--{\textgreater} can be seen in machine translation models: "syntactic phenomena" [Shi et al., 2016].
--{\textgreater} "semantic relationships translation" ['Levy et al., 2014].

3) GenX Dataset : [FitzGerald et al., 2013]'s communication game.

4) Linear operators are able to capture hierarchical and relational structures: [Paccanaro and Hinto, 2002, Bordes et al, 2014]

Ideas:
1) "How do we judge the semantic equivalence between natural language and vector representations?" 
--{\textgreater} formal semantics-inspired approach: "represent the meaning of messages via their truth conditions".
--{\textgreater} "The truch-conditional account of meaning suggests that we should judge e and f to be equivalent if they designate the same set of objects in the world [Davidson, 1967]" 
--{\textgreater} truth-conditional account of meaning approaches the problem of testing equivalence of messages/translations from the problem of whether they assert the same belief about the world in any systems that can consume the message (and then output a belief about the world that would have the same form, so that we can compare beliefs...).

Yet, it is not enough to compare the belief that is induced on the world that has seen it be generated by the speaker (indeed, if the speaker-listener pair is able to achieve high-accuracy on the classification task, then of course it will yield the same beliefs on the condition of the world input being the correct one), it is important to also test those messages/translations with different world inputs. So, inducing the same (conditional) beliefs independently of the world input (condition) is the true test of messages/translations equivalence.

2) "evaluate [...] our ability to understand [the RNN model's] behavior. [...] Or does it behave in a way indistinguishable from a random classifier?"
--{\textgreater} Is the inner representation shaped in an interpretable fashion, did it converge to anything interpretible at all?




Results:
1) "humans and RNNs send messages whose interpretations agree on nearly 90{\%} of object-level decisions"

2) "language-like structure naturally arises in the space of representations".
--{\textgreater} 'the model has learned a communication strategy that is at least superficially language-like: it admits representations of the same kinds of communicative abstractions that human use, and make use of these abstractions with some frequency".


Future works/Limitations:
1) "What happens when multiple transformations are applied hierarchically"?},
archivePrefix = {arXiv},
arxivId = {1707.08139},
author = {Andreas, Jacob and Klein, Dan},
eprint = {1707.08139},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas, Klein - 2017 - Analogs of Linguistic Structure in Deep Representations(3).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience/Linguistic,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Translation},
month = {jul},
title = {{Analogs of Linguistic Structure in Deep Representations}},
url = {http://arxiv.org/abs/1707.08139},
year = {2017}
}



@article{Branavan2011,
annote = {Jointly learning to ground language and learn complex policies is enabled by associating relevant textual information to game states.

Learning things that hard way is made difficult because of the "large state spaces, long planning horizons, and high-branching factors." Leveraging textual information and their proximity to observations should yield greater sample-efficiency at learning the task.

Problems:
1) Language Grounding at the descriptive and predictive (being able to identify predicates) levels
2) Learning language grounding from feedback alone / no supervision.
3) Language-based information are noisy and maybe suboptimal, yet we want to integrate it effectively into the Monte-Carlo search framework.

Approaches:
Because of (2) text information are modelled as hidden variables.

Biblio:
1) Language-grounding :
a. from parallel data (e.g. image captioning, game events paired with text commentary, robotic task described in NL), in a traduction setting. 
Use of aligned rich representations.
b.from control feedback, assuming correlation between increase of the performances and the quality of the language grounding.

Q-learning is applied on a state+manual - action+most-relevant-sentence-of-the-manual+predicate-over-that-sentence tuple of State-Action. The integrated architecture encodes state-action-manual feature that is fed to a sentence-relevance softmax layer and a sentence-predicate softmax layer. Following a winner-takes-all approach, the outputs are concatenated to the 

Experimenting on the importance of the predicate labels, it turns out that with uniformely random predicate labels the win rate drops drastically. It shows that language(/predicate labels) grounding can be learned even under a noisy feedback signal.

Future Works:
1) Efficiently learning from twitch streaming to play},
author = {Branavan, S.R.K and Silver, David and Barzilay, Regina},
doi = {10.1613/jair.3560},
file = {:home/kevin/Documents/PAPERS/LearningToWinByReadingManualsInAMonteCarloFramework.pdf.pdf:pdf},
isbn = {9781932432879},
issn = {1076-9757},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Visual Captionning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning},
pages = {268--277},
title = {{Learning to Win by Reading Manuals in a Monte-Carlo Framework}},
url = {http://dl.acm.org/citation.cfm?id=2002472.2002507},
volume = {43},
year = {2011}
}


@article{Das2017,
abstract = {We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results. First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision. Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.},
archivePrefix = {arXiv},
arxivId = {1703.06585},
author = {Das, Abhishek and Kottur, Satwik and Moura, Jos{\'{e}} M. F. and Lee, Stefan and Batra, Dhruv},
eprint = {1703.06585},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Das et al. - 2017 - Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/NLP/Dialogue,Article Gestion/1) TO READ/NLP/Visual Captioning,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Self-Play},
month = {mar},
title = {{Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.06585},
year = {2017}
}


@article{Kottur2017,
abstract = {A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.},
annote = {"while most agent-invented languages are effective (i.e.e achive near-perfect task rewards), they are decidedly not interpretable or compositional."
How to "coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate"?
"What are the conditions that lead to the emergence of human-interpretable or compositional grounded language?"


Biblio:
1) goal: "develop a goal-driven dialog agents"
--{\textgreater} slot-filling-based paradigm : historical [Lemon et al. 2006]
--{\textgreater} neural dialog paradigm: [Weston 2016, Serban 2017a,b, Das et al. 2017a]

2) Situated Language Learning: [Winograd 1971, Kirby et al. 2014] and recently [Lazaridou et al. 2017, Havrylov and Titov 2017, Mordatch and Abbeel 2017, Das et al 2017b]

Ideas:
1) "While [supervising agent to mimic the human response] teaches the agent correlations between symbols, it does not convey the functional meaning of language, grounding (mapping words to physical concepts), compositionality (combining knowledge of simpler concepts to describe richer concepts), or aspects of planning (understanding the goal of the conversation)".

2) Start from the work of [Das et al. 2017b]: look for conditions that are necessary for compositional grounded languages to emerge.
--{\textgreater} tabular Q-learning vs REINFORCE.
--{\textgreater} generalization to novel environment.

3) As opposed to Lewis' Signaling Game: the asymmetry ("A-BOT sees the object while Q-BOT does not; similarly Q-BOT knows the task while A-BOT does not") requires a dialog, thus calling for the grounding of both agent's vocabulary. 

4) The agents utters one token at a time.

5) In the REINFORCe algorithm implemented, "though the agents receive the reward at the end of gameplay, all intermediate actions [-utterances (q{\_}t and a{\_}t) and attribute prediction(w{\_}G)-] are assigned the same reward R".



Results:
1) "natural language does not emerge 'naturally' in multi-agent dialog"
--{\textgreater} OVERCOMPLETE VOCAB: when the vocabulary size is greater than the number of objects in the environment, the learned policy is entirely holistic, because it can... Does not generalize to unseen instances since the task is not shared from the Q-BOT to the A-BOT, we only observe the A-BOT describing the world...

2) RESTRICTED VOCAB--{\textgreater} 'set partitioning strategy' consitent with "known results from game theory on Nash equilibria in 'cheap talk' games [Crawford and Sobe 1982]". It is able to generalize over unseen instances as it can communicate the task from Q-BOT to A-BOT this time, but the environment description is not compositional.
--{\textgreater} OVER-PARAMETERIZED COMMUNICATION PROTOCOL: the meaning of a{\_}t=1 and a{\_}{\{}t+k{\}}=1 are different since the agent have memory.

3) MEMORY-LESS+LIMITED VOCAB A-BOT: the LSTM state is resetted in-between rounds. Enable the grounding of the "individual symbols into attributes and ther states".
--{\textgreater} is to be opposed to the work of [Das et al. 2017b] where Q-BOT is the one to be memory-less instead of A-BOT here.
--{\textgreater} generalizes greatly to unseen instances (74.4{\%}).
--{\textgreater} errors arise because of A-BOT "giving an incorrect answers despite Q-BOT asking the correct questions".


Future works/Limitations:
1)},
archivePrefix = {arXiv},
arxivId = {1706.08502},
author = {Kottur, Satwik and Moura, Jos{\'{e}} M. F. and Lee, Stefan and Batra, Dhruv},
eprint = {1706.08502},
month = {jun},
title = {{Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog}},
url = {http://arxiv.org/abs/1706.08502},
year = {2017}
}


@misc{Foerster2016,
author = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Foerster et al. - 2016 - Learning to Communicate with Deep Multi-Agent Reinforcement Learning(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/MultiAgents},
pages = {2137--2145},
title = {{Learning to Communicate with Deep Multi-Agent Reinforcement Learning}},
url = {http://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning},
year = {2016}
}


@article{Jiang2019-language-abstraction-hierarchical-rl,
abstract = {Solving complex, temporally-extended tasks is a long-standing problem in reinforcement learning (RL). We hypothesize that one critical element of solving such problems is the notion of compositionality. With the ability to learn concepts and sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL, we can acquire temporally-extended behaviors. However, acquiring effective yet general abstractions for hierarchical RL is remarkably challenging. In this paper, we propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. Our approach learns an instruction-following low-level policy and a high-level policy that can reuse abstractions across tasks, in essence, permitting agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment built using the MuJoCo physics engine and the CLEVR engine. We find that, using our approach, agents can learn to solve to diverse, temporally-extended tasks such as object sorting and multi-object rearrangement, including from raw pixel observations. Our analysis find that the compositional nature of language is critical for learning diverse sub-skills and systematically generalizing to new sub-skills in comparison to non-compositional abstractions that use the same supervision.},
annote = {"the ability to learn a variety of compositional, long-horizon skills while generalizing to novel concepts remain an open challenge"
How to leverage language structures, like compositionality, to perform long-horizon tasks and generalize to new goals in a systematic fashion?

Problem:
HRL needs sub-policies with the right kind of abstractions to be composed in good performance.

Biblio:
1) HRL's abstractions: 
--{\textgreater} hard-coded not flexible enough, i.e. task-specific.
--{\textgreater} learned: "find degenerate solutions without careful tuning"
|{\_}{\_}{\textgreater} goal-specification is the state-of-the-art but it lacks flexibility... 

2) "humans use language as an abstraction for reasoning and planning[...,] and skill acquisition [...] throughout our life." : [Gleitman, 2005 / Piantadosi, 2012]



Ideas:
1) Language "is a flexible representation for transferring a variety of ideas and intentions with minimal assumptions about the problem setting". "Compositional nature makes it a powerful abstractio nfor representing combinatorial concepts and for transferring knowledge".

2) Interpretible low-level policies via language instructions.

3) "language abstractions can be viewed as a strict generalization of goal states": "region of states that satisfy some abstract criteria"


Future Works/Limitations:
1)},
archivePrefix = {arXiv},
arxivId = {1906.07343},
author = {Jiang, Yiding and Gu, Shixiang and Murphy, Kevin and Finn, Chelsea},
eprint = {1906.07343},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2019 - Language as an Abstraction for Hierarchical Deep Reinforcement Learning.pdf:pdf},
month = {jun},
title = {{Language as an Abstraction for Hierarchical Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1906.07343},
year = {2019}
}


@article{Kottur2018,
abstract = {Visual dialog entails answering a series of questions grounded in an image, using dialog history as context. In addition to the challenges found in visual question answering (VQA), which can be seen as one-round dialog, visual dialog encompasses several more. We focus on one such problem called visual coreference resolution that involves determining which words, typically noun phrases and pronouns, co-refer to the same entity/object instance in an image. This is crucial, especially for pronouns (e.g., `it'), as the dialog agent must first link it to a previous coreference (e.g., `boat'), and only then can rely on the visual grounding of the coreference `boat' to reason about the pronoun `it'. Prior work (in visual dialog) models visual coreference resolution either (a) implicitly via a memory network over history, or (b) at a coarse level for the entire question; and not explicitly at a phrase level of granularity. In this work, we propose a neural module network architecture for visual dialog by introducing two novel modules - Refer and Exclude - that perform explicit, grounded, coreference resolution at a finer word level. We demonstrate the effectiveness of our model on MNIST Dialog, a visually simple yet coreference-wise complex dataset, by achieving near perfect accuracy, and on VisDial, a large and challenging visual dialog dataset on real images, where our model outperforms other approaches, and is more interpretable, grounded, and consistent qualitatively.},
archivePrefix = {arXiv},
arxivId = {1809.01816},
author = {Kottur, Satwik and Moura, Jos{\'{e}} M. F. and Parikh, Devi and Batra, Dhruv and Rohrbach, Marcus},
eprint = {1809.01816},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kottur et al. - 2018 - Visual Coreference Resolution in Visual Dialog using Neural Module Networks.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Natural Language Grounding,iGGi/Literature Review/Entity Recognition},
month = {sep},
title = {{Visual Coreference Resolution in Visual Dialog using Neural Module Networks}},
url = {http://arxiv.org/abs/1809.01816},
year = {2018}
}


@inproceedings{DeVries2017,
  title={Guesswhat?! visual object discovery through multi-modal dialogue},
  author={De Vries, Harm and Strub, Florian and Chandar, Sarath and Pietquin, Olivier and Larochelle, Hugo and Courville, Aaron},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5503--5512},
  year={2017}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@article{geman2015visual,
  title={Visual turing test for computer vision systems},
  author={Geman, Donald and Geman, Stuart and Hallonquist, Neil and Younes, Laurent},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={12},
  pages={3618--3623},
  year={2015},
  publisher={National Acad Sciences}
}

@inproceedings{malinowski2014multi,
  title={A multi-world approach to question answering about real-world scenes based on uncertain input},
  author={Malinowski, Mateusz and Fritz, Mario},
  booktitle={Advances in neural information processing systems},
  pages={1682--1690},
  year={2014}
}

@inproceedings{jabri2016revisiting,
  title={Revisiting visual question answering baselines},
  author={Jabri, Allan and Joulin, Armand and Van Der Maaten, Laurens},
  booktitle={European conference on computer vision},
  pages={727--739},
  year={2016},
  organization={Springer}
}

@article{kafle2017visual,
  title={Visual question answering: Datasets, algorithms, and future challenges},
  author={Kafle, Kushal and Kanan, Christopher},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={3--20},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{huang2017arbitrary,
  title={Arbitrary style transfer in real-time with adaptive instance normalization},
  author={Huang, Xun and Belongie, Serge},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1501--1510},
  year={2017}
}


@article{DAutume2019,
abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\~{}}50-90{\%}) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
annote = {Machine learning models "fail to isolate and reuse previously acquired knowledge when the data distribution shifts (e.g., when presented with a new dataset) -- a phenomenon known as catastrophic forgetting(McCloskey {\&} Cohen, 1989; Ratcliff 1990)".
Episodic memory is considered in a lifelong setup to "allow the model to continually learn from eaxmples drawn from different data distributions".
--{\textgreater} 1{\%} experience replay is enough


Biblio:
1) Catastrophic forgetting (McCloskey {\&} Cohen, 1989; Ratcliff, 1990)
--{\textgreater} how to solve it:
a. regulatization terms [Kirkpatrick et al., 2017; Zenke et al., 2017; Chaudrhy et al., 2018]
b. knowledge distillation / experience replay phase [Schwarz et al., 2018; Wang et al., 2019]
c. episodic memory module [Sprechmann et al., 2018]
--{\textgreater} combination using "samples from the episodic memory [Lopez-Paz {\&} Ranzato, 2017; Chaudhry et al., 2019]".

2) Catastrophic forgetting in language learning [Yogatama et al., 2019]

3) "memory consolidation in human learning [McGaugh, 2000]".

4) "Memory-based Parameter Adaptation (MbPA; Sprechmann et al., 2018]".


Ideas:
1) No dataset descriptor upon training.

2) 3 components:
(i) example encoder: Transformer architecture [Vaswani et al., 2017], based on BERT [Devlin et al., 2018].
(ii) task decoder: for text classification, it follows the original BERT model... for question answering, the prediction is "the start and end indices of the correct answer in the context".
(iii) episodic memory module: key -value memory
--{\textgreater} key network : pretrained BERT model that is frozen in order to prevent "key representations from drifting as data distribution changes (i.e. the problem that the key of a test example tends to be closer to keys of recently stored examples)".
--{\textgreater} Reading: with random sampling, for sparse experience replay," and K-nearest neighbors for local adaptation".

3) Use of special symbols:
--{\textgreater} beginning-of-document symbol CLS
--{\textgreater} special separator symbol SEP, between paragraph and related question.

4) Write: simple random write has been shown more efficient than "surprisal [Ramalho {\&} Garnelo, 2019]" and "forgettable eaxmples [Toneva et al., 2019]".

5) Very sparse use of epxerience replay: 100 eaxmples retrieved for every 1e4 new eaxmples. "Only perform one gradient update for the 100 retrieved eaxmples".

6) During local adaptation, "only perform L local adaptation gradient steps".


Future work/Limitations:
1) Fixed key network requires the ability to pre-trained, so it is not adaptable to non-stationary tasks like language emergence...
--{\textgreater} While MbPA [Sprechmann et al., 2018] used a trainable key network previously, it is shown here to perform worse that the fiexd counterpart...},
archivePrefix = {arXiv},
arxivId = {1906.01076},
author = {D'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
eprint = {1906.01076},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/d'Autume et al. - 2019 - Episodic Memory in Lifelong Language Learning(2).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Curriculum/Curiosity/Episodic/Imitation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Disentanglement/Life-Long Learning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP},
month = {jun},
title = {{Episodic Memory in Lifelong Language Learning}},
url = {http://arxiv.org/abs/1906.01076},
year = {2019}
}


@article{Zhang2019,
abstract = {Despite our unique ability to use natural languages, we know little about their origins like how they are created and evolved. The answer lies deeply in the evolution of our cognitive and social abilities over a very long period of time which is beyond our scrutiny. Existing studies on the origin of languages are often focused on the emergence of specific language features (such as recursion) without supporting a comprehensive view. Investigation of restricted language representations, such as temporal logic, unfortunately does not reveal much about the impetus underlying language formation and evolution, since much of their construction is based on natural languages themselves. In this paper, we investigate the origin of "natural languages" in a restricted setting involving only planning agents. Similar to a common view that considers languages as a tool for grounding symbols to semantic meanings, we take the view that a language for planning agents is a tool for grounding symbols to physical configurations. From this perspective, a language is used by the agents to coordinate their behaviors during planning. With a few assumptions, we show that language is closely connected to a type of domain abstractions, based on which a language can be constructed. We study how such abstractions can be identified and discuss how to use them during planning. We apply our method to several domains, discuss the results, and relaxation of the assumptions made.},
annote = {Language emergence have been studied from evolutionary/social/cognitive standpoints, and more specifically the emergence of some properties in language, here the authors investigate the "origin of 'natural languages' in a restricted setting involving only planning agents".

Problem:
"When are agents required to communicate?"
"how to relate the basic symbols for physical configurations to high level language symbols for language construction via a computational process"?
--{\textgreater} how communication protocol (i.e. pragmatical, context-dependant, dialogue-like,....?) emerges from a hashing protocol (i.e. 'basic symbols for physical configurations')
--{\textgreater} thus defining "a language as a symbolic system used to distinguish desirable from undesirable situations for team performance".


Biblio:
1) The "dual problem of grounding and inverse grounding": [Gauthier and Mordatch, 2016; Tellex et al., 2014]

2) "languages as a tool for grounding symbols to semantic meanings [Harnad, 1990]"

3) "the function of languages is to communicate information [Chomsky, 2014; Hauser et al., 2002]"

3) Origin of languages [Hauser et al., 2022 ; 2014].

4) "how to develop certain symbols from interactions [Savage-Rumbaugh et al., 1986]".

5) "translation problem via a learning approach" [Andreas et al., 2017].

6) "developing communication schemes to maximiez team performance [...] [Sukhbaatar et al., 2016; Goldman and Zilberstein, 2003]".

7) " language construction is focused on the 'abstraction' of physical configurations for agent coordination while communication scheme deals mainly with when communication helps improve performance".
--{\textgreater} [Konidaris et al., 2018; Erol et al., 1994] "learning action abstractions, and computing abstractions that enable more efficient planning".



Ideas:
1) "converts the language construction problem to the problem of searching for a symbolic system that specifically resolves coordination issues in cases where communication is required", thanks to the observation that "Intuitively, when there is no need to communicate, there is no need for languages".


2) Agents are faced with a task to coordinate on using a language solely before trying to execute the task: planning and execution are separated.


3) Definition of "a language as a tool to generate constraints on the physical configurations (i.e. joint states) of a domain as sentences".
--{\textgreater} a language "for a domain M=(P,A,I{\_}S) and its state graph D=(S{\_}0, E{\_}0) is a tuple L=(W,R) where W is the set of words, and R is the set of operators that can be performed on the words. Each word w $\backslash$in W represents a set of states, so that w $\backslash$includedin S{\_}0".

4) "An abstraction of a state graph (S{\_}0, E{\_}0) of a domain is a tuple A=(S,I,E) where S is a set of abstract states. Each abstract state is a subset of the original set of states and satisfies [full coverage]. $\backslash$Pi is a set of local connections among states wihtin each abstract state; E{\_}b$\backslash$in$\backslash$Pi specifies the local connections within an abstract state b$\backslash$in S. E is the set of edges that connect the abstract states".
--{\textgreater} typo, most likely: "[I] is a set of local connections among sstates withing each abstract state".

5) "Given a domain, a language is specifiable by a perfect abstraction and vice versa".


Future works/Limitations:
1) The definition of a language requires each word to be defining a subset of the vertices of the state graph, i.e. of the set of states.

2) Their definition of an abstraction might be applicable to disentanglement in the sense that full coverage is the},
archivePrefix = {arXiv},
arxivId = {1905.00517},
author = {Zhang, Yu and Wang, Li},
eprint = {1905.00517},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Wang - 2019 - From Abstractions to {\&}quotNatural Languages{\&}quot for Planning Agents.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
month = {may},
title = {{From Abstractions to "Natural Languages" for Planning Agents}},
url = {http://arxiv.org/abs/1905.00517},
year = {2019}
}



@article{Baroni2019,
abstract = {In the last decade, deep artificial neural networks have achieved astounding performance in many natural language processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language.},
annote = {Language's high productivity is handle by humans by "means of algebraic compositional rules": "Are deep networks similarly compositional?"

Biblio:
1) "cognitive or bioligical plausibility was a central concern" when developping deep nets architecture, while now those are more guided by/"optimized for practical goals".


2) "widespread interests in understanding whether [astounding performance in machine translation] depends on shallow heuristics, or whether the networks are indeed capturing grammar-based generalizations, of the sort that would be supported by symbolic compositional rules [48,49,50]".




Ideas:
1) Because NN achieve high performance in NLP then they are "worth studying from the perspective of cognitive science". It should be treated as "comparative psychology"[8]

2) Compositionality or SEMANTIC compositionality is the main focus of research.

3) "Compositional operations in language (and thought) are argued to constitute a rule-based algebraic system, of the sort that can be formally captured by symbolic functions with variable slots"
--{\textgreater} "compositionality is 'systematic', in the sense that a function must apply in the same way to all variables of the right type"
--{\textgreater} "As famously put by Fodor, if you know the correct compositional rules to understand 'John loves Mary', you must also understand 'Mary loves John'. Neural networks ar not thought to be capable of acquiring this kind of systematic rules".

4) PRODUCTIVITY != COMPOSITIONALITY !!! 
--{\textgreater} "Note that productivity per se does not entail system-atic compositionality".
--{\textgreater} e.g. "similarity-based reasoning about concept instances" is not "rule-based" generalization form.

5) Current next-word prediction training regime for language models is interesting "from a cognitive point of view, since humans in many culture are also exposed to large amounts of raw language data during acquisition, and predicting what comes next plays a central role in cognition".

6) NN are expected to solve complex linguistic tasks by tree based approach, yet they have been shown to solve them differently, and that is important in order to compare with human cognition.


7) Probing language models by adding semantical twists to sentences and then ask them to decide between two different continuation of the sentences is assumed efficient at evaluating the extent with which the model has acquired "abstract grammatical generalization" as this "nonsensical twist strips off possible semantic, lexical and collocational confounds", urging the model to capture and focus on grammar generalization to be able to solve the task.
--{\textgreater} models thus seem to perform well at that task, thus showing that they do capture grammar generalization rules. This is an indirect evidence.
--{\textgreater} but there are counter examples that would rather argue that the models learn shallow heuristics...


8) NN "can be productive without being compositional" (citing [Andreas 2019]).},
archivePrefix = {arXiv},
arxivId = {1904.00157},
author = {Baroni, Marco},
eprint = {1904.00157},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baroni - 2019 - Linguistic generalization and compositionality in modern artificial neural networks.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Dialogue / Natural Language Generation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality},
month = {mar},
title = {{Linguistic generalization and compositionality in modern artificial neural networks}},
url = {http://arxiv.org/abs/1904.00157},
year = {2019}
}


@techreport{Kemp2008,
abstract = {Algorithms for finding structure in data have become increasingly important both as tools for scientific data analysis and as models of human learning, yet they suffer from a critical limitation. Scientists discover qualitatively new forms of structure in observed data: For instance, Linnaeus recognized the hierarchical organization of biological species, and Mendeleev recognized the periodic structure of the chemical elements. Analogous insights play a pivotal role in cognitive development: Children discover that object category labels can be organized into hierarchies, friendship networks are organized into cliques, and comparative relations (e.g., ''bigger than'' or ''better than'') respect a transitive order. Standard algorithms, however, can only learn structures of a single form that must be specified in advance: For instance, algorithms for hierarchical clustering create tree structures, whereas algorithms for dimensionality-reduction create low-dimensional spaces. Here, we present a computational model that learns structures of many different forms and that discovers which form is best for a given dataset. The model makes probabilistic inferences over a space of graph grammars representing trees, linear orders, multidimen-sional spaces, rings, dominance hierarchies, cliques, and other forms and successfully discovers the underlying structure of a variety of physical, biological, and social domains. Our approach brings structure learning methods closer to human abilities and may lead to a deeper computational understanding of cognitive development. cognitive development structure discovery unsupervised learning D iscovering the underlying structure of a set of entities is a fundamental challenge for scientists and children alike (1-7). Scientists may attempt to understand relationships between biological species or chemical elements, and children may attempt to understand relationships between category labels or the individuals in their social landscape, but both must solve problems at two distinct levels. The higher-level problem is to discover the form of the underlying structure. The entities may be organized into a tree, a ring, a dimensional order, a set of clusters, or some other kind of configuration, and a learner must infer which of these forms is best. Given a commitment to one of these structural forms, the lower-level problem is to identify the instance of this form that best explains the available data. The lower-level problem is routinely confronted in science and cognitive development. Biologists have long agreed that tree structures are useful for organizing living kinds but continue to debate which tree is best-for instance, are crocodiles better grouped with lizards and snakes or with birds (8)? Similar issues arise when children attempt to fit a new acquaintance into a set of social cliques or to place a novel word in an intuitive hierarchy of category labels. Inferences like these can be captured by standard structure-learning algorithms, which search for structures of a single form that is assumed to be known in advance (Fig. 1A). Clustering or competitive-learning algorithms (9, 10) search for a partition of the data into disjoint groups, algorithms for hierarchical clustering (11) or phylogenetic reconstruction (12) search for a tree structure, and algorithms for dimension-ality reduction (13, 14) or multidimensional scaling (15) search for a spatial representation of the data. Higher-level discoveries about structural form are rarer but more fundamental, and often occur at pivotal moments in the development of a scientific field or a child's understanding (1, 2, 4). For centuries, the natural representation for biological species was held to be the ''great chain of being,'' a linear structure in which every living thing found a place according to its degree of perfection (16). In 1735, Linnaeus famously proposed that relationships between plant and animal species are best captured by a tree structure, setting the agenda for all biological classification since. Modern chemistry also began with a discovery about structural form, the discovery that the elements have a periodic structure. Analogous discoveries are made by children, who learn, for example, that social networks are often organized into cliques, that temporal categories such as the seasons and the days of the week can be arranged into cycles, that comparative relations such as ''longer than'' or ''better than'' are transitive (17, 18) and that category labels can be organized into hierarchies (19). Structural forms for some cognitive domains may be known innately, but many appear to be genuine discoveries. When learning the meanings of words, children initially seem to organize objects into nonoverlapping clusters, with one category label allowed per cluster (20); hierarchies of category labels are recognized only later (19). When reasoning about comparative relations, children's inferences respect a transitive ordering by the age of 7 but not before (21). In both of these cases, structural forms appear to be learned, but children are not explicitly taught to organize these domains into hierarchies or dimensional orders. Here, we show that discoveries about structural form can be understood computationally as probabilistic inferences about the organizing principles of a dataset. Unlike most structure-learning algorithms (Fig. 1 A), the model we present can simultaneously discover the structural form and the instance of that form that best explain the data (Fig. 1B). Our approach can handle many kinds of data, including attributes, relations, and measures of similarity, and we show that it successfully discovers the structural forms of a diverse set of real-world domains. Any model of form discovery must specify the space of structural forms it is able to discover. We represent structures using graphs and use graph grammars (22) as a unifying language for expressing a wide range of structural forms (Fig. 2). Of the many possible forms, we assume that the most natural are those that can be derived from simple generative processes (23). Each of the first six forms in Fig. 2 A can be generated by using a single context-free production that replaces a parent node with two child nodes and specifies how to connect the children to each other and to the neighbors of},
author = {Kemp, Charles and Tenenbaum, Joshua B},
booktitle = {COMPUTER SCIENCES PSYCHOLOGY SEE COMMENTARY},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kemp, Tenenbaum - 2008 - The discovery of structural form.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Paradigm},
pages = {10687--10692},
title = {{The discovery of structural form}},
url = {www.pnas.org/cgi/content/full/0802631105/DCSupplemental.www.pnas.orgcgidoi10.1073pnas.0802631105},
volume = {5},
year = {2008}
}


@article{Li&Bowling2019,
abstract = {Artificial agents have been shown to learn to communicate when needed to complete a cooperative task. Some level of language structure (e.g., compositionality) has been found in the learned communication protocols. This observed structure is often the result of specific environmental pressures during training. By introducing new agents periodically to replace old ones, sequentially and within a population, we explore such a new pressure -- ease of teaching -- and show its impact on the structure of the resulting language.},
archivePrefix = {arXiv},
arxivId = {1906.02403},
author = {Li, Fushan and Bowling, Michael},
eprint = {1906.02403},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Bowling - 2019 - Ease-of-Teaching and Language Structure from Emergent Communication.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Compositionality,Article Gestion/1) TO READ/MultiAgents,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Paradigm,Article Gestion/1) TO READ/Curriculum/Active/Episodic learning},
month = {jun},
title = {{Ease-of-Teaching and Language Structure from Emergent Communication}},
url = {http://arxiv.org/abs/1906.02403},
year = {2019}
}


@article{Lake&Baroni2018,
abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
annote = {"RNNs fail spectacularly" "when generalization requires systematic compositional skills"

This proposed SCAN dataset highlights this fact as well as how RNN's "notorious training data thirst" is able to alleviate on this lack.

Biblio:
1) Definition of "systematic compositionality, the algebraic capacity to understand and produce a potentially infinite number of novel combinations from known components [Chomsky, 1957; Montague, 1970]".

2) "neural entworks are not plausible models of the mind because they are associative devices that cannot capture systematic compositionality" [Fodor {\&} Pylyshyn, 1988; Marcus, 1998; Fodor {\&} Lepore, 2002; Marcus, 2003; Calvo {\&} Symons, 2014]


3) [Baroni, 2009] acknowledged that "even word sequences in a language only occur once, even in a large corpus", so the fact that NNs are successfull might "points to strong generalization abilities". Yet, "it is commonly observed that NNs are extremely sample inefficient, requiring very large training sets, which suggests they may lack the same algebraic compositionality that humans exploit, and they might only be sensitive to broad patterns over lots of accumulated statistics [Lake et al. 2017]".

4) EXP3: Fodor et al thought experiment suggets that meaningful embedding of (sparsely or finely) familiar word/component should enable proficient productivity and understanding in denser/broader test contexts related to that word/component. Here, the author propose to have the model being "exposed to the primitive command only denoting a certain basic action (e.g. 'jump') [and] to all primitive and composed commands for all other actions" while training, and the testing would consist solely 


Ideas;
1) "test generalization beyond the background set in systematic, compositional ways".

2) EXP1: part2: training RNNs with "varying numbers of distinct examples (the actual number of training presentations was kept constant at 100K)" yields progressively better test accuracy (1{\%}data--{\textgreater}5{\%}acc / 2{\%}data--{\textgreater}54{\%}acc / 4{\%}data--{\textgreater}93{\%}acc !!), very fast, thus, "not only can networks generalize random subsets of the tasks, they can do so from relatively sparse coverage of the compositional command space".
--{\textgreater} What was the coverage per element though? It is likely that the failure cases occurs onto the compositional stimuli whose components have the least coverage over the training set, right?


3) EXP2: "bootstrap to commands requiring longer action sequences" : only 20.8{\%} as best test accuracy with a GRU model "with considerably less capacity [..] but it uses attention, which might help, to a limited degree, to generalize to longer action sequences".


Limitations:
1)},
archivePrefix = {arXiv},
arxivId = {1711.00350},
author = {Lake, Brenden M. and Baroni, Marco},
eprint = {1711.00350},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake, Baroni - 2017 - Generalization without systematicity On the compositional skills of sequence-to-sequence recurrent networks(2).pdf:pdf},
journal = {35th International Conference on Machine Learning, ICML 2018},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Writing/referential-games+relational-reasoning,Writing/referential-games+relational-reasoning/Language Grounding},
month = {oct},
pages = {4487--4499},
title = {{Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks}},
url = {http://arxiv.org/abs/1711.00350},
volume = {7},
year = {2018}
}


@article{Liska2018,
abstract = {Neural networks are very powerful learning systems, but they do not readily generalize from one task to the other. This is partly due to the fact that they do not learn in a compositional way, that is, by discovering skills that are shared by different tasks, and recombining them to solve new problems. In this paper, we explore the compositional generalization capabilities of recurrent neural networks (RNNs). We first propose the lookup table composition domain as a simple setup to test compositional behaviour and show that it is theoretically possible for a standard RNN to learn to behave compositionally in this domain when trained with standard gradient descent and provided with additional supervision. We then remove this additional supervision and perform a search over a large number of model initializations to investigate the proportion of RNNs that can still converge to a compositional solution. We discover that a small but non-negligible proportion of RNNs do reach partial compositional solutions even without special architectural constraints. This suggests that a combination of gradient descent and evolutionary strategies directly favouring the minority models that developed more compositional approaches might suffice to lead standard RNNs towards compositional solutions.},
annote = {Can RNNs "learn to solve a function composition task compositionally, that is, by storing the constituent functions, and by combining them to solve new problems in a zero-shot fashion"?


Systematic compositionality is looked for by training a collection of networks with different weight initialization and benchmarking them on a zero-shot compositional generalization task (where components are familiar...).

While 2{\%} of the trained network achieve generalization accuracies higher than 80{\%}, they do not appear to generalize in a systematic fashion.

The authors ask "whether the compositional RNNs learned to parse the 'language' of the prompts, and thus interpret, say, the {\textless}{\textless}gc{\textgreater}{\textgreater} sequences as an instruction to apply lookup table g followed by lookup table c [that would constitute] a stronger form of compositionality, akin to the one we encounter in natural language, where string composition mirrors meaning composition [Montague, 1970]". 
They benchmarked it by "obfuscating the prompts so that it is no longer possible to identify the atomic tasks involved in a compositional operation" and we can see that it does not change the overall performance. Thus, "the latter is associated to arbitrary codes that must be memorized, rather than to a decompositional analysis of the prompts".


Results:
1) "initializations have no effect on the odds to converge to a successful model"

2) "the determining factor is the (random) order in which tasks are presented and weights updated during training".


Future Works/Limitation:
1) "testing the comprehension of the prompt 'language' by evaluating on zero-shot compositions instead of zero-shot inputs only".},
archivePrefix = {arXiv},
arxivId = {1802.06467},
author = {Li{\v{s}}ka, Adam and Kruszewski, Germ{\'{a}}n and Baroni, Marco},
eprint = {1802.06467},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li{\v{s}}ka, Kruszewski, Baroni - 2018 - Memorize or generalize Searching for a compositional RNN in a haystack(2).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Writing/referential-games+relational-reasoning,Writing/referential-games+relational-reasoning/Compositionality + Systematic Generalization,Writing/Emergent Textual Hindsight Experience Replay/Compositionality + Systematic Generalization},
month = {feb},
title = {{Memorize or generalize? Searching for a compositional RNN in a haystack}},
url = {http://arxiv.org/abs/1802.06467},
year = {2018}
}


@article{Loula2018,
abstract = {Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it's seen as key to humans' capacity for generalization in language. Recent work has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool, requiring models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as "around" and "right") in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of "X around right" to "jump around right"), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of "around right" from those of "right" and "around").},
annote = {While SCAN was about learning new embedding (i.e. few-shot learning setting), the authors repurpose SCAN to address zero-shot learning/combinations, "the network needs only to recombine well-trained functional words".



Biblio:
0) Definition of "compositionality, the algebraic capacity to understand and produce novel combinations from known components [Montague, 1970]".

1) Definition to "Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes" "and it's seen as key to the human capacity for generalization in language".


2) RNN 's thirst for data is acknowledge via [Lake et al. 2017], thus suggesting "that they lack compositional abilities".

3) Do NNs "display some degree of compositional generalization"?
--{\textgreater} 
[Fodor and Pylyshyn 1988]
[Christiansen and Chater, 1994]
[Marcus, 1998]
[ Phillips, 1998]
[Chang, 2002]
[Marcus 2003]
[van der Velde et al., 2004]
[Bowers et al., 2009]
[Botvinick and Plaut, 2009]
[Brakel and Frank, 2009]
[Frank, 2014]
Recently, [Lake and Baroni, 2018] revisited it "in light of the latest advances in deep neural networks for natural language processing".

4) [Lake and Baroni. 2018] "showed [that RNNs have] impressive zero-shot generalization capabilities when commands were arbitrarily split between train and test set, but they failed in cases that required systematic compositionality, that is, to extract algebraic composition rules from the training examples".


5) "problem of quicklu learning meaningful new-word embeddings [Herbelot and Baroni, 2017 ; Lampinen and McCelland, 2017]" and not "strictly a failure of compositionality".



Ideas:

0) "repurpose SCAN to test another kind of compositionality, namely one that requires combining highly familiar words in new ways to create novel meaning".
--{\textgreater} "[RNNS] performance dramatically decreases as the difference between training and testing becomes more systematic, even though all test examples could be correctly processed by relying on simple composition rules amply illustrated in the training data."


1) The "manner adverbs [..] such as 'around' and 'opposite', act as second-order modifiers"


2) "Need to study what are the right priors to encode in seq2seq models to endow them with the ability of systematic generalization without losintg their generality".



Limitations/FutureWorks:
1) [Lake and Baroni, 2018] revisits compositional generalization issues in DNN for NLP solely. It calls for revisiting it in other paradigm.},
archivePrefix = {arXiv},
arxivId = {1807.07545},
author = {Loula, Jo{\~{a}}o and Baroni, Marco and Lake, Brenden M.},
eprint = {1807.07545},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loula, Baroni, Lake - 2018 - Rearranging the Familiar Testing Compositional Generalization in Recurrent Networks(4).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Instructions / Goal-based / Skills,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Writing/referential-games+relational-reasoning,Writing/referential-games+relational-reasoning/Language Grounding,Writing/referential-games+relational-reasoning/Compositionality + Systematic Generalization,Writing/Emergent Textual Hindsight Experience Replay/Compositionality + Systematic Generalization,Writing/Emergent Textual Hindsight Experience Replay/Language Grounding},
month = {jul},
title = {{Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks}},
url = {http://arxiv.org/abs/1807.07545},
year = {2018}
}


@article{Bahdanau2019,
abstract = {Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.},
annote = {What is required to generalize systematically?
--{\textgreater} Results: 
- modularity
- explicit regularizers
- explicit priors
"investigate the impact of explicit modularity and structure on systematic generalization of NMNs and contrast their generalization abilities to those of generic models."

"We have conducted a rigorous investigation of an important form of systematic generalization re- quired for grounded language understanding: the ability to reason about all possible pairs of objects despite being trained on a small subset of such pairs."


Biblio:
0) SYSTEMATICITY in CONNECTIONNISM: 
"The question of whether or not connectionist models of cognition can account for the systematicity phenomenon has been a subject of a long debate in cognitive science (Fodor {\&} Pylyshyn, 1988; Smolensky, 1987; Marcus, 1998; 2003; Calvo {\&} Colunga, 2003)."

1)"Despite theses successes [in Recognizing Textual Entailment, Visual Question Answering, and Reading Comprehension], a growing body of literature suggests that these approaches do not generalize outside of the specific distributions on which they are trained, something that is necessary for a language understanding system to be widely deployed in a the real world."

2) NN shortcomings and reliance on 'statistical regularities': "neural models easily latch onto statistical regularities which are omnipresent in existing datasets (Agrawal et al., 2016; Gururangan et al., 2018; Jia {\&} Liang, 2017) and extremely hard to avoid in large scale data collection."

3) RNN shortcomings with the work of [Lake {\&} Baroni,2018] (and [Loula2018 etc...]).

4) SYSTEMATICITY [Fodor {\&} Pylyshyn, 1988]

5) MODULARITY to the rescue... but shortcomings in "large amount of domain knowledge that is required to decide [Andreas et al. 2016] or predict [Johnson et al. 2017; Hu et al. 2017] how the modules should be created (parametrization) and how they should be connected (layour) based on a natural language utterance".

--{\textgreater} still only performs as good as "FiLM [Perez et al, 2017], Relations Networks [Santoro et al., 2017], and MAC networks [Hudson {\&} Manning, 2018]".


Ideas:
1)"Among many systematic generalization requirements that are desirable for a VQA model, we choose the following basic one: a good model should be able to reason about all possible object combinations despite being trained on a very small subset ofthem."
--{\textgreater} ONLY EVALUATES IN A ZERO-SHOT FASHION: "The main challenge in SQOOP is that models are evaluated on all possible object pairs, but trained on only a subset of them."


Results:
1) CNN+LSTM and RelNet completely fail the SQOOP test.

2) MAC and FiLM fails the harder versions (with lower rhs/lhs parameter).

3) The MAC models sometimes generalize strongly (on the hardest version then) when it is able to develop an adecuate focus strategy.



Discussions/Future Works/Limitations:
1) Their results is a counter argument to [Hill2019:Emergence of Systematic Generalization], where systematic generalization emerges without explicit priors nor modularity but simply via a great number of values per attribute, egocentric first-person viewpoint, and some other aspects of 3D embodiement.
--{\textgreater} Thus, the two raise the question, is it because we are considering relational reasoning here?
If so, then let us tackle relational reasoning with some other training scheme and see if it changes anything, e.g. referential games, (related works on learning analogies by contrasting paper!!!):

EXPERIMENT: pre-training a referential game on a relational reasoning dataset (e.g. deterministic Sort-Of-CLEVR) and evaluate whether there is any success at the game itself, and whether the language converge on a compositional and systematic code.
EXPERIMENT2: How many values per attribute does it takes? thus better than sort of clevr let us build an alphabet-based sort of clevr dataset where we can have many shapes and/or many colors... AhSoCLEVR would thus be deterministic and encompass all the possible combinations (even overlapping ones, thus introducing some important concern about infering the lack of existance vs existance...). object id and color id could be disentangled too, thus having many objects of the same color and or the same shape too...
--{\textgreater} Actually no, it is important the the color be distinct to make sure we can refer to the objects at question time...

LIMIT1:
"in particular its simplest binary form, when the answer is either “yes” or “no”."},
archivePrefix = {arXiv},
arxivId = {1811.12889},
author = {Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
eprint = {1811.12889},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau et al. - 2018 - Systematic Generalization What Is Required and Can It Be Learned(3).pdf:pdf},
journal = {International Conference on Learning Representations},
mendeley-groups = {Article Gestion/1) TO READ/Compositionality,Writing/referential-games+relational-reasoning/Compositionality + Systematic Generalization,Writing/Emergent Textual Hindsight Experience Replay/Compositionality + Systematic Generalization},
month = {nov},
title = {{Systematic Generalization: What Is Required and Can It Be Learned?}},
url = {http://arxiv.org/abs/1811.12889},
year = {2019}
}


@article{Hill2019,
abstract = {The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we demonstrate strong emergent systematic generalisation in a neural network agent and isolate the factors that support this ability. In environments ranging from a grid-world to a rich interactive 3D Unity room, we show that an agent can correctly exploit the compositional nature of a symbolic language to interpret never-seen-before instructions. We observe this capacity not only when instructions refer to object properties (colors and shapes) but also verb-like motor skills (lifting and putting) and abstract modifying operations (negation). We identify three factors that can contribute to this facility for systematic generalisation: (a) the number of object/word experiences in the training set; (b) the invariances afforded by a first-person, egocentric perspective; and (c) the variety of visual input experienced by an agent that perceives the world actively over time. Thus, while neural nets trained in idealised or reduced situations may fail to exhibit a compositional or systematic understanding of their experience, this competence can readily emerge when, like human learners, they have access to many examples of richly varying, multi-modal observations as they learn.},
annote = {What are the "factors that support [strong emergent systematic generalisation in a neural network]"?
"Our focus here, however, is on providing careful experiments that isolate the significance of environmental factors on systematicity and to provide measures of the robustness (i.e. variance) of these effects."

(i) "number of object/word experiences in the training set"
(ii) "the invariances afforded by a first-person, egocentric perspective"
(iii) "the variety of visual input experienced by an agent that perceives the world actively over time"

"when and under what conditions neural networks are systematic"?


Biblio:

0) Definition of EMERGENT SYSTEMACITY: "because the architecture of our agent does not include components that are explicitly engineered to promote systemacity, we refer to the behaviour it exhibits as emergent systematicity [McClelland et al., 2010]"

1) "Language can serve to promote compositional behaviour in deep RL agents (Andreas et al., 2017)"

2) "the human capacity to exploit the compositionality of the world, when learning to generalize in systematic ways, might be replicated in artificial neural networks if those networks are afforded access to a rich, interactive, multimodal stream of stimuli that better matches the experience of an embodied human learning (Clerkin et al., 2017; Kellman {\&} Arterberry, 200; James et al., 2014; Yurovsky et al., 2013; Anderson, 2003)"

3) "Systematicity is the property of human cognition whereby "the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents" (Fodor {\&} Pylyshyn, 1988)".



4) "systematic generalisation is also sometimes referred to as 'combinatorial generalization' (O'reilly, 2001; Battaglia et al., 2018)"

5) "systematic generalization requires inductive biases that are specifically designed to support some form of symbolic computation":
- graphs (Battaglia et all., 2018),
- modular-components defined by symbolic parses (Andreas et al., 2016; Bahdanau et al., 2018),
- explicit latent variables (Higgins et al., 2017),
- neuro-symbolic hybrid methods (Mao et al., 2019)

5.5) but there are some "instances of systematicity in the absence of such specific inductive biases (Chaplot et al., 2018; Yu et al., 2018; Lake, 2019).

6) They concluded that "language is not a large factor (and certainly not a necessary cause) of the systematic generalisation that [they] have observed emerging in other experiments".
--{\textgreater} Nevertheless, it might be that the egocentric viewpoint does enforce an AVF framing that can also be enforced by a language referential game. Thus, if the AVF approach is of importance, then it is worth comparing using a language-like communication channel to a VAE-like communication channel (that is not a communication channel...).

UNDERSTANDING:
1) They refer to the first-person egocentric perspective and to the diversity of perceptual input that it affords both as seperate conditions to emergent systemacity, so is there something else that the egocentric perspective brings on to the table? How as it been identified?
--{\textgreater} In further investigations, they question whether the partial observability is in cause, and it was shown to be an aspect of concern in the 2D world, at least...

Limitations/Future works:
1) "we do not provide give a precise answer to the question of why? systematic generalisation necessarily emerges"
--{\textgreater} this question could be addressed by the AVFs approach to what good learned representation are.
In this context, though, there is no explicit minimax formulation of the problem, although the invariances provided by the egocentric viewpoint does emphasize an implicit formulation of it all.
--{\textgreater} It may be worth investigating whether an explicit minimax formulation of the training process (by using a referential game) would yield greater results (maybe in terms of sample-efficiency alone...)? It is not clear how a fair comparison could be undertaken though, since it would be an RL game with auxiliary tasks... or with pre-training or fixed-interval training maybe?


2) What is the sample-efficiency though? It seems to be awfully sample inefficient: 100 Millions episodes or timesteps according to Fig2. This is clearly not the same as "systematic generalization" as it requires "piling up evidences", althought it is arguably 'emergent' systematicity?
--{\textgreater} It thus becomes very interesting to make this emergent systematicity way more sample-efficient, with some auxiliary tasks or pre-training...},
archivePrefix = {arXiv},
arxivId = {1910.00571},
author = {Hill, Felix and Lampinen, Andrew and Schneider, Rosalia and Clark, Stephen and Botvinick, Matthew and McClelland, James L. and Santoro, Adam},
eprint = {1910.00571},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hill et al. - 2019 - Emergent Systematic Generalization in a Situated Agent.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Disentanglement/Life-Long Learning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Writing/referential-games+relational-reasoning/Compositionality + Systematic Generalization,Writing/Emergent Textual Hindsight Experience Replay/Compositionality + Systematic Generalization,Writing/referential-games+relational-reasoning/Language Grounding,Writing/Emergent Textual Hindsight Experience Replay/Language Grounding},
month = {oct},
title = {{Emergent Systematic Generalization in a Situated Agent}},
url = {http://arxiv.org/abs/1910.00571},
year = {2019}
}

@techreport{Hill2019a,
abstract = {Analogical reasoning has been a principal focus of various waves of AI research. Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience. Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data. We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model. The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features. Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.},
annote = {Make analogies in simple neural networks with data input stream design, with raw data input, thus "apply[ing] abstract relations to never-before-seen source-target domain mappings, and even to entirely unfamiliar target domains".

ANALOGY: "flexibly map familiar relations from one domain of experience to another", potentially unknown.
"Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experiences".



Ideas:
1) Not about architecture but more about training scheme, in terms of instantiating the insights of cognitive theories. "the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model."

2) Stimulus representation and cross-domain mapping are the main focus.

3) LABC: training by constrasting on the candidate answers, since the "c{\_}i all completes a decoy relation $\backslash$hat{\{}r{\}}{\_}i != r with the target sequence".


Biblio:
0) Analogy making, what is it?
"The Romans ‘aligned' relational principles about water waves (periodicity, bending round corners, rebounding off solids) to phenomena observed in acoustics, in spite of the numerous perceptual and physical differences between water and sound. This flexible alignment, or mapping, of relational structure between source and target domains, independent of perceptual congruence, is a prototypical example of analogy making."

1) Structure Mapping Theory : analogical reasoning.
--{\textgreater} similarity vs analogy
--{\textgreater} learned representations are not dependant on the alignment process/mapping using those.
2) High-Level Perception theory of analogy
--{\textgreater} stimulus representations are tied to alignment processes.

Future Works/Limitations:
1) Cross-domain mapping-focused training might prevent extrapolation to never-before-seen domain ? (mapping overfitting?)
- never-before-seen pairs of target and source domains are tested on, thus ensuring generalizability, but not ensuring extrapolation to never-before-seen domains.
2) Extrapolation to never-before-seen target domains and/or attributes are finally tackled in an experiment.

3) What about extrapolation to never-before-seen target domains raw visual inputs: it would require compositional understanding on the raw visual inputs for it to fit into many subcategories that would have been seen during training.
--{\textgreater} Compositional Matching/Decompositional Network?},
archivePrefix = {arXiv},
arxivId = {1902.00120v1},
author = {Hill, Felix and Santoro, Adam and {T Barrett}, David G and Morcos, Ari and {Lillicrap Deepmind}, Tim},
eprint = {1902.00120v1},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hill et al. - Unknown - LEARNING TO MAKE ANALOGIES BY CONTRASTING ABSTRACT RELATIONAL STRUCTURE.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Neuroscience,Article Gestion/1) TO READ/Reasoning,Writing/referential-games+relational-reasoning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience/Multimodal fusion/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Paradigm,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,Writing/referential-games+relational-reasoning/(Contrastive) Training Schemes,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Disentanglement/Life-Long Learning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
title = {{LEARNING TO MAKE ANALOGIES BY CONTRASTING ABSTRACT RELATIONAL STRUCTURE}}
}



@article{Slowik2020,
abstract = {Human language and thought are characterized by the ability to systematically generate a potentially infinite number of complex structures (e.g., sentences) from a finite set of familiar components (e.g., words). Recent works in emergent communication have discussed the propensity of artificial agents to develop a systematically compositional language through playing co-operative referential games. The degree of structure in the input data was found to affect the compositionality of the emerged communication protocols. Thus, we explore various structural priors in multi-agent communication and propose a novel graph referential game. We compare the effect of structural inductive bias (bag-of-words, sequences and graphs) on the emergence of compositional understanding of the input concepts measured by topographic similarity and generalization to unseen combinations of familiar properties. We empirically show that graph neural networks induce a better compositional language prior and a stronger generalization to out-of-domain data. We further perform ablation studies that show the robustness of the emerged protocol in graph referential games.},
annote = {What are the best inductive biases to foster compositionality in emerging languages: graph networks!
--{\textgreater} better topo sym
--{\textgreater} stronger (systematic) generalization
--{\textgreater} vs BoW vs Seq2Seq


Biblio:
1) COMPOSITIONALITY / SYSTEMATIC GENERALIZATION : [Smith et al. 2003, Andreas 2019, Baroni 2020 / Bahdanau et al 2019]

2) "the assumptions that human language derives meaning from its use [Wittgenstein, 1953]".
--{\textgreater} introduce the rationale of emergent communication studies


3) Definition of referential games, does not address all the possible variants that may be implemented!!!
--{\textgreater}PROS: motivate the use of ref game for it "mitigates some of the issues observed in supervised training of language models such as sample inefficiency and exploiting superficial statistical signals [Lake et al, 2017; Baroni 2020]"

--{\textgreater}CONS: limitation in the fact that successful communication is "insufficient tp guarantee emergence of a compositional language [Kottur et al. 2017]".

--{\textgreater} HOW were the limitations addressed so far: [Lazaridou 2018; Li and Bowling 2019; Resnick 2019]...

3)


LIMITATIONS:
1) What are the data like?! "the hierarchy of an object and its properties is represented as a tree of fixed topology. Each child node encodes the property value in a concatenation of the one-hot vectors representing the property and the property value, respectively". So, the input data are graph objects.

2) There are means to expand this work:
- by adapting it to pixel inputs (dSprites dataset / object dataset in Lazaridou 2018)
- extanding to pixel inputs using Relational Networks (RN/ SARN/ MHDPA etc... Recurrent RNs?)
- comparing ST-GS vs. Obverter vs. Consistent Obverter
--{\textgreater} Is there a combinaison of architecture bias and agent bias that yields better results?!},
archivePrefix = {arXiv},
arxivId = {2002.01335},
author = {S{\l}owik, Agnieszka and Gupta, Abhinav and Hamilton, William L. and Jamnik, Mateja and Holden, Sean B. and Pal, Christopher},
eprint = {2002.01335},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\l}owik et al. - 2020 - Exploring Structural Inductive Biases in Emergent Communication.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Neuroscience/Language,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Compositionality},
month = {feb},
title = {{Exploring Structural Inductive Biases in Emergent Communication}},
url = {http://arxiv.org/abs/2002.01335},
year = {2020}
}


@article{Korrel2019,
abstract = {While sequence-to-sequence models have shown remarkable generalization power across several natural language tasks, their construct of solutions are argued to be less compositional than human-like generalization. In this paper, we present seq2attn, a new architecture that is specifically designed to exploit attention to find compositional patterns in the input. In seq2attn, the two standard components of an encoder-decoder model are connected via a transcoder, that modulates the information flow between them. We show that seq2attn can successfully generalize, without requiring any additional supervision, on two tasks which are specifically constructed to challenge the compositional skills of neural networks. The solutions found by the model are highly interpretable, allowing easy analysis of both the types of solutions that are found and potential causes for mistakes. We exploit this opportunity to introduce a new paradigm to test compositionality that studies the extent to which a model overgeneralizes when confronted with exceptions. We show that seq2attn exhibits such overgeneralization to a larger degree than a standard sequence-to-sequence model.},
annote = {Seq2Seq models fail to generalize compositionally.

"to what extent can [neural networks] high accuracy be taken as evidence that they in fact understood the task they are modeling"? Literature says none.

Authors introduce a new architecture specifically designed to "exploit attention to find compositional patterns in the input", as the language modeling task is about understanding compositionality and other linguistic properties of natural language (recurrence and hierarchy).


--{\textgreater} "in search for a neural network architecture that exhibits a bias towards systematic generalization"

--{\textgreater} introduces "new paradigm to test compositionality that studies the extent to which a model overgeneralizes when confronted with exceptions".


Biblio:
1) "[previous work] argue that rather than understanding those implicit rules and being able to compositionally apply them, RNN models exploit biases in the data that are unrelated to the underlying system".
--{\textgreater} "the lack of understanding of the actual task leads to sample inefficiency, inability to transker knowledge between tasks and difficulty to generalize to sequences that are drawn from the same rule space, but differ distributionally from the training data".

2) Attentive Guidance [Hupkes et al. 2018a]

3)


Ideas:
1) --{\textgreater} acknowledged the need for tasks involving natural language modeling and translation that would be "directly linked to compositionality".
--{\textgreater} propose then to monitor "to what extent a model overgeneralizes".

2) New component to address the "lack of understanding of the actual task", "which is a recurrent attention module that can be integrated in any form of encoder-decoder model, [that] modulates the information flow from encoder to decoder".
--{\textgreater} then a good test would be to test its "sample []efficiency, []ability ot transfer knowledge between tasks and [ease] to generalize to sequences that are drawn from the same rule space...."


3) "Using two tasks that are designed such that their accuracy reflects directly whether the underlying rule-based system is learned -- the lookup table task [Liska et al. 2108] and SCAN [Lake and Baroni, 2018; Loula et al. 2018]"

4) Rather than "designing models that have compositionality explicitly built in", the authors propose to "focus on inducing compositional solutions in RNN models, that are less rigid and generally require fewer supervision".


5) Attentional mechanisms commonly rely on softmax distribution which "often results in distributed vectors that attend to many input symbols at the same time, while an ideal compositional attention vector only focuses on the relevant parts of the input. To force the transcoder to be more selective in the information it selects, we use Gumbel-Softmax".


6) The decoder is initialized with a fixed, learned initialization vector.
--{\textgreater} "restricts the decoder to work only with disentangled representations of the input sequence, which encourages it to learn and process the individual meaning of all input symbols".

7) In order to "make the output of the decoder at decoding step t more directly dependent on the current context vector c{\_}t", full focus is applied as a "multiplication of the context vector with the previous hidden state of the decoder".

8) Overgeneralizations is painted as a proxy to evaluate the extent with which a given system has a compositional bias.
--{\textgreater} "Whether overgeneralization is actually preferentiable behavior is depend on the task to be solved".
--{\textgreater} Accuracy on the original target (for exceptions inputs) is reported as a measure of the extent with which a model processes exceptions compositionally (instead of memorizing as it is exposed to those exceptions in the training set).


9) Progressively growing the training set with 1 filler conditions on the SCAN dataset with the "primitives around right" context shows that "Seq2attn [...] is able to induce the compositional rules" whereas, "rather than systematically understanding the rule, the [seq2seq] model is piling up evidence for a very specific pattern".







Limitations/Future Works:
1) "the flow of information from transcoder to decoder is very rigid"
--{\textgreater} "use of less skewed activations than Gumbel-Softmax such as Sparsemax[] using adaptive computation time[]"

2) Incoporating it into the THER architecture could help evaluate its sample efficiency?},
archivePrefix = {arXiv},
arxivId = {1906.01234},
author = {Korrel, Kris and Hupkes, Dieuwke and Dankers, Verna and Bruni, Elia},
eprint = {1906.01234},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Korrel et al. - 2019 - Transcoding compositionally using attention to find more generalizable solutions.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Writing/Emergent Textual Hindsight Experience Replay/Compositionality + Systematic Generalization,Writing/referential-games+relational-reasoning/Compositionality + Systematic Generalization},
month = {jun},
title = {{Transcoding compositionally: using attention to find more generalizable solutions}},
url = {http://arxiv.org/abs/1906.01234},
year = {2019}
}


@article{Hupkes2019,
abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
annote = {What are the many ways in which models can exhibit compositionality?


Ideas:

LOCALISM: 
"In its basic form, the principle of compositionality states that the meaning of a complex expression derives from the meanings of its constituents and how they are combined. It does not impose any restrictions on what counts as an admissible way of combining different elements, which is why the principle taken in isolation is formally vacuous."
i.e. [Janseen 1983] showed that "arbitrary sets of expressions can be mapped to arbitrary sets of meanings without violating the principle of compositionality, as long as one is not bound to a fixed syntax."
--{\textgreater} "As a consequence, the interpretation of the principle of compositionality dependds on the type of constraints that are put on the semantic and syntactic theories involved."

--{\textgreater} "One important consideration concerns -- on an abstract level -- how local the compositional operations should be." I.e. "When operations are very local (a case also referred to as strong or first-level compositionality), the meaninig of complex expression depends only on its local structure and the meanings of its immediate parts [Pagin and Westerstahl, 2010; Jacobson 2002]".

ISSUE: "In global or weak compositionality, the meaning of an expression follows from its total (global) structure and the meanings of its atomic parts. In this interpretation, compounds can have different meanings, depending on the larger expression that they are a part of."

"As a contrast, consider an arithmetic task, where the outcome of 14-(2+3) does not change when the subsequence (2+3) is replaced by 5, a sequence with the same (local) meaning, but a different structure.

TEST: ??????!!



SYSTEMATICITY: "rather than asking if a model is systematic, a more interesting question is qhether the rules and constituents the model uses are in line with what we believe to be the actual rules and constituents underlying a particular data set or language."

OVERGENERALIZATION: "(when) do they consistently follow a global rule set, and (when) do they (over)fit the traning samples individually?"
--{\textgreater} In NLP: " While it is generally agreed upon that compositional skills are required to appropriately model natural language, successfully modelling natural data requires far more than understanding compositional structures. As a consequence, a negative result may stem not from a model's incapacity to model compositionality, but rather from the lack of signal in the data that should induce compositional behaviour. A positive result, on the other hand, cannot necessarily be explained as successful compositional learning, since it is difficult to establish that a good performance cannot be reached through heuristics and memorisation."



UNDERSTANDING:
1) LOCALISM: In the context of visual compositionality, what are the "semantic and syntactic theories involved" then?},
archivePrefix = {arXiv},
arxivId = {1908.08351},
author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
eprint = {1908.08351},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hupkes et al. - 2019 - Compositionality decomposed how do neural networks generalise.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Neuroscience/Language,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Compositionality,Writing/Emergent Textual Hindsight Experience Replay/Compositionality + Systematic Generalization,Writing/referential-games+relational-reasoning/Compositionality + Systematic Generalization},
month = {aug},
title = {{Compositionality decomposed: how do neural networks generalise?}},
url = {http://arxiv.org/abs/1908.08351},
year = {2019}
}


@article{Battaglia2018,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences—a hallmark of human intelligence from infancy—remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between " hand-engineering " and " end-to-end " learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias—the graph network —which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning.},
author = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and graph networks.pdf:pdf},
mendeley-groups = {iGGi,iGGi/Literature Review,Article Gestion/1) TO READ,Writing/referential-games+relational-reasoning/(Spatial) Relational Reasoning},
title = {{Relational inductive biases, deep learning, and graph networks}},
url = {https://arxiv.org/pdf/1806.01261.pdf},
year ={2018}
}

@article{Fodor&Pylyshyn1988,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W and others},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988}
}


@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
author = {Santoro, Adam and Raposo, David and Barrett, David G T and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy and London, Deepmind},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - Unknown - A simple neural network module for relational reasoning(3).pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {iGGi/Literature Review/Entity Recognition,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning},
title = {{A simple neural network module for relational reasoning}},
url = {https://arxiv.org/pdf/1706.01427.pdf},
year = {2017}
}

@article{Kirby2014,
abstract = {Iterated learning describes the process whereby an individual learns their behaviour by exposure to another individual's behaviour, who themselves learnt it in the same way. It can be seen as a key mechanism of cultural evolution. We review various methods for understanding how behaviour is shaped by the iterated learning process: computational agent-based simulations; mathematical modelling; and laboratory experiments in humans and non-human animals. We show how this framework has been used to explain the origins of structure in language, and argue that cultural evolution must be considered alongside biological evolution in explanations of language origins.},
annote = {Survey tools and models that argues that "key structure design features of language" can be explained by the cultural transmission of language.

Biblio:
1) "Computational simulation as a tool for modelling the [bio] and [cult] evolution of language." {\textless}-- cf. Hurford [9]

2) Role of interaction and negotiation [10,11]
3) Role of learners' biases [12,13] 

4) Iterated learning as a fitting model, emphasizing the "role of the learning bottleneck" [14,15 and 4,15-19]

5) Biological evolution mathematical model -the replicator dynamics- can be used to investigate language evolution. [29-32]

) Effect of Transmission on Language: Vertical transmission chains with iterated learning, and assumption that learners perform bayesian inferences. Yields a prior distribution over languages.


Future Works/Limitations:


) Use this framework to put high likelyhood on natural languages in order to investigate the emergence of an AL highly similar to NL.},
author={Kirby, Simon and Griffiths, Tom and Smith, Kenny},
file = {:home/kevin/Documents/PAPERS/Language/Kirby-2014-IteratedLearningAndTheEvolutionOfLanguage.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Neuroscience,Article Gestion/1) TO READ/Linguistic},
pages = {108--114},
title = {{Iterated learning and the evolution of language}},
url = {https://www.sciencedirect.com/science/article/pii/S0959438814001421},
publisher={Elsevier},
journal={Current opinion in neurobiology},
volume = {28},
year = {2014}
}

@article{kirby2014iterated,
  title={Iterated learning and the evolution of language},
  author={Kirby, Simon and Griffiths, Tom and Smith, Kenny},
  journal={Current opinion in neurobiology},
  volume={28},
  pages={108--114},
  year={2014},
  publisher={Elsevier}
}


@article{Guo2019,
  title={The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents},
  author={Guo, Shangmin and Ren, Yi and Havrylov, Serhii and Frank, Stella and Titov, Ivan and Smith, Kenny},
  journal={arXiv preprint arXiv:1910.05291},
  year={2019}
}


@article{Kirby2015,
abstract = {Language exhibits striking systematic structure. Words are composed of combinations of reusable sounds, and those words in turn are combined to form complex sentences. These properties make language unique among natural communication systems and enable our species to convey an open-ended set of messages. We provide a cultural evolutionary account of the origins of this structure. We show, using simulations of rational learners and laboratory experiments, that structure arises from a trade-off between pressures for compressibility (imposed during learning) and expressivity (imposed during communication). We further demonstrate that the relative strength of these two pressures can be varied in different social contexts, leading to novel predictions about the emergence of structured behaviour in the wild.},
annote = {"Language exhibits striking systematic structure"
What explanatory mechanism?
--{\textgreater} "Cultural evolutionary account of the origins of this structure"
--{\textgreater} "structure arises from a trade-off between pressures for compressibility (imposed during learning) and expressivity (imposed during communication)"

"However, no one model or experimental paradigm completely decouples learnability and expressivity"?



Biblio:
1) "The precise way in which this combinatorial and compositional structure is realised differs from language to language and is part of the knowledge that each language learner must acquire"

2) [Hockett, 1960]: "this kind of systematicity [..] is one of the fundamental design features of human language", talking about combinatorial and compositional structure.

3) "trade-off between competing pressures has a long history" 
--{\textgreater} in cognitive sciences: [Zipf, 1949]... "utterances in a language will tend to minimise effort for the speaker as long as distinctiveness for the hearer is not compromised"
--{\textgreater} in linguistics: [Givon, 1979][Kirby, 1997; J{\"{a}}ger, 2007]
--{\textgreater} leads to "the inverse relationship between frequency and length of words identified by Zipf (1936)"

4) Grammars: "concise descriptions of the generative system underlying a language"

5) [Chater and Vitanyi, 2003][Kemp and Regier 2012]: "a preference for simplicity is a fundamental cognitive principle: languages which permit the formation of compressed mental representations are easier to learn than those which do not"

6) "natural language lexicons exhibit a newr-optimal trade-off between these two pressures, [compressibility and expressivity,] being among the most expressive and yet compressible of all possible systems" [Kemp and Regier, 2012]

7) PROBLEM OF LINKAGE [Kirby, 1999]: 'provide an explanatory mechanism for this striking fit between the design and the function of language'

8) Iterated Learning and bottleneck... ... [Kirby, Cornish {\&} Smith, 2008]
--{\textgreater} [Kirby, 2002; Brighton, Smith {\&} Kirby, 2005b]
--{\textgreater} expressivity bias : {\{}Brighton et al., 2005b] 
--{\textgreater} "transmission chain method" [Mesoudi{\&} Whiten, 2008]


9) Combinatorial structure in phonology.
with models that incorporate biases that favour simple systems
--{\textgreater} 'merging categories, or reinforcing frequently recurring representations'


10) Issue with highly ambiguous/largely degenerate languages: "adding artificial experimental intervention to discourage degenerate languages, for example by the experimenter removing ambiguous strings from the training data (Kirby et al., 2008), leads to the emergence of compositionally structured languages".

11) Closed group design [Mesoudi {\&} Whiten 2008]...


Ideas:
1) Cultural evolution "leads to the emergence of languages which are both highly compressible and highly expressive"

2) "this same trade-off between compressibility and expressivity [...] also explains the existence of structural design features like compositionality".


Future Works/Limitations:
1)}},
author = {Kirby, Simon and Tamariz, Monica and Cornish, Hannah and Smith, Kenny},
doi = {10.1016/J.COGNITION.2015.03.016},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirby et al. - 2015 - Compression and communication in the cultural evolution of linguistic structure(2).pdf:pdf},
issn = {0010-0277},
journal = {Cognition},
keywords = {Cultural transmission,Iterated learning,Language evolution},
mendeley-groups = {Article Gestion/1) TO READ/Linguistic},
month = {aug},
pages = {87--102},
publisher = {Elsevier},
title = {{Compression and communication in the cultural evolution of linguistic structure}},
url = {https://www.sciencedirect.com/science/article/pii/S0010027715000815},
volume = {141},
year = {2015}
}


@incollection{Kirby&Hurford2002,
abstract = {Introduction As language users humans possess a culturally transmitted system of unparalleled complexity in the natural world. Linguistics has revealed over the past 40 years the degree to which the syntactic structure of language in particular is strikingly complex. Furthermore, as Pinker and Bloom point out in their agenda-setting paper Natural Language and Natural Selection grammar is a complex mechanism tailored to the transmission of propositional structures through a serial interface"...},
author = {Kirby, Simon and Hurford, James R.},
booktitle = {Simulating the Evolution of Language},
doi = {10.1007/978-1-4471-0663-0_6},
file = {:home/kevin/Documents/PAPERS/Language/The Emergence of Linguistic Structure An overview of the Iterated.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Compositionality,Article Gestion/1) TO READ/Neuroscience/Language},
pages = {121--147},
publisher = {Springer London},
title = {{The Emergence of Linguistic Structure: An Overview of the Iterated Learning Model}},
year = {2002}
}



@article{Lake2019,
abstract = {People can learn a new concept and use it compositionally, understanding how to "blicket twice" after learning how to "blicket." In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show that neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply rules to variables.},
annote = {Meta seq2seq learning is proposed to enable (R)NN to generalize compositionally.

Biblio:
1) systematic compositionality / algebraic capacity to combine familiar primitives in human cognition: [5,26]

2) lack of compositionality in (R)NN --{\textgreater} 'inappropriate for modeling language and thought' [8,23,24]

3) modern neural architectures for NLP benchmarking etc...


Ideas:
1)


Future works/Limitations:
1)},
archivePrefix = {arXiv},
arxivId = {1906.05381},
author = {Lake, Brenden M.},
eprint = {1906.05381},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake - 2019 - Compositional generalization through meta sequence-to-sequence learning(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Compositionality},
month = {jun},
title = {{Compositional generalization through meta sequence-to-sequence learning}},
url = {http://arxiv.org/abs/1906.05381},
year = {2019}
}


@article{Brighton&Kirby2006,
author = {Brighton, Henry and Kirby, Simon},
doi = {10.1162/artl.2006.12.2.229},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brighton, Kirby - 2006 - Understanding Linguistic Evolution by Visualizing the Emergence of Topographic Mappings.pdf:pdf},
issn = {1064-5462},
journal = {Artificial Life},
mendeley-groups = {Article Gestion/1) TO READ/Compositionality,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Neuroscience/Language},
month = {jan},
number = {2},
pages = {229--242},
title = {{Understanding Linguistic Evolution by Visualizing the Emergence of Topographic Mappings}},
url = {http://www.mitpressjournals.org/doi/10.1162/artl.2006.12.2.229},
volume = {12},
year = {2006}
}


@article{Baroni2017,
abstract = {With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.},
archivePrefix = {arXiv},
arxivId = {1701.08954},
author = {Baroni, Marco and Joulin, Armand and Jabri, Allan and Kruszewski, Germ{\`{a}}n and Lazaridou, Angeliki and Simonic, Klemen and Mikolov, Tomas},
eprint = {1701.08954},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baroni et al. - 2017 - CommAI Evaluating the first steps towards a useful general AI.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/MultiAgents,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Paradigm},
month = {jan},
title = {{CommAI: Evaluating the first steps towards a useful general AI}},
url = {http://arxiv.org/abs/1701.08954},
year = {2017}
}

@article{Mikolov2015,
abstract = {The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.},
archivePrefix = {arXiv},
arxivId = {1511.08130},
author = {Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
eprint = {1511.08130},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Joulin, Baroni - 2015 - A Roadmap towards Machine Intelligence(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Paradigm},
month = {nov},
title = {{A Roadmap towards Machine Intelligence}},
url = {http://arxiv.org/abs/1511.08130},
year = {2015}
}


@article{Winograd1972,
abstract = {This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language—syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system.},
author = {Winograd, Terry},
doi = {10.1016/0010-0285(72)90002-3},
file = {:home/kevin/Documents/PAPERS/Language/Winograd-UnderstandingNaturalLanguage.pdf:pdf},
issn = {0010-0285},
journal = {Cognitive Psychology},
mendeley-groups = {Article Gestion/1) TO READ,Article Gestion/1) TO READ/Paradigm},
month = {jan},
number = {1},
pages = {1--191},
publisher = {Academic Press},
title = {{Understanding natural language}},
url = {https://www.sciencedirect.com/science/article/pii/0010028572900023?via{\%}3Dihub},
volume = {3},
year = {1972}
}


@article{Lowe2019,
abstract = {How do we know if communication is emerging in a multi-agent system? The vast majority of recent papers on emergent communication show that adding a communication channel leads to an increase in reward or task success. This is a useful indicator, but provides only a coarse measure of the agent's learned communication abilities. As we move towards more complex environments, it becomes imperative to have a set of finer tools that allow qualitative and quantitative insights into the emergence of communication. This may be especially useful to allow humans to monitor agents' behaviour, whether for fault detection, assessing performance, or even building trust. In this paper, we examine a few intuitive existing metrics for measuring communication, and show that they can be misleading. Specifically, by training deep reinforcement learning agents to play simple matrix games augmented with a communication channel, we find a scenario where agents appear to communicate (their messages provide information about their subsequent action), and yet the messages do not impact the environment or other agent in any way. We explain this phenomenon using ablation studies and by visualizing the representations of the learned policies. We also survey some commonly used metrics for measuring emergent communication, and provide recommendations as to when these metrics should be used.},
annote = {Authors acknowledge the lack of "formal study evaluating how we measure emergent communication".
--{\textgreater} "it is useful to quantify more finely the degree of communication as it can provide more insights into the agent behaviour".
--{\textgreater} "Our aim is to develop a set of evaluation tools for detecting and measuring emergent communication that increases the robustness of emergent communication research on the way to developing agents that communicate with humans".

Introduction of a "simple class of matrix games augmented with a bidirectional communication channel, [...] Matrix Communication Games (MCGs)".

--{\textgreater} Positive listening is not measured in the literature.
--{\textgreater} Positive signaling is exhibited by deep RL agents in the absence of positive listening, which "falsifies the hypothesis that the agents are communicating to share information to increase the reward of both agents".
--{\textgreater} develop a metric "causal influence of communication (CIC)"


Biblio:
1) "One of the approaches for developing agent capable of language is through emergent communication".
--{\textgreater} "This approach is motivated by a utilitarian view of communication, where language is viewed as one of many tools an agent can use towards achieving its goals, and an agent 'understands' language once it can use it to accomplish these goals [Gauthier {\&} Mordatch, 2016 ; Wittgenstein, 1953]."

2) "The question of emergent communication is also of scientific interest, as it could provide insights into the origins of human language [32]".

3) deep RL with emergent communication [7,9,11,18,23-25].
--{\textgreater} "agents navigate a simulated world with a limited field of view, and communication can help the agents coordinate to achieve a common goal [4,13,21,39]".
--{\textgreater} "pixel-based inputs [4,7,24]".
--{\textgreater}"incorporating human language [25,27]".
--{\textgreater} "Hanabi [13]".





Ideas:
1) Positive signaling: "indicates that an agent is sending messages that are related in some way with its observation or action".
--{\textgreater} Positive Listening: "indicating that the messages are influencing the agents' behaviour in some way".

2) Definition of a "non-situated envronment (or simulation)" as "one where agents' actions consist solely of sending and receiving signals". [following "Progress in the simulation of emergent communication and language", Wagner et al., 2003]
--{\textgreater} "thus, if task success increases in this setting, it is likely because the sender agent has developed a better communication protocol, or the listener agent has become better at understanding it".


3) Qualitative analysis of communication protocols is "only testing positive signaling", and not positive listening...

4) Speaker Consistency "is a measure of positive signaling, [...] and does not tell us anything about [positive listening]".
--{\textgreater} "quantify the degree of alignment between an agent's messages and its actions [4, 21]".

5) Context Independence: "captures the same intuition as speaker consistency: [whether] a speaker is consistently using a specific word to refer to a specific concept".


6) Entropy of message distribution: "perpleity or entropy (H(pi{\^{}}c)) of the distribution over messages for different inputs".
--{\textgreater} not a measure of positive signaling nor listening... questionable metric...

7) Instantaneous coordination: "calculates the mutual information between one agent's message and the other agent's action, again by averaging (message, action) co-occurences over episodes".
--{\textgreater} measure of positive listening.
--{\textgreater} "it only detects cases where a message from one agent changes the other agent's action regardless of contet".

8) Propose "one-step causal influence communication (CIC)": "mutual information between an gent's message and the other agent's action, similarly as for IC, however the probabilities p(a,m)=pi1(a|m)*pi2(m) represent the change in the agent's (pi1) action probability distribution when intervening to change the message m spoken by the other agent (pi2)".
--{\textgreater} "back-door paths [34]"


8) Positive signaling does not imply positive listening: in the scrambled message settings, SC is still positive and indistibuishable from control setting, thus it is "convincing evidence that the correlation between actions and communications does not emerge because the message are useful, but rather as a byproduct of optimiation".

9) The choice of architecture resulted in the emergence signaling behaviour in the absence of positive listening...

10) Only "focused on the one-step approimation to CIC, which only calculates the effect of an agent's message (which concists of one symbol)".
--{\textgreater} Need to assess longer messages...


Future Works/Limitations:
1) One-step (one-symbol) approimation to CIC alone is evaluated...
--{\textgreater} longer messages (n-symbols)
--{\textgreater} more steps...

2) Computationnaly epensive...
--{\textgreater} proposed to sample from the agent's communication policy rather than iterating over all possible messages and all agents....},
archivePrefix = {arXiv},
arxivId = {1903.05168},
author = {Lowe, Ryan and Foerster, Jakob and Boureau, Y-Lan and Pineau, Joelle and Dauphin, Yann},
eprint = {1903.05168},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe et al. - 2019 - On the Pitfalls of Measuring Emergent Communication(2).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
month = {mar},
title = {{On the Pitfalls of Measuring Emergent Communication}},
url = {http://arxiv.org/abs/1903.05168},
year = {2019}
}


@article{Chevalier-Boisvert2018,
abstract = {Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.},
annote = {"How can a human train an intelligent agent to understand natural language instructions?"
with a simulated human expert teacher in the BabyAI 2D gridworld platform.

Interactive learning (DAGGER, TAMER etc...) requires human-machine interactions. In order to accelerate the research process and provide a benchmarch against which to evaluate new agents, a simulated human expert is provided.

Reward assignement, like in the real world, is given by the simulated human in the loop.

Only sparse reward is given though. It is clearly an issue that prevents the agent from increasing data-efficiency.



Further works:
1) Extension to multi-agent collaboration tasks, framed with an information-seeking focus.

2) Use dense reward to enable data-efficiency or an actual sparse-reward-dedicated algorithm.

3) Counterfactual learning could help bring about demonstrations-efficiency (since counterfactual approaches are not real demonstrations, but the agent can still learn a fair deal from it).},
archivePrefix = {arXiv},
arxivId = {1810.08272},
author = {Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
eprint = {1810.08272},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chevalier-Boisvert et al. - 2018 - BabyAI First Steps Towards Grounded Language Learning With a Human In the Loop.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ,Article Gestion/1) TO READ/Curriculum/Active/Episodic learning},
month = {oct},
title = {{BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop}},
url = {http://arxiv.org/abs/1810.08272},
year = {2018}
}


@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}

@article{Kim2018,
  title={Disentangling by factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  journal={arXiv preprint arXiv:1802.05983},
  year={2018}
}


@article{Baroni2020,
abstract = {Deep-agent communities developing their own language-like communication protocol are a hot (or at least warm) topic in AI. Such agents could be very useful in machine-machine and human-machine interaction scenarios long before they have evolved a protocol as complex as human language. Here, I propose a small set of priorities we should focus on, if we want to get as fast as possible to a stage where deep agents speak a useful protolanguage.},
annote = {"how should a useful protolanguage for deep agents look like? And, consequently, what should be our priorities in terms of the linguistic abilities we want the agents to develop?"

Biblio:
1) "interactive scenarios, letting them develop a code to cooperatively solve their tasks [Foerster et al. 2016, Lazaridou et al. 2017, ...]"
--{\textgreater} Overview in [Lazaridou and Baroni 2020].

2) Linguistic protolanguage hypothesis


3) Definition: PREDICATION: "a property is asserted of an object, a predication".

4) Definition: PARATAXIS: "defines the tendency to juxtapose simple clauses, instead of sub-ordinating them to construct complex sentences. Cf. 'I came, I saw, I conquered' vs 'After coming, having seen, I decided to conquer'."

5) "being able to talk about concrete things is probably a necessary stepping stone towards abstract language [Lakoff and Johnson 1999]".

6) Definition: ICONICITY "orderiing predications in the way in which the corresponding events occured, placing the cause before the effect, etc."

7) DUALITY OF PATTERNING [Martinet 1965]: "treating primitive symbols as meaningless units to be combined into meaningful words".

--{\textgreater} "From an evolutionary perspective, it only makes sense for a language to start composing words in order to denote complex meanings after the language has coined enough primitive words to justify combinatorial strategies".

ISSUES:
1) Environmental Constrains:
--{\textgreater} "Agent inputs are continuous and noisy"
--{\textgreater} "There is no close set of 'things' that agents need to talk about: new objects, actions, properties can always appear"


2) "There is however little ongoing language emergence research on how agents categorize their perceptual input through language."



3) "From a methodological perspective, if we do not understand the means a language uses to refer to new primitive concepts, we will be doomed when trying to analyze how it refers to composite ones".


4) HOW TO EXTEND THE VOCABULARY? "Agent able to communicate about an open set of primitive categories by adding new words to their repertoire would already be quite a feat".

5) HOW TO EXTRACT PREDICATES: as something in common for two different object: "It is one thing to learn a red predicate from input objects represented by attribute-value sets such as {\{}color:RED, shapes:SQUARE{\}}, and another to discover that red hair and red tomatoes, as depicted in natural images, have something in common that can be capture by a shared red predicate".

5) CLAUSAL SUBORDINATION AND RECURSION should not be our concerned for the moment, "paratactic concatenation of unary predications" is already quite enough.



6) LANGUAGE DRIFT: "consider a language that achieves composition by symbol inter-leaving: 'rceadr' denoting a red car), we'll need to bring it back to Terran grounds somehow".
--{\textgreater} "forcing the agents to directly mimic natural language [Havrylov {\&} Titov 2017; Lazaridou et al., 2017, Lee et al, 2018]"
--{\textgreater} "imposing human-like bottlenecks, for example, on agent memory and channel capacity [Kottur et al., 2017, Resnick et al., 2020]".


Future Works/Limitations:

-2) LANGUAGE DRIFT? Is it an issue? Framing the human-machine communication problem as a translation problem would have two benefit: firstly the learning agents would be unconstrained to 'think' like humans do, and maybe gain some other dimensions of information processing. Secondly, few-shot translation learning / inter-lingua learning is an interesting prior to be built around for it enables seamless adaptations of the vocabulary and syntax to the interlocutor. In a way, context is built into the model.
--{\textgreater} It then enables the possibility of learning new context to better express an idea, or to change the style of the writing, for instance!


-1) PREDICATE EXTRACTION? Disentanglement seems to be the answer, as it has the property of discovering the latent axises of variations that are common from one pattern to another. So far, though, VAE models do not separate elements that compose a natural image into semantic elements whose predicates can be inferred easily. 
--{\textgreater} How to separate semantically? Using attention mechanism most likely or relational induction bias that will, hypothetical, force the representation to be entity-based?
But if so, the number of latent axis would be fixed by the CNN kernel architecture...
It is important, in a lifelong learning setting, to let the agent come up with the number of latents it needs, therefore a pressured VAE that constrain the latent code seems very advisable, rather than a relational induction bias...?


0) VOCABULARY EXTENSION: VQ-VAE with their repertoire of embeddings seem to be a sound basis from where to start developping such a mechanism?


1) The protolanguage scenarios proposed do not involve emotional support eventhough children are clearly able to convey emotional semantics with their protolanguage (cf. The Iron Giant's protolanguage)

2) The environmental constraints requires the agents to be able to use compositionality, i.e. recombine familiar components in novel combinations!

3) UNDERSTANDING: [Lazaridou et al., 2018] "showed that already using synthetic images instead of purely symbolic inputs has a big impact on the emerging language".

4) DUALITY OF PATTERNING: ... by contrasting similar meaning pattern into refinement?

5) Considering disentanglement and compositional needs, i.e. the former requiring the ability to decompose elements into their constituating values on element-wise common latent axises of variations, while the latter is requiring the ability to recognise said elements as parts of a whole that is just a composition of multiple such elements (that may be even of different nature , i.e. decomposing over different sets of latent axises), it becomes even more obvious that the literature has been focused on composing words/combinatorial strategies long before it would have even started showing the ability to decompose stimulus into latent axises that can be mapped onto "coined [..] primitive words".
To some extent, referential games are currently concerned with the wrong kind of generalization: rather than assuming utterances to be sentences to map onto/"denote complex meanings", we should be treating those utterances as coined primitive words to map onto simple meanings, i.e. not composed/complex meanings, but rather element-sized meaning.
--{\textgreater} How to frame referential games with complex stimuli into considering
--{\textgreater} Rather than delimitating multiple objects in the visual modality, we should delimitate them with attention still but on a projected space for entity understanding, i.e. a space where the visual modality and others (think about temporality or functional modality --an object starts to be as much when it finds a function, therefore anything that allow us to go from one room to another in a door, independently of its shape/visual appearance--) can be projected in a coherent/embodiement-relevant picture of what the embodied agent perceives. Such a space is at the center of the MuZero approach to embedding: this embedding encompasses temporal meaning and functional meaning. Being able to attend to entities in that space is more empowering for embodied agents to understand the world around them. From there, word coinage to characterise each element/node is a training scheme that can be hypothesized to yield disentangled representations.
Once each element/node can be understood to constitute a state that summarizes the embodied agents past and present perceptions, as well as its future affordances, it is possible to understand it, and maybe communicate it as a composition of elements to speak about with a compositional language!!

--{\textgreater} What kind of architectural prior to use? 
--{\textgreater} Obvious answer: MONet?
--{\textgreater}Less obvious: recruiting spiking nets?
Define a node tank that recruits node whenever the need appears (measuring the amount of information that each node has focused on from each modality could be a good indicator whether someting is missing? How to make sure that similar objects can be focused upon without having to recruit as many node as there are duplicates?), where each node focus onto a predefined kind of patterns and embeds said perception pattern into the (functional, in the end?)/multi-modal space (with a cyclic-consistency constraint of course...). The multi-modal embeddings can have a growing number of latent variables, thus creating on-the-fly new latent axises of variations. (Each node would maybe also output a mask that focuses their resulting embedding? to constraint the latent variable recruitment... and somehow protect the previous latent? catastrophic forgetting is always an issue...) Or it could have a VQ-VAE-based architecture, thus curating a dictionary of possible embeddings and recruiting new embeddings as new 'tools' are discovered, while alleviating the catastrophic forgetting issue :)!!
Yes, indeed, this multi-modal/maybe-functionally-focused embedding is actually a tool embedding, in the sense that an object is an object only if it has affordances to leverage, right?
Anyway, from that cloud of nodes/neurone constellations that activate sparsly/spike emerges a consciousness, i.e. a world representation that can be manipulated into a world model similarly to MuZero ?
In this framework, the referential game, which should rather be called a self-pondering game, could take place in a similar fashion to that little voice at the back of our heads, that voice that bring to consciousness our trail of thought and ponders/reflect onto it, trying to define by contrasting at two different level: firstly at the level of word coinage, i.e. taking as input one node's output at a time and contrasting it against the others, and then secondly at a syntactical and/or logical level by combining those words with a grammar/some operators (that may be logical) to modulate/nuance and/or causally-define (is that all that logics does? deriving inferences?) the relationship that each word entertain with each other? 
--{\textgreater} But then, this relation-infering part of the self-pondering game is both an inverse and forward world model that is able to predict outcomes?

N.B.: It is important to note that the word-coining level is motivate by Wittgenstein's views according to which what can be perceived needs to be referable to, although I may be twisting his views...? 
Another motivation is to make sure that this emerging understanding of the world can be communicated and population of such agent can coordinate about it.

Here comes the coordination bit: the emerging language that the agent comes up with should not be an open loop but rather enjoys some forms of feedback from other agent communication attemps. Firstly, as shown in the obverter approach and the jaccard similarity measure, by simply letting the agents communicate with each other they should be able to build a commun language and hopefully common linguistic and/or logical operators to manipulate and compose said word.

--{\textgreater} It is worth investigating if this is still possible with complex meanings being perceived, by opposition to simple/element-focused ones that have been used in [Choi et al. 2018].


--{\textgreater} This approach of separating word-coinage with manipulation seems similar to that that separates syntax and word-level prediction, seen in a paper somewhere? [Russin et al, "Compositional generalization in a deep seq2seq model by separating syntax and semantics", 2019]},
archivePrefix = {arXiv},
arxivId = {2003.11922},
author = {Baroni, Marco},
eprint = {2003.11922},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baroni - 2020 - Rat big, cat eaten! Ideas for a useful deep-agent protolanguage.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Curriculum/Curiosity/Episodic/Imitation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Disentanglement/Life-Long Learning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Instructions / Goal-based / Skills,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/MultiAgent,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience/Linguistic,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Translation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Dialogue / Natural Language Generation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Paradigm,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Paradigm/Human-in-the-loop,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Video games},
month = {mar},
title = {{Rat big, cat eaten! Ideas for a useful deep-agent protolanguage}},
url = {http://arxiv.org/abs/2003.11922},
year = {2020}
}


@article{Chaabouni2019a,
abstract = {Despite renewed interest in emergent language simulations with neural networks, little is known about the basic properties of the induced code, and how they compare to human language. One fundamental characteristic of the latter, known as Zipf's Law of Abbreviation (ZLA), is that more frequent words are efficiently associated to shorter strings. We study whether the same pattern emerges when two neural networks, a "speaker" and a "listener", are trained to play a signaling game. Surprisingly, we find that networks develop an $\backslash$emph{\{}anti-efficient{\}} encoding scheme, in which the most frequent inputs are associated to the longest messages, and messages in general are skewed towards the maximum length threshold. This anti-efficient code appears easier to discriminate for the listener, and, unlike in human communication, the speaker does not impose a contrasting least-effort pressure towards brevity. Indeed, when the cost function includes a penalty for longer messages, the resulting message distribution starts respecting ZLA. Our analysis stresses the importance of studying the basic features of emergent communication in a highly controlled setup, to ensure the latter will not strand too far from human language. Moreover, we present a concrete illustration of how different functional pressures can lead to successful communication codes that lack basic properties of human language, thus highlighting the role such pressures play in the latter.},
annote = {Are emerging languages comparable to human languages?
"Very basic characteristics of the emergent codes have yet to be understood"

--{\textgreater} Here: focus on "the length distribution of the messages that two neural networks playing a simple signaling game come to associate to their inputs, in function of input frequency" :
--{\textgreater} Do they abide by "Zipf's Law of Abbreviation (ZLA), [where] more frequent words are efficiently associated to shorter strings"?

--{\textgreater} What explains those pattern? Is it "functional pressures toward effort minimization" or "some random-typing distributions [that] also respect ZLA"?

Results: NN "develop an anti-efficient encoding scheme" where "messages in general are skewed towards the maximum length threshold". "all messages are long, and the most frequent inputs are associated to the longest messages".

Biblio:
1) "the desire to develop automated agents that can communicate with humans [Havrylov and Titov 2017 ...]"

2) "analyze the properties of the emergent codes [Kottur2017 Bouchacourt2018, Evtimova2018, Lowe2019, Graesser2019]"

3) ZLA: "There is an inverse (non- linear) correlation between word frequency and length [Zipf, 1949, Teahan et al., 2000, Sigurd et al., 2004, Strauss et al., 2007]."

4) Power Law Word Distribution: "word distributions are highly skewed, following a power-law distribution" [Zipf,1949]

--{\textgreater} "language approaches an optimal code in information-theoretic terms [Cover and Thomas, 2006].

5) Functional Pressure --{\textgreater} Effort Minimisation: "used ZLA as evidence that language is shaped by functional pressures toward effort minimization [Piantadosi2011, Mahowald2018,Gibson2019]"


6) "ZLA emerges from the competing pressures to communicate in a perceptually distinct and articulatorily efficient manner [Zipf,1949;Kanwal2017]".


RESULTS:
1) "Natural langauges are not easy to process by Listeners. This suggest that the emergence of "natural" langauges in LSTM agents is unlikely, without imposing ad-hoc pressures".

2) "impose an artificial pressure on Seaker to produce short messages, to counterbalance Listener's preference for longer ones. Specifically, we add a regularizer disfavoring longer messages to the original loss [...] The emergent messages clearly follow ZLA. Speaker now assigns messages of ascending lengths to the 40 most frequent inputs".

--{\textgreater} "still, the encoding is not as efficient as the one observed in natural language (and OC). Also, when adding length regularization, we noted a slower convergence with a smaller number of successful runs, that further diminishes when $\backslash$alpha increases".


3) "despite the lack of phonetic pressures, Speaker is respecting 'phonotactic' constraints that are even sharper than those reflected in the natural language bigram distributions".


Understanding:
1) How does this power law on the input factors with the distribution of token used in the emerging code?
Does the emerging code present a power law on its vocabulary usage distribution too?
--{\textgreater} so according to figure 5a, yes, "even if at initialization Speaker starts with a uniform distribution over its alphaber [...], by end of training it has converged to a very skewed one".


Future works/Limitations:
1) Following the addition of the length regularizer, the encoding is still not as efficient as NLs and the convergence is reduice. It hints at the fact that this pressure is maybe not the one driving the ZLA approach.
Hypothesis: what if the pressure whose in terms of behaviour learning, i.e. while learning a language, there is pressure from the motor difficulty involved in sound production, therefore there is a pressure on the quantity of sound/token that are available. Rather than minimizing the message length, alone, could we pressure the vocabulary size too?
--{\textgreater} For instance, we could require the vocabulary usage distribution to follow a form of power law?
--{\textgreater} Well, looking at figure 5.a., this already happens without asking for it, wierdly enough...

2) Could the anti-efficient encoding be explained by the sequence bias of RNNs?
--{\textgreater} NEED TO RE-READ PAPER ON WORD ORDER BIAS IN EMERGING LANGUAGES!!!!
--{\textgreater} Hypothesis: what happens when we build the sentence all at once using something similar to graph networks? 
---{\textgreater} or per-sentence-position outputs (that enable the EOS and PAD tokens to be generated)?
--{\textgreater} or recurrent all-at-once sentence generation using an architecture similar to that of the paper that solves SAT problems: NeuroSAT 
--{\textgreater} graph network with only one position on the sentence at the beginning, and a query output that asks whether to add another word or not. If another word is added, then we re-run the graph.
--{\textgreater} some proxies to this decision node could be investigated, for instance:
(i) obverter approach with stop condition as in the obverter situation. Is this already the obverter approach?! How is the emergent code in obverter approach comparing to NLs????
--{\textgreater} No, the obverter approach does not try to re-evaluate the previous tokens... That is the incremental innovation we could try...

(ii) ask the generating network to estimate the uniqueness or expressivity of the generate message and trigger new word either with low uniqueness or high expressivity (if it is high then it can mean a lot of different things, i.e. it is not specific enough...)},
archivePrefix = {arXiv},
arxivId = {1905.12561},
author = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
eprint = {1905.12561},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaabouni et al. - 2019 - Anti-efficient encoding in emergent communication(2).pdf:pdf},
journal = {NeurIPS},
mendeley-groups = {Article Gestion/1) TO READ/Neuroscience/Language,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Compositionality},
month = {may},
title = {{Anti-efficient encoding in emergent communication}},
url = {http://arxiv.org/abs/1905.12561},
year = {2019}
}


@article{Russin2019,
abstract = {Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in neuroscience suggesting separate brain systems for syntactic and semantic processing, we implement a modification to standard approaches in neural machine translation, imposing an analogous separation. The novel model, which we call Syntactic Attention, substantially outperforms standard methods in deep learning on the SCAN dataset, a compositional generalization task, without any hand-engineered features or additional supervision. Our work suggests that separating syntactic from semantic learning may be a useful heuristic for capturing compositional structure.},
annote = {Inspired by neuroscience "suggesting separate brain systems for syntactic and semantic processing", Syntactic Attention imposes analogous separation and is thus successful at SCAN compositional generalization tasks "without any hand-engineered features or additional supervision".

"Given that humans can perform well on certain o.o.d. extrapolation tasks, the human brain must be implementing principles that allow humans to generalize systematically, but which are lacking in current deep learning models".

--{\textgreater} "encodes syntactic and semantic information in seperate streams before producing output sequences".


"the semantic system learns and stores representations of the meanings of words, and the syntactic system, housed in Broca's area of the prefrontal cortex, learns how to selectively attend to these semantic representations according to grammatical rules".


Biblio:
1) SYSTEMATICITY:

2) Failures of RNNs to generalize systematically when compared to human proficiency...
--{\textgreater} Stastical understanding of the failure: "Because "jump" has never been seen with any adverb, it would not be irrational to assume that "jump twice" is an invalid sentence in this language".

3) Neuroscience of "not to learn some of the correlations that are actually present in the training set": [Kriete et al., 2013].

4) "SYNTAX is the aspect of language underlying its systematicity [Fodor {\&} Pylyshyn, 1989]".

5) Hypothesis of specific cognitive machinery/areas in the brain to process syntax: [Chomsky, 1957, "Syntactic Structures"]
--{\textgreater} Broca's area's involvement "in syntactic processing in general[6;26]".
--{\textgreater} Evidenced by the difficulty involved in understanding sentences where "semantics is not enough to understand their meanings", while having "lesions to this area".


--{\textgreater} [26: Thompson-Schill, 2004, "Dissecting the language organ"] highlights that the Broca's area "may simply be a part of prefrontal cortex specialized for language".

6) "The prefrontal cortex is known to be important for cognitive control, or the active maintenance of top-down attentional signals that bias processing in the other areas of the brain [21:Miller {\&} Cohen, 2001, "An integrative theory of prefrontal cortex function"]".
Also, see [20:Miller, 2013, "The 'working' of working memory"]

--{\textgreater} "responsible for selectively attending to linguistic representataions housed in other areas of the brain [26: Thompson-Schill, 2004, Dissecting the language organ: A new look at the role of Broca's area in language processing"]"


7) PREFRONTAL CORTEX {\&} COMP. GEN.:
[Kriete et al, 2013, "Indirection and symbol-like processing in the prefrontal cortex and basal ganglia"]
--{\textgreater} attend to Oz's shared-literature!!!!!


Ideas:
1) " The SCAN task requires networks to make an inferential leap about the entire structure of part of the distribution they have not seen - that is, it requires them to make an out-of-domain (o.o.d.) extrapolation [Marcus, 2018], rather than merely interpolate according to the assumption that trian and test data are independent and identically distributed (i.i.d.)".


2) "The underlying assumption made by the Syntactic Attention architecture is that the dependence of target words on the input sequence can be separated into two independent factors. One factor, p(y{\_}i|x{\_}j), which we refer to as 'semantics', models the conditional distribution from individual words in thei nput to individual words in the target. Note that unlike in the model of Bahdanau et al. [2], there x{\_}j do not contain any information about the other words in the input sequence because they are not processed with an RNN. They are 'semantic' in the sense that tehy contain the information relevant to translating into the target language. The other factor, p(j-{\textgreater}i|x), which we refer to as 'syntax' models the conditional probability that word j in the input is relevant to word i in the target sequence , given the entire input sequence. This alignment is accomplished from encodings of the inputs produced by an RNN. This factor is 'syntactic' in the sense that it must capture all of the temporal informationa in the input that is releveant to determining the serial order of outputs".

UNDERSTANDING:
--{\textgreater} "The crucial architectural assumption, then, is that any temporal dependency between individual words in the input that can be captured by an RNN should only be relevant to their alignment to words in the target sequence, and not to the translation of individual words".

In other words, temporality or structure or arrangement should only bias the meaning of the output (as understood as compositional meaning) and not the meaning of the individual inputs in themselves. This architecture enforces each input to be meaningful in themselves in the first place, and then, only, give rise to a compositional meaning as output.
--{\textgreater} Evidenced in the following:"Note that because there is no sequence information in the semantic representations, all of the information required to parse (i.e. align) the input sequence correctly (e.g. phrase structure, modifying relationships, etc.) must be encoded by the biRNN".

Giving rise to the compositional meaning is about parsing/aligning by considering phrase structure and modifying relationships.


3) "we use an RNN to determine an attention distribution over the inputs at each time step (i.e. align words in the input to the current target)".
--{\textgreater} aligning can take thte meaning of "relating and attending" to what is relevant in order to produce the current output target.

As always, the alignment mechanism actually enforces both an emphasizement of what is relevant and a associative constraint on those relevant elements via the summation.
In other words, the representation is associative and commutative with respect ot its inputs, so the input's representations are constrained to be what?
It stands to reason that they probably consist of a basis of vectors, to some extent they mush show linear independence otherwise they cannot sum into an entirely associative representation, right?

TOTEST: Maybe it would be found that the vectors associated with words of similar syntactic type form a basis that the modifyer words can then skew in different directioins?
If so, skewing is limited to additive operations (moving on a single axis?). 
Facing more complex problems may require more powerful kind of operations, maybe fully embrace a linear/matrix product ? 

TOTEST: the capacity of the embeddings is then a crucial hyperparameter as it fixes the size of the space that the basis-like family of embeddings can describe. It can be hypothesized that the more verb and adverb can be learned the greater the need for this space to be larger. Tackling lifelong-learning settings would require a compression and expansion mechanism on the embedding vector size, maybe?

4) "the hidden state of the RNN is updated with the same weighted combinations of the syntactic representations of the input": i.e. the decoder RNN's input, at each time step, is a weighted average of the relevant encoder RNN's hidden states!},
archivePrefix = {arXiv},
arxivId = {1904.09708},
author = {Russin, Jake and Jo, Jason and O'Reilly, Randall C. and Bengio, Yoshua},
eprint = {1904.09708},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russin et al. - 2019 - Compositional generalization in a deep seq2seq model by separating syntax and semantics(2).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Disentanglement/Life-Long Learning,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Paradigm,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Hierarchy},
month = {apr},
title = {{Compositional generalization in a deep seq2seq model by separating syntax and semantics}},
url = {http://arxiv.org/abs/1904.09708},
year = {2019}
}



@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@article{Resnick2019,
abstract = {Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality of emergent languages. Our foremost contribution is to explore how capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
annote = {"How does an agent's capacity effect the resulting language's compositionality?" (in the context of an overly constrained model...)

Authors conjecture that there is a minimal model capacity (in parameter ocunt) neccessary to capture the tested compositional language. Above the threshold, there recall metric is "almost always perfect, implying that the model has likely capture the compositional structure underlying L$\backslash$star from a finite set of training strings". 
--{\textgreater} "It is left for future studies to determine whether this is due to the lack of model capacity to memorize the hash map between all the strings in L$\backslash$star and 2{\^{}}l latent strings or due to an inclination towards composiitonality in our variational autoencoder". CONN: [Valle-Perez et al. 2018, 'Deep learning generalizes because the parameter-function map is biased towards simple functions']


IDEAS:
1) Definition of a compositional language with reference to the production rules P and the non-terminal symbols N, thus calling a language "L is compositional if and only if |L| {\textgreater} |P|+|N|".

ASSUMPTIONS/CONSTRAINTS: 
0) The language analysed already exists as compositional.

1) "constrain the latent variable to be a binary string of fixed length" in order to be able to better 'understand the resulting capacity of the model'. Also, "all the alphabets in the string s $\backslash$in L$\backslash$star indicate the underlying concepts".

2) inputs are encoded by the model as a bag of words representation.

3) The decoder is tailored to the hierarchy of non-terminal symbols in N as it is running an LSTM for |N| timesteps... 

4) While the capacity of the communication channel is clearly defined by modeling it as a discrete sequence of latent variables, it is far from being as evident to characterise "the capacity of the encoder and decoder to map between the latent variable z and a string s in the original language L$\backslash$star". The authors choose to "use the number of parameters as a proxy to measure [this] capacity" as it seems "reasonable to assume that there is a monotic relationship between the number of parameters [of the encoder], or [decoder], and the capacity of the network to encode the compositional structures underlying the original language [Collins et al., 2016]".

UNDERSTANDING:
1) The train/val/test split is not understood... Are they testing systematic generalization by ensuring that all concepts are known but not all combinations of those concepts?
It seems so, as they just removed from the train set all the strings that contains two concepts together belonging either to the test or val set.

2) metric of "degree of compositionality as the ratio between the variability of each concept Ci and the variability explained by a latent subsequence z[pi] indexed by an associated partition pi", or formally put as a residual entropy.
What is a residual entropy? 
How do they measure variability?
Why the need to index the partition from the latent variable?
- because the latent are assumed to map to the non-terminals so we compare the entropy partitions of latents to that of the non-terminals of the string.

RESULTS:
1) "This inverse relationship between bits and parameters [count] shows that the more parameters in the model, the fewer bits it needs to solve the task".
--{\textgreater} solving the task with fewer than 20 bits mean solving it non-compositionally, and this can only be done by memorization potentially, thus the need for more parameters!

2) "learning compositional codes requires less capacity than learning non-compositional codes".},
archivePrefix = {arXiv},
arxivId = {1910.11424},
author = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
eprint = {1910.11424},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Resnick et al. - 2019 - Capacity, Bandwidth, and Compositionality in Emergent Language Learning.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence},
month = {oct},
title = {{Capacity, Bandwidth, and Compositionality in Emergent Language Learning}},
url = {http://arxiv.org/abs/1910.11424},
year = {2019}
}


@techreport{Chollet2019,
abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks, such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow ex-perimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience, as critical pieces to be accounted for in characterizing intelligent systems. Using this definition , we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a new benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans. * I thank Jos{\'{e}} Hern{\'{a}}ndez-Orallo, Julian Togelius, Christian Szegedy, and Martin Wicke for their valuable comments on the draft of this document.},
archivePrefix = {arXiv},
arxivId = {1911.01547v2},
author = {Chollet, Fran{\c{c}}ois},
eprint = {1911.01547v2},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chollet - 2019 - On the Measure of Intelligence.pdf:pdf},
title = {{On the Measure of Intelligence}},
year = {2019}
}


@article{Liu2018,
abstract = {Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either perfect translation invariance or varying degrees of translation dependence, as required by the task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24{\%} better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.},
archivePrefix = {arXiv},
arxivId = {1807.03247},
author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
eprint = {1807.03247},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution.pdf:pdf},
mendeley-groups = {iGGi/Literature Review/Compositionality},
month = {jul},
title = {{An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution}},
url = {http://arxiv.org/abs/1807.03247},
year = {2018}
}


@article{Ren2020,
abstract = {The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents in language games. In this paper, we propose an effective neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL. We provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication.},
annote = {Iterated Kearning Model with Neural Networks, but with cultural bottlenecks at during the pre-training phases!

"highlight the positive effect of high-$\backslash$rho languages on the neural agents generalization ability -- which shows the potential of iterated learning for NLU tasks".

"under various experimental settings, the validation performance and the topological similarity are strongly correlated".

Unfortunately, it does not show systematic generalization abilities as the increase of the validation set size makes it harder for the agents to perform well, at a size of 32 the performance drops from 97{\%} training to 55{\%} testing. Although it is better than not having any NIL, it is not as good as it should be.
--{\textgreater} Evaluating the amount of memorization vs the amount of generalization would provide a better picture of what is actually happening, maybe?

--{\textgreater} Also, there validation may be showing the limitation of finding the correct value for Ia and Ib implicit bottlenecks. Indeed, they do not say they have performed fine-tuning of those values as the validation set was increasing. Nor do they report the topological similarity values achieved in those runs, therefore it is possible that the bottleneck may be ill-chosen.




Biblio:
1) hypotehsis that "early language learning was focused on problem-solving [Kirby {\&} Hurford, 2002]". "While related to NLU, it focuses on the pragmatics [Clark, 1996] of learning natural language, as it implies learning language from scratch, grouned in experience".

2) COORDINATION: "an array of recent work [Havrylov {\&} Tito, 2017; Mordatch {\&} Abbeel, 2018; Kottur et al., 2017; Foerster et al., 2016] has shown that in many game settings, the neural agents can use their emergent language to exchange useful coordinating information".

3) CONDITIONS TO COMP. LANG. EMERGENCE: "several experiments have already demonstrated that by properly choosing the maximm message length and vocabulary size, the agents can be brought together to develop a compositional language that shares similarities with natural languages [Li {\&} Bowling, 2019; Lazaridou et al., 2018; Cogswell et al., 2019]".

4) EVO. LING. : "studied the origins of compositionality for decades [Kirby {\&} Hurford 2002; Kirby et al., 2014;2015]".

5) FAVOUR COMP. LANG.: "In language evolution, highly compositional languages are favored because they are structurally simple and hence are easier to learn [Carr et al., 2017]".
--{\textgreater} "Intuitively, this is because the structured mapping described by a language with high $\backslash$rho is smoother, and hence has a lower sample complexity, which makes resulting examples easier to learn for the speaker agent [Vapnik, 2013]".

IDEAS:
1) Verify NN's preference for structure?!
"directly applying this framework to ground language learning is not straightforward: we should first verify the existence of the preference of compositional language at the neural agent, and then design an effective training procedure for the neural agent to amplify such an advantage".

Future Works/Issues:
1) Here, they 'validate' the NIL framework in a perfect setting where the meaning space is very well structure, which is one of the conditions that have been identified by [Kirby et al 2002 etc]. But, what happens when the meaning space lacks structure --which is clearly more likely to happen in real context--? 


2) Using the generation of a perfect high-$\backslash$rho language using the method proposed in [Kirby et al., 2015], it would be worse evaluating the ease-of-learning provided to the listener agent and its systematic generalization abilities? Do [Li {\&} Bowling, 2019] tried that? At least not with pixel inputs, maybe?



Understanding:
1) "The role of interacting phase is to filter out those ambiguous language. This may change [the expected $\backslash$rho of the language], but without a preference for language with specific $\backslash$rho".
--{\textgreater} Because of the batch-induced bottleneck -- if there is one?--, it might affect the language preference....?!

2) Ia and Ib acts as cultural bottleneck but it is not acknowledge what is the actual value of the bottleneck, nor whether this bottleneck is task specific or maybe specific to the capacity of the agent [cf. Resnick et al., 2019].},
archivePrefix = {arXiv},
arxivId = {2002.01365},
author = {Ren, Yi and Guo, Shangmin and Labeau, Matthieu and Cohen, Shay B. and Kirby, Simon},
eprint = {2002.01365},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2020 - Compositional Languages Emerge in a Neural Iterated Learning Model.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Compositionality,Article Gestion/1) TO READ/Paradigm,Article Gestion/1) TO READ/Neuroscience/Language},
month = {feb},
title = {{Compositional Languages Emerge in a Neural Iterated Learning Model}},
url = {http://arxiv.org/abs/2002.01365},
year = {2020}
}

@article{Chaabouni2020,
abstract = {Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as $\backslash$emph{\{}compositionality{\}}. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results. First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.},
annote = {Do emerging languages enables an "ability to refer to novel primitve combinations"?

Is it accomplished "by strategies akin to human-language compositionality"?

"even non-compositional languages (w.r.t. any definition of composiitonality) can generalize well. Indeed, for very high test accuracy ({\textgreater}98{\%}), we witness a large spread of posdis(between 0.02 and 0.72), bosdis (between 0.03 and 04) and topsim (between 0.11 and 0.64). In other words, deep agents are able to communicate about new attribute combinations while using non-composiitonal languages".

"While compositionality is not a necessary condition for generalization, it appears that the strongest form of compositionality, namely posdis, is at least sufficient for generalization".

--{\textgreater} "The need to generalize to new composite inputs does not appear

Biblio:
1) Language systematicity: "Most concepts we need to express are composite in some way. Language gives us the prodigious ability to assemble messages referring to novel composite concepts by systematically combining expressions denoting their parts".

2) Need for compositionality metrics [Andreas, 2019; Nefdt, 2020]
--{\textgreater} different from performance on "unseen composite inputs" [Kottur et al. 2017; Cogswell et all., 2019].
Although it quantifies the degree of generalization/compositionality, it does "not provide any insights on how this ability comes about".

--{\textgreater} "While more informative than generalization, topographic similarity is still rather agnostic about the nature of composiiton".


3) Disentangled representation learning: "disentangled representations are expected to enable a consequent model to generalize on new domains and tasks [Bengio et al., 2013]".
--{\textgreater} challenged by [Bozkurt et al., 2019; Locatello et al., 2019].


4) "Limiting channel capacity has been proposed as an important constraint for the emergence of compositionality [Nowak and Krakauer, 1999]".


Ideas:
1) "'opinionated' measures of compositionality that capture some intuitive properties of what we would expect to happen in a compositional emergent language".
--{\textgreater} "order-independent juxtapositions of primitive forms could denote the corresponding union of meanings".
--{\textgreater} "relies on juxtaposition, but exploits order to denote different classes of meanings, as in English adjective-noun phrases".

--{\textgreater} "both strategies result in disentangled messages, where each primitive symbol (or symbol+position pair) univocally refers to a distinct primitive meaning independently of context".

2) "Are neural agents able to generalize to unseen input combinations in a simple communicatoin game? We find that generalizing languages reliably emerge when the input domain is sufficiently large".
--{\textgreater} "as failure-to-generalize claims in the recent literature are often based on very small input spaces".


3) "focus on input reconstruction instead of discrimination of a target input among distractors as the latter option adds further complications: for example languages in that setup have been shown to be sensitive to the number and distribution of the distractors [Lazaridou et al., 2018]".


4) Positional Disentanglement metric: "measures whether symbol in a specific positions tend to univocally refer to the values of a specific attribute".
"captures the intuition that, for a language to be compositional given our inputs, each position of the message should only be informative about a single attribute".

5) Bag-of-symbols disentanglement : "symbols univocally refer to distinct input element independently of where they occur, making order irrelevant" "maintains the requirement for symbols to univocally refer to distinct meanings, but captures the intuition of a permutation invariant language, where only symbol counts are informative".

6) Spearman correlations between each metric are rather low, "which is reassuring as all metrics attempt to capture compositionality" in different 'opinionated' ways, especially between topsim/posdis:0.08 --{\textgreater} "the most 'opinionated' posdis measure is the one that behaves most differently from topsim".


7) Performing generalization test on converging runs, i.e. train accuracy {\textgreater} 99.9{\%}, spearman correlation with input set size |I| is high and significative, thus their claim that: "emergent languages are able to almost perfectly generalize to unseen combinations as long as input size |I| is sufficiently large".

--{\textgreater} appendix 8.4 investigate whether the density is relevant by sampling a fixed number of values from meaning spaces with varying density, or apparent density that the agent is exposed to. They show that "the high generalization observed in the main paper is (also) a consequence of density, and hence combinatorial variety, of the inputs the agents are trained on, and not (only) of the number of training examples".




Future works:
1) There claim that generalizing languages emerge when the input domain is sufficiently large seems to be related to [Lake {\&} Baroni,2018]'s findings that RNN are able to generalize when provided with enough supporting evidence. In other words, this kind of generalization does not qualify as systematic generalization, as explained in [Korrel et al., 2019], for instance.
--{\textgreater} Either the issue is architectural, as shown in the "inscrutably entangled ways" through which "emergent languages successfully refer to novel composite concepts" --because of their reliance on RNNs as complex systems...--, and as [Korrel et al., 2019; Russin et al., 2019] skirt the problem via attention --used as a constraint to the complex system maybe?--, or the issue is with the training scheme [Hill et al., 2019; Papers Abstract analogy making from hill barrett and santoro with RN modules].

At the end of the day, following their results, we need to change our architecture, and, given the biological inspiration, the architecture of [Russin et al., 2019] or [Korrel et al., 2019] are worth investigating as they provide inductive biases that can be found in human brains and thus provide an explanation to our ability to generalize in a systematic fashion, with few examples, maybe?



"Simple strategy of exposing them to a richer input should always be tried:.
This sentence seem to emphasize that the authors are not distringuing systematic generalization from generalization alone:
Making a difference between systematic generalization and generalization in the literature is therefore critical. The authors argue that generalization (potentially even interpolation, to be more specific) is possible when the agent is presented with enough supporting evidence, similarly to the findings of [Lake {\&} Baroni, 2018; Liska et al., 2018].
But, for sure, systematic generalization is not currently achievable (when there is only a "small pool of carefully-crafted examples"). It is even likely that Kottur et al. did test for systematic generalization without specifying it, and maybe even in the context of extrapolation.},
archivePrefix = {arXiv},
arxivId = {2004.09124},
author = {Chaabouni, Rahma and Kharitonov, Eugene and Bouchacourt, Diane and Dupoux, Emmanuel and Baroni, Marco},
eprint = {2004.09124},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaabouni et al. - 2020 - Compositionality and Generalization in Emergent Languages.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Paradigm,Article Gestion/1) TO READ/Linguistic,Article Gestion/1) TO READ/Compositionality,Article Gestion/1) TO READ/Disentanglement/Life-Long Learning},
month = {apr},
title = {{Compositionality and Generalization in Emergent Languages}},
url = {http://arxiv.org/abs/2004.09124},
year = {2020}
}

@article{Chaabouni2019b,
abstract = {Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to "natural" word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of "effort" into neural networks, as a possible way to make their linguistic behavior more human-like.},
annote = {"which biases [neural networks] display with respect to 'natural' word-order constraints"?

--{\textgreater} "temporal iconicity, defined as the tendency of clauses denoting events to reflect the chronological order of the denoted events"
--{\textgreater}"need to disambiguate the role of sentence constituents"
--{\textgreater} "avoid or minimize long-distance dependencies"


Biblio:
1) Seq2Seq [Sutskever et al. 2014]
2) Language processing mechanisms [Lake and Baroni, 2018]

3) Temporal iconicity [...]
4) Role disambiguation of sentence constituents
--{\textgreater} Trade-off [Comrie, 1981; Blake 2001]

5)Long-distance dependancies minimization: [Hawkins, 1994; Gibson 1998; Futrell et al. 2015]

6) Emerging communication protocal in RGs are "very different from human language" [Kottur et al. 2017; Lewis et al. 2017; Bouchacourt and Baroni, 2018]


Ideas:
1) Situated agent training 2D gridworld with navigation instructions.

2) Consider "Ease of learning" w.r.t. constraints:

3) "Input and output vocabularies are identical and contain all possible actions and words. When an agent plays the Speaker role, it uses input action representations and output word representations and conversely in the Listener role".

4) Tie encoder input and decoder output embeddings [Press and Wolf. 2016] yields faster convergence...

5) Teacher forcing is used to train the listener [Goodfellow et al. 2016, p.376]


6) The loss computation is sub-sampled with n=6 "when training free- and fixed-order languages" in order to speed up training, because there can be way too many target utterances for a given example when language is free- or fixed-ordered. No performance loss...


Results:
"No clear avoidance for redundant coding in either modes. In sum, we confirm a preference for iconic orders. Only attention-enhanaced agent in speaking mode displays avoidance of redundant coding".

"As predicted a clear preference for local constructions emerges, confirming the presence of a distance minimization bias in Seq2Seq models."



Future works/limitations:
1) "A better understanding of what are the 'innate' biases of standard models [...] should complement large-scale simulations, as part of the effort to develop new methods to encourage the emergence of more human-like language".

2) "How to incorporate 'effort'-based pressures in neural networks"?


3) With the word-action space, affordances are grounded in the language as well as the world/environment. This is a huge deal because it enforces an induction bias towards building a language with innately-grounded elements (actions) and learnable elements (words), rather than starting from scratch.
i.e. the vocabulary and action space may be fused together into a common space. It can be argued that it makes sense at a reasoning level to think of a word X as an action 'utter word X', thus making the common space a common action/thought/reasoning space.},
archivePrefix = {arXiv},
arxivId = {1905.12330},
author = {Chaabouni, Rahma and Kharitonov, Eugene and Lazaric, Alessandro and Dupoux, Emmanuel and Baroni, Marco},
eprint = {1905.12330},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaabouni et al. - 2019 - Word-order biases in deep-agent emergent communication.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience/Linguistic,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/NLP/Dialogue / Natural Language Generation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
month = {may},
title = {{Word-order biases in deep-agent emergent communication}},
url = {http://arxiv.org/abs/1905.12330},
year = {2019}
}



@article{Bogin2018,
abstract = {Training agents to communicate with one another given task-based supervision only has attracted considerable attention recently, due to the growing interest in developing models for human-agent interaction. Prior work on the topic focused on simple environments, where training using policy gradient was feasible despite the non-stationarity of the agents during training. In this paper, we present a more challenging environment for testing the emergence of communication from raw pixels, where training using policy gradient fails. We propose a new model and training algorithm, that utilizes the structure of a learned representation space to produce more consistent speakers at the initial phases of training, which stabilizes learning. We empirically show that our algorithm substantially improves performance compared to policy gradient. We also propose a new alignment-based metric for measuring context-independence in emerged communication and find our method increases context-independence compared to policy gradient and other competitive baselines.},
annote = {Policy gradient was once "feasible despite the non-stationarity of the agents during training", but not on this raw-pixels task. 
Authors propose a new method which enhances initial stationarity/stability.
+ alignement-based metric for measuring context-independence in the communication. 

Biblio:
1) language learning/grounding is an active and interactive task.

2) interactive environments where the goal is specified by linguistic input.

3) end-to-end language emergence for the purpose of task completion.

4) compositionality [wagner 2003, nowak and krakauer 1999]

5) Referential Games BIBLIO...

6) "DIFFERENTIABLE RELAXATION TO FACILITATE END-TO-END LEARNING" and sometimes the setup relies on a model-based RL framework which limits the applicability.

7) Recognie the lack of proper metrics: "evaluating the properties of emergent communication is a difficult problem"
--{\textgreater} "qualitative analyses"
--{\textgreater} "generalization tests"

Ideas:
1) Referential game is too simple a task.
--{\textgreater} immediate feedback is observed by the listener, that is not viable compared to real world with delayed rewards and long sequence of actions.
--{\textgreater} only ever solve a single task.

2) New environment requires the agent to perform many actions and different tasks.
--{\textgreater} policy gradient "fails in this interactive environment [...] due to the non-stationarity of both agents [..] combined with the long sequence of actions required to solve the task" 
--{\textgreater} "since the probability of randomly performing a correct action sequence is vanishing".


3) Consistancy in the agents' utterances can be achieved via the obverter technique.

4) "Context-independence, namely, whether the symbols retain their semantics in various contexts which, under a mild assumption, implies compositionality".


5) The RL approach proposed here is model-free and thus greatly applicable.

6) Multi-symbol alignment algorithms (IBM model 1 [Brown et al 1993]), as opposed to the single-symbol - concepts alignment tested in [Lazaridou 2016b], or "multi-symbol communication, but not explicitly test for alignment between symbols and concepts" in [Lazaridou et al. 2018]

7) The interactive environment accomodates multiple task in order to "examine information transfer between tasks"!!!!!!!!!!!!!!

8) "given similar worlds observed by the speaker, different utterances will be observed by the listener with high probability at the beginning of training, which will make it hard for the listener to learn a mapping from symbols to actions."
--{\textgreater} "output the same utterance with high probability even at the initial phase of training, while still having the fleibility to change the meaning of utterances based on the training signal."
--{\textgreater} instead of learning a speaker as a mapping from world-goal pair to utterance, it learns a "scalar-valued scoring function that evaluates how good the speaker thinkgs an utterance is, given the initial world and the goal".

9) The speaker is optimised towards "push[ing] utterances that lead to high reward closer to the world-goal representation they were uttered in, and utterances that lead to low reward farther away".
--{\textgreater} Objective is computted over all the prefixes of u.
--{\textgreater} the coefficient alpha is putting higher weight on positive reward samples!
--{\textgreater} "The geometric structure breaks the symmetry between utterances at the beginning of training and stabilizes learning".

10) Compared to [Choi et al, 2018]:
--{\textgreater} "we focus on exploiting the structure of the latent space to break the symmetry between utterances and get a consistent speaker".
--{\textgreater} "works in an environment with delayed reward", so it cannot "be trained with maximum likelihood"...
--{\textgreater} Only the listener used to be trained, whereas here they "train the speaker using feedback from the listener".

11) Pre-training is done using the speaker architecture and policy gradient. 
--{\textgreater} From there, the listener agent will differentiate in that it will no longer experience the goal g but the utterance u instead.
--{\textgreater} The speaker agent will now "only update the parameters that represent u"

12) PGLowEnt: One of the baseline is a low-entropy Policy Gradient algorithm, which means that the actions of the agent are more consistent from one run to the next...
--{\textgreater} It performs better than the vanilla Policy Gradient algorithm!! (sample-efficiency and final success rate)

13) FIXEDRAND: One of the baseline is a "perfectly consistent communication protocol that does not emerge and is not compositional", thus showing:
--{\textgreater} consistency is the most important thing that lets PG agents learn in this context, since this arch outperforms the other PG-baselines.
--{\textgreater} what does compositionality brings onto the table compared to having only consistency : CCO turns out to be more sample-efficient, thus hinting at the idea that compositionality is what helps increase sample-efficiency, if not the emergence process...

13) Obverter algorithm solves Referential task but not the interactive settings!!!!

14) "Context-independence: whether atomic symbols retain their meaning regardless of context".
--{\textgreater} How does it relate to compositionality: it relates to compositionality under the "mild" assumption that "there exist multiple words in the language that refer to multiple properties and can be combined in a single utterance"

15) CI is increased by:
--{\textgreater} CCO (vs Obverter/PG)
--{\textgreater} BOW (vs GRU...?!)

16) Considering a fair comparison of CI scores between control env. and an environment with increase number of missions, the hypothesis that the CI should increase has been rejected...?




Limitations/Future Works:
1) Pre-training is applied in a single-agent setting, but we do not see any of those learning curves. Supposedly they should be all the same since those part of the network agents are not changing.

2) Bag-of-words-based architecture always performs better than GRU architecture in terms of Context-Independence and even success rate for very structured environments (3C/3N/ 2M and 3M envs).
--{\textgreater} This conforts the idea that RNN are not bringing on the table a viable learning bias.

3) "NATURAL LANGUAGE IS NOT CONTEXT-INDEPENDENT, but words retain semantics in many contexts".
Why not comparing CI with Topographic Similarity to have a better idea of where we stand...?},
archivePrefix = {arXiv},
arxivId = {1809.00549},
author = {Bogin, Ben and Geva, Mor and Berant, Jonathan},
eprint = {1809.00549},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bogin, Geva, Berant - 2018 - Emergence of Communication in an Interactive World with Consistent Speakers(2).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience/Linguistic,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue},
month = {sep},
title = {{Emergence of Communication in an Interactive World with Consistent Speakers}},
url = {http://arxiv.org/abs/1809.00549},
year = {2018}
}


@misc{dsprites17,
author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
title = {dSprites: Disentanglement testing Sprites dataset},
howpublished= {https://github.com/deepmind/dsprites-dataset/},
year = "2017",
}

@article{marcus2018deep,
  title={Deep learning: A critical appraisal},
  author={Marcus, Gary},
  journal={arXiv preprint arXiv:1801.00631},
  year={2018}
}

@article{Voita2020info,
abstract = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.},
annote = {"Despite widespread adotpion of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones".

Rather than "constrain either the amount of probe training data or its model size", the authors focus on switching to "information-theoretic probing with minimum description length (MDL)".

Two methods can be used: "variational coding and online coding; they differ in a way they incorporate model cost: directly or indirectly".

--{\textgreater} " we can conclude that the ability of a probe to achieve good quality using a small amount of data and its ability to achieve good quality using a small probe architecture reflect the same property: strength of the regularity in the data".



Biblio:
1) Probing review,: [Belinkov and Glass, 2019]
--{\textgreater} issue with random representation: [Zhang and Bowman, 2018]

Idea:
1) Problem reformulation: "Formally, we recast learning a model of data (i.e. training a probing classifier) as training it to transmit the data (i.e. labels) in as few bits as possible)".
--{\textgreater} "Any regularity in representations with respect to labels can be exploited both to make predictions and to compress these labels, i.e. reduce length of the code needed to transmit them".

--{\textgreater} Change of measure: "instead of evaluating probe accuracy, we evaluate minimum description length (MDL) of labels given representations, i.e. the minimum number of bits needed to transmit the labels knowing the representations".

--{\textgreater} "If representations have some clear structure with respect to labels, the relation between the representations and the labels can be understood with less effort".
If so, then "(i) the 'rule' predicting the label (i.e. the probing model) can be simple, and/or (ii) the amount of data needed to reveal this structure can be small".

2) Online coding "plots quality as a function of the number of training examples" / "area under the learning curve"...},
archivePrefix = {arXiv},
arxivId = {2003.12298},
author = {Voita, Elena and Titov, Ivan},
eprint = {2003.12298},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Voita, Titov - 2020 - Information-Theoretic Probing with Minimum Description Length.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Paradigm},
month = {mar},
title = {{Information-Theoretic Probing with Minimum Description Length}},
url = {http://arxiv.org/abs/2003.12298},
year = {2020}
}



@article{Lazaridou2018,
abstract = {The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.},
annote = {Language Evolution and language Acquisition are two problems of emergent communication. The author study how "environmental/pre-linguistic conditions" shape the EL?

Biblio:
1) Meaning of language derived from its use. cf Wittgenstein x)!!!
2) Emergence in co-operative scenarios.
3) relation to MA and Self-Play.
4) COMPOSITIONALITY IN NL (cf Frege, 1892). and in EL with little successes and/or limitations though...
5) Lewis signaling game and biblio of language evolution studies.
6) Kottur, 2017 : non-compositional language conditionned on overcompleteness of alphabets used.
7) Partial Pooling Equilibrium : ambiguity induiced polysemy
8) "no formal mathematical definition of compositionality" : usually evaluated via the ability to generalize to novel situations.
9) Measure of message structure used in language evolution literature.
10) TOPOGRAPHIC SIMILARITY: levenshtein distances and cosine similarity, ?VisA vectors?
--{\textgreater} negative Spearman rho correlation.


Ideas:
1.1) What happens when we give the agents a Symbolic Representation as input?
1.2) What happens, in comparaison, when we give them Raw Perceptual Representation as input?
1.3) polysemy and ambiguity
1.4)distractors distribution: uniform vs context-dependant: it affects the language learning dynamics. 
a. uniform --{\textgreater} (object similarity ={\textgreater} object confusability, "as similar objects tend to be mapped onto the same message")
b. context-dependant --{\textgreater} poor correlation between object similarity and confusability. 
1.5) Measuring compositionality through the measure of generalization to novel (with differing degree of novelty) objects: test / unigram chimeras / uniform chimeras. Measure of PRODUCTIVITY as a predicate for compositionality (e.g. pourcentage of novel messages).
1.6) Topographic similarity: similar objects maps to similar signals with a high negative Spearman correlation : shows qualitatively the emergence of category-specific prefix bigrams.

2.1) Presenting the agents with entangled (raw pixels) representations, look at how the communication-based guiding reward affects this learning: without convnet pretraining!
a. --{\textgreater} the more distractors there are, the higher the topographic similarity is.
b. --{\textgreater} bias towards color description
c. highly constrained message space allows for greater stability of the nature of the protocol.
d. information encoding is dependant on the pre-linguistic pressure (environment), thus, so is the emerging protocal and the "data structures"/representations, thus furthering ad-hoc naming conventions, like in human-human interaction literature.
2.2) Alleviate on the formation of ad-hoc naming convention requires more complexe games and transfer across games.
2.3) Even when relevant, object shape is not encoded into the speaker's ConveNets. 


Future Works/Limitations:
1) Sampling at training time: use of REINFORCE update rule.
2) At test time, the choice of the distractors must be important in terms of the final results when the language learned comprises of ambiguity. So, in order to enforce language compositionality while diminuishing ambiguity, the number of distractors should be increased, right? Assuming it is increased to all the memory, and this memory keeps on growing, then the problem of describing each new souvenir with a discriminative utterence enforces the emergence of compositionality. How can we measure the compositionality of an emerged language??
3) What about the emergence of relational and spatial utterrances of language? Can it be achieved through design choices, like the context-dependant distractors, or viewpoint in the context of experiences/video/policies...

4) Measure of productivity in the context-dependant distractors case for training and uniform distractors for testing?
5) Comparison of the topographic similarity between NL and the ALs is missing?
6) Expand the raw visual games to be using trajectories...
7) Investigate the instability of the emerged language in terms of semantics and interpretability.
8) Investigate how the addition of unsupervised learning processes affects the whole "communication behaviour", when learning from raw visual inputs. (cf. Doya, CoSyne 2019 for interest in terms of biological plausibility of this approach)
9) In terms of stability issue because of the speaker and listeners parallel learning, can we throw at it a non-stationnary-environment-purposed learning scheme in order to yield better stability and, maybe a premice to zero-shot language grounding/acquisition?
10) 2.2 --{\textgreater} What about transfer across speakers? with pragmatic listener on a multi-turn channel?
11) 2.3 --{\textgreater} Knowing that Image classification convents do learn a shape bias, the fact that this learning scheme escape it is quite surprising. (cf. paper from DeepMind about visual biases learned in image classification convnets...)},
archivePrefix = {arXiv},
arxivId = {1804.03984},
author = {Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Clark, Stephen},
eprint = {1804.03984},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazaridou et al. - 2018 - Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input(3).pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Neuroscience},
month = {apr},
title = {{Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input}},
url = {http://arxiv.org/abs/1804.03984},
year = {2018}
}


@article{Bouchacourt2018,
abstract = {There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents' symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017) and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we are interested in developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.},
archivePrefix = {arXiv},
arxivId = {1808.10696},
author = {Bouchacourt, Diane and Baroni, Marco},
eprint = {1808.10696},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouchacourt, Baroni - 2018 - How agents see things On visual representations in an emergent language game.pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Curriculum/Active/Episodic learning,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Neuroscience,Article Gestion/1) TO READ/NLP/Visual Captioning,Article Gestion/1) TO READ/Linguistic},
month = {aug},
title = {{How agents see things: On visual representations in an emergent language game}},
url = {http://arxiv.org/abs/1808.10696},
year = {2018}
}

@article{Korbak2019,
abstract = {This paper explores a novel approach to achieving emergent compositional communication in multi-agent systems. We propose a training regime implementing template transfer, the idea of carrying over learned biases across contexts. In our method, a sender-receiver pair is first trained with disentangled loss functions and then the receiver is transferred to train a new sender with a standard loss. Unlike other methods (e.g. the obverter algorithm), our approach does not require imposing inductive biases on the architecture of the agents. We experimentally show the emergence of compositional communication using topographical similarity, zero-shot generalization and context independence as evaluation metrics. The presented approach is connected to an important line of work in semiotics and developmental psycholinguistics: it supports a conjecture that compositional communication is scaffolded on simpler communication protocols.},
annote = {Propose a referential game-based training regime "implementing template transfer", i.e. "carrying over learned biases across contexts".
--{\textgreater} "demonstration that communication protocols exhibiting compositionality can emerge via adaptation of pre-existing, simpler non-compositional protocols to a new environment"

They show that "the ability to communicate compositionality can emerge in a model less cognitively demanding than the obverter approach" by relying on boostrapping from pre-existing simpler protocols, i.e. that it does not require "strong inductive biases to be imposed on communicating agents [Kottur et al., 2017]".




Biblio:
1) "Language-like communication protocols" in challenging environments:
--{\textgreater} requiring "share information and coordinate behavior".

2) Compositionality "facilitates generaliaztion" [Lake et al. 2016]


3) Template transfer [Barrett and Skyrms, 2017].


4) Semiotic framework showing how to go from holistic language to "complex compositional communication protocols" [Deacon, 1998; Peirce, 1998].

5) Language development research "demonstrate[d] that children learn to speak compositionally [...] [through] progressively more complex [...] simple language games [Stern, 1974; Bruner, 1983; Nomikou et al., 2017; Raczaszek-Leonardi et al., 2018]".



Ideas:
1) Three phase learning: as supported by semiotic and is more akin to "human language development than other approaches".
--{\textgreater} "(i) learning a visual classifier"
--{\textgreater} "(ii) learning non-compositional communication protocols"
--{\textgreater} "(iii) learning a compositional communication protocol".

2) Rather than a strong inductive bias, the idea is to place "pressure on agents to use symbols consistently accross varying contexts". 
--{\textgreater} Memory reset during dialog:[Kottur et al., 2017; Das et al., 2017]
--{\textgreater} Obverter algorithm from [Oliphant and Batali, 1997; Batali, 1998] in [Choi et al., 2018; Bogin et al., 2018].
{\textless}-- Indeed, the obverter approach can only be implemented where "the agents share an identical architecture and the task must be symmetric". (I do not buy into the idea of agents needing the same architecture...)


3) Let us use "population-based training that incentivizes the creation of communication protocols that are easy to teach to new agents [Birghton, 2002; Li and Bowling, 2019] and gradually increasing task complexity that incentivizes reusing existing patterns of communication [De Beule and Bergen, 2006]".

4) (i) "two senders communicate with one receiver in two sub-games [that] are disentangled in the sense that their tasks are to correctly indicate one aaspect of the object (color or shape)". 
(ii) "Then, the second phase follows, in which the receiver is passed (via template transfer) to the object naming game [...] with a new sender".



Future works/Limitations:
1) Isn't it what has been demonstrated by [Smith, Kirby, and Brighton, 2003]?!

2) In order to be gradually increasing the complexity of the task, some control over the input data is necessary and thus limits the range of application of this scheme. Although the authors mention it, there is no

3) "the sender generates a sequence of T [(=2 during object naming, and T=1 during template transfer game)] discrete messages sampled from a closed vocabulary of 10 symbols".
--{\textgreater} Following [Kottur et al., 2017], isn't it obvious that compositional language emerges in this setting with a closed vocabulary? 
--{\textgreater} and so little margin for not being compositional given that the message length is so small?!},
archivePrefix = {arXiv},
arxivId = {1910.06079},
author = {Korbak, Tomasz and Zubek, Julian and Kuci{\'{n}}ski, {\L}ukasz and Mi{\l}o{\'{s}}, Piotr and R{\c{a}}czaszek-Leonardi, Joanna},
eprint = {1910.06079},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Korbak et al. - 2019 - Developmentally motivated emergence of compositional communication via template transfer.pdf:pdf},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality},
month = {oct},
title = {{Developmentally motivated emergence of compositional communication via template transfer}},
url = {http://arxiv.org/abs/1910.06079},
year = {2019}
}

@misc{manetensorboard,
  title={TensorBoard: TensorFlow’s visualization toolkit, 2015},
  author={Man{\'e}, D and others}
}

@misc{huang2018tensorboardx,
  title={Tensorboardx},
  url={https://github.com/lanpa/tensorboardX},
  author={Huang, Tzu-Wei},
  year={2018}
}

@article{jaques2018social,
  title={Social influence as intrinsic motivation for multi-agent deep reinforcement learning},
  author={Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro A and Strouse, DJ and Leibo, Joel Z and De Freitas, Nando},
  journal={arXiv preprint arXiv:1810.08647},
  year={2018}
}

@article{kornuta2019pytorchpipe,
  title={PyTorchPipe: a framework for rapid prototyping of pipelines combining language and vision},
  author={Kornuta, Tomasz},
  journal={arXiv preprint arXiv:1910.08654},
  year={2019}
}

@article{kharitonov2019egg,
  title={EGG: a toolkit for research on Emergence of lanGuage in Games},
  author={Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
  journal={arXiv preprint arXiv:1907.00852},
  year={2019}
}

@article{burgess2019monet,
  title={Monet: Unsupervised scene decomposition and representation},
  author={Burgess, Christopher P and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1901.11390},
  year={2019}
}

@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{mcclelland2010letting,
  title={Letting structure emerge: connectionist and dynamical systems approaches to cognition},
  author={McClelland, James L and Botvinick, Matthew M and Noelle, David C and Plaut, David C and Rogers, Timothy T and Seidenberg, Mark S and Smith, Linda B},
  journal={Trends in cognitive sciences},
  volume={14},
  number={8},
  pages={348--356},
  year={2010},
  publisher={Elsevier}
}

@article{Mordatch2017,
abstract = {By capturing statistical patterns in large cor-pora, machine learning has enabled significant advances in natural language processing, includ-ing in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply cap-turing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. To-wards this end, we propose a multi-agent learn-ing environment and learning methods that bring about emergence of a basic compositional lan-guage. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal com-munication such as pointing and guiding when language communication is unavailable.},
annote = {Statistical learning of language is not the best way to learn it. Rather learning from first principles is what we should aim for. To wit, learning out of the necessity to communicate in order to achieve some non-verbal goal that requires coordination between multiple agents/humans. 

Language composition emerges from:

1) the complexity of the environment(e.g. through duplicate objects, multiple agents).

2) encouraging small vocabulary size (e.g. through soft penalty).

Non-verbal communication strategies emerge from pointing and guiding effector use-ability.

Limitations: 
1) 2D world with agent-centric observations 
--{\textgreater} investigate POMDPS agent-centric observations (in 3D world) so question how much it may scale to more complexe problems. 
=={\textgreater} We can expect the complexity of the language to grow as the environment is (location status, object status, question/answer-filled dialogues - not just the instructions...)
2) Learned via backpropagation through time (LOLA-like approach...) : in reality, one agent cannot have access to other agents differential graph.
--{\textgreater} investigate the viability of model-free RL .
--{\textgreater} investigate the viability of meta-RL with a symbol randomization layer so that agents learn to learn a strategy to learn to communicate with other agents in a un-seen setting.
=={\textgreater} We can expect to yield an information-seeking agent that attempts to learn how to communicate effectively on-the-fly ( with/without non-verbal communication helps/cues?). Two conditions can be investigated: one when there is only one meta-learner whereas the pairing agents are agents that have been trained in the current paper's regime. The other condition would be that where the pairing agent is the replica of the meta-learning agent. Thus, it would give us insights about what is necessary to learn to learn strategies to communicate.
3) Curiosity have not been tackled. It seems unnecessary given the fact that there is no need to gather information. Nevertheless, the auxiliary goal prediction could be understood as a curiosity-fulfilling auxiliary goal. It might induce a learning bias into the architecture of the RNN dynamic graph?
--{\textgreater} investigate RNN dynamic graph in the presence of auxiliary task, with regards to try to quantify in what way is the RNN affected by the auxiliary task and try to draw some comparaison with information seeking agents.
=={\textgreater} we can expect to see a decrease in the RNN output's uncertainty (dropout bayesian evaluation?) as the auxiliary task goal gets fulfilled step-by-step?
=={\textgreater} In other words, are auxiliary tasks inducing bias towards building a fictitious curiosity-guided algorithm in the RNN dynamic that would be lead towards reducing the amount of uncertainty about the auxiliary output? compared to feedforward NN counterparts which might induce a information-representation bias, maybe?},
author = {Mordatch, Igor and Abbeel, Pieter},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mordatch, Abbeel - Unknown - Emergence of Grounded Compositional Language in Multi-Agent Populations.pdf:pdf},
mendeley-groups = {to read - Novelty,to read - NLU,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE,iGGi/Literature Review/Interpretability,iGGi/Literature Review/Conversational models,iGGi/Literature Review/Knowledge Representation,iGGi/Literature Review/Compositionality,iGGi/Literature Review/Multi-Agent,iGGi/Literature Review,iGGi/Literature Review/Multimodality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/MultiAgent},
title = {{Emergence of Grounded Compositional Language in Multi-Agent Populations}},
url = {https://arxiv.org/pdf/1703.04908.pdf}
}


@article{Keresztury2020,
abstract = {Recent findings in multi-agent deep learning systems point towards the emergence of compositional languages. These claims are often made without exact analysis or testing of the language. In this work, we analyze the emergent language resulting from two different cooperative multi-agent game with more exact measures for compositionality. Our findings suggest that solutions found by deep learning models are often lacking the ability to reason on an abstract level therefore failing to generalize the learned knowledge to out of the training distribution examples. Strategies for testing compositional capacities and emergence of human-level concepts are discussed.},
annote = {"One good measure of compositional thinking is the ability to systematically generalize to new data distributions"},
archivePrefix = {arXiv},
arxivId = {2001.08618},
author = {Keresztury, Bence and Bruni, Elia},
eprint = {2001.08618},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keresztury, Bruni - 2020 - Compositional properties of emergent languages in deep learning(2).pdf:pdf},
mendeley-groups = {Article Gestion/1) TO READ/Compositionality,Article Gestion/1) TO READ/Neuroscience/Language,Article Gestion/1) TO READ/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/1) TO READ/Neuroscience/Multimodal fusion/Compositionality,Article Gestion/1) TO READ/NLP/Dialogue,Article Gestion/1) TO READ/Paradigm},
month = {jan},
title = {{Compositional properties of emergent languages in deep learning}},
url = {http://arxiv.org/abs/2001.08618},
year = {2020}
}

@article{montague1970universal,
  title={Universal grammar},
  author={Montague, Richard},
  journal={Theoria},
  volume={36},
  number={3},
  pages={373--398},
  year={1970},
  publisher={Wiley Online Library}
}


@article{Denamganai2020a,
  title={ReferentialGym: A Nomenclature and Framework for Language Emergence \& Grounding in (Visual) Referential Games},
  author={Denamganaï, Kevin and Walker, James A.},
  journal={4th NeurIPS Workshop on Emergent Communication},
  year={2020}
}


@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}



@article{DenamganaiAndWalker2020a,
  title={ReferentialGym: A Framework for Language Emergence \& Grounding in (Visual) Referential Games},
  author={Denamganaï, Kevin and Walker, James Alfred},
  journal={4th NeurIPS Workshop on Emergent Communication},
  year={2020}
}

@article{DenamganaiAndWalker2020b,
  title={On (Emergent) Systematic Generalisation and Compositionality in Visual Referential Games with Straight-Through Gumbel-Softmax Estimator},
  author={Denamganaï, Kevin and Walker, James Alfred},
  journal={4th NeurIPS Workshop on Emergent Communication},
  year={2020}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@ARTICLE{SciPy-NMeth2020,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@ARTICLE{NumPy-Array2020,
  author  = {Harris, Charles R. and Millman, K. Jarrod and
            van der Walt, Stéfan J and Gommers, Ralf and
            Virtanen, Pauli and Cournapeau, David and
            Wieser, Eric and Taylor, Julian and Berg, Sebastian and
            Smith, Nathaniel J. and Kern, Robert and Picus, Matti and
            Hoyer, Stephan and van Kerkwijk, Marten H. and
            Brett, Matthew and Haldane, Allan and
            Fernández del Río, Jaime and Wiebe, Mark and
            Peterson, Pearu and Gérard-Marchant, Pierre and
            Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and
            Abbasi, Hameer and Gohlke, Christoph and
            Oliphant, Travis E.},
  title   = {Array programming with {NumPy}},
  journal = {Nature},
  year    = {2020},
  volume  = {585},
  pages   = {357–362},
  doi     = {10.1038/s41586-020-2649-2}
}

@article{Scikit-learn:JMLR:v12:pedregosa11a,
  author  = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay},
  title   = {Scikit-learn: Machine Learning in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {85},
  pages   = {2825-2830},
  url     = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@article{scikit-image-van2014,
  title={scikit-image: image processing in Python},
  author={Van der Walt, Stefan and Sch{\"o}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  journal={PeerJ},
  volume={2},
  pages={e453},
  year={2014},
  publisher={PeerJ Inc.}
}

@ARTICLE{matplotlib-hunter-2007,
  author={J. D. {Hunter}},
  journal={Computing in Science   Engineering}, 
  title={Matplotlib: A 2D Graphics Environment}, 
  year={2007},
  volume={9},
  number={3},
  pages={90-95},
  doi={10.1109/MCSE.2007.55}
}

@ARTICLE{ipython-perez-2007,
  author={F. {Perez} and B. E. {Granger}},
  journal={Computing in Science   Engineering}, 
  title={IPython: A System for Interactive Scientific Computing}, 
  year={2007},
  volume={9},
  number={3},
  pages={21-29},
  doi={10.1109/MCSE.2007.53}
}

@incollection{pytorch-paszke-NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@InProceedings{ pandas1-mckinney-proc-scipy-2010,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}

@software{pandas2-reback2020,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    version      = {latest},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}

@book{python-2009,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}

@article{YuEtAl2017,
abstract = {We tackle a task where an agent learns to navigate in a 2D maze-like environment called XWORLD. In each session, the agent perceives a sequence of raw-pixel frames, a natural language command issued by a teacher, and a set of rewards. The agent learns the teacher's language from scratch in a grounded and compositional manner, such that after training it is able to correctly execute zero-shot commands: 1) the combination of words in the command never appeared before, and/or 2) the command contains new object concepts that are learned from another task but never learned from navigation. Our deep framework for the agent is trained end to end: it learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and the action module that outputs actions. The zero-shot learning capability of our framework results from its compositionality and modularity with parameter tying. We visualize the intermediate outputs of the framework, demonstrating that the agent truly understands how to solve the problem. We believe that our results provide some preliminary insights on how to train an agent with similar abilities in a 3D environment.},
archivePrefix = {arXiv},
arxivId = {1703.09831},
author = {Yu, Haonan and Zhang, Haichao and Xu, Wei},
doi = {10.1080/01691864.2016.1164622},
eprint = {1703.09831},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Zhang, Xu - 2017 - A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment(3).pdf:pdf},
issn = {15685535},
mendeley-groups = {Article Gestion/1) TO READ/NLP},
pages = {1--16},
title = {{A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment}},
url = {http://arxiv.org/abs/1703.09831},
year = {2017}
}


@article{MulBouchacourtBruni2019,
abstract = {To cooperate with humans effectively, virtual agents need to be able to understand and execute language instructions. A typical setup to achieve this is with a scripted teacher which guides a virtual agent using language instructions. However, such setup has clear limitations in scalability and, more importantly, it is not interactive. Here, we introduce an autonomous agent that uses discrete communication to interactively guide other agents to navigate and act on a simulated environment. The developed communication protocol is trainable, emergent and requires no additional supervision. The emergent language speeds up learning of new agents, it generalizes across incrementally more difficult tasks and, contrary to most other emergent languages, it is highly interpretable. We demonstrate how the emitted messages correlate with particular actions and observations, and how new agents become less dependent on this guidance as training progresses. By exploiting the correlations identified in our analysis, we manage to successfully address the agents in their own language.},
annote = {Must re-read Co-Reyes et al (2018) to understand how the "correction module[...] provides additional linguistic feedback to an agent".
Or Cideron et al. 2020, as it contains a similar HER-based mechanism of providing information for building a policy, but addresses a different problem (sparse reward when goal-oriented and unsuccessful goals...)

"The observation that Guides can even be helpful to new Learners at levels for which they were not optimized suggests that their messages convey information that is relevant to the internalization of skills that generalize accross levels".

Using behavioral cloning "reduced training to a supervised setting", instead of using an RL setting...

Future Works:

1) ST-GS is used to parameterized the comm. channel, but V=3 and L=2, so it is slightly a given that the resulting protocols are compositional.
What happens in a lifelong-learning setting where V and L have to be large from the very beginning is worth investigating?

2) Also, rather than making the learner able to understand natural language, the authors attempted to translate the artificial language. Subsequent work could go the other way around by using THER as an auxiliary loss on the Guide, for instance?

3) Progressive independance of the Learner from the Guide's messages is shown, but the Guide seem to keep on issuing messages, which is slightly weird: it might be worth investigating a loss that promote a less-talkative Guide?
--{\textgreater} Given that the learner sees the mission, penalizing the Guide might converge into a trivial agent that does not listen to the Guide. Making the mission solely known to the Guide might resolve that issue.
--{\textgreater} What is the point then? The resulting Learner knows an artificial language and can follow instructions in that language, but how much generalization does it have? Does it generalize better or train faster than a regular RL agent? Are its inner representations more disentangled? more modular?
Have a look at the recent paper from Deepmind where two agents interact to learn to do task in a 3D environment: why the need of the interaction? does it allow for a curriculum of some sort (i.e. better data-efficiency) or anything else, compared to normal RL instruction-following?},
archivePrefix = {arXiv},
arxivId = {1908.05135},
author = {Mul, Mathijs and Bouchacourt, Diane and Bruni, Elia},
eprint = {1908.05135},
file = {:home/kevin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mul, Bouchacourt, Bruni - 2019 - Mastering emergent language learning to guide in simulated navigation.pdf:pdf},
journal = {arXiv},
mendeley-groups = {Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Compositionality,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Curriculum/Curiosity/Episodic/Imitation,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Instructions / Goal-based / Skills,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Language Grounding/Emergence,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/MultiAgent,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Paradigm/Human-in-the-loop,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Reasoning/Emergence/Pragmatics/Language/Query/Dialogue,Article Gestion/2) READ --{\textgreater} TO SUMMARIZE/Video games},
month = {aug},
publisher = {arXiv},
title = {{Mastering emergent language: learning to guide in simulated navigation}},
url = {http://arxiv.org/abs/1908.05135},
year = {2019}
}


@ARTICLE{Van_Steenkiste2019-xm,
  title    = "Are Disentangled Representations Helpful for Abstract Visual
              Reasoning?",
  author   = "van Steenkiste, Sjoerd and Locatello, Francesco and Schmidhuber,
              J{\"u}rgen and Bachem, Olivier",
  abstract = "A disentangled representation encodes information about the
              salient factors of variation in the data independently. Although
              it is often argued that this representational format is useful in
              learning to solve many real-world up-stream tasks, there is
              little empirical evidence that supports this claim. In this
              paper, we conduct a large-scale study that investigates whether
              disentangled representations are more suitable for abstract
              reasoning tasks. Using two new tasks similar to Raven's
              Progressive Matrices, we evaluate the usefulness of the
              representations learned by 360 state-of-the-art unsupervised
              disentanglement models. Based on these representations, we train
              3600 abstract reasoning models and observe that disentangled
              representations do in fact lead to better up-stream performance.
              In particular, they appear to enable quicker learning using fewer
              samples.",
  month    =  may,
  year     =  2019
}


@ARTICLE{Locatello2020-cx,
  title         = "A Sober Look at the Unsupervised Learning of Disentangled
                   Representations and their Evaluation",
  author        = "Locatello, Francesco and Bauer, Stefan and Lucic, Mario and
                   R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf,
                   Bernhard and Bachem, Olivier",
  abstract      = "The idea behind the \textbackslashemph\{unsupervised\}
                   learning of \textbackslashemph\{disentangled\}
                   representations is that real-world data is generated by a
                   few explanatory factors of variation which can be recovered
                   by unsupervised learning algorithms. In this paper, we
                   provide a sober look at recent progress in the field and
                   challenge some common assumptions. We first theoretically
                   show that the unsupervised learning of disentangled
                   representations is fundamentally impossible without
                   inductive biases on both the models and the data. Then, we
                   train over $14000$ models covering most prominent methods
                   and evaluation metrics in a reproducible large-scale
                   experimental study on eight data sets. We observe that while
                   the different methods successfully enforce properties
                   ``encouraged'' by the corresponding losses,
                   well-disentangled models seemingly cannot be identified
                   without supervision. Furthermore, different evaluation
                   metrics do not always agree on what should be considered
                   ``disentangled'' and exhibit systematic differences in the
                   estimation. Finally, increased disentanglement does not seem
                   to necessarily lead to a decreased sample complexity of
                   learning for downstream tasks. Our results suggest that
                   future work on disentanglement learning should be explicit
                   about the role of inductive biases and (implicit)
                   supervision, investigate concrete benefits of enforcing
                   disentanglement of the learned representations, and consider
                   a reproducible experimental setup covering several data
                   sets.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2010.14766"
}


@MISC{Chen_2018-MIG,
  title        = "Isolating sources of disentanglement in {VAEs}",
  author       = "Chen, Ricky T Q and Li, Xuechen and Grosse, Roger and
                  Duvenaud, David",
  howpublished = "\url{https://papers.nips.cc/paper/2018/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf}",
  note         = "Accessed: 2021-3-17"
}


@MISC{Eastwood_2018-cr,
  title        = "A framework for the quantitative evaluation of disentangled
                  representations",
  author       = "Eastwood, Cian and Williams, Christopher K I",
  howpublished = "\url{https://openreview.net/pdf?id=By-7dz-AZ}",
  note         = "Accessed: 2021-4-4"
}


@MISC{Ridgeway2018-xo,
  title  = "Learning Deep Disentangled Embeddings With the {F-Statistic} Loss",
  author = "Ridgeway, Karl and Mozer, Michael C",
  pages  = "185--194",
  year   =  2018
}



@ARTICLE{Hill2019-tm,
  title         = "Environmental drivers of systematicity and generalization in
                   a situated agent",
  author        = "Hill, Felix and Lampinen, Andrew and Schneider, Rosalia and
                   Clark, Stephen and Botvinick, Matthew and McClelland, James
                   L and Santoro, Adam",
  abstract      = "The question of whether deep neural networks are good at
                   generalising beyond their immediate training experience is
                   of critical importance for learning-based approaches to AI.
                   Here, we consider tests of out-of-sample generalisation that
                   require an agent to respond to never-seen-before
                   instructions by manipulating and positioning objects in a 3D
                   Unity simulated room. We first describe a comparatively
                   generic agent architecture that exhibits strong performance
                   on these tests. We then identify three aspects of the
                   training regime and environment that make a significant
                   difference to its performance: (a) the number of object/word
                   experiences in the training set; (b) the visual invariances
                   afforded by the agent's perspective, or frame of reference;
                   and (c) the variety of visual input inherent in the
                   perceptual aspect of the agent's perception. Our findings
                   indicate that the degree of generalisation that networks
                   exhibit can depend critically on particulars of the
                   environment in which a given task is instantiated. They
                   further suggest that the propensity for neural networks to
                   generalise in systematic ways may increase if, like human
                   children, those networks have access to many frames of
                   richly varying, multi-modal observations as they learn.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1910.00571"
}

@ARTICLE{Ruis2020-vj,
  title         = "A Benchmark for Systematic Generalization in Grounded
                   Language Understanding",
  author        = "Ruis, Laura and Andreas, Jacob and Baroni, Marco and
                   Bouchacourt, Diane and Lake, Brenden M",
  abstract      = "Humans easily interpret expressions that describe unfamiliar
                   situations composed from familiar parts (``greet the pink
                   brontosaurus by the ferris wheel''). Modern neural networks,
                   by contrast, struggle to interpret novel compositions. In
                   this paper, we introduce a new benchmark, gSCAN, for
                   evaluating compositional generalization in situated language
                   understanding. Going beyond a related benchmark that focused
                   on syntactic aspects of generalization, gSCAN defines a
                   language grounded in the states of a grid world,
                   facilitating novel evaluations of acquiring linguistically
                   motivated rules. For example, agents must understand how
                   adjectives such as 'small' are interpreted relative to the
                   current world state or how adverbs such as 'cautiously'
                   combine with new verbs. We test a strong multi-modal
                   baseline model and a state-of-the-art compositional method
                   finding that, in most cases, they fail dramatically when
                   generalization requires systematic compositional rules.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2003.05161"
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@inproceedings{
montero2021the,
title={The role of Disentanglement in Generalisation},
author={Milton Llera Montero and Casimir JH Ludwig and Rui Ponte Costa and Gaurav Malhotra and Jeffrey Bowers},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qbH974jKUVy}
}

@article{batali1998computational,
  title={Computational simulations of the emergence of grammar},
  author={Batali, John},
  journal={Approach to the Evolution of Language},
  pages={405--426},
  year={1998},
  publisher={Cambridge University Press}
}

@article{chen2016infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1606.03657},
  year={2016}
}

@article{Denamganai2023visual-COMPODIS,
  title={Visual Referential Games Further the Emergence of Disentangled Representations},
  author={Denamgana{\"\i}, Kevin and Missaoui, Sondess and Walker, James Alfred},
  journal={arXiv preprint arXiv:2304.14511},
  year={2023}
}

@article{Denamganai2021b,
  title={Meta-Referential Games to Learn CompositionalLearning Behaviours},
  author={Denamganaï, Kevin and Missaoui, Sondess and Walker, James A.},
  journal={draft to be submitted to ICLR/AAAI 2022},
  year={2021}
}

@article{denamganai2022meta-rg-arxiv,
  title={Meta-Referential Games to Learn Compositional Learning Behaviours},
  author={Denamgana{\"\i}, Kevin and Missaoui, Sondess and Walker, James Alfred},
  journal={arXiv preprint arXiv:2207.08012},
  year={2022}
}

@article{denamganai2023meta-rg-openreview-iclr2024,
title={Meta-Referential Games to Learn Compositional Learning Behaviours},
author={Denamgana{\"\i}, Kevin and Missaoui, Sondess and Walker, James Alfred},
booktitle={Submitted to The Twelfth International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=17BA0Tl2Id},
note={under review at ICLR 2024}
}

@article{Denamganai2021c,
  title={ETHER: Emerging Textual Hindsight Experience Replay},
  author={Denamganaï, Kevin and Missaoui, Sondess and Walker, James A.},
  journal={draft to be submitted to CVPR 2022},
  year={2021}
}

@article{denamganai2023ether-arxiv,
  title={ETHER: Aligning Emergent Communication for Hindsight Experience Replay},
  author={Denamgana{\"\i}, Kevin and Hernandez, Daniel and Vardal, Ozan and Missaoui, Sondess and Walker, James Alfred},
  journal={arXiv preprint arXiv:2307.15494},
  year={2023}
}

@article{denamganai2023erelela-wip,
  title={EReLELA: Exploration in Reinforcement Learning via Emergent Language Abstractions},
  author={Denamgana{\"\i}, Kevin and Missaoui, Sondess and Moss, Guy and Walker, James Alfred},
  note={work in progress, not published yet},
  year={2023}
}

@article{Denamganai2021AITAO,
  title={AITAO: Adversarial Iterative Amplification for Obverter Agent in Visual Referential Games},
  author={Denamganaï, Kevin and Missaoui, Sondess and Walker, James A.},
  journal={draft to be submitted to ICLR/AAAI 2022},
  year={2021}
}

@article{Denamganai2022a,
  title={RS2B: Referential Game Symbolic Behaviour Benchmark},
  author={Denamganaï, Kevin and Missaoui, Sondess and Walker, James A.},
  journal={paper proposal to be submitted current 2022},
  year={2022}
}

@inproceedings{kapturowski2018recurrent,
  title={Recurrent experience replay in distributed reinforcement learning},
  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle={International conference on learning representations},
  year={2018}
}

@article{paine2019making,
  title={Making efficient use of demonstrations to solve hard exploration problems},
  author={Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and others},
  journal={arXiv preprint arXiv:1909.01387},
  year={2019}
}

@article{greff2020binding,
  title={On the binding problem in artificial neural networks},
  author={Greff, Klaus and van Steenkiste, Sjoerd and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2012.05208},
  year={2020}
}

@article{santoro2021symbolic,
  title={Symbolic behaviour in artificial intelligence},
  author={Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},
  journal={arXiv preprint arXiv:2102.03406},
  year={2021}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017}
}

@incollection{jakobson1960linguistics,
  title={Linguistics and poetics},
  author={Jakobson, Roman},
  booktitle={Style in language},
  pages={350--377},
  year={1960},
  publisher={MA: MIT Press}
}

@inproceedings{
levy2018hierarchical,
title={Hierarchical Reinforcement Learning with Hindsight},
author={Andrew Levy and Robert Platt and Kate Saenko},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryzECoAcY7},
}

@inproceedings{cideron2020higher,
  title={Higher: Improving instruction following with hindsight generation for experience replay},
  author={Cideron, Geoffrey and Seurin, Mathieu and Strub, Florian and Pietquin, Olivier},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={225--232},
  year={2020},
  organization={IEEE}
}

@inproceedings{hu2020other,
  title={“Other-Play” for Zero-Shot Coordination},
  author={Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={4399--4410},
  year={2020},
  organization={PMLR}
}

@article{cope2021learning,
  title={Learning to Communicate with Strangers via Channel Randomisation Methods},
  author={Cope, Dylan and Schoots, Nandi},
  journal={arXiv preprint arXiv:2104.09557},
  year={2021}
}

@article{graves2016hybrid,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Publishing Group}
}


@TECHREPORT{Hill2020-grounding-fast-and-slow,
  title    = "Grounded Language Learning Fast and Slow",
  author   = "Hill, Felix and Tieleman, Olivier and von Glehn, Tamara and Wong,
              Nathaniel and Merzic, Hamza and Clark DeepMind, Stephen",
  abstract = "Recent work has shown that large text-based neural language
              models, trained with conventional supervised learning objectives,
              acquire a surprising propensity for few-and one-shot learning.
              Here, we show that an embodied agent situated in a simulated 3D
              world, and endowed with a novel dual-coding external memory, can
              exhibit similar one-shot word learning when trained with
              conventional reinforcement learning algorithms. After a single
              introduction to a novel object via continuous visual perception
              and a language prompt (``This is a dax''), the agent can
              re-identify the object and manipulate it as instructed (``Put the
              dax on the bed''). In doing so, it seamlessly integrates
              short-term, within-episode knowledge of the appropriate referent
              for the word ``dax'' with long-term lexical and motor knowledge
              acquired across episodes (i.e. ``bed'' and ``putting''). We find
              that, under certain training conditions and with a particular
              memory writing mechanism, the agent's one-shot word-object
              binding generalizes to novel exemplars within the same ShapeNet
              category, and is effective in settings with unfamiliar numbers of
              objects. We further show how dual-coding memory can be exploited
              as a signal for intrinsic motivation, stimulating the agent to
              seek names for objects that may be useful for later executing
              instructions. Together, the results demonstrate that deep neural
              networks can exploit meta-learning, episodic memory and an
              explicitly multi-modal environment to account for fast-mapping, a
              fundamental pillar of human cognitive development and a
              potentially transformative capacity for agents that interact with
              human users.",
  year     =  2020
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}


@ARTICLE{Wu2021-entropy-decomposition,
  title         = "Reinforced Natural Language Interfaces via Entropy
                   Decomposition",
  author        = "Wu, Xiaoran",
  abstract      = "In this paper, we study the technical problem of developing
                   conversational agents that can quickly adapt to unseen
                   tasks, learn task-specific communication tactics, and help
                   listeners finish complex, temporally extended tasks. We find
                   that the uncertainty of language learning can be decomposed
                   to an entropy term and a mutual information term,
                   corresponding to the structural and functional aspect of
                   language, respectively. Combined with reinforcement
                   learning, our method automatically requests human samples
                   for training when adapting to new tasks and learns
                   communication protocols that are succinct and helpful for
                   task completion. Human and simulation test results on a
                   referential game and a 3D navigation game prove the
                   effectiveness of the proposed method.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2109.11408"
}


@ARTICLE{Sharma2021-sl3,
  title         = "Skill Induction and Planning with Latent Language",
  author        = "Sharma, Pratyusha and Torralba, Antonio and Andreas, Jacob",
  abstract      = "We present a framework for learning hierarchical policies
                   from demonstrations, using sparse natural language
                   annotations to guide the discovery of reusable skills for
                   autonomous decision-making. We formulate a generative model
                   of action sequences in which goals generate sequences of
                   high-level subtask descriptions, and these descriptions
                   generate sequences of low-level actions. We describe how to
                   train this model using primarily unannotated demonstrations
                   by parsing demonstrations into sequences of named high-level
                   subtasks, using only a small number of seed annotations to
                   ground language in action. In trained models, the space of
                   natural language commands indexes a combinatorial library of
                   skills; agents can use these skills to plan by generating
                   high-level instruction sequences tailored to novel goals. We
                   evaluate this approach in the ALFRED household simulation
                   environment, providing natural language annotations for only
                   10\% of demonstrations. It completes more than twice as many
                   tasks as a standard approach to learning from
                   demonstrations, matching the performance of instruction
                   following models with access to ground-truth plans during
                   both training and evaluation.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.01517"
}


@ARTICLE{Steenbrugge2018-sq,
  title         = "Improving Generalization for Abstract Reasoning Tasks Using
                   Disentangled Feature Representations",
  author        = "Steenbrugge, Xander and Leroux, Sam and Verbelen, Tim and
                   Dhoedt, Bart",
  abstract      = "In this work we explore the generalization characteristics
                   of unsupervised representation learning by leveraging
                   disentangled VAE's to learn a useful latent space on a set
                   of relational reasoning problems derived from Raven
                   Progressive Matrices. We show that the latent
                   representations, learned by unsupervised training using the
                   right objective function, significantly outperform the same
                   architectures trained with purely supervised learning,
                   especially when it comes to generalization.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1811.04784"
}

@ARTICLE{Kingma2013-VAE,
  title     = "Auto-encoding variational bayes",
  author    = "Kingma, D P and Welling, M",
  abstract  = "How can we perform efficient inference and learning in directed
               probabilistic models, in the presence of continuous latent
               variables with intractable posterior distributions, and large …",
  journal   = "arXiv preprint arXiv:1312.6114",
  publisher = "arxiv.org",
  year      =  2013
}



@ARTICLE{Santoro2021-gl,
  title    = "Symbolic Behaviour in Artificial Intelligence",
  author   = "Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and
              Lillicrap, Timothy and Raposo, David",
  abstract = "The ability to use symbols is the pinnacle of human intelligence,
              but has yet to be fully replicated in machines. Here we argue
              that the path towards symbolically fluent artificial intelligence
              (AI) begins with a reinterpretation of what symbols are, how they
              come to exist, and how a system behaves when it uses them. We
              begin by offering an interpretation of symbols as entities whose
              meaning is established by convention. But crucially, something is
              a symbol only for those who demonstrably and actively participate
              in this convention. We then outline how this interpretation
              thematically unifies the behavioural traits humans exhibit when
              they use symbols. This motivates our proposal that the field
              place a greater emphasis on symbolic behaviour rather than
              particular computational mechanisms inspired by more restrictive
              interpretations of symbols. Finally, we suggest that AI research
              explore social and cultural engagement as a tool to develop the
              cognitive machinery necessary for symbolic behaviour to emerge.
              This approach will allow for AI to interpret something as
              symbolic on its own rather than simply manipulate things that are
              only symbols to human onlookers, and thus will ultimately lead to
              AI with more human-like symbolic fluency.",
  month    =  feb,
  year     =  2021,
  keywords = "Connectionism; Symbolic AI"
}


@book{whorf2012language,
  title={Language, thought, and reality: Selected writings of Benjamin Lee Whorf},
  author={Whorf, Benjamin Lee},
  year={2012},
  publisher={MIT press}
}

@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}

@article{horgan2018apex,
  title={Distributed prioritized experience replay},
  author={Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1803.00933},
  year={2018}
}


@ARTICLE{Co-Reyes2018-jl,
  title    = "Guiding Policies with Language via {Meta-Learning}",
  author   = "Co-Reyes, John D and Gupta, Abhishek and Sanjeev, Suvansh and
              Altieri, Nick and DeNero, John and Abbeel, Pieter and Levine,
              Sergey",
  abstract = "Behavioral skills or policies for autonomous agents are
              conventionally learned from reward functions, via reinforcement
              learning, or from demonstrations, via imitation learning.
              However, both modes of task specification have their
              disadvantages: reward functions require manual engineering, while
              demonstrations require a human expert to be able to actually
              perform the task in order to generate the demonstration.
              Instruction following from natural language instructions provides
              an appealing alternative: in the same way that we can specify
              goals to other humans simply by speaking or writing, we would
              like to be able to specify tasks for our machines. However, a
              single instruction may be insufficient to fully communicate our
              intent or, even if it is, may be insufficient for an autonomous
              agent to actually understand how to perform the desired task. In
              this work, we propose an interactive formulation of the task
              specification problem, where iterative language corrections are
              provided to an autonomous agent, guiding it in acquiring the
              desired skill. Our proposed language-guided policy learning
              algorithm can integrate an instruction and a sequence of
              corrections to acquire new skills very quickly. In our
              experiments, we show that this method can enable a policy to
              follow instructions and corrections for simulated navigation and
              manipulation tasks, substantially outperforming direct,
              non-interactive instruction following.",
  month    =  nov,
  year     =  2018
}


@INPROCEEDINGS{Sukhbaatar2016-eh,
  title     = "Learning Multiagent Communication with Backpropagation",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Sukhbaatar, Sainbayar and Szlam, Arthur and Fergus, Rob",
  editor    = "Lee, D and Sugiyama, M and Luxburg, U and Guyon, I and Garnett,
               R",
  publisher = "Curran Associates, Inc.",
  volume    =  29,
  year      =  2016
}


@ARTICLE{Lin2021-ae-comm,
  title         = "Learning to Ground {Multi-Agent} Communication with
                   Autoencoders",
  author        = "Lin, Toru and Huh, Minyoung and Stauffer, Chris and Lim,
                   Ser-Nam and Isola, Phillip",
  abstract      = "Communication requires having a common language, a lingua
                   franca, between agents. This language could emerge via a
                   consensus process, but it may require many generations of
                   trial and error. Alternatively, the lingua franca can be
                   given by the environment, where agents ground their language
                   in representations of the observed world. We demonstrate a
                   simple way to ground language in learned representations,
                   which facilitates decentralized multi-agent communication
                   and coordination. We find that a standard representation
                   learning algorithm -- autoencoding -- is sufficient for
                   arriving at a grounded common language. When agents
                   broadcast these representations, they learn to understand
                   and respond to each other's utterances and achieve
                   surprisingly strong task performance across a variety of
                   multi-agent communication environments.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.15349"
}


@ARTICLE{Li2021-broadcast-and-listen,
  title         = "Learning Emergent Discrete Message Communication for
                   Cooperative Reinforcement Learning",
  author        = "Li, Sheng and Zhou, Yutai and Allen, Ross and Kochenderfer,
                   Mykel J",
  abstract      = "Communication is a important factor that enables agents work
                   cooperatively in multi-agent reinforcement learning (MARL).
                   Most previous work uses continuous message communication
                   whose high representational capacity comes at the expense of
                   interpretability. Allowing agents to learn their own
                   discrete message communication protocol emerged from a
                   variety of domains can increase the interpretability for
                   human designers and other agents.This paper proposes a
                   method to generate discrete messages analogous to human
                   languages, and achieve communication by a
                   broadcast-and-listen mechanism based on self-attention. We
                   show that discrete message communication has performance
                   comparable to continuous message communication but with much
                   a much smaller vocabulary size.Furthermore, we propose an
                   approach that allows humans to interactively send discrete
                   messages to agents.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2102.12550"
}


@UNPUBLISHED{Singh2018-IC3Net,
  title    = "Learning when to Communicate at Scale in Multiagent Cooperative
              and Competitive Tasks",
  author   = "Singh, Amanpreet and Jain, Tushar and Sukhbaatar, Sainbayar",
  abstract = "Learning when to communicate and doing that effectively is
              essential in multi-agent tasks. Recent works show that continuous
              communication allows efficient training with back-propagation in
              multi-agent scenarios, but have been restricted to
              fully-cooperative tasks. In this paper, we present Individualized
              Controlled Continuous Communication Model (IC3Net) which has
              better training efficiency than simple continuous communication
              model, and can be applied to semi-cooperative and competitive
              settings along with the cooperative settings. IC3Net controls
              continuous communication with a gating mechanism and uses
              individualized rewards foreach agent to gain better performance
              and scalability while fixing credit assignment issues. Using
              variety of tasks including StarCraft BroodWars explore and combat
              scenarios, we show that our network yields improved performance
              and convergence rates than the baselines as the scale increases.
              Our results convey that IC3Net agents learn when to communicate
              based on the scenario and profitability.",
  month    =  sep,
  year     =  2018
}


@ARTICLE{Eccles2019-biases,
  title         = "Biases for Emergent Communication in Multi-agent
                   Reinforcement Learning",
  author        = "Eccles, Tom and Bachrach, Yoram and Lever, Guy and
                   Lazaridou, Angeliki and Graepel, Thore",
  abstract      = "We study the problem of emergent communication, in which
                   language arises because speakers and listeners must
                   communicate information in order to solve tasks. In
                   temporally extended reinforcement learning domains, it has
                   proved hard to learn such communication without centralized
                   training of agents, due in part to a difficult joint
                   exploration problem. We introduce inductive biases for
                   positive signalling and positive listening, which ease this
                   problem. In a simple one-step environment, we demonstrate
                   how these biases ease the learning problem. We also apply
                   our methods to a more extended environment, showing that
                   agents with these inductive biases achieve better
                   performance, and analyse the resulting communication
                   protocols.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.MA",
  eprint        = "1912.05676"
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@ARTICLE{Dagan2020-yf,
  title         = "Co-evolution of language and agents in referential games",
  author        = "Dagan, Gautier and Hupkes, Dieuwke and Bruni, Elia",
  abstract      = "Referential games offer a grounded learning environment for
                   neural agents which accounts for the fact that language is
                   functionally used to communicate. However, they do not take
                   into account a second constraint considered to be
                   fundamental for the shape of human language: that it must be
                   learnable by new language learners. Cogswell et al. (2019)
                   introduced cultural transmission within referential games
                   through a changing population of agents to constrain the
                   emerging language to be learnable. However, the resulting
                   languages remain inherently biased by the agents' underlying
                   capabilities. In this work, we introduce Language
                   Transmission Engine to model both cultural and architectural
                   evolution in a population of agents. As our core
                   contribution, we empirically show that the optimal situation
                   is to take into account also the learning biases of the
                   language learners and thus let language and agents
                   co-evolve. When we allow the agent population to evolve
                   through architectural evolution, we achieve across the board
                   improvements on all considered metrics and surpass the gains
                   made with cultural transmission. These results stress the
                   importance of studying the underlying agent architecture and
                   pave the way to investigate the co-evolution of language and
                   agent in language emergence studies.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2001.03361"
}



@ARTICLE{Lu2020-ssil,
  title         = "Supervised Seeded Iterated Learning for Interactive Language
                   Learning",
  author        = "Lu, Yuchen and Singhal, Soumye and Strub, Florian and
                   Pietquin, Olivier and Courville, Aaron",
  abstract      = "Language drift has been one of the major obstacles to train
                   language models through interaction. When word-based
                   conversational agents are trained towards completing a task,
                   they tend to invent their language rather than leveraging
                   natural language. In recent literature, two general methods
                   partially counter this phenomenon: Supervised Selfplay (S2P)
                   and Seeded Iterated Learning (SIL). While S2P jointly trains
                   interactive and supervised losses to counter the drift, SIL
                   changes the training dynamics to prevent language drift from
                   occurring. In this paper, we first highlight their
                   respective weaknesses, i.e., late-stage training collapses
                   and higher negative likelihood when evaluated on human
                   corpus. Given these observations, we introduce Supervised
                   Seeded Iterated Learning to combine both methods to minimize
                   their respective weaknesses. We then show the effectiveness
                   of \textbackslashalgo in the language-drift translation
                   game.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.02975"
}


@ARTICLE{Lowe2020-supselfplay,
  title         = "On the interaction between supervision and self-play in
                   emergent communication",
  author        = "Lowe, Ryan and Gupta, Abhinav and Foerster, Jakob and Kiela,
                   Douwe and Pineau, Joelle",
  abstract      = "A promising approach for teaching artificial agents to use
                   natural language involves using human-in-the-loop training.
                   However, recent work suggests that current machine learning
                   methods are too data inefficient to be trained in this way
                   from scratch. In this paper, we investigate the relationship
                   between two categories of learning signals with the ultimate
                   goal of improving sample efficiency: imitating human
                   language data via supervised learning, and maximizing reward
                   in a simulated multi-agent environment via self-play (as
                   done in emergent communication), and introduce the term
                   supervised self-play (S2P) for algorithms using both of
                   these signals. We find that first training agents via
                   supervised learning on human data followed by self-play
                   outperforms the converse, suggesting that it is not
                   beneficial to emerge languages from scratch. We then
                   empirically investigate various S2P schedules that begin
                   with supervised learning in two environments: a Lewis
                   signaling game with symbolic inputs, and an image-based
                   referential game with natural language descriptions. Lastly,
                   we introduce population based approaches to S2P, which
                   further improves the performance over single-agent methods.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2002.01093"
}


@ARTICLE{Shih2021-convention-role,
  title         = "On the Critical Role of Conventions in Adaptive {Human-AI}
                   Collaboration",
  author        = "Shih, Andy and Sawhney, Arjun and Kondic, Jovana and Ermon,
                   Stefano and Sadigh, Dorsa",
  abstract      = "Humans can quickly adapt to new partners in collaborative
                   tasks (e.g. playing basketball), because they understand
                   which fundamental skills of the task (e.g. how to dribble,
                   how to shoot) carry over across new partners. Humans can
                   also quickly adapt to similar tasks with the same partners
                   by carrying over conventions that they have developed (e.g.
                   raising hand signals pass the ball), without learning to
                   coordinate from scratch. To collaborate seamlessly with
                   humans, AI agents should adapt quickly to new partners and
                   new tasks as well. However, current approaches have not
                   attempted to distinguish between the complexities intrinsic
                   to a task and the conventions used by a partner, and more
                   generally there has been little focus on leveraging
                   conventions for adapting to new settings. In this work, we
                   propose a learning framework that teases apart
                   rule-dependent representation from convention-dependent
                   representation in a principled way. We show that, under some
                   assumptions, our rule-dependent representation is a
                   sufficient statistic of the distribution over best-response
                   strategies across partners. Using this separation of
                   representations, our agents are able to adapt quickly to new
                   partners, and to coordinate with old partners on new tasks
                   in a zero-shot manner. We experimentally validate our
                   approach on three collaborative tasks varying in complexity:
                   a contextual multi-armed bandit, a block placing task, and
                   the card game Hanabi.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2104.02871"
}


@ARTICLE{Leike2018-agent-alignment,
  title    = "Scalable agent alignment via reward modeling: a research
              direction",
  author   = "Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan
              and Maini, Vishal and Legg, Shane",
  abstract = "One obstacle to applying reinforcement learning algorithms to
              real-world problems is the lack of suitable reward functions.
              Designing such reward functions is difficult in part because the
              user only has an implicit understanding of the task objective.
              This gives rise to the agent alignment problem: how do we create
              agents that behave in accordance with the user's intentions? We
              outline a high-level research direction to solve the agent
              alignment problem centered around reward modeling: learning a
              reward function from interaction with the user and optimizing the
              learned reward function with reinforcement learning. We discuss
              the key challenges we expect to face when scaling reward modeling
              to complex and general domains, concrete approaches to mitigate
              these challenges, and ways to establish trust in the resulting
              agents.",
  month    =  nov,
  year     =  2018
}

@inproceedings{hill2020groundedfastandslow-iclr,
  title={Grounded Language Learning Fast and Slow},
  author={Hill, Felix and Tieleman, Olivier and von Glehn, Tamara and Wong, Nathaniel and Merzic, Hamza and Clark, Stephen},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@ARTICLE{Bengio2012-ml,
  title    = "Deep Learning of Representations for Unsupervised and Transfer
              Learning",
  author   = "Bengio, Yoshua",
  abstract = "Deep learning algorithms seek to exploit the unknown structure in
              the input distribution in order to discover good representations,
              often at multiple levels, with higher-level learned features
              defined in terms of lower-level features. The objective is to
              make these higher-level representations more abstract, with their
              individual features more invariant to most of the variations that
              are typically present in the training distribution, while
              collectively preserving as much as possible of the information in
              the input. Ideally, we would like these representations to
              disentangle the unknown factors of variation that underlie the
              training distribution. Such unsupervised learning of
              representations can be exploited usefully under the hypothesis
              that the input distribution P (x) is structurally related to some
              task of interest, say predicting P (y|x). This paper focuses on
              the context of the Unsupervised and Transfer Learning Challenge,
              on why unsupervised pre-training of representations can be
              useful, and how it can be exploited in the transfer learning
              scenario, where we care about predictions on examples that are
              not from the same distribution as the training distribution.",
  journal  = "Conf. Proc. IEEE Eng. Med. Biol. Soc.",
  volume   =  27,
  pages    = "17--37",
  year     =  2012,
  keywords = "Auto-encoders; Deep Learning; Re-stricted Boltzmann Machines;
              domain adaptation; multi-task learning; neural networks;
              representation learning; self-taught learning; transfer
              learn-ing; unsupervised learning"
}


@UNPUBLISHED{Dessi2021-emecom_as_ssl,
  title    = "Interpretable agent communication from scratch (with a generic
              visual processor emerging on the side)",
  author   = "Dessi, Roberto and Kharitonov, Eugene and Baroni, Marco",
  abstract = "As deep networks begin to be deployed as autonomous agents, the
              issue of how they can communicate with each other becomes
              important. Here, we train two deep nets from scratch to perform
              realistic referent identification through unsupervised emergent
              communication. We show that the largely interpretable emergent
              protocol allows the nets to successfully communicate even about
              object types they did not see at training time. The visual
              representations induced as a by-product of our training regime,
              moreover, show comparable quality, when re-used as generic visual
              features, to a recent self-supervised learning model. Our results
              provide concrete evidence of the viability of (interpretable)
              emergent deep net communication in a more realistic scenario than
              previously considered, as well as establishing an intriguing link
              between this field and self-supervised visual learning.",
  month    =  may,
  year     =  2021
}


@ARTICLE{Lazaridou2020-emecom_dl_era,
  title         = "Emergent {Multi-Agent} Communication in the Deep Learning
                   Era",
  author        = "Lazaridou, Angeliki and Baroni, Marco",
  abstract      = "The ability to cooperate through language is a defining
                   feature of humans. As the perceptual, motory and planning
                   capabilities of deep artificial networks increase,
                   researchers are studying whether they also can develop a
                   shared language to interact. From a scientific perspective,
                   understanding the conditions under which language evolves in
                   communities of deep agents and its emergent features can
                   shed light on human language evolution. From an applied
                   perspective, endowing deep networks with the ability to
                   solve problems interactively by communicating with each
                   other and with us should make them more flexible and useful
                   in everyday life. This article surveys representative recent
                   language emergence studies from both of these two angles.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2006.02419"
}

@article{mu2021emergent,
  title={Emergent Communication of Generalizations},
  author={Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17994--18007},
  year={2021}
}


@inproceedings{webb2020emergent,
  title={Emergent Symbols through Binding in External Memory},
  author={Webb, Taylor Whittington and Sinha, Ishan and Cohen, Jonathan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{schaul2015universal,
  title={Universal value function approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International conference on machine learning},
  pages={1312--1320},
  year={2015},
  organization={PMLR}
}


@ARTICLE{Xu2022-COMPODIS,
  title         = "Compositional Generalization in Unsupervised Compositional
                   Representation Learning: A Study on Disentanglement and
                   Emergent Language",
  author        = "Xu, Zhenlin and Niethammer, Marc and Raffel, Colin",
  abstract      = "Deep learning models struggle with compositional
                   generalization, i.e. the ability to recognize or generate
                   novel combinations of observed elementary concepts. In hopes
                   of enabling compositional generalization, various
                   unsupervised learning algorithms have been proposed with
                   inductive biases that aim to induce compositional structure
                   in learned representations (e.g. disentangled representation
                   and emergent language learning). In this work, we evaluate
                   these unsupervised learning algorithms in terms of how well
                   they enable compositional generalization. Specifically, our
                   evaluation protocol focuses on whether or not it is easy to
                   train a simple model on top of the learned representation
                   that generalizes to new combinations of compositional
                   factors. We systematically study three unsupervised
                   representation learning algorithms - $\beta$-VAE,
                   $\beta$-TCVAE, and emergent language (EL) autoencoders - on
                   two datasets that allow directly testing compositional
                   generalization. We find that directly using the bottleneck
                   representation with simple models and few labels may lead to
                   worse generalization than using representations from layers
                   before or after the learned representation itself. In
                   addition, we find that the previously proposed metrics for
                   evaluating the levels of compositionality are not correlated
                   with actual compositional generalization in our framework.
                   Surprisingly, we find that increasing pressure to produce a
                   disentangled representation produces representations with
                   worse generalization, while representations from EL models
                   show strong compositional generalization. Taken together,
                   our results shed new light on the compositional
                   generalization behavior of different unsupervised learning
                   algorithms with a new setting to rigorously test this
                   behavior, and suggest the potential benefits of delevoping
                   EL learning algorithms for more generalizable
                   representations.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.00482"
}

@article{carmeli2022quantizedEmeCom,
  title={Emergent Quantized Communication},
  author={Carmeli, Boaz and Meir, Ron and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2211.02412},
  year={2022}
}

@article{salimans2016improved-techniques-for-training-gans,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{radford2021CLIP,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{rita2020lazimpa,
  title={" LazImpa": Lazy and Impatient neural agents learn to communicate efficiently},
  author={Rita, Mathieu and Chaabouni, Rahma and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2010.01878},
  year={2020}
}

@book{zipf2016human-zla,
  title={Human behavior and the principle of least effort: An introduction to human ecology},
  author={Zipf, George Kingsley},
  year={2016},
  publisher={Ravenio Books}
}

@book{strauss2007word-zla,
  title={Word length and word frequency},
  author={Strauss, Udo and Grzybek, Peter and Altmann, Gabriel},
  year={2007},
  publisher={Springer}
}


@ARTICLE{Brandizzi2023-EmeComSurvey,
  title         = "Towards More Human-like {AI} Communication: A Review of
                   Emergent Communication Research",
  author        = "Brandizzi, Nicolo'",
  abstract      = "In the recent shift towards human-centric AI, the need for
                   machines to accurately use natural language has become
                   increasingly important. While a common approach to achieve
                   this is to train large language models, this method presents
                   a form of learning misalignment where the model may not
                   capture the underlying structure and reasoning humans employ
                   in using natural language, potentially leading to unexpected
                   or unreliable behavior. Emergent communication (Emecom) is a
                   field of research that has seen a growing number of
                   publications in recent years, aiming to develop artificial
                   agents capable of using natural language in a way that goes
                   beyond simple discriminative tasks and can effectively
                   communicate and learn new concepts. In this review, we
                   present Emecom under two aspects. Firstly, we delineate all
                   the common proprieties we find across the literature and how
                   they relate to human interactions. Secondly, we identify two
                   subcategories and highlight their characteristics and open
                   challenges. We encourage researchers to work together by
                   demonstrating that different methods can be viewed as
                   diverse solutions to a common problem and emphasize the
                   importance of including diverse perspectives and expertise
                   in the field. We believe a deeper understanding of human
                   communication is crucial to developing machines that can
                   accurately use natural language in human-machine
                   interactions.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.02541"
}

@article{sunehag2017VDN,
  title={Value-decomposition networks for cooperative multi-agent learning},
  author={Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z and Tuyls, Karl and others},
  journal={arXiv preprint arXiv:1706.05296},
  year={2017}
}

@inproceedings{hu2019SAD,
  title={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},
  author={Hu, Hengyuan and Foerster, Jakob N},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{mishra2018simple,
  title={A Simple Neural Attentive Meta-Learner},
  author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{beck2023surveyMeatRL,
  title={A survey of meta-reinforcement learning},
  author={Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2301.08028},
  year={2023}
}

@inproceedings{vinyals2016matching,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{lake2023MLC,
  title={Human-like systematic generalization through a meta-learning neural network},
  author={Lake, Brenden M and Baroni, Marco},
  journal={Nature},
  pages={1--7},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{lake2019compositional_NeurIPS,
  title={Compositional generalization through meta sequence-to-sequence learning},
  author={Lake, Brenden M},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{roy2022explainability,
  title={Explainability Via Causal Self-Talk},
  author={Roy, Nicholas A and Kim, Junkyung and Rabinowitz, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7655--7670},
  year={2022}
}

@article{tam2022semantic,
  title={Semantic exploration from language abstractions and pretrained representations},
  author={Tam, Allison and Rabinowitz, Neil and Lampinen, Andrew and Roy, Nicholas A and Chan, Stephanie and Strouse, DJ and Wang, Jane and Banino, Andrea and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25377--25389},
  year={2022}
}

@article{dayan1992feudal,
  title={Feudal reinforcement learning},
  author={Dayan, Peter and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@inproceedings{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={3540--3549},
  year={2017},
  organization={PMLR}
}

@article{mu2022language-exploration-intrinsic,
  title={Improving intrinsic exploration with language abstractions},
  author={Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah and Rockt{\"a}schel, Tim and Grefenstette, Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33947--33960},
  year={2022}
}

@inproceedings{jaderberg2016UNREAL,
  title={Reinforcement Learning with Unsupervised Auxiliary Tasks},
  author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{shen2023LLMAlignementSurvey,
  title={Large language model alignment: A survey},
  author={Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  journal={arXiv preprint arXiv:2309.15025},
  year={2023}
}

@article{wolf2023fundamentalLLMAlignmentLimitations,
  title={Fundamental limitations of alignment in large language models},
  author={Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

@article{li2023multimodalFoundationModel,
  title={Multimodal foundation models: From specialists to general-purpose assistants},
  author={Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2309.10020},
  volume={1},
  number={2},
  year={2023}
}

@article{fei2022towardsAGIWithMultimodalFoundationModels,
  title={Towards artificial general intelligence via a multimodal foundation model},
  author={Fei, Nanyi and Lu, Zhiwu and Gao, Yizhao and Yang, Guoxing and Huo, Yuqi and Wen, Jingyuan and Lu, Haoyu and Song, Ruihua and Gao, Xin and Xiang, Tao and others},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={3094},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{milani2023navigatesLikeMe,
  title={Navigates Like Me: Understanding How People Evaluate Human-Like AI in Video Games},
  author={Milani, Stephanie and Juliani, Arthur and Momennejad, Ida and Georgescu, Raluca and Rzepecki, Jaroslaw and Shaw, Alison and Costello, Gavin and Fang, Fei and Devlin, Sam and Hofmann, Katja},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2023}
}

@inproceedings{devlin2021navigationTuringTest,
  title={Navigation turing test (NTT): Learning to evaluate human-like navigation},
  author={Devlin, Sam and Georgescu, Raluca and Momennejad, Ida and Rzepecki, Jaroslaw and Zuniga, Evelyn and Costello, Gavin and Leroy, Guy and Shaw, Ali and Hofmann, Katja},
  booktitle={International Conference on Machine Learning},
  pages={2644--2653},
  year={2021},
  organization={PMLR}
}

@inproceedings{ash2022investigatingNegativesInContrastiveRepresentationLearning,
  title={Investigating the Role of Negatives in Contrastive Representation Learning},
  author={Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Misra, Dipendra},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7187--7209},
  year={2022},
  organization={PMLR}
}

@inproceedings{booch2021thinkingFastAndSlow,
  title={Thinking fast and slow in AI},
  author={Booch, Grady and Fabiano, Francesco and Horesh, Lior and Kate, Kiran and Lenchner, Jonathan and Linck, Nick and Loreggia, Andreas and Murgesan, Keerthiram and Mattei, Nicholas and Rossi, Francesca and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={17},
  pages={15042--15046},
  year={2021}
}

@book{Kahneman2017thinkingFastAndSlow,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  year={2017}
}

@article{knezic2010socraticDialogue,
  title={The Socratic Dialogue and teacher education},
  author={Knezic, Dubravka and Wubbels, Theo and Elbers, Ed and Hajer, Maaike},
  journal={Teaching and teacher education},
  volume={26},
  number={4},
  pages={1104--1111},
  year={2010},
  publisher={Elsevier}
}

@inproceedings{xie2021interactionGroundedLearning,
  title={Interaction-grounded learning},
  author={Xie, Tengyang and Langford, John and Mineiro, Paul and Momennejad, Ida},
  booktitle={International Conference on Machine Learning},
  pages={11414--11423},
  year={2021},
  organization={PMLR}
}

@article{xie2022interactionGroundedLearningWithAction,
  title={Interaction-grounded learning with action-inclusive feedback},
  author={Xie, Tengyang and Saran, Akanksha and Foster, Dylan J and Molu, Lekan and Momennejad, Ida and Jiang, Nan and Mineiro, Paul and Langford, John},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12529--12541},
  year={2022}
}