@article{sparse_V1,
title = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
journal = {Vision Research},
volume = {37},
number = {23},
pages = {3311-3325},
year = {1997},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(97)00169-7},
url = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
author = {Bruno A. Olshausen and David J. Field},
keywords = {Coding, V1, Gabor-wavelet, Natural images},
abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.}
}


@article{rozell2008sparse,
    author = {Rozell, Christopher J. and Johnson, Don H. and Baraniuk, Richard G. and Olshausen, Bruno A.},
    title = "{Sparse Coding via Thresholding and Local Competition in Neural Circuits}",
    journal = {Neural Computation},
    volume = {20},
    number = {10},
    pages = {2526-2563},
    year = {2008},
    month = {Oct.},
    abstract = "{While evidence indicates that neural systems may be employing sparse approximations to represent sensed stimuli, the mechanisms underlying this ability are not understood. We describe a locally competitive algorithm (LCA) that solves a collection of sparse coding principles minimizing a weighted combination of mean-squared error and a coefficient cost function. LCAs are designed to be implemented in a dynamical system composed of many neuron-like elements operating in parallel. These algorithms use thresholding functions to induce local (usually one-way) inhibitory competitions between nodes to produce sparse representations. LCAs produce coefficients with sparsity levels comparable to the most popular centralized sparse coding algorithms while being readily suited for neural implementation. Additionally, LCA coefficients for video sequences demonstrate inertial properties that are both qualitatively and quantitatively more regular (i.e., smoother and more predictable) than the coefficients produced by greedy algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.2008.03-07-486},
    url = {https://doi.org/10.1162/neco.2008.03-07-486}
}

@article{zhu2013visual,
  title={Visual nonclassical receptive field effects emerge from sparse coding in a dynamical system},
  author={Zhu, Mengchen and Rozell, Christopher J},
  journal={PLoS computational biology},
  volume={9},
  number={8},
  pages={e1003191},
  year={2013},
  publisher={Public Library of Science San Francisco, USA}
}

@article{Rozell2008SparseCV,
  title={Sparse Coding via Thresholding and Local Competition in Neural Circuits},
  author={Christopher J. Rozell and Don H. Johnson and Richard Baraniuk and Bruno A. Olshausen},
  journal={Neural Computation},
  year={2008},
  volume={20},
  pages={2526-2563}
}



@article{accumulator_neurons,
  author    = {Aaron R. Voelker and
               Daniel Rasmussen and
               Chris Eliasmith},
  title     = {A Spike in Performance: Training Hybrid-Spiking Neural Networks with
               Quantized Activation Functions},
  journal   = {CoRR},
  volume    = {abs/2002.03553},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.03553},
  eprinttype = {arXiv},
  eprint    = {2002.03553},
  timestamp = {Wed, 12 Feb 2020 16:38:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-03553.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  numpages  = {18},
}


@inproceedings{dictionarylearning,
author = {Parpart, Gavin and Gonzalez Rivera, Carlos and Stewart, Terrence and Kim, Edward and Rego, Jocelyn and O'Brien, Andrew and Nesbit, Steven and Kenyon, Garrett and Watkins, Yijing},
title = {Dictionary Learning with Accumulator Neurons},
year = {2022},
isbn = {9781450397896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546790.3546801},
doi = {10.1145/3546790.3546801},
abstract = {The Locally Competitive Algorithm (LCA) uses local competition between non-spiking leaky integrator neurons to infer sparse representations, allowing for potentially real-time execution on massively parallel neuromorphic architectures such as Intel’s Loihi processor. Here, we focus on the problem of inferring sparse representations from streaming video using dictionaries of spatiotemporal features optimized in an unsupervised manner for sparse reconstruction. Non-spiking LCA has previously been used to achieve unsupervised learning of spatiotemporal dictionaries composed of convolutional kernels from raw, unlabeled video. We demonstrate how unsupervised dictionary learning with spiking LCA (S-LCA) can be efficiently implemented using accumulator neurons, which combine a conventional leaky-integrate-and-fire (LIF) spike generator with an additional state variable that is used to minimize the difference between the integrated input and the spiking output. We demonstrate dictionary learning across a wide range of dynamical regimes, from graded to intermittent spiking, for inferring sparse representations of both static images drawn from the CIFAR database as well as video frames captured from a DVS camera. On a classification task that requires identification of the suite from a deck of cards being rapidly flipped through as viewed by a DVS camera, we find essentially no degradation in performance as the LCA model used to infer sparse spatiotemporal representations migrates from graded to spiking. We conclude that accumulator neurons are likely to provide a powerful enabling component of future neuromorphic hardware for implementing online unsupervised learning of spatiotemporal dictionaries optimized for sparse reconstruction of streaming video from event based DVS cameras.},
booktitle = {Proceedings of the International Conference on Neuromorphic Systems 2022},
articleno = {11},
numpages = {9},
keywords = {accumulator neurons, local competitive algorithms, spiking neural networks, dynamic vision sensors, dictionary learning, leaky integrator neurons, sparse coding},
location = {Knoxville, TN, USA},
series = {ICONS '22}
}

@inproceedings{appletospikes,
author = {Henke, Kyle and Teti, Michael and Kenyon, Garrett and Migliori, Ben and Kunde, Gerd},
title = {Apples-to-Spikes: The First Detailed Comparison of LASSO Solutions Generated by a Spiking Neuromorphic Processor},
year = {2022},
isbn = {9781450397896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546790.3546811},
doi = {10.1145/3546790.3546811},
abstract = {The Locally Competitive Algorithm (LCA) is a model of simple cells in the primary visual cortex, based on convex sparse coding via recurrent lateral competition between neighboring neurons. Previous work implemented spiking LCA (S-LCA) on the Loihi neuromorphic processor in which the lateral connections were constrained to be inhibitory, unlike non-spiking, analog LCA (A-LCA) where both excitatory and inhibitory connections are present. In the absence of lateral excitation, an implementation of S-LCA on the Loihi neuromorphic processor inferred sparse representations of image patches that were close to the global minimum, but an examination of the individual neural activations (i.e. solution) was not performed. In this work, we first prove that the constraints placed on the lateral connections in the previous S-LCA implementation were unnecessarily restrictive, and we develop an S-LCA implementation with both excitatory and inhibitory lateral connections. We implemented this improved S-LCA with both inhibitory and excitatory lateral connections on Loihi and show that the resulting sparse latent representations were much closer to those inferred by A-LCA. Specifically, we perform the first comparison of individual neuron activations between S-LCA and A-LCA and show that the final solution of our S-LCA converges to that of A-LCA. To date, this work provides one of the only instances in which a spiking algorithm implemented on modern neuromorphic hardware and performing a realistic task has exhibited such close behavior to its non-spiking counterpart.},
booktitle = {Proceedings of the International Conference on Neuromorphic Systems 2022},
articleno = {19},
numpages = {8},
keywords = {sparse coding, neuromorphic computing, spiking neural networks, computer vision},
location = {Knoxville, TN, USA},
series = {ICONS '22}
}

@ARTICLE{loihi1,
  author={Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A. Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R.},
  journal={Proceedings of the IEEE}, 
  title={Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook}, 
  year={2021},
  volume={109},
  number={5},
  pages={911-934},
  doi={10.1109/JPROC.2021.3067593}}


@article{loihi2,
  author    = {Garrick Orchard and
               Edward Paxon Frady and
               Daniel Ben Dayan Rubin and
               Sophia Sanborn and
               Sumit Bam Shrestha and
               Friedrich T. Sommer and
               Mike Davies},
  title     = {Efficient Neuromorphic Signal Processing with Loihi 2},
  journal   = {CoRR},
  volume    = {abs/2111.03746},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.03746},
  eprinttype = {arXiv},
  eprint    = {2111.03746},
  timestamp = {Wed, 10 Nov 2021 16:07:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-03746.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  numpages  = {6},
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.21},
  organization = {Google},
  year = {2018},
}

@artifactdataset{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  url={http://yann.lecun.com/exdb/mnist},
  organization={ATT Labs},
  year={2010}
}

@article{schultz2014replicating,
      title={Replicating Kernels with a Short Stride Allows Sparse Reconstructions with Fewer Independent Kernels}, 
      author={Peter F. Schultz and Dylan M. Paiton and Wei Lu and Garrett T. Kenyon},
      year={2014},
      eprint={1406.4205},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      journal   = {CoRR},
      volume    = {abs/1406.4205},
      numpages  = {12},
}

@online{inrcweb,
      title={Intel Neuromorphic Research Community},
      key={Intel},
      organization={Intel},
      url={https://www.intel.com/content/www/us/en/research/neuromorphic-community.html},
      year={2018}
}

@INPROCEEDINGS{LCA_compression,
  author={Watkins, Yijing and Iaroshenko, Oleksandr and Sayeh, Mohammad and Kenyon, Garrett},
  booktitle={2018 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)}, 
  title={Image Compression: Sparse Coding vs. Bottleneck Autoencoders}, 
  year={2018},
  volume={},
  number={},
  pages={17-20},
  publisher={IEEE},
  address={Las Vegas, NV, USA},
  doi={10.1109/SSIAI.2018.8470336}}

@inproceedings{LCA_event_recons,
author = {Watkins, Yijing and Thresher, Austin and Mascarenas, David and Kenyon, Garrett T.},
title = {Sparse Coding Enables the Reconstruction of High-Fidelity Images and Video from Retinal Spike Trains},
year = {2018},
isbn = {9781450365444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229884.3229892},
doi = {10.1145/3229884.3229892},
abstract = {The optic nerve transmits visual information to the brain as trains of discrete events, a low-power, low-bandwidth communication channel also exploited by silicon retina cameras. Extracting high-fidelity visual input from retinal event trains is thus a key challenge for both computational neuroscience and neuromorphic engineering. Here, we investigate whether sparse coding can enable the reconstruction of high-fidelity images and video from retinal event trains. Our approach is analogous to compressive sensing, in which only a random subset of pixels are transmitted and the missing information is estimated via inference. We employed a variant of the Locally Competitive Algorithm to infer sparse representations from retinal event trains, using a dictionary of convolutional features optimized via stochastic gradient descent and trained in an unsupervised manner using a local Hebbian learning rule with momentum.We used an anatomically realistic retinal model with stochastic graded release from cones and bipolar cells to encode thumbnail images as spike trains arising from ON and OFF retinal ganglion cells. The spikes from each model ganglion cell were summed over a 32 msec time window, yielding a noisy rate-coded image. Analogous to how the primary visual cortex is postulated to infer features from noisy spike trains arising from the optic nerve, we inferred a higher-fidelity sparse reconstruction from the noisy rate-coded image using a convolutional dictionary trained on the original CIFAR10 database.To investigate whether a similar approach works on non-stochastic data, we demonstrate that the same procedure can be used to reconstruct high-frequency video from the asynchronous events arising from a silicon retina camera moving through a laboratory environment.},
booktitle = {Proceedings of the International Conference on Neuromorphic Systems},
articleno = {8},
numpages = {5},
keywords = {Event Train, Sparse Coding, Retinal Model, Spike Train, Silicon Retina, Compressive Sensing},
location = {Knoxville, TN, USA},
series = {ICONS '18}
}

@InProceedings{ brendt_wohlberg-proc-scipy-2017,
  author    = { {B}rendt {W}ohlberg },
  title     = { {S}{P}{O}{R}{C}{O}: {A} {P}ython package for standard and convolutional sparse representations },
  booktitle = { {P}roceedings of the 16th {P}ython in {S}cience {C}onference },
  pages     = { 1 - 8 },
  year      = { 2017 },
  editor    = { {K}aty {H}uff and {D}avid {L}ippa and {D}illon {N}iederhut and {M} {P}acer },
  doi       = { 10.25080/shinma-7f4c6e7-001 }
}


@InProceedings{TetiKMM22,
  title = 	 {{LCAN}ets: Lateral Competition Improves Robustness Against Corruption and Attack},
  author =       {Teti, Michael and Kenyon, Garrett and Migliori, Ben and Moore, Juston},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {21232--21252},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  address = {Baltimore, MD, USA},
  pdf = 	 {https://proceedings.mlr.press/v162/teti22a/teti22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/teti22a.html},
  abstract = 	 {Although Convolutional Neural Networks (CNNs) achieve high accuracy on image recognition tasks, they lack robustness against realistic corruptions and fail catastrophically when deliberately attacked. Previous CNNs with representations similar to primary visual cortex (V1) were more robust to adversarial attacks on images than current adversarial defense techniques, but they required training on large-scale neural recordings or handcrafting neuroscientific models. Motivated by evidence that neural activity in V1 is sparse, we develop a class of hybrid CNNs, called LCANets, which feature a frontend that performs sparse coding via local lateral competition. We demonstrate that LCANets achieve competitive clean accuracy to standard CNNs on action and image recognition tasks and significantly greater accuracy under various image corruptions. We also perform the first adversarial attacks with full knowledge of a sparse coding CNN layer by attacking LCANets with white-box and black-box attacks, and we show that, contrary to previous hypotheses, sparse coding layers are not very robust to white-box attacks. Finally, we propose a way to use sparse coding layers as a plug-and-play robust frontend by showing that they significantly increase the robustness of adversarially-trained CNNs over corruptions and attacks.}
}

@article{Carroll_denoising,
  author       = {Jacob Carroll and
                  Nils Carlson and
                  Garrett T. Kenyon},
  title        = {Phase Transitions in Image Denoising via Sparsely Coding Convolutional
                  Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1710.09875},
  year         = {2017},
  url          = {http://arxiv.org/abs/1710.09875},
  eprinttype    = {arXiv},
  eprint       = {1710.09875},
  timestamp    = {Mon, 13 Aug 2018 16:47:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1710-09875.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  numpages     = {4},
}
