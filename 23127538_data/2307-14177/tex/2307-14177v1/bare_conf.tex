
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/

\usepackage{graphicx} 
\usepackage{subcaption}
\usepackage{threeparttable}

% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.


\section{Introduction}
\label{sec:intro}

%stare
%Współczesny świat charakteryzuje się bardzo dużą dynamiką.
%Spostrzeżenie to dotyczy nie tylko ludzi, ale także pojazdów autonomicznych, od których oczekuje się maksymalnie szybkiego, poprawnego i bezpiecznego działania.
%Szczególnie ważne jest to w przypadku bezzałogowych statków powietrznych, zwanych popularnie dronami, które w ostatnich latach znajdują zastosowanie w wielu dziedzinach życia, w tym %chociażby do transportu przesyłek, inspekcji sieci przesyłowych czy pomocy w misjach ratunkowych \cite{Shakhatreh2019}.
%Aby umożliwić większą niezależność od operatora drona i jego umiejętności, coraz częściej dokonuje się autonomizacji części wykonywanych przez drona zadań.
%Można tu wymienić przykładowo detekcję obiektów \cite{Wan2021} lub ich śledzenie, wyznaczanie przepływu optycznego \cite{Liu2017}, czy detekcję i dopasowanie punktów charakterystycznych %\cite{Clady2015}.
%stare
%Sprawne wykonywanie zadań, w tym przykładowo przelot przez dynamiczne środowisko z przeszkodami, wymaga szybkiej akwizycji i przetwarzania danych w celu ominięcia występujących na trajektorii lotu przeszkód.
%Do akwizycji danych najczęściej stosuje się zestaw czujników, takich jak kamery, czujniki inercyjne (IMU –- Inertial Measurement Unit), ultradźwiękowe czy pomiary z systemu GPS.
%Jednak w przypadku dynamicznego ruchu drona lub obiektów w jego otoczeniu są one niewystarczające.

Event cameras (DVS -- Dynamic Vision Sensors) are state-of-the-art vision sensors inspired by biology.
They only detect changes in brightness in an image and can operate with microsecond resolution.
For this reason, they are able to register changes in the environment very quickly, i.e. they have low latency.
In addition, they operate correctly under difficult lighting conditions, e.g. limited brightness or high dynamic range.
The aforementioned properties make DVS an interesting sensor for currently developing autonomous vehicles, including drones, where both speed of operation and robustness to a~variety of lighting conditions are important.

However, processing event data, which is in the form of a~sparse spatio-temporal cloud, is challenging.
Several approaches are encountered in the literature: from the direct analysis of the point cloud, to the projection of events onto a~plane (representations) and reconstruction \cite{Rebecq2021}.
Particularly popular is the use of a~representation in the form of a~so-called event frames, i.e. events collected over a~certain time interval, projected onto the image plane. 
This allows the use of typical, well-known vision algorithms -- both classical and those based on deep learning.

In order to process event data quickly enough, a~powerful hardware platform is needed on which relevant information can be extracted in real-time, e.g. the position and direction of potential objects in the drone's environment.
This prerequisite is met by SoC FPGA (System-on-Chip Field Programmable Gate Array) or embedded GPU (embedded Graphics Processing Unit) platforms, which enable parallel computing and, in addition, are small and lightweight and therefore well suited to be placed on a~drone.

In this paper, we address the \replaced{subject}{topic} of event frame generation in SoC FPGAs for data in HD (High Definition -- $1280 \times 720$ pixels) resolution or higher.
To the best of our knowledge, this issue has not yet been considered in the scientific literature for such high resolutions (Section \ref{ssec:prev_work}) and it presents many challenges, both scientific (hardware architecture development) and technical (implementation).

The remainder of this paper is organised as follows.
Section \ref{sec:dvs} provides background information on event cameras and the approaches available in the scientific literature to the problem \replaced{considered}{under consideration}.
Section \ref{sec:repr} is devoted to the presentation and comparison of different event data representations.
Details of the hardware implementation of the algorithm, together with its potential enhancements, are collected in Section \ref{sec:impl}.
The final Section \ref{sec:concl} summarises the work done and indicates further directions for development.

%PL
% Kamery zdarzeniowe (DVS -– Dynamic Vision Sensors) to nowoczesne czujniki wizyjne inspirowane biologicznie.
% Wykrywają one jedynie zmiany jasności na obrazie i potrafią działać z mikrosekundową rozdzielczością.
% Z tego powodu są w stanie odpowiednio szybko rejestrować zmiany występujące w otoczeniu, tj. charakteryzują się niską latencją.
% Ponadto działają poprawnie w trudnych warunkach oświetleniowych, np. ograniczonej jasności czy dużej rozpiętości tonalnej.
% Wymienione wyżej właściwości czynią DVS interesującym czujnikiem dla obecnie rozwijających się pojazdów autonomicznych, w tym dronów, gdzie znaczenie mają zarówno szybkość działania, jak i odporność na różnorodne warunki oświetleniowe.

% Jednakże przetwarzanie danych zdarzeniowych, które mają postać rzadkiej przestrzenno-czasowej chmury, stanowi duże wyzwanie.
% W literaturze spotyka się kilka podejść: od bezpośredniej analizy chmury punktów, poprzez rzutowanie zdarzeń na płaszczyznę (reprezentacje), po rekonstrukcję \cite{Rebecq2021}.
% Szczególnie popularne jest wykorzystanie reprezentacji w postaci tzw. ramki zdarzeniowej, czyli zebranych w pewnym przedziale czasowym zdarzeń, zrzutowanych na płaszczyznę obrazu. 
% Pozawala to na stosowanie typowych, dobrze znanych algorytmów wizyjnych -- zarówno klasycznych, jak i wykorzystujących uczenie głębokie.

% Aby odpowiednio szybko przetwarzać dane zdarzeniowe, potrzebna jest wydajna platforma sprzętowa, na której można w czasie rzeczywistym wydobyć istotne informacje, np. o położeniu i kierunku ruchu potencjalnych obiektów w otoczeniu drona.
% Warunek ten spełniają platformy SoC FPGA (System-on-Chip Field Programmable Gate Array) czy wbudowane GPU (embedded Graphics Processing Unit), które umożliwiają prowadzenie obliczeń w sposób równoległy oraz dodatkowo są małe i lekkie, dlatego dobrze nadają się do umieszczenia ich na dronie.

% W niniejszej pracy podejmujemy temat generacji ramek zdarzeniowych w układach SoC FPGA dla danych w rozdzielczości HD (High Definition –- $1280 \times 720$ pikseli) lub wyższej.
% Zgodnie z naszą wiedzą, zagadnienie to nie było dotąd rozważane w literaturze naukowej dla tak dużych rozdzielczości (Rozdział \ref{ssec:prev_work}), zatem stanowi ono wyzwanie zarówno naukowe (opracowanie architektury sprzętowej), jak i techniczne (implementacja).

% The remainder of this paper is organised as follows.
% W Rozdziale \ref{sec:dvs} zamieszczono podstawowe informacje na temat kamer zdarzeniowych oraz dostępnych w literaturze naukowej podejść do rozważanego problemu.
% Rozdział \ref{sec:repr} poświęcony został przedstawieniu i porównaniu różnych reprezentacji danych zdarzeniowych.
% Szczegóły implementacji sprzętowej algorytmu wraz z jego potencjalnymi rozszerzeniami zostały zebrane w Rozdziale \ref{sec:impl}.
% Rozdział ostatni \ref{sec:concl} zawiera podsumowanie wykonanych prac i wskazanie dalszych kierunków rozwoju.

%stare
%W literaturze naukowej powstało już kilka prac, w których tematyka ta została podjęta bądź moduł generacji ramki był komponentem większego systemu (Rozdział \ref{ssec:prev_work}).
%Chęć zastosowania takiego algorytmu na platformie sprzętowej zamontowanej na autonomicznym dronie rodzi szereg trudności i wyzwań, jednak pozwala na bardzo wydajną energetycznie implementację.
%Therefore, main contribution is opracowanie architektury sprzętowej na platformę SoC FPGA algorytmu do generacji ramek zdarzeniowych w rozdzielczości HD.
%stare
%Therefore, main contribution of our work can be summarised in following points:
%\begin{itemize}
%    \item opracowanie architektury sprzętowej na platformę SoC FPGA algorytmu do generacji ramek zdarzeniowych w rozdzielczości HD,
%    \item przedstawienie metody rolling window do zastosowania na danych zdarzeniowych,
%    \item przedstawienie szeregu usprawnień implementacyjnych algorytmu na platformie FPGA.
%\end{itemize}

% ---------------------------------------------------------------------------------------------------------------------------------------------
\section{Event-frame generation}
\label{sec:dvs}

\subsection{Event camera}
\label{ssec:camera}

Dynamic Vision Sensors differ from traditional frame cameras in both design and operation.
Firstly, they only record \added{brightness} changes \deleted{in brightness in the observed scene,} independently (asynchronously) for each pixel. 
Each such change is called an event and consists of four pieces of information: the exact time of occurrence (timestamp), the $x$ and $y$ positions in the image, and the polarity, i.e. information about the increase or decrease in brightness (positive and negative polarity respectively), \replaced{which}{ -- this} can be written as $e = \{t,x,y,p\}$.

When monitoring a~static scene, event cameras do not transmit any data (apart from noise), which significantly reduces data transfer and energy requirements. 
Secondly, changes are recorded at a~very high rate -- more than a~thousand times per second, whereas frame cameras typically operate at \replaced{lower frequencies}{around 60 fps (frames per second)} \added{(e.g. 30, 60, 120 or up to 240 frames per second)}.
Another difference resulting from this is the significant reduction in blur effect associated with object movement on event data, which enables correct registration of a~highly dynamic scene. 
A~fourth advantage over traditional cameras is the high dynamic range, allowing very bright and very dark areas to be recorded in a~single image, which is usually not possible with frame cameras.

Event cameras are used, among others, in unmanned aerial vehicles \cite{Vidal2018}, for increasing the frequency of video sequences \cite{Pan2019}, for traffic sign detection \cite{Wzorek2022}, or for fast objects counting \cite{Bialik2023}.
A~comprehensive overview of event camera applications can be found in the article \cite{Gallego2022}. 

%PL
% %Część z opisanych we wstępie problemów można rozwiązać, stosując nowoczesne czujniki wizyjne, jakimi są Dynamic Vision Sensors (DVS), zwane także kamerami zdarzeniowymi. 
% %Ich działanie różni się od ich tradycyjnych odpowiedników kilkoma cechami. 

% Kamery zdarzeniowe Dynamic Vision Sensors (DVS) różnią się od tradycyjnych kamer klatkowych zarówno budową, jak i sposobem działania.
% Po pierwsze, rejestrują tylko zmiany jasności na obserwowanej scenie, niezależnie (asynchronicznie) dla każdego piksela. 
% Każda taka zmiana nazywana jest zdarzeniem i składa się z czterech informacji: dokładnego czasu wystąpienia (timestamp), pozycji x i y na obrazie oraz polaryzacji, czyli informacji o wzroście lub spadku jasności (odpowiednio polaryzacja dodatnia i ujemna) -- można to zapisać jako (\ref{eq:event}).
% %Natomiast kamery klatkowe rejestrują bezpośrednio jasności wszystkich pikseli na obrazie w regularnych odstępach czasu.

% \begin{equation}
%     e = \{t,x,y,p\}
%     \label{eq:event}
% \end{equation}

% W przypadku monitorowania sceny statycznej, kamery zdarzeniowe nie przesyłają żadnych danych (pomijając szumy), co znacznie ogranicza transfer danych i zapotrzebowanie energetyczne. %, dlatego mogą być lepsze w niektórych systemach wbudowanych. 
% Po drugie, zmiany rejestrowane są z bardzo wysoką częstotliwością -– ponad tysiąc razy na sekundę, natomiast kamery klatkowe działają typowo z częstotliwością ok. 60 fps (frames per second). %, zatem niewątpliwą zaletą tych pierwszych jest dużo szybsze działanie i mniejsze opóźnienia między akwizycją a przetwarzaniem danych.
% Kolejna różnica z tego wynikająca to znaczna redukcja rozmycia związanego z ruchem obiektów na danych zdarzeniowych, co umożliwia poprawną rejestrację sceny o bardzo dużej dynamice. 
% Czwarta zaleta względem kamer tradycyjnych to szeroka rozpiętość tonalna, pozwalająca na rejestrację bardzo jasnych i bardzo ciemnych obszarów na jednym obrazie, co w przypadku kamer klatkowych zwykle nie jest możliwe.
% %W ostatnich latach zauważalny jest duży wzrost zainteresowania opisanymi czujnikami, stąd też coraz częściej pojawiają się rozwiązania i aplikacje, w których są one stosowane.

% Kamery zdarzeniowe znajdują zastosowanie m.in. w bezzałogowych statkach powietrznych \cite{Vidal2018}, do zwiększania częstotliwości sekwencji wideo \cite{Pan2019}, do detekcji znaków drogowych \cite{Wzorek2022}, czy do zliczania szybkich obiektów \cite{Bialik2023}.
% Obszerny przegląd zastosowań kamer zdarzeniowych może znaleźć w artykule \cite{Gallego2022}. 
% % Działanie obu rodzajów kamer zostało porównane na rys. \ref{fig:disk}, pochodzącym z pracy \cite{Rebecq2021}.

% % % Figure environment removed


\subsection{Previous works}
\label{ssec:prev_work}

A~number of solutions can be found in the scientific literature where frames were generated to perform certain tasks based on event data.
The paper \cite{Wzorek2022} realised traffic sign detection using neural networks and proposed a~multi-channel approach (different \added{simple} representations on three image channels).
The work \cite{Chen2018} proposed a~system for object detection using neural networks and a~representation based on the frequency of event occurrence.
The paper \cite{Wan2021} presented a~system for pedestrian detection using a~neural network and a~representation called neighbourhood suppression time surface.
For the gesture recognition task, a~temporal binary representation method was proposed in \cite{Innocenti2021}, where a~sequence of 8 binary frames was generated, aggregated to a~single greyscale image.
The only work where frame generation was done for HD data is \cite{Bialik2023}, which proposed a~system to control the flow of elements in time (falling corn grains).

One of a~few similar hardware implementations on an FPGA platform is \cite{Linares2021}, whose authors performed object classification using a~NullHop accelerator based on a~convolutional neural network.
In this work, frames were generated for a~specific number of events instead of a~time interval.
Two memory blocks for data of only $64 \times 64$ elements were used, so the authors did not encounter a~number of challenges in implementing the algorithm for high-resolution data.

The generation of frames without distinguishing events' polarity was part of the optical flow determination algorithm on an FPGA platform \cite{Liu2017}, in which data was accumulated in 3 blocks from consecutive time intervals.
In the project an event camera with a~resolution of $240 \times 180$ pixels was used.

The work \cite{Tapiador2020} developed a~gesture recognition system using an FPGA platform and a~hierarchy of time surfaces representation, inspired by an approach from multi-layer neural networks, and a~single-layer linear decay kernel representation, i.e. linear decay of events in time.
Each single event was specified using the time context concept, a~quadratic matrix of timestamp differences in a~small neighbourhood.
In the project two event sensors (with $304 \times 240$ and $128 \times 128$ pixels resolutions) were used, but for the purposes of the algorithm, the data was stored in an accumulator scaled to $128 \times 128$.

In summary, in a~number of research papers \deleted{proposed} projecting event data onto the image plane using different representations \added{was proposed}, and further processing of the frames was \replaced{rather}{usually} performed using neural networks \added{than classical methods}. 
\replaced{In some of these papers, an FPGA platform was used for the implementation of the frame generation algorithm}{Single papers have also been devoted to the implementation of the algorithm on an FPGA platform}, but for very low resolution data.
\deleted{To our knowledge, the problem of frame generation for high-resolution (e.g. HD) event data on an FPGA platform in real-time has not yet been addressed.}
The implementation of such an algorithm \added{for high-resolution data has not yet been addressed as it} raises \deleted{some} \added{several} problems, mainly related to the handling of incoming events and real-time frame generation, which at low resolutions -- due to much less data per time interval -- simply do not occur.
This paper therefore attempts to fill this gap, which becomes particularly relevant as the resolution of commercially available event cameras increases \added{-- from $128 \times 128$ pixels in 2008 up to $1280 \times 960$ in 2020 \cite{Gallego2022}}.

%PL
% W literaturze naukowej można znaleźć szereg rozwiązań, w których do realizacji pewnych zadań na podstawie danych zdarzeniowych generowane były ramki.
% W pracy \cite{Chen2018} zaproponowano system do detekcji obiektów z wykorzystaniem sieci neuronowych i reprezentacji bazującej na częstotliwości pojawiania się zdarzeń.
% W artykule \cite{Wzorek2022} zrealizowano detekcję znaków drogowych przy pomocy sieci i zaproponowano podejście wielokanałowe (różne reprezentacje na 3 kanałach obrazu).
% W pracy \cite{Wan2021} przedstawiony został system do detekcji pieszych z wykorzystaniem sieci i reprezentacji nazwanej neighbourhood suppression time surface.
% Do zadania rozpoznawania gestów w ramach pracy \cite{Innocenti2021} zaproponowana została metoda temporal binary representation, w której generowany jest ciąg 8 ramek binarnych, agregowanych do jednej w greyscale.
% Jedyną pracą, gdzie generacja ramek odbywała się dla danych w rozdzielczości HD, jest \cite{Bialik2023}, w której zaproponowano system do kontroli przepływu elementów w czasie (spadających corn grains).

% Jedną z nielicznych podobnych implementacji sprzętowych na platformie FPGA jest \cite{Linares2021}, której autorzy wykonali klasyfikację obiektów przy pomocy akceleratora NullHop, bazującego na konwolucyjnej sieci neuronowej.
% W tej pracy ramki generowane były dla określonej liczby zdarzeń zamiast dla przedziału czasowego.
% Wykorzystano dwa bloki pamięci na dane w rozmiarze zaledwie $64 \times 64$ elementy, więc autorzy nie zetknęli się z szeregiem wyzwań, jakie niesie ze sobą implementacja algorytmu dla danych w wysokiej rozdzielczości.

% Generacja ramek dla zdarzeń bez rozróżnienia ich polaryzacji była elementem algorytmu wyznaczania przepływu optycznego na platformie FPGA \cite{Liu2017}, w którym dane były akumulowane w 3 blokach z kolejnych przedziałów czasu.
% W projekcie wykorzystana została kamera zdarzeniowa o rozdzielczości $240 \times 180$ pikseli.

% W ramach pracy \cite{Tapiador2020} opracowany został system do rozpoznawania gestów z wykorzystaniem platformy FPGA i reprezentacji hierarchy of time surfaces, zainspirowaną podejściem znanym z multi-layer sieci neuronowych, a także reprezentacji linear decay kernel w pojedynczej warstwie, czyli liniowego zanikania zdarzeń wraz z upływem czasu.
% Każde ze zdarzeń było natomiast określone przy użyciu time context concept, czyli kwadratowej macierzy różnic znaczników czasowych w niewielkim otoczeniu.
% W projekcie wykorzystane zostały dwa czujniki zdarzeniowe o rozdzielczościach $304 \times 240$ oraz $128 \times 128$ pikseli, jednak na potrzeby algorytmu przechowywano w akumulatorze dane przeskalowane do rozmiaru $128 \times 128$.

% Podsumowując, w wielu pracach naukowych proponowane było rzutowanie danych zdarzeniowych na płaszczyznę obrazu przy użyciu różnych reprezentacji, a dalsze przetwarzanie ramek zwykle było wykonywane przy użyciu sieci neuronowych. 
% Pojedyncze prace zostały także poświęcone implementacji algorytmu na platformie FPGA, jednak dla danych o bardzo niskich rozdzielczościach.

% Zgodnie z wiedzą autorów niniejszego artykułu, problem generacji ramki dla danych zdarzeniowych w wysokiej rozdzielczości (np. HD) na platformie FPGA w czasie rzeczywistym nie został jeszcze zaadresowany.
% Implementacja takiego algorytmu rodzi pewne problemy, głównie związane z obsługą przychodzących zdarzeń i generacją ramek w czasie rzeczywistym, które w niskich rozdzielczościach -- z uwagi na wielokrotnie mniejszą ilość danych w jednostce czasu -- po prostu nie występują.
% Dlatego też niniejsza praca stanowi próbę wypełnienia tej luki, która staje się szczególnie istotna wraz ze wzrostem rozdzielczości dostępnych komercyjnie kamer zdarzeniowych.

% ----------------------------------------------------------------------------------------------------------------
\section{Event-frame representations}
\label{sec:repr}

There are several approaches to processing event data, e.g. `straightforward', which requires the development of new algorithms due to the different data format.
Therefore, a~more `natural' approach is the generation of so-called event frames, which consist of events from certain time intervals projected onto a~plane, resulting in pseudo-images similar to these of traditional frame cameras, to which well-known algorithms can be applied.
However, the frame generation poses a~number of challenges: selecting the event accumulation period, taking into account time information when projecting onto the plane or ensuring real-time frame generation and further processing.

To date, several basic data representations for event frame generation have been proposed in the literature, with additional modifications for specific applications.
%od PW with modifications
According to \cite{Clady2015}, a~function $\Sigma_{e}$ can be defined that assigns the time of occurrence of an event $t$ to each coordinate pair $\textbf{u}(x,y)$, and a~function $P_{e}$ that assigns a~polarity from the set $\{1, -1\}$ or, using another convention, $\{1, 0\}$.
%In the case of the latter function, another convention is also encountered in the literature, in which a value from the set ${1, 0}$ is assigned.
% \begin{equation}
% \label{eq:sigma}
% \textbf{u}: t = \Sigma_{e}(\textbf{u}), \Sigma_{e}: R^{2} \rightarrow R
% \end{equation}
% \begin{equation}
% \label{eq:pe}
% \textbf{u}: p = P_{e}(\textbf{u}), P_{e}: R^{2} \rightarrow \{1, -1\}
% \end{equation}
In addition, a~parameter $\tau$ can be defined, indicating the event accumulation time for the generation of one frame.
The representations listed in this section are defined (for simplicity) for times $t \in (0, \tau)$.

% %PL
% Istnieją różne podejścia do przetwarzania danych zdarzeniowych, np. przetwarzanie ich "wprost", co wymaga opracowania nowych algorytmów z uwagi na inny format danych.
% Dlatego też bardziej „naturalnym” podejściem jest generacja tzw. ramki zdarzeniowej, polegająca na zrzutowaniu zdarzeń z pewnego przedziału czasowego na płaszczyznę, dzięki czemu otrzymuje się pseudo-obraz podobny do tego z tradycyjnych kamer klatkowych, do którego można zastosować dobrze znane algorytmy.
% Jednak sama generacja ramki powoduje szereg wyzwań: dobór okresu akumulacji zdarzeń, uwzględnienie informacji o czasie przy rzutowaniu na płaszczyznę czy zapewnienie generacji i dalszego przetwarzania ramek w czasie rzeczywistym.

% Do tej pory w literaturze naukowej zaproponowano kilka podstawowych reprezentacji danych do generacji ramki zdarzeniowej wraz z dodatkowymi modyfikacjami pod konkretne aplikacje.
% %od PW z modyfikacjami
% Zgodnie z pracą \cite{Clady2015}, można zdefiniować funkcję $\Sigma_{e}$, która każdej parze współrzędnych $\textbf{u}(x,y)$ przypisuje czas wystąpienia zdarzenia $t$, jak we wzorze (\ref{eq:sigma}), a także funkcję $P_{e}$, która parze współrzędnych przypisuje polaryzację, zgodnie z (\ref{eq:pe}).
% W przypadku drugiej z wymienionych funkcji, spotyka się też w literaturze inną konwencję, w której przypisywana jest wartość ze zbioru ${1, 0}$.

% \begin{equation}
% \label{eq:sigma}
% \textbf{u}: t = \Sigma_{e}(\textbf{u}), \Sigma_{e}: R^{2} \rightarrow R
% \end{equation}

% \begin{equation}
% \label{eq:pe}
% \textbf{u}: p = P_{e}(\textbf{u}), P_{e}: R^{2} \rightarrow \{1, -1\}
% \end{equation}

% Dodatkowo można zdefiniować parametr $\tau$, oznaczający czas akumulacji zdarzeń do generacji jednej ramki.
% Wymienione w dalszej części rozdziału reprezentacje określone zostały (dla uproszczenia) dla czasów $t \in (0, \tau)$.


\subsection{Binary frame}
\label{ssec:binary}
The first representation -- for \deleted{reasons of} maximum simplicity -- can be a~binary information about the occurrence of an event in given coordinates, as in \cite{Liu2017}: $f(\textbf{u},t) = 1$.
A~binary frame can also be generated from events of only one polarity. %, e.g. positive.

%PL
% Pierwszym z rodzajów reprezentacji -- z uwagi na maksymalną prostotę -- może być informacja binarna o wystąpieniu zdarzenia dla danych współrzędnych, tak jak w pracy \cite{Liu2017}, zgodnie ze wzorem (\ref{eq:event_bin}).
% Pewnym rodzajem tej reprezentacji może być ramka wygenerowana na podstawie zdarzeń o tylko jednej polaryzacji, np. dodatniej.

% \begin{equation}
% \label{eq:event_bin}
% f(\textbf{u},t) = 1 %\textrm{ for } P_{e}(\textbf{u}) \in \{1, -1\}
% %\left\{ \begin{array}{lr} 255 & \textrm{ for }  P_{e}(\textbf{u}) \in \{1, -1\} \\ 0 & \textrm{ otherwise } \end{array}\right.
% %255 \textrm{ for } P_{e}(\textbf{u}) \in \{1, -1\} %\left\{ \begin{array}{lr} 1 & for \ t-\Sigma_{e}(\textbf{u})\in(0;\tau) \\ 0 & for \ t-\Sigma_{e}(\textbf{u})\in(\tau;+\infty) \end{array}\right.
% \end{equation}


\subsection{Event frame}
\label{ssec:event_frame}
The second approach is a~slight modification of the previous one, as the polarity of events is taken into account, according to the work \cite{Afshar2020}: $f(\textbf{u},t) = P_{e}(\textbf{u})$.
This is one of the most popular representations due to its simplicity, but also the considerable amount of information it contains.

%PL
% Drugie podejście jest niewielką modyfikacją poprzedniego, gdyż uwzględniana jest polaryzacja zdarzeń, zgodnie z pracą \cite{Afshar2020}, co przedstawione zostało w równaniu (\ref{eq:event_frame}).
% Jest to jedna z najbardziej popularnych reprezentacji z uwagi na prostotę, ale także zawartą w sobie sporą ilość informacji.

% \begin{equation}
% \label{eq:event_frame}
% f(\textbf{u},t) = 
% %\left\{ \begin{array}{lr} 255 & \textrm{ for }  P_{e}(\textbf{u}) = 1 \\ 0 & \textrm{ for }  P_{e}(\textbf{u}) = -1 \\ 128 & \textrm{ otherwise } \end{array}\right.
% P_{e}(\textbf{u}) %\textrm{ for } P_{e}(\textbf{u}) \in \{1, -1\}
% \end{equation}


\subsection{Exponentially decaying time surface}
\label{ssec:exp_decaying_ts}
In this representation the temporal information is taken into account, precisely by specifying the time from the event occurrence to the end of the accumulation period, as in \cite{Afshar2020}.
This value is then used for determining the brightness of a~pixel in the resulting image: $f(\textbf{u},t) = P_{e}(\textbf{u}) \cdot e^{\frac{\Sigma_{e}(u)-t}{\tau}}$.
This is motivated by the observation that the weight of information contributed by an event decreases with time to 0.

%PL
% W kolejnej reprezentacji uwzględnia się już informację czasową, dokładnie poprzez określenie czasu od wystąpienia danego zdarzenia od końca okresu akumulacji, tak jak w pracy \cite{Afshar2020}.
% Wartość ta jest następnie używana podczas określania jasności piksela na obrazie wynikowym, jak w (\ref{eq:decay}).
% Jest to umotywowane spostrzeżeniem, że waga informacji wnoszonej przez dane zdarzenie maleje wraz z upływem czasu do 0.

% \begin{equation}
% \label{eq:decay}
% f(\textbf{u},t) = P_{e}(\textbf{u}) \cdot e^{\frac{\Sigma_{e}(u)-t}{\tau}} %\left\{ \begin{array}{lr} P_{e}(\textbf{u})*e^{\frac{\Sigma_{e}(\textbf{u})-t}{\tau}} & for \ \Sigma_{e}(\textbf{u}) \leq t \\ 0 & for \ \Sigma_{e}(\textbf{u}) > t \end{array}\right.
% \end{equation}


\subsection{Event frequency}
\label{ssec:event_freq}
The next approach uses information about the frequency of a~given pixel in the image, as proposed in \cite{Chen2018}: $f(x) = \frac{255}{1+e^{-x/2}}$, \added{where} the sum of polarisations for a~pixel is denoted as $x$.

%PL
% W następnym podejściu wykorzystywana jest informacja o częstotliwości występowania danego piksela na obrazie, zgodnie z wyrażeniem (\ref{eq:event_freq}), co zaproponowano w pracy \cite{Chen2018}.
% Przez $x$ oznaczona została suma polaryzacji dla danego piksela.

% \begin{equation}
% \label{eq:event_freq}
% f(x) = 255 \cdot \frac{1}{1+e^{-x/2}}
% \end{equation}

\subsection{Comparison}
\label{ssec:comp}
A~comparison of generated event frames using the representations described \deleted{for an example sequence} is shown in Figure \ref{fig:repr}.
\added{An exemplary sequence shows a~fast-moving ball flying through a~room recorded by a~moving event camera.}
Apart from aforementioned representations, more complex arithmetic operations can be proposed to determine the values of individual pixels, as \replaced{mentioned}{discussed} in Section \ref{ssec:prev_work}.
There are also solutions in which an image with multiple channels of particular representations \cite{Wzorek2022} \added{or aggregated into one-channel \cite{Innocenti2021}} is generated. %, thus exploiting the \added{additional} advantages. \deleted{ of several of them simultaneously.}

%PL
% Porównanie wygenerowanych ramek zdarzeniowych przy wykorzystaniu opisanych reprezentacji dla przykładowej sekwencji przedstawione zostało na rys. \ref{fig:repr}.
% Oprócz wymienionych reprezentacji, można zaproponować bardziej skomplikowane operacje arytmetyczne do wyznaczenia wartości poszczególnych pikseli, co omówiono w Rozdziale \ref{ssec:prev_work}.
% Istnieją także rozwiązania, w których generowany jest obraz o wielu kanałach, a poszczególne jego składowe stanowią wybrane reprezentacje \cite{Wzorek2022}, co pozwala wykorzystać zalety kilku z nich jednocześnie.

% Figure environment removed


\section{The proposed frame generation module}
\label{sec:impl}
%TODO Tu coś w stylu: W ramach niniejszych prac opracowaliśmy kilka wersji modułu sprzętowego do genereacji ramek zdarzeniowych. Został on opisane w j. System Verilog i środowsku Vivado oraz przetestowany dla platformy FPGA firmy AMD Xilinx. 

In this \replaced{work}{project}, we have developed several versions of a~hardware module for event frame generation on an FPGA platform.
Their architectures were prepared in SystemVerilog language using the Vivado environment and tested for AMD Xilinx's FPGA platforms.
The input of each \added{module} was an event $e=\{t,x,y,p\}$ and the output was an 8-bit pixel brightness value.
For testing purposes, sample sequences \added{with a~thrown ball} were recorded with a~\added{moving (rotating)} Prophesee EVK1 camera in HD resolution\deleted{ ($1280 \times 720$ pixels)}.
The events were saved to a~text file, from which they were read in the Vivado simulation.

%PL
% W ramach niniejszego projektu opracowaliśmy kilka wersji sprzętowego modułu do generacji ramek zdarzeniowych na platformie FPGA.
% Architektury tych modułów opisane zostały w języku SystemVerilog i środowisku Vivado oraz przetestowane dla platform FPGA firmy AMD Xilinx.
% %w języku SystemVerilog i środowisku Vivado opracowany został moduł do generacji ramek zdarzeniowych na platformie FPGA.
% Wejście każdego z nich stanowiły zdarzenia $e={t,x,y,p}$, a wyjściem były wartości (jasności) piksela zapisane na 8 bitach.
% Na potrzeby testów nagrano przykładowe sekwencje kamerą Prophesee EVK1 w rozdzielczości HD ($1280 \times 720$ pikseli).
% Zarejestrowane zdarzenia zostały zapisane do pliku tekstowego, z którego były odczytywane w symulacji w Vivado.


\subsection{Basic version}
\label{ssec:basic}
The architecture of the basic version of the algorithm consists of three elements: a~memory for storing accumulated events, logic controlling the reading and writing of data, and a~temporary buffer for incoming data during frame reading.

The first of these elements can be realised in two ways -- using the FPGA chip's internal block memory (BRAM) or external RAM (usually dynamic).
In the first case, the memory resources are quite limited, so the appropriate representation and resolution of the image must be selected.
In the second case, the resources are much larger, but this approach increases the complexity of the architecture, introduces additional latency in data processing and is more energy consuming.
Therefore, the first solution is used in this work due to its greater simplicity and operational efficiency.
The number of elements in memory was set to 921600 (the number of pixels in an HD image)\deleted{ ($1280 \times 720$)}.
The number of bits \replaced{allocated for}{dedicated to} one element depends on the chosen representation, but in the simplest case just 1 bit can be assumed (Section \ref{ssec:binary}).
Other variants are described as algorithm extensions in Section \ref{sssec:other_repr}.

The second element is the logic that controls the communication with the two-port block memory, of which one port is used for writing data and the other for reading.
As long as events belonging to a~certain time interval $\tau$ (e.g. 10 ms) occur at the input, the algorithm operates in write mode.
Based on the coordinates of the incoming event, the address in memory is determined as: $address = address\_Y \cdot image\_width + address\_X$, while the value stored equals 1.

When the condition to stop the write mode is met (e.g. $\tau$ value exceeded), the algorithm enters read mode, resetting the pixel position counter in the image.
Its value is given as the address of the memory cell from under which the content of the accumulator is read before being incremented by 1.
The value read is further decoded to the range 0-255 to display the output event frame in greyscale.
At the same time, the address used for reading data is delayed by one clock cycle and fed to the input of the second port used for writing, together with the value 0, to reset the memory cell.
When the counter reaches the last pixel in the image, the algorithm returns to write mode.
A~schematic visualisation of the operation of the algorithm in write and read mode is shown in Figure \ref{fig:algo_basic_full}.

% Figure environment removed

Due to event camera properties, just after entering the read mode, new events may appear at the input of the module, \replaced{which should}{ to} be used for the generation of the next frame.
So the last necessary element is to handle these events.
In order to separate the `older' events (in the accumulator) from the `newer' ones (at the input), a~temporary buffer must be prepared to store them until the read mode \replaced{finishes}{completed}.
It was realised as a~FIFO (First In First Out) queue using the FPGA's block memory, in which all event components were stored.

Regardless of the operation mode, each event was firstly written to the FIFO.
Once in write mode, data was read from the queue and, in the meantime, the latest events from the input were written to the FIFO.
A~difficult issue is to determine the maximum size of the queue in order to store all incoming events but with low memory usage, as it depends on the dynamics of the scene and the $\tau$ value.
In our solution, the queue size was set to 32768 elements.
In addition, a~mechanism of removing `the oldest' events from the queue to add `the newest' was implemented, if the FIFO was full in read mode.
This was motivated by the fact that `newer' events are more important, as they represent the most recent changes.

%PL
% Architektura podstawowej wersji algorytmu generacji ramek zdarzeniowych składa się z trzech elementów: pamięci do przechowywania akumulowanych zdarzeń, logiki sterującej odczytem i zapisem danych oraz bufora tymczasowego na dane przychodzące w czasie odczytu ramki.

% Pierwszy z tych elementów może być zrealizowany na dwa sposoby -- korzystając z wewnętrznej pamięci blokowej układu FPGA (BRAM) lub zewnętrznej pamięci RAM.
% W pierwszym przypadku zasoby pamięciowe są dość mocno ograniczone, więc przy generacji należy dobrać odpowiednią reprezentację i rozdzielczość obrazu.
% W drugim przypadku zasoby te są dużo większe, jednak to podejście powoduje wzrost złożoności architektury oraz wprowadza dodatkowe opóźnienia w przetwarzaniu danych.
% Dlatego w niniejszej pracy zastosowano pierwsze rozwiązanie z uwagi na większą prostotę i wydajność działania.
% Liczbę elementów w pamięci ustalono na 921600, gdyż tyle pikseli jest na obrazie w rozdzielczości HD ($1280 \times 720$).
% Liczba bitów dedykowana jednemu elementowi jest uzależniona od wybranej reprezentacji, ale w przypadku najprostszym można przyjąć wartość 1-bitową (Rozdział \ref{ssec:binary}).
% Pozostałe warianty opisane są w ramach rozszerzeń algorytmu w Rozdziale \ref{sssec:other_repr}.

% Drugi element to logika sterująca komunikacją z wybraną w tym rozwiązaniu dwuportową pamięcią blokową, w której jeden port służy do zapisu danych, a drugi do odczytu.
% Dopóki na wejściu pojawiają się zdarzenia należące do określonego przedziału czasowego $\tau$ (np. 10 ms), algorytm działa w trybie zapisu.
% Na podstawie współrzędnych przychodzącego zdarzenia określany jest adres w pamięci jako: $address\_Y \cdot image\_width + address\_X$, natomiast zapisywaną wartość stanowi liczba 1.

% Gdy spełniony zostanie warunek na koniec zapisywania (np. przekroczona wartość $\tau$), algorytm przechodzi w tryb odczytu, zerując licznik pozycji piksela na obrazie.
% Wartość tego licznika jest podawana jako adres komórki pamięci, spod którego odczytywana jest zawartość akumulatora, a następnie inkrementowana o 1.
% Odczytana wartość jest dalej dekodowana do przedziału 0-255, aby umożliwić wyświetlenie wyjściowej ramki zdarzeniowej w greyscale.
% Jednocześnie adres służący do odczytu danych opóźniany jest o jeden takt zegara i podawany na wejście drugiego portu, służącego do zapisu, wraz z wartością 0, aby zresetować daną komórkę pamięci.
% %Takie zerowanie elementów jest bardziej uniwersalne od resetu całej pamięci -- szczegóły w Rozdziale \ref{sssec:rolling}.
% Po osiągnięciu przez licznik pozycji ostatniego piksela na obrazie, algorytm wraca do trybu zapisu.
% Schematycznie przedstawiony sposób działania opracowanego algorytmu w trybie zapisu oraz odczytu przedstawiony został na rys. \ref{fig:algo_basic_full}.
% %ach \ref{fig:algo_write} oraz \ref{fig:algo_read}.

% Z uwagi na specyfikę działania kamery zdarzeniowej, tuż po przejściu w tryb odczytu obrazu, na wejściu modułu mogą się pojawić nowe zdarzenia, które należy wykorzystać do generacji kolejnej ramki zdarzeniowej, dlatego też ostatnim niezbędnym elementem jest obsługa tych zdarzeń.
% Aby rozdzielić "starsze" (w akumulatorze) od "nowszych" (na wejściu modułu), należy przygotować bufor tymczasowy, w którym będzie można je przechować do czasu zakończenia trybu odczytu.
% Bufor zrealizowany został jako kolejka FIFO (First In First Out) przy wykorzystaniu pamięci blokowej układu FPGA, w której zapamiętywane były wszystkie składowe zdarzeń.

% Niezależnie od trybu działania algorytmu, każde zdarzenie było najpierw wpisywane do FIFO.
% Po przejściu w tryb zapisu, dane były odczytywane z kolejki, a w międzyczasie najnowsze zdarzenia z wejścia były zapisywane do FIFO.
% Trudną kwestią jest określenie maksymalnego rozmiaru kolejki, aby z jednej strony zapamiętać wszystkie przychodzące zdarzenia, a z drugiej oszczędzić zasoby pamięciowe, gdyż jest to uzależnione od dynamiki sceny i wartości $\tau$.
% W niniejszym rozwiązaniu ustalono rozmiar kolejki na 32768 elementy, a dodatkowo zaimplementowano mechanizm, w którym w trybie odczytu i przy pełnym FIFO, najstarsze zdarzenia z kolejki są usuwane, a najnowsze dodawane -- jest to umotywowane faktem, że "nowsze" zdarzenia są ważniejsze, gdyż to one reprezentują ostatnie zmiany na scenie.

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed


\subsection{Extensions}
\label{ssec:ext}
The architecture described in Section \ref{ssec:basic} is sufficient to correctly generate event frames in real-time on an FPGA platform.
However, a~number of enhancements and extensions can be applied to achieve better performance or results.

%PL
% Opisana w Rozdziale \ref{ssec:basic} architektura jest wystarczająca do poprawnego generowania ramek zdarzeniowych w czasie rzeczywistym na platformie FPGA.
% Można jednak zastosować szereg usprawnień i rozszerzeń, dzięki którym możliwe jest uzyskanie lepszych parametrów i wyników działania.


\subsubsection{Other representations}
\label{sssec:other_repr}
The first element is another representation of event data.
The simplest and most common modification is to add polarisation, creating an event frame.
In this case, an event with a~polarity of 1 generates an output pixel value of 255, the one with $-$1 polarity generates 0, while pixels without events receive a~value of 128.
To save hardware resources, a~2-bit value is stored in the accumulator, denoting \deleted{the number} 1 \added{(positive event)}, $-$1 \added{(negative event) or 0 (no event)}. \deleted{which is decoded to the range 0-255 after being read.}

For other representations described in Section \ref{sec:repr}, Look-Up-Table (LUT) can be used to store the approximate values of the exponential function.
In the case of the method in Section \ref{ssec:exp_decaying_ts}, knowing the value of $\tau$ (e.g. 10 ms), the difference between the current timestamp and the maximum value for the interval can be calculated.
The rounded result can be stored in an 8-bit accumulator, thus obtaining the output pixel value.

For the representation from Section \ref{ssec:event_freq}, it was necessary to determine the maximum number of events for the given coordinates, which depends on dynamics of the scene and $\tau$ value.
After tests on an example sequence, a~limitation to the interval $(-16, 15)$ turned out to be sufficient, as larger polarity sums had no apparent effect on the final pixel value.
Therefore, 5 bits were stored in the accumulator and then were decoded to an 8-bit output.
The polarity counting itself was done in a~way that with the arrival of a~new event with the given coordinates, the current value was read from the accumulator using one port of the block memory, after which, depending on the polarity, the number 1 was added or subtracted and then written back to the same address using the other port.

%PL
% Pierwszym z elementów są inne reprezentacje danych zdarzeniowych.
% Najprostszą, ale i najbardziej popularną modyfikacją, jest dodanie informacji o polaryzacji, tworząc event frame.
% W takim przypadku zdarzenie o polaryzacji 1 generuje wyjściową wartość piksela równą 255, o polaryzacji -1 generuje 0, natomiast pozostałe piksele otrzymują wartość 128.
% Aby jednak zaoszczędzić zasoby sprzętowe, w akumulatorze przechowywana jest wartość 2-bitowa, oznaczająca liczbę 1 lub -1, która przy odczycie zostaje przekodowana do zakresu 0-255.

% Dla innych reprezentacji opisanych w Rozdziale \ref{sec:repr} dobrze jest wykorzystać Look-Up-Table (LUT) do przechowania przybliżonych wartości funkcji eksponencjalnej.
% W przypadku metody z Rozdziału \ref{ssec:exp_decaying_ts}, znając wartość $\tau$ (np. 10 ms), można obliczyć różnicę między aktualnym znacznikiem czasowym a wartością maksymalną dla danego przedziału.
% Zaokrąglony wynik można zapisać w akumulatorze na 8 bitach, uzyskując tym samym wyjściową wartość piksela.

% Natomiast dla reprezentacji z Rozdziału \ref{ssec:event_freq} konieczne było określenie maksymalnej liczby zdarzeń dla danych współrzędnych przy monitorowaniu typowej sceny, co jednak zależy od jej dynamiki i wartości $\tau$.
% Na podstawie testów na przykładowej sekwencji ustalono, że wystarczające jest ograniczenie do przedziału $(-16, 15)$, gdyż większe sumy polaryzacji nie mają widocznego wpływu na końcową wartość piksela (zgodnie ze wzorem (\ref{eq:event_freq}).
% Dlatego wystarczy wartości te zapisać na 5 bitach w akumulatorze, a potem dokonać ich przekodowania na 8-bitowe wyjście.
% Natomiast samo zliczanie polaryzacji odbywa się w ten sposób, że wraz z nadejściem nowego zdarzenia o danych współrzędnych, odczytywana jest odpowiadająca wartość z akumulatora przy pomocy jednego portu pamięci blokowej, po czym w zależności od polaryzacji liczba 1 jest dodawana lub odejmowana, a następnie zapisywana pod ten sam adres przy użyciu drugiego z portów.


\subsubsection{Multiple Block RAMs}
\label{sssec:multiple_brams}

% Figure environment removed

It is possible to use a~set of smaller block memories, working in parallel, so that the reading of several pixels can be performed in the same clock cycle.
This approach is inspired by processing a~traditional video stream on an FPGA platform in a~vector format, the so-called Xppc (X pixels per clock cycle) as in \cite{Kowalczyk2018}. 
Apart from calculating the address based on the event coordinates, the horizontal index determines the memory block into which it is written, as in Figure \ref{fig:bram}.
In read mode, pixels with consecutive indices (from 0 onwards) are read from all X blocks simultaneously and fed to the module output in successive clock cycles.
With this approach, the output image is identical to that of a~single memory block, but it allows to reduce \added{X times} the latency of reading the image or \deleted{to reduce X times} the clock frequency and thus the energy consumed.
It is also possible to read multiple values from one block using memory properties in Vivado, but this solution has limited versatility in terms of possible X values and the way of resetting memory (all cells at once).

%PL
% Jednym z przetestowanych rozwiązań jest zastosowanie zestawu mniejszych pamięci blokowych, pracujących równolegle, dzięki czemu można wykonać odczyt kilku pikseli w tym samym takcie zegara.
% Podejście to jest zainspirowane przetwarzaniem tradycyjnego strumienia wideo na platformie FPGA w formacie wektorowym, tzw. Xppc (X pixels per clock cycle) jak w pracy \cite{Kowalczyk2018}. 
% Oprócz ustalenia adresu na podstawie współrzędnych zdarzenia, jego poziomy indeks określa dodatkowo blok pamięci, do którego ma zostać wpisany, tak jak w przykładzie na rys. \ref{fig:bram}.
% W trybie odczytu piksele o kolejnych indeksach (od 0) odczytywane są ze wszystkich X bloków równocześnie i podawane na wyjście modułu w kolejnych taktach zegara.
% Przy takim podejściu wyjściowy obraz jest identyczny jak przy jednym bloku pamięci, ale pozwala ono zmniejszyć latencję odczytu obrazu lub ograniczyć X-krotnie częstotliwość taktowania zegara, a tym samym zużywaną energię.

\subsubsection{Rolling window}
\label{sssec:rolling}
A~so-called rolling window, used in typical contextual operations on the image, can be proposed for event frame generation, which is partly inspired by \cite{Liu2017}.
The idea is to accumulate event data from $N$ ms, with an image generated every $K$ ms and covering the last $M$ ms, where $K \le M \le N$.
For the test, the values chosen were $N$ = 8, $M$ = 4, $K$ = 1, so a~new frame was generated every 1 ms, covering the last 4 ms, while accumulating events from the last 8 ms.
In this way, in images generated at high frequency, the oldest events are removed and the newest are added. 

% Figure environment removed

On the hardware implementation side, it required additional memory due to `adding' of $\log_2 N = 3$ bits to the data representation, informing about the index of the `sub-window'.
Its resetting included only the oldest index, i.e. events before $N -$1 = 7 ms relative to the current timestamp.
A~schematic of how this method works is presented in Figure~\ref{fig:rolling_window}.

It is worth mentioning that this approach preserves more accurate information about the timestamp of events and can be applied to any data representation.
Other variants of this method may include the use of several frames, offset by $K$ ms, as components of a~multi-channel image or aggregated into a~single one as in \cite{Innocenti2021}, but also \added{the use of} dynamically changing $M$ value from which the output images are generated.

%PL
% Innym podejściem jest zaproponowanie tzw. rolling window, stosowanego w typowych operacjach kontekstowych na obrazie.
% Jego idea w przypadku generacji ramki zdarzeniowej, częściowo wzorowana na \cite{Liu2017}, polega na akumulacji danych zdarzeniowych z $N$ ms, przy czym obraz generowany jest co $K$ ms i obejmuje ostatnie $M$ ms, gdzie $K \le M \le N$.
% W ramach testów wybrano wartości $N$ = 8, $M$ = 4, $K$ = 1, zatem co 1 ms generowana była nowa ramka zdarzeniowa, obejmująca ostatnie 4 ms, natomiast w pamięci zachowywane były zdarzenia z ostatnich 8 ms.
% W ten sposób na obrazach generowanych z dużą częstotliwością usuwana jest część najstarszych zdarzeń, a w ich miejsce dodawane są najnowsze. 

% Od strony implementacji sprzętowej wymagało to dodatkowej pamięci z uwagi na "doklejanie" $\log_2 N$ = 3 dodatkowych bitów do wybranej reprezentacji danych, informujących o indeksie "podobrazu", a jej resetowanie obejmowało tylko najstarszy indeks, czyli zdarzenia sprzed $N$ - 1 = 7 ms względem aktualnego znacznika czasowego.
% Schemat działania tej metody zaprezentowany został na rys. \label{fig:rolling_window}.

% Warto dodać, że takie podejście pozwala na zachowanie dokładniejszych informacji o czasie wystąpienia zdarzeń i można zastosować je do dowolnej reprezentacji danych.
% Inne warianty tej metody mogą obejmować wykorzystanie kilku ramek, przesuniętych względem siebie o $K$ ms, jako składowych wielokanałowego obrazu wyjściowego lub zagregowanych w jeden \cite{Innocenti2021}, ale także dynamiczną zmianę wartości $M$, na podstawie której generowane są wyjściowe obrazy.

\subsubsection{Ultra RAM}
\label{sssec:ultra_ram}
Another option could be to use Ultra RAM resources available in some SoC FPGAs.
This additional internal memory is larger than BRAM and can be used to store the contents of an accumulator or a~FIFO queue. 
This can allow the use of more complex representations, but also support more dynamic scenes due to possible larger queue size.
However, only a~subset of AMD Xilinx's chips are equipped with this memory (UltraScale and UltraScale+ series), so this enhancement is dependent on the chosen hardware platform.

\subsubsection{Temporary buffer}
\label{sssec:temp_buffer}
Apart from described buffering (Section \ref{ssec:basic}), other ways of handling input events can be used.

One is to duplicate the accumulator module and use a~`ping-pong buffering' method, as in the work of \cite{Linares2021}, and swap the roles of two accumulators in a~way that one is in read mode and the other in write mode.
However, this solution is not very efficient, as the entire accumulator memory must be doubled, which may not be feasible for more complex representations due to the limited memory resources of the FPGA device.

Second idea is to use external RAM, located on the FPGA board. 
Its size is considerably larger than block memory, so a~large FIFO queue can be generated.
%An example comparison of the available memory types in \added{several popular} FPGA \added{platforms} from \added{different} AMD Xilinx \added{board families} is provided in Table \ref{table:memory}.
However, this approach also has several disadvantages, among which are: greater system complexity, higher resource utilisation, additional latency (due to reading and transferring data from memory), and the need to use the processor for communication and thus higher power consumption.
An example comparison of the available memory types in \added{several popular} FPGA \added{platforms} from \added{various} AMD Xilinx \added{board families} is provided in Table \ref{table:memory}.

\begin{table*}[t]
\caption{Comparison of available memory resources for exemplary AMD Xilinx platforms. Values in parentheses specify the number of `units' of a~memory type.}
%\caption{Porównanie dostępnych zasobów pamięciowych różnego rodzaju dla wybranych platform od AMD Xilinx. BRAM -- Block RAM, URAM -- Ultra RAM, Ext. RAM -- External RAM. Wartości w nawiasie oznaczają liczbę "sztuk" danego rodzaju pamięci.}
\centering
\label{table:memory}
\begin{tabularx}{0.575\textwidth}{lccc}%{@{} l *{5}{c} c @{}}
\toprule
SoC FPGA platform & Block RAM [Kb] & Ultra RAM [Mb] & External RAM [GB] \\ 
\midrule
ZCU 104 & 11 (312) & 27 (96) & 4.5 \\
Kria KV260 & 5 (144) & 18 (64) & 4 \\
Zybo Z7-20 & 5 (140) & - & 1 \\
\bottomrule
\end{tabularx}
\end{table*}

% \begin{threeparttable}[!t]
%     \caption{Comparison of available memory resources for exemplary AMD Xilinx platforms. BRAM -- Block RAM, URAM -- Ultra RAM, Ext. RAM -- External RAM. Values in parentheses specify the number of `units' of a~memory type.}
%     %\caption{Porównanie dostępnych zasobów pamięciowych różnego rodzaju dla wybranych platform od AMD Xilinx. BRAM -- Block RAM, URAM -- Ultra RAM, Ext. RAM -- External RAM. Wartości w nawiasie oznaczają liczbę "sztuk" danego rodzaju pamięci.}
%     \label{table:memory}
%     \small
%     \setlength\tabcolsep{0pt}
%     \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} lccc @{}}
%     \toprule
%     Board & BRAM [Kb] & URAM [Mb] & Ext. RAM [GB] \\ 
%     \midrule
%     ZCU 104 & 11 (312) & 27 (96) & 4.5 \\
%     Kria KV260 & 5 (144) & 18 (64) & 4 \\
%     Zybo Z7-20 & 5 (140) & - & 1 \\
%     \bottomrule
%     \\
% \end{tabular*}
% \end{threeparttable}

%PL
% Oprócz opisanego w Rozdziale \ref{ssec:basic} bufora przy użyciu kolejki FIFO w pamięci blokowej układu FPGA, można zaproponować inne sposoby obsługi przychodzących zdarzeń.

% Jednym z nich jest duplikacja modułu akumulatora i zastosowanie metody „ping-pong buffering”, jak w pracy \cite{Linares2021}, czyli takie działanie dwóch akumulatorów, z których jeden jest w trybie odczytu, a drugi w trybie zapisu, a następnie zamieniają się rolami itd.
% Jednak rozwiązanie to nie jest zbyt efektywne, gdyż do jego realizacji należy podwoić całą pamięć przeznaczoną na akumulator, co w przypadku bardziej złożonych reprezentacji może się okazać niemożliwe do wykonania z uwagi na ograniczone zasoby pamięciowe układu FPGA.

% Drugim pomysłem jest wykorzystanie zewnętrznej pamięci RAM, znajdującej się na płytce z układem FPGA. 
% Jej rozmiar jest zdecydowanie większy od pamięci blokowej, zatem można wygenerować kolejkę FIFO o bardzo dużych rozmiarach.
% Przykładowe porównanie dostępnych rodzajów pamięci w układach FPGA od AMD Xilinx zamieszczono w tabeli \ref{table:memory}.
% Jednak podejście to posiada kilka wad, wśród których wymienić można: większe skomplikowanie systemu, większe wykorzystanie zasobów, generację dodatkowych opóźnień (z uwagi na odczyt i transfer danych z pamięci), a także konieczność wykorzystania procesora do komunikacji i tym samym większe zapotrzebowanie energetyczne.

% \begin{table*}[t]
% \caption{.}
% %\centering
% \label{tab:repr}
% \begin{tabularx}{1.0\linewidth}{lccc}%{@{} l *{5}{c} c @{}}
% \toprule
% Board & BRAM [Kb] & URAM [Mb] & Ext. RAM [GB] \\ 
% \midrule
% ZCU 104 & 11 & 27 & 4.5 \\
% Kria KV & 5 & 18 & 4 \\
% Zybo Z7-20 & 0.63 & - & 1 \\
% \bottomrule
% \end{tabularx}
% \end{table*}

%PL
% Inną opcją może być wykorzystanie zasobów Ultra RAM, którymi dysponuje część układów SoC FPGA.
% Jest to dodatkowa pamięć wewnętrzna o rozmiarze większym niż blokowa, którą można wykorzystać do przechowania zawartości akumulatora bądź kolejki FIFO. Może to pozwolić na zastosowanie bardziej złożonych reprezentacji, ale też obsługi bardziej dynamicznych scen z uwagi na możliwe większe wymiary kolejki.
% Jednak tylko część układów jest wyposażona w tę pamięć -- w przypadku AMD Xilinx są to serie UltraScale i UltraScale+, zatem niniejsze usprawnienie jest uzależnione od posiadanej platformy sprzętowej.


\subsubsection{Accumulation time}
\label{sssec:acc_time}
Event accumulation time $\tau$ is one of the most important parameters in the event frame generation algorithm.
A~smaller value provides more frames per second and \added{better} information about the dynamics of the scene, while a~larger value allows a~better understanding of the overall motion and reduces the impact of noise.
Therefore, a~potential improvement of the algorithm in this case could be an adaptive approach in which the accumulation time $\tau$ can be increased or decreased depending on the dynamics of the scene.

%PL
% Czas akumulacji zdarzeń $\tau$ to jeden z najważniejszych parametrów w algorytmie generacji ramek zdarzeniowych.
% Mniejsza wartość zapewnia więcej klatek na sekundę i informacji o dynamice sceny, a większa pozwala lepiej zrozumieć ogólny ruch i zmniejszyć wpływ szumów.
% %Mniejsza wartość pozwala uzyskać większą liczbę klatek na sekundę i więcej informacji o dynamice sceny, jednak wymaga wyższej częstotliwością działania i ewentualnie większej ilości zasobów.
% %Natomiast wartość większa pozwala lepiej zinterpretować ogólne zmiany na scenie oraz zmniejszyć wpływ szumów z kamery, ale kosztem pewnej utraty zalet wynikających z bardzo wysokiej częstotliwości akwizycji zdarzeń.
% %Porównanie wygenerowanej ramki zdarzeniowej na testowej sekwencji z ruchomą kamerą oraz obiektem (piłką) przelatującą przez pokój ze stanowiskiem komputerowym dla różnych czasów akumulacji $\tau$ zostało przedstawione na rys. \ref{fig:repr}.
% Dlatego potencjalnym usprawnieniem algorytmu pod tym kątem może być podejście adaptacyjne, w którym w zależności od dynamiki sceny, czas akumulacji $\tau$ może być zwiększany lub zmniejszany.

% % Figure environment removed


\subsubsection{Accumulation number}
\label{sssec:acc_number}
Another enhancement could be the generation of frames every fixed number of events $Z$, as in the work of \cite{Linares2021}.
Event data is recorded asynchronously for each pixel and the dynamics of the scene determines the exact number of events per time interval.
In this approach, it may possible (depending on the representation) to omit the timestamps and reduce the amount of data processed.
However, the generated frames will appear at the output in different time intervals, which may raise problems especially in cases with high dynamics \added{(and thus high fps value)}.
A~potential solution to this problem could be an adaptive version, in which the length of the time interval is additionally analysed and the number of $Z$ events is reduced or increased based on it.

%PL
% Inne usprawnienie algorytmu stanowić może generacja ramek co ustaloną liczbę zdarzeń $Z$, jak w pracy \cite{Linares2021}.
% Dane te rejestrowane są asynchronicznie dla każdego piksela i to od dynamiki sceny zależy ich dokładna liczba w przedziale czasu.
% W takim podejściu możliwe jest (w zależności od przyjętej reprezentacji) pominięcie znaczników czasowych i zmniejszenie ilości przetwarzanych danych.
% Z drugiej strony, generowane ramki będą się pojawiać na wyjściu w różnych odstępach czasu, co może rodzić problemy zwłaszcza w przypadkach o dużej dynamice.
% Potencjalnym rozwiązaniem tego problemu może być wersja adaptacyjna, w której dodatkowo analizowana jest długość przedziału czasowego i na tej podstawie liczba zdarzeń $Z$ jest odpowiednio zmniejszana lub zwiększana.


\subsection{Performance}
\label{ssec:perf}

\begin{table*}[t]
\caption{Comparison of hardware resource utilisation on an FPGA platform for different data representations and variants (for a~2-bit event frame only) for a~10 ms accumulation interval. The size of the FIFO queue was set to the minimum (512 elements) not to disturb the comparisons. Power estimation \added{for the FPGA chip only} was performed by the Vivado software. Due to the nature of BRAM memory, the smallest indivisible element has 0.5 size of a~single block (meant as a~`piece').}
%\caption{Porównanie wykorzystania zasobów sprzętowych na platformie FPGA dla różnych reprezentacji dla czasu akumulacji zdarzeń równego 10 ms. Rozmiar kolejki FIFO został ustawiony na minimalny (512 elementów), aby nie zaburzać porównania samych reprezentacji. Estymacja mocy jest wykonywana przez program Vivado. Z uwagi na specyfikę pamięci BRAM, najmniejszy niepodzielny element ma rozmiar równy 0.5 pojedynczego bloku ("sztuki").}
\centering
\label{tab:repr}
\begin{tabularx}{0.73\textwidth}{lccccc}%{@{} l *{5}{c} c @{}}
\toprule
Representation/Algorithm variant & No. of bits & Block RAM & LUT & Flip-Flop & Power est. [W] \\ 
\midrule
Binary frame (Sec. \ref{ssec:basic}) & 1 & 29.5 & 191 & 142 & 5.9 \\
Event frame (Sec. \ref{sssec:other_repr}) & 2 & 57.5 & 240 & 157 & 9.0 \\
Exp. decaying time surface (Sec. \ref{sssec:other_repr}) & 8 & 226 & 1289 & 168 & 29.1 \\
Event frequency (Sec. \ref{sssec:other_repr}) & 5 & 142 & 363 & 158 & 27.3 \\
\midrule
Basic (Sec. \ref{ssec:basic}) & 2 & 57.5 & 240 & 157 & 9.0 \\
Multiple BRAMs (Sec. \ref{sssec:multiple_brams}) & 2 & 61 & 204 & 258 & 12.0 \\
Rolling window (Sec. \ref{sssec:rolling}) & 5 & 142 & 8369 & 150 & 18.2 \\
\bottomrule
\end{tabularx}
\end{table*}

A~comparison of the most important parameters of several data representations and variants for event frame generation algorithm is provided in Table \ref{tab:repr}.
The use of memory resources increases significantly with the number of bits per pixel due to larger accumulator size.
Due to the amount of available memory resources, some options described may not be feasible to implement on smaller FPGA chips -- therefore simpler representations, additional approximations or lower data resolution may be needed.
The use of the remaining resources is very low, as the logic itself controlling the writing and reading of data from the accumulator is relatively simple.
%A comparison of the hardware resource consumption and the most important parameters of several variants of the developed algorithm is presented in Table \ref{tab:variants}.

The described variants concern only one most popular data representation (event frame).
In case of the rolling window method, it is necessary to allocate more memory to store the `sub-window' indices (3 extra bits -- 8 indices).
The use of multiple memory blocks generates a~slight increase in resource consumption, because with suboptimal parameters, a~part of each allocated block remains unused.
However, this solution allows faster reading of pixels, vector data processing (Section \ref{sssec:multiple_brams}) and reduced clock frequency and power consumption.
Therefore, using one block configured in Vivado with a~specified set of parameters and a~global reset can also be considered, if smaller versatility of the module is not a~problem.

%PL
% Porównanie najważniejszych parametrów zaimplementowanych wybranych reprezentacji danych do generacji ramki zdarzeniowej zamieszczone zostało w tabeli \ref{tab:repr}.
% Jak łatwo się domyślić, wraz ze wzrostem liczby bitów na piksel, znacząco rośnie wykorzystanie zasobów pamięciowych z uwagi na większe rozmiary akumulatora.
% Z uwagi na dostępność zasobów pamięciowych w poszczególnych układach FPGA, niektóre z wymienionych reprezentacji i wariantów mogą się okazać niemożliwe do zaimplementowania na mniejszych układach, w przypadku których warto rozważyć prostsze reprezentacje, dodatkowe przybliżenia lub niższą rozdzielczość danych.
% Wykorzystanie pozostałych zasobów jest bardzo małe, gdyż sama logika sterująca zapisem i odczytem danych z akumulatora jest stosunkowo prosta.

% W tabeli \ref{tab:variants} przedstawione zostało porównanie zużycia zasobów sprzętowych i najważniejszych parametrów dla kilku wariantów opracowanego algorytmu.
% Modyfikacje dotyczą wyłącznie części sprzętowej dla jednej reprezentacji danych (event frame).
% W przypadku metody rolling window, konieczne jest przeznaczenie części pamięci na zapamiętanie indeksów "podokien" (w tym przypadku 8, bo używane są 3 bity).
% Natomiast zastosowanie wielu bloków pamięci generuje niewielki wzrost zużywanych zasobów pamięciowych, gdyż przy nieoptymalnych parametrach część każdego zaalokowanego bloku pozostaje nieużywana.
% Rozwiązanie to pozwala jednak na szybszy odczyt pikseli, przetwarzanie danych w formacie wektorowym (Rozdział \ref{sssec:multiple_brams}) i zmniejszenie częstotliwości taktowania zegara oraz zużycia energii.

% \begin{table*}[t]
% \caption{Comparison of the use of hardware resources on an FPGA platform for the chosen representation (2-bit event frame, as in Section \ref{ssec:event_frame}) together with implementation modifications to the algorithm for an event accumulation time of 10 ms. The size of the FIFO queue has been set to a minimum (512 elements) in order not to disturb the comparison of the algorithm variants for frame generation themselves. Power estimation is performed by the Vivado software.}
% %\caption{Porównanie wykorzystania zasobów sprzętowych na platformie FPGA dla wybranej reprezentacji (2-bitowej event frame, jak w Rozdziale \ref{ssec:event_frame}) wraz z implementacyjnymi modyfikacjami algorytmu dla czasu akumulacji zdarzeń równego 10 ms. Rozmiar kolejki FIFO został ustawiony na minimalny (512 elementów), aby nie zaburzać porównania samych wariantów algorytmu do generacji ramki. Estymacja mocy jest wykonywana przez program Vivado.}
% \centering
% \label{tab:variants}
% \begin{tabularx}{0.64\textwidth}{lccccc}%{@{} l *{5}{c} c @{}}
% \toprule
% Algorithm variant & No. of bits & Block RAM & LUT & Flip-Flop & Power [W] \\ 
% \midrule
% Basic (Sec. \ref{ssec:basic}) & 2 & 57.5 & 240 & 157 & 9.0 \\
% Multiple BRAMs (Sec. \ref{sssec:multiple_brams}) & 2 & 61 & 204 & 258 & 12.0 \\
% Rolling window (Sec. \ref{sssec:rolling}) & 5 & 142 & 8369 & 150 & 18.2 \\
% \bottomrule
% \end{tabularx}
% \end{table*}


\section{Conclusion}
\label{sec:concl}
In this paper we have proposed and compared various ways of event frame generation in SoC FPGA \added{devices for} \deleted{.
In particular, we focused on} an HD \deleted{($1280 \times 720$ pixels)} event stream.
\deleted{The resulting event frames can be used in typical vision algorithms with both classical methods and deep neural networks.}
\deleted{On the other hand, the use of SoC FPGAs should yield real-time and energy-efficient event data processing systems.}
For a~pipelined hardware implementation on an FPGA platform, this operation brings a~number of challenges and trade-offs depending on the available resources, mainly memory: the choice of resolution of the generated frames, the representation used, the realisation of a~temporary buffer and additional hardware enhancements.

\added{The use of SoC FPGA chips from AMD Xilinx can yield real-time and energy-efficient data processing, but} the resources available on a~chosen platform condition the details of the implementation.
In case of Zybo Z7-20, only the simplest representations (binary and event frame) can be realised due to the small size of the available memory.
For Kria KV260, any of the presented variants can be used, but Ultra RAM or external memory must be used, while for ZCU 104, all of them can be implemented using only block memory.

As part of future work, several applications of the proposed event frame generation module are planned, \added{as the frames themselves can be used in typical vision-based systems with both classical methods and deep neural networks.}
Running on an exemplary hardware platform together with the loading of data from an SD card \replaced{(or later receiving}{and then transmitted} directly from the camera), it can be possible to use this module in a~larger vision system, e.g. for the detection of fast-moving objects.
Ultimately, the developed system is planned to be used on an unmanned autonomous drone to enable it, among other things, to fly through a~dynamic environment with obstacles.  

%PL
% W niniejszej pracy zaproponowaliśmy i porównaliśmy szereg sposobów implementacji sprzętowej generacji ramek zdarzeniowych w układach SoC FPGA.
% W szczególności skupiliśmy się na strumieniu o wysokiej rozdzielczości (HD -- $1280 \times 720$ pikseli).
% Otrzymane w ten sposób ramki zdarzeniowe można wykorzystać w typowych algorytmach wizyjnych, zarówno przy użyciu metod klasycznych, jak i głębokich sieci neuronowych.
% Natomiast zastosowanie układów SoC FPGA powinno pozwolić uzyskać działające w czasie rzeczywistym i efektywne energetycznie systemy przetwarzania danych zdarzeniowych.
%stare
%W niniejszej pracy zastosowano nowoczesne czujniki wizyjne -- kamery zdarzeniowe, które rejestrują jedynie zmiany jasności na monitorowanej scenie, dlatego nie można w prosty sposób zastosować do nich typowych algorytmów wizyjnych.
%Jednym z rozwiązań tego problemu jest generacja ramek zdarzeniowych, czyli obrazów, na które rzutowane są zdarzenia z określonych przedziałów czasowych.
%Wy%brane metody zostały porównane i opracowane na platformę FPGA, aby umożliwić działanie projektowanego systemu wizyjnego w czasie rzeczywistym i przy niskim zużyciu energii także dla kamer zdarzeniowych o wysokiej rozdzielczości (HD -- $1280 \times 720$ pikseli).

%PL
% Generacja ramki zdarzeniowej to stosunkowo prosta operacja, jednak w przypadku potokowej implementacji sprzętowej na platformie FPGA niesie ze sobą szereg wyzwań i kompromisów zależnych od dostępnych zasobów, głównie pamięciowych: dobór rozdzielczości generowanej ramki i używanej reprezentacji, wykorzystanie bufora tymczasowego, dodatkowych optymalizacji sprzętowych.
% Zasoby dostępne na przykładowych platformach sprzętowych od firmy AMD Xilinx warunkują szczegóły potencjalnej implementacji.
% W przypadku Zybo Z7-20, można zrealizować tylko najprostsze reprezentacje (binary frame i event frame) z uwagi na niewielkie rozmiary dostępnej pamięci.
% Dla Kria KV260 możliwe jest wykorzystanie każdego z wymienionych wariantów, jednak konieczne jest użycie Ultra RAM lub pamięci zewnętrznej.
% W przypadku ZCU 104 można zaimplementować wszystkie z wymienionych reprezentacji z wykorzystaniem wyłącznie pamięci blokowej.

% W ramach przyszłych prac planowanych jest kilka aplikacji opracowanego i przedstawionego modułu generacji ramek zdarzeniowych.
% Uruchomienie na przykładowej platformie sprzętowej wraz z wczytywaniem danych z karty SD, a potem przekazywanych bezpośrednio z kamery, umożliwi wykorzystanie tego modułu w większym systemie wizyjnym, np. do detekcji szybko poruszających się obiektów na platformie FPGA.
% Docelowo opracowany system będzie można wykorzystać chociażby na bezzałogowym autonomicznym dronie, aby umożliwić mu m.in. przelot przez dynamiczne środowisko z przeszkodami.  


% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgment}
%The work presented in this paper was supported by the programme ``Excellence initiative – research university'' for the AGH University of Krakow and was partly supported by the National Science Centre project no. 2021/41/N/ST6/03915 entitled ``Acceleration of processing event-based visual data with the use of heterogeneous, reprogrammable computing devices''.
%The authors would like to thank Piotr Wzorek and Mateusz Wąsala for their help with the project.


\begin{thebibliography}{20}

\bibitem{Rebecq2021}
Rebecq, H. et al. (2021). High Speed and High Dynamic Range Video with an Event Camera. IEEE Transactions on PAMI, 43(6), 1964–1980. https://doi.org/10.1109/TPAMI.2019.2963386.

\bibitem{Vidal2018}
A. Rosinol Vidal et al. “Ultimate SLAM? Combining events, images, and IMU for robust visual SLAM in HDR and high speed scenarios,” IEEE Robot. Autom. Lett., vol. 3, no. 2, pp. 994–1001, Apr. 2018.

\bibitem{Pan2019}
L. Pan et al. “Bringing a blurry frame alive at high frame-rate with an event camera,” in Proc. IEEE Conf. CVPR, 2019, pp. 6813–6822.

\bibitem{Wzorek2022}
Wzorek, P. et al. (2022, September). Traffic Sign Detection With Event Cameras and DCNN. 2022 SPA Conf.. https://doi.org/10.23919/spa53010.2022.9927864.

\bibitem{Bialik2023}
Bialik, K. et al. (2023). Fast-moving object counting with an event camera. Pomiary Automatyka Robotyka, 27(247), 79–84. https://doi.org/10.14313/PAR\_247/79.

\bibitem{Gallego2022}
Gallego, G. et al. (2022). Event-Based Vision: A~Survey. IEEE Transactions on PAMI, 44(1), 154–180. https://doi.org/10.1109/TPAMI.2020.3008413.

\bibitem{Chen2018}
Chen, N. Y. (2018). Pseudo-Labels for Supervised Learning on Dynamic Vision Sensor Data, Applied to Object Detection Under Ego-Motion. 2018 IEEE/CVF Conf. CVPR Workshops, 757–75709. https://doi.org/10.1109/CVPRW.2018.00107.

\bibitem{Wan2021}
Wan, J. et al. (2021). Event-based pedestrian detection using dynamic vision sensors. Electronics, 10(8), 888.

\bibitem{Innocenti2021}
Innocenti, S. et al. (2021). Temporal Binary Representation for Event-Based Action Recognition. 2020 25th ICPR Conf., 10426–10432. https://doi.org/10.1109/ICPR48806.2021.9412991.

\bibitem{Linares2021}
Linares-Barranco, A. et al. (2021). Dynamic Vision Sensor Integration on FPGA-Based CNN Accelerators for High-Speed Visual Classification. ICNS Conf. 2021. https://doi.org/10.1145/3477145.3477167.

\bibitem{Liu2017}
Liu, M. et al. (2017). Block-matching optical flow for dynamic vision sensors: Algorithm and FPGA implementation. 2017 IEEE ISCAS, 1–4. https://doi.org/10.1109/ISCAS.2017.8050295.

\bibitem{Tapiador2020}
Tapiador-Morales, R. et al. Event-Based Gesture Recognition through a Hierarchy of Time-Surfaces for FPGA. Sensors 2020, 20, 3404. https://doi.org/10.3390/s20123404.

\bibitem{Clady2015}
Clady, X. et al. (2015). Asynchronous event-based corner detection and matching. Neural Networks, 66, 91–106. https://doi.org/https://doi.org/10.1016/j.neunet.2015.02.013.

\bibitem{Afshar2020}
Afshar, S. et al. Event-Based Feature Extraction Using Adaptive Selection Thresholds. Sensors 2020, 20, 1600. https://doi.org/10.3390/s20061600.

\bibitem{Kowalczyk2018}
M. Kowalczyk et al. "Real-Time Implementation of Contextual Image Processing Operations for 4K Video Stream in Zynq UltraScale+ MPSoC," 2018 DASIP Conf., Porto, Portugal, 2018, pp. 37-42, doi: 10.1109/DASIP.2018.8597105.

\end{thebibliography}

% that's all folks
\end{document}