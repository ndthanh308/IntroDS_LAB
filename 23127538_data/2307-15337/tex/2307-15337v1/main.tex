\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
\usepackage[preprint,nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{amsmath}


\newcommand{\says}[3]{{\color{#3}#1:\emph{#2}\color{black}}\xspace}
\newcommand{\todo}[1]{\says{TODO}{#1}{orange}}
\newcommand{\toupdate}[1]{{\color{red}\emph{#1}\color{black}}\xspace}

\input{macro}
\usepackage{hyperref}       % hyperlinks


\title{\method{}: \\Large Language Models Can Do Parallel Decoding}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Xuefei Ning$^1$\thanks{Equal contribution.}\\
  \texttt{foxdoraame@gmail.com} \\
  \And
  Zinan Lin$^{2*}$ \\
  \texttt{linzinan1995@gmail.com} \\
  \And
  Zixuan Zhou$^{1*}$ \\
  \texttt{zhouzx21@mails.tsinghua.edu.cn} \\
  \And
  Huazhong Yang$^{1}$ \\
  \texttt{yanghz@tsinghua.edu.cn} \\
  \And
  Yu Wang$^{1}$ \\
  \texttt{yu-wang@tsinghua.edu.cn} \\
}


% \newcommand{\nodagfootnote}[1]{%
%   \begingroup
%   \renewcommand{\thefootnote}{}% Remove footnote number
%   \footnotetext{\textsuperscript{\dag}#1}% Add custom marker and footnote text
%   \endgroup
% }

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\begin{document}

\maketitle

\begin{center}
  \vspace{-22pt}
  {$^{1}$ Department of Electronic Engineering, Tsinghua University, Beijing, China\\ $^{2}$ Microsoft Research, Redmond, Washinton, USA}%
%\\  {\small }
\end{center}



\begin{abstract}
  This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the %fact that humans' thinking and writing are not fully sequential
  thinking and writing process of humans, we propose ``\method{}'' (\methodshort{}), which guides  LLMs to first generate the \emph{skeleton} of the answer, and then conducts parallel API calls or batched decoding to %expand multiple segments parallelly. 
  complete the contents of each skeleton point \emph{in parallel}.
  Not only does \methodshort{} provide considerable speed-up (up to 2.39$\times$ across 11 different LLMs), %($1.31\sim 2.39\times$ across eight different LLMs),
  but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. 
  \methodshort{} is an initial attempt at data-centric optimization %(more specifically, content co-organization) 
  for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
  %revealing the potential of adding a new data level to the existing technique stack of ``model-software-hardware co-design for efficiency''.
  
\end{abstract}

% \footnotemarker \footnotetext[\dag]{Technical Report}
% \begin{NoHyper}
% \nodagfootnote{Technical Report.}
% \end{NoHyper}
\blfootnote{$^\dagger$ A technical report.}

%\todo{discuss the word ``point skeletons''?}

% \todo{maybe we need multiple runs to get the std bar for the efficiency speed up}

% \todo{after batch implmentation, let's actual test for three time}


\section{Introduction}

% In recent years, large language models (LLMs) have shown exceptional performance in natural language processing and revolutionized the chatbot systems. In particular, a series of pretrained LLMs, such as GPT, T5, OPT, GLM, and LLaMA, have shown astonishing performances in a varitey of tasks, and their instruction or conversation fine-tuned verion are widely used in chatbot systems.


% A typical LLM generative process consists of two stages: 1) the prefill stage in which the prompt is parsed to generate the key-value cache for further use, and 2) the decoding stage in which tokens are generated one by one in an incremental manner. The decoding stage accounts for the majority of the end-to-end latency, especially when generating a long response. As shown in \cref{}, due to the low arithmetic intensity, the decoding stage is highly I/O bounded and thus cannot fully utilize the GPUs' computation power. Improving the GPU utilization is essential for accelerating the decoding stage, and existing decoding acceleration methods can be classifed into ...

% In this study, we explore the off-the-shelf potential of parallel decoding in large language models. Specifically, we wonder why cannot we decode multiple segments in the meantime. As shown in \cref{}, .. Then, how do we know which segments can be parallely decoded? Instead of relying on some position-based heuristics, it might be the most appropriate choice to let language model itself tell us. Therefore, we design a prompt template to guide the LLM to give out its ``Skeleton of Thought'' (SoT), which tell us how to parallel decode.

% To draw a parallel between the human thinking and the SoT method, when we take test, answer a question, we usually organize our answer in advance. For example, when discussing about a ..., after deciding the answer skeleton, we'll leverage our language capability to expand on each point of the skeleton by adding evidences, details, and linguistic styles and techniques. ... Results in more comprehensive and organized answer ...

% To summarize, we explore a Skeleton-of-Thought way to boost the LLMs' decoding efficiency by prompt engineering and batch inference. Unlike previous algorithm co-design for efficiency, this attempt can be seem as a content co-guidance for efficency, which might become relevant in the era of general language models. As shown in Fig.~, SoT can achieve xxx speed-up ... Besides improving the decoding efficiency, SoT can also improve the diversity and comprehensiveness of the LLM's answer in several types of scenarios, which can be owe to the resemblance of SoT to the organized writing and thinking process of human. Though using self-organized decoding seems promising for both efficiency concern and algorithmic performance, there are many challenges that cannot be well handled by our method or even current models. In \cref{}, we'll discuss the current limitations, near-term and long-term outlooks to call for future exploration in this direction.


% --- story v2 ---
% LLMs are amazing. But slow. Troublesome for end users or companies. E.g., it takes 2 min for GPT4 API to answer XXX.  

% We ask: without changing the model itself, can we accelerate the inference time? E.g., can we answer the same question XXX in 30 seconds with the same existing GPT4 API? [乍一听不太可能，勾起读者兴趣] This is in contrast (and orthogonal) to existing solutions that require model changes (e.g., quantization, pruning, multi-step decoding). 

% Reason for slow speed: sequential decoding. Chain-of-thought emphasizes the importance of it, and people take it as granted. However, we question if it is a universal rule. Think about humans: our thought is not linear; for many types of questions, we derive the skeleton first and then fill in the details for each point. 

% Motivated by this insight, we propose "Skeleton-of-Thought" [figure, use XXX as an example to contrast sequential decoding v.s. ours]: ask LLMs to derive an outline, and complete each point in parallel. Suitable for both open-source models (parallel decoding) and closed-source models (parallel API calls). Not only gives speed up (e.g., ...), and can potentially improve the answer quality (e.g., ...).

% SoT opens up more questions than it answered. E.g., graph-of-thought, content-oriented optimization (v.s. xxx), ....

% However, due to the autoregressive decoding approach, the generation process of the state-of-the-art LLMs is slow, especially when generating a long sequence. For example,

% --- expand ---
Large language models (LLMs)~\cite{brown2020gpt3,touvron2023llama,du2022glm,openai2023gp4,zheng2023judging} have shown exceptional performance in natural language processing and revolutionized chatbot systems.
However, the inference process of the state-of-the-art LLMs is slow, hindering their interactive use.
For example, it takes 22 seconds for Claude \cite{claude} (accessed through Slack) and 43 seconds for \Remark{vicuna33B1.3} (a 33B LLaMA-based model, running locally on one NVIDIA A100 GPU) to answer the question in \cref{fig:sequential-vs-sot}.
%Besides, as shown in \cref{tab:inf-latency}, 
% And when running LLaMA-based models locally, the end-to-end latency of generating a long answer consisting of \toupdate{...} tokens can be as high as \toupdate{1 minute}.



We conclude three major causes of LLMs' slow inference problem: (1) A \emph{large model size} requires a large amount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT-3 take 350GB memory, which means 5$\times$80GB A100 GPUs are needed just for inference. Even with enough GPUs, the heavy memory access and computation slow down the inference (and also training). (2) %The increasing sequence length also leads to substantial efficiency issue, as
The core \emph{attention operation} in the prevailing transformer architecture is I/O bounded and has a quadratic memory and computation complexity in sequence length. (3) The \emph{sequential decoding} approach in inference generates tokens one by one, where each token depends on previously generated tokens. This approach introduces a significant inference latency since the generation of tokens cannot be parallelized. 
There is a bunch of literature addressing the first two axes: ``large model size''~\cite{xiao2022smoothquant,frantar2022gptq,lin2023awq,lu2017flexflow,sheng2023flexgen,wang2021spatten} and ``attention operation''~\cite{kitaev2020reformer,wang2020linformer,dao2022flashattention,zaheer2020big,chen2023dynamic}. These works either compress/redesign the model~\cite{xiao2022smoothquant,frantar2022gptq,lin2023awq,kitaev2020reformer,wang2020linformer,dao2022flashattention,zaheer2020big} or redesign the serving system~\cite{lu2017flexflow,sheng2023flexgen,chen2023dynamic} and hardware~\cite{wang2021spatten}.


% Figure environment removed

In contrast to prior work, we tackle the third axis and question the common assumption that LLMs have to do sequential decoding. We show the feasibility of \textbf{parallel decoding of off-the-shelf LLMs \emph{without} any changes to their model, system, or hardware}. For instance, for the question in \cref{fig:sequential-vs-sot}, we can reduce the latency from 22 seconds to 12 seconds (1.83$\times$ speed-up) with the Claude model over Slack, and from 43 seconds to 16 seconds (2.69$\times$ speed-up) with Vicuna-33B V1.3 on A100.


%\textbf{Without changing the model, system, or hardware, can we improve the inference latency?} 
% As shown in \cref{tab:inf-latency}, the generation process accounts for the majority of the end-to-end inference latency on modern GPUs. Focusing on the ``sequential decoding approach'' cause of inefficiency, we question if the generation process needs to be fully sequential. 

The idea stems from reflecting on how humans ourselves answer questions. 
Humans do \emph{not} always think about questions and write answers in a sequential fashion. In contrast, for many types of questions, we first derive the \emph{skeleton} according to some protocols and strategies, and then add evidence and details to refine and explicate each point. This is especially the case on formal occasions like offering consultancy, taking tests, writing papers, and so on. 
Can we make LLMs think in the same way?
To this end, we propose \emph{\method{} (\methodshort{})}. Specifically, as shown in \cref{fig:sequential-vs-sot}, we guide the LLM  to derive a skeleton first by itself. Based on the skeleton, the LLMs can complete each point \emph{in parallel} so that we get a speed-up. \methodshort{} can be utilized to accelerate both open-source models with batched decoding and closed-source models with parallel API calls.

We test \methodshort{} on 11 recently released LLMs. Not only does \methodshort{} provide considerable speed-up (up to 2.39$\times$), but it can also improve the answer quality on several question categories in terms of diversity and relevance (\cref{fig:quality_vs_speedup}).

% For example, when encountering the question in \cref{fig:sequential-vs-sot}, a human will usually organize the thinking or writing structure %to discuss the topic 
%from multiple aspects such as \todo{} before filling in the details for each aspect. 

% \todo{make a highlighting result table/figure here, let's do it finally when we finished the whole paper? @zinan}

% Figure environment removed

\methodshort{} opens up more questions than it answered, for example:
\begin{enumerate}[I]
\item In general, the thinking process of humans is more complicated than \methodshort{}; it contains many steps structured in a complicated graph. Generalizing \methodshort{}, should we and can we organize the LLMs' thinking and writing process as ``Graph-of-Thought''? This could potentially combine the benefits of \method{} on efficiency and global understanding with the benefits of Chain-of-Thoughts (CoT)~\cite{kojima2022large,wei2022chain} on logical reasoning and impromptu thinking.

\item The machine learning field is moving from the model-centric era to the data-centric era. For example, it is acknowledged that designing the data pipeline to guide learning becomes even more important than designing the model structure for improving the \emph{model quality}~\cite{zha2023data,dcai2023}.
However, in the area of improving \emph{model efficiency}, prior work is still in the model-centric paradigm (e.g., model compression or model design for efficiency~\cite{han2015deep,krishnamoorthi2018quantizing,cai2019once}).
\methodshort{} is a first attempt on \emph{data-centric optimization for efficiency}, in that \methodshort{} \textit{guides LLM to organize its outputted contents} during inference for improving the \textit{efficiency}. We are optimistic about the opportunities and potential of this direction.

% We are curious will data-centric optimization begin to play a more important role, not only for the quality, but also for efficiency?
\end{enumerate}

We will share more insights and knowledge on the positioning 
and outlooks of \methodshort{} in \cref{sec:literature} and \cref{sec:limit-and-outlook}.




%\todo{unify generation and decode? algorithm performance?}

%\todo{outline stage ? expanding  stage? skeleton stage? let's unify}

%\todo{update the citation here. find some paper on data-centric AI}
% Chain-of-thought emphasizes the importance of it, and people take it as granted. However, we question if it is a universal rule. Think about humans: our thought is not linear; for many types of questions, we derive the skeleton first and then fill in the details for each point.

% we explore the off-the-shelf potential of parallel decoding in large language models. Specifically, we wonder why cannot we decode multiple segments in the meantime. As shown in \cref{}, .. Then, how do we know which segments can be parallely decoded? Instead of relying on some position-based heuristics, it might be the most appropriate choice to let language model itself tell us. Therefore, we design a prompt template to guide the LLM to give out its ``Skeleton of Thought'' (SoT), which tell us how to parallel decode.

%To summarize, we explore a \method{} way to boost the LLMs' decoding efficiency by prompt engineering and batch inference. Unlike previous algorithm co-design for efficiency, this attempt can be seem as a content co-guidance for efficency, which might become relevant in the era of general language models. As shown in Fig.~, SoT can achieve xxx speed-up ... Besides improving the decoding efficiency, SoT can also improve the diversity and comprehensiveness of the LLM's answer in several types of scenarios, which can be owe to the resemblance of SoT to the organized writing and thinking process of human. Though using self-organized decoding seems promising for both efficiency concern and algorithmic performance, there are many challenges that cannot be well handled by our method or even current models. In \cref{}, we'll discuss the current limitations, near-term and long-term outlooks to call for future exploration in this direction.


%\todo{first describe local serving, and then discuss can also be applied to API?}

%\todo{I think ``chain-of-thought emphasize the importance of sequential generation'' is not so obvious to all audiences. let's discuss. maybe we mention chain-of-thought later, especially when discussing the math/coding problem}


%This is in contrast (and orthogonal) to existing solutions that require model changes (e.g., quantization, pruning, multi-step decoding). 


% \section{Background}
% \label{sec:background}


\section{\method{} (\methodshort{})}
\label{sec:method}

\subsection{Method}

\myparatightestn{Overview.} \cref{fig:sequential-vs-sot} illustrates how \methodshort{} produces the final answer to a user \concept{question} $q$. 

\myparaemphtightestn{(1) Skeleton stage.}
\methodshort{} first assembles a \concept{skeleton request}, $T^s(\mbox{question}=q)$, using the \concept{skeleton prompt template} $T^s$ with the question $q$ as the parameter. 
The skeleton prompt template is written to guide the LLM to output a concise skeleton of the answer. Then, we extract the $B$ points from the \concept{skeleton response} $R^s$ of the LLM.

\myparaemphtightestn{(2) Point-expanding stage.} Based on the skeleton, we let the LLM expand on each point in parallel. Specifically, for the point with index $b$ and skeleton $R^s_b$, \methodshort{} uses $T^{pe}(\mbox{question}=q, \mbox{skeleton}=R^s, \mbox{point index}=b, \mbox{point skeleton}=R_{b}^s)$ as the \concept{point-expanding request} for the LLM, where $T^{pe}$ is the \concept{point-expanding prompt template}. Finally, after completing all points, we concatenate the point-expanding responses $\{R^{pe}_b\}_{b=1,\cdots,B}$ to get the \concept{final answer}. 
%\todo{@lzn echo the highight result in Figure1 here?}

\cref{prompt:ts} and \cref{prompt:tp} show the \concept{skeleton prompt template} $T^s$ and \concept{point-expanding prompt template} $T^{pe}$ used by our current implementation. 

\myparatightestn{Skeleton prompt template.} 
In order to make the output skeleton short and in a consistent format for the good of efficiency and ease of point extraction, the skeleton prompt template (1) describes the task precisely, (2) uses two simple demonstrations, and (3) provides a partial answer ``1.'' for the LLM to continue writing. 
We find that, in most cases, the skeleton responses are in the desired format. Therefore, we can simply use a regular expression to extract point indexes and point skeletons from the skeleton response.

\myparatightestn{Point-expanding prompt template.}
The point-expanding prompt template describes the point-expanding task and provides a partial answer. We also provide instructions ``Write it **very shortly** in 1$\sim$2 sentence'' so that the LLMs keep the answers concise. Unlike the skeleton prompt template, we find that demonstrations are not necessary to get reasonable results. 


\begin{promptenv}{Skeleton Prompt Template $T^s$}{You're an organizer responsible for only giving the skeleton (not the full content) for answering the question. Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full sentence, each skeleton point should be very short with only 3$\sim$5 words. Generally, the skeleton should have 3$\sim$10 points.\\\\Question:\\What are the typical types of Chinese dishes?\\Skeleton:\\1. Dumplings.\\2. Noodles.\\3. Dim Sum.\\4. Hot Pot.\\5. Wonton.\\6. Ma Po Tofu.\\7. Char Siu.\\8. Fried Rice.\\\\Question:\\What are some practical tips for individuals to reduce their carbon emissions?\\Skeleton:\\1. Energy conservation.\\2. Efficient transportation. \\3. Home energy efficiency. \\4. Reduce water consumption. \\5. Sustainable diet. \\6. Sustainable travel.\\\\Now, please provide the skeleton for the following question.\\\promptarg{question}\\Skeleton:}{1.}
  \label{prompt:ts}
\end{promptenv}

\begin{promptenv}{Point-Expanding Prompt Template $T^{pe}$}{You're responsible for continuing the writing of one and only one point in the overall answer to the following question.\\\\\promptarg{question}\\\\The skeleton of the answer is\\\\\promptarg{skeleton}\\\\Continue and only continue the writing of point \promptarg{point index}. Write it **very shortly** in 1$\sim$2 sentence and do not continue with other points!}{\promptarg{point index}. \promptarg{point skeleton}}
  \label{prompt:tp}
\end{promptenv}

\myparatightestn{Parallel point-expanding. }
For proprietary models with only API access, we can issue multiple parallel API calls. For open-source models, we let the model process the point-expanding requests as a batch (paddings are added to the left of the point-expanding requests). %One immediate question is that compared to processing only one sequence, how much peak memory overhead and latency increase will be brought by processing a batch of sequences.

Please refer to \cref{app:implementation_details} for more implementation details.


\subsection{Why \methodshort{} Reduces Decoding Latency}
Before diving into detailed efficiency evaluation results, we can first grasp a high-level understanding of why \methodshort{} can bring significant end-to-end speed-up. For simplicity, let us focus on the point-expanding stage here, and leave the consideration of the overhead of the skeleton stage to the actual efficiency evaluation.

\myparatightestn{API-based models with parallel API calls.}
The vanilla approach sends one API request to the server, whereas \methodshort{} sends multiple API requests in parallel to get different parts of answers. Empirically, we observe that the latency of the APIs we use in the paper (\cref{app:implementation_details}) is positively correlated with the number of tokens in the response. If the number of requests does not hit the rate limit, \methodshort{} would obviously lead to a speed-up.


\myparatightestn{Open-source models with batched decoding.}
The vanilla approach processes only one question and decodes the answers sequentially, whereas \methodshort{} processes multiple point-expanding requests and the answers in a batch.
We focus on the following question: ``Compared to processing only one sequence, how much peak memory overhead and latency increase will be brought by processing a batch of sequences?''
%\todo{Move this to 3.1 if 3.1 illustrate how we prompt, and also how we implement the acceleration (refer to some short code snippet?), parallel API call and batched decoding}

% \begin{table}[tb]
% \centering
%   \begin{tabular}{ccc}
%     \toprule
%      Model   & Prefill/Generation Latency (ms) & Prefill/Generation GPU Utilization  \\ \midrule
%     \Remark{llama}-7B  &  109.59 / 96282.14  &    41.18\% / 0.099\%   \\
%     \Remark{llama}-13B &  185.79 / 117683.18 &  46.89\% / 0.156\% \\
%     \Remark{llama}-33B &    427.45 / 193354.60  & 50.88\% / 0.244\%  \\
%     \bottomrule
%   \end{tabular}
%   \caption{The latency and GPU utilization of inferencing LLMs on NVIDIA A100. The number of prefilling tokens is 1024, and the number of generated tokens is 2048.}
%   \label{tab:inf-latency}
% \end{table}

\begin{table}[tb]
\centering
  \begin{tabular}{ccc}
    \toprule
     Model   & Prefill/Decode Latency (ms) & Prefill/Decode GPU Perf. (TFLOPS) \\ \midrule
    \Remark{llama}-7B  &  40  / 2735  & 43 / 0.31 \\
    \Remark{llama}-13B &  54  / 3725  & 62 / 0.44 \\
    \Remark{llama}-33B &  100 / 5506  & 85 / 0.75 \\
    \bottomrule
  \end{tabular}
  \caption{The latency and average GPU performance of the prefilling and decoding phases when inferencing LLMs. The prefilling token length is 128, the decoding token length is 64, and the batch size is 1. The test is run on one NVIDIA A100 GPU.}
  \label{tab:inf-latency}
\end{table}

A typical LLM generative process consists of two phases: (1) the prefilling phase in which the prompt is parsed to generate the key-value cache for further use, and (2) the decoding phase in which tokens are generated one by one in a sequential manner. The decoding phase accounts for the majority of the end-to-end latency, especially when generating a long response. As shown in \cref{tab:inf-latency}, when running Vicuna-7B on NVIDIA A100-80G, the actual computing performance is only 0.31 TFLOPS (0.1\% utilization) in the decoding phase, compared to 40 TFLOPS (12.8\% utilization) during prefilling. The utilization is calculated with respect to the FP16\footnote{All of our experiments are run with FP16 inference.} tensor core peak performance -- 312 TFLOPS for NVIDIA-A100. As a result, the latency of decoding only one token is comparable to that of prefilling 128 tokens (40ms).
This huge gap in actual computing performance and thereby the latency arises from the fact that all LLM weights need to be loaded onto the GPU chip at least once only for decoding one token, so the decoding is heavily bottlenecked by the I/O of weights and the GPU computation units cannot be well utilized.

When conducting batched decoding, as the sequence batch size $B$ increases, the latency of decoding one token for each sequence stays roughly the same (\cref{fig:efficiency-batch-vs-single-latency}), as the amount of LLM weights that needs to be loaded onto the chip does not change.
As a result, the GPU computation utilization ($\frac{\text{Actual GPU Performance}}{\text{Peak GPU Performance}}$) increases almost linearly as $B$ increases (\cref{fig:efficiency-batch-vs-single-perf}). In other words, for generating a final answer of length $N$, if we cut the answer into $B$ segments of length $N/B$ and decode them as a batch, we can get a $B \times$ decoding speed-up compared to sequential decoding. Nevertheless, in practice, as prefilling longer requests brings some overhead, and the lengths of the $B$ segments could be imbalanced, the actual speed-up of the batched point-expanding stage compared with the original prefilling and sequential decoding process is smaller than $B$. We will discuss the detailed efficiency evaluation in \cref{sec:eval_eff}.
%cannot control the final answer length precisely nor necessarily get perfectly balanced segments,.

As for the peak memory overhead, the amount of LLM weights can be one to two orders of magnitude larger than that of all the intermediate activations as long as the prefilling token length is not too large, not to mention that most activations do not need to be saved for back-propagation during inference. Therefore, the LLM weights account for the majority of the memory footprint in our test cases. Consequently, as shown in \cref{fig:efficiency-batch-vs-single-mem}, the peak memory overhead due to the increasing size of the KV cache and activation grows at a slow pace as the batch size $B$ increases. Thanks to the small peak memory overhead, in all of our experiments, we managed to use one GPU to run \methodshort{} without seeking help from other peak memory optimization techniques (e.g., quantization~\cite{frantar2022gptq,lin2023awq}, offloading~\cite{sheng2023flexgen}).
%When the model size, prefilling token length, or batch size get larger, we can certainly employ these peak memory optimization techniques to reduce the memory footprint of activations and weights.


% Figure environment removed

% \todo{Need we simply mention how do we expand to multi-turn conversation here: We just paste the original query and the final aggregated answer in the conversation history.}


\section{Experimental Results}
\myparatightestn{Datasets.} We use \vicunadataset{} dataset~\cite{vicuna2023}, which consists of 80 questions spanning nine categories, such as coding, math, writing, roleplay, and so on. %This dataset has been used to evaluate the helpfulness of recent chat models~\cite{vicuna2023,xu2023wizardlm}. \todo{need more} % such as Vicuna \cite{vicuna2023},  and \todo{}.

\myparatightestn{Models.} We test \methodshort{} on 11 recently released models, including 9 open-source models and 2 API-based models (\cref{tab:model}). We obtain the weights of all the open-source models from HuggingFace. See \cref{app:implementation_details} for more details. 
%\todo{@lzn Let's give the detailed api endpoint and huggingface url? should we put it in table2 or in another table in the appendix... including the added two sentences maybe. Do we have other information to provide? maybe we can add an appendix section for them}


\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c|c}
        \toprule
        Access & Model Name & Institution & Released Date \\\midrule
        \multirow{8}{*}{Open-Source} &
        \Remark{llamachat7B2} \cite{touvron2023llama2} & Meta \& Microsoft & 2023/07\\
        &
        \Remark{llamachat13B2} \cite{touvron2023llama2} & Meta \& Microsoft & 2023/07\\
        &\Remark{openchat13B}~\cite{openllms23} & Tsinghua & 2023/07\\ % url: https://huggingface.co/openchat/openchat
        & \Remark{vicuna7B1.3}~\cite{vicuna2023} & LMSYS & 2023/06\\
        & \Remark{vicuna13B1.3}~\cite{vicuna2023} & LMSYS & 2023/06\\
        & \Remark{vicuna33B1.3}~\cite{vicuna2023} & LMSYS & 2023/06\\
        & \Remark{stablevicuna13B}~\cite{stablevicuna2023}& CarperAI &2023/05 \\ % https://huggingface.co/CarperAI/stable-vicuna-13b-delta
        & \Remark{ultralm13B}~\cite{ding2023enhancing} & OpenBMB \& Tsinghua & 2023/05\\
        & \Remark{vicuna7B1.1}~\cite{vicuna2023} & LMSYS & 2023/03 \\\midrule
        \multirow{2}{*}{API-Based} & \Remark{claude}~\cite{claude} & Anthropic & 2023/05\\
        & \Remark{chatgpt} & OpenAI & 2022/11\\
        \bottomrule
    \end{tabular}
    \caption{Models in our evaluation. All the open-source models are fine-tuned from \Remark{llama} models with different sizes.}
    \label{tab:model}
\end{table}


\subsection{Evaluation of Efficiency}
\label{sec:eval_eff}

%\subsubsection{Method}

% \todo{How we evaluate efficiency: which gpu, models; evaluation method for latency, utilization, and memory}

% \paragraph{Settings} We evaluate the effiency of SoT on Vicuna family (i.e., Vicuna-7b/13b/33b) with two types of GPUs (i.e., A100 and RTX3090).

\myparatightestn{API-based models.} We record the latency of every API call with \verb/start = time.time(); ...; elapsed_time = time.time() - start/, and add the latency of the skeleton API call and the slowest point-expanding API call as the \methodshort{} latency.

\myparatightestn{Open-source models.} All open-source models we currently evaluate are based on the \Remark{llama} 7B, 13B, or 33B architectures. In order to enable fast analysis, we first make a latency profiling table for each \Remark{llama} architecture on NVIDIA A100. The table contains the architecture's (1) latency for prefilling sequences of length 1 to 700 with different batch sizes (from 1 to 16), and (2) decoding one token with context of length 1 to 1024 with different batch sizes (from 1 to 16). With these latency profiling tables for the three \Remark{llama} architecture, given the number of points $K$, the token lengths of the skeleton request, skeleton response, point-expanding request, and point-expanding response, we can quickly estimate the \methodshort{} latency, by simply looking up entries in the tables and add them up. See \cref{sec:app-profiling-estimate} for a more detailed description of how we conduct the profiling and estimate the latency and actual GPU performance. % with pre-made profiling tables.

For the three 7B open-source models (i.e., \Remark{llamachat7B2}, \Remark{vicuna7B1.1}, and \Remark{vicuna7B1.3}), we also conduct experiments on NVIDIA RTX 3090. See \cref{sec:app-3090} for the results.

In addition to the above approach, %pre-profiling and latency estimation, 
we also compare the actual latency of \methodshort{} and \methodbase{} sequential generation (abbreviated as ``\methodbase{}'' in the following discussion)  in \cref{sec:app-actual-eff-test}.

\subsubsection{Speed-up Breakdown: Models}
We investigate how \methodshort{} reduces the end-to-end latency on different models. We compute the average speed-up on each model across all question categories in \cref{fig:speed-up_model_average}. We can see that the current \methodshort{} solution obtains a >$2\times$ speed-up on 6 out of 11 models (i.e., \Remark{llamachat7B2}, \Remark{llamachat13B2}, \Remark{vicuna7B1.1}, \Remark{openchat13B}, \Remark{vicuna33B1.3}, \Remark{ultralm13B}), and a >$1.8$ speed-up on \Remark{chatgpt}, \Remark{vicuna13B1.3}, \Remark{vicuna7B1.3}. \methodshort{} achieves almost no speed-up when using \Remark{stablevicuna13B}. If we exclude the \textit{math} and \textit{code} question categories at which the current \methodshort{} version is intrinsically unsuitable (see \cref{sec:eval_algo_category}), the speed-ups are a bit higher, as shown in \cref{fig:speed-up_model_nomathcode_average}.

% Figure environment removed


We report the detailed statistics about token lengths and numbers of points in \cref{fig:statistics}. In terms of the point number $B$ (\cref{fig:outline_num_points_average}), \Remark{llama2}, \Remark{vicuna7B1.1}, \Remark{vicuna7B1.3}, and \Remark{chatgpt} yield relatively fewer points (<$6$), while \Remark{stablevicuna13B} generates the largest number of points on average ($\approx 9$).

\cref{fig:naive_response_average,fig:outline_response_2_average,fig:maxlen_div_len_average} reveal that (1) \Remark{stablevicuna13B} tends to produce shorter \methodbase{} answers, and (2) \Remark{stablevicuna13B}'s longest point-expanding responses for many question categories (including coding, math, roleplay, conterfactual, and common-sense) can be as lengthy as the overall \methodbase{} answer. 
This is because that \Remark{stablevicuna13B} fails to adhere to the ``Write it **very shortly**'' instruction in the point-expanding request. Consequently, the current \methodshort{} solution cannot accelerate \Remark{stablevicuna13B} well.
Also, we can see \Remark{chatgpt} and \Remark{claude} follow the point-expanding request better and generate shorter point-expanding responses than other open-source models. 
Besides, \cref{fig:balance_average} %and \cref{fig:outline_response_2_average} 
shows that \Remark{llama2}, \Remark{chatgpt} and \Remark{claude} generate more balanced point-expanding responses. 

As for the token length of the final aggregated answer (\cref{fig:totlen_div_len_average}), employing \methodshort{} on most models results in an average answer length of $1\sim 2\times$ longer, whereas using \methodshort{} on \Remark{stablevicuna13B} results in significantly increased final answer length due to its large point numbers and long point-expanding responses.


%Using \methodshort{} on \Remark{vicuna33B1.3} and \Remark{openchat13B} achieves very high speed-ups since they both tend to generate long responses when using the naive manner (see \cref{fig:statistics}(d)). \Remark{stablevicuna13B} obtains the lowest speed-up since it tends to generate long point-expanding response when using the SoT method (see \cref{fig:statistics}(c)). Besides, \Remark{chatgpt} and \Remark{claude} tend to generate the responses with balanced length for each points (see \cref{fig:statistics}(a)).

\subsubsection{Speed-up Breakdown: Question Categories}

Then, we investigate how \methodshort{} reduces the end-to-end latency for different question categories. We compute the average speed-up for each question category across all models in \cref{fig:speed-up_category_average}. The question categories for which \methodshort{} can provide high-quality answers are marked in green, and other categories are marked in red (see \cref{sec:eval_algo_category}). We can see that current \methodshort{} can already obtain speed-up for all the question categories. For the five question categories that \methodshort{} can provide high-quality answers (i.e., \textit{knowledge}, \textit{common-sense}, \textit{generic}, \textit{roleplay}, \textit{counterfactual}), \methodshort{} can speed-up the overall answer generation process by 1.95$\times$ to 2.27$\times$ in the meantime.


% Figure environment removed

\subsubsection{Latency Breakdown: \methodshort{} Stages and Phases}

%The skeleton stage of \methodshort{} exhibits a shorter processing time on average than the point-expanding stage. And 
\cref{fig:latency_breakdown} presents the absolute latencies of \methodbase{} and \methodshort{} generations. 
Again, the speed-up of \methodshort{} compared with \methodbase{} generation is evident. We can see that the decoding phases predominantly account for the end-to-end latency. Consequently, although \methodshort{} has higher prefilling latency in the skeleton stage than the \methodbase{} generation and introduces additional point-expanding prefilling latency -- which is expected -- this has negligible impact on the overall latency and thereby the overall speed-up.

%We note that on \Remark{stablevicuna13B}, despite the average speed-up being \toupdate{1.11$\times$} by calculating the speed-up across 80 questions (as shown in \cref{fig:speed-up_model_average}), \methodshort{} does not reduce the absolute latency of processing 80 questions.

% Figure environment removed


\subsection{Evaluation of Answer Quality}
\label{sec:eval_algo}

In order to compare the answer quality of the \methodbase{} sequential generation (abbreviated as ``\methodbase{}'' in the following discussion) and \methodshort{} generation, we adopt two LLM-based evaluation frameworks: \fastchat{} \cite{zheng2023judging} and \llmzoo{} \cite{llm-zoo-2023}. The evaluation process is to present a question and a pair of answers (from \methodbase{} or \methodshort{} generation) to an LLM judge (\chatgpt{} in this paper) and ask for its preference. The response can be that \methodshort{}'s answer wins/ties/loses compared to the \methodbase{} answer. 

Here are more details about the evaluation of the answer quality:

\emph{(1) Detailed metrics:} \fastchat{} evaluation provides one metric for the general quality of the answers. In addition to a general metric, \llmzoo{} provides five detailed metrics on the answers' coherence, diversity, immersion, integrity, and relevance. 

\emph{(2) Question categories:} \fastchat{} provides two special evaluation prompts for coding and math questions for more accurate evaluation, whereas \llmzoo{} does not. Following the implementation in \llmzoo{}, we exclude math and coding questions in all \llmzoo{} evaluation results.

\emph{(3) Extentions to avoid evaluation bias.}
To avoid the potential bias from the order of the two answers presented to the LLM judge, we extend \fastchat{} and \llmzoo{} evaluation frameworks by running the evaluation twice with either ordering of the two answers. In either evaluation, a score of 1, 0, and -1 is assigned when \methodshort{} wins, ties, or loses, respectively. The final evaluation is that \methodshort{} wins/ties/loses when the sum of the two scores is positive/zero/negative. For example, if \methodshort{} wins in one evaluation and loses in the other evaluation, the result is ``tie''. If \methodshort{} wins (loses) in one evaluation and ties in the other, the result is ``win'' (``lose'').


% \myparatightestn{Outline.}
The organization of this section on answer quality evaluation is as follows. We first present the overall quality of \methodshort{} answers (\cref{sec:eval_algo_overall}), and then go into the details across different question categories (\cref{sec:eval_algo_category}), models (\cref{sec:eval_algo_model}), and metrics (\cref{sec:eval_algo_metric}).

\subsubsection{Overall Quality}
\label{sec:eval_algo_overall}

In \cref{fig:win_tie_lose_bar_GENERAL}, we show the win/tie/lose rates (the percentage of the cases when \methodshort{} wins/ties/loses compared to \methodbase{} generation) across all models and questions using the two metrics from \fastchat{} and \llmzoo{} that capture the general quality of the answers. We notice a discrepancy between the two metrics on when \methodshort{} is strictly better than the baseline (49.0\% v.s. 10.4\%). Despite that, the two metrics agree that \methodshort{} is not worse than the baseline in more than 76\% of the cases. For \fastchat{} metric, we also show the rates excluding math and coding questions that \methodshort{} is not suitable for (see \cref{sec:eval_algo_category}); \methodshort{} is not worse than the baseline in more than 90\% of the cases. \emph{This result suggests that the answers of \methodshort{} maintain good quality.}

% Figure environment removed

\subsubsection{Quality Breakdown: Question Categories}
\label{sec:eval_algo_category}
Next, we investigate how \methodshort{} performs on different question categories.
We compute \emph{net win rates} (win rates minus lose rates) across all question categories in \cref{fig:net_win_rates_category}. Similar to \cref{fig:win_tie_lose_bar_GENERAL}, we see that \llmzoo{} tends to be more optimistic about the quality of \methodshort{} than \fastchat{}. 
Nevertheless, the conclusions are consistent: \methodshort{} performs relatively \emph{well} on generic, common-sense, knowledge, roleplay, and counterfactual. \methodshort{} performs relatively \emph{badly} on writing, fermi, math, and coding. %As explained in \cref{sec:eval_algo}, math and coding are only evaluated in \fastchat{}, and they have the worst net win rates.

% Figure environment removed

% Note that we use the same prompt across all question categories.

We investigate the answers and summarize the findings below. 

\myparatightestn{Categories with low net win rates.}

\emph{(1) Math.} Math questions require step-by-step thinking. Without knowing the previous steps, it is hard to derive the following steps. Chain-of-Thoughts \cite{kojima2022large,wei2022chain} emphasize the importance of expanding the sequential thinking steps to solve these problems successfully. In contrast, \methodshort{} requires the models to (a) come up with the skeleton of the solution first and (b) deduce each individual step independently without referring to previous results. Both of these are challenging tasks.
Strong models are able to get (a) correct but fail at (b). 
In the following example, \Remark{chatgpt} gets the skeleton steps correct. However, without knowing the results from the first step, the model starts making mistakes in the second step.\footnote{One additional (minor) issue of the API-based \Remark{chatgpt} is that the skeleton point ``1.'' is repeated in the answer. We will discuss this issue in \cref{app:implementation_details_partial_answer}. } 

\example{\Remark{chatgpt}}{(Math) Solve for x in the equation 3x + 10 = 5(x - 2).}{\outline{1. Move all x terms to one side.} 1. Move all x terms to one side by subtracting 5x from both sides, giving -2x + 10 = -10.

\outline{2. Move all constant terms to the other side.} This can be done by subtracting 5x - 10 from both sides, leaving 3x - 5x = -10 - 10.

\outline{3. Simplify the equation.} Combine like terms on both sides of the equation to get 3x + 10 = 5x - 10.

\outline{4. Divide both sides by the coefficient of x.} Divide both sides by 2 to get x = -5.

\outline{5. Check the solution by plugging it back into the original equation.}Substitute the value of x obtained in step 4 back into the original equation and verify that both sides are equal.}{incorrect}{3x + 10 = 5(x - 2) can be simplified as follows:

3x + 10 = 5x - 10 (distribute the 5 on the right side)
10 + 10 = 5x - 3x (subtract 3x from both sides)
20 = 2x (combine like terms on the right side)
x = 10 (divide both sides by 2)

Therefore, the solution for x in the equation 3x + 10 = 5(x - 2) is x = 10.}{correct}

For weaker models, it is hard to even get step (a) correct. For example, as shown below,  the third step ``Apply FOIL'' in \Remark{vicuna13B1.3}'s \methodshort{} solution comes out of nowhere. This makes \methodshort{}'s solution incorrect (though the solution from the \methodbase{} generation is also incorrect).

\example{\Remark{vicuna13B1.3}}{(Math) Solve for x in the equation 3x + 10 = 5(x - 2).}{\outline{1. Simplify equation.}

We can simplify the equation by combining like terms:

3x + 10 = 5(x - 2)
=> 3x + 10 = 5x - 10
=> x + 5 = 2
=> x = 2 - 5
=> x = -3

\outline{2. Factor 5(x - 2).}

5(x - 2) = 5x - 10.

\outline{3. Apply FOIL.}

* Multiply the binomials in the equation, using the first term of each binomial for the first term of the product, the outside term for the second term of the product, the second term of each binomial for the third term of the product, and the remaining terms for the fourth term of the product.
* 3(x - 2) + 10 = 5x + 5(-2)
* 3x - 6 = 5x - 10
* 3x = 5x - 10
* x = 5x - 10 / 3
* x = 5/3x - 10/3
* x = 2/3x - 5/3

\outline{4. Solve for x.}

After applying the FOIL method and simplifying the equation, we get 3x = 17. Therefore, x = 17/3.}{incorrect}{To solve for x in the equation 3x + 10 = 5(x - 2), we need to first distribute the 5 to multiply both sides by 5.

3x + 10 = 5(x - 2)

5(3x + 10) = 5(x - 2)

15x + 50 = 5x - 10

Add 5x to both sides:

20x + 50 = 5x

Subtract 5x from both sides:

15x = 0

Divide both sides by 15:

x = 0

Therefore, the solution to the equation is x = 0.}{incorrect}

\emph{(2) Fermi.}
Similar to math questions, questions in the fermi category require solvers to make assumptions and calculate the answer step-by-step. For example, to answer the question in the figure below, the solver is required to make an assumption about when life began on Earth. In step 4, \Remark{chatgpt} does not know that it has already assumed 3.5 billion years in step 3 and uses the wrong number 4.54 billion in the calculation. In step 5, \Remark{chatgpt} does not know that it has already made the above assumptions and refuses to answer ``due to the uncertainties
in estimating the age of the Earth''. In contrast, \methodbase{} generation is able to create a consistent solution.


\example{\Remark{chatgpt}}{(Fermi) How many times has the Earth orbited the Sun since the beginning of life? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.}{\outline{1. Estimate the age of the Earth.} The age of the Earth is estimated to be around 4.54 billion years, based on radiometric dating of rocks and meteorites.

\outline{2. Determine when life began on Earth.} Life on Earth is estimated to have begun around 3.5 billion years ago, based on the discovery of ancient microfossils.

\outline{3. Calculate the number of years since life began.} The oldest known fossils suggest that life on Earth began around 3.5 billion years ago. Therefore, the number of years since life began on Earth is approximately 3.5 billion years.

\outline{4. Divide the number of years by the length of one orbit.} Assuming an orbital period of 365.25 days, the Earth has orbited the Sun approximately 4.54 billion / 365.25 = 12,425,000 times since the beginning of life.

\outline{5. The Earth has orbited the Sun approximately X times since the beginning of life.} Unfortunately, it is impossible to give a specific number for the amount of times the Earth has orbited the Sun since the beginning of life due to the uncertainties in estimating the age of the Earth and when life first emerged.}{incorrect}{The beginning of life on Earth is estimated to have occurred around 3.5 to 4 billion years ago. Since that time, the Earth has been orbiting the Sun continuously. 

The length of one orbit, or one year, is approximately 365.25 days. This means that the Earth completes one full orbit around the Sun every 365.25 days. 

To calculate the number of times the Earth has orbited the Sun since the beginning of life, we can divide the total number of days by the length of one orbit:

Total number of days since the beginning of life = (3.5 billion years) x (365 days per year) = 1.2775 trillion days

Number of orbits = (1.2775 trillion days) / (365.25 days per orbit) = 3.5 billion orbits

Therefore, the Earth has orbited the Sun approximately 3.5 billion times since the beginning of life.}{}

As the following example shows, there are also cases when \methodshort{} solves the question (successfully) by embedding the computation steps in the skeleton directly in the skeleton stage. The point-expanding stage will then add little content (except some undesired repetition of the following skeletons occasionally). This happens both on small/weak models and large/powerful models. Although this behavior is not what we originally expect, it sheds light on a promising direction for making \methodshort{} a general framework: in the problem where step-by-step thinking is needed, the \methodshort{} model automatically falls back to the sequential generation mode by outputting the complete answer in the skeleton stage and skipping the point-expanding stage. Future work needs to be done to control and encourage this behavior better.

\example{\Remark{vicuna7B1.1}}{(Fermi) How many times does the average human blink in a lifetime? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.}{\outline{1. The average human blinks 12-15 times per minute.}

\outline{2. There are 60 minutes in an hour.}

\outline{3. Therefore, the average human blinks 60 x 12-15 = 720-900 times per hour.}

\outline{4. There are 24 hours in a day.}Therefore, the average human blinks 720-900 x 24 = 17,520-22,400 times per day.

\outline{5. Therefore, the average human blinks 720-900 x 24 = 17,520-22,400 times per day.}This translates to blinking approximately 50-65 times per hour, which is a significant portion of the day.

\outline{6. There are 365 days in a year.}Therefore, the average human blinks 17,520-22,400 x 365 = 5,857,600-8,124,000 times per year.

\outline{7. Therefore, the average human blinks 17,520-22,400 x 365 = 5,857,600-8,124,000 times per year.}

\outline{8. The human lifespan is typically 70-80 years.}Therefore, the average human blinks 80,000-112,000 times in their lifetime.

\outline{9. Therefore, the average human blinks 5,857,600-8,124,000 / 70-80 = 80,000-112,000 times in their lifetime.}}{}{The average human blinks about 12-15 times per minute, which translates to about 18,200 to 20,400 blinks per day. Over the course of a lifetime, the number of blinks is estimated to be around 7,500,000 to 8,000,000.

Blinking is a reflexive action that helps to keep the eyes moist and comfortable, as well as to protect the eyes from dust and other foreign particles. The frequency of blinking can vary depending on a variety of factors, including age, health, and environmental conditions. For example, people who work in dusty or dry environments may blink more frequently than those who work in a clean, moist environment.

Over the course of a lifetime, the average person will spend approximately 27,375 days or 700,000 hours awake. This means that the majority of the time spent blinking will occur in the first few decades of life, with the frequency of blinking gradually decreasing as people age.

It is important to note that there is a significant amount of individual variation in the frequency of blinking. Some people may blink more or less than the average, depending on a variety of factors such as age, health, and environmental conditions. Additionally, blinking patterns can be affected by factors such as stress, fatigue, and certain medical conditions.}{}



\emph{(3) Coding.}
While in most cases, the models are able to decompose the coding problem into smaller tasks in the skeleton stage, the generation quality during the point-expanding stage is bad, potentially due to the fact that we did not carefully craft a special point-expanding prompt for coding. The failure cases can be grouped into two cases.
(a) In some cases, the model only generates a description of how implementation can be done without giving the code.

\examplesot{\Remark{claude}}{(Coding) Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.}{ \outline{1. Open file stream} by declaring an ifstream object and using the open() method to open the file in read mode.

 \outline{2. Read line} by line from the file stream into a string using getline().

 \outline{3. Find word} by searching each line for the target word using string::find and keeping track of matches.

 \outline{4. Increment counter} by one each time the specified word is encountered in the current line.

 \outline{5. Close file}

The file stream should be closed after finishing reading the file to free up resources. This can be done by calling the close() method on the file stream object.}{}

This might be due to the fact that the powerful models (e.g., \Remark{claude}) follow the point-expanding prompt ``Write it **very shortly** in 1$\sim$2 sentence'' precisely. Better prompts tailed to coding could help. (b) In some other cases, the model tends to generate the complete code repeatedly in several points.

\examplesot{\Remark{openchat13B}}{(Coding) Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.}{\outline{1. Read file line by line.}

\outline{2. Store lines in a container.}

\outline{3. Count occurrences of specific word.}
```cpp
\hide{}
```
This code uses the C++ standard library to read a text file line by line, store each line in a container, and count the occurrences of a specific word. The results are printed.

\outline{4. Print results.}

Here's the C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file:
```cpp
\hide{}
```
Please note that this is a very basic implementation. You may need to adjust it depending on your specific requirements.}{}
This problem might also be alleviated by prompts that explicitly tell the model to only generate a ``code snippet'' in each skeleton point.


%the answer shouldn't be separated into several parallely decoding segments. 
% Separating them into several pieces would result in strong dependencies between the pieces,
%solving the coding questions in  \vicunadataset{} well with \methodshort{}  might be challenging and . 
% Note that coding problems in \vicunadataset{} can usually be implemented in a few lines. 
% Separating them into several pieces means that there will be strong dependencies between the pieces. For example,
Although better prompt engineering could lead to better results, we expect that \methodshort{} is not suitable for the coding questions in \vicunadataset{}. The reason is that the solutions to the problems in \vicunadataset{} are usually in a few lines with strong dependencies between each other. 
Without knowing the previously defined variable names or imported libraries, it is hard (or even impossible) to implement the subsequent code correctly. 
As a consequence, generating different parts of the answers in parallel is not suitable here. 
Similar to the math questions, automatically falling back to outputting all the code in the outline stage and not triggering the point-expanding stage might be more suitable for answering this type of questions.

However, we expect that \methodshort{} could be helpful for larger coding tasks that involve multiple modules (e.g., functions, classes). The skeleton stage could be in charge of defining the interfaces between different modules (e.g., functionalities, names, parameters, return types). The point-expanding stage could be in charge of implementing these modules or using these modules to complete the final task, which can be done in parallel. This mimics the common practice in software engineering. 

%Ideally, we want each skeleton point to focus on one sub-module (e.g., functions, classes) of the code, and the aggregated answers represent the entire code. However, 

\emph{(4) Writing.} The writing questions usually ask for an email, a blog post, or a review in a given scenario. 
In the detailed evaluation output from \fastchat{} and \llmzoo{} metrics, the major and common complaint is that \methodshort{}'s answers are (slightly) less detailed. This could potentially be improved by point-expanding prompts that request more details.

While inspecting \methodshort{}'s answers manually, we observe one additional issue. 
Writing problems usually require a coherent passage \emph{without} embedded skeleton points, whereas our current \methodshort{} pipeline (\cref{sec:method}) concatenates skeleton points as part of the answer. One example is shown below.

\example{\Remark{chatgpt}}{(Writing) Can you help me write a formal email to a potential business partner proposing a joint venture?}{\outline{1. Introduce yourself and your company.} Dear [Recipient], 

I hope this email finds you well. My name is [Your Name] and I am the [Your Position] at [Your Company Name].

\outline{2. State the purpose of the email.} The purpose of this email is to propose a joint venture between our companies.

\outline{3. Explain the potential benefits of the joint venture.} By combining our resources and expertise, we can increase market share and profitability for both companies.

\outline{4. Provide details on the proposed collaboration.} Our proposed collaboration would involve combining our expertise in marketing with your strong manufacturing capabilities to create a new line of products.

\outline{5. Suggest a follow-up meeting or call.} Please let me know your availability for a follow-up call next week to discuss this opportunity further.

\outline{6. Thank the recipient for their time and consideration.}Thank you for considering this proposal and we look forward to hearing back from you soon.}{}{Of course, I can help you with that. Here's a sample email you can use as a guide:

Subject: Proposal for Joint Venture

Dear [Business Partner’s Name],

I hope this email finds you well. I am writing to propose a joint venture between our companies, [Your Company Name] and [Potential Partner Company Name]. We believe that our respective expertise can create a successful partnership that will benefit both of our businesses.

We have conducted thorough research on your company and admire your innovative approach to [insert industry or niche]. We think our [insert expertise or specialty] could complement your [insert potential partner’s specialty or expertise]. By collaborating on this joint venture, we believe we can create a new and innovative product that will appeal to our respective markets.

We are confident that our companies would work well together and that our joint venture would be successful. We would like to set up a meeting to discuss the specifics of our proposal and explore how we can move forward with the joint venture.

Thank you for considering this proposal. We look forward to hearing from you soon.

Best regards,

[Your Name]}{}
In such cases, simply removing the skeleton points would greatly improve the answer quality. To make \methodshort{} more general, one future direction is to let the LLM itself decide whether the point index and point skeleton should be included to make the final answer more natural and fluent.
%s indicate whether skeleton points should be included in the final answer in the skeleton stage so that \methodshort{} can make the optimal decision adaptively.


\myparatightestn{Categories with high net win rates.}

\emph{(1) Counterfactual, knowledge, common sense, generic.} All these four categories have the same characteristic: the ideal answer should cover several relatively \emph{independent} points. 
Having the LLM generate a skeleton before expanding the details can result in more comprehensive discussions for the question. Besides, organizing the answers as a list of points makes the answers easier to read, whereas the answers from \methodbase{} generation are sometimes less structured and less readable. 
%Therefore, \methodshort{} can produce answers to each point in parallel with good quality.
Below, we give one example for each question category to illustrate these points.

\example{\Remark{chatgpt}}{(Counterfactual) What if the Internet had been invented during the Renaissance period?}{\outline{1. Advancements in communication.}The invention of the Internet during the Renaissance period would have revolutionized communication, allowing for faster and more efficient exchange of ideas and information across vast distances.

\outline{2. Increased access to knowledge.}The invention of the printing press during the Renaissance period already allowed for greater access to knowledge, but the internet would have revolutionized this even further by providing instantaneous access to information from all over the world.

\outline{3. Improved dissemination of ideas.}If the internet had been invented during the Renaissance period, the spread of ideas and knowledge would have been much faster and widespread than it was at the time. This could have led to a faster progression of scientific and artistic innovations.

\outline{4. Impact on art and literature.}The invention of the internet during the Renaissance period would have revolutionized the way art and literature are created, shared, and consumed, leading to a more diverse and globally-influenced artistic and literary scene.

\outline{5. Potential for global trade.}The invention of the internet during the Renaissance period would have greatly expanded global trade opportunities, allowing for easier communication and collaboration between merchants and traders across continents.

\outline{6. Influence on political and social structures.}The invention of the internet during the Renaissance period could have led to significant changes in political and social structures, allowing for greater democratization and communication among people from different regions and classes.}{}{If the Internet had been invented during the Renaissance period, it would have had a significant impact on the way people communicated, shared ideas, and gained knowledge. The Renaissance was a time of great intellectual and artistic growth, and the Internet would have facilitated the spread of these ideas and innovations.

Scholars and artists would have been able to share their work more easily, and people from different parts of the world would have been able to connect and collaborate more easily. The Internet would have also made it easier to access information, which could have accelerated the pace of scientific and technological advancements.

However, it's important to note that the infrastructure required for the Internet to exist did not exist during the Renaissance period. There were no telecommunications networks or electricity grids, so the idea of the Internet would have been impossible to realize. Nonetheless, it's interesting to think about how the Renaissance might have been different if the Internet had existed at that time.}{}

\example{\Remark{chatgpt}}{(Knowledge) What are some potential implications of using a single-use plastic bottle versus a reusable bottle on both the environment and human health?}{\outline{1. Single-use plastic waste.}Single-use plastic bottles contribute to the massive amount of plastic waste that ends up in landfills and oceans, harming wildlife and ecosystems.

\outline{2. Pollution of waterways.}Single-use plastic bottles contribute to the pollution of waterways, as they often end up in rivers, lakes, and oceans, causing harm to aquatic life and the overall ecosystem.

\outline{3. Harmful chemicals leaching.}Single-use plastic bottles may release harmful chemicals such as bisphenol A (BPA) and phthalates into the water or liquid they contain, posing a risk to human health.

\outline{4. Greenhouse gas emissions.}The production and disposal of single-use plastic bottles contribute to greenhouse gas emissions, which contribute to climate change. Using a reusable bottle reduces the need for constant production and disposal, therefore reducing greenhouse gas emissions.

\outline{5. Microplastic ingestion.}Single-use plastic bottles contribute to the accumulation of microplastics in the environment, which can be ingested by humans through food and water, leading to potential health risks.

\outline{6. Increased risk of disease.}Single-use plastic bottles can harbor harmful bacteria, leading to an increased risk of disease if not disposed of properly. Reusable bottles, when cleaned regularly, pose a lower risk.}{}{Using a single-use plastic bottle has significant implications on the environment and human health. 

Environmental implications: 

1. Plastic pollution: Single-use plastic bottles are one of the major contributors to plastic pollution in oceans and landfills. They take hundreds of years to decompose, and during that time, they release toxic chemicals and microplastics into the environment.

2. Carbon emissions: Manufacturing and transporting single-use plastic bottles require a significant amount of energy and resources, which contribute to greenhouse gas emissions and climate change.

3. Natural resource depletion: The production of single-use plastic bottles requires large amounts of water and oil, which are finite resources.

Health implications:

1. Chemical exposure: Single-use plastic bottles contain chemicals such as Bisphenol A (BPA) and phthalates, which can leach into the water and pose a risk to human health.

2. Microplastics ingestion: When plastic bottles break down, they release microplastics, which can enter the food chain and be ingested by humans.

Using a reusable bottle, on the other hand, has a positive impact on the environment and human health. 

Environmental benefits:

1. Reduced plastic waste: Reusable bottles significantly reduce the amount of plastic waste generated.

2. Lower carbon emissions: Reusable bottles have a lower carbon footprint as they can be used multiple times.

3. Reduced resource depletion: The production of reusable bottles requires fewer resources than single-use plastic bottles.

Health benefits:

1. Chemical-free: Reusable bottles are typically made from materials that are free from harmful chemicals, reducing the risk of chemical exposure.

2. Reduced microplastics: Reusable bottles do not break down easily, reducing the release of microplastics into the environment.

Overall, using a reusable bottle is a more sustainable and healthier option than using a single-use plastic bottle.}{}

\example{\Remark{chatgpt}}{(Common sense) How can you determine if a person is genuinely interested in a conversation or simply being polite?}{\outline{1. Pay attention to body language.}Notice if their posture is open or closed, if they are facing towards you or away, and if they are fidgeting or relaxed. These can all indicate their level of interest in the conversation.

\outline{2. Listen for engaged responses.}If the person is genuinely interested in the conversation, they will respond with thoughtful and relevant comments or questions rather than short, generic responses.

\outline{3. Observe eye contact.}If the person maintains steady eye contact and their gaze is focused on you, they are likely genuinely interested in the conversation. On the other hand, if they frequently look away or seem distracted, they may just be being polite.

\outline{4. Look for follow-up questions.}If the person asks follow-up questions and seems genuinely interested in what you're saying, then they are likely interested in the conversation and not just being polite.

\outline{5. Notice tone of voice.}A person who is genuinely interested in a conversation will often have a more enthusiastic and animated tone of voice, compared to someone who is simply being polite and giving short, monotone responses.}{}{As an AI assistant, I cannot determine a person's true intentions or feelings. However, there are some social cues that can indicate whether a person is genuinely interested in a conversation or not. For example, if they maintain eye contact, lean in, nod their head and ask follow-up questions, it can be a sign that they are engaged in the conversation. On the other hand, if they avoid eye contact, give short or vague responses, or seem distracted, it may indicate that they are not as interested. Ultimately, it's important to remember that everyone has different communication styles, and it can be difficult to accurately interpret someone's intentions without directly asking them.}{}

\example{\Remark{chatgpt}}{(Generic) How can I improve my time management skills?}{\outline{1. Set clear goals.} Setting clear goals is important in improving time management skills as it helps you to focus on what needs to be done and what can be accomplished within a certain timeframe.

\outline{2. Prioritize tasks.} Make a list of tasks and prioritize them based on their importance and deadline. This will help you focus on the most critical tasks and avoid wasting time on less important ones.

\outline{3. Create a schedule.} Creating a schedule helps you allocate time for each task and ensures that you complete them within the allotted time.

\outline{4. Minimize distractions.} Distractions such as social media, emails, and phone notifications can waste a lot of time. Try to limit them by turning off notifications or setting specific times to check them.

\outline{5. Take breaks.} Taking breaks can actually improve productivity and help prevent burnout. It's important to schedule short breaks throughout the day to recharge and refocus.

\outline{6. Learn to say no.} It's important to prioritize your own tasks and not take on too much at once. Saying no to requests that don't align with your goals or schedule can help you manage your time more effectively.

\outline{7. Delegate tasks.} If you have too many tasks to handle, consider delegating some of them to others who can help you. This will allow you to focus on more important tasks and manage your time more effectively.

\outline{8. Use time management tools.}There are a variety of time management tools available, such as calendars, to-do lists, and productivity apps, that can help you stay organized and on track with your tasks.}{}{Improving time management skills can be a challenge, but there are several strategies that can help. Here are a few:

1. Set clear goals and priorities: Identify your most important tasks and make sure to complete them first.

2. Make a schedule: Create a schedule that includes your most important tasks and set aside time for them.

3. Use a timer: Set a timer for a specific amount of time to help you stay focused and avoid distractions.

4. Take breaks: Take short breaks throughout the day to help you recharge and avoid burnout.

5. Eliminate distractions: Try to limit distractions as much as possible, such as by turning off your phone or closing your email.

6. Delegate tasks: If possible, delegate tasks to others to free up time for more important activities.

Remember, improving time management skills takes practice and perseverance. Be patient with yourself and keep trying until you find strategies that work well for you.}{}


\emph{(2) Roleplay.} Roleplay questions require LLMs to assume a character and present an introduction or speech to other people. \llmzoo{} and \fastchat{} metrics usually favor the level of details of \methodshort{} when assigning it a better score. Similar to the writing category discussed before, \methodshort{}'s answers could be even better if the skeleton points are removed from the answer.

\example{\Remark{claude}}{(Roleplay) How would you introduce yourself as a medieval knight at a royal banquet?}{\outline{1. Name and house.} My name is Sir Percival of Camelot, knight of the Round Table and protector of King Arthur's realm.

 \outline{2. Lands and titles.} I hold fiefdom over the lands of Westmarch and Ravenshire, and bear the titles of Baron of the Western Marches and Knight of the Raven.

 \outline{3. Military feats.} I have led my men to great victories against the kingdom's enemies, winning many battles through strength of arms and tactical cunning.

 \outline{4. Allegiance to king.} I pledge my sword, my loyalty, and my life to serve and protect my noble king and this great kingdom.

 \outline{5. Request to join table.} Your Highness, it would be the greatest honor to join your exalted table and share in this magnificent feast with my fellow knights and loyal subjects.}{}{Greetings, your majesty and honored guests. I am Sir Claude, a knight in service of the king. I hail from the province of Assistantia, where I was trained in the knightly virtues of chivalry, courage and courtesy. Though I am but a humble knight, I shall endeavor to serve you and this kingdom with all the strength and valor I possess. I thank you for allowing me to join you at this royal feast, and hope only that my conduct brings further honor to my liege and this noble court. Please let me know if I can be of any service during the celebrations, as it would be my honor to assist you in any way.}{}


\myparatightestn{Summary.} 
\methodshort{} performs well when the question can be answered in several points whose details can be expanded independently. This includes a wide range of questions in the real world. 
On the other hand, it is fundamentally challenging to apply \methodshort{} on questions that require step-by-step thinking, in which the latter steps require the details from the earlier steps, such as math questions. 
To make \methodshort{} general across broader question categories, one promising pathway is to enable \methodshort{} to adaptively fall back to 1-stage sequential generation without triggering the point-expanding stage according to the question. Some of our results suggest that some LLMs are already able to do that occasionally without special prompting or tuning.

\subsubsection{Quality Breakdown: Models}
\label{sec:eval_algo_model}
Next, we investigate how \methodshort{} performs on different models. We compute net win rates across all models in \cref{fig:net_win_rates_model}. Again, we see that the two general metrics from \fastchat{} and \llmzoo{} have different absolute values but similar rankings. In particular, both metrics agree that \Remark{openchat13B}, \Remark{vicuna7B1.1}, \Remark{claude}, \Remark{chatgpt} have \emph{low} net win rates, whereas \Remark{vicuna13B1.3}, \Remark{stablevicuna13B}, and \Remark{ultralm13B} have \emph{high} net win rates. 

% Figure environment removed

We investigate the answers and summarize the findings below.

\myparatightestn{Models with low net win rates.}
We observe two types of reasons for the low net win rates. 

\emph{(1) \Remark{openchat13B} and \Remark{vicuna7B1.1}.} For the weak models such as \Remark{openchat13B} and \Remark{vicuna7B1.1}, they are not able to follow the \methodshort{} prompts precisely. For \Remark{openchat13B}, sometimes the skeleton contains undesired contents (e.g., completing the skeleton points as shown below). 
\examplesot{\Remark{openchat13B}}{How many lightning strikes occur on Earth each day? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.}{\emph{(Only showing the skeleton)}\\\outline{1.Introduction.
2. The Earth's atmosphere.
3. Lightning formation.
4. Electrical discharge.
5. Lightning strikes per second.
6. Calculating the total.

Skeleton:
1. Introduction.
Lightning is a beautiful and awe-inspiring natural phenomenon, but it can also be dangerous. It occurs when positively charged particles in a thundercloud discharge to the negatively charged Earth. The process is still not fully understood, but scientists have been able to estimate how many lightning strikes occur on Earth each day.

Skeleton:
1. The Earth's atmosphere.

The Earth's atmosphere plays a crucial role in the formation of lightning. It consists of different layers, with the troposphere being the lowest and most active. The troposphere is where most of the weather occurs, and it is where lightning primarily forms. The atmosphere's temperature, humidity, and wind patterns all contribute to the likelihood of a lightning strike.

Skeleton:
2. Lightning formation.

When warm, moist air rises, it cools and forms water \hide{droplets or ice crystals. If these particles become large enough, they can generate an electric field. As they move through the atmosphere, they gain or lose electrons, becoming negatively or positively charged. This process is called triboelectric effect.

Skeleton:
3. Electrical discharge.

Eventually, the positive and negative charges in the cloud attract each other. The process is called electrical discharge, and it results in a flash of lightning. The distance between the clouds and the ground, as well as the amount of charge, determines the type of lightning (e.g., cloud-to-ground or intracloud).

Skeleton:
4. Lightning strikes per second.

Scientists estimate that the Earth experiences about 100 lightning strikes per second. However, this number can vary greatly depending on factors such as the region, weather conditions, and time of year.

Skeleton:
5. Calculating the total.

It is difficult to provide an exact number for the total number of lightning strikes per day because of the varying factors. However, considering that there are about 9,1}}}{}
For both \Remark{openchat13B} and \Remark{vicuna7B1.1}, sometimes they do not write anything in the point-expanding stage when details are needed.
\examplesot{\Remark{vicuna7B1.1}}{As a superhero, how would you explain your origin story to a curious child?}{\outline{1. Born with extraordinary abilities.}

\outline{2. Trained to use powers for the greater good.}

\outline{3. Protecting the innocent and fighting villains.}I use my powers to defend the defenseless and bring justice to those who have been wronged.

\outline{4. Inspired by courage, justice, and selflessness.}}{}
These two issues result in bad net win rates.

\emph{(2) \Remark{claude} and \Remark{chatgpt}.} For strong models such as \Remark{claude} and \Remark{chatgpt}, we do not see major issues with \methodshort{}'s answers except the common issues across all models discussed in \cref{sec:eval_algo_category}. We hypothesize that the low net win rates are majorly due to the fact that their \methodbase{} sequential generation is already very good. 


\myparatightestn{Models with high net win rates.} The models with high net win rates (\Remark{vicuna13B1.3}, \Remark{stablevicuna13B}, and \Remark{ultralm13B}) are between the above two extremes. On one hand, they are good enough to understand \methodshort{} prompts. While we observe that they sometimes have the same issues as \Remark{openchat13B} and \Remark{vicuna7B1.1} discussed before, these issues happen much less frequently.  On the other hand, their \methodbase{} sequential generation performance has a larger room for improvement than \Remark{claude} and \Remark{chatgpt} so that the benefits from \methodshort{}  are more visible on these models.

\myparatightestn{Summary.}
%that are too weak to understand
In its current form, \methodshort{} does not work well for some models that cannot understand the skeleton and point-expanding prompts well. For models that are able to understand the \methodshort{} prompts, the answer quality might get improved. We expect that further improving \methodshort{} prompts or fine-tuning the models can make it easier for LLMs to understand the skeleton and point-expanding prompts and ultimately result in better answer quality.



\subsubsection{Quality Breakdown: Metrics}
\label{sec:eval_algo_metric}
All previous evaluations use metrics about the general quality of the answer.
In \cref{fig:win_tie_lose_bar_DETAILS}, we show more detailed metrics from \llmzoo{} to reveal in which aspects \methodshort{} can improve or hurt the answer quality. On average, we can see that \methodshort{} improves the diversity and relevance while hurting the immersion and coherence.

% Figure environment removed

We investigate the answers and summarize the findings below.

\myparatightestn{Metrics with low net win rates.}

\emph{(1) Coherence.}
In the prompt of \llmzoo{} evaluation, coherence is defined as ``The response should be coherent and flow logically from one point to the next that is easy to read and understand without major gaps or inconsistencies. The more coherent they are, the better.'' \methodshort{}'s answer is structured in a list of points. It is suitable for some questions but not all. For example, in the questions that require LLMs to write a passage, email, or script, the introduction and conclusion paragraphs are usually missing, and the answers usually lack transition sentences between consecutive points. These make the answers less coherent. Below is one example from \Remark{claude}.

\example{\Remark{claude}}{Write a script for a YouTube video exploring the history and cultural significance of jazz.}{\outline{1. Origins.} Jazz originated in the late 19th and early 20th centuries among African Americans in New Orleans, emerging from a combination of African rhythmic traditions, blues, ragtime, and other influences.

 \outline{2. Early jazz styles.} The early styles of jazz that emerged in the late 19th and early 20th centuries included ragtime, Dixieland, and the blues.

 \outline{3. Jazz age.} The Jazz Age was the period during the 1920s when jazz music and dance styles like the Charleston became popular cultural phenomena in American and European society.

 \outline{4. Bebop era.} Bebop emerged in the early 1940s with artists like Dizzy Gillespie and Charlie Parker experimenting with faster tempos, complex chord changes, and unpredictable melodic phrasing.

 \outline{5. Cool jazz.} Emerging in the late 1940s and early 1950s, cool jazz was a more subdued, mellow style that broke from the high-energy virtuosic improvisation of bebop.

 \outline{6. Free jazz} emerged in the late 1950s and 1960s as musicians explored free improvisation and atonality, moving away from traditional jazz structures.

 \outline{7. Fusion jazz.} Major jazz artists such as Miles Davis and Herbie Hancock began experimenting with combining elements of rock, funk, and electronic music in the 1970s, resulting in the emergence of the subgenre known as fusion jazz.

 \outline{8. Jazz festivals.} Jazz festivals emerged in the 1950s and 1960s as a way to celebrate jazz music, with famous early festivals like the Newport Jazz Festival bringing thousands of fans together to hear top performers.

 \outline{9. Jazz instruments.} The trumpet, saxophone, piano, double bass, and drum set are some of the key instruments commonly used in jazz.

 \outline{10. Influential musicians} include Louis Armstrong, Duke Ellington, Charlie Parker, Miles Davis, John Coltrane, and many others who pioneered new styles and techniques.

 \outline{11. Legacy and impact.} Jazz has left an indelible mark on music and culture over the past century, influencing countless artists and genres while spreading the spirit of improvisation and freedom across the globe.}{}{Here is a draft script for a YouTube video exploring the history and cultural significance of jazz:

Introduction

Hey everyone! Welcome to my channel. In today's video, we're going to be exploring the history and cultural impact of one of America's original art forms - jazz music. 

Jazz Origins 

Jazz grew out of the African American communities in the southern United States in the late 19th and early 20th centuries. It was influenced by a combination of African and European musical traditions. Some key elements that fed into early jazz included blues, ragtime, spirituals, work songs, field hollers, and brass band music. 

New Orleans is considered the birthplace of jazz, emerging around the turn of the 20th century. The city was a melting pot of cultures and musical styles. Musicians mixed improvisation, syncopated rhythms, bent notes and a loose ensemble playing style to create a new genre that became known as jazz. 

Early jazz greats like Buddy Bolden, Jelly Roll Morton and Sidney Bechet began fusing these various influences into the distinct jazz sound. As the music spread via travel and recordings in the 1910s, jazz captivated listeners and began to evolve into different styles.

Jazz Styles Emerge

In the 1920s, jazz expanded out from New Orleans and Chicago emerged as a hotspot. Louis Armstrong's innovative solos and expressive phrasing became influential. The swing style also developed during the 1930s and 40s, propelled by star big bands and dancers. 

Bebop arose in the 1940s with soloists like Charlie Parker and Dizzy Gillespie exploring new levels of harmonic and rhythmic complexity. In the 50s and 60s, jazz expanded into new fusions like cool jazz, hard bop, free jazz, modal jazz and soul jazz. Artists used jazz to express their unique regional, social and political identities.

Cultural Significance

Over the decades, jazz has become woven into the fabric of American society. It is equated with many values and ideals like individualism, freedom, spontaneity, innovation and social harmony.

Jazz was associated with breaking down barriers between races, classes and genres. It represented modernism and urban culture. Musicians like Louis Armstrong and Duke Ellington became some of the first widely popular African American artists.

Jazz also spread American culture and values abroad through performances, radio and film. It influenced music around the world and became a truly global art form.

Today jazz continues to thrive and evolve both in the U.S. and internationally. Its history and sounds reflect the diversity of both American and global cultures. From its folk roots, jazz has become a sophisticated art music that retains its spirit of freedom and improvisation.

Conclusion

That covers some of the key points in the history and cultural impact of jazz music. Let me know in the comments about your favorite jazz styles and musicians! Be sure to like and subscribe. See you next time.}{}

While the content of \methodshort{}'s answer is good, it reads less fluent for the purpose of a YouTube script.

\emph{(2) Immersion.}
In the prompt of \llmzoo{} evaluation, immersion is defined as ``the response should act like the assigned role using the tone, manner, and vocabulary the role would use. The more assistant-like tones, the worse. The more in-character, the better.'' 
The way that \methodshort{} breaks the answers into a list makes its answer less in-character. Therefore, \methodshort{} receives a lower immersion score. Here is one example.

\example{\Remark{claude}}{Pretend to be a world-famous chef. How would you describe your signature dish to a panel of judges?}{\outline{1. Main ingredient}: The incredibly fresh and tender salmon caught just this morning from the crystal clear waters of the Pacific Northwest.

 \outline{2. Cooking method} 

I slowly braise the main ingredient in a rich broth to bring out its natural flavors before searing it to a perfect golden crust.

 \outline{3. Special seasoning}

The dish is seasoned with a blend of fresh herbs like thyme, rosemary, and oregano which provides an aromatic backbone that pairs nicely with the natural flavors of the main ingredients. I add just a touch of smoked paprika to provide a subtle smokiness without overpowering the other flavors.

 \outline{4. Texture}: The dish has a perfect balance of crispy on the outside and tender on the inside, with each bite yielding a delightful mix of crunchy and smooth textures that dance across your tongue.

 \outline{5. Plating presentation}

I purposefully create height and dimension on the plate with the carefully arranged components of the dish, so that the dish appears vibrant and alive. The interplay of colors, textures and shapes is intended to delight the eye before the first bite.

 \outline{6. Inspiration}: This dish was inspired by my dear grandmother, who taught me the secret family recipe when I was just a child.}{}{ *clears throat and speaks in a fancy French accent*

Bonjour honored judges. Today I present to you my signature dish - la crème de la crème of French haute cuisine. Behold, le poulet à la Claude! 

This succulent chicken is firstly marinated overnight in a bath of fine herbes - thyme, rosemary, and sage cultivated from my private garden. The young spring chicken is then roasted to perfection, its skin crisping to a beautiful caramel brown. 

Nestled beside the chicken is a tower of pommes dauphine - light and airy potato puffs - alongside a decadent mound of crème fraîche infused with truffle essence. 

But the pièce de résistance is the sauce. A luxurious pool of velouté enriched with cream and butter, brightened with a dash of lemon, and studded with morsels of morel mushrooms foraged at dawn. 

One bite of this ensemble will transport you to food heaven. The tender chicken, the creaminess of the potato puffs, and the rich decadence of the sauce meld together in blissful harmony. 

So tell me honored judges, will my poulet à la Claude earn your coveted approval? I await your discerning verdict as I swirl more vintage Bordeaux into your glasses. Santé!}{}

Again, while the answer from \methodshort{} covers sufficient aspects and details, it reads rather stiffly. The \methodbase{} answer, in contrast, has the opening sentence "Bonjour honored judges. Today I present to you my signature dish..." and the conclusion statement "So tell me honored judges, will my poulet à la Claude earn your coveted approval? ..." which reads more like a  casual talk from a chef.

\myparatightestn{Metrics with high net win rates.}

\emph{(1) Diversity.} In the prompt of \llmzoo{} evaluation, diversity is defined as ``The response should be comprehensive and provide a range of information that is not limited to a single perspective. More perspectives are better.'' The skeleton stage in \methodshort{} encourages LLMs to think from multiple perspectives. Therefore, \methodshort{}'s answers tend to have better diversity. Below is one example showing that \methodshort{}'s answer has better diversity. 

\example{\Remark{chatgpt}}{How can you determine if a person is genuinely interested in a conversation or simply being polite?}{\outline{1. Pay attention to body language.}Notice if their posture is open or closed, if they are facing towards you or away, and if they are fidgeting or relaxed. These can all indicate their level of interest in the conversation.

\outline{2. Listen for engaged responses.}If the person is genuinely interested in the conversation, they will respond with thoughtful and relevant comments or questions rather than short, generic responses.

\outline{3. Observe eye contact.}If the person maintains steady eye contact and their gaze is focused on you, they are likely genuinely interested in the conversation. On the other hand, if they frequently look away or seem distracted, they may just be being polite.

\outline{4. Look for follow-up questions.}If the person asks follow-up questions and seems genuinely interested in what you're saying, then they are likely interested in the conversation and not just being polite.

\outline{5. Notice tone of voice.}A person who is genuinely interested in a conversation will often have a more enthusiastic and animated tone of voice, compared to someone who is simply being polite and giving short, monotone responses.}{}{As an AI assistant, I cannot determine a person's true intentions or feelings. However, there are some social cues that can indicate whether a person is genuinely interested in a conversation or not. For example, if they maintain eye contact, lean in, nod their head and ask follow-up questions, it can be a sign that they are engaged in the conversation. On the other hand, if they avoid eye contact, give short or vague responses, or seem distracted, it may indicate that they are not as interested. Ultimately, it's important to remember that everyone has different communication styles, and it can be difficult to accurately interpret someone's intentions without directly asking them.}{}

\methodshort{} answers the question from five aspects, whereas the \methodbase{} answer only mentions one aspect.

\emph{(2) Relevance.} In the prompt of \llmzoo{} evaluation, relevance is defined as ``The response should be closely related to the question and answer the question accurately with sufficient details without repetition or redundancy. The more relevant they are, the better.'' In the skeleton stage, LLMs are forced to propose several points around the question; and in the point-expanding stage, LLMs are required to only discuss these points. Therefore, answers from \methodshort{} tend to have better relevance. 
One typical example is shown below.

\example{\Remark{chatgpt}}{Why do some people enjoy the sensation of being scared, such as by watching horror movies or going on roller coasters, while others avoid these experiences?}{\outline{1. Neurological differences.}Some people enjoy the sensation of being scared due to the release of adrenaline and dopamine in the brain, while others may have a lower threshold for fear due to differences in their amygdala and prefrontal cortex.

\outline{2. Psychological factors.}Some people may enjoy the sensation of being scared because it can provide a sense of excitement, thrill, and a temporary escape from the mundane aspects of everyday life. On the other hand, others may avoid these experiences due to past trauma or anxiety.

\outline{3. Developmental experiences.}Individuals who had negative experiences with fear during childhood, such as trauma or abuse, may avoid scary situations as adults, while those who had positive experiences with fear may seek out these experiences.

\outline{4. Cultural influences.}Some cultures place a greater emphasis on seeking excitement and thrill, while others may prioritize safety and security, which can impact whether individuals enjoy scary experiences or avoid them.

\outline{5. Genetics.}Research has suggested that some individuals may have a genetic predisposition towards seeking out thrilling or risky experiences, which may include enjoying the sensation of being scared.}{}{The enjoyment of being scared is a complex psychological phenomenon. Some people may enjoy the adrenaline rush that comes with being scared, while others may enjoy the feeling of being able to conquer their fears. Additionally, some people may be more naturally inclined to seek out new experiences and sensations, including those that are scary or thrilling. On the other hand, some people may be more risk-averse or may have had negative experiences with fear or anxiety in the past, which could make them more likely to avoid scary experiences. Ultimately, the reasons why some people enjoy being scared while others do not are likely to be influenced by a range of factors, including personality, past experiences, and individual preferences.}{}

In the answer from the \methodbase{} generation, the first two sentences provide little information in answering the question, and the last sentence only gives keywords such as ``personality, past
experiences, and individual preferences'' without providing concrete explanations to each. In contrast, \methodshort{}'s answer is well-structured into five reasons with sufficient explanations and it does not waste space in irrelevant contents.

\myparatightestn{Summary.} In summary, \methodshort{} encourages LLMs to directly discuss the answers from multiple aspects without filler words. This improves the diversity and relevance of the answers with a loss of coherence and immersion. However, note that even for coherence and immersion, \methodshort{} is not worse than the \methodbase{} generation around 60\% of the time. One future direction is to improve the \methodshort{} prompts so that the answers can be better in more metrics.

%\todo{Detailed eval with examples}

%\todo{Do we need to have some discussion here about: SoT can enable generating answer longer than the trained context window, but still with reasonable attention to the overall outline (the large picture)... This depends on whether we can find some examples demonstrating this... somehow a very small point... I vote for not mentioning this currently}




%We further investigate the statistics of the generation results in \cref{fig:statistics}, and summarize the findings below. On writing and knowledge questions, models obtains the highest speed-up, since they tend to generate long responses when using the naive manner (see \cref{fig:statistics}(d)). On math questions, models achieves the lowest speed-up, since they tend to generate very short responses when using the naive manner (see \cref{fig:statistics}(d)). Besides, models tend to generate the outline with less points (see \cref{fig:statistics}(b)), making it hard to accelerate the generation with SoT.


% \todo{speed-up results on Vicuna-80 dataset and analyses}

% % Figure environment removed

% % Figure environment removed

% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c|c|c}
%         \toprule
%         Access & Model Name & Average speed-up & Average speed-up \\ & & & w.o. math\&coding \\\midrule
%         \multirow{7}{*}{open-source} & \Remark{openchat13B}~\cite{openllms23} & 2.18 & 2.30 \\ 
%         & \Remark{stablevicuna13B}~\cite{stablevicuna2023}& 1.05 & 1.08\\ 
%         & \Remark{vicuna7B1.1}~\cite{vicuna2023} & 2.20 & 2.32 \\
%         & \Remark{vicuna7B1.3}~\cite{vicuna2023} & 1.81 & 1.91\\
%         & \Remark{vicuna13B1.3}~\cite{vicuna2023} & 1.85 & 1.97\\
%         & \Remark{vicuna33B1.3}~\cite{vicuna2023} & 2.24 & 2.30\\
%         & \Remark{ultralm13B}~\cite{ding2023enhancing} & 2.09 & 2.09\\\midrule
%         \multirow{2}{*}{API-Based} & \Remark{claude}~\cite{claude} & 1.31 & 1.42\\
%         & \Remark{chatgpt} & 1.97 & 2.01\\
%         \bottomrule
%     \end{tabular}
%     \caption{The average speed-up for each models.}
%     \label{tab:model}
% \end{table}


%\toupdate{整体平均加速比、去掉math和code任务的}

% We demonstrate the speed-up evaluation results for the 9 models (listed in \cref{tab:model}) on each type of tasks from {\vicunadataset} on \cref{fig:speed-up_average_heatmap}. As can be seen, our SoT method can obtain speed-up on most of the cases. Specifically, for OpenChat-13B, Vicuna-7B, Vicuna-33B, UltraLM-13B and ChatGPT-3.5, SoT accelerates the inference time on all the tasks. The results show the effectiveness of the proposed SoT method.

% Nevertheless, we also notice that SoT fails on math problems for most of the models. For example, SoT achieves 0.6$\times$, 0.8$\times$ and 0.7$\times$ speed-up for StableVicuna-13B, Vicuna-7B and Clause on math problems, which means that SoT even slows down the model inference. We attribute the failure to the following three factors. \textbf{First}, on math problems, models tend to give out the responses with imbalance length for each points. As shown in \cref{fig:balance_average_heatmap}, the imbalance degree on math problems is very high, which means that the GPU cannot be fully used in the generation process. \textbf{Second}, on math problems, models tend to generate the outline with less points. Ideally, when the total output length is fixed, dividing the response into more points can bring higher speed-up. However, as shown in \cref{fig:bs_average_heatmap}, the average number of points on math problems is only 4.5, which is quite smaller than on other types of problems. \textbf{Third}, on math problems, models tend to generate the long response for each point (as shown in \cref{fig:outl_average_heatmap}), leading to the longer inference time.


% % Figure environment removed

% % Figure environment removed

% % Figure environment removed


% \paragraph{Vicuna}


% As shown in \cref{fig:speed-up_average_heatmap}, our SoT method can significant accelerate the models in the Vicuna familiy. Specifically, on the NVIDIA A100 GPU , SoT can achieve 2.17, 2.16 and 2.17 for Vicuna-7B, 13B and 33B on Vicuna-80 dataset, respectively. While on the NVIDIA RTX3090 GPU, SoT can achieve 1.88 for Vicuna-7B. Besides, we also observe that, on the most types of tasks(e.g., writing, knowledge, generic), SoT achieves more than 2$\times$ speed-up. But on the math problem, SoT obtains nearly no speed-up for Vicuna-7B and 33B. To explore the reason causes to the above difference, we further inspect the number of the points and the output token length of the SoT generation results, and show them in \cref{fig:vicuna_bs} and \cref{fig:vicuna_len}. As can be seen, the average output token length of the math problem of Vicuna-7B is the longest one, which might cause the low speed-up. Besides, for the math problem of Vicuna-33b, the average number of the output points is the smallest one, which might also cause the negligible acceleration.


% % Figure environment removed

% % Figure environment removed

% \paragraph{ChatGPT-3.5 \& Claude} For ChatGPT-3.5 and Claude models, we use their API to generate the responses and recode the calling time. The speed-up evaluation results are shown in \cref{fig:gpt_claude_speed-up}. SoT can achieve 2.01$\times$ and 1.37$\times$ speed-up on average for ChatGPT-3.5 and Claude, respectively.
%For ChatGPT-3.5, our SoT method can achieve 2.01$\times$ speed-up on average. While for Claude, the average speed-up is 1.0, which means that SoT cannot accelerate the this model.

% % Figure environment removed



% \subsubsection{Profile Results and Analysis}

% % \todo{profile results plot out, analyses}

% %\paragraph{Numer of Points} 

% In this section, we evaluate how the hardware efficiency (e.g., latency, memory and GPU utilization) changes with the number of points in the SoT generation process. 

% Ideally, with the increase of the number of points, the output token length of each points decreases. We set the total output token length as 128, 256 and 512, respectively, and show the efficiency trend with the number of points. As shown in \cref{fig:profile_bs_user} (top), the speed-up increases linearly with the number of points. The underlying reason causes to the increase is the GPU utilization improvement (\cref{fig:profile_bs_user} (down)). Besides, as shown in \cref{fig:profile_bs_user} (middle), the growth rate of the memory cosume also increases linearly with the number of points.

% However, from the actual generation results, we find that the output token length of each points often has less to do with the number of points. Thus, we set the token length of each point-expanding response as 50, 100 and 200, respectively, and show the efficiency trend with the number of points. As shown in \cref{fig:profile_bs}, when the output is short, the speed-up remains constant with the number of points, since.... When the output is long enough, the speed-up slights decrease with the increasing of the number of points, since....

% % \paragraph{Prompt Length}

% % Figure environment removed

% % Figure environment removed


\section{In the Context of Literature}
\label{sec:literature}

In this section, we position \methodshort{} in the context of current research ecosystem %(e.g., efficient algorithm co-design, system-level acceleration techniques, prompting techniques)
to reveal how SoT (1) is connected to, (2) is different from, %or even paradigmatically different from,
and (3) can harness the power of other methods.

\subsection{Efficient LLMs}

%\zinan{It is unclear what the connections between the three bullets are at the first glance. We can either make the paragraph titles in a consistent format, or add an introduction paragraph at the beginning.   OK! go for the latter one}

% We have witnessed a continuous growing of the model size and token size, which drives LLMs to achieve stronger abilities,
% Scaling up the training data and model size has tremendously improved the ability of language models. However,

%\paragraph{The causes of LLMs' efficiency problem.} The scaling of model size and sequence length poses serious problem for the training and inference efficiency of LLMs. We conclude three main algorithmic causes for the efficiency problem: (1) A large \textbf{model size} requires a large amount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT-3 takes 350GB memory, which means 5$\times$80GB A100 GPUs are needed just for inference. Even if we have enough GPUs, the heavy memory access and computation to slow training and inference. (2) The increasing sequence length also leads to substantial efficiency issue, as the core \textbf{Attention operation} in the prevailing Transformer architecture has a quadratic memory and computation complexity in sequence length. (3) Finally, the \textbf{autoregressive decoding approach} in inference generates tokens one by one, where each token depends on previously generated tokens. This approach introduces a significant inference latency since the generation for tokens cannot be parallelized.


%\mytodo{The transformer architecture... I/O bound. analyze, how to accelerate. I/O bound or compute bound}

%\mytodo{introduce the LLM basic, inference include prefill and decode stage}

Extensive research has been dedicated to enhancing the throughput and latency of LLM training and inference. We first discuss model-level compression techniques that reduce the model complexity from certain aspects. These techniques change the model and can benefit both the latency and throughput at the risk of degrading model quality. Then, we discuss system-level efforts that optimize the computational graph or the assignment and scheduling of the computational graph on computation and storage devices. The current solutions for LLMs mainly accelerate the prefilling phase or improve the training throughput. Finally, focusing on inference-latency-oriented scenarios, we introduce existing methods that accelerate the decoding phase.
  
\paragraph{Model-level optimization.} There have been efforts to address the quadratic complexity of vanilla attention by designing new attention mechanisms~\cite{kitaev2020reformer,wang2020linformer}. However, these methods usually require a significant re-training cost. The model compression techniques have been applied to reduce the LLM workload~\cite{ganesh2021compressing}. The quantization~\cite{xiao2022smoothquant,frantar2022gptq,lin2023awq}, the static or dynamic pruning of weights, activation, and attention~\cite{semi_first,zaheer2020big,wang2021spatten,chen2023dynamic} are popular approaches for LLM compression, since they require only a limited amount of fine-tuning to keep up the original algorithm performance.

Zooming out from LLM compression to the whole field of model compression, we can see that model co-design or compression for efficiency has received tremendous attention in the past few years and has grown into large research fields, such as pruning~\cite{han2015deep,group-lasso}, quantization~\cite{krishnamoorthi2018quantizing}, factorization~\cite{denton2014exploiting}, and neural architecture search~\cite{zoph2016neural,cai2019once}. %These research works simplify the computation of certain parts in the model from certain aspects.
\textit{Different from the model co-design paradigm, SoT is in a ``\textbf{content co-organization for efficiency}'' paradigm for improving the LLM efficiency}. As we are now moving from the model-centric era to the data-centric era, and AI is capable of understanding and generating longer and longer sequences well, the co-organization of content for %the good of 
efficiency might become an important tool in the efficient LLM toolbox.



\paragraph{System-level optimization.} % targeting throughput-oriented scenarios.}
In the realm of lossless acceleration, considerable efforts have been devoted to addressing the I/O-bound nature of LLMs on modern hardware platforms~\cite{dao2022flashattention}. Numerous studies~\cite{dao2022flashattention,zhai2022bytetransformer,ivanov2021data,fastertransformer} have focused on adjusting the computational graph by fusing and implementing operations in an I/O-friendly way. As a representative method, FlashAttention~\cite{dao2022flashattention} fuses all operations of one attention into one GPU kernel with spatially tiled computation to reduce the off-chip I/O of the attention map. While FlashAttention can effectively accelerate training and the prefilling phase of inference, it cannot accelerate the decoding phase much (when the batch size is small), as it is the I/O of weights rather than activation or attention map that bottlenecks the decoding phase. For example, when the context length is 64, decoding one token using \Remark{llama}-7B needs to load each of the 7B parameters from the off-chip HBM onto the GPU chip at least once, but only transferring about 20M (0.02B) activation values between the off-chip HBM and GPU chip.
% (\todo{Let's get a feel for the numbers by giving the order of magnitudes difference of I/O amount of weights and activation here}).

Various model parallelism~\cite{lu2017flexflow,huang2019gpipe,narayanan2019pipedream,rajbhandari2020zero,narayanan2021memory,li2021terapipe,zheng2022alpa} and offloading~\cite{ren2021zerooffload,sheng2023flexgen} techniques have been proposed to maximize the throughput of LLM training or batched inference. In a nutshell, given the computational graph and the device configurations, these techniques optimize the split, assignment, and scheduling of computations, storage, and communications on devices. In order to satisfy Service Level Objectives, serving systems focus on improving the serving throughput under latency constraints. To this end, serving systems such as TurboTransfromer~\cite{fang2021turbotransformers}, Triton Inference Server~\cite{triton}, and Tensorflow Serving~\cite{tfserving} pack multiple queries together into a batch to improve the hardware utilization.
The batching technique has proven highly effective in enhancing throughput, leading to the development of various variants. For example, some work designs methods to decide which queries to batch together~\cite{fang2021turbotransformers,zhou2022pets}, while others selectively batch parts of the model to enable fine-grained iteration-level batching~\cite{yu2022orca} or multi-task batching~\cite{zhou2022pets}.
%The batching technique is very effective in improving the throughput, so that many systems have designed its variants by decide which queries to batch together~\cite{fang2021turbotransformers,zhou2022pets}, selectively batch part of the model to enable finegrained iteration-level batching~\cite{yu2022orca} or multi-task batching~\cite{zhou2022pets}), and so on.

To sum up, these system-level techniques mainly help with throughput-oriented scenarios like training and batched inference, and cannot effectively optimize the end-to-end decoding latency in inference. Nevertheless, \textit{\methodshort{} can harness the power of these throughput-oriented techniques and make them help with the end-to-end latency}: \methodshort{} parallelly decodes multiple segments and aggregates them afterward, and can utilize these techniques to improve the throughput of the batched decoding of multiple segments (e.g., we can use FlexGen~\cite{sheng2023flexgen} to enable high-throughput batched decoding on relatively low-end GPUs).

Another parallelism perspective to position \methodshort{} is that \textit{\methodshort{} guides the LLM to adjust the sequential workload to be ``inter-content'' parallelizable}, which is different from previous parallelism levels, including inter-instance~\cite{krizhevsky2014dp,rajbhandari2020zero}, inter-operation~\cite{huang2019gpipe,narayanan2019pipedream,narayanan2021memory}, intra-operation~\cite{xu2021gspmd}, and inter-token~\cite{li2021terapipe}. % (prefill stage only).


\paragraph{Acceleration for latency-oriented scenarios.} In latency-sensitive scenarios, % such as ChatBots,
the bottleneck for the end-to-end latency lies in the autoregressive decoding phase, where tokens must be generated one by one. Due to the dependency between tokens, the computation of different tokens cannot be parallelized, causing severe under-utilization of GPU. In order to improve the end-to-end decoding latency of a given LLM, speculative decoding methods~\cite{stern2018blockwise,leviathan2022fast,chen2023accelerating,hugging2023assisted,sun2023spectr} propose to use cheaper approaches to generate a short candidate token sequence, for example, by sequentially decoding with an assisting model much smaller than the given LLM. Then, they use the LLM to parallelly verify the candidate and keep the prefix sequence that matches the LLM's verification results.
As the decoding process is bottlenecked by the I/O of the model weights and the computation units are underutilized, the latency of verifying multiple tokens in parallel is comparable to that of generating only one token. Therefore, the longer the average matching length is, the higher acceleration these methods can achieve.
In order to increase the average matching length, a recent study~\cite{miao2023specinfer} proposes to tune a pool of assisting models and generate a candidate token tree instead of one candidate token sequence.

While the speculative decoding methods use assisting models to enable \textit{the parallel parsing of consecutive tokens}, \methodshort{} guides the LLM to reorganize the content to enable \textit{the parallel generation of multiple tokens in different segments}.



%Nowadays, as we might be at the door of AGI driven by the understanding and generation and long language sequence
%Given the computational graph, there are a lot of work studying how to (1) split the computation and storage workloads of the model, (2) assign the splitted workloads to a hierarchy of computing and storage devices, and (3) schedule the computation and I/O to maximize the throughput. 
%The proposed techniques, including data~\cite{krizhevsky2014dp,rajbhandari2020zero} / operator~\cite{xu2021gspmd} / pipeline parallelism~\cite{huang2019gpipe,narayanan2019pipedream,narayanan2021memory,li2021terapipe}, offloading techniques~\cite{ren2021zerooffload} and etc, have became the core component of LLM training and serving system.
%Representative systems implementing these techniques include Google GPipe~\cite{huang2019gpipe}, Stanford FlexFlow\cite{lu2017flexflow}, Microsoft DeepSpeed~\cite{rajbhandari2020zero,ren2021zerooffload}, Alpa~\cite{zheng2022alpa}, FlexGen~\cite{sheng2023flexgen}, and so on.
%decrease the I/O cost improve the GPU utilization ,  The most recent technique to reduce the end-to-end decoding latency is Speculative decoding~\cite{stern2018blockwise,chen2023accelerating} propose to a continuous sequence s当前最常见的减少end-to-end latency的方式是用speculative inference的思想, 具体来说是用一些更cheap的方式生成一些候选序列, 用大模型来对候选序列做verifying, 如果match大模型的verify结果则可以... 为了提升匹配率, 进一步提出用ensemble of models生成tree of candidate tokens

%\paragraph{Content co-organization for improving latency}
%\subsection{Sparse Attention and SoT}
% compared to architecture redesign and structure pruning.
% As for system-level lossless acceleration, based on the fact that LLMs are heavily I/O-bounded workloads on modern hardware platforms~\cite{dao2022flashattention}, lots of work adjust the computational graph by fuse and implement operations in an I/O friendly way.
%The proposed techniques, including data~\cite{krizhevsky2014dp,rajbhandari2020zero}, operator~\cite{xu2021gspmd} and pipeline parallelism~\cite{huang2019gpipe,narayanan2019pipedream,narayanan2021memory}, offloading techniques~\cite{ren2021zerooffload} have the core component of LLM training and serving system.
%data parallelelism, operator parallelism, pipeline parallelism, and automatic parallelization, offloading
%As one widely acknowledged fact is that LLMs are heavily I/O-bounded workloads for modern hardware platforms~\cite{dao2022flashattention}.
%, as the compute of modern GPU platforms has gotten faster relative to memory speed
%the large model size, quadratical-complexity attention, and autoregressive decoding approach altogether result in an extremely heavy I/O cost, which renders
%As the inference and training of LLM is largely I/O bound, most efficient LLM work are focusing on lowering
%heavily I/O bound. 都是在降低I/O cost, 提高计算单元利用效率
%On the one hand, to put down a large model requires a large amount of storage (e.g., just putting down the fp16 weight of GPT-3 requires...) large storage; on the other hand, the surge in the amount of memory access and calculations leads to a long run time for an inference.

%\subsubsection{Decoding Acceleration}
%\subsubsection{Sparse Attention}


\subsection{Prompting Methods for LLMs}

In recent years, the ``pre-train, prompt, and predict'' paradigm has emerged~\cite{liu2023pre}, which designs prompts comprising task descriptions and (optionally) a few demonstrations to guide pre-trained LLMs in generating answers for a wide range of downstream tasks. % without weight tuning.
Researchers found that instruction-tuned LLMs~\cite{brown2020gpt3,wei2021flan,ouyang2022instructgpt,chung2022scaling,alpaca2023} possess a strong ability to (1) generalize to new tasks thanks to the diverse natural language descriptions encountered during instruction tuning, and (2) learn in-context using a few demonstrations without weight tuning.

In virtue of these abilities, the field has been manually engineering~\cite{brown2020gpt3,kojima2022large,shen2023hugginggpt,li2023camel}, automatic searching~\cite{shin2020autoprompt}, or continuously tuning~\cite{li2021prefix,lester2021power} the prompts for uncovering the capabilities of LLMs on downstream tasks. For example, the Chain-of-Thought prompts largely improve the performance on tasks that require logical thinking and reasoning by simply providing a ``Let's think step by step''~\cite{kojima2022large} instruction or a few demonstrations~\cite{wei2022chain}. HuggingGPT~\cite{shen2023hugginggpt} design prompts to guide the LLM to generate structural JSON for the orchestration of multi-model execution to finish complex tasks.

The large literature on prompting methods has been aiming at uncovering different capabilities of LLM and improving the answer quality %algorithm performance
on different downstream tasks. In contrast, \textit{\methodshort{} exploits the power of prompting to improve efficiency}.

%As the NLP paradigm shifts from ~\cite{liu2023pre}
%~\cite{li2021prefix}
%As LLMs get stronger capability of understanding the instruction and demonstrations in prompts. That is to say, without tuning the weights, with appropriate prompt, ... different behaviors.


%\subsubsection{Thought Organization}
%Chain-of-Thought

%\subsubsection{Prompt Tuning}

\section{Limitations, Future Work, and Open Questions}
\label{sec:limit-and-outlook}

\paragraph{Limitations.}
Our current evaluation of the answer quality is far from perfect due to the limited prompt set, the bias of current LLM judges, and the inherent difficulty of evaluating LLM generations. Expanding the prompt set and complementing LLM-based evaluation with human evaluation is important for a more reliable quality evaluation. Nevertheless, the current manuscript mainly focuses on revealing the potential efficiency benefits, i.e., we can achieve considerable speed-up by rethinking the necessity of ``fully sequential decoding'' of current LLMs. Therefore, we leave a more thorough evaluation of answer quality to future work.
%\todo{@lzn should we put this here as a limitation of our current evaluation method, or put it at 3.2. I'm not sure} \zinan{putting it here is fine}



% \paragraph{Limitations.} First, \methodshort{} cannot answer math questions well. This is reasonable since the current \methodshort{} solution is somehow contradictory with CoT: CoT relies on the expanded details of each step to continue the following reasoning, while \methodshort{} hopes to strategically list out the skeleton in advance. Therefore, intuitively, \methodshort{} is more suitable for the questions with a clear answer structure that can be planned in advance.
% Second, \methodshort{} is not good at code questions. We observe that all models tend to provide a bunch of coding strategies and comments, instead of giving out the code directly, which is usually preferred by users. \todo{let's check the concrete result again and modify the discussion here}
% Third, there are some redundant ..\todo{@lzn do u think we should make this a limitation? It seems a concrete thing to improve, but not fundamental limitation.}

% \todo{Discussion about randomness, the controllability of budget}

% \todo{for narrative types, maybe we should mention the linguistic cohesion and coherent problem between points. @lzn let's analyze some points}
% 对于写故事类型的... 前后段的衔接问题要不要提一下. 拿几个代表性问题分析一下

%Potentials of \methodshort{} on algorithm performance

%Potentials of \methodshort{} on efficiency
%still has a number of limitations. 
%summarize the limitations,

While we think \methodshort{} is conceptually enticing and reasonable and have experimentally demonstrated its effectiveness in improving efficiency, efforts are still needed to improve the current \methodshort{} solution. 
Below we suggest some directions for future work and raise open questions.

\paragraph{Future work.} 
(1) Currently, \methodshort{} cannot answer math questions well. This is reasonable since the current \methodshort{} solution is somehow contradictory with CoT: CoT relies on the expanded details of each step to continue the following reasoning, while \methodshort{} hopes to strategically list out the skeleton in advance. 
More broadly, \methodshort{} is more suitable for questions with a clear answer structure that can be planned in advance.
In order to use \methodshort{} appropriately, we should design a pipeline to only trigger the \methodshort{} mode for question types and descriptions that are suitable for SoT. 
Suppose the user requires a very short answer (e.g., by formulating a multi-choice question or explicitly adding words like ``please give a succinct and short answer'' in the prompt) or raises a question that mainly needs impromptu thinking or reasoning instead of strategically structuring (e.g., equation solving). In that case, we should choose other generation modes instead of \methodshort{}.

%Therefore, intuitively, 
% , i.e., have clear structure and prefer comprehensiveness. 
%For example, 


% In order to improve the ability of handling different types of questions, one can consider giving out structural information in the skeleton, so as to give more concrete instruction to the expanding stage. Intuitively, the strategy of answering different types of questions can be different: When answering the question ``What are the main differences between Python and JavaScript programming languages'', we can give out a skeleton containing topics such as ``Syntax'', ``Execution environment'', ``Performance'', ``Typing'', and so on. In contrast, when answering the question ``How would you introduce yourself as a medieval knight at a royal banquet'', each point of the skeleton can contain the narrative strategy and the answer prefix to make the aggregated answer fluent (improve the linguistic cohesion).
% \todo{maybe discuss a better example here?}

%In order to elicit the capability of LLMs on in-adavance skeleton planning and expanding, %we can leverage their in-context learning capabiltiy by providing demonstrations of giving short and informative skeleton and expanding precisely. Moreover, 
(2) In order to improve the \methodshort{} capability of LLMs,
we can fine-tune them to better understand the skeleton and expanding instructions, and produce more fluent and natural answers for a wider range of question categories. For example, fine-tuning could help the model learn to adaptively choose the \methodshort{} mode or fall back to 1-stage sequential decoding.
% on in-adavance skeleton planning and expanding,
 %Fortunately,
%Specifically, we can leverage the LLM itself to prepare the supervision data, for example, by (1) expanding the training prompt set~\cite{wang2022self}, (2) sequentially decoding the content, and (3) summarizing and splitting the decoded content into several points.

% might improve the generalizability of \methodshort{} on different types of questions.
%Specifically, if each skeleton point is a JSON string like \verb|{`topic': `treasure value', `strategy': `first tell the treasure value, then explain why the value is high, finally call for engagement', `partial_answer': `The great treasure values [to be continue]'}| give more concrete instruction. 

%\todo{Let the LLM itself decide whether to hierarchical decode (toolformer)}

%实用性 
%1. according to user's instruction and question type, automatically trigger the \methodshort{} mode for question types that need long and detailed answer (maybe feasible by just prompting too). For example, explicit require to answer shortly, or multi-choice type questions...

%算法效果
%2. Better organization of contents, give out structural information in outline (e.g., topic, strategy, partial answer), and respect them when expanding

(3) Currently, we only test \methodshort{} with the standard HuggingFace Transformer library. Further integrating \methodshort{} with existing throughput-oriented optimization techniques~\cite{sheng2023flexgen}, inference engines~\cite{fastertransformer,dao2022flashattention,zhai2022bytetransformer}, and serving systems~\cite{fang2021turbotransformers,triton} is an interesting future direction to reveal the full potential of \methodshort{} in enhancing both the throughput and latency.



\paragraph{Open questions.}
(1) The current \methodshort{} solution forces a fully parallelized decoding of all points, ignoring the possible sequential dependencies between points. A conceptually better way is to organize the points as a ``Graph-of-Thoughts'', where the edges represent sequential dependencies. Then, the decoding of a certain point can be conditioned on the detailed contents of its ancestor points. Not only does the skeleton structure might need to be generalized from independent points to a dependency graph of points, but the structure might also need to be dynamic. Consider how we think and write: In addition to systematic thinking and writing according to well-established protocols, we can come up with new ideas, think of new information, and realize new connections during the process. This impromptu process can be vital for logical reasoning and creative thinking. Therefore, instead of complying with a static content skeleton, we prospect the need to guide AI to self-organize its thoughts as a ``dynamic Graph-of-Thought'', in which the thought structure can be dynamically adjusted. This could potentially combine the benefits of \method{} on efficiency and global thinking with the benefits of CoT~\cite{kojima2022large,wei2022chain} on logical reasoning or impromptu thinking.

(2) Although many efforts to improve model \emph{quality} have been shifted from model-centric engineering to data-centric engineering, the field has not seriously explored the potential of data-centric engineering for \emph{efficiency}. \methodshort{} is an initial attempt toward this direction. We are looking forward to more explorations to reveal the full potential of data-centric optimization for efficiency, adding a new data level to the existing technique stack of ``model-software-hardware co-design for effciency''~\cite{guo2017software}.



%Beyond skeleton of thoughts graph of thought, dynamic changing the graph. can help solve a wider range of questions, including addressing the current limitation on math, coding and so on. Consider one human's thought process... besides organized thinking according to a well-established protocol or strategy, come up with new ideas, think of new information, realize new connections, inspirations during thinking (impromptu thinking), a potential to bring AI closer to the thinking process of human...

%

% 适合知识/经验型问题 (回答知识性问题,考试,写功能性信)... 作为一般的chatbot助手, 用systematically organized方法回答大多数问题是perfered。 不过有时候也有creativity要求...
% 除了能想到的, 甚至还有看似非常弱的连接. 更不可能是strategically模板式的查询和回答的
% Human一个时刻只能展开一个... 这也是为什么有文字和语言来帮助组织人类思考极大提升了人的智商. 但是AI不一定只能单线, 完全可以展开多条线, 同时发生多种关联. more efficient
% ``knowledge (discuss the difference between two languages. 知道从哪几个方面回答), template (how to write a letter)''

\section*{Acknowledgements}
We thank Tianyu Fu for many initial discussions on the idea. We thank Ke Hong and Genghan Zhang for their discussions about profiling. We thank Yue Wu for the help on the \Remark{claude} scripts. We thank Da Yu, Chulin Xie, Saiqian Zhang, and Tianji Wu for their suggestions on revising the paper.


\bibliographystyle{plain}
\bibliography{top,example_paper}

\clearpage

\appendix

\section{Implementation Details of \method{}}
\label{app:implementation_details}


\subsection{Model Details}
\cref{tab:model_detail} shows sources of the models we use in the paper. We use \Remark{chatgpt} as the judge for \fastchat{} and \llmzoo{} evaluation.
\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c}
        \toprule
        Access & Model Name & HuggingFace or API Endpoints \\\midrule
        \multirow{8}{*}{Open-Source} &
        \Remark{llamachat7B2} \cite{touvron2023llama2} & meta-llama/Llama-2-7b-chat-hf \\
        &
        \Remark{llamachat13B2} \cite{touvron2023llama2} & meta-llama/Llama-2-13b-chat-hf \\
        &\Remark{openchat13B}~\cite{openllms23} & openchat/openchat \\
        & \Remark{vicuna7B1.3}~\cite{vicuna2023} & lmsys/vicuna-7b-v1.3 \\
        & \Remark{vicuna13B1.3}~\cite{vicuna2023} & lmsys/vicuna-13b-v1.3\\
        & \Remark{vicuna33B1.3}~\cite{vicuna2023} & lmsys/vicuna-33b-v1.3\\
        & \Remark{stablevicuna13B}~\cite{stablevicuna2023}& CarperAI/stable-vicuna-13b-delta\tablefootnote{For convenience, we use the non-official endpoint TheBloke/stable-vicuna-13B-HF and TheBloke/UltraLM-13B-fp16 to get merged weights.\label{footnote:thebloke}} \\ 
        & \Remark{ultralm13B}~\cite{ding2023enhancing} & openbmb/UltraLM-13b\footref{footnote:thebloke} \\
        & \Remark{vicuna7B1.1}~\cite{vicuna2023} & lmsys/vicuna-7b-delta-v1.1 \\\midrule
        \multirow{2}{*}{API-Based} & \Remark{claude}~\cite{claude} & Claude extension on Slack\tablefootnote{\url{https://www.anthropic.com/claude-in-slack}} \\
        & \Remark{chatgpt} & Azure OpenAI, gpt-35-turbo 0301 version\tablefootnote{\url{https://azure.microsoft.com/en-us/products/ai-services/openai-service}}\\
        \bottomrule
    \end{tabular}
    %   \begin{minipage}{0.8\textwidth}
    % {\small
    %   $\dagger$: We use the non-official endpoint TheBloke/stable-vicuna-13B-HF and TheBloke/UltraLM-13B-fp16 to get merged weights for convenience.
    % }
    % \end{minipage}
    \caption{The HuggingFace or API endpoints used in the paper.}
    \label{tab:model_detail}
\end{table}


\subsection{Extracting Points from Skeleton}
We use the regular expression \verb/(\d+)\.\s?([\s\S]+?)(?=\n|\n*$)/ to extract the point indexes and the point skeletons from the skeleton response.

\subsection{Prompts}
\label{app:implementation_details_partial_answer}

% All the open-source models natively support partial answers; however, they implement it in slightly different ways. In more detail, the concrete texts for ``\textbf{[User:]}'' and ``\textbf{[Assistant:]}'', which specify who the words are from, are different. %designated by the model-specific conversation template.

\paragraph{Partial Answer.} In the prompts \cref{prompt:ts,prompt:tp}, we provide partial answers so that LLMs can follow the desired response format better.

We can put the partial answer at the end of the prompt for the open-source models to continue writing. An implementation detail is that different open-source models have different conversation templates (i.e., different ways to combine user and assistant messages into one string). 
For example, \Remark{vicuna}~\cite{vicuna2023} uses the string ``USER:'' and `` ASSISTANT:'' for the placeholder ``\textbf{[User:]}'' and ``\textbf{[Role]}'' in the \cref{prompt:ts,prompt:tp}, respectively, while \Remark{ultralm}~\cite{ding2023enhancing} uses ``User:'' and ``</s>Assistant:''. We build our open-source model experiments with the help of the FastChat codebase~\cite{zheng2023judging}, in which the conversation templates of many models are already correctly handled. We implement the conversation templates of \Remark{openchat13B}, \Remark{stablevicuna13B}, and \Remark{ultralm13B} according to their official guides and codes.

For \Remark{chatgpt}, we provide partial answers as a last message in the chat history from the assistant. Note that it is not a documented approach. We find it works well in most cases, in that \Remark{chatgpt} continues the texts from the provided partial answer. However, in some rare cases, \Remark{chatgpt} repeats the provided partial answers.

For \Remark{claude} over Slack, there is no obvious way to give the API a partial answer. We resort to modifying the prompt template slightly by adding \begin{center}\textit{Please start your answer from ``\promptarg{partial answer}'' and do not output other things before that} \end{center} at the end. We find that \Remark{claude} understands and obeys it well.


\paragraph{Point-Expanding Prompt.} We find that \Remark{claude} follows the instruction ``Write it **very shortly** in 1$\sim$2 sentence and do not continue with other points!'' in \cref{prompt:tp} very well, so that the answers are very short. Therefore, we delete ``**very shortly**'' from the prompt template in \Remark{claude}. This and the partial answer prompts discussed above are the only two prompt template customizations we did across all models and all evaluations.

\paragraph{System Message.}
We do not include the system message in the prompts for open-source models except \Remark{llama2}.


\section{Answer Quality Evaluation}
\subsection{Quality Breakdown: Question Categories and Models}
In the main text, we analyze how question categories and models affect \methodshort{}'s answer quality \emph{independently}. Here, we show their joint effect.
For each model and question category, we compute the number of questions that \methodshort{} wins (\#win) and loses (\#lose), and compute a summarized value $\nicefrac{\text{\#win}-\text{\#lose}}{\text{number of questions in the category}}$. The results are in \cref{sec:model_category}. 0\% means that \methodshort{} performs competitively to the \methodbase{} baseline (wins and loses in the same number of questions). Higher values mean that \methodshort{} performs better. 
% Similar to \cref{{fig:win_tie_lose_bar_GENERAL}}, we observe that \llmzoo{} favors \methodshort{} more than \fastchat{} metric as suggested by the higher values. 

% Figure environment removed

\section{Efficiency Evaluation}
\subsection{Detailed Statistics of Token Lengths and Point Numbers}

% Figure environment removed


% We didn't set the clock rate of the GPU due to the limitation of our development environment, but we observed the GPUs run at their highest clock rate for most time during the profiling.


\subsection{Profiling and Estimation for Latency and Peak Memory}
\label{sec:app-profiling-estimate}

We run the profiling on the target GPU (NVIDIA A100-80G and NVIDIA RTX 3090) with CUDA 11.7, using the HuggingFace transformer library 4.28.1 and PyTorch 2.0.1. The host of A100-80G has an Intel Xeon Platinum 8358P CPU and 1T memory. And the host of RTX 3090 has an Intel Xeon Gold 6246R CPU and 512G memory.


\paragraph{Latency profiling and estimation.} For the decoding phase, we denote $t_{B}^{D}(k)$ as the latency of batched decoding the $k$-th token with batch size $B$, where the superscript $D$ stands for ``decode''. For each batch size $B=1,\cdots,16$ and each context length $k=1,\cdots,1024$, we use \verb/torch.cuda.Event/ to record the latency of decoding one token. We run each decoding three times continuously and take their geometric mean as $\{t_{B}^{D}(k)\}_{k=1,\cdots,1024; B=1,\cdots,16}$.
For the prefilling phase, we profile the latency of batched prefilling the inputs with token length $k$ in $\mbox{range}(1,700,10)$ and batch size $B=1,\cdots,16$, and denote it as $t_{B}^{P}(k)$, where the superscript $P$ stands for ``prefill''. We run each test seven times continuously, regard the first two times as the warmup tests, and take the geometric mean of the last five times as $\{t_{B}^{P}(k)\}_{k=1,11,\cdots,691; B=1,\cdots,16}$. Once we get the latency profiling table, given a request with $l_i$ tokens and the decoding batch size $B$, the latency of generating $l_o$ tokens can be estimated as:
\begin{equation}
    T(l_i, l_o, B) = \tilde{t}_{B}^{P}(l_i) + \sum_{k=l_i}^{l_i+l_o} t_{B}^{D}(k),
  \end{equation}
  where the subscripts $i$ and $o$ stand for ``input'' and ``output''. Note that we only test the prefilling latency every ten token lengths (i.e., $1, 11, 21, \cdots$) for fast profiling and estimate $\tilde{t}_{B}^{P}(l_i)$ by $t_{B}^{P}(\lfloor\frac{l_i}{10}\rfloor \times 10 + 1)$.

The \methodshort{} decoding process consists of two stages: the skeleton stage and the point-expanding stage. Denoting the token length of the skeleton request and skeleton response as $l_i^{s}$ and $l_o^{s}$, the token length of the longest point-expanding request and the longest point-expanding response as $l_i^{pe}$ and $l_o^{pe}$, the number of the points as $B$, we can compute the latency of the skeleton and point-expanding stages as:
\begin{align}
    L^{s}(l_i^{s}, l_o^{s}) &= T(l_i^{s}, l_o^{s}, 1), \\
    L^{pe}(l_i^{pe}, l_o^{pe}, B) &= T(l_i^{pe}, l_o^{pe}, B).
\end{align}


Using the latency profiling table, we can further estimate the average GPU computing performance (GFLOPS) of decoding $l_o$ tokens with prefilling length $l_i$ as 
\begin{equation}
    P^{D}(l_i, l_o, B) = \frac{\sum_{k=l_i}^{l_i+l_o} f_{B}^{D}(k)}{\sum_{k=l_i}^{l_i+l_o} t_{B}^{D}(k)},
  \end{equation}
  where $f_{B}^{D}(k)$ denotes the FLOPs of decoding one token with context length $k$, which is calculated by DeepSpeed's FLOPs profiler~\footnote{\url{https://deepspeed.readthedocs.io/en/latest/flops-profiler.html}}. 
  \cref{fig:efficiency-batch-vs-single-perf} reports the average GPU computing performance during the process of decoding 64 tokens (prefilling length=128), i.e., $P^{D}(128, 64, B)$.

\paragraph{Memory profiling and evaluation.} To evaluate the peak memory, we use \verb/torch.cuda.max_memory_allocated/ to record the memory consumption of prefilling sequences of different lengths and decoding with different context lengths and a batch size ranging from 1 to 16. Then, we calculate the peak memory of each stage as the maximum value of the prefilling and decoding phases, and calculate the overall peak memory of \methodshort{} as the maximum value of the skeleton and point-expanding stages.


  % one token with context length $k$ and batch size $b$ by $\frac{f_{b}^{D}(k)}{t_{b}^{D}(k)}$. Then, given an input with $l_i$ tokens and the decoding batch size $b$, we can estimate the average one-token decoding GPU performance of decoding one token during the process of decoding $l_o$ tokens as:
  
%\paragraph{Utilization Evaluation} The GPU utilization can be defined as the ratio of the actual computing performance and the peak computing performance (e.g., 312 FP16 TFLOPS for NVIDIA-A100). For the LLM generation process, the actual computing performance can be estimated as the ratio of the FLOPs and the latency. Thus, we count the FLOPs of batched decoding the $k$-th token with batch size as $b$ (denoted as $f_{b}^{decode}(k)$). 
%, and also count the FLOPs of batched prefilling the inputs with $k$ tokens with batch size as $b$ (denoted as $f_{b}^{prefill}(k)$). 



% \begin{align}
%     U^{prefill}(l_i, b) &= \frac{f_{b}^{prefill}(l_i)}{t_{b}^{prefill}(l_i)}  \\
%     U^{decode}(l_i, l_o, b) &= \frac{1}{l_o} \sum_{k=l_i}^{l_i+l_o} \frac{f_{b}^{decode}(k)}{t_{b}^{decode}(k)}
% \end{align}



% \paragraph{Memory Evaluation} For evaluating the peak memory among the generation process, we follow the latency evaluation pipeline to get the memory usage of batched decoding $k$-th token with batch size as $B$ (denoted as $m_{B}^{decode}(k)$). 
% %and of batched prefilling the inputs with $k$ tokens with batch size as $b$ (denoted as $m_{b}^{prefill}(k)$). 
% Given an input with $l_i$ tokens and the decoding batch size $B$, we can separately estimate the GPU utilization among the process of decoding $l_o$ tokens as:

% % \begin{align}
% %     M^{prefill}(l_i, b) &= m_{b}^{prefill}(l_i)  \\
% %     M^{decode}(l_i, l_o, b) &= \mathop{\max}_{k=l_i}^{l_i+l_o} m_{b}^{decode}(k)
% % \end{align}

% \begin{equation}
%     M^{D}(l_i, l_o, B) = \mathop{\max}_{k=l_i}^{l_i+l_o} m_{B}^{D}(k).
% \end{equation}

\subsection{Efficiency Evaluation on NVIDIA RTX 3090}
\label{sec:app-3090}

We present the \methodshort{} speed-up and latency breakdown on RTX 3090 in \cref{fig:speed-up_3090}. We test the three 7B models, as their FP16-precision version can be run on an RTX 3090 GPU without further peak memory optimization techniques such as weight quantization~\cite{frantar2022gptq,lin2023awq} or offloading~\cite{sheng2023flexgen}. On these three models, \methodshort{} can obtain 1.94$\times$ to 2.40$\times$ speed-up on average.

For the five question categories that \methodshort{} can provide high-quality answers (i.e., \textit{knowledge}, \textit{common-sense}, \textit{generic}, \textit{roleplay}, \textit{counterfactual}), \methodshort{} can speed-up the overall answer generation process by 1.96$\times$ to 2.52$\times$ in the meantime. Note that for the \textit{math} category, despite the average speed-up being 1.20$\times$ by calculating the speed-up across the three math questions, \methodshort{} does not reduce the absolute latency of processing the three questions.


% Figure environment removed

\subsection{Actual Latency Testing}
\label{sec:app-actual-eff-test}

This section reports the actual \methodshort{} speed-up with batch testing (instead of analyzing with pre-made profiling tables) on NVIDIA A100. We test the actual end-to-end latency of the \methodshort{} and \methodbase{} decoding with the 9 open-source models. For each model, we run the speed-up test for five times and plot the box in \cref{fig:actual_speed_up}.

As shown in \cref{fig:model_speed_box}, the current \methodshort{} solution obtains a >$2\times$ speed-up on 6 out of the 9 open-source models (i.e., \Remark{vicuna7B1.1}, \Remark{vicuna7B1.3}, \Remark{ultralm13B}, \Remark{llamachat7B2}, \Remark{vicuna13B1.3}, and \Remark{llamachat13B2}), and a >$1.7$ speed-up on \Remark{openchat13B} and \Remark{vicuna33B1.3}. \methodshort{} achieves no speed-up on \Remark{stablevicuna13B}. As shown in \cref{fig:category_speedup_box}, for the five question categories that \methodshort{} can provide high-quality answers (i.e., \textit{knowledge}, \textit{common-sense}, \textit{generic}, \textit{roleplay}, \textit{counterfactual}), \methodshort{} can speed-up the overall answer generation process by 2.15$\times$ to 2.50$\times$ in the meantime.

% Figure environment removed

\end{document}