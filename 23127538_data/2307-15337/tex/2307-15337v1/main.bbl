\begin{thebibliography}{10}

\bibitem{claude}
Anthropic.
\newblock Introducing claude, May 2023.

\bibitem{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{cai2019once}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
\newblock Once-for-all: Train one network and specialize it for efficient
  deployment.
\newblock {\em arXiv preprint arXiv:1908.09791}, 2019.

\bibitem{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau,
  Laurent Sifre, and John Jumper.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock {\em arXiv preprint arXiv:2302.01318}, 2023.

\bibitem{chen2023dynamic}
Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, and Yuan Xie.
\newblock Dynamic n: M fine-grained structured sparse attention mechanism.
\newblock In {\em Proceedings of the 28th ACM SIGPLAN Annual Symposium on
  Principles and Practice of Parallel Programming}, pages 369--379, 2023.

\bibitem{llm-zoo-2023}
Zhihong Chen, Junying Chen, Hongbo Zhang, Feng Jiang, Guiming Chen, Fei Yu,
  Tiannan Wang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan,
  Haizhou Li, and Benyou Wang.
\newblock Llm zoo: democratizing chatgpt.
\newblock \url{https://github.com/FreedomIntelligence/LLMZoo}, 2023.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.

\bibitem{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock {\em Advances in Neural Information Processing Systems},
  35:16344--16359, 2022.

\bibitem{denton2014exploiting}
Emily~L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
  Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations.
\newblock {\em arXiv preprint arXiv:2305.14233}, 2023.

\bibitem{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, 2022.

\bibitem{fang2021turbotransformers}
Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou.
\newblock Turbotransformers: an efficient gpu serving system for transformer
  models.
\newblock In {\em Proceedings of the 26th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, pages 389--402, 2021.

\bibitem{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{ganesh2021compressing}
Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad~Ali Khan, Yin Yang, Hassan Sajjad,
  Preslav Nakov, Deming Chen, and Marianne Winslett.
\newblock Compressing large-scale transformer-based models: A case study on
  bert.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:1061--1080, 2021.

\bibitem{hugging2023assisted}
Joao Gante.
\newblock Assisted generation: a new direction toward low-latency text
  generation.
\newblock \url{https://huggingface.co/blog/assisted-generation}, 2023.
\newblock Accessed: 2023-06-23.

\bibitem{tfserving}
Google.
\newblock Tensorflow serving, 2021.

\bibitem{guo2017software}
Kaiyuan Guo, Song Han, Song Yao, Yu~Wang, Yuan Xie, and Huazhong Yang.
\newblock Software-hardware codesign for efficient neural network acceleration.
\newblock {\em IEEE Micro}, 37(2):18--25, 2017.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{dcai2023}
HazyResearch.
\newblock Data-centric ai.
\newblock \url{https://github.com/HazyResearch/data-centric-ai}.
\newblock Accessed: 2023-07-04.

\bibitem{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
  HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{ivanov2021data}
Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.
\newblock Data movement is all you need: A case study on optimizing
  transformers.
\newblock {\em Proceedings of Machine Learning and Systems}, 3:711--732, 2021.

\bibitem{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em Advances in neural information processing systems},
  35:22199--22213, 2022.

\bibitem{krishnamoorthi2018quantizing}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper.
\newblock {\em arXiv preprint arXiv:1806.08342}, 2018.

\bibitem{krizhevsky2014dp}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1404.5997}, 2014.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock {\em arXiv preprint arXiv:2104.08691}, 2021.

\bibitem{leviathan2022fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock {\em arXiv preprint arXiv:2211.17192}, 2022.

\bibitem{li2023camel}
Guohao Li, Hasan Abed Al~Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and
  Bernard Ghanem.
\newblock Camel: Communicative agents for "mind" exploration of large scale
  language model society, 2023.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock {\em arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{li2021terapipe}
Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and
  Ion Stoica.
\newblock Terapipe: Token-level pipeline parallelism for training large-scale
  language models.
\newblock In {\em International Conference on Machine Learning}, pages
  6543--6552. PMLR, 2021.

\bibitem{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and
  acceleration.
\newblock {\em arXiv preprint arXiv:2306.00978}, 2023.

\bibitem{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 55(9):1--35, 2023.

\bibitem{lu2017flexflow}
Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han, and Xiaowei Li.
\newblock Flexflow: A flexible dataflow accelerator architecture for
  convolutional neural networks.
\newblock In {\em 2017 IEEE International Symposium on High Performance
  Computer Architecture (HPCA)}, pages 553--564. IEEE, 2017.

\bibitem{miao2023specinfer}
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae
  Ying~Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao
  Jia.
\newblock Specinfer: Accelerating generative llm serving with speculative
  inference and token tree verification.
\newblock {\em arXiv preprint arXiv:2305.09781}, 2023.

\bibitem{semi_first}
Asit Mishra, Jorge~Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic,
  Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius.
\newblock Accelerating sparse deep neural networks.
\newblock {\em arXiv preprint arXiv:2104.08378}, 2021.

\bibitem{narayanan2019pipedream}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R
  Devanur, Gregory~R Ganger, Phillip~B Gibbons, and Matei Zaharia.
\newblock Pipedream: Generalized pipeline parallelism for dnn training.
\newblock In {\em Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, pages 1--15, 2019.

\bibitem{narayanan2021memory}
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
\newblock Memory-efficient pipeline-parallel dnn training.
\newblock In {\em International Conference on Machine Learning}, pages
  7937--7947. PMLR, 2021.

\bibitem{fastertransformer}
NVIDIA.
\newblock Fastertransformer, 2019.

\bibitem{triton}
NVIDIA.
\newblock Triton inference server, 2021.

\bibitem{openai2023gp4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{ouyang2022instructgpt}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744, 2022.

\bibitem{stablevicuna2023}
Duy Phung.
\newblock Stablevicuna-13b, May 2023.

\bibitem{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In {\em SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem{ren2021zerooffload}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
  Yang, Minjia Zhang, Dong Li, and Yuxiong He.
\newblock $\{$ZeRO-Offload$\}$: Democratizing $\{$Billion-Scale$\}$ model
  training.
\newblock In {\em 2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages 551--564, 2021.

\bibitem{shen2023hugginggpt}
Yongliang Shen, Kaitao Song, Xu~Tan, Dongsheng Li, Weiming Lu, and Yueting
  Zhuang.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in
  huggingface.
\newblock {\em arXiv preprint arXiv:2303.17580}, 2023.

\bibitem{sheng2023flexgen}
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel~Y Fu,
  Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph~E Gonzalez, et~al.
\newblock High-throughput generative inference of large language models with a
  single gpu.
\newblock {\em arXiv preprint arXiv:2303.06865}, 2023.

\bibitem{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4222--4235, 2020.

\bibitem{stern2018blockwise}
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{sun2023spectr}
Ziteng Sun, Ananda~Theertha Suresh, Jae~Hun Ro, Ahmad Beirami, Himanshu Jain,
  Felix Yu, Michael Riley, and Sanjiv Kumar.
\newblock Spectr: Fast speculative decoding via optimal transport.
\newblock In {\em Workshop on Efficient Systems for Foundation Models @
  ICML2023}, 2023.

\bibitem{alpaca2023}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori Hashimoto.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \url{https://crfm.stanford.edu/2023/03/13/alpaca.html}.
\newblock Accessed: 2023-06-23.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem{openllms23}
Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu.
\newblock Openllms: Less is more for open-source models, July 2023.

\bibitem{wang2021spatten}
Hanrui Wang, Zhekai Zhang, and Song Han.
\newblock Spatten: Efficient sparse attention architecture with cascade token
  and head pruning.
\newblock In {\em 2021 IEEE International Symposium on High-Performance
  Computer Architecture (HPCA)}, pages 97--110. IEEE, 2021.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{wei2021flan}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:24824--24837, 2022.

\bibitem{group-lasso}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{xiao2022smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock {\em arXiv preprint arXiv:2211.10438}, 2022.

\bibitem{xu2021gspmd}
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul
  Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et~al.
\newblock Gspmd: general and scalable parallelization for ml computation
  graphs.
\newblock {\em arXiv preprint arXiv:2105.04663}, 2021.

\bibitem{yu2022orca}
Gyeong-In Yu, Joo~Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.
\newblock Orca: A distributed serving system for $\{$Transformer-Based$\}$
  generative models.
\newblock In {\em 16th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 22)}, pages 521--538, 2022.

\bibitem{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock {\em Advances in neural information processing systems},
  33:17283--17297, 2020.

\bibitem{zha2023data}
Daochen Zha, Zaid~Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang,
  Shaochen Zhong, and Xia Hu.
\newblock Data-centric artificial intelligence: A survey.
\newblock {\em arXiv preprint arXiv:2303.10158}, 2023.

\bibitem{zhai2022bytetransformer}
Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong
  Chen, Xin Liu, and Yibo Zhu.
\newblock Bytetransformer: A high-performance transformer boosted for
  variable-length inputs.
\newblock {\em arXiv preprint arXiv:2210.03052}, 2022.

\bibitem{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem{zheng2022alpa}
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
  Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric~P Xing, et~al.
\newblock Alpa: Automating inter-and $\{$Intra-Operator$\}$ parallelism for
  distributed deep learning.
\newblock In {\em 16th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 22)}, pages 559--578, 2022.

\bibitem{zhou2022pets}
Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun.
\newblock $\{$PetS$\}$: A unified framework for $\{$Parameter-Efficient$\}$
  transformers serving.
\newblock In {\em 2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages 489--504, 2022.

\bibitem{zoph2016neural}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\end{thebibliography}
