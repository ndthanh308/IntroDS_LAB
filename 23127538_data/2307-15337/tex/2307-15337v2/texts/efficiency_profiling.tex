\label{sec:app-profiling-estimate}

We run the profiling on the target GPU (NVIDIA A100-80G and NVIDIA RTX 3090) with CUDA 11.7, using the Hugging Face transformer library 4.28.1 and PyTorch 2.0.1. The host of A100-80G has an Intel Xeon Platinum 8358P CPU and 1T memory. The host of RTX 3090 has an Intel Xeon Gold 6246R CPU and 512G memory.


\paragraph{Latency profiling and estimation.} For the decoding phase, we denote $t_{B}^{D}(k)$ as the latency of batched decoding the $k+1$-th token with batch size $B$, where the superscript $D$ stands for ``decode''. For each batch size $B=1,\cdots,16$ and each context length $k=1,\cdots,1024$, we use \verb/torch.cuda.Event/ to record the latency of decoding one token. We run each decoding three times continuously and take their geometric mean as $\{t_{B}^{D}(k)\}_{k=1,\cdots,1024; B=1,\cdots,16}$.
For the prefilling phase, we profile the latency of batched prefilling the inputs with token length $k$ in $\mbox{range}(1,700,10)$ and batch size $B=1,\cdots,16$, and denote it as $t_{B}^{P}(k)$, where the superscript $P$ stands for ``prefill''. We run each test seven times continuously, regard the first two times as the warmup tests, and take the geometric mean of the last five times as $\{t_{B}^{P}(k)\}_{k=1,11,\cdots,691; B=1,\cdots,16}$. Once we get the latency profiling table, given a request with $l_i$ tokens and the decoding batch size $B$, the latency of generating $l_o$ tokens can be estimated as:
\begin{equation}
    T(l_i, l_o, B) = \tilde{t}_{B}^{P}(l_i) + \sum_{k=l_i}^{l_i+l_o-1} t_{B}^{D}(k),
  \end{equation}
  where the subscripts $i$ and $o$ stand for ``input'' and ``output''. Note that we only test the prefilling latency every ten token lengths (i.e., $1, 11, 21, \cdots$) for fast profiling and estimate $\tilde{t}_{B}^{P}(l_i)$ by $t_{B}^{P}(\lfloor\frac{l_i}{10}\rfloor \times 10 + 1)$.

The \methodshort{} decoding process consists of two stages: the skeleton stage and the point-expanding stage. Denoting the token length of the skeleton request and skeleton response as $l_i^{s}$ and $l_o^{s}$, the token length of the longest point-expanding request and the longest point-expanding response as $l_i^{pe}$ and $l_o^{pe}$, the number of the points as $B$, we can compute the latency of the skeleton and point-expanding stages as:
\begin{align}
    L^{s}(l_i^{s}, l_o^{s}) &= T(l_i^{s}, l_o^{s}, 1), \\
    L^{pe}(l_i^{pe}, l_o^{pe}, B) &= T(l_i^{pe}, l_o^{pe}, B).
\end{align}


Using the latency profiling table, we can further estimate the average GPU computing performance in FLOPS (i.e., FLOPs per second) of decoding $l_o$ tokens with prefilling length $l_i$ as 
\begin{equation}
    P^{D}(l_i, l_o, B) = \frac{\sum_{k=l_i}^{l_i+l_o-1} f_{B}^{D}(k)}{\sum_{k=l_i}^{l_i+l_o-1} t_{B}^{D}(k)},
  \end{equation}
  where $f_{B}^{D}(k)$ denotes the FLOPs of decoding one token with context length $k$, which is calculated by DeepSpeed's FLOPs profiler~\footnote{\url{https://deepspeed.readthedocs.io/en/latest/flops-profiler.html}}. 
  \cref{fig:efficiency-batch-vs-single-perf} reports the average GPU computing performance during the process of decoding 64 tokens (prefilling length=128), i.e., $P^{D}(128, 64, B)$.

\paragraph{Memory profiling and evaluation.} To evaluate the peak memory, we use \verb/torch.cuda.max_memory_allocated/ to record the memory consumption of prefilling sequences of different lengths and decoding with different context lengths and a batch size ranging from 1 to 16. Then, we calculate the peak memory of each stage as the maximum value of the prefilling and decoding phases, and calculate the overall peak memory of \methodshort{} as the maximum value of the skeleton and point-expanding stages.


  








