

\subsection{Efficient LLMs}







Extensive research has been dedicated to enhancing the throughput and latency of LLM inference. We first discuss model-level architecture design or compression techniques. 
These techniques change the model and can benefit both the latency and throughput but require finetuning to retain the model quality.
Then, we discuss system-level efforts that optimize the computational graph or the assignment and scheduling of the computational graph on computation and storage devices.
Most system-level efforts accelerate the prefilling phase or focus on improving the throughput.
Finally, we discuss some research efforts that share a similar motivation to ours, namely, addressing the efficiency issue of sequential decoding.

  
\paragraph{Model-level optimization.} 
Considerable architectural design efforts have emerged to (1) improve the scalability w.r.t. model size by introducing mixture-of-expert inference~\citep{lepikhin2021gshard,fedus2022switch}, (2) address the quadratic complexity w.r.t. input size of attention by designing new attention mechanisms~\citep{kitaev2020reformer,wang2020linformer}, (3) reduce the memory access and footprint of attention by using multi-query attention~\citep{shazeer2019fast}, and so on. However, these methods usually require a substantial re-training cost. The model compression techniques require a smaller amount of fine-tuning by reducing the model complexity of a pre-trained LLM from certain aspects~\citep{ganesh2021compressing}. Representative techniques include quantization~\citep{xiao2022smoothquant,frantar2022gptq,lin2023awq}, the static or dynamic pruning of weights, activation, and attention~\citep{semi_first,zaheer2020big,wang2021spatten,chen2023dynamic}, and so on.

Zooming out from LLM compression to the whole field of model compression, we can see that model co-design or compression for efficiency has received tremendous attention in the past few years and has grown into large research fields, such as pruning~\citep{han2015deep,group-lasso}, quantization~\citep{krishnamoorthi2018quantizing}, factorization~\citep{denton2014exploiting}, and neural architecture search~\citep{zoph2016neural,elsken2019neural,cai2019once}. %
\textit{Different from the model co-design paradigm, SoT is in a ``\textbf{content co-organization for efficiency}'' paradigm for improving the LLM efficiency}.
Along with the growth in the LLM capabilities and amount of LLM-generated data, data-level techniques could become important tools in the efficient LLM toolbox.



\paragraph{System-level optimization.} %
In the realm of lossless acceleration, considerable efforts have been devoted to addressing the I/O-bound nature of LLMs on modern hardware platforms~\citep{dao2022flashattention}. Numerous studies~\citep{dao2022flashattention,zhai2022bytetransformer,ivanov2021data,fastertransformer} have focused on adjusting the computational graph by fusing and implementing operations in an I/O-friendly way. As a representative method, FlashAttention~\citep{dao2022flashattention} fuses all operations of one attention into one GPU kernel with spatially tiled computation to reduce the off-chip I/O of the attention map. While FlashAttention can effectively accelerate training and the prefilling phase of inference, it cannot accelerate the decoding phase much (when the batch size is small), as it is the I/O of weights rather than activation or attention map that bottlenecks the decoding phase. For example, when the context length is 64, decoding one token using \Remark{llama}-7B needs to load each of the 7B parameters from the off-chip HBM onto the GPU chip at least once, but only transferring about 20M (0.02B) activation values between the off-chip HBM and GPU chip.

In order to satisfy Service Level Objectives, serving systems focus on improving the serving throughput under latency constraints. To this end, serving systems~\citep{fang2021turbotransformers,triton,tfserving}
pack multiple queries together into a batch to improve the hardware utilization.
The batching technique has proven highly effective in enhancing throughput, leading to the development of various variants. For example, some work designs methods to decide which queries to batch together~\citep{fang2021turbotransformers,zhou2022pets}, while others selectively batch parts of the model to enable fine-grained iteration-level batching~\citep{yu2022orca} or multi-task batching~\citep{zhou2022pets}.
Various model parallelism~\citep{lu2017flexflow,huang2019gpipe,narayanan2019pipedream,rajbhandari2020zero,narayanan2021memory,li2021terapipe,zheng2022alpa} and offloading~\citep{ren2021zerooffload,sheng2023flexgen} techniques have been proposed to maximize the throughput of LLM training or inference. In a nutshell, given the computational graph and device configurations, these techniques optimize the split, assignment, and scheduling of computations, storage, and communications on devices.
In addition to the model parallelism and batching techniques, an efficient memory management mechanism for LLM workloads is also an essential feature in the serving systems~\citep{kwon2023efficient,sensetime2023lightllm,sensetime2023openppl}.

To sum up, these system-level techniques mainly help with the throughput in training and batched inference. %
They can be used by \methodshort{} to improve the throughput of the batched decoding of multiple segments. %
This means that \textit{\methodshort{} can harness the power of these throughput-oriented techniques and make them help with the end-to-end latency}, offering a new dimension for better trading off latency and throughput in future serving systems.

Another parallelism perspective to position \methodshort{} is that \textit{\methodshort{} guides the LLM to adjust the sequential workload to become ``inter-content'' parallelizable}, which differs from the parallelism levels in existing serving systems, including inter-instance~\citep{krizhevsky2014dp,rajbhandari2020zero}, inter-operation~\citep{huang2019gpipe,narayanan2019pipedream,narayanan2021memory}, intra-operation~\citep{xu2021gspmd}, and inter-token~\citep{li2021terapipe}.
It may be worthwhile to explore \textit{the integration of \methodshort{} into serving systems to maximize the hardware utilization}. %


\paragraph{Decoding optimization.} %
One bottleneck for the end-to-end latency lies in the autoregressive decoding phase, where tokens must be generated one by one. Due to the dependency between tokens, the computation of different tokens cannot be parallelized, causing severe under-utilization of GPU. In order to improve the end-to-end decoding latency of a given LLM, speculative decoding methods~\citep{stern2018blockwise,leviathan2022fast,chen2023accelerating,hugging2023assisted,sun2023spectr,miao2023specinfer} propose to use cheaper approaches to generate short candidate token sequences, for example, by sequentially decoding with an assisting model much smaller than the given LLM. Then, they use the LLM to parallelly verify the candidates and keep the prefix sequence that matches the LLM's verification results.

Another line of work that shares the motivation of addressing the autoregressive efficiency issue is non-autoregressive generation (NAG) methods~\citep{gu2018nonautoregressive,xiao2023survey}. 
NAG methods sample consecutive tokens parallelly, often with the aid of a modified and tuned model. To maintain the 
answer quality, instead of sampling for one iteration, many NAG methods refine the output parallelly for multiple iterations~\citep{xiao2023survey,santilli2023accelerating}.

To summarize, the speculative decoding methods use assisting models for \textit{letting the LLM conduct parallel verification of consecutive tokens}, and the NAG methods rely on specially designed models, training schemes, or sampling schemes for \textit{the parallel sampling and refinement of consecutive tokens}. In contrast, \methodshort{} prompts the LLM itself to plan the contents in a way that permits \textit{the parallel generation of multiple tokens in different segments}. \methodshort{} exploits the emerging instruction-following and planning ability of SoTA LLMs rather than relying on specially designed modeling, sampling, and training schemes. This is different from all existing work that targets the autoregressive efficiency issue. 










\subsection{Prompting Methods for LLMs}

In recent years, the ``pre-train, prompt, and predict'' paradigm has emerged~\citep{liu2023pre}, which designs prompts comprising task descriptions and (optionally) a few demonstrations to guide pre-trained LLMs in generating answers for a wide range of downstream tasks. %
Researchers found that instruction-tuned LLMs~\citep{brown2020gpt3,wei2021flan,ouyang2022instructgpt,chung2022scaling,alpaca2023} possess a strong ability to (1) generalize to new tasks thanks to the diverse natural language descriptions encountered during instruction tuning, and (2) learn in-context using a few demonstrations without weight tuning.

In virtue of these abilities, the field has been manually engineering~\citep{brown2020gpt3,kojima2022large,shen2023hugginggpt,li2023camel}, automatic searching~\citep{shin2020autoprompt}, or continuously tuning~\citep{li2021prefix,lester2021power} the prompts for uncovering the capabilities of LLMs on downstream tasks. There are a bunch of prompting methods that improves the reasoning performance of LLMs by designing thinking flows mimicking human reasoning: (1) mimicking the step-by-step or compositional thinking structure~\citep{wei2022chain,kojima2022large,press2022selfask,yao2023tree,besta2023graph,zhang2023cumulative}, (2) designing multiple reasoning paths and their aggregation~\citep{wang2022selfconsistency,yao2023tree,li2023diverse}, and (3) using tools for calculation and information retrieval~\citep{chen2022program,yao2022react,schick2023toolformer}. As a representative example, the Chain-of-Thought prompts largely improve the performance on tasks that require logical reasoning by simply providing a ``Let's think step by step''~\citep{kojima2022large} instruction or a few demonstrations~\citep{wei2022chain}. Another topic that arises quite a surge of interests is to prompt LLMs to help finish complex multi-modality task~\citep{shen2023hugginggpt,zhu2023ghost}. For example, HuggingGPT~\citep{shen2023hugginggpt} design prompts to guide the LLM to generate structural JSON for the orchestration of multi-model execution to finish complex tasks.

To summarize, the large literature on prompting methods has been aiming at uncovering different capabilities of LLM and improving the answer quality %
on different downstream tasks. In contrast, \textit{\methodshort{} is a first attempt at exploiting the power of prompting to improve efficiency}.




