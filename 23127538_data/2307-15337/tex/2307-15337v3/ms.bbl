\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anthropic(2023)]{claude}
Anthropic.
\newblock Introducing claude, May 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Besta et~al.(2023)Besta, Blach, Kubicek, Gerstenberger, Gianinazzi,
  Gajda, Lehmann, Podstawski, Niewiadomski, Nyczyk, et~al.]{besta2023graph}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi,
  Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr
  Nyczyk, et~al.
\newblock Graph of thoughts: Solving elaborate problems with large language
  models.
\newblock \emph{arXiv preprint arXiv:2308.09687}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Cai et~al.(2019)Cai, Gan, Wang, Zhang, and Han]{cai2019once}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
\newblock Once-for-all: Train one network and specialize it for efficient
  deployment.
\newblock \emph{arXiv preprint arXiv:1908.09791}, 2019.

\bibitem[Chase(2022)]{Chase_LangChain_2022}
Harrison Chase.
\newblock {LangChain}, October 2022.
\newblock URL \url{https://github.com/hwchase17/langchain}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Borgeaud, Irving, Lespiau, Sifre,
  and Jumper]{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau,
  Laurent Sifre, and John Jumper.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock \emph{arXiv preprint arXiv:2302.01318}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2022)Chen, Ma, Wang, and Cohen]{chen2022program}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W Cohen.
\newblock Program of thoughts prompting: Disentangling computation from
  reasoning for numerical reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.12588}, 2022.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Qu, Quan, Liu, Ding, and
  Xie]{chen2023dynamic}
Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, and Yuan Xie.
\newblock Dynamic n: M fine-grained structured sparse attention mechanism.
\newblock In \emph{Proceedings of the 28th ACM SIGPLAN Annual Symposium on
  Principles and Practice of Parallel Programming}, pp.\  369--379,
  2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Chen, Zhang, Jiang, Chen, Yu,
  Wang, Liang, Zhang, Zhang, Li, Wan, Li, and Wang]{llm-zoo-2023}
Zhihong Chen, Junying Chen, Hongbo Zhang, Feng Jiang, Guiming Chen, Fei Yu,
  Tiannan Wang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan,
  Haizhou Li, and Benyou Wang.
\newblock Llm zoo: democratizing chatgpt.
\newblock \url{https://github.com/FreedomIntelligence/LLMZoo},
  2023{\natexlab{c}}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16344--16359, 2022.

\bibitem[Denton et~al.(2014)Denton, Zaremba, Bruna, LeCun, and
  Fergus]{denton2014exploiting}
Emily~L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and
  Zhou]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
  Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}, 2023.

\bibitem[Du et~al.(2022)Du, Qian, Liu, Ding, Qiu, Yang, and Tang]{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  320--335, 2022.

\bibitem[Elsken et~al.(2019)Elsken, Metzen, and Hutter]{elsken2019neural}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 1997--2017, 2019.

\bibitem[Fang et~al.(2021)Fang, Yu, Zhao, and Zhou]{fang2021turbotransformers}
Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou.
\newblock Turbotransformers: an efficient gpu serving system for transformer
  models.
\newblock In \emph{Proceedings of the 26th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, pp.\  389--402, 2021.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0
  (1):\penalty0 5232--5270, 2022.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Ganesh et~al.(2021)Ganesh, Chen, Lou, Khan, Yang, Sajjad, Nakov, Chen,
  and Winslett]{ganesh2021compressing}
Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad~Ali Khan, Yin Yang, Hassan Sajjad,
  Preslav Nakov, Deming Chen, and Marianne Winslett.
\newblock Compressing large-scale transformer-based models: A case study on
  bert.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 1061--1080, 2021.

\bibitem[Gante(2023)]{hugging2023assisted}
Joao Gante.
\newblock Assisted generation: a new direction toward low-latency text
  generation.
\newblock \url{https://huggingface.co/blog/assisted-generation}, 2023.
\newblock Accessed: 2023-06-23.

\bibitem[Google(2021)]{tfserving}
Google.
\newblock Tensorflow serving, 2021.
\newblock URL \url{https://github.com/tensorflow/serving}.

\bibitem[Gu et~al.(2018)Gu, Bradbury, Xiong, Li, and
  Socher]{gu2018nonautoregressive}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~O.K. Li, and Richard Socher.
\newblock Non-autoregressive neural machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1l8BtlCb}.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[HazyResearch(2023)]{dcai2023}
HazyResearch.
\newblock Data-centric ai.
\newblock \url{https://github.com/HazyResearch/data-centric-ai}, 2023.
\newblock Accessed: 2023-07-04.

\bibitem[Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and
  Han]{huang2022large}
Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu,
  and Jiawei Han.
\newblock Large language models can self-improve.
\newblock \emph{arXiv preprint arXiv:2210.11610}, 2022.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, et~al.]{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
  HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Ivanov et~al.(2021)Ivanov, Dryden, Ben-Nun, Li, and
  Hoefler]{ivanov2021data}
Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.
\newblock Data movement is all you need: A case study on optimizing
  transformers.
\newblock \emph{Proceedings of Machine Learning and Systems}, 3:\penalty0
  711--732, 2021.

\bibitem[Jiang et~al.(2023)Jiang, Wu, Lin, Yang, and
  Qiu]{jiang-etal-2023-llmlingua}
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock Llmlingua: Compressing prompts for accelerated inference of large
  language models.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics,
  December 2023.
\newblock URL \url{https://arxiv.org/abs/2310.05736}.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 22199--22213, 2022.

\bibitem[Krishnamoorthi(2018)]{krishnamoorthi2018quantizing}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper.
\newblock \emph{arXiv preprint arXiv:1806.08342}, 2018.

\bibitem[Krizhevsky(2014)]{krizhevsky2014dp}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1404.5997}, 2014.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang,
  and Stoica]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu,
  Joseph~E Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with
  pagedattention.
\newblock \emph{arXiv preprint arXiv:2309.06180}, 2023.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{lepikhin2021gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock {\{}GS{\}}hard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qrwe7XHTmYb}.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Leviathan et~al.(2022)Leviathan, Kalman, and
  Matias]{leviathan2022fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock \emph{arXiv preprint arXiv:2211.17192}, 2022.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Hammoud, Itani, Khizbullin, and
  Ghanem]{li2023camel}
Guohao Li, Hasan Abed Al~Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and
  Bernard Ghanem.
\newblock Camel: Communicative agents for "mind" exploration of large scale
  language model society, 2023{\natexlab{a}}.

\bibitem[Li \& Liang(2021)Li and Liang]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhang, Dubois, Taori, Gulrajani,
  Guestrin, Liang, and Hashimoto]{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Lin, Zhang, Fu, Chen, Lou, and
  Chen]{li2023diverse}
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and
  Weizhu Chen.
\newblock Making language models better reasoners with step-aware verifier.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  5315--5333,
  2023{\natexlab{c}}.

\bibitem[Li et~al.(2021)Li, Zhuang, Guo, Zhuo, Zhang, Song, and
  Stoica]{li2021terapipe}
Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and
  Ion Stoica.
\newblock Terapipe: Token-level pipeline parallelism for training large-scale
  language models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6543--6552. PMLR, 2021.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, and Han]{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and
  acceleration.
\newblock \emph{arXiv preprint arXiv:2306.00978}, 2023.

\bibitem[Liu et~al.(2023)Liu, Yuan, Fu, Jiang, Hayashi, and Neubig]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (9):\penalty0 1--35, 2023.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lu et~al.(2017)Lu, Yan, Li, Gong, Han, and Li]{lu2017flexflow}
Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han, and Xiaowei Li.
\newblock Flexflow: A flexible dataflow accelerator architecture for
  convolutional neural networks.
\newblock In \emph{2017 IEEE International Symposium on High Performance
  Computer Architecture (HPCA)}, pp.\  553--564. IEEE, 2017.

\bibitem[Miao et~al.(2023)Miao, Oliaro, Zhang, Cheng, Wang, Wong, Chen, Arfeen,
  Abhyankar, and Jia]{miao2023specinfer}
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae
  Ying~Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao
  Jia.
\newblock Specinfer: Accelerating generative llm serving with speculative
  inference and token tree verification.
\newblock \emph{arXiv preprint arXiv:2305.09781}, 2023.

\bibitem[Mishra et~al.(2021)Mishra, Latorre, Pool, Stosic, Stosic, Venkatesh,
  Yu, and Micikevicius]{semi_first}
Asit Mishra, Jorge~Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic,
  Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius.
\newblock Accelerating sparse deep neural networks.
\newblock \emph{arXiv preprint arXiv:2104.08378}, 2021.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri,
  Devanur, Ganger, Gibbons, and Zaharia]{narayanan2019pipedream}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R
  Devanur, Gregory~R Ganger, Phillip~B Gibbons, and Matei Zaharia.
\newblock Pipedream: Generalized pipeline parallelism for dnn training.
\newblock In \emph{Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, pp.\  1--15, 2019.

\bibitem[Narayanan et~al.(2021)Narayanan, Phanishayee, Shi, Chen, and
  Zaharia]{narayanan2021memory}
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
\newblock Memory-efficient pipeline-parallel dnn training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7937--7947. PMLR, 2021.

\bibitem[NVIDIA(2019)]{fastertransformer}
NVIDIA.
\newblock Fastertransformer, 2019.
\newblock URL \url{https://github.com/NVIDIA/FasterTransformer}.

\bibitem[NVIDIA(2021)]{triton}
NVIDIA.
\newblock Triton inference server, 2021.
\newblock URL \url{https://developer.nvidia.com/triton-inference-server}.

\bibitem[OpenAI(2023)]{openai2023gp4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022instructgpt}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Phung(2023)]{stablevicuna2023}
Duy Phung.
\newblock Stablevicuna-13b, May 2023.
\newblock URL \url{https://huggingface.co/CarperAI/stable-vicuna-13b-delta}.

\bibitem[Press et~al.(2022)Press, Zhang, Min, Schmidt, Smith, and
  Lewis]{press2022selfask}
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah~A Smith, and Mike
  Lewis.
\newblock Measuring and narrowing the compositionality gap in language models.
\newblock \emph{arXiv preprint arXiv:2210.03350}, 2022.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pp.\  1--16. IEEE, 2020.

\bibitem[Ren et~al.(2021)Ren, Rajbhandari, Aminabadi, Ruwase, Yang, Zhang, Li,
  and He]{ren2021zerooffload}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
  Yang, Minjia Zhang, Dong Li, and Yuxiong He.
\newblock $\{$ZeRO-Offload$\}$: Democratizing $\{$Billion-Scale$\}$ model
  training.
\newblock In \emph{2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pp.\  551--564, 2021.

\bibitem[Santilli et~al.(2023)Santilli, Severino, Postolache, Maiorca, Mancusi,
  Marin, and Rodol{\`a}]{santilli2023accelerating}
Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca,
  Michele Mancusi, Riccardo Marin, and Emanuele Rodol{\`a}.
\newblock Accelerating transformer inference for translation via parallel
  decoding.
\newblock In \emph{acl}, 2023.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli,
  Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria
  Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{arXiv preprint arXiv:2302.04761}, 2023.

\bibitem[SenseTime(2023{\natexlab{a}})]{sensetime2023lightllm}
SenseTime.
\newblock Lightllm.
\newblock \url{https://github.com/ModelTC/lightllm}, 2023{\natexlab{a}}.
\newblock Accessed: 2023-09-26.

\bibitem[SenseTime(2023{\natexlab{b}})]{sensetime2023openppl}
SenseTime.
\newblock Openppl.
\newblock \url{https://github.com/openppl-public/ppl.nn}, 2023{\natexlab{b}}.
\newblock Accessed: 2023-09-26.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem[Shen et~al.(2023)Shen, Song, Tan, Li, Lu, and
  Zhuang]{shen2023hugginggpt}
Yongliang Shen, Kaitao Song, Xu~Tan, Dongsheng Li, Weiming Lu, and Yueting
  Zhuang.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in
  huggingface.
\newblock \emph{arXiv preprint arXiv:2303.17580}, 2023.

\bibitem[Sheng et~al.(2023)Sheng, Zheng, Yuan, Li, Ryabinin, Fu, Xie, Chen,
  Barrett, Gonzalez, et~al.]{sheng2023flexgen}
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel~Y Fu,
  Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph~E Gonzalez, et~al.
\newblock High-throughput generative inference of large language models with a
  single gpu.
\newblock \emph{arXiv preprint arXiv:2303.06865}, 2023.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  4222--4235, 2020.

\bibitem[Stern et~al.(2018)Stern, Shazeer, and Uszkoreit]{stern2018blockwise}
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Sun et~al.(2023)Sun, Suresh, Ro, Beirami, Jain, Yu, Riley, and
  Kumar]{sun2023spectr}
Ziteng Sun, Ananda~Theertha Suresh, Jae~Hun Ro, Ahmad Beirami, Himanshu Jain,
  Felix Yu, Michael Riley, and Sanjiv Kumar.
\newblock Spectr: Fast speculative decoding via optimal transport.
\newblock In \emph{Workshop on Efficient Systems for Foundation Models @
  ICML2023}, 2023.
\newblock URL \url{https://openreview.net/forum?id=d0mGsaheuT}.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2818--2826, 2016.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca2023}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori Hashimoto.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \url{https://crfm.stanford.edu/2023/03/13/alpaca.html}, 2023.
\newblock Accessed: 2023-06-23.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher,
  Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami,
  Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann,
  Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov,
  Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva,
  Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang,
  Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and
  Scialom]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models,
  2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Cheng, Yu, and Liu]{openllms23}
Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu.
\newblock Openllms: Less is more for open-source models, July
  2023{\natexlab{a}}.
\newblock URL \url{https://github.com/imoneoi/openchat}.

\bibitem[Wang et~al.(2021)Wang, Zhang, and Han]{wang2021spatten}
Hanrui Wang, Zhekai Zhang, and Song Han.
\newblock Spatten: Efficient sparse attention architecture with cascade token
  and head pruning.
\newblock In \emph{2021 IEEE International Symposium on High-Performance
  Computer Architecture (HPCA)}, pp.\  97--110. IEEE, 2021.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery,
  and Zhou]{wang2022selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Popordanoska, Bertels, Lemmens,
  and Blaschko]{wang2023dice}
Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, and Matthew~B
  Blaschko.
\newblock Dice semimetric losses: Optimizing the dice score with soft labels.
\newblock In \emph{Medical Image Computing and Computer Assisted Intervention},
  2023{\natexlab{b}}.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2021flan}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou,
  et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24824--24837, 2022.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{group-lasso}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and
  Han]{xiao2022smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Wu, Guo, Li, Zhang, Qin, and
  Liu]{xiao2023survey}
Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and
  Tie-yan Liu.
\newblock A survey on non-autoregressive generation for neural machine
  translation and beyond.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2023.

\bibitem[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and
  Jiang]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex
  instructions.
\newblock \emph{arXiv preprint arXiv:2304.12244}, 2023.

\bibitem[Xu et~al.(2021)Xu, Lee, Chen, Hechtman, Huang, Joshi, Krikun,
  Lepikhin, Ly, Maggioni, et~al.]{xu2021gspmd}
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul
  Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et~al.
\newblock Gspmd: general and scalable parallelization for ml computation
  graphs.
\newblock \emph{arXiv preprint arXiv:2105.04663}, 2021.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and
  Cao]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
  and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and
  Narasimhan]{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao,
  and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock \emph{arXiv preprint arXiv:2305.10601}, 2023.

\bibitem[Yu et~al.(2022)Yu, Jeong, Kim, Kim, and Chun]{yu2022orca}
Gyeong-In Yu, Joo~Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.
\newblock Orca: A distributed serving system for $\{$Transformer-Based$\}$
  generative models.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 22)}, pp.\  521--538, 2022.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17283--17297, 2020.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 15476--15488, 2022.

\bibitem[Zha et~al.(2023)Zha, Bhat, Lai, Yang, Jiang, Zhong, and
  Hu]{zha2023data}
Daochen Zha, Zaid~Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang,
  Shaochen Zhong, and Xia Hu.
\newblock Data-centric artificial intelligence: A survey.
\newblock \emph{arXiv preprint arXiv:2303.10158}, 2023.

\bibitem[Zhai et~al.(2022)Zhai, Jiang, Wang, Jia, Zhang, Chen, Liu, and
  Zhu]{zhai2022bytetransformer}
Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong
  Chen, Xin Liu, and Yibo Zhu.
\newblock Bytetransformer: A high-performance transformer boosted for
  variable-length inputs.
\newblock \emph{arXiv preprint arXiv:2210.03052}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Yang, Yuan, and Yao]{zhang2023cumulative}
Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao.
\newblock Cumulative reasoning with large language models.
\newblock \emph{arXiv preprint arXiv:2308.04371}, 2023.

\bibitem[Zheng et~al.(2022)Zheng, Li, Zhang, Zhuang, Chen, Huang, Wang, Xu,
  Zhuo, Xing, et~al.]{zheng2022alpa}
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
  Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric~P Xing, et~al.
\newblock Alpa: Automating inter-and $\{$Intra-Operator$\}$ parallelism for
  distributed deep learning.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 22)}, pp.\  559--578, 2022.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu,
  et~al.]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Wei, Zhang, and Sun]{zhou2022pets}
Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun.
\newblock $\{$PetS$\}$: A unified framework for $\{$Parameter-Efficient$\}$
  transformers serving.
\newblock In \emph{2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pp.\  489--504, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Tian, Tao, Su, Yang, Huang, Li, Lu, Wang,
  et~al.]{zhu2023ghost}
Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao
  Huang, Bin Li, Lewei Lu, Xiaogang Wang, et~al.
\newblock Ghost in the minecraft: Generally capable agents for open-world
  enviroments via large language models with text-based knowledge and memory.
\newblock \emph{arXiv preprint arXiv:2305.17144}, 2023.

\bibitem[Zoph \& Le(2017)Zoph and Le]{zoph2016neural}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\end{thebibliography}
