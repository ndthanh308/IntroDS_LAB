

Despite the optimizations made to the decoding phase, \methodshort{} brings overhead to the prefilling phase as the model needs to handle additional \methodshort{} prompts. \cref{tab:api_token_usage} reports \methodshort{}'s prefilling overhead for the API-based models. These statistics are averaged across the \vicunadataset{} questions that are suitable for \methodshort{} (according to our manual annotation). We can see that \methodshort{} significantly increases the number of prefilling tokens. This is because that \methodshort{} issues an independent point-expanding request for each point, with the average number of points being 6.8 on \vicunadataset{} dataset across all evaluated models. Consequently, the APIs need to prefill the point-expanding request multiple times.


\begin{table}[H]
  \centering
  \caption{\methodshort{}'s prefilling token overhead for API-based models.}
\label{tab:api_token_usage}
\begin{tabular}{c|cccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c}{Prefill Phase}  \\ 
\cmidrule(lr){2-5} & Normal & SoT Stage 1 & SoT Stage 2 & Ratio (SoT / Normal) \\ \midrule
\Remark{claude} & 10.33 & 155.33 & 730.91 & 85.79   \\ 
\Remark{chatgpt} & 10.21 & 136.33 & 480.95 & 60.46   \\ 
\Remark{gpt4} & 10.21 & 72.44 & 838.26 & 89.20  \\ 
\bottomrule
\end{tabular}
\end{table}

When using \methodshort{} to serve the open-source models, a simple and small trick is to prefill the common prefix of point-expanding requests with a batch size of 1 during Stage 2 (i.e., the point-expanding stage). \cref{tab:opensource_token_usage} shows the prefilling overhead after applying the trick. Although the ratio is considerably smaller compared to that of the API-based models, this computational overhead remains a concern, especially during periods of high system workload.

There are some possibilities to further reduce the token and computational overhead that are worth exploring in future work. To name a few: (1) When using \methodshort{} in serving systems, we can simply reuse the key-value cache containing the question and skeleton from Stage 1 during Stage 2, rather than re-prefilling them as in a multi-round conversation. (2) Generally, as LLM capabilities continue to evolve and prompt tuning techniques advance~\citep{shin2020autoprompt,li2021prefix,lester2021power,jiang-etal-2023-llmlingua}, the possibility of using much shorter prompts to activate the \methodshort{} mode in the future holds promise, which would significantly mitigate the token or computational overhead.



\begin{table}[H]
  \centering
  \caption{\methodshort{}'s computational overhead (in terms of the number of prefilling tokens) for open-source models.}
\label{tab:opensource_token_usage}
\begin{tabular}{c|cccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c}{Prefill Phase}  \\ 
\cmidrule(lr){2-5}  & Naive & SoT Stage 1  & SoT Stage 2 & Ratio (SoT / Normal)  \\ \midrule
\Remark{llamachat7B2} & 12.52 & 171.41     & 216.49 & 30.98   \\ 
\Remark{llamachat13B2} & 12.52 & 171.41    & 216.41 & 30.98   \\ 
\Remark{openchat13B} & 12.52 & 171.41      & 234.38 & 32.41  \\ 
\Remark{vicuna7B1.3} & 12.52 & 171.41      & 211.61 & 30.59  \\ 
\Remark{vicuna13B1.3} & 12.52 & 171.41     & 273.39 & 35.53  \\ 
\Remark{vicuna33B1.3} & 12.52 & 171.41     & 258.88 & 34.37  \\ 
\Remark{stablevicuna13B} & 12.52 & 171.41  & 312.03 & 38.61  \\ 
\Remark{ultralm13B} & 12.52 & 171.41       & 290.88 & 36.92  \\ 
\Remark{vicuna7B1.1} & 12.52 & 171.41      & 196.64 & 29.40  \\ 
\bottomrule
\end{tabular}
\end{table}