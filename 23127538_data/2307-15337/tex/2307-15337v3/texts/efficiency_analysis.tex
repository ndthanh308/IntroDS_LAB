\label{app:why_sot_reduce_latency_local}

This section gives a detailed explanation on why \methodshort{} can reduce the overall decoding latency with the same computational resource for local models.





The vanilla approach processes only one question and decodes the answers sequentially, whereas \methodshort{} processes multiple point-expanding requests and the answers in a batch.
We focus on the following question: ``Compared to processing only one sequence, how much peak memory overhead and latency increase will be brought by processing a batch of sequences?''


\begin{table}[tb]
  \centering
    \caption{The latency and average GPU performance of the prefilling and decoding phases when inferencing LLMs. The prefilling token length is 128, the decoding token length is 64, and the batch size is 1. The test is run on one NVIDIA A100 GPU.}
  \label{tab:inf-latency}
  \begin{tabular}{ccc}
    \toprule
     Model   & Prefill/Decode Latency (ms) & Prefill/Decode GPU Perf. (TFLOPS) \\ \midrule
    \Remark{llama}-7B  &  40  / 2735  & 43 / 0.31 \\
    \Remark{llama}-13B &  54  / 3725  & 62 / 0.44 \\
    \Remark{llama}-33B &  100 / 5506  & 85 / 0.75 \\
    \bottomrule
  \end{tabular}
\end{table}

A typical LLM generative process consists of two phases: (1) the prefilling phase in which the prompt is parsed to generate the key-value cache for further use, and (2) the decoding phase in which tokens are generated one by one in a sequential manner. The decoding phase accounts for the majority of the end-to-end latency, especially when generating a long response. As shown in \cref{tab:inf-latency}, when running Vicuna-7B on NVIDIA A100-80G, the actual computing performance is only 0.31 TFLOPS (0.1\% utilization) in the decoding phase, compared to 43 TFLOPS (13.8\% utilization) during prefilling. The utilization is calculated with respect to the FP16\footnote{All of our experiments are run with FP16 inference.} tensor core peak performance -- 312 TFLOPS for NVIDIA-A100. As a result, the latency of decoding only one token is comparable to that of prefilling 128 tokens (40ms).
This huge gap in actual computing performance and thereby the latency arises from the fact that all LLM weights need to be loaded onto the GPU chip at least once only for decoding one token, so the decoding is heavily bottlenecked by the I/O of weights and the GPU computation units cannot be well utilized.

When conducting batched decoding, as the sequence batch size $B$ increases, the latency of decoding one token for each sequence stays roughly the same (\cref{fig:efficiency-batch-vs-single-latency}), as the amount of LLM weights that needs to be loaded onto the chip does not change.
As a result, the GPU computation utilization ($\frac{\text{Actual GPU Performance}}{\text{Peak GPU Performance}}$) increases almost linearly as $B$ increases (\cref{fig:efficiency-batch-vs-single-perf}). In other words, for generating a final answer of length $N$, if we cut the answer into $B$ segments of length $N/B$ and decode them as a batch, we can get a $B \times$ decoding speed-up compared to sequential decoding. Nevertheless, in practice, as prefilling longer requests brings some overhead, and the lengths of the $B$ segments could be imbalanced, the actual speed-up of the batched point-expanding stage compared with the original prefilling and sequential decoding process is smaller than $B$. %


As for the peak memory overhead, the amount of LLM weights can be one to two orders of magnitude larger than that of all the intermediate activations as long as the prefilling token length is not too large, not to mention that most activations do not need to be saved for back-propagation during inference. Therefore, the LLM weights account for the majority of the memory footprint in our test cases. Consequently, as shown in \cref{fig:efficiency-batch-vs-single-mem}, the peak memory overhead due to the increasing size of the KV cache and activation grows at a slow pace as the batch size $B$ increases. Thanks to the small peak memory overhead, in all of our experiments, we managed to use one GPU to run \methodshort{} without seeking help from other peak memory optimization techniques (e.g., quantization~\citep{frantar2022gptq,lin2023awq}, offloading~\citep{sheng2023flexgen}).


% Figure environment removed
