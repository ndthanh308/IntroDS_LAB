\subsection{Prompting Router}
\label{app:prompting_router}
We use \cref{prompt:router} for querying \Remark{gpt4} as the router. If the answer is ``A'' (i.e., the question can be answered in a list of independent points), we will use \methodshort{}. Otherwise, if the answer is ``B'' (i.e., the answer is in a list of points but they depend on each other) or ``C'' (i.e., the answer should \emph{not} be in a list of points), \methodshort{} is not suitable and we will fall back to \methodbase{} decoding. 

\begin{promptenv}{LLM Prompting as the Router}{Question: \promptarg{question}\\\\How would you like to answer the question?\\A. Organize the answer as a list of points or perspectives (in the format of 1., 2., 3., etc.), and the points or perspectives can be answered independently without referring to the contents of the previous points.\\B. Organize the answer as a list of points or perspectives (in the format of 1., 2., 3., etc.), and the contents of later points or perspectives cannot be answered independently without referring to the contents of the previous ones.\\C. Do not organize the answer as a list of points or perspectives.\\\\Just say A, B, or C. Do not explain. Do not provide an answer to the question.}{}
  \label{prompt:router}
\end{promptenv}

\subsection{Trained Router}

We tackle the routing problem as a sequence classification task. We first annotate the LIMA training set~\citep{zhou2023lima}, and then fine-tune a \roberta{} model~\citep{liu2019roberta} using the labeled data. Finally, we apply the tuned \roberta{} as the router on \vicunadataset{} and \wizardlm{}. We detail the steps in the following.


\subsubsection{Annotation Process}
\label{app:annotation_process_router}
In the classification task, a label of 1 (positive) indicates that this question can be answered with \methodshort{}, while a label of 0 (negative) suggests that using the normal generation mode is more suitable. We annotate the LIMA training set, which consists of 1,030 Q\&As sourced from three community webpages: Stack Exchange, wikiHow, and the Pushshift Reddit.
We also annotate the \vicunadataset{} and \wizardlm{} datasets for evaluation.


We use GPT-4 to assist the annotation process. Specifically, we present each question to GPT-4 and analyze its answer to determine whether \methodshort{} can be triggered for this question. We assign a positive label to a question if GPT-4's response meets two criteria: (1) it contains a list of points that can be expanded in parallel, (2) each point provides sufficient details (i.e., the point-expanding response is not too short), which will enable \methodshort{} to achieve a speed-up. Two of the paper's authors conduct the annotation process independently, and discuss the inconsistent annotations to decide the final label.




\subsubsection{Training Details} 
\label{app:training_details_roberta}
We use \texttt{roberta-base} with 120M parameters as the router model. The finetuning is conducted using the AdamW optimizer~\citep{loshchilov2017decoupled} with a weight decay of 0.01. The learning rate undergoes a warm-up phase during the first 1\% of iterations to 5e-5 and then decays linearly. We train the model for 2 epochs using a batch size of 32. Input sequences are either padded or truncated to achieve a consistent length of 512 tokens.

In the application of \methodshort{}, false positives (\methodshort{} is incorrectly triggered when it should not be, resulting in degraded answer quality) are of more significant concern than false negatives (the router misses a potential \methodshort{} trigger, resulting in a reduced speed-up). Thus, to mitigate false positives, we employ the Tversky loss~\citep{wang2023dice} with parameters $\alpha=0.7$ and $\beta=0.3$, which penalizes false positives more heavily than false negatives. We also incorporate label smoothing~\citep{szegedy2016rethinking} with a factor of $\epsilon=0.2$. Overall, the entire fine-tuning process is efficient, completing in 2 minutes on an NVIDIA A100 GPU.

\subsection{Router Consistency}
\label{app:router_consistency}
We present the confusion matrices for the three routers to illustrate their consistency. The results on \vicunadataset{} and \wizardlm{} are shown in \cref{tab:confusion_matrices_vicuna,tab:confusion_matrices_wizardlm}, respectively.

On \vicunadataset{}, we can observe a notable level of agreement among the three routers. Compared with the GPT-4-prompting router, the trained router exhibits a slightly higher number of false negatives w.r.t. the human annotations. Conversely, on \wizardlm{}, given the intricate answer structure and the presence of many ambiguous cases, the routers show significant discrepancies. Specifically, the GPT-4 router produces many false positives, which pose adverse affects on the answer quality (see \cref{app:quality_router}). The \roberta{} router aligns more closely with the human annotations.

\begin{table}[]
  \centering
  \caption{Router confusion matrices on the \vicunadataset{} dataset. \textbf{Left:} Rows are human annotations (H) and columns are the GPT-4 router (G). \textbf{Middle:} Rows are human annotations (H) and columns are the \roberta{} router (R). \textbf{Right:} Rows are the GPT-4 router (G) and columns are the \roberta{} router (R).}
\label{tab:confusion_matrices_vicuna}
\begin{tabular}{c|c|c|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{G0} & \multicolumn{1}{c}{G1} \\
\cline{2-3}
H0 & 38 & 5 \\
\cline{2-3}
H1 & 0 & 37 \\
\cline{2-3}
\end{tabular}
\quad
\begin{tabular}{c|c|c|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{R0} & \multicolumn{1}{c}{R1} \\
\cline{2-3}
H0 & 37 & 6 \\
\cline{2-3}
H1 & 5 & 32 \\
\cline{2-3}
\end{tabular}
\quad
\begin{tabular}{c|c|c|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{R0} & \multicolumn{1}{c}{R1} \\
\cline{2-3}
G0 & 34 & 4 \\
\cline{2-3}
G1 & 8 & 34 \\
\cline{2-3}
\end{tabular}

\caption{Router confusion matrices on the \wizardlm{} dataset. \textbf{Left:} Rows are human annotations (H) and columns are the GPT-4 router (G). \textbf{Middle:} Rows are human annotations (H) and columns are the \roberta{} router (R). \textbf{Right:} Rows are the GPT-4 router (G) and columns are the \roberta{} router (R).}
\label{tab:confusion_matrices_wizardlm}
\begin{tabular}{c|c|c|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{G0} & \multicolumn{1}{c}{G1} \\
\cline{2-3}
H0 & 94 & 66 \\
\cline{2-3}
H1 & 3 & 55 \\
\cline{2-3}
\end{tabular}
\quad
\begin{tabular}{c|c|c|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{R0} & \multicolumn{1}{c}{R1} \\
\cline{2-3}
H0 & 135 & 25 \\
\cline{2-3}
H1 & 31 & 27 \\
\cline{2-3}
\end{tabular}
\quad
\begin{tabular}{c|c|c|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{R0} & \multicolumn{1}{c}{R1} \\
\cline{2-3}
G0 & 93 & 4 \\
\cline{2-3}
G1 & 73 & 48 \\
\cline{2-3}
\end{tabular}
\end{table}

\subsection{Concurrent execution for \methodrshort{}}
In \methodrshort{}, the router serves as an additional stage that extends the two-stage \methodshort{} pipeline, as illustrated in \cref{fig:router_pipeline}. To push the limit of latency optimization, we can run the router, normal generation, and \methodshort{} generation concurrently. Once the router makes a decision, one of the normal and \methodshort{} generation processes can be aborted. However, this approach will increase the token overhead. Therefore, we did not employ this approach in this work and leave it to future work. %




% Figure environment removed