\subsection{\method{}}




\subsubsection{Detailed Statistics of Token Lengths and Point Numbers}


% Figure environment removed




\subsubsection{Latency Breakdown: \methodshort{} Stages and Phases}
\label{app:latency_breakdown_stage_phase}

\cref{fig:latency_breakdown} presents the absolute latencies of \methodbase{} and \methodshort{} generations on \vicunadataset{}. 
Again, the speed-ups of \methodshort{} compared with \methodbase{} generation is evident. We can see that the decoding phases predominantly account for the end-to-end latency. Consequently, although \methodshort{} has higher prefilling latency in the skeleton stage than the \methodbase{} generation and introduces additional point-expanding prefilling latency -- which is expected -- this has negligible impact on the overall latency and thereby the overall speed-up.


% Figure environment removed


\subsubsection{Efficiency Evaluation on NVIDIA RTX 3090}
\label{sec:app-3090}

We present the \methodshort{} speed-ups and latency breakdown on RTX 3090 in \cref{fig:speed-up_3090}. We test the three 7B models, as their FP16-precision version can be run on an RTX 3090 GPU without further peak memory optimization techniques such as weight quantization~\citep{frantar2022gptq,lin2023awq} or offloading~\citep{sheng2023flexgen}. On these three models, \methodshort{} can obtain 1.94$\times$ to 2.40$\times$ speed-up on average on \vicunadataset{}.

For the five question categories that \methodshort{} can provide high-quality answers (i.e., \textit{knowledge}, \textit{common-sense}, \textit{generic}, \textit{roleplay}, \textit{counterfactual}), \methodshort{} can speed-up the overall answer generation process by 1.96$\times$ to 2.52$\times$ in the meantime. Note that for the \textit{math} category, despite the average speed-up being 1.20$\times$ by calculating the speed-up across the three math questions, \methodshort{} does not reduce the absolute latency of processing the three questions.


% Figure environment removed

\subsubsection{Actual Latency Testing}
\label{sec:app-actual-eff-test}

This section reports the actual \methodshort{} speed-up on the \vicunadataset{} with batch testing (instead of analyzing with pre-made profiling tables), using a single NVIDIA A100 GPU. We test the actual end-to-end latency of the \methodshort{} and \methodbase{} decoding with the 9 open-source models. For each model, we run the speed-up test for five times and plot the box in \cref{fig:actual_speed_up}.

As shown in \cref{fig:model_speed_box}, the current \methodshort{} solution obtains a $>2\times$ speed-up on 6 out of the 9 open-source models (i.e., \Remark{vicuna7B1.1}, \Remark{vicuna7B1.3}, \Remark{ultralm13B}, \Remark{llamachat7B2}, \Remark{vicuna13B1.3}, and \Remark{llamachat13B2}), and a $>1.7$ speed-up on \Remark{openchat13B} and \Remark{vicuna33B1.3}. \methodshort{} achieves no speed-up on \Remark{stablevicuna13B}. As shown in \cref{fig:category_speedup_box}, for the five question categories that \methodshort{} can provide high-quality answers (i.e., \textit{knowledge}, \textit{common-sense}, \textit{generic}, \textit{roleplay}, \textit{counterfactual}), \methodshort{} can speed-up the overall answer generation process by 2.15$\times$ to 2.50$\times$ in the meantime.

% Figure environment removed


\subsection{\method{} with Router}
\label{sec:more-effiency-sotr}

The overhead brought by the router inference is relatively small: On the \vicunadataset{} dataset, the prompting and trained router have an average latency of 0.65s (0.39s$\sim$1.37s) and 0.04s (0.008s$\sim$1.55s), respectively. On the WizardLM dataset, the average latency of the prompting and trained router is 0.80s (0.36s$\sim$2.22s) and 0.03s (0.009s$\sim$2.52s), respectively.

\subsubsection{Speed-up breakdown: models}

\cref{fig:bar_sotr_vicuna_speedup_sum_model} shows the speed-ups of \methodrshort{} on different models on the \vicunadataset{} dataset. %
\cref{fig:bar_sotr_wizardlm_speedup_sum_model} and \cref{fig:efficiency_router_wizardlm_model} show the speed-ups of \methodrshort{} on different models on the WizardLM dataset. We can observe that on \vicunadataset{}, the two methods yield similar speed-ups, whereas on \wizardlm{}, GPT-4 prompting router usually obtains higher speed-ups than the trained router, especially on GPT-4 itself.

% Figure environment removed

% Figure environment removed

% Figure environment removed


\subsubsection{Speed-up breakdown: categories}

\cref{fig:bar_sotr_vicuna_speedup_sum_category} and \cref{fig:efficiency_router_vicuna_category} show the speed-ups of \methodrshort{} on different question categories of \vicunadataset{} dataset. The trained router achieves slightly higher speed-up on most of the categories (except for \textit{knowledge}, \textit{writing}, and \textit{fermi}). \cref{fig:bar_sotr_wizardlm_speedup_sum_category} and \cref{fig:efficiency_router_wizardlm_category} show the speed-ups of \methodrshort{} on different question categories of WizardLM dataset. We can observe that on 19 out of 29 categories, using the prompting router achieves higher speed-ups than using the trained router.


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


