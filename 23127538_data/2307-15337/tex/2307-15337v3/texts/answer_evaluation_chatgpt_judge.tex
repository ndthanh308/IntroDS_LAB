In this section, we provide quality evaluation results with \Remark{chatgpt} as the judge in \fastchat{} and \llmzoo{} metrics.  Note that as prior work  (e.g., \citep{alpaca_eval}) shows, \Remark{gpt4}-based evaluation usually aligns with human better than \Remark{chatgpt}. Therefore, readers should refer to the results in the main paper (with \Remark{gpt4} as the judge) for a more accurate view of the performance of \methodshort{}. 
However, the takeaway messages from \Remark{chatgpt} are similar to the ones from \Remark{gpt4}.

\subsubsection{Overall Quality}

In \cref{fig:win_tie_lose_bar_GENERAL}, we show the win/tie/lose rates (the percentage of the cases when \methodshort{} wins/ties/loses compared to \methodbase{} generation) across all models and questions using the two metrics from \fastchat{} and \llmzoo{} that capture the general quality of the answers. We notice a discrepancy between the two metrics on when \methodshort{} is strictly better than the baseline (50.2\% v.s. 12.4\%). Despite that, the two metrics agree that \methodshort{} is not worse than the baseline in more than 76\% of the cases. For \fastchat{} metric, we also show the rates excluding math and coding questions that \methodshort{} is not suitable for (see \cref{sec:eval_algo_category}); \methodshort{} is not worse than the baseline in more than 89\% of the cases. \emph{This result suggests that the answers of \methodshort{} maintain good quality.}

% Figure environment removed

\subsubsection{Quality Breakdown: Question Categories}
Next, we investigate how \methodshort{} performs on different question categories.
We compute \emph{net win rates} (win rates minus lose rates) across all question categories in \cref{fig:net_win_rates_category}. Similar to \cref{fig:win_tie_lose_bar_GENERAL}, we see that \llmzoo{} tends to be more optimistic about the quality of \methodshort{} than \fastchat{}. 
Nevertheless, the conclusions are consistent: \methodshort{} performs relatively \emph{well} on generic, common-sense, knowledge, roleplay, and counterfactual. \methodshort{} performs relatively \emph{badly} on writing, fermi, math, and coding. %

% Figure environment removed

\subsubsection{Quality Breakdown: Models}
Next, we investigate how \methodshort{} performs on different models. We compute net win rates across all models in \cref{fig:net_win_rates_model}. Again, we see that the two general metrics from \fastchat{} and \llmzoo{} have different absolute values but similar rankings. In particular, both metrics agree that \Remark{openchat13B}, \Remark{vicuna7B1.1}, \Remark{claude}, \Remark{chatgpt} have \emph{low} net win rates, whereas \Remark{vicuna13B1.3}, \Remark{stablevicuna13B}, and \Remark{ultralm13B} have \emph{high} net win rates. 

% Figure environment removed

\subsubsection{Quality Breakdown: Question Categories and Models}
In the main text, we analyze how question categories and models affect \methodshort{}'s answer quality. Here, we show the per-model and per-category results.
For each model and question category, we compute the net win rates.
The results are in \cref{fig:model_category}. %

% Figure environment removed

\subsubsection{Quality Breakdown: Metrics}
All previous evaluations use metrics about the general quality of the answer.
In \cref{fig:win_tie_lose_bar_DETAILS}, we show more detailed metrics from \llmzoo{} to reveal in which aspects \methodshort{} can improve or hurt the answer quality. On average, we can see that \methodshort{} improves the diversity and relevance while hurting the immersion and coherence.

% Figure environment removed


