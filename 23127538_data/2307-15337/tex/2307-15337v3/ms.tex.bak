\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}
\iclrfinaltrue
\arxivtrue

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\input{macro}

\usepackage{booktabs}       % professional-quality tables
\usepackage{enumerate}
% \usepackage{hyperref}
\usepackage{url}
\usepackage{xspace}

% ---- for appendix toc ----
\usepackage{minitoc}
\renewcommand \thepart{}
\renewcommand \partname{}


\usepackage{xcolor}         % colors
\newcommand{\says}[3]{{\color{#3}#1:\emph{#2}\color{black}}\xspace}
\newcommand{\todo}[1]{\says{TODO}{#1}{orange}}
\newcommand{\toupdate}[1]{{\color{red}\emph{#1}\color{black}}\xspace}


\title{\method{}: Large Language Models Can Do Parallel Decoding}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xuefei Ning$^1$\thanks{Equal contribution.}\\
  \texttt{foxdoraame@gmail.com} \\
  \And
  Zinan Lin$^{2*}$ \\
  \texttt{linzinan1995@gmail.com} \\
  \And
  Zixuan Zhou$^{1*}$ \\
  \texttt{zhouzx21@mails.tsinghua.edu.cn} \\
  \And
  Zifu Wang$^{3}$ \\
  \texttt{zifu.wang@kuleuven.be} \\
  \And
  Huazhong Yang$^{1}$ \\
  \texttt{yanghz@tsinghua.edu.cn} \\
  \And
  Yu Wang$^{1}$ \\
  \texttt{yu-wang@tsinghua.edu.cn} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs

\maketitle

\begin{center}
  \vspace{-22pt}
  {$^{1}$ Department of Electronic Engineering, Tsinghua University, Beijing, China\\$^{2}$ Microsoft Research, Redmond, Washington, USA\\$^{3}$ ESAT-PSI, KU Leuven, Leuven, Belgium}%
  \\~\\
  Website: \url{https://sites.google.com/view/sot-llm}\\
  Code: \url{https://github.com/imagination-research/sot}
%\\  {\small }
\end{center}

\begin{abstract}
  This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the 
  thinking and writing process of humans, we propose \emph{\method{} (\methodshort{})}, which first guides  LLMs to generate the \emph{skeleton} of the answer, and then conducts parallel API calls or batched decoding to 
  complete the contents of each skeleton point \emph{in parallel}.
  Not only does \methodshort{} provide considerable speed-ups across 12 LLMs,
  but it can also potentially improve the answer quality on several question categories. % in terms of diversity and relevance. 
  \methodshort{} is an initial attempt at data-centric optimization 
  for inference efficiency, and further underscores the potential of pushing LLMs to think more like a human for answer quality.
\end{abstract}

\blfootnote{$^\dagger$ The main updates in arXiv V2 are as follows: (1) Add the quality and efficiency evaluation of \methodshort{} on \Remark{gpt4}. (2) Use \Remark{gpt4} as the judge for answer quality evaluation. The old results with \Remark{chatgpt} as the judge are moved to \cref{app:quality_gpt3.5}. (3) Add the \emph{\methodshort{} with Router (\methodrshort{})} method (\cref{sec:router}) which adaptively triggers \methodshort{} on suitable questions. (4) Move detailed answer analysis to the appendices.}

\section{Introduction}

% In recent years, large language models (LLMs) have shown exceptional performance in natural language processing and revolutionized the chatbot systems. In particular, a series of pretrained LLMs, such as GPT, T5, OPT, GLM, and LLaMA, have shown astonishing performances in a varitey of tasks, and their instruction or conversation fine-tuned verion are widely used in chatbot systems.


% A typical LLM generative process consists of two stages: 1) the prefill stage in which the prompt is parsed to generate the key-value cache for further use, and 2) the decoding stage in which tokens are generated one by one in an incremental manner. The decoding stage accounts for the majority of the end-to-end latency, especially when generating a long response. As shown in \cref{}, due to the low arithmetic intensity, the decoding stage is highly I/O bounded and thus cannot fully utilize the GPUs' computation power. Improving the GPU utilization is essential for accelerating the decoding stage, and existing decoding acceleration methods can be classifed into ...

% In this study, we explore the off-the-shelf potential of parallel decoding in large language models. Specifically, we wonder why cannot we decode multiple segments in the meantime. As shown in \cref{}, .. Then, how do we know which segments can be parallely decoded? Instead of relying on some position-based heuristics, it might be the most appropriate choice to let language model itself tell us. Therefore, we design a prompt template to guide the LLM to give out its ``Skeleton of Thought'' (SoT), which tell us how to parallel decode.

% To draw a parallel between the human thinking and the SoT method, when we take test, answer a question, we usually organize our answer in advance. For example, when discussing about a ..., after deciding the answer skeleton, we'll leverage our language capability to expand on each point of the skeleton by adding evidences, details, and linguistic styles and techniques. ... Results in more comprehensive and organized answer ...

% To summarize, we explore a Skeleton-of-Thought way to boost the LLMs' decoding efficiency by prompt engineering and batch inference. Unlike previous algorithm co-design for efficiency, this attempt can be seem as a content co-guidance for efficency, which might become relevant in the era of general language models. As shown in Fig.~, SoT can achieve xxx speed-up ... Besides improving the decoding efficiency, SoT can also improve the diversity and comprehensiveness of the LLM's answer in several types of scenarios, which can be owe to the resemblance of SoT to the organized writing and thinking process of human. Though using self-organized decoding seems promising for both efficiency concern and algorithmic performance, there are many challenges that cannot be well handled by our method or even current models. In \cref{}, we'll discuss the current limitations, near-term and long-term outlooks to call for future exploration in this direction.


% --- story v2 ---
% LLMs are amazing. But slow. Troublesome for end users or companies. E.g., it takes 2 min for GPT4 API to answer XXX.  

% We ask: without changing the model itself, can we accelerate the inference time? E.g., can we answer the same question XXX in 30 seconds with the same existing GPT4 API? [乍一听不太可能，勾起读者兴趣] This is in contrast (and orthogonal) to existing solutions that require model changes (e.g., quantization, pruning, multi-step decoding). 

% Reason for slow speed: sequential decoding. Chain-of-thought emphasizes the importance of it, and people take it as granted. However, we question if it is a universal rule. Think about humans: our thought is not linear; for many types of questions, we derive the skeleton first and then fill in the details for each point. 

% Motivated by this insight, we propose "Skeleton-of-Thought" [figure, use XXX as an example to contrast sequential decoding v.s. ours]: ask LLMs to derive an outline, and complete each point in parallel. Suitable for both open-source models (parallel decoding) and closed-source models (parallel API calls). Not only gives speed up (e.g., ...), and can potentially improve the answer quality (e.g., ...).

% SoT opens up more questions than it answered. E.g., graph-of-thought, content-oriented optimization (v.s. xxx), ....

% However, due to the autoregressive decoding approach, the generation process of the state-of-the-art LLMs is slow, especially when generating a long sequence. For example,

% --- expand ---
Large language models (LLMs)~\citep{brown2020gpt3,touvron2023llama,du2022glm,openai2023gp4,zheng2023judging} have shown exceptional performance in natural language processing and %revolutionized
chatbot systems.
However, the inference process of the state-of-the-art LLMs is slow, hindering their interactive use.
For example, it takes 22 seconds for Claude \citep{claude} (accessed through Slack API) and 43 seconds for \Remark{vicuna33B1.3} (a 33B LLaMA-based model, running locally on one NVIDIA A100 GPU) to answer the question in \cref{fig:sequential-vs-sot}.
%Besides, as shown in \cref{tab:inf-latency}, 
% And when running LLaMA-based models locally, the end-to-end latency of generating a long answer consisting of \toupdate{...} tokens can be as high as \toupdate{1 minute}.



We conclude three major causes of LLMs' slow inference: (1) A \emph{large model size} requires a large amount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT-3 take 350GB memory, which means at least 5$\times$80GB A100 GPUs are needed to keep the model in GPU memory. Even with enough GPUs, the heavy memory access and computation slow down the inference. (2) %The increasing sequence length also leads to substantial efficiency issue, as
The \emph{attention operation} in the prevailing transformer architecture is I/O bounded and has a quadratic memory and computation complexity in sequence length. (3) The \emph{sequential decoding} approach in inference generates tokens one by one. %, where each token depends on previously generated tokens. 
This approach introduces a significant inference latency since the generation of tokens cannot be parallelized. 
There is a bunch of literature addressing the first two axes: \emph{large model size}~\citep{xiao2022smoothquant,frantar2022gptq,lin2023awq,sheng2023flexgen,wang2021spatten} and \emph{attention operation}~\citep{kitaev2020reformer,wang2020linformer,dao2022flashattention,zaheer2020big,chen2023dynamic}. These works either compress/redesign the model~\citep{xiao2022smoothquant,frantar2022gptq,lin2023awq,kitaev2020reformer,wang2020linformer,dao2022flashattention,zaheer2020big} or redesign the serving system~\citep{sheng2023flexgen,chen2023dynamic} and hardware~\citep{wang2021spatten}.


\begin{figure}[tb]
  \centering

    \begin{subfigure}[b]{.5\textwidth}
       \centering
       \includegraphics[width=\linewidth]{figs/method/sot_v04.pdf}
     \end{subfigure}
     ~~~
     \begin{subfigure}[b]{.45\textwidth}
       	\centering
        \includegraphics[width=\linewidth]{figs/method/General_FastChat.pdf}
     \end{subfigure}
  \caption{\textbf{Left:} An illustration of \method{} (\methodshort{}). Instead of producing answers sequentially, \methodshort{} produces different parts of answers \emph{in parallel}.
  In more detail, given the question, \methodshort{} first prompts the LLM to give out the skeleton, then conducts batched decoding or parallel API calls to expand multiple points in parallel, and finally aggregates the outputs to get the final answer. \textbf{Right:} The net win rates and speed-ups of \methodshort{} with router (\methodrshort{}) compared to \methodbase{} generation on \vicunadataset{}. The net win rate is the difference between the fraction of questions that \methodrshort{} has better and worse answers than \methodbase{} generation. The speed-up is the ratio between the latency of \methodbase{} and \methodrshort{} generation. $(1.0, 0.0)$ represents \methodbase{} generation. Higher is better on both axes. For most models, \methodrshort{} not only accelerates the generation but also improves the quality of the answers (evaluated with \fastchat{} metric \citep{zheng2023judging}). See \cref{sec:eval_algo,sec:router} for more details.} %\todo{state the speed-up ratio and the overall length here xxx}}
    %\todo{Illustrative figure: Put one of the question, GPT-4's answer, SoT's process and final answer in the figure. Maybe the first question in vicuna-80? Or the python-javascript difference question}
  \label{fig:sequential-vs-sot}
\end{figure}

In contrast to prior work, we tackle the third axis and question the common assumption that LLMs have to do fully sequential decoding. We show the feasibility of \textbf{parallel decoding of off-the-shelf LLMs \emph{without} any changes to their model, system, or hardware}. For instance, for the question in \cref{fig:sequential-vs-sot}, we can reduce the latency from 22 seconds to 12 seconds (1.83$\times$ speed-up) with Claude, % model over Slack, 
and from 43 seconds to 16 seconds (2.69$\times$ speed-up) with Vicuna-33B V1.3 on an NVIDIA A100.


%\textbf{Without changing the model, system, or hardware, can we improve the inference latency?} 
% As shown in \cref{tab:inf-latency}, the generation process accounts for the majority of the end-to-end inference latency on modern GPUs. Focusing on the ``sequential decoding approach'' cause of inefficiency, we question if the generation process needs to be fully sequential. 

The idea stems from reflecting on how humans ourselves answer questions. 
Humans do \emph{not} always think about questions and write answers in a sequential fashion. In contrast, for many question types, we first derive the \emph{skeleton} according to some protocols and strategies, and then add evidence and details to refine and explicate each point. This is especially the case on formal occasions like offering consultancy, taking tests, writing papers, and so on. 
Can we make LLMs think in the same way?
To this end, we propose \emph{\method{} (\methodshort{})}. Specifically, as shown in \cref{fig:sequential-vs-sot}, we guide the LLM to derive a skeleton first by itself. Based on the skeleton, the LLMs can complete each point \emph{in parallel} so that we get a speed-up. \methodshort{} can be utilized to accelerate both open-source models with batched decoding and API-based models with parallel API calls.

To make the overall solution more practical, we also design an extension, \methodshort{} with router (\methodrshort{}), which employs a router to only trigger \methodshort{} for suitable questions.

We test \methodshort{} on 12 recently released LLMs. Not only does \methodshort{} provide considerable speed-ups (up to 2.39$\times$), but it can also improve the answer quality in many cases (\cref{fig:sequential-vs-sot}).
%\toupdate{several question categories in terms of diversity and relevance (\cref{fig:quality_vs_speedup})}.

Note that in contrast to existing model- and system-level efforts for inference efficiency, \methodshort{} takes a novel ``data-level'' pathway %to achieve speed-ups 
by letting the LLM organize its output content. This novel perspective is becoming feasible and is expected to grow in relevance, owing to the evolving capabilities of state-of-the-art LLMs. We hope this work can stimulate more research in the realm of data-centric optimization~\citep{zha2023data,dcai2023} for efficiency.
% As LLMs are getting more and more powerful,





%Moreover, aligning with the recent research trend~\cite{}, \methodshort{} further underscores the potential to draw inspiration from human thought processes. 


%Existing efforts for the inference efficiency of neural networks can be generally classified into model-level ones and system-level ones. In contrast, \methodshort{} takes a different path to achieve speed-ups by letting the LLM to organize its output content. This ``data-level'' technique becomes viable thanks to the emerging instruction-following and planning ability of SoTA LLMs. It is exciting that, being the first attempt at data-centric optimization~\citep{zha2023data,dcai2023} for efficiency and drawing the idea from the human thinking process, \methodshort{} opens up more research and engineering questions to be further explored than it has answered. We discuss these questions in \cref{sec:limit-and-outlook}.



% For example, when encountering the question in \cref{fig:sequential-vs-sot}, a human will usually organize the thinking or writing structure %to discuss the topic 
%from multiple aspects such as \todo{} before filling in the details for each aspect. 

% \todo{make a highlighting result table/figure here, let's do it finally when we finished the whole paper? @zinan}

%\begin{figure*}[ht]
%  \begin{center}
%    \subfloat[Y-axis: net win rates of \methodshort{} on diversity \citep{llm-zoo-2023}.]{
%      \includegraphics[width=0.45\linewidth]{figs/method/Diversity_LLMZoo.pdf}
%      \label{fig:diversity_vs_speedup}
%    }
%    ~
%    \subfloat[Y-axis: net win rates of \methodshort{} on relevance \citep{llm-zoo-2023}.]{
%      \includegraphics[width=0.45\linewidth]{figs/method/Relevance_LLMZoo.pdf}
%      \label{fig:relevance_vs_speedup}
%    }
%    \caption{\methodshort{}'s net win rates and speed-up compared to \methodbase{} generation across all questions in \vicunadataset{}. The net win rate is defined as the difference between the fraction of questions that \methodshort{} has better and worse answers than \methodbase{} generation (see \cref{sec:eval_algo}). The speed-up is defined as the ratio between the latency of \methodbase{} generation and \methodshort{}. $(1.0, 0.0)$ represents \methodbase{} generation. Higher is better on both axes. For most models, \methodshort{} not only accelerates the generation but also improves the quality of the answers.}
%    \label{fig:quality_vs_speedup}
%  \end{center}
%\end{figure*}

% \methodshort{} opens up more questions than it answered, for example:
% \begin{enumerate}[I]
% \item In general, the thinking process of humans is more complicated than \methodshort{}; it contains many steps structured in a complicated graph. Generalizing \methodshort{}, should we and can we organize the LLMs' thinking and writing process as ``Graph-of-Thought''? This could potentially combine the benefits of \method{} on efficiency and global understanding with the benefits of Chain-of-Thoughts (CoT)~\citep{kojima2022large,wei2022chain} on logical reasoning and impromptu thinking.

% \item The machine learning field is moving from the model-centric era to the data-centric era. For example, it is acknowledged that designing the data pipeline to guide learning becomes even more important than designing the model structure for improving the \emph{model quality}~\citep{zha2023data,dcai2023}.
% However, in the area of improving \emph{model efficiency}, prior work is still in the model-centric paradigm (e.g., model compression or model design for efficiency~\citep{han2015deep,krishnamoorthi2018quantizing,cai2019once}).
% \methodshort{} is a first attempt on \emph{data-centric optimization for efficiency}, in that \methodshort{} \textit{guides LLM to organize its outputted contents} during inference for improving the \textit{efficiency}. We are optimistic about the opportunities and potential of this direction.

% We are curious will data-centric optimization begin to play a more important role, not only for the quality, but also for efficiency?
%\end{enumerate}

The rest of the paper is organized as follows. We first introduce \methodshort{} in \cref{sec:method} and show its results in \cref{sec:exp}. %To make the overall solution more practical, in \cref{sec:router}, we design an extension, \methodshort{} with router (\methodrshort{}), which employs a router to only trigger \methodshort{} for suitable questions. %when encountering questions suitable for \methodshort{}. 
Then, we expand on the \methodrshort{} extension in \cref{sec:router}.
\cref{sec:literature} positions \methodshort{} in the research ecosystem (expanded in \cref{sec:literature_complete}). Finally, we analyze the limitations and share outlooks of \methodshort{} in \cref{sec:limit-and-outlook}.




%\todo{unify generation and decode? algorithm performance?}

%\todo{outline stage ? expanding  stage? skeleton stage? let's unify}

%\todo{update the citation here. find some paper on data-centric AI}
% Chain-of-thought emphasizes the importance of it, and people take it as granted. However, we question if it is a universal rule. Think about humans: our thought is not linear; for many types of questions, we derive the skeleton first and then fill in the details for each point.

% we explore the off-the-shelf potential of parallel decoding in large language models. Specifically, we wonder why cannot we decode multiple segments in the meantime. As shown in \cref{}, .. Then, how do we know which segments can be parallely decoded? Instead of relying on some position-based heuristics, it might be the most appropriate choice to let language model itself tell us. Therefore, we design a prompt template to guide the LLM to give out its ``Skeleton of Thought'' (SoT), which tell us how to parallel decode.

%To summarize, we explore a \method{} way to boost the LLMs' decoding efficiency by prompt engineering and batch inference. Unlike previous algorithm co-design for efficiency, this attempt can be seem as a content co-guidance for efficency, which might become relevant in the era of general language models. As shown in Fig.~, SoT can achieve xxx speed-up ... Besides improving the decoding efficiency, SoT can also improve the diversity and comprehensiveness of the LLM's answer in several types of scenarios, which can be owe to the resemblance of SoT to the organized writing and thinking process of human. Though using self-organized decoding seems promising for both efficiency concern and algorithmic performance, there are many challenges that cannot be well handled by our method or even current models. In \cref{}, we'll discuss the current limitations, near-term and long-term outlooks to call for future exploration in this direction.


%\todo{first describe local serving, and then discuss can also be applied to API?}

%\todo{I think ``chain-of-thought emphasize the importance of sequential generation'' is not so obvious to all audiences. let's discuss. maybe we mention chain-of-thought later, especially when discussing the math/coding problem}


%This is in contrast (and orthogonal) to existing solutions that require model changes (e.g., quantization, pruning, multi-step decoding). 


% \section{Background}
% \label{sec:background}


\section{\method{} (\methodshort{})}
\label{sec:method}

\subsection{Method}

\myparatightestn{Overview.}
Based on the intuition that humans usually think about and answer a question in an organized way, the core idea of this work is to guide the LLM itself to give a skeleton first and then write the overall answer parallelly instead of sequentially.
\cref{fig:sequential-vs-sot} illustrates how \methodshort{} produces the final answer to a user \concept{question} $q$. 

\myparaemphtightestn{(1) Skeleton stage.}
\methodshort{} first assembles a \concept{skeleton request}, $T^s(\mbox{question}=q)$, using the \concept{skeleton prompt template} $T^s$ (\cref{prompt:ts}, and \cref{prompt:ts_full} in \cref{app:implementation_details_prompt}) with the question $q$ as the parameter. 
The skeleton prompt template is written to guide the LLM to output a concise skeleton of the answer. Then, we extract the $B$ points from the \concept{skeleton response} $R^s$ of the LLM.

\myparaemphtightestn{(2) Point-expanding stage.} Based on the skeleton, we let the LLM expand on each point in parallel. Specifically, for the point with index $b$ and skeleton $R^s_b$, \methodshort{} uses $T^{pe}(\mbox{question}=q, \mbox{skeleton}=R^s, \mbox{point index}=b, \mbox{point skeleton}=R_{b}^s)$ as the \concept{point-expanding request} for the LLM, where $T^{pe}$ is the \concept{point-expanding prompt template} (\cref{prompt:tp}). Finally, after completing all points, we concatenate the point-expanding responses $\{R^{pe}_b\}_{b=1,\cdots,B}$ to get the \concept{final answer}. 

\begin{promptenv}{Skeleton Prompt Template $T^s$}{You're an organizer responsible for only giving the skeleton (not the full content) for answering the question. Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full sentence, each skeleton point should be very short with only 3$\sim$5 words. Generally, the skeleton should have 3$\sim$10 points. Now, please provide the skeleton for the following question.\\\promptarg{question}\\Skeleton:}{1.}
  \label{prompt:ts}
\end{promptenv}

\begin{promptenv}{Point-Expanding Prompt Template $T^{pe}$}{You're responsible for continuing the writing of one and only one point in the overall answer to the following question.\\\\\promptarg{question}\\\\The skeleton of the answer is\\\\\promptarg{skeleton}\\\\Continue and only continue the writing of point \promptarg{point index}. Write it **very shortly** in 1$\sim$2 sentence and do not continue with other points!}{\promptarg{point index}. \promptarg{point skeleton}}
  \label{prompt:tp}
\end{promptenv}

% \cref{prompt:ts,prompt:tp} show the \concept{skeleton prompt template} $T^s$ and \concept{point-expanding prompt template} $T^{pe}$ used by our current implementation. 




\myparatightestn{Parallel point expanding.}
We conduct \emph{parallel} point-expanding so that \methodshort{} is able to achieve a speed-up than \methodbase{} decoding. 
%It is the parallel point-expanding stage during which \methodshort{} achieves speed-ups than \methodbase{} decoding. 

\myparaemphtightestn{(1) For proprietary models with only API access}, we can issue multiple parallel API calls to get an end-to-end latency gain at the cost of an increased number of API requests and tokens. % and computational resources. 

\myparaemphtightestn{(2) For open-source models that we can run locally}, we let them process the point-expanding requests as a batch (paddings are added to the left of the point-expanding requests). We explain below why this could achieve speed-ups.
A typical LLM generative process consists of two phases: (a) the \emph{prefilling} phase in which the prompt is parsed to generate the key-value cache for further use, and (b) the \emph{decoding} phase in which tokens are generated one by one in a sequential manner.
The decoding phase accounts for the majority of the end-to-end latency, especially when generating a long response. Note that the decoding phase is bottlenecked by weight loading instead of activation loading or computation.\footnote{This is true when the number of concurrent queries is small; see \cref{sec:limit-and-outlook} for discussion on other scenarios.}
Consequently, running LLM inference with increased batch sizes does not increase the per-token latency much.
Therefore, \methodshort{} 
%Based on this fact, \textbf{for open-source models that we can run locally}, we let them process the point-expanding requests as a batch (paddings are added to the left of the point-expanding requests).
%This 
allows us to decode roughly $B\times$ more tokens within the same amount of time if we parallelly decode $B$ points. 
See \cref{app:why_sot_reduce_latency_local} for the expanded discussions and the supporting experiments.
%For a detailed explanation of why \methodshort{} reduces the latency with the same computational resource by improving the hardware utilization, \cref{app:why_sot_reduce_latency_local} can be referred to.

Please refer to \cref{app:implementation_details} for more implementation details of \methodshort{}. 

% \todo{Need we simply mention how do we expand to multi-turn conversation here: We just paste the original query and the final aggregated answer in the conversation history.}


% \section{Experimental Results}
\section{\methodshort{} Evaluation}
\label{sec:exp}
\vspace{-0.1cm}

\myparatightestn{Datasets.} We evaluate \methodshort{} on two recent assistant-style datasets: (1) \vicunadataset{}~\citep{vicuna2023}, which contains 80 questions spanning nine categories, such as \textit{coding}, \textit{math}, \textit{writing}, \textit{roleplay}, and so on, and (2) \wizardlm{}~\citep{xu2023wizardlm}, which contains 218 questions spanning more categories and diverse difficulties. Due to space constraints, we only report \vicunadataset{} results in the main paper, and defer \wizardlm{} results to the \cref{app:efficiency,app:quality}. %\todo{@lzn see what else need to be noted here}. %This dataset has been used to evaluate the helpfulness of recent chat models~\citep{vicuna2023,xu2023wizardlm}. \todo{need more} % such as Vicuna \citep{vicuna2023},  and \todo{}.



\myparatightestn{Models.} We test \methodshort{} on 12 recently released models, including 9 open-source models and 3 API-based models (\cref{tab:model}). We obtain the weights of all the open-source models from Hugging Face. See \cref{app:model_details} for more details. 
%\todo{@lzn Let's give the detailed api endpoint and huggingface url? should we put it in table2 or in another table in the appendix... including the added two sentences maybe. Do we have other information to provide? maybe we can add an appendix section for them}


%\todo{@nxf The numbers in \cref{sec:router_exp} will be used to update Figure2 in the introduction. And the current Figure2 can be put in apendix and referred to here. Let's add a discussion here to give the overall takeaway message, and then go into the details}

\subsection{Evaluation of Efficiency}
\label{sec:eval_eff}

%\subsubsection{Method}

% \todo{How we evaluate efficiency: which gpu, models; evaluation method for latency, utilization, and memory}

% \paragraph{Settings} We evaluate the effiency of SoT on Vicuna family (i.e., Vicuna-7b/13b/33b) with two types of GPUs (i.e., A100 and RTX3090).

\myparatightestn{API-based models.} We record the latency of every API call with \verb/start = time.time(); ...; elapsed_time = time.time() - start/, and add the latency of the skeleton API call and the slowest point-expanding API call as the \methodshort{} latency.

\myparatightestn{Open-source models.} All open-source models we currently evaluate are based on the \Remark{llama} 7B, 13B, or 33B architectures. Thus, to enable fast analysis, we first make a latency profiling table for each \Remark{llama} architecture on NVIDIA A100. The table contains the architecture's (1) latency for prefilling sequences of length 1 to 700 with different batch sizes (from 1 to 16), and (2) decoding one token with a context of length 1 to 1024 with different batch sizes (from 1 to 16). With these three latency profiling tables, given the number of points $B$, the token lengths of the requests and responses in the skeleton and point-expanding stages, we can quickly estimate the \methodshort{} latency by simply looking up entries in the tables and adding them up. See \cref{sec:app-profiling-estimate} for a more detailed description of how we conduct the profiling and estimate the latency. % with pre-made profiling tables.



In addition to the above approach, 
we also compare the actual latency of \methodshort{} and \methodbase{} sequential generation (abbreviated as ``\methodbase{}'' in the following discussion)  in \cref{sec:app-actual-eff-test}.

The rest of this section shows the speed-ups of \methodshort{} on different models (\cref{sec:eval_eff_model}) and question categories (\cref{sec:eval_eff_category}). In addition, we also report the latency breakdown of \methodshort{} stages in \cref{app:latency_breakdown_stage_phase} and the \methodshort{} speed-ups on an RTX 3090 GPU in \cref{sec:app-3090}.

%pre-profiling and latency estimation, 
%For the three 7B open-source models (i.e., \Remark{llamachat7B2}, \Remark{vicuna7B1.1}, and \Remark{vicuna7B1.3}), we also conduct experiments on NVIDIA RTX 3090. See \cref{sec:app-3090} for the results.

%\todo{@zzx, 1. update figure to add GPT-4, 2. update discussion a little}

\subsubsection{Speed-up Breakdown: Models}
\label{sec:eval_eff_model}

We investigate how \methodshort{} reduces the end-to-end latency on different models. \cref{fig:speed-up_model_average} shows the average speed-up for each model across all question categories. We can see that  \methodshort{} obtains a $>$2$\times$ speed-up (up to 2.39$\times$) on 8 out of 12 models. %(i.e., \Remark{llamachat7B2}, \Remark{llamachat13B2}, \Remark{vicuna7B1.1}, \Remark{openchat13B}, \Remark{vicuna33B1.3}, \Remark{ultralm13B}, \Remark{gpt4}),
%and a $>1.8\times$ speed-up on \Remark{chatgpt}, \Remark{vicuna13B1.3}, \Remark{vicuna7B1.3}.
%\methodshort{} achieves almost no speed-up when using \Remark{stablevicuna13B}. % If we exclude the \textit{math} and \textit{code} question categories at which the current \methodshort{} version is intrinsically unsuitable (see \cref{sec:eval_algo_category}), the speed-ups are a bit higher, as shown in \cref{fig:speed-up_model_nomathcode_average}.


We report the detailed statistics about token lengths and numbers of points in \cref{fig:statistics}.
(1) In terms of \emph{the point number $B$} (\cref{fig:outline_num_points_average}), \Remark{llama2}, \Remark{vicuna7B1.1}, \Remark{vicuna7B1.3}, and \Remark{chatgpt} yield relatively fewer points ($<$6), while \Remark{gpt4} and \Remark{stablevicuna13B} generates the largest number of points on average ($\approx$9).
(2) Regarding \emph{the point-expanding response length}, \cref{fig:naive_response_average,fig:outline_response_2_average,fig:maxlen_div_len_average} show that the API-based models, \Remark{chatgpt}, \Remark{claude}, and \Remark{gpt4}, follow the point-expanding request better and generate shorter point-expanding responses than the open-source models.  %(1) \Remark{stablevicuna13B} tends to produce shorter \methodbase{} answers, and (2)
One can also notice that \Remark{stablevicuna13B}'s longest point-expanding responses for many question categories %(including coding, math, roleplay, conterfactual, and common-sense)
can be as lengthy as the overall \methodbase{} answer, 
since it fails to adhere to the ``Write it **very shortly**'' instruction in the point-expanding request. Consequently, \methodshort{} cannot accelerate \Remark{stablevicuna13B} well.
(3) Regarding \emph{the length balance degree between point responses}, \cref{fig:balance_average}
shows that \Remark{llama2} and the API-based models generate more balanced point-expanding responses.
(4) As for \emph{the overall length of the final aggregated answer} (\cref{fig:totlen_div_len_average}), employing \methodshort{} on most models results in answers that are, on average, 1$\sim$2$\times$ longer than the \methodbase{} answer.
% , whereas using \methodshort{} on \Remark{stablevicuna13B} results in significantly increased final answer length due to its large point numbers and long point-expanding responses.


\begin{figure*}[ht]
  \begin{center}
    \subfloat[Different models.]{
      \includegraphics[width=0.45\linewidth]{figs/efficiency/bar_vicuna_speedup_model.pdf}
      \label{fig:speed-up_model_average}
    }
    ~
    \subfloat[Different categories.]{
      \includegraphics[width=0.45\linewidth]{figs/efficiency/bar_vicuna_speedup_category.pdf}
      \label{fig:speed-up_category_average}
    }
    \caption{Average speed-ups of \methodshort{} on different models and question categories.}
  \end{center}
\end{figure*}


%Using \methodshort{} on \Remark{vicuna33B1.3} and \Remark{openchat13B} achieves very high speed-ups since they both tend to generate long responses when using the naive manner (see \cref{fig:statistics}(d)). \Remark{stablevicuna13B} obtains the lowest speed-up since it tends to generate long point-expanding response when using the SoT method (see \cref{fig:statistics}(c)). Besides, \Remark{chatgpt} and \Remark{claude} tend to generate the responses with balanced length for each points (see \cref{fig:statistics}(a)).

\subsubsection{Speed-up Breakdown: Question Categories}
\label{sec:eval_eff_category}

Here we investigate how \methodshort{} reduces the end-to-end latency for different question categories. \cref{fig:speed-up_category_average} shows the average speed-up for each question category across all models. The question categories for which \methodshort{} can provide high-quality answers are marked in green, and other categories are marked in red (see \cref{sec:eval_algo_category} for the answer quality evaluation). We can see that \methodshort{} can obtain speed-ups for all question categories. For the five question categories that \methodshort{} can provide high-quality answers (i.e., \textit{knowledge}, \textit{generic}, \textit{common-sense}, \textit{roleplay}, \textit{counterfactual}), \methodshort{} can speed up the overall answer generation process by 1.89$\times$ to 2.33$\times$ in the meantime.


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/efficiency/bar_vicuna_speedup_category.pdf}
%     \caption{Speed-up of \methodshort{} on different question categories.}
%     \label{fig:speed-up_category_average}
% \end{figure}


\subsection{Evaluation of Answer Quality}
\label{sec:eval_algo}

In order to compare the answer quality of the \methodbase{} sequential generation (abbreviated as ``\methodbase{}'' in the following discussion) and \methodshort{} generation, we adopt two LLM-based evaluation frameworks: \fastchat{} \citep{zheng2023judging} and \llmzoo{} \citep{llm-zoo-2023}. The evaluation process is to present a question and a pair of answers (from \methodbase{} or \methodshort{} generation) to an LLM judge (\Remark{gpt4} in the main paper; see \cref{app:quality_gpt3.5} for the results evaluated using \Remark{chatgpt}) and ask for its preference. The response can be that \methodshort{}'s answer wins/ties/loses compared to the \methodbase{} answer. 

Here are more details about the evaluation of the answer quality:

\myparaemphtightestn{(1) Detailed metrics.} \fastchat{} evaluation provides one metric for the general quality of the answers. In addition to a general metric, \llmzoo{} provides five detailed metrics on the answers' coherence, diversity, immersion, integrity, and relevance. 

\myparaemphtightestn{(2) Question categories.} \fastchat{} provides two special evaluation prompts for coding and math questions for more accurate evaluation, whereas \llmzoo{} does not. Following the implementation in \llmzoo{}, we exclude math and coding questions in all \llmzoo{} evaluation results.

\myparaemphtightestn{(3) Extentions to avoid evaluation bias.}
To avoid the potential bias from the order of the two answers presented to the LLM judge, we extend \fastchat{} and \llmzoo{} evaluation frameworks by running the evaluation twice with either ordering of the two answers. In either evaluation, a score of 1, 0, and -1 is assigned when \methodshort{} wins, ties, or loses, respectively. The final evaluation is that \methodshort{} wins/ties/loses when the sum of the two scores is positive/zero/negative. For example, if \methodshort{} wins in one evaluation and loses in the other evaluation, the result is ``tie''. If \methodshort{} wins (loses) in one evaluation and ties in the other, the result is ``win'' (``lose'').

\myparaemphtightestn{(4) Net win rates.} We further define net win rates to give a summarized view of the answer quality. Given the number of questions that \methodshort{} wins (\#win) and loses (\#lose), we define \emph{net win rates} as $\nicefrac{\text{\#win}-\text{\#lose}}{\text{total number of questions}}$.
0\% means that \methodshort{} performs competitively to the \methodbase{} baseline (wins and loses in the same number of questions). Higher values mean that \methodshort{} performs better. 

% \myparatightestn{Outline.}
The organization of this section on answer quality evaluation is as follows. We first present the overall quality of \methodshort{} answers (\cref{sec:eval_algo_overall}), and then go into the details across different question categories (\cref{sec:eval_algo_category}), models (\cref{sec:eval_algo_model}), and metrics (\cref{sec:eval_algo_metric}).


\subsubsection{Overall Quality}
\label{sec:eval_algo_overall}

% For \fastchat{} metric, we also show the rates excluding math and coding questions that \methodshort{} is not suitable for (see \cref{sec:eval_algo_category}); \methodshort{} is not worse than the baseline in more than 90\% of the cases.
In \cref{fig:win_tie_lose_bar_GENERAL_gpt4}, we show the win/tie/lose rates (the percentage of the cases when \methodshort{} wins/ties/loses compared to \methodbase{} generation) across all models and questions using the two metrics from \fastchat{} and \llmzoo{} that capture the general quality of the answers. We notice a discrepancy between the two metrics on when \methodshort{} is strictly better than the baseline (45.8\% v.s. 29.5\%). Despite that, the two metrics agree that \methodshort{} is not worse than the baseline in around 60\% of the cases, and the win rates are close to the lose rates.
\emph{This result suggests that the answers of \methodshort{} maintain good quality of that of the normal generation.}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/answer_quality/gpt4/win_tie_lose_bar_GENERAL.pdf}
    \caption{Win/tie/lose rates of \methodshort{} v.s. \methodbase{} generation using ``general'' metrics from \fastchat{} and \llmzoo{}. \methodshort{} performs better than or equal to \methodbase{} generation in around 60\% cases.}
    \label{fig:win_tie_lose_bar_GENERAL_gpt4}
\end{figure}

\subsubsection{Quality Breakdown: Models}
\label{sec:eval_algo_model}
Next, we investigate how \methodshort{} performs on different models. 
We compute net win rates on all models in \cref{fig:net_win_rates_model_gpt4}. Again, we see that the two general metrics from \fastchat{} and \llmzoo{} have different absolute values but similar rankings. In particular, both metrics agree that \Remark{openchat13B}, \Remark{vicuna7B1.1}, \Remark{claude}, \Remark{llamachat13B2} have \emph{low} net win rates, whereas \Remark{vicuna13B1.3}, \Remark{stablevicuna13B}, and \Remark{ultralm13B} have \emph{high} net win rates. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/answer_quality/gpt4/win_tie_lose_bar_GENERAL_General_FastChat_per_model_NETWINRATES.pdf}
         \caption{Metric: general quality (\fastchat{}).}
         \label{fig:net_win_rates_model_fastchat_gpt4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/answer_quality/gpt4/win_tie_lose_bar_GENERAL_General_LLMZoo_per_model_NETWINRATES.pdf}
         \caption{Metric: general quality (\llmzoo{}).}
         \label{fig:net_win_rates__llmzoo_gpt4}
     \end{subfigure}
     \caption{Net win rates of \methodshort{} on different models.}
     \label{fig:net_win_rates_model_gpt4}
\end{figure}

We investigate the answers in \cref{sec:app-eval-quality-model-pattern}, and summarize the key takeaways as follows.
%\myparatightestn{Summary.}
%that are too weak to understand
%In its current form,
%Currently, 
Some models have low \methodshort{} quality
as they cannot understand the skeleton and point-expanding prompts well. 
Some other models have low \methodshort{} quality as their normal answers already have good quality, making it hard for \methodshort{} to beat them (e.g., \Remark{claude}). 
For models that are able to understand the \methodshort{} prompts, the answer quality is improved. 
We expect that further improving SoT prompts or
fine-tuning the models can make it easier for LLMs to understand the skeleton and point-expanding
prompts and ultimately result in better answer quality.
% cannot answer questions in the \methodshort{} way very well %does not work well for some models that



\subsubsection{Quality Breakdown: Question Categories}
\label{sec:eval_algo_category}
Next, we investigate how \methodshort{} performs on different question categories.
We compute \emph{net win rates} (win rates minus lose rates) on all question categories in \cref{fig:net_win_rates_category_gpt4}. Similar to \cref{fig:win_tie_lose_bar_GENERAL_gpt4}, we see that \llmzoo{} tends to be more optimistic about the quality of \methodshort{} than \fastchat{}. 
Nevertheless, the conclusions are consistent: \methodshort{} performs relatively \emph{well} on \textit{generic}, \textit{common-sense}, \textit{knowledge}, \textit{roleplay}, and \textit{counterfactual}. \methodshort{} performs relatively \emph{poorly} on \textit{writing}, \textit{fermi}, \textit{math}, and \textit{coding}. %As explained in \cref{sec:eval_algo}, math and coding are only evaluated in \fastchat{}, and they have the worst net win rates.

\begin{figure}[th]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/answer_quality/gpt4/win_tie_lose_bar_GENERAL_General_FastChat_per_category_NETWINRATES.pdf}
         \caption{Metric: general quality (\fastchat{}).}
         \label{fig:net_win_rates_category_fastchat_gpt4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/answer_quality/gpt4/win_tie_lose_bar_GENERAL_General_LLMZoo_per_category_NETWINRATES.pdf}
         \caption{Metric: general quality (\llmzoo{}).}
         \label{fig:net_win_rates_category_llmzoo_gpt4}
     \end{subfigure}
     \caption{Net win rates of \methodshort{} on different question categories.}
     \label{fig:net_win_rates_category_gpt4}
\end{figure}

% Note that we use the same prompt across all question categories.

% \myparatightestn{Summary.}
%without triggering the point-expanding stage according to the question.
We investigate the answers in \cref{sec:app-eval-quality-category-pattern}, and summarize the key takeaways as follows.
\methodshort{} performs well when the question can be answered in several points whose details can be expanded independently. This includes a wide range of real-world questions.
On the other hand, it is fundamentally challenging to apply \methodshort{} on questions that require step-by-step thinking, in which the latter steps require the details from the earlier steps, such as math questions. 
To make \methodshort{} general across broader question categories, one promising pathway is to enable \methodshort{} to adaptively fall back to normal generation, which we explore in \cref{sec:router}. 
Interestingly, our results suggest that some LLMs are already able to do that occasionally without special prompting or tuning (see \cref{sec:app-eval-quality-category-pattern}).


\subsubsection{Quality Breakdown: Metrics}
\label{sec:eval_algo_metric}
All previous evaluations use metrics about the general quality of the answer.
In \cref{fig:win_tie_lose_bar_DETAILS_gpt4}, we show more detailed metrics from \llmzoo{} to reveal in which aspects \methodshort{} can improve or hurt the answer quality. On average, we can see that \methodshort{} improves the diversity and relevance while hurting the immersion and coherence.

%\begin{wrapfigure}[14]{R}{0.55\linewidth}
\begin{figure}[ht]
    \centering
    \vspace{-0.4cm}
    \includegraphics[width=0.55\linewidth]{figs/answer_quality/gpt4/win_tie_lose_bar_DETAILS.pdf}
    \vspace{-0.4cm}
    \caption{Win/tie/lose rates of \methodshort{} v.s. \methodbase{} generations using metrics from \llmzoo{}. \methodshort{} performs well on diversity and relevance, and relatively worse on coherence and immersion.}
    \label{fig:win_tie_lose_bar_DETAILS_gpt4}
\end{figure}
%\end{wrapfigure}

Through answer investigation (\cref{sec:app-eval-quality-metric-pattern}), we summarize the key takeaways as follows.
% \myparatightestn{Summary.} In summary,
The skeleton stage of \methodshort{} explicitly require LLMs to discuss the answers from multiple aspects without filler words. This improves the diversity and relevance of the answers. %with a loss of coherence and immersion.
As for coherence and immersion, \methodshort{} is not worse than the \methodbase{} generation around 60\% of the time. One future direction is to improve the \methodshort{} prompts or pipeline so that the answers can be better in more metrics.

%\todo{Detailed eval with examples}

%\todo{Do we need to have some discussion here about: SoT can enable generating answer longer than the trained context window, but still with reasonable attention to the overall outline (the large picture)... This depends on whether we can find some examples demonstrating this... somehow a very small point... I vote for not mentioning this currently}




%We further investigate the statistics of the generation results in \cref{fig:statistics}, and summarize the findings below. On writing and knowledge questions, models obtains the highest speed-up, since they tend to generate long responses when using the naive manner (see \cref{fig:statistics}(d)). On math questions, models achieves the lowest speed-up, since they tend to generate very short responses when using the naive manner (see \cref{fig:statistics}(d)). Besides, models tend to generate the outline with less points (see \cref{fig:statistics}(b)), making it hard to accelerate the generation with SoT.


% \todo{speed-up results on Vicuna-80 dataset and analyses}

% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figs/vicuna_speed-up.pdf}
%   \caption{speed-up of our proposed SoT method for Vicuna-7b/13b/33b models on NVIDIA RTX3090 and A100 GPUs on Vicuna-80 benchmark. X-axis: speed-up. Y-axis: the task type of Vicuna-80 dataset.}
%   \label{fig:vicuna_speed-up}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/speed-up_average.pdf}
%     \caption{The speed-up for the 9 models on all types of the tasks from {\vicunadataset}.}
%     \label{fig:speed-up_average_heatmap}
% \end{figure}

% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c|c|c}
%         \toprule
%         Access & Model Name & Average speed-up & Average speed-up \\ & & & w.o. math\&coding \\\midrule
%         \multirow{7}{*}{open-source} & \Remark{openchat13B}~\citep{openllms23} & 2.18 & 2.30 \\ 
%         & \Remark{stablevicuna13B}~\citep{stablevicuna2023}& 1.05 & 1.08\\ 
%         & \Remark{vicuna7B1.1}~\citep{vicuna2023} & 2.20 & 2.32 \\
%         & \Remark{vicuna7B1.3}~\citep{vicuna2023} & 1.81 & 1.91\\
%         & \Remark{vicuna13B1.3}~\citep{vicuna2023} & 1.85 & 1.97\\
%         & \Remark{vicuna33B1.3}~\citep{vicuna2023} & 2.24 & 2.30\\
%         & \Remark{ultralm13B}~\citep{ding2023enhancing} & 2.09 & 2.09\\\midrule
%         \multirow{2}{*}{API-Based} & \Remark{claude}~\citep{claude} & 1.31 & 1.42\\
%         & \Remark{chatgpt} & 1.97 & 2.01\\
%         \bottomrule
%     \end{tabular}
%     \caption{The average speed-up for each models.}
%     \label{tab:model}
% \end{table}


%\toupdate{整体平均加速比、去掉math和code任务的}

% We demonstrate the speed-up evaluation results for the 9 models (listed in \cref{tab:model}) on each type of tasks from {\vicunadataset} on \cref{fig:speed-up_average_heatmap}. As can be seen, our SoT method can obtain speed-up on most of the cases. Specifically, for OpenChat-13B, Vicuna-7B, Vicuna-33B, UltraLM-13B and ChatGPT-3.5, SoT accelerates the inference time on all the tasks. The results show the effectiveness of the proposed SoT method.

% Nevertheless, we also notice that SoT fails on math problems for most of the models. For example, SoT achieves 0.6$\times$, 0.8$\times$ and 0.7$\times$ speed-up for StableVicuna-13B, Vicuna-7B and Clause on math problems, which means that SoT even slows down the model inference. We attribute the failure to the following three factors. \textbf{First}, on math problems, models tend to give out the responses with imbalance length for each points. As shown in \cref{fig:balance_average_heatmap}, the imbalance degree on math problems is very high, which means that the GPU cannot be fully used in the generation process. \textbf{Second}, on math problems, models tend to generate the outline with less points. Ideally, when the total output length is fixed, dividing the response into more points can bring higher speed-up. However, as shown in \cref{fig:bs_average_heatmap}, the average number of points on math problems is only 4.5, which is quite smaller than on other types of problems. \textbf{Third}, on math problems, models tend to generate the long response for each point (as shown in \cref{fig:outl_average_heatmap}), leading to the longer inference time.


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/balance_average.pdf}
%     \caption{The imbalance degree for the 9 models on all types of the tasks from {\vicunadataset}.}
%     \label{fig:balance_average_heatmap}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/outline_num_points_average.pdf}
%     \caption{The number of points for the 9 models on all types of the tasks from {\vicunadataset}.}
%     \label{fig:bs_average_heatmap}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/outline_response_2_average.pdf}
%     \caption{The maximum token length of the point-expanding responses for the 9 models on all types of the tasks from {\vicunadataset}.}
%     \label{fig:outl_average_heatmap}
% \end{figure}


% \paragraph{Vicuna}


% As shown in \cref{fig:speed-up_average_heatmap}, our SoT method can significant accelerate the models in the Vicuna familiy. Specifically, on the NVIDIA A100 GPU , SoT can achieve 2.17, 2.16 and 2.17 for Vicuna-7B, 13B and 33B on Vicuna-80 dataset, respectively. While on the NVIDIA RTX3090 GPU, SoT can achieve 1.88 for Vicuna-7B. Besides, we also observe that, on the most types of tasks(e.g., writing, knowledge, generic), SoT achieves more than 2$\times$ speed-up. But on the math problem, SoT obtains nearly no speed-up for Vicuna-7B and 33B. To explore the reason causes to the above difference, we further inspect the number of the points and the output token length of the SoT generation results, and show them in \cref{fig:vicuna_bs} and \cref{fig:vicuna_len}. As can be seen, the average output token length of the math problem of Vicuna-7B is the longest one, which might cause the low speed-up. Besides, for the math problem of Vicuna-33b, the average number of the output points is the smallest one, which might also cause the negligible acceleration.


% \begin{figure}[htb]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figs/vicuna_bs.pdf}
%   \caption{Number of the points in the generation results of our SoT method for Vicuna-7b/33b models on Vicuna-80 benchmark. X-axis: number of the points. Y-axis: the task type of Vicuna-80 dataset.}
%   \label{fig:vicuna_bs}
% \end{figure}

% \begin{figure}[htb]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figs/vicuna_len.pdf}
%   \caption{Token length of the generated points of our SoT method for Vicuna-7b/33b models on Vicuna-80 benchmark. X-axis: token length. Y-axis: the task type of Vicuna-80 dataset.}
%   \label{fig:vicuna_len}
% \end{figure}

% \paragraph{ChatGPT-3.5 \& Claude} For ChatGPT-3.5 and Claude models, we use their API to generate the responses and recode the calling time. The speed-up evaluation results are shown in \cref{fig:gpt_claude_speed-up}. SoT can achieve 2.01$\times$ and 1.37$\times$ speed-up on average for ChatGPT-3.5 and Claude, respectively.
%For ChatGPT-3.5, our SoT method can achieve 2.01$\times$ speed-up on average. While for Claude, the average speed-up is 1.0, which means that SoT cannot accelerate the this model.

% \begin{figure}[htb]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figs/gpt_claude_speed-up.pdf}
%   \caption{speed-up of our proposed SoT method for ChatGPT-3.5 and Claude models on Vicuna-80 benchmark. X-axis: speed-up. Y-axis: the task type of Vicuna-80 dataset.}
%   \label{fig:gpt_claude_speed-up}
% \end{figure}



% \subsubsection{Profile Results and Analysis}

% % \todo{profile results plot out, analyses}

% %\paragraph{Numer of Points} 

% In this section, we evaluate how the hardware efficiency (e.g., latency, memory and GPU utilization) changes with the number of points in the SoT generation process. 

% Ideally, with the increase of the number of points, the output token length of each points decreases. We set the total output token length as 128, 256 and 512, respectively, and show the efficiency trend with the number of points. As shown in \cref{fig:profile_bs_user} (top), the speed-up increases linearly with the number of points. The underlying reason causes to the increase is the GPU utilization improvement (\cref{fig:profile_bs_user} (down)). Besides, as shown in \cref{fig:profile_bs_user} (middle), the growth rate of the memory cosume also increases linearly with the number of points.

% However, from the actual generation results, we find that the output token length of each points often has less to do with the number of points. Thus, we set the token length of each point-expanding response as 50, 100 and 200, respectively, and show the efficiency trend with the number of points. As shown in \cref{fig:profile_bs}, when the output is short, the speed-up remains constant with the number of points, since.... When the output is long enough, the speed-up slights decrease with the increasing of the number of points, since....

% % \paragraph{Prompt Length}

% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figs/profile_batch_size_user.pdf}
%   \caption{speed-up (top), memory growth rate (middle) and GPU utilization growth rate (down) with the increase of the number of points. We set the total token length of the answer as 128 (first column), 256 (second column) and 512 (third column).}
%   \label{fig:profile_bs_user}
% \end{figure}

% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figs/profile_batch_size.pdf}
%   \caption{speed-up (top), memory growth rate (middle) and GPU utilization growth rate (down) with the increase of the number of points. We set the token length of each point-expanding response as 50 (first column), 100 (second column) and 150 (third column).}
%   \label{fig:profile_bs}
% \end{figure}


\section{\methodshort{} with Router (\methodrshort{}): Adapatively Triggering \methodshort{}}
\label{sec:router}
\vspace{-0.1cm}
% \todo{@lzn, write why we need router}
In \cref{sec:exp}, we see that \methodshort{} provides considerable speed-ups while maintaining (or even improving) answer quality for many question types. However, the biggest limitation is that \methodshort{} is not suitable for questions that require step-by-step reasoning (\cref{sec:eval_algo_category}). 
Towards pushing the practical adoption of \methodshort{}, we explore the possibility of \emph{adaptively triggering \methodshort{}} only when it is suitable. To achieve that, we propose a \emph{router} module that decides if \methodshort{} should be applied for the user request, and then call either \methodshort{} or \methodbase{} decoding accordingly. This paradigm aligns with the recent trends of composing multiple models to solve complicated tasks \citep{Chase_LangChain_2022,shen2023hugginggpt}. To implement the router, we explore two options: LLM prompting as the router (no model training is needed) (\cref{sec:fallback_prompting}), and trained \roberta{} as the router (\cref{sec:trained_roberta}). The evaluation is provided in \cref{sec:router_exp}.

%an example of the vision of hugging gpt, langchain etc.

%two approaches

\subsection{Prompting Router}
\label{sec:fallback_prompting}
% \todo{@lzn, how we do fallback prompting |}

We directly ask an LLM if the question is suitable for \methodshort{}. More specifically, we ask the LLM if the desired answer is in a list of independent points (see \cref{app:prompting_router} for the prompt). If the answer is yes, we will use \methodshort{}; otherwise, we will use \methodbase{} generation (i.e., directly feeding the question to the LLM). We employ \Remark{gpt4} as the LLM router given its strong capability. %The benefit of LLM prompting as the router is that we do not need to do any model training; however, the routing performance is limited by the capability of the routing model.

\subsection{Trained Router}
\label{sec:trained_roberta}
% \todo{@wzf, how we label the data and train the roberta}
% Consequently,

While leveraging GPT-4 as the router obviates the need for model training, its performance remains sensitive to prompt design. %, and its capability is still bounded. 
Therefore, we approach the problem as a sequence classification task by fine-tuning a small language model as the router. Specifically, we annotate the LIMA dataset \citep{zhou2023lima} as the training set to train a \roberta{} model \citep{liu2019roberta}, which has only 120M parameters.
Comprehensive details regarding the annotation and training processes can be found in \cref{app:annotation_process_router,app:training_details_roberta}, respectively.

% Once trained, the router can be seamlessly integrated with other datasets like \vicunadataset{} and \wizardlm{}. 


%\subsection{Efficiency Considerations}

%router latency can be covered

\subsection{\methodrshort{} Evaluation}
\label{sec:router_exp}
We compare \methodshort{} and \methodrshort{} under the same evaluation setup in \cref{sec:exp}. Besides the prompting and trained routers, we also consider a ``human router'' where we manually judge whether \methodshort{} should be applied for each question. This serves as a benchmark for comparison.

\subsubsection{Evaluation of Efficiency}

%\todo{@zzx efficiency: sum (max -> appendix), bar \& comparison}

%We follow the same evaluation setup as \cref{sec:exp}.
 % on different datasets.

\cref{fig:efficiency_router_vicuna} shows the speed-ups of \methodshort{} and \methodrshort{} for different models on the \vicunadataset{} dataset (see \cref{sec:more-effiency-sotr} for more results on the \wizardlm{} dataset). We can see that: (1) As expected, \methodrshort{} obtains lower speed-ups than \methodshort{}, since \methodshort{} is not triggered for some questions and the router induces a small latency overhead. Nevertheless, \methodrshort{} can still benefit most models with $>$1$\times$ speed-ups. (2) %The two types of routers have different latencies on different datasets.
\methodrshort{} with the trained router obtains slightly higher speed-ups for 7 out of 12 models on \vicunadataset{}, while \methodrshort{} with the prompting router obtains higher speed-ups for all models on the WizardLM dataset (see \cref{fig:efficiency_router_wizardlm_model} in \cref{sec:more-effiency-sotr}).


%\methodrshort{} with trained router obtains slightly higher speed-up than \methodrshort{} with prompting router, since our trained router can give out the fallback with less time.

\begin{figure*}[ht]
    \vspace{-0.4cm}
    \begin{minipage}[t]{0.48\linewidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figs/efficiency/bar_vicuna_comparison_model.pdf}
        \vspace{-0.6cm}
      \caption{Speed-ups of \methodshort{} and \methodrshort{} on different models across all question categories of the \vicunadataset{} dataset.}
      \label{fig:efficiency_router_vicuna}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
      \centering
      \includegraphics[width=1.0\linewidth]{figs/answer_quality/gpt4_with_router/vicuna80/win_tie_lose_bar_GENERAL_General_FastChat_per_category_NETWINRATES.pdf}
        \vspace{-0.6cm}
      \caption{Net win rates of \methodshort{} and \methodrshort{} on different question categories of the \vicunadataset{} dataset (evaluated with the \fastchat{} metrics).}
      \label{fig:quality_router_gpt4_fastchat}
    \end{minipage}
\end{figure*}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/efficiency/bar_vicuna_comparison_model.pdf}
%     \caption{Speed-up of \methodshort{} and \methodrshort{} on different models across all question categories of \vicunadataset{} dataset.}
%     \label{fig:efficiency_router_vicuna}
% \end{figure}


\subsubsection{Evaluation of Answer Quality}
%We follow the same evaluation setup as \cref{sec:exp}. 
\cref{fig:quality_router_gpt4_fastchat} shows the net win rates (averaged across all models) of \methodshort{} and \methodrshort{} on \vicunadataset{} with the \fastchat{} metrics (see \cref{app:quality_router} for results of the \wizardlm{} dataset and \llmzoo{} metrics). We can see that: (1) \methodrshort{} significantly improves the answer quality on questions where \methodshort{} is not suitable (e.g., \textit{coding}, \textit{math}, \textit{writing}, \textit{fermi}) by falling back to \methodbase{} decoding. At the same time, \methodrshort{} maintains answer quality improvements on questions where \methodshort{} is good at. (2) The trained router performs similar to (on \vicunadataset{}) or better than (on \wizardlm{}; see \cref{app:quality_router}) the prompting router. This accords with our intuition in \cref{sec:trained_roberta}. (3) The prompting and trained routers could even surpass human router (e.g., on roleplay questions; see more examples on \wizardlm{} in \cref{app:quality_router}).

We discuss the consistency across three routers in \cref{app:router_consistency}. The primary takeaways include: (1) on \vicunadataset{}, there is a notable consistency among all three routers, and (2) on \wizardlm{}, greater discrepancies emerge, with the trained router showing higher alignment with human annotations.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/answer_quality/gpt4_with_router/vicuna80/win_tie_lose_bar_GENERAL_General_FastChat_per_category_NETWINRATES.pdf}
%     \caption{Net win rates of \methodshort{} and \methodrshort{} on different question categories of \vicunadataset{} dataset using the general quality metric from \fastchat{}. Blue dots are from \cref{fig:net_win_rates_model_fastchat_gpt4}. \methodrshort{} correctly falls back to \methodbase{} decoding on questions where \methodshort{} is not suitable.}
%     \label{fig:quality_router_gpt4_fastchat}
% \end{figure}

\vspace{-0.2cm}
% \section{\methodshort{} In the Context of Literature}
\section{Related Work}
\label{sec:literature}
\vspace{-0.2cm}
This section positions \methodshort{} in related work to reveal how \methodshort{} (1) is connected to, (2) is different from, and (3) can harness the power of other methods. See \cref{sec:literature_complete} for the expanded discussion.

\myparatightestn{Efficient LLM methods at model and system levels.}
% Existing efficient LLM techniques can be categorized into model- and system-level ones.
At the model level, prior work proposes efficient architectures, including dynamic mixture-of-experts~\citep{lepikhin2021gshard}, low-complexity attention~\citep{kitaev2020reformer}, and multi-query attention~\citep{shazeer2019fast}. However, they usually require a significant re-training cost. In contrast, compression methods require a smaller amount of fine-tuning cost by reducing the complexity of pre-trained LLMs,  % from certain aspects. 
such as quantization~\citep{frantar2022gptq} and weight or activation sparsification~\citep{semi_first,zaheer2020big}. %, and so on.
% These techniques change the model and can benefit both the latency and throughput at the risk of degrading model quality.

%Going down the technical stack, 
At the system level, prior work (1) optimizes the computational graph~\citep{dao2022flashattention}, (2) optimizes the assignment and scheduling of computational graph on devices~\citep{sheng2023flexgen}, or (3) designs batching or caching mechanisms for serving multiple users~\citep{fang2021turbotransformers}. These techniques address the large memory access and footprint posed by the vast model scale and attention mechanism, and mainly aim at enhancing the throughput rather than the end-to-end latency. As \methodshort{} trades off throughput for end-to-end latency, \textit{\methodshort{} can 
%harness the power of 
make
these throughput-oriented techniques %and make them help with 
help with
end-to-end latency}. This interesting synergy offers opportunities for achieving better trade-offs between latency and throughput in future serving systems.

\textit{In contrast to model- and system-level techniques, SoT is a data-level technique in a new ``content co-organization for efficiency'' paradigm}.
% \textit{In contrast to model- and system-level techniques, SoT is on data-level}.
See \cref{sec:limit-and-outlook} for more discussions. % in a new ``content co-organization for efficiency'' paradigm}.
% Along with the growth in the LLM capabilities and amount of LLM-generated data, data-level techniques could become more important in the future. %an important tool in the efficient LLM toolbox.
% As AI is capable of understanding instructions and is generating longer and longer sequences, data-level techniques might become an important tool in the efficient LLM toolbox.



%Focusing on inference-latency-oriented scenarios, we introduce existing methods that accelerate the decoding phase.

%强调是new data-level efforts rather than model-level and system-level; 且可以利用

\myparatightestn{Efficient LLM methods through parallel generation.}
Some prior work also addresses
%Some research efforts share a similar motivation to ours, namely, focusing on addressing 
the sequential decoding issues. 
Speculative decoding (SD) methods~\citep{stern2018blockwise} employ smaller models %as the assistance 
to generate some consecutive tokens sequentially and apply the target LLMs to verify them parallelly. Non-autoregressive generation (NAG) methods~\citep{gu2018nonautoregressive,xiao2023survey} sample and refine consecutive tokens parallelly, often with the support of a modified and tuned model.

Relying on either assisting models or special models and sampling schemes, SD and NAG methods conduct \textit{parallel verification or sampling and refinement of consecutive tokens}. In contrast, \methodshort{} prompts the LLM \emph{itself} to plan the contents in a way that permits \textit{the parallel generation of tokens in different segments}, by exploiting the emerging instruction-following and planning ability of LLMs. %, instead of specially designed modeling, sampling, and training schemes.

%强调对emerging ability的利用

\myparatightestn{Prompting methods for LLMs.}
Recent years have witnessed the emergence of the ``pre-train, prompt, and predict'' paradigm, %which designs prompts comprising task descriptions and (optionally) a few demonstrations to guide pre-trained LLMs in generating answers for a wide range of downstream tasks. This paradigm 、
which has shown promise in enhancing LLMs' quality in math and commonsense reasoning~\citep{wei2022chain,kojima2022large,wang2022selfconsistency,chen2022program} and planning for multi-modality tasks~\citep{shen2023hugginggpt,zhu2023ghost}. %, and so on.
%While the large prompting literature aims to uncover various capabilities of LLM and improve the answer quality
%on different downstream tasks, 
Instead of focusing on answer quality, \textit{\methodshort{} is a first attempt at exploiting the power of prompting to improve efficiency}.

% 强调第一个让prompting针对efficiency rather than...
\vspace{-0.2cm}
\section{Limitations, Future Work, and Open Questions}
\label{sec:limit-and-outlook}
\vspace{-0.2cm}

\myparatightestn{Answer quality evaluation.} Our answer quality evaluation is far from perfect due to the limited prompt set, the potential bias of GPT-4 judges, and the inherent difficulty of evaluating LLM generations. %Fruther expanding the prompt set and complementing LLM-based evaluation with human evaluation is important. % for a more reliable quality evaluation.
Currently, we did not conduct human evaluation since it is easy for a human to tell whether an answer is generated with \methodshort{} due to its distinctive pattern, which might cause evaluation bias. We leave a more thorough evaluation of answer quality to future work.

\myparatightestn{Eliciting or improving LLMs' ability.}
\cref{sec:eval_algo_metric} demonstrates \methodshort{}'s potential of enhancing answer quality. It is part of a broader trend in recent research, exemplified by work including CoT~\citep{kojima2022large,wei2022chain}, ToT~\citep{yao2023tree}, and ReAct~\citep{yao2022react}, which collectively affirm the notion that \emph{explicitly articulating the thought process in language can elicit high-quality answers from LLMs}.
These findings resemble human thinking: rather than relying solely on the first intuition or purely sequential thinking, 
we often document step-by-step reasoning or thought organization to attain 
high-quality answers. This intriguing parallel prompts us to explore further how we can draw from the human thinking process to facilitate more effective and efficient AI.
%correct solution or produce a 

%forces a fully parallelized decoding of all points, ignoring 
For instance, \methodshort{} currently 
ignores the dependencies between points. A conceptually better way is to organize the points as \emph{Graph-of-Thoughts}, where the edges represent the dependencies, and each point is decoded conditioned on the contents of its ancestor points.
%Then, decoding a certain point can be conditioned on the detailed contents of its ancestor points. 
%Additionally, the structure might also need to be dynamic:
In addition, instead of complying with a \emph{static} graph, we expect the need %to guide AI to self-organize its thoughts as a 
of having
\emph{dynamic Graph-of-Thoughts}, where the high-level thought structure is adjusted dynamically by LLMs themselves. This could potentially combine the efficiency and global thinking advantages of \methodshort{} with the logical reasoning and impromptu thinking strengths of methods like CoT~\citep{kojima2022large,wei2022chain}. Notably, a contemporary work~\citep{besta2023graph} has attempted to design Graph-of-Thoughts to elicit reasoning.

Furthermore, there exist self-improving training pipelines~\citep{zelikman2022star,huang2022large} that use rationales generated by CoT to fine-tune LLMs, thereby enhancing their reasoning abilities. Likewise, it is interesting to investigate how the more structured answers from \methodshort{} can be used to fine-tune LLMs to enhance their ability to generate well-organized and comprehensive answers.


\myparatightestn{Efficiency and overhead of \methodshort{} in different scenarios.}
Serving systems commonly adopt batch processing to handle concurrent queries. %, %gradually shifting the workloads from memory-bounded to computation-bounded. 
This raises a concern of whether \methodshort{} may hurt %effective 
serving throughput due to parallel requests.
%as the additional processing of \methodshort{} prompts brings computational overhead.
(1) When there is an unsaturated number of concurrent queries,
\methodshort{} can effectively 
reduce latency and
enhance GPU utilization. % without requiring additional hardware resources.
Example scenarios include (a) Edge-side applications with a single user; (b) Centralized services during periods with unsaturated user requests and underutilized computing capacity.
%In these scenarios, \methodshort{} can effectively reduce latency without requiring additional hardware resources.
%Consequently, despite introducing additional FLOPs, \methodshort{} effectively reduces latency without requiring additional hardware resources. These scenarios extend beyond edge-side services serving single users to encompass server-side services as well: During periods with unsaturated user requests, where hardware computing capacity remains underutilized, \methodshort{} can be triggered to optimize end-to-end latencies. 
It is %an 
interesting %engineering consideration 
to study the appropriate \methodshort{} triggering conditions based on system workloads.
(2) When there is a saturated number of concurrent queries, % , when using \methodshort{} in large-batch-size serving scenarios 
% (maybe to harness \methodshort{}'s ability to improve answer quality), 
\methodshort{} is still useful for improving answer quality.
However, in this case, it is important to consider the computation overhead from \methodshort{}. %, ensuring that it does not deteriorate the system's throughput. 
We delve into this concern in \cref{sec:app-token-overhead}.

%When \methodshort{} can only access models via APIs, 
For API-based models, a notable concern arises regarding the increased number of prefilling tokens (\cref{sec:app-token-overhead}). Given that many APIs charge token usage, \methodshort{} may lead to higher costs. To address this, one can tune the number of parallel API requests (by expanding multiple points in a single API call), or use prompt tuning to design shorter \methodshort{} prompts \citep{jiang-etal-2023-llmlingua} (see \cref{sec:app-token-overhead}). % to balance the cost and the latency according to application requirements and budgets. %, API pricing policies, and budget constraints.

\myparatightestn{Data-centric efficiency optimization.}
While data-centric engineering for improving answer \emph{quality}~\citep{zha2023data,dcai2023} is gaining popularity,
%While significant efforts are shifting towards data-centric engineering to improve model \emph{quality}~\citep{zha2023data,dcai2023}, 
its potential for \emph{inference efficiency} is not explored yet. \methodshort{} is the first attempt. % at \emph{data-centric optimization for efficiency}.
As LLM capabilities and the amount of LLM-generated data are growing rapidly, data-centric techniques could become more useful in the future.
We look forward to more explorations to unlock the full potential of data-centric efficiency optimization. % towards a new layer to the existing %model-software-hardware co-design 
%efficiency
%toolkit~\citep{guo2017software}.


% \methodshort{}, together with notable recent studies, such as
% are consolidating the phenomena that ``explicitly giving out thought process as language helps elicit high-quality answer from LLMs''.
%Actually, we humans are very familiar to these phenomena: Instead of solely relying on the first intuition or fully sequential thought process, we need to write down the step-by-step reasoning and thought organization on a draft to reach a correct solution or finish a high-quality answer. This interesting resemblance inspires us to borrow more from the human thinking process to facilitate better and faster AI models.



%Consider how we think and write: In addition to systematic thinking and writing according to well-established protocols, we can come up with new ideas, think of new information, and realize new connections during the process. 
%Therefore,

%have built self-improving training pipeline of LLMs, which use the rationales outputted by CoT to tune the LLM to improve its intrinsic reasoning ability. Analogically, it is also an interesting direction to use the more structured answers generated by \methodshort{} to tune the LLM itself to improve its ability of organizing better answers.

%Not only does the skeleton structure might need to be generalized from independent points to a dependency graph of points, but
%This impromptu process can be vital for logical reasoning and creative thinking.





% the utilization of small-batch-size LLM inference is hampered by the overhead of loading model weights. In such cases,

% proves valuable in enhancing GPU utilization.



% The serving system will batch multiple queries together when serving multiple concurrent queries. In scenarios with a small number of concurrent queries, the small-batch-size LLM inference is bottlenecked by weight loading. In these cases, \methodshort{} can boost the GPU utilization, such that even if \methodshort{} introduces additional FLOPs, it can reduce the latency with the same hardware resource. The related real-world scenarios include not only edge-side services that serve a single user but also server-side services. Since there are periods of fewer user requests in which the hardware computation capability cannot be well utilized. 
% In these periods, \methodshort{} can be triggered to optimize the end-to-end latencies for end users. Determining when to trigger SoT based on the current system workload is a worthwhile engineering consideration.

% When one wants to leverage \methodshort{}'s capability for enhanced answer quality in large-batch-size serving scenarios, we should consider and minimize the computation overhead introduced by SoT to not deteriorate the system throughput, which we discuss in \cref{sec:app-token-overhead}.

% If \methodshort{} can only access models through APIs, a notable overhead is that \methodshort{} results in a much larger number of prefilling tokens (\cref{sec:app-token-overhead}). As many APIs are priced according to the token usage, \methodshort{} will cause a high cost for the end user. One could adjust the number of parallel API requests (e.g., expand multiple points in an API request) to trade off the price and latency according to the application needs, API pricing policy, and price budget.



%\todo{@nxf change this section}

%\todo{@nxf modify the ``open questions'' part in the introduction to be more compact, and expand here}

%\paragraph{Limitations.}
%Our current evaluation of the answer quality is far from perfect due to the limited prompt set, the bias of current LLM judges, and the inherent difficulty of evaluating LLM generations. Expanding the prompt set and complementing LLM-based evaluation with human evaluation is important for a more reliable quality evaluation. Nevertheless, the current manuscript mainly focuses on revealing the potential efficiency benefits, i.e., we can achieve considerable speed-up by rethinking the necessity of ``fully sequential decoding'' of current LLMs. Therefore, we leave a more thorough evaluation of answer quality to future work.
%\todo{@lzn should we put this here as a limitation of our current evaluation method, or put it at 3.2. I'm not sure} \zinan{putting it here is fine}



% \paragraph{Limitations.} First, \methodshort{} cannot answer math questions well. This is reasonable since the current \methodshort{} solution is somehow contradictory with CoT: CoT relies on the expanded details of each step to continue the following reasoning, while \methodshort{} hopes to strategically list out the skeleton in advance. Therefore, intuitively, \methodshort{} is more suitable for the questions with a clear answer structure that can be planned in advance.
% Second, \methodshort{} is not good at code questions. We observe that all models tend to provide a bunch of coding strategies and comments, instead of giving out the code directly, which is usually preferred by users. \todo{let's check the concrete result again and modify the discussion here}
% Third, there are some redundant ..\todo{@lzn do u think we should make this a limitation? It seems a concrete thing to improve, but not fundamental limitation.}

% \todo{Discussion about randomness, the controllability of budget}

% \todo{for narrative types, maybe we should mention the linguistic cohesion and coherent problem between points. @lzn let's analyze some points}
% 对于写故事类型的... 前后段的衔接问题要不要提一下. 拿几个代表性问题分析一下

%Potentials of \methodshort{} on algorithm performance

%Potentials of \methodshort{} on efficiency
%still has a number of limitations. 
%summarize the limitations,

%While we think \methodshort{} is conceptually enticing and reasonable and have experimentally demonstrated its effectiveness in improving efficiency, efforts are still needed to improve the current \methodshort{} solution. 
%Below we suggest some directions for future work and raise open questions.

% \paragraph{Future work.} 
% (1) Currently, \methodshort{} cannot answer math questions well. This is reasonable since the current \methodshort{} solution is somehow contradictory with CoT: CoT relies on the expanded details of each step to continue the following reasoning, while \methodshort{} hopes to strategically list out the skeleton in advance. 
% More broadly, \methodshort{} is more suitable for questions with a clear answer structure that can be planned in advance.
% In order to use \methodshort{} appropriately, we should design a pipeline to only trigger the \methodshort{} mode for question types and descriptions that are suitable for SoT. 
% Suppose the user requires a very short answer (e.g., by formulating a multi-choice question or explicitly adding words like ``please give a succinct and short answer'' in the prompt) or raises a question that mainly needs impromptu thinking or reasoning instead of strategically structuring (e.g., equation solving). In that case, we should choose other generation modes instead of \methodshort{}.

%Therefore, intuitively, 
% , i.e., have clear structure and prefer comprehensiveness. 
%For example, 


% In order to improve the ability of handling different types of questions, one can consider giving out structural information in the skeleton, so as to give more concrete instruction to the expanding stage. Intuitively, the strategy of answering different types of questions can be different: When answering the question ``What are the main differences between Python and JavaScript programming languages'', we can give out a skeleton containing topics such as ``Syntax'', ``Execution environment'', ``Performance'', ``Typing'', and so on. In contrast, when answering the question ``How would you introduce yourself as a medieval knight at a royal banquet'', each point of the skeleton can contain the narrative strategy and the answer prefix to make the aggregated answer fluent (improve the linguistic cohesion).
% \todo{maybe discuss a better example here?}


%(2) In order to improve the \methodshort{} capability of LLMs,
%we can fine-tune them to better understand the skeleton and expanding instructions, and produce more fluent and natural answers for a wider range of question categories. For example, fine-tuning could help the model learn to adaptively choose the \methodshort{} mode or fall back to 1-stage sequential decoding.

%In order to elicit the capability of LLMs on in-adavance skeleton planning and expanding, %we can leverage their in-context learning capabiltiy by providing demonstrations of giving short and informative skeleton and expanding precisely. Moreover, 
% on in-adavance skeleton planning and expanding,
 %Fortunately,
%Specifically, we can leverage the LLM itself to prepare the supervision data, for example, by (1) expanding the training prompt set~\citep{wang2022self}, (2) sequentially decoding the content, and (3) summarizing and splitting the decoded content into several points.

% might improve the generalizability of \methodshort{} on different types of questions.
%Specifically, if each skeleton point is a JSON string like \verb|{`topic': `treasure value', `strategy': `first tell the treasure value, then explain why the value is high, finally call for engagement', `partial_answer': `The great treasure values [to be continue]'}| give more concrete instruction. 

%\todo{Let the LLM itself decide whether to hierarchical decode (toolformer)}

%实用性 
%1. according to user's instruction and question type, automatically trigger the \methodshort{} mode for question types that need long and detailed answer (maybe feasible by just prompting too). For example, explicit require to answer shortly, or multi-choice type questions...

%算法效果
%2. Better organization of contents, give out structural information in outline (e.g., topic, strategy, partial answer), and respect them when expanding

%(3) Currently, we only test \methodshort{} with the standard HuggingFace Transformer library. Further integrating \methodshort{} with existing throughput-oriented optimization techniques~\citep{sheng2023flexgen}, inference engines~\citep{fastertransformer,dao2022flashattention,zhai2022bytetransformer}, and serving systems~\citep{fang2021turbotransformers,triton} is an interesting future direction to reveal the full potential of \methodshort{} in enhancing both the throughput and latency.



% \paragraph{Open questions.}
% (1) The current \methodshort{} solution forces a fully parallelized decoding of all points, ignoring the possible sequential dependencies between points. A conceptually better way is to organize the points as a ``Graph-of-Thoughts'', where the edges represent sequential dependencies. Then, the decoding of a certain point can be conditioned on the detailed contents of its ancestor points. Not only does the skeleton structure might need to be generalized from independent points to a dependency graph of points, but the structure might also need to be dynamic. Consider how we think and write: In addition to systematic thinking and writing according to well-established protocols, we can come up with new ideas, think of new information, and realize new connections during the process. This impromptu process can be vital for logical reasoning and creative thinking. Therefore, instead of complying with a static content skeleton, we prospect the need to guide AI to self-organize its thoughts as a ``dynamic Graph-of-Thought'', in which the thought structure can be dynamically adjusted. This could potentially combine the benefits of \method{} on efficiency and global thinking with the benefits of CoT~\citep{kojima2022large,wei2022chain} on logical reasoning or impromptu thinking.

% (2) Although many efforts to improve model \emph{quality} have been shifted from model-centric engineering to data-centric engineering, the field has not seriously explored the potential of data-centric engineering for \emph{efficiency}. \methodshort{} is an initial attempt toward this direction. We are looking forward to more explorations to reveal the full potential of data-centric optimization for efficiency, adding a new data level to the existing technique stack of ``model-software-hardware co-design for effciency''~\citep{guo2017software}.


\section*{Acknowledgements}
We thank Sergey Yekhanin (Microsoft Research), and Tianji Wu (Infinigence AI) for their support and suggestions on the work. We thank Tianyu Fu for many initial discussions on the idea. We thank Ke Hong and Genghan Zhang for their discussions about profiling. We thank Yue Wu for the help on the \Remark{claude} scripts. We thank Da Yu, Chulin Xie, and Saiqian Zhang for their suggestions on revising the first version of the paper. We thank Rui Hu, Cheng Cheng, Jack Jin, Zhoutong Ye, Mingze Sun, Jun Yan, Zhi Zhang, Yuxuan Tong, Nianhui Guo, and Andrea Santilli for their suggestions on revising the second version of the paper.

\bibliographystyle{iclr2024_conference}
\bibliography{top,iclr2024_conference}

\clearpage
\appendix
\part{Appendix}
\parttoc
\clearpage
% \renewcommand{\thefigure}{App. \arabic{figure}}
% \setcounter{figure}{0}
% \renewcommand{\thetable}{App. \arabic{table}}
% \setcounter{table}{0}
% \renewcommand{\thepromptenv}{App. \arabic{promptenv}}
% \setcounter{promptenv}{0}

\section{Model Details}
\label{app:model_details}
\input{texts/model_details}

\section{Implementation Details of \method{}}
\label{app:implementation_details}

\subsection{Prompt}
\label{app:implementation_details_prompt}


The skeleton prompt is shown in \cref{prompt:ts,prompt:ts_full} and the point-expanding prompt is shown in \cref{prompt:tp}.

\begin{promptenv}{Skeleton Prompt Template $T^s$ (with Two-Shot Demonstrations)}{You're an organizer responsible for only giving the skeleton (not the full content) for answering the question. Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full sentence, each skeleton point should be very short with only 3$\sim$5 words. Generally, the skeleton should have 3$\sim$10 points.\\\\Question:\\What are the typical types of Chinese dishes?\\Skeleton:\\1. Dumplings.\\2. Noodles.\\3. Dim Sum.\\4. Hot Pot.\\5. Wonton.\\6. Ma Po Tofu.\\7. Char Siu.\\8. Fried Rice.\\\\Question:\\What are some practical tips for individuals to reduce their carbon emissions?\\Skeleton:\\1. Energy conservation.\\2. Efficient transportation. \\3. Home energy efficiency. \\4. Reduce water consumption. \\5. Sustainable diet. \\6. Sustainable travel.\\\\Now, please provide the skeleton for the following question.\\\promptarg{question}\\Skeleton:}{1.}
  \label{prompt:ts_full}
\end{promptenv}

% All the open-source models natively support partial answers; however, they implement it in slightly different ways. In more detail, the concrete texts for ``\textbf{[User:]}'' and ``\textbf{[Assistant:]}'', which specify who the words are from, are different. %designated by the model-specific conversation template.

\myparatightestn{Skeleton prompt template.} 
In order to make the output skeleton short and in a consistent format for the good of efficiency and ease of point extraction, the skeleton prompt template (1) describes the task precisely, %(2) uses two demonstrations, 
and (2) provides a partial answer ``1.'' for the LLM to continue writing. 
The skeleton responses are in the desired format in most cases. Therefore, we can use a simple regular expression \verb/(\d+)\.\s?([\s\S]+?)(?=\n|\n*$)/ to extract point indexes and point skeletons from the skeleton response.

% \paragraph{Few-shot demonstrations.}
We find that \Remark{gpt4} can work well without the two demonstrations in the skeleton prompt. Therefore, we do not include the two demonstrations for \Remark{gpt4} (\cref{prompt:ts}). For all other models, the two demonstrations are included, as shown in \cref{prompt:ts_full}.

\myparatightestn{Point-expanding prompt template.}
It describes the point-expanding task and provides a partial answer. We also provide instructions ``Write it **very shortly** in 1$\sim$2 sentence'' so that the LLMs keep the answers concise. Unlike the skeleton prompt template, we find that demonstrations are not necessary to get reasonable results.

We find that \Remark{claude} and \Remark{gpt4} follows the instruction ``Write it **very shortly** in 1$\sim$2 sentence and do not continue with other points!'' in \cref{prompt:tp} very well, so that the answers are very short. Therefore, we delete ``**very shortly**'' from the prompt template in \Remark{claude} and \Remark{gpt4}. %This and the partial answer prompts discussed above are the only two prompt template customizations we did across all models and all evaluations.

\paragraph{Partial answer.} In the \cref{prompt:ts,prompt:tp}, we provide partial answers so that LLMs can follow the desired response format better.

We can put the partial answer at the end of the prompt for the open-source models to continue writing. An implementation detail is that different open-source models have different conversation templates (i.e., different ways to combine user and assistant messages into one string). 
For example, \Remark{vicuna}~\citep{vicuna2023} uses the string ``USER:'' and `` ASSISTANT:'' for the placeholder ``\textbf{[User:]}'' and ``\textbf{[Role]}'' in the \cref{prompt:ts,prompt:tp}, respectively, while \Remark{ultralm}~\citep{ding2023enhancing} uses ``User:'' and ``\textlangle/s\textrangle Assistant:''. We build our open-source model experiments with the help of the FastChat codebase~\citep{zheng2023judging}, in which the conversation templates of many models are already  handled correctly. We implement the conversation templates of \Remark{openchat13B}, \Remark{stablevicuna13B}, and \Remark{ultralm13B} according to their official guides and codes.

For \Remark{chatgpt}, we provide partial answers as a last message in the chat history from the assistant. Note that it is not a documented approach. We find it works well in most cases, in that \Remark{chatgpt} continues the texts from the provided partial answer. However, in some rare cases, \Remark{chatgpt} repeats the provided partial answers.

For \Remark{claude} over Slack, there is no obvious way to give the API a partial answer. We resort to modifying the prompt template slightly by adding \begin{center}\textit{Please start your answer from ``\promptarg{partial answer}'' and do not output other things before that} \end{center} at the end. We find that \Remark{claude} understands and obeys it well. For \Remark{gpt4}, we also take this approach.

\paragraph{System Message.}
We do not include the system message in the prompts for open-source models except \Remark{llama2}.

The partial answer, ``**very shortly**'', and the 2-shot demonstrations discussed above are the only differences between the prompts we used across all models and all evaluations.

%\subsection{Point-Expanding Prompt}
%\label{app:implementation_details_point_expanding}

%The point-expanding prompt template is shown in \cref{prompt:tp}.


%\paragraph{Few-shot demonstrations.} We find that \Remark{gpt4} can work well without the two demonstrations in the skeleton prompt. Therefore, we do not include the two demonstrations for \Remark{gpt4}. For all other models, the two demonstrations are included, as shown in \cref{prompt:ts_full}.




%\paragraph{Point-expanding prompt.} 

%\subsection{Extracting Points from the Skeleton}
%We use the regular expression \verb/(\d+)\.\s?([\s\S]+?)(?=\n|\n*$)/ to extract the point indexes and the point skeletons from the skeleton response.

\subsection{Supporting Multi-Round Conversation}
To use \methodshort{} in a multi-round conversation, we can just put the question and the final aggregated answer in the history, removing all the \methodshort{} prompts. In this way, using \methodshort{} in one conversation round will not introduce additional prefill cost in future rounds.


\section{Implementation Details of \method{} with Router}
\label{app:implementation_details_router}
\input{texts/app_router_implementation}


% \section{\methodshort{} In the Context of Literature (Expanded)}
\section{Related Work (Expanded)}
\label{sec:literature_complete}
\input{texts/literature}

\section{Efficiency Analysis}
\input{texts/efficiency_analysis}

\section{Efficiency Profiling}
\input{texts/efficiency_profiling}

% \section{Efficiency Evaluation on \vicunadataset{}}
\section{Efficiency Evaluation}
\label{app:efficiency}
\input{texts/efficiency_evaluation}

%\section{Efficiency Evaluation on \wizardlm{}}
%\input{texts/efficiency_evaluation_wizardlm}

\section{Overhead of \methodshort{} in Different Scenarios}
\label{sec:app-token-overhead}
\input{texts/sot_overhead}

\section{Answer Quality Evaluation}
\label{app:quality}

\subsection{\method{}}
\input{texts/answer_analysis_model}
\input{texts/answer_analysis_category}
\input{texts/answer_analysis_metric}


\subsection{\method{} with Router}
\label{app:quality_router}

\cref{fig:quality_router_gpt4_llmzoo} shows net win rates of \methodshort{} on \vicunadataset{} dataset with \llmzoo{} metrics, and \cref{fig:quality_router_gpt4_fastchat_wizardlm} shows net win rates of \methodshort{} on \wizardlm{} dataset with \fastchat{} metrics.
The key takeaways are:
(1) In both cases, \methodrshort{} achieves similar or better quality than \methodshort{}, and the net win rates of \methodrshort{} are usually non-negative. This indicates that \methodrshort{} falls back to \methodbase{} decoding on the right question categories. 
(2) On the \wizardlm{} dataset, we see that the trained router has better performance than the prompting router in most cases. This is reasonable, as the prompting router is limited by the capability of \Remark{gpt4}, whereas the trained router is dedicated to this task.
(3) Sometimes, our routers can even achieve better performance than humans. 

%We can see that: (1) \methodrshort{} significantly improves the net win rates on questions where \methodshort{} is not suitable (e.g., coding, math, writing, fermi) by falling back to \methodbase{} decoding. At the same time, \methodrshort{} maintains improvements of answer quality on questions where \methodshort{} is good at. (2) The two routers achieve similar performance. (3) Sometimes, our routers can even achieve better performance than humans (e.g., on roleplay questions; see more examples on \wizardlm{} dataset in \cref{app:quality_router}).

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/answer_quality/gpt4_with_router/vicuna80/win_tie_lose_bar_GENERAL_General_LLMZoo_per_category_NETWINRATES.pdf}
    \caption{Net win rates of \methodshort{} and \methodrshort{} on different question categories of \vicunadataset{} dataset using the general quality metric from \llmzoo{}. Blue dots are from \cref{fig:net_win_rates_category_llmzoo_gpt4}. \methodrshort{} correctly falls back to \methodbase{} decoding on questions where \methodshort{} is not suitable.}
    \label{fig:quality_router_gpt4_llmzoo}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/answer_quality/gpt4_with_router/wizardlm/win_tie_lose_bar_GENERAL_General_FastChat_per_category_NETWINRATES.pdf}
    \caption{Net win rates of \methodshort{} and \methodrshort{} on different question categories of \wizardlm{} dataset using the general quality metric from \fastchat{}. \methodrshort{} correctly falls back to \methodbase{} decoding on questions where \methodshort{} is not suitable.}
    \label{fig:quality_router_gpt4_fastchat_wizardlm}
\end{figure}


\subsection{\Remark{chatgpt} as the Judge}
\label{app:quality_gpt3.5}
\input{texts/answer_evaluation_chatgpt_judge}

% \section{Some Notes on the Application Scenarios}
% 1. some platform scenario: Some edge GPU generate slow.
% 2. some user preference: sometimes, user do not need to fully read the contents, sometimes we just need to check a skeleton before we think this answer does not match my desire, and i should rephrase my question or emphasize what i'm caring about
% 3. new application scenario: agent-agent interaction

\end{document}

