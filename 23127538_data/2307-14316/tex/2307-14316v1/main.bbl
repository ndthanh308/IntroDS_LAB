\newcommand{\arXiv}[1]{\href{https://arxiv.org/abs/#1}{\emph{arXiv:#1}}}
\begin{thebibliography}{10}

\bibitem{DBLP:conf/icml/AbelHL16}
David Abel, D.~Ellis Hershkowitz, and Michael~L. Littman, `Near optimal
  behavior via approximate state abstraction', in {\em {ICML}}, pp. 2915--2923,
  (2016).

\bibitem{achiam2017cpo}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel, `{Constrained Policy
  Optimization}', in {\em {ICML}}, pp. 22--31, (2017).

\bibitem{Alshiekh2018}
Mohammed Alshiekh, Roderick Bloem, R{\"{u}}diger Ehlers, Bettina
  K{\"{o}}nighofer, Scott Niekum, and Ufuk Topcu, `{Safe Reinforcement Learning
  via Shielding}', in {\em {AAAI}}, pp. 2669--2678, (2018).

\bibitem{altman1999cmdp}
Eitan Altman, {\em {Constrained Markov decision processes}}, volume~7, CRC
  Press, 1999.

\bibitem{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston,
  `Curriculum learning', in {\em {ICML}}, pp. 41--48, (2009).

\bibitem{bertsekas2014constrained}
Dimitri~P Bertsekas, {\em {Constrained Optimization and Lagrange Multiplier
  Methods}}, volume~1, Academic press, 1982.

\bibitem{Borkar2005}
Vivek~S Borkar, `{An actor-critic algorithm for constrained Markov decision
  processes}', {\em Systems \& control letters}, {\bf 54}(3),  207--213,
  (2005).

\bibitem{shield-pomdp}
Steven Carr, Nils Jansen, Sebastian Junges, and Ufuk Topcu, `Safe reinforcement
  learning via shielding under partial observability', in {\em {AAAI}}, pp.
  14748--14756, (2023).

\bibitem{chow2017risk}
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone,
  `Risk-constrained reinforcement learning with percentile risk criteria', {\em
  JMLR}, {\bf 18}(1),  6070--6120, (2017).

\bibitem{di2022recommender}
Tommaso Di~Noia, Nava Tintarev, Panagiota Fatourou, and Markus Schedl,
  `Recommender systems under european ai regulations', {\em Communications of
  the ACM}, {\bf 65}(4),  69--73, (2022).

\bibitem{Dulac-Arnold2021}
Gabriel Dulac-Arnold, Nir Levine, Daniel~J. Mankowitz, Jerry Li, Cosmin
  Paduraru, Sven Gowal, and Todd Hester, `{Challenges of real-world
  reinforcement learning: definitions, benchmarks and analysis}', {\em Mach.
  Learn.}, {\bf 110}(9),  2419--2468, (2021).

\bibitem{eysenbachdiversity}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine, `Diversity
  is all you need: Learning skills without a reward function', in {\em {ICLR}},
  (2019).

\bibitem{eysenbach2021maximum}
Benjamin Eysenbach and Sergey Levine, `Maximum entropy {RL} (provably) solves
  some robust {RL} problems', in {\em ICLR}, (2022).

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine, `{Model-Agnostic Meta-Learning
  for Fast Adaptation of Deep Networks}', in {\em {ICML}}, pp. 1126--1135,
  (2017).

\bibitem{Garcia2015survey}
Javier Garc{{\'i}}a and Fernando Fern{{\'a}}ndez, `{A Comprehensive Survey on
  Safe Reinforcement Learning}', {\em {JMLR}}, {\bf 16}(1),  1437--1480,
  (2015).

\bibitem{Ghasemi2021}
Mahsa Ghasemi, Evan Scope~Crafts, Bo~Zhao, and Ufuk Topcu, `{Multiple Plans are
  Better than One: Diverse Stochastic Planning}', in {\em {ICAPS}}, pp.
  140--148, (2021).

\bibitem{gimelfarb2021risk}
Michael Gimelfarb, Andre Barreto, Scott Sanner, and Chi-Guhn Lee, `Risk-aware
  transfer in reinforcement learning using successor features', in {\em
  {NeurIPS}}, pp. 17298--17310, (2021).

\bibitem{Grbic2020}
Djordje Grbic and Sebastian Risi, `{Safe Reinforcement Learning through
  Meta-learned Instincts}', in {\em {ALIFE}}, pp. 183--291, (2020).

\bibitem{ha2020learning}
Sehoon Ha, Peng Xu, Zhenyu Tan, Sergey Levine, and Jie Tan, `{Learning to Walk
  in the Real World with Minimal Human Effort}', in {\em CoRL}, pp. 1110--1120,
  (2020).

\bibitem{Haarnoja2017}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine, `{Reinforcement
  Learning with Deep Energy-Based Policies}', in {\em {ICML}}, pp. 1352--1361,
  (2017).

\bibitem{haarnoja2018soft1}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine, `{Soft
  Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a
  Stochastic Actor}', in {\em {ICML}}, pp. 1861--1870, (2018).

\bibitem{haarnoja2018soft2}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,
  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey
  Levine.
\newblock {Soft Actor-Critic Algorithms and Applications}, 2018.
\newblock \arXiv{1812.05905}.

\bibitem{hazan2019provably}
Elad Hazan, Sham Kakade, Karan Singh, and Abby Van~Soest, `{Provably Efficient
  Maximum Entropy Exploration}', in {\em {ICML}}, pp. 2681--2691, (2019).

\bibitem{Igl2021}
Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon
  Whiteson, `{Transient Non-stationarity and Generalisation in Deep
  Reinforcement Learning}', in {\em {ICLR}}, (2021).

\bibitem{islam2019marginalized}
Riashat Islam, Zafarali Ahmed, and Doina Precup.
\newblock {Marginalized State Distribution Entropy Regularization in Policy
  Optimization}, 2019.
\newblock \arXiv{1912.05128}.

\bibitem{Jansen2020}
Nils Jansen, Bettina K{\"{o}}nighofer, Sebastian Junges, Alex Serban, and
  Roderick Bloem, `{Safe Reinforcement Learning Using Probabilistic Shields
  (Invited Paper)}', in {\em {CONCUR}}, pp. 1--16, (2020).

\bibitem{karimpanal2020learning}
Thommen~George Karimpanal, Santu Rana, Sunil Gupta, Truyen Tran, and Svetha
  Venkatesh, `Learning transferable domain priors for safe exploration in
  reinforcement learning', in {\em IJCNN}, pp. 1--10. IEEE, (2020).

\bibitem{Kumar2020}
Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn, `{One Solution is
  Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL}', in {\em
  NeurIPS}, p. 8198–8210, (2020).

\bibitem{lee2019efficient}
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and
  Ruslan Salakhutdinov.
\newblock {Efficient Exploration via State Marginal Matching}, 2019.
\newblock \arXiv{1906.05274}.

\bibitem{Lew2020safe}
Thomas Lew, Apoorva Sharma, James Harrison, Andrew Bylard, and Marco Pavone,
  `{Safe Active Dynamics Learning and Control: A Sequential
  Exploration–Exploitation Framework}', {\em IEEE Transactions on Robotics},
  {\bf 38}(5),  2888--2907, (2022).

\bibitem{Li2006}
Lihong Li, Thomas~J Walsh, and Michael~L Littman, `{Towards a Unified Theory of
  State Abstraction for MDPs}', in {\em AI{\&}M}, pp. 1--10, (2006).

\bibitem{luo2021mesa}
Michael Luo, Ashwin Balakrishna, Brijen Thananjeyan, Suraj Nair, Julian Ibarz,
  Jie Tan, Chelsea Finn, Ion Stoica, and Ken Goldberg.
\newblock {MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance},
  2021.
\newblock \arXiv{2112.03575}.

\bibitem{marzari2021curriculum}
Luca Marzari, Davide Corsi, Enrico Marchesini, and Alessandro Farinelli,
  `Curriculum learning for safe mapless navigation', in {\em {SAC}}, pp.
  766--769, (2022).

\bibitem{DBLP:conf/icml/MiryoosefiJ22}
Sobhan Miryoosefi and Chi Jin, `A simple reward-free approach to constrained
  reinforcement learning', in {\em {ICML}}, pp. 15666--15698, (2022).

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin~A. Riedmiller, Andreas Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis, `{Human-level control through deep reinforcement learning}',
  {\em Nature}, {\bf 518}(7540),  529--533, (2015).

\bibitem{peng2022safe}
Zhenghao Peng, Quanyi Li, Chunxiao Liu, and Bolei Zhou, `Safe driving via
  expert guided policy optimization', in {\em CoRL}, pp. 1554--1563, (2022).

\bibitem{qin2021density}
Zengyi Qin, Yuxiao Chen, and Chuchu Fan, `{Density Constrained Reinforcement
  Learning}', in {\em ICML}, pp. 8682--8692, (2021).

\bibitem{ray2019benchmarking}
Alex Ray, Joshua Achiam, and Dario Amodei.
\newblock {Benchmarking Safe Exploration in Deep Reinforcement Learning}, 2019.
\newblock \url{https://cdn.openai.com/safexp-short.pdf}.

\bibitem{Savas2018}
Yagiz Savas, Melkior Ornik, Murat Cubuktepe, and Ufuk Topcu, `{Entropy
  Maximization for Constrained Markov Decision Processes}', in {\em 56th Annual
  Allerton Conference on Communication, Control, and Computing}, pp. 911--918,
  (2018).

\bibitem{Schuitema2010}
Erik Schuitema, Martijn Wisse, Thijs Ramakers, and Pieter Jonker, `{The design
  of LEO: A 2D bipedal walking robot for online autonomous Reinforcement
  Learning}', in {\em {IROS}}, pp. 3238--3243, (2010).

\bibitem{seo2021state}
Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin
  Lee, `{State Entropy Maximization with Random Encoders for Efficient
  Exploration}', in {\em {ICML}}, pp. 9443--9454, (2021).

\bibitem{Simao2021alwayssafe}
Thiago~D. Sim{\~a}o, Nils Jansen, and Matthijs T.~J. Spaan, `{AlwaysSafe:
  Reinforcement Learning Without Safety Constraint Violations During
  Training}', in {\em AAMAS}, p. 1226–1235, (2021).

\bibitem{srinivasan2020learning}
Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn.
\newblock {Learning to be Safe: Deep RL with a Safety Critic}, 2020.
\newblock \arXiv{2010.14603}.

\bibitem{Sui2015}
Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause, `{Safe Exploration
  for Optimization with Gaussian Processes}', in {\em ICML}, pp. 997--1005,
  (2015).

\bibitem{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto, {\em {Reinforcement Learning: An
  Introduction}}, volume~2, MIT press, 2018.

\bibitem{sutton2016emphatic}
Richard~S. Sutton, A.~Rupam Mahmood, and Martha White, `{An Emphatic Approach
  to the Problem of Off-policy Temporal-Difference Learning}', {\em JMLR}, {\bf
  17}(1),  2603--2631, (2016).

\bibitem{svidchenko2021maximum}
Oleg Svidchenko and Aleksei Shpilman.
\newblock {Maximum Entropy Model-based Reinforcement Learning}, 2021.
\newblock \arXiv{2112.01195}.

\bibitem{Taylor2009transfer}
Matthew~E. Taylor and Peter Stone, `{Transfer Learning for Reinforcement
  Learning Domains: A Survey}', {\em JMLR}, {\bf 10}(56),  1633--1685, (2009).

\bibitem{Tessler2019}
Chen Tessler, Daniel~J. Mankowitz, and Shie Mannor, `{Reward Constrained Policy
  Optimization}', in {\em {ICLR}}, (2019).

\bibitem{thananjeyan2021recovery}
Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan
  Srinivasan, Minho Hwang, Joseph~E Gonzalez, Julian Ibarz, Chelsea Finn, and
  Ken Goldberg, `{Recovery {RL:} Safe Reinforcement Learning With Learned
  Recovery Zones}', {\em IEEE Robotics and Automation Letters}, {\bf 6}(3),
  4915--4922, (2021).

\bibitem{turchetta2020safe}
Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh
  Agarwal, `{Safe Reinforcement Learning via Curriculum Induction}', in {\em
  {NeurIPS}}, pp. 12151--12162, (2020).

\bibitem{vezzani2019learning}
Giulia Vezzani, Abhishek Gupta, Lorenzo Natale, and Pieter Abbeel.
\newblock {Learning latent state representation for speeding up exploration},
  2019.
\newblock \arXiv{1905.12621}.

\bibitem{Xie2019}
Zhaoming Xie, Patrick Clary, Jeremy Dao, Pedro Morais, Jonathan~W. Hurst, and
  Michiel van~de Panne, `{Learning Locomotion Skills for Cassie: Iterative
  Design and Sim-to-Real}', in {\em {CoRL}}, pp. 317--329, (2019).

\bibitem{Yang2021}
Qisong Yang, Thiago~D. Sim{\~a}o, Simon~H. Tindemans, and Matthijs T.~J. Spaan,
  `{WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement
  Learning}', in {\em {AAAI}}, pp. 10639--10646, (2021).

\bibitem{Yang2023cem}
Qisong Yang and Matthijs T.~J. Spaan, `{CEM: Constrained Entropy Maximization
  for Task-Agnostic Safe Exploration}', in {\em AAAI}, pp. 10798--10806,
  (2023).

\bibitem{Yang2020}
Tsung{-}Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter~J. Ramadge,
  `{Projection-Based Constrained Policy Optimization}', in {\em {ICLR}},
  (2020).

\bibitem{yang2021accelerating}
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter~J Ramadge,
  `Accelerating safe reinforcement learning with constraint-mismatched baseline
  policies', in {\em {ICML}}, pp. 11795--11807, (2021).

\bibitem{zahavy2021discovering}
Tom Zahavy, Brendan O'Donoghue, Andr{\'e} Barreto, Sebastian Flennerhag,
  Volodymyr Mnih, and Satinder Singh.
\newblock {Discovering Diverse Nearly Optimal Policies with Successor
  Features}, 2021.
\newblock \arXiv{2106.00669}.

\bibitem{Zanger2021}
Moritz~A. Zanger, Karam Daaboul, and J.~Marius Z{\"{o}}llner, `{Safe Continuous
  Control with Constrained Model-Based Policy Optimization}', in {\em {IROS}},
  pp. 3512--3519, (2021).

\bibitem{Zhang2020}
Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman,
  `{Cautious Adaptation For Reinforcement Learning in Safety-Critical
  Settings}', in {\em {ICML}}, p. 11055–11065, (2020).

\bibitem{zhu2020transfer}
Zhuangdi Zhu, Kaixiang Lin, Anil~K. Jain, and Jiayu Zhou, `Transfer learning in
  deep reinforcement learning: A survey', {\em IEEE Transactions on Pattern
  Analysis and Machine Intelligence},  1--20, (2023).

\bibitem{ziebart2010modeling}
Brian~D Ziebart, {\em Modeling Purposeful Adaptive Behavior with the Principle
  of Maximum Causal Entropy}, Ph.D.\ dissertation, Carnegie Mellon University,
  2010.

\end{thebibliography}
