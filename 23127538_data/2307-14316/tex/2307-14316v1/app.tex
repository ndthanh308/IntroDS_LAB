
\newpage
\appendix




\section{SAC-Lagrangian}
\label{app:saclag}
In this section, we present how we learn the parameters in SAC-$\lambda$. In SAC-$\lambda$, the constrained optimization problem is solved by Lagrangian methods \cite{bertsekas2014constrained}, where an entropy weight $\alpha$ and a safety weight $\beta$ (Lagrange-multipliers) are introduced to the constrained optimization:
\begin{equation}
\max_\pi \min_{\alpha \geq 0} \min_{\beta \geq 0}  f(\pi) - \alpha e(\pi) - \beta g(\pi),
\label{eq:lagopt}
\end{equation}
where
$f(\pi) = \E_{s_0 \sim \iota(\cdot), a_0 \sim \pi(\cdot\mid s_0)}\left[Q_{\pi}^{r}(s_0,a_0)\right]$,
$e(\pi) = \E_{s_t \sim \rho_{\pi}} \left[ \log(\pi(\cdot\mid s_t))  + \minH \right]$ , \text{and \hfill}
$g(\pi) =  \E_{s_0 \sim \iota(\cdot), a_0 \sim \pi(\cdot\mid s_0)}\left[Q_{\pi}^{c}(s_0,a_0) - d\right]$.
In \eqref{eq:lagopt}, the max-min optimization problem can be solved by gradient ascent on $\pi$, and descent on $\alpha$ and~$\beta$.

Initially, SAC-$\lambda$ was developed for local constraints, which means that the safety cost is constrained at each timestep \cite{ha2020learning}.
However, it can be easily generalized to constrain the expected cost-return\footnote{A similar approach can be found at \url{https://github.com/openai/safety-starter-agents}.}. 

Using a similar formulation \cite{haarnoja2018soft2}, we can get the actor loss:
\begin{equation}
    J_\pi(\theta_\pi) =
    - \E_{\begin{subarray}{c} s_t \sim \mathcal{D} \\ a_t \sim \pi(\cdot \mid s_t)\end{subarray}} 
    \left[ 
                Q_{\pi}^r(s_t,a_t) 
                -\alpha\log \pi(a_t \mid s_t) 
                -\beta Q_{\pi}^c(s_t,a_t)
    \right],
\label{eq:UpdatePi}
\end{equation}
where $\mathcal{D}$ is the replay buffer and $\theta_\pi$ indicates the parameters of the policy~$\pi$.

The safety and reward critics (including a bonus for the policy entropy) are, respectively, trained to minimize
\begin{equation}
    J_C(\theta_C) \!=\!\!
    \E_{(s_t,a_t) \sim \mathcal{D}}
    \left[
        \frac{1}{2}
        \left(
                Q^c_{\theta_C}(s_t,a_t) -
                 (c_t \!+\!\gamma Q^c_{\theta_C}(s_{t\!+\!1},a_{t\!+\!1}))
        \right)^2    
    \right]
\label{eq:lossjc}
\end{equation}
and
\begin{equation}
\begin{aligned}
    J_R(\theta_R) = \E_{(s_t,a_t) \sim \mathcal{D}} \Big[\frac{1}{2} (Q^r_{\theta_R}(s_t,a_t)-(r_t  +\gamma (Q^r_{\theta_R}(s_{t+1},a_{t+1})-\alpha \log (\pi(a_{t+1} \mid s_{t+1})))))^2    \Big],
\end{aligned}
\label{eq:Updateqr}
\end{equation}
where $a_{t+1} \sim \pi(\cdot \mid s_{t+1})$, $Q^c$ and $Q^r$ are parameterized by $\theta_C$ and $\theta_R$, respectively.

\input{algorithms/training_safe_explorer}

Finally, 
let $\theta_\alpha$ and $\theta_\beta$ be the parameters learned for the exploration and safety weight such that $\alpha = {\rm softplus}(\theta_\alpha)$ and $\beta = {\rm softplus}(\theta_\beta)$, where
\[
\softplus(x) = \log(\exp(x) + 1).
\]
We can learn $\alpha$ and $\beta$ by minimizing the loss functions:
\begin{align}
J_\alpha(\theta_\alpha) = \E_{\begin{subarray}{c} s_t \sim \mathcal{D}\\ a_t \sim \pi(\cdot \mid s_t)\end{subarray}} \left[ -\alpha(\log(\pi(a_t \mid s_t))+ \minH ) \right],
\end{align}
\text{and}
\begin{align}
J_\beta(\theta_\beta) = \E_{\begin{subarray}{c} s_t \sim \mathcal{D}\\ a_t \sim \pi(\cdot \mid s_t)\end{subarray}} \left[ \beta(d-Q_{\pi}^c(s_t,a_t))\right].
\end{align}
So the corresponding weight will be adjusted if the constraints are violated, that is, if we estimate that the current policy is unsafe or if it does not have enough entropy.

In this paper, we train the \textit{guide} agent by solving the constraint optimization problem \eqref{eq:SafeMERL} based on the auxiliary reward $r^\delta$, defined by \eqref{eq:DefAuxRew}.
Then, we can use SAC-$\lambda$ directly employed to solve \eqref{eq:SafeMERL}, as \cref{alg:safe-explorer} shows.


\section{Relation between source and target tasks}

In this section, we describe the source task given a target task and the mapping from the target task to the source task.

\subsection{State Abstraction}
\label{app:state_abstraction}

To build the source task based on a target task and a mapping $\Xi$ from the target state space to the source state space, we assume $\Xi$ is a state abstraction function \cite{Li2006}.

Let $\mathcal{M}^\targ = \langle \mathcal{S}^\targ, \mathcal{A}^\targ, \mathcal{P}^\targ, r^\targ, c^\targ, d^\targ, \iota^\targ, \gamma \rangle$ be the target task, 
$\mathcal{M}^\src = \langle \mathcal{S}^\src, \mathcal{A}^\src, \mathcal{P}^\src, \emptyset, c^\src, d^\src, \iota^\src, \gamma \rangle$ be the source task,  and
$\Xi: \mathcal{S}^\targ \rightarrow \mathcal{S}^\src$ the state abstraction function.
We define $\Xi^{-1}$ as the inverse of the abstraction function such that  $\Xi^{-1}(s^\targ) = \{s^\src \in \mathcal{S}^\src | \Xi(s^\src) = s^\targ \}$.
We assume a weighting function $w\colon \mathcal{S} \mapsto [0,1]$,
where 
\begin{equation}\label{eq:w}
\sum_{s^\targ \in \Xi^{-1}(s^\src)} w(s^\targ) = 1, \forall s^\src \in \mathcal{S}^\src.    
\end{equation}
Now we can define the transition and cost function of the target task:
\begin{align}
\mathcal{P}^{\src}(s^{\src'} \mid s^\src, a)
    ~&= \sum_{s^\targ \in \Xi^{-1}(s^\src)} \quad \sum_{s^{\targ'} \in \Xi^{-1}(s^{\src'})} w(s^\targ) \mathcal{P}^{\targ}(s^{\targ'} \mid s^\targ, a)\\
c^\src(s^\src, a)
    ~&= \sum_{s^\targ \in \Xi^{-1}(s^\src) } w(s^\targ) c^\targ(s^\targ, a)\\
\iota^\src(s^\src)
    ~&= \sum_{s^\targ \in \Xi^{-1}(s^\src)} w(s^\targ) \iota^\targ(s^\targ).
\end{align}

\subsection{Proof of Lemma 1}
\label{sec:proof_lemma}

In this section, we show that if $\Xi$ is a $Q_{\pi}^{c}$-irrelevance state abstraction, then the expected cost of any source policy is the same in the source task and in the target task.
For the convenience of the reader, we restate our assumption and lemma.


\paragraph{Assumption 3.}
\textit{
$\Xi$ is a $Q_{\pi}^{c}$-irrelevance abstraction~\cite{Li2006}, therefore
\[
\Xi(s) = \Xi(s')
        \Rightarrow Q_{\pi^\targ}^{c}(s, a) = Q_{\pi^\targ}^{c}(s', a), 
    \forall s, s' \in \mathcal{S}^\targ, a \in \mathcal{A}, \pi^\targ.
\]
}

\paragraph{Lemma 1.}\textit{
Given \cref{a:shared_action} and \cref{a:abstraction}, we have
\[
    Q_{\pi^\src}^{c,\src}(\Xi(s),a) 
    =
    Q_{\pi^{\src \rightarrow \targ}}^{c,\targ}(s,a)
    \quad \forall s \in \mathcal{S}^\targ, a \in \mathcal{A}, \pi^\src.
\]
That is, the expected cost of a source policy is the same in the source task and in the target task.
}
Our proof follows an induction strategy inspired by previous work \cite[Claim 1]{DBLP:conf/icml/AbelHL16}.
\begin{proof}
Let us consider a non-Markovian constrained decision process $\mathcal{M}_T = \langle \mathcal{S}_T, \mathcal{A}, \mathcal{P}_T, \emptyset, c^T, d^\src, \iota_T, \gamma \rangle$ which is parameterized by an integer $T$.
In this process, the agent takes $T$ steps on the source task and then switches to the target task.
Thus,
\begin{align}
\mathcal{S}_T ~&=
\begin{cases}
    \mathcal{S}^\targ & \text{ if } T = 0 \\
    \mathcal{S}^\src & \text{ otherwise.}
\end{cases} \\ 
c_T(s, a) ~&= 
\begin{cases}
    c^\targ(s, a) & \text{ if } T = 0 \\
    c^\src(s, a) & \text{ otherwise.}
\end{cases} \\     
\mathcal{P}_T(s' \mid s, a)~&= 
\begin{cases}
    \mathcal{P}^{\targ}(s' \mid s, a) & \text{ if } T = 0 \\
    \sum_{s^\targ \in \Xi^{-1}(s)} w(s^\targ) \mathcal{P}^{\targ}(s' \mid s^\targ, a) & \text{ if } T = 1 \\
    \mathcal{P}^{\src}(s' \mid s, a) & \text{ otherwise. }
\end{cases} \\
\iota_T(s) ~&= 
\begin{cases}
    \iota^\targ(s) & \text{ if } T = 0 \\
    \iota^\src(s) & \text{ otherwise.}
\end{cases} 
\end{align}
The $Q_{\pi}^{c,\targ}(s,a)$-value for taking action $a \in \mathcal{A}$ in state $s \in \mathcal{S}_T$ and follow the policy $\pi$ is:
\begin{align}
Q_{T,\pi}^{c}(s,a) = 
\begin{cases}
    Q_{\pi}^{c,\targ}(s, a) & \text{ if } T = 0 \\
    \sum_{s^\targ \in \Xi^{-1}(s) } w(s^\targ) Q_{\pi}^{c,\targ}(s^\targ,a) & \text{ if } T = 1 \\
    c^\src(s, a) + \gamma \sum_{s' \in \mathcal{S}^\src} \mathcal{P}^\src(s' \mid s, a)  \sum_{a' \in \mathcal{A}}  \pi(a' \mid s') Q_{T-1,\pi}^{c}(s',a')  & \text{ otherwise.}
\end{cases}
\end{align}
We proceed by induction on $T$ to show that 
\[
\forall T, s^\targ, a, \pi: Q_{\pi}^{c,\targ}(s_T, a) = Q_{\pi}^{c,\targ}(s^\targ,a),
\]
where $s_T = s^\targ$ if $T=0$ and
$s_T = \Xi(s^\targ)$ otherwise.

\paragraph{Base case: $T=0$.}
As $Q_{0}^{c} =  Q^{c,\targ}$ this case follows trivially.

\paragraph{Base case: $T=1$.}
From the definition of $Q_{1,\pi}^{c}$, we have:
\begin{align}
    Q_{1,\pi}^{c}(s_T,a)
        = \sum_{s^{\targ'} \in \Xi^{-1}(s_T) } w(s^{\targ'}) Q_{\pi}^{c,\targ}(s^{\targ'},a) \\
        &= \sum_{s^{\targ'} \in \Xi^{-1}(s_T) } w(s^{\targ'}) Q_{\pi}^{c,\targ}(s^\targ,a) \label{step:s_star}\\
        &= Q_{\pi}^{c,\targ}(s^\targ,a) \sum_{s^{\targ'} \in \Xi^{-1}(s) } w(s^{\targ'}) \label{step:common_term}\\
        &= Q_{\pi}^{c,\targ}(s^\targ,a). \label{step:w}
\end{align}
In \cref{step:s_star}, we replace every $s^{\targ'}$ by the state $s^\targ$ applying Assumption 3.
As $s^\targ$ is independent of $s^{\targ'}$, in \cref{step:common_term}, we can move the Q-values out of the summation.
Finally, in \cref{step:w}, we can use \cref{eq:w} to replace the summation by 1, which concludes this case.



\paragraph{Incuctive case: $T> 1$.}
We assume as our inductive hypothesis that:
\[
\forall s^\targ, a, \pi: Q_{T-1, \pi}^{c}(s_{T}, a) = Q_{\pi}^{c,\targ}(s^\targ,a).
\]


We start applying the definition of $Q_T$ for $T > 1$:
\begin{align}
Q_{T, \pi}^{c}&(s_{T}, a) =
c^\src(s_{T}, a)
 + \gamma \sum_{s' \in \mathcal{S}^\src} \mathcal{P}^\src(s' \mid s_{T}, a) 
 \sum_{a' \in \mathcal{A}}  \pi(a' \mid s') Q_{T-1,\pi}^{c}(s',a')  \\
&=
\sum_{s^\targ \in \Xi^{-1}(s_{T}) } w(s^\targ)  c^\targ(s^\targ, a)
 + \gamma  
\sum_{s' \in \mathcal{S}^\src} \sum_{s^\targ \in \Xi^{-1}(s_{T})}  \sum_{s^{\targ'} \in \Xi^{-1}(s')} w(s^\targ) \mathcal{P}^{\targ}(s^{\targ'} \mid s^\targ, a)
 \sum_{a' \in \mathcal{A}}  \pi(a' \mid s') Q_{T-1,\pi}^{c}(s',a')  \label{step:definitions}\\
&=
\sum_{s^\targ \in \Xi^{-1}(s_{T}) } w(s^\targ)  c^\targ(s^\targ, a)
 + 
\sum_{s^\targ \in \Xi^{-1}(s_{T})} w(s^\targ) \gamma  \sum_{s' \in \mathcal{S}^\src} \sum_{s^{\targ'} \in \Xi^{-1}(s')}  \mathcal{P}^{\targ}(s^{\targ'} \mid s^\targ, a)
 \sum_{a' \in \mathcal{A}}  \pi(a' \mid s') Q_{T-1,\pi}^{c}(s',a')  \label{step:rearange1}\\
&=
\sum_{s^\targ \in \Xi^{-1}(s_{T}) } w(s^\targ) \left[ c^\targ(s^\targ, a)
 +  \gamma  \sum_{s' \in \mathcal{S}^\src} \sum_{s^{\targ'} \in \Xi^{-1}(s')}  \mathcal{P}^{\targ}(s^{\targ'} \mid s^\targ, a)
 \sum_{a' \in \mathcal{A}}  \pi(a' \mid s') Q_{T-1,\pi}^{c}(s',a')  \right] \label{step:rearange2}\\
&=
\sum_{s^\targ \in \Xi^{-1}(s_{T}) } w(s^\targ) \left[ c^\targ(s^\targ, a)
 +  \gamma  \sum_{s' \in \mathcal{S}^\src} \sum_{s^{\targ'} \in \Xi^{-1}(s')}  \mathcal{P}^{\targ}(s^{\targ'} \mid s^\targ, a)
 \sum_{a' \in \mathcal{A}}  \pi(a' \mid s') Q_{\pi}^{c,\targ}(s^{\targ'},a')
 \right]  \label{step:hyp}\\
&=
\sum_{s^\targ \in \Xi^{-1}(s_{T}) } w(s^\targ) \left[ c^\targ(s^\targ, a)
 +  \gamma  \sum_{s^{\targ'} \in \mathcal{S}^\targ}  \mathcal{P}^{\targ}(s^{\targ'} \mid s^\targ, a)
 \sum_{a' \in \mathcal{A}}  \pi(a' \mid s')  Q_{\pi}^{c,\targ}(s^{\targ'},a')
 \right]  \label{step:join_sums}\\
&=
\sum_{s^\targ \in \Xi^{-1}(s_{T}) } w(s^\targ) Q_{\pi}^{c,\targ}(s^\targ,a) \label{step:q_value_definition}\\
& = Q_{\pi}^{c,\targ}(s^\targ,a). \label{step:arbitrary_state}
\end{align}
In this derivation, 
\cref{step:definitions} applies the definitions of $c^\src$ and $\mathcal{P}^\src$,
\cref{step:rearange1,step:rearange2} rearrange our terms,
\cref{step:hyp} applies our inductive hypothesis, \cref{step:join_sums} join the two summations as we are considering all possible states in $\mathcal{S}^\targ$, and \cref{step:q_value_definition} we apply the Q-value definition.
Finally, in \cref{step:arbitrary_state} we can choose any arbitrary state $s^\targ \in \Xi^{-1}(s_{T})$, which concludes our proof.
\end{proof}

\section{Regularized Reward}
\label{app:regularized_reward}

\begin{equation}
\begin{aligned}
    \omega r^{\textrm{KL}} {+} \alpha r^\mathcal{H}
= &\omega \log \frac{\pi^\src(a_t\mid \Xi(s_t))}{\pi^\targ(a_t\mid s_t)} + \omega r^\mathcal{H}\\
= &\omega (\log(\pi^\src(a| \Xi(s))) - \log( \pi^\targ(a| s))) + \alpha r^\mathcal{H}\\
= &\omega \log(\pi^\src(a| \Xi(s))) + \omega (-\log(\pi^\targ(a| s))) + \alpha r^\mathcal{H}\\
= &\omega \log(\pi^\src(a| \Xi(s))) + \omega r^\mathcal{H} + \alpha r^\mathcal{H}\\
= &\omega r^\src + (\omega + \alpha) r^\mathcal{H}.
\end{aligned}
\end{equation}


\section{Two strategies in composite sampling}
\label{app:composite-sampling}

\paragraph{\textbf{Linear-decay (\cref{alg:composite_sampling-ld}).}}
This strategy linearly decreases the probability of using $\pi^\src$ with a constant decay rate after each iteration of the algorithm, conversely increasing the probability of using $\pi^\targ$. 
We have two modes with \textit{linear-decay}: 
    \emph{step-wise}, where in each time step we may change $\pib$; and 
    \emph{trajectory-wise}, where $\pib$ only changes at the start of a trajectory.
The mode is decided before executing an episode and smoothly switches from the complete \emph{step-wise} to the complete \emph{trajectory-wise} over the training process. 
We linearly decrease the probability of executing the \emph{step-wise} and use the \textit{guide} with a constant decay rate after each iteration of the algorithm, conversely increasing the probability of executing the \emph{trajectory-wise} and using the student policy. So, we initialize the probabilities $P_{\pi} = 1$ to determine $\pib$, and $P_{wise} = 1$ to determine the mode at the beginning (line~\ref{line:initialize-p}). We linearly decrease them with a constant decay rate $\upsilon$ (lines~\ref{line:decrease-pwise} and \ref{line:decrease-ppi}), determined by the training length. At the beginning of each episode, we sample $\kappa_{wise} \sim U(0,1)$, so if $\kappa_{wise} < P_{wise}$, we will execute \emph{step-wise}, or we are in \emph{trajectory-wise} (lines~\ref{line:determine-wise-begin}-\ref{line:determine-wise-end}).
Under \emph{step-wise}, at each time step, we sample from the \textit{guide} $\pi^\src$ with probability $P_{\pi}$, and sample from the student $\pi^\targ$ with probability $1-P_{\pi}$ (lines~\ref{line:decide-pib-stepwise-begin}-\ref{line:decide-pib-stepwise-end}). Under \emph{trajectory-wise}, we only make a decision once at the beginning of the trajectory (line~\ref{line:decide-pib-trajwise}).

\paragraph{\textbf{Control-switch (\cref{alg:composite_sampling-cs}).}}
To balance between the safe exploration and the sample efficiency (the samples from the target policy are relatively more valuable), the student policy keeps sampling, i.e.,  $\pib = \pi^\targ$ at the start of a trajectory (line~\ref{line:StuSam-cs}); after we meet the first $c_{t-1}>0$, we have $\pib = \pi^\src$ until the end of the trajectory (lines~\ref{line:switch_control_begin}-\ref{line:switch_control_end}). Therefore, the guide policy serves as a \textit{rescue policy} to improve safety during sampling. In addition, we leverage two replay buffers $\mathcal{D}^\src$ and $\mathcal{D}^{\targ}$ to save the guide and student samples separately (lines~\ref{line:save_experience_begin}-\ref{line:save_experience_end}), so as to control the probability $P_{\mathcal{D}^\targ}$ to use the more on-policy samples in $\mathcal{D}^{\targ}$. Thus, we have the probability $P_{\mathcal{D}^\src} = 1-P_{\mathcal{D}^\targ}$ to sample from $\mathcal{D}^\src$.
In practice, we train the safe guide to achieve $Q_{\pi^\src}^c(s,a) \leq d, s \sim \mathcal{D}, a \sim \pi^\src(\cdot \mid s)$. From the definition of $Q_{\pi^\src}^c(s,a)$, we can basically ensure $\E_{\tau \sim \rho_{\pi^\src}} \left[\sum^{\infty}_{t=0} \gamma^t  c_t \middle| s_0 = s, a_0 = a\right] \leq d$ even starting with $c_0 >0$.

\paragraph{\textbf{Main difference}.} The key distinction between linear-decay and control-switch approaches lies in the number of off-policy interactions from the student's perspective. Linear-decay entails the collection of more samples from the guide during early episodes, whereas control-switch enables the agent to collect more on-policy samples and only occasionally relies on off-policy samples from the guide following unsafe interactions. Additionally, linear-decay necessitates predefined schedules for the behaviour policy, while control-switch is adaptive. The pursuit of novel adaptive schedules presents a promising avenue for future research.

\input{algorithms/linear-decay}
\input{algorithms/control-switch}


\newpage

\section{Ablation Study }
\label{app:ablation-study}

% Figure environment removed

 \newpage 
\section{Evaluation of the target policy}
\label{app:target-policy}
\paragraph{Comparison with baselines}
In \cref{DuringTraining}, we evaluate the behaviour policy $\pib$ for all algorithms: CPO, SAC-$\lambda$, \textsc{CPO-pre}, \textsc{SAC-$\lambda$-pre}, EGPO, and \textsc{\ours{}}. So, in \cref{DuringTrainingTarget}, we show how their resulted target policy will perform during training. In all these algorithms, \textsc{\ours{}} (control-switch) is the only one that can find a safe optimal target policy in all environments. However, \textsc{\ours{}} (linear-decay) cannot achieve similar performance, especially in \textit{Semi-dynamic} and \textit{Dynamic}. We infer that \textsc{\ours{}} (linear-decay) lack samples from the target policy, especially at the early stage of training. The behaviour policy of EGPO (with benefits from the targeted expert policy) has outstanding performance during training (\cref{DuringTraining}), but EGPO fails to find a safe target policy finally. As to the pre-training baselines, \textsc{CPO-pre} and \textsc{SAC-$\lambda$-pre} do not attain obvious improvement compared to CPO and SAC-$\lambda$ that are trained from scratch. Instead, pre-training may have some negative impacts on getting a good target policy. The only exception is that \textsc{CPO-pre} is largely improved in the relatively simple environment \textit{Static}.

% Figure environment removed

\section{Hyperparameters}
\label{app:hyper}
We list the hyperparameters used in \textsc{\ours{}},
which are summarized in Table~\ref{hyper}. As to the baselines, we use the default hyperparameters in \url{https://github.com/openai/safety-starter-agents}. 
All runs in the experiment use separate feedforward Multilayer Perceptron (MLP) actor and critic networks.
The size of the neural network (all actors and critics of the algorithms) depend on the complexity of the tasks.
We use a replay buffer of size $10^6$ for each off-policy algorithm to store the experience.
The discount factor is set to be $\gamma = 0.99$, the target smoothing coefficient is set to be $0.005$ to update the target networks, and the learning rate to $0.001$.
The clipping intervale hyper-parameters $[\mathcal{I}_l, \mathcal{I}_u]$ is set to $[0.1, 2.0]$, while the sampling probabilities $P_{\mathcal{D}^\src}$ and $P_{\mathcal{D}^\targ}$ are set to $0.25$ and $0.75$, respectively.
The maximum episode length is 1000 steps in all experiments.
We set the safety constraint $d$ based on the problem.
The rest of the hyperparameters are explained in the Empirical Analysis part of the paper. 
All experiments are performed on an
Intel(R) Xeon(R) CPU@3.50GHz with 16 GB of RAM.

\begin{table}
    \centering
    \begin{tabular}{lrrrr}
         \toprule
        Parameter& Static& Semi-Dynamic& Dynamic & Note\\
        \midrule
        Size of networks& $(32,32)$& $(64,64)$ & $(256, 256)$ \\
        Size of replay buffer& $10^6$&$10^6$&$10^6$&  $|\mathcal{D}|$\\
        Batch size& 32& 64& 256 \\
        Number of epochs& $50$& $100$& $150$& \\
        Safety constraint& $5$& $8$&$25$& $d$ \\
        \bottomrule 
    \end{tabular}
    \caption{Summary of hyperparameters in \textsc{\ours{}}.}
    \label{hyper}
\end{table}

\paragraph{Safety-mapping function.}
The state spaces of the source and target task differ by the presence of the LiDAR observation of the target location.
While the source task only has a safety-related signal $x_c$, the target task has an additional goal-related signal $x_r$.
Thus, following the definition in Section~\ref{sec:problem-setting}, we can map the target state $[x_c, x_r]$ to the source state ignoring the target-related signal: $\Xi([x_c, x_r]) = [x_c]$. 


\section{Expert Guided Policy Optimization}
\label{app:egpo}
We also compare our algorithms to an Expert-in-the-loop RL method called Expert Guided Policy Optimization (EGPO) that incorporates a well-performing expert policy as a demonstrator as well as a safety guardian \cite{peng2022safe}.
However, EGPO constrains safety behaviours at each timestep, which is different from our safety defined on long-term cost-return.
In terms of the safe guide, EGPO assumes the access to the well-performing expert policy, but our safe guide is task-agnostic.
Thus, the expert in EGPO depends on the target task and does not undertake the task of exploration, while our safe guide can be useful for different reward functions and enhance the exploration capabilities of the student.
Even though, EGPO can be easily adapted to our setting.
The constraint of EGPO on the guardian intervention frequency can be directly transferred to be our safety constraint.
Also, we do not minimize intervention anymore.
Once the EGPO agent starts to take unsafe actions, the expert policy can take over the control until the end.
    