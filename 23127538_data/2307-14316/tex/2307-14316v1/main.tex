\documentclass{ecai}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{balance}
\usepackage{subfigure}

\usepackage{enumerate}
\usepackage[inline]{enumitem}
    \setlist[enumerate]{label = \emph{\roman*})}
\usepackage[most]{tcolorbox}
\usepackage{amsfonts}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalize,noabbrev,nameinlink]{cleveref}
\usepackage{fontawesome5}

\definecolor{tab:blue}{HTML}{1F77B4}
\definecolor{tab:orange}{HTML}{FF7F0E}
\definecolor{tab:green}{HTML}{2CA02C}
\definecolor{tab:red}{HTML}{D62728}
\definecolor{tab:purple}{HTML}{9467BD}
\definecolor{tab:brown}{HTML}{8C564B}
\definecolor{tab:pink}{HTML}{E377C2}
\definecolor{tab:gray}{HTML}{7F7F7F}
\definecolor{tab:olive}{HTML}{BCBD22}
\definecolor{tab:cyan}{HTML}{17BECF}

\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator*{\softplus}{softplus}
\definecolor{algorithmiccommentcolor}{gray}{0.5}
\renewcommand{\algorithmiccomment}[1]{\hfill\small\textcolor{algorithmiccommentcolor}{\textit{$\triangleright$ #1}}}


\newtcolorbox{leftvrule}[1][]{colback=white,
    boxrule=0pt, boxsep=0pt, breakable, enhanced jigsaw,
    borderline west={1.5pt}{0pt}{black},
before skip=10pt,after skip=10pt,
#1}

\newcommand{\minH}{{\overline{\mathcal{H}}}}
\newcommand{\targ}{{\ensuremath{\odot}}}
\newcommand{\src}{{\ensuremath{\diamond}}}
\newcommand{\pib}{{\ensuremath{\pi_b}}}

\newcommand{\ours}{SaGui}

\graphicspath{{Figures/}}



\usepackage{xcolor}
\begin{document}

\begin{frontmatter}

\title{Reinforcement Learning by Guided Safe Exploration}


\author[A]{\fnms{Qisong}~\snm{Yang}\thanks{Equal contribution.}}
\author[B]{\fnms{Thiago}~D.~\snm{Sim√£o}\thanks{}}
\author[B]{\fnms{Nils}~\snm{Jansen}}
\author[A]{\fnms{Simon}~H.~\snm{Tindemans}}
\author[A]{\fnms{Matthijs}~T.~J.~\snm{Spaan}}

\address[A]{Delft University of Technology -- The Netherlands}
\address[B]{Radboud University, Nijmegen -- The Netherlands}

\begin{abstract}
Safety is critical to broadening the application of reinforcement learning (RL).
Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world.
However, the real-world target task might be unknown prior to deployment.
Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed.
We consider the \emph{constrained} reward-free setting, where an agent (the guide) learns to explore safely without the reward signal.
This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal.
After the target task is revealed, safety violations are not allowed anymore.
Thus, the guide is leveraged to compose a safe behaviour policy.
Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable
and gradually eliminate the influence of the guide as training progresses.
The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.
\end{abstract}

\end{frontmatter}

\input{md}


\ack
We thank the reviewers for their insightful comments.
This work has been partially funded by the ERC Starting Grant 101077178 (DEUCE) and the NWO grant NWA.1160.18.238 (PrimaVera). 
Qisong Yang is supported by Xidian University.


\bibliography{main}

\newpage
\onecolumn
\appendix

\input{app}

\end{document}
