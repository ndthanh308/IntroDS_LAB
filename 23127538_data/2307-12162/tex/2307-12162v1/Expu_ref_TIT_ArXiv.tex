\documentclass[draft,onecolumn]{IEEEtran}
%\documentclass[]{IEEEtran}
\usepackage{Expu_ref_TIT_ArXiv}

\usepackage[utf8]{inputenc}

\usepackage{url}
\title{A Refinement of Expurgation}


\author{Giuseppe Cocco, Albert Guill\'en i F\`{a}bregas and Josep Font-Segura
		\thanks{Giuseppe Cocco is with the Department of Signal Theory and Communications, Universitat Polit\`ecnica de Catalunya, 08034, Barcelona, Spain (e-mail: giuseppe.cocco@upc.edu).

Albert Guill\'en i F\`{a}bregas is with the Department of Engineering, University
of Cambridge, CB2 1PZ Cambridge, U.K., and also with the Department
of Information and Communication Technologies, Universitat Pompeu Fabra,
08018 Barcelona, Spain (e-mail: guillen@ieee.org).

Josep Font-Segura is with the Department of Information and Communication Technologies, Universitat Pompeu Fabra, 08018, Barcelona, Spain (e-mail: josep.font@upf.edu).


 This work was supported in part by the Ramon y Cajal fellowship program (grant RYC2021-033908-I) funded by 
MCIN/AEI/10.13039/501100011033, the European Union ``NextGenerationEU'' Recovery Plan for Europe, the European
Research Council under ERC Agreement 725411 and by the Spanish
Ministry of Economy and Competitiveness under Grant PID2020-116683GB-
C22.
}
	}
	

\begin{document}
\maketitle

\begin{abstract}
We show that with probability that tends to $1$ with the code length, expurgating an arbitrarily small fraction of codewords from most pairwise independent codebooks will result in a code attaining the expurgated exponent. We connect the result with the typical error exponent.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}

We consider the problem of reliable communication of $M_n$ equiprobable messages over noisy channels described by a random transformation $\Wnvec$, where $\x\in \mathcal{X}^n$ and $\y\in\mathcal{Y}^n$ are the channel input and output sequences, and $\mathcal{X}$ and $\mathcal{Y}$ are the input and output alphabets, respectively. Each message $m=1,\dotsc,M_n$, where $M_n=\lceil 2^{nR}\rceil$, $R$ being the code rate, is mapped onto a $n$-length codeword $\x_m$ sent over the channel. The code is defined as $\CnallMdet=\{\x_1,\dotsc,\x_{M_n}\}$.
%
We denote with $\PecnmallMdet$ the error probability when codeword $m\in\{1,\dotsc,M_n\}$ from code $\CnallMdet$ is transmitted; similarly $\PecnallMdet=\frac{1}{M_n}\sum_{m=1}^{M_n}\PecnmallMdet$ denotes the average error probability of the code. 
Let $\CnallM=\{\X_1, \dotsc, \X_{M_n}\}$ be a pairwise independent random codebook ensemble, where the probability of a given codebook is $\PP[\CnallM = \CnallMdet] = \PP[\X_1=\x_1,\dotsc,\X_{M_n}=\x_{M_n}]$ and for any two indices $m,k\in\{1,\ldots,M_n\}, m\neq k$, $\PP[\X_m=\x_m,\X_{k}=\x_k]=Q^n(\X_m)Q^n(\X_k)$ where $Q^n(\X_m)=\PP[\X_m=\x_m]$ is a probability distribution defined over $\mathcal{X}^n$.

Let $\PecnmallM$ and $\PecnallM$ be the random variables denoting the error probability of the $m$-th codeword for  random code $\CnallM$ and the average error probability of the code, respectively.
We denote the $n$-length error exponents of such random variables by $\EmnallM = - \frac{1}{n}\log \PecnmallM$ and $\EnallM =  - \frac{1}{n}\log \PecnallM$, respectively.
For some ensembles and channels the ensemble average of the code error probability $\EE\bigl[\PecnallM\bigr]$ is known to decay exponentially in $n$ \cite{feinstein1955}. A lower bound on the error exponent $-\frac{1}{n}\log\EE\bigl[\PecnallM\bigr]$ is given by Gallager's multi-letter random coding exponent $\Ern$ \cite[Eq.~(5.6.16)]{gallagerBook}. This bound is known to be tight for the  \ac{DMC} in the high-rate region  \cite{fano1961transmission}.

In \cite{gallager1965simple} Gallager  showed that, for some channels and ensembles, there exists at least one code with strictly higher error exponent than $\Ern$ at low rates. In order to show this, Gallager considered a pairwise independent ensemble with $M_n'=2M_n-1$ codewords. Using Markov's inequality he showed that
\begin{eqnarray}\label{eq:Gallag}
\PP\Bigl [ \Pecnmall \geq 2^{\frac{1}{s}}\EE[\Pecnmall^s]^{\frac{1}{s}} \Bigr ]\leq \frac{1}{2}
\end{eqnarray}
for any $s>0$.
He then introduced the indicator function
\begin{eqnarray}\label{eq:condit_Gall}
\varphi_m\bigl(\CnallMdet\bigr)=\begin{cases}
1		\ \text{ if } \PecnmallMdet < 2^{\frac{1}{s}}\EE\bigl[\PecnmallMdet^s\bigr]^{\frac{1}{s}}\\
0		\ \text{ otherwise}
\end{cases}
\end{eqnarray}
and showed that, using \eqref{eq:Gallag} and \eqref{eq:condit_Gall},  the following holds
\begin{eqnarray}\label{eq:gall_sum}
\EE\left[\sum_{m=1}^{M_n'}\varphi_m(\Cnall)\right]\triangleq\sum_{m=1}^{M_n'}\EE\bigl[\varphi_m\bigl(\Cnall\bigr)\bigr] \geq  M_n.
\end{eqnarray}
From \eqref{eq:gall_sum} it follows that, since the average number of codewords that have a probability of error smaller than $2^{\frac{1}{s}}\EE\bigl[\Pecnmalldet^s\bigr]^{\frac{1}{s}}$  in a randomly generated code with $M_n'=2M_n-1$ codewords is at least $M_n$, there must exist a code having at least $M_n$ codewords, out of the $M_n'$, fulfilling this property. Thus, by removing (expurgating) the worst half of the codewords from the code with $M_n'$ codewords we obtain a new code with $M_n$ codewords, each of which satisfies the condition in the first line of the right-hand side in~\eqref{eq:condit_Gall}. Finally, restricting $s$ to $0<s\leq 1$, Gallager derives an upper bound on the exponent of $2^{\frac{1}{s}}\EE[\Pecnmalldet^s]^{\frac{1}{s}}$, given by
\begin{align}\label{eqn:expu}
	\Eexn = E_{\rm x}^n(\hat\rho_n,Q^n) - \hat\rho_n R,
\end{align}
where
\begin{align}\label{eqn:ex}
\Exnvarrho& =  -\frac{1}{n}\log \biggl( \sum_{\x}\sum_{\x'} Q^n(\x)Q^n(\x') Z_n(\x,\x')^\frac{1}{\rho}\biggr)^\rho,
\end{align}
$Z_n(\x,\x')=\sum_{\y}\sqrt{\Wnvec\Wnvecp}$ is the Bhattacharyya coefficient between codewords $\x,\x'\in\Xc^n$ while
\begin{align}
\hat\rho_n = \argmax_{\rho\geq 1} \bigl\{ E_{\rm x}^n(\rho,Q^n) - \rho R\bigr\}
\label{eq:rhohatn}
\end{align}
is the bound parameter that yields the highest exponent. Observe that the preceding argument is implicitly valid for the maximal probability of error, since every codeword in the expurgated code attains the same exponent. In addition, observe that \eqref{eq:gall_sum} uses the standard ensemble-average argument, i.e., by taking the average over the ensemble, we show the existence of a code with the desired property.
The exponent in~\eqref{eqn:expu} is the expurgated exponent. We refer to the code with $M_n'$ codewords before expurgation as a mother code. We say that a mother code is good if, once expurgated, we obtain a code with asymptotically the same rate, the codewords of which each have an exponent at least as large as the expurgated.


Implied in \cite[Lemma 1]{scarlett_TIT2014} is a refined expurgation result. Specifically, for $\epsilon>0$ Scarlett \emph{et al.}~show that there exists a code with $M_n'=M_n(1+\epsilon)$ codewords such that removing $\epsilon M_n$ codewords  yields a code that attains the expurgated exponent. Although \cite[Lemma 1]{scarlett_TIT2014} strengthens Gallager's method, it shows the existence of a code that attains the expurgated exponent.
 


\section{Main Result}\label{sec:main}
%
This paper strengthens the existing results on expurgation by showing that the probability of finding a code with $M'=(1+\epsilon)M_n$ codewords that contains a code with at least $M_n$ codewords each of which achieves the expurgated exponent tends to $1$ with the code length. Similarly to Gallager, for a given $\delta_n$, we define the  indicator function
\begin{eqnarray}\label{eq:condit}
\phi_m\bigl(\CnallMdet\bigr)=\begin{cases}
1		\ \text{ if } \EmnallMdet > \Eexn -\delta_n\\
0		\ \text{ otherwise},
\end{cases}
\end{eqnarray}
and the number of codewords attaining an exponent higher than $\Eexn -\delta_n$ as
\begin{eqnarray}\label{eq:numcodws}
\Phi\bigl(\Cnalldet\bigr)\triangleq\sum_{m=1}^{M_n'}\phi_m\bigl(\Cnalldet\bigr).
\end{eqnarray}

\begin{theorem}\label{theo:1}
For a pairwise independent code ensembles with $M_n'=M_n(1+\epsilon)$ codewords and any $\epsilon>0$, if the  sequence $\{\delta_n\}_{n=1}^\infty$, which depends on the channel and the ensemble, satisfies $\lim_{n\rightarrow\infty}\delta_n=0$, then for any $0<\epsilon_1<\epsilon$, it holds that
 \begin{align}\label{eq:statement}
\lim_{n\rightarrow\infty}\PP\bigl[\Phi\bigl(\Cnall\bigr)\geq M_n(1+\epsilon_1)\bigr]=1.
\end{align}

\end{theorem}

\begin{IEEEproof}
See Section \ref{sec:proof}.
\end{IEEEproof}


In words, the probability to find a mother code with $M'=(1+\epsilon)M_n$ codewords that contains a subcode with at least $M_n$ codewords each of which achieves the expurgated exponent tends to $1$ asymptotically in $n$. That is, good mother codes are found easily and only contain an arbitrarily small fraction $\epsilon/(1+\epsilon)$ of additional codewords that need to be expurgated. Theorem~\ref{theo:1} applies to i.i.d. and constant composition codes over \ac{DMC}s, as well as  channels with memory such as the finite-state channel in \cite[Sec.~4.6]{gallagerBook}, for which the expurgated exponent is derived in \cite{coccoTIT2022}. 
Our theorem extends both Gallager's and Scarlett's methods, showing that for many channels and code ensembles the vast majority of codes lead, once expurgated from a small fraction of codewords, to a code for which each codeword achieves the expurgated exponent in~\eqref{eqn:expu}.
Our result is valid for both \ac{ML} and mismatched decoding. The latter can be proved following similar steps as in \cite{coccoTIT2022}. 

The proof, in Section \ref{sec:proof}, makes use of similar arguments as Gallager and Scarlett {\em et al.}, such as the application of Markov's inequality to a tilted average error probability and the indicator function in~\eqref{eq:condit}. Unlike such works, our proof bounds the variance of the number of codewords that meet the expurgated exponent \eqref{eq:numcodws} and employs Chebychev's inequality to bound the probability that a random code does not have enough codewords that attain the expurgated exponent.

We next point out connections between Theorem \ref{theo:1} and the \ac{TRC} exponent \cite{barg_forney_TIT2002,merhav_TIT2018}. At low rates the \ac{TRC} exponent is known to be strictly smaller than the expurgated exponent.
Recent works such as \cite{TruongGJF2022a} and \cite{LargeDevLogErrProb_tamir_TIT2020} show that most $n$-length pairwise-independently generated codes have an exponent that concentrates around its expectation, namely  the \ac{TRC} exponent.  
This suggests that expurgated-achieving codes are rare. Theorem \ref{theo:1} shows that most codes in pairwise independent ensembles and channels for which the bound in \cite{coccoTIT2022} is non-trivial, contain an expurgated-achieving one of asymptotically the same rate. This suggests that many different mother codes can be expurgated to the same expurgated-achieving code. Theorem \ref{theo:1} also suggests that the fraction of codewords responsible for achieving the \ac{TRC} exponent and not the expurgated is very small. Observe that\footnote{We refer to a \emph{typical code} as a code for which the lower bound to the \ac{TRC} exponent in \cite{coccoTIT2022} is non-trivial.}
\begin{align}\label{eq:trc_good}
\PP\bigl[&\Cnall \text{ is typical}\bigr]\notag\\
&=\PP\bigl[ \Cnall \text{ is a good mother code}\,|\,\Cnall \text{ is typical}\bigr]\cdot\PP\bigl[ \Cnall \text{ is typical}\bigr]\notag\\
&~~+ \PP\bigl[\Cnall \text{ is typical}\,|\, \Cnall \text{ is a bad mother code}\bigr]\cdot\PP\bigl[ \Cnall \text{ is a bad mother code}\bigr].
\end{align}
Theorem \ref{theo:1} implies that  
\begin{equation}
\lim_{n\to\infty} \PP\left[ \Cnall \text{ is a bad mother code}\right]= 0,
\end{equation}  while \cite[Theorem 1]{coccoTIT2022} implies that
\begin{equation}
\lim_{n\to\infty} \PP\left[\Cnall \text{ is typical }\right]
= 1.
\end{equation}
Using these facts in \eqref{eq:trc_good} we obtain
\begin{align}
\lim_{n\to\infty} \PP\left[\Cnall \text{ is a good mother code}|\Cnall \text{ is typical }\right]= 1.
\end{align}
This suggests that by removing a fraction $\epsilon/(1+\epsilon)$ of codewords of a typical code with $M_n'=M_n(1+\epsilon)$ codewords we are likely to obtain a code where each of the codewords attains the expurgated exponent.  

As a final remark, recent  works \cite{merhav_TIT2018,LargeDevLogErrProb_tamir_TIT2020,coccoTIT2022,TruongGJF2022a} show that for many ensembles, most codes have an error exponent  $\EnallM$ that, at low rates, is strictly larger than the exponent of the ensemble average error probability, i.e., the random coding exponent. Similarly, Theorem \ref{theo:1} implies that for most codes, almost any codeword has an associated error exponent $\EmnallM$ that is strictly larger than the ensemble average of the exponent of the error probability of the codebook $\EE\big[\EnallM \big]$. In both cases the smaller error exponent of the average probability of error is due to a relatively small number of elements (codes in the first case, codewords in the second) that perform poorly. 
Furthermore, for some ensembles and channels the error exponents of the codes in the ensemble concentrate around the \ac{TRC}. Similarly to such works, it can be shown that the error exponent $\EmnallM$, for any $m$, concentrates around its mean, the expurgated exponent. The proof makes use of Lemma \ref{th:lb_exp} in Section \ref{sec:proof}, and follows almost identical steps as in \cite[Theorem 1]{TruongGJF2022a}, \cite[Theorem 1]{coccoTIT2022} and \cite[Theorem 2]{coccoTIT2022} once $\Pecns$ is replaced by $\Pecnmdet$; so is omitted here.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem 1}\label{sec:proof}
We start with the following lemma, whose proof is almost identical to that of \cite[Lemma 1]{coccoTIT2022}.
\begin{lemma}\label{th:lb_exp}
For a channel $W^n$ and a pairwise-independent $M_n'$-codewords code ensemble with codeword distribution $Q^n$, for any ${m}\in \{1,\ldots,M_n'\}$ it holds that
\begin{equation}\label{eqn:theo5_statement}
\mathbb{P}\bigl[ \Emnall > \Eexn  - \delta_n \bigr]\geq 1-\frac{1}{\gamn},
\end{equation}
where $\gamma_n$ and $\delta_n$ are positive real-valued sequences. 
\end{lemma}

The proof of Lemma \ref{th:lb_exp} follows from Markov's inequality
\begin{eqnarray}
\PP\Bigl [ \Pmcn \geq \gamn^{\frac{1}{s}}\EE[\Pmcn^s]^{\frac{1}{s}} \Bigr ]\leq \frac{1}{\gamn}
\end{eqnarray}
and applying the same steps as in \cite[Theorem 1]{coccoTIT2022} once $\Pecn$ is replaced with $\Pmcn$. Note that $\gamma_n$ and $\delta_n$ are the same sequences presented in the statement of Theorem \ref{theo:1}.
As a side remark, $\lim_{n\rightarrow\infty}\Eexn$ is a lower bound on $\lim_{n\rightarrow\infty}\EE[\EmnallM]$ and is tight for some ensembles and channels. This can be shown by following similar steps as in \cite{coccoTIT2022}.



Applying Lemma \ref{th:lb_exp} for low rates\footnote{While Lemma \ref{th:lb_exp} holds for all rates, at high rates it can be tightened using Gallager's random coding exponent. See the proof of Lemma \ref{th:lb_exp} and \cite{coccoTIT2022} for a similar discussion in the context of the ac{TRC}.} and choosing a sequence  $\delta_n = \frac{\hat\rho_n}{n} \log \gamma_n$ where $\gamn$ is such that $\lim_{n\rightarrow\infty} \gamn=\infty$ while $\lim_{n\rightarrow\infty} \frac{\log\gamn}{n}=0$ \footnote{See \cite{coccoTIT2022} for a discussion on such constraints on $\gamn$.}, $\hat\rho_n$ being defined in \eqref{eq:rhohatn}, we have:
\begin{equation}\label{eqn:expu_common1}
\mathbb{P}\bigl[ \Emnall > \Eexn -\delta_n \bigr]\geq 1-\frac{1}{\gamn}.
\end{equation}
The random variable $\Phi(\Cnall)$, averaged across the ensemble, satisfies
\begin{align}\label{eq:ineq_sum}
\EE[\Phi(\Cnall)]&=\sum_{m=1}^{M_n(1+\epsilon)}\EE[\phi_m(\Cnall)]\\\label{eq:ineq_sum1}
&\geq \sum_{m=1}^{M_n(1+\epsilon)}\left(1-\frac{1}{\gamn}\right)\\
&=M_n(1+\epsilon)\left(1-\frac{1}{\gamn}\right),\label{eq:ineq_sum3}
\end{align}
where \eqref{eq:ineq_sum1} follows from the definition of the indicator function \eqref{eq:condit} and \eqref{eqn:expu_common1}.
%
 The following also holds
\begin{eqnarray}\label{eq:eta_up_2}
{\EE[\Phi(\Cnall)^2]} \leq {M_n^2}(1+\epsilon)^2
\end{eqnarray}
because $0\leq\Phi(\Cnall)\leq M_n(1+\epsilon)$ and $M_n(1+\epsilon)>1$.
Let $\sigma_{\Phi}^2$ be the variance of $\Phi(\Cnall)$. From \eqref{eq:ineq_sum3} and \eqref{eq:eta_up_2} we have
\begin{align}
\sigma_{\Phi}^2&\triangleq \EE[\Phi(\Cnall)^2] - \left({\EE[\Phi(\Cnall)]}\right)^2\\
%&\leq {M_n^2}(1+\epsilon)^2 - {M_n^2}(1+\epsilon)^2\left(1-\frac{1}{\gamn}\right)^2\\
&\leq\frac{M_n^2(1+\epsilon)^2}{\gamn^2}.\label{eq:var_bound}
\end{align}
Using \eqref{eq:var_bound}, by Chebyshev's inequality, for any fixed $\epsilon$ and $\epsilon_2$ such that $0<\epsilon_2<\epsilon\leq 1$ \footnote{The inequality holds actually for any $\epsilon, \epsilon_2>0$, but we restrict ourselves to the case $0<\epsilon_2<\epsilon\leq 1$ to guarantee that $\epsilon_1>0$ in the Theorem statement.} we have:
\begin{align}\label{eq:chain1}
\PP\Big[\big|{\Phi(\Cnall)}-{\EE[\Phi(\Cnall)]}\big|>{M_n}\epsilon_2\Big]&\leq \frac{\sigma_{\Phi}^2}{M_n^2\epsilon_2^2}\\
&\leq \frac{(1+\epsilon)^2}{\epsilon_2^2\gamn^2}\label{eq:chain_last}
%&=\frac{\frac{\EE[\Phi(\Cnall)^2]}{M_n^2} - \left(\frac{\EE[\Phi(\Cnall)]}{M_n}\right)^2}{\epsilon_2^2}\\
%&\leq \frac{(1+\epsilon)^2 - (1+\epsilon)^2\left(1-\frac{1}{\gamn}\right)^2}{\epsilon_2^2}\\
%&= \frac{(1+\epsilon)^2 \left(1-1-\frac{1}{\gamn^2}+\frac{2}{\gamn}\right)}{\epsilon_2^2}\\
%&= \frac{(1+\epsilon)^2 }{\epsilon_2^2}\left(\frac{2}{\gamn}-\frac{1}{\gamn^2}\right)\\
%&\xrightarrow{n} 0, 
\end{align}
which tends to zero for a sequence $\gamn$ such that $\lim_{n\rightarrow\infty} \gamn=0$.
Let us now rewrite the left-hand side of \eqref{eq:chain1} taking into account that ${M_n}\epsilon_2>0$ as 
\begin{align}
&\PP\Big[\big|{\Phi(\Cnall)}-{\EE[\Phi(\Cnall)]}\big|>{M_n}\epsilon_2\Big]\\ \label{eq:chain_a_1}
& \hspace{1em} ~ =\PP\Big[{\Phi(\Cnall)}-{\EE[\Phi(\Cnall)]}>{M_n}\epsilon_2\Big] + \PP\Big[{\EE[\Phi(\Cnall)]-{\Phi(\Cnall)}}>{M_n}\epsilon_2\Big].
\end{align}
When $n$ is large, the second term in \eqref{eq:chain_a_1} can be expressed as
\begin{align}\label{eq:chain_b_1}
\PP\Big[{\EE[\Phi(\Cnall)]-{\Phi(\Cnall)}}>{M_n}\epsilon_2\Big]&=\PP\Big[{{\Phi(\Cnall)}}<\EE[\Phi(\Cnall)]-{M_n}\epsilon_2\Big]\\ 
\label{eq:chain_b_2}&\geq\PP\bigg[\Phi(\Cnall)<{M_n}(1+\epsilon)\left(1-\frac{1}{\gamn}\right)-{M_n}\epsilon_2\bigg]\\
%\label{eq:chain_b_3}&=\PP\bigg[\Phi(\Cnall)< M_n + {M_n}(\epsilon-\epsilon_2)\left(1-\frac{1}{\gamn}\right)\bigg]\\
%\label{eq:chain_b_3}&=\PP\bigg[\Phi(\Cnall)< M_n + M_n\left(\epsilon-\epsilon_2 - \frac{1+\epsilon}{\gamn}\right)\bigg]\\
%\label{eq:chain_b_4}&=\PP\bigg[\Phi(\Cnall)< M_n + {M_n}\epsilon_1\left(1-\frac{1}{\gamn}\right)\bigg]\\
%\label{eq:chain_b_4}&=\PP\bigg[\Phi(\Cnall)< M_n + M_n\left(\epsilon_1 - \frac{1+\epsilon}{\gamn}\right)\bigg]\\
\label{eq:chain_b_5}&\geq\PP\Big[\Phi(\Cnall)< M_n + {M_n}\epsilon_1\Big],
\end{align}
where \eqref{eq:chain_b_2} follows from \eqref{eq:ineq_sum3}, in \eqref{eq:chain_b_5} we defined $\epsilon_1=(\epsilon-\epsilon_2)/2$, which is positive from the definition of $\epsilon_2$, and we used the fact that, since $n$ is large and $\lim_{n\to\infty} \gamma_n = \infty$, there exists an $n_0$ such that for $n>n_0$ we have that $({1+\epsilon})/{\gamn}<\epsilon_1$. Using  that \eqref{eq:chain_last} tends to zero when $\lim_{n\rightarrow\infty} \gamn=\infty$ along with equations \eqref{eq:chain_a_1},  \eqref{eq:chain_b_5}, for $n>n_0$ we have
\begin{align}\label{eq:chain_c1}
\PP\Big[\Phi(\Cnall)< M_n + {M_n}\epsilon_1\Big]&\leq\PP\Big[\big|{\Phi(\Cnall)}-{\EE[\Phi(\Cnall)]}\big|>{M_n}\epsilon_2\Big] .
\end{align}
Finally, from \eqref{eq:chain_last}, the right-hand side of \eqref{eq:chain_c1} tends to zero yielding the desired result, that is
\begin{align}\label{eq:chain_d1}
\lim_{n\to\infty} \PP\Big[\Phi(\Cnall)\geq M_n(1 + \epsilon_1)\Big]&= 1- \lim_{n\to\infty} \PP\Big[\Phi(\Cnall)< M_n(1 + \epsilon_1)\Big]\\
&=  1.\label{eq:chain_d3}
\end{align}
%



\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Expu_ref_TIT_ArXiv}



\end{document}

