\documentclass{article}

\usepackage[numbers]{natbib}
\usepackage[preprint]{neurips_2022}

\usepackage[dvipsnames]{xcolor}         % colors
\definecolor{linkColor}{rgb}{0.18,0.39,0.62}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true,linkcolor=linkColor,citecolor=linkColor,filecolor=linkColor,urlcolor=linkColor]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{longtable}
\usepackage{color, colortbl}

\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{csquotes}
\usepackage{epigraph}
\usepackage{wrapfig}
\usepackage{import}
\usepackage{chemformula}  

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

\input{settings.tex}
\input{math_commands.tex}

% \newcommand{\cmark}{\ding{51}\xspace}%
\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}\xspace}%
% \newcommand{\xmark}{\ding{55}\xspace}%
\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}\xspace}%

\newcommand{\tblidx}[1]{{\scriptsize \texttt{[#1]}}}
\newtheorem{theorem}{Property}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{{\color{blue}\ding{51}}}%
\newcommand{\xmark}{{\color{black}\ding{55}}}%
\definecolor{bluecode}{RGB}{0, 150, 199}
\definecolor{lightcyan}{rgb}{0.88, 1, 1}

\definecolor{greensentence}{RGB}{112, 173, 73}
\definecolor{blueword}{RGB}{91, 155, 213}


\newcommand{\hytt}[1]{\texttt{\hyphenchar \font=\defaulthyphenchar#1}}

% \title{\our{}: Aligning Perception with Language Models}
% \title{From LLM to MLLM: Aligning Perception with Language Models}
\title{Scaling Sentence Representation \\ with Large Language Models}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{
\vspace{-0.25in} \\
Ting Jiang\thanks{~Equal contribution. $\dagger$ Corresponding author.},~~{Shaohan Huang}$^\dagger$ \\
Microsoft \\
% \url{https://github.com/microsoft/unilm} 
\vspace{-0.4cm}
\\}
%\url{https://aka.ms/nlpagi} \\}
% ~~\textbf{Saurabh Tiwary,}~~\textbf{Lidong Zhou,}

\date{}

\begin{document}
\maketitle


\begin{abstract}
Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embedding tasks remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. 
Our approach involves adapting the previous prompt-based method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes.
Through extensive experiments, we derive three primary findings: 1) In-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. The in-context learning-based method achieves performance comparable to contrastive learning methods. 2) For semantic textual similarity (STS) tasks, scaling LLMs up to over a billion parameters does not yield significant improvements. 3) For transfer tasks, scaling LLMs up to 65 billion parameters achieves new state-of-the-art results.
We also fine-tune LLMs using a contrastive learning approach based on our representation method. Employing memory-efficient fine-tuning, our model, with a 2.7 billion parameters, outperforms the previous state-of-the-art method that has 4.8 billion parameters.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Sentence representation is a fundamental problem in natural language processing, requiring language models to project sentences into a vector space based on their semantics. Current methods based on contrastive learning, such as SimCSE~\cite{gao2021simcse}, have successfully leveraged pretrained language models to generate high-quality embeddings. A significant amount of research has been devoted to refining the contrastive learning framework in order to further improve sentence embeddings~\cite{chuang2022diffcse, wu2022pcl, wu-etal-2022-infocse, cheng2023improving}.

% background
Recently, large language models (LLMs), such as GPT-3~\cite{gpt3} and LLaMA~\cite{touvron2023llama}, have demonstrated significant potential on various natural language processing tasks such as translation, question answering, and text classification.
Current research has also explored the application of LLMs for data augmentation in sentence embeddings task. By generating better sentence pairs for contrastive learning, LLMs can help alleviate the scarcity of labeled data~\cite{cheng2023improving, zhang2023contrastive}. 
However, directly utilizing LLMs for generating sentence embeddings poses two primary challenges. Firstly, LLMs, as autoregressive models, produce text instead of vectors, which necessitates vectorizing the output. Secondly, it is crucial to determine an effective approach for incorporating the capabilities of in-context learning into sentence embedding tasks.

In this work, we aim to investigate the capabilities of current LLMs for sentence embeddings, facilitated by the availability of open-source LLMs~\cite{touvron2023llama, zhang2022opt}. We address the following research questions: 1) How can LLMs be used to represent sentence embeddings, and does prompt engineering, as demonstrated by PromptBERT~\cite{jiang2022promptbert}? 2) Can in-context learning~\cite{liu2023pre} enhance the quality of sentence embeddings? 3) Do the scaling laws~\cite{kaplan2020scaling} still work when the number of parameters increases from millions to tens of billions? 4) What improvements can be achieved by incorporating the current contrastive learning framework into LLMs?


To address these questions, we conduct a systematic study by evaluating LLaMA~\cite{touvron2023llama} and OPT~\cite{zhang2022opt} on both semantic textual similarity (STS) tasks~\cite{cer2017semeval} and transfer tasks\textcolor{red}{add more citations}. Following~\cite{jiang2022promptbert}, we utilize a prompt such as \textit{This sentence: ``} \texttt{[text]} \textit{'' means} to enable LLMs to generate sentence embeddings, where \texttt{[text]} serves as the input slot. This method outperforms traditional representation methods, such as averaging output tokens to represent sentences.
Considering the uni-direction architecture and pretraining tasks of LLMs compared to BERT, we can refine the prompt to generate better representations by instructing LLMs to encapsulate as much semantic information of the sentences as possible within the target token. Inspired by~\cite{tsukagoshi-etal-2021-defsent}, which uses definition sentences from a word dictionary to learn sentence embeddings, we find that performance can be further improved by adding definition sentences and corresponding words as examples to guide LLMs.

To mitigate the gap between examples of in-context learning and inputs, we also use sentences from the STS-B~\cite{cer2017semeval} training set as examples by instructing ChatGPT to generate a single word to represent the meaning of sentences. By evaluating the demonstration examples based on the STS-B development set, LLMs can outperform previous contrastive learning-based sentence models, which were fine-tuned on unsupervised data.

By scaling up the parameters of LLMs, we find that scaling laws do not apply to STS tasks, when parameters exceed several billions. However, they still work for transfer tasks, and LLaMA with 65B parameters can achieve state-of-the-art performance without fine-tuning. With the advancement of parameter-efficient fine-tuning techniques\cite{hu2021lora, dettmers2023qlora} and post-training quantization methods\cite{frantar2022gptq}, we can fine-tune LLMs with large batch sizes to conduct contrastive learning, even with limited computational resources.
For instance, fine-tuning 7B parameter LLMs can be accomplished using the same hardware employed for previous BERT-based models like SimCSE~\cite{gao2021simcse}. Even without fine-tuning the full parameters and using the 4-bit quantized method~\cite{dettmers2023qlora}, a 2.7B parameter OPT with our instruct sentence representation outperforms a 4.8B parameter ST5~\cite{sentencet5} and achieves state-of-the-art results on STS tasks. By increasing the parameters, the best results can be obtained using 7B parameter LLMs.

Our main contributions are as follows:
\begin{enumerate}
    \item We propose a novel sentence representation method that leverages LLMs to enhance the representation of sentences. Additionally, we incorporate in-context learning to further improve the quality of sentence embeddings. Our approach demonstrates that LLMs can generate high-quality sentence embeddings without the need for fine-tuning.
    \item We conduct an analysis of scaling up the parameters of LLMs from millions to tens of billions in sentence embeddings and observe that when the parameters of LLMs reach a certain threshold, further scaling up does not lead to an improvement in the performance of sentence embeddings.
    \item Based on our sentence representation method, we discover that performance can be further enhanced by employing contrastive learning. By adopting efficient fine-tuning techniques, LLMs achieve state-of-the-art performance on STS tasks, even with limited computational resources.
\end{enumerate}

\section{Related Work}

\paragraph{Sentence embeddings}
Sentence embedding task is to convert a sentence into a fixed-size vector, which captures the semantic meaning and context of the sentence. 
It allows for the efficient retrieval of similar sentences through the similarity between vectors.
Recently, SimCSE~\cite{gao2021simcse} demonstrated that contrastive learning is an effective approach for learning sentence embeddings using BERT in both unsupervised and supervised settings. In the unsupervised setting, SimCSE predicts the input sentence itself from in-batch negatives, with different dropout~\cite{srivastava2014dropout} masks applied.
In the supervised setting, natural language inference (NLI) datasets~\textcolor{red}{add citation} are used to provide positive and negative pairs.
Following the success of SimCSE, there has been a surge of work exploring contrastive learning-based methods. DiffCSE~\cite{chuang2022diffcse} incorporates a replaced token detection loss into the contrastive learning framework. PromptBERT~\cite{jiang2022promptbert} reveals that prompts can enhance BERT's ability to represent sentences. RankEncoder~\cite{seonwoo2022ranking} uses neighboring sentences to capture fine-grained semantic differences. Additionally, several studies~\cite{cheng2023improving, zhang2023contrastive} have investigated data augmentation for sentence embeddings using LLMs.
SentenceT5~\cite{sentencet5} leverages the encoder-decoder structure of models, such as T5~\cite{raffel2020exploring}, for generating sentence embeddings and demonstrates improvements by scaling T5 from millions to billions of parameters. However, directly using large language models (LLMs) for generating sentence embeddings remains an area of ongoing research.

\paragraph{Large Language Models}
LLMs~\cite{zhang2022opt,bloom,chowdhery2022palm,touvron2023llama} recently show impressive performance on various natural language process, benefiting from their large parameter sizes compared to previous pretrained language models.
LLMs can efficiently learn a new task with in-context learning by using training data as demonstrations~\cite{gpt3}.
Without any gradient updates, LLMs with in-context learning can solve challenging tasks like multitask language understanding~\cite{hendrycks2020measuring}, commonsense reasoning~\cite{lin2021truthfulqa}, and math problems~\cite{cobbe2021training}.
This performance can be further improved by scaling up language models~\cite{hoffmann2022training, kaplan2020scaling}.
In this work, we use OPT~\cite{zhang2022opt} and LLaMA~\cite{touvron2023llama} to investigate the generation of sentence embeddings.

\section{Methodology}
In this section, we first discuss current sentence representation methods with LLMs, and then introduce a new Prompt-based method with Explicit One word Limitation (PromptEOL) \textcolor{red}{(You didn't introduce this method or mention this phrase in methodology subsections)} for LLMs in Section~\ref{sec:represent_llm}.
Based on this method, we describe two settings: without and with fine-tuning.
For the setting without fine-tuning,
we utilize the in-context learning ability of LLMs to enhance sentence embeddings.
To address the issue of lacking textual outputs, we propose two methods to automatically generate demonstrations for in-context learning in Section~\ref{sec:method_icl}.
For the setting with fine-tuning, we employ contrastive learning framework, and combine it with the efficient fine-tuning method to alleviate substantial memory requirement in Section~\ref{sec:contrastive_method}.
%First method is gradient free by leveraging in-context learning to help LLMs represent sentence.
%We focus on how to build demonstration set since sentence embeddings lack textual outputs.
%Second method is based on contrastive learning framework by fine-tuning LLMs.
%To overcome the

\subsection{Represent Sentence with LLMs}
\label{sec:represent_llm}
Previous works~\cite{li2020sentence, su2021whitening, jiang2022promptbert} have extensively studied on improving sentence embeddings from encoder-based pretrained models (such as BERT) without fine-tuning.
Recently, PromptBERT~\cite{jiang2022promptbert} introduced a prompt-based method to represent sentence.
It uses manual templates like \textit{This sentence: ``} \texttt{[text]} \textit{'' means} \texttt{[MASK].}, where \texttt{[text]} is a placeholder for a sentence. The output vector of \texttt{[MASK]} token is used as sentence representation.
It demonstrates superior results compared to traditional sentence representation methods like averaging output hidden vectors or using the output vector of \texttt{[CLS]} token.

\begin{wrapfigure}[20]{r}{7cm}%右侧
\centering
\vspace{-10pt}
	% Figure removed
\caption{
  Performances of OPT in STS-B development set with three representation methods.
  Dash line represents the results of BERT. %\hytt{bert-base-uncased} with 110M parameters.
}
\label{fig:LLM_rep}
\end{wrapfigure}

Considering to LLMs as autoregressive models, which do not have special tokens like \texttt{[CLS]} or \texttt{[MASK]}, we modify the prompt-based method in~\cite{jiang2022promptbert} to make it compatible with LLMs.
We use \textit{This sentence: ``} \texttt{[text]} \textit{'' means} to prompt LLMs to generate next token and extract the hidden vectors of the final token as sentence embeddings.
To validate the prompt-based method with LLMs, we compare it with two other methods, such as averaging or using the last token as sentence embeddings.
For LLMs, we use OPT~\cite{zhang2022opt} from 125 million to 66 billions and evaluate it on STS-B development set in Figure~\ref{fig:LLM_rep}.
Following the results in~\cite{jiang2022promptbert}, we observe that our prompt-based method can enhance sentence representation across all OPTs, ranging from millions to billions parameters.
\textcolor{red}{Despite the improvements observed with the prompt-based method for LLMs such as OPT, it is important to note that these OPT models still fail to outperform the BERT model using the same prompt-based method. }
%============== Ting =======================
% Although both BERT and OPT represent sentence embedding out of box without any gradient updates, OPT fails to outperform BERT with prompt-based method.

Consider to bidirectional attention in BERT, we hypothesize that BERT can implicitly condense the entire semantic information corresponding to a sentence into a single \texttt{[MASK]} token when using templates like ``\textit{This sentence: ``} \texttt{[text]} \textit{'' means} \texttt{[MASK].}''.
Since the \texttt{[MASK]} token follows a period, this implicitly restricts BERT to condense the meaning into one word. 
However, this template fails to add the similar ``one word limitation'' when it is used in autoregressive models like OPT with unidirectional attention.
To validate this, we simply remove the period in template to transfer it into ``\textit{This sentence: ``} \texttt{[text]} \textit{'' means} \texttt{[MASK]}''.
Despite only one word difference, and no modification to meaning of the template, the performance of BERT on STS-B development set plummeted from 73.44 to 33.89 Spearman correlation, which means BERT without this implicit ``one word limitation'' fails to represent sentence. \textcolor{red}{this (73->33)experiments have been shown in PromptBERT? right? please cite PromptBERT}

Inspired by this, our objective is to enhance prompt-based method for LLMs by introducing a ``one word limitation''.
%To achieve it, we find simply add some tokens in prompt to instruct LLMs to predict the meaning of sentence in one word.
Our method is simple and straightforward by adding some tokens in the template to instruct LLMs in predicting the meaning of sentence in one word.
The template we used after modification is following:

\centerline{\textit{This sentence: ``} \texttt{[text]} \textit{'' means in one word: ``}}

Compared to the template in~\cite{jiang2022promptbert}, we introduce two simple modifications for LLMs.
First, we append \textit{in one word} to the prompt to constrain LLMs in predicting semantic information in next token.
Secondly, we incorporate \textit{: ``} at the end of template to prevent model form generating punctuations in next token, as \textit{This sentence: ``} is used to indicate the input of a sentence.
We find this template improve all OPT models and allow them to match or even outperform BERT with prompt-based method in Table~\ref{tab:sts_wo_ft}.

% Figure environment removed

\subsection{Improve Sentence Representation with In-Context Learning}\label{sec:method_icl}
In-context learning is widely utilized as an effective method to help LLMs understand problems.  It improves their comprehension of inputs and outputs by directly adding a few examples in the prompts.
However, when considering the problem of sentence embeddings, we need to project sentences into vectors based on their semantic information, separately.
In other word, sentence embeddings lack textual outputs that could be used as examples to perform in-context learning, such as answers for question answer problems or labels for text classification problems. Moreover, there are also no predetermined gold vectors for a given sentence.

To leverage in-context learning in sentence representation, we propose an framework to automatically build demonstration sets and search demonstration to improve LLMs sentence embeddings in Figure~\ref{fig:framework}.
For the demonstration set, the goal is to create sentence and word pairs, where the word can represents the semantic information of the sentence.
We propose two methods to generate pairs.

The first method involves using ChatGPT to generate corresponding words according to the semantic information of given sentences. We randomly select some sentences from the STS-B training set and use the same template as shown in~\ref{fig:framework}, ChatGPT outputs a one-word summary for the given sentence.
As observed in Section~\ref{sec:represent_llm}, the "one-word limitation" is crucial for ChatGPT. 
Removing \textit{in one word} from the template, ChatGPT tends to explain the meaning of a sentence in a lengthy way, and the first word often becomes an article such as ``The'', which lacks clear meaning.
In our prompt-based sentence representation, we employ the hidden state of the next token as the sentence embeddings.
For example, given the sentence ``A jocky riding a horse.'', the hidden state achieves the highest dot product similarity for ``Equestrain'' among its word embeddings.
However, without ``one word limitation'', it will achieve the highest dot product similarity for word without specific meaning such as ``The'' among its word embeddings, which can not represent sentence properly.

Inspired by DefSent~\cite{tsukagoshi-etal-2021-defsent}, which leverages definition sentences with their words as labels to train unsupervised sentence embedding,
our second method is also based on a word dictionary. We directly use words and their definition sentences in the Oxford dictionary as word-sentence pairs.

%\begin{wrapfigure}{r}{7cm}%右侧
%\centering
%%\vspace{-13pt}
%	% Figure removed
%\caption{
%  Performance of OPT in STS-B development set with three representation methods.
%}
%\label{fig:LLM_icl}
%\end{wrapfigure}

Based on these methods, we construct a demonstration set consisting of 300 pairs of sentences and words.
100 pairs are from STS-B training set, with words labeled by ChatGPT, while the remaining are from the Oxford dictionary.
To find demonstration that help model to represent sentences, we directly evaluate each demonstration on the STS-B development set and use the demonstration with the best Spearman correlation as the demonstration for corresponding models.
We also visualize the distribution of Spearman correlations for OPT from 125M to 66B parameters in Figure~\ref{fig:LLM_icl_hist}.
Following the previous study~\cite{kaplan2020scaling}, we notice that in-context learning achieves better performance, when increasing model parameter from 125M to 2.7B.
For example, there are only one demonstration that helps the 125M parameters OPT achieve better performance compared to without demonstration.
However, around 98\% of demonstrations improve the performance of the 2.7B parameters OPT.
In-context learning significantly enhance the sentence representation, especially for OPT with more than 1B parameters.
With only in-context learning, OPT with more than 1.3B parameters even achieve better results on STS-B development set compared to contrastive learning based method like SimCSE~\cite{gao2021simcse}.

% Figure environment removed


\subsection{Contrastive Learning with Efficient Fine-tuning}
\label{sec:contrastive_method}

Since in-context learning boosts LLMs' performances on sentence representation without any gradient update,
we also exploit contrastive learning on LLMs, which has been demonstrated as an efficient way to learn sentence representation~\cite{gao2021simcse}.
Contrastive learning can be divided into unsupervised and supervised settings, depending on the datasets.
For unsupervised setting, the sentences in dataset lack corresponding positive and negative sentences to perform contrastive learning.
For supervised settings, natural language inference (NLI) datasets are used as the datasets, and each sentence has corresponding positive and negative sentences.
In this section, we mainly focus on supervised settings.

However, contrastive learning requires a large batch size to increase the number of negative samples, which demands a high amount of GPU memory, especially in the supervised setting.
For example, SimCSE uses a batch size of 512 to fine-tune BERT in the supervised setting, which has 110M parameters. Each batch includes 1536 sentences, containing both their positive and hard negative sentences. It requires 58GB of GPU memory when utilizing 4 parallel GPUs.
As a result, fine-tuning LLMs with contrastive learning becomes challenging due to the memory requirements, particularly for models with significantly larger parameter sizes than BERT. 
% Thus, it would be unafforded for us to fine-tune LLMs, which can have more than hundreds of times parameters compared to BERT.

To solve this problem, we leverage current efficient fine-tuning method QLoRA~\cite{dettmers2023qlora}.
QLoRA combines two techniques to significantly reduces memory usage: 4-bit quantization and parameter efficient fine-tuning.
Quantization reduces the memory usage of LLMs by quantizing their weight from 16-bit to 4-bit.
Parameter efficient fine-tuning with LoRA~\cite{hu2021lora} significantly reduces the memory usage of optimizer compared to full fine-tuning by only fine-tuning small proportion of weight.

Following~\cite{gao2021simcse}, we use SNLI and MNLI datasets where each sentence $x_i$ has corresponding a positive sentence $x_i^{+}$ and a hard negative sentence $x_i^{-}$. To represent sentence, we use our prompt based method in Section~\ref{sec:represent_llm}.
Formally, given sentence $x_i$, we first add $x_i$ to the template and get hidden states:

\begin{equation}
\begin{split}
  \mathbf{h}_{i1}, \ldots, \mathbf{h}_{il} = {\rm LLM}(\textit{This  sen}&\textit{tence: ``} x_i \textit{'' means in one word: ``})\\
  %h &= h_L
\end{split}
\end{equation}

where $l$ is the number of hidden states. We then use last token hidden state as its sentence embedding $\mathbf{h}_{i} = \mathbf{h}_{il}$.
Since we can represent the sentence pair $(x_i, x_i^{+}, x_i^{-})$ to their embeddings $(\mathbf{h}_i, \mathbf{h}_i^{+}, \mathbf{h}_i^{-})$.
Our training objective is following:

\begin{equation}
\ell_{i}=-\log \frac{e^{\operatorname{cos}\left(\mathbf{h}_i, \mathbf{h}_i^{+}\right) / \tau}}{\sum_{j=1}^N\left(e^{\operatorname{cos}\left(\mathbf{h}_i, \mathbf{h}_j^{+}\right) / \tau}+e^{\operatorname{cos}\left(\mathbf{h}_i, \mathbf{h}_j^{-}\right) / \tau}\right)}
\end{equation}

where $N$ is the batch size and \(\tau\) is the temperature hyperparameter in contrastive learning.


\section{Experiment}
\subsection{Implementation Details}
For the setting without fine-tuning, we use OPT from 125M to 66B parameters, and LLaMA from 7B to 65B parameters.
All models use the same template in Section~\ref{sec:represent_llm}.
 We use 300 pairs of sentences and words as demonstration set for in-context learning. % 100 pairs are from STS-B training set, and we use gpt-3.5-turbo to label their words. The remaining 200 pairs from oxford dictionary.
 Among these, 100 pairs are from the STS-B training set, and we use \hytt{gpt-3.5-turbo} to label their words. The remaining 200 pairs are from the Oxford dictionary. We provide all demonstrations in Appendix~\ref{apx:demo}.
For each model, we choose only one demonstration that has the highest Spearman correlation on the STS-B development set as their demonstration for evaluation.
Our results from models with 16-bit weights. We also present results using quantization methods in Appendix~\ref{apx:quant}.

For the setting with fine-tuning,
we use QLoRA~\cite{dettmers2023qlora} to fine-tune OPT and LLaMA with contrastive learning.
Following QLoRA, we use LoRA $r=64, \alpha=16$, dropout $=0.05$, and add LoRA modules on all linear layers of the 4-bit quantized model.
We fine-tune models on the NLI datasets~\cite{gao2021simcse} with one epoch, temperature \(\tau = 0.5\) and learning rate 5e-4.
Due to hardware limitations, we only conduct our experiments with model parameters less than or equal to 13B with 8 RTX-3090 GPUs.
For models with fewer than 7B parameters, we fine-tune them on 2 GPUs with a batch size of 256. For 7B models, we use 4 GPUs with a batch size of 256. For 13B models, we use 8 GPUs with a batch size of 200.
%For models with less than 7B parameters, we fine-tune them on 2 GPUs with 256 batch size. For 7B models, we use 4 GPUs with 256 batch size.For 13B models, we use 8 GPUs with 200 batch size.
%We find contrastive learning with our

\subsection{Dataset}
Following previous works~\cite{gao2021simcse,  jiang2022promptbert},
We use the SentEval toolkit~\cite{conneau2018senteval} to conduct our experiments on seven STS datasets and seven transfer learning datasets.
The STS datasets include STS tasks 2012-2016~\cite{agirre2012semeval, agirre2013sem, agirre2014semeval, agirre2015semeval, agirre2016semeval} STS-B~\cite{cer2017semeval}, SICK-R~\cite{marelli2014sick}. Sentence pairs in each STS dataset are scored from 0 to 5 to indicate semantic similarity.
Spearman correlation is used as a metric to evaluate the correlation between the cosine similarity of sentence embeddings  and the golden similarity scores.
The transfer learning datasets include MR~\cite{pang2005seeing_mr}, CR~\cite{hu2004mining_cr}, SUBJ~\cite{pang2004sentimental_subj}, MPQA~\cite{wiebe2005annotating_mpqa}, SST-2~\cite{socher2013recursive_sst-2}, TREC~\cite{voorhees2000building_trec} and MRPC~\cite{mrpc2005}.
Sentence embeddings are used as input feature to train corresponding logistic regression classification.

\begin{table*}[t]
\vspace{-13pt}
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STS-B} & \textbf{SICK-R} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on unsupervised datasets}}\\
\midrule
SimCSE-BERT\(^\dagger \)  & 110M & 68.40 & 82.41 & 74.38 & 80.91 & 78.56 &    76.85     &      72.23      & 76.25 \\
SimCSE-RoBERTa\(^\dagger \) & 123M & 70.16 &  81.77 & 73.24 & 81.36 & 80.65 & 80.22 & 68.56 & 76.57 \\
PromptBERT\(^\ddagger \)  & 110M & 71.56 & 84.58 & 76.98 & 84.47 & 80.60 &  81.60 & 69.87& 78.54 \\
PromptRoBERTa\(^\ddagger \) & 123M &  73.94 & 84.74 & 77.28 & 84.99 & 81.74 & 81.88 & 69.50 & 79.15 \\
\midrule
\midrule
\multicolumn{10}{c}{\it{Without fine-tuning}}\\
\midrule
BERT avg.\(^\dagger \) & 110M & 30.87 & 59.89 & 47.73 & 60.29 & 63.73 & 47.29 & 58.22 & 52.57 \\
%BERT last-first avg.\(^\dagger\) & 110M & 39.70 & 59.38 & 49.67 & 66.03 & 66.19 & 53.87 & 62.06 & 56.70\\
%BERT-flow \(^\dagger\) & 58.40 & 67.10 & 60.85 & 75.16 & 71.22 & 68.66 & 64.47 & 66.55 \\
%BERT-whitening  \(^\dagger\) & 57.83 & 66.90 & 60.90 & 75.08 & 71.31 & 68.24 & 63.73 & 66.28 \\
BERT prompt\(^\ddagger \) & 110M & 60.96  & 73.83 & 62.18 & 71.54 & 68.68 & 70.60 & 67.16 & 67.85 \\
ST5-Enc\(^\S\) & 4.8B & 34.97 & 60.19 & 47.59 & 66.40 & 70.62 & 62.83 & 63.57 & 58.02 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
& 125M & 59.90 & 71.55 & 60.93 & 70.76 & 72.83 & 67.89 & 65.14 & 67.00 \\
& 350M & 54.70 & 71.52 & 59.99 & 64.51 & 71.39 & 66.55 & 66.58 & 65.03 \\
& 1.3B\cellcolor{lightcyan} & 64.59\cellcolor{lightcyan} & 79.06\cellcolor{lightcyan} & 68.46\cellcolor{lightcyan} & 78.88\cellcolor{lightcyan} & 78.64\cellcolor{lightcyan} & 73.22\cellcolor{lightcyan} & 69.41\cellcolor{lightcyan} & 73.18\cellcolor{lightcyan} \\
& 2.7B & 60.03 & 75.51 & 64.30 & 74.56 & 77.62 & 67.73 & 65.35 & 69.30 \\
& 6.7B & 60.91 & 80.05 & 67.65 & 75.49 & 80.11 & 72.91 & 67.57 & 72.10 \\
& 13B &  60.21 & 81.36 & 69.69 & 75.46 & 79.58 & 70.73 & 65.99 & 71.86 \\
& 30B &  59.99 & 80.52 & 69.80 & 75.20 & 78.03 & 73.57 & 69.87 & 72.43\\
& 66B &  55.66 & 74.62 & 64.90 & 72.34 & 75.21 & 71.72 & 67.43 & 68.84 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL+ICL\\OPT}
 &  125M& 62.22 & 73.10 & 61.84 & 71.09 & 72.08 & 67.80 & 64.10 & 67.46 \\
 &  350M& 63.87 & 73.85 & 63.41 & 72.45 & 73.13 & 70.84 & 65.61 & 69.02 \\
 &  1.3B& 72.78 & 83.77 & 73.61 & 83.42 & 80.60 & 78.80 & 69.69 & 77.52 \\
 &  2.7B& 68.49 & 84.72 & 75.15 & 83.62 & 81.34 & 80.94 & 72.97 & 78.18 \\
 &  6.7B\cellcolor{lightcyan}& 70.65\cellcolor{lightcyan} & 84.51\cellcolor{lightcyan} & 75.01\cellcolor{lightcyan} & 83.51\cellcolor{lightcyan} & 82.00\cellcolor{lightcyan} & 81.12\cellcolor{lightcyan} & 76.77\cellcolor{lightcyan} & 79.08\cellcolor{lightcyan} \\
 &  13B & 71.99 & 85.22 & 76.04 & 82.23 & 81.38 & 81.42 & 75.00 & 79.04 \\
 &  30B & 69.99 & 83.35 & 74.75 & 83.14 & 82.42 & 81.45 & 77.46 & 78.94 \\
 &  66B & 69.93 & 83.29 & 74.88 & 80.10 & 81.11 & 81.76 & 76.26 & 78.19 \\

\bottomrule
\end{tabular}
}
\caption{ Performances of our method on STS tasks without fine-tuning. ICL denotes in-context learning with our demonstration set.
  %Avg. denotes averaging the last layer output of the model.
  %Prompt denots using the template from~\cite{jiang2022promptbert}.
  \(\dagger\): results from~\cite{gao2021simcse}.
  \(\ddagger \): results from~\cite{jiang2022promptbert}.
  \(\S\): results from~\cite{sentencet5}.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
}\label{tab:sts_wo_ft}
\vspace{-10pt}
\end{table*}

\subsection{Results}
We compare our method with BERT-based methods such as SBERT~\cite{reimers2019sentence}, SimCSE~\cite{gao2021simcse}, and PromptBERT~\cite{jiang2022promptbert}. In addition, we include other sentence methods based on LLMs as baselines, such as ST5~\cite{sentencet5} and SGPT~\cite{muennighoff2022sgpt}.
Among these baselines, ST5 achieves state-of-the-art results on both STS and transfer learning tasks by further fine-tuning 4.8B parameters T5 encoder  with contrastive learning.




\begin{table*}[t]
\vspace{-13pt}
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STS-B} & \textbf{SICK-R} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on supervised datasets}}\\
\midrule
SBERT-NLI\(^\dagger \) & 220M & 72.27 & 78.46 & 74.90 & 80.99 & 76.25 & 79.23 & 73.75 & 76.55 \\
\multirowcell{2}{SimCSE-RoBERTa\(^\dagger \)} & 123M & 76.53 & 85.21 & 80.95 & 86.03 & 82.57 & 85.83 & 80.50 & 82.52 \\
 & 354M & 77.46 & 87.27 & 82.36 & 86.66 & 83.93 & 86.70 & 81.95 & 83.76 \\
PromptRoBERTa\(^\ddagger \) & 123M & 76.75 & 85.93 & 82.28 & 86.69 & 82.80 & 86.14 & 80.04 & 82.95 \\
SGPT\(^\mathparagraph\) & 5.8B & 74.28 & 85.35 & 79.21 & 85.52 & 82.54 & 85.50 &  79.53 & 81.70 \\
ST5-Enc\(^\S\) & 4.8B & 80.10 & 88.75 & 84.70 & 88.86 & 85.17 & 86.77 & 80.39 & 84.96 \\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL+CSE\\OPT}
 & 1.3B & 79.01 & 89.26 & 84.10 & 88.30 & 84.62 & 87.71 & 80.52 & 84.79\\
 & 2.7B & 79.49 & 89.64 & 84.80 & 89.51 & 85.91 & 88.33 & 81.64 & 85.62\\
 & 6.7B & 80.14 & 90.02 & 84.94 & 89.78 & 85.84 & 88.75 & 81.29 & 85.82\\
 &\cellcolor{lightcyan}13B &\cellcolor{lightcyan}80.20 &\cellcolor{lightcyan}90.24 &\cellcolor{lightcyan}85.34 &\cellcolor{lightcyan}89.52 &\cellcolor{lightcyan}85.90 &\cellcolor{lightcyan}88.56 &\cellcolor{lightcyan}82.06 &\cellcolor{lightcyan}85.97\\
\midrule
\multirowcell{2}[0pt][l]{PromptEOL+CSE\\LLaMA}
%& 13B &
& 7B & 79.16 & 90.22 & 85.40 & 88.99 & 86.25 & 88.37 & 81.51 & 85.70 \\
&\cellcolor{lightcyan}13B &\cellcolor{lightcyan}78.63 &\cellcolor{lightcyan}90.03 &\cellcolor{lightcyan}85.46 &\cellcolor{lightcyan}89.48 &\cellcolor{lightcyan}86.18 &\cellcolor{lightcyan}88.45 &\cellcolor{lightcyan}82.69 &\cellcolor{lightcyan}85.85 \\
\bottomrule
\end{tabular}
}
\caption{ Performances of our method on STS tasks with fine-tuning. CSE denotes contrastive learning for sentence embeddings.
  \(\dagger\): results from~\cite{gao2021simcse}.
  %\(\ddagger \): results from~\cite{jiang2022promptbert}.
  \(\S\): results from~\cite{sentencet5}.
  \( \mathparagraph \): results from evaluation the public checkpoint~\cite{muennighoff2022sgpt} on STS tasks.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
}\label{tab:sts_w_ft}
\end{table*}


\begin{table*}[t]
\vspace{-8pt}
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
%\textbf{Method}\;\;\;\;\;\;\;\;\;\;\;\; & \textbf{Params} & \ \textbf{MR}\quad & \ \textbf{CR}\quad & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\textbf{Method}\quad\quad\quad\quad\quad & \quad\quad\textbf{Params}\quad\quad & \textbf{MR} & \textbf{CR} & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on supervised datasets}}\\
\midrule
\multirowcell{2}{SimCSE-RoBERTa\(^\dagger \)} & 123M & 84.92 & 92.00 & 94.11 & 89.82 & 91.27 & 88.80 & 75.65 & 88.08 \\
 & 220M & 81.12 & 92.37 & 95.11 & 90.49 & 92.75 & 91.80 & 76.64 & 89.61 \\
PromptRoBERTa\(^\ddagger \) & 123M & 85.74 & 91.47 & 94.81 & 90.93 & 92.53 & 90.40 & 77.10 & 89.00\\
ST5-Enc^\(\S\) & 4.8B & 90.83 & 94.44 & 96.33 & 91.68 & 94.84 & 95.40 & 77.91 & 91.63 \\
\midrule
\midrule
\multicolumn{10}{c}{\it{Without fine-tuning}}\\
\midrule
%SimCSE-RoBERTa\(^\dagger \) & 220M & 881.12 & 92.37 & 95.11 & 90.49 & 92.75 & 91.80 & 76.64 & 89.61 \\
%SGPT & 5.8B & 74.28 & 85.35 & 79.21 & 85.52 & 82.54 & 85.50 &  79.53 & 81.70 \\
%OPT avg. & 66B & 89.17 & 92.00 & 96.86 & 90.80 & 94.67 & 96.40 & 71.07 & 90.14\\
%OPT prompt & 66B & 89.15 & 91.90 & 97.17 & 91.32 & 93.96 & 96.20 & 71.42 & 90.16\\
%LLaMA avg. & 65B & 89.59 & 91.97 & 97.64 & 89.94 & 93.74 & 97.00 & 72.17 & 90.29 \\
%LLaMA prompt & 65B  & 89.73 & 91.58 & 97.65 & 90.58 & 94.40 & 96.80 & 69.33 & 90.01 \\
BERT avg. & 110M & 78.66 & 86.25 & 94.37 & 88.66 & 84.40 & 92.80 & 69.54 & 84.94 \\
ST5-Enc^\(\S\) & 4.8B & 91.15 & 93.33 & 97.55 & 90.20 & 94.07 & 94.40 & 74.26 & 90.71 \\
\midrule
\multirowcell{6}[0pt][l]{PromptEOL\\OPT}
 %&  125M & 80.86 & 87.66 & 93.19 & 89.77 & 87.31 & 92.20 & 72.64 & 86.23 \\
 %&  350M & 84.14 & 88.08 & 93.17 & 89.77 & 89.73 & 91.20 & 71.36 & 86.78 \\
 &  1.3B & 88.06 & 91.55 & 95.90 & 91.55 & 93.08 & 95.00 & 73.97 & 89.87 \\
 &  2.7B & 88.83 & 92.29 & 95.93 & 91.76 & 94.62 & 96.00 & 75.94 & 90.77 \\
 &  6.7B & 90.26 & 92.50 & 96.67 & 91.39 & 94.67 & 96.00 & 77.91 & 91.34 \\
 &  13B  & 90.73 & 92.90 & 96.69 & 91.48 & 94.01 & 96.80 & 75.59 & 91.17 \\
 &  30B  & 90.95 & 92.77 & 96.99 & 91.79 & 95.28 & 97.00 & 73.97 & 91.25 \\
 &  66\cellcolor{lightcyan}B  & 90.96\cellcolor{lightcyan} & 93.40\cellcolor{lightcyan} & 97.01\cellcolor{lightcyan} & 91.93\cellcolor{lightcyan} & 95.22\cellcolor{lightcyan} & 96.40\cellcolor{lightcyan} & 75.25\cellcolor{lightcyan} & 91.45\cellcolor{lightcyan} \\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL\\LLaMA}
& 7B & 90.40 & 92.90 & 96.88 & 91.57 & 95.11 & 95.40 & 75.13 & 91.06 \\
& 13B & 92.02 & 93.22 & 97.29 & 91.40 & 95.66 & 95.80 & 76.46 & 91.69 \\
& 30B & 91.64 & 93.27 & 97.10 & 91.86 & 95.99 & 95.80 & 78.43 & 92.01 \\
& 65B\cellcolor{lightcyan} & 92.13\cellcolor{lightcyan} & 93.43\cellcolor{lightcyan} & 97.16\cellcolor{lightcyan} & 91.91\cellcolor{lightcyan} & 95.33\cellcolor{lightcyan} & 97.40\cellcolor{lightcyan} & 77.28\cellcolor{lightcyan} & 92.09\cellcolor{lightcyan} \\
\bottomrule
\end{tabular}}
\caption{ Performances of our method on transfer learning tasks.
  \(\dagger\): results from~\cite{gao2021simcse}.
  \(\ddagger \): results from~\cite{jiang2022promptbert}.
  \(\S\): results from~\cite{sentencet5}.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
}\label{tab:tran_w_ft}
\vspace{-15pt}
\end{table*}

\textbf{STS tasks without fine-tuning}
Table~\ref{tab:sts_wo_ft} shows the results of PromptEOL with and without in-contex learning on STS tasks.
Even without corresponding textual outputs for sentence embeddings, in-context learning still helps model to generate better embeddings.
As the model size grows, improvements from in-context learning also increase.
Moreover, in-context learning shows significantly improvements on STS tasks for model with more than billions parameters.
For instances, it raises the Spearman correlation from 68.84 to 78.19 on 66B OPT.
Our method with in-context learning also outperforms among methods without fine-tuning.
%Even these modeles are only pretrained according to next token prediction task, without any
Even if we do not use any method to avoid anisotropy~\cite{ethayarajh2019contextual}, which is widely regarded as the main reason for poor performance on STS tasks~\cite{gao2021simcse, sentencet5}, our method still outperforms unsupervised methods such as SimCSE and PromptBERT, which use contrastive learning to avoid anistoropy.
Additionally, we find the performance is not sensitive to the model size while scaling model beyond a billion parameters.
Smaller models, such as 1.3B OPT, even outperforms SimCSE without fine-tuning.

\textbf{STS tasks with fine-tuning}
Table~\ref{tab:sts_w_ft} shows the results by fine-tuning with PromptEOL on the supervised dataset.
%We use the same dataset with SimCSE and PromptBERT.
Compared to ST5-Enc, which fine-tuned all 4.8B parameters on Community QA and NLI datasets,
our method with 2.7B OPT achieves superior results through parameter-efficient fine tuning on the 4-bit model with only NLI datasets.
%we find that our method by fine-tuning 2.7B OPT with parameter-efficient tuning and 4-bit quantization can surpass it.
Keep scaling up the parameters size, 13B OPT and LLaMA achieve the best performance on STS tasks.
However, the improvement in scaling model parameters from 2.7B to 13B is not significant.
%Due to hardware limitations, we are unable to fine-tune it with
%Each models can be trained in several hours on RTX 3090. We report training time in Appendix~\ref{apx:train_time}.


\textbf{Transfer tasks}
We also report the results of our method on the transfer learning tasks in Table~\ref{tab:tran_w_ft}.
Unlike STS tasks, we observe that LLMs achieve better performance as the model size increases.
Specifically, the 66B OPT and 65B LLaMA models outperform their smaller counterparts with our representation method.
Based on our representation method, LLMs show good performance without in-context learning and contrastive learning.
Following ST5~\cite{sentencet5}, we find that applying contrastive learning solely on NLI datasets can even harm performance on transfer tasks.
To solve this problem, ST5 utilizes additional datasets, such as the Community QA dataset, to enhance its performance in transfer tasks.
For in-context learning, as it is widely used in text classification,
we find that using examples not relevant to tasks, such as STS-B or the dictionary, does not enhance transfer task performance.
We present these results in Appendix~\ref{apx:transfer_task}.



\section{Analysis}
\subsection{Sentence Representation Methods}
We present the results obtained using three sentence representation methods, across models ranging in size from 125M to 66B parameters, as shown in Figure~\ref{fig:PromptEOL_rep}. Different representation methods can yield significantly different results.
Prompt based methods outperform direct averaging in three settings.
Among these methods, PromptEOL exhibits the best performance, as it introduces an explicit ``one-word limitation''.
More detail results can be find in Appendix~\ref{apx:sentence_rep}.

% Figure environment removed

\subsection{In-context Learning}

\begin{wraptable}[10]{r}{7cm}
\tiny
\centering
\vspace{-18pt}
\begin{tabular}{lp{2.8cm}cc}
\hline
& Sentence & Word & Improve\\
\hline
125M & A man is smoking. & Smoking & 0.46\\
350M & A man is playing on a guitar and singing. & Music & 3.99\\
1.3B & relating to switzerland or its people. & Swiss & 4.34\\
2.7B & A jockey riding a horse. & Equestrian & 8.88\\
6.7B & The man is riding a horse. & Horseback-riding & 6.98\\
13B & meat from a deer. & Venison & 7.18\\
30B & The man is riding a motorcycle down the road. & Motorcycling & 6.51\\
66B & of or relating to tutors or tutoring. & Tutorial & 9.35\\
\hline
\end{tabular}
\caption{ In-context learning examples used in varous model size.
}
\end{wraptable}
We demonstrate in-context learning examples that were obtained from each model on the STS-B development set, along with corresponding improvements on Spearman correlation for STS tasks.
As the size of the model increases to 2.7B, the improvements in in-context learning become more and more pronounced, and related examples are usually more implicit. For instance, the 125M OPT uses examples where words are incorporated within the sentence.

\section{Conclusion}

In this paper, we focus on exploiting Large Language Models (LLMs) to improve sentence embeddings. To achieve this, we propose a new sentence representation method called PromptEOL, which adapts previous prompt-based methods to autoregression models. Further, we leverage in-context learning to generate superior sentence embeddings. We utilize ChatGPT and the Oxford dictionary to create in-context learning examples, demonstrating that in-context learning allows LLMs to achieve performance comparable to that of contrastive learning methods. With our representation method, we also discover that further fine-tuning of LLMs can achieve state-of-the-art performance using only efficient fine-tuning methods.

\bibliographystyle{alpha}
\bibliography{scal_sent}


\appendix
%\section{Training Time}
\input{all-300-examples}

\section{Influence of Quantization}
\label{apx:quant}


\section{Transfer Tasks}\label{apx:transfer_task}
%We show the results of PromptEOL with in-context learning (ICL) and contrastive learning (CSE) in Table~\ref{fig:transfer_icl_cse}.
%Compared to PromptEOL, PromptEOL+ICL and PromptEOL+CSE even harm the performance on transfer tasks.
\begin{table*}[h]
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
%\textbf{Method}\;\;\;\;\;\;\;\;\;\;\;\; & \textbf{Params} & \ \textbf{MR}\quad & \ \textbf{CR}\quad & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\textbf{Method}\quad\quad\quad\quad\quad & \quad\quad\textbf{Params}\quad\quad & \textbf{MR} & \textbf{CR} & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\midrule
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
 &  125M & 80.86 & 87.66 & 93.19 & 89.77 & 87.31 & 92.20 & 72.64 & 86.23 \\
 &  350M & 84.14 & 88.08 & 93.17 & 89.77 & 89.73 & 91.20 & 71.36 & 86.78 \\
 &  1.3B & 88.06 & 91.55 & 95.90 & 91.55 & 93.08 & 95.00 & 73.97 & 89.87 \\
 &  2.7B & 88.83 & 92.29 & 95.93 & 91.76 & 94.62 & 96.00 & 75.94 & 90.77 \\
 &  6.7B & 90.26 & 92.50 & 96.67 & 91.39 & 94.67 & 96.00 & 77.91 & 91.34 \\
 &  13B  & 90.73 & 92.90 & 96.69 & 91.48 & 94.01 & 96.80 & 75.59 & 91.17 \\
 &  30B  & 90.95 & 92.77 & 96.99 & 91.79 & 95.28 & 97.00 & 73.97 & 91.25 \\
 &  66B  & 90.96 & 93.40 & 97.01 & 91.93 & 95.22 & 96.40 & 75.25 & 91.45 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL+ICL\\OPT}
 &  125M & 80.86 & 87.10 & 93.08 & 89.55 & 87.10 & 92.00 & 73.28 & 86.14\\
 &  350M & 82.20 & 86.65 & 93.21 & 89.70 & 87.86 & 87.60 & 72.52 & 85.68\\
 &  1.3B & 87.05 & 90.49 & 95.34 & 91.54 & 90.72 & 95.80 & 72.64 & 89.08\\
 &  2.7B & 88.73 & 91.79 & 95.44 & 91.54 & 93.52 & 95.20 & 75.30 & 90.22\\
 &  6.7B & 89.80 & 93.27 & 96.32 & 91.46 & 93.79 & 95.40 & 74.43 & 90.64\\
 &  13B  & 89.45 & 92.98 & 96.23 & 91.28 & 94.51 & 95.40 & 75.71 & 90.79\\
 &  30B  & 90.27 & 92.82 & 96.46 & 91.76 & 94.34 & 97.00 & 76.29 & 91.28\\
 &  66B  & 90.40 & 92.50 & 97.08 & 91.24 & 94.34 & 97.40 & 75.01 & 91.14\\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL+CSE\\OPT}
 &  1.3B& 88.62 & 91.89 & 95.49 & 91.64 & 94.29 & 94.80 & 73.22 & 89.99 \\
 &  2.7B& 88.40 & 92.16 & 95.57 & 91.51 & 94.12 & 95.20 & 74.09 & 90.15 \\
 &  6.7B& 89.60 & 92.05 & 95.91 & 91.09 & 94.78 & 95.80 & 75.71 & 90.71 \\
 &  13B & 89.20 & 92.40 & 95.92 & 90.86 & 93.74 & 95.40 & 73.10 & 90.09 \\
\midrule
\multirowcell{4}[0pt][l]{PromptEOL\\LLaMA}
& 7B & 90.40 & 92.90 & 96.88 & 91.57 & 95.11 & 95.40 & 75.13 & 91.06 \\
& 13B & 92.02 & 93.22 & 97.29 & 91.40 & 95.66 & 95.80 & 76.46 & 91.69 \\
& 30B & 91.64 & 93.27 & 97.10 & 91.86 & 95.99 & 95.80 & 78.43 & 92.01 \\
& 65B & 92.13 & 93.43 & 97.16 & 91.91 & 95.33 & 97.40 & 77.28 & 92.09 \\
\midrule
\multirowcell{2}[0pt][l]{PromptEOL+CSE\\LLaMA}
& 7B & 90.28 & 93.27 & 96.67 & 91.45 & 94.73 & 95.60 & 75.54 & 91.08 \\
& 13B& 91.22 & 93.22 & 96.83 & 91.52 & 94.89 & 95.80 & 74.26 & 91.11 \\
\bottomrule
\end{tabular}}
\caption{ Performances of our method with in-context learning and contrastive learning on transfer learning tasks.
   %Last avg. denotes averaging the last layer of BERT.   First-last avg. uses the first and last layer.  means removing biased tokens in static avg., which we have introduced before.
} \label{fig:transfer_icl_cse}
\end{table*}


\section{Sentence Representation Methods}\label{apx:sentence_rep}

\begin{table*}[t]
\vspace{-13pt}
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STS-B} & \textbf{SICK-R} & \textbf{Avg.}\\
\midrule
\midrule
\multicolumn{10}{c}{\it{Without fine-tuning}}\\
\midrule
\multirowcell{8}[0pt][l]{OPT avg.}
& 125M& 44.27 & 50.38 & 44.95 & 62.39 & 55.52 & 45.39 & 53.24 & 50.88 \\
& 350M& 40.61 & 47.25 & 40.45 & 55.12 & 55.57 & 40.53 & 47.66 & 46.74 \\
& 1.3B& 45.12 & 54.01 & 46.52 & 62.94 & 55.96 & 46.31 & 54.32 & 52.17 \\
& 2.7B& 44.11 & 54.35 & 47.89 & 63.91 & 57.02 & 47.85 & 54.44 & 52.80 \\
& 6.7B& 43.61 & 51.69 & 45.86 & 60.11 & 55.41 & 45.42 & 54.93 & 51.00 \\
& 13B & 46.95 & 54.92 & 48.74 & 60.13 & 54.96 & 48.07 & 53.93 & 52.53 \\
& 30B & 43.93 & 52.44 & 46.04 & 58.80 & 55.15 & 47.13 & 53.46 & 50.99 \\
& 66B & 40.81 & 47.98 & 44.21 & 59.37 & 56.37 & 43.80 & 53.19 & 49.39 \\
\midrule
\multirowcell{8}[0pt][l]{OPT prompt}
& 125M & 56.25 & 71.61 & 58.62 & 63.47 & 70.29 & 59.77 & 63.23 & 63.32 \\
& 350M & 56.56 & 69.27 & 55.81 & 60.05 & 68.73 & 61.75 & 64.15 & 62.33 \\
& 1.3B & 60.26 & 75.64 & 62.93 & 70.63 & 76.52 & 67.31 & 65.95 & 68.46 \\
& 2.7B & 59.34 & 75.47 & 62.64 & 69.76 & 75.65 & 68.35 & 67.48 & 68.38 \\
& 6.7B & 55.20 & 76.91 & 62.53 & 69.41 & 76.39 & 67.33 & 65.86 & 67.66 \\
& 13B  & 49.60 & 75.43 & 61.58 & 67.33 & 75.53 & 65.98 & 63.79 & 65.61 \\
& 30B  & 46.69 & 72.42 & 58.00 & 67.52 & 72.98 & 64.77 & 65.66 & 64.01 \\
& 66B  & 50.21 & 69.65 & 56.78 & 70.20 & 73.37 & 64.31 & 66.93 & 64.49 \\
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
& 125M & 59.90 & 71.55 & 60.93 & 70.76 & 72.83 & 67.89 & 65.14 & 67.00 \\
& 350M & 54.70 & 71.52 & 59.99 & 64.51 & 71.39 & 66.55 & 66.58 & 65.03 \\
& 1.3B & 64.59 & 79.06 & 68.46 & 78.88 & 78.64 & 73.22 & 69.41 & 73.18 \\
& 2.7B & 60.03 & 75.51 & 64.30 & 74.56 & 77.62 & 67.73 & 65.35 & 69.30 \\
& 6.7B & 60.91 & 80.05 & 67.65 & 75.49 & 80.11 & 72.91 & 67.57 & 72.10 \\
& 13B &  60.21 & 81.36 & 69.69 & 75.46 & 79.58 & 70.73 & 65.99 & 71.86 \\
& 30B &  59.99 & 80.52 & 69.80 & 75.20 & 78.03 & 73.57 & 69.87 & 72.43 \\
& 66B &  55.66 & 74.62 & 64.90 & 72.34 & 75.21 & 71.72 & 67.43 & 68.84 \\
\midrule
\midrule
\multicolumn{10}{c}{\it{Fine-tuning on unsupervised datasets}}\\
\midrule
\multirowcell{6}[0pt][l]{PromptEOL\\OPT}
 & 125M & 76.53 & 85.56 & 79.75 & 85.43 & 81.17 & 84.32 & 79.04 & 81.69 \\
 & 350M & 75.96 & 85.51 & 81.32 & 86.50 & 81.42 & 85.24 & 80.35 & 82.33 \\
 & 1.3B & 79.01 & 89.26 & 84.10 & 88.30 & 84.62 & 87.71 & 80.52 & 84.79\\
 & 2.7B & 79.49 & 89.64 & 84.80 & 89.51 & 85.91 & 88.33 & 81.64 & 85.62\\
 & 6.7B & 80.14 & 90.02 & 84.94 & 89.78 & 85.84 & 88.75 & 81.29 & 85.82\\
 & 13B  & 80.20 & 90.24 & 85.34 & 89.52 & 85.90 & 88.56 & 82.06 & 85.97\\
\midrule
\multirowcell{6}[0pt][l]{OPT avg.}

 &125M  &74.08  & 82.70 & 77.76 & 83.65 & 79.74 & 82.43 & 78.55 & 79.84 \\
 &350M  &74.07  & 83.78 & 78.06 & 84.62 & 80.70 & 83.93 & 78.61 & 80.54 \\
 &1.3B  &75.38  & 84.99 & 80.34 & 86.10 & 81.49 & 84.35 & 79.98 & 81.80 \\
 & 2.7B &\\
 & 6.7B & \\
 & 13B  & 75.86 & 86.32 & 80.73 & 86.25 & 82.13 & 85.55 & 79.62 & 82.35 \\
\midrule
\multirowcell{6}[0pt][l]{OPT prompt}
 & 125M & 76.05 & 85.24 & 79.82 & 85.27 & 81.30 & 84.56 & 79.09 & 81.62 \\
 & 350M & 76.28 & 86.01 & 80.96 & 86.13 & 81.87 & 85.33 & 79.73 & 82.33 \\
 & 1.3B & 78.56 & 89.21 & 84.21 & 88.71 & 84.17 & 87.39 & 81.16 & 84.77 \\
 & 2.7B & 78.89 & 89.21 & 84.43 & 89.43 & 85.75 & 88.07 & 81.40 & 85.31 \\
 & 6.7B & 78.66 & 89.81 & 84.45 & 89.70 & 85.71 & 88.63 & 81.79 & 85.54 \\
 & 13B  & 79.66 & 89.84 & 84.88 & 89.54 & 85.59 & 88.65 & 81.93 & 85.73 \\
\bottomrule
\end{tabular}
}
\caption{ Comparison of three sentence representation methods on STS tasks.
}
\end{table*}


\begin{table*}[h]
\renewcommand\arraystretch{1.1}
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
%\textbf{Method}\;\;\;\;\;\;\;\;\;\;\;\; & \textbf{Params} & \ \textbf{MR}\quad & \ \textbf{CR}\quad & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\textbf{Method}\quad\quad\quad\quad\quad & \quad\quad\textbf{Params}\quad\quad & \textbf{MR} & \textbf{CR} & \textbf{SUBJ} & \textbf{MPQA} & \textbf{SST} & \textbf{TREC} & \textbf{MRPC} & \textbf{Avg.}\\
\midrule
\midrule
\multirowcell{8}[0pt][l]{PromptEOL\\OPT}
 &  125M & 80.86 & 87.66 & 93.19 & 89.77 & 87.31 & 92.20 & 72.64 & 86.23 \\
 &  350M & 84.14 & 88.08 & 93.17 & 89.77 & 89.73 & 91.20 & 71.36 & 86.78 \\
 &  1.3B & 88.06 & 91.55 & 95.90 & 91.55 & 93.08 & 95.00 & 73.97 & 89.87 \\
 &  2.7B & 88.83 & 92.29 & 95.93 & 91.76 & 94.62 & 96.00 & 75.94 & 90.77 \\
 &  6.7B & 90.26 & 92.50 & 96.67 & 91.39 & 94.67 & 96.00 & 77.91 & 91.34 \\
 &  13B  & 90.73 & 92.90 & 96.69 & 91.48 & 94.01 & 96.80 & 75.59 & 91.17 \\
 &  30B  & 90.95 & 92.77 & 96.99 & 91.79 & 95.28 & 97.00 & 73.97 & 91.25 \\
 &  66B  & 90.96 & 93.40 & 97.01 & 91.93 & 95.22 & 96.40 & 75.25 & 91.45 \\
\midrule
\multirowcell{8}[0pt][l]{OPT avg.}
 &  125M & 80.63 & 86.41 & 93.91 & 87.85 & 86.22 & 92.60 & 71.83 & 85.64 \\
 &  350M & 80.73 & 85.16 & 93.42 & 87.26 & 86.11 & 87.80 & 69.57 & 84.29 \\
 &  1.3B & 85.89 & 90.04 & 95.71 & 90.10 & 91.38 & 94.20 & 72.99 & 88.62 \\
 &  2.7B & 87.55 & 90.76 & 95.78 & 90.26 & 91.71 & 94.40 & 68.00 & 88.35 \\
 &  6.7B & 87.93 & 91.07 & 96.58 & 90.65 & 92.70 & 96.20 & 72.17 & 89.61 \\
 &  13B  & 88.33 & 91.76 & 96.74 & 90.78 & 93.25 & 95.20 & 70.90 & 89.57 \\
 &  30B  & 88.54 & 92.11 & 96.85 & 90.61 & 93.74 & 94.40 & 70.72 & 89.57 \\
 &  66B  & 89.17 & 92.00 & 96.86 & 90.80 & 94.67 & 96.40 & 71.07 & 90.14 \\
\midrule
\multirowcell{8}[0pt][l]{OPT prompt}
 &  125M & 83.54 & 87.60 & 94.28 & 89.36 & 88.74 & 91.60 & 67.01 & 86.02 \\
 &  350M & 80.99 & 84.08 & 93.30 & 89.38 & 86.88 & 88.80 & 60.99 & 83.49 \\
 &  1.3B & 87.31 & 90.68 & 95.73 & 91.30 & 93.47 & 94.40 & 72.99 & 89.41 \\
 &  2.7B & 88.58 & 91.60 & 96.22 & 91.36 & 93.90 & 95.80 & 70.96 & 89.77 \\
 &  6.7B & 90.55 & 92.21 & 97.09 & 91.31 & 95.06 & 96.60 & 74.90 & 91.10 \\
 &  13B  & 90.45 & 92.66 & 96.85 & 91.57 & 95.44 & 96.00 & 74.55 & 91.07 \\
 &  30B  & 90.56 & 92.79 & 97.28 & 91.93 & 94.78 & 96.00 & 72.93 & 90.90 \\
 &  66B  & 90.95 & 92.48 & 97.27 & 91.72 & 95.55 & 95.80 & 75.30 & 91.30 \\
\bottomrule
\end{tabular}}
\caption{ Comparison of three sentence representation methods on STS tasks.
}
\end{table*}



\end{document}
