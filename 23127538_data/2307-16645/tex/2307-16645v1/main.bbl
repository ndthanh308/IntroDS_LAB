\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{ACDGA12}

\bibitem[ABC{\etalchar{+}}14]{agirre2014semeval}
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor
  Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe.
\newblock Semeval-2014 task 10: Multilingual semantic textual similarity.
\newblock In {\em Proceedings of the 8th international workshop on semantic
  evaluation (SemEval 2014)}, pages 81--91, 2014.

\bibitem[ABC{\etalchar{+}}15]{agirre2015semeval}
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor
  Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
  Mihalcea, et~al.
\newblock Semeval-2015 task 2: Semantic textual similarity, english, spanish
  and pilot on interpretability.
\newblock In {\em Proceedings of the 9th international workshop on semantic
  evaluation (SemEval 2015)}, pages 252--263, 2015.

\bibitem[ABC{\etalchar{+}}16]{agirre2016semeval}
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez~Agirre, Rada
  Mihalcea, German Rigau~Claramunt, and Janyce Wiebe.
\newblock Semeval-2016 task 1: Semantic textual similarity, monolingual and
  cross-lingual evaluation.
\newblock In {\em SemEval-2016. 10th International Workshop on Semantic
  Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p.
  497-511.} ACL (Association for Computational Linguistics), 2016.

\bibitem[ACD{\etalchar{+}}13]{agirre2013sem}
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
\newblock * sem 2013 shared task: Semantic textual similarity.
\newblock In {\em Second joint conference on lexical and computational
  semantics (* SEM), volume 1: proceedings of the Main conference and the
  shared task: semantic textual similarity}, pages 32--43, 2013.

\bibitem[ACDGA12]{agirre2012semeval}
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre.
\newblock Semeval-2012 task 6: A pilot on semantic textual similarity.
\newblock In {\em * SEM 2012: The First Joint Conference on Lexical and
  Computational Semantics--Volume 1: Proceedings of the main conference and the
  shared task, and Volume 2: Proceedings of the Sixth International Workshop on
  Semantic Evaluation (SemEval 2012)}, pages 385--393, 2012.

\bibitem[BMR{\etalchar{+}}20]{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem[CDA{\etalchar{+}}17]{cer2017semeval}
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.
\newblock Semeval-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock {\em arXiv preprint arXiv:1708.00055}, 2017.

\bibitem[CDL{\etalchar{+}}22]{chuang2022diffcse}
Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin
  Solja{\v{c}}i{\'c}, Shang-Wen Li, Wen-tau Yih, Yoon Kim, and James Glass.
\newblock Diffcse: Difference-based contrastive learning for sentence
  embeddings.
\newblock {\em arXiv preprint arXiv:2204.10298}, 2022.

\bibitem[CK18]{conneau2018senteval}
Alexis Conneau and Douwe Kiela.
\newblock Senteval: An evaluation toolkit for universal sentence
  representations.
\newblock {\em arXiv preprint arXiv:1803.05449}, 2018.

\bibitem[CKB{\etalchar{+}}21]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[CKS{\etalchar{+}}17]{conneau-etal-2017-supervised-infersent}
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo{\"\i}c Barrault, and Antoine
  Bordes.
\newblock Supervised learning of universal sentence representations from
  natural language inference data.
\newblock In {\em emnlp}, pages 670--680, 2017.

\bibitem[CND{\etalchar{+}}22]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[CYS{\etalchar{+}}23]{cheng2023improving}
Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang Li, and Xipeng Qiu.
\newblock Improving contrastive learning of sentence embeddings from ai
  feedback.
\newblock {\em arXiv preprint arXiv:2305.01918}, 2023.

\bibitem[DB05]{mrpc2005}
William~B Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In {\em Proceedings of the Third International Workshop on
  Paraphrasing (IWP2005)}, 2005.

\bibitem[DPHZ23]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock {\em arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Eth19]{ethayarajh2019contextual}
Kawin Ethayarajh.
\newblock How contextual are contextualized word representations? comparing the
  geometry of bert, elmo, and gpt-2 embeddings.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 55--65, 2019.

\bibitem[FAHA22]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[GYC21]{gao2021simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock {\em arXiv preprint arXiv:2104.08821}, 2021.

\bibitem[HBB{\etalchar{+}}20]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock {\em arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[HBM{\etalchar{+}}22]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[HL04]{hu2004mining_cr}
Minqing Hu and Bing Liu.
\newblock Mining and summarizing customer reviews.
\newblock In {\em ACM SIGKDD international conference on Knowledge discovery
  and data mining}, 2004.

\bibitem[HSW{\etalchar{+}}21]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[JJH{\etalchar{+}}22]{jiang2022promptbert}
Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang,
  Furu Wei, Haizhen Huang, Denvy Deng, and Qi~Zhang.
\newblock Promptbert: Improving bert sentence embeddings with prompts.
\newblock {\em arXiv preprint arXiv:2201.04337}, 2022.

\bibitem[KMH{\etalchar{+}}20]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[LHE21]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock {\em arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[LYF{\etalchar{+}}23]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 55(9):1--35, 2023.

\bibitem[LZH{\etalchar{+}}20]{li2020sentence}
Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li.
\newblock On the sentence embeddings from pre-trained language models.
\newblock {\em arXiv preprint arXiv:2011.05864}, 2020.

\bibitem[MMB{\etalchar{+}}14]{marelli2014sick}
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella
  Bernardi, Roberto Zamparelli, et~al.
\newblock A sick cure for the evaluation of compositional distributional
  semantic models.
\newblock In {\em Lrec}, pages 216--223. Reykjavik, 2014.

\bibitem[Mue22]{muennighoff2022sgpt}
Niklas Muennighoff.
\newblock Sgpt: Gpt sentence embeddings for semantic search.
\newblock {\em arXiv preprint arXiv:2202.08904}, 2022.

\bibitem[N{\'A}C{\etalchar{+}}21]{sentencet5}
Jianmo Ni, Gustavo~Hern{\'a}ndez {\'A}brego, Noah Constant, Ji~Ma, Keith~B
  Hall, Daniel Cer, and Yinfei Yang.
\newblock Sentence-t5: Scalable sentence encoders from pre-trained text-to-text
  models.
\newblock {\em arXiv preprint arXiv:2108.08877}, 2021.

\bibitem[PL04]{pang2004sentimental_subj}
Bo~Pang and Lillian Lee.
\newblock A sentimental education: Sentiment analysis using subjectivity
  summarization based on minimum cuts.
\newblock In {\em acl}, pages 271--278, 2004.

\bibitem[PL05]{pang2005seeing_mr}
Bo~Pang and Lillian Lee.
\newblock Seeing stars: Exploiting class relationships for sentiment
  categorization with respect to rating scales.
\newblock In {\em acl}, pages 115--124, 2005.

\bibitem[RG19]{reimers2019sentence}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock {\em arXiv preprint arXiv:1908.10084}, 2019.

\bibitem[RSR{\etalchar{+}}20]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551,
  2020.

\bibitem[SAW22]{bloom}
Teven~Le Scao, 388 Authors, and Thomas Wolf.
\newblock {BLOOM}: A {176B}-parameter open-access multilingual language model.
\newblock {\em ArXiv}, abs/2211.05100, 2022.

\bibitem[SCLO21]{su2021whitening}
Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.
\newblock Whitening sentence representations for better semantics and faster
  retrieval.
\newblock {\em arXiv preprint arXiv:2103.15316}, 2021.

\bibitem[SHK{\etalchar{+}}14]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem[SPW{\etalchar{+}}13]{socher2013recursive_sst-2}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em emnlp}, pages 1631--1642, 2013.

\bibitem[TLI{\etalchar{+}}23]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[TST21]{tsukagoshi-etal-2021-defsent}
Hayato Tsukagoshi, Ryohei Sasano, and Koichi Takeda.
\newblock {D}ef{S}ent: Sentence embeddings using definition sentences.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 2: Short Papers)}, pages 411--418,
  Online, August 2021. Association for Computational Linguistics.

\bibitem[VT00]{voorhees2000building_trec}
Ellen~M Voorhees and Dawn~M Tice.
\newblock Building a question answering test collection.
\newblock In {\em the 23rd annual international ACM SIGIR conference on
  Research and development in information retrieval}, pages 200--207, 2000.

\bibitem[WGL{\etalchar{+}}22]{wu-etal-2022-infocse}
Xing Wu, Chaochen Gao, Zijia Lin, Jizhong Han, Zhongyuan Wang, and Songlin Hu.
\newblock {I}nfo{CSE}: Information-aggregated contrastive learning of sentence
  embeddings.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 3060--3070, Abu Dhabi, United Arab Emirates, December
  2022. Association for Computational Linguistics.

\bibitem[WTS{\etalchar{+}}22]{wu2022pcl}
Qiyu Wu, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, and Daxin Jiang.
\newblock Pcl: Peer-contrastive learning with diverse augmentations for
  unsupervised sentence embeddings.
\newblock {\em arXiv preprint arXiv:2201.12093}, 2022.

\bibitem[WWC05]{wiebe2005annotating_mpqa}
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
\newblock Annotating expressions of opinions and emotions in language.
\newblock {\em Language resources and evaluation}, 39(2-3):165--210, 2005.

\bibitem[ZLH23]{zhang2023contrastive}
Junlei Zhang, Zhenzhong Lan, and Junxian He.
\newblock Contrastive learning of sentence embeddings from scratch.
\newblock {\em arXiv preprint arXiv:2305.15077}, 2023.

\bibitem[ZRG{\etalchar{+}}22]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\end{thebibliography}
