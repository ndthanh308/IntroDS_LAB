%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
% \documentclass[sigconf,review,anonymous]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% \usepackage{}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\settopmatter{printacmref=false} %remove ACM reference format
\renewcommand\footnotetextcopyrightpermission[1]{}
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{18}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{DocDeshadower: Frequency-aware Transformer for Document Shadow Removal}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Shenghong Luo}
\authornote{Both authors contributed equally to this research.}
\author{Ruifeng Xu}
\authornotemark[1]
\affiliation{%
  \institution{Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences}
  \country{China}
}

\author{Xuhang Chen}
\author{Zinuo Li}
\affiliation{%
  \institution{Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences}
  \country{China}
}
\affiliation{%
  \institution{University of Macau}
  \country{China}
}

\author{Chi-Man Pun}
\authornote{Corresponding Author.}
\affiliation{%
  \institution{University of Macau}
  \country{China}
}

\author{Shuqiang Wang}
\authornotemark[2]
\affiliation{%
    \institution{Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences}
    \country{China}
}
 
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The presence of shadows significantly impacts the visual quality of scanned documents. However, the existing traditional techniques and deep learning methods used for shadow removal have several limitations. These methods either rely heavily on heuristics, resulting in suboptimal performance, or require large datasets to learn shadow-related features. In this study, we propose the DocDeshadower, a multi-frequency Transformer-based model built on Laplacian Pyramid. DocDeshadower is designed to remove shadows at different frequencies in a coarse-to-fine manner. To achieve this, we decompose the shadow image into different frequency bands using Laplacian Pyramid. In addition, we introduce two novel components to this model: the Attention-Aggregation Network and the Gated Multi-scale Fusion Transformer. The Attention-Aggregation Network is designed to remove shadows in the low-frequency part of the image, whereas the Gated Multi-scale Fusion Transformer refines the entire image at a global scale with its large perceptive field. Our extensive experiments demonstrate that DocDeshadower outperforms the current state-of-the-art methods in both qualitative and quantitative terms.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010224</concept_id>
<concept_desc>Computing methodologies~Computer vision</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer vision}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Document shadow removal, Multi-frequency, Vision Transformer}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\input{figtex/1_teaser}
%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

The use of smartphones to digitize documents has become increasingly popular due to their convenient nature and high-quality cameras. However, capturing document images using a smartphone may result in shadows caused by the user's hand or the camera obstructing light sources, and uneven illumination may occur even without obstructions. Removing shadows from document images is crucial because they can negatively impact legibility and hinder the performance of various computer vision tasks, particularly optical character recognition (OCR)~\cite{mori1999optical}.

Several conventional document shadow removal techniques rely on heuristics to analyze image features~\cite{bako2017removing,kligler2018document,jung2019water,wang2019effective,wang2020shadow}. These methods typically detect intensity changes or examine illumination in digitized documents. For instance, Bako \etal~\cite{bako2017removing} identify intensity changes to achieve a uniform background color in the output, although they assume document images have a homogeneous background. Jung \etal~\cite{jung2019water} employ a water-filling method that converts the input image into a topographic surface to correct illumination. However, these heuristics are limited as they perform well for some document images but are ineffective for others as demonstrated in Figure~\ref{fig:teaser} (b).

Recently, deep learning approaches have been utilized to address the aforementioned issues within various computer vision and document shadow removal applications, achieving exceptional results. A plethora of techniques for removing shadows in both natural and document images have been proposed~\cite{wang2018stacked,hu2019mask,lin2020bedsr,cun2020towards,le2020shadow,fu2021auto,liu2021shadow,jin2021dc,wan2022style,guo2023shadowformer}. These techniques utilize a set of shadow images, shadow-free images, and shadow masks to effectively detect and remove shadows through an end-to-end training process. However, deep learning methods necessitate a substantial number of paired document images. Researchers aiming to remove shadows from individual documents have proposed real datasets comprising pairs of shadow-free and shadowed images. However, due to the high cost of constructing large datasets, only a limited number of small real datasets are available ~\cite{kligler2018document,jung2019water}. To address this issue, Lin \etal~\cite{lin2020bedsr} synthesized shadows on documents using a graphical renderer to produce a relatively extensive dataset. However, real document scanning and capturing are still required, resulting in a small number of distinct documents the trained network can learn from, hindering the network's performance as shown in Figure~\ref{fig:teaser} (c) and (d). 

We address the issues mentioned above from a multi-scale perspective because shadows mainly exhibit color distortion in low-frequency sections and edge features in high-frequency parts. To take into account the unique features of shadows at different scales, we introduce the Attention-Aggregation Network to target shadow color information at the pixel level and handle shadow edge refinement using the Multi-scale Fusion Transformer on a global scale. This approach improves the handling of shadow characteristics across various frequencies and scales. The frequency decomposition is achieved using Laplacian Pyramid~\cite{burt1987laplacian}, which can divide the image into several frequency bands and reconstruct the image losslessly. This method provides a clear-cut and efficient way of analyzing and reconstructing images while retaining complete information. To address the problem of insufficient data, we utilize various data augmentation techniques.
We summarize our contributions as follows:
\begin{enumerate}
    \item We propose DocDeshadower, which addresses the issue of shadows appearing mainly as color in low-frequency information and as features such as edges in high-frequency information by utilizing Laplacian pyramid from a multi-scale perspective.
    \item  In our proposed method, we utilize the Attention-Aggregation Network to eliminate color distortion in the low-frequency component and the Gated Multi-scale Fusion Transformer to refine high-frequency edge features. This approach enhances the handling of shadows across various frequencies, resulting in better visual quality.
    \item Based on extensive benchmark experiments, our model significantly outperforms state-of-the-art techniques.
\end{enumerate}

\section{Related Work}

\subsection{Natural Shadow Removal}
ST-CGAN~\cite{wang2018stacked} comprised of two successive CGANs - one dedicated to shadow detection, and the other aimed at removing them.
Mask-ShadowGAN~\cite{hu2019mask} produced a shadow mask from the input shadow image and then used the mask to guide the shadow generation via re-formulated cycle-consistency constraints.
Cun \etal~\cite{cun2020towards} proposed a Dual Hierarchical Aggregation Network (DHAN) to eliminate shadows and a Shadow Matting GAN (SMGAN) utilized for synthesizing shadow images.
SP+M Net~\cite{le2020shadow} used a physical linear transformation model to enhance the shadow region and reconstruct the shadow-free image via image decomposition.	
AEFNet~\cite{fu2021auto} aligns the color of shadow areas with the shadow-free parts by estimating the over-exposure image condition and combining it with the original input in the shadow region.
LG-ShadowNet~\cite{liu2021shadow} first trained a convolutional neural network (CNN) module to estimate lighting and then trained a second CNN module, incorporating guidance from the first CNN module's lighting information, for removing shadows.
DC-ShadowNet~\cite{jin2021dc} is an unsupervised domain classifier-guided network designed for single-image hard and soft shadow removal that integrates a domain classifier for shadowed and shadow-free regions.
SG-ShadowNet~\cite{wan2022style} utilizes a simple region style estimator to learn the style representation of the non-shadow region and a normalization strategy to better align the recovered shadow region.
ShadowFormer~\cite{guo2023shadowformer} utilizes a Retinex-based shadow model and a Shadow-Interaction Module along with Shadow-Interaction Attention to capture the contextual correlation between global shadowed and shadow-free regions.

\subsection{Document Shadow Removal}
Kligler \etal~\cite{kligler2018document} curated a document shadow dataset and developed a deshadowing method based on a 3D point cloud.
Jung \etal~\cite{jung2019water} introduced the water-filling method and also curated a document shadow dataset.
BEDSR-Net~\cite{lin2020bedsr} is the first deep network designed specifically for removing shadows from document images, and it is composed of the Background Estimation Network (BE-Net) and a dedicated shadow removal module. The BE-Net predicts the global background color of the document, while the shadow removal module works with both the predicted background color and attention map to achieve precise shadow removal with a higher degree of accuracy.

\section{Methodology}

\input{figtex/2_model}

\subsection{Overview}
Shadows have two types of features: color information in low frequency and textual features in high frequency.	Thus, decomposing an image into various scales of low-frequency and high-frequency information facilitates the extraction of multi-scale information in the image, thereby effectively removing the shadows. We deconstruct shadow images into low-frequency and high-frequency components using Laplacian Pyramid since the Laplacian Pyramid enables multi-scale processing and preserves details and texture information more effectively. Once we decompose the input image, we obtain multiple sections, where the Gated Multi-scale Fusion Transformer (GMFT) processes the high-frequency components while the Attention-Aggregation Network (AAN) process the low-frequency component.	

\subsection{Gated Multi-scale Fusion Transformer}
We present a novel Gated Multi-scale Fusion Transformer for processing the high-frequency component of the Laplacian Pyramid decomposition. The input undergoes feature enhancement through a ResidualBlock before being fed into a lightweight Gated Identity Attention (GIA) Block. The ResidualBlock is employed to improve both accuracy and training speed of the model. The GIA Block, on the other hand, adjusts features in the color channel, enhances model performance, and reduces model parameter and computational complexity. To reduce computational complexity, SimpleGate~\cite{testa2022aggregate} is applied as a means of introducing nonlinearity. Additionally, we leverage simple channel attention to improve channel interaction and textural quality of the output.

Next, the input features are fused and reduced via a Transformer Block, allowing the model to focus on significant features while eliminating noise and redundant information. Inspired by~\cite{Wang_Zhang_Shen_Luo_Stenger_Lu_2022}, we introduce the Bi-directional Multi-head Transformer (BMT), which incorporates self-attention sub-modules. This bidirectional attention mechanism allows us to capture essential information in the feature map. Finally, we adopt Dual Gated Feed-forward Network (DGFN) with gating mechanisms to perform non-linear transformations on the input feature maps, resulting in better global adjustment.

\subsection{Attention-Aggregation Network}
Inspired by~\cite{cun2020towards}, we propose the Attention-Aggregation Network to enhance the low-frequency parts of the image. The network has two branches.

The first branch comprises of the Channel-wise Normalized Residual (CNR) block. This block can locate shadows in the image effectively, thereby providing an essential component in distinguishing between the foreground and background image elements in shadow removal tasks. Additionally, the CNR block increases the focus on building features across channels.	

\input{tabletex/1_sota}

\input{tabletex/2_ablation}

The second branch involves the Multi-stage Attentive Aggregation (MAA) block that performs cascaded convolution and fusion. The Attentive Aggregation Node fuses features and attention separately. Convolution MLP (CMLP) is a basic model applied in performing nonlinear transformations on features. The Spatial Pyramid Pooling (SPP) further contributes to the model by pooling features at various stages and enhancing the breadth and depth of perception.	

\subsection{Objective Functions}
Our objective functions consist of Mean Squared Error ($L_{MSE}$) and Structural Similarity Index ($L_{SSIM}$). These losses are combined to form the complete loss function, $L_{total}$, as defined in Equation~\eqref{eq:total}:
\begin{equation}
    L_{total} = L_{MSE} + 0.2 * L_{SSIM}
    \label{eq:total}
\end{equation}
where $L_{MSE}$ and $L_{SSIM}$ are denoted as Equation~\eqref{eq:mse}, Equation~\eqref{eq:ssim} respectively.

\begin{equation}
L_{MSE}=\sum_{i=1}^{n}(x_i-y_i)^2
\label{eq:mse}
\end{equation}

\begin{equation}
L_{SSIM}=\frac{\left(2 \mu_{\mathrm{x}} \mu_{\mathrm{y}}+C_{1}\right)\left(2 \sigma_{\mathrm{xy}}+C_{2}\right)}{\left(\mu_{\mathrm{x}}^{2}+\mu_{\mathrm{y}}^{2}+C_{1}\right)\left(\sigma_{\mathrm{x}}^{2}+\sigma_{\mathrm{y}}^{2}+C_{2}\right)}
\label{eq:ssim}
\end{equation}
the symbols $\mu_{\mathrm{x}}$ and $\mu_{\mathrm{y}}$ denote the means of images X and Y, respectively. The term $\sigma_{\mathrm{x}}y$ represents the covariance between images X and Y, while symbols $\sigma_{\mathrm{x}}$ and $\sigma_{\mathrm{y}}$ denote the standard deviation of images X and Y, respectively. The values of the constants $C_1$ and $C_2$ are specified in this work, where $C_1=(0.01 \times 255)^{2}$ and $C_2=(0.03 \times 255)^{2}$.

\section{Experiments}

\input{figtex/ref}

\input{figtex/Ablation}

\subsection{Experimental Setup}

\subsubsection{Datasets and Preprocessing}
In our study, we compared the performance of each method on two publicly available datasets since Bako's dataset~\cite{bako2017removing} and RDSRD~\cite{lin2020bedsr} were not accessible. The datasets used were:

\begin{enumerate}
    \item Kligler's dataset~\cite{kligler2018document}, comprising 300 image pairs, with 272 for training and the remaining paired images for testing.
    \item Jung's dataset~\cite{jung2019water}, containing 87 pairs of shadow images and shadow masks where 67 were for training and the rest for testing.
\end{enumerate}

For Kligler's dataset and Jung's dataset, we derived their shadow masks using the AEFNet method~\cite{fu2021auto} to facilitate model training. Additionally, to ensure a fair comparison, all images were resized to $512 \times 512$.

\subsubsection{Evaluation Metrics}
We evaluated image quality using three widely adopted metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) and Root Mean Square Error (RMSE). PSNR and SSIM are commonly used to evaluate image restoration and low-level computer vision tasks. In contrast, RMSE is crucial and frequently employed exclusively in shadow removal tasks. Higher PSNR and SSIM scores indicate better visual quality, while a lower RMSE score suggests fewer shadow artifacts.

\subsubsection{Implementation Details}
We implemented the model using PyTorch. The NVIDIA RTX Titan trained our model using the typical Adam optimizer's default parameters. The batch size and learning rate were set to 1 and $1e-4$, respectively. We augmented the images in our training dataset through random cropping, resizing, flipping, and mixup and applied brightness and saturation adjustments.

\subsection{Comparisons with State-of-the-Arts}
In our study, we compared the proposed network to various state-of-the-art methods, including two traditional methods proposed by Kligler \etal~\cite{kligler2018document} and Jung \etal~\cite{jung2019water}, along with several deep learning-based methods: ST-CGAN~\cite{wang2018stacked}, Mask-ShadowGAN~\cite{hu2019mask}, DHAN~\cite{cun2020towards}, SP+M Net~\cite{le2020shadow}, AEFNet~\cite{fu2021auto}, LG-ShadowNet~\cite{liu2021shadow}, DC-ShadowNet~\cite{jin2021dc}, SG-ShadowNet~\cite{wan2022style}, and ShadowFormerr~\cite{guo2023shadowformer}. Since the training codes for ST-CGAN~\cite{wang2018stacked} and BEDSR-Net~\cite{lin2020bedsr} are not publicly available, we re-implemented these models based on their original papers.

\subsubsection{Quantitative Comparison}
Table~\ref{t1:sota} presents the quantitative results of the Jung and Kligler datasets. It reveals that our DocDeshadower outperforming the existing methods for all metric. 

Traditional methods, such as the approaches proposed by Jung~\cite{jung2019water} and Kligler~\cite{kligler2018document}, rely on heuristic features specific to the dataset used, resulting in inconsistency across different datasets. Consequently, such methods may not perform well when the data distribution is not compatible with their physical models. 

On the other hand, learning-based methods exhibit consistent performance across various datasets. However, their efficacy may be limited as they do not have access to all required features of the shadows due to the limited size of the training datasets.


\subsubsection{Qualitative Comparison}
The Figure~\ref{fig:ref} displays the quantitative results of the Jung and Kligler datasets. Our method exhibits the best performance, as demonstrated in the figure. In comparison, other methods tend to leave residual shadow edges, while remnants of the original shadow remain visible in considerable areas. Moreover, a significant difference in luminosity occurs between the target and the result images using other methods. In contrast, our method efficiently removes shadow edges and preserves the targeted image's luminosity.

\subsection{Ablation Studies}
In order to evaluate the effectiveness of our proposed DocDeshadower method, we conducted ablation studies that tested several components, including: 

\begin{enumerate}
    \item \textbf{Ours w/o AAN:} our DocDeshadower without AAN.
    \item \textbf{Ours w/o GMFT:} our DocDeshadower without GMFT.
    \item \textbf{U-Net+GMFT:} U-Net in low-frequency and GMFT.
    \item \textbf{U-Net+AAN:} U-Net in high-frequency and AAN.
    \item \textbf{Ours:} our full DocDeshadower architecture. 
\end{enumerate}

We present the results of our ablation studies on the Jung and Kligler datasets in Table~\ref{t2:ablation}. Our full-method significantly outperforms all the variants. We observed a significant contribution to overall performance from both the Attention-Aggregation Network and the Global Multi-scale Feature Transformer modules. The metrics without one of these modules result in a significant decline.

We observed that replacing our module with U-Net~\cite{ronneberger2015u} did not result in any performance improvement, indicating that U-Net lacks the ability to handle pixel-level enhancement at the local level and perform textual adjustment at the global level. Additionally, we found that U-Net is less sensitive to features of different frequencies than our modules.

To summarize, our experiments demonstrate the significant superiority of our full-version method on the Jung and Kligler datasets. Our comparison of the different components revealed the importance of each module and enabled us to optimize the overall performance of the method.

\section{Conclusion}
In this paper, We propose DocDeshadower, an approach for shadow removal that combines Fusion Transformer and Attention-Aggregation at multi-frequency. Our approach consistently outperforms current shadow removal methods based on extensive experimental results with the Jung and Kligler datasets. However, in highly saturated images, some loss of the original image's color information occurs with our approach. In future work, we plan to incorporate traditional methods and prior knowledge to enhance the effectiveness of shadow removal while maintaining image color.

% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
