%!TEX root = main-paper-template.tex


%\apcomment{Paragraphs are not indented in this paper. Please modify formatting to allow indentation.} \gwcomment{Done.}
% \textbf{Paper Organization.} {\color{orange}TODO marked in orange.} 
% \begin{itemize}
%     \item Section 2 proposes the problem setting and notations used for linearly structure PCA. 
%     \item Section 3 discusses a statistically optimal estimator and error-rate guarantees for this estimator.
%     \item Section 4 introduces a projected power method when one can access to an exact projection oracle.
%     \item {\color{orange}\textbf{Try projected gradient descent in Ludwigs.} Section 5 generalizes the result presented in Section 5 when one can only access to approximated tail/head projection oracles.}
%     \item Section 6 provides a minimax lower bound for linearly structured PCA estimation problem and compare the result in Theorem~\ref{thm:minimax-lb} with minimax lower bound of sparse PCA / path PCA. 
%     \item Section 7 studies some specific but widely-considered examples of linearly structured PCA, which includes the study of their hardness (SDP hardness and {\color{blue}average-case hardness}), projection oracles, and initialization method.
% \end{itemize}

\section{Introduction} \label{sec:intro}

Principal component analysis (PCA) is a preponderant tool for dimensionality reduction and feature extraction. PCA and its generalizations have been used for numerous applications including wavelet decomposition~\citep{mallat1999wavelet, baraniuk2010model}, representative stock selection from
business sectors~\citep{asteris2015stay}, human face recognition \citep{hancock1996face, tran2020tensor}, eigen-gene selection and shaving \citep{alter2000singular,hastie2000gene,feng2019supervised}, handwriting classification \citep{hastie2009elements}, clustering of functional connectivity \citep{frusque2019sparse}, and single-cell RNA sequencing analysis \citep{wang2021manifold}, to name but a few. %\apcomment{Say concrete things here to motivate structure in the principal components. For example, when should we expect path/tree sparsity to be good assumptions?} \gwcomment{Should we explicit point out that some of the applications above motivate additional structure in the PC in this paragraph, or may we discuss this issue in the next paragraph? See orange part.}

Given a set of $n$ samples $\bm{x}_1, \ldots, \bm{x}_n \in \mathbb{R}^d$, PCA is traditionally phrased as the problem of recovering the direction of maximal variance.
%ground truth principal component (PC) direction $\bm{v}_* \in \mathbb{R}^d$ -- a linear combination of the $d$ features that captures the maximal variance. A statistically optimal estimator of PCA can be obtained by solving the following optimization problem,
However, in high dimensions when $d \gg n$, it is well-known that the vanilla estimator given by the maximal eigenvector of the sample covariance matrix of the data is inconsistent (see, e.g.,~\citet{johnstone2009consistency} and references therein).
% In particular, letting $\widehat{\bm{\Sigma}} := \frac{1}{n} \sum_{i = 1}^n \bm{x}_i \bm{x}_i^{\top}$ denote the sample covariance matrix, the solution to the following optimization problem
% \begin{align}
%     \max_{\bm{v} \in \mathbb{R}^d} ~ \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} ~~\text{s.t.}~~ \|\bm{v}\|_2 = 1, \label{eq:PCA}
% \end{align}
% where $\widehat{\bm{\Sigma}} := \frac{1}{n} \sum_{i = 1}^n \bm{x}_i \bm{x}_i^{\top}$ denotes the sample covariance matrix. 
This motivates imposing additional structural assumptions on the ``ground-truth" principal component, and studying the resulting problem under a generative model for the data. Indeed, sparsity has emerged as a key such structural assumption that is motivated by several of the aforementioned applications, and a wealth of literature now exists for the sparse PCA problem (see Section~\ref{sec:related-works} to follow).
While sparsity is a standard assumption for applications, it is often the case that the desired principal component possesses additional structure.
For example, applications with wavelet decompositions often involve structured sparsity on a suitably defined binary tree \citep{baraniuk2010model}, and path sparsity on the principal component is often a good assumption when the data consists of representative stocks among distinct business sectors \citep{asteris2015stay}. 
%the above estimator of the PCA cannot consistently recover the population/true PC in the high-dimensional setting $(d \gg n)$. Thus, additional structures have been imposed on the classical PCA problem to overcome the above inconsistency. \\
%
%PCA and its generalizations have numerous applications, such as human face recognition \citet{hancock1996face, tran2020tensor}, eigen-genes selection/shaving \citet{alter2000singular,hastie2000gene,feng2019supervised}, handwritten classification \citet{hastie2009elements}, clustering of functional connectivity \citet{frusque2019sparse}, single-cell RNA sequencing analysis \citet{wang2021manifold}, and so on.
%Numerous such structures have been proposed and extensively studied in the past few decades (see Section~\ref{sec:related-works} for a detailed literature review). %Nevertheless, most of these results are merely restricted to one specific type of structure. \\
%

Motivated by such applications, our goal is to unify a family of structured PCA problems by viewing them simultaneously through a statistical and computational lens. The structure that we consider (to be formally introduced in Section~\ref{sec:setting-background}) is a class of union-of-linearly-structured models, including vanilla sparse PCA and path/tree sparse PCA as special cases. As articulated by the title of this paper, our goal is to understand if and to what extent the insights developed for vanilla sparsity extend to these structured settings.
%In this paper, we study a generalized PCA problem in which the goal is to recover a PC with an underlying linear structure from noisy observations/samples. Here, the linear
 %The contributions of this paper are listed below. 

\subsection{Contributions and organization.}
In Section~\ref{sec:setting-background}, we formally introduce a family of 
%\apcomment{replace ``linearly structured with union of linearly structured'' everywhere. In a footnote, explain why you don't just call it union-of-subspaces.} 
\emph{union of linearly structured} PCA problems under the spiked Wishart model. Section~\ref{sec:general-results} presents our main results:
We begin by studying the fundamental limits of estimation under this model, providing both upper and lower bounds on the $\ell_2$ error of estimation that depend on the geometry of the problem.
%We give the general upper bound of the $\ell_2$-norm error rate of the estimator obtained from solving a non-convex optimization problem \eqref{eq:linearly-structured-PCA}. While samples are i.i.d. from the Wishart model with the ground truth satisfying the linear structure condition, the proposed estimator recovers the ground truth within a statistical error. We provide the minimax lower bound of the statistically optimal estimator (Theorem~\ref{thm:minimax-lb}) for the linearly structured PCA, in which the lower bound depends only on sample dimension $d$, size of collection $\mathcal{L}$, and local structure of each linear subspace. 
Our upper bound is achieved by an exhaustive search algorithm, and we analyze a natural projected power method to approximately compute its solution. We show that this iterative method enjoys local geometric convergence to within a neighborhood of the ground truth solution that attains the optimal statistical rate as a function of the sample size and geometry of the problem instance. We also present a general initialization algorithm for this method.
%provided a certain ``replacement property" (see Proposition~\ref{prop:equ-update}) holds. \gwcomment{Remove the statement of ``replacement property".}
%if an exact projection oracle is accessible. 
% We don't need to talk about PGD here.
%\apcomment{Please rewrite the orange sentence. What does it mean?} 
%\gwcomment{Do you mean the next sentence $\Rightarrow$?} 
%\gwcomment{I think we used to mean that: Compared with the convergence analysis provided in \citet{chen2015fast}, our convergence analysis of this PPM does not require the convexity on the feasible region.} 
%{\color{orange}Compared with the projected gradient method, the projected power method ensures local geometric convergence without the convexity assumption on the feasible set.}
%
%
% To the best of our knowledge, such geometric convergence occurs without the convexity assumption on the problem and is not enjoyed by the classical projected gradient method. 
In Section~\ref{sec:specific-examples}, we study two prototypical examples of structured PCA---those given by path and tree sparsity---in an end-to-end fashion, additionally providing explicit initialization methods and evidence of computational hardness (see in particular Propositions~\ref{prop:reduction-PathPCA} and~\ref{prop:TS-SDP-hard}). Overall, our results reveal that several features of vanilla sparse PCA---on both the statistical and computational fronts---extend in natural ways to its structured counterparts.

% \paragraph{Organization.} The remaining paper is organized as follows. Section~\ref{setting-background} formally proposes the problem setting of linearly structured PCA, compares the proposed setting with the structured sparse PCA, provides two typical examples of linearly structured PCA, and gives notations used in the paper. Section~\ref{sec:general-results} presents the general results in this paper. In particular, Section~\ref{sec:fund-limits} studies the fundamental limits (general upper bound of estimator and minimax lower bound) for linearly structured PCA. Section~\ref{sec:PPM-EM} introduces a variant of the projected power method for the linearly structured PCA and shows the locally geometric convergence when an exact projection oracle is accessible. Finally, Section~\ref{sec:specific-examples} provides end-to-end analysis for two typical examples of linearly structured PCA, i.e., tree-sparse PCA and path-sparse PCA, respectively. The end-to-end analysis for each example includes the study of fundamental limits, initialization methods, corresponding performances of the projected power method, and computational hardness results to complete the story. 

\subsection{Related work} \label{sec:related-works}
Structured PCA has been studied extensively over the past two decades, and we cannot hope to cover this vast literature here. We discuss the papers most relevant to our results.

\paragraph{Optimization algorithms for sparse PCA.} The most commonly used and studied structural assumption in PCA is (vanilla) sparsity, in which the true principal component is assumed to be $k$-sparse. Letting $\widehat{\bm{\Sigma}} := \frac{1}{n} \sum_{i = 1}^n \bm{x}_i \bm{x}_i^{\top}$ denote the sample covariance matrix, such a sparse principal component can be found by solving the following optimization problem: 
\begin{align}
    \max_{\bm{v} \in \mathbb{R}^d} ~ \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} ~~\text{s.t.}~~ \|\bm{v}\|_2 = 1, \|\bm{v}\|_0 \leq k, \label{eq:sparse-PCA}
\end{align} 
where $\| \cdot \|_0$ denotes the $\ell_0$ norm or number of nonzeros. This program was first proposed by \cite{cadima1995loading}. In contrast to classical PCA (which is akin to program~\eqref{eq:sparse-PCA} but without the $\ell_0$ norm constraint), solving the sparse PCA problem \eqref{eq:sparse-PCA} is NP-hard. Many computationally efficient reformulations of sparse PCA have been proposed over the years. \cite{jolliffe2003modified} give the first computational tractable method---termed SCoTLASS---which reformulates the program~\eqref{eq:sparse-PCA} using an $\ell_1$-norm regularization akin to the LASSO \citep{tibshirani1996regression}. \cite{zou2005regularization} and \cite{ zou2006sparse} propose an ElasticNet version of SPCA, and \cite{witten2009penalized} study connections between SCoTLASS and ElasticNet SPCA. \cite{erichson2020sparse, chen2020alternating} propose alternative formulations and show the convergence of their alternating gradient methods to stationary points. Another approach focuses on convex relaxations of sparse PCA. For example, \cite{d2004direct, zhang2012sparse, vu2013fantope, d2014approximation, kim2019convexification} consider a convex relaxation by lifting the variable space $\bm{v} \in \mathbb{R}^d$ to its product space, and relax to a semidefinite programming problem. More recently, \cite{dey2021using, dey2020solving, li2020exact} provide a more computationally scalable type of convex relaxation for problem~\eqref{eq:sparse-PCA} using mixed-integer programming with theoretical worst-case guarantees. Other than methods based on convex relaxation, there is also a substantial literature on specialized iterative algorithms for finding good feasible solutions. Examples include the deflation method~\citep{mackey2008deflation}, generalized power method~\citep{journee2010generalized}, truncated power method~\citep{yuan2013truncated}, and iterative thresholding~\citep{ma2013sparse}. 
%\apcomment{Why do you call these heuristic? Many come with guarantees} \gwcomment{Replace heuristics with algorithms}. When the sample covariance matrix $\widehat{\bm{\Sigma}}$ has a fixed low rank, \cite{papailiopoulos2013sparse, asteris2015sparse, del2022sparse} present exact combinatorial algorithms which guarantee to find the global optimal solution of sparse PCA with computational complexity polynomial in dimension $d$, but exponential in $\text{rank}(\widehat{\bm{\Sigma}})$. 
%\apcomment{Expand on statistical results; these only hold under a generative model for your data.} \gwcomment{Will add later} 



\paragraph{Statistical and computational limits of sparse PCA.} 

%There are several papers that study the statistical limits of vanilla sparse PCA. 
%For instance, \cite{birnbaum2013minimax, cai2013sparse, vu2013minimax} provide minimax lower bounds for sparse PCA. 

	Several papers have established (by now classical) minimax lower bounds for sparse PCA in a purely statistical sense, i.e., without computational considerations. Examples for vector recovery in $\ell_2$ norm include \cite{birnbaum2013minimax} and \cite{cai2013sparse}; the latter is phrased in terms of estimating the principal subspace and considers a more general model than the rank-1 model. \cite{vu2013minimax} present nonasymptotic lower and upper bounds for the minimax risk considering both row-sparse and column-sparse principal subspaces. \cite{amini2008high} study the rank-1 spiked covariance model considered here, but establish minimax lower bounds for support recovery. 

	%\cite{lesieur2015phase} focuses on the approximate message passing method, showing its asymptotic optimality for a special rank-one case but failure for larger ranks due to phase transition phenomena. \cite{cai2015optimal} investigates both the minimax lower bounds on vector recovery and the minimax rank detection, which generalizes the rank-one results in \cite{berthet2013optimal}. \cite{banks2018information} derives upper and lower bounds on phase transition thresholds for clustering, sparse PCA, and submatrix localization. Most recently, \cite{macris2020all} provides statistical limits for the all-or-nothing phase transition in sparse spiked matrix estimation, which includes sparse PCA as a special case.
	

%	{\color{orange}For instances, on the side of vector recovery, \cite{birnbaum2013minimax} establishs a minimax lower bound on vector recovery in $\ell_2$ norm under various statistical models for the sparse PCA. \cite{cai2013sparse} provides the so-called rate-sharp minimax lower bounds for estimating the principal subspace under the $\ell_2$ loss. 
%	
%	\cite{vu2013minimax} presents nonasymptotic lower and upper bounds in $\ell_2$-norm loss for estimation of both row sparse and column sparse principal subspaces. 
%	
%	Beyond the vector recovery, \cite{amini2008high} studies the semidefinite relaxation for sparse PCA problem under high-dimensional spiked covariance model, and proposes a phase transition (information-theoretic limitations) result in probability for support recovery. Later, \cite{lesieur2015phase} focuses on approximate message passing (AMP) method. This paper shows that AMP is asymptotically optimal for a special rank-one case, but fails for large rank case due to a series of phase transitions. 
%	
%	Then \cite{cai2015optimal} investigates both the minimax lower bounds on vector recovery and the minimax rank detection, which generalize the special case of rank on proposed in \cite{berthet2013optimal}. \cite{banks2018information} derives upper and lower bounds on phase transition thresholds of detecting problem for clustering, sparse PCA, and submatrix localization. A recent study \cite{macris2020all} provides statistical limits for the sharp all-or-nothing phase transition in sparse spiked matrix estimation, which  includes sparse PCA as a special case.  
%	}

%\apcomment{There are many ways one could provide minimax lower bounds here. Are they providing lower bounds on sparsity pattern recovery or vector recovery in $\ell_2$ norm? Let us be more detailed and add more references if necessary.} \gwcomment{The above three papers \cite{birnbaum2013minimax, cai2013sparse, vu2013minimax} are all about lower bounds on vector recovery in $\ell_2$ norm. The newly added one \cite{cai2015optimal} has something new for the rank detection problem of sparse PCA. But actually, I did not find minimax lower bounds on sparsity pattern recovery. May I have some related references about this topic?} \gwcomment{Add \cite{amini2008high} to literature review, as a reference for phase transition of sparse PCA.}


Sparse PCA has also been a key cog in the study of \emph{computational} lower bounds in high dimensional statistics problems, and has received a lot of attention from the perspective of reductions, sum-of-squares and low-degree lower bounds, as well as approaches rooted in statistical physics; let us cover a non-exhaustive list of examples here. 
%\apcomment{Needs to be better organized into low degree, reductions, AMP, SOS. I will do this later.} 
Assuming the planted clique conjecture,~\cite{berthet2013complexity} show that a sub-Gaussian variant of sparse PCA is hard, in that the optimal rate of estimation is not achievable in polynomial time. %and gives a computationally theoretic lower bound. 
\citet{ma2015sum} show degree-4 sum of squares lower bounds for $k$-sparse PCA %still requires $n \geq \tilde{O}(k^2)$ samples to solve the detection problem 
(see Section~\ref{sec:TS-PCA}). 
%as semi-definite relaxations for sparse PCA. 
\cite{zdeborova2016statistical} study the fundamental statistical-computational barriers of inference and estimation problems as phase transitions and develop new algorithms using techniques from statistical physics.
%} 
\cite{wang2016statistical} show computational lower bounds for estimation for a distributionally-robust variant of sparse PCA. \cite{gao2017sparse} show computational lower bounds for sparse PCA in the spiked covariance model, and \cite{brennan2018reducibility} provide an alternative reduction based on random rotations to strengthen these lower bounds. 
%\cite{ding2019subexponential} propose a method for the construction of estimators for generalized degrees of freedom (GDF) and prediction error using approximate message passing (AMP) algorithms. The authors further show that the model selected by the proposed estimator is close to that which minimizes the true prediction error. 
%\citet{kunisky2019notes} study the low-degree method towards understanding statistical-versus-computational tradeoffs in high-dimensional inference problems. Later,  
\citet{ding2019subexponential} explore subexponential-time algorithms for sparse PCA, and give rigorous evidence that their proposed algorithm is optimal by analyzing the low-degree likelihood ratio.
%from \cite{gao2017sparse} to hold under the spiked covariance model. 
\cite{brennan2019optimal} give a reduction from planted clique that yields the first complete characterization of the computational barrier in the spiked covariance model, providing tight lower bounds at all sparsities $k$. 
%Under a generalization of the planted clique conjecture based on ``secrete leakage",~\cite{brennan2020reducibility} deduce tight statistical-computational tradeoffs for a diverse range of problems, including sparse PCA, and introduces several new average-case reduction techniques that reveal novel connections to other designs with combinatorial structures. 

%\apcomment{You are not covering several other papers that address computational hardness. E.g. lower bounds on SOS relaxations, low-degree predictions, predictions based on approximate message passing and statistical physics. This is important since you build later on both reductions and SDP lower bounds.} \gwcomment{Got it, added some references to this part, need to check again.} \gwcomment{Should we add more references for the above paragraph?} 


\paragraph{Structured PCA and related problems.} While vanilla sparsity (and the resulting sparse PCA problem) is by far the most well-studied, there also exist other examples of structure one could impose. Examples from the literature on sparse linear regression include graph sparsity~\citep{hegde2015nearly}, group sparsity structure~\citep{yuan2006model}, block and tree sparsity \citep{baraniuk2010model}, and subspace constraints \citep{bie2004learning}. For PCA in particular, several structural constraints have been studied, such as non-negative orthant cone structure \citep{montanari2015non}, and general cone structure \citep{deshpande2014cone, yi2020non}. \citet{asteris2015stay} study path-sparse structure in the PCA problem. 
Some of these papers study fundamental limits of estimation for their specific forms of structure, and \citet{asteris2015stay} and \cite{yi2020non} propose specialized projected power methods.
%and present fundamental limits of estimation. Although a projected power method is presented in this paper, they do not prove any results on the convergence properties of this estimator nor results on its statistical performance. %\apcomment{I don't understand the following sentence.} 
% \cite{yi2020non}, unlike \cite{deshpande2014cone} in a spiked Gaussian Wigner model, proposes a convex cone projected power method for the non-sparse PCA to recover the true PC within an error comparable with their minimax lower bound under the spiked covariance Wishart model. 
%{\color{orange} In contrast with the spiked Gaussian Wigner setting of \cite{deshpande2014cone}, 
%\cite{yi2020non} study the non-sparse PCA problem and propose a convex cone projected power method which ensures the local geometric convergence to the statistically near-optimal solution.}
\cite{cai2021optimal} present a unified framework for the statistical analysis of structured principal subspace estimation and lower and upper bounds on the minimax risk. In recent work,~\cite{liu2021generative} study structured PCA under the assumption that the true principal component is generated from an $L$-Lipschitz continuous generative model, showing that the projected power method enjoys local geometric convergence. While their result is related in spirit to a subset of our results on the projected power method, our structural assumptions are different (see Definition~\ref{cond:linear-structure} and discussions following Theorem~\ref{thm:convergence}) for a detailed comparison. 
%\apcomment{and discussion following theorem XXX for a detailed comparison} \gwcomment{Done}). 
There are also papers that study computational hardness in structured settings, both from the perspective of low-degree polynomials~\citep{bandeira2019computational} and reductions from the so-called ``secret-leakage'' variant of the planted clique conjecture~\citep{brennan2020reducibility}. Our work adds to this literature for a particular family of structured PCA problems.
%\apcomment{There is also a paper by Bandeira/Wein/Kunisky that studies low-degree lower bounds for sparse PCA under a general prior?}\gwcomment{Added this reference to Computational hardness of Sparse PCA.} 




% PCA has a great many applications in the field of statistics and machine learning such as neuroscience, quantitative finance, image compression, facial recognition, and so on. 
