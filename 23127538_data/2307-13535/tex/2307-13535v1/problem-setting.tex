%!TEX root = main-paper-template.tex


\section{Problem setting, background, and examples}\label{sec:setting-background}

Throughout this paper, we operate under the spiked Wishart model. Assume that our data set consists of $n$ i.i.d. samples $\{\bm{x}_i\}_{i = 1}^n$ drawn from a $d$-dimensional Gaussian distribution with zero-mean and covariance
\begin{align}
    \bm{\Sigma} := \lambda \bm{v}_* \bm{v}_*^{\top} + \bm{I}_{d \times d}. \notag
\end{align}
For brevity, we use $\mathcal{D}(\lambda; \bm{v}_*) := \mathcal{N}(\bm{0}_d, \lambda \bm{v}_* \bm{v}_*^{\top} + \bm{I}_{d \times d})$ to denote the distribution of each $\bm{x}_i$.
Here $\lambda > 0$ represents the \emph{strength of the signal}, and $\bm{v}_*$ is a $d$-dimensional, unit-norm \emph{ground truth} vector that we wish to estimate. In addition to the unit norm condition, we also assume the inclusion $\bm{v}^* \in \mathcal{M}$, where $\mathcal{M}$ is a known union of subspaces satisfying a certain \emph{union of linear structures} assumption defined below.


\begin{definition} \label{cond:linear-structure}
\textbf{Union of linear structures condition.} Let $\mathcal{B} := \{\phi_1, \ldots, \phi_d\}$ be an orthonormal basis of $\mathbb{R}^d$ and $\mathcal{L} := \left\{ L_1, \ldots, L_M \right\}$ be a collection of $M$ distinct linear subspaces such that for each $m \in [M]$, we have $L_m = \text{span}(\mathcal{B}_m)$ for some $\mathcal{B}_m \subseteq \mathcal{B}$. We say set $\mathcal{M}$ obeys the union of linear structures condition if
\begin{align*}
    \mathcal{M} := \bigcup_{m = 1}^M L_m,
\end{align*}
i.e., $\mathcal{M}$ is the union of all linear subspaces in $\mathcal{L}$.
\end{definition}

% \apcomment{deleted a paragraph here; will move to later.}
%\iffalse
%By a sufficiency argument, the problem can be phrased as one of estimating $\bm{v}_*$ from the sample covariance matrix $\widehat{\bm{\Sigma}} = \frac{1}{n} \sum_{i =1}^n \bm{x}_i \bm{x}_i^\top$. We use 
%\begin{align}
%    \widehat{\bm{v}}_{\mathsf{ES}} := \argmax_{\bm{v}} ~  \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} ~~\text{s.t.}~~ \|\bm{v}\|_2 = 1, ~~ \bm{v} \in \mathcal{M} \label{eq:linearly-structured-PCA}
%\end{align} 
%to denote the estimator of $\bm{v}_*$ from exhaustive search by seeking the leading principal component within the set $\mathcal{M}$.
%\fi

%\apcomment{Put notation for identity matrix in background notation.} \gwcomment{Done.}
%In short, define above distribution as $\mathcal{D}(\lambda; \bm{v}_*) := \mathcal{N}(\bm{0}_d, \bm{\Sigma}) = \mathcal{N}(\bm{0}_d, \lambda \bm{v}_* \bm{v}_*^{\top} + \bm{I}_{d \times d})$. 

% As alluded to in the introduction, we also place a structural assumption on $\bm{v}_*$ in addition to a unit-norm constraint. We call this the \emph{linear structure condition} for the rest of the paper. 
%\apcomment{I think a more natural way to say this is that $\bm{v}_*$ is in the union-of-subspaces $\mathcal{M} = \cup_{i = 1}^M L_i$ and $\mathcal{M}$ satisfies the linear structure condition. Overall, $\mathcal{M}$ is an important object and should be defined up front.} \gwcomment{Update the linear structure condition wrt $\mathcal{M}$ as your comments.}
%The linearly structured PCA is a generalized PCA which assumes that the true PC $\bm{v}_*$ (vector with ``hidden data'') is a unit $\ell_2$-norm vector and satisfies the below-mentioned \emph{linear structure condition}. 

% \begin{definition} \label{def:wishart-model}
% \textbf{Wishart Model.} We say $\{\bm{x}_i\}_{i = 1}^n$ satisfies the Wishart Model if and only if the samples are i.i.d. generated from the $d$-dimensional Gaussian distribution with zero-mean and covariance
% \begin{align}
%     \bm{\Sigma} := \lambda \bm{v}_* \bm{v}_*^{\top} + \bm{I}_{d \times d}, \notag
% \end{align}
% where $\lambda$ represents the strength of the signal, $\bm{v}_*$ satisfies the linear structure condition~\ref{cond:linear-structure}, and $\bm{I}_{d \times d}$ denotes the $d$-by-$d$ identity matrix. In short, define above distribution as $\mathcal{D}(\lambda; \bm{v}_*) := \mathcal{N}(\bm{0}_d, \bm{\Sigma}) = \mathcal{N}(\bm{0}_d, \lambda \bm{v}_* \bm{v}_*^{\top} + \bm{I}_{d \times d})$. 
% \end{definition}

% removed this paragraph because the optimization problem comes later, when you prove guarantees for it.

% Therefore, the goal of the linearly structured PCA is to estimate the unknown true PC $\bm{v}_*$ via a given set of noisy samples $\{\bm{x}_i\}_{i = 1}^n$ generated from the Wishart Model with linear structure condition \ref{cond:linear-structure}. In order to estimate $\bm{v}_*$, it is natural to consider the following optimization problem~\eqref{eq:linearly-structured-PCA}
% \begin{align}
%     \begin{array}{rlll}
%         \widehat{\bm{v}} := \argmax_{\bm{v}} & \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v}  \\
%         \text{s.t.} & \|\bm{v}\|_2 = 1 \\
%         & \bm{v} \in \mathcal{M} := \bigcup_{m = 1}^M L_m
%     \end{array} \label{eq:linearly-structured-PCA}
% \end{align}
% and use the optimal solution $\widehat{\bm{v}} = \widehat{\bm{v}}(\bm{x}_1, \ldots, \bm{x}_n)$ as an estimator for the unknown true PC $\bm{v}_*$, where $\widehat{\bm{\Sigma}} := \frac{1}{n} \bm{X} \bm{X}^{\top}$ is the sample covariance matrix with $\bm{X} := (\bm{x}_1 ~|~ \cdots ~|~ \bm{x}_n) \in \mathbb{R}^{d \times n}$, and $\mathcal{M} := \bigcup_{m = 1}^M L_m$ is the union of linear subspaces $L_1, \ldots, L_M \in \mathcal{L}$ as given in the linear structure condition~\ref{cond:linear-structure}.

\begin{remark} \label{rem:eq-structured}
It is worth noting that the union of linear structures condition in Definition~\ref{cond:linear-structure} resembles a structured sparsity condition. Indeed, using the rotation invariance of the Gaussian distribution, the problem of estimating $\bm{v}_*$ from observations $\{\bm{x}_i\}_{i = 1}^n$ is \emph{statistically} equivalent to estimating the structured-sparse vector $\bm{\Phi}^\top \bm{v}_*$ from $\{ \bm{\Phi}^\top \bm{x}_{i} \}_{i = 1}^n$, where $\bm{\Phi} \in \mathbb{R}^{d \times d}$ is an orthonormal matrix with columns $\phi_1, \ldots, \phi_d$. However, the two problems may not be \emph{computationally} equivalent when $\bm{\Phi}$ is unknown. We provide an example in Appendix~\ref{app:time-consuming-case} to illustrate that if an efficient projection oracle onto the union of subspaces $\mathcal{M}$ is accessible, then it is more computationally efficient to estimate the vector $\bm{v}_*$ directly, rather than to estimate $\bm{\Phi}^\top \bm{v}_*$ from $\{ \bm{\Phi}^\top \bm{x}_{i} \}_{i = 1}^n$ by first computing $\bm{\Phi}$. Accordingly, the rest of the paper assumes that $\bm{\Phi}$ is unknown, and that we have access to a projection oracle onto the union of subspaces $\mathcal{M}$.
\end{remark}
%The attentive reader may notice that the linear structure condition in Definition~\ref{cond:linear-structure} resembles a structured sparsity condition. 
%	Indeed, using the rotation invariance of the Gaussian distribution, estimating $\bm{v}_*$ from observations $\{\bm{x}_i\}_{i = 1}^n$ is statistically equivalent to estimating the structured-sparse vector $\bm{\Phi}^\top \bm{v}_*$ from $\{ \bm{\Phi}^\top \bm{x}_{i} \}_{i = 1}^n$ with $\bm{\Phi} \in \mathbb{R}^{d \times d}$ the orthonormal matrix with columns $\phi_1, \ldots, \phi_d$. While the above two problems are \emph{statistically} equivalent when $\bm{\Phi}$ is known, they may not be \emph{computationally} equivalent when $\bm{\Phi}$ is unknown. To illustrate, we provide an example (see Example~\ref{exmp:time-consuming-case} of Appendix~\ref{app:time-consuming-case}) in which the linear structure vector $\bm{v}_*$ can be estimated  faster than estimating $\bm{\Phi}^\top \bm{v}_*$ from $\{ \bm{\Phi}^\top \bm{x}_{i} \}_{i = 1}^n$ without computing $\bm{\Phi}$, if an efficient projection oracle onto the union of subspaces $\mathcal{M}$ is accessible. Accordingly, the rest of this paper assumes the linear structure condition in Definition~\ref{cond:linear-structure} with a general, unknown $\bm{\Phi}$, combined with an efficient, known projection oracle. 



%{\color{orange}
% Attentive readers may note that the linear structure condition in Definition~\ref{cond:linear-structure} resembles a sparsity setting if the size of $\mathcal{B}_m$ is upper bounded by some fixed sparsity parameter $k$ for all $1\leq m\leq M$. Indeed, the transformed ground truth $\bm{\Phi}^\top \bm{v}_*$ is at most $k$-sparse. Using the rotation invariance of the Gaussian distribution, it is \emph{statistically} equivalent to estimate the sparse vector $\bm{\Phi}^\top \bm{v}_*$ from observations $\{ \bm{\Phi}^\top \bm{x}_{i} \}_{i = 1}^n$. Hence, once one can compute $\bm{\Phi}$ exactly, the linear structure setting can be reduced to the sparsity setting. However, computing $\bm{\Phi}$ is \emph{computationally} more expensive than using the projection oracle (see Definition~\ref{assump:exact-proj}) under some cases. To illustrate, we provide an Example~\ref{exmp:time-consuming-case} which shows that the running time of computing $\bm{\Phi}$ is $O(d^{3})$ while the projection oracle takes $O(d^{2})$. Combined with the results on local convergence of the proposed method (Theorem~\ref{thm:convergence}), the above illustration indicates that $\bm{v}_*$ can be more efficiently estimated via proposed method than computing $\bm{\Phi}$. Accordingly, the rest of this paper assumes the linear structure condition in Definition~\ref{cond:linear-structure} with general, unknown $\bm{\Phi}$.} \apcomment{What exactly does this mean? What do you have oracle access to?} \gwcomment{Modified the above two sentences, please have a check.}

% Indeed, every linear subspace $L_m \in \mathcal{L}$ is the span of a subset of vectors in $\mathcal{B}$ with $L_m = \textup{span}(\mathcal{B}_m)$ for some $\mathcal{B}_m \subseteq \mathcal{B}$. Let $\bm{\Phi} = [\phi_1\;|\cdots|\;\phi_{d}] \in \mathbb{R}^{d \times d}$.


% While the two problems are statistically equivalent when $\bm{\Phi}$ is known, they may not be equivalent \emph{computationally} when $\bm{\Phi}$ is unknown. To illustrate, we provide an example of an instance (in Example~\ref{exmp:time-consuming-case} of Appendix~\ref{app:time-consuming-case}) {\color{orange}that runs much faster than computing the matrix $\bm{\Phi}$ exactly, once the projection oracle (step 4 of Algorithm~\ref{alg:PPM}) is efficient\footnote{Actually, in some typical examples, such exact projection is very fast.}.}
%in which a projection oracle onto the union of subspaces $\mathcal{M}$ can be implemented much faster than the time taken to find the matrix $\bm{\Phi}$.
%Combined with our results in the sequel showing local convergence of the projected power method, this indicates that $\bm{v}_*$ can be recovered without having to compute $\bm{\Phi}$. 

%By the rotation invariance of the Gaussian distribution, it is thus statistical
% It is easy to observe that if the unknown basis $\bm{\Phi}$ is achievable from $\mathcal{L}$, by rotation invariance of the Gaussian distribution, the linear structure assumption~\ref{cond:linear-structure} is equivalent to the structured sparsity. Thus one natural question is: 

%\apcomment{Please move the following two paragraphs with your time consuming example to an appendix.}\gwcomment{Sure, moved the following discussions to the appendix.}
% \begin{center}
%     % \emph{Whether $\bm{\Phi}$ necessary to know for the projection $\Pi_{\mathcal{M}}$ in the projected power method?} 
%     \emph{Whether $\bm{\Phi}$ necessary to recover the true PC $\bm{v}_*$ for linearly structured PCA?}
% \end{center}
% The answer to the above question is No in two parts.
% In the first part, as presented later, the convergence result of the projected power method does not need to know $\bm{\Phi}$. In the second part, we further claim that, in some of the cases, computing $\bm{\Phi}$ from $\mathcal{L}$ is much more time-consuming compared with computing a projection onto the set $\mathcal{M}$.\\ 


\subsection{Examples of linearly structured PCA}

Clearly, vanilla sparse PCA is covered by our formulation. We instantiate the union of linear structures assumption with two other examples.

\subsubsection{Example 1: Tree-Sparse PCA} \label{sec:TS-PCA-intro}

Motivated by applications in signal and image processing and computer graphics \citep{baraniuk2010model}, a particular model for the underlying signal is \emph{tree sparsity} in an underlying basis. In particular, consider the following simplified model for tree-sparsity with one-dimensional signals and binary wavelet trees as a typical such instance. We require some notation to introduce it formally.

%Wavelet decompositions are widely-used in data compression, signal/image processing, pattern recognition, and computer graphics \citep{baraniuk2010model}. In addition, large coefficients of the wavelet basis in the above applications cluster along the branches of trees due to the multi-scale nature of wavelets. Such property motivates the requirement of connected tree sparsity structures for the wavelet coefficients. {\color{orange}In this example, we focuses on the following \emph{tree sparsity structure} (with one-dimensional signals and binary wavelet trees) as a typical instance to decompose a covariance matrix constructed from wavelet representations of images.} 
%As illustrated in \citet{baraniuk2010model}, this paper focuses on one-dimensional signals and binary wavelet trees. Then we study the following \emph{tree sparsity structure} as a typical instance to decompose a covariance matrix constructed from wavelet representations of images.

% Using the fact that the large coefficients form connected trees leads to performance improvements when working with wavelet representations of images. Another example of tree sparsity occurs in genomics, in which features in a supervised learning problem can be arranged in a hierarchy \citep{kim2012tree}. 

% As illustrated in \citet{baraniuk2010model}, in this paper, we also focus on one-dimensional signals and binary wavelet trees, but still the results for tree-sparsity extend directly to $d$-dimensional signals and $2^d$-ary wavelet trees. 
\iffalse
\apcomment{Need to be more specific about why this makes sense in the PCA context. Are you trying to decompose a covariance matrix constructed from images?} \gwcomment{Add some discussions on why using this tree sparsity structure.}
\fi

%\paragraph{Tree Sparsity Structure and Tree-Sparse PCA.}
Given a natural number $h$, a \textit{complete binary tree} or $\mathsf{CBT}$ of size $d = 2^h - 1$ is given by the following construction. 
%\apcomment{You are using $L$ for your subspaces. Use different notation.} \gwcomment{Replace $L$ by $h$.} a fixed integer parameter. In particular, the complete binary tree $\text{CBT}$ has
Create $h$ levels $\{1, \ldots, h\}$, with $2^{\ell - 1}$ nodes in $\ell$-th level. Index each node from $1$ to $d$, top to bottom and left to right in the following way. The root node $r_{\mathsf{CBT}}$ of $\mathsf{CBT}$ has index $1$, and for any node with index $i \in \{2, \ldots, 2^{h - 1} - 1\}$, its parent is the node with index $\lfloor \frac{i}{2} \rfloor$ and its children are the nodes with indices $2i, 2i + 1$. Define the collection of vertex sets
\[
\mathcal{T}^k := \left\{T: |T| = k, ~ \text{root node }1 \in T, ~ \text{the subgraph of $\mathsf{CBT}$ induced by $T$ is connected} \right\}.
\]
Abusing notation slightly, consider a bijection between the coordinates of any $d$-dimensional vector and the vertices of a $\mathsf{CBT}$.
The vector $\bm{v}_*$ is said to be $k$-\emph{tree-sparse} if $\mathsf{supp}(\bm{v}_*) \in \mathcal{T}^k$.

%where, for any binary subtree $T$, we use $\text{supp}(T)$ to denote the set of vertex index (i.e., support set) in $T$. \\

Therefore, tree-sparse PCA is a specific example of union of linear structures in our formulation. To see this, let $\bm{e}_i \in \mathcal{S}^{d - 1}$ denote the $i$-th standard basis vector in $\mathbb{R}^d$, and set
\begin{align*}
    & ~ \mathcal{B} := \{\bm{e}_1, \ldots, \bm{e}_d\}, \text{ and } ~ \mathcal{L} := \left\{L = \mathsf{span}(\{\bm{e}_i\}_{i \in T}) ~|~ T \in \mathcal{T}^k \right\}
\end{align*}
in Definition~\ref{cond:linear-structure}. 
% More generally, the linear structure condition allows tree-sparsity to exist in any (possibly unknown) basis.

\subsubsection{Example 2: Path-Sparse PCA} \label{sec:PS-PCA-intro}

Another commonly used variant of linearly structured PCA is path-sparse PCA~\citep{asteris2015stay}, in which the support set of $\bm{v}_*$ forms a path on an underlying directed acyclic graph $G = (V,E)$. For a vertex $v$ in this graph, let $\delta_{\text{out}}(v)$ denote the out-neighborhood of $v$.

\begin{definition}
\textbf{$(d,k)$-Layered Graph.} 
A directed acyclic graph $G = (V,E)$ is a $(d,k)$-layered graph if
\begin{itemize}
    \item
     $V = \{v_s, v_t\} \cup \widetilde{V}$ such that $|\widetilde{V}| = d-2$ and $v_s,v_t \notin \widetilde{V}$.
    \item $\widetilde{V} = \cup_{i=1}^{k} V_i$ where $V_i \cap V_j = \emptyset$ for all $i \neq j \in [k]$ and $|V_1| =  \cdots =  |V_k| = \frac{d - 2}{k}$. 
    \item $\delta_{\text{out}}(v) = V_{i + 1}$ for all $v \in V_{i}$ and $i = 1, \ldots, k - 1$, and 
        \item $\delta_{\text{out}}(v_s) = V_1$ and $\delta_{\text{out}}(v) = \{v_t\}$ for all $v \in V_k$. 
\end{itemize}
\end{definition}

%\paragraph{Path Sparsity Structure and Path-Sparse PCA.} 
Let $G = (V,E)$ be a $(d,k)$-layered graph and we define the collection of vertex sets
\begin{align}
 \mathcal{P}^k := \big\{ P \subseteq V ~|~ v_s,v_t \in P \text{ and } |P\cap V_i|=1 \text{ for any }i\in [k]\big\}. 
\end{align}
Once again, we consider the natural bijection between the coordinates of any $d$-dimensional vector and the vertices of a $(d, k)$-layered graph, and a vector $\bm{v}_*$ is said to be $k$-\emph{path-sparse} if $\mathsf{supp}(\bm{v}_*) \in \mathcal{P}^k$. It is straightforward to see that the set of all $k$-\emph{path-sparse} vectors satisfies the union of linear structures condition in Definition~\ref{cond:linear-structure} with 
\begin{align*}
    & ~ \mathcal{B} := \{\bm{e}_1, \ldots, \bm{e}_d\}, \text{ and } ~ \mathcal{L} := \left\{L = \mathsf{span}(\{\bm{e}_i\}_{i \in P}) ~|~ P \in \mathcal{P}^k \right\}.
\end{align*}
%\apcomment{Specify $\mathcal{B}$ and $\mathcal{L}$?} \gwcomment{Done.}

\subsection{Notation} \label{sec:notations}
We use $\bm{I}_{d \times d}$ to denote the $d$-by-$d$ identity matrix, and $\lambda_i(\bm{M})$ to denote the $i$-th largest eigenvalue of a symmetric matrix $\bm{M}$. 
%Let $\bm{P}_L \in \mathbb{R}^{d \times d}$ be the projection matrix onto a subspace $L$. 
We use $\bm{X} := [\bm{x}_1 ~|~ \cdots ~|~ \bm{x}_n]^{\top} \in \mathbb{R}^{n \times d}$ to denote the sample matrix where the $i$-th row of $\bm{X}$ is the $i$-th sample $\bm{x}_i$. The sample covariance matrix is given by $\widehat{\bm{\Sigma}} := \frac{1}{n} \bm{X}^{\top} \bm{X}$, and we let 
\begin{align} \label{eq:noise-def}
\bm{W} := \widehat{\bm{\Sigma}} - \bm{\Sigma}
\end{align}
denote the $d \times d$ matrix of noise. For any linear subspace $L \subseteq \mathbb{R}^d$ and its projection matrix $\bm{P}_L \in \mathbb{R}^{d \times d}$, we use $\widehat{\bm{\Sigma}}_L := \bm{P}_L^{\top} \widehat{\bm{\Sigma}} \bm{P}_L$ to denote the sample covariance matrix restricted to the subspace $L$. We also use the analogous notation $\bm{\Sigma}_L := \bm{P}_L^{\top} \bm{\Sigma} \bm{P}_L$ and $\bm{W}_L := \bm{P}_L^{\top} \bm{W} \bm{P}_L$. 
%\apcomment{We should define $\mathcal{M}$ in a more prominent way, without needing an optimization 
We index the subspaces $L_1, \ldots, L_M$ in some consistent lexicographic order.
We reserve the notation $\mathcal{M} := \bigcup_{m = 1}^M L_m$ to denote the set containing $\bm{v}_*$, and the notation $L_* \in \{L_{1}, \dots, L_{M}\}$ to denote the specific linear subspace that contains $\bm{v}_*$, with ties broken lexicographically.
% As mentioned in optimization problem~\eqref{eq:linearly-structured-PCA}, we use $\mathcal{M} := \bigcup_{m = 1}^M L_m$ as the union of all possible candidate linear subspaces. 
We let $\mathcal{S}^{d - 1}:= \{\bm{v} \in \mathbb{R}^{d}: \|\bm{v}\|_2 = 1\}$ denote the unit $\ell_2$-sphere in $d$-dimensional Euclidean space. For any subspace $L$, let  
\[
\widehat{\bm{v}}_L := \argmax_{\bm{v} \in \mathcal{S}^{d-1}} \bm{v}^{\top} \widehat{\bm{\Sigma}}_L \bm{v} = \argmax_{\bm{v} \in \mathcal{S}^{d-1} \cap L} \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v}.
\]
be the leading eigenvector of the restricted sample covariance $\widehat{\bm{\Sigma}}_L$. For an arbitrary symmetric matrix $\bm{M} \in \mathbb{R}^{d \times d}$ and set $S \subseteq \mathbb{R}^d$, define the scalar 
\begin{align}\label{eqs:rho}
    \rho(\bm{M}, S) := \max_{\|\bm{v}\|_2 = 1,\bm{v} \in S} \big| \bm{v}^{\top} \bm{M} \bm{v} \big|.
\end{align}
For two sequences of non-negative reals $\{f_n\}_{n \geq 1}$ and $\{g_n\}_{n \geq 1}$, we use $f_n \gtrsim g_n$ to indicate that there is a universal positive constant $C$ such that $f_n \leq C g_n$ for all $n \geq 1$. We also use standard order notation $f_n = O(g_n)$ to indicate that $f_n \lesssim g_n$ and $f_n = \tilde{O}(g_n)$ to indicate that $f_n \lesssim g_n \ln^c n$ for some universal constant $c$. We say that $f_n = \Omega(g_n)$ (resp. $f_n = \tilde{\Omega}(g_n)$) if
$g_n = \Omega(f_n)$ (resp. $g_n = \Omega(f_n)$). We use $f_n = \Theta(g_n)$ (resp. $f_n = \tilde{\Theta}(g_n)$) if $f_n = O(g_n)$ and $f_n = \Omega(g_n)$ (resp. $f_n = \tilde{O}(g_n)$ and $f_n = \tilde{\Omega}(g_n)$). We say that $f_n = o(g_n)$ (resp. $f_n = \tilde{o}(g_n)$) when $\lim_{n \rightarrow \infty} f_n / g_n = 0$ (resp. $\lim_{n \rightarrow \infty} f_n / (g_n \ln^c n) = 0$ for some universal constant $c$). We also use $f_n = \omega(g_n)$ to indicate that $\lim_{n \rightarrow \infty} f_n / g_n = \infty$. Throughout, we use $c, c_1, c_2, \ldots$ and $C, C_1, C_2, \ldots$ to denote universal positive constants, and their values may change from line to line. 

% We say that $f_n = \Omega(g_n)$ (resp. $f_n = \tilde{\Omega}(g_n)$) if $f_n = O(g_n)$ (resp. $f_n = \tilde{O}(g_n)$).


% \paragraph{In Section~\ref{sec:lower-bound},}let $\mathcal{B}(\mathcal{L}) := \{\mathcal{B}_1, \ldots, \mathcal{B}_{M}\}$ be the collection of sub-basis with respect to each linear subspace in $\mathcal{L}$. For each $i \in [d]$, let $\mathcal{B}(\phi_i) := \{\mathcal{B}_m \in \mathcal{B}(\mathcal{L}) ~|~ \phi_i \in \mathcal{B}_m \}$ be the collection of sub-basis which contains $\phi_i$. We use $|\mathcal{B}(\phi_i)|$ to denote the number of sub-basis that contains $\phi_i$ in $\mathcal{B}(\mathcal{L})$. Define $i_* := \argmax_{i \in [d]} |\mathcal{B}(\phi_i)|$ be the index with respect to the maximum size/cardinality among $\mathcal{B}(\phi_1), \ldots, \mathcal{B}(\phi_d)$. Define the set of characteristic vectors $\mathcal{Z}$ as
% \begin{align}
%     \mathcal{Z} := \left\{ \bm{z} \in \{0,1\}^d ~|~ \mathrm{supp}(\bm{z}) \in \mathrm{supp}(\mathcal{B}(\phi_{i_*})) \right\}, \notag
% \end{align}
% where $\mathrm{supp}(\mathcal{B}(\phi_{i_*})) := \left\{ \mathrm{supp}(\mathcal{B}_m) ~|~ \mathcal{B}_m \in \mathcal{B}(\phi_{i_*}) \right\}$ and $\mathrm{supp}(\mathcal{B}_m)$ denotes the support of $\mathcal{B}_m$, i.e., the indices of $\{\phi_1, \ldots, \phi_d\}$ that contained in $\mathcal{B}_m$. Given $\bm{z}, \bm{z}' \in \{0, 1\}^d$, let $\delta_H(\bm{z}, \bm{z}') := |\{i: \bm{z}_i \neq \bm{z}_i' \}|$ be the Hamming distance between $\bm{z}$ and $\bm{z}'$. Under Assumption~A\ref{assump:minimax-assumption}.1 listed below, for any $\xi \in (0,1)$, let $\mathcal{Z}_{\xi} \subseteq \mathcal{Z}$ be a subset of $\mathcal{Z}$ such that $\delta_H(\bm{z}, \bm{z}') > 2 (1 - \xi) k$ for all $\bm{z} \neq \bm{z}' \in \mathcal{Z}_{\xi}$. For any integer $r \geq 0$, let 
% \begin{align}
%     \mathcal{N}_H(\bm{z}; r) := \left\{ \bm{z}' \in \mathcal{Z} ~|~ \delta_H(\bm{z}, \bm{z}') \leq r \right\} \notag
% \end{align}
% be the neighborhood of $\bm{z}$ in $\mathcal{Z}$ with Hamming ball distance $\delta_H$ at most $r$. 
 
% \paragraph{In Section~\ref{sec:PPM-EM},}\label{para:notation-4} given $T$ the total number of iterations of Algorithm~\ref{alg:PPM}, we use $\bm{v}_{t + 1}$ to denote the solution obtained in $t$-th iteration. For $t = 0, 1, \ldots, T - 1$, define $L^{(t)} \in \mathcal{L}$ the linear subspaces that contains $\bm{v}_t$, and define the linear subspace $F^{(t)} := \text{conv}(L^{(t)} \cup L^{(t + 1)} \cup L_*)$ as the convex hull of $L^{(t)}, L^{(t + 1)}, L_*$. Decomposing the vector $\bm{v}_t$ over $\widehat{\bm{v}}_{F^{(t)}}$ and its orthonormal direction $\widehat{\bm{v}}_{\bot}$, we have $\bm{v}_t = \alpha_t \widehat{\bm{v}}_{F^{(t)}} + \beta_t \widehat{\bm{v}}_{\bot}$, where $\alpha_t := \langle \bm{v}_t, \widehat{\bm{v}}_{F^{(t)}}\rangle, \beta_t := \langle \bm{v}_t, \widehat{\bm{v}}_{\bot}\rangle, \alpha_t^2 + \beta_t^2 = 1, \|\widehat{\bm{v}}_{F^{(t)}}\|_2 = \|\widehat{\bm{v}}_{\bot}\|_2 = 1,$ and $\langle \widehat{\bm{v}}_{F^{(t)}}, ~ \widehat{\bm{v}}_{\bot} \rangle = 0$. We use $\lambda_i$ as the $i$-th eigenvalue of the covariance matrix $\bm{\Sigma}$, $\widehat{\lambda}_i := \lambda_i(\widehat{\bm{\Sigma}})$ as the $i$-th eigenvalue of the sample covariance matrix $\widehat{\bm{\Sigma}}$, and $\widehat{\lambda}_{i}^{F^{(t)}} := \lambda_{i}(\widehat{\bm{\Sigma}}_{F^{(t)}})$ as a shorthand for the $i$-th eigenvalue of $\widehat{\bm{\Sigma}}_{F^{(t)}}$. We denote $\kappa := \lambda_2 / \lambda_1 = 1 / (1 + \lambda)$ as the signal-to-noise ratio of the true covariance matrix $\bm{\Sigma}$ and $\widehat{\kappa}_{F^{(t)}} := \widehat{\lambda}_2^{F^{(t)}} / \widehat{\lambda}_{\max}^{F^{(t)}}$ as the signal-to-noise ratio of the restricted (in linear subspace $F^{(t)}$) sample covariance matrix $\widehat{\bm{\Sigma}}_{F^{(t)}}$. Moreover, define 
% \begin{align}
%     F^* := \argmax_F \rho(\bm{W}, {F}) ~~\text{s.t.}~~ F = \text{conv}(L_{m_1} \cup L_{m_2} \cup L_{m_3}), ~ \forall ~ m_1 \neq m_2 \neq m_3 \in [M], \notag
% \end{align}  
% be the union of three linear subspaces in $\mathcal{L}$ with respect to the maximum $\rho(\bm{W}, {F})$.

% \paragraph{In Section~\ref{sec:specific-examples},} we use $\mathcal{T}^k, \mathcal{P}^k$ to denote the structure set (i.e., feasible set $\mathcal{M}$ of \eqref{eq:linearly-structured-PCA}) of tree/path sparse PCA, repsectively. 
 





