%!TEX root = IEEETSP-main-paper.tex

\section{End-to-end analysis for specific examples} \label{sec:specific-examples}

In this section, we provide end-to-end analyses for path-sparse and tree-sparse PCA, including results on their information-theoretic limits of estimation as well as the performance of the projected power method when initialized using covariance thresholding. We complement these with what may be considered as the main results of this section: matching suggestions of computational hardness. 
%in these problems. 

\subsection{Path-Sparse PCA} \label{sec:PS-PCA}

\subsubsection{Fundamental limits for Path-Sparse PCA} \label{sec:limits-PS-PCA}

Recall the notation $\mathcal{P}^k$ as the structure set of path-sparse PCA from Section~\ref{sec:PS-PCA-intro}. We write $\bm{v} \in \mathcal{P}^k$ if the support set satisfies $\mathsf{supp}(\bm{v}) \in \mathcal{P}^k$. We use 
\begin{align}
    \widehat{\bm{v}}_{\textsf{PS}} := \argmax_{\bm{v}} ~ \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} ~~\text{s.t.}~~ \bm{v} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k \label{eq:PS-PCA-est}
\end{align}  
to denote the corresponding estimate from exhaustive search. 
%Similarly to $F^*$, define 
%$
%P^{*} := \argmax_F \rho(\bm{W}, {F}) \; \text{s.t.}\; F = \text{conv}\big(L_{m_1} \cup L_{m_2} \cup L_{m_3} \big),\; \forall\; L_{m_1}, L_{m_2}, L_{m_3} \in \mathcal{P}^k.
%$
%\mqcomment{we seems didn't use the definition of $P^{*}$ in the main text.}

\begin{corollary}\label{coro:PS-PCA-fund-limits}
%Let $\mathcal{P}^k$ be defined as in Section~\ref{sec:PS-PCA-intro}. 
There exists a pair of positive constants $(c, C)$ such that the following holds.
\noindent (a) Without loss of generality, assume $\langle \bm{v}_*, \widehat{\bm{v}}_{\mathsf{PS}} \rangle \geq 0$. Then for any $c_1>0$ and $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$, we have 
\[
 \big\|\widehat{\bm{v}}_{\mathsf{PS}} - \bm{v}_*\big\|_2 \leq C \left( \frac{1 + \lambda}{\lambda} \right) \sqrt{\frac{3 (\ln d - \ln k)k + c_1k}{n}}
 \] 
 with probability at least $1 - 2\exp(- c_1k)$.

\noindent (b) Suppose that $d\geq 16 k^2$ and $k\geq 4$. Then we have the minimax lower bound 
\begin{align*}
&\inf_{\widehat{\bm{v}}} \; \sup_{\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k} \mathbb{E}\left[ \left\| \widehat{\bm{v}} \widehat{\bm{v}}^{\top} - \bm{v}_* \bm{v}_*^{\top}  \right\|_F \right] \geq  c \cdot \min\bigg\{1,  \sqrt{\frac{1 + \lambda}{8 \lambda^2}} \sqrt{\frac{k \cdot \big( \frac{\ln d}{2} - \ln k \big)}{n}} \bigg\}.
\end{align*}
Here, the infimum is taken over all measurable functions of the observations $\{ \bm{x}_i \}_{i = 1}^n$ drawn i.i.d. from the distribution $\mathcal{D}(\lambda; \bm{v}_*)$.
\end{corollary}

Corollary~\ref{coro:PS-PCA-fund-limits} is proved in Section~\ref{proof-coro-PS-PCA-fund-limits} of the supplementary material, and is based on Theorem~\ref{thm:fund-limits}. In particular, Corollary~\ref{coro:PS-PCA-fund-limits}(a) gives an upper bound on the estimation error of $\widehat{\bm{v}}_{\textsf{PS}}$ by showing that the statistical noise term\footnote{As expected, this term does not differ significantly from the corresponding term for vanilla sparse PCA, since the number of sparsity patterns for path sparse PCA $|\mathcal{P}^k|$ is on the order $(d/k)^k$.} $\rho(\bm{W}, P^*)$ is of the order $(\lambda + 1) \sqrt{k \cdot ( \ln d - \ln k)/n}$. 
%{\color{red}Compared with the vanilla sparse PCA, its logarithmic term changes from $\log d$ to $\log(d / k)$ due to the path sparsity structure.}
The minimax lower bound obtained in Corollary~\ref{coro:PS-PCA-fund-limits}(b) is of the same order as the minimax lower bound given in [Theorem 1, \cite{asteris2015stay}] with the outer degree parameter $|\Gamma_{\text{out}}(v)| = (d - 2) / k$. 


\subsubsection{Local convergence and initialization} \label{sec:initialization-PS-PCA}
\paragraph{Exact projection oracle} We build the exact projection oracle for path-sparse PCA $\Pi_{\mathcal{P}^k}$ by picking the component with the largest absolute value in each partition (layer) for a given $(d,k)$-layered graph $G$. The formal procedure is given in Algorithm~\ref{alg:projection-PSPCA} (see Section~\ref{app:initial-examples}), and has running time $O(d)$.  


\begin{corollary} \label{coro:PS-PCA-PPM}
 Suppose the initiatialization $\bm{v}_0$ in Algorithm~\ref{alg:PPM} satisfies $\bm{v}_{0} \in \mathcal{P}^{k} \cap \mathcal{S}^{d-1}$ and $\langle \bm{v}_0,\bm{v}_* \rangle \geq 1/2$. There exists a tuple of universal positive constants $(c,C_1,C_2,C_3)$ such that for $\lambda \geq C_1$, $n\geq C_2k\ln(d)$,  and all $t \geq 1$, the iterate $\bm{v}_t$ from Algorithm~\ref{alg:PPM} satisfies 
 \[
 \| \bm{v}_{t} - \bm{v}_{*} \|_2 \leq \frac{1}{2^t} \cdot \| \bm{v}_{0} - \bm{v}_{*} \|_2 +  C_3 \sqrt{\frac{k(2\ln d - \ln k)}{n}},
 \] 
 with probability at least $1-\exp(-ck)$. 
\end{corollary}
Corollary~\ref{coro:PS-PCA-PPM} is proved in Section~\ref{proof-coro-PS-PCA-PPM}
%. {\color{orange}
%It is proved by 
by applying Theorem~\ref{thm:convergence}. %and showing
%\begin{align*}
%    \rho(\bm{W}, P^*) \lesssim  (1+\lambda)\sqrt{\frac{k\ln d + ck}{n}}
%\end{align*}
%holds with probability at least $1 - \exp(- ck)$.}
%where $P^*$ above is defined similar to $F^*$ with respect to path sparsity set $\mathcal{P}^k$. 
%\apcomment{Define $P^*$ before by making explicit reference to $F^*$.} \gwcomment{Done, at the beginning of Section 4.1.1.}
%

The final problem is to obtain an initialization $\bm{v}_0$.
To do so, note that the set $\mathcal{P}^k$ satisfies Assumption~\ref{assump:M-set-initial}, leading to the following corollary of Theorem~\ref{thm:initialization-method}.

%We set $\mathcal{M} = \mathcal{P}^k$ and use the exact projection oracle of path-sparse PCA $\Pi_{\mathcal{P}^k}$ \footnote{The exact projection $\Pi_{\mathcal{P}^k}$ can be established by picking the component of largest absolute value in each partition (layer) for a given $(d,k)$-layered graph $G$.} (with running time $O(d)$) in both Algorithm~\ref{alg:PPM-project} and Algorithm~\ref{alg:initialization}.
\begin{corollary} \label{coro:initial-PS-PCA}
Assume $k^2 \leq d / e$. There exists a pair of universal positive constants $(C, C')$ such that if $n\geq \max\{C\log d,k^{2}\}$ and $n \geq C'\max \big\{1,\lambda^{-2} \big\} \log(d/k^2) k^{2}, $ then the initial vector $\bm{v}_0 \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$ obtained from Algorithm~\ref{alg:initialization} satisfies $\langle \bm{v}_0, \bm{v}_* \rangle \geq 7/8$ with probability $1 - C'\exp(- \min\{\sqrt{d}, n\}/C')$. 
%\gwcomment{Add some discussions in next paragraph. This $7/8$ is selected to be the same with the $7/8$ presented in the discussion of Theorem 3 for initialization.}
\end{corollary}

%It is straightforward to see that Corollary~\ref{coro:initial-PS-PCA} follows from Theorem~\ref{thm:initialization-method} by specifying $c_0 = 0.5$. \apcomment{It was $7/8$ before. Please stay consistent. In fact, I would remove this sentence.}
%The proof of Corollary~\ref{coro:initial-PS-PCA} is derived from Theorem~\ref{thm:convergence} in Appendix~\ref{app:initial-methods}. 
%The proof of Corollary~\ref{coro:initial-PS-PCA} exactly follows the proof of Theorem~\ref{thm:initialization-method} by setting $c_0$ presented in Theorem~\ref{thm:initialization-method} as $7/8$. Thus, we omit the proof of Corollary~\ref{coro:initial-PS-PCA} in appendix. 
In words, Corollary~\ref{coro:initial-PS-PCA} provides an initialization method whose outputs can be used for the general projected power method (Algorithm~\ref{alg:PPM}) for path-sparse PCA when the number of samples\footnote{The constant $7/8$ in $\langle \bm{v}_0, \bm{v}_* \rangle \geq 7/8$ can be replaced by any positive constant within $(0, 1)$  provided it ensures the good region condition $\langle \bm{v}_0, \bm{v}_* \rangle > t_2(\lambda)$.} $n \gtrsim k^2 \log(d/k^2)$. 

As previously mentioned, there is a gap between the condition $n \gtrsim k$ required for Corollary~\ref{coro:PS-PCA-PPM} and the stronger condition above. We will now show evidence that $k^{2}$ samples are necessary. In particular, %in Section~\ref{sec:examples-average-hard}, 
we will show that no randomized polynomial-time algorithm can ``solve" (i.e. produce a consistent estimate for) path-sparse when $n \ll k^2$, provided we assume the  average-case hardness of the secret-leakage planted clique problem. This can be regarded as the main takeaway for path-sparse PCA: The additional structure has minimal effect on its statistical and computational limits.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Average-Case Hardness of Path Sparse PCA}\label{sec:examples-average-hard}


%\apcomment{Please add the setup for computational lower bound here.} \gwcomment{Done.}
%\gwcomment{Partially done. (Will discuss with Ashwin and Mengqi)}
%\gwcomment{Need: 
%\begin{itemize}
%	\item Def 5 
%	\item Def 6 K-PC
%	\item Def hardness. What it means to be hard? K-PC is hard
%	\item Coro: if K-PC is hard, then Path sparse PCA is hard
%\end{itemize}
%}

This section focuses on the average-case hardness of the path sparse PCA, which is obtained via a reduction from the $K$-partite planted clique (PC) detection problem, which is in turn conjectured to be hard. 
%{\color{blue}Due to the page limit, we leave the following necessary definitions, e.g., Secret Leakage $\text{PC}_{\mathcal{D}}$ Detection Problem, $K$-Partite Planted Clique Detection Problem, $K$-Partite PC Hardness Conjecture, Qualified Estimator$(\epsilon)$, to Appendix~\ref{app:definition-examples}.}

\begin{definition} \label{defn:SL-PC}
\textbf{Secret Leakage $\text{PC}_{\mathcal{D}}$ Detection Problem,} \cite{brennan2020reducibility}. Given a distribution $\mathcal{D}$ on $K$-subsets of $[N]$, let $\mathcal{G}_{\mathcal{D}}(N, K, 1/2)$ be the distribution on $N$-vertex graphs sampled by first sampling $G \sim \mathcal{G}(N, 1/2)$ and $S \sim \mathcal{D}$ independently and then planting a $K$-clique on the vertex set $S$ in $G$. The secret leakage $\text{PC}_{\mathcal{D}}$ detect problem $\text{PC}_{\mathcal{D}}(N, K, 1/2)$ is defined as the resulting hypothesis testing problem between 
\[
	H_0: ~ G \sim \mathcal{G}(N, 1/2) \quad \text{and} \quad H_1: ~ G \sim \mathcal{G}_{\mathcal{D}}(N, K, 1/2).
\] 
\end{definition}

Now consider the following $K$-partite PC as a special case of the secret leakage $\text{PC}_{\mathcal{D}}$ detection problem. 

\begin{definition} \label{def:KPC}
\textbf{$K$-Partite Planted Clique Detection Problem (with source and terminal).} The $K$-partite planted clique detection problem $K\text{-PC}(N, K, 1/2)$ is a special case of the secret leakage planted clique detection problem $\text{PC}_{\mathcal{D}}(N, K, 1/2)$. Here the vertex set of $G$ has two special vertices: source and terminal, and the remaining vertices are evenly partition into $K$ parts of size $(N - 2) / K$. The distribution $\mathcal{D}$ always picks source, terminal and uniformly picks one element at random in each part.  
\end{definition}

Like the well-known planted clique conjecture, the $K$-Partite PC problem $K\text{-PC}(N, K, 1/2)$ is believed to satisfy the following hardness conjecture. 

\begin{conjecture} \label{conj:KPC-hardness}
\textbf{$K$-Partite PC Hardness Conjecture}, restatement of \cite{brennan2020reducibility}.
Suppose that $\{\mathcal{A}_N\}$ is a sequence of randomized polynomial time algorithms $\mathcal{A}_N: \mathcal{G}_N \to \{0,1\}$ and $K_N$ is a sequence of positive integers satisfying that $\limsup_{N \rightarrow \infty} \log_N K_N < 1/2$ with $\mathcal{G}_N$ the set of graphs with $N$ nodes. Then if $G$ is an instance of $K\text{-PC}(N, K_N, 1/2)$, it holds that $\liminf_{N \rightarrow \infty} \left( \mathbb{P}_{H_0}[\mathcal{A}_N(G) = 1] + \mathbb{P}_{H_1}[\mathcal{A}_N(G) = 0] \right) \geq 1. $
\end{conjecture}

\begin{definition}\label{def:qual-est}
\textbf{Qualified Estimator.} 
%\gwcomment{We have $\| \widehat{\bm{v}} - \bm{v}_* \|_2 < \frac{1}{4}$ implies $\| \widehat{\bm{v}} / \|\widehat{\bm{v}} \|_2 - \bm{v}_* \|_2 < \sqrt{2 - 2\sqrt{15/16}} < 1/2$. Modify the definition of qualified estimator due to this issue.}
A qualified estimator $\widehat{\bm{v}}(n, d_n, k_n, \lambda_n, \epsilon)$ for path-sparse PCA is a sequence of functions $\EST_n: \mathbb{R}^{d_n \times n} \to \mathbb{R}^{d_n}$ mapping $\{\bm{x}_i\}_{i = 1}^n \mapsto \widehat{\bm{v}}$ such that if the set of samples $\{\bm{x}_i\}_{i = 1}^n$ are drawn i.i.d. from $\mathcal{D}(\lambda_n, \bm{v}_*)$ for some $\bm{v}_* \in \mathcal{S}^{d_n - 1} \cap \mathcal{P}^{k_n}$ then $\liminf_{n \to \infty} \Pr \left\{ \| \widehat{\bm{v}} - \bm{v}_* \|_2 < \frac{1}{4} \right\} \geq \frac{1}{2} + \epsilon$ for some fixed $0 < \epsilon < 1/2$. 
\end{definition}

%\apcomment{Discussion of why this notion is reasonable. Also add comment saying that the init/end-to-end estimator is qualified provided $n \gtrsim k^2$.} \gwcomment{Done.}
From this point onward, we do not make $\epsilon$ explicit when referring to a qualified estimator. It suffices for the reader to think of it as a small positive constant that does not depend on $n$.
Geometrically, a qualified estimator $\widehat{\bm{v}}$ exhibits proximity to the ground truth $\bm{v}_* \in \mathcal{S}^{d_n - 1} \cap \mathcal{P}^{k_n}$ with probability at least $1/2 + \epsilon$ as $n \rightarrow \infty$. Note that Definition~\ref{def:qual-est} does not require explicit control on the behavior of $\widehat{\bm{v}}$ for a general vector $\bm{v}_* \notin \mathcal{S}^{d_n - 1} \cap \mathcal{P}^{k_n}$. 


%We require some definitions to make this precise.
%Before stating the main result of the reduction, we start with a generalization of the well-known planted clique detection problem (PC, see Definition~\ref{defn:PC}) -- Secret Leakage PC detection problem. Compared with the PC, the random graph $G$ in the Secret Leakage PC comes with some information about the vertex set of the planted clique. 


%Later, in the proof of Proposition~\ref{prop:reduction-PathPCA}, we present an approach to address a specific case when $\bm{v}_* = \bm{0}_{d_n}$ for ``null hypothesis'' of detection problem of the path-sparse PCA. 

%Back to the proposed methods in Section~\ref{sec:PS-PCA}, based on the 
It is also worth noting (using Corollary~\ref{coro:PS-PCA-PPM} and Corollary~\ref{coro:initial-PS-PCA} and the corresponding algorithms) 
%the exact projection (Algorithm~\ref{alg:projection-PSPCA}), the initialization (Algorithm~\ref{alg:initialization}), and the projected power method (Algorithm~\ref{alg:PPM}) 
that our end-to-end  estimator for path-sparse PCA is a polynomial-time computable qualified estimator provided $n \geq C k^2 \log(d /k)$ and $\lambda = \Omega(1)$. 
%Notably, such a resulting qualified estimator $\widehat{\bm{v}}$ always resides within the feasible set $\mathcal{S}^{d_n - 1} \cap \mathcal{P}^k$ and ensures polynomial running time in $n$.

\begin{proposition} \label{prop:reduction-PathPCA}
There exists a universal constant $c > 0$ such that the following holds. Let $1/2 \leq \beta <1$ and $0 < \epsilon < 1/2$ be fixed. Here, we use integer $\subindex$ as our index parameter. Suppose the sequence of parameters $\{(k_{\subindex}, d_{\subindex}, \lambda_{\subindex}, \tau_{\subindex})\}_{ \subindex \in \mathbb{N}}$ is in the parameter regime 
\[k_{\subindex} = \lceil \subindex^{\beta} \rceil, \quad d_{\subindex} = \subindex, \quad \lambda_{\subindex} = \frac{k_{\subindex}^2}{\tau_{\subindex} \cdot \subindex } \cdot \frac{(\log 2)^2}{4 (6 \log (\subindex) + 2 \log 2)},
\] 
%\begin{align} \label{eq:regime-of-int}
%	k_n = \lceil n^{\beta} \rceil, \quad d_n = n, \quad \lambda_n = \frac{k_n^2}{\tau_n n} \cdot \frac{(\log 2)^2}{4 (6 \log n + 2 \log 2)}
%\end{align}
where $\tau_{\subindex}$ is an arbitrarily slowly growing function of $\subindex$. If the K-Partite PC hardness conjecture (Conjecture~\ref{conj:KPC-hardness}) holds, then there is no qualified estimator $\widehat{\bm{v}}(n_{\subindex}, d_{\subindex}, k_{\subindex}, \lambda_{\subindex}, \epsilon)$ running in time polynomial in $d_{\subindex}$ when the sample size $n_{\subindex}$ satisfies $n_{\subindex} \leq c \left( \frac{k_{\subindex}^2}{2 \tau_{\subindex} \log k_{\subindex}} \right).$ 
%\mqcomment{the sequence $n$ and sample size $n$ is confusing and what is $N_n$?}
\end{proposition}
%
%\apcomment{Discuss in words what the implications are.}
The proof of Proposition~\ref{prop:reduction-PathPCA} is given in Section~\ref{app:reduction-PathPCA} of the supplementary material.
%which is based on an existing average-case reduction method proposed in \cite{brennan2018reducibility}. 
%
%In words, Proposition~\ref{prop:reduction-PathPCA} provides an average-case hardness lower bound on the sample size required to compute a nontrivial solution for path sparse PCA w
In particular, when the eigengap satisfies\footnote{This can be ensured for dimension $d_j = j$ growing such that $\frac{k_j}{\tau_j d_j \log d_j} = \Theta(1)$.} $\lambda = \Theta(1)$, it shows that $n = \widetilde{\Omega} \left(k^2\right)$ is necessary for computationally efficient estimation. 
%unless the $K$-Partite PC Hardness (Conjecture~\ref{conj:KPC-hardness}) is not true. Therefore, the number of samples $O(k^2 \log(d / k))$ needed in Corollary~\ref{coro:initial-PS-PCA} for path-sparse PCA of our end-to-end qualified estimator is tight comparing with this average-case hardness lower bound, ignoring logarithm terms, which completes the story. 
%implies that the estimation problem of path sparse PCA has an average-case hardness lower bound on the sample size $\left( O \left( \frac{k^2}{2 \tau \log k}\right) = \tilde{\Theta}(k^2)\right)$ with eigengap with a known constant $\lambda = \Theta(1)$ satisfying the eigengap condition (Inequality~\eqref{eigen-gap-condition}), unless the $K$-Partite PC Hardness Conjecture~\ref{conj:KPC-hardness} is not ture. Thus the samples needed for path-sparse PCA in Corollary~\ref{coro:initial-PS-PCA} is tight comparing with this average-case hardness lower bound ignoring logarithm terms.


%\begin{definition} 
%\textbf{$K$-Partite PC Hardness Conjecture, }\cite{brennan2020reducibility}.  There is no (randomized) polynomial time algorithm (over $N, K$) solving $K\text{-PC}(N, K, 1/2)$ when $K = o(\sqrt{N})$. 
%\end{definition}


%\apcomment{remove from here.}
%Moreover, we give a formal definition of the estimation problem of path sparse PCA that we would like to reduce the $K$-Partite PC detection problem (Definition~\ref{def:KPC}) to. 

%{\color{orange}
%\gwcomment{Need to discuss: Should we add $\bm{0}_d$ as a choice for $\bm{v}_*$? Since $\bm{X} \sim N(\bm{0}_d, \bm{I}_{d \times d})$ is also a possible instance for detector $\EST$.} 
%%\begin{definition}
%%\textbf{Estimation problem of path sparse PCA.} The estimation problem of path sparse PCA $\text{E-PSPCA}(n, k, d, \lambda)$ is defined as follows: Given a set of $n$ i.i.d. samples $\bm{X}$ drawn from $\mathcal{D}(\lambda; \bm{v}_*)$ with a unknown ground truth $\bm{v}_* \in \{\mathcal{S}^{d - 1} \cap \mathcal{P}^k\} \cup {\color{red}\{\bm{0}_d\}}$, the task is to find a vector $\widehat{\bm{v}} \in \{\mathcal{S}^{d - 1} \cap \mathcal{P}^k\} \cup {\color{red}\{\bm{0}_d\}}$ such that $\|\bm{v}_* - \widehat{\bm{v}}\|_2 < \frac{1}{2}$ holds with probability at least $1/2 + \epsilon$ for some fixed $0 < \epsilon < 1/2$.    
%%\end{definition}
%
%\gwcomment{Claim that our PPM is a qualified estimator.}
%Here we would like to mention that the proposed projected power method (Algorithm~\ref{alg:PPM}) succeeds for this estimation problem $\text{E-PSPCA}(n, k, d, \lambda)$ when the parameters $n, k, d, \lambda$ satisfy the conditions in Theorem~\ref{thm:convergence} with a good initialization $\bm{v}_0$. The estimation for $\bm{0}_d$ zero ground truth could be done by simply comparing the objective value of the optimization problem~\eqref{exhaustive-search-estimator} with a lower bound, see Algorithm~\ref{alg:D-from-E} for details. 
%
%
%\gwcomment{Done. Move the detection problem of PSPCA to the proof of Proposition 1. The statement of Proposition 1 is updated. But I am not sure if this statement is well-polished...}
%	
%Given parameters $n, k, d, \lambda$, we use $\mathcal{X}_n$ to denote the collection of all instances for $\text{E-PSPCA}(n, k, d, \lambda)$, and $\EST$ to denote the family of all possible estimator of path sparse PCA. Now we are ready to present the main result in this section.}



\iffalse
\begin{proposition} \label{prop:reduction-PathPCA}
%Let $\beta \in [1/2, ~ 1)$. There is a sequence $\{(k_n, d_n, \mu_n, \lambda_n, \tau(n))\}_{n \in \mathbb{N}}$ of parameters such that 
%\begin{enumerate}
%	\item The parameters are in the regime
%		\begin{align*}
%			k_n = \lceil n^{\beta} \rceil, \quad d_n = n, \quad \mu_n = \frac{\log 2}{2\sqrt{6 \log n + 2 \log 2}}, \quad \lambda_n = \frac{k_n^2}{\tau n} \cdot \mu_n^2
%		\end{align*}
%		with $\tau = \tau(n) \rightarrow \infty$, i.e., an arbitrarily slowly growing function of $n$. 
%	\item Let $0 < \epsilon < 1/2$ be fixed and let $\bm{X}_n$ be an instance of $\text{D-PSPCA}(n, k_n, d_n, \lambda_n)$. There is no sequence of randomized polynomial-time computable functions $\EST_n: \mathcal{X}_n \to \mathbb{R}^d$ that solves the $\text{E-PSPCA}(n, k_n, d_n, \lambda_n)$ with probability at least $1/2 + \epsilon$, assuming the K-Partite PC hardness conjecture (Conjecture~\ref{conj:KPC-hardness}). 
%\end{enumerate}
%Therefore, given the K-Partite PC hardness conjecture (Conjecture~\ref{conj:KPC-hardness}), by setting $\beta = 1/2$, there is no sequence of randomized polynomial-time computable functions $\EST_n: \mathcal{X}_n \to \mathbb{R}^d$ that solves the $\text{E-PSPCA}(n, k_n, d_n, \lambda_n)$ when $n \ll O(k^2)$ with a known $\lambda_n = O(1/(\tau \log n)) = \tilde{\Theta}(1)$. Moreover, by setting $k_n := \lceil c n^{1/2} \tau^{1/2} (\log n)^{1/2} \rceil$\footnote{This setting is in the parameter regime for $k_n = \lceil n^{\beta} \rceil$ with $\beta \in [1/2, 1)$.} for some constant $c$, there is no sequence of randomized polynomial-time computable functions $\EST_n: \mathcal{X}_n \to \mathbb{R}^d$ that solves the $\text{E-PSPCA}(n, k_n, d_n, \lambda_n)$ when $n \ll O \left( \frac{k^2}{2 \tau \log k}\right) = \tilde{\Theta}(k^2)$ with a known constant $\lambda = \lambda_n = \Theta(1)$ satisfying the eigengap condition (Inequality~\eqref{eigen-gap-condition}).

Let $\beta \in [1/2, ~ 1)$ and $0 < \epsilon < 1/2$ be fixed. Let the sequence $\{(k_n, d_n, \mu_n, \lambda_n, \tau(n))\}_{n \in \mathbb{N}}$ of parameters is in the regime
\begin{align} \label{eq:regime-of-int}
	k_n = \lceil n^{\beta} \rceil, \quad d_n = n, \quad \mu_n = \frac{\log 2}{2\sqrt{6 \log n + 2 \log 2}}, \quad \lambda_n = \frac{k_n^2}{\tau n} \cdot \mu_n^2
\end{align}
with $\tau = \tau(n) \rightarrow \infty$, i.e., an arbitrarily slowly growing function of $n$. Assume the K-Partite PC hardness conjecture (Conjecture~\ref{conj:KPC-hardness}) holds. By setting $k_n := \lceil c n^{1/2} \tau^{1/2} (\log n)^{1/2} \rceil$\footnote{This setting is in the parameter regime for $k_n = \lceil n^{\beta} \rceil$ with $\beta \in [1/2, 1)$.} for some constant $c$, there is no sequence of randomized polynomial-time computable functions $\EST_n: \mathcal{X}_n \to \mathbb{R}^d$ that solves the $\text{E-PSPCA}(N_n, k_n, d_n, \lambda_n)$ with probability at least $1/2 + \epsilon$, when the number of samples $N_n \ll O \left( \frac{k_n^2}{2 \tau \log k_n}\right) = \tilde{\Theta}(k_n^2)$ with a known constant $\lambda = \lambda_n = \Theta(1)$ satisfying the eigengap condition (Inequality~\eqref{eigen-gap-condition}).
\iffalse
we have the following results:
\begin{itemize}
	\item By setting $k_n := \lceil n^{1/2} \rceil$, i.e., $\beta = 1/2$, there is no sequence of randomized polynomial-time computable functions $\EST_n: \mathcal{X}_n \to \mathbb{R}^d$ that solves the $\text{E-PSPCA}(N_n, k_n, d_n, \lambda_n)$ with probability at least $1/2 + \epsilon$, when the number of samples $N_n \ll O(k_n^2)$ with a known $\lambda_n = O(1/(\tau \log n)) = \tilde{\Theta}(1)$.
	\item By setting $k_n := \lceil c n^{1/2} \tau^{1/2} (\log n)^{1/2} \rceil$\footnote{This setting is in the parameter regime for $k_n = \lceil n^{\beta} \rceil$ with $\beta \in [1/2, 1)$.} for some constant $c$, there is no sequence of randomized polynomial-time computable functions $\EST_n: \mathcal{X}_n \to \mathbb{R}^d$ that solves the $\text{E-PSPCA}(N_n, k_n, d_n, \lambda_n)$ with probability at least $1/2 + \epsilon$, when the number of samples $N_n \ll O \left( \frac{k_n^2}{2 \tau \log k_n}\right) = \tilde{\Theta}(k_n^2)$ with a known constant $\lambda = \lambda_n = \Theta(1)$ satisfying the eigengap condition (Inequality~\eqref{eigen-gap-condition}). 
\end{itemize}
\fi
\end{proposition}
\fi




%\begin{proof}
%%\apcomment{Make notation consistent.}
%%\gwcomment{This proof will be moved to Section~\ref{app:reduction-PathPCA}.}
%%\gwcomment{The proof has been updated.}
%For the proof, one should think of the estimator and parameters as indexed by the natural number $n$, although we drop this explicit dependence for clarity of exposition.
%We begin by introducing formal definition for the detection problem for path sparse PCA. Recall the definition of a qualified estimator (Definition~\ref{def:qual-est}).
%%, respectively.  
%
%\begin{definition}
%\textbf{Detection problem for path sparse PCA.} Suppose $\bm{v}_*$ is $\bm{0}_d$ with probability $1/2$ and an arbitrary vector in the set $\mathcal{S}^{d - 1} \cap \mathcal{P}^k$ with probability $1/2$. The detection problem for path sparse PCA -- $\text{D-PSPCA}(n, k, d, \lambda)$ is defined as the resulting hypothesis testing problem 
%\begin{align*}
%	H_0: \bm{X} \sim \mathcal{D}(0; \bm{0}_d)^{\otimes n} ~~~~\text{and}~~~~  H_1: \bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)^{\otimes n}.
%\end{align*}
%%with a unknown ground truth $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$. 
%\end{definition}
%
%%\begin{definition}
%%\textbf{Estimation problem for path sparse PCA.} The estimation problem of path sparse PCA -- $\text{E-PSPCA}(n, k, d, \lambda)$ is to find a qualified estimator$(\epsilon)$ denoted by $\widehat{\bm{v}}$ which additionally guarantees that
%%	\begin{align*}
%%		\liminf_{n \to \infty} \Pr \left\{  \widehat{\bm{v}} = \bm{0}_{d_n} \right\} \geq \frac{1}{2} + \epsilon ,
%%	\end{align*} 
%%	if $\bm{v}_* = \bm{0}_d$. % with some fixed $\epsilon$ used in the qualified estimator. 
%%%defined as follows: Given a set of $n$ i.i.d. samples $\bm{X}$ drawn from $\mathcal{D}(\lambda; \bm{v}_*)$ with a unknown ground truth $\bm{v}_* \in \{\mathcal{S}^{d - 1} \cap \mathcal{P}^k\} \cup {\color{red}\{\bm{0}_d\}}$, the task is to find a vector $\widehat{\bm{v}} \in \{\mathcal{S}^{d - 1} \cap \mathcal{P}^k\} \cup {\color{red}\{\bm{0}_d\}}$ such that $\|\bm{v}_* - \widehat{\bm{v}}\|_2 < \frac{1}{2}$ holds with probability at least $1/2 + \epsilon$ for some fixed $0 < \epsilon < 1/2$.    
%%\end{definition} 
%%\apcomment{Why do you need to define the estimation problem at all? You directly construct a detector it seems?}
%
%We show our average-case hardness result by contradiction, i.e., we assume there exists a randomized polynomial-time qualified estimator. We then transform it into a good detector for the path-sparse PCA problem and eventually the secret leakage planted clique problem. The proof of Proposition~\ref{prop:reduction-PathPCA} can be separated into four parts: 
%
%\begin{enumerate}
%	\item \textbf{Constructing a randomized polynomial time algorithm for detection based on an estimation algorithm for path sparse PCA.} 
%	
%	Suppose $\EST_n: (\mathbb{R}^{d_n})^n \to \mathbb{R}^{d_n}$ is a sequence of randomized polynomial time functions for the assumed qualified estimator. 
%	
%%	\gwcomment{To be general, let the successful prob of nontrivial detector wp $> 1/2$} 
%%	$\EST_n$ maps an instance $\bm{X}$ of $n$ samples for path sparse PCA problem to an estimation $\widehat{\bm{v}} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$ which satisfies the following property: {\color{orange}when we have $\lambda = \Theta(1)$ some known constant}, and $\bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)^{\otimes n}$ with 
%%		\begin{align*}
%%			\bm{v}_* \in \text{UPSPCA}^k := \left\{ \left. \bm{v} \in \left\{0, \frac{1}{\sqrt{k}} \right\}^d ~\right|~ \bm{v} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k \right\},  
%%		\end{align*} 
%%		$\mathcal{R}_n$ ensures that $\widehat{\bm{v}} \in \text{UPSPCA}^k$ and $\|\bm{v}_* - \widehat{\bm{v}}\|_2 < \frac{1}{2}$ holds with probability $1 - o(1)$ as $n \rightarrow \infty$. 
%
%
%	Given $\EST_n$, we construct a detector $\DET_n: \EST \times (\mathbb{R}^{d_n})^n \times \mathbb{R} \to \{0,1\}$ of $\text{D-PSPCA}(n, k, d, \lambda)$, which maps a tuple of three components including an estimator function $\EST_n$, an instance $\bm{X}$ and a known eigengap $\lambda$ to  a detection algorithm that outputs one of the two hypotheses; this is presented in Algorithm~\ref{alg:D-from-E}. %\apcomment{Add unit normalization of $\widehat{v}$.} 
%%	\gwcomment{DONE. We cannot simply add unit normalization. Since $\| \widehat{\bm{v}} - \bm{v}_* \|_2 \leq \frac{1}{2}$ does not imply $\| \widehat{\bm{v}} / \|\widehat{\bm{v}} \|_2 - \bm{v}_* \|_2 \leq \frac{1}{2}$ when $\|\widehat{\bm{v}} \|_2 < 1$ and $\bm{v}_* \in \mathcal{S}^{d_n} \cap \mathcal{P}^k$. Modify the definition of qualified estimator due to this issue.}
%	
%%	based on $\EST_n$ for the detection problem of path sparse PCA with unit signals $\bm{v}_* \in \{0, 1/\sqrt{k}\}^d$ as presented in Algorithm~\ref{alg:D-from-E}. 
%
%\begin{algorithm}
%\caption{Detector $\DET_n$ of path sparse PCA}
%\label{alg:D-from-E}
%\textbf{Input:} A function $\EST_n$, an instance $\bm{X}$ drawn from $\text{D-PSPCA}(n,k,d,\lambda)$, and an eigengap $\lambda$.  
%\begin{algorithmic}[1]
%\State Compute the estimation $\widehat{\bm{v}} := \EST_n(\bm{X})$ of path sparse PCA. \label{alg:D-from-E-1}
%\State Normalize the estimation 
%	\begin{align*}
%		\tilde{\bm{v}} := \left\{
%		\begin{array}{lll}
%			\widehat{\bm{v}} / \|\widehat{\bm{v}}\|_2 & \text{ if } \widehat{\bm{v}} \neq \bm{0}_{d_n} \\
%			\text{any point in $\mathcal{S}^{d_n} \cap \mathcal{P}^k$} & \text{ if } \widehat{\bm{v}} = \bm{0}_{d_n}
%		\end{array}
%		\right. .
%	\end{align*} \label{alg:D-from-E-2}
%\State Set the sample covariance matrix $\widehat{\bm{\Sigma}} := \frac{1}{n} \bm{X} \bm{X}^{\top}$. \label{alg:D-from-E-3}
%\If{$\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}} \geq 1 + \lambda/4 - (1 + \lambda) \sqrt{k \ln d / n}$} \label{alg:D-from-E-4}
%\State \textbf{Output:} $1$, i.e., $\bm{X}$ is from $H_1$.
%\Else 
%\State \textbf{Output:} $0$, i.e., $\bm{X}$ is from $H_0$.
%\EndIf
%\end{algorithmic}
%\end{algorithm} 
%
%	The detector $\DET_n$ clearly runs in polynomial time. Next, we show that for any instance $\bm{X}$ of $\text{D-PSPCA}(n, k, d, \lambda)$ and a known eigengap $\lambda$ (satisfying the conditions required in Theorem~\ref{thm:convergence} and Theorem~\ref{thm:initialization-method}), $\DET_n$ satifies
%	\begin{align} \label{eq:key-claim}
%		\liminf_{n \rightarrow \infty} \left( \mathbb{P}_{H_0}[\DET_n(\bm{X}) = 1] + \mathbb{P}_{H_1}[\DET_n(\bm{X}) = 0] \right) < 1. 
%	\end{align}
%
%%	maps an instance $\bm{X}$ to an estimation $\widehat{\bm{v}} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$ such that $\|\bm{v}_* - \widehat{\bm{v}}\|_2 < \frac{1}{2}$ holds with probability greater than $\frac{1}{2} + \epsilon$ for any $0 < \epsilon < 1/2$. \footnote{Recall that our proposed projected power method for path sparse PCA ensures estimation task with probability at least $1 - \exp(- ck) > \frac{1}{2} + \epsilon$.}.
%		
%%		Set the sample covariance matrix $\widehat{\bm{\Sigma}} := \frac{1}{n} \bm{X} \bm{X}^{\top}$. If $\widehat{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \widehat{\bm{v}} \geq 1 + \lambda/4 - (1 + \lambda) \sqrt{k \ln d / n}$, then $\DET_n(\bm{X}) = 1$, i.e., $\bm{X}$ is generated from hypothesis $H_1$. Otherwise, we have $\mathcal{DE}_n(\bm{X}) = 0$, i.e., $\bm{X}$ is generated from null hypothesis $H_0$. 
%		
%		To establish claim~\eqref{eq:key-claim}, note that the ``if criteria'' (Step-\eqref{alg:D-from-E-3} in Algorithm~\ref{alg:D-from-E}) is based on the following two-part calculation. 
% On the one hand, suppose the instance $\bm{X}$ is drawn from $\mathcal{D}(\lambda; \bm{v}_*)^{\otimes n}$ for $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$. Consider the orthogonal decomposition of $\tilde{\bm{v}}$ given by $\tilde{\bm{v}} = \alpha \bm{v}_* + \beta \bm{v}_{\perp}$ with $\|\bm{v}_*\|_2 = \|\bm{v}_{\perp}\|_2 = 1, ~ \langle \bm{v}_*, \bm{v}_{\perp} \rangle = 0, \alpha^2 + \beta^2 = 1$. Based on the definition of the qualified estimator, the normalized vector $\|\tilde{\bm{v}} - \bm{v}_*\|_2$ satisfies $\| \widehat{\bm{v}} / \|\widehat{\bm{v}} \|_2 - \bm{v}_* \|_2 < \sqrt{2 - 2\sqrt{15/16}} < 1/2$. Consequently, we have $\alpha > 1/2$, and therefore the objective satisfies
%		\begin{align*}
%			\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}} = & ~ (\alpha \bm{v}_* + \beta \bm{v}_{\perp})^{\top} \widehat{\bm{\Sigma}}  (\alpha \bm{v}_* + \beta \bm{v}_{\perp}) \\
%			= & ~ \alpha^2 \bm{v}_*^{\top} \bm{\Sigma} \bm{v}_* + \beta^2 \bm{v}_{\perp}^{\top} \bm{\Sigma} \bm{v}_{\perp} + 2 \alpha \beta \bm{v}_{*}^{\top} \bm{\Sigma} \bm{v}_{\perp} + \widehat{\bm{v}}^{\top} \bm{W} \widehat{\bm{v}} \\
%			\geq & ~ \alpha^2 (1 + \lambda) + \beta^2 + 0 + \rho(\bm{W}, P^*) \\
%			\geq & ~ 1 + \frac{\lambda}{4} - (1 + \lambda) \sqrt{k \ln d / n}, 
%		\end{align*}
%		where the final inequality holds due to $\rho(\bm{W}, P^*) \leq (1 + \lambda) \sqrt{k \ln d / n}$ with probability $1 - \exp(- ck)$.  
%		On the other hand, if the instance $\bm{X}$ is drawn from $\mathcal{D}(0; \bm{0}_d)^{\otimes n}$, then the objective satisfies
%		\begin{align*}
%			\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}} = & ~ \tilde{\bm{v}}^{\top} (\bm{I}_d + \bm{W}) \tilde{\bm{v}} \\
%			\leq & ~ 1 + \rho(\bm{W}, P^*) \\
%			\leq & ~ 1 + (1 + \lambda) \sqrt{k \ln d / n} \\
%			< & ~ 1 + \frac{\lambda}{4} - (1 + \lambda) \sqrt{k \ln d / n},
%		\end{align*}
%		where the final strict inequality holds in the parameter regime of interest~\eqref{eq:regime-of-int}. Assuming that $n$ is sufficiently large and combining the above two cases implies 
%		\begin{align*}
%			\mathbb{P}_{H_0}[\DET_n(\bm{X}) = 1] \leq & ~ \frac{1}{2} - \epsilon + \exp(- ck), \\
%			\mathbb{P}_{H_1}[\DET_n(\bm{X}) = 0] \leq & ~ \frac{1}{2} - \epsilon + \exp(- ck), 
%		\end{align*} 
%		and therefore, 
%		\begin{align*}
%			\liminf_{n \rightarrow \infty} \left( \mathbb{P}_{H_0}[\DET_n(\bm{X}) = 1] + \mathbb{P}_{H_1}[\DET_n(\bm{X}) = 0] \right) < 1. 
%		\end{align*}
%		%For computational complexity, since Step-\eqref{alg:D-from-E-1} -- computing the estimation $\widehat{\bm{v}}$, Step-\eqref{alg:D-from-E-2} -- normalizing $\widehat{\bm{v}}$ for $\tilde{\bm{v}}$, Step-\eqref{alg:D-from-E-3} -- setting the sample covariance matrix $\widehat{\bm{\Sigma}}$, and Step-\eqref{alg:D-from-E-4} -- computing objective value $\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}}$, take polynomial running time by our assumption, the detector $\DET_n$ is still a randomized polynomial-time algorithm. 
%	
%	\item \textbf{Reduction from $K$-Partite PC detection problem to path sparse PCA detection problem.} 
%		
%%		\gwcomment{SPCA-RECOVERY method \cite{brennan2018reducibility}: 
%%		\begin{itemize}
%%			\item BC-Recovery: maps $\textbf{PDS}_R(n, k, 1/2 + \rho, 1/2)$ to $\text{BC}_R(n,k,\mu)$ with $\mu = \frac{\log(1 + 2 \rho)}{2\sqrt{6 \log n + 2 \log 2}}$ (structure preserving)
%%			\item Random-Rotation: maps $\text{BC}_R(n,k,\mu)$ to $\text{UBSPCA}(n, k, d, \theta)$ with $d = n$ and $\theta = \frac{k^2 \mu^2}{\tau n}$. 
%%		\end{itemize}
%%		When $\rho = 1/2$, we have $\textbf{PDS}_R(n, k, 1, 1/2) = \textbf{PC}_R(n, k, 1/2)$. Instance from the recovery problem can be used for the detection problem, and vice versa.
%%		}
%		
%		%As a brief summary for this part, 
%		In this step, we use the average-case reduction method -- SPCA-RECOVERY [Figure 19, \citep{brennan2018reducibility}] to be our reduction method. Recall \cite{brennan2018reducibility} show that SPCA-RECOVERY maps an instance of planted clique problem $G \sim \text{PC}(n,k,1/2)$ to an instance $\bm{X}$ of vanilla sparse PCA detection problem (with ground truth $\bm{v}_*$ takes values in discrete set $\{0, 1 / \sqrt{k}\}$) approximately under total variance distance. Note that SPCA-RECOVERY ensures a preserving property (See Remark~\ref{remark:str-pre} for details). In words, SPCA-RECOVERY maps an instance $G \sim \text{K-PC}(n,k,1/2)$ to an instance $\bm{X} \sim \text{D-PSPCA}(n, k, d, \lambda)$ approximately under total variance distance while maintaining the structure, i.e., mapping rows associated with planted $k$-clique to rows associated with the corresponding path.
%
%		\begin{remark} \label{remark:str-pre}
%		\textbf{Structure preserving property of \textup{SPCA-RECOVERY}.} Given any $G \sim K\text{-PC}(n, k, 1/2)$ as an input instance of \textup{SPCA-RECOVERY}, \textup{SPCA-RECOVERY} maps the rows in the adjacency matrix $\bm{A}(G)$ concerning the planted $k$-clique of $G$ to the rows corresponding to the support set of the underlying path structure for the sample matrix (instance) $\bm{X} = [\bm{x}_1 ~|~ \cdots ~|~ \bm{x}_n]^{\top} \sim \text{D-PSPCA}(n, k, d, \lambda)$. 
%		\end{remark} 
%		
%%		\gwcomment{Proof for this part starts here... (1) define planted clique for a fixed clique structure (2) fix this clique, we have 2018 SPCA-RE maps a PC instance to a SPCA instance with corresponding support set}
%		Armed with these tools, we now present a quantitative analysis of the total variance distance that arises from our reductions. Let us start by recalling the parameter regime~\eqref{eq:regime-of-int} with some additional notation. Let $\beta \geq 1/2$. For path sparse PCA detection problem, define parameters 
%		\begin{align*}
%			k_n = \lceil n^{\beta} \rceil, \quad \rho_n = \frac{1}{2}, \quad, d_n = n, \quad \mu_n = \frac{\log 2}{2\sqrt{6 \log n + 2 \log 2}}, \quad \lambda_n = \frac{k_n^2}{\tau_n n} \cdot \frac{(\log 2)^2}{4 (6 \log n + 2 \log 2)},
%		\end{align*}
%		where $\tau_n \rightarrow \infty$ as $n \rightarrow \infty$, i.e., an arbitrarily slowly growing function of $n$. Let $\varphi_n = \text{SPCA-RECOVERY}$ be the reduction method. We use graph $G_n$ to denote an instance of $\text{K-PC}(n,k_n,1/2)$, and use $\bm{X}_n = \varphi_n (G_n)$ to be the output of $\text{SPCA-RECOVERY}$ with input $G_n$. To be concise, we use $\mathcal{L}(\bm{X}_n)$ to denote the distribution of a given instance $\bm{X}_n$.
%		
%		
%		Suppose $G_n \sim \mathcal{G}_{\mathcal{D}}(n, k_n, 1/2)$, is drawn from the $H_1$ hypothesis of $\text{K-PC}(n,k_n,1/2)$. Let $\bm{v}_*$ denote the unit vector supported on indices with respect to the clique in $G_n$ with nonzero entries equal to $1/\sqrt{k_n}$. Using\footnote{It is easy to observe that our parameter regime of $(n, \mu_n, \rho_n, \tau(n))$ satisfies the conditions presented in Lemma 6.7 and Lemma 8.2 in \cite{brennan2018reducibility}.} Lemma 6.7 and Lemma 8.2 in \cite{brennan2018reducibility}, we have
%		\begin{align*}
%			\text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_n), \mathcal{D}(\lambda; \bm{v}_*) \right) \leq O\left( \frac{1}{\sqrt{\log n}} \right) + \frac{2 (n + 3)}{\tau n - n - 3} \rightarrow 0 \quad \text{as} \quad n \rightarrow \infty. 
%		\end{align*}
%		On the other hand, if an instance $G_n \sim \mathcal{G}_{\mathcal{D}}(n, 1/2)$, is drawn from the $H_0$ hypothesis of $\text{K-PC}(n,k_n,1/2)$. Still using Lemma 6.7 and Lemma 8.2 in \cite{brennan2018reducibility}, we also have 
%		\begin{align*}
%			\text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_n), \mathcal{D}(0; \bm{0}_d) \right) \leq O\left( \frac{1}{\sqrt{\log n}} \right) + \frac{2 (n + 3)}{\tau n - n - 3} \rightarrow 0 \quad \text{as} \quad n \rightarrow \infty.
%		\end{align*}
%		Combining the above two cases ensures the reduction from $K$-Partite PC detection problem to path sparse PCA detection problem as we desired. 
% 		
% 		
% 		%		Due to the structure preserving property of \textup{SPCA-RECOVERY}, each part of the vertex set $V$ in graph $G \sim K\text{-PC}(n, k, 1/2)$ can be viewed as the vertex set of a layer in the layered graph with size $(n - 2) / k$. The planted $k$-clique (with one vertex in each part) in $G \sim K\text{-PC}(n, k, 1/2)$ corresponds to a path of length $k$ (excluding source and terminal) in the layered graph. 
%		
%	\item \textbf{Constructing a detection algorithm for $K$-Partite PC based on a detection algorithm for path sparse PCA.} 
%	
%		
%		Recall that there exists a sequence of detector algorithms $\DET_n$ that solves the detection problem of path-sparse PCA ($\text{D-PSPCA}(n, k, d, \lambda)$) as described above. Observe that under $H_1$ hypothesis,
%		\begin{align*}
%			\left|\mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n(\bm{X}) = 1] - \mathbb{P}_{\bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)}[\DET_n(\bm{X}) = 1] \right| \leq \text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_n), \mathcal{D}(\lambda; \bm{v}_*) \right) \rightarrow 0 \quad \text{as } n \rightarrow \infty. 
%		\end{align*}
%		Since 
%		\begin{align*}
%			\mathbb{P}_{\bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)}[\DET_n(\bm{X}) = 1] = \mathbb{P}_{H_1}[\DET_n(\bm{X}) = 1] \geq \frac{1}{2} + \epsilon
%		\end{align*} 
%		for sufficiently large $n$ by the definition of the detector $\DET_n$, it follows that 
%		\begin{align*}
%			\mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n \circ \varphi_n(G_n) = 1] = \mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n (\bm{X}_n) = 1] \geq \frac{1}{2} + \frac{\epsilon}{2} 
%		\end{align*}
%		for sufficiently large $n$. On the other hand, under $H_0$ hypothesis, 
%		\begin{align*}
%			\left|\mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n(\bm{X}) = 0] - \mathbb{P}_{\bm{X} \sim \mathcal{D}(0; \bm{0}_d)}[\DET_n(\bm{X}) = 0] \right| \leq \text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_n), \mathcal{D}(0; \bm{0}_d) \right) \rightarrow 0 \quad \text{as } n \rightarrow \infty. 
%		\end{align*}
%		Still 
%		\begin{align*}
%			\mathbb{P}_{\bm{X} \sim \mathcal{D}(0; \bm{0}_d)}[\DET_n(\bm{X}) = 0] = \mathbb{P}_{H_0}[\DET_n(\bm{X}) = 0] \geq \frac{1}{2} + \epsilon
%		\end{align*} 
%		holds for sufficiently large $n$ by the definition of the detector $\DET_n$. Then 
%		\begin{align*}
%			\mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n \circ \varphi_n(G_n) = 0] = \mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n (\bm{X}_n) = 0] \geq \frac{1}{2} + \frac{\epsilon}{2}. 
%		\end{align*}
%		Combining the above two results together implies
%		\begin{align*}
%			\liminf_{n \rightarrow \infty} \left( \mathbb{P}_{G_n \sim \mathcal{G}(n, 1/2)}[\DET_n \circ \varphi_n(G_n) = 1] + \mathbb{P}_{G_n \sim \mathcal{G}(n, k_n, 1/2)}[\DET_n \circ \varphi_n(G_n) = 0] \right) \leq 1 - \epsilon, 
%		\end{align*}
%		i.e., the detection method works.
%		
%		
%	\item \textbf{Putting together the pieces.}
%
%		Putting the above three parts together, we have used a polynomial-time qualified estimator to construct a sequence of functions $\DET_n \circ \varphi_n$ that detects K-Partite PC (Definition~\ref{def:KPC}) in polynomial time. This contradicts the K-Partite PC conjecture (Conjecture~\ref{conj:KPC-hardness}). Therefore, no such sequence of qualified estimator functions $\EST_n$ exists when the sequence of parameters $\{(k_n, d_n, \lambda_n, \tau_n)\}_{n \in \mathbb{N}}$ is in the proposed regime.
%		%, i.e., 
%	%\begin{align*}
%	%	k_n := \lceil c n^{1/2} \tau_n^{1/2} (\log n)^{1/2} \rceil, \quad d_n := n,\quad
%	%	\lambda_n := \frac{k_n^2}{\tau n} \cdot \mu_n^2 = \left\lceil \frac{c^2 (\log 2)^2}{24 + \frac{8\log 2}{\log n}} \right\rceil = \Theta \left( 1 \right),  
%	%\end{align*}
%	which completes the proof of the theorem.
%	%\footnote{Recall that, by letting $c \geq 12/\log 2$, we have the eigengap $\lambda \geq 5$, which could be further used to ensure the good region condition as required in Corollary~\ref{coro:initial-PS-PCA}.} %\apcomment{Explicitly have a point ``4. Putting together the steps''}
%	
%%		Assuming the Hardness Assumption for Planted Clique Problem [Definition~\ref{defn:PC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving $\text{UBSPCA}(k,n,n,\lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$. Now in Algorithm , let $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ be a special case of UBSPCA problem with signal $\bm{v}_* \in \textup{UBS}_k \cap \mathcal{P}^k$, where $\mathcal{P}^k$ is the path structure set. The \textup{SPCA-RECOVERY} satisfies the following property. 
%		
%		
%	
%%		Recall that the sequence of detectors $\mathcal{DE}_n$ is still a sequence of randomized polynomial time algorithms for path sparse PCA. Consider any instance $G \sim K\text{-}PC(N, K, 1/2)$ and corresponding $\bm{X}$ obtained by the reduction \textup{SPCA-RECOVERY} with input $G$. Once 
%%		\begin{align*}
%%			\left\{ 
%%			\begin{array}{llll}
%%				\mathcal{DE}_n(\bm{X}) = 1 & \text{ A path sparse clique is detected,} \\
%%				\mathcal{DE}_n(\bm{X}) = 0 & \text{ No path sparse clique is detected.}
%%			\end{array}
%%			\right.
%%		\end{align*}
%%		Due to the structure preserving property of \textup{SPCA-RECOVERY}, a path sparsity signal corresponds with a planted clique in graph $G$. Thus $\mathcal{DE}_n(\bm{X}) = 1$ implies a planted clique of $G$ for $K$-Partite PC, and vice versa. Then we have
%%		\begin{align*}
%%			\mathcal{DE}_n \circ \textup{SPCA-RECOVERY}: \mathcal{G}_n \to \{0, 1\} 
%%		\end{align*}
%%		a randomized polynomial time detector for $K$-Partite PC detection problem, which contradicts to the Conjecture~\ref{conj:KPC-hardness}. \gwcomment{be formal as the conjecture 1 for K-PC.}
%%		
%%		
%%		Therefore, there is no randomized polynomial time algorithms for the recovery problem of path sparse PCA. 
%\end{enumerate}
%
%
%\end{proof}































