%!TEX root = IEEETSP-main-paper.tex

%\section{End-to-end analysis for specific examples} \label{sec:specific-examples}
%
%In this section, we provide an end-to-end analysis for tree-sparse and path-sparse PCA, including results on their information-theoretic limits of estimation as well as the performance of the projected power method when initialized using covariance thresholding. We complement these with some matching suggestions of computational hardness in these problems. 

\subsection{Tree-sparse PCA} \label{sec:TS-PCA}

\subsubsection{Fundamental limits for Tree-Sparse PCA} \label{sec:limits-TS-PCA}

Recall the notation $\mathcal{T}^k$ as the set of all rooted binary subtrees in the underlying \emph{complete binary tree} from Section~\ref{sec:TS-PCA-intro}. We write $\bm{v} \in \mathcal{T}^k$ if the support set of $\bm{v}$ satisfies $\mathsf{supp}(\bm{v}) \in \mathcal{T}^k$. Let
\begin{align}
    \widehat{\bm{v}}_{\mathsf{TS}} := \argmax_{\bm{v}} ~ \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} ~~\text{s.t.}~~ \bm{v} \in \mathcal{S}^{d - 1} \cap \mathcal{T}^k \label{eq:TS-PCA-est}
\end{align} 
denote the estimator obtained from exhaustive search. 
%Define $T^{*} := \argmax_F \rho(\bm{W}, {F}) \quad \text{s.t.}\quad F = \text{conv}(L_{m_1} \cup L_{m_2} \cup L_{m_3}), ~ \text{for all} ~ L_{m_1} \neq L_{m_2} \neq L_{m_3} \in \mathcal{T}^k.$ \mqcomment{We seems didn't use the definition of $T^{*}$ in the main text.}

\begin{corollary}\label{coro:TS-PCA-fund-limits}
%Let $\mathcal{T}^k$ be defined as in Section~\ref{sec:TS-PCA-intro}. 
There exists a pair of positive constants $(c, C)$ such that the following holds. \\
\noindent (a) Without loss of generality, suppose $\langle \bm{v}_*, \widehat{\bm{v}}_{\mathsf{TS}} \rangle \geq 0$. Then for any $c_1 > 0$ and $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{T}^k$, we have
\[  \big\|\widehat{\bm{v}}_{\mathsf{TS}} - \bm{v}_*\big\|_2 \leq  C \left( \frac{1 + \lambda}{\lambda}\right) \cdot \sqrt{\frac{(3 + \ln 2 + c_1)k}{n}}
\]
with probability at least $1 - 2\exp(- c_1k)$. \\
\noindent (b) We have the minimax lower bound 
\begin{align*}
&\inf_{\widehat{\bm{v}}} \; \sup_{\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{T}^k} \mathbb{E}\left[ \left\| \widehat{\bm{v}} \widehat{\bm{v}}^{\top} - \bm{v}_* \bm{v}_*^{\top}  \right\|_F \right] \geq c \cdot  
    \min \Bigg\{\frac{1}{4\sqrt{\log k}}, ~~ \frac{1}{4}\sqrt{\frac{1 + \lambda}{8 \lambda^2}} \sqrt{\frac{k/\log k}{n}} \Bigg\}.
\end{align*}
Here, the infimum is taken over all measurable functions of the observations $\{ \bm{x}_i \}_{i = 1}^n$ drawn i.i.d. from the distribution $\mathcal{D}(\lambda; \bm{v}_*)$.
\end{corollary}
Corollary~\ref{coro:TS-PCA-fund-limits} is proved in Section~\ref{proof-coro-TS-PCA-fund-limits}. The term
%
%Corollary~\ref{coro:TS-PCA-fund-limits}(a) evaluates the term 
$\sqrt{k/n}$ arises from evaluating the cardinality of the set  $\mathcal{T}^k$ in tree-sparse PCA. In particular, 
%this term is of the order $O(\sqrt{k/n})$, since the size of 
we have $|\mathcal{T}^k| \leq (2e)^k / (k + 1)$  \citep{baraniuk2010model}, and taking logarithms results in a logarithmic factor gain over vanilla sparse PCA. Corollary~\ref{coro:TS-PCA-fund-limits}(b) provides a minimax lower bound of $\Omega(\sqrt{k / (n \log k)})$ for tree-sparse PCA, which has a logarithm gap $\sqrt{1/\log k}$ compared with the upper bound in Corollary~\ref{coro:TS-PCA-fund-limits}(a). This gap is small for small $k$, but we conjecture that it can be eliminated.

\begin{remark} \label{remark:logd-saving-TSPCA}
Compared with the fundamental limits for vanilla sparse PCA, the upper bounds for tree-sparse PCA in Corollary~\ref{coro:TS-PCA-fund-limits} save a factor $\log d$, which parallels the 
%Such a difference is obtained due to tighter upper bounds for the noise $\rho(\bm{W}, \mathcal{T}^k)$ and term $\max_{\bm{z} \in \mathcal{Z}_{*}}|\mathcal{N}_H(\bm{z}; k/2)|$ (see Theorem~\ref{thm:fund-limits}) in the tree-sparse case. In particular, we would like to point out that the tree sparsity structure can at most reduce the sample complexity/rate by an $O(\log d)$ factor, which parallels the 
model-based compressed sensing literature. The saving could be significant in practice when $d$ is large (see Figures~\ref{figure-tree-sparse} to follow)---indeed, this is one of the successes behind model-based compressive sensing. 
\end{remark}

% Figure environment removed


% \apcomment{At this point, we need a single figure with two subfigures. Condense the current text into a caption for the main figure. The subfigures will then be (a) and (b) of the big figure. They will have their own captions.}

% This section aims to demonstrate the efficiency and advantages {\color{blue}(a saving of the logarithm term $\log d$, see Remark~\ref{remark:logd-saving-TSPCA})} of using exact tree sparsity projection by comparing the performances of proposed methods (i.e., initialize with Algorithm~\ref{alg:initialization} -- Covariance Thresholding, followed by Algorithm~\ref{alg:PPM} -- Projected Power Method) with or without using exact tree sparsity projection. 

% We begin with the settings and procedures of our simulation experiments. Given the sample dimension $d = 2^L - 1$, sparsity $k$, and eigengap $\lambda$, we choose a particular tree sparsity support set $T_* \in \mathcal{T}^k$ and set the ground truth vector $\bm{v}_*$ as $ [\bm{v}_*]_i = \pm \frac{1}{\sqrt{k}}$ if $i \in T_*$ and $[\bm{v}_*]_i = 0$ if $i \notin T_*$. For each tuple of $(d, k, \lambda)$ and a pre-determined number of samples $n$, we generate 50 independent sample sets $\bm{X}^{(1)}, \ldots, \bm{X}^{(50)}$. Each sample set $\bm{X}^{(\ell)}$ has $n$ i.i.d. samples from the distribution $\mathcal{D}(\lambda, \bm{v}_*)$ based on the Wishart model as mentioned in Section~\ref{sec:setting-background}. That is to say, $\bm{X}^{(1)} := \left\{ \bm{x}_1^{(1)}, \ldots, \bm{x}_n^{(1)} \right\} \sim \mathcal{D}^{n}(\lambda, \bm{v}_*), \ldots, \bm{X}^{(50)} := \left\{ \bm{x}_1^{(50)}, \ldots, \bm{x}_n^{(50)} \right\} \sim \mathcal{D}^{n}(\lambda, \bm{v}_*)$. We then run Algorithm~\ref{alg:initialization} as our initialization method, followed by Algorithm~\ref{alg:PPM} based on every sample set. 

% The numerical comparison is between whether an exact tree sparsity projection or a classical $k$-sparsity projection is used in both Algorithm~\ref{alg:initialization} and Algorithm~\ref{alg:PPM}. The performance is measured by two metrics. The first metric is the point distance $\|\bm{v}_T - \bm{v}_*\|_2$ of the final iteration as we discussed in Theorem~\ref{thm:convergence}. Moreover, for people of independent interest, we further consider the support recovery property of the compared two methods using the success probability of support recovery, i.e., $\mathbb{P} \left[ \widehat{T}^{\mathcal{A}}(\bm{X}) = T_* \right]$, where $\widehat{T}^{\mathcal{A}}(\bm{X})$ denotes the support set obtained by the proposed methods $\mathcal{A}$ based on sample set $\bm{X}$. To be precise, we have $\mathcal{A} \in \{$Methods with k-sparse proj, Methods with tree-sparse proj$\}$ and $\bm{X} \in \left\{\bm{X}^{(1)}, \ldots, \bm{X}^{(50)} \right\}$. Therefore, given a fixed set of parameters (i.e., $(d, k, \lambda, n)$), the empirical success probability for performance measure can be computed by $\mathbb{P} \left[ \widehat{T}^{\mathcal{A}}(\bm{X}) = T_* \right] = \frac{1}{50} \sum_{\ell = 1}^{50} \bm{1}(\widehat{T}^{\mathcal{A}}(\bm{X}^{(\ell)}) = T_*).$ The numerical results are listed in Figure~\ref{fig:comparison-point-dist} and Figure~\ref{fig:comparison-prob-supp}. It is easy to observe that methods using tree-sparse projection outperform methods without using tree-sparse projection under all parameter settings we proposed. 

%This gap is due to the challenge of obtaining a tight upper bound for the term $\max_{\bm{z} \in \mathcal{Z}_{*}}|\mathcal{N}_H(\bm{z}; k/2)|$ (see Theorem~\ref{thm:fund-limits}) in the tree-sparse case, and we believe that a tighter bound would close such a gap.

\subsubsection{Local convergence and initialization} \label{sec:initialization-TS-PCA}

%Note that both the projected power method and initialization algorithm require an exact projection oracle.

%{\color{orange}
%Here, we propose the results of tractable exact projection onto $\mathcal{T}^k$, local geometric convergence for projected power method, and initialization method of tree sparse PCA. 
\paragraph{Exact projection oracle} We use the projection method proposed in \cite{cartis2013exact} as our tractable exact projection oracle $\Pi_{\mathcal{T}^k}$ for tree sparse PCA. This oracle has running time $O(kd)$. 
With our projection oracle in hand, we can now state our corollaries for the projected power method for tree sparse PCA.

%\gwcomment{Put projection oracle first.}

\begin{corollary} \label{coro:TS-PCA-PPM}
 Suppose in Algorithm~\ref{alg:PPM} that the initialization $\bm{v}_{0} \in \mathcal{T}^{k} \cap \mathcal{S}^{d-1}$ satisfies $\langle \bm{v}_0,\bm{v}_* \rangle \geq 1/2$. There exists a tuple of universal positive constants $(c,C_1,C_2, C_3)$ such that for $\lambda \geq C_1$, $n\geq C_2k$ and all $t \geq 1$, the iterate $\bm{v}_t$ from Algorithm~\ref{alg:PPM-project} satisfies 
 \[ 
 \| \bm{v}_{t} - \bm{v}_{*} \|_2 \leq \frac{1}{2^t} \cdot \| \bm{v}_{0} - \bm{v}_{*} \|_2 + C_3 \sqrt{\frac{k}{n}},
 \]
 with probability at least $1-\exp(- ck)$. 
%\apcomment{Is this for all $t$ or for each $t$? From discussion below it seems like a statement uniformly for all $t$. Please state properly.} \gwcomment{The above conditions holds for all $t$. The high probability condition is used to control $\rho(\mathcal{M}, F)$, which is decided when $\hat{\bm{\Sigma}}$ is given.
\end{corollary}
Corollary~\ref{coro:TS-PCA-PPM} is proved in Section~\ref{proof-coro-TS-PCA-PPM} from Theorem~\ref{thm:convergence}. 
%and by showing that
%\begin{align*}
%    \rho(\bm{W}, T^*) \leq (\lambda+1) \sqrt{\frac{(2 + \ln 2 + c)k}{n}} 
%\end{align*}
%holds with probability at least $1 - \exp(- ck)$, where $T^*$ above is defined similar to $P^*$ with respect to the tree sparsity set $\mathcal{T}^k$. \apcomment{This last sentence is weird. Improve definition of $F^*$, maybe by calling it $T^*$?} \gwcomment{Done}
%under current tree-sparse example.} {\color{orange}
%Based on $\Pi_{\mathcal{T}^k}$, we also have the following corollary of initialization method.}
We can also use the exact projection oracle $\Pi_{\mathcal{T}^k}$ to obtain the following corollary for our initialization method.
%\apcomment{Switch order. Local convergence first. Maintain parallel flow with main results.}\gwcomment{TODO.}  


\begin{corollary} \label{coro:initial-TS-PCA}
Assume $k^2 \leq d / e$. There exists a pair of universal positive constants $(C, C')$ such that if  $n\geq \max\{C\log d,k^{2}\}$ and $n \geq C'\max \big\{1,\lambda^{-2}\big\} \log(d/k^2) k^2,$ then Algorithm~\ref{alg:initialization} returns an initial vector $\bm{v}_0 \in \mathcal{S}^{d - 1} \cap \mathcal{T}^k$ satisfying $\langle \bm{v}_0, \bm{v}_* \rangle \geq 1/2$ with probability $1 - C'\exp(- \min\{\sqrt{d}, n\}/C')$. %$ for some positive constant $C'$}.
\end{corollary} 
Like Corollary~\ref{coro:initial-PS-PCA}, it is straightforward to see that Corollary~\ref{coro:initial-TS-PCA} follows from Theorem~\ref{thm:initialization-method} by specifying $c_0 = 1/2$. 

% We begin with the settings and procedures of our simulation experiments. Given the sample dimension $d = 2^L - 1$, sparsity $k$, and eigengap $\lambda$, we choose a particular tree sparsity support set $T_* \in \mathcal{T}^k$ and set the ground truth vector $\bm{v}_*$ as $ [\bm{v}_*]_i = \pm \frac{1}{\sqrt{k}}$ if $i \in T_*$ and $[\bm{v}_*]_i = 0$ if $i \notin T_*$. For each tuple of $(d, k, \lambda)$ and a pre-determined number of samples $n$, we generate 50 independent sample sets $\bm{X}^{(1)}, \ldots, \bm{X}^{(50)}$. Each sample set $\bm{X}^{(\ell)}$ has $n$ i.i.d. samples from the distribution $\mathcal{D}(\lambda, \bm{v}_*)$ based on the Wishart model as mentioned in Section~\ref{sec:setting-background}. That is to say, $\bm{X}^{(1)} := \left\{ \bm{x}_1^{(1)}, \ldots, \bm{x}_n^{(1)} \right\} \sim \mathcal{D}^{n}(\lambda, \bm{v}_*), \ldots, \bm{X}^{(50)} := \left\{ \bm{x}_1^{(50)}, \ldots, \bm{x}_n^{(50)} \right\} \sim \mathcal{D}^{n}(\lambda, \bm{v}_*)$. We then run Algorithm~\ref{alg:initialization} as our initialization method, followed by Algorithm~\ref{alg:PPM} based on every sample set. 

% The numerical comparison is between whether an exact tree sparsity projection or a classical $k$-sparsity projection is used in both Algorithm~\ref{alg:initialization} and Algorithm~\ref{alg:PPM}. The performance is measured by two metrics. The first metric is the point distance $\|\bm{v}_T - \bm{v}_*\|_2$ of the final iteration as we discussed in Theorem~\ref{thm:convergence}. Moreover, for people of independent interest, we further consider the support recovery property of the compared two methods using the success probability of support recovery, i.e., $\mathbb{P} \left[ \widehat{T}^{\mathcal{A}}(\bm{X}) = T_* \right]$, where $\widehat{T}^{\mathcal{A}}(\bm{X})$ denotes the support set obtained by the proposed methods $\mathcal{A}$ based on sample set $\bm{X}$. To be precise, we have $\mathcal{A} \in \{$Methods with k-sparse proj, Methods with tree-sparse proj$\}$ and $\bm{X} \in \left\{\bm{X}^{(1)}, \ldots, \bm{X}^{(50)} \right\}$. Therefore, given a fixed set of parameters (i.e., $(d, k, \lambda, n)$), the empirical success probability for performance measure can be computed by $\mathbb{P} \left[ \widehat{T}^{\mathcal{A}}(\bm{X}) = T_* \right] = \frac{1}{50} \sum_{\ell = 1}^{50} \bm{1}(\widehat{T}^{\mathcal{A}}(\bm{X}^{(\ell)}) = T_*).$ The numerical results are listed in Figure~\ref{fig:comparison-point-dist} and Figure~\ref{fig:comparison-prob-supp}. It is easy to observe that methods using tree-sparse projection outperform methods without using tree-sparse projection under all parameter settings we proposed. 



% \begin{subfigure}
% \centering
    % % Figure removed
    % % Figure removed
    % % Figure removed
    % \caption{Plot of the value of point distances $\|\bm{v}_T - \bm{v}_*\|_2$
     % verse the number of samples $n = \{20, 40, \ldots, 200\}$. For all three panels, we set $\lambda = 3$. We set $(d, k) = (255, 9)$, $(d, k) = (511, 10)$ and $(d, k) = (1023, 13)$ respectively for the three panels. The two curves in each panel correspond to the averaged values of point distance of the proposed methods with or without using exact tree sparse projection; the shaded parts represent the empirical standard deviations computed from 50 repetitions for each dot/cross in the panel, respectively. As we can observe, the point distances for the methods using tree-sparse projection are always smaller than the methods without tree-sparse projection, which demonstrate the efficient of using tree sparse projection. 
    % }\label{fig:comparison-point-dist}
% \end{subfigure}
% \hfill
% \begin{subfigure}
    % \centering
    % % Figure removed
    % % Figure removed
    % % Figure removed
    % \caption{Plot of the success probability $\mathbb{P} \left[ \widehat{T}^{\mathcal{A}}(\bm{X}) = T_* \right]$ 
    % verse the number of samples $n = \{20, 40, \ldots, 200\}$. We set $(d, k) = (255, 9)$, $(d, k) = (511, 10)$ and $(d, k) = (1023, 13)$ respectively for the three panels. The two curves in each panel correspond to the empirical probability of success of the proposed methods with or without using exact tree sparse projection. It is easy to observe that the phase transition for the methods using tree-sparse projection is earlier (with fewer samples) than the methods without tree-sparse projection.
    % }
    % \label{fig:comparison-prob-supp}
% \end{subfigure}






Corollary~\ref{coro:initial-TS-PCA} shows that provided $n = \Omega(k^{2})$, the output $\bm{v}_0 \in \mathcal{S}^{d - 1} \cap \mathcal{T}^k$ satisfies the initialization condition required for the subsequent projected power method to succeed. Putting these two results together, we have produced an end-to-end and computationally efficient algorithm that produces a statistically efficient solution provided $n = \Omega(k^{2})$. The next section is concerned with the question of whether the condition $n = \Omega(k^{2})$ is necessary for polynomial-time algorithms. %applied for tree-sparse PCA with  samples.  \apcomment{Following discussion should be used to motivate lower bound. Changing order of local/init will help.} \gwcomment{Move to the first paragraph of Section 4.1.3.}
%Although, the number of samples $O(k^2)$ required in Corollary~\ref{coro:initial-TS-PCA} is greater than the minimax lower bound proposed in Corollary~\ref{coro:TS-PCA-fund-limits}(b), Section~\ref{sec:examples-SDP-hard} shows that $O(k^2)$ is sharp up to a logarithm term of SDP hardness.


\subsubsection{SDP Hardness for Tree Sparse PCA.} \label{sec:examples-SDP-hard}


To understand the aforementioned gap in sample size, we now provide a computational lower bound for a class of SDP solutions to tree-sparse PCA, showing that they require on the order of $k^2$ samples.

To make things formal, we consider the following subclass of tree sparse PCA problems: every entry of the $k$ tree-sparse ground truth unit vector $\bm{v}_*$ only takes one of the values $\{0, \pm k^{-1/2}\}$. With knowledge of this side information in addition to tree sparsity, the natural choice of exhaustive estimator is given by the maximizer of the following optimization problem:
\begin{align}\label{re-opt-tree-sparse}
	\begin{array}{rllll}
		\max_{\bm{v}} & \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} \\
		\text{s.t.} & \|\bm{v}\|_2^2 = 1,\;\|\bm{v}\|_0 = k \\
		& \bm{v}(i)^2 \leq \bm{v}(\lfloor i / 2 \rfloor)^2\;\text{ for all } 2\leq i \leq d. 
	\end{array}	
\end{align}
%\begin{align}\label{re-opt-tree-sparse}
%%\widehat{\bm{v}}_{\mathsf{TS-side}} := 
%\max_{\bm{v}} & ~ \;\; \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} \\
%\text{s.t.} & ~ \|\bm{v}\|_2^2 = 1,\;\|\bm{v}\|_0 = k \notag \\
%& ~ \bm{v}(i)^2 \leq \bm{v}(\lfloor i / 2 \rfloor)^2,\;\text{ for all } 2\leq i \leq d. \notag
%\end{align} 
%\apcomment{removed $\widehat{\bm{v}}_{\mathsf{TS-side}}$ definition. Define once again if needed in proof.} \gwcomment{Done. $\widehat{\bm{v}}_{\mathsf{TS-side}}$ is not used in the proof.}
%to denote the estimator of TS-side from exhaustive search. 
The natural semidefinite programming (SDP) relaxation of the program~\eqref{re-opt-tree-sparse} is then given by
\begin{align}\label{SDP-tree-sparse-PCA}
	\begin{array}{rllll}
		\mathsf{SDP}(\widehat{\bm{\Sigma}}) = \max_{\bm{M} \in \mathbb{R}^{d \times d}} & \sum_{i=1}^{d} \sum_{j=1}^{d} \widehat{\bm{\Sigma}}_{ij} \bm{M}_{ij} \\
		\text{s.t.} & \sum_{i=1}^{d} \bm{M}_{ii}^{2} = 1 \\
		& \sum_{i=1}^{d}\sum_{j=1}^{d} |\bm{M}_{ij}| \leq k \\
		& \bm{M} \succeq \bm{0}_{d \times d} \\
		& \bm{M}_{ii} \leq \bm{M}_{\lfloor i / 2 \rfloor\lfloor i / 2 \rfloor} \text{ for all }\; 2\leq i\leq d.
	\end{array}
\end{align}
%\begin{align}\label{SDP-tree-sparse-PCA}
%\begin{split}
%    \mathsf{SDP}(\widehat{\bm{\Sigma}}) = \max_{\bm{M} \in \mathbb{R}^{d \times d}} &\sum_{i=1}^{d} \sum_{j=1}^{d} \widehat{\bm{\Sigma}}_{ij} \bm{M}_{ij} 
%    \\  \text{s.t.} \quad &\sum_{i=1}^{d} \bm{M}_{ii}^{2} = 1,\; \sum_{i=1}^{d}\sum_{j=1}^{d} |\bm{M}_{ij}| \leq k \\
%    & \bm{M} \succeq \bm{0}_{d \times d}
%    \\ & \bm{M}_{ii} \leq \bm{M}_{\lfloor i / 2 \rfloor\lfloor i / 2 \rfloor} \; \text{ for all }\; 2\leq i\leq d. 
%\end{split}
%\end{align}

It is well-known that for vanilla sparse PCA, the SDP attains the best-known sample complexity among all polynomial time algorithms. Proving a lower bound for this class of algorithms is thus powerful---when this subclass of low-degree estimators fails at the indicated threshold, it suggests a natural hardness result.

\begin{proposition}\label{prop:TS-SDP-hard}
    Suppose data $\bm{X}$ are drawn from the distribution $\mathcal{D}(\lambda; \bm{v}_*)$ with ground truth $\bm{v}_*$ given by a $k$ tree-sparse unit vector with every entry of taking one of the values in the set $\{0, \pm k^{-1/2}\}$.
    There exists a tuple of universal positive constants $(c,c_1,C,C_1)$ such that for $c_1d \leq n \leq C_1 d,\; n \leq ck^{2}$ and $1 \leq \lambda \leq \frac{d}{Cn}$, the optimal solution $\bm{M}_*$ of the SDP relaxation~\eqref{SDP-tree-sparse-PCA} satisfies $\big\| \bm{M}_* - \bm{v}_{*}\bm{v}_{*}^{\top}\big\|_{2} \geq \frac{1}{5}$ with probability at least $1- \tilde{c} d^{-\tilde{c}}$ for some constant $\tilde{c} \geq 1$.
\end{proposition}


In words, Proposition~\ref{prop:TS-SDP-hard} shows that unless the number of samples satisfies $n \geq C' k^2$ for some positive constant $C' \geq c$, the optimal solution $\bm{M}_*$ of the SDP relaxation~\eqref{SDP-tree-sparse-PCA} fails to estimate the ground truth consistently, even with the side information that its entries take only one of three values. The proof of Proposition~\ref{prop:TS-SDP-hard} can be found in Section~\ref{proof-prop-TS-SDP-hard}, and builds on the techniques proposed in [Section 4, \cite{ma2015sum}]. 
%Moreover, in Section~\ref{proof-prop-TS-SDP-hard}, we show that when the number of samples $n \ll k^2$, the \emph{detection task} of SDP relaxation~\eqref{SDP-tree-sparse-PCA} for TS-side (and tree-sparse PCA) also fails.  
%In particular, we prove that the optimal solution $\bm{M}_*$ is not too close with the desired rank-1 matrix $\bm{v}_* \bm{v}_*^{\top}$. Proposition~\ref{prop:TS-SDP-hard} ensures that when the number of samples $n \ll k^2$, optimal solution $\bm{M}_*$ of SDP relaxation~\eqref{SDP-tree-sparse-PCA} for tree-sparse PCA fails to estimate the ground truth. 
%
%Moreover, for people with independent interests, in Section~\ref{proof-prop-TS-SDP-hard}, we show that when the number of samples $n \ll k^2$, the \emph{detection task} of SDP relaxation~\eqref{SDP-tree-sparse-PCA} for tree-sparse PCA also fails. 
%}


%\section{Numerical Experiments for Tree Sparsity} 
%\apcomment{Create a section (not subsection) called Numerical Experiments.} \gwcomment{I am wondering whether this section numerical experiment should be placed under the Section IV: end-to-end analysis for specific examples}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\subsubsection{SDP Hardness for Tree Sparse PCA.} \label{sec:examples-SDP-hard}

We use the following Assumption~\ref{assump:discrete-entries} to simplify the proof of SDP hardness. 

\begin{assumption} \label{assump:discrete-entries}
\textbf{Discrete Entries.} Suppose that $\bm{v}_*$ has discrete entries, namely that $[\bm{v}_*]_i \in \left\{0, \pm 1 / \sqrt{k} \right\}$ for all $i \in [d]$, where $k$ denotes the number of non-zero entries. 
\end{assumption} 

Based on Assumption~\ref{assump:discrete-entries}, we scale the variables $\bm{v}$ up by a factor of $\sqrt{k}$ for simplicity, i.e., each entry of $\bm{v}$ now takes value from $\left\{0, \pm 1 \right\}$. The \emph{re-scaled estimation} $\widehat{\bm{v}}_{\mathsf{TS}}'$ for tree-sparse PCA can be therefore formulated explicitly as follows: 
\begin{align}
    \widehat{\bm{v}}_{\mathsf{TS}}' := \argmax_{\bm{v}} ~  \frac{1}{k} \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} ~~\text{s.t.}~~ \|\bm{v}\|_2^2 = k, ~~ \bm{v} \in \mathcal{T}^k = \left\{ \bm{v} \left|
    \begin{array}{llll}
        \|\bm{v}\|_0 = k \\
        \bm{v}_{i}^2 \leq \bm{v}_{\lfloor i / 2 \rfloor}^2 ~~ \forall i \in \{2, \ldots, d\}
    \end{array} \right.
    \right\},  \label{eq:TS-PCA-est-2}
\end{align} 
where the constraint $\bm{v}_{i}^2 \leq \bm{v}_{\lfloor i / 2 \rfloor}^2, ~ \forall i \in \{2, \ldots, d\}$ ensures that $\text{supp}(\bm{v})$ represents the support set of a connected subtree of $\text{CBT}$ with root node $1$. 
Here, it is easy to observe that the original estimation $\widehat{\bm{v}}_{\mathsf{TS}}$ of tree-sparse PCA satisfies $\widehat{\bm{v}}_{\mathsf{TS}} = \widehat{\bm{v}}_{\mathsf{TS}}' / \sqrt{k}$ by re-scaling the factor $\sqrt{k}$ back. 

Recall the number of samples $n = O(k^2 / \lambda^2)$ required for the initialization method is greater than the minimax lower bound from Part (b) of Corollary~\ref{coro:TS-PCA-fund-limits}. In this part, we show that $O(k^2 / \lambda^2)$ samples required for the initialization method is almost ``tight'' in some sense. In particular, consider the SDP relaxation of the optimization for exhaustive search~\eqref{eq:TS-PCA-est-2} of the tree sparse PCA, 
\begin{align}
    \begin{array}{rllll}
        \text{SDP}(\widehat{\bm{\Sigma}}) := \max_{M \in \mathbb{R}^{d \times d}} & \frac{1}{k} \sum_{i,j = 1}^d \widehat{\bm{\Sigma}}_{ij} M(\bm{v}_i \bm{v}_j) \\
        \text{s.t.} & \sum_{i = 1}^d M(\bm{v}_i^2) = k \\
        & \sum_{i,j = 1}^d |M(\bm{v}_i\bm{v}_j)| \leq k^2 \\
        & M(\bm{v}_i^2) \leq M(\bm{v}_{\lfloor i / 2 \rfloor}^2) & \forall i \in \{2, \ldots, d\} \\
        & M \succeq \bm{0}_{d \times d} 
    \end{array},  \label{eq:SDP-tree-sparsity}
\end{align}
where $M: \mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}$ can be viewed as a ``pseudo moment''. The Proposition~\ref{thm:TS-estimation} shows that the optimal solution $M^*$ of the SDP relaxation~\eqref{eq:SDP-tree-sparsity} fails to estimate the principal component $\bm{v}_*$ when the number of samples $n$ is smaller than the following lower bounds. 
 
% Based on the SDP relaxation~\eqref{eq:SDP-tree-sparsity}, the next proposition (Proposition~\ref{thm:TS-estimation}) shows that the estimation problem for the tree-sparse PCA fails when the number of samples $n$ is smaller than the corresponding lower bounds.
% The proof of the Proposition~\ref{thm:TS-estimation} uses the techniques proposed in [Section 4, \cite{ma2015sum}]. In particular, we prove that the optimal solution $M^*$ obtained by solving  \eqref{eq:SDP-tree-sparsity} is not too close with the desired rank-1 matrix $\bm{v}_* \bm{v}_*^{\top}$.  

% In particular, we construct a feasible ``pesudo-moment'' $M(\cdot)$ of the SDP relaxation~\eqref{eq:SDP-tree-sparsity} that: when samples $\{\bm{x}^i\}_{i = 1}^n$ are i.i.d. generated from the $H_0$, the $\text{SDP}(\widehat{\bm{\Sigma}}) \geq 1 + \lambda$ holds with high probability, i.e., the detection task fails (see Appendix~\ref{app:TS-PCA} for details). As a result, we could obtain a lower bound on the number of samples for the detection problem of this SDP relaxation. \gwcomment{1. Should we say the following two results as Propositions? 2. Should we remove the result about the \textit{detection} problem (i.e., previous paragraph defining the detection problem and Proposition 1) to Appendix?}

% For the estimation problem, we prove that the optimal solution $M^*$ obtained by solving  \eqref{eq:SDP-tree-sparsity} is not too close with the desired rank-1 matrix $\bm{v}_* \bm{v}_*^{\top}$. 

\begin{proposition}\label{thm:TS-estimation} 
For any constant $B$, there exists absolute constants $C$ and $r$ such that for $\lambda \leq B/2, ~ Bn \geq d \geq 2 \lambda n$ and $o(d) \geq k \geq C \sqrt{n} \log^r d$, suppose the data $\bm{X}$ is drawn from the distribution $\mathcal{D}(\lambda; \bm{v}_*)$ with $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{T}^k$, then with high probability $(1 - d^{-10})$ over the randomness of the data, the optimal solution $M^*$ satisfies $\|\frac{1}{k} M^* - \bm{v}_* \bm{v}_*^{\top}\|_2 \geq \frac{1}{5}$. 
\end{proposition}

The proof of Proposition~\ref{thm:TS-estimation} can be found in Appendix~\ref{app:TS-estimation}, which uses the techniques proposed in [Section 4, \cite{ma2015sum}]. In particular, we prove that the optimal solution $M^*$ is not too close with the desired rank-1 matrix $\bm{v}_* \bm{v}_*^{\top}$. Proposition~\ref{thm:TS-estimation} ensures that when the number of samples $n \ll k^2$, optimal solution $M^*$ of SDP relaxation~\eqref{eq:SDP-tree-sparsity} for tree-sparse PCA fails to estimate the ground truth. Moreover, for people with independent interests, in Appendix~\ref{app:TS-detection}, we show that when the number of samples $n \ll k^2$, the \emph{detection task} of SDP relaxation~\eqref{eq:SDP-tree-sparsity} for tree-sparse PCA also fails. 

\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse 
\subsection{Path-Sparse PCA} \label{sec:PS-PCA}

%\apcomment{Make changes to this section in parallel to suggested changes from the tree-sparse section.}

Recall the notation $\mathcal{P}^k$ as the structure set of path-sparse PCA from Section~\ref{sec:PS-PCA-intro}. We write $\bm{v} \in \mathcal{P}^k$ if the support set satisfies $\mathsf{supp}(\bm{v}) \in \mathcal{P}^k$. 


\subsubsection{Fundamental statistical limits}

Let
\begin{align}
    \widehat{\bm{v}}_{\textsf{PS}} := \argmax_{\bm{v}} ~ \bm{v}^{\top} \widehat{\bm{\Sigma}} \bm{v} ~~\text{s.t.}~~ \bm{v} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k \label{eq:PS-PCA-est}
\end{align}  
denote the corresponding estimate from exhaustive search.


\apcomment{I want the technical writing in this subsection and the next to look EXACTLY parallel to tree-sparse case. It currently does not. You have Proposition 2 instead of corollary, and there is impreciseness in result statements.}
\gwcomment{The following corollaries are revised.}

\begin{corollary}\label{coro:PS-PCA-fund-limits}
{\color{orange}Let $\mathcal{P}^k$ be defined as in Section~\ref{sec:PS-PCA-intro}. There is a pair of positive constants $(c, C)$ such that the following is true.

%Based on Theorem~\ref{thm:fund-limits}, suppose the union-of-linear structures condition in Definition~\ref{cond:linear-structure} holds.
\noindent (a) Without loss of generality, assume $\langle \bm{v}_*, \widehat{\bm{v}}_{\mathsf{PS}} \rangle \geq 1/2$. Then for any $c > 0$ and $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$, we have
\begin{subequations}
{\color{orange}
\begin{align*}
    \big\|\widehat{\bm{v}}_{\mathsf{PS}} - \bm{v}_*\big\|_2 \leq C \left( \frac{1 + \lambda}{\lambda} \right) \sqrt{\frac{3 (\ln d - \ln k)k + ck}{n}} 
\end{align*}
with probability at least $1 - 2\exp(- ck)$.}

\noindent (b) Suppose that $d\geq 16 k^2$ and $k\geq 4$. Then we have the minimax lower bound
\begin{align*}
    \inf_{\widehat{\bm{v}}} \; \sup_{\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k} \mathbb{E}\left[ \left\| \widehat{\bm{v}} \widehat{\bm{v}}^{\top} - \bm{v}_* \bm{v}_*^{\top}  \right\|_F \right] \geq  c \cdot \min\bigg\{1,  \sqrt{\frac{1 + \lambda}{8 \lambda^2}} \sqrt{\frac{k \cdot \big( \frac{\ln d}{2} - \ln k \big)}{n}} \bigg\}. 
\end{align*}
\end{subequations}
Here, the infimum is taken over all measurable functions of the observations $\{ \bm{x}_i \}_{i = 1}^n$ drawn i.i.d. from the distribution $\mathcal{D}(\lambda; \bm{v}_*)$.}
\end{corollary}

Corollary~\ref{coro:PS-PCA-fund-limits} is proved in Section~\ref{proof-coro-PS-PCA-fund-limits}. Corollary~\ref{coro:PS-PCA-fund-limits} studies fundamental limits for path-sparse PCA based on Theorem~\ref{thm:fund-limits}. In particular, Corollary~\ref{coro:PS-PCA-fund-limits}(a) gives an upper bound of the estimation $\widehat{\bm{v}}_{\textsf{PS}}$, where the statistical noise term $\rho(\bm{W}, F^*)$ is of the order $(\lambda + 1) \sqrt{k \cdot ( \ln d - \ln k)/n}$ under the path sparsity case. In comparison with existing result, the minimax lower bound obtained in Corollary~\ref{coro:PS-PCA-fund-limits}(b) is in the same order of the minimax lower bound given in [Theorem 1, \cite{asteris2015stay}] with the outer degree parameter $|\Gamma_{\text{out}}(v)| = (d - 2) / k$. 

\iffalse
Corollary~\ref{coro:PS-PCA-fund-limits} studies fundamental limits for path-sparse PCA based on Theorem~\ref{thm:fund-limits}. In particular, Corollary~\ref{coro:PS-PCA-fund-limits}(a) gives an upper bound of the estimation $\widehat{\bm{v}}_{\textsf{PS}}$, where the statistical noise term $\rho(\bm{W}, F^*)$ is of the order $\sqrt{(1 + \lambda) / 8 \lambda^2}\sqrt{k \ln d / n}$ under the path sparsity case. The proof of Corollary~\ref{coro:PS-PCA-fund-limits}(a) can be found in Appendix~\ref{app:ub-coro}. Corollary~\ref{coro:PS-PCA-fund-limits}(b) provides a minimax lower bound for the tree-sparse PCA under a mild condition $d \geq \max\left\{4k, ~ 2^{H(\xi)/\xi} k^{1/\xi} \right\}$. To illustrate the above claim, by setting $\xi = 7/8 \in (3/4, 1)$, we only request $d \geq \max\{4k, 2^{8H(7/8)/7} k^{8/7} \} = O(k^{8/7})$. In comparison with existing result, the minimax lower bound obtained in Corollary~\ref{coro:PS-PCA-fund-limits}(b) is in the same order of the minimax lower bound given in [Theorem 1, \cite{asteris2015stay}] with the outer degree parameter $|\Gamma_{\text{out}}(v)| = (d - 2) / k$. The proof of Corollary~\ref{coro:PS-PCA-fund-limits}(b) is given in Appendix~\ref{app:coro-minimax-lb}. 
\fi

\subsubsection{Initialization and local convergence} 

{\color{orange}Like Section~\ref{sec:TS-PCA}, we first establish the exact projection oracle.\\

\noindent \textbf{Exact projection oracle:} The exact projection oracle of path-sparse PCA $\Pi_{\mathcal{P}^k}$ is built by picking the component of the largest absolute value in each partition (layer) for a given $(d,k)$-layered graph $G$ (with running time $O(d)$). Using such projection oracle, we can now state our corollaries for the projected power method for path sparse PCA.}

\begin{corollary} \label{coro:PS-PCA-PPM}
 Suppose in Algorithm~\ref{alg:PPM-project} the initialization $\bm{v}_{0} \in \mathcal{T}^{k} \cap \mathcal{S}^{d-1}$ and $\langle \bm{v}_0,\bm{v}_* \rangle \geq 1/2$. There are universal positive constants $C_1,C_2,C_3$ and $c$ such that for $\lambda \geq C_1$, $n\geq C_2k\ln(d)$,  {\color{orange}and all $t \geq 1$, the iterate $\bm{v}_t$  in Algorithm~\ref{alg:PPM-project} satisfies }
% the iterates $\{\bm{v}_{t}\}_{t=0}^{T}$ in Algorithm~\ref{alg:PPM-project} satisfies
\begin{align*}
    \| \bm{v}_{t} - \bm{v}_{*} \|_2 \leq \frac{1}{2^t} \cdot \| \bm{v}_{0} - \bm{v}_{*} \|_2 +  C_3 \sqrt{\frac{k(2\ln d - \ln k)}{n}}
\end{align*}
with probability at least $1-\exp(-ck)$. 
\end{corollary}
Corollary~\ref{coro:PS-PCA-PPM} is proved in Section~\ref{proof-coro-PS-PCA-PPM}. It is proved by applying Theorem~\ref{thm:convergence} and showing
\begin{align*}
    \rho(\bm{W}, P^*) \lesssim  (1+\lambda)\sqrt{\frac{k\ln d + ck}{n}}
\end{align*}
holds with probability at least $1 - \exp(- ck)$, where $P^*$ above is defined similar to $F^*$ with respect to path sparsity set $\mathcal{P}^k$.

{\color{orange} 
We are now ready to propose the result of initialization for path sparse PCA using $\Pi_{\mathcal{P}^k}$.} 

%We set $\mathcal{M} = \mathcal{P}^k$ and use the exact projection oracle of path-sparse PCA $\Pi_{\mathcal{P}^k}$ \footnote{The exact projection $\Pi_{\mathcal{P}^k}$ can be established by picking the component of largest absolute value in each partition (layer) for a given $(d,k)$-layered graph $G$.} (with running time $O(d)$) in both Algorithm~\ref{alg:PPM-project} and Algorithm~\ref{alg:initialization}.
{\color{orange}
\begin{corollary} \label{coro:initial-PS-PCA}
Assume $k^2 \leq d / e$. There exists universal constant $(C, C')$ such that if $n\geq \max\{C\log d,k^{2}\}$ and 
\begin{align*}
    n \geq \frac{C'\max\{\lambda^2,1\} k^2}{\lambda^2} \log(d/k^2), 
\end{align*}
the initial vector $\bm{v}_0 \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$ obtained from Algorithm~\ref{alg:initialization} satisfies $\langle \bm{v}_0, \bm{v}_* \rangle \geq 1/2$ with probability $1 - C'\exp(- \min\{\sqrt{d}, n\}/C')$ for some positive constant $C'$.
\end{corollary}}

It is straightforward to see that Corollary~\ref{coro:initial-PS-PCA} follows from Theorem~\ref{thm:initialization-method} by specifying $c_0 = 0.5$. 
%The proof of Corollary~\ref{coro:initial-PS-PCA} is derived from Theorem~\ref{thm:convergence} in Appendix~\ref{app:initial-methods}. 
Like Corollary~\ref{coro:initial-TS-PCA}, Corollary~\ref{coro:initial-PS-PCA} provides an initialization method whose outputs can be used for the general projected power method (Algorithm~\ref{alg:PPM}) for path-sparse PCA when the number of samples $n \geq O(k^2)$. Still, there is a gap in sample complexity between the minimax lower bound and what we obtained in Corollary~\ref{coro:initial-PS-PCA}. In the next Section~\ref{sec:examples-average-hard}, we show no randomized polynomial-time algorithm solving path-sparse with $n \ll k^2$ assuming the average-case hardness of the planted clique problem and its variants. 
\fi 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\iffalse
\subsubsection{Average-Case Hardness of Path Sparse PCA.}\label{sec:examples-average-hard}


\apcomment{I have not gone through this subsection. From a glance, it looks like it could be significantly polished.} \gwcomment{Partially done. (Will discuss with Ashwin and Mengqi)}

This section focuses on the average-case hardness of path sparse PCA problem. Before stating the main result of reduction, we start with a generalization of the well-known planted clique (PC) problem (see Definition~\ref{defn:PC}) -- Secret Leakage PC. Compared with the PC problem, the random graph $G$ in the Secret Leakage PC comes with some information about the vertex set of the planted clique. 

% \gwcomment{The following result follows from [\cite{brennan2019optimal}, \cite{brennan2020reducibility}]. So, fairly speaking, it cannot be viewed as our main result... What we did here is applied their result to path-sparse PCA. The reason of giving this section is to show that the number samples used in the initialization method is somehow "tight" from average-case hardness.} 

%\begin{definition} \label{defn:PC}
%\textbf{Planted Clique Problem (PC).} The task of planted clique is to find the vertex set of a $K$-clique planted uniformly at random in an $N$-vertex Erd\"{o}s-Renyi random graph $G$, which can be represented as a testing problem $\text{PC}(N, K, 1/2)$ between the following two hypothesises 
%\begin{align}
%    & H_0: ~ G \sim \mathcal{G}(N, 1/2) ~~~~\text{and}~~~~ H_1: ~ G \sim \mathcal{G}(N, K, 1/2), \notag
%\end{align}
%where $\mathcal{G}(N, 1/2)$ denotes the $N$-vertex Erd\"{o}s-Renyi random graph with edge density $1/2$ and $\mathcal{G}(N, K, 1/2)$ the distribution resulting from planting a $K$-clique uniformly at random in $\mathcal{G}(N, 1/2)$.
%\end{definition}

%A generalization of the planted clique problem (PC) -- Secret Leakage PC is proposed in \cite{brennan2020reducibility}, where the random graph $G$ comes with some information about the vertex set of the planted clique. 

\gwcomment{Need: 
\begin{itemize}
	\item Def 5 
	\item Def 6 K-PC
	\item Def hardness. What it means to be hard? K-PC is hard
	\item Coro: if K-PC is hard, then Path sparse PCA is hard
\end{itemize}
}

\begin{definition} \label{defn:SL-PC}
\textbf{Secret Leakage $\text{PC}_{\mathcal{D}}$,} \cite{brennan2020reducibility}. Given a distribution $\mathcal{D}$ on $k$-subsets of $[N]$, let $\mathcal{G}_{\mathcal{D}}(N, K, 1/2)$ be the distribution on $N$-vertex graphs sampled by first sampling $G \sim \mathcal{G}(N, 1/2)$ and $S \sim \mathcal{D}$ independently and then planting a $K$-clique on the vertex set $S$ in $G$. Let $\text{PC}_{\mathcal{D}}(N, K, 1/2)$ denote the resulting hypothesis testing problem between  
\begin{align}
    & H_0: ~ G \sim \mathcal{G}(N, 1/2) ~~~~\text{and}~~~~ H_1: ~ G \sim \mathcal{G}_{\mathcal{D}}(N, K, 1/2). \notag
\end{align} 
\end{definition}
\gwcomment{detection problem}

Now consider the following $K$-partite PC as a special case of the Secret Leakage $\text{PC}_{\mathcal{D}}$. 

\begin{definition} \label{def:KPC}
\textbf{$K$-Partite Planted Clique (with source and terminal).} The $K$-partite planted clique problem $K\text{-PC}(N, K, 1/2)$ is a special case of the secret leakage planted clique problem $\text{PC}_{\mathcal{D}}(N, K, 1/2)$. Here the vertex set of $G$ has two special vertices: source and terminal, and the remaining vertices are evenly partition into $K$ parts of size $(N - 2) / K$. The distribution $\mathcal{D}$ always picks source, terminal and uniformly picks one element at random in each part.  
\end{definition}

In order to simplify the analysis of average-case hardness, we use the following variant ofsparse PCA -- uniformly biased sparse PCA (UBSPCA) proposed in \cite{brennan2018reducibility}.  

\begin{definition} \label{def:UBSPCA}
\textbf{UBSPCA Problem,} \cite{brennan2018reducibility}. Consider the following simple hypothesis testing variant $\text{UBSPCA}(k,n,d,\lambda)$ of sparse PCA with hypothesis: 
\begin{align*}
    H_0: ~ \bm{X} \sim \mathcal{N}(\bm{0}_d, \bm{I}_d)^{\otimes n} ~~~~\text{and}~~~~ H_1: ~ \bm{X} \sim \mathcal{N}(\bm{0}_d, \lambda \bm{v}_* \bm{v}_*^{\top} + \bm{I}_d)^{\otimes n}, 
\end{align*}
where $\bm{v}_*$ is in the uniformly biased set $\textup{UBS}_k := \left\{ \left. \bm{v} \in \left\{ 0, 1/\sqrt{k}\right\}^d ~ \right| ~ \|\bm{v}\|_2 = 1, ~ \|\bm{v}\|_0 = k  \right\}.$
\end{definition}


{\color{blue}
The reduction of path sparse PCA is based on an existing average-case reduction method SPCA-RECOVERY which was proposed in [Figure 19, Section 8, \cite{brennan2018reducibility}]. In this section, we omit the theoretical guarantees of SPCA-RECOVERY given by \cite{brennan2018reducibility}. Reader with independent interests could see Appendix~\ref{app:reduction-PathPCA} (Theorem~\ref{thm:reduction-UBSPCA}) for details. 

\gwcomment{Maybe move the following commented Theorem to appendix?} 
\iffalse
The following Theorem~\ref{thm:reduction-UBSPCA} is a restatement of the theoretical results for SPCA-RECOVERY given by \cite{brennan2018reducibility}. 

\begin{theorem} \label{thm:reduction-UBSPCA}
\textbf{Restate of [Theorem 8.7, \cite{brennan2018reducibility}].} Let $\alpha \in \mathbb{R}, \beta \in (0,1)$, and suppose there exists a sequence $\{(N_i, K_i, D_i, \lambda_i)\}_{i = 1}^{\infty}$ of parameters such that: 
\begin{enumerate}
    \item The parameters are in the regime $d = \Theta(N), \lambda = \tilde{\Theta}(N^{- \alpha})$ and $k = \tilde{\Theta}(N^{\beta})$ or equivalently,
    \begin{align*}
        & \lim_{i \rightarrow \infty} \log (\lambda_i^{-1} / \log N_i) = \alpha & \lim_{i \rightarrow \infty} \log (K_i / \log N_i) = \beta
    \end{align*}
    \item If $\alpha > 0, \beta > 1/2$, then the following holds. Let $\epsilon > 0$ be fixed and let $\bm{X}_i$ be an instance of $\text{UBSPCA}(K_i,N_i,D_i,\lambda_i)$. There is no sequence of randomized polynomial-time computable functions $\phi_i: \mathbb{R}_n: \mathbb{R}^{D_i \times N_i} \to \binom{[N_i]}{k}^2$ such that for all sufficiently large $i$ the probability that $\phi_i(\bm{X}_i)$ is exactly the pair of latent row and column supports of $\bm{X}_i$ is at least $\epsilon$, assuming the PC recovery conjecture. 
\end{enumerate}
Therefore, given the PC recovery conjecture, the computational boundary for $\text{UBSPCA}(k,n,d,\lambda)$ 
in the parameter regime $\theta = \tilde{\Theta}(n^{- \alpha})$ and $k = \tilde{\Theta}(n^{\beta})$ is $\alpha^* = 0$ when $\beta > 1/2$. 
\end{theorem}
\fi

As a brief summary of Theorem~\ref{thm:reduction-UBSPCA}, the SPCA-RECOVERY maps an instance of planted clique problem $G \sim \text{PC}(n,k,1/2)$ to an instance $\bm{X} \sim \text{UBSPCA}(k,n,d,\lambda)$ approximately under total variance distance with $\lambda = \tilde{\Theta}(k^2/n), d = n$. Assuming the Hardness Assumption for Planted Clique Problem [Definition~\ref{defn:PC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving $\text{UBSPCA}(k,n,n,\lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$.}\\

Let $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ be the path-sparse PCA problem with signal $\bm{v}_* \in \textup{UBS}_k \cap \mathcal{P}^k$ with $\textup{UBS}_k$ defined in Definition~\ref{def:UBSPCA}. Combined with previous results, we have

\begin{corollary} \label{coro:reduction-PathPCA}
Under the same parameter regime presented in Theorem~\ref{thm:reduction-UBSPCA}, 
\begin{itemize}
    \item The SPCA-RECOVERY maps an instance $G \sim K\text{-PC}(n, k, 1/2)$ directly to an instance $\bm{X} \sim \text{Path-UBSPCA}(k + 2, n, d, \lambda)$ approximately under total variance distance with $\lambda = \tilde{\Theta}(k^2/n), d = n$. 
    \item Assuming the Hardness Assumption [Conjecture2 \& Conjecture 3, \cite{brennan2020reducibility}] for $K$-Partite Planted Clique Problem [Definition~\ref{def:KPC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving  $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$, which matches the number of samples $n$ required in Corollary~\ref{coro:initial-PS-PCA} (ignoring logarithm term). 
\end{itemize}
\end{corollary}
The proof of Corollary~\ref{coro:reduction-PathPCA} is presented in Section~\ref{app:reduction-PathPCA}. Corollary~\ref{coro:reduction-PathPCA} shows that the $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ has an average-case hardness lower bound on the sample size ($n \geq c k^2$ for some constant $c$). Since $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ is a special case of the path sparse PCA, the above result also ensures an average-case hardness lower bound for the path sparse PCA. 

\iffalse
Let us start with an existing average-case reduction method SPCA-RECOVERY that proposed in [Figure 19, Section 8, \cite{brennan2018reducibility}]. Based on [Theorem 8.7, \cite{brennan2018reducibility}], the SPCA-RECOVERY maps an instance of planted clique problem $G \sim \text{PC}(n,k,1/2)$ to an instance $\bm{X} \sim \text{UBSPCA}(k,n,d,\lambda)$ approximately under total variance distance with $\lambda = \tilde{\Theta}(k^2/n), d = n$. Assuming the Hardness Assumption for Planted Clique Problem [Definition~\ref{defn:PC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving $\text{UBSPCA}(k,n,n,\lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$.\\


In order to show average-case hardness for path-sparse PCA, we would like to point out that: The SPCA-RECOVERY Algorithm is designed to \emph{preserve the structure} during the reduction. 
\begin{itemize}
    \item \textbf{Observation.} Note that: each part of the vertex set $V$ in graph $G \sim K\text{-PC}(n, k, 1/2)$ can be viewed as the vertex set of a layer in the layered graph with size $(n - 2) / k$. Thus the planted $k$-clique (with one vertex in each part) in $G \sim K\text{-PC}(n, k, 1/2)$ corresponds to a path of length $k$ (excluding source and terminal) in the layered graph.
    
    \item \textbf{Structure Preserving.} Based on the structure preserving property and above observation, given $G \sim K\text{-PC}(n, k, 1/2)$ as input instance of SPCA-RECOVERY, the rows in the adjacency matrix $\bm{A}(G)$ concerning the planted $k$-clique of $G$ transfer/map to the rows corresponding to the support set of the underlying path of the path-sparse PCA for the sample matrix $\bm{X} = (\bm{x}_1 ~|~ \cdots ~|~ \bm{x}_n)^{\top}$ as the output of $\textup{SPCA-RECOVERY}(G)$.
    % \gwcomment{This is what I learned from \cite{brennan2019optimal}. However, since the Algorithm is very complex, I did not check the proof detail. Or we may refer to the result in \cite{brennan2018reducibility} for sparse PCA by restricting $\lambda \asymp \sqrt{k^2 / n \log n}$ with $k = K = w(\sqrt{N})$.} 
\end{itemize}

In conclusion, using [Theorem 8.7, \cite{brennan2018reducibility}] directly, the SPCA-RECOVERY maps an instance $G \sim K\text{-PC}(n, k, 1/2)$ directly to an instance $\bm{X} \sim \text{Path-UBSPCA}(k + 2, n, d, \lambda)$ approximately under total variance distance with $\lambda = \tilde{\Theta}(k^2/n), d = n$. Here we use $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ to denote the path-sparse PCA problem with signal $\bm{v}_* \in \textup{UBS}_k \cap \mathcal{P}^k$ with $\textup{UBS}_k$ defined in Definition~\ref{def:UBSPCA}. Similarly, assuming the Hardness Assumption [Conjecture2 \& Conjecture 3, \cite{brennan2020reducibility}] for $K$-Partite Planted Clique Problem [Definition~\ref{def:KPC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving  $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$, which matches the number of samples $n$ required in Corollary~\ref{coro:initial-PS-PCA} (ignoring logarithm term). 


\begin{remark} 
Notice that $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ of samples from SPCA-RECOVERY is a special case of path-sparse PCA defined in Section~\ref{sec:setting-background}. Moreover, as also mentioned in \cite{brennan2018reducibility}, the lower bound on the number of samples are only tight when $\lambda = \tilde{\Theta}(1)$. 
\end{remark}
\fi 

\fi


