%!TEX root = IEEETSP-main-paper.tex

\section{Appendix} 

Before proceeding to proofs of our main results, we present some calculations that were alluded to in the main text.

\subsection{Time-Consuming Case in Section~\ref{sec:setting-background}} \label{app:time-consuming-case}

Consider the following time-consuming case. 

\begin{example} \label{exmp:time-consuming-case}
\textbf{Time-Consuming Case.}
Given $\mathcal{L} = \{L_1, \ldots, L_{d - 1}, L_d\}$ with $L_i = \text{span}(\phi_i, \phi_{i + 1})$ for $i = 1, \ldots, d - 1$ and $L_d = \text{span}(\phi_1, \phi_d)$. Each linear subspace $L_i$ is known by given two linearly independent but not necessarily orthonormal vectors, say $\bm{u}^{(i)}_1, \bm{u}^{(i)}_2$, in $L_i$. As a result, for a given linear subspace $L$, we do not know the index $i \in [d]$ of this linear subspace $L$ based on the given vectors $\bm{u}^{(\cdot)}_1, \bm{u}^{(\cdot)}_2 \in L$. Hence the corresponding two bases that spans this known linear subspace $L$ is unknown to us.
\end{example}

From the Example~\ref{exmp:time-consuming-case}, if two linear subspaces $L, L'$ have a non-zero intersection, i.e., $L \cap L' \neq \{\bm{0}\}$, then the base $\phi = L \cap L' \in \mathcal{B}$ is uniquely determined, and so as the rest two bases in $L, L'$ respectively. Thus computing $\bm{\Phi}$ from $\mathcal{L} := \{L_1, \ldots, L_{d - 1}, L_d\}$ is equivalent to find out all bases $\phi \in \mathcal{B}$ via intersection verification. Since we do not know the index corresponding to each linear subspace, to compute one base in $\mathcal{B}$, what we can do is to verify the intersection of two randomly chosen linear subspaces. The detailed procedures of computing the unknown orthonormal basis $\mathcal{B}$ are presented in the randomized algorithm~\ref{alg:IV}. 

\begin{algorithm}
\caption{Intersection Verification for $\mathcal{B}$ or $\bm{\Phi}$}
\label{alg:IV}
\textbf{Input.} $\mathcal{L}$, each linear subspace $L \in \mathcal{L}$ is represented by $\text{dim}(L)$ independent vectors in $L$.
\begin{algorithmic}[1]
\State \textbf{Initialize} $\mathcal{L}^{(0)} := \emptyset, \mathcal{B}^{(0)} := \emptyset, t = 0$.
\While{$|\mathcal{B}^{(t)}| < d$} \hfill \textbf{outer while-loop} \label{alg:IV-outer}
\State Pick a linear subspace $L^{(t)} \in \mathcal{L} \backslash \mathcal{L}^{(t)}$.
\While{True} \hfill \textbf{inner while-loop} \label{alg:IV-inner} 
\State Select $\tilde{L}^{(t)} \in \mathcal{L} \backslash \{L^{(t)}\}$ uniformly at random without replacement. \label{alg:IV-selection}
\If{$L^{(t)} \cap \tilde{L}^{(t)} \neq \{\bm{0}\}$}
\State Compute three bases for $L^{(t)}, \tilde{L}^{(t)}$. 
\State Update $\mathcal{B}^{(t + 1)}$ via adding the above three bases. \label{alg:IV-basis}
\State Update $\mathcal{L}^{(t + 1)} := \mathcal{L}^{(t)} \cup \{L^{(t)}, \tilde{L}^{(t)}\}$.
\State \textbf{Break} inner while-loop.
\EndIf
\EndWhile
\EndWhile
\end{algorithmic}
\textbf{Output.} $\mathcal{B}^{(t + 1)}$ or $\bm{\Phi}$ with columns all bases in $\mathcal{B}^{(t + 1)}$.  
\end{algorithm} 

\begin{proposition} \label{prop:ERT-IV}
\textbf{Expected Running Time of Algorithm~\ref{alg:IV}.} 
Under the setting of $\mathcal{L}$ presented in Example~\ref{exmp:time-consuming-case}, the expected running time of Algorithm~\ref{alg:IV} is of order $O(d^3)$. 
\end{proposition}

The proof of the Proposition~\ref{prop:ERT-IV} is presented later in this Section. As a conclusion, finding all bases takes more than $d^2 / 9$ intersection verifications in expectation. Each intersection verification requires $O(d)$ time. Then the expected running time of computing $\bm{\Phi}$ is $O(d^3)$. 
%see Proposition~\ref{prop:ERT-IV} \footnote{See Appendix~\ref{app:ERT-IV} for detailed analysis.}. \gwcomment{move Proposition 2 to here.} 
In contrast, computing the exact projection of $\bm{v}$ onto $\mathcal{M}$ takes $O(d^2)$ running time\footnote{Projecting onto a 1D linear subspace takes $O(d)$ time, and there are $d$ linear subspaces in total.}. Therefore, the above analysis illustrates that extracting $\bm{\Phi}$ takes way more time than just implementing the projection, which further explains why $\bm{\Phi}$ is not necessary to recover the true PC $\bm{v}_*$. \\

Moreover, under the general setting of $\mathcal{L} = \{L_1, \ldots, L_M\}$, given a set of independent and not necessarily orthonormal vectors $\bm{u}_1^{(m)}, \ldots, \bm{u}_{\text{dim}(L_m)}^{(m)}$ of each linear subspace $L_m$ with $m \in [M]$, it is unclear whether and how long one could find the orthonormal basis $\bm{\Phi}$ from $\mathcal{L}$ via solving the following variant of dictionary learning problem~\eqref{eq:dictionary}, 
\begin{align}
    \begin{array}{rlll}
        \min_{\bm{\Phi}, \bm{R}} & \left\| [\bm{U}^{(1)} ~|~ \cdots ~|~ \bm{U}^{(M)}] - \bm{\Phi} [\bm{R}^{(1)} ~|~ \cdots ~|~ \bm{R}^{(M)}] \right\|_F^2 \\
        \text{s.t.} & \bm{\Phi} \bm{\Phi}^{\top} = \bm{I}_d, ~~ \| \bm{R}^{(m)} \|_0 \leq \text{dim}(L_m) ~~ \forall ~ m \in M
    \end{array}, \label{eq:dictionary}
\end{align}
where, for all $m \in [M]$, $\bm{U}^{(m)}$ denotes the matrix with columns $\bm{u}_1^{(m)}, \ldots, \bm{u}_{\text{dim}(L_m)}^{(m)}$ and $\| \bm{R}^{(m)} \|_0 \leq \text{dim}(L_m)$ denotes that the number of non-zero rows of $\bm{R}^{(m)}$ is at most $\text{dim}(L_m)$.

%\subsubsection{Proof of Proposition~\ref{prop:ERT-IV} for Expected Running Time} \label{app:ERT-IV}





%\begin{proposition} \label{prop:ERT-IV}
%\textbf{Expected Running Time of Algorithm~\ref{alg:IV}.} 
%Under the setting of $\mathcal{L}$ presented in Example~\ref{exmp:time-consuming-case}, the expected running time of Algorithm~\ref{alg:IV} is of order $O(d^3)$. 
%\end{proposition}

\begin{proof}
\textbf{Proof of Proposition~\ref{prop:ERT-IV}.} 
First, based on the setting of each $L_i$ for $i = 1, \ldots, d$, $L_i$ has non-zero intersection with $L_{i - 1}$ and $L_{i + 1}$. Thus the expected number of selections (i.e., inner while-loop~\eqref{alg:IV-inner}) for step~\eqref{alg:IV-selection} of Algorithm~\ref{alg:IV} satisfies 
\begin{align*}
    \mathbb{E}[\textup{number of selections}] = & ~ 1 \cdot \frac{2}{d - 1} + 2 \cdot \frac{d - 3}{d - 1} \frac{2}{d - 2} + 3 \cdot \frac{d - 3}{d - 1} \frac{d - 4}{d - 2} \frac{2}{d - 3} + \cdots \\
    = & ~ \sum_{i = 1}^{d - 2} i \cdot \frac{2 (d - i - 1)}{(d - 1)(d - 2)} \\
    = & ~ \frac{d}{3}. 
\end{align*}
Every time we find $L^{(t)} \cap \tilde{L}^{(t)} \neq \{\bm{0}\}$, in step~\eqref{alg:IV-basis} of Algorithm~\ref{alg:IV}, we can add three more new bases to $\mathcal{B}^{(t + 1)}$ if $\tilde{L}^{(t)} \notin \mathcal{L}^{(t)}$, and one more new base to $\mathcal{B}^{(t + 1)}$ if $\tilde{L}^{(t)} \in \mathcal{L}^{(t)}$. Therefore, the number of outer while-loop~\eqref{alg:IV-outer} of Algorithm~\ref{alg:IV} satisfies 
\begin{align*}
    & ~ \text{number of outer while-loop} = \text{selection with 3 more bases} + \text{selection with 1 more bases}.
\end{align*}
Moreover, the stopping criteria of outer while-loop~\eqref{alg:IV-outer} of Algorithm~\ref{alg:IV} ensures that the number of outer while-loop~\eqref{alg:IV-outer} is greater than or equal to $d/3$, where the equality holds when we can add three more new bases at every inner while-loop~\eqref{alg:IV-inner} of Algorithm~\ref{alg:IV}. Therefore, in expectation, the total number of selections of Algorithm~\ref{alg:IV} satisfies 
\begin{align*}
    \mathbb{E}[\text{total number of selections}] 
    = & ~ \text{number of outer while-loop} \times \mathbb{E}[\textup{number of selections}]\\
    \geq & ~ d^2 / 9.
\end{align*}
Since we do an intersection verification for each selection, and an intersection verification takes $O(d)$ running time, then the expected total running time of computing $\bm{\Phi}$ is $O(d^3)$. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\apcomment{Specify $\mathcal{B}$ and $\mathcal{L}$?} \gwcomment{Done.}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{fundamental-limits-proof}

\input{local-convergence-proof}





\subsection{Proofs of Theorem~\ref{thm:initialization-method}} \label{app:initialization}
We first restate the following two existing results in \cite{deshpande2016sparse} that will be used in Theorem~\ref{thm:initialization-method}. 
\begin{proposition} \label{prop:cov-threshold-2}
\textbf{Restated, [Theorem 2, \cite{deshpande2016sparse}].} There exists a numerical constant $C_3 > 0$ such that the following holds. Assume $n > \max\{ C \log d, k^2 \}$ and $k^2 \leq d / e$. Set $\tau$ according to equation~\eqref{initia-threshold-tau}. Then with probability $1 - o(1)$ 
\begin{align*}
    \|\widehat{\bm{v}}_{\textup{soft}} - \bm{v}_*\|_2^2 \leq \frac{C_3}{\lambda^2} \frac{k^2 \max\{\lambda^2, 1\}}{n} \log (d / k^2) =: 2 \cdot \textup{dis}_{\textup{soft}}(n). 
\end{align*}
\end{proposition}

We note that it follows from \textbf{Remark 6.6} of \cite{deshpande2016sparse} that the probability $1-o(1)$ is of the form $1-C'\exp(-\min\{\sqrt{d},n\}/C')$ for every $n$ large enough. Based on the above proposition, we now present the proof of Theorem~\ref{thm:initialization-method}.


\paragraph{Proof of Theorem~\ref{thm:initialization-method}.}
Consider the following two orthogonal decompositions of $\widehat{\bm{v}}_{\textup{soft}}$ in Algorithm~\ref{alg:initialization}, 
\begin{align*}
    & ~ \widehat{\bm{v}}_{\textup{soft}} = \alpha_{\textup{soft}} \bm{v}_* + \beta_{\textup{soft}} \bm{v}_{\bot} \\
    & ~ \alpha_{\textup{soft}}^2 + \beta_{\textup{soft}}^2 = 1, ~ \langle \bm{v}_*, ~\bm{v}_{\bot} \rangle = 0, ~ \|\bm{v}_*\|_2 = \|\bm{v}_{\bot}\|_2 = 1, \\
    & ~ \widehat{\bm{v}}_{\textup{soft}} = \alpha_{\textup{soft}}' \bm{v}_0 +  \beta_{\textup{soft}}' \bm{v}_{\bot}' \\
    & ~ (\alpha_{\textup{soft}}')^2 + (\beta_{\textup{soft}}')^2 = 1, ~ \langle \bm{v}_0, ~\bm{v}_{\bot}' \rangle = 0, ~ \|\bm{v}_0\|_2 = \|\bm{v}_{\bot}'\|_2 = 1.
\end{align*}
Based on Proposition~\ref{prop:cov-threshold-2}, the first orthogonal decomposition decomposes $\widehat{\bm{v}}_{\textup{soft}}$ along the direction of $\bm{v}_*$ such that $\alpha_{\textup{soft}} \in [1 - \textup{dis}_{\textup{soft}}(n), ~ 1]$. In contrast, the second orthogonal decomposition decomposes $\widehat{\bm{v}}_{\textup{soft}}$ along the direction of $\bm{v}_0$. Since $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{M}$ and $\bm{v}_0 = \Pi_{\mathcal{S}^{d - 1} \cap \mathcal{M}}(\widehat{\bm{v}}_{\textup{soft}}) \in \mathcal{S}^{d - 1} \cap \mathcal{M}$, then we have $\alpha_{\textup{soft}}' \geq \alpha_{\textup{soft}}$. Using $\|\widehat{\bm{v}}_{\textup{soft}}\|_2 = 1$, we obtain that
\begin{align*}
    1 = & ~ \langle \widehat{\bm{v}}_{\textup{soft}}, ~  \widehat{\bm{v}}_{\textup{soft}} \rangle \\
    = & ~ \langle \alpha_{\textup{soft}} \bm{v}_* + \beta_{\textup{soft}} \bm{v}_{\bot}, ~  \alpha_{\textup{soft}}' \bm{v}_0 +  \beta_{\textup{soft}}' \bm{v}_{\bot}' \rangle \\
    = & ~ \alpha_{\textup{soft}} \alpha_{\textup{soft}}' \langle \bm{v}_*, ~ \bm{v}_0 \rangle + \alpha_{\textup{soft}} \beta_{\textup{soft}}' \langle \bm{v}_*, ~ \bm{v}_{\bot}' \rangle + \beta_{\textup{soft}} \alpha_{\textup{soft}}' \langle \bm{v}_{\bot}, ~ \bm{v}_{0}' \rangle + \beta_{\textup{soft}} \beta_{\textup{soft}}' \langle \bm{v}_{\bot}, ~ \bm{v}_{\bot}' \rangle. 
\end{align*}
Continuing, we obtain that
\begin{align*}
    \langle \bm{v}_0, ~ \bm{v}_* \rangle 
    = & ~ \frac{1}{\alpha_{\textup{soft}} \alpha_{\textup{soft}}'} \big( 1 - \alpha_{\textup{soft}} \beta_{\textup{soft}}' \langle \bm{v}_*, ~ \bm{v}_{\bot}' \rangle \\
    & ~ - \beta_{\textup{soft}} \alpha_{\textup{soft}}' \langle \bm{v}_{\bot}, ~ \bm{v}_{0}' \rangle - \beta_{\textup{soft}} \beta_{\textup{soft}}' \langle \bm{v}_{\bot}, ~ \bm{v}_{\bot}' \rangle \big) \\
    = & ~ \frac{1}{\alpha_{\textup{soft}} \alpha_{\textup{soft}}'} - \frac{\beta_{\textup{soft}}'}{\alpha_{\textup{soft}}'} \langle \bm{v}_*, ~ \bm{v}_{\bot}' \rangle - \frac{\beta_{\textup{soft}}}{\alpha_{\textup{soft}}} \langle \bm{v}_{\bot}, ~ \bm{v}_{0}' \rangle \\
    & ~  - \frac{\beta_{\textup{soft}} \beta_{\textup{soft}}'}{\alpha_{\textup{soft}} \alpha_{\textup{soft}}'} \langle \bm{v}_{\bot}, ~ \bm{v}_{\bot}' \rangle \\
    \geq & ~ 1 - \frac{\beta_{\textup{soft}}'}{\alpha_{\textup{soft}}'} - \frac{\beta_{\textup{soft}}}{\alpha_{\textup{soft}}} - \frac{\beta_{\textup{soft}} \beta_{\textup{soft}}'}{\alpha_{\textup{soft}} \alpha_{\textup{soft}}'}. 
\end{align*}
It is sufficient to have
\begin{align*}
    1 - \frac{\beta_{\textup{soft}}'}{\alpha_{\textup{soft}}'} - \frac{\beta_{\textup{soft}}}{\alpha_{\textup{soft}}} - \frac{\beta_{\textup{soft}} \beta_{\textup{soft}}'}{\alpha_{\textup{soft}} \alpha_{\textup{soft}}'} \geq & ~ 1 - 3 \frac{\beta_{\textup{soft}}}{\alpha_{\textup{soft}}} \\
    \geq & ~ 1 - 3 \frac{\sqrt{2 \textup{dis}_{\textup{soft}}(n) - \textup{dis}_{\textup{soft}}^2(n)}}{1 - \textup{dis}_{\textup{soft}}(n)} \\
    \geq & ~ c_{0}.
\end{align*}
Therefore, by setting 
\begin{align*}
    n \geq \frac{18 C_4 \max\{\lambda^2, ~ 1\} k^2}{2(1 - c_0)^2 \lambda^2} \log(d/k^2),  
\end{align*} 
%{\color{red}This step need to be verified carefullly.}
and applying Proposition~\ref{prop:cov-threshold-2}, the above inequality holds, thus the desired result follows immmediately.

\input{corollaries-proof}

