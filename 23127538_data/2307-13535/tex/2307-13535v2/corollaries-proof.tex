\subsection{Initialization Algorithms Proposed in Section~\ref{sec:specific-examples}} \label{app:initial-examples}

\begin{algorithm}
\caption{Exact Projection Oracle -- Path Sparse PCA}
\label{alg:projection-PSPCA}
\textbf{Input:} A $(d,k)$-layered graph $G$, a vector $\bm{v} \in \mathbb{R}^d$.
\begin{algorithmic}[1]
\For{$\ell = 1, \ldots, k$}
\State Pick $S_{\ell}$ the index set of the $\ell$-th layer in $G$. 
\State Compute path sparsity vector $\bm{v}^{\tt PS}$ as follows: for its sub-vector $\bm{v}^{\tt PS}_{S_{\ell}}$, set
	\begin{align*}
		~ [\bm{v}^{\tt PS}_{S_{\ell}}]_i := \left\{
		\begin{array}{lll}
			~ [\bm{v}_{S_{\ell}}]_i & \text{ if component $i$ has} \\
			& \text{ the largest absolute value,} \\
			& \text{ breaking ties lexicographically} \\
			~ 0 & \text{ otherwise}
		\end{array}
		\right. .
	\end{align*} 
\EndFor 
\State Normalize $\bm{v}^{\tt PS} := \bm{v}^{\tt PS} / \|\bm{v}^{\tt PS}\|_2$.  
\end{algorithmic}
\textbf{Output:} $\bm{v}^{\tt PS}$.  
\end{algorithm} 


%\subsection{Omitted Definitions/Discussions in Section~\ref{sec:specific-examples}} \label{app:definition-examples}
%
%\subsubsection{For Section~\ref{sec:TS-PCA}}
%
%
%\subsubsection{For Section~\ref{sec:PS-PCA}}




\subsection{Proof of Corollaries in Section~\ref{sec:specific-examples}}
We first state an auxilary Lemma that will be used later. 
\begin{lemma}\label{lemma-rho-probability-bound}
 Let $\bm{W}$ be the noise matrix defined in equation~\eqref{eq:noise-def} and $F$ be a fixed linear subspace of dimension $k$. There exists a universal constant $C>0$ such that 
 \begin{align*}
 \Pr\bigg\{ \rho\big(\bm{W},F\big) \geq C(\lambda+1) \sqrt{\frac{k+u}{n}} \bigg\} \leq 2\exp(-u).
 \end{align*} 
\end{lemma}
The proof of Lemma~\ref{lemma-rho-probability-bound} can be found at the end of this section.


\subsubsection{Proof of Corollary~\ref{coro:PS-PCA-fund-limits}} \label{proof-coro-PS-PCA-fund-limits}
\paragraph{Proof of part (a)}
Let $\widehat{P},P_{*}\in \mathcal{P}^k$ be the index sets such that $\mathsf{supp}(\widehat{\bm{v}}_{\mathsf{PS}}) \subseteq \widehat{P}$ and $\mathsf{supp}(\bm{v}_*) \subseteq P_*$. Moreover, let 
\begin{align*}
	& ~ \widehat{L} = \mathsf{span}\big(\{\bm{e}_i\}_{i \in \widehat{P}} \big),\; L_* = \mathsf{span}(\{\bm{e}_i\}_{i \in P_*}), \\
    & ~ \widehat{F} = \mathsf{conv}\big(\widehat{L} \cup L_*\big), \\
    & ~ \mathcal{L} = \bigg\{ \mathsf{span}\big(\{\bm{e}_i\}_{i \in P_1 \cup P_2}\big) \;|\; P_1 \neq P_2 \in \mathcal{P}^{k} \bigg\},
\end{align*}
where $\bm{e}_i$ is the standard base vector with the $i$-th entry being $1$. It follows from part (a) of Theorem~\ref{thm:fund-limits} that
\[
    \big\|\widehat{\bm{v}}_{\mathsf{PS}} - \bm{v}_* \big\|_2 \leq \frac{2\sqrt{2}}{\lambda} \cdot \rho\big(\bm{W}, \widehat{F} \big) \leq \frac{2\sqrt{2}}{\lambda} \cdot \max_{F \in \mathcal{L}}\rho\big(\bm{W}, F \big).
\]
Applying Lemma~\ref{lemma-rho-probability-bound} and union bound, we obtain that there exists a universal constant $C$ for which 
\begin{align*}
    & ~ \Pr\bigg\{\max_{F \in \mathcal{L}}\rho\big(\bm{W}, F \big) \geq C(\lambda+1) \sqrt{\frac{2k+u}{n}} \bigg\}\leq \big|\mathcal{L}\big| \cdot  2\exp(-u) \leq  (d/k)^{2k} \cdot 2\exp(-u),
\end{align*}
where in the last step we use $\big|\mathcal{L}\big| \leq \big|\mathcal{P}^{k} \big|^{2} \leq (d/k)^{2k}$. By setting $u = 2k \cdot \ln(d/k) - \ln(\tau/2)$ and putting the pieces together, we obtain that with probability at least $1-\tau$,
\begin{align*}
    \big\|\widehat{\bm{v}}_{\mathsf{PS}} - \bm{v}_* \big\|_2 &\lesssim \frac{1+\lambda}{\lambda} \cdot \sqrt{\frac{(2+2\ln(d/k))k - \ln(\tau/2)}{n}} \\ &\leq 
    \frac{1+\lambda}{\lambda} \cdot \sqrt{\frac{3(\ln d - \ln k)k - \ln(\tau/2)}{n}}.
\end{align*}
Choosing $\tau = 2\exp(-ck)$ yields the desired result.

\paragraph{Proof of part (b)}
Denote $\mathcal{P}^{k} = \{P_1,...,P_M\}$, where $M = \big|\mathcal{P}^{k} \big|$. For each $m\in [M]$, let $\bm{z}_m \in \mathbb{R}^{d}$ such that
\[
    \bm{z}_{m}(i) = 1 \text{ if }i\in P_m \quad \text{and} \quad \bm{z}_{m}(i) = 0 \text{ if }i\notin P_m.
\]
Note that $1 \in P_m$ for each $m\in M$ since $1$ is index corresponding to the sourse vertex $v_s$. It follows from the definition of $i_*$ and $\mathcal{Z}_*$ in equations~\eqref{definition-i-star}~\eqref{definition-Z-star} that 
\[
i_* = 1 \quad \text{and} \quad \mathcal{Z}_{*} = \{\bm{z}_m\}_{m=1}^{M}.
\]
We next verify Assumption~\ref{assump:minimax-assumption}. Note that $M = (d-2)^{k}/k^{k}$ and it follows from [Lemma 7.4, \cite{asteris2015stay}] that
\begin{align*}
    \big|\mathcal{N}_H(\bm{z}; k/2)\big| \leq \binom{k}{3k/4} d^{k/4}  \leq 2^{2 k} d^{k/4}.
\end{align*}
Consequently, we obtain that
\[
    \frac{|\mathcal{Z}_*|}{\big|\mathcal{N}_H(\bm{z}; k/2)\big|}\geq \frac{(d-2)^k}{k^{k}4^kd^{k/4}} = \frac{d^{3k/4}}{(4k)^{k}} \cdot (1-2/d)^{k} \geq 2^{4},
\]
where the last step follows from the assumption that $d \geq 16k^{2}$ and $k\geq 4$. So we conclude that Assumption~\ref{assump:minimax-assumption} holds by setting $\xi = 3/4$. Continuing, we obtain that
\begin{align*}
    \log \bigg( \frac{|\mathcal{Z}_*|}{ \big|\mathcal{N}_H(\bm{z}; k/2)\big|} \bigg) \geq & ~ \log \bigg( \frac{(d-2)^k}{k^{k}4^kd^{k/4}} \bigg) \\
    \gtrsim & ~ k \big( \log(d)/2 - \log(4k) \big).
\end{align*}
Now, applying part (b) of Theorem~\ref{thm:fund-limits} (by setting $\xi = 3/4$) yiels the desired result. 


\subsubsection{Proof of Corollary~\ref{coro:PS-PCA-PPM}}\label{proof-coro-PS-PCA-PPM}
Denote $\mathcal{P}^{k} = \{P_1,...,P_M\}$, where $M = |\mathcal{P}^{k}|$. For each $m \in M$, let $L_m = \mathsf{span}\big(\{\bm{e}_i\}_{i\in P_m}\big)$. Let
\begin{align*}
	& ~ F^{*} = \argmax_{F} \rho(\bm{W},F) \\
	\text{s.t.} ~ & ~ F = \mathsf{conv}(L_{m_1} \cup L_{m_2} \cup L_{m_3}),\;\forall\;m_1 \neq m_2 \neq m_3 \in [M].
\end{align*}
Note that the number of subspaces $F$ such that $F = \mathsf{conv}(L_{m_1} \cup L_{m_2} \cup L_{m_3})$ where $m_1 \neq m_2 \neq m_3 \in [M]$ is bounded by $\big|\mathcal{P}^{k}\big|^{3}$. Applying Lemma~\ref{lemma-rho-probability-bound} and using union bound yield
\begin{align*}
    & ~ \Pr\bigg\{ \rho\big(\bm{W},F^{*}\big) \geq C(\lambda+1) \sqrt{\frac{3k+u}{n}} \bigg\} \leq \big|\mathcal{P}^{k}\big|^{3} \cdot 2\exp(-u) \leq \frac{d^{3k}}{k^{3k}}\cdot 2\exp(-u),
\end{align*}
where the last step follows since $\big|\mathcal{P}^{k}\big| \leq \frac{d^{k}}{k^k}$. By setting $u = (1+\ln2 + 3\ln(d/k))k$, we obtain that with probability at least $1-\exp(-k)$,
\begin{align}\label{bound-error-ps-pca}
    \rho\big(\bm{W},F^{*}\big) \leq & ~ C' (\lambda + 1)\sqrt{ \frac{k(1+\ln d - \ln k)}{n} } \\
    \lesssim & ~  (\lambda + 1)\sqrt{ \frac{k(2\ln d - \ln k)}{n} }. \notag
\end{align}
Letting $n \geq C_{2}k\ln d$ for a large enough constant $C_{2}$, we ontain that
\begin{align*}
    \frac{4}{\lambda + 1 -  \rho\big(\bm{W},F^{*}\big)} + \frac{5 \rho\big(\bm{W},F^{*}\big)}{\lambda - 2 \rho\big(\bm{W},F^{*}\big)}
    \overset{(1)}{\leq} & ~  \frac{8}{\lambda} + \frac{10\rho\big(\bm{W},F^{*}\big)}{\lambda} \\
    \leq & ~ \frac{8}{\lambda} + 20C' \sqrt{\frac{k\ln d}{n}} \leq 0.5,
\end{align*} 
where in step $(1)$ we use $\rho\big(\bm{W},F^{*}\big) \leq \lambda/4$ and in last step we let $\lambda$ be large enough. Consequently, for $\bm{v}_0 \in \mathcal{P}^{k} \cap \mathcal{S}^{d-1}$ and $\langle \bm{v}_0, \bm{v}_*\rangle \geq 0.5$, we obtain that $\bm{v}_0  \in \mathbb{G}(\lambda)$. Similarly, condition~\eqref{eigen-gap-condition} can be verified. Applying Theorem~\ref{thm:convergence} yield
\begin{align*}
	\|\bm{v}_{t+1} - \bm{v}_*\|_2 \leq & ~ \frac{1}{2^{t}} \cdot \|\bm{v}_{0} - \bm{v}_*\|_2 + \frac{6\rho\big(\bm{W},F^{*}\big)}{ \lambda - 2\rho\big(\bm{W},F^{*}\big)} \\
    \leq & ~ \frac{1}{2^{t}} \cdot \|\bm{v}_{0} - \bm{v}_*\|_2 + C_3 \sqrt{ \frac{k(2\ln d - \ln k)}{n} }, 
\end{align*}
where the last inequality holds with probability at least $1-\exp(-k)$ by inequaity~\eqref{bound-error-ps-pca}.














\subsubsection{Proof of Corollary~\ref{coro:TS-PCA-fund-limits}} \label{proof-coro-TS-PCA-fund-limits}
\paragraph{Proof of part (a)}
Let $\widehat{T},T_{*}\in \mathcal{T}^k$ be the index sets such that $\mathsf{supp}(\widehat{\bm{v}}_{\mathsf{TS}}) \subseteq \widehat{T}$ and $\mathsf{supp}(\bm{v}_*) \subseteq T_*$. Moreover, let 
\begin{align*}
	& ~ \widehat{L} = \mathsf{span}\big(\{\bm{e}_i\}_{i \in \widehat{T}} \big),\; L_* = \mathsf{span}(\{\bm{e}_i\}_{i \in T_*}), \\
	& ~ \widehat{F} = \mathsf{conv}\big(\widehat{L} \cup L_*\big)\;\text{and}\; \mathcal{L}^{2k} = \bigg\{ \mathsf{span}\big(\{\bm{e}_i\}_{i \in T}\big) \;|\; T \in \mathcal{T}^{2k} \bigg\},
\end{align*}
where $\bm{e}_i$ is the standard base vector with the $i$-th entry being $1$. It follows from part (a) of Theorem~\ref{thm:fund-limits} that
\[
    \big\|\widehat{\bm{v}}_{\mathsf{TS}} - \bm{v}_* \big\|_2 \leq \frac{2\sqrt{2}}{\lambda} \cdot \rho\big(\bm{W}, \widehat{F} \big) \leq \frac{2\sqrt{2}}{\lambda} \cdot \max_{F \in \mathcal{L}^{2k}}\rho\big(\bm{W}, F \big).
\]
Applying Lemma~\ref{lemma-rho-probability-bound} and union bound, we obtain that there exists a universal constant $C$ for which 
\begin{align*}
    & ~ \Pr\bigg\{\max_{F \in \mathcal{L}^{2k}}\rho\big(\bm{W}, F \big) \geq C(\lambda+1) \sqrt{\frac{k+u}{n}} \bigg\} \leq \big|\mathcal{L}^{2k}\big| \cdot  2\exp(-u) \leq  \frac{(2e)^{2k}}{2k + 1} \cdot 2\exp(-u),
\end{align*}
where in the last step we use $\big|\mathcal{L}^{2k}\big| = \big|\mathcal{T}^{2k} \big| \leq \frac{(2e)^{2k}}{2k + 1}$. By setting $u = 2(1+\ln2)k - \ln(\tau/2)$ and putting the pieces together, we obtain that with probability at least $1-\tau$,
\[
    \big\|\widehat{\bm{v}}_{\mathsf{TS}} - \bm{v}_* \big\|_2 \lesssim \frac{1+\lambda}{\lambda} \cdot \sqrt{\frac{(3+2\ln2)k - \ln(\tau/2)}{n}}.
\]
Choosing $\tau = 2\exp(-ck)$ yields the desired result.
\paragraph{Proof of part (b)}
Denote $\mathcal{T}^{k} = \{T_1,...,T_M\}$, where $M = \big|\mathcal{T}^{k} \big|$. For each $m\in [M]$, let $\bm{z}_m \in \mathbb{R}^{d}$ such that
\[
    \bm{z}_{m}(i) = 1 \text{ if }i\in T_m \quad \text{and} \quad \bm{z}_{m}(i) = 0 \text{ if }i\notin T_m.
\]
Note that $1 \in T_m$ for each $m\in M$. It follows from the definition of $i_*$ and $\mathcal{Z}_*$ in equations~\eqref{definition-i-star}~\eqref{definition-Z-star} that 
\[
i_* = 1 \quad \text{and} \quad \mathcal{Z}_{*} = \{\bm{z}_m\}_{m=1}^{M}.
\]
Let us first lower bound the size of $\mathcal{Z}_{*}$. Note that, for a fixed $k$, when $\mathsf{CBT}$ is of height greater than or equal to $k$, any nodes with height $\geq k$ will never be selected. That is to say, it is sufficient to consider $d \leq 2^{k} - 1$. Based on the proof of [Proposition 1, \cite{baraniuk2010model}], when $k \geq \log_2 d$, the total number of subtrees with size $k$ in $\mathsf{CBT}$ can be represented as
\begin{align*}
	M = |\mathcal{T}^k| = & ~ \sum_{h = \lfloor \log_2 k \rfloor + 1}^{\log_2 d} t_{k, h}. 
\end{align*}
Here $t_{k, h}$ denotes the number of binary subtrees with size $k$ and height $h$, which can be lower bounded by 
\begin{align*}
	& ~ t_{k, h} \\
	\geq & ~ \frac{4^{k + 1.5}}{h^4} \sum_{m \geq 1} \left[ \frac{2k}{h^2} (2\pi m)^4 - 3 (2 \pi m)^2 \right] \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right) \\
	\geq & ~ \frac{4^{k + 1.5}}{h^4} \sum_{m \geq h / (\sqrt{2 \pi^2 k})} \left[ \frac{2k}{h^2} (2\pi m)^4 - 3 (2 \pi m)^2 \right] \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right) \\
	\geq & ~ \frac{4^{k + 1.5}}{h^4} \sum_{m \geq h / (\sqrt{2 \pi^2 k})} (2 \pi m)^2 \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right) \\
	\geq & ~ \frac{4^{k + 1.5}}{h^4} \int_{m \geq h / (\sqrt{2 \pi^2 k})} (2 \pi m)^2 \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right) \mathrm{d}m
%	= & ~ \frac{4^{k + 1.5}}{h^4} \underbrace{ \sum_{m \geq 1} \frac{2k}{h^2} (2\pi m)^4 \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right) }_{=: T_h^1} -  \frac{4^{k + 1.5}}{h^4} \underbrace{ \sum_{m \geq 1} 3 (2 \pi m)^2 \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right)}_{=: T_h^2}. 
\end{align*}
where the third inequality holds since $\frac{2k}{h^2} (2 \pi m)^4  \geq 4 (2 \pi m)^2$ if $m \geq \frac{h}{\sqrt{2 \pi^2 k}}$, and the forth inequality (the infinity summation is greater than or equal to the integration) holds since the function $f(m) := (2 \pi m)^2 \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right)$ becomes monotone decreasing when $m \geq \frac{h}{\sqrt{4 \pi^2 k}}$. Therefore, by integration, we have 
\begin{align*}
	t_{k, h} 
	\geq & ~ \frac{4^{k + 1.5}}{h^4} \left[ \frac{\sqrt{\pi} h^3}{4 k^{3/2}}\text{erfc}\left( \frac{1}{\sqrt{2 \pi^2}} \right) + \frac{h^3}{\sqrt{8 \pi^2} k^{3/2}} \exp\left( - \frac{1}{2 \pi^2} \right) \right] \\
%	= & ~ 4^{k + 1.5} \left[ \frac{\sqrt{\pi}}{4 k^{3/2} h}\text{erfc}\left( \frac{1}{\sqrt{2 \pi^2}} \right) + \frac{1}{\sqrt{8 \pi^2} k^{3/2} h} \exp\left( - \frac{1}{2 \pi^2} \right) \right] \\
	= & ~ \frac{4^{k + 1.5}}{k^{3/2}} \cdot \underbrace{ \left[ \frac{\sqrt{\pi}}{4}\text{erfc}\left( \frac{1}{\sqrt{2 \pi^2}} \right) + \frac{1}{\sqrt{8 \pi^2}} \exp\left( - \frac{1}{2 \pi^2} \right) \right]}_{=: c_1 > 0} \cdot \frac{1}{h},
\end{align*}
where $\text{erfc}(x) := 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} \exp(- t^2) \mathrm{d}t \in (0, 1)$ denotes the complementary error function. Inserting the above result into the formulation for total number of subtrees gives
\begin{align*}
	M = \sum_{h = \lfloor \log_2 k \rfloor + 1}^{\log_2 d} t_{k, h} \geq & ~ \frac{4^{k + 1.5}}{k^{3/2}} \cdot \sum_{h = \lfloor \log_2 k \rfloor + 1}^{\log_2 d} \frac{c_1}{h} \\
	\geq & ~ \frac{4^{k + 1.5}}{k^{3/2}} \cdot c_1 \cdot \ln \left( \frac{\log_2 d}{\lfloor \log_2 k \rfloor + 1} \right). 
\end{align*} 


\iffalse
We bound $T_h^1$ and $T_h^2$ separately as follows. 
\begin{align*}
	T_h^1 \geq & ~ \int_{m = 1}^{\infty} \frac{2k}{h^2} (2\pi m)^4 \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right) \mathrm{d}m - \frac{8 h^2}{e^2 k} \\
	= & ~ \frac{3 h^3}{8 \sqrt{\pi} k^{3/2}} \text{erfc}(2 \pi \sqrt{k} / h) + \frac{(8 \pi^2 k + 3 h^2)}{2 k} \exp(- 4 \pi^2 k / h^2) - \frac{8 h^2}{e^2 k} \\
	T_h^2 \leq & ~ \frac{2 h^2}{e k} + \int_{m = 1}^{\infty} (2 \pi m)^2 \exp\left( - \frac{k(2 \pi m)^2}{h^2}\right) \mathrm{d}m \\
	= & ~ \frac{2 h^2}{e k} + \frac{h^3}{8 \sqrt{\pi} k^{3/2}} \text{erfc}(2 \pi \sqrt{k} / h) + \frac{h^2}{2 k} \exp(- 4\pi^2k / h^2),
\end{align*}
where $\text{erfc}(x) := 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} \exp(- t^2) \mathrm{d}t \in (0, 1)$ denotes the complementary error function. Combine the above two inequalities gives
\begin{align*}
	t_{k, h} \geq & ~ \frac{4^{k + 1.5}}{h^4} \left[ \frac{h^3}{4 \sqrt{\pi} k^{3/2}} \text{erfc}(2 \pi \sqrt{k} / h) + \frac{(4 \pi^2 k + 2 h^2)}{k} \exp(- 4 \pi^2 k / h^2) - \frac{8 + 2e}{e^2} \frac{h^2}{k} \right] \\
	\geq & ~ 4^{k + 1.5} \left[ \frac{\text{erfc}(2 \pi \sqrt{k} / h)}{4 \sqrt{\pi} k^{3/2} h} + \frac{4 \pi^2}{h^4} \exp(- 4 \pi^2 k / h^2) + \frac{2}{k h^2} \exp(- 4 \pi^2 k / h^2) - \frac{8 + 2e}{e^2 k h^2} \right]
\end{align*}
\begin{align*}
	M = |\mathcal{T}^k| = \sum_{h = \lfloor \log_2 k \rfloor + 1}^{\log_2 d} t_{k, h} \geq c_1 \frac{4^k}{k \log_2 k},  
\end{align*}
where $c_1 \in (0,1)$ is some fixed constant. 
\fi

By picking $i_* = 1$, since all the subtree are rooted at the root node $r_{\mathsf{CBT}}$ with index $1$, then we have $M = |\mathcal{Z}_{*}|$. Given any $\xi \in (3/4, 1)$ and any characteristic vector $\bm{z} \in \mathcal{Z}$, the size of neighborhood $\mathcal{N}_H(\bm{z}; 2(1 - \xi)k)$ can be upper bounded by
\begin{align*}
	|\mathcal{N}_H(\bm{z}; 2(1 - \xi)k)| = & ~ \sum_{i = 0}^{(1 - \xi)k} |\{\bm{z}' ~|~ \delta_H(\bm{z}, \bm{z}') = i\}| \\
	\leq & ~ \sum_{i = 0}^{(1 - \xi)k} \binom{k}{i} k^i \leq 2^{2 (1 - \xi) k \cdot \log_2 k},
\end{align*}
where the first inequality holds since $|\{\bm{z}' ~|~ \delta_H(\bm{z}, \bm{z}') = i\}|$ can be upper bounded by the product between the upper bound of all sub-subtrees in $T(\bm{z})$ with size $k - i$ and root node $r_{\mathsf{CBT}}$ (i.e., $\binom{k}{i}$) and the upper bound of all possible choices for expanding a subtree with size $k - i$ to size $k$ (i.e., $k^i$). When $2(1 - \xi) k \leq k / \log_2 k$, we have 
\begin{align*}
	|\mathcal{N}_H(\bm{z}; 2(1 - \xi)k)| \leq 2^{2 \log_2 k (1 - \xi) k} \leq 2^{k}
\end{align*}
and therefore, the Assumption~\ref{assump:minimax-assumption}.2 
\begin{align*}
	\frac{|\mathcal{Z}_{*}|}{|\mathcal{N}_H(\bm{z}; 2(1 - \xi)k)|} \geq \frac{c_1 4^{k + 1.5}}{k^{3/2} \cdot 2^{k}} \cdot \ln \left( \frac{\log_2 d}{\lfloor \log_2 k \rfloor + 1} \right) \geq 16
\end{align*} 
holds with $k$ sufficiently greater than some constant. To achieve the above result, by setting $\xi = 1 - \frac{1}{2 \log_2 k}$, using results in part (b) of Theorem~\ref{thm:fund-limits}, we have
\begin{align*}
    & ~ \inf_{\widehat{\bm{v}}} \; \sup_{\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{M}} \mathbb{E} \left[ \left\| \widehat{\bm{v}} \widehat{\bm{v}}^{\top} - \bm{v}_* \bm{v}_*^{\top}  \right\|_F \right]  \notag\\
    \geq & ~ \frac{\sqrt{2(1 - \xi)}}{4} \min\Bigg\{1, ~ \sqrt{\frac{1 + \lambda}{8 \lambda^2}} \sqrt{\frac{ \log \left(|\mathcal{Z}_{*}|\right) - \log \big( \max_{\bm{z} \in \mathcal{Z}_{*}}|\mathcal{N}_H(\bm{z}; 2(1 - \xi) k)| \big)}{n}} \Bigg\} \\
    = & ~ \frac{1}{4\sqrt{\log k}} \min \Bigg\{1,  ~ \sqrt{\frac{1 + \lambda}{8 \lambda^2}} \sqrt{\frac{k + 3 + \log c_1 - \frac{3}{2} \log k - \log \ln \left( \frac{\log_2 d}{\lfloor \log_2 k \rfloor + 1} \right) }{n}} \Bigg\} 
\end{align*}
As mentioned before, it is sufficient to consider $d \leq 2^{k} - 1$, then the term $\ln \left( \frac{\log_2 d}{\lfloor \log_2 k \rfloor + 1} \right) \leq \ln k$, and thus
\begin{align*}
	\inf_{\widehat{\bm{v}}} \; \sup_{\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{M}} \mathbb{E} \left[ \left\| \widehat{\bm{v}} \widehat{\bm{v}}^{\top} - \bm{v}_* \bm{v}_*^{\top}  \right\|_F \right]
	\gtrsim & ~ \min \Bigg\{ \frac{1}{4 \sqrt{\log k}}, ~~ \frac{1}{4}\sqrt{\frac{1 + \lambda}{8 \lambda^2}} \sqrt{\frac{k/\log k}{n}} \Bigg\}. 
\end{align*}

\iffalse
Based on the proof of [Propsotion1, \cite{baraniuk2010model}], when $k < \log_2 d$, i.e., the height of $\mathsf{CBT}$ is at least $k + 1$, then the total number of subtrees in $\mathsf{CBT}$ is the Catalan number $M = |\mathcal{T}^k| = \frac{1}{k + 1} \binom{2k}{k}$. Based on \citet{macwilliams1977theory}, one can lower bound the above number of subtrees by 
\begin{align*}
	M \geq \frac{1}{k + 1} \cdot \sqrt{\frac{2k}{8k(2k - k)}} 2^{2k H(1/2)} = \frac{2^{2k}}{2 \sqrt{k} (k + 1)}.   
\end{align*}
Since every subtree in $\mathsf{CBT}$ contains 
Note that any nodes on layer greater than $k + 1$ will never be selected by a subtree with root node $r_{\mathsf{CBT}}$, then the size of $\mathcal{Z}_{*}$ is at most 
\begin{align*}
	|\mathcal{Z}_{*}| \geq \frac{2^{2k}}{2 \sqrt{k} (k + 1) 2^{k + 1}} 
\end{align*}
%Based on the proof of [Propsotion1, \cite{baraniuk2010model}], there exists a universal constant $c_1 > 0$ such that $\log M = \log |\mathcal{T}^k| \geq c_1 k \log (2e)$. 
Now, for any $\xi \in (3/4, 1)$ and any $\bm{z} \in \mathcal{Z}$, the size of neighborhood $\mathcal{N}_H(\bm{z}; 2(1 - \xi)k)$ is upper bounded by 
\begin{align*}
    |\mathcal{N}_H(\bm{z}; 2(1 - \xi)k)| \leq \sum_{i = 0}^{2(1 - \xi)k} \binom{k}{i} \binom{d - k}{i} \leq k (kd)^{2(1 - \xi)k}.  
\end{align*}
Thus for any positive constant $c_2 > 1$, by setting $\xi = 1 - \frac{1}{2} \frac{c_1}{c_2} \frac{\log (2e)}{\log (kd)} > 3 / 4$, we have
\begin{align*}
    \log\left( \frac{|\mathcal{T}^k|}{d \cdot \max_{\bm{z} \in \mathcal{Z}}|\mathcal{N}_H(\bm{z}; 2(1 - \xi)k)|} \right) \geq & ~ c_1 k \log (2e) - \log d - 2(1 - \xi)k \log(kd) \\
    \gtrsim & ~ \frac{c_2 - 1}{c_2} k \log (2e).
\end{align*}
\fi 
% {\color{red}Here, we need to verify Assumption~\ref{assump:minimax-assumption}.} \gwcomment{Done.} 


\subsubsection{Proof of Corollary~\ref{coro:TS-PCA-PPM}}\label{proof-coro-TS-PCA-PPM}
Denote $\mathcal{T}^{k} = \{T_1,...,T_M\}$, where $M = |\mathcal{T}^{k}|$. For each $m \in M$, let $L_m = \mathsf{span}\big(\{\bm{e}_i\}_{i\in T_m}\big)$. Let
\begin{align*}
	& ~ F^{*} = \argmax_{F} \rho(\bm{W},F) \\
	\text{s.t.} & ~ F = \mathsf{conv}(L_{m_1} \cup L_{m_2} \cup L_{m_3}),\;\forall\;m_1 \neq m_2 \neq m_3 \in [M]. 
\end{align*}
Note that the number of subspaces $F$ such that $F = \mathsf{conv}(L_{m_1} \cup L_{m_2} \cup L_{m_3})$ where $m_1 \neq m_2 \neq m_3 \in [M]$ is bounded by $\big|\mathcal{T}^{3k}\big|$. Applying Lemma~\ref{lemma-rho-probability-bound} and using union bound yield
\begin{align*}
    \Pr\bigg\{ \rho\big(\bm{W},F^{*}\big) \geq C(\lambda+1) \sqrt{\frac{3k+u}{n}} \bigg\}
    \leq & ~ \big|\mathcal{T}^{3k}\big| \cdot 2\exp(-u) \leq \frac{(2e)^{3k}}{3k + 1}\cdot 2\exp(-u),
\end{align*}
where the last step follows since $\big|\mathcal{T}^{3k}\big| \leq \frac{(2e)^{3k}}{3k + 1}$. By setting $u = 4(1+\ln2)k$, we obtain that with probability at least $1-\exp(-k)$,
\begin{align}\label{bound-error-ts-pca}
    \rho\big(\bm{W},F^{*}\big) \leq C' (\lambda + 1)\sqrt{ \frac{k}{n} } \leq 2C' \lambda \sqrt{\frac{k}{n}},
\end{align}
where the last step follows for $\lambda \geq 1$. For $n \geq (50C')^2k$, we ontain that
\begin{align*}
    \frac{4}{\lambda + 1 -  \rho\big(\bm{W},F^{*}\big)} + \frac{5 \rho\big(\bm{W},F^{*}\big)}{\lambda - 2 \rho\big(\bm{W},F^{*}\big)} 
    \overset{(1)}{\leq} & ~ \frac{8}{\lambda} + \frac{10\rho\big(\bm{W},F^{*}\big)}{\lambda} \leq \frac{8}{\lambda} + 20C' \sqrt{\frac{k}{n}} \leq 0.5,
\end{align*} 
where in step $(1)$ we use $\rho\big(\bm{W},F^{*}\big) \leq \lambda/4$ and in last step we let $\lambda$ be large enough. Consequently, for $\bm{v}_0 \in \mathcal{T}^{k} \cap \mathcal{S}^{d-1}$ and $\langle \bm{v}_0, \bm{v}_*\rangle \geq 0.5$, we obtain that $\bm{v}_0  \in \mathbb{G}(\lambda)$. Similarly, condition~\eqref{eigen-gap-condition} can be verified. Applying Theorem~\ref{thm:convergence} yield
\begin{align*}
	\|\bm{v}_{t+1} - \bm{v}_*\|_2 \leq & ~ \frac{1}{2^{t}} \cdot \|\bm{v}_{0} - \bm{v}_*\|_2 + \frac{6\rho\big(\bm{W},F^{*}\big)}{ \lambda - 2\rho\big(\bm{W},F^{*}\big)} \\
	\leq & ~ \frac{1}{2^{t}} \cdot \|\bm{v}_{0} - \bm{v}_*\|_2 + C_3 \sqrt{\frac{k}{n}},
\end{align*}
where the last inequality holds with probability at least $1-\exp(-k)$ by inequaity~\eqref{bound-error-ts-pca}.







\subsubsection{Proof of Lemma~\ref{lemma-rho-probability-bound}}

\paragraph{Proof of Lemma~\ref{lemma-rho-probability-bound}:} Without loss of generality, let $\{\phi_1,...,\phi_k\}$ be an orthonormal basis of $F$. Let $\Phi = [\phi_1\;|\cdots|\phi_k] \in \mathbb{R}^{d \times k}$. By definition, we obtain that
\begin{align*}
    \rho\big(\bm{W},F\big) = & ~ \max_{\bm{v}\in \mathcal{S}^{d-1} \cap F} \bm{v}^{\top} \bm{W} \bm{v} \\
    = & ~ \max_{\bm{u} \in \mathcal{S}^{k-1}} \bm{u}^{\top} \Phi^{\top} \bm{W} \Phi \bm{u} = \big\|\Phi^{\top} \bm{W} \Phi\big\|.
\end{align*}
Continuing, we obtain that
\begin{align*}
\big\|\Phi^{\top} \bm{W} \Phi\big\| = \bigg\| \frac{1}{n}\sum_{i=1}^{n} (\Phi^{\top}\bm{x}_{i}) (\Phi^{\top}\bm{x}_{i})^{\top} - \Phi^{\top} \bm{\Sigma} \Phi \bigg\|.
\end{align*}
Note that $\Phi^{\top}\bm{x}_{i} \sim \mathcal{N}(\bm{0},\Phi^{\top} \bm{\Sigma} \Phi)$. Applying the result in [Exercise 4.7.3 (Tail bound), \cite{vershynin2018high}] yields that
\begin{align*}
    \Pr\bigg\{ \big\|\Phi^{\top} \bm{W} \Phi\big\| \leq C \|\Phi^{\top} \bm{\Sigma} \Phi\| \cdot  \sqrt{\frac{k+u}{n}} \bigg\}  \geq 1 - 2\exp(-u).
\end{align*}
Note that $\|\Phi^{\top} \bm{\Sigma} \Phi\| \leq \lambda+1$. Putting the pieces together yields the desired result.

\subsubsection{Proof of inequality~\eqref{eq:SPCA-known}}\label{min-max-pca-proof}
In the setting of sparse PCA, the number of possible linear subspaces is $M = \binom{d}{k}$. By definitions of $\mathcal{Z}_*$ and $i_*$, we obtain that
\begin{align*}
	|\mathcal{Z}_*| = & ~ \sum_{m=1}^{M} \bm{z}_m(i_*) \geq \frac{\sum_{i=1}^{d} \sum_{m=1}^{M} \bm{z}_m(i) }{d} \\
	= & ~ \frac{ \sum_{m=1}^{M} \sum_{i=1}^{d} \bm{z}_m(i) }{d} = \frac{Mk}{d} \geq \frac{M}{d}.
\end{align*}
The neighborhood $\mathcal{N}_H(\bm{z}; k/2)$ with a common support index $i_*$ satisfies 
\begin{align*}
    \big|\mathcal{N}_H(\bm{z}; k/2) \big| = & ~ \sum_{i = 0}^{k/2} \binom{k - 1}{i} \binom{d - k}{i} \leq [k(d - k)]^{k/2}.
\end{align*}
Continuing, we obtain that
\begin{align*}
    & ~ \frac{|\mathcal{Z}_*|}{\max_{\bm{z} \in \mathcal{Z}_*}|\mathcal{N}_H(\bm{z}; k/2)|}\\
    \geq & ~ \frac{M}{d \cdot \max_{\bm{z} \in \mathcal{Z}_*}|\mathcal{N}_H(\bm{z}; k/2)|} \\
    \geq & ~ \frac{\binom{d}{k}}{d \cdot [k(d - k)]^{k/2}} \geq \frac{d^k}{k^k} \frac{1}{k^{k/2}d^{k/2}d}  = \bigg( \frac{d}{k^{3}}\bigg)^{k/2} \cdot \frac{1}{d}.
\end{align*}
Consequently,
\[
    \log \bigg( \frac{|\mathcal{Z}_*|}{\max_{\bm{z} \in \mathcal{Z}_*}|\mathcal{N}_H(\bm{z}; k/2)|} \bigg) \gtrsim k \log d.
\]
Applying Theorem~\ref{thm:fund-limits}(b) by setting $\xi = 3/4$ yields the desired result.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Propositions in Section~\ref{sec:specific-examples}}






\subsubsection{Proof of Proposition~\ref{prop:reduction-PathPCA}} \label{app:reduction-PathPCA}
%\gwcomment{Will be modified later.}

\begin{proof}
%\apcomment{Make notation consistent.}
%\gwcomment{This proof will be moved to Section~\ref{app:reduction-PathPCA}.}
%\gwcomment{The proof has been updated.}
For the proof, one should think of the estimator and parameters as indexed by the natural number $n$, although we drop this explicit dependence for clarity of exposition.
We begin by introducing formal definition for the detection problem for path sparse PCA. Recall the definition of a qualified estimator (Definition~\ref{def:qual-est}).
%, respectively.  

\begin{definition}
\textbf{Detection problem for path sparse PCA.} Suppose $\bm{v}_*$ is $\bm{0}_d$ with probability $1/2$ and an arbitrary vector in the set $\mathcal{S}^{d - 1} \cap \mathcal{P}^k$ with probability $1/2$. The detection problem for path sparse PCA -- $\text{D-PSPCA}(n, k, d, \lambda)$ is defined as the resulting hypothesis testing problem 
\begin{align*}
	H_0: \bm{X} \sim \mathcal{D}(0; \bm{0}_d)^{\otimes n} ~~~~\text{and}~~~~  H_1: \bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)^{\otimes n}.
\end{align*}
%with a unknown ground truth $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$. 
\end{definition}

%\begin{definition}
%\textbf{Estimation problem for path sparse PCA.} The estimation problem of path sparse PCA -- $\text{E-PSPCA}(n, k, d, \lambda)$ is to find a qualified estimator$(\epsilon)$ denoted by $\widehat{\bm{v}}$ which additionally guarantees that
%	\begin{align*}
%		\liminf_{n \to \infty} \Pr \left\{  \widehat{\bm{v}} = \bm{0}_{d_n} \right\} \geq \frac{1}{2} + \epsilon ,
%	\end{align*} 
%	if $\bm{v}_* = \bm{0}_d$. % with some fixed $\epsilon$ used in the qualified estimator. 
%%defined as follows: Given a set of $n$ i.i.d. samples $\bm{X}$ drawn from $\mathcal{D}(\lambda; \bm{v}_*)$ with a unknown ground truth $\bm{v}_* \in \{\mathcal{S}^{d - 1} \cap \mathcal{P}^k\} \cup {\color{red}\{\bm{0}_d\}}$, the task is to find a vector $\widehat{\bm{v}} \in \{\mathcal{S}^{d - 1} \cap \mathcal{P}^k\} \cup {\color{red}\{\bm{0}_d\}}$ such that $\|\bm{v}_* - \widehat{\bm{v}}\|_2 < \frac{1}{2}$ holds with probability at least $1/2 + \epsilon$ for some fixed $0 < \epsilon < 1/2$.    
%\end{definition} 
%\apcomment{Why do you need to define the estimation problem at all? You directly construct a detector it seems?}

We show our average-case hardness result by contradiction, i.e., we assume there exists a randomized polynomial-time qualified estimator. We then transform it into a good detector for the path-sparse PCA problem and eventually the secret leakage planted clique problem. The proof of Proposition~\ref{prop:reduction-PathPCA} can be separated into four parts: 

\begin{enumerate}
	\item \textbf{Constructing a randomized polynomial time algorithm for detection based on an estimation algorithm for path sparse PCA.} 
	
	Suppose $\EST_n: (\mathbb{R}^{d_n})^n \to \mathbb{R}^{d_n}$ is a sequence of randomized polynomial time functions for the assumed qualified estimator. 
	
%	\gwcomment{To be general, let the successful prob of nontrivial detector wp $> 1/2$} 
%	$\EST_n$ maps an instance $\bm{X}$ of $n$ samples for path sparse PCA problem to an estimation $\widehat{\bm{v}} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$ which satisfies the following property: {\color{orange}when we have $\lambda = \Theta(1)$ some known constant}, and $\bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)^{\otimes n}$ with 
%		\begin{align*}
%			\bm{v}_* \in \text{UPSPCA}^k := \left\{ \left. \bm{v} \in \left\{0, \frac{1}{\sqrt{k}} \right\}^d ~\right|~ \bm{v} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k \right\},  
%		\end{align*} 
%		$\mathcal{R}_n$ ensures that $\widehat{\bm{v}} \in \text{UPSPCA}^k$ and $\|\bm{v}_* - \widehat{\bm{v}}\|_2 < \frac{1}{2}$ holds with probability $1 - o(1)$ as $n \rightarrow \infty$. 


	Given $\EST_n$, we construct a detector $\DET_n: \EST \times (\mathbb{R}^{d_n})^n \times \mathbb{R} \to \{0,1\}$ of $\text{D-PSPCA}(n, k, d, \lambda)$, which maps a tuple of three components including an estimator function $\EST_n$, an instance $\bm{X}$ and a known eigengap $\lambda$ to  a detection algorithm that outputs one of the two hypotheses; this is presented in Algorithm~\ref{alg:D-from-E}. %\apcomment{Add unit normalization of $\widehat{v}$.} 
%	\gwcomment{DONE. We cannot simply add unit normalization. Since $\| \widehat{\bm{v}} - \bm{v}_* \|_2 \leq \frac{1}{2}$ does not imply $\| \widehat{\bm{v}} / \|\widehat{\bm{v}} \|_2 - \bm{v}_* \|_2 \leq \frac{1}{2}$ when $\|\widehat{\bm{v}} \|_2 < 1$ and $\bm{v}_* \in \mathcal{S}^{d_n} \cap \mathcal{P}^k$. Modify the definition of qualified estimator due to this issue.}
	
%	based on $\EST_n$ for the detection problem of path sparse PCA with unit signals $\bm{v}_* \in \{0, 1/\sqrt{k}\}^d$ as presented in Algorithm~\ref{alg:D-from-E}. 

\begin{algorithm}
\caption{Detector $\DET_n$ of path sparse PCA}
\label{alg:D-from-E}
\textbf{Input:} A function $\EST_n$, an instance $\bm{X}$ drawn from $\text{D-PSPCA}(n,k,d,\lambda)$, and an eigengap $\lambda$.  
\begin{algorithmic}[1]
\State Compute the estimation $\widehat{\bm{v}} := \EST_n(\bm{X})$ of path sparse PCA. \label{alg:D-from-E-1}
\State Normalize the estimation 
	\begin{align*}
		\tilde{\bm{v}} := \left\{
		\begin{array}{lll}
			\widehat{\bm{v}} / \|\widehat{\bm{v}}\|_2 & \text{ if } \widehat{\bm{v}} \neq \bm{0}_{d_n} \\
			\text{any point in $\mathcal{S}^{d_n} \cap \mathcal{P}^k$} & \text{ if } \widehat{\bm{v}} = \bm{0}_{d_n}
		\end{array}
		\right. .
	\end{align*} \label{alg:D-from-E-2}
\State Set the sample covariance matrix $\widehat{\bm{\Sigma}} := \frac{1}{n} \bm{X} \bm{X}^{\top}$. \label{alg:D-from-E-3}
\If{$\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}} \geq 1 + \lambda/4 - (1 + \lambda) \sqrt{k \ln d / n}$} \label{alg:D-from-E-4}
\State \textbf{Output:} $1$, i.e., $\bm{X}$ is from $H_1$.
\Else 
\State \textbf{Output:} $0$, i.e., $\bm{X}$ is from $H_0$.
\EndIf
\end{algorithmic}
\end{algorithm} 

	The detector $\DET_n$ clearly runs in polynomial time. Next, we show that for any instance $\bm{X}$ of $\text{D-PSPCA}(n, k, d, \lambda)$ and a known eigengap $\lambda$ (satisfying the conditions required in Theorem~\ref{thm:convergence} and Theorem~\ref{thm:initialization-method}), $\DET_n$ satifies
	\begin{align} \label{eq:key-claim}
		\liminf_{n \rightarrow \infty} \left( \mathbb{P}_{H_0}[\DET_n(\bm{X}) = 1] + \mathbb{P}_{H_1}[\DET_n(\bm{X}) = 0] \right) < 1. 
	\end{align}

%	maps an instance $\bm{X}$ to an estimation $\widehat{\bm{v}} \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$ such that $\|\bm{v}_* - \widehat{\bm{v}}\|_2 < \frac{1}{2}$ holds with probability greater than $\frac{1}{2} + \epsilon$ for any $0 < \epsilon < 1/2$. \footnote{Recall that our proposed projected power method for path sparse PCA ensures estimation task with probability at least $1 - \exp(- ck) > \frac{1}{2} + \epsilon$.}.
		
%		Set the sample covariance matrix $\widehat{\bm{\Sigma}} := \frac{1}{n} \bm{X} \bm{X}^{\top}$. If $\widehat{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \widehat{\bm{v}} \geq 1 + \lambda/4 - (1 + \lambda) \sqrt{k \ln d / n}$, then $\DET_n(\bm{X}) = 1$, i.e., $\bm{X}$ is generated from hypothesis $H_1$. Otherwise, we have $\mathcal{DE}_n(\bm{X}) = 0$, i.e., $\bm{X}$ is generated from null hypothesis $H_0$. 
		
		To establish claim~\eqref{eq:key-claim}, note that the ``if criteria'' (Step-\eqref{alg:D-from-E-3} in Algorithm~\ref{alg:D-from-E}) is based on the following two-part calculation. 
 On the one hand, suppose the instance $\bm{X}$ is drawn from $\mathcal{D}(\lambda; \bm{v}_*)^{\otimes n}$ for $\bm{v}_* \in \mathcal{S}^{d - 1} \cap \mathcal{P}^k$. Consider the orthogonal decomposition of $\tilde{\bm{v}}$ given by $\tilde{\bm{v}} = \alpha \bm{v}_* + \beta \bm{v}_{\perp}$ with $\|\bm{v}_*\|_2 = \|\bm{v}_{\perp}\|_2 = 1, ~ \langle \bm{v}_*, \bm{v}_{\perp} \rangle = 0, \alpha^2 + \beta^2 = 1$. Based on the definition of the qualified estimator, the normalized vector $\|\tilde{\bm{v}} - \bm{v}_*\|_2$ satisfies $\| \widehat{\bm{v}} / \|\widehat{\bm{v}} \|_2 - \bm{v}_* \|_2 < \sqrt{2 - 2\sqrt{15/16}} < 1/2$. Consequently, we have $\alpha > 1/2$, and therefore the objective satisfies
		\begin{align*}
			\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}} = & ~ (\alpha \bm{v}_* + \beta \bm{v}_{\perp})^{\top} \widehat{\bm{\Sigma}}  (\alpha \bm{v}_* + \beta \bm{v}_{\perp}) \\
			= & ~ \alpha^2 \bm{v}_*^{\top} \bm{\Sigma} \bm{v}_* + \beta^2 \bm{v}_{\perp}^{\top} \bm{\Sigma} \bm{v}_{\perp} + 2 \alpha \beta \bm{v}_{*}^{\top} \bm{\Sigma} \bm{v}_{\perp} + \widehat{\bm{v}}^{\top} \bm{W} \widehat{\bm{v}} \\
			\geq & ~ \alpha^2 (1 + \lambda) + \beta^2 + 0 + \rho(\bm{W}, P^*) \\
			\geq & ~ 1 + \frac{\lambda}{4} - (1 + \lambda) \sqrt{k \ln d / n}, 
		\end{align*}
		where the final inequality holds due to $\rho(\bm{W}, P^*) \leq (1 + \lambda) \sqrt{k \ln d / n}$ with probability $1 - \exp(- ck)$.  
		On the other hand, if the instance $\bm{X}$ is drawn from $\mathcal{D}(0; \bm{0}_d)^{\otimes n}$, then the objective satisfies
		\begin{align*}
			\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}} = & ~ \tilde{\bm{v}}^{\top} (\bm{I}_d + \bm{W}) \tilde{\bm{v}} \\
			\leq & ~ 1 + \rho(\bm{W}, P^*) \\
			\leq & ~ 1 + (1 + \lambda) \sqrt{k \ln d / n} \\
			< & ~ 1 + \frac{\lambda}{4} - (1 + \lambda) \sqrt{k \ln d / n},
		\end{align*}
		where the final strict inequality holds in the parameter regime of interest. Assuming that $n$ is sufficiently large and combining the above two cases implies 
		\begin{align*}
			\mathbb{P}_{H_0}[\DET_n(\bm{X}) = 1] \leq & ~ \frac{1}{2} - \epsilon + \exp(- ck), \\
			\mathbb{P}_{H_1}[\DET_n(\bm{X}) = 0] \leq & ~ \frac{1}{2} - \epsilon + \exp(- ck), 
		\end{align*} 
		and therefore, 
		\begin{align*}
			\liminf_{n \rightarrow \infty} \left( \mathbb{P}_{H_0}[\DET_n(\bm{X}) = 1] + \mathbb{P}_{H_1}[\DET_n(\bm{X}) = 0] \right) < 1. 
		\end{align*}
		%For computational complexity, since Step-\eqref{alg:D-from-E-1} -- computing the estimation $\widehat{\bm{v}}$, Step-\eqref{alg:D-from-E-2} -- normalizing $\widehat{\bm{v}}$ for $\tilde{\bm{v}}$, Step-\eqref{alg:D-from-E-3} -- setting the sample covariance matrix $\widehat{\bm{\Sigma}}$, and Step-\eqref{alg:D-from-E-4} -- computing objective value $\tilde{\bm{v}}^{\top} \widehat{\bm{\Sigma}} \tilde{\bm{v}}$, take polynomial running time by our assumption, the detector $\DET_n$ is still a randomized polynomial-time algorithm. 
	
	\item \textbf{Reduction from $K$-Partite PC detection problem to path sparse PCA detection problem.} 
		
%		\gwcomment{SPCA-RECOVERY method \cite{brennan2018reducibility}: 
%		\begin{itemize}
%			\item BC-Recovery: maps $\textbf{PDS}_R(n, k, 1/2 + \rho, 1/2)$ to $\text{BC}_R(n,k,\mu)$ with $\mu = \frac{\log(1 + 2 \rho)}{2\sqrt{6 \log n + 2 \log 2}}$ (structure preserving)
%			\item Random-Rotation: maps $\text{BC}_R(n,k,\mu)$ to $\text{UBSPCA}(n, k, d, \theta)$ with $d = n$ and $\theta = \frac{k^2 \mu^2}{\tau n}$. 
%		\end{itemize}
%		When $\rho = 1/2$, we have $\textbf{PDS}_R(n, k, 1, 1/2) = \textbf{PC}_R(n, k, 1/2)$. Instance from the recovery problem can be used for the detection problem, and vice versa.
%		}
		
		%As a brief summary for this part, 
		In this step, we use the average-case reduction method -- SPCA-RECOVERY [Figure 19, \citep{brennan2018reducibility}] to be our reduction method. Recall \cite{brennan2018reducibility} show that SPCA-RECOVERY maps an instance of planted clique problem $G \sim \text{PC}(n,k,1/2)$ to an instance $\bm{X}$ of vanilla sparse PCA detection problem (with ground truth $\bm{v}_*$ takes values in discrete set $\{0, 1 / \sqrt{k}\}$) approximately under total variance distance. Note that SPCA-RECOVERY ensures a preserving property (See Remark~\ref{remark:str-pre} for details). In words, SPCA-RECOVERY maps an instance $G \sim \text{K-PC}(n,k,1/2)$ to an instance $\bm{X} \sim \text{D-PSPCA}(n, k, d, \lambda)$ approximately under total variance distance while maintaining the structure, i.e., mapping rows associated with planted $k$-clique to rows associated with the corresponding path.

		\begin{remark} \label{remark:str-pre}
		\textbf{Structure preserving property of \textup{SPCA-RECOVERY}.} Given any $G \sim K\text{-PC}(n, k, 1/2)$ as an input instance of \textup{SPCA-RECOVERY}, \textup{SPCA-RECOVERY} maps the rows in the adjacency matrix $\bm{A}(G)$ concerning the planted $k$-clique of $G$ to the rows corresponding to the support set of the underlying path structure for the sample matrix (instance) $\bm{X} = [\bm{x}_1 ~|~ \cdots ~|~ \bm{x}_n]^{\top} \sim \text{D-PSPCA}(n, k, d, \lambda)$. 
		\end{remark} 
		
%		\gwcomment{Proof for this part starts here... (1) define planted clique for a fixed clique structure (2) fix this clique, we have 2018 SPCA-RE maps a PC instance to a SPCA instance with corresponding support set}

		Armed with these tools, we now present a quantitative analysis of the total variance distance that arises from our reductions. Let us start by recalling the parameter regime with some additional notation as follows. Let $\beta \geq 1/2$. For path sparse PCA detection problem, define the following sequence of parameters 
		\begin{align*}
			& ~ k_{\subindex} = \lceil \subindex^{\beta} \rceil, \quad \rho_{\subindex} = \frac{1}{2}, \quad n_{\subindex} = d_{\subindex} = \subindex, \quad \mu_{\subindex} = \frac{\log 2}{2\sqrt{6 \log \subindex + 2 \log 2}}, \quad \lambda_{\subindex} = \frac{k_{\subindex}^2}{\tau_{\subindex} \subindex} \cdot \frac{(\log 2)^2}{4 (6 \log \subindex + 2 \log 2)},
		\end{align*}
		where $\tau_{\subindex} \rightarrow \infty$ as ${\subindex} \rightarrow \infty$, i.e., an arbitrarily slowly growing function of $\subindex$. In this part 2), to be clear, we use $\subindex \in \mathbb{Z}$ to denote the integer index of the above sequence of parameters, and $n_{\subindex} ~ (= \subindex)$ to denote the sample size corresponding with index $\subindex$. Later in this proof, we do not distinguish the difference between sample size $n, n_{\subindex}$ and index $z$. Let $\varphi_{\subindex} = \text{SPCA-RECOVERY}$ be the reduction method. We use graph $G_{\subindex}$ to denote an instance of $\text{K-PC}(n_{\subindex},k_{\subindex},1/2)$, and use $\bm{X}_{\subindex} = \varphi_{\subindex} (G_{\subindex})$ to be the output of $\text{SPCA-RECOVERY}$ with input $G_{\subindex}$. To be concise, we use $\mathcal{L}(\bm{X}_{\subindex})$ to denote the distribution of a given instance $\bm{X}_{\subindex}$.
		
		
		Suppose $G_{\subindex} \sim \mathcal{G}_{\mathcal{D}}(\subindex, k_{\subindex}, 1/2)$, is drawn from the $H_1$ hypothesis of $\text{K-PC}(\subindex,k_{\subindex}, 1/2)$. Let $\bm{v}_*$ denote the unit vector supported on indices with respect to the clique in $G_{\subindex}$ with nonzero entries equal to $1/\sqrt{k_{\subindex}}$. Using\footnote{It is easy to observe that our parameter regime of $(\subindex, \mu_{\subindex}, \rho_{\subindex}, \tau(\subindex))$ satisfies the conditions presented in Lemma 6.7 and Lemma 8.2 in \cite{brennan2018reducibility}.} Lemma 6.7 and Lemma 8.2 in \cite{brennan2018reducibility}, we have
		\begin{align*}
			\text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_{\subindex}), \mathcal{D}(\lambda; \bm{v}_*) \right) \leq O\left( \frac{1}{\sqrt{\log {\subindex}}} \right) + \frac{2 (\subindex + 3)}{\tau \subindex - \subindex - 3} \rightarrow 0 
		\end{align*}
		as $\subindex \rightarrow \infty$. On the other hand, if an instance $G_{\subindex} \sim \mathcal{G}_{\mathcal{D}}(\subindex, 1/2)$, is drawn from the $H_0$ hypothesis of $\text{K-PC}(\subindex,k_{\subindex},1/2)$. Still using Lemma 6.7 and Lemma 8.2 in \cite{brennan2018reducibility}, we also have 
		\begin{align*}
			\text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_{\subindex}), \mathcal{D}(0; \bm{0}_d) \right) \leq O\left( \frac{1}{\sqrt{\log \subindex}} \right) + \frac{2 (\subindex + 3)}{\tau \subindex - \subindex - 3} \rightarrow 0 
		\end{align*}
		as $\subindex \rightarrow \infty$. Combining the above two cases ensures the reduction from $K$-Partite PC detection problem to path sparse PCA detection problem as we desired. 
 		
 		
 		%		Due to the structure preserving property of \textup{SPCA-RECOVERY}, each part of the vertex set $V$ in graph $G \sim K\text{-PC}(n, k, 1/2)$ can be viewed as the vertex set of a layer in the layered graph with size $(n - 2) / k$. The planted $k$-clique (with one vertex in each part) in $G \sim K\text{-PC}(n, k, 1/2)$ corresponds to a path of length $k$ (excluding source and terminal) in the layered graph. 
		
	\item \textbf{Constructing a detection algorithm for $K$-Partite PC based on a detection algorithm for path sparse PCA.} 
	
		
		Recall that there exists a sequence of detector algorithms $\DET_n$ that solves the detection problem of path-sparse PCA ($\text{D-PSPCA}(n, k, d, \lambda)$) as described above. Observe that under $H_1$ hypothesis,
		\begin{align*}
			& ~ \left|\mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n(\bm{X}) = 1] - \mathbb{P}_{\bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)}[\DET_n(\bm{X}) = 1] \right| \\
			\leq & ~ \text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_n), \mathcal{D}(\lambda; \bm{v}_*) \right) \rightarrow 0 \quad \text{as } n \rightarrow \infty,  
		\end{align*}
		where the limitation of the final inequality holds due to the analysis in part 2) with $n_{\subindex} = z$. Since 
		\begin{align*}
			& ~ \mathbb{P}_{\bm{X} \sim \mathcal{D}(\lambda; \bm{v}_*)}[\DET_n(\bm{X}) = 1] = \mathbb{P}_{H_1}[\DET_n(\bm{X}) = 1] \geq \frac{1}{2} + \epsilon
		\end{align*} 
		for sufficiently large $n$ by the definition of the detector $\DET_n$, it follows that 
		\begin{align*}
			\mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n \circ \varphi_n(G_n) = 1] 
			= & ~ \mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n (\bm{X}_n) = 1] \geq \frac{1}{2} + \frac{\epsilon}{2} 
		\end{align*}
		for sufficiently large $n$. On the other hand, under $H_0$ hypothesis, 
		\begin{align*}
			& ~ \big|\mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n(\bm{X}) = 0] - \mathbb{P}_{\bm{X} \sim \mathcal{D}(0; \bm{0}_d)}[\DET_n(\bm{X}) = 0] \big| \\
			\leq & ~ \text{d}_{\text{TV}} \left( \mathcal{L}(\bm{X}_n), \mathcal{D}(0; \bm{0}_d) \right) \rightarrow 0 \quad \text{as } n \rightarrow \infty. 
		\end{align*}
		Still 
		\begin{align*}
			& ~ \mathbb{P}_{\bm{X} \sim \mathcal{D}(0; \bm{0}_d)}[\DET_n(\bm{X}) = 0] = \mathbb{P}_{H_0}[\DET_n(\bm{X}) = 0] \geq \frac{1}{2} + \epsilon
		\end{align*} 
		holds for sufficiently large $n$ by the definition of the detector $\DET_n$. Then 
		\begin{align*}
			& ~ \mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n \circ \varphi_n(G_n) = 0] = \mathbb{P}_{\bm{X} \sim \mathcal{L}(\bm{X}_n)}[\DET_n (\bm{X}_n) = 0] \geq \frac{1}{2} + \frac{\epsilon}{2}. 
		\end{align*}
		Combining the above two results together implies
		\begin{align*}
			\liminf_{n \rightarrow \infty} & ~ \bigg( \mathbb{P}_{G_n \sim \mathcal{G}(n, 1/2)}[\DET_n \circ \varphi_n(G_n) = 1] + \mathbb{P}_{G_n \sim \mathcal{G}(n, k_n, 1/2)}[\DET_n \circ \varphi_n(G_n) = 0] \bigg) \leq 1 - \epsilon, 
		\end{align*}
		i.e., the detection method works.
		
		
	\item \textbf{Putting together the pieces.}

		Putting the above three parts together, we have used a polynomial-time qualified estimator to construct a sequence of functions $\DET_n \circ \varphi_n$ that detects K-Partite PC (Definition~\ref{def:KPC}) in polynomial time. This contradicts the K-Partite PC conjecture (Conjecture~\ref{conj:KPC-hardness}). Therefore, no such sequence of qualified estimator functions $\EST_n$ exists when the sequence of parameters $\{(k_n, d_n, \lambda_n, \tau_n)\}_{n \in \mathbb{N}}$ is in the proposed regime.
		%, i.e., 
	%\begin{align*}
	%	k_n := \lceil c n^{1/2} \tau_n^{1/2} (\log n)^{1/2} \rceil, \quad d_n := n,\quad
	%	\lambda_n := \frac{k_n^2}{\tau n} \cdot \mu_n^2 = \left\lceil \frac{c^2 (\log 2)^2}{24 + \frac{8\log 2}{\log n}} \right\rceil = \Theta \left( 1 \right),  
	%\end{align*}
	which completes the proof of the theorem.
	%\footnote{Recall that, by letting $c \geq 12/\log 2$, we have the eigengap $\lambda \geq 5$, which could be further used to ensure the good region condition as required in Corollary~\ref{coro:initial-PS-PCA}.} %\apcomment{Explicitly have a point ``4. Putting together the steps''}
	
%		Assuming the Hardness Assumption for Planted Clique Problem [Definition~\ref{defn:PC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving $\text{UBSPCA}(k,n,n,\lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$. Now in Algorithm , let $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ be a special case of UBSPCA problem with signal $\bm{v}_* \in \textup{UBS}_k \cap \mathcal{P}^k$, where $\mathcal{P}^k$ is the path structure set. The \textup{SPCA-RECOVERY} satisfies the following property. 
		
		
	
%		Recall that the sequence of detectors $\mathcal{DE}_n$ is still a sequence of randomized polynomial time algorithms for path sparse PCA. Consider any instance $G \sim K\text{-}PC(N, K, 1/2)$ and corresponding $\bm{X}$ obtained by the reduction \textup{SPCA-RECOVERY} with input $G$. Once 
%		\begin{align*}
%			\left\{ 
%			\begin{array}{llll}
%				\mathcal{DE}_n(\bm{X}) = 1 & \text{ A path sparse clique is detected,} \\
%				\mathcal{DE}_n(\bm{X}) = 0 & \text{ No path sparse clique is detected.}
%			\end{array}
%			\right.
%		\end{align*}
%		Due to the structure preserving property of \textup{SPCA-RECOVERY}, a path sparsity signal corresponds with a planted clique in graph $G$. Thus $\mathcal{DE}_n(\bm{X}) = 1$ implies a planted clique of $G$ for $K$-Partite PC, and vice versa. Then we have
%		\begin{align*}
%			\mathcal{DE}_n \circ \textup{SPCA-RECOVERY}: \mathcal{G}_n \to \{0, 1\} 
%		\end{align*}
%		a randomized polynomial time detector for $K$-Partite PC detection problem, which contradicts to the Conjecture~\ref{conj:KPC-hardness}. \gwcomment{be formal as the conjecture 1 for K-PC.}
%		
%		
%		Therefore, there is no randomized polynomial time algorithms for the recovery problem of path sparse PCA. 
\end{enumerate}


\end{proof}


%The formal definition of the Planted Clique Detection Problem (PC) is presented as follows. 
%
%\begin{definition} \label{defn:PC}
%\textbf{Planted Clique Problem (PC).} The task of planted clique is to find the vertex set of a $K$-clique planted uniformly at random in an $N$-vertex Erd\"{o}s-Renyi random graph $G$, which can be represented as a testing problem $\text{PC}(N, K, 1/2)$ between the following two hypothesises 
%\begin{align}
%    & H_0: ~ G \sim \mathcal{G}(N, 1/2) ~~~~\text{and}~~~~ H_1: ~ G \sim \mathcal{G}(N, K, 1/2), \notag
%\end{align}
%where $\mathcal{G}(N, 1/2)$ denotes the $N$-vertex Erd\"{o}s-Renyi random graph with edge density $1/2$ and $\mathcal{G}(N, K, 1/2)$ the distribution resulting from planting a $K$-clique uniformly at random in $\mathcal{G}(N, 1/2)$.
%\end{definition} 
%
%\gwcomment{Maybe move the following Theorem to appendix?} 
%
%To simplify the analysis of average-case hardness, we use the following variant of sparse PCA -- uniformly biased sparse PCA (UBSPCA) proposed in \cite{brennan2018reducibility}.  
%
%\begin{definition} \label{def:UBSPCA}
%\textbf{UBSPCA Problem,} \cite{brennan2018reducibility}. Consider the following simple hypothesis testing variant $\text{UBSPCA}(k,n,d,\lambda)$ of sparse PCA with hypothesis: 
%\begin{align*}
%    H_0: ~ \bm{X} \sim \mathcal{N}(\bm{0}_d, \bm{I}_d)^{\otimes n} ~~~~\text{and}~~~~ H_1: ~ \bm{X} \sim \mathcal{N}(\bm{0}_d, \lambda \bm{v}_* \bm{v}_*^{\top} + \bm{I}_d)^{\otimes n}, 
%\end{align*}
%where $\bm{v}_*$ is in the uniformly biased set $\textup{UBS}_k := \left\{ \left. \bm{v} \in \left\{ 0, 1/\sqrt{k}\right\}^d ~ \right| ~ \|\bm{v}\|_2 = 1, ~ \|\bm{v}\|_0 = k  \right\}.$
%\end{definition}

%The reduction of path sparse PCA is based on an existing average-case reduction method SPCA-RECOVERY which was proposed in [Figure 19, Section 8, \cite{brennan2018reducibility}].  The following Theorem~\ref{thm:reduction-UBSPCA} is a restatement of the theoretical results for SPCA-RECOVERY given by \cite{brennan2018reducibility}. 
%
%\begin{theorem} \label{thm:reduction-UBSPCA}
%\textbf{Restate of [Theorem 8.7, \cite{brennan2018reducibility}].} Let $\alpha \in \mathbb{R}, \beta \in (0,1)$, and suppose there exists a sequence $\{(N_i, K_i, D_i, \lambda_i)\}_{i = 1}^{\infty}$ of parameters such that: 
%\begin{enumerate}
%    \item The parameters are in the regime $d = \Theta(N), \lambda = \tilde{\Theta}(N^{- \alpha})$ and $k = \tilde{\Theta}(N^{\beta})$ or equivalently,
%    \begin{align*}
%        & \lim_{i \rightarrow \infty} \log (\lambda_i^{-1} / \log N_i) = \alpha & \lim_{i \rightarrow \infty} \log (K_i / \log N_i) = \beta
%    \end{align*}
%    \item If $\alpha > 0, \beta > 1/2$, then the following holds. Let $\epsilon > 0$ be fixed and let $\bm{X}_i$ be an instance of $\text{UBSPCA}(K_i,N_i,D_i,\lambda_i)$. There is no sequence of randomized polynomial-time computable functions $\phi_i: \mathbb{R}_n: \mathbb{R}^{D_i \times N_i} \to \binom{[N_i]}{k}^2$ such that for all sufficiently large $i$ the probability that $\phi_i(\bm{X}_i)$ is exactly the pair of latent row and column supports of $\bm{X}_i$ is at least $\epsilon$, assuming the PC recovery conjecture. 
%\end{enumerate}
%Therefore, given the PC recovery conjecture, the computational boundary for $\text{UBSPCA}(k,n,d,\lambda)$ 
%in the parameter regime $\theta = \tilde{\Theta}(n^{- \alpha})$ and $k = \tilde{\Theta}(n^{\beta})$ is $\alpha^* = 0$ when $\beta > 1/2$. 
%\end{theorem}

%
%As a brief summary of Theorem~\ref{thm:reduction-UBSPCA}, the SPCA-RECOVERY maps an instance of planted clique problem $G \sim \text{PC}(n,k,1/2)$ to an instance $\bm{X} \sim \text{UBSPCA}(k,n,d,\lambda)$ approximately under total variance distance with $\lambda = \tilde{\Theta}(k^2/n), d = n$. Assuming the Hardness Assumption for Planted Clique Problem [Definition~\ref{defn:PC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving $\text{UBSPCA}(k,n,n,\lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$. Now, let $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ be a special case of UBSPCA problem with signal $\bm{v}_* \in \textup{UBS}_k \cap \mathcal{P}^k$, where $\mathcal{P}^k$ is the path structure set. The \textup{SPCA-RECOVERY} satisfies the following property. 
%\begin{remark}
%\textbf{Structure Preserving of \textup{SPCA-RECOVERY}.} Given any $G \sim K\text{-PC}(n, k, 1/2)$ as an input instance of \textup{SPCA-RECOVERY}, \textup{SPCA-RECOVERY} maps the rows in the adjacency matrix $\bm{A}(G)$ concerning the planted $k$-clique of $G$ to the rows corresponding to the support set of the underlying path structure for the sample matrix (instance) $\bm{X} = (\bm{x}_1 ~|~ \cdots ~|~ \bm{x}_n)^{\top} \sim \text{Path-UBSPCA}(k + 2, n, d, \lambda)$. 
%\end{remark}
%


%\begin{corollary} \label{coro:reduction-PathPCA}
%Under the same parameter regime presented in Theorem~\ref{thm:reduction-UBSPCA}, 
%\begin{itemize}
%    \item The SPCA-RECOVERY maps an instance $G \sim K\text{-PC}(n, k, 1/2)$ directly to an instance $\bm{X} \sim \text{Path-UBSPCA}(k + 2, n, d, \lambda)$ approximately under total variance distance with $\lambda = \tilde{\Theta}(k^2/n), d = n$. 
%    \item Assuming the Hardness Assumption [Conjecture2 \& Conjecture 3, \cite{brennan2020reducibility}] for $K$-Partite Planted Clique Problem [Definition~\ref{def:KPC}], the above reduction SPCA-RECOVERY ensures that there is no randomized polynomial-time algorithm solving  $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ with $n \ll k^2$ when $\lambda = \tilde{\Theta}(1)$, which matches the number of samples $n$ required in Corollary~\ref{coro:initial-PS-PCA} (ignoring logarithm term). 
%\end{itemize}
%\end{corollary}
%The proof of Corollary~\ref{coro:reduction-PathPCA} is presented in Section~\ref{app:reduction-PathPCA}. Corollary~\ref{coro:reduction-PathPCA} shows that the $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ has an average-case hardness lower bound on the sample size ($n \geq c k^2$ for some constant $c$). Since $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ is a special case of the path sparse PCA, the above result also ensures an average-case hardness lower bound for the path sparse PCA.


%\textbf{Proof of Corollary~\ref{coro:reduction-PathPCA}.}
%Thus, to show the average-case hardness for path-sparse PCA, it is sufficient to show the average-case hardness for the detection problem of $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$. Since the SPCA-RECOVERY Algorithm is designed to \emph{preserve the structure} during the reduction, each part of the vertex set $V$ in graph $G \sim K\text{-PC}(n, k, 1/2)$ can be viewed as the vertex set of a layer in the layered graph with size $(n - 2) / k$. The planted $k$-clique (with one vertex in each part) in $G \sim K\text{-PC}(n, k, 1/2)$ corresponds to a path of length $k$ (excluding source and terminal) in the layered graph. Therefore, assuming the $K$-Partite PC Hardness (Definition~\ref{def:KPC-hardness}), using Theorem~\ref{thm:reduction-UBSPCA} and structure preserving property, the $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ has an average-case hardness lower bound on the sample size ($n \geq c k^2$ for some constant $c$). Since $\text{Path-UBSPCA}(k + 2, n, d, \lambda)$ is a special case of the path sparse PCA, the above hardness result also ensures an average-case hardness lower bound for the path sparse PCA.



%\begin{itemize}
%    \item \textbf{Observation.} Note that: each part of the vertex set $V$ in graph $G \sim K\text{-PC}(n, k, 1/2)$ can be viewed as the vertex set of a layer in the layered graph with size $(n - 2) / k$. Thus the planted $k$-clique (with one vertex in each part) in $G \sim K\text{-PC}(n, k, 1/2)$ corresponds to a path of length $k$ (excluding source and terminal) in the layered graph.
%    
%    \item {\color{blue}\textbf{Structure Preserving.} Based on the structure preserving property and above observation, given $G \sim K\text{-PC}(n, k, 1/2)$ as input instance of SPCA-RECOVERY, the rows in the adjacency matrix $\bm{A}(G)$ concerning the planted $k$-clique of $G$ transfer/map to the rows corresponding to the support set of the underlying path of the path-sparse PCA for the sample matrix $\bm{X} = (\bm{x}_1 ~|~ \cdots ~|~ \bm{x}_n)^{\top}$ as the output of $\textup{SPCA-RECOVERY}(G)$.}
%    % \gwcomment{This is what I learned from \cite{brennan2019optimal}. However, since the Algorithm is very complex, I did not check the proof detail. Or we may refer to the result in \cite{brennan2018reducibility} for sparse PCA by restricting $\lambda \asymp \sqrt{k^2 / n \log n}$ with $k = K = w(\sqrt{N})$.} 
%\end{itemize}

\subsubsection{Proof of Proposition~\ref{prop:TS-SDP-hard}} \label{proof-prop-TS-SDP-hard}
We first state an auxilary lemma.
\begin{lemma}\label{lemma:concentration-prop-TS-SDP-hard}
Let $\bm{Z} \in \mathbb{R}^{n \times d}$ be a random matrix such that entries of $\bm{Z}$ are independent gaussian random variables with zero mean and unit variance. Let $\bm{Z}^{(i)} \in \mathbb{R}^{n}$ denotes the $i$-th column of $\bm{Z}$ for each $i \in [d]$. Further, let
\[
    \bar{\bm{Z}}^{(i)} = \frac{\sqrt{n} \bm{Z}^{(i)} }{\|\bm{Z}^{(i)}\|_{2}},\;\forall \; i \in [d] \quad \text{and} \quad \bar{\bm{Z}} = \big[ \bar{\bm{Z}}^{(1)}\;|\cdots|\;\bar{\bm{Z}}^{(d)} \big].
\]
Then the following holds with probability at least $1- c d^{- c}$ for some constant $c \geq 1$
\begin{subequations}
\begin{align}
    &\| \bar{\bm{Z}}^{(i)} \|_2 = \sqrt{n},\;\forall \; i \in [d] \label{ineq1-concentration-prop-TS-SDP-hard} \\
    &| \langle \bar{\bm{Z}}^{(i)}, \bar{\bm{Z}}^{(j)} \rangle| \lesssim \sqrt{n},\; \forall \; i \neq j \in [d] \label{ineq2-concentration-prop-TS-SDP-hard}\\
    &\big| \|\bm{Z}^{(i)}\|_{2} - \sqrt{n} \big| \lesssim \log (dn), \;\forall \; i \in [d] \label{ineq3-concentration-prop-TS-SDP-hard} \quad \text{and} \\
    & \| \bar{\bm{Z}}^{\top} \bar{\bm{Z}} \|_{F}^{2} \geq (1-o(1)) \cdot nd^{2}.\label{ineq4-concentration-prop-TS-SDP-hard}
\end{align}
\end{subequations}
\end{lemma}
The inequalities~\eqref{ineq1-concentration-prop-TS-SDP-hard},~\eqref{ineq2-concentration-prop-TS-SDP-hard} and~\eqref{ineq4-concentration-prop-TS-SDP-hard} follows directly from Theorem 7.1 in \cite{ma2015sum}. The inequality~\eqref{ineq3-concentration-prop-TS-SDP-hard} follows from Theorem 3.1.1 in \cite{vershynin2018high} and applying union bound. We are now poised to prove Proposition~\ref{prop:TS-SDP-hard}.
\begin{proof}
    Let $\bm{M}_*$ be the optimal solution of the problem~\eqref{SDP-tree-sparse-PCA}. We will prove Proposition~\ref{prop:TS-SDP-hard} by contradiction. Assume that
    \begin{align}\label{assum1-proof-prop-TS-SDP-hard}
        \big\| \bm{M}_* - \bm{v}_*\bm{v}_{*}^{\top} \big\|_2 \leq \frac{1}{5},
    \end{align}
    where $\|\cdot\|_2$ denotes the spectral norm of matrices. By the triangle inequality, we obtain that
    \[
        \big\| \bm{M}_* \big\|_2 \geq \big \|\bm{v}_* \bm{v}_{*}^{\top} \big \|_2 - \big\| \bm{M}_* - \bm{v}_*\bm{v}_{*}^{\top} \big\|_2 \geq 1 - \frac{1}{5} = \frac{4}{5}.
    \]
    Let $\widehat{\bm{v}}$ be the eigenvector of $\bm{M}_*$ corresponding to the largest eigenvalue and $\|\widehat{\bm{v}}\|_{2}=1$. Then $\bm{M}_* - \frac{4}{5}\widehat{\bm{v}}\widehat{\bm{v}}^{\top}$ is positive semidefinite. We use the decomposition
    \[
        \bm{M}_* - \bm{v}_*\bm{v}_{*}^{\top} = \underbrace{\bm{M}_* - \frac{4}{5}\widehat{\bm{v}}\widehat{\bm{v}}^{\top} }_{\bm{M}_1} + \underbrace{\frac{4}{5} \big( \widehat{\bm{v}}\widehat{\bm{v}}^{\top} - \bm{v}_*\bm{v}_{*}^{\top}\big)}_{\bm{M}_2} - \frac{1}{5}\bm{v}_*\bm{v}_{*}^{\top}.
    \]
    Applying the triangle inequality, we obtain that
    \begin{align*}
        \big \| \bm{M}_2 \big\|_{2} &\leq \big\| \bm{M}_* - \bm{v}_*\bm{v}_{*}^{\top} \big\|_{2} + \big\| \bm{M}_1\big\|_{2} + \big \| \frac{1}{5}\bm{v}_*\bm{v}_{*}^{\top} \big\|_{2}
        \\ &\leq \frac{1}{5} + \mathsf{Tr}\big( \bm{M}_1 \big) + \frac{1}{5} \leq \frac{3}{5},
    \end{align*}
    where the last step follows since $\mathsf{Tr}\big( \bm{M}_1 \big) = \mathsf{Tr}\big( \bm{M}_* \big) - \frac{4}{5}\mathsf{Tr}\big(\widehat{\bm{v}}\widehat{\bm{v}}^{\top}\big) = 1/5$. Consequently, we obtain that
    \[
        \big \| \widehat{\bm{v}}\widehat{\bm{v}}^{\top} - \bm{v}_*\bm{v}_{*}^{\top} \big\|_{2} \leq \frac{3}{4}.
    \]
    It immediately follows that
    \begin{align}\label{lower-bound-alpha-proof-TSSDP}
        (\langle \widehat{\bm{v}}, \bm{v}_*\rangle)^{2} = & ~ 1 - \frac{1}{2}\cdot \big\| \widehat{\bm{v}}\widehat{\bm{v}}^{\top} - \bm{v}_*\bm{v}_{*}^{\top} \big\|_{F}^{2} \\ 
        \geq & ~ 1 - \frac{1}{2}\cdot 2\big\| \widehat{\bm{v}}\widehat{\bm{v}}^{\top} - \bm{v}_*\bm{v}_{*}^{\top} \big\|_{2}^{2} \geq \frac{7}{16}. \notag
    \end{align}
    Continuing, we use the decomposition
    \[
        \widehat{\bm{v}} = \alpha \bm{v}_* + \beta\bm{v}_{\perp},\quad \text{where }  \langle \bm{v}_*,\bm{v}_{\perp} \rangle = 0 \text{ and } \|\bm{v}_*\|_2 = \|\bm{v}_{\perp}\|_2 =1.
    \]
    Note that it follows from inequality~\eqref{lower-bound-alpha-proof-TSSDP} that $\alpha^{2} \geq 7/16$ and $\beta^{2}\leq 9/16$.
    We next claim that if $1\leq \lambda \leq d/Cn$ for a large enough constant $C$, then with probability at least $1- c_1 \exp(- c_1 n)$ for some positive constant $c_1$,
    \begin{align}\label{claim1-proof-prop-TS-SDP-hard}
            \bm{v}_{*}^{\top} \widehat{\bm{\Sigma}} \bm{v}_* \leq \frac{0.01d}{n} \quad \text{and} \quad \big\|  \widehat{\bm{\Sigma}} \big\|_{2}  \leq \frac{1.01d}{n}.
    \end{align}
    We defer the proof of Claim~\eqref{claim1-proof-prop-TS-SDP-hard} to the end. Using Claim~\eqref{claim1-proof-prop-TS-SDP-hard}, we obtain that
    \begin{align*}
         \| \widehat{\bm{v}} \|_{\widehat{\bm{\Sigma}}} \leq & ~ |\alpha| \cdot  \| \bm{v}_* \|_{\widehat{\bm{\Sigma}}} + |\beta| \cdot  \| \bm{v}_{\perp} \|_{\widehat{\bm{\Sigma}}} \\
         \leq & ~ 0.1\sqrt{d/n} + \frac{3}{4} \cdot \big\|  \widehat{\bm{\Sigma}} \big\|_{2}^{1/2} \leq (0.1 + 3\sqrt{1.01}/4) \cdot \sqrt{d/n}.
    \end{align*}
    Putting the pieces together yields 
    \begin{align}\label{upper-bound-optimal-SDP-TS}
    \begin{split}
            \mathsf{SDP}(\widehat{\bm{\Sigma}}) =  \big \langle \bm{M}_*, \widehat{\bm{\Sigma}}\big\rangle 
            = & ~ \big \langle \bm{M}_1, \widehat{\bm{\Sigma}}\big\rangle + \big \langle \frac{4}{5}\widehat{\bm{v}}\widehat{\bm{v}}^{\top} , \widehat{\bm{\Sigma}}\big\rangle \\ 
            \overset{\1}{\leq} & ~ \mathsf{Tr}(\bm{M}_1) \cdot \| \widehat{\bm{\Sigma}} \|_{2} + \frac{4}{5} \cdot \|\widehat{\bm{v}}\|_{\widehat{\bm{\Sigma}}}^{2} \\
            \leq & ~ \frac{1}{5} \cdot \frac{1.01d}{n} + \frac{4}{5} \cdot (0.1 + 3\sqrt{1.01}/4)^{2} \cdot \frac{d}{n} \leq 0.79 \cdot \frac{d}{n},
    \end{split}
    \end{align}
    where step $\1$ follows since $\bm{M}_1$ is positive semidefinite. We claim that with probability at least $1- c_2 (dn)^{-c_2}$ for some positive constant $c_2 \geq 1$, the optimal value $\mathsf{SDP}(\widehat{\bm{\Sigma}})$ in the optimization problem~\eqref{SDP-tree-sparse-PCA} satisfies
    \begin{align}\label{claim2-proof-prop-TS-SDP-hard}
        \mathsf{SDP}(\widehat{\bm{\Sigma}}) \geq \frac{0.99d}{n}.
    \end{align}
    We defer the proof of Claim~\eqref{claim2-proof-prop-TS-SDP-hard} to the end. Note that claim~\eqref{claim2-proof-prop-TS-SDP-hard} contradicts with the inequality~\eqref{upper-bound-optimal-SDP-TS}. Consequently, the assumption~\eqref{assum1-proof-prop-TS-SDP-hard} is wrong and then we conclude that
    \[
        \big\| \bm{M}_* - \bm{v}_*\bm{v}_{*}^{\top} \big\|_2 \geq \frac{1}{5}.
    \]
    \paragraph{Proof of Claim~\eqref{claim1-proof-prop-TS-SDP-hard}:} Recall that
    \[
        \widehat{\bm{\Sigma}} = \frac{1}{n}\sum_{i=1}^{n} \bm{x}_{i}\bm{x}_{i}^{\top}, \quad \text{where } \{\bm{x}_{i}\}_{i=1}^{n} \overset{\mathsf{i.i.d.}}{\sim} \mathcal{N}(\bm{0}, \bm{\Sigma}). 
    \]
    Let $\bm{z}_{i} = \bm{\Sigma}^{-1/2} \bm{x}_{i}$ for each $i \in [d]$. Some straightforward calculation yields
    \begin{align*}
        \bm{v}_{*}^{\top} \widehat{\bm{\Sigma}} \bm{v}_* = & ~ \frac{1}{n} \sum_{i=1}^{n} (\bm{z}_{i}^{\top} \bm{\Sigma}^{1/2} \bm{v}_{*})^{2} \\
        = & ~ \frac{\|\bm{\Sigma}^{1/2} \bm{v}_{*}\|_{2}^{2}}{n} \cdot \sum_{i=1}^{n} \bigg( \frac{\bm{z}_{i}^{\top} \bm{\Sigma}^{1/2} \bm{v}_{*}}{\|\bm{\Sigma}^{1/2} \bm{v}_{*}\|_{2}} \bigg)^{2} \\
        = & ~ \frac{\lambda+1}{n} \cdot \sum_{i=1}^{n} \bigg( \frac{\bm{z}_{i}^{\top} \bm{\Sigma}^{1/2} \bm{v}_{*}}{\|\bm{\Sigma}^{1/2} \bm{v}_{*}\|_{2}} \bigg)^{2}.
    \end{align*}
    Let $\bm{G} \sim \mathcal{N}(\bm{0},\bm{I}_{n\times n})$ be independent of data $\bm{X}$. Note that $\sum_{i=1}^{n} \bigg( \frac{\bm{z}_{i}^{\top} \bm{\Sigma}^{1/2} \bm{v}_{*}}{\|\bm{\Sigma}^{1/2} \bm{v}_{*}\|_{2}} \bigg)^{2}$ and $\|\bm{G}\|_{2}^{2}$ are identically and independently distributed. Using the concentration of $\|\bm{G}\|_{2}^{2}$, we obtain that with probability at least $1- c_1 \exp(- c_1 n)$ for some positive constant $c_1$,
    \[
        \sum_{i=1}^{n} \bigg( \frac{\bm{z}_{i}^{\top} \bm{\Sigma}^{1/2} \bm{v}_{*}}{\|\bm{\Sigma}^{1/2} \bm{v}_{*}\|_{2}} \bigg)^{2} \leq 2n.
    \]
    Consequently, for $1 \leq \lambda \leq d/(Cn)$, we obtain that with probability at least $1- c_1 \exp(- c_1 n)$ for some positive constant $c_1$,
    \[
        \bm{v}_{*}^{\top} \widehat{\bm{\Sigma}} \bm{v}_* \leq 2(\lambda+1) \leq 4\lambda \leq \frac{4d}{Cn} \leq \frac{0.01d}{n}.
    \]
    It follows from Proposition 5.3 of \cite{krauthgamer2015semidefinite} that if $1 \leq \lambda \leq d/(Cn)$ for a large enough $C$ then
    \[
        \| \widehat{\bm{\Sigma}} \|_{2} \leq \frac{1.01d}{n}.
    \]

    \paragraph{Proof of Claim~\eqref{claim2-proof-prop-TS-SDP-hard}:}
    Let $S = \mathsf{supp}(\bm{v}_*)$ be the support set of the ground truth $\bm{v}_*$ and let $T = [d] \setminus S$ be the complement of $S$. Without loss of generality, we assume $S = [k]$. Let $\bm{X}^{(i)}$ denotes the $i$-th column of the data matrix $\bm{X}$ for $i\in [d]$. Note that $\{ \bm{X}^{(i)}\}_{i \in T} \overset{i.i.d.}{\sim} \mathcal{N}(\bm{0},\bm{I}_{n\times n})$. Let
    \[
         \bar{\bm{X}}^{(i)} = \frac{\sqrt{n} \bm{X}^{(i)} }{\|\bm{X}^{(i)}\|_{2}},\;\forall \; i \in [d] \quad \text{and} \quad \bar{\bm{X}} = \big[ \bar{\bm{X}}^{(1)}\;|\cdots|\;\bar{\bm{X}}^{(d)} \big].
    \]
    Moreover, we use $\bm{X}_{T}$ and $\bar{\bm{X}}_{T}$ to denote the submatrices of $\bm{X}$ and $\bar{\bm{X}}$ by restricting to the columns indexed by $T$. We construct $\bm{M} \in \mathbb{R}^{d}$ such that
    \begin{align*}
        \bm{M}_{ij} = \left\{ \begin{array}{llll} \langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(i)} \rangle/(dn)&,\;\text{if}\; i=j\in [d]\\ 
        \langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(j)} \rangle/(dn)&,\;\text{if}\; i\neq j \in T \\
        0&,\;\text{else}  \end{array} \right. .
    \end{align*}
    We first verify that $\bm{M}$ is feasible for the optimization problem~\eqref{SDP-tree-sparse-PCA}. Note that
    \begin{align*}
        \sum_{i=1}^{d} \bm{M}_{ii} = & ~ \sum_{i=1}^{d} \langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(i)} \rangle/(dn) = \sum_{i=1}^{d} \| \bar{\bm{X}}^{(i)} \|_{2}^{2}/(dn) = \sum_{i=1}^{d} n/(dn) =1, \\
        \bm{M}_{ii} = & ~ \bm{M}_{\lfloor \frac{i}{2} \rfloor \lfloor \frac{i}{2} \rfloor},\; \forall\; i \in [d] \quad \text{ and} \\
        \sum_{i,j\in [d]} | \bm{M}_{ij} | = & ~ \sum_{i=1}^{d} |\bm{M}_{ii}| + \sum_{i \neq j\in T} | \bm{M}_{ij} | \\
        = & ~ 1 + \sum_{i \neq j\in T} | \langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(j)} \rangle|/(dn) \\
        \overset{\1}{\leq} & ~ 1 + Cd^{2} \cdot \sqrt{n} /(dn) = 1 + Cd/\sqrt{n} \leq k,\\
    \end{align*}
    where in step $\1$ we use $\langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(j)} \rangle \lesssim \sqrt{n}$ by Lemma~\ref{lemma:concentration-prop-TS-SDP-hard} and in the last step we use $d \asymp n$ and $n \leq ck^{2}$. Moreover, $\bm{M}$ is positive semidefinite since
    \[
        \bm{M} = \left[\begin{array}{cc} \frac{1}{dn}\bm{I}_{k \times k} & \bm{O} \\ \bm{0} & \frac{1}{dn} \bar{\bm{X}}_{T}^{\top}\bar{\bm{X}}_{T}  \end{array}\right].
    \]
    Consequently, $\bm{M}$ is a feasible solution of problem~\eqref{SDP-tree-sparse-PCA}. We next turn to obtain a lower bound of the objective value of $\bm{M}$. Note that
    \begin{align*}
        \sum_{i=1}^{d} \sum_{j=1}^{d} \widehat{\bm{\Sigma}}_{ij} \bm{M}_{ij} &= \sum_{i\in S} \widehat{\bm{\Sigma}}_{ii} \bm{M}_{ii} + \sum_{i,j\in T} \widehat{\bm{\Sigma}}_{ij} \bm{M}_{ij} \\
        & = \sum_{i\in S} \widehat{\bm{\Sigma}}_{ii} + \sum_{i,j\in T} \frac{\langle \bm{X}^{(i)}, \bm{X}^{(j)}\rangle}{n} \cdot \frac{\langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(j)} \rangle}{dn} \\
        & \geq \sum_{i,j\in T} \frac{ \| \bm{X}^{(i)}\|_2 \| \bm{X}^{(j)} \|_2}{n^2} \cdot \frac{ \big( \langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(j)} \rangle \big)^{2}}{dn} \\
        & \overset{\1}{\geq} \frac{(\sqrt{n} - C\log (dn))^{2}}{dn^{3}} \cdot \sum_{i,j\in T} \big( \langle \bar{\bm{X}}^{(i)}, \bar{\bm{X}}^{(j)} \rangle \big)^{2} \\
        & = \frac{(\sqrt{n} - C\log n)^{2}}{dn^{3}} \cdot \| \bar{\bm{X}}_{T}^{\top} \bar{\bm{X}}_{T} \|_{F}^{2} \\
        & \overset{\2}{\geq} \frac{(\sqrt{n} - C\log (dn))^{2}}{dn^{3}} \cdot (1-o(1)) \cdot  n(d-k)^{2} \\
        & = \frac{d}{n} \cdot \big(1-\frac{C\log (dn)}{\sqrt{n}} \big) \cdot (1-o(1)) \cdot (1-\frac{k}{d})^{2},
    \end{align*}
    where in steps $\1$ and $\2$ we use the result of Lemma~\ref{lemma:concentration-prop-TS-SDP-hard} that
	\begin{align*}
		& ~ \|\bm{X}^{(i)}\|_2 \geq \sqrt{n} - C\log n, \; \forall \; i \in T \\
        \quad \text{and} & ~ \| \bar{\bm{X}}_{T}^{\top} \bar{\bm{X}}_{T} \|_{F}^{2} \geq (1-o(1))n(d-k)^{2}
	\end{align*}
    holds with probability at least $1- c_2 (dn)^{-c_2}$ for some positive constant $c_2 \geq 1$. 
    Consequently,
	\begin{align*}
		\mathsf{SDP}(\widehat{\bm{\Sigma}}) \geq & ~ \sum_{i=1}^{d} \sum_{j=1}^{d} \widehat{\bm{\Sigma}}_{ij} \bm{M}_{ij} \\
		\geq & ~ \frac{d}{n} \cdot \big(1-\frac{C\log n}{\sqrt{n}} \big) \cdot (1-o(1)) \cdot (1-\frac{k}{d})^{2} \\
		\geq & ~ \frac{0.99d}{n}.
	\end{align*}
\end{proof}





