%!TEX root = IEEETSP-main-paper.tex

\subsection{A locally convergent projected power method} \label{sec:PPM-EM} 

In Section~\ref{sec:fund-limits}, we studied the fundamental limits of the problem, where our upper bounds were achieved by the exhaustive search estimator $\ESest$.
%\apcomment{Please give this a name. Possibly $\widehat{\bm{v}}_{\mathsf{ES}}$ for exhaustive search. Also use a macro for it so it is easy to type out.} \gwcomment{Done.} for exhaustive search.
Given the computational challenge of searching over every linear subspace $L \in \mathcal{L}$, we propose the following iterative projected power method (Algorithm~\ref{alg:PPM}) and show that with access to a suitable exact projection oracle, it locally converges to a statistical neighborhood of the ground truth.


\begin{definition}[Exact projection] \label{assump:exact-proj}
For all $\bm{v} \in \mathbb{R}^d$, let 
\[ \Pi_{\mathcal{M}} (\bm{v}) := \argmin_{\bm{v}' \in \mathcal{M}}\; \|\bm{v}' - \bm{v}\|_2 = \argmin_{\bm{v}' \in L_{m},m \in [M]} \; \| \bm{v}' - \bm{v}\|_2,
\]
where ties between subspaces are broken lexicographically.
\end{definition}
Owing to the tie-breaking rule, this projection is always unique. As we will see in Section~\ref{sec:specific-examples}, an exact projection oracle $\Pi_{\mathcal{M}}$ can be constructed efficiently (in time nearly logarithmic in $M$) in some specific examples of union-of-linearly structured PCA. 
% \apcomment{Breaking ties lexicographically removes the need for any of the older additional remarks.}
%
\iffalse
\begin{remark}
For any $\bm{v} \in \mathbb{R}^d$, let $L_{\Pi} \in \mathcal{L}$ be the linear subspace that contains $\Pi_{\mathcal{M}}(\bm{v})$. When the linear subspace $L_{\Pi} \in \mathcal{L}$ is known, the exact non-convex projection $\Pi_{\mathcal{M}}$ is equivalent to the classical Euclidean projection onto $L_{\Pi}$, i.e., $\Pi_{\mathcal{M}} (\bm{v}) = \argmin_{\bm{\theta} \in L_{\Pi} } \|\bm{\theta} - \bm{v}\|_2.$ 
\end{remark} 
\begin{remark}
\textbf{Uniqueness of the Projection $\Pi_{\mathcal{M}}$.}
To ensure the uniqueness of $\Pi_{\mathcal{M}}$, when there are two linear subspace $L_{m_1} \neq L_{m_2} \in \mathcal{L}$ with index $m_1 < m_2$ that satisfy
\begin{align}
    \min_{\bm{u}_{m_1} \in L_{m_1}} \|\bm{u}_{m_1} - \bm{v}\|_2 = \min_{\bm{u}_{m_2} \in L_{m_2}} \|\bm{u}_{m_2} - \bm{v}\|_2 \leq \argmin_{\bm{u}_{m} \in L_{m}} \|\bm{u}_{m} - \bm{v}\|_2  ~~ \forall m \in [M], \notag 
\end{align}
the exact projection $\Pi_{\mathcal{M}}$ projects $\bm{v}$ onto the linear subspace with smaller index $m$, i.e., in this case, $\Pi_{\mathcal{M}}$ projects $\bm{v}$ onto the linear subspace $L_{m_1}$. 
\end{remark}
\fi
%
We are now in a position to present the projected power method, described formally in Algorithm~\ref{alg:PPM}.


\begin{algorithm}
\caption{Projected Power Method}
\label{alg:PPM}
\textbf{Input:} Sample covariance matrix $\widehat{\bm{\Sigma}}$.
\begin{algorithmic}[1]
\State \textbf{Initialize} with a vector $\bm{v}_0 \in \mathcal{M} \cap \mathcal{S}^{d - 1}$. 
\For{$t = 0, 1, \ldots, T - 1$}
\State Compute $\tilde{\bm{v}}_{t + 1} = \widehat{\bm{\Sigma}} \bm{v}_t / \big\|\widehat{\bm{\Sigma}} \bm{v}_t \big\|_2$. \label{alg:PPM-compute}
\State Project $\bm{v}_{t + 1}^{\mathcal{M}} = \Pi_{\mathcal{M} }(\tilde{\bm{v}}_{t + 1})$. 
% \apcomment{Split this into two steps, one of which explicitly uses the projection onto $\mathcal{M}$. Define additional vector if necessary.} \gwcomment{Got it.} 
\label{alg:PPM-project}
\State Normalize to unit sphere $\bm{v}_{t + 1} = \frac{ \bm{v}_{t + 1}^{\mathcal{M}} }{ \|\bm{v}_{t + 1}^{\mathcal{M}} \|_2} \in \mathcal{M} \cap \mathcal{S}^{d - 1}$. 
\EndFor
\end{algorithmic}
\textbf{Output:} $\bm{v}_T$.  
\end{algorithm} 

\iffalse
\begin{remark}
Notice that, the projection used in step~\eqref{alg:PPM-project} of Algorithm~\ref{alg:PPM} is $\Pi_{\mathcal{M} \cap \mathcal{S}^{d - 1}}: \mathcal{S}^{d - 1} \mapsto \mathcal{M} \cap \mathcal{S}^{d - 1}$, which exactly projects any unit vector onto the intersection of the non-convex set $\mathcal{M}$ and unit $\ell_2$-ball, i.e., for any $\bm{v} \in \mathcal{S}^{d - 1}$, 
\begin{align}
    \Pi_{\mathcal{M} \cap \mathcal{S}^{d - 1}} (\bm{v}) := \argmin_{\bm{\theta} \in \mathcal{M} \cap \mathcal{S}^{d - 1}} \|\bm{\theta} - \bm{v}\|_2. \notag
\end{align}
In Appendix~\ref{app:prop-decom-covar}, we show that such projection $\Pi_{\mathcal{M} \cap \mathcal{S}^{d - 1}}$ used in the projected power method (Algorithm~\ref{alg:PPM}) can be constructed using the given exact projection $\Pi_{\mathcal{M}}$. 
\end{remark}

\begin{remark}
\textbf{Uniqueness of the Projection $\Pi_{\mathcal{M} \cap \mathcal{S}^{d - 1}}$.} The uniqueness of $\Pi_{\mathcal{M}}$ ensures the uniqueness of $\Pi_{\mathcal{M} \cap \mathcal{S}^{d - 1}}$. 
\end{remark}
\fi

\iffalse
%We then introduce notations for Algorithm~\ref{alg:PPM} and Theorem~\ref{thm:convergence}. \apcomment{You haven't yet defined the algorithm. Weird to give notation for it.} \gwcomment{Have moved to current position.}  Given $T$ the total number of iterations of Algorithm~\ref{alg:PPM}, we use $\bm{v}_{t + 1}$ to denote the solution obtained in $t$-th iteration. For $t = 0, 1, \ldots, T - 1$, define
Let $L^{(t)} \in \mathcal{L}$ denote the linear subspace that contains $\bm{v}_t$ (with ties broken lexicographically), and define the linear subspace $F^{(t)} := \text{conv}(L^{(t)} \cup L^{(t + 1)} \cup L_*)$ as the convex hull of $L^{(t)}, L^{(t + 1)}$ and $L_*$.
%, where $L_* \in \mathcal{L}$ is the linear subspace that contains the ground truth $\bm{v}_*$. 
Decomposing the vector $\bm{v}_t$ over the subspace $F^{(t)}$ and its orthonormal complement, as %$\widehat{\bm{v}}_{F^{(t)}}$ and its orthonormal direction $\widehat{\bm{v}}_{\bot}$, we have 
$\bm{v}_t = \alpha_t \widehat{\bm{v}}_{F^{(t)}} + \beta_t \widehat{\bm{v}}_{\bot}$, where $\widehat{\bm{v}}_{F^{(t)}}$ and $\widehat{\bm{v}}_{\bot}$ are unit norm vectors and 
\[
\alpha_t := \langle \bm{v}_t, \widehat{\bm{v}}_{F^{(t)}}\rangle, \quad \beta_t := \langle \bm{v}_t, \widehat{\bm{v}}_{\bot}\rangle, , \text{ with } \alpha_t^2 + \beta_t^2 = 1.
\]
We use the shorthand $\lambda_i:= \lambda_i(\bm{\Sigma})$, $\widehat{\lambda}_i := \lambda_i(\widehat{\bm{\Sigma}})$, and $\widehat{\lambda}_{i}^{F^{(t)}} := \lambda_{i}(\widehat{\bm{\Sigma}}_{F^{(t)}})$. Let $\kappa := \lambda_2 / \lambda_1 = 1 / (1 + \lambda)$ denote the signal-to-noise ratio of the true covariance matrix $\bm{\Sigma}$ and $\widehat{\kappa}_{F^{(t)}} := \widehat{\lambda}_2^{F^{(t)}} / \widehat{\lambda}_{\max}^{F^{(t)}}$ denote the analogous quantity for the sample covariance matrix restricted to the linear subspace $F^{(t)}$. Using the notation $\rho(\bm{M}, S)$ and $\rho(\bm{M}, \mathcal{M})$ from Eq.~\eqref{eqs:rho}, define
\begin{align*}
    F^* := \argmax_F \rho(\bm{W}, {F}) ~~\text{s.t.}~~ F = \text{conv}(L_{m_1} \cup L_{m_2} \cup L_{m_3}), ~ \forall ~ m_1 \neq m_2 \neq m_3 \in [M].
\end{align*}  
\apcomment{Don't use align with notag. align* should be used in such cases.} \gwcomment{Got it, modified.}

Let 
\begin{align*}
    c_1(\lambda) := \lambda - 2 \rho(\bm{W}, F^*), \;\;\text{ and }  \;\; c_2(\lambda) \in \left(0, ~ \left(\frac{1}{2} - \frac{\rho(\bm{W}, F^*)}{2 c_1(\lambda)}\right) \frac{1 + \lambda - \rho(\bm{W}, {F^*})}{1 + \rho(\bm{W}, {F^*})} - 1 \right), 
\end{align*}
be two strictly positive parameters, i.e., 
\begin{align*}
    \lambda > \max\left\{2 \rho(\bm{W}, F^*), ~ 2 \frac{1 + \rho(\bm{W}, F^*)}{1 - \rho(\bm{W}, F^*)/c_1(\lambda)} + \rho(\bm{W}, F^*) - 1\right\}, 
\end{align*}
and define for convenience the scalars
\begin{align*}
    \alpha_* : = 2 \cdot (1 + c_2(\lambda)) \cdot \frac{\lambda_2(\bm{\Sigma}) + \rho(\bm{W}, {F^*})}{\lambda_1(\bm{\Sigma}) - \rho(\bm{W}, {F^*})}, \quad 
\mu^* := \frac{2}{\alpha^*} \frac{\lambda_2(\bm{\Sigma}) + \rho(\bm{W}, {F^*})}{\lambda_1(\bm{\Sigma}) - \rho(\bm{W}, {F^*})}, \;\;\text{ and }  \;\;
\epsilon^* := \frac{3 \rho(\bm{W}, F^{*})}{c_1(\lambda)}.
\end{align*}

\begin{theorem} \label{thm:convergence}
Using the notations above, suppose that $c_1(\lambda)$ and $c_2(\lambda)$ are strictly positive, thereby ensuring that $0 < \alpha_* < 1 - \frac{\rho(\bm{W}, {F^*})}{c_1(\lambda)}$ and $\mu^* < \frac{1}{1 + c_2(\lambda)}$. Suppose Algorithm~\ref{alg:PPM-equ} is run from an initialization $\bm{v}_0 \in \mathcal{M} \cap \mathcal{S}^{d - 1}$ satisfying $\langle \bm{v}_0, \bm{v}_* \rangle \geq \alpha_* + \frac{\rho(\bm{W}, {F^*})}{c_1(\lambda)}$. Then either $\|\bm{v}_* - \bm{v}_0 \|_2 \leq \frac{\epsilon^*}{1 - \mu^*}$, or for all $t \geq 0$, we have
\begin{align}
    \left\|\bm{v}_* - \bm{v}_{t}\right\|_2 \leq & ~ (\mu^*)^t \left\|\bm{v}_* - \bm{v}_{0}\right\|_2 + \frac{\epsilon^*}{1 - \mu^*}.
\end{align}
\end{theorem}
\noindent The proof of Theorem~\ref{thm:convergence} can be found in Appendix~\ref{app:PPM-EM}. 
\fi




Using the notation $\rho(\bm{M}, S)$ from Eq.~\eqref{eqs:rho}, we define 
$
F^{*} := \argmax_{F} \rho\big(\bm{W}, F \big)\;\text{s.t.}\; F = \text{conv}\big(L_{m_1} \cup  L_{m_2} \cup L_{m_3} \big),\; \forall \;  m_1,m_2,m_3 \in [M].
$
We now state the definition of a ``good region"; Theorem~\ref{thm:convergence} to follow shows that once Algorithm~\ref{alg:PPM} is initialized in this region, it will converge geometrically to a neighborhood of the ground truth $\bm{v}_*$.
\begin{definition}[Good region]\label{good-region}
For eigen gap $\lambda > 2 \rho(\bm{W}, F^{*})$, we define the good region 
\begin{align*} 
&\mathbb{G}(\lambda) = \big\{ \bm{v} \in \mathcal{M} \cap \mathcal{S}^{d - 1}: \langle \bm{v}, \bm{v}_{*} \rangle \geq t_1(\lambda) \big\},\quad \text{where}
\\& \quad t_1(\lambda):= \frac{4}{\lambda + 1 - \rho(\bm{W}, F^{*})} + \frac{5\rho(\bm{W}, F^{*})}{\lambda - 2\rho(\bm{W}, F^{*})}.
\end{align*}
\end{definition}
\noindent Note that $\mathbb{G}(\lambda)$ becomes a larger set as $\lambda$ increases. To ensure that such a good region is non-empty, it is necessary to have $t_1(\lambda) < 1$. In our proof of convergence (see Appendix~\ref{app:PPM-EM}), we require that the good region is not just non-empty but large enough. In particular, we require the eigengap $\lambda$ to be large enough so that
\begin{align}\label{eigen-gap-condition}
     t_2(\lambda) := \frac{4}{\lambda + 1 - \rho(\bm{W}, F^{*})} + \frac{10\rho(\bm{W}, F^{*})}{\lambda - 2\rho(\bm{W}, F^{*})} < 1. 
\end{align}
Note that this automatically ensures that $t_1(\lambda) < 1$ since $t_{2}(\lambda) > t_1(\lambda)$. 
%{\color{orange}These quantities are chosen to ensure the validitiness of our convergence result, see proof~\ref{app:PPM-EM} for details.}
%{\color{orange}Note that the second term of Inequality~\eqref{eigen-gap-condition} is slightly greater than the second term of the lower bound of $\langle \bm{v}_0, \bm{v}_{*} \rangle$ in Good region. It is designed to ensure the validness of our convergence result, see proof~\ref{app:PPM-EM} for details.}
We are now poised to state our main result for this subsection.
\begin{theorem} \label{thm:convergence}
Suppose the eigengap satisfies $\lambda > 2 \rho(\bm{W}, F^{*})$, and condition~\eqref{eigen-gap-condition} holds. Suppose in Algorithm~\ref{alg:PPM-project} the initialization satisfies $\bm{v}_{0} \in \mathbb{G}(\lambda)$. Then for all $t\geq 1$, we have
\begin{align}\label{ineq-thm-convergence}
    \| \bm{v}_{t+1} - \bm{v}_{*} \|_2 \leq \frac{1}{2^{t}} \cdot \| \bm{v}_{0} - \bm{v}_{*} \|_2 + \frac{6\rho(\bm{W}, F^{*})}{\lambda - 2\rho(\bm{W}, F^{*})}.
\end{align}

\end{theorem}
%\noindent The proof of Theorem~\ref{thm:convergence} can be found in Section~\ref{app:PPM-EM}.

 

The proof of Theorem~\ref{thm:convergence} can be found in Section~\ref{app:PPM-EM}. Once (a) an exact projection oracle $\Pi_{\mathcal{M}}$ is accessible; and (b) an initial vector $\bm{v}_0$ is in the good region $\mathbb{G}(\lambda)$, Theorem~\ref{thm:convergence} ensures a deterministic convergence result. 
%{\color{red}once the noise $\rho(\bm{W}, F^{*})$ is given.} 
Note that the result parallels that of~\cite{yuan2013truncated} for vanilla sparse PCA, where $\rho(\bm{W}, F^{*}) = O(\sqrt{k\log d / n})$. The key additional technique that we use to control the error accumulated at each iteration is based on an ``equivalent replacement'' step; see Section~\ref{proof-lemma-equivalence-of-v}. 
%which follows the framework of local convergence presented in \citet{yuan2013truncated}. The key difference is a so-called ``equivalent replacement'' step that is used to control the noise involved in each iteration, see Section~\ref{app:PPM-EM} for details.  

The projected power method was also recently analyzed by~\citet{liu2021generative} for PCA with generative models when given access to an exact projection oracle.
%. . While there may be some similarities between our work and theirs, such as the use of projection oracle and 
While they also proved local geometric convergence results given access to a sufficiently correlated initialization, there are significant differences in the assumptions of that paper and our own. First, our work imposes the union-of-linear structure assumption on the principal component, which is an altogether different structural assumption from a generative model. Given this, our proof techniques differ significantly from those of~\citet{liu2021generative}. Second, %and we provide fundamental limits of estimation for this problem. Additionally, our work 
we present a computationally efficient initialization method and matching evidence of computational hardness for two prototypical examples; see below. %Let us now turn to proving these results. 

%two prototypical examples, including exact projection oracles, initialization methods, and hardness results, which complement the theoretical results.

\iffalse
Theorem~\ref{thm:convergence} is a deterministic convergence result.
In the case where $c_1(\lambda)$ and $c_2(\lambda)$ are further bounded below by universal constants, it shows that a natural variant of power method guarantees local geometric convergence to the ground truth parameter $\bm{v}_*$ within a neighborhood of radius $\epsilon^* = \mathcal{O}(\rho(\bm{W}, F^*))$. This result relies on (1) an exact projection oracle $\Pi_{\mathcal{M}}$ being accessible; and (2) an initial vector $\bm{v}_0 \in \mathcal{M} \cap \mathcal{S}^{d - 1}$ that satisfies the initialization condition $\langle \bm{v}_0, \bm{v}_* \rangle \geq \alpha_* + \frac{\rho(\bm{W}, {F^*})}{\lambda - 2 \rho(\bm{W}, {F^*})}$. 
%Here, we only state the convergence result concerning the general restricted statistical error $\rho(\bm{W}, F^*)$. As we will see in Section~\ref{sec:specific-examples}, this general error $\rho(\bm{W}, F^*)$ will be replaced by some specific non-asymptotic upper bounds under tree/path sparsity samples, respectively. For people with independent interests, 
The proof of Theorem~\ref{thm:convergence}, follows the framework of local convergence in \citet{yuan2013truncated}. The key difference is a so-called ``equivalent replacement'' step that is used to control the noise involved in each iteration, see Appendix~\ref{app:prop-decom-covar} for details.
% In comparison with the local convergence result of sparse PCA \citep{yuan2013truncated}, the initialization condition and the local geometric convergence are of a similar format. The only difference is the requirement of the exact projection oracle $\Pi_{\mathcal{M}}$ which is accessible in sparse PCA.  \\
% The projected power method was also studied in recent work for generative PCA by \cite{liu2021generative}, under a different structural assumption on the principal component. 
\apcomment{Please rewrite the following text; it does not read well.}
{\color{orange}
A recent work from \cite{liu2021generative} studies the similar projected power method for the so-called generative PCA problem. In the present paper, the structural assumption we placed on the principal component is distinct from theirs, and the proof techniques we used is also different. We further give the fundamental limits of estimation for union-of-linearly structured PCA, which are not covered in \cite{liu2021generative}. Moreover, end-to-end analysis (which includes efficient initialization methods and hardness results) is proposed for two prototypical examples. In contrast, the initial oracle assumed in \cite{liu2021generative} may not be easily implementable under their assumptions for generative  PCA.  
}
\fi

% In particular, [Assumption 2, \cite{liu2021generative}] is not required in the linear structure condition; (2) and linear subspaces in our setting have different dimensions, unlike their feasible set $G(\mathbb{B}^k_2(1)) = \{G(\bm{x}) ~|~ \bm{x} \in \mathbb{B}^k_2(1)\}$ which is intrinsic $k$-dimensional with $\mathbb{B}^k_2(1) = \{\bm{x} \in \mathbb{R}^k ~|~ \|\bm{x}\|_2 \leq 1\}$. Although the local geometric convergence result in \cite{liu2021generative} for the projected power method is similar, the proof techniques are different. Besides the above algorithmic results, we also give a general form of minimax lower bounds for the linearly structured PCA (see Section~\ref{sec:fund-limits}), study the computational hardness (instead of the information-theoretical lower bound) of two prototypical examples, and propose initialization methods to complete the story (see Section~\ref{sec:specific-examples}). 

%Recall that Theorem~\ref{thm:convergence} requires an initialization $\bm{v}_0$ in the good region $\mathbb{G}(\lambda)$. To complete the story, we provide such an initialization method (see Algorithm~\ref{alg:initialization} in Section~\ref{sec:initial-method-general}) that works when given a projection oracle, with an additional assumption holds true. \mqcomment{I think we can move the initialization result to the main text.}

\subsection{Initialization method} \label{sec:initial-method-general}

Recall that Theorem~\ref{thm:convergence} requires an initialization $\bm{v}_0$ in the good region $\mathbb{G}(\lambda)$. In this subsection, we provide such an initialization method (see Algorithm~\ref{alg:initialization}) that works when given a projection oracle, provided the following assumption holds. 
%\apcomment{Just making sure: We don't need any independence assumption between $v_0$ and the data?}

%whose output ensures the required initialization condition of $\bm{v}_0$.

% \begin{assumption} \label{assump:M-set-initial}
% Suppose the linear structure set $\mathcal{M}$ can be represented as $\mathcal{M} = \{\bm{v} \in \mathbb{R}^d ~|~ \|\bm{v}\|_0 = k\} \cap \{\textup{additional-structure}\}$, i.e., an intersection between set of $k$-sparsity vectors and some additional structure constraints. 
% \end{assumption}

\begin{assumption}\label{assump:M-set-initial}
The set $\mathcal{M}$ in  satisfies
\[
    \mathcal{M} \subseteq \{\bm{v} \in \mathbb{R}^{d} : \|\bm{v}\|_{0} = k\},\quad \text{where} \quad k \in \mathbb{N}.
\]
\end{assumption}


Assumption~\ref{assump:M-set-initial} is not guaranteed by Definition~\ref{cond:linear-structure}, but includes many typical examples. For instance, the sets $\mathcal{T}^k$ and $\mathcal{P}^k$ for tree-sparse or path-sparse PCA, respectively, satisfy Assumption~\ref{assump:M-set-initial} in addition to union-of-linear structure. Moreover, if the orthonormal matrix $\bm{\Phi}$ is known, one can reformulate the problem as  estimating the structured-sparse vector $\bm{\Phi}^{\top} \bm{v}_{*}$ from observations $\{\bm{\Phi}^{\top} \bm{x}_i\}_{i = 1}^n$ (see Remark~\ref{rem:eq-structured}).

%\apcomment{Fill in once the previous discussion about structured sparsity is made into a remark.}\gwcomment{Got it.}
% In this part, we propose an initialization method (Covariance Thresholding with Projection, see Algorithm~\ref{alg:initialization}) for the projected power method (Algorithm~\ref{alg:PPM}). 

% We start with the notations used in this subsection. Let $C_1, C_2, C_3 > 0$ be three numerical constants. Define $\tau_* := C_1 \max\{\lambda, ~ 1\} \sqrt{\log (d/k^2)}$. Set the thresholding parameter
% \begin{align*}
%     \tau := \left\{
%     \begin{array}{lll}
%         \tau_* & \textup{when  } \tau_* \leq \sqrt{\log d} / 2, ~ k^2 \leq d / e, \\
%         C_2 \tau_* & \textup{when  } \tau_* \geq \sqrt{\log d} / 2, ~ k^2 \leq d / e, \\
%         0 & \textup{otherwise.}
%     \end{array}
%     \right. ~~ . 
% \end{align*}
% Recall the set $F^*$, strict positive parameters $c_1(\lambda)$, $c_2(\lambda)$, and parameters $\alpha_*$, $\mu^* < 1 / (1 + c_2(\lambda))$, $\epsilon^*$ given in Section~\ref{sec:PPM-EM}.

\begin{algorithm}
\caption{Initialization Method -- Covariance Thresholding with Projection Oracle}
\label{alg:initialization}
\textbf{Input.} $\{\bm{x}_i\}_{i = 1}^{n}$, parameter $k \in \mathbb{N}$, thresholding parameter $\tau$ and exact projection $\Pi_{\mathcal{M}}$. 
\begin{algorithmic}[1]
\State Compute covariance matrix $\widehat{\bm{\Sigma}} = \sum_{i = 1}^n \bm{x}_i \bm{x}_i^{\top} / n$. 
\State Set the soft-thresholding matrix $\widehat{\bm{G}}(\tau)$ as:
\begin{align*}
&\text{If} \quad \quad~~~ \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} \geq \tau/\sqrt{n}, \quad ~~ \text{then} \quad [\widehat{\bm{G}}(\tau)]_{ij} = \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} - \tau/\sqrt{n};\\
&\text{else if} \quad \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} \leq -\tau/\sqrt{n}, \quad \text{then} \quad [\widehat{\bm{G}}(\tau)]_{ij} = \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} + \tau/\sqrt{n};
\\&\text{else} \quad \quad [\widehat{\bm{G}}(\tau)]_{ij} = 0.
\end{align*}
% \begin{align*}
%     [\widehat{\bm{G}}(\tau)]_{ij} := \left\{
%     \begin{array}{llll}
%         \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} - \tau/\sqrt{n} \\
%         \text{~ if } ~ \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} \geq \tau/\sqrt{n}, \\
%         0 \\
%         \text{~ if } ~ \left| \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} \right| < \tau / \sqrt{n}, \\
%         \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} + \tau/\sqrt{n} \\
%         \text{~ if } ~ \widehat{\bm{\Sigma}}_{ij} - [\bm{I}_d]_{ij} \leq - \tau/\sqrt{n}, 
%     \end{array}
%     \right. ~~ .
% \end{align*}
\State Compute  $\widehat{\bm{v}}_{\textup{soft}} := \max_{\|\bm{v}\|_2 = 1} \bm{v}^{\top} \widehat{\bm{G}}(\tau) \bm{v}$ as the leading eigenvector of $\widehat{\bm{G}}(\tau)$.
\State Project $\bm{v}_0 :=  \Pi_{\mathcal{M}}(\widehat{\bm{v}}_{\textup{soft}}) / \|\Pi_{\mathcal{M}}(\widehat{\bm{v}}_{\textup{soft}})\|_2$.
\end{algorithmic}
\textbf{Return} $\bm{v}_0 \in \mathcal{S}^{d - 1} \cap \mathcal{M}$. 
\end{algorithm}  
 
% Thus the output $\bm{v}_0 = \Pi_{\mathcal{M}}(\widehat{\bm{v}}_{\textup{soft}})$ of Algorithm~\ref{alg:initialization} satisfies the following theorem. 

\begin{theorem} \label{thm:initialization-method}
Suppose Assumption~\ref{assump:M-set-initial} holds and $k^2 \leq d/e$. There exists a tuple of universal, positive constants $(C_1,C_2,C_3,C)$ such that the following holds. Suppose $n\geq \max\{C\log d,k^{2}\}$ and let $\tau_{*} := C_1 \max\{\lambda, 1\} \sqrt{\log (d/k^2)}$. Set the thresholding level according to 
\begin{align}\label{initia-threshold-tau}
    \tau := \left\{
    \begin{array}{lll}
        \tau_* & \textup{when  } \tau_* \leq \sqrt{\log d} / 2, \\
        C_2 \tau_* & \textup{when  } \tau_* \geq \sqrt{\log d} / 2, \\
        0 & \textup{otherwise.}
    \end{array}
    \right. 
\end{align}
Then for any $0<c_{0} <1$, if
\begin{align*}
    n \geq  n_0(c_0) := \frac{18 C_3 \max\{\lambda^2, ~ 1\} k^2}{2(1 - c_0)^2 \lambda^2} \log(d/k^2),
\end{align*}
then the initial vector $\bm{v}_0 \in \mathcal{S}^{d - 1} \cap \mathcal{M}$ obtained from Algorithm~\ref{alg:initialization} satisfies $\langle \bm{v}_0, \bm{v}_* \rangle \geq c_0$ with probability $1 - C'\exp(- \min\{\sqrt{d}, n\}/C')$ for some positive constant $C'$.

%\gwcomment{Here the contraintuitive behavior of term $d$ (as discussed below) is due to the setting of $\tau_* = O(\sqrt{\log (d/k)}) \leq \sqrt{\log d} / 2$. Based on such setting, as $d$ increasing, the RHS probability bound in [formulation (104), Covariance Thresholding] decreases due to a Chernoff-Hoeffding type bound, which will be used to control [Prop 13, Covariance Thresholding].} \\ 
%
%\gwcomment{To Ashwin: In the paper Covariance Thresholding for SPCA, this error probability $o(1)$ is of the form $C'\exp(- \min\{\sqrt{d}, n\}/C')$. However, we find that such error probability contradicts to our intuition. In particular, the error probability $C'\exp(- \min\{\sqrt{d}, n\}/C')$ increases as the dimension $d$ decreases (when $n^2 \geq d \geq n \geq k^2$), i.e., the lower dimension $d$, the easier estimation task for SPCA.} 
\end{theorem}

%\gwcomment{Write carefully the event $\bm{v}_0$ are dependent on the data $\bm{X}$, and $t_2(\lambda) < \frac{1}{2}$ (some constant) with high prob.}\\ 
%\gwcomment{Now written in the following logic: (1) Set $c_0 = 7/8$. (2) Say Theorem 3 shows $\langle \bm{v}_0, \bm{v}_{*} \rangle \geq 7/8$ whp. (3) Show that $7/8 > t_2(\lambda)$ whp. (4) $v_0$ is in good region.}

The proof of Theorem~\ref{thm:initialization-method}, which builds on existing results in \cite{deshpande2016sparse}, can be found in Appendix~\ref{app:initialization}. Let us now show that the output of this algorithm serves as a valid initialization for the projected power method, since this is not immediate given that the event $\mathcal{E}_1 = \{\langle \bm{v}_0, \bm{v}_{*} \rangle \geq c_0\}$  depends on the samples $\{\bm{x}_i\}_{i = 1}^n$.
%
%
%Observe that for any $0 < c_0 < 1$, the event $\{\langle \bm{v}_0, \bm{v}_{*} \rangle \geq c_0\}$  depends on the sample $\{\bm{x}_i\}_{i = 1}^n$. 
Recall the quantities $t_1(\lambda)$ and $t_2(\lambda)$ in Definition~\ref{good-region} and Eq.~\eqref{eigen-gap-condition}, respectively. 
In Theorem~\ref{thm:initialization-method}, set $c_0 := t_2(\lambda)$ and recall that $t_2(\lambda) > t_2(\lambda)$ by definition. Suppose $\lambda \geq 5$ for convenience. Then it can be shown that the event $\mathcal{E}_2 = \{ t_1(\lambda) < t_2(\lambda) = c_0 < 7/8\} \subseteq \{\rho(\bm{W}, F^*) < 9/400\}$ occurs with probability at least $1 - C' \exp(- \min\{\sqrt{d}, n\} / C')$. 
%Here, this probabilistic bound is a trivial corollary based on the similar noise upper bound for vanilla sparse PCA, as also mentioned in Remark 6.6 of \citet{deshpande2016sparse}. 
%
%\mqcomment{Why this event happen with high probability? Can we cite the proof?}
%
%and suppose that $\lambda \geq 5$ and $n \geq n_0(7/8)$. Then %some calculations show that with high probability \apcomment{How high?}, we have that 
%the event
%%Setting $c_0 = 7/8$ and thus the following event for $t_2(\lambda)$
%%\begin{align} \label{eq:c_0-upper-bound}
% $   \mathcal{E}_2 = \{ t_1(\lambda) <  7/8\}$
%%\end{align}
%occurs with high probability 
%holds with high probability when $\lambda \geq 5$ and $n \geq n_0(7/8)$.
Consequently, on the high probability event $\mathcal{E}_1 \cap \mathcal{E}_2$, we have that the initialization $\bm{v}_{0}$ obtained by Algorithm~\ref{alg:initialization} satisfies $\bm{v}_0 \in \mathbb{G}(\lambda)$. The projected power method can thus be employed after this initialization to guarantee convergence to a small neighborhood of $\bm{v}_*$.

%the good region condition. {\color{orange}Although t

A key feature of Theorem~\ref{thm:initialization-method} is the lower bound $n_0 = \Theta(k^2 \log(d / k^2) )$ on the number of samples required for the Algorithm~\ref{thm:initialization-method} to succeed. 
Note that this is of a strictly %in Theorem~\ref{thm:initialization-method} is 
larger order than the number of samples required information-theoretically even for vanilla sparse PCA---this is a well-known phenomenon. In the next section, we show that even with the additional structure afforded by tree and path sparsity, this larger sample size is in some sense necessary for computationally efficient algorithms.


% by showing matching evidence of computation hardness in these problems.
%the minimax lower bound proposed in Corollary~\ref{coro:TS-PCA-fund-limits}(b), Section~\ref{sec:examples-SDP-hard} shows that $O(k^2)$ number of samples is necessary in term of SDP hardness for tree-sparse PCA; while Section~\ref{sec:examples-average-hard} proves that $O(k^2)$ number of samples is necessary in term of average-case hardness for path-sparse PCA. Furthermore, we would like to point that the above probability $1 - C'\exp(- \min\{\sqrt{d}, n\}/C')$ ensures the high probability condition $1 - o(1)$ as $d, n \rightarrow \infty$.} \\
%\gwcomment{Move to Section 3.3 after the result of initialization methiod.} 
%{\color{orange} 
%Here, we would like to point out a recent study \cite{liu2021generative} on the projected power method for PCA with generative models. While there may be some similarities between our work and theirs, such as the use of projection oracle and local geometric convergence results for power methods, there are also significant differences in the assumptions and goals of the two papers. Our work focuses on the linear structure assumption for the principal components, which is a different structural assumption from those in \cite{liu2021generative}, and we provide fundamental limits of estimation for this problem. Additionally, our work presents an end-to-end analysis of two prototypical examples, including exact projection oracles, initialization methods, and hardness results, which complement the theoretical results.
%a recent study \cite{liu2021generative}. This paper explores the projected power method for PCA problem in which its ground truth follows a generative model, and prove a local geometric convergence result under appropriate conditions with an accessible exact projection oracle. In comparison, our paper focuses on a different structural assumption on the principal components and employs novel proof techniques. Additionally, we provide fundamental limits of estimation for linearly structured PCA. Later, Section~\ref{sec:specific-examples} further presents an end-to-end analysis, which encompasses tractable exact projection oracles, efficient initialization methods and hardness results, for two prototypical examples.
%}


%\subsection{Compare with some recent works}
%\gwcomment{Consider whether add this subsection or not.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




