In this work we introduced a video-to-speech synthesis framework that uses video and audio inputs during both training and inference. This is accomplished in a two step-process; firstly, by using a pre-trained video-to-speech model to synthesize the missing audio and secondly, by using the video and the synthesized audio samples as inputs to a new, audio-visual-to-audio (AV2A) model. We proposed a simple method to obtain such a model, by appending an audio encoder to an existing video-to-speech model. We conduct experiments in both the raw waveform and mel spectrogram domains, and introduce two variants of modality dropout during training. Our experiments demonstrate the effectiveness of our approach, as measured by objective metrics.

In future work it would be interesting to extend this framework to other speech synthesis tasks, such as audio-visual speech enhancement, separation and inpainting (i.e., by using the synthesized outputs of such a model in the inputs of a second model). Inspired by the literature on speech enhancement\cite{an_overview_of_deep_learning_based_av_speech_enhancement_and_separation}, another beneficial research direction would be to experiment with different training objectives in the AV2A model, such as mask approximation and indirect mapping. Furthermore, one may also investigate mixing models designed for different tasks, such as using a V2A model to synthesize the audio and then conducting AV2A on an off-the-shelf audio-visual speech enhancement model.