\IEEEPARstart{D}{eep} learning has revolutionized the field of automatic speech recognition (ASR), paving the way for communication between humans and machines across multiple domains such as autonomous vehicles\cite{driven_to_distraction}, virtual assistants\cite{manifestation_of_virtual_assistants} and voicebots for services including digital banking\cite{intelligent_voice_bots_for_digital_banking}. However, the performance of ASR systems degrades in the presence of acoustic noise \cite{deep_av_speech_recognition}. This has motivated research that leverages the multi-modal nature of speech, and in particular the co-occurrence of spoken words and lip movements, to conduct ASR with both visual and audio inputs \cite{deep_av_speech_recognition, end_to_end_av_speech_recognition, end_to_end_av_speech_recognition_with_conformers, av_speech_recognition_using_bimodal_trained_bottleneck_features, av_speech_recognition_with_hybrid_ctc_attention_architecture}. Lipreading, i.e., predicting text from a silent video, has also been investigated in situations where the audio modality is missing or very noisy \cite{deep_learning_based_automated_lipreading_a_survey, review_on_research_progress_of_machine_lipreading}.

Although lipreading has undergone significant progress in recent years, synthesizing the missing (or noisy) audio from the video modality is beneficial in multiple scenarios. Firstly, this is an additional and indirect approach to perform lipreading, i.e., one may synthesize the missing audio from the silent video first and then perform ASR on the synthesized audio. For example, several video-to-speech works employ pre-trained ASR models to compute the word error rate of their synthesized audio samples (e.g., \cite{video_driven_speech_reconstruction_using_gans, end_to_end_video_to_speech_synthesis_using_gans, lip_to_speech_synthesis_with_visual_context_attention_gan, svts}). Secondly, there are several applications that make use of the synthesized audio, including speech enhancement for videoconferecing \cite{ephrat2017vid2speech}, speech reconstruction in silent videos from surveillance cameras\cite{ephrat2017vid2speech, av_event_recognition_in_surveillance}, synthesizing speech for individuals with aphonia \cite{end_to_end_video_to_speech_synthesis_using_gans}, and making silent speech interfaces for environments requiring silence \cite{silent_speech_interfaces}.

The above considerations have led to increasing interest in the task of video-to-speech synthesis (V2A), involving the reconstruction of the speech signal from a silent video. Although one possible approach is to predict text with a lipreading model first followed by audio synthesis with a text-to-speech (TTS) system, this entails multiple disadvantages: one requires text labels, the text modality does not contain information about a speaker's voice, and the performance of the lipreading model bounds the accuracy of the synthesized speech. Thus, several V2A methods have been introduced that predict either intermediate acoustic features (including linear predictive codes\cite{ephrat2017vid2speech} and mel spectrograms \cite{lip2wav, lip_to_speech_synthesis_with_visual_context_attention_gan, speech_reconstruction_visual_voice_memory, lipsound2}) followed by vocoder synthesis, or raw waveforms directly \cite{video_driven_speech_reconstruction_using_gans, end_to_end_video_to_speech_synthesis_using_gans}. Most of these works train models with silent video inputs only, although some employ audio inputs as well \cite{lipsound2, speech_reconstruction_visual_voice_memory}. For example, \cite{lipsound2} employs teacher forcing during training using the ground truth mel spectrograms as inputs, and \cite{speech_reconstruction_visual_voice_memory} learns face and voice associations using a key-value memory structure that discards the audio encoder after training. However, none of these methods have investigated the possibility of including audio inputs (in addition to the silent video) during inference as well. 

% Figure environment removed

In this work we investigate video-to-speech synthesis models, following an encoder-decoder structure, that include audio and video inputs during both training and inference. We do so in both the raw waveform and mel spectrogram domains, as illustrated in Fig. \ref{fig:high_level_av2a}. Given that the V2A task posits we do not have access to the audio samples of input videos  during inference, we first synthesize them using a pre-trained V2A model. Once we have obtained the synthesized audio samples we combine them with their corresponding silent video samples as inputs to an audio-visual-to-speech model (AV2A). To obtain the latter, we propose appending an audio encoder to the V2A model used above, thus constructing a corresponding AV2A model. This approach, which involves transforming a V2A model to an AV2A model and using the V2A model's synthesized samples, has the additional benefit of allowing us to investigate whether a given V2A model can be improved by using its own outputs as self-supervision. Hence, we refer to the V2A model in question as the base model to highlight that we use it both to synthesize the input audio, and as the starting point to construct the AV2A model.

In addition we introduce three procedures for training the AV2A models. The first, baseline approach, involves reconstructing audio directly from the audio-visual inputs. Inspired by recent work in audio-visual self-supervised learning\cite{av_hubert}, the second procedure employs modality dropout, whereby we train in an alternating fashion between the baseline approach above, dropping the audio modality and dropping the visual modality. Finally, we also train with the aforementioned modality dropout, modified such that we use the ground truth audio, instead of the synthesized audio, when dropping the visual modality (i.e., when using the audio modality only). Modality dropout is also motivated by the observation that conditional models ignore parts of their input when the conditioning information is very strong (e.g. ignoring the noise vector in conditional image GANs \cite{deep_multi_scale_video_prediction_beyond_mse, image_to_image_translation_with_cgans}, and the presence of the audio modality dominating predictions in audio-visual speech recognition \cite{av_hubert, deep_av_speech_recognition, end_to_end_av_speech_recognition_with_conformers}). By dropping a modality, our models are forced to reconstruct the audio using the remaining modality only, thus avoiding the trivial solution of relying on the ``easier'' modality for reconstruction.  

Our contributions are summarized as follows:
\begin{itemize}
\item We propose a video-to-speech synthesis approach that employs audio and video inputs during both training and inference by transforming the V2A model into a corresponding AV2A model, and using the former to synthesize the audio inputs. We do so in both the raw waveform and the mel spectrogram domains.

\item We introduce two versions of training with modality dropout \cite{av_hubert}, which involves training by alternating between the input modalities (audio only, visual only, audio-visual).

\item We modify the batch normalization \cite{batch_norm} layers in our models such that they keep track of separate running statistics for the audio, video and audio-visual modalities (i.e., for each setting of modality dropout).

\item We carry out experiments on popular audio-visual face and speech datasets on seen (GRID \cite{grid_database}, TCD-TIMIT \cite{tcd_timit_databse}) and unseen (GRID\cite{grid_database}, LRW\cite{lip_reading_in_the_wild}) speakers.
\end{itemize}