\subsection{Lipreading}
Lipreading, also known as visual speech recognition, involves predicting a speaker's words from silent videos. It has been a longstanding research interest in the speech processing community and was originally proposed in \cite{visual_contribution_to_speech_intelligibility_in_noise}, particularly for supplementing audio speech recognition in environments with high acoustic noise. The feasibility of computer-based lipreading was demonstrated in early experiments in both visual and audio-visual speech recognition in \cite{continuous_optical_automatic_speech_recognition_by_lipreading} and \cite{automatic_lipreading_to_enhance_speech_recognition} respectively.

Prior to the widespread adoption of deep learning, lipreading methods employed handcrafted visual features used as input to Hidden Markov Models (HMMs) \cite{information_theoretic_feature_extraction_for_av_speech_recognition} or Support Vector Machines (SVMs) \cite{lipreading_with_local_spatiotemporal_descriptors} to predict text. These include DCT features \cite{information_theoretic_feature_extraction_for_av_speech_recognition}, DWT features \cite{an_image_transform_approach_for_hmm_based_automatic_lipreading}, geometric features \cite{lip_feature_extraction_and_reduction_for_hmm, lip_feature_extraction_based_on_improved_jumping_snake_model, profile_view_lip_reading} and Active Appearance Models \cite{active_appearance_models, view_independent_computer_lip_reading, insights_into_machine_lip_reading}. In recent years, research has focused on end-to-end deep learning (DL) models which have outperformed traditional methods. Early DL lipreading works studied word and phoneme-level prediction in two stages: extracting deep features first, then training a classifier \cite{lipreading_using_convolutional_neural_network, deep_learning_of_mouth_shapes_for_sign_language, improved_speaker_independent_lip_reading, av_speech_recognition_using_bimodal_trained_bottleneck_features, deep_complementary_bottleneck_features_for_visual_speech_recognition}. Lipnet \cite{lipnet} was the first end-to-end work on sentence-level prediction and comprised a spatio-temporal convolutional encoder, a bidirectional GRU \cite{empirical_evaluation_of_gated_rnn_on_sequence_modeling} and a Linear layer. Trained with a CTC loss \cite{connectionist_temporal_classification}, it produced state-of-the-art results on the GRID database \cite{grid_database}.

Other end-to-end lipreading models have focused on predictions at the word and character level. For example, in \cite{end_to_end_visual_speech_recognition_with_lstms, end_to_end_visual_speech_recognition_for_small_scale_datasets} a model with fully-connected layers and LSTMs predicted words from cropped mouth regions and their corresponding difference images. A CNN-based model was proposed in \cite{lip_reading_in_the_wild} and applied to silent videos of single-word utterances, recorded in-the-wild. In \cite{large_scale_visual_speech_recognition} a CNN+LSTM based model that predicted phonemes was presented, followed by word decoding, while \cite{combining_residual_networks_with_lstms_for_lipreading} introduced a word-level prediction model with residual connections and LSTMs.

By incorporating the audio modality as well (i.e. performing audio-visual speech recognition) one may leverage the natural co-occurrence of audio and video to obtain superior results to a unimodal model, for example by distinguishing homophenes. In \cite{end_to_end_av_speech_recognition} two ResNet-based encoders, followed by bidirectional GRUs, encode the mouth frames and raw audio respectively, while in \cite{deep_av_speech_recognition} ResNet+Transformer back-end is employed. For further coverage of lipreading methods and approaches, we refer the reader to relevant surveys \cite{deep_learning_based_automated_lipreading_a_survey, review_on_research_progress_of_machine_lipreading}.