\subsection{Video-to-speech synthesis}

Early work in video-to-speech synthesis\cite{reconstructing_intelligible_audio_speech_from_visual_speech_features, generating_intelligible_audio_speech_from_visual_speech} relied on handcrafted visual features, such as 2D-DCT and AAM features. To the best of our knowledge, \cite{reconstructing_intelligible_audio_speech_from_visual_speech_features} was the first work predicting speech directly from visual inputs and involved a model receiving visual features and using Gaussian Mixture Models (GMMs) or fully-connected neural networks to to predict spectral envelope representations (LPCs or mel-filterbank amplitudes). A STRAIGHT vocoder \cite{straight_vocoder} then processed the output and synthesized raw waveforms. In \cite{generating_intelligible_audio_speech_from_visual_speech} this was extended, where a classification framework was employed to predict codebook entries corresponding to audio vectors, resulting in better speech intelligibility.

A model with CNNs was employed in \cite{ephrat2017vid2speech} that learned features from raw pixels (grayscale video frames), outputting line spectral pairs (LSP) features. 
These were fed with Gaussian noise to a source-filter speech synthesizer to generate the raw waveforms. In \cite{improved_speech_reconstruction_from_silent_video} this was improved by
employing one ResNet encoder for raw video and another one for optical flow. The two sets of embeddings are then concatenated and fed into a fully-connected network to output mel spectrograms, followed by post-processing module to generate linear spectrograms and the Griffin-Lim algorithm (GLA) \cite{griffin_lim} to obtain the raw waveforms. Lip2AudSpec \cite{lip2audspec} first trains a fully-connected auto-encoder on spectrograms and then uses the learned bottleneck features as training targets for a CNN+RNN lipreading model. A multi-task model was presented in \cite{vocoder_based_speech_synthesis}, which predicts the spectral envelope, aperiodic parameters and the fundamental frequency as inputs to a vocoder to synthesize the raw waveform. It is also trained jointly for a lipreading task using a connectionist temporal classification (CTC) \cite{connectionist_temporal_classification} loss. Lip2Wav \cite{lip2wav} uses an encoder-decoder architecture to generate mel spectrograms from video frames. The model features a stack of 3D convolutions and an attention-based decoder inspired by Tacotron2\cite{tacotron2}, with GLA applied to extract the raw waveforms from the mel spectrograms.

In VCA-GAN \cite{lip_to_speech_synthesis_with_visual_context_attention_gan} the input video sequence is summarized and included as a global conditioning variable and a multi-scale generator with residual blocks is used to generate mel spectrograms from coarse to fine-level. This is followed by a post-net, similar to that proposed in \cite{improved_speech_reconstruction_from_silent_video}, and GLA to synthesize the raw waveforms. Along with a multi-scale discriminator, it is trained with an adversarial loss, reconstruction and synchronization losses.

In addition, a VAE-based\cite{vae_paper} model was introduced in \cite{speech_prediction_in_silent_videos_using_vaes}, in order to model the uncertainty in generating speech. This involves an encoder-decoder model, where the encoder and decoder are bridged by LSTMs to model the uncertainty autoregressively, using conditional probability distributions. The model generates mel spectrograms, and GLA is applied to obtain the raw waveforms.

Visual Voice Memory \cite{speech_reconstruction_visual_voice_memory} proposes a key-value memory structure, inspired by memory networks \cite{key_value_memory_networks}, to map visual features (keys) to audio features (values). During training, the model employs video and speech encoders to extract features from the video frames and ground truth mel spectrograms respectively. The decoder synthesizes mel spectrograms by first concatenating the visual features with these audio features, or with audio features as predicted by the intermediate memory module. The audio encoder is discarded during inference, and the memory module receives the visual features to output imprinted audio features, to be concatenated with the visual features and fed to the decoder.

LipSound2 \cite{lipsound2} proposed an encoder-decoder model with an auto-regressive bridge module in the middle. During training, teacher forcing is employed, whereby the ground truth mel spectrogram is passed as input to the bridge module, along with the visual features. During inference, the predicted outputs from previous timesteps are employed instead, in an auto-regressive manner. A highlight of this work is that it investigated the effect of pre-training this video-to-speech model, along with a speech recognition model, on VoxCeleb2 \cite{voxceleb2_paper} and LibriSpeech \cite{librispeech} respectively. The video-to-speech model was then fine-tuned on an audio-visual dataset of interest, followed by fine-tuning of the speech recognition model on the predicted audio, achieving state-of-the-art results on GRID and TCD-TIMIT.

In order to scale V2A models to large datasets in a principled manner, SVTS \cite{svts} introduced a 3D Convolution+ResNet-18 encoder followed by a conformer decoder that generates mel spectrograms and a pre-trained neural vocoder. The decoder is of varying depth, in proportion to the size of the audio-visual dataset in question. It is the first V2A model applied to LRS3 \cite{lrs3} and also achieved state-of-the-art results on the GRID and LRW \cite{lip_reading_in_the_wild} datasets.

Some recent works have employed GANs to synthesize raw waveforms from silent videos directly, trained end-to-end. In \cite{video_driven_speech_reconstruction_using_gans} an encoder-decoder GAN was proposed, whose generator is fed a silent video and outputs a raw waveform. It consists of a convolutional decoder, a GRU and a decoder with transposed convolutions. Using a convolutional waveform critic, the model is trained with a Wasserstein GAN loss\cite{wasserstein_gan, improved_training_of_wasserstein_gans} and three reconstruction losses. This work was extended in \cite{end_to_end_video_to_speech_synthesis_using_gans}, where the convolutional encoder was replaced by a 3D Convolution+ResNet-18 and a spectrogram critic was added as well.

Finally, our previous work \cite{large_scale_unsupervised_audio_pretraining_for_v2a} investigated the effect of pre-training the decoder of V2A encoder-decoder models on large volumes of audio data, followed by fine-tuning the V2A model on an audio-visual dataset of interest. This involved training audio-to-audio (A2A) encoder-decoder models on c. 3,500 hours of audio data and then initializing the decoder of a V2A model with that of a pre-trained A2A model. To this end, we proposed a family of V2A models in the raw waveform and mel spectrogram domains (inspired by the Wasserstein GAN in \cite{end_to_end_video_to_speech_synthesis_using_gans} and SVTS\cite{svts}, and denoted by V2A-WaveGAN and V2A-MelSpec respectively) and their A2A counterparts. Our experiments across GRID, TCD-TIMIT and LRW demonstrated that in most cases, this unsupervised pre-training step leads to higher quality audio, as measured by objective metrics.