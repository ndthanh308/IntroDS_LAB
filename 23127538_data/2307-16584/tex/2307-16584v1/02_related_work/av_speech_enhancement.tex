\subsection{Audio-visual speech enhancement}

Speech enhancement, involving the extraction of clean speech from a noisy or corrupt speech signal, is a well-studied problem in signal processing \cite{an_overview_of_deep_learning_based_av_speech_enhancement_and_separation, supervised_speech_separation_based_on_deep_learning}. Inspired by the bimodal nature of speech perception\cite{hearing_by_eye} many research works have also investigated audio-visual speech enhancement (AVSE), whereby the visual modality, and lip movements in particular, is incorporated. 

The relevant literature is vast\cite{an_overview_of_deep_learning_based_av_speech_enhancement_and_separation} and can be categorized according to the type of input acoustic features (e.g., spectrograms, STFT, raw waveforms), input visual features (e.g. raw video pixels, AAMs, optical flow), models (e.g. auto-encoders, VAEs \cite{vae_paper}, GANs\cite{gan_paper}), audio-visual fusion methods (e.g. concatenation-based, attention-based), training targets (e.g. spectrograms, masks, raw waveforms) and objective functions (e.g. MSE, MAE, triplet loss). Research can also be grouped according to the the type of sound distortion (e.g. additive noise, reverberation, clipping) and whether it involves the presence of other speakers (leading to the problem of speech separation), both of which motivate different models and training targets \cite{an_overview_of_deep_learning_based_av_speech_enhancement_and_separation, supervised_speech_separation_based_on_deep_learning}. 

To the best of our knowledge, the first two concurrent works in AVSE were \cite{audio_visual_enhancement_of_speech_in_noise,
audio_visual_segmentation_and_the_cocktail_party_effect}. Before the advent of deep learning, subsequent methods focused on predicting time-frequency masks\cite{source_separation_of_convolutive_and_noisy_mixtures} \cite{speaker_separation_using_visually_derived_binary_masks}, as well as other training targets \cite{avss_via_hidden_markov_models, noisy_audio_feature_enhancement_using_av_speech_data, avse_with_avcdcn, av_graphical_models_for_speech_processing, video_assisted_speech_source_separation, effective_visually_derived_wiener_filtering_for_av_speech_processing}. Early deep-learning based works include \cite{seeing_through_noise}, where the video-to-speech model in \cite{improved_speech_reconstruction_from_silent_video} is used to generate a mask to filter the noisy spectrogram. In \cite{the_conversation_deep_av_speech_enhancement} a model that jointly predicts a magnitude spectrogram mask and a phase adjustment is proposed and was applied successfully to unseen speakers in-the-wild, including to LRS2 \cite{deep_av_speech_recognition} and VoxCeleb2\cite{voxceleb2_paper}. In \cite{time_domain_av_speech_separation} a model performing speech separation on raw waveforms is proposed, trained with scale-invariant source-to-noise ratio \cite{tasnet}. Other approaches include U-Net based models \cite{muse, visualvoice} as well conditional VAEs \cite{avse_using_conditional_vaes}.

Several works have investigated deep-learning based AVSE models with direct mapping, i.e., the setting where either the raw waveform or some acoustic features constitute the training target. In \cite{visual_speech_enhancement} an encoder-decoder model is proposed that is fed both the input video and the noisy mel spectrogram to output the clean mel spectrogram. A CNN-based architecture was employed in \cite{audio_visual_speech_enhancement_using_multimodal_deep_cnns} that received noisy spectrograms and cropped mouth video inputs, and reconstructed both the clean spectrograms and the cropped mouth frames. A CNN+LSTM model is introduced in \cite{av_speech_codecs_rethinking_avse} to predict mel spectrograms, followed by a pre-trained neural vocoder to reconstruct the raw audio. Recently, La-VocE \cite{la_voce} was proposed, to perform AVSE in the mel spectrogram domain in a two-step process: firstly, the ResNet+Transformer-based generator predicts the enhanced mel spectrograms and secondly, a HiFiGAN\cite{hifi_gan} vocoder (trained from scratch) transforms them into the final raw waveforms. This approach demonstrated superior results across multiple noise sources and languages.

\begin{comment}
La Voce, AVSE summary (in introduction):

Recent AVSE methods are often based on U-Nets [7â€“9], inspired
by their audio-only counterparts [2, 3, 10], or simple convolutional networks [11], frequently combined with LSTMs [12]. Existing
speech enhancement models are typically combined with a video encoder which extracts visual features and concatenates them with the
acoustic features to perform audio-visual enhancement. These approaches draw from speech enhancement literature, but fail to leverage state-of-the-art audio-visual encoders [13, 14]. Most methods
estimate (either directly or via a mask) the magnitude and phase of
the clean spectrogram, which are converted into waveform using the
inverse Short-Time Fourier Transform (iSTFT) [7, 9, 11], while others attempt to perform enhancement in the time domain directly [8].
Both of these reconstruction techniques rely on very accurate predictions, which can be difficult to achieve, especially in low-SNR
environments where audio supervision is unreliable. Recent works
in audio-only [15, 16] and audio-visual [12] speech enhancement
have introduced neural vocoders as an alternative synthesis method,
but choose to focus on high-SNR scenarios where this reconstruction
technique is likely to have a lesser impact. Alternatively, new works
introduce neural codecs [17] for waveform synthesis but focus heavily on achieving compressed representations, which is not a priority
for most speech enhancement frameworks.
\end{comment}

