\subsection{Datasets}
\label{subsection:av2a_datasets}

We conduct experiments on three audio-visual face and speech datasets which are widely used in the video-to-speech literature: GRID \cite{grid_database}, TCD-TIMIT \cite{tcd_timit_databse} and LRW\cite{lip_reading_in_the_wild}. These are summarized in Table \ref{table:av2a_av_datasets}.

GRID \cite{grid_database} is composed of 33 speakers that utter structured sentences. Each sentence is recorded in laboratory conditions and contains 6 words selected at random from a fixed vocabulary of 51 words. In line with the literature, we experiment with three versions of this dataset: (1) a seen speaker setting with 4 speakers, used in several previous works \cite{end_to_end_video_to_speech_synthesis_using_gans, video_driven_speech_reconstruction_using_gans, lip2wav, lip_to_speech_synthesis_with_visual_context_attention_gan, large_scale_unsupervised_audio_pretraining_for_v2a}; (2) a seen speaker setting with 33 speakers, originally proposed in \cite{end_to_end_video_to_speech_synthesis_using_gans} and (3) an unseen speaker setting with 33 speakers used in \cite{end_to_end_video_to_speech_synthesis_using_gans, video_driven_speech_reconstruction_using_gans, lip_to_speech_synthesis_with_visual_context_attention_gan, large_scale_unsupervised_audio_pretraining_for_v2a}.

TCD-TIMIT \cite{tcd_timit_databse} consists of 62 speakers (59 volunteers and 3 lipspeakers) uttering sentences recorded in laboratory conditions. In line with previous works \cite{lip2wav, end_to_end_video_to_speech_synthesis_using_gans, lip_to_speech_synthesis_with_visual_context_attention_gan, large_scale_unsupervised_audio_pretraining_for_v2a} we use the audio and video samples of the 3 lipspeakers only, which amount to 377 clips per lipspeaker. We employ a seen speaker split proposed in \cite{end_to_end_video_to_speech_synthesis_using_gans}.

LRW \cite{lip_reading_in_the_wild} consists of 'in-the-wild' audio-visual clips of one word utterances from hundreds of speakers, out of a vocabulary of 500 words. These samples were obtained from television shows and include variations in lighting conditions, head pose and background noise. This dataset is therefore more challenging for face and speech-related tasks than GRID and TCD-TIMIT.

\begin{table}[h]
\captionsetup{justification=centering}
\caption{Summary of audio-visual datasets}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset}                & \textbf{\begin{tabular}[c]{@{}c@{}}Training set\\ (samples/hours)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Validation set\\ (samples/hours)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Test set\\ (samples/hours)\end{tabular}} \\ \midrule
GRID (4 speakers, seen)         & 3543 / 2.95                                                                              & 209 / 0.17                                                                                 & 209 / 0.17                                                                           \\
GRID (33 speakers, seen)        & 29333 / 24.44                                                                              & 1627 / 1.36                                                                                 & 1630 / 1.36                                                                          \\
GRID (33 speakers, unseen)      & 15648 / 13.04                                                                              & 6996 / 5.83                                                                                 & 9946 / 8.29                                                                          \\
TCD-TIMIT (3 lipspeakers, seen) & 1014 / 1.64                                                                              & 57 / 0.09                                                                                 & 60 / 0.10                                                                           \\
LRW (unseen)                    & 480456 / 154.81                                                                              & 24728 / 7.97                                                                                & 24663 / 7.95                                                                          \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{table:av2a_av_datasets}
\end{table}

\subsection{Data pre-processing}
We extract cropped mouth regions from the video frames by first conducting face detection using the S$^3$FD \cite{s3fd}, followed by landmark localization of 68 points with a pre-trained 2D-FAN \cite{fan} and alignment of each face to a reference mean face. We then crop mouth regions of dimensionality $128\times74$ for GRID and $96\times96$ for TCD-TIMIT and LRW in line with \cite{end_to_end_video_to_speech_synthesis_using_gans} and normalize the resulting images. We also employ data augmentation during training by applying horizontal flipping to each cropped frame with a 50\% probability.

All audio is sampled at 24 kHz. To extract log-mel spectrograms we use 80 mel bands, an FFT size of 2048, a hop size of 12.5 ms, a window length of 50 ms, and a Hann window. Furthermore we clip values outside the $[-6, 6]$ range and rescale the resulting spectrogram to $[-1, 1]$. We do not pre-process the raw waveforms.

\subsection{AV2A model training}
Given a pre-trained V2A model, we begin by synthesizing the audio (raw waveforms or mel spectrograms) of a dataset of interest, including the training, validation and test subsets. Then, in the corresponding AV2A model, we initialize the video frames encoder and the decoder with the parameters of the pre-trained V2A model, and use default initialization for the audio encoder and the temporal module. If the V2A model includes a Discriminator, as in V2A-WaveGAN, we initialize the Discriminator of the the corresponding AV2A model with the pre-trained parameters.

For a given pre-trained V2A model, we experiment with three training procedures with the corresponding AV2A model:

\begin{itemize}
\item \textbf{Baseline:} The temporal module receives a sequence of features from the audio and video frames encoders (including the speaker embedding), i.e. from both audio and visual modalities. 
\item \textbf{Modality dropout:} This consists of alternating between reconstructing speech from all modalities (as in the baseline), from the visual modality only (including the speaker embedding), and from the audio modality only. This is described in Algorithm \ref{algorithm:av2a_training_with_modality_dropout} where $E_A, E_V, E_I$ denote the audio encoder, video frames encoder and speaker encoder respectively, $T_{AV}$ and $F_{AV}$ denote the temporal module and decoder and $D_{AV}$ is the Discriminator (included only if part of the model).
\item \textbf{Modality dropout (ground truth audio): } This involves performing modality dropout, as above, whereby during reconstruction with the audio modality only, we compute audio features from the ground truth audio rather than the synthesized audio. When reconstructing speech from all modalities we keep using the synthesized audio as input to the audio decoder, as with modality dropout above. 
\end{itemize}

\subsubsection{Selection of pre-trained base V2A models}
For each audio domain (raw waveform and mel spectrograms) and each dataset, we obtain two pre-trained base V2A models from our previous work \cite{large_scale_unsupervised_audio_pretraining_for_v2a}: the first model was trained from scratch (i.e. with random parameter initialization) and the second model was trained with all parameters initialized at random, except of the decoder and optionally the Discriminator. The parameters of the latter were pre-trained on a large volume of audio data, as part of the task of audio auto-encoding which was the subject of our investigation \cite{large_scale_unsupervised_audio_pretraining_for_v2a}. As we proposed multiple methods for fine-tuning V2A models with these pre-trained modules, in this work we select for each audio domain and dataset the fine-tuned model with the lowest validation loss. Thus, for each audio domain and dataset we obtain two pre-trained base V2A models which we note in section \ref{section:av2a_results} in the format V2A-X and V2A-X with audio pre-training.


% ********************** Modality dropout algorithm ********************** %
\begin{algorithm}[t]
\caption{AV2A-WaveGAN and AV2A-MelSpec training with modality dropout}
\begin{algorithmic}[1]
\STATE \textbf{Input:} \texttt{train\_data}, \texttt{ground\_truth\_audio}, \\ $E_A, E_V, E_I, T_{AV}, F_{AV}$, [optional] $D_{AV}$
\STATE \texttt{for epoch in num\_epochs:}
\STATE \quad \texttt{for batch in train\_data:}
\STATE \qquad \textbackslash\textbackslash \: Synthesized audio, video frames, ground truth \\ \qquad audio and input to speaker encoder
\STATE \qquad $\mathbf{x_a}, \mathbf{x_v}, \mathbf{x}, \mathbf{x_I} \gets $ \texttt{batch}
\STATE 
\STATE \qquad \textbackslash\textbackslash \: Reconstruct speech with all modalities
\STATE \qquad $\mathbf{z_a}, \mathbf{z_v}, \mathbf{z_I} = E_A(\mathbf{x_a}), E_V(\mathbf{x_v}), E_I(\mathbf{x_I})$
\STATE \qquad $\mathbf{z} = T_{AV}(\mathbf{z_a}, \mathbf{z_v}, \mathbf{z_I})$
\STATE \qquad $\mathbf{\Tilde{x}} = F_{AV}(\mathbf{z})$
\STATE \qquad \texttt{compute\_loss} $(\mathbf{x}, \mathbf{\Tilde{x}})$
\STATE \qquad \texttt{backpropagation} ($E_A, E_V, T_{AV}, F_{AV}, D_{AV}$)
\STATE
\STATE \qquad \textbackslash\textbackslash \: Reconstruct speech with visual modality only \\ \qquad (including speaker embedding)
\STATE \qquad $\mathbf{z_v}, \mathbf{z_I} = E_V(\mathbf{x_v}), E_I(\mathbf{x_I})$
\STATE \qquad $\mathbf{z} = T_{AV}(\mathbf{0}, \mathbf{z_v}, \mathbf{z_I})$
\STATE \qquad $\mathbf{\Tilde{x}} = F_{AV}(\mathbf{z})$
\STATE \qquad \texttt{compute\_loss} $(\mathbf{x}, \mathbf{\Tilde{x}})$
\STATE \qquad \texttt{backpropagation} ($E_V, T_{AV}, F_{AV}, D_{AV}$)
\STATE 
\STATE \qquad \textbackslash\textbackslash \: Reconstruct speech with audio modality only
\STATE \qquad \textbf{if} \texttt{ground\_truth\_audio} \textbf{ is True:}
\STATE \qquad \qquad $\mathbf{z_a} = E_A(\mathbf{x})$
\STATE \qquad \textbf{else:}
\STATE \qquad \qquad $\mathbf{z_a} = E_A(\mathbf{x_a})$
\STATE \qquad $\mathbf{z} = T_{AV}(\mathbf{z_a}, \mathbf{0}, \mathbf{0})$
\STATE \qquad $\mathbf{\Tilde{x}} = F_{AV}(\mathbf{z})$
\STATE \qquad \texttt{compute\_loss} $(\mathbf{x}, \mathbf{\Tilde{x}})$
\STATE \qquad \texttt{backpropagation} ($E_A, T_{AV}, F_{AV}, D_{AV}$)
\STATE \textbf{Output:} $E_A, E_V, E_I, T_{AV}, F_{AV}$, [optional] $D_{AV}$
\end{algorithmic}
\label{algorithm:av2a_training_with_modality_dropout}
\end{algorithm}

\subsubsection{Raw waveform training details}
We train the Generator and Discriminator of AV2A-WaveGAN using a batch size of 4 and with the Adam \cite{adam} optimizer with a learning rate of $1 \times 10^{-4}$, $\beta_1 = 0.5$ and $\beta_2 = 0.99$. In line with \cite{end_to_end_video_to_speech_synthesis_using_gans} the Discriminator receives a 1 second audio clip cropped at random from the ground truth and reconstructed audio. The Generator is fed input samples of duration up to 3 seconds.

\subsubsection{Mel spectrogram training details}
We train AV2A-MelSpec using the AdamW \cite{adamw} optimizer with $\beta_1 = 0.9, \beta_2 = 0.98$ and weight decay of $1 \times 10^{-2}$. Training is conducted in two stages:
\begin{itemize}
\item \textbf{Training audio encoder and temporal module only:} The video frames encoder and the decoder are kept frozen. The remaining trainable modules (audio encoder and Bidirectional LSTM in the temporal module) are trained using a learning rate of $1 \times 10^{-3}$ for all experiments with seen speakers and $5 \times 10^{-4}$ for all experiments with unseen speakers. We warmup the learning rate for 20 epochs, for experiments with GRID and TCD-TIMIT, and for 15 epochs for experiments with LRW. Following the warmup phase, we decay the learning rate using a cosine schedule with warm restarts\cite{sgdr}, where $T_0 = 1, T_{mult} = 2$. We save a checkpoint at the end of each epoch.
\item \textbf{Continuing training the entire model:} From the above we select the checkpoint of the epoch with the lowest validation loss during the warmup phase and un-freeze the video frames encoder and decoder. We continue training the model with the given learning rate schedule, where the learning rates for the audio encoder and temporal module are as above and those of the video frames encoder and decoder are set to $1 \times 10^{-4}$ for experiments with seen speakers and $1 \times 10^{-5}$ for experiments with unseen speakers.
\end{itemize}

\subsection{Evaluation metrics}
To measure the quality and intelligibility of our reconstructed speech we employ 4 objective metrics widely used in speech synthesis works: PESQ, STOI, ESTOI  and the word error rate (WER). Although it is widely acknowledged that existing metrics correlate imperfectly with human perception \cite{video_driven_speech_reconstruction_using_gans, end_to_end_video_to_speech_synthesis_using_gans}, these metrics are widely used in the video-to-speech literature and are useful for comparing different works.

PESQ (perceptual evaluation of speech quality)\cite{pesq}, originally created to measure speech quality of speech codecs and telephone networks, aims to capture the perceptual quality of the speech. STOI (short-time objective intelligibility)\cite{stoi} and its extended version ESTOI \cite{estoi} aim to measure the intelligibility of speech samples and show high correlation with reported intelligibility scores in subjective listening tests. For these three metrics, higher scores are better.

WER measures the word-level accuracy of speech samples. Following common practice, we use pre-trained speech recognition models to compute it. For experiments with GRID we use a pre-trained model \cite{visual_speech_recognition_for_multiple_languages_in_the_wild, svts} which achieves a WER of 0.1\% on the real audio test set (following the split in \cite{lipnet}). For LRW we use a model trained on LRW \cite{end_to_end_av_speech_recognition} and achieving a WER of 1.68\% on the test set. We did not calculate WER for TCD-TIMIT as we were unable to find an accurate, publicly available speech recognition model for it.