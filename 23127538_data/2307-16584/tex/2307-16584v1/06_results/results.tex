\label{section:av2a_results}

This section presents our experiments using the datasets and splits defined in section \ref{subsection:av2a_datasets} and Table \ref{table:av2a_av_datasets}, with the results shown in Tables \ref{table:av2a_grid_4_seen_speakers} - \ref{table:av2a_lrw}. We benchmark our models with comparable methods in the same audio domain, i.e. AV2A-WaveGAN models with other raw waveform methods and AV2A-MelSpec models with other methods generating acoustic features (such as mel spectrograms). Within each domain, we group the results of the base V2A model with those of our corresponding AV2A experiments to make explicit the impact of AV2A over the base V2A model. For all methods, we report results on test set samples provided by their respective authors, with the exception of Lip2Wav\cite{lip2wav} where results are shown as stated in their paper. Our generated samples can be viewed on our project website \footnotemark.

\footnotetext{\texttt{\url{https://sites.google.com/view/v2a-audio-visual/home}}}

We observe that our raw waveform models outperform all other comparable methods across all metrics, except for WER in GRID (33 speakers, unseen). We observe that employing modality dropout with ground truth audio outperforms both the base V2A-WaveGAN and AV2A-WaveGAN. Furthermore, this holds both when the base V2A model is trained from scratch and when audio pre-training is used. Across all datasets, except with GRID (33 speakers, unseen), modality dropout with ground truth audio produces better results than modality dropout. In most cases we also see that when the base V2A model is trained with audio pre-training, the corresponding AV2A models outperform the AV2A models where the base V2A model was trained from scratch. This suggests that in those cases the advantage of audio pre-training in V2A is maintained when fine-tuning the shared modules (video frames encoder, decoder and Discriminator) in AV2A.

Within acoustic features models, we observe that our mel spectrogram models produce improved metrics compared to other works in the majority of cases. Across all datasets, except TCD-TIMIT, our AV2A models show higher reconstruction metrics than their base V2A models, as well as lower WER in most cases. However, unlike our AV2A models in the raw waveform domain, there is no clear winner among our three proposed training procedures, despite there being at least one such method outperforming the base V2A model in every dataset (except TCD-TIMIT). We note that across all datasets, the validation and test set losses in all AV2A-MelSpec models were lower than the corresponding losses of the base V2A-MelSpec model when the latter was trained from scratch. When the base V2A-MelSpec model was trained with audio pre-training, the validation and test set losses were sometimes higher in the corresponding AV2A-MelSpec experiments compared to the base V2A-MelSpec models. For example, this occurs with GRID (33 speakers, unseen), despite the fact that the base V2A-MelSpec model with audio pre-training has both lower loss and better objective metrics than V2A-MelSpec trained from scratch. This suggests that the advantage of audio pre-training in the base model, in the form of lower loss, does not necessarily carry on in the corresponding AV2A model. We conjecture that this is an optimization difficulty and a consequence of the fact that the decoder has been fine-tuned twice: once when training the V2A model (i.e., fine-tuning from the audio pre-training), and a second time in this case when training the corresponding AV2A model. We defer further investigation of this problem to future work.

\subsection{Results on seen speakers}

Our results on experiments using dataset splits with seen speakers are shown in Tables \ref{table:av2a_grid_4_seen_speakers} - \ref{table:av2a_tcd_timit_lipspeakers}. In our experiments with GRID (4 speakers, seen), AV2A-WaveGAN with audio pre-training and modality dropout (with ground truth audio) outperforms all other raw waveform models across all metrics. All three training procedures of AV2A-WaveGAN result to improved reconstruction metrics compared to their base V2A-WaveGAN model, and the WER is equal or lower in most cases. AV2A-MelSpec-VS with audio pre-training improves upon its corresponding base V2A model in most metrics, and achieves the lowest WER among all comparable methods when trained with modality dropout (GT audio). However, VCA-GAN\cite{lip_to_speech_synthesis_with_visual_context_attention_gan} outperforms it in PESQ and Lip2Wav\cite{lip2wav} in STOI and ESTOI.


% **************** GRID seen speakers (4 speakers) ******************** %
\begin{table}[h]
\captionsetup{justification=centering}
\caption{Results on GRID (4 speakers, seen)}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method}                    & \textbf{PESQ$\uparrow$} & \textbf{STOI$\uparrow$} & \textbf{ESTOI$\uparrow$} & \textbf{WER (\%)$\downarrow$} \\ \midrule \midrule
\textbf{Raw waveform models} \\ \midrule \midrule
End-to-end WGAN (2018) \cite{video_driven_speech_reconstruction_using_gans}                           & 1.47             & 0.570             & 0.329              & 19.94                 \\
End-to-end WGAN (2022) \cite{end_to_end_video_to_speech_synthesis_using_gans}                            & 1.76             & 0.662             & 0.468              & 4.07                 \\

\midrule
V2A-WaveGAN \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.87          & 0.693         & 0.513        & 4.68              \\
\midrule

AV2A-WaveGAN & 1.90          & 0.696         & 0.520        & 4.84              \\
\hspace{3mm} + modality dropout & 1.91          & 0.696         & 0.520        & 4.84              \\
\hspace{3mm} + modality dropout (GT audio)& 1.91          & \textbf{0.698}         & 0.527        & 4.76              \\

\midrule
V2A-WaveGAN with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.90          & 0.690         & 0.513          & 4.99                       \\
\midrule

AV2A-WaveGAN with audio pre-training & 1.91          & 0.696         & 0.518        & 4.99              \\
\hspace{3mm} + modality dropout & 1.91          & 0.694         & 0.527        & 4.28              \\
\hspace{3mm} + modality dropout (GT audio)& \textbf{1.95}          & \textbf{0.698}         & \textbf{0.532}        & \textbf{3.67}              \\
\midrule \midrule

\textbf{Acoustic features models} \\ \midrule \midrule
Vid2Voc \cite{vocoder_based_speech_synthesis}                            & 1.61             & 0.650             & 0.455              & 9.29                 \\
Lip2Wav \cite{lip2wav}          & 1.77             & \textbf{0.731}             & \textbf{0.535}              & 14.08\footnotemark[2] \\
VCA-GAN \cite{lip_to_speech_synthesis_with_visual_context_attention_gan}                          & \textbf{2.03}             & 0.682             & 0.510  & \textbf{5.62} \\
Visual Voice Memory \cite{speech_reconstruction_visual_voice_memory}          & 1.82             & 0.643             & 0.481              & 6.08 \\

\midrule
V2A-MelSpec-VS \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.83          & 0.693         & 0.505        & 6.70              \\
\midrule
AV2A-MelSpec-VS          & 1.83         & 0.690        & 0.499 & 5.80              \\
\hspace{3mm} + modality dropout & 1.86          & 0.691         & 0.509        & 6.16              \\
\hspace{3mm} + modality dropout (GT audio)& 1.84          & 0.689         & 0.504        & 5.34              \\

\midrule
V2A-MelSpec-VS with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.87          & 0.695         & 0.512          & 5.74              \\
\midrule
AV2A-MelSpec-VS with audio pre-training & 1.87          & 0.696         & 0.516        & 4.71              \\
\hspace{3mm} + modality dropout & 1.88          & 0.699         & 0.522        & 4.55              \\
\hspace{3mm} + modality dropout (GT audio)& 1.88          & 0.694         & 0.518        & \textbf{4.22}              \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vfill
\hspace{3mm} \\
$^2$Reported using Google speech-to-text API
\label{table:av2a_grid_4_seen_speakers}
\end{table}


% **************** GRID seen speakers (all speakers) ******************** %
\begin{table}[h]
\captionsetup{justification=centering}
\caption{Results on GRID (33 speakers, seen)}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method}                    & \textbf{PESQ$\uparrow$} & \textbf{STOI$\uparrow$} & \textbf{ESTOI$\uparrow$} & \textbf{WER (\%)$\downarrow$} \\ \midrule \midrule
\textbf{Raw waveform models} \\ \midrule \midrule
End-to-end WGAN (2022) \cite{end_to_end_video_to_speech_synthesis_using_gans}                            & 1.70             & 0.667             & 0.465              & 4.59                 \\
\midrule
V2A-WaveGAN \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 2.00          & 0.712         & 0.529        & 2.79              \\
\midrule

AV2A-WaveGAN & 2.05          & 0.716         & 0.537        & 2.81              \\
\hspace{3mm} + modality dropout & 2.03          & 0.715         & 0.536        & 2.78              \\
\hspace{3mm} + modality dropout (GT audio)& 2.06          & 0.717         & 0.544        & \textbf{2.50}              \\

\midrule
V2A-WaveGAN with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a}    & 2.07          & 0.716       & 0.539          & 2.83              \\
\midrule

AV2A-WaveGAN with audio pre-training & 2.09          & 0.720         & 0.547        & 2.87              \\
\hspace{3mm} + modality dropout & \textbf{2.10}          & 0.720         & 0.548        & 2.92              \\
\hspace{3mm} + modality dropout (GT audio)& \textbf{2.10}          & \textbf{0.723}         & \textbf{0.553}        & 2.65              \\ \midrule \midrule

\textbf{Acoustic features models} \\ \midrule \midrule
VCA-GAN \cite{lip_to_speech_synthesis_with_visual_context_attention_gan}                            & 1.97             & 0.695             & 0.505              & 5.10                 \\
SVTS-S \cite{svts}                             & 1.97             & 0.705             & 0.523              & \textbf{2.37}                 \\
\midrule
V2A-MelSpec-S \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 2.02          & 0.720         & 0.538          & 2.66              \\
\midrule
AV2A-MelSpec-S & 2.00          & 0.717         & 0.535        & 2.72              \\
\hspace{3mm} + modality dropout & 2.04          & 0.715         & 0.536        & 2.82              \\
\hspace{3mm} + modality dropout (GT audio)& 2.02          & 0.720         & 0.539        & 2.53              \\

\midrule
V2A-MelSpec-S with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 2.01          & 0.719         & 0.536          & 3.66              \\
\midrule

AV2A-MelSpec-S with audio pre-training & 2.02          & 0.719         & 0.539        & 3.40              \\
\hspace{3mm} + modality dropout & \textbf{2.05}          & \textbf{0.721}         & \textbf{0.543}        & 3.32              \\
\hspace{3mm} + modality dropout (GT audio)          & 2.02         & 0.717        & 0.536 & 3.58              \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:av2a_grid_allspeakers_seen_speakers}
\end{table}


% ********************* TCD-TIMIT lipspeakers ************************* %
\begin{table}[h]
\captionsetup{justification=centering}
\caption{Results on TCD-TIMIT (3 lipspeakers, seen)}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method}                   & \hspace{1cm} & \textbf{PESQ$\uparrow$} & \textbf{STOI$\uparrow$} & \textbf{ESTOI$\uparrow$} \\ \midrule \midrule
\textbf{Raw waveform models} \\ \midrule \midrule
End-to-end WGAN (2022) \cite{end_to_end_video_to_speech_synthesis_using_gans} &                           & 1.40             & 0.538             & 0.357                               \\
\midrule
V2A-WaveGAN \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & & 1.41             & 0.552             & 0.364                               \\
\midrule
AV2A-WaveGAN & & 1.42          & 0.550         & 0.383                      \\
\hspace{3mm} + modality dropout &          & 1.41         & 0.547        & 0.358              \\
\hspace{3mm} + modality dropout (GT audio)&          & 1.43         & \textbf{0.569}        & 0.404              \\


\midrule
V2A-WaveGAN with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & & 1.43             & 0.562             & 0.395                             \\
\midrule
AV2A-WaveGAN with audio pre-training & & 1.43          & 0.558         & 0.403                      \\
\hspace{3mm} + modality dropout &          & 1.42         & 0.564        & 0.407              \\
\hspace{3mm} + modality dropout (GT audio)& & \textbf{1.44}          & 0.566         & \textbf{0.411}                      \\ \midrule \midrule

\textbf{Acoustic features models} \\ \midrule \midrule
VCA-GAN \cite{lip_to_speech_synthesis_with_visual_context_attention_gan} &                           & \textbf{1.43}             & \textbf{0.595}             & \textbf{0.420}  \\
Lip2Wav \cite{lip2wav} &                            & 1.35             & 0.558             & 0.365                               \\

\midrule
V2A-MelSpec-VS \cite{large_scale_unsupervised_audio_pretraining_for_v2a}& & 1.35          & 0.492         & 0.296                        \\
\midrule
AV2A-MelSpec-VS &          & 1.34         & 0.482        & 0.295              \\
\hspace{3mm} + modality dropout &          & 1.33         & 0.476        & 0.294              \\
\hspace{3mm} + modality dropout (GT audio) & & 1.34          & 0.485         & 0.290                      \\

\midrule
V2A-MelSpec-VS with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & & 1.39          & 0.503         & 0.328                        \\
\midrule
AV2A-MelSpec-VS with audio pre-training & & 1.39          & 0.503         & 0.335              \\
\hspace{3mm} + modality dropout &         & 1.39         & 0.494        & 0.316              \\
\hspace{3mm} + modality dropout (GT audio)&          & 1.38         & 0.495        & 0.318              \\

\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:av2a_tcd_timit_lipspeakers}
\end{table}

In experiments with GRID (33 speakers, seen), shown in Table \ref{table:av2a_grid_allspeakers_seen_speakers}, we observe that AV2A-WaveGAN with audio pre-training and modality dropout (GT audio) outperforms all other comparable methods across reconstruction metrics. Although its WER (2.65\%) is lower than the WER of its base V2A model (2.83\%), it is higher than AV2A-WaveGAN with modality dropout (GT audio) which is at 2.50\%. As with GRID (4 speakers, seen), we observe that all three training procedures of AV2A-WaveGAN produce improved reconstruction metrics compared to the corresponding base V2A-WaveGAN models, whereas the WER fluctuates more. AV2A-MelSpec-S with audio pre-training and modality dropout outperforms all other works across reconstruction metrics; however, SVTS-S achieves a lower WER.

With the TCD-TIMIT (3 lipspeakers, seen) split, AV2A-WaveGAN with audio pre-training and modality dropout (GT audio) outperforms other raw waveform models on PESQ and STOI, but AV2A-WaveGAN and modality dropout (GT audio) achieves the highest STOI. Note that both models achieve higher metrics than their corresponding base V2A model. However, we observe that most of our AV2A models in the mel spectrogram domain do not achieve higher reconstruction metrics than their base V2A model. Only AV2A-MelSpec-VS with audio pre-training achieves equal PESQ and STOI to its base V2A model, and a higher ESTOI. In addition, our models are outperformed on this dataset by VCA-GAN\cite{lip_to_speech_synthesis_with_visual_context_attention_gan}.

\subsection{Results on unseen speakers}

Tables \ref{table:av2a_grid_unseen_speakers} and \ref{table:av2a_lrw} show our results on datasets with unseen speaker splits. On GRID (33 speakers, unseen)  (Table \ref{table:av2a_grid_unseen_speakers}) AV2A-WaveGAN with modality dropout produces the highest PESQ and STOI, while the equivalent model with audio pre-training produces the highest ESTOI. However, the End-to-end WGAN in \cite{end_to_end_video_to_speech_synthesis_using_gans} reports a lower WER than our models. We note that AV2A-WaveGAN shows a reduction in WER compared to the base V2A-WaveGAN model. AV2A-MelSpec-S with modality dropout (GT audio) produces the highest reconstruction metrics among all comparable methods, while the equivalent model with audio pre-training shows the best WER.

% ********************* GRID unseen speakers ************************* %
\begin{table}[h]
\captionsetup{justification=centering}
\caption{Results on GRID (33 speakers, unseen)}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method}                    & \textbf{PESQ$\uparrow$} & \textbf{STOI$\uparrow$} & \textbf{ESTOI$\uparrow$} & \textbf{WER (\%)$\downarrow$} \\ \midrule \midrule
\textbf{Raw waveform models} \\ \midrule \midrule
End-to-end WGAN (2018) \cite{video_driven_speech_reconstruction_using_gans}                           & 1.26             & 0.494             & 0.198              & 32.76                 \\
End-to-end WGAN (2022) \cite{end_to_end_video_to_speech_synthesis_using_gans}                            & 1.37             & 0.568             & 0.289              & \textbf{16.05}                 \\

\midrule
V2A-WaveGAN \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.43          & 0.589         & 0.316      & 19.88 \\
\midrule
AV2A-WaveGAN & 1.42          & 0.592         & 0.325        & 18.97              \\
\hspace{3mm} + modality dropout & \textbf{1.45}          & \textbf{0.598}         & 0.332        & 18.32              \\
\hspace{3mm} + modality dropout (GT audio)& 1.44          & 0.597         & 0.327        & 18.83              \\

\midrule
V2A-WaveGAN with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.43          & 0.595         & 0.326          & 17.63              \\
\midrule
AV2A-WaveGAN with audio pre-training & 1.42          & 0.595         & 0.330        & 18.50             \\
\hspace{3mm} + modality dropout & 1.42          & 0.595         & 0.330        & 18.13              \\
\hspace{3mm} + modality dropout (GT audio)& 1.43          & 0.593         & \textbf{0.337}        & 17.70              \\

\midrule \midrule
\textbf{Acoustic features models} \\ \midrule \midrule
Vid2Voc \cite{vocoder_based_speech_synthesis}   & 1.26             & 0.541             & 0.227              & 38.15 \\
VCA-GAN \cite{lip_to_speech_synthesis_with_visual_context_attention_gan}                            & 1.39             & 0.570             & 0.283              & 24.52                 \\
Visual Voice Memory \cite{speech_reconstruction_visual_voice_memory}          & 1.33             & 0.531             & 0.271              & 26.11                 \\
SVTS-S \cite{svts}                             & 1.40             & 0.588             & 0.318              & 17.84 \\

\midrule
V2A-MelSpec-S \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.40          & 0.594         & 0.322          & 18.00              \\
\midrule
AV2A-MelSpec-S & 1.43          & 0.602         & 0.340        & 17.69              \\
\hspace{3mm} + modality dropout & 1.40          & 0.599         & 0.332        & 18.44              \\
\hspace{3mm} + modality dropout (GT audio)& \textbf{1.45}          & \textbf{0.604}         & \textbf{0.342}        & 18.04              \\

\midrule
V2A-MelSpec-S with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.43          & 0.598         & 0.335          & 17.90              \\
\midrule
AV2A-MelSpec-S with audio pre-training & 1.42          & 0.596         & 0.333        & 18.39              \\
\hspace{3mm} + modality dropout & 1.44          & 0.599         & 0.334        & 18.23              \\
\hspace{3mm} + modality dropout (GT audio)& 1.44          & 0.595         & 0.337        & \textbf{17.74}              \\

\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:av2a_grid_unseen_speakers}
\end{table}


% ******************************* LRW *********************************** %
\begin{table}[h]
\captionsetup{justification=centering}
\caption{Results on LRW}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method}                    & \textbf{PESQ$\uparrow$} & \textbf{STOI$\uparrow$} & \textbf{ESTOI$\uparrow$} & \textbf{WER (\%)$\downarrow$} \\ \midrule \midrule
\textbf{Raw waveform models} \\ \midrule \midrule
End-to-end WGAN (2022) \cite{end_to_end_video_to_speech_synthesis_using_gans}                            & 1.33             & 0.552             & 0.331              & 42.38                 \\
\midrule
V2A-WaveGAN \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.46          & 0.623         & 0.445        & 29.79 \\
\midrule
AV2A-WaveGAN & 1.47          & 0.623         & 0.446        & 30.73              \\
\hspace{3mm} + modality dropout & 1.46          & 0.623         & 0.450        & 27.47              \\
\hspace{3mm} + modality dropout (GT audio)& 1.48          & 0.632         & 0.459        & \textbf{24.96}              \\

\midrule
V2A-WaveGAN with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.47          & 0.630         & 0.443          & 29.88             \\
\midrule
AV2A-WaveGAN with audio pre-training & 1.47          & 0.630         & 0.447        & 31.07              \\
\hspace{3mm} + modality dropout & \textbf{1.48}          & 0.626         & 0.447        & 28.19              \\
\hspace{3mm} + modality dropout (GT audio)& \textbf{1.48}          & \textbf{0.637}         & \textbf{0.463}        & 26.37              \\

\midrule \midrule
\textbf{Acoustic features models} \\ \midrule \midrule
VCA-GAN \cite{lip_to_speech_synthesis_with_visual_context_attention_gan}                            & 1.34             & 0.565             & 0.364              & 37.07                 \\
Lip2Wav \cite{lip2wav}          & 1.20             & 0.543             & 0.344              & 34.20\footnotemark[2]                 \\
SVTS-M \cite{svts}                             & 1.46             & 0.649             & 0.482              & \textbf{12.90}                 \\

\midrule
V2A-MelSpec-M \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.48          & 0.649         & 0.484          & 14.96              \\
\midrule
AV2A-MelSpec-M & \textbf{1.49}          & \textbf{0.656}         & \textbf{0.494}        & 14.24              \\
\hspace{3mm} + modality dropout & 1.48          & 0.653         & 0.486        & 15.20              \\
\hspace{3mm} + modality dropout (GT audio)& 1.47          & 0.652         & 0.484        & 15.52              \\

\midrule
V2A-MelSpec-M with audio pre-training \cite{large_scale_unsupervised_audio_pretraining_for_v2a} & 1.46          & 0.646         & 0.476          & 18.70              \\
\midrule
AV2A-MelSpec-M with audio pre-training & 1.47          & 0.652         & 0.483        & 18.44              \\
\hspace{3mm} + modality dropout & 1.47          & 0.648         & 0.476        & 19.41              \\
\hspace{3mm} + modality dropout (GT audio)& 1.46          & 0.648         & 0.474        & 19.96              \\

\bottomrule
\end{tabular}
\end{adjustbox}
\vfill
\hspace{3mm} \\
$^2$Reported using Google speech-to-text API
\label{table:av2a_lrw}
\end{table}

Finally, Table \ref{table:av2a_lrw} shows the results on LRW. AV2A-WaveGAN with audio pre-training and modality dropout (GT audio) reports the best reconstruction metrics among raw waveform models, but the equivalent model without audio pre-training show a lower WER. AV2A-MelSpec-M also outperforms comparable works across reconstruction metrics, but SVTS-M reports a lower WER. 