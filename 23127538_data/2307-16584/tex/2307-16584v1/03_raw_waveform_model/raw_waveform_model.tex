\label{section:av2a_raw_waveform_models}

% Figure environment removed

Fig. \ref{fig:av2a_raw_waveform_high_level} shows an overview of the structure of the video-to-audio models we are considering in the raw waveform domain. We employ V2A-WaveGAN as the base model, introduced in our previous work\cite{large_scale_unsupervised_audio_pretraining_for_v2a} and illustrated in Fig. \ref{fig:av2a_raw_waveform_high_level}(a). It is composed of a Generator (a video frames encoder and a speaker encoder, followed by a bidirectional LSTM and a convolutional decoder) and a convolutional Discriminator, to improve the realism of the reconstructed waveforms. The video frames encoder is based on the ResNet-18 architecture (as in \cite{deep_residual_learning_for_image_recognition, visual_speech_recognition_for_multiple_languages_in_the_wild, end_to_end_video_to_speech_synthesis_using_gans}) and the speaker encoder is pre-trained on large-scale speech corpora\cite{real_time_voice_cloning}. We then construct the corresponding audio-visual-to-audio model, AV2A-WaveGAN, by adding an audio encoder to receive the synthesized raw waveforms and by modifying the input dimensionality of the temporal module.

\subsection{Video frames encoder}

% Figure environment removed

To encode the RGB video frames (Fig. \ref{video_frames_encoder_simplified}) we use a 3D spatio-temporal convolution layer, with batch normalization, a PReLU (Parametric Rectified Linear Unit) activation function and max pooling. By using a receptive field of 5 frames, and centering on the current video frame (timestep) this layer has a temporal context of 2 future and 2 past frames. This is followed by a 2D Resnet-18 comprised of 4 blocks of 4 convolutional residual stacks each. Finally, the extracted features are passed to an adaptive average pooling layer. An input video frame of arbitrary height and width is thus encoded into a $512$-D vector.

\subsection{Speaker encoder}
We include a speaker identity module that encodes information about the speaker's biometric characteristics, following \cite{svts, large_scale_unsupervised_audio_pretraining_for_v2a}, and inspired by multi-speaker TTS\cite{transfer_learning_from_speaker_verification}. As in \cite{svts, large_scale_unsupervised_audio_pretraining_for_v2a}, we encode a speaker's voice using the pre-trained speaker encoder of \cite{real_time_voice_cloning, real_time_voice_cloning_github}, trained on the task of speaker verification on VoxCeleb1\cite{voxceleb1_paper}, VoxCeleb2 \cite{voxceleb2_paper} and LibriSpeech \cite{librispeech}. The resulting $256$-D representation is known as a d-vector. To compute a speaker embedding for a given input video, an audio sample from the same speaker (but corresponding to a different video) is selected at random and encoded into a d-vector. The parameters of the speaker encoder are frozen during training.

\subsection{Video-to-audio Generator}
\label{subsection:av2a_raw_waveform_models_v2a_generator}

Given an input video of $N$ frames and a randomly selected audio sample from the same speaker, the video frames and speaker encoders produce a sequence of $N$ $512$-D visual features and a $256$-D speaker embedding respectively. The speaker embedding is then concatenated at each video timestep resulting in a sequence of $N$ $768$-D features passed as input to the temporal module. This consists of a 2-layer bidirectional LSTM and outputs a sequence of $N$ $512$-D temporal features.

The temporal features are then fed to the decoder, consisting of a convolution, a transposed convolution block and 5 residual blocks followed by a final tanh activation function. Each residual block contains two residual stacks followed by Leaky ReLU and a convolution block (Fig. \ref{av2a_raw_waveform_encoder_and_decoder_simplified}). To imprint an inductive bias of temporal correlation in the audio signal\cite{melgan} we employ dilated convolutions\cite{wavenet} in the residual stacks. The dilation factor increases with increasing number of stacks, in order to increase the induced receptive field for each output timestep. Additionally, the convolution blocks upsample their input, ensuring that each residual block progressively increases the temporal resolution. 

Given a sequence of $N$ temporal features the decoder upsamples them to a raw waveform of $T$ timesteps. Thus, with an audio-video pair sampled at a video frame rate of 25 frames per second and an audio sampling rate of 24 kHz, the decoder outputs an audio signal of 960 timesteps per video timestep.

\subsection{Audio-visual-to-audio Generator}
\label{subsection:av2a_raw_waveform_models_av2a_generator}

% Figure environment removed

We construct the audio-visual-to-audio Generator by appending an audio encoder to the video-to-audio Generator, which receives the synthesized raw waveforms as input and outputs a lower-dimensional representation, as shown in Fig. \ref{fig:av2a_raw_waveform_high_level} (right). We also increase the input dimensionality of the temporal module to accommodate the increased dimensionality of the concatenated input features.

The audio encoder consists of a transposed convolution layer with batch normalization, followed by two residual blocks, a Leaky ReLU activation and a final transposed convolution layer with the tanh activation. This is inspired from the raw waveform encoder used in \cite{large_scale_unsupervised_audio_pretraining_for_v2a} which used 5 residual blocks compared to 2 in this work. Given an input raw waveform sampled at 24 kHz the audio encoder produces a sequence of $256$-D features at a rate of 25 features per second, in line with the sampling rate of the video in our experiments.

Given an input video of $N$ frames, sampled at 25 frames per second, the corresponding synthesized raw waveform of $T$ timesteps, sampled at 24 kHz, the video frames encoder and audio encoder produce sequences of $N$ $512$-D and $256$-D features respectively. These are concatenated, along with an extracted $256$-D speaker embedding at each timestep, to produce an $N$ $1024$-D sequence of features. As with V2A-WaveGAN\cite{large_scale_unsupervised_audio_pretraining_for_v2a}, the temporal module is a 2-layer bidirectional LSTM, modified with input dimensionality of 1024, producing a sequence of $N$ $512$-D features. The decoder remains the same as in the video-to-audio model. 

\subsection{Discriminator}
\label{subsubsection:av2a_raw_waveform_generation_discriminator}

% Figure environment removed

We employ the multi-scale discriminator architecture of MelGAN \cite{melgan} (Fig. \ref{av2a_raw_waveform_discriminator}) as in \cite{large_scale_unsupervised_audio_pretraining_for_v2a}, for both V2A-WaveGAN and AV2A-WaveGAN. The Discriminator contains 3 networks of identical architecture, where each network computes a low-dimensional representation of an input waveform at some given scale.

The first discriminator operates at the scale of the original waveform, while we downsample by 2x for each subsequent discriminator. By employing multiple discriminators at different scales one may capture structures in raw audio that are present in different frequencies \cite{melgan}. In addition, it has been observed that using only one discriminator on raw waveforms results in the generator producing metallic audio \cite{melgan}. Following \cite{melgan} we apply weight normalization to all Discriminator layers.

\subsection{Loss function}
\label{subsection:av2a_raw_waveform_generation_loss_function}
Both V2A-WaveGAN and AV2A-WaveGAN are trained using the LS-GAN loss \cite{lsgan}, as in \cite{large_scale_unsupervised_audio_pretraining_for_v2a}, defined as follows for the Generator and the multi-scale discriminator respectively:

\begin{equation}
\label{eq:gen_adv_loss}
L_G = \mathbb{E}_{\mathbf{\Tilde{x}} \sim \mathbb{P}_G}\bigg{[}\sum_{k = 1}^{K}(D_k(\mathbf{\Tilde{x}})-1)^2\bigg{]}
\end{equation}

\begin{equation}
\label{eq:disc_adv_loss}
L_D = \mathbb{E}_{\mathbf{x} \sim \mathbb{P}_X}\bigg{[}\sum_{k = 1}^{K} (D_k(\mathbf{x})-1)^2\bigg{]} + \mathbb{E}_{\mathbf{\Tilde{x}} \sim \mathbb{P}_G}\bigg{[}\sum_{k = 1}^{K}D_k(\mathbf{\Tilde{x}})^2\bigg{]}
\end{equation}

\noindent
where $G$ is the Generator, $D$ is the multi-scale discriminator, $D_k$ is the $k$th discriminator for $k=1, 2, ... K$ scales, $\mathbf{x} \sim \mathbb{P}_X$ are samples from the data distribution and $\mathbf{\Tilde{x}} \sim \mathbb{P}_G$ are samples from the Generator's distribution.

In addition, we include two reconstruction losses to train the Generator \cite{large_scale_unsupervised_audio_pretraining_for_v2a}. The first is the multi-resolution STFT loss \cite{parallel_wavegan}. The STFT loss for a single resolution, $L_S$, is defined as:

\begin{gather}
\label{eq:single_stft_loss}
L_S(\mathbf{x}, \mathbf{\Tilde{x}}) = L_{SC}(\mathbf{x}, \mathbf{\Tilde{x}}) + L_{MAG}(\mathbf{x}, \mathbf{\Tilde{x}}) \\
L_{SC}(\mathbf{x}, \mathbf{\Tilde{x}}) = \frac{|| \ |STFT(\mathbf{x})| \ ||_F - || \ |STFT(\mathbf{\Tilde{x}})| \ ||_F}{|| \ |STFT(\mathbf{x})| \ ||_F} \\
L_{MAG}(\mathbf{x}, \mathbf{\Tilde{x}}) = \frac{1}{n}|| \ log|STFT(\mathbf{x})| - log|STFT(\Tilde{\mathbf{x}})| \ ||_1
\end{gather}

\noindent
and consists of the spectral convergence loss $L_{SC}$ and the log-STFT magnitude loss $L_{MAG}$, where $||\cdot||_F$ $||\cdot||_1$ are the Frobenius and L1 norms respectively and $n$ is the number of elements in the spectrogram.

By combining $M$ STFT losses with different analysis parameters (e.g. FFT size, window size, hop size) one obtains the multi-resolution STFT loss:

\begin{equation}
\label{eq:multi_resolution_stft_loss}
L_{MR\_ STFT}(\mathbf{x}, \mathbf{\Tilde{x}}) = \frac{1}{M}\sum_{m=1}^{M}L_S^{(m)}(\mathbf{x}, \mathbf{\Tilde{x}})
\end{equation}

\noindent
where the $m = 1, 2, ..., M$ denotes the $m$th set of STFT analysis parameters.

The second reconstruction loss is the MFCC loss, introduced in \cite{end_to_end_video_to_speech_synthesis_using_gans}, which aims to increase intelligibility and accuracy of the generated speech as MFCCs (mel-frequency cepstral coefficients \cite{mfcc_paper}) are often used in speech and emotion recognition \cite{end_to_end_video_to_speech_synthesis_using_gans}. It is defined as:

\begin{equation}
\label{eq:mfcc_loss}
L_{MFCC}(\mathbf{x}, \mathbf{\Tilde{x}}) = ||MFCC(\mathbf{x}) - MFCC(\mathbf{\Tilde{x}})||_1
\end{equation}

\noindent
where the $MFCC$ function extracts 25 MFCCs from the raw waveform.

Finally, the Generator's loss function combines the adversarial loss with the aforementioned two reconstruction losses:

\begin{equation}
\label{eq:v2a_wavegan_generator_loss}
L_{GEN} = \lambda_1 L_G + \lambda_2 L_{MR\_ STFT} + \lambda_3 L_{MFCC}
\end{equation}

\noindent
where $\lambda_1, \lambda_2, \lambda_3 >0$ are hyperparameters.

In \cite{large_scale_unsupervised_audio_pretraining_for_v2a}, and following \cite{end_to_end_video_to_speech_synthesis_using_gans}, we tune these coefficients sequentially by incrementally finding the values that yield the lowest word error rate on the validation set of GRID (4 speakers, seen). This yields $\lambda_1 = 1.0$, $\lambda_2 = 80.0$, $\lambda_3 = 15.0$. As these coefficients were employed in V2A-WaveGAN, we thus employ them in our AV2A-WaveGAN experiments as well.