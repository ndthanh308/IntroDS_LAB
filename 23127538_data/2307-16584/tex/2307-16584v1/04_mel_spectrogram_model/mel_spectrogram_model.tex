% Figure environment removed

Fig. \ref{fig:av2a_melspec_high_level} shows an overview of the structure of the video-to-audio models we are considering in the mel spectrogram domain. We employ V2A-MelSpec as the base model, introduced in our previous work\cite{large_scale_unsupervised_audio_pretraining_for_v2a}. It contains a Generator composed of a video frames encoder and a speaker encoder, followed by a temporal module that upsamples across the temporal axis and a decoder that outputs mel spectrograms. The video frames encoder and speaker encoder consist of the same architectures as in the raw waveform generators in Section \ref{section:av2a_raw_waveform_models}. The temporal module is composed of an LSTM - temporal upsampling - LSTM sequence of layers and the decoder consists of a conformer-based model and a linear projection. We  construct the corresponding audio-visual-to-audio model, AV2A-MelSpec, by adding an audio encoder (one Linear layer) that is fed the synthesized mel spectrograms. In addition, as in Section \ref{section:av2a_raw_waveform_models}, we modify the temporal module to account for the new dimensionality of the inputs as well as the different temporal resolutions of the mel spectrogram and the video frames.

\subsection{Video-to-audio Generator}

As with raw waveform generation in Section \ref{section:av2a_raw_waveform_models}, the video frames encoder receives an input video of $N$ frames and outputs a sequence of $N$ $512$-D visual features. The speaker encoder also produces a $256$-D speaker embedding from another audio sample of the same speaker selected at random. By concatenating the speaker embedding with the visual features at each timestep the resulting sequence of $N$ $768$-D features is fed to the temporal module. The latter is composed of a 1-layer bidirectional LSTM followed by temporal upsampling and another bidirectional LSTM and outputs a sequence of $N$ $768$-D temporal features.

The decoder, consisting of a Linear layer followed by dropout, $B$ conformer blocks and a final linear layer, receives the temporal features and outputs a mel spectrogram. Fig. \ref{av2a_melspec_encoder_and_decoder_simplified} (left) and Table \ref{table:av2a_melspec_decoder_architectures} illustrate the decoder architecture and the conformer block configurations. Inspired by the originally proposed Conformer \cite{conformer} and in line with with previous works \cite{svts, large_scale_unsupervised_audio_pretraining_for_v2a}, the number of conformer blocks $B$ is set as a hyperparameter in proportion to the size of the training dataset. This results in three versions of the decoder, and thus the Generator.



\begin{table}[h]
\captionsetup{justification=centering}
\caption{Summary of V2A-MelSpec / AV2A-MelSpec \\ decoder architectures}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{c}{Model} & \begin{tabular}[c]{@{}c@{}}V2A-MelSpec \slash\\AV2A-MelSpec\\ (VS)\end{tabular} & \begin{tabular}[c]{@{}c@{}}V2A-MelSpec \slash\\AV2A-MelSpec\\ (S)\end{tabular} & \begin{tabular}[c]{@{}c@{}}V2A-MelSpec \slash\\AV2A-MelSpec\\ (M)\end{tabular} \\ \midrule
Conformer blocks          & 2                                                          & 6                                                         & 12                                                        \\
Attention dim.            & 256                                                        & 256                                                       & 256                                                       \\
Attention heads           & 4                                                          & 4                                                         & 4                                                         \\
Conv. kernel size         & 31                                                         & 31                                                        & 31                                                        \\
Feedforward dim.          & 2048                                                       & 2048                                                      & 2048                                                      \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{table:av2a_melspec_decoder_architectures}
\end{table}

\subsection{Audio-visual-to-audio Generator}
To construct the audio-visual-to-audio Generator we append a Linear layer as the audio encoder (following \cite{av_hubert, la_voce}), to the video-to-audio Generator. This receives synthesized mel spectrograms as input and produces a lower-dimensional representation. By using a simple, lightweight audio encoder we prevent the Generator from relying excessively on the audio input \cite{av_hubert}, i.e. from trivially ignoring the visual modality and learning an identity mapping.

Given a synthesized mel spectrogram of $T$ timesteps and $80$ frequency bins, the audio encoder outputs a sequence of $T$ $64$-D features. As the video frames encoder outputs a sequence of $N$ $512$-D features (corresponding to $N$ video frames, sampled at 25 frames per second, $N<T$), we need to align the temporal resolutions of the audio and video representations prior to concatenating them. Thus, we upsample the visual features from $N$ to $T$ timesteps using nearest-neighbor upsampling along the time axis. We then concatenate these as well as the $256$-D extracted speaker embedding at each timestep, resulting in a sequence of $T$ $832$-D features. These are then fed to a bidirectional LSTM outputting a sequence of $T$ $768$-D temporal features to be fed to the decoder. The decoder has the same architecture as in the video-to-audio model.

To generate the raw waveforms from the reconstructed mel spectrograms we use HiFiGAN\cite{hifi_gan}, a neural vocoder pre-trained on the LibriTTS\cite{libri_tts} dataset containing more than 500 hours of speech data from audiobooks. The pre-trained model is publicly available\footnotemark.

\footnotetext{\texttt{\url{https://github.com/kan-bayashi/ParallelWaveGAN}}}

\subsection{Loss function}
\label{subsection:av2a_melspec_generation_loss_function}

We employ an L1 loss on the mel spectrograms \cite{large_scale_unsupervised_audio_pretraining_for_v2a}:

\begin{equation}
\label{eq:v2a_melspec_generator_loss}
L_{GEN}(\mathbf{X}, \mathbf{\Tilde{X}}) = || \mathbf{X} - \mathbf{\Tilde{X}} ||_1
\end{equation}

\noindent
where $\mathbf{X}$ and $\mathbf{\Tilde{X}}$ denote the ground truth and generated mel spectrograms, respectively.

% Figure environment removed