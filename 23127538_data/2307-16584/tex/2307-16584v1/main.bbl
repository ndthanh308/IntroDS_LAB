% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{driven_to_distraction}
R.~Young and J.~Zhang, ``Driven to distraction? a review of speech technologies
  in the automobile,'' \emph{J Assoc Voice Interact Design}, vol.~2, no.~1, pp.
  1--34, 2017.

\bibitem{manifestation_of_virtual_assistants}
R.~Rawassizadeh, T.~Sen, S.~J. Kim, C.~Meurisch, H.~Keshavarz,
  M.~M{\"u}hlh{\"a}user, and M.~Pazzani, ``Manifestation of virtual assistants
  and robots into daily life: Vision and challenges,'' \emph{CCF Transactions
  on Pervasive Computing and Interaction}, vol.~1, pp. 163--174, 2019.

\bibitem{intelligent_voice_bots_for_digital_banking}
R.~Kaur, R.~S. Sandhu, A.~Gera, T.~Kaur, and P.~Gera, ``Intelligent voice bots
  for digital banking,'' in \emph{Smart Systems and IoT: Innovations in
  Computing: Proceeding of SSIC 2019}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2020, pp. 401--408.

\bibitem{deep_av_speech_recognition}
T.~Afouras, J.~S. Chung, A.~Senior, O.~Vinyals, and A.~Zisserman, ``Deep
  audio-visual speech recognition,'' \emph{IEEE Transactions on Pattern
  Analysis and Machine Intelligence}, pp. 1--1, 2018.

\bibitem{end_to_end_av_speech_recognition}
S.~Petridis, T.~Stafylakis, P.~Ma, F.~Cai, G.~Tzimiropoulos, and M.~Pantic,
  ``End-to-end audiovisual speech recognition,'' in \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, 2018, pp. 6548--6552.

\bibitem{end_to_end_av_speech_recognition_with_conformers}
P.~Ma, S.~Petridis, and M.~Pantic, ``End-to-end audio-visual speech recognition
  with conformers,'' in \emph{ICASSP 2021 - 2021 IEEE International Conference
  on Acoustics, Speech and Signal Processing (ICASSP)}, 2021, pp. 7613--7617.

\bibitem{av_speech_recognition_using_bimodal_trained_bottleneck_features}
Y.~Takashima, R.~Aihara, T.~Takiguchi, Y.~Ariki, N.~Mitani, K.~Omori, and
  K.~Nakazono, ``Audio-visual speech recognition using bimodal-trained
  bottleneck features for a person with severe hearing loss.'' in
  \emph{Interspeech}, 2016, pp. 277--281.

\bibitem{av_speech_recognition_with_hybrid_ctc_attention_architecture}
S.~Petridis, T.~Stafylakis, P.~Ma, G.~Tzimiropoulos, and M.~Pantic,
  ``Audio-visual speech recognition with a hybrid {CTC/Attention}
  architecture,'' \emph{2018 IEEE Spoken Language Technology Workshop (SLT)},
  pp. 513--520, 2018.

\bibitem{deep_learning_based_automated_lipreading_a_survey}
S.~Fenghour, D.~Chen, K.~Guo, B.~Li, and P.~Xiao, ``Deep learning-based
  automated lip-reading: A survey,'' \emph{IEEE Access}, vol.~9, pp.
  121\,184--121\,205, 2021.

\bibitem{review_on_research_progress_of_machine_lipreading}
G.~Pu and H.~Wang, ``Review on research progress of machine lip reading,''
  \emph{The Visual Computer}, pp. 1--17, 2022.

\bibitem{video_driven_speech_reconstruction_using_gans}
K.~Vougioukas, P.~Ma, S.~Petridis, and M.~Pantic, ``Video-driven speech
  reconstruction using generative adversarial networks,'' in
  \emph{INTERSPEECH}, 2019.

\bibitem{end_to_end_video_to_speech_synthesis_using_gans}
R.~Mira, K.~Vougioukas, P.~Ma, S.~Petridis, B.~W. Schuller, and M.~Pantic,
  ``End-to-end video-to-speech synthesis using generative adversarial
  networks,'' \emph{IEEE Transactions on Cybernetics}, vol.~53, no.~6, pp.
  3454--3466, 2023.

\bibitem{lip_to_speech_synthesis_with_visual_context_attention_gan}
M.~Kim, J.~Hong, and Y.~M. Ro, ``Lip to speech synthesis with visual context
  attentional {GAN},'' \emph{Advances in Neural Information Processing
  Systems}, vol.~34, 2021.

\bibitem{svts}
R.~Mira, A.~Haliassos, S.~Petridis, B.~W. Schuller, and M.~Pantic, ``{SVTS}:
  Scalable video-to-speech synthesis,'' in \emph{Proc. Interspeech 2022}, 2022,
  pp. 1836--1840.

\bibitem{ephrat2017vid2speech}
A.~Ephrat and S.~Peleg, ``{Vid2Speech}: speech reconstruction from silent
  video,'' in \emph{2017 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, 2017.

\bibitem{av_event_recognition_in_surveillance}
M.~Cristani, M.~Bicego, and V.~Murino, ``Audio-visual event recognition in
  surveillance video sequences,'' \emph{IEEE Transactions on Multimedia},
  vol.~9, no.~2, pp. 257--267, 2007.

\bibitem{silent_speech_interfaces}
\BIBentryALTinterwordspacing
B.~Denby, T.~Schultz, K.~Honda, T.~Hueber, J.~Gilbert, and J.~Brumberg,
  ``Silent speech interfaces,'' \emph{Speech Communication}, vol.~52, no.~4,
  pp. 270--287, 2010, silent Speech Interfaces. [Online]. Available:
  \url{https://www.sciencedirect.com/science/article/pii/S0167639309001307}
\BIBentrySTDinterwordspacing

\bibitem{lip2wav}
K.~R. Prajwal, R.~Mukhopadhyay, V.~P. Namboodiri, and C.~Jawahar, ``Learning
  individual speaking styles for accurate lip to speech synthesis,'' in
  \emph{The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)}, June 2020.

\bibitem{speech_reconstruction_visual_voice_memory}
J.~Hong, M.~Kim, S.~J. Park, and Y.~M. Ro, ``Speech reconstruction with
  reminiscent sound via visual voice memory,'' \emph{IEEE/ACM Transactions on
  Audio, Speech, and Language Processing}, vol.~29, pp. 3654--3667, 2021.

\bibitem{lipsound2}
L.~Qu, C.~Weber, and S.~Wermter, ``Lip{S}ound2: Self-supervised pre-training
  for lip-to-speech reconstruction and lip reading,'' \emph{IEEE Transactions
  on Neural Networks and Learning Systems}, pp. 1--11, 2022.

\bibitem{av_hubert}
\BIBentryALTinterwordspacing
B.~Shi, W.-N. Hsu, K.~Lakhotia, and A.~Mohamed, ``Learning audio-visual speech
  representation by masked multimodal cluster prediction,'' in
  \emph{International Conference on Learning Representations}, 2022. [Online].
  Available: \url{https://openreview.net/forum?id=Z1Qlm11uOM}
\BIBentrySTDinterwordspacing

\bibitem{deep_multi_scale_video_prediction_beyond_mse}
\BIBentryALTinterwordspacing
M.~Mathieu, C.~Couprie, and Y.~LeCun, ``Deep multi-scale video prediction
  beyond mean square error,'' in \emph{4th International Conference on Learning
  Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016,
  Conference Track Proceedings}, Y.~Bengio and Y.~LeCun, Eds., 2016. [Online].
  Available: \url{http://arxiv.org/abs/1511.05440}
\BIBentrySTDinterwordspacing

\bibitem{image_to_image_translation_with_cgans}
P.~Isola, J.-Y. Zhu, T.~Zhou, and A.~A. Efros, ``Image-to-image translation
  with conditional adversarial networks,'' in \emph{Proceedings of the IEEE
  conference on computer vision and pattern recognition}, 2017, pp. 1125--1134.

\bibitem{batch_norm}
\BIBentryALTinterwordspacing
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in \emph{Proceedings of the
  32nd International Conference on Machine Learning}, ser. Proceedings of
  Machine Learning Research, F.~Bach and D.~Blei, Eds., vol.~37.\hskip 1em plus
  0.5em minus 0.4em\relax Lille, France: PMLR, 07--09 Jul 2015, pp. 448--456.
  [Online]. Available: \url{https://proceedings.mlr.press/v37/ioffe15.html}
\BIBentrySTDinterwordspacing

\bibitem{grid_database}
M.~Cooke, J.~Barker, S.~Cunningham, and X.~Shao, ``An audio-visual corpus for
  speech perception and automatic speech recognition (l),'' \emph{The Journal
  of the Acoustical Society of America}, vol. 120, pp. 2421--4, 12 2006.

\bibitem{tcd_timit_databse}
N.~{Harte} and E.~{Gillen}, ``{TCD-TIMIT}: An audio-visual corpus of continuous
  speech,'' \emph{IEEE Transactions on Multimedia}, vol.~17, no.~5, pp.
  603--615, May 2015.

\bibitem{lip_reading_in_the_wild}
J.~S. Chung and A.~Zisserman, ``Lip reading in the wild,'' in \emph{Asian
  conference on computer vision}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2016, pp. 87--103.

\bibitem{visual_contribution_to_speech_intelligibility_in_noise}
W.~H. Sumby and I.~Pollack, ``Visual contribution to speech intelligibility in
  noise,'' \emph{The journal of the acoustical society of america}, vol.~26,
  no.~2, pp. 212--215, 1954.

\bibitem{continuous_optical_automatic_speech_recognition_by_lipreading}
A.~J. Goldschen, O.~N. Garcia, and E.~Petajan, ``Continuous optical automatic
  speech recognition by lipreading,'' in \emph{Proceedings of 1994 28th
  Asilomar Conference on Signals, Systems and Computers}, vol.~1.\hskip 1em
  plus 0.5em minus 0.4em\relax Ieee, 1994, pp. 572--577.

\bibitem{automatic_lipreading_to_enhance_speech_recognition}
E.~D. Petajan, ``Automatic lipreading to enhance speech recognition (speech
  reading),'' Ph.D. dissertation, USA, 1984, aAI8502266.

\bibitem{information_theoretic_feature_extraction_for_av_speech_recognition}
M.~Gurban and J.-P. Thiran, ``Information theoretic feature extraction for
  audio-visual speech recognition,'' \emph{IEEE Transactions on signal
  processing}, vol.~57, no.~12, pp. 4765--4776, 2009.

\bibitem{lipreading_with_local_spatiotemporal_descriptors}
G.~Zhao, M.~Barnard, and M.~Pietikainen, ``Lipreading with local spatiotemporal
  descriptors,'' \emph{IEEE Transactions on Multimedia}, vol.~11, no.~7, pp.
  1254--1265, 2009.

\bibitem{an_image_transform_approach_for_hmm_based_automatic_lipreading}
G.~Potamianos, H.~P. Graf, and E.~Cosatto, ``An image transform approach for
  {HMM} based automatic lipreading,'' in \emph{Proceedings 1998 International
  Conference on Image Processing. ICIP98 (Cat. No. 98CB36269)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 1998, pp. 173--177.

\bibitem{lip_feature_extraction_and_reduction_for_hmm}
S.~Alizadeh, R.~Boostani, and V.~Asadpour, ``Lip feature extraction and
  reduction for {HMM}-based visual speech recognition systems,'' in \emph{2008
  9th International Conference on Signal Processing}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2008, pp. 561--564.

\bibitem{lip_feature_extraction_based_on_improved_jumping_snake_model}
X.~Ma, L.~Yan, and Q.~Zhong, ``Lip feature extraction based on improved
  jumping-snake model,'' in \emph{2016 35th Chinese Control Conference
  (CCC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp. 6928--6933.

\bibitem{profile_view_lip_reading}
K.~Kumar, T.~Chen, and R.~M. Stern, ``Profile view lip reading,'' in \emph{2007
  IEEE International Conference on Acoustics, Speech and Signal
  Processing-ICASSP'07}, vol.~4.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2007, pp. IV--429.

\bibitem{active_appearance_models}
T.~F. Cootes, G.~J. Edwards, and C.~J. Taylor, ``Active appearance models,''
  \emph{IEEE Transactions on pattern analysis and machine intelligence},
  vol.~23, no.~6, pp. 681--685, 2001.

\bibitem{view_independent_computer_lip_reading}
Y.~Lan, B.-J. Theobald, and R.~Harvey, ``View independent computer
  lip-reading,'' in \emph{2012 IEEE International Conference on Multimedia and
  Expo}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2012, pp. 432--437.

\bibitem{insights_into_machine_lip_reading}
Y.~Lan, R.~Harvey, and B.-J. Theobald, ``Insights into machine lip reading,''
  in \emph{2012 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2012, pp.
  4825--4828.

\bibitem{lipreading_using_convolutional_neural_network}
K.~Noda, Y.~Yamaguchi, K.~Nakadai, H.~G. Okuno, and T.~Ogata, ``Lipreading
  using convolutional neural network,'' in \emph{fifteenth annual conference of
  the international speech communication association}, 2014.

\bibitem{deep_learning_of_mouth_shapes_for_sign_language}
O.~Koller, H.~Ney, and R.~Bowden, ``Deep learning of mouth shapes for sign
  language,'' in \emph{Proceedings of the IEEE International Conference on
  Computer Vision Workshops}, 2015, pp. 85--91.

\bibitem{improved_speaker_independent_lip_reading}
I.~Almajai, S.~Cox, R.~Harvey, and Y.~Lan, ``Improved speaker independent lip
  reading using speaker adaptive training and deep neural networks,'' in
  \emph{2016 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp.
  2722--2726.

\bibitem{deep_complementary_bottleneck_features_for_visual_speech_recognition}
S.~Petridis and M.~Pantic, ``Deep complementary bottleneck features for visual
  speech recognition,'' in \emph{2016 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2016, pp. 2304--2308.

\bibitem{lipnet}
Y.~M. Assael, B.~Shillingford, S.~Whiteson, and N.~De~Freitas, ``{LipNet}:
  End-to-end sentence-level lipreading,'' \emph{arXiv preprint
  arXiv:1611.01599}, 2016.

\bibitem{empirical_evaluation_of_gated_rnn_on_sequence_modeling}
J.~Chung, C.~Gulcehre, K.~Cho, and Y.~Bengio, ``Empirical evaluation of gated
  recurrent neural networks on sequence modeling,'' \emph{arXiv preprint
  arXiv:1412.3555}, 2014.

\bibitem{connectionist_temporal_classification}
A.~Graves, S.~Fern{\'a}ndez, F.~Gomez, and J.~Schmidhuber, ``Connectionist
  temporal classification: labelling unsegmented sequence data with recurrent
  neural networks,'' in \emph{Proceedings of the 23rd international conference
  on Machine learning}, 2006, pp. 369--376.

\bibitem{end_to_end_visual_speech_recognition_with_lstms}
S.~Petridis, Z.~Li, and M.~Pantic, ``End-to-end visual speech recognition with
  {LSTMs},'' in \emph{2017 IEEE international conference on acoustics, speech
  and signal processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2017, pp. 2592--2596.

\bibitem{end_to_end_visual_speech_recognition_for_small_scale_datasets}
S.~Petridis, Y.~Wang, P.~Ma, Z.~Li, and M.~Pantic, ``End-to-end visual speech
  recognition for small-scale datasets,'' \emph{Pattern Recognition Letters},
  vol. 131, pp. 421--427, 2020.

\bibitem{large_scale_visual_speech_recognition}
B.~Shillingford, Y.~Assael, M.~W. Hoffman, T.~Paine, C.~Hughes, U.~Prabhu,
  H.~Liao, H.~Sak, K.~Rao, L.~Bennett \emph{et~al.}, ``Large-scale visual
  speech recognition,'' \emph{arXiv preprint arXiv:1807.05162}, 2018.

\bibitem{combining_residual_networks_with_lstms_for_lipreading}
\BIBentryALTinterwordspacing
T.~Stafylakis and G.~Tzimiropoulos, ``Combining residual networks with {LSTMs}
  for lipreading,'' in \emph{Proc. Interspeech 2017}, 2017, pp. 3652--3656.
  [Online]. Available: \url{http://dx.doi.org/10.21437/Interspeech.2017-85}
\BIBentrySTDinterwordspacing

\bibitem{reconstructing_intelligible_audio_speech_from_visual_speech_features}
T.~L. Cornu and B.~Milner, ``{Reconstructing intelligible audio speech from
  visual speech features},'' in \emph{Proc. Interspeech 2015}, 2015, pp.
  3355--3359.

\bibitem{generating_intelligible_audio_speech_from_visual_speech}
T.~Le~Cornu and B.~Milner, ``Generating intelligible audio speech from visual
  speech,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, vol.~25, no.~9, pp. 1751--1761, 2017.

\bibitem{straight_vocoder}
H.~Kawahara, I.~Masuda-Katsuse, and A.~De~Cheveigne, ``Restructuring speech
  representations using a pitch-adaptive time--frequency smoothing and an
  instantaneous-frequency-based {F0} extraction: Possible role of a repetitive
  structure in sounds,'' \emph{Speech communication}, vol.~27, no. 3-4, pp.
  187--207, 1999.

\bibitem{improved_speech_reconstruction_from_silent_video}
A.~Ephrat, T.~Halperin, and S.~Peleg, ``Improved speech reconstruction from
  silent video,'' \emph{2017 IEEE International Conference on Computer Vision
  Workshops (ICCVW)}, pp. 455--462, 2017.

\bibitem{griffin_lim}
D.~Griffin and J.~Lim, ``Signal estimation from modified short-time {F}ourier
  transform,'' \emph{IEEE Transactions on acoustics, speech, and signal
  processing}, vol.~32, no.~2, pp. 236--243, 1984.

\bibitem{lip2audspec}
H.~Akbari, H.~Arora, L.~Cao, and N.~Mesgarani, ``{Lip2AudSpec}: Speech
  reconstruction from silent lip movements video,'' \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, pp. 2516--2520, 2018.

\bibitem{vocoder_based_speech_synthesis}
D.~Michelsanti, O.~Slizovskaia, G.~Haro, E.~Gómez, Z.-H. Tan, and J.~Jensen,
  ``Vocoder-based speech synthesis from silent videos,'' in \emph{Proc.
  Interspeech 2020}, 2020, pp. 3530--3534.

\bibitem{tacotron2}
J.~Shen, R.~Pang, R.~J. Weiss, M.~Schuster, N.~Jaitly, Z.~Yang, Z.~Chen,
  Y.~Zhang, Y.~Wang, R.~Skerrv-Ryan \emph{et~al.}, ``Natural {TTS} synthesis by
  conditioning wavenet on mel spectrogram predictions,'' in \emph{2018 IEEE
  international conference on acoustics, speech and signal processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 4779--4783.

\bibitem{vae_paper}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' \emph{arXiv
  preprint arXiv:1312.6114}, 2013.

\bibitem{speech_prediction_in_silent_videos_using_vaes}
R.~Yadav, A.~Sardana, V.~P. Namboodiri, and R.~M. Hegde, ``Speech prediction in
  silent videos using variational autoencoders,'' in \emph{ICASSP 2021 - 2021
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, 2021, pp. 7048--7052.

\bibitem{key_value_memory_networks}
\BIBentryALTinterwordspacing
A.~H. Miller, A.~Fisch, J.~Dodge, A.~Karimi, A.~Bordes, and J.~Weston,
  ``Key-value memory networks for directly reading documents,'' in
  \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2016, Austin, Texas, USA, November 1-4, 2016},
  J.~Su, X.~Carreras, and K.~Duh, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  The Association for Computational Linguistics, 2016, pp. 1400--1409.
  [Online]. Available: \url{https://doi.org/10.18653/v1/d16-1147}
\BIBentrySTDinterwordspacing

\bibitem{voxceleb2_paper}
J.~S. Chung, A.~Nagrani, and A.~Zisserman, ``{VoxCeleb2}: Deep speaker
  recognition,'' in \emph{Proc. Interspeech 2018}, 2018, pp. 1086--1090.

\bibitem{librispeech}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``{LibriSpeech}: An {ASR}
  corpus based on public domain audio books,'' in \emph{2015 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2015, pp.
  5206--5210.

\bibitem{lrs3}
T.~Afouras, J.~S. Chung, and A.~Zisserman, ``{LRS3-TED}: a large-scale dataset
  for visual speech recognition,'' \emph{arXiv preprint arXiv:1809.00496},
  2018.

\bibitem{wasserstein_gan}
M.~Arjovsky, S.~Chintala, and L.~Bottou, ``Wasserstein generative adversarial
  networks,'' in \emph{International conference on machine learning}.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 214--223.

\bibitem{improved_training_of_wasserstein_gans}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~C. Courville,
  ``Improved training of {W}asserstein {GAN}s,'' \emph{Advances in neural
  information processing systems}, vol.~30, 2017.

\bibitem{large_scale_unsupervised_audio_pretraining_for_v2a}
T.~Kefalas, Y.~Panagakis, and M.~Pantic, ``Large-scale unsupervised audio
  pre-training for video-to-speech synthesis,'' \emph{arXiv preprint
  arXiv:2306.15464}, 2023.

\bibitem{an_overview_of_deep_learning_based_av_speech_enhancement_and_separation}
D.~Michelsanti, Z.-H. Tan, S.-X. Zhang, Y.~Xu, M.~Yu, D.~Yu, and J.~Jensen,
  ``An overview of deep-learning-based audio-visual speech enhancement and
  separation,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, vol.~29, pp. 1368--1396, 2021.

\bibitem{supervised_speech_separation_based_on_deep_learning}
D.~Wang and J.~Chen, ``Supervised speech separation based on deep learning: An
  overview,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, vol.~26, no.~10, pp. 1702--1726, 2018.

\bibitem{hearing_by_eye}
Q.~Summerfield, ``Some preliminaries to a comprehensive account of audio-visual
  speech perception. hearing by eye,'' \emph{The psychology of lip-reading},
  pp. 3--51, 1987.

\bibitem{gan_paper}
\BIBentryALTinterwordspacing
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio, ``Generative adversarial networks,''
  \emph{Commun. ACM}, vol.~63, no.~11, p. 139–144, oct 2020. [Online].
  Available: \url{https://doi.org/10.1145/3422622}
\BIBentrySTDinterwordspacing

\bibitem{audio_visual_enhancement_of_speech_in_noise}
L.~Girin, J.-L. Schwartz, and G.~Feng, ``Audio-visual enhancement of speech in
  noise,'' \emph{The Journal of the Acoustical Society of America}, vol. 109,
  no.~6, pp. 3007--3020, 2001.

\bibitem{audio_visual_segmentation_and_the_cocktail_party_effect}
T.~Darrell, J.~W. Fisher~Iii, and P.~Viola, ``Audio-visual segmentation and
  “the cocktail party effect”,'' in \emph{International Conference on
  Multimodal Interfaces}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2000, pp. 32--40.

\bibitem{source_separation_of_convolutive_and_noisy_mixtures}
Q.~Liu, W.~Wang, P.~J. Jackson, M.~Barnard, J.~Kittler, and J.~Chambers,
  ``Source separation of convolutive and noisy mixtures using audio-visual
  dictionary learning and probabilistic time-frequency masking,'' \emph{IEEE
  Transactions on Signal Processing}, vol.~61, no.~22, pp. 5520--5535, 2013.

\bibitem{speaker_separation_using_visually_derived_binary_masks}
F.~Khan and B.~Milner, ``Speaker separation using visually-derived binary
  masks,'' in \emph{Auditory-Visual Speech Processing (AVSP) 2013}, 2013.

\bibitem{avss_via_hidden_markov_models}
J.~Hershey and M.~Casey, ``Audio-visual sound separation via hidden {M}arkov
  models,'' \emph{Advances in Neural Information Processing Systems}, vol.~14,
  2001.

\bibitem{noisy_audio_feature_enhancement_using_av_speech_data}
R.~Goecke, G.~Potamianos, and C.~Neti, ``Noisy audio feature enhancement using
  audio-visual speech data,'' in \emph{2002 IEEE International Conference on
  Acoustics, Speech, and Signal Processing}, vol.~2.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2002, pp. II--2025.

\bibitem{avse_with_avcdcn}
S.~Deligne, G.~Potamianos, and C.~Neti, ``Audio-visual speech enhancement with
  {AVCDCN} (audio-visual codebook dependent cepstral normalization),'' in
  \emph{Sensor Array and Multichannel Signal Processing Workshop Proceedings,
  2002}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2002, pp. 68--71.

\bibitem{av_graphical_models_for_speech_processing}
J.~Hershey, H.~Attias, N.~Jojic, and T.~Kristjansson, ``Audio-visual graphical
  models for speech processing,'' in \emph{2004 IEEE International Conference
  on Acoustics, Speech, and Signal Processing}, vol.~5.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2004, pp. V--649.

\bibitem{video_assisted_speech_source_separation}
W.~Wang, D.~Cosker, Y.~Hicks, S.~Saneit, and J.~Chambers, ``Video assisted
  speech source separation,'' in \emph{Proceedings.(ICASSP'05). IEEE
  International Conference on Acoustics, Speech, and Signal Processing, 2005.},
  vol.~5.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2005, pp. v--425.

\bibitem{effective_visually_derived_wiener_filtering_for_av_speech_processing}
I.~Almajai and B.~Milner, ``Effective visually-derived {W}iener filtering for
  audio-visual speech processing.'' in \emph{AVSP}, 2009, pp. 134--139.

\bibitem{seeing_through_noise}
A.~Gabbay, A.~Ephrat, T.~Halperin, and S.~Peleg, ``Seeing through noise:
  Visually driven speaker separation and enhancement,'' in \emph{2018 IEEE
  international conference on acoustics, speech and signal processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 3051--3055.

\bibitem{the_conversation_deep_av_speech_enhancement}
\BIBentryALTinterwordspacing
T.~Afouras, J.~S. Chung, and A.~Zisserman, ``The conversation: Deep
  audio-visual speech enhancement,'' in \emph{Proc. Interspeech 2018}, 2018,
  pp. 3244--3248. [Online]. Available:
  \url{http://dx.doi.org/10.21437/Interspeech.2018-1400}
\BIBentrySTDinterwordspacing

\bibitem{time_domain_av_speech_separation}
J.~Wu, Y.~Xu, S.-X. Zhang, L.-W. Chen, M.~Yu, L.~Xie, and D.~Yu, ``Time domain
  audio visual speech separation,'' in \emph{2019 IEEE automatic speech
  recognition and understanding workshop (ASRU)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2019, pp. 667--673.

\bibitem{tasnet}
Y.~Luo and N.~Mesgarani, ``{TaSNet}: time-domain audio separation network for
  real-time, single-channel speech separation,'' in \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 696--700.

\bibitem{muse}
Z.~Pan, R.~Tao, C.~Xu, and H.~Li, ``Muse: Multi-modal target speaker extraction
  with visual cues,'' in \emph{ICASSP 2021-2021 IEEE International Conference
  on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2021, pp. 6678--6682.

\bibitem{visualvoice}
R.~Gao and K.~Grauman, ``{VisualVoice}: Audio-visual speech separation with
  cross-modal consistency,'' in \emph{CVPR}, 2021.

\bibitem{avse_using_conditional_vaes}
M.~Sadeghi, S.~Leglaive, X.~Alameda-Pineda, L.~Girin, and R.~Horaud,
  ``Audio-visual speech enhancement using conditional variational
  auto-encoders,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, vol.~28, pp. 1788--1800, 2020.

\bibitem{visual_speech_enhancement}
A.~Gabbay, A.~Shamir, and S.~Peleg, ``Visual speech enhancement,'' in
  \emph{Proc. Interspeech 2018}, 2018, pp. 1170--1174.

\bibitem{audio_visual_speech_enhancement_using_multimodal_deep_cnns}
J.-C. Hou, S.-S. Wang, Y.-H. Lai, Y.~Tsao, H.-W. Chang, and H.-M. Wang,
  ``Audio-visual speech enhancement using multimodal deep convolutional neural
  networks,'' \emph{IEEE Transactions on Emerging Topics in Computational
  Intelligence}, vol.~2, no.~2, pp. 117--128, 2018.

\bibitem{av_speech_codecs_rethinking_avse}
K.~Yang, D.~Markovi{\'c}, S.~Krenn, V.~Agrawal, and A.~Richard, ``Audio-visual
  speech codecs: Rethinking audio-visual speech enhancement by re-synthesis,''
  in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, 2022, pp. 8227--8237.

\bibitem{la_voce}
R.~Mira, B.~Xu, J.~Donley, A.~Kumar, S.~Petridis, V.~K. Ithapu, and M.~Pantic,
  ``{LA-VocE}: Low-{SNR} audio-visual speech enhancement using neural
  vocoders,'' in \emph{ICASSP 2023-2023 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2023, pp. 1--5.

\bibitem{hifi_gan}
J.~Kong, J.~Kim, and J.~Bae, ``{HiFi-GAN}: Generative adversarial networks for
  efficient and high fidelity speech synthesis,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~33, pp. 17\,022--17\,033, 2020.

\bibitem{deep_residual_learning_for_image_recognition}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{visual_speech_recognition_for_multiple_languages_in_the_wild}
P.~Ma, S.~Petridis, and M.~Pantic, ``Visual speech recognition for multiple
  languages in the wild,'' \emph{Nature Machine Intelligence}, pp. 1--10, 2022.

\bibitem{real_time_voice_cloning}
C.~Jemine, ``Real-time-voice-cloning,'' Tech. Rep., 2019.

\bibitem{transfer_learning_from_speaker_verification}
Y.~Jia, Y.~Zhang, R.~Weiss, Q.~Wang, J.~Shen, F.~Ren, P.~Nguyen, R.~Pang,
  I.~Lopez~Moreno, Y.~Wu \emph{et~al.}, ``Transfer learning from speaker
  verification to multispeaker text-to-speech synthesis,'' \emph{Advances in
  neural information processing systems}, vol.~31, 2018.

\bibitem{real_time_voice_cloning_github}
``Real-time voice cloning,''
  \url{https://github.com/CorentinJ/Real-Time-Voice-Cloning}, accessed:
  2023-04-21.

\bibitem{voxceleb1_paper}
A.~Nagrani, J.~S. Chung, and A.~Zisserman, ``{VoxCeleb}: A large-scale speaker
  identification dataset,'' in \emph{Proc. Interspeech 2017}, 2017, pp.
  2616--2620.

\bibitem{melgan}
K.~Kumar, R.~Kumar, T.~de~Boissiere, L.~Gestin, W.~Z. Teoh, J.~Sotelo,
  A.~de~Br{\'e}bisson, Y.~Bengio, and A.~C. Courville, ``{MelGAN}: Generative
  adversarial networks for conditional waveform synthesis,'' \emph{Advances in
  neural information processing systems}, vol.~32, 2019.

\bibitem{wavenet}
A.~v.~d. Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves,
  N.~Kalchbrenner, A.~Senior, and K.~Kavukcuoglu, ``Wavenet: A generative model
  for raw audio,'' \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem{lsgan}
X.~Mao, Q.~Li, H.~Xie, R.~Y. Lau, Z.~Wang, and S.~P. Smolley, ``Least squares
  generative adversarial networks,'' in \emph{2017 IEEE International
  Conference on Computer Vision (ICCV)}, 2017, pp. 2813--2821.

\bibitem{parallel_wavegan}
R.~Yamamoto, E.~Song, and J.-M. Kim, ``{Parallel WaveGAN}: A fast waveform
  generation model based on generative adversarial networks with
  multi-resolution spectrogram,'' in \emph{ICASSP 2020-2020 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 6199--6203.

\bibitem{mfcc_paper}
S.~{Davis} and P.~{Mermelstein}, ``Comparison of parametric representations for
  monosyllabic word recognition in continuously spoken sentences,'' \emph{IEEE
  Transactions on Acoustics, Speech, and Signal Processing}, vol.~28, no.~4,
  pp. 357--366, 1980.

\bibitem{conformer}
A.~Gulati, C.-C. Chiu, J.~Qin, J.~Yu, N.~Parmar, R.~Pang, S.~Wang, W.~Han,
  Y.~Wu, Y.~Zhang, and Z.~Zhang, Eds., \emph{Conformer: Convolution-augmented
  Transformer for Speech Recognition}, 2020.

\bibitem{libri_tts}
\BIBentryALTinterwordspacing
H.~Zen, R.~Clark, R.~J. Weiss, V.~Dang, Y.~Jia, Y.~Wu, Y.~Zhang, and Z.~Chen,
  ``Libri{TTS}: A corpus derived from {LibriSpeech} for text-to-speech,'' in
  \emph{Interspeech}, 2019. [Online]. Available:
  \url{https://arxiv.org/abs/1904.02882}
\BIBentrySTDinterwordspacing

\bibitem{s3fd}
S.~Zhang, X.~Zhu, Z.~Lei, H.~Shi, X.~Wang, and S.~Z. Li, ``S3fd: Single shot
  scale-invariant face detector,'' in \emph{Proceedings of the IEEE
  international conference on computer vision}, 2017, pp. 192--201.

\bibitem{fan}
A.~Bulat and G.~Tzimiropoulos, ``How far are we from solving the 2d \& 3d face
  alignment problem?(and a dataset of 230,000 3d facial landmarks),'' in
  \emph{Proceedings of the IEEE international conference on computer vision},
  2017, pp. 1021--1030.

\bibitem{adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{adamw}
\BIBentryALTinterwordspacing
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in
  \emph{International Conference on Learning Representations}, 2019. [Online].
  Available: \url{https://openreview.net/forum?id=Bkg6RiCqY7}
\BIBentrySTDinterwordspacing

\bibitem{sgdr}
\BIBentryALTinterwordspacing
------, ``{SGDR}: Stochastic gradient descent with warm restarts,'' in
  \emph{International Conference on Learning Representations}, 2017. [Online].
  Available: \url{https://openreview.net/forum?id=Skq89Scxx}
\BIBentrySTDinterwordspacing

\bibitem{pesq}
A.~W. Rix, J.~G. Beerends, M.~P. Hollier, and A.~P. Hekstra, ``Perceptual
  evaluation of speech quality ({PESQ})-a new method for speech quality
  assessment of telephone networks and codecs,'' in \emph{2001 IEEE
  international conference on acoustics, speech, and signal processing.
  Proceedings (Cat. No. 01CH37221)}, vol.~2.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2001, pp. 749--752.

\bibitem{stoi}
C.~H. Taal, R.~C. Hendriks, R.~Heusdens, and J.~Jensen, ``A short-time
  objective intelligibility measure for time-frequency weighted noisy speech,''
  in \emph{2010 IEEE international conference on acoustics, speech and signal
  processing}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2010, pp.
  4214--4217.

\bibitem{estoi}
J.~Jensen and C.~H. Taal, ``An algorithm for predicting the intelligibility of
  speech masked by modulated noise maskers,'' \emph{IEEE/ACM Transactions on
  Audio, Speech, and Language Processing}, vol.~24, no.~11, pp. 2009--2022,
  2016.

\end{thebibliography}
