% ************************************************************************* %
@article{large_scale_unsupervised_audio_pretraining_for_v2a,
  title={Large-scale unsupervised audio pre-training for video-to-speech synthesis},
  author={Kefalas, Triantafyllos and Panagakis, Yannis and Pantic, Maja},
  journal={arXiv preprint arXiv:2306.15464},
  year={2023}
}
% ************************************************************************* %

@article{driven_to_distraction,
  title={Driven to distraction? A review of speech technologies in the automobile},
  author={Young, R and Zhang, J},
  journal={J Assoc Voice Interact Design},
  volume={2},
  number={1},
  pages={1--34},
  year={2017}
}

@article{manifestation_of_virtual_assistants,
  title={Manifestation of virtual assistants and robots into daily life: Vision and challenges},
  author={Rawassizadeh, Reza and Sen, Taylan and Kim, Sunny Jung and Meurisch, Christian and Keshavarz, Hamidreza and M{\"u}hlh{\"a}user, Max and Pazzani, Michael},
  journal={CCF Transactions on Pervasive Computing and Interaction},
  volume={1},
  pages={163--174},
  year={2019},
  publisher={Springer}
}

@inproceedings{intelligent_voice_bots_for_digital_banking,
  title={Intelligent voice bots for digital banking},
  author={Kaur, Ravneet and Sandhu, Ravtej Singh and Gera, Ayush and Kaur, Tarlochan and Gera, Purva},
  booktitle={Smart Systems and IoT: Innovations in Computing: Proceeding of SSIC 2019},
  pages={401--408},
  year={2020},
  organization={Springer}
}

@inproceedings{libri_tts,
title	= {Libri{TTS}: A Corpus Derived from {LibriSpeech} for Text-to-Speech},
author	= {Heiga Zen and Rob Clark and Ron J. Weiss and Viet Dang and Ye Jia and Yonghui Wu and Yu Zhang and Zhifeng Chen},
year	= {2019},
URL	= {https://arxiv.org/abs/1904.02882},
booktitle	= {Interspeech}
}

@article{hifi_tts,
  title={{Hi-Fi Multi-Speaker English TTS Dataset}},
  author={Bakhturina, Evelina and Lavrukhin, Vitaly and Ginsburg, Boris and Zhang, Yang},
  journal={arXiv preprint arXiv:2104.01497},
  year={2021}
}

@misc{vctk,
  author={Yamagishi, Junichi and Veaux, Christophe and MacDonald, Kirsten},
  title={{CSTR VCTK Corpus}: English Multi-speaker Corpus for {CSTR} Voice Cloning Toolkit (version 0.92)},
  publisher={University of Edinburgh. The Centre for Speech Technology Research (CSTR)},
  year=2019,
  doi={10.7488/ds/2645},
}

@inproceedings{omceb,
    title = "Open-source Multi-speaker Corpora of the {E}nglish Accents in the {B}ritish Isles",
    author = "Demirsahin, Isin  and
      Kjartansson, Oddur  and
      Gutkin, Alexander  and
      Rivera, Clara",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.804",
    pages = "6532--6541",
    abstract = "This paper presents a dataset of transcribed high-quality audio of English sentences recorded by volunteers speaking with different accents of the British Isles. The dataset is intended for linguistic analysis as well as use for speech technologies. The recording scripts were curated specifically for accent elicitation, covering a variety of phonological phenomena and providing a high phoneme coverage. The scripts include pronunciations of global locations, major airlines and common personal names in different accents; and native speaker pronunciations of local words. Overlapping lines for all speakers were included for idiolect elicitation, which include the same or similar lines with other existing resources such as the CSTR VCTK corpus and the Speech Accent Archive to allow for easy comparison of personal and regional accents. The resulting corpora include over 31 hours of recordings from 120 volunteers who self-identify as native speakers of Southern England, Midlands, Northern England, Welsh, Scottish and Irish varieties of English.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@Article{spoken_wikipedia,
author="Baumann, Timo
and K{\"o}hn, Arne
and Hennig, Felix",
title="{The Spoken Wikipedia Corpus} collection: Harvesting, alignment and an application to hyperlistening",
journal="Language Resources and Evaluation",
year="2018",
month="Jan",
day="09",
issn="1574-0218",
doi="10.1007/s10579-017-9410-y",
url="https://doi.org/10.1007/s10579-017-9410-y"
}

@inproceedings{common_voice,
    title = "{Common Voice}: A Massively-Multilingual Speech Corpus",
    author = "Ardila, Rosana  and
      Branson, Megan  and
      Davis, Kelly  and
      Kohler, Michael  and
      Meyer, Josh  and
      Henretty, Michael  and
      Morais, Reuben  and
      Saunders, Lindsay  and
      Tyers, Francis  and
      Weber, Gregor",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.520",
    pages = "4218--4222",
    abstract = "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\mbox{$\pm$}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{speech_stew,
  author    = {William Chan and
               Daniel S. Park and
               Chris A. Lee and
               Yu Zhang and
               Quoc V. Le and
               Mohammad Norouzi},
  title     = {{SpeechStew}: Simply Mix All Available Speech Recognition Data to Train
               One Large Neural Network},
  journal   = {CoRR},
  volume    = {abs/2104.02133},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.02133},
  eprinttype = {arXiv},
  eprint    = {2104.02133},
  timestamp = {Tue, 16 Nov 2021 09:43:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-02133.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{conformer,
title	= {Conformer: Convolution-augmented Transformer for Speech Recognition},
editor	= {Anmol Gulati and Chung-Cheng Chiu and James Qin and Jiahui Yu and Niki Parmar and Ruoming Pang and Shibo Wang and Wei Han and Yonghui Wu and Yu Zhang and Zhengdong Zhang},
year	= {2020}
}

@INPROCEEDINGS{lsgan,
  author={Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y.K. and Wang, Zhen and Smolley, Stephen Paul},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Least Squares Generative Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2813-2821},
  doi={10.1109/ICCV.2017.304}}
  
  
@InProceedings{batch_norm,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, Sergey and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}}

@article{layer_norm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{grid_database,
author = {Cooke, M. and Barker, J. and Cunningham, S. and Shao, X.},
year = {2006},
month = {12},
pages = {2421-4},
title = {An audio-visual corpus for speech perception and automatic speech recognition (L)},
volume = {120},
journal = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.2229005}
}

@article{tcd_timit_databse,
author={N. {Harte} and E. {Gillen}},
journal={IEEE Transactions on Multimedia},
title={{TCD-TIMIT}: An Audio-Visual Corpus of Continuous Speech},
year={2015},
volume={17},
number={5},
pages={603-615},
keywords={audio-visual systems;speech recognition;video signal processing;TCD-TIMIT;audio-visual corpus;continuous speech;continuous audio-visual speech recognition research;high-quality audio footage;high-quality video footage;professionally-trained lipspeakers;automatic visual speech recognition systems;audio clips;video clips;audio-visual baseline experiments;Speech recognition;Cameras;Visualization;Speech;Dictionaries;Visual databases;Audio-visual speech recognition},
doi={10.1109/TMM.2015.2407694},
ISSN={},
month={May},}

@ARTICLE{an_overview_of_deep_learning_based_av_speech_enhancement_and_separation,
  author={Michelsanti, Daniel and Tan, Zheng-Hua and Zhang, Shi-Xiong and Xu, Yong and Yu, Meng and Yu, Dong and Jensen, Jesper},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation}, 
  year={2021},
  volume={29},
  number={},
  pages={1368-1396},
  doi={10.1109/TASLP.2021.3066303}}

@inproceedings{the_conversation_deep_av_speech_enhancement,
  author={Triantafyllos Afouras and Joon Son Chung and Andrew Zisserman},
  title={The Conversation: Deep Audio-Visual Speech Enhancement},
  year=2018,
  booktitle={Proc. Interspeech 2018},
  pages={3244--3248},
  doi={10.21437/Interspeech.2018-1400},
  url={http://dx.doi.org/10.21437/Interspeech.2018-1400}
}

@inproceedings{towards_next_generation_lipreading_driven_hearing_aids,
  title={Towards next-generation lipreading driven hearing-aids: A preliminary prototype demo},
  author={Adeel, Ahsan and Gogate, Mandar and Hussain, Amir},
  booktitle={Proceedings of the International Workshop on Challenges in Hearing Assistive Technology (CHAT-2017), Stockholm, Sweden},
  volume={19},
  number={2},
  year={2017}
}
  
  @ARTICLE{deep_av_speech_recognition,
  author={Afouras, Triantafyllos and Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Deep Audio-visual Speech Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2018.2889052}}
  
  @ARTICLE{lip_reading_driven_deep_learning_approach_for_speech_enhancement,
  author={Adeel, Ahsan and Gogate, Mandar and Hussain, Amir and Whitmer, William M.},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Lip-Reading Driven Deep Learning Approach for Speech Enhancement}, 
  year={2021},
  volume={5},
  number={3},
  pages={481-490},
  doi={10.1109/TETCI.2019.2917039}}
  
@inproceedings{my_lips_are_concealed,
author = {Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
year = {2019},
month = {09},
pages = {4295-4299},
title = {My Lips Are Concealed: Audio-Visual Speech Enhancement Through Obstructions},
doi = {10.21437/Interspeech.2019-3114}
}

@inproceedings{lite_av_speech_enhancement,
  author={Shang-Yi Chuang and Yu Tsao and Chen-Chou Lo and Hsin-Min Wang},
  title={Lite Audio-Visual Speech Enhancement},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={1131--1135},
  doi={10.21437/Interspeech.2020-1617}
}

@article{multi_modal_multi_target_speech_separation,
author = {Rongzhi, Gu and Zhang, Shi-Xiong and Xu, Yong and Chen, Lianwu and Zou, Yuexian and Yu, Dong},
year = {2020},
month = {03},
pages = {1-1},
title = {Multi-Modal Multi-Channel Target Speech Separation},
volume = {PP},
journal = {IEEE Journal of Selected Topics in Signal Processing},
doi = {10.1109/JSTSP.2020.2980956}
}

@article{looking_to_listen_at_the_cocktail_party,
author = {Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T. and Rubinstein, Michael},
title = {Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201357},
doi = {10.1145/3197517.3201357},
abstract = {We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {112},
numpages = {11},
keywords = {source separation, CNN, deep learning, BLSTM, speech enhancement, audio-visual}
}

@inproceedings{visualvoice,
  title = {{VisualVoice}: Audio-Visual Speech Separation with Cross-Modal Consistency},
  author = {Gao, Ruohan and Grauman, Kristen},
  booktitle = {CVPR},
  year = {2021}
}

@INPROCEEDINGS{deep_av_speech_separation_with_attention_mechanism,
  author={Li, Chenda and Qian, Yanmin},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Deep Audio-Visual Speech Separation with Attention Mechanism}, 
  year={2020},
  volume={},
  number={},
  pages={7314-7318},
  doi={10.1109/ICASSP40776.2020.9054180}}
  
@inproceedings{rnn_transducer_for_av_speech_recognition,
title	= {RECURRENT NEURAL NETWORK TRANSDUCER FOR AUDIO-VISUAL SPEECH RECOGNITION},
author	= {Basi Garcia and Brendan Shillingford and Hank Liao and Olivier Siohan and Otavio de Pinho Forin Braga and Takaki Makino and Yannis Assael},
year	= {2019},
URL	= {https://arxiv.org/abs/1911.04890},
booktitle	= {Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop}
}

@INPROCEEDINGS{visual_features_for_context_aware_speech_recognition,
  author={Gupta, Abhinav and Miao, Yajie and Neves, Leonardo and Metze, Florian},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Visual features for context-aware speech recognition}, 
  year={2017},
  volume={},
  number={},
  pages={5020-5024},
  doi={10.1109/ICASSP.2017.7953112}}
  
@ARTICLE{recent_advances_in_the_automatic_recognition_of_av_speech,
author={Potamianos, G. and Neti, C. and Gravier, G. and Garg, A. and Senior, A.W.},
journal={Proceedings of the IEEE}, 
title={Recent advances in the automatic recognition of audiovisual speech}, 
year={2003},
volume={91},
number={9},
pages={1306-1326},
doi={10.1109/JPROC.2003.817150}}

@INPROCEEDINGS{end_to_end_av_speech_recognition,
  author={Petridis, Stavros and Stafylakis, Themos and Ma, Pingchuan and Cai, Feipeng and Tzimiropoulos, Georgios and Pantic, Maja},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={End-to-End Audiovisual Speech Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={6548-6552},
  doi={10.1109/ICASSP.2018.8461326}}

@article{av_speech_recognition_with_hybrid_ctc_attention_architecture,
  title={Audio-Visual Speech Recognition with a Hybrid {CTC/Attention} Architecture},
  author={Stavros Petridis and Themos Stafylakis and Pingchuan Ma and Georgios Tzimiropoulos and Maja Pantic},
  journal={2018 IEEE Spoken Language Technology Workshop (SLT)},
  year={2018},
  pages={513-520}
}

@INPROCEEDINGS{end_to_end_av_speech_recognition_with_conformers,
  author={Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={End-To-End Audio-Visual Speech Recognition with Conformers}, 
  year={2021},
  volume={},
  number={},
  pages={7613-7617},
  doi={10.1109/ICASSP39728.2021.9414567}}

@inproceedings{ephrat2017vid2speech,
  title     = {{Vid2Speech}: speech reconstruction from silent video},
  author    = {Ariel Ephrat and Shmuel Peleg},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2017},
}

@ARTICLE{av_event_recognition_in_surveillance,
  author={Cristani, Marco and Bicego, Manuele and Murino, Vittorio},
  journal={IEEE Transactions on Multimedia}, 
  title={Audio-Visual Event Recognition in Surveillance Video Sequences}, 
  year={2007},
  volume={9},
  number={2},
  pages={257-267},
  doi={10.1109/TMM.2006.886263}}

@ARTICLE{end_to_end_video_to_speech_synthesis_using_gans,
  author={Mira, Rodrigo and Vougioukas, Konstantinos and Ma, Pingchuan and Petridis, Stavros and Schuller, Björn W. and Pantic, Maja},
  journal={IEEE Transactions on Cybernetics}, 
  title={End-to-End Video-to-Speech Synthesis Using Generative Adversarial Networks}, 
  year={2023},
  volume={53},
  number={6},
  pages={3454-3466},
  doi={10.1109/TCYB.2022.3162495}}
  
@article{silent_speech_interfaces,
title = {Silent speech interfaces},
journal = {Speech Communication},
volume = {52},
number = {4},
pages = {270-287},
year = {2010},
note = {Silent Speech Interfaces},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2009.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167639309001307},
author = {B. Denby and T. Schultz and K. Honda and T. Hueber and J.M. Gilbert and J.S. Brumberg},
keywords = {Silent speech, Speech pathologies, Cellular telephones, Speech recognition, Speech synthesis},
abstract = {The possibility of speech processing in the absence of an intelligible acoustic signal has given rise to the idea of a ‘silent speech’ interface, to be used as an aid for the speech-handicapped, or as part of a communications system operating in silence-required or high-background-noise environments. The article first outlines the emergence of the silent speech interface from the fields of speech production, automatic speech processing, speech pathology research, and telecommunications privacy issues, and then follows with a presentation of demonstrator systems based on seven different types of technologies. A concluding section underlining some of the common challenges faced by silent speech interface researchers, and ideas for possible future directions, is also provided.}
} 

@inproceedings{reconstructing_intelligible_audio_speech_from_visual_speech_features,
  author={Thomas Le Cornu and Ben Milner},
  title={{Reconstructing intelligible audio speech from visual speech features}},
  year=2015,
  booktitle={Proc. Interspeech 2015},
  pages={3355--3359},
  doi={10.21437/Interspeech.2015-139}
}

@article{lip2audspec,
  title={{Lip2AudSpec}: Speech Reconstruction from Silent Lip Movements Video},
  author={Hassan Akbari and Himani Arora and Liangliang Cao and Nima Mesgarani},
  journal={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2018},
  pages={2516-2520}
}

@inproceedings{svts,
  author={Rodrigo Mira and Alexandros Haliassos and Stavros Petridis and Björn W. Schuller and Maja Pantic},
  title={{SVTS}: Scalable Video-to-Speech Synthesis},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={1836--1840},
  doi={10.21437/Interspeech.2022-10770}
}

@article{lip_to_speech_synthesis_with_visual_context_attention_gan,
  title={Lip to Speech Synthesis with Visual Context Attentional {GAN}},
  author={Kim, Minsu and Hong, Joanna and Ro, Yong Man},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{improved_speech_reconstruction_from_silent_video,
  title={Improved Speech Reconstruction from Silent Video},
  author={Ariel Ephrat and Tavi Halperin and Shmuel Peleg},
  journal={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
  year={2017},
  pages={455-462}
}

@INPROCEEDINGS{speech_prediction_in_silent_videos_using_vaes,
  author={Yadav, Ravindra and Sardana, Ashish and Namboodiri, Vinay P and Hegde, Rajesh M},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Speech Prediction in Silent Videos Using Variational Autoencoders}, 
  year={2021},
  volume={},
  number={},
  pages={7048-7052},
  doi={10.1109/ICASSP39728.2021.9414040}}
  
@article{speaker_disentanglement_in_video_to-speech_conversion,
  title={Speaker disentanglement in video-to-speech conversion},
  author={Dan Oneață and Adriana Stan and Horia Cucu},
  journal={2021 29th European Signal Processing Conference (EUSIPCO)},
  year={2021},
  pages={46-50}
}

@InProceedings{lip2wav,
author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
title = {Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{video_driven_speech_reconstruction_using_gans,
  title={Video-Driven Speech Reconstruction using Generative Adversarial Networks},
  author={Konstantinos Vougioukas and Pingchuan Ma and Stavros Petridis and Maja Pantic},
  booktitle={INTERSPEECH},
  year={2019}
}

@article{e2e_v2s_resnet,
title = {{E2E-V2SResNet}: Deep residual convolutional neural networks for end-to-end video driven speech synthesis},
journal = {Image and Vision Computing},
volume = {119},
pages = {104389},
year = {2022},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2022.104389},
url = {https://www.sciencedirect.com/science/article/pii/S026288562200018X},
author = {Nasir Saleem and Jiechao Gao and Muhammad Irfan and Elena Verdu and Javier Parra Fuente},
keywords = {Video processing, E2E speech synthesis, ResNet-18, Residual CNN, Waveform CRITIC},
abstract = {Speechreading which infers spoken message from a visually detected articulated facial trend is a challenging task. In this paper, we propose an end-to-end ResNet (E2E-ResNet) model for synthesizing speech signals from the silent video of a speaking individual. The model is the convolutional encoder-decoder framework which captures the frames of video and encodes into a latent space of visual features. The outputs of the decoder are spectrograms which are converted into waveforms corresponding to a speech articulated in the input video. The speech waveforms are then fed to a waveform critic used to decide the real or synthesized speech. The experiments show that the proposed E2E-V2SResNet model is apt to synthesize speech with realism and intelligibility/quality for GRID database. To further demonstrate the potentials of the proposed model, we also conduct experiments on the TCD-TIMIT database. We examine the synthesized speech in unseen speakers using three objective metrics use to measure the intelligibility, quality, and word error rate (WER) of the synthesized speech. We show that E2E-V2SResNet model outscores the competing approaches in most metrics on the GRID and TCD-TIMIT databases. By comparing with the baseline, the proposed model achieved 3.077% improvement in speech quality and 2.593% improvement in speech intelligibility.}
}

@ARTICLE{lipsound2,
  author={Qu, Leyuan and Weber, Cornelius and Wermter, Stefan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Lip{S}ound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading}, 
  year={2022},
  volume={},
  number={},
  pages={1-11},
  doi={10.1109/TNNLS.2022.3191677}}

@INPROCEEDINGS{librispeech,
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={{LibriSpeech}: An {ASR} corpus based on public domain audio books}, 
  year={2015},
  volume={},
  number={},
  pages={5206-5210},
  doi={10.1109/ICASSP.2015.7178964}}
  
@inproceedings{spotify_podcasts_dataset,
    title = "100,000 Podcasts: A Spoken {E}nglish Document Corpus",
    author = "Clifton, Ann  and
      Reddy, Sravana  and
      Yu, Yongze  and
      Pappu, Aasish  and
      Rezapour, Rezvaneh  and
      Bonab, Hamed  and
      Eskevich, Maria  and
      Jones, Gareth  and
      Karlgren, Jussi  and
      Carterette, Ben  and
      Jones, Rosie",

    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/ anthology/2020.coling-main.519 ",
    pages = "5903--5917",
    abstract = "Podcasts are a large and growing repository of spoken audio. As an audio format, podcasts are more varied in style and production type than broadcast news, contain more genres than typically studied in video data, and are more varied in style and format than previous corpora of conversations. When transcribed with automatic speech recognition they represent a noisy but fascinating collection of documents which can be studied through the lens of natural language processing, information retrieval, and linguistics. Paired with the audio files, they are also a resource for speech processing and the study of paralinguistic, sociolinguistic, and acoustic aspects of the domain. We introduce the Spotify Podcast Dataset, a new corpus of 100,000 podcasts. We demonstrate the complexity of the domain with a case study of two tasks: (1) passage search and (2) summarization. This is orders of magnitude larger than previous speech corpora used for search and summarization. Our results show that the size and variability of this corpus opens up new avenues for research.",
}

@article{boston_university_radio_news_corpus,
  title={{The Boston University} radio news corpus},
  author={Ostendorf, Mari and Price, Patti J and Shattuck-Hufnagel, Stefanie},
  journal={Linguistic Data Consortium},
  pages={1--19},
  year={1995}
}

@inproceedings{voxpopuli,
    title = "{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",
    author = "Wang, Changhan  and
      Riviere, Morgane  and
      Lee, Ann  and
      Wu, Anne  and
      Talnikar, Chaitanya  and
      Haziza, Daniel  and
      Williamson, Mary  and
      Pino, Juan  and
      Dupoux, Emmanuel",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.80",
    pages = "993--1003",
}

@article{gigaspeech,
  title={Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio},
  author={Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and others},
  journal={arXiv preprint arXiv:2106.06909},
  year={2021}
}

@misc{ljspeech,
  added-at = {2021-02-01T10:41:16.000+0100},
  author = {Ito, Keith and Johnson, Linda},
  biburl = {https://www.bibsonomy.org/bibtex/236c7c43e9200b58ee3dad4a54bf38f5d/m-toman},
  howpublished = {\url{https://keithito.com/LJ-Speech-Dataset/}},
  interhash = {188d9174567740c36874397e5d609479},
  intrahash = {36c7c43e9200b58ee3dad4a54bf38f5d},
  keywords = {data tts},
  timestamp = {2021-02-01T10:41:16.000+0100},
  title = {The {LJ} {Speech} {Dataset}},
  year = 2017
}

@inproceedings{wasserstein_gan,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@article{melgan,
  title={{MelGAN}: Generative adversarial networks for conditional waveform synthesis},
  author={Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Br{\'e}bisson, Alexandre and Bengio, Yoshua and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{multi_band_melgan,
  title={Multi-band {MelGAN}: Faster waveform generation for high-quality text-to-speech},
  author={Yang, Geng and Yang, Shan and Liu, Kai and Fang, Peng and Chen, Wei and Xie, Lei},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={492--498},
  year={2021},
  organization={IEEE}
}

@inproceedings{style_melgan,
  title={{StyleMelGAN}: An efficient high-fidelity adversarial vocoder with temporal adaptive normalization},
  author={Mustafa, Ahmed and Pia, Nicola and Fuchs, Guillaume},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6034--6038},
  year={2021},
  organization={IEEE}
}

@article{hifi_gan,
  title={{HiFi-GAN}: Generative adversarial networks for efficient and high fidelity speech synthesis},
  author={Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17022--17033},
  year={2020}
}

@inproceedings{pase,
  author={Santiago Pascual and Mirco Ravanelli and Joan Serrà and Antonio Bonafonte and Yoshua Bengio},
  title={Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={161--165},
  doi={10.21437/Interspeech.2019-2605}
}

@inproceedings{parallel_wavegan,
  title={{Parallel WaveGAN}: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram},
  author={Yamamoto, Ryuichi and Song, Eunwoo and Kim, Jae-Min},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6199--6203},
  year={2020},
  organization={IEEE}
}

@article{wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@article{bidirectional_lstm,
  title={Framewise phoneme classification with bidirectional {LSTM} and other neural network architectures},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{autovc,
  title={{AutoVC}: Zero-shot voice style transfer with only autoencoder loss},
  author={Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark},
  booktitle={International Conference on Machine Learning},
  pages={5210--5219},
  year={2019},
  organization={PMLR}
}

@article{visual_contribution_to_speech_intelligibility_in_noise,
  title={Visual contribution to speech intelligibility in noise},
  author={Sumby, William H and Pollack, Irwin},
  journal={The journal of the acoustical society of america},
  volume={26},
  number={2},
  pages={212--215},
  year={1954},
  publisher={Acoustical Society of America}
}

@phdthesis{automatic_lipreading_to_enhance_speech_recognition,
author = {Petajan, Eric David},
title = {Automatic Lipreading to Enhance Speech Recognition (Speech Reading)},
year = {1984},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {Automatic recognition of the acoustic speech signal alone is inaccurate and computationally expensive. Additional sources of speech information, such as lipreading (or speechreading), should enhance automatic speech recognition, just as lipreading is used by humans to enhance speech recognition when the acoustic signal is degraded. This paper describes an automatic lipreading system which has been developed. A commercial device performs the acoustic speech recognition independently of the lipreading system.The recognition domain is restricted to isolated utterances and speaker dependent recognition. The speaker faces a solid state camera which sends digitized video to a minicomputer system with custom video processing hardware. The video data is sampled during an utterance and then reduced to a template consisting of visual speech parameter time sequences. The distances between the incoming template and all of the trained templates for each utterance in the vocabulary are computed and a visual recognition candidate is obtained. The combination of the acoustic and visual recognition candidates is shown to yield a final recognition accuracy which greatly exceeds the acoustic recognition accuracy alone. Practical considerations and the possible enhancement of speaker independent and continuous speech recognition systems are also discussed.},
note = {AAI8502266}
}

@inproceedings{continuous_optical_automatic_speech_recognition_by_lipreading,
  title={Continuous optical automatic speech recognition by lipreading},
  author={Goldschen, Alan J and Garcia, Oscar N and Petajan, Eric},
  booktitle={Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers},
  volume={1},
  pages={572--577},
  year={1994},
  organization={Ieee}
}

@inproceedings{lip_feature_extraction_and_reduction_for_hmm,
  title={Lip feature extraction and reduction for {HMM}-based visual speech recognition systems},
  author={Alizadeh, S and Boostani, R and Asadpour, V},
  booktitle={2008 9th International Conference on Signal Processing},
  pages={561--564},
  year={2008},
  organization={IEEE}
}


@inproceedings{lip_feature_extraction_based_on_improved_jumping_snake_model,
  title={Lip feature extraction based on improved jumping-snake model},
  author={Ma, Xinjun and Yan, Long and Zhong, Qianyuan},
  booktitle={2016 35th Chinese Control Conference (CCC)},
  pages={6928--6933},
  year={2016},
  organization={IEEE}
}

@inproceedings{profile_view_lip_reading,
  title={Profile view lip reading},
  author={Kumar, Kshitiz and Chen, Tsuhan and Stern, Richard M},
  booktitle={2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07},
  volume={4},
  pages={IV--429},
  year={2007},
  organization={IEEE}
}

@article{active_appearance_models,
  title={Active appearance models},
  author={Cootes, Timothy F. and Edwards, Gareth J. and Taylor, Christopher J.},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={23},
  number={6},
  pages={681--685},
  year={2001},
  publisher={IEEE}
}

@inproceedings{view_independent_computer_lip_reading,
  title={View independent computer lip-reading},
  author={Lan, Yuxuan and Theobald, Barry-John and Harvey, Richard},
  booktitle={2012 IEEE International Conference on Multimedia and Expo},
  pages={432--437},
  year={2012},
  organization={IEEE}
}

@inproceedings{insights_into_machine_lip_reading,
  title={Insights into machine lip reading},
  author={Lan, Yuxuan and Harvey, Richard and Theobald, Barry-John},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4825--4828},
  year={2012},
  organization={IEEE}
}

@article{information_theoretic_feature_extraction_for_av_speech_recognition,
  title={Information theoretic feature extraction for audio-visual speech recognition},
  author={Gurban, Mihai and Thiran, Jean-Philippe},
  journal={IEEE Transactions on signal processing},
  volume={57},
  number={12},
  pages={4765--4776},
  year={2009},
  publisher={IEEE}
}

@article{lipreading_with_local_spatiotemporal_descriptors,
  title={Lipreading with local spatiotemporal descriptors},
  author={Zhao, Guoying and Barnard, Mark and Pietikainen, Matti},
  journal={IEEE Transactions on Multimedia},
  volume={11},
  number={7},
  pages={1254--1265},
  year={2009},
  publisher={IEEE}
}

@inproceedings{an_image_transform_approach_for_hmm_based_automatic_lipreading,
  title={An image transform approach for {HMM} based automatic lipreading},
  author={Potamianos, Gerasimos and Graf, Hans Peter and Cosatto, Eric},
  booktitle={Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No. 98CB36269)},
  pages={173--177},
  year={1998},
  organization={IEEE}
}

@inproceedings{improved_speaker_independent_lip_reading,
  title={Improved speaker independent lip reading using speaker adaptive training and deep neural networks},
  author={Almajai, Ibrahim and Cox, Stephen and Harvey, Richard and Lan, Yuxuan},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2722--2726},
  year={2016},
  organization={IEEE}
}

@inproceedings{av_speech_recognition_using_bimodal_trained_bottleneck_features,
  title={Audio-Visual Speech Recognition Using Bimodal-Trained Bottleneck Features for a Person with Severe Hearing Loss.},
  author={Takashima, Yuki and Aihara, Ryo and Takiguchi, Tetsuya and Ariki, Yasuo and Mitani, Nobuyuki and Omori, Kiyohiro and Nakazono, Kaoru},
  booktitle={Interspeech},
  pages={277--281},
  year={2016}
}

@inproceedings{lipreading_using_convolutional_neural_network,
  title={Lipreading using convolutional neural network},
  author={Noda, Kuniaki and Yamaguchi, Yuki and Nakadai, Kazuhiro and Okuno, Hiroshi G and Ogata, Tetsuya},
  booktitle={fifteenth annual conference of the international speech communication association},
  year={2014}
}

@inproceedings{deep_learning_of_mouth_shapes_for_sign_language,
  title={Deep learning of mouth shapes for sign language},
  author={Koller, Oscar and Ney, Hermann and Bowden, Richard},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={85--91},
  year={2015}
}

@inproceedings{deep_complementary_bottleneck_features_for_visual_speech_recognition,
  title={Deep complementary bottleneck features for visual speech recognition},
  author={Petridis, Stavros and Pantic, Maja},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2304--2308},
  year={2016},
  organization={IEEE}
}

@article{lipnet,
  title={{LipNet}: End-to-end sentence-level lipreading},
  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and De Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01599},
  year={2016}
}

@article{empirical_evaluation_of_gated_rnn_on_sequence_modeling,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@inproceedings{connectionist_temporal_classification,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006}
}

@inproceedings{lip_reading_in_the_wild,
  title={Lip reading in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Asian conference on computer vision},
  pages={87--103},
  year={2016},
  organization={Springer}
}

@inproceedings{deep_residual_learning_for_image_recognition,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{visual_speech_recognition_for_multiple_languages_in_the_wild,
  title={Visual speech recognition for multiple languages in the wild},
  author={Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  journal={Nature Machine Intelligence},
  pages={1--10},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{deep_face_recognition,
  title={Deep face recognition},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew},
  year={2015},
  publisher={British Machine Vision Association}
}

@inproceedings{return_of_devil_in_the_details,
	title = {Return of the Devil in the Details: Delving Deep into Convolutional Nets},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2014},
	booktitle = {Proceedings of the British Machine Vision Conference},
	publisher = {BMVA Press},
	editors = {Valstar, Michel and French, Andrew and Pridmore, Tony}
}

@article{transfer_learning_from_speaker_verification,
  title={Transfer learning from speaker verification to multispeaker text-to-speech synthesis},
  author={Jia, Ye and Zhang, Yu and Weiss, Ron and Wang, Quan and Shen, Jonathan and Ren, Fei and Nguyen, Patrick and Pang, Ruoming and Lopez Moreno, Ignacio and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@TECHREPORT{real_time_voice_cloning,
  title={Real-time-voice-cloning},
  author={Jemine, Corentin},
  journal={University of Li{\'e}ge, Li{\'e}ge, Belgium},
  year={2019}
}

@misc{real_time_voice_cloning_github,
  title = {Real-Time Voice Cloning},
  howpublished = {\url{https://github.com/CorentinJ/Real-Time-Voice-Cloning}},
  note = {Accessed: 2023-04-21}
}

@inproceedings{voxceleb1_paper,
  author={Arsha Nagrani and Joon Son Chung and Andrew Zisserman},
  title={{VoxCeleb}: A Large-Scale Speaker Identification Dataset},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={2616--2620},
  doi={10.21437/Interspeech.2017-950}
}

@inproceedings{voxceleb2_paper,
  author={Joon Son Chung and Arsha Nagrani and Andrew Zisserman},
  title={{VoxCeleb2}: Deep Speaker Recognition},
  year=2018,
  booktitle={Proc. Interspeech 2018},
  pages={1086--1090},
  doi={10.21437/Interspeech.2018-1929}
}

@misc{vgg_m_face_pytorch,
  title = {"{VGG-M} {F}ace "},
  howpublished = {\url{http://www.robots.ox.ac.uk/~albanie/models/pytorch-mcn/vgg\_m\_face\_bn\_dag.py}},
  note = {Accessed: 30-May-2020}
}

@ARTICLE{mfcc_paper,
  author={S. {Davis} and P. {Mermelstein}},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  title={Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}, 
  year={1980},
  volume={28},
  number={4},
  pages={357-366},}

@inproceedings{weight_normalization,
 author = {Salimans, Tim and Kingma, Durk P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf},
 volume = {29},
 year = {2016}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{training_speech_enhancement_systems_with_noisy_speech_datasets,
  title={Training speech enhancement systems with noisy speech datasets},
  author={Saito, Koichi and Uhlich, Stefan and Fabbro, Giorgio and Mitsufuji, Yuki},
  journal={arXiv preprint arXiv:2105.12315},
  year={2021}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{can_we_use_common_voice_to_train,
  title={Can we use {Common Voice} to train a Multi-Speaker {TTS} system?},
  author={Ogun, Sewade and Colotte, Vincent and Vincent, Emmanuel},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)},
  pages={900--905},
  year={2023},
  organization={IEEE}
}

@inproceedings{cmgan_interspeech,
  author={Cao, Ruizhe and Abdulatif, Sherif and Yang, Bin},
  title={{CMGAN}: Conformer-based Metric {GAN} for Speech Enhancement},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={936--940},
  doi={10.21437/Interspeech.2022-517}
}

@inproceedings{s3fd,
  title={S3fd: Single shot scale-invariant face detector},
  author={Zhang, Shifeng and Zhu, Xiangyu and Lei, Zhen and Shi, Hailin and Wang, Xiaobo and Li, Stan Z},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={192--201},
  year={2017}
}

@inproceedings{fan,
  title={How far are we from solving the 2d \& 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)},
  author={Bulat, Adrian and Tzimiropoulos, Georgios},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1021--1030},
  year={2017}
}

@inproceedings{
adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@inproceedings{
sgdr,
title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Skq89Scxx}
}

@inproceedings{pesq,
  title={Perceptual evaluation of speech quality ({PESQ})-a new method for speech quality assessment of telephone networks and codecs},
  author={Rix, Antony W and Beerends, John G and Hollier, Michael P and Hekstra, Andries P},
  booktitle={2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221)},
  volume={2},
  pages={749--752},
  year={2001},
  organization={IEEE}
}

@inproceedings{stoi,
  title={A short-time objective intelligibility measure for time-frequency weighted noisy speech},
  author={Taal, Cees H and Hendriks, Richard C and Heusdens, Richard and Jensen, Jesper},
  booktitle={2010 IEEE international conference on acoustics, speech and signal processing},
  pages={4214--4217},
  year={2010},
  organization={IEEE}
}

@article{estoi,
  title={An algorithm for predicting the intelligibility of speech masked by modulated noise maskers},
  author={Jensen, Jesper and Taal, Cees H},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={24},
  number={11},
  pages={2009--2022},
  year={2016},
  publisher={IEEE}
}

@article{overview_of_affective_speech_synthesis,
  title={An overview of affective speech synthesis and conversion in the deep learning era},
  author={Triantafyllopoulos, Andreas and Schuller, Bj{\"o}rn W and {\.I}ymen, G{\"o}k{\c{c}}e and Sezgin, Metin and He, Xiangheng and Yang, Zijiang and Tzirakis, Panagiotis and Liu, Shuo and Mertes, Silvan and Andr{\'e}, Elisabeth and others},
  journal={Proceedings of the IEEE},
  year={2023},
  publisher={IEEE}
}

@article{overview_of_voice_conversion_and_its_challenges,
  title={An overview of voice conversion and its challenges: From statistical modeling to deep learning},
  author={Sisman, Berrak and Yamagishi, Junichi and King, Simon and Li, Haizhou},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={132--157},
  year={2020},
  publisher={IEEE}
}

@article{gan_paper,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3422622},
doi = {10.1145/3422622},
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
journal = {Commun. ACM},
month = {oct},
pages = {139–144},
numpages = {6}
}

@article{vae_paper,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{parallel_wavenet,
  title={{Parallel WaveNet}: Fast high-fidelity speech synthesis},
  author={Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and others},
  booktitle={International conference on machine learning},
  pages={3918--3926},
  year={2018},
  organization={PMLR}
}

@inproceedings{adversarial_audio_synthesis,
title={Adversarial Audio Synthesis},
author={Chris Donahue and Julian McAuley and Miller Puckette},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ByMVTsR5KQ},
}

@inproceedings{toward_domain_invariant_speech_recognition,
  title={Toward domain-invariant speech recognition via large scale training},
  author={Narayanan, Arun and Misra, Ananya and Sim, Khe Chai and Pundak, Golan and Tripathi, Anshuman and Elfeky, Mohamed and Haghani, Parisa and Strohman, Trevor and Bacchiani, Michiel},
  booktitle={2018 IEEE Spoken Language Technology Workshop (SLT)},
  pages={441--447},
  year={2018},
  organization={IEEE}
}

@inproceedings{unsupervised_pre_training_for_data_efficient_tts,
  title={Unsupervised Pre-Training for Data-Efficient Text-to-Speech on Low Resource Languages},
  author={Park, Seongyeon and Song, Myungseo and Kim, Bohyung and Oh, Tae-Hyun},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{hubert,
  title={{HuBERT}: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@inproceedings{superb_sg,
    title = "{SUPERB}-{SG}: Enhanced Speech processing Universal {PER}formance Benchmark for Semantic and Generative Capabilities",
    author = "Tsai, Hsiang-Sheng  and
      Chang, Heng-Jui  and
      Huang, Wen-Chin  and
      Huang, Zili  and
      Lakhotia, Kushal  and
      Yang, Shu-wen  and
      Dong, Shuyan  and
      Liu, Andy  and
      Lai, Cheng-I  and
      Shi, Jiatong  and
      Chang, Xuankai  and
      Hall, Phil  and
      Chen, Hsuan-Jui  and
      Li, Shang-Wen  and
      Watanabe, Shinji  and
      Mohamed, Abdelrahman  and
      Lee, Hung-yi",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.580",
    doi = "10.18653/v1/2022.acl-long.580",
    pages = "8479--8492",
    abstract = "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pre-trained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focusing on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pre-trained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation.",
}

@article{straight_vocoder,
  title={Restructuring speech representations using a pitch-adaptive time--frequency smoothing and an instantaneous-frequency-based {F0} extraction: Possible role of a repetitive structure in sounds},
  author={Kawahara, Hideki and Masuda-Katsuse, Ikuyo and De Cheveigne, Alain},
  journal={Speech communication},
  volume={27},
  number={3-4},
  pages={187--207},
  year={1999},
  publisher={Elsevier}
}

@article{generating_intelligible_audio_speech_from_visual_speech,
  title={Generating intelligible audio speech from visual speech},
  author={Le Cornu, Thomas and Milner, Ben},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={25},
  number={9},
  pages={1751--1761},
  year={2017},
  publisher={IEEE}
}

@article{griffin_lim,
  title={Signal estimation from modified short-time {F}ourier transform},
  author={Griffin, Daniel and Lim, Jae},
  journal={IEEE Transactions on acoustics, speech, and signal processing},
  volume={32},
  number={2},
  pages={236--243},
  year={1984},
  publisher={IEEE}
}

@inproceedings{vocoder_based_speech_synthesis,
  author={Daniel Michelsanti and Olga Slizovskaia and Gloria Haro and Emilia Gómez and Zheng-Hua Tan and Jesper Jensen},
  title={Vocoder-Based Speech Synthesis from Silent Videos},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={3530--3534},
  doi={10.21437/Interspeech.2020-1026}
}

@inproceedings{tacotron2,
  title={Natural {TTS} synthesis by conditioning wavenet on mel spectrogram predictions},
  author={Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={4779--4783},
  year={2018},
  organization={IEEE}
}

@article{lrs3,
  title={{LRS3-TED}: a large-scale dataset for visual speech recognition},
  author={Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1809.00496},
  year={2018}
}

@article{speech_reconstruction_visual_voice_memory,
  title={Speech reconstruction with reminiscent sound via visual voice memory},
  author={Hong, Joanna and Kim, Minsu and Park, Se Jin and Ro, Yong Man},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3654--3667},
  year={2021},
  publisher={IEEE}
}

@inproceedings{end_to_end_visual_speech_recognition_with_lstms,
  title={End-to-end visual speech recognition with {LSTMs}},
  author={Petridis, Stavros and Li, Zuwei and Pantic, Maja},
  booktitle={2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={2592--2596},
  year={2017},
  organization={IEEE}
}

@article{end_to_end_visual_speech_recognition_for_small_scale_datasets,
  title={End-to-end visual speech recognition for small-scale datasets},
  author={Petridis, Stavros and Wang, Yujiang and Ma, Pingchuan and Li, Zuwei and Pantic, Maja},
  journal={Pattern Recognition Letters},
  volume={131},
  pages={421--427},
  year={2020},
  publisher={Elsevier}
}

@article{large_scale_visual_speech_recognition,
  title={Large-scale visual speech recognition},
  author={Shillingford, Brendan and Assael, Yannis and Hoffman, Matthew W and Paine, Thomas and Hughes, C{\'\i}an and Prabhu, Utsav and Liao, Hank and Sak, Hasim and Rao, Kanishka and Bennett, Lorrayne and others},
  journal={arXiv preprint arXiv:1807.05162},
  year={2018}
}

@inproceedings{combining_residual_networks_with_lstms_for_lipreading,
  author={Themos Stafylakis and Georgios Tzimiropoulos},
  title={Combining Residual Networks with {LSTMs} for Lipreading},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={3652--3656},
  doi={10.21437/Interspeech.2017-85},
  url={http://dx.doi.org/10.21437/Interspeech.2017-85}
}

@article{deep_learning_based_automated_lipreading_a_survey,
  title={Deep learning-based automated lip-reading: A survey},
  author={Fenghour, Souheil and Chen, Daqing and Guo, Kun and Li, Bo and Xiao, Perry},
  journal={IEEE Access},
  volume={9},
  pages={121184--121205},
  year={2021},
  publisher={IEEE}
}

@article{review_on_research_progress_of_machine_lipreading,
  title={Review on research progress of machine lip reading},
  author={Pu, Gangqiang and Wang, Huijuan},
  journal={The Visual Computer},
  pages={1--17},
  year={2022},
  publisher={Springer}
}

@inproceedings{la_voce,
  title={{LA-VocE}: Low-{SNR} Audio-visual Speech Enhancement using Neural Vocoders},
  author={Mira, Rodrigo and Xu, Buye and Donley, Jacob and Kumar, Anurag and Petridis, Stavros and Ithapu, Vamsi Krishna and Pantic, Maja},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{av_hubert,
title={Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction},
author={Bowen Shi and Wei-Ning Hsu and Kushal Lakhotia and Abdelrahman Mohamed},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Z1Qlm11uOM}
}

@inproceedings{deep_multi_scale_video_prediction_beyond_mse,
  author       = {Micha{\"{e}}l Mathieu and
                  Camille Couprie and
                  Yann LeCun},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Deep multi-scale video prediction beyond mean square error},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.05440},
  timestamp    = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MathieuCL15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{image_to_image_translation_with_cgans,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

@inproceedings{key_value_memory_networks,
  author       = {Alexander H. Miller and
                  Adam Fisch and
                  Jesse Dodge and
                  Amir{-}Hossein Karimi and
                  Antoine Bordes and
                  Jason Weston},
  editor       = {Jian Su and
                  Xavier Carreras and
                  Kevin Duh},
  title        = {Key-Value Memory Networks for Directly Reading Documents},
  booktitle    = {Proceedings of the 2016 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2016, Austin, Texas, USA, November 1-4,
                  2016},
  pages        = {1400--1409},
  publisher    = {The Association for Computational Linguistics},
  year         = {2016},
  url          = {https://doi.org/10.18653/v1/d16-1147},
  doi          = {10.18653/v1/d16-1147},
  timestamp    = {Fri, 06 Aug 2021 00:40:32 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/MillerFDKBW16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{supervised_speech_separation_based_on_deep_learning,
  title={Supervised speech separation based on deep learning: An overview},
  author={Wang, DeLiang and Chen, Jitong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={26},
  number={10},
  pages={1702--1726},
  year={2018},
  publisher={IEEE}
}

@article{hearing_by_eye,
  title={Some preliminaries to a comprehensive account of audio-visual speech perception. Hearing by eye},
  author={Summerfield, Quentin},
  journal={The psychology of lip-reading},
  pages={3--51},
  year={1987},
  publisher={Lawrence Erlbaum Associates}
}

@article{audio_visual_enhancement_of_speech_in_noise,
  title={Audio-visual enhancement of speech in noise},
  author={Girin, Laurent and Schwartz, Jean-Luc and Feng, Gang},
  journal={The Journal of the Acoustical Society of America},
  volume={109},
  number={6},
  pages={3007--3020},
  year={2001},
  publisher={Acoustical Society of America}
}

@inproceedings{audio_visual_segmentation_and_the_cocktail_party_effect,
  title={Audio-visual segmentation and “the cocktail party effect”},
  author={Darrell, Trevor and Fisher Iii, John W and Viola, Paul},
  booktitle={International Conference on Multimodal Interfaces},
  pages={32--40},
  year={2000},
  organization={Springer}
}

@article{source_separation_of_convolutive_and_noisy_mixtures,
  title={Source separation of convolutive and noisy mixtures using audio-visual dictionary learning and probabilistic time-frequency masking},
  author={Liu, Qingju and Wang, Wenwu and Jackson, Philip JB and Barnard, Mark and Kittler, Josef and Chambers, Jonathon},
  journal={IEEE Transactions on Signal Processing},
  volume={61},
  number={22},
  pages={5520--5535},
  year={2013},
  publisher={IEEE}
}

@inproceedings{speaker_separation_using_visually_derived_binary_masks,
  title={Speaker separation using visually-derived binary masks},
  author={Khan, Faheem and Milner, Ben},
  booktitle={Auditory-Visual Speech Processing (AVSP) 2013},
  year={2013}
}

@article{avss_via_hidden_markov_models,
  title={Audio-visual sound separation via hidden {M}arkov models},
  author={Hershey, John and Casey, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}

@inproceedings{noisy_audio_feature_enhancement_using_av_speech_data,
  title={Noisy audio feature enhancement using audio-visual speech data},
  author={Goecke, Roland and Potamianos, Gerasimos and Neti, Chalapathy},
  booktitle={2002 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={2},
  pages={II--2025},
  year={2002},
  organization={IEEE}
}

@inproceedings{avse_with_avcdcn,
  title={Audio-visual speech enhancement with {AVCDCN} (audio-visual codebook dependent cepstral normalization)},
  author={Deligne, Sabine and Potamianos, Gerasimos and Neti, Chalapathy},
  booktitle={Sensor Array and Multichannel Signal Processing Workshop Proceedings, 2002},
  pages={68--71},
  year={2002},
  organization={IEEE}
}

@inproceedings{av_graphical_models_for_speech_processing,
  title={Audio-visual graphical models for speech processing},
  author={Hershey, John and Attias, Hagai and Jojic, Nebojsa and Kristjansson, Trausti},
  booktitle={2004 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={5},
  pages={V--649},
  year={2004},
  organization={IEEE}
}

@inproceedings{video_assisted_speech_source_separation,
  title={Video assisted speech source separation},
  author={Wang, Wenwu and Cosker, Darren and Hicks, Yulia and Saneit, S and Chambers, Jonathon},
  booktitle={Proceedings.(ICASSP'05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.},
  volume={5},
  pages={v--425},
  year={2005},
  organization={IEEE}
}

@inproceedings{effective_visually_derived_wiener_filtering_for_av_speech_processing,
  title={Effective visually-derived {W}iener filtering for audio-visual speech processing.},
  author={Almajai, Ibrahim and Milner, Ben},
  booktitle={AVSP},
  pages={134--139},
  year={2009}
}

@inproceedings{seeing_through_noise,
  title={Seeing through noise: Visually driven speaker separation and enhancement},
  author={Gabbay, Aviv and Ephrat, Ariel and Halperin, Tavi and Peleg, Shmuel},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={3051--3055},
  year={2018},
  organization={IEEE}
}

@inproceedings{visual_speech_enhancement,
  author={Aviv Gabbay and Asaph Shamir and Shmuel Peleg},
  title={Visual Speech Enhancement},
  year=2018,
  booktitle={Proc. Interspeech 2018},
  pages={1170--1174},
  doi={10.21437/Interspeech.2018-1955}
}

@article{audio_visual_speech_enhancement_using_multimodal_deep_cnns,
  title={Audio-visual speech enhancement using multimodal deep convolutional neural networks},
  author={Hou, Jen-Cheng and Wang, Syu-Siang and Lai, Ying-Hui and Tsao, Yu and Chang, Hsiu-Wen and Wang, Hsin-Min},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={2},
  number={2},
  pages={117--128},
  year={2018},
  publisher={IEEE}
}

@inproceedings{time_domain_av_speech_separation,
  title={Time domain audio visual speech separation},
  author={Wu, Jian and Xu, Yong and Zhang, Shi-Xiong and Chen, Lian-Wu and Yu, Meng and Xie, Lei and Yu, Dong},
  booktitle={2019 IEEE automatic speech recognition and understanding workshop (ASRU)},
  pages={667--673},
  year={2019},
  organization={IEEE}
}

@inproceedings{tasnet,
  title={{TaSNet}: time-domain audio separation network for real-time, single-channel speech separation},
  author={Luo, Yi and Mesgarani, Nima},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={696--700},
  year={2018},
  organization={IEEE}
}

@inproceedings{muse,
  title={Muse: Multi-modal target speaker extraction with visual cues},
  author={Pan, Zexu and Tao, Ruijie and Xu, Chenglin and Li, Haizhou},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6678--6682},
  year={2021},
  organization={IEEE}
}

@inproceedings{av_speech_codecs_rethinking_avse,
  title={Audio-visual speech codecs: Rethinking audio-visual speech enhancement by re-synthesis},
  author={Yang, Karren and Markovi{\'c}, Dejan and Krenn, Steven and Agrawal, Vasu and Richard, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8227--8237},
  year={2022}
}

@article{avse_using_conditional_vaes,
  title={Audio-visual speech enhancement using conditional variational auto-encoders},
  author={Sadeghi, Mostafa and Leglaive, Simon and Alameda-Pineda, Xavier and Girin, Laurent and Horaud, Radu},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={1788--1800},
  year={2020},
  publisher={IEEE}
}

@article{improved_training_of_wasserstein_gans,
  title={Improved training of {W}asserstein {GAN}s},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

















