{
  "title": "Dimensionless Policies based on the Buckingham $π$ Theorem: Is This a Good Way to Generalize Numerical Results?",
  "authors": [
    "Alexandre Girard"
  ],
  "submission_date": "2023-07-29T00:51:26+00:00",
  "revised_dates": [
    "2024-02-28T21:52:19+00:00"
  ],
  "abstract": "The answer to the question posed in the title is yes if the context (the list of variables defining the motion control problem) is dimensionally similar. This article explores the use of the Buckingham $π$ theorem as a tool to encode the control policies of physical systems into a more generic form of knowledge that can be reused in various situations. This approach can be interpreted as enforcing invariance to the scaling of the fundamental units in an algorithm learning a control policy. First, we show, by restating the solution to a motion control problem using dimensionless variables, that (1) the policy mapping involves a reduced number of parameters and (2) control policies generated numerically for a specific system can be transferred exactly to a subset of dimensionally similar systems by scaling the input and output variables appropriately. Those two generic theoretical results are then demonstrated, with numerically generated optimal controllers, for the classic motion control problem of swinging up a torque-limited inverted pendulum and positioning a vehicle in slippery conditions. We also discuss the concept of regime, a region in the space of context variables, that can help to relax the similarity condition. Furthermore, we discuss how applying dimensional scaling of the input and output of a context-specific black-box policy is equivalent to substituting new system parameters in an analytical equation under some conditions, using a linear quadratic regulator (LQR) and a computed torque controller as examples. It remains to be seen how practical this approach can be to generalize policies for more complex high-dimensional problems, but the early results show that it is a promising transfer learning tool for numerical approaches like dynamic programming and reinforcement learning.",
  "categories": [
    "math.OC",
    "cs.AI",
    "cs.RO",
    "eess.SY"
  ],
  "primary_category": "math.OC",
  "doi": "10.3390/math12050709",
  "journal_ref": "Mathematics 2024, 12(5), 709",
  "arxiv_id": "2307.15852",
  "pdf_url": "https://arxiv.org/pdf/2307.15852v2",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 56926373,
  "size_after_bytes": 2669624
}