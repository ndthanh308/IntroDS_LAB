\subsection{Oriented Point Cloud Estimation}
\label{sec:opcl_est}

The oriented point cloud estimation module takes the coarsely deformed meshes $\mathcal{M}_c$ as input and predicts the coordinates and normals of an upsampled oriented point cloud $\mathcal{O}_{up}$ for Poisson surface reconstruction. This is achieved through encoding features on the graph (mesh), followed by the estimation of point positions and normals in sequence.

\vspace{-0.8em}
\paragraph{Feature Encoding} Same as Vox2Cortex, we applied a GCN-based residual block to encode point features based on the image features. The residual block is composed of three GCNs with subsequent batch normalization layers and ReLU activations. The input residuum is added before the last ReLU output and reshaped with nearest-neighbor interpolation if needed. Let $\mathbf{f}_i \in \mathbb{R}^{d_{out}}$ define the encoded feature of a vertex $\boldsymbol{v_i} \in \mathcal{V}_c$.  

\vspace{-0.8em}
\paragraph{Position Estimation} Our method predicts multiple offsets associated with each vertex to upsample the point cloud and improve shape representation accuracy. Instead of using GCNs, which tend to generate over-smoothed offsets, we employ the gated linear unit (GLU) \cite{dauphin2017language} to estimate the displacements $\boldsymbol{d_i} \in \mathbb{R}^{(S \times 3)}$ with an upsample scale of $S$ for each vertex. This can be expressed as:
\begin{equation}
    \boldsymbol{d_i} = (\mathbf{W}_0 \mathbf{f}_i+\mathbf{b}_0) \odot \sigma(\mathbf{W}_1 \mathbf{f}_i+\mathbf{b}_1)
    \label{eq:glu_point}
\end{equation}
where $\mathbf{W}_0, \mathbf{W}_1 \in \mathbb{R}^{(S \times 3) \times d_{out}}$ together with $\mathbf{b}_0, \mathbf{b}_1 \in \mathbb{R}^{(S \times 3)}$ represent linear projections and $\sigma$ represents sigmoid function. The upsampled displacements are then added to the vertex $\boldsymbol{v}_i$ to obtain the point cloud position $\boldsymbol{p}_i^{up} \in \mathbb{R}^{S \times 3}$. In our experiments, $S$ is set to be 7.

\vspace{-0.8em}
\paragraph{Normal Estimation} We begin by extracting multi-scale image features for each point in $\mathcal{P}_{up}$. Features from points that have been displaced from the same vertex in $\mathcal{M}_c$ are stacked together, and point features $\mathbf{f}_i^{up}$ are learned through a residual GCN layer. 
Based on $\mathbf{f}_i^{up}$ and $\boldsymbol{p}_i^{up}$, the normals $\boldsymbol{n}_i^{up} \in \mathbb{R}^{S \times 3}$ are predicted by a GLU, formulated as
\begin{equation}
\begin{split}
    \boldsymbol{n}_i^{up} = (\mathbf{W}_0^{up} [\mathbf{f}_i^{up}, \boldsymbol{p}_i^{up}]+\mathbf{b}_0^{up}) \odot \\
    \sigma (\mathbf{W}_1^{up}[\mathbf{f}_i^{up}, \boldsymbol{p}_i^{up}]+\mathbf{b}_1^{up})
    \label{eq:glu_normal}
\end{split}
\end{equation}
where $\mathbf{W}_0^{up}, \mathbf{W}_1^{up} \in \mathbb{R}^{(S \times 3) \times (d_{out} + S \times 3)}$ together with $\mathbf{b}_0, \mathbf{b}_1 \in \mathbb{R}^{(S \times 3)}$ represent linear projections and $\sigma$ represents sigmoid function. Now, the upsampled oriented point cloud $\mathcal{O}_{up} = (\mathcal{P}_{up}=\{\boldsymbol{p}_i^{up}\}_{i=1}^K, ~ \mathcal{N}_{up}=\{\boldsymbol{n}_i^{up}\}_{i=1}^K)$ is achieved.

\vspace{-0.8em}
\paragraph{Surface Reconstruction}
In practice, the Differentiable Poisson Surface Reconstruction (DPSR) method acquires a uniformly discretized point normal field $\mathbf{v}$ by rasterizing the predicted oriented point cloud normals $\mathcal{N}_{up}$ onto four uniformly sampled 3D grids. Utilizing Eq.~\ref{eq:dpsr_nor}, an $r^3$ indicator grid $\hat{\chi}$ can be obtained for each cortical structure, and subsequently used to reconstruct cortical meshes $\hat{\mathcal{M}}=\left(\hat{\mathcal{V}}, \hat{\mathcal{E}}\right)$ via Marching Cubes.
%%
The smoothness and resolution of the indicator grid $\hat{\chi}$ can be modulated by the values of $\sigma$ and $r$. In our experiments, we set $\sigma$ to 2 and $r$ to 256 for both training and inference.

\vspace{-0.8em}
\paragraph{Training} 
For each cortical surface, we acquire the ground truth indicator grid $\chi$ by running DPSR on densely sampled oriented point clouds sampled from the (pseudo) ground truth meshes. Next, we use a Sobel filter \cite{kroon2009numerical} to detect edges in $\chi$. The resulting edge map is then smoothed using a Gaussian filter with a kernel size of 7 and a standard deviation of 1. Finally, we apply a weighted mean square error (wMSE) to measure the difference between predicted and ground truth indicator grids, especially along the boundary regions, and train Hybrid-CSR end-to-end, given as 
\begin{equation}
    \mathcal{L}_{\text {DPSR}}=\|w_{edge} \odot \left( \hat{\chi}-\chi \right)\|^2
\label{eq:loss}
\end{equation}
where $w_{edge} \in \mathbb{R}^n$ is the smoothed edge map of the ground truth indicator grid. More implementation details are included in the supplementary.