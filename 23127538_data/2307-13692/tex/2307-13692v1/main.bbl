\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alabdulmohsin et~al.(2022)Alabdulmohsin, Neyshabur, and
  Zhai]{alabdulmohsin2022revisiting}
Ibrahim~M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 22300--22312, 2022.

\bibitem[Arora et~al.(2023)Arora, Singh, and Mausam]{arora2023llms}
Daman Arora, Himanshu~Gaurav Singh, and Mausam.
\newblock Have {LLMs} advanced enough? {A} challenging problem solving
  benchmark for {Large Language Models}, 2023.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen,
  Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli,
  Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite,
  Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson,
  Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly,
  Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish,
  Brown, and Kaplan]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain,
  Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
  Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,
  Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi
  Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott
  Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham,
  Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R.
  Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam
  McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional {AI}: {Harmlessness} from {AI}, feedback, 2022.

\bibitem[Barbri(2007)]{barbri2007}
Barbri.
\newblock \emph{Barbri Practice Questions: Multistate Testing Practice
  Questions}.
\newblock Thomson/Bar/Bri, 2007.
\newblock ISBN 9780314174017.

\bibitem[Bommarito~II and Katz(2022)]{bommarito2022gpt}
Michael Bommarito~II and Daniel~Martin Katz.
\newblock {GPT} takes the bar exam.
\newblock \emph{arXiv preprint arXiv:2212.14402}, 2022.

\bibitem[Bowman(2021)]{bowman2021dangers}
Samuel~R. Bowman.
\newblock The dangers of underclaiming: {Reasons} for caution when reporting
  how {NLP} systems fail, 2021.

\bibitem[Brayman and Kukush(2018)]{braymankukush}
Volodymyr Brayman and A.~G. Kukush.
\newblock \emph{Undergraduate Mathematics Competitions (1995-2016): {Taras}
  Shevchenko National University of Kyiv}.
\newblock Springer, 2018.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{CoRR}, abs/2005.14165, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: {Early} experiments with
  {GPT-4}.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Caballero et~al.(2023)Caballero, Gupta, Rish, and
  Krueger]{caballero2023broken}
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger.
\newblock Broken neural scaling laws, 2023.

\bibitem[Campbell et~al.(2017)Campbell, Murphree, Warner, Wachholz, Zahler, and
  Hademenos]{MCAT}
Candice~McCloskey Campbell, Shaun Murphree, Jennifer~M. Warner, Amy~B.
  Wachholz, Kathy~A. Zahler, and George~J. Hademenos.
\newblock \emph{McGraw-Hill Education 3 {MCAT} Practice Tests, Third Edition}.
\newblock McGraw-Hill Education, Jan 2017.
\newblock ISBN 1259859622.

\bibitem[Caplan(2023)]{caplan2023gpt}
Bryan Caplan.
\newblock {GPT} retakes my midterm and gets an {A}, 2023.
\newblock URL
  \url{https://betonit.substack.com/p/gpt-retakes-my-midterm-and-gets-an}.

\bibitem[{Chiang} and {Lee}(2023)]{2023arXiv230501937C}
Cheng-Han {Chiang} and Hung-yi {Lee}.
\newblock Can {Large Language Models} be an alternative to human evaluations?
\newblock \emph{arXiv e-prints}, art. arXiv:2305.01937, may 2023.
\newblock \doi{10.48550/arXiv.2305.01937}.

\bibitem[Chollet(2019)]{chollet2019measure}
Fran{\c{c}}ois Chollet.
\newblock On the measure of intelligence.
\newblock \emph{arXiv preprint arXiv:1911.01547}, 2019.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{PaLM}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock {PaLM}: {Scaling} language modeling with {Pathways}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Hilton, Nakano, Hesse,
  and Schulman]{GSM8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{CoRR}, abs/2110.14168, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {Pre-training} of deep bidirectional transformers for
  language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, jun 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[{Fu} et~al.(2023){Fu}, {Ng}, {Jiang}, and {Liu}]{2023arXiv230204166F}
Jinlan {Fu}, See-Kiong {Ng}, Zhengbao {Jiang}, and Pengfei {Liu}.
\newblock {GPTScore}: {Evaluate} as you desire.
\newblock \emph{arXiv e-prints}, art. arXiv:2302.04166, feb 2023.
\newblock \doi{10.48550/arXiv.2302.04166}.

\bibitem[Fu et~al.(2023)Fu, Ou, Chen, Wan, Peng, and
  Khot]{fu2023chainofthought}
Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot.
\newblock Chain-of-thought hub: {A} continuous effort to measure large language
  models' reasoning performance, 2023.

\bibitem[Gelca and Andreescu(2017)]{putnam}
Răzvan Gelca and Titu Andreescu.
\newblock \emph{Putnam and beyond}.
\newblock Springer, 2017.

\bibitem[Gendron et~al.(2023)Gendron, Bao, Witbrock, and
  Dobbie]{gendron2023large}
Gaël Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie.
\newblock Large language models are not abstract reasoners, 2023.

\bibitem[Geva et~al.(2021)Geva, Khashabi, Segal, Khot, Roth, and
  Berant]{geva2021did}
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan
  Berant.
\newblock Did aristotle use a laptop? a question answering benchmark with
  implicit reasoning strategies, 2021.

\bibitem[Ghahramani(2023)]{palm2}
Zoubin Ghahramani.
\newblock Introducing {PaLM} 2, 2023.
\newblock URL
  \url{https://blog.google/technology/ai/google-palm-2-ai-large-language-model}.

\bibitem[Harvard~University(2021)]{harvard}
Department of~Mathematics Harvard~University.
\newblock Qualifying examination for fall 2021, Aug 2021.
\newblock URL \url{https://www.math.harvard.edu/media/quals-
  F21_with_solutions.pdf}.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{MMMLU}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.03300}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang,
  Song, and Steinhardt]{MATH}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the {MATH} dataset.
\newblock \emph{CoRR}, abs/2103.03874, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.03874}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican,
  Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and
  Sifre]{Chinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den
  Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
  Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Katz et~al.(2023)Katz, Bommarito, Gao, and Arredondo]{katz2023gpt}
Daniel~Martin Katz, Michael~James Bommarito, Shang Gao, and Pablo Arredondo.
\newblock {GPT-4} passes the bar exam.
\newblock \emph{Available at SSRN 4389233}, 2023.

\bibitem[{Kocmi} and {Federmann}(2023)]{2023arXiv230214520K}
Tom {Kocmi} and Christian {Federmann}.
\newblock Large language models are state-of-the-art evaluators of translation
  quality.
\newblock \emph{arXiv e-prints}, art. arXiv:2302.14520, feb 2023.
\newblock \doi{10.48550/arXiv.2302.14520}.

\bibitem[Kojima et~al.(2023)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2023large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners, 2023.

\bibitem[Koncel-Kedziorski et~al.(2016)Koncel-Kedziorski, Roy, Amini, Kushman,
  and Hajishirzi]{koncel-kedziorski-etal-2016-mawps}
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh
  Hajishirzi.
\newblock {MAWPS}: {A} math word problem repository.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1152--1157, San Diego, California, jun 2016. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1136}.
\newblock URL \url{https://aclanthology.org/N16-1136}.

\bibitem[Kung et~al.(2023)Kung, Cheatham, Medenilla, Sillos, De~Leon,
  Elepa{\~n}o, Madriaga, Aggabao, Diaz-Candido, Maningo,
  et~al.]{kung2023performance}
Tiffany~H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie
  De~Leon, Camille Elepa{\~n}o, Maria Madriaga, Rimel Aggabao, Giezel
  Diaz-Candido, James Maningo, et~al.
\newblock Performance of {ChatGPT} on {USMLE}: {Potential} for {AI-assisted}
  medical education using large language models.
\newblock \emph{PLOS Digital Health}, 2\penalty0 (2):\penalty0 e0000198, 2023.

\bibitem[Laskar et~al.(2023)Laskar, Bari, Rahman, Bhuiyan, Joty, and
  Huang]{laskar2023systematic}
Md~Tahmid~Rahman Laskar, M~Saiful Bari, Mizanur Rahman, Md~Amran~Hossen
  Bhuiyan, Shafiq Joty, and Jimmy~Xiangji Huang.
\newblock A systematic study and comprehensive evaluation of {ChatGPT} on
  benchmark datasets, 2023.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski,
  Ramasesh, Slone, Anil, Schlag, Gutman-Solo, Wu, Neyshabur, Gur-Ari, and
  Misra]{Minerva}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk
  Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo
  Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.
\newblock Solving quantitative reasoning problems with language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2206.14858}.

\bibitem[Li et~al.(2023)Li, Lin, Zhang, Fu, Chen, Lou, and Chen]{li2023making}
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and
  Weizhu Chen.
\newblock Making large language models better reasoners with step-aware
  verifier, 2023.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, Newman, Yuan, Yan, Zhang, Cosgrove, Manning,
  Ré, Acosta-Navas, Hudson, Zelikman, Durmus, Ladhak, Rong, Ren, Yao, Wang,
  Santhanam, Orr, Zheng, Yuksekgonul, Suzgun, Kim, Guha, Chatterji, Khattab,
  Henderson, Huang, Chi, Xie, Santurkar, Ganguli, Hashimoto, Icard, Zhang,
  Chaudhary, Wang, Li, Mai, Zhang, and Koreeda]{liang2022holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  Benjamin Newman, Binhang Yuan, Bobby Yan, Ce~Zhang, Christian Cosgrove,
  Christopher~D. Manning, Christopher Ré, Diana Acosta-Navas, Drew~A. Hudson,
  Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu
  Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul,
  Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter
  Henderson, Qian Huang, Ryan Chi, Sang~Michael Xie, Shibani Santurkar, Surya
  Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,
  William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.
\newblock Holistic evaluation of language models, 2022.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2023lets}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step, 2023.

\bibitem[Lim et~al.(2019)Lim, Lai, and Kwek]{optics}
Swee~Cheng Lim, Choy~Heng Lai, and Leong~Chuan Kwek.
\newblock \emph{Problems and solutions on optics}.
\newblock World Scientific, 2019.

\bibitem[Lim(1996)]{statmech}
Yung-kuo Lim.
\newblock \emph{Problems and solutions on thermodynamics and Statistical
  Mechanics}.
\newblock World Scientific, 1996.

\bibitem[Lim(1998)]{quantum}
Yung-kuo Lim.
\newblock \emph{Problems and solutions in quantum mechanics: {Major}, American
  universities ph. D. qualifying questions and, solutions}.
\newblock World Scientific, 1998.

\bibitem[Lim(2007)]{electromagnetism}
Yung-kuo Lim.
\newblock \emph{Problems and solutions on electromagnetism}.
\newblock World Scientific Pub. Co, 2007.

\bibitem[Lim and Qiang(2001)]{mechanics}
Yung-kuo Lim and Yuan-qi Qiang.
\newblock \emph{Problems and solutions on Mechanics}.
\newblock World Scientif., 2001.

\bibitem[Ling et~al.(2017)Ling, Yogatama, Dyer, and
  Blunsom]{ling-etal-2017-program}
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
\newblock Program induction by rationale generation: {Learning}, to solve and
  explain algebraic word problems.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 158--167,
  Vancouver, Canada, jul 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1015}.
\newblock URL \url{https://aclanthology.org/P17-1015}.

\bibitem[{Liu} et~al.(2023){Liu}, {Iter}, {Xu}, {Wang}, {Xu}, and
  {Zhu}]{2023arXiv230316634L}
Yang {Liu}, Dan {Iter}, Yichong {Xu}, Shuohang {Wang}, Ruochen {Xu}, and
  Chenguang {Zhu}.
\newblock G-eval: {NLG} evaluation using {GPT-4} with better human alignment.
\newblock \emph{arXiv e-prints}, art. arXiv:2303.16634, mar 2023.
\newblock \doi{10.48550/arXiv.2303.16634}.

\bibitem[Meurer et~al.(2017)Meurer, Smith, Paprocki, \v{C}ert\'{i}k, Kirpichev,
  Rocklin, Kumar, Ivanov, Moore, Singh, Rathnayake, Vig, Granger, Muller,
  Bonazzi, Gupta, Vats, Johansson, Pedregosa, Curry, Terrel, Rou\v{c}ka, Saboo,
  Fernando, Kulal, Cimrman, and Scopatz]{10.7717/peerj-cs.103}
Aaron Meurer, Christopher~P. Smith, Mateusz Paprocki, Ond\v{r}ej
  \v{C}ert\'{i}k, Sergey~B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu
  Ivanov, Jason~K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian~E.
  Granger, Richard~P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats,
  Fredrik Johansson, Fabian Pedregosa, Matthew~J. Curry, Andy~R. Terrel,
  \v{S}t\v{e}p\'{a}n Rou\v{c}ka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal,
  Robert Cimrman, and Anthony Scopatz.
\newblock Sympy: {Symbolic} computing in python.
\newblock \emph{PeerJ Computer Science}, 3:\penalty0 e103, jan 2017.
\newblock ISSN 2376-5992.
\newblock \doi{10.7717/peerj-cs.103}.
\newblock URL \url{https://doi.org/10.7717/peerj-cs.103}.

\bibitem[Miao et~al.(2020)Miao, Liang, and Su]{miao-etal-2020-diverse}
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
\newblock A diverse corpus for evaluating and developing {E}nglish math word
  problem solvers.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 975--984, Online, jul 2020. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.92}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.92}.

\bibitem[Noy and Zhang(2023)]{noy2023experimental}
Shakked Noy and Whitney Zhang.
\newblock Experimental evidence on the productivity effects of generative
  artificial intelligence.
\newblock \emph{Available at SSRN 4375283}, 2023.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber,
  Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{nye2021work}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, Charles Sutton, and Augustus Odena.
\newblock Show your work: {Scratchpads} for intermediate computation with
  language models, 2021.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock {GPT-4} technical report, 2023.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel2021nlp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are {NLP} models really able to solve simple math word problems?,
  2021.

\bibitem[Perez et~al.(2022)Perez, Ringer, Luko{\v{s}}i{\=u}t{\.e}, Nguyen,
  Chen, Heiner, Pettit, Olsson, Kundu, Kadavath, et~al.]{perez2022discovering}
Ethan Perez, Sam Ringer, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Karina Nguyen,
  Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu,
  Saurav Kadavath, et~al.
\newblock Discovering language model behaviors with model-written evaluations.
\newblock \emph{arXiv preprint arXiv:2212.09251}, 2022.

\bibitem[{Pushkarna} et~al.(2022){Pushkarna}, {Zaldivar}, and
  {Kjartansson}]{2022arXiv220401075P}
Mahima {Pushkarna}, Andrew {Zaldivar}, and Oddur {Kjartansson}.
\newblock {Data Cards: Purposeful and Transparent Dataset Documentation for
  Responsible AI}.
\newblock \emph{arXiv e-prints}, art. arXiv:2204.01075, April 2022.
\newblock \doi{10.48550/arXiv.2204.01075}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Roy and Roth(2016)]{roy2016solving}
Subhro Roy and Dan Roth.
\newblock Solving general arithmetic word problems, 2016.

\bibitem[Shakarian et~al.(2023)Shakarian, Koyyalamudi, Ngu, and
  Mareedu]{shakarian2023independent}
Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu.
\newblock An independent evaluation of {ChatGPT} on mathematical word problems
  {(MWP)}.
\newblock \emph{arXiv preprint arXiv:2302.13814}, 2023.

\bibitem[Shapira et~al.(2023)Shapira, Levy, Alavi, Zhou, Choi, Goldberg, Sap,
  and Shwartz]{shapira2023clever}
Natalie Shapira, Mosh Levy, Seyed~Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav
  Goldberg, Maarten Sap, and Vered Shwartz.
\newblock {Clever Hans} or {Neural Theory of Mind}? {Stress} testing social
  reasoning in large language models.
\newblock \emph{arXiv preprint arXiv:2305.14763}, 2023.

\bibitem[Souza and Silva(2008)]{berkeley}
Paulo N~de Souza and Jorge~N. Silva.
\newblock \emph{Berkeley problems in Mathematics}.
\newblock Springer New York, 2008.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: {Quantifying} and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.
\newblock \doi{10.48550/ARXIV.2206.04615}.
\newblock URL \url{https://arxiv.org/abs/2206.04615}.

\bibitem[Steinhardt(2022)]{steinhardt2022oneyearin}
Jacob Steinhardt.
\newblock {AI} forecasting: {One} year in, 2022.
\newblock URL
  \url{https://bounded-regret.ghost.io/ai-forecasting-one-year-in/}.

\bibitem[Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant]{CommonSenseQA}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {CommonsenseQA}: {A} question answering challenge targeting
  commonsense knowledge.
\newblock \emph{CoRR}, abs/1811.00937, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.00937}.

\bibitem[Valmeekam et~al.(2023)Valmeekam, Olmo, Sreedharan, and
  Kambhampati]{valmeekam2023large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
\newblock Large language models still can't plan (a benchmark for {LLMs} on
  planning and reasoning about change), 2023.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Pruksachatkun, Nangia, Singh,
  Michael, Hill, Levy, and Bowman]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel~R. Bowman.
\newblock {SuperGLUE}: {A} stickier benchmark for general-purpose language
  understanding systems, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock 2019{\natexlab{b}}.
\newblock In the Proceedings of ICLR.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery,
  and Zhou]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models, 2023.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang,
  Dean, and Fedus]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus.
\newblock Emergent abilities of large language models, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Ichter,
  Xia, Chi, Le, and Zhou]{ChainOfThought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
  Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2201.11903}.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and
  Manning]{yang2018hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W. Cohen, Ruslan
  Salakhutdinov, and Christopher~D. Manning.
\newblock {HotpotQA}: {A} dataset for diverse, explainable multi-hop question
  answering, 2018.

\bibitem[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and
  Narasimhan]{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L. Griffiths, Yuan
  Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: {Deliberate} problem solving with large language
  models, 2023.

\bibitem[Yu et~al.(2022)Yu, Wang, Golovneva, Alkhamissy, Ghosh, Diab, and
  Celikyilmaz]{yu2022alert}
Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab,
  and Asli Celikyilmaz.
\newblock {ALERT}: {Adapting} language models to reasoning tasks, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Shuttleworth, Austin, Hicke, Tang, Karnik,
  Granberry, and Drori]{zhang2022dataset-machine-learning}
Sarah Zhang, Reece Shuttleworth, Derek Austin, Yann Hicke, Leonard Tang,
  Sathwik Karnik, Darnell Granberry, and Iddo Drori.
\newblock A dataset and benchmark for automatically answering and generating
  machine learning final exams.
\newblock \emph{arXiv preprint arXiv:2206.05442}, 2022.

\bibitem[Zhongguo-Kexue-Jishu-Daxue(1990)]{zhongguo1990major}
Hefei Zhongguo-Kexue-Jishu-Daxue.
\newblock \emph{Major American universities Ph. D. qualifying questions and
  solutions. 5. Problems and solutions on thermodynamics and statistical
  mechanics}.
\newblock World Scientific, 1990.

\end{thebibliography}
