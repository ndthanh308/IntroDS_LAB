\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(2018)Anderson, Wu, Teney, Bruce, Johnson, Sünderhauf,
  Reid, Gould, and van~den Hengel]{Anderson_2018_CVPR}
Peter Anderson, Qi~Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko
  Sünderhauf, Ian Reid, Stephen Gould, and Anton van~den Hengel.
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2018.

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017learning}
Jacob Andreas, Dan Klein, and Sergey Levine.
\newblock Learning with latent language.
\newblock \emph{arXiv preprint arXiv:1711.00482}, 2017.

\bibitem[Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and
  Chen]{chevalier2023adapting}
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.
\newblock Adapting language models to compress contexts.
\newblock \emph{arXiv preprint arXiv:2305.14788}, 2023.

\bibitem[Chevalier-Boisvert et~al.(2019)Chevalier-Boisvert, Bahdanau, Lahlou,
  Willems, Saharia, Nguyen, and Bengio]{chevalier-boisvert2018babyai}
Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,
  Chitwan Saharia, Thien~Huu Nguyen, and Yoshua Bengio.
\newblock Baby{AI}: First steps towards grounded language learning with a human
  in the loop.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJeXCo0cYX}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Narang, Mishra,
  Yu, Zhao, Huang, Dai, Yu, Petrov, Chi, Dean, Devlin, Roberts, Zhou, Le, and
  Wei]{flan-t5}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping
  Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob
  Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.11416}.

\bibitem[Deng et~al.(2022)Deng, Wang, Hsieh, Wang, Guo, Shu, Song, Xing, and
  Hu]{deng-etal-2022-rlprompt}
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu,
  Meng Song, Eric Xing, and Zhiting Hu.
\newblock {RLP}rompt: Optimizing discrete text prompts with reinforcement
  learning.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  3369--3391, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.222}.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter,
  Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
  Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock \emph{arXiv preprint arXiv:2303.03378}, 2023.

\bibitem[Gao et~al.(2021{\natexlab{a}})Gao, Geng, Zhang, Ma, Fang, Zhang, Li,
  and Qiao]{gao2021clip}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang,
  Hongsheng Li, and Yu~Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{arXiv preprint arXiv:2110.04544}, 2021{\natexlab{a}}.

\bibitem[Gao et~al.(2021{\natexlab{b}})Gao, Fisch, and
  Chen]{gao-etal-2021-making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  3816--3830,
  Online, August 2021{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.295}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.295}.

\bibitem[Guhur et~al.(2023)Guhur, Chen, Pinel, Tapaswi, Laptev, and
  Schmid]{guhur2023instruction}
Pierre-Louis Guhur, Shizhe Chen, Ricardo~Garcia Pinel, Makarand Tapaswi, Ivan
  Laptev, and Cordelia Schmid.
\newblock Instruction-driven history-aware policies for robotic manipulations.
\newblock In \emph{Conference on Robot Learning}, pp.\  175--187. PMLR, 2023.

\bibitem[Gupta et~al.(2019)Gupta, Dollar, and Girshick]{gupta2019lvis}
Agrim Gupta, Piotr Dollar, and Ross Girshick.
\newblock Lvis: A dataset for large vocabulary instance segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  5356--5364, 2019.

\bibitem[He et~al.(2017)He, Gkioxari, Doll{\'a}r, and Girshick]{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  2961--2969, 2017.

\bibitem[Huang et~al.(2022{\natexlab{a}})Huang, Abbeel, Pathak, and
  Mordatch]{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable
  knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9118--9147. PMLR, 2022{\natexlab{a}}.

\bibitem[Huang et~al.(2022{\natexlab{b}})Huang, Xia, Xiao, Chan, Liang,
  Florence, Zeng, Tompson, Mordatch, Chebotar, et~al.]{huanginner}
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy
  Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et~al.
\newblock Inner monologue: Embodied reasoning through planning with language
  models.
\newblock In \emph{6th Annual Conference on Robot Learning},
  2022{\natexlab{b}}.

\bibitem[Huang et~al.(2023)Huang, Xia, Shah, Driess, Zeng, Lu, Florence,
  Mordatch, Levine, Hausman, et~al.]{huang2023grounded}
Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete
  Florence, Igor Mordatch, Sergey Levine, Karol Hausman, et~al.
\newblock Grounded decoding: Guiding text generation with grounded models for
  robot control.
\newblock \emph{arXiv preprint arXiv:2303.00855}, 2023.

\bibitem[Ichter et~al.(2022)Ichter, Brohan, Chebotar, Finn, Hausman, Herzog,
  Ho, Ibarz, Irpan, Jang, Julian, Kalashnikov, Levine, Lu, Parada, Rao,
  Sermanet, Toshev, Vanhoucke, Xia, Xiao, Xu, Yan, Brown, Ahn, Cortes, Sievers,
  Tan, Xu, Reyes, Rettinghouse, Quiambao, Pastor, Luu, Lee, Kuang, Jesmonth,
  Jeffrey, Ruano, Hsu, Gopalakrishnan, David, Zeng, and Fu]{ichter2022do}
Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
  Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan
  Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka
  Rao, Pierre Sermanet, Alexander~T Toshev, Vincent Vanhoucke, Fei Xia, Ted
  Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas
  Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell
  Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally
  Jesmonth, Kyle Jeffrey, Rosario~Jauregui Ruano, Jasmine Hsu, Keerthana
  Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan~Kelly Fu.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock In \emph{6th Annual Conference on Robot Learning}, 2022.
\newblock URL \url{https://openreview.net/forum?id=bdHkMjBJG_w}.

\bibitem[Jiang et~al.(2022)Jiang, Gupta, Zhang, Wang, Dou, Chen, Fei-Fei,
  Anandkumar, Zhu, and Fan]{jiang2022vima}
Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun
  Chen, Li~Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan.
\newblock Vima: General robot manipulation with multimodal prompts.
\newblock \emph{arXiv preprint arXiv:2210.03094}, 2022.

\bibitem[Ju et~al.(2022)Ju, Han, Zheng, Zhang, and Xie]{ju2022prompting}
Chen Ju, Tengda Han, Kunhao Zheng, Ya~Zhang, and Weidi Xie.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV}, pp.\  105--124.
  Springer, 2022.

\bibitem[Kemp et~al.(2022)Kemp, Edsinger, Clever, and
  Matulevich]{kemp2022design}
Charles~C Kemp, Aaron Edsinger, Henry~M Clever, and Blaine Matulevich.
\newblock The design of stretch: A compact, lightweight mobile manipulator for
  indoor human environments.
\newblock In \emph{2022 International Conference on Robotics and Automation
  (ICRA)}, pp.\  3150--3157. IEEE, 2022.

\bibitem[Kim et~al.(2023)Kim, Baldi, and McAleer]{kim2023language}
Geunwoo Kim, Pierre Baldi, and Stephen McAleer.
\newblock Language models can solve computer tasks.
\newblock \emph{arXiv preprint arXiv:2303.17491}, 2023.

\bibitem[Ku et~al.(2020)Ku, Anderson, Patel, Ie, and Baldridge]{ku2020room}
Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge.
\newblock Room-across-room: Multilingual vision-and-language navigation with
  dense spatiotemporal grounding.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  4392--4412, 2020.

\bibitem[K\"{u}ttler et~al.(2020)K\"{u}ttler, Nardelli, Miller, Raileanu,
  Selvatici, Grefenstette, and Rockt\"{a}schel]{kuttler2020nle}
Heinrich K\"{u}ttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu,
  Marco Selvatici, Edward Grefenstette, and Tim Rockt\"{a}schel.
\newblock The nethack learning environment.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  7671--7684. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/569ff987c643b4bedf504efda8f786c2-Paper.pdf}.

\bibitem[Leblond et~al.(2021)Leblond, Alayrac, Sifre, Pislar, Jean-Baptiste,
  Antonoglou, Simonyan, and Vinyals]{leblond2021machine}
R{\'e}mi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar, Lespiau
  Jean-Baptiste, Ioannis Antonoglou, Karen Simonyan, and Oriol Vinyals.
\newblock Machine translation decoding beyond beam search.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  8410--8434, 2021.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and
  Constant]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  3045--3059, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.243}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.243}.

\bibitem[Liang et~al.(2022)Liang, Huang, Xia, Xu, Hausman, Florence, Zeng,
  et~al.]{liang2022code}
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Pete Florence,
  Andy Zeng, et~al.
\newblock Code as policies: Language model programs for embodied control.
\newblock In \emph{Workshop on Language and Robotics at CoRL 2022}, 2022.

\bibitem[Liang et~al.(2023)Liang, Wu, Song, Wu, Xia, Liu, Ou, Lu, Ji, Mao,
  et~al.]{liang2023taskmatrix}
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu~Liu, Yang Ou, Shuai
  Lu, Lei Ji, Shaoguang Mao, et~al.
\newblock Taskmatrix. ai: Completing tasks by connecting foundation models with
  millions of apis.
\newblock \emph{arXiv preprint arXiv:2303.16434}, 2023.

\bibitem[Lin et~al.(2023)Lin, Agia, Migimatsu, Pavone, and
  Bohg]{lin2023text2motion}
Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg.
\newblock Text2motion: From natural language instructions to feasible plans.
\newblock \emph{arXiv preprint arXiv:2303.12153}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Jiang, Zhang, Liu, Zhang, Biswas,
  and Stone]{liu2023llm+}
Bo~Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas,
  and Peter Stone.
\newblock Llm+ p: Empowering large language models with optimal planning
  proficiency.
\newblock \emph{arXiv preprint arXiv:2304.11477}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (9):\penalty0 1--35,
  2023{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Zheng, Du, Ding, Qian, Yang, and
  Tang]{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
  Jie Tang.
\newblock Gpt understands, too, 2021.

\bibitem[Manceron(2022)]{manceron_pierre_2022_6551158}
Pierre Manceron.
\newblock Ikpy, May 2022.
\newblock URL \url{https://doi.org/10.5281/zenodo.6551158}.
\newblock {If you use this software, please cite it using the metadata from
  this file.}

\bibitem[Mu et~al.(2022)Mu, Zhong, Raileanu, Jiang, Goodman, Rockt{\"a}schel,
  and Grefenstette]{mu2022improving}
Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim
  Rockt{\"a}schel, and Edward Grefenstette.
\newblock Improving intrinsic exploration with language abstractions.
\newblock \emph{arXiv preprint arXiv:2202.08938}, 2022.

\bibitem[Mu et~al.(2023)Mu, Li, and Goodman]{mu2023learning}
Jesse Mu, Xiang~Lisa Li, and Noah Goodman.
\newblock Learning to compress prompts with gist tokens, 2023.

\bibitem[Nottingham et~al.(2023)Nottingham, Ammanabrolu, Suhr, Choi,
  Hajishirzi, Singh, and Fox]{nottingham2023embodied}
Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh
  Hajishirzi, Sameer Singh, and Roy Fox.
\newblock Do embodied agents dream of pixelated sheep?: Embodied decision
  making using language guided world modelling.
\newblock \emph{arXiv preprint arXiv:2301.12050}, 2023.

\bibitem[Qin \& Eisner(2021)Qin and Eisner]{qin-eisner-2021-learning}
Guanghui Qin and Jason Eisner.
\newblock Learning how to ask: Querying {LM}s with mixtures of soft prompts.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  5203--5212, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.410}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.410}.

\bibitem[Qiu et~al.(2020)Qiu, Sun, Xu, Shao, Dai, and Huang]{qiu2020pre}
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.
\newblock Pre-trained models for natural language processing: A survey.
\newblock \emph{Science China Technological Sciences}, 63\penalty0
  (10):\penalty0 1872--1897, 2020.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-maron, Gim{\'e}nez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi,
  Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and de~Freitas]{reed2022a}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~G{\'o}mez Colmenarejo,
  Alexander Novikov, Gabriel Barth-maron, Mai Gim{\'e}nez, Yury Sulsky, Jackie
  Kay, Jost~Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley
  Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar
  Bordbar, and Nando de~Freitas.
\newblock A generalist agent.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=1ikK0kHjvj}.
\newblock Featured Certification, Outstanding Certification.

\bibitem[Samvelyan et~al.(2021)Samvelyan, Kirk, Kurin, Parker-Holder, Jiang,
  Hambro, Petroni, Kuttler, Grefenstette, and
  Rockt{\"a}schel]{samvelyan1minihack}
Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang,
  Eric Hambro, Fabio Petroni, Heinrich Kuttler, Edward Grefenstette, and Tim
  Rockt{\"a}schel.
\newblock Minihack the planet: A sandbox for open-ended reinforcement learning
  research.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021.

\bibitem[Schick \& Sch{\"u}tze(2020)Schick and
  Sch{\"u}tze]{schick2020exploiting}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Exploiting cloze questions for few shot text classification and
  natural language inference.
\newblock \emph{arXiv preprint arXiv:2001.07676}, 2020.

\bibitem[Shi et~al.(2022)Shi, Han, Gonen, Holtzman, Tsvetkov, and
  Zettlemoyer]{shi2022toward}
Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke
  Zettlemoyer.
\newblock Toward human readable prompt tuning: Kubrick's the shining is a good
  movie, and a good prompt too?
\newblock \emph{arXiv preprint arXiv:2212.10539}, 2022.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  4222--4235, 2020.

\bibitem[Shridhar et~al.(2020)Shridhar, Thomason, Gordon, Bisk, Han, Mottaghi,
  Zettlemoyer, and Fox]{Shridhar_2020_CVPR}
Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han,
  Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox.
\newblock Alfred: A benchmark for interpreting grounded instructions for
  everyday tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Singh et~al.(2022)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox,
  Thomason, and Garg]{singhprogprompt}
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan
  Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg.
\newblock Progprompt: Generating situated robot task plans using large language
  models.
\newblock In \emph{Second Workshop on Language and Reinforcement Learning},
  2022.

\bibitem[Skreta et~al.(2023)Skreta, Yoshikawa, Arellano-Rubach, Ji, Kristensen,
  Darvish, Aspuru-Guzik, Shkurti, and Garg]{skreta2023errors}
Marta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji,
  Lasse~Bj{\o}rn Kristensen, Kourosh Darvish, Al{\'a}n Aspuru-Guzik, Florian
  Shkurti, and Animesh Garg.
\newblock Errors are useful prompts: Instruction guided task programming with
  verifier-assisted iterative prompting.
\newblock \emph{arXiv preprint arXiv:2303.14100}, 2023.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3008--3021, 2020.

\bibitem[Tam et~al.(2022)Tam, Rabinowitz, Lampinen, Roy, Chan, Strouse, Wang,
  Banino, and Hill]{tam2022semantic}
Allison Tam, Neil Rabinowitz, Andrew Lampinen, Nicholas~A Roy, Stephanie Chan,
  DJ~Strouse, Jane Wang, Andrea Banino, and Felix Hill.
\newblock Semantic exploration from language abstractions and pretrained
  representations.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 25377--25389, 2022.

\bibitem[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and
  Kambhampati]{valmeekam2022large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
\newblock Large language models still can't plan (a benchmark for llms on
  planning and reasoning about change).
\newblock In \emph{NeurIPS 2022 Foundation Models for Decision Making
  Workshop}, 2022.

\bibitem[Vemprala et~al.(2023)Vemprala, Bonatti, Bucker, and
  Kapoor]{vemprala2023chatgpt}
Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor.
\newblock Chatgpt for robotics: Design principles and model abilities.
\newblock \emph{Microsoft}, 2023.

\bibitem[Wake et~al.(2023)Wake, Kanehira, Sasabuchi, Takamatsu, and
  Ikeuchi]{wake2023chatgpt}
Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi
  Ikeuchi.
\newblock Chatgpt empowered long-step robot control in various environments: A
  case application.
\newblock \emph{arXiv preprint arXiv:2304.03893}, 2023.

\bibitem[Wu et~al.(2019)Wu, Kirillov, Massa, Lo, and
  Girshick]{wu2019detectron2}
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
\newblock Detectron2.
\newblock \url{https://github.com/facebookresearch/detectron2}, 2019.

\bibitem[Yao et~al.(2021)Yao, Zhang, Zhang, Liu, Chua, and Sun]{yao2021cpt}
Yuan Yao, Ao~Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong
  Sun.
\newblock Cpt: Colorful prompt tuning for pre-trained vision-language models.
\newblock \emph{arXiv preprint arXiv:2109.11797}, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Zhou, Schuurmans, and
  Gonzalez]{zhang2023tempera}
Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph~E Gonzalez.
\newblock Tempera: Test-time prompt editing via reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Li, Weber, Hafez, and Wermter]{zhao2023chat}
Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad~Burhan Hafez, and Stefan
  Wermter.
\newblock Chat with the environment: Interactive multimodal perception using
  large language models.
\newblock \emph{arXiv preprint arXiv:2303.08268}, 2023.

\bibitem[Zhong et~al.(2021)Zhong, Friedman, and Chen]{zhong-etal-2021-factual}
Zexuan Zhong, Dan Friedman, and Danqi Chen.
\newblock Factual probing is [{MASK}]: Learning vs. learning to recall.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  5017--5033, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.398}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.398}.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and
  Liu]{Zhou_2022_CVPR}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  16816--16825, June 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and
  Liu]{zhou2022learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0
  (9):\penalty0 2337--2348, 2022{\natexlab{b}}.

\end{thebibliography}
