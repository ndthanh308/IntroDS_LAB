
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


% Custom
\usepackage{booktabs} % for professional tables
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{amsfonts}
\usepackage{subcaption}
\RequirePackage{algorithm}
\RequirePackage{algorithmic}
\usepackage{wrapfig}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\usepackage{colortbl}  % table color

% color definition
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\definecolor{carnationpink}{rgb}{1.0, 0.65, 0.79}

\renewcommand{\algorithmiccomment}[1]{// #1}

\usepackage[symbol]{footmisc}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\usepackage{amsmath}
\newcommand{\Tau}{\mathrm{T}}

\newcommand{\methodshort}{BLINDER}
\newcommand{\methodlong}{Brief Language INputs for DEcision-making Responses}
\providecommand{\blue}[1]{\textcolor{blue}{#1}}
\providecommand{\red}[1]{\textcolor{purple}{#1}}

\title{Selective Perception: Optimizing State \\Descriptions with Reinforcement Learning for Language Model Actors}

\author{Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, \\ \textbf{Pierre Baldi, Roy Fox, Sameer Singh} \\
  University of California Irvine \\
  \texttt{\{knotting,yrazeghi,kyungk7,jblanier,pfbaldi,royf,sameer\}@uci.edu}}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}
\maketitle

\begin{abstract}
    Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. 
    However, previous work does little to explore what environment state information is provided to LLM actors via language.
    Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on \emph{hand-engineered}, \emph{task-specific} protocols to determine which features to communicate about a state and which to leave out.
    In this work, we propose \methodlong{} (\methodshort{}), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions.
    We evaluate \methodshort{} on the challenging video game NetHack and a robotic manipulation task.
    Our method improves task success rate, reduces input size and compute costs, and generalizes between LLM actors.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) excel at a variety of tasks and have been shown to be skilled actors for decision making in games and robotic planning \citep{ichter2022do,huanginner,liang2022code,singhprogprompt,skreta2023errors,zhao2023chat,lin2023text2motion,wake2023chatgpt}.
The input to an LLM actor typically consists of a description of the current task and state, and the actor then outputs actions to accomplish the task.
For example, consider a robot tasked with arranging items on a table according to a task specification \citep{valmeekam2022large}.
An LLM actor outputs high-level actions that the robot can take to manipulate the items such as ``move the apple to position X''.
While it would be simple to communicate the state of the items using an image, most LLM actors require exclusively language-based inputs.
Previous work has side-stepped this problem by either (1) exhaustively describing the state \citep{valmeekam2022large} or (2) engineering an ideal input format for the current task \citep{huanginner}.

The naive approach to language grounding would be to provide an exhaustive set of linguistic state features that describes everything about the state \citep{valmeekam2022large}, independent of the current task.
While an exhaustive set of state features can be readily fed to an LLM actor, independent of the current task, it tends to result in excessively long and potentially distracting state descriptions. 
Full state descriptions can impede performance and increase inference costs by including unnecessary state information.

Alternatively, manually engineered state descriptions for a LLM actor would increase performance and decrease the length and cost of inputs \citep{huanginner,liang2022code,singhprogprompt}.
However, manual descriptions are typically also task-specific.
This limits the system's ability to generalize to new tasks that may require descriptions of different portions of the state.
Additionally, manual state descriptions assume expert task knowledge, further limiting the application of this approach in the real world.
An ideal solution for describing the state to an LLM actor would learn to automatically generate task-conditioned state descriptions.

We propose \textbf{\methodlong{} (\methodshort{})}\footnote{\href{https://kolbytn.github.io/blinder}{https://kolbytn.github.io/blinder}}, for automatically selecting task-conditioned state descriptions from a set of task-independent state features. 
For example, consider a robot tasked with arranging fruit by size.
The set of state features may include items other than fruit on the table or attributes of the fruit not relevant to size.
\methodshort{} selects state features relevant to the position and the size of each fruit.
By selecting relevant state features for the LLM actor's state description, \methodshort{} improves performance and decreases input length and compute costs.

Unlike prior work, \methodshort{} automatically constructs state descriptions without requiring task-specific prompt engineering.
We frame the construction of state descriptions as a reinforcement learning problem.
Given a small set of expert task demonstrations---that include sets of state features and expert actions---and a pretrained LLM actor, we assign rewards to sampled state descriptions based on expert action likelihood from the LLM actor.
We train a value model with these rewards, and, at inference time, we use the value model to select state descriptions for new tasks, unseen during training.

We evaluate \methodshort{} on challenging NetHack \citep{kuttler2020nle} and robotic item arrangement tasks.
Notably, all of our evaluation is done on heldout test tasks that were not part of the expert demonstrations for training.
By finetuning an LLM to act as our value function, we find that \methodshort{} generalizes well to tasks that include novel entities and task compositions.
We also show that \methodshort{}'s learned state descriptions are intuitive enough to generalize between LLM actors, allowing us to train with one LLM actor and evaluate on another.
In our experiments, \methodshort{} compared to exhaustive state descriptions reduces context lengths by a factor of 6x and improves the downstream-task performance of LLM actors by up to 150\%.
We also compare to hand-engineered state descriptions and show that \methodshort{} performs comparably without the expert knowledge that manual descriptions require.
Finally, we compare to zero-shot summaries of exhaustive state descriptions from an LLM 4x larger than \methodshort{} and show that \methodshort{} outperforms this baseline as well.

% Figure environment removed

\section{Related Work}

\subsection{Language for State Descriptions}

For some time, language has been viewed as an interesting medium for communicating task requirements \citep{chevalier-boisvert2018babyai,Anderson_2018_CVPR,ku2020room,Shridhar_2020_CVPR} or state information \citep{andreas2017learning,tam2022semantic,mu2022improving} due to its compositionality and interpretability.
Since the prevalence of LLMs for planning and decision making it has become essential to describe state features in langauge to better ground LLM plans in the environment state \citep{huanginner}.

\subsection{Foundational Models for Planning}

LLMs have recently become popular for planning and high-level decision making in robotic tasks \citep{ichter2022do,huang2022language,huang2023grounded,vemprala2023chatgpt} and other sequential decision making benchmarks \citep{nottingham2023embodied,kim2023language,liu2023llm+,liang2023taskmatrix}.
However, LLMs are not natively grounded in the state of the environment when prompted for planning, resulting in sometimes nonsensical actions.
While some learned grounding techniques exist \citep{ichter2022do}, the most straightforward way to ground LLMs in state is to include state descriptions in the LLM context \citep{huanginner}.
Many LLM applications for robotics achieve this by defining a formal language for planning that includes definitions for state features \citep{liang2022code,singhprogprompt,skreta2023errors,zhao2023chat,lin2023text2motion,wake2023chatgpt}.
Utilizing multimodal models—which directly accept image inputs and output subsequent moves—may be posited as an alternative approach for planning within foundational models \cite{driess2023palm, guhur2023instruction}. 
Although this could potentially offer promising solutions to the issues discussed in this paper, current multimodal models tend to be domain-specific \cite{guhur2023instruction, jiang2022vima} and lack robustness across diverse domains.
Additionally, these models are not as widely available for public use \cite{reed2022a, driess2023palm}. Consequently, the use of text-only models, which are easily accessible, continues to be a crucial approach for addressing planning tasks.

\subsection{Learning Inputs for Pretrained Models}

An extensive range of studies investigate efficient techniques for learning effective input for a variety of pretrained language models \citep{liu2023pre, qiu2020pre} in discrete \citep{shin2020autoprompt, liu2023pre,  schick2020exploiting, gao-etal-2021-making, shi2022toward} or continuous \citep{qin-eisner-2021-learning, liu2021gpt, lester-etal-2021-power, zhong-etal-2021-factual} input space.
This line of work is also studied in pretrained vision models \citep{gao2021clip, Zhou_2022_CVPR, ju2022prompting, zhou2022learning, yao2021cpt}.

Among these prompt tuning methods, \citet{deng-etal-2022-rlprompt} use reinforcement learning to optimize the prompt for a variety of NLP tasks. 
Perhaps the closest work to our own is that of \citet{zhang2023tempera}, which introduces an RL agent trained to edit language models input at test time for solving NLP benchmarks in a more sample-efficient way.
Their approach differs significantly from our own and focuses on static NLP tasks rather than sequential tasks.
However, the prospect of applying similar techniques to modify the state description in sequential tasks, could serve as a potential avenue for our future work. 

Recent methods have explored the idea of compressing input tokens for shorter context length \citep{mu2023learning, chevalier2023adapting}, which is related to our effort to learn pruned state descriptions but does not meet the needs of sequential decision making nor maintain intuitive inputs that generalize between models.

\section{Background}
\label{sec:background}

It is important to distinguish the two levels of decision-making present when selecting state descriptions for an LLM actor.
At one level of decision-making, the LLM actor $LLM: \mathcal{X} \times \Tau \to A$ receives a task $\tau \in \Tau$ and state description $x \in \mathcal{X}$ as input and then outputs an environment action $a \in A$ to execute. The set of possible state descriptions $\mathcal{X}$ is the power set of all possible state feature strings $\omega \in \Omega$, $\mathcal{X} = \{x | x \subseteq \Omega\}$. Each linguistic feature $\omega \in \Omega$ is a minimal atomic statement that describes some aspect of the environment's state.
For example, a given $\omega$ may indicate the presence of an object in a scene \citep{huanginner}, the completion of subgoals \citep{nottingham2023embodied}, or an attribute of or relationship between environment entities.

Independent from the LLM actor's decision making, \methodshort{} selects which subset of possible features in $\Omega$ to include as a state description $x$.
We frame the problem of selecting state descriptions as a reinforcement learning task with a markov decision process (MDP) in which state descriptions are assembled one feature at a time. In this MDP, states consist of the current description $x_t \in \mathcal{X}$, and actions are a choice over individual features $\omega_t \in \tilde{\Omega}$ to add to $x_t$ and form $x_{t+1}$.
Here $\hat{\Omega} \subset \Omega$ is the subset of all possible state features that are factually true in the current LLM actor's environment.
This description-building MDP has deterministic transitions where each step adds a new state feature to the state description. We define our transition function $P: \mathcal{X} \times \Omega \to \mathcal{X}$ as $P(x_t, \omega_t) = x_t \cup \{\omega_t\}$.
Our reward function $R: \mathcal{X} \times \Tau \to \mathbb{R}$ represents the likelihood that a state description yields an optimal target action from the LLM actor (See Section \ref{sec:value}).
Finally, we learn a policy $\pi: \mathcal{X} \times \Tau \to \tilde{\Omega}$ that iteratively constructs task-conditioned state descriptions.

\section{\methodshort{}}
\label{sec:method}

We propose learning to automatically select a state description $x$ from an exhaustive set of factual state features $\hat{\Omega}$ given a task $\tau$.
Our goal is to automatically learn optimal state descriptions without requiring hand-engineered descriptions for each task or datasets of annotated examples of state descriptions.
Instead, we suggest a method that learns to automatically produce state descriptions for novel tasks that remove distracting information and reduce the required context-length of LLM actors.

\subsection{Selecting State Descriptions}

Our method, \methodshort{}, defines a value-based policy 

\begin{equation}
    \pi(x_t, \tau, \hat{\Omega}) = \argmax_{\omega'_t\in (\hat{\Omega}-x_t)} V_\theta(x_t\cup \{\omega'_t\}, \tau)
    \label{eq:pi}
\end{equation}

that selects unused state features $\omega'_t \in \hat{\Omega}-x_t$ to add to $x_t$. $V_\theta: \mathcal{X} \times \Tau \to \mathbb{R}$ is a value function trained to approximate the return of a state description (see Section \ref{sec:value}).
Since the transition model $P(x_t, \omega_t) = x_t \cup \{\omega_t\}$ is known and deterministic, we can use a state value function directly to define our policy.

\begin{wrapfigure}{R}{0.42\textwidth}
\vspace{-.5cm}
  \begin{minipage}{0.42\textwidth}
    \begin{algorithm}[H]
    \footnotesize
      \caption{\footnotesize State Description Selection}\label{alg:select}
      \begin{algorithmic}
        \REQUIRE $V_\theta,\;\tau,\;\hat{\Omega}$
        \STATE $x_0 \gets \emptyset$
        \STATE $t \gets 0$
        \WHILE{$|\hat{\Omega}-x_t|>0$}
        \STATE $\omega'_t \gets \pi(x_t, \tau, \hat{\Omega})$
        \IF{$V_\theta(x_t \cup \{\omega'_t\}) > V_\theta(x_t)$}
            \STATE $x_{t+1} \gets x_t \cup \{\omega_t\}$
            \STATE $t \gets t + 1$

        \ELSE
            \STATE break
        \ENDIF
        \ENDWHILE
        \STATE $x_f \gets x_t$
        \RETURN $x_f$
      \end{algorithmic}
    \end{algorithm}
  \end{minipage}
\vspace{-0.2cm}
\end{wrapfigure}

We use $\pi$ to construct a state description $x$ out of state features $\omega$.
Starting with $x_0=\emptyset$, $\pi$ iteritively adds $\omega'_t$ to $x_t$ as long as it increases in value and doesn't run out of state features (see Algorithm \ref{alg:select}).
Once no state features remain, $\hat{\Omega}-x_t = \emptyset$, or no state feature increases the value of the state description, we terminate and use the last $x_t$ as the final state description, $x_f$.
This state description is then used to prompt the LLM actor for the downstream sequential decision making task.

\begin{equation}
    LLM: x_f \times \tau \to a
\end{equation}

\subsection{State Description Value}
\label{sec:value}

The key to \methodshort{} is learning a useful and general value function $V_\theta$.
We first define the reward function $R$ used to train $V_\theta$.

We propose using action likelihood from an LLM actor to define rewards for \methodshort{}.
We collect a set of expert trajectories composed of state features, task descriptions, and target actions $D_e=\{(\hat{\Omega}_0,\tau_0,a^*_0), (\hat{\Omega}_1,\tau_1,a^*_1), ...\}$, we define the following sparse reward function for terminal $x_f$:

\begin{equation}
R(x_t, \tau) =
\begin{cases}
     \mathbb{E}_{a^*\sim (D_e | x_t \subset \hat{\Omega}, \tau)} \bigg[LLM(a^*|x_t, \tau)\bigg] & \text{if } x_t = x_f \\
     0 & \text{otherwise}
\end{cases}
\label{eq:reward}
\end{equation}

where $a^*$ is sampled from tuples in $D_e$ with matching $\tau$ and where $x_f$ is a valid subset of $\hat{\Omega}$.
Intuitively, we reward \methodshort{} to maximize the likelihood that $x_f$ elicits the same action from the LLM actor as the target action $a^*$ from the expert trajectory.

Although $R$ depends on a specific LLM actor, we find in our experiments that \methodshort{} selects intuitive state features, consistently removing the most distracting information and including the most relevant information as shown in Table \ref{tab:examples}.
However, previous work that optimizes inputs for a frozen LLM often learns nonsensical language inputs \citep{shin2020autoprompt}.
\methodshort{} generates intuitive state descriptions due, in part, to being constrained to the set of defined state features.
Because of this, we find that \methodshort{} generalizes well to other LLM actors.
In Section \ref{sec:generalization}, we demonstrate that we can train \methodshort{} with a smaller LLM actor and then use it to improve the performance of a larger black-box LLM actor.

\subsection{Learning the Value Function}

To learn $V_\theta$, we choose to finetune a pretrained LLM with a value head to maintain generalizability to new tasks and state features. While we refer to $x$ as a set, the text input to $V_\theta$ is ordered using $x$'s insertion order. Given a dataset of precollected experience $D_V=\{(\tau^0, x_{t}^0, r_f^0), (\tau^1, x_{t}^1, r_f^1),...\}$ where $r^i_f = R(x^i_f, \tau^i)$ \methodshort{} optimizes $V_\theta$ with the loss:

\begin{equation}
\begin{split}
    J_V(\theta) = 
    \mathbb{E}_{\tau, x_t, r_f \sim D_V} 
    \bigg[ 
        \mathcal{L}\bigg(
            V_\theta(x_t, \tau),\; 
            r_f
        \bigg)
        + \phi 
     \bigg]
\end{split}
\end{equation}

$\mathcal{L}$ is a loss metric for the value (mean squared error, quantile regression, etc.), and $\phi$ is a $KL$ penalty for normalizing $V_\theta$, commonly used when finetuning LLMs with RL \citep{stiennon2020learning,leblond2021machine}. 
In practice, evaluating the reward function $R(x_t, \tau)$ requires running inference on an expensive LLM actor. To lower training costs, we collect $D_V$ only once prior to training with a random policy $\pi_r$. Trading-off accuracy for computational efficiency, we train $V_\theta$ to model $V_{\pi_r}$ as a cost-effective approximation of the greedy optimal value function $V_{\pi^*}$. Despite this, \methodshort{} still learns to produce highly effective and concise state descriptions that improve LLM actor performance and are competitive with manually engineered state descriptions.

\section{Experiment Setup}

\begin{table}
    \small
    \center
    \begin{tabular}{p{2.5cm}p{3cm}p{3cm}p{3.5cm}}
        Task Description & Manual & Zeroshot & \methodshort{} \\
        \toprule
        Drink the potion and navigate to the stairs down. & 
        \blue{You see a stairs down very near east.}{} &
        \blue{You see a stairs down very near east.}{} \red{You see a lava very near north northeast.}{} &
        \blue{You see a stairs down very near east.}{} You have 20 uncursed flint stones (in quiver pouch). Time: 5. Constitution: 13. \\
        \midrule
        Pick up and eat the apple. &
        \blue{You see a apple very near east southeast.}{} & 
        \red{You have an apple.}{} & 
        \blue{You see a apple very near east southeast.}{} You have a blessed +2 sling (alternate weapon; not wielded). Time: 1. Condition: None. \\
    \end{tabular}
    \caption{
    Qualitative example output from \methodshort{} and baselines.
    \blue{Blue}{} text indicates the most relevant information for a task, while \red{red}{} text indicates distracting or inaccurate information for the current task.
    Note that Zeroshot descriptions often contain information that hinders performance.
    Although \methodshort{} sometimes includes unintuitive information, it does not distract from the current task.
    }
    \label{tab:examples}
\end{table}

In each of our experiments, \methodshort{} is trained on a set of training tasks and is provided five expert trajectories for each task.
The trajectories are then used to generate rewards and train the value function for \methodshort{}.

We use two different LLM actors in our experiments.
(1) Our \textbf{zeroshot flan-T5 actor} is a three billion parameter \verb_flan-t5-xl_ model \citep{flan-t5}.
This is the model \methodshort{} uses for training.
For each action, $a$, at each environment step, we calculate the geometric mean of the logits of the tokens in $a$ and use this to sample an action from the LLM actor.
(2) our \textbf{Fewshot GPT3 actor} uses \verb_gpt3.5-turbo-0301_ 
to select actions from a list of admissible actions given a task and state desciption.
We use this model to evaluate \methodshort{}'s ability to generalize between LLM actors.
\methodshort{} itself finetunes a 780 million parameter \verb_flan-t5-large_ model for its value function.
See Appendix \ref{app:actors} for model prompting details.

\subsection{Baselines}

As an initial baseline, we compare state descriptions from \methodshort{} to exhaustive state descriptions.
We refer to the exhaustive state description that includes all feaures of $\hat{\Omega}$ as the ``full'' state description in our experiments.
We obtain ``full'' state descriptions $\hat{\Omega}$ for NetHack from the natural language wrapper recommended by \citet{kuttler2020nle}\footnote{\href{https://github.com/ngoodger/nle-language-wrapper}{https://github.com/ngoodger/nle-language-wrapper}}.
The resulting sets of state features range in size from $30 \le |\hat{\Omega}| \le 40$.
For our robotic task, we use a list of the pairwise spatial relationship of each object to every other object, totalling 90 state features for our task setup.
These ``full'' states contain mostly irrelevent or distracting information (see Tables \ref{tab:full_state_nethack} and \ref{tab:full_state_robot} in the appendix).
On average, \methodshort{} removes 83\% of the original state features.

We also compare state descriptions from \methodshort{} to those of a zeroshot \verb_flan-t5-xl_ model \citep{flan-t5} prompted to summarize relevant state features from $\hat{\Omega}$.
We refer to these as ``zeroshot'' state descriptions in our experiments. 
Note that this model has 4x the the number of parameters as the \verb_flan-t5-large_ model we use to train \methodshort{}.
Using a \verb_flan-t5-large_ model for the zeroshot baseline failed to produce useful descriptions.
Despite the zeroshot descriptions being from a larger model, Table \ref{tab:examples} shows how this method still regularly produced distracting or incorrect descriptions.

Additionally, we compare to hand-engineered state descriptions.
To create these, we specify a set of relevant keywords for each task.
Then, we include all state features that contain those keywords.
We refer to this baseline as ``manual'' state descriptions in our experiments.
Because such summaries have to be manually designed for each task, this baseline is not usually available for test tasks and would not generalize across tasks like \methodshort{} does.

% Figure environment removed

\section{NetHack Experiments}
\label{sec:nethack}

We evaluate \methodshort{} on NetHack tasks using the Minihack library \citep{samvelyan1minihack}. 
NetHack, recently proposed as a testbed for AI research \citep{kuttler2020nle}, is a grid-based dungeon crawler with complex dynamics and large observation and action spaces.
We select a set of five training tasks from the Minihack environment zoo: \verb_Room-Monster-5x5_, \verb_Eat_, \verb_Wear_, \verb_LavaCross-Levitate-Potion-Inv_, and \verb_LavaCross-Levitate-Ring-Inv_. 
We use just 25 expert trajectories for training with a total of 148 pairs of states and expert actions that required under an hour for a human annotator to collect.

We design five difficult custom test tasks to evaluate the performance of \methodshort{}.
Two tasks, \verb_Eat pear_ and \verb_Wear shoes_, are variants of the \verb_Eat_ and \verb_Wear_ training tasks but with different target items, larger rooms, monsters, and distractor items.
We also define three \verb_Lava cross_ test tasks.
Unlike the training variants of this task, the item needed to cross the lava does not start in the player inventory, necessitating improved multi-step planning from the LLM actor.
Also, the boots variant of the lava cross task was not seen during training. 
See Appendix \ref{app:trajectory} for an example trajectory.

\subsection{\methodshort{} Generalization}
\label{sec:generalization}

Figure \ref{fig:success} shows success rate on test tasks in the NetHack domain, comparing \methodshort{} to other baselines that can generalize to unseen tasks.
\methodshort{} consistently outperforms or matches these baselines when providing descriptions for the flan-T5 actor that it was trained with.
It also generalizes well to the much larger GPT3 actor, outperforming both baselines on three of five tasks.

% Figure environment removed

\methodshort{} also performs competitively against our manual baseline that is engineered to only include task relevant information in the state description.
Figure \ref{fig:manual} shows \methodshort{} outperforming the manual baseline in three of five tasks.
Notably these are the tasks that require multi-step planning by first picking up the target item and second using it to cross the lava.
We hypothesize that \methodshort{} does better on these tasks by learning that different state features are relevant at different points in a trajectory.

Overall, \methodshort{} demonstrates that it can learn to generalize state descriptions for new tasks of increasing complexity that involve entities not seen during training.
It is also able to generate state descriptions that are intuitive enough to generalize to new LLM actors.
This allows our method to be trained using smaller/local models and then deployed with large/black-box models.

\subsection{Effective Context Use}

Most LLM actors have a limited input size, or context length, with which they are effective.
Also, longer contexts result directly in more expensive compute.
Success often requires effective use of context length, and with shorter state descriptions we can apply the freed context to improving the quality of our LLM actor.

One way to utilize the additional context length we have is with additional fewshot examples.
Figure \ref{fig:fewshot} shows how \methodshort{} drastically reduces context length by a factor of 6x and allows more effective use of our fewshot LLM actor's context.
With shorter contexts we can include more fewshot examples and improve LLM actor performance.

% Figure environment removed

\section{Robotic Experiments}

We also evaluate \methodshort{} on a robotic object arrangement task.
In this task, several objects must be rearranged on a table in order from left to right.
The table is divided into a 2x5 grid of 10 possible positions that a single object may occupy. Various distractor objects are also placed in this grid to complicate the state space.
\methodshort{} is trained using 25 expert demonstrations arranging two or three items.
We evaluate on arranging two, three, or four items from a set of items not seen during training.
See Appendix \ref{app:robot} for more details.

The robot environment uses an object segmentation model to map visual observations to natural language labels as shown in Figure \ref{fig:segmentation}.
Detected object labels and coordinates are used to generate a set of text features $\hat{\Omega}$ describing spatial relationships between each object in the scene. For example, a table with an apple, banana, and orange arranged from left to right would result in the features \emph{the orange is to the right of the apple}, \emph{the banana is to the left of the orange}, etc. The locations of any empty grid positions are also included in $\hat{\Omega}$.

In this environment, the actions available to the LLM actor consist of specifying both which object to move and which unoccupied grid position to move it to. 
Our setup has ten possible grid positions and seven to nine objects, creating 90 state features in $\hat{\Omega}$, and 9-21 admissible actions at each step.
After the LLM actor selects an action, the robot then executes the low-level control to move the specified object to its new grid position.

Figure \ref{fig:arrange} shows the benefit of using \methodshort{} to select state descriptions for this task.
Like our NetHack task, we compare to ``full'' and ``zeroshot'' baselines with zeroshot T5 and fewshot GPT3 actors.
Consistent with the results from our NetHack experiments, \methodshort{} successfully selects state descriptions that improve actor performance further supporting the generalizability of our method.

\section{Discussion \& Conclusion}

Most prior work that evaluates the potential for using LLMs as actors in sequential decision making tasks ignore the issue of defining state descriptions.
Instead they use exhaustive non-optimal state descriptions \citep{valmeekam2022large} or hand-engineered task-specific state descriptions \citep{huanginner,liang2022code,singhprogprompt}.
However, for LLM actors to be deployed in real world settings as general problem solvers, they will need to the ability to filter state features for novel tasks automatically.
To this end, we introduce \methodlong{} (\methodshort{}).

We show that selecting learned state descriptions not only improves actor performance by reducing distracting information, but we also demonstrate how the decreased context length can provide additional benefits.
In our experiments, our methods reduce the needed context length for decision making by a factor of 6x, which can translate directly do decreased inference costs.
Alternatively, the additional context space can be applied to including more in-context examples and further improving performance of fewshot actors.

We believe that using LLM output to learn intuitive, linguistic inputs can be a powerful method for improving prompting and context efficiency.
While our method focuses on selecting a prompt from an existing set of linguistic features, we encourage additional research that extends this idea to free-form summaries or feature abstractions.
These ideas are also not limited to sequential decision making and should be researched in the context of other problem domains as well.

LLMs are being applied to a greater number of problems that require increasingly large context lengths.
We believe that learning task-conditioned LLM input is a powerful method for improving performance and decreasing inference costs.
We hope to encourage continued research into using LLM contexts more efficiently through methods such as pruning, summarization, and retrieval.

\bibliography{custom}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix

\section*{Appendix}
\label{sec:appendix}

\section{\methodshort{} Details}
\label{app:blinder}

% Figure environment removed

\begin{table}[h]
    \small
    \center
    \begin{tabular}{ll}
        Hyperparameter & Value \\
        \midrule
        $\gamma$ & 1 \\
        lr & 1e-6 \\
        batch size & 4 \\
        KL regularization coefficient & 1 \\
        max state description length (nethack) & 10 \\
        max state description length (robot) & 5 \\
    \end{tabular}
    \caption{\methodshort{} training hyperparameters}
    \label{tab:hyper}
\end{table}

Table \ref{tab:hyper} shows hyperparameters used during \methodshort{} training. We fintune a \verb_flan-t5-large_ model with a value head after the final hidden state in the decoder.
We prompt the model with the following instructions to take advantage of flan-T5's instruction finetuning:

\begin{verbatim}
Describe the relevant information from the game state for 
the current task. Your current task is to [task description].
\end{verbatim}

We finetune on monte carlo value estimates from the datasets described in Section \ref{sec:value}.
We also upsample instances from environment steps with less common actions.
Figure \ref{fig:train} shows LLM actor success rate on NetHack test tasks throughout training.

\iffalse
\begin{algorithm}
\caption{State Description Selection}\label{alg:select}
\begin{algorithmic}
    \REQUIRE $V,\;\tau,\;\hat{\Omega}$
    \STATE $x \gets \emptyset$
    \WHILE{$|\hat{\Omega}-x|>0$}
        \STATE $\omega \gets \pi(x, \tau, \hat{\Omega})$
        \IF{$V(x \cup \{\omega\}) > V(x)$}
            \STATE $x \gets x \cup \{\omega\}$
        \ELSE
            \STATE break
        \ENDIF
    \ENDWHILE
    \STATE $x_f \gets x$
    \RETURN $x_f$
\end{algorithmic}
\end{algorithm}
\fi

\section{Actor Details}
\label{app:actors}

\subsection{T5 Actor}

We prompt a pretrined \verb_flan-t5-xl_ model with the below prompts for use as an LLM actor:

NetHack Domain:
\begin{verbatim}
You are playing the rogue-like game NetHack. Your task is to 
[task description]. You can move north, south, east, west, 
northeast, southeast, southwest, or northwest. You can attack 
monsters adjacent to you, pick up items under you, zap wands, 
eat food, wear armor, use keys, drink potions, and put on 
rings. [state description]
You choose to:
\end{verbatim}

Robot Domain:
\begin{verbatim}
You are controlling a helpful household robot. Your task is to 
[task description]. You can move items from their current 
positions to empty positions indicated by their cooresponding 
letter. [state description]
You choose to:
\end{verbatim}

At each task step, we compute the likelihood of the admissible actions, normalize the probabilities using the softmax function, and then sample an action to execute in the environment.

\subsection{GPT Actor}

Our fewshot actor is a \verb_gpt3.5-turbo-0301_ model prompted with the below system messages:

\textbf{NetHack Domain:}
\begin{verbatim}
You are playing the rogue-like game NetHack. You can move north, 
south, east, west, northeast, southeast, southwest, or 
northwest. You can attack monsters adjacent to you, pick up 
items under you, zap wands, eat food, wear armor, use keys, 
drink potions, and put on rings.
\end{verbatim}

\textbf{Robot Domain:}
\begin{verbatim}
You are controlling a helpful household robot. You can move 
items from their current positions to empty positions indicated 
by their cooresponding letter.
\end{verbatim}

We provide fewshot examples and prompt the actor with the below prompt:

\begin{verbatim}
Your task is to [task description].

Game Description:
[state description]

Choose the best action:
[list of admissible actions]
\end{verbatim}

The model generates an admissible action that is executed in the environment.

\section{Example Trajectories}
\label{app:trajectory}

Table \ref{tab:nethack_success} shows a successful NetHack trajectory for the ``Lava Crossing: Potion'' task, and tables \ref{tab:robot_success1} and \ref{tab:robot_success2} show successful trajectories for the robot arrangement task with three and four target items. All examples were collected with the fewshot GPT3 actor.

\def\arraystretch{0.5}  % vertical padding
\begin{table}
    \small
    \center
    \begin{tabular}{ m{0.5cm} m{2.9cm} m{5cm} m{2cm} }
    Time & Image & Selected State Description & Action \\
        \toprule
        t=0 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize You see a effervescent potion very near south southwest. You see a stairs down near east southeast. You see a lava near south southeast. You have a +2 sling (alternate weapon; not wielded).
        & 
        \scriptsize move southwest
        \\
        \midrule
        t=1 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize You see a effervescent potion very near south southwest. You see a stairs down near east southeast. You see a lava near south southeast. You have a +2 sling (You see a effervescent potion adjacent south. You have a +2 sling (alternate weapon; not wielded). Score: 0. You have an uncursed +0 leather armor (being worn). You have 16 uncursed flint stones (in quiver pouch).
        & 
        \scriptsize move south
        \\
        \midrule
        t=2 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize You see here an effervescent potion. You have a +2 sling (alternate weapon; not wielded). Score: 0. Depth: 1.
        & 
        \scriptsize pick up an effervescent potion
        \\
        \midrule
        t=3 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize f - an effervescent potion. You have a +2 sling (alternate weapon; not wielded). Score: 0. Encumbrance: Unencumbered.
        & 
        \scriptsize drink an effervescent potion
        \\
        \midrule
        t=4 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize Dexterity: 13. You have 16 uncursed flint stones (in quiver pouch). You have 25 uncursed rocks.
        & 
        \scriptsize move east
        \\
        \midrule
        t=5 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize Dexterity: 13. You have 16 uncursed flint stones (in quiver pouch). You have 25 uncursed rocks.
        & 
        \scriptsize move east
        \\
        \midrule
        t=6 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize Dexterity: 13. You have 16 uncursed flint stones (in quiver pouch). You have 25 uncursed rocks.
        & 
        \scriptsize move east
        \\
        \midrule
        t=7 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize Dexterity: 13. You have 16 uncursed flint stones (in quiver pouch). You have 25 uncursed rocks.
        & 
        \scriptsize move east
        \\
        \midrule
        t=8 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize You see a stairs down adjacent northeast. You have 16 uncursed flint stones (in quiver pouch). You have 25 uncursed rocks.
        & 
        \scriptsize move northeast
        \\
        \midrule
        t=9 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        -
        & 
        \scriptsize -
        \\
        \bottomrule
    \end{tabular}
    \caption{
    Here, our fewshot actor is prompted with ``Your task is to pick up and drink the potion and navigate to the stairs down.''. The table shows a NetHack visualization alongside selected state descriptions and the action selected by the LLM actor. 
    }
    \label{tab:nethack_success}
\end{table}

\def\arraystretch{0.5}
\begin{table}
    \small
    \center
    \begin{tabular}{ m{0.5cm} m{2.9cm} m{3.9cm} m{2.2cm} m{2.2cm} }
    Time & Image & Grid & Selected \newline State Description & Action \\
        \toprule
        t=0 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize position A is to the left of the doughnut. position A is to the left of and behind the ball.
        & 
        \scriptsize move the ball to position A.
        \\
        \midrule
        t=1 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        &
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize position I is behind position J. the doughnut is to the left of position I.
        & 
        \scriptsize move the doughnut to position I.
        \\
        \midrule
        t=2 &  
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        &
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize position G is to the left of the doughnut. position G is to the right of the ball.
        & 
        \scriptsize move the soda to position G.
        \\
        \midrule
        t=3 &  
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        &
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize -
        & 
        \scriptsize -
        \\
        \bottomrule
    \end{tabular}
    \caption{
    Successful trajectory for the task ``Arrange the objects in the order: \textbf{ball, soda, doughnut.}" The items relevant to the task are highlighted -- \textcolor{celadon}{green color} denotes correct alignment and \textcolor{carnationpink}{red} means incorrect alignment.
    }
    \label{tab:robot_success1}
\end{table}

\def\arraystretch{0.5}
\begin{table}
    \small
    \center
    \begin{tabular}{ m{0.5cm} m{2.9cm} m{3.9cm} m{2.2cm} m{2.2cm} }
    Time & Image & Grid & Selected \newline State Description & Action \\
        \toprule
        t=0 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        & 
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize the bottle is to the right of position C. position C is behind the toothpaste. position C is to the right of and behind the ball.
        & 
        \scriptsize move the ball to position C.
        \\
        \midrule
        t=1 & 
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        &
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize position B is to the left of the toothpaste. the ball is behind the toothpaste. position F is to the right of position B. the orange is to the right of position B. the soda is to the right of the ball.
        & 
        \scriptsize move the toothpaste to position B.
        \\
        \midrule
        t=2 &  
        \begin{minipage}[t]{3cm}
        % Figure removed
        \end{minipage}
        &
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize -
        & 
        \scriptsize -
        \\
        \bottomrule
    \end{tabular}
    \caption{
    Successful trajectory for the task ``Arrange the objects in the order: \textbf{toothpaste, ball, bottle, soda.}" Notably, \methodshort{} can be generalized to arranging an unseen number of items.
    }
    \label{tab:robot_success2}
\end{table}

\newpage

% Figure environment removed
\section{Robot Task Details}
\label{app:robot}

\subsection{Robot Details}

For the robotics experiment, we use the Stretch RE2~\citep{kemp2022design} from Hello Robot Inc. Stretch is a lightweight, low-cost mobile manipulator equipped with a variety of sensors, including an RGB-D camera and a 2D LiDAR (see Figure~\ref{fig:appendix_stretch}). The Stretch uses an Intel RealSense D435i camera to collect RGB and depth images of the table and we use both the onboard RP-LiDAR A1 and an added HTC Vive motion tracker for position estimates.

\begin{wrapfigure}{r}{0.35\textwidth}
  \vspace{-10mm}
  \begin{center}
    % Figure removed
  \end{center}
  \caption{Stretch RE2.}
  \label{fig:appendix_stretch}
  \vspace{-10mm}
\end{wrapfigure}

% \textbf{RGB.}
% We obtain an RGB image of $1280\times720$. Images in this paper are cropped and resized to guide readers toward the most important details in the scene.
% We process an RGB image and convert it to a natural language state description such that LLM planners can take as an input. Details can be found in Appendix~\ref{sec:appendix_mapping}.

% \textbf{Depth.}
% We use depth information to estimate 3D location of detected objects in the RGB image. Specifically, we use \texttt{aligned\_depth\_to\_color} ROS topic to obtain a depth image aligned to RGB image to which we query depth information with 2D pixel point. Finally, we call \texttt{rs2\_deproject\_pixel\_to\_point} from the pyrealsense2 library to compute the point in 3D space relative to the camera given pixel coordinates and depth.

% \textbf{2D LiDAR.}
% LiDAR is installed in the base and provides a 340 degree field-of-view. LiDAR is used for 2D navigation, specifically for localization and collision avoidance.

% \textbf{Motion Tracker.}
% We additionally mount an HTC Vive motion-tracking device on top of the robot. The robot's location is calibrated with the motion tracker as it gives more precise position estimates than the onboard LiDAR.

\subsection{Environment Details}
The goal of the robot manipulation task is to rearrange several objects on a table in a target order from left to right. Objects may each occupy and be placed in predefined locations in a 2-row by 5-column grid on the table. Target object arrangements are defined by the horizontal order of each item, and an object's final row location does not affect task success. A trial is successful if the robot arranges the objects in a specified order in 10 actions or less.

To investigate the generalization ability of \methodshort{}, we test on held-out items not seen during training.
Additionally, while agents are tasked with arranging only two or three items during training, they are evaluated on arranging two, three, and four items at test time.

\subsubsection{Observation space}
\label{sec:appendix_mapping}
Using LLMs as high-level planners requires natural language input, however object locations are perceived by the Stretch robot with camera inputs. We apply a pipeline to parse image observations to a canonical state representation and then to natural language state descriptions.

Given an RGB image of the table, we detect table objects with an off-the-shelf semantic segmentation mask and determine their 3D location using a corresponding depth image. The locations of grid cells are implicitly defined relative to the table. The canonical state we extract describes which objects are in which grid cells.
For segmentation, we use a \texttt{Mask R-CNN}~\citep{he2017mask} model with a ResNet-101-FPN backbone pre-trained on LVIS~\citep{gupta2019lvis} using the Detectron2 library~\citep{wu2019detectron2}.
Figure~\ref{fig:appendix_rgbseg} shows an example of an RGB image and its corresponding segmentation result. 
Figure~\ref{fig:appendix_grid} shows the parsed grid state corresponding to the image in Figure~\ref{fig:appendix_rgbseg}.

Next, we construct a natural language state description from the current grid layout by listing spatial relationships between each grid cell and every other grid cell.
Spatial relationships include left, right, behind, and beyond.
For grid cells that are populated, the residing object labels are used as identifiers for the grid cells.
Table~\ref{tab:full_state_robot} shows an example of a grid state being converted to a full natural language state description.

\subsubsection{Action space} 

The high-level action defined for object arrangement follows the format ``\textit{move object\_name to empty\_position\_name}''.
In our experiments, the Stretch robot achieves this by using 3D object coordinates obtained from segmented RGB-D camera images in combination with IKPy~\citep{manceron_pierre_2022_6551158}, a python library for inverse kinematics.

\begin{table}[p]
    \small
    \center
    \begin{tabular}{ m{3.8cm} m{9.2cm}}
    \toprule
    NetHack & Full State Description \\
        \midrule
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize ``You have a +1 club (weapon in hand). You have a +2 sling (alternate weapon; not wielded). You have 16 uncursed flint stones (in quiver pouch). You have 25 uncursed rocks. You have an uncursed +0 leather armor (being worn). Strength: 22/19. Dexterity: 13. Constitution: 15. Intelligence: 9. Wisdom: 10. Charisma: 6. Depth: 1. Gold: 0. HP: 16/16. Energy: 2/2. AC: 8. XP: 1/0. Time: 1. Position: 36|9. Hunger: Not Hungry. Monster Level: 0. Encumbrance: Unencumbered. Dungeon Number: 0. Level Number: 1. Score: 0. Alignment: Neutral. Condition: None. You see a vertical wall far east. You see a stairs down near east southeast. You see a horizontal wall near southeast and south. You see a lava near south southeast. You see a southwest corner near southwest. You see a vertical wall near west. You see a horizontal wall very near north, northeast, and northwest. You see a lava very near east northeast, east, east southeast, and southeast. You see a effervescent potion very near south southwest. Hello Agent, welcome to NetHack!  You are a neutral human Caveman" \\
        \bottomrule
    \end{tabular}
    \caption{The ``Lava Cross: Potion'' NetHack task observation and full state description.}
    \label{tab:full_state_nethack}
\end{table}

\begin{table}[p]
    \small
    \center
    \begin{tabular}{ m{3.8cm} m{9.2cm}}
    \toprule
    Grid & Full State Description \\
        \midrule
        \begin{minipage}[t]{4cm}
        % Figure removed
        \end{minipage}
        & 
        \scriptsize ``position E is behind the water bottle. position E is to the left of and behind the apple. position E is to the left of and behind the soda. position E is to the left of position G. position E is to the left of the doughnut. position E is to the right of and behind the bottle. position E is to the right of and behind the orange. position E is to the right of the ball. position E is to the right of the toothpaste. position G is behind the soda. position G is to the left of and behind the apple. position G is to the left of the doughnut. position G is to the right of and behind the bottle. position G is to the right of and behind the orange. position G is to the right of and behind the water bottle. position G is to the right of position E. position G is to the right of the ball. position G is to the right of the toothpaste. the apple is beyond the doughnut. the apple is to the right of and beyond position E. the apple is to the right of and beyond position G. the apple is to the right of and beyond the ball. the apple is to the right of and beyond the toothpaste. the apple is to the right of the bottle. the apple is to the right of the orange. the apple is to the right of the soda. the apple is to the right of the water bottle. the ball is behind the bottle. the ball is to the left of and behind the apple. the ball is to the left of and behind the orange. the ball is to the left of and behind the soda. the ball is to the left of and behind the water bottle. the ball is to the left of position E. the ball is to the left of position G. the ball is to the left of the doughnut. the ball is to the left of the toothpaste. the bottle is beyond the ball. the bottle is to the left of and beyond position E. the bottle is to the left of and beyond position G. the bottle is to the left of and beyond the doughnut. the bottle is to the left of and beyond the toothpaste. the bottle is to the left of the apple. the bottle is to the left of the orange. the bottle is to the left of the soda. the bottle is to the left of the water bottle. the doughnut is behind the apple. the doughnut is to the right of and behind the bottle. the doughnut is to the right of and behind the orange. the doughnut is to the right of and behind the soda. the doughnut is to the right of and behind the water bottle. the doughnut is to the right of position E. the doughnut is to the right of position G. the doughnut is to the right of the ball. the doughnut is to the right of the toothpaste. the orange is beyond the toothpaste. the orange is to the left of and beyond position E. the orange is to the left of and beyond position G. the orange is to the left of and beyond the doughnut. the orange is to the left of the apple. the orange is to the left of the soda. the orange is to the left of the water bottle. the orange is to the right of and beyond the ball. the orange is to the right of the bottle. the soda is beyond position G. the soda is to the left of and beyond the doughnut. the soda is to the left of the apple. the soda is to the right of and beyond position E. the soda is to the right of and beyond the ball. the soda is to the right of and beyond the toothpaste. the soda is to the right of the bottle. the soda is to the right of the orange. the soda is to the right of the water bottle. the toothpaste is behind the orange. the toothpaste is to the left of and behind the apple. the toothpaste is to the left of and behind the soda. the toothpaste is to the left of and behind the water bottle. the toothpaste is to the left of position E. the toothpaste is to the left of position G. the toothpaste is to the left of the doughnut. the toothpaste is to the right of and behind the bottle. the toothpaste is to the right of the ball. the water bottle is beyond position E. the water bottle is to the left of and beyond position G. the water bottle is to the left of and beyond the doughnut. the water bottle is to the left of the apple. the water bottle is to the left of the soda. the water bottle is to the right of and beyond the ball. the water bottle is to the right of and beyond the toothpaste. the water bottle is to the right of the bottle. the water bottle is to the right of the orange" \\
        \bottomrule
    \end{tabular}
    \caption{Robot arrangement task observation and full state description.}
    \label{tab:full_state_robot}
\end{table}

\end{document}
