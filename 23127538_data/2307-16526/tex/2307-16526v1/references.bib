@inproceedings{vermaCausalNetworksSemantics1990,
  title = {Causal Networks: Semantics and Expressiveness},
  shorttitle = {Causal Networks},
  booktitle = {Proceedings of the {{Fourth Annual Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Verma, Thomas and Pearl, Judea},
  year = {1990},
  month = jan,
  series = {{{UAI}} '88},
  pages = {69--78},
  publisher = {{North-Holland Publishing Co.}},
  address = {{NLD}},
  urldate = {2022-08-26},
  isbn = {978-0-444-88650-7}
}

@article{castroCausalityMattersMedical2020,
  title = {Causality Matters in Medical Imaging.},
  author = {Castro, Daniel Coelho and Walker, Ian and Glocker, Ben},
  year = {2020},
  journal = {Nature Communications},
  doi = {10.1038/s41467-020-17478-w},
  abstract = {Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies. Scarcity of high-quality annotated data and mismatch between the development dataset and the target environment are two of the main challenges in developing predictive tools from medical imaging. In this Perspective, the authors show how causal reasoning can shed new light on these challenges.},
  pmid = {32699250}
}

@inproceedings{duttaThereTradeOffFairness2020,
  title = {Is {{There}} a {{Trade-Off Between Fairness}} and {{Accuracy}}? {{A Perspective Using Mismatched Hypothesis Testing}}},
  shorttitle = {Is {{There}} a {{Trade-Off Between Fairness}} and {{Accuracy}}?},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Dutta, Sanghamitra and Wei, Dennis and Yueksel, Hazar and Chen, Pin-Yu and Liu, Sijia and Varshney, Kush},
  year = {2020},
  month = nov,
  pages = {2803--2813},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-06-06},
  abstract = {A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.},
  langid = {english},

  file = {/data/Zotero/storage/2WF2PLWU/Dutta et al. - 2020 - Is There a Trade-Off Between Fairness and Accuracy.pdf;/data/Zotero/storage/KDF2T7DR/Dutta et al. - 2020 - Is There a Trade-Off Between Fairness and Accuracy.pdf}
}

@inproceedings{friedlerComparativeStudyFairnessenhancing2019,
  title = {A Comparative Study of Fairness-Enhancing Interventions in Machine Learning},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {329--338},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3287560.3287589},
  urldate = {2022-06-14},
  abstract = {Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption. We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.},
  isbn = {978-1-4503-6125-5},
  keywords = {benchmarks,Fairness-aware machine learning},
  file = {/data/Zotero/storage/TFWUKHE5/Friedler et al. - 2019 - A comparative study of fairness-enhancing interven.pdf}
}

@inproceedings{subbaswamyCounterfactualNormalizationProactively2018,
  title = {{Counterfactual normalization: Proactively addressing dataset shift using causal mechanisms}},
  shorttitle = {{Counterfactual normalization}},
  booktitle = {{34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018}},
  author = {Subbaswamy, Adarsh and Saria, Suchi},
  year = {2018},
  month = jan,
  pages = {947--957},
  publisher = {{Association For Uncertainty in Artificial Intelligence (AUAI)}},
  urldate = {2022-08-12},
  langid = {English (US)},
  file = {/data/Zotero/storage/NN395NWM/counterfactual-normalization-proactively-addressing-dataset-shift.html}
}

@inproceedings{zhangMultiSourceDomainAdaptation2015,
  title = {Multi-{{Source Domain Adaptation}}: {{A Causal View}}},
  booktitle = {Twenty-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Zhang, Kun and Gong, Mingming and Schoelkopf, Bernhard},
  year = {2015},
  month = feb,
  pages = {3150},
  urldate = {2022-02-18},
  abstract = {This paper is concerned with the problem of domain adaptation with multiple sources from a causal point of view. In particular, we use causal models to represent the relationship between the features X and class label Y , and consider possible situations where different modules of the causal model change with the domain. In each situation, we investigate what knowledge is appropriate to transfer and find the optimal target-domain hypothesis. This gives an intuitive interpretation of the assumptions underlying certain previous methods and motivates new ones. We finally focus on the case where Y is the cause for X with changing PY and PX|Y , that is, PY and PX|Y change independently across domains. Under appropriate assumptions, the availability of multiple source domains allows a natural way to reconstruct the conditional distribution on the target domain; we propose to model PX|Y (the process to generate effect X from cause Y ) on the target domain as a linear mixture of those on source domains, and estimate all involved parameters by matching the target-domain feature distribution. Experimental results on both synthetic and real-world data verify our theoretical results.},
  keywords = {causal knowledge,conditional shift,domain adaptation,target shift},
  file = {/data/Zotero/storage/DQFPXQVR/full-text.pdf}
}

@article{larrazabalGenderImbalanceMedical2020,
  title = {Gender Imbalance in Medical Imaging Datasets Produces Biased Classifiers for Computer-Aided Diagnosis},
  author = {Larrazabal, Agostina J. and Nieto, Nicol{\'a}s and Peterson, Victoria and Milone, Diego H. and Ferrante, Enzo},
  year = {2020},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {23},
  pages = {12592--12594},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1919012117},
  urldate = {2022-06-14},
  file = {/data/Zotero/storage/CI6CKRN7/Larrazabal et al. - 2020 - Gender imbalance in medical imaging datasets produ.pdf;/data/Zotero/storage/YTW3CB6B/Larrazabal et al. - 2020 - Gender imbalance in medical imaging datasets produ.pdf}
}

@article{madrasLearningAdversariallyFair2018,
  title = {Learning {{Adversarially Fair}} and {{Transferable Representations}}},
  author = {Madras, David and Creager, Elliot and Pitassi, T and Zemel, R},
  year = {2018},
  journal = {ICML},
  doi = {null},
  abstract = {In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.},
  pmid = {null}
}

@article{friedlerImPossibilityFairness2016,
  title = {On the (Im)Possibility of Fairness},
  author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year = {2016},
  month = sep,
  journal = {ArXiv preprint},
  eprint = {1609.07236},
  urldate = {2022-02-17},
  abstract = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the "observed" space) and outputs (the "decision" space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
  archiveprefix = {arxiv},
  file = {/data/Zotero/storage/5GLLLZ4X/full-text.pdf}
}

@article{boykoUseRiskFactors1990,
  title = {The Use of Risk Factors in Medical Diagnosis: Opportunities and Cautions},
  shorttitle = {The Use of Risk Factors in Medical Diagnosis},
  author = {Boyko, E. J. and Alderman, B. W.},
  year = {1990},
  journal = {Journal of Clinical Epidemiology},
  volume = {43},
  number = {9},
  pages = {851--858},
  issn = {0895-4356},
  doi = {10.1016/0895-4356(90)90068-z},
  abstract = {We discuss in this paper the extent to which disease risk factors may assist in the diagnostic process. We caution that disease risk factors need not be very sensitive or specific. Risk factor specificity and sensitivity may be further reduced if, in the former case, the risk factor is related to other illnesses having the same clinical presentation as the disease of interest, or if, in the latter case, the risk factor disappears with the onset of illness. We illustrate these points in a discussion of the utility of smoking as a diagnostic test for malignancy in two clinical situations, the patient with asymptomatic microscopic hematuria and the patient with a solitary pulmonary nodule. Risk factors hold great promise as aids to medical diagnosis, as this information is readily available to clinicians at little or no cost. Clinicians, however, should exercise caution when using risk factors of unproven diagnostic utility in medical diagnosis, as their presence may have little or no effect on disease probability.},
  langid = {english},
  pmid = {2213074},
  keywords = {{Confounding Factors, Epidemiologic},Diagnosis,Female,Hematuria,Humans,Lung Neoplasms,Male,Odds Ratio,Probability,Risk Factors,Sensitivity and Specificity,Smoking,Urologic Neoplasms}
}

@book{petersElementsCausalInference2017,
  title = {Elements of {{Causal Inference}}: {{Foundations}} and {{Learning Algorithms}}},
  shorttitle = {Elements of {{Causal Inference}}},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  publisher = {{The MIT Press}},
  urldate = {2022-07-25},
  abstract = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning.The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.},
  isbn = {978-0-262-03731-0 978-0-262-34429-6},
  langid = {english},
  keywords = {algorithmic independence,assumptions,bic Book Industry Communication::U Computing \& information technology::UM Computer programming / software development::UMS Mobile \& handheld device programming / Apps programming,bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence::UYQM Machine learning,bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence::UYQN Neural networks \& fuzzy systems,causal minimality,Causality,cause-effect models,computer science,conditional independence,counterfactuals,covariate shift,do-calculus,domain adaptation,episodic reinforcement learning,faithfulness,falsifiability,half-sibling regression,identifiability,interventions,machine learning,markov,multivariate causal models,potential outcomes,probability theory,SCMs,semi-supervised learning,simpson's paradox,statistical models,statistics},
  annotation = {Accepted: 2019-01-20 23:42:51},
  file = {/data/Zotero/storage/L5TFPVNA/Peters et al. - 2017 - Elements of Causal Inference Foundations and Lear.pdf}
}

@inproceedings{kimLearningNotLearn2019,
  title = {Learning Not to Learn: {{Training}} Deep Neural Networks with Biased Data},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kim, Byungju and Kim, Hyunwoo and Kim, Kyungsu and Kim, Sungjin and Kim, Junmo},
  year = {2019},
  pages = {9012--9020}
}

@inproceedings{beedeHumanCenteredEvaluationDeep2020,
  title = {A {{Human-Centered Evaluation}} of a {{Deep Learning System Deployed}} in {{Clinics}} for the {{Detection}} of {{Diabetic Retinopathy}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Beede, Emma and Baylor, Elizabeth and Hersch, Fred and Iurchenko, Anna and Wilcox, Lauren and Ruamviboonsuk, Paisan and Vardoulakis, Laura M.},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376718},
  urldate = {2022-08-12},
  abstract = {Deep learning algorithms promise to improve clinician workflows and patient outcomes. However, these gains have yet to be fully demonstrated in real world clinical settings. In this paper, we describe a human-centered study of a deep learning system used in clinics for the detection of diabetic eye disease. From interviews and observation across eleven clinics in Thailand, we characterize current eye-screening workflows, user expectations for an AI-assisted screening process, and post-deployment experiences. Our findings indicate that several socio-environmental factors impact model performance, nursing workflows, and the patient experience. We draw on these findings to reflect on the value of conducting human-centered evaluative research alongside prospective evaluations of model accuracy.},
  isbn = {978-1-4503-6708-0},
  keywords = {deep learning,diabetes,health,human-centered ai},
  file = {/data/Zotero/storage/EIXUU9QP/Beede et al. - 2020 - A Human-Centered Evaluation of a Deep Learning Sys.pdf}
}

@article{ribeiroHighFidelityImage2023,
  title = {High {{Fidelity Image Counterfactuals}} with {{Probabilistic Causal Models}}},
  author = {Ribeiro, Fabio De Sousa and Xia, Tian and Monteiro, Miguel and Pawlowski, Nick and Glocker, Ben},
  year = {2023},
  month = jun,
  journal = {arXiv; To appear in ICML2023},
  number = {arXiv:2306.15764},
  eprint = {2306.15764},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.15764},
  urldate = {2023-07-17},
  abstract = {We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Methodology},
  file = {/data/Zotero/storage/QKA99762/Ribeiro et al. - 2023 - High Fidelity Image Counterfactuals with Probabili.pdf;/data/Zotero/storage/N867NUUN/2306.html}
}

@inproceedings{wangFairnessVisualRecognition2020,
  title = {Towards {{Fairness}} in {{Visual Recognition}}: {{Effective Strategies}} for {{Bias Mitigation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Zeyu and Qinami, Klint and Karakozis, Ioannis Christos and Genova, Kyle and Nair, Prem and Hata, Kenji and Russakovsky, Olga},
  year = {2020},
  month = jun
}

@incollection{bareinboim2022pearl,
  title = {On Pearl's Hierarchy and the Foundations of Causal Inference},
  booktitle = {Probabilistic and Causal Inference: {{The}} Works of Judea Pearl},
  author = {Bareinboim, Elias and Correa, Juan D and Ibeling, Duligur and Icard, Thomas},
  year = {2022},
  pages = {507--556}
}

@misc{yangChangeHardCloser2023,
  title = {Change Is {{Hard}}: {{A Closer Look}} at {{Subpopulation Shift}}},
  shorttitle = {Change Is {{Hard}}},
  author = {Yang, Yuzhe and Zhang, Haoran and Katabi, Dina and Ghassemi, Marzyeh},
  year = {2023},
  month = feb,
  number = {arXiv:2302.12254},
  eprint = {2302.12254},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.12254},
  urldate = {2023-05-11},
  abstract = {Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly effective even without any group information. Finally, unlike existing works that solely aim to improve worst-group accuracy (WGA), we demonstrate the fundamental tradeoff between WGA and other important metrics, highlighting the need to carefully choose testing metrics. Code and data are available at: https://github.com/YyzHarry/SubpopBench.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/data/Zotero/storage/QLQPUPAI/Yang et al. - 2023 - Change is Hard A Closer Look at Subpopulation Shi.pdf;/data/Zotero/storage/NK6N2RSU/2302.html}
}

@inproceedings{alviTurningBlindEye2018,
  title = {Turning a {{Blind Eye}}: {{Explicit Removal}} of {{Biases}} and {{Variation}} from {{Deep Neural Network Embeddings}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}}) {{Workshops}}},
  author = {Alvi, Mohsan and Zisserman, Andrew and Nellaaker, Christoffer},
  year = {2018},
  month = sep,
  pages = {0--0},
  urldate = {2022-02-14},
  abstract = {Neural networks achieve the state-of-the-art in image classification tasks. However, they can encode spurious variations or biases that may be present in the training data. For example, training an age predictor on a dataset that is not balanced for gender can lead to gender biased predicitons (e.g. wrongly predicting that males are older if only elderly males are in the training set). We present two distinct contributions: 1) An algorithm that can remove multiple sources of variation from the feature representation of a network. We demonstrate that this algorithm can be used to remove biases from the feature representation, and thereby improve classification accuracies, when training networks on extremely biased datasets. 2) An ancestral origin database of 14,000 images of individuals from East Asia, the Indian subcontinent, sub-Saharan Africa, and Western Europe. We demonstrate on this dataset, for a number of facial attribute classification tasks, that we are able to remove racial biases from the network feature representation .},
  keywords = {Ancestral origin dataset,Dataset bias {$\cdot$},Face attribute classification {$\cdot$}},
  file = {/data/Zotero/storage/LPRHC37P/full-text.pdf}
}

@misc{corbett-daviesMeasureMismeasureFairness2018,
  title = {The {{Measure}} and {{Mismeasure}} of {{Fairness}}: {{A Critical Review}} of {{Fair Machine Learning}}},
  shorttitle = {The {{Measure}} and {{Mismeasure}} of {{Fairness}}},
  author = {{Corbett-Davies}, Sam and Goel, Sharad},
  year = {2018},
  month = aug,
  number = {arXiv:1808.00023},
  eprint = {1808.00023},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1808.00023},
  urldate = {2022-06-14},
  abstract = {The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes---like race, gender, and their proxies---are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society},
  file = {/data/Zotero/storage/MN2RTHHV/Corbett-Davies and Goel - 2018 - The Measure and Mismeasure of Fairness A Critical.pdf;/data/Zotero/storage/G6TJHC89/1808.html}
}

@inproceedings{scholkopfCausalAnticausalLearning2012,
  title = {On {{Causal}} and {{Anticausal Learning}}},
  booktitle = {Proceedings of the 29th {{International Coference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  year = {2012},
  pages = {459--466},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
  isbn = {978-1-4503-1285-1}
}

@article{gianfrancescoPotentialBiasesMachine2018,
  title = {Potential {{Biases}} in {{Machine Learning Algorithms Using Electronic Health Record Data}}},
  author = {Gianfrancesco, Milena A. and Tamang, Suzanne and Yazdany, Jinoos and Schmajuk, Gabriela},
  year = {2018},
  month = nov,
  journal = {JAMA internal medicine},
  volume = {178},
  number = {11},
  pages = {1544--1547},
  issn = {2168-6106},
  doi = {10.1001/jamainternmed.2018.3763},
  urldate = {2022-08-12},
  abstract = {A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning\textendash based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.},
  pmcid = {PMC6347576},
  pmid = {30128552},
  file = {/data/Zotero/storage/GD6T6J6Y/Gianfrancesco et al. - 2018 - Potential Biases in Machine Learning Algorithms Us.pdf}
}

@article{edwardsCensoringRepresentationsAdversary2016,
  title = {Censoring {{Representations}} with an {{Adversary}}},
  author = {Edwards, Harrison and Storkey, A},
  year = {2016},
  journal = {ICLR},
  doi = {null},
  abstract = {In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from unaligned training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model.},
  pmid = {null}
}

@inproceedings{martinezMinimaxParetoFairness2020,
  title = {Minimax {{Pareto Fairness}}: {{A Multi Objective Perspective}}},
  shorttitle = {Minimax {{Pareto Fairness}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Martinez, Natalia and Bertran, Martin and Sapiro, Guillermo},
  year = {2020},
  month = nov,
  pages = {6755--6764},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-06-06},
  abstract = {In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.},
  langid = {english},
  file = {/data/Zotero/storage/HS92UINV/Martinez et al. - 2020 - Minimax Pareto Fairness A Multi Objective Perspec.pdf;/data/Zotero/storage/VN3CTQCD/Martinez et al. - 2020 - Minimax Pareto Fairness A Multi Objective Perspec.pdf}
}

@article{niccoliAgeingRiskFactor2012,
  title = {Ageing as a Risk Factor for Disease},
  author = {Niccoli, Teresa and Partridge, Linda},
  year = {2012},
  month = sep,
  journal = {Current biology: CB},
  volume = {22},
  number = {17},
  pages = {R741-752},
  issn = {1879-0445},
  doi = {10.1016/j.cub.2012.07.024},
  abstract = {Age is the main risk factor for the prevalent diseases of developed countries: cancer, cardiovascular disease and neurodegeneration. The ageing process is deleterious for fitness, but can nonetheless evolve as a consequence of the declining force of natural selection at later ages, attributable to extrinsic hazards to survival: ageing can then occur as a side-effect of accumulation of mutations that lower fitness at later ages, or of natural selection in favour of mutations that increase fitness of the young but at the cost of a higher subsequent rate of ageing. Once thought of as an inexorable, complex and lineage-specific process of accumulation of damage, ageing has turned out to be influenced by mechanisms that show strong evolutionary conservation. Lowered activity of the nutrient-sensing insulin/insulin-like growth factor/Target of Rapamycin signalling network can extend healthy lifespan in yeast, multicellular invertebrates, mice and, possibly, humans. Mitochondrial activity can also promote ageing, while genome maintenance and autophagy can protect against it. We discuss the relationship between evolutionarily conserved mechanisms of ageing and disease, and the associated scientific challenges and opportunities.},
  langid = {english},
  pmid = {22975005},
  keywords = {Aging,Autophagy,Cardiovascular Diseases,DNA Damage,Humans,Life Expectancy,Mitochondria,Neoplasms,Neurodegenerative Diseases,Risk Factors,Signal Transduction,Time Factors},
  file = {/data/Zotero/storage/KTZWYQKB/Niccoli and Partridge - 2012 - Ageing as a risk factor for disease.pdf}
}

@article{jonesRoleSubgroupSeparability2023,
  title = {The {{Role}} of {{Subgroup Separability}} in {{Group-Fair Medical Image Classification}}},
  author = {Jones, Charles and Roschewitz, M{\'e}lanie and Glocker, Ben},
  year = {2023},
  month = jul,
  journal = {arXiv; To appear in MICCAI2023},
  number = {arXiv:2307.02791},
  eprint = {2307.02791},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.02791},
  urldate = {2023-07-10},
  abstract = {We investigate performance disparities in deep classifiers. We find that the ability of classifiers to separate individuals into subgroups varies substantially across medical imaging modalities and protected characteristics; crucially, we show that this property is predictive of algorithmic bias. Through theoretical analysis and extensive empirical evaluation, we find a relationship between subgroup separability, subgroup disparities, and performance degradation when models are trained on data with systematic bias such as underdiagnosis. Our findings shed new light on the question of how models become biased, providing important insights for the development of fair medical imaging AI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/data/Zotero/storage/73B9DV3S/Jones et al. - 2023 - The Role of Subgroup Separability in Group-Fair Me.pdf;/data/Zotero/storage/ABKX36AE/2307.html}
}

@inproceedings{magliacaneDomainAdaptationUsing2018,
  title = {Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Magliacane, Sara and {van Ommen}, Thijs and Claassen, Tom and Bongers, Stephan and Versteeg, Philip and Mooij, Joris M.},
  year = {2018},
  month = dec,
  series = {{{NIPS}}'18},
  pages = {10869--10879},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  urldate = {2022-05-24},
  abstract = {An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases, these different distributions can be modeled as different contexts of a single underlying system, in which each distribution corresponds to a different perturbation of the system, or in causal terms, an intervention. We focus on a class of such causal domain adaptation problems, where data for one or more source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data.},
  file = {/data/Zotero/storage/KDAY9AD2/Magliacane et al. - 2018 - Domain adaptation by using causal inference to pre.pdf}
}

@article{plecko2022causal,
  title = {Causal Fairness Analysis},
  author = {Plecko, Drago and Bareinboim, Elias},
  year = {2022},
  journal = {arXiv preprint arXiv:2207.11385},
  eprint = {2207.11385},
  archiveprefix = {arxiv}
}

@inproceedings{yueTransportingCausalMechanisms2021,
  title = {Transporting {{Causal Mechanisms}} for {{Unsupervised Domain Adaptation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}}), 2021},
  author = {Yue, Zhongqi and Sun, Qianru and Hua, Xian-Sheng and Zhang, Hanwang},
  year = {2021},
  pages = {8599--8608},
  urldate = {2022-02-18},
  abstract = {Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate shift and conditional shift assumptions , which essentially encourage models to learn common features across domains. However, due to the lack of supervision in the target domain, they suffer from the semantic loss: the feature will inevitably lose non-discriminative semantics in source domain, which is however discriminative in target domain. We use a causal view-transportability theory [40]-to identify that such loss is in fact a confounding effect, which can only be removed by causal intervention. However, the theoretical solution provided by transportability is far from practical for UDA, because it requires the stratification and representation of the unobserved confounder that is the cause of the domain gap. To this end, we propose a practical solution: Transporting Causal Mechanisms (TCM), to identify the confounder stratum and representations by using the domain-invariant disentangled causal mechanisms, which are discovered in an unsupervised fashion. Our TCM is both theoretically and empirically grounded. Extensive experiments show that TCM achieves state-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA, Office-Home, and VisDA-2017. Codes are available at https://github.com/yue-zhongqi/ tcm.},
  file = {/data/Zotero/storage/MFLUJC4X/full-text.pdf}
}

@inproceedings{zongMEDFAIRBenchmarkingFairness2023,
  title = {{{MEDFAIR}}: {{Benchmarking Fairness}} for {{Medical Imaging}}},
  shorttitle = {{{MEDFAIR}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Zong, Yongshuo and Yang, Yongxin and Hospedales, Timothy},
  year = {2023},
  month = feb,
  urldate = {2023-04-02},
  abstract = {A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, ten datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR.},
  langid = {english},
  file = {/data/Zotero/storage/ZGLBWYZA/Zong et al. - 2023 - MEDFAIR Benchmarking Fairness for Medical Imaging.pdf}
}
@article{seyyed-kalantariUnderdiagnosisBiasArtificial2021,
  title = {Underdiagnosis Bias of Artificial Intelligence Algorithms Applied to Chest Radiographs in Under-Served Patient Populations},
  author = {{Seyyed-Kalantari}, Laleh and Zhang, Haoran and McDermott, Matthew B.A. and Chen, Irene Y. and Ghassemi, Marzyeh},
  year = {2021},
  month = dec,
  journal = {Nature Medicine 2021 27:12},
  volume = {27},
  number = {12},
  pages = {2176--2182},
  publisher = {{Nature Publishing Group}},
  issn = {1546-170X},
  doi = {10.1038/s41591-021-01595-0},
  urldate = {2022-02-18},
  abstract = {Artificial intelligence (AI) systems have increasingly achieved expert-level performance in medical imaging applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations such as female patients, Black patients, or patients of low socioeconomic status. Such biases are especially troubling in the context of underdiagnosis, whereby the AI algorithm would inaccurately label an individual with a disease as healthy, potentially delaying access to care. Here, we examine algorithmic underdiagnosis in chest X-ray pathology classification across three large chest X-ray datasets, as well as one multi-source dataset. We find that classifiers produced using state-of-the-art computer vision techniques consistently and selectively underdiagnosed under-served patient populations and that the underdiagnosis rate was higher for intersectional under-served subpopulations, for example, Hispanic female patients. Deployment of AI systems using medical imaging for disease diagnosis with such biases risks exacerbation of existing care biases and can potentially lead to unequal access to medical treatment, thereby raising ethical concerns for the use of these models in the clinic. Artificial intelligence algorithms trained using chest X-rays consistently underdiagnose pulmonary abnormalities or diseases in historically under-served patient populations, raising ethical concerns about the clinical use of such algorithms.},
  pmid = {34893776},
  keywords = {Machine learning,Medical imaging},
  file = {/data/Zotero/storage/4IULRSLT/full-text.pdf;/data/Zotero/storage/NEKZHKTJ/s41591-021-01595-0.html}
}

@article{richardsonAccessHealthHealth2010,
  title = {Access to {{Health}} and {{Health Care}}: {{How Race}} and {{Ethnicity Matter}}},
  shorttitle = {Access to {{Health}} and {{Health Care}}},
  author = {Richardson, Lynne D. and Norris, Marlaina},
  year = {2010},
  journal = {Mount Sinai Journal of Medicine: A Journal of Translational and Personalized Medicine},
  volume = {77},
  number = {2},
  pages = {166--177},
  issn = {1931-7581},
  doi = {10.1002/msj.20174},
  urldate = {2023-01-13},
  abstract = {Racial and ethnic disparities in health are multifactorial; they reflect differences in biological vulnerability to disease as well as differences in social resources, environmental factors, and health care interventions. Understanding and intervening in health inequity require an understanding of the disparate access to all of the personal resources and environmental conditions that are needed to generate and sustain health, a set of circumstances that constitute access to health. These include access to health information, participation in health promotion and disease prevention activities, safe housing, nutritious foods, convenient exercise spaces, freedom from ambient violence, adequate social support, communities with social capital, and access to quality health care. Access to health care is facilitated by health insurance, a regular source of care, and a usual primary care provider. Various mechanisms through which access to health and access to health care are mediated by race and ethnicity are discussed; these include the built environment, social environment, residential segregation, stress, racism, and discrimination. Empirical evidence supporting the association between these factors and health inequities is also reviewed. Mt Sinai J Med 77:166\textendash 177, 2010. \textcopyright{} 2010 Mount Sinai School of Medicine},
  langid = {english},
  keywords = {access to health care,disparities in health care,ethnic disparities,health disparities,institutional racism,racial disparities,residential segregation,social determinants of health},
  file = {/data/Zotero/storage/Q5DJIIH3/msj.html}
}

@inproceedings{dworkDecoupledClassifiersGroupFair2018,
  title = {Decoupled {{Classifiers}} for {{Group-Fair}} and {{Efficient Machine Learning}}},
  booktitle = {Proceedings of the 1st {{Conference}} on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  author = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
  editor = {Friedler, Sorelle A and Wilson, Christo},
  year = {2018},
  month = mar,
  volume = {81},
  pages = {119--133},
  publisher = {{PMLR}},
  abstract = {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.}
}

@article{vapnikOverviewStatisticalLearning1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, V.N.},
  year = {1999},
  month = sep,
  journal = {IEEE Transactions on Neural Networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  issn = {1941-0093},
  doi = {10.1109/72.788640},
  abstract = {Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems.},
  keywords = {Algorithm design and analysis,Loss measurement,Machine learning,Multidimensional systems,Pattern recognition,Probability distribution,Risk management,Statistical learning,Support vector machines},
  file = {/data/Zotero/storage/CUZ52PKA/Vapnik - 1999 - An overview of statistical learning theory.pdf;/data/Zotero/storage/3QKIFBVQ/788640.html}
}

@article{huangDistributionShiftMining2017,
  title = {Behind {{Distribution Shift}}: {{Mining Driving Forces}} of {{Changes}} and {{Causal Arrows}}},
  shorttitle = {Behind {{Distribution Shift}}},
  author = {Huang, Biwei and Zhang, Kun and Zhang, Jiji and {Sanchez-Romero}, Ruben and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  month = nov,
  journal = {Proceedings. IEEE International Conference on Data Mining},
  volume = {2017},
  pages = {913--918},
  issn = {1550-4786},
  doi = {10.1109/ICDM.2017.114},
  urldate = {2022-08-12},
  abstract = {We address two important issues in causal discovery from nonstationary or heterogeneous data, where parameters associated with a causal structure may change over time or across data sets. First, we investigate how to efficiently estimate the ``driving force'' of the nonstationarity of a causal mechanism. That is, given a causal mechanism that varies over time or across data sets and whose qualitative structure is known, we aim to extract from data a low-dimensional and interpretable representation of the main components of the changes. For this purpose we develop a novel kernel embedding of nonstationary conditional distributions that does not rely on sliding windows. Second, the embedding also leads to a measure of dependence between the changes of causal modules that can be used to determine the directions of many causal arrows. We demonstrate the power of our methods with experiments on both synthetic and real data.},
  pmcid = {PMC6502242},
  pmid = {31068766},
  file = {/data/Zotero/storage/QZDM32RB/Huang et al. - 2017 - Behind Distribution Shift Mining Driving Forces o.pdf}
}

@incollection{hendricksWomenAlsoSnowboard2018,
  title = {Women {{Also Snowboard}}: {{Overcoming Bias}} in {{Captioning Models}}},
  shorttitle = {Women {{Also Snowboard}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11207},
  pages = {793--811},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01219-9\_47},
  urldate = {2022-08-12},
  isbn = {978-3-030-01218-2 978-3-030-01219-9},
  langid = {english},
  file = {/data/Zotero/storage/6ZURSX2N/Hendricks et al. - 2018 - Women Also Snowboard Overcoming Bias in Captionin.pdf}
}

@inproceedings{hardtEqualityOpportunitySupervised2016,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
  editor = {Lee, D and Sugiyama, M and Luxburg, U and Guyon, I and Garnett, R},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{pearlIdentifyingIndependenciesCausal1996,
  title = {Identifying Independencies in Causal Graphs with Feedback},
  booktitle = {Proceedings of the {{Twelfth}} International Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Pearl, Judea and Dechter, Rina},
  year = {1996},
  month = aug,
  series = {{{UAI}}'96},
  pages = {420--426},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  urldate = {2022-08-26},
  abstract = {We show that the d-separation criterion constitutes a valid test for conditional independence relationships that are induced by feedback systems involving discrete variables.},
  isbn = {978-1-55860-412-4},
  file = {/data/Zotero/storage/9W2T37E5/Pearl and Dechter - 1996 - Identifying independencies in causal graphs with f.pdf}
}

@inproceedings{mitchellModelCardsModel2019,
  title = {Model {{Cards}} for {{Model Reporting}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {220--229},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3287560.3287596},
  urldate = {2023-04-20},
  abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
  isbn = {978-1-4503-6125-5},
  keywords = {datasheets,disaggregated evaluation,documentation,ethical considerations,fairness evaluation,ML model evaluation,model cards},
  file = {/data/Zotero/storage/GWL77R9C/Mitchell et al. - 2019 - Model Cards for Model Reporting.pdf}
}

@inproceedings{quadriantoDiscoveringFairRepresentations2019,
  title = {Discovering {{Fair Representations}} in the {{Data Domain}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Quadrianto, Novi and Sharmanska, Viktoriia and Thomas, Oliver},
  year = {2019},
  month = jun,
  pages = {8219--8228},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00842},
  abstract = {Interpretability and fairness are critical in computer vision and machine learning applications, in particular when dealing with human outcomes, e.g. inviting or not inviting for a job interview based on application materials that may include photographs. One promising direction to achieve fairness is by learning data representations that remove the semantics of protected characteristics, and are therefore able to mitigate unfair outcomes. All available models however learn latent embeddings which comes at the cost of being uninterpretable. We propose to cast this problem as data-to-data translation, i.e. learning a mapping from an input domain to a fair target domain, where a fairness definition is being enforced. Here the data domain can be images, or any tabular data representation. This task would be straightforward if we had fair target data available, but this is not the case. To overcome this, we learn a highly unconstrained mapping by exploiting statistics of residuals \textendash{} the difference between input data and its translated version \textendash{} and the protected characteristics. When applied to the CelebA dataset of face images with gender attribute as the protected characteristic, our model enforces equality of opportunity by adjusting the eyes and lips regions. Intriguingly, on the same dataset we arrive at similar conclusions when using semantic attribute representations of images for translation. On face images of the recent DiF dataset, with the same gender attribute, our method adjusts nose regions. In the Adult income dataset, also with protected gender attribute, our model achieves equality of opportunity by, among others, obfuscating the wife and husband relationship. Analyzing those systematic changes will allow us to scrutinize the interplay of fairness criterion, chosen protected characteristics, and prediction performance.},
  keywords = {Image and Video Synthesis,Representation Learning},
  file = {/data/Zotero/storage/EBW64A3N/Quadrianto et al. - 2019 - Discovering Fair Representations in the Data Domai.pdf}
}

@article{glockerAlgorithmicEncodingProtected2023,
  title = {Algorithmic Encoding of Protected Characteristics in Chest {{X-ray}} Disease Detection Models},
  author = {Glocker, Ben and Jones, Charles and Bernhardt, M{\'e}lanie and Winzeck, Stefan},
  year = {2023},
  month = mar,
  journal = {eBioMedicine},
  volume = {89},
  publisher = {{Elsevier}},
  issn = {2352-3964},
  doi = {10.1016/j.ebiom.2023.104467},
  urldate = {2023-02-13},
  langid = {english},
  keywords = {Algorithmic bias,Artificial intelligence,Image-based disease detection,Subgroup disparities},
  file = {/data/Zotero/storage/JMAFSDGI/Glocker et al. - 2023 - Algorithmic encoding of protected characteristics .pdf}
}

@inproceedings{wickUnlockingFairnessTradeoff2019,
  title = {Unlocking {{Fairness}}: A {{Trade-off Revisited}}},
  shorttitle = {Unlocking {{Fairness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wick, Michael and {panda}, swetasudha and Tristan, Jean-Baptiste},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-06-06},
  abstract = {The prevailing wisdom is that a model's fairness and its accuracy   are in tension with one another.  However, there is a pernicious   \{\textbackslash em modeling-evaluating dualism\} bedeviling fair machine learning   in which phenomena such as label bias are appropriately acknowledged   as a source of unfairness when designing fair models,   only to be tacitly abandoned when evaluating them.  We investigate   fairness and accuracy, but this time under a variety of controlled   conditions in which we vary the amount and type of bias.  We find,   under reasonable assumptions, that the tension between fairness and   accuracy is illusive, and vanishes as soon as we account for these   phenomena during evaluation.  Moreover, our results are consistent   with an opposing conclusion: fairness and accuracy are sometimes in   accord.  This raises the question, \{\textbackslash em might there be a way to     harness fairness to improve accuracy after all?\}  Since most   notions of fairness are with respect to the model's predictions and   not the ground truth labels, this provides an opportunity to see if   we can improve accuracy by harnessing appropriate notions of   fairness over large quantities of \{\textbackslash em unlabeled\} data with   techniques like posterior regularization and generalized   expectation.  Indeed, we find that semi-supervision not only   improves fairness, but also accuracy and has advantages over   existing in-processing methods that succumb to selection bias on the   training set.},
  file = {/data/Zotero/storage/9KNNID69/Wick et al. - 2019 - Unlocking Fairness a Trade-off Revisited.pdf}
}

@incollection{dianaMinimaxGroupFairness2021,
  title = {Minimax {{Group Fairness}}: {{Algorithms}} and {{Experiments}}},
  shorttitle = {Minimax {{Group Fairness}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Diana, Emily and Gill, Wesley and Kearns, Michael and Kenthapadi, Krishnaram and Roth, Aaron},
  year = {2021},
  month = jul,
  pages = {66--76},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  urldate = {2022-06-06},
  abstract = {We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.},
  isbn = {978-1-4503-8473-5},
  keywords = {fair machine learning,game theory,minimax fairness},
  file = {/data/Zotero/storage/DV2DMD3G/Diana et al. - 2021 - Minimax Group Fairness Algorithms and Experiments.pdf}
}

@inproceedings{pushkarnaDataCardsPurposeful2022,
  title = {Data {{Cards}}: {{Purposeful}} and {{Transparent Dataset Documentation}} for {{Responsible AI}}},
  shorttitle = {Data {{Cards}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {1776--1826},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3531146.3533231},
  urldate = {2023-04-20},
  abstract = {As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset's origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset's lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models\textemdash such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.x},
  isbn = {978-1-4503-9352-2},
  keywords = {data cards,dataset documentation,datasheets,model cards,responsible AI,transparency},
  file = {/data/Zotero/storage/L8ELMD9E/Pushkarna et al. - 2022 - Data Cards Purposeful and Transparent Dataset Doc.pdf}
}

@inproceedings{subbaswamyPreventingFailuresDue2019,
  title = {Preventing {{Failures Due}} to {{Dataset Shift}}: {{Learning Predictive Models That Transport}}},
  shorttitle = {Preventing {{Failures Due}} to {{Dataset Shift}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Subbaswamy, Adarsh and Schulam, Peter and Saria, Suchi},
  year = {2019},
  month = apr,
  pages = {3118--3127},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-08-12},
  abstract = {Classical supervised learning produces unreliable models when training and target distributions differ, with most existing solutions requiring samples from the target domain. We propose a proactive approach which learns a relationship in the training domain that will generalize to the target domain by incorporating prior knowledge of aspects of the data generating process that are expected to differ as expressed in a causal selection diagram. Specifically, we remove variables generated by unstable mechanisms from the joint factorization to yield the Surgery Estimator\textemdash an interventional distribution that is invariant to the differences across environments. We prove that the surgery estimator finds stable relationships in strictly more scenarios than previous approaches which only consider conditional relationships, and demonstrate this in simulated experiments. We also evaluate on real world data for which the true causal diagram is unknown, performing competitively against entirely data-driven approaches.},
  langid = {english},
  file = {/data/Zotero/storage/7HCMXUFM/Subbaswamy et al. - 2019 - Preventing Failures Due to Dataset Shift Learning.pdf;/data/Zotero/storage/VMD5VX5Y/Subbaswamy et al. - 2019 - Preventing Failures Due to Dataset Shift Learning.pdf}
}

@article{riedelAgeAPOESex2016,
  title = {Age, {{APOE}} and Sex: {{Triad}} of Risk of {{Alzheimer}}'s Disease},
  shorttitle = {Age, {{APOE}} and Sex},
  author = {Riedel, Brandalyn C. and Thompson, Paul M. and Brinton, Roberta Diaz},
  year = {2016},
  month = jun,
  journal = {The Journal of Steroid Biochemistry and Molecular Biology},
  series = {{{SI}}:{{Steroids}} \& {{Nervous System}}},
  volume = {160},
  pages = {134--147},
  issn = {0960-0760},
  doi = {10.1016/j.jsbmb.2016.03.012},
  urldate = {2023-01-13},
  abstract = {Age, apolipoprotein E {$\epsilon$}4 (APOE) and chromosomal sex are well-established risk factors for late-onset Alzheimer's disease (LOAD; AD). Over 60\% of persons with AD harbor at least one APOE-{$\epsilon$}4 allele. The sex-based prevalence of AD is well documented with over 60\% of persons with AD being female. Evidence indicates that the APOE-{$\epsilon$}4 risk for AD is greater in women than men, which is particularly evident in heterozygous women carrying one APOE-{$\epsilon$}4 allele. Paradoxically, men homozygous for APOE-{$\epsilon$}4 are reported to be at greater risk for mild cognitive impairment and AD. Herein, we discuss the complex interplay between the three greatest risk factors for Alzheimer's disease, age, APOE-{$\epsilon$}4 genotype and chromosomal sex. We propose that the convergence of these three risk factors, and specifically the bioenergetic aging perimenopause to menopause transition unique to the female, creates a risk profile for AD unique to the female. Further, we discuss the specific risk of the APOE-{$\epsilon$}4 positive male which appears to emerge early in the aging process. Evidence for impact of the triad of AD risk factors is most evident in the temporal trajectory of AD progression and burden of pathology in relation to APOE genotype, age and sex. Collectively, the data indicate complex interactions between age, APOE genotype and gender that belies a one size fits all approach and argues for a precision medicine approach that integrates across the three main risk factors for Alzheimer's disease.},
  langid = {english},
  keywords = {Age,Alzheimer's disease,ApoE,Bioenergetics,Brain,Mitochondria,Sex difference},
  file = {/data/Zotero/storage/I4F8JQ8N/Riedel et al. - 2016 - Age, APOE and sex Triad of risk of Alzheimers di.pdf}
}

@article{bernhardtPotentialSourcesDataset2022,
  title = {Potential Sources of Dataset Bias Complicate Investigation of Underdiagnosis by Machine Learning Algorithms},
  author = {Bernhardt, M{\'e}lanie and Jones, Charles and Glocker, Ben},
  year = {2022},
  month = jun,
  journal = {Nature Medicine},
  volume = {28},
  number = {6},
  pages = {1157--1158},
  publisher = {{Nature Publishing Group}},
  issn = {1546-170X},
  doi = {10.1038/s41591-022-01846-8},
  urldate = {2022-09-02},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Diagnosis,Ethics,Medical imaging},
  file = {/data/Zotero/storage/2XL4G52T/Bernhardt et al. - 2022 - Potential sources of dataset bias complicate inves.pdf;/data/Zotero/storage/WAVPVLQY/s41591-022-01846-8.html}
}

@inproceedings{zemelLearningFairRepresentations2013,
  title = {Learning {{Fair Representations}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  year = {2013},
  month = may,
  pages = {325--333},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2023-04-24},
  abstract = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.},
  langid = {english},
  file = {/data/Zotero/storage/DG82QLT4/Zemel et al. - 2013 - Learning Fair Representations.pdf}
}

@article{johnsonMIMICCXRDeidentifiedPublicly2019,
  title = {{{MIMIC-CXR}}, a de-Identified Publicly Available Database of Chest Radiographs with Free-Text Reports},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Berkowitz, Seth J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih-ying and Mark, Roger G. and Horng, Steven},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {317},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0322-0},
  urldate = {2023-02-21},
  abstract = {Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's chest, but requires specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. Here we describe MIMIC-CXR, a large dataset of 227,835 imaging studies for 65,379 patients presenting to the Beth Israel Deaconess Medical Center Emergency Department between 2011\textendash 2016. Each imaging study can contain one or more images, usually a frontal view and a lateral view. A total of 377,110 images are available in the dataset. Studies are made available with a semi-structured free-text radiology report that describes the radiological findings of the images, written by a practicing radiologist contemporaneously during routine clinical care. All images and reports have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in computer vision, natural language processing, and clinical data mining.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Radiography,Translational research},
  file = {/data/Zotero/storage/LI3UB4UG/Johnson et al. - 2019 - MIMIC-CXR, a de-identified publicly available data.pdf}
}

@article{szczepuraAccessHealthCare2005,
  title = {Access to Health Care for Ethnic Minority Populations},
  author = {Szczepura, A.},
  year = {2005},
  month = mar,
  journal = {Postgraduate Medical Journal},
  volume = {81},
  number = {953},
  pages = {141--147},
  publisher = {{The Fellowship of Postgraduate Medicine}},
  issn = {0032-5473, 1469-0756},
  doi = {10.1136/pgmj.2004.026237},
  urldate = {2023-01-13},
  abstract = {This paper reviews the research evidence on access to health care by ethnic minority populations, and discusses what might need to be done to improve access to services. Research on the process of care, and the quality of care received, is considered as well as studies examining uptake of services. Changes in legal context are increasing the pressure on healthcare organisations to examine and adapt their services to ensure equitable access. Examples presented include a new UK population cancer screening programme. The main challenges for clinicians, managers, and policy makers in ensuring equitable access are discussed.},
  chapter = {Review},
  copyright = {Copyright 2005 The Fellowship of Postgraduate Medicine},
  langid = {english},
  pmid = {15749788},
  keywords = {{BME, black and minority ethnic},cancer screening,{CRC, colorectal cancer},cultural barriers,cultural competence,ethnicity,{FOBt, faecal occult blood test},health care access,language},
  file = {/data/Zotero/storage/X9QC8TID/Szczepura - 2005 - Access to health care for ethnic minority populati.pdf}
}

@inproceedings{pearlTransportabilityCausalStatistical2011,
  title = {Transportability of {{Causal}} and {{Statistical Relations}}: {{A Formal Approach}}},
  shorttitle = {Transportability of {{Causal}} and {{Statistical Relations}}},
  booktitle = {Twenty-{{Fifth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Pearl, Judea and Bareinboim, Elias},
  year = {2011},
  month = aug,
  urldate = {2023-01-17},
  abstract = {We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected.   We introduce a formal representation called "selection diagrams" for expressing knowledge about differences and commonalities between environments  and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is affirmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  langid = {english},
  file = {/data/Zotero/storage/W4SD3IT2/Pearl and Bareinboim - 2011 - Transportability of Causal and Statistical Relatio.pdf}
}

@inproceedings{maoCausalTransportabilityVisual2022,
  title = {Causal {{Transportability}} for {{Visual Recognition}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mao, Chengzhi and Xia, Kevin and Wang, James and Wang, Hao and Yang, Junfeng and Bareinboim, Elias and Vondrick, Carl},
  year = {2022},
  pages = {7521--7531},
  urldate = {2022-12-19},
  langid = {english},
  file = {/data/Zotero/storage/24KEVFQE/Mao et al. - 2022 - Causal Transportability for Visual Recognition.pdf}
}

@article{chiappaPathSpecificCounterfactualFairness2019,
  title = {Path-{{Specific Counterfactual Fairness}}.},
  author = {Chiappa, Silvia},
  year = {2019},
  journal = {AAAI},
  doi = {10.1609/aaai.v33i01.33017801},
  abstract = {We consider the problem of learning fair decision systems from data in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a counterfactual approach to disregard effects along unfair pathways that does not incur in the same loss of individual-specific information as previous approaches. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. We leverage recent developments in deep learning and approximate inference to develop a VAE-type method that is widely applicable to complex nonlinear models.},
  pmid = {null},
  file = {/data/Zotero/storage/YYRARPCG/4777-Article Text-7816-1-10-20190708.pdf}
}

@article{iglehartNewEraMedical2006,
  title = {The {{New Era}} of {{Medical Imaging}} \textemdash{} {{Progress}} and {{Pitfalls}}},
  author = {Iglehart, John K.},
  year = {2006},
  month = jun,
  journal = {New England Journal of Medicine},
  volume = {354},
  number = {26},
  pages = {2822--2828},
  publisher = {{Massachusetts Medical Society}},
  issn = {0028-4793},
  doi = {10.1056/NEJMhpr061219},
  urldate = {2023-01-16},
  abstract = {Rapid advances in biomedical imaging have greatly enhanced the ability of physicians to diagnose and treat a variety of diseases. This enhanced ability often leads to improved outcomes for patients. However, these improvements, combined with a rise in entrepreneurial activity by physicians, the practice of defensive medicine in order to thwart malpractice suits, and the power of patients who demand more tests, have led to sharp increases in the volume of imaging services and the expenditures for them. In recent years, this growth in spending has outstripped that of most other services covered by Medicare and private insurers. In response, . . .},
  pmid = {16807422},
  file = {/data/Zotero/storage/S8NV3R48/Iglehart - 2006 - The New Era of Medical Imaging  Progress and Pitf.pdf}
}

@article{chenAlgorithmicFairnessArtificial2023,
  title = {Algorithmic Fairness in Artificial Intelligence for Medicine and Healthcare},
  author = {Chen, Richard J. and Wang, Judy J. and Williamson, Drew F. K. and Chen, Tiffany Y. and Lipkova, Jana and Lu, Ming Y. and Sahai, Sharifa and Mahmood, Faisal},
  year = {2023},
  month = jun,
  journal = {Nature Biomedical Engineering},
  volume = {7},
  number = {6},
  pages = {719--742},
  publisher = {{Nature Publishing Group}},
  issn = {2157-846X},
  doi = {10.1038/s41551-023-01056-8},
  urldate = {2023-07-18},
  abstract = {In healthcare, the development and deployment of insufficiently fair systems of artificial intelligence (AI) can undermine the delivery of equitable care. Assessments of AI models stratified across subpopulations have revealed inequalities in how patients are diagnosed, treated and billed. In this Perspective, we outline fairness in machine learning through the lens of healthcare, and discuss how algorithmic biases (in data acquisition, genetic variation and intra-observer labelling variability, in particular) arise in clinical workflows and the resulting healthcare disparities. We also review emerging technology for mitigating biases via disentanglement, federated learning and model explainability, and their role in the development of AI-based software as a medical device.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Health policy,Machine learning},
  file = {/data/Zotero/storage/RES7KKJG/Chen et al. - 2023 - Algorithmic fairness in artificial intelligence fo.pdf}
}

@article{pearlCausalDiagramsEmpirical1995,
  title = {Causal Diagrams for Empirical Research},
  author = {Pearl, Judea},
  year = {1995},
  month = dec,
  journal = {Biometrika},
  volume = {82},
  number = {4},
  pages = {669--688},
  issn = {0006-3444},
  doi = {10.1093/biomet/82.4.669},
  urldate = {2022-09-20},
  abstract = {The primary aim of this paper is to show how graphical models can be used as a mathematical language for integrating statistical and subject-matter information. In particular, the paper develops a principled, nonparametric framework for causal inference, in which diagrams are queried to determine if the assumptions available are sufficient for identifying causal effects from nonexperimental data. If so the diagrams can be queried to produce mathematical expressions for causal effects in terms of observed distributions; otherwise, the diagrams can be queried to suggest additional observations or auxiliary experiments from which the desired inferences can be obtained.},
  file = {/data/Zotero/storage/LQQ86LNA/PEARL - 1995 - Causal diagrams for empirical research.pdf;/data/Zotero/storage/YVCLL3ZQ/251647.html}
}

@article{rosenbaumCentralRolePropensity1983,
  title = {The Central Role of the Propensity Score in Observational Studies for Causal Effects},
  author = {Rosenbaum, Paul R. and Rubin, Donald B.},
  year = {1983},
  month = apr,
  journal = {Biometrika},
  volume = {70},
  number = {1},
  pages = {41--55},
  issn = {0006-3444},
  doi = {10.1093/biomet/70.1.41},
  urldate = {2022-09-20},
  abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.},
  file = {/data/Zotero/storage/E93V2WQ9/ROSENBAUM and RUBIN - 1983 - The central role of the propensity score in observ.pdf;/data/Zotero/storage/GHG8N5M9/240879.html}
}

@article{pawlowskiDeepStructuralCausal2020,
  title = {Deep Structural Causal Models for Tractable Counterfactual Inference},
  author = {Pawlowski, Nick and Castro, Daniel C. and Glocker, Ben},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2020-December},
  eprint = {2006.06485},
  publisher = {{Neural information processing systems foundation}},
  urldate = {2021-10-18},
  abstract = {We formulate a general framework for building structural causal models (SCMs) with deep learning components. The proposed approach employs normalising flows and variational inference to enable tractable inference of exogenous noise variables\textemdash a crucial step for counterfactual inference that is missing from existing deep causal learning methods. Our framework is validated on a synthetic dataset built on MNIST as well as on a real-world medical dataset of brain MRI scans. Our experimental results indicate that we can successfully train deep SCMs that are capable of all three levels of Pearl's ladder of causation: association, intervention, and counterfactuals, giving rise to a powerful new approach for answering causal questions in imaging applications and beyond. The code for all our experiments is available at https://github.com/biomedia-mira/deepscm.},
  archiveprefix = {arxiv}
}

@article{gichoyaAIRecognitionPatient2022,
  title = {{{AI}} Recognition of Patient Race in Medical Imaging: A Modelling Study},
  shorttitle = {{{AI}} Recognition of Patient Race in Medical Imaging},
  author = {Gichoya, Judy Wawira and Banerjee, Imon and Bhimireddy, Ananth Reddy and Burns, John L and Celi, Leo Anthony and Chen, Li-Ching and Correa, Ramon and Dullerud, Natalie and Ghassemi, Marzyeh and Huang, Shih-Cheng and Kuo, Po-Chih and Lungren, Matthew P and Palmer, Lyle J and Price, Brandon J and Purkayastha, Saptarshi and Pyrros, Ayis T and {Oakden-Rayner}, Lauren and Okechukwu, Chima and {Seyyed-Kalantari}, Laleh and Trivedi, Hari and Wang, Ryan and Zaiman, Zachary and Zhang, Haoran},
  year = {2022},
  month = jun,
  journal = {The Lancet Digital Health},
  volume = {4},
  number = {6},
  pages = {e406-e414},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(22)00063-2},
  urldate = {2023-01-18},
  abstract = {Background Previous studies in medical imaging have shown disparate abilities of artificial intelligence (AI) to detect a person's race, yet there is no known correlation for race on medical imaging that would be obvious to human experts when interpreting the images. We aimed to conduct a comprehensive evaluation of the ability of AI to recognise a patient's racial identity from medical images. Methods Using private (Emory CXR, Emory Chest CT, Emory Cervical Spine, and Emory Mammogram) and public (MIMIC-CXR, CheXpert, National Lung Cancer Screening Trial, RSNA Pulmonary Embolism CT, and Digital Hand Atlas) datasets, we evaluated, first, performance quantification of deep learning models in detecting race from medical images, including the ability of these models to generalise to external environments and across multiple imaging modalities. Second, we assessed possible confounding of anatomic and phenotypic population features by assessing the ability of these hypothesised confounders to detect race in isolation using regression models, and by re-evaluating the deep learning models by testing them on datasets stratified by these hypothesised confounding variables. Last, by exploring the effect of image corruptions on model performance, we investigated the underlying mechanism by which AI models can recognise race. Findings In our study, we show that standard AI deep learning models can be trained to predict race from medical images with high performance across multiple imaging modalities, which was sustained under external validation conditions (x-ray imaging [area under the receiver operating characteristics curve (AUC) range 0{$\cdot$}91\textendash 0{$\cdot$}99], CT chest imaging [0{$\cdot$}87\textendash 0{$\cdot$}96], and mammography [0{$\cdot$}81]). We also showed that this detection is not due to proxies or imaging-related surrogate covariates for race (eg, performance of possible confounders: body-mass index [AUC 0{$\cdot$}55], disease distribution [0{$\cdot$}61], and breast density [0{$\cdot$}61]). Finally, we provide evidence to show that the ability of AI deep learning models persisted over all anatomical regions and frequency spectrums of the images, suggesting the efforts to control this behaviour when it is undesirable will be challenging and demand further study. Interpretation The results from our study emphasise that the ability of AI deep learning models to predict self-reported race is itself not the issue of importance. However, our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, creates an enormous risk for all model deployments in medical imaging. Funding National Institute of Biomedical Imaging and Bioengineering, MIDRC grant of National Institutes of Health, US National Science Foundation, National Library of Medicine of the National Institutes of Health, and Taiwan Ministry of Science and Technology},
  langid = {english},
  file = {/data/Zotero/storage/ASAMFIF3/Gichoya et al. - 2022 - AI recognition of patient race in medical imaging.pdf;/data/Zotero/storage/IIF4QVV8/S2589750022000632.html}
}

@article{wolpertNoFreeLunch1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  issn = {1941-0026},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  keywords = {Algorithm design and analysis,Bayesian methods,Evolutionary computation,Information theory,Iron,Minimax techniques,Performance analysis,Probability distribution,Simulated annealing},
  file = {/data/Zotero/storage/J2UXNANR/Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf;/data/Zotero/storage/AFA8V565/585893.html}
}

@article{oakden-raynerHiddenStratificationCauses2020,
  title = {Hidden {{Stratification Causes Clinically Meaningful Failures}} in {{Machine Learning}} for {{Medical Imaging}}},
  author = {{Oakden-Rayner}, Luke and Dunnmon, Jared and Carneiro, Gustavo and R{\'e}, Christopher},
  year = {2020},
  month = apr,
  journal = {Proceedings of the ACM Conference on Health, Inference, and Learning},
  volume = {2020},
  pages = {151--159},
  doi = {10.1145/3368555.3384468},
  urldate = {2022-08-12},
  abstract = {Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20\% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.},
  pmcid = {PMC7665161},
  pmid = {33196064},
  file = {/data/Zotero/storage/ALLMZX3F/Oakden-Rayner et al. - 2020 - Hidden Stratification Causes Clinically Meaningful.pdf}
}

@inproceedings{buolamwiniGenderShadesIntersectional2018,
  title = {Gender {{Shades}}: {{Intersectional Accuracy Disparities}} in {{Commercial Gender Classification}}},
  shorttitle = {Gender {{Shades}}},
  booktitle = {Proceedings of the 1st {{Conference}} on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  author = {Buolamwini, Joy and Gebru, Timnit},
  year = {2018},
  month = jan,
  pages = {77--91},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-06-14},
  abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
  langid = {english},
  file = {/data/Zotero/storage/3CUL36IN/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf;/data/Zotero/storage/3PWCP63R/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf;/data/Zotero/storage/F85DE77Q/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf;/data/Zotero/storage/KCU3WNQ7/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities.pdf}
}

@article{mamaryRaceGenderDisparities2018,
  title = {Race and {{Gender Disparities}} Are {{Evident}} in {{COPD Underdiagnoses Across}} All {{Severities}} of {{Measured Airflow Obstruction}}},
  author = {Mamary, A. James and Stewart, Jeffery I. and Kinney, Gregory L. and Hokanson, John E. and Shenoy, Kartik and Dransfield, Mark T. and Foreman, Marilyn G. and Vance, Gwendolyn B. and Criner, Gerard J. and {COPDGene\textregistered{} Investigators}},
  year = {2018},
  month = jul,
  journal = {Chronic Obstructive Pulmonary Diseases (Miami, Fla.)},
  volume = {5},
  number = {3},
  pages = {177--184},
  issn = {2372-952X},
  doi = {10.15326/jcopdf.5.3.2017.0145},
  abstract = {The COPD Genetic Epidemiology (COPDGene\textregistered ) study provides a rich cross-sectional dataset of patients with substantial tobacco smoke exposure, varied by race, gender, chronic obstructive pulmonary disease (COPD) diagnosis, and disease. We aimed to determine the influence of race, gender and Global initiative for chronic Obstructive Lung Disease (GOLD) stage on prevalence of prior COPD diagnosis at COPDGene\textregistered{} enrollment. Data from the complete phase 1 cohort of 10,192 participants were analyzed. Participants were non-Hispanic white and African-American, {$\geq$}45 years of age with a minimum of 10 pack years of cigarette smoking. Characterization upon enrollment included spirometry, demographics and history of COPD diagnosis determined by questionnaire. We evaluated the effects of race and gender on the likelihood of prior diagnosis of COPD and the interaction of race and GOLD stage, and gender and GOLD stage, as determined at study enrollment, on likelihood of prior diagnosis of COPD. We evaluated the 3-way interaction of race, gender and GOLD stage on prior diagnosis. African-Americans had higher odds of not having a prior COPD diagnosis at all GOLD stages of airflow obstruction versus non-Hispanic whites (p{$<$}0.0001). Women had higher odds of having a prior COPD diagnosis at all GOLD stages versus men (p{$<$}0.0001). Three-way interaction of race, gender and GOLD stage was not significant. African-Americans were less likely to have prior COPD regardless of the severity of airflow obstruction determined at study enrollment. Women were more likely to have a prior COPD diagnosis regardless of the severity of measured airflow obstruction. Race and gender are associated with significant disparities in COPD diagnosis.},
  langid = {english},
  pmcid = {PMC6296789},
  pmid = {30584581},
  keywords = {asthma,diagnosis,epidemiology,smokers,spirometry,tobacco},
  file = {/data/Zotero/storage/6RDKQGBW/Mamary et al. - 2018 - Race and Gender Disparities are Evident in COPD Un.pdf}
}

@inproceedings{ramaswamyFairAttributeClassification2021,
  title = {Fair {{Attribute Classification Through Latent Space De-Biasing}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ramaswamy, Vikram V and Kim, Sunnie S Y and Russakovsky, Olga},
  year = {2021},
  month = jun,
  pages = {9301--9310}
}

@article{subbaswamyDevelopmentDeploymentDataset2020,
  title = {From Development to Deployment: Dataset Shift, Causality, and Shift-Stable Models in Health {{AI}}},
  shorttitle = {From Development to Deployment},
  author = {Subbaswamy, Adarsh and Saria, Suchi},
  year = {2020},
  month = apr,
  journal = {Biostatistics},
  volume = {21},
  number = {2},
  pages = {345--352},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxz041},
  urldate = {2022-05-24},
  abstract = {The deployment of machine learning (ML) and statistical models is beginning to transform the practice of healthcare, with models now able to help clinicians diagnose conditions like pneumonia and skin cancer, and to predict which hospital patients are at risk of adverse events such as septic shock. A major concern, however, is that model performance is heavily tied to details particular to the dataset the model was developed on\textemdash even slight deviations from the training conditions can result in wildly different performance. For example, when researchers trained a model to diagnose pneumonia from chest X-rays using data from one health system, but evaluated on data from an external health system, they found the model performed significantly worse than it did internally (Zech and others, 2018). The model failed to generalize (i.e., predict accurately) due to the shifts between the training conditions (health system one) and the deployment/testing conditions (health system two). These shifts are very common when moving a model from the training phase to deployment and can take a variety of forms, including changes in patient demographics, disease prevalence, measurement timing, equipment, treatment patterns, and more. Beyond contributing to poor performance, failing to account for shifts can also lead to dangerous decisions in practice: the system can fail to diagnose severely ill patients or recommend harmful treatments. This problem of shifting conditions which prevent generalization is referred to as dataset shift (Qui\~nonero-Candela and others, 2009), and in this article, we explain what it is, why it occurs, give an overview of the types of existing solutions, and discuss open challenges that remain.},
  file = {/data/Zotero/storage/JNT77JUV/Subbaswamy and Saria - 2020 - From development to deployment dataset shift, cau.pdf;/data/Zotero/storage/ZR4EVDM9/5631850.html}
}

@article{liuMedicalAlgorithmicAudit2022,
  title = {The Medical Algorithmic Audit},
  author = {Liu, Xiaoxuan and Glocker, Ben and McCradden, Melissa M and Ghassemi, Marzyeh and Denniston, Alastair K and {Oakden-Rayner}, Lauren},
  year = {2022},
  month = may,
  journal = {The Lancet Digital Health},
  volume = {4},
  number = {5},
  pages = {e384-e397},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(22)00003-6},
  urldate = {2022-08-10},
  abstract = {Artificial intelligence systems for health care, like any other medical device, have the potential to fail. However, specific qualities of artificial intelligence systems, such as the tendency to learn spurious correlates in training data, poor generalisability to new deployment settings, and a paucity of reliable explainability mechanisms, mean they can yield unpredictable errors that might be entirely missed without proactive investigation. We propose a medical algorithmic audit framework that guides the auditor through a process of considering potential algorithmic errors in the context of a clinical task, mapping the components that might contribute to the occurrence of errors, and anticipating their potential consequences. We suggest several approaches for testing algorithmic errors, including exploratory error analysis, subgroup testing, and adversarial testing, and provide examples from our own work and previous studies. The medical algorithmic audit is a tool that can be used to better understand the weaknesses of an artificial intelligence system and put in place mechanisms to mitigate their impact. We propose that safety monitoring and medical algorithmic auditing should be a joint responsibility between users and developers, and encourage the use of feedback mechanisms between these groups to promote learning and maintain safe deployment of artificial intelligence systems.},
  langid = {english},
  file = {/data/Zotero/storage/LSAALCPJ/Liu et al. - 2022 - The medical algorithmic audit.pdf;/data/Zotero/storage/J3L2HG9C/S2589750022000036.html}
}

@article{wiensNoHarmRoadmap2019,
  title = {Do No Harm: A Roadmap for Responsible Machine Learning for Health Care},
  shorttitle = {Do No Harm},
  author = {Wiens, Jenna and Saria, Suchi and Sendak, Mark and Ghassemi, Marzyeh and Liu, Vincent X. and {Doshi-Velez}, Finale and Jung, Kenneth and Heller, Katherine and Kale, David and Saeed, Mohammed and Ossorio, Pilar N. and {Thadaney-Israni}, Sonoo and Goldenberg, Anna},
  year = {2019},
  month = sep,
  journal = {Nature Medicine},
  volume = {25},
  number = {9},
  pages = {1337--1340},
  publisher = {{Nature Publishing Group}},
  issn = {1546-170X},
  doi = {10.1038/s41591-019-0548-6},
  urldate = {2022-06-14},
  abstract = {Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).},
  copyright = {2019 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Health care,Research management,Scientific community},
  file = {/data/Zotero/storage/V57VML2C/Wiens et al. - 2019 - Do no harm a roadmap for responsible machine lear.pdf;/data/Zotero/storage/Q7W6JQHZ/s41591-019-0548-6.html}
}

@article{iglehartHealthInsurersMedicalImaging2009,
  title = {Health {{Insurers}} and {{Medical-Imaging Policy}} \textemdash{} {{A Work}} in {{Progress}}},
  author = {Iglehart, John K.},
  year = {2009},
  month = mar,
  journal = {New England Journal of Medicine},
  volume = {360},
  number = {10},
  pages = {1030--1037},
  publisher = {{Massachusetts Medical Society}},
  issn = {0028-4793},
  doi = {10.1056/NEJMhpr0808703},
  urldate = {2023-01-16},
  abstract = {The impressive strides that have been made in the field of advanced imaging technology have led to major enhancements in a physician's ability to diagnose a variety of diseases. However, since the use of and expenditures for imaging services have increased more rapidly than other physician-ordered services, without concomitant evidence of their value overall, government and private insurers have taken steps to slow the growth of their use. At the same time, in an effort to avert enactment of more stringent imaging policies by Medicare and more intrusive steps by private health plans, specialty organizations have accelerated their efforts to . . .},
  pmid = {19264694},
  file = {/data/Zotero/storage/63C9UUYM/Iglehart - 2009 - Health Insurers and Medical-Imaging Policy  A Wor.pdf}
}

@inproceedings{liREPAIRRemovingRepresentation2019,
  title = {{{REPAIR}}: {{Removing Representation Bias}} by {{Dataset Resampling}}},
  shorttitle = {{{REPAIR}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Yi and Vasconcelos, Nuno},
  year = {2019},
  month = jun,
  pages = {9564--9573},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00980},
  urldate = {2022-08-12},
  abstract = {Modern machine learning datasets can have biases for certain representations that are leveraged by algorithms to achieve high performance without learning to solve the underlying task. This problem is referred to as ``representation bias''. The question of how to reduce the representation biases of a dataset is investigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure is proposed. This formulates bias minimization as an optimization problem, seeking a weight distribution that penalizes examples easy for a classifier built on a given feature representation. Bias reduction is then equated to maximizing the ratio between the classification loss on the reweighted dataset and the uncertainty of the ground-truth class labels. This is a minimax problem that REPAIR solves by alternatingly updating classifier parameters and dataset resampling weights, using stochastic gradient descent. An experimental set-up is also introduced to measure the bias of any dataset for a given representation, and the impact of this bias on the performance of recognition models. Experiments with synthetic and action recognition data show that dataset REPAIR can significantly reduce representation bias, and lead to improved generalization of models trained on REPAIRed datasets. The tools used for characterizing representation bias, and the proposed dataset REPAIR algorithm, are available at https://github.com/ JerryYLi/Dataset-REPAIR/.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/data/Zotero/storage/AHQ3MKF4/Li and Vasconcelos - 2019 - REPAIR Removing Representation Bias by Dataset Re.pdf}
}

@article{wachterBiasPreservationMachine2021,
  title = {Bias Preservation in Machine Learning: The Legality of Fairness Metrics under {{EU}} Non-Discrimination Law},
  author = {Wachter, S and Mittelstadt, B and Russell, C},
  year = {2021},
  journal = {West Virginia Law Review},
  publisher = {{West Virginia University}}
}

@article{charImplementingMachineLearning2018,
  title = {Implementing {{Machine Learning}} in {{Health Care}} \textemdash{} {{Addressing Ethical Challenges}}},
  author = {Char, Danton S. and Shah, Nigam H. and Magnus, David},
  year = {2018},
  month = mar,
  journal = {The New England journal of medicine},
  volume = {378},
  number = {11},
  pages = {981--983},
  issn = {0028-4793},
  doi = {10.1056/NEJMp1714229},
  urldate = {2022-06-14},
  pmcid = {PMC5962261},
  pmid = {29539284},
  file = {/data/Zotero/storage/ZRNN98TA/Char et al. - 2018 - Implementing Machine Learning in Health Care  Add.pdf}
}

@article{hollandStatisticsCausalInference1986,
  title = {Statistics and {{Causal Inference}}},
  author = {Holland, Paul W.},
  year = {1986},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {81},
  number = {396},
  pages = {945--960},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1986.10478354},
  urldate = {2023-06-21},
  abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
  keywords = {Association,Causal effect,Causal model,Experiments,Granger causality,Hill's nine factors,Koch's postulates,Mill's methods,Path diagrams,Philosophy,Probabilistic causality},
  file = {/data/Zotero/storage/S952FXIM/Holland - 1986 - Statistics and Causal Inference.pdf}
}

@book{pearlCausalityModelsReasoning2011,
  title = {Causality: {{Models}}, Reasoning, and Inference, Second Edition},
  author = {Pearl, Judea},
  year = {2011},
  month = jan,
  journal = {Causality: Models, Reasoning, and Inference, Second Edition},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511803161},
  urldate = {2022-02-18},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  isbn = {978-0-511-80316-1}
}

@article{hernanStructuralApproachSelection2004,
  title = {A {{Structural Approach}} to {{Selection Bias}}},
  author = {Hern{\'a}n, Miguel A. and {Hern{\'a}ndez-D{\'i}az}, Sonia and Robins, James M.},
  year = {2004},
  journal = {Epidemiology},
  volume = {15},
  number = {5},
  eprint = {20485961},
  eprinttype = {jstor},
  pages = {615--625},
  publisher = {{Lippincott Williams \& Wilkins}},
  issn = {1044-3983},
  urldate = {2022-08-11},
  abstract = {The term "selection bias" encompasses various biases in epidemiology. We describe examples of selection bias in case-control studies (eg, inappropriate selection of controls) and cohort studies (eg, informative censoring). We argue that the causal structure underlying the bias in each example is essentially the same: conditioning on a common effect of 2 variables, one of which is either exposure or a cause of exposure and the other is either the outcome or a cause of the outcome. This structure is shared by other biases (eg, adjustment for variables affected by prior exposure). A structural classification of bias distinguishes between biases resulting from conditioning on common effects ("selection bias") and those resulting from the existence of common causes of exposure and outcome ("confounding"). This classification also leads to a unified approach to adjust for selection bias.},
  file = {/data/Zotero/storage/ISSXP4KK/Hernn et al. - 2004 - A Structural Approach to Selection Bias.pdf}
}

@misc{schrouffDiagnosingFailuresFairness2023,
  title = {Diagnosing Failures of Fairness Transfer across Distribution Shift in Real-World Medical Settings},
  author = {Schrouff, Jessica and Harris, Natalie and Koyejo, Oluwasanmi and Alabdulmohsin, Ibrahim and Schnider, Eva and {Opsahl-Ong}, Krista and Brown, Alex and Roy, Subhrajit and Mincu, Diana and Chen, Christina and Dieng, Awa and Liu, Yuan and Natarajan, Vivek and Karthikesalingam, Alan and Heller, Katherine and Chiappa, Silvia and D'Amour, Alexander},
  year = {2023},
  month = feb,
  number = {arXiv:2202.01034},
  eprint = {2202.01034},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-28},
  abstract = {Diagnosing and mitigating changes in model fairness under distribution shift is an important component of the safe deployment of machine learning in healthcare settings. Importantly, the success of any mitigation strategy strongly depends on the structure of the shift. Despite this, there has been little discussion of how to empirically assess the structure of a distribution shift that one is encountering in practice. In this work, we adopt a causal framing to motivate conditional independence tests as a key tool for characterizing distribution shifts. Using our approach in two medical applications, we show that this knowledge can help diagnose failures of fairness transfer, including cases where real-world shifts are more complex than is often assumed in the literature. Based on these results, we discuss potential remedies at each step of the machine learning pipeline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/data/Zotero/storage/VG7JI34Z/Schrouff et al. - 2023 - Diagnosing failures of fairness transfer across di.pdf;/data/Zotero/storage/6YJEF3MK/2202.html}
}

@article{gebruDatasheetsDatasets2021,
  title = {Datasheets for Datasets},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  year = {2021},
  month = dec,
  journal = {Communications of the ACM},
  volume = {64},
  number = {12},
  pages = {86--92},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3458723},
  urldate = {2023-04-20},
  abstract = {Documentation to facilitate communication between dataset creators and consumers.},
  langid = {english}
}

@inproceedings{wangBalancedDatasetsAre2019,
  title = {Balanced {{Datasets Are Not Enough}}: {{Estimating}} and {{Mitigating Gender Bias}} in {{Deep Image Representations}}},
  shorttitle = {Balanced {{Datasets Are Not Enough}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wang, Tianlu and Zhao, Jieyu and Yatskar, Mark and Chang, Kai-Wei and Ordonez, Vicente},
  year = {2019},
  month = oct,
  pages = {5309--5318},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00541},
  urldate = {2022-08-12},
  abstract = {In this work, we present a framework to measure and mitigate intrinsic biases with respect to protected variables \textendash such as gender\textendash{} in visual recognition tasks. We show that trained models significantly amplify the association of target labels with gender beyond what one would expect from biased datasets. Surprisingly, we show that even when datasets are balanced such that each label co-occurs equally with each gender, learned models amplify the association between labels and gender, as much as if data had not been balanced! To mitigate this, we adopt an adversarial approach to remove unwanted features corresponding to protected variables from intermediate representations in a deep neural network \textendash{} and provide a detailed analysis of its effectiveness. Experiments on two datasets: the COCO dataset (objects), and the imSitu dataset (actions), show reductions in gender bias amplification while maintaining most of the accuracy of the original models.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/data/Zotero/storage/MFAD2J84/Wang et al. - 2019 - Balanced Datasets Are Not Enough Estimating and M.pdf}
}

@inproceedings{jiangIdentifyingCorrectingLabel2020,
  title = {Identifying and {{Correcting Label Bias}} in {{Machine Learning}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Jiang, Heinrich and Nachum, Ofir},
  year = {2020},
  month = jun,
  pages = {702--712},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-07-17},
  abstract = {Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.},
  langid = {english},
  file = {/data/Zotero/storage/B4VGCLSQ/Jiang and Nachum - 2020 - Identifying and Correcting Label Bias in Machine L.pdf;/data/Zotero/storage/XMWLQ3F7/Jiang and Nachum - 2020 - Identifying and Correcting Label Bias in Machine L.pdf}
}

@incollection{seyyed-kalantariCheXclusionFairnessGaps2020,
  title = {{{CheXclusion}}: {{Fairness}} Gaps in Deep Chest {{X-ray}} Classifiers},
  shorttitle = {{{CheXclusion}}},
  booktitle = {Biocomputing 2021},
  author = {{Seyyed-Kalantari}, Laleh and Liu, Guanxiong and McDermott, Matthew and Chen, Irene Y. and Ghassemi, Marzyeh},
  year = {2020},
  month = oct,
  pages = {232--243},
  publisher = {{WORLD SCIENTIFIC}},
  doi = {10.1142/9789811232701\_0022},
  urldate = {2022-06-14},
  isbn = {9789811232695},
  keywords = {chest x-ray classifier,computer vision,fairness,medical imaging},
  file = {/data/Zotero/storage/FEZTXCRX/Seyyed-Kalantari et al. - 2020 - CheXclusion Fairness gaps in deep chest X-ray cla.pdf}
}

@inproceedings{zietlowLevelingComputerVision2022,
  title = {Leveling {{Down}} in {{Computer Vision}}: {{Pareto Inefficiencies}} in {{Fair Deep Classifiers}}},
  shorttitle = {Leveling {{Down}} in {{Computer Vision}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zietlow, Dominik and Lohaus, Michael and Balakrishnan, Guha and Kleindessner, Matth{\"a}us and Locatello, Francesco and Sch{\"o}lkopf, Bernhard and Russell, Chris},
  year = {2022},
  pages = {10410--10421},
  urldate = {2022-09-01},
  langid = {english},
  file = {/data/Zotero/storage/87KLK4AK/Zietlow et al. - 2022 - Leveling Down in Computer Vision Pareto Inefficie.pdf;/data/Zotero/storage/32PT8Q49/Zietlow_Leveling_Down_in_Computer_Vision_Pareto_Inefficiencies_in_Fair_Deep_CVPR_2022_paper.html}
}

@article{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {590--597},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.3301590},
  urldate = {2023-01-18},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/data/Zotero/storage/7NSZEGP5/Irvin et al. - 2019 - CheXpert A Large Chest Radiograph Dataset with Un.pdf;/data/Zotero/storage/SZQ5544L/Irvin et al. - 2019 - CheXpert A Large Chest Radiograph Dataset with Un.pdf}
}

@inproceedings{wangRacialFacesWild2019,
  title = {Racial {{Faces}} in the {{Wild}}: {{Reducing Racial Bias}} by {{Information Maximization Adaptation Network}}},
  shorttitle = {Racial {{Faces}} in the {{Wild}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Wang, Mei and Deng, Weihong and Hu, Jiani and Tao, Xunqiang and Huang, Yaohai},
  year = {2019},
  pages = {692--702},
  urldate = {2022-06-14},
  file = {/data/Zotero/storage/3RPAXFIT/Wang et al. - 2019 - Racial Faces in the Wild Reducing Racial Bias by .pdf;/data/Zotero/storage/EZRZU3FY/Wang_Racial_Faces_in_the_Wild_Reducing_Racial_Bias_by_Information_ICCV_2019_paper.html}
}

@article{obermeyerDissectingRacialBias2019,
  title = {Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations},
  author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  year = {2019},
  month = oct,
  journal = {Science},
  volume = {366},
  number = {6464},
  pages = {447--453},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aax2342},
  urldate = {2022-06-14},
  file = {/data/Zotero/storage/EUNXWWLV/Obermeyer et al. - 2019 - Dissecting racial bias in an algorithm used to man.pdf}
}

@inproceedings{noriega-camperoActiveFairnessAlgorithmic2019,
  title = {Active {{Fairness}} in {{Algorithmic Decision Making}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {{Noriega-Campero}, Alejandro and Bakker, Michiel A. and {Garcia-Bulle}, Bernardo and Pentland, Alex 'Sandy'},
  year = {2019},
  month = jan,
  series = {{{AIES}} '19},
  pages = {77--83},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3306618.3314277},
  urldate = {2023-07-24},
  abstract = {Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.},
  isbn = {978-1-4503-6324-2},
  keywords = {active feature acquisition,adaptive inquiry,algorithmic fairness},
  file = {/data/Zotero/storage/W2EHVB9M/Noriega-Campero et al. - 2019 - Active Fairness in Algorithmic Decision Making.pdf}
}

@article{jiangInvariantTransportableRepresentations2022,
  title = {Invariant and {{Transportable Representations}} for {{Anti-Causal Domain Shifts}}},
  author = {Jiang, Yibo and Veitch, Victor},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {20782--20794},
  urldate = {2023-07-26},
  langid = {english},
  file = {/data/Zotero/storage/FI3CSRHB/Jiang and Veitch - 2022 - Invariant and Transportable Representations for An.pdf}
}
@article{arjovskyInvariantRiskMinimization2019,
  title = {Invariant Risk Minimization},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2019},
  journal = {arXiv preprint arXiv:1907.02893},
  eprint = {1907.02893},
  archiveprefix = {arxiv}
}
@inproceedings{mccraddenWhatFairFair2023,
  title = {What's Fair Is\ldots{} Fair? {{Presenting JustEFAB}}, an Ethical Framework for Operationalizing Medical Ethics and Social Justice in the Integration of Clinical Machine Learning: {{JustEFAB}}},
  shorttitle = {What's Fair Is\ldots{} Fair?},
  booktitle = {Proceedings of the 2023 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mccradden, Melissa and Odusi, Oluwadara and Joshi, Shalmali and Akrout, Ismail and Ndlovu, Kagiso and Glocker, Ben and Maicas, Gabriel and Liu, Xiaoxuan and Mazwi, Mjaye and Garnett, Tee and {Oakden-Rayner}, Lauren and Alfred, Myrtede and Sihlahla, Irvine and Shafei, Oswa and Goldenberg, Anna},
  year = {2023},
  month = jun,
  series = {{{FAccT}} '23},
  pages = {1505--1519},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3593013.3594096},
  urldate = {2023-07-26},
  abstract = {The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.},
  isbn = {9798400701924},
  keywords = {accountability,algorithmic bias,clinical machine learning,ethics,fairness,health policy,healthcare,justice,organizational ethics,safe deployment},
  file = {/data/Zotero/storage/8E988YKJ/Mccradden et al. - 2023 - What's fair is fair Presenting JustEFAB, an ethi.pdf}
}
@inproceedings{vanbreugelDECAFGeneratingFair2021,
  title = {{{DECAF}}: {{Generating Fair Synthetic Data Using Causally-Aware Generative Networks}}},
  shorttitle = {{{DECAF}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{van Breugel}, Boris and Kyono, Trent and Berrevoets, Jeroen and {van der Schaar}, Mihaela},
  year = {2021},
  volume = {34},
  pages = {22221--22233},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-07-26},
  abstract = {Machine learning models have been criticized for reflecting unfair biases in the training data.  Instead of solving for this by introducing fair learning algorithms directly, we focus on generating fair synthetic data, such that any downstream learner is fair. Generating fair synthetic data from unfair data - while remaining truthful to the underlying data-generating process (DGP) - is non-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data generator for tabular data.  With DECAF we embed the DGP explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents.  This procedure enables inference time debiasing, where biased edges can be strategically removed for satisfying user-defined fairness requirements. The DECAF framework is versatile and compatible with several popular definitions of fairness. In our experiments, we show that DECAF successfully removes undesired bias and - in contrast to existing methods - is capable of generating high-quality synthetic data. Furthermore, we provide theoretical guarantees on the generator's convergence and the fairness of downstream models.},
  file = {/data/Zotero/storage/MB72VEWQ/van Breugel et al. - 2021 - DECAF Generating Fair Synthetic Data Using Causal.pdf}
}