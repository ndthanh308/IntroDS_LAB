{
  "title": "No Fair Lunch: A Causal Perspective on Dataset Bias in Machine Learning for Medical Imaging",
  "authors": [
    "Charles Jones",
    "Daniel C. Castro",
    "Fabio De Sousa Ribeiro",
    "Ozan Oktay",
    "Melissa McCradden",
    "Ben Glocker"
  ],
  "submission_date": "2023-07-31T09:48:32+00:00",
  "revised_dates": [],
  "abstract": "As machine learning methods gain prominence within clinical decision-making, addressing fairness concerns becomes increasingly urgent. Despite considerable work dedicated to detecting and ameliorating algorithmic bias, today's methods are deficient with potentially harmful consequences. Our causal perspective sheds new light on algorithmic bias, highlighting how different sources of dataset bias may appear indistinguishable yet require substantially different mitigation strategies. We introduce three families of causal bias mechanisms stemming from disparities in prevalence, presentation, and annotation. Our causal analysis underscores how current mitigation methods tackle only a narrow and often unrealistic subset of scenarios. We provide a practical three-step framework for reasoning about fairness in medical imaging, supporting the development of safe and equitable AI prediction models.",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16526",
  "pdf_url": "https://arxiv.org/pdf/2307.16526v1",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 1289611,
  "size_after_bytes": 1025994
}