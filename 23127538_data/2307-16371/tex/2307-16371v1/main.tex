
\documentclass[sigconf,nonacm]{acmart}
\makeatletter
\def\@ACM@checkaffil{% Only warnings
    \if@ACM@instpresent\else
    \ClassWarningNoLine{\@classname}{No institution present for an affiliation}%
    \fi
    \if@ACM@citypresent\else
    \ClassWarningNoLine{\@classname}{No city present for an affiliation}%
    \fi
    \if@ACM@countrypresent\else
        \ClassWarningNoLine{\@classname}{No country present for an affiliation}%
    \fi
}
\makeatother
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{amsmath}
\let\Bbbk\relax
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{float}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{enumitem}
\newcommand{\TODO}{\textcolor{red}}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{MobileVidFactory: Automatic Diffusion-Based Social Media Video Generation for Mobile Devices from Text}


\author{Junchen Zhu$^1$,\hspace{0.8em} Huan Yang$^2$*,\hspace{0.8em} Wenjing Wang$^2$,\hspace{0.8em} Huiguo He$^2$,\hspace{0.8em} Zixi Tuo$^2$,\hspace{0.8em} Yongsheng Yu$^3$, \hspace{0.8em} Wen-Huang Cheng$^4$,\hspace{0.8em} Lianli Gao$^1$,\hspace{0.8em} Jingkuan Song$^1$*,\hspace{0.8em} Jianlong Fu$^2$,\hspace{0.8em} Jiebo Luo$^3$}
\affiliation{%
 \vspace{0em}\institution{$^1$Center for Future Media, University of Electronic Science and Technology of China \hspace{0.3em} \\$^2$Microsoft Research \hspace{0.3em} $^3$University of Rochester \hspace{0.3em} $^4$National Taiwan University}
}
\affiliation{%
  \institution{junchen.zhu@hotmail.com \hspace{0.1em} \{huayan,v-wenjiwang,v-huiguohe,v-zixituo,jianf\}@microsoft.com \hspace{0.1em}yyu90@ur.rochester.edu}
}
\affiliation{%
  \institution{wenhuang@csie.ntu.edu.tw \hspace{0.1em} lianli.gao@uestc.edu.cn \hspace{0.1em} jingkuan.song@gmail.com \hspace{0.1em} jluo@cs.rochester.edu}
}

\renewcommand{\shortauthors}{Zhu et al.}

\begin{abstract}
Videos for mobile devices become the most popular access to share and acquire information recently. For the convenience of users' creation, in this paper, we present a system, namely \textbf{MobileVidFactory}, to automatically generate vertical mobile videos where users only need to give simple texts mainly. Our system consists of two parts: basic and customized generation. In the basic generation, we take advantage of the pretrained image diffusion model, and adapt it to a high-quality open-domain vertical video generator for mobile devices. As for the audio, by retrieving from our big database, our system matches a suitable background sound for the video. Additionally to produce customized content, our system allows users to add specified screen texts to the video for enriching visual expression, and specify texts for automatic reading with optional voices as they like.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003227.10003251.10003256</concept_id>
       <concept_desc>Information systems~Multimedia content creation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>

   <concept>
       <concept_id>10010147.10010178.10010224</concept_id>
       <concept_desc>Computing methodologies~Computer vision</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Multimedia content creation}
\ccsdesc[500]{Computing methodologies~Computer vision}
\keywords{Vertical Video Generation, Diffusion Model, Mobile Video}

\maketitle

% Figure environment removed

\let\thefootnote\relax\footnote{*Huan Yang and Jingkuan Song are the corresponding authors.}
\section{Introduction}
The rise of mobile devices, e.g., smartphones, and the exponential growth in social media has revolutionized the way we consume video content. A prominent shift in viewing habits has occurred, with vertical videos gaining significant traction among mobile users. This paper investigates the landscape of vertical mobile video generation, focusing on the distinct challenges, opportunities, and implications it presents for content creators and consumers alike. By exploring this evolving form of visual storytelling, we uncover the transformative potential it holds for the future of content consumption. However, creating such videos is not easy for everyone, as professional skills are always required from shooting to editing.

In this paper, we present a novel system \textbf{MobileVidFactory} to elegantly help users create their own videos for mobile devices. 
In summary, our system is leading in the following aspects:
\begin{itemize}
    \item[1)]
    Our system is the first automatic mobile-device video generation framework to the best of our knowledge, which can be used to generate high-quality, sounding, and vertical videos with simple text from users.
    \item[2)]
    Our system contains both deep networks to generate the basic visual and auditory content, and fixed modules to add customized content for users, where all components are controlled by text mainly. This makes our system meet different needs of users as much as possible and operate easily.
    \item[3)] 
    Our system is able to learn to synthesize open-domain high-quality vertical videos after finetuning with a few vertical video data, as we design an efficient training strategy to learn motion information from large-scale horizontal video datasets and migrate it losslessly to vertical video.
\end{itemize}



\section{System}
We aim to automatically generate vertical videos that can be directly used on mobile devices and can be easily controlled by simple texts from users.
As shown in Fig.\ref{fig:framework}, our system contains two generation processes: basic generation, which produces the elemental video and audio, and customized generation, which offers the option to add screen text and dubbing.

\subsection{Basic Generation}
In the basic generation, not like prior works~\cite{MMDiffusion,svgvqgan}, we independently produce the visual and auditory content. The video is synthesized using a diffusion model, and the audio is obtained through a retrieval model.
To generate high-quality vertical videos, we pretrain our model on a large-scale horizontal video dataset~\cite{webvid,VideoFactory}, and adapt it to the vertical generation using a few vertical data. Specifically, we firstly extend a pretrained image diffusion model~\cite{latentdiffusion,stablediffusion} to fit the spatial distribution (e.g., new aspect ratio and picture style) of the video dataset by adding learnable blocks in each level of the U-Net~\cite{unet} network, and only train these blocks while fixing all pretrained layers. 
Then, building upon previous works~\cite{MakeAVideo,VideoLDM}, we add new temporal layers, including temporal 1D residual blocks and transformer blocks~\cite{transformer}, and utilize the video dataset to train these additional layers~\cite{moviefactory}. 
After pretraining, we fix all layers except for the ones added to adjust the spatial distribution. We then finetune these spatial blocks to adapt to the vertical screens of mobile devices using some collected data. 
Additionally, we provide a frame interpolation module~\cite{AMT,TTVFI} to achieve smoother motion in certain cases.
To match vivid audio to the visual content, we adopt the previous work~\cite{OncescuKHAA21,Koepke21} as our retrieval model.
The network employs a text encoder and an audio encoder to encode text and audio data, respectively. It then evaluates the distance between the encoded features of multiple pairs to search for the most relevant sample. This retrieval model is trained using a contrastive ranking loss.
In our scenario, we directly use the textual content provided by users to retrieve the top 3 matching audios from the database. Users can then choose their favorite audio and select the most appropriate section to add to the video.

\subsection{Customized Generation}
In our system, we design two optional customized functions for users.
First, we support adding customized screen text to the video, as the addition of text to short mobile videos can provide context, clarify concepts, and enhance viewer engagement.
Text overlays also promote accessibility for individuals with hearing impairments and cater to diverse audiences. 
With the provided text to be shown, the specified font size and font color to use, and the designated position to place, our system directly overlays the text on top of the video for rendering.
Second, to add a personal touch, enhance storytelling, provide explanations, and foster a sense of authenticity, our system allows users to add dubbing to their videos. 
Using sentences from users, we utilize bark~\cite{bark} to transform the text into dubbing, known as text-to-speech (TTS). 
In this process, users can choose the voice they prefer, and various languages, such as English and Chinese, are well supported.

% Figure environment removed
\subsection{Demonstration}
We demonstrate some generated videos in Fig.\ref{fig:result}. Focusing on the vertical videos for mobile devices, our system can synthesize frames with abundant details and create compositions that highlight the subject. Additionally, smooth motion can be captured to describe vivid scenes. More samples are included in our video.

\section{Conclusion}
In this paper, we present MobileVidFactory, a system that automatically generates vertical videos for mobile devices using text inputs. 
The system consists of a visual generator that creates high-quality videos by leveraging a pretrained image diffusion model and a finetuning process. 
Users can enrich visual expression by adding specified texts. The audio generator matches suitable background sounds from a database and provides optional text-to-speech narration.


\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


\end{document}
\endinput

