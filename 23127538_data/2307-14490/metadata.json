{
  "title": "HUGE: Huge Unsupervised Graph Embeddings with TPUs",
  "authors": [
    "Brandon Mayer",
    "Anton Tsitsulin",
    "Hendrik Fichtenberger",
    "Jonathan Halcrow",
    "Bryan Perozzi"
  ],
  "submission_date": "2023-07-26T20:29:15+00:00",
  "revised_dates": [],
  "abstract": "Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.",
  "categories": [
    "cs.LG",
    "cs.DC",
    "cs.SI"
  ],
  "primary_category": "cs.LG",
  "doi": "10.1145/3580305.3599840",
  "journal_ref": null,
  "arxiv_id": "2307.14490",
  "pdf_url": null,
  "comment": "As appeared at KDD 2023",
  "num_versions": null,
  "size_before_bytes": 1607967,
  "size_after_bytes": 307485
}