\vspace{-0.1in}
\section{Events drive sentiment}
\label{sec:event_sentiment}
\vspace{-0.05in}

Redditors' sentiments on \starlinksubreddit{} are influenced by events. We systematically identify peaks of strong sentiment and automate the discovery of related events of interest. 

% Figure environment removed

For each day between Jan'$21$ and Dec'$22$, we analyze the sentiment of individual post content (text) using Azure's Cognitive Services.
The sentiment analysis service assigns $3$ different scores -- positive, negative, and neutral -- to each piece of text, which add up to $1$. We count the number of posts with strong positive ($\geq${}$0.7$) and negative ($\geq${}$0.7$) scores per day and plot them in Fig.~\ref{fig:sentiment_temporal}.

Our framework could discover sentiment peaks (day-wise bins), generate word clouds (using NLTK) across all posts and comments over a day for those peaks, and discover relevant news articles by searching online for the keywords (top $3$ uni-grams) appended with `Starlink' for the custom date. This pipeline enables the framework to annotate sentiment peaks with events that drive them. 

The top $3$ sentiment peaks, as shown in Fig.~\ref{fig:sentiment_temporal}, correspond to events of $3$ distinct flavors. On $9^{th}$ Feb'$21$, Redditors showed strong positive sentiment towards Starlink opening up pre-ordering of user terminals in the US, Canada, and UK~\cite{starlink_preorder}. On $24^{th}$ Nov'$21$, SpaceX's email~\cite{starlink_delay} to pre-order customers on delay in terminal delivery led to a negative sentiment peak. Fig.~\ref{fig:outage_wordcloud} shows the word cloud corresponding to the third highest peak ($22^{nd}$ Apr'$22$) which is driven by negative sentiment. The $3^{rd}$ most common word in the generated word cloud is `outage'. Interestingly, we could not find any relevant news though for this date, although Redditors from $14$ different countries (including $\sim${}$190$ reports from the US) confirmed an outage online. 

On further exploration, we could detect more such sentiment peaks which correspond to similar service outages, many of which were reported by the media~\cite{starlink_outage1,starlink_outage2,starlink_outage3}. Across such outages, negative sentiment (number of strongly negative posts and comments on outage as a fraction of all related posts and comments) varies by as much as $70\%$ depending on the nature of the outage. While some outages lasted for tens of minutes to hours, the one event that generated the most negative sentiment was on $10^{th}$ Mar'$21$ -- during the early beta-testing phase. Redditors reported~\cite{reddit_outage_thread_10Mar21} multiple short outages resulting in frequent call drops throughout the day, thus adding to their annoyance.

% Figure environment removed

% Figure environment removed

While the above approach (step $1$) is aimed at identifying sentiment peaks tied to various events, both networking and otherwise, we dive deeper and focus more on network service outages next (step $2$). We created a library of uni/bi-grams which are common in `outage' posts and comments identified following the above procedure. We manually verified that keywords in this library are meaningful in the specific context and got rid of outliers. We then identified threads with such keywords and negative sentiments (i.e., we filtered out threads with such keywords but positive or neutral sentiments) assuming outages to be `negative' events. Fig.~\ref{fig:outage_temporal} plots the day-wise frequencies of these keywords in such \textsl{negative} sentiment threads. $7^{th}$ Jan'$22$ and $30^{th}$ Aug'$22$ have the largest spikes of such keywords and correspond to large reported outages~\cite{starlink_outage7Jan22,starlink_outage30Aug22}. But more interestingly, there are numerous shorter peaks over time which correspond to local transient outages. Interestingly, in Fig.~\ref{fig:sentiment_temporal} we did not observe any sentiment peaks for the above $2$ dates when there were global outages because Redditors were also discussing other non-networking positive events that coincided with these network outages. The $2$-step process (leveraging specific keywords library and sentiment analysis) helps us zoom in on the particular event of interest -- network outage in this case.

OOkla's Downdetector~\cite{downdetector} also uses sentiment data from Twitter alongside direct reports, albeit the detailed methodology is not publicly available. Our framework, on the other hand, leverages a 2-step process, as detailed above, that could be used to analyze both networking and non-networking events of interest. Social platform-based methods could detect transient outages (probably due to network updates, misconfigurations, etc.) which might go undetected/unreported otherwise. In these initial days of LEO broadband offerings, understanding such transient downtimes is crucial in fixing performance bottlenecks and misconfigurations.

IODA~\cite{ioda_outage} relies on active probing and BGP data to detect Internet outages. While the methodology works well for the terrestrial Internet, it would be interesting to revisit the same in the context of LEO networks -- LEO terminals in a region might transiently have no satellites within the field of view and experience temporary downtime. We keep a deeper comparison with IODA for future work.
