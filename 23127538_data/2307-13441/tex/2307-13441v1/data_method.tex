%\vspace{-0.1in}
\section{Methodology}
\label{sec:methods}
%\vspace{-0.05in}

In this section, we briefly discuss the APIs and language and vision tools we use to collect, extract, and analyze publicly available information on the Reddit social platform. We also describe the data we analyze.

We used Reddit~\cite{reddit_api} and pushshift.io~\cite{pushshift_api} APIs to get all the post and comment data available on \starlinksubreddit{} for the entire period between Jan'$21$ and Dec'$22$ ($24$ months). We used the Python Reddit API Wrapper (PRAW)~\cite{praw} to access the Reddit APIs. While gathering the data, we cleaned it to get rid of user IDs and content that have been explicitly requested to be removed by users. The former step makes sure that our work does not raise any ethical concerns while the latter step conforms to data privacy guidelines. We only collected data available \texttt{publicly} on the Reddit platform.

We use the following language and computer vision tools for the analyses presented in this paper:
%\vspace{-0.1in}
\begin{itemize}
    \item Azure's Cognitive Services~\cite{azure_acs}: We use sentiment analysis and opinion mining APIs to quantify the sentiment of posts and comments. We also use ACS' optical character recognition (OCR) service to extract network measurement information from screenshots of speed-test reports shared by Redditors (Reddit users) on the platform.%\vspace{-0.1in}
    \item Natural Language Toolkit (NLTK)~\cite{nltk}: We use the libraries to filter out stop-words, extract $n$-grams, and create word clouds of posts and comments.
\end{itemize}

Rather than reinventing the wheel,
we rely on the already available language and vision tools. Our focus is on demonstrating how the capabilities of these tools could be leveraged to understand users' perceptions of networks better. The accuracy of our analyses depends on the accuracy of these tools.

% Figure environment removed

  
Nevertheless, we built a set of heuristics to extract network measurement metrics from the OCR output of semi-structured speed-test reports across all test providers. Fig.~\ref{fig:ookla_template} show two such templates from Ookla. Our relative pixel-distance-based heuristics could extract downlink and uplink speeds and latency (also jitter and packet loss rates, when available) along with the corresponding units, date and time information, network provider name, server location, etc. While we do not use speed-test location information in our current analyses, we could have used the server location as a proxy for client location, depending on the use case, as test providers like Ookla (present at $1$,$000+$ locations~\cite{ookla_server_sel}) usually pick a `closest'-on-the-network server for the measurements. The heuristics could parse all the different speed-test templates, including more complex table-structured ones (having multiple sub-reports together, like in Fig.~\ref{fig:ookla2}), which we could collect over the entire period. We discuss the related results in \S\ref{sec:bandwidth}.

While the readers already have a rough idea of the data by now, let us briefly list down the inputs to our analysis framework for clarity:
%\vspace{-0.05in}
\begin{itemize}
    \item Reddit posts: We gather post text, embedded URLs, photos, and videos, submission timestamp, post ID, and post metadata (number of comments and upvotes).%\vspace{-0.1in}
    \item Comments: We collect comment text, embedded URLs, photos, and videos, submission timestamp, comment ID, and metadata. We regenerate the comment tree for a post leveraging parent IDs in the comment metadata.%\vspace{-0.1in}
    \item Speed-test screenshots: Many people post screenshots of speed-tests (across providers) on online forums. We gather these screenshots for further analysis while getting post data.
\end{itemize}
\vspace{-0.1in}

