In this section, we estimate the latency reduction of our Board and Clerk scheme and compare it with existing protocols. The Hyper-Block Model is omitted since it is a pure gain of function that relies on heavier local computation. The main target of our experiment is to demonstrate how fast commit rule can affect the latency under different \textbf{TPS}, \textbf{number of nodes}, and \textbf{number of faulty nodes}. Since our scheme does not introduce communication overhead, we can estimate all these factors in a cheaper local testbed experiment. We have implemented Board and Clerk based on the code base of MystenLabs\cite{narwhal_code}. 

Our implementation solely modifies the local execution flow without introducing additional communication costs. Thus, we only perform local tests to evaluate the latency reduction achieved by our algorithm. Our Clerk algorithm follows the original Tusk to commit the block~\cite{Danezis_Kokoris-Kogias_Sonnino_Spiegelman_2022}. To compare the results, we analyze the performance of our algorithm against Tusk's and Bullshark's~\cite{Spiegelman_Giridharan_Sonnino_2022}. Bullshark is a variant of Tusk optimized for the partial synchronous setting.

The simulation was conducted on a desktop machine with an Intel i7-13700K CPU, 32GB DDR5 memory, and 2TB SSD storage. We test different TPS for a 4-node system, and Figure~\ref{fig_latency_tps} illustrates our latency reduction in both the asynchronous setting (top), where $\netdelay$ is unknown, and the partial-synchronous setting (bottom), where it is known and used as the timeout parameter. In Figure~\ref{fig_latency_tps}, the latency is measured by the end-to-end time between the client's submission of transactions and the node processing them and updating the state. The clerk (block marked) line shows the latency of transactions that are not fast-committed. The clerk-fc (plus marked) line shows the latency of fast-committed transactions.

% Figure environment removed

We draw two observations from this result: First, our algorithm keeps the regular commitment time compared to the original Tusk protocol. In addition, our fast commitment channel \textbf{reduces the latency by nearly 50\%}, outperforming even the Bullshark protocol in partially synchronous settings. We observe this 50\% latency reduction even when increasing the number of nodes. This reduction is near-optimal because the vote-based BFT consensus needs at least $3$ rounds. Given that Tusk's average latency is $7$ rounds, and our scheme only commits on odd rounds, 50\% latency reduction is near optimal. 

Second, our algorithm reaches the computation power bottleneck earlier than other protocols. It happens at an estimated 64K input TPS. This is attributed to the increased metadata handling and transaction commitment on Board. In Figure~\ref{fig_latency_tps}, this bottleneck manifests as a latency increase at the rightmost point for the orange and red curves. Despite this, our additional load yields substantial gains in reduced latency and check of external validity, which Tusk and Bullshark will accomplish afterward. We predict that realistic network delay and node capability make this bottleneck more challenging. Considering that TPS does not affect our metrics, the following tests used a load of 200 TPS to reduce the computation load. 

The fast-commitment rate, defined as the ratio of fast-committed transactions to all transactions, is another significant metric for our scheme. Although not depicted in Figure~\ref{fig_latency_tps}, the \textbf{fast-commitment rate remains steady} at approximately 50\% of all transactions until reaching the computational bottleneck. This outcome aligns with our expectation that the fast-commitment rate should correlate solely with the network topology, \ie, the committee size and number of faulty nodes.

The scalability test in Figure~\ref{fig_scale_nodes} offers an alternative perspective by varying the number of nodes. Generally, the fast-commitment rate positively correlates with the maximum $f$ and slightly negatively correlates with $N$. When $N\in \{4,5,6\}$, we have $f=1$, and the fast-commitment rate is around 50\% of all transactions. While $N\in \{7,8,9\}$, we have $f=2$, which greatly improves the fast-commitment rate to a level of around 70\% of all transactions. This is because these additional nodes in the system may not contribute to leader selection but still help to increase the number of transaction votes.

% Figure environment removed

The faulty node test depicted in Figure~\ref{fig_redundancy_gain} provides the most compelling evidence. In this test, we gradually shut down nodes within an 8-node system. It was observed that the fast-commitment rate gradually decreased in response to node shutdowns. Ultimately, the rate reached zero, indicating that no transactions could employ the fast commitment channel when nodes reach consensus every two rounds. We omit simulation for Byzantine faulty nodes. This is because the reliable broadcast protocol in the Narwhal memory pool will not accept proposals from byzantine nodes. Therefore, byzantine nodes are the same as crashed nodes for Board and Clerk.

% Figure environment removed

The scalability test shows that the latency of Clerk's formal commitment escalates more rapidly than that of the Tusk algorithm when scaling nodes. This outcome is anticipated, given that Board must manage an increased volume of votes and the corresponding counting process. It is also noteworthy that dummy transactions, which may be introduced due to concerns about Byzantine nodes, can further reduce the fast-commit latency as more proposals will include the transaction and increase the number of votes. Last, our Clerk implementation is a modification on Tusk for asynchronous settings. However, we can also use the partial synchronous setting and implement Clerk, based on Bullshark, reducing the latency of non-fast-committed transactions while keeping our fast-commitment channel advantages.

