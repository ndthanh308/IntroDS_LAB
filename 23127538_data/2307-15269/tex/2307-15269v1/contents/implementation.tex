We have implemented Board and Clerk based on the code base of MystenLabs\cite{narwhal_code}. The Hyper-Block Model is omitted since it is a pure gain of function that relies on heavier local computation.

Our implementation solely modifies the local execution flow without introducing additional communication. Thus, we only perform local tests to evaluate the latency reduction achieved by our algorithm. Our Clerk algorithm follows the original Tusk to commit the block~\cite{Danezis_Kokoris-Kogias_Sonnino_Spiegelman_2022}. To compare the results, we analyze the performance of our algorithm against, Tusk's and Bullshark's\cite{Spiegelman_Giridharan_Sonnino_2022}. Bullshark is a variant of Tusk optimized for the partial synchronous setting.

The simulation was conducted on a desktop machine with an Intel i7-13700K CPU, 32GB DDR5 memory, and 2TB SSD storage. We test different TPS for a 4-node system, and Figure~\ref{fig_latency_tps} illustrates our latency reduction in both the asynchronous setting (top), where $\netdelay$ is unknown, and the partial-synchronous setting (bottom), where it is known and used as the timeout parameter. In Figure~\ref{fig_latency_tps}, the latency is measured by the end-to-end time between the client's submission of transactions and the node processing them and updating the state. The clerk (block marked) line shows the latency of transactions that are not fast-committed. The clerk-fc (plus marked) line shows the latency of fast-committed transactions.

% Figure environment removed

We draw two observations from this result: (1) Our algorithm does not decrease the formal commitment time compared to the original Tusk protocol. Conversely, our fast commitment channel reduces the latency by nearly 50\%, outperforming even the Bullshark protocol in partially synchronous settings. Considering TPS does not affect our metrics, the following tests used 200 TPS load to reduce the computation load. (2) Our algorithm reaches the computation power bottleneck earlier than others, at an estimated 64K input TPS. This is attributed to the increased metadata handling and transaction commitment on Board. In Figure~\ref{fig_latency_tps}, this bottleneck manifests as a latency increase at the rightmost point for the orange and red curves. Despite this, our additional load yields substantial gains in reduced latency and check of external validity, which Tusk and Bullshark will accomplish afterward. On the other hand, we predict that realistic network delay and node capability will make this bottleneck challenging to reach.

The fast-commitment rate, defined as the ratio of fast-committed transactions to all transactions, is another significant metric for our scheme. Although not depicted in Figure~\ref{fig_latency_tps}, the fast-commitment rate remains steady at approximately 50\% until reaching the computational bottleneck. This outcome aligns with our expectation that the fast-commitment rate should correlate solely with the network topology (i.e., the committee size and number of faulty nodes).

The scalability test in Figure~\ref{fig_scale_nodes} offers an alternative perspective by varying the number of nodes. Generally, the fast-commitment rate positively correlates with $\frac{f}{N}$. when $N\in \{4,5,6\}$ we have $f=1$, while $N\in \{7,8,9\}$, we have $f=2$ which greatly improves the fast-commitment rate. This is because two additional nodes in the system may not contribute to the consensus due to the DAG definition, but their votes still contribute to the fast-commitment process.

% Figure environment removed

The faulty node test depicted in Figure~\ref{fig_redundancy_gain} provides the most compelling evidence. In this test, we gradually shut down nodes within an 8-node system. It was observed that the fast-commitment rate gradually decreased in response to node shutdowns. Ultimately, the rate reached zero, indicating that no transactions could employ the fast commitment channel when nodes reach consensus every two rounds.

% Figure environment removed

The scalability test shows that the latency of Clerk's formal commitment escalates more rapidly than that of the Tusk algorithm when scaling nodes. This outcome is anticipated, given that Board must manage an increased volume of votes and the corresponding counting process. It is also noteworthy that dummy transactions, which may be introduced due to concerns about Byzantine nodes, can further reduce the fast-commit latency as more proposals will include the transaction and increase the number of votes. Last, our Clerk implementation is a modification on Tusk for asynchronous settings. However, we can also use the partial synchronous setting and implement Clerk, based on Bullshark, reducing the latency of non-fast-committed transactions while keeping our fast-commitment channel advantages.

