This section surveys some of the common concepts invented 
by the human factors community 
 to model, explain and predict the outcomes of 
human-machine systems. 


First we examine some of the latent states of a human  
such as workload, and situational awareness.  These latent states are useful for predicting 
performance of a HMS but may not 
have any singular physiological manifestation in the human. 
Due to the complexity of a ``human system'', these latent variables 
cannot be measured directly nor are they simple functions of measurable physiological indicators.  

\subsection{Workload} 
\label{subsec:workload}
Mental workload is an extensively researched topic in human factors~\cite{moray2013mental}.  It stems from the study of attention and information processing by cognitive psychologists~\cite{navon1979economy}.  Mental workload is conceived to be a multi-dimensional construct of time stress, mental effort and psychological stress~\cite{sheridan1979toward,gopher1986workload,reid1988subjective} that is independent of behavior and performance~\cite{yeh1988dissociation}.  There is no commonly accepted formal definition of workload, however generally speaking, workload is often defined in terms of a relation between the mental resources required by a task and the available mental resources that are supplied by the human.  In~\cite{navon1979economy}, it is noted that every human has a bounded set of resources available at any given time.  Early findings in the literature established that this pool of resources was singular and undifferentiated~\cite{gopher1984psychophysics}. However the prevailing current theory of workload is that the pool of resources is also multi-dimensional, meaning that it is divided into different capacities~\cite{wickens1980processing,jex1988measuring,Wic91}. The idea of different capacities of resources was introduced in earlier works such as~\cite{kahneman1973attention} and eventually developed into what is known as the multiple resource theory (MRT)~\cite{wickens1991processing}.  In MRT, the human operator has separate resource blocks that span multiple dimensions of input modalities, cognitive stages, and code processing.  In the modalities dimension, we find the visual, auditory, tactile and olfactory blocks. In the stages dimension, there are the perception, processing, and action blocks.  Some empirical evidence providing the separation between perception, processing and action resources can be found in~\cite{isreal1980p300}. In the codes dimension, there are the symbolic (spatial) and the linguistic (verbal) blocks.  The codes dimension is based on the hemispheric laterality framework where some tasks are exclusively executed by a certain hemisphere of the brain (the left brain versus right brain paradigm).  In MRT, the degradation of the human performance is simply the result of overloading in any of the individual resource blocks due to the performance of one or more tasks. Using the MRT model, the phenomenon of cross-task interference can be measured by quantifying the amount and the type of resources taken up by each of the tasks. 

Other workload theories in the literature can be interpreted as a special instance of the multiple resource theory either by using a hemispheric dimension in~\cite{polson1988task,friedman1981hemispheres}, 
by expanding one dimension into other sub-dimensions in~\cite{boles1998simultaneous}, 
or by restricting to only one dimension in~\cite{pashler1994dual}.  
The response selection bottleneck theory of Pashler~\cite{pashler1994dual} is an example of 
multiple resource theory that captures concurrent task interference only within the stages dimension.  
In Pashler's model, the interference between the two tasks is total, meaning that the response selection portion (processing stage) 
of the task acts like a semaphore which uses all of the available resources of the human operator. 
Underlying the bottleneck theory is the theory of psychological refractory period (PRP)~\cite{smith1967theories} which establishes that the human operator's ability to respond to external stimuli is completely 
degraded during a certain period right after it has received an earlier stimulus. 
The PRP phenomenon is captured in the perceptual modality within the EPIC cognitive architecture~\cite{Mey97}. 

Workload is difficult to measure as it is context and operator-dependent.  Disassociations can occur frequently between the different measures of workload~\cite{yeh1988dissociation}. Workload varies based on many factors such as individual's capabilities,  environment factors, and the difficulty, type, timing, and criticality of the task~\cite{meshkati1988toward}. There are 3 predominant basis to measure workload: performance, subjective evaluation ratings, and psycho-physiological measurements. Inferring workload using task performance measurements can be problematic. There is experimental evidence that low workload leads to degraded performance~\cite{young2002malleable}.  The latter can be caused by, for example, the induced boredom in the human operator, which has an effect on human vigilance~\cite{sawin1995effects}. 
It has been noted that more often than not, task performance fails to reflect the resource re-allocation prompted by the changes in the task demands~\cite{gopher1986workload}.  For example, the human operator frequently adjusts its effort in response to more challenging tasks. 
Workload measures can disassociate from performance measures~\cite{yeh1988dissociation} for either simple or challenging tasks.  Nonetheless performance-based measures of either primary or secondary tasks is a widely-used approach to evaluate workload and human-machine system performance in various contexts~\cite{eggemeier1991performance,hart1990workload}. 
It was noted in~\cite{meshkati1988eclectic} that different performance metrics across different tasks make standardization of a performance-based workload measure difficult across domains.  Subjective evaluation techniques such as SWAT~\cite{reid1988subjective} or the NASA-TLX (task load index)~\cite{hart1988development} are methods that rely on self-reported ratings from operators, and/or observers. This is the most common technique used for the measurement of workload.  There are both one-dimensional and multi-dimensional workload rating scales in the literature. The latter typically aggregates the individual ratings into a single final workload value using either pair-wise weightings~\cite{hart1988development} or conjoint analysis~\cite{reid1988subjective}. The multi-dimensional scaled ratings are based on the assumption that the operator can evaluate the individual components of the workload better than the overall workload. Finally, the psycho-physiological techniques~\cite{dussault2005eeg,iqbal2004task,jorna1992spectral,may1990eye,vicente1987spectral} attempt to quantify workload by interpreting the measures of physical and neurological cues such as heart rate, EEG output, eye movements, pupil response, respiration rate, blood pressure, and sinus abnormalities. 


\subsection{Situational Awareness}
\label{subsec:sa} 

\als{Situation awareness (SA)}{We place SA in this section III where we talk about latent states. Is SA really not measurable?} is defined by Endsley in~\cite{End00} as ``knowing what is going on around you'' with an inherent notion of knowing what is important. 
Many other definitions of SA have been developed either in the domain of
aviation or more general applications.  A good survey of SA definitions
relating to aircraft piloting can be found in~\cite{Dom94}.  Endsley gave a detailed descriptive model of SA in~\cite{End95} as having a hierarchical structure, with the first level being perception, the second level being  comprehension, and the third level being projection.  
The last level captures the capability of the human operator
to not only being able to comprehend the current state of the situation, denoted by $x(t)$, but also to understand what is going to happened in the future, denoted by $x(t+\Delta)$.
In the Endsley model, SA is a separate stage from decision making
because it represents the construction of the human operator's internal model
of the external world.  Any decisions that the operator made in constructing
this internal world has no impact on the external world. Moreover it has been
observed that human operators can make poor decisions even during a state of
excellent situational awareness.



\subsection{Vigilance, Complacency and Trust} 
\label{subsec:folk}

The notion of complacency is mentioned frequently in the aviation human factors literature~\cite{billings1984human} and the term has long been absorbed into the vernacular of aviation accident reports~\cite{ntsb_aar121,funk1999flight} as a causal factor.  In a survey of leading airline captains~\cite{wiener1981complacency}, more than half thought that complacency was the leading cause of aviation accidents. The notion is considered by some as controversial.  It is criticized, along with other concepts such as SA, mental workload, and trust  as ``folk'' models in~\cite{dekker2004human} with little underlying scientific basis and more common sense appeal. A counter-argument is given in~\cite{parasuraman2008situation} where Parasuraman et al. provided a literature survey supporting these notions with an extensive data-base of empirical evidences, computational models and psychological foundations supporting SA, mental workload and trust. 


Wiener in~\cite{wiener1981complacency} discussed complacency as a low level of suspicion but suggested more empirical research to understand any underlying mechanisms behind complacency.  In~\cite{parasuraman1993performance}, complacency is a term used to explain the higher rate of mis-detections of automation failure when the reliability of automation is increased.  While there is no consensus on the exact definition of complacency, it is suggested in~\cite{parasuraman2010complacency} that a working definition of complacency could be derived from the features of complacency definitions common to both aviation accidental analyses and human performance studies.  This set of features include human monitoring of automation, a sub-optimal frequency of monitoring of the automation~\cite{moray2000attention}, and finally observable degradation in system performance due to the sub-optimal monitoring.  A conceptual model of complacency is given~\cite{parasuraman2010complacency}.  In this model, complacency is a dynamic variable that varies overtime through interactions with the automation, and affects the attention allocation of the operator, which in turns may or may not lead to errors in the operation of machine. 

Vigilance can be interpreted as the lack of complacency~\cite{parasuraman1993performance}. The notion of vigilance is usually defined, 
in the human factors literature as the ability of the human operator to focus attention on a signal source 
for extended period of time while remaining alert to a particular signal~\cite{hancock2017nature}.  
The study of vigilance started with Mackworth's work in World War 2 to understand why sonar operators miss certain weak signals indicating the presence of submarines at the end of their watch.  
A review of recent research on vigilance in~\cite{warm2008vigilance} shows, through various subjective reports and more objective physiological measure of blood flow within the brain, that maintaining vigilance require high workload, and that a decrease in vigilance over extended period of time can be explained by the attentional resource theory of MRT.  

Trust is often defined in the various human sciences as some expectation by the human of certain outcome behavior.  As stated by the Harvard sociologist Bernard Barber in~\cite{barber1983logic}, it is the ``expectation of technically role performance.''  Some of the earliest works on human trust in the automation domain can be found in~\cite{sheridan1984research,muir1987trust} where trust was raised as a factor in the context of modeling the supervisory control of computer or automation.
In these early works, the main hypothesis is that trust has an impact on the supervisor's intervention behavior, and the quality of the automation was raised as a factor in affecting  trust. 
Some early conceptual models of human trust in automation performing continuous process control include~\cite{lee1992trust,lee1994trust, muir1994trust} which related the various factors including the use and mis-use of automation, the trust of the operator in the automation, and the competence of the automation.  The authors in~\cite{muir1994trust} validated their conceptual model with two experiments, one qualitative and the other quantitative, involving an automatic process control~\cite{muir1996trust}.  The experiments showed that trust can be eroded by  temporary automation errors that have no consequence on the long term operation of the system, and it also showed that trust is not a binary variable but rather that there are multiple and possibly continuous levels of trust. The work in~\cite{lee1994trust} provided a mathematical model of how trust and confidence relate to the performance of the human-machine system. 
A contextually focused approach was taken in~\cite{cohen1998trust} which produced a relatively early computational model of trust. In~\cite{cohen1998trust}, trust is qualitatively  a ``context-specific'' argument against the performance of the system, and quantitatively a probability distribution of the correctness of the system actions given the system and the context or situation. In this framework, the problem of the acceptance of the output of decision support tools is not due to some steady state notion of under or over-trust but rather to a context-specific, transient notion of ``inappropriate trust'' in which there is a failure by the human operator to understand the conditions leading to either good or bad performance of the decision support tools. 


\subsection{Mode Confusions}  

\als{Mode confusion}{We add mode confusion to this section, but again it seems to me that we can measure mode confusion (just run an experiment where you keep track of the external world and then ask questions to the human about it). Should this subsection just be absorbed into others that talk about cognition? After all, this is a cognitive "error".} is the result of various gaps between the operator's 
internal world model of the machine, and the actual state of the machine.  
For a rigorous definition of mode confusion, 
one can refer to the work of Bredereke and Lankenau~\cite{bredereke2002rigorous} 
who also provided a formal model of mode confusion in~\cite{bredereke2005safety}.
Leveson et al. in~\cite{leveson1997analyzing} discussed some of the common design flaws which 
can lead to mode confusion such as interface interpretation error (the 
user does not interpret the interface purpose correctly),  
indirect mode changes (automation mode changes that occur without explicit command from the user), 
inconsistent behaviors (automation exhibits the same behavior in many modes except for one or few),
and lack of appropriate feedback (automation does not provide the right visual/aural indications to the user that the mode has changed). 
    
\tim{add more work on mode confusion in aviation}


\subsection{Human Errors} 

\als{The main approach to modeling human errors}{Same here. We can measure errors perhaps?} has been to use the phenotypical types as 
described by Hollnagel in~\cite{hollnagel1993phenotype}. 
The increase in reliability of aircraft systems have in turn 
highlighted cockpit crew error as a major cause of aviation incidents in recent years.   
A comprehensive framework that discusses human errors in aviation can be found in~\cite{wiegmann2001applying}.  The taxonomy framework, known as Human Factors Analysis and Classification System (HFACS), has its theoretical groundings in Reason's swiss-cheese model of human errors~\cite{reason1990human}, and groups human errors in aviation into 19 causal categories ranging from crew resource management errors~\cite{helmreich2000error} to individual errors in decision-making, perception and the execution of skill-based flying.  Navigation errors during surface or runway operations, which are defined in~\cite{hooey2001post} as when the aircraft has moved into an uncleared taxiway or when the aircraft has deviated from the centerline in a cleared taxiway, have been modeled in D-OMAR~\cite{deutsch2002modeling} and ACT-R~\cite{byrne2005using}.  In the former, navigation errors are captured in the model as the result of an unconscious winner-take-all selection of the wrong ``intention-to-act'', where each ``intention-to-act'' is some pre-generated cognitive plan to be executed. In the latter, the errors are revealed as the result of complex interactions between the cognitive model of the pilot, co-pilot, air traffic controller, and the model of the airport environment.
Cockpit task management (CTM) errors occur when pilots initiate, monitor, prioritize and terminate concurrent cockpit tasks. The increase in automation of aircraft systems has resulted in a corresponding increase in CTM errors as a factor in aviation incidents. Empirical studies in~\cite{chou1996studies} show that CTM errors occurred in nearly 23\% of aviation incidents surveyed in the work. 

\subsection{Synthesis}
See Table~\ref{tab:summary2}

\begin{table*}
    \caption{Summary}
    \centering
\begin{tabular}{ |p{3cm}||c|c|c|c|c| }
    \hline
	Model & Property  & Probalistic & Temporal & Computational & Composability  \\
	\hline
    Endsley 1 & SA & no & no & yes & no \\
	\hline
    Endsley 2 & SA & no & yes & no & no\\
	\hline
\end{tabular}
\label{tab:summary2}
\end{table*}