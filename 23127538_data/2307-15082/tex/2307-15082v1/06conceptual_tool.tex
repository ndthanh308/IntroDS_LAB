
%We refrain from providing a definition of autonomy and intelligence which is not needed for this discussion. Several attempts have been made to define level of autonomy (see for example ~\cite{sae2014taxonomy}), but these definitions always lead to controversy. 
%For the purpose of this discussion, we only acknowledge that 




Software and hardware are becoming more complex and capable.  Current trends in autonomy 
points to a future in which complex functions such as 
object detection, tactical decision making and strategic planning that are currently reliably 
performed by human operators are going to be allocated to the machines instead.  
Existing models of humans and machines may no longer be relevant for design and  
V\&V of reliable human-autonomy systems. 
Particularly as machine exhibits more human-like traits such as using knowledge to make 
complex decisions, there emerge a need to 
reason about not only about what the system knows, but also what the human operator knows about the system, and what the system knows about what the human operator knows about the system, and so on. 

Furthermore,  the expected increase in the complexity of human-autonomy systems makes testing and simulation less effective, which suggests a need for 
formal modeling, verification and validation tools and techniques such as \emph{formal methods}~\cite{Peled}.  
Applying formal methods leads to strong guarantees of safety and/or performance, 
but a trade-off exist in which higher fidelity models under analysis 
could easily lead to tractability problems. 
To mitigate this tradeoff, appropriate abstraction levels need to be defined, and compositional verification 
need to be effectively utilized. 
Those techniques become more and more important as the system 
scale to teams of multiple humans, multiple autonomous machines, and many missions.   

% For controlled or automatic systems, the interface between the human an the machine is at the level of a set of vivid facts from sensors, and a finite set of commands that act more or less directly on actuators. In this case, it is possible to create a good abstraction of the machine that typically has few modes of operation. Most mode switching are assigned to the human. Not only the software, as complex as it might seem, can be abstracted into a simpler finite state machine, but the state is also directly observable by the human through the interface since in most cases the human is either co-located with the machine or can rely on a dedicated communication link over which vivid facts are relayed to the interface. Even a mild increase in automation may induce operational complexity leading to problems such as mode confusion \cite{rushby2002using}. As the level of autonomy drastically increases, and perhaps the same human operator is asked to command a fleet of vehicles, we can only expect these kinds of issues to become even more complex. 

% Thus, the subject of modeling will need to expand towards higher levels of cognition. The interest will no longer be on modeling skills such as pointing and clicking on a display, paying attention to the right areas of a display, or making simple yes/no decisions, but rather on modeling the cognitive processes of the human to assess and track a complex state that she does not directly observe, but that is communicated after assessment by the autonomous system. Similarly, humans do not provide direct commands to actuators, but rather goals to the system which then computes and executes plans, changing the environment as a function  of their objectives and cost functions. The number of states (or modes) in of the system cannot be easily abstracted and the humans must have a deeper understanding of how the machine operates and why it makes certain decision  (or how it assesses the environment). In this case, the chances for misunderstanding dramatically increase. There is a need to model what the human knows about the machine knowledge of the world. 

% We can no-longer treat the machine as being programmed to execute certain simple routines with well-defined inputs, outputs and outcomes. By design, the machine will need to interact with the human to align mental models, to report the current situational assessment results in a concise and understandable way, and to share intentions. Thus, the machine itself must include a model of the human operator. There is, therefore, a need not only for modeling humans for the purpose of verification and validation, but also for operations. Clearly, the autonomous system must be able to reason about what the human believes that the machine knows. This is a higher level of uncertainty, or belief over belief, which demands a different kind of modeling. 

% New research is needed in the area of workload estimation. We foresee the cognitive load to be predominant when humans interact with intelligent machines. The very same notion of task is different. Human will need to execute tasks that are more about strategic thinking than piloting or following checklists. This requires not only a new class of models to be developed for estimating workloads, but also new models of performance, especially models of accuracy or precision in the execution of a task. 

% Lastly, while the complexity of models increases, verification and validation complexity also does. Appropriate abstraction levels need to be defined, and compositional verification methods should be employed. This is important as we scale to teams of multiple humans, multiple machines, and many tasks.  

% We believe that there is a need for a
We introduce an experimental human-machine modeling tool based on an 
unified modeling approach for both human operators and machines with the following features: 
 \begin{itemize}
\item Contracts with a formal declarative specification language for compositional modeling and verification of the human-machine system.    
\item Epistemic modality for capturing and 
reasoning about the beliefs and knowledge of the system as they both drive actions.
 \item Stochastic modality for capturing and reasoning about uncertainties in the system.  
 \item Temporal modality for capturing and reasoning about evolution of behaviors in time.  
 \end{itemize} 
 
In declarative modeling~\cite{kowalski1979algorithm,fahland2009declarative}, one specifies what the program does without prescribing the details of the implementation of the program i.e. how the program does it.  
Declarative modeling enables us to provide a description of the capabilities of the human operator rather than details behind the mechanism which implements those capabilities. 
Declarative modeling can be coupled with contract-based design (CBD) \cite{benveniste2018contracts} that support compositional verification of systems.  

In CBD, each component is modeled using an assume-guarantee abstraction. A set of components can be composed to yield a new component contract. Components are ordered by a refinement relation that indicates whether one component can be replaced by another in all contexts where the former works. Refinement checking is problem of verifying when a contract is refinement by another contract. For example, a high-level task can be decomposed into a set of sub-tasks, and refinement checking in this case verifies that the composition of the sub-tasks (itself a contract) can be executed in all environment where the original task can, and achieves the same goals. Once refinement is verified, the high-level task (which has hopefully a simpler description than its decomposition) can be used in place of the decomposition to verify system level properties. One difficulty facing compositional modeling of human operators is that workload measures of tasks are difficult to compose. Typically, getting the workload of two or more tasks being performed concurrently requires re-collecting workload ratings for the combination of tasks being performed. Multiple-resource models provide a somewhat limited computational way of composing workloads. However, those compositional techniques are still experimental and have not been extensively validated. 

Modeling knowledge and higher order of belief can be achieved using the framework provided by epistemic logic \cite{fagin2004reasoning}. This framework has been used, for instance, in \cite{fisher2013verifying}. Models of humans should be able to describe the state of knowledge which includes what the human knows about the knowledge (not the vivid facts) of the machine. There are several tools that have appeared in the area of verification of epistemic dynamic systems (see for example \cite{vcermak2014mcmas}) that could be used to perform formal verification. However, the problem is in modeling the epistemic state in a declarative way. Moreover, issues such as awareness \cite{fagin1987belief,sim1997epistemic} has to be taken into account, and modeling techniques and verification tools for modeling awareness seem to be lacking. Furthermore, the declarative modeling language should mix epistemic modalities with several others including temporal and probabilistic ones, leading to verification problems that become intractable. Research in both areas of modeling and decision procedures is needed to enable formal verification of these new class of models. 
