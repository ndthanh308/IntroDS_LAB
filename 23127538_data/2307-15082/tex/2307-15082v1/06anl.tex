As more functions, traditionally under the responsibility of a pilot, are allocated to the on-board system, the modeling and verification problem shifts from the design of interfaces to human-machine teaming. 
%We refrain from providing a definition of autonomy and intelligence which is not needed for this discussion. Several attempts have been made to define level of autonomy (see for example ~\cite{sae2014taxonomy}), but these definitions always lead to controversy. 
%For the purpose of this discussion, we only acknowledge that 
Software and hardware are becoming more complex and capable, and, by leveraging advances in Artificial Intelligence and computing power, they are taking responsibility for complex tasks such as object identification and classification, decision-making under uncertainty, and strategic planning. It is likely, therefore, that existing models of humans and machines are no longer sufficiently rich to capture the new types of interaction issues that can arise. In particular, models for both humans and machines tend to converge as systems exhibits more human-like traits.

Furthermore,  drastic increase in the complexity of the on-board algorithms makes testing and simulation less effective, which suggests the need for other ways to conduct verification and validation such as formal methods. Formal methods can be exhaustive, but they require modeling the system at a certain level of fidelity (the level needed to prove the satisfaction of critical requirements). In addition, both human and machines are non-deterministic systems due to learning and on-line adaptation 
which goes against the traditional philosophy of off-line V\&V and certification. The dynamics of evolution of the human-machine system should be modeled as well.

For controlled or automatic systems, the interface between the human an the machine is at the level of a set of vivid facts from sensors, and a finite set of commands that act more or less directly on actuators. In this case, it is possible to create a good abstraction of the machine that typically has few modes of operation. Most mode switching are assigned to the human. Not only the software, as complex as it might seem, can be abstracted into a simpler finite state machine, but the state is also directly observable by the human through the interface since in most cases the human is either co-located with the machine or can rely on a dedicated communication link over which vivid facts are relayed to the interface. Even a mild increase in automation may induce operational complexity leading to problems such as mode confusion \cite{rushby2002using}. As the level of autonomy drastically increases, and perhaps the same human operator is asked to command a fleet of vehicles, we can only expect these kinds of issues to become even more complex. 

Thus, the subject of modeling will need to expand towards higher levels of cognition. The interest will no longer be on modeling skills such as pointing and clicking on a display, paying attention to the right areas of a display, or making simple yes/no decisions, but rather on modeling the cognitive processes of the human to assess and track a complex state that she does not directly observe, but that is communicated after assessment by the autonomous system. Similarly, humans do not provide direct commands to actuators, but rather goals to the system which then computes and executes plans, changing the environment as a function  of their objectives and cost functions. The number of states (or modes) in of the system cannot be easily abstracted and the humans must have a deeper understanding of how the machine operates and why it makes certain decision  (or how it assesses the environment). In this case, the chances for misunderstanding dramatically increase. There is a need to model what the human knows about the machine knowledge of the world. 

We can no-longer treat the machine as being programmed to execute certain simple routines with well-defined inputs, outputs and outcomes. By design, the machine will need to interact with the human to align mental models, to report the current situational assessment results in a concise and understandable way, and to share intentions. Thus, the machine itself must include a model of the human operator. There is, therefore, a need not only for modeling humans for the purpose of verification and validation, but also for operations. Clearly, the autonomous system must be able to reason about what the human believes that the machine knows. This is a higher level of uncertainty, or belief over belief, which demands a different kind of modeling. 

New research is needed in the area of workload estimation. We foresee the cognitive load to be predominant when humans interact with intelligent machines. The very same notion of task is different. Human will need to execute tasks that are more about strategic thinking than piloting or following checklists. This requires not only a new class of models to be developed for estimating workloads, but also new models of performance, especially models of accuracy or precision in the execution of a task. 

Lastly, while the complexity of models increases, verification and validation complexity also does. Appropriate abstraction levels need to be defined, and compositional verification methods should be employed. This is important as we scale to teams of multiple humans, multiple machines, and many tasks.  

We believe that there is a need for a
general unified modeling approach for both human operators and machines with the following characteristics: 
 \begin{itemize}
 \item It should be declarative to model and formally reason about the ``what'' rather than the ``how''. It should be compositional, not only to enable scalability of the V\&V process, but also to allow adding capabilities as the on-board autonomy and training curricula evolve without incurring in the possibility of invalidating properties that the system already satisfies. 
 
 \item It should be able to capture and reason about the beliefs and knowledge of the system as they both drive actions.
 

 \item It should allow capturing uncertainty and the evolution of behaviors over time. 
 
 \item It should enable on-line reasoning that the system uses to track the mental model and the current knowledge of the human operator.
 \end{itemize} 
 
We provide a list of approaches that could be used to satisfy some of the requirements above. In declarative modeling~\cite{kowalski1979algorithm,fahland2009declarative}, one specifies what the program does without prescribing the details of the implementation of the program i.e. how the program does it.  
Declarative modeling enables us to provide a description of the capabilities of the human operator rather than details behind the mechanism which implements those capabilities. 

Declarative modeling can be coupled with contract-based design (CBD) \cite{benveniste2018contracts} that support compositional verification of systems.  
In CBD, each component is modeled using an assume-guarantee abstraction. A set of components can be composed to yield a new component contract. Components are ordered by a refinement relation that indicates whether one component can be replaced by another in all contexts where the former works. Refinement checking is problem of verifying when a contract is refinement by another contract. For example, a high-level task can be decomposed into a set of sub-tasks, and refinement checking in this case verifies that the composition of the sub-tasks (itself a contract) can be executed in all environment where the original task can, and achieves the same goals. Once refinement is verified, the high-level task (which has hopefully a simpler description than its decomposition) can be used in place of the decomposition to verify system level properties. One difficulty facing compositional modeling of human operators is that workload measures of tasks are difficult to compose. Typically, getting the workload of two or more tasks being performed concurrently requires re-collecting workload ratings for the combination of tasks being performed. Multiple-resource models provide a somewhat limited computational way of composing workloads. However, those compositional techniques are still experimental and have not been extensively validated. 

Modeling knowledge and higher order of belief can be achieved using the framework provided by epistemic logic \cite{fagin2004reasoning}. This framework has been used, for instance, in \cite{fisher2013verifying}. Models of humans should be able to describe the state of knowledge which includes what the human knows about the knowledge (not the vivid facts) of the machine. There are several tools that have appeared in the area of verification of epistemic dynamic systems (see for example \cite{vcermak2014mcmas}) that could be used to perform formal verification. However, the problem is in modeling the epistemic state in a declarative way. Moreover, issues such as awareness \cite{fagin1987belief,sim1997epistemic} has to be taken into account, and modeling techniques and verification tools for modeling awareness seem to be lacking. Furthermore, the declarative modeling language should mix epistemic modalities with several others including temporal and probabilistic ones, leading to verification problems that become intractable. Research in both areas of modeling and decision procedures is needed to enable formal verification of these new class of models. 
