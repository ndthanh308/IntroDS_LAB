We provided an overview of human modeling including fundamental theories of human cognition, decision-making, perception and actuation, as well as the state-of-art human operator modeling and human factors techniques for assessing human performance in aviation, and for the design, verification and validation of human-machine systems. 

We have found that human operator for human-\emph{automation} systems is much more common than for human-\emph{autonomy} systems. By automation, we mean systems helping humans to perform a limited set of tasks in a limited set of environments. In this case, engineers can provide dedicate software programs to handle each case. When the number of goals and environments makes this standard approach impractical, then the system must include perception and decision-making, and we refer to it as autonomy. In this latter case, there is a need to reason about what the system and the human ``know'' about the environment, what the system ``believes'' that the human ``knows'', what the human believes that the system knows, and so on. Also, we have found that, while general purpose human models exist (such as cognitive architectural models),  they tend to be difficult to construct as they may require a considerable knowledge capturing effort, they are rather brittle, and are not suitable for systematic design, verification and validation of human-machine systems.

System-theoretic models, such as stochastic processes, are easier to construct. However, they may not scale well to industrial-scale applications and generalize poorly with respect to the task and the context. Formal methods models have been primarily built for the analysis and verification of interactions between human and existing automation. There is a lack of formal modeling of human operator for assessing  performance and capabilities in scenarios involving the operation of autonomous systems including single-pilot applications. Furthermore, the approaches we have surveyed tend to analyze the combined human-machine system at one level of abstraction, hitting the well-known state explosion problem.

Based on the outcomes of this survey, we feel that further research and development efforts are required to deliver a unified framework for modeling and verification of human-autonomy systems. We make the following recommendations  

\noindent
\emph{Declarative and compositional specifications.} 
Declarative modeling~\cite{kowalski1979algorithm,fahland2009declarative} enables us to provide a description of the capabilities of the human operator rather than details behind the mechanism which implements those capabilities. The expected increase in the complexity of human-autonomy systems makes testing and simulation less effective due to the large possible set of environments to be considered. Declarative models enable \emph{Formal methods}~\cite{Peled} are exhaustive and lead to strong guarantees on safety and/or performance at the expense of computational complexity. Tractability problems can be tackled with appropriate abstractions and compositional frameworks such as contract-based design (CBD) \cite{benveniste2018contracts}.  
%In CBD, each component is modeled using an assume-guarantee abstraction. A set of components can be composed to yield a new component contract. Components are ordered by a refinement relation that indicates whether one component can be replaced by another in all contexts where the former works. Refinement checking is problem of verifying when a contract is refinement by another contract. For example, a high-level task can be decomposed into a set of sub-tasks, and refinement checking in this case verifies that the composition of the sub-tasks (itself a contract) can be executed in all environment where the original task can, and achieves the same goals. Once refinement is verified, the high-level task (which has hopefully a simpler description than its decomposition) can be used in place of the decomposition to verify system level properties. 
One difficulty facing compositional modeling of human operators is that workload measures of tasks are difficult to compose. Typically, getting the workload of two or more tasks being performed concurrently requires re-collecting workload ratings for the combination of tasks being performed. Multiple-resource models provide a somewhat limited computational way of composing workloads. However, those compositional techniques are still experimental and have not been extensively validated. 

\noindent
\emph{New specification languages to account for different kinds of uncertainty and time.} 
As machine exhibits more human-like traits such as using knowledge to make 
complex decisions, there emerge a need to 
reason about not only about what the system knows, but also what the human operator knows about the system, and what the system knows about what the human operator knows about the system, and so on. 
Modeling knowledge and higher order of belief can be achieved using the framework provided by epistemic logic \cite{fagin2004reasoning}. This framework has been used, for instance, in \cite{fisher2013verifying}. Models of humans should be able to describe the state of knowledge which includes what the human knows about the knowledge (not the vivid facts) of the machine. There are several tools that have appeared in the area of verification of epistemic dynamic systems (see for example \cite{vcermak2014mcmas}) that could be used to perform formal verification. However, the problem is in modeling the epistemic state in a declarative way. Moreover, issues such as awareness \cite{fagin1987belief,sim1997epistemic} has to be taken into account, and modeling techniques and verification tools for modeling awareness seem to be lacking. Furthermore, the declarative modeling language should mix epistemic modalities with several others including temporal and probabilistic ones, leading to verification problems that become intractable. Research in both areas of modeling and decision procedures is needed to enable formal verification of these new class of models. 

\noindent
\emph{Human-factors, subject-matter experts, and engineering interfaces.} 
The analysis of human-machine systems will require collaboration among several experts in different areas such as human-factors (HF), subject-matter (SM), and engineers (E). An integrated modeling and verification environment will need to provide appropriate interfaces to all. HF and SM experts may not have the appropriate background to build models amenable to formal analysis. Similarly, they may not be able to interpret results generated by formal verification tools. 

Interpretability of analysis results by HF and SM experts will be essential given the current state of research in human modeling as outlined in this survey. There is a certain degree of model uncertainty that may lead to incorrect analysis results. Thus, we believe that it is important to provide the right level of explanation to experts in various field to assess the results and identify potential modeling errors. 