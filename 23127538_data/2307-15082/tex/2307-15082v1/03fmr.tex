
This section samples humans models used in the \emph{formal verification} of human-machine systems.  

In formal verification, models of human, machine and the properties of interest (safety and/or performance) to be verified are captured using formal language(s) with unambiguous mathematical meaning. 
The human-machine system is verified when a  proof is generated that shows that the
system satisfies the properties of interest. This proof is typically either automatically generated using 
model-checking~\cite{clarke2018model}, or semi-automatically 
using a proof assistant~\cite{nipkow2002isabelle}. 


\subsection{Formal Model of Tasks} 

We survey some efforts to formalize task and cognitive models of human operators.  
A more detailed review of various formal task and cognitive models can be found in this earlier survey paper~\cite{Bol13}.  
Formal models of human operators have been developed in the past using general languages such as process algebra~\cite{gunter2009specifying}, Petri-Nets~\cite{basnyat2007formal}, and more domain specific ones such as task analytic or cognitive modeling languages.  
This review focuses on the latter two as process algebra and Petri-Nets are already formal representations with well defined semantics and a variety of analysis tools already available to users, and the main issue is, therefore, the encoding process to represent a human model. 

Task analytic models are hierarchical representations of the observable set of human behaviors.  
In a task model, a high-level activity is decomposed into a set of composite actions 
which are further decomposed into a set of atomic actions. 
Some task analytic modeling languages are ConcurTaskTrees (CTT)~\cite{paterno1997concurtasktrees}, Operator Function Model (OFM)\cite{mitchell1986discrete}, Enhanced Operator Function Models (EOFM)~\cite{bolton2009enhanced}, and User Action Notation (UAN)~\cite{hix1993developing}.
Formal task analytic models can be encoded in a task modeling language, 
and then translated into a model checking or theorem proving representation for analysis and verification 
(see~\cite{palanque1995validating,ait2006formal,bolton2011systematic,ameur2003formal}). They can also be 
directly encoded in the formal language. 
Task model analyses in general are more limited by scalability issues than interface analyses 
since they cover the system beyond the interface. 
Nevertheless they have been extended to analyses with a team of operators or multiple operators (see~\cite{paterno1998formal,bass2011toward}) and errors~\cite{martinie2014fine}

\subsection{Formal Model of Cognition}

Lindsay and Connelly et al. in~\cite{cerone2005formal} applied model checking on a human operator model expressed in the cognitive architecture Operator Choice Model (OCM), and the approach was then applied in~\cite{lindsay2002modelling} to study erroneous behaviors of human air traffic controllers for various ATC tasks. OCM is not a generic cognitive architecture as it is specialized for ATC tasks.  Other works on more generic cognitive models such as Programmable User Models (PUM) include Blandford, Curzon, Butterworth et al. in~\cite{blandford2004models,butterworth1998role,butterworth2000demonstrating,rukvsenas2006formal}.  The verification in~\cite{butterworth1998role} was done using a theorem proving approach while~\cite{rukvsenas2006formal} used model-checking. One advantage that a cognitive model has over a task analytic model is that it can capture the organic underlying reasons for human errors rather than requiring researchers to introduce them explicitly into the model. The main drawback of cognitive models is that they are more complex hence less tractable by formal verification techniques. The work in~\cite{rukvsenas2009verification} is an example of  assessment of human errors in the context of ATM.
The work by Masci et al. in~\cite{masci2011modelling} uses the theorem-proving language PVS~\cite{owre:pvs:sri} to model a distributed cognitive model describing the information flow among multiple human operators. 
Distributed cognition~\cite{flor1991analyzing} is a conceptual framework in which cognition is a property of the whole system i.e. not just confined to the human minds. 
Additionally, a recent work in~\cite{Neo16} approached the problem by translating an existing cognitive model into a formal language representation.  The cognitive model, a SOAR model of air traffic contingency procedures performed by a human and automation team, was translated into a hybrid input-output automaton, which was then used to formally verify safety properties as the autonomy level of the system was increased. 



 
\subsection{Formal Models of Workload} 

Formal models of workload is a relatively unexplored area.  
In~\cite{moore2014modeling}, the authors argued that one limitation of existing workload models 
is their level of resolution being too detailed and thus requiring
modeling efforts that are impractically time-consuming.  To address this limitation, 
they created an abstraction of a UAS-human teaming scenario with all of its actors modelled by finite-state machines augmented with workload metrics. Behavioral execution traces of the FSM model result in traces of workload metrics which can be monitored for violation of specification.  
Moore et. al however did not present any formal verification results. 
A more recent work~\cite{houser2018using} presented a novel approach which combined 
model-checking and simulation for the analysis of task loads and resource conflicts in 
air traffic control scenarios.  In this work, a trace of an air traffic simulation model  
for a particular scenario and time window is translated into a timed-automata model, which is used to model-check the entire ``neighborhood'' around the particular scenario for any violation of key properties. 
The counter-examples obtained by the model-checker are then fed back to the simulator for further analysis using higher fidelity models.  



\subsection{Formal Model of Failure Modes}

Mode confusion have been analyzed formally using either model-checking and/or theorem proving (see~\cite{joshi2003mode,butler1998formal, campos2011modelling}). In all of those works, the human operator was not modelled 
modulo the human inputs to the machine. 
In~\cite{joshi2003mode} for example, model-checking and theorem proving were used to explore all the unexpected inputs that can trigger a particular flight guidance mode.  Other approaches towards formal analysis of mode confusion include modeling what the operator thinks is 
the state of the machine (also referred to as the mental model)~\cite{rushby2002using,degani2000pilot}, or how the operator accomplishes goals using the machine (also referred to as knowledge model)~\cite{javaux2002method}. Crow et al. in~\cite{crow2000models} proposed that mental models should be constructed from questionnaires to be incrementally refined and verified until the smallest safe mental model is found. However, extracting an accurate mental model from a human operator remains a challenge.   
In~\cite{combefis2015automatic}, the automation interface design tool ADEPT was used for the analysis of mode 
confusion.  Based on the mental model paradigm, the system and mental model are captured in ADEPT 
using an enriched labelled transition system called HMILTS. 

In the context of formal verification, one of the main approach to classifying and modeling human errors has been the binary phenotypes as described by Hollnagel in~\cite{hollnagel1993phenotype}. Several works~\cite{bastide2006error,bolton2008using,bolton2008formal} have incorporated Hollnagel's phenotypes of error into their task models and used  model-checking to determine whether the errors would lead to any violation of the specifications. 


A more recent taxonomy of human errors developed to be more aligned with task analytic modeling and analysis 
techniques has been proposed in~\cite{bolton2017task}.  This taxonomy was first applied 
in~\cite{bolton2019formal} and then in~\cite{bolton2021formal}. In the latter work, reliability 
analysis and probabilistic model-checking were used to verify temporal-stochastic specifications such as 
the probability of an eventual failure being reached is less than some time bound.   
This supports the alternative view of human errors as probabilistic distributions of behavioral outputs under cognitive, motor and perceptual constraints. 


\subsection{Formal Model of Knowledge} 

The epistemic approach~\cite{fagin2004reasoning} uses modal logic to capture and reason about the belief and knowledge of multi-agent systems. While epistemic modal logic has been studied for the past several decades~\cite{kripke1963semantical}, there appears to be little work which uses this framework to describe and reason about human behaviors in general. One possible reason is the lack of modeling tools that are intuitive to the domain expert.  This could be due to the lack of a grounded semantics for the systems under analysis which in our case is the human operator. The interpreted system~\cite{fagin2004reasoning} model has a well-known grounded semantics that has been used to model knowledge, belief and communications of computing systems.

Another possible reason is the dearth of efficient verification tools.  
Though recent model-checking literature~\cite{van2002model,kacprzak2004verification,wu2005model} have tackled the problem of verifying various language fragments with temporal and knowledge modalities, there are only a few modeling and verification tools that support epistemic and temporal modalities. One such tool is the MCMAS, described in~\cite{Lom15,Pen03}, which was developed for the verification of multi-agent interpreted systems. 

Perhaps another weakness of the epistemic approach is the problem of logical omniscience: an agent knows all the consequences of a set of logical assertions. Logical omniscience does not capture human characteristics such as bounded rationality, bounded reasoning and introspective forgetting.  Various modifications to the epistemic approach have been proposed to address  logical omniscience such as impossible possible worlds~\cite{hintikka1979impossible}, algorithmic knowledge~\cite{halpern1994algorithmic}, and non-classical propositional logic~\cite{fagin1995nonstandard}.  Other research works such as the one in~\cite{Van09} have proposed ways of enhancing the expressiveness of epistemic logic in order to succinctly capture introspective forgetting. 

Relative to task and workload models, the literature on modeling of 
knowledge and belief of human operators is very limited.  The work in~\cite{Pri14} extended CTLK (a temporal-epistemic first-order logic) with a notion of degree of
belief, and introduced a model-checking algorithm for such extended logic. 
The authors applied this extension to the Air France 447 (AF447) incident and duplicated
the NTSB conclusion that right before the crash, the pilots did
not believe any of the signals coming out of the cockpit display were true. 
The same case study was used to illustrate agent safety logic (ASL) 
in~\cite{ahrenbach2018formal}, a modal logic formalized in Coq~\cite{Coq:manual} 
with both epistemic and doxastic (belief) modalities combined 
with a safety logic based on the flight manual. 
ASL was used to illustrate the increasingly common phenomenon in aviation in which the pilot neither knows 
the state of the aircraft nor knows that he or she does not know the state of the aircraft. 

\subsection{Comparative summary}
\label{sec:formal-models-comparative-summary}
Table~\ref{tab:summary5} lists the models we have surveyed in this section. Each model is classified in terms of six key attributes that we believe are important for the formal verification of HMS. \emph{Focus} is the primary aspect of the human considered by the model. \emph{Formal verification} is the technique used to prove properties about the model, and can be either model-checking or theorem proving. By ``model-checking'' we mean any tool that performs symbolic or explicit state-space exploration, and that is typically fully automatic.  By ``theorem proving'', we meant a proof assistant tool such as PVS or Coq which is interactive. \emph{Timing} indicates whether the cited work has demonstrated the ability to verify non-trivial properties that involve time (either discrete or continuous). The use of temporal logic alone would not be considered as evidence that the model can handle properties about time. The same rationale applies 
to \emph{Probabilistic} and \emph{Epistemic}. The former indicates the ability to handle systematic uncertainty and answer queries about the probability of certain events. The latter indicates the ability to formally reason about the belief of the human. Finally, the \emph{Tool} column indicates whether the work has produced an integrated tool-chain with a frontend modeling environment that is connected to backend solver(s) for verification.

Table~\ref{tab:summary5} shows a general lack of support for uncertainty, both systematic and epistemic, which is expected given the inherent complexity of the verification problem for uncertain systems. Furthermore, there seem to be a lack of interest in the development of integrated tools bridging the gap between models and formal verification engines. 

\begin{table*}
    \centering 
    \caption{Summary of surveyed formal verification models}
\begin{tabular}{ |p{3cm}||c|c|c|c|c|c|}
    \hline
	Model & Focus & Formal Verification & Timing & Probabilistic & Epistemic &  Tool \\
	\hline
    Multiple~\cite{bolton2009enhanced,bolton2011systematic} & Task & Model checking & No & No & No & No \\
    \hline
    Multiple~\cite{palanque1995validating,ait2006formal,ameur2003formal} & Task & Theorem proving & No & No & No & No \\
	\hline
    Butterworth et al.~\cite{butterworth1998role} & Cognition & Theorem proving & Yes & No & No & No \\
    \hline
    Rukvsenas et al.~\cite{rukvsenas2006formal} & Cognition & Model checking & Yes & No & No & No \\
    \hline 
    Musci et al.~\cite{masci2011modelling} & Distributed Cognition & Theorem proving & No & No & Yes & No \\
    \hline 
    Multiple~\cite{rushby2002using,degani2000pilot} & Mode confusion & Model checking & No & No & No & No \\
    \hline 
    Houser et al.~\cite{houser2018using} & Workload & Model checking & Yes & No  & No & No \\
    \hline 
    Moore et al.~\cite{moore2014modeling} & Workload & No & Yes & No & No & No  \\
    \hline 
    Bolton, Zheng et al.~\cite{bolton2021formal} & Task + Error & Model checking & Yes & Yes  & No & No \\
    \hline 
    Multiple~\cite{bastide2006error,bolton2008using,bolton2008formal,bolton2019formal} & Task + Error & Model-checking & No & No & No & No \\
    \hline  
    Primero et al.~\cite{Pri14} & Beliefs & Model-checking & No & No & Yes & No \\
    \hline   
    Ahrenbach et al.~\cite{ahrenbach2018formal} & Beliefs & Theorem proving & No & No & Yes & No \\
    \hline 
    
\end{tabular}
\label{tab:summary5}
\end{table*}
