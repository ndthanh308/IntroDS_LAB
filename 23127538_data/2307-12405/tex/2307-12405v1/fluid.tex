%% This is file `elsarticle-template-5-harv.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: elsarticle-template-5-harv.tex 159 2009-10-08 06:08:33Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-5-harv.tex $
%%
\documentclass[preprint,authoryear,3p, 11pt]{elsarticle}
%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'


%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newdefinition{rmk}{Remark}
\newproof{pf}{Proof}
\newtheorem{cor}[thm]{Corollary}


\usepackage{bbm}
\usepackage{physics}
\usepackage{bm}
%\usepackage[english]{babel}
%\usepackage[utf8x]{inputenc}
\usepackage[short]{optidef}
\usepackage{booktabs}       % professional-quality tables
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\usepackage{amsmath}
%\allowdisplaybreaks 
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{acronym}

\usepackage{algorithmic}
\usepackage[ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output} 
\usepackage{multirow}
\usepackage{mathtools}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
% \usepackage{array}
% \usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage[colorinlistoftodos]{todonotes}
% \usepackage[hidelinks]{hyperref}

\usepackage{tikz}
\usetikzlibrary{calc,patterns,arrows,shapes.arrows,intersections}

\hypersetup{pdfauthor={Name}}
%\newacronym{rfe}{RFE}{recursive feature elimination}

%\newcommand{\norm}[1]{\left\|#1\right\|}

% \usepackage[usenames,dvipsnames]{pstricks}
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Operations Research}
\begin{document}

\begin{frontmatter}

%% Title, authors and addresses


\title{Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach}

%% use optional labels to link authors explicitly to addresses:
\author[1]{Dimitris Bertsimas\corref{cor1}}
\ead{dbertsim@mit.edu}

\author[2]{Cheol Woo Kim}
\ead{acwkim@mit.edu}


\cortext[cor1]{Corresponding author}

 % \affiliation[1]{organization={Sloan School of Management, Massachusetts Institute of Technology}, 
 %                 addressline={100 Main Street},
 %                 city={Cambridge}, 
 %                postcode={02142}, 
 %                 country={United States}}

\address[1]{Sloan School of Management, Massachusetts Institute of Technology, 100 Main Street, Cambridge, 02142, United States}

 % \affiliation[2]{organization={Operations Research Center, Massachusetts Institute of Technology},
 %                 addressline={1 Amherst Street}, 
 %                 city={Cambridge},
 %                 postcode={02142}, 
 %                 country={United States}}
\address[2]{Operations Research Center, Massachusetts Institute of Technology, 1 Amherst Street, Cambridge, 02142, United States}

\begin{abstract}
We propose a machine learning approach to the optimal control of multiclass fluid queueing networks (MFQNETs) that provides explicit and insightful control policies.  
%MFQNETs are continuous, deterministic approximations of multiclass queueing networks (MQNETs) %that are amenable to tractable  analysis for both performance analysis and  optimal control. 
We prove that a threshold type optimal policy exists for MFQNET control problems, where the threshold curves are hyperplanes passing through the origin. We use Optimal Classification Trees with hyperplane splits (OCT-H) to learn an optimal control policy for MFQNETs. We use numerical solutions of MFQNET control problems as a training set and apply OCT-H to learn explicit control policies. We report 
experimental results with up to 33 servers and 99 classes that demonstrate that the learned policies achieve 100\% accuracy on the test set.  While the offline training of OCT-H can take days in large networks, the online application 
takes milliseconds.  

\end{abstract}

%%Graphical abstract


%%Research highlights


\begin{keyword}
Queueing Network Control \sep Optimal Decision Trees \sep Machine Learning \sep Fluid Approximation \sep Optimal Control
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text


%%%%%%%%%%%%%%%%%
%% SEC. INTRODUCTION
%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}


Multiclass queueing networks (MQNETs) are complex systems that model the behavior of multiple classes of jobs, each with their own arrival and service rates, routing paths and holding costs. These networks find numerous applications in diverse fields, including manufacturing \citep{manufacturing}, healthcare \citep{healthcare} and communication networks \citep{comminucation} among many. The control of MQNETs is of great importance in improving system efficiency, optimizing resource allocation, and reducing operational costs. However, the inherent complexity of these systems makes their analysis and control a challenging task. 

Multiclass fluid queueing networks (MFQNETs) have been developed as a deterministic, continuous approximation of MQNETs, primarily to provide a tractable method for analyzing the stability of the underlying MQNETs.  \citep{dai1995, Stolyar1995} demonstrate that the stability of MFQNETs implies the stability of underlying MQNETs. Several related studies including \citep{Meyn1995, dumas1999, gamarnikhans2005} have also explored the topic. See \citep{queuebook} for a comprehensive review. 

MFQNETs also provide a useful way to construct control policies for MQNETs as the optimal control of MFQNETs is often much more tractable than the optimal control of underlying MQNETs. To this end, several approaches have been proposed in the literature.  \citep{Mag1999, Mag2000} propose discrete review policies and show that they achieve asymptotic optimality and stability under fluid scailing. \citep{robustfluid} provide a robust formulation of MFQNET control problem and translate the resulting policy to the underlying MQNET. \citep{BertSethu2002, DaiWeiss2002} propose methods to approximately minimize make-span based on the associated fluid models. For a comprehensive review of the topic, see \citep{Meyn2007} and  \citep{queuebook}.

Mathematically, optimal control of MFQNETs falls into a subclass of infinite dimensional linear optimization models known as separated continuous linear programs (SCLPs). Several researchers have investigated the theoretical properties of SCLPs, such as \citep{AndNashPer1983, Pullan1995, Pullan1996, Pullan1997}.
 \citep{avrambertsimasricard} find closed-form optimal policies for specific MFQNETs using optimality conditions from optimal control theory. Other works have proposed numerical algorithms for solving SCLPs. \citep{Pullan1993a, Pullan2002, LuoBertsimas} develop algorithms based on discretization, while \citep{Weiss2008ASB, evgenyweiss2021} propose simplex-like methods.  \citep{FleiSethu2005, BampouKuhn2012} propose polynomial-time approximation algorithms.

Despite significant efforts in the field and its practical applications, optimal control of MFQNETs remains a challenging computational task. Moreover, current algorithms typically provide only numerical solutions, making it difficult to gain insight into the underlying structure of the optimal policy.

Recently, there has been growing interest in applying machine learning techniques to solve challenging optimization and control problems. For instance, \citep{Khalili2016, Alvarez2017AML,OPT, Voice, Prune, cauligi2021coco} propose machine learning-based approaches to mixed-integer optimization. \cite{ARO} develop a method to solve two-stage adaptive robust optimization problems using machine learning. Machine learning has been used for hyperparameter tuning in optimization algorithms as well \citep{Hutter2011, Balcan2020LearningTO}. For queueing network control, reinforcement learning methods are proposed in \citep{Raeis2021QueueLearningAR, Bai2019, Dai2022}. Although these approaches have shown to be effective in addressing computational challenges, it can be difficult to provide theoretical guarantees that the machine learning methods lead to optimal solutions.

In this paper, we present a novel approach that leverages machine learning to solve MFQNET control problems. The MFQNET control problem we consider is the fluid analog of the sequencing problem in MQNETs. The sequencing problem in MQNETs is a stochastic and discrete control problem that involves deciding which class of jobs to process at each server at any given time, with the aim of minimizing the expected total cost. We formulate the fluid analog of this problem as a SCLP problem and propose a machine learning-based algorithm to address it.

We solve multiple MFQNET control problems and use the resulting numerical solutions to learn an optimal policy. The machine learning algorithm we use is Optimal Classification Trees with hyperplane splits (OCT-H) proposed by \citep{OCT, MLOPT}. OCT-H is a classification algorithm that partitions the feature space using hyperplanes and assigns a prediction to each region. We prove that OCT-H can learn exact  optimal policies for the MFQNET control problems.

%Given a training set where each data point consists of a covariate vector and an associated target, OCT-H learns a near-optimal decision tree for classification tasks. For each node split, it uses arbitrary linear combination of the features, as opposed to using a single future which is more common form of decision trees. 

\noindent
The contributions of the paper are as follows.
\begin{enumerate}
    \item We prove the existence of a threshold-type optimal policy for MFQNET control problems, where the threshold curves are hyperplanes passing through the origin. This result was previously proven only for special cases.
    \item Based on the theoretical findings, we propose an efficient algorithm that can learn an exact optimal control policy for MFQNETs using OCT-H. We report 
experimental results with up to 33 servers and 99 classes that demonstrate that the learned policies achieve 100\% accuracy on the test set.  
    \item Once a policy is learned offline, it can be directly applied  online to unseen states in milliseconds, leading to a significant speed-up compared to solving the problem numerically.
    \item The high interpretability of decision trees allows us to gain insights into the structure of the optimal policy, which is a significant advantage that numerical optimization algorithms often lack. By providing the actual decision trees learned by OCT-H, we develop a deeper understanding of MFQNETs and their optimal policy.
\end{enumerate}

The structure of this paper is as follows. Section \ref{sec:background} provides the definition of MFQNET control, along with the associated optimality conditions. We also provide a brief review of OCT-H. Section \ref{sec:main} provides our theoretical results on the structure of optimal policy for MFQNET control. We then develop a learning algorithm based on OCT-H and provide a small example to illustrate the method. Section \ref{sec:numerical experiment} reports the results of computational experiments, where we analyze the accuracy, speed, and interpretability of our approach.

 \paragraph{Notational conventions} Throughout this paper, we use lower case boldface letters to denote vectors and upper case boldface letters to denote matrices. The $i_{th} $ entry of a vector $\bm{x}$ is denoted $x_i$, and the entry in the $i_{th}$ row and $j_{th}$ column of a matrix $\bm{A}$ is denoted $a_{ij}$. Division between two vectors is always assumed to be entry-wise. We use $\bm{e}$ to denote the vector of all ones and $\bm{0}$ to denote the vector of zeros. We use $x(\cdot)$ to denote a real-valued function, and $\bm{x}(\cdot)$ to denote a vector whose entries are real-valued functions. We use $\bm{x}$ instead of $\bm{x}(\cdot)$ when it is clear from the context that $\bm{x}$ is referring to a vector of functions.  


\section{Background}
\label{sec:background}

In this section, we first define the optimal control problem for MFQNETs. Then, we review necessary optimality conditions in optimal control theory and provide a brief overview of OCT-H.

\subsection{Optimal Control of MFQNETs}
\label{sec:fluid}
Consider a queueing network with $m$ servers and $n$ job classes. Each job class $i \in [n]$ is processed by a single server $s(i) \in [m]$ with service rate $\mu_i$. After jobs of class $i$ are processed, they either leave the system or change to a different class in a deterministic manner. Jobs may arrive from either another server or from outside the system with external arrival rate $\lambda_i$. If there is no external arrival for class $i$, then $\lambda_i = 0$. The cost per unit time for holding a job of class $i$ is denoted $c_i$. 

For each class $i \in [n]$, the control variable $u_i(t)$ denotes the fraction of effort the server $s(i)$ spends processing class $i$ jobs at time $t$. The state variable $x_i(t)$ is the number of jobs of class $i$ at time $t$.  The dynamics of the system can be expressed using a matrix $\bm{A} \in \mathbb{R}^{n \times n}$, where $a_{ii} = - \mu_i$ and ${a}_{ij} = \mu_j$ if class $i$ receives arrivals from class $j, \; j \neq i$. The rest of the entries of $\bm{A}$ are zero. Then, the dynamics of the system is
$$
\dot{\bm{x}}(t) = \bm{A}\bm{u}(t) + \bm{\lambda}.
$$
\noindent In addition, the sum of the control variables for all classes that are processed at the same server should be less than or equal to one. This constraint can be expressed as
$$
\bm{D}\bm{u}(t) \leq \bm{e},
$$
\noindent where $\bm{D} \in \{0,1\}^{m \times n}$ is a binary matrix with $d_{ij} = 1$ if $s(j) = i$ and $d_{ij} = 0$, otherwise. 

The MFQNET control problem aims to find a control $\bm{u}$ that minimizes the total holding cost of the jobs in the system over the time interval $[0,T]$. We define the MFQNET control problem with an initial state $\bm{x}_0$ as the following:
\begin{alignat}{2}
&\underset{\bm{u}(\cdot), \bm{x}(\cdot)}\min\quad && \int_{0}^{T} \bm{c}^{\top}\bm{x}(t) \,dt  \label{eq:fluid}\\
&s.t. && \dot{\bm{x}}(t) = \bm{A}\bm{u}(t) + \bm{\lambda}, \quad \forall t \in [0,T], \nonumber\\
&\qquad \ && \bm{D}\bm{u}(t) \leq \bm{e}, \quad \forall t \in [0,T],  \nonumber \\
&\qquad \ && \bm{u}(t), \bm{x}(t) \geq \bm{0}, \quad \forall t \in [0,T], \nonumber \\
&\qquad \ && \bm{x}(0) = \bm{x}_0. \nonumber
\end{alignat}
We use $\bm{u}^{*}_{\bm{x}_0}$ and $\bm{x}^{*}_{\bm{x}_0}$ to denote the optimal control and the associated state trajectory of problem \eqref{eq:fluid} with the initial state $\bm{x}_0$. We use $\bm{u}^*$ and $\bm{x}^*$ to denote the optimal control and the associated state trajectory of general MFQNET control problems when the initial state is not specified. 

We define the workload vector $-\bm{D}\bm{A}^{-1}\bm{\lambda} \in \mathbb{R}^{m}$, and assume that all the entries of the workload vector are strictly smaller than 1 for stability.  We further assume  $T$ is large enough, so that the system can be emptied by time $T$ \citep{Meyn2007}. Under this setting, identifying the optimal initial control $\bm{u}^*_{\bm{x}_0}(0)$ for any initial state $\bm{x}_0$ is equivalent to identifying the optimal policy $\bm{u}^*(t)$ for any state $\bm{x}(t)$. 



\subsection{Optimality Conditions of MFQNET Control}
\label{sec:conditions}

The Pontryagin Maximum Principle \citep{Bittner1963LSP, Sethi2019} provides necessary optimality conditions for general optimal control problems. Due to the non-negativity constraints on the state variable in Problem \eqref{eq:fluid}, the conditions that we provide are tailored for the optimal control problems with pure state constraints. 

We define the Hamiltonian of Problem \eqref{eq:fluid} as
$$
H(\bm{x}, \bm{u}, \bm{y}, t) = \bm{c}^{\top}\bm{x}(t) + \bm{y}(t)^{\top}[\bm{A}\bm{u}(t) + \bm{\lambda}],
$$
where $\bm{y}(t)$ is \textcolor{black}{known as} the costate variable. 

\begin{lem}[Pontryagin Maximum Principle \citep{Bittner1963LSP, Sethi2019}]\label{th:pontryagin} If the feasible control ${\bm{u}^*}$ and the state trajectory ${\bm{x}^*}$ is optimal for Problem \eqref{eq:fluid}, there exists ${\bm{y}}(t)$ for any $t \in [0,T]$ that satisfies the following conditions.
\begin{enumerate}
\item[{\bf (a)}]  $H(\bm{x}^*, \bm{u}^*, \bm{y}, t) \leq H(\bm{x}^*, \bm{u}, \bm{y}, t)$ for all $\bm{u}(t)$ satisfying $\bm{u}(t) \geq \bm{0}$, $\bm{D}\bm{u}(t) \leq \bm{e}$. 
\item[{\bf (b)}]  Whenever ${\bm{u}}^*(t)$ is continuous,
    $
    \dot{\bm{y}}(t) =  -\bm{c} + \bm{\pi}(t),
    $
    where $\bm{\pi}(t) \geq \bm{0}, \bm{\pi}(t)^{\top}{\bm{x}^*}(t) = 0$.
\item[{\bf (c)}]  $\bm{y}(T) = \bm{0}$.
\end{enumerate}
\end{lem}

\begin{pf}
See \citep{Sethi2019}.  \qed
\end{pf}


\subsection{Optimal Classification Trees with Hyperplane Splits}
\label{sec:oct}

Optimal Classification Trees (OCT) is an algorithm to learn near-optimal decision trees for classification tasks. Classification and Regression Trees (CART) \citep{BreiFrieStonOlsh84}, an earlier algorithm to learn decision trees for prediction tasks, learns a decision tree in a greedy manner using recursive partitioning of the feature space at each child node. However, OCT aims to learn a globally optimal decision tree using mixed\textcolor{black}{-}integer optimization and local heuristics. 

Similar to CART and other classification algorithms, OCT takes $N$ data inputs $\{(\bm{\theta}_i, z_i)\}_{i=1}^{N}$, where $\bm{\theta}_i$ is the feature vector and $z_i$ is the label for the $i_{th}$ data point. Given this data set, OCT learns a decision tree that uses a single feature for the split at each node and assigns a label to each node of the tree. Given a new data point $\bm{\theta}_0$, it traverses the decision tree until it reaches a leaf node. The prediction of the tree for $\bm{\theta}_0$ is the label assigned to the leaf node. 

OCT-H, a generalization of OCT, can use an arbitrary linear combination of the features for splits at the nodes. This means that OCT-H can use general hyperplanes for splits, whereas OCT is confined to use hyperplanes that are perpendicular to the axes in the feature space. Essentially, OCT-H partitions the feature space with hyperplanes, and assigns a prediction to each region. This observation is the key to our work to solve Problem \eqref{eq:fluid} using OCT-H. Compared to OCT, OCT-H generally shows higher prediction accuracy and learns shallower trees. 

In OCT-H, it is possible to limit the number of features that can be used for splits, which can result in a more interpretable tree. This version of OCT-H is denoted as OCT-H with sparsity throughout the remainder of the paper. We simply use OCT-H to denote regular OCT-H, where the entire features can be used. For a more detailed explanation on OCT and OCT-H, we refer readers to \citet{OCT, MLOPT}.


\section{OCT-H for the Optimal Control of MFQNETs}
\label{sec:main}

In this section, we first prove that OCT-H can learn an optimal policy of Problem \eqref{eq:fluid}. \textcolor{black}{Based on this result, we then proceed to develop an efficient algorithm to learn an optimal policy of Problem \eqref{eq:fluid} using OCT-H.}


\subsection{Theoretical Results}
\label{sec:theory}

For each job class $i \in [n]$, we define the depletion time $T_i = \inf\{t \in (0,T]: x^*_i(t) = 0\}$ under an optimal state trajectory $\bm{x}^*$, assuming that $x_i^*(0) > 0$. The following Lemma will be used to prove our main theorem.

\begin{lem}\label{th:costate} The costate variable $\bm{y}(t)$ in Lemma \ref{th:pontryagin} satisfies the following statements.
\begin{enumerate}
\item[{\bf (a)}]  \textcolor{black}{If $x_i^*(t) = 0$ for some interval $t \in (\tau_1,\tau_2)$, then $y_i(t) = 0$ for $t \in (\tau_1,\tau_2)$.}
\item[{\bf (b)}]  $\bm{y}(t)$ is continuous, piecewise linear function of $t$.
\item[{\bf (c)}]  {At any time $t \in [0,T]$, the value of $\bm{y}(t)$ can be expressed as a linear function of $(T_1, \dots, T_n)$.}
\end{enumerate}
\end{lem}

\begin{pf}

\noindent
{\bf (a)} We consider the relation between the costate variable for the indirect method and the costate variable for the direct method \citep{Sethi2019}. For optimal control problems with pure state constraints, there are different ways to associate multipliers to the constraints. The method we presented in Section \ref{sec:conditions} is known as the direct method. For the indirect method, the Lagrangian is defined as 
$$
L(\bm{x}, \bm{u}, \bm{y}, t) = \bm{c}^{\top}\bm{x}(t) + \tilde{\bm{y}}(t)^{\top}[\bm{A}\bm{u}(t) + \bm{\lambda}]+ \bm{\eta}(t)^{\top}[\bm{A}\bm{u}(t) + \bm{\lambda}].
$$
It is known that  $\frac{\partial L}{\partial \bm{u}}\Bigr|_{\substack{\bm{u} = \bm{u}^*(t)}} = 0$, leading to the equality $(\tilde{\bm{y}}(t)+\bm{\eta}(t))^{\top}\bm{A} = \bm{0}$. As $\bm{A}$ is invertible in our setting, we get $\tilde{\bm{y}}(t)+\bm{\eta}(t) = \bm{0}$. Finally, using the relation between the costate variables for the direct and the indirect method in a boundary interval, we get ${y}_i(t) = \tilde{{y}_i}(t) + {\eta_i}(t) = 0, \forall t \in (\tau_1, \tau_2)$ \citep{Hatl1995}. For more details on the direct and the indirect method and the associated optimality conditions, we refer readers to \citep{Sethi2019}.

\noindent
{\bf (b)} We define the value function $V(\bm{x})$ as the optimal objective value of Problem \eqref{eq:fluid} associated with the initial state $\bm{x}$. We use the fact that $V(\bm{x})$ is continuously differentiable \citep{bauerle2001discounted}, and the interpretation of the costate variable that $\bm{y}(t) = \frac{\partial V(\bm{x})}{\partial \bm{x}}\Bigr|_{\substack{\bm{x} = \bm{x}^*(t)}}$ \citep{Sethi2019}. Since $\bm{x}^*(t)$ is continuous in $t$, the continuity of $\bm{y}(t)$ follows from the continuity of the composition of continuous functions. Piecewise linearity is then straightforward from condition {(b)} in Lemma \ref{th:pontryagin} and statement {(a)} in Lemma \ref{th:costate}.


\noindent
{\bf (c)} This part is implied by the structure of $\bm{y}(t)$ given in the statements (a) and (b) of Lemma \ref{th:costate}.  Assuming that $x_i(0) > 0$, $y_i(t)$ initially decreases with the slope $-c_i$ until the depletion time $T_i$, at which $y(T_i) = 0$. For all $t \in [T_i,T]$, $y_i(t) =0 $. This fact is implied by condition (c) in Lemma \ref{th:pontryagin} that $\bm{y}(T) = 0$. If $x_i$ becomes positive again after $T_i$, the slope will become $-c_i$ as well, making $y_i(t)$ negative. Then, it becomes impossible to satisfy ${y}_i(T) = 0$. Hence, once $x_i$ becomes zero, it is kept to zero. A direct consequence is that for all $t \in [T_i,T]$, $y_i(t) =0 $ by (a) in Lemma \ref{th:costate}. Hence,  $y_i(t)$ is a linear function of $T_i$ for any $t \in [0,T]$. \qed
\end{pf}

The following theorem is our main theoretical result that generalizes the results by \citet{avrambertsimasricard} to general MFQNETs.

\begin{thm}\label{th:main}
For Problem \eqref{eq:fluid}, there exists a threshold type optimal policy where the threshold curves are hyperplanes passing through the origin. 
\end{thm}

\begin{pf}

Our proof is based upon the algorithm by \citet{avram1997optimal} to solve Problem \eqref{eq:fluid} with any given initial state. We prove that the solution this algorithm finds is a threshold type policy and the threshold curves are hyperplanes passing through the origin. 


 By condition (a) of Lemma \ref{th:pontryagin}, the optimal control at each time $t$ is decided by the priority index $r_{i}(t)$ defined for each job class $i \in [n]$, where $r_{i}(t) = [\bm{y}(t)^{\top}\bm{A}]_{i}$. Lemma \ref{th:costate} indicates that $r_{i}(t)$ is a continuous, piecewise linear function and its slope can only change at $\{T_1, \dots, T_n\}$.  At each server, the optimal policy is to put maximum effort to the job class with the smallest priority index, and put zero effort to the rest, without violating the non-negativity constraint on the state variables. When all the job classes at the server have positive priority indices, the optimal policy is to idle. This case can be captured by considering idling as a job class with the constant priority index 0. Hence, Lemma \ref{th:pontryagin} implies that as long as the rank of the priority indices does not change, the optimal control is a constant vector. 

The condition under which a server transfers effort from one class to another is defined by the equalities of the form $r_{j}(t) = r_{k}(t)$, given that class $j$ jobs and class $k$ jobs are processed by the same server. If this equality holds, then it is indifferent whether the server prioritizes class \textcolor{black}{$j$} or $k$. If this equality becomes inequality, then it would be beneficial to prioritize one class over the other. This description indicates that the optimal policy is a threshold type policy, and the threshold curves are defined by the equalities between the priority indices. 

We derive the condition that the priority is switched from class $j$ jobs to class $k$ jobs, starting from an initial state $\bm{x}_0$. Depending on the parameters $\bm{c}$ and $\bm{A}$, certain switches might not be always possible. Furthermore, the order of the depletion times $\{T_1, \dots, T_n\}$ and the future switches associated with the trajectory should be adequately decided as well (Specific examples of how $\bm{A}$, $\bm{c}$ and the order of $\{T_1, \dots, T_n\}$ can make a switch possible or not are given in \citep{avrambertsimasricard, avram1997optimal}). We assume that $\bm{A}$, $\bm{c}$, the order of $\{T_1, \dots, T_n\}$ and the switches associated with the trajectory are appropriately fixed. The specific order of $\{T_1, \dots, T_n\}$ leads to a collection of equalities between the indices $r_i(t)$
during the entire trajectory, and completely determines the optimal solution of the problem \citep{avrambertsimasricard, avram1997optimal}. 

Without loss of generality, we assume that the switch from class $j$ to $k$ happens at $t = 0$. The switching curve that we would like to derive is then $r_j(0) = r_k(0)$, where $r_j(0)$ and $r_k(0)$ 
 are both linear functions of $(T_1, \dots, T_n)$ due to Lemma \ref{th:costate}. We now prove that  $(T_1, \dots, T_n)$ is a linear function of $\bm{x}_0$, which verifies that $r_j(0) = r_k(0)$ represents a hyperplane passing through the origin in the state space.

 By definition, $T_i$ can be computed from the equation of the form $\int_{0}^{b_1} [\bm{A}\bm{u}(t) + \bm{\lambda}]_i  \,dt + \dots + \int_{b_{q}}^{T_i} [\bm{A}\bm{u}(t) + \bm{\lambda}]_i \,dt = -x_i$, where \textcolor{black}{$b_l, l \in [q] $}, represents a breakpoint in $[\bm{A}\bm{u}(t) + \bm{\lambda}]_i $ . The control $[\bm{A}\bm{u}(t) + \bm{\lambda}]_i$ is constant between the breakpoints, and the breakpoints $b_i$ are always the intersections between two indices. Any time of intersection between two indices can be expressed as a linear function of the vector $(T_1, \dots, T_n)$ due to Lemma \ref{th:costate}. Hence, the above equation leads to an equality between $x_i$ and a linear function of $(T_1, \dots, T_n)$. Likewise, the collection of equalities that we have all \textcolor{black}{lead} to equalities between $\bm{x}_0$ and linear functions of $(T_1, \dots, T_n)$. Rearranging these equalities leads to the expression of $(T_1, \dots, T_n)$ as a linear function of $\bm{x}_0$. \qed
\end{pf}


We use the term switching curve to denote threshold curves for Problem \eqref{eq:fluid}. The following Corollary \ref{th:oct} is the building block to develop a learning algorithm in Section \ref{sec:algorithm}.


\begin{cor}
\label{th:oct}
OCT-H can learn an optimal policy of Problem \eqref{eq:fluid}.
\end{cor}

\begin{pf}

By Theorem \ref{th:main}, there exist switching curves that are hyperplanes passing through the origin. As described in Section \ref{sec:oct}, OCT-H learns a decision tree that partitions the feature space with hyperplanes, and assigns a label to each region. Hence, it can naturally learn the switching curves and the optimal control at each region partitioned by the switching curves. 

Another condition to consider is whether $x_i(t) = 0$ for some class $i \in [n]$. If $x_i = 0$, then server splitting might occur to satisfy the non-negativity constraint on the state vector. We first note that the condition $x_i = 0$ is also a hyperplane in the state space passing through the origin. 

In general, decision trees are confined to use inequalities for node splits. In our context, however, equality conditions such as $x_i(t) = 0$ can be learned as the condition $x_i \leq 0$. Since the state vector is always non-negative, these two conditions are equivalent for Problem \eqref{eq:fluid}.
Hence, OCT-H can learn the optimal policy of Problem \eqref{eq:fluid} both in the interior and the boundary of the state space. \qed
\end{pf}

The following Corolloary \ref{th:scalar} \textcolor{black}{and Theorem \ref{th:scalar2}} will be used in Section \ref{sec:algorithm} to develop a more efficient learning algorithm.

\begin{cor}\label{th:scalar}
Consider two initial states $\bm{x}_0$ and $\alpha\bm{x}_0$, where $\alpha$ is some positive scalar. Then, $\bm{u}^{*}_{\bm{x}_0}(0) = \bm{u}^{*}_{\alpha\bm{x}_0}(0)$
\end{cor}

\begin{pf}
$\bm{x}_0$ and $\alpha\bm{x}_0$ lie in the same region defined by the switching curves, since the switching curves are hyperplanes passing through the origin. Thus, by Theorem \ref{th:main}, the optimal control at these two states are identical. \qed
\end{pf}

\color{black}

\begin{thm}\label{th:scalar2}
Consider a pair of optimal control and the associated state trajectory $\{(\bm{u}_{\bm{x}_0}^*(t), \bm{x}^*_{\bm{x}_0}(t)): t \in [0,T]\}$ and a positive scalar $\alpha$. Then, $\{(\bm{u}_{\bm{x}_0}^*(\frac{t}{\alpha}), \alpha \bm{x}^*_{\bm{x}_0}(\frac{t}{\alpha})): t \in [0,\alpha T]\}$ is optimal for Problem \eqref{eq:fluid} with the initial state  $\alpha \bm{x}_{0}$.
\end{thm}

\begin{pf}
The proof of this theorem follows  from the proof of Theorem 3 in \citep{bäuerle2002}. By Theorem 3 in  \citep{bäuerle2002}, we know $V(\alpha \bm{x}_0) = \alpha^2 V(\bm{x}_0)$ and also that $\{(\bm{u}_{\bm{x}_0}^*(\frac{t}{\alpha}), \alpha \bm{x}^*_{\bm{x}_0}(\frac{t}{\alpha})): t \in [0,\alpha T]\}$ is feasible. The objective cost associated with the pair $\{(\bm{u}_{\bm{x}_0}^*(\frac{t}{\alpha}), \alpha \bm{x}^*_{\bm{x}_0}(\frac{t}{\alpha})): t \in [0,\alpha T]\}$ is 
$$
\alpha \int_{0}^{\alpha T} \bm{c}^{\top}\bm{x}^*_{\bm{x}_0}(\frac{t}{\alpha}) \,dt  
= \alpha^2 \int_{0}^{T} \bm{c}^{\top}\bm{x}^*_{\bm{x}_0}(t) \,dt
= \alpha^2 V(\bm{x}_0).
$$
As this solution achieves the optimal objective cost and is also feasible, it is optimal. \qed
\end{pf}


\color{black}




\subsection{Algorithm}
\label{sec:algorithm}

 \textcolor{black}{We present an algorithm that utilizes OCT-H to learn an optimal policy for Problem \eqref{eq:fluid}. To ensure a more comprehensive and efficient learning process, we discuss several key considerations that have been taken into account while developing the algorithm.}  

Given Problem \eqref{eq:fluid} with the initial state $\bm{x}_0$, we can solve it to optimality using the algorithm proposed by \citet{evgenyweiss2021}. Once we solve it, we \textcolor{black}{obtain} the optimal control $\bm{u}_{\bm{x}_0}^*(t)$ and the optimal state trajectory $\bm{x}_{\bm{x}_0}^{*}(t)$ for the entire time interval $t \in [0,T]$. We choose $N \in \mathbb{N}$ elements $t_1, \dots, t_N$ from the interval $[0,T]$, and extract the corresponding state values $\bm{x}^*(t_1), \dots, \bm{x}^*(t_N)$ and the control values $\bm{u}^*(t_1), \dots, \bm{u}^*(t_N)$. The training data that we obtain from this procedure is $\{(\bm{x}_{\bm{x}_0}^*(t_i), \bm{u}_{\bm{x}_0}^*(t_i))\}_{i=1}^{N}$.

To ensure a comprehensive coverage of the state space, we generate multiple initial states and solve the associated Problem \eqref{eq:fluid} for each initial state. By considering multiple instances with different initial states, we can obtain a more diverse set of state trajectories. Instead of arbitrarily generating the initial states, we develop a more systematic approach. Assuming that there are $n$ job classes, there are $\binom{n}{1} + \dots + \binom{n}{n} = 2^{n} - 1$ possible cases of which classes among $n$ are non-empty (excluding the trivial case that the entire system is empty). We let $\mathcal{S} = \{s_1, s_2, \dots, s_{2^n - 1} \}$ be the set of such cases, where each element $s_i, i \in [2^n-1], $ represents a set of non-empty classes. For example, if $n=2$, then $\mathcal{S} = \Big\{\{1\},\{2\}, \{1,2\} \Big\}$. For each $s \in \mathcal{S}$, we generate values for the non-zero entries of the initial state specified in $s$ and fix the remaining entries to zero. This systematic approach ensures that the training data covers the state space seamlessly, including both the interior and the boundary regions.

Another consideration is that as $n$ increases, the number of hyperplanes required to describe the optimal policy can get prohibitively large. To address this issue, we propose training multiple decision trees, if necessary. Each data point in the training set corresponds to an element of $\mathcal{S}$, depending on which entries of the state vector are non-zero. Hence, once we define a partition of $\mathcal{S}$, this partition can also be used to partition the training set. Then, we train a decision tree for each partition of the training set. For example, if we define a partition of the set $\mathcal{S} = \Big\{\{1\},\{2\}, \{1,2\} \Big\}$ to be  $\mathcal{P} = \Bigg\{ \Big\{ \{1\},\{2\}    \Big\} ,\Big\{\{1,2\} \Big\}    \Bigg\}$, we train two decision trees. The first decision tree is trained using the state vectors where either the first or the second entry is zero. The second decision tree is trained using the state vectors that are strictly positive. Essentially, we are dividing the state space into multiple regions and learning the optimal policy for each region. This approach allows us to distribute the learning process across multiple decision trees, reducing the computational burden and enabling efficient training even when dealing with a large number of job classes.

\color{black}
Finally, we use Corollary \ref{th:scalar} and Theorem \ref{th:scalar2} for a more efficient data generation. According to Theorem \ref{th:scalar2}, solving Problem \eqref{eq:fluid} with the initial state $\bm{x}_0$ and solving it again with $\alpha \bm{x}_0$ would be redundant. This observation allows us to streamline the data generation process. Instead of generating initial states arbitrarily, we sample them uniformly at random from the unit sphere in the non-negative orthant. This approach ensures that we cover a diverse range of initial states while avoiding unnecessary repetitions. Furthermore, Corollary \ref{th:scalar} suggests that we can augment the training data $\{(\bm{x}_{\bm{x}_0}^*(t_i), \bm{u}_{\bm{x}_0}^*(t_i))\}_{i=1}^{N}$ by including additional data points $\{(\alpha\bm{x}_{\bm{x}_0}^*(t_i), \bm{u}_{\bm{x}_0}^*(t_i))\}_{i=1}^{N}$, possibly multiple times with varying $\alpha$ values. 

\color{black}
Algorithm \ref{alg:main} outlines the entire procedure more rigorously. We let $\mathcal{A}$ denote the set of $\alpha$ that we use to augment data. We let $\mathcal{P}$ denote the partition of  $\mathcal{S}$. We use $M$ to denote the number of initial states we sample for each element in $\mathcal{S}$. We use $\bm{x}_{[s]}$ to denote the entries of $\bm{x}$ in $s$. For a set $K = \{(\bm{x}_{\bm{x}_0}^*(t_i), \bm{u}_{\bm{x}_0}^*(t_i))\}_{i=1}^{N}$, we use $\alpha K$ to denote $\{(\alpha\bm{x}_{\bm{x}_0}^*(t_i), \bm{u}_{\bm{x}_0}^*(t_i))\}_{i=1}^{N}$. Without loss of generality, we assume that the order of the cells in the partition $\mathcal{P}$ is fixed and $\mathcal{P}_{[j]}$ is the $j_{th}$ cell of $\mathcal{P}$.


\paragraph{Remark} In the data generation phase, the choice of the algorithm to solve Problem \eqref{eq:fluid} is flexible as long as it can guarantee exact optimality. For example, the algorithm by \citet{LuoBertsimas} is known to handle problems with hundreds of constraints and variables \citep{robustfluid}. However, this algorithm outputs solutions that are near-optimal, not exactly optimal. This might lead to inaccurate training data, as the control vector associated with a state vector might not be its true optimal control. Thus, we use the algorithm by \citet{evgenyweiss2021}, which finds the exact optimal solution of Problem \eqref{eq:fluid}.



\begin{algorithm}
 \KwInput{$\bm{c}, \bm{A}, \bm{\lambda},\bm{D}, \{t_1, \dots, t_N\}, \mathcal{A}, \mathcal{S}, \mathcal{P}, M$}
\KwOutput{$|\mathcal{P}|$ classification trees with hyperplane splits.} 
\textbf{Initialization: $K_{\mathcal{S}}, K_1, \dots, K_{|\mathcal{P}|} \leftarrow \emptyset$} 

\vspace{3mm}


\textbf{1. Data Generation} \\
%  \For{$j \in [|\mathcal{P}|]$}{
%  $p \leftarrow \mathcal{P}_{[j]}$ \\
%   \For{$s \in p$}{
%   \For{$i \in [M]$}{
%     $\bm{x}_0 \leftarrow \bm{0} \in \mathbb{R}^n$\\
%     Sample a positive vector $\hat{\bm{x}}$ from the $|s|$ dimensional unit sphere. \\
%     $\bm{x}_{0[s]} \leftarrow \hat{\bm{x}}$ \\
%     Solve problem \eqref{eq:fluid} with the initial state $\bm{x}_0$, denote the optimal control and state trajectory as $\bm{u}^*(\cdot), \bm{x}^*(\cdot)$. \\
%     $K_j \leftarrow K_j \cup \{(\bm{x}_{\bm{x}_0}^*(t_i), \bm{u}_{\bm{x}_0}^*(t_i))\}_{i=1}^{N}$
% }
%   }

  
%   }
\For{$s \in \mathcal{S}$}{
  $j \leftarrow 1$ \\
  \While{$j \leq M$}{
    $\bm{x}_0 \leftarrow \bm{0} \in \mathbb{R}^n$\\
    Sample a positive vector $\hat{\bm{x}}$ from the $|s|$ dimensional unit sphere. \\
    $\bm{x}_{0[s]} \leftarrow \hat{\bm{x}}$ \\
    Solve Problem \eqref{eq:fluid} with the initial state $\bm{x}_0$. \\
    $K_{\mathcal{S}} \leftarrow K_{\mathcal{S}} \cup \{(\bm{x}_{\bm{x}_0}^*(t_i), \bm{u}_{\bm{x}_0}^*(t_i))\}_{i=1}^{N}$ \\
    $j \leftarrow j+1$
}
  }


\For{$(\bm{x}, \bm{u}) \in K_{\mathcal{S}}$}{
 $\hat{s} \leftarrow \{i \in [n]: x_i > 0\}$ \\

 \For{$j \in [|\mathcal{P}|]$}{
\For{$s \in \mathcal{P}_{[j]}$}{
\If{$s = \hat{s}$}{
${K}_j \leftarrow {K}_j \cup (\bm{x}, \bm{u})$
}




}
 }




}


  \vspace{3mm}

\textbf{2. Data Augmentation} \\
\For{$K \in \{K_1, \dots, K_{|\mathcal{P}|}\}$}{
  \For{$\alpha \in \mathcal{A}$}{
    $K \leftarrow K \cup \alpha K$
  }
  }

  \vspace{3mm}

\textbf{3. Training} \\
\For{$K \in \{K_1, \dots, K_{|\mathcal{P}|}\}$}{
Use OCT-H to train a classification tree on $K$.
}

  \caption{OCT-H for MFQNET control.}
 \label{alg:main}
\end{algorithm}




\subsection{Example}
\label{sec:example}

We provide two small examples to illustrate Algorithm \ref{alg:main}. For both examples, the closed-form expressions of the optimal policies are already known. We compare the policy learned by Algorithm \ref{alg:main} with the closed-form optimal policy to demonstrate that it can learn near-optimal policies. The first example is to demonstrate that Algorithm \ref{alg:main} can learn the optimal switching curve, and the second example is to demonstrate that it can learn the optimal server splitting policy when some job classes are empty. 

The first example is the criss-cross network considered by \citet{crisscross}. The criss-cross network is composed of three classes and two servers. Server 1 processes Class 1 and 2 jobs, and Server 2 processes Class 3 jobs. Class 1 and 2 jobs take external arrivals. After Class 1 jobs are processed at Server 1, they become Class 3 jobs and move to Server 2. After Class 2 and 3 jobs are processed, they leave the system. Its graphical representation is given in Figure \ref{fig:crisscross}. It is clear that $u_3(t) = 1$ as long as Server 2 is not empty. The problem is to choose which class to process at Server 1. For this example, we only demonstrate the case in which none of the classes are empty. In other words, we learn the optimal policy for the case $s = \{1,2,3\}$. We let $\bm{c} = \bm{e}$, $\lambda_1 = \lambda_3 = 0.5$, $\mu_1 = 1.5$, $\mu_2 =2$ and $\mu_3 = 1$. Under this set of parameters, the switching curve and the corresponding optimal policy of this network derived by \citet{avrambertsimasricard} are given in Table \ref{table:crisscross}. The closed-form expression of the switching curve is $x_1(t) = 6x_3(t)$. Other parameters we used for Algorithm \ref{alg:main} are $N = 1, t_1 = 0, \mathcal{A} = \{0.5, 1.5\}, \mathcal{P} = \bigg\{\big\{\{s_1,s_2,s_3\}\big\}, \dots \bigg\}$ and $ M = 1000$.

% Figure environment removed

% \begin{table*}\centering
% \ra{1.3}

% \begin{tabular}{lll}
% \toprule
%   Case & Conditions  & $\bm{u}(t)$    \\ 
% \midrule
% Case 1 & $c_1\mu_1 \leq c_3\mu_3$ & (0,1,1)\\ 
% \hline
% \multirow{2}{*}{Case 2} & $c_1\mu_1 \geq c_3\mu_3$& \multirow{2}{*}{(1,1,0)}\\ 
% &$c_1\mu_1 - c_3\mu_3 \geq  c_2\mu_1$& \\
% \hline
% \multirow{3}{*}{Case 3} & $c_1\mu_1 \geq c_3\mu_3$ & \multirow{3}{*}{(0,1,1)}\\
%  &  $c_1\mu_1 - c_3\mu_3 \leq  c_2\mu_1$ & \\
%   &   $\mu_1 \geq \mu_2$ & \\

% \hline 
% \multirow{4}{*}{Case 4} & $c_1\mu_1 \geq c_3\mu_3$ & \multirow{4}{*}{(1,1,0)}\\\
%  & $c_1\mu_1 - c_3\mu_3 \leq  c_2\mu_1$ & \\
%   &$\mu_1 \leq \mu_2$& \\
%    &$\frac{x_1(t)}{x_2(t)} \geq \frac{c_2\mu_1}{c_1\mu_1 - c_3\mu_3} \times \frac{\mu_1 - \lambda_1}{\mu_2 - \mu_1}$& \\

% \hline
% \multirow{4}{*}{Case 5} & $c_1\mu_1 \geq c_3\mu_3$ & \multirow{4}{*}{(0,1,1)}\\ 
%  & $c_1\mu_1 - c_3\mu_3 \leq  c_2\mu_1$ & \\
%   &$\mu_1 \leq \mu_2$& \\
%    &$\frac{x_1(t)}{x_2(t)} \leq \frac{c_2\mu_1}{c_1\mu_1 - c_3\mu_3} \times \frac{\mu_1 - \lambda_1}{\mu_2 - \mu_1}$& \\

% \bottomrule
% \end{tabular}
% \caption{Optimal policy for the criss-cross network.}
% \label{table:crisscross}
% \end{table*}


\begin{table*}\centering

\begin{tabular}{cc}
\toprule
Conditions  & $\bm{u}^*(t)$    \\ 
\midrule


$\frac{x_1(t)}{x_3(t)} \geq \frac{c_2\mu_1}{c_1\mu_1 - c_3\mu_3} \times \frac{\mu_1 - \lambda_1}{\mu_2 - \mu_1}$& $(1,0,1)$\\

\hline

$\frac{x_1(t)}{x_3(t)} \leq \frac{c_2\mu_1}{c_1\mu_1 - c_3\mu_3} \times \frac{\mu_1 - \lambda_1}{\mu_2 - \mu_1}$& $(0,1,1)$\\


\bottomrule
\end{tabular}
\caption{Optimal policy for the criss-cross network when $\bm{x}(t) > \bm{0}$.}
\label{table:crisscross}
\end{table*}


Figure \ref{fig:crisscross_tree} displays the decision tree learned by OCT-H, where each node contains the prediction made on that node. The decision tree that OCT-H learned predicts $\bm{u}^*(t) = (0,1,1)$ if $x_1(t)  \leq 5.93x_3(t) + 0.01$ and predicts $\bm{u}^*(t) = (1,0,1)$ if $x_1(t)  \geq 5.93x_3(t) + 0.01$. This closely resembles the optimal policy, exhibiting only minor numerical differences.


% Figure environment removed

 

The second example is the Rybko-Stolyar network studied in \citet{RybkoStolyar92}. This network is composed of four classes and two servers. Server 1 processes Class 1 and 4, and Server 2 processes Class 2 and 3 jobs. Class 1 and 3 jobs take external arrivals. After Class 1 jobs are processed, they become Class 2 job and move to Server 2. After Class 3 jobs are processed, they become Class 4 jobs and move to Server 1. After Class 2 and 4 jobs are processed, they exit the system. Its graphical representation is given in Figure \ref{fig:rybko}. We let $\bm{c} = \bm{e}$, $\lambda_1 = \lambda_3 = 1$, $\mu_1 = \mu_3 = 6$ and $\mu_2 = \mu_4 = 1.5$. Under this set of parameters, the optimal policy is to prioritize Class 2 and 4 jobs unless either one of them is empty. If any one of them is empty, server splitting occurs. For this example, we train a single decision tree to learn the optimal policy that covers the entire state space. The parameters we used for Algorithm \ref{alg:main} are $N = 1, t_1 = 0, \mathcal{A} = \{0.5, 1.5\}, \mathcal{P} = \{\mathcal{S} \}$ and $ M = 1000$. 

% Figure environment removed

Figure \ref{fig:rs_tree} displays the decision tree learned by OCT-H. Since decision trees are confined to use inequalities for node splits, we can observe that the condition $x_i = 0$ for some $i \in [4]$ is learned as $x_i \leq \epsilon$ for a number $\epsilon$ with small absolute value. As the state vectors are always non-negative, these two conditions are effectively equivalent. Additionally, when both Class 2 and 4 are non-empty, the decision tree prioritizes them. When either one of them is empty, server splitting occurs. This policy aligns with the description provided earlier. To assess the quality of this policy when server splitting occurs, we generated a test set following the same procedure as the training set but with $M = 200, \mathcal{A} = \{5\}$. The classification accuracy was  $100\%,$ implying that OCT-H learned a high-quality policy that is empirically optimal. 

% % Figure environment removed



% Figure environment removed







% \begin{table*}\centering
% \ra{1.3}

% \begin{tabular}{ll}
% \toprule
%    Conditions  & $\bm{u}(t)$    \\ 
% \midrule
%   $x_2(t) , x_4(t) > 0$ & $(0,1,0,1)$   \\  \hline
%  $x_1(t) > 0, x_2(t), x_3(t), x_4(t) = 0$  &  $(1 - \frac{\lambda_3}{\mu_4}, 1 - \frac{\lambda_3}{\mu_3}, \frac{\lambda_3}{\mu_3}, \frac{\lambda_3}{\mu_4})$  \\  \hline
%  $x_1(t), x_4(t) > 0, x_2(t) = 0$  &  $(\frac{\mu_2}{\mu_1}, 1 , 0,1- \frac{\mu_2}{\mu_1})$  \\  \hline
%   $x_2(t), x_3(t) > 0, x_4(t) = 0$ &  $(0, 1 - \frac{\mu_4}{\mu_3}, \frac{\mu_4}{\mu_3}, 1)$   \\  \hline
%  $x_1(t), x_2(t) = 0, x_3(t),x_4(t) > 0$  &  $( \frac{\lambda_1}{\mu_1}, \frac{\lambda_1}{\mu_2},1 - \frac{\lambda_1}{\mu_2}, 1 - \frac{\lambda_1}{\mu_1})$   \\  \hline
%  $x_1(t), x_2(t) > 0, x_3(t),x_4(t) = 0$   &  $(1 - \frac{\lambda_3}{\mu_4}, 1 - \frac{\lambda_3}{\mu_3}, \frac{\lambda_3}{\mu_3}, \frac{\lambda_3}{\mu_4})$  \\  \hline
%  $x_1(t), x_2(t), x_3(t) = 0, x_4(t) > 0$  &  $(\frac{\lambda_1}{\mu_1}, \frac{\lambda_1}{\mu_2}, \frac{\lambda_3}{\mu_3}, 1 - \frac{\lambda_1}{\mu_1})$  \\  \hline
%  $x_1(t),x_3(t), x_4(t) = 0, x_2(t) > 0$  &  $( \frac{\lambda_1}{\mu_1}, 1 - \frac{\lambda_3}{\mu_3}, \frac{\lambda_3}{\mu_3}, \frac{\lambda_3}{\mu_4})$  \\  \hline
%   $x_1(t), x_3(t) > 0, x_2(t),x_4(t) = 0$ &  $(\frac{\mu_2}{\mu_1+\mu_2}, \frac{\mu_1}{\mu_1+\mu_2}, \frac{\mu_4}{\mu_1+\mu_2}, \frac{\mu_3}{\mu_1+\mu_2})$  \\  \hline
%   $x_1(t), x_2(t), x_4(t) = 0, x_3(t) > 0$ & $( \frac{\lambda_1}{\mu_1}, \frac{\lambda_1}{\mu_2}, 1 - \frac{\lambda_1}{\mu_2}, 1 - \frac{\lambda_1}{\mu_1})$   \\  

% \bottomrule
% \end{tabular}
% \caption{Optimal policy for the Rybko-Stolyar network. }
% \label{table:rybko}
% \end{table*}




\section{Computational Experiments}
\label{sec:numerical experiment}

This section presents the findings of computational experiments conducted on MFQNETs with varying sizes. We analyze the accuracy of the policy learned by Algorithm \ref{alg:main} and compare its online application speed with that of the algorithm by \citet{evgenyweiss2021}. In addition, we provide insights on the optimal policy of MFQNET control problems by presenting some of the actual decision trees. We also apply OCT-H with sparsity on several MFQNET problems and analyze the impact of sparsity on the performance and the resulting decision tree. The networks in this section are taken from \citep{robustfluid} and \citep{evgenyweiss2021}. 

\subsection{Experiment Setting}

We consider a reentrant network with $m$ servers and $3m$ classes of jobs. Each Server $i \in [m]$ processes jobs of Classes $3(i-1)+1,3(i-1)+2$ and $3(i-1) + 3$. Only Class 1 jobs take external arrivals with the arrival rate $\lambda_1$. Class $3(i-1) + 1$ jobs become Class $3i + 1$ until they become Class $3(m-1) + 1$. After Class $3(m-1) + 1$ jobs are processed, they change to Class 2 and enter Server 1. Class $3(i-1)+2$ jobs become Class $3i + 2$ jobs until they become Class $3(m-1) + 2$. After  Class $3(m-1) + 2$ jobs are processed, they change to Class 3 to enter Server 1. Class $3(i-1)+3$ jobs become $3i+3$ jobs, until they become Class $3m$ and exit the system after processed. We provide a graphical representation in Figure \ref{fig:reentrant}. The parameters $\bm{\lambda}, \bm{\mu}, \bm{c}$ are randomly generated using the software by \citet{evgenyweiss2021}.

% We also consider another set of reentrant networks with $m$ servers and $3m$ classes of jobs, but with randomly generated $\bm{D}$. In this set of networks, the sequence of stations that a job visits until it exits the system is different for every network we generated.



% Figure environment removed

We test Algorithm \ref{alg:main} on the reentrant networks with varying $m$. We treat all $2^{3m} - 1$ cases of non-zero entries separately. Using the formalism in Section \ref{sec:algorithm}, we let $\mathcal{P} = \big\{\{s_1\},\{s_2\}, \dots, \{s_{2^{3m}-1}\}\big\}$. Instead of exhaustively demonstrating our approach on the entire cells of $\mathcal{P}$, we randomly choose three cells and learn the optimal policy for each cell. We always include the case where non of the classes are empty. 
After we fix some $s \in \mathcal{S}$, we generate a training set with  $N = 1, t_1 = 0, \mathcal{A} = \{ 0.5, 0.75, 5\}$ and $M = 10000$. We generate a test set with $N = 1, t_1 = 0, \mathcal{A} = \{ 10\}$ and $M = 2000$. We report the classification accuracy of OCT-H on the test set. For each instance in the test set, we also measure the time it takes to solve the problem using the algorithm by \citet{evgenyweiss2021}, and divide it by the time it takes for the trained decision tree to make a prediction. We report the mean of the ratios rounded to the nearest integer as the relative speed-up of Algorithm \ref{alg:main}. 

Software for OCT-H is available at \citet{InterpretableAI}. We tune the maximum depth of the tree by grid searching over the list [3,5,10]. Public implementation of the algorithm by \citet{evgenyweiss2021} is available at https://github.com/IBM/SCLPsolver. When we use this implementation, we set the zero entries in $\bm{\lambda}$ to a small number $10^{-6}$ instead of $0$, as we have observed that this results in better numerical stability. The experiments were executed on a MacBook Pro with 2.6 GHz Intel Core i7 CPU and 16GB of RAM, except for the training part. We trained decision trees on MIT Engaging Computing Cluster with Dell C6300,
2 socket Intel E5-2690v4 processor, 14 Cores per CPU and 128 GB RAM.






\subsection{Speed and Accuracy}
\label{sec:speed}

\begin{table*}\centering


\begin{tabular}{cccccc}
\toprule
$m$ & $|s|$  & $|\{\bm{u}^*\}|$ & Training Time (hours:minutes) & Speed-up & Classification Accuracy (\%)    \\ 
\midrule


\multirow{3}{*}{3}& 9 &3& 00:30 & 153 & 100 \\

& 7& 3& 00:38& 196 & 100 \\

& 5& 3& 00:48 & 125 & 100 \\



\hline

\multirow{3}{*}{7}& 21&4 & 01:26 & 255 &100  \\

& 9&4& 00:34& 151 &100 \\

& 7& 4& 00:34 &  245 & 100 \\

\hline

\multirow{3}{*}{8}& 24& 4& 00:28 & 296&100 \\

& 12 &4& 00:44& 278 & 100 \\

& 7& 4& 00:23 & 313 & 100 \\



\hline 

\multirow{3}{*}{9}& 27& 4& 00:30 &   364 &100 \\

& 24 &2& 00:33 &  360 &100  \\

& 7& 2& 00:40 &  343 & 100 \\

\hline

\multirow{3}{*}{14}& 42& 6& 03:50 & 781  &100 \\

& 36 & 6& 04:34 & 790  & 100  \\


& 7& 4& 04:20 & 661 & 100 \\


\hline

\multirow{3}{*}{20}& 60& 9& 46:20 &  700 &100 \\

& 30 &9 & 44:10 &  599 &100  \\

& 7&  9& 40:24 &  628 & 100 \\

\hline

\multirow{3}{*}{33}& 99 & 9& 48:20 & 6014 &100 \\

& 51 &9&  47:30 &  994  &100  \\

& 45& 9& 42:20 &  982  & 100 \\



\bottomrule
\end{tabular}
\caption{Experiment results for the reentrant network.}
\label{table:reentrant}
\end{table*}





In Table \ref{table:reentrant}, we report the results of numerical experiments, focusing on the speed and accuracy of Algorithm \ref{alg:main}. In the second column, we report the number of non-zero entries of the state vector, denoted by $|s|$. In the third column, we report the number of distinct labels for the classification task, denoted by $|\{\bm{u}^*\}|$. In the rest of the columns we report $m$, the training time for OCT-H, the relative speed-up of Algorithm \ref{alg:main} and the out-of-sample classification error on the test set, rounded to the third decimal place.

\paragraph{Observations from Table \ref{table:reentrant}}
\begin{itemize}
    \item Algorithm 1 achieves perfect accuracy regardless of the size of the network, the number of unique labels and the number of non-zero entries. 
    \item The training time for OCT-H takes at most 48 hours in our experiment, suggesting that data generation and training might take hours to days in practice. 
    \item Once a policy is learned, Algorithm \ref{alg:main} is significantly faster than the algorithm by \citet{evgenyweiss2021}, with a speed-up ranging from hundreds to thousands of times faster in our experiment. In general, this relative speed-up becomes even greater as the dimension of the state space gets higher.
    \item As the dimension of the state space gets higher, the number of distinct labels do not increase significantly. This observation suggests that even for high dimensional problems, the structure of the optimal policy might be simple enough to be learned by OCT-H with shallow decision trees.
\end{itemize}

\subsection{Interpretability}
\label{sec:interpretability}

We now provide the decision tree for a problem solved in Section \ref{sec:speed} and develop insights on the structure of the learned policy. Although not all of the node splits have straightforward interpretations, we highlight a few splits that make intuitive sense.  

Due to space concerns, we display the prediction targets in the tree figures as the list of job classes that are prioritized, rather than the optimal control vector $\bm{u}^*$ itself. The job classes that are prioritized receive effort 1, and the rest of the job classes receive effort 0. In addition, we assign a number to each node split and provide a separate table that contains information on the hyperplane for each split. 

Furthermore, we introduce a vector ${\bm{c}}/{\bm{\mu}}$ that offers an interesting interpretation on the learned policy. This vector captures the relative cost of holding each job class in terms of their service rate. A higher value in this vector indicates that the corresponding job class poses a greater challenge to the fluid network controller.

In Figure \ref{fig:21_tree}, we provide the decision tree for the problem with $m = 7$, confined to strictly positive state vectors ($|s| = 21$). In Table \ref{table:nodesplit_reentrant_21}, we provide the node split information associated with the decision tree. For this problem, the parameters rounded to the third decimal place are
\begin{align*}
    & \bm{\mu} = (0.143, 0.253, 0.002, 0.287, 0.169, 0.278, 0.22, 0.11, 0.207, 0.216, 0.299, 0.004, 0.185, 0.205,\\
    & 0.25,0.268, 0.027, 0.028, 0.245, 0.168, 0.248), \\
    & \bm{c} = (0.705, 0.235, 0.972, 0.968, 0.719, 0.107, 1.484, 1.395, 0.493, 0.746, 1.584, 1.512, 0.07, 0.892, \\
       & 1.255, 0.305, 1.941, 1.496, 0.643, 1.021, 1.975).
\end{align*}
After we compute $\bm{c}/\bm{\mu}$ and sort it in descending order, the resulting indices in the sorted order is
$$(3,12,17,18,8,21,7,20,11,15,1,14,5,10,4,19,9,16,2,6,13).
$$

% Figure environment removed

\begin{table*}\centering


\begin{tabular}{cc}
\toprule
Node split number & Hyperplane     \\ 
\midrule


1&  $0.144x_1 - 0.002x_4 + 0.17x_7 - 0.05x_{10} - 0.0005$ \\

\hline

2& $-0.096x_4 + 1.036x_7$ \\

\hline

\multirow{3}{*}{3}& $-732.7x_1 - 34.84x_{2} - 5.311x_{3} - 1989.5x_4 + 19.1x_{5}   $  \\

& $+ 14304.6x_7 + 4.175x_{8} + 2939.7x_{10} +33.72x_{11}+ 62.25x_{12}$ \\

& $- 6.67x_{13} - 12.42x_{14} + 1.382x_{15} + 8.84x_{16} - 2.794x_{19} + 12.92x_{20}$ \\

\hline

4& $-2711.2x_3 + 0.5262x_{12} - 0.0001$  \\





\bottomrule
\end{tabular}
\caption{Node split information on the decision tree in Figure \ref{fig:21_tree}.}
\label{table:nodesplit_reentrant_21}
\end{table*}

\paragraph{Observations from Figure \ref{fig:21_tree}}

\begin{itemize}
    \item Class 3,7,12,17,21 jobs are always prioritized, regardless of the node. These job classes are often the highest ranking classes within their respective server in terms of the value in the vector ${\bm{c}}/{\bm{\mu}}$. The only exception is Class 8, as Class 8 is not prioritized even though it is ranked the highest in its server. This observation implies that the learned policy for this network is to drain the ``toughest"  job classes from the system first.
    \item The only difference in the nodes is whether to process Class 4 jobs or idle Server 2. See split 1 and 2, for example. If $x_4$ is relatively large compared to a linear combination of $x_1, x_7, x_{10}$, and if $x_4$ is again relatively large compared to $x_7$, the decision is to process Class 4 jobs instead of idling Server 2. However, after traversing the left edge in split 1, if $x_7$ turns out to be too large compared to $x_4$, Class 4 jobs are not processed. A possible explanation is that as Class 4 jobs become Class 7 after processed, it might be beneficial to idle Server 2 in case there are too many Class 7 jobs waiting in the queue.
    \item See split 3. If we focus on the terms associated with $x_4$ and $x_7$, again the decision is to process Class 4 jobs if $x_4$ is relatively large compared to $x_7$. The same interpretation as above can be applied to this decision.
    
\end{itemize}


\subsection{OCT-H with sparsity}


In this experiment, we apply OCT-H with sparsity instead of OCT-H in Algorithm \ref{alg:main} on a subset of the problems solved in Section \ref{sec:speed}. As mentioned in Section \ref{sec:oct}, OCT-H with sparsity often results in more interpretable decision trees compared to OCT-H. The purpose of this experiment is to analyze the price we have to pay in order to gain more interpretability. We vary the proportion of the total number of states allowed to be used for splits, denoted by sparsity parameter. We analyze how the sparsity parameter affects the training time and the classification accuracy on the test set. Table \ref{table:reentrant_sparsity} provides the experiment results, where the same notations as Table \ref{table:reentrant} are used. We summarize our findings in the following.

% Figure environment removed

\begin{table*}\centering


\begin{tabular}{cc}
\toprule
Node split number & Hyperplane     \\ 
\midrule


1& $-0.316x_4 + 3.398x_7$  \\

\hline

2& $-0.034x_1 - 0.103x_4 + 0.753x_7 + 0.152x_{10} + 0.0003x_{11}$ \\



\bottomrule
\end{tabular}
\caption{Node split information on the decision tree in Figure \ref{fig:21cl_sparse}.}
\label{table:nodesplit_reentrant_21_sparse}
\end{table*}


\begin{table*}\centering


\begin{tabular}{ccccc}
\toprule
$m$ & $|s|$ & Sparsity Parameter  & Training Time (hours:minutes) & Classification Accuracy (\%)    \\ 
\midrule



\multirow{2}{*}{7}& \multirow{2}{*}{21}& 0.5 & 00:48 & 99.8   \\

&  & 0.25 &  00:35 & 99.8  \\




\hline 

\multirow{2}{*}{9}& \multirow{2}{*}{27} & 0.5& 00:08 &   100  \\

&  & 0.25 & 00:07 & 100  \\


\hline

\multirow{2}{*}{14}& \multirow{2}{*}{42}& 0.5 & 02:52 & 98.3   \\

&  & 0.25 & 00:58 &   97 \\




\hline

\multirow{2}{*}{20}& \multirow{2}{*}{60}& 0.5 & 27:28 &  95.3  \\

&  & 0.25 & 14:20 & 94   \\


\hline

\multirow{2}{*}{33}& \multirow{2}{*}{99} &  0.5 & 26:51 &  94 \\

&  & 0.25 &  14:14 &  94    \\




\bottomrule
\end{tabular}
\caption{Experiment results for the reentrant network using OCT-H with sparsity.}
\label{table:reentrant_sparsity}
\end{table*}


\paragraph{Observations from Table \ref{table:reentrant_sparsity} }

\begin{itemize}
    \item In general, classification accuracy slightly degrades as sparsity parameter gets smaller. However, classification accuracy never gets below 94\% in our experiment, suggesting that OCT-H with sparsity can still learn high-quality policies.
    \item Training becomes faster as the sparsity parameter gets smaller. For the sparsity parameter 0.25, training can be around 4 times faster than OCT-H.
\end{itemize}

We provide the decision tree for the problem with $m=7, s = 21$ and the sparsity parameter 0.25 in Figure \ref{fig:21cl_sparse}. We compare this tree with the tree in Figure \ref{fig:21_tree}, which is learned by OCT-H on the same problem. Note that OCT-H with sparsity achieves 99.8 \% accuracy on this problem, which is only 0.2 \% decrease compared to OCT-H. Node split information is given in Table \ref{table:nodesplit_reentrant_21_sparse}.  

\paragraph{Observations from Figure \ref{fig:21cl_sparse} }


\begin{itemize}
    \item The states used for the splits are a strict subset of the states used for the splits in Figure \ref{fig:21_tree}. 
    \item The learned policy is also qualitatively similar to the policy learned with OCT-H. For example, in split 1, if $x_4$ is relatively large compared to $x_7$, the decision is to process Class 4 jobs. In split 2, if we focus on the terms associated with $x_4$ and $x_7$, again the decision is to process Class 4 jobs if $x_4$ is relatively large compared to $x_7$. Else, we idle Server 2 so that the queue on Class 7 jobs do not increase.
\end{itemize}








\section{Conclusions}
\label{sec:conclusion}

We presented an approach to solve MFQNET control problems using OCT-H. We proved that MFQNET control problems have threshold type optimal policies, and the threshold curves are hyperplanes passing through the origin. Based on this result, we developed an algorithm to use OCT-H to learn the optimal policy of MFQNET control problems. Computational experiments demonstrate that OCT-H can learn empirically optimal policies of MFQNET control problems with varying sizes. Once the policy is learned, we can solve MFQNET control problems considerably faster than the state-of-the-art algorithm by \citet{evgenyweiss2021}. Furthermore, we demonstrated that the simple decision tree structure enables us to develop insights on large dimensional MFQNET control problems.





 









%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{model5-names} 
\bibliography{reference}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
