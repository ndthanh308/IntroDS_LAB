
\newcommand{\heightDiffResSupp}{13ex}
\newcommand{\makeRowResultsSupp}[1]{\subfloat{% Figure removed}
                                \hfill
                                \subfloat{% Figure removed}
                                \hfill
                                \subfloat{% Figure removed}
                                \hfill
                                \subfloat{% Figure removed}
                                \hfill
                                \subfloat{% Figure removed}}
\newcommand{\makeResultsFigSupp}[7]{% Figure environment removed
}
\newcommand{\heightPatchResSupp}{0.16\linewidth}%
\newcommand{\makePatchRowSupp}[1]{
    \subfloat{% Figure removed}
    \hfill
    \subfloat{% Figure removed}
    \hfill
    \subfloat{% Figure removed}
    \hfill
    \subfloat{% Figure removed}
    \hfill
    \subfloat{% Figure removed}
    \hfill
    \subfloat{% Figure removed}
}%
\newcommand{\makePatchesFigSupp}[7]{% Figure environment removed}%
\newcommand{\makePatchesFigHallucinationSupp}[7]{% Figure environment removed}%




\section*{Network Architecture}
A schematic diagram of the whole network is given in \cref{fig:methods:network} in the main text and enlarged in \cref{supp_network}. Below is a detailed description of the network architecture, from the UNET encoder-decoder to the kernel estimation block and the offset block.

First, we describe the UNET encoder-decoder.
We use a tensor with $\nFrames$ channels of gray-level frames as the input to the network. The input tensor undergoes a $3\times 3$ convolution (conv) layer that encodes it from $\nFrames$ channels to $\nChannels$ channels without any activation function. The encoded features then pass through the encoder and decoder parts of the network, where the number of channels is multiplied by a factor of $\scaleFactor$ at each level. The encoder and decoder blocks consist of three $3\times 3$ conv-layers each. The first two layers in each block have GeLU~\cite{gelu} and batch normalization (norm)~\cite{batchNorm}, while the last layer has neither activation nor norm. The last layer in each block produces $\nChannels\times\scaleFactor^i$ channels, where $i$ is the level index. The encoder block also applies an average pooling layer with a $\scaleFactor\times\scaleFactor$ window and stride $\scaleFactor$ to reduce the spatial resolution, while the decoder block uses a pixel shuffle layer~\cite{shuffle_block} with an upsample factor of $\scaleFactor$ to increase it. We concatenate the encoder block output before pooling with the pixel shuffle output at each level to feed it into the decoder block. The encoder and decoder block structures are shown in \cref{table:nn_architecture} in the main text.

After the last decoder block in the UNET, we add a kernel-estimation block composed of three $1\times1$ conv layers. The first two layers have GeLU, and the last layer has no activation. The kernel estimation block structure is shown in \cref{table:kpnBlock} in the main text. 
The block outputs $\nFrames\times K\times K$ channels. We reshape the output of this block as kernels of size $K\times K$ for each frame. We then sample a patch of size $K\times K$ around each pixel in each frame and compute the inner product of the corresponding kernel and patch, as in the first term of \cref{eq:methods:KPN} in the main text. We sum the inner products from all frames for each pixel.

To map the temperature estimation to the camera range, we use an offset block that takes the means of all input gray-level frames as input and outputs a single scalar. 
The offset block is a fully connected layer that acts as a polynomial function of the input. The offset block is explained in more detail in \cref{sec:methods:net} in the main text.

The final temperature estimation is obtained by adding the offset scalar to the pixel-wise summation of the gain from the kernel estimation block.

The scale factor for decoder and encoder blocks is $\scaleFactor\equiv2$ and the number of channels is $\nChannels\equiv64$ throughout the work. 
The number of levels was empirically set to $4$.

% Figure environment removed
\begin{table*}[h]
    \caption{Architecture of the encoder and decoder blocks.}
    \centering
    \begin{tabular}{c c c c c}
        \hline
        \textbf{Type} & \textbf{BN}~\cite{batchNorm} & \textbf{GeLU}~\cite{gelu} & \textbf{Kernel} & \textbf{Output} \\ [0.5ex]
        \hline
        Conv2D & $\surd$ & $\surd$ & 3 & $\nChannels \times h \times w$ \\
        Conv2D & $\surd$ & $\surd$ & 3 & $\nChannels \times h \times w$ \\
        Conv2D & $\times$ & $\times$ & 3 & $(\scaleFactor \times \nChannels) \times h \times w$ \\
        \hline
    \end{tabular}
    \label{table:nn_architecture}
\end{table*}
\begin{table*}[h]
    \caption{Architecture of the kernel predictor block.}
    \centering
        \begin{tabular}{c c c c}
            \hline
            \textbf{Type} &  \textbf{GeLU}~\cite{gelu} & \textbf{Kernel} & \textbf{Output} \\ [0.5ex]
            \hline
            Conv2D & $\surd$ & 1 & $\nChannels \times h \times w$ \\
            Conv2D & $\surd$ & 1 & $\nChannels \times h \times w$ \\
            Conv2D & $\times$ & 1 & $ (\nFrames\times K^2) \times h \times w$ \\
            \hline
        \end{tabular}
    \label{table:kpnBlock}
\end{table*}


\section*{Figure list}
\begin{enumerate}
    \item \cref{supp_realdata} shows more results of the proposed method on real data.
    \item \cref{supp_diff_7,supp_diff_8,supp_diff_9,supp_diff_10,supp_diff_11} display the mean absolute error per pixel as a function of number of frames, both quantitatively and qualitatively.
    \item \cref{supp_patch_1,supp_patch_2,supp_patch_3,supp_patch_4,supp_patch_5,supp_patch_6} compare the results of the proposed method to ADMIRE~\cite{Tendero12}, DeepIR~\cite{Saragadam2021} and He~\cite{He2018} et al.'s methods. \cref{supp_patch_5,supp_patch_6} specifically show the hallucination effect of the DeepIR~\cite{Saragadam2021} method.
    \item \cref{supp_H,supp_I,supp_A,supp_O,supp_B,supp_M} are the original images used for the real data results in \cref{fig:results:realdata} in the main text. On the left of each figure is the ground truth (GT) temperature map acquired by the \scientificCamera, and on the right is the temperature map estimated by the proposed method. The raw data cannot be displayed because they consist of 7 frames. Both the GT and the estimated temperature map undergoes an histogram equalization to improve the visualization.
    \item \cref{supp_uav} shows the unmanned aerial vehicle (UAV) used for the real data experiments.
    \item \cref{tab:cameraParams} specifies the \taucamera\ parameters used throughout all of the experiments.
\end{enumerate}


\newcommand{\sizeRealDataSupp}{0.48}%
% Figure environment removed%

% In article NeveYaar_210520_1, NeveYaar_210520_2, MevoBytar_210818_2, Tzora_210523_9, NirEliyho_211005_0, NirEliyho_211005_10
\makeResultsFigSupp{180805_Peach_18}{Gilat_210809_0}{Gilat_210809_3}{Gilat_210809_4}{Gilat_210809_11}{Gilat_210809_18}{7}
\makeResultsFigSupp{Gilat_210809_19}{MevoBytar_210818_0}{MevoBytar_210818_3}{MevoBytar_210818_7}{MevoBytar_210818_8}{MevoBytar_210818_9}{8}
\makeResultsFigSupp{MevoBytar_210818_11}{MevoBytar_210818_13}{NeveYaar_210520_0}{NeveYaar_210520_5}{NeveYaar_210520_6}{NeveYaar_210520_9}{9}
\makeResultsFigSupp{NeveYaar_210520_14}{NirEliyho_211005_5}{NirEliyho_211005_9}{NirEliyho_211005_14}{NirEliyho_211005_16}{Tzora_210523_3}{10}
\makeResultsFigSupp{Tzora_210523_8}{Tzora_210523_14}{Tzora_210523_16}{YanivReshef_190816_7}{YanivReshef_190816_8}{YanivReshef_190816_11}{11}

% In article: 180725_Ramon_1, 180805_Peach_31, 180805_Peach_37, MevoBytar_210818_69, YanivReshef_190816_32
\makePatchesFigSupp{180725_Ramon_2}{180725_Ramon_4}{180725_Ramon_5}{180725_Ramon_7}{180725_Ramon_8}{180725_Ramon_10}{1}%
\makePatchesFigSupp{180725_Ramon_11}{180805_Peach_14}{180805_Peach_16}{180805_Peach_34}{180805_Peach_35}{180805_Peach_40}{2}%
\makePatchesFigSupp{180805_Peach_57}{180805_Peach_60}{180805_Peach_87}{180805_Peach_90}{180805_Peach_91}{Gilat_210809_37}{3}%
\makePatchesFigSupp{Gilat_210809_44}{Gilat_210809_260}{MevoBytar_210818_42}{MevoBytar_210818_82}{MevoBytar_210818_96}{NirEliyho_211005_560}{4}%
\makePatchesFigHallucinationSupp{NeveYaar_210520_14}{NeveYaar_210520_55}{NeveYaar_210520_148}{NeveYaar_210520_394}{NeveYaar_210520_421}{NeveYaar_210520_424}{5}%
\makePatchesFigHallucinationSupp{NeveYaar_210520_867}{NeveYaar_210520_966}{NeveYaar_210520_1033}{NeveYaar_210520_867}{NirEliyho_211005_169}{Tzora_210523_35}{6}%

% GT and estimated temperature maps
\newcommand{\sizeGTSupp}{0.48}%
\newcommand{\sizeEstSupp}{0.36}%
\newcommand{\makeGTSupp}[3]{
    % Figure environment removed
}
\makeGTSupp{H}{\cref{fig:results:realdata} (a)}\\
\makeGTSupp{I}{\cref{fig:results:realdata} (b)}\\
\makeGTSupp{A}{\cref{fig:results:realdata} (c)}\\
\makeGTSupp{O}{\cref{fig:results:realdata} (d)}\\
\makeGTSupp{B}{\cref{fig:results:realdata} (e)}\\
\makeGTSupp{M}{\cref{fig:results:realdata} (f)}


% Figure environment removed


\begin{table*}[h]
    \centering
    \caption{The FLIR \taucamera\ settings as described in Tau2 Quark Software IDD}
    \begin{tabular}{|c|c||c|c|}
        \hline
        Function   & State  & Function        & State                  \\
        \hline\hline
        FFC Mode   & Auto   & FPS             & 4 ($60_{Hz}$)          \\
        \hline
        FFC Period & 0      & CMOS Depth      & 0 ($14_{bit}$ w/o AGC) \\
        \hline
        Isotherm   & 0      & LVDS            & 0                      \\
        \hline
        DDE        & 0      & LVDS Depth      & 0 ($14_{bit}$)         \\
        \hline
        T-Linear   & 0      & XP              & 2 ($14_{bit}$)         \\
        \hline
        AGC        & Manual & Brightness Bias & 0                      \\
        \hline
        Contrast   & 0      & Brightness      & 0                      \\
        \hline
        ACE        & 0      & SSO             & 0                      \\
        \hline
        Gain       & High   &                 &                        \\
        \hline
    \end{tabular}
    \label{tab:cameraParams}
\end{table*}

