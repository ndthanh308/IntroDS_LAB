\section{Related Work}
\label{sec:related}
We surveyed various lines of research that our work draws upon, and grouped them into two main areas: \emph{i)} AI regulation and governance, and \emph{ii)} responsible AI practices and toolkits. 

\subsection{AI Regulation and Governance}
The landscape of AI regulation and governance is constantly evolving~\cite{jobin2019global, mittelstadt2016ethics}. At the time of writing, the European Union (EU) has endorsed new transparency and risk-management rules for AI systems known as the EU AI Act~\cite{eu_ai_act}, which is expected to become law in 2023. Similarly, the United States (US) has recently passed a blueprint of the AI Bill of Rights in late 2022~\cite{us_ai_bill}. This bill comprises \emph{``five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of AI.''} While both the EU and US share a conceptual alignment on key principles of responsible AI, such as fairness and explainability, as well as the importance of international standards (e.g., ISO), the specific AI risk management regimes they are developing are potentially diverging, creating an ``artificial divide''~\cite{ecfr}. The EU aims to become the leading regulator for AI globally, while the US takes the view that excessive regulation may impede innovation.

Notable predecessors to AI regulations include the EU GDPR law on data protection and privacy~\cite{eu_gdpr}, the US Anti-discrimination Act~\cite{us_anti_discrimination}, and the UK Equality Act 2010~\cite{uk_equality}. GDPR's Article 25 mandates that data controllers must implement appropriate technical and organizational measures during the design and implementation stages of data processing to safeguard the rights of data subjects. The Anti-discrimination Act prohibits employment decisions based on an individual's race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability, or genetic information. This legislation ensures fairness in AI-assisted hiring systems. Similarly, the UK Equality Act provides legal protection against discrimination in the workplace and wider society.

The National Institute of Standards and Technology (NIST), a renowned organization for developing frameworks and standards, recently published an AI risk management framework~\cite{nist2023aiRisk}. According to the NIST framework, an AI system is defined as \emph{``an engineered or machine-based system capable of generating outputs such as predictions, recommendations, or decisions that influence real or virtual environments, based on a given set of objectives. These systems are designed to operate with varying levels of autonomy.''} Similarly, the Principled Artificial Intelligence white paper from the Berkman Klein Center~\cite{fjeld2020principled} highlights eight key thematic trends that represent a growing consensus on responsible AI. These themes include privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and the promotion of human values.

As AI regulation and governance continue to evolve, AI practitioners are faced with the challenge of staying updated with the changing guidelines and regulations, requiring significant time and effort. Therefore, the focus of this work is to develop an adaptable methodology for generating responsible AI guidelines.

\subsection{Responsible AI Practices and Toolkits}
\subsubsection{Responsible AI Practices.} 
\label{sec:sub-raipractices}
A growing body of research, typically discussed in conferences with a long-standing commitment to human-centered design, such as the Conference on Human Factors in Computing Systems (CHI) and the Conference on Computer-Supported Cooperative Work and Social Computing (CSCW), as well as in newer conferences like the Conference on AI, Ethics, and Society (AIES) and the Conference on Fairness, Accountability, and Transparency (FAccT), focuses on the work practices of AI practitioners in addressing responsible AI issues. This strand of research encompasses various aspects of responsible AI, including fairness, explainability, sustainability, and best practices for data and model documentation and evaluation.

Fairness is a fundamental value in responsible AI, but its definition is complex and multifaceted~\cite{narayanan21fairness}. To assess bias in classification outputs, various research efforts have introduced quantitative metrics such as disparate impact and equalized odds, as discussed by Dixon et al.~\cite{dixon2018measuring}. Another concept explored in the literature is ``equality of opportunity,'' advocated by Hardt et al.~\cite{hardt2016equality}, which ensures that predictive models are equally accurate across different groups defined by protected attributes like race or gender.

Explainable AI (XAI) is another aspect of responsible AI. XAI involves tools and frameworks that assist end users and stakeholders in understanding and interpreting predictions made by machine learning models~\cite{arrieta2020explainable, kulesza2015principles, gunning2019xai,liao2021human, ehsan2020human, ibm2019fairness}. Furthermore, the environmental impact of training AI models should also be considered. Numerous reports have highlighted the significant carbon footprint associated with deep learning and large language models~\cite{sharir2020cost, hao2019training, strubell2019energy}. 

Best practices for data documentation and model evaluation have also been developed to promote fairness in AI systems. Gebru et al.~\cite{gebru2021datasheets} proposed ``Datasheets for Datasets'' as a comprehensive means of providing information about a dataset, including data provenance, key characteristics, relevant regulations, test results, and potential biases. Similarly, Bender et al.\cite{bender2018data} introduced ``data statements'' as qualitative summaries that offer crucial context about a dataset's population, aiding in identifying biases and understanding generalizability. For model evaluation, Mitchell et al.~\cite{mitchell2019model} suggested the use of model cards, which provide standardized information about machine learning models, including their intended use, performance metrics, potential biases, and data limitations. Transparent reporting practices, such as the TRIPOD statement by Collins et al.~\cite{collins2015transparent} in the medical domain, emphasize standardized and comprehensive reporting to enhance credibility and reproducibility of AI prediction models.\\

\subsubsection{Responsible AI Toolkits} Translating these practices into practical responsible AI is another area of growing research. New tools and frameworks are being proposed to assist developers in mitigating biases~\cite{bird2020fairlearn, gebru2021datasheets}, explaining algorithmic decisions~\cite{arya2019one}, and ensuring privacy-preserving AI systems~\cite{fjeld2020principled}.

Fairness auditing tools typically offer a set of metrics to test for potential biases, and algorithms to mitigate biases that may arise in AI models~\cite{saleiro2018aequitas, baeza2018bias}. For instance, Google's fairness-indicators toolkit~\cite{google2022fairness} enables developers to evaluate the distribution of datasets, performance of models across user-defined groups, and delve into individual slices to identify root causes and areas for improvement. IBM's AI Fairness 360~\cite{ibm2022ai} implements metrics for comparing subgroups of datasets (e.g., differential fairness and bias amplification~\cite{foulds2020intersectional}) and algorithms for mitigating biases (e.g., learning fair representations~\cite{zemel2013learning}, adversarial debiasing~\cite{zhang2018mitigating}). Microsoft's Fairlearn provides metrics to assess the negative impact on specific groups by a model and compare multiple models in terms of fairness and accuracy metrics. It also offers algorithms to mitigate unfairness across various AI tasks and definitions of fairness~\cite{fairlearn2022}.

Explainable AI systems are typically achieved through interpretable models or model-agnostic methods. Interpretable models employ simpler models like linear or logistic regression to explain the outputs of black-box models. On the other hand, model-agnostic methods (e.g., LIME~\cite{ribeiro2016should} or SHAP~\cite{lundberg2017unified}) have shown effectiveness with any model. IBM's AI Explainability 360 provides metrics that serve as quantitative proxies for the quality of explanations and offers guidance to developers and practitioners on ensuring AI explainability~\cite{ibm2022ai}. Another research direction introduced new genres of AI-related visualizations for explainability, drawing inspiration from domains such as visual storytelling, uncertainty visualizations, and visual analytics. Examples include Google's explorables, which are interactive visual explanations of the internal workings of AI techniques~\cite{google2022pair}; model and data cards that support model transparency and accountability (e.g., NVIDIA's Model Card++)\cite{nvidia2022}; computational notebook additions for data validations like AIF360\cite{ibm2022ai}, Fairlearn~\cite{fairlearn2022}, and Aequitas~\cite{saleiro2018aequitas}; and data exploration dashboards such as Google's Know Your Data~\cite{google2022know} and Microsoft's Responsible AI dashboard~\cite{microsoft2022aiLab}.

Ensuring privacy-preserving AI systems is commonly attributed to the practice of ``Privacy by Design''~\cite{cavoukian2009privacy, cavoukian2010privacy}, which involves integrating data privacy considerations throughout the AI lifecycle, particularly during the design stage to ensure compliance with laws, regulations, and standards~\cite{fjeld2020principled} such as the European General Data Protection Regulation (GDPR)~\cite{eu_gdpr}. IBM's AI Privacy 360 is an example of a toolkit that assesses privacy risks and helps mitigate potential privacy concerns. It includes modules for data anonymization (training a model on anonymized data) and data minimization (collecting only relevant and necessary data for model training) to evaluate privacy risks and ensure compliance with privacy regulations.

While many toolkits and frameworks emphasize the importance of involving stakeholders from diverse roles and backgrounds, they often lack sufficient support for collaborative action. Wong et al.~\cite{wong2023seeing} have also highlighted the ``mismatch between the promise of toolkits and their current design'' in terms of supporting collaboration. Collaboration is key to enhance creativity by allowing AI practitioners to share knowledge with other stakeholders. To address this gap, we aim to develop a set of actionable guidelines that will facilitate the engagement of a diverse range of stakeholders in AI ethics. By doing so, we hope to take a significant step forward in fostering collaboration and inclusivity within the field.