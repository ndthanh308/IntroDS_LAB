\section{Method for Generating Responsible AI Guidelines}
\label{sec:method}
To answer our \textbf{RQ\textsubscript{1}}, we followed a four-step process (Figure~\ref{fig:steps}), based on the methodology proposed by Michie et al.~\cite{michie2013behavior}. This process allowed us to identify the essential element of a guideline, referred to as the ``active ingredient,'' focusing on the ``what'' rather than the ``how''~\cite{michie2011strengthening}. A similar parallel can be drawn in software engineering, where the ``what'' represents the software requirements and the ``how'' represents the software design, both of which are important for a successful software product~\cite{aggarwal2005software}. However, by shifting the focus to the ``what,'' AI practitioners can develop a clearer understanding of the objectives and goals they need to achieve, fostering a deeper comprehension of complex underlying ethical concepts. Throughout this process, we actively engaged a diverse group of stakeholders, including AI engineers, researchers, and experts in law and standardization. As a result, we were able to develop a total of \textbf{22} responsible AI guidelines. \\

% Figure environment removed

\subsection{Compiling a List of Papers on Responsible AI}
In the first step, we compiled a list of key scientific articles focusing on responsible AI guidelines for AI practitioners (discussed in detail in \S\ref{sec:sub-raipractices}). We created this list by targeting influential papers published in renowned computer science conferences, such as the ACM CHI, CSCW, FAccT, AAAI/ACM Conference on AI, Ethics, and Society (AIES), and scientific literature from the medical domain (e.g., the Annals of Internal Medicine). Note that we did not conduct a systematic literature but rather relied on snowball sampling by identifying key publications related to the topic at hand. Overall, we identified 17 key papers that covered a broad range of responsible AI aspects, including fairness, explainability, sustainability, and best practices for data and model documentation and evaluation.

\subsection{Creating a Catalog of Responsible AI Guidelines From the Papers}
For each source, we compiled a list of techniques that could be employed to create responsible AI guidelines, focusing on the actions developers should consider during AI development. Following the methodology proposed by Michie et al.~\cite{michie2013behavior} (which was also used to identify community engagement techniques by Dittus et al.~\cite{dittus2017community}), we sought techniques that describe the ``active ingredient'' of what needs to be done. This means that the phrasing of the technique should focus on what developers need to do (\emph{what}), rather than the specific implementation details (\emph{how}). In total, we formulated a set of 16 techniques based on relevant literature sources~\cite{mitchell2019model, madaio2020co, dixon2018measuring, hardt2016equality, mitchell2018prediction, verma2018fairness, arrieta2020explainable, kulesza2015principles, fjeld2020principled, bender2018data, gebru2021datasheets, holland2018dataset, wang2020revise, collins2015transparent, sharir2020cost, hao2019training}.

For instance, a recommended practice for ensuring fairness involves evaluating an AI system across different demographic groups~\cite{madaio2020co, dixon2018measuring, hardt2016equality}. In this case, the technique specifies ``what'' needs to be done rather than ``how'' it should be implemented (e.g., using common fairness metrics such as demographic parity or equalized odds). We then conducted an iterative review of the collection of techniques to identify duplicates, which were instances where multiple sources referred to the same technique. For example, four sources indicated that data biases could affect the model~\cite{mitchell2019model, gebru2021datasheets, bender2018data, holland2018dataset}, emphasizing the need to report the characteristics of training and testing datasets. We consolidated such instances by retaining the specific actions to be taken (e.g., reporting dataset characteristics). This process resulted in an initial list of 16 distinct techniques. We provided a concise summary sentence for each technique, utilizing active verbs to emphasize the recommended actions for developers.\\

\subsection{Examining the Catalog With AI Developers and Standardization Experts Through Interviews}
The catalog of techniques underwent eleven iterations to ensure clarity and comprehensive thematic coverage. The iterations were carried out by two authors, with the first author conducting interviews with five AI researchers and developers. During the interviews, the participants were asked to consider their current AI projects and provide insights on the implementation of each technique, focusing on the ``how'' aspect. This served two purposes: firstly, to identify any statements that were unclear or vague, prompting suggestions for alternative phrasing; and secondly, to expand the catalog further. The interviews yielded two main recommendations for improvement: \emph{i)} mapping duplicate techniques to the same underlying action(s); and \emph{ii)} adding examples to support each technique.

In addition to the interviews, the two authors who developed the initial catalog conducted a series of six 1-hour workshops with two standardization experts from a large organization. The purpose of these workshops was to review the initial catalog for ISO compliance. The standardization experts examined six AI-related ISOs, including ISO 38507, ISO 23894, ISO 5338, ISO 24028, ISO 24027, and ISO 24368, which were developed at the time of writing (we provide a high-level summary\footnote{Note that the summary provided is a brief and simplified description due to a paywall restriction.} of each ISO next). The experts provided input on any missing techniques and mapped each technique in the initial catalog to the corresponding ISO that covers it. As a result of this exercise, six new techniques (\#2, \#7, \#12, \#13, \#14, \#21) were added to the catalog.\\

\noindent \textbf{ISO 38507 (Governance, 28 pages).} It provides guidance to organizations on how to effectively and responsibly govern the use of AI (e.g., identify potential harms and risks for each intended use(s) of the systems). It offers recommendations to the governing body of an organization, as well as various stakeholders such as executive managers, external specialists, public authorities, service providers, assessors, and auditors. The standard is applicable to organizations of all types and sizes, regardless of their reliance on data or information technologies, addressing both current and future uses of AI and their implications. 

\smallskip
\noindent \textbf{ISO 23894 (Risk Management, 26 pages).} It offers guidance to organizations involved in the development, production, deployment, or use of products, systems, and services utilizing AI to effectively manage AI-related risks (e.g., develop mechanisms for incentivizing reporting of system harms). It provides recommendations on integrating risk management into AI activities and functions, along with describing processes for the successful implementation and integration of AI risk management. The guidance is adaptable to suit the specific needs and context of any organization.

\smallskip
\noindent \textbf{ISO 5338 (AI Lifecycle Process, 27 pages).} It establishes a framework for describing the life cycle of AI systems that rely on machine learning and heuristic systems. It defines processes and concepts (e.g., through reporting of harms and risks, obtaining approval of intended uses) that enable the effective definition, control, management, execution, and enhancement of AI systems throughout their life cycle stages. These processes are applicable to organizations or projects involved in the development or procurement of AI systems, providing a structured approach to their development and implementation. 

\smallskip
\noindent \textbf{ISO 24028 (Trustworthiness, 43 pages).} It provides an overview of trustworthiness in AI systems, covering various aspects. It explores approaches to establish trust in AI systems through transparency, explainability, and controllability. It also addresses potential engineering pitfalls, associated threats, and risks to AI systems, offering mitigation techniques. Additionally, it discusses approaches to assess and ensure the availability, resiliency, reliability, accuracy, safety, security, and privacy of AI systems. However, it does not specify the levels of trustworthiness for AI systems. 

\smallskip
\noindent \textbf{ISO 24027 (Bias, 39 pages).} It focuses on bias (i.e., related to protected attributes, such as age, gender, and ethnicity, being used in the training of AI) in AI systems, particularly in the context of AI-aided decision-making. It provides techniques and methods for measuring and assessing bias, with the objective of identifying and addressing vulnerabilities related to bias. The standard covers all phases of the AI system lifecycle, encompassing data collection, training, continual learning, design, testing, evaluation, and use. 

\smallskip
\noindent \textbf{ISO 24368 (Ethical and Societal Concerns, 48 pages).} It offers a broad introduction to ethical and societal concerns related to AI. It provides information on principles, processes, and methods in this field and is aimed at technologists, regulators, interest groups, and society as a whole. It does not promote any specific set of values or value systems. Additionally, the document provides an overview of existing International Standards that tackle issues arising from ethical and societal concerns in AI. \\

While our method is comprehensive, it is important to note that the responsible AI guidelines were checked against six ISOs in their current form. However, the flexibility of our approach allows for amending or adding new responsible AI techniques as scientific literature advances and ISO standards evolve. At the time of writing, there are additional ISOs that are in the committee draft stage and can be included in the guidelines, such as ISO 42001 (AI management system), ISO 5469 (Functional safety), ISO 5259 (Data quality), ISO 6254 (Explainability), and ISO 12831 (Testing). 

\begin{table}
\caption{\textbf{Responsible AI guidelines.} 22 techniques that describe a responsible AI guideline (i.e., an actionable item that a developer should consider during the AI development lifecycle). These techniques are grounded in the scientific literature (main sources are reported), and were checked for ISO compliance: ISO 38507 (Governance); ISO 23894 (Risk management); ISO 5338 (AI lifecycle processes); ISO 24028 (Trustworthiness); ISO 24027 (Bias); ISO 24368 (Ethical considerations). They were also cross-referenced with the EU AI Act's articles~\cite{eu_ai_act_2022}, following guidance from~\cite{golpayegani2023high}. Each technique is followed by an example, and the techniques are categorized thematically into six categories, concerning the \emph{intended uses, harms, system, data, oversight, and team}. }
\label{tbl:techniques}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllll}
\toprule
\textbf{Number} & \textbf{Technique} & \textbf{Source(s)} & \textbf{ISO} & \textbf{AI Act}\\ \midrule
\multicolumn{2}{l}{\textbf{INTENDED USES}} &  &  \\
1 & \begin{tabular}[t]{l}Work with relevant parties to identify intended uses. \\ (e.g., identify the system's usage, deployment, and contextual conditions)\end{tabular} &
\begin{tabular}[t]{l} \cite{mitchell2019model} \end{tabular}
 & \begin{tabular}[t]{l} 5338, 38507, 23894, \\ 24027, 24368\end{tabular} & Art. 6\\

2 & \begin{tabular}[t]{l}Obtain approval from an Ethics Committee or similar body for intended uses.\\ (e.g., Obtain Ethics Committee approval for the intended use, aligned with sustainability goals)\end{tabular} & \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}38507, 5338, 23894\end{tabular} & Art. 6, 9 \\ 

\midrule
\multicolumn{2}{l}{\textbf{HARMS}} &  &  \\
3 & \begin{tabular}[t]{l}Identify potential harms and risks associated with the intended uses. \\  (e.g., prevent privacy violation, discrimination, and adversarial attacks, provide interpretable output)\end{tabular} & 
\begin{tabular}[t]{l} \cite{madaio2020co} \end{tabular} & \begin{tabular}[t]{l}23894, 24028, 38507, \\ 24368\end{tabular}  & Art. 9\\

4 & \begin{tabular}[t]{l}Provide mechanism(s) for incentivizing reporting of system harms. \\  (e.g.,provide contact emails and feedback form to raise concerns)\end{tabular} &
\begin{tabular}[t]{l} \cite{madaio2020co} \end{tabular} & \begin{tabular}[t]{l}38507, 23894\end{tabular} & Art. 9\\

5 & \begin{tabular}[t]{l}Develop strategies to mitigate identified harms or risks for each intended use. \\ (e.g., use stratified sampling and safeguards against adversarial attacks during training)\end{tabular} &
\begin{tabular}[t]{l} \cite{mitchell2019model} \end{tabular}
 & \begin{tabular}[t]{l}24368, 23894\end{tabular} & Art. 9\\ 

\midrule
\multicolumn{2}{l}{\textbf{SYSTEM}} &  &  \\
6 & \begin{tabular}[t]{l}Document all system components, including the AI models, to enable reproducibility and scrutiny. \\ (e.g., create UML diagrams, flowcharts, and specify model types, versions, hardware architecture)\end{tabular} & \begin{tabular}[t]{l} \cite{madaio2020co} \end{tabular} & \begin{tabular}[t]{l}5338, 23894, 24027\end{tabular} & Art. 15\\

7 & \begin{tabular}[t]{l}Review the code for reliability\\ (e.g., manage version control using software.)\end{tabular} & \begin{tabular}[t]{l} --- \end{tabular} & \begin{tabular}[t]{l}5338\end{tabular} & Art. 15\\

8 & \begin{tabular}[t]{l}Report evaluation metrics for various groups based on factors such as age, gender, and ethnicity. \\ (e.g., evaluate false positive/negative, AUC, and feature importance across protected attributes)\end{tabular} & 
\begin{tabular}[t]{l} \cite{madaio2020co, dixon2018measuring, hardt2016equality} \\ \cite{mitchell2018prediction, verma2018fairness}\end{tabular} & \begin{tabular}[t]{l}23894, 5338, 24028, \\ 24027\end{tabular} & Art. 10, 13, 15\\

9 & \begin{tabular}[t]{l} Provide mechanisms for interpretable outputs and auditing. \\ (e.g., output feature importance and provide human-understandable explanations)\end{tabular} & 
\begin{tabular}[t]{l} \cite{arrieta2020explainable, kulesza2015principles}  \end{tabular} & \begin{tabular}[t]{l}38507, 24028\end{tabular} & Art. 13\\

10 & \begin{tabular}[t]{l}Document the security of all system components in consultation with experts. \\ (e.g., guard against adversarial attacks and unauthorized access)\end{tabular} & \begin{tabular}[t]{l} \cite{fjeld2020principled} \end{tabular} & \begin{tabular}[t]{l}24028, 24368\end{tabular} & Art. 15\\

11 & \begin{tabular}[t]{l}Provide an environmental assessment of the system. \\ (e.g., report the number of GPU hours used in training and deployment)\end{tabular} &
\begin{tabular}[t]{l} \cite{sharir2020cost, hao2019training} \end{tabular}
 & \begin{tabular}[t]{l}38507, 23894,\\ 5338, 24368\end{tabular} & Art. 15, 17\\

12 & \begin{tabular}[t]{l}Develop feedback mechanisms to update the system. \\ (e.g., provide contact email, feedback form, and notification of new knowledge extracted)\end{tabular} & \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}24028\end{tabular} & Art. 15\\

13 & \begin{tabular}[t]{l}Ensure safe system decommissioning.\\ (e.g., ensure decommissioned data is either deleted or restricted to authorized personnel.)\end{tabular} & \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}38507, 24368\end{tabular} & Art. 10, 15\\

14 & \begin{tabular}[t]{l}Redocument model information and contractual requirements at every system update.\\  (e.g., update the model information when re-training the system or using datasets with new contractual requirements)\end{tabular} & \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}23894, 5338, 24368 \end{tabular} & Art. 10, 15\\ 

\midrule
\multicolumn{2}{l}{\textbf{DATA}} &  &  \\

15 & \begin{tabular}[t]{l}Ensure compliance with agreements and legal requirements when handling data. \\  (e.g., create data sharing and non-disclosure agreements and secure servers)\end{tabular} & \begin{tabular}[t]{l} --- \end{tabular} & \begin{tabular}[t]{l}38507, 23894, 5338 \end{tabular} & Art. 10\\ 

16 & \begin{tabular}[t]{l}Compare the quality, representativeness, and fit of training and testing datasets with the intended uses. \\  (e.g., report dataset details such as public/private, personal information, demographics, and data provenance)\end{tabular} & 
\begin{tabular}[t]{l}\cite{bender2018data, gebru2021datasheets, holland2018dataset, wang2020revise}\\ \cite{madaio2020co, mitchell2018prediction, verma2018fairness}\end{tabular}
& \begin{tabular}[t]{l}38507, 5338,\\ 24028, 24027\end{tabular} & Art. 10 \\

17 & \begin{tabular}[t]{l}Identify any measurement errors in input data and their associated assumptions. \\ (e.g., account for potential input errors in the input device, text data, audio, and video)\end{tabular} & \begin{tabular}[t]{l} \cite{collins2015transparent} \end{tabular} & \begin{tabular}[t]{l}38507\end{tabular} & Art. 10 \\

18 & \begin{tabular}[t]{l}Protect sensitive variables in training/testing datasets. \\ (e.g., protect sensitive data and use techniques such as k-anonymity and differential privacy)\end{tabular} & \begin{tabular}[t]{l} \cite{dworkdifferential} \end{tabular} & \begin{tabular}[t]{l}38507, 24028\end{tabular} & Art. 10 \\ 

\midrule
\multicolumn{2}{l}{\textbf{OVERSIGHT}} &  &  \\
19 & \begin{tabular}[t]{l}Continuously monitor metrics and utilize guardrails or rollbacks to ensure the system's output stays within a desired range. \\ (e.g., validate against concept drift and test with diverse testers and compliance and adversarial cases)\end{tabular} & \begin{tabular}[t]{l} \cite{fjeld2020principled} \end{tabular}  & \begin{tabular}[t]{l}38507, 5338, 24028, \\ 24027, 24368\end{tabular} & Art. 14 \\

20 & \begin{tabular}[t]{l}Ensure human control over the system, particularly for designers, developers, and end-users.  \\ (e.g., include human in the loop with the ability to inspect data, models, and training methods)\end{tabular} & \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}38507, 5338,\\ 24028, 24368\end{tabular} & Art. 14\\ 

\midrule
\multicolumn{2}{l}{\textbf{TEAM}} &  &  \\

21 & \begin{tabular}[t]{l}Ensure team diversity.  \\ (e.g., consider diversity in gender, neurotypes, personality traits, and thinking styles)\end{tabular} & \begin{tabular}[t]{l} --- \end{tabular} & \begin{tabular}[t]{l}38507, 5338,\\ 24028, 24368\end{tabular} & Art. 17\\ 

22 & \begin{tabular}[t]{l}Train team members on ethical values and regulations. \\ (e.g., train on privacy regulations, ethical issues, and raising concerns)\end{tabular} & \begin{tabular}[t]{l} \cite{fjeld2020principled}\end{tabular}  & \begin{tabular}[t]{l}38507, 24368\end{tabular} & Art. 17\\ 

\bottomrule
\end{tabular}%
}
\end{table}

\subsection{Revising the Catalog}
In response to the interviews with AI developers and standardization experts, we incorporated an example for each guideline. For instance, under the guideline on system interpretability (guideline \#9), the example provided reads: ``output feature importance and provide human-understandable explanations.'' Furthermore, we simplified the language by avoiding domain-specific or technical jargon. We also categorized each guideline into six thematically distinct categories, namely \emph{intended uses}, \emph{harms}, \emph{system}, \emph{data}, \emph{oversight}, and \emph{team}.

Recognizing that certain guidelines may only be applicable at specific stages (e.g., monitoring AI after deployment), we assigned them to three phases based on previous research (e.g., \cite{madaio2020co, mitchell2019model}). These phases are development (designing and coding the system), deployment (transferring the system into the production stage), and use (actual usage of the system). For example, guidelines like identifying the system's intended uses (guideline \#1) are relevant to all three phases, while those related to system updates (guideline \#14) or decommissioning (guideline \#13) are applicable during the use phase. The revised and final catalog, consisting of 22 unique guidelines, is presented in Table~\ref{tbl:techniques}. To ensure the timeliness and relevance of our guidelines, we cross-referenced them with the articles of the EU AI Act~\cite{eu_ai_act_2022} by following guidance provided by Golpayegani et al.~\cite{golpayegani2023high}.