\section{Evaluation of a tool using our Responsible AI Guidelines}
\label{sec:userstudy}

\subsection{Populating a Tool Using Our Responsible AI Guidelines}

\subsubsection{Eliciting Requirements of the Tool Through a Formative Study}
To determine the design requirements for implementing our 22 guidelines into a responsible AI tool, we conducted a formative study involving interviews with AI practitioners. We used open-ended questions during these interviews to gather insights and perspectives from the participants. \\

\noindent\textbf{Participants and Procedure.}
We recruited 10 AI practitioners (4 females and 6 males) who were in their 30s and 40s and employed at a large tech company. The participants had a range of work experience, spanning from 1 to 8 years, and were skilled in areas such as data science, data visualization, UX design, natural language processing, and machine learning. Participants were asked to consider their ongoing AI projects, and the interviewer guided them through the 22 guidelines. The interviewer prompted participants to reflect on how these guidelines could be incorporated into an interactive responsible AI tool. \\

\noindent\textbf{Design Requirements.}
By conducting an inductive thematic analysis of the interview transcripts~\cite{saldana2015coding, miles1994qualitative, mcdonald2019reliability, braun2006thematic}, two authors identified three key design requirements, supported by quotes from our participants (referred to as FP). These requirements pertained to \textbf{unpacking complexity of AI Ethics}, to \textbf{increasing awareness}, and to \textbf{providing examples and reading materials}: The first requirement was about offering an easy-to-use functionality that unpacks the complexity of AI Ethics. The tool's functionality should be user-friendly, allowing for effortless interaction and navigation through complex terminology to enhance the overall user experience. As expressed by FP5, \emph{``the sheer number of the guidelines is the main difficulty [...] they should be separated in bite-sized questions and allow me to understand the complex terminology used''}. To assist users in systematically moving through the guidelines, FP9 suggested that \emph{``the system should provide clear navigation [...] for example, using a progress bar.''}. The second requirement was about increasing awareness. The tool should increase users' awareness of ethical considerations. FP5 emphasized the importance of \emph{``gaining insights while engaging with the 22 guidelines,''} while FP8 described this need as having \emph{``visual feedback or a score that shows how responsible [their] AI system is.''}. Yet, the user FP2 suggested that the implementation of the feedback \emph{``should not make me anxious and feel like I have not done enough''}. FP5 also recommended that the tool should store user's answers and produce a documentation of their tool experience: \emph{``there should be some functionality there that captures the answers I gave, so it'd help me reflect''}. The third requirement was about providing examples and reading materials. The tool should incorporate examples to assist users in comprehending and effectively utilizing the system. FP9 suggested that \emph{``references to these guidelines or practical examples could be added. These additions would enhance the sense of credibility.''} \\

% Figure environment removed

\subsubsection{Designing and Populating the Tool} We then describe the content and design choices, as well as the tool's flow and interactions.

\smallskip
\noindent\textbf{Content and Design Choices.} Using the 22 guidelines (Table~\ref{tbl:techniques}), we designed and developed an interactive responsible AI tool. Each guideline is presented as a rectangular box, with both the front and back sides being interactive boxes---creating a notion of a digital card. The front side includes a symbolic graphic collage representing the guideline, a brief guideline name, and a concise textual description. On the other hand, the back side includes an example illustrating how the guideline can be applied in an AI system, along with input fields where users can document their specific implementation of the guideline within their context (Figure~\ref{fig:card-elements}).

Digital cards often replicate the appearance and interactions of physical cards, allowing for gestures like stacking, shuffling, and swiping~\cite{cardInteractions_2020}. In our interactive responsible AI tool, guidelines can be viewed from both sides by using the flipping button located in the bottom-left corner, and users have the option to put the guidelines back into a stack for further consideration. We explored different layout options for displaying the guidelines, considering previous research that involved scrolling through a deck or organizing them into multiple groups~\cite{dittus2017community}. However, due to the limited screen size and repetitive guidelines for each phase, we opted to stack the guidelines into three groups based on the phase of the AI system: \emph{i)} development (designing and coding), \emph{ii)} deployment (transitioning into production), and \emph{iii)} use (actual usage of the system). The number of guidelines in each group varied: 20 for development, 19 for deployment, and 21 for use (\S\ref{sec:method}, Step 4) to accommodate the specific requirements of each phase. \\

\noindent\textbf{Flow and Interactions.} The interactive tool includes two follow-up questions for each guideline, as shown in Figure~\ref{fig:game-sorting}. These questions offer users a systematic approach to consider each guideline within a specific context of their own projects. The first question asks the developer whether the guideline has been successfully implemented in their AI system. For example, a guideline related to fairness asks the developer to consider if they have reported evaluation metrics for various groups based on factors such as age, gender, and ethnicity (technique \#8 in Table~\ref{tbl:techniques}). This prompts the developer to evaluate whether fairness has been addressed in their AI system. If the developer answers ``yes,'' they are then prompted to provide specific details on how fairness was implemented. Upon sharing this information, the tool moves the guideline to the ``successfully implemented'' stack. In contrast, if the developer answers ``no,'' the tool asks a second follow-up question regarding whether the guideline should be implemented in a future iteration of the AI system. If the developer answers ``yes,'' they are prompted to provide specific details on how to implement it. The tool then moves the guideline to the ``should be considered'' stack. However, if the developer answers ``no'' to both questions, indicating that the guideline is not applicable to their AI system, the tool moves the guideline to the ``inapplicable'' stack.

The layout of the tool consists of three sections, as shown in Figure~\ref{fig:ui-sections}. In the first section, users can enter the name of the developed AI system (Figure \ref{fig:ui-sections}A) and select the phase it belongs to (Figure \ref{fig:ui-sections}B). Once the phase is selected, the second section displays a stack of guidelines (Figure \ref{fig:ui-sections}C). As users interact with the stack, a counter on the left side color-codes the guidelines and indicates their assignment to the three stacks. The counter also shows the number of remaining guidelines. Blue leaves represent guidelines that have been successfully used, magenta leaves represent guidelines for future considerations, and empty leaves represent inapplicable guidelines. After completing the sorting process, the user is presented with an automatically generated report (available for download as a PDF) that separates the guidelines into the three distinct stacks (Figure \ref{fig:ui-sections}D). If desired, the user can repeat the guideline sorting procedure for other phases (Figure \ref{fig:ui-sections}E).

% Figure environment removed

\subsection{Evaluating the Toolâ€™s Usability and Revising It}
To answer our \textbf{RQ\textsubscript{2}} and evaluate the tool, we conducted an interview study with 14 additional AI researchers and developers. 

\subsubsection{Participants}
We recruited participants from the same large research-intensive technology company.\footnote{Participants who took part in the formative study were not eligible to participate in this evaluation study.} The recruitment process took place in October and November 2022. All participants had significant expertise in AI, including areas such as machine learning, deep learning, and computer vision. Additionally, each participant was actively involved in at least one ongoing AI project during the time of the interviews. Table~\ref{tab:demographics} summarizes participants' demographics.


\begin{table}
\centering
\caption{User study participants' demographics.}
\label{tab:demographics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllll}
\toprule
\textbf{ID} & \textbf{Gender} & \textbf{Yrs of expr. In AI} & \textbf{Education} & \textbf{Current continent} & \textbf{Expertise} \\ \midrule
1 & Male & 6 & Ph.D. & EU & Deep learning, computer vision \\
2 & Male & +10 & Ph.D. & North America & Machine learning, computer vision \\
3 & Male & 8 & Ph.D. & EU & Machine learning \\
4 & Male & 4 & Ph.D. & North America & Deep learning, IoT, computer vision \\
5 & Female & 5 & Ph.D. & EU & Machine learning \\
6 & Female & 8 & Ph.D. & EU & Computer vision \\
7 & Male & 2 & Ph.D. & North America & Computer vision \\
8 & Male & 10 & Ph.D. & EU & Machine learning \\
9 & Male & 4 & Ph.D. & North America & Computer vision \\
10 & Male & +10 & M.S. & EU & Machine learning, natural language processing \\
11 & Male & +10 & Ph.D. & EU & Machine learning \\
12 & Male & 6 & Ph.D. & EU & Machine learning \\
13 & Male & 4 & Ph.D. & EU & Reinforcement learning, decision making \\
14 & Male & 8 & Ph.D. & EU & Computer vision, robotics \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Procedure}
Ahead of the interviews, we sent an email to all participants, providing a concise explanation of the study along with a brief demographics survey. The survey consisted of questions regarding participants' age, domain of expertise, and years of experience in AI system development. The survey is available in Appendix~\ref{app:demographics-survey}. It is important to note that our organization approved the study, and we adhered to established guidelines for user studies, ensuring that no personal identifiers were collected, personal information was removed, and the data remained accessible solely to the research team.

During the interview session, we presented our tool to the participants and allocated 20 minutes (or less if they completed the task sooner) for them to interact with the guidelines. To make the scenario as realistic as possible, we encouraged participants to reflect on their ongoing AI projects and consider how the guidelines could be applied in those specific contexts. Subsequently, we administered the System Usability Scale (SUS)~\cite{brooke1996sus} to assess the usability of the tool. We further engaged participants by asking about their preferences, dislikes, and the relevance of the guidelines to their work. We also sought their suggestions for improvements and potential use cases for the tool. The session concluded with a discussion on enhancing the tool's usability through shared ideas and insights.

We piloted our study with two researchers (1 female, 1 male), which helped us make minor changes to the study guide (e.g., clarifying question-wording and changing the order of questions for a better interview flow). These pilot interviews were not included in the analysis.

\subsubsection{Analysis}
Two authors conducted an inductive thematic analysis (bottom-up) of the interview transcripts, following established coding methodologies~\cite{saldana2015coding, miles1994qualitative, mcdonald2019reliability}. The authors used sticky notes on the Miro platform~\cite{miro2022} to capture the participants' answers, and collaboratively created affinity diagrams based on these notes. They held seven meetings, totaling 14 hours, to discuss and resolve any disagreements that arose during the analysis process. Feedback from the last author was sought during these meetings. In some cases, a single note was relevant to multiple themes, leading to overlap between themes. All themes included quotes from at least two participants, indicating that data saturation had been achieved~\cite{guest2006many}. As a result, participant recruitment was concluded after the $14^{\text{th}}$ interview. The resulting themes, along with their corresponding codes, are provided in Table~\ref{tab:codebook-interviews} in the Appendix.

\subsubsection{Results}
\label{sec:users-results}
First, we present the results regarding the usability and effectiveness of our tool. Then, we provide feedback from our participants regarding potential improvements to the tool. \\

\noindent \textbf{Usability and Effectiveness}. The guidelines were generally well-received by the participants, with a majority considering them as a valuable tool for raising awareness and facilitating self-learning about responsible AI (12 out of 14 participants). For example, one participant expressed, \blockquote[P3]{It made me reflect on my previous choices and how I would describe my decisions when I had to develop the system.} Additionally, seven participants acknowledged the usefulness of the provided examples, which helped them think about potential scenarios and make the guidelines more actionable. Some participants also appreciated the visual simplicity of the guidelines (mentioned by 3 out of 14 participants) and the sequential flow of information, which allowed them to have a more pleasant experience and sufficient time to digest the information (mentioned by 2 out of 14 participants).

Participants, on average, rated the guidelines' usability with a score of 66 out of 100 in SUS, with a standard deviation of 16.01. This indicates a generally positive user experience~\cite{sauro2011practical}. However, participants also identified areas where improvements could be made, which we discuss next.\\

\noindent \textbf{Improvements.} 
Although participants found the guidelines to be a valuable starting point for reflection and engagement with AI ethical considerations, they also provided recommendations for improvement. Many participants expressed the desire for guidelines tailored to their specific project and role (10/14): \blockquote[P6]{It would be helpful if the guidelines were tailored to the specific tasks or challenges I encounter in my project, such as processing the data set and seeking feedback from other people.} Additionally, five participants suggested making the tool more collaborative, allowing them to engage with other experts and stakeholders: \blockquote[P1]{I appreciated that the guidelines emphasized the importance of seeking support from experts when needed. It would be great if the tool facilitated collaboration and discussions with other stakeholders.} Some participants found the tool's summary to be less useful and were uncertain about its future application (3/14), while others expressed the need to keep the examples visible throughout the process (currently, they disappear after consideration).

% Figure environment removed

\subsubsection{Revisions}
Based on the feedback, we made three revisions to our tool (Figure~\ref{fig:prompts-revision}). That is, we: \emph{a)} added roles; \emph{b)} implemented features to foster collaboration; and \emph{c)} improved user experience.\\

\noindent\textbf{Adding Roles.} To assign roles to each guideline (Table~\ref{tbl:techniques_roles} in the Appendix), we referred to previous literature that focused on understanding the best practices of AI practitioners and the development and evaluation of responsible AI toolkits. Wang et al.~\cite{wang2023designing} interviewed UX practitioners and responsible AI experts to understand their work practices. UX practitioners included designers, researchers, and engineers, while responsible AI experts included ethics advisors and specialists. Wong et al.~\cite{wong2023seeing} analyzed 27 ethics toolkits to identify the intended audience of these toolkits, specifically those who are expected to engage in AI ethics work. The intended audience roles identified included software engineers, data scientists, designers, members of cross-functional or cross-disciplinary teams, risk or internal governance teams, C-level executives, and board members. Additionally, Madaio et al.~\cite{madaio2020co} co-designed a fairness checklist with a diverse set of stakeholders, including product managers, data scientists and AI/ML engineers, designers, software engineers, researchers, and consultants. Following guidance therefore from these studies~\cite{wang2023designing,wong2023seeing,madaio2020co}, we formulated three roles as follows:
\begin{enumerate}
    \item Decision-maker or Advisor: This role includes individuals such as product managers, C-suite executives, ethics advisors/responsible AI consultants, and ethical board members.
    \item Engineer or Researcher: This role includes AI/ML engineers, AI/ML researchers, data scientists, software engineers, UX engineers, and UX researchers.
    \item Designer: This role includes interaction designers and UX designers.
\end{enumerate} 

\noindent\textbf{Fostering Collaboration.} Our participants also recognized the potential to enhance the collaborative nature of the tool, allowing them to share knowledge with other stakeholders. To achieve this, we implemented two user interface features. First, we introduced a feature that enables users to keep a history of their interactions with the tool by revising their answers and tracking the changes made. When users revise the content of the guidelines, the newly generated content is stored in a ``responsible AI knowledge base.'' This functionality allows distributed teams to leverage this content, fostering a shared understanding of the project at hand. Additionally, the interface utilizes color coding to indicate the relevance or irrelevance of guidelines at different points in time. Second, we redesigned the PDF summary to include responsible AI blindspots, which encompass specific actions to be taken and shared among the development team. This improvement enhances the utility of the summary by highlighting areas that require attention and providing actionable insights.
\smallskip

\noindent\textbf{Improving User Experience.} While our participants appreciated the simplicity of the user interface, we modified our initial idea of mimicking physical interactions, such as flipping. This decision was made based on feedback from users who found it challenging to remember the guideline while writing the corresponding action after the guideline was flipped. Instead, in the revised version of the tool, we adopted a simplified approach. Both the guideline and its corresponding example(s) are now visible at all times to ensure better usability. We achieved this by dividing each guideline into two side-by-side parts: the left side displays the guideline, while the right side presents its examples in interactive boxes. By making both the guideline and examples consistently visible, users can easily refer to the information they need while formulating their responses. This design change aims to improve the user experience and enhance the effectiveness of the tool in guiding responsible AI practices. \\


