\section{Introduction}
\label{sec:introduction}
The development of responsible AI systems~\cite{shneiderman2021responsible, kissinger2021age, tahaei2023human} has become a significant concern as AI technologies continue to permeate various aspects of society~\cite{shneiderman2022human}. While AI holds the potential to benefit humanity, concerns regarding biases~\cite{baeza2018bias, cramer2019translation, buolamwini2018gender} and the lack of transparency and accountability~\cite{mitchell2019model, rakova2021responsible} hinder its ability to unlock human capabilities on a large scale. In response, AI practitioners\footnote{We use the term practitioners to cover a wide range of stakeholders including AI engineers, developers, researchers, designers, ethics experts.} are actively exploring ways to enhance responsible AI development and deployment. One popular approach is the use of tools such as checklists~\cite{madaio2020co} or guideline cards~\cite{elsayed2023responsible, amershi2019guidelines, FeministDeck_2022} that are designed to promote AI fairness, transparency, and sustainability. These tools provide practical frameworks that enable practitioners to systematically assess and address ethical considerations throughout the AI development lifecycle. By incorporating checklists and guideline cards into their workflows, practitioners can evaluate key aspects such as data sources, model training, and decision-making processes to mitigate potential biases, ensure transparency, and promote the long-term sustainability of AI. However, these tools face two main challenges, creating a mismatch between their potential to support ethical AI development and their current design.

The first challenge is that these tools often exhibit a static nature, lacking the ability to dynamically incorporate the latest advancements in responsible AI literature and international standards~\cite{nist2023aiRisk, fjeld2020principled}. In the rapidly evolving field of responsible AI, new ethical considerations and regulatory guidelines constantly emerge (e.g., the EU AI Act~\cite{eu_ai_act}). It is therefore crucial for AI practitioners to stay updated of these developments to ensure their AI systems align with the current ethical and responsible AI practices. While checklists and guideline cards are increasingly used to assist and enhance the development of responsible AI systems, can quickly become outdated (e.g., the AI Blindspots deck has undergone several iterations~\cite{AIBlindspotsV1, AIBlindspotsV2}), limiting their effectiveness in addressing evolving concerns related to fairness, transparency, and accountability.

The second challenge is that these tools often prioritize individual usage instead of promoting collaboration among AI practitioners~\cite{wong2023seeing}. While individual adoption of responsible AI practices is important, fostering collaboration among practitioners can facilitate collective learning and a broader understanding of responsible AI principles (e.g., the inclusion of ``crowd audits''~\cite{shen2021everyday} of algorithmic harms has not yet been integrated into responsible AI tools). By emphasizing individual usage, these tools may inadvertently hinder the exchange of ideas, best practices, and lessons learned among practitioners. On the other hand, collaboration is crucial in addressing complex ethical and societal challenges associated with AI systems. Therefore, it is essential to enhance the collaborative nature of these tools, enabling knowledge sharing and fostering a community-driven approach to responsible AI development and deployment.

To overcome these challenges, we developed an adaptable and updatable method that consists of actionable guidelines. These guidelines aim to address the static and often non-collaborative nature of existing responsible AI tools. To achieve this, we focused on answering two main research questions (RQs):

\begin{enumerate}
    \item[\textbf{RQ\textsubscript{1}}:] How can actionable responsible AI guidelines be systematically developed?   
    \item[\textbf{RQ\textsubscript{2}:}] How can responsible AI guidelines be implemented into a practical tool, and to what extent the tool is usable?
\end{enumerate}

In addressing these two questions, we made three main contributions: 

\begin{itemize}
\item Our method enables the integration of dynamic and comprehensive content in responsible AI tool (e.g., checklists or guideline cards) (\S\ref{sec:method}). It facilitates easy updating of responsible AI guidelines from research papers and ISO\footnote{International Organization for Standardization.} standards, ensuring that the content remains relevant and up to date. Our method has yielded 22 actionable guidelines that can be implemented by a diverse range of AI practitioners, centered around what practitioners need to do, rather than specific implementation details.

\item We validated our method through a case study conducted at a large tech company, where we designed and deployed a tool that provides interactive and actionable guidelines (\S\ref{sec:userstudy}). Initially, we conducted a formative study involving 10 AI practitioners to identify the key design requirements for populating the tool with responsible AI guidelines. Based on these requirements, we designed and populated the tool, which was then evaluated for its usability and effectiveness in an interview study with an additional set of 14 AI practitioners. The guidelines were found to be practical and actionable, effectively encouraging self-reflection and facilitating a better understanding of the ethical considerations involved in AI development.

\item In light of these findings, we discuss how our method contributes to the idea of ``Responsible AI by Design'' by contextualizing the guidelines, informing existing or new theories, and offering practical recommendations for designing responsible AI toolkits with the aim of fostering collaboration and enabling organizational accountability (\S\ref{sec:discussion}).
\end{itemize}







