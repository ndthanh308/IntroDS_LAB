\section{Discussion}
\label{sec:discussion}
To assist AI practitioners in navigating the rapidly evolving landscape of AI ethics, governance, and regulations, we have developed a method for generating responsible AI guidelines that are grounded in regulation and usable by different roles. We validated our method in a user study at a large technology company, where we designed and evaluated a tool that incorporates our responsible AI guidelines. We conducted a formative study involving 10 AI practitioners to design the tool, and evaluated our guidelines in a user study with an additional 14 AI practitioners. The results indicate that the guidelines were perceived as practical and actionable, promoting self-reflection and enhancing understanding of the ethical considerations associated with AI during the early stages of development. 

We now discuss the inherent problem of decontextualization in responsible AI toolkits; dwell on the concept of meta-responsibility; and provide practical recommendations for incorporating responsible AI guidelines into toolkits and for  enabling organizational accountability. 

% The inherent challenge in responsible AI toolkits lies in their attempt to reconcile the tension between scalability and context specificity~\cite{wong2023seeing}.

\subsection{Theoretical Implications}

\noindent 
\textbf{Decontextualization.}  Traditional approaches to toolkit development have often favored a universal, top-down approach that assumes a one-size-fits-all solution~\cite{kelty_participatory_toolkit, mattern_toolkit}. However, participatory development, such as the methodology we followed in designing and populating a responsible AI tool with our guidelines, emphasizes the importance of tailoring responsible AI guidelines to specific contexts and job roles needs. Various AI professionals like designers, developers, engineers, and executives have unique needs and concerns. Treating them all the same can lead to issues like decontextualization in responsible AI toolkits~\cite{wong2023seeing}.


To tackle this problem, our proposed method incorporates two key elements: \emph{guidelines usable by different roles} and \emph{guiding questions}. Firstly, the integration of guidelines tailored to different roles and projects provides practical steps and recommendations that technical practitioners can easily implement, or C-level executives can make informed decisions upon. These guidelines serve as a starting point for ethical decision-making throughout the AI lifecycle, contributing to the vision of responsible AI by design (borrowing from the idea of `privacy by design'\footnote{``Privacy by design'' is a standard practice for incorporating data protection into the design of technology. In other words, data protection is achieved when it is already integrated into the technology during its design and development~\cite{cavoukian2009privacy}.}). Secondly, the inclusion of the two guiding questions (\S\ref{subsec:populate_tool}), one on how the guideline was implemented, and the other on how it could have been implemented, enhances our toolkit's ability to capture the complexities of different social and organizational contexts. 
\smallskip

\noindent\textbf{Meta-responsibility.} Scholars have long recognized the need for a socio-technical approach that considers the contextual factors governing the use of AI systems, including social, organizational, and cultural factors~\cite{tahaei2023toward}. In fact, Ackerman~\cite{ackerman2000intellectual} introduced the concept of socio-technical gap to highlight the disparity between human requirements and  technical solutions.  Along similar lines, \citet{stahl2023embedding} introduced the concept of meta-responsibility to stress that AI systems should be viewed as systems of systems  rather than single entities. Our work contributes to the integration of ethical, legal, and social knowledge into the AI development process---what Stahl referred to as ``adaptive governance structure''.

\subsection{Practical Implications}

\noindent \textbf{Recommendations for incorporating responsible AI guidelines into toolkits.} Our work identified four essential design requirements for incorporating guidelines into tools. They include: simplifying guidelines into smaller visual components; implementing clear navigation; tracking and sharing progress; and developing mechanisms for reflection.

For simplifying guidelines, we displayed each guideline as a digital card and accompanied it with two guiding questions. Future work could explore how to further divide guidelines into additional visual elements on the cards and how to refine the guiding questions. For example, guideline \#15---\emph{ensuring compliance with agreements and legal requirements when handling data}---could be further divided into step-by-step processes, with each one marked by a visual element like a card tab or a link to a specific ISO, or excerpts from the EU AI Act. Regarding the guiding questions, we observed that their formulation  is a delicate task, requiring a balance between directness and respect for the user's autonomy. For example, a question formulated as ``How did you consider the potential impact of your AI system on different user groups?'' employs a proactive stance, avoiding any direct accusation or presumption of oversight. This method resonates with the experiences of our participants (e.g., P14) who found value in open-ended questions. However, guiding questions can be refined in various ways by, for example, ``reminding consequences'' or ``providing multiple viewpoints''~\cite{caraban2019ways}.

For ensuring clear navigation, we organized the guidelines into a one-page layout and incorporated multiple buttons along with a counter for easy navigation. Future work could explore how to develop alternative layouts and include different navigation mechanisms. For example, complementary guidelines with related content, such as guideline \#3 \emph{identify potential harms and risks associated with the intended use} and guideline \#5 \emph{develop strategies to mitigate identified harms or risks for each intended use} can be paired side by side to improve the quality of responses. Additionally, new navigation mechanisms might include a chart to illustrate the relationships between guidelines and a search bar to enable users to quickly locate specific guidelines.

For tracking how guidelines are applied and sharing progress among team members, we introduced a feature to store user responses locally within the browser session and dynamically generate a PDF report from these responses. Future work could explore how to structure user responses in formats suitable for automated analysis and integration with other tools. For instance, using JSON format as input for machine learning algorithms and Large Language Models (LLMs) can enable the analysis of user responses and the generation of automated insights and recommendations within the PDF report.

For enabling post-hoc reflections, we created a summary page where users can view the number of guidelines they have considered and their responses to each guideline. Future work could explore how to improve this summary page, for example, by adding visual elements for recognizing responsible AI champions (e.g., responsible AI badges) and fostering empathy (e.g., animations presenting the environmental impact of an AI system), or by implementing a collaborative aspect where users can share and discuss their summary pages with peers or mentors.
\smallskip

\noindent \textbf{Recommendations for enabling organizational accountability.}
While individual adoption of responsible AI best practices is crucial, fostering effective communication between technical and non-technical roles is equally important. Many existing responsible AI toolkits prioritize individual usage~\cite{wong2023seeing}. However, addressing complex ethical and societal challenges associated with AI systems requires diverse perspectives. Our interactive tool populated with guidelines addresses this need by offering features that make the guidelines usable by different roles (e.g., adjusting which guidelines are shown to different roles and in different system phases). However, our tool can further improve communication between roles by creating a knowledge base of responses. Such a knowledge base, according to Stahl \cite{stahl2023embedding}, empowers team members to fulfill their responsibilities and supports distributed teams in constructing a shared understanding of their AI system. Furthermore, we suggest a mechanism for keeping this knowledge base up to date and enriched with diverse perspectives. This includes regularly revisiting the guidelines through our tool and providing responses at key project milestones, such as when the AI system enters a new phase. This approach ensures that the knowledge base remains dynamic and reflect the evolving insights and perspectives within the team.

Our guidelines and the tool that incorporates them can also be used to enable organizational accountability. Similar to Google's five-stage internal algorithmic auditing framework~\cite{raji2020closing}, our guidelines serve as a practical tool for partially closing the AI accountability gap. The automatically generated report plays a crucial role in this process by providing a summary of the guidelines that were effectively implemented, and of those that should be considered for future development. These reports establish an additional chain of accountability that can be shared with stakeholders at various levels, including managers, senior leadership, and AI engineers. By offering more oversight and the ability to troubleshoot, if needed, these reports help mitigate unintentional harm. When an organization follows our guidelines, it needs to set up clear processes though. If incentives are not right, AI professionals may avoid using them because they fear being responsible for their actions.


\subsection{Limitations and Future Work} Our work has four main limitations that highlight the need for future research efforts. Firstly, although we followed a rigorous four-step process involving multiple stakeholders, the list of 22 guidelines may not be exhaustive. The rapidly evolving nature of AI ethics, governance, and regulations necessitates an ongoing effort to stay abreast of emerging developments. However, one of the strengths of our method lies in its modular design, which allows for ongoing refinement and expansion of the set of guidelines.  Future work could incorporate ISOs that are currently under development such as those for functional safety (ISO 5469), data quality (ISO 5259), explainability (ISO 6254), AI system impact assessment (ISO 42005), and requirements for bodies providing audit and certification of AI management systems (ISO 42006). Additionally, the European Committee for Electrotechnical Standardization~\cite{cen_cenelec} (CEN-CENELEC) body was recently tasked to translate the EU AI Act into standards; such standards can also be cross-referenced with our guidelines as part of future work. However, we acknowledge that there may be limitations in ensuring that all standards are accessible to everyone and that experts may not always be available to evaluate them. A partial solution would be to create forums or discussion groups where individuals can share their experiences and insights about regulations and standards. At the same time, future research could also investigate the frequency with which our method should be updated as new literature emerges. One possibility would be to create an automated system that regularly collects research articles on responsible AI best practices, pairing them with current and upcoming regulations, to extract new guidelines.

Secondly, it is important to consider the qualitative nature of our user study. It  involved in-depth interviews, but its findings should be interpreted with caution, understanding that the reported frequency of themes should be viewed in a comparative manner rather than taken at face value~\cite{fossey2002understanding}. This would  avoid potential misinterpretation or overgeneralization of the results.

Thirdly, we need to acknowledge the limitations associated with the sample size and demographics of our user study. The study was conducted with a specific group of participants, and, therefore, the findings may not fully represent the practices and perspectives of all AI practitioners. Our sample predominantly consisted of male participants, which aligns with the gender distribution reported in Stack Overflow's 2022 Developer Survey, where 92.85\% of professional developer respondents identified as male~\cite{stackoverflow2022survey}. Additionally, our participants were drawn from a large research-focused technology company. While the results may offer insights into practices within certain companies, they also serve as a case study for future research. 

Lastly, our qualitative results suggest indicators of ease of use for AI practitioners but does not provide direct information on the actual effectiveness of the guidelines. Understanding the impact of guidelines (or other AI toolkits~\cite{wong2023seeing}) requires long-term studies that consider multiple projects, with some utilizing the toolkit and others not. One potential avenue is to conduct observational studies with users of an AI system in a ``naturalistic setting''. Another approach is to use proxies such as measuring users' attitudes, beliefs, and mindset regarding ethical values before and after utilizing the guidelines. 