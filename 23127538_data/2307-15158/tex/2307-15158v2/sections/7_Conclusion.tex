\section{Conclusion}
\label{sec:conclusion}

We proposed a method for generating a list of responsible AI guidelines that are grounded in regulations and are usable by different roles. The resulting 22 guidelines were integrated into an interactive tool and evaluated through a user study with 14 AI researchers, engineers, designers, and managers from a large technology company. Our participants found the guidelines well-aligned with their roles, enabling them to communicate complex ethical concepts in a structured manner. The guidelines are also grounded in ISOs and the EU AI Act articles, receiving positive feedback for being comprehensive. The usefulness of examples in guidelines was particularly noted as they enabled participants to reflect on their choices concerning ethical issues. As these guidelines are likely to become part of future responsible AI toolkits, it is important to implement features that provide users with time and space for reflection. Additionally, these toolkits should take users' reflections and roles into account to offer actionable recommendations tailored to a specific project, using, for example, large language models.