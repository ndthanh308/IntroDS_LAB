\section{Related Work}
\label{sec:related}

We surveyed various lines of research that our work draws upon, and grouped them into two main areas: \emph{(1)} AI regulation and governance (\S\ref{subsec:ai_governance}), and \emph{(2)} responsible AI practices and toolkits (\S\ref{sec:sub-raipractices}). 

\subsection{AI Regulation and Governance}
\label{subsec:ai_governance}
The landscape of AI regulation and governance is constantly evolving~\cite{jobin2019global, mittelstadt2016ethics}. At the time of writing, the European Union (EU) has endorsed new transparency and risk-management rules for AI systems known as the EU AI Act~\cite{eu_ai_act_2022}, which is expected to become law in 2024. Similarly, the United States (US) has recently passed a blueprint of the AI Bill of Rights in late 2022~\cite{us_ai_bill}. This bill comprises \emph{``five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of AI.''} Both the EU and US share a conceptual alignment on key principles of responsible AI, such as fairness and explainability, as well as the importance of international standards (e.g., ISO 24028 for Trustworthiness). 

%the specific AI risk management regimes they are developing are potentially diverging, creating an ``artificial divide''~\cite{ecfr}. The EU aims to become the leading regulator for AI globally, while the US takes the view that excessive regulation may impede innovation.

Notable predecessors to AI regulations include the EU GDPR law on data protection and privacy~\cite{eu_gdpr}, the US Anti-discrimination Act~\cite{us_anti_discrimination}, and the UK Equality Act 2010~\cite{uk_equality}. GDPR's Article 25 mandates that data controllers must implement appropriate technical and organizational measures during the design and implementation stages of data processing to safeguard the rights of data subjects. The Anti-discrimination Act prohibits employment decisions based on an individual's race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability, or genetic information. This legislation ensures fairness in AI-assisted hiring systems. Similarly, the UK Equality Act provides legal protection against discrimination in the workplace and wider society.

The National Institute of Standards and Technology (NIST), a renowned organization for developing frameworks and standards, recently published an AI risk management framework~\cite{nist2023aiRisk}. According to the NIST framework, an AI system is defined as \emph{``an engineered or machine-based system capable of generating outputs such as predictions, recommendations, or decisions that influence real or virtual environments, based on a given set of objectives. These systems are designed to operate with varying levels of autonomy.''} Similarly, the Principled Artificial Intelligence white paper from the Berkman Klein Center~\cite{fjeld2020principled} highlights eight key thematic trends that represent a growing consensus on responsible AI. These themes include privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and the promotion of human values. Building on these themes, previous works have proposed a set of guidelines involving specific groups of AI practitioners. Saleema \emph{et al.}~\cite{humanAIGuidelines2019} proposed 168 guidelines on how to design AI tailored to HCI practitioners. Similarly, Vakkuri \emph{et al.}~\cite{eccolaCards2021} formulated AI ethics guidelines tailored to researchers and technologists. No subsequent work has associated these guidelines with current international standards or regulations.
\smallskip

\noindent\textbf{Research Gaps.} As AI regulation and governance continue to evolve, AI practitioners are faced with the challenge of staying updated not only with the changing guidelines, but also with regulations, requiring significant time and effort. Because prior guidelines lacked alignment with regulations, standards, and the input of experts in those fields, this work aims to create a methodology for crafting responsible AI guidelines that adhere to regulations and standards.

\subsection{Responsible AI Practices and Toolkits}
\label{sec:sub-raipractices}

\noindent\textbf{Responsible AI Toolkits.} At the time of writing, the OECD's website lists 613 toolkits dedicated to fostering the development and deployment of responsible AI systems~\cite{oecd_rai_toolkits}. These toolkits are essential for operationalizing guidelines and regulations to assist AI practitioners such as engineers and researchers in addressing algorithmic bias~\cite{bird2020fairlearn, gebru2021datasheets}, explaining algorithmic decisions~\cite{arya2019one}, and ensuring privacy in AI systems~\cite{fjeld2020principled}. For addressing algorithmic bias, Google's Fairness Indicators toolkit allows developers to assess data distribution and model performance across user-defined groups~\cite{google2022fairness}. IBM's AI Fairness 360 offers fairness metrics for bias mitigation~\cite{ibm2022ai}. Microsoft's Fairlearn assesses model impact on specific groups (e.g., under-represented populations) in terms of fairness and accuracy~\cite{fairlearn2022}. For explaining algorithmic decisions, IBM's AI Explainability 360 provides metrics and guidance for explainability, and new visualization techniques to enhance transparency~\cite{ibm2019_XAI, google2022pair, nvidia2022}. Finally, for ensuring privacy in AI systems, IBM's AI Privacy 360 helps assess and mitigate privacy risks through data anonymization and minimization~\cite{cavoukian2009privacy, cavoukian2010privacy, fjeld2020principled}. 
\smallskip

\noindent\textbf{Toolkits Used in Practice.} Developing toolkits specialized for certain audiences such as AI developers can lead to techno-solutionism, focusing exclusively on technical fixes. However, responsible AI entails broader socio-technical challenges (e.g., diversity and inclusion in decision-making) that require involvement of different roles with diverse expertise and background~\cite{selbst2019}, and such an involvement is typically discussed in venues with a long-standing commitment to human-centered design such as CHI, CSCW, AIES, and FAccT.

Different roles (e.g., data scientists, ML engineers and developers, UX designers) use toolkits in various ways. Data scientists often struggle to fully grasp visualizations of interpretable tools (e.g., InterpretML~\cite{interpret_ml} and SHAP~\cite{lundberg2017unified}), hindering their ability to understand datasets and underlying models~\cite{kaur2020interpreting}. Experienced ML developers and engineers often go beyond what fairness toolkits offer to tackle algorithmic unfairness, while those with less experience typically use only a few metrics and methods from these toolkits~\cite{balayn2023fairness}. UX designers often rely on custom prototypes and their own past experiences to help contextualize responsible AI issues for non-technical colleagues~\cite{deng2022exploring} due to communication gaps~\cite{yildirim2023investigating}.

Major communication gaps between technical and non-technical roles typically arise because these roles are involved in different stages of a project, which is likely to create fragmentation in communication~\cite{piorkowski2021ai}. By exploring how data science teams collaborate, Zhang \emph{et al.}~\cite{zhang2020data} found that non-technical roles play more prominent roles in the early and late stages of projects, while technical roles primarily handle the core data and modeling tasks. However, this disparity in involvement at various project stages is likely to create fragmentation. In fact, Organizational Science research reinforces the notion that effective communication and collaboration is crucial for overcoming the ``silo mentality''~\cite{forbes_silo}. Due to this fragmentation and a lack of robust organizational support, practitioners often take on ``bridging'' roles to help the communication between the technical and non-technical project members~\cite{deng2023investigating}. One way of doing so is through ``leaky abstractions''~\cite{subramonyam2022solving}. These are representations that are meant to communicate the inner workings and technical aspects of an AI system to these roles. Similarly, Nahar \emph{et al.}~\cite{nahar2022collaboration} highlighted the extreme difficulty faced by non-technical practitioners in eliciting requirements due to the absence of suitable tools and the involvement of diverse stakeholders, highlighting the need for integrating communication features into toolkits. The design of such features was explored by Elsayed-Ali \emph{et al.}~\cite{elsayed2023responsible} who developed question cards to facilitate stakeholder group discussions. These cards included built-in mechanisms for the automatic and cyclical assignment of cards to different participants, ensuring that everyone had the opportunity to share their opinions during the discussion. \\ 

\noindent\textbf{Research Gaps.} While many toolkits emphasize the importance of involving stakeholders from diverse roles and backgrounds, they are frequently designed for specific stakeholders (e.g., ML engineers), thereby neglecting a broader spectrum of roles (e.g., non-technical). To address this gap, we aim to develop a set of actionable guidelines that are usable by a diverse range of stakeholders.