\section{Introduction}
\label{sec:introduction}
The development of responsible AI systems~\cite{shneiderman2021responsible, kissinger2021age, tahaei2023human} has become a significant concern as AI technologies continue to permeate various aspects of society~\cite{shneiderman2022human}. While AI holds the potential to benefit humanity, concerns regarding biases~\cite{baeza2018bias, cramer2019translation, buolamwini2018gender} and the lack of transparency and accountability~\cite{mitchell2019model, rakova2021responsible} hinder its ability to unlock human capabilities on a large scale. In response, AI practitioners\footnote{We use the term practitioners to cover a wide range of stakeholders including AI engineers, developers, researchers, designers, ethics experts.} are actively exploring ways to enhance responsible AI development and deployment. One popular approach is the use of tools such as checklists~\cite{madaio2020co} or guideline cards~\cite{elsayed2023responsible, humanAIGuidelines2019, FeministDeck_2022} that are designed to promote AI fairness, transparency, and sustainability. These tools provide practical frameworks that enable practitioners to systematically assess and address ethical considerations throughout the AI development lifecycle. By incorporating checklists and guideline cards into their workflows, practitioners can evaluate key aspects such as data sources, model training, and decision-making processes to mitigate potential biases, ensure transparency, and promote the long-term sustainability of AI. However, these tools face two main challenges, creating a mismatch between their potential to support ethical AI development and their current design.

The first challenge is that these tools often exhibit a static nature, lacking the ability to dynamically incorporate the latest advancements in responsible AI literature and international standards~\cite{nist2023aiRisk, fjeld2020principled}. In the rapidly evolving field of responsible AI, new ethical considerations and regulatory guidelines constantly emerge (e.g., the EU AI Act~\cite{eu_ai_act_2022}). It is therefore crucial for AI practitioners to stay updated of these developments to ensure their AI systems align with the current ethical and responsible AI practices. While checklists and guideline cards are increasingly used to assist and enhance the development of responsible AI systems, they are rarely grounded in current regulations. For example, Vakkuri \emph{et al.}~\cite{eccolaCards2021} proposed the ECCOLA cards that are based on AI ethics guidelines (e.g., IEEE Ethically Aligned Design and EU Trustworthy AI), which are not meant to be grounded on any specific regulations. Additionally, guidelines can quickly become outdated (e.g., the AI Blindspots deck has undergone several iterations~\cite{AIBlindspotsV1, AIBlindspotsV2}), limiting their effectiveness in addressing evolving concerns related to fairness, transparency, and accountability.

The second challenge is that, while these tools emphasize the importance of involving stakeholders from diverse roles and backgrounds, they are often designed for specific AI practitioners (e.g., ML engineers), neglecting a broader spectrum of stakeholders (e.g., non-technical roles). Balayn \emph{et al.}~\cite{balayn2023fairness} found that less experienced practitioners in machine learning tend to use a limited set of metrics and methods from toolkits. Similarly, Deng \emph{et al.}~\cite{deng2022exploring} stressed the lack of standardized guidelines in toolkits like AIF360 for introducing fairness issues to non-technical collaborators. Therefore, it is important that toolkits enhance communication, provide comprehensive guidance and support for cross-functional collaboration~\cite{yildirim2023investigating}.

To overcome these challenges, we developed a four-step method to generate a list of responsible AI guidelines which we then incorporated in a tool to evaluate them (Figure~\ref{fig:card-elements}). With this method, we aim to equip different roles with actionable guidelines that are grounded in regulations. To achieve this, we focused on answering this main research question: \emph{How to generate responsible AI guidelines that are grounded in regulations and are usable by different roles?} In addressing this question, we made two main contributions\footnote{The project's site is at \url{https://social-dynamics.net/rai-guidelines}}: 

\begin{enumerate}
\item We proposed a four-step method for generating Responsible AI guidelines; these steps are: \emph{(1)} manual coding of 17 papers on responsible AI; \emph{(2)} compiling an initial catalog of responsible AI guidelines; \emph{(3)} refining the catalog through interviews with 10 AI researchers and engineers, and workshops with 4 standardization experts; and \emph{(4)} finalizing the catalog. This procedure resulted into a set of 22 Responsible AI guidelines (\S\ref{sec:method}).

\item We evaluated the 22 guidelines in a user study with 14 AI researchers, engineers, designers, product managers from a large technology company (\S\ref{sec:userstudy}) by designing and deploying a tool incorporating the guidelines. To develop the tool, we conducted a formative study with 10 AI practitioners to determine key design requirements. Using these requirements, we populated the tool with the guidelines and conducted the case study. Interviews with the 14 AI researchers, engineers, designers, and managers revealed that the guidelines were grounded in current regulations and were effectively usable across different roles, promoting self-reflection on ethical considerations in early development stages.
\end{enumerate}


In light of these findings, we discuss how our method contributes to the idea of ``Responsible AI by Design'' by contextualizing the guidelines, informing existing or new theories, and offering practical recommendations for incorporating responsible AI guidelines into toolkits, and
recommendations for technical and non-technical roles in enabling organizational accountability (\S\ref{sec:discussion}).




