\section{Method for Generating Responsible AI Guidelines}
\label{sec:method}
To generate a list of responsible AI guidelines, we followed a four-step process (Figure~\ref{fig:steps}), based on the methodology proposed by Michie \emph{et al.}~\cite{michie2013behavior}. This process allowed us to identify the essential element of a guideline, referred to as the ``active ingredient,'' focusing on the ``what'' rather than the ``how''~\cite{michie2011strengthening}. A similar parallel can be drawn in software engineering, where the ``what'' represents the software requirements and the ``how'' represents the software design, both of which are important for a successful software product~\cite{aggarwal2005software}. However, by shifting the focus to the ``what,'' AI practitioners can develop a clearer understanding of the objectives and goals they need to achieve, fostering a deeper comprehension of complex underlying ethical concepts. Throughout this process, we actively engaged a diverse group of stakeholders, including AI engineers, researchers, designers, product managers, and experts in law and standardization. As a result, we were able to formulate a total of \textbf{22} responsible AI guidelines (Panel A of Figure~\ref{fig:card-elements}). \\

% Figure environment removed

\subsection{Manual Coding of Responsible AI papers}
\label{subsec:step1}
In the first step, we compiled a list of key scientific articles focusing on responsible AI guidelines applicable to a diverse set of roles, and manually coded them. We created this list by targeting papers published in renowned computer science conferences, such as the ACM CHI, CSCW, FAccT, AAAI/ACM Conference on AI, Ethics, and Society (AIES), and scientific literature from the medical domain (e.g., the Annals of Internal Medicine). Note that we did not conduct a systematic literature. Instead, we identified 17 papers that served as a starting point to compile an initial catalogue of techniques covering a broad range of responsible AI aspects, including fairness, explainability, sustainability, and best practices for data and model documentation and evaluation. These are foundational papers in responsible AI, and, as we shall see in a subsequent step of our methodology (\S\ref{sec:step_adding_iso}), we refined the techniques identified from those papers through interviews and expert panels as well as cross-referencing them with the EU AI Act and ISO standards.

These papers encompass a growing body of research focusing on the work practices (e.g., ensuring fairness or models' explainable outputs) of AI practitioners in addressing responsible AI issues. This strand of research covers various aspects of responsible AI, including fairness, explainability, sustainability, and best practices for data and model documentation and evaluation. Fairness is a fundamental value in responsible AI, but its definition is complex and multifaceted~\cite{narayanan21fairness}. To assess bias in classification outputs, various research efforts have introduced quantitative metrics such as disparate impact and equalized odds, as discussed by Dixon \emph{et al.}~\cite{dixon2018measuring}. Another concept explored in the literature is ``equality of opportunity,'' advocated by Hardt \emph{et al.}\cite{hardt2016equality}, which ensures that predictive models are equally accurate across different groups defined by protected attributes like race or gender. Equally important is the development of dedicated checklists for fairness~\cite{madaio2020co}. Explainable AI (XAI) is another aspect of responsible AI. XAI involves tools and frameworks that assist end users and stakeholders in understanding and interpreting predictions made by machine learning models~\cite{arrieta2020explainable, kulesza2015principles, gunning2019xai,liao2021human, ehsan2020human, ibm2019_XAI}. Furthermore, the environmental impact of training AI models should also be considered. Numerous reports have highlighted the significant carbon footprint associated with deep learning and large language models~\cite{sharir2020cost, hao2019training, strubell2019energy}. Best practices for data documentation and model evaluation have also been developed to promote fairness in AI systems. Gebru \emph{et al.}~\cite{gebru2021datasheets} proposed ``Datasheets for Datasets'' as a comprehensive means of providing information about a dataset, including data provenance, key characteristics, relevant regulations, test results, and potential biases. Similarly, Bender \emph{et al.}\cite{bender2018data} introduced ``data statements'' as qualitative summaries that offer crucial context about a dataset's population, aiding in identifying biases and understanding generalizability. For model evaluation, Mitchell \emph{et al.}~\cite{mitchell2019model} suggested the use of model cards, which provide standardized information about machine learning models, including their intended use, performance metrics, potential biases, and data limitations. Transparent reporting practices, such as the TRIPOD statement by Collins \emph{et al.}~\cite{collins2015transparent} in the medical domain, emphasize standardized and comprehensive reporting to enhance credibility and reproducibility of AI prediction models.

\subsection{Compiling an Initial Catalog of Responsible AI Guidelines}
\label{subsec:step2}
For each research article previously identified, we compiled a list of techniques that could be employed to create responsible AI guidelines, focusing on the actions different roles (i.e., designers, researchers, developers, product managers) should consider during AI development. Following the methodology proposed by Michie \emph{et al.}~\cite{michie2013behavior} (which was also used to identify community engagement techniques by Dittus \emph{et al.}~\cite{dittus2017community}), we sought techniques that describe the ``active ingredient'' of what needs to be done. This means that the phrasing of the technique should focus on \emph{what} needs to  be done, rather than the specific implementation details of \emph{how} it should be done. For example, a recommended practice for ensuring fairness involves evaluating an AI system across different demographic groups~\cite{madaio2020co, dixon2018measuring, hardt2016equality}. In this case, the technique specifies ``what'' needs to be done  (e.g., using common fairness metrics such as demographic parity or equalized odds) rather than ``how'' it should be implemented. In total, we formulated a set of 16 techniques based on relevant literature sources~\cite{mitchell2019model, madaio2020co, dixon2018measuring, hardt2016equality, mitchell2018prediction, verma2018fairness, arrieta2020explainable, kulesza2015principles, fjeld2020principled, bender2018data, gebru2021datasheets, holland2018dataset, wang2020revise, collins2015transparent, sharir2020cost, hao2019training}. 

We then conducted an iterative review of the collection of techniques to identify duplicates, which were instances where multiple sources referred to the same technique. For example, four sources indicated that data biases could affect the model~\cite{mitchell2019model, gebru2021datasheets, bender2018data, holland2018dataset}, emphasizing the need to report the characteristics of training and testing datasets. We consolidated such instances by retaining the specific actions to be taken (e.g., \emph{reporting} dataset characteristics). This process resulted in an initial list of 16 distinct techniques. We provided a concise summary sentence for each technique, utilizing active verbs to emphasize the recommended actions.

\subsection{Refining the Catalog Through Interviews and Expert Panels}
\label{sec:step_adding_iso}
The catalog of techniques underwent eleven iterations to ensure clarity and comprehensive thematic coverage. The iterations were carried out by two authors, with the first author conducting interviews with five AI researchers and developers. During the interviews, the participants were asked to consider their current AI projects and provide insights on the implementation of each technique, focusing on the ``how'' aspect. This served two purposes: firstly, to identify any statements that were unclear or vague, prompting suggestions for alternative phrasing; and secondly, to expand the catalog further. The interviews yielded two main recommendations for improvement: \emph{(1)} mapping duplicate techniques to the same underlying action(s); and \emph{(2)} adding examples to support each technique (each guideline in Table~\ref{tbl:techniques} indeed comes with an example).

In addition to the interviews, the two authors who developed the initial catalog conducted a series of eight 1-hour expert panels with two standardization experts from a large organization. The purpose of these panels was to review the initial catalog for ISO compliance. The standardization experts examined eight AI-related ISOs, including ISO 38507, ISO 23894, ISO 5338, ISO 24028, ISO 24027, ISO 24368, ISO 42001, and ISO 25059, which were developed at the time of writing. Then the experts provided input on any missing techniques and mapped each technique in the initial catalog to the corresponding ISO that covers it. As a result of this exercise, six new techniques (\#2, \#7, \#12, \#13, \#14, \#21 in Table~\ref{tbl:techniques}) were added to the catalog, resulting in a total of 22 guidelines. Next, we provide we provide a high-level summary of each ISO.\footnote{Note that the summary provided is a brief and simplified description due to a paywall restriction.} 
\smallskip

\noindent\textbf{ISO 38507 (Governance, 28 pages).} It offers guidance on responsible AI use (e.g., identify potential harms and risks for each intended use(s) of the systems), and recommendations about current and future AI uses to governing bodies and various stakeholders such as managers and auditors.

\noindent\textbf{ISO 23894 (Risk Management, 26 pages).} It provides guidelines for managing AI-related risks (e.g., mechanisms for incentivizing reporting of system harms) in developing, producing, deploying, or using AI products and systems, including recommendations for integrating risk management into AI processes.

\noindent\textbf{ISO 5338 (AI Lifecycle Process, 27 pages).} It provides a framework for the life cycle of AI systems, detailing processes for managing and enhancing these systems from development to implementation (e.g., through reporting of harms and risks, obtaining approval of intended uses).

\noindent\textbf{ISO 24028 (Trustworthiness, 43 pages).} It offers guidance on trustworthiness in AI systems, focusing on transparency, explainability, controllability, and addressing potential risks with mitigation techniques. It also covers AI systems' availability, resiliency, reliability, accuracy, safety, security, and privacy.

\noindent\textbf{ISO 24027 (Bias, 39 pages).} It discusses bias in AI systems related to protected attributes such as age and gender, especially in AI-aided decision-making, providing techniques to measure and assess bias throughout the AI system lifecycle.

\noindent\textbf{ISO 24368 (Ethical and Societal Concerns, 48 pages).} It provides an introduction to ethical and societal concerns related to AI (e.g., principles, processes, and methods), targeting technologists, regulators, interest groups, and society as a whole.

% \todo{add description of the new ISO 42001 and 25059.}

\noindent\textbf{ISO 42001 (AI Management System, 51 pages).} It outlines the requirements for the establishment, implementation, maintenance, and continuous improvement of an Artificial Intelligence Management System in organizations.

% 1, 

% published: 42001, 25059
% draft: 42005, 42006 (auditing - under commeting period) and EU standards  CEN/CENELEC

% 42005 - AI Impact Assessment - send the draft
% European body to make standards from legislation

\noindent\textbf{ISO 25059 (Quality Model for AI Systems, 15 pages).} It describes characteristics and sub-characteristics that offer a unified terminology for specifying, measuring, and evaluating the quality of AI systems.


As the final step of refining the catalog, the two authors reviewed the 85 articles of the EU AI Act~\cite{eu_ai_act_2022} to map each of the 22 guidelines with the most relevant article(s), as shown in the last column of Table~\ref{tbl:techniques}. They began with Article 3 of the Act, which defines the key concepts of an AI system, including its definition, intended purpose, performance, training, validation, and post-deployment monitoring. After reading all the articles and annotating them, they identified 22 unique articles corresponding to the guidelines. Articles 9, 10, and 17 were mapped to multiple guidelines.
For example, Article 9 (\emph{Risk management system}) states that ``a risk management system shall be established, implemented, documented and maintained throughout the entire lifecycle of a high-risk AI system''. This article aligns with guidelines \#1, \#3-5, and \#13 as it is about the identification of harms and risks of the AI system's intended use. Article 10 (\emph{Data and data governance}) states that \emph{``training, validation and testing data sets shall be subject to appropriate data governance and management practices''.} This article aligns with guidelines \#8 and \#15-18 as it discusses the management and quality of data for training, validation, and testing, including aspects of diversity and minimizing biases. Finally, Article 17 (\emph{Quality management system}) states that \emph{``an AI system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions''}. This article aligns with guidelines \#6, \#7, \#10, and \#14-18 because it is about documentation of all system components, including AI models and testing and validation procedures. The full mapping along with justifications is provided in Appendix~\ref{app:mapping_guidelines}.

\begin{table}
\caption{Responsible AI guidelines are actionable items that can be considered during the 3 phases of AI development lifecycle. These guidelines are grounded in the scientific literature (main sources are reported), and were checked for ISO ``compliance'': ISO 38507 (Governance); ISO 23894 (Risk management); ISO 5338 (AI lifecycle processes); ISO 24028 (Trustworthiness); ISO 24027 (Bias); ISO 24368 (Ethical considerations); ISO 42001 (AI management system); and ISO 25059 (Quality model for AI systems). They were also cross-referenced with the EU AI Act's articles~\cite{eu_ai_act_2022}. They are marked with the `Phase' during which a guideline can be applied. There are three phases: development ($P_1$), deployment ($P_2$), and use ($P_3$). Guidelines are also marked with the job `Role' that should consider them. There are three roles: designer ($R_{D}$),  engineer or researcher ($R_E$), and manager or executive ($R_M$). Each guideline is followed by an example, and the guidelines are categorized thematically into six categories, concerning the \emph{intended uses, harms, system, data, oversight, and team}.}
\label{tbl:techniques}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllll}
\toprule
\textbf{Number} & \textbf{Guideline} & \textbf{Phase} & \textbf{Role} & \textbf{Source(s)} & \textbf{ISO} & \textbf{AI Act}\\ \midrule
\multicolumn{2}{l}{\textbf{INTENDED USES}} &  &  \\
1 & \begin{tabular}[t]{l}Work with relevant parties to identify intended uses. \\ (e.g., identify the system's usage, deployment, and contextual conditions)\end{tabular} & 
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} \cite{mitchell2019model} \end{tabular}
 & \begin{tabular}[t]{l} 5338, 38507, 23894, \\ 24027, 24368, 42001\end{tabular} & Art. 6, 9\\

2 & \begin{tabular}[t]{l}Obtain approval from an Ethics Committee or similar body for intended uses.\\ (e.g., Obtain Ethics Committee approval for the intended use, aligned with sustainability goals)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}38507, 5338, 23894, \\ 42001\end{tabular} & Art. 11, 69 \\ 

\midrule
\multicolumn{2}{l}{\textbf{HARMS}} &  &  \\
3 & \begin{tabular}[t]{l}Identify potential harms and risks associated with the intended uses. \\  (e.g., prevent privacy violation, discrimination, and adversarial attacks, provide interpretable output)\end{tabular} & 
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} \cite{madaio2020co} \end{tabular} & \begin{tabular}[t]{l}23894, 24028, 38507, \\ 24368, 42001, 25059\end{tabular}  & Art. 9, 65\\

4 & \begin{tabular}[t]{l}Provide mechanism(s) for incentivizing reporting of system harms. \\  (e.g., provide contact emails and feedback form to raise concerns)\end{tabular} &
$P_{1}$ & $R_{D,E}$ &
\begin{tabular}[t]{l} \cite{madaio2020co} \end{tabular} & \begin{tabular}[t]{l}38507, 23894, 42001\end{tabular} & Art. 9, 60-63\\

5 & \begin{tabular}[t]{l}Develop strategies to mitigate identified harms or risks for each intended use. \\ (e.g., use stratified sampling and safeguards against adversarial attacks during training)\end{tabular} &
$P_{1-3}$ & $R_{D,M}$ &
\begin{tabular}[t]{l} \cite{mitchell2019model} \end{tabular}
 & \begin{tabular}[t]{l}24368, 23894,\\ 42001, 25059\end{tabular} & Art. 9, 67\\ 

\midrule
\multicolumn{2}{l}{\textbf{SYSTEM}} &  &  \\
6 & \begin{tabular}[t]{l}Document all system components, including the AI models, to enable reproducibility and scrutiny. \\ (e.g., create UML diagrams, flowcharts, and specify model types, versions, hardware architecture)\end{tabular} &
$P_{1-3}$ & $R_{D,E}$ &
 \begin{tabular}[t]{l} \cite{madaio2020co} \end{tabular} & \begin{tabular}[t]{l}5338, 23894, 24027, \\ 42001, 25059\end{tabular} & Art. 11, 12, 16-18, 50\\

7 & \begin{tabular}[t]{l}Review the code for reliability\\ (e.g., manage version control using software.)\end{tabular} &
$P_{1-3}$ & $R_{D,E}$ &
 \begin{tabular}[t]{l} --- \end{tabular} & \begin{tabular}[t]{l}5338, 25059 \end{tabular} & Art. 17\\

8 & \begin{tabular}[t]{l}Report evaluation metrics for various groups based on factors such as age, gender, and ethnicity. \\ (e.g., evaluate false positive/negative, AUC, and feature importance across protected attributes)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} \cite{madaio2020co, dixon2018measuring, hardt2016equality} \\ \cite{mitchell2018prediction, verma2018fairness}\end{tabular} & \begin{tabular}[t]{l}23894, 5338, 24028, \\ 24027, 42001 \end{tabular} & Art. 10, 13\\

9 & \begin{tabular}[t]{l} Provide mechanisms for interpretable outputs and auditing. \\ (e.g., output feature importance and provide human-understandable explanations)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
 \begin{tabular}[t]{l} \cite{arrieta2020explainable, kulesza2015principles}  \end{tabular} & \begin{tabular}[t]{l}38507, 24028, \\ 42001, 25059\end{tabular} & Art. 12-14\\

10 & \begin{tabular}[t]{l}Document the security of all system components in consultation with experts. \\ (e.g., guard against adversarial attacks and unauthorized access)\end{tabular} &
$P_{1-3}$ & $R_{E,M}$ &
 \begin{tabular}[t]{l} \cite{fjeld2020principled} \end{tabular} & \begin{tabular}[t]{l}24028, 24368, 42001, 25059 \end{tabular} & Art. 12, 13, 15, 17\\

11 & \begin{tabular}[t]{l}Provide an environmental assessment of the system. \\ (e.g., report the number of GPU hours used in training and deployment)\end{tabular} &
$P_{1-3}$ & $R_{E}$ &
\begin{tabular}[t]{l} \cite{sharir2020cost, hao2019training} \end{tabular}
 & \begin{tabular}[t]{l}38507, 23894, 5338, \\ 24368, 42001, 25059\end{tabular} & Art. 69\\

12 & \begin{tabular}[t]{l}Develop feedback mechanisms to update the system. \\ (e.g., provide contact email, feedback form, and notification of new knowledge extracted)\end{tabular} &
$P_{1-3}$ & $R_{D,E}$ &
 \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}24028, 42001\end{tabular} & Art. 61\\

13 & \begin{tabular}[t]{l}Ensure safe system decommissioning.\\ (e.g., ensure decommissioned data is either deleted or restricted to authorized personnel.)\end{tabular} &
$P_{3}$ & $R_{E}$ &
 \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}38507, 24368, 42001\end{tabular} & Art. 9\\

14 & \begin{tabular}[t]{l}Redocument model information and contractual requirements at every system update.\\  (e.g., update the model information when re-training the system or using datasets with new contractual requirements)\end{tabular} &
$P_{3}$ & $R_{E}$ &
 \begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}23894, 5338, 24368, \\ 42001 \end{tabular} & Art. 11, 12, 17, 61\\ 

\midrule
\multicolumn{2}{l}{\textbf{DATA}} &  &  \\

15 & \begin{tabular}[t]{l}Ensure compliance with agreements and legal requirements when handling data. \\  (e.g., create data sharing and non-disclosure agreements and secure servers)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
 \begin{tabular}[t]{l} --- \end{tabular} & \begin{tabular}[t]{l}38507, 23894, 5338 \\ 42001\end{tabular} & Art. 10, 17, 61\\ 

16 & \begin{tabular}[t]{l}Compare the quality, representativeness, and fit of training and testing datasets with the intended uses. \\  (e.g., report dataset details such as public/private, personal information, demographics, and data provenance)\end{tabular} &
$P_{1-3}$ & $R_{E}$ &
 \begin{tabular}[t]{l}\cite{bender2018data, gebru2021datasheets, holland2018dataset, wang2020revise}\\ \cite{madaio2020co, mitchell2018prediction, verma2018fairness}\end{tabular}
& \begin{tabular}[t]{l}38507, 5338, 24028, \\ 24027, 42001, 25059\end{tabular} & Art. 10, 13, 17, 64 \\

17 & \begin{tabular}[t]{l}Identify any measurement errors in input data and their associated assumptions. \\ (e.g., account for potential input errors in the input device, text data, audio, and video)\end{tabular} &
$P_{1-3}$ & $R_{E}$ &
\begin{tabular}[t]{l} \cite{collins2015transparent} \end{tabular} & \begin{tabular}[t]{l}38507, 42001, 25059\end{tabular} & Art. 10, 13, 17, 64 \\

18 & \begin{tabular}[t]{l}Protect sensitive variables in training/testing datasets. \\ (e.g., protect sensitive data and use techniques such as k-anonymity and differential privacy)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} \cite{dworkdifferential} \end{tabular} & \begin{tabular}[t]{l}38507, 24028, 42001\end{tabular} & Art. 10, 13, 17 \\ 

\midrule
\multicolumn{2}{l}{\textbf{OVERSIGHT}} &  &  \\
19 & \begin{tabular}[t]{l}Continuously monitor metrics and utilize guardrails or rollbacks to ensure the system's output stays within a desired range. \\ (e.g., validate against concept drift and test with diverse testers and compliance and adversarial cases)\end{tabular} &
$P_{1-3}$ & $R_{D,E}$ &
\begin{tabular}[t]{l} \cite{fjeld2020principled} \end{tabular}  & \begin{tabular}[t]{l}38507, 5338, 24028, \\ 24027, 24368, 42001\end{tabular} & Art. 12, 20, 29, 61 \\

20 & \begin{tabular}[t]{l}Ensure human control over the system, particularly for designers, developers, and end-users.  \\ (e.g., include human in the loop with the ability to inspect data, models, and training methods)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} ---\end{tabular} & \begin{tabular}[t]{l}38507, 5338,\\ 24028, 24368, 25059\end{tabular} & Art. 13, 14\\ 

\midrule
\multicolumn{2}{l}{\textbf{TEAM}} &  &  \\

21 & \begin{tabular}[t]{l}Ensure team diversity.  \\ (e.g., consider diversity in gender, neurotypes, personality traits, and thinking styles)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} --- \end{tabular} & \begin{tabular}[t]{l}38507, 5338, 24028, \\ 24368, 42001\end{tabular} & Art. 69\\ 

22 & \begin{tabular}[t]{l}Train team members on ethical values and regulations. \\ (e.g., train on privacy regulations, ethical issues, and raising concerns)\end{tabular} &
$P_{1-3}$ & $R_{D,E,M}$ &
\begin{tabular}[t]{l} \cite{fjeld2020principled}\end{tabular}  & \begin{tabular}[t]{l}38507, 24368, 42001\end{tabular} & Art. 69\\ 

\bottomrule
\end{tabular}%
}
\end{table}


\subsection{Finalizing the Catalog}
In response to the interviews with AI developers and standardization experts, we incorporated an example for each guideline. For instance, under the guideline on system interpretability (guideline \#9), the example provided reads: ``output feature importance and provide human-understandable explanations.'' Furthermore, we simplified the language by avoiding domain-specific or technical jargon. We also categorized each guideline into six thematically distinct categories, namely \emph{intended uses}, \emph{harms}, \emph{system}, \emph{data}, \emph{oversight}, and \emph{team}.

Recognizing that certain guidelines may only be applicable at specific stages (e.g., monitoring AI after deployment) by specific roles (e.g., developers, managers), we went through two steps. First, we assigned the guidelines to three phases based on previous research (e.g., \cite{madaio2020co, mitchell2019model}). These phases are development (designing and coding the system), deployment (transferring the system into the production stage), and use (actual usage of the system). For example, guidelines like identifying the system's intended uses (guideline \#1) are relevant to all three phases, while those related to system updates (guideline \#14) or decommissioning (guideline \#13) are applicable during the use phase. 

Second, based on previous literature, we assigned the guidelines to the three roles of designers, engineers/researchers, and managers/executives  (Table~\ref{tbl:techniques}). Wang \emph{et al.}~\cite{wang2023designing} interviewed UX practitioners and responsible AI experts to understand their work practices. UX practitioners included designers, researchers, and engineers, while responsible AI experts included ethics advisors and specialists. Wong \emph{et al.}~\cite{wong2023seeing} analyzed 27 ethics toolkits to identify the intended audience of these toolkits, specifically those who are expected to engage in AI ethics work. The intended audience roles identified included software engineers, data scientists, designers, members of cross-functional or cross-disciplinary teams, risk or internal governance teams, C-level executives, and board members. Additionally, Madaio \emph{et al.}~\cite{madaio2020co} co-designed a fairness checklist with a diverse set of stakeholders, including product managers, data scientists and AI/ML engineers, designers, software engineers, researchers, and consultants. Following guidance  from these studies~\cite{wang2023designing,wong2023seeing,madaio2020co}, we formulated three roles as follows: 
\begin{enumerate}
    \item Designer: This role includes interaction designers and UX designers.
    \item Engineer or Researcher: This role includes AI/ML engineers, AI/ML researchers, data scientists, software engineers, UX engineers, and UX researchers.
    \item Manager or Executive: This role includes  product managers, C-suite executives, ethics advisors/responsible AI consultants, and ethical board members.
\end{enumerate} 

The revised and final catalog, consisting of 22 unique guidelines, is presented in Table~\ref{tbl:techniques}.
