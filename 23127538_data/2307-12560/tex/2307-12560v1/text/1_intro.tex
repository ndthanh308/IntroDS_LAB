\vspace{-3pt}
\section{Introduction}
\label{sec:intro}

Image editing has long been a central topic in computer vision and generative modeling. Advances in generative models have enabled increasingly sophisticated techniques for controlled editing of real images \cite{imagic, controlnet, mokady2022nulltext}, with many of the latest developments emerging from denoising diffusion models \cite{ddpm, ddim, ldm, dalle2, imagen}. But to our knowledge, no techniques have been demonstrated to date for generating high quality interpolations between real images that differ in style and/or content.

Current image interpolation techniques operate in limited contexts. Interpolation between generated images has been used to study the characteristics of the latent space in generative adversarial networks \cite{stylegan, stylegan2}, but such interpolations are difficult to extend to arbitrary real images as such models only effectively represent a subset of the image manifold (\eg, photorealistic human faces) and poorly reconstruct most real images \cite{xia2022gan}. Video interpolation techniques are not designed to smoothly interpolate between images that differ in style; style transfer techniques are not designed to simultaneously transfer style and content gradually over many frames. We argue that the task of interpolating images with large differences in appearance, though rarely observed in the real world and hence difficult to evaluate, will enable many creative applications in art, media and design.

We introduce a method for using pre-trained latent diffusion models to generate high-quality interpolations between images from a wide range of domains and layouts (Fig. \ref{fig:teaser}), optionally guided by pose estimation and CLIP scoring. Our pipeline is readily deployable as it offers significant user control via text conditioning, noise scheduling, and the option to manually select among generated candidates, while requiring little to no hyperparameter tuning between different pairs of input images. We compare various interpolation schemes and present qualitative results for a diverse set of image pairs. We plan to deploy this tool as an add-on to the existing Stable Diffusion \cite{ldm} pipeline.
% However, the quality of such interpolations degenerates substantially.
% generative adversarial networks cannot obtain perfect reconstructions of real images from latent space.
% While latent diffusion models enable near-perfect inversion of real images, a naive latent space interpolation often yields poor quality intermediate frames. Our strategy is to guide the interpolation process using the information already available in the given images to yield superior interpolations. Due to the flexibility of diffusion models' conditioning information, 

% Our contributions are as follows:
% \begin{itemize}%[leftmargin=1.4em]
%     \item We introduce the \modelname, the first general framework for learning from implicit neural datasets of arbitrary parameterization.
% \end{itemize}

% \input{tables/method}