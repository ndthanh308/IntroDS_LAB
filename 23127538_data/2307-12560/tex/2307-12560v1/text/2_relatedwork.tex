\vspace{-3pt}
\section{Related Work} \label{sec:related}


\paragraph{Image editing with latent diffusion models}
Denoising diffusion models \cite{ddpm} and latent diffusion models \cite{ldm} are powerful models for text-conditioned image generation across a wide range of domains and styles. They have become popular for their highly photorealistic outputs, degree of control offered via detailed text prompts, and ability to generalize to out-of-distribution prompts \cite{dalle2, imagen}. Follow-up research continued to expand their capabilities, including numerous techniques for editing real images \cite{imagic, brooks2023instructpix2pix, mokady2022nulltext} and providing new types of conditioning mechanisms \cite{controlnet}.

% \paragraph{Latent space interpolation}
Perhaps the most sophisticated techniques for traversing latent space have been designed in the context of generative adversarial networks (GANs), where disentanglement between style and content \cite{stylegan2}, alias-free interpolations \cite{stylegan3}, and interpretable directions \cite{lucy_steerability} have been developed. However, most such GANs with rich latent spaces exhibit poor reconstruction ability on real images, a problem referred to as GAN inversion \cite{xia2022gan}. Moreover, compared to denoising diffusion models, GANs have fewer robust mechanisms for conditioning on other information such as text or pose. Latent diffusion models such as Stable Diffusion \cite{ldm} can readily produce interpolations of generated images \cite{latentblending}, although to our knowledge this is the first work to interpolate real images in the latent space.

% \paragraph{Style transfer}
% Many works have investigated how to generate an image that combines elements of two real images, usually by incorporating the content of one image and style of another. The separation between content and style is often built into the feature space of many generative models and image encoders, allowing for the generation of a combined image by combining and manipulating the features of each image \cite{gatys2016image, li2018closedform}. Our task is distinct from this setting as we want to produce a series of interpolations that may change simultaneously in both style and content.

% \paragraph{Frame interpolation}
% Interpolation of real images emerges in the context of video frame interpolation or novel view synthesis, but in almost all cases, the basic assumption is that the observed frames mostly depict the same style and content. As a result the task often centers on inferring occlusions and reproducing motion via techniques such as optical flow prediction and depth prediction \cite{bao2019depthaware, kong2022ifrnet}.
