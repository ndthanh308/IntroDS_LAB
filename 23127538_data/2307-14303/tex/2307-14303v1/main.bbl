\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@rmstyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{bronkhorst2000cocktail}
A.~W. Bronkhorst, ``The cocktail party phenomenon: A review of research on
  speech intelligibility in multiple-talker conditions,'' \emph{Acta Acustica
  united with Acustica}, vol.~86, no.~1, pp. 117--128, 2000.

\bibitem{cherry1953some}
E.~C. Cherry, ``Some experiments on the recognition of speech, with one and
  with two ears,'' \emph{The Journal of the acoustical society of America},
  vol.~25, no.~5, pp. 975--979, 1953.

\bibitem{wang2017deep}
D.~Wang, ``Deep learning reinvents the hearing aid,'' \emph{IEEE spectrum},
  vol.~54, no.~3, pp. 32--37, 2017.

\bibitem{tao2021someone}
R.~Tao, Z.~Pan, R.~K. Das, X.~Qian, M.~Z. Shou, and H.~Li, ``Is someone
  speaking? {E}xploring long-term temporal features for audio-visual active
  speaker detection,'' in \emph{Proc. ACM Multimedia}, 2021, pp. 3927--3935.

\bibitem{wang2022predict}
J.~Wang, X.~Qian, and H.~Li, ``Predict-and-update network: Audio-visual speech
  recognition inspired by human speech perception,'' \emph{arXiv preprint
  arXiv:2209.01768}, 2022.

\bibitem{liu2022mfa}
T.~Liu, R.~K. Das, K.~A. Lee, and H.~Li, ``{MFA}: {TDNN} with multi-scale
  frequency-channel attention for text-independent speaker verification with
  short utterances,'' in \emph{Proc. ICASSP}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2022, pp. 7517--7521.

\bibitem{qian2021multi}
X.~Qian, M.~Madhavi, Z.~Pan, J.~Wang, and H.~Li, ``Multi-target {DoA}
  estimation with an audio-visual fusion mechanism,'' in \emph{Proc. ICASSP},
  2021, pp. 4280--4284.

\bibitem{pan2020multi}
Z.~Pan, Z.~Luo, J.~Yang, and H.~Li, ``Multi-modal attention for speech emotion
  recognition,'' in \emph{Proc. Interspeech}, 2020, pp. 364--368.

\bibitem{lyon1983computational}
R.~Lyon, ``A computational model of binaural localization and separation,'' in
  \emph{Proc. ICASSP}, vol.~8, 1983, pp. 1148--1151.

\bibitem{hershey2016deep}
J.~R. {Hershey}, Z.~{Chen}, J.~{Le Roux}, and S.~{Watanabe}, ``Deep clustering:
  Discriminative embeddings for segmentation and separation,'' in \emph{Proc.
  ICASSP}, 2016, pp. 31--35.

\bibitem{luo2019conv}
Y.~{Luo} and N.~{Mesgarani}, ``Conv-{TasNet}: Surpassing ideal time–frequency
  magnitude masking for speech separation,'' \emph{IEEE/ACM Trans. Audio,
  Speech, Lang. Process.}, vol.~27, no.~8, pp. 1256--1266, 2019.

\bibitem{luo2020dual}
Y.~{Luo}, Z.~{Chen}, and T.~{Yoshioka}, ``{Dual-Path RNN}: Efficient long
  sequence modeling for time-domain single-channel speech separation,'' in
  \emph{Proc. ICASSP}, 2020, pp. 46--50.

\bibitem{xu2018single}
C.~{Xu}, W.~{Rao}, X.~{Xiao}, E.~S. {Chng}, and H.~{Li}, ``{Single channel
  speech separation with constrained utterance level permutation invariant
  training using grid LSTM},'' in \emph{Proc. ICASSP}, 2018, pp. 6--10.

\bibitem{chen2017deep}
Z.~Chen, Y.~Luo, and N.~Mesgarani, ``Deep attractor network for
  single-microphone speaker separation,'' in \emph{Proc. ICASSP}, 2017, pp.
  246--250.

\bibitem{zeghidour2020wavesplit}
N.~Zeghidour and D.~Grangier, ``Wavesplit: End-to-end speech separation by
  speaker clustering,'' \emph{preprint arXiv:2002.08933}, 2020.

\bibitem{stoller2018wave}
D.~Stoller, S.~Ewert, and S.~Dixon, ``{Wave-U-Net: A multi-scale neural network
  for end-to-end audio source separation},'' in \emph{Proceedings of 19th
  International Society for Music Information Retrieval Conference}, 2018.

\bibitem{liu2019divide}
Y.~Liu and D.~Wang, ``Divide and conquer: A deep {CASA} approach to
  talker-independent monaural speaker separation,'' \emph{IEEE/ACM Trans.
  Audio, Speech, Lang. Process.}, vol.~27, no.~12, pp. 2092--2102, 2019.

\bibitem{wang2023tf}
Z.-Q. Wang, S.~Cornell, S.~Choi, Y.~Lee, B.-Y. Kim, and S.~Watanabe,
  ``{TF-GridNet}: Making time-frequency domain models great again for monaural
  speaker separation,'' in \emph{Proc. ICASSP}, 2023.

\bibitem{geravanchizadeh2021ear}
M.~Geravanchizadeh and S.~Zakeri, ``Ear-{EEG}-based binaural speech enhancement
  (ee-{BSE}) using auditory attention detection and audiometric characteristics
  of hearing-impaired subjects,'' \emph{Journal of Neural Engineering},
  vol.~18, no.~4, p. 0460d6, 2021.

\bibitem{pan2022seg}
Z.~Pan, X.~Qian, and H.~Li, ``Speaker extraction with co-speech gestures cue,''
  \emph{IEEE Signal Process. Lett.}, vol.~29, pp. 1467--1471, 2022.

\bibitem{Chenglin2020spex}
C.~{Xu}, W.~{Rao}, E.~S. {Chng}, and H.~{Li}, ``Sp{E}x: Multi-scale time domain
  speaker extraction network,'' \emph{IEEE/ACM Trans. Audio, Speech, Lang.
  Process.}, vol.~28, pp. 1370--1384, 2020.

\bibitem{spex_plus2020}
M.~Ge, C.~Xu, L.~Wang, E.~S. Chng, J.~Dang, and H.~Li, ``{SpEx}+: A complete
  time domain speaker extraction network,'' in \emph{Proc. Interspeech}, 2020,
  pp. 1406--1410.

\bibitem{wang2019voicefilter}
Q.~Wang, H.~Muckenhirn, K.~Wilson, P.~Sridhar, Z.~Wu, J.~R. Hershey, R.~A.
  Saurous, R.~J. Weiss, Y.~Jia, and I.~L. Moreno, ``{VoiceFilter}: Targeted
  voice separation by speaker-conditioned spectrogram masking,'' in \emph{Proc.
  Interspeech}, 2019, pp. 2728--2732.

\bibitem{he2020speakerfilter}
S.~{He}, H.~{Li}, and X.~{Zhang}, ``{Speakerfilter}: Deep learning-based target
  speaker extraction using anchor speech,'' in \emph{Proc. ICASSP}, 2020, pp.
  376--380.

\bibitem{vzmolikova2019speakerbeam}
K.~{Žmolíková}, M.~{Delcroix}, K.~{Kinoshita}, T.~{Ochiai}, T.~{Nakatani},
  L.~{Burget}, and J.~{Černocký}, ``Speaker{B}eam: Speaker aware neural
  network for target speaker extraction in speech mixtures,'' \emph{IEEE
  Journal of Selected Topics in Signal Processing}, vol.~13, no.~4, pp.
  800--814, 2019.

\bibitem{xiao2019single}
X.~Xiao, Z.~Chen, T.~Yoshioka, H.~Erdogan, C.~Liu, D.~Dimitriadis, J.~Droppo,
  and Y.~Gong, ``Single-channel speech extraction using speaker inventory and
  attention network,'' in \emph{Proc. ICASSP}, 2019, pp. 86--90.

\bibitem{shi2020speaker}
J.~Shi, J.~Xu, Y.~Fujita, S.~Watanabe, and B.~Xu, ``Speaker-conditional chain
  model for speech separation and extraction,'' in \emph{Proc. Interspeech},
  2020, pp. 2707--2711.

\bibitem{delcroix2020improving}
M.~Delcroix, T.~Ochiai, K.~Zmolikova, K.~Kinoshita, N.~Tawara, T.~Nakatani, and
  S.~Araki, ``Improving speaker discrimination of target speech extraction with
  time-domain {SpeakerBeam},'' in \emph{Proc. ICASSP}, 2020, pp. 691--695.

\bibitem{sato2021multimodal}
H.~Sato, T.~Ochiai, K.~Kinoshita, M.~Delcroix, T.~Nakatani, and S.~Araki,
  ``Multimodal attention fusion for target speaker extraction,'' in \emph{Proc.
  SLT}, 2021, pp. 778--784.

\bibitem{ochiai2019multimodal}
T.~Ochiai, M.~Delcroix, K.~Kinoshita, A.~Ogawa, and T.~Nakatani, ``{Multimodal
  SpeakerBeam}: Single channel target speech extraction with audio-visual
  speaker clues,'' in \emph{Proc. Interspeech}, 2019, pp. 2718--2722.

\bibitem{marvin2021}
M.~Borsdorf, C.~Xu, H.~Li, and T.~Schultz, ``Universal speaker extraction in
  the presence and absence of target speakers for speech of one and two
  talkers,'' in \emph{Proc. Interspeech}, 2021, pp. 1469--1473.

\bibitem{wang21aa_interspeech}
W.~Wang, C.~Xu, M.~Ge, and H.~Li, ``Neural speaker extraction with
  speaker-speech cross-attention network,'' in \emph{Proc. Interspeech}, 2021,
  pp. 3535--3539.

\bibitem{usev21}
Z.~Pan, M.~Ge, and H.~Li, ``{USEV}: Universal speaker extraction with visual
  cue,'' \emph{IEEE/ACM Trans. Audio, Speech, Lang. Process.}, vol.~30, pp.
  3032--3045, 2022.

\bibitem{ephrat2018looking}
A.~Ephrat, I.~Mosseri, O.~Lang, T.~Dekel, K.~Wilson, A.~Hassidim, W.~T.
  Freeman, and M.~Rubinstein, ``Looking to listen at the cocktail party: a
  speaker-independent audio-visual model for speech separation,'' \emph{ACM
  Transactions on Graphics}, vol.~37, no.~4, pp. 1--11, 2018.

\bibitem{wu2019time}
J.~{Wu}, Y.~{Xu}, S.~{Zhang}, L.~{Chen}, M.~{Yu}, L.~{Xie}, and D.~{Yu}, ``Time
  domain audio visual speech separation,'' in \emph{Proc. ASRU}, 2019, pp.
  667--673.

\bibitem{mesgarani2012selective}
N.~Mesgarani and E.~F. Chang, ``Selective cortical representation of attended
  speaker in multi-talker speech perception,'' \emph{Nature}, vol. 485, no.
  7397, pp. 233--236, 2012.

\bibitem{choi2013quantifying}
I.~Choi, S.~Rajaram, L.~A. Varghese, and B.~G. Shinn-Cunningham, ``Quantifying
  attentional modulation of auditory-evoked cortical responses from
  single-trial electroencephalography,'' \emph{Frontiers in human
  neuroscience}, vol.~7, p. 115, 2013.

\bibitem{mirkovic2015decoding}
B.~Mirkovic, S.~Debener, M.~Jaeger, and M.~De~Vos, ``Decoding the attended
  speech stream with multi-channel {EEG}: implications for online, daily-life
  applications,'' \emph{Journal of neural engineering}, vol.~12, no.~4, p.
  046007, 2015.

\bibitem{cai2021low}
S.~Cai, P.~Sun, T.~Schultz, and H.~Li, ``Low-latency auditory spatial attention
  detection based on spectro-spatial features from {EEG},'' in \emph{Proc.
  EMBC}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 5812--5815.

\bibitem{cai2021eeg}
S.~Cai, E.~Su, L.~Xie, and H.~Li, ``{EEG}-based auditory attention detection
  via frequency and channel neural attention,'' \emph{IEEE Transactions on
  Human-Machine Systems}, vol.~52, no.~2, pp. 256--266, 2021.

\bibitem{wang2020robust}
L.~Wang, E.~X. Wu, and F.~Chen, ``Robust {EEG}-based decoding of auditory
  attention with high-rms-level speech segments in noisy conditions,''
  \emph{Frontiers in human neuroscience}, vol.~14, p. 557534, 2020.

\bibitem{cai2021auditory}
S.~Cai, P.~Li, E.~Su, and L.~Xie, ``Auditory attention detection via
  cross-modal attention,'' \emph{Frontiers in Neuroscience}, vol.~15, p.
  652058, 2021.

\bibitem{das2019auditory}
N.~Das, T.~Francart, and A.~Bertrand, ``Auditory attention detection dataset
  {KULeuven},'' \emph{Zenodo}, 2019.

\bibitem{TEA_PSE2023}
Y.~Ju, S.~Zhang, W.~Rao, Y.~Wang, T.~Yu, L.~Xie, and S.~Shang, ``{TEA-PSE} 2.0:
  Sub-band network for real-time personalized speech enhancement,'' in
  \emph{Proc. SLT}, 2023, pp. 472--479.

\bibitem{wang2020online}
H.~Wang, Y.~Song, Z.-X. Li, I.~McLoughlin, and L.-R. Dai, ``An online
  speaker-aware speech separation approach based on time-domain
  representation,'' in \emph{Proc. ICASSP}, 2020, pp. 6379--6383.

\bibitem{von2019all}
T.~Von~Neumann, K.~Kinoshita, M.~Delcroix, S.~Araki, T.~Nakatani, and
  R.~Haeb-Umbach, ``All-neural online source separation, counting, and
  diarization for meeting analysis,'' in \emph{Proc. ICASSP}, 2019, pp. 91--95.

\bibitem{han2019online}
C.~Han, Y.~Luo, and N.~Mesgarani, ``Online deep attractor network for real-time
  single-channel speech separation,'' in \emph{Proc. ICASSP}, 2019, pp.
  361--365.

\bibitem{giri21_interspeech}
R.~Giri, S.~Venkataramani, J.-M. Valin, U.~Isik, and A.~Krishnaswamy,
  ``{Personalized PercepNet}: Real-time, low-complexity target voice separation
  and enhancement,'' in \emph{Proc. Interspeech}, 2021, pp. 1124--1128.

\bibitem{wang2020voicefilter}
Q.~Wang, I.~L. Moreno, M.~Saglam, K.~Wilson, A.~Chiao, R.~Liu, Y.~He, W.~Li,
  J.~Pelecanos, M.~Nika, \emph{et~al.}, ``{VoiceFilter-Lite}: Streaming
  targeted voice separation for on-device speech recognition,'' \emph{Proc.
  Interspeech}, pp. 2677--2681, 2020.

\bibitem{han2023online}
C.~Han and N.~Mesgarani, ``Online binaural speech separation of moving speakers
  with a wavesplit network,'' in \emph{Proc. ICASSP}, 2023.

\bibitem{li2023design}
K.~Li and Y.~Luo, ``On the design and training strategies for {RNN}-based
  online neural speech separation systems,'' in \emph{Proc. ICASSP}, 2023.

\bibitem{brungart2007cocktail}
D.~S. Brungart and B.~D. Simpson, ``Cocktail party listening in a dynamic
  multitalker environment,'' \emph{Perception \& psychophysics}, vol.~69,
  no.~1, pp. 79--91, 2007.

\bibitem{kidd2005advantage}
G.~Kidd~Jr, T.~L. Arbogast, C.~R. Mason, and F.~J. Gallun, ``The advantage of
  knowing where to listen,'' \emph{The Journal of the Acoustical Society of
  America}, vol. 118, no.~6, pp. 3804--3815, 2005.

\bibitem{dehak2010front}
N.~Dehak, P.~J. Kenny, R.~Dehak, P.~Dumouchel, and P.~Ouellet, ``Front-end
  factor analysis for speaker verification,'' \emph{IEEE Trans. Audio, Speech,
  Lang. Process.}, vol.~19, no.~4, pp. 788--798, 2010.

\bibitem{wan2018generalized}
L.~{Wan}, Q.~{Wang}, A.~{Papir}, and I.~L. {Moreno}, ``Generalized end-to-end
  loss for speaker verification,'' in \emph{Proc. ICASSP}, 2018, pp.
  4879--4883.

\bibitem{svector2015}
Y.~Z. Işik, H.~Erdogan, and R.~Sarikaya, ``S-vector: A discriminative
  representation derived from i-vector for speaker verification,'' in
  \emph{23rd European Signal Processing Conference}, 2015, pp. 2097--2101.

\bibitem{xu2019time}
C.~{Xu}, W.~{Rao}, E.~S. {Chng}, and H.~{Li}, ``Time-domain speaker extraction
  network,'' in \emph{Proc. ASRU}, 2019, pp. 327--334.

\bibitem{delcroix2018single}
M.~{Delcroix}, K.~{Zmolikova}, K.~{Kinoshita}, A.~{Ogawa}, and T.~{Nakatani},
  ``Single channel target speaker extraction and recognition with speaker
  beam,'' in \emph{Proc. ICASSP}, 2018, pp. 5554--5558.

\bibitem{vzmolikova2017learning}
K.~{Žmolíková}, M.~{Delcroix}, K.~{Kinoshita}, T.~{Higuchi}, A.~{Ogawa}, and
  T.~{Nakatani}, ``Learning speaker representation for neural network based
  multichannel speaker extraction,'' in \emph{Proc. ASRU}, 2017, pp. 8--15.

\bibitem{xu2019optimization}
C.~{Xu}, W.~{Rao}, E.~S. {Chng}, and H.~{Li}, ``Optimization of speaker
  extraction neural network with magnitude and temporal spectrum approximation
  loss,'' in \emph{Proc. ICASSP}, 2019, pp. 6990--6994.

\bibitem{pan2020muse}
Z.~Pan, R.~Tao, C.~Xu, and H.~Li, ``{MuSE}: Multi-modal target speaker
  extraction with visual cues,'' in \emph{Proc. ICASSP}, 2021, pp. 6678--6682.

\bibitem{pan2021reentry}
------, ``Selective listening by synchronizing speech with lips,''
  \emph{IEEE/ACM Trans. Audio, Speech, Lang. Process.}, vol.~30, pp.
  1650--1664, 2022.

\bibitem{afouras2019my}
T.~Afouras, J.~S. Chung, and A.~Zisserman, ``My lips are concealed:
  Audio-visual speech enhancement through obstructions,'' in \emph{Proc.
  Interspeech}, 2019, pp. 4295--4299.

\bibitem{ito2021audio}
K.~Ito, M.~Yamamoto, and K.~Nagamatsu, ``Audio-visual speech enhancement method
  conditioned in the lip motion and speaker-discriminative embeddings,'' in
  \emph{Proc. ICASSP}, 2021, pp. 6668--6672.

\bibitem{parthasarathy2020bottomup}
A.~Parthasarathy, K.~E. Hancock, K.~Bennett, V.~DeGruttola, and D.~B. Polley,
  ``Bottom-up and top-down neural signatures of disordered multi-talker speech
  perception in adults with normal hearing,'' \emph{Elife}, vol.~9, p. e51419,
  2020.

\bibitem{ding2012emergence}
N.~Ding and J.~Z. Simon, ``Emergence of neural encoding of auditory objects
  while listening to competing speakers,'' \emph{Proceedings of the National
  Academy of Sciences}, vol. 109, no.~29, pp. 11\,854--11\,859, 2012.

\bibitem{o2015attentional}
J.~A. O'sullivan, A.~J. Power, N.~Mesgarani, S.~Rajaram, J.~J. Foxe, B.~G.
  Shinn-Cunningham, M.~Slaney, S.~A. Shamma, and E.~C. Lalor, ``Attentional
  selection in a cocktail party environment can be decoded from single-trial
  {EEG},'' \emph{Cerebral cortex}, vol.~25, no.~7, pp. 1697--1706, 2015.

\bibitem{biesmans2016auditory}
W.~Biesmans, N.~Das, T.~Francart, and A.~Bertrand, ``{Auditory-inspired speech
  envelope extraction methods for improved {EEG}-based auditory attention
  detection in a cocktail party scenario},'' \emph{IEEE Transactions on Neural
  Systems and Rehabilitation Engineering}, vol.~25, no.~5, pp. 402--412, 2016.

\bibitem{o2017neural}
J.~O’Sullivan, Z.~Chen, J.~Herrero, G.~M. McKhann, S.~A. Sheth, A.~D. Mehta,
  and N.~Mesgarani, ``Neural decoding of attentional selection in multi-speaker
  environments without access to clean sources,'' \emph{Journal of neural
  engineering}, vol.~14, no.~5, p. 056001, 2017.

\bibitem{aroudi2020cognitive}
A.~Aroudi and S.~Doclo, ``Cognitive-driven binaural beamforming using
  {EEG}-based auditory attention decoding,'' \emph{IEEE/ACM Trans. Audio,
  Speech, Lang. Process.}, vol.~28, pp. 862--875, 2020.

\bibitem{geirnaert2021electroencephalography}
{S.Geirnaert}, S.~Vandecappelle, E.~Alickovic, A.~de~Cheveigne, E.~Lalor, B.~T.
  Meyer, S.~Miran, T.~Francart, and A.~Bertrand, ``Electroencephalography-based
  auditory attention decoding: {Toward} neurosteered hearing devices,''
  \emph{IEEE Signal Process. Mag.}, vol.~38, no.~4, pp. 89--102, 2021.

\bibitem{su2022stanet}
E.~Su, S.~Cai, L.~Xie, H.~Li, and T.~Schultz, ``{STAnet}: A spatiotemporal
  attention network for decoding auditory spatial attention from {EEG},''
  \emph{IEEE Transactions on Biomedical Engineering}, vol.~69, no.~7, pp.
  2233--2242, 2022.

\bibitem{borsdorf2023multi}
M.~Borsdorf, S.~Pahuja, G.~Ivucic, S.~Cai, H.~Li, and T.~Schultz, ``Multi-head
  attention and {GRU} for improved match-mismatch classification of speech
  stimulus and {EEG} response,'' in \emph{Proc. ICASSP}, 2023.

\bibitem{han2019speaker}
C.~Han, J.~O’Sullivan, Y.~Luo, J.~Herrero, A.~D. Mehta, and N.~Mesgarani,
  ``Speaker-independent auditory attention decoding without access to clean
  speech sources,'' \emph{Science advances}, vol.~5, no.~5, p. eaav6134, 2019.

\bibitem{van2016eeg}
S.~Van~Eyndhoven, T.~Francart, and A.~Bertrand, ``{EEG}-informed attended
  speaker extraction from recorded speech mixtures with application in
  neuro-steered hearing prostheses,'' \emph{IEEE Transactions on Biomedical
  Engineering}, vol.~64, no.~5, pp. 1045--1056, 2016.

\bibitem{biss2020}
E.~Ceolini, J.~Hjortkj{\ae}r, D.~D. Wong, J.~O’Sullivan, V.~S. Raghavan,
  J.~Herrero, A.~D. Mehta, S.-C. Liu, and N.~Mesgarani, ``Brain-informed speech
  separation ({BISS}) for enhancement of target speaker in multitalker speech
  perception,'' \emph{NeuroImage}, vol. 223, p. 117282, 2020.

\bibitem{hosseini2021}
M.~Hosseini, L.~Celotti, and {\'E}.~Plourde, ``Speaker-independent brain
  enhanced speech denoising,'' in \emph{Proc. ICASSP}, 2021, pp. 1310--1314.

\bibitem{hosseini2022}
M.~Hosseini, L.~Celotti, and E.~Plourde, ``End-to-end brain-driven speech
  enhancement in multi-talker conditions,'' \emph{IEEE/ACM Trans. Audio,
  Speech, Lang. Process.}, vol.~30, pp. 1718--1733, 2022.

\bibitem{perez2018film}
E.~Perez, F.~Strub, H.~De~Vries, V.~Dumoulin, and A.~Courville, ``{FiLM}:
  Visual reasoning with a general conditioning layer,'' in \emph{Proc. AAAI},
  vol.~32, no.~1, 2018.

\bibitem{zhang2023basen}
J.~Zhang, Q.-T. Xu, Q.-S. Zhu, and Z.-H. Ling, ``{BASEN}: Time-domain
  brain-assisted speech enhancement network with convolutional cross attention
  in multi-talker conditions,'' \emph{arXiv preprint arXiv:2305.09994}, 2023.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' \emph{Proc.
  NeurIPS}, vol.~30, 2017.

\bibitem{de2020machine}
T.~de~Taillez, B.~Kollmeier, and B.~T. Meyer, ``Machine learning for decoding
  listeners’ attention from electroencephalography evoked by continuous
  speech,'' \emph{European Journal of Neuroscience}, vol.~51, no.~5, pp.
  1234--1241, 2020.

\bibitem{vandecappelle2021eeg}
S.~Vandecappelle, L.~Deckers, N.~Das, A.~H. Ansari, A.~Bertrand, and
  T.~Francart, ``{EEG-based} detection of the locus of auditory attention with
  convolutional neural networks,'' \emph{Elife}, vol.~10, p. e56481, 2021.

\bibitem{oppenheim1978theory}
A.~Oppenheim and R.~Schafer, ``Theory and application of digital signal
  processing,'' \emph{Englewood Cliffs}, 1978.

\bibitem{le2019sdr}
J.~Le~Roux, S.~Wisdom, H.~Erdogan, and J.~R. Hershey, ``{SDR}--half-baked or
  well done?'' in \emph{Proc. ICASSP}, 2019, pp. 626--630.

\bibitem{williams1989learning}
R.~J. Williams and D.~Zipser, ``A learning algorithm for continually running
  fully recurrent neural networks,'' \emph{Neural computation}, vol.~1, no.~2,
  pp. 270--280, 1989.

\bibitem{bengio2015scheduled}
S.~Bengio, O.~Vinyals, N.~Jaitly, and N.~Shazeer, ``Scheduled sampling for
  sequence prediction with recurrent neural networks,'' \emph{Proc. NeurIPS},
  vol.~28, 2015.

\bibitem{kolbaek2017multitalker}
M.~Kolb{\ae}k, D.~Yu, Z.-H. Tan, and J.~Jensen, ``Multitalker speech separation
  with utterance-level permutation invariant training of deep recurrent neural
  networks,'' \emph{IEEE/ACM Trans. Audio, Speech, Lang. Process.}, vol.~25,
  no.~10, pp. 1901--1913, 2017.

\bibitem{kingma2015adam}
D.~P. Kingma and J.~Ba, ``Adam, a method for stochastic optimization,'' in
  \emph{Proc. ICLR}, vol. 1412, 2015.

\bibitem{vincent2006performance}
E.~Vincent, R.~Gribonval, and C.~F{\'e}votte, ``Performance measurement in
  blind audio source separation,'' \emph{IEEE Trans. Audio, Speech, Lang.
  Process.}, vol.~14, no.~4, pp. 1462--1469, 2006.

\bibitem{rix2001perceptual}
A.~W. Rix, J.~G. Beerends, M.~P. Hollier, and A.~P. Hekstra, ``Perceptual
  evaluation of speech quality ({PESQ})-a new method for speech quality
  assessment of telephone networks and codecs,'' in \emph{Proc. ICASSP}, 2001,
  pp. 749--752.

\bibitem{taal2010short}
C.~H. Taal, R.~C. Hendriks, R.~Heusdens, and J.~Jensen, ``A short-time
  objective intelligibility measure for time-frequency weighted noisy speech,''
  in \emph{Proc. ICASSP}, 2010, pp. 4214--4217.

\end{thebibliography}
