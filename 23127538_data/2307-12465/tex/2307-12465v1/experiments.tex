We present an empirical study of the proposed \sysname\ approach for \js vulnerabilities with \codeql as the \sa\ tool. In particular, we look at two vulnerabilities, \unvalidatedmembership and \xss,  prevalent in \js repositories. The goal of our study is to investigate how well the strategies learned by \sysname\ generalize to code in the wild, and how our method fares in comparison with state-of-the-art techniques for repair.
% \begin{enumerate}
%     \item \textit{How well do the strategies learned by \emph{\sysname}\ generalize to code in the wild?} We evaluate various repair techniques on a large number of public \js repositories with vulnerabilities detected by \codeql (Section~\ref{sec:benchwild}).
%     \item \textit{How effective is the proposed \emph{\sawitness}\ strategy?} We evaluate (a) various repair techniques using a fixed train-test split of the data collected by our strategy for the two vulnerabilities (Section~\ref{sec:benchsmall}); and (b) the effect of training size on the performance of \sysname (Section~\ref{sec:ablations}). 
% \end{enumerate}

%\subsection{TODOS}
% \begin{enumerate}
%     \item Increase coverage of the current "good" queries. Can we run the ML-version of CodeQL?
%     \item Get data from LGTM on code marked as vulnerable
%     \item Improve prototype poll results
%     \item Memory blowup issue
%     \item \sout{Take a look at serialization of strategies} 
%     \item \sout{Build unsafe wrappers directly}
%     \item Splitting on repo level instead of edit level
%     \item Serialization bugs - github
%     \item Bad code from good code bugsL ocalizing for smaller asts (dummy functions, etc)
%     \item cross validation strategy


% \end{enumerate}

% \subsection{Implementation}
% \label{subsec:implementation}

\subsection{Datasets}
We work with two types of datasets --- one dataset exclusively for training and the other exclusively for evaluation.\footnote{Our datasets can be found at \url{http://aka.ms/StaticFixer
}} 

For \textit{training} (e.g., for learning strategies in \sysname{}), we use \js\ programs in the repositories available on \textsc{LGTM}~\cite{lgtm} that are witnessing-safe (discussed in Section~\ref{sec:data}). We form a dataset (``\benchsmall'') of (safe, unsafe) programs, using our \sawitness technique and \codeql\ queries on \textsc{LGTM}~\cite{lgtm}, for the two vulnerability classes \unvalidatedcall and \xss. This dataset contains \unsure{800} paired programs for \unvalidatedcall and \unsure{66} paired programs for \xss; each pair consists of (i) a \js \safeprog that has a \textit{witness} relation (a sanitizer or a guard, with the corresponding source and sink nodes) discussed in Section~\ref{subsec:sa-witness}, and (ii) the corresponding ``unsafe'' program that is obtained by removing the witness using techniques discussed in Section~\ref{subsec:witness-removal}. %In our experiments, we use the \emph{\benchsmall} dataset exclusively for training the models (e.g., for learning strategies in \emph{\sysname{}}). 

For \textit{evaluation}, we consider \js code in the repositories on LGTM~\cite{lgtm} (``\benchwild'') that are flagged as vulnerable (\xss or \unvalidatedmembership) by \codeql. We purge all the duplicate files --- e.g., common \js\ libraries are part of multiple repositories. After de-duplication, we have 330 unsafe \js\ files from 204 repositories for \unvalidatedmembership, and 672 unsafe \js\ files from 595 repositories for \xss.


%\unsure{Can we present any interesting stats on the datasets? if we get to files with >1 vulnerability then perhaps we can show distribution of the vulnerability counts.}
% \begin{enumerate}
%     \item Test set 1: For each vulnerability, we have a set of repos. We leave out a set of repos from synthesis and use it for testing
%     \item Test set 2: We run the "bad" query on LGTM and apply the transformations on these repos
%     \item Test set 3: Interna repos?
% \end{enumerate}

\subsection{Compared Techniques}
The datasets we use are real world \js files, and there is no prior work on repairing 
information flow vulnerabilities in \js\ --- so we cannot use prior work in static repair as baselines (we qualitatively compare against them in Section~\ref{sec:related}). For example, if we were to use \othersysname{Phoenix}~\cite{bavishi2019phoenix}, the most closely related system to \sysname, as a baseline then we would need to (i) port its front-end to consume \js programs instead of Java, and (ii) generalize it to handle the repairs we seek from the more localized repairs it performs, which is a huge engineering effort. Furthermore, \othersysname{Phoenix} implementation is proprietary and not available for this purpose.
 However, fine-tuning {\em neural techniques}  to repair information flow vulnerabilities in \js is feasible and we use them as baselines. The engineering effort required here is tenable as the models are available for public use, there are no front-end issues as they consume program strings, and we only need to provide natural language descriptions and program examples. 

Therefore, we compare \sysname{} with the following state-of-the-art neural techniques for code synthesis, adapted for repair:
\begin{enumerate}
    \item \codetfivejs\ --- we fine-tune the state-of-the-art code synthesis model \codetfive~\cite{Wang2021CodeT5IU} on the training set (\benchsmall), to synthesize fixed code given vulnerable code as input.
    \item \codex\ --- we use few-shot learning on the OpenAI's \codex\ model~\cite{Codex}, to synthesize fixed code given vulnerable code and a set of paired programs (from \benchsmall) as input.
    % \item \unsure{AST-only baseline - ablation only for \unvalidatedcall} \aksays{If we are not evaluating this baseline on XSS, it is better to shift this to the subsection on ablations.}
\end{enumerate}

As discussed in Section~\ref{sec:learning}, \sysname solves the problems of both localization and repair. However, given a source-code file, the neural baselines do not have the ability to localize the program statements which need to be transformed for repairing the vulnerabilities. So, we parse the CodeQL warnings and provide the functions identified by CodeQL in its results as part of the input to the neural models. We give more details below. \\
%For \benchsmall, on the other hand, we do provide the functions containing the ground-truth repair locations (identified during data preparation) as part of the input to the neural baselines (in Section~\ref{sec:benchsmall}). Consequently, both our neural baselines are given the benefit of localization whereas \sysname is required to perform end-to-end localization \emph{and} repair. \\
%https://meet.google.com/gsg-sjfm-mtb
\textbf{Implementation details:} We implement \codetfivejs\ as follows. We use the {\tt CodeT5-small} variant of the model made up of 60M parameters initialised with pre-trained weights. 
For each of the two vulnerability classes, we finetune the model on the corresponding \benchsmall dataset to produce fixed code given vulnerable code.
Given a vulnerable file, we use \codeql to identify function where the sink is, and pass this function as input to the model.
% Since \benchsmall is created by perturbing witness-safe files, the edits that need to be performed to fix the vulnerable files are already known.
% We use this to narrow down the parts of the source code being passed to the model, by identifying and passing the function definition containing the required edits as part of input to the model.
% To further aid the model, we insert mask tokens at locations in the vulnerable file where code needs to be added in order to fix the vulnerability. 
% For lines that need to be modified, we comment them out with a special token to indicate that modification is required.
We then finetune the model to produce non-vulnerable code (i.e., the corresponding sink function from the safe program in the training data), given the (localized) vulnerable code.
During evaluation, we use beam-search decoding with a beam size of 20 and a temperature of 1.0, and generate 20 candidate snippets for each input. This ensures a fair comparison to \sysname{} where multiple strategies are applied to a given vulnerable code to generate multiple candidates (for 95\% of the files in~\benchwild,~\sysname{} generates at most 17 candidate fixes). %\spsays{Should we also say that we do the localization here, while in our strategies we learn them as well?} 

%\unsure{ADD DETAILS ABOUT PROMPTS AND INCORPORATE HOW LOCALIZATION IS PERFORMED TO HELP THE NEURAL MODELS ALONG THE LINES OF THE ABOVE PARA.}

For the \codex\ model, we use a subset of unsafe, safe program pairs chosen from the \benchsmall dataset to assemble a ``prompt'' encoding a description of the task, followed by the actual ``question'' (i.e., the vulnerable code) to generate output. 
In the experiments, we construct a prompt of the form $\{d^v, (u^v_1, s^v_1), \dots, (u^v_k, s^v_k), u^v_q\}$; where $d^v$ is a natural language description of how the vulnerability $v$ should be fixed (taken from the CodeQL documentation), $u^v_i$ and $s^v_i$ are unsafe (vulnerable) and the corresponding safe code snippets that demonstrate how the vulnerability should be fixed, and finally $u^v_q$ which is the \textit{localised} vulnerable code snippet that we want the model to fix.
The vulnerability-fixing $(u^v_i, s^v_i)$ examples are drawn from a list of manually-chosen fixes (sink functions, to be consistent with \codetfivejs\ model) from \benchsmall; \codex~has a limit of 8000 input tokens, so the number of such examples in the prompt is determined dynamically depending on the size of $u^v_q$.
We use a temperature setting of 0.9 and generate top 20 candidate outputs ranked by their probability. %\spsays{we can mention here that prompt engineering as well as tuning temperature is beyond the scope of this baseline.}

% For both \codetfivejs and \codex, we  


% \begin{lstlisting}[autogobble,basicstyle=\footnotesize]


% \end{lstlisting}


% \begin{lstlisting}[autogobble,basicstyle=\footnotesize]

% \end{lstlisting}

% \sysname{} on the other hand is able to 

% \begin{lstlisting}[autogobble,basicstyle=\footnotesize]
% if (crawlers.hasOwnProperty(ctx.params.type)) {
% ojFunc = crawlers[ctx.params.type];
% }
% \end{lstlisting}

% \begin{enumerate}
%     \item End to end neural fixer: Fine-tune CodeT5 to output safe code, given a vulnerability code. We use the same data set used to learn strategies for fine-tuning
%     \item Token based methods: Like Edit distance
%     \item Codex and Codex-E?
% \end{enumerate}
\subsection{Metrics}
%\unsure{Do we need a subsection for this or can we state it just ahead of presenting results?} 
We report the number of successful fixes --- we count an unsafe (as flagged by \codeql) code as fixed if the corresponding output code from a method passes the vulnerability check of \codeql. Additionally, \unsure{we manually inspect the generated code to check if there are any unintended changes introduced in the original code. We deem the candidate fixes unsuccessful if there are any unintended changes or if they have any syntactic errors.} 
% \begin{enumerate}
%     \item Exact match with "safe" code we have
%     \item Only CodeQL check (Oracle)
%     \item Data flow match?
% \end{enumerate}

\subsection{Results}
\label{sec:benchwild}
The number of successful fixes obtained via our method and the baselines on the \benchwild~dataset is reported in Table~\ref{tab:resultswild}. Recall that, for all the methods, we use the unsafe, safe program pairs from the \benchsmall{} dataset for training. It is clear from the results that~\sysname{} is not only significantly better than the baselines relatively but is also highly accurate in an absolute scale. In particular,~\sysname{} (a) generates a successful fix (out of the possibly multiple fixes generated) for nearly 94\% of the (vulnerable) files in the \unvalidatedmembership~class and for nearly 92\% of the files in the \xss~class, and (b) significantly outperforms, by as much as 2.5x, both the state-of-the-art neural techniques in both the vulnerability classes.

Even though we provide additional context to help the neural models localize (such as functions in the CodeQL warnings), these models fundamentally do not try to encode or exploit the domain knowledge. Fine-tuning very large neural models typically needs thousands, if not more, of training examples to be able to generalize well. However, in the real world, it is extremely challenging to collect training data of such scale without significant human effort. 

Consider the following function in \benchwild that is flagged for \unvalidatedmembership vulnerability in line 2:

\begin{lstlisting}[autogobble,basicstyle=\scriptsize,xleftmargin=.24\textwidth, xrightmargin=.2\textwidth]
router.get('/api/crawlers/:type/:username', async (ctx) => {
  const ojFunc = crawlers[ctx.params.type]
  if (!_.isFunction(ojFunc)) {
    throw new Error('Crawler of the oj does not exist')
  }
  ctx.rest(await ojFunc(ctx.params.username))
})
\end{lstlisting}
\lstMakeShortInline[columns=fixed]@

When passed as input to \codetfivejs, we observe that the top candidate fixes it generates fall into two categories: (a) adding a redundant type check after line 5 such as @if (typeof ojFunc === 'function') { ojFunc = crawlers[ctx.params.type] }@, or (b) adding an incorrect membership check such as @if (ctx.params.username in crawlers)@ at line 2. Even though (b) captures the structure of the desired fix, it is not semantically correct, i.e., it checks for the wrong member @username@ instead of @type@. We find that many of the unsuccessful cases for the neural models have similar failure modes --- it reflects the inability of the neural models to capture both the structural and semantic contexts needed to produce intended repairs, for real-world scenarios. \sysname{}, by design, takes into account the data flow information and the semantics and produces semantically correct fixes in a vast majority of cases. In particular, for this example, it generates @if (crawlers.hasOwnProperty(ctx.params.type)) {ojFunc = crawlers[ctx.params.type];}@, at line 2.
\lstDeleteShortInline@
We also notice from Table~\ref{tab:resultswild} that \codex performs poorly on~\xss vulnerability with the most common failure case being that it copies over the vulnerable input as the output.

% While the fix for \xss  usually only involves calling a function on a particular variable, the context in which it needs to be done is wide ranging.
% Codex might be suffering her since it cannot capture the variation in context due to only having access to a limited number of examples in the prompt.

% To enable \codex to generalise to a wide range of cases, it would need a sufficiently varied prompt, which is difficult to engineer given the limited input capacity of the model.


%On the other hand,~\sysname{} generalizes extremely well even though it relies on a limited set (in the order of a few hundreds) of training programs collected via our proposed \safeprog technique. 

The success of our method can be attributed to two factors: a) our DSL provides a rich space of strategies, b) our learning algorithm learns a diverse set of strategies (\unsure{292} in total for \unvalidatedmembership, and \unsure{28} for \xss) guided by the limited set of ``perturbed'' safe programs and the witnesses in the training set. This diversity of strategies helps generalize to programs in the wild, which may deviate significantly in size, structure, and semantics from those in the training dataset. Furthermore, we find that, on average,~\sysname{} produces about 4 unique fixes for a given vulnerable code in the~\benchwild dataset, of which about 3 are successful. Thus,~\sysname{} is not only successful on a vast majority of the test files, but also produces multiple, unique correct fixes. This can be especially helpful in practice, when factors besides correctness can determine the suitability of a fix. 

%\unsure{why codex xss is low?}

\begin{table*}[!t]
    \centering
    \begin{tabular}{l|c|c}
       \toprule
       Method & \unvalidatedmembership & \xss  \\ \toprule
       \codetfivejs &  127 (38.48\%)& 541 (80.51\%)\\
       \codex &  220 (66.67\%)& 219 (32.59\%)\\
       %\unsure{AST-only-baseline} & ? (?\%)& ? (?\%)\\ \midrule
       \sysname &  \textbf{310} (93.94\%)& \textbf{617} (91.82\%)\\ \toprule
    \end{tabular}
    \caption{Number of successful fixes by various methods on the \benchwild\ dataset, out of (i) 330 \js files for \unvalidatedmembership, and (ii) 672 \js files for \xss. All the methods are trained on the~\benchsmall dataset.}
    \label{tab:resultswild}
    \vspace{-25pt}
\end{table*}

% \begin{table*}[!t]
%     \centering
%     \begin{tabular}{l|c|c}
%        \toprule
%        Method & \unvalidatedmembership & \xss  \\ \toprule
%        \codetfivejs & 42 (91.30\%)& 17 (?\%)\\
%        \codex & 41 (89.13\%)& 11 (?\%)\\
%        %\unsure{AST-only-baseline} & ? (?\%)& ? (?\%) \\ \midrule
%        \sysname & 45 (97.82\%)& ? (?\%)\\ \toprule
%     \end{tabular}
%     \caption{Number of successful fixes by various methods on the \benchsmall\ dataset, out of (i) 46 test \js files for \unvalidatedmembership, and (ii) 25 test \js files for \xss.}
%     \label{tab:resultssmall}
% \end{table*}

% \begin{table*}[!t]
%     \centering
%     \begin{tabular}{c|c|c}
%        \toprule
%        Training Set Size & Number of strategies & Number of fixes \\ \toprule
%        \unsure{??} & ? & ? \\
%        \unsure{??} & ? & ? \\
%        \unsure{??} & ? & ? \\ \toprule
%     \end{tabular}
%     \caption{Each row corresponds to training \sysname{} with the given number of paired programs (``Training Set Size'') from the \benchsmall{} dataset. The second column is the number of strategies learned using the training data; the last column is the number of successful fixes by \sysname{} out of \unsure{??} test \js files for \unvalidatedmembership.}
%     \label{tab:ablation}
% \end{table*}

% \subsection{Ablations}
% \label{sec:ablations}

% \subsubsection{Effect of semantic edges} \aksays{This section is needed only if we are evaluating the AST-only baseline on one of the vulnerabilities. Otherwise, we can lift the comparison to Section 6.4 and Tables 2-3.}

% \subsubsection{Effect of the training set size}

% Table~\ref{tab:ablation}