\begin{table}[!t]
    \centering
    \begin{tabular}{|p{0.17\linewidth} | p{0.80\linewidth}|}
       \hline
       \textbf{Term} & \textbf{Definition} \\
       \hline
       source & a variable whose value is directly set by an (untrusted) user \\
       tainted variable & a variable whose value is derived from a source variable and is controllable by an (untrusted) user; the value of such a variable is called tainted value \\
       sink & a program execution performing a security-critical operation using an input \\
       sanitizer & function that takes the tainted variable as input and removes the taint\\
       guard & a check performed on the tainted values to block execution on malicious inputs \\
       \hline
    \end{tabular}
    \caption{\taintprop vulnerability terminologies and definitions}
    \label{tab:taint-terminologies}
\end{table}

%This section will introduce the repair problem and motivate our approach using examples of vulnerable and safe programs. 

\subsection{Problem Background and Motivating Examples}

\input{example1.tex}

\lstMakeShortInline[columns=fixed]@
In the previous section we defined information flow vulnerabilities as flow of information from an untrusted source to a trusted or sensitive sink without appropriate sanitization.
In Table~\ref{tab:taint-terminologies} we define these terms more precisely. 
\sa tools find these vulnerabilities by detecting sources and sinks, and then performing a dataflow analysis between them.
We call a program unsafe or safe depending on whether \sa detects a violation or not. Further, we call a program a  {\em \safeprog} if it is safe because a {\em witness}, i.e., a sanitizer or a guard, blocks the flow between a source and a sink. Note that a safe program is not necessarily a witnessing-safe program. In particular, a program that does not have any sources or sinks is vacuously safe, but not a witnessing-safe program.
Next, we give two simplified examples with violations and show how one can fix them. 
%These vulnerabilities can be detected. Static taint analysis detects these violations by finding potential sources and sinks in programs and checking the existence of taint-flow paths between them. The taint-propagation also needs to be sanitizer or guard aware i.e. the taints do not propagate on the output of sanitizer or inside a guarded block. Thus, when these tools find a violation, they will return the corresponding source-sink pair and optionally also the taint-flow path between them. 

\noindent \textbf{Example 1.} Figure~\ref{fig:unsafememberex} is an unsafe program containing the \unvalidatedmembership vulnerability where an untrusted user input is used as a key to index into a record of functions. A malicious user can exploit this vulnerability by passing missing keys or keys defined by parent or base classes like @"__proto__"@ or @"constructor"@.
Here, the vulnerability arises from the flow between the source @event@ and the unvalidated function call on line 6. The @commHandler@ function in Line~\ref{lst:line:commHandler-source} takes in a user input in the form of @event@ variable, thus making @event@ the source (highlighted in orange). This source variable now propagates taint across expressions in the program, as depicted by the dataflow edges 1-7 (in cyan). Specifically @event.data@, @data@, and @data.id@ are all tainted expressions. Notice that the \taintprop happens across method boundaries through the @handlers["run"]@ function in Line~\ref{lst:line:handlers-run}. In Line~\ref{lst:line:callerId-index}, the @handlers@ object is dynamically indexed with the @data.id@ tainted-variable and the indexed value is called as a function in Line~\ref{lst:line:callerId-sink} 
without checking whether the record  @handlers@ has a function with key @data.id@ as its ``own'' property\footnote{\url{https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global\_Objects/Object/hasOwnProperty}}. 

Figure~\ref{fig:safememberex} depicts a  corresponding safe program with the transformation highlighted in green. The vulnerability is fixed by replacing the statement containing the sink, \wrapbox{red}{foo(data)} with the if-statement (guard) between Lines~\ref{lst:line:fix-start} and ~\ref{lst:line:fix-end}. This guard blocks the execution of the sink on malicious user inputs. Specifically, @handlers.hasOwnProperty(data.id)@ (Line ~\ref{lst:line:fix-start}) checks whether @data.id@ is indeed an \textit{own property} of the object and blocks the execution of the sink otherwise. Notice that the variables in the guard  (@handlers@ and @data.id@) do not syntactically appear in the sink statement (\wrapboxintext{red}{foo(data)}). Hence,  the repair strategy must use  non-local dataflow information to repair.

\noindent \textbf{Example 2.} Figure~\ref{fig:unsafexss}
is an unsafe program with the cross-site scripting (\xss for short) vulnerability where untrusted user-input flows to an HTML response~\cite{xsscodeql,wasp}. A user can create a request where the @req.id@ attribute contains malicious \js code; this code will get executed on the application side making the program unsafe. The
@requestListener@ HTTP server handler on Line~\ref{lst:line:requestlistener} reads the @userId@ input in Line~\ref{lst:line:userId}. This @userId@ is used internally to handle the request as needed and then concatenated with a prefix into the @message@ variable in Line~\ref{lst:line:concat}. Finally, the tainted @message@ variable is sent inside the HTTP response sink. 
\input{example2.tex}

Figure~\ref{fig:safexss} depicts a corresponding safe program with the transformation highlighted in green. The vulnerability is fixed by inserting a new statement containing the sanitizer. It removes the taint from a variable and makes execution of the sink on the variable safe. Specifically, the @escape@ function in Line~\ref{lst:line:escape-fix} sanitizes the @userId@ variable and removes the taint.
Note that  this transformation is applied at an intermediate location between the source and the sink.


In this paper, we are interested in automatically repairing  \taintprop vulnerabilities by introducing sanitizers and guards. 
We first use a snapshot of a training code base to learn repair strategies. Next, given
an input program with a violation, we provide a high-level overview of how these learned strategies repair the input.
%Correcting the violations requires appropriately adapting these sanitizers or guards for a given program. These repairs are non-trivial and appear similarly in programs, making them a good choice for learning from examples. \naman{does some of this go in introductions??}

%Notice that the guard @handlers.hasOwnProperty@ in Figure~\ref{fig:safememberex} also requires knowledge of program semantics. For example, given information about the sink, we only @@

\subsection{Applying repair strategies}
%We present the overview of the different stages in Figure~\ref{fig:staticfixer-architecture}. \naman{Todo - make the figure correctly and add details here - plan to do this over the weekend}

%There are two critical challenges in building an automatic repair strategy learning system - a.) collecting appropriate data that provides signals about different kinds of sanitizers, and b.) designing the strategy learning system to use the collected data.

%\textbf{I) Data Collection} We cannot run the \sa tool across version-controlled source code because of the costly \taintprop analysis. To collect data more efficiently, we make the following observation -- \sa tool distinguishes between the programs in Figures~\ref{fig:static-witnessing-1},\ref{fig:static-analysis} as it throws an alert on only the unsafe programs. The safe programs contain the same source and sink but are not detected as rule-violating because they also have sanitizers or guards (\textit{witnesses}, and the corresponding programs, therefore, called \safeprog). \sa tools internally reason about these witnesses using the domain knowledge implemented by experts, and we leverage this domain-knowledge to detect the \safeprogs along with the witnesses directly. We call this approach \sawitnessfull and formally describe it in Section~\ref{subsec:sa-witness}. These \safeprogs and witnesses precisely capture the distinguishing factors between unsafe and safe programs and therefore serve as strong signals for learning how to repair unsafe programs. We then perturb the \safeprogs by removing these witnesses to build our dataset of unsafe programs and record the edits.
%\input{architecture.tex}

%\textbf{II)  Strategy Learning}
Given an AST of an unsafe program annotated with sources, sinks, and vulnerable flows, our repair strategies follow a two-step process: find the \textit{edit-location} and then apply an \textit{edit-operation}, where:
\begin{enumerate}
    \item the edit-location is an \astree-node where the edit occurs, and
    \item the edit-operation is a tree-edit operation at the edit-location. Since these vulnerabilities are fixed by introducing sanitizers or guards in programs, we support inserting a child and replacing a child with another tree as the edit operations. 
\end{enumerate}

\textbf{Example 1.} The repair in Figure~\ref{fig:vulnerabilty-example1} introduces an if-statement with @handlers.hasOwnProperty(data.id)@ guard that makes the program safe. %The guard is placed inside an if-statement that was added during the repair. Specifically, the if-statement replaces the statement on Line~\ref{lst:line:callerId-sink} in Figure~\ref{fig:unsafememberex}.
Thus the edit-location is the \astree-node corresponding to the block statement between Lines~\ref{lst:line:handlers-run} and ~\ref{lst:line:handlers-run-end}. The edit-operation replaces the child containing the sink \astree-node with the if-statement.

\textbf{Example 2.} The repair in Figure~\ref{fig:vulnerabilty-example2} inserts an assignment statement at Line~\ref{lst:line:escape-fix} with the @escape@ sanitizer to make the program safe.  Thus the edit-location is the \astree corresponding to the block statement between Lines~\ref{lst:line:requestlistener} and ~\ref{lst:line:requestlistener-end}. The edit-operation inserts the assignment statement as an additional child to this block. 

%To perform these operations, we use programmatic repair strategies that take the original program along with semantic information from \sa. 
The repair strategies have two components: (1) an edit localization component, which predicts the edit-location, and (2) an edit operation component, which constructs the input-program-specific edit that needs to be applied at the edit-location. We describe these components below.

\medskip
\noindent \textbf{Edit Localization}.
This component of the repair strategy encodes a path to the edit location starting from some known location in the program. In the case of information flow violations, the \sa tool outputs the locations of the source and sink, and our repair strategy makes use of these anchors. An example encoding of reaching the edit location from the location of the source is as follows: \textit{traverse data flow semantic edges in the annotated AST from the source along a path till a function call is encountered. Then, traverse syntactic AST edges from this call node to find the innermost block statement which is an ancestor to the function call. That block statement is the edit-location}. 
In Section 4 (Figure 9), we show a DSL for representing repair strategies. The above encoding is represented in this DSL using a \kleeneedge that allows crossing multiple edges of a particular \textit{kind} until reaching a stopping-location. 
In Figure~\ref{fig:vulnerabilty-example1}, this localization component follows the semantic edges 1-7 from \wrapboxintext{orange}{event} to reach the \wrapboxintext{red}{foo} call in Line~\ref{lst:line:callerId-sink}. Then, we traverse  the syntactic parent of the @foo@ call multiple times  to reach the block statement of lines 1--8. Abstracting over the number of edges using \kleeneedge enables the localization component to generalize over other programs, which could be syntactic variations of this example. 
We learn these edge-types (syntactic or semantic) and stopping-locations (function call and block statement) from the training data.

\medskip
\noindent \textbf{Edit Operation.}
%Consider the example repairs in Figure~\ref{fig:vulnerabilty-example1} and Figure~\ref{fig:vulnerabilty-example2}. They introduce sanitizers or guards in the unsafe programs. 
%As mentioned above, we consider edit-operations that either insert or replace a child of the \astree. 
This component has abstract programs that are instantiated to ASTs using the input program. For example, a repair strategy can have an abstract guard @REF1.hasOwnProperty(REF2){REF3}@, where @REF1@ @REF2@ @REF3@ are materialized with AST nodes that can be obtained by traversing {\em reference paths} of the input program. 
For Figure~\ref{fig:vulnerabilty-example1},
instantiating this abstract guard with the unsafe program provides the guard @handlers.hasOwnProperty(data.id){foo(data)}@. Here, @REF2@ (which materializes into @data.id@) is found by traversing the {\em reference path} containing @semantic-parent@ edges from \textit{semantic-location}\footnote{Semantic Location is the node on the edit-path at the boundary between semantic-edges and syntactic-edges defined in Section~\ref{sec:learning}}  \wrapboxintext{red}{foo}. Similarly, @REF1@ and @REF3@ materialize into @handlers@ and @foo(data);@ respectively by traversing reference paths associated with them.

%Next, we describe the data collection procedure to generate examples that are used to learn such localization and edit operation components.

%Our strategies predict what code needs to be inserted or replaced-by at the edit-location along with the \astree-node children-index where insertion or replacement happens. To predict the code, we represent the new code abstractly in its \astree form where some nodes have a type and value while others are copied from existing program elements. Figure~\ref{fig:vulnerabilty-example1}, to predict the if-statement, we store it abstractly as @REF1.hasOwnProperty(REF2){REF3}@. Here, @REF1@ @REF2@ @REF3@ are all references to existing portions of the program. These references materialize at the time of application of strategy to a program by traversing reference paths corresponding to the reference. For example, @REF2@ (which materializes into @data.id@) is found by traversing the @semantic-parent@ edge twice from \textit{semantic-location}\footnote{Semantic Location is the node on the edit-path at the boundary between semantic-edges and syntactic-edges defined in Section~\ref{sec:learning} -- I think a better terminology or \dsl structuring might simplify this definition and make this seem more general and easy to explain} (here \wrapboxintext{red}{handler}). Similarly, @REF1@ and @REF3@ materialize into @handlers@ and @handler(data);@ respectively have similar reference paths associated with them.

%RS the following is too much detail for this point in paper
%Last, we  need to predict the \astree-node children-index at which insertion or replacement must occur. We store it abstractly as the index with 0 offset from \textit{semantic-location} (here \wrapboxintext{red}{handler}). Thus we replace the \astree-child @handlers(data);@ with the if-statement.

\lstDeleteShortInline@
