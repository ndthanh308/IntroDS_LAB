\begin{thebibliography}{10}

\bibitem{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas,
  ``Communication-efficient learning of deep networks from decentralized
  data,'' in {\em Artificial intelligence and statistics}, pp.~1273--1282,
  PMLR, 2017.

\bibitem{zhang2021survey}
C.~Zhang, Y.~Xie, H.~Bai, B.~Yu, W.~Li, and Y.~Gao, ``A survey on federated
  learning,'' {\em Knowledge-Based Systems}, vol.~216, p.~106775, 2021.

\bibitem{kairouz2021advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, {\em et~al.}, ``Advances
  and open problems in federated learning,'' {\em Foundations and
  Trends{\textregistered} in Machine Learning}, vol.~14, no.~1--2, pp.~1--210,
  2021.

\bibitem{lyu2020threats}
L.~Lyu, H.~Yu, and Q.~Yang, ``Threats to federated learning: A survey,'' {\em
  arXiv preprint arXiv:2003.02133}, 2020.

\bibitem{bagdasaryan2020backdoor}
E.~Bagdasaryan, A.~Veit, Y.~Hua, D.~Estrin, and V.~Shmatikov, ``How to backdoor
  federated learning,'' in {\em International conference on artificial
  intelligence and statistics}, pp.~2938--2948, PMLR, 2020.

\bibitem{bhagoji2019analyzing}
A.~N. Bhagoji, S.~Chakraborty, P.~Mittal, and S.~Calo, ``Analyzing federated
  learning through an adversarial lens,'' in {\em International Conference on
  Machine Learning}, pp.~634--643, PMLR, 2019.

\bibitem{fang2020local}
M.~Fang, X.~Cao, J.~Jia, and N.~Gong, ``Local model poisoning attacks to
  $\{$Byzantine-Robust$\}$ federated learning,'' in {\em 29th USENIX security
  symposium (USENIX Security 20)}, pp.~1605--1622, 2020.

\bibitem{sun2021data}
G.~Sun, Y.~Cong, J.~Dong, Q.~Wang, L.~Lyu, and J.~Liu, ``Data poisoning attacks
  on federated machine learning,'' {\em IEEE Internet of Things Journal},
  vol.~9, no.~13, pp.~11365--11375, 2021.

\bibitem{luo2021feature}
X.~Luo, Y.~Wu, X.~Xiao, and B.~C. Ooi, ``Feature inference attack on model
  predictions in vertical federated learning,'' in {\em 2021 IEEE 37th
  International Conference on Data Engineering (ICDE)}, pp.~181--192, IEEE,
  2021.

\bibitem{lamport1982byzantine}
L.~Lamport, R.~Shostak, and M.~Pease, ``The byzantine generals problem,'' {\em
  ACM Transactions on Programming Languages and Systems}, vol.~4, no.~3,
  pp.~382--401, 1982.

\bibitem{blanchard2017machine}
P.~Blanchard, E.~M. El~Mhamdi, R.~Guerraoui, and J.~Stainer, ``Machine learning
  with adversaries: Byzantine tolerant gradient descent,'' in {\em Advances in
  neural information processing systems}, vol.~30, 2017.

\bibitem{chen2017distributed}
Y.~Chen, L.~Su, and J.~Xu, ``Distributed statistical machine learning in
  adversarial settings: Byzantine gradient descent,'' {\em Proceedings of the
  ACM on Measurement and Analysis of Computing Systems}, vol.~1, no.~2,
  pp.~1--25, 2017.

\bibitem{yin2018byzantine}
D.~Yin, Y.~Chen, R.~Kannan, and P.~Bartlett, ``Byzantine-robust distributed
  learning: Towards optimal statistical rates,'' in {\em International
  Conference on Machine Learning}, pp.~5650--5659, PMLR, 2018.

\bibitem{cao2019distributed}
X.~Cao and L.~Lai, ``Distributed gradient descent algorithm robust to an
  arbitrary number of byzantine attackers,'' {\em IEEE Transactions on Signal
  Processing}, vol.~67, no.~22, pp.~5850--5864, 2019.

\bibitem{regatti2020bygars}
J.~Regatti, H.~Chen, and A.~Gupta, ``Bygars: Byzantine sgd with arbitrary
  number of attackers,'' {\em arXiv preprint arXiv:2006.13421}, 2020.

\bibitem{xie2019zeno}
C.~Xie, S.~Koyejo, and I.~Gupta, ``Zeno: Distributed stochastic gradient
  descent with suspicion-based fault-tolerance,'' in {\em International
  Conference on Machine Learning}, pp.~6893--6901, PMLR, 2019.

\bibitem{xie2020zeno++}
C.~Xie, S.~Koyejo, and I.~Gupta, ``Zeno++: Robust fully asynchronous sgd,'' in
  {\em International Conference on Machine Learning}, pp.~10495--10503, PMLR,
  2020.

\bibitem{zhu2023byzantine}
B.~Zhu, L.~Wang, Q.~Pang, S.~Wang, J.~Jiao, D.~Song, and M.~I. Jordan,
  ``Byzantine-robust federated learning with optimal statistical rates,'' in
  {\em International Conference on Artificial Intelligence and Statistics},
  pp.~3151--3178, PMLR, 2023.

\bibitem{diakonikolas2016robust}
I.~Diakonikolas, G.~Kamath, D.~M. Kane, J.~Li, A.~Moitra, and A.~Stewart,
  ``Robust estimators in high dimensions without the computational
  intractability,'' in {\em 57th Annual Symposium on Foundations of Computer
  Science}, pp.~655--664, Institute of Electrical and Electronics Engineers
  (IEEE), 2016.

\bibitem{diakonikolas2017being}
I.~Diakonikolas, G.~Kamath, D.~M. Kane, J.~Li, A.~Moitra, and A.~Stewart,
  ``Being robust (in high dimensions) can be practical,'' in {\em International
  Conference on Machine Learning}, pp.~999--1008, PMLR, 2017.

\bibitem{diakonikolas2020outlier}
I.~Diakonikolas, D.~M. Kane, and A.~Pensia, ``Outlier robust mean estimation
  with subgaussian rates via stability,'' in {\em Advances in Neural
  Information Processing Systems}, pp.~1830--1840, Dec 2020.

\bibitem{steinhardt2018robust}
J.~Steinhardt, {\em Robust learning: Information theory and algorithms}.
\newblock PhD thesis, 2018.

\bibitem{zhu2022a}
B.~Zhu, J.~Jiao, and J.~Steinhardt, ``Generalized resilience and robust
  statistics,'' {\em The Annals of Statistics}, vol.~50, no.~4, pp.~2256--2283,
  2022.

\bibitem{zhu2022b}
B.~Zhu, J.~Jiao, and J.~Steinhardt, ``Robust estimation via generalized
  quasi-gradients,'' {\em Information and Inference: A Journal of the IMA},
  vol.~11, no.~2, pp.~581--636, 2022.

\bibitem{zhu2022c}
B.~Zhu, J.~Jiao, and M.~I. Jordan, ``Robust estimation for non-parametric
  families via generative adversarial networks,'' in {\em 2022 IEEE
  International Symposium on Information Theory (ISIT)}, pp.~1100--1105, IEEE,
  2022.

\bibitem{diakonikolas2023algorithmic}
I.~Diakonikolas and D.~M. Kane, {\em Algorithmic high-dimensional robust
  statistics}.
\newblock Cambridge University Press, 2023.

\bibitem{charikar2017learning}
M.~Charikar, J.~Steinhardt, and G.~Valiant, ``Learning from untrusted data,''
  in {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
  Computing}, pp.~47--60, 2017.

\bibitem{diakonikolas2021list}
I.~Diakonikolas, D.~Kane, D.~Kongsgaard, J.~Li, and K.~Tian, ``List-decodable
  mean estimation in nearly-pca time,'' in {\em Advances in Neural Information
  Processing Systems}, vol.~34, pp.~10195--10208, 2021.

\bibitem{huber1964robust}
P.~J. Huber, ``Robust estimation of a location parameter,'' {\em The Annals of
  Mathematical Statistics}, pp.~73--101, 1964.

\bibitem{huber2004robust}
P.~J. Huber, {\em Robust statistics}, vol.~523.
\newblock John Wiley \& Sons, 2004.

\bibitem{tukey1975mathematics}
J.~W. Tukey, ``Mathematics and the picturing of data,'' in {\em Proceedings of
  the International Congress of Mathematicians, Vancouver, 1975}, vol.~2,
  pp.~523--531, 1975.

\bibitem{lai2016agnostic}
K.~A. Lai, A.~B. Rao, and S.~Vempala, ``Agnostic estimation of mean and
  covariance,'' in {\em 2016 IEEE 57th Annual Symposium on Foundations of
  Computer Science (FOCS)}, pp.~665--674, IEEE, 2016.

\bibitem{cheng2019high}
Y.~Cheng, I.~Diakonikolas, and R.~Ge, ``High-dimensional robust mean estimation
  in nearly-linear time,'' in {\em Proceedings of the thirtieth annual ACM-SIAM
  symposium on discrete algorithms}, pp.~2755--2771, SIAM, 2019.

\bibitem{balakrishnan2017computationally}
S.~Balakrishnan, S.~S. Du, J.~Li, and A.~Singh, ``Computationally efficient
  robust sparse estimation in high dimensions,'' in {\em Conference on Learning
  Theory}, pp.~169--212, PMLR, 2017.

\bibitem{hopkins2018mixture}
S.~B. Hopkins and J.~Li, ``Mixture models, robustness, and sum of squares
  proofs,'' in {\em Proceedings of the 50th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.~1021--1034, 2018.

\bibitem{diakonikolas2018list}
I.~Diakonikolas, D.~M. Kane, and A.~Stewart, ``List-decodable robust mean
  estimation and learning mixtures of spherical gaussians,'' in {\em
  Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing},
  pp.~1047--1060, 2018.

\bibitem{cherapanamjeri2020list}
Y.~Cherapanamjeri, S.~Mohanty, and M.~Yau, ``List decodable mean estimation in
  nearly linear time,'' in {\em 2020 IEEE 61st Annual Symposium on Foundations
  of Computer Science (FOCS)}, pp.~141--148, IEEE, 2020.

\bibitem{diakonikolas2022list}
I.~Diakonikolas, D.~Kane, S.~Karmalkar, A.~Pensia, and T.~Pittas,
  ``List-decodable sparse mean estimation via difference-of-pairs filtering,''
  {\em Advances in Neural Information Processing Systems}, vol.~35,
  pp.~13947--13960, 2022.

\bibitem{raghavendra2020list}
P.~Raghavendra and M.~Yau, ``List decodable learning via sum of squares,'' in
  {\em Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pp.~161--180, SIAM, 2020.

\bibitem{zeng2022list}
S.~Zeng and J.~Shen, ``List-decodable sparse mean estimation,'' {\em Advances
  in Neural Information Processing Systems}, vol.~35, pp.~24031--24045, 2022.

\bibitem{lecun1998mnist}
Y.~LeCun, ``The mnist database of handwritten digits,'' {\em http://yann.
  lecun. com/exdb/mnist/}, 1998.

\bibitem{tropp2015introduction}
J.~A. Tropp {\em et~al.}, ``An introduction to matrix concentration
  inequalities,'' {\em Foundations and Trends{\textregistered} in Machine
  Learning}, vol.~8, no.~1-2, pp.~1--230, 2015.

\end{thebibliography}
