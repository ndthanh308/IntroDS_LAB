\documentclass[a4paper,twocolumn,11pt,unpublished]{quantumarticle}
\pdfoutput=1
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{tikz}
\usepackage{lipsum}

\newtheorem{theorem}{Theorem}

%%%%%%%%%%%%%%%%%%%%These are the packages from the previous version

%\usepackage{cite}
\usepackage{amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\comma}{,}
\newcommand{\QLayer}[3]{\gategroup[#1,steps=#2,style={dashed,rounded corners,fill=blue!20, inner xsep=2pt},background,
label style={label position=below,anchor=north,yshift=-0.2cm}]{{#3}}}
\newcommand{\tightQLayer}[3]{\gategroup[#1,steps=#2,style={dashed,rounded corners,fill=blue!20,inner xsep=0.1pt, inner ysep=-0.5cm},background,label style={label
position=below,anchor=north,yshift=-0.2cm}]{{#3}}}
\usepackage{amsthm} %%gives me the proof environment
\usepackage{amssymb}
\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}
\usepackage{float}
\usepackage{braket}
\pgfplotsset{compat=1.18}
%\usepgfplotslibrary{external} 
%\tikzexternalize[prefix=tikz/]

\usepackage{tikz}
\usetikzlibrary{quantikz}
\usetikzlibrary{shapes.geometric, arrows}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{blank_bounds} = [rectangle,rounded corners=0cm, minimum width=0cm, minimum height=0cm,text centered, draw=black, fill=white, line width=.6]
\tikzstyle{blank_rectangular} = [rectangle, minimum width=0cm, minimum height=0cm,text centered, draw=black, fill=white]
\tikzstyle{meter} = [rectangle,rounded corners=0cm, inner sep=10,  fill=white, minimum width=30,draw=black, line width=.6,
 path picture={\draw[black] ([shift={(.1,.3)}]path picture bounding box.south west) to[bend left=50] ([shift={(-.1,.3)}]path picture bounding box.south east);
 \draw[black,-latex] ([shift={(0,.1)}]path picture bounding box.south) -- ([shift={(.3,-.1)}]path picture bounding box.north);}]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{arrow_} = [thick,-,>=stealth]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{definition}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{definition_and_theorem}{Definition and Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{innercustomthm}{Theorem}
\newtheorem*{remark}{Remark}
\newenvironment{customthm}[1]{\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
\usepackage{amsfonts}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{hyperref}
\usepackage{url}
\usepackage{dsfont}
\usepackage{graphicx}% Include figure files

\usepackage{adjustbox}
\raggedbottom
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%










\begin{document}

\title{Parametrized Quantum Circuits and their approximation capacities in the context of quantum machine learning}

\author{Alberto Manzano}
\affiliation{Department of Mathematics and CITIC, Universidade da Coruña, Campus de Elviña s/n, A Coruña, Spain}
\email{alberto.manzano.herrero@udc.es}

\author{David Dechant}
\affiliation{$\langle aQa^L\rangle$ Applied Quantum Algorithms Leiden, The Netherlands}
\affiliation{Instituut-Lorentz, Universiteit Leiden, P.O. Box 9506, 2300 RA Leiden, The Netherlands} 

\author{Jordi Tura}
\affiliation{$\langle aQa^L\rangle$ Applied Quantum Algorithms Leiden, The Netherlands}
\affiliation{Instituut-Lorentz, Universiteit Leiden, P.O. Box 9506, 2300 RA Leiden, The Netherlands} 


\author{Vedran Dunjko}
\affiliation{$\langle aQa^L\rangle$ Applied Quantum Algorithms Leiden, The Netherlands}
\affiliation{Instituut-Lorentz, Universiteit Leiden, P.O. Box 9506, 2300 RA Leiden, The Netherlands}
\affiliation{LIACS, Universiteit Leiden, P.O. Box 9512, 2300 RA Leiden, Netherlands}


\maketitle


\begin{abstract}
Parametrized quantum circuits (PQC) are quantum circuits which
consist of both fixed and parametrized gates. In recent approaches to quantum machine learning (QML), PQCs are essentially ubiquitous and play the role analogous to classical neural networks. 
They are used to learn various types of data, with an underlying expectation that if the PQC is made sufficiently deep, and the data plentiful, the generalisation error will vanish, and the model will capture the essential features of the distribution. 
While there exist results proving the
approximability of square-integrable functions by PQCs under the $L^2$ distance, the approximation for other function spaces and under
other distances has been less explored. 
In this work we show that PQCs
can approximate the space of continuous functions, $p$-integrable functions and the $H^k$ Sobolev spaces under specific distances. Moreover, we develop generalisation bounds that connect different function spaces and distances. 
These results provide a rigorous basis for the theory of explored classes of uses of PQCs. Such as for solving new potential uses of PQCs such as solving differential equations. Further, they provide us with new insight on how to design PQCs and loss functions which better suit the specific needs of the users.
\end{abstract}

\section{Introduction}
Machine learning has gained significant attention in recent years for its practical applications and transformative impact in various fields. As a consequence, there has been a rising interest in exploring the use of quantum circuits as machine learning models, capitalizing on the advancements in both fields to unlock new possibilities and potential breakthroughs. Among the various possibilities for leveraging quantum circuits in machine learning, our particular focus lies on parametric quantum circuits (PQC). These quantum circuits consist of both fixed and adjustable (hence 'parametrized') gates. When used for a learning task, a classical optimiser updates the parameters of the PQC in order to minimise a cost function depending on measurement results from this quantum circuit (see Figure \ref{fig: sketch of cicuit and variational algorithm}).\\ \\ 
% Figure environment removed
In this context, a growing line of research studies the expressivity of PQCs. 
More precisely, the capacity of PQCs to approximate any function belonging to a particular \textit{function space} defined in a prescribed \textit{domain} up to arbitrary precision with respect to a specific \textit{distance}. 
In \cite{schuld2021effect}, they showed that PQCs can be written as a generalized trigonometric series in the following way: 
\begin{align}\label{eq:circuit_fourier}
    f_{\bm{\theta}}(\bm{x}) &= \bra{0}U^{\dagger}(\bm{x};\bm{\theta})MU(\bm{x};\bm{\theta}) \ket{0} \\
    &=\sum_{\bm{\omega} \in \Omega} c_{\bm{\omega}}(\theta)e^{i\bm{\omega x}},
\end{align} 
The authors further showed in \cite{schuld2021effect} that, if the PQC is chosen carefully, the increase of its depth and number of parameter can arbitrarily reduce the $L^2$ distance between the expected value $f_{\bm{\theta}}(\bm{x})$ of the PQC and any square-integrable function with the domain $[0,2\pi]^N$. Throughout the paper we will refer to the PQC as the one approximating the functions to make the text more fluent, although technically it is the expectation value of the PQC that approximates that approximates the function.

This result had a significant impact on the motivation to study PQC-based QML, analogous to the impact the famous Universality theorem for neural networks of Cybenko \cite{cybenko1989approximation} had on the domain of classical machine learning. However, as it turns out, there are numerous different notions of universality, and not all are useful for all applications. 
For instance, as will be discussed later, in the context of Physics-Inspired Neural Networks (PINN) the "vanilla" universality does not sufffice. This raises the question of whether PQCs can approximate functions belonging to other function spaces or in terms of other distances. \\ \\

In this paper we present two novel results. 
The first result of this paper is that PQCs can arbitrarily approximate the space of continuous functions, the space of $p$-integrable functions and the $H^k$ space, which is the set of functions whose derivatives up to order $k$ are square integrable. Furthermore, we explain how this properties can be easily achieved in practice by a simple min-max feature rescaling (see \eqref{eq:min_max_feature_scaling}) of the input data.\\ \\
The second result of the paper are generalisation bounds that connect distances with loss functions which are not built via the discretization of the integrals present in the definition of the distance. 
To make it more clear, we recall that in a machine learning problem one needs to choose an architecture, which defines the class of functions that can be approximated, and a target distance, which is intimately connected with the generalisation error\footnote{In practice we may not explicitly think about the target distance, i.e. with respect to which distance we wish to approximate the "true" labeling function.
But this decision is implicitly made, once the loss is chosen.}. 
However, in general it is not possible to compute the target distance, as we would need to have available infinitely many data points.
Instead, one chooses a different distance function which can be computed from the available data: a loss function. 
This loss function is a different function than the target distance but it should be chosen in such a way that we call \textit{consistent} with the target distance, i.e., that the minimization of the loss function yields the minimization of the target distance up to an error which asymptotically tends to zero \textit{when the number of samples and the expressivity} (here meant to mean architecturally, as e.g. depth) of the PQC increases.
For example, the mean square error (as a loss function) is consistent with the $L^2$ error (as a target distance) but is inconsistent with the supremum distance. 
The usual generalisation bounds connect target distances which are continuous with loss functions which are their discrete version. \\
The generalisation bounds that we derive give a mapping \textit{across} different distances and loss functions, i.e., they relate distances with loss functions which are not built via the discretization of the integrals present in the definition of the distance. 
A particular loss function we shall define, denoted $h^1$, which consists of the sum of the mean square error of the function and its derivatives, is consistent with the supremum distance in one-dimensional problems. 
In the described case, this allows us to reduce the supremum distance while choosing a loss function which is differentiable. \\ \\
Our results apply in many settings.
For example, our first result has a direct consequence in that it allows one to capture not only the coarse grain behaviour of the underlying data but also the details. 
For instance, the minimization of the ubiquitous $L^2$ distance may allow functions to dramatically differ from the target function in some regions where we have plenty of data points available, whereas the minimization of the supremum norm in Theorem \ref{thm:C_0_approximation} will force the PQC to converge for any given point in the domain of the target function.
Our second result had direct applications e.g. in settings where we have access to data of the function and its derivatives. One case where this is standard are settings involving the solving of differential equations. For example in physics-informed neural networks (PINN) problems \cite{Raissi2017PINN} and differential machine learning (DML) \cite{huge2020differential} both function values and derivatives are accessible and in fact critical.
\\

This paper is organized as follows: in Section \ref{sec:data_normalization} we explain the new results on the expressivity of PQCs. 
In Section \ref{sec:differential_machine_learning} we discuss the proposed generalisation bounds. 
Then, in Section \ref{sec:numerical_experiments} we illustrate the theoretical result of Sections \ref{sec:data_normalization} and \ref{sec:differential_machine_learning} by means of some experiments. 
Lastly, in Section \ref{sec:conclusions} we wrap up with the conclusions. \\ \\

During the final stages of our work, we became aware of the paper \cite{gonon2023universal} which overlaps in some parts with our own results in Section \ref{sec:data_normalization}. However, the results presented here were developed independently and follow a different line of reasoning.













\section{PQCs and universal approximation}
\label{sec:data_normalization}
In this section, we will review the established result on universality in \cite{schuld2021effect} and then present our new universality results in Theorems \ref{thm:C_0_approximation}, \ref{thm:L_p_approximation} and \ref{thm:H^k_approximation}.\\ \\
Schuld et al. showed in \cite{schuld2021effect}, how a quantum machine learning model of the form $f_{\theta}(\bm{x})=\bra{0}U^{\dagger}(\bm{x};\bm{\theta})MU(\bm{x};\bm{\theta}) \ket{0}$ can be written as a generalized trigonometric series:
\begin{align}%\label{eq:circuit_fourier}
    \bra{0}U^{\dagger}(\bm{x};\bm{\theta})MU(\bm{x};\bm{\theta}) \ket{0} &= f_{m}(\bm{x};\bm{\theta}) \\
    &=\sum_{\bm{\omega} \in \Omega} c_{\bm{\omega}}(\theta)e^{i\bm{\omega x}},
\end{align}
where $M$ is an observable, $U(\bm{x};\bm{\theta})$ is a quantum circuit modelled as a unitary that depends on inputs $\bm{x} = \left(x_0,x_1,...,x_N\right)$ and the variational parameters $\bm{\theta}=\left(\theta_0,\theta_1,...,\theta_T \right)$. In the above, $\bm{\omega}\in\Omega$ denotes the set of available frequencies which always contain $0$. The quantum circuit consists of $L$ layers each consisting of a trainable circuit block $W_i(\bm{\theta}), i\in\{1,...,L+1\}$ and a data encoding block $S(\bm{x})$ as shown in Figure \ref{fig:Schuld_circuit}. The data encoding blocks determine which frequencies $\bm{\omega}$ are accessible in the sum and are implemented as Pauli rotations. The blocks $W(\bm{\theta})$ can be built from single-qubit rotation gates and CNOT gates and they determine the coefficients $c_{\bm{\omega}}(\theta)$ of the sum.\\ \\

% Figure environment removed

For the needs of our discussion, we will briefly describe a more specific set-up under which the authors of \cite{schuld2021effect} proved a universality theorem of these quantum models. 

Let us construct a model of the form in \eqref{eq:circuit_fourier},
with the measurement $M$ and a quantum circuit of one layer, $L=1$:
\begin{align}
    f_{\bm{\theta}}&=\bra{0}U^{\dagger}(\bm{\theta},\bm{x})MU(\bm{\theta},\bm{x})\ket{0}\ ,\text{ with }\\
    U(\bm{\theta},\bm{x})&=W^{(2)}(\bm{\theta}^{(2)})S(\bm{x})W^{(1)}(\bm{\theta}^{(1)})\ ,
\end{align}
where $\bm{\theta}^{(1)}$ and $\bm{\theta}^{(2)}$ are those parameters in $\bm{\theta}$ that affect $W^{(1)}$ and $W^{(2)}$, respectively. Let us further make the following two assumptions:
Firstly, we assume that the data-encoding blocks $S(\bm{x})$ are written in the following way:
\begin{align}
    S(\bm{x})&=e^{-x_1H}\otimes \cdots\otimes e^{-x_NH}\\
    &=:S_H(\bm{x})\ ,
\end{align}
where $H$ is a Hamiltonian that we specify later.
Secondly, we assume that the trainable circuit blocks $W^{(1)}(\bm{\theta}^{(1)})$ and $W^{(2)}(\bm{\theta}^{(2)})$ are able to represent arbitrary global unitaries. In practice, this may require exponential circuit depth. With this assumption, we drop the dependence on $\bm{\theta}$ and reformulate the assumption as being able to prepare an arbitrary initial state $\ket{\Gamma}:=W^{(1)}(\bm{\theta}^{(1)})\ket{0}$ and by absorbing $W^{(2)}(\bm{\theta}^{(2)})$ into the measurement $M$. We can then write the above quantum model as:
\begin{align}
   f(\bm{x})=\bra{\Gamma}S_H^{\dagger}(\bm{x})MS_H(\bm{x})\ket{\Gamma} \ .
\end{align}
Let us further present the notion of a universal Hamiltonian family, as defined in \cite{schuld2021effect}:
\begin{definition}
\label{definition: universal hamiltonian family} Let $\{H_m|m\in\mathbb{N}\}$ be a Hamiltonian family where $H_m$ acts on $m$ subsystems of dimension $d$. \\
Such a Hamiltonian family gives rise to a family of models $\{f_m\}$ in the following way:
\begin{align}\label{eq:family_of_models_dep_on_Hamiltonian_family}
   f_m(\bm{x)}=\bra{\Gamma}S_{H_m}^{\dagger}(\bm{x})MS_{H_m}(\bm{x})\ket{\Gamma} \ .
\end{align}
Further, we call the set
\begin{align}
    \Omega_{H_m}:=\{\mathbf{\lambda}_j-\mathbf{\lambda}_k|j,k\in\{1,...,d^m\}\}\ 
\end{align}
where $\{\lambda_1,...,\lambda_{d^m}\}$ are the eigenvalues of $H_m$, the frequency spectrum of $H_m$.\\
\end{definition}
\begin{remark}
We call a Hamiltonian family $\{H_m\}$
a universal Hamiltonian family, if for all $K\in\mathbb{N}$, there exists an $m\in\mathbb{N}$, such that:
\begin{align}
\mathbb{Z}_K=\{-K,...,0,...,K\}\subseteq\Omega_{H_m}\ ,
\end{align}
hence if the frequency spectrum of $\{H_m\}$ asymptotically contains any integer frequency.
\end{remark}
As shown in \cite{schuld2021effect}, a simple example of a universal Hamiltonian family is one which consists of tensor products of single-qubit Pauli gates:
\begin{align}
    H_m=\sum_{i=1}^m\sigma_{q}^{(i)}\ ,
\end{align}
with $\sigma_q^{(i)}$,$q\in\{X,Y,Z\}$ and $d=2$. The scaling of the frequency spectrum for this example goes as $K=m$.\\
With these definitions, we can give the following theorem:
\begin{thm}\label{thm:Schuld_universality} \cite{schuld2021effect}
    Let $\{H_m\}$ be a universal Hamiltonian family, and $\{f_m\}$ the associated quantum model family, defined
via \eqref{eq:family_of_models_dep_on_Hamiltonian_family}. For all functions $f^*\in L^2\left([0, 2\pi]^N\right)$, and for all $\epsilon >0$, there exists some $m'\in \mathbb{N}$, some state $\ket{\Gamma} \in \mathbb{C}^{m'}$
and some observable $M$ such that
\begin{equation}\label{eq:schuld_l2_approximation}
   \norm{f_{m'}-f^*}_{L^2}<\epsilon.
\end{equation}
\end{thm}
Here, we clearly see that there are two conditions on the target function $f^*$ that must be fulfilled in order for the theorem to work properly. The first condition is that $f^*$ belongs to $L^2$. This is not surprising, we need to assume certain regularity on the target function to make the theorem work. The second condition is that the target function $f^*$ needs to be restricted to the domain $[0,2\pi]^N$. However, as suggested in the original paper \cite{schuld2021effect}, if the function $f^*$ does not belong to this domain, we can easily map $[a,b]^N$ to the required domain $[0,2\pi]^N$ (or $[-\pi,\pi]^N$ equivalently).\\ \\
We would like to highlight the fact that the distance we use to bring the approximator closer to the target function is the $L^2$ distance. Note that convergence in the $L^2$ sense does not imply other modes of convergence. For example, this does not give us information about the more general case of $p$-integrable spaces with $1\leq p <\infty$. We explicitly address this more general case in the following theorem:
\begin{thm}\label{thm:L_p_approximation}
    Let $\{H_m\}$ be a universal Hamiltonian family, and $\{f_m\}$ the associated quantum model family, defined
via \eqref{eq:circuit_fourier}. For all functions $f^*\in L^p\left(U\right)$ where $1\leq p<\infty$, $U$ is compactly contained in the closed cube $[0,2\pi]^N$, and for all $\epsilon >0$, there exists some $m'\in \mathbb{N}$, some state $\ket{\Gamma} \in \mathbb{C}^{m'}$,
and some observable $M$ such that $f_{m'}$ converges uniformly to $f^*$:
\begin{equation}\label{eq:L_p_approximation}
   \norm{f_{m'}-f^*}_{L^p}<\epsilon.
\end{equation}
\end{thm}
A set $U\subset\mathbb{R}^N$ is compactly contained in another set $V\subset\mathbb{R}^N$, if the closure of $U$ is compact and contained in the interior of $V$.

Let us emphasize what the differences between Theorems \ref{thm:Schuld_universality} and \ref{thm:L_p_approximation} are. The first difference is that the function $f^*$ has to be defined in a domain $U$ which is compactly contained in $\left[0,2\pi\right]^N$. 
A simple example of $U$ is the interval $\left(\left[-\dfrac{\pi}{2}, \dfrac{\pi}{2}\right]^N\right)$ (or $\left([0, \pi]^N\right)$ equivalently). By restricting ourselves to half of the original space we can always find an $L^p$ extension of the function $f^*$ in $\mathbb{T}^N$. 
The second difference is that the target function can belong to any $L^p$ space with $1\leq p<\infty$ in contrast with the previous requirement of being square-integrable ($L^2$). 
This is essentially achieved by the fact that PQCs are not only able to represent Fourier series as it is discussed in \cite{schuld2021effect} but they are also able to represent more general trigonometric series. 
This allow us to identify the expectation value of the quantum circuit with the Cèsaro summation of the partial Fourier series of $f^*$ and leverage the power of Fèjer-like theorems \cite{fejer1903untersuchungen}.  

Nevertheless, the ability to approximate functions in $L^p$ does not prevent us from having arbitrarily big errors in certain points. Intimately related to this problem is the so-called Gibbs phenomenon \cite{gibbs}. Namely, the approximation of a continuous, but non-periodic function by a Fourier series is increasingly better in the interior of the domain but increasingly poorer on its boundaries. 
That leads to the fundamental question if we can approximate $f^*$ in a stronger sense, so that we ensure that the target function $f^*$ is well approximated in any given point. We answer this question in the next theorem.

\begin{thm}\label{thm:C_0_approximation}
    Let $\{H_m\}$ be a universal Hamiltonian family, and $\{f_m\}$ the associated quantum model family, defined
via \eqref{eq:circuit_fourier}. For all functions $f^*\in C^0\left(U\right)$ where $U$ is compactly contained in the closed cube $[0,2\pi]^N$, and for all $\epsilon >0$, there exists some $m'\in \mathbb{N}$, some state $\ket{\Gamma} \in \mathbb{C}^{m'}$,
and some observable $M$ such that $f_{m'}$ converges uniformly to $f^*$:
\begin{equation}\label{eq:C_0_approximation}
   \norm{f_{m'}-f^*}_{C^0}<\epsilon,
\end{equation}
with 
\begin{align}
    \norm{f_{m'}-f^*}_{C^0}:=\sup_{\mathbf{x}\in [0,2\pi]^N}\|f_{m'}(\mathbf{x})-f^*(\mathbf{x})\| \footnote{Since $f^*$ is defined on a compact domain $U$, the supremum is equivalent to the maximum in this case.}\ .
\end{align}
\end{thm}
The proof of Theorem \ref{thm:C_0_approximation} can be found in Appendix \ref{appendix:fourier_series_uniform_approximation}.\\

Simply stated, this theorem means that $f_{m'}$ converges uniformly to $f^*$.
In other words, if we select a given target error $\epsilon$ we are always able to find a finite PQC such that the error on any point is smaller than the prescribed $\epsilon$. 
Let us emphasize again the differences between Theorems \ref{thm:Schuld_universality} and \ref{thm:C_0_approximation}. 
The first difference, which is related to the domain of the target function, was already discussed after the previous theorem. The second difference is that the target function now belongs to the class of continuous functions in contrast to the previous requirement of being square-integrable ($L^2$).
\\ \\
A last result that we will show in this regard is about the approximation of the function and its derivativesthe parametric quantum circuit. This might seem as a purely synthetic question but it has many implications in practice. When we approximate a target function, in many occasions we not only want to recover its value but also its dynamics. This is particularly relevant in physical problems, where we typically have a differential equation which describes the behaviour of the system. As we will see in the following theorem, the universality results translate to functions defined in the Sobolev space $H^k$ as well:
\begin{definition} The Sobolev space $H^k(\Omega)$ is defined as the space of square integrable functions on a domain $\Omega\subseteq\mathbb{R}^N$ which derivatives up to order $k$ have a finite $L^2$ norm:
\begin{align} 
f^{\alpha}=D^{\alpha}f\ , \text{ and }
\norm{f^{(\alpha)}}_2<\infty\ ,
\end{align}
for $\abs{\alpha}\leq k$ and $D^{\alpha}:=\frac{\partial^{\abs{\alpha}}}{\partial x_1^{\alpha_1}...\partial x_N^{\alpha_N}}$.\\
The Sobolev norm $\norm{\cdot}_{H^k}$ is defined as
\begin{align}
    \norm{\cdot}_{H^k}:=\left(\sum_{\abs{\alpha}\leq k}\int_{\Omega}\abs{D^{\alpha}f}^2\right)^{1/2}\ .
\end{align}
\end{definition}
\begin{thm}\label{thm:H^k_approximation}
    Let $\{H_m\}$ be a universal Hamiltonian family, and $\{f_m\}$ the associated quantum model family, defined
via \eqref{eq:circuit_fourier}. For all functions $f^*\in H^{k+1}\left(U\right)$ where $U$ is compactly contained in the closed cube $[0,2\pi]^N$ and has a Lipschitz boundary, and for all $\epsilon >0$, there exists some $m'\in \mathbb{N}$, some state $\ket{\Gamma} \in \mathbb{C}^{m'}$,
and some observable $M$ such that $f_{m'}$ converges to $f^*$ with respect to the $H^k-$distance:
\begin{equation}\label{eq:H^k_approximation}
   \norm{f_{m'}-f^*}_{H^k}<\epsilon.
\end{equation}
\end{thm}
As in Theorems \ref{thm:L_p_approximation}, \ref{thm:C_0_approximation} and \ref{thm:H^k_approximation}, we required that the target function is defined on a subset of $[0,2\pi]^N$ we propose to perform a min-max feature scaling of the input data:
\begin{equation}
\begin{aligned}\label{eq:min_max_feature_scaling}
    \bm{x} = (x_1,...,x_n)
    \longrightarrow \ 
     \bm{\tilde{x}}= (\tilde{x}_1,...,\tilde{x}_n)\ ,
\end{aligned}
\end{equation}
where $\bm{x}\in [a,b]^N$ and $\bm{\tilde{x}}\in \left[-\dfrac{\pi}{2},\dfrac{\pi}{2}\right]^N$, and 
\begin{align}
    \bm{\tilde{x}}= \left(-\dfrac{\pi}{2}+\pi\dfrac{x_1-a}{b-a},...,-\dfrac{\pi}{2}+\pi\dfrac{x_n-a}{b-a}\right)\ .
\end{align}
This simple recipe allows the PQC to approximate a much wider set of function spaces as shown throughout this section. This normalization strategy works very well in practice as can be seen in Section \ref{sec:numerical_experiments}. However, we would like to emphasize that this particular normalization is not the only choice. The classical strategy in machine learning of normalizing the input data to lay in the $[-1,1]^N$ domain is also completely valid. 

Throughout this section, we have discussed the expressive power of the quantum circuits, but when we do machine learning we have more ingredients that we need to take into account. In the next section we will discuss the role that the loss function plays in accordance with the type of approximation that our PQC can get.


\section{Connections between different generalization bounds}\label{sec:differential_machine_learning}
As we have seen in the previous section, the notion of approximation depends on a prescribed distance. This distance is not given by the problem itself, but instead chosen by the user, this is why we refer to it as target distance. In general, it is however not possible to compute the target distance, which for example is the case for the $L^p$ and $H^k$ distances. 
This is why one needs to choose a distance function which can be computed from data, a loss function. It has to be chosen in such a way that it is consistent with the target function. To discuss the topic in more depth, let us formally introduce the continuous regression problem, which is the problem that we are most interested in.\\ \\
In general, we can describe the continuous regression problem in the following way. Assume that there is some target function $f^*\in \mathcal{F}\subseteq H^k$ mapping inputs $x\in \mathcal{X}$ to target labels $y\in \mathcal{Y}$. Moreover, assume that the points in $\mathcal{X}$ are sampled according to a bounded \footnote{It is possible to have more general density functions. However, we restrict ourselves with this one since it simplifies the analysis.} density function $p$. Our goal is to find the best approximation $f\in \mathcal{M}\subseteq H^k$ of the target function $f^*$. 
\\ The notion of what is understood as a ``good'' approximation as clarified, allows for some freedom. For this reason, one has to make a choice by specifying a functional $D : H^k\; \times \;  H^k \longrightarrow \mathbb{R}^+\cup \{0\}$ which defines a distance between the elements of $\mathcal{F}$ and $\mathcal{M}$. The problem can then be stated as:
\begin{equation}\label{eq:minimization_problem}
    f = \argmin_{\hat{f}\in \mathcal{M}}D(f^*,\hat{f}).
\end{equation}
%This functional is typically chosen in such a way that it defines a distance. 
The most common distance in the literature for continuous regression problems is the one induced by the $L^2(\mathcal{X},P)$ norm:
\begin{equation}\label{eq:l2_problem}
\begin{aligned}
    D_{L^2}\left(f^*,f\right) &= ||f^*-f||_{L^2}\\ &= \left(\int_{\mathcal{X}}(f^*(\bm{x})-f(\bm{x}))^2dP\right)^{\frac{1}{2}}.
\end{aligned}
\end{equation}
However, in regression we do not typically have access to the full information (i.e., we cannot compute the integral). It is for this reason that instead we work with the empirical risk minimisation problem, which is the discrete version of the previous setting. The difference with the previous setup is that, for the empirical risk minimisation problem, we are given a finite set of $M$ inputs sampled from the same probability density $p$, together with their target labels $\{(x_1, y_1),...,(x_M, y_M)\}$ with $(x, y)\in \mathcal{X}\times \mathcal{Y}$, according to the target function $f^*:\mathcal{X}\to \mathcal{Y}$, $f\in\mathcal{F}$. Now, instead of minimizing a continuous functional, we will minimise a discrete one. Similarly to the continuous case, we are concerned with the $\ell^2$ distance, which is defined as:
\begin{align}\label{eq:empirical_risk_standard_loss}
    D_{\ell^2}(f^*,f) := \left(\frac{1}{I}\sum_{i = 0}^{I-1}  \left(f^*(\bm{x}^i)-f(\bm{x}^i)\right)^2\right)^{\frac{1}{2}},
\end{align}
with $\bm{x}^i$ denoting the i-th input.\\ \\
Although we are solving the minimization problem associated with the loss function defined in \eqref{eq:empirical_risk_standard_loss}, in general we are interested in the generalization performance, i.e., the distance in terms of \eqref{eq:l2_problem}. Using generalization bounds \cite{mohri2018foundations} we can relate the performance in terms of the distance given by \eqref{eq:empirical_risk_standard_loss} with the distance given by \eqref{eq:l2_problem}. However, this classical results in machine learning do not relate the $\ell^2$ distance with the $C^0$ distance in general. In other words, even a solution which, as the model and the number of points grows larger asymptotically makes the $D_{L^2}$ go to zero, does not make the $D_{C^0}$ distance vanish, which is defined as:
\begin{align}\label{eq:empirical_risk_uniform_loss}
    D_{C^0}(f^*,f) := \sup_{\mathbf{x}\in \mathcal{X}}\abs{f^*(\mathbf{x})-f(\mathbf{x})}.
\end{align}
One possible solution would be to use a different distance than $D_{\ell^2}$. For example one could try with:
\begin{equation}\label{eq:max_distance}
    D_{l^\infty} = \max_{i\in \{0,...,I-1\}}\abs{f^*(\mathbf{x}^i)-f(\mathbf{x}^i)},
\end{equation}
but this distance is not differentiable, making the optimization process much harder.\\ \\
Thus, we identify two requirements on the loss function. The first requirement is that the solution of the minimization problem that it defines, tends uniformly to the target function $f^*$ as we increase the number of given points $D$ and we increase the size of our PQC.
 
The second one is that it has to be differentiable in order to make minimization easier. The solution that we propose here is to use a distance motivated by discretizing the Sobolev norm, as shown in \eqref{eq:h1_risk_problem}:

\begin{widetext}
\begin{align}
\label{eq:h1_risk_problem}
 D_{h^1}(f^*,f)&:=\left[\frac{1}{I}\sum_{i = 0}^{I-1}\left(f^*(\bm{x}^i)-f(\bm{x}^i)\right)^2+ \sum_{j = 0}^{N-1}\sum_{i = 0}^{I-1}\frac{1}{I}\left(\dfrac{\partial f^*}{\partial x_j}(\bm{x}^i)-\dfrac{\partial f}{\partial x_j}(\bm{x}^i)\right)^2\right]^{\frac{1}{2}},\\
   \label{eq:hk_risk_problem}
    D_{h^k}(f^*,f)&:=\left[\frac{1}{I}\sum_{i = 0}^{I-1}\left(f^*(\bm{x}^i)-f(\bm{x}^i)\right)^2+ \sum_{\abs{\alpha}\leq k}\sum_{i = 0}^{I-1}\frac{1}{I}\left(D^{\alpha}f^*(\bm{x}^i)-D^{\alpha}f(\bm{x}^i)\right)^2\right]^{\frac{1}{2}}.
\end{align}
\end{widetext}



We write $I$ for the number of input dimensions. The loss function given in \eqref{eq:h1_risk_problem} was first introduced in \cite{huge2020differential} and gives rise to a new subfield of machine learning known in the literature as DML. The case where $f^*$ lies in $H^k$ and higher-dimensional derivatives are available, is shown in \eqref{eq:hk_risk_problem},
where $\frac{\partial^{(p)} f^*}{\partial x_j^{(p)}}$ and $\frac{\partial^{(p)} f}{\partial x_j^{(p)}}$ are the $p$-th order derivative functions in direction $x_j$ of $f^*$ and $f$, respectively.
With classical neural networks, DML has proven to yield better generalization results in terms of the $D_{\ell^2}$ norm than the solution of the $D_{\ell^2}$ itself. This means that, if we take the solutions $f_{h^1}$ and $f_{\ell^2}$ of the minimization problems defined by Equations \eqref{eq:h1_risk_problem} with the same number of labels and \eqref{eq:empirical_risk_standard_loss} respectively and evaluate their performance in terms of the $D_{L^2}$, in practice $f_{h^1}$ performs better than $f_{\ell^2}$:
\begin{equation*}
    D_{L^2}\left(f^*,f_{h^1}\right)\leq D_{L^2}\left(f^*,f_{\ell^2}\right).
\end{equation*}
However, to the best of our knowledge there is no theoretical explanation in the literature on why this happens or under which condition we might expect this behaviour. In the next theorem we shed some light on it:

\begin{thm}\label{thm:sobolev_loss}
Given a target function $f^*\in \mathcal{F}\subseteq H^k(\Omega)$, $\Omega\subset\mathbb{R}^N$ and a model family $\mathcal{M}$ which is a universal approximator of the space $H^k$, we denote $f_{h^k}\in \mathcal{M}$ as the solution of the minimization problem defined by the distance $d_{h^k}$ (with derivatives up to order $k$). Let us further assume that $D_{H^k}\left(f^*(\mathbf{x}),f_{h^k}(\mathbf{x})\right)\leq B$ for all $\mathbf{x}\in\mathcal{X}, f^* \in\mathcal{F}$ and $f_{h^k}\in \mathcal{M}$, and that the Rademacher complexity $\Re_D(\mathcal{M})$ of the function family $\mathcal{M}$ is finite. Then, the following holds:
\begin{align}
    \dfrac{1}{C}D_{L^q}\left(f^*,f_{h^k}\right) &\leq D_{H^k}\left(f^*,f_{h^k}\right) \\
    &\leq D_{h^k}\left(f^*,f_{h^k}\right)+r(|\mathcal{M}|,I,\delta),
\end{align}
where $C$ is a constant, $r(|\mathcal{M}|,I,\delta)\to 0$ as $I\to\infty$, for the specific choices for $k$ and $q$:
\begin{enumerate}
    \item For the case when $k$ is in the interval:
\begin{equation}\label{eq:Kondrachov_constraint}
    N\left(\frac{1}{2}-\frac{1}{q}\right)<k<N/2,
\end{equation}
    the Theorem holds for $1\leq q<N$.
    \item For the case that $k\geq N/2$,  the Theorem holds for $1\leq q<\infty$.
\end{enumerate}
Furthermore, for $k>N/2$, we have that
\begin{align}
    \dfrac{1}{C}D_{C^0}\left(f^*,f_{h^k}\right) \leq D_{H^k}\left(f^*,f_{h^k}\right)\ ,
\end{align}
where $C$ is a constant.
\end{thm}
The proof of this theorem can be found in Appendix \ref{appendix:appendix2}.\\
A consequence of Theorem \ref{thm:sobolev_loss} is that, if the order of the derivatives that we have at our disposal are higher than half the number of input dimensions ($k>n/2$), our solution of the $D_{h^k}$ problem is also a solution of the $D_{C^0}$ problem. This is important, as the latter corresponds to uniform convergence.

Note that we face a curse of dimensionality-like phenomenon as the dimension of the input grows. In this case, the number of terms that go into the $h^k$ loss function grows exponentially with $k$, as we have to take into account mixed derivatives. Hence, for high dimensional problems it is not clear whether the derivatives are going to be very useful in a practical setting.\\
The last property we wish to highlight is the fact that we need our universal approximator to be able to approximate functions in the $L^q$ space. For example, if we try to fit a function which is not periodic, using Fourier series and the $H^1$ norm, then the results that we will obtain will be worse than those obtained with the $L^2$ problem (see figure \ref{fig:f_DML}). The bottom line is that more information does not always equate to a better approximation, if we are not very careful how to use it.\\

\section{Numerical experiments}\label{sec:numerical_experiments}

In this section we illustrate the theoretical discussion of Sections \ref{sec:data_normalization} and \ref{sec:differential_machine_learning} with an illustrative example: the approximation of function $f^* = \frac{x}{2\pi}, \, x\in [-\pi,\pi]$ by the PQC in Figure \ref{fig:PQC_arquitecture}:
% Figure environment removed
We conduct two different numerical experiments and show them in Figures \ref{fig:f_pqc} and \ref{fig:f_DML}.
All simulations have been performed using $10$ points uniformly distributed along the domain for the training phase. Each experiment has been repeated $100$ times and we depict the $25$, $50$ and $75$ percentiles in colored solid lines in Figure \ref{fig:f_pqc}. The legends call the result of the PQCs as $f_{\bullet}(\cdot)$, where the subscript denotes under which loss function we have done the training and in the parentheses we indicate which normalisation we have chosen.\\
In Figure \ref{fig:f_pqc} we compare the performance of our PQC under different normalizations. We normalise the data to lay in the domains $\left[-\frac{\pi}{2},\frac{\pi}{2}\right]$, $[-\pi,\pi]$ and $[-2\pi,2\pi]$, respectively. 
When we normalised our data to lay in the range $[-\frac{\pi}{2},\frac{\pi}{2}]$ we get the best results,  as we expected due to Theorem \ref{thm:C_0_approximation}.

In contrast, when the data is normalised to lie in the range $[-2\pi,2\pi]$ we obtain very poor approximation results. The intermediate regime happens when we normalise the data to lay in the range $[-\pi,\pi]$, here we obtain a reasonable approximation except for the boundaries. This is a consequence of being a $L^2$-approximator instead of a $C^0$-approximator: we cannot guarantee that the error will be reduced on any given point. This behaviour remains even when we increase the size of the circuit and the number of given points.
% Figure environment removed


In Figure \ref{fig:f_DML} we study the impact of the different loss functions with different normalisations in the learning problem. We simulated the regression using the two different loss functions, $h^1$ and $\ell^2$ under two different normalisations, with the domains $\left[-\frac{\pi}{2},\frac{\pi}{2}\right]$ and $\left[-\pi,\pi\right]$. The first noticeable phenomenon that we can see is that using the $h^1$ norm instead of the $\ell^2$ norm when the data is normalised to lay in the interval $\left[-\frac{\pi}{2},\frac{\pi}{2}\right]$ not only reduces the variance, but also has some impact on the bias. What might be more surprising is the effect of the $h^1$ norm when the data is normalised to lay in the interval $\left[-\pi,\pi\right]$. Instead of getting a better approximation w.r.t. the $\ell^2$ we worsen it. We explain it with the fact that, when we normalise the data to lay in the interval $\left[-\pi,\pi\right]$, our PQC is not an approximator of $H^1$ but it is an approximator of $L^2$, i.e., it can approximate the function but it cannot simultaneously approximate the function and the derivatives. Thus, in the minimization process the PQC tries to find a balance between the error in the function and the error in the derivatives, worsening the results with respect to the quality of the function approximation.



% Figure environment removed






\section{Conclusions}\label{sec:conclusions}
In this work we have developed a broader theory of approximation capacities of PQCs. We have shown how an appropriate choice of the data normalization greatly improves the expressivity of the PQCs. More specifically, we showed that a min-max feature scaling that normalises the input data along each dimension to lay in the range $[-\frac{\pi}{2},\frac{\pi}{2}]$ makes PQCs universal approximators in continuous function space, the $L^p$ space with $1\leq p< \infty$ and the $H^k$ space.

Moreover, since with this normalisation we are able to approximate functions in the sense of the $C^0$ distance, we discussed that a loss function which is coherent with such distance might be more appropriate than other elections. In particular, the natural choice would be the $l^\infty$ distance. However, since the $l^\infty$ distance is not differentiable and that makes the optimization of PQCs harder, we leveraged Sobolev inequalities to show that the $h^1$ distance is consistent with the $C^0$ distance with the additional property of being differentiable. Lastly, we performed some numerical experiments to illustrate how this simple choice of normalisation and loss function can vastly improve the results in practice.\\ \\
The data normalization technique can be seen as a complementary result to the work of \cite{schuld2021effect}. Nevertheless, there is still much work to do in this direction. For example, if instead of only taking a min-max feature scaling, we can combine it with a mapping of the form $\tilde{x} = \arcsin(x)$ to end up with a series that closely resembles Chebyshev polynomials, which are better suited for certain problems. In analogy with neural networks, the data encoding strategy is playing a similar role to that of the activation functions.\\ \\
The relation between the $h^k$ loss functions and the $L^q$ generalisation bounds can be seen as a complementary result on differential machine learning \cite{huge2020differential}. This is the first work that gives some insight on why differential machine learning leads to better generalization results. 
From the relations that we derived, one would expect this technique to fail as we increase the input dimension. However, in practice it has shown to give very good results. An interesting line of research would be to study the threshold at which differential machine learning starts failing.\\
Since a natural application are physical systems governed by differential equations
where data on the derivatives of a target function are available, another open question remains regarding how our approach compares to standard differential equation solvers in these scenarios.

\section{Acknowledgements}
JT, VD and DD acknowledge the support received by the Dutch National Growth Fund (NGF), as part of the Quantum Delta NL programme.

JT acknowledges the support received from the European Union’s Horizon Europe research and innovation programme through the ERC StG FINE-TEA-SQUAD (Grant No. 101040729).

VD and AM acknowledge the support by the project
NEASQC funded from the European Union’s Horizon
2020 research and innovation programme (grant agree-
ment No 951821).

VD acknowledges by the Dutch Research Council
(NWO/OCW), as part of the Quantum Software Consortium programme (project number 024.003.037).

AM acknowledges the support received from the Centro de Investigación de Galicia ``CITIC", funded by Xunta de Galicia and the European Union (European Regional Development Fund- Galicia 2014-2020 Program), by grant ED431G 2019/01.

The views and opinions expressed here are solely those of the authors and do not necessarily reflect those of the funding institutions. Neither of the funding institution can be held responsible for them. 



\bibliographystyle{plainnat}
\bibliography{mainbib}



\onecolumn
\appendix

\section{Proof of Theorems \ref{thm:L_p_approximation},\ref{thm:C_0_approximation} and \ref{thm:H^k_approximation}}\label{appendix:fourier_series_uniform_approximation}
For proving Theorems \ref{thm:L_p_approximation}, \ref{thm:C_0_approximation} and \ref{thm:H^k_approximation}, we need two preliminary results. Firstly, we need to show that a quantum circuit can realise the $\ell^1$-Fèjer's mean of $C^0\left(\mathbb{T}^N\right)$ and $L^p\left(\mathbb{T}^N\right), \, \forall \, 1\leq p< \infty$ functions.
Secondly, we need to prove that we can define periodic extensions of functions belonging to $C^0\left(U\right)$, $L^p\left(U\right), \, \forall \, 1\leq p< \infty$ and $H^k\left(U\right), \, \forall \, 1\leq k< \infty$,  where $U$ is compactly embedded in $\mathbb{T}^N$
to functions belonging to $C^0\left(\mathbb{T}^N\right)$, $L^p\left(\mathbb{T}^N\right), \, \forall \, 1\leq p< \infty$ and $H^k\left(\mathbb{T}^N\right), \, \forall \, 1\leq k< \infty$ respectively . 
The combination of both results plus Fejer's theorem in multiple dimensions naturally yields Theorems \ref{thm:L_p_approximation} and \ref{thm:C_0_approximation}. Theorem \ref{thm:H^k_approximation} can be proven by a standard approximation Theorem of the Fourier series.

\subsection{Féjer's mean}\label{appendix:Fejer_mean}
In Appendix C of \cite{schuld2021effect} they showed that the quantum model
family $f_{m'}$ can rise a generalized trigonometric series of the form
\begin{align}
\label{eq:quantummodelasfourierappendix_Fejersmean}
    f_{m'}(\mathbf{x})=\sum_{\mathbf{j}\in\mathbb{Z}_K^N}c_{\mathbf{j}}e^{i\mathbf{x}\cdot\mathbf{j}}\ ,
\end{align}
where $\mathbb{Z}_K^N=\{-K,-K+1,...,0,...,K-1,K\}^N$ is contained in the Cartesian product of the frequency spectrum associated with $H_m$, as defined in Definition \ref{definition: universal hamiltonian family} and the $c_{\mathbf{j}}$ coefficients are completely determined by the observable  freely up to the complex-conjugation
symmetry that guarantees that the model output is a real-valued function. If we choose:
\begin{equation}
    c_{\mathbf{j}} = \left(1-\dfrac{\norm{\mathbf{j}}_1}{NK}\right)\hat{f}_{\mathbf{j}},
\end{equation}
where $\hat{f}_{\mathbf{j}}$ is the $\mathbf{j}$-th Fourier coefficient, then $\tilde{f}$ is the $\ell^1$-Fèjer's mean $\sigma_{NK}(f)$.

\subsection{\texorpdfstring{Periodic extension for $C^0$ functions}{Periodic extension for functions}}\label{appendix:periodic_extension}
By the Tietze extension theorem \cite{royden1968real}, there exists a function $g_1\in C^0(\mathbb{R}^N)$ with $g_1|_{U}=f^*$.
Then, we define a function $g_2\in C^0(\mathbb{R}^N)$ with $g_2|_{\overline{U}}=1$ and $g_2|_{\mathbb{R}^N\backslash V}=0$, where $V$ is defined as $\overline{U}\subset V\subset (0,2\pi)^N$. This set $V$ exists since $U$ is compactly contained in $[0,2\pi]^N$.\\ 
We can explicitly construct the function $g_2$ in the following way:
Let $\delta>0$, such that the closure $\overline{\omega_{2\delta}}$ of the $2\delta$-neighbourhood of $\omega$, is contained in $[0,2\pi]^N$, which is possible due to $U$ being compactly contained in $[0,2\pi]^N$. 
We define $V:=\omega_{2\delta}$ and a function $\psi_{\delta}\in C^0(\mathbb{R}^N)$, supported on the $\delta$ Ball in $\mathbb{R}^N$ centred around $0$ and normalised as $\int_{\mathbb{R}^N}\psi_{\delta}(x)dx=1$. Then, we define $g_2$ as the convolution of $\mathds{1}_{U_\delta}$ and $\psi_{\delta}$:
\begin{align}
    g_2(x)=\int_{\mathbb{R}^N}\mathds{1}_{U_\delta}(\tau)\psi_{\delta}(\tau-x)d\tau\ .
\end{align}
With this construction, $g_2$ satisfied the asked properties.
We define the extension $f_{ext}$ as the product $g_1g_2$, which yields a function $f^*_{ext}$ with 
\begin{align}
    f^*_{ext}|_{U}&=f^*\ ,\\
    f^*_{ext}|_{\mathbb{R}^N\backslash
V}&=0\ ,\text{ hence}\\
f^*_{ext}(x)&=f^*_{ext}(y)\quad\forall x,y\in\partial \mathbb{T}^N\ .
\end{align}
The such defined extension $f^*_{ext}$ is thus an element of $C^0([0,2\pi]^N)$ with periodic boundary conditions, so we can map it onto the $N$-dimensional torus $\mathbb{T}^N$. 
\subsection{\texorpdfstring{Periodic extension for $L^p$ functions}{Periodic extension for functions}}
Given a target function $f^*\in L^p(U)$, we construct an extension $f^*_{ext}$ in the following way:
\begin{align}
    f^*_{ext}|_{U}&=f^*\ ,\\
    f^*_{ext}|_{\mathbb{T}^N\backslash
U}&=0\ ,\text{ hence}\\
f^*_{ext}(x)&=f^*_{ext}(y)\quad\forall x,y\in\partial \mathbb{T}^N\ .
\end{align}
The function $f^*_{ext}$ is thus an element of $L^p(\mathbb{T}^N)$

\subsection{\texorpdfstring{Periodic extension for $H^k$ functions}{Periodic extension for functions}}\label{appendix:periodic_extension_Hk}
By the extension theorems for Sobolev functions \cite[Theorem 2.2, Part 2]{burenkov1999extension}, there exists a function $g_1\in H^{k+1}(\mathbb{R}^N)$ with $g_1|_{U}=f^*$.
Then, we define a function $g_2\in H^{k+1}(\mathbb{R}^N)$ with $g_2|_{\overline{U}}=1$ and $g_2|_{\mathbb{R}^N\backslash V}=0$, where $V$ is defined as $\overline{U}\subset V\subset (0,2\pi)^N$. This set $V$ exists since $U$ is compactly contained in $[0,2\pi]^N$.\\ 
We can explicitly construct the function $g_2$ in the following way:
Let $\delta>0$, such that the closure $\overline{\omega_{2\delta}}$ of the $2\delta$-neighbourhood of $\omega$, is contained in $[0,2\pi]^N$, which is possible due to $U$ being compactly contained in $[0,2\pi]^N$. 
We define $V:=\omega_{2\delta}$ and a function $\psi_{\delta}\in H^{k+1}(\mathbb{R}^N)$, supported on the $\delta$ Ball in $\mathbb{R}^N$ centred around $0$ and normalised as $\int_{\mathbb{R}^N}\psi_{\delta}(x)dx=1$. Then, we define $g_2$ as the convolution of $\mathds{1}_{U_\delta}$ and $\psi_{\delta}$:
\begin{align}
    g_2(x)=\int_{\mathbb{R}^N}\mathds{1}_{U_\delta}(\tau)\psi_{\delta}(\tau-x)d\tau\ .
\end{align}
With this construction, $g_2$ satisfied the asked properties.
We define the extension $f_{ext}$ as the product $g_1g_2$, which yields a function $f^*_{ext}$ with 
\begin{align}
    f^*_{ext}|_{U}&=f^*\ ,\\
    f^*_{ext}|_{\mathbb{R}^N\backslash
V}&=0\ ,\text{ hence}\\
f^*_{ext}(x)&=f^*_{ext}(y)\quad\forall x,y\in\partial \mathbb{T}^N\ .
\end{align}
The such defined extension $f^*_{ext}$ is thus an element of $H^{k+1}([0,2\pi]^N)$ with periodic boundary conditions, so we can map it onto the $N$-dimensional torus $\mathbb{T}^N$. 


\subsection{Proof of Theorems \ref{thm:L_p_approximation}, \ref{thm:C_0_approximation} and \ref{thm:H^k_approximation}}
The final step leverages the power of Fejèr's theorem in multiple dimensions \cite{WEISZ201199}:
\begin{thm}\label{thm:fejer_L_p}
    For all functions $f^*\in L^p\left(\mathbb{T}^N\right)$ with $1\leq p <\infty$, and for all $\epsilon >0$, there exists some $t \in \mathbb{N}$, such that such that
\begin{equation}
   \norm{\sigma_{t}\left(f\right)-f^*}_{L^p}<\epsilon.
\end{equation}
\end{thm}
Combining Theorem \ref{thm:fejer_L_p} with the fact that quantum circuits can recover any $\ell^1$-Fèjer's mean as shown in Appendix \ref{appendix:Fejer_mean}  and the fact that we can extend any function in $L^p\left(U\right)^N, \, \forall \, 1\leq p< \infty$ where $U$ is compactly contained in $\mathbb{T}^N$ to a function in $L^p \left(\mathbb{T}^N\right), \, \forall \, 1\leq p< \infty$ as shown in Appendix \ref{appendix:periodic_extension} directly implies Theorem \ref{thm:L_p_approximation}.\\ \\
Similarly, for continuous functions we have another version of Fèjer's theorem for continuous functions \cite{WEISZ201199}:
\begin{thm}\label{thm:fejer_C_0}
    For all functions $f^*\in C^0\left(\mathbb{T}^N\right)$, and for all $\epsilon >0$, there exists some $t \in \mathbb{N}$, such that such that
\begin{equation}
   \norm{\sigma_{t}\left(f\right)-f^*}_{\infty}<\epsilon.
\end{equation}
\end{thm}
Combining Theorem \ref{thm:fejer_C_0} with the fact that quantum circuits can recover any $\ell^1$-Fèjer's mean as shown in Appendix \ref{appendix:Fejer_mean}  and the fact that we can extend any function in $C^0\left(U\right)^N, \, \forall \, 1\leq p< \infty$ where $U$ is compactly contained in $\mathbb{T}^N$ to a function in $C^0 \left(\mathbb{T}^N\right), \, \forall \, 1\leq p< \infty$ as shown in Appendix \ref{appendix:periodic_extension} directly implies Theorem \ref{thm:C_0_approximation}.\\




We finally prove Theorem \ref{thm:H^k_approximation}, which uses the setup in \cite{schuld2021effect} as described in section \ref{sec:data_normalization}:
    We note firstly that the quantum model family $f_{m'}$ generates a truncated Fourier series $\tilde{f}$ in the domain $[0,2\pi]^N$ of the form
\begin{align}
\label{eq:quantummodelasfourierappendix_H^k}
    \tilde{f}(\mathbf{x})=\sum_{\mathbf{j}\in\mathbb{Z}_K^N}c_{\mathbf{j}}e^{i\mathbf{x}\cdot\mathbf{j}}\ ,
\end{align}
where $\mathbb{Z}_K^N=\{-K,-K+1,...,0,...,K-1,K\}^N$ is contained in the Cartesian product of the frequency spectrum associated with $H_m$, as defined in Definition \ref{definition: universal hamiltonian family}.
The proof of that is written in Appendix C of \cite{schuld2021effect}.\\
Secondly, we can extend the function $f^*$ defined on $U$ to a periodic function $f^*_{ext}$ on $[0,2\pi]^N$ via the construction shown in Appendix \ref{appendix:periodic_extension_Hk}.
As written in Theorem 1.1 in \cite{canuto1982approximation}, the Fourier series of $f^*_{ext}$, which we can write in the form of equation \ref{eq:quantummodelasfourierappendix_H^k}, converges in the $H^k$-distance to $f^*_{ext}$. 
As $f^*_{ext}(x)=f^*(x)$ for all $x\in U$, the Fourier series of $f^*_{ext}$ converges in the $H^k$-distance  to $f^*$ on $U$. This implies Theorem \ref{thm:H^k_approximation}.




\section{Proof of Theorem \ref{thm:sobolev_loss}}\label{appendix:appendix2}
In this appendix, we prove Theorem \ref{thm:sobolev_loss}, which we restate here:

\begin{customthm}{5}
Given a target function $f^*\in \mathcal{F}\subseteq H^k(\Omega)$, $\Omega\subset\mathbb{R}^N$ and a model family $\mathcal{M}$ which is a universal approximator of the space $H^k$, we denote $f_{h^k}\in \mathcal{M}$ as the solution of the minimization problem defined by the distance $d_{h^k}$ (with derivatives up to order $k$). Let us further assume that $D_{H^k}\left(f^*(\mathbf{x}),f_{h^k}(\mathbf{x})\right)\leq B$ for all $\mathbf{x}\in\mathcal{X}, f^* \in\mathcal{F}$ and $f_{h^k}\in \mathcal{M}$, and that the Rademacher complexity $\Re_D(\mathcal{M})$ of the function family $\mathcal{M}$ is finite. Then, the following holds:
\begin{align}
    \dfrac{1}{C}D_{L^q}\left(f^*,f_{h^k}\right) &\leq D_{H^k}\left(f^*,f_{h^k}\right) \\
    &\leq D_{h^k}\left(f^*,f_{h^k}\right)+r(|\mathcal{M}|,I,\delta),
\end{align}
where $C$ is a constant, $r(|\mathcal{M}|,I,\delta)\to 0$ as $I\to\infty$, for the specific choices for $k$ and $q$:
\begin{enumerate}
    \item For the case when $k$ is in the interval:
\begin{equation}\label{eq:Kondrachov_constraint_appendix}
    N\left(\frac{1}{2}-\frac{1}{q}\right)<k<N/2,
\end{equation}
the Theorem holds for $1\leq q<N$.
    \item For the case that $k\geq N/2$, the Theorem holds for $1\leq q<\infty$.
\end{enumerate}
Furthermore, for $k>N/2$, we have that
\begin{align}
    \dfrac{1}{C}D_{C^0}\left(f^*,f_{h^k}\right) \leq D_{H^k}\left(f^*,f_{h^k}\right)\ ,
\end{align}
where $C$ is a constant.



\end{customthm}
\begin{proof}
    Let us begin with proving the inequality on the right hand side:
    We use standard generalization bound for the empirical risk problem, like stated in \cite[p.270]{mohri2018foundations}. Since the $H^k$ loss function is $2-$Lipschitz and we assumed it to be upper bounded by $B$, we have the inequality
    \begin{align}
        D_{H^k}\left(f^*,f_{h^k}\right) \leq D_{h^k}\left(f^*,f_{h^k}\right)+r(|\mathcal{M}|,I,\delta)\ ,
    \end{align}
    with a function $r(|\mathcal{M}|,I,\delta)$ which decays as $I\to\infty$.\\
    Secondly, let us prove the inequality of the left hand side: according to the Rellich-Kondrachov theorem in the form of
    \cite[Theorem 6.3, Part 1 and 2]{adams2003sobolev}, there exists a constant $C$, such that \begin{align}
    \|f^*-f_{h^k}\|_{L^q}\leq C\|f^*-f_{h^k}\|_{H^k}\ ,
    \end{align}
    and for the case that $k<N/2$, we have the following contraint:
\begin{equation}
    k>N\left(\frac{1}{2}-\frac{1}{q}\right).
\end{equation}. 
For the other two cases, where $k=N/2$ and $k>N/2$, \cite[Theorem 6.3, Part 1 and 2]{adams2003sobolev} is valid for $1\leq q<\infty$
    Lastly, the upper bound on the distance $D_{C^0}\left(f^*,f_{h^k}\right)$ in the supremum norm is a direct consequence of the Rellich-Kondrachov theorem in the form of
    \cite[Theorem 6.3, Part 2]{adams2003sobolev}.
\end{proof}


\end{document}
