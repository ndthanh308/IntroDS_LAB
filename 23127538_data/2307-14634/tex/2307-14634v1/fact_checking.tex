% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%

\usepackage{booktabs}

\usepackage{xcolor}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\redb}[1]{\textcolor{red}{\small{\textbf{[#1]}}}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\blueb}[1]{\textcolor{blue}{\small{\textbf{[#1]}}}}
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}}
\newcommand{\cyanb}[1]{\textcolor{cyan}{[#1]}}

\newcommand{\py}[1]{{\textbf{\color{red}{\small{[PY: #1]}}}}}


\begin{document}
%
\title{Fact-Checking of AI-Generated Reports} %\thanks{Supported by organization x.}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
\author{Razi Mahmood\inst{1} \and
 Ge Wang\inst{2}\and Mannudeep Kalra\inst{3} \and Pingkun Yan\inst{3}}
\authorrunning{R. Mahmood et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
\institute{$^{1}$Rensselaer Polytechnic Institute, $^{2}$Harvard Medical School}
%
\maketitle
% typeset the header of the contribution
%
\begin{abstract}
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in expediting clinical workflows.
%Specifically, we synthesize a new dataset of images paired with real and fake report sentences drawn from a collection of associated ground truth reports and train a classifier. Given a candidate sentence generated from an AI-generated report, and the associated image, we then classify the sentence as real or fake. The fake sentences are dropped from the AI-generated report to produce a higher quality report. Results are presented on open source benchmark datasets of chest X-rays. 
\keywords{Generative AI \and Chest X-rays \and Fact-checking \and Radiology Report.}
\end{abstract}
\section{Introduction}
\vspace{-0.05in}
With the developments in radiology artificial intelligence (AI), many researchers have turned to the problem of automated reporting of imaging studies \cite{Endo2021,gale2018producing,li2019knowledge,li2018hybrid,szolovits,Pang2023,syeda-mahmood2020,xiong2019reinforced}.  This can significantly reduce the dictation workload of radiologists, leading to more consistent reports with improved accuracy and lower overall costs. While the previous work has largely used image captioning \cite{vinyals2015show,xu2015show} or image-to-text generation methods for report generation, more recent works have been using large language models (LLMs) such as GPT-4 \cite{Grewal2023,li2023artificial}. These newly emerged LLMs can generate longer and more natural sentences when prompted with good radiology-specific linguistic cues \cite{guo-2018,krause-2019}. %such as the report generated from GPT-4 for the prompt {\em "There is a 4 cm hepatic hemangioma in segment VI of liver. Please write an MRI abdomen report on this."}. 
%Clearly, the veracity of such a report depends on the associated imaging. 

However, with powerful language generation capabilities, hallucinations or false sentences are prevalent as it is difficult for those methods to identify their own errors. This has led to fact-checking methods for output generated by LLMs and large vision models (LVMs)\cite{Passi2022,nieman,midas}. Those methods detect errors either through patterns of phrases found repeatedly in text or by consulting other external textual sources for the veracity of information\cite{Passi2022,nieman,midas}. In radiology report generation, however, we have a potentially good source for fact checking, namely, the associated images, as findings reported in textual data must be verifiable through visual detection in the associated imaging.  However, as most methods of report generation already examine the images in order to detect findings and generate the sentences,  bootstrapping them with an independent source of verification is needed in order to identify their own errors. %where the likelihood of a particular sentence in the report being real or fake could be inferred, given both the sentence itself and the associated image. 
%\py{This paragraph needs to be updated to be consistent with the abstract.}

In this paper, we propose a new imaging-driven method of fact-checking of AI-generated reports. Specifically, we develop a fact-checking examiner to differentiate between real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings.  To train such an examiner, we first create a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real or fake labels via a classifier. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the examiner to bootstrap their report generation leading to potentially more reliable reports. This can lead to a more responsible use of AI in expediting future clinical workflows.
%To train such classifier, we synthesize a new dataset of fake reports by perturbing the findings in original ground truth radiology reports associated with imaging. %CLIP-based text encodings of real and fake sentences drawn from these reports are then paired with CLIP-based image encodings to learn the mapping to fake or real labels via machine learning classifiers. 
%The utility of the discriminator is demonstrated for improving the quality of a large dataset of automatically generated reports by detecting and removing fake sentences. As we will make this dataset available in open source, future generative AI approaches can use the resulting tool to validate their reports. This can lead to a more responsible use of AI in expediting clinical workflows.
%Specifically, we synthesize a new dataset of medical images paired with real and fake report sentences drawn from a collection of associated ground truth reports. We then train a classifier using the images with real/fake sentence pairs.  Given a candidate sentence generated from an AI-generated report, and the associated image, we then classify the sentence as real or fake. The fake sentences are dropped from the AI-generated report to produce a higher quality report. Results are presented on open source benchmark datasets of chest X-rays. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in clinical workflows. The paired set of images with real and fake sentences will be made available in open source upon paper acceptance.
%% Figure environment removed
% Figure environment removed
\vspace{-0.5in}
%owever, to create a robust discriminator that is not attuned for any particular automated reporting software, we would need to draw from original reports and stimulate the various situations that could make the reports internally incorrect or incomplete, with the understanding that most of the potential defects in an AI software would be covered. This approach ensures that we are still using actual valid sentences that were shuffled from other reports, without the risk of automated generation software potentially giving out-of-place results. 
\section{Overall approach}
\vspace{-0.05in}
Our overall approach to training and inference using the examiner is illustrated in Figure~\ref{fig:overview}. To create a robust examiner that is not attuned for any particular automated reporting software,
%differentiate fake content hallucinated by AI models, it is important to learn from examples; therefore, 
it is critical to create a dataset for training that encompasses a wide array of authentic and fabricated samples. Hence we first synthesize a dataset of real and fake reports using a carefully controlled process of perturbation of actual radiology reports associated with the images. We then pair each image with sentences from its corresponding actual report as real sentences with real label and the perturbed sentences from fake reports as fake sentences with fake label. Both textual sentence and images are then encoded by projecting in a joint image-text embedding space using the CLIP model\cite{Radford2021}. The encoded vectors of image and the paired sentence are then concatenated to form the feature vector for classification. %The feature vector containing the perturbed sentence encoding is given the fake label while the feature vector combining image with a sentence from the original report is given the real label.  
A binary classifier is then trained on this dataset to produce a discriminator for real/fake sentences associated with a given image. 

The fact-checker can be used for report verification in inference mode.  Given an automatically produced radiology report, and the  corresponding input imaging study, the examiner extracts sentences from the report, and the image-sentence pair is then subjected to the same encoding process as used in training. The combined feature vector is then given to the classifier for determination of the sentence as real or fake. A revised report is assembled by removing those sentences that are deemed fake by the classifier to produce the revised report.  

The rest of the paper describes the approach in detail. In Section~\ref{errormodeling}, we model the different types of errors found in automated reports and present an approach of synthesizing these errors by centering them around findings in sentences We then present our examiner and show how it can be applied to verify automatic reports in Section~\ref{examine}. Finally, in Section~\ref{results}, we present results describing details of dataset created and evaluation experiments.
\vspace{-0.1in}
\section{Generation of a synthetic report dataset}
\label{errormodeling}
\vspace{-0.05in}
%To simulate the full spectrum of errors seen across automated report generation software, we developed a synthetic report dataset by carefully perturbing actual radiology reports. 
The key idea in synthetic report generation is to center the perturbation operations around findings described in the finding sections of reports, as these are critical to preliminary reads of imaging studies.  
\vspace{-0.1in}
%through addition/replacement of sentences containing findings.   
\subsection{Modeling finding-related errors in automated reports}
%We now analyze the typical errors found in automatically produced reports. 
\label{findingtypes}
The typical errors seen in the finding sections of reports can be due to (a) addition of incorrect findings not seen in the accompanying image, (b) exchange errors, where certain findings are missed and others added, (c) reverse findings reported i.e. positive instance reported when negative instances of them are seen in image and vice versa,  (d) spurious or unnecessary findings not relevant for reporting, and finally (e) incorrect description of findings in terms of fine-grained appearance, such as extent of severity, location correctness, etc. 

From the point of real/fake detection, we focus on the first 3 classes of errors for synthesis as they are the most common. Let $R=\{S_{i}\}$ be a ground-truthed report corresponding to an image $I$ consisting of sentences $\{S_{i}\}$ describing corresponding findings $\{F_{i}\}$.  Then we can simulate a random addition of a new finding by extending the report $R$ as $R_{a}=\{S_{i}\}\cup \{S_{a}\}$ where $S_{a}$ describes a new finding $F_{a}\not\in\{F_{i}\}$. Similarly, we simulate condition (b) through an exchange of finding where one finding sentence $S_{r}$ is removed to be replaced by another finding sentence $S_{a}$ as $R_{e}=\{S_{i}\}-\{S_{r}\}\cup \{S_{a}\}$. Finally, we can simulate the replacement of positive with negative findings and vice versa to form a revised report $R_{r}=\{S_{i}\}-\{S_{p}\}\cup \{S_{p'}\}$ where $S_{p}$ is a sentence corresponding to a finding $F_{p}$ and $S_{p'}$ is a sentence corresponding to the finding $F_{p'}$ which is in opposite sense of the meaning. For example, a sentence "There is pneumothorax", could be replaced by  "There is no pneumothorax" to represent a reversal of polarity of the finding.  Figure~\ref{fig3} shows examples of each of the type of operations of add, exchange and reverse findings respectively. 

%Our overall approach to training and inference using the classifier is illustrated in Figure~\ref{fig:overview}. \blue{To differentiate fake content hallucinated by AI models, it is important to learn from examples; therefore, it is critical to create a dataset for training that encompasses a wide array of authentic and fabricated samples.} Thus, we first synthesize a dataset of real and fake reports using a carefully controlled process of perturbing actual radiology reports associated with the images. We then pair each image with sentences from its corresponding actual report as real sentences and the perturbed sentences from fake reports as fake sentences. Both textual sentence and images are then encoded by projecting in a joint image-text embedding space using the CLIP model\cite{Radford2021}. The encoded vectors of image and the paired sentence are then concatenated to form the feature vector for classification. The feature vector containing the perturbed sentence encoding is given the fake label while the feature vector combining image with a sentence from the original report is given the real label. A binary classifier is then trained on this dataset to produce a discriminator for real/fake sentences associated with a given image. 

%During the inference phase, given a radiology report automatically produced by any automated report generation tool, and the corresponding imaging study, the discriminator extracts sentences from the report, and the image-sentence pair is then subjected to the same encoding process as used in training. The combined feature vector is then given to the classifier for determination of the sentence as real or fake. A revised report can then be assembled by removing or modifying those sentences that are deemed fake by the classifier. This is expected to improve the overall quality of the auto-generated report.  

%\subsection{Modeling finding-related errors in automated reports}
%We now analyze the typical errors found in automatically produced reports. These tend to be in the finding sections, and can be due to (a) addition of incorrect findings not seen in the accompanying image, (b) exchange errors, where certain findings are missed and others added, (c) reverse findings reported i.e. positive instance reported when negative instances of them are seen in image and vice versa,  (d) spurious or unnecessary findings not relevant for reporting, and finally (e) incorrect description of findings in terms of fine-grained appearance, such as extent of severity, location correctness, etc. 
%From the point of real/fake detection, we focus on the first 3 classes of errors as they are the most common and simulate them through the following operations. Let $R=\{S_{i}\}$ be a ground-truthed report corresponding to an image $I$ consisting of sentences $\{S_{i}\}$ describing corresponding findings $\{F_{i}\}$.  Then we can simulate a random addition of a new finding by extending the report $R$ as $R_{a}=\{S_{i}\}\cup \{S_{a}\}$ where $S_{a}$ describes a new finding $F_{a}\not\in\{F_{i}\}$. Similarly, we simulate condition (b) through an exchange of finding where one finding sentence $S_{r}$ is removed to be replaced by another finding sentence $S_{a}$ as $R_{e}=\{S_{i}\}-\{S_{r}\}\cup \{S_{a}\}$. Finally, we can simulate the replacement of positive with negative findings and vice versa to form a revised report $R_{r}=\{S_{i}\}-\{S_{p}\}\cup \{S_{p'}\}$ where $S_{p}$ is a sentence corresponding to a finding $F_{p}$ and $S_{p'}$ is a sentence corresponding to the finding $F_{p'}$ which is in opposite sense of the meaning. For example, a sentence "There is pneumothorax", could be replaced by  "There is no pneumothorax" to represent a reversal of polarity of the finding.  Figure~\ref{fig3} shows examples of each of the type of operations of add, exchange and reverse findings respectively. 
% Figure environment removed
%A key element of our approach is the generation of a fake report datasets with a controlled perturbation process. This is described in the following sections. 
%Although the original images were in high resolution DICOM format, the open collection was converted to a 8-bit png format and performed some post-processing to clip 0.5\% pixel values to eliminate very dark or very bright pixel outliers as described in the data set description. 
\vspace{-0.5in}
\subsection{Detecting findings in sentences}
\label{findingdetector}
 Since detecting findings is key to our approach,  our synthetic dataset generation focused on chest X-ray datasets as finding detectors are well-developed for these datasets. Further, the majority of work on automated reporting has been done on chest X-rays and finding-labeled datasets are publicly available\cite{Misc:NIHChestXray2017,mimic-4,irvin2019chexpert}. However, most of the existing approaches summarize findings at the report level. To locate findings at the sentence level, we used NLP tools such as Spacy to separate sentences. We then used a combination of ChexPert\cite{irvin2019chexpert} labeler and NegSpacy\cite{negspacy} parser to extract positive and negative findings from sentences. Table~\ref{tab1} shows examples of findings detected in sentences. The detected findings were then validated against the ground truth labels provided at the report level in the datasets. All unique findings across reports were then aggregated into a pool $\{F_{pool}\}$ and  all unique sentences in the original reports were aggregated and mapped to their findings (positive or negative) to create the pool of sentences $\{S_{pool}\}$. 

%In our dataset, the important findings were already documented by previous annotators at report level. For modeling the errors, however, we need to know the findings at the level of sentences within reports. To detect these, we used a combination of ChexPert~\cite{irvin2019chexpert} labeler and NegSpacy~\cite{negspacy} parser to extract positive and negative findings from sentences. Table~\ref{tab1} shows examples of findings detected in sentences. The detected findings were then validated against the ground truth labels documented at the report level. All unique findings across reports were then aggregated into a pool $\{F_{pool}\}$ and  all unique sentences in the original reports were aggregated and mapped to their findings (positive or negative) to create the pool of sentences $\{S_{pool}\}$. 

\begin{table*}
\caption{Illustration of extracting findings from reports. Negated findings are shown within square brackets.}\label{tab1}
\begin{tabular}{l|l}
\toprule
{\bf Sentences} & {\bf Detected findings}\\
\midrule
There is effusion and pneumothorax. 	& `effusion', `pneumothorax'\\
\midrule
No pneumothorax, pleural effusion, but there is  & `consolidation',\\
 lobar air space consolidation. 	& [`pneumothorax', `pleural effusion']\\
\midrule
No visible pneumothorax or large pleural effusion. 	 &[`pneumothorax', `pleural effusion']\\
\midrule
Specifically, no evidence of focal consolidation, &[`focal consolidation'],\\
pneumothorax, or pleural effusion. 	& [`pneumothorax', `pleural effusion']\\
\midrule
No definite focal alveolar consolidation, & [`alveolar consolidation'],\\
no pleural effusion demonstrated. 	& [`pleural effusion']\\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}
\caption{Details of the fake report dataset distribution. 2557 frontal views were retained for images. 64 negative findings were retained and 114 positive findings.}\label{tab2}
\begin{tabular}{l|l|l|l|l|l|l|l}
\toprule
{\bf Dataset} & {\bf Patients} & {\bf Images} & {\bf Reports} & {\bf Pos/Neg}  & {\bf Unique}&{\bf Image-Sent} & {\bf Fake}\\
& & {\bf /Views}& & {\bf Findings}& {\bf Sentences} & {\bf Pairs} & {\bf Reports}\\
\midrule
Original & 1786 & 7470/2557 & 2557 & 119/64 & 3850 & 25535 & 7671\\ %11017
Training & 1071 & 2037 & 2037 & 68 & 2661 & 20326 & 4074 \\
Testing & 357 & 254 & 254 & 68 & 919 & 2550 & 508\\
%Report evaluation & 358\\
\bottomrule
\end{tabular}
\end{table*}
\subsection{Fake report creation}
\label{fakereport}
For each original report $R$ associated with an image $I$, we create three instances of fake reports $R_{a}, R_{e},R_{r}$ corresponding to the operations of addition, exchange and reversal of findings respectively. Specifically, for creating $R_{a}$ type of reports, we randomly draw from $S_{pool}$ a sentence that contains a randomly selected finding $F_{a}\notin\{F_{i}\}$ where $\{F_{i}\}$ are the set of findings in $R$ (positive or negative). Similarly, to create $R_{e}$, we randomly select a finding pair $(F_{ei},F_{eo})$ where $F_{ei}\in\{F_{i}\}$ and $F_{eo}\in\{F_{pool}\}-\{F_{i}\}$ . We then remove the associated sentence with $F_{ei}$ in $R$ and replace it with a randomly chosen sentence associated with $F_{eo}$ in $\{S_{pool}\}$. Finally, to create the reversed findings reports, $R_{r}$, we randomly select a positive or negative finding $F_{p}\in\{F_{i}\}$, remove its corresponding sentence and swap it with a randomly chosen sentence $S_{p'}\in\{ S_{pool}\}$, containing findings $F_{p'}$ that is reversed in polarity. The images, their perturbed finding and associated sentences were recorded in each case of fake reports so that they could be used for forming the pairing dataset for training the fact-checking examiner described next. 
%% Figure environment removed
\vspace{-0.2in}
\section{Fact-checking of AI-generated reports}
\label{examine}
We now present details of our fact-checking examiner and discuss how it can be used to improve the quality of automatically generated reports through verification.  
\subsection{Fact-checking Examiner}
%We now describe the development of the Real/Fake discriminator/classifier from the labeled dataset created. 
The fact-checking examiner is a classifier using deep-learned features derived from a joint image-text encodings. Specifically, 
since we combine images with textual sentences, we chose a feature encoding that is already trained on joint image and text pairs. In particular we chose the CLIP joint image-text embedding model\cite{Radford2021} to project the image and textual sentences into a common 512-length encoding each. The CLIP model we chose was originally pre-trained   on  natural  image-text  pair and subsequently trained on radiology report-image pairs as described in\cite{Endo2021}. We then concatenate the image and textual embedding into a 1024-length feature vector to train a binary classifier. In the splits chosen, the real/fake incidence distribution was relatively balanced (2:1) so that the accuracy could be used as a reliable measure of performance.  We experimented with several classifiers ranging from support vector machines (SVM) to neural net classifiers and as we observed similar performance, we retained a simple linear SVM as sufficient for the task. 
\vspace{-0.08in}
\subsection{Improving the quality of reports through verification}
\label{qi}
We apply the fact-checking examiner to filter our incorrect/irrelevant sentences in automatically produced reports as shown in Figure~\ref{fig:overview}b. Specifically, given an automatically generated report for an image, we pair the image with each sentence of the report. We then use the same CLIP encoder used in training the examiner,  to encode each pair of image and sentence to form a concatenated feature vector. The examiner predicted fake sentences are then removed to produce the revised report. 

We develop a new measure to judge the improvement in the quality of the automatic report after applying the fact-checking examiner. Unlike popular report comparison measures such as BLEU, ROUGE scores that perform lexical comparisons, we use a semantic distance measure formed from encoding the reports through large language models such as SentenceBERT\cite{sbert}.  Specifically, let $R=\{S_{i}\}$, $R_{auto}=\{S_{auto}\}$, $R_{corrected}=\{S_{corrected}\}$ be the  original, automated, and corrected reports with their sentences respectively.  To judge the improvement in quality of the report, we adopt SentenceBERT\cite{sbert} to encode the individual sentences of the respective reports to produce an average encoding per report as $E_{R},E_{auto},E_{corrected}$ respectively. Then the quality improvement score, $QI(R)$ per triple of reports $(R, R_{auto}, R_{corrected})$ is given by the difference in the cosine similarity between the pairwise encodings  as
\begin{equation}
    QI(R,R_{auto},R_{corrected})=d(E_{R},E_{corrected})-d(E_{R},R_{auto})
\end{equation} 
\noindent where $d$  is the cosine similarity between the average encodings. This measure allows for unequal lengths of reports. A positive value indicates an improvement while a negative value indicates a worsening of the performance. %which could arise from the discriminator's own classification performance. 
The overall improvement in the quality of automatically generated reports is then given by
\begin{equation}
    QI=n_{positive} / n_{R}
\end{equation} where $n_{positive}=arg_{R}(c(E_{R},E_{corrected})>c(E_{R},R_{auto}))$ or the number of times the corrected reports are closer to original reports by applying the examiner, and $n_{R}$ is the total number of automated reports evaluated. 
\vspace{-0.1in}
\section{Results}
\label{results}
\vspace{-0.05in}
 To test our approach for fact-checking of radiology reports,  we selected an open access dataset of  chest X-rays from Indiana University\cite{indiana}  provided on Kaggle, which contains 7,470 chest X-Ray (frontal and lateral views) images with corresponding 2557 non-duplicate reports from 1786 patients. The dataset also came with annotations  documenting important findings at the report level. Of the 1786 patients, we used a (60-20-20)\% patient split for training the examiner, testing the examiner, and evaluating its effectiveness in report correction respectively, thus ensuring no patient overlap between the partitions.
% Figure environment removed
\vspace{-0.4in}
\subsection{Fake report dataset created}
By applying NLP methods of sentence extraction, we extracted 3850 unique sentences from radiology reports. By applying the finding extractor at the sentence level as described in Section~\ref{findingdetector}, we catalogued a total of 119 distinct positive and 64 negative findings as shown Table~\ref{tab2}. Using these findings and their sentences in the 2557 unique reports, and the 3 types of single perturbation operations described in Section~\ref{findingtypes}, we generated 7,671 fake reports as shown in Table~\ref{tab2}.

%As described earlier, we set aside reports corresponding to 20\% of the patients for quality improvement testing experiments. The rest of the reports were used to form the training and test dataset for the classifier. 
The training and test dataset for the fact-checking examiner was generated by randomly drawing sentences from sentence pool $\{S_{pool}\}$.  Each image was first paired with each sentence from its original report and the pair was given the "Real" label. The perturbed sentence  drawn from $\{S_{pool}\}$ from the fake reports was then retrieved from each fake report and paired with the image and given the "Fake" label.  By this process, we generated 20,326 pairs of images with real/fake sentences for training, and 2,550 pairs for testing as shown in Table~\ref{tab2} using 80\% of the 1786 patients. 
%The resulting dataset for training the discriminator is shown via a small sample in Figure~\ref{fig4} for the purpose of illustration.
\vspace{-0.1in}
\subsection{Fact-checking examiner accuracy}
%We now present results illustrating the approach of developing a real/fake discriminator and its use in report quality improvement.
Using the train-test splits shown in Table~\ref{tab2}, we trained fact-checking examiner with encodings of image-sentence pairs shown in Table~\ref{tab2}. The resulting classifier achieved an average accuracy of 84.2\% and the AUC was 0.87 as shown in Figure~\ref{fig5}b. By using 10 fold cross-validation in the generation of the (60-20-20) splits for the image-report dataset, and using different classifiers provided in the Sklearn library (decision tree, logistic regression, etc.) the average accuracy lay in the range $0.84 \pm 0.02$. 
\vspace{-0.2in}
\subsection{Overall report quality improvement evaluation}
We evaluated the efficacy of the fact-checking examiner on two report datasets, one synthetic with controlled "fakeness" and another dataset generated by a published algorithm described in \cite{syeda-mahmood2020}. Specifically, using the 20\% partition of patients from the Indiana reports that was not used to train or test the examiner, we selected 3089 of the fake reports shown in Table~\ref{tab2}. We evaluated the improvement in report quality using the method described in Section~\ref{qi}.  These results are summarized in Table~\ref{tab3}. Since our fake reports had only one fake sentence added, the performance improvement while still present, is modest around 5.3\% but the quality still improved 89\% of the time as shown in Table~\ref{tab3}.
\begin{table*}
\caption{Report quality evaluation by our examiner on two automatically generated report datasets. }\label{tab3}
\begin{tabular}{l|l|l|l}
\toprule
{\bf Dataset} & {\bf Reports} & {\bf QI score} &{\bf Similarity improvement}\\
\midrule
Synthetic Reports from Indiana & 3089 & 0.81 & 5.3\%\\
\midrule
NIH Reports & 198 & 0.89 & 15.4\%\\
\bottomrule
\end{tabular}
\end{table*}
To test the performance on automated reports generated by existing algorithms, we obtained a reference dataset consisting of freshly created reports on the NIH image dataset\cite{Misc:NIHChestXray2017} created by radiologists as described in \cite{Wu2020}. We retained the output of an automated report generation algorithm for the same images described in \cite{syeda-mahmood2020}. As this algorithm reported the highest recorded clinical accuracy on comparison with manually created reports, any improvement provided for such reports by our examiner could imply even more improvement in quality for automated reports generated by other methods.  A total of 198 pairs of original and automatically created reports along with their associated imaging from the NIH dataset was used for this experiment. The results of quality improvement is shown in Table~\ref{tab3} row 2. As can be seen, the quality improvement is even greater for reports produced by automated report extraction methods. 
%Following the creation of the dataset with robust fabrications across a multitude
%of text data an with images, we would have to pass this into a classifier. Thekey challenge was understanding how to combine image and text data into newvectors to train on.Initial experiments involved working with the VGG-16 model to combine thedata. The problem here is the model is not able to compile large sets of differentdimensions. The image and text datasets had sizes of (...) and (...) respectively.We then chose the CLIP method, as it is already pre-trained on image-textpairs

%To be able to train a discriminator with robustness of of errors, we needed to develop a dataset that was able to capture different forms of fabrications within the reports. The core datasets that were used were the Indiana Radiology reports and the MIMIC finding dataset. The fabrications of detected findings being simulated were narrowed down to 4 major approaches: adding findings, removing findings, swapping findings, and negating findings.

%The goal was to create a fake report dataset, which would then be used as a benchmark to simulate automated generative reports.


%Each image classified manually into frontal and lateral chest X-ray categories.
%\section{Training of the classifier}

%Following the creation of the dataset with robust fabrications across a multitude of text data an with images, we would have to pass this into a classifier. The key challenge was understanding how to combine image and text data into new vectors to train on. 

%Initial experiments involved working with the VGG-16 model to combine the data. The problem here is the model is not able to compile large sets of different dimensions. The image and text datasets had sizes of (...) and (...) respectively. 

%We then chose the CLIP model, as it is already pre-trained on image-text pairs. The CLIP model
\vspace{-0.1in}
\section{Conclusion}
\vspace{-0.05in}
In this paper, we have proposed for the first time, an image-driven verification of automatically produced radiology reports. A dataset was carefully constructed to elicit the different types of errors produced by such methods.  novel fact-checking examiner was developed using pairs of real and fake sentences with their corresponding imaging. The work will be extended in future to cover larger variety of defects and extended evaluation on a larger number automated reports. 
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\bibliography{anthology, refs_py}
\bibliographystyle{splncs04}
\end{document}
