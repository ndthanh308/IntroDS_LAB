\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{square,numbers}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
   %  \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\title{Supplementary materials}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\begin{document}

\maketitle

\section{Dendritic integration based quadratic neural network (DIQNN)}

The form of DIQNN discussed in the main text are given as follows:
\begin{equation}\label{DIQNN}
    f(x)=x^TAx
\end{equation}

\subsection{Performance of DIQNN on other datasets}

\begin{table}[htbp!]
    \caption{Datasets' information.}
    \label{toy_dataset}
    \centering
    \begin{tabular}{cccc}
    \toprule
       \textbf{Name} & \textbf{Feature dimension} & \textbf{Number of classes}  & 
     \textbf{Dataset size}\\
     \midrule
       \textbf{Iris} & 4 & 3 & 150 \\
        \midrule
        \textbf{Wine}  &  13   & 3  &   178     \\
                \midrule
        \textbf{Wiscosin}  &  30   & 2  &  569     \\
                \midrule
       \textbf{Handwritten digit}  &  64   & 10 &   1794     \\
        \bottomrule
    \end{tabular}
\end{table}

% Figure environment removed


Here we conduct numerical experiments on four classification datasets: Iris, Wine, Wisconsin and Handwritten digit, all these datasets are available at \url{http://archive.ics.uci.edu/ml}. Information about these four datasets are given in Table~\ref{toy_dataset}. And here we randomly choose some of the data points to form test set, and the remaining data points are treated as train set. Then we train a single layer linear net and a single layer DIQNN under cross-entropy loss with gradient descent algorithm , and compare the final results under the same choice of hyperparameters as shown in Table~\ref{experiment_set_up}.

Performance is given in Figure~\ref{toy_dataset_qnn}. These results indicate that DIQNN outperforms the linear net, in terms of both generalization (test error) and training loss.

\subsection{Low rank properties of DIQNN}

In the main text, we have discussed the low rank properties of DIQNN from two aspects: (a) the relationship between test accuracy and the number of retained eigenvectors sorted in descending order demonstrates that the original test accuracy can be achieved when the first few eigenvectors are kept in weight matrix $A$; (b) the similarity between the leading eigenvectors of weight matrices and the spike-triggered average of output neurons indicates that the majority of information in spike-triggered average could be encoded by the leading eigenvector of the weight matrix $A$. Here, the spike-triggered average is computed by averaging the inputs that are classified into the same category. These results are shown in Figure~\ref{low_rank_property}.

% Figure environment removed


\section{Experimental set up}

The experimental details, i.e. the choice of hyperparameters, are given in Table~\ref{experiment_set_up}, and the comparison between linear net and our models (DIQNN and Low-Rank DIQNN) is under the same choice of hyperparameters. It should be noted that on CIFAR-10 dataset, we use a weight decay of 0.001 and momentum of 0.9, and start with a learning
rate of 0.002, divide it by 10 at 60 epochs and terminate training at 250 epochs.

\begin{table}[htbp!]
    \caption{Experiment details.}
    \small
    \label{experiment_set_up}
    \centering
    \begin{tabular}{cccc}
    \toprule
       \textbf{Dataset} & \textbf{Learning rate} & \textbf{Batch size}  & 
     \textbf{Number of epochs}\\
     \midrule
       \begin{tabular}{c}
           \textbf{Iris, Wine,}   \\
             \textbf{Wiscosin, Handwritten digit}
       \end{tabular} & 0.01 & - (Full batch) & 1000 \\
        \midrule
        \textbf{MNIST}  &  0.01   & 100  &   5     \\
                \midrule
        \textbf{FMNIST}  &  0.01   & 100  &  20     \\
                \midrule
       \textbf{CIFAR-10}  &  0.002   & 128 &   250     \\
        \bottomrule
    \end{tabular}
\end{table}



\section{Low-Rank DIQNN}

The form of weight matrix in Low-Rank DIQNN discussed in the main text is given as follows:
\begin{equation}\label{low rank DIQNN}
    A=\sum_{i=1}^{r}\mathbf{c}_i\mathbf{c}_i^T
\end{equation}

\subsection{Performance of Low-Rank DIQNN on other datasets}

% Figure environment removed

Here, we conduct numerical experiments on Iris, Wine, Wiscosin and Handwritten digit. And the training details are given in Table~\ref{experiment_set_up}. Figure~\ref{toy_dataset_lrnn} shows the performance of Low-Rank DIQNN. Low-Rank DIQNN with one rank outperforms linear net on these datasets and the test accuracy is high and almost saturated when the weight matrix is rank one. 


\subsection{Proof of the theorem on XOR problem}

\begin{theorem}
Using single layer Low-Rank DIQNN with one rank under cross-entropy loss $\mathcal{L}(\theta)$ and gradient flow algorithm on XOR problem\footnote{Here we only consider the intialization: $c_1=[1,0]^T, c_2=[0,1]^T$. }, we have:
\begin{equation*}
    \frac{d\Delta\mu}{dt}> 0\ \  \text{and}\ \lim_{t\rightarrow\infty}\Delta\mu=1.
\end{equation*}
\end{theorem}

\begin{proof}
\textbf{Idea:} Here we first try to reduce these four data points to one. After we have proved that we only need to consider a single data point, we can prove the result by explicitly writing down $\frac{d\Delta\mu}{dt}$.

% Figure environment removed

Here the network mapping is: $\Phi(\cdot,\theta): x\in\mathbb{R}^2\rightarrow (\langle c_1,x\rangle^2,\langle c_2,x\rangle^2)^T\in\mathbb{R}^2$, where $\theta = (c_1,c_2)$ is the trainable parameters of Low-Rank DIQNN. Due to the fact that $x_1=-x_4, x_2=-x_3$, The cross-entropy loss can be written as:
\begin{equation*}
    L(c_1,c_2)=(\log(1+e^{-s_{1}})+\log(1+e^{-s_{2}}))/2,
\end{equation*}
where $s_1 = \langle c_1,x_1 \rangle^2-\langle c_2,x_1\rangle^2, s_2=\langle c_2,x_2\rangle^2-\langle c_1,x_2\rangle^2$. Then by gradient flow algorithm, we can explicitly write down the dynamics of $\theta$:
\begin{equation*}
    \frac{dc_1}{dt}=-\frac{\partial L(c_1,c_2)}{\partial s_{12}}\langle c_1,x_1\rangle x_1+\frac{\partial L(c_1,c_2)}{\partial s_{21}}\langle c_1,x_2\rangle x_2,
\end{equation*}
\begin{equation*}
    \frac{dc_2}{dt}=-\frac{\partial L(c_1,c_2)}{\partial s_{21}}\langle c_2,x_2\rangle x_2+\frac{\partial L(c_1,c_2)}{\partial s_{12}}\langle c_2,x_1\rangle x_1.
\end{equation*}
Then we have: 
\begin{equation*}
    \frac{d\langle c_1,x_1\rangle ^2}{dt}=2v_{12}\langle c_1,x_1\rangle ^2, \ 
    \frac{d\langle c_2,x_1\rangle ^2}{dt}=-2v_{12}\langle c_2,x_1\rangle ^2,
\end{equation*}
where $v_{12}=\frac{1}{1+e^{-s_{1}}}, v_{21}=\frac{1}{1+e^{-s_{2}}}$. Then we have:
\begin{equation*}
    \frac{d(\langle c_1,x_1\rangle ^2*\langle c_2,x_1\rangle ^2)}{dt}=0\Rightarrow \langle c_1,x_1\rangle ^2*\langle c_2,x_1\rangle ^2=constant.
\end{equation*}
So $\langle c_1,x_1\rangle ^2, \langle c_2,x_2\rangle ^2$ satisfy the same dynamic system: 
\begin{equation*}
    \frac{df(t)}{dt}=2f(t)*\frac{1}{1+e^{f(t)-\frac{1}{f(t)}}}.
\end{equation*}
Under our set up, $\langle c_1,x_1\rangle ^2, \langle c_2,x_2\rangle ^2$ also share the same initial condition, then by the uniqueness and existence of ODE with a Lipshitz continuous condition, we have: $\langle c_1,x_1\rangle ^2 = \langle c_2,x_2\rangle ^2$. Similarily, we can prove that $\langle c_2,x_1\rangle^2=\langle c_1,x_2\rangle^2$. Then the explicit form of our margin can be written as:
\begin{equation*}
    \Delta\mu = \frac{a-b}{\sqrt{a^2+b^2}},
\end{equation*}
where $a=\langle c_1,x_1\rangle^2, b=\langle c_1,x_2\rangle^2$. Then computing $\frac{d\Delta\mu}{dt}$ explicitly:
\begin{equation*}
    \frac{d\Delta\mu}{dt}=\frac{4v_{12}(a+b)ab}{(a^2+b^2)\sqrt{a^2+b^2}}> 0.
\end{equation*}
Since at initialization $a=b=1\Rightarrow \lim\limits_{t\rightarrow\infty}a=+\infty, \lim\limits_{t\rightarrow\infty}b=0\Rightarrow \lim\limits_{t\rightarrow\infty}\Delta\mu=1$.

\end{proof}

\subsection{Proof of the theorem under general set up}

Here we try to understand our margin in a more general set up, which includes multi-layer Low-Rank DIQNN, general datasets and algorithms such as stochastic gradient descent. (SGD) 

Given training dataset $\{(x_n,y_n)\}_{n=1}^N$, which consists of $N$ data points. Each data point $x_n\in \mathbb{R}^d$ has a true label $y_n\in [k]$, where $k$ ($k\geq 2$) denotes the total number of classes. Given a certain input $x_n$, the network produces an output vector $\Phi(x_n,\theta)\in \mathbb{R}^{k}$, where $\theta$ represents the trainable network parameters.

\begin{definition}(Homogeneous property)
     We say a network satisfy the homogeneous property for some integer $L$ if:
     \begin{equation*}
         \Phi(x_n,\theta_t)=\|\theta_t\|_2^L\Phi(x_n,\frac{\theta_t}{\|\theta_t\|_2}).
     \end{equation*}
\end{definition}

\begin{lemma}(Homogeneous property)
 Low-Rank DIQNN satisfy the homogeneous property: $\Phi(x_n,\theta_t)=\|\theta_t\|_2^L\Phi(x_n,\frac{\theta_t}{\|\theta_t\|_2})$ for $L=2^{l+1}-2$, where $l$ is the total number of layers of Low-Rank DIQNN. Moreover, we can prove that $\langle\partial_\theta(\Phi(x_n,\theta_t))_i,\theta_t\rangle=L(\Phi(x_n,\theta_t))_i$  $\forall i\in [k]$.
\end{lemma}
\begin{proof}
    Due to the fact that Low-Rank DIQNN does not have bias term, and the nonlinearity is square, it is obvious that $\exists L$ such that $\Phi(x_n,a\theta_t)=a^L\Phi(x_n,\theta_t)$. And if the total number of layers of Low-Rank DIQNN is $l$, we can write down the relation between $l$ and $L$:
    \begin{equation*}
        L=2^{l+1}-2.
    \end{equation*}
   If we take the derivative w.r.t $a$ on both sides in the equation: $(\Phi(x_n,a\theta_t))_i=a^L(\Phi(x_n,\theta_t))_i, \forall a\in\mathbb{R}$, we have:
    \begin{equation*}
    \langle\partial_\theta(\Phi(x_n,a\theta_t))_i,\theta_t\rangle=La^{L-1}(\Phi(x_n,a\theta_t))_i.
    \end{equation*}
    If we set $a=1$ on both side, then we have proved this lemma.
\end{proof}

\begin{definition}
      $s_{nj}=(\Phi(x_n,\theta_t))_{y_n}-(\Phi(x_n,\theta_t))_{j}$; $j_n = \arg\max_{i\neq y_n} {(\Phi(x_n,\theta))_i}$; $s_{n}=s_{nj_{n}}$; $c$ is the condition number of matrix $A=\partial_\theta S\partial_\theta S^T-\frac{L^2}{\|\theta_t\|_2^2}SS^T$, where $S=(s_1,\dots,s_n)^T$, $\partial_\theta S=(\partial_\theta s_1, \dots, \partial_\theta s_n)^T$; $\mathbf{v}=(v_1,\dots,v_N)^T$, where $v_n=\frac{e^{(\Phi(x_n,\theta_t))_{j_n}}}{\sum_{i=1}^k e^{(\Phi(x_n,\theta_t))_i}}$, it should be noted that all of these values or vectors are time dependent.
\end{definition}

\begin{definition} 
     A set that consist of real numbers $\{y_1,y_2,\cdots,y_m\}$ satisfy the $\varepsilon-$separated condition if $y_j-\min_i (y_i)\geq 1/\varepsilon, \forall j
     \in [m], j\neq \arg\min_i (y_i)$.
\end{definition}

Before proving the theorem, we firstly prove two lemmas which characterize the property of Low-Rank DIQNN and gradient flow algorithm with Crossentropy loss, respectively.

\begin{lemma}(Approximate $\frac{d\theta_t}{dt}$)
 If the set $\{s_{nj}\}_{j=1,j\neq y_n}^{k}$ satisfy the $\varepsilon-$separated condition for some $\varepsilon>0$ ($\forall n\in [N]$), and $\{\|\partial_\theta s_{nj}\|_2\}_{n\in [N], j\in [k]}$ is uniformly bounded with some constant $M$. $\forall n, \forall j\in [k]$  then under gradient flow dynamics with cross-entropy loss, we have: 
    \begin{equation*}
        \|\frac{d\theta_t}{dt}-\frac{1}{N}\sum_{n=1}^N v_n\partial_\theta s_n\|_2\leq M(k-2)e^{-\frac{1}{\varepsilon}}.
    \end{equation*}
\end{lemma}

\begin{proof}
    we can define $v_{nj}=\frac{e^{(\Phi(x_n,\theta_t))_j}}{\sum_{i=1}^k e^{(\Phi(x_n,\theta_t))_i}}$, which is the $j$-th softmax output when given input $x_n$ ($v_n=v_{nj_n}$). Then the crossentropy loss can be written as: $\mathcal{L}(\theta_t)=\frac{1}{N}\sum\limits_{n=1}^N\log(1+\sum\limits_{j\neq y_n}e^{-s_{nj}})$. Then by the gradient flow dynamics ($\frac{d\theta_t}{dt}=-\nabla_\theta\mathcal{L}(\theta_t)$), we have: $\frac{d\theta_t}{dt}=\frac{1}{N}\sum\limits_{n=1}^N\sum\limits_{j\neq y_n} v_{nj}\partial_\theta s_{nj}$. Then we can know:
    \begin{equation*}
    \|\frac{d\theta_t}{dt}-\frac{1}{N}\sum_{n} v_n\partial_\theta s_n\|_2=\frac{1}{N}\|\sum\limits_{n=1}^N\sum\limits_{j\neq y_n,j_n} v_{nj}\partial_\theta s_{nj}\|_2.
    \end{equation*}
    Due to the $\varepsilon-$separated condition, we have for $\forall n, \forall j\neq y_n,j_n$: 
    \begin{equation*}
        \frac{v_n}{v_{nj}} = e^{s_{nj}-s_n}\geq e^{\frac{1}{\varepsilon}},
    \end{equation*}
    because of the fact that $v_{n}\leq 1$, we can obtain:
    \begin{equation*}
        \|\frac{d\theta_t}{dt}-\frac{1}{N}\sum_{n} v_n\partial_\theta s_n\|_2\leq M(k-2)e^{-\frac{1}{\varepsilon}}.
    \end{equation*}
\end{proof}

\begin{theorem}
  Training Low-Rank DIQNN under cross-entropy loss and gradient flow algorithm, if the following three assumptions are satisfied in a small time interval $[t-\Delta t,t+\Delta t]$ for some $\Delta t>0$:
   \begin{itemize}
       \item $\|\Phi(x_n,\theta_t)\|_2=a_n\|\theta_t\|_2^L$
       \item The set $\{s_{nj}\}_{j=1,j\neq y_n}^{k}$ satisfy the $\varepsilon-$separated condition for some $\varepsilon>0$ ($\forall n\in [N]$), and $\{\|\partial_\theta s_{nj}\|_2\}_{n\in [N], j\in [k]}$ is uniformly bounded with some constant $M$.
       \item $c-1 \leq \frac{2m}{\sqrt{1-m^2}}$, where $m=\cos(\mathbf{a},\mathbf{v})>0$, $\mathbf{a}=(1/a_1,\dots,1/a_N)^T$
   \end{itemize}
   Then at time $t$ we have:
   \begin{equation*}
    \frac{d\Delta\mu}{dt}\geq -\frac{M^2(k-2)\|\mathbf{a}\|_1}{N\|\theta_t\|_2^L}e^{-\frac{1}{\varepsilon}}.
\end{equation*}
\end{theorem}

\begin{proof}
    It is obvious that $s_{n}=(\Phi(x_n,\theta))_{y_n}-(\Phi(x_n,\theta))_{j_n}$ which satisfy $s_n(a\theta)=a^Ls_n(\theta)$, then by Lemma 1 we know $\langle \partial_\theta s_{n}, \theta_t\rangle= Ls_n$, then we can decompose $\partial_\theta s_n$ into two orthogonal directions:
    \begin{equation*}
        \partial_\theta s_n=\frac{Ls_n}{\|\theta_t\|_2^2}\theta_t+\mathbf{\alpha_n}.
    \end{equation*}
    And using the first assumption, we can take the derivative w.r.t $t$ on both sides and then have: 
    \begin{equation*}
\langle \partial_{\theta}\|\Phi(x_n,\theta_t)\|_2,\frac{d\theta_t}{dt}\rangle=\langle\frac{L\|\Phi(x_n,\theta_t)\|_2}{\|\theta_t\|_2^2}\theta_t,\frac{d\theta_t}{dt}\rangle.
    \end{equation*}
    Then we can explicitly compute $\frac{d\Delta\mu}{dt}$:
    \begin{equation}\label{1}
        \begin{aligned}
        \frac{d}{dt}(\Delta\mu)=&\langle\frac{1}{N}\sum_{n}\frac{\|\Phi(x_n,\theta_t)\|_2\partial_\theta s_{n}-s_{n}\partial_\theta(\|\Phi(x_n,\theta_t)\|_2)}{\|\Phi(x_n,\theta_t)\|_2^2},\frac{d\theta_t}{dt}\rangle\\
        =&\langle\frac{1}{N}\sum_{n}\frac{\mathbf{\alpha_{n}}}{\|\Phi(x_n,\theta_t)\|_2},\frac{d\theta_t}{dt}\rangle\\
        =&\langle\frac{1}{N\|\theta_t\|_2^L}\sum_{n}\frac{\mathbf{\alpha_{n}}}{a_n},\frac{d\theta_t}{dt}\rangle.
        \end{aligned}
    \end{equation}
    And by the decomposition of $\partial_\theta s_n$, we have:
    \begin{equation*}
        \begin{aligned}
        \langle\frac{1}{N\|\theta_t\|_2^L}\sum_{n}\frac{\mathbf{\alpha_{n}}}{a_n}, \frac{1}{N}\sum_{n} v_n\partial_\theta s_n\rangle =& \frac{1}{N^2\|\theta_t\|_2^L}\langle\sum_n\frac{\mathbf{\alpha_n}}{a_n},\sum_n v_n\mathbf{\alpha_n}\rangle\\
        =& \frac{\mathbf{a}^TA\mathbf{v}}{N^2\|\theta_t\|_2^L}
        \end{aligned}.
    \end{equation*}
    Since $\cos(\mathbf{a},\mathbf{v})=m$, we can decompose $\mathbf{a}$: $\mathbf{a}=\frac{m\|a\|_2}{\|v\|_2}\mathbf{v}+\mathbf{b}$. Then we have: 
    \begin{equation*}
        \mathbf{a}^TA\mathbf{v}=\frac{\mathbf{v}^TA\mathbf{v}\|\mathbf{b}\|_2}{\|\mathbf{v}\|_2}(\frac{m}{\sqrt{1-m^2}}+\frac{\mathbf{\hat{b}}^TA\mathbf{\hat{v}}}{\mathbf{\hat{v}}^TA\mathbf{\hat{v}}}),
    \end{equation*}
    where $\mathbf{\hat{b}},\mathbf{\hat{v}}$ are unit vectors. Then we can do spectrum decomposition to the Gram matrix $A=\partial_\theta S\partial_\theta S^T-\frac{L^2}{\|\theta_t\|_2^2}SS^T=(\mathbf{\alpha_1},\mathbf{\alpha_2,\dots,\mathbf{\alpha_N}})^T(\mathbf{\alpha_1},\mathbf{\alpha_2,\dots,\mathbf{\alpha_N}})$: $A=Q^TDQ$, where $D=diag(\lambda_1,\lambda_2,\dots,\lambda_N)$ consist of eigenvalues of $A$, $Q$ is a orthogonal matrix, and we can assume $\lambda_1\geq \lambda_2\geq\cdots\geq \lambda_N\geq 0$, then we know $\mathbf{\hat{v}}^TA\mathbf{\hat{v}}\geq \lambda_N$. And since $\langle \mathbf{\hat{b}},\mathbf{\hat{v}}\rangle=0\Rightarrow\langle Q\mathbf{\hat{b}},Q\mathbf{\hat{v}}\rangle=0$, we can have:
    \begin{equation*}
        \mathbf{\hat{b}}^TA\mathbf{\hat{v}}=(Q\mathbf{\hat{b}})^T(D-\frac{\lambda_N+\lambda_1}{2} I)(Q\mathbf{\hat{v}})\geq \frac{\lambda_N-\lambda_1}{2}.
    \end{equation*}
    Then we get:  
    $\frac{\mathbf{\hat{b}}^TA\mathbf{\hat{v}}}{\mathbf{\hat{v}}^TA\mathbf{\hat{v}}}\geq \frac{\lambda_N-\lambda_1}{2\lambda_N}=\frac{1-c}{2}$. And by the third assumption, we have:
    \begin{equation*}
        \mathbf{a}^TA\mathbf{v}=\frac{\mathbf{v}^TA\mathbf{v}\|\mathbf{b}\|_2}{\|\mathbf{v}\|_2}(\frac{m}{\sqrt{1-m^2}}+\frac{\mathbf{\hat{b}}^TA\mathbf{\hat{v}}}{\mathbf{\hat{v}}^TA\mathbf{\hat{v}}})\geq \frac{\mathbf{v}^TA\mathbf{v}\|\mathbf{b}\|_2}{\|\mathbf{v}\|_2}(\frac{m}{\sqrt{1-m^2}}+\frac{1-c}{2})\geq 0.
    \end{equation*}
    Then we can get:
    \begin{equation*}
        \frac{d}{dt}(\Delta\mu)\geq \langle\frac{1}{N\|\theta_t\|_2^L}\sum_{n}\frac{\mathbf{\alpha_{n}}}{a_n}, \frac{d\theta_t}{dt}-\frac{1}{N}\sum_{n=1}^N v_n\partial_\theta s_n\rangle,
    \end{equation*}
    where 
    \begin{equation*}
        |\langle\frac{1}{N\|\theta_t\|_2^L}\sum_{n}\frac{\mathbf{\alpha_{n}}}{a_n}, \frac{d\theta_t}{dt}-\frac{1}{N}\sum_{n=1}^N v_n\partial_\theta s_n\rangle|\leq \frac{1}{N\|\theta_t\|_2^L}\|\sum_{n}\frac{\mathbf{\alpha_{n}}}{a_n}\|_2\cdot\|\frac{d\theta_t}{dt}-\frac{1}{N}\sum_{n=1}^N v_n\partial_\theta s_n\|_2.
    \end{equation*}
    Then by lemma 2 we can obtain:
    \begin{equation*}
    \begin{aligned}
    |\langle\frac{1}{N\|\theta_t\|_2^L}\sum_{n}\frac{\mathbf{\alpha_{n}}}{a_n}, \frac{d\theta_t}{dt}-\frac{1}{N}\sum_{n=1}^N v_n\partial_\theta s_n\rangle|&\leq \frac{1}{N\|\theta_t\|_2^L}\sum_{n}\|\frac{\partial_\theta s_n}{a_n}\|_2M(k-2)e^{-\frac{1}{\varepsilon}}\\
    &\leq \frac{M^2(k-2)\|\mathbf{a}\|_1}{N\|\theta_t\|_2^L}e^{-\frac{1}{\varepsilon}}.
    \end{aligned}
    \end{equation*}
    Finally, we obtain the result in the theorem:
    \begin{equation*}
    \frac{d\Delta\mu}{dt}\geq -\frac{M^2(k-2)\|\mathbf{a}\|_1}{N\|\theta_t\|_2^L}e^{-\frac{1}{\varepsilon}}.
\end{equation*}
\end{proof}

We observe that $k=2$ for binary classification task, therefore, the above inequality becomes: $\frac{d}{dt}(\Delta\mu)\geq 0$, which is consistent with the theorem in solving XOR problem.

% Figure environment removed

Note that under the first assumption in our theorem, we obtain the following equality in our proof:
\begin{equation*}
\frac{d}{dt}(\Delta\mu)=\langle\frac{1}{N}\sum_{n}\frac{\mathbf{\alpha_{n}}}{\|\Phi(x_n,\theta_t)\|_2},\frac{d\theta_t}{dt}\rangle. 
\end{equation*} 
This equality can be verified by numerical experiments on MNIST throughout the training process of Low-Rank DIQNN (with one rank) under both GD and SGD, as shown in Figure~\ref{assumption_verification}. 

Finally, we point out that our theorem can be extended to models that satisfy the homogeneous property: 
\begin{equation*}
\exists \ L\ \ s.t.\ \ \Phi(x_n,\theta_t)=\|\theta_t\|_2^L\Phi(x_n,\frac{\theta_t}{\|\theta_t\|_2}),\ \forall a\in\mathbb{R},
\end{equation*}
e.g. DIQNN.


\medskip

%\bibliography{ref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

% Figure environment removed

% Figure environment removed