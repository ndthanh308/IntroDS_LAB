\section{Experiments}
\subsection{Pre-training}

We use \texttt{vox2vec} to pre-train both FPN and UNet models (further \texttt{vox2vec}-FPN and \texttt{vox2vec}-UNet) in order to ablate the effect of using a feature pyramid instead of single full-resolution feature map for modeling voxel-wise representations. For pre-training, we use $6$ public CT datasets~\cite{amos,flare,nlst,nsclc,lidc,midrc}, totaling more than $6550$ CTs, covering abdomen and thorax domains. We do not use the annotations for these datasets during the pre-training stage. Pre-processing includes the following steps: 1) cropping to the minimal volume containing all the voxels with the intensity greater than $-500$ HU; 2) interpolation to the voxel spacing of $1\times 1 \times 2$~mm$^3$ (intensities are clipped and rescaled at the augmentation step, see Section~\ref{subseq:method:sampling}). We pre-train both models for $100$K batches using the Adam optimizer \cite{adam} with a learning rate of $0.0003$. Both models are trained on a single A100-40Gb GPU for an average of $3$ days. Further details about the pre-training setup can be found in Supplementary materials.

\subsection{Evaluation}

We evaluate our method on the Beyond the Cranial Vault Abdomen (BTCV)~\cite{btcv} and Medical Segmentation Decathlon (MSD)~\cite{msd} datasets. The BTCV dataset consists of $30$ CT scans along with $13$ different organ annotations. We test our method on $6$ CT MSD datasets, which include $9$ different organ and tumor segmentation tasks. A $5$ fold cross-validation is used for BTCV experiments, and a $3$ fold cross-validation for MSD experiments. The segmentation performance of each model on BTCV and MSD datasets is evaluated by the Dice score.

For our method, the pre-processing steps are the same for all datasets, as at the pre-training stage, but in addition, intensities are clipped to $(-1350, 1000)$ HU window and rescaled to $(0, 1)$.

We compare our results with the current state-of-the-art self-supervised methods~\cite{swin_unetr,transvw} in medical imaging. The pre-trained weights for the SwinUNETR encoder and TransVW UNet are taken from the official repositories of corresponding papers. In these experiments, we keep the crucial pipeline hyperparameters (e.g., spacing, clipping window, patch size) the same as in the original works. To evaluate the pre-trained SwinUNETR and TransVW in linear and non-linear probing setups, we use similar linear and non-linear head architectures as for \texttt{vox2vec}-FPN (see Section~\ref{subseq:method:evaluation}). SwinUNETR and TransVW cost $391$ GFLOPs and $1.2$ TFLOPS, correspondingly, compared to $115$ GFLOPs of \texttt{vox2vec}-FPN.

We train all models for $45000$ batches of size $7$ (batch size for SwinUNETR is set to $3$ due to memory constraints), using the Adam optimizer with a learning rate of $0.0003$. In the fine-tuning setup, we freeze the backbone for the first $15000$ batches and then exponentially increase the learning rate for the backbone parameters from $0.00003$ up to $0.0003$ during $1200$ batches. 