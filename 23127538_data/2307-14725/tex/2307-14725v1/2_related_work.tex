\section{Related work}

In recent years, self-supervised learning in computer vision has evolved from simple pretext tasks like Jigsaw Puzzles \cite{jigsaw}, Rotation Prediction \cite{rotation}, and Patch Position Prediction \cite{context_patch} to the current SotA methods such as restorative autoencoders \cite{mae} and contrastive \cite{simclr} or non-contrastive \cite{simsiam} joint embedding methods.

Several methods produce dense or pixel-wise vector representations~\cite{vader,propagate_yourself,vicregl} to pre-train models for downstream tasks like segmentation or object detection. In \cite{vader}, pixel-wise representations are learned by forcing local features to remain constant over different viewing conditions. This means that matching regions describing the same location of the scene on different views should be positive pairs, while non-matching regions should be negative pairs. In \cite{propagate_yourself}, authors define positive and negative pairs as spatially close and distant pixels, respectively. While in \cite{vicregl}, authors minimize the mean square distance between matched pixel embeddings, simultaneously preserving the embedding variance along the batch and decorrelating different embedding vector components.

The methods initially proposed for natural images are often used to pre-train models on medical images. In \cite{3d_ssl}, authors propose the 3D adaptation of Jigsaw Puzzle, Rotation Prediction, Patch Position Prediction, and image-level contrastive learning. Another common way for pre-training on medical images is to combine different approaches such as rotation prediction \cite{swin_unetr}, restorative autoencoders \cite{transvw,swin_unetr}, and image-level contrastive learning \cite{transvw,swin_unetr}.

Several methods allows to obtain voxel-wise features. The model \cite{sam} maximizes the consistency of local features in the intersection between two differently augmented images. The algorithm \cite{sam} was mainly proposed for image retrieval and uses only feature representations in the largest and smallest scales in separate contrastive losses, while \texttt{vox2vec} produce voxels' representations via concatenation of feature vectors from a feature pyramid and pre-train them in a unified manner using a single contrastive loss. Finally, a number of works propose semi-supervised contrastive learning methods \cite{semi1}, however, they require additional task-specific manual labeling.
