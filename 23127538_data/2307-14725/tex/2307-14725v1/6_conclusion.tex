
\section{Conclusion}

In this work, we present \texttt{vox2vec} --- a self-supervised framework for voxel-wise representation learning in medical imaging. Our method expands the contrastive learning setup to the feature pyramid architecture allowing to pre-train effective representations in full resolution. By pre-training a FPN backbone to extract informative representations from unlabeled data, our method scales to large datasets across multiple task domains. We pre-train a FPN architecture on more than 6500 CT images and test it on various segmentation tasks, including different organs and tumors segmentation in three setups: linear probing, non-linear probing, and fine-tuning. Our model outperformed existing methods in all regimes. Moreover, \texttt{vox2vec} establishes a new state-of-the-art result on the linear and non-linear probing scenarios. 

Still, this work has a few limitations to consider. We plan to investigate further how the performance of \texttt{vox2vec} scales with the increasing size of the pre-training dataset and the pre-trained architecture size. Another interesting research direction is exploring the effectiveness of \texttt{vox2vec} in the domain adaptation and few-shot learning scenarios.
