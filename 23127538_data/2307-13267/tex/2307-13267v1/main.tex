%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools,cuted}
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage[inkscapeversion=1.2.1]{svg}
\svgsetup{inkscapeexe=C:/Inkscape/bin/inkscape}
\usepackage{units}
\newcommand{\highlight}[1]{\textit{#1}}
\usepackage{import}
\usepackage{xspace}
\usepackage{pst-node,pst-plot}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{xtab,booktabs}
\usepackage{longtable}
\usepackage{multicol}
\usepackage{dsfont}
\usepackage{bm}
\newcommand{\onevec}{\mathbf{1}}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\setlength{\parindent}{0pt}
\allowdisplaybreaks
\newcommand{\st}{\text{s.\,t. }}
\usepackage{hyperref}


\renewcommand{\vec}[1]{\mathbf{#1}} % Vectors
\newcommand{\gvec}[1]{\bm{#1}} % Greek vectors
\newcommand{\vbody}[2]{{\mathbb{#1}^{n_{#2}}}} % Vector body
\newcommand{\mbody}[3]{{\mathbb{#1}^{n_{#2}	\times n_{#3}}}} % Matrix body






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed
The goal of $ K $-means clustering is to assign a set of observations $ \vec{y}_j \in\vbody{R}{\vec{y}},\;j \in \mathcal{J} = \{1,\dots,\vert\mathcal{J}\vert\} $ to a set of clusters $ \mathcal{K} = \{1,\dots,K\} $ and to compute the centroids of each cluster. The number of clusters is a hyper-parameter and is set a priori or in an iterative manner. This problem can be formulated as a mixed-integer nonlinear programming (MINLP) problem \cite{Aloise2012,Gambella2021},
\begin{subequations}\label{Clustering_MINLP}
	\begin{align}
		\underset{w_{jk},\vec{m}_k}{\min}&\sum_{j\in\mathcal{J}}\sum_{k\in\mathcal{K}} w_{jk}\cdot \Vert \vec{y}_j - \vec{m}_k \Vert_2^2,\\
		\st &\sum_{k\in\mathcal{K}}w_{jk} = 1,\forall j\in\mathcal{J},\\
		&w_{jk} \in \{0,1\},\;\forall j\in\mathcal{J},k\in\mathcal{K},\;\vec{m}_k\in\vbody{R}{\vec{y}}\;\forall k\in\mathcal{K}.
	\end{align}
\end{subequations}
The binary variables $ w_{jk} $ indicate if observation $ \vec{y}_j $ is assigned to cluster $ k $ and $ \vec{m}_k $ is the centroid of cluster $ k $. Constraint (\ref{Clustering_MINLP}b) enforces that each observation is assigned to exactly one cluster, while the objective is to minimize the sum of the squared Euclidean distances of all observations to the centroids of their assigned clusters.
Problem \eqref{Clustering_MINLP} is a nonconvex MINLP which is hard to solve. In practice it is more efficient to use a linearized formulation by introducing the variable $ d_{jk} $, which describes the squared distance between an observation $ j $ and the centroid of cluster $ k $ \cite{Gambella2021},
\pagebreak
\begin{subequations}\label{Clustering_MILP}
	\begin{align}
		\underset{w_{jk},d_{jk},\vec{m}_k}{\min}&\sum_{j\in\mathcal{J}}\sum_{k\in\mathcal{K}}d_{jk}\\
		\st &\sum_{k\in\mathcal{K}}w_{jk} = 1,\forall j\in\mathcal{J},\\
        \nonumber
		&d_{jk} \ge \Vert \vec{y}_j - \vec{m}_k \Vert_2^2 - M_j\cdot(1-w_{jk}),\\
        &\forall j\in\mathcal{J},k\in\mathcal{K}\\
        \nonumber
		&w_{jk} \in \{0,1\}, d_{jk}\ge0,\\
        &\forall j\in\mathcal{J},k\in\mathcal{K},\;\vec{m}_k\in\vbody{R}{\vec{y}}\;\forall k\in\mathcal{K}.
	\end{align}
\end{subequations}
\eqref{Clustering_MILP} is a mixed-integer quadratically constrained programming (MIQCP) problem with a convex integer relaxation. Constraint (\ref{Clustering_MILP}c) is an epigraph formulation of the squared Euclidean distance if observation $ j $ is assigned to cluster $ k $, i.e., when $ w_{jk} = 1 $. Otherwise, the parameter $ M_j $ has to be large enough so that the constraint is trivially satisfied for $ w_{jk} = 0 $. In theory, a common big-M parameter can be used for all constraints described by (\ref{Clustering_MILP}c). However, the parameter should be chosen as small as possible to avoid weak integer relaxations. In the following, the big-M parameter is set as
\begin{subequations}\label{big_M}
	\begin{alignat}{2}
    M_j = &\underset{\gvec{\chi} \in \mathcal{Y}}{\max}\;\Vert \vec{y}_j - \gvec{\chi}\Vert_2^2,\; \forall j \in \mathcal{J},\\
		\mathcal{Y} = &\{\vec{y}\in \vbody{R}{\vec{y}} \vert\; \underset{j \in \mathcal{J}}{\min}\; [\vec{y}_j]_l \le [\vec{y}]_l \le  \underset{j \in \mathcal{J}}{\max}\; [\vec{y}_j]_l\\
        &l=1.\dots,n_{\vec{y}}\}.
	\end{alignat}
\end{subequations}

Different approaches have been proposed to solve the clustering optimization problem. Bagirov and Yearwood present a heuristic method based on nonsmooth optimization \cite{Bagirov2006}, Aloise et al. propose a column generation algorithm \cite{Aloise2012} and Karmitsa et al. use a diagonal bundle method \cite{Karmitsa2017}. Fig.~\ref{fig:clustered_data_0} illustrates the concept of $ K $-means clustering. The unlabeled data (left) is split into 3 clusters according to their distance to the computed cluster centroid (crosses).
\subsection{Distributed consensus formulation}
Problem \eqref{Clustering_MILP} describes the case in which the entire data set is accessible from a single node. However, this might not always be the case, especially if the underlying data is confidential. In the following it is assumed that the data set is split across several nodes $ \mathcal{I} = \{1,\dots,N_s\} $, with each node having access to the data-subset $ \mathcal{J}_i \subset \mathcal{J} $. The MIQCP problem \eqref{Clustering_MILP} can be extended to the case of multiple nodes,
\begin{subequations}\label{Clustering_MILP_Central}
	\begin{align}
		\underset{w_{ijk},d_{ijk},\vec{m}_{k}}{\min}&\sum_{i\in\mathcal{I}}\sum_{j\in\mathcal{J}_i}\sum_{k\in\mathcal{K}}d_{ijk}\\
		\st &\sum_{k\in\mathcal{K}}w_{ijk} = 1,\forall i \in\mathcal{I}, j\in\mathcal{J}_i,\\
		\nonumber
        &d_{ijk} \ge \Vert \vec{y}_j - \vec{m}_{k} \Vert_2^2 - M_j\cdot(1-w_{ijk}),\\
        &\forall i \in\mathcal{I}, j\in\mathcal{J}_i,k\in\mathcal{K},\\
	       \nonumber	
        &w_{ijk} \in \{0,1\}, d_{ijk}\ge0,\;\forall i \in \mathcal{I}, j\in\mathcal{J}_i,k\in\mathcal{K},\\
        &\vec{m}_{k}\in\vbody{R}{\vec{y}},\;\forall k\in\mathcal{K}.
	\end{align}
\end{subequations}
The goal of problem \eqref{Clustering_MILP_Central} is again to compute a set of cluster centroids $ \vec{m}_k $ and to assign the observations of all nodes to these clusters. However, if the nodes cannot share their data, problem \eqref{Clustering_MILP_Central} cannot be solved in a centralized manner. A simple distributed approach would be to solve a clustering problem in each node $ i $. This could lead to a situation as depicted in Fig.~\ref{fig:clustered_data_1} and Fig.~\ref{fig:clustered_data_2}. If each the data set is split across two nodes, each one can solve a clustering problem. However, both nodes will compute different cluster centroids.\\
The goal of a federated learning approach is to train a global model, i.e., global cluster centroids in the case of $ K $-means clustering, without sharing the local data between the nodes. To this end each node $ i $ can compute individual cluster centroids $ \vec{m}_{ik} $,
\begin{subequations}\label{Clustering_MILP_Consenus}
	\begin{align}
		\underset{w_{ijk},d_{ijk},\vec{m}_{ik}}{\min}&\sum_{i\in\mathcal{I}}\sum_{j\in\mathcal{J}_i}\sum_{k\in\mathcal{K}}d_{ijk}\\
		\st &\sum_{k\in\mathcal{K}}w_{ijk} = 1,\forall i \in\mathcal{I}, j\in\mathcal{J}_i,\\
        \nonumber
		&d_{ijk} \ge \Vert \vec{y}_j - \vec{m}_{ik} \Vert_2^2 - M_j\cdot(1-w_{ijk}),\\
        &\forall i \in\mathcal{I}, j\in\mathcal{J}_i,k\in\mathcal{K},\\
		&\vec{m}_{ik} = \vec{m}_{i'k},\;\forall i \in \mathcal{I}, i' \in \mathcal{N}_i, k \in \mathcal{K},\\
        \nonumber
		&w_{ijk} \in \{0,1\}, d_{ijk}\ge0,\\
        &\forall i \in \mathcal{I}, j\in\mathcal{J}_i,k\in\mathcal{K},\\
        &\vec{m}_{ik}\in\vbody{R}{\vec{y}}\;\forall, i \in \mathcal{I}, k\in\mathcal{K}.
	\end{align}
\end{subequations}
Since the goal is to obtain global cluster centroids, the individual cluster centroids are coupled through consensus constraints (\ref{Clustering_MILP_Consenus}d), where $ \mathcal{N}_i $ contains the set of neighboring nodes of node $ i $. Problem \eqref{Clustering_MILP_Consenus} describes a set of $ N_s $ subproblems coupled through the consensus constraints. In the following subsection dual variables are used to decouple the clustering problems of the different nodes.
\section{Dual decomposition-based distributed clustering}
This section presents how the consensus formulation (\ref{Clustering_MILP_Consenus}) of the clustering problem can be decomposed by introducing dual variables. Dual decomposition can be applied to constraint-coupled optimization problems of the form
\begin{subequations}\label{Constraint_Coupled}
	\begin{align}
		\underset{\vec{x}}{\min}\;&\sum_{i\in\mathcal{I}}f_i(\vec{x}_i),\\
		\st & \sum_{i\in\mathcal{I}}\vec{A}_i\vec{x}_i = \vec{b},\\
		&\vec{x}_i \in \mathcal{X}.
	\end{align}
\end{subequations}
Equation (\ref{Constraint_Coupled}) describes an optimization problem consisting of a set of $ {\mathcal{I}=\{1,\dots,N_s\}} $ subproblems. The subproblems are coupled through the constraints (\ref{Constraint_Coupled}b) and each one is described by individual variables $ \vec{x}_i $ and constraints $ \mathcal{X}_i $. Dual decomposition is based on the introduction of dual variables for the coupling constraints (\ref{Constraint_Coupled}b) and the solution of the resulting dual optimization problem. The idea was first introduced by Everett \cite{Everett.1963} for problems involving shared limited resources. Problem \eqref{Clustering_MILP_Consenus} can also be rewritten as a general constraint-coupled optimization problem by defining the matrix $ \vec{A} $ describing the connections between the different nodes. In the following only linear network topologies as depicted in Fig.~\ref{fig:cluster_network} are considered. Note that the discussion in the remainder of this paper can be easily extended to different network topologies.\\
% Figure environment removed

By defining the vector of stacked cluster centroids of each node $ i $,
\begin{equation}\label{Stacked_Centroids}
	\hat{\vec{m}}_{i} \coloneqq \begin{bmatrix}
		\vec{m}_{i,1}\\
		\vdots\\
		\vec{m}_{i,k}
	\end{bmatrix} \in \mathbb{R}^{K\cdot n_{\vec{y}}},
\end{equation}
the consensus constraints can be rewritten as
\begin{subequations}\label{Consensus_Alg}
	\begin{align}
		&\hat{\vec{m}}_{1} - \hat{\vec{m}}_{2} = \vec{0},\\
		&\hat{\vec{m}}_{2} - \hat{\vec{m}}_{3} = \vec{0},\\
		\nonumber										  &\vdots\\								&\hat{\vec{m}}_{N_s -1} - \hat{\vec{m}}_{N_s} = \vec{0}.			  
	\end{align}
\end{subequations}
Constraints \eqref{Consensus_Alg} can subsequently be rewritten in matrix form
\begin{subequations}\label{Consensus_Matrix}
	\begin{align}
		\underbrace{\begin{bmatrix}
				\vec{I} & -\vec{I} & \vec{0} & \cdots & \vec{0} & \vec{0} \\
				\vec{0} & \vec{I} & -\vec{I} & \cdots & \vec{0} & \vec{0} \\
				\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
				\vec{0} & \vec{0} & \vec{0} & \cdots & \vec{I} & -\vec{I}
		\end{bmatrix}}_{=:\vec{A} \in \mathbb{R}^{K\cdot n_{\vec{y}}\cdot(N_s-1) \times K \cdot n_{\vec{y}}\cdot N_s  } } \cdot \begin{bmatrix}
			\hat{\vec{m}}_1 \\
			\hat{\vec{m}}_2 \\
			\hat{\vec{m}}_3 \\
			\vdots \\
			\hat{\vec{m}}_{N_s}
		\end{bmatrix} = \vec{0},
	\end{align}
\end{subequations}
or in a more compact way
\begin{equation}\label{Consensus_Matrix_Compact}
	\sum_{i\in\mathcal{I}}\vec{A}_i\hat{\vec{m}}_{i} = \vec{0}
\end{equation}
with $ \vec{A}_i \in  \mathbb{R}^{K\cdot n_{\vec{y}}\cdot(N_s-1) \times K\cdot n_{\vec{y}} }$. By introducing dual variables $ \gvec{\lambda} \in \vbody{R}{K\cdot n_{\vec{y}}\cdot(N_s-1)} $  for the consensus constraints \eqref{Consensus_Matrix_Compact} the Lagrange function of problem (\ref{Clustering_MILP_Consenus}) can be defined,
\begin{equation}\label{Clustering_Lagrange}
	\mathcal{L}(w_{ijk},d_{ijk},\vec{m}_{ik},\gvec{\lambda}) =  \sum_{i\in\mathcal{I}}\underbrace{\left(\sum_{j\in\mathcal{J}_i}\sum_{k\in\mathcal{K}}d_{ijk} + \gvec{\lambda}^T \vec{A}_i\hat{\vec{m}}_{i}\right)}_{=:\mathcal{L}_i(w_{ijk},d_{ijk},\vec{m}_{ik},\gvec{\lambda})}.
\end{equation}
The minimization of the Lagrange function for a fixed value of the dual variables $ \gvec{\lambda} $ gives the corresponding value of the dual function.
	\begin{align}\label{Clustering_Dual}
		d(\gvec{\lambda}) \coloneqq 	\underset{w_{ijk},d_{ijk},\vec{m}_{ik}}{\min}&\sum_{i\in\mathcal{I}}\mathcal{L}_i(w_{ijk},d_{ijk},\vec{m}_{ik},\gvec{\lambda})\\
  \nonumber
		\st & \text{(\ref{Clustering_MILP_Consenus}b), (\ref{Clustering_MILP_Consenus}c), (\ref{Clustering_MILP_Consenus}e), (\ref{Clustering_MILP_Consenus}f)}
	\end{align}
The dual function has two important properties. First, the value of the dual function is always a lower bound on the solution of its corresponding primal problem, in this case, problem (\ref{Clustering_MILP_Consenus}) \cite{Nocedal2006}. The problem of finding the dual variables that result in the best lower bound is referred to as the dual optimization problem,
\begin{equation}\label{Dual_Problem}
	\underset{\gvec{\lambda}}{\max}\;d(\gvec{\lambda}).
\end{equation}
The resulting dual problem can be solved in a distributed manner by solving the individual clustering problems for the current value of the dual variables,
\begin{subequations}\label{Individual_Clustering_Problem}
	\begin{align}
		\underset{w_{ijk},d_{ijk},\vec{m}_{ik}}{\min}&\mathcal{L}_i(w_{ijk},d_{ijk},\vec{m}_{ik},\gvec{\lambda})\\
		\st &\sum_{k\in\mathcal{K}}w_{ijk} = 1,\;\forall j\in\mathcal{J}_i,\\
        \nonumber
		&d_{ijk} \ge \Vert \vec{y}_j - \vec{m}_{ik} \Vert_2^2 - M_j\cdot(1-w_{ijk}),\\
        &\forall j\in\mathcal{J}_i,k\in\mathcal{K},\\
		\nonumber
		&w_{ijk} \in \{0,1\}, d_{ijk}\ge0,\;\forall  j\in\mathcal{J}_i,k\in\mathcal{K},\\
		&\vec{m}_{ik}\in\vbody{R}{\vec{y}}\;\forall k\in\mathcal{K}.
	\end{align}
\end{subequations}
Second, the dual function (\ref{Clustering_Dual}) is always concave, regardless of whether the primal problem is convex or not \cite{Nocedal2006}. Therefore the dual problem (\ref{Dual_Problem}) is a convex optimization problem. However, the dual function is usually nondifferentiable due to a changing set of active individual constraints, which means that problem (\ref{Dual_Problem}) is a nonsmooth optimization problem \cite{Yfantis2023EURO}. The following subsections present some algorithms for the solution of the dual problem, namely the subgradient method, the bundle trust method, and the quasi-Newton dual ascent algorithm.
\subsection{Subgradient method}
Since the dual function is nondifferentiable a gradient cannot be defined for every value of the dual variables. Instead, a subgradient can be used. A vector $ \gvec{\xi}\in\vbody{R}{\gvec{\chi}} $ is a subgradient of a concave function $ \phi(\gvec{\chi}) $ at a point $ \gvec{\chi}_0 $ if
\begin{equation}\label{Subgradient_Condition}
	\phi(\gvec{\chi}) \le \phi(\gvec{\chi}_0) + \gvec{\xi}^T(\gvec{\chi}-\gvec{\chi}) 
\end{equation}
 for all $ \gvec{\chi}\in \text{dom}\;\phi $. The set of all subgradients at a point $ \gvec{\chi}_0 $ comprise the subdifferential $ \partial \phi(\gvec{\chi}_0) $ Technically equation (\ref{Subgradient_Condition}) defines a supergradient. Nevertheless, the term subgradient is commonly used in the literature for both convex and concave functions.\\
 A subgradient of the dual function for a given value of the dual variables $ \gvec{\lambda}^{(t)} $ can be computed by evaluating the coupling constraints (\ref{Consensus_Matrix_Compact}),
 \begin{equation}\label{Subgradient_Consensus}
 	\vec{g}(\gvec{\lambda}^{(t)}) = \sum_{i\in\mathcal{I}}\vec{A}_i\hat{\vec{m}}_{i}(\gvec{\lambda}^{(t)}) \in \partial d(\gvec{\lambda}^{(t)}),
 \end{equation}
where $ \hat{\vec{m}}_{i}(\gvec{\lambda}^{(t)}) $ are the cluster centroids obtained by solving the individual clustering problems (\ref{Individual_Clustering_Problem}).\\
In the subgradient method the dual variables are updated in each iteration $ t $ along the direction of the subgradient \cite{Shor2012minimization}
\begin{equation}\label{Subgradient_Update}
	\gvec{\lambda}^{(t+1)} = \gvec{\lambda}^{(t)} + \alpha^{(t)}\vec{g}(\gvec{\lambda}^{(t)}),
\end{equation} 
where $ \alpha^{(t)} $ is a step size parameter. The step size parameter plays an important role in the convergence of the algorithm. If it is chosen too large the algorithm might diverge, while a too small choice might significantly slow down its convergence. A common choice to adapt the step size throughout the iterations is
\begin{equation}\label{Step_Size}
	\alpha^{(t)} = \alpha^{(0)}/\sqrt{t},
\end{equation}
with an initial step size $ \alpha^{(0)} $ \cite{Bertsekas.1999}.
\subsection{Bundle trust method}
The subgradient method usually exhibits a slow rate of convergence, since only using information from the current subgradient may not provide an ascent direction for the algorithm. Bundle methods are generally more efficient by utilizing multiple subgradients from previous iterations \cite{Makela.2002}. To this end the data
\begin{multline}\label{Bundle}
	\mathcal{B}^{(t)} = \{(\gvec{\lambda}^{(l)}, \vec{g}(\gvec{\lambda}^{(l)}), d(\gvec{\lambda}^{(l)})) \in \vbody{R}{\gvec{\lambda}}\times \vbody{R}{\gvec{\lambda}} \times \mathbb{R}\\
    \vert\;l=t-\tau+1,\dots, t	\}
\end{multline}
is stored in each iteration, where $ n_{\gvec{\lambda}} $ denotes the number of dual variables. $ \mathcal{B}^{(t)} $ is referred to as a bundle and it contains the dual variables, subgradients, and values of the dual function from previous iterations. Since storing all information from all previous iterations might cause memory issues, only data from the previous $ \tau $ iterations is used.\\
The idea of bundle methods is to use the collected information to construct a piece-wise linear over-approximation of the nonsmooth dual function $ d(\gvec{\lambda}) $, a so-called cutting plane model,
\begin{equation}\label{CPM}
	\hat{d}^{(t)}(\gvec{\lambda}) \coloneqq \underset{l\in \{t-\tau+1,\dots, t\}}{\min}\{d(\gvec{\lambda}^{(l)}) + \vec{g}^T(\gvec{\lambda}^{(l)})(\gvec{\lambda} - \gvec{\lambda}^{(l)})\}.
\end{equation}
The approximation can be written in an equivalent form as
\begin{equation}\label{CPM_LinError}
	\hat{d}^{(t)}(\gvec{\lambda}) = \underset{l\in \{t-\tau+1,\dots, t\}}{\min}\{d(\gvec{\lambda}^{(t)}) + \vec{g}^T(\gvec{\lambda}^{(l)})(\gvec{\lambda} - \gvec{\lambda}^{(t)}) - \beta^{(l,t)}\},
\end{equation}
with the linearization error
\begin{multline}\label{LinError}
	\beta^{(l,t)} = d(\gvec{\lambda}^{(t)}) - d(\gvec{\lambda}^{(l)})- \vec{g}^T(\gvec{\lambda}^{(l)})(\gvec{\lambda}^{(t)} - \gvec{\lambda}^{(l)}),\\
 \forall l\in \{t-\tau+1,\dots, t\}.
\end{multline}
The update direction of the dual variables can then be computed by solving a direction-finding problem
\begin{subequations}\label{NonsmoothDirectipnFinding}
	\begin{align}
		\underset{\vec{s
			}\in \vbody{R}{\gvec{\lambda}}}{\max}\;&\hat{d}^{(t)}(\gvec{\lambda}^{(t)} + \vec{s}),\\
		\text{s.\ t. } & \Vert \vec{s} \Vert_2^2 \le \alpha^{(t)},\\
	\end{align}
\end{subequations}
where constraint (\ref{NonsmoothDirectipnFinding}b) represents a trust region. Therefore, this variant of the bundle method is referred to as the bundle trust method (BTM). Other variants include proximal bundle methods, where the trust region is replaced by a regularization term in the objective function \cite{Bagirov2014}. Problem (\ref{NonsmoothDirectipnFinding}) is still a nonsmooth optimization problem and can be transformed into a smooth quadratic direction finding problem by using an epigraph formulation,\pagebreak
\begin{subequations}\label{SmoothDirectipnFinding}
	\begin{align}
		\underset{v\in\mathbb{R},\;\vec{s} \in \vbody{R}{\gvec{\lambda}}}{\max}\;&v,\\
		\text{s.\ t. } & \Vert \vec{s} \Vert_2^2 \le \alpha^{(t)},\\
		&\vec{g}^T(\gvec{\lambda}^{(l)})\vec{s} - \beta^{(l,t)} \ge v,\;\forall l\in \{t-\tau+1,\dots, t\}.
	\end{align}
\end{subequations}
After computing a direction the dual variables are updated according to
\begin{equation}\label{BTM_Update}
	\gvec{\lambda}^{(t+1)} = \gvec{\lambda}^{(t)} + \vec{s}^{(t)}.
\end{equation}
Bundle methods are widely used in machine learning, as nonsmoothness is encountered in many training problems involving regularization terms \cite{Le2007}. Bundle methods can also be used to solve the clustering problem (\ref{Clustering_MILP}) \cite{Karmitsa2017}. However, note that in this paper the BTM algorithm is used to solve the nonsmooth dual problem (\ref{Dual_Problem}).
\subsection{Quasi-Newton dual ascent}
Since the dual function is always concave it can be locally approximated by a quadratic function. Yfantis et al.  recently proposed the quasi-Newton dual ascent (QNDA) algorithm that approximates the dual function by a quadratic function \cite{Yfantis2022, Yfantis2023EURO},
\begin{multline}\label{QNDA_Dual}
	d_B^{(t)}(\gvec{\lambda})=\frac{1}{2}(\gvec{\lambda}-\gvec{\lambda}^{(t)})^T \vec{B}^{(t)}(\gvec{\lambda}-\gvec{\lambda}^{(t)}) \\+\vec{g}^T(\gvec{\lambda}^{(t)})(\gvec{\lambda}-\gvec{\lambda}^{(t)}) + d(\gvec{\lambda}^{(t)}).
\end{multline}
This follows the idea of Newton methods, where the gradient and Hessian of the function are used within the approximation. However, due to the nonsmoothness of the dual function, the gradient and Hessian are not defined for each value of the dual variable. Instead, the gradient is replaced in eq. (\ref{QNDA_Dual}) by the subgradient and the Hessian is approximated by the matrix $ \vec{B}^{(t)} $. The approximated Hessian can be updated in each iteration using a Broyden-Fletcher-Goldfarb-Shanno (BFGS) update,
\begin{equation}\label{BFGSUpdate}
	\vec{B}^{(t)} = \vec{B}^{(t-1)} + \frac{\vec{y}^{(t)}\vec{y}^{(t),T}}{\vec{y}^{(t),T}\vec{s}^{(t)}} - \frac{\vec{B}^{(t-1)}\vec{s}^{(t)}\vec{s}^{(t),T}\vec{B}^{(t-1),T}}{\vec{s}^{(t),T}\vec{B}^{(t-1)}\vec{s}^{(t)}},
\end{equation}
where
\begin{equation}\label{Stepsize}
	\vec{s}^{(t)} \coloneqq \gvec{\lambda}^{(t)} - \gvec{\lambda}^{(t-1)}
\end{equation}
is the variation of the dual variables and 
\begin{equation}\label{SubgradientDifference}
	\vec{y}^{(t)} = \vec{g}(\gvec{\lambda}^{(t)}) - \vec{g}(\gvec{\lambda}^{(t-1)})
\end{equation}
is the variation of the subgradients.\\
The approximated dual function $ d_B(\gvec{\lambda}) $ is differentiable, while the actual dual function is nonsmooth. This can lead to significant approximation errors and poor update directions. This issue can be addressed by utilizing the same information as in the BTM algorithm. However, instead of using the bundle to construct an over-approximator of the dual function, it is used to further constrain the update of the dual variables,
\begin{multline}\label{Bundle_Cut}
	d_B^{(t)}(\gvec{\lambda}^{(t+1)}) \le d(\gvec{\lambda}^{(l)}) + \vec{g}^T(\gvec{\lambda}^{(l)}) (\gvec{\lambda}^{(t+1)} - \gvec{\lambda}^{(l)}),\\\forall l \in \{t-\tau+1,\dots,t\}.
\end{multline}

Constraints (\ref{Bundle_Cut}) are derived from the definition of the subgradient (\ref{Subgradient_Condition}). A violation of these constraints would indicate that the updated dual variables $ \gvec{\lambda}^{(t+1)} $ are outside the range of validity of the approximated dual function. These constraints are referred to as bundle cuts and they can be summarized as
\begin{multline}\label{BC_Summary}
		\mathcal{BC}^{(t)} = \{\gvec{\lambda}\in\vbody{R}{\vec{b}}\vert\;d_B^{(t)}(\gvec{\lambda}) \le d(\gvec{\lambda}^{(l)}) + \vec{g}^T(\gvec{\lambda}^{(l)}) (\gvec{\lambda} - \gvec{\lambda}^{(l)}),\\	\forall l \in \{t-\tau+1,\dots,t\}\}.
\end{multline}
In the QNDA algorithm, the dual variables are updated in each iteration by solving the optimization problem
	\begin{subequations}\label{QNDA_Update}
		\begin{align}
			\gvec{\lambda}^{(t+1)} = \text{arg}\underset{\gvec{\lambda}}{\max}\;&d_B^{(t)}(\gvec{\lambda}),\\
			\text{s.\,t. }&\Vert \gvec{\lambda} - \gvec{\lambda}^{(t)}\Vert_2^2 \le \alpha^{(t)},\\
			&\gvec{\lambda} \in \mathcal{BC}^{(t)}.
		\end{align}
	\end{subequations}
To avoid too aggressive update steps the same trust region (\ref{QNDA_Update}b) as in the BTM algorithm is used.
\subsection{Primal heuristics}
The following sections provide some additional heuristics related to the primal optimization problem (\ref{Clustering_MILP_Consenus}), namely an averaging heuristic used to obtain feasible primal solutions, and the addition of symmetry-breaking constraints to the clustering problem.
\subsubsection{Averaging heuristic}\label{Sec:Avg_Heuristic}
The $ K $-means clustering problem involves integrality constraints and is therefore nonconvex. While the (optimal) value of the dual function \eqref{Clustering_Dual} provides a lower bound on the optimal value of the primal problem \eqref{Clustering_MILP_Consenus}, the feasibility of the primal problem is not guaranteed upon the convergence of a dual decomposition-based algorithm, i.e., the consensus constraints may not be satisfied. Nevertheless, in the case of $ K $-means clustering it is straightforward to compute a feasible primal solution using an averaging step. In each iteration $ t $ of a dual decomposition-based algorithm the coordinator communicates the dual variables $ \gvec{\lambda}^{(t)} $ to the nodes. The nodes in turn solve their clustering problems and communicate their computed cluster centroids $ \hat{\vec{m}}_i(\gvec{\lambda}^{(t)}) $ to the coordinator. Based on this response the coordinator can compute the average of the primal variables, i.e., the average cluster centroids,
\begin{equation}\label{Avg_Centroids}
	\overline{\vec{m}}_k(\gvec{\lambda}^{(t)}) = \frac{1}{N_s} \sum_{i\in\mathcal{I}} \vec{m}_{ik}(\gvec{\lambda}^{(t)})
\end{equation}
which are then communicated back to the nodes. Using the mean cluster centroids the nodes can compute their resulting primal objective value
\begin{equation}\label{Node_Objective}
	z_i(\gvec{\lambda}^{(t)}) = \sum_{j\in\mathcal{J}} \underset{k\in\mathcal{K}}{\min}\Vert \vec{y}_j - \overline{\vec{m}}_k(\gvec{\lambda}^{(t)}) \Vert_2^2.
\end{equation}
The primal objective value can be used to compute the relative duality gap in each iteration,
\begin{equation}\label{Clustering_DG}
	\text{rel. DG} = 100\cdot\left(1-\frac{d(\gvec{\lambda}^{(t)})}{\sum_{i\in\mathcal{I}}z_i(\gvec{\lambda}^{(t)}) }\right).
\end{equation}
Since the value of the dual function provides a lower bound on the optimal primal objective value the relative duality gap can be used to assess the distance of a found solution to the global optimum. The entire communication process between the coordinator and the nodes is illustrated in Fig.~\ref{fig:distr_clustering}. Note that the average cluster centroids are only used to compute the duality gap. They do not influence the update of the dual variables.
% Figure environment removed
\subsubsection{Symmetry breaking constraints}
The clustering problem \eqref{Clustering_MILP_Central} is highly symmetric, i.e., it contains solutions with the same objective values. This is because the index assigned to a cluster does not influence the objective function. Fig.~\ref{fig:symmetric_clustering} illustrates the situation of two symmetric solutions.
% Figure environment removed
This symmetry can lead to problems for the averaging heuristic presented in the previous section, as the computed cluster centroids of a single node can switch from one iteration to the next. For instance, while some points are assigned to cluster $ k $ in iteration $ t $, they could be assigned to cluster $ k' $ in iteration $ t+1 $ by switching the centroids of clusters $ k $ and $ k' $ without affecting the objective.\\
To prevent this behavior symmetry breaking constraints are added to the optimization problems of the nodes. In the first iteration, one of the nodes acts as the reference node, providing reference centroids $ \overline{\vec{m}}_k^{\text{ref}} $. In the subsequent iterations the quadratic constraint
\begin{equation}\label{Symmetry_Constraint}
	\Vert \vec{m}_{ik} - \overline{\vec{m}}_k^{\text{ref}} \Vert_2^2 \le \Vert \vec{m}_{ik'} - \overline{\vec{m}}_k^{\text{ref}} \Vert_2^2, \forall k,k'\in\mathcal{K},
\end{equation}
is added to each node $ i $. This ensures that cluster $ k $ of each node $ i $ will be the one closest to the reference centroid $ \overline{\vec{m}}_k^{\text{ref}} $. The choice of the node which provides the reference centroid can be performed arbitrarily, as it does not affect the optimization of the other nodes. Furthermore, the added constraint also does not affect the optimal objective value while rendering all symmetric solutions, except for one, infeasible.
\section{Numerical analysis of distributed clustering problems}
\input{Tables/Parameter_Table_Clustering}



The dual decomposition-based distributed clustering approach was evaluated on a set of benchmark problems of varying sizes. The data for each benchmark problem was generated randomly. First, initial cluster centroids $ \vec{m}_{k}^0 $ were generated, with $  [\vec{m}_{k}^0]_l \in \mathcal{U}_c(-1,1),\;l = 1,\dots,n_{\vec{y}} $.  Then, for each cluster $ k $ five random data points were added within a radius of 0.5 from the generated centroid. The parameters of the benchmark problems were varied as follows:
\pagebreak
\begin{align*}
	&\text{Number of nodes: } N_s \in \{2,3,4\},\\
	&\text{Number of dimensions: }n_{\vec{y}} \in \{2,3,4\},\\
	&\text{Number of clusters: }K \in \{3,4\}.
\end{align*}
Five benchmark problems were generated for each combination of nodes, dimensions, and clusters, resulting in a total of 90 benchmark problems. A benchmark problem is characterized by its number of nodes, dimension of the data, and number of clusters. For instance, problem  $ \text{3N2D4K}_{\text{5}} $ is the 5th benchmark problem comprised of 3 nodes with 2-dimensional data sorted into 4 clusters.\\
The benchmark problems were solved using the subgradient method, the bundle trust method, and the quasi-Newton dual ascent algorithm.

The initial step size (subgradient method)/ trust region (BTM, QNDA) parameter was set to $ \alpha^{(0)} = 0.5 $ and varied according to
\begin{equation}\label{Step_Size_Clustering}
	\alpha^{(t)} = \alpha^{(0)}/\sqrt{t}.
\end{equation}
The size of the bundle for BTM and QNDA was set to $ \tau = 50 $ points. All algorithms were initialized with $ \gvec{\lambda}^{(0)} = \vec{0} $ and the initial approximated Hessian of the QNDA algorithm was set to the negative identity matrix. The algorithms were terminated either when the Euclidean norm of the primal residual
\begin{equation}\label{key}
	\Vert \vec{w}_p\Vert_2 = \left\Vert \sum_{i\in\mathcal{I}}\vec{A}_i\hat{\vec{m}}_i \right\Vert_2,
\end{equation}
i.e., the violation of the consensus constraints, lied below a threshold of $ \epsilon_p = 10^{-2} $ or when the relative duality gap \eqref{Clustering_DG} reached a value of $ \epsilon_{DG} = 0.25\;\% $. The used parameters for the different algorithms are summarized in Tab.~\ref{tab:Parameter_Table_Clustering}. The MIQCP clustering problems of all nodes were solved using the commercial solver Gurobi \cite{gurobi} and the total computation time was computed as
\begin{equation}\label{Comp_Time}
	T_{\text{comp}} = N_{\text{iter}}\cdot T_{\text{comm}} + \sum_{t = 1}^{N_{\text{iter}}}  (T_{\text{update}}^{(t)} +    \underset{i\in\mathcal{I}}{\max}\;T_{\text{sub},i}^{(t)}),
\end{equation}
where $ N_{\text{iter}} $ is the number of required iterations, $ T_{\text{comm}}=800\;ms $ is the required communication time between the coordinator and the subproblems, which is assumed to be constant, $ T_{\text{update}}^{(t)} $ is the time required by the coordinator to update the dual variables in iteration $ t $ and $ T_{\text{sub},i}^{(t)} $ is the solution time for the clustering problem of node $ i $ in itereation $ t $.

\input{Tables/Results_Summary_Clustering}

The results for the clustering benchmarks are summarized in Tab.~\ref{tab:Results_Clustering_Summary}. Out of the examined algorithms, QNDA shows the best performance in terms of the required number of iterations and computation time as well as in terms of the achieved relative duality gap. The BTM algorithm shows similar performance in terms of the number of iterations and the achieved duality gap. However, in the case of distributed clustering, each iteration is costly due to the underlying MIQCP problems. Therefore, a slight performance increase in the number of iterations results in a substantial performance increase in terms of computation times. More detailed results for the clustering benchmarks are summarized in Tab.~\ref{tab:Results_Clustering} in the appendix.
% Figure environment removed

% Figure environment removed

Fig.~\ref{fig:rel_DG_2N2D4K_3} shows the evolution of the relative duality gap for benchmark problem $ \text{2N2D4K}_{\text{3}}$. The subgradient method converges rather slowly. In comparison, the BTM and QNDA algorithms exhibit a faster rate of convergence. Between these two algorithms, BTM exhibits an oscillatory behavior before converging. In contrast, the QNDA algorithm does not exhibit oscillations and therefore converges earlier. Additionally, it should be noted that the QNDA algorithm achieves a relative duality gap of $ 0\; \%$, i.e., it converges to a proven global optimum.

{Fig.~\ref{fig:2N2D4K_3_Iterations} further illustrates the results. Fig.~\ref{fig:Node_1_It_1} and \ref{fig:Node_2_It_1} show the results of the clustering in the first iteration, i.e., the individual global optima. Fig.~\ref{fig:Node_1_It_4} and \ref{fig:Node_2_It_4} depict the solutions upon convergence of the QNDA algorithm. Each node computes the same cluster centroids corresponding to the globally optimal solution with respect to the entire data set, but not to the individual data sets. It is therefore possible to compute a global model locally in each node while only accessing local data.}

\section{Comparison to the central solution}
% Figure environment removed
As shown in the previous section, solving the MIQCP clustering problems is computationally expensive. This is due to the weak integer relaxation of problem \eqref{Clustering_MILP}, which means that the solution of the relaxed problem within the branch-and-bound algorithm is far away from the integer solution. This results in slow-moving relative integrality gaps and slow convergence of the solution algorithm. While the main motivation of the distributed clustering approach is the training of a global model without the exchange of local data, it can also be used to efficiently solve larger clustering problems. Fig.~\ref{fig:central_clustering}	 depicts the evolution of the relative duality gap of the QNDA algorithm as well as the evolution of the relative integrality gap of Gurobi for the complete data set of benchmark problem $ \text{4N4D4K}_{3} $. The clustering problems of the individual nodes were solved sequentially in the case of QNDA, also using Gurobi. While the relative gap of the central solution improves very slowly, the QNDA algorithm quickly converges to a solution close to the global optimum. Note, that both relative gaps prove a worst-case distance to the global optimum. Hence, decomposing a large clustering problem into smaller subproblems and coordinating the solutions via a distributed optimization algorithm can offer significant performance improvements compared to a central solution.
\section{Conclusion}
This paper demonstrated how dual decomposition-based distributed optimization can be applied to the solution of clustering problems. The approach ensures privacy, i.e., enables federated learning, as each node only has access to its local data. A global model can still be obtained by coordinating the solutions of the individual clustering problems. Numerical tests on a large set of benchmark problems demonstrated that the QNDA algorithm outperforms the subgradient method and the BTM algorithm. Furthermore, the distributed optimization approach exhibited superior performance compared to a central solution approach. In the future, the developed algorithms can also be applied to other federated learning problems, like the distributed training of support vector machines.
\bibliography{main}
\bibliographystyle{IEEEtran}
\appendix
\subsection{Benchmark Problems}
All benchmark problems can be found under:
\url{https://github.com/VaYf/Clustering-Benchmark-Problems}
%\subsection{Results for the clustering benchmark problems}
%The results for the clustering benchmarks are summarized in Tab. \ref{tab:Results_Clustering}.
\input{Tables/Results_Table_Clustering}


\end{document}


