\section{Introduction}\label{sec:intro}

Evaluation is one of the biggest challenges of recommender systems research. Online A/B testing is already an approximation of the recommender's true goal, limited by KPIs that can be measured. Splitting traffic fairly, preventing information leak between groups and eliminating biases~\cite{jeunen2023common} are also non-trivial. From the business perspective, A/B tests are expensive and slow because a portion of the traffic is served by suboptimal models, and getting statistically significant differences for informative but noisy KPIs can take weeks/months. From the scientific perspective, A/B tests are not reproducible due to the proprietary nature of production recommenders preventing independent validation of A/B test setups and verification of online results. Therefore, the inherently imperfect approximation of online performance through offline evaluation is likely to stay the main way of assessing performance in research, at least until simulators~\cite{mcinerney2021accordion} become more mature. Thus, it is imperative that offline evaluation is correct and follows how production recommenders work as closely as possible. Unfortunately, flaws are quite common, due to later works copying flawed evaluation setups from their predecessors without questioning their validity~\cite{ferrari2020methodological}.

In this paper we discuss four widespread evaluation flaws we observed en masse in research papers of the last decade. Our main contribution is presented in Section~\ref{sec:flaws}, where we go through the most important steps of designing offline evaluation setups and discuss common pitfalls. We closely examine four severe flaws commonly found in the evaluation setup of various recommenders: dataset--task mismatch, general claims on heavily preprocessed data, information leaking through time, and negative item sampling. Their effect is demonstrated on sequential recommenders.


