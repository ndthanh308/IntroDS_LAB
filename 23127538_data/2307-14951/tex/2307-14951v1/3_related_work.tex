\section{Related work}\label{sec:related}

Offline evaluation of top-N recommenders has been discussed since top-N recommendation became dominant. Early works mainly focused on metrics~\cite{hurley2011novelty,vargas2011rank} and finding connections between offline and online results~\cite{10.1145/2959100.2959176}. The pace of the work has increased in recent years~\cite{10.1145/3523227.3547408}, simultaneously with the sharp increase of papers lacking satisfactory evaluation. 

Evaluation has many aspects from metrics~\cite{hurley2011novelty,vargas2011rank} to hyperparameter optimization~\cite{rendle2022revisiting}. \cite{ferrari2019we,ferrari2020methodological} suggests that one of the reasons behind the low reproducibility rate of recent papers is that evaluation setups are lifted from earlier work without their validity being checked. 

Our work focuses on this problem, i.e.~the design of evaluation setups and the flaws within. Of the four flaws discussed in this paper, only negative item sampling has been scrutinized, aside from \cite{tang2018personalized} briefly mentioning that the Amazon dataset has weak sequential signals. \cite{krichene2020sampled} discussed the background of how sampling introduces bias to measurements, and~\cite{canamares2020target,dallmann2021case} demonstrated that the relative performance of models can change when sampling is applied. Our experiments confirm their results and give additional explanation on why this change happens.