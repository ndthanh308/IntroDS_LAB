\section{Discussion \& Conclusion}\label{sec:conclusion}

Offline evaluation is fundamentally imperfect, but it will likely remain the main approach for assessing performance in recommendation systems research. Online A/B tests are not just expensive and slow, but inherently not reproducible; and simulators are still in their early stages. Unfortunately, flaws are quite widespread in offline evaluation setups. In this paper, we went through the main steps of designing evaluation setups and pointed out four widespread flaws and demonstrated their effect through the example of sequential recommendations. We chose sequential recommendation for the demonstration, because it is one of the areas severely plagued by low quality evaluation. Evaluation flaws of some of the earlier sequential recommender papers -- e.g.~\cite{kang2018self} has all four we discussed -- were missed by both the authors and reviewers. Later works copied the setups -- with their flaws included -- without questioning whether they are correct. There is not a single best way for offline evaluation, but by pointing out these four flaws, we hope to see the quality of experimentation sections of research papers improving in the future.