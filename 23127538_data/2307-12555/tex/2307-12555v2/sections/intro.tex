\section{Introduction}
%{\color{red}
%\ul{Please keep the original version and compare the revised version. For every paragraph, write one sentence at the beginning to describe the main idea of that paragraph.}}


%{\color{red} The first five paragraphs are too lengthy. Specifically,
%\begin{itemize}
%    \item The first two paragraphs describe a "very" general background of graph representation learning. This is "too far away" from our main topic. At most, a few sentences are enough.
%    \item In this first part of introduction, we should describe the following key points clearly: 1) the general context of graph learning, 2) because of the lack of labels, unsupervised learning is important, 3) in particular, GCL is one of the main stream, how does GCL work (very briefly, the description in paragraph 5 is too detailed. you can put the detailed description in Preliminary), 4) GCL has achieved great success and has many applications. That is, it is important.
%\end{itemize}
%}

%General Concept of Graph Learning.
%Recently, graph data has been extensively explored across various domains. To effectively handle the graph data, researchers have introduced a series of machine learning models to encode the affluent topology and semantic information incorporated in the graph data to the node/graph representations. 
%Then, the node/graph representations can serve as good-quality inputs for graph-based downstream tasks such as node classification~\cite{bhagat2011node, GNN, graphsage} and graph clustering~\cite{schaeffer2007graph, zhou2009graph, GAP}, etc. 
%To date, the most powerful graph machine learning models are \underline{G}raph \underline{N}eural \underline{N}etworks~\cite{GNN} (GNNs) and their variants. GNN utilizes the crafted message-passing mechanism to aggregate the neighbors' features incrementally along the graph topology. In this way, the node representations can encode the structural information of the graph data for semi-supervised node classification. 

%Importance of unsupervised learning.
%However, the nature of the GNN requires a good quality of node labels' information which is hard to collect in the real world~\cite{gilmer2017neural}. Besides, the label noise caused by the neglectful operations during the data collection phase will also degenerate the quality of the graph representation learning. To alleviate this problem, the unsupervised graph representation learning framework has been proposed to capture the graph's proximity and semantic information without querying the node labels. %Traditional unsupervised learning such as DeepWalk~\cite{DeepWalk} and node2vec~\cite{node2vec} stem from the skip-gram model~\cite{skip-gram} and sample multiple random walks starting from the target nodes and push nodes along the random walks to be similar on the embedding space. However, these methods highly rely on the graph's structural information and cannot provide fascinating performance on the graph-based downstream tasks. 

%Introduce GCL framework.
%Graph representation learning is a highly effective tool for dealing with relational data which is ubiquitous in the real world. Currently, there exists a prosperous development of the \underline{G}raph \underline{C}ontrastive \underline{L}earning (GCL) models stemming from SimCLR~\cite{SimCLR} which belong to the unsupervised graph representation learning method.  
%In general, the GCL model seeks the largest agreement between the positive pairs' node features obtained from the GNN (\underline{G}raph \underline{N}eural \underline{N}etwork~\cite{GNN}) encoder and enlarges the discrimination between the negative pairs. Those pairs are generated by data augmentations (feature masking, edge dropping, etc.) to the initially given graph data. 
%Then, the GCL model optimizes the contrastive loss to maximize the mutual information between different views. 

Graph representation learning~\cite{node2vec, DeepWalk} has revolutionized the analysis of graph data, which is prevalent across diverse domains. In practice, the scarcity of ground-truth labels has led to a surge in research on unsupervised graph learning approaches. Among these methods, \underline{G}raph \underline{C}ontrastive \underline{L}earning (GCL)~\cite{GRACE, GCA, ARIEL, SPAN} has emerged as a highly effective unsupervised approach, outperforming other methods on various downstream tasks, including node classification and graph clustering~\cite{GRACE, GCA, ARIEL}.

%A series of literatures~\cite{GRACE, GCA, ARIEL, SPAN} have demonstrated that the GCL models are highly effective and surpass other unsupervised graph representation learning methods on multiple downstream tasks such as semi-supervised node classification and graph clustering~\cite{GRACE, GCA, ARIEL}, etc.

%To be specific, the GCL model first pre-trains the GNN encoder via optimizing the pre-defined contrastive loss. Then, the high-quality node embeddings are generated from the pre-trained GNN encoder and serve as inputs to logistic regression to classify node labels. 
%Indeed, the powerful learned representation of GCL can precisely capture the local structure (node tends to link similar nodes) and community structure (similar nodes are densely connected) incorporated in the graph data and encode the affluent information on the low-dimensional feature space.    

% GCL and homophily.
The label-preserving property of GCL is a key factor behind its main benefits~\cite{autogcl}. Specifically, the node embeddings generated by GCL, which incorporate both topological and semantic information, are consistent with node label information even without explicitly querying the node labels. This consistency arises from the \textit{homophily}~\cite{homophily} assumption of the graph data, which states that nodes tend to connect with ``similar" others. This phenomenon is widely observed in real-world graph data such as friendship networks~\cite{homophily}, political networks~\cite{politicalnetwork}, citation networks~\cite{CitationNetwork}, etc. By contrasting positive and negative samples of nodes that are similar or dissimilar in semantic information, the GCL framework encourages the GNN encoder to learn node embeddings that capture the homophily patterns present in the graph. 
%Reflecting on the low-dimensional embedding space, a node will probably share the same label with the other nodes if the Euclidean distances of their embeddings are small enough and vice versa.    

%The GCL model seeks the largest agreement between the inputs (node attributes) and their representations (node features) by contrasting positive pairs with their counterparts. Most of them choose mutual information (MI) to measure the similarity between features on input and representation space. However, the mutual information is intractable due to its high complexity. A series of GCL models choose the infoNCE~\cite{infoNCE} loss as the lower bound approximation of MI to contrast positive node pairs from negative node pairs. To be specific, the GCL model first generates stochastic augmentation views by randomly adding/deleting nodes or links or masking features (zeroing out elements of the attribute vector) from the original graph. Then, the augmentation views are fed into a shared GNN encoder and provide the node embeddings optimized by minimizing the infoNCE loss. A series of literatures~\cite{GRACE, GCA, ARIEL, SPAN} have demonstrated that the powerful GCL models can outperform other unsupervised graph representation learning methods across various graph datasets.  


%Recently, graph data has been extensively explored across various disciplines and inter-discipline such as social networks, biology, finance and computer engineering. For instance, in the e-commercial platform, a bipartite graph with nodes representing users or items and edges denoting the purchase records. The data analyst can deploy a recommendation system~\cite{RecSys} to mine the bipartite graph and capture the target user's purchase preference which in turn increases the user's purchase frequency. In the biology field, the bioinformatician regards the structure of the chemical compounds as attributed graphs and implements the graph mining technique to mine the function of the specific chemical compounds~\cite{asgn}, i.e., whether the protein is a mutagen. For financial security, the risk manager spots potential financial fraudsters In the transaction network by mining social ties and transaction records. 

%To effectively handle the relational data, researchers have introduced a series of machine learning models to encode the affluent structural and semantic information incorporated in the relational data to the node/graph representations. Then, the node/graph representations can serve as good-quality input for graph-based downstream tasks such as node classification~\cite{bhagat2011node, GNN, graphsage}, graph clustering~\cite{schaeffer2007graph, zhou2009graph, GAP}, graph-based anomaly detection~\cite{ma2021comprehensive, noble2003graph, deng2021graph} and graph classification~\cite{GIN, GraphUNets, zhang2019hierarchical}. To date, the most powerful graph machine learning models are Graph Neural Networks~\cite{GNN} (GNNs) and their variants. GNN utilizes the crafted message-passing mechanism to aggregate the neighbors' features incrementally along the graph topology. In this way, the node representations can encode the structural information of the graph data for semi-supervised node classification. 



%To prevent this issue, the unsupervised graph representation learning framework has been proposed to capture the graph's structural and attribute information without access to the node labels. Traditional unsupervised graph representation learning such as DeepWalk~\cite{DeepWalk} and node2vec~\cite{node2vec} follow the \textit{contrastive} framework stem from the skip-gram model~\cite{skip-gram}. They first sample random walks starting from the target nodes and push nodes along the random walks to be similar on the embedding space. However, these methods highly rely on the graph's structural information and cannot provide fascinating performance on the graph-based downstream tasks.

%Inspired by the contrastive framework, there is a prosperous development of GNN-based graph contrastive learning models which can provide high-quality node embeddings. The graph contrastive learning (GCL) model seeks the largest agreement between the inputs (node attributes) and their representations (node features) by contrasting positive pairs with their counterparts. Most of them choose mutual information (MI) to measure the similarity between features on input and representation space. However, the mutual information is intractable due to its high complexity. A series of GCL models choose the infoNCE~\cite{infoNCE} loss as the lower bound approximation of MI to contrast positive node pairs from negative node pairs. To be specific, the GCL model first generates stochastic augmentation views by randomly adding/deleting nodes or links or masking features (zeroing out elements of the attribute vector) from the original graph. Then, the augmentation views are fed into a shared GNN encoder and provide the node embeddings optimized by minimizing the infoNCE loss. A series of literatures~\cite{GRACE, GCA, ARIEL, SPAN} have demonstrated that the powerful GCL models can outperform other unsupervised graph representation learning methods across various graph datasets.  

%{\color{red}
%\begin{itemize}
%    \item The following states the "motivations" of this work.
%    \item In the following two paragraphs, you describe the attack and defense of GNN in general, which is correct. But, you should put the emphasis on GCL. Imagine that, previously, you have introduced that GCL is very popular, now you need to talk about the attack and defense against GCL.
%    \item Here is the logic: 1) graph models are vulnerable to attacks, in particular, there are already attacks against GCL. 2) Then, the natural question is "are there defense methods?" 3) there are many defense methods for GNN, are they applicable to GCL? there are already defenses for GCL, are they adequate?
%    \item There is a clear important problem to solve but existing approaches are not enough. So there is a clear motivation. Describing "why existing approaches are not enough" is very important.
%\end{itemize}}

% GCL is vulnerable to structural attacks.
However, similar to the vanilla \underline{G}raph \underline{N}eural \underline{N}etwork (GNN)~\cite{GNN} and its variants, GCL models' label-preserving property also makes them susceptible to graph structural attacks~\cite{Mettack,CLGA}. %inject adversarial noises into the clean graph to enlarge the disparity between the topology information and label information. The graph attacker may 
In such attacks, the adversary can manipulate the graph topology by adding or deleting edges in the original graph to undermine the quality of node embeddings and subsequently degrade performance on downstream tasks.
%manipulate the graph topology by proactively adding or deleting edges in the clean graph to sabotage the quality of the node embeddings and then degenerate the performance of the downstream tasks. 
For instance, a malicious entity can manipulate social ties by connecting with normal accounts to evade graph-based anti-fraud systems~\cite{BinarizedAttack}. 
Indeed, existing research has already demonstrated the vulnerability of GCL under structural attacks. In particular, CLGA~\cite{CLGA} is an attack method specifically designed against GCL and has demonstrated  effective attack performance.
%Currently, there already exists the graph structural attack methods against vanilla GCL models called CLGA~\cite{CLGA}. Particularly, this attack model formulates the attack problem as a bi-level optimization problem and selects node pairs that maximize the converged graph contrastive loss for manipulation. 
%Then, the graph attacker manipulates the node pair with the largest gradient in a greedy manner until reaches the attack budget. 
Moreover, although Mettack~\cite{Mettack} is an attack method designed to attack semi-supervised GNN, extensive experiments~\cite{CLGA} demonstrate that it can also successfully attack GCL, at times generating attacks that are more potent than CLGA.
%and the attack performance can even exceed CLGA in some cases. 
Therefore, investigating potential countermeasures against structural attacks is essential to ensuring security and robustness of GCL. 

% discuss defense on GCL.
Our main goal is to \textit{design the GCL framework that is robust against attacks on the graph structure}.  While there have been a number of advances in devising countermeasures against structural attacks for GNN models~\cite{RGCN, ProGNN, GCNJaccard, SimPGCN}, these are primarily designed for  semi-supervised settings and intimately depend on having access to node label information.
Since GCL is fully unsupervised, such methods are not readily applicable.
%, making them challenging to deploy for GCL models.
%For example, \cite{RGCN} learns the Gaussian distribution of the node features and regards the nodes with large variance as malicious nodes. \cite{GNNGUARD} computes the removing link probability via the cosine similarity of two nodes' features during training. \cite{SimPGCN} constructs a kNN graph based on the node attribute matrix and incorporates the kNN graph into GNN to ensure attribute consistency. \cite{ProGNN} penalizes the significant changes in the statistics of several graph properties after graph structural attacks to recover the performance of GNN. 
%More recently, there have been some emerging works on improving the robustness of GCL models. 

Two recent approaches, ARIEL and SPAN, tackle specifically the problem of learning robust GCL models.
ARIEL~\cite{ARIEL} employs an adversarial training approach by introducing an adversarial view into the contrastive learning process to enhance robustness. 
%and implements adversarial training to generate a more poisoned graph via projection gradient descent. Then, the GCL framework contrasts the adversarial view with two standard augmentation views.
SPAN~\cite{SPAN}, on the other hand, modifies the topology augmentation process by maximizing and minimizing the spectrum of the original graph to generate augmentation views. 
%This results in a more effective GCL model that can capture the spectrum invariance of graphs.
% Limitations of current robust GCLs.
However, these methods have significant limitations. 
While adversarial training is effective to combat attacks at inference time, their use to counter poisoning attacks on graph structure has the effect of further poisoning the structure of the graph.
%Adversarial training inherently introduces more poison into the input poisoned graph, leading to limited improvements. 
As a consequence---as we show in the experiments below---ARIEL exhibits limited effectiveness as we increase attack strength.
%In fact, our experiments show that ARIEL can hardly enhance robustness when the attack power is relatively high. 
Moreover, adversarial training is tied to the specific mechanism presumed to poisoned the graph structure, making ARIEL vulnerable whenever actual attacks make use of different poisoning techniques.
%heavily depends on the mechanism that generates adversarial samples and may not be effective if the actual attack graphs are not generated in the same way. 
As for SPAN, minimizing the spectrum of graph Laplacian will naturally add inter-class edges (similar to adversarial noise) to the input graph, inheriting similar limitations as adversarial training. In addition, computing spectral changes is quite time-consuming due to the involvement of eigenvalue decomposition of the graph Laplacian ($\mathcal{O}(N^{3})$ where $N$ is the node number).
Indeed we observe that SPAN demonstrates better robustness than ARIEL because of an augmentation view generated by maximizing the spectrum of graph Laplacian, which can 
prune the inter-class edges, resulting in a ``cleaner" graph.
%that boosts the robust performance of downstream tasks. 
%The key point is that sanitizing the original graph and producing cleaner data instances is better than adversarial training. 
This observation suggests a more direct approach for sanitizing the graph that is focused specifically on recovering homophily.
%a better potential for data sanitation in defending against poisoning attacks. 
Finally, both SPAN and ARIEL tune the vital hyperparameters based on the performance on downstream tasks, requiring access to node labels. 
This takes them partly outside the unsupervised setting that motivates GCL.
%This actually contradicts the primary purpose of deploying GCL in an unsupervised learning setting.
%Therefore, there is an urgent need to design a new approach to tune the hyperparameters of GCL models without querying the node labels.

%Fortunately, a surge of defense methods against graph structural attacks have been proposed in recent years. For example, \cite{RGCN} learns the Gaussian distribution of the node features and regards the nodes with large variance as malicious nodes. \cite{GNNGUARD} computes the removing link probability via the cosine similarity of two nodes' features during training. \cite{SimPGCN} constructs a kNN graph based on the node attribute matrix and incorporates the kNN graph into GNN to ensure attribute consistency. \cite{ProGNN} penalizes the significant changes in the statistics of several graph properties after graph structural attacks to recover the performance of GNN. However, these workable defense methods are all based on the GNNs and can only tackle the semi-supervised node classification problem. For unsupervised learning, it has been verified that GCLs are also prone to be influenced by the malicious attacker (CLGA~\cite{CLGA}). Therefore, it is necessary to explore the vulnerability of GCL and enhance its robustness against graph structural attacks.         

%{\color{red} Now, describe our approach.
%\begin{itemize}
%    \item The main idea of our approach. That is, \underline{ignoring all the implementation details}, how our approach can intuitively solve the problem. Here, you convince the readers that if we can do what we have promised, we can solve the problem.
%    \item Then, \underline{in order to implement our approach}, what are the challenges (state them very explicitly)? Currently, the challenges are not clear. The consequence is that the innovations and contributions are also not clear. 
%    \item Possible challenges: 1) efficiency issues, so we need "learnable" sanitation view. 2) ...
%    \item What techniques did you propose to address those challenges? (briefly describe, because you have the rest paper to describe your methods)
%\end{itemize}}

%{\color{red} If you check the following paragraph, it is too long, meaning that you have mixed a lot of things together, and they are not clearly stated.}

% Main idea of our approach.
We propose a robust GCL framework that integrates a homophily-driven learnable \textit{sanitation view} that can effectively prune potential malicious edges in the poisoned graph during training. 
Of course, the central challenge is how to effectively design such a sanitation view.
%How to design this sanitation view remains a central challenge.  
To address this challenge, we first investigate the vulnerability of GCL  under state of the art structural attacks.
%, which provides two valuable guidance. 
This investigation yields two crucial insights.
Firstly, extensive experiments demonstrate that the graph structural attacks will break an important graph property: \textit{graph homophily}~\cite{homophily} (i.e., the tendency of similar nodes to be connected). Secondly, we theoretically prove that structural attacks will adversely affect the mutual information of the graph and its representations from the information-theoretic perspective. Based on these results, we propose to learn the sanitation view under the guidance of both graph homophily and mutual information.
%in a multi-task manner.
%, balanced by a hyperparameter $\eta$. 
%The overall framework is then trained %via gradient descent 
%in an end-to-end manner.
%updating the parameters of learnable sanitation view and GNN encoder simultaneously for each step.   

%One of our intuitions is that the message passing mechanism of the graph convolutional layer~\cite{GNN} is well-suited for handling the homophilous graph, as the aggregation function tends to aggregate similar node features and increases the discrimination among nodes with different types. Besides, the graph attacker tends to inject inter-class edges~\cite{GCNJaccard} to the clean graphs to invalidate the function of the graph convolutional layer. Based on these observations, a natural approach to enhancing the robustness of graph-based models is to remove potential inter-class edges from the given poisoned graph during training. }
%Moreover, this new GCL model is trained in an end-to-end manner and produces high-quality node embeddings based on the sanitation view for downstream tasks such as semi-supervised node classification and graph clustering. Finally, to tune the hyperparameters without querying the node labels, we innovatively refer to the unsupervised normalized cut loss~\cite{NormCut} as a metric to decide the best choice of the hyperparameter.  

% Challenges of our approach.
%However, 
There are several technical challenges in effectively training our approach in a fully end-to-end manner.
%learning encounters several technical challenges.
%optimizing the homophily-driven learnable sanitation view poses several challenges. 
Firstly, measuring graph homophily typically requires information about node labels, which is missing in our unsupervised setting.
%the node labels' information which is prohibitive in an  unsupervised setting. 
Secondly, augmenting topology changes to graph data is a complex discrete optimization problem not directly amenable to gradient-based learning techniques. 
%that involves computing the gradient of the in-differentiable distribution function. 
Thirdly, all prior approaches fine-tune hyperparameters by utilizing supervised information about node labels; while this is infeasible in a fully unsupervised setting, it is not evident how to avoid it.
%it is not clear how to fine-tune hyperparameters without 
%without querying the node labels is a non-trivial task.  

% How to tackle the challenges.

We propose a series of techniques to address the above challenges to enable full end-to-end gradient-based robust GCL. 
%To address these challenges, we design the learnable sanitation view as a random vector that follows the Bernoulli distribution which is parametrized by the vector with each entry represents the probability of edge removal.
Specifically, we use a random vector following the Bernoulli distribution to control the generation of the sanitation view, where the parameters of the distribution are optimized via learning. 
%we formulate the learnable sanitation view as a random vector that follows the Bernoulli distribution parametrized by edges masking probability vectors. 
During the gradient update phase, we employ the Gumbel-Softmax re-parametrization trick~\cite{GumbelSoftmax} to approximate the Bernoulli distribution and use the straight-through estimator~\cite{STE} to address non-differentiability.
%such that all the model parameters could be efficiently optimized.
%for optimizing model parameters via the in-differentiable probability mapping. 
The entire framework is then optimized in an end-to-end manner,  training the learnable sanitation view generator and GNN encoder simultaneously.
Our approach is therefore considerably more time-efficient than the two-stage GCLs such as SPAN. 
Additionally, to measure graph homophily without the knowledge of node labels, we use the raw node attribute matrix as an alternative. Moreover, we implement the early stopping technique based on a pseudo normalized cut loss to determine the best choice of the vital hyperparameter $\eta$. 
%{\color{red} how to address the third challenge is not stated}
We refer to our proposed robust model as GCHS (\underline{G}raph \underline{C}ontrastive Learning with \underline{H}omophily-Driven \underline{S}anitation View). 

We conduct extensive experiments to evaluate GCHS.
We compare GCHS with seven strong baselines and report robustness results against two representative state-of-the-art structural attacks over six widely-used graph datasets.
Our evaluation considers two important downstream tasks for GCL: 
%under two major tasks of 
node classification and graph clustering. 
Our results demonstrate that GCHS consistently outperforms all baselines in nearly all cases, often by a large margin.
%under various attacking strengths. 
%Additionally, we also provide the visualization of node embeddings to qualitatively present the high-quality of the node embeddings generated by our sanitation view as well as report the robustness performance of our proposed method with two other typical robust models from the information theory perspective. 

The main contributions are summarized as follows:

%we propose training an edges masking probability matrix that follows the Bernoulli distribution to automatically prune potential heterophilous edges and recover the homophily degree of the poisoned graph. To quantify the graph homophily degree without querying the labels' information, we measure the graph homophily based on the raw node attribute matrix. We then jointly train the edges masking probability matrix and the GNN encoder's weights by optimizing a newly defined loss function that balances the graph contrastive loss and the graph homophily. To optimize model parameters via the indifferentiable probability mappings, we utilize the Gumbel-Softmax re-parametrization trick~\cite{GumbelSoftmax} to approximate the Bernoulli distribution and compute the gradient via straight-through estimator~\cite{STE}. The entire framework is optimized end-to-end manner, training the learnable sanitation view and GNN encoder simultaneously, which is more time-efficient than the two-stage GCLs like SPAN. We verify the effectiveness of our method on six widely-used graph datasets and report the robust performances against two typical graph structural attacks of our method compared with seven strong baselines for node classification and graph clustering. The experiment results demonstrate that our method outperforms most baselines under various attacking scenarios. Additionally, we also provide the visualization of node embeddings to qualitatively present the high-quality of the node embeddings generated by our sanitation view. For convenience, we refer to our model as GCHS (\underline{G}raph \underline{C}ontrastive Learning with \underline{H}omophily-Driven \underline{S}anitation View). To summarize, our main contributions are as follows:
\begin{itemize}
    %\item We propose a novel learnable \textit{homophily-preserving} sanitation view to boost the robustness of the graph contrastive learning framework.
    \item We propose a novel GCL framework that integrates a learnable sanitation view, which  significantly improves  the adversarial robustness of unsupervised graph contrastive learning under structural attacks. 
    %\item We tackle the in-differentiation during the backward pass when optimizing the sanitation view via Gumbel-Softmax approximation. 
    \item We propose a series of techniques to enable effective end-to-end gradient-based training of the learnable sanitation view generator and GCL.
    %\item We leverage the Gumbel-Softmax re-parametrization trick with the straight-through estimator to tackle the indifferentiation during the back-propagation and optimize the whole framework in an end-to-end manner via projection gradient-descent.
    \item We propose a pseudo normalized cut loss to conveniently fine-tune the vital hyperparameter $\eta$ without altering the GCL's model structure, and thus entirely avoid the need for node labels, unlike past approaches for GCL models. 
    %One of our key innovations is to utilize the unsupervised normalized cut loss to tune the hyperparameter.  Thus, we are able to entirely avoid the need to node labels, unlike past approaches for robust GCL.
    %without querying node labels' information.
    \item Extensive experiments demonstrate the effectiveness of our proposed robust GCL framework under different attack settings.
    %\item Extensive experiments demonstrate that our proposed robust GCL framework outperforms all baseline models in terms of the quality of generated node embeddings and the performances on two important downstream tasks.
\end{itemize}

%The rest of this paper is organized as follows: In Sec.~\ref{sec-related}, we introduce the topics related to this paper; In Sec.~\ref{sec-preliminaries}, we provide the details of the important concepts with related to the proposed model; In Sec.~\ref{sec-problem} and \ref{sec-vunlerability}, we introduce the adversarial environment of the graph contrastive learning framework and provide a comprehensive analysis on the adversarial robustness of GCL model; In Sec.~\ref{Sec-Methodology}, we elaborate on the overall framework of our proposed model; In Sec.~\ref{Sec-exp}, we conduct comprehensive experiments to verify the effectiveness of our proposed model.

%In this paper, we start the enhancement of the GCL robustness from a vital concept--\textit{graph homophily}~\cite{homophily}. Graph homophily describes the phenomenon that the connected nodes tend to be similar in the graph (``like attracts like"). A surge of graphs in the real world including social networks and citation networks are homophilous graphs. It has been observed that the message passing mechanism of the graph convolutional layer~\cite{GNN} is naturally suitable for tackling the homophilous graph, i.e., the aggregation function is inclined to aggregate the similar node features and enlarges the discrimination among nodes with different types. On the other hand, the graph attacker tends to inject heterophilous edges~\cite{GCNJaccard} to the clean graphs to invalidate the function of the graph convolutional layer. Based on these observations, we resort to introducing a \textit{learnable sanitation view} during the stochastic augmentation phase to assist the GCL to produce relatively high-quality node embeddings given the poisoned graph. The main insight of the sanitation view is to train an edges masking probability matrix following the Bernoulli distribution to automatically prune the potential heterophilous edges and recover the homophily degree of the poisoned graph. To better quantify the graph homophily degree without querying the labels' information, we measure the graph homophily based on the raw node attribute matrix. Then, we jointly train the edges masking probability matrix and the GNN encoder's weights by optimizing the newly defined loss function which trade-off between the infoNCE loss and the graph homophily. In order to optimize the model parameters via the indifferentiable probability mappings, we utilize the Gumbel-Softmax re-parametrization trick~\cite{GumbelSoftmax} to approximate the Bernoulli distribution and compute the gradient via straight-through estimator~\cite{STE}. To effectively tune the hyperparameter in an unsupervised manner, we innovatively deploy the normalized cut loss~\cite{NormCut} originated from the graph clustering topic to supervise the best setting under the scenario of the GCL's adversarial robustness. The whole framework is optimized in an end-to-end manner and trains the learnable sanitation view and GNN encoder in the meanwhile, which is far more time-efficient than the two-stage GCLs like SPAN~\cite{SPAN} etc. To verify the effectiveness of our method, sufficient experiments have been conducted on six widely-used graph datasets. We report the robust performance against two typical graph structural attacks of our method with other seven strong baselines for node classification and graph clustering downstream tasks. The experiment results demonstrate that our method outperforms other baselines under most of the various attacking scenarios. Moreover, we also provide the visualization of node embeddings to qualitatively present the high-quality of the node embeddings generated by our sanitation view. To summarize, our main contributions are as follows:
%\begin{itemize}
%    \item We propose a novel learnable \textit{homophily-preserving} sanitation view to boost the robustness of the graph contrastive learning framework.
%    \item We leverage the Gumbel-Softmax re-parametrization trick with the straight-through estimator to tackle the indifferentiation during the back-propagation and optimize the whole framework in an end-to-end manner via projection gradient-descent.
%    \item We innovatively utilize the unsupervised normalized cut loss as supervision to tune the hyperparameter without querying node labels' information.
%    \item Extensive experiments demonstrate that node embeddings generated from our learnable sanitation view achieve the best quality under various attacking scenarios. 
%\end{itemize}

%{\color{red} Finally, state the results.}


















































