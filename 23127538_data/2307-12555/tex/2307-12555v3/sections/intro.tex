\section{Introduction}
\label{Sec-intro}
Graph representation learning~\cite{node2vec, DeepWalk} has revolutionized the analysis of graph data, which is prevalent across diverse domains. In practice, the scarcity of ground-truth labels has led to a surge in research on unsupervised graph learning approaches. Among these methods, \underline{G}raph \underline{C}ontrastive \underline{L}earning (GCL)~\cite{GRACE, GCA, ARIEL, SPAN, 10597723} has emerged as a highly effective unsupervised approach, outperforming other methods on various downstream tasks, including semi-supervised node classification~\cite{GNN}.   

% GCL and homophily.
The label-preserving property of GCL is a key factor behind its main benefits~\cite{autogcl}. Specifically, the node embeddings generated by GCL, which incorporates both topological and semantic information, are consistent with node label information even without explicitly querying the node labels. This consistency arises from the \textit{homophily}~\cite{homophily, 10889118, GNNhomophily, HeteRobust} assumption of the graph data, which states that nodes tend to connect with ``similar" others. This phenomenon is widely observed in real-world graph data such as friendship networks~\cite{homophily}, political networks~\cite{politicalnetwork}, citation networks~\cite{CitationNetwork}, etc. By contrasting positive and negative samples of nodes that are similar or dissimilar in semantic information, the GCL framework encourages the GNN encoder to learn node embeddings that capture the homophily patterns present in the graph. 

% GCL is vulnerable to structural attacks.
However, similar to the vanilla \underline{G}raph \underline{N}eural \underline{N}etwork (GNN)~\cite{GNN, graphsage} and its variants, GCL models are also susceptible to graph structural attacks~\cite{Nettack, Mettack, CLGA, HRAT, TopologyAttack, GODZISZEWSKI2024104173, 10535517, 10693599, 10433700, 10296883, 10446170, 10646863,9679165}. 
In such attacks, the adversary can manipulate the graph topology by adding or deleting edges in the original graph to undermine the quality of node embeddings.
For instance, a malicious entity can manipulate social ties by connecting with normal accounts to evade graph-based anti-fraud systems~\cite{BinarizedAttack}. 
Indeed, existing research has already demonstrated the vulnerability of GCL under structural attacks. In particular, CLGA~\cite{CLGA} is an attack method specifically designed against GCL and has demonstrated effective attack performance.
Moreover, although Mettack~\cite{Mettack} is an attack method designed to attack semi-supervised GNN, extensive experiments~\cite{CLGA} demonstrate that it can also successfully attack GCL, at times generating attacks that are more potent than CLGA. 
Therefore, investigating potential countermeasures is essential to ensuring the security and robustness of GCL. 

% discuss defense on GCL.
Our main goal is to \textit{refine the GCL framework so that it is robust against graph structural attacks}. While there have been a series of advances in devising countermeasures against structural attacks for GNN models~\cite{RGCN, ProGNN, GCNJaccard, SimPGCN, GRV, HeteRobust, FocusedCleaner, XLX, GNNGUARD, GASOLINE}, they are primarily designed for semi-supervised settings and intimately require access to node label information. Since GCL is fully unsupervised, such methods are not readily applicable. Two recent approaches, ARIEL~\cite{ARIEL} and SPAN~\cite{SPAN}, specifically address the challenge of learning robust GCL models. ARIEL~\cite{ARIEL} employs an adversarial training approach by introducing an adversarial view into the contrastive learning process to improve robustness. SPAN~\cite{SPAN}, on the other hand, modifies the topology augmentation process by maximizing and minimizing the spectrum of the original graph to generate augmentation views.
However, 
%these methods have significant limitations. The 
the major drawback with these methods is that their strategy to mitigate graph structural attacks has the effect of further poisoning the graph's structure. For instance, the adversarial view of ARIEL injects adversarial noises into both the feature and topology spaces of the original
poisoned graph. Nevertheless, the adversarial noises could still have
persistent negative impact on graph representation learning, leading to limited defense performance. The same problem also exists in one of the augmented views of SPAN whose goal is to minimize the spectrum of the graph Laplacian. 

%To break through the above-mentioned bottleneck, I
To avoid the above problem, in this paper, we take a quite different angle to achieve adversarial robustness, by
%we endeavor to 
\textit{restoring} the graph's properties contaminated by adversarial attacks. Intuitively, refining the poisoned graph is more effective than adversarial training since the latter cannot omit the harmful effects of the existing adversarial noises. Besides, the observation that SPAN achieves better robustness than ARIEL is because of an augmented view generated by maximizing the spectrum of graph Laplacian, which can restore the diminished graph homophily~\cite{homophily} and thus lead to a ``cleaner" graph. Therefore, restoring the graph property is a promising and effective way to improve GCL robustness against attacks.
%Thus, it is vital to restore the graph's property to achieve robustness.
%Hence, our goal is to scrutinize the relationship between the vital graph's properties and graph structural attacks 

%In order to scrutinize the vital graph's properties, 
To this end, 
we begin by assessing the vulnerability of the GCL framework to graph structural attacks. Our focus is on identifying important graph properties that are significantly changed during the attack. Apart from the common observation that structural attacks will typically reduce graph homophily, we discover that these attacks will essentially degrade the mutual information estimation between the graph and its representations (refer to Sec.~\ref{sec-analyze-infoNCE}). This sheds light on how attacks affect the graph learning tasks from an information-theoretic perspective. Specifically, the mutual information between the graph and its representations serves as a crucial cornerstone for generating high-quality node representations within the GCL framework. The graph attacker undermines the quality of the GCL's node embeddings by reducing the alignment between the graph's information (including semantic and topology information) and the low-dimensional representations. Consequently, the poisoned GCL's node embeddings fail to capture the ``true" information of the graph data. This critical observation underscores the significance of restoring mutual information to effectively defend against such attacks.

Furthermore, GCL models are unsupervised approaches, meaning that the GCL's node embeddings are pre-trained without accessing the node labels. However, the current GCL models~\cite{GCA,ARIEL,SPAN,MVGRL,DGI} often rely on the 
\textit{validation label set} to tune the hyperparameters of the GCL framework, which contradicts the principles of unsupervised learning. 
%Therefore, we encounter two main challenges to obtaining reliable GCL node embeddings under the adversarial environment:
Therefore, training robust GCL models in a fully unsupervised setting in an adversarial environment encounters two major challenges: \textbf{1)} How to restore the diminished mutual information caused by the noisy data during the training of the GCL framework? \textbf{2)} How to determine the vital hyperparameters of the GCL framework in a fully unsupervised setting?

% Main idea of our approach.
To address the challenges, we propose a robust GCL framework termed \textbf{GCIR} that endeavors to restore diminished mutual information after structural attacks, thus achieving adversarial robustness. Specifically, \textbf{GCIR} novelty integrates a learnable \textit{sanitation view} into the framework and contrasts it with an augmented view (through feature masking or link dropping~\cite{GRACE,GCA}) to gradually restore the diminished mutual information.
%during training 
%and obtain higher quality node embeddings. 
The sanitation view is trained jointly with the GNN encoder 
%and thus forms an end-to-end training framework. 
in an end-to-end manner. 
%To tackle the second problem, 
To train the model in a fully unsupervised manner, we design an early stopping strategy based on a pseudo-normalized cut~\cite{NormCut} metric without requiring the label's information for the downstream tasks to determine the best choice of the vital hyperparameter. 


%\st{To increase persuasiveness, we tune the vital hyperparameter of \textbf{GCIR} for all the experiments. However, we still use the label information of the validation set to tune the vital hyperparameters of state-of-the-art baselines. Moreover, we use the unsupervised pseudo-normalized cut metric to report the sensitivity analysis and use the classification accuracies to serve as its contrast.}     

%We propose a robust GCL framework that integrates a homophily-driven learnable \textit{sanitation view} that can effectively prune potential malicious edges in the poisoned graph during training. Of course, the central challenge is how to effectively design such a sanitation view. To address this challenge, we first investigate the vulnerability of GCL under state-of-the-art structural attacks. This investigation yields two crucial insights. Firstly, extensive experiments demonstrate that the graph structural attacks will break an important graph property: \textit{graph homophily}~\cite{homophily, GNNhomophily} (i.e., the tendency of similar nodes to be connected). Secondly, we theoretically prove that structural attacks will adversely affect the mutual information of the graph and its representations from the information-theoretic perspective. Based on these results, we propose to learn the sanitation view under the guidance of both graph homophily and mutual information. During training, we parametrize our sanitation view by a random vector following Bernoulli distribution and employ the Gumbel-Softmax reparametrization trick~\cite{GumbelSoftmax} to optimize it. The entire framework is then optimized in an end-to-end manner, training the learnable sanitation view generator and GNN encoder simultaneously. Moreover, we implement the early stopping technique based on a crafted pseudo normalized cut~\cite{NormCut} loss to determine the best choice of the vital hyperparameter $\eta$ in an unsupervised manner.

The main contributions are summarized as follows:
\begin{itemize}
    \item We provide the vulnerability analysis of the GCL framework under graph structural attacks from an information theoretical perspective and verify that \textit{graph structural attacks will degrade the mutual information estimation between the graph and its representations}. This finding advances our understanding of attacks and provides valuable insights for developing defense approaches.
    \item We propose a novel robust GCL framework, termed \textbf{GCIR}, that achieves adversarial robustness by introducing a learnable sanitation view to restore the diminished mutual information during training. Moreover, we design an unsupervised tuning strategy to determine the best choice of the vital hyperparameter without label information. 
    \item Extensive experiments demonstrate the effectiveness of our proposed robust GCL framework across various attack scenarios.
\end{itemize}















































