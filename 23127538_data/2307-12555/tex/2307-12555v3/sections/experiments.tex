\section{Experiments}
\label{Sec-exp}
In this section, we introduce the experimental settings and conduct comprehensive experiments to validate the effectiveness and efficiency of our robust model. Specifically, we outline the questions that we seek to answer below: 
\begin{itemize}
    \item \textbf{RQ1:} Comparing with baseline models, does our model achieve better robust performances?
    \item \textbf{RQ2:} How do different components of the model contribute to its performance?
    \item \textbf{RQ3:} How do the hyperparameters in the proposed model impact the quality of the node embeddings?
\end{itemize}

\subsection{Dataset \& Settings}
\begin{table}[h]
	\centering
	\caption{Dataset statistics.}
	\label{tab-dataset}
	\resizebox{0.8\columnwidth}{!}{%
		\begin{tabular}{c|cccc}
			\toprule[1.pt]
			Datasets & \#Nodes & \#Edges & \#Classes & \#Features \\
			\hline
			Cora     & $2708$  & $5278$  & $7$   & $1433$\\ 
			Citeseer & $3327$  & $4552$  & $6$   & $3703$\\
			Cora-ML & $2995$  & $8416$ & $7$ & $2879$\\
                Photo & $7650$ & $119081$ & $8$ & $745$\\
                Computers & $13752$ & $245861$ & $10$ & $767$\\
                WikiCS & $11701$ & $215863$ & $10$ & $300$\\
			\bottomrule[1.pt]
		\end{tabular}
	}
\end{table}

We use six popular benchmark graph datasets: Cora, CiteSeer, Cora-ML, Photo, Computers and WikiCS~\cite{CitationNetwork, WikiCS, Amazon} for evaluation. To validate the quality of the node embeddings generated by GCL effectively, we train a multi-class logistic regression model with Adam optimizer~\cite{Adam} and randomly split the node labels into three groups: training, validation and testing set with the ratio as $10\%$, $10\%$ and $80\%$. 
%The logistic regression model has a learning rate of $0.01$ and $5000$ training iterations. 
%For graph clustering, the K-means clustering~\cite{KMeans} algorithm is run based on the pre-trained node embeddings derived from GCL models and we report the normalized mutual information (NMI) to quantify the robustness of the graph clustering performances.
%A two-layered GNN encoder is deployed for all the GCL models. The embedding dimension of the first layer is $128$ for all the datasets and the embedding dimension of the second layer is $32$ for Cora, CiteSeer and Cora-ML and $128$ for Photo, Computers and WikiCS. We train the GNN encoder parameters $\theta$ via Adam optimizer with the learning rate equal to $0.0005$ and the training epochs equal to $1000$. 
We run all the models $10$ times with different seeds and report the mean value for a fair comparison. The vital hyperparameter $\eta$ is tuned based on the unsupervised tuning strategy without querying the node labels. 
%All the models are deployed on an Intel 10C20T Core i9-10850K CPU with GIGABYTE RTX3090 24GB GPU.
\begin{table*}[h]
\centering
    \caption{Performances on node classification against Mettack.}
	\label{tab-defend-Mettack}
	\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{c|ccccccccccc}
\toprule[1.pt]
 Dataset & $\frac{B}{|E|}$ & PiGCL & BGRL       & DGI        & MVGRL               & GRACE       & ARIEL       & GCA         & SPAN   & SPAGCL & \textbf{GCIR}                 \\ \hline
\multirow{4}*{Cora}&5\% & 75.25 (0.9) & 66.97(1.4) & 73.41(1.0) & 75.73(0.5) & 72.46(1.8) & 74.92(1.2) & 73.80(1.1) & 74.18(0.9) & 76.26 (0.8) & \textbf{78.78(1.0)} \\
                  &10\% & 70.27 (1.3) & 60.11(1.7) & 68.03(1.8) & 71.10(1.4) & 65.37(2.8) & 71.93(1.2) & 70.26(1.3) & 68.81(1.9) & 71.33 (0.5) & \textbf{75.93(0.9)} \\
                  &15\% & 65.94 (0.6) & 51.85(3.0) & 59.70(2.7) & 64.69(1.9) & 52.62(3.1) & 52.91(4.4) & 57.89(1.5) & 58.44(3.2) & 63.83 (0.7) & \textbf{71.89(1.6)} \\
                  &20\% & 56.54 (1.8) & 44.18(4.3) & 51.43(2.3) & 57.29(2.8) & 41.17(4.0) & 43.71(3.9) & 46.03(3.0) & 45.71(2.5) & 50.96 (1.0) & \textbf{68.81(2.4)} \\
\hline
\multirow{4}*{CiteSeer}
                       &5\%  & 69.60 (1.3) & 56.90(2.7) & 66.43(1.1) & 68.93(0.9) & 69.55(2.0) & 70.79(1.7) & 67.93(1.1) & 69.08(1.3) & \textbf{71.21 (1.2)} & 70.93(1.3) \\
                       &10\% & 65.40 (0.9) & 52.30(2.6) & 60.78(2.8) & 65.01(1.2) & 63.49(2.3) & 69.14(2.1) & 60.82(1.3) & 65.18(2.6) & \textbf{69.60 (1.4)} & 69.43(1.3) \\
                       &15\% & 59.42 (2.1) & 44.44(2.7) & 53.30(2.4) & 58.51(1.3) & 53.67(2.7) & 56.46(2.3) & 52.52(1.7) & 56.08(3.5) & 62.44 (0.9) & \textbf{65.84(1.2)} \\
                       &20\% & 51.78 (0.8) & 40.89(3.2) & 49.54(1.9) & 51.69(1.6) & 47.62(3.0) & 47.93(3.2) & 48.79(1.4) & 50.68(3.0) & 54.56 (0.6) & \textbf{62.65(1.5)} \\
\hline
\multirow{4}*{Cora-ML}
                      &5\%  & 68.59 (0.8) & 64.26(1.9) & 67.06(2.9) & 70.18(2.5) & 68.65(2.0) & 71.21(1.7) & 69.84(1.7) & 69.34(0.9) & 72.33 (1.1) & \textbf{77.90(0.8)} \\
                      &10\% & 49.78 (1.8) & 53.08(2.6) & 49.34(2.1) & 54.20(2.4) & 50.49(1.7) & 51.87(1.8) & 46.81(1.9) & 50.44(1.2) & 55.60 (0.9) & \textbf{71.57(1.7)} \\
                      &15\% & 39.19 (1.3) & 44.09(3.6) & 42.21(1.5) & 45.96(3.3) & 40.89(3.4) & 41.11(2.3) & 37.17(3.6) & 41.77(2.4) & 47.24 (1.3) & \textbf{64.56(1.8)} \\
                      &20\% & 29.45 (0.89) & 37.41(3.3) & 34.35(0.4) & 37.40(2.3) & 31.57(3.3) & 28.52(2.2) & 26.69(2.1) & 33.46(2.5) & 34.79 (1.3) & \textbf{56.43(2.5)} \\
\hline
\multirow{4}*{Photo}
                    &5\%  & 70.12 (1.7) & 66.50(2.8) & 62.24(3.4) & 73.14(1.3) & 70.87(1.1) & 69.85(2.4) & 76.45(2.1) & 72.46(2.3) & 70.07 (2.0) & \textbf{78.37(2.2)} \\
                    &10\% & 50.75 (2.3) & 52.62(3.9) & 47.76(3.7) & 58.63(3.3) & 51.39(1.5) & 53.92(1.8) & 60.57(2.0) & 63.46(0.4) & 56.98 (1.5) & \textbf{67.80(2.4)} \\
                    &15\% & 43.70 (1.) & 43.56(2.4) & 40.73(3.6) & 49.82(2.0) & 44.37(1.6) & 45.98(1.8) & 51.66(1.4) & 50.02(0.4) & 49.87 (0.9) & \textbf{59.19(1.5)} \\
                    &20\% & 27.35 (1.5) & 35.68(3.5) & 37.91(4.0) & 45.00(2.0) & 32.45(1.4) & 34.19(1.8) & 45.35(1.3) & 46.81(3.4) & 35.03 (1.3) & \textbf{54.68(2.9)} \\
\hline
\multirow{4}*{Computers}
                        &5\%  & 62.78 (1.5) & 63.12(1.3) & 63.91(1.9) & 69.16(0.6) & 66.22(1.0) & 67.51(1.7) & 68.02(0.9) & 64.06(1.4) & 64.85 (1.1) & \textbf{72.25(2.2)} \\
                        &10\% & 59.70 (2.5) & 61.33(2.1) & 57.06(1.9) & 64.58(0.4) & 58.07(2.0) & 57.04(1.5) & 58.90(2.1) & 54.81(1.3) & 57.68 (1.1) & \textbf{66.14(2.1)} \\
                        &15\% & 54.26 (0.7) & 52.59(3.4) & 51.40(2.0) & 54.99(2.6) & 52.58(2.1) & 43.65(2.6) & 52.90(2.6) & 45.63(3.2) & 54.82 (0.8) & \textbf{59.93(3.8)} \\
                        &20\% & 49.89 (1.3) & 37.61(2.3) & 41.54(4.9) & 44.98(3.5) & 37.21(3.9) & 34.71(1.7) & 38.81(3.6) & 37.39(2.1) & 42.87 (0.6) & \textbf{51.13(4.1)} \\
\hline
\multirow{4}*{WikiCS}
                     &5\%  & 41.58 (3.12) & 40.91(1.0) & 43.71(2.2) & 42.17(1.8) & 41.30(1.1) & 43.92(1.0) & 41.67(0.8) & 40.60(1.1) & 43.70 (1.3) & \textbf{55.93(0.8)} \\
                     &10\% & 32.27 (1.87) & 29.93(0.9) & 33.88(2.7) & 30.73(0.7) & 30.89(1.1) & 33.55(1.4) & 31.18(0.7) & 28.48(0.7) & 30.56 (1.2) & \textbf{49.30(0.6)} \\
                     &15\% & 25.68 (2.2) & 24.35(0.7) & 27.10(2.0) & 24.54(1.1) & 25.40(0.4) & 27.37(0.6) & 25.25(0.5) & 23.32(0.7) & 27.32 (1.5) & \textbf{42.96(1.1)} \\
                     &20\% & 23.83 (1.84) & 21.60(1.8) & 23.79(1.3) & 22.38(0.8) & 23.23(0.4) & 24.56(0.7) & 23.03(0.7) & 21.79(0.5) & 23.94 (0.9) & \textbf{36.47(1.0)} \\
\bottomrule[1.pt]
\end{tabular}}
\end{table*}

\begin{table*}[h]
\centering
    \caption{Performances on node classification against CLGA.}
	\label{tab-defend-CLGA}
	\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{c|ccccccccccc}
\toprule[1.pt]
 Dataset & $\frac{B}{|E|}$ & PiGCL & BGRL       & DGI        & MVGRL               & GRACE       & ARIEL       & GCA         & SPAN   & SPAGCL     & \textbf{GCIR}                 \\ \hline
\multirow{4}*{Cora}
&5\%      & 79.72 (0.9) & 70.78(1.1) & 78.96(1.3) & \textbf{80.09(0.9)} & 78.95(0.9) & 78.47(0.7) & 78.55(0.7) & 78.96(0.7) & 77.46 (1.0) & 79.13(0.6)           \\
&10\%     & 78.11 (1.2) & 68.35(1.9) & 75.64(1.2) & 77.92(0.9)          & 76.79(1.4) & 76.31(0.9) & 77.15(0.8) & 77.22(1.1) & 75.93 (0.9) & \textbf{78.62(1.1))} \\
&15\%     & 77.01 (2.3) & 66.15(1.3) & 73.72(0.7) & 75.89(1.0)          & 75.38(1.1) & 74.75(0.9) & 74.71(1.2) & 76.11(1.2) & 74.65 (1.6) & \textbf{78.18(1.0)}  \\
&20\%     & 75.20 (1.8) & 64.44(1.8) & 71.94(1.7) & 74.64(0.8)          & 73.13(1.1) & 74.44(1.2) & 71.78(1.0) & 74.43(1.6) & 73.74 (1.4) & \textbf{76.82(1.2)} \\
\hline
\multirow{4}*{CiteSeer}
&5\%      & 68.95 (2.3) & 58.52(1.3) & 66.24(1.5) & 68.60(1.2) & 69.38(1.2) & 70.43(1.2) & 67.92(1.0)          & 70.38(1.1) & 
70.14 (1.4) & \textbf{71.18(1.2)} \\
&10\%     & 68.48 (1.8) & 56.27(1.3) & 65.62(1.5) & 67.65(1.0)          & 67.80(1.0) & 69.25(2.1) & 65.43(1.1)          & 68.74(1.3) & 
68.36 (0.9) & \textbf{70.11(0.9)} \\
&15\%     & 67.30 (1.5) & 54.09(2.0) & 63.75(1.6) & 66.29(0.8)          & 67.52(1.6) & 66.77(2.4) & 62.43(1.7)          & 67.71(1.3) & 
67.18 (1.0) & \textbf{69.58(1.5)} \\
&20\%     & 64.10 (1.8) & 51.24(1.9) & 62.55(1.5) & 64.24(1.3)          & 65.68(1.7) & 64.99(2.1) & 60.52(1.7)          & 66.62(1.4) & 
66.58 (1.1) & \textbf{67.90(1.1)} \\
\hline
\multirow{4}*{Cora-ML}
&5\%      & 78.38 (1.2) & 70.38(2.1) & 77.91(0.9) & 78.62(0.8) & 79.16(1.0) & 79.27(0.6) & 77.53(0.7)          & 79.36(1.0) & 
79.18 (1.3) & \textbf{80.02(0.8)} \\
&10\%     & 77.89 (0.7) & 68.75(1.7) & 76.40(1.1) & 76.29(0.9)          & 77.34(1.0) & 77.87(0.7) & 76.18(0.9)          & 77.96(0.8) & 
77.62 (1.3) & \textbf{78.68(0.9)} \\
&15\%     & 76.77 (1.6) & 67.38(1.1) & 75.46(1.4) & 74.61(0.6)          & 76.54(0.8) & 76.97(0.9) & 74.09(1.2)          & 77.10(0.6) & 
76.38 (0.8) & \textbf{77.34(0.9)} \\
&20\%     & 75.71 (1.2) & 65.09(2.0) & 73.42(1.7) & 73.40(0.8)          & 75.36(0.8) & 75.36(1.3) & 72.54(0.9)          & 75.99(0.5) & 
75.22 (1.5) & \textbf{76.72(0.8)} \\
\hline
\multirow{4}*{Photo}
&5\%      & 77.78 (1.2) & 80.79(1.4) & 75.80(2.8) & 84.86(0.7) & 81.91(0.8) & 82.28(1.0) & 82.25(1.1) & 84.18(0.8) & 83.62 (0.6) & \textbf{87.09(0.6)} \\
&10\%     & 75.33 (0.9) & 79.11(1.0) & 72.74(3.4) & 82.27(0.9) & 77.49(1.0) & 78.12(0.9) & 79.17(0.8) & 82.21(0.7) & 81.86 (0.9) & \textbf{85.04(0.5)} \\
&15\%     & 70.97 (1.8) & 77.83(0.9) & 68.99(0.5) & 80.39(0.7) & 75.28(1.0) & 76.98(0.6) & 76.68(0.8) & 80.78(0.5) & 80.83 (1.1) & \textbf{82.11(1.1)} \\
&20\%     & 67.15 (1.6) & 76.97(0.8) & 68.59(2.0) & 78.88(0.8) & 71.85(1.2) & 74.92(1.3) & 73.10(0.7) & 78.90(0.9) & 77.66 (1.1) & \textbf{79.21(2.2)} \\
\bottomrule[1.pt]
\end{tabular}}
\end{table*}

% Figure environment removed

\subsection{Baselines}
\label{Sec-Baselines}
To thoroughly validate the robustness of our proposed method, we have conducted a comparative analysis with several state-of-the-art graph contrastive learning models: BGRL~\cite{BGRL}, DGI~\cite{DGI}, MVGRL~\cite{MVGRL}, GRACE~\cite{GRACE}, GCA~\cite{GCA}, ARIEL~\cite{ARIEL}, and SPAN~\cite{SPAN}. Among them, ARIEL and SPAN are known for their robustness in GCL. Meanwhile, it is worth noting that the defense methods proposed in \cite{GRV, LocalGlobal} require access to the \textit{clean} graph, which is not available in a more practical setting such as ours, ARIEL and SPAN. We list the brief introduction of these baselines as follows:
\begin{itemize}
    \item \textbf{BGRL}~\cite{BGRL} It introduces the Bootstrapped Graph Latents technique to improve the scalability of graph contrastive learning effectively. It contrasts the two embeddings from distinct encoders rather than augmentation views, in which one encoder is the exponential moving average of the other. 
    \item \textbf{DGI}~\cite{DGI} It relies on maximizing the mutual information between patch representations and corresponding high-level summaries of graphs, such as the readout mapping of the node embeddings. 
    \item \textbf{MVGRL}~\cite{MVGRL} It learns node and graph level representations by contrasting low-dimensional embeddings from two augmented views of graphs, including first-order neighbors and a graph diffusion. 
    \item \textbf{GRACE}~\cite{GRACE} It is an unsupervised graph representation learning that generates two augmentation views by corruption and learns node representations by maximizing the agreement of node representations in these two views.
    \item \textbf{ARIEL}~\cite{ARIEL} it leverages the adversarial training strategy to construct the adversarial view with information regularization to achieve adversarial robustness. 
    \item \textbf{GCA}~\cite{GCA} It is a graph contrastive learning framework that employs more complex adaptive augmentation schemes such as degree centrality, eigenvector centrality, and PageRank centrality.
    \item \textbf{SPAN}~\cite{SPAN} It introduces two augmentation views to minimize and maximize the spectrum of the augmented graphs to preserve the spectrum invariance of the GCL.
    \item \textbf{PiGCL}~\cite{PiGCL} It is a graph contrastive learning framework that dynamically captures the implicit conflicts from the negative pairs during training by detecting the gradient of representation similarities to eliminate the conflict phenomenon caused by the InfoNCE loss.
    \item \textbf{SPAGCL}~\cite{SPAGCL} It is a graph contrastive learning framework that combines the node similarity-preserving view and an adversarial view to improve the node similarity during the adversarial training.
\end{itemize}

%To ensure a rigorous comparison, we have used the same hyperparameter settings as reported in the corresponding literature, where all the baselines are fine-tuned based on the performance of the validation set. 

\subsection{Robustness on Node Classification}
\label{Sec-Robust-NC}
The objective of this paper is to examine the adversarial robustness of our proposed model against two common graph structural attacks: Mettack~\cite{Mettack} (a graph structural attack with GNN as its surrogate model) and CLGA~\cite{CLGA} (a graph structural attack with GCL as its surrogate model). 
\subsubsection{Against Mettack}
Tab.~\ref{tab-defend-Mettack} displays the accuracies of various GCL models for the semi-supervised node classification under different attacking powers. In particular, the attacking power is defined as the percentage $\frac{B}{|E|}$, where $B$ is the attack budget (number of modified edges) and $|E|$ is the total edge number of the clean graph. To cover most attacking scenarios, we have chosen the attacking power from $\{5\%, 10\%, 15\%, 20\%\}$, which is consistent with the prior literature~\cite{Mettack, CLGA}. 

The results in Tab.~\ref{tab-defend-Mettack} indicate that Mettack affects all GCL methods, although it was originally designed to attack vanilla GNNs. This is because the GCL models also rely on the GNN encoder to convert the graph data into low-dimensional embeddings, and thus, Mettack can indirectly compromise the quality of the GCL's node embeddings.
%by decreasing the homophily degree of the clean graph.  

The second observation is that our proposed model outperforms the other baselines across various levels of attacking power. Moreover, the performance gap between our model and other baselines widens as the attacking power grows from $5\%$ to $20\%$. For example, on the Cora dataset, the performance gaps between our model with GRACE~\cite{GRACE}, ARIEL~\cite{ARIEL}, GCA~\cite{GCA}, SPAN~\cite{SPAN}, BGRL~\cite{BGRL}, DGI~\cite{DGI} and MVGRL~\cite{MVGRL} are $8.02\%$, $4.90\%$, $6.32\%$, $5.84\%$, $14.99\%$, $6.82\%$ and $3.87\%$ when the attacking power is $5\%$. The corresponding margins under $20\%$ attacking power are $40.17\%$, $36.47\%$, $33.11\%$, $33.57\%$, $35.79\%$, $25.26\%$, $16.74\%$. This can be attributed to the fact that as the attacking power grows, more malicious links are inserted. However, the sanitation view in our proposed model can effectively eliminate a greater number of malicious edges from the heavily poisoned graph, thereby improving the quality of node embeddings and making them more distinguishable across different classes. 
%In addition to that, it is observed that in certain scenarios, our model cannot surpass other strong baselines on clean graphs. For instance, on the CiteSeer dataset, the node classification accuracy of our model is slightly lower than that of GCA and MVGRL by $0.57\%$ and $0.55\%$. This phenomenon is common in the field of adversarial machine learning and can be attributed to the trade-off between expressive power and the robustness of the model~\cite{robustness_tradeoff, zhang2019theoretically}. 

\subsubsection{Against CLGA}
Tab.~\ref{tab-defend-CLGA} presents the robustness results under the unsupervised attack CLGA. Similar to its performance under Mettack, our model outperforms other baselines across various levels of attacking power. However, since CLGA is an unsupervised attack, the degree of decreasing node classification accuracy for GCL models is not as significant as in Mettack, which specifically targets the test set. 
We note that MVGRL performs well against CLGA and even outperforms our method when the attacking power is equal to $5\%$ on the Cora dataset. This is because  MVGRL does not use the InfoNCE loss for training, which is the attack objective of CLGA. Nevertheless, our model outperforms MVGRL on the Cora dataset when the attacking power is larger than or equal to $10\%$. Overall, the proposed learnable sanitation view effectively promotes the robustness of the vanilla GCL framework by automatically detecting and pruning potential malicious edges in the poisoned graph, as demonstrated by the robust performance results shown in Tab.~\ref{tab-defend-Mettack} and Tab.~\ref{tab-defend-CLGA}.

% Figure environment removed

% Figure environment removed

\subsection{Ablation Study}
In this section, we present the ablation study to verify the effectiveness of two vital components of the loss function defined in Eqn.~\ref{eqn-san-obj-2}, corresponding to restoring the mutual information between the graph and its representations and the graph homophily respectively. In Particular, we introduce two model variants as follows:
\begin{itemize}
    \item \textbf{GCIR} w.o. $\mathcal{L}_{info}$: we remove the InfoNCE loss in Eqn.~\ref{eqn-san-obj-2} and first train the sanitation view to optimize $\delta_{x}$. Then, we train the GCL model with the pre-trained sanitation view. 
    \item \textbf{GCIR} w.o. $\delta_{x}$: we remove the $\delta_{x}$ term in Eqn.~\ref{eqn-san-obj-2} and only optimize the InfoNCE loss in \textbf{GCIR}.
\end{itemize}
The results are presented in Fig.~\ref{fig-Ablation}. Firstly, comparing the robust performance of \textbf{GCIR} w.o. $\mathcal{L}_{info}$, \textbf{GCIR} w.o. $\delta_{x}$ and GRACE, we find out that both optimizing the InfoNCE loss and the graph homophily $\delta_{x}$ can enhance the robustness of the GCL model since in most cases the two model variants outperform GRACE. Secondly, the comparison of \textbf{GCIR} w.o. $\mathcal{L}_{info}$ and \textbf{GCIR} w.o. $\delta_{x}$ under CLGA reveals that restoring the diminished mutual information will be more effective than the graph homophily. Thirdly,  the comparison of \textbf{GCIR} w.o. $\mathcal{L}_{info}$ and \textbf{GCIR} w.o. $\delta_{x}$ under Mettack indicates that restoring the graph homophily will be more effective than the mutual information. The reason is probably that Mettack is such attacks that degenerate the GNN's performance, which does not directly rely on information theory.  

\subsection{Sensitivity Analysis}
In this section, we discuss the tuning strategy of the vital hyperparameter $\eta$ in Eqn.~\ref{eqn-san-obj-2}. To mimic the real environment, it is assumed that the defender cannot acquire the attacking scenarios (such as the attack type, attack degree, and victim nodes or edges and etc.), and the label information of the given graph. That is, the defender can only utilize the attribute matrix, the suspected perturbed adjacency matrix, and the node embeddings of GCL to determine the best choice of $\eta$. To tackle this issue, as previously mentioned in Sec.~\ref{sec-unsupervised-tuning}, we craft a pseudo normalized cut loss $\mathcal{L}_{pnc}^{*}$ to tune the vital hyperparameter $\eta$.
To be specific, we report the best pseudo normalized cut loss $\mathcal{L}_{pnc}^{t^{*}}$ and the node classification accuracy with attack power equal to $10\%$ for CLGA and Mettack as exemplar and present the results in Fig.~\ref{fig-Sensitivity}. Moreover, we tune the hyperparameter $\eta$ ranged in $[0,0.0001,0.001,0.01,0.1,1,3,5].$ The experiments demonstrate that too large and too small $\eta$ may lead to sub-optimal performance on \textbf{GCIR} under different attacking scenarios. Alternatively, for all the cases listed in Fig.~\ref{fig-Sensitivity} $\eta=1$ can achieve the minimum $\mathcal{L}_{pnc}^{t^{*}}$ (coincide with the best node classification accuracy). %\textcolor{blue}{It is noticed that in some cases when $\eta\in\{0.001, 0.01\}$ the $\mathcal{L}_{pnc}^{t^{*}}$ remains unchanged reveal that it is insensitive to the extremely low value of the vital hyperparameter $\eta$. Alternatively, when $\eta$ increases to more than $0.1$ we could observe significant changes in the traceplot.}  

% Figure environment removed

\subsection{Node Embeddings Visualization}
\label{sec-node-embs-visualization}
We provide the node embeddings visualization for \textbf{GCIR}, SPAN, and ARIEL (robust GCLs) to qualitatively evaluate the quality of node embeddings under different attacking scenarios (CLGA and Mettack with attacking power equal to $20\%$) in Fig.~\ref{fig-visual-embs}. The visualization is depicted on the transformed node embeddings generated by \textbf{GCIR}, SPAN, and ARIEL via t-SNE~\cite{tSNE}. Clearly, the node embeddings for \textbf{GCIR} are more discriminative than SPAN and ARIEL and the clusters of \textbf{GCIR} are more cohesive than the others. 

\subsection{Robustness on Graph Representation Vulnerability}
In this section, we introduce the concept of graph representation vulnerability and its relationship with the evaluation of the GCL's robustness. We then test the robustness of the robust GCLs based on the defined metric from an information-theoretical perspective. 

\begin{definition}[GRV~\cite{GRV, LocalGlobal}]
\label{lemma-GRV}
    The graph representation vulnerability (GRV) quantifies the discrimination between the mutual information of the graph and its node representations based on clean graph and poisoned graph:
    \begin{equation}
        \begin{split}
            GRV(G,G^p)=I(G;f_{\theta}(G))-I(G^p;f_{\theta}(G^p)).
        \end{split}
    \end{equation}
\end{definition}
Basically, GRV can be utilized as a metric to quantify the adversarial robustness of an embedding-generating model $f_\theta$. Intuitively, when GRV is smaller, the embeddings generated from the poisoned graph are closer to those generated from the clean graph; that is, the model $f_\theta$ is more robust.  
%}
\begin{definition}[Defense of GCL]
\label{def-GCL-defense}
Given the poisoned graph generated by the graph attacker $G^{p}=\{\mathbf{X}, \mathbf{A}^{p}\}$, the defender aims to train a robust model $f_{\theta^{*}}$
%the node embeddings $\mathbf{Z}^{*}=f_{\theta^{*}}(\mathbf{X}, t^{*}(\mathbf{A}^{p}))$ which 
that can minimize the GRV defined in Definition~\ref{lemma-GRV}:
\begin{equation}
    \label{eqn-infomax}
    \begin{split}
        %\mathbf{Z}^{*}=\arg\min I(\mathbf{X},f_{\theta}(G))-I(\mathbf{X},\mathbf{Z}^{*}(G^p)),
        f^{*}=\arg\min_{f\in\mathcal{F}} I(G;f_{\theta}(G))-I(G^p;f_{\theta}(G^p)).
    \end{split}
\end{equation}

where $I(\cdot,\cdot)$ represents the mutual information, $f_{\theta}(\cdot)$ represents the GNN encoder parametrized by weights $\theta$. 
%$t(\cdot)$ denotes the learnable augmentation view. 
\end{definition}
We note that the clean graph $G$ required to compute GRV is not available during the learning process. Thus, GRV is solely used as a robustness metric in the test phase in addition to the performances over downstream tasks.

We then validate the robust performances of our proposed method with two robust baselines from an information-theoretical perspective. Specifically, we utilize GRV~\cite{GRV, LocalGlobal} mentioned in Def.~\ref{lemma-GRV} to indirectly quantify the GCL's robustness. It is worth noting that GRV measures the difference between the mutual information of the node embeddings for the clean graph and the poisoned graph. Here we utilize the InfoNCE loss to approximate the mutual information. We emphasize that we only use the clean graph's information on computing GRV instead of training the robust model and hence prevent the information leakage. We compare \textbf{GCIR} with SPAN and ARIEL in this part and report the GRVs for each robust model in Fig.~\ref{fig-GRV}. A lower GRV indicates better robustness. It is observed that the \textbf{GCIR} achieves the minimum GRV for both CLGA and Mettack, demonstrating that the node embeddings of \textbf{GCIR} are more robust than those of the other models from an information-theoretical perspective. 

\subsection{Time Cost}
\begin{table}[h]
	\centering
	\caption{Training time per epoch (s).}
	\label{tab-time}
	\resizebox{0.95\columnwidth}{!}{%
	\begin{tabular}{cccccc|ccc}
        \toprule[1.pt]
Dataset & BGRL  & DGI   & MVGRL & GRACE & GCA   & ARIEL & SPAN  & \textbf{GCIR}  \\
\hline
Cora      & $0.011$ & $0.005$ & $0.008$ & $0.008$ & $0.009$ & $0.084$ & $0.245$ & $0.045$ \\
CiteSeer  & $0.017$ & $0.006$ & $0.011$ & $0.010$ & $0.009$ & $0.068$ & $0.174$ & $0.041$ \\
Cora-ML & $0.029$ & $0.006$ & $0.013$ & $0.015$ & $0.011$ & $0.111$ & $0.310$ & $0.067$ \\
Photo & $0.031$ & $0.010$ & $0.019$ & $0.038$ & $0.051$ & $1.056$ & $4.347$ & $0.468$ \\
Computers & $0.059$ & $0.023$ & $0.031$ & $0.099$ & $0.090$ & $5.198$ & $8.694$ & $2.293$ \\
WikiCS & $0.043$ & $0.018$ & $0.026$ & $0.067$ & $0.066$ & $4.085$ & $7.394$ & $1.360$ \\
        \bottomrule[1.pt]
        \end{tabular}}
\end{table}

Tab.~\ref{tab-time} presents the time required for each iteration of the training phase of the GCL models. The last three columns show the time cost of using the learnable augmentation views. It is observed that the GCL model with learnable augmentation views requires more computation time than the others. Moreover, our model consumes the least amount of computation time compared to the other two models with learnable augmentation views.  

\subsection{Discussions on Heterophilic Graphs}

% Figure environment removed

In this section, we investigate the adversarial robustness of graph contrastive learning methods on heterophilic graphs. We take two typical heterophilic graphs, i.e., Chameleon and Squirrel~\cite{chameleon} as examplars. Similarly, we deploy Mettack to inject adversarial noises into the clean heterophilic graphs with different attacking powers and evaluate the adversarial robustness of GCLs on the poisoned graphs. The experimental results in Fig.~\ref{fig-heter} demonstrate that our proposed method can still consistently outperform other baselines on heterophilic graphs. It is worth noting that the hyperparameter $\eta$ in Eqn.~\ref{eqn-san-obj-2} is tuned to be zero under this case. It is due to the fact that the feature smoothness is invalid for heterophilic graphs whose node pairs are dissimilar in the majority. Hence, the proposed graph contrastive learning framework solely relies on the information restoration mechanism to achieve robustness. In future work, we will follow this research line and investigate how to further enhance the adversarial robustness of the graph contrastive learning framework over heterophilic graphs while maintaining its robustness over homophilic graphs. 













