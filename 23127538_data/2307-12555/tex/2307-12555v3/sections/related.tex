\section{Related Works}
\label{sec-related}
\subsection{Graph Structural Attack}
Graph-based machine learning models have been shown to be vulnerable to structural attacks. 
%Nettack~\cite{Nettack} was the first targeted attack on GNNs that manipulated both on node attribute matrix and adjacency matrix. 
Mettack~\cite{Mettack} formulated the global structural poisoning attacks on GNNs as a bi-level optimization problem and leveraged a meta-learning framework to solve it. 
%BinarizedAttack~\cite{BinarizedAttack} simplified graph poisoning attacks against the graph-based anomaly detection to a one-level optimization problem and optimize it by mimicking the training of the binary neural network. 
%HRAT~\cite{HRAT} proposed a heuristic optimization model integrated with reinforcement learning to optimize the structural attacks against Android Malware Detection. 
CLGA~\cite{CLGA} deployed GCA~\cite{GCA} as the surrogate model and formulated graph structural attacks as a bi-level optimization problem to degenerate the performance of GCL models through poisoning. There are other structural attacks against graph-based machine learning models~\cite{BinarizedAttack, HRAT, TopologyAttack, bojchevski2019adversarial} such as graph anomaly detection, malware detection, random walks, etc. 

\subsection{Graph Contrastive Learning}
Contrastive learning is a widely used deep learning model that originated from SimCLR~\cite{SimCLR}, a simple self-supervised contrastive framework for learning useful representations for image data. 
%SimCLR introduces the infoNCE loss to approximate the mutual information between latent representations from different augmentation views, such as randomly cropping and resizing, rotation, Gaussian blurring and color distortion. 
GRACE~\cite{GRACE} extended the SimCLR framework to graph data by generating augmentation views through random link removal and feature masking. Later, GCA~\cite{GCA} prevents the removal of the important links during the stochastic augmentations by designing several link removal mechanisms based on degree centrality, etc. ARIEL~\cite{ARIEL} introduced an adversarial view via a PGD attack for robust training. SPAN~\cite{SPAN} explored spectrum invariance during augmentation views and generated augmentation views by maximizing and minimizing the spectral change. MVGRL~\cite{MVGRL} utilized a local-global contrastive loss to measure the agreement between the neighbor nodes and graph diffusion. DGI~\cite{DGI} generated augmentation views by corrupting the original graph and contrasting the node embeddings between the original graph and the corrupted graphs. BGRL~\cite{BGRL} introduced bootstrapped graph latent training by predicting alternative augmentations of the input without contrasting with negative samples. SPAGCL~\cite{SPAGCL} integrated the node similarity-preserving view and the adversarial view to enhance the adversarial robustness of GCL model. PiGCL~\cite{PiGCL} endeavored to mitigate the implicit conflicts from the negative pairs caused by the InfoNCE loss of GCL to boost its robust performance. GPS~\cite{GPS} deployed a special pooling mechanism to enhance the adversarial robustness of GCL on the graph classification problem.

However, the above-mentioned (robust)-GCL methods contain their unique limitations to some extent. For example, the adversarial view of ARIEL and SPAGCL injects adversarial noises into the feature and topology space of the graph data during training, which will still hinder the robust learning of GCL. The view that minimizes the spectral norm will introduce more inter-class links and produce similar detrimental effects on the graph representation learning. On the other hand, although PiGCL endeavors to improve the robustness of GCL by refining the gradients of the negative pairs, it still cannot essentially erase the adverse impact from the topology space of the graph data. Lastly, the adversarial pooling of GPS is specifically designed to boost the adversarial robustness of GCL on the graph classification task, which falls outside the scope of our topic (robustness of node classification). It is worth noting that unlike other baselines, our work focuses on sanitizing the detrimental effects of the adversarial attacks on the input space, which is supervised by a specially crafted sanitation view of the GCL framework from an information-theoretical perspective.    

%\subsection{Graph Sanitation}
%GASOLINE~\cite{GASOLINE} first proposed the concept of graph sanitation, which addresses the problem of improving the quality of a given graph for a graph mining task. To tackle this problem, the author proposed a bi-level graph structural learning framework that sanitized the noisy graph for better semi-supervised node classification performance via deploying the GNN as its surrogate model. Later, FocusedCleaner~\cite{FocusedCleaner} further extended the graph sanitation problem to the graph security field and proposed a poisoned graph sanitation framework via jointly training a bi-level structural learning module with a victim node detection module to enhance the robustness of GNN. However, these graph sanitation algorithms are specially crafted for the semi-supervised node classification task and are not suitable for the unsupervised GCL. To boost the robustness of the GCL models, we introduce a homophily-preserving learnable sanitation view to scholastically produce more sanitized graph samples during training and thus improve the quality of the node embeddings under various attacking scenarios. 




































