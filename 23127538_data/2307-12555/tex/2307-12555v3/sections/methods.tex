\section{Preliminaries}
\label{sec-preliminaries}
In this section, we will provide a brief introduction to the prerequisite knowledge and notations for our work.  

\subsection{Graph Representation Learning}
Given an attribute graph $\mathcal{G}=\{\mathbf{X},\mathbf{A}\}$, where $\mathbf{X}\in\mathbbm{R}^{N\times p}$ is the nodal attribute matrix and $\mathbf{A}\in\mathbbm{R}^{N\times N}$ is the adjacency matrix, graph representation learning aims at training a GNN encoder $f_{\theta}: \mathcal{G}\rightarrow \mathbbm{R}^{N\times d}$ to produce low-dimensional embeddings $\mathbf{H}\in\mathbbm{R}^{N\times d}$ for each node. Then, the pre-trained node embeddings can be fed into a graph-related downstream task such as node classification. $\theta$ summarizes the model parameters to be trained via a pre-defined loss function.     

\subsection{Graph Contrastive Learning Framework}
GCL is a powerful graph representation learning framework that seeks to obtain high-quality node embeddings via maximizing the intractable mutual information between the graph data and its low-dimensional representations, i.e., 
\begin{equation}
    I(G, \mathbf{Z})=\int_{z}\int_{g}p(g,z)\log(\frac{p(g,z)}{p(g)p(z)})dgdz.
    \nonumber
\end{equation}
In order to achieve this goal, a common way is to maximize the similarity between positive pairs (same node with different views) and enlarge the difference between negative pairs (different nodes within the same view and cross different views).  
%More specifically, 
In general, the training of the GCL model has two stages. Firstly, it generates two augmentation views $G_1$ and $G_2$ based on the input graph $G$ via random link removal and feature masking. Then, Two graph samples generated from augmentation views are fed into a shared GNN encoder to produce node embeddings. Finally, a contrastive loss (InfoNCE loss) based on the node embeddings is optimized. 
%{\color{red} this sentence is too long and not clear: More specifically, given a graph $\mathcal{G}$, graph contrastive learning first generates two different graph views $G_{1}=t_{1}(\mathcal{G})$ and $G_{2}=t_{2}(\mathcal{G})$ where $t_1\in\mathcal{T}_1$ and $t_2\in\mathcal{T}_2$ by implementing stochastic graph augmentations $\mathcal{T}_1$ and $\mathcal{T}_2$ on graph data to feed into a shared GNN encoder $f_{\theta}(\cdot)$ and then utilizes a crafted graph contrastive loss, i.e., infoNCE~\cite{infoNCE} to enforce the embeddings of each node for different views to be similar and enlarge the distance between different nodes.} 
The InfoNCE loss is formulated as:
\begin{equation*}
	%\small
    \begin{split}
        &\mathcal{L}_{info}(G_1, G_2)=\frac{1}{2N}\sum_{i=1}^{N}(l(u_i,v_i)+l(v_i,u_i)), \\
        &l(u_i,v_i)= \\
        &-\log\frac{e^{\rho(u_i,v_i)/\tau}}{\begin{matrix}
        \underbrace{e^{\rho(u_i,v_i)/\tau}} \\ \text{positive}
        \end{matrix}+
        \begin{matrix}
        \underbrace{\sum_{k\neq i}e^{\rho(u_i,v_k)/\tau}} \\ \text{inter-view negative}
        \end{matrix}+
        \begin{matrix}
        \underbrace{\sum_{k\neq i}e^{\rho(u_i,u_k)/\tau}} \\ \text{intra-view negative}
        \end{matrix}},
    \end{split}
\end{equation*}
where $u_{i}$ is the $i$-th node for $G_1$, $v_{i}$ is the $i$-th node for $G_2$, and $\tau$ is a temperature hyperparameter. Usually, the similarity function $\rho(\cdot)$ is defined as: 
\begin{equation*}
    \begin{split}
        \rho(u_i,v_i)=cos(g(\mathbf{H}_{1i}), g(\mathbf{H}_{2i}))=\frac{g(\mathbf{H}_{1i})\cdot g(\mathbf{H}_{2i})}{\|g(\mathbf{H}_{1i})\|\cdot\|g(\mathbf{H}_{2i})\|},
    \end{split}
\end{equation*}
which represents the cosine similarity between the projected embeddings of $u_i$ and $v_i$, $g(\cdot)$ is the projection head to enhance the expressive power of the graph representation learning, $\mathbf{H}$ is the node embeddings to be detailed later. We use the two-layered graph convolution~\cite{GNN} as the shared GNN encoder for both two views to get the node embeddings $\mathbf{H}_1=f_{\theta}(G_1)$ and $\mathbf{H}_2=f_{\theta}(G_2)$ respectively. Specifically, $G_1=\{\mathbf{A}_1, \mathbf{X}_1\}$ and $G_2=\{\mathbf{A}_2, \mathbf{X}_2\}$; then we have
\begin{equation}
    \begin{split}
        &\mathbf{H}_l=f_{\theta}(\mathbf{A}_l,\mathbf{X}_l)=\sigma(\hat{\mathbf{A}}_l\sigma(\hat{\mathbf{A}}_l\mathbf{X}_{l}\mathbf{W}^{(1)})\mathbf{W}^{(2)}), \forall  l=1,2, \\
        &\text{where} \ \hat{\mathbf{A}}=\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}, \  \tilde{\mathbf{A}}=\mathbf{A+I}, \  \tilde{D}_{ii}=\sum_{j}\tilde{A}_{ij}.
    \end{split}
    \label{eqn-GNN-encoder}
\end{equation}
After that, the graph contrastive learning framework is formulated as follows:
\begin{equation*}
    \begin{split}
        \min_{\theta} \ \mathcal{L}_{info}(t_1(\mathcal{G}), t_2(\mathcal{G}), \theta), \ \text{where} \  t_l\in\mathcal{T}_l, \  l=\{1,2\}.
    \end{split}
\end{equation*}
Then, the optimized node embeddings $\mathbf{H}^{*}=f_{\theta^{*}}(\mathbf{A},\mathbf{X})$ serve as the input to the logistic regression for semi-supervised node classification.

%\subsection{Normalized Cut}
%\label{sec-preliminaries-NormalizedCut}
%Normalized cut loss~\cite{NormCut, maskGVAE, GAP, shi2000normalized, ng2001spectral} is an unsupervised loss function that hcounts  the fraction of the inter-class links for a given graph partition result. Thus, a good graph partition should obtain a lower normalized cut loss, i.e., the majority of the links connect two nodes with the same class. The normalized cut loss is defined as:
%\begin{equation}
%    \begin{split}
%        &\mathcal{L}_{nc}=\frac{1}{K}\Tr((\mathbf{C}^{\top}\mathbf{LC})\oslash(\mathbf{C}^{\top}\mathbf{DC})), \ \mathbf{L}=\mathbf{I}-\hat{\mathbf{A}}.
%    \end{split}
%    \label{eqn-normalized-cut}
%\end{equation}
%Here $\mathbf{C}$ is the cluster assignment matrix, i.e., $C_{ij}$ represents the node $i$ belongs to the cluster $j$, $\mathbf{L}$ is the graph Laplacian matrix, $\oslash$ represents element-wise division. 
%{\color{red}Explain notation in the above equation} 
%In this paper, we use this normalized cut loss as supervision to tune the hyperparameters in the model to eliminate the usage of the validation node labels. Thus, our model is a totally unsupervised learning method within the graph contrastive learning framework.


\section{Vulnerability Analysis of GCL}
\label{sec-vunlerability}
In this section, we present a detailed theoretical and empirical analysis of how existing attacks influence GCL, focusing on key graph properties. These explorations of the vulnerabilities of GCL provide strong insights for our design of the robust GCL framework.  
%We begin with investigating the common principles of graph structural attacks against the GCL framework. 
Without loss of generality, we choose two graph structural attacks, i.e., Mettack~\cite{Mettack} and CLGA~\cite{CLGA} as our graph attackers to simulate real-world attacking scenarios. Specifically, Mettack is the most representative structural attack against GNNs, as other ones~\cite{TopologyAttack} share similar attack loss with Mettack. Meanwhile, CLGA is the up-to-date representative attack method against the GCL framework. Therefore, exploring the underlying principles of Mettack and CLGA can sufficiently reveal the vulnerability of the GCL framework under structural attacks.   

\subsection{Threat Model}
We consider a system consisting of two parties: an attacker and a defender. Specifically, the attacker can manipulate the graph data to degenerate the graph-based learning models deployed by the defender. Conversely, the defender's objective is to recover the quality of the graph representation learning given the poisoned graph to improve node classification performance. To be concrete, we summarize the attacker's goal (Mettack and CLGA as exemplar), knowledge, and capability as follows:

\begin{itemize}
    \item \textbf{Attacker's goal}: The attacker’s goal is assumed to be decreasing the classification accuracy of a node classification problem achieved after training on poisoned graph data manipulated by the attacker. 
    \item \textbf{Attacker's knowledge}: The attacker can have different levels of knowledge of the data, model, and model parameters. For Mettack and CLGA, the attacker has no knowledge about the model with its parameters. In the meanwhile, the attacker can observe all nodes’ attributes, the graph structure. The main difference between Mettack and CLGA is that Mettack can observe the training labels. However, CLGA cannot query the label information of the graph data since it is an unsupervised attack method.
    \item \textbf{Attacker's capability}: In order to achieve unnoticeable attacks, Mettack and CLGA both impose a budget constraint $\Delta$ to restrict the number of perturbations on the topology space of the graph data, i.e., $\frac{1}{2}\|\mathbf{A}-\mathbf{A}^{p}\|\leq\Delta$, where $\mathbf{A}^{p}$ is the poisoned adjacency matrix.
    \item \textbf{Defender's knowledge}: In reality, the defender cannot acquire the attacking scenarios (such as the attack type, attack degree, and victim nodes or edges, etc.), and the label information of the graph data. However, the defender can have full knowledge of the given graph data's information (the node attribute matrix and adjacency matrix) as well as the model's information (model structure and model parameters).
\end{itemize}


\subsection{Disturbance on Mutual Information}
\label{sec-analyze-infoNCE}
% Figure environment removed

The powerful graph representation learning of the GCL framework highly relies on capturing the mutual information between the graph and its representations in low-dimensional space. To explore the failure of the GCL framework under the adversarial attack scenario, we start by analyzing the vulnerability of the GCL framework from an information theoretical perspective. For graph contrastive learning, a common way to curve the intractable mutual information is to utilize the InfoNCE object:
\begin{lemma}[InfoNCE~\cite{infoNCE}]
\label{lemma-infoNCE}
The InfoNCE object~\cite{GCA} $I_{info}(\cdot)=-\mathcal{L}_{info}(\cdot)$ is the lower bound approximation of the intractable mutual information between the graph and its representations $I(G;\mathbf{Z}=f_{\theta}(G))$.
%{\color{red} Is this function I (X,G) correct?}
\end{lemma}

Then, we provide the theorem to curve the relationship between the graph attacker and the mutual information $I(G;\mathbf{Z})$.
\begin{theorem}
\label{theorem-attack-info}
    The %graph attacker (Mettack or CLGA) 
    two attacks, Mettack and CLGA, maliciously degenerate the GCL's performance by diminishing the mutual information between the graph and its representations. 
\end{theorem}
\begin{proof}
    We denote the poisoned graph generated by the graph attacker as $G^{p}=\{\mathbf{X},\mathbf{A}^{p}\}$, the corresponding poisoned node embeddings as $\mathbf{Z}^{p}=f_{\theta}(G^{p})$, where $f_{\theta}(\cdot)$ is the GNN encoder. Given the InfoNCE object $I_{info}(\cdot)=-\mathcal{L}_{info}(\cdot)$, if $G^{p}$ is generated by CLGA, we have
    \begin{equation}
        G^{p}=\arg\min_{G} \ I_{info}(t_{1}(G); t_{2}(G)),
    \end{equation}
    since the InfoNCE loss is the objective function of CLGA. Then, we have 
    \begin{equation}
    \label{eqn-attack-Info}
    \begin{split}
        I_{info}(t_{1}(G); t_{2}(G))>I_{info}(t_{1}(G^{p}); t_{2}(G^{p})), 
    \end{split}
    \end{equation}
    based on Lemma~\ref{lemma-infoNCE}, we have:
    \begin{equation}
    \label{eqn-attack-MI}
    \begin{split}
        I(G; \mathbf{Z}=f_{\theta}(G))>I(G^p; \mathbf{Z}^{p}=f_{\theta}(G^p)).
    \end{split}
    \end{equation}
    If $G^{p}$ is generated by Mettack, we have
    \begin{equation}
        \begin{split}
            G^p=\arg\max_{G} \ \mathcal{L}_{CE}(\mathbf{Z}^{p}, \mathbf{Y}), 
        \end{split}
    \end{equation}
    where $\mathcal{L}_{CE}$ is the cross-entropy loss. We then partition the $\mathcal{L}_{CE}$ as: 
    \begin{equation}
    \label{eqn-proof-CE}
        \begin{split}
            \mathcal{L}_{CE}&=H(\mathbf{Y})+D_{KL}(p_Y\|p_Z) \\
            &=H(\mathbf{Y}|\mathbf{Z})+I(\mathbf{Y};\mathbf{Z})+D_{KL}(p_Y\|p_Z) \\
            &=H(\mathbf{Y}|\mathbf{Z})+D_{KL}(p_{YZ}\| p_Y p_Z)+D_{KL}(p_Y\|p_Z),
        \end{split}
    \end{equation}
    Next, we build the relationship between mutual information $I(G;\mathbf{Z})$ with the conditional entropy $H(\mathbf{Y}|\mathbf{Z})$ as 
    \begin{equation}
    \label{eqn-proof-MI}
        \begin{split}
            I(G;\mathbf{Z})&=H(G)-H(G|\mathbf{Z})\\
            &=H(G)-H(\mathbf{Y}|\mathbf{Z})+H(\mathbf{Y}|G,\mathbf{Z})-H(G|\mathbf{Y},\mathbf{Z}),
        \end{split}
    \end{equation}
    Based on Eqn.~\ref{eqn-proof-CE} and \ref{eqn-proof-MI}, we have
    \begin{equation}
    \label{eqn-proof-MI-CE}
        \begin{split}
            I(G; \mathbf{Z})&=H(G)-\mathcal{L}_{CE}+D_{KL}(p_{YZ}\|p_{Y}p_{Z})\\
            &+D_{KL}(p_{Y}\|p_{Z})+H(\mathbf{Y}|G,\mathbf{Z})-H(G|\mathbf{Y},\mathbf{Z}).
        \end{split}
    \end{equation}
    Based on Eqn.~\ref{eqn-proof-MI-CE}, we observe that the mutual information $I(G;\mathbf{Z})$ is negatively correlated with $\mathcal{L}_{CE}$. Since Mettack can increase the $\mathcal{L}_{CE}$:
    \begin{equation}
        \begin{split}
            \mathcal{L}_{CE}(\mathbf{Z},\mathbf{Y})<\mathcal{L}_{CE}(\mathbf{Z}^{p},\mathbf{Y}),
        \end{split}
    \end{equation}
    then, Eqn.~\ref{eqn-attack-MI} also holds for the poisoned graph generated by Mettack. 
\end{proof}

% Figure environment removed

Theorem \ref{theorem-attack-info} demonstrates that by maliciously altering the graph topology, attacks can degrade the mutual information estimation between the graph data and its representations. On the other hand, GCL's high-quality node representations are derived from maximizing this mutual information. Therefore, the GCL model may only be able to optimize the mutual information to a sub-optimal level, resulting in the production of low-quality embeddings. We further conduct empirical studies to verify whether the structural attacks may maliciously influence mutual information. We first implement Mettack and CLGA individually to poison the clean graph (Cora~\cite{CitationNetwork}) under different attacking scenarios. Next, we train a simple GCL model, i.e., GRACE~\cite{GRACE} on the poisoned graphs and report the converged InfoNCE object in Fig.~\ref{fig-attack-vs-info}. The results demonstrate that the structural attacks on GRACE indeed degrade the mutual information estimation between the graph and its representations.

\subsection{Disturbance on Graph Homophily}
\label{sec-analyze-homo}
It has been widely explored that structural attacks against GNNs (Mettack) will diminish the homophily level of the clean graphs~\cite{GCNJaccard,ProGNN,GNNGUARD,GNNhomophily,SimPGCN}. In this paper, we also explore whether the above-mentioned phenomenon is a common principle for graph structural attacks, including Mettack and CLGA. 
In fact, the graph homophily is defined as:
\begin{definition}[Graph homophily on label space~\cite{homophily}]
    The graph homophily based on node labels is given by:
    \begin{equation}
        \begin{split}
        h_{y}=\frac{1}{\mathcal{E}}\sum_{(v_{i},v_{j})\in\mathcal{E}}\mathbbm{1}_{(y_{i}=y_{j})}, \label{eqn-homo-label}
        \end{split}
    \end{equation}
    where $y_{{i}}$ is the label of node $v_{i}$, $\mathcal{E}$ is the link set.
\end{definition}

Besides using node labels, the graph homophily can also be measured by the Euclidean distance between the attribute vectors of the connected nodes~\cite{XLX,ProGNN}. It is common to use a trace formula to reformulate such distances in matrix format. We then present another metric on graph homophily as follows:
\begin{definition}[Graph homophily on feature space~\cite{ProGNN}]
    The graph homophily based on node attributes is given by:
    \begin{equation}
        \begin{split}
            \delta_{x}=\frac{1}{2}\sum_{i,j=1}^{N} A_{ij}(\frac{\mathbf{x}_{i}}{\sqrt{d_{i}}}-\frac{\mathbf{x}_{j}}{\sqrt{d_{j}}})^2=tr(\mathbf{X}^{T}\mathbf{L}\mathbf{X}).
        \end{split}
    \end{equation}
    where $\mathbf{L}=\mathbf{I}-\mathbf{\tilde{D}}^{-\frac{1}{2}}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-\frac{1}{2}}$ is the normalized graph Laplacian, $\mathbf{\tilde{A}}$ is the adjacency matrix with self-loop. 
\end{definition}
A higher $h_y$ (or a lower $\delta_x$) indicates that the graph tends to be more homophilous. We conduct empirical studies to verify whether \textit{structural attacks will diminish the graph homophily} is a common principle for Mettack and CLGA. Fig.~\ref{fig-attack-vs-homo} presents the relationship between the node classification accuracy of GRACE~\cite{GRACE} and the graph homophily metrics ($h_{y}$ and $\delta_{x}$) on poisoned graphs with varying attacking powers. It is clearly observed that the accuracy is positively correlated with $h_y$ and is negatively correlated with $\delta_x$. That is, both attacks will significantly diminish the graph homophily.

% Figure environment removed

%\begin{table}[h]
%	\centering
%	\caption{Number of inserted intra-class links, inserted inter-class links, deleted intra-class links and deleted inter-class links for Mettack and CLGA on Cora dataset.}
%	\label{tab-homo-attack}
%	\resizebox{0.8\columnwidth}{!}{%
%		\begin{tabular}{cccccc}
%			\toprule[1.pt]
%			Attack & Attack power & $5\%$ & $10\%$ & $15\%$ & $20\%$ \\
%        \hline
%	\multirow{4}*{Mettack} & \#Inserted intra-class links & $3$ & $9$ & $14$ & $25$ \\
%               & \#Inserted inter-class links & $248$ & $486$ & $705$ & $914$ \\
%               & \#Deleted intra-class links & $2$ & $11$ & $38$ & $68$ \\
%               & \#Deleted inter-class links & $0$ & $0$ & $3$ & $6$\\
%               \hline
%               \multirow{4}*{CLGA} & \#Inserted intra-class links & $18$ & $35$ & $59$ & $77$ \\
%               & \#Inserted inter-class links & $232$ & $465$ & $690$ & $912$ \\
%               & \#Deleted intra-class links & $3$ & $5$ & $10$ & $21$ \\
%               & \#Deleted inter-class links & $0$ & $1$ & $1$ & $3$ \\
%			\bottomrule[1.pt]
%		\end{tabular}
%	}
%\end{table}

%In addition, Tab.~\ref{tab-homo-attack} shows the number of inserted intra/inter-class links and deleted intra/inter-class links for Mettack and CLGA at different attacking powers. The results indicate that, for both CLGA and Mettack, the majority of manipulations involve inserting inter-class links into the clean graph. This finding is consistent with the phenomenon presented in Fig.~\ref{fig-attack-vs-homo}. All these experiment results demonstrate that structural attacks against GCL take effect by reducing the graph homophily degrees.

Overall, the findings in Sec.~\ref{sec-analyze-infoNCE} and \ref{sec-analyze-homo} that adversarial attacks will provide malicious effects to the mutual information and the graph homophily can serve as strong insights for us to design the defense strategy for the GCL framework. That is, restoring the diminished mutual information and graph homophily is promising to sanitize the malicious effects and thus achieve adversarial robustness.   

\section{Defense Methodology}
\label{Sec-Methodology}
% Figure environment removed

\subsection{Overview} 
The proposed model's overall framework is illustrated in Fig.~\ref{fig-overview}. The model comprises a data augmentation phase and a contrastive learning phase. In the first phase, we introduce a learnable sanitation view to restore the diminished mutual information and graph homophily. In the second phase, the sanitation view is contrasted with the second augmented view, which is produced by random link removal and feature masking. Finally, we optimize a cross-view contrastive learning objective and the learnable sanitation view in an end-to-end manner. In this way, the learned node embeddings will restore the diminished mutual information and achieve adversarial robustness. 

\subsection{Sanitation View Generation}
As previously mentioned in Sec.~\ref{Sec-intro}, we endeavor to achieve the robustness of GCL via restoring the contaminated mutual information after attacks. Unlike the traditional adversarial training method that achieves robustness by further ``polluting" the poisoned graph data, we instead choose to ``sanitize" the poisoned graph data to rectify the mutual information distortion caused by the attacks during the data augmentation phase. To achieve this goal, we introduce the sanitation view with a stochastic edge-dropping scheme to sanitize malicious effects provided by attacks. To be concrete, the stochastic edge-dropping scheme of the sanitation view can adaptively restore the mutual information during graph data augmentation and thus achieve robustness. Specifically, we use an edge vector $\mathbf{E}$ to represent all the existing edges in a graph. Then, we define a manipulation vector $\mathbf{M} = \{0,1\}^{|\mathbf{E}|}$ that follows the Bernoulli distribution parameterized by $\mathbf{P}$; that is $\mathbf{M}\sim Ber(\mathbf{P})$. For a particular entry $\mathbf{M}_e$, we will remove the corresponding edge $e$ if $\mathbf{M}_e = 1$, which is controlled by the probability $\mathbf{P}_e$. Now, the sanitation view can be regarded as a mapping $\mathcal{T}_{\mathbf{P}} (\mathbf{E}) \rightarrow \mathbf{E}_{\mathcal{S}}$ parameterized by $\mathbf{P}$, which takes a graph with edge vector $\mathbf{E}$ as input and produces a sanitized view $\mathbf{E}_{\mathcal{S}}$. Formally, we have:
%In our framework, the sanitation view focuses on implementing stochastic topology augmentation to automatically sanitize the candidate inter-class links injected by the graph attacker in the poisoned graph. To this end, we focus on the edge-dropping scheme for the sanitation view and parametrize it with the edge-dropping probability matrix which follows the Bernoulli distribution. More specifically, we define the sanitation view as a crafted mapping to the edge vector $\mathbf{E}$ of the input graph and a manipulation vector $\mathbf{Z}$ with each entry following the Bernoulli distribution:
\begin{equation}
    \begin{split}
        \mathbf{E}_{\mathcal{S}}=\mathcal{T}_{\mathbf{P}}(\mathbf{E}) \triangleq \mathbf{E}\odot(\mathbf{1}_{|\mathbf{E}|}-\mathbf{M}), \ \ \mathbf{M}\sim Ber(\mathbf{P}),
    \end{split}
    \label{eqn-san-sampling}
\end{equation} 
where $\odot(\cdot)$ is the element-wise product operation, $\mathbf{1}_{|\mathbf{E}|}$ is an all-one vector with length equal to the edge number ${|\mathbf{E}|}$, $\mathbf{P}=\{p_{e}\}_{e=1}^{|\mathbf{E}|}$ is the parameters of the sanitizer. 
%Here $\mathbf{Z}_{e}=1$ means the sanitation view removes the corresponding link $e$ and vice versa. 

Thus, the remaining task amounts to choosing the proper parameters $\mathbf{P}$ for the sanitizer $\mathcal{T}_{\mathbf{P}}$ so that the generated sanitation views can facilitate contrastive learning to achieve adversarial robustness. Next, we show how to learn the parameters $\mathbf{P}$. 

\subsubsection{Learning of Robust GCL}
The key challenges to learning $\mathcal{T}_{\mathbf{P}}$ are two-fold: choosing the proper objective to guide learning, and integrating learning into the framework of GCL. 
We design an objective function that contains two components, i.e., InfoNCE loss and feature smoothness. Intuitively, forcing the learning of the sanitation view to decrease the InfoNCE loss and feature smoothness can restore the diminished mutual information and graph homophily during training. Specifically, the objective function is :
\begin{equation}
    \begin{split}
        \min_{\mathbf{P}\in\mathcal{S}_1, \theta} \ &\mathcal{L}=\mathcal{L}_{info}(f_{\theta}(\mathbf{E}_{\mathcal{S}}), f_{\theta}(t_\mathcal{R}(\mathbf{E})))+\eta\Tr(\mathbf{X}^{\top}\mathbf{L}_{\mathcal{S}}\mathbf{X}), \\ 
        &\text{s.t.} \ \mathbf{E}_{\mathcal{S}}=\mathbf{E}\odot(\mathbf{1}_{|\mathbf{E}|}-\mathbf{M}), \ \mathbf{M}\sim Ber(\mathbf{P}),\\
        & \quad \  t_{\mathcal{R}}(\mathbf{E})\in\mathcal{T}_{\mathcal{R}}, \ \mathbf{L}_{\mathcal{S}}=\mathbf{I}-\tilde{\mathbf{D}}_{\mathcal{S}}^{-\frac{1}{2}}(\mathbf{A}_{\mathcal{S}}+\mathbf{I})\tilde{\mathbf{D}}_{\mathcal{S}}^{-\frac{1}{2}},
    \end{split}
    \label{eqn-san-obj-2}
\end{equation} 
where the first view $\mathbf{E}_{\mathcal{S}}$ is the learnable sanitation view, the second augmented view $t_\mathcal{R}(\mathbf{E})$ is generated from typical random augmentation scheme $\mathcal{T}_R$ that will randomly drop edges and feature masking with a fixed probability, $\Tr(\cdot)$ represents the trace of a matrix  and $\mathbf{S}_1$ is a constrained space of the sanitation operation with $\epsilon$ controlling the sanitation degree: 
\begin{equation}
    \begin{split}
        \mathbf{S}_1=\{s|s\in[0,1]^{E}, \|s\|_1\leq \epsilon\}.
    \end{split}
\end{equation}
$\mathbf{L}_{\mathcal{S}}$ is the graph Laplacian based on the sanitation view $\mathbf{E}_{\mathcal{S}}$. 


%In this way, the augmented graph derived from the sanitation view can degrade the mutual information estimation and the graph homophily guided by the crafted objection. 

In our design, the training objective of the sanitation view is embedded into that of the GCL. That is, we train the learnable sanitation view together with the contrastive learning, using a hyperparameter $\eta$ to control the relative importance of optimizing InfoNCE loss and the graph homophily $\delta_{x}$. %
Then, the learnable sanitation view samples $\mathbf{E}_{\mathcal{S}}$ based on Eqn.~\ref{eqn-san-sampling} for each iteration. After that, the shared GNN encoder $f_{\theta}(\cdot)$ converts the graph data from the two views to low-dimensional node embedding matrices and obtains the training loss $\mathcal{L}$. During the backward pass, we compute the gradient of $\mathcal{L}$ with respect to the parameters $\mathbf{P}$ and $\theta$ and update them via gradient descent. Consequently, the sanitation view can restore the diminished mutual information guided by the sanitation view's objection. 

\subsubsection{Learning of Stochastic Edge-dropping Scheme}
However, the above-mentioned learning scheme still encounters a unique challenge: It is difficult to directly optimize the stochastic edge-dropping scheme of the sanitation view due to the undifferentiable sampling procedure. To tackle this issue, we refer to the Gumbel-Softmax re-parametrization technique~\cite{GumbelSoftmax} to relax the discrete Bernoulli sampling procedure to a differentiable Bernoulli approximation. Using the relaxed sample, we implement the straight-through estimator~\cite{STE} to discretize the relaxed samples in the forward pass and set the gradient of discretization operation to $1$, i.e., the gradients are directly passed to the relaxed samples rather than the discrete values. Hence, we reconstruct the mapping from $\mathbf{P}$ to $\mathbf{M}$ as:
\begin{equation}
    \begin{split}
        \mathbf{M}_{e}=\lfloor\frac{1}{1+e^{-(\log\mathbf{P}_e+g)/\tau}}+\frac{1}{2}\rceil,
    \end{split}
\end{equation}
where $\lfloor\cdot\rceil$ is the rounding function, $g\sim Gumbel(0,1)$ is a standard Gumbel random variable with zero mean and the scale equal to $1$. Then, we can reformulate Eqn.~\ref{eqn-san-sampling} to:
\begin{equation}
	%\small
    \begin{split}
        &\mathbf{E}_{\mathcal{S}}=\mathbf{E}\odot(\mathbf{1}_{|E|}-\lfloor\frac{1}{1+e^{-(\log\mathbf{P}+g)/\tau}}+\frac{1}{2}\rceil), \\ 
        &\text{where} \ \mathbf{P}\in\mathcal{S}_1=\{\mathbf{P}_{e}|\sum_{e}\mathbf{P}_{e}\leq \epsilon, \mathbf{P}_{e}\in[0,1] \ \forall e\in\{1,..,E\}\}. 
    \end{split}
    \label{eqn-san-Gumbel-sampling}
\end{equation}  
Formally, to optimize the parameters $\mathbf{P}$ under the constraint $\mathcal{S}_{1}$ defined in Eqn.~\ref{eqn-san-Gumbel-sampling}, we deploy the projection gradient descent to optimize the relaxed constrained optimization problem introduced in Eqn.~\ref{eqn-san-Gumbel-sampling}:
\begin{equation}
    \begin{split}
        \mathbf{P}^{(t)}=\textstyle\prod_{\mathcal{S}_1}(\mathbf{P}^{(t-1)}-\alpha\nabla_{\mathbf{P}^{(t-1)}}\mathcal{L}(t_1(\mathcal{G}), t_2(\mathcal{G}), \theta))
    \end{split}
    \label{eqn-PGD}
\end{equation}
at the $t$-th iteration, where $\alpha$ is the learning rate of the projection gradient descent, $\nabla_{\mathbf{P}}\mathcal{L}(t_1(\mathcal{G}),t_2(\mathcal{G}),\theta)$ denotes the gradient of the loss defined in Eqn.~\ref{eqn-san-obj-2}, $\prod_{\mathcal{S}_1}(\cdot)$ is the projection operator to project the updated parameters to satisfy the constraint. Referring to \cite{TopologyAttack}, the projection operator $\prod_{\mathcal{S}_1}(\cdot)$ has the closed-form solutions:
\begin{equation*}
    \begin{split}
        \textstyle\prod_{\mathcal{S}_1}(\mathbf{P})=\begin{cases}
                                            \prod_{[0,1]}[\mathbf{P}-\mu\mathbf{1}_{E}],&\mbox{if } \mu>0\mbox{ and }
                                            \\ &\sum_{e}\prod_{[0,1]}[\mathbf{P}-\mu\mathbf{1}_{E}]=\epsilon, \\ \\ 
                                            \prod_{[0,1]}[\mathbf{P}],&\mbox{if } \sum_{e}\prod_{[0,1]}[\mathbf{P}]\leq\epsilon, 
                                        \end{cases}
    \end{split}
\end{equation*}
where $\prod_{[0,1]}[\mathbf{P}]$ clips the parameters vector $\mathbf{P}$ into the range $[0,1]$. 
The bisection method~\cite{bisection} is used over $\mu\in\{\min(\mathbf{P}-\mathbf{1}_{E}), \max(\mathbf{P})\}$ to find the solution to $\sum_{e}\prod_{[0,1]}[\mathbf{P}-\mu\mathbf{1}_{E}]=\epsilon$ with the convergence rate $\log_{2}[(\max(\mathbf{P})-\min(\mathbf{P}-\mathbf{1}_{E}))/\xi]$ with $\xi$-error tolerance~\cite{liu2015sparsity}. We present the details of algorithm in Alg.~\ref{alg-RGCL}.
\begin{algorithm}[htp]
    \caption{\textbf{GCIR}}
    \flushleft
    \label{alg-RGCL}
    \textbf{Input}: Poisoned graph $\mathcal{G}=\{\mathbf{E},\mathbf{X}\}$, $\eta$.\\
        \textbf{Parameters}: Sanitation probability $\mathbf{P}$, GNN encoder $f_{\theta}(\cdot)$ with weight matrices $\theta=\{\mathbf{W}^{(1)}, \mathbf{W}^{(2)}\}$.\\
    \textbf{Output}: Sanitized node embeddings matrix $\mathbf{H}_1$.\\
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE Let $t=0$, initialize parameters $\mathbf{P}=\mathbf{P}^{0}$, $\theta=\theta^{0}$.
		\WHILE{$t\leq T$}
        \STATE Sample $g\sim Gumbel(0,1)$.
        \STATE Sample sanitation view $\mathcal{G}_{1}^{t}$ from $\mathbf{E}_{\mathcal{S}}^{t}=\mathbf{E}\odot(\mathbf{1}_{E}-\lfloor\frac{1}{1+e^{-(\log\mathbf{P}^{t}+g)/\tau}}+\frac{1}{2}\rceil)$.
        \STATE Sample another view $\mathcal{G}_{2}^{t}$ by randomly dropping links. 
        \STATE Compute graph Laplacian $\mathbf{L}^{t}=\mathbf{I}-(\tilde{\mathbf{D}}^{t})^{-\frac{1}{2}}\tilde{\mathbf{A}}^{t}(\tilde{\mathbf{D}}^{t})^{-\frac{1}{2}}$.
        \STATE Compute loss $\mathcal{L}=\mathcal{L}_{info}(\mathcal{G}_{1}^{t}, \mathcal{G}_{2}^{t})+\eta\Tr(\mathbf{X}^{\top}\mathbf{L}^{t}\mathbf{X})$.
        \STATE Projection gradient descent and update $\mathbf{P}^{t}$ from Eqn.~\ref{eqn-PGD}.
        \STATE Gradient descent and update $\theta^{t}$ based on $\mathcal{L}$.
        \STATE Compute pseudo normalized cut loss $\mathcal{L}_{pnc}^{t}$ from Eqn.~\ref{eqn-pseudo-normalized-cut}. 
        \ENDWHILE
        \STATE Get the best iteration $t^{*}$ with the minimum value $\mathcal{L}_{pnc}^{t^{*}}$.
        \RETURN Sanitized node embeddings $\mathbf{H}_1=f_{\theta^{t^{*}}}(\mathbf{A}^{t^{*}},\mathbf{X})$.
	\end{algorithmic}
\end{algorithm} 

\subsection{Unsupervised Tuning Strategy}
\label{sec-unsupervised-tuning}
Strictly speaking, the whole training procedure of the GCL models is label-free, that is, the data analyst cannot query the label's information from the downstream tasks to supervise the training of GCL, including the parameter tuning phase. Hence, it is vital to craft a totally unsupervised tuning strategy to select the best hyperparameters. We herein concentrate on designing a new metric in an unsupervised manner without additional training or fine-tuning. 

In particular, normalized cut loss~\cite{NormCut} is an unsupervised loss function that counts the fraction of the inter-class links for a given graph partition result. Thus, a good graph partition should obtain a lower normalized cut loss, i.e., the majority of the links connect two nodes with the same class. The normalized cut loss is defined as:
\begin{equation}
    \begin{split}
        &\mathcal{L}_{nc}=\frac{1}{K}\Tr((\mathbf{C}^{\top}\mathbf{LC})\oslash(\mathbf{C}^{\top}\mathbf{DC})), \ \mathbf{L}=\mathbf{I}-\hat{\mathbf{A}}.
    \end{split}
    \label{eqn-normalized-cut}
\end{equation}
Here $\mathbf{C}$ is the cluster assignment matrix, i.e., $C_{ij}$ represents the node $i$ belongs to the cluster $j$, $\mathbf{L}$ is the graph Laplacian matrix, $\oslash$ represents element-wise division. We propose to utilize this normalized cut loss as the supervision for searching the vital parameter $\eta$ in Eqn.~\ref{eqn-san-obj-2}. The intuition is that if the generated node embeddings have high quality, the clustering result should be accurate, leading to a low loss value. In fact, several previous works~\cite{maskGVAE} have utilized this loss as supervision in different ways. 
Specifically, during the $t$-th iteration, we obtain the sanitized adjacency matrix $\mathbf{A}_{\mathcal{S}}^{(t)}$ (corresponding with $\mathbf{E}_{\mathcal{S}}^{(t)}$ in Eqn.~\eqref{eqn-san-Gumbel-sampling}). 
Then we feed it into a two-layered GNN encoder to the sanitized node embeddings $\mathbf{H}_{\mathcal{S}}^{(t)}$
%=f_{\theta}(\mathbf{A}_{\mathcal{S}}^{(t)},\mathbf{X})$ from Eqn.~\ref{eqn-GNN-encoder} 
and get the pseudo normalized cut:
\begin{equation*}
	\small
    \begin{split}
        \mathcal{L}_{pnc}^{(t)}=\Tr((\sigma(\mathbf{H}_{1}^{(t)})^{\top}\mathbf{L}^{(t)}\sigma(\mathbf{H}_{1}^{(t)}))\oslash(\sigma(\mathbf{H}_{1}^{(t)})^{\top}\mathbf{D}^{(t)}\sigma(\mathbf{H}_{1}^{(t)})),
    \end{split}
    \label{eqn-pseudo-normalized-cut}
\end{equation*}
where $\sigma(\cdot)$ is the sigmoid function. We record the value of $\mathcal{L}_{pnc}^{(t)}$ for each iteration during training and select the model with the minimum $\mathcal{L}_{pnc}$ as our final result. We denote the best metric as $\mathcal{L}_{pnc}^{t^{*}}$. It is worth noting that the computing of the pseudo normalized cut $\mathcal{L}_{pnc}$ does not rely on the labels, which is suitable for the unsupervised training of GCL. 

\begin{wrapfigure}{r}{0.25\textwidth}
    \centering
    % Figure removed
    \caption{$\mathcal{L}_{pnc}^{t^{*}}$ v.s. Acc.}
    \label{fig-pnc-acc}
\end{wrapfigure}

Besides, we do not introduce additional parameters to ensure the time efficiency of computing $\mathcal{L}_{pnc}$.  

%% Figure environment removed

In order to verify the effectiveness of the crafted unsupervised tuning strategy, we present the values of $\mathcal{L}_{pnc}^{t^{*}}$ and node classification accuracy with varying $\eta$ in Fig.~\ref{fig-pnc-acc}. The results indicate that in most cases $\mathcal{L}_{pnc}^{t^{*}}$ is negatively related to the node classification accuracy, especially when the value of $\mathcal{L}_{pnc}^{t^{*}}$ falls into the neighborhood of the minimum value, i.e., $\eta\in[0.1, 3]$ (the Pearson correlation coefficients $r(\mathcal{L}_{pnc}^{t^{*}}, accuracy)=-0.89$). This phenomenon demonstrates that it is reasonable to tune $\eta$ via searching for the best $\mathcal{L}_{pnc}^{t^{*}}$, since the GCL model that achieves the minimum $\mathcal{L}_{pnc}^{t^{*}}$ is likely to get the best accuracy. 












































