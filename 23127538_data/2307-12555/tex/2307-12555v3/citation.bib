@article{homophily,
  title={Birds of a feather: Homophily in social networks},
  author={McPherson, Miller and Smith-Lovin, Lynn and Cook, James M},
  journal={Annual review of sociology},
  volume={27},
  number={1},
  pages={415--444},
  year={2001},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{autogcl,
  title={AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators},
  author={Yin, Yihang and Wang, Qingzhong and Huang, Siyu and Xiong, Haoyi and Zhang, Xiang},
  journal={arXiv preprint arXiv:2109.10259},
  year={2021}
}

@ARTICLE{infomax,
  author={Linsker, R.},
  journal={Computer}, 
  title={Self-organization in a perceptual network}, 
  year={1988},
  volume={21},
  number={3},
  pages={105-117},
  doi={10.1109/2.36}}

@article{politicalnetwork,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/23496641},
 abstract = {We study the extent of political homophily—the tendency to form connections with others who are politically similar—in local governments' decisions to participate in an important form of intergovernmental collaboration: regional planning networks. Using data from a recent survey of California planners and government officials, we develop and test hypotheses about the factors that lead local governments to collaborate within regional planning networks. We find that local governments whose constituents are similar politically, in terms of partisanship and voting behavior, are more likely to collaborate with one another in regional planning efforts than those whose constituents are politically diverse. We conclude that political homophily reduces the transaction costs associated with institutional collective action, even in settings where we expect political considerations to be minimal.},
 author = {Elisabeth R. Gerber and Adam Douglas Henry and Mark Lubell},
 journal = {American Journal of Political Science},
 number = {3},
 pages = {598--610},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {Political Homophily and Collaboration in Regional Planning Networks},
 urldate = {2023-06-15},
 volume = {57},
 year = {2013}
}

@article{GNNhomophily,
  title={Beyond homophily in graph neural networks: Current limitations and effective designs},
  author={Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and Koutra, Danai},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7793--7804},
  year={2020}
}

@INPROCEEDINGS{9679165,
  author={Godziszewski, Michał Tomasz and Michalak, Tomasz P. and Waniek, Marcin and Rahwan, Talal and Zhou, Kai and Zhu, Yulin},
  booktitle={2021 IEEE International Conference on Data Mining (ICDM)}, 
  title={Attacking Similarity-Based Sign Prediction}, 
  year={2021},
  volume={},
  number={},
  pages={1072-1077},
  keywords={Heuristic algorithms;Conferences;Prediction algorithms;Data mining;networks;sign prediction;link prediction;complexity;np-hardness;similarity measures},
  doi={10.1109/ICDM51629.2021.00173}}

@INPROCEEDINGS{10889118,
  author={Bu, Yu and Zhu, Yulin and Zhou, Kai},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Crowdsourced Homophily Ties Based Graph Annotation Via Large Language Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  keywords={Accuracy;Codes;Annotations;Large language models;Signal processing;Graph neural networks;Acoustics;Reliability;Speech processing;graph annotation;crowdsourcing;homophily ties;large language model;graph neural networks},
  doi={10.1109/ICASSP49660.2025.10889118}}

@INPROCEEDINGS{10646863,
  author={Lai, Yuni and Zhu, Yulin and Pan, Bailin and Zhou, Kai},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)}, 
  title={Node-aware Bi-smoothing: Certified Robustness against Graph Injection Attacks}, 
  year={2024},
  volume={},
  number={},
  pages={2958-2976},
  keywords={Threat modeling;Bridges;Privacy;Smoothing methods;Benchmark testing;Robustness;Security},
  doi={10.1109/SP54263.2024.00241}}

@article{GNN,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{XLX,
  title={Learning on graph with Laplacian regularization},
  author={Ando, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}

@inproceedings{ProGNN,
  title={Graph Structure Learning for Robust Graph Neural Networks},
  author={Jin, Wei and Ma, Yao and Liu, Xiaorui and Tang, Xianfeng and Wang, Suhang and Tang, Jiliang},
  booktitle={26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2020},
  pages={66--74},
  year={2020},
  organization={Association for Computing Machinery}
}

@inproceedings{HeteRobust,
author = {Zhu, Jiong and Jin, Junchen and Loveland, Donald and Schaub, Michael T. and Koutra, Danai},
title = {How Does Heterophily Impact the Robustness of Graph Neural Networks? Theoretical Connections and Practical Implications},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539418},
doi = {10.1145/3534678.3539418},
abstract = {We bridge two research directions on graph neural networks (GNNs), by formalizing the relation between heterophily of node labels (i.e., connected nodes tend to have dissimilar labels) and the robustness of GNNs to adversarial attacks. Our theoretical and empirical analyses show that for homophilous graph data, impactful structural attacks always lead to reduced homophily, while for heterophilous graph data the change in the homophily level depends on the node degrees. These insights have practical implications for defending against attacks on real-world graphs: we deduce that separate aggregators for ego- and neighbor-embeddings, a design principle which has been identified to significantly improve prediction for heterophilous graph data, can also offer increased robustness to GNNs. Our comprehensive experiments show that GNNs merely adopting this design achieve improved empirical and certifiable robustness compared to the best-performing unvaccinated model. Additionally, combining this design with explicit defense mechanisms against adversarial attacks leads to an improved robustness with up to 18.33% performance increase under attacks compared to the best-performing vaccinated model.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2637–2647},
numpages = {11},
keywords = {structural perturbation, graph neural networks, heterophily, robustness, adversarial attacks, relation},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{
Mettack,
title={Adversarial Attacks on Graph Neural Networks via Meta Learning},
author={Daniel Zügner and Stephan Günnemann},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bylnx209YX},
}

@inproceedings{CLGA,
  title={Unsupervised graph poisoning attack via contrastive loss back-propagation},
  author={Zhang, Sixiao and Chen, Hongxu and Sun, Xiangguo and Li, Yicong and Xu, Guandong},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={1322--1330},
  year={2022}
}

@inproceedings{CitationNetwork,
  title={Revisiting semi-supervised learning with graph embeddings},
  author={Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
  booktitle={International conference on machine learning},
  pages={40--48},
  year={2016},
  organization={PMLR}
}

@article{infoNCE,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{NormCut,
  title={Normalized cuts and image segmentation},
  author={Shi, Jianbo and Malik, Jitendra},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={22},
  number={8},
  pages={888--905},
  year={2000},
  publisher={Ieee}
}

@inproceedings{maskGVAE,
  title={Mask-GVAE: Blind Denoising Graphs via Partition},
  author={Li, Jia and Liu, Mengzhou and Zhang, Honglei and Wang, Pengyun and Wen, Yong and Pan, Lujia and Cheng, Hong},
  booktitle={Proceedings of the Web Conference 2021},
  pages={3688--3698},
  year={2021}
}

@inproceedings{bojchevski2019adversarial,
  title={Adversarial attacks on node embeddings via graph poisoning},
  author={Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
  booktitle={International Conference on Machine Learning},
  pages={695--704},
  year={2019},
  organization={PMLR}
}

@ARTICLE{FocusedCleaner,
  author={Zhu, Yulin and Tong, Liang and Li, Gaolei and Luo, Xiapu and Zhou, Kai},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification}, 
  year={2023},
  volume={},
  number={},
  pages={1-14},
  keywords={Robustness;Toxicology;Task analysis;Training;Topology;Data models;Adaptation models;Graph Learning and Mining;Graph Adversarial Robustness;Discrete Optimization;Victim Node Detection},
  doi={10.1109/TKDE.2023.3322129}}

@inproceedings{GASOLINE,
author = {Xu, Zhe and Du, Boxin and Tong, Hanghang},
title = {Graph Sanitation with Application to Node Classification},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3485447.3512180},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1136–1147},
numpages = {12},
keywords = {graph mining, node classification, graph sanitation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{
GumbelSoftmax,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
}

@inproceedings{
STE,
title={Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets},
author={Penghang Yin and Jiancheng Lyu and Shuai Zhang and Stanley J. Osher and Yingyong Qi and Jack Xin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Skh4jRcKQ},
}

@article{TopologyAttack,
  title={Topology attack and defense for graph neural networks: An optimization perspective},
  author={Xu, Kaidi and Chen, Hongge and Liu, Sijia and Chen, Pin-Yu and Weng, Tsui-Wei and Hong, Mingyi and Lin, Xue},
  journal={arXiv preprint arXiv:1906.04214},
  year={2019}
}

@book{bisection,
  title={Convex optimization},
  author={Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{liu2015sparsity,
  title={Sparsity-aware sensor collaboration for linear coherent estimation},
  author={Liu, Sijia and Kar, Swarnendu and Fardad, Makan and Varshney, Pramod K},
  journal={IEEE Transactions on Signal Processing},
  volume={63},
  number={10},
  pages={2582--2596},
  year={2015},
  publisher={IEEE}
}

@article{shi2000normalized,
  title={Normalized cuts and image segmentation},
  author={Shi, Jianbo and Malik, Jitendra},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={22},
  number={8},
  pages={888--905},
  year={2000},
  publisher={Ieee}
}

@article{ng2001spectral,
  title={On spectral clustering: Analysis and an algorithm},
  author={Ng, Andrew and Jordan, Michael and Weiss, Yair},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@article{von2007tutorial,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, Ulrike},
  journal={Statistics and computing},
  volume={17},
  pages={395--416},
  year={2007},
  publisher={Springer}
}

@Inbook{Jin2010,
author="Jin, Xin
and Han, Jiawei",
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="K-Means Clustering",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="563--564",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_425",
url="https://doi.org/10.1007/978-0-387-30164-8_425"
}

@article{WikiCS,
  title={Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks},
  author={Mernyei, P{\'e}ter and Cangea, C{\u{a}}t{\u{a}}lina},
  journal={arXiv preprint arXiv:2007.02901},
  year={2020}
}

@article{Amazon,
  title={Pitfalls of graph neural network evaluation},
  author={Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:1811.05868},
  year={2018}
}

@ARTICLE{BagofWords,
  author={Sivic, Josef and Zisserman, Andrew},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient Visual Search of Videos Cast as Text Retrieval}, 
  year={2009},
  volume={31},
  number={4},
  pages={591-606},
  doi={10.1109/TPAMI.2008.111}}

@inproceedings{Glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@misc{Adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{GRACE,
  title={Deep graph contrastive representation learning},
  author={Zhu, Yanqiao and Xu, Yichen and Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang},
  journal={arXiv preprint arXiv:2006.04131},
  year={2020}
}

@inproceedings{ARIEL,
  title={Adversarial graph contrastive learning with information regularization},
  author={Feng, Shengyu and Jing, Baoyu and Zhu, Yada and Tong, Hanghang},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={1362--1371},
  year={2022}
}

@inproceedings{
SPAN,
title={Spectral Augmentation for Self-Supervised Learning on Graphs},
author={Lu Lin and Jinghui Chen and Hongning Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=DjzBCrMBJ_p}
}

@inproceedings{GCA,
    author = {Zhu, Yanqiao and Xu, Yichen and Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang},
    title={Graph Contrastive Learning with Adaptive Augmentation},
    year={2021},
    isbn={9781450383127},
    publisher={Association for Computing Machinery},
    address={New York, NY, USA},
    url={https://doi.org/10.1145/3442381.3449802},
    doi={10.1145/3442381.3449802},
    booktitle={Proceedings of the Web Conference 2021},
    pages={2069–2080},
    numpages={12},
    keywords={unsupervised learning, self-supervised learning, graph representation learning, Contrastive learning},
    location={Ljubljana, Slovenia},
    series={WWW '21}
}

@Inbook{RecSys,
author="Ricci, Francesco
and Rokach, Lior
and Shapira, Bracha",
editor="Ricci, Francesco
and Rokach, Lior
and Shapira, Bracha",
title="Recommender Systems: Techniques, Applications, and Challenges",
bookTitle="Recommender Systems Handbook",
year="2022",
publisher="Springer US",
address="New York, NY",
pages="1--35",
abstract="Recommender systems (RSs) are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user. In this introductory chapter, we briefly discuss basic RS ideas and concepts. Our main goal is to delineate, in a coherent and structured way, the chapters in this handbook. Additionally, we aim to help the reader navigate the rich and detailed content that this handbook offers.",
isbn="978-1-0716-2197-4",
doi="10.1007/978-1-0716-2197-4_1",
url="https://doi.org/10.1007/978-1-0716-2197-4_1"
}

@inproceedings{asgn,
  title={ASGN: An active semi-supervised graph neural network for molecular property prediction},
  author={Hao, Zhongkai and Lu, Chengqiang and Huang, Zhenya and Wang, Hao and Hu, Zheyuan and Liu, Qi and Chen, Enhong and Lee, Cheekong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={731--752},
  year={2020}
}

@inproceedings{graphsage,
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
title = {Inductive Representation Learning on Large Graphs},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {1025–1035},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{bhagat2011node,
  title={Node classification in social networks},
  author={Bhagat, Smriti and Cormode, Graham and Muthukrishnan, S},
  journal={arXiv preprint arXiv:1101.3291},
  year={2011}
}

@article{schaeffer2007graph,
  title={Graph clustering},
  author={Schaeffer, Satu Elisa},
  journal={Computer science review},
  volume={1},
  number={1},
  pages={27--64},
  year={2007},
  publisher={Elsevier}
}

@article{zhou2009graph,
  title={Graph clustering based on structural/attribute similarities},
  author={Zhou, Yang and Cheng, Hong and Yu, Jeffrey Xu},
  journal={Proceedings of the VLDB Endowment},
  volume={2},
  number={1},
  pages={718--729},
  year={2009},
  publisher={VLDB Endowment}
}

@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}

@inproceedings{DeepWalk,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: Online Learning of Social Representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {network classification, learning with partial labels, deep learning, latent representations, social networks, online learning},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{node2vec,
author = {Grover, Aditya and Leskovec, Jure},
title = {Node2vec: Scalable Feature Learning for Networks},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939754},
doi = {10.1145/2939672.2939754},
abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {855–864},
numpages = {10},
keywords = {feature learning, graph representations, information networks, node embeddings},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{skip-gram,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}

@inproceedings{RGCN,
  title={Robust graph convolutional networks against adversarial attacks},
  author={Zhu, Dingyuan and Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1399--1407},
  year={2019}
}

@inproceedings{GNNGUARD,
 author = {Zhang, Xiang and Zitnik, Marinka},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9263--9275},
 publisher = {Curran Associates, Inc.},
 title = {GNNGuard: Defending Graph Neural Networks against Adversarial Attacks},
 volume = {33},
 year = {2020}
}

@inproceedings{SimPGCN,
  title={Node similarity preserving graph convolutional networks},
  author={Jin, Wei and Derr, Tyler and Wang, Yiqi and Ma, Yao and Liu, Zitao and Tang, Jiliang},
  booktitle={Proceedings of the 14th ACM international conference on web search and data mining},
  pages={148--156},
  year={2021}
}

@inproceedings{GCNJaccard,
  title     = {Adversarial Examples for Graph Data: Deep Insights into Attack and Defense},
  author    = {Wu, Huijun and Wang, Chen and Tyshetskiy, Yuriy and Docherty, Andrew and Lu, Kai and Zhu, Liming},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {4816--4823},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/669},
  url       = {https://doi.org/10.24963/ijcai.2019/669},
}

@inproceedings{SimCLR,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{MVGRL,
  title={Contrastive multi-view representation learning on graphs},
  author={Hassani, Kaveh and Khasahmadi, Amir Hosein},
  booktitle={International conference on machine learning},
  pages={4116--4126},
  year={2020},
  organization={PMLR}
}

@article{DGI,
  title={Deep graph infomax.},
  author={Velickovic, Petar and Fedus, William and Hamilton, William L and Li{\`o}, Pietro and Bengio, Yoshua and Hjelm, R Devon},
  journal={ICLR (Poster)},
  volume={2},
  number={3},
  pages={4},
  year={2019}
}

@inproceedings{
BGRL,
title={Large-Scale Representation Learning on Graphs via Bootstrapping},
author={Shantanu Thakoor and Corentin Tallec and Mohammad Gheshlaghi Azar and Mehdi Azabou and Eva L Dyer and Remi Munos and Petar Veli{\v{c}}kovi{\'c} and Michal Valko},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0UXT6PpRpW}
}

@inproceedings{Nettack,
author = {Z\"{u}gner, Daniel and Akbarnejad, Amir and G\"{u}nnemann, Stephan},
title = {Adversarial Attacks on Neural Networks for Graph Data},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220078},
doi = {10.1145/3219819.3220078},
abstract = {Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model.We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2847–2856},
numpages = {10},
keywords = {semi-supervised learning, adversarial machine learning, network mining, graph mining, graph convolutional networks},
location = {London, United Kingdom},
series = {KDD '18}
}

@INPROCEEDINGS{BinarizedAttack,
  author={Zhu, Yulin and Lai, Yuni and Zhao, Kaifa and Luo, Xiapu and Yuan, Mingquan and Ren, Jian and Zhou, Kai},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, 
  title={BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection}, 
  year={2022},
  volume={},
  number={},
  pages={14-26},
  doi={10.1109/ICDE53745.2022.00006}}

@inproceedings{HRAT,
author = {Zhao, Kaifa and Zhou, Hao and Zhu, Yulin and Zhan, Xian and Zhou, Kai and Li, Jianfeng and Yu, Le and Yuan, Wei and Luo, Xiapu},
title = {Structural Attack against Graph Based Android Malware Detection},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485387},
doi = {10.1145/3460120.3485387},
abstract = {Malware detection techniques achieve great success with deeper insight into the semantics of malware. Among existing detection techniques, function call graph (FCG) based methods achieve promising performance due to their prominent representations of malware's functionalities. Meanwhile, recent adversarial attacks not only perturb feature vectors to deceive classifiers (i.e., feature-space attacks) but also investigate how to generate real evasive malware (i.e., problem-space attacks). However, existing problem-space attacks are limited due to their inconsistent transformations between feature space and problem space.In this paper, we propose the first structural attack against graph-based Android malware detection techniques, which addresses the inverse-transformation problem [1] between feature-space attacks and problem-space attacks. We design a Heuristic optimization model integrated with Reinforcement learning framework to optimize our structural ATtack (HRAT). HRAT includes four types of graph modifications (i.e., inserting and deleting nodes, adding edges and rewiring) that correspond to four manipulations on apps (i.e., inserting and deleting methods, adding call relation, rewiring). Through extensive experiments on over 30k Android apps, HRAT demonstrates outstanding attack performance on both feature space (over 90% attack success rate) and problem space (up to 100% attack success rate in most cases). Besides, the experiment results show that combing multiple attack behaviors strategically makes the attack more effective and efficient.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3218–3235},
numpages = {18},
keywords = {android malware detection, function call graph, structural attack},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@Article{KMeans,
AUTHOR = {Ahmed, Mohiuddin and Seraj, Raihan and Islam, Syed Mohammed Shamsul},
TITLE = {The k-means Algorithm: A Comprehensive Survey and Performance Evaluation},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1295},
URL = {https://www.mdpi.com/2079-9292/9/8/1295},
ISSN = {2079-9292},
ABSTRACT = {The k-means clustering algorithm is considered one of the most powerful and popular data mining algorithms in the research community. However, despite its popularity, the algorithm has certain limitations, including problems associated with random initialization of the centroids which leads to unexpected convergence. Additionally, such a clustering algorithm requires the number of clusters to be defined beforehand, which is responsible for different cluster shapes and outlier effects. A fundamental problem of the k-means algorithm is its inability to handle various data types. This paper provides a structured and synoptic overview of research conducted on the k-means algorithm to overcome such shortcomings. Variants of the k-means algorithms including their recent developments are discussed, where their effectiveness is investigated based on the experimental analysis of a variety of datasets. The detailed experimental analysis along with a thorough comparison among different k-means clustering algorithms differentiates our work compared to other existing survey papers. Furthermore, it outlines a clear and thorough understanding of the k-means algorithm along with its different research directions.},
DOI = {10.3390/electronics9081295}
}

@inproceedings{
robustness_tradeoff,
title={Robustness May Be at Odds with Accuracy},
author={Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander Madry},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SyxAb30cY7},
}

@inproceedings{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={7472--7482},
  year={2019},
  organization={PMLR}
}

@book{Chung:1997,
  added-at = {2009-02-05T15:34:40.000+0100},
  author = {Chung, F. R. K.},
  biburl = {https://www.bibsonomy.org/bibtex/295ef10b5a69a03d8507240b6cf410f8a/folke},
  description = {This monograph is an intertwined tale of eigenvalues and their use in unlocking a thousand secrets about graphs. The stories will be told --- how the spectrum reveals fundamental properties of a graph, how spectral graph theory links the discrete universe to the continuous one through geometric, analytic and algebraic techniques, and how, through eigenvalues, theory and applications in communications and computer science come together in symbiotic harmony....},
  interhash = {0f0fd754924d4dd54bc185bd1c71d00b},
  intrahash = {95ef10b5a69a03d8507240b6cf410f8a},
  keywords = {graph spectral theory},
  publisher = {American Mathematical Society},
  timestamp = {2009-02-05T15:34:40.000+0100},
  title = {Spectral Graph Theory},
  year = 1997
}

@article{10.1145/2665063,
author = {Lee, James R. and Gharan, Shayan Oveis and Trevisan, Luca},
title = {Multiway Spectral Partitioning and Higher-Order Cheeger Inequalities},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {6},
issn = {0004-5411},
url = {https://doi.org/10.1145/2665063},
doi = {10.1145/2665063},
abstract = {A basic fact in spectral graph theory is that the number of connected components in an undirected graph is equal to the multiplicity of the eigenvalue zero in the Laplacian matrix of the graph. In particular, the graph is disconnected if and only if there are at least two eigenvalues equal to zero. Cheeger's inequality and its variants provide an approximate version of the latter fact; they state that a graph has a sparse cut if and only if there are at least two eigenvalues that are close to zero.It has been conjectured that an analogous characterization holds for higher multiplicities: There are k eigenvalues close to zero if and only if the vertex set can be partitioned into k subsets, each defining a sparse cut. We resolve this conjecture positively. Our result provides a theoretical justification for clustering algorithms that use the bottom k eigenvectors to embed the vertices into Rk, and then apply geometric considerations to the embedding.We also show that these techniques yield a nearly optimal quantitative connection between the expansion of sets of size ≈ n/k and λk, the kth smallest eigenvalue of the normalized Laplacian, where n is the number of vertices. In particular, we show that in every graph there are at least k/2 disjoint sets (one of which will have size at most 2n/k), each having expansion at most O(√λk log k). Louis, Raghavendra, Tetali, and Vempala have independently proved a slightly weaker version of this last result. The √log k bound is tight, up to constant factors, for the “noisy hypercube” graphs.},
journal = {J. ACM},
month = {dec},
articleno = {37},
numpages = {30},
keywords = {sparsest cut, spectral clustering, Cheeger's inequality, spectral algorithms}
}

@ARTICLE{EarlyStop,
  author={Girosi, Federico and Jones, Michael and Poggio, Tomaso},
  journal={Neural Computation}, 
  title={Regularization Theory and Neural Networks Architectures}, 
  year={1995},
  volume={7},
  number={2},
  pages={219-269},
  doi={10.1162/neco.1995.7.2.219}}

@inproceedings{GRV,
  title={Unsupervised adversarially robust representation learning on graphs},
  author={Xu, Jiarong and Yang, Yang and Chen, Junru and Jiang, Xin and Wang, Chunping and Lu, Jiangang and Sun, Yizhou},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={4},
  pages={4290--4298},
  year={2022}
}

@article{tSNE,
  title={Visualization of SNPs with t-SNE},
  author={Platzer, Alexander},
  journal={PloS one},
  volume={8},
  number={2},
  pages={e56883},
  year={2013},
  publisher={Public Library of Science San Francisco, USA}
}

@article{LocalGlobal, title={Local-Global Defense against Unsupervised Adversarial Attacks on Graphs}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/25979}, DOI={10.1609/aaai.v37i7.25979}, abstractNote={Unsupervised pre-training algorithms for graph representation learning are vulnerable to adversarial attacks, such as first-order perturbations on graphs, which will have an impact on particular downstream applications. Designing an effective representation learning strategy against white-box attacks remains a crucial open topic. Prior research attempts to improve representation robustness by maximizing mutual information between the representation and the perturbed graph, which is sub-optimal because it does not adapt its defense techniques to the severity of the attack. To address this issue, we propose an unsupervised defense method that combines local and global defense to improve the robustness of representation. Note that we put forward the Perturbed Edges Harmfulness (PEH) metric to determine the riskiness of the attack. Thus, when the edges are attacked, the model can automatically identify the risk of attack. We present a method of attention-based protection against high-risk attacks that penalizes attention coefficients of perturbed edges to encoders. Extensive experiments demonstrate that our strategies can enhance the robustness of representation against various adversarial attacks on three benchmark graphs.}, number={7}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Jin, Di and Feng, Bingdao and Guo, Siqi and Wang, Xiaobao and Wei, Jianguo and Wang, Zhen}, year={2023}, month={Jun.}, pages={8105-8113} }

@article{PiGCL, title={A New Mechanism for Eliminating Implicit Conflict in Graph Contrastive Learning}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29125}, DOI={10.1609/aaai.v38i11.29125}, abstractNote={Graph contrastive learning (GCL) has attracted considerable attention because it can self-supervisedly extract low-dimensional representation of graph data. InfoNCE-based loss function is widely used in graph contrastive learning, which pulls the representations of positive pairs close to each other and pulls the representations of negative pairs away from each other. Recent works mainly focus on designing new augmentation methods or sampling strategies. However, we argue that the widely used InfoNCE-based methods may contain an implicit conflict which seriously confuses models when learning from negative pairs. This conflict is engendered by the encoder’s message-passing mechanism and the InfoNCE loss function. As a result, the learned representations between negative samples cannot be far away from each other, compromising the model performance. To our best knowledge, this is the first time to report and analysis this conflict of GCL. To address this problem, we propose a simple but effective method called Partial ignored Graph Contrastive Learning (PiGCL). Specifically, PiGCL first dynamically captures the conflicts during training by detecting the gradient of representation similarities. It then enables the loss function to ignore the conflict, allowing the encoder to adaptively learn the ignored information without self-supervised samples. Extensive experiments demonstrate the effectiveness of our method.}, number={11}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={He, Dongxiao and Zhao, Jitao and Huo, Cuiying and Huang, Yongqi and Huang, Yuxiao and Feng, Zhiyong}, year={2024}, month={Mar.}, pages={12340-12348} }

@inproceedings{chameleon,
     title={The Network Data Repository with Interactive Graph Analytics and Visualization},
     author={Ryan A. Rossi and Nesreen K. Ahmed},
     booktitle={AAAI},
     url={https://networkrepository.com},
     year={2015}
}

@inproceedings{SPAGCL,
author = {In, Yeonjun and Yoon, Kanghoon and Park, Chanyoung},
title = {Similarity Preserving Adversarial Graph Contrastive Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599503},
doi = {10.1145/3580305.3599503},
abstract = {Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of different properties (i.e., the node similarity-preserving view and the adversarial view). Extensive experiments demonstrate that SP-AGCL achieves a competitive performance on several downstream tasks, and shows its effectiveness in various scenarios, e.g., a network with adversarial attacks,noisy labels, and heterophilous neighbors. Our code is available at https://github.com/yeonjun-in/torch-SP-AGCL.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {867–878},
numpages = {12},
keywords = {adversarial attack, graph representation learning, robust graph contrastive learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{GPS,
  title={GPS: Graph contrastive learning via multi-scale augmented views from adversarial pooling},
  author={Ju, Wei and Gu, Yiyang and Mao, Zhengyang and Qiao, Ziyue and Qin, Yifang and Luo, Xiao and Xiong, Hui and Zhang, Ming},
  journal={Science China Information Sciences},
  volume={68},
  number={1},
  pages={112101},
  year={2025},
  publisher={Springer}
}

@article{GODZISZEWSKI2024104173,
title = {Adversarial analysis of similarity-based sign prediction},
journal = {Artificial Intelligence},
volume = {335},
pages = {104173},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104173},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224001097},
author = {Michał T. Godziszewski and Marcin Waniek and Yulin Zhu and Kai Zhou and Talal Rahwan and Tomasz P. Michalak},
keywords = {Signed networks, Adversarial sign prediction},
abstract = {Adversarial social network analysis explores how social links can be altered or otherwise manipulated to hinder unwanted information collection. To date, however, problems of this kind have not been studied in the context of signed networks in which links have positive and negative labels. Such formalism is often used to model social networks with positive links indicating friendship or support and negative links indicating antagonism or opposition. In this work, we present a computational analysis of the problem of attacking sign prediction in signed networks, whereby the aim of the attacker (a network member) is to hide from the defender (an analyst) the signs of a target set of links by removing the signs of some other, non-target, links. While the problem turns out to be NP-hard if either local or global similarity measures are used for sign prediction, we provide a number of positive computational results, including an FPT-algorithm for eliminating common signed neighborhood and heuristic algorithms for evading local similarity-based link prediction in signed networks.}
}

@ARTICLE{10535517,
  author={Zhu, Yulin and Lai, Yuni and Zhao, Kaifa and Luo, Xiapu and Yuan, Mingquan and Wu, Jun and Ren, Jian and Zhou, Kai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={From Bi-Level to One-Level: A Framework for Structural Attacks to Graph Anomaly Detection}, 
  year={2025},
  volume={36},
  number={4},
  pages={6174-6187},
  keywords={Optimization;Anomaly detection;Training;Task analysis;Transforms;Topology;Representation learning;Adversarial graph analysis;discrete optimization;graph anomaly detection (GAD);graph neural networks;structural poisoning attack},
  doi={10.1109/TNNLS.2024.3400395}}

@ARTICLE{10693599,
  author={Lai, Yuni and Waniek, Marcin and Li, Liying and Wu, Jingwen and Zhu, Yulin and Michalak, Tomasz P. and Rahwan, Talal and Zhou, Kai},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Coupled-Space Attacks Against Random-Walk-Based Anomaly Detection}, 
  year={2024},
  volume={19},
  number={},
  pages={9315-9329},
  keywords={Anomaly detection;Feature extraction;Optimization;Robustness;Security;Vectors;Pipelines;Graph-based anomaly detection;random walk;poisoning attack;adversarial attacks;security and privacy},
  doi={10.1109/TIFS.2024.3468156}}

@ARTICLE{10433700,
  author={Zhu, Yulin and Michalak, Tomasz and Luo, Xiapu and Zhang, Xiaoge and Zhou, Kai},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Toward Secrecy-Aware Attacks Against Trust Prediction in Signed Social Networks}, 
  year={2024},
  volume={19},
  number={},
  pages={3567-3580},
  keywords={Predictive models;Measurement;Detectors;Task analysis;Optimization;Cryptocurrency;Feature extraction;Signed social networks;trust prediction;adversarial attack;secrecy-aware attack},
  doi={10.1109/TIFS.2024.3364366}}

@INPROCEEDINGS{10597723,
  author={Ai, Xing and Zhou, Jialong and Zhu, Yulin and Li, Gaolei and Michalak, Tomasz P. and Luo, Xiapu and Zhou, Kai},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach}, 
  year={2024},
  volume={},
  number={},
  pages={1213-1227},
  keywords={Learning systems;Finance;Contrastive learning;Topology;Fraud;Task analysis;Anomaly detection},
  doi={10.1109/ICDE60146.2024.00098}}

@INPROCEEDINGS{10446170,
  author={Han, Yuwei and Lai, Yuni and Zhu, Yulin and Zhou, Kai},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Cost Aware Untargeted Poisoning Attack Against Graph Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={4940-4944},
  keywords={Costs;Perturbation methods;Signal processing;Graph neural networks;Acoustics;Resource management;Speech processing;Poisoning attack;graph neural networks;node classification},
  doi={10.1109/ICASSP48485.2024.10446170}}

@ARTICLE{10296883,
  author={Lai, Yuni and Zhu, Yulin and Fan, Wenqi and Zhang, Xiaoge and Zhou, Kai},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Toward Adversarially Robust Recommendation From Adaptive Fraudster Detection}, 
  year={2024},
  volume={19},
  number={},
  pages={907-919},
  keywords={Robustness;Recommender systems;Training;Feature extraction;Anomaly detection;Adaptation models;Uncertainty;Recommender system;adversarial robustness;graph neural networks;anomaly detection;label uncertainty},
  doi={10.1109/TIFS.2023.3327876}}
