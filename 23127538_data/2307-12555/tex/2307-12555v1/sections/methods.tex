\section{Preliminaries}
\label{sec-preliminaries}
In this section, we will provide a brief introduction to the prerequisite knowledge and notations to our work.  

\subsection{Graph Representation Learning}
Given an attribute graph $\mathcal{G}=\{\mathbf{X},\mathbf{A}\}$, where $\mathbf{X}\in\mathbbm{R}^{N\times p}$ is the nodal attribute matrix and $\mathbf{A}\in\mathbbm{R}^{N\times N}$ is the adjacency matrix, graph representation learning aims at training a GNN encoder $f_{\theta}: \mathcal{G}\rightarrow \mathbbm{R}^{N\times d}$ to produce low-dimensional embeddings $\mathbf{H}\in\mathbbm{R}^{N\times d}$ for each node. Then, the pre-trained node embeddings can be fed into a graph-related downstream task such as node classification. $\theta$ summarizes the model parameters to be trained via a pre-defined loss function.     

\subsection{Graph Contrastive Learning Framework}
GCL is a powerful graph representation learning method that seeks to maximize the similarity between positive pairs (same node with different views) and enlarge the difference between negative pairs (different nodes within the same view and cross different views). 
More specifically, the training of the GCL model has two stages. Firstly, it generates two augmentation views $G_1$ and $G_2$ based on the input graph $G$ via pre-defined augmentation operations such as random link removal and feature masking. Then, Two graph samples generated from augmentation views are fed into a shared GNN encoder to produce node embeddings. Finally, a contrastive loss (infoNCE loss) based on the node embeddings is optimized and update the parameters of the GNN encoder. 
%{\color{red} this sentence is too long and not clear: More specifically, given a graph $\mathcal{G}$, graph contrastive learning first generates two different graph views $G_{1}=t_{1}(\mathcal{G})$ and $G_{2}=t_{2}(\mathcal{G})$ where $t_1\in\mathcal{T}_1$ and $t_2\in\mathcal{T}_2$ by implementing stochastic graph augmentations $\mathcal{T}_1$ and $\mathcal{T}_2$ on graph data to feed into a shared GNN encoder $f_{\theta}(\cdot)$ and then utilizes a crafted graph contrastive loss, i.e., infoNCE~\cite{infoNCE} to enforce the embeddings of each node for different views to be similar and enlarge the distance between different nodes.} 
The infoNCE loss is formulated as:
\begin{equation*}
    \begin{split}
        &\mathcal{L}_{info}(G_1, G_2)=\frac{1}{2N}\sum_{i=1}^{N}(l(u_i,v_i)+l(v_i,u_i)), \\
        &l(u_i,v_i)= \\
        &-\log\frac{e^{\rho(u_i,v_i)/\tau}}{\begin{matrix}
        \underbrace{e^{\rho(u_i,v_i)/\tau}} \\ \text{positive}
        \end{matrix}+
        \begin{matrix}
        \underbrace{\sum_{k\neq i}e^{\rho(u_i,v_k)/\tau}} \\ \text{inter-view negative}
        \end{matrix}+
        \begin{matrix}
        \underbrace{\sum_{k\neq i}e^{\rho(u_i,u_k)/\tau}} \\ \text{intra-view negative}
        \end{matrix}},
    \end{split}
\end{equation*}
where $u_{i}$ is the $i$-th node for $G_1$ and $v_{i}$ is the $i$-th node for $G_2$, $\tau$ is a temperature hyperparameter. Usually, the similarity function $\rho(\cdot)$ is defined as: 
\begin{equation*}
    \begin{split}
        \rho(u_i,v_i)=cos(g(\mathbf{H}_{1i}), g(\mathbf{H}_{2i}))=\frac{g(\mathbf{H}_{1i})\cdot g(\mathbf{H}_{2i})}{\|g(\mathbf{H}_{1i})\|\cdot\|g(\mathbf{H}_{2i})\|},
    \end{split}
\end{equation*}
which represents the cosine similarity between the projected embeddings of $u_i$ and $v_i$, $g(\cdot)$ is the projection head to enhance the expressive power of the graph representation learning, $\mathbf{H}$ is the node embeddings to be detailed later. In this paper, we use the two-layered graph convolution~\cite{GNN} as the shared GNN encoder for both two views to get the node embeddings $\mathbf{H}_1=f_{\theta}(G_1)$ and $\mathbf{H}_2=f_{\theta}(G_2)$ respectively. Specifically, $G_1=\{\mathbf{A}_1, \mathbf{X}_1\}$ and $G_2=\{\mathbf{A}_2, \mathbf{X}_2\}$; then we have
\begin{equation}
    \begin{split}
        &\mathbf{H}_l=f_{\theta}(\mathbf{A}_l,\mathbf{X}_l)=\sigma(\hat{\mathbf{A}}_l\sigma(\hat{\mathbf{A}}_l\mathbf{X}_{l}\mathbf{W}^{(1)})\mathbf{W}^{(2)}), \forall  l=1,2, \\
        &\text{where} \ \hat{\mathbf{A}}=\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}, \  \tilde{\mathbf{A}}=\mathbf{A+I}, \  \tilde{D}_{ii}=\sum_{j}\tilde{A}_{ij}.
    \end{split}
    \label{eqn-GNN-encoder}
\end{equation}
After that, the graph contrastive learning framework is formulated as follows:
\begin{equation*}
    \begin{split}
        \min_{\theta} \ \mathcal{L}_{info}(t_1(\mathcal{G}), t_2(\mathcal{G}), \theta), \ \text{where} \  t_l\in\mathcal{T}_l, \  l=\{1,2\}.
    \end{split}
\end{equation*}
Then, the optimized node embeddings $\mathbf{H}^{*}=f_{\theta^{*}}(\mathbf{A},\mathbf{X})$ act as the inputs to the logistic regression for semi-supervised node classification.

\subsection{Normalized Cut}
\label{sec-preliminaries-NormalizedCut}
Normalized cut loss~\cite{NormCut, maskGVAE} is an unsupervised loss function that counts  the fraction of the inter-class links for a given graph partition result. Thus, a good graph partition should obtain a lower normalized cut loss, i.e., the majority of the links connect two nodes with the same class. The normalized cut loss is defined as:
\begin{equation}
    \begin{split}
        &\mathcal{L}_{nc}=\frac{1}{K}\Tr((\mathbf{C}^{\top}\mathbf{LC})\oslash(\mathbf{C}^{\top}\mathbf{DC})), \ \mathbf{L}=\mathbf{I}-\hat{\mathbf{A}}.
    \end{split}
    \label{eqn-normalized-cut}
\end{equation}
Here $\mathbf{C}$ is the cluster assignment matrix, i.e., $C_{ij}$ represents the node $i$ belongs to the cluster $j$, $\mathbf{L}$ is the graph Laplacian matrix, $\oslash$ represents element-wise division. 
%{\color{red}Explain notation in the above equation} 
In this paper, we use this normalized cut loss as supervision to tune the hyperparameters in the model to eliminate the usage of the validation node labels. Thus, our model is a totally unsupervised learning method within the graph contrastive learning framework.

%\section{Vulnerability of GCL}
\section{Problem Statement}
\label{sec-problem}
%In this section, we introduce the adversarial environment of the graph contrastive learning framework and explore GCL's vulnerability. Those vulnerability explorations will in turn provide strong insights for the designing of the robust model.
In this section, we introduce the threat model and formally describe the problem of defense of GCL.

%\subsection{Adversarial Environment}
\subsection{Threat Model}
%{\color{red} Threat model: goal, knowledge, ability of attacker and defender}
%We consider an adversarial environment consisting of two roles against each other: the graph attacker aims at injecting topology noises to the clean graph to maliciously decrease the quality of GCL's node embeddings, while the defender tries to ameliorate the GCL's node embeddings for better downstream tasks' performances. In practice, the defender would first collect the data from the environment and construct the graph, then, the GCL models transform the graph data to low-dimensional node embeddings for ease of data analysis. Nevertheless, the graph attacker could maliciously hinder the data process procedure and produce a poisoned graph to deteriorate the graph-based tasks' performances. To formalize, we define the defense against the graph structural attacks on the graph contrastive learning framework from the information perspective:
We consider an adversarial environment involving two opposing roles. A graph attacker seeks to inject topology noise into the clean graph to intentionally reduce the quality of the node embeddings produced by GCL, while the defender aims to improve embeddings to enhance the performance of downstream tasks. In practical scenarios, the defender typically collects data from the environment and constructs a graph, after which the GCL models transform the graph data into low-dimensional node embeddings for ease of analysis. However, the graph attacker may interfere with the data processing procedure and create a poisoned graph to degrade the performance of graph-based tasks. 

Faced with attacks against GCL, our primary goal is to design a robust GCL model for the defender. We assume that the defender does not know the detailed generation mechanism of the attack graph as well as the strength of the attack.  The defender only has access to a poisoned graph and importantly, any label information is not available to the defender.
%We summarize the attacker's and defender's knowledge and ability as follows:
%\begin{itemize}
%    \item \textbf{Attacker's Knowledge}. By Kerckhoffsâ€™s principle, we assume the attacker has full access to the model's framework, training data and labels and testing data.
 %   \item \textbf{Attacker's Capability}. We still consider the worst-case scenario where the attacker can modify the topology of the graph data under constrained budget.  
 %   \item \textbf{Defender's Knowledge}. We assume the defender cannot access the generation mechanism of the poisoned graph and the corresponding attacking degree. 
 %   \item \textbf{Defender's Capability}. The defender can alter the topology of the input graph during training and cannot query the node labels. 
%\end{itemize}
%{\color{red} also describe the knowledge and ability of the attacker and defender.}


\subsection{Problem of Defense}
%To address this issue, we formulate a defense against graph structural attacks on the graph contrastive learning framework from an information perspective.
In this paper, our primary goal is to develop an adversarial robust GCL model that can generate high-quality embeddings from poisoned graphs. While the model robustness is often measured by its performance on downstream tasks, we go one step deeper to analyze the robustness through an information perspective. Specifically, we adopt a metric from the literature to measure robustness:
%{\color{red} Not accurate
\begin{definition}[GRV~\cite{GRV}]
\label{lemma-GRV}
    The graph representation vulnerability (GRV) quantifies the discrimination between the mutual information of the graph and its node representations based on clean graph and poisoned graph:
    \begin{equation}
        \begin{split}
            GRV(G,G^p)=I(G,f_{\theta}(G))-I(G^p,f_{\theta}(G^p)).
        \end{split}
    \end{equation}
\end{definition}
Basically, GRV can be utilized as a metric to quantify the adversarial robustness of an embedding-generating model $f_\theta$. Intuitively, when GRV is smaller, the embeddings generated from the poisoned graph are closer to those generated from the clean graph; that is, the model $f_\theta$ is more robust.  
%}
\begin{definition}[Defense of GCL]
\label{def-GCL-defense}
Given the poisoned graph generated by the graph attacker $G^{p}=\{\mathbf{X}, \mathbf{A}^{p}\}$, the defender aims to train a robust model $f_{\theta^{*}}$
%the node embeddings $\mathbf{Z}^{*}=f_{\theta^{*}}(\mathbf{X}, t^{*}(\mathbf{A}^{p}))$ which 
that can minimize the GRV defined in Definition~\ref{lemma-GRV}:
\begin{equation}
    \label{eqn-infomax}
    \begin{split}
        %\mathbf{Z}^{*}=\arg\min I(\mathbf{X},f_{\theta}(G))-I(\mathbf{X},\mathbf{Z}^{*}(G^p)),
        f^{*}=\arg\min_{f\in\mathcal{F}} I(G,f_{\theta}(G))-I(G^p,f_{\theta}(G^p)).
    \end{split}
\end{equation}

%where $I(\cdot,\cdot)$ represents the mutual information, $f_{\theta}(\cdot)$ represents the GNN encoder parametrized by weights $\theta$. 
%$t(\cdot)$ denotes the learnable augmentation view. 
\end{definition}
We note that the clean graph $G$ required to compute GRV is not available during the learning process. Thus, GRV is solely used as a robustness metric in the test phase in addition to the performances over downstream tasks.
%Definition.~\ref{def-GCL-defense} curves the robustness of the graph contrastive learning based on the information theory. We want to emphasize that the GRV metric defined in \cite{GRV} is hard to obtain in the real world since the defender cannot access the clean graph in the adversarial environment. Hence, we only use this metric in the experiment as an indirect support to illustrate the robust performance of GCL models apart from the downstream tasks' performances.  

%In reality, the complex mutual information $I(\mathbf{X}, f_{\theta}(t(G^{p})))$ is usually intractable, it is common to use the InfoNCE loss to approximate it. So we reformulate Eqn.~\ref{eqn-infomax} to
%\begin{equation}
%    \label{eqn-defense-infoNCE}
%    \begin{split}
%        \{\theta^{*}, t^{*}\}=\arg\min_{\theta, t} \mathcal{L}_{info}(f_{\theta}(t^{\prime}(G^{p})), f_{\theta}(t(G^{p}))),
%    \end{split}
%\end{equation}
%where $t^{\prime}(\cdot)$ is another augmentation view. 

%{\color{red} Need to first explicitly state the problem. Then we have the method.}


\section{Vulnerability Analysis of GCL}
\label{sec-vunlerability}
In this section, we provide an in-depth analysis, both empirically and theoretically, of how existing attacks would affect GCL through the lens of an important property of graphs. These explorations of the vulnerabilities of GCL provide strong insights for our design of the robust GCL. 
%\subsection{Graph Attack \& Homophily}
\subsection{Empirical Analysis of Attack Effects}
\label{sec-attack-homophily}
A fundamental property that guides GCL is \textit{homophily}, which states that the nodes connected by edges in the graph tend to be more similar. In fact, the majority of the graphs observed in the real world are homophilous~\cite{homophily}. Previous studies have designed various metrics to measure the degree of homophily of a graph.  A primary metric is to calculate the 
percentage of the intra-class links in a graph:
%{\color{red} this is not our definition.}
\begin{definition}[Graph homophily on label space~\cite{homophily}]
    The graph homophily based on node labels is given by:
    \begin{equation}
        \begin{split}
        h_{y}=\frac{1}{\mathcal{E}}\sum_{(v_{i},v_{j})\in\mathcal{E}}\mathbbm{1}_{(y_{i}=y_{j})}, \label{eqn-homo-label}
        \end{split}
    \end{equation}
    where $y_{{i}}$ is the label of node $v_{i}$, $\mathcal{E}$ is the link set.
%{\color{red} $h_x$ measures a kind of distance. The meaning is opposite to $h_y$, so use different terms and notations.}
\end{definition}
Besides using node labels, the graph homophily can also be measured by the Euclidean distance between the attribute vectors of the connected nodes~\cite{XLX,ProGNN}:
\begin{definition}[Graph homophily on feature space~\cite{ProGNN}]
    The graph homophily based on node attributes is given by:
    \begin{equation}
        \begin{split}
            \delta_{x}=\frac{1}{2}\sum_{i,j=1}^{N} A_{ij}(\frac{\mathbf{x}_{i}}{\sqrt{d_{i}}}-\frac{\mathbf{x}_{j}}{\sqrt{d_{j}}})^2=tr(\mathbf{X}^{T}\mathbf{L}\mathbf{X}).
        \end{split}
    \end{equation}
    where $\mathbf{L}=\mathbf{I}-\mathbf{\tilde{D}}^{-\frac{1}{2}}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-\frac{1}{2}}$ is the normalized graph Laplacian, $\mathbf{\tilde{A}}$ is the adjacency matrix with self-loop. 
\end{definition}
A higher $h_y$ (or a lower $\delta_x$) indicates that the graph tends to be more homophilous. 

Previous studies~\cite{HeteRobust} have revealed that attacks against GNNs (e.g., Mettack~\cite{Mettack}) tend to inject inter-class edges into the clean graph to downgrade the performance of GNNs.
%It has been observed that Mettack tends to inject inter-class edges to the clean graph to degenerate the GNN's performance~\cite{HeteRobust}. 
We then conduct comprehensive experiments to verify whether attacks against GCL models have similar properties. 
%To verify that the graph attackers (Mettack and CLGA) present a similar property to the GCL models, we deploy empirical analysis on how the graph attacker effect both the GCL model and the graph homophily.
Specifically, we test an existing attack method (termed CLGA~\cite{CLGA}) against a particular GCL model (GCA~\cite{GCA}) on the Cora dataset as an example. We also test Mettack on GCL model. We firstly train CLGA and Mettack (Mettack can access to the data split) on the clean graph and generate the corresponding poisoned graph with multiple attacking powers ranged in $\{5\%, 10\%, 15\%, 20\%\}$. Then, we feed the GCL model with the poisoned graph to evaluate the performance on node classification.
%{\color{red} explain how mettack is used to attack GCL}
%We take Cora~\cite{CitationNetwork} dataset as an exemplar. 
Fig.~\ref{fig-attack-vs-homo} presents the relationship between the semi-supervised node classification accuracy of GRACE~\cite{GRACE} (a typical GCL baseline) and the graph homophily metrics ($h_{y}$ and $\delta_{x}$) on poisoned graphs with varying attacking powers. It is clearly observed that the accuracy is positively correlated with $h_y$ and is negatively correlated with $\delta_x$. That is, both attacks will significantly decrease the homophily of the graph.
%It is observed that the graph attacker tends to increase $\delta_x$ and decrease $h_y$ and $\delta_x$ and $h_y$ are negatively correlated with each other. 
%Since our work focuses on unsupervised graph contrastive learning without querying node labels, it is more convenient to use $\delta_x$ as the proxy to measure the graph homophily. 
% Figure environment removed

\begin{table}[h]
	\centering
	\caption{Number of inserted intra-class links, inserted inter-class links, deleted intra-class links and deleted inter-class links for Mettack and CLGA on Cora dataset.}
	\label{tab-homo-attack}
	\resizebox{0.8\columnwidth}{!}{%
		\begin{tabular}{cccccc}
			\toprule[1.pt]
			Attack & Attack power & $5\%$ & $10\%$ & $15\%$ & $20\%$ \\
        \hline
	\multirow{4}*{Mettack} & \#Inserted intra-class links & $3$ & $9$ & $14$ & $25$ \\
               & \#Inserted inter-class links & $248$ & $486$ & $705$ & $914$ \\
               & \#Deleted intra-class links & $2$ & $11$ & $38$ & $68$ \\
               & \#Deleted inter-class links & $0$ & $0$ & $3$ & $6$\\
               \hline
               \multirow{4}*{CLGA} & \#Inserted intra-class links & $18$ & $35$ & $59$ & $77$ \\
               & \#Inserted inter-class links & $232$ & $465$ & $690$ & $912$ \\
               & \#Deleted intra-class links & $3$ & $5$ & $10$ & $21$ \\
               & \#Deleted inter-class links & $0$ & $1$ & $1$ & $3$ \\
			\bottomrule[1.pt]
		\end{tabular}
	}
\end{table}

% Figure environment removed
%In the meanwhile, Tab.~\ref{tab-homo-attack} provides the number of inserted intra-class links, inserted inter-class links, deleted intra-class links and deleted inter-class links for Mettack and CLGA with varying attacking powers. The results demonstrate that for CLGA and Mettack the majority of manipulations are inserting the inter-class links to the clean graph and is consistent with the phenomenon presented in Fig.~\ref{fig-attack-vs-homo}.   
In addition, Tab.~\ref{tab-homo-attack} shows the number of inserted intra-class links, inserted inter-class links, deleted intra-class links, and deleted inter-class links for Mettack and CLGA at different attacking powers. The results indicate that, for both CLGA and Mettack, the majority of manipulations involve inserting inter-class links into the clean graph. This finding is consistent with the phenomenon presented in Fig.~\ref{fig-attack-vs-homo}. All these experiment results demonstrate that attacks against GCL take effect by reducing the homophily levels of graphs.



%\subsection{Graph Attack \& GCL}
\subsection{Theoretical Analysis}
\label{sec-attack-GCL}
%{\color{red}the analysis should be a part of your methodology. it should not appear in the problem statement section}
In this section, we provide the theoretical analysis of how the attacks can negatively impact the learning of the GCL model using mutual information as the bridge. For ease of proof, we first introduce a lemma:
%$the information theory perspective.


%{\color{red} I did not read this theorem carefully. Please make sure it is correct.}
\begin{lemma}[infoNCE~\cite{infoNCE}]
\label{lemma-infoNCE}
The infoNCE object~\cite{GCA} $I_{info}(\cdot)=-\mathcal{L}_{info}(\cdot)$ is the lower bound approximation of the intractable mutual information between the graph and its representations $I(G, \mathbf{Z}=f_{\theta}(G))$.
%{\color{red} Is this function I (X,G) correct?}
\end{lemma}
Then, we provide the theorem to curve the relationship between the graph attacker and the mutual information $I(G,\mathbf{Z})$.
\begin{theorem}
\label{theorem-attack-info}
    The %graph attacker (Mettack or CLGA) 
    two attacks, Mettack and CLGA, maliciously degenerate the GCL's performance by minimizing the mutual information between the graph and its representations. 
\end{theorem}
\begin{proof}
    We denote the poisoned graph generated by the graph attacker as $G^{p}=\{\mathbf{X},\mathbf{A}^{p}\}$, the corresponding poisoned node embeddings as $\mathbf{Z}^{p}=f_{\theta}(G^{p})$, where $f_{\theta}(\cdot)$ is the GNN encoder. Given the infoNCE object $I_{info}(\cdot)=-\mathcal{L}_{info}(\cdot)$, if $G^{p}$ is generated by CLGA, we have
    \begin{equation}
        G^{p}=\arg\min_{G} \ I_{info}(t_{1}(G), t_{2}(G)),
    \end{equation}
    since the infoNCE loss is the objective function of CLGA. Then, we have 
    \begin{equation}
    \label{eqn-attack-Info}
    \begin{split}
        I_{info}(t_{1}(G), t_{2}(G))>I_{info}(t_{1}(G^{p}), t_{2}(G^{p})), 
    \end{split}
    \end{equation}
    based on Lemma~\ref{lemma-infoNCE}, we have:
    \begin{equation}
    \label{eqn-attack-MI}
    \begin{split}
        I(G, \mathbf{Z}=f_{\theta}(G))>I(G^p, \mathbf{Z}^{p}=f_{\theta}(G^p)).
    \end{split}
    \end{equation}
    If $G^{p}$ is generated by Mettack, we have
    \begin{equation}
        \begin{split}
            G^p=\arg\max_{G} \ \mathcal{L}_{CE}(\mathbf{Z}^{p}, \mathbf{Y}), 
        \end{split}
    \end{equation}
    where $\mathcal{L}_{CE}$ is the cross-entropy loss. We then partition the $\mathcal{L}_{CE}$ as: 
    \begin{equation}
    \label{eqn-proof-CE}
        \begin{split}
            \mathcal{L}_{CE}&=H(\mathbf{Y})+D_{KL}(p_Y\|p_Z) \\
            &=H(\mathbf{Y}|\mathbf{Z})+I(\mathbf{Y},\mathbf{Z})+D_{KL}(p_Y\|p_Z) \\
            &=H(\mathbf{Y}|\mathbf{Z})+D_{KL}(p_{YZ}\| p_Y p_Z)+D_{KL}(p_Y\|p_Z),
        \end{split}
    \end{equation}
    Next, we build the relationship between mutual information $I(G,\mathbf{Z})$ with the conditional entropy $H(\mathbf{Y}|\mathbf{Z})$ as 
    \begin{equation}
    \label{eqn-proof-MI}
        \begin{split}
            I(G,\mathbf{Z})&=H(G)-H(G|\mathbf{Z})\\
            &=H(G)-H(\mathbf{Y}|\mathbf{Z})+H(\mathbf{Y}|G,\mathbf{Z})-H(G|\mathbf{Y},\mathbf{Z}),
        \end{split}
    \end{equation}
    Based on Eqn.~\ref{eqn-proof-CE} and \ref{eqn-proof-MI}, we have
    \begin{equation}
    \label{eqn-proof-MI-CE}
        \begin{split}
            I(G, \mathbf{Z})=&H(G)-\mathcal{L}_{CE}+D_{KL}(p_{YZ}\|p_{Y}p_{Z})+D_{KL}(p_{Y}\|p_{Z})\\
            +&H(\mathbf{Y}|G,\mathbf{Z})-H(G|\mathbf{Y},\mathbf{Z}).
        \end{split}
    \end{equation}
    Based on Eqn.~\ref{eqn-proof-MI-CE}, we observe that the mutual information $I(G,\mathbf{Z})$ is negatively correlated with $\mathcal{L}_{CE}$. Since Mettack can increase the $\mathcal{L}_{CE}$:
    \begin{equation}
        \begin{split}
            \mathcal{L}_{CE}(\mathbf{Z},\mathbf{Y})<\mathcal{L}_{CE}(\mathbf{Z}^{p},\mathbf{Y}),
        \end{split}
    \end{equation}
    then, Eqn.~\ref{eqn-attack-MI} also holds for the poisoned graph generated by Mettack. 
\end{proof}



%{\color{red} This is not a lemma. Lemma is used to prove a theorem. Simply state this conclusion as sentences.}


%{\color{red} the below description is still very vague. You should be very explicit. How attack will impact the training of GCL? you should clearly summarize the relation among attack, training objective, and mutual information.}
%\st{Hence, the theoretical analysis provides the clue that both CLGA and Mettack will directly produce detrimental effects on the objective of the graph contrastive learning framework and thus lead to low-quality node embeddings.}
Theorem \ref{theorem-attack-info} demonstrates that by maliciously altering the graph topology, attacks can decrease the mutual information between the graph data and its representations. On the other hand, GCL's high-quality node representations are derived from maximizing this mutual information. Therefore, the GCL model may only be able to optimize the mutual information to a sub-optimal level, resulting in the production of low-quality embeddings. 

%\textcolor{blue}{Thus, the poisoned graph generated by the graph attacker will provide detrimental effects to the quality of the GCL's node embeddings through eliminating the consistency between the graph and its representations during training. As a result, it is difficult for the GCL model to optimize the infoNCE loss based on the poisoned graph to a relatively low value and produce sub-optimal results.}


\section{Defense Methodology}
\label{Sec-Methodology}
In this section, we first provide an overview of our framework to clearly describe the whole picture and then clarify our homophily-driven learnable sanitation view for robust graph contrastive learning.  
%\subsection{Pre-Analysis on Graph Attack}{\color{red} more insights and empirical study? compare to adversarial training?}Referring to \cite{HeteRobust}, It has been proved that the graph attacker tends to break the graph homophily by adding heterophilous links or removing homophily links in the graph. The results in Fig.~\ref{fig-attack-vs-homo} are consistent with this argument. We also summarize the number of the injected heterophilous links, injected homophilous links, pruned heterophilous links and pruned homophilous links with varying attacking powers for Mettack and CLGA in Tab.~\ref{}.

\subsection{Overview} 

In this paper, we propose the homophily-driven learnable sanitation view for enhancing the robustness of unsupervised graph contrastive learning. 
%\textcolor{blue}{The overall framework of the proposed model is depicted in Fig.. Similar to vanilla GCL framework, our model is constructed by data augmentation phase and contrastive learning phase. In the first phase, we design a homophily-driven learnable sanitizer to stochastically generate sanitation view. The learnable sanitizer is parametrized by the link removal probability matrix. In the second phase, the sanitizer generate sanitation view and contrast this view with the second augmented view which is generated by random link removal. Then, we feed the sanitation view and augmented view to a share GNN encoder and produce two node features for contrast. Finally, a crafted homophily-driven objective is optimized in an end-to-end manner based on the two node features and update the link removal probability matrix and the parameters in GNN encoder via gradient descent.}
The proposed model's overall framework is illustrated in Fig.~\ref{fig-overview}. Similar to the vanilla GCL framework, our model comprises a data augmentation phase and a contrastive learning phase. In the first phase, we introduce a homophily-driven learnable sanitizer that stochastically generates a sanitation view. This learnable sanitizer is parameterized by the link removal probability matrix. In the second phase, the sanitizer generates a sanitation view and contrasts it with the second augmented view, which is produced by random link removal. We then feed these two views to a shared GNN encoder and generate two node features for contrast. Finally, we optimize a homophily-driven objective in an end-to-end manner based on the two node features, updating the link removal probability matrix and the GNN encoder's parameters via gradient descent.


%{\color{red}The overall framework of the proposed model is depicted in Fig.~\ref{fig-overview}. In order to enhance the robustness of the graph contrastive learning framework, our work aims at sanitizing the given poisoned graph during the augmentation phase of the graph contrastive learning framework for obtaining the higher-quality of node embeddings. More specifically, our work contains two augmentation view and one of them is the learnable sanitation view. The sanitation view is specially designed to stochastically generate sanitized graph sample for each iteration and is fed into a shared GNN encoder and output the sanitized node embeddings. Then, the GCL model contrasts the node embeddings generated from these two views in node level and optimize the infoNCE loss to update the parameters. Finally, the node embeddings generated by the sanitation view serve as the input for graph-based downstream tasks.   } 

%\subsection{Edge-dropping Scheme of Sanitation View}
\subsection{End-to-End Design of Learnable Sanitizer}
In this section, we introduce our design and implementation of the learnable sanitizer, which can generate sanitation views for GCL. In particular, this sanitizer is seamlessly embedded into the GCL framework and is jointly trained with the contrastive learning process.


\subsubsection{Edge-dropping sanitizer}
Our design of the sanitizer relies on a stochastic edge-dropping scheme that aims to sanitize the inter-class links injected by the attacker. This design is based on the preliminary experiments (Section~\ref{sec-attack-homophily}) that existing attacks tend to insert inter-class links into the clean graph. Specifically, we use an edge vector $\mathbf{E}$ to represent all the existing edges in a graph. Then, we define a manipulation vector $\mathbf{M} = \{0,1\}^{|\mathbf{E}|}$ that follows the Bernoulli distribution parameterized by $\mathbf{P}$; that is $\mathbf{M}\sim Ber(\mathbf{P})$. For a particular entry $\mathbf{M}_e$, we will remove the corresponding edge $e$ if $\mathbf{M}_e = 1$, which is controlled by the probability $\mathbf{P}_e$. Now, the sanitizer can be regarded as a mapping $\mathcal{T}_{\mathbf{P}} (\mathbf{E}) \rightarrow \mathbf{E}_{\mathcal{S}}$ parameterized by $\mathbf{P}$, which takes a graph with edge vector $\mathbf{E}$ as input and produce a sanitized view $\mathbf{E}_{\mathcal{S}}$. Formally, we have:
%In our framework, the sanitation view focuses on implementing stochastic topology augmentation to automatically sanitize the candidate inter-class links injected by the graph attacker in the poisoned graph. To this end, we focus on the edge-dropping scheme for the sanitation view and parametrize it with the edge-dropping probability matrix which follows the Bernoulli distribution. More specifically, we define the sanitation view as a crafted mapping to the edge vector $\mathbf{E}$ of the input graph and a manipulation vector $\mathbf{Z}$ with each entry following the Bernoulli distribution:
\begin{equation}
    \begin{split}
        \mathbf{E}_{\mathcal{S}}=\mathcal{T}_{\mathbf{P}}(\mathbf{E}) \triangleq \mathbf{E}\odot(\mathbf{1}_{|\mathbf{E}|}-\mathbf{M}), \ \ \mathbf{M}\sim Ber(\mathbf{P}),
    \end{split}
    \label{eqn-san-sampling}
\end{equation} 
where $\odot(\cdot)$ is the element-wise product operation, $\mathbf{1}_{|\mathbf{E}|}$ is an all-one vector with length equal to the edge number ${|\mathbf{E}|}$, $\mathbf{P}=\{p_{e}\}_{e=1}^{|\mathbf{E}|}$ is the parameters of the sanitizer. 
%Here $\mathbf{Z}_{e}=1$ means the sanitation view removes the corresponding link $e$ and vice versa. 

Thus, the remaining task amounts to choosing the proper parameters $\mathbf{P}$ for the sanitizer $\mathcal{T}_{\mathbf{P}}$ so that the generated sanitation views can facilitate contrastive learning to achieve adversarial robustness. Next, we show how to learn the parameters $\mathbf{P}$.


%Therefore, the sanitation view aims at choosing appropriate parameters $\mathbf{P}$ as a supervisor to produce a sanitized graph. Then, the node embeddings generated from the sanitized graph can enhance the robustness of the GCL model. Next, we will clarify the motivations and give a detailed description of the design of the learnable sanitation view.

%To this end, we \ul{focus on the link sanitation augmentation scheme based on a Bernoulli probability matrix, where each entry represents the pruning link probability respectively}. 

%We define the {\color{red} topology augmentation of the sanitation view} $\mathcal{T}(\mathcal{G})$ as a random vector $\mathbf{Z}=\mathcal{B}(\mathbf{E})$ {\color{red} whose entries follow the Bernoulli distribution} $Ber(\mathbf{P})$ where $\mathbf{P}=\{p_{e}\}_{e=1}^{E}$ is the parameter vector to be trained, $E$ is the link number. Then, we get the link set for the sanitation view as 
%\begin{equation}
%    \begin{split}
%        \mathbf{E}_1=\mathcal{B}(\mathbf{E})=\mathbf{E}\odot(\mathbf{1}_{E}-\mathbf{Z}), \ \text{where} \ \mathbf{Z}\sim Ber(\mathbf{P}).
%    \end{split}
%    \label{eqn-san-sampling}
%\end{equation}    
%where $\odot(\cdot)$ is the element-wise product operation. Here $\mathbf{Z}_{e}=1$ means the {\color{red} view generator} removes the corresponding link $e$. Therefore, the sanitation view aims at choosing appropriate parameters $\mathbf{P}$ to supervise the graph generator to produce a sanitized graph which can enhance the robustness of the graph contrastive learning.

%\subsection{Design of Learnable Sanitizer}
%\label{Sec-sanitizer-design}
%In this part, we provide the motivations on how to craft a reasonable edge-dropping probability matrix $\mathbf{P}$ defined in Eqn.~\ref{eqn-san-sampling}.  

%{\color{red} up to now, it is the general description. You did not even specify whether $P$ is fixed or learnable. You did not specify how to obtain $P$. Below, you introduce your implementation.}

%{\color{red} There are several steps for the implementation, for which you can use section titles to make them clear.
%\begin{itemize}
%    \item How to formulate the learning problem for the generator, specifically, the objective function.
%    \item How to solve the optimization problem?
%    \item There are actually two phases? learn the parameter $P$ of the generator and generate graphs during execution?
%\end{itemize}}
%\textcolor{red}{describe the insight for infoNCE loss in new section}
%\subsubsection{Formulation}
\subsubsection{End-to-end learning}
The key challenges to learning $\mathcal{T}_{\mathbf{P}}$ are two-fold: choosing the proper objective to guide learning, and integrating learning into the framework of GCL. In this paper, we design the objective function as follows (while deferring the explanation to the next section):
%In our framework, we target on designing a learnable $\mathbf{P}$ to automatically determine the best sampling procedure for the sanitation view during training. Then, the choice of the loss function to supervise the optimization of $\mathbf{P}$ is the bottleneck. To formalize, we design the objective function as:
\begin{equation}
    \begin{split}
        \min_{\mathbf{P}\in\mathcal{S}_1, \theta} \ &\mathcal{L}=\mathcal{L}_{info}(f_{\theta}(\mathbf{E}_{\mathcal{S}}), f_{\theta}(t_\mathcal{R}(\mathbf{E})))+\eta\Tr(\mathbf{X}^{\top}\mathbf{L}_{\mathcal{S}}\mathbf{X}), \\ 
        &\text{s.t.} \ \mathbf{E}_{\mathcal{S}}=\mathbf{E}\odot(\mathbf{1}_{|\mathbf{E}|}-\mathbf{M}), \ \mathbf{M}\sim Ber(\mathbf{P}),\\
        & \quad \  t_{\mathcal{R}}(\mathbf{E})\in\mathcal{T}_{\mathcal{R}}, \ \mathbf{L}_{\mathcal{S}}=\mathbf{I}-\tilde{\mathbf{D}}_{\mathcal{S}}^{-\frac{1}{2}}(\mathbf{A}_{\mathcal{S}}+\mathbf{I})\tilde{\mathbf{D}}_{\mathcal{S}}^{-\frac{1}{2}},
    \end{split}
    \label{eqn-san-obj-2}
\end{equation} 
%{\color{red} add $T_p$ into equation 16, currently, the dependency on P is not clear}
where the first view $\mathbf{E}_{\mathcal{S}}$ is the sanitation view generated from the sanitizer, the second view $t_\mathcal{R}(\mathbf{E})$ is generated from typical random augmentation scheme $\mathcal{T}_R$ that will randomly drop edges with a fixed probability, $\Tr()$ represents the trace of a matrix  and $\mathbf{S}_1$ is a constrained space of the sanitation operation with $\epsilon$ controlling the sanitation degree: 
%Furthermore, we restrict the sanitation operation to a constrained space: 
\begin{equation}
    \begin{split}
        \mathbf{S}_1=\{s|s\in[0,1]^{E}, \|s\|_1\leq \epsilon\}.
    \end{split}
\end{equation}
$\mathbf{L}_{\mathcal{S}}$ is the graph Laplacian based on the sanitation view $\mathbf{E}_{\mathcal{S}}$.
%where $\epsilon$ controls the sanitation degree. 

In our design, the training objective of the sanitizer is embedded into that of the GCL. That is, we train the sanitizer together with the contrastive learning, using a hyperparameter $\eta$ to control the relative importance of optimizing the %We optimize the sanitation view in a multi-task learning manner and trade-off between the importance of optimizing 
infoNCE loss and the graph homophily $\delta_{x}$. %
Then, the learnable sanitizer sample the sanitation view $\mathbf{E}_{\mathcal{S}}$ based on Eqn.~\ref{eqn-san-sampling} for each iteration. After that, the shared GNN encoder $f_{\theta}(\cdot)$ converts the graph data from the two views to low-dimensional node embedding matrices and obtain the training loss $\mathcal{L}$. During the backward pass, we compute the gradient of $\mathcal{L}$ with respect to the parameters $\mathbf{P}$ and $\theta$ and update them via gradient descent.
%by fine-tuning the value of $\eta$. 
%{\color{red} how the sanitation view is sampled in each iteration is not explained}

%In this way, the sanitation view is specially designed to mine a better topology on the poisoned graph and can offer a good-quality sample for contrastive learning. Moreover, we design the optimization of the sanitation view in an end-to-end manner and jointly train all the parameters by gradient descent. Later, we will provide the ablation study in Sec.~\ref{Sec-exp} to verify the importance of these two components. 

\subsubsection{Insights behind our design}
\label{Sec-insights}
The objective function of our robust model in Eqn.~\ref{eqn-san-obj-2} contains two vital components: the infoNCE loss over two views and the graph homophily measurement $\delta_{x}=\Tr(\mathbf{X}^{\top}\mathbf{LX})$. 
In Sec.~\ref{sec-attack-GCL}, it was (Theorem~\ref{theorem-attack-info}) shown that the attacks would degrade the consistency (mutual information) between the graph and its representations, leading to a decrease in the quality of node embeddings. 
Therefore, an effective robust model should restore the properties distorted by malicious topology manipulations, acting as a countermeasure against the graph attacker. To achieve this, 
the sanitizer should be designed in a way that the mutual information between graph and representation is preserved. 
%should be designed to preserve the consistency (mutual information) between the graph and its representations. {\color{red} what's the difference between infoNCE object and infoNCE loss?}
As the infoNCE objective is a common approximation to the intractable mutual information,
%Since the infoNCE objective is the common approximation to the intractable mutual information statistics, 
we optimize the sanitation view to minimize the infoNCE loss (i.e., maximize the mutual information). There are other graph sanitation frameworks for GNNs, such as those proposed in \cite{GASOLINE, FocusedCleaner}, which adopt the cross-entropy loss as the objective for sanitation. 
However, these are infeasible in an unsupervised learning setting as they require access to node labels for training.
Nevertheless, we want to emphasize that we are the first to theoretically describe the inherent relationship between the objective of the GCL model and the graph attacks, which provides strong insights for learning the sanitation view.


%Other graph sanitation framework on GNN~\cite{GASOLINE, FocusedCleaner} design the sanitation scheme to optimize the cross-entropy loss. However, we emphasize that we are the first to theoretically describe the inherent relationship between the objection of the GCL model and the graph attacker and this theory in turn is the strong supporting to our insights for the design of sanitation view.    

Furthermore, as discussed in Sec.~\ref{sec-attack-homophily}, it has been observed that attacks (Mettack and CLGA) tend to add heterophilous links, which can decrease the homophily degree of the graph (e.g., increasing $\delta_{x}$). Based on this observation, we choose the graph homophily metric $\delta_{x}$ as a second supervision term to sanitize the poisoned graph to obtain the sanitation view. Compared to $h_y$, which requires node labels, $\delta_{x}$ does not, and is thus suitable for unsupervised learning. 
%This helps us refine the distorted properties caused by the graph attacker.
%Furthermore, as discussed in Sec.~\ref{sec-attack-homophily}, it has been observed that attacks (Mettack and CLGA) are inclined to add heterophilous links and thus decrease the homophily degree of the graph data (increase $\delta_{x}$). Based on this fact, we choose the graph homophily metric $\delta_{x}$ as another supervisor to sanitize the poisoned graph on the sanitation view to refine the distorted property caused by the graph attacker.  
In Sec.~\ref{Sec-exp}, we provide a comprehensive ablation study to verify the significance of these two components.
%Later, we will provide the ablation study  to verify the importance of these two components. 


%Inspired by the graph sanitation framework on GNN~\cite{GASOLINE, FocusedCleaner}, a useful way to improve the quality of the initially given graph is to optimize the topology of the given graph based on the loss function of the downstream tasks. In this way, this refined topology is specially crafted for the specific graph-based learning task and hence can enhance the corresponding performance. For example, an appropriate way of refining the given graph is to learn its topology via minimizing the NLL (negative log-likelihood) loss for the semi-supervised node classification task~\cite{GASOLINE}. For the unsupervised graph contrastive learning framework, we choose the InfoNCE loss as a supervisor to mine the topology of the poisoned graph on the sanitation view, i.e., 
%However, choosing an appropriate proxy to supervise the graph generator for the sanitation view remains a problem. Intuitively, a suitable proxy should serve as a good graph miner to produce a more sanitized graph. Referring to \cite{GASOLINE, FocusedCleaner}, the NLL (negative log-likelihood) loss {\color{red} of what?} is a suitable proxy and minimizing the NLL loss can lead to a more sanitized graph. Unfortunately, NLL loss requires the node label's information and is not suitable for unsupervised graph contrastive learning. To tackle this issue, {\color{red} we choose the infoNCE loss as the proxy to mine the topology of the initially given graph} during the graph augmentation process, i.e., 
%\begin{equation}
%    \begin{split}
%        \min_{\mathbf{P}\in\mathcal{S}_1, \theta} \ \mathcal{L}_{info}(t_1(\mathcal{G}), t_2(\mathcal{G}), \theta), \ \text{s.t.} \ t_1\in\mathcal{T}_{\mathbf{P}}, t_2\in\mathcal{T}_2,
%    \end{split}
%    \label{eqn-san-obj-1}
%\end{equation}
%Besides, it has been observed that graph structural attacks (Mettack and CLGA) are inclined to add heterophilous links and thus decrease the homophily degree of the graph data. Based on this fact, we choose the graph homophily metric $\delta_{x}$ as another supervisor to sanitize the poisoned graph on the sanitation view. We reformulate the objective function of the sanitation view in a multi-task learning manner, i.e.,  
%it is natural to penalize low level of graph homophily during the view generation phase to produce a more sanitized graph. Hence, we reformulate Eqn.~\ref{eqn-san-obj-1} as:

\subsection{Implementation}
Now, we introduce several techniques to efficiently solve the optimization problem~\eqref{eqn-san-obj-2}.

%{\color{red} make sure that all the notations are correct.}
\subsubsection{Gumbel-Softmax Re-parametrization}
To optimize the parameters $\mathbf{P}$ via gradient descent, it is necessary to tackle the in-differentiation of the sampling procedure in Eqn.~\ref{eqn-san-sampling} during the backward pass.  
%\subsection{Method}
%However, we encounter three vital problems when optimizing Eqn.~\ref{eqn-san-obj-2}: \textbf{First}, we cannot access to the gradient of $\mathbf{P}$ directly since the sampling procedure in Eqn.~\ref{eqn-san-sampling} is non-differentiable; \textbf{Second}, optimizing $\mathbf{P}$ under the budget $\mathcal{S}_1$ is a complex constrained optimization problem which is hard to solve; \textbf{Third}, how to determine the value of $\eta$ remains a problem. 
To tackle this problem, we utilize the Gumbel-Softmax re-parametrization technique~\cite{GumbelSoftmax} to relax the discrete Bernoulli sampling procedure to a differentiable Bernoulli approximation. Using the relaxed sample, we implement the straight-through estimator~\cite{STE} to discretize the relaxed samples in the forward pass and set the gradient of discretization operation to $1$, i.e., the gradients are directly passed to the relaxed samples rather than the discrete values. Hence, we reconstruct the mapping from $\mathbf{P}$ to $\mathbf{M}$ as:
\begin{equation}
    \begin{split}
        \mathbf{M}_{e}=\lfloor\frac{1}{1+e^{-(\log\mathbf{P}_e+g)/\tau}}+\frac{1}{2}\rceil,
    \end{split}
\end{equation}
where $\lfloor\cdot\rceil$ is the rounding function, $g\sim Gumbel(0,1)$ is a standard Gumbel random variable with zero mean and the scale equal to $1$. Then, we can reformulate Eqn.~\ref{eqn-san-sampling} to:
\begin{equation}
    \begin{split}
        &\mathbf{E}_{\mathcal{S}}=\mathbf{E}\odot(\mathbf{1}_{|E|}-\lfloor\frac{1}{1+e^{-(\log\mathbf{P}+g)/\tau}}+\frac{1}{2}\rceil), \\ 
        &\text{where} \ \mathbf{P}\in\mathcal{S}_1=\{\mathbf{P}_{e}|\sum_{e}\mathbf{P}_{e}\leq \epsilon, \mathbf{P}_{e}\in[0,1] \ \forall e\in\{1,..,E\}\}. 
    \end{split}
    \label{eqn-san-Gumbel-sampling}
\end{equation}  

\subsubsection{Projection Gradient Descent}
Formally, to optimize the parameters $\mathbf{P}$ under the constraint $\mathcal{S}_{1}$ defined in Eqn.~\ref{eqn-san-Gumbel-sampling}, we deploy the projection gradient descent to optimize the relaxed constrained optimization problem introduced in Eqn.~\ref{eqn-san-Gumbel-sampling}:
\begin{equation}
    \begin{split}
        \mathbf{P}^{(t)}=\text{Proj}_{\mathcal{S}_1}(\mathbf{P}^{(t-1)}-\alpha\nabla_{\mathbf{P}^{(t-1)}}\mathcal{L}(t_1(\mathcal{G}), t_2(\mathcal{G}), \theta))
    \end{split}
    \label{eqn-PGD}
\end{equation}
at the $t$-th iteration, where $\alpha$ is the learning rate of the projection gradient descent, $\nabla_{\mathbf{P}}\mathcal{L}(t_1(\mathcal{G}),t_2(\mathcal{G}),\theta)$ denotes the gradient of the loss defined in Eqn.~\ref{eqn-san-obj-2}, $\text{Proj}_{\mathcal{S}_1}$ is the projection operator to project the updated parameters to satisfy the constraint. Referring to \cite{TopologyAttack}, the projection operator $\text{Proj}_{\mathcal{S}_1}$ has the closed-form solutions:
\begin{equation*}
    \begin{split}
        \text{Proj}_{\mathcal{S}_1}(\mathbf{P})=\begin{cases}
                                            \text{Proj}_{[0,1]}[\mathbf{P}-\mu\mathbf{1}_{E}],&\mbox{if } \mu>0\mbox{ and }
                                            \\ &\sum_{e}\text{Proj}_{[0,1]}[\mathbf{P}-\mu\mathbf{1}_{E}]=\epsilon, \\ \\ 
                                            \text{Proj}_{[0,1]}[\mathbf{P}],&\mbox{if } \sum_{e}\text{Proj}_{[0,1]}[\mathbf{P}]\leq\epsilon, 
                                        \end{cases}
    \end{split}
\end{equation*}
where $\text{Proj}_{[0,1]}[\mathbf{P}]$ clips the parameters vector $\mathbf{P}$ into the range $[0,1]$. The bisection method~\cite{bisection} is used over $\mu\in\{\min(\mathbf{P}-\mathbf{1}_{E}), \max(\mathbf{P})\}$ to find the solution to $\sum_{e}\text{Proj}_{[0,1]}[\mathbf{P}-\mu\mathbf{1}_{E}]=\epsilon$ with the convergence rate $\log_{2}[(\max(\mathbf{P})-\min(\mathbf{P}-\mathbf{1}_{E}))/\xi]$ with $\xi$-error tolerance~\cite{liu2015sparsity}. 

\subsection{Unsupervised Hyperparameter Tuning}
One issue that remains unsolved is the tuning of the hyperparameter $\eta$. While some previous works~\cite{SPAN, ARIEL, GCA} use node labels for tuning hyperparameters, 
%({\color{red} I am thinking if we should give such negative examples}), 
we consider a strict unsupervised setting where node labels are not available during training and are only used in the downstream node classification task for testing. We thus propose a fully unsupervised method to tune the parameter $\eta$.

In Sec.~\ref{sec-preliminaries-NormalizedCut},  we introduced the normalized cut loss in Eqn.~\ref{eqn-normalized-cut}, which counts the number of inter-cluster links within a given graph. The loss value depends on the quality of the clustering result determined by cluster assignment matrix $\mathbf{C}$. We propose to use this normalized cut loss as the supervision for searching the parameter $\eta$. The intuition is that if the generated node embeddings have high quality, the clustering result should be accurate, leading to a low loss value. In fact, several previous works~\cite{GAP, maskGVAE} have utilized this loss as supervision in different ways.
%For instance, spectrum-clustering-based methods~\cite{shi2000normalized, ng2001spectral, von2007tutorial} generate high-quality cluster assignment matrices by computing node embeddings via the eigenvalue decomposition of the graph Laplacian and applying K-means clustering~\cite{Jin2010} to the embeddings.
%{\color{red} The logic of this example is confusing. You want to say that cut loss is used as a supervision, but cut loss is not even mentioned in this example.}
%\cite{GAP, maskGVAE} used the normalized cut loss to supervise the node embeddings generated by a two-layered GNN encoder. 

%These methods all rely on the node embeddings corresponding to a low value of the normalized cut loss to achieve good graph partition performance. Therefore, it is natural to use the normalized cut loss as a metric to evaluate the quality of the node embeddings generated by graph representation learning. In other words, high-quality node embeddings should preserve the node labels and inherently incorporate the semantic information of the original graph.


%It has been mentioned in Sec.~\ref{sec-preliminaries-NormalizedCut} the normalized cut loss in Eqn.~\ref{eqn-normalized-cut} counts on the number of inter-cluster links inside a given graph. The value of $\mathcal{L}_{u}$ in Eqn.~\ref{eqn-normalized-cut} highly depends on the quality of the cluster assignment matrix $\mathbf{C}$. 
%For example, spectrum-clustering-based methods \cite{shi2000normalized, ng2001spectral, von2007tutorial} produce high-quality cluster assignment matrix by generating the node embeddings via the eigenvalue decomposition of the graph Laplacian and apply Kmeans clustering~\cite{Jin2010} to the node embeddings. GNN-based methods~\cite{GAP, maskGVAE} utilizes the normalized cut loss to supervise the node embeddings generated by the two-layered GNN encoder. The common part of these methods is that they all rely on the node embeddings corresponding with the low value of the normalized cut loss to get good graph partition performance. From this perspective, it is natural to utilize the normalized cut loss as a metric to quantify the quality of the node embeddings generated by graph representation learning. That is, node embeddings with good quality should be label-preserving and inherently incorporate with semantic information of the original graph.    

%In this paper, our work follows the line of the GNN-based methods to deploy the hyperparameter tuning procedure. 
Following this intuition, we use the normalized cut loss as a metric to quantify the quality of node embedding and tune $\eta$ by monitoring this loss during training. 
Specifically, during the $t$-th iteration, we obtain the sanitized adjacency matrix $\mathbf{A}_{\mathcal{S}}^{(t)}$ (corresponding with $\mathbf{E}_{\mathcal{S}}^{(t)}$ in Eqn.~\eqref{eqn-san-Gumbel-sampling}). 
Then we feed it into a two-layered GNN encoder to the sanitized node embeddings $\mathbf{H}_{\mathcal{S}}^{(t)}=f_{\theta}(\mathbf{A}_{\mathcal{S}}^{(t)},\mathbf{X})$ from Eqn.~\ref{eqn-GNN-encoder} and get the pseudo normalized cut loss:
\begin{equation*}
    \begin{split}
        \mathcal{L}_{pnc}^{(t)}=\Tr((\sigma(\mathbf{H}_{1}^{(t)})^{\top}\mathbf{L}^{(t)}\sigma(\mathbf{H}_{1}^{(t)}))\oslash(\sigma(\mathbf{H}_{1}^{(t)})^{\top}\mathbf{D}^{(t)}\sigma(\mathbf{H}_{1}^{(t)})),
    \end{split}
    \label{eqn-pseudo-normalized-cut}
\end{equation*}
where $\sigma(\cdot)$ is the sigmoid function. $\mathcal{L}_{pnc}$ is time-efficient since it does not require additional models or parameters to be trained. We use the early-stopping trick~\cite{EarlyStop} to effectively tune the hyperparameter $\eta$. That is, 
%The trace plot of the pseudo normalized cut loss $\mathcal{L}_{pnc}$ and the node classification accuracy during training for Cora dataset is depicted in Fig.\textcolor{red}{add empirical study and analysis here}. 
\begin{wrapfigure}{r}{0.24\textwidth}
    %\centering
    % Figure removed
    \caption{$\mathcal{L}_{pnc}^{t^{*}}$ v.s. Accuracy.}
    \label{fig-pnc-acc}
\end{wrapfigure}
we record the value of $\mathcal{L}_{pnc}^{(t)}$ for each iteration during training and select the model with the minimum $\mathcal{L}_{pnc}$ as our final result for evaluation. We denote the best metric as $\mathcal{L}_{pnc}^{t^{*}}$. Fig.~\ref{fig-pnc-acc} presents the values of $\mathcal{L}_{pnc}^{t^{*}}$ and node classification accuracy with varying $\eta$. The results indicate that in most cases $\mathcal{L}_{pnc}^{t^{*}}$ is negatively related to the node classification accuracy, especially when the value of $\mathcal{L}_{pnc}^{t^{*}}$ falls into the neighborhood of the minimum value, i.e., $\eta\in[0.1, 3]$ (the Pearson correlation coefficients $r(\mathcal{L}_{pnc}^{t^{*}}, accuracy)=-0.89$). This phenomenon demonstrates that it is reasonable to tune $\eta$ via searching for the best $\mathcal{L}_{pnc}^{t^{*}}$, since the GCL model that achieves the minimum $\mathcal{L}_{pnc}^{t^{*}}$ is likely to get the best accuracy. 
The algorithm of our work is in Alg.~\ref{alg-RGCL}.  
\begin{algorithm}[h]
	\caption{\textsf{GCHS}}
	\label{alg-RGCL}
	\textbf{Input}: Poisoned graph $\mathcal{G}=\{\mathbf{E},\mathbf{X}\}$, $\eta$.\\
        \textbf{Parameters}: Sanitation probability $\mathbf{P}$, GNN encoder $f_{\theta}(\cdot)$ with weight matrices $\theta=\{\mathbf{W}^{(1)}, \mathbf{W}^{(2)}\}$.\\
	\textbf{Output}: Sanitized node embeddings matrix $\mathbf{H}_1$.\\
	\begin{algorithmic}[1] %[1] enables line numbers
		\STATE Let $t=0$, initialize parameters $\mathbf{P}=\mathbf{P}^{0}$, $\theta=\theta^{0}$.
		\WHILE{$t\leq T$}
                \STATE Sample $g\sim Gumbel(0,1)$.
                \STATE Sample sanitation view $\mathcal{G}_{1}^{t}$ from $\mathbf{E}_{\mathcal{S}}^{t}=\mathbf{E}\odot(\mathbf{1}_{E}-\lfloor\frac{1}{1+e^{-(\log\mathbf{P}^{t}+g)/\tau}}+\frac{1}{2}\rceil)$.
                \STATE Sample another view $\mathcal{G}_{2}^{t}$ by randomly dropping links. 
                \STATE Compute graph Laplacian $\mathbf{L}^{t}=\mathbf{I}-(\tilde{\mathbf{D}}^{t})^{-\frac{1}{2}}\tilde{\mathbf{A}}^{t}(\tilde{\mathbf{D}}^{t})^{-\frac{1}{2}}$.
                \STATE Compute loss $\mathcal{L}=\mathcal{L}_{info}(\mathcal{G}_{1}^{t}, \mathcal{G}_{2}^{t})+\eta\Tr(\mathbf{X}^{\top}\mathbf{L}^{t}\mathbf{X})$.
                \STATE Projection gradient descent and update $\mathbf{P}^{t}$ from Eqn.~\ref{eqn-PGD}.
                \STATE Gradient descent and update $\theta^{t}$ based on $\mathcal{L}$.
                \STATE Compute pseudo normalized cut loss $\mathcal{L}_{pnc}^{t}$ from Eqn.~\ref{eqn-pseudo-normalized-cut}. 
            \ENDWHILE
            \STATE Get the best iteration $t^{*}$ with the minimum value $\mathcal{L}_{pnc}^{t^{*}}$.
            \RETURN Sanitized node embeddings $\mathbf{H}_1=f_{\theta^{t^{*}}}(\mathbf{A}^{t^{*}},\mathbf{X})$.
	\end{algorithmic}
\end{algorithm} 












































