\section{Appendix}

\subsection{Proof of Proposition 1} \label{proof: proposition 1}

The conditional entropy $H(A|Y)$ can be formulated as:

\begin{equation}
    H(A \mid Y)=-\sum_{y \in Y} P(y) \sum_{a \in A} P(a \mid y) \log P(a \mid y)
\end{equation}

Since the strong relevance between $A$ and $Y$, conditional probabilities $P(a \mid y)$ tend to or close to 0 or 1, thus:

\begin{equation}
    \begin{aligned}
        & \sum_{a \in A} P(a \mid y) \log P(a \mid y) \rightarrow 0 \\
        & H(A \mid Y) \rightarrow 0.
    \end{aligned}
\end{equation}

The proof of $H(Y \mid A) \rightarrow 0$ is same as the above proof.

\subsection{Proof of Proposition 2} \label{proof: proposition 2}

The cross-entropy loss can be represented as:

\begin{equation}\label{LCE}
\begin{split}
    \mathcal{L}_{CE} &= \min -y \log \hat{y}-(1-y) \log (1-\widehat{y}) \\
    &= \min_{f} -y \log f(x,a) - (1-y) \log (1-f(x,a)),
\end{split}
\end{equation}

A well-trained model $f^{*}$ can minimize Eq.~\ref{LCE} by making $f^{*}(x,a)$ extremely close to 0 or 1. Therefore, the conditional entropy tends to be 0 according to the definition of conditional entropy shown in the following equation.

\begin{equation}
\begin{split}
    H(Y|f^{*}(X,A)) & = -\sum_{f^{*}(x,a) \in f^{*}(X,A)} P(f^{*}(x,a)) \\
    & \sum_{y \in Y} P(y \mid f^{*}(x,a)) \log P(y \mid f^{*}(x,a)).
\end{split}
\end{equation}

\subsection{Proof of Lemma 1} \label{proof: lemma 1}

The attack methods perturb neighbors of each node (i.e. removing/adding edges) but maintian the node attributes $X$ and node labels $Y$ to enlarge the loss $\mathcal{L}_{CE}$. The loss after attack is:

\begin{equation}\label{LCE'}
    \mathcal{L}_{CE}^{'} = \min_{f} -y \log f(x,a^{'}) - (1-y) \log (1-f(x,a^{'})),
\end{equation}

where $a^{'} \in A^{'}$. The attack enlarges loss, therefore:

\begin{equation}
\begin{split}
    \mathcal{L}_{CE}^{'} & \geq \mathcal{L}_{CE} \\
    \log f(x,a^{'}) \leq & \log f(x,a), \forall a \in A, \forall a^{'} \in A^{'}.
\end{split}
\end{equation}

Therefore, conditional entropy fulfills:

\begin{equation}\label{HYF*}
    H(Y|f^{*}(X,A)) < H(Y|f^{*}(X,A^{'})).
\end{equation}

Since $f^{*}$ is a well-trained model that learns label-related information from the distributions of $X$ and $Y$. So, the reason causing Eq.~\ref{HYF*} is the label can not be determined by distributions. In other words, the uncertainty between the distributions and labels is larger. Thus we have:

\begin{equation}
    H(Y|X,A) < H(Y|X,A^{'}).
\end{equation}

\subsection{Proof of Lemma 2} \label{proof: lemma 2}

According to the definition of information entropy, the entropy $H(X)$ of node attribute set $X$ can be represented by conditional entropy and mutual information:

\begin{equation} \label{HX}
    \begin{aligned}
        H(X) & =H(X \mid A^{'})+I(X ; A^{'}) \\
            & =H(X \mid A^{'}) + I(X ; A^{'} ; Y) + I(X ; A^{'} \mid Y) \\
    \end{aligned}
\end{equation}

According to Proposition.~1:

\begin{equation}
    \begin{aligned}
        \because \quad & H(Y|A^{'}) = H(Y|X,A^{'}) + I(X;Y|A^{'}) \\
                    & H(Y|A^{'}) \rightarrow 0 \\
        \therefore \quad & I(X;Y|A^{'}) \rightarrow 0 \\ 
                        & H(Y|X,A^{'}) \rightarrow 0.
    \end{aligned}
\end{equation}

Therefore, Eq.~\ref{HX} can be rewritten as:

\begin{equation} \label{Appro HX}
     H(X) \rightarrow H(X|A^{'}) + I(X ; A^{'} ; Y)
\end{equation}

The above equation means the information entropy $H(X)$ tends equal to the sum of conditional entropy $H(X|A^{'})$ and mutual information $I(X ; A^{'} ; Y)$ when Proposition.~1 holds.

Due to the $H(X)$ being solely dependent on the input, it is thus fixed. Therefore, minimizing $H(X|A^{'})$ is maximizing $I(X ; A^{'} ; Y)$.

\subsection{Proof of Lemma 3} \label{proof: lemma 3}

According to the properties of information entropy, the entropy $H(Y)$ of label set $Y$ hodls:

\begin{equation} \label{HY}
    \begin{aligned}
        H(Y) & = I(Y;A^{'}) + H(Y|A^{'})\\
            & = I(Y;A^{'}) + H(Y|X,A^{'}) + I(X;Y|A^{'}).
    \end{aligned}
\end{equation}

Additionally, there are:

\begin{equation}
    \begin{aligned}
        & \because \quad I(X;Y|A^{'}) \rightarrow 0 \\
        & \therefore \quad H(Y) \rightarrow I(Y;A^{'}) + H(Y|X,A^{'}).
    \end{aligned}
\end{equation}

$H(Y)$ is fixed since it is only dependent on the input labels. Therefore, maximizing $I(Y;A^{'})$ is minimizing $H(Y|X,A^{'})$.

\subsection{Proof of Theorem 1}  \label{proof: theorem 1}

Based on the properties of mutual information, the three variables mutual information $I(X;A^{'};Y)$ is less than two variables mutual information $I(Y;A^{'})$:

$$
    I(X;A^{'};Y) \leq I(Y;A^{'})
$$

Therefore, maximizing $I(X;A^{'};Y)$ is equal to maximizing $I(Y;A^{'})$. Besides, Lemma.~1 indicates minimizing $H(X|A^{'})$ is actually maximizing $I(X;A^{'};Y)$, thus minimizing $H(X|A^{'})$ is equal to maximizing $I(Y;A^{'})$. Furthermore, Lemma.~2 implies maximizing $I(Y;A^{'})$ is equal to minimizing $H(Y|X,A^{'})$, so minimizing $H(X|A^{'})$ is equal to minimizing $H(Y|X,A^{'})$.