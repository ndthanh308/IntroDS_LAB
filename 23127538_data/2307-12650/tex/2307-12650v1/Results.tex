\section{Results of RL active flow control}\label{sec:Results}

%In this section, RL control results with other forms of reward functions (introduced in \S\ref{subsec:Reward}) are provided and discussed.

\subsection{Convergence of learning}

We performed RL with the maximum entropy TQC algorithm to discover control policies for the three  cases shown in figure \ref{fig:Case_Demo}, which  maximise the net-power-saving reward function given by \req{eq: PowerR}. During the learning stage, each  episode (1 DNS simulation) corresponds to $200$ non-dimensional time units.  To accelerate learning, $65$ environments run in parallel.


Figure \ref{fig:Learning_Curve} shows the learning curves of the three cases.  Table \ref{tab:LearningConvergence} shows the number of episodes needed for convergence and relevant parameters for each case.
%In PM environments, the learning behaviour of a static controller is compared with a dynamic controller design. 
It can be observed from the curve of episode reward that the RL agent is updated after every 65 episodes, i.e. $1$ iteration, where the episode reward is defined as 
\begin{equation}
R_{ep} = \sum_{k=1}^{N_k} r_{k},
\label{eq:Epi_R}
\end{equation}
where $k$ denotes the $k^{th}$ RL step in one episode and $N_k$ is the total number of samples in one episode.
The root mean square (RMS) value of the drag coefficient, $C_D^{RMS}$, at the asymptotic regime of control is also shown to demonstrate convergence, defined as 
$C_D^{RMS} = \sqrt { (\mathcal{D}(\langle C_D\rangle_{env}))^2 }$,
where the operator $\mathcal{D}$ detrends the signal with a $9^{th}$-order polynomial and removes the transient part, and $\langle ~ \rangle_{env}$ denotes the average value of parallel environments in a single iteration. %The convergence criteria are chosen according to both $R_{ep}$ and $C_D^{RMS}$. If $R_{ep}$ stops increasing and the standard deviation reduces below a threshold of $ C_D^{RMS} <3\times10^{-3}$, the learning is considered to be converged. 

% Figure environment removed

\begin{table}
  \begin{center}
\def~{\hphantom{0}}
  \begin{tabular}{lcccccc}
    
      Environment  & Algorithm  &  $N_{c}$ & $R_{c}$ & (Layers, Neurons) & $N_{fs}$ & Number of Inputs \\ 
       FM-Static   & TQC & $325$ & $37.72$ & (3,512) & $0$ & $64p_t+2a_{t-1}$\\
       PM-Static   & TQC & $1235$ & $21.87$ & (3,512) & $0$ & $64p_t+2a_{t-1}$\\
       PM-Dynamic  & TQC & $715$ & $34.35$ & (3,512) & $27$ & $N_{fs} (64p_t+2a_{t-1})$\\
  \end{tabular}
  \caption{Number of episodes $N_{c}$ required for RL convergence in different environments. The episode reward $R_{c}$ at the convergence point, the configuration of NN and the dimension of inputs are presented for each case. $N_{fs}$ is the finite-horizon length of past actions-measurements.}
  \label{tab:LearningConvergence}
  \end{center}
\end{table}

In figure \ref{fig:Learning_Curve}, it can be noticed that in the FM environment, RL converges around $325$ episodes ($5$ iterations) to an optimal policy using a static controller. As will be shown in \S\ref{Result_drag_reduction}, this policy is (globally) optimal since the vortex shedding is fully attenuated and the jets converge to  zero mass flow actuation, thus recovering the unstable base flow and the minimum drag state.  However, with the same static controller in a PM environment (POMDP), the RL agent fails to discover the optimal solution, requiring around $1235$ episodes for convergence but only obtaining a relatively low episode reward.
Introducing a dynamic controller in the PM environment, the RL agent convergences to a near-optimal solution in 735 episodes. The dynamic controller trained by RL achieves a higher episode reward (34.35) than the static controller in the PM case (21.87), which is close the optimal one in the FM case (37.72). The learning curves illustrate that using a finite horizon of past actions-measurements to train a dynamic controller facilitates learning in terms of the speed of convergence and the accumulated reward. 


\subsection{Drag reduction with dynamic RL controllers} \label{Result_drag_reduction}

% Figure environment removed

The trained controllers for the cases shown in figure \ref{fig:Case_Demo} are evaluated to obtain the results shown in figure \ref{fig:TQC_FMPM}.  
The evaluation of control starts at $t=0$ with the same initial condition, i.e. steady vortex shedding and average drag coefficient $\langle C_D\rangle_T \approx 1.45$ (baseline case). Consistent with the learning curves, the discrepancy of control performance in the three cases can be observed both from the drag coefficient $C_D$ after control and the actuation $Q_1$.

\begin{itemize}

\item {\bf FM-Static:} With a static controller trained in an full-measurement environment, a drag reduction of $102\%$ is obtained with respect to the base flow (steady unstable fixed point; maximum drag reduction). This indicates that an RL controller informed with full-state information can entirely stabilize the vortex shedding and cancel the unsteady part of the pressure drag.

\item {\bf PM-Static:} A static/memoryless controller in a partial-measurement environment leads to a performance degradation and a drag reduction of $47.57\%$ in the asymptotic control stage, i.e. after $t=80$, compared to the performance of ``FM-Static''. This performance loss can also be observed from the control actuation curve, as $Q_1$ oscillates with a relatively large fluctuation in ``PM-Static'' while it stays about zero in the ``FM-Static'' case. 
The discrepancy between FM and PM environments using a static controller reveals the challenge of designing a controller with a POMDP environment. The RL agent cannot fully identify the dominant dynamics with only partial measurements on the base surface of the bluff body, resulting in a sub-optimal control behaviour.

\item{\bf PM-Dynamic:} With a dynamic controller (NARX model specified in \S\ref{subsec:PM_Dynamic}) in a partial-measurement environment, the vortex shedding is stabilised and the dynamic controller achieves $100.78\%$ of the maximum drag reduction after time $t=60$. Although there are minor fluctuations in the actuation $Q_1$, the energy spent in the synthetic jets is relatively low compared to the ``PM-Static'' case. Thus, a dynamic controller in PM environments can achieve near-optimal drag reduction, even if the RL agent only collects information from pressure sensors on the body rear surface. The improvement in control indicates that the POMDP due to the PM condition of the sensors can be reduced to an approximate MDP by training a dynamic controller with a finite horizon of past actions-measurements. Furthermore, high frequency action oscillations, which can be amplified with static controllers, are attenuated in the case of dynamic control. These encouraging and unexpected results support the effectiveness and robustness of model-free RL control in practical flow control applications, in which sensors can only be placed on a solid surface/wall.

\end{itemize}





% \iffalse
% s
% \textbf{clarify the conditions of inputs, like including actions and how many frame stacks}

% The effect of partial measurements is also illustrated by figure \ref{fig:TQC_FMPM}, which shows the control performance using the same RL agent and algorithm with both full and partial measurements. For the case with full measurements, the agent can learn a near-optimal control strategy that drives the flowfield towards base flow without noticeable vortex shedding.  However, using the same RL controller in a partial measurement environment, due to missing information from the flowfield, a significant performance degradation of $XX\%$ drag reduction loss (compared to FM) can be noticed from the control results. 
% \fi

% Figure environment removed

In figure \ref{fig:Contour}, snapshots of the velocity magnitude field are presented for ``Baseline'' without control, ``PM-Static'', ``PM-Dynamic'' and ``FM-Static'' control cases. Snapshots are captured at $t=100$ in the asymptotic regime of control. A vortex-shedding structure of different strengths can be observed in the wake of all three controlled cases. In ``PM-Static'', the recirculation area is lengthened compared to the baseline flow, corresponding to base pressure recovery and pressure drag reduction. A longer recirculation area can be noticed in ``PM-Dynamic'', due to the enhanced attenuation of vortex shedding and pressure drag reduction. The dynamic controller in the PM case renders a $326.22\%$ increase of recirculation area with respect to the baseline flow, while only a $116.78\%$ increase is achieved by a static controller. The ``FM-Static'' case has the longest recirculation area, and the vortex shedding is almost fully stabilized, which is consistent with the most drag reduction as figure \ref{fig:TQC_FMPM}.

% Figure environment removed


 
Figure \ref{fig:Obs} presents first- and second-order base pressure statistics for the baseline case without control and PM cases with control. In figure \ref{fig:Obs}(a), the time-averaged value of base pressure, $\overline{p}$ demonstrates the base pressure recovery after control is applied. Due to flow separation and recirculation, the time-averaged base pressure is higher at the middle of the bluff body base, which is retained with control. The base pressure increase is directly linked to pressure drag reduction, which quantifies the control performance of both static and dynamic controllers. Up to $49.56\%$ of pressure increase at the centre of the base is obtained in the ``PM-Dynamic'' case while only $21.15\%$ can be achieved by a static controller. In figure \ref{fig:Obs}(b), the base pressure RMS is shown. For the baseline flow, strong vortex-induced fluctuations of the base pressure can be noticed around the top and bottom base of the bluff body. In the ``PM-Static'' case, the RL controller suppresses partially the vortex shedding, leading to a sub-optimal reduction of the pressure fluctuation. The sensors close to the top and bottom corners are also affected by the synthetic jets, which change the RMS trend for the two top and bottom measurements. In the ``PM-Dynamic'' case,  the pressure fluctuations are nearly zero for all the measurements on the base, highlighting the success of vortex shedding suppression by a dynamic RL controller in a PM environment.

\subsection{Horizon of the finite-history sufficient statistic}\label{subsec:Nfs}

A parametric study on the horizon of the finite history in NARX (equation \req{eq:NARX}), i.e. the number of frames stacked $N_{fs}$, is presented in this section. Since the NARX model uses an finite horizon of past actions-measurements in  \req{eq:Sufficient_statistic}, the horizon of the finite history affects the convergence of the approximation, as discussed in \cite{yu_near_2008}. And this approximation affects the optimization during the learning of RL because it determines whether the RL agent can observe sufficient information to converge to an optimal policy. %This study attempts to find a reference for finding a horizon that optimizes the finite horizon of past actions-measurements, rendering a potential connection between the horizon and the dynamics in this specific flow control application. 

Since vortex shedding is the dominant instability to be controlled, the choice of $N_{fs}$ should intuitively link to the timescale of the vortex shedding period. The ``frames'' of observations are obtained every RL step ($0.5$ time units), while the vortex shedding period is $t_{vs}\approx6.85$ time units. Thus, $N_{fs}$ is rounded to integer values for different numbers of vortex shedding periods, as shown in table \ref{tab:Frame_Stack}.


% Figure environment removed
\begin{table}
  \setlength{\tabcolsep}{12pt}
  \begin{center}
\def~{\hphantom{0}}
  \begin{tabular}{ccc}
      Number of  & Time units &  History length \\
      VS periods &              &  ($N_{fs}$)         \\ [3pt]
      \hline
       0.5   & 3.43 & 7 \\
       1   & 6.85 & 14 \\
       2  & 13.70 & 27 \\
       3 & 20.55 & 41\\
       4 & 27.40 & 55\\
       5 & 34.25 & 68\\
  \end{tabular}
  \caption{Correspondence between number of vortex shedding (VS) periods and frame stack (history) length in samples $N_{fs}$. The RL control step size is $t_a =0.5$ and $N_{fs}$ is rounded to an integer.}
  \label{tab:Frame_Stack}
  \end{center}
\end{table}

The results of time-averaged drag coefficients $\langle C_{D}\rangle$ after control and the average episode rewards $\langle R_{ep}\rangle$ in the final stage of training are presented in figure \ref{fig:Frame_Stack}. As $N_{fs}$ increases from 0 to the optimal values of 27, the performance of RL control improves, resulting in a lower $\langle C_{D}\rangle$ and a higher $\langle R_{ep}\rangle$. $N_{fs}=2$ is specially examined because the latent dimension of vortex shedding limit cycle is 2. However, the control performance with $N_{fs}=2$ is marginally improved to the one with $N_{fs}=0$, i.e. a static controller. This result indicates that the horizon consistent with the vortex shedding dimension is not long enough for the finite horizon of past actions-measurements. The optimal history length to achieve stabilisation of the vortex shedding with sensors only on the base of the body is 27 samples, which are equivalent to 13.5 convective time units or $\sim 2$ vortex shedding periods. 

With $N_{fs}=41$ and $N_{fs}=55$, the drag reduction and episode rewards drop slightly compared to $N_{fs}=27$. The decline in performance is non-negligible as $N_{fs}$ increases further to 68. This decline shows that excessive inputs to the neural networks, as table \ref{tab:LearningConvergence}, may impede training because more parameters need to be tuned or larger neural networks need to be trained. 



\subsection{Observation sequence with past actions}\label{subsec:past_actions}

Past actions (exogenous terms in NARX) facilitate reducing a POMDP to an MDP problem, as discussed in \S\ref{subsec:PM_Dynamic}. In the near-optimal control of a PM environment using a dynamic controller with inputs $\left( o_t, o_{t-1}, ..., o_{t-N_{fs}} \right)$, a sequence of observations $o_t = \left \{ p_t, a_{t-1}\right \}$ at step $t$ is constructed to include pressure measurements and actions. In the FM environment, due to the introduction of one-step delayed action due to the first-order-hold interpolation given by \req{eq:FOH_action}, the inclusion of the past action along with the current pressure measurement, meaning $o_t = \left \{ p_t, a_{t-1} \right \}$, is required even when the sensors are placed in the wake and cover the wavemaker region. 

Figure \ref{fig:ActionInObs} presents the control performance for the same environment with and without past actions included.
In the FM case, there is no apparent difference between RL control with $o_t = \left \{ p_t, a_{t-1} \right \}$ or $o_t = \left \{ p_t \right \}$, which indicates that the inclusion of the past action is negligible to the performance. This is the case when the RL sampling frequency is sufficiently faster than the timescale of the vortex shedding dynamics. 
In PM cases, if exogenous action terms are not included in the observations but only the finite history of pressure measurements is used, the RL control fails to converge to a near-optimal policy, with only $67.96\%$ drag reduction. With past actions included, the drag reduction of the same environment increases up to $100.78\%$. 

The above results show that in PM environments the sufficient statistic cannot be constructed only from the finite history of measurements. This can be explained as the missing state information needs to be replaced by both state-related measurements and control actions. With past actions, an OD-MDP is also reduced to an undelayed augmented MDP, thus improving the control performance.

% Figure environment removed

\subsection{Reward study}
\label{subsec:Rewards_Study}

In \S\ref{Result_drag_reduction}, a power-based reward function given by \req{eq: PowerR} has been implemented and stabilising controllers can be learned by RL, as shown. In this section, RL control results with other forms of reward functions (introduced in \S\ref{subsec:Reward}) are provided and discussed.

% Figure environment removed

The control performance of RL control with the different reward functions is evaluated based on the drag coefficient $C_D$ shown in figure \ref{fig:Reward_Study}. Static controllers are trained in FM environments, and dynamic controllers are trained in PM environments. In FM cases, control performance is not sensitive to the choice of reward function (power or force based).  
In PM cases, the discrepancies between RL-step time-averaged and instantaneous rewards can be observed in the asymptotic regime of control. The controllers with both rewards (power  or force) achieve nearly-optimal control performance, but there is some unsteadiness in the cases using instantaneous rewards due to slow statistical convergence of the rewards and limited correlation to the partial observations.

All four types of reward functions studied in this work  achieve nearly-optimal drag reduction around $100\%$. However, the energy-based reward (``PowerR'') offers an intuitive reward design, attributable to its physical properties and the dimensionally consistent addition of the constituent terms of the reward function. Further enhancing its practicality, since the power of the actuator can be directly measured, it avoids the necessity for hyperparameter tuning, as in the force-based reward. Additionally, the results show similar performance with both time-averaged between RL steps and instantaneous rewards, avoiding the necessity for faster sampling for the calculation of the rewards. This choice of reward function can be extended to various RL flow control problems and can be beneficial to experimental studies.



% There is a minor discrepancy between power-based rewards and force-based rewards during the transient phase of control, e.g., between $t=10$ and $t=30$. It takes a longer time for the RL controller with a power-based reward to stabilize the unsteadiness in the flow. The reason is that power-based optimization tries to find the most energy-efficient way, and smaller actuation is regarded as a better solution if possible.




\subsection{TQC vs SAC}\label{subsec:SACvsTQC}

% Figure environment removed

Control results with TQC and SAC are presented in figure \ref{fig:TQCvsSAC} in terms of $C_D$. TQC shows a more robust control performance. In the case of FM, SAC might demonstrate a slightly more stable transient behaviour attributed to the fact that the quantile regression process in TQC introduced complexity to the optimization process. Both controllers achieved an identical level of drag reduction in the FM case. 

However, in the context of the PM cases, it is observed that TQC outperforms SAC in drag reduction with both static and dynamic controllers. For static control, TQC achieved an average drag reduction of $58.5\%$, compared to the $48.7\%$ reduction achieved by SAC. The performance under dynamic control conditions is more compelling, where TQC fully reduced the drag, achieving $100.78\%$ of drag reduction, reverting it to a near-base-flow scenario. In contrast, SAC managed to achieve an average drag reduction of $96.6\%$.

The fundamental mechanism for updating Q-functions in RL involves selecting the maximum expected Q-functions among possible future actions. This process, however, can potentially lead to overestimation of certain Q-functions \citep{hasselt_double_2010}. In POMDP, this overestimation bias might be exacerbated due to the inherent uncertainty arising from the partial-state information. Therefore, the Q-learning-based algorithm, when applied to POMDPs, might be more prone to choosing these overestimated values, thereby affecting the overall learning and decision-making process.

As mentioned in \S\ref{subsec:SACTQC}, the core benefit of TQC under these conditions can be attributed to its advanced handling of the overestimation bias of rewards. By constructing a more accurate representation of possible returns, TQC provides a more accurate Q-function approximation than SAC. This process of modulating the probability distribution of the Q-function assists TQC in managing the uncertainties inherent in environments with only partial-state information. In this case, TQC can adapt more robustly to changes and uncertainties, leading to betetr performance in both static and dynamic control tasks.
% \\
% It is also worth noting that TQC's enhanced performance in handling uncertainties may come at the cost of added computational complexity, which may not be feasible in every application.

% \subsection{Behavior of NN Controllers}

% The policy neural network is trained as a controller to provide the expected mass flow rate of jets. The controller can be regarded as a MISO sub-system that takes measurements and actions as inputs and outputs a control signal. Analyses of frequency response and singular value decomposition (SVD) on trained NN controllers have been conducted to interpret the behavior of the controller as a sub-system.


% % Figure environment removed
