\begin{thebibliography}{66}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\def\au#1{#1} \def\ed#1{#1} \def\yr#1{#1}\def\at#1{#1}\def\jt#1{\textit{#1}}
  \def\bt#1{#1}\def\bvol#1{\textbf{#1}} \def\vol#1{#1} \def\pg#1{#1}
  \def\publ#1{#1}\def\arxiv#1{#1}\def\org#1{#1}\def\st#1{\textit{#1}}

\bibitem[Altman \& Nain(1992)]{altman1992closed}
{\sc \au{Altman, E.} \& \au{Nain, P.}} \yr{1992}  \at{Closed-loop control with
  delayed information}.  \jt{ACM sigmetrics performance evaluation review}
  \bvol{20}~(1),  \pg{193--204}.

\bibitem[Barros {\em et~al.\/}(2016)Barros, Borée, Noack, Spohn \&
  Ruiz]{barros_bluff_2016}
{\sc \au{Barros, D.}, \au{Borée, J.}, \au{Noack, B.~R.}, \au{Spohn, A.} \&
  \au{Ruiz, T.}} \yr{2016}  \at{Bluff body drag manipulation using pulsed jets
  and {Coanda} effect}.  \jt{Journal of Fluid Mechanics}  \bvol{805},
  \pg{422--459}.

\bibitem[Beaudoin {\em et~al.\/}(2006)Beaudoin, Cadot, Aider \&
  Wesfreid]{beaudoin_bluff-body_2006}
{\sc \au{Beaudoin, J.-F.}, \au{Cadot, O.}, \au{Aider, J.-L.} \& \au{Wesfreid,
  J.~E.}} \yr{2006}  \at{Bluff-body drag reduction by extremum-seeking
  control}.  \jt{Journal of fluids and structures}  \bvol{22}~(6-7),
  \pg{973--978}.

\bibitem[Bertsekas(2012)]{bertsekas_dynamic_2012}
{\sc \au{Bertsekas, D.}} \yr{2012} {\em Dynamic programming and optimal
  control: Volume I\/}.  \publ{Athena Scientific}.

\bibitem[Bertsekas(2019)]{bertsekas_reinforcement_2019}
{\sc \au{Bertsekas, D.}} \yr{2019} {\em Reinforcement learning and optimal
  control\/}.  \publ{Athena Scientific}.

\bibitem[Brackston {\em et~al.\/}(2018)Brackston, Wynn \&
  Morrison]{brackston_modelling_2018}
{\sc \au{Brackston, R.D.}, \au{Wynn, A.} \& \au{Morrison, J.F.}} \yr{2018}
  \at{Modelling and feedback control of vortex shedding for drag reduction of a
  turbulent bluff body wake}.  \jt{International Journal of Heat and Fluid
  Flow}  \bvol{71},  \pg{127--136}.

\bibitem[Brackston {\em et~al.\/}(2016)Brackston, García de~la Cruz, Wynn,
  Rigas \& Morrison]{brackston_stochastic_2016}
{\sc \au{Brackston, R.~D.}, \au{García de~la Cruz, J.~M.}, \au{Wynn, A.},
  \au{Rigas, G.} \& \au{Morrison, J.~F.}} \yr{2016}  \at{Stochastic modelling
  and feedback control of bistability in a turbulent bluff body wake}.
  \jt{Journal of Fluid Mechanics}  \bvol{802},  \pg{726--749}.

\bibitem[Brunton \& Noack(2015)]{brunton2015closed}
{\sc \au{Brunton, S.~L.} \& \au{Noack, B.~R.}} \yr{2015}  \at{Closed-loop
  turbulence control: Progress and challenges}.  \jt{Applied Mechanics Reviews}
   \bvol{67}~(5),  \pg{050801}.

\bibitem[Bucci {\em et~al.\/}(2019)Bucci, Semeraro, Allauzen, Wisniewski,
  Cordier \& Mathelin]{bucci_control_2019}
{\sc \au{Bucci, M.A.}, \au{Semeraro, O.}, \au{Allauzen, A.}, \au{Wisniewski,
  G.}, \au{Cordier, L.} \& \au{Mathelin, L.}} \yr{2019}  \at{Control of chaotic
  systems by deep reinforcement learning}.  \jt{Proceedings of the Royal
  Society A}  \bvol{475}~(2231),  \pg{20190351}.

\bibitem[Cassandra(1998)]{cassandra_survey_1998}
{\sc \au{Cassandra, A.R.}} \yr{1998} {A survey of POMDP applications}.  \bt{In
  {\em Working notes of AAAI 1998 fall symposium on planning with partially
  observable Markov decision processes\/}}, ,  \vol{vol. 1724}.

\bibitem[Choi {\em et~al.\/}(2014)Choi, Lee \& Park]{choi2014aerodynamics}
{\sc \au{Choi, H.}, \au{Lee, J.} \& \au{Park, H.}} \yr{2014}  \at{Aerodynamics
  of heavy vehicles}.  \jt{Annual Review of Fluid Mechanics}  \bvol{46},
  \pg{441--468}.

\bibitem[Corke {\em et~al.\/}(2010)Corke, Enloe \&
  Wilkinson]{corke_dielectric_2010}
{\sc \au{Corke, T.~C.}, \au{Enloe, C.~L.} \& \au{Wilkinson, S.~P.}} \yr{2010}
  \at{Dielectric {Barrier} {Discharge} {Plasma} {Actuators} for {Flow}
  {Control}}.  \jt{Annual Review of Fluid Mechanics}  \bvol{42}~(1),
  \pg{505--529}.

\bibitem[Dabney {\em et~al.\/}(2018)Dabney, Rowland, Bellemare \&
  Munos]{dabney_distributional_2018}
{\sc \au{Dabney, W.}, \au{Rowland, M.}, \au{Bellemare, M.} \& \au{Munos, R.}}
  \yr{2018} Distributional reinforcement learning with quantile regression.
  \bt{In {\em Proceedings of the {AAAI} {Conference} on {Artificial}
  {Intelligence}\/}}, ,  \vol{vol.~32}.

\bibitem[Dahan {\em et~al.\/}(2012)Dahan, Morgans \&
  Lardeau]{dahan_feedback_2012}
{\sc \au{Dahan, J.~A.}, \au{Morgans, A.~S.} \& \au{Lardeau, S.}} \yr{2012}
  \at{Feedback control for form-drag reduction on a bluff body with a blunt
  trailing edge}.  \jt{Journal of Fluid Mechanics}  \bvol{704},  \pg{360--387}.

\bibitem[Dalla~Longa {\em et~al.\/}(2017)Dalla~Longa, Morgans \&
  Dahan]{dalla2017reducing}
{\sc \au{Dalla~Longa, L.}, \au{Morgans, A.~S.} \& \au{Dahan, J.~A.}} \yr{2017}
  \at{Reducing the pressure drag of a d-shaped bluff body using linear feedback
  control}.  \jt{Theoretical and Computational Fluid Dynamics}  \bvol{31},
  \pg{567--577}.

\bibitem[Duan {\em et~al.\/}(2016)Duan, Chen, Houthooft, Schulman \&
  Abbeel]{duan_benchmarking_2016}
{\sc \au{Duan, Y.}, \au{Chen, X.}, \au{Houthooft, R.}, \au{Schulman, J.} \&
  \au{Abbeel, P.}} \yr{2016} Benchmarking deep reinforcement learning for
  continuous control.  \bt{In {\em International conference on machine
  learning\/}},  \pg{pp. 1329--1338}.

\bibitem[Fujimoto {\em et~al.\/}(2018)Fujimoto, Hoof \&
  Meger]{fujimoto_addressing_2018}
{\sc \au{Fujimoto, S.}, \au{Hoof, H.} \& \au{Meger, D.}} \yr{2018} Addressing
  function approximation error in actor-critic methods.  \bt{In {\em
  International conference on machine learning\/}},  \pg{pp. 1587--1596}.

\bibitem[Garnier {\em et~al.\/}(2021)Garnier, Viquerat, Rabault, Larcher,
  Kuhnle \& Hachem]{garnier_review_2021}
{\sc \au{Garnier, P.}, \au{Viquerat, J.}, \au{Rabault, J.}, \au{Larcher, A.},
  \au{Kuhnle, A.} \& \au{Hachem, E.}} \yr{2021}  \at{A review on deep
  reinforcement learning for fluid mechanics}.  \jt{Computers \& Fluids}
  \bvol{225},  \pg{104973}.

\bibitem[Gerhard {\em et~al.\/}(2003)Gerhard, Pastoor, King, Noack, Dillmann,
  Morzynski \& Tadmor]{gerhard_model-based_2003}
{\sc \au{Gerhard, J.}, \au{Pastoor, M.}, \au{King, R.}, \au{Noack, B.~R.},
  \au{Dillmann, A.}, \au{Morzynski, M.} \& \au{Tadmor, G.}} \yr{2003}
  Model-based control of vortex shedding using low-dimensional {Galerkin}
  models.  \bt{In {\em 33rd {AIAA} fluid dynamics conference\/}},  \pg{p.
  4262}.

\bibitem[Glezer \& Amitay(2002)]{glezer_synthetic_2002}
{\sc \au{Glezer, A.} \& \au{Amitay, M.}} \yr{2002}  \at{Synthetic {Jets}}.
  \jt{Annual Review of Fluid Mechanics}  \bvol{34}~(1),  \pg{503--529}.

\bibitem[Haarnoja {\em et~al.\/}(2017)Haarnoja, Tang, Abbeel \&
  Levine]{haarnoja_reinforcement_2017}
{\sc \au{Haarnoja, T.}, \au{Tang, H.}, \au{Abbeel, P.} \& \au{Levine, S.}}
  \yr{2017} Reinforcement learning with deep energy-based policies.  \bt{In
  {\em International conference on machine learning\/}},  \pg{pp. 1352--1361}.

\bibitem[Haarnoja {\em et~al.\/}(2018{\natexlab{{\em a\/}}})Haarnoja, Zhou,
  Abbeel \& Levine]{haarnoja_soft_2018-1}
{\sc \au{Haarnoja, T.}, \au{Zhou, A.}, \au{Abbeel, P.} \& \au{Levine, S.}}
  \yr{2018{\natexlab{{\em a\/}}}} Soft actor-critic: {Off}-policy maximum
  entropy deep reinforcement learning with a stochastic actor.  \bt{In {\em
  International conference on machine learning\/}},  \pg{pp. 1861--1870}.

\bibitem[Haarnoja {\em et~al.\/}(2018{\natexlab{{\em b\/}}})Haarnoja, Zhou,
  Hartikainen, Tucker, Ha, Tan, Kumar, Zhu, Gupta \&
  Abbeel]{haarnoja_soft_2018}
{\sc \au{Haarnoja, T.}, \au{Zhou, A.}, \au{Hartikainen, K.}, \au{Tucker, G.},
  \au{Ha, S.}, \au{Tan, J.}, \au{Kumar, V.}, \au{Zhu, H.}, \au{Gupta, A.} \&
  \au{Abbeel, P.}} \yr{2018{\natexlab{{\em b\/}}}}  \at{Soft actor-critic
  algorithms and applications}.  \jt{arXiv preprint arXiv:1812.05905} .

\bibitem[Hasselt(2010)]{hasselt_double_2010}
{\sc \au{Hasselt, Hado}} \yr{2010}  \at{Double {Q}-learning}.  \jt{Advances in
  neural information processing systems}  \bvol{23}.

\bibitem[Henderson {\em et~al.\/}(2018)Henderson, Islam, Bachman, Pineau,
  Precup \& Meger]{henderson_deep_2018}
{\sc \au{Henderson, P.}, \au{Islam, R.}, \au{Bachman, P.}, \au{Pineau, J.},
  \au{Precup, D.} \& \au{Meger, D.}} \yr{2018} Deep reinforcement learning that
  matters.  \bt{In {\em Proceedings of the {AAAI} conference on artificial
  intelligence\/}}, ,  \vol{vol.~32}.

\bibitem[Hou \& Xu(2009)]{hou_data-driven_2009}
{\sc \au{Hou, Z.-S.} \& \au{Xu, J.-X.}} \yr{2009}  \at{On data-driven control
  theory: the state of the art and perspective}.  \jt{Scopus} .

\bibitem[Illingworth(2016)]{illingworth_model-based_2016}
{\sc \au{Illingworth, S.~J.}} \yr{2016}  \at{Model-based control of vortex
  shedding at low {Reynolds} numbers}.  \jt{Theoretical and Computational Fluid
  Dynamics}  \bvol{30}~(5),  \pg{429--448}.

\bibitem[Jin {\em et~al.\/}(2020)Jin, Illingworth \&
  Sandberg]{jin_feedback_2020}
{\sc \au{Jin, B.}, \au{Illingworth, S.~J.} \& \au{Sandberg, R.~D.}} \yr{2020}
  \at{Feedback control of vortex shedding using a resolvent-based modelling
  approach}.  \jt{Journal of Fluid Mechanics}  \bvol{897}.

\bibitem[Kiran {\em et~al.\/}(2021)Kiran, Sobh, Talpaert, Mannion, Al~Sallab,
  Yogamani \& Pérez]{kiran_deep_2021}
{\sc \au{Kiran, B.~R.}, \au{Sobh, I.}, \au{Talpaert, V.}, \au{Mannion, P.},
  \au{Al~Sallab, A.~A.}, \au{Yogamani, S.} \& \au{Pérez, P.}} \yr{2021}
  \at{Deep reinforcement learning for autonomous driving: {A} survey}.
  \jt{IEEE Transactions on Intelligent Transportation Systems}  \bvol{23}~(6),
  \pg{4909--4926}.

\bibitem[Kober {\em et~al.\/}(2013)Kober, Bagnell \&
  Peters]{kober_reinforcement_2013}
{\sc \au{Kober, J.}, \au{Bagnell, J.~A.} \& \au{Peters, J.}} \yr{2013}
  \at{Reinforcement learning in robotics: {A} survey}.  \jt{The International
  Journal of Robotics Research}  \bvol{32}~(11),  \pg{1238--1274}.

\bibitem[Kuznetsov {\em et~al.\/}(2020)Kuznetsov, Shvechikov, Grishin \&
  Vetrov]{kuznetsov_controlling_2020}
{\sc \au{Kuznetsov, A.}, \au{Shvechikov, P.}, \au{Grishin, A.} \& \au{Vetrov,
  D.}} \yr{2020} Controlling overestimation bias with truncated mixture of
  continuous distributional quantile critics.  \bt{In {\em International
  {Conference} on {Machine} {Learning}\/}},  \pg{pp. 5556--5566}.

\bibitem[Lanser {\em et~al.\/}(1991)Lanser, Ross \&
  Kaufman]{lanser_aerodynamic_1991}
{\sc \au{Lanser, W.~R.}, \au{Ross, J.~C.} \& \au{Kaufman, A.~E.}} \yr{1991}
  \at{Aerodynamic {Performance} of a {Drag} {Reduction} {Device} on a
  {Full}-{Scale} {Tractor}/{Trailer}}.  \jt{SAE Transactions}  \bvol{100},
  \pg{2443--2451}.

\bibitem[Li \& Zhang(2022)]{li_reinforcement-learning-based_2022}
{\sc \au{Li, J.} \& \au{Zhang, M.}} \yr{2022}  \at{Reinforcement-learning-based
  control of confined cylinder wakes with stability analyses}.  \jt{Journal of
  Fluid Mechanics}  \bvol{932},  \pg{A44}.

\bibitem[Li {\em et~al.\/}(2016)Li, Barros, Bor{\'e}e, Cadot, Noack \&
  Cordier]{li2016feedback}
{\sc \au{Li, R.}, \au{Barros, D.}, \au{Bor{\'e}e, J.}, \au{Cadot, O.},
  \au{Noack, B.~R.} \& \au{Cordier, L.}} \yr{2016}  \at{Feedback control of
  bimodal wake dynamics}.  \jt{Experiments in Fluids}  \bvol{57}~(10),
  \pg{158}.

\bibitem[Lillicrap {\em et~al.\/}(2015)Lillicrap, Hunt, Pritzel, Heess, Erez,
  Tassa, Silver \& Wierstra]{lillicrap_continuous_2015}
{\sc \au{Lillicrap, T.~P.}, \au{Hunt, J.~J.}, \au{Pritzel, A.}, \au{Heess, N.},
  \au{Erez, T.}, \au{Tassa, Y.}, \au{Silver, D.} \& \au{Wierstra, D.}}
  \yr{2015}  \at{Continuous control with deep reinforcement learning}.
  \jt{arXiv preprint arXiv:1509.02971} .

\bibitem[Lin(2002)]{lin_review_2002}
{\sc \au{Lin, J.~C.}} \yr{2002}  \at{Review of research on low-profile vortex
  generators to control boundary-layer separation}.  \jt{Progress in Aerospace
  Sciences}  \bvol{38}~(4),  \pg{389--420}.

\bibitem[Logg {\em et~al.\/}(2012)Logg, Wells \& Hake]{logg_dolfin_2012}
{\sc \au{Logg, A.}, \au{Wells, G.~N.} \& \au{Hake, J.}} \yr{2012}
  \at{{DOLFIN}: a {C}++/{Python} finite element library}.  \bt{In {\em
  Automated {Solution} of {Differential} {Equations} by the {Finite} {Element}
  {Method}: {The} {FEniCS} {Book}\/} (ed. \ed{A.~Logg, K.-A. Mardal \&
  G.~Wells})},  \pg{pp. 173--225}.

\bibitem[Maei {\em et~al.\/}(2009)Maei, Szepesvari, Bhatnagar, Precup, Silver
  \& Sutton]{maei_convergent_2009}
{\sc \au{Maei, H.}, \au{Szepesvari, C.}, \au{Bhatnagar, S.}, \au{Precup, D.},
  \au{Silver, D.} \& \au{Sutton, R.S.}} \yr{2009}  \at{Convergent
  temporal-difference learning with arbitrary smooth function approximation}.
  \jt{Advances in neural information processing systems}  \bvol{22}.

\bibitem[Mnih {\em et~al.\/}(2016)Mnih, Badia, Mirza, Graves, Lillicrap,
  Harley, Silver \& Kavukcuoglu]{mnih_asynchronous_2016}
{\sc \au{Mnih, V.}, \au{Badia, A.~P.}, \au{Mirza, M.}, \au{Graves, A.},
  \au{Lillicrap, T.}, \au{Harley, T.}, \au{Silver, D.} \& \au{Kavukcuoglu, K.}}
  \yr{2016} Asynchronous methods for deep reinforcement learning.  \bt{In {\em
  International conference on machine learning\/}},  \pg{pp. 1928--1937}.

\bibitem[Mnih {\em et~al.\/}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,
  Bellemare, Graves, Riedmiller, Fidjeland \& Ostrovski]{mnih_human-level_2015}
{\sc \au{Mnih, V.}, \au{Kavukcuoglu, K.}, \au{Silver, D.}, \au{Rusu, A.A.},
  \au{Veness, J.}, \au{Bellemare, M.G.}, \au{Graves, A.}, \au{Riedmiller, M.},
  \au{Fidjeland, A.K.} \& \au{Ostrovski, G.}} \yr{2015}  \at{Human-level
  control through deep reinforcement learning}.  \jt{Nature}
  \bvol{518}~(7540),  \pg{529--533}.

\bibitem[Paris {\em et~al.\/}(2021)Paris, Beneddine \&
  Dandois]{paris_robust_2021}
{\sc \au{Paris, R.}, \au{Beneddine, S.} \& \au{Dandois, J.}} \yr{2021}
  \at{Robust flow control and optimal sensor placement using deep reinforcement
  learning}.  \jt{Journal of Fluid Mechanics}  \bvol{913}.

\bibitem[Paris {\em et~al.\/}(2023)Paris, Beneddine \&
  Dandois]{paris_reinforcement-learning-based_2023}
{\sc \au{Paris, R.}, \au{Beneddine, S.} \& \au{Dandois, J.}} \yr{2023}
  \at{Reinforcement-learning-based actuator selection method for active flow
  control}.  \jt{Journal of Fluid Mechanics}  \bvol{955},  \pg{A8}.

\bibitem[Pastoor {\em et~al.\/}(2008)Pastoor, Henning, Noack, King \&
  Tadmor]{pastoor2008feedback}
{\sc \au{Pastoor, M.}, \au{Henning, L.}, \au{Noack, B.~R.}, \au{King, R.} \&
  \au{Tadmor, G.}} \yr{2008}  \at{Feedback shear layer control for bluff body
  drag reduction}.  \jt{Journal of Fluid Mechanics}  \bvol{608},
  \pg{161--196}.

\bibitem[Pino {\em et~al.\/}(2023)Pino, Schena, Rabault \&
  Mendez]{pino_comparative_2023}
{\sc \au{Pino, F.}, \au{Schena, L.}, \au{Rabault, J.} \& \au{Mendez, M.~A}}
  \yr{2023}  \at{Comparative analysis of machine learning methods for active
  flow control}.  \jt{Journal of Fluid Mechanics}  \bvol{958},  \pg{A39}.

\bibitem[Protas(2004)]{protas_linear_2004}
{\sc \au{Protas, B.}} \yr{2004}  \at{Linear feedback stabilization of laminar
  vortex shedding based on a point vortex model}.  \jt{Physics of Fluids}
  \bvol{16}~(12),  \pg{4473--4488}.

\bibitem[Rabault {\em et~al.\/}(2019)Rabault, Kuchta, Jensen, Reglade \&
  Cerardi]{rabault_artificial_2019}
{\sc \au{Rabault, J.}, \au{Kuchta, M.}, \au{Jensen, A.}, \au{Reglade, U.} \&
  \au{Cerardi, N.}} \yr{2019}  \at{Artificial neural networks trained through
  deep reinforcement learning discover control strategies for active flow
  control}.  \jt{Journal of Fluid Mechanics}  \bvol{865},  \pg{281--302}.

\bibitem[Rabault \& Kuhnle(2019)]{rabault_accelerating_2019}
{\sc \au{Rabault, J.} \& \au{Kuhnle, A.}} \yr{2019}  \at{Accelerating deep
  reinforcement learning strategies of flow control through a multi-environment
  approach}.  \jt{Physics of Fluids}  \bvol{31}~(9),  \pg{094105 (9 pp.)}.

\bibitem[Schulman {\em et~al.\/}(2015)Schulman, Levine, Abbeel, Jordan \&
  Moritz]{schulman_trust_2015}
{\sc \au{Schulman, J.}, \au{Levine, S.}, \au{Abbeel, P.}, \au{Jordan, M.} \&
  \au{Moritz, P.}} \yr{2015} Trust region policy optimization.  \bt{In {\em
  International conference on machine learning\/}},  \pg{pp. 1889--1897}.

\bibitem[Schulman {\em et~al.\/}(2017)Schulman, Wolski, Dhariwal, Radford \&
  Klimov]{schulman_proximal_2017}
{\sc \au{Schulman, J.}, \au{Wolski, F.}, \au{Dhariwal, P.}, \au{Radford, A.} \&
  \au{Klimov, O.}} \yr{2017}  \at{Proximal {Policy} {Optimization}
  {Algorithms}}.  \jt{arXiv:1707.06347 [cs]} .

\bibitem[Silver {\em et~al.\/}(2014)Silver, Lever, Heess, Degris, Wierstra \&
  Riedmiller]{silver_deterministic_2014}
{\sc \au{Silver, D.}, \au{Lever, G.}, \au{Heess, N.}, \au{Degris, T.},
  \au{Wierstra, D.} \& \au{Riedmiller, M.}} \yr{2014} Deterministic policy
  gradient algorithms.  \bt{In {\em International conference on machine
  learning\/}},  \pg{pp. 387--395}.

\bibitem[Singh {\em et~al.\/}(1994)Singh, Jaakkola \&
  Jordan]{singh1994learning}
{\sc \au{Singh, S.~P.}, \au{Jaakkola, T.} \& \au{Jordan, M.~I.}} \yr{1994}
  \at{Learning without state-estimation in partially observable markovian
  decision processes}.  \bt{In {\em Machine Learning Proceedings 1994\/}},
  \pg{pp. 284--292}.

\bibitem[Sudin {\em et~al.\/}(2014)Sudin, Abdullah, Shamsuddin, R. \&
  Tahir]{sudin_review_2014}
{\sc \au{Sudin, M.~N.}, \au{Abdullah, M.~A.}, \au{Shamsuddin, S.~A.}, \au{R.,
  Ramli~F.} \& \au{Tahir, M.~M.}} \yr{2014}  \at{Review of research on vehicles
  aerodynamic drag reduction methods}.  \jt{International Journal of Mechanical
  and Mechatronics Engineering}  \bvol{14}~(02),  \pg{37--47}.

\bibitem[Sutton \& Barto(2018)]{sutton_reinforcement_2018}
{\sc \au{Sutton, R.S.} \& \au{Barto, A.G.}} \yr{2018} {\em Reinforcement
  learning: {An} introduction\/}.  \publ{MIT press}.

\bibitem[Sutton {\em et~al.\/}(2009)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesvári \& Wiewiora]{sutton_fast_2009}
{\sc \au{Sutton, R.S.}, \au{Maei, H.R.}, \au{Precup, D.}, \au{Bhatnagar, S.},
  \au{Silver, D.}, \au{Szepesvári, C.} \& \au{Wiewiora, E.}} \yr{2009} Fast
  gradient-descent methods for temporal-difference learning with linear
  function approximation.  \bt{In {\em Proceedings of the 26th annual
  international conference on machine learning\/}},  \pg{pp. 993--1000}.

\bibitem[Sutton {\em et~al.\/}(2008)Sutton, Szepesvári \&
  Maei]{sutton_convergent_2008}
{\sc \au{Sutton, R.S.}, \au{Szepesvári, C.} \& \au{Maei, H.R.}} \yr{2008}
  \at{A convergent {O} (n) algorithm for off-policy temporal-difference
  learning with linear function approximation}.  \jt{Advances in neural
  information processing systems}  \bvol{21}~(21),  \pg{1609--1616}.

\bibitem[Takens(1981)]{takens_detecting_1981}
{\sc \au{Takens, F.}} \yr{1981}  \at{Detecting strange attractors in
  turbulence}.  \bt{In {\em Dynamical systems and turbulence, {Warwick}
  1980\/}},  \pg{pp. 366--381}.

\bibitem[Verma {\em et~al.\/}(2018)Verma, Novati \&
  Koumoutsakos]{verma2018efficient}
{\sc \au{Verma, S.}, \au{Novati, G.} \& \au{Koumoutsakos, P.}} \yr{2018}
  \at{Efficient collective swimming by harnessing vortices through deep
  reinforcement learning}.  \jt{Proceedings of the National Academy of
  Sciences}  \bvol{115}~(23),  \pg{5849--5854}.

\bibitem[Vignon {\em et~al.\/}(2023)Vignon, Rabault \&
  Vinuesa]{vignon2023recent}
{\sc \au{Vignon, C.}, \au{Rabault, J.} \& \au{Vinuesa, R.}} \yr{2023}
  \at{Recent advances in applying deep reinforcement learning for flow control:
  Perspectives and future directions}.  \jt{Physics of Fluids}  \bvol{35}~(3).

\bibitem[Wang {\em et~al.\/}(2023)Wang, Yan, Hu, Chen, Rabault \&
  Noack]{wang2023dynamic}
{\sc \au{Wang, Q.}, \au{Yan, L.}, \au{Hu, G.}, \au{Chen, W.}, \au{Rabault, J.}
  \& \au{Noack, B.~R.}} \yr{2023}  \at{Dynamic feature-based deep reinforcement
  learning for flow control of circular cylinder with sparse surface pressure
  sensing}.  \jt{arXiv preprint arXiv:2307.01995} .

\bibitem[Wang {\em et~al.\/}(2016)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu
  \& de~Freitas]{wang_sample_2016}
{\sc \au{Wang, Z.}, \au{Bapst, V.}, \au{Heess, N.}, \au{Mnih, V.}, \au{Munos,
  R.}, \au{Kavukcuoglu, K.} \& \au{de~Freitas, N.}} \yr{2016}  \at{Sample
  efficient actor-critic with experience replay}.  \jt{arXiv preprint
  arXiv:1611.01224} .

\bibitem[Watkins \& Dayan(1992)]{watkins_q-learning_1992}
{\sc \au{Watkins, C.J.} \& \au{Dayan, P.}} \yr{1992}  \at{Q-learning}.
  \jt{Machine learning}  \bvol{8}~(3),  \pg{279--292}.

\bibitem[White~III \& Scherer(1994)]{white_iii_finite-memory_1994}
{\sc \au{White~III, C.~C.} \& \au{Scherer, W.~T.}} \yr{1994}  \at{Finite-memory
  suboptimal design for partially observed {Markov} decision processes}.
  \jt{Operations Research}  \bvol{42}~(3),  \pg{439--455}.

\bibitem[Xu \& Zhang(2023)]{xu_reinforcement-learning-based_2023}
{\sc \au{Xu, D.} \& \au{Zhang, M.}} \yr{2023}  \at{Reinforcement-learning-based
  control of convectively unstable flows}.  \jt{Journal of Fluid Mechanics}
  \bvol{954},  \pg{A37}.

\bibitem[Yu \& Bertsekas(2008)]{yu_near_2008}
{\sc \au{Yu, H} \& \au{Bertsekas, D.~P.}} \yr{2008}  \at{On near optimality of
  the set of finite-state controllers for average cost {POMDP}}.
  \jt{Mathematics of Operations Research}  \bvol{33}~(1),  \pg{1--11}.

\bibitem[Zeng \& Graham(2021)]{zeng_symmetry_2021}
{\sc \au{Zeng, K.} \& \au{Graham, M.~D.}} \yr{2021}  \at{Symmetry reduction for
  deep reinforcement learning active control of chaotic spatiotemporal
  dynamics}.  \jt{Physical Review E}  \bvol{104}~(1),  \pg{014210}.

\bibitem[Ziebart {\em et~al.\/}(2008)Ziebart, Maas, Bagnell \&
  Dey]{ziebart_maximum_2008}
{\sc \au{Ziebart, B.~D.}, \au{Maas, A.}, \au{Bagnell, J.~A.} \& \au{Dey,
  A.~K.}} \yr{2008} Maximum entropy inverse reinforcement learning.  \bt{In
  {\em Aaai\/}}, ,  \vol{vol.~8},  \pg{pp. 1433--1438}.

\end{thebibliography}
