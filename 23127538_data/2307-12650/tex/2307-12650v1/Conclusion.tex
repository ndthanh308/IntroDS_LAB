\section{Conclusions}\label{sec:Conclusions}

In this study, maximum entropy RL with TQC is performed in an active flow control application with partial measurements to learn a feedback controller for bluff body drag reduction. Neural network controllers have been trained by the RL algorithm to discover a control strategy to stabilize the vortex shedding behind a 2D square bluff body at $Re=100$. By comparing the control performances in FM environments to PM environments, we showed a non-negligible degradation of RL control performance if the controller is not trained with full-state information. To solve this issue, we proposed a method to train a dynamic neural network controller with an approximation of a finite-history sufficient statistic and formulate the dynamic controller as a NARX model. The dynamic controller was able to improve the drag reduction performance in PM environments and achieve near-optimal performance (100\% with respect to the baseflow drag) compared to a static controller (48\%).   We found that the optimal horizon of the finite history in NARX is approximately two vortex shedding periods, when the sensors are located only on the base of the body. The importance of including exogenous action terms in the observations of RL is discussed, by pointing out the degradation of $32.04\%$ on drag reduction if only past measurements are used in the PM environment. Finally, we proposed an optimal power consumption design for the reward function based on the drag power savings and the power of the actuator. This power-based reward function offers an intuitive understanding of the system's performance whereas electromechanical losses can be also added directly, once a specific actuator is chosen. Moreover, its inherent feature of being hyperparameter-free contributes to a straightforward reward function design process in the context of flow control problems.

%and energy-efficient control performance is indicated by positive rewards during the training. This reward function ensures that RL optimizes a controller to achieve maximum energy efficiency, which generalizes a drag reduction problem to the discovery of an energy-saving controller. This work systematically constructs a fundamental of a feasible RL flow control scheme that learns energy-efficient controllers with a practical sensor layout that only obtains partial-state information. 

It was shown that model-free RL was able to discover an optimal control strategy without any prior knowledge of the system dynamics using partial realistic measurements, exploiting only input-output data from the simulation environment. Therefore, this particular study on RL-based active flow control in 2D laminar flow simulations can be seen as a promising direction for controlling the complex dynamics of 3D turbulent flows by replacing the simulation environment with the experimental setup. %In practical applications, issues such as signal delay, noise and computation speed have to be considered in the implementation of RL.


\

Data availability: the RL code and data will become available at github.com/orgs/RigasLab.