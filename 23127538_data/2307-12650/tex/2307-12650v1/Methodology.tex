\section{Methodology}\label{sec:Method}

% Figure environment removed

We demonstrate the RL drag reduction framework on the flow past a 2D square bluff body at laminar regimes characterized by two-dimensional vortex shedding. Control is applied by two jet actuators at the rear edge of the body before the fixed separation and partial- or full-state observations are obtained from pressure sensors on the rear base or near wake region, respectively.
The RL agent handles the optimization, control and interaction with the flow simulation environment, as shown in figure \ref{fig:RL_framework}. $a_t$, $o_t$ and $r_t$ are used to denote actions, observations and rewards at time step $t$.
% The RL agent interacts with the environment by obtaining observations $o_t$ and correspondingly applying actions $a_t$, then rewards $r_t$ are evaluated to approximate the value of $a_t$.

Details of the flow environment are provided in \S\ref{subsec:Flow}. The SAC and TQC RL algorithms used in this work are introduced in \S\ref{subsec:SACTQC}. The reward functions based on optimal energy efficiency are presented in \S\ref{subsec:Reward}. The method to convert a POMDP to an MDP by designing a dynamic controller for achieving nearly-optimal RL control performance is discussed in \S\ref{subsec:PM_Dynamic}.



\subsection{Flow environment}\label{subsec:Flow}
The environment is a 2D Direct Numerical Simulation (DNS) of the flow past a square bluff body of height $B$. The velocity profile at the inflow of the computational domain is uniform with freestream velocity $U_\infty$. All quantities are non-dimensionalized with the bluff body height $B$ and the freestream velocity $U_\infty$. The Reynolds number, defined as $Re = U_{\infty} B/\nu$, is $100$. The computational domain is rectangular with boundaries at  $(-20.5,26.5)$ in the streamwise $x$ direction  and $(-12.5,12.5)$ in the transverse $y$ direction. The centre of the square bluff body is at $(x,y) = (0,0)$. 

The DNS flow environment is simulated using FEniCS and the Dolfin library \citep{logg_dolfin_2012}, based on the implementation of \cite{rabault_artificial_2019,rabault_accelerating_2019}. The  incompressible unsteady Navier-Stokes equations are solved using a finite element method and the incremental pressure correction scheme. The DNS time step is $dt = 0.004$.

Two blowing and suction jet actuators are placed on the top and bottom surfaces of the bluff body before separation. The velocity profile $\boldsymbol{U_{j}}$ of the two jets ($j=1, 2$; 1 for the top jet and 2 for the bottom jet) is defined as
\begin{equation}
\boldsymbol{U_{j}} = \left(0, \quad \frac{3 Q_{j}}{2 w}\left[1-\left(\frac{2 x-B-w}{w}\right)^{2}\right]\right),
\label{eq:jet_u}
\end{equation}
where $Q$ is the mass flow rate of the jets, and $w$ is the width of the jet actuator. In this study, $w=0.1$. 
A zero mass flow rate condition of the two jets enforces momentum conservation as
\begin{equation}
Q_{1}+Q_{2}=0.
\label{eq:zero_mass}
\end{equation}
The mass flow rate of the jets is also constrained as $|Q_j|\leqslant0.1$ to avoid excessive actuation.

%% Figure environment removed

%The sketch and the mesh of the flow environment are shown in figure \ref{fig:Flow_mesh}. The mesh of the simulation is provided by Gmsh \citep{geuzaine_gmsh_2009}, and it is refined around the body and at the regions of jets. 
%Coordinate $x$ corresponds to the stream wise flow direction and $y$ is the vertical one.

In PM environments, $N=64$  vertically equispaced pressure sensors are placed on the base of the bluff body, the coordinates of which are given by
\begin{equation}
\boldsymbol{P_{base,k}}=\left(\frac{B}{2},\frac{-B}{2}+ k \frac{B}{N+1}\right),
\label{eq:Probe_base}
\end{equation}
where $k = 1,2....,N$.
In FM environments, $64$ pressure sensors are placed in the wake region with a refined bias close to the body. The locations of sensors in the wake are defined with sets $\boldsymbol{x_s} = \left[0.25, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0\right]$ and $\boldsymbol{y_s} = \left[-1.5, -1.0, -0.5, -0.25, 0.25, 0.5, 1.0, 1.5\right]$, following the formula
\begin{equation}
\boldsymbol{P_{wake,i,j}}=\left(\frac{B}{2} + x_{s,i}, y_{s,j}\right),
\label{eq:Probe_wake}
\end{equation}
where $i = 1,2....,8$ and $j = 1,2....,8$.

%The drag force $F_{D}$ of the cylinder is defined as the surface integral of the forces on the cylinder with respect to the $x$ coordinate. Similarly, the lift force $F_{L}$ is defined as the surface integral of the forces on the cylinder with respect to the $y$ coordinate.

% \begin{equation}
% F_{D}=\int_{S}(\sigma \cdot \boldsymbol{n}) \cdot \boldsymbol{i}_{x} \mathrm{~d} s,
% \label{eq:drag}
% \end{equation}
% where $\sigma$ is the Cauchy stress tensor, $\boldsymbol{n}$ is the wall-normal unit vector pointing to the outer surface of the cylinder, and $\boldsymbol{i}_{x}$ is a unit vector in $x$ direction as $(1,0)$. 

The bluff body drag coefficient $C_{D}$ is defined as
\begin{equation}
C_{D}=\frac{F_{D}}{\frac{1}{2} \rho_{\infty} {U_{\infty}}^{2} B},
\label{eq:CD}
\end{equation}
and the lift coefficient $C_{L}$ as
\begin{equation}
C_{L}=\frac{F_{L}}{\frac{1}{2} \rho_{\infty} {U_{\infty}}^{2} B},
\label{eq:CL}
\end{equation}
where $F_{D}$ and $F_{L}$ are the drag and lift forces, defined as the surface integral of the pressure and viscous forces on the bluff body with respect to the $x$ and $y$ coordinates, respectively.

% Similarly, the lift force $F_{L}$ is defined as
% \begin{equation}
% F_{L}=\int_{S}(\sigma \cdot \boldsymbol{n}) \cdot \boldsymbol{i}_{y} \mathrm{~d} s,
% \label{eq:lift}
% \end{equation}
% where $\boldsymbol{i}_{y}$ is a unit vector in $y$ direction as $(0,1)$, 

%%%%% SAC&TQC %%%%%
\subsection{Maximum entropy reinforcement learning of MDPs} \label{subsec:SACTQC}

RL can be defined as policy search in a Markov Decision Process (MDP), with a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})$ where $\mathcal{S}$ is a set of states, and $\mathcal{A}$ is a set of actions. $\mathcal{P}\left(s_{t+1} \mid s_t, a_t\right)$ is a state transition function that contains the probability from current states $s_t$ and actions $a_t$ to the next state $s_{t+1}$. $\mathcal{R}(s, a)$ is a reward function (cost function) to be maximised. The RL agent collects data as states $s_t \in \mathcal{S}$ from the environment, and a policy $\pi(s_t)$ executes actions $a_t \in \mathcal{A}$ to drive the environment to the next state $s_{t+1}$. 
%After reaching $s_{t+1}$, rewards $r \in \mathcal{R}$ are evaluated to approximate the value of $a$, and the agent optimizes the neural-network parameterised policy based on the rewards using various algorithms. 

A state is considered to have the Markov property if the state at time $t$ retains all the necessary information to determine the future dynamics at $t+1$, without any information from the past \citep{sutton_reinforcement_2018}. This property can be presented as
\begin{equation}
\mathcal{P}\left\{r_{t+1}, s_{t+1} \mid s_{t}, a_{t}\right\} \equiv \mathcal{P}\left\{r_{t+1}, s_{t+1} \mid s_{0}, a_{0}, r_{1}, \ldots, s_{t-1}, a_{t-1}, r_{t}, s_{t}, a_{t}\right\}.
\label{eq:markov_property}
\end{equation}
In the present flow control application, the control task can be regarded as an MDP if observations $o_t$ contain full-state information, i.e. $o_t = s_t$, and satisfy  \req{eq:markov_property}.


SAC and TQC are two maximum entropy RL algorithms used in the present work. TQC is used by default since it is an improved version of SAC.
The maximum entropy RL maximizes
\begin{equation}
J\left(\pi\right) = \sum_{t=0}^T \mathbb{E} \left[r_t\left(s_t, a_t\right)+\alpha \mathcal{H}\left(\pi\left(\cdot \mid s_t\right)\right)\right],
\label{eq:RL_Optimize}
\end{equation}
where $r_t$ is the reward (reward functions given in \S\ref{subsec:Reward}). The entropy term or ``information entropy'' is by definition $\mathcal{H}\left(\pi\right) = \mathbb{E} \left[ -\log\pi\right]$ and $\alpha$ is the entropy coefficient, which controls the stochasticity (exploration) of the optimal policy. For $\alpha=0$, the standard reward optimisation in conventional reinforcement learning is recovered.

SAC was developed based on Soft Policy Iteration (SPI) \citep{haarnoja_soft_2018}. SPI uses a soft Q-function to evaluate the value of a policy and optimizes the policy based on its value. The soft Q-function is calculated by applying a Bellman backup operator $\mathcal{T}^\pi$ as
\begin{equation}
\mathcal{T}^\pi Q\left({s}_t, {a}_t\right) \triangleq r_t\left({s}_t, {a}_t\right)+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim \mathcal{P}}\left[V\left({s}_{t+1}\right)\right],
\end{equation}
where $\gamma$ is a discount factor (here $\gamma=0.99$), and $V\left({s}_{t+1}\right)$ satisfies
\begin{equation}
V\left({s}_t\right)=\mathbb{E}_{{a}_t \sim \pi}\left[Q\left({s}_t, {a}_t\right)-\log \pi\left({a}_t \mid {s}_t\right)\right].
\end{equation}
The target soft Q-function can be obtained by repeating $Q = \mathcal{T}^\pi Q$, and the proof of convergence can be referred to as Soft Policy Evaluation (Lemma 1) in \cite{haarnoja_soft_2018}. With a soft Q-function rendering values for the policy, the policy optimization is given as Soft Policy Improvement (Lemma 2 in \cite{haarnoja_soft_2018}). 

In SAC, a stochastic soft Q-function $Q_\theta\left({s}_t, {a}_t\right)$ and a policy $\pi_\phi\left({a}_t \mid {s}_t\right)$ are parameterized by artificial neural networks $\theta$ (critic) and $\phi$ (actor) respectively. 
During training, $Q_\theta\left({s}_t, {a}_t\right)$ and $\pi_\phi\left({a}_t \mid {s}_t\right)$ are optimized with stochastic gradients ${\nabla}_{\theta}J_Q(\theta)$ and $\nabla_\phi J_\pi(\phi)$ designed  corresponding to Soft Policy Evaluation and Soft Policy Improvement respectively (see equation (6) and (10) in \cite{haarnoja_soft_2018}). With these gradients, SAC updates the critic and actor networks by
\begin{equation}
\theta \leftarrow \theta-\lambda_Q {\nabla}_{\theta} J_Q\left(\theta\right),
\label{eq:Q_update}
\end{equation}
\begin{equation}
\phi \leftarrow \phi-\lambda_\pi \nabla_\phi J_\pi(\phi),
\label{eq:Pi_update}
\end{equation}
where $\lambda_Q$ and $\lambda_\pi$ are the learning rates of Q-function and policy, respectively.
Typically, two Q-functions are trained independently, and then the minimum of the Q-functions is brought into the calculation of stochastic gradient and policy gradient. This method is also used in our work to increase the stability and speed of training.
SAC also supports automatic adjustment of temperature $\alpha$ by optimization, 
\begin{equation}
\alpha^*=\arg \min _{\alpha} \mathbb{E}_{{a}_t \sim \pi^*}\left[-\alpha \log \pi^*\left({a}_t \mid {s}_t ; \alpha\right)-\alpha \overline{\mathcal{H}}\right].
\label{eq:temp_optimize}
\end{equation}
This adjustment transforms a hyperparameter-tuning challenge into a trivial optimization problem  \citep{haarnoja_soft_2018}.

TQC \citep{kuznetsov_controlling_2020} can be regarded as an improved version of SAC as it alleviates the overestimation bias of the Q-function on the basic algorithm of SAC. 
TQC adapts the idea of distributional reinforcement learning with quantile regression, i.e. QR-DQN \citep{dabney_distributional_2018}, to format the return function $R(s, a):=\sum_{t=0}^{\infty} \gamma^t r_t\left(s_t, a_t\right)$ into a distributional representation with Dirac delta functions as 
\begin{equation}
R_{\psi}(s, a):=\frac{1}{M} \sum_{m=1}^M \delta\left(z_{\psi}^m(s, a)\right),
\label{eq:R_distribution}
\end{equation}
where $R(s, a)$ is parameterized by $\psi$, and $R_{\psi}(s, a)$ is converted into a summation of $M$ ``atoms'' as $z_{\psi}^m(s, a)$. Here only one approximation of $R(s, a)$ is used for demonstration.
Then, only $k$ smallest atoms of $z_{\psi}^m(s, a)$ are preserved as a truncation to obtain
truncated atoms 
\begin{equation}
y_i(s, a):=r(s, a)+\gamma\left[z_{\psi}^i\left(s^{\prime}, a^{\prime}\right)-\alpha \log \pi_\phi\left(a^{\prime} \mid s^{\prime}\right)\right], \quad i \in[1 . . k],
\label{eq:truncated_atoms}
\end{equation}
where $s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)$ and $ a^{\prime} \sim \pi\left(\cdot \mid s^{\prime}\right)$. The truncated atoms form a target distribution as 
\begin{equation}
Y(s, a):=\frac{1}{k} \sum_{i=1}^{k} \delta\left(y_i(s, a)\right),
\label{eq:target_distribution}
\end{equation}
and the algorithm minimizes the 1-Wasserstein distance between the original distribution $R_{\psi}(s, a)$ and the target distribution $Y(s, a)$ to obtain a truncated quantile critic.
Further details, such as the design of loss functions and the pseudocode of TQC can be found in \cite{kuznetsov_controlling_2020}.

In this work, the RL interaction runs on a longer time step $t_a = 0.5$ compared to the numerical time step $dt$. RL-related data $o_t$, $a_t$ and $r_t$ are sampled every $t_a$ time interval. With a different numerical step and an RL step, control actuation $c_{n_s}$ for every numerical step should be distinguished from action $a_t$ in RL. There are $\frac{t_a}{dt}=125$ numerical steps between two RL steps, and control actuation is applied based on a first-order-hold function as
\begin{equation}
c_{n_s}= a_{t-1} + (a_t - a_{t-1})\frac{n_s dt}{t_a},
\label{eq:FOH_action}
\end{equation}
where $n_s$ denotes the number of numerical steps after the previous action $a_{t-1}$. Equation \req{eq:FOH_action} smooths the control actuation with linear interpolation to avoid numerical instability.
Unless specified, the neural network configuration is set as 3 layers of 512 neurons for both actor and critic. The entropy coefficient in \req{eq:RL_Optimize} is initialised to $0.01$ and automatically tuned based on \req{eq:temp_optimize} during training. 
%\subsubsection{POMDP and RL}

% \iffalse
% %%%% Introduce Time Chart %%%%
% In the RL framework, the design of a reward function $r_t$ is essential because it directly affects the optimization problem to maximize the cost function in equation \ref{eq:RL_Optimize}. The reward functions in this study are sampled from a continuous flow environment (relatively small numerical step). One common-used reward function with time-averaged value is inherited from Rabault et al. \citep{rabault_artificial_2019,rabault_accelerating_2019}, as 
% \begin{equation}
% r_t[k]= - C_{D0}-\left\langle C_D\right\rangle_T-\alpha\left|\left\langle C_L\right\rangle_T\right|,
% \label{TavgR}
% \end{equation}
% where $k$ denotes the $k^{th}$ sample during one episode. $C_{D0}$ is the drag coefficient of the baseline flow (control not applied), $<>_T$ denotes the time-averaged value, and $\alpha=0.2$ is used as an empirical coefficient. This time-averaged reward is computed from the samplings during one action step period, i.e. between $(k-1)^{th}$ and $k^{th}$ samples, while a FOH is implemented between two actions, as 

% %%%%% PPO %%%%%
% The RL agent uses a state-of-the-art algorithm as Proximal Policy Optimization (PPO) \citep{schulman_proximal_2017}. In PPO, an actor-critic architecture is embedded that the actor contains a policy $\pi_{\theta}$ which provides actions. At the same time, the critic evaluates the quality of the actions using value or advantage functions ($V_{\pi}$ or $A_{\pi}$). In this actor-critic architecture, a policy gradient method is used to realize gradient ascent as 
% \begin{equation}
% \theta \leftarrow \theta+\alpha \nabla_{\theta} L(\theta),
% \label{eq:policy_gradient}
% \end{equation}
% where $\theta$ is a set of weights that parameterizes the policy $\pi_{\theta}$, and $\alpha$ is a learning parameter. $\nabla_{\theta}$ denotes the policy gradient and $L(\theta)$ is an objective function.
% The PPO usually uses a clipped surrogate objective function as 
% \iffalse
% \begin{equation}
% L^{C L I P}(\theta)=\hat{\mathbb{E}}_{t}\left[\min \left(pr_{t}(\theta) \hat{A}_{t}, \operatorname{clip}\left(pr_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right],
% \end{equation}
% where  $\hat{A}_{t}$ is an estimator of the advantage function that evaluates the value of the actions at timestep t. The term $\operatorname{clip}\left(pr_{t}(\theta), 1-\epsilon, 1+\epsilon\right)$ clips the probability ratio $pr_{t}(\theta)$ to a constant outside the range $[1-\epsilon, 1+\epsilon]$, with a hyperparameter $\epsilon$. The probability ratio $pr_{t}(\theta)$ is defined as
% \begin{equation}
% pr_{t}(\theta)=\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)}.
% \end{equation}

% Therefore, this clipped objective function $L^{C L I P}(\theta)$ constraints the amplitude of the policy gradient, increasing the robustness of RL. In PPO, the critic calculates the objective function $L^{C L I P}(\theta)$ and the policy gradient $\nabla_{\theta} L(\theta)$ after every learning iteration. Then, the policy is updated as equation \req{eq:policy_gradient}, and new actions are provided by a new set of weights $\theta$ to the environment.
% \fi
% PPO has an advantage that it can handle several actors in parallel with multiple episodes, which guarantees the efficiency of learning in flow environments. Another advantage of PPO is that it maintains the robustness while reduces the complexity of implementation, compared to another similar algorithm, Trust Region Policy Optimization (TRPO) \citep{schulman_trust_2015}.

% There are other state-of-the-art policy gradient RL algorithms which were developed in recent years, besides PPO and TRPO. Some of these algorithms use off-policy strategy, such as Deterministic Policy Gradient (DPG) \citep{silver_deterministic_2014}, Deep Deterministic Policy Gradient (DDPG) \citep{lillicrap_continuous_2015}, Actor-Critic with Experience Replay (ACER) \citep{wang_sample_2016}, Twin Delayed Deep Deterministic Policy Gradient (TD3) \citep{fujimoto_addressing_2018}, and Soft Actor-Critic (SAC) \citep{haarnoja_soft_2018}. Others use an on-policy strategy, such as Asynchronous Advantage Actor-Critic (A3C) \citep{mnih_asynchronous_2016}.
% These algorithms are simply introduced in case of interests and the details of these algorithms can be referred to their article sources.
% \fi

\subsection{Reward design for optimal energy efficiency} \label{subsec:Reward}

%The design of the reward function is essential in RL because the learning process maximizes the reward as the optimization problem in Eq. \req{eq:RL_Optimize}. 
We propose a hyperparameter-free reward function based on net power saving to discover  energy-efficient flow control policies, calculated as the difference between the power saved from drag reduction $\Delta P_{D}$ and the power consumed from actuation $P_{act}$. Then, the power reward (``PowerR'')
 at the RL control frequency is
%
\begin{equation}
r_t= \underbrace{\Delta P_{D}}_{\textrm{power saved}}- \underbrace{P_{act}}_{\textrm{power spent}}.
\label{eq: PowerR}
\end{equation}
%
The power saved from drag reduction is given by 
\begin{equation}
\Delta P_{D} = P_{D0}-P_{Dt} = \left(\left\langle F_{D0}\right\rangle_T - \left\langle F_{Dt}\right\rangle_a\right) U_{\infty},
\label{eq: Drag Power Saving}
\end{equation}
where $P_{D0}$ is the time-averaged baseline drag power without control, and $\left\langle F_{D0}\right\rangle_T$ is the time-averaged baseline drag over a sufficiently long period. $P_{Dt}$ denotes the time-averaged drag power calculated from the time-averaged drag $\left\langle F_{Dt}\right\rangle_a$ during one RL step $t_a$. Specifically, $\langle ~ 
 \rangle_a$ quantities are calculated at each RL step using 125 DNS samples. 
The jet power consumption of actuation $P_{act}$ \citep{barros_bluff_2016} is defined as
\begin{equation}
P_{act}=\sum_{j=1}^2\left|\rho_{\infty} \langle U_{j} \rangle_a^{3} S_{j}\right| = \sum_{j=1}^2\left|\frac{ \left\langle a_{t}\right\rangle_a^{3}}{\rho_{\infty}^{2} S_{j}^{2}}\right|,
\label{eq: Actuation Power}
\end{equation}
where $\langle U_{j} \rangle_a$ is the average jet velocity, and $S_j$ denotes the area of one jet. 

The reward function given by  \req{eq: PowerR} quantifies the control efficiency of a controller directly. Thus, it guarantees the learning of a control strategy which simultaneously maximises the drag reduction and minimises the required control actuation. Additionally, this energy-based reward function avoids the effort of hyperparameter tuning.

%%%%%% If needed %%%%%%
% On the basis of power evaluation, a power-saving ratio (PSR) is also used to evaluate the energy efficiency of an RL-trained controller. The definition of PSR was given by \cite{protas_drag_2002}, as
% \begin{equation}
% \mathrm{PSR}=\frac{\Delta P_{D}}{P_{act}}=\left|\frac{(\left\langle F_{D0}\right\rangle_T - F_{Dt}) U_{\infty} \rho_{\infty}^{2} S_{j}^{2}}{n_{j} a_{t}^{3}}\right|.
% \label{eq: PSR}
% \end{equation}

All the cases in this work use the power-based reward function defined in \req{eq: PowerR} unless otherwise specified. For comparison, a reward function based on drag and lift coefficient (``ForceR'') is also implemented, as suggested by \citep{rabault_artificial_2019} with a pre-tuned hyperparameter $\epsilon=0.2$, as
\begin{equation}
r_t^a= C_{D0} - \left\langle C_{Dt}\right\rangle_a - \epsilon\left| \left\langle C_{Lt}\right\rangle_a\right|,
\label{eq: DragR}
\end{equation}
where $C_{D0}$ and $\left\langle C_{Dt}\right\rangle_a$ are calculated from a constant baseline drag and RL-step-averaged drag and lift. The RL-step-averaged lift $\left| \left\langle C_{Lt}\right\rangle_a\right|$ is used to penalize the amplitude of actuation on both sides of the body, avoiding excessive lift force (i.e. the lateral deflection of the wake reduces the drag but increases the side force), and indirectly penalising control actuation and the discovery of unrealistic control strategies. $\epsilon$ is a hyperparameter designed to balance the penalty on drag and lift force. 

The instantaneous versions of these two reward functions are also investigated for practical implementation purposes (both experimentally and numerically) because they can significantly reduce memory used during computation and also support a lower sampling rate. These instantaneous reward functions are computed only from observations at each RL step. In comparison, the reward functions above take into account the time history between two RL steps, while the instantaneous version of the power reward (``PowerInsR'') is defined as
\begin{equation}
r_{t,ins}= \Delta P_{D,ins}-P_{act,ins},
\label{eq: InsPowerR}
\end{equation}
where $\Delta P_{D,ins}$ is given by
\begin{equation}
\Delta P_{D,ins}= \left(\left\langle F_{D0}\right\rangle_T -  F_{Dt}\right) U_{\infty},
\label{eq: Ins Drag Power}
\end{equation}
and $P_{act,ins}$ is defined as 
\begin{equation}
P_{act,ins}=\sum_{j=1}^2\left|\rho_{\infty} \overline{U_{j}}^{3} S_{j}\right| = \sum_{j=1}^2\left|\frac{ a_{t}^{3}}{\rho_{\infty}^{2} S_{j}^{2}}\right|.
\label{eq: Ins Actuation Power}
\end{equation}
Notice that the definition of reward in \req{eq: InsPowerR} - \req{eq: Ins Actuation Power} is similar to \req{eq: PowerR} - \req{eq: Ins Actuation Power}, and the only difference is that the average operator $\langle ~ 
 \rangle_a$ is removed.
Similarly, the instantaneous version of the force-based reward function (``ForceInsR'') is defined as
\begin{equation}
r_{t,ins}^a= C_{D0} - C_{Dt} - \epsilon\left| C_{Lt}\right|.
\label{eq: InsDragR}
\end{equation}
In \S\ref{subsec:Rewards_Study}, we present results on the study of different reward functions and compare the RL performance.

\subsection{POMDP and dynamic controllers}\label{subsec:PM_Dynamic}

In practical applications, the Markov property \req{eq:markov_property} is often not valid due to noise, broken sensors, partial state information and delays. This means the observations available to the RL agent do not provide full or true state information, i.e. $o_t \neq s_t $, while in MDP $o_t = s_t $. Then, RL can be generalized as a Partially Observable Markov Decision Process (POMDP) \citep{cassandra_survey_1998}.
A POMDP can be defined as a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{Y}, \mathcal{O})$, where $\mathcal{Y}$ is a finite set of observations $o_t$ and $\mathcal{O}$ is an observation function that relates observations to underlying states.

With only PM available in the flow environments (sensors on the base of the body instead in the wake), the spatial information is missing along the streamwise direction. Takens' embedding theorem \citep{takens_detecting_1981} states that the underlying dynamics of a high-dimensional dynamical system can be reconstructed from low-dimensional measurements with their time history. Therefore, past measurements can be incorporated into a sufficient statistic. Furthermore, convective delays may be introduced in the state observation, since the sensors are not located in the wavemaker region of the flow. According to \cite{altman1992closed}, past actions are also required in the states of an delayed problem to reduce it into an undelayed problem. This is because a typical delayed-MDP (DMDP) implicitly subverts the Markov Property, as the past measurements and actions only encapsulate partial information. 

Therefore, combining the ideas of augmenting past measurements and past actions, we  form a sufficient statistic \citep{bertsekas_dynamic_2012}, for reducing the POMDP problem to an MDP, defined as
\begin{equation}
I_k= [p_0,...,p_k, a_0,...,a_{k-1}],
\label{eq:Sufficient_statistic}
\end{equation}
which consists of the time history of pressure measurements $p_0,...,p_k$ and control actions $a_0,...,a_{k-1}$ at time steps $0,...,k$. This enlarged state at time $k$ contains all the information known to the controller at time $k$. 

%There exists a function parameterised by a policy neural network $\pi_\phi$ that satisfies
%\begin{equation}
%\mu_k^*\left(\Gamma_k\right)=\pi_{\phi}\left(\Gamma_k\right),
%\end{equation}
%where $\mu_k^*$ is optimal control.

However, the size of the sufficient statistic in \req{eq:Sufficient_statistic} grows over time, leading to a non-stationary closed-loop system with control and introducing a challenge in RL that the number of inputs to the networks varies over time. This problem can be solved by reducing \req{eq:Sufficient_statistic} to a finite-history approximation \citep{white_iii_finite-memory_1994}. The controller using this finite-history approximation of the sufficient statistic is usually known as a ``finite-state'' controller, and the error of this approximation converges as the size of the finite history increases \citep{yu_near_2008}. The trade-off is that the dimension of the input increases based on the history length required.  
The nonlinear policy, which is parameterised by a neural network controller, has the algebraic description 
\begin{equation}
a_t \sim \pi_{\phi} \left(a_t \mid \underbrace{a_{t-1}, a_{t-2}, \ldots, a_{t-N_{fs}-1}}_{\text{past actions}}, p_t, \underbrace{p_{t-1}, p_{t-2}, \ldots, p_{t-N_{fs}}}_{\text{past measurements}} \right),
\label{eq:NARX}
\end{equation}
where $p_t$ represents pressure measurements at time step $t$, and $N_{fs}$ denotes the size of the finite history. The above expression \label{eq:NARX} is equivalent to a nonlinear autoregressive exogenous model (NARX). 

A ``frame stack'' technique is used to feed the ``finite history sufficient statistic'' to the RL agent as input to both the actor and critic neural networks. Frame stack  constructs the observation $o_t$ from the latest actions and measurements at step $t$ as a ``frame'' $ o_t = (a_{t-1}, p_t)$, and piles up the finite history of $N_{fs}$ frames together into a stack. The number of stacked frames is equivalent to the size of the finite history $N_{fs}$. 

The neural network controller trained as a NARX model benefits from past information to approximate the next optimal control action since the policy has been parameterised as a nonlinear transfer function. Thus, a controller parameterised as a NARX model is denoted as a ``dynamic'' controller because the time history in the NARX model contains dynamic information of the system. Correspondingly, a controller fed with only the latest actions and measurements is denoted as a ``static'' controller. %Also, the RL training depends on the augmented state, as the approximation of Q-function, $Q_\theta$, includes past information. Therefore, \req{eq:NARX} only interprets an RL-trained controller, but the effect of past action-measurement observations is more profound in the whole RL framework. 

% Figure environment removed

Figure \ref{fig:Case_Demo} demonstrates three cases with both FM and PM environments which will be investigated. In the FM environment, sensors are located in the wake as $\boldsymbol{P_{base}}$ given by \req{eq:Probe_base}. In the PM environment, sensors are placed only on the back surface of the body as $\boldsymbol{P_{wake}}$  given by \req{eq:Probe_wake}. The static controller is employed in the FM environment, and both static and dynamic controllers are applied in the PM environment.
 Results will be shown with $N_{fs} = 27$ and in \S\ref{subsec:Nfs} a parametric study of the effect of the finite history length is presented.



% A "frame stack" (FS) technique is used to adapt the idea of delay embedding, facilitating the RL agent to identify the complete dynamics of the entire flowfield from past information.  
% Figure \ref{fig:FS} demonstrates the idea behind FS to construct an observation $\mathcal{F}_n$ at step $n$ with past information. 
% It is worth noticing that Eq.\req{eq:FS} can be converted to Eq.\req{eq:Delay_embedding} when the observations $\gamma(t)$ are considered as a frame $F_n$.
% However, in this work, FS is not equivalent to delay embedding. This is because the RL observations $o_t$ are designed to include not only pressure measurements $p_t$ but also previous actions $a_{t-1}$, while no action is considered in the delay embedding theorem. The observation construction is simply given as $o_t = \{ p_t, a_{t-1} \}$.

% With the FS technique and observation construction in the RL framework, the controller parameterized by a policy neural network $\pi_{\phi}$ can be formulated as a nonlinear autoregressive exogenous model (NARX), which has the algebraic description as 
% \begin{equation}
% a_t \sim \pi_{\phi} \left(a_t \mid \underbrace{a_{t-1}, a_{t-2}, \ldots, a_{t-N_{fs}-1}}_{\text{past actions}}, p_t, \underbrace{p_{t-1}, p_{t-2}, \ldots, p_{t-N_{fs}}}_{\text{past measurements}} \right),
% \label{NARX}
% \end{equation}
% where $N_{fs}$ denotes the number of frames stacked. The neural network controller trained as a NARX model benefits from the past information to render a better approximation of the next optimal control action. The past actions are included in Eq.\req{NARX} as exogeneous terms. It should be noted that not only the controller but also the RL training process is affected by FS, as the approximation of Q-function $Q_\theta$ and optimization both include past information. Therefore, Eq.\req{NARX} only interprets an RL-trained controller, but the effect of FS and action-included observations is more profound in the whole RL framework. Importantly, a controller trained as a NARX model is denoted as a ``dynamic'' controller, because the past time series in the NARX model contains dynamic information of the system. Correspondingly, a controller fed with only current actions and measurements is denoted as a ``static'' controller.

% % Figure environment removed

\iffalse
To implement delay embedding in the RL control, a frame stack method is used to process the inputs to NNs. The frame stack method implicitly forms delayed measurement signals by collecting data from the buffer and stacking them in a large input vector. Several stacked frames, i.e., delay horizons, were used in the implementation to find a fair number of delayed signals in the inputs to NNs.

An appropriate choice of time delay $\tau$ is essential in practical applications. The present study uses a technique with Average Mutual Information (AMI) $\textbf{cite Fraser}$ to choose $\tau$. The idea of using AMI is that AMI represents a general correlation between two signals, which ensures, for example, $[x(t),x(t+\tau)]$ are not indistinguishable and also not independent. If a series of measurements are available as $[x(t_i),x(t_i+\tau)]$, where $i=1,2,...,k$ is the number of samples, the AMI is defined as 
\begin{equation}
I(\tau)=\sum_{i=1}^{k} P\left(x\left(t_i\right), x\left(t_i+\tau\right)\right) \log _2\left[\frac{P\left(x\left(t_i\right), x\left(t_i+\tau\right)\right)}{P\left(x\left(t_i\right)\right) P\left(x\left(t_i+\tau\right)\right)}\right],
\label{eq:AMI}
\end{equation}
where $P\left(x\left(t_i\right), x\left(t_i+\tau\right)\right)$ is a joint probability distribution of $x\left(t_i\right)$ and $x\left(t_i+\tau\right)$, and $P\left(x\left(t_i\right)\right)$ is a marginal probability distribution of $x\left(t_i\right)$. The optimal time delay $\tau$ is chosen at the first local minimum of $I(\tau)$ $\textbf{cite Fraser}$, or choose $\tau$ such that $I(\tau)/I(0)\approx1/e$ $\textbf{cite Abarbanel}$. There is no unique choice of $tau$, and the best criterion we believe is based on the final performance. Another method is to use autocorrelation function $\textbf{cite Schuster}$, which will not be further introduced.

The embedding dimension $m$ is also vital for implementing the delay embedding technique. In Takens' Embedding Theorem, for a system with a manifold of dimension $d$, a choice of $m>2d$ is enough to reconstruct the dynamics. Still, a technique is required without knowing the actual $d$. A False Nearest Neighbors (FNN) method $\textbf{cite Kennel}$ is used in this work to choose a proper $m$.

\begin{equation}
\frac{\left[R_{d+1}^2(n, r)-R_d^2(n, r)\right]^{1 / 2}}{R_d^2(n, r)}=\frac{\left|x(n+d T)-x^{(r)}(n+d T)\right|}{R_d(n, r)}>R_T
\label{eq:FNN}
\end{equation}

\begin{equation}
R_d^2(n, r)=\sum_{k=1}^d\left[x(n+k T)-x^{(r)}(n+k T)\right]^2
\label{eq:Euclidian_d}
\end{equation}

Connection to RL
\fi