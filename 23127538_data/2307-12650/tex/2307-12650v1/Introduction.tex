\section{Introduction}\label{sec:Intro}

Up to $50\%$ of total road vehicle energy consumption is due to aerodynamic drag  \citep{sudin_review_2014}.  
In order to improve vehicle aerodynamics, flow control approaches have been applied targeting the wake pressure drag, which is the dominant source of drag. Passive flow control has been applied \citep{choi2014aerodynamics} through geometry/surface modifications, e.g., boat tails \citep{lanser_aerodynamic_1991} and vortex generators \citep{lin_review_2002}. However, passive control designs   do not adapt to environmental changes (disturbances, operating regimes), leading to sub-optimal performance under variable operating conditions. 
Active open-loop techniques, where pre-determined signals drive actuators, are typically energy inefficient since they target mean flow modifications. Actuators typically employed are synthetic jets \citep{glezer_synthetic_2002}, movable flaps \citep{beaudoin_bluff-body_2006,brackston_stochastic_2016} and plasma actuators \citep{corke_dielectric_2010}, among others. Since the flow behind vehicles is unsteady and subject to environmental disturbances and uncertainty, active feedback control is required to achieve optimal performance.  
 However, two major challenges arise in feedback control design, which we aim to tackle in this study: (i) the flow dynamics are governed by the infinite-dimensional, nonlinear and non-local Navier-Stokes equations \citep{brunton2015closed} and (ii) are partially observable in realistic applications due to sensor limitations.  

\subsection{Model-based active flow control} 

Model-based feedback control design requires a tractable model for the dynamics of the flow, usually obtained by data-driven or operator-driven techniques. 
Such methods have been applied successfully to control benchmark two-dimensional (2D) bluff body wakes, obtaining improved aerodynamic performance, e.g. vortex shedding suppression and drag reduction. 
For example, \cite{gerhard_model-based_2003} controlled the circular cylinder wake at low Reynolds numbers based on a low-dimensional model obtained from the Galerkin projection of Karhunen-Loeve modes on the governing Navier-Stokes equations. 
\cite{protas_linear_2004} applied Linear Quadratic Gaussian control to stabilize vortex shedding based on a F\"oppl point vortex model.
\cite{illingworth_model-based_2016} applied the Eigensystem Realization Algorithm as a system identification technique to obtain a reduced-order model of the flow and used robust control methods to obtain feedback control laws. 
\cite{jin_feedback_2020} employed resolvent analysis to obtain a low-order input-output model from the Navier-Stokes equations based on which feedback control was applied to suppress vortex shedding. 

Model-based flow control has also been applied at high Reynolds numbers to control dominant coherent structures (persisting spatio-temporal symmetry breaking modes) which contribute to drag, including unsteady vortex shedding   \citep{pastoor2008feedback, dahan_feedback_2012, dalla2017reducing, brackston_modelling_2018} and steady spatial symmetry breaking modes \citep{li2016feedback,   brackston_stochastic_2016}. 
For inhomogeneous flows in all three spatial dimensions, low-order models typically fail to capture the intractable and complex  turbulent dynamics, leading inevitably to suboptimal control performance when used in control synthesis.  

\subsection{Model-free active flow control by reinforcement learning} 

Model-free data-driven control methods bypass the above limitations by using input/output data from the dynamical system (environment) to learn the optimal control law (policy) directly without exploiting information from a mathematical model of the underlying process \citep{hou_data-driven_2009}.

Model-free reinforcement learning (RL) has been successfully used for controlling complex systems, for which obtaining accurate and tractable models can be challenging. RL learns an optimal policy (controller) based on observed states that generate control actions which maximize a reward by exploring and exploiting  state-action pairs. The system dynamics governing the evolution of the states for a specific action (environment) are assumed to be a Markov Decision Process (MDP). The policy is parameterized by artificial neural networks as a universal function approximator that can be optimized to an arbitrary control function with any order of complexity.
RL can also be interpreted as parameterized dynamic programming with the feature of universal function approximation \citep{bertsekas_reinforcement_2019}, showing its ability to conduct optimization with input-output data from complex systems.

RL can effectively control complex systems in various types of tasks, such as robotics \citep{kober_reinforcement_2013}, and autonomous driving \citep{kiran_deep_2021}. In the context of fluid dynamics, \citep{bucci_control_2019, zeng_symmetry_2021} applied RL to control the chaotic  Kuramotoâ€“Sivashinsky system. 
In the context of flow control for drag reduction, \cite{rabault_artificial_2019,rabault_accelerating_2019}  used RL control for the first time in 2D bluff body simulations at a laminar regime. The RL algorithm discovered an optimal policy that, using pressure sensors in the wake and near the body,  drives blowing and suction actuators on the circular cylinder to decrease the mean drag and wake unsteadiness. 
%\cite{feng_ren_applying_2021} extended the work \citep{rabault_artificial_2019} from $Re = 100$ to a chaotic regime at $Re = 1000$, and RL can still learn an effective control with longer training time than low Reynolds number case.
\cite{paris_robust_2021} applied the ``S-PPO-CMA'' RL algorithm to control the wake behind a 2D cylinder and optimise the sensor locations in the near wake.
\cite{li_reinforcement-learning-based_2022} augmented and guided RL with global linear stability and sensitivity analyses in order to control the confined cylinder wake. They showed that, if the sensors cover the wavemaker region, the RL is robust and successfully stabilises the vortex shedding. 
\cite{paris_reinforcement-learning-based_2023} proposed an RL methodology to optimize actuator placement in a laminar 2D flow around an airfoil, addressing the trade-off between performance and the number of actuators.
\cite{xu_reinforcement-learning-based_2023} used RL to suppress instabilities both in the Kuramoto-Sivashinsky system and 2D boundary layers, showing the effectiveness and robustness of RL control.
\cite{pino_comparative_2023} compared RL and genetic programming algorithms to global optimization techniques for various cases, including the viscous Burger's equation and vortex shedding behind a 2D cylinder.
Further information about RL and its applications in fluid mechanics can be found in the reviews of \cite{garnier_review_2021} and \cite{vignon2023recent}.

\subsection{Maximum entropy reinforcement learning}

In RL algorithms, two major branches have been developed:``on-policy'' learning and ``off-policy'' learning. RL algorithms can also be classified into value-based, policy-based, and actor-critic methods \citep{sutton_reinforcement_2018}. The actor-critic architecture combines advantages from both value-based and policy-based methods, so the state-of-the-art algorithms mainly use actor-critic architecture. 

The state-of-the-art on-policy algorithms include Trust Region Policy Optimization (TRPO, \cite{schulman_trust_2015}), Asynchronous Advantage Actor-Critic (A3C, \cite{mnih_asynchronous_2016}), and Proximal Policy Optimization (PPO, \cite{schulman_proximal_2017}). On-policy algorithms require fewer computational resources than off-policy algorithms, but they are demanding in terms of available data (interactions with the environment). They  use the same policy to obtain experience in the environment and update with policy gradient, which introduces a high self-relevant experience that may restrict convergence to a local minimum and limit exploration. As the amount of data needed for training grows with the complexity of applications, on-policy algorithms usually require a long training time for collecting data and converging. 

By contrast, off-policy algorithms usually have both behaviour and target policies to facilitate exploration while retaining exploitation. The behaviour policy usually employs stochastic behaviour to interact with an environment and collect experience, which is used to update the target policy. There are many off-policy algorithms emerging in the past decade, such as Deterministic Policy Gradient (DPG, \cite{silver_deterministic_2014}), Deep Deterministic Policy Gradient (DDPG, \cite{lillicrap_continuous_2015}), Actor-Critic with Experience Replay (ACER, \cite{wang_sample_2016}), Twin Delayed Deep Deterministic Policy Gradient (TD3, \cite{fujimoto_addressing_2018}), Soft Actor-Critic (SAC, \cite{haarnoja_soft_2018-1,haarnoja_soft_2018}) and Truncated Quantile Critics (TQC, \cite{kuznetsov_controlling_2020}). 
Due to the behaviour-target framework, off-policy algorithms are able to  exploit past information from a replay buffer to further increase sample efficiency. This ``experience replay'' suits a value-function-based method \citep{mnih_human-level_2015}, such as Q-learning \citep{watkins_q-learning_1992}, instead of calculating the policy gradient directly. Therefore, most of the off-policy algorithms implement an actor-critic architecture with a Q-learning basis, e.g. SAC. 

One of the challenges of off-policy algorithms is the brittleness in terms of convergence. \cite{sutton_convergent_2008,sutton_fast_2009} solved the instability issue of off-policy learning with linear approximations. They used a Bellman-error-based cost function together with stochastic gradient descent (SGD) to ensure the convergence of learning. \cite{maei_convergent_2009} further extended this method to nonlinear function approximation using a modified temporal difference algorithm. However, some algorithms nowadays still experience the problem of brittleness when using improper hyperparameters. Adapting these algorithms for control in various environments is sometimes challenging, as the learning stability is sensitive to their hyperparameters, such as DDPG \citep{duan_benchmarking_2016,henderson_deep_2018}.

To increase sample efficiency and learning stability, state-of-the-art off-policy algorithms were developed with a maximum entropy framework \citep{ziebart_maximum_2008,haarnoja_reinforcement_2017}, known as ``maximum entropy reinforcement learning''. Maximum entropy RL solves an optimization problem by maximizing the cumulative reward augmented with an entropy term. With the entropy term in the cost function, these algorithms have wider exploration in the action space during the learning, and the policy can approximate near-optimal behaviours, increasing the robustness of the RL controller. More details about two particular maximum entropy RL algorithms (SAC and TQC) can be found in \S\ref{subsec:SACTQC}.

\subsection{Partial measurements and POMDP}

In most RL flow control applications, RL controllers have been assumed to have information from the entire flowfield or an optimal sensor layout without any limitations on the sensor locations. This is denoted as ``full measurement'' (FM) in this study as the measurements contain full-state information. In practical applications, measurements are typically obtained on the surface of the body (e.g. pressure taps), and only partial-state information is available. This is denoted as ``partial measurement'' (PM), comparatively. 
PM can lead to control performance degradation compared to FM because the sensors are restricted from observing enough information from the entire flow. 

In the language of RL, control with PM can be described as a Partially Observable Markov Decision Process (POMDP)\citep{cassandra_survey_1998} instead of an MDP. 
In POMDP problems, the best stationary policy can be arbitrarily worse than the optimal policy in the underlying MDP \citep{singh1994learning}. In order to improve the performance of RL with POMDP, additional steps are required to reduce the POMDP problem to an MDP problem. This can be done trivially by using an augmented state, known as ``sufficient statistic'' \citep{bertsekas_dynamic_2012}, i.e. augmenting the state vector with past measurements and actions \citep{bucci_control_2019,wang2023dynamic}, or  Recurrent Neural Networks (RNN), such as Long-Short Term Memory \citep{verma2018efficient}. 
%Several studies applied RL with RNN to POMDP problems \citep{wierstra_solving_2007,hausknecht_deep_2015,zhu_improving_2018,meng_memory-based_2021}, but there is still a lack of investigations on RL flow control with a POMDP condition.

\subsection{Contribution of the present work}

The present work uses RL to discover control strategies of partially observable fluid flow environments. Fluid flow systems typically exhibit more complex sampling in higher dimensional observation space compared to other physical systems, necessitating a robust exploration strategy and rapid convergence in the optimization process. To address these challenges, we employ off-policy-maximum entropy RL algorithms (SAC and TQC) that efficiently identify optimal policies in the large action space inherent to fluid flow systems, especially for cases with partial measurements and observability.

We aim to achieve two objectives related to RL flow control for bluff body drag reduction problems. First, we aim to improve the RL control performance in a PM environment by reducing a POMDP problem to an MDP problem. More details about this method are introduced in 
\S\ref{subsec:PM_Dynamic}. Second, we present investigations on different reward functions and key hyperparameters to develop an approach that can be adapted to a broader range of flow control applications. We demonstrate the proposed framework and its capability to discover optimal feedback control strategies in the benchmark laminar flow of a square 2D bluff body with fixed separation at the trailing edge, using sensors only on the base of the body. 

%Third, we present investigations on different reward functions and key hyperparameters to develop a more versatile approach that can be effectively adapted to a broader range of applications.
%We believe the implementation of RL with PM in this study can provide a solid foundation for practical applications of RL control.

The article is structured as follows. In Section \S\ref{sec:Method}, the RL framework is presented, which consists of the SAC and TQC optimization algorithms interacting with the flow simulation environment. A hyperparameter-free reward function is proposed to optimise the energy efficiency of the dynamically controlled system. Exploiting past action-state information converts the POMDP problem in a PM environment to an MDP, enabling the discovery of near-optimal policies.
Results will be presented and discussed in Section \S\ref{sec:Results}. The convergence study of RL is first introduced. The degradation of RL control performance in PM environments (POMDP) is presented, and the improvement is addressed by exploiting a sequence of past action-measuremnt information. At the end of this section, we compare the results from TQC with SAC, addressing the advantages of using TQC as an improved version of SAC.
In Section \S\ref{sec:Conclusions}, we provide conclusions for the current research and discuss future research directions. 