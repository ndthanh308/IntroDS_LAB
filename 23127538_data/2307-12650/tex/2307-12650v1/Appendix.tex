\appendix

%\section{Reward Study} \label{App:Rewards}

%In section \ref{Result_drag_reduction}, a time-averaged power-based reward function given by Eq. \req{eq: PowerR} has been implemented, and effective controllers can be learned by RL as presented. In this section, RL control results with other forms of reward functions (introduced in \ref{subsec:Reward}) are provided and discussed.

%% Figure environment removed

%The control performance of RL control with different reward functions is demonstrated by drag coefficient $C_D$ in figure \ref{fig:Reward_Study}. Static controllers are trained in FM environments and dynamic controllers are trained in PM environments. In FM cases, control performance is not sensitive to the choice of reward functions. This is because the RL agent is informed with full-state information to construct a good approximation of the value function via a critic NN. Thus, the optimization algorithm can always find an optimal control strategy based on the value approximation, as long as the design of a reward function is reasonable. There is a minor discrepancy between power-based rewards and force-based rewards during the transient phase of control, e.g. between $t=10$ and $t=30$. It takes a longer time for the RL controller with a power-based reward to stabilize the unsteadiness in the flow. The reason is that power-based optimization tries to find the most energy-efficient way and smaller actuation is regarded as a better solution if possible.
%In PM cases, due to the incompleteness of information from the flow system, the training is more sensitive to the reward function to obtain an accurate value approximation. With time-averaged power-based and force-based rewards, the controllers both achieve sub-optimal performances around $99\%$ drag reduction. Although there is unsteadiness or less drag reduction in the cases using instantaneous rewards, due to missing information between RL steps, the $C_D$ curves still show promising control performances with fluctuations less than $1\%$ of drag reduction.
%In general, all four types of reward functions studied in this work show their feasibility in this particular RL flow control application. The results show that RL with TQC is compatible with both time-averaged and instantaneous rewards. These designs of reward functions can be extended to various RL flow control problems in both simulation and experiment environments.

%\section{SAC vs TQC}\label{App:SACvsTQC}

%% Figure environment removed

%There is no apparent difference between TQC and SAC because they share a similar core of the optimization algorithm. In the case of FM, SAC might demonstrate faster convergence speed, but they converge to the same performance measure, such as reduced drag. This could be attributed to the fact that the quantile regression process in TQC provides a richer understanding of the potential outcomes, adding complexity to the evaluation process. Despite this, it does not necessarily mean that it leads to a less accurate result in fully observable cases compared to SAC.
%\\In the PM cases, TQC outperforms SAC at both convergence speed and drag reduced in both static and dynamic control scenarios. The core benefit of TQC under these conditions can be attributed to its advanced handling of overestimation bias of rewards. By constructing a richer representation of possible returns, TQC provides an enhanced understanding of the value distribution, which allows for a more accurate estimation of the state-action values. This process of learning a distribution over returns, instead of just the expected return as in SAC, assists TQC in managing the uncertainties inherent in partially observable environments. This way, TQC can adapt more robustly to changes and uncertainties, leading to superior performance in both static and dynamic control tasks.\\
%Therefore, the choice between SAC and TQC might depend largely on the specific characteristics of the task or environment in question. It is also worth noting that TQC's enhanced performance in handling uncertainties may come at the cost of added computational complexity, which may not be feasible in every application.

