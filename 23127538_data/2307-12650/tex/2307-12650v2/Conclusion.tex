\section{Conclusions}\label{sec:Conclusions}

In this study, maximum entropy RL with TQC has been performed in an active flow control application with partial measurements to learn a feedback controller for bluff body drag reduction. Neural network controllers have been trained by the RL algorithm to discover a drag reduction control strategy behind a 2D square bluff body at $Re=100$. By comparing the control performances in FM environments to PM environments, we showed a non-negligible degradation of RL control performance if the controller is not trained with full-state information. To solve this issue, we proposed a method to train a dynamic neural network controller with an approximation of a finite-history sufficient statistic and formulate the dynamic controller as a NARX model. The dynamic controller was able to improve the drag reduction performance in PM environments and achieve near-optimal performance ({drag reduction ratio $\eta = 97\% $} with respect to the baseflow drag) compared to a static controller ({$\eta = 56\% $}). We found that the optimal horizon of the finite history in NARX is approximately two vortex shedding periods when the sensors are located only on the base of the body. The importance of including exogenous action terms in the observations of RL is discussed, by pointing out the degradation of    {$\eta = 29.55\% $} on drag reduction if only past measurements are used in the PM environment. Also, we proposed a net power consumption design for the reward function based on the drag power savings and the power of the actuator. This power-based reward function offers an intuitive understanding of the closed-loop performance, whereas electromechanical losses can also be added directly, once a specific actuator is chosen. Moreover, its inherent feature of being hyperparameter-free contributes to a straightforward reward function design process in the context of flow control problems.    {Results from SAC are compared with TQC, and we showed the improvement by TQC, which attenuates overestimation in neural networks.}

It was shown that model-free RL was able to discover a nearly optimal control strategy without any prior knowledge of the system dynamics using partial realistic measurements, exploiting only input-output data from the simulation environment. Therefore, this particular study on RL-based active flow control in 2D laminar flow simulations can be seen as a    {step towards} controlling the complex dynamics of 3D turbulent flows    {in practical applications} by replacing the simulation environment with the experimental setup.    {Also, the frame stack method employed here to convert the POMDP to an MDP can be replaced by recurrent neural networks and attention-based architectures, which may further improve control performance in a scenario with complex dynamics.}

\

{\small
\noindent {\bf Funding.} We acknowledge support from the UKRI AI for Net Zero grant EP/Y005619/1.

\

\noindent  {\bf Data availability statement.} The open-source code of this project is available on the GitHub repository: https://github.com/RigasLab/Square2DFlowControlDRL-PM-NARX-SB3.
The code is developed from the work by \citet{rabault_artificial_2019} and \citet{rabault_accelerating_2019}, using a simulation environment by FEniCS v2017.2.0 \citep{logg_dolfin_2012}.
The RL algorithm is adapted to a version with SAC/TQC, implemented by Stable-Baselines3 and Stable-Baselines3-contrib \citep{stable-baselines3} in a PyTorch \citep{paszke2019pytorch} environment. See Appendix \ref{App:Sim_details} and the GitHub repository for more details of the simulation.

\

\noindent  {\bf Declaration of interests.} The authors report no conflict of interest.
}