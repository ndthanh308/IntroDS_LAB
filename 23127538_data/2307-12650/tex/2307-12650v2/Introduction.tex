\section{Introduction}\label{sec:Intro}

Up to $50\%$ of total road vehicle energy consumption is due to aerodynamic drag  \citep{sudin_review_2014}.  
In order to improve vehicle aerodynamics, flow control approaches have been applied targeting the wake pressure drag, which is the dominant source of drag. Passive flow control has been applied \citep{choi2014aerodynamics} through geometry/surface modifications, e.g., boat tails \citep{lanser_aerodynamic_1991} and vortex generators \citep{lin_review_2002}. However, passive control designs do not adapt to environmental changes (disturbances, operating regimes), leading to sub-optimal performance under variable operating conditions. 
Active open-loop techniques, where pre-determined signals drive actuators, are typically energy inefficient since they target mean flow modifications. Actuators typically employed are synthetic jets \citep{glezer_synthetic_2002}, movable flaps \citep{beaudoin_bluff-body_2006,brackston_stochastic_2016} and plasma actuators \citep{corke_dielectric_2010}, among others. Since the flow behind vehicles is unsteady and subject to environmental disturbances and uncertainty, active feedback control is required to achieve optimal performance.  
However, two major challenges arise in feedback control design, which we aim to tackle in this study: (i) the flow dynamics are governed by the infinite-dimensional, nonlinear and non-local Navier-Stokes equations \citep{brunton2015closed}, and (ii) are partially observable in realistic applications due to sensor limitations. 
This study aims to tackle these challenges, particularly focusing on the potential of model-free control for a partially observable laminar flow, characterised by bluff body vortex shedding, as a preliminary step towards more complex flows and applications.

\subsection{Model-based active flow control} 

Model-based feedback control design requires a tractable model for the dynamics of the flow, usually obtained by data-driven or operator-driven techniques. 
Such methods have been applied successfully to control benchmark two-dimensional (2D) bluff body wakes, obtaining improved aerodynamic performance, e.g. vortex shedding suppression and drag reduction. 
For example, \cite{gerhard_model-based_2003} controlled the circular cylinder wake at low Reynolds numbers based on a low-dimensional model obtained from the Galerkin projection of Karhunen-Loeve modes on the governing Navier-Stokes equations. 
\cite{protas_linear_2004} applied Linear Quadratic Gaussian control to stabilise vortex shedding based on a F\"oppl point vortex model.
\cite{illingworth_model-based_2016} applied the Eigensystem Realization Algorithm as a system identification technique to obtain a reduced-order model of the flow and used robust control methods to obtain feedback control laws. 
\cite{jin_feedback_2020} employed resolvent analysis to obtain a low-order input-output model from the Navier-Stokes equations based on which feedback control was applied to suppress vortex shedding. 

Model-based flow control has also been applied at high Reynolds numbers to control dominant coherent structures (persisting spatio-temporal symmetry breaking modes) which contribute to drag, including unsteady vortex shedding   \citep{pastoor2008feedback, dahan_feedback_2012, dalla2017reducing, brackston_modelling_2018} and steady spatial symmetry breaking modes \citep{li2016feedback,   brackston_stochastic_2016}. 
For inhomogeneous flows in all three spatial dimensions, low-order models typically fail to capture the intractable and complex turbulent dynamics, leading inevitably to sub-optimal control performance when used in control synthesis.  

\subsection{Model-free active flow control by reinforcement learning} 

Model-free data-driven control methods bypass the above limitations by using input/output data from the dynamical system (environment) to learn the optimal control law (policy) directly without exploiting information from a mathematical model of the underlying process \citep{hou_data-driven_2009}.

Model-free reinforcement learning (RL) has been successfully used for controlling complex systems, for which obtaining accurate and tractable models can be challenging. RL learns a control policy based on observed states and generates control actions which maximise a reward by exploring and exploiting state-action pairs. The system dynamics governing the evolution of the states for a specific action (environment) are assumed to be a Markov Decision Process (MDP). The policy is parameterised by artificial neural networks as a universal function approximator that can be optimised to an arbitrary control function with any order of complexity.
RL with neural networks can also be interpreted as parameterised dynamic programming with the feature of universal function approximation \citep{bertsekas_reinforcement_2019}. Therefore, RL requires only input-output data from complex systems in order to discover control policies using model-free optimisation.

RL can effectively learn to control complex systems in various types of tasks, such as robotics \citep{kober_reinforcement_2013} and autonomous driving \citep{kiran_deep_2021}. In the context of chaotic dynamics related to fluid mechanics, \cite{bucci_control_2019} and \cite{zeng_symmetry_2021} applied RL to control the chaotic Kuramotoâ€“Sivashinsky system. 
In the context of flow control for drag reduction, \citet{rabault_artificial_2019,rabault_accelerating_2019} used RL control for the first time in 2D bluff body simulations at a laminar regime. The RL algorithm discovered a policy that, using pressure sensors in the wake and near the body, drives blowing and suction actuators on the circular cylinder to decrease the mean drag and wake unsteadiness. 
\citet{tang2020robust} trained RL-controlled synthetic jets in the flow past a 2D cylinder at several Reynolds numbers, [100, 200, 300, 400], and achieved drag reduction in a range of Reynolds number from 60 to 400, showing the generalisation ability of RL active flow control.
\citet{paris_robust_2021} applied the ``S-PPO-CMA'' RL algorithm to control the wake behind a 2D cylinder and optimise the sensor locations in the near wake.
\citet{li_reinforcement-learning-based_2022} augmented and guided RL with global linear stability and sensitivity analyses in order to control the confined cylinder wake. They showed that if the sensors cover the wavemaker region, the RL is robust and successfully stabilises the vortex shedding. 
\citet{paris_reinforcement-learning-based_2023} proposed an RL methodology to optimise actuator placement in a laminar 2D flow around an airfoil, addressing the trade-off between performance and the number of actuators.
\citet{xu_reinforcement-learning-based_2023} used RL to suppress instabilities both in the Kuramoto-Sivashinsky system and 2D boundary layers, showing the effectiveness and robustness of RL control.
\citet{pino_comparative_2023} compared RL and genetic programming algorithms to global optimisation techniques for various cases, including the viscous Burger's equation and vortex shedding behind a 2D cylinder.
\citet{chen2023deep} applied RL in the flow control of vortex-induced vibration of a 2D square bluff body with various actuator layouts. The vibration and drag of the body were both reduced and mitigated effectively by RL policies.

Recently, RL has been used to control complex fluid systems, such as flows in turbulent regimes, in both simulations and experiments, addressing the potential of RL flow control in realistic applications.
\citet{fan_reinforcement_2020} extended RL flow control to a turbulent regime in experiments at Reynolds number of $O\left(10^5\right)$, achieving effective drag reduction by controlling the rotation speed of two cylinders downstream of a bluff body. RL successfully discovered the global optimal open-loop control strategy that was previously found from a laborious non-automated, systematic grid search. The experimental results were further verified by high-fidelity numerical simulations.
\citet{ren_applying_2021} examined RL-controlled synthetic jets in a weakly turbulent regime, demonstrating effective control at Reynolds number of 1000. This flow control problem of drag reduction of a 2D cylinder flow using synthetic jets was extended to Reynolds number of 2000 by \citet{varela2022deep}. In their work, RL discovered a strategy of separation delay via high-frequency perturbations to achieve drag reduction.
\citet{sonoda2023reinforcement} and \citet{guastoni2023deep} applied RL control in numerical simulations of turbulent channel flow and showed that RL control can outperform opposition control in this complex flow control task. 

RL techniques have been also applied to various flow control problems with different geometries, such as flow past a 2D cylinder \citep{rabault_artificial_2019}, vortex-induced vibration of a 2D square bluff body \citep{chen2023deep}, and a 2D boundary layer \citep{xu_reinforcement-learning-based_2023}. However, model-free RL control techniques also have several drawbacks compared to model-based control. For example, it is usually challenging to tune the various RL hyperparameters. Also,  model-free RL typically requires large amounts of training data through interactions with the environment, which makes RL expensive and infeasible for certain applications.
\color{black}
Further information about RL and its applications in fluid mechanics can be found in the reviews of \cite{garnier_review_2021} and \cite{vignon2023recent}.

\subsection{Maximum entropy reinforcement learning}

In RL algorithms, two major branches have been developed: ``on-policy'' learning and ``off-policy'' learning. RL algorithms can also be classified into value-based, policy-based, and actor-critic methods \citep{sutton_reinforcement_2018}. The actor-critic architecture combines advantages from both value-based and policy-based methods, so the state-of-the-art algorithms mainly use actor-critic architecture. 

The state-of-the-art on-policy algorithms include Trust Region Policy Optimization (TRPO, \cite{schulman_trust_2015}), Asynchronous Advantage Actor-Critic (A3C, \cite{mnih_asynchronous_2016}), and Proximal Policy Optimization (PPO, \cite{schulman_proximal_2017}). On-policy algorithms require fewer computational resources than off-policy algorithms, but they are demanding in terms of available data (interactions with the environment). They use the same policy to obtain experience in the environment and update with policy gradient, which introduces a high self-relevant experience that may restrict convergence to a local minimum and limit exploration. As the amount of data needed for training grows with the complexity of applications, on-policy algorithms usually require a long training time for collecting data and converging. 

By contrast, off-policy algorithms usually have both behaviour and target policies to facilitate exploration while retaining exploitation. The behaviour policy usually employs stochastic behaviour to interact with an environment and collect experience, which is used to update the target policy. There are many off-policy algorithms emerging in the past decade, such as Deterministic Policy Gradient (DPG, \cite{silver_deterministic_2014}), Deep Deterministic Policy Gradient (DDPG, \cite{lillicrap_continuous_2015}), Actor-Critic with Experience Replay (ACER, \cite{wang_sample_2016}), Twin Delayed Deep Deterministic Policy Gradient (TD3, \cite{fujimoto_addressing_2018}), Soft Actor-Critic (SAC, \cite{haarnoja_soft_2018-1,haarnoja_soft_2018}) and Truncated Quantile Critics (TQC, \cite{kuznetsov_controlling_2020}). 
Due to the behaviour-target framework, off-policy algorithms are able to exploit past information from a replay buffer to further increase sample efficiency. This ``experience replay'' suits a value-function-based method \citep{mnih_human-level_2015}, instead of calculating the policy gradient directly. Therefore, most of the off-policy algorithms implement an actor-critic architecture, e.g. SAC. 

One of the challenges of off-policy algorithms is the brittleness in terms of convergence. \cite{sutton_convergent_2008,sutton_fast_2009} tackled the instability issue of off-policy learning with linear approximations. They used a Bellman-error-based cost function together with stochastic gradient descent (SGD) to ensure the convergence of learning. \cite{maei_convergent_2009} further extended this method to nonlinear function approximation using a modified temporal difference algorithm. However, some algorithms nowadays still experience the problem of brittleness when using improper hyperparameters. Adapting these algorithms for control in various environments is sometimes challenging, as the learning stability is sensitive to their hyperparameters, such as DDPG \citep{duan_benchmarking_2016,henderson_deep_2018}.

To increase sample efficiency and learning stability, off-policy algorithms were developed within a maximum entropy framework \citep{ziebart_maximum_2008,haarnoja_reinforcement_2017}, known as ``maximum entropy reinforcement learning''. Maximum entropy RL solves an optimisation problem by maximizing the cumulative reward augmented with an entropy term. In this context, the concept of entropy was first introduced by \citet{shannon1948mathematical} in the information theory. The entropy quantifies the uncertainty of a data source, which is extended to the uncertainty of the outputs of stochastic neural networks in the RL framework. During the training phase, the maximum entropy RL maximises rewards and entropy simultaneously to improve control robustness \citep{ziebart2010modeling} and increase exploration via diverse behaviours \citep{haarnoja_reinforcement_2017}. Further details about maximum entropy RL and two particular algorithms used in the present work (SAC and TQC) are introduced in \S\ref{subsec:SACTQC}.

\subsection{Partial measurements and POMDP}

In most RL flow control applications, RL controllers have been assumed to have {full-state} information {(the term ``state'' is in the context of control theory)} or a sensor layout without any limitations on the sensor locations. In this study, it is denoted as ``full measurement'' (FM) when measurements contain full-state information. In practical applications, measurements are typically obtained on the surface of the body (e.g. pressure taps), and only partial-state information is available {due to the missing downstream evolution of the system dynamics}. This is denoted as ``partial measurement'' (PM), comparatively. 
PM can lead to control performance degradation compared to FM because the sensors are restricted from observing enough information from the flowfield. 
In the control of vortex shedding, full stabilisation can be achieved by placing sensors within the wavemaker region of bluff bodies, which is located approximately at the end of the recirculation region. In this case, full-state information regarding the vortex shedding is available to sensors. Placing sensors far from the recirculation region, for example, on the rear surface of the bluff body (denoted as PM in this work), introduces a convection delay of vortex shedding sensing and partial observation of the state of the system.
 
In the language of RL, control with PM can be described as a Partially Observable Markov Decision Process (POMDP)\citep{cassandra_survey_1998} instead of an MDP. 
In POMDP problems, the best stationary policy can be arbitrarily worse than the optimal policy in the underlying MDP \citep{singh1994learning}. In order to improve the performance of RL with POMDP, additional steps are required to reduce the POMDP problem to an MDP problem. This can be done trivially by using an augmented state, known as ``sufficient statistic'' \citep{bertsekas_dynamic_2012}, i.e. augmenting the state vector with past measurements and actions \citep{bucci_control_2019,wang2023dynamic}, or  Recurrent Neural Networks (RNN), such as Long-Short Term Memory (LSTM) \citep{verma2018efficient}. Theoretically, LSTM networks and augmented state approaches can yield comparable performance in partially observable problems \citep[see][Supplementary]{cobbe2020leveraging}. Practically, the augmented state methodology provides notable benefits, including reduced training complexity and ease in parameter tuning, provided that the control state dynamics are tractable and short-term correlated.

In the specific case for which flowfield information is available, POMDP can also be reduced to an MDP by flow reconstruction techniques based on supervised learning. For instance, \citet{bright2013compressive} estimates the full state based on a library containing the reduced order information from the full flowfield. However, there might be difficulties in constructing such a library as the entire flowfield might not be available in practical applications.


\subsection{Contribution of the present work}

The present work uses RL to discover control strategies of partially observable fluid flow environments {without access to the full flow-field/state measurements}. Fluid flow systems typically exhibit more complex sampling in higher dimensional observation space compared to other physical systems, necessitating a robust exploration strategy and rapid convergence in the optimisation process. To address these challenges, we employ off-policy-maximum entropy RL algorithms (SAC and TQC) that efficiently identify {nearly} optimal policies in the large action space inherent to fluid flow systems, especially for cases with partial measurements and observability.

We aim to achieve two objectives related to RL flow control for bluff body drag reduction problems. First, we aim to improve the RL control performance in a PM environment by reducing a POMDP problem to an MDP problem. More details about this method are introduced in 
\S\ref{subsec:PM_Dynamic}. Second, we present investigations on different reward functions and key hyperparameters to develop an approach that can be adapted to a broader range of flow control applications. We demonstrate the proposed framework and its capability to discover {nearly} optimal feedback control strategies in the benchmark laminar flow of a square 2D bluff body with fixed separation at the trailing edge, using sensors only on the {downstream surface} of the body. 

The article is structured as follows. In Section \S\ref{sec:Method}, the RL framework is presented, which consists of the SAC and TQC optimisation algorithms interacting with the flow simulation environment. A hyperparameter-free reward function is proposed to optimise the energy efficiency of the dynamically controlled system. Exploiting past action-state information converts the POMDP problem in a PM environment to an MDP, enabling the discovery of nearly optimal policies.
Results are presented and discussed in Section \S\ref{sec:Results}. The convergence study of RL is first introduced. The degradation of RL control performance in PM environments (POMDP) is presented, and the improvement is addressed by exploiting a sequence of past action-measurement information. At the end of this section, we compare the results from TQC with SAC, addressing the advantages of using TQC as an improved version of SAC.
In Section \S\ref{sec:Conclusions}, we provide conclusions for the current research and discuss future research directions. 