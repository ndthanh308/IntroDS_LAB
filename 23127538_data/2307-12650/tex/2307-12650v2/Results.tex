\section{Results of RL active flow control}\label{sec:Results}

In this section, we discuss the converge of the RL algorithms for the three FM and PM cases (\S\ref{subsec:Convergence}) and evaluate their drag reduction performance (\S\ref{Result_drag_reduction}). A parametric analysis of the effect of NARX memory length is presented (\S\ref{subsec:Nfs}) and the isolated effect of including past actions as observations during the RL training and control (\S\ref{subsec:past_actions}). Studies of reward function (\S\ref{subsec:Rewards_Study}), sensor placement (\S\ref{subsec:Sensor_study}) and generalisability to Reynolds number changes (\S\ref{subsec:Res}) are presented, followed by a comparison of SAC and TQC algorithms (\S\ref{subsec:SACvsTQC}). 

\subsection{Convergence of learning}\label{subsec:Convergence}

We perform RL with the maximum entropy TQC algorithm to discover control policies for the three cases shown in figure \ref{fig:Case_Demo}, which maximise the net-power-saving reward function given by \req{eq: PowerR}. During the learning stage, each episode (1 DNS simulation) corresponds to $200$ non-dimensional time units.  To accelerate learning, $65$ environments run in parallel.


Figure \ref{fig:Learning_Curve} shows the learning curves of the three cases.  Table \ref{tab:LearningConvergence} shows the number of episodes needed for convergence and relevant parameters for each case.
It can be observed from the curve of episode reward that the RL agent is updated after every 65 episodes, i.e. $1$ iteration, where the episode reward is defined as 
\begin{equation}
R_{ep} = \sum_{k=1}^{N_k} r_{k},
\label{eq:Epi_R}
\end{equation}
where $k$ denotes the $k^{th}$ RL step in one episode and $N_k$ is the total number of samples in one episode.
The root mean square (RMS) value of the drag coefficient, $C_D^{RMS}$, at the asymptotic regime of control, is also shown to demonstrate convergence, defined as 
$C_D^{RMS} = \sqrt { (\mathcal{D}(\langle C_D\rangle_{env}))^2 }$,
where the operator $\mathcal{D}$ detrends the signal with a $9^{th}$-order polynomial and removes the transient part, and $\langle ~ \rangle_{env}$ denotes the average value of parallel environments in a single iteration. 

% Figure environment removed

\begin{table}
  \begin{center}
\def~{\hphantom{0}}
  \begin{tabular}{lcccccc}
    
      Environment  & Algorithm  &  $N_{c}$ & $R_{ep,c}$ & (Layers, Neurons) & $N_{fs}$ & Number of Inputs \\ 
       FM-Static   & TQC & $325$ & $37.72$ & (3,512) & $0$ & $64p_t+2a_{t-1}$\\
       PM-Static   & TQC & $1235$ & $21.87$ & (3,512) & $0$ & $64p_t+2a_{t-1}$\\
       PM-Dynamic  & TQC & $715$ & $34.35$ & (3,512) & $27$ & $N_{fs} (64p_t+2a_{t-1})$\\
  \end{tabular}
  \caption{Number of episodes $N_{c}$ required for RL convergence in different environments. The episode reward $R_{ep,c}$ at the convergence point, the configuration of NN and the dimension of inputs are presented for each case. $N_{fs}$ is the finite-horizon length of past actions-measurements.}
  \label{tab:LearningConvergence}
  \end{center}
\end{table}

In figure \ref{fig:Learning_Curve}, it can be noticed that in the FM environment, RL converges after approximately $325$ episodes ($5$ iterations) to a   {nearly} optimal policy using a static   {feedback} controller. As will be shown in \S\ref{Result_drag_reduction}, this policy is globally optimal since the vortex shedding is fully attenuated and the jets converge to zero mass flow actuation, thus recovering the unstable base flow and the minimum drag state.  However, with the same static   {feedback} controller in a PM environment (POMDP), the RL agent fails to discover the   {nearly} optimal solution, requiring around $1235$ episodes for convergence but only obtaining a relatively low episode reward.
Introducing a dynamic   {feedback} controller in the PM environment, the RL agent convergences to a near-optimal solution in 735 episodes. The dynamic   {feedback} controller trained by RL achieves a higher episode reward (34.35) than the static   {feedback} controller in the PM case (21.87), which is close to the FM case (37.72). The learning curves illustrate that using a finite horizon of past actions-measurements ($N_{fs} = 27$) to train a dynamic   {feedback} controller in the PM case improves learning in terms of speed of convergence and accumulated reward achieving nearly optimal performance with only wall pressure measurements. 


\subsection{Drag reduction with dynamic RL controllers} \label{Result_drag_reduction}

% Figure environment removed

The trained controllers for the cases shown in figure \ref{fig:Case_Demo} are evaluated to obtain the results shown in figure \ref{fig:TQC_FMPM}.   {Evaluation tests are performed for 120 non-dimensional time units to show both transient and asymptotic dynamics of the closed-loop system.}
Control is applied at $t=0$ with the same initial condition for each case, i.e. steady vortex shedding with average drag coefficient $\langle C_{D0}\rangle \approx 1.45$ (baseline without control). Consistent with the learning curves, the difference in control performance in the three cases can be observed both from the drag coefficient $C_D$ and the actuation $Q_1$.
  {The drag reduction is quantified by a ratio $\eta$ using the asymptotic time-averaged drag coefficient with control $C_{Da} = \langle C_{D}\rangle_{t \in [80,120]}$, the drag coefficient $C_{Db}$ of the base flow (details presented in Appendix \ref{App:BaseFlow}), and the baseline time-averaged drag coefficient without control $\langle C_{D0}\rangle$, as
\begin{equation}
\eta = \frac{\langle C_{D0}\rangle - C_{Da}}{\langle C_{D0}\rangle - C_{Db}} \times 100\%.
\label{eq:drag_reduction}
\end{equation}}

\begin{itemize}

\item {\bf FM-Static:} With a static   {feedback} controller trained in a full-measurement environment, a drag reduction of $\eta = 101.96\%$ is obtained with respect to the base flow (steady unstable fixed point; maximum drag reduction). This indicates that an RL controller informed with full-state information can entirely stabilise the vortex shedding and cancel the unsteady part of the pressure drag.

\item {\bf PM-Static:} A static/memoryless controller in a partial-measurement environment leads to performance degradation and a drag reduction of   {$\eta = 56.00\%$} in the asymptotic control stage, i.e. after $t=80$, compared to the performance of ``FM-Static''. This performance loss can also be observed from the control actuation curve, as $Q_1$ oscillates with a relatively large fluctuation in ``PM-Static'' while it stays about zero in the ``FM-Static'' case. 
The discrepancy between FM and PM environments using a static   {feedback} controller reveals the challenge of designing a controller with a POMDP environment. The RL agent cannot fully identify the dominant dynamics with only partial measurements on the   {downstream} surface of the bluff body, resulting in sub-optimal control behaviour.

\item{\bf PM-Dynamic:} With a dynamic   {feedback} controller (NARX model presented in \S\ref{subsec:PM_Dynamic}) in a partial-measurement environment, the vortex shedding is stabilised and the dynamic   {feedback} controller achieves   {$\eta = 97.00\%$} of the maximum drag reduction after time $t=60$. Although there are minor fluctuations in the actuation $Q_1$, the energy spent in the synthetic jets is significantly lower compared to the ``PM-Static'' case. Thus, a dynamic   {feedback} controller in PM environments can achieve nearly optimal drag reduction, even if the RL agent only collects information from pressure sensors on the   {downstream} surface of the body. The improvement in control indicates that the POMDP due to the PM condition of the sensors can be reduced to an approximate MDP by training a dynamic   {feedback} controller with a finite horizon of past actions-measurements. Furthermore, high-frequency action oscillations, which can be amplified with static   {feedback} controllers, are attenuated in the case of dynamic   {feedback} control. These encouraging and unexpected results support the effectiveness and robustness of model-free RL control in practical flow control applications, in which sensors can only be placed on a solid surface/wall.

\end{itemize}


% Figure environment removed

In figure \ref{fig:Contour}, snapshots of the velocity magnitude   {$|\boldsymbol{u}| = \sqrt{u^2+v^2}$} are presented for ``Baseline'' without control, ``PM-Static'', ``PM-Dynamic'' and ``FM-Static'' control cases. Snapshots are captured at $t=100$ in the asymptotic regime of control. A vortex-shedding structure of different strengths can be observed in the wake of all three controlled cases. In ``PM-Static'', the recirculation area is lengthened compared to the baseline flow, corresponding to base pressure recovery and pressure drag reduction. A longer recirculation area can be noticed in ``PM-Dynamic'' due to the enhanced attenuation of vortex shedding and pressure drag reduction. The dynamic   {feedback} controller in the PM case renders a $326.22\%$ increase of recirculation area with respect to the baseline flow, while only a $116.78\%$ increase is achieved by a static   {feedback} controller. The ``FM-Static'' case has the longest recirculation area, and the vortex shedding is almost fully stabilised, which is consistent with the drag reduction shown in figure \ref{fig:TQC_FMPM}.

% Figure environment removed

Figure \ref{fig:Obs} presents first- and second-order base pressure statistics for the baseline case without control and PM cases with control. In figure \ref{fig:Obs}(a), the time-averaged value of base pressure, $\overline{p}$, demonstrates the base pressure recovery after control is applied. Due to flow separation and recirculation, the time-averaged base pressure is higher at the middle of the   {downstream surface}, which is retained with control. The base pressure increase is directly linked to pressure drag reduction, which quantifies the control performance of both static and dynamic   {feedback} controllers. Up to $49.56\%$ of pressure increase at the centre of the   {downstream surface}  is obtained in the ``PM-Dynamic'' case, while only $21.15\%$ can be achieved by a static   {feedback} controller. In figure \ref{fig:Obs}(b), the base pressure RMS is shown. For the baseline flow, strong vortex-induced fluctuations of the base pressure can be noticed around the top and bottom   {on the downstream surface} of the bluff body. In the ``PM-Static'' case, the RL controller   {partially suppresses} the vortex shedding, leading to a sub-optimal reduction of the pressure fluctuation. The sensors close to the top and bottom corners are also affected by the synthetic jets, which change the RMS trend for the two top and bottom measurements. In the ``PM-Dynamic'' case,  the pressure fluctuations are nearly zero for all the measurements on the   {downstream surface}, highlighting the success of vortex shedding suppression by a dynamic RL controller in a PM environment.

% Figure environment removed

The differences between static and dynamic controllers in PM environments are further elucidated in figure \ref{fig:Action_analysis} by examining  the time series of pressure differences $\Delta p_t$ from surface sensors (control input) and control actions $a_{t-1}$ (output). The pressure differences are calculated from sensor pairs at $y=\pm y_{sensor}$, where $y_{sensor}$ is defined in Eq. \req{eq:Probe_base}. For $N=64$, there are 32 time series of $\Delta p_t$ for each case. 
%
During  the initial stages of control ($t \in [0,11]$), the control actions are similar  for the two PM cases and they deviate for $t>11$, resulting in discernible control performance at the asymptotic regime. 
At the initial stages, the controllers operate in nearly anti-phase to $\Delta p_t$, in order to eliminate the antisymmetric pressure component due to vortex shedding. The inability of the static controller to have a frequency dependent amplitude (and phase), manifests as well through the amplification of high frequency noise. For $t>11$, the static feedback controller continues to operate in nearly anti-phase to the pressure difference, resulting in partial stabilisation of unsteadiness. However, the dynamic feedback controller adjusts its phase and amplitude significantly, which attenuates the antisymmetric fluctuation of base pressure and drives $\Delta p_t$ to near zero. 

% Figure environment removed

Figure \ref{fig:ContourComparision} shows instantaneous vorticity contours for PM-Dynamic and PM-Static cases, showing both the similarities and discrepancies between the two cases. At $t=2$, flow is expelled from the bottom jet for both cases, generating a clockwise vortex, termed V1. This V1 vortex, shown in black, works against the primary counter-clockwise vortex labelled as P1, depicted in red, emerging from the bottom surface. At $t=5.5$, a secondary vortex, V2, forms from the jets to oppose the primary vortex shedding from the top surface (labelled as P2). 
%
 At $t=13$, the suppression of the two primary vortices near the bluff body is evident in both cases, indicated by their less tilted shapes compared to the previous time instances. At $t=13$, the PM-Dynamic adjusted the phase of the control signal, which corresponds to a marginal action at this time instance at figure \ref{fig:Action_analysis}. Consequently, no additional counteracting vortex is formed in PM-Dynamic. However, in the PM-Static scenario, the jets generate a third vortex, labelled V3, which emerges from the top surface. This corresponds to a peak in the action of the PM-Static controller at this time. The inability of the PM-Static controller to adapt the amplitude/phase of the input/output behaviour results in suboptimal performance.

\subsection{Horizon of the finite-history sufficient statistic}\label{subsec:Nfs}

A parametric study on the horizon of the finite history in NARX (equation \req{eq:NARX}), i.e. the number of frames stacked $N_{fs}$, is presented in this section. Since the NARX model uses a finite horizon of past actions-measurements in  \req{eq:Sufficient_statistic}, the horizon of the finite history affects the convergence of the approximation \citep{yu_near_2008}. This approximation affects the optimisation during the learning of RL because it determines whether the RL agent can observe sufficient information to converge to an optimal policy. 

Since vortex shedding is the dominant instability to be controlled, the choice of $N_{fs}$ should intuitively link to the timescale of the vortex shedding period. The ``frames'' of observations are obtained every RL step ($0.5$ time units), while the vortex shedding period is $t_{vs}\approx6.85$ time units. Thus, $N_{fs}$ is rounded to integer values for different numbers of vortex shedding periods, as shown in table \ref{tab:Frame_Stack}.


% Figure environment removed

\begin{table}
  \setlength{\tabcolsep}{12pt}
  \begin{center}
\def~{\hphantom{0}}
  \begin{tabular}{ccc}
      Number of  & Non-dimensional &  History length \\
      VS periods &    time units          &  ($N_{fs}$)         \\ [3pt]
      \hline
       0.5   & 3.43 & 7 \\
       1   & 6.85 & 14 \\
       2  & 13.70 & 27 \\
       3 & 20.55 & 41\\
       4 & 27.40 & 55\\
       5 & 34.25 & 68\\
  \end{tabular}
  \caption{Correspondence between the number of vortex shedding (VS) periods and frame stack (history) length in samples $N_{fs}$. The RL control step size is $t_a =0.5$, and $N_{fs}$ is rounded to an integer.}
  \label{tab:Frame_Stack}
  \end{center}
\end{table}

The results of time-averaged drag coefficients $\langle C_{D}\rangle$ after control and the average episode rewards $\langle R_{ep}\rangle$ in the final stage of training are presented in figure \ref{fig:Frame_Stack}. As $N_{fs}$ increases from 0 to 27, the performance of RL control improves, resulting in a lower $\langle C_{D}\rangle$ and a higher $\langle R_{ep}\rangle$. $N_{fs}=2$ is specially examined because the latent dimension of the vortex shedding limit cycle is 2. However, the control performance with $N_{fs}=2$ is marginally improved to the one with $N_{fs}=0$, i.e. a static   {feedback} controller. This result indicates that the horizon consistent with the vortex shedding dimension is not long enough for the finite horizon of past action measurements. The optimal history length to achieve stabilisation of the vortex shedding   {in PM environments} is 27 samples, which are equivalent to 13.5 convective time units or $\sim 2$ vortex shedding periods. 

With $N_{fs}=41$ and $N_{fs}=55$, the drag reduction and episode rewards drop slightly compared to $N_{fs}=27$. The decline in performance is non-negligible as $N_{fs}$ increases further to 68. This decline shows that excessive inputs to the neural networks (see table \ref{tab:LearningConvergence}), may impede training because more parameters need to be tuned or larger neural networks need to be trained. 

\subsection{Observation sequence with past actions}\label{subsec:past_actions}

Past actions (exogenous terms in NARX) facilitate reducing a POMDP to an MDP problem, as discussed in \S\ref{subsec:PM_Dynamic}. In the near-optimal control of a PM environment using a dynamic   {feedback} controller with inputs $\left( o_t, o_{t-1}, ..., o_{t-N_{fs}} \right)$, a sequence of observations $o_t = \left \{ p_t, a_{t-1}\right \}$ at step $t$ is constructed to include pressure measurements and actions. In the FM environment, due to the introduction of one-step delayed action due to the first-order-hold interpolation given by \req{eq:FOH_action}, the inclusion of the past action along with the current pressure measurement, meaning $o_t = \left \{ p_t, a_{t-1} \right \}$, is required even when the sensors are placed in the wake and cover the wavemaker region. 

Figure \ref{fig:ActionInObs} presents the control performance for the same environment with and without past actions included.
In the FM case, there is no apparent difference between RL control with $o_t = \left \{ p_t, a_{t-1} \right \}$ or $o_t = \left \{ p_t \right \}$, which indicates that the inclusion of the past action is negligible to the performance. This is the case when the RL sampling frequency is sufficiently faster than the timescale of the vortex shedding dynamics. 
In PM cases, if exogenous action terms are not included in the observations but only the finite history of pressure measurements is used, the RL control fails to converge to a near-optimal policy, with only   {$\eta = 67.45\%$}  drag reduction. With past actions included, the drag reduction of the same environment increases up to   {$\eta = 97.00\%$}. 

The above results show that in PM environments, sufficient statistics cannot be constructed only from the finite history of measurements. Missing state information needs to be reconstructed by both state-related measurements and control actions. 

% Figure environment removed

\subsection{Reward study}
\label{subsec:Rewards_Study}

In \S\ref{Result_drag_reduction}, a power-based reward function given by \req{eq: PowerR} has been implemented, and stabilising controllers can be learned by RL, as shown. In this section, RL control results with other forms of reward functions (introduced in \S\ref{subsec:Reward}) are provided and discussed.

% Figure environment removed

The control performance of RL control with the different reward functions is evaluated based on the drag coefficient $C_D$ shown in figure \ref{fig:Reward_Study}. Static   {feedback} controllers are trained in FM environments, and dynamic   {feedback} controllers are trained in PM environments. In FM cases, control performance is not sensitive to the choice of reward function (power or force-based).  
In PM cases, the discrepancies between RL-step time-averaged and instantaneous rewards can be observed in the asymptotic regime of control. The controllers with both rewards (power or force-based) achieve nearly optimal control performance, but there is some unsteadiness in the cases using instantaneous rewards due to slow statistical convergence of the rewards and limited correlation to the partial observations.

All four types of reward functions studied in this work achieve nearly optimal drag reduction around $100\%$. However, the energy-based reward (``PowerR'') offers an intuitive reward design, attributable to its physical properties and the dimensionally consistent addition of the constituent terms of the reward function. Further enhancing its practicality, since the power of the actuator can be directly measured, it avoids the necessity for hyperparameter tuning, as in the force-based reward. Additionally, the results show similar performance with both time-averaged between RL steps and instantaneous rewards, avoiding the necessity for faster sampling for the calculation of the rewards. This choice of reward function can be extended to various RL flow control problems and can be beneficial to experimental studies.


\subsection{Sensor configuration study with partial measurements}\label{subsec:Sensor_study}

% Figure environment removed

In the PM environment, the configuration of sensors (number and location on the downstream surface) may also affect the information contained in the observations and thus control performance. 
Control results of drag coefficient $C_D$ for different sensor configurations in PM-dynamic cases are presented in figure \ref{fig:Sensor_config}. In the configuration with $N = 2$, two sensors are placed at $y=\pm 0.25$, and for $N = 1$, only one sensor is placed at $y = 0.25$. Other configurations are consistent with equation \req{eq:Probe_base}. 

The $C_D$ curves in figure \ref{fig:Sensor_config} show that, as the number of sensors is reduced from 64 to 2, RL control achieves the same level of performance with minor discrepancies due to randomness in different learning cases. However, if RL control uses observations from only one sensor at $y = 0.25$, performance degradation can be observed in the asymptotic stage with 19.79\% on average less drag reduction. The sub-figure presents the relationship between the number of sensors and asymptotic drag coefficient $\langle C_D \rangle$. These results indicate a limit on sensor configuration for the use of the NARX-modeled controller to stabilise the vortex shedding. 

% Figure environment removed

To understand the cause of performance degradation in the $N=1$ case, the pressure measurements from two sensors in both baseline and PM-Dynamic cases are presented in figure \ref{fig:Pressure2Sensors}. In the baseline case, two sensors are placed at the same location as the $N=2$ case ($y=\pm 0.25$) only for observations. It can be observed that the pressure measurements from two sensors are anti-symmetric since they are placed symmetrically on the downstream surface.
In the PM-Dynamic case, the NARX controller is used, and control is applied at $t=0$. In this closed-loop system, the anti-symmetric relationship between two sensors (from the symmetric position) is broken by the control actuation, and no correlation is evident. This can be seen during the transient dynamics, e.g. in $t \in [0,10]$. Therefore, when the number of sensors is reduced to $N=1$ by removing one sensor from the $N=2$ case, the dynamic feedback from the removed sensor cannot be fully reflected by the remaining sensor in the closed-loop system. This loss of information affects the fidelity of the control response to the dynamics of the sensor-removing side, causing suboptimal drag reduction in the $N=1$ scenario.

It should be noted that the configuration of 64 sensors is not necessary for control, as $N = 2$ or $N = 16$ also achieves nearly optimal performance. The number of sensors $N = 64$ in PM-Static environments is used for comparison with the FM-Static configuration (Eq. \ref{eq:Probe_wake}), which eliminates the effect from different input dimensions between two static cases. Also, 64 sensors sufficiently cover the downstream surface of the bluff body to avoid missing spatial information. 
The optimal configuration of sensors can be tuned with optimisation techniques such as \cite{paris_robust_2021}, but the results in figure \ref{fig:Sensor_config} indicate that RL adapts with nearly optimal performance to non-optimised sensor placement in the present environment.

\subsection{Performance of RL controllers to unseen $Re$} \label{subsec:Res}

% Figure environment removed

The RL controller is tested at different Reynolds numbers, in order to examine its generalisability to environment changes. The controllers have been trained at $Re=100$ with both FM and PM conditions, and tested at $Re= 80, 90, 100, 110, 120, 150$. The controllers were further trained at $Re=150$, denoted as continual learning (CL), and tested again at $Re=150$. 

As shown in figure \ref{fig:Res}, in both ``PM-Dynamic'' and ``FM-Static'' cases, the RL controllers are able to reduce drag by $\eta=64.68\%$ in the worst case, when $Re$ is close to the training point at $Re=100$, i.e. the test cases with $Re= 80, 90, 100, 110, 120$. 
However, when applying the controllers trained at $Re=100$ to an environment at $Re=150$, the drag reduction drops to $\eta=41.98\%$ and $\eta = 74.04\%$ in PM-Dynamic and FM-Static cases, respectively.

Performing CL at $Re=150$, the drag reduction is improved to $\eta = 78.07\%$ in PM-Dynamic after 1105 training episodes while $\eta = 88.13\%$ in FM-Static after 390 episodes, with the same RL parameters as the training at $Re=100$.
Overall, the results of these tests indicate that the RL-trained controllers can achieve significant drag reduction in the vicinity of the training point (i.e. $\pm\%20$ $Re$ change). If the test point is far from the training point, a CL procedure can be implemented to achieve nearly optimal control.

\subsection{TQC vs SAC}\label{subsec:SACvsTQC}

% Figure environment removed

Control results with TQC and SAC are presented in figure \ref{fig:TQCvsSAC} in terms of $C_D$. TQC shows a more robust control performance. In the case of FM, SAC might demonstrate a slightly more stable transient behaviour attributed to the fact that the quantile regression process in TQC introduced complexity to the optimisation process. Both controllers achieved an identical level of drag reduction in the FM case. 

However, in the context of the PM cases, it is observed that TQC outperforms SAC in drag reduction with both static and dynamic   {feedback} controllers. For static   {feedback} control, TQC achieved an average drag reduction of   {$\eta = 56.00\%$}, compared to the   {$\eta = 46.31\%$}  reduction achieved by SAC. The performance under dynamic   {feedback} control conditions is more compelling, where TQC fully reduced the drag, achieving   {$\eta = 97.00\%$}  of drag reduction, reverting it to a near-base-flow scenario. In contrast, SAC managed to achieve an average drag reduction of   {$\eta = 96.52\%$}.

The fundamental mechanism for updating Q-functions in RL involves selecting the maximum expected Q-functions among possible future actions. This process, however, can potentially lead to overestimation of certain Q-functions \citep{hasselt_double_2010}. In POMDP, this overestimation bias might be exacerbated due to the inherent uncertainty arising from the partial-state information. Therefore, the Q-learning-based algorithm, when applied to POMDPs, might be more prone to choosing these overestimated values, thereby affecting the overall learning and decision-making process.

As mentioned in \S\ref{subsec:SACTQC}, the core benefit of TQC under these conditions can be attributed to its advanced handling of the overestimation bias of rewards. By constructing a more accurate representation of possible returns, TQC provides a more accurate Q-function approximation than SAC. This process of modulating the probability distribution of the Q-function assists TQC in managing the uncertainties inherent in environments with only partial-state information. In this case, TQC can adapt more robustly to changes and uncertainties, leading to better performance in both static and dynamic feedback control tasks.