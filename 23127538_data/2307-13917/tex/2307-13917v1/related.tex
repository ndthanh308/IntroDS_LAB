\section{Related Work}
\label{appsec: related work}
Bayesian causal discovery literature has primarily focused on inference in linear models with closed-form posteriors or marginalized parameters. Early works considered sampling directed acyclic graphs (DAGs) for discrete~\cite{cooper1992bayesian, madigan1995bayesian, heckerman2006bayesian} and Gaussian random variables~\cite{friedman2003being, tong2001active} using Markov chain Monte Carlo (MCMC) in the DAG space. However, these approaches exhibit slow mixing and convergence~\cite{eaton2012bayesian,grzegorczyk2008improving}, often requiring restrictions on number of parents~\cite{kuipers2017partition}. %Alternative exact dynamic programming methods are limited to small settings~\cite{koivisto2012advances}. 

Recent advances in variational inference~\cite{zhang2018advances} have facilitated graph inference in DAG space, with gradient-based methods employing the NOTEARS DAG penalty \cite{zheng2018dags}.\cite{annadani2021variational} samples DAGs from autoregressive adjacency matrix distributions, while \cite{lorch2021dibs} utilizes Stein variational approach \cite{liu2016stein} for DAGs and causal model parameters. \cite{cundy2021bcd} proposed a variational inference framework on node orderings using the gumbel-sinkhorn gradient estimator \cite{mena2018learning}. \cite{deleu2022bayesian,nishikawa2022bayesian} employ the GFlowNet framework \cite{bengio2021gflownet} for inferring the DAG posterior. Most methods, except\cite{lorch2021dibs} are restricted to linear models, while \cite{lorch2021dibs} has high computational costs and lacks DAG generation guarantees compared to our method.
% at least quadratic scaling complexity, both with respect to the number of nodes (due to the DAG penalty) as well as number of posterior samples. Our proposed approach instead has linear complexity with respect to number of posterior samples and does not require any additional DAG penalty.     

In contrast, \emph{quasi-Bayesian} methods, such as DAG bootstrap \cite{friedman2013data}, demonstrate competitive performance. DAG bootstrap resamples data and estimates a single DAG using PC \cite{spirtes2000causation}, GES \cite{chickering2002optimal}, or similar algorithms, weighting the obtained DAGs by their unnormalized posterior probabilities. Recent neural network-based works employ variational inference to learn DAG distributions and point estimates for nonlinear model parameters \cite{charpentier2022differentiable,geffner2022deep}.