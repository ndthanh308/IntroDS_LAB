\subsection{Bayesian Inference in $W,p$ Space}
\label{subsec: Bayesian inference W p space}
The NoCurl formulation (\cref{eq: NoCurl mapping}) focuses on learning \emph{a single weighted} DAG, which is not directly useful for our purpose. We need to address two key questions: (1) considering only binary adjacency matrices without weights; (2) ensuring Bayesian inference in $(\mW,\vp)$ is valid.

First, we introduce a modification $\tau:\binaryset\times\sR^d\rightarrow \binaryset$:
\begin{equation}
\tau(\mW,\vp) = \mW\odot \step(\grad \vp)
\label{eq: Binary NoCurl}
\end{equation}
where we abuse the term $\mW$ for binary matrices, and replace $\relu(\cdot)$ with $\step(\cdot)$. $\mW$ acts as mask to disable the edge existence. Thus, due to the $\step$, $\tau$ can only output a binary adjacency matrix.

Next, we show that performing Bayesian inference in such augmented $(\mW,\vp)$ space is valid, i.e., using the posterior $p(\mW,\vp\vert \mD)$ to replace $p(\mG\vert \mD)$. This differs from NoCurl, which focuses on a single graph rather than the validity for Bayesian inference, requiring a new theory for soundness.

\begin{theorem}[Equivalence of inference in $(\mW,\vp)$ and binary DAG space]
Assume graph $\mG$ is a binary adjacency matrix representing a DAG and node potential $\vp$ does not contain the same values, i.e.~$p_i\neq p_j$ $\forall i,j$. Then, with the induced joint observational distribution $p(\mD,\mG)$, dataset $\mD$, and a corresponding prior $p(\mG)$, we have
\begin{align}
p(\mG\vert \mD) = \int p_\tau(\vp,\mW\vert \mD)\indicator(\mG=\tau(\mW,\vp))d\mW d\vp
\label{eq: equivalence of bayesian inference}
\end{align}
if $p(\mG)=\int p_\tau(\vp,\mW)\indicator(\mG=\tau(\mW,\vp))d\mW d\vp$, where $p_\tau(\mW,\vp)$ is the prior, $\indicator(\cdot)$ is the indicator function, and $p_\tau(\vp,\mW\vert D)$ is the posterior distribution over $\vp,\mW$.
\label{thm: equivalence of bayesian inference}
\end{theorem}

Refer to \cref{subapp: proof of equivalence of bayesian inference} for detailed proof.


This theorem guarantees that instead of performing inference directly in the constrained space (i.e.~DAG space), we can apply Bayesian inference in a less complex $(\mW,\vp)$ space where $\mW\in\binaryset$ and $\vp\in\sR^d$ without explicit constraints. 


For inference of $\vp$, we adopt a sampling-based approach, which is asymptotically accurate \cite{ma2015complete}. In particular, we consider SG-MCMC (refer to \cref{sec: SGMCMC sampling framework}), which avoids the expensive Metropolis-Hastings acceptance step and scales to large datasets. We emphasize that any other suitable sampling algorithms can be directly plugged in, thanks to the generality of the framework.  


However, the mapping $\tau$ does not provide meaningful gradient information for $\vp$ due to the piecewise constant $\step(\cdot)$ function, which is required by SG-MCMC.

% However, one major problem with this mapping $\tau$ is that it does not provide meaningful gradient information for $\vp$ due to the piecewise constant $\step(\cdot)$, which is required by SG-MCMC methods.



