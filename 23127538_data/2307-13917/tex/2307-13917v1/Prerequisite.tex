\section{Background}
\label{sec: Prerequisite}
% In this section, we provide a brief overview of the necessary prerequisite knowledge. Specifically, we focus on fundamental terminologies, including the causal graph and structural causal model (SCM), the Bayesian causal discovery formulation \cite{friedman2003being}, and a gentle introduction to the NoCurl \cite{yu2021dags} characterization of the causal graph.


\paragraph{Causal Graph and Structural Causal Model}
Consider a data generation process with $d$ variables $\mX\in\sR^{d}$. The causal relationships among these variables is  represented by a Structural Causal Model (SCM) which consists of a set of structural equations~\cite{peters2017elements} where each variable $X_i$ is a function of its direct causes $\mX_{\Pa^i}$ and an exogenous noise variable $\epsilon_i$ with distribution $P_{\epsilon_i}$: 
\begin{equation}
X_i\coloneqq f_i(\mX_{\Pa^i},\epsilon_i)
\label{eq: CM}
\end{equation}
These equations induce a 
causal graph $\mG=(\mV,\mE)$, comprising a node set $\mV$ with $\vert\mV\vert=d$ indexing the variables $\mX$ and a directed edge set $\mE$. If a directed edge $e_{ij}\in\mE$ exists between a node pair $v_i,v_j\in\mV$ (i.e., $v_i\rightarrow v_j$), we say that $X_i$ causes $X_j$ or $X_i$ is the parent of $X_j$. We use the binary adjacency matrix $\mG\in\binaryset$ to represent the causal graph, where the entry $G_{ij}=1$ denotes $v_i \rightarrow v_j$. %Given $\mG$, the functional relationship between variables can be described using a structural causal model (SCM) defined as
A standard assumption in causality is that the structural assignments are acyclic and the induced causal graph is a DAG~\cite{bongers2021foundations, pearl2009causality}, which we adopt in this work.
We further assume that the SCM is causally sufficient i.e. all variables are measurable and exogenous noise variables $\epsilon_i$ are mutually independent. Throughout this work, we consider a special form of SCM called Gaussian additive noise model (ANM):
\begin{equation}
X_i\coloneqq f_i(\mX_{\Pa^i}) + \epsilon_i~~~~~~\text{where}~~~\epsilon_i\sim\mathcal{N}(0,\sigma^2_i)
\label{eq: ANM}
\end{equation}
% We refer to \cref{subsect: model formulation} for a detailed model formulation of \ModelName{}.
If the functions are not linear or constant in any of its arguments, the Gaussian ANM is structurally identifiable~\cite{hoyer2008nonlinear,peters2014causal}.

\paragraph{Bayesian Causal Discovery}
Given a dataset $\mD=\{\vx^{(1)},\ldots,\vx^{(N)}\}$ with i.i.d observations, underlying graph $\mG$ and SCM parameters $\mTheta$%\footnote{In this work, $\mTheta$ corresponds to the parameters of functions $f_i$ and the variance of Gaussian exogenous noise variables $P_{\epsilon_i}=\mathcal{N}(0,\sigma^2_i)$.}
, they induce a unique joint distribution $p(\mD,\mTheta, \mG) = p(\mD\vert \mG,\mTheta)p(\mG,\mTheta)$ with the prior $p(\mG,\mTheta)$ and likelihood $p(\mD\vert \mG,\mTheta)$ \cite{friedman2003being}. Under finite data and/or limited identifiability of SCM (e.g upto MEC), it is desirable to have accurate uncertainty estimation for downstream decision making rather than inferring a single SCM and its graph (for e.g. with a maximum likelihood estimate). Bayesian causal discovery therefore aims to infer the posterior $p(\mG,\mTheta\vert \mD)=p(\mD,\mTheta, \mG)/p(\mD)$. However, this posterior is intractable due to the super-exponential growth of the possible DAGs $\mG$~\cite{robinson1973counting} and continuously valued model parameters $\mTheta$ in nonlinear functions. VI \cite{zhang2018advances} or SG-MCMC \cite{gong2022advances,ma2015complete} are two types of methods developed to tackle general Bayesian inference problems, but adaptations are required for Bayesian causal discovery. 

\paragraph{NoCurl Characterization}
Inferring causal graphs is challenging due to the DAG constraint. Previous works \cite{geffner2022deep, gong2022rhino,lachapelle2019gradient,lorch2021dibs,yu2019dag} directly infer adjacency matrix with the DAG regularizer~\cite{zheng2018dags}. However, it requires an annealing schedule, resulting in slow convergence, and no guarantees on generating DAGs. Recently, \cite{yu2021dags} introduced NoCurl, a novel characterization of the \textbf{weighted DAG} space. They define a potential $p_i\in \sR$ for each node $i$, grouped as potential vector $\vp\in\sR^d$ . Further, a gradient operator on $\vp$ mapping it to a skew-symmetric matrix is introduced:
\begin{equation}
    (\grad \vp)(i,j) = p_i-p_j
    \label{eq: grad operator}
\end{equation}
Based on the above operation, a mapping that directly maps from the augmented space $(\mW,\vp)$ to the DAG space $\gamma(\cdot,\cdot):\sR^{d\times d}\times \sR^{d}\rightarrow \sR^{d\times d}$ was proposed:
\begin{equation}
\gamma(\mW,\vp) = \mW\odot \relu(\grad \vp)
\label{eq: NoCurl mapping}
\end{equation}
where $\relu(\cdot)$ is the ReLU activation function and $\mW$ is a skew-symmetric \textbf{continuously weighted} matrix. This formulation is complete (Theorem 2.1 in \cite{yu2021dags}), as any continuously weighted DAG can be represented by a $(\mW,\vp)$ pair and vice versa.
NoCurl translates the learning of a single weighted DAG to a corresponding $(\mW,\vp)$ pair. However, direct gradient-based optimization is challenging due to a highly non-convex loss landscape, which leads to the reported failure in \cite{yu2021dags}. 

Although NoCurl appears suitable for our purpose, the failure in directly learning suggests non-trivial optimizations. We hypothesize that this arises from the continuously weighted matrix $\mW$. In the following, we introduce our proposed parametrization inspired by NoCurl to characterize the \textbf{binary} DAG adjacency matrix.




