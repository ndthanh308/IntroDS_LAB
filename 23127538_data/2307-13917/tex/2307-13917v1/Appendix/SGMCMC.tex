\section{SG-MCMC Update}
\label{app: SG-MCMC update}

Assume we want to draw samples $\vp\sim p(\vp\vert \mD,\mW,\mTheta)\propto \exp(-U(\vp,\mW,\mTheta))$ with $U(\vp,\mW,\mTheta)=-\log p(\vp,\mW,\mTheta)$, we can compute $U$ by
\begin{equation}
    U(\vp,\mW,\mTheta) = -\sum_{n=1}^N\log p(\vx_n|\mG=\tau(\mW,\vp),\mTheta) - \log p(\vp,\mW,\mTheta)
    \label{eq: def of U}
\end{equation}
In practice, we typically use mini-batches $\mathcal{S}$ instead of the entire dataset $\mD$. Therefore, an approximation is 
\begin{equation}
    \tilde{U}(\vp,\mW,\mTheta) = -\frac{\vert \mD\vert}{\vert \mathcal{S}\vert}\sum_{n\in\mathcal{S}} \log p(\vx_n\vert \mG=\tau(\mW,\vp),\mTheta) -\log p(\vp,\mW,\mTheta)
    \label{eq: def of minibatch U}
\end{equation}
where $\vert \mathcal{S}\vert$ and $\vert \mD\vert$ are the minibatch and dataset sizes, respectively.

\cite{gong2019icebreaker} uses the preconditioning technique on \emph{stochastic gradient Hamiltonian Monte Carlo}(SG-HMC), similar to the preconditioning technique in \cite{li2016preconditioned}. In particular, they use a moving-average approximation of diagonal Fisher information to adjust the momentum.
The transition dynamics at step $t$ with EM discretization is 
\begin{align}
    &B = \frac{1}{2}l \nonumber\\
    &\mV_t = \beta_2 \mV_{t-1} + (1-\beta_2)\nabla_{\vp}\tilde{U}(\vp,\mW,\mTheta)\odot \nabla_{\vp}\tilde{U}(\vp,\mW,\mTheta)\nonumber\\
    &g_t=\frac{1}{\sqrt{1+\sqrt{\mV_t}}}\nonumber\\
&\vr_t = \beta_1\vr_{t-1} - lg_t\nabla_{\vp}\tilde{U}(\vp,\mW,\mTheta) + l\frac{\partial g_t}{\partial \vp_t} + s\sqrt{2l(\frac{1-\beta_1}{l}-B)}\eta\nonumber \nonumber \\
&\vp_t=\vp_{t-1}+lg_{t}\vr_t
\label{eq: SGMCMC update rule}
\end{align}
where $l^2$ is the learning rate; $(\beta_1,\beta_2)$ controls the preconditioning decay rate, $\eta$ is the Gaussian noise with $0$ mean and unit variance, and $s$ is the hyperparameter controlling the level of injected noise to SG-MCMC.
Throughout the paper, we use $(\beta_1, \beta_2)= (0.9,0.99)$ for all experiments. 



