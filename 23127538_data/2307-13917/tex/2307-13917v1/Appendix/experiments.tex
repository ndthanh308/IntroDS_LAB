\section{Experimental Settings}
\label{app:experiments}
\subsection{Baselines}
\label{app:baselines}
 For all the experimental settings, we compare with the following baselines: 
 \begin{itemize}
     \item Bootstrap GES (\textbf{BGES})~\cite{friedman2013data,chickering2002optimal} is a bootstrap based quasi-Bayesian approach for linear Gaussian models which first resamples with replacement data points at random and then estimates a linear SCM using the GES algorithm~\cite{chickering2002optimal} for each bootstrap set. GES is a score based approach to learn a point estimate of a linear Gaussian SCM. For all the experimental settings, we use 50 bootstrap sets. 
     \item Differentiable DAG Sampling (\textbf{DDS}) is a VI based approach to learn distribution over DAGs and a point estimate over the nonlinear functional parameters. DDS performs inference on the node permutation matrices, thus directly generating DAGs. Gumbel-sinkhorn~\cite{mena2018learning} is used for obtaining valid gradients and Hungarian algorithm is used for the straight-through gradient estimator~\cite{bengio2013estimating}. In the author provided implementation, for evaluation, a single permutation matrix is sampled and the logits of the edge beliefs are directly thresholded. In this work, in order to make the comaprison fair to Bayesian learning methods, we directly sample the binary adjacency matrix based on the edge logits.
     \item \textbf{BCD} Nets~\cite{cundy2021bcd} is a VI based fully Bayesian structure learning approach for linear causal models. BCD performs inference on both the node permutations through the Gumbel-sinkhorn~\cite{mena2018learning} operator as well as the model parameters through a VI distribution. Both DDS and BCD nets operate directly on full rank initializations to the Gumbel-sinkhorn operator, unlike our rank-1 initialization, which saves computations in practice.
     \item  \textbf{DIBS}~\cite{lorch2021dibs} uses SVGD~\cite{liu2016stein} with the DAG regularizer \cite{zheng2018dags} and bilinear embeddings to perform inference over both linear and nonlinear causal models. As our data generation process involves SCM with unequal noise variance, we extend DIBS framework with an inference over noise variance using SVGD, similar to the original paper. 
 \end{itemize}
 While DIBS and DDS can handle nonlinear parameterization, approaches like BGES and BCD, which are primarily designed for linear models still give competitive results when applied on nonlinear data. Given that there are limited number of baselines in the nonlinear case, and DIBS being the only fully Bayesian nonlinear baseline, we compare with BGES and BCD for all settings despite their model misspecification.
\subsection{Evaluation Metrics}
\label{app:metrics}
For higher dimensional settings with nonlinear models, the true posterior is intractable. While in general it is hard to evaluate the posterior inference quality in high dimensions, prior work has suggested to evaluate on proxy metrics which we adopt in this work as well~\cite{lorch2021dibs,geffner2022deep,annadani2023differentiable}. In particular, we evaluate the following metrics:
\begin{itemize}
    \item \textbf{$\E$-SHD}: Structural Hamming Distance (SHD) measures the hamming distance between graphs. In particular, it is a measure of number of edges that are to be added, removed or reversed to get the ground truth from the estimated graph. Since we have a posterior distribution $q(\mG)$ over graphs, we measure the \emph{expected} SHD: 
    \begin{equation*}
    \E\text{-SHD} \coloneqq \E_{\mG\sim q(\mG)}[\mathrm{SHD}(\mG, \mG^{{GT}})] \approx \frac{1}{N_e}\sum_{i=1}^{N_e}[\mathrm{SHD}(\mG^{(i)}, \mG^{{GT}})]~~~~,\text{with}~~~\mG^{(i)}\sim q(\mG)
\end{equation*} where $\mG^{GT}$ is the ground-truth causal graph. 
\item \textbf{Edge F1}: It is F1 score of each edge being present or absent in comparison to the true edge set, averaged over all edges.
\item \textbf{NLL}: We also measure the negative log-likelihood of the held-out data, which is also typically used as a proxy metric on evaluating the posterior inference quality \cite{gong2018meta,ma2019variational,sun2019functional}.
\end{itemize} 
The first two metrics measure the goodness of the graph posterior while the NLL measures the goodness of the joint posterior over the entire causal model.

For $d=5$ with linear models (unequal noise variance, identifiable upto MEC~\cite{peters2014identifiability, hoyer2008nonlinear}), we evaluate the following metrics:
\begin{itemize}
    \item \textbf{MMD True Posterior:} Since the true posterior is tractable, we compare the approximation with the ground truth using a Maximum Mean Discrepancy (MMD) metric~\cite{gretton2012kernel}. If $\mathrm{P}\coloneqq p(\mG\mid\mD)$ is the marginalized true posterior over graphs and $\mathrm{Q}$ is the approximated posterior over graphs, then the MMD between these two distributions is defined as:
\begin{equation*}
    \mathrm{MMD}^2(\mathrm{P},\mathrm{Q}) = \E_{\mG\sim\mathrm{P}}[k(\mG,\mG)] + \E_{\mG\sim\mathrm{Q}}[k(\mG,\mG)] - 2\E_{\mG\sim\mathrm{P},\mG'\sim\mathrm{Q}}[k(\mG,\mG')]
\end{equation*}
where $k(\mG,\mG')=1-\frac{H(\mG,\mG')}{d^2}$ is the Hamming kernel, and $H$ is the Hamming distance between $\mG$ and $\mG'$. This requires just the samples from the true posterior and the model. 
For calculating the true posterior which involves marginalization of the model parameters, appropriate prior over these parameters are required. This is ensured by using BGe score~\cite{geiger2002parameter,kuipers2014addendum} which places a Gaussian Wishart prior on the parameters. This leads to closed form marginal likelihood which is distribution equivalent, i.e. all graphs within the MEC will have the same likelihood. In addition, due to the low dimensonality ($d=5$), we can enumerate all possible DAGs and compute the normalizing constant $p(\mD)$. We refer to \cite{geiger2002parameter} for details. This metric has been used in prior work~\cite{annadani2021variational}.
\item \textbf{$\E$-CPDAG SHD:} An MEC can be represented by a Completed Partially Directed Acyclic Graph (CPDAG)~\cite{peters2017elements} which contains both directed edges and arcs (undirected edges). When causal relations between certain set of variables can be established, a directed edge is present. If there is an association between a certain set of variables for which causal direction is not identifiable, an undirected edge is present. For any graph, it has a corresponding CPDAG associated to the MEC which it belongs to. Since the ground truth graph is identifiable only upto MEC, we compare the (structural) Hamming distance between the graph posterior and the CPDAG of the ground truth. This is done by computing the $\E$-CPDAG SHD:
\begin{equation*}
    \E\text{-CPDAG SHD} \coloneqq \E_{\mG\sim q(\mG)}[\mathrm{SHD}(\mG_{\text{CPDAG}}, \mG_{\text{CPDAG}}^{{GT}})] \approx \frac{1}{N_e}\sum_{i=1}^{N_e}[\mathrm{SHD}(\mG_{\text{CPDAG}}^{(i)}, \mG_{\text{CPDAG}}^{{GT}})]
\end{equation*}
with $\mG^{(i)}\sim q(\mG)$ and $\mG_{CPDAG}^{GT}$ is the ground-truth CPDAG.
\end{itemize}


\subsection{Synthetic Data}
\label{app:synthetic_data}
As knowledge of ground truth graph is not possible in many real world settings, it is standard across causal discovery to benchmark in synthetic data settings. Following prior work, we generate synthetic data by first sampling a DAG at random from either Erdos-RÃ¨nyi (ER)~\cite{erdHos1960evolution} or Scale-Free (SF)~\cite{barabasi1999emergence} family. For $d=5$, we ensure that the graphs have $d$ edges in expectation and $2d$ edges for $d>5$. The ground truth parameters for linear functions are drawn at random from a fixed range of $[0.5, 1.5]$. For nonlinear models, the nonlinear functions are defined by randomly initialized Multi-Layer Perceptrons (MLP) with a single hidden layer of 5 nodes and ReLU nonlinearity. The variance of the exogenous Gaussian noise variable is drawn from an Inverse Gamma prior with concentration $\alpha=1.5$ and rate $\beta=1$. For $d=5$ linear case, we sample at random $N=500$ samples from the SCM for training and $N=100$ for held-out evaluation. For higher dimensional settings, we consider $N=5000$ random samples for training and $N=1000$ samples for held-out evaluation. For all settings, we evaluate on 30 random datasets.

\subsection{Hyperparameter Selection}
\label{appsubsec: hyperparameter selection}
In this section, we will give the details our how to select the hyperparameters for our method and all the baseline models. 

 We employ a cross-validation-like procedure for hyperparameter tuning in BayesDAG and DIBS to optimize MMD true posterior (for $d=5$ linear setting) and $\mathbb{E}-$SHD value (for nonlinear setting). For each ER and SF dataset with varying dimensions, we initially generate five tuning datasets. After determining the optimal hyperparameters, we fix them and evaluate the models on 30 test datasets. For DDS, we adopt the hyperparameters provided in the original paper \cite{charpentier2022differentiable}. BCD and BGES do not necessitate hyperparameter tuning since BCD already incorporates the correct prior graph for ER and SF datasets. For semi-synthetic Syntren and real world Sachs protein cells datasets, we assume the number of edges in the ground truth graphs are known and we tune our hyperparameters to produce roughly correct number of edges. BCD and DIBS also assume access to the ground truth edge number and use the graph prior to enforce the number of edges. 

\begin{table}[]
\centering
\begin{tabular}{lllll}
\hline
\multicolumn{5}{c}{\ModelName{}}                                                                                                                     \\ \hline
                                          & \multicolumn{1}{l|}{$\lambda_s$} & \multicolumn{1}{l|}{Scale $\vp$} & \multicolumn{1}{l|}{Scale $\mTheta$} & Sparse Init. \\ \hline
\multicolumn{1}{l|}{linear ER $d=5$}      & \multicolumn{1}{l|}{50}            & \multicolumn{1}{l|}{0.001}            & \multicolumn{1}{l|}{0.001}                &  False            \\
\multicolumn{1}{l|}{linear SF $d=5$}      & \multicolumn{1}{l|}{50}            & \multicolumn{1}{l|}{0.01}            & \multicolumn{1}{l|}{0.001}                &   False           \\
\multicolumn{1}{l|}{nonlinear ER $d=20$}  & \multicolumn{1}{l|}{300}         & \multicolumn{1}{l|}{0.01}        & \multicolumn{1}{l|}{0.01}            & False        \\
\multicolumn{1}{l|}{nonlinear SF $d=20$}  & \multicolumn{1}{l|}{200}         & \multicolumn{1}{l|}{0.1}         & \multicolumn{1}{l|}{0.1}             & False        \\
\multicolumn{1}{l|}{nonlinear ER $d=30$}  & \multicolumn{1}{l|}{500}         & \multicolumn{1}{l|}{1}           & \multicolumn{1}{l|}{0.01}            & False        \\
\multicolumn{1}{l|}{nonlinear SF $d=30$}  & \multicolumn{1}{l|}{300}         & \multicolumn{1}{l|}{0.01}        & \multicolumn{1}{l|}{0.01}            & False        \\
\multicolumn{1}{l|}{nonlinear ER $d=50$}  & \multicolumn{1}{l|}{500}         & \multicolumn{1}{l|}{0.01}        & \multicolumn{1}{l|}{0.01}            & True         \\
\multicolumn{1}{l|}{nonlinear SF $d=50$}  & \multicolumn{1}{l|}{300}         & \multicolumn{1}{l|}{0.1}         & \multicolumn{1}{l|}{0.01}            & False        \\
\multicolumn{1}{l|}{nonlinear ER $d=70$}  & \multicolumn{1}{l|}{700}         & \multicolumn{1}{l|}{0.1}         & \multicolumn{1}{l|}{0.01}            & True         \\
\multicolumn{1}{l|}{nonlinear SF $d=70$}  & \multicolumn{1}{l|}{300}         & \multicolumn{1}{l|}{0.01}        & \multicolumn{1}{l|}{0.01}            & False        \\
\multicolumn{1}{l|}{nonlinear ER $d=100$} & \multicolumn{1}{l|}{700}            & \multicolumn{1}{l|}{0.1}            & \multicolumn{1}{l|}{0.01}                &  False            \\
\multicolumn{1}{l|}{nonlinear SF $d=100$} & \multicolumn{1}{l|}{700}            &\multicolumn{1}{l|}{0.1 }            & \multicolumn{1}{l|}{0.01}                &  False            \\
\multicolumn{1}{l|}{SynTren}              & \multicolumn{1}{l|}{300}         & \multicolumn{1}{l|}{0.1}         & \multicolumn{1}{l|}{0.01}            & False        \\
\multicolumn{1}{l|}{Sachs Protein Cells}              & \multicolumn{1}{l|}{1200}        & \multicolumn{1}{l|}{0.1}         & \multicolumn{1}{l|}{0.01}            & False        \\ \hline
\end{tabular}
\caption{The hyperparameter selection for \ModelName{} for each setting.}
\label{tab: BayesDAG hyperparameter}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{lllll}
\hline
\multicolumn{5}{c}{DIBS}                                                                                                                                  \\ \hline
                                         & \multicolumn{1}{l|}{$\alpha$} & \multicolumn{1}{l|}{$h$ latent} & \multicolumn{1}{l|}{$h_\theta$} & $h_\sigma$ \\ \hline
\multicolumn{1}{l|}{linear ER $d=5$}     & \multicolumn{1}{l|}{0.02}     & \multicolumn{1}{l|}{5}          & \multicolumn{1}{l|}{1000}       & 1          \\
\multicolumn{1}{l|}{linear SF $d=5$}     & \multicolumn{1}{l|}{0.02}     & \multicolumn{1}{l|}{15}         & \multicolumn{1}{l|}{500}        & 1          \\
\multicolumn{1}{l|}{nonlinear ER $d=20$} & \multicolumn{1}{l|}{0.02}     & \multicolumn{1}{l|}{5}          & \multicolumn{1}{l|}{1500}       & 10         \\
\multicolumn{1}{l|}{nonlinear SF $d=20$} & \multicolumn{1}{l|}{0.2}      & \multicolumn{1}{l|}{5}          & \multicolumn{1}{l|}{1500}       & 10         \\
\multicolumn{1}{l|}{nonlinear ER $d=30$} & \multicolumn{1}{l|}{0.2}      & \multicolumn{1}{l|}{5}          & \multicolumn{1}{l|}{500}        & 1          \\
\multicolumn{1}{l|}{nonlinear SF $d=30$} & \multicolumn{1}{l|}{0.2}      & \multicolumn{1}{l|}{5}          & \multicolumn{1}{l|}{1000}       & 1          \\
\multicolumn{1}{l|}{nonlinear ER $d=50$} & \multicolumn{1}{l|}{0.2}      & \multicolumn{1}{l|}{5}          & \multicolumn{1}{l|}{500}        & 10         \\
\multicolumn{1}{l|}{nonlinear SF $d=50$} & \multicolumn{1}{l|}{0.2}      & \multicolumn{1}{l|}{5}          & \multicolumn{1}{l|}{1500}       & 1          \\
\multicolumn{1}{l|}{SynTren}             & \multicolumn{1}{l|}{0.2}         & \multicolumn{1}{l|}{5}           & \multicolumn{1}{l|}{500}           &  10          \\
\multicolumn{1}{l|}{Sachs Protein Cells}             & \multicolumn{1}{l|}{0.2}         & \multicolumn{1}{l|}{5}           & \multicolumn{1}{l|}{500}           &  10          \\ \hline
\end{tabular}
\caption{The hyperparameter selection for DIBS for each setting.}
\label{tab: Dibs hyperparameter}
\end{table}

\paragraph{Network structure}
We use one hidden layer MLP with hidden size of $\max(4*d, 64)$ for the nonlinear functional relations, where $d$ is the dimensionalilty of dataset. We use \textbf{LeakyReLU} as the activation function. We also enable the \textbf{LayerNorm} and \textbf{residual connections} in the network. In particular, for variational network $\mu_\phi$ in \ModelName{}, we apply the \textbf{LayerNorm} on $\vp$ before inputting it to the network. We use 2 hidden layer MLP with size 48, \textbf{LayerNorm} and \textbf{residual connections} for $\mu_\phi$. 

\paragraph{Sparse initialization for \ModelName{}}
For \ModelName{}, we additionally allow sparse initialization by sampling a sparse $\mW$ from the $\mu_\phi$. This can be achieved by substracting a constant $1$ from the existing logits (i.e.~the output from $\mu_\phi$). 
\paragraph{Other hyperparameters}
For \ModelName{}, we run $10$ parallel SG-MCMC chains for $\vp$ and $\mTheta$. We implement an adaptive sinkhorn iteration where the iteration automatically stops when the sum of rows and columns are closed to $1$ within the threshold $0.001$ (upto a maximum of $3000$ iterations). Typically, we found this to require only around $300$ iterations. We set the sinkhorn temperature $t$ to be $0.2$. For the reparametrization of $\mW$ matrix with Gumbel-softmax trick, we use temperature $0.2$.
During evaluation, we use $100$ SG-MCMC particles extracted from the particle buffer. We use $0.0003$ for SG-MCMC learning rate $l$ and batch size $512$. We run $700$ epochs to make sure the model is fully converged.

For DIBS, we can only use $20$ SVGD particles for evaluation due to the quadratic scaling with the number of particles. We use $0.1$ for Gumbel-softmax temperature. We run $10000$ epochs for convergence. The learning rate is selected as $0.01$. 

\cref{tab: BayesDAG hyperparameter} shows the hyperparameter selection for \ModelName{}. \cref{tab: Dibs hyperparameter} shows the hyperparameter selection for DIBS.


