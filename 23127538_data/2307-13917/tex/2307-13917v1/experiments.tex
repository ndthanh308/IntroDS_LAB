\section{Experiments}
In this section, we aim to study empirically the following aspects: (1) posterior inference quality of \ModelName~as compared to the true posterior when the causal model is identifiable only upto Markov Equivalence Class (MEC); (2) posterior inference quality of \ModelName~in high dimensional nonlinear causal models (3) ablation studies of \ModelName~ and (4) performance in semi-synthetic and real world applications. 

\paragraph{Baselines.} We mainly compare \ModelName~ with the following baselines: Bootstrap GES (\textbf{BGES})~\cite{chickering2002optimal,friedman2013data}, \textbf{BCD} Nets~\cite{cundy2021bcd}, Differentiable DAG Sampling (\textbf{DDS} \cite{charpentier2022differentiable}) and \textbf{DIBS}~\cite{lorch2021dibs}. 
% BCD Nets and DDS are VI based Bayesian inference approaches parameterizing a DAG through node permutations, while DIBS parameterizes a DAG by performing constrained optimization through the NOTEARS~\cite{zhang2018advances} penalty. 
%Further baseline details are given in~\cref{app:baselines}.
\subsection{Evaluation on Synthetic Data}
\paragraph{Synthetic data.} 


We evaluate our method on synthetic data, where ground truth graphs are known. Following previous work, we generate data by randomly sampling DAGs from Erdos-RÃ¨nyi (ER)~\cite{erdHos1960evolution} or Scale-Free (SF)~\cite{barabasi1999emergence} graphs with per node degree 2 and drawing at random ground truth parameters for linear or nonlinear models. For $d=5$, we use $N=500$ training, while for higher dimensions, we use $N=5000$. We assess performance on 30 random datasets for each setting. %Detailed data generation procedures can be found in \cref{app:synthetic_data}.
 
\paragraph{Metrics} For $d=5$ linear models, we compare the approximate and true posterior over DAGs using Maximum Mean Discrepancy (MMD) and also evaluate the expected CPDAG Structural Hamming Distance (SHD). For higher-dimensional nonlinear models with intractable posterior, we compute the expected SHD (\textbf{$\E$-SHD}), expected orientation F1 score (\textbf{Edge F1}) and negative log-likelihood of the held-out data (\textbf{NLL}). Our synthetic data generation and evaluation protocol follows prior work~\cite{annadani2021variational, geffner2022deep,lorch2021dibs}. All the experimental details, including hyperparameters are provided in \cref{app:experiments}.

\begin{wrapfigure}[18]{r}{0.4\textwidth}
    \vspace{-2em}
    \centering
    % Figure removed
    \caption{\looseness=-1 Posterior inference on linear synthetic datasets with $d=5$. Metrics are computed against the true posterior. $\downarrow$ denotes lower is better.}
    \label{fig:linear_5_5}
\end{wrapfigure}
\subsubsection{Comparison with True Posterior}
  
Capturing equivalence classes and quantifying epistemic uncertainty are crucial in Bayesian causal discovery. We benchmark our 
method
%inference technique
against the true posterior using a 5-variable linear SCM with unequal noise variance (identifiable upto MEC~\cite{peters2014identifiability}). The true posterior over graphs $p(\mG\mid \mD)$ can be computed using the BGe score~\cite{geiger2002parameter,kuipers2014addendum}. Results in \cref{fig:linear_5_5} show that our method outperforms
DIBS and DDS in both ER and SF settings. Compared to BCD, we perform better in terms of MMD in ER but worse in SF. We find that BGES performs very well in low-dimensional linear settings, but suffers significantly in more realistic nonlinear settings (see below).


\subsubsection{Evaluation in Higher Dimensions}
We evaluate our method on high dimensional scenarios with nonlinear relations. Our approach is the first to attempt full posterior inference in nonlinear models using permutation-based methods. Results for $d=30$ variables in \cref{fig:nonlinear_30_60} demonstrate that \ModelName~ significantly outperforms other \emph{permutation-based approaches} and DIBS in most of the metrics. For $d=50$, \ModelName~ performs comparably to DIBS in ER but a little worse in SF. However, our method achieves better NLL on held-out data compared to most baselines including DIBS for $d=30,50$, ER and SF settings. Only DDS gives better NLL for $d=30$ ER setting, but this doesn't translate well to other metrics and settings. 
\begin{wraptable}{r}{0.4\textwidth}
\vspace{-0.5em}
\caption{$\E$-SHD (with $95\%$ CI) for ER graphs in higher dimensional nonlinear causal models. DIBS becomes computationally prohibitive for $d>50$.}
\label{tab:scaling_Exp}
\begin{adjustbox}{width=0.4\textwidth}
\begin{tabular}{l|c|c}
      & $d=70$ & $d=100$ \\ \hline
BGES     &  355.77 $\pm$ 18.02   & 563.02 $\pm$ 27.21      \\
BCD    & 217.05 $\pm$ 9.58    &362.66 $\pm$ 29.18       \\
DIBS      &  N/A   &  N/A     \\
BaDAG     & \textbf{143.70 $\pm$ 11.61}    & \textbf{295.92 $\pm$ 24.67}    
\end{tabular}
\end{adjustbox}
\vspace{-1em}
\end{wraptable}

We additionally evaluate on $d\in \{70,100\}$ variables (\cref{tab:scaling_Exp}). We find that our method consistently outperforms the baselines with $d=70$ and in terms of $\mathbb{E}$-SHD with $d=100$. Full results are presented in \cref{appsubsec: higher dimensional datasets}. Competitive performance for $d>50$ in nonlinear settings further demonstrates the applicability and computational efficiency of the proposed approach. In contrast, the only fully Bayesian nonlinear method, DIBS, is not computationally efficient to run for $d>50$.


\subsection{Ablation Studies}

We conduct ablation studies on our method using the nonlinear ER $d=30$ dataset. Key findings are summarized below, with detailed results available in \cref{appsubsec: ablation study}. 
% init_p_scale
\paragraph{Initialized $\vp$ scale} \cref{fig: ablation init p scale} investigates the influence of the initialized scale of $\vp$. We found that the performance is the best with $\alpha=0.01$ or $10^{-5}$, and deteriorates with increasing scales. This is because with larger initialization scale, the absolute value of the $\vp$ is large. Longer SG-MCMC updates are needed to reverse the node potential order, which hinders the exploration of possible permutations, resulting in the convergence to poor local optima.

% num_particles
\paragraph{Number of SG-MCMC chains}
We examine the impact of the number of parallel SG-MCMC chains in \cref{fig: ablation num particles}. We observe that it does not have a significant impact on the performance, especially with respect to the $\E$-SHD and Edge F1 metrics. 
\paragraph{Injected noise level for SG-MCMC}
In \cref{fig: ablation scale,fig: ablation scale_p}, we study the performance differences arising from various injected noise levels for $\vp$ and $\mTheta$ in the SG-MCMC algorithm (i.e.~$s$ of the SG-MCMC formulation in \cref{app: SG-MCMC update}). Interestingly, the noise level of $\vp$ does not impact the performance as much as the level of $\mTheta$. Injecting noise helps improve the performance, but a smaller noise level should be chosen for $\mTheta$ to avoid divergence from optima.

\subsection{Application 1: Evaluation on Semi-Synthetic Data}
We evaluate our method on the SynTReN simulator~\cite{van2006syntren}. This simulator creates synthetic transcriptional regulatory
networks and produces simulated gene expression data that approximates real experimental data. We use five different simulated datasets provided by \cite{lachapelle2019gradient} with $N=500$ samples each. \cref{tab:real_results} presents the results of all the methods. We find that our method recovers the true network much better in terms of $\mathbb{E}$-SHD as well as Edge F1 compared to baselines.

\subsection{Application 2: Evaluation on Real Data}
We also evaluate on a real dataset which measures the expression
level of different proteins and phospholipids in human cells (called the Sachs Protein Cells Dataset)~\cite{sachs2005causal}. The data corresponds to a network of protein-protein interactions of 11 different proteins with 17 edges in total among them. There are 853 observational samples in total, from which we bootstrap 800 samples of $5$ different datasets. It is to be noted that this data does not necessarily adhere to the additive noise and DAG assumptions, thereby having significant model misspecification. Results in \cref{tab:real_results} demonstrate that our method performs well as compared to the baselines even with model misspecification, proving the suitability of the proposed framework for real-world settings.
% Figure environment removed




\begin{table}[]
\centering
\caption{Results (with 95$\%$ confidence intervals) on Syntren (semi-synthetic) and Sachs Protein Cells (real-world) datasets. For Syntren, results are averaged over 5 different datasets. For Sachs, results are averaged over 5 different restarts. $\downarrow$ denotes lower is better and $\uparrow$ denotes higher is better. }
\label{tab:real_results}
%\begin{adjustbox}{width=0.5\textwidth}
\begin{tabular}{lcc||cc}
\hline
                           & \multicolumn{2}{c||}{Syntren $(d=20)$}                                                                                 & \multicolumn{2}{c}{Sachs Protein Cells $(d=11)$}                                                                      \\ \hline
                           & \multicolumn{1}{c|}{$\mathbb{E}$-SHD $(\downarrow)$} & Edge F1 $(\uparrow)$ & \multicolumn{1}{c|}{$\mathbb{E}$-SHD $(\downarrow)$} & Edge F1 $(\uparrow)$ \\ \hline
\multicolumn{1}{l|}{BGES}  & \multicolumn{1}{c|}{66.18 $\pm$ 9.47}                & \textbf{0.21 $\pm$ 0.05}    & \multicolumn{1}{c|}{\textbf{16.61 $\pm$ 0.44}}                & 0.22 $\pm$ 0.02      \\
\multicolumn{1}{l|}{DDS}   & \multicolumn{1}{c|}{134.37 $\pm$ 4.58}                                &   0.13 $\pm$ 0.02                  &   \multicolumn{1}{c|}{34.90 $\pm$ 0.73}                                               &  0.21 $\pm$ 0.02                              \\
\multicolumn{1}{l|}{BCD}   & \multicolumn{1}{c|}{38.38 $\pm$ 7.12}                & 0.15 $\pm$ 0.07      & \multicolumn{1}{c|}{17.05 $\pm$ 1.93}                & 0.20 $\pm$ 0.08      \\
\multicolumn{1}{l|}{DIBS}  & \multicolumn{1}{c|}{46.43 $\pm$ 4.12}                & 0.16 $\pm$ 0.02      & \multicolumn{1}{c|}{22.3 $\pm$ 0.31}                 & 0.20 $\pm$ 0.01        \\
\multicolumn{1}{l|}{BaDAG} & \multicolumn{1}{c|}{\textbf{34.21 $\pm$ 2.82}}                                & \textbf{0.20 $\pm$ 0.02}                                    & \multicolumn{1}{c|}{18.92 $\pm$ 1.0}                              & \textbf{0.26 $\pm$ 0.04}                \\
\hline
\end{tabular}
%\end{adjustbox}
\end{table}







