\section{Introduction}
\label{sec:introduction}
The quest for discovering causal relationships in data-generating processes lies at the heart of empirical sciences and decision-making~\cite{pe2001inferring,sachs2005causal, van2006application}. Structural Causal Models (SCMs) \cite{pearl2009causality} and their associated Directed Acyclic Graphs (DAGs) provide a robust mathematical framework for modeling such relationships. Knowledge of the underlying SCM and its corresponding DAG permits predictions of unseen interventions and causal reasoning, thus making causal discovery -- learning an unknown SCM and its associated DAG from observed data -- a subject of extensive research \cite{peters2017elements,spirtes2000causation}.
  

In contrast to traditional methods that infer a single graph or its Markov equivalence class (MEC) \cite{chickering2002optimal, spirtes2000causation},  Bayesian causal discovery~\cite{friedman2003being,heckerman2006bayesian,tong2001active} aims to infer a posterior distribution over SCMs and their DAGs from observed data. This approach encapsulates the epistemic uncertainty, degree of confidence in every causal hypothesis, which is particularly valuable for real-world applications when data is scarce. It is also beneficial for downstream tasks such as experimental design \cite{agrawal2019abcd,annadani2023differentiable,murphy2001active, tigas2022interventions}.


\looseness=-1 The central challenge in Bayesian causal discovery lies in inferring the posterior distribution over the union of the exponentially growing (discrete) DAGs and (continuous) 
function parameters. Prior works have used Markov Chain Monte Carlo (MCMC) to directly sample DAGs or bootstrap traditional discovery methods \cite{chickering2002optimal, murphy2001active, tong2001active}, but these methods are typically limited to linear models which admit closed-form marginalization over continuous parameters. Recent advances have begun to utilize gradient information for more efficient inference. These approaches are either: (1) DAG regularizer-based methods, e.g.~ DIBS \cite{lorch2021dibs}, which use continuous relaxation of adjacency matrices together with DAG regularizer \cite{zheng2018dags}. But DIBS formulation fails to model edge co-dependencies and suffer from inference quality due to its inference engine (Stein variational gradient descent) \cite{gong2020sliced, gong2021active}. Additionally, all DAG regularizer based methods cannot guarantee DAG generation; (2) permutation-based DAG learning, which directly infers permutation matrices and guarantees to generate DAGs. However, existing works focus on using only variational inference \cite{charpentier2022differentiable,cundy2021bcd}, which may suffer from inaccurate inference quality \cite{gong2019icebreaker,springenberg2016bayesian, trippe2018overpruning} and is sometimes restricted to only linear models \cite{cundy2021bcd}. %We provide a more detailed literature review in \cref{appsec: related work}.



In this work, we introduce \ModelName, a gradient-based Bayesian causal discovery framework that overcomes the above limitations. Our contributions are:
\begin{enumerate}
\item We prove that an augmented space of edge beliefs and node potentials $(\mW,\vp)$, similar to NoCurl \cite{yu2021dags}, permits equivalent Bayesian inference in DAG space without the need for any regularizer. (\cref{subsec: Bayesian inference W p space})

\item We derive an equivalence relation from this augmented space to permutation-based DAG learning which provides a general framework for gradient-based posterior inference. (\cref{subsec: equivalent formulation})

\item Based on this general framework, we propose a scalable Bayesian causal discovery that is model-agnostic for linear and non-linear cases and also offers improved inference quality. We instantiate our approach through two formulations: (1) a combination of SG-MCMC and VI  (2) SG-MCMC with a continuous relaxation. (\cref{sec: SGMCMC sampling framework})

\item We demonstrate the effectiveness of our approach in providing accurate Bayesian inference quality and superior causal discovery performance with comprehensive empirical evaluations on various datasets. We also demonstrate that our method can be easily scaled to $100$ variables with nonlinear relationships. (\cref{sec: experiments})
\end{enumerate} 

