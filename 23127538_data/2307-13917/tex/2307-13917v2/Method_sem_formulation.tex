\subsection{Model Formulation}
\label{subsect: model formulation}
We build upon the model formulation of \cite{geffner2022deep}, which combines the additive noise model with neural networks to describe the functional relationship. Specifically, $X_i \coloneqq f_i(\mX_{\Pa^i}) + \epsilon_i$, where $f_i$ adheres to the adjacency relation specified by $\mG$, i.e.~$\partial f_i(\vx) / \partial x_j = 0$ if no edge exists between nodes $i$ and $j$. We define $f_i$ as 
\begin{equation}
    f_i(\vx) = \zeta_i\left(\sum_{j=1}^dG_{ji}l_j(x_j)\right),
\end{equation}
where $\zeta_i$ and $l_i$ are neural networks with parameters $\mTheta$, and $\mG$ serves as a mask disabling non-parent values. To reduce the number of neural networks, we adopt a weight-sharing mechanism: $\zeta_i(\cdot) = \zeta(\vu_i,\cdot)$ and $l_i(\cdot) = l(\vu_i,\cdot)$, with trainable node embeddings $\vu_i$.

\paragraph{Likelihood of SCM}
The likelihood can be evaluated through the noise $\bm{\epsilon} = \vx - \vf(\vx;\mTheta)$. \cite{geffner2022deep} showed that if $\mG$ is a DAG, then the mapping from $\bm{\epsilon}$ to $\vx$ is invertible with a Jacobian determinant of 1. Thus, the observational data likelihood is:
\begin{equation}
    p(\vx \vert \mG) = p_\epsilon(\vx-\vf(\vx;\mTheta)) = \prod_{i=1}^d p_{\epsilon_i}(x_i-f_i(\vx_{\Pa_G^i}))
    \label{eq: SEM likelihood}
\end{equation}
\paragraph{Prior design}
We implicitly define the prior $p(\mG)$ via $p(\vp,\mW)$. We propose the following for the joint prior:
\begin{equation}
    p(\mW,\vp, \mTheta) \propto \nonumber \mathcal{N}(\mTheta;\bm{0},\bm{I})  \mathcal{N}(\vp;\bm{0},\alpha\bm{I})\mathcal{N}(\mW;\bm{0},\bm{I})\exp(-\lambda_s\Vert\tau(\mW,\vp)\Vert^2_F)
    \label{eq: Prior p w}
\end{equation}
where $\alpha$ controls the initialization scale of $\vp$ and $\lambda_s$ controls the sparseness of $\mG$. 

\subsection{Bayesian Inference of $W,p,\Theta$}
%\subsubsection{Combined inference: SG-MCMC + VI}
\label{subsubsec: combined inference}

The main challenge lies in the binary nature of $\mW \in \{0,1\}^{d\times d}$, which requires a discrete sampler. Although recent progress has been made \cite{grathwohl2021oops,sun2022discrete,zanella2020informed,zhang2022langevin}, these methods either involve expensive Metropolis-Hasting (MH) steps or require strong assumptions on the target posterior when handling batched gradients. To address this, we propose a combination of SG-MCMC for $\vp,\mTheta$ and VI for $\mW$. It should be noted that our framework can incorporate any suitable discrete sampler if needed. 
\begin{wrapfigure}[13]{r}{0.25\textwidth}
    \vspace{2em}
    \input{Fig/graphical_model_vi}
    \caption{Graphical model of the inference problem.}
    \label{fig: graphical model with latent}
    %\vspace{-1em}
\end{wrapfigure}
% Since the joint inference requires the introduction of the additional latent variable $\tmW$ as a surrogate for $\mW$, we can sidestep it by proposing an alternative procedure, where we use SG-MCMC for $\vp, \mTheta$ but VI for $\mW$. 

We employ a Gibbs sampling procedure \cite{casella1992explaining}, which iteratively applies (1) sampling $\vp,\mTheta\sim p(\vp,\mTheta|\mD,\mW)$ with SG-MCMC; (2) updating the variational posterior $q_\phi(\mW|\vp,\mD)\approx p(\mW|\vp,\mTheta,\mD)$.

We define the posterior $p(\vp,\mTheta \vert \mD,\mW)\propto \exp(-U(\vp,\mW,\mTheta))$, where $U(\vp,\mW,\mTheta) = -\log p(\vp,\mD,\mW,\mTheta)$. SG-MCMC in continuous time defines a specific form of It\^o diffusion that maintains the target distribution invariant \cite{ma2015complete} without the expensive computation of the MH step. We adopt the Euler-Maruyama discretization for simplicity. Other advanced discretization can be easily incorporated \cite{chen2015convergence,platen2010numerical}. 

Preconditioning techniques have been shown to accelerate SG-MCMC convergence \cite{chen2014stochastic, gong2019icebreaker,li2016preconditioned,welling2011bayesian,ye2017langevin}. We modify the sampler based on \cite{gong2019icebreaker}, which is inspired by Adam \cite{kingma2014adam}. Detailed update equations can be found in \cref{app: SG-MCMC update}.

The following proposition specifies the gradients required by SG-MCMC: $\nabla_{\vp,\mTheta} U(\vp,\mW,\mTheta)$.

% For $\vp$ and $\mTheta$, their sampling procedures are identical as the joint inference (\cref{subsubsec: joint inference}). The following proposition specifies the required gradient $\nabla_{\vp,\mTheta}U(\vp,\mW,\mTheta)$

\begin{proposition}
Assume the model is defined as above, then we have the following:
\begin{equation}
    \nabla_{\vp} U=-\nabla_{\vp}\log p(\vp) - \nabla_{\vp}\log p(\mD\vert \mTheta, \tau(\mW,\vp))
    \label{eq: p gradient}
\end{equation}
and 
\begin{equation}
    \nabla_{\mTheta} U =-\nabla_{\mTheta}\log p(\mTheta) - \nabla_{\mTheta}\log p(\mD\vert \mTheta,\tau(\vp,\mW))
    \label{eq: mTheta gradient}
\end{equation}
\label{prop: gradient computation}
\end{proposition}
Refer to \cref{subapp: proof of proposition gradient computation} for details.

\paragraph{Variational inference for $\mW$}
We use the variational posterior $q_\phi(\mW\vert \vp)$ to approximate the true posterior $p(\mW\vert \vp,\mTheta,\mD)$. Specifically, we select an independent Bernoulli distribution with logits defined by the output of a neural network $\mu_\phi(\vp)$:
\begin{equation}
    q_\phi(\mW\vert \vp)=\prod_{ij}Ber(\mu_\phi(\vp)_{ij})
    \label{eq:VI bernoulli}
\end{equation}
To train $q_\phi$, we derive the corresponding \emph{evidence lower bound} (ELBO):
\begin{equation}
    \ELBO(\phi) = \E_{q_\phi(\mW|\vp)}\left[
    \log p(\mD,\vp,\mTheta\vert \mW)]-\KL\left[q_\phi(\mW\vert \vp)\Vert p(\mW)\right]
    \right].
    \label{eq: ELBO for W}
\end{equation}
where $\KL$ is the Kullback-Leibler divergence. 
The derivation is in \cref{subapp: derivation of ELBO}.
\cref{alg: combined inference} summarizes this inference procedure. 
\begin{algorithm}[tb]
\caption{\ModelName~ SG-MCMC+VI Inference}
\label{alg: combined inference}
\begin{algorithmic}
\STATE {\bfseries Input:} dataset $\mD$; prior $p(\vp, \mW),p(\mTheta)$; SG-MCMC sampler $\sampler$; sampler hyperparameters $\Psi$; network $\mu_\phi(\cdot)$; training iteration $T$.
\STATE {\bfseries Output:} samples $\{\mTheta,\vp\}$ and variational posterior $q_\phi$
\STATE Initialize $\mTheta^{(0)}, \vp^{(0)},\phi$
\FOR{$t=1\ldots,T$}
    \STATE Sample $\mW^{(t-1)}\sim q_{\phi}(\mW\vert \vp^{(t-1)})$
    \STATE Evaluate $\nabla_{\vp,\mTheta} U$ (\cref{eq: p gradient,eq: mTheta gradient}) with $\mTheta^{(t-1)},\vp^{(t-1)},\mW^{(t-1)}$
    \STATE $\mTheta^{(t)},\vp^{(t)} = \sampler(\nabla_{\vp,\mTheta}U;\Psi)$
    \IF{storing condition met}
        \STATE $\{\vp,\mTheta\}\leftarrow \vp^{(t)},\mTheta^{(t)}$
    \ENDIF
    \STATE Maximize ELBO (\cref{eq: ELBO for W}) w.r.t. $\phi$ with $\vp^{(t)}, \mTheta^{(t)}$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\paragraph{SG-MCMC with continuous relaxation}
Furthermore, we explore an alternative formulation that circumvents the need for variational inference. Instead, we employ SG-MCMC to sample $\tmW$, a continuous relaxation of $\mW$, facilitating a fully sampling-based approach. For a detailed formulation, please refer to \cref{appsec: joint inference SG-MCMC}. We report its performance in \cref{appsubsec: fully SG-MCMC performance}, which surprisingly is inferior to SG-MCMC+VI. We hypothesize that coupling $\mW, \vp$ through $\mu_\phi$ is important since changes in $\vp$ results in changes of the permutation matrix $\perm(\vp)$, which should also influence $\mW$ accordingly during posterior inference. However, through sampling $\tmW$ with few SG-MCMC steps, this change cannot be immediately reflected, resulting in inferior performance. Thus, we focus only on the performance of SG-MCMC+VI for our experiments. 

\paragraph{Computational complexity}
Our proposed SG-MCMC+VI offers a notable improvement in computational cost compared to existing approaches, such as DIBS \cite{lorch2021dibs}. 
The computational complexity of our method is $O(BN_p+N_pd^3)$, where $B$ represents the batch size and $N_p$ is the number of parallel SG-MCMC chains. This former term stems from the forward and backward passes, and the latter comes from the Hungarian algorithm, which can be parallelized to further reduce computational cost. In comparison, DIBS has a complexity of $O(N_p^2N+N_pd^3)$ with $N\gg B$ being the full dataset size. This is due to the kernel computation involving the entire dataset and the evaluation of the matrix exponential in the DAG regularizer \cite{zheng2018dags}. As a result, our approach provides linear scalability w.r.t. $N_p$ with substantially smaller batch size $B$. Conversely, DIBS exhibits quadratic scaling in terms of $N_p$ and lacks support for mini-batch gradients.

