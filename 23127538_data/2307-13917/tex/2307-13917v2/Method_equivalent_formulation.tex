\subsection{Equivalent Formulation}
\label{subsec: equivalent formulation}
\looseness=-1 In this section, we address the above issue by deriving an equivalence to a permutation learning problem. This alternative formulation enables various techniques that can approximate the gradient of $\vp$. 

\paragraph{Intuition} The node potential $\vp$ implicitly defines a topological ordering through the mapping $\step(\grad(\cdot))$. In particular, $\grad(\cdot)$ outputs a skew-symmetric adjacency matrix, where each entry specifies the potential difference between nodes. $\step(\grad(\cdot))$ zeros out the negative potential differences (i.e.~$p_i\leq p_j$), and only permits the edge direction from higher potential to the lower one (i.e.~$p_i>p_j$). This implicitly defines a sorting operation based on the descending node potentials, which can be cast as a particular $\argmax$ problem \cite{blondel2020fast,kuhn1955hungarian, mena2018learning, niculae2018sparsemap,zantedeschi2023dag} involving a permutation matrix.

\paragraph{Alternative formulation} We define $\mL\in\binaryset$ as a matrix with lower triangular part to be $1$, and vector $\vo=[1,\ldots, d]$. We propose the following formulation:
\begin{align}
&\mG = \mW \odot \left[\perm(\vp)\mL \perm(\vp)^T\right] \
&\text{where}\; \perm(\vp) = \argmax_{\perm'\in\bm{\Sigma}_d} \vp^T(\perm' \vo)
\label{eq: alternative formulation}
\end{align}
Here, $\bm{\Sigma}_d$ represents the space of all $d$ dimensional permutation matrices. The following theorem states the equivalence of this formulation to  \cref{eq: Binary NoCurl}.
\begin{theorem}[Equivalence to NoCurl formulation]
Assuming the conditions in \cref{thm: equivalence of bayesian inference} are satisfied. Then, for a given $(\mW,\vp)$, we have
$$
\mG=\mW\odot \step(\grad \vp) = \mW\odot \left[ \perm(\vp)\mL\perm(\vp)^T\right]
$$
where $\mG$ is a DAG and $\perm(\vp)$ is defined in \cref{eq: alternative formulation}.
\label{thm: equivalence of alternative formulation}
\end{theorem}
Refer to \cref{subapp: proof of theorem alternative formulation} for details.


This theorem translates our proposed operator $\step(\grad(\vp))$ into finding a corresponding permutation matrix $\perm(\vp)$. Although this does not directly solve the uninformative gradient, it opens the door for approximating this gradient with the tools from the differentiable permutation literature \cite{blondel2020fast,mena2018learning, niculae2018sparsemap}. For simplicity, we adopt the Sinkhorn approach \cite{mena2018learning}, but we emphasize that this equivalence is general enough that any past or future approximation methods can be easily applied.


\paragraph{Sinkhorn operator} The Sinkhorn operator $\Sinkhorn(\mM)$ on a matrix $\mM$ \cite{adams2011ranking} is defined as a sequence of row and column normalizations, each is called Sinkhorn iteration.

\cite{mena2018learning} showed that the non-differentiable $\argmax$ problem
\begin{equation}
    \perm = \argmax_{\perm'\in\Sigma_d}\left\langle\perm', \mM\right\rangle
    \label{eq: permutation argmax}
\end{equation}
\looseness=-1 can be relaxed through an entropy regularizer with its solution being expressed by $\Sinkhorn(\cdot)$.
% \cite{mena2018learning} showed that the $\argmax$ problem of learning permutation matrix in \cref{eq: permutation argmax} can be relaxed through an entropy regularizer, and its solution can be obtained using $\Sinkhorn(\cdot)$.
In particular, they showed that $\Sinkhorn(\mM/t)=\argmax_{\perm'\in \polytope}\left\langle\perm',\mM\right\rangle+th(\perm')$, where $h(\cdot)$ is the entropy function. This regularized solution converges to the solution of \cref{eq: permutation argmax} when $t\rightarrow 0$, i.e.~$\lim_{t\rightarrow 0}\Sinkhorn(\mM/t)$. 
Since the Sinkhorn operator is differentiable, $\Sinkhorn(\mM/t)$ can be viewed as a differentiable approximation to \cref{eq: permutation argmax}, which can be used to obtain the solution of \cref{eq: alternative formulation}. Specifically, we have
\begin{equation}
    \argmax_{\perm'\in\bm{\Sigma}_d} \vp^T(\perm'\vo) = \argmax_{\perm'\in\bm{\Sigma}_d}\langle\perm',\vp\vo^T\rangle = \lim_{t\rightarrow 0}\Sinkhorn(\frac{\vp\vo^T}{t})
    \label{eq: Sinkhorn solution}
\end{equation}
In practice, we approximate it wth $t>0$, resulting in a doubly stochastic matrix. To get the binary permutation matrix, we apply the Hungarian algorithm \cite{munkres1957algorithms}. During the backward pass, we use a straight-through estimator~\cite{bengio2013estimating} for $\vp$. 

Some of the previous works \cite{charpentier2022differentiable,cundy2021bcd} have leveraged the Sinkhorn operator to model variational distributions over permutation matrices. However, they start with a full rank $\mM$, which has been reported to require over \textbf{1000} Sinkho    rn iterations to converge \cite{cundy2021bcd}. However, our formulation, based on explicit node potential $\vp\vo^T$, generates a rank-1 matrix, requiring much fewer Sinkhorn steps (around \textbf{300}) in practice, saving two-thirds of the computational cost.



