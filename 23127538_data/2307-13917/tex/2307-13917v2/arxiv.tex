\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission

\usepackage[final,nonatbib]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}
     % hyperlinks
\usepackage{graphicx}
\usepackage{hyperref}  
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{adjustbox}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usetikzlibrary{bayesnet}  
\usetikzlibrary{positioning}  

% if you use cleveref..
\usepackage[nameinlink,capitalize,noabbrev]{cleveref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{input_packages.tex}
\input{math_commands.tex}

\title{BayesDAG:  Gradient-Based Posterior Inference for Causal Discovery}

\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
\textbf{Yashas Annadani}$^{\printfnsymbol{2}\thanks{Equal contribution. \printfnsymbol{2} Work done during internship at Microsoft Research. Correspondence to \texttt{wenbogong@microsoft.com}. Code: \url{https://github.com/microsoft/Project-BayesDAG}}\,\,\,1,3,4}$
\quad
\textbf{Nick Pawlowski}$^{2}$
\quad
\textbf{Joel Jennings}$^{2}$
\quad
\textbf{Stefan Bauer}$^{3, 4}$
\\
\textbf{Cheng Zhang}$^{2}$
\quad
\textbf{Wenbo Gong}$^{\printfnsymbol{1}2}$
\\
$^1$ KTH Royal Institute of Technology, Stockholm \quad $^2$ Microsoft Research\\ $^3$ Helmholtz AI, Munich $^4$ TU Munich\\
}

\begin{document}


\maketitle


\begin{abstract}
\looseness=-1 Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs,  existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on a combination of stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and Variational Inference (VI) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluation on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.
\end{abstract}

\input{Introduction.tex}
\input{Prerequisite.tex}
\input{Methodology}
\input{related}
\input{experiments}
\input{Discussion}


% reference
\bibliography{reference}
\bibliographystyle{plain}

% App
\newpage
\appendix
%\input{Appendix/related_work}
\input{Appendix/Joint_inference_SGMCMC}
\input{Appendix/App_theory.tex}
\input{Appendix/SGMCMC.tex}
\input{Appendix/experiments}
\input{Appendix/Additional_results}
\input{Appendix/extras}
\end{document}