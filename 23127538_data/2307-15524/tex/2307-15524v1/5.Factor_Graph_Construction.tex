\section{Factor Graph Construction}\label{sec:Factor Graph Construction}


In this section, we first describe how to extract discriminative features by deep neural networks, and then elaborate how to model them as factors in a factor graph to enable gradual learning. 


\subsection{Discriminative Feature Extraction}\label{sec:Ifve}

%Since CNN has been widely used for few-shot image classification~\cite{2017Model,afrasiyabi2022matching,wertheimer2021few,huang2022few}, we leverage the existing CNN backbones for discriminative feature extraction. 

Our solution essentially trains a CNN model to learn high-dimensional vector representations of images in a class-sensitive embedding space, and then extracts monotonic features based on the learned representations to capture their class correlation. Since CNNs are prone to overfitting due to the limited amount of labeled images, we use two different backbones, ResNet-12 and WRN-28-10, for discriminative feature extraction to ensure feature diversity.  
	
	%Compared with increasing the depth, increasing the width has been empirically shown to be more effective in improving performance of residual networks. 
	
	 %Our proposed solution employs these two distinct, but to some extent complementary networks to extract diverse visual features to improve gradual learning. 
	
%{\color{blue} 
We have sketched the network structures of ResNet-12 and WRN-28-10 in Figure~\ref{fig:network structure} (b) and (c) respectively. ResNet-12 consists of four residual blocks, each of which contains three convolutional layers. Based on ResNet, the WRN-28-10 (Wide Residual Network) architecture increases the width of residual blocks from 12 to 28. By increasing the width of residual blocks, WRN-28-10 attains more advanced representational capabilities, and thus can capture finer-granular image features. In ResNet-12, we extract vector representations by the output of Average pooling2D and the input of the FC layer. In WRN-28-10, we extract vector representations by the output of the BatchNormalization layer and the input of the FC layer by the backbones. Both extracted vectors have the dimension of 640. Our ablation study has shown that using both ResNet-12 and WRN-28-10 for feature extraction is considerably better that using either of them. 
%}

We train our backbones using the methodology called EASY presented in~\cite{bendou2022easy}. Its basic principle is to take a standard classification architecture, and then set up two classifiers after the second-to-last layer of the network: one classifier for identifying the class of input samples and a new logistic regression classifier which can determine which one of the four possible rotations (one-quarter rotations of 360Â°) was applied to the input samples. EASY uses a two-step process to train model. In the first step, samples are directly input to the model and its first classifier. In the second step, samples are arbitrarily rotated and separately input into the two classifiers. Once training is complete, we freeze the backbone and use it, $f(\theta)$ as shown in Figure~\ref{fig:network structure} (a), to extract representation vectors for the images in $\mathcal{D}_V$ and $\mathcal{D}_T$. 
 

	%We apply ResNet-12 and WRN-28-10 for the aforementioned operations and obtain $f(\theta_1)$ and $f(\theta_2)$, respectively. 
	


Then, based on the learned vector representations, we extract two types of monotonic features as follows:
	
\begin{itemize}
  \item {\bf Class Centroid Distance (\emph{CCD}).} We estimate the prototype class centroid of each class by its support set, and then measure an image's similarity with a class by calculating its distance to the class centroid, which is defined as 1.0 minus vector cosine similarity. It is obvious that the smaller the distance is, the more likely the image belongs to the class. We denote the unary feature of class centroid distance by \emph{CCD}. 
	
	%For each image in the query set, we record its k-nearest neighbors in a class-sensitive deep embedding space. 
	
	\item {\bf K-nearest Neighborhood (\emph{KNN}).} Since a CNN classifier tends to separate the images with different class labels as far as possible while clustering the images with the same label, two images appearing very close in its corresponding embedding space usually have the same label. Therefore, we extract k-nearest neighborhood relations, ($v_i$, $v_j$, $sim_{i,i}$), in which $sim_{i,j}$ denotes the cosine similarity between the vector representations of $v_i$ and $v_j$. We denote the binary feature of k-nearest neighborhood by \emph{KNN}. In practical implementation, we suggest to set the value of $k$ within the reasonable range of [5,7].
\end{itemize}	

% Figure environment removed

% Figure environment removed

We have visualized the unary \emph{CCD} and the binary \emph{KNN} features in Figure~\ref{fig:distance structure} (b) and (c) respectively. 
It is noteworthy that \emph{CCD} and \emph{KNN} are complementary to each other, in that \emph{CCD} captures an image's correlation with global representatives, the prototype class centroids, while \emph{KNN} captures an image's correlation with local representatives, its nearest neighbors. 

%{\color{purple} Therefore, the hybrid mechanism of combining \emph{CCD} and \emph{KNN} can facilitate more effective knowledge conveyance compared with the homogeneous mechanism of using only \emph{CCD} or \emph{KNN}. Our ablation study in Section has also clearly demonstrated the efficacy of the hybrid mechanism.} Furthermore, our ablation study has also shown that using both ResNet-12 and WRN-28-10 for feature extraction is considerably better that using either of them. This observation clearly demonstrates that even though both ResNet-12 and WRN-28-10 are built upon the CNN backbone, they are to some extent complementary to each other in feature representation. 


\subsection{Feature Influence Modeling}\label{sec:GML}




 Since both \emph{CCD} and \emph{KNN} features are monotonic w.r.t class probability, we model their influence over class status by the sigmoid function as shown in Figure~\ref{fig:sigmoid}. Formally, denoting a \emph{CCD} feature by $f_c$, in which $c$ denotes a class, we model the influence of $f_c$ over a variable, $v$, by a unary factor defined as follows: 
\begin{equation}
	\label{eq:unary factor}
	\varphi_{f_c}(v) = 
	\left \{
	\begin{array}{ll}
	e^{w_{f_c}(v)}      &    
     if $ $ v = c;  \\
  1 &  if $ $ v\neq  c.
	\end{array} 
\right.
\end{equation}
where $w_{f_c}(v)$ denotes the factor weight of $v$, and 

\begin{equation}
\label{eq:unaryfactorweight}
  w_{f_c}(v) = \theta_{f_c}(v)\cdot \tau_{f_c}\cdot (x_{f_c}(v) - {\alpha_{f_c})},
\end{equation}
in which $\theta_{f_c}(v)$ denotes the confidence on influence modeling of $f_c$, $x_{f_c}(v)$ denotes the feature value of $v$, or the distance to the class centroid of $c$, and $\tau_{f_c}$ and $\alpha_{f_c}$ denote the steepness and mid-point of a sigmoid function respectively. In our implementation, as in previous GML work~\cite{hou2019gradual,HouTKDE}, we estimate $\theta_{f_c}(v)$ by the theory of regression error bound. The parameter values of $\tau_{f_c}$ and $\alpha_{f_c}$ are however supposed to be continuously optimized based on evidential observations in the process of gradual learning. 


	
	Similarly, we model the influence of the \emph{KNN} feature by the following binary factor: 
\begin{equation}
	\label{eq:binary factor}
	\varphi_{f_b}(v_i, v_j) = 
	\left \{
	\begin{array}{ll}
		e^{w_{f_b}(v_i,v_j)}     &      if \ v_i = v_j; \\
		1    &   otherwise.
	\end{array} 
	\right.
\end{equation}	
in which $f_b$ denotes the binary \emph{KNN} feature, $v_i$ and $v_j$ denote the two variables sharing the feature of $f_b$, $w_{f_b}(v_i,v_j)$ denotes the factor weight, and
\begin{equation}
\label{eq:unaryfactorweight}
  w_{f_b}(v_i,v_j) = \theta_{f_u}(v_i,v_j)\cdot \tau_{f_b}\cdot (x_{f_b}(v_i,v_j) - {\alpha_{f_b}}), 
\end{equation}
in which $\theta_{f_u}(v_i,v_j)$ denotes the confidence on binary feature influence modeling, $x_{f_b}(v_i,v_j)$ denotes the vector similarity of $v_i$ and $v_j$, and $\tau_{f_b}$ and $\alpha_{f_b}$ denote the steepness and mid-point of a sigmoid function. Similar to the unary factor, we estimate $\theta_{f_u}(v_i,v_j)$ by the theory of regression error bound, while $\tau_{f_b}$ and $\alpha_{f_b}$ need to be 
continuously optimized in the process of gradual learning. 
