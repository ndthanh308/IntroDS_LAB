\documentclass{scrartcl}
\usepackage[utf8]{inputenc}

\title{Gzip versus bag-of-words for text classification with KNN}
\subtitle{}
\author{Juri Opitz \\ \texttt{\normalsize opitz.sci@gmail.com} }
\date{}

%\usepackage{natbib}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{adjustbox}
\usepackage{xspace}
\usepackage{multicol}
\usepackage{appendix}
\usepackage{amsmath}
\usepackage{hyperref}

\begin{document}

\maketitle

\newcommand{\gzip}{\textsc{gzip}\xspace}
\newcommand{\simple}{\textsc{simple}\xspace}


\begin{abstract}
The effectiveness of compression distance in KNN-based text classification (`\gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple `bag-of-words' matching can achieve similar or better accuracy, and is more efficient. 
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

KNN-based text classification uses distance measurements between data points: For a given testing point, we first calculate its distance to every other point from some labeled training set. Then, given a manually selected $K$ parameter, we check the labels of the $K$ points that lie most closely to our testing point (i.e., its \textit{\textbf{K}-\textbf{N}eirest} \textbf{N}eighbors), and predict the label that we observe most frequently. 

An obvious upside of KNN-based text classification is its simplicity: there are only two parameters, $K$ and a distance function. Interestingly, recent findings \cite{jiang-etal-2023-low} suggest that we can exploit compression to assess the distance of two documents, by comparing their individual compression lengths to the length of their compressed concatenation (we call this distance measurement \gzip). With this approach, \cite{jiang-etal-2023-low} achieve strong text classification performance across different data sets, sometimes outperforming trained neural classifiers such BERT \cite{devlin-etal-2019-bert}. Against this background, it is not surprising that \gzip has quickly attracted lots of attention from machine learning researchers.\footnote{In only a few days, the \gzip github repository (\url{https://github.com/bazingagin/npc_gzip}) attracted more than 1.5k `stars', and \gzip-focused twitter micro blogs got several thousands of `likes'.} 

However, we find that there is more to the story:

\begin{enumerate}
    \item While indeed KNN-based classification is a strong baseline for text categorization, we find that it is competitive with trained models only in few-shot scenarios.
    \item We also show that there are simpler distance functions that work equally well as \gzip, if not better.
\end{enumerate}

\section{Experiments}

\subsection{Setup}

% Figure environment removed

\paragraph{Our simple approach} is shown in Figure \ref{fig:simple}.\footnote{Full code for experiments can be found here: \url{https://github.com/flipz357/npc_gzip_exp}} First, we apply standard string pre-processing such as removing dots and commas as well as lower-casing and token splitting at white space. Finally, we na\"ively filter out tokens that do not exceed some minimum amount of characters. The assumption that very short tokens tend to not contribute (so much) to the meaning of a text has been previously exploited in text classification, e.g., see \cite{2007:phd-Ana-Cardoso-Cachopo}. Finally, to compare two strings, we apply a simple bag-of-tokens set-distance based on the Jaccard index that dates back to at least 1884 \cite{gilbert1884finley}. 

\paragraph{Data sets and baselines} are mainly based on \cite{jiang-etal-2023-low}'s study. As our main testing data, we use KinyarwandaNews, KirundiNews, DengueFilipino, SwahiliNews. For few shot experiments, we also report results on large data sets such as DBpedia, YahooAnswers and AGNews. Since in DengueFilipino, it has been found that the testing data is the same as the training data\footnote{Presumably due to a bug in a hugging-face data loader, c.f.: \url{https://github.com/bazingagin/npc_gzip/issues/13}} we report results on DengueFilipino in brackets. Our main baseline is \gzip, which we re-calculate for most experiments (see next paragraph). Same as \cite{jiang-etal-2023-low}, we use K=2 in the KNN. Where possible, we also show results from BERT fine-tuning, extracting any scores from \cite{jiang-etal-2023-low}.

\paragraph{Evaluation scores} It is important to note that the original accuracy results reported in \cite{jiang-etal-2023-low} are \textit{optimistic}. This is because of the applied tie-breaking strategy needed for handling cases where the KNN selects a set of neighbors among which we cannot determine exactly one most frequent label. So in case there are two or more most frequent labels of different classes, including the right one, we select the latter. This has generated some discussion\footnote{E.g., see  \url{https://kenschutte.com/gzip-knn-paper/}}, since in a real world setting, we do not have access to the correct label at inference. Therefore, we report \textit{realistic} evaluation scores that are achieved by taking the mean of the optimistic score (if there is a tie involving the correct label, pick correct) and the pessimistic score (if there is a tie involving the correct label, pick incorrect). Since we use K=2, the realistic accuracy can be viewed as the expected accuracy when using a random guesser to break ties in KNN (see Appendix \ref{app:accuracy}).

\subsection{Main results}

\paragraph{Optimistic results} show that \simple performs better than \gzip on all data sets except SwahiliNews. The performance cannot be compared against BERT in this optimistic scenario, since BERT cannot profit from label-aware tie-breaking.

\paragraph{Realistic results} show that BERT trained on full training data outperforms all KNN based methods. Comparing \gzip and \simple, \gzip is better in KinyarwandaNews (+0.3 points) and SwahiliNews (+0.8 points), while \simple fares better on 20News (+1.4 points) and KirundiNews (+3.3 points), as well as DengueFilipino (+3.4 points). 

\begin{table}
    \centering
    \scalebox{0.96}{
    \begin{tabular}{lrrrrr}
    \toprule
   & 20News & KinyarwandaNews	& KirundiNews	& (DengueFilipino) & SwahiliNews \\
        BERT & \textbf{0.868} & 0.838 & 0.879 & (0.979) & 0.897 \\
        \gzip & 0.685 & 0.891	& 0.905	 & (0.998)	& \textbf{0.927} \\
        \simple  &  0.704 & \textbf{0.893} & \textbf{0.930} & (0.999) & 0.921 \\ 
        \bottomrule
    \end{tabular}}
    \caption{Results using full training data. Optimistic evaluation scores.}
    \label{tab:optimistic_results}
\end{table}

\begin{table}
    \centering
    \scalebox{0.96}{
    \begin{tabular}{lrrrrr}
    \toprule
  & 20News &   KinyarwandaNews	& KirundiNews	& (DengueFilipino) & SwahiliNews \\
        BERT & \textbf{0.868} & \textbf{0.838} & \textbf{0.879} & (\textbf{0.979}) & \textbf{0.897} \\
        \gzip & 0.563  & 0.828  &  0.807   & (0.851) &  0.842 \\
        \simple  & 0.578 &0.825 & 0.840 & (0.885) & 0.834 \\ 
        \bottomrule
    \end{tabular}}
    \caption{Results using full training data. Realistic evaluation scores.}
    \label{tab:realistic_results}
\end{table}

\subsection{Few-shot experiments and efficiency assessment}

It is interesting to study a few-shot scenario and assess model capability for low-resource settings where only few training data are available.\footnote{Note that few-shot also alleviates the quadratic complexity bottleneck of KNN.}

\begin{table}
    \centering
    \scalebox{0.96}{\begin{tabular}{lrrrrr}
    \toprule
  & 20News &   KinyarwandaNews	& KirundiNews	& (DengueFilipino) & SwahiliNews \\
        BERT & - & 0.240 &  \textbf{0.386} & (0.409) &  0.396 \\
        \gzip &  0.143 & 0.266 & 0.277 & (0.376) & 0.416 \\
        \simple  & \textbf{0.164} & \textbf{0.305} & 0.383 & (\textbf{0.545}) & \textbf{0.425} \\ 
        \bottomrule
    \end{tabular}}
    \caption{5-shot evaluation, mean over five runs. Realistic evaluation scores.}
    \label{tab:few-shot-5}
\end{table}

\paragraph{Realistic 5-shot evaluation results} are displayed in Table \ref{tab:few-shot-5}. We see that \simple offers same or higher performance than \gzip on all tested data sets, and also outperforms BERT (except for KirundiNews, where BERT fine-tuning worked slightly better).


\paragraph{Time-efficiency and 100-shot results} are displayed in Table \ref{tab:few-shot-100}. We see that \simple tends to offer better performance than \gzip on all data sets, while performing a much faster distance calculation (E.g., in DBpedia and YahooAnswers \simple reduces the inference time from hours to minutes). 

\begin{table}
    \centering
   \scalebox{0.96}{ \begin{tabular}{lrrrrrr}
    \toprule
  & \multicolumn{2}{c}{AGNews} &  \multicolumn{2}{c}{DBPedia}	& \multicolumn{2}{c}{YahooAnswers} \\ 
  & acc$\uparrow$ & time$\downarrow$ & acc$\uparrow$ & time$\downarrow$ &acc$\uparrow$ &time$\downarrow$  \\
        \gzip & 0.549 & 244.9s & 0.748 & 57073.1s & 0.212 & 6693.8s \\
        \simple & \textbf{0.581} & \textbf{7.9s} & \textbf{0.812} & \textbf{316.2s} & \textbf{0.216} & \textbf{308.1} \\
        \bottomrule
    \end{tabular}}
    \caption{100-shot evaluation of large data sets, mean over five runs. Realistic evaluation scores.}
    \label{tab:few-shot-100}
\end{table}

\subsection{Assessing hyper-parameter of SIMPLE}

 In Table \ref{tab:filter_ablation}, we study the robustness of \simple in different configurations. Regarding the removel of tokens that do not exceed a length of $n$, we find that lower values of $n$ seem to similarly well. However, when $n$ is increased too much, say $n=10$, we filter out too many tokens and the performance degrades notably (see last row of Table \ref{tab:filter_ablation}). Abstaining from the removal of dots and commas and also lower-casing tends to result in lower performance, particularly for 20News (incurring an accuracy loss of 2.8 points).

\begin{table}
    \centering
    \scalebox{0.91}{\begin{tabular}{lrrrrr}
    \toprule
    \simple variant & 20News &  KinyarwaNews & KirundiNews	& (DengueFilipino) & SwahiliNews \\ 
    \midrule
        basic (n=3) & 0.578 & 0.825 & 0.840 & \textbf{0.885} & 0.834 \\   
        n=0 & 0.574 &\textbf{ 0.829} & 0.838 & 0.865 & 0.830 \\
        n=1 & 0.571 & 0.827 & \textbf{0.856} & 0.865 & 0.832 \\
        n=2 & \textbf{0.579} & 0.826 & 0.841 & 0.876 & 0.834 \\
        n=4 & 0.571 & 0.826 & 0.851 & 0.879 & 0.832 \\
        n=10 & 0.411 & 0.734 & 0.770 & 0.418 & 0.565 \\ 
        only split, n=3 & 0.550 &  0.826 & 0.851 & 0.871 & 0.833 \\
        only split, n=0 & 0.538 & 0.829 & 0.838 & 0.842 & 0.835 \\
        \midrule
        \gzip baseline & 0.563 & 0.828 & 0.807 & 0.851 & \textbf{0.842} \\
        \bottomrule
    \end{tabular}}
    \caption{\simple variants, removing tokens with length $\leq$ n, and abstaining from any string normalization (only split, i.e., no removal of dots and commas, no lower-casing), with and without removing tokens that do not exceed a certain length. Full training data, realistic evaluation scores.}
    \label{tab:filter_ablation}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and discussion}

For a KNN-text classifier, we find that distance measurement based on `bag-of-words' is a strong alternative to \gzip. A disadvantage of \simple against \gzip may be that its representation is lossy in the sense that we cannot reconstruct the exact input text, which may potentially be relevant for some applications. However, this theoretical downside may be outweighed by \simple's much greater efficiency. %Finally, we believe that it can be interesting to see if we can use \gzip in conjunction with text normalization to improve its results.




\bibliographystyle{plain}
\bibliography{references}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

\begin{appendices}
\section{Expected accuracy}
\label{app:accuracy}
Let us first view how we can calculate expected accuracy of KNN in the general case with any $K$. Here, we understand as the expected accuracy the accuracy that we achieve on average if we use a random guesser to break ties. 

Consider $c^t$ the number of candidates in a tie $t$. So, on a data set with $N$ untied examples and $T$ tied examples, we have
\begin{equation}
E[Accuracy]= \frac{\sum_{i=1}^N I[pred(i) = label(i)] + \sum_{t=1}^T 1/c^t}{N+T},
\end{equation}
where $I[x]$ returns 1 if $x$ is true and and 0 else. When $K=2$, we have $c^t=\frac{1}{2} ~\forall t$, i.e.,

\begin{align}
   %& \frac{\sum_{i=1}^N I[pred(i) = label(i)] + \sum_{t=1}^T 1/c^t}{N+T} \\
    & \frac{\sum_{i=1}^N I[pred(i) = label(i)] + T/2}{N+T} \\
    =~& \frac{1}{2}\bigg(\frac{\sum_{i=1}^N I[pred(i) = label(i)] + T}{N+T} + \frac{\sum_{i=1}^N I[pred(i) = label(i)]}{N+T}\bigg) \\
    =~& \frac{1}{2}\bigg(O + P\bigg), 
\end{align}
\end{appendices}

with $O$ the optimistic accuracy and $P$ the pessimistic accuracy.



\end{document}
%\citep{adams1995hitchhiker}
