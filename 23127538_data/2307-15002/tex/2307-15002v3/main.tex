% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1

\documentclass{scrartcl}
\usepackage[utf8]{inputenc}

\title{Gzip versus bag-of-words for text classification with KNN}
\subtitle{}
\author{Juri Opitz \\ \texttt{\normalsize opitz.sci@gmail.com} }
\date{}

\usepackage{comment}
\usepackage{booktabs}
\usepackage{url}
\usepackage{adjustbox}
\usepackage{xspace}
\usepackage{multicol}
\usepackage{appendix}
\usepackage{amsmath}
\usepackage{hyperref}
%\usepackage{minted}
\usepackage{xcolor}
\usepackage{listings}


\lstset{
language=Python,
basicstyle=\ttfamily,
keywordstyle=\bfseries,
otherkeywords={self},             
keywordstyle=\ttfamily\color{blue!90!black},
keywords=[2]{True,False},
keywords=[3]{ttk},
keywordstyle={[2]\ttfamily\color{yellow!80!orange}},
keywordstyle={[3]\ttfamily\color{red!80!orange}},
emph={MyClass,__init__},          
emphstyle=\ttfamily\color{red!80!black},    
stringstyle=\color{red!80!black},
showstringspaces=false            
}

%\lstset{emph={%  
%    string2set, distance%
%    },emphstyle={\bfseries}%
%}%


\begin{document}

\maketitle

\newcommand{\gzip}{\textsc{gzip}\xspace}
\newcommand{\simple}{\textsc{simple}\xspace}


\begin{abstract}
The effectiveness of compression distance in KNN-based text classification (`\gzip') has recently garnered lots of attention. In this note we show that simpler means can also be effective, and compression may not be needed. Indeed, a `bag-of-words' matching can achieve similar or better results, and is more efficient. 
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

KNN-based text classification uses distance measurements between data points: For a given testing point, we first calculate its distance to every other point from some labeled training set. Then, given a manually selected $K$ parameter, we check the labels of the $K$ points that lie most closely to our testing point (i.e., its \textit{\textbf{K}-\textbf{N}eirest} \textbf{N}eighbors), and predict the label that we observe most frequently. 

An obvious upside of KNN-based text classification is its simplicity: KNN has only two parameters, $K$ and a distance function. Interestingly, recent findings \cite{jiang-etal-2023-low} suggest that we can exploit compression to assess the distance of two documents, by comparing their individual compression lengths to the length of their compressed concatenation (we call this distance measurement \gzip). With this approach, \cite{jiang-etal-2023-low} show strong text classification performance across different data sets, sometimes achieving higher accuracy than trained neural classifiers such as BERT \cite{devlin-etal-2019-bert}. Against this background, it is not surprising that \gzip has quickly attracted lots of attention.\footnote{In only a few days, the \gzip github repository (\url{https://github.com/bazingagin/npc_gzip}) attracted more than 1.5k `stars', and \gzip-focused twitter micro blogs got several thousands of `likes'.} 


However, we find that there is more to the story:

\begin{enumerate}
    \item While KNN overall seems useful in few-shot setups, after correcting optimistic results in \cite{jiang-etal-2023-low}, we observe some accuracy loss when using \gzip distance.
    
    \item We show that a much simpler distance based on `bag-of-words' works equally well as \gzip, if not better.
\end{enumerate}

\section{Experiments}

\subsection{Setup}

%% Figure environment removed

\begin{comment}
% Figure environment removed
\end{comment}

% Figure environment removed


\paragraph{Our simple approach} is shown in Figure \ref{fig:simple}.\footnote{Full code for experiments can be found here: \url{https://github.com/flipz357/npc_gzip_exp}} First we apply string pre-processing such as removing dots and commas as well as lower-casing and token splitting at white space. Then we na\"ively filter out tokens that do not exceed some minimum amount of characters. The assumption that very short tokens tend to not contribute (so much) to the meaning of a text has been previously exploited in text classification, e.g., see \cite{2007:phd-Ana-Cardoso-Cachopo}. Finally, to compare two strings, we apply a simple bag-of-tokens set-distance based on the Jaccard index that dates back to at least 1884 \cite{gilbert1884finley}. 

\paragraph{Data sets and baselines} are mainly based on \cite{jiang-etal-2023-low}'s study. We use 20News, KinyarwandaNews, KirundiNews, DengueFilipino, SwahiliNews. For few shot experiments, we also report results on large data sets such as DBpedia, YahooAnswers and AGNews, and on an additional Twitter sentiment classification task. Since in DengueFilipino, it has been found that the testing data is the same as the training data\footnote{Presumably due to a bug in a hugging-face data loader, c.f.: \url{https://github.com/bazingagin/npc_gzip/issues/13}} we report results on DengueFilipino in brackets. Our main baseline is \gzip, which we re-calculate for most experiments (see next paragraph). Same as \cite{jiang-etal-2023-low}, we mostly use K=2 in the KNN. Where possible, we also show results from BERT fine-tuning, extracting any scores from \cite{jiang-etal-2023-low}. All experiments are run on a laptop with i5-8350 CPU and 16GB memory.

\paragraph{Evaluation scores} It is important to note that the original accuracy results reported in \cite{jiang-etal-2023-low} are \textit{optimistic}. This is because of the applied tie-breaking strategy needed for handling cases where the KNN selects a set of neighbors among which we cannot determine exactly one most frequent label. So in case there are two or more most frequent labels of different classes, including the right one, we select the latter. This has generated some discussion\footnote{E.g., see  \url{https://kenschutte.com/gzip-knn-paper/}}, since in a real world setting, we do not have access to the correct label at inference. Therefore, we report \textit{realistic} evaluation scores that are achieved by two options: \begin{enumerate}
    \item To best compare with \cite{jiang-etal-2023-low}, who mainly use optimistic K=2, we (re-)calculate realistic scores by taking the mean of the optimistic score (if there is a tie involving the correct label, pick correct) and the pessimistic score (if there is a tie involving the correct label, pick incorrect). The realistic accuracy can be viewed as the expected accuracy when using a random guesser to break ties in KNN (see Appendix \ref{app:accuracy}).
    \item Another realistic score is calculated by using K=1, which is equivalent to K=2 when applying a tie breaking strategy that is informed by distances. We denote these models \gzip-1 and \simple-1. 
\end{enumerate} 

\subsection{Main results}

\paragraph{Optimistic results} show that \simple performs better than \gzip on all data sets except SwahiliNews. The performance cannot be compared against BERT in this optimistic scenario, since BERT cannot profit from label-aware tie-breaking.

\paragraph{Realistic results} show that BERT trained on full training data tends to provide best accuracy. This is particularly true for 20News, which is the most established data set (+24.9 points vs.\ \simple, +26.1 vs.\ \gzip). Comparing \gzip and \simple with random tie-breaking, \gzip is better in KinyarwandaNews (+0.3 points) and SwahiliNews (+0.8 points), while \simple fares better on 20News (+1.4 points) and KirundiNews (+3.3 points). When using K=1, both \gzip and \simple improve results, but the overall tendency is similar (\simple vs.\ \gzip, 20News: +1.2, KinyarwandaNews: +1.0, KirundiNews +1.2, SwahiliNews: -0.9). 

\begin{table}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{lrrrrr}
    \toprule
   & 20News & KinyarwandaNews	& KirundiNews	& (DengueFilipino) & SwahiliNews \\
        BERT & \textbf{0.868} & 0.838 & 0.879 & (0.979) & 0.897 \\
        \midrule
        \gzip & 0.685 & 0.891	& 0.905	 & (0.998)	& \textbf{0.927} \\
        \simple  &  0.704 & \textbf{0.893} & \textbf{0.930} & (0.999) & 0.921 \\ 
        \bottomrule
    \end{tabular}}
    \caption{Results using full training data. Optimistic evaluation scores.}
    \label{tab:optimistic_results}
\end{table}

\begin{table}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{lrrrrr}
    \toprule
  & 20News &   KinyarwandaNews	& KirundiNews	& (DengueFilipino) & SwahiliNews \\
        BERT & \textbf{0.868} & 0.838 & \textbf{0.879} & (\textbf{0.979}) & \textbf{0.897} \\
        \midrule
        \gzip & 0.563  & 0.828  &  0.807   & (0.851) &  0.842 \\
        \simple  & 0.578 &0.825 & 0.840 & (0.885) & 0.834 \\ 
                \midrule
        \gzip-1 & 0.607 &  0.835 &   0.858  &  (0.999)  &  0.850 \\
        \simple-1 & \textbf{0.619} & \textbf{0.845} & 0.870 & (0.997) & 0.841 \\
        \bottomrule
    \end{tabular}}
    \caption{Results using full training data. Realistic evaluation scores.}
    \label{tab:realistic_results}
\end{table}

\subsection{Few-shot experiments and efficiency assessment}

It is interesting to study \gzip or \simple in a few-shot scenario, since `non-parametric' methods like KNN seem particularly attractive in a low-resource setting.

\begin{table}
    \centering
    \scalebox{0.9}{\begin{tabular}{lrrrrr}
    \toprule
  & 20News &   KinyarwandaNews	& KirundiNews	& (DengueFilipino) & SwahiliNews \\
  \midrule
        BERT & - & 0.240 & 0.386 & (0.409) &  0.396 \\
        \midrule
        \gzip &  0.143 & 0.266 & 0.277 & (0.376) & 0.416 \\
        \simple  & 0.164 & 0.305 & 0.383 & (0.545) & 0.425 \\ 
        \midrule
        \gzip-1 & 0.166 & 0.340 & 0.350 & (0.456) & 0.426\\
        \simple-1 & \textbf{0.194} & \textbf{0.345} & \textbf{0.451} & (\textbf{0.538}) & \textbf{0.673}\\
        \bottomrule
    \end{tabular}}
    \caption{5-shot evaluation, mean over five runs. Realistic evaluation scores.}
    \label{tab:few-shot-5}
\end{table}

\paragraph{Realistic 5-shot evaluation results} are displayed in Table \ref{tab:few-shot-5}. We see that \simple offers same or higher performance than \gzip on all tested data sets, and also outperforms BERT (except for KirundiNews, where BERT fine-tuning worked slightly better).


\paragraph{Time-efficiency and \{5,100\}-shot results} on larger English  data are displayed in Table \ref{tab:few-shot-100}. We see that \simple offers better performance than \gzip across all data sets, while performing a much faster distance calculation (E.g., in DBpedia and YahooAnswers \simple reduces the inference time from hours to minutes). In Appendix \ref{app:100-5-shot-big} (Table \ref{tab:few-shot-100-k1}) we see that the same insight holds for \gzip-1 and \simple-1.

\begin{table}
    \centering
   \scalebox{0.9}{ \begin{tabular}{lrrrrrrr}
    \toprule
  & & \multicolumn{2}{c}{AGNews} &  \multicolumn{2}{c}{DBPedia}	& \multicolumn{2}{c}{YahooAnswers} \\ 
  & shots & acc$\uparrow$ & time$\downarrow$ & acc$\uparrow$ & time$\downarrow$ &acc$\uparrow$ &time$\downarrow$  \\
  \midrule
        \gzip & 100 &  0.549 & 244.9s & 0.748 & 57073.1s & 0.212 & 6693.8s  \\
        \simple & 100 & \textbf{0.581}  & \textbf{7.9s} & \textbf{0.812} & \textbf{316.2s} & \textbf{0.216} & \textbf{308.1s} \\
        \midrule
        \gzip & 5 &  0.313 & 9.9s & 0.316 & 383.3s & 0.141 & 277.0s \\
        \simple & 5 & \textbf{0.384}  & \textbf{0.6s} & \textbf{0.592} & \textbf{15.8s} & \textbf{0.153} & \textbf{13.3s} \\
        \bottomrule
    \end{tabular}}
    \caption{100- and 5-shot evaluation of large data sets, mean over five runs. Realistic evaluation scores.}
    \label{tab:few-shot-100}
\end{table}

\paragraph{Twitter sentiment few-shot classification} We retrieve an additional, refined twitter sentiment classification data set with balanced class distribution of positive and negative sentiment.\footnote{The data can be downloaded from \url{https://github.com/cblancac/SentimentAnalysisBert/tree/main/data}. See \url{https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis} for more information.} The random baseline here is simply 0.5 accuracy. Results are displayed in Table \ref{tab:few-shot-twitter}: for very low number of shots (5), both \simple and \gzip perform randomly. When more shots are available, \simple outperforms \gzip across the board with about +2 to +4 accuracy points.

\begin{table}
    \centering
   \scalebox{0.75}{ \begin{tabular}{lrrrrrrrrrrrrr}
    \toprule
  &  \multicolumn{2}{c}{5-shot} &  \multicolumn{2}{c}{25-shot}	& \multicolumn{2}{c}{50-shot} & \multicolumn{2}{c}{100-shot} & \multicolumn{2}{c}{250-shot} & \multicolumn{2}{c}{1000-shot}\\
  &  acc$\uparrow$ & time$\downarrow$ & acc$\uparrow$ & time$\downarrow$ &acc$\uparrow$ &time$\downarrow$&acc$\uparrow$ &time$\downarrow$ &acc$\uparrow$ &time$\downarrow$ &acc$\uparrow$ &time$\downarrow$ \\
  \midrule
  \gzip & 0.502 &  30.9s & 0.516 & 164.9s & 0.523 & 313.1s & 0.521 &724.3s & 0.539 & 1914.4s & 0.557 & 6894.8s \\
  \simple & 0.503 & \textbf{2.2s} & \textbf{0.535} & \textbf{5.6s} & \textbf{0.546} & \textbf{10.5s}  & \textbf{0.554} & \textbf{18.3s} & \textbf{0.561} & \textbf{54.0s} & \textbf{0.594} & \textbf{262.3s} \\
  \midrule 
  \gzip-1 &0.513&29.3s&0.515&183.2s&0.530&302.1s& 0.533 & 733.0s & 0.547 & 1912.8s & 0.559 & 7208.0s  \\
  \simple-1 &0.513&\textbf{2.6s}&\textbf{0.534}&\textbf{6.5s}&\textbf{0.547}&\textbf{9.9s}&\textbf{0.558} & \textbf{19.1s} & \textbf{0.576} & \textbf{52.3s} & \textbf{0.597} & \textbf{271.2s}\\
        \bottomrule
    \end{tabular}}
    \caption{Few-shot evaluation on Twitter sentiment, mean over five runs. Realistic evaluation scores.}
    \label{tab:few-shot-twitter}
\end{table}

\subsection{Assessing hyper-parameter of SIMPLE}

 In Table \ref{tab:filter_ablation}, Appendix \ref{app:hyper-param}, we study \simple in different configurations. Regarding the removal of tokens that do not exceed a length of $n$, we find that lower values of $n$ seem to work similarly well. However, when $n$ is increased too much, say $n=10$, we filter out too many tokens and the performance degrades notably (see last row `n=10' in \ref{tab:filter_ablation}). Abstaining from the removal of dots, commas and leaving case tends to result in lower performance, particularly for 20News (incurring an accuracy loss of 2.8 points).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and discussion}

In a KNN-classifier, we find that distance measurement of two documents based on `bag-of-words' (\simple) seems preferable to \gzip, since it achieves similar or better performance, especially in few-shot scenarios. A disadvantage of \simple against \gzip may be that its representation is lossy. However, this theoretical downside seems outweighed by \simple's  greater efficiency. 




\bibliographystyle{plain}
\bibliography{references}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

\begin{appendices}
\section{Expected accuracy}
\label{app:accuracy}
Let us first view how we can calculate expected accuracy of KNN in the general case with any $K$. Here, we understand as the expected accuracy the accuracy that we achieve on average if we use a random guesser to break ties. 

Consider $c^t$ the number of candidates in a tie $t$. So, on a data set with $N$ untied examples and $T$ tied examples, we have
\begin{equation}
E[Accuracy]= \frac{\sum_{i=1}^N I[pred(i) = label(i)] + \sum_{t=1}^T 1/c^t}{N+T},
\end{equation}
where $I[x]$ returns 1 if $x$ is true and and 0 else. When $K=2$, we have $c^t=\frac{1}{2} ~\forall t$, i.e.,

\begin{align}
    & \frac{\sum_{i=1}^N I[pred(i) = label(i)] + T/2}{N+T} \\
    =~& \frac{1}{2}\bigg(\frac{\sum_{i=1}^N I[pred(i) = label(i)] + T}{N+T} + \frac{\sum_{i=1}^N I[pred(i) = label(i)]}{N+T}\bigg) \\
    =~& \frac{1}{2}\bigg(O + P\bigg), 
\end{align}
\end{appendices}

with $O$ the optimistic accuracy and $P$ the pessimistic accuracy.

\section{Few-shot experiment results with K=1}
\label{app:100-5-shot-big}

See Table \ref{tab:few-shot-100-k1}.

\begin{table}
    \centering
   \scalebox{0.96}{ \begin{tabular}{lrrrr}
    \toprule
  & shots& AGNews &  DBPedia	& YahooAnswers \\ 
  \midrule
        \gzip-1 & 100 & 0.599 & 0.779 & 0.227  \\
        \simple-1 & 100 & \textbf{0.611} & \textbf{0.825} & \textbf{0.234} \\
        \midrule
        \gzip-1 & 5 & 0.374 & 0.502 & 0.148 \\
        \simple-1 & 5 & \textbf{0.405} & \textbf{0.638} & \textbf{0.165} \\
        \bottomrule
    \end{tabular}}
    \caption{100- and 5-shot evaluation of large data sets, mean over five runs. Realistic evaluation scores.}
    \label{tab:few-shot-100-k1}
\end{table}

\section{Hyper-parameters of \simple}

See Table \ref{tab:filter_ablation}. All presented numbers are expected (realistic) accuracy scores of using  K=2 and random tie-breaking.
\label{app:hyper-param}
\begin{table}
    \centering
    \scalebox{0.91}{\begin{tabular}{lrrrrr}
    \toprule
    \simple variant & 20News &  KinyarwaNews & KirundiNews	& (DengueFilipino) & SwahiliNews \\ 
    \midrule
        basic (n=3) & 0.578 & 0.825 & 0.840 & \textbf{0.885} & 0.834 \\   
        n=0 & 0.574 &\textbf{ 0.829} & 0.838 & 0.865 & 0.830 \\
        n=1 & 0.571 & 0.827 & \textbf{0.856} & 0.865 & 0.832 \\
        n=2 & \textbf{0.579} & 0.826 & 0.841 & 0.876 & 0.834 \\
        n=4 & 0.571 & 0.826 & 0.851 & 0.879 & 0.832 \\
        n=10 & 0.411 & 0.734 & 0.770 & 0.418 & 0.565 \\ 
        keep only $n\leq 3$ & 0.360 & 0.661 & 0.839 & 0.834 & 0.545 \\
        only split, n=3 & 0.550 &  0.826 & 0.851 & 0.871 & 0.833 \\
        only split, n=0 & 0.538 & 0.829 & 0.838 & 0.842 & 0.835 \\
        \bottomrule
    \end{tabular}}
    \caption{\simple variants, removing tokens with length $\leq$ n, and abstaining from any string normalization (only split, i.e., no removal of dots and commas, no lower-casing), with and without removing tokens that do not exceed a certain length. Full training data, realistic evaluation scores.}
    \label{tab:filter_ablation}
\end{table}

\end{document}
%\citep{adams1995hitchhiker}
