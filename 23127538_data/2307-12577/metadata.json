{
  "title": "PRIOR: Prototype Representation Joint Learning from Medical Images and Reports",
  "authors": [
    "Pujin Cheng",
    "Li Lin",
    "Junyan Lyu",
    "Yijin Huang",
    "Wenhan Luo",
    "Xiaoying Tang"
  ],
  "submission_date": "2023-07-24T07:49:01+00:00",
  "revised_dates": [
    "2023-09-28T00:19:47+00:00",
    "2024-03-12T01:21:34+00:00"
  ],
  "abstract": "Contrastive learning based vision-language joint pre-training has emerged as a successful representation learning strategy. In this paper, we present a prototype representation learning framework incorporating both global and local alignment between medical images and reports. In contrast to standard global multi-modality alignment methods, we employ a local alignment module for fine-grained representation. Furthermore, a cross-modality conditional reconstruction module is designed to interchange information across modalities in the training phase by reconstructing masked images and reports. For reconstructing long reports, a sentence-wise prototype memory bank is constructed, enabling the network to focus on low-level localized visual and high-level clinical linguistic features. Additionally, a non-auto-regressive generation paradigm is proposed for reconstructing non-sequential reports. Experimental results on five downstream tasks, including supervised classification, zero-shot classification, image-to-text retrieval, semantic segmentation, and object detection, show the proposed method outperforms other state-of-the-art methods across multiple datasets and under different dataset size settings. The code is available at https://github.com/QtacierP/PRIOR.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12577",
  "pdf_url": null,
  "comment": "Accepted by ICCV 2023",
  "num_versions": null,
  "size_before_bytes": 4126810,
  "size_after_bytes": 472363
}