\begin{thebibliography}{10}

\bibitem{Bengio1994}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE Transactions on Neural Networks}, 5(2):157--166, 1994.

\bibitem{Courbariaux2016}
M.~Courbariaux, I.~Hubara, D.~Soudry, R.~El-Yaniv, and Y.~Bengio.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to +1 or -1.
\newblock {\em arXiv preprint arXiv:1602.02830}.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{Gupta2015}
S.~Gupta, A.~Agrawal, K.~Gopalakrishnan, and P.~Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning (ICML-15)}, pages 1737--1746, 2015.

\bibitem{Han2015}
S.~Han, H.~Mao, and W.~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}.

\bibitem{He2015}
K.~He et~al.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem{batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning}, pages 448--456, 2015.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{maxpool}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Proceedings of the 25th International Conference on Neural
  Information Processing Systems - Volume 1}, pages 1097--1105, 2012.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{Micikevicius2017}
P.~Micikevicius et~al.
\newblock Mixed precision training.
\newblock {\em arXiv preprint arXiv:1710.03740}.

\bibitem{ruder2017overview}
S.~Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock {\em arXiv preprint arXiv:1609.04747}, 2017.

\bibitem{Rumelhart1986}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533--536, 1986.

\bibitem{tieleman2012lecture}
T.~Tieleman and G.~Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock COURSERA: Neural Networks for Machine Learning, 2012.

\bibitem{yun2023defense}
J.~Yun et~al.
\newblock In defense of pure 16-bit floating-point neural networks.
\newblock {\em arXiv preprint arXiv:2305.10947}, 2023.

\end{thebibliography}
