In the realm of deep learning, achieving trustworthiness and reliability is paramount, especially when the field faces persistent challenges like numerical instability. Our research introduces a groundbreaking strategy designed to bolster the trustworthiness of neural networks, which is most evident during backpropagation processes. By innovatively tweaking the gradient update mechanism in the Adam optimizer, we've effectively curtailed the prevalent instability issues encountered in 16-bit neural training. Empirical tests leveraging the MNIST dataset with a rudimentary DNN underscored the superiority of our optimizer, frequently outclassing the conventional Adam setup in a 16-bit environment. Impressively, our trustworthy 16-bit methodology managed to truncate training durations by approximately 45\% relative to its 32-bit counterpart, simultaneously preserving stability across an array of epsilon magnitudes. Further experiments on the Vision Transformer model for the cifar-10 dataset substantiated the enhanced trustworthiness of our approach: our 16-bit models processed data at a rate 30\% swifter than the 32-bit models while ensuring heightened stability. Nonetheless, our investigations were primarily concentrated on image classification through Linear DNN and VIT. Anticipating future endeavors, we aim to delve into the reliability and trustworthiness of our strategy when applied to intricate architectures and diverse tasks, including NLP. Our overarching ambition is to architect a holistic solution to combat numerical instability in deep learning, and we're confident that our current contributions establish a robust groundwork for ensuing advancements in ensuring model trustworthiness.