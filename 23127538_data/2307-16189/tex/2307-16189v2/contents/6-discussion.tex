The novel approach for mitigating numerical instability presented in this work has demonstrated promising results with respect to the backpropagation phase of deep learning models when dealing with a simple Linear Layer. Our modification to the gradient computation equation within the Adam optimizer ensures that the denominator stays within a safe range, thereby providing a stable update for the gradient. This has the potential to significantly enhance the stability of the training process, and by extension, the accuracy of the resultant model. However, as in all scientific studies, this work is not without its limitations. The empirical validation of our proposed method has been conducted with image classification model such as the vision transformer models. While this forms the basis for more complex architectures, it remains only one part of training of the wide array of layers and structures currently utilized in modern deep learning architectures. Specifically, future research should aim to validate the robustness and utility of our approach across a broader spectrum of deep learning architectures. In particular, Transformer models for Natural Language Processing is a significant portion of the current deep learning landscape, particularly in language-related tasks. The complexity of these models, with their intricate hierarchies and highly non-linear transformations, may pose additional challenges that are not fully encapsulated by a Linear Layer. As such, it is imperative that further validation is conducted on these architectures to establish the generalizability of our method. Furthermore, while our focus has primarily been on mitigating numerical instability, it would also be beneficial to investigate any potential side effects this approach might have on other aspects of the model training process. For instance, it would be interesting to explore the implications for training time, memory requirements, and robustness to variations in hyperparameters. In conclusion, this work presents an exciting step forward in the pursuit of more robust and stable training methodologies for deep learning models. The road ahead, while challenging, is filled with opportunities for further innovation and refinement.