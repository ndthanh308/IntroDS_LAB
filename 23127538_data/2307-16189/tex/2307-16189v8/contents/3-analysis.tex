Through theoretical analysis, we analyze which part of the neural network occurs numerical instability which can ruin the training process.

\subsection{Forward Propagation}
\noindent\textbf{Linear Network.} During forward propagation, each column of the input matrix $X$ represents a distinct training example. This matrix layout simplifies the network's computation, as each feature of every training sample can be processed in parallel, exploiting the parallel nature of matrix operations.
\begin{equation*}
X = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1m} \\
x_{21} & x_{22} & \cdots & x_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nm}
\end{bmatrix}
\end{equation*}
The weight matrix $W$ represents the strength and direction of connections between neurons. Each column of $W$ corresponds to the set of weights connecting every input neuron to a specific hidden neuron.
\begin{equation*}
W = \begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1k} \\
w_{21} & w_{22} & \cdots & w_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
w_{n1} & w_{n2} & \cdots & w_{nk}
\end{bmatrix}
\end{equation*}
The bias vector $b$ provides an additional degree of freedom, allowing each neuron in the hidden layer to be activated not just based on weighted input but also based on this inherent property.
\begin{equation*}
b = \begin{bmatrix}
b_{1} \\
b_{2} \\
\vdots \\
b_{k}
\end{bmatrix}
\end{equation*}
The raw outputs or pre-activations (denoted as $Z$) are computed by multiplying the transposed weight matrix with the input matrix and adding the bias. 
\begin{equation*}
Z = W^T X + b
\end{equation*}
After computing $Z$, we need a mechanism to introduce non-linearity into our model. Without this, no matter how deep our model, it would behave just like a single-layer perceptron. This non-linearity is introduced using an activation function $\sigma(\cdot)$, applied element-wise to the matrix $Z$.
\begin{equation*}
A = \sigma(Z)
\end{equation*}
Here, each entry $a_{ij}$ in $A$ is the activated value of the $j$-th neuron in the hidden layer when the model is provided the $i$-th training example. These activations will either be used as input for subsequent layers or be the final output of the network.

In Deep Neural Network (Linear Network.), each operation in the forward pass involves straightforward mathematical operations like addition and multiplication. There is no division or complex operation that could amplify small numerical errors or lead to potential instability. \\

\noindent\textbf{Convolutional Network.} In the realm of deep learning, especially when processing image data, CNNs have gained significant prominence. The foundational blocks of CNNs involve convolving filters over input data and pooling layers \cite{lecun1998gradient}. 

The convolution layer can be detailed by observing the element-wise multiplication and summation \cite{lecun1998gradient}. Specifically, in our 3x3 input matrix \(I\) and 2x2 filter \(F\) example:
\begin{align*}
I &= \begin{bmatrix}
i_{11} & i_{12} & i_{13} \\
i_{21} & i_{22} & i_{23} \\
i_{31} & i_{32} & i_{33}
\end{bmatrix}
&
F &= \begin{bmatrix}
f_{11} & f_{12} \\
f_{21} & f_{22}
\end{bmatrix}
\end{align*}
For the top-left corner of \(I\), the convolution operation using the filter \(F\) is as follows:
\begin{equation*}
o_{11} = \begin{bmatrix}
i_{11} & i_{12} \\
i_{21} & i_{22}
\end{bmatrix} 
\odot 
\begin{bmatrix}
f_{11} & f_{12} \\
f_{21} & f_{22}
\end{bmatrix}
\end{equation*}
Where \(\odot\) represents element-wise multiplication. This implies:
\begin{equation*}
o_{11} = i_{11} \cdot f_{11} + i_{12} \cdot f_{12} + i_{21} \cdot f_{21} + i_{22} \cdot f_{22}
\end{equation*}
The filter \(F\) continues sliding across \(I\), performing similar computations for each position, resulting in the matrix:
\begin{equation*}
O = \begin{bmatrix}
o_{11} & o_{12} \\
o_{21} & o_{22}
\end{bmatrix}
\end{equation*}
Each element in the output matrix \(O\) is computed in the same fashion, with each \(o_{ij}\) being the result of convolving a 2x2 segment of \(I\) with the filter \(F\).

Pooling layers can also be represented using matrix notation, though the operation is simpler. For max pooling with a 2x2 window, given an input matrix $I$, the operation for a single position can be illustrated as \cite{maxpool}:
\begin{equation*}
P_{ij} = \max \begin{bmatrix}
I_{2i-1, 2j-1} & I_{2i-1, 2j} \\
I_{2i, 2j-1} & I_{2i, 2j}
\end{bmatrix}
\end{equation*}
For an example 4x4 input matrix being processed by a 2x2 max pooling operation:
\begin{equation*}
I = \begin{bmatrix}
i_{11} & i_{12} & i_{13} & i_{14} \\
i_{21} & i_{22} & i_{23} & i_{24} \\
i_{31} & i_{32} & i_{33} & i_{34} \\
i_{41} & i_{42} & i_{43} & i_{44}
\end{bmatrix}
\end{equation*}
The output matrix $P$ from this max pooling operation will be:
\begin{equation*}
P = \begin{bmatrix}
\max(i_{11}, i_{12}, i_{21}, i_{22}) & \max(i_{13}, i_{14}, i_{23}, i_{24}) \\
\max(i_{31}, i_{32}, i_{41}, i_{42}) & \max(i_{33}, i_{34}, i_{43}, i_{44})
\end{bmatrix}
\end{equation*}
In CNNs, actions taken during the forward pass primarily consist of basic arithmetic, such as addition and multiplication. There's an absence of division or intricate calculations that might magnify minor numerical inaccuracies or trigger potential instability. 

Especially those with a significant number of layers, the iterative multiplications during forward propagation can intermittently lead to either vanishingly small values or notably large increments in the activations. However, methods such as batch normalization\cite{batch} have been introduced to regulate and prevent these activations from attaining extreme values. Therefore, forward propagation, which involves the transmission of input data through the network to produce an output, is generally more resilient to such instabilities. This comparative robustness can be ascribed to a variety of underlying reasons:
\\
\begin{itemize}
\item Simplified Operations: Forward propagation predominantly involves elementary mathematical operations such as addition and multiplication. Thus, the results are less prone to reaching extreme values unless the input data or model parameters are improperly scaled or exhibit a high dynamic range \cite{He2015}.

\item Absence of Derivatives: Contrasting backpropagation, forward propagation does not necessitate the computation of derivatives, a process that could engender exceedingly large or small numbers, thus inducing numerical instability \cite{Rumelhart1986}.

\item Limited Propagation of Errors: Forward propagation is less likely to accumulate and propagate numerical errors throughout the network. This contrasts with backpropagation, where errors could proliferate through the derivative chain \cite{lecun1998gradient}. \\
\end{itemize}

% \subsection{Gradient Analysis}
% Figure \ref{fig:DNN} illustrates the gradient behaviors during the training epochs of a Deep Neural Network (DNN) structured with three dense layers, each comprising 2048 units and applying a custom Rectified Linear Unit (ReLU) activation function. The left subplot vividly portrays the maximum gradient values, revealing an initial peak that tends to stabilize in subsequent epochs. The right subplot, on the other hand, depicts the minimum gradient values.
% % Figure environment removed
% Most importantly, it is discernible from the graphs that the gradient values predominantly lie within the float16 numerical representation range, which is approximately between -65504 and 65504. This observation is paramount, substantiating that the 16-bit DNN manifests a minimized susceptibility to overflow occurrences during its training phase. This intrinsic robustness against overflow enhances the reliability and stability of the network's training process, supporting its effective implementation and operation.

% % Figure environment removed

% Figure \ref{fig:CNN} shows the gradient behaviors within a Convolutional Neural Network (CNN) utilizing a ResNet-56 architecture during the training epochs. The examination is focused on discerning the susceptibility of the network to gradient overflow issues, considering the computational constraints of float16 numerical representation.

% The left subplot delineates the maximum gradient values through the training epochs, uncovering a pronounced variability, notably within the initial epochs, which then navigates towards stabilization in the subsequent epochs. Concurrently, the right subplot illustrates the minimum gradient values, which, while unveiling substantial fluctuations, also trends towards stabilization across the epochs.

% A pivotal observation conveyed through these visualizations is the encapsulation of gradient values primarily within the numerical boundaries of float16 representation, ranging approximately from -65504 to 65504. This crucial observation illustrates the resilience of the 16-bit ResNet-56 architecture against potential overflow challenges during the training process. The empirical evidence presented through these gradients reinforces the practical feasibility of deploying the 16-bit CNN ResNet-56 architecture, ensuring robustness and stability throughout the training lifecycle.

\subsection{Backward Propagation}
We focused on backpropagation rather than forward propagation. The reason for this is that forward propagation consists of the product and sum of the matrix without division operations. In pursuit of a thorough mathematical scrutiny, our objective pivots on three principal tenets: firstly, to dissect and understand the operational intricacies of each optimizer; secondly, to pinpoint the exact circumstances and causes that give rise to numerical instability; and finally, to formulate effective solutions that can aptly mitigate these issues. 

\subsubsection{Gradient Analysis}
The gradient computations is crucial in the training of neural networks. Accurate gradient propagation is vital for effective learning, as it directs the optimization algorithm to adjust the model weights, thereby minimizing the loss function. This section investigates the gradient behaviors within DNN and CNN architectures to ensure the learning process's stability and efficiency. We meticulously analyze the gradient magnitudes throughout the training epochs to identify any potential numerical issues that could hinder the network's learning capabilities based on Floating Point 32-bit. Although overflow is a commonly acknowledged risk, our findings indicate that underflow might pose a more insidious threat, which we will address in subsequent sections.

\textbf{Deep Neural Network}\\
In the training progression of a DNN with a linearly connected architecture, each comprising 2048 neurons, Figure \ref{fig:DNN} presents the gradient dynamics. The left panel of the figure clearly traces the maximum gradient values, displaying an initial peak followed by a leveling off, suggesting stabilization as training progresses. In contrast, the right panel sheds light on the minimum gradient values, mapping their trajectory across the training epochs.

% Figure environment removed

% Figure environment removed

\textbf{Convolutional Neural Network}

Similarly, Figure \ref{fig:CNN} examines the gradient behavior within a CNN that utilizes the ResNet-56 architecture\cite{he2016deep} during its training epochs. The examination is centered on the network's resilience to gradient overflow, considering the computational constraints of the float16 numerical format. The left panel of the figure reveals the maximum gradient values, showing significant variability in the initial stages of training, which then tend towards stabilization in later epochs. The right panel, concurrently, depicts the minimum gradient values, which, despite noticeable fluctuations, also demonstrate a trend towards stabilization with ongoing training. At this time, the reason why the results of this experiment are small, unlike the above DNN results, is that Batch Normalization\cite{batch} is used as explained in the previous section to prevent the value from increasing.

A critical examination of the gradient behaviors in both the DNN and CNN architectures reveals that the gradients predominantly remain within the float16 numerical domain, which spans approximately from -65504 to 65504 \cite{ieee754}. This observation is crucial as it confirms that both the 16-bit DNN and CNN architectures exhibit a reduced risk of numerical overflow during training. This resilience to overflow is indicative of the architectures' inherent stability and reliability, which are imperative for the networks' operational integrity and successful deployment.

The analysis thus far has concentrated on mitigating overflow in gradient values, a vital component of numerical stability in neural network training. The consistent containment of gradient values within the float16 range underscores the robustness of the 16-bit architectures against overflow, ensuring their viability during the training process.

\subsection{Update Rule}
Although DNN and CNN are robust about overflow, there might be underflow issue during training. Given the potential for underflow, it is essential to consider its implications on the update rule, which is the cornerstone of the learning process in neural networks. The update rule is sensitive to the scale of the gradients; if the gradients are too small, they may not effect meaningful change in the parameters, leading to stalled training or suboptimal convergence. This concern is not merely theoretical but has practical ramifications for the design and implementation of neural network training algorithms. In this sub-section, we will explain about the risk of several update rules regarding underflow. To exemplify, consider the update rule in the gradient descent method for optimizing a neural network \cite{ruder2017overview}:
\begin{align*}
\theta = \theta - \eta \nabla_{\theta} J(\theta)
\end{align*}
In this equation, $\theta$ denotes the parameters of the model, $\eta$ represents the learning rate, and $\nabla_{\theta} J(\theta)$ signifies the gradient of the loss function $J(\theta)$ with respect to the parameters. Understanding such equations allows us to uncover the internal mechanics of each optimizer, thereby facilitating our quest to alleviate numerical instability.
\subsection{Mini-Batch Gradient Descent}
Mini-batch gradient descent is a variant of the gradient descent algorithm that divides the training datasets into small batches to compute errors \cite{ruder2017overview}. The update rule for the parameters in mini-batch gradient descent can be written as:
\begin{align*}
\theta =& \theta - \eta \nabla_{\theta} J(\theta; x^{(i:i+n)}; y^{(i:i+n)})
\end{align*} 
This optimizer leverages mini-batches of $m$ examples $(x^{(i:i+n)}, y^{(i:i+n)})$ from the training dataset ${x^{(i)},..., x^{(n)}}$ to update the model's parameters $\theta \in \mathbb{R}^{d}$. It aims to minimize the loss function $J(\theta)$ by taking calculated steps that are antithetical to the gradient of the function with respect to the parameters \cite{ruder2017overview}. The magnitude of these steps is dictated by the learning rate, denoted by $\eta$. It is postulated that Mini-Batch Gradient Descent can function optimally in a 16-bit environment, given the limited number of hyperparameters that can provoke numerical instability within the neural network during the process of minimizing $J(\theta)$.
\\
\begin{itemize}
\item \textbf{Assumption 3.1} \textit{Consider the input image data $x \in X{x_1,…,x_i}$, the label $y \in Y{y_1,…,y_i}$, where each $x \in \mathbb{R}$ satisfies $0<x<255$, and each $y \in \mathbb{Z}$. The normalized data $\bar{x}$ will lie in the range $0.0<\Bar{x}<1.0$. If $fp16_{min}<f(x,y)=\nabla_{\theta} J(\theta; x^{(i)}; y^{(i)}) )<fp16_{max}$, and $f(x,y)$ does not engender numerical instability $I={NaN, Inf}$ during the update of $\theta$, it can be deduced that the mini-batch gradient descent method will function properly for training the 16-bit neural network.} \\
\end{itemize}
Based on \textit{Assumption 3.1}, the number of hyperparameters in the Mini-Batch Gradient Descent that could instigate numerical instability is restricted. Consequently, we anticipate that this optimizer should function efficiently within a 16-bit neural network with default hyperparameters.

\subsection{RMSProp}
Root Mean Square Propagation, more commonly referred to as RMSProp, is an optimization methodology utilized in the training of neural networks. This technique was proposed by Geoff Hinton during his Coursera class in Lecture 6e \cite{tieleman2012lecture}. However, when implemented within a 16-bit neural network using default hyperparameters, RMSProp results in the model weights turning into $NaN$, thereby causing a complete halt in the training process. 

% Figure environment removed

Figure \ref{fig:RMS} shows that when training MNIST with a 16-bit neural network using RMSProp, the weights become NaN (white points) after a few epochs if a value less than 1e-4 is used as epsilon. To circumvent this numerical problem, it is essential to understand the source of numerical instability that RMSProp introduces in 16-bit environments. The update rule for RMSProp can be expressed as follows:
\begin{align*}
w_t =& w_{t-1} - \eta \frac{g_t}{\sqrt{v_t} + \epsilon}
\end{align*}
The 16-bit floating-point representation has a more constrained computational scope in contrast to the 32-bit variant. When values exceed or fall below the valid range of $fp16$, numerical instability emerges during the training sequence. In the context of the RMSProp optimizer, the learning rate $\eta$ typically doesn't instigate issues provided $\eta > fp16_{min}$. However, the denominator $v_t$, adaptive learning rate, and $\epsilon$ can trigger numerical instability if their values extend beyond the allowable range.
\begin{align*}
v_t =& {\begin{cases}
0 & if \: v_t < fp16_{min} \\
{\beta} {v_{t-1}} + {(1 - \beta) {{g_t}^{2}}} & otherwise\\
\infty & if \: v_t > fp16_{max}
\end{cases}}
\end{align*}
If the value of ${v_t}$ falls below $fp16_{min}$, ${v_t}$ converges to zero, which in turn affects $w_t$. The expression for $w_t$ can then be rearranged so that only $\epsilon$ remains in the denominator. Under these circumstances, the numerical instability of the optimizer is primarily induced by the default $\epsilon$ value. In TensorFlow, the default value for $\epsilon$ is set to 1e-07, and the default learning rate $\eta$ is configured as 1e-03.
\begin{align*}
w_t =& w_{t-1} - \eta \cdot {g_t} \cdot {\epsilon}^{-1}
\end{align*} 
The adjustment of weights is contingent upon the $\epsilon$ value. Nevertheless, during this process, numerical instability may emerge when this value is inversed, leading to underflow and overflow incidents in the optimizer.
\begin{align*}
\epsilon^{-1} =& 
{\begin{cases}
\infty & if \: \epsilon^{-1} > fp16_{Max} \\
\epsilon^{-1} & otherwise \\
\end{cases}}
\end{align*}
Should the value of epsilon decline to 1e-07, its reciprocal soars to 1e+07. While the 16-bit format accommodates 1e-07, it cannot handle 1e+07, leading to an overflow that escalates to $\infty$. This overflow is an inevitable outcome, independent of $g_t$, the gradient's value. The progressive unfolding of such overflow instances stirs up numerical instability, eventually compromising the integrity of the neural network. Whenever $\epsilon^{-1} > fp16_{max}$, the term $g_{t} \cdot \epsilon^{-1}$ will invariably result in $I = {\infty, -\infty}$, contingent on the sign of $g_t$.
\begin{align*}
w_t =& 
{\begin{cases}
w_{t-1} - \infty & if \: g_t > 0 \\
w_{t-1} + \infty & if \: g_t < 0 \\
\end{cases}}
\end{align*}
The current weight is represented by $w_{t} = w_{t-1} - i$ ($i \in I$) which yields ${NaN, -\infty}$ when $w_{t-1} \in {\infty, -\infty}$. Upon the occurrence of $NaN$, this optimizer ceases to function on a 16-bit neural network, a consequence of the TensorFlow default epsilon setting being at 1e-07.
\\
\begin{itemize}
\item \textbf{Assumption 3.2} \textit{We posit that the input image data $x \in X{x_1,…,x_i}$ and label $y \in Y{y_1,…,y_i}$, where all $x \in \mathbb{R}$, $0<x<255$, and all $y \in \mathbb{Z}$. The normalized data $\bar{x}$ will lie within the range of $0.0<\Bar{x}<1.0$. Similarly, the gradient $g_t$ will fall within the range $fp16_{min}<g_t<fp16_{max}$. There exist two conditions under which RMSProp can operate effectively in a 16-bit environment. Firstly, if $v_t \ne 0$, $v_t \ge fp16_{min}$ and $fp16_{min} < \sqrt{v_t} + \epsilon < fp16_{max}$, $w_t$ will not undergo $overflow$. Secondly, when $v_t < fp16_{min}$, $v_t$ becomes $0$. In this case, if $g_t \cdot \epsilon^{-1} < fp16_{max}$, RMSProp successfully evades critical numerical instability. Provided either of these conditions are met, the RMSProp optimizer will function appropriately in a 16-bit neural network.} \\ 
\end{itemize}
In the event that \textit{Assumption 3.2} holds true, the RMSProp optimizer is capable of executing effectively in 16-bit neural networks. An improper selection of the epsilon parameter could misdirect the course of learning in the 16-bit neural network, potentially causing numerical instability. This may lead to erroneous conclusions regarding the compatibility of the RMSProp optimizer with 16-bit neural networks.

\subsection{Adam} 
Adam is a sophisticated optimization algorithm that amalgamates the salient features of the RMSProp and Momentum methods. Conceived by Diederik Kingma and Jimmy Ba, the Adam optimizer implements the technique of adaptive moment estimation to iteratively refine neural network weight calculations with enhanced efficiency \cite{kingma2014adam}. 
% Figure environment removed
By extending stochastic gradient descent, Adam optimally addresses non-convex problems at an accelerated pace, requiring fewer computational resources compared to a host of other optimizers. The potency of this algorithm is particularly noticeable when dealing with large datasets, as it maintains a tighter trajectory over numerous training iterations.
\begin{align*}
w_t =& w_{t-1} - \eta \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
\end{align*} 
Figure \ref{fig:RMS} shows that when training MNIST with a 16-bit neural network using Adam, the weights become NaN (white points) after a few epochs if a value less than 1e-3 is used as epsilon. When considering the Adam optimizer, numerical instability arises following principles akin to those governing RMSProp. However, an additional distinguishing element in Adam is the introduction of a momentum variable, $m$. In the Adam optimizer, both $m$ and $v$ are initialized to zero. Hence, at the onset of learning, both $m_t$ and $v_t$ are inclined towards zero, undergoing an initial process to remove this bias.
\begin{align*}
m_t =& \beta_1 \cdot m_{t-1}  + (1 - \beta_1) \cdot g_t \\ 
v_t =& {\beta_2} \cdot {m_{t-1}} + {(1 - \beta_2) \cdot {{g_t}^{2}}}
\end{align*}
The velocity variable $v_t$ in the Adam optimizer is a non-negative real number. The adjusted velocity $\hat{v}t$ is calculated as the ratio of $v_t$ to $(1-\beta{2})$. Therefore, if $v_t$ is less than the maximum value permissible in 16-bit floating point precision, $fp16_{max}$, then $\hat{v_t}$ will approach zero. This condition significantly influences the calculated value of $w_t$ within the Adam optimizer.
\begin{align*}
w_t =& w_{t-1} - \eta \cdot \hat{m_t} \cdot {\epsilon^{-1}}
\end{align*}
In the event where the inverse of epsilon ($\epsilon^{-1}$) exceeds the maximum value permissible in a 16-bit floating point ($fp16_{max}$), $\epsilon^{-1}$ essentially becomes infinite. The default value of $\epsilon$ in TensorFlow is $1e-07$, while the default learning rate ($\eta$) is $1e-03$. Therefore, if $\epsilon$ is $1e-07$, its reciprocal escalates to infinity. Depending on the sign of the adjusted momentum ($\hat{m_t}$), the value of $\eta \cdot \hat{m_t} \slash \epsilon$ will either be positive or negative infinity. Consequently, irrespective of the learning rate $\eta$, $\hat{m_t} \slash \epsilon$ will belong to the set $I = {\infty, -\infty}$. As a result, the new weight update equation $w_{t} = w_{t-1} - i$ ($i \in I$) leads to a situation where $w_t$ belongs to the set ${NaN, -\infty}$ whenever $w_{t-1}$ is ${\infty, -\infty}$. This introduces numerical instability in the learning process.
\begin{align*}
w_t =& 
{\begin{cases}
w_{t-1} - \infty & if \: \hat{m_t} > 0 \\
w_{t-1} + \infty & if \: \hat{m_t} < 0 \\
\end{cases}}
\end{align*}
This process induces a numerical instability in a 16-bit neural network. In particular, if the previous weight $w_{t-1}$ is $-\infty$, the operation $w_{t-1}$ - $\infty$ returns $-\infty$, but $w_{t-1}$ + $\infty$ produces a non-numeric value ($NaN$). Consequently, the current weight $w_t$ becomes $NaN$, thereby halting training within the 16-bit Neural Network utilizing the Adam optimizer.
\\
\begin{itemize}
\item \textbf{Assumption 3.3} posits that the input image data $x \in X{x_1,…,x_i}$, label $y \in Y{y_1,…,y_i}$, where all $x \in \mathbb{R}$ satisfy $0<x<255$, and all $y \in \mathbb{Z}$. The normalized data $\bar{x}$ from $x$ will lie in the range $0.0<\Bar{x}<1.0$, and the momentum $m_t$ will fall within $fp16_{min}<m_t<fp16_{max}$. Two conditions may allow the Adam optimizer to function in a 16-bit setting. Primarily, if $\hat{v_t} \ne 0$, $\hat{v_t} \ge fp16_{min}$ and $fp16_{min} < \sqrt{\hat{v_t}} + \epsilon < fp16_{max}$, the weight $w_t$ will not overflow. Additionally, when $\hat{v_t} < fp16_{min}$, $\hat{v_t}$ becomes $0$. If $\hat{m_t} \cdot \epsilon^{-1} < fp16_{max}$, the Adam optimizer circumvents critical numerical instability. Provided one of these conditions is met, the Adam optimizer functions effectively in a 16-bit neural network. \\
\end{itemize}
Given that the Adam optimizer incorporates the method of the RMSProp optimizer, similar issues impede its performance in a 16-bit Neural Network. However, if Adam satisfies \textit{Assumption 3.3}, the optimizer can function within a 16-bit Neural Network. Proper comprehension and handling of the epsilon issue enables the utilization of a variety of optimizers even within 16-bit computations.

Our exploration into the root causes of numerical instability has led us to a singularly intriguing finding - the value of the hyperparameter, Epsilon, plays a remarkably pivotal role. This discovery, though unassuming at first glance, has far-reaching implications in our quest for numerical stability within the realm of neural network optimization. Epsilon is often introduced in the denominator of certain equations to prevent division by zero errors or to avoid the pitfalls of numerical underflow. In the context of optimizers such as RMSProp and Adam, it's usually involved in the update rule, safeguarding against drastic weight changes arising from exceedingly small gradient values. However, in a 16-bit computational environment, when the reciprocal of Epsilon becomes larger than the maximum representable number, it leads to a condition known as numerical overflow. The overflow subsequently manifests as numerical instability, disrupting the otherwise orderly progression of the learning process, and effectively halting the training of the 16-bit neural network.

\noindent\textbf{Epsilon.} Consider the function $f(x) = \frac{1}{x}$. As $x$ nears 0, $f(x)$ grows indefinitely. By adding $\epsilon$ (i.e., $f(x) = \frac{1}{x + \epsilon}$), the function remains bounded near 0. In optimization, this ensures parameter updates stay bounded, aiding numerical stability during training. However, in low precision scenarios, the presence of $\epsilon$ can induce instability, as gradients become overshadowed by this constant. This isn't unique to the Adam optimizer and adjusting Epsilon's value isn't straightforward, with its ideal value often being context-specific. We've developed a method to address this, offering a systematic approach to Epsilon tuning, ensuring stable optimization amidst numerical challenges.