\documentclass{article}

\usepackage[final, nonatbib]{neurips_2023}
\usepackage[square,sort,comma,numbers]{natbib}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}


\input{macros}


\title{An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
Juyoung Yun \\
Department of Computer Science\\
Stony Brook University\\
juyoung.yun@stonybrook.edu\\
}

\begin{document}
% \footnotetext[1]{Address of Institute A}%
% \footnotetext[2]{These authors contributed equally to this work.}%

\maketitle

\begin{abstract}
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit computations. This study contributes to better understanding of optimization in low-precision computations and provides an effective solution to a longstanding issue in training deep neural networks, opening new avenues for more efficient and stable model training.
\end{abstract}


\section{Introduction}
\input{contents/1-intro}

\section{Related Works}
\input{contents/2-related}

\section{Analysis}
\input{contents/3-analysis}

\section{Method}
\input{contents/4-method}

\section{Results}
\input{contents/5-results}

\section{Discussion}
\input{contents/6-discussion}

\section{Conclusions}
\input{contents/7-conclusion}

\bibliographystyle{plain}
\bibliography{main}
%\bibliographystyle{icml2023}

\end{document}