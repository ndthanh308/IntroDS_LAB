\begin{table*}[ht]
\small
\caption{Performance comparison between RMSProp and Adam optimizers in 16-bit and 32-bit precision settings for training DNN.}
\centering
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{ccccccc}
\hline
\hline
&  &\textbf{Numerical} & \textbf{RMSProp} & \textbf{RMSProp} &  \textbf{Adam} & \textbf{Adam} \\
\textbf{Precision} & \textbf{Epsilon} &\textbf{Guarantee Method} & \textbf{Test Accuracy} & \textbf{Training Time (seconds)} &  \textbf{Test Accuracy} & \textbf{Training Time (seconds)} \\ 
\hline

16-bit & 1E-1 & O & 0.985 & 98.90 & 0.967 & 111.23 \\ 
16-bit & 1E-2 & O & 0.987 & 98.41 & 0.987 & 110.99 \\ 
16-bit & 1E-3 & O & 0.988 & 98.21 & 0.987 & 110.79 \\ 
16-bit & 1E-4 & O & 0.988 & 98.25 & 0.987 & 110.52 \\ 
16-bit & 1E-5 & O & 0.987 & 98.23 & 0.988 & 110.54 \\ 
16-bit & 1E-6 & O & 0.984 & 97.67 & 0.987 & 110.72 \\ 
16-bit & 1E-7 & O & 0.980 & 99.58 & 0.980 & 110.61 \\ 
% \hline
16-bit  & 1E-1 & X & 0.987 & 99.84 & 0.987 & 111.20 \\ 
16-bit  & 1E-2 & X & 0.988 & 99.81 & 0.988 & 111.12 \\ 
16-bit  & 1E-3 & X & 0.988 & 99.91 & 0.986 & 111.12 \\ 
16-bit  & 1E-4 & X & 0.986 & 99.52 & 0.098 & 111.02 \\ 
16-bit  & 1E-5 & X & 0.098 & 99.73 & 0.098 & 111.21 \\ 
16-bit  & 1E-6 & X & 0.098 & 100.29 & 0.098 & 111.51 \\ 
16-bit  & 1E-7 & X & 0.098 & 99.95 & 0.098 & 111.23 \\ 
% \hline
32-bit & 1E-1 & X & 0.981 & 182.79 & 0.981 & 203.71 \\ 
32-bit & 1E-2 & X & 0.983 & 183.81 & 0.982 & 203.61 \\ 
32-bit & 1E-3 & X & 0.983 & 182.44 & 0.984 & 202.72 \\ 
32-bit & 1E-4 & X & 0.977 & 183.66 & 0.967 & 203.45 \\ 
32-bit & 1E-5 & X & 0.976 & 180.85 & 0.963 & 203.70 \\ 
32-bit & 1E-6 & X & 0.960 & 181.21 & 0.966 & 204.27 \\ 
32-bit & 1E-7 & X & 0.964 & 180.59 & 0.960 & 205.10 \\ 
\hline
\hline
\end{tabular}
\end{adjustbox}
\label{tab1}
\end{table*}
Our first experimental setup was comprised of a deep neural network (DNN) with 3 linear layers trained on the MNIST dataset \cite{lecun1998gradient}, a ubiquitous benchmark in the field of machine learning, to evaluate the performance of two widely adopted optimization algorithms: RMSProp \cite{tieleman2012lecture} and Adam \cite{kingma2014adam}. The network was implemented using the TensorFlow framework, and a batch size of 512 was utilized during the training process. The architecture of the DNN was relatively simple, containing a single linear layer, which provided a streamlined environment to assess the relative merits of the two optimization strategies. The results of the performance comparison between the RMSProp and Adam optimizers under different precision settings are summarized in Table \ref{tab1}. 

\begin{table*}[ht]
\tiny
\caption{Performance comparison of the Vision Transformer (ViT) based on 16-bit and 32-bit precision with Adam optimizer, incorporating the novel numerical guarantee method.}
\centering
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{ccccccccc}
\hline
\hline
&  &\textbf{Numerical} & \textbf{VIT-8} & \textbf{VIT-8} &  \textbf{VIT-12} & \textbf{VIT-12} &  \textbf{VIT-16} & \textbf{VIT-16} \\

\textbf{Precision} & \textbf{Epsilon} &\textbf{Guarantee} & \textbf{Test} & \textbf{Training Time} &  \textbf{Test} & \textbf{Training Time} &  \textbf{Test} & \textbf{Training Time} \\ 

 &  &\textbf{Method} & \textbf{Accuracy} & (seconds) &  \textbf{Accuracy} & (seconds) &  \textbf{Accuracy} & (seconds)  \\
\hline
16-bit & 1E-1 & O & 0.431 & 630.51 & 0.427 & 951.43 & 0.427 & 1256.43\\ 
16-bit & 1E-2 & O & 0.596 & 612.67 & 0.596 & 955.27 & 0.579 & 1253.62\\ 
16-bit & 1E-3 & O & 0.701 & 612.51 & 0.696 & 960.97 & 0.686 & 1255.35\\ 
16-bit & 1E-4 & O & 0.704 & 638.90 & 0.698 & 964.98 & 0.691 & 1258.62\\ 
16-bit & 1E-5 & O & 0.703 & 638.16 & 0.698 & 963.71 & 0.683 & 1252.89\\ 
16-bit & 1E-6 & O & 0.714 & 640.23 & 0.693 & 944.75 & 0.688 & 1256.02\\ 
16-bit & 1E-7 & O & 0.699 & 644.34 & 0.691 & 924.36 & 0.668 & 1251.00\\ 
% \hline
16-bit  & 1E-1 & X & 0.588 & 640.09 & 0.584 & 923.62 & 0.562 & 1247.62\\ 
16-bit  & 1E-2 & X & 0.701 & 639.77 & 0.698 & 931.64 & 0.677 & 1247.47\\ 
16-bit  & 1E-3 & X & 0.707 & 644.30 & 0.687 & 930.34 & 0.672 & 1248.48\\ 
16-bit  & 1E-4 & X & 0.099 & 640.00 & 0.099 & 926.79 & 0.099 & 1253.36\\ 
16-bit  & 1E-5 & X & 0.099 & 625.91 & 0.099 & 930.08 & 0.099 & 1247.89\\ 
16-bit  & 1E-6 & X & 0.099 & 618.37 & 0.099 & 928.83 & 0.099 & 1244.07\\ 
16-bit  & 1E-7 & X & 0.099 & 615.77 & 0.099 & 931.00 & 0.099 & 1245.86\\ 
% \hline
32-bit & 1E-1 & X & 0.646 & 870.35 & 0.559 & 1307.61 & 0.555 & 1735.48\\ 
32-bit & 1E-2 & X & 0.706 & 873.57 & 0.644 & 1297.50 & 0.638 & 1743.17\\ 
32-bit & 1E-3 & X & 0.700 & 876.43 & 0.682 & 1303.82 & 0.693 & 1733.14\\ 
32-bit & 1E-4 & X & 0.694 & 871.89 & 0.695 & 1304.96 & 0.692 & 1727.79\\ 
32-bit & 1E-5 & X & 0.706 & 875.07 & 0.705 & 1305.68 & 0.697 & 1734.85\\ 
32-bit & 1E-6 & X & 0.698 & 871.53 & 0.697 & 1305.60 & 0.701 & 1720.12\\ 
32-bit & 1E-7 & X & 0.704 & 873.44 & 0.702 & 1307.34 & 0.699 & 1749.34\\ 
\hline
\hline
\end{tabular}
\end{adjustbox}
\label{tab2}
\end{table*}
Our second experiment focuses on the Vision Transformer (ViT)\cite{dosovitskiy2020image}, a complex and sophisticated image training model. The Vision Transformer (ViT) was originally introduced and benchmarked using the Adam optimizer in the foundational paper by Dosovitskiy et al. \cite{dosovitskiy2020image}. In their experiments, Adam was chosen for training the ViT on large datasets like ImageNet. The authors demonstrated that with Adam and the specific training regimen described in the paper, the ViT model achieves competitive performance compared to state-of-the-art convolutional networks. Therefore, we had empirical experiments about VITs with adam optimizer incorporating with numerical guarantee method. Specifically, we employed variants of the ViT model, including ViT-8, ViT-12, and ViT-16, to train on the CIFAR-10 image dataset\cite{krizhevsky2009learning}. The performance of the Adam optimizer under various precision settings, when used with these ViT models (with no data augmentation) and trained in 100 epochs, is summarized in Table \ref{tab2}. These findings are detailed as follows:

\subsection{16-bit Precision with Numerical Guarantee}
In the realm of 16-bit precision, the integration of numerical guarantee methods—highlighted as 'O' in Table \ref{tab1}—led both RMSProp and Adam optimizers to consistently achieve stellar test accuracies, all surpassing the $0.98$ mark. This commendable performance was evident across various epsilon values, underscoring the robust numerical stability even when working within precision-constrained environments. As for time efficiency, disparities were minimal: RMSProp took an estimated $98$ to $100$ seconds per training epoch, while Adam hovered around $110$ seconds. Table \ref{tab2} reveals that the 16-bit vision transformer models fortified with our numerical guarantee technique align closely with their 32-bit VIT counterparts in test accuracy. They possess a distinct edge, completing training at a pace that's briskly 30\% swifter than the 32-bit variants across all VIT iterations.

\subsection{16-bit Precision without Numerical Guarantee}
In scenarios where the numerical guarantee was not enforced (indicated by 'X' in Table \ref{tab1}, \ref{tab2}), a decline in test accuracy was observed for both RMSProp and Adam optimizers when the epsilon value decreased below $10^{-3}$. Despite this decline in accuracy, the computation times were relatively stable, with minor increases compared to the cases where numerical guarantees were incorporated. This shows our observation about risk of epsilon for optimizer is true.

\subsection{32-bit Precision without Numerical Guarantee}
Within the 32-bit precision domain, both the RMSProp and Adam optimizers sustained consistent test accuracies, even without the application of numerical guarantees. However, a conspicuous surge in computational time was observed when juxtaposed with the 16-bit precision settings: the RMSProp optimizer's duration spanned from $180.59$ to $183.81$ seconds, while the Adam optimizer's span fell between $202.72$ to $205.10$ seconds. As corroborated by Table \ref{tab2}, all VIT models operating in 32-bit exhibited a computational pace that lagged approximately 30\% behind their 16-bit counterparts, irrespective of the incorporation of the numerical guarantee method.

The experiments were conducted on laptop GPUs featuring RTX 3080. An intriguing observation was that 16-bit computations were at least 40 percent faster than their 32-bit counterparts. This advantage of computational speed underlines the potential of utilizing 16-bit precision, particularly in contexts with constrained computing resources such as laptops. This study successfully establishes the critical role of numerical guarantee methods in maintaining high levels of accuracy, particularly in lower precision settings. It also emphasizes the trade-off between numerical stability and computational efficiency. While higher precision settings may be less prone to numerical instability, they demand significantly more computational resources.

To summarize, our research offers a novel approach to mitigate numerical instability in 16-bit neural networks without sacrificing accuracy. This contribution holds the potential to stimulate further advancements in the field of machine learning, particularly in applications where computational resources are limited. The findings underscore the importance of careful consideration of numerical precision in the implementation of deep learning models and highlight the potential advantages of lower-precision computations. Further investigations would be beneficial to validate these findings in more other deep learning models, such as transformers for natural language processing which is not for image classification.