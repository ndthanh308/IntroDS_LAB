\subsection{Existing Method}
\textbf{Loss Scaling in Mixed Precision}: Mixed precision training \cite{Micikevicius2017} is a technique that utilizes both 16-bit and 32-bit floating-point types during training to make it more memory efficient and faster. However, the reduced precision of 16-bit calculations can sometimes lead to numerical underflow, especially in the intermediate gradient values in some optimizers such as Adam and RMSProp. To prevent this, the "loss scaling" method is employed. Loss scaling is a straightforward concept. Before performing backpropagation, the loss is scaled up by a large factor. This artificially increases the scale of the gradients and thus helps prevent underflow. After the gradients are calculated, they are scaled back down to ensure the weights are updated correctly. In essence, this process amplifies the small gradient values to make them representable in the reduced precision and then scales them back down to ensure that the actual model updates remain accurate. While mixed precision does use 16-bit calculations, it leans on 32-bit calculations for the critical parts to maintain accuracy, especially for maintaining running sums and optimizer states. This implies that we aren't fully exploiting the speed and memory benefits of 16-bit precision. According to Table \ref{tabmp}, Mixed precision based VIT-16 will not reduce GPU memory usage because it utilizes 32-bit computation for mixed precision. 
\begin{table*}[ht]
\caption{Performance comparison between 16-bit, 32-bit, and Mixed Precision 
settings for training VIT-16\cite{dosovitskiy2020image} with cifar-10 images and Adam optimizer.}
\vspace{0.25cm}
\small
\centering
% \begin{adjustbox}{width=1\textwidth}
\begin{tabular}{ccccc}
\hline
\hline
& Epsilon & Test Accuracy & Training Time (seconds) & GPU Memory Usage \\
\hline
Floating Point 16-bit & 1E-3 & 0.686 & 1255.35 & 1.234 GB \\ 
Floating Point 32-bit & 1E-3 & 0.693 & 1733.14 & 2.412 GB \\ 
Mixed Precision (MP) & 1E-3 & 0.685 & 1430.12 & 2.415 GB \\ 
\hline
\hline
\end{tabular}
% \end{adjustbox}
\label{tabmp}
\end{table*}

Contrary to the mixed precision method, a pure 16-bit neural network operates entirely in the 16-bit precision realm. There are several reasons why pure 16-bit computations can outshine both 32-bit and mixed precision. First, 16-bit operations are faster than their 32-bit and mixed precision. Second, 16-bit representations require half the memory of 32-bit, which means we can fit larger models or batch sizes into GPU memory, further enhancing performance. Yun et at\cite{yun2023defense} shows pure 16-bit neural networks are faster than both 32-bit and mixed precision. This means that mixed precision with loss scaling method cannot utilize advantages of 16-bit operations. The inefficiencies and the constant juggling between 16-bit and 32-bit in mixed precision, along with the need for methods like loss scaling, indicate that a more robust solution is needed for numerical stability in low precision environments. This is where the "Numerical Guarantee Method" comes into play. Instead of hopping between precisions or artificially scaling losses, the Numerical Guarantee Method aims to provide a stable environment for optimizers like Adam to function efficiently in pure 16-bit computations. This approach makes the training process faster and potentially more accurate than using 32-bit or mixed precision models.

\subsection{Numerical Guarantee Method}
In this study, we present a novel approach designed to alleviate numerical instability, a common problem associated with widely-used optimizers such as Adam and RMSProp, in the training of 16-bit neural networks. These optimizers have frequently encountered challenges when working with smaller epsilon values, particularly in instances involving division operations. The genesis of our method stems from an in-depth mathematical exploration of the update rules in the Adam optimizer, which revealed that instability originates during the gradient update step - a phase where the momentum term ($\hat{m}$) is divided by the sum of the square root of the velocity term ($\hat{v}$) and the epsilon parameter ($\epsilon_t$). To circumvent this instability, we propose a refined version of the gradient computation equation, which effectively prevents division by an exceedingly small epsilon value. This is achieved by introducing a maximum function that sets a lower threshold on the denominator. Consequently, the updated Adam optimizer follows the equation:
\begin{align*}
w_{t} & = w_{t-1} - \frac{\eta \cdot \hat{m_t}}{\sqrt{\max(\hat{v_t}, \epsilon)}}
\end{align*}
The modification guarantees that the denominator remains within a safer numerical range, thus facilitating a more stable gradient update. The addition of the stabilizing term, $\sqrt{\max(\hat{v_t}, \epsilon)}$, ensures the denominator does not dwindle excessively, thereby avoiding abnormally large updates. Should $\hat{v_t}$ fall below $\epsilon$, the term assumes the square root of $\epsilon$ instead. The precise value employed as the lower limit (in this case, $\epsilon$ is 1e-4) can be tailored according to the specific requirements of the neural network in consideration. Our proposed adjustment to the Adam optimizer offers a myriad of benefits, which are elaborated upon subsequently:
\\
\begin{itemize}
\item Enhanced Numerical Stability: By taking the maximum of $\hat{v_t}$ and a small constant inside the square root function, we mitigate the numerical instability that arises when $\hat{v_t}$ is very close to zero. This enhancement significantly reduces the chances of overflow and underflow during computations, which in turn increases the robustness of the optimizer.

\item Preservation of Adam's Benefits: The modification does not alter the fundamental characteristics and benefits of the Adam optimizer. It retains the benefits of Adam, such as efficient computation, suitability for problems with large data or parameters, and robustness to diagonal rescale or translations of the objective functions.

\item Ease of Implementation: The modification requires just a minor change to the original Adam algorithm, making it easy to implement in practice. This simplicity of modification allows for easy integration into existing machine learning frameworks and pipelines, thereby increasing its accessibility to practitioners.

\item Applicability to 16-bit Computations: The updated method enables Adam to work effectively on 16-bit neural networks, which was a challenge with the original Adam optimizer due to numerical instability issues. Thus, it extends the applicability of Adam to systems and applications where memory efficiency is crucial.

\item Flexibility: This modification is generalizable and can be applied to other optimizers based on the same concept as Adam, thereby providing a broad range of applications.\\
\end{itemize}

Overall, these advantages make this modification an effective solution to the problem of numerical instability in Adam and similar optimizers, particularly in the context of 16-bit computations. Our approach presents a novel way to alleviate numerical instability in 16-bit neural networks, without losing the advantages offered by advanced optimizers such as Adam and RMSProp. We will demonstrate the effectiveness of our approach through comprehensive experimental results in the following section.
