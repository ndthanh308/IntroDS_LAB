Through theoretical analysis, we analyze which part of the neural network occurs numerical instability which can ruin the training process.

\subsection{Forward Propagation}
\noindent\textbf{Linear Network.} During forward propagation, each column of the input matrix $X$ represents a distinct training example. This matrix layout simplifies the network's computation, as each feature of every training sample can be processed in parallel, exploiting the parallel nature of matrix operations.
\begin{equation*}
X = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1m} \\
x_{21} & x_{22} & \cdots & x_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nm}
\end{bmatrix}
\end{equation*}
The weight matrix $W$ represents the strength and direction of connections between neurons. Each column of $W$ corresponds to the set of weights connecting every input neuron to a specific hidden neuron.
\begin{equation*}
W = \begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1k} \\
w_{21} & w_{22} & \cdots & w_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
w_{n1} & w_{n2} & \cdots & w_{nk}
\end{bmatrix}
\end{equation*}
The bias vector $b$ provides an additional degree of freedom, allowing each neuron in the hidden layer to be activated not just based on weighted input but also based on this inherent property.
\begin{equation*}
b = \begin{bmatrix}
b_{1} \\
b_{2} \\
\vdots \\
b_{k}
\end{bmatrix}
\end{equation*}
The raw outputs or pre-activations (denoted as $Z$) are computed by multiplying the transposed weight matrix with the input matrix and adding the bias. 
\begin{equation*}
Z = W^T X + b
\end{equation*}
After computing $Z$, we need a mechanism to introduce non-linearity into our model. Without this, no matter how deep our model, it would behave just like a single-layer perceptron. This non-linearity is introduced using an activation function $\sigma(\cdot)$, applied element-wise to the matrix $Z$.
\begin{equation*}
A = \sigma(Z)
\end{equation*}
Here, each entry $a_{ij}$ in $A$ is the activated value of the $j$-th neuron in the hidden layer when the model is provided the $i$-th training example. These activations will either be used as input for subsequent layers or be the final output of the network.

In Deep Neural Network (Linear Network.), each operation in the forward pass involves straightforward mathematical operations like addition and multiplication. There is no division or complex operation that could amplify small numerical errors or lead to potential instability. \\

\noindent\textbf{Convolutional Network.} In the realm of deep learning, especially when processing image data, CNNs have gained significant prominence. The foundational blocks of CNNs involve convolving filters over input data and pooling layers \cite{lecun1998gradient}. 

The convolution layer can be detailed by observing the element-wise multiplication and summation \cite{lecun1998gradient}. Specifically, in our 3x3 input matrix \(I\) and 2x2 filter \(F\) example:
\begin{align*}
I &= \begin{bmatrix}
i_{11} & i_{12} & i_{13} \\
i_{21} & i_{22} & i_{23} \\
i_{31} & i_{32} & i_{33}
\end{bmatrix}
&
F &= \begin{bmatrix}
f_{11} & f_{12} \\
f_{21} & f_{22}
\end{bmatrix}
\end{align*}
For the top-left corner of \(I\), the convolution operation using the filter \(F\) is as follows:
\begin{equation*}
o_{11} = \begin{bmatrix}
i_{11} & i_{12} \\
i_{21} & i_{22}
\end{bmatrix} 
\odot 
\begin{bmatrix}
f_{11} & f_{12} \\
f_{21} & f_{22}
\end{bmatrix}
\end{equation*}
Where \(\odot\) represents element-wise multiplication. This implies:
\begin{equation*}
o_{11} = i_{11} \cdot f_{11} + i_{12} \cdot f_{12} + i_{21} \cdot f_{21} + i_{22} \cdot f_{22}
\end{equation*}
The filter \(F\) continues sliding across \(I\), performing similar computations for each position, resulting in the matrix:
\begin{equation*}
O = \begin{bmatrix}
o_{11} & o_{12} \\
o_{21} & o_{22}
\end{bmatrix}
\end{equation*}
Each element in the output matrix \(O\) is computed in the same fashion, with each \(o_{ij}\) being the result of convolving a 2x2 segment of \(I\) with the filter \(F\).

Pooling layers can also be represented using matrix notation, though the operation is simpler. For max pooling with a 2x2 window, given an input matrix $I$, the operation for a single position can be illustrated as \cite{maxpool}:
\begin{equation*}
P_{ij} = \max \begin{bmatrix}
I_{2i-1, 2j-1} & I_{2i-1, 2j} \\
I_{2i, 2j-1} & I_{2i, 2j}
\end{bmatrix}
\end{equation*}
For an example 4x4 input matrix being processed by a 2x2 max pooling operation:
\begin{equation*}
I = \begin{bmatrix}
i_{11} & i_{12} & i_{13} & i_{14} \\
i_{21} & i_{22} & i_{23} & i_{24} \\
i_{31} & i_{32} & i_{33} & i_{34} \\
i_{41} & i_{42} & i_{43} & i_{44}
\end{bmatrix}
\end{equation*}
The output matrix $P$ from this max pooling operation will be:
\begin{equation*}
P = \begin{bmatrix}
\max(i_{11}, i_{12}, i_{21}, i_{22}) & \max(i_{13}, i_{14}, i_{23}, i_{24}) \\
\max(i_{31}, i_{32}, i_{41}, i_{42}) & \max(i_{33}, i_{34}, i_{43}, i_{44})
\end{bmatrix}
\end{equation*}
In CNNs, actions taken during the forward pass primarily consist of basic arithmetic, such as addition and multiplication. There's an absence of division or intricate calculations that might magnify minor numerical inaccuracies or trigger potential instability. 

Especially those with a significant number of layers, the iterative multiplications during forward propagation can intermittently lead to either vanishingly small values or notably large increments in the activations. However, methods such as batch normalization\cite{batch} have been introduced to regulate and prevent these activations from attaining extreme values. Therefore, forward propagation, which involves the transmission of input data through the network to produce an output, is generally more resilient to such instabilities. This comparative robustness can be ascribed to a variety of underlying reasons:
\\
\begin{itemize}
\item Simplified Operations: Forward propagation predominantly involves elementary mathematical operations such as addition and multiplication. Thus, the results are less prone to reaching extreme values unless the input data or model parameters are improperly scaled or exhibit a high dynamic range \cite{He2015}.

\item Absence of Derivatives: Contrasting backpropagation, forward propagation does not necessitate the computation of derivatives, a process that could engender exceedingly large or small numbers, thus inducing numerical instability \cite{Rumelhart1986}.

\item Limited Propagation of Errors: Forward propagation is less likely to accumulate and propagate numerical errors throughout the network. This contrasts with backpropagation, where errors could proliferate through the derivative chain \cite{lecun1998gradient}. \\
\end{itemize}

\subsection{Backward Propagation}
We focused on backpropagation rather than forward propagation. The reason for this is that forward propagation consists of the product and sum of the matrix without division operations. In pursuit of a thorough mathematical scrutiny, our objective pivots on three principal tenets: firstly, to dissect and understand the operational intricacies of each optimizer; secondly, to pinpoint the exact circumstances and causes that give rise to numerical instability; and finally, to formulate effective solutions that can aptly mitigate these issues. 

\subsubsection{Update Rule}
Although DNN and CNN are robust about overflow, there might be underflow issue during training. Given the potential for underflow, it is essential to consider its implications on the update rule, which is the cornerstone of the learning process in neural networks. The update rule is sensitive to the scale of the gradients; if the gradients are too small, they may not effect meaningful change in the parameters, leading to stalled training or suboptimal convergence. This concern is not merely theoretical but has practical ramifications for the design and implementation of neural network training algorithms. In this sub-section, we will explain about the risk of several update rules regarding underflow. To exemplify, consider the update rule in the gradient descent method for optimizing a neural network \cite{ruder2017overview}:
\begin{align*}
\theta = \theta - \eta \nabla_{\theta} J(\theta)
\end{align*}
In this equation, $\theta$ denotes the parameters of the model, $\eta$ represents the learning rate, and $\nabla_{\theta} J(\theta)$ signifies the gradient of the loss function $J(\theta)$ with respect to the parameters. Understanding such equations allows us to uncover the internal mechanics of each optimizer, thereby facilitating our quest to alleviate numerical instability.
\subsubsection{Mini-Batch Gradient Descent}
Mini-batch gradient descent is a variant of the gradient descent algorithm that divides the training datasets into small batches to compute errors \cite{ruder2017overview}. The update rule for the parameters in mini-batch gradient descent can be written as:
\begin{align*}
\theta =& \theta - \eta \nabla_{\theta} J(\theta; x^{(i:i+n)}; y^{(i:i+n)})
\end{align*} 
This optimizer leverages mini-batches of $m$ examples $(x^{(i:i+n)}, y^{(i:i+n)})$ from the training dataset ${x^{(i)},..., x^{(n)}}$ to update the model's parameters $\theta \in \mathbb{R}^{d}$. It aims to minimize the loss function $J(\theta)$ by taking calculated steps that are antithetical to the gradient of the function with respect to the parameters \cite{ruder2017overview}. The magnitude of these steps is dictated by the learning rate, denoted by $\eta$. It is postulated that Mini-Batch Gradient Descent can function optimally in a 16-bit environment, given the limited number of hyperparameters that can provoke numerical instability within the neural network during the process of minimizing $J(\theta)$.
\\
\begin{itemize}
\item \textbf{Assumption 3.1} \textit{Consider the input image data $x \in X{x_1,…,x_i}$, the label $y \in Y{y_1,…,y_i}$, where each $x \in \mathbb{R}$ satisfies $0<x<255$, and each $y \in \mathbb{Z}$. The normalized data $\bar{x}$ will lie in the range $0.0<\Bar{x}<1.0$. If $fp16_{min}<f(x,y)=\nabla_{\theta} J(\theta; x^{(i)}; y^{(i)}) )<fp16_{max}$, and $f(x,y)$ does not engender numerical instability $I={NaN, Inf}$ during the update of $\theta$, it can be deduced that the mini-batch gradient descent method will function properly for training the 16-bit neural network.} \\
\end{itemize}
Based on \textit{Assumption 3.1}, the number of hyperparameters in the Mini-Batch Gradient Descent that could instigate numerical instability is restricted. Consequently, we anticipate that this optimizer should function efficiently within a 16-bit neural network with default hyperparameters.

\subsubsection{Adam} 
Conceived by Diederik Kingma and Jimmy Ba, the Adam optimizer implements the technique of adaptive moment estimation to iteratively refine neural network weight calculations with enhanced efficiency \cite{kingma2014adam}. 
% Figure environment removed

By extending stochastic gradient descent, Adam optimally addresses non-convex problems at an accelerated pace, requiring fewer computational resources compared to a host of other optimizers. The potency of this algorithm is particularly noticeable when dealing with large datasets, as it maintains a tighter trajectory over numerous training iterations.
\begin{align*}
w_t =& w_{t-1} - \eta \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
\end{align*} 
Figure \ref{fig:ADAM} shows that when training MNIST with a 16-bit neural network using Adam, the weights become NaN (white points) after a few epochs if a value less than 1e-3 is used as epsilon. When considering the Adam optimizer, numerical instability arises. However, an additional distinguishing element in Adam is the introduction of a momentum variable, $m$. In the Adam optimizer, both $m$ and $v$ are initialized to zero. Hence, at the onset of learning, both $m_t$ and $v_t$ are inclined towards zero, undergoing an initial process to remove this bias.
\begin{align*}
m_t =& \beta_1 \cdot m_{t-1}  + (1 - \beta_1) \cdot g_t \\ 
v_t =& {\beta_2} \cdot {m_{t-1}} + {(1 - \beta_2) \cdot {{g_t}^{2}}}
\end{align*}
The velocity variable $v_t$ in the Adam optimizer is a non-negative real number. The adjusted velocity $\hat{v}t$ is calculated as the ratio of $v_t$ to $(1-\beta{2})$. Therefore, if $v_t$ is less than the maximum value permissible in 16-bit floating point precision, $fp16_{max}$, then $\hat{v_t}$ will approach zero. This condition significantly influences the calculated value of $w_t$ within the Adam optimizer.
\begin{align*}
w_t =& w_{t-1} - \eta \cdot \hat{m_t} \cdot {\epsilon^{-1}}
\end{align*}
In the event where the inverse of epsilon ($\epsilon^{-1}$) exceeds the maximum value permissible in a 16-bit floating point ($fp16_{max}$), $\epsilon^{-1}$ essentially becomes infinite. The default value of $\epsilon$ in TensorFlow is $1e-07$, while the default learning rate ($\eta$) is $1e-03$. Therefore, if $\epsilon$ is $1e-07$, its reciprocal escalates to infinity. Depending on the sign of the adjusted momentum ($\hat{m_t}$), the value of $\eta \cdot \hat{m_t} \slash \epsilon$ will either be positive or negative infinity. Consequently, irrespective of the learning rate $\eta$, $\hat{m_t} \slash \epsilon$ will belong to the set $I = {\infty, -\infty}$. As a result, the new weight update equation $w_{t} = w_{t-1} - i$ ($i \in I$) leads to a situation where $w_t$ belongs to the set ${NaN, -\infty}$ whenever $w_{t-1}$ is ${\infty, -\infty}$. This introduces numerical instability in the learning process.
\begin{align*}
w_t =& 
{\begin{cases}
w_{t-1} - \infty & if \: \hat{m_t} > 0 \\
w_{t-1} + \infty & if \: \hat{m_t} < 0 \\
\end{cases}}
\end{align*}
This process induces a numerical instability in a 16-bit neural network. In particular, if the previous weight $w_{t-1}$ is $-\infty$, the operation $w_{t-1}$ - $\infty$ returns $-\infty$, but $w_{t-1}$ + $\infty$ produces a non-numeric value ($NaN$). Consequently, the current weight $w_t$ becomes $NaN$, thereby halting training within the 16-bit Neural Network utilizing the Adam optimizer.

\begin{table*}[ht]
\caption{Performance comparison between 16-bit, 32-bit, and Mixed Precision 
settings for training VIT-16\cite{dosovitskiy2020image} with cifar-10 images and Adam optimizer.}
\vspace{0.25cm}
\small
\centering
% \begin{adjustbox}{width=1\textwidth}
\begin{tabular}{ccccc}
\hline

& Epsilon & Test Accuracy & Training Time (seconds) & GPU Memory Usage \\
\hline
Floating Point 16-bit & 1E-3 & 0.686 & 1255.35 & 1.234 GB \\ 
Floating Point 32-bit & 1E-3 & 0.693 & 1733.14 & 2.412 GB \\ 
Mixed Precision (MP) & 1E-3 & 0.685 & 1430.12 & 2.415 GB \\ 
\hline
\end{tabular}
% \end{adjustbox}
\label{tabmp}
\end{table*}
\begin{itemize}
\item \textbf{Assumption 3.3} posits that the input image data $x \in X{x_1,…,x_i}$, label $y \in Y{y_1,…,y_i}$, where all $x \in \mathbb{R}$ satisfy $0<x<255$, and all $y \in \mathbb{Z}$. The normalized data $\bar{x}$ from $x$ will lie in the range $0.0<\Bar{x}<1.0$, and the momentum $m_t$ will fall within $fp16_{min}<m_t<fp16_{max}$. Two conditions may allow the Adam optimizer to function in a 16-bit setting. Primarily, if $\hat{v_t} \ne 0$, $\hat{v_t} \ge fp16_{min}$ and $fp16_{min} < \sqrt{\hat{v_t}} + \epsilon < fp16_{max}$, the weight $w_t$ will not overflow. Additionally, when $\hat{v_t} < fp16_{min}$, $\hat{v_t}$ becomes $0$. If $\hat{m_t} \cdot \epsilon^{-1} < fp16_{max}$, the Adam optimizer circumvents critical numerical instability. Provided one of these conditions is met, the Adam optimizer functions effectively in a 16-bit neural network. \\
\end{itemize}
If Adam satisfies \textit{Assumption 3.3}, the optimizer can function within a 16-bit Neural Network. Proper comprehension and handling of the epsilon issue enables the utilization of a variety of optimizers even within 16-bit computations.

Our exploration into the root causes of numerical instability has led us to a singularly intriguing finding - the value of the hyperparameter, Epsilon, plays a remarkably pivotal role. This discovery, though unassuming at first glance, has far-reaching implications in our quest for numerical stability within the realm of neural network optimization. Epsilon is often introduced in the denominator of certain equations to prevent division by zero errors or to avoid the pitfalls of numerical underflow. In the context of optimizers such as Adam, it's usually involved in the update rule, safeguarding against drastic weight changes arising from exceedingly small gradient values. However, in a 16-bit computational environment, when the reciprocal of Epsilon becomes larger than the maximum representable number, it leads to a condition known as numerical overflow. The overflow subsequently manifests as numerical instability, disrupting the otherwise orderly progression of the learning process, and effectively halting the training of the 16-bit neural network.

\noindent\textbf{Epsilon.} Consider the function $f(x) = \frac{1}{x}$. As $x$ nears 0, $f(x)$ grows indefinitely. By adding $\epsilon$ (i.e., $f(x) = \frac{1}{x + \epsilon}$), the function remains bounded near 0. In optimization, this ensures parameter updates stay bounded, aiding numerical stability during training. However, in low precision scenarios, the presence of $\epsilon$ can induce instability, as gradients become overshadowed by this constant. This isn't unique to the Adam optimizer and adjusting Epsilon's value isn't straightforward, with its ideal value often being context-specific. We've developed a method to address this, offering a systematic approach to Epsilon tuning, ensuring stable optimization amidst numerical challenges.