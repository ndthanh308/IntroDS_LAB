This study aimed to investigate the impact of the Numerical Guarantee Method on the stability and performance of 16-bit precision training in neural networks, and to compare these results with traditional 32-bit precision training. Our experiments were conducted across various architectures, including Deep Neural Networks (DNNs), Convolutional Neural Networks (CNNs), and Vision Transformers (ViTs), using the MNIST and CIFAR-10 datasets.

\subsection{Deep Neural Networks}
In the DNN experiments with the MNIST dataset, conducted over 100 epochs, the Numerical Guarantee Method significantly improved stability in 16-bit precision settings. As shown in Table \ref{tab1}, with the guarantee method, the highest test accuracy in 16-bit precision was 0.988 at an epsilon of 1E-5, with a training time of 110.54 seconds. 
\begin{table}[ht]
\small
\caption{Performance comparison for Adam optimizers in 16-bit and 32-bit precision settings for training DNN.}
\centering
\begin{adjustbox}{width=0.7\columnwidth}
\begin{tabular}{ccccccc}
\hline
&  &\textbf{Numerical} &  \textbf{Adam} & \textbf{Adam} \\
\textbf{Precision} & \textbf{Epsilon} &\textbf{Guarantee Method} &  \textbf{Test Accuracy} & \textbf{Training Time (seconds)} \\ 
\hline

16-bit & 1E-1 & O & 0.967 & 111.23 \\ 
16-bit & 1E-2 & O & 0.987 & 110.99 \\ 
16-bit & 1E-3 & O & 0.987 & 110.79 \\ 
16-bit & 1E-4 & O & 0.987 & 110.52 \\ 
16-bit & 1E-5 & O & 0.988 & 110.54 \\ 
16-bit & 1E-6 & O & 0.987 & 110.72 \\ 
16-bit & 1E-7 & O & 0.980 & 110.61 \\ 
% \hline
16-bit  & 1E-1 & X & 0.987 & 111.20 \\ 
16-bit  & 1E-2 & X & 0.988 & 111.12 \\ 
16-bit  & 1E-3 & X & 0.986 & 111.12 \\ 
16-bit  & 1E-4 & X & \color{red}0.098 & 111.02 \\ 
16-bit  & 1E-5 & X & \color{red}0.098 & 111.21 \\ 
16-bit  & 1E-6 & X & \color{red}0.098 & 111.51 \\ 
16-bit  & 1E-7 & X & \color{red}0.098 & 111.23 \\ 
% \hline
32-bit & 1E-1 & X  & 0.981 & 203.71 \\ 
32-bit & 1E-2 & X  & 0.982 & 203.61 \\ 
32-bit & 1E-3 & X  & 0.984 & 202.72 \\ 
32-bit & 1E-4 & X  & 0.967 & 203.45 \\ 
32-bit & 1E-5 & X  & 0.963 & 203.70 \\ 
32-bit & 1E-6 & X  & 0.966 & 204.27 \\ 
32-bit & 1E-7 & X  & 0.960 & 205.10 \\ 
\hline

\end{tabular}
\end{adjustbox}
\label{tab1}
\end{table}
This was comparable to the 32-bit precision setting, which achieved a maximum accuracy of 0.984 but with a longer training time of approximately 203 seconds, indicating a 45.8\% reduction in training time with 16-bit precision. Notably, without the Numerical Guarantee Method, the 16-bit precision training failed to converge for epsilon values lower than 1E-4, highlighting the method's importance in maintaining numerical stability.

\subsection{Convolutional Neural Networks}
For the CNN experiments using ResNet-56\cite{he2016deep} on CIFAR-10\cite{krizhevsky2009learning}, conducted over 100 epochs, the Numerical Guarantee Method again showed its effectiveness. The 16-bit precision with the guarantee method achieved a top test accuracy of 0.809, as per Table \ref{tab3}. 

\begin{table}[ht]
\small
\caption{Performance of ResNet-56 with Adam optimizer on CIFAR-10 in 16-bit and 32-bit precision settings.}
\centering
\begin{adjustbox}{width=0.7\columnwidth}
\begin{tabular}{ccccccc}
\hline
&  &\textbf{Numerical} &  \textbf{Adam} & \textbf{Adam} \\
\textbf{Precision} & \textbf{Epsilon} &\textbf{Guarantee Method} &  \textbf{Test Accuracy} & \textbf{Training Time (seconds)} \\ 
\hline

16-bit & 1E-1 & O & 0.645 & 1802.86 \\ 
16-bit & 1E-2 & O & 0.746 & 1788.08 \\ 
16-bit & 1E-3 & O & 0.790 & 1855.88 \\ 
16-bit & 1E-4 & O & 0.788 & 1914.40 \\ 
16-bit & 1E-5 & O & 0.809 & 1905.80 \\ 
16-bit & 1E-6 & O & 0.780 & 1868.62 \\ 
16-bit & 1E-7 & O & 0.787 & 1889.74 \\ 
% \hline
16-bit  & 1E-1 & X & 0.740 & 1880.58 \\ 
16-bit  & 1E-2 & X & 0.784 & 1901.09 \\ 
16-bit  & 1E-3 & X & 0.775 & 1843.53 \\ 
16-bit  & 1E-4 & X & 0.766 & 1831.36 \\ 
16-bit  & 1E-5 & X & 0.672 & 1860.22 \\ 
16-bit  & 1E-6 & X & \color{red}0.099 & 1821.46 \\ 
16-bit  & 1E-7 & X & \color{red}0.099 & 1821.46 \\ 
% \hline
32-bit & 1E-1 & X  & 0.681 & 2501.07 \\ 
32-bit & 1E-2 & X  & 0.793 & 2436.03 \\ 
32-bit & 1E-3 & X  & 0.792 & 2440.91 \\ 
32-bit & 1E-4 & X  & 0.786 & 2474.66 \\ 
32-bit & 1E-5 & X  & 0.779 & 2456.38 \\ 
32-bit & 1E-6 & X  & 0.794 & 2470.05 \\ 
32-bit & 1E-7 & X  & 0.792 & 2464.40 \\ 
\hline
\end{tabular}
\end{adjustbox}
\label{tab3}
\end{table}

This significantly outperformed the same precision setting without the guarantee method, where the accuracy dramatically dropped to 0.099 for epsilon values lower than 1E-4, indicating a failure in training convergence. The 16-bit setting with the guarantee method also showed a training time reduction of approximately \textbf{28.4\%} compared to the 32-bit setting.

\subsection{Vision Transformers}
In the ViT\cite{dosovitskiy2020image} experiments, conducted over 200 epochs, the Numerical Guarantee Method was crucial for maintaining performance in 16-bit precision settings. For instance, the ViT-8 model reached a test accuracy of 0.714 with the guarantee method, compared to just 0.099 without it for epsilon values lower than 1E-4, as detailed in Table \ref{tab2}. 

\begin{table*}[ht]
\tiny
\caption{Performance comparison of the Vision Transformer (ViT) based on 16-bit and 32-bit precision with Adam optimizer, incorporating the novel numerical guarantee method.}
\centering
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{ccccccccc}

\hline
&  &\textbf{Numerical} & \textbf{VIT-8} & \textbf{VIT-8} &  \textbf{VIT-12} & \textbf{VIT-12} &  \textbf{VIT-16} & \textbf{VIT-16} \\

\textbf{Precision} & \textbf{Epsilon} &\textbf{Guarantee} & \textbf{Test} & \textbf{Training Time} &  \textbf{Test} & \textbf{Training Time} &  \textbf{Test} & \textbf{Training Time} \\ 

 &  &\textbf{Method} & \textbf{Accuracy} & (seconds) &  \textbf{Accuracy} & (seconds) &  \textbf{Accuracy} & (seconds)  \\
\hline
16-bit & 1E-1 & O & 0.431 & 630.51 & 0.427 & 951.43 & 0.427 & 1256.43\\ 
16-bit & 1E-2 & O & 0.596 & 612.67 & 0.596 & 955.27 & 0.579 & 1253.62\\ 
16-bit & 1E-3 & O & 0.701 & 612.51 & 0.696 & 960.97 & 0.686 & 1255.35\\ 
16-bit & 1E-4 & O & 0.704 & 638.90 & 0.698 & 964.98 & 0.691 & 1258.62\\ 
16-bit & 1E-5 & O & 0.703 & 638.16 & 0.698 & 963.71 & 0.683 & 1252.89\\ 
16-bit & 1E-6 & O & 0.714 & 640.23 & 0.693 & 944.75 & 0.688 & 1256.02\\ 
16-bit & 1E-7 & O & 0.699 & 644.34 & 0.691 & 924.36 & 0.668 & 1251.00\\ 
% \hline
16-bit  & 1E-1 & X & 0.588 & 640.09 & 0.584 & 923.62 & 0.562 & 1247.62\\ 
16-bit  & 1E-2 & X & 0.701 & 639.77 & 0.698 & 931.64 & 0.677 & 1247.47\\ 
16-bit  & 1E-3 & X & 0.707 & 644.30 & 0.687 & 930.34 & 0.672 & 1248.48\\ 
16-bit  & 1E-4 & X & \color{red}0.099 & 640.00 & \color{red}0.099 & 926.79 & \color{red}0.099 & 1253.36\\ 
16-bit  & 1E-5 & X & \color{red}0.099 & 625.91 & \color{red}0.099 & 930.08 & \color{red}0.099 & 1247.89\\ 
16-bit  & 1E-6 & X & \color{red}0.099 & 618.37 & \color{red}0.099 & 928.83 & \color{red}0.099 & 1244.07\\ 
16-bit  & 1E-7 & X & \color{red}0.099 & 615.77 & \color{red}0.099 & 931.00 & \color{red}0.099 & 1245.86\\ 
% \hline
32-bit & 1E-1 & X & 0.646 & 870.35 & 0.559 & 1307.61 & 0.555 & 1735.48\\ 
32-bit & 1E-2 & X & 0.706 & 873.57 & 0.644 & 1297.50 & 0.638 & 1743.17\\ 
32-bit & 1E-3 & X & 0.700 & 876.43 & 0.682 & 1303.82 & 0.693 & 1733.14\\ 
32-bit & 1E-4 & X & 0.694 & 871.89 & 0.695 & 1304.96 & 0.692 & 1727.79\\ 
32-bit & 1E-5 & X & 0.706 & 875.07 & 0.705 & 1305.68 & 0.697 & 1734.85\\ 
32-bit & 1E-6 & X & 0.698 & 871.53 & 0.697 & 1305.60 & 0.701 & 1720.12\\ 
32-bit & 1E-7 & X & 0.704 & 873.44 & 0.702 & 1307.34 & 0.699 & 1749.34\\ 

\hline
\end{tabular}
\end{adjustbox}
\label{tab2}
\end{table*}

This performance was competitive with the 32-bit precision setting, which achieved a maximum accuracy of 0.706. Moreover, the 16-bit setting with the guarantee method required significantly less training time, showing a \textbf{26.3\%} reduction in training time compared to the 32-bit setting.

\subsection{Summary}
Our comprehensive experimental analysis reveals that the Numerical Guarantee Method markedly improves the stability and efficacy of 16-bit precision training in various neural network architectures. This method effectively bridges the performance gap between 16-bit and 32-bit precision training, while simultaneously offering significant speed enhancements.

In our experiments with Deep Neural Networks (DNNs) using the MNIST dataset, we observed that the Numerical Guarantee Method not only stabilized the training process but also accelerated it. The training times were reduced by approximately 45\% compared to 32-bit training, without compromising on accuracy. This is particularly noteworthy in scenarios where computational resources or time are limited.

For Convolutional Neural Networks (CNNs), specifically the ResNet-56 model trained on the CIFAR-10 dataset, the improvements were equally impressive. The 16-bit training with the Numerical Guarantee Method was 28.4\% faster than its 32-bit counterpart. This acceleration is crucial for deep learning models that require extensive computational resources and time.

In the case of Vision Transformers (ViTs) trained on the CIFAR-10 dataset, our method demonstrated a 26.3\% reduction in training time compared to 32-bit precision, while maintaining comparable accuracy levels. This finding is significant given the increasing popularity and computational demands of transformer models in various applications.

A critical observation across all experiments was the mitigation of training failures in pure 16-bit settings at lower epsilon values. Typically, training in such high-precision environments is prone to numerical instability, leading to failed training or suboptimal models. Our method effectively addresses this issue, ensuring stable and successful training across a range of epsilon values.

In summary, the Numerical Guarantee Method presents a robust solution for enhancing the stability and efficiency of neural network training, particularly in 16-bit precision settings. It offers a viable alternative to 32-bit training, reducing computational demands and training times, which is invaluable in the rapidly evolving field of deep learning.



