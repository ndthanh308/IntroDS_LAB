\documentclass{article}

\usepackage[final, nonatbib]{neurips_2023}
\usepackage[square,sort,comma,numbers]{natbib}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}


\input{macros}


\title{Trustworthy Optimization: A Novel Approach to Counter Numerical Instability in 16-bit Neural Network Training}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
Juyoung Yun \\
Department of Computer Science\\
Stony Brook University\\
juyoung.yun@stonybrook.edu\\
}

\begin{document}
% \footnotetext[1]{Address of Institute A}%
% \footnotetext[2]{These authors contributed equally to this work.}%

\maketitle

\begin{abstract}
In this research, we address critical trustworthiness concerns related to the numerical instability observed in 16-bit computations of machine learning models. Such instability, particularly when employing popular optimization algorithms like RMSProp and Adam, often leads to unreliable training of deep neural networks. This not only disrupts the learning process but also poses significant challenges in deploying dependable models in real-world applications. Our investigation identifies the epsilon hyperparameter as the primary source of this instability. A nuanced exploration reveals that subtle adjustments to epsilon within 16-bit computations can enhance the reliability of RMSProp and Adam, enabling more trustworthy training of 16-bit neural networks. We propose a novel, dependable approach that leverages updates from the Adam optimizer to bolster the stability of the learning process. Our contributions provide deeper insights into optimization challenges in low-precision computations and offer solutions to ensure the trustworthiness and stability of deep neural network training, paving the way for their dependable use in various applications.
\end{abstract}


\section{Introduction}
\input{contents/1-intro}

\section{Related Works}
\input{contents/2-related}

\section{Analysis}
\input{contents/3-analysis}

\section{Method}
\input{contents/4-method}

\section{Results}
\input{contents/5-results}

\section{Discussion}
\input{contents/6-discussion}

\section{Conclusions}
\input{contents/7-conclusion}

\bibliographystyle{plain}
\bibliography{main}
%\bibliographystyle{icml2023}

\end{document}