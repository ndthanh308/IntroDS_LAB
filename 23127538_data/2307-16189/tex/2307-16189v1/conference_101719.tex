\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{array}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training
}

\author{
\\
\IEEEauthorblockN{Juyoung Yun}
\\
\IEEEauthorblockA{\textit{Department of Computer Science, Stony Brook University} \\
\textit{Department of Applied Mathematics and Statistics, Stony Brook University} \\
juyoung.yun@stonybrook.edu
\\
}
}
\maketitle

\begin{abstract}
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit computations. This study contributes to better understanding of optimization in low-precision computations and provides an effective solution to a longstanding issue in training deep neural networks, opening new avenues for more efficient and stable model training.
\end{abstract}

\begin{IEEEkeywords}
Deep Learning, Low Precision, Neural Network, High Performance, Optimizer
\end{IEEEkeywords}

\section{Introduction}
\input{Contents/1-intro}

\section{Related Works}
\input{Contents/2-related}

\section{Analysis}
\input{Contents/3-analysis}

\section{Method}
\input{Contents/4-method}

\section{Results}
\input{Contents/5-results}

\section{Discussion}
\input{Contents/6-discussion}

\section{Conclusions}
\input{Contents/7-conclusion}

\input{Contents/ref}

\end{document}
