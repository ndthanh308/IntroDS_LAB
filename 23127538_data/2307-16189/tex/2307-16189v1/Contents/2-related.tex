The matter of numerical precision in deep learning model training has garnered substantial attention in recent years. A milestone study by Gupta et al. \cite{Gupta2015} was among the first to explore the potential of lower numerical precision in deep learning, emphasizing the critical balance between computational efficiency and precision. They suggested that with careful implementation, lower precision models can be as effective as their higher precision counterparts while requiring less computational and memory resources. Building upon these findings, significant advancements have been made in utilizing 16-bit operations for training convolutional neural networks. Courbariaux et al. \cite{Courbariaux2016} pioneered a novel technique to train neural networks using binary weights and activations, significantly reducing memory and computational demands. Similarly, Micikevicius et al. \cite{Micikevicius2017} stressed the imperative transition from 32-bit to 16-bit operations in deep learning, given the memory and computational constraints associated with training increasingly complex neural networks. Their research demonstrated that half-precision computations offer a more memory and computation-efficient alternative without compromising the model's performance. Despite these advancements, numerical instability during backpropagation remains a persistent challenge. Bengio et al. \cite{Bengio1994} illustrated the difficulties encountered in learning long-term dependencies with gradient descent due to numerical instability. Such findings underscore the need for innovative solutions to mitigate this widespread issue. One critical component to addressing this challenge lies in the development of effective optimizers. Adam, an optimizer introduced by Kingma and Ba \cite{kingma2014adam}, is known to face issues of numerical instability when employed in lower-precision environments. Another research by Yun et al. \cite{yun2023defense}, provided a comprehensive theoretical analysis, focusing on the performance of pure 16-bit floating-point neural networks. They introduced the concepts of floating-point error and tolerance to define the conditions under which 16-bit models could approximate their 32-bit counterparts. Their results indicate that pure 16-bit floating-point neural networks can achieve similar or superior performance compared to their mixed-precision and 32-bit counterparts, offering a unique perspective on the benefits of pure 16-bit networks. Lastly, in the context of efficient neural network training, Han et al. \cite{Han2015} proposed a three-stage pipeline that significantly reduces the storage requirements of the network. Their work forms an integral part of the broader discussion on efficient neural network training, further reinforcing the relevance of our research on 16-bit operations. Through our investigation, we aim to contribute to this body of work by presenting a novel approach to addressing numerical instability issues associated with 16-bit precision in the realm of deep learning model training.