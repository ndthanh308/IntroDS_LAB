Our experimental setup was comprised of a deep neural network (DNN) trained on the MNIST dataset \cite{lecun1998gradient}, a ubiquitous benchmark in the field of machine learning, to evaluate the performance of two widely adopted optimization algorithms: RMSProp \cite{tieleman2012lecture} and Adam \cite{kingma2014adam}. The network was implemented using the TensorFlow framework, and a batch size of 512 was utilized during the training process. The architecture of the DNN was relatively simple, containing a single linear layer, which provided a streamlined environment to assess the relative merits of the two optimization strategies.

The results of the performance comparison between the RMSProp and Adam optimizers under different precision settings are summarized in Table \ref{tab1}. These findings are detailed as follows:

\subsection{16-bit Precision with Numerical Guarantee}
In the 16-bit precision setting, with the application of numerical guarantee methods (denoted as 'O' in Table \ref{tab1}), both the RMSProp and Adam optimizers demonstrated consistently high test accuracies, with all values exceeding $0.98$. This was observed across all epsilon values, indicating a high degree of numerical stability even under limited precision conditions. In terms of time efficiency, there were only minor fluctuations, with the RMSProp optimizer requiring roughly $98$ to $100$ seconds per training epoch, and the Adam optimizer needing approximately $110$ seconds.

\subsection{16-bit Precision without Numerical Guarantee}
In scenarios where the numerical guarantee was not enforced (indicated by 'X' in Table \ref{tab1}), a decline in test accuracy was observed for both RMSProp and Adam optimizers when the epsilon value decreased below $10^{-3}$. Despite this decline in accuracy, the computation times were relatively stable, with minor increases compared to the cases where numerical guarantees were incorporated. This shows our observation about risk of epsilon for optimizer is true.

\subsection{32-bit Precision without Numerical Guarantee}
In contrast, for the 32-bit precision setting, even in the absence of a numerical guarantee, the RMSProp and Adam optimizers both maintained stable test accuracies. However, it is noteworthy that the time taken for computation increased significantly compared to the 16-bit precision settings, with the RMSProp optimizer ranging between $180.59$ to $183.81$ seconds and the Adam optimizer requiring between $202.72$ to $205.10$ seconds.

The experiments were conducted on laptop GPUs featuring RTX 3080. An intriguing observation was that 16-bit computations were at least 40 percent faster than their 32-bit counterparts. This advantage of computational speed underlines the potential of utilizing 16-bit precision, particularly in contexts with constrained computing resources such as laptops.

This study successfully establishes the critical role of numerical guarantee methods in maintaining high levels of accuracy, particularly in lower precision settings. It also emphasizes the trade-off between numerical stability and computational efficiency. While higher precision settings may be less prone to numerical instability, they demand significantly more computational resources.

To summarize, our research offers a novel approach to mitigate numerical instability in 16-bit neural networks without sacrificing accuracy. This contribution holds the potential to stimulate further advancements in the field of machine learning, particularly in applications where computational resources are limited. The findings underscore the importance of careful consideration of numerical precision in the implementation of deep learning models and highlight the potential advantages of lower-precision computations. Further investigations would be beneficial to validate these findings in more complex models, such as Convolutional Neural Networks (CNNs) and Vision Transformers.