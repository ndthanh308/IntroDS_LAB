In this study, we present a novel approach designed to alleviate numerical instability, a common problem associated with widely-used optimizers such as Adam and RMSProp, in the training of 16-bit neural networks. These optimizers have frequently encountered challenges when working with smaller epsilon values, particularly in instances involving division operations. The genesis of our method stems from an in-depth mathematical exploration of the update rules in the Adam optimizer, which revealed that instability originates during the gradient update step - a phase where the momentum term ($\hat{m}$) is divided by the sum of the square root of the velocity term ($\hat{v}$) and the epsilon parameter ($\epsilon_t$). To circumvent this instability, we propose a refined version of the gradient computation equation, which effectively prevents division by an exceedingly small epsilon value. This is achieved by introducing a maximum function that sets a lower threshold on the denominator. Consequently, the updated Adam optimizer follows the equation:
\begin{align*}
w_{t} & = w_{t-1} - \frac{\eta \cdot \hat{m_t}}{\sqrt{\max(\hat{v_t}, \epsilon)}}
\end{align*}
\begin{table*}[ht]
\caption{Performance comparison between RMSProp and Adam optimizers in 16-bit and 32-bit precision settings.}
\centering
\begin{tabular}{ccccccc}
\hline

\textbf{Precision} & \textbf{Epsilon} &\textbf{Numerical Guarantee} & \textbf{Test Accuracy} & \textbf{Time (Seconds)} &  \textbf{Test Accuracy} & \textbf{Time (Seconds)} \\ 
 &  &\textbf{Method} & \textbf{RMSProp} & \textbf{RMSProp} &  \textbf{Adam} & \textbf{Adam} \\
\hline
16-bit & 1E-1 & O & 0.985 & 98.90 & 0.967 & 111.23 \\ 
16-bit & 1E-2 & O & 0.987 & 98.41 & 0.987 & 110.99 \\ 
16-bit & 1E-3 & O & 0.988 & 98.21 & 0.987 & 110.79 \\ 
16-bit & 1E-4 & O & 0.988 & 98.25 & 0.987 & 110.52 \\ 
16-bit & 1E-5 & O & 0.987 & 98.23 & 0.988 & 110.54 \\ 
16-bit & 1E-6 & O & 0.984 & 97.67 & 0.987 & 110.72 \\ 
16-bit & 1E-7 & O & 0.980 & 99.58 & 0.980 & 110.61 \\ 
\hline
16-bit  & 1E-1 & X & 0.987 & 99.84 & 0.987 & 111.20 \\ 
16-bit  & 1E-2 & X & 0.988 & 99.81 & 0.988 & 111.12 \\ 
16-bit  & 1E-3 & X & 0.988 & 99.91 & 0.986 & 111.12 \\ 
16-bit  & 1E-4 & X & 0.986 & 99.52 & 0.098 & 111.02 \\ 
16-bit  & 1E-5 & X & 0.098 & 99.73 & 0.098 & 111.21 \\ 
16-bit  & 1E-6 & X & 0.098 & 100.29 & 0.098 & 111.51 \\ 
16-bit  & 1E-7 & X & 0.098 & 99.95 & 0.098 & 111.23 \\ 
\hline
32-bit & 1E-1 & X & 0.981 & 182.79 & 0.981 & 203.71 \\ 
32-bit & 1E-2 & X & 0.983 & 183.81 & 0.982 & 203.61 \\ 
32-bit & 1E-3 & X & 0.983 & 182.44 & 0.984 & 202.72 \\ 
32-bit & 1E-4 & X & 0.977 & 183.66 & 0.967 & 203.45 \\ 
32-bit & 1E-5 & X & 0.976 & 180.85 & 0.963 & 203.70 \\ 
32-bit & 1E-6 & X & 0.960 & 181.21 & 0.966 & 204.27 \\ 
32-bit & 1E-7 & X & 0.964 & 180.59 & 0.960 & 205.10 \\ 
\hline
\end{tabular}
\label{tab1}
\end{table*}
\textbf{Numerical Guarantee Method}: The modification guarantees that the denominator remains within a safer numerical range, thus facilitating a more stable gradient update. The addition of the stabilizing term, $\sqrt{\max(\hat{v_t}, \epsilon)}$, ensures the denominator does not dwindle excessively, thereby avoiding abnormally large updates. Should $\hat{v_t}$ fall below $\epsilon$, the term assumes the square root of $\epsilon$ instead. The precise value employed as the lower limit (in this case, $\epsilon$ is 1e-4) can be tailored according to the specific requirements of the neural network in consideration. Our proposed adjustment to the Adam optimizer offers a myriad of benefits, which are elaborated upon subsequently:
\\
\begin{itemize}
\item Enhanced Numerical Stability: By taking the maximum of $\hat{v_t}$ and a small constant inside the square root function, we mitigate the numerical instability that arises when $\hat{v_t}$ is very close to zero. This enhancement significantly reduces the chances of overflow and underflow during computations, which in turn increases the robustness of the optimizer.

\item Preservation of Adam's Benefits: The modification does not alter the fundamental characteristics and benefits of the Adam optimizer. It retains the benefits of Adam, such as efficient computation, suitability for problems with large data or parameters, and robustness to diagonal rescale or translations of the objective functions.

\item Ease of Implementation: The modification requires just a minor change to the original Adam algorithm, making it easy to implement in practice. This simplicity of modification allows for easy integration into existing machine learning frameworks and pipelines, thereby increasing its accessibility to practitioners.

\item Applicability to 16-bit Computations: The updated method enables Adam to work effectively on 16-bit neural networks, which was a challenge with the original Adam optimizer due to numerical instability issues. Thus, it extends the applicability of Adam to systems and applications where memory efficiency is crucial.

\item Flexibility: This modification is generalizable and can be applied to other optimizers based on the same concept as Adam, thereby providing a broad range of applications.\\
\end{itemize}

Overall, these advantages make this modification an effective solution to the problem of numerical instability in Adam and similar optimizers, particularly in the context of 16-bit computations. Our approach presents a novel way to alleviate numerical instability in 16-bit neural networks, without losing the advantages offered by advanced optimizers such as Adam and RMSProp. We will demonstrate the effectiveness of our approach through comprehensive experimental results in the following section.
