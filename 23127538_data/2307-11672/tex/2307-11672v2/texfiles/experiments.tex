\section{Experimental Results}
\label{sec:exp}
We present the following experimental analysis of RFI in this section: (1) evaluation of RFI against adaptive attacks resulting in consistent improvement in the robust performance in Section~\ref{ss:rfi_robustness_small_models}; 
(2) transfer attack evaluation of RFI showing the strength of RFI as well as establishing that RFI does not result in gradient obfuscation in Section~\ref{ss:rfi_transfer_main_draft};
(3) in Section~\ref{ss:static_dyn_rfi} we adapt static RFI to a dynamic adaptive test-time defense and show that static RFI is better than dynamic RFI. Consequently, we compare static RFI to other test-time defenses in Section~\ref{ss:adaptive_defense_comparison} showing RFI outperforms other dynamic test-time defenses;
(4) we discuss the abalations on RFI in Section~\ref{ss:abalation}.
%
%The experiments section is structured as follows (1) Description of the Datasets and Resources, Attacks and Defenses, and details about benchmarks used in evaluation. (2) In section~\cref{ss:rfi_robustness_small_models} we evaluate the performance of RFI against adaptive attacks. (3) In section~\ref{ss:rfi_transfer_main_draft} we evaluate the transfer attack performance and establish that RFI does not perform gradient obfuscation. (4) In section~\ref{ss:static_dyn_rfi} we also propose an approach to make RFI a dynamic adaptive test-time defense. (5) Section~\ref{ss:adaptive_defense_comparison} compares static-RFI to other test-time defenses. (6) We discuss the abalations on RFI in section~\ref{ss:abalation}.
%Since \citet{qin2021improving, grabinski2022robust, stutz2020confidence} show that calibration improves adversarial robustness and \citet{tao2023benchmark} demonstrate that they are highly correlated. 
Additionally, we present the performance of RFI on calibrated models using temperature scaling \citep{guo2017calibration} in the appendix since it has been shown to improve robustness ~\citep{qin2021improving, grabinski2022robust, stutz2020confidence, tao2023benchmark}. 
%We first introduce the different evaluation methods for adversarial robustness that we use, network architectures, and other test-time defenses.
%For our experimental analysis, we use the CIFAR-10 and CIFAR 100~\cite{cifar10} datasets.

\textbf{Datasets \& Resources.} We evaluate RFI on CIFAR-10, CIFAR-100 \citep{cifar10}, Robust CIFAR-10 \citep{featuresnotbugs}, tiny ImageNet \citep{le2015tiny} and ImageNet \citep{russakovsky2015imagenet} datasets. We use Pytorch~\cite{paszke2019pytorch} for all our experiments \& a single Nvidia DGX A100 to run all of our experiments. We also open-source our implementation at \url{https://github.com/Anurag14/RFI}.


\textbf{Adversarial Attacks.} We evaluate RFI on different white and black-box adversarial attacks namely, \emph{Projected Gradient Descent (PGD)}~\cite{madry2018towards}, a white-box attack that perturbs the input within a small $\ell_p$ radius $\epsilon$, so that it maximizes the loss of a model. We perform both $\ell_{\infty}$ and $\ell_2$ PGD attack with standard perturbation $\epsilon$, attack step size and iteration for each dataset. %$\epsilon=8/255$ and $\epsilon=0.5$, respectively, with the attack step size $\epsilon/4$ and $40$ iterations for $\ell_{\infty}$ and $\epsilon/5$ and $100$ iterations for $\ell_2$ to have an overall high strength PGD attack. 
%(b) Parameter-free attacks such as Auto PGD-Cross Entropy (APGD-CE) and Auto PGD-Difference Logit Ratios (APGD-DLR) \citep{croce2020reliable} under $\ell_{\infty}$ threat model, that are extensions of PGD attack with no step size parameter and stronger than PGD.
%We then obtain robust accuracy for standard and adversarially trained models before and after integrating RFI. 
%In table~\ref{tab:benchmarkcomparison} we evaluate the effectiveness of RFI on APGD-CE, APGD-DLR~\cite{croce2020reliable}, and RobustBench~\cite{croce2020robustbench} under $\ell_{\infty}(\epsilon=8/255)$ threat model. APGD-CE and APGD-DLR are parameter-free attack extensions of PGD attack which are based on cross entropy (CE) and difference of logit ratios (DLR). 
\emph{AutoAttack} \citep{croce2020robustbench}, a suite of white-box and black-box attacks including Auto PGD-Cross Entropy (APGD-CE), Auto PGD-Difference Logit Ratios (APGD-DLR) \citep{croce2020reliable}, Fast Adaptive Boundary Attack (FAB) \citep{croce2020minimally}, and Square Attacks \citep{andriushchenko2020square}. 
\emph{APGD-CE and APGD-DLR} are parameter-free white-box attacks that are extensions of PGD attack with no step size parameter and stronger than PGD.
\emph{FAB} is a white-box attack that minimizes the norm of the adversarial perturbation.
\emph{Square Attack} is an efficient black-box attack that is score based and uses random search without gradient approximations.
While \emph{adaptive attacks} generate the adversarial images using the target model, \emph{transfer attacks} generate adversarial images using a surrogate model and attack the target model. 
%RobustBench overcomes the limitations of auto-attack by flagging potential overestimation of robustness.

\textbf{Benchmarking on SoTA defenses.}
We evaluate RFI on various architectures trained differently: \emph{ResNet-18} and \emph{ResNet-50} with standard training and the popular adversarial training methods such as \emph{PGD}~\cite{madry2018towards}, \emph{Interpolated Adversarial Training (IAT)}~\cite{lamb2019interpolated}, \emph{Carlini-Wagner (C\&W)} loss~\cite{carlini2017towards} and \emph{TRADES}~\citep{zhang2019theoretically}. %also use Robust CIFAR-10 dataset created by \citet{featuresnotbugs}.
%We observe that the addition of RFI at inference time does improve the performance across all the methods. 
%For our table~\ref{tab:benchmarkcomparison} 
 We select different \emph{state-of-the-art adversarially trained  models} from RobustBench upon which at the test time we integrate RFI ~\citep{carmon2019unlabeled,engstrom2019adversarial,rice2020overfitting,wang2023better,pang2022robustness}.
These methods either use additional data ~\citep{carmon2019unlabeled, wang2023better}, informed adversarial prior~\citep{engstrom2019adversarial} or early stopping ~\citep{rice2020overfitting} to improve the robustness of models.
%\citet{carmon2019unlabeled} show that additional $500,000$ unlabeled data improve robustness on CIFAR-10. \citet{engstrom2019adversarial} argue that the limitation of a network to represent input data in high-level features causes two semantically different images to have similar representation and propose training with an adversarial prior improves robustness. \citet{rice2020overfitting} suggest early stopping-based training to overcome this robust overfitting. \citet{wang2023better} propose use of class-conditioned diffusion models to generate additional labeled data for adversarial training and observe state-of-the-art performance using additional $50$ million synthetic images in training.
%\citet{pang2022robustness} .. 
We detail each training method in appendix. 


\textbf{Evaluation measures.}
We measure the performance of models with and without RFI by the accuracy of predictions to both clean/original samples and adversarial samples averaged over $5$ runs. We use `Clean' to denote accuracy of models to original samples.
Note that there are no standard deviation in our evaluation of models from RobustBench as we are directly loading the models without training, hence no stochasticity.
Details of the model evaluation are in Appendix~\ref{app:exp_details}.
We further remark that our defense strategy does not circumvent gradient based attacks due to gradient masking \citep{athalye2018synthesizing} since we simply project the last layer feature in its covariance eigenspace, hence the network remains differentiable with active gradients.

\textbf{Comparison to adaptive test-time defenses.}
%Recently, \citet{croce2022evaluating} provide an overview of $9$ adaptive test-time defenses of which only four methods actually improve robustness and are competitive with the current state-of-the-art performances. 
We compare RFI with two adaptive test-time defenses: \emph{SODEF}~\citep{kang2021stable} and \emph{Anti-adv}~\citep{alfarra2022combating}. The choice of SODEF and Anti-adv is due to their relatively faster inference costs $2\times$ and $8\times$, respectively, and are representative of model adaptation and input modification strategies for adaptive test-time defenses, respectively.

\begin{comment}
\begin{table*}[t]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI.} Accuracy for (a) Adaptive attacks are evaluated. (b) and (c) are study of transfer attacks where the adversarial examples are generated from the base method+RFI in order to evaluate the base method, and vice versa. We consider $\ell_\infty$ and $\ell_2$ PGD attack on CIFAR-10 with Resnet-18. $\ell_\infty$ attack with step size $\epsilon/4$ and $40$ iterations. $\ell_2$ attack with size $\epsilon/5$ and $100$ iterations. Results for CIFAR-100 and tiny ImageNet are provided in Appendix (Tables~\ref{tab:cifar100table} and \ref{tab:tinyImagenet}).}
    \begin{subtable}[]{0.54\textwidth}
    \caption{{Adaptive Attack evaluation} }
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} &
\multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Method & +RFI && Method & +RFI &&  Method & +RFI\\ \midrule
%\multirow{5}{*}{CIFAR-10}&
Standard & \textbf{95.28} & 88.53 && 1.02  & \textbf{4.35}  && 0.39 & \textbf{9.73}\\
Robust CIFAR-10 & 78.69 & \textbf{78.75} && 1.30 & \textbf{7.01} && 9.63 & \textbf{11.00}\\
         PGD & \textbf{83.53} & 83.22 && 42.20  & \textbf{43.29} && 54.61 & \textbf{55.03}\\
         IAT & \textbf{91.86} & 91.26 && 44.76  & \textbf{46.95}  && 62.53 & \textbf{64.31}\\
         C\&W & \textbf{85.11} & 84.97 && 40.01 & \textbf{42.56} && 55.02 & \textbf{56.79}\\
         TRADES &  \textbf{81.13} & 80.76 && 51.98 &\textbf{53.45} && 60.03  & \textbf{61.39}\\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp}
    \end{subtable}
    \begin{subtable}[]{0.23\textwidth}
        \caption{{Adversary from Method+RFI}}
    \resizebox{0.96\linewidth}{!}{
    \begin{tabular}{{@{}ccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} & \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& 
%\multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
%\phantom{} & 
%\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
%\cmidrule{1-2} \cmidrule{4-5}
\multicolumn{1}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{1}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{1-1} \cmidrule{3-3}
%&
 % Method & +RFI &&  Method & +RFI\\ 
 Method && Method \\
 \midrule
%\multirow{5}{*}{CIFAR-10}&
 \color{brightmaroon}0.00  && \color{brightmaroon}0.01 \\
  \color{brightmaroon}0.03  && \color{brightmaroon}1.05 \\
          \color{brightmaroon}34.80   && \color{brightmaroon}49.90 \\
          \color{brightmaroon}35.78    && \color{brightmaroon}55.42 \\
          \color{brightmaroon}\textbf{3.92}  && \color{brightmaroon}\textbf{13.50} \\
          \color{brightmaroon}{51.70}  && \color{brightmaroon}{58.09}  \\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp_transferability_1}
    \end{subtable}
     \begin{subtable}[]{0.21\textwidth}
        \caption{{Adversary from Method}}
    \resizebox{1.04\linewidth}{!}{
    \begin{tabular}{{@{}ccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} & \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& 
%\multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
%\phantom{} & 
%\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
%\cmidrule{1-2} \cmidrule{4-5}
\multicolumn{1}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{1}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{1-1} \cmidrule{3-3}
%&
 % Method & +RFI &&  Method & +RFI\\ 
 +RFI && +RFI \\
 \midrule
%\multirow{5}{*}{CIFAR-10}&
 \color{applegreen}10.36  && \color{applegreen}12.09 \\
 \color{applegreen}15.41  && \color{applegreen}17.38 \\
          \color{applegreen}\textbf{46.02}   && \color{applegreen}\textbf{58.81} \\
          \color{applegreen}49.06    && \color{applegreen}66.67 \\
          \color{applegreen}\textbf{45.48}  && \color{applegreen}58.95 \\
          \color{applegreen}54.33  && \color{applegreen}\textbf{65.23}  \\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp_transferability_2}
    \end{subtable}
    \label{tab:basic_cifar10}
\end{table*}
\end{comment}
\begin{table}[t]
\centering
    \caption{\textbf{Adaptive attack performance of RFI.} We consider $\ell_\infty$ and $\ell_2$ PGD attack on CIFAR-10 with Resnet-18 and $\ell_\infty$ attack with step size $\epsilon/4$ and $40$ iterations. $\ell_2$ attack with size $\epsilon/5$ and $100$ iterations. 
    RFI improves the performance on an average by \color{applegreen}{$\mathbf{2\%}$}. }
    \vspace{-0.1cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{{@{}lccccccccccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} &
\multirow{2.5}{*}{Training} & \multicolumn{3}{c}{Clean} & \phantom{}& \multicolumn{3}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{3}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}
%&
& Method & +RFI & $\%$ Gain && Method & +RFI & $\%$ Gain &&  Method & +RFI & $\%$ Gain\\ \midrule
%\multirow{5}{*}{CIFAR-10}&
Standard & \textbf{95.28} \tiny{$\pm$ 0.04} & 88.53 \tiny{$\pm$ 0.04} & \color{brightmaroon}\textbf{-6.75} && 1.02\tiny{$\pm$ 0.12}   & \textbf{4.35}\tiny{$\pm$ 0.08} & \color{applegreen}\textbf{+3.33} && 0.39\tiny{$\pm$ 0.00}  & \textbf{9.73}\tiny{$\pm$ 0.10} & \color{applegreen}\textbf{+9.34}\\
Robust CIFAR-10 & 78.69\tiny{$\pm$ 0.01}  & \textbf{78.75}\tiny{$\pm$ 0.02} & \color{applegreen}\textbf{+0.06} && 1.30\tiny{$\pm$ 0.09}  & \textbf{7.01}\tiny{$\pm$ 0.10} & \color{applegreen}\textbf{+5.71} && 9.63\tiny{$\pm$ 0.15}  & \textbf{11.00}\tiny{$\pm$ 0.14} & \color{applegreen}\textbf{+1.37}\\
         PGD & \textbf{83.53}\tiny{$\pm$ 0.01} & 83.22\tiny{$\pm$ 0.02} & \color{brightmaroon}\textbf{-0.31}  && 42.20\tiny{$\pm$ 0.00}   & \textbf{43.29}\tiny{$\pm$ 0.00} & \color{applegreen}\textbf{+1.09} && 54.61\tiny{$\pm$ 0.00}  & \textbf{55.03}\tiny{$\pm$ 0.00} & \color{applegreen}\textbf{+0.42} \\
         IAT & \textbf{91.86}\tiny{$\pm$ 0.01}  & 91.26\tiny{$\pm$ 0.00} & \color{brightmaroon}\textbf{-0.60} && 44.76\tiny{$\pm$ 0.03}   & \textbf{46.95}\tiny{$\pm$ 0.00} & \color{applegreen}\textbf{+2.19}  && 62.53\tiny{$\pm$ 0.01}  & \textbf{64.31}\tiny{$\pm$ 0.01} & \color{applegreen}\textbf{+1.78} \\
         C\&W & \textbf{85.16} \tiny{$\pm$ 0.12} & 84.91 \tiny{$\pm$ 0.16} & \color{brightmaroon}\textbf{-0.25} && 40.12 \tiny{$\pm$ 0.16} & \textbf{42.33} \tiny{$\pm$ 0.32} & \color{applegreen}\textbf{+2.21} && 55.18 \tiny{$\pm$ 0.28} & \textbf{56.68} \tiny{$\pm$ 0.30} & \color{applegreen}\textbf{+1.50}\\
         TRADES &  \textbf{81.22} \tiny{$\pm$ 0.21} & 80.68 \tiny{$\pm$ 0.38} & \color{brightmaroon}\textbf{-0.54} && 51.93 \tiny{$\pm$ 0.25} &\textbf{53.50} \tiny{$\pm$ 0.27} & \color{applegreen}\textbf{+1.57} && 59.87 \tiny{$\pm$ 0.36}  & \textbf{61.27} \tiny{$\pm$ 0.44} & \color{applegreen}\textbf{+1.40}\\
         \bottomrule
    \end{tabular}}
    \vspace{-0.3cm}
    \label{tab:basicexp}
\end{table}
%\subsection{Results and Discussion}
\subsection{RFI improves adversarial robustness consistently} \label{ss:rfi_robustness_small_models}
% \textbf{Adaptive attack evaluation.} 
We evaluate RFI for adaptive attacks by generating adversarial samples to specifically target our defense~\citep{tramer2020adaptive}. 
%We first calibrate the standard and adversarially trained models using temperature scaling~\citep{guo2017calibration} and obtain clean and robust accuracy
We obtain clean and robust accuracy for standard and adversarially trained models before and after integrating RFI and setting $K$ to the number of classes.%, that is, $K=10$ and $K=100$ for CIFAR-10 and CIFAR-100, respectively. 
The results for different training procedures on CIFAR-10 with ResNet-18 are presented in Table~\ref{tab:basicexp} (CIFAR-100 with ResNet-18 and tiny ImageNet with ResNet-50 in Tables~\ref{tab:cifar100table} and \ref{tab:tinyImagenet}, respectively, in Appendix). Robust CIFAR-10 denotes standard training using Robust CIFAR-10 dataset.
% We allow all attacks in Table~\ref{tab:basicexp} and ~\ref{tab:benchmarkcomparison} to adapt to our defense therefore making them adaptive~\citep{tramer2020adaptive}.
We observe that \emph{our method consistently improves the robust performance of adversarially trained models, on an average by $2\%$}. 
There is a minor drop in the clean performance as we choose only a subset of the informative features that are also robust ($K\ne p$), hence a loss in information to achieve the best possible clean performance as derived in Corollary~\ref{cor:information}.
Nevertheless, the gain in robust performance is with \emph{almost no computational overhead}. 
The seemingly small improvement in performance is mainly due to the fact that we are adapting the trained model without any further learning, as well as the adaptive attacks on RFI results in a stronger adversary than the base model as we discuss in transfer attack evaluation subsequently.
Additional experiments showing the effectiveness of RFI on Expectation Over Transformation attack \citep{athalye2018synthesizing} is presented in Table~\ref{tab:rfi_eot} of Appendix.
Furthermore, RFI improves the robustness of calibrated models by \emph{$4\%-8\%$} as shown in Tables~\ref{tab:rfi_calib}, \ref{tab:cifar100table} and \ref{tab:tinyImagenet} of Appendix.
\begin{comment}
\begin{table}[h]
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{{@{}lccccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{4}{c}{Clean} &
    \phantom{} & 
    \multicolumn{4}{c}{$\ell_\infty(\epsilon=\frac{8}{255})$} \\
    \cmidrule{2-5} \cmidrule{7-10}
    %&
        & Base & +Calib & +RFI & +Calib+RFI && Base & +Calib & +RFI &  +Calib+RFI \\ \midrule 
         PGD & 83.53 & 83.10 & 83.22 & 82.34 && 42.20 & 42.95 & 43.29 & \textbf{50.10} \\
         IAT & 91.86 & 91.22 & 91.26 & 90.78 && 44.76 & 46.01 & 46.95 & \textbf{51.66} \\
         C\&W & 85.11 & 84.32 & 84.97 & 83.21 && 40.01 & 41.88 & 42.56 &
         \textbf{51.22} \\
         TRADES& 81.13 & 81.10 & 80.76 & 79.30 && 51.98 & 53.28 & 53.45 & \textbf{58.13}\\
        \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
    \caption{Performance of RFI in adaptive attack for calibrated models using temperature scaling $[0.05, 0.1]$.}
    \vspace{-0.6cm}
    \label{tab:rfi_calib}
\end{table}
\end{comment}
\begin{comment}
\begin{table}[h]
    \centering
    % \vspace{-0.5cm}
    \caption{\textbf{Static RFI is better than Dynamic/Adaptive RFI.} Setting same as Table~\ref{tab:basicexp}. Adversarial examples are generated from the base model for fair comparison.}
    \vspace{-0.1cm}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Static & Dynamic && Static & Dynamic &&  Static & Dynamic\\ \midrule
      %  Standard &  \textbf{10.36} &  &&20.08 &\textbf{20.91}\\
       %  Robust CIFAR-10 & \textbf{58.94} & 58.88 && 22.56 & \textbf{23.58}\\
         %PGD & 83.22 & 82.86 && 46.02 & \textbf{46.83}  &&58.81 & \textbf{59.23}\\
         %IAT & 91.26 & \textbf{91.35} && \textbf{49.06} & 48.53 && \textbf{66.67} & 66.28\\
         %C\&W & 84.97 & 83.01 && \textbf{45.48} & 43.98 &&     
         %\textbf{58.95} & 57.82 \\
         %TRADES& 80.76 & 78.98 && \textbf{54.33} & 
         %53.58 && \textbf{65.23} & 65.00 \\
         PGD & \textbf{82.36} & 81.72 && 41.08 & \textbf{41.92}  &&\textbf{58.81} & 57.20\\
         IAT & \textbf{90.82} & 89.94 && \textbf{43.10} & 41.86 && 62.10 & \textbf{62.70}\\
         C\&W & 83.38 & \textbf{83.68} && \textbf{39.21} & 38.88 &&     
         \textbf{54.90} & 54.06 \\
         TRADES& \textbf{79.40} & 79.32 && \textbf{52.01} & 
         51.23 && \textbf{59.91} & 57.68 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
    \label{tab:rfi_static_adaptive}
\end{table}
\end{comment}
\begin{comment}
\begin{table*}[t]
    \centering
    \caption{\textbf{Adaptive attack performance of RFI on SoTA models.} We evaluate APGD-CE, APGD-DLR, FAB, Square and AutoAttack under $\ell_{\infty}(\epsilon=8/255)$ on CIFAR-10 and CIFAR-100. Inference time of RFI: $1\times$, Anti-adv: $8\times$, SODEF: $2\times$. Calibration+RFI gives $\color{applegreen}\mathbf{1-2\%}$ gain, whereas SODEF and Anti-adv results in {\color{warningyellow}{\textbf{no ($\mathbf{<0.1\%}$)}}} or {\color{brightmaroon}{\textbf{negative}}} gain. More results including ImageNet are in Appendix (Table~\ref{tab:benchmarkcomparison_app}). There is no standard deviation as the trained models are from RobustBench.}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{@{}lclcccccc@{}}
         \toprule 
         %Dataset & 
         & Base Method & Defense & Clean & APGD-CE  & APGD-DLR  & FAB & Square & AutoAttack\\
         \midrule
         \multirow{5}{*}{\begin{sideways}CIFAR-10\end{sideways}}&
         \multirowcell{4}{~\citet{carmon2019unlabeled}\\WideResNet-28-10} & None & \textbf{89.69} &  61.82 & 60.85 & 60.18 & 66.51 & 59.53\\
         &
         & Calibration & \textbf{89.69} & 62.29 &  61.48   &  61.90  &  66.83   &   60.59 \\
          % &
          % &Anti-adv & \textbf{89.69} & 61.81 & 60.89 & 60.11 & 66.58 & 58.70\\
          % &
          % &SODEF & 89.68 & 60.20 & 60.72 & 58.04 & 65.28 & 57.23\\
          &
          &Calibration+Anti-adv & 89.69 & 62.08 & 60.98 & 61.88 & 66.90 & \color{brightmaroon}\textbf{59.59}\\
          &
          &Calibration+SODEF & \textbf{90.25} & 60.91 & 58.58 & 61.02 & 64.28 & \color{brightmaroon}\textbf{57.10}\\
          &
          & Calibration+RFI & 89.60 & \textbf{62.45}  & \textbf{62.60}   &  \textbf{63.38}  &  \textbf{66.90}  &  \color{applegreen}\textbf{62.51} \\
          % &
          % &RFI ($K=10$) & 89.60 & 62.38 & 61.58 & 60.21 & 66.59 & 60.72\\
          % &
          % &RFI (opt. $K=20$) & 89.60 & \textbf{62.45} & \textbf{61.60}& \textbf{60.38}& \textbf{66.90} & \color{applegreen}\textbf{61.02}\\
         \cmidrule{1-9}
         %\midrule
         \multirow{5}{*}{\begin{sideways}CIFAR-100\end{sideways}}&
         \multirowcell{4}{~\citet{pang2022robustness}\\WideResNet-28-10} & None & {63.66} & 35.29 & 31.71& 31.32 & 35.70 & 31.08\\
          % &
          % &Anti-adv & 63.41 & 32.50 & 30.32 & 31.30 & 35.76 & 30.10\\
          % &
          % &SODEF & 63.08 & 30.96 & 29.54 & 31.44 & 32.27 &30.56\\
          &
          &Calibration & 63.64 & \textbf{37.90} & 31.90 & 31.28 & 35.71 & 31.10\\
          &
          &Calibration+Anti-adv & 63.66 & 37.83 & 31.09 & 31.14 & \textbf{35.82} & \color{warningyellow}\textbf{31.09}\\
          &
          &Calibration+SODEF & 63.66 & 37.23 & 31.62 & 31.21 & 35.75 & \color{warningyellow}\textbf{31.15}\\
          &
          &Calibration+RFI & \textbf{63.67} & 37.82 & \textbf{32.18} & \textbf{32.32} & 35.68 & \color{applegreen}\textbf{32.14}\\
          % &
          % &RFI ($K=100$)& 63.01 & 36.03 & \textbf{31.95}& 31.88 & 35.79 & 31.29\\
          % &
          % & RFI (opt. $K=115$) & 63.10 & \textbf{36.07} & \textbf{31.95} & \textbf{31.96} & \textbf{35.88} & \color{applegreen}\textbf{31.91}\\
         \bottomrule
    \end{tabular}
    }
    %\caption{(Base/Ours) Indicate the performance of adversarial training methods after adding our adaptive test time defense on APGD-CE and APGD-DLR attacks ~\cite{croce2020reliable}}
    \vspace{-0.2cm}
    \label{tab:benchmarkcomparison}
\end{table*}
\end{comment}
\subsection{Transfer Attack Evaluation: RFI is stronger than base model}
\label{ss:rfi_transfer_main_draft}
Many defences show remarkable robustness to adaptive attacks by obfuscating gradients, thereby circumventing gradient-based attacks and offering a false sense of security \citep{athalye2018obfuscated, huang2021adversarial}.
Therefore to validate the true effectiveness of a defense, evaluating transfer attack is crucial. Hence, we expand our evaluation from Table~\ref{tab:basicexp} to transfer attacks, where we assess the performance with and without RFI against adversarial samples generated from the base and base model+RFI.
%In this section, we focus on the transfer attacks on non-calibrated models with and without RFI. We further analyse robustness of RFI under transfer attacks from other defence and under calibration in Tables \ref{tab:transfer_attack_def_comparison} and~\ref{tab:rfi_calib_transferability}, respectively. 
%We observe that \emph{RFI outperforms the other methods in all the cases by a large margin of $2-5\%$ even when attacked by the adversary from the base. Moreover, base models lose considerable robustness when attacked with the adversary from the RFI}.
The results in Table~\ref{tab:transfercifar10} shows that \emph{RFI is more robust to attacks from base model whereas the base model loses considerable robustness when attacked with the adversary from RFI demonstrating that RFI is a stronger adversary than the base model}.
%The increase in performance with RFI is also considerable since the transfer attack from base model is weaker for RFI and the attack from base model + RFI is stronger for the base model. 
It is interesting to note that {the robustness of C\&W trained model is completely lost when tested against adversarial examples from C\&W+RFI model}. This clearly establishes that the \emph{RFI is not resulting in gradient obfuscation as C\&W is not a gradient based attack}. Contrastingly, TRADES results in a more robust model that withstands attack from TRADES+RFI. While the performance of TRADES is almost the same for adversarial attacks generated from TRADES and TRADES+RFI, RFI results in more robust models in both cases. We present the results on calibrated models in Tables \ref{tab:transfer_attack_def_comparison} and~\ref{tab:rfi_calib_transferability} in Appendix.
\begin{table}[b]
\caption{\textbf{Transfer attack on ResNet-18 for CIFAR-10.} Setting same as Table~\ref{tab:basicexp}. RFI results in much stronger adversary than the base method.}
\vspace{-0.1cm}
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.92\linewidth}{!}{
    \begin{tabular}{{@{}lcccccc@{}}}
    \multicolumn{6}{c}{Adversarial Examples are generated from \textbf{base model}}\\
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
    \phantom{} & 
    \multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
    \cmidrule{2-3} \cmidrule{5-6}
    %&
        & Method & +RFI &&  Method & +RFI\\ \midrule 
        Standard &  1.02 &\textbf{10.36} && 0.39 & \textbf{12.09}\\
        Robust  CIFAR-10 & 1.30 &\textbf{15.41}&& 9.63 &\textbf{17.38}\\
         PGD &  42.20 &\textbf{46.02}&& 54.61 &\textbf{58.81}\\
         IAT & 44.76&\textbf{49.06}&& 62.53 &\textbf{66.67}\\
         C\&W & 40.01 & \textbf{45.48} && 55.02 &\textbf{58.95} \\
         TRADES &51.98  & \textbf{54.33} && 60.03& \textbf{65.23} \\
        \bottomrule
    \end{tabular}}
}
\hfill
\parbox{.5\linewidth}{
\centering
\resizebox{0.94\linewidth}{!}{
\begin{tabular}{{@{}lcccccc@{}}}
    \multicolumn{6}{c}{Adversarial Examples are generated from \textbf{base model+RFI}}\\
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
    \phantom{} & 
    \multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
    \cmidrule{2-3} \cmidrule{5-6}
    %&
        & Method & +RFI &&  Method & +RFI\\ \midrule 
        Standard &  0.00 &\textbf{4.35} && 0.01 & \textbf{9.39}\\
        Robust CIFAR-10 &  0.03 &\textbf{7.01} && 1.05 &\textbf{11.00}\\
         PGD & 34.80  & \textbf{43.29} && 49.90 &\textbf{55.03}\\
         IAT &  35.78 &\textbf{46.95} && 55.42 &\textbf{64.31}\\
         C\&W & 3.92 & \textbf{42.56} && 13.50 & \textbf{56.79}\\
         TRADES & 51.70  & \textbf{53.45} && 58.09 & \textbf{61.39}\\
        \bottomrule
    \end{tabular}}
}
\label{tab:transfercifar10}
\vspace{-0.2cm}
\end{table}
\subsection{Static RFI is better than Dynamic/Adaptive RFI}
\label{ss:static_dyn_rfi}
%\subsection{Static is better than adaptive test-time defense using RFI and outperforms others for SoTA models}
%\textbf{Static is better than Dynamic/Adaptive RFI.}
The principle of RFI can be effectively used to adapt the model at test-time to every input by computing the transformation matrix $\Tilde{\bm{U}}$ using the robust feature score $s(\bm{u})$ of eigenvectors of \emph{test set} feature covariance $\Sigma_{test}$.
%and choosing the eigenvectors as per the robust feature score $s(\bm{u})$ of $\Sigma_{test}$. 
The results for this adaptive strategy is in Table~\ref{tab:rfi_static_adaptive} evaluated for the robust training settings of Table~\ref{tab:basicexp}. We consider transfer attack using the base model for fair comparison and observe  that \emph{static RFI is better than dynamic RFI}. This reinforces the theoretical result that the eigendirections of the training set feature covariance determines the most robust features (Corollary~\ref{cor:robust_score}).
Moreover, adaptive attacks in dynamic RFI needs further information on when to adapt since the model should be static until the attacker creates an adversarial sample, and the adaptive transformation using $\Tilde{\bm{U}}$ should be done only in the case of defender. Details of the challenges in deploying dynamic RFI when the use case is unknown, and the results for adaptive attacks are in Table~\ref{tab:rfi_static_adaptive_extra} in Appendix~\ref{app:static_vs_dynamic}.
\begin{table}[h]
    \centering
    \caption{\textbf{Comparison of static and dynamic/adaptive RFI.} Setting same as Table~\ref{tab:basicexp}. Adversarial examples are generated from the base model for fair comparison.}
    \vspace{-0.1cm}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Static & Dynamic && Static & Dynamic &&  Static & Dynamic\\ \midrule
      %  Standard &  \textbf{10.36} &  &&20.08 &\textbf{20.91}\\
       %  Robust CIFAR-10 & \textbf{58.94} & 58.88 && 22.56 & \textbf{23.58}\\
         PGD & 83.22 & 82.86 && 46.02 & \textbf{46.83}  &&58.81 & \textbf{59.23}\\
         IAT & 91.26 & \textbf{91.35} && \textbf{49.06} & 48.53 && \textbf{66.67} & 66.28\\
         C\&W & 84.97 & 83.01 && \textbf{45.48} & 43.98 &&     
         \textbf{58.95} & 57.82 \\
         TRADES& 80.76 & 78.98 && \textbf{54.33} & 
         53.58 && \textbf{65.23} & 65.00 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
    \label{tab:rfi_static_adaptive}
\end{table}

\subsection{Static RFI outperforms adaptive test-time defenses}
\label{ss:adaptive_defense_comparison}
%\textbf{RFI outperforms other adaptive test-time defenses on SoTA models.}
\begin{comment}
As a result of the static vs dynamic RFI evaluation, we compare the effectiveness of static RFI with other adaptive test-time defenses such as SODEF and Anti-adv against both white-box and black-box attacks. 
Results for APGD-CE, APGD-DLR, FAB, Square and AutoAttack for different SoTA methods across CIFAR-10, CIFAR-100 and ImageNet are summarized in Tables~\ref{tab:benchmarkcomparison} and \ref{tab:benchmarkcomparison_app} (Appendix). 
We observe: \emph{ (i) static RFI clearly outperforms other adaptive test-time defenses}.
Notably, RFI achieves robustness improvement without resorting to computationally intense strategies such as additional data or further training like Anti-adv and SODEF.
Moreover, surprisingly both SODEF and Anti-adv result in degradation of robustness to the attacks on large SoTA models, agreeing with the findings in \citet{croce2022evaluating}.
%Importantly, RFI despite being non-adaptive improves the robust performance ($1-2\%$) for all the SoTA methods, even though SODEF and Anti-adv achieve robustness (surprisingly, degrades the robustness in some cases) by incorporating computationally intense strategies like additional data or further training using informed priors. 
While this shows the strength and effectiveness of RFI, it also raises a fundamental question of whether it is necessary to adapt the model to individual test samples in order to improve the robustness in the adaptive test-time defense strategy. 
\textit{(ii) Despite a seemingly marginal performance improvement with RFI ($1-2\%$), it is noteworthy given the already large size and robust training of the SoTA models, leaving limited room for further improvement.} 
\emph{(iii) Combining Calibration and RFI proves more efficient in strengthening models against both white-box and black-box attacks compared to using only calibration or only RFI.} This synergy is confirmed on smaller models as well across different datasets in Tables~\ref{tab:rfi_calib}, \ref{tab:cifar100table} and \ref{tab:tinyImagenet}  in Appendix.
\end{comment}
%We provide evaluation of other SoTA models for CIFAR-10, CIFAR-100 and ImageNet showing similar observations in Table~\ref{tab:benchmarkcomparison_app} in Appendix.
As a result of the static vs dynamic RFI evaluation in Section~\ref{ss:static_dyn_rfi}, we compare the effectiveness of static RFI on both white-box and black-box attacks with other adaptive test-time defenses such as SODEF and Anti-adv. 
In Table~\ref{tab:benchmarkcomparison}, we present the result for APGD-CE, APGD-DLR, FAB, Square and AutoAttack for different SOTA methods. 
We observe that \emph{adding static RFI improves the performance across all the methods}. 
Importantly, RFI despite being non-adaptive improves the robust performance (at least marginally) for all the SOTA methods, even though each one of them achieves robustness by incorporating very different strategies like additional data, early stopping or informed prior. While this shows the strength and effectiveness of RFI, it also raises a fundamental question of whether it is necessary to adapt the model to individual test samples in order to improve the robustness in the adaptive test-time defense strategy. 
Evaluation of other SOTA models for CIFAR-10, CIFAR-100 and ImageNet showing similar observations are in Table~\ref{tab:benchmarkcomparison_app} in Appendix.
\begin{table*}[h]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI on state-of-the-art methods.} We evaluate APGD-CE, APGD-DLR, FAB, Square and AutoAttack under $\ell_{\infty}(\epsilon=8/255)$ on CIFAR-10 and CIFAR-100. RFI consistently {\color{applegreen}\textbf{improves}} the performance of the base model, whereas Anti-adv and SODEF results in slight {\color{brightmaroon}\textbf{decrease}} in the performance to AutoAttack. The inference time of RFI is $1\times$, whereas Anti-adv \citep{alfarra2022combating} and SODEF~\citep{kang2021stable} are $8\times$ and $2\times$, respectively. Additional results are in Table~\ref{tab:benchmarkcomparison_app} in Appendix. There is no standard deviation as the trained models are from RobustBench.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lclcccccc@{}}
         \toprule 
         %Dataset & 
         & Base Method & Defense & Clean & APGD-CE  & APGD-DLR  & FAB & Square & AutoAttack\\
         \midrule
         \multirow{5}{*}{\begin{sideways}CIFAR-10\end{sideways}}&
         \multirowcell{4}{~\citet{carmon2019unlabeled}\\WideResNet-28-10} & None & \textbf{89.69} &  61.82 & 60.85 & 60.18 & 66.51 & 59.53\\
          &
          &Anti-adv & \textbf{89.69} & 61.81 & 60.89 & 60.11 & 66.58 & \color{brightmaroon}\textbf{58.70}\\
          &
          &SODEF & 89.68 & 60.20 & 60.72 & 58.04 & 65.28 & \color{brightmaroon}\textbf{57.23}\\
          &
          &RFI ($K=10$) & 89.60 & 62.38 & 61.58 & 60.21 & 66.59 & 60.72\\
          &
          &RFI (opt. $K=20$) & 89.60 & \textbf{62.45} & \textbf{61.60}& \textbf{60.38}& \textbf{66.90} & \color{applegreen}\textbf{61.02}\\
         \cmidrule{2-9}
         %\midrule
         \multirow{5}{*}{\begin{sideways}CIFAR-100\end{sideways}}&
         \multirowcell{4}{~\citet{pang2022robustness}\\WideResNet-28-10} & None & \textbf{63.66} & 35.29 & 31.71& 31.32 & 35.70 & 31.08\\
          &
          &Anti-adv & 63.41 & 32.50 & 30.32 & 31.30 & 35.76 & \color{brightmaroon}\textbf{30.10}\\
          &
          &SODEF & 63.08 & 30.96 & 29.54 & 31.44 & 32.27 &\color{brightmaroon}\textbf{30.56}\\
          &
          &RFI ($K=100$)& 63.01 & 36.03 & \textbf{31.95}& 31.88 & 35.79 & 31.29\\
          &
          & RFI (opt. $K=115$) & 63.10 & \textbf{36.07} & \textbf{31.95} & \textbf{31.96} & \textbf{35.88} & \color{applegreen}\textbf{31.91}\\
         \bottomrule
    \end{tabular}
    }
    %\caption{(Base/Ours) Indicate the performance of adversarial training methods after adding our adaptive test time defense on APGD-CE and APGD-DLR attacks ~\cite{croce2020reliable}}
    \label{tab:benchmarkcomparison}
    \vspace{-0.2cm}
\end{table*}

We compare the algorithmic time complexity for different test-time defenses in  Tables~\ref{tab:benchmarkcomparison} and \ref{tab:benchmarkcomparison_app} and provide the average time to infer a single sample on a Nvidia DGX-A100 in the Table~\ref{tab:time_comp}. Note that the average time closely follows the time complexity. RFI does not add additional computation overhead. However,  Anti-adv and SODEF lead to $8\times$ and $2\times$ computation compared to the base model. 
\begin{table}[h]
    \centering
    \caption{\textbf{Time comparison} for RFI, Anti-adv, SODEF. RFI: $1\times$, Anti-adv: $8 \times$, SODEF: $2\times$.}
    \begin{tabular}{lcccc}
    \toprule 
    \multirow{2.5}{*}{Model} & \multicolumn{4}{c}{Time Comparison in (ms)}\\
    & Base & RFI  & Anti-adv & SODEF \\
    \midrule 
    PreActResnet-18 & 0.2760 &\textbf{0.2777} & 1.5127 & 0.5133 \\
    ResNet-50      & 0.3692 & \textbf{0.3703} & 2.7684 & 0.6877 \\
    WideResnet-28-10& 0.3780 & \textbf{0.3763} & 2.9735  & 0.7338 \\
    WideResnet-34-10& 0.8619 & \textbf{0.8654} & 6.8380  & 1.6599 \\
    \bottomrule
    \end{tabular}
    \label{tab:time_comp}
\end{table}
\subsection{Abalation Studies}
\label{ss:abalation}
\subsubsection{Effect of adversary strength} 
We study the effect of adversary strength on our method, RFI, by taking the adversarially trained ResNet-18 on CIFAR-10 using PGD ($\epsilon=8/255$ for $\ell_{\infty}$ and $ \epsilon=0.5$ for $\ell_2$) as the base model. % where 10 is the number of steps for gradient descent. 
Table~\ref{tab:strengthpgd} shows the evaluation of RFI with $\ell_{\infty}$ PGD attack for $\epsilon=\{2/255,4/255,12/255,16/255\}$ and $40$ iterations, and $\ell_2$ attack for $\epsilon=\{0.25,0.75,1\}$ and $100$ iterations. 
The results show that \emph{the underlying model augmented with RFI consistently improves over baseline across the perturbations of various strengths}, especially by over $1\%$ for adversary that is stronger than the base model ($\epsilon=\{12/255,16/255\}$ for $\ell_\infty$ and $\epsilon=\{0.75, 1.00\}$ for $\ell_2$.

\begin{table}[H]
    \centering
    \caption{\textbf{RFI consistently improves over baseline across the perturbations of various strengths.} Evaluation of RFI for $\ell_{\infty}$ and $\ell_2$ on ResNet-18 adversarially trained with CIFAR-10 and PGD.}
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule
\multirow{2.5}{*}{Method} & \multicolumn{4}{c}{$\ell_\infty$ attack} & \phantom{}& \multicolumn{3}{c}{$\ell_2 $ attack}\\
\cmidrule{2-5} \cmidrule{7-9}
& $\epsilon=\frac{2}{255}$ & $\epsilon=\frac{4}{255}$ & $\epsilon=\frac{12}{255}$ & $\epsilon=\frac{16}{255}$ && $\epsilon=0.25$ & $\epsilon=0.75$ & $\epsilon=1.00$\\ \midrule
PGD &  74.60 & 64.02 & 23.34 & 11.66 && 71.34 & 40.91 & 28.25\\
         PGD+RFI & \textbf{74.99} & \textbf{64.91} & \textbf{24.32} & \textbf{12.55} && \textbf{71.48} & \textbf{41.95} & \textbf{29.24}\\ 
         \bottomrule
    \end{tabular}}
    \label{tab:strengthpgd}
\end{table}
Further empirical analysis of the effect of step size in PGD attack is provided in Table~\ref{tab:step_size_pgd} in Appendix.

\subsubsection{Choice of $K$}
% \paragraph{Effect of parameter $K$.}
To study the effect of parameter $K$ in detail, we vary $K$ for the adversarial training methods on CIFAR-10 with ResNet-18 under $\ell_{\infty}(\epsilon=8/255)$ threat model, same setting as in Table~\ref{tab:basicexp} setting. 
Figure~\ref{fig:ablation_k} (left plot) shows
%In fig~\ref{fig:alltopk} we observe the effect of changing hyperparameter $K$ across different adversarial training methods on CIFAR10 dataset under $\ell_{\infty}(\epsilon=8/255)$ threat model. We observe 
that the adversarial training methods (PGD, IAT, C\&W and TRADES) behave similarly in their accuracy profile as compared to standard training even on Robust CIFAR-10 dataset. Moreover, the best performance is for $K=10$ for all the robust training methods. The corresponding eigenvalue spectrum exhibits a knee drop after top-$10$ eigenvalues  (right plot), which motivates \emph{our choice of $K$ as top-$10$ features for each class, equivalent to the number of classes.} As a complementary explanation for our choice of $K$, neural collapse phenomenon observes that the penultimate feature of each class collapses to its mean after the training error is almost zero \citep{papyan2020prevalence}. This implies that there is principally only $C$ number of feature vectors, one for each class, justifying our choice. 
Further ablation studies on the effect of parameter $K$  for SoTA models are in Figures~\ref{fig:ablation} and \ref{fig:ablation_sota} in Appendix.
While we set $K$ to be the number of classes, we also report the best performance of RFI by finding the optimal $K$ using grid search for SoTA models in Table~\ref{tab:benchmarkcomparison_app} in Appendix. Although $K=$ number of classes is not the optimum for the SoTA models, it is still better than SODEF and Anti-adv.
%Since top-10 useful features across classes are not the same we effectively select the first 15 eigenvectors after the union.

% Figure environment removed
\begin{comment}
\paragraph{Effect of adversary strength.} 
We study the effect of adversary strength on our method, RFI, by taking the adversarially trained ResNet-18 on CIFAR-10 using PGD ($\epsilon=8/255$ for $\ell_{\infty}$ and $ \epsilon=0.5$ for $\ell_2$) as the base model. % where 10 is the number of steps for gradient descent. 
%Table~\ref{tab:strengthpgd} shows the evaluation of RFI with $\ell_{\infty}$ PGD attack for $\epsilon=\{2/255,4/255,12/255,16/255\}$ and $40$ iterations, and $\ell_2$ attack for $\epsilon=\{0.25,0.75,1\}$ and $100$ iterations. 
The results in Tables~\ref{tab:strengthpgd_main} and \ref{tab:strengthpgd} in Appendix show that the underlying model augmented with RFI consistently improves over baseline across the perturbations of various strengths, especially by over $1\%$ for adversary that is stronger than the base model ($\epsilon=\{12/255,16/255\}$ for $\ell_\infty$ and $\epsilon=\{0.75, 1.00\}$ for $\ell_2$). 

\begin{table}[H]
    \centering
    \caption{\textbf{Effect of Adversary Strength.}}% Evaluation of RFI for $\ell_{\infty}$ and $\ell_2$ PGD attack. }
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{{@{}lccccc@{}}}
    \toprule
\multirow{2.5}{*}{Method} & \multicolumn{2}{c}{$\ell_\infty$} & \phantom{}& \multicolumn{2}{c}{$\ell_2 $}\\
\cmidrule{2-3} \cmidrule{5-6}
& $\epsilon=\frac{4}{255}$ & $\epsilon=\frac{16}{255}$ && $\epsilon=0.25$ &  $\epsilon=1.00$\\ \midrule
PGD &   64.02 & 11.66 && 71.34  & 28.25\\
         PGD+RFI & \textbf{64.91} & \textbf{12.55} && \textbf{71.48} &  \textbf{29.24}\\ 
         \bottomrule
    \end{tabular}}
    \label{tab:strengthpgd_main}
\end{table}
\end{comment}
\subsubsection{Comparison of RFI to similar conceptual methods}
\label{ss:rfi_last_intermediate}
The conceptual counterparts to RFI include performing the projection of intermediary layers to a low dimensional space instead of the last layer, or enforcing low dimensional last layer directly. 
%It is natural to wonder about the viability of the conceptual counterpart to RFI which is performing the eigen truncation of the intermediary layers instead of the last hidden layer. Another conceptual idea is to directly enforce low dimensional last layer instead of projecting to a low dimensional space as in the RFI. We evaluate both the ideas empirically and observe that RFI is more robust. 
In Table~\ref{tab:rfi_intermediary_main}, we evaluate effectiveness of performing RFI on intermediate layers by truncating the last but one hidden layer of ResNet-18 and evaluate the PGD trained model considered in Table~\ref{tab:basicexp}. This hidden layer has $512\times 4\times 4$ convolution which we project to $10\times 4\times 4$ using RFI procedure. 
\emph{While it is clear that performing RFI on the last layer as derived theoretically improves the robust performance, RFI on intermediary layers harm the robustness.}
We provide the result for enforcing low dimension last layer in appendix (Table~\ref{tab:low_dim}) which also demonstrates the superiority of RFI.
%Doing RFI on the last hidden layer shows the best performance. Empirical evaluation of intermediate layer truncation in Table~\ref{tab:rfi_intermediary_main} and enforcing low dimension last layer in appendix (Tables~\ref{tab:low_dim}) show the superiority of RFI.
\begin{table}[H]
    \centering
    \caption{\textbf{RFI on last layer outperforms  intermediate layer.} Evaluation of PGD trained ResNet-18 on CIFAR-10.}
    \vspace{-0.2cm}
    \resizebox{0.55\linewidth}{!}{
    \begin{tabular}{ccc}
    \toprule
         None & RFI on last layer & RFI on last but one layer  \\
         \midrule
          42.20 & \textbf{43.29} & 36.06\\
         \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
    \label{tab:rfi_intermediary_main}
\end{table}




%\textbf{Robust features are learned first.} 
%The theoretical results from Theorem~\ref{theorem:gradientdescentineigenbasis} conclude that the robust features are learned first for the random features model. 
%Using the theoretical results from our analysis and the feature definition of NTK by looking at ~\ref{eq:dynamicsntk} after substituting NTK features. 
%We compute the contribution of every feature to overall risk of the model and define the order of their risk contribution as the risk order. We observe that the risk order initially in the training follows $\lambda$ order in the spectrum and then reverses, which means initially larger risk comes from robust features and as they are learned the order changes. 
%Detailed discussion is provided in the appendix. 

