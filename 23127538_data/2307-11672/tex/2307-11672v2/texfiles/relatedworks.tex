\section{Related Works}
\label{sec:related_works}
In recent years, there has been a significant amount of research on generating adversarial examples and simultaneously improving the robustness of DNNs against such examples. We review the most relevant works below along with static and adaptive test-time defenses.

\textbf{Adversarial robustness.} 
\citet{szegedy2013} first observed that the adversarial examples, which are small imperceptible perturbations to the original data, can fool the DNN easily and make incorrect predictions.
%by showing that small perturbations to the input of a DNN can cause it to make a mistake, even when the perturbations are imperceptible to the human eye.
To generate adversarial examples, Fast Gradient Sign Method (FGSM) is proposed by \citet{goodfellow2014explaining}.
%for generating adversarial examples  with a finding that the `adversarial subspaces' exist in the input space where DNNs are particularly vulnerable to adversarial examples. 
\citet{madry2018towards} introduced an effective defense against adversarial examples known as adversarial training, where the network is trained by minimizing the maximum loss on the adversarially perturbed inputs. Adversarial training remains a promising defense to significantly improve the robustness of DNNs against adversarial attacks \citep{rice2020overfitting,carmon2019unlabeled,engstrom2019adversarial,wang2023better,pang2022robustness}. However, sophisticated attacks are developed to break the defenses such as Carlini-Wagner (C\&W) attack~\cite{carlini2017towards}, a method for generating adversarial examples;
%and is highly effective at fooling deep neural networks;
`obfuscated gradients' hypothesis \citep{athalye2018synthesizing} posits that the vulnerability to adversarial examples is due to the presence of easy to manipulate gradients in the model; `feature collision' hypothesis ~\cite{featuresnotbugs} postulates that the vulnerability is due to the presence of features in the data that are correlated with the labels, but loses the correlation when perturbed. As an advanced counter defense, methods to constrain 
%To counter such attacks, advanced defense strategy such as constraining 
the Lipschitzness of the model \citep{wang2019global} are developed. 

\textbf{Static test-time defenses.}
Static defenses change the model parameters or inputs after training without the knowledge of the test data and remains fixed during inference. A theoretically guaranteed approach to update the model parameters is through randomized smoothing  \citep{cohen2019certified, liu2018towards}. 
Another approach is to first detect the adversarial input and correct it using a trained classifier, and input the corrected sample to the base model for prediction. \citet{guo2018countering} suggest model agnostic image transformation such as total variance minimization and image quilting for the test data as an effective defense against any adversary. Other works detect the adversarial inputs using a separate trained network and corrects it either by removing the high frequency component in Fourier domain~\citep{nayak2022dad} or by a trained masked autoencoder~\citep{chao2023test}.


\textbf{Adaptive test-time defenses.} 
%Standard defenses against adversarial attacks such as adversarial training are static, that is, inputs and model parameters are fixed during inference. Whereas 
Adaptive defenses update model parameters and inputs at inference to defend against the attack. One strategy of adaptive test-time defenses is \emph{input purification}, in which the inputs to a model (usually pre-trained with a robustness objective) is optimized with a test-time objective. This test-time optimization can be hand crafted~\cite{alfarra2022combating,wu2021attacking} or learned~\cite{mao2021adversarial,hwang2023aid} with the help of an auxiliary network~\cite{nie2022diffusion}. Another strategy for building adaptive test-time defenses is \emph{model adaptation}, where model parameters are augmented with activations~\cite{chen2021towards}, implicit representations~\cite{kang2021stable, qian2021improving} and additional normalization layers~\cite{wang2021fighting}. 
%Adaptive test-time defenses also share similarities in their approaches to compute input purifications and model adaptations, often an iterative solver such as gradient descent is used to perform the computation of purified input or update parameters~\cite{qian2021improving,mao2021adversarial}. In addition to this, randomization by adding noise to input~\cite{wang2021fighting}, and additional data~\cite{mao2021adversarial} are also exploited at the test time, such that the output at inference time does not only depend on one single input. 
Although several methods are developed for adaptive test-time defenses, all of them increase the inference cost at least $2\times$~\citep{kang2021stable} and sometimes $500\times$~\citep{shionline} compared to the underlying model. 
More importantly, most of the existing adaptive test-time defenses results in a \emph{weaker adversary than the base model}, hence overestimated the robustness to adaptive attacks and are not really competitive with the static defenses as categorically shown in \citet{croce2022evaluating}.

%\textbf{Adversarial attacks using Neural Tangent Kernels (NTKs).}
%\citet{jacotntk, arora2019exact, gregyang2019scaling} show the equivalence of training a large width neural network by gradient descent to a deterministic kernel machine called Neural Tangent Kernel.
%In the context of adversarial attacks and robustness, \cite{tsilivis2022can} propose a method to generate adversarial examples using NTK and show transferability of the attack to the finite width neural network counterpart successfully.
%behavior and generalization properties of randomly initialized wide neural networks trained by gradient descent with infinitesimally small learning rate is equivalent to a deterministic kernel machine. %Furthermore, \citet{jacotntk} also shows that the change in the kernel during training decreases as the network width increases, and hence, asymptotically, one can represent an infinitely wide neural network by a deterministic NTK. 
%With this finding, a plethora of works used NTK to theoretically analyze the generalization properties of neural networks \citep{adlam2020neural, huang2020deep, cao2019generalization, wei2019regularization, ju2021generalization, ortiz2021can}, the convergence of gradient descent based optimization \citep{du2019gradient, nitanda2020optimal} and also different aspects of graph neural networks \citep{sabanayagam2022representation}.
%Besides the theoretical analysis, NTK based methods are proposed for neural network architecture search \citep{xu2021knas, chen2021neural}, dataset distillation \citep{nguyen2021dataset}, model selection \citep{deshpande2021linearized}, network pruning \citep{liu2020finding, yang2023neural} and adversarial example generation \citep{yuan2021neural}. 
%In the context of our work, \citet{tsilivis2022can} study adversarial attacks using NTK. 
%The authors propose a method to generate adversarial examples for infinitely-wide neural networks and show that they transfer to fool their associated (finite width) nets. 
%Additionally, the authors define NTK features using the   eigenspectrum of the NTK gram matrix and observe that the robust features correspond to the top of the eigenspectrum. 
%This supports previously conjectured hypotheses that prediction relies on robust and non-robust features and that robustness is traded for accuracy during standard training~\cite{tsipras2018robustness}. 
%We also define NTK features exactly as defined in \citet{tsilivis2022can} and 
%As a supplementary result of our analysis, we theoretically show that the robust NTK features are at the top of the spectrum and also give   
%study their dynamics giving 
%exact relationships in closed form for their convergence during training.


%In this work, we use the NTK to analyze the dynamics of adversarial vulnerability in neural networks by adopting the NTK feature map definition from \citet{tsilivis2022can}. Using this feature definition, we derive the correspondence of the network features to the NTK features and show that the top useful and robust features are learnt first during training. 

%study the robustness of the features learned by the network. Additionally, we use the NTK to study the relationship between the robustness of the features learned by the network and the generalization properties of the network.
