\section{Experimental Results}
\label{sec:exp}

\begin{comment}
\begin{table*}[t]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI.} (a) Adaptive attacks are evaluated. (b) and (c) are study of transfer attacks where the adversarial examples are generated from the base method+RFI in order to evaluate the base method, and vice versa. We consider $\ell_\infty$ and $\ell_2$ PGD attack on CIFAR-10 with Resnet-18. $\ell_\infty$ attack with step size $\epsilon/4$ and $40$ iterations. $\ell_2$ attack with size $\epsilon/5$ and $100$ iterations. Results for CIFAR-100 and tiny ImageNet are provided in Appendix (Tables~\ref{tab:cifar100table} and \ref{tab:tinyImagenet}).}
    \begin{subtable}[]{0.54\textwidth}
    \caption{{Adaptive Attack evaluation} }
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} &
\multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Method & +RFI && Method & +RFI &&  Method & +RFI\\ \midrule
%\multirow{5}{*}{CIFAR-10}&
Standard & \textbf{95.28} & 88.53 && 1.02  & \textbf{4.35}  && 0.39 & \textbf{9.73}\\
Robust CIFAR-10 & 78.69 & \textbf{78.75} && 1.30 & \textbf{7.01} && 9.63 & \textbf{11.00}\\
         PGD & \textbf{83.53} & 83.22 && 42.20  & \textbf{43.29} && 54.61 & \textbf{55.03}\\
         IAT & \textbf{91.86} & 91.26 && 44.76  & \textbf{46.95}  && 62.53 & \textbf{64.31}\\
         C\&W attack & \textbf{85.11} & 84.97 && 40.01 & \textbf{42.56} && 55.02 & \textbf{56.79}\\
         TRADES &  \textbf{81.13} & 80.76 && 51.98 &\textbf{53.45} && 60.03  & \textbf{61.39}\\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp}
    \end{subtable}
    \begin{subtable}[]{0.23\textwidth}
        \caption{{Adversary from Method+RFI}}
    \resizebox{0.96\linewidth}{!}{
    \begin{tabular}{{@{}ccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} & \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& 
%\multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
%\phantom{} & 
%\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
%\cmidrule{1-2} \cmidrule{4-5}
\multicolumn{1}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{1}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{1-1} \cmidrule{3-3}
%&
 % Method & +RFI &&  Method & +RFI\\ 
 Method && Method \\
 \midrule
%\multirow{5}{*}{CIFAR-10}&
 \color{brightmaroon}0.00  && \color{brightmaroon}0.01 \\
  \color{brightmaroon}0.03  && \color{brightmaroon}1.05 \\
          \color{brightmaroon}34.80   && \color{brightmaroon}49.90 \\
          \color{brightmaroon}35.78    && \color{brightmaroon}55.42 \\
          \color{brightmaroon}\textbf{3.92}  && \color{brightmaroon}\textbf{13.50} \\
          \color{brightmaroon}{51.70}  && \color{brightmaroon}{58.09}  \\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp_transferability_1}
    \end{subtable}
     \begin{subtable}[]{0.21\textwidth}
        \caption{{Adversary from Method}}
    \resizebox{1.04\linewidth}{!}{
    \begin{tabular}{{@{}ccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} & \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& 
%\multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
%\phantom{} & 
%\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
%\cmidrule{1-2} \cmidrule{4-5}
\multicolumn{1}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{1}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{1-1} \cmidrule{3-3}
%&
 % Method & +RFI &&  Method & +RFI\\ 
 +RFI && +RFI \\
 \midrule
%\multirow{5}{*}{CIFAR-10}&
 \color{applegreen}10.36  && \color{applegreen}12.09 \\
 \color{applegreen}15.41  && \color{applegreen}17.38 \\
          \color{applegreen}46.02   && \color{applegreen}58.81 \\
          \color{applegreen}49.06    && \color{applegreen}66.67 \\
          \color{applegreen}45.48  && \color{applegreen}58.95 \\
          \color{applegreen}54.33  && \color{applegreen}65.23  \\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp_transferability_2}
    \end{subtable}
    \label{tab:basic_cifar10}
\end{table*}
\end{comment}

We conduct extensive experiments to validate the performance of RFI to different adversarial attacks %on CIFAR-10, CIFAR-100 \citep{cifar10}, tiny ImageNet and ImageNet 
by integrating RFI to several DNNs and different training procedures. We also propose an approach to make RFI a dynamic adaptive test-time defense and compare it to other test-time defenses. %We first introduce the different evaluation methods for adversarial robustness that we use, network architectures, and other test-time defenses.
%For our experimental analysis, we use the CIFAR-10 and CIFAR 100~\cite{cifar10} datasets.

\paragraph{Datasets \& Compute.} We evaluate RFI on CIFAR-10, CIFAR-100 \citep{cifar10}, Robust CIFAR-10 \citep{featuresnotbugs}, tiny ImageNet \citep{le2015tiny} and ImageNet \citep{russakovsky2015imagenet} datasets. We use Pytorch~\cite{paszke2019pytorch} for all our experiments \& a single Nvidia DGX A100 to run all of our experiments.

\begin{comment}
% Figure environment removed
\end{comment}

\paragraph{Adversarial Attacks.} We evaluate RFI on different white and black-box adversarial attacks namely, \emph{Projected Gradient Descent (PGD)}~\cite{madry2018towards}, a white-box attack that perturbs the input within a small $\ell_p$ radius $\epsilon$, so that it maximizes the loss of a model. We perform both $\ell_{\infty}$ and $\ell_2$ PGD attack with standard perturbation $\epsilon$, attack step size and iteration for each dataset. %$\epsilon=8/255$ and $\epsilon=0.5$, respectively, with the attack step size $\epsilon/4$ and $40$ iterations for $\ell_{\infty}$ and $\epsilon/5$ and $100$ iterations for $\ell_2$ to have an overall high strength PGD attack. 
%(b) Parameter-free attacks such as Auto PGD-Cross Entropy (APGD-CE) and Auto PGD-Difference Logit Ratios (APGD-DLR) \citep{croce2020reliable} under $\ell_{\infty}$ threat model, that are extensions of PGD attack with no step size parameter and stronger than PGD.
%We then obtain robust accuracy for standard and adversarially trained models before and after integrating RFI. 
%In table~\ref{tab:benchmarkcomparison} we evaluate the effectiveness of RFI on APGD-CE, APGD-DLR~\cite{croce2020reliable}, and RobustBench~\cite{croce2020robustbench} under $\ell_{\infty}(\epsilon=8/255)$ threat model. APGD-CE and APGD-DLR are parameter-free attack extensions of PGD attack which are based on cross entropy (CE) and difference of logit ratios (DLR). 
\emph{AutoAttack} \citep{croce2020robustbench}, a suite of white-box and black-box attacks including Auto PGD-Cross Entropy (APGD-CE), Auto PGD-Difference Logit Ratios (APGD-DLR) \citep{croce2020reliable}, Fast Adaptive Boundary Attack (FAB) \citep{croce2020minimally}, and Square Attacks \citep{andriushchenko2020square}. 
\emph{APGD-CE and APGD-DLR} are parameter-free white-box attacks that are extensions of PGD attack with no step size parameter and stronger than PGD.
\emph{FAB} is a white-box attack that minimizes the norm of the adversarial perturbation.
\emph{Square Attack} is an efficient black-box attack that is score based and uses random search without gradient approximations.
While \emph{adaptive attacks} generate the adversarial images using the target model, \emph{transfer attacks} generate adversarial images using a surrogate model and attack the target model. 
%RobustBench overcomes the limitations of auto-attack by flagging potential overestimation of robustness.

\paragraph{Benchmarking on SOTA defenses.}
We evaluate RFI on various architectures trained differently: \emph{ResNet-18} and \emph{ResNet-50} with standard training and the popular adversarial training methods such as \emph{PGD}~\cite{madry2018towards}, \emph{Interpolated Adversarial Training (IAT)}~\cite{lamb2019interpolated}, \emph{Carlini-Wagner (C\&W)} loss~\cite{carlini2017towards} and \emph{TRADES}~\citep{zhang2019theoretically}. %also use Robust CIFAR-10 dataset created by \citet{featuresnotbugs}.
%We observe that the addition of RFI at inference time does improve the performance across all the methods. 
%For our table~\ref{tab:benchmarkcomparison} 
 We select different \emph{state-of-the-art adversarially trained  models} from RobustBench upon which at the test time we integrate RFI ~\cite{carmon2019unlabeled,engstrom2019adversarial,rice2020overfitting,wang2023better,pang2022robustness}.
These methods either use additional data ~\citep{carmon2019unlabeled, wang2023better}, informed adversarial prior~\citep{engstrom2019adversarial} or early stopping ~\citep{rice2020overfitting} to improve the robustness of models.
%\citet{carmon2019unlabeled} show that additional $500,000$ unlabeled data improve robustness on CIFAR-10. \citet{engstrom2019adversarial} argue that the limitation of a network to represent input data in high-level features causes two semantically different images to have similar representation and propose training with an adversarial prior improves robustness. \citet{rice2020overfitting} suggest early stopping-based training to overcome this robust overfitting. \citet{wang2023better} propose use of class-conditioned diffusion models to generate additional labeled data for adversarial training and observe state-of-the-art performance using additional $50$ million synthetic images in training.
%\citet{pang2022robustness} .. 
We detail each training method in appendix. 


\paragraph{Evaluation measures.}
We measure the performance of models with and without RFI by the accuracy of prediction to both clean/original non-adversarial samples and adversarial samples. We use `Clean' to denote accuracy of models to original samples.
Note that there will be no variance in our measure since we are using a pretrained model, so the parameters are fixed, and hence there is no stochasticity.
We further remark that our defense strategy does not circumvent gradient based attacks due to gradient masking \citep{athalye2018synthesizing} since we simply project the last layer feature in the eigenspace of its covariance, hence the network is still differentiable with active gradients.

\paragraph{Comparison to adaptive test-time defenses.}
%Recently, \citet{croce2022evaluating} provide an overview of $9$ adaptive test-time defenses of which only four methods actually improve robustness and are competitive with the current state-of-the-art performances. 
We compare RFI with two adaptive test-time defenses: \emph{SODEF}~\cite{kang2021stable} and \emph{Anti-Adv}~\cite{alfarra2022combating}. The choice of SODEF and Anti-Adv is due to their relatively faster inference costs $2\times$ and $8\times$, respectively, and that they are representative of model adaptation and input modification strategies for adaptive test-time defenses, respectively.

\begin{comment}
\begin{table*}[]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI.} $\ell_\infty$ and $\ell_2$ PGD attack on CIFAR-10 with Resnet-18. $\ell_\infty$ attack with step size $\epsilon/4$ and $40$ iterations. $\ell_2$ attack with size $\epsilon/5$ and $100$ iterations.}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} &
\multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Method & +RFI && Method & +RFI &&  Method & +RFI\\ \midrule
%\multirow{5}{*}{CIFAR-10}&
Standard & \textbf{95.28} & 88.53 && 1.02  & \textbf{4.35}  && 0.39 & \textbf{9.73}\\
         PGD~\cite{madry2018towards} & \textbf{83.53} & 83.22 && 42.20  & \textbf{43.29} && 54.61 & \textbf{55.03}\\
         IAT~\cite{lamb2019interpolated} & \textbf{91.86} & 91.26 && 44.76  & \textbf{46.95}  && 62.53 & \textbf{64.31}\\
         Robust CIFAR-10~\cite{featuresnotbugs} & 78.69 & \textbf{78.75} && 1.30 & \textbf{7.01} && 9.63 & \textbf{11.00}\\
         C\&W attack~\cite{carlini2017towards} & \textbf{85.11} & 84.97 && 40.01 & \textbf{42.56} && 55.02 & \textbf{56.79}\\
         TRADES &  \textbf{81.13} & 80.76 && 51.70 &\textbf{53.45} && 60.03  & \textbf{61.39}\\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp}
\end{table*}
\end{comment}

%\subsection{Results and Discussion}
\subsection{RFI improves robustness across attacks}
\paragraph{Adaptive attack evaluation.} 
We evaluate RFI for adaptive attacks by generating adversarial samples to specifically target our defense~\citep{tramer2020adaptive}. 
We obtain clean and robust accuracy for standard and adversarially trained models before and after integrating RFI and setting $K$ to the number of classes. %, that is, $K=10$ and $K=100$ for CIFAR-10 and CIFAR-100, respectively. 
The results for different training procedures on CIFAR-10 with ResNet-18 are presented in Table~\ref{tab:basicexp} (and CIFAR-100 with ResNet-18 and tiny ImageNet with ResNet-50 in Tables~\ref{tab:cifar100table} and \ref{tab:tinyImagenet}, respectively, in Appendix). Robust CIFAR-10 denotes standard training using Robust CIFAR-10 dataset.
% We allow all attacks in Table~\ref{tab:basicexp} and ~\ref{tab:benchmarkcomparison} to adapt to our defense therefore making them adaptive~\citep{tramer2020adaptive}.
We observe that \emph{our method consistently improves the robust performance of adversarially trained models, on an average by $2\%$}. 
There is a minor drop in the clean performance as we choose only subset of the informative features that are also robust.
Nevertheless, the gain in robust performance is with almost no computational overhead. % and maximum by $6\%$ for $\ell_\infty$ attack on Robust CIFAR-10 training.

\paragraph{Transfer attack evaluation.} We extend the evaluation in Table~\ref{tab:basicexp} to transfer attacks, that is, we evaluate the base method by generating adversarial samples with the base method integrated with RFI, and similarly evaluate RFI for adversarial samples from the base method. We present the results in Tables~\ref{tab:basicexp_transferability_1} and \ref{tab:basicexp_transferability_2}, respectively. 
As expected the adversarial samples from the base method + RFI are more powerful and reduce the robustness of the base method to a greater extent (Table~\ref{tab:basicexp_transferability_1}). Comparing `Method' columns in Tables~\ref{tab:basic_cifar10} and \ref{tab:basicexp_transferability_1}, \emph{we note that while the loss in robust performance is comparatively smaller for TRADES which is in line with observation in \citet{croce2022evaluating},  C\&W loses its robustness completely suggesting the brittleness of C\&W defense.} On the other hand, the adversarial samples from the base method are much weaker and RFI is very robust, achieving the best performance (Table~\ref{tab:basicexp_transferability_2}). The improvement in robustness compared to adaptive attack is between $2-5\%$ for the different training defenses. 
Table~\ref{tab:transfer_attack_def_comparison} in Appendix reflects similar observations in further transfer attack results for a SOTA model~\citep{rice2020overfitting} on CIFAR-100.

\subsection{Static defense is better than adaptive test-time defense using RFI}
\textbf{Static vs Dynamic/Adaptive RFI.}
The principle of RFI can be effectively used to adapt the model at test-time to every input by computing the transformation matrix $\Tilde{\bm{U}}$ using the covariance of \emph{test set} features $\Sigma_{test}$ and choosing the eigenvectors as per the robust feature score $s(\bm{u})$ of $\Sigma_{test}$. The results for this adaptive strategy is evaluated for the robust training settings of Table~\ref{tab:basic_cifar10}, and reported in Table~\ref{tab:rfi_static_adaptive}. We consider transfer attack using the base model for fair comparison.
We observe  that \emph{static RFI is better than dynamic RFI}. This reinforces the theoretical result that the eigendirections of the training set feature covariance determines the most robust features (Corollary~\ref{cor:robust_score}).
Moreover, adaptive attacks in dynamic RFI needs further information on when to adapt since the model should be static until the attacker creates an adversarial sample, and the adaptive transformation should be done only in the case of defender. We detail the challenges in deploying dynamic RFI when the use case is unknown, and provide the results for adaptive attacks in appendix (Tables~\ref{tab:rifar_full_val} and \ref{tab:rifar_full_val_single}).
%
\begin{table}[t]
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Static & Dynamic && Static & Dynamic &&  Static & Dynamic\\ \midrule
      %  Standard &  \textbf{10.36} &  &&20.08 &\textbf{20.91}\\
       %  Robust CIFAR-10 & \textbf{58.94} & 58.88 && 22.56 & \textbf{23.58}\\
         PGD & 83.22 & 82.86 && 46.02 & \textbf{46.83}  &&58.81 & \textbf{59.23}\\
         IAT & 91.26 & \textbf{91.35} && \textbf{49.06} & 48.53 && \textbf{66.67} & 66.28\\
         C\&W & 84.97 & 83.01 && \textbf{45.48} & 43.98 &&     
         \textbf{58.95} & 57.82 \\
         TRADES& 80.76 & 78.98 && \textbf{54.33} & 
         53.58 && \textbf{65.23} & 65.00 \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{Comparison of static and dynamic/adaptive RFI.} Setting same as Table~\ref{tab:basic_cifar10}. Adversarial examples are generated from the base model for fair comparison.}
    \vspace{-0.2cm}
    \label{tab:rfi_static_adaptive}
\end{table}

\paragraph{Adaptive test-time defense evaluation.}
As a result of the above static vs dynamic RFI evaluation, we compare the effectiveness of static RFI on both white-box and black-box attacks with other adaptive test-time defenses such as SODEF and Anti-adv. 
In Table~\ref{tab:benchmarkcomparison}, we present the result for APGD-CE, APGD-DLR, FAB, Square and AutoAttack for different SOTA methods. 
We observe that \emph{adding static RFI improves the performance across all the methods}. 
Importantly, RFI despite being non-adaptive improves the robust performance (atleast marginally) for all the SOTA methods, even though each one of them achieves robustness by incorporating very different strategies like additional data, early stopping or informed prior. While this shows the strength and effectiveness of our method, it also raises a fundamental question of whether it is necessary to adapt the model to individual test samples in order to improve the robustness in the adaptive test-time defense strategy. 
We provide evaluation of other SOTA models for CIFAR-10, CIFAR-100 and ImageNet showing similar observation in Table~\ref{tab:benchmarkcomparison_app} in Appendix.
\begin{table*}[h]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI on state-of-the-art methods.} We evaluate APGD-CE, APGD-DLR, FAB, Square and AutoAttack under $\ell_{\infty}(\epsilon=8/255)$ on CIFAR-10 and CIFAR-100. The inference time of RFI is $1\times$, whereas Anti-adv \citep{alfarra2022combating} and SODEF~\citep{kang2021stable} are $8\times$ and $2\times$, respectively. Additional results in Table~\ref{tab:benchmarkcomparison_app} in Appendix.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lclcccccc@{}}
         \toprule 
         %Dataset & 
         & Base Method & Defense & Clean & APGD-CE  & APGD-DLR  & FAB & Square & AutoAttack\\
         \midrule
         \multirow{5}{*}{\begin{sideways}CIFAR-10\end{sideways}}&
         \multirowcell{4}{~\citet{carmon2019unlabeled}\\WideResNet-28-10} & None & \textbf{89.69} &  61.82 & 60.85 & 60.18 & 66.51 & 59.53\\
          &
          &Anti-adv & \textbf{89.69} & 61.81 & 60.89 & 60.11 & 66.58 & 58.70\\
          &
          &SODEF & 89.68 & 60.20 & 60.72 & 58.04 & 65.28 & 57.23\\
          &
          &RFI ($K=10$) & 89.60 & 62.38 & 61.58 & 60.21 & 66.59 & 60.72\\
          &
          &RFI (opt. $K=20$) & 89.60 & \textbf{62.45} & \textbf{61.60}& \textbf{60.38}& \textbf{66.90} & \color{applegreen}\textbf{61.02}\\
         \cmidrule{2-9}
         %\midrule
         \multirow{5}{*}{\begin{sideways}CIFAR-100\end{sideways}}&
         \multirowcell{4}{~\citet{pang2022robustness}\\WideResNet-28-10} & None & \textbf{63.66} & 35.29 & 31.71& 31.32 & 35.70 & 31.08\\
          &
          &Anti-adv & 63.41 & 32.50 & 30.32 & 31.30 & 35.76 & 30.10\\
          &
          &SODEF & 63.08 & 30.96 & 29.54 & 31.44 & 32.27 &30.56\\
          &
          &RFI ($K=100$)& 63.01 & 36.03 & \textbf{31.95}& 31.88 & 35.79 & 31.29\\
          &
          & RFI (opt. $K=115$) & 63.10 & \textbf{36.07} & \textbf{31.95} & \textbf{31.96} & \textbf{35.88} & \color{applegreen}\textbf{31.91}\\
         \bottomrule
    \end{tabular}
    }
    %\caption{(Base/Ours) Indicate the performance of adversarial training methods after adding our adaptive test time defense on APGD-CE and APGD-DLR attacks ~\cite{croce2020reliable}}
    \label{tab:benchmarkcomparison}
\end{table*}

\vspace{-0.2cm}
\subsection{Ablation: $K=$ number of classes is the best choice}
% \paragraph{Effect of parameter $K$.} 
While we set $K$ to be the number of classes, we also report the best performance of RFI by finding the optimal $K$ using grid search in Table~\ref{tab:benchmarkcomparison}. Note that although $K=$ number of classes is not the optimum for the SOTA models, it is still better than SODEF and Anti-adv.
To study the effect of the parameter in detail, we vary $K$ for the adversarial training methods on CIFAR-10 with ResNet-18 under $\ell_{\infty}(\epsilon=8/255)$ threat model (Table~\ref{tab:basic_cifar10} setting). 
Figure~\ref{fig:ablation_k} (left plot) shows
%In fig~\ref{fig:alltopk} we observe the effect of changing hyperparameter $K$ across different adversarial training methods on CIFAR10 dataset under $\ell_{\infty}(\epsilon=8/255)$ threat model. We observe 
that the adversarial training methods (PGD, IAT, C\&W and TRADES) behave similarly in their accuracy profile as compared to standard training even on Robust CIFAR-10 dataset. Moreover, the best performance is for $K=10$ for all the robust training methods. The corresponding eigenvalue spectrum exhibits a knee drop after top-$10$ eigenvalues  (right plot), which motivates our choice of $K$ as top-$10$ features for each class, equivalent to the number of classes. As a complementary explanation for our choice of $K$, neural collapse phenomenon observes that the penultimate feature of each class collapses to its mean after the training error is almost zero \citep{papyan2020prevalence}. This implies that there is principally only $C$ number of feature vectors, one for each class, justifying our choice. 
Further ablation studies on the effect of parameter $K$  for SOTA models are in Figures~\ref{fig:ablation} and \ref{fig:ablation_sota}.
%Since top-10 useful features across classes are not the same we effectively select the first 15 eigenvectors after the union.

% Figure environment removed

\begin{comment}
\paragraph{Effect of adversary strength.} 
We study the effect of adversary strength on our method, RFI, by taking the adversarially trained ResNet-18 on CIFAR-10 using PGD ($\epsilon=8/255$ for $\ell_{\infty}$ and $ \epsilon=0.5$ for $\ell_2$) as the base model. % where 10 is the number of steps for gradient descent. 
%Table~\ref{tab:strengthpgd} shows the evaluation of RFI with $\ell_{\infty}$ PGD attack for $\epsilon=\{2/255,4/255,12/255,16/255\}$ and $40$ iterations, and $\ell_2$ attack for $\epsilon=\{0.25,0.75,1\}$ and $100$ iterations. 
The results in Tables~\ref{tab:strengthpgd_main} and \ref{tab:strengthpgd} in Appendix show that the underlying model augmented with RFI consistently improves over baseline across the perturbations of various strengths, especially by over $1\%$ for adversary that is stronger than the base model ($\epsilon=\{12/255,16/255\}$ for $\ell_\infty$ and $\epsilon=\{0.75, 1.00\}$ for $\ell_2$). 

\begin{table}[H]
    \centering
    \caption{\textbf{Effect of Adversary Strength.}}% Evaluation of RFI for $\ell_{\infty}$ and $\ell_2$ PGD attack. }
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{{@{}lccccc@{}}}
    \toprule
\multirow{2.5}{*}{Method} & \multicolumn{2}{c}{$\ell_\infty$} & \phantom{}& \multicolumn{2}{c}{$\ell_2 $}\\
\cmidrule{2-3} \cmidrule{5-6}
& $\epsilon=\frac{4}{255}$ & $\epsilon=\frac{16}{255}$ && $\epsilon=0.25$ &  $\epsilon=1.00$\\ \midrule
PGD &   64.02 & 11.66 && 71.34  & 28.25\\
         PGD+RFI & \textbf{64.91} & \textbf{12.55} && \textbf{71.48} &  \textbf{29.24}\\ 
         \bottomrule
    \end{tabular}}
    \label{tab:strengthpgd_main}
\end{table}
\end{comment}

Further empirical analysis of the effect of adversary strength and step size in PGD attack, and the exact run time comparison of RFI, SODEF and Anti-adv are in appendix (Tables~\ref{tab:strengthpgd}, \ref{tab:step_size_pgd} and \ref{tab:time_comp}).

\paragraph{Comparing RFI to similar conceptual algorithms.}
The conceptual counterparts to RFI include performing the projection of intermediary layers to a low dimensional space instead of the last layer, or enforcing low dimensional last layer directly. 
%It is natural to wonder about the viability of the conceptual counterpart to RFI which is performing the eigen truncation of the intermediary layers instead of the last hidden layer. Another conceptual idea is to directly enforce low dimensional last layer instead of projecting to a low dimensional space as in the RFI. We evaluate both the ideas empirically and observe that RFI is more robust. 
In Table~\ref{tab:rfi_intermediary_main}, we evaluate effectiveness of performing RFI on intermediate layers by truncating the last but one hidden layer of ResNet-18 and evaluate the PGD trained model considered in Table~\ref{tab:basic_cifar10}. This hidden layer has $512\times 4\times 4$ convolution which we project to $10\times 4\times 4$ using RFI procedure. 
While it is clear that performing RFI on the last layer as per the theory improves the robust performance, RFI on intermediary layers harm the robustness.
We provide the result for enforcing low dimension last layer in appendix (Tables~\ref{tab:low_dim}) showing the superiority of RFI.
%Doing RFI on the last hidden layer shows the best performance. Empirical evaluation of intermediate layer truncation in Table~\ref{tab:rfi_intermediary_main} and enforcing low dimension last layer in appendix (Tables~\ref{tab:low_dim}) show the superiority of RFI.
\begin{table}[H]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{ccc}
    \toprule
         None & RFI on last layer & RFI on last but one layer  \\
         \midrule
          42.20 & \textbf{43.29} & 36.06\\
         \bottomrule
    \end{tabular}}
    \caption{\textbf{RFI on last layer vs intermediate layers.} Evaluation of ResNet-18 on CIFAR-10.}
    \label{tab:rfi_intermediary_main}
\end{table}




%\textbf{Robust features are learned first.} 
%The theoretical results from Theorem~\ref{theorem:gradientdescentineigenbasis} conclude that the robust features are learned first for the random features model. 
%Using the theoretical results from our analysis and the feature definition of NTK by looking at ~\ref{eq:dynamicsntk} after substituting NTK features. 
%We compute the contribution of every feature to overall risk of the model and define the order of their risk contribution as the risk order. We observe that the risk order initially in the training follows $\lambda$ order in the spectrum and then reverses, which means initially larger risk comes from robust features and as they are learned the order changes. 
%Detailed discussion is provided in the appendix. 

