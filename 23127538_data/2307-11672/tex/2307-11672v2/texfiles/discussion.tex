\section{Discussion}
The simplicity and effectiveness of RFI at test-time is impressive as the robustness gain is achieved with zero additional computation overhead for inference. 
While RFI on smaller models like ResNet demonstrate more improvement in robustness than larger SoTA models like WideResNet, it is important to note that these SoTA models are already optimized to their full potential, hence even a small improvement is significant in these cases.
Furthermore, RFI is well-founded theoretically.
Consequently, the idea of RFI can also be used to develop a robust training procedure by incorporating the projection onto the robust feature space during training. 
We leave the experimental analysis for future study as the current work focuses on test-time defenses. However,
%While we analysed the robustness of the learned model in the previous section, 
it is intriguing to theoretically analyze the robustness of features during training to understand the RFI's potential as an idea and the cause of vulnerability to adversarial examples. % and a potential to design better training algorithms.
Therefore, we derive the learning dynamics of full batch gradient descent on population squared error loss of GAM (stated informally in Proposition~\ref{prop:gd_top_features} and proved in Appendix~\ref{pf:dynamics_gam}). 

\begin{proposition}[Learning dynamics of GAM]
\label{prop:gd_top_features}
    Given $h(\bm{x})= \bm{\beta}^\top \phi(\bm{x})$ and $\Sigma = \mathbb{E}_\mathbf{x} \left[\phi(\mathbf{x})\phi(\mathbf{x})^\top\right]$. Let $(\lambda_i, \bm{u}_i)$ be the eigenpair of $\Sigma$. Then full batch gradient descent learns features in the direction of $\bm{u}_i$ with large eigenvalues $\lambda_i$ first during the training and those directions are robust only if they align with the original signal direction $\bm\beta$. 
\end{proposition}

This result further strengthens the idea and suggests that truncating the non-robust directions during training, which is one of the plausible causes for the existence of adversaries, could improve the robustness of the model.

\textbf{Connection to Neural Tangent Kernel (NTK) features.}
One of the related results to Proposition~\ref{prop:gd_top_features} is using the NTK features. \citet{tsilivis2022can} defined features using NTK gram matrix and empirically observed that the features corresponding to the top spectrum of NTK are more robust and learned first during training. Our theoretical framework enables us to establish the equivalence of NTK features to the robust feature definition and more importantly prove that the robust NTK features indeed correspond to the top of the spectrum. The NTK gram matrix $\mathbf{\Theta} \in \mathbb{R}^{n\times n}$ is between all pairs of datapoints. NTK features of input $\bm{x}$ is defined using the eigendecomposition of $\mathbf{\Theta} = \sum_{i=1}^{n}\lambda_i \mathbf{v_i}\mathbf{v_i}^{T}$ as $f^{ker}_{i}(\bm{x}) := g(\lambda_i,\bm{v}_i, \bm{x})$ for a specific function $g$.
We state the result in the following proposition and prove along with empirical verification in Appendix~\ref{sec:ntk} (Figure~\ref{fig:ntk}).

\begin{proposition}[NTK feature robustness lies at the top]
\label{prop:robust_top}
    Let feature $f^{ker}_{i}$ be Lipschitz continuous in gradient of NTK with respect to $\mathbf{x}$ and an adversarial perturbation $\bm{\delta}$ such that $||\bm{\delta}||_p \leq \Delta$. Then,
    %of max $\Delta$ radius around input $\mathbf{x}$ i.e. $\delta\in \mathcal{B}(\Delta)(\mathbf{x})$ under $\ell_p$-norm. We can say that
    $||f^{ker}_{i}(\mathbf{x}+\bm\delta)- f^{ker}_{i}(\mathbf{x})||_2\leq\Theta(\frac{1}{\lambda_i})$.
\end{proposition}

Although we prove that the robust NTK features correspond to the top of the spectrum, we leave the challenge to establish its connection to the DNN for future analysis.
%Overall, our work establishes that robust training methods can be leveraged even at test-time. 
%Our analysis also gives scope to improve the robust training procedures by incorporating our idea of projection to the most robust feature space during training. 
Overall, our work develops a guaranteed algorithm to improve adversarial robustness at test-time along with possibilities to improve the robust training procedures.
%Overall, our explorations contribute to the ongoing efforts to develop robust models that can resist adversarial examples and improve the security and reliability of DNNs.
Additionally, we note that while the current work focused on evaluating SOTA convolution-based models, the effectiveness of RFI on transformer-based models is an interesting future direction. 
