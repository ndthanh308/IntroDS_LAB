\section{Proofs of the Main Results}

In this section, we prove Theorem \ref{thm:compute-robustness}, and related results, Corollaries \ref{cor:robust_score}--\ref{cor:information} and Remark \ref{rem:thm-robust-linear}.

\subsection{Proof of Theorem \ref{thm:compute-robustness}}
\label{pf:sf_lower_bound}
\begin{proof}
%\textbf{To prove Theorem \ref{thm:compute-robustness}} and subsequent results, 
Recall that we assume 
$y = h(\mathbf{x}) + \bm{\epsilon} = \bm{\beta}^\top \phi(\mathbf{x}) + \bm{\epsilon}$, where $\bm{\epsilon} \in \mathbb{R}^C$ has independent coordinates, each satisfying $\mathbb{E}[\epsilon_c] = 0$, $\mathbb{E}[\epsilon_c^2] \leq \sigma^2$ for all $c \in \{1,\ldots,C\}$.
The features for which we wish to compute robustness are of the form $f = \bm{M}\phi$ where $\bm{M}$ is a linear map.

We are interested in robustness with respect to the $c$-th component, which is computed as
\begin{align}
s_{\mathcal{D},\bm{\beta},c}(f) &= \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top  f(\tilde{\mathbf{x}})\right] 
\nonumber\\
&= \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[y_c \bm{\beta}_c^\top  f(\mathbf{x})\right] 
+\mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)\right]  
\label{eq:robust-decomp}
\end{align}

%
We compute the first term exactly as
\begin{align*}
   \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[y_c \bm{\beta}_c^\top  f(\mathbf{x})\right] 
   &= \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ (\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c) \bm{\beta}_c^\top  f(\mathbf{x})\right] 
   & (\text{since } \mathbb{E}[\epsilon_c] = 0),
   \\&= \mathbb{E}_{\mathbf{x}}\left[ \bm{\beta}_c^\top \phi(\mathbf{x}) \phi(\mathbf{x})^\top \bm{M} \bm{\beta}_c\right] 
   \\&= \bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c,
   &(\Sigma = \mathbb{E}_\mathbf{x} \left[\phi(\mathbf{x})\phi(\mathbf{x})^\top\right]).
\end{align*}
%
For the second term in \eqref{eq:robust-decomp}, we aim to derive a lower bound. Observe that
\begin{align*}
    y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)
    &= y_c \bm{\beta}_c^\top \bm{M} \big( \phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\big)
    \\&\geq - |y_c|\cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot \Vert \bm{M}\big(\phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\big)\Vert_\mathcal{H}
    \\&\geq - |y_c|\cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot \Vert\bm{M} \Vert_{op} \cdot \Vert \phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\Vert_\mathcal{H}.
\end{align*}
Using $L$-Lipschitzness of $\phi$, we have $\Vert \phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\Vert_\mathcal{H} \leq L||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq L\Delta$. Hence, the second term in \eqref{eq:robust-decomp} can be bounded from below as
\begin{align*}
    \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)\right] &= - \Vert\bm{M} \Vert_{op} \cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot L\Delta \cdot \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ |y_c|\right]\\
    &\geq - \Vert\bm{M} \Vert_{op} \cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot L\Delta \cdot \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ |\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c|\right]  
\end{align*}
Finally, using Jensen's inequality, we can write
\begin{align*}
   \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ |\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c|\right] \leq \sqrt{\mathbb{E}_{\mathbf{x},\epsilon_c}\left[ (\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c)^2\right] }
   \leq \sqrt{\sigma^2+ \bm{\beta}_c^\top \Sigma \bm{\beta}_c}.
\end{align*}
Combining the above computation leads to 
\begin{align*}
s_{\mathcal{D},\bm{\beta},c}(f) ~\geq~ \bm{\beta}_c^\top \Sigma \bm{M}  \bm{\beta}_c - L \Delta \Vert \bm{M}\Vert_{op} \Vert \bm{\beta}_c\Vert_\mathcal{H} \sqrt{\sigma^2 + \bm{\beta}_c^\top \Sigma \bm{\beta}_c},
\end{align*}
which proves Theorem \ref{thm:compute-robustness}.
\end{proof}

%%%%%
\subsection{Proof of Remark \ref{rem:thm-robust-linear}}
\label{pf:sf_lower_bound_tight}
\begin{proof}
The proof requires assumption of a Gaussian model, i.e., $\mathbf{x} \sim \mathcal{N}(0,\Sigma)$ and $\epsilon_c\sim \mathcal{N}(0,\sigma^2)$. Since the feature map is assumed to be linear, $\phi(\mathbf{x}) =\mathbf{x}$, it follows that $y_c = \bm{\beta}_c^\top \mathbf{x} + \epsilon_c$ is also Gaussian $y_c \sim \mathcal{N}(0,\sigma^2 + \bm{\beta}_c^\top \Sigma \bm{\beta}_c)$ and hence, $|y_c|$ is half-normal distributed.

Now recall that the first term in \eqref{eq:robust-decomp} can be computed exactly as $\bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c$. To compute the second term in \eqref{eq:robust-decomp}, note that
\begin{align*}
    \inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)
    &= \inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \bm{M} (\tilde{\mathbf{x}}- \mathbf{x})
\end{align*}
and the infimum is achieved when the difference is aligned with $\bm{M}^\top \bm{\beta}_c$, that is,  $\tilde{\mathbf{x}} = \mathbf{x} \pm \Delta \frac{\bm{M}^\top \bm{\beta}_c}{\Vert \bm{M}^\top \bm{\beta}_c \Vert_\mathcal{H}}$.
The sign depends on the sign of $y_c$, which leads to the second term in \eqref{eq:robust-decomp} compute to
\begin{align*}
    \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)\right]
    =  - \mathbb{E}_{\mathbf{x},\epsilon_c} \big[ |y_c|  \big] \cdot \Delta \cdot \Vert \bm{M}^\top \bm{\beta}_c \Vert_\mathcal{H}.
\end{align*}
Since $|y_c|$ is half-normal, $\mathbb{E}[|y_c|] = \sqrt{2/\pi}\sqrt{\sigma^2 + \bm{\beta}_c^\top\Sigma\bm{\beta}_c}$, while $\Vert\bm{M}^\top \bm{\beta}_c \Vert_\mathcal{H} \leq \Vert\bm{M}\Vert_{op} \Vert \bm{\beta}_c \Vert_\mathcal{H}$, with the inequality being tight when $\bm{\beta}_c$ is the eigenvector of $\bm{M}$, corresponding to the largest eigenvalue.
\end{proof}

%%%%%
\subsection{Proofs of Corollary \ref{cor:robust_score} and Corollary \ref{cor:information}}
\label{pf:robust_info_features}
\begin{proof}
In what follows, we restrict the linear map $\bm{M}$ as $\bm{M} = \tilde{\mathbf{U}}\tilde{\mathbf{U}}^\top = \sum_{i=1}^K \mathbf{u}_i\mathbf{u}_i^\top$ where $\tilde{\mathbf{U}} =[ \mathbf{u}_1, \ldots, \mathbf{u}_K]$ is an orthonormal matrix of basis for a $K$-dimensional subspace.
Since the operator norm $\Vert \bm{M} \Vert_{op} =1$ for projection matrix, the problem of finding the most robust subspace corresponds to maximising $\bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c = \sum_{i=1}^K \bm{\beta}_c^\top \Sigma \mathbf{u}_i\mathbf{u}_i^\top \bm{\beta}_c$. 


Note that if $(\lambda,\mathbf{u})$ is an eigenpair of $\Sigma$, then $\bm{\beta}_c^\top \Sigma \mathbf{u}\mathbf{u}^\top \bm{\beta}_c = \lambda (\bm{\beta}_c^\top \mathbf{u})^2$.
Hence, if we restrict the choice of $\mathbf{u}_1,\ldots,\mathbf{u}_K$ to the eigenvectors of $\Sigma$, the optimal projection is obtained by choosing the $K$ eigenvectors for which the robustness score $s_c(\mathbf{u}) = \lambda (\bm{\beta}_c^\top \mathbf{u})^2$ are largest. So the claim of Corollary \ref{cor:robust_score} holds only if the projections are restricted to eigenspaces of $\Sigma$.
The claim of Corollary \ref{cor:information} follows along the same line as the information of the feature $f = \bm{M}\phi$ can be computed as $\rho_{\mathcal{D},\bm{\beta},c}(f) = \bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c$. 
For the case of $\bm{M} = \tilde{\mathbf{U}}\tilde{\mathbf{U}}^\top$ where $\tilde{\mathbf{U}}$ is matrix of $K$ eigenvectors of $\Sigma$, we have $\rho_{\mathcal{D},\bm{\beta},c}(f) = \sum_{i=1}^K \lambda_i (\bm{\beta}_c^\top \mathbf{u}_i)^2$. Hence, if the search is restricted to eigenspaces of $\Sigma$, the most robust features also correspond to the most informative ones.
\end{proof}


\paragraph{Robust and informative features over all possible $K$-dimensional subspaces.}
If we consider $\bm{M} = \tilde{\mathbf{U}}\tilde{\mathbf{U}}^\top$ for any $\tilde{\mathbf{U}} =[ \mathbf{u}_1,\ldots,\mathbf{u}_K]$ with orthonormal columns, as assumed in Corollary \ref{cor:robust_score}, then 
\begin{align}
    \bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c
= \sum_{i=1}^K \bm{\beta}_c^\top \Sigma \mathbf{u}_i\mathbf{u}_i^\top \bm{\beta}_c
= \text{Trace}\left(\tilde{\mathbf{U}} \bm{\beta}_c \bm{\beta}_c^\top \Sigma \tilde{\mathbf{U}}^\top \right)
= \text{Trace}\left(\tilde{\mathbf{U}} \Sigma \bm{\beta}_c \bm{\beta}_c^\top \tilde{\mathbf{U}}^\top \right), \nonumber
\end{align}
where the last equality follows from taking transpose. Hence, the resulting maximisation problem can be written as 
\begin{align}
\underset{\tilde{\mathbf{U}}}{\max} ~\bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c 
~\equiv~ \underset{\tilde{\mathbf{U}}}{\max} ~\text{Trace}\left(\tilde{\mathbf{U}} \bm{\beta}_c \bm{\beta}_c^\top \Sigma \tilde{\mathbf{U}}^\top \right)
~\equiv~ \underset{\tilde{\mathbf{U}}}{\max} ~\text{Trace}\left(\tilde{\mathbf{U}} \mathbf{B}_c \tilde{\mathbf{U}}^\top \right),
\label{eq:alternate-max}
\end{align}
where $\mathbf{B}_c = \frac12 (\bm{\beta}_c \bm{\beta}_c^\top \Sigma + \Sigma \bm{\beta}_c \bm{\beta}_c^\top)$.
The above trace maximisation problem corresponds to finding the $K$ dominant eigenvectors of the matrix $\mathbf{B}_c$.
This leads to an alternative to Algorithm \ref{alg:cap} for finding robust projections for the test-time defense. 
The approach comprises of computing the dominant eigenvectors $\tilde{\mathbf{U}}_c$ of the matrix $\mathbf{B}_c$ for every class component $c\in\{1,\ldots,C\}$ and defining the robust output as $\tilde{h}(\mathbf{x}) = [\bm{\beta}_1^\top \tilde{\mathbf{U}}_1\tilde{\mathbf{U}}_1^\top \phi(\mathbf{x}),\ldots,\bm{\beta}_C^\top \tilde{\mathbf{U}}_C\tilde{\mathbf{U}}_C^\top \phi(\mathbf{x})]$.
The approach would result in theoretically more robust projections, but suffers computationally since it requires $(C+1)$ eigendecompositions instead of only one eigendecomposition in Algorithm \ref{alg:cap}. 
Hence, it has $O(C)$ more one-time computation than Algorithm \ref{alg:cap}, but with identical inference time.
The conclusion of Corollary \ref{cor:information} that the most robust features, obtained from the maximisation in \eqref{eq:alternate-max}, are also the most informative features still holds in this case.

\subsection{Dynamics of robust feature learning under GAM}
In this short analysis, we argue that if the trained model is a Generalized Additive Model (GAM), $h(\mathbf{x}) = \bm{\beta}^T \phi(x)$, the test-time defense of Algorithm \ref{alg:cap} could also be replicated through an early stopping of the training process. In other words, we argue that the components of $\bm{\beta}_c^T \phi(x)$ along the robust features---the eigen directions for which $s(\bm{u}) = \lambda (\bm{\beta}_c^\top \bm{u})^2$ are higher---are learned earlier.

For simplicity of analysis, we consider only the learning for $c$-th components, which corresponds to the following regression problem under GAM: Given training sample $\mathcal{D}_{\text{train}} := \{(\mathbf{x_i},y_i)\}_{i=1}^n \subseteq \mathcal{X}\times \mathbb{R}$, minimize the squared loss 
\[\underset{\bm{b}\in\mathbb{R}^p}{\text{minimize}} ~\frac{1}{2n} \sum_{i=1}^n \Vert y_i - \bm{b}^\top \phi(\mathbf{x}_i) \Vert_2^2.\]

\subsection{Proof of Proposition~\ref{prop:gd_top_features}}
\label{pf:dynamics_gam}
\begin{proof}
The optimal solution for $\bm{b}$ for the above problem when population squared loss is minimized is given by $\bm{\beta}_c = (\Phi \Phi^\top)^{-1} \Phi \bm{y}$, where $\Phi = [\phi(\mathbf{x}_1), \ldots, \phi(\mathbf{x}_n)]$ and $\bm{y}=[y_1 \ldots y_n]^\top$.
Furthermore, if the above optimisation is solved using gradient descent with learning rate $\eta >0$ and initialisation $\bm{b}^{(0)} = 0$, the parameters $\bm{b}^{(t)}$ are learned over the iterations as
\begin{align}
    \bm{b}^{(t)} &= \left( I - \dfrac{\eta}{n} \Phi\Phi^\top \right)\bm{b}^{(t-1)} + \dfrac{\eta}{n} \Phi \bm{y} \nonumber \\
    &= \sum_{k=0}^{t-1} \left( I - \dfrac{\eta}{n} \Phi\Phi^\top \right)^k \dfrac{\eta}{n}\Phi \bm{y} \nonumber \\
    &= \sum_{k=0}^{t-1} \left( I - \dfrac{\eta}{n} \Phi\Phi^\top \right)^k \cdot \dfrac{\eta}{n}\Phi\Phi^\top \bm{\beta}_c \label{eq:beta_dyn},
\end{align}
%\[\bm{b}^{(t)} ~=~ \left( I - \eta \Phi\Phi^\top \right)\bm{b}^{(t-1)} + \eta \Phi \bm{y} ~=~ \eta \sum_{k=0}^{t-1} \left( I - \eta \Phi\Phi^\top \right)^k \Phi \bm{y} ~=~ \eta \sum_{k=0}^{t-1} \left( I - \eta \Phi\Phi^\top \right)^k \cdot \Phi\Phi^\top \bm{\beta}_c,\]
with $\bm{b}^{(t)} \to \bm{\beta}_c =(\Phi \Phi^\top)^{-1} \Phi \bm{y}$ as $t\to\infty$. Suppose the eigen decomposition $\Sigma_{\text{train}} = \frac{1}{n}\Phi\Phi^\top$ is given by  $\Sigma_{\text{train}} = {\mathbf{U}} \text{diag}(\bm{\lambda}){\mathbf{U}}^\top = \sum\limits_{i=1}^p \lambda_i \bm{u}_i\bm{u}_i^\top$. Hence, \eqref{eq:beta_dyn} becomes
\begin{align}
    \eqref{eq:beta_dyn} &= \sum_{k=0}^{t-1} \left(\mathbf{U}\mathbf{U}^\top  - \eta \mathbf{U} \text{diag}(\bm{\lambda})\mathbf{U}^\top \right)^k \cdot \eta \mathbf{U} \text{diag}(\bm{\lambda})\mathbf{U}^\top \bm{\beta}_c \nonumber \\
    &= \sum_{k=0}^{t-1} \eta \mathbf{U} (I - \eta \text{diag}(\bm{\lambda}))^k \text{diag}(\bm{\lambda})  \mathbf{U}^\top \bm{\beta}_c \nonumber \\
    \bm{b}^{(t)} &= \sum_{i=1}^p (1-(1-\eta {\lambda_i})^t) \bm{u_i} \bm{u_i}^\top \bm{\beta}_c \label{eq:beta_t_dyn}
\end{align}
From \eqref{eq:beta_t_dyn}, it is clear that $\bm{u}_i$ directions are learnt in the order of $\lambda_i$. That is large $\lambda_i$ learned early during the training since $(1-(1-\eta {\lambda_i})^t)$ is decreasing and at fixed $t$, the eigendirection $\bm{u}_i$ with the largest $\lambda_i$ is learned first. This proves that the direction with maximum variance is learned first.
When the top eigendirection $\bm{u}_i$ aligns with the true signal $\bm{\beta}_c$, $\bm{u}_i$ will be the most robust direction as well. Hence, the top directions based on descending order of $\lambda$ is more robust if the directions align with the true underlying signal.
\end{proof}

\subsection{Connection to Neural Tangent Kernel features}
\label{sec:ntk}
We first briefly discuss NTK and the NTK features before proving the Proposition~\ref{prop:robust_top}.

\paragraph{Neural Tangent Kernels (NTKs) and NTK features.}
\citet{jacotntk, arora2019exact, gregyang2019scaling} show the equivalence of training a large width neural network by gradient descent to a deterministic kernel machine called Neural Tangent Kernel.
In the context of adversarial attacks and robustness, \cite{tsilivis2022can} propose a method to generate adversarial examples using NTK and show transferability of the attack to the finite width neural network counterpart successfully.
Additionally, the authors define NTK features using the eigenspectrum of the NTK gram matrix and observe that the robust features correspond to the top of the eigenspectrum and learned first during training. 
%
%Recently, \citet{tsilivis2022can} defined features using NTK gram matrix and empirically observed that the features corresponding to the top spectrum of NTK are more robust and learned first during training.
In the following, we define the NTK and NTK features and show its equivalence to our robust feature definition along with the proof that the robust NTK features correspond to the top of the spectrum.
The NTK gram matrix $\mathbf{\Theta} \in \mathbb{R}^{n\times n}$ is between all pairs of datapoints and the NTK between $\mathbf{x_i}$ and $\mathbf{x_j}$ for a network that outputs $f(\mathbf{w},\mathbf{x})$ at data point $\mathbf{x} \in \mathbb{R}^d$ parameterized by $\mathbf{w} \in \mathbb{R}^p$ is defined by the gradient of the network with respect to $\mathbf{w}$ as
\begin{equation}
     \mathbf{\Theta}(\mathbf{x_i},\mathbf{x_j}) = \mathbb{E}_{\mathbf{w}\sim \mathcal{N}(0,\mathbf{I}_p)}[\nabla_\mathbf{w} f(\mathbf{w},\mathbf{x_i})^T\nabla_\mathbf{w} f(\mathbf{w},\mathbf{x_j})].
\end{equation}
For an extremely large width network, gradient descent optimization with least square loss is equivalent to kernel regression, the kernel being the NTK. Formally, for a data $\mathbf{x}$, the converged network output in the large width limit is $f(\mathbf{w},\mathbf{x})=\mathbf{\Theta}(\mathbf{x},\mathbf{X})^{T}\mathbf{\Theta}(\mathbf{X},\mathbf{X})^{-1}\mathbf{Y} $.
\citet{tsilivis2022can} define NTK features using the eigendecomposition of $\mathbf{\Theta}(\mathbf{X},\mathbf{X}) = \sum_{i=1}^{n}\lambda_i \mathbf{v_i}\mathbf{v_i}^{T}$ as

\begin{align}
    f(\mathbf{w},\mathbf{x}) &= \mathbf{\Theta}(\mathbf{x},\mathbf{X})^{T}\mathbf{\Theta}(\mathbf{X},\mathbf{X})^{-1}\mathbf{Y}
    = \sum_{i=1}^{n}\lambda_i^{-1}\mathbf{\Theta}(\mathbf{x},\mathbf{X})^{T} \mathbf{v_i}\mathbf{v_i}^{T}\mathbf{Y} := \sum_{i=1}^{n}f^{ker}_{i}(\mathbf{x})
\label{eq:ntk_features}
\end{align}


where $f^{ker}_{i}(\mathbf{x}) \in \mathbb{R}^C$ is the $i$-th NTK feature of $\mathbf{x}$. Note that $f^{ker}_{i}$ is in accordance to our feature definition.
We prove the empirical observation that the top spectrum-induced NTK features $f^{ker}$ are more robust in the following. 
% \begin{prop}[Robustness lies at the top.]
% \label{prop:robust_top}
%     Let feature $f^{ker}_{i}$ be Lipschitz continuous in gradient of NTK with respect to $\mathbf{x}$ and an adversarial perturbation $\bm{\delta}$ for input $\mathbf{x}$ such that $||\bm{\delta}||_p \leq \Delta$. Then,
%     %of max $\Delta$ radius around input $\mathbf{x}$ i.e. $\delta\in \mathcal{B}(\Delta)(\mathbf{x})$ under $\ell_p$-norm. We can say that
%     $||f^{ker}_{i}(\mathbf{x}+\bm\delta)- f^{ker}_{i}(\mathbf{x})||_2\leq\Theta(\frac{1}{\lambda_i})$.
% \end{prop}
%Proposition~\ref{prop:robust_top} shows that the NTK features corresponding to the higher eigenvalues are more robust to adversarial perturbations, and hence robustness lies at the top. 
%This complements and strengthens the empirical observation in \citet{tsilivis2022can}. The proof is provided in appendix.


\subsection{Proof of Proposition \ref{prop:robust_top}}

\begin{proof}
Suppose that the NTK feature $f_i^{ker}$ is $L$-Lipschitz continuous in gradient of NTK with respect to $\mathbf{x}$. Then, we have 
\begin{align}
\begin{split}
  \left\Vert \nabla_\mathbf{x}\mathbf{\Theta}(\mathbf{x}+\bm\delta, \mathbf{X}) - \nabla_\mathbf{x}\mathbf{\Theta}(\mathbf{x}, \mathbf{X}) \Vert_2 \leq L \Vert \bm\delta \right\Vert_2.   
\end{split}
\label{eq:ntk_feature_grad_lip}
\end{align}
Recall that we can write the $i$-th NTK feature as $f^{ker}_{i}(\mathbf{x}) := \lambda_i^{-1}\mathbf{\Theta}(\mathbf{x},\mathbf{X})^{\top} \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y}$. 
Bounding $\Vert f^{ker}_{i}(\mathbf{x}+\bm\delta)-f^{ker}_{i}(\mathbf{x}) \Vert_2$ by Taylor's expansion and applying \eqref{eq:ntk_feature_grad_lip} yield
\begin{flalign}
\Vert f^{ker}_{i}(\mathbf{x}+\bm\delta) - f^{ker}_{i}(\mathbf{x}) \Vert_2 
&\stackrel{(a)}{=}\left\Vert \lambda_i^{-1}\bm\delta^{\top} \nabla_{\mathbf{x}}\mathbf{\Theta}(\mathbf{x}, \mathbf{X})\mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} + \lambda_i^{-1}\mathbf{R} \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} \right\Vert_2 &
(\text{Where }\mathbf{R} \text{ : remainder}) \nonumber \\
&\stackrel{(b)}{\leq} \left\Vert \lambda_i^{-1}\bm\delta^{\top} \nabla_{\mathbf{x}}\mathbf{\Theta}(\mathbf{x}, \mathbf{X})\mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} + \dfrac{\lambda_i^{-1}L}{2}\Vert \bm\delta \Vert_2 \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} \right\Vert_2 
&(\text{from \eqref{eq:ntk_feature_grad_lip}}) \nonumber \\
&\leq \lambda_i^{-1} \left\Vert \left(\bm\delta^{\top} \nabla_{\mathbf{x}}\mathbf{\Theta}(\mathbf{x}, \mathbf{X}) + \dfrac{L}{2}\Vert \bm\delta \Vert_2 \right) \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} \right\Vert_2 \nonumber \\
&= \Theta \left(\dfrac{1}{\lambda_i}\right) \nonumber
\end{flalign}
where $(a)$ follows from the Taylor's expansion of $f^{ker}_{i}(\mathbf{x}+\bm\delta)$ where $\mathbf{R}$ is the remainder terms and $(b)$ follows from \eqref{eq:ntk_feature_grad_lip}, i.e., $\mathbf{R}\leq (L/2) \Vert \bm\delta \Vert_2$.
\end{proof}

\paragraph{Empirical validation: Top NTK features are indeed robust.}
To verify Proposition~\ref{prop:robust_top}, we construct a sanity experiment using a simple 1-layer NN $f(\mathbf{x})=\frac{1}{d}\mathbf{w}^T\mathbf{x}$ with parameters $\mathbf{w}\in \mathbb{R}^{d}$ initialized from $\mathcal{N}(\mathbf{0}, \mathbf{I}_d)$. 
Let the data dimension $d$ be $100$, the number of training samples $n$ be $1000$ and the data is sampled from a Gaussian $\mathcal{N}(\mathbf{0}, \Sigma)$ where the covariance $\Sigma$ is a diagonal spiked matrix, that is, $\Sigma_{11} := 1+\sqrt{d/n}$ and $\Sigma_{ii}:=1 \, \forall i \ne 1$.
%zero mean gaussian. However, in order to obtain an isolated eigenvector with a large eigenvalue we alter the isotropic covariance matrix with $cov_{0,0}=1+\sqrt{\frac{d}{n_{train}}}$. 
We then construct NTK features from the spectral decomposition of the exact NTK. Plot 3 of Figure~\ref{fig:ntk} shows the norm of difference in the original, and adversarially perturbed NTK features with respect to the eigenvalues of the NTK spectrum for different perturbation strengths of $\Delta=\{0.01, 0.05, 0.1\}$. This validates our 
theory that the NTK features corresponding to the large eigenvalues are more robust and hence remain closer to the original feature even when perturbed.

% Figure environment removed

%%%%%%%%%%%%%%%
\section{Experiments}\label{app:exp}

\subsection{Parameters for different algorithms}
We set the parameters to the standard values in the literature. Refer to RobustBench for most of the attack parameters.
\begin{enumerate}
    \item PGD: We perform PGD with the standard parameters in Table~\ref{tab:parameters_pgd} to have an overall high strength PGD attack.% $\ell_{\infty}$ and $\ell_2$ PGD attack with standard $\epsilon=8/255$ and $\epsilon=0.5$, respectively, with the attack step size $\epsilon/4$ and $40$ iterations for $\ell_{\infty}$ and $\epsilon/5$ and $100$ iterations for $\ell_2$ to have an overall high strength PGD attack. 
    \begin{table}[H]
        \centering
        \caption{\textbf{Parameters for PGD.} We use these parameters for both training and attack.}
        \resizebox{0.5\linewidth}{!}{
        \begin{tabular}{lcccc}
        \toprule
            Dataset & $\ell_p$ & $\epsilon$ & step size & iteration \\
            \midrule
            \multirow{2}{*}{CIFAR-10, CIFAR-100}
             & $\ell_\infty$ & $8/255$ & $\epsilon/4$ & $40$ \\
             & $\ell_2$ & $0.5$ & $\epsilon/5$ & $100$ \\
             \multirow{1}{*}{tiny ImageNet}
             & $\ell_\infty$ & $4/255$ & $\epsilon/4$ & $40$ \\
             %& $\ell_2$ & $0.5$ & $\epsilon/5$ & $100$ \\
            \bottomrule
        \end{tabular}}
        \label{tab:parameters_pgd}
    \end{table}
    \item APGD-CE, APGD-DLR: We perform standard $\ell_{\infty}$ perturbation with the budget $\epsilon=8/255$. 
    \item For Adversarial training in Table ~\ref{tab:rfi_calib} we use same parameters for PGD, IAT, CW and TRADES as used in PGD attack from Table~\ref{tab:parameters_pgd}
\end{enumerate}

\subsection{Details of benchmarking baseline methods} \label{app:exp_details}
We perform benchmarking of our test-time defense on multiple SOTA methods that achieves adversarial robustness in the model. For our analysis of RFI on CIFAR-10 in table~\ref{tab:rfi_calib} we used PGD~\cite{madry2018towards}, Interpolated Adversarial Training~\cite{lamb2019interpolated}, Carlini-Warger Loss~\cite{carlini2017towards} and TRADES~\cite{zhang2019theoretically} to adversarially train the baseline model. In the case of Robust CIFAR-10~\cite{featuresnotbugs}, we only replaced the standard CIFAR-10 dataset with the robust dataset. In general for PGD, IAT and C\&W attacks the adversarial training works as generating an adversarial example using the underlying attack and the objective is to minimize loss on these adversarial examples. PGD attack uses gradient descent to iteratively maximize the classification loss with respect to the input while projecting the perturbed example into the norm ball defined for the attack. IAT uses a joint objective that minimizes the classification loss of perturbed examples generated from PGD or any other attack along with classification loss on clean data with MixUP~\cite{mixup}. We use Robust CIFAR-10 proposed in ~\cite{featuresnotbugs}, although is not an adversarial training method but rather the final dataset from a procedure to only retain robust features in the dataset. \citet{featuresnotbugs} disentangle the robust and non-robust features by creating a one-to-one mapping of an input to its robustified image. From an adversarially pretrained backbone (ResNet-18 using PGD $\ell_2-$norm and $\epsilon=0.25$) linear layer features are extracted for the natural image and also from a noise input. Then by minimizing the distance between these two representations in the input space over the noise, an image that only retains robust features of the original input is obtained.

For training using all these baseline adversarial training methods, we set the batch size as $128$. We use SGD with momentum as the optimizer where we set the momentum to $0.1$, we also set the weight decay to $0.0002$. We run our training for $200$ epochs and set the learning rate schedule as starting with $0.1$ for the first $100$ epochs and then we reduce the learning rate by $90$ percent every $50$ epochs. For calibration using temperature scaling~\citep{guo2017calibration}, we take the trained model and optimize for the temperature parameter. The standard deviation in all the cases of calibrated models is reported by loading the pretrained models and 5 runs of calibration. Hence, there is no standard deviation for the non-calibrated models, and we also do not report the standard deviation for the SoTA models directly loaded from RobustBench.

\subsection{Adaptive attack performance of RFI on Expectation Over Transformation (EOT) attack using ResNet-18 for CIFAR-10}

Expectation Over Transformation (EOT) is a procedure to synthesize examples that are adversarial over a chosen distribution of transformations \citep{athalye2018synthesizing}. This procedure is shown to generate adversarial examples that are more robust to noise, distortions and affine transformations, and are consistently adversarial to the neural networks. EOT as an adversarial attack is observed to be stronger \citep{tramer2020adaptive} where a randomized transformation is applied to an input $\mathbf{x}$ before being fed into a classifier. RFI can be easily integrated into the neural network classifier in such settings by computing the transformation matrix $\Tilde{\mathbf{U}}$ in RFI by applying random transformations to the training samples to ensure a similar distribution of the train and test sets. 

We evaluate ResNet-18 using all the training settings considered in Table~\ref{tab:basicexp} on CIFAR-10 for Expectation Over Transformation (EOT) as an adaptive attack. The hyperparameters are the same as considered for adaptive attack evaluation in Section~\ref{ss:rfi_robustness_small_models}.
We evaluate $\ell_\infty$ and $\ell_2$ attacks with $\epsilon=8/255$ budget, $\epsilon/4$ step size and $40$ iterations, and $0.5$ budget, $\epsilon/5$ step size and $100$ iterations, respectively. For RFI, we set $K=10$.
We observe that \emph{RFI improves the performance by $1$ to $2\%$ consistently for EOT attack as well.}

\begin{table*}[ht!]
    \centering
    \caption{\textbf{Adaptive attack performance of RFI on Expectation over Transformation (EOT) attack}. We consider $\ell_\infty$ (step size $\epsilon/4$, $40$ iterations) and $\ell_2$ (step size $\epsilon/5$, $100$ iterations) attack on CIFAR-10 with ResNet-18. %$\ell_\infty$ attack with step size $\epsilon/4$ and $40$ iterations. $\ell_2$ attack with size $\epsilon/5$ and $100$ iterations. 
    RFI improves robustness by {\color{applegreen}{$\mathbf{1}$ to $\mathbf{2\%}$}} as shown in $\%$ Gain column.
    %Results for CIFAR-100 and tiny ImageNet are in Appendix (Tables~\ref{tab:cifar100table} and \ref{tab:tinyImagenet}).
    }
    \vspace{-0.1cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{{@{}lccccccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{3}{c}{Clean} &
    \phantom{} & 
    \multicolumn{3}{c}{$\ell_\infty(\epsilon=\frac{8}{255})$} & \phantom{}&\multicolumn{3}{c}{$\ell_2 (\epsilon=0.5)$} \\
    \cmidrule{2-4} \cmidrule{6-8}\cmidrule{10-12}
    %&
        & Method & +RFI & $\%$ Gain  &&  Method & +RFI  & $\%$ Gain && Method & +RFI & $\%$ Gain\\ \midrule 
        PGD &  \textbf{81.08} \tiny{$\pm$ 0.01} & 80.49 \tiny{$\pm$ 0.08} & \color{brightmaroon}\textbf{-0.59} && 36.01 \tiny{$\pm$ 0.01} & \textbf{37.85} \tiny{$\pm$ 0.02}& \color{applegreen}\textbf{+1.84} && 35.52 \tiny{$\pm$ 0.01}  & \textbf{36.65} \tiny{$\pm$ 0.01}& \color{applegreen}\textbf{+1.13}\\
         IAT &  \textbf{90.32} \tiny{$\pm$ 0.01} & 89.89 \tiny{$\pm$ 0.01}& \color{brightmaroon}\textbf{-0.43} && 26.92 \tiny{$\pm$ 0.00} & \textbf{28.30} \tiny{$\pm$ 0.01}& \color{applegreen}\textbf{+1.38} && 30.30 \tiny{$\pm$ 0.00}  & \textbf{31.47} \tiny{$\pm$ 0.02}& \color{applegreen}\textbf{+1.17}\\
         C\&W & \textbf{77.55} \tiny{$\pm$ 0.03} & 77.50 \tiny{$\pm$ 0.02}& \color{brightmaroon}\textbf{-0.05} && 22.51 \tiny{$\pm$ 0.02} & 
         \textbf{23.88} \tiny{$\pm$ 0.03}& \color{applegreen}\textbf{+1.37} && 25.71 \tiny{$\pm$ 0.01}  & \textbf{26.95} \tiny{$\pm$ 0.03}& \color{applegreen}\textbf{+1.24}\\
         TRADES& \textbf{79.17} \tiny{$\pm$ 0.02} & 79.02 \tiny{$\pm$ 0.01}& \color{brightmaroon}\textbf{-0.15} && 47.20 \tiny{$\pm$ 0.01} & \textbf{47.98} \tiny{$\pm$ 0.01}& \color{applegreen}\textbf{+0.78} && 48.55 \tiny{$\pm$ 0.02}  & \textbf{49.61} \tiny{$\pm$ 0.01}& \color{applegreen}\textbf{+1.06}\\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.2cm}
    \label{tab:rfi_eot}
\end{table*}


\subsection{Adaptive attack performance of RFI on calibrated ResNet-18 for CIFAR-10}
We evaluate ResNet-18 using all the training settings considered in Table~\ref{tab:basicexp} on CIFAR-10 for calibrated models. The hyperparameters are the same as non-calibrated setting.
We evaluate $\ell_\infty$ and $\ell_2$ attacks with $\epsilon=8/255$ budget, $\epsilon/4$ step size and $40$ iterations, and $0.5$ budget, $\epsilon/5$ step size and $100$ iterations, respectively. For RFI, we set $K=10$.
We observe that \emph{RFI improves the performance by $4$ to $9\%$ for calibrated models.}
\begin{table*}[ht!]
    \centering
    \caption{\textbf{Adaptive attack performance of RFI on calibrated models} using temperature scaling. We consider $\ell_\infty$ (step size $\epsilon/4$, $40$ iterations) and $\ell_2$ (step size $\epsilon/5$, $100$ iterations) attack on CIFAR-10 with ResNet-18. %$\ell_\infty$ attack with step size $\epsilon/4$ and $40$ iterations. $\ell_2$ attack with size $\epsilon/5$ and $100$ iterations. 
    RFI improves robustness by {\color{applegreen}{$\mathbf{4}$ to $\mathbf{9\%}$}} as shown in $\%$ Gain column.
    %Results for CIFAR-100 and tiny ImageNet are in Appendix (Tables~\ref{tab:cifar100table} and \ref{tab:tinyImagenet}).
    }
    \vspace{-0.1cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{{@{}lccccccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{3}{c}{Clean} &
    \phantom{} & 
    \multicolumn{3}{c}{$\ell_\infty(\epsilon=\frac{8}{255})$} & \phantom{}&\multicolumn{3}{c}{$\ell_2 (\epsilon=0.5)$} \\
    \cmidrule{2-4} \cmidrule{6-8}\cmidrule{10-12}
    %&
        & Method & +RFI & $\%$ Gain  &&  Method & +RFI  & $\%$ Gain && Method & +RFI & $\%$ Gain\\ \midrule 
        Standard & \textbf{95.20} \tiny{$\pm$ 0.08}  & 88.20 \tiny{$\pm$ 0.10} & \color{brightmaroon}\textbf{-7.00} && 2.01 \tiny{$\pm$ 0.38} & \textbf{6.83} \tiny{$\pm$ 0.22} & \color{applegreen}\textbf{+4.82} && 2.58 \tiny{$\pm$ 0.62}  & \textbf{10.21} \tiny{$\pm$ 0.81} & \color{applegreen}\textbf{+7.63}\\
        Robust CIFAR-10 & {78.70} \tiny{$\pm$ 0.04} & \textbf{78.73} \tiny{$\pm$ 0.06} & \color{applegreen}\textbf{+0.03} && 3.81 \tiny{$\pm$ 0.14} & \textbf{8.03} \tiny{$\pm$ 0.21} & \color{applegreen}\textbf{+4.22} && 9.10 \tiny{$\pm$ 0.92}  & \textbf{11.21} \tiny{$\pm$ 0.68} & \color{applegreen}\textbf{+2.11}\\
         PGD &  \textbf{83.11} \tiny{$\pm$ 0.02} & 82.32 \tiny{$\pm$ 0.08} & \color{brightmaroon}\textbf{-0.79} && 42.96 \tiny{$\pm$ 0.75} & \textbf{50.08} \tiny{$\pm$ 0.88}& \color{applegreen}\textbf{+7.12} && 56.48 \tiny{$\pm$ 0.42}  & \textbf{62.13} \tiny{$\pm$ 0.92}& \color{applegreen}\textbf{+5.65}\\
         IAT &  \textbf{91.24} \tiny{$\pm$ 0.10} & 90.83 \tiny{$\pm$ 0.08}& \color{brightmaroon}\textbf{-0.41} && 46.22 \tiny{$\pm$ 0.10} & \textbf{51.34} \tiny{$\pm$ 0.83}& \color{applegreen}\textbf{+5.12} && 63.48 \tiny{$\pm$ 0.96}  & \textbf{71.12} \tiny{$\pm$ 0.29}& \color{applegreen}\textbf{+7.64}\\
         C\&W & \textbf{84.36} \tiny{$\pm$ 0.10} & 83.32 \tiny{$\pm$ 0.05}& \color{brightmaroon}\textbf{-1.03} && 41.62 \tiny{$\pm$ 0.90} & 
         \textbf{50.48} \tiny{$\pm$ 1.07}& \color{applegreen}\textbf{+8.86} && 56.63 \tiny{$\pm$ 0.68}  & \textbf{63.21} \tiny{$\pm$ 0.72}& \color{applegreen}\textbf{+6.58}\\
         TRADES& \textbf{81.11} \tiny{$\pm$ 0.01} & 79.38 \tiny{$\pm$ 0.04}& \color{brightmaroon}\textbf{-1.73} && 53.67 \tiny{$\pm$ 0.43} & \textbf{58.20} \tiny{$\pm$ 0.61}& \color{applegreen}\textbf{+4.53} && 62.12 \tiny{$\pm$ 0.28}  & \textbf{68.47} \tiny{$\pm$ 0.32}& \color{applegreen}\textbf{+6.35}\\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.2cm}
    \label{tab:rfi_calib}
\end{table*}

\subsection{Adaptive attack performance of RFI for CIFAR-100 and tiny ImageNet}
We evaluate both calibrated and non-calibrated ResNet-18 using all the adversarial training setting considered in Table~\ref{tab:rfi_calib} on CIFAR-100 since standard training would not result in robust model. We also consider tiny ImageNet dataset that has $100,000$ training and $10,000$ validation samples with $200$ classes and ResNet-50 pretrained adversarially on ImageNet. 
We evaluate $\ell_\infty$ attack with $\epsilon=8/255$ and $\epsilon=4/255$ for CIFAR-100 and tiny ImageNet, respectively. The attack budget is standard, taken from RobustBench.
For RFI, we set $K=100$ and $200$ (number of classes) for CIFAR-100 (Table~\ref{tab:cifar100table}) and tiny ImageNet (Table~\ref{tab:tinyImagenet}), respectively.
$\%$ Gain in tables is between Calibration+RFI and the base method.

\begin{table}[h]
    \centering
    \caption{\textbf{Adaptive attack performance of RFI on non-calibrated and calibrated models.} Robust performance evaluation of RFI on CIFAR-100 with ResNet-18 (step size $\epsilon/4$ and $40$ iterations). RFI improves the performace on an average by \color{applegreen}$\mathbf{4\%}$.}
    \vspace{-0.1cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{5}{c}{Clean} &
    \phantom{} & 
    \multicolumn{5}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$}\\
    \cmidrule{2-6} \cmidrule{8-12}
    %&
        & Method & +RFI & +Calibration &+Calibration+RFI & $\%$ Gain &&  Method & +RFI  & +Calibration &+Calibration+RFI& $\%$ Gain\\ \midrule 
         PGD &  {55.30} & 55.27 & \textbf{55.82} & 55.08& \color{brightmaroon}\textbf{-0.22} &&20.08 &{20.91} & 21.86 & \textbf{25.96}& \color{applegreen}\textbf{+5.88}\\
         IAT & \textbf{58.94} & 58.88  & 58.86 & 58.09 & \color{brightmaroon}\textbf{-0.85}&& 22.56 & {23.58} & 23.04 & \textbf{26.72}& \color{applegreen}\textbf{+4.16}\\
         C\&W & \textbf{49.36} & 49.31  & 49.30 & 49.02 & \color{brightmaroon}\textbf{-0.34}&&     
         10.44 & {11.86}  & 11.28 & \textbf{14.72}&\color{applegreen}\textbf{+4.28}\\
         TRADES&\textbf{55.17} &55.11  & \textbf{55.17} & 55.10 & \color{brightmaroon}\textbf{-0.07}&& 28.25 &{28.56}  & 28.43 & \textbf{30.91}&\color{applegreen}\textbf{+2.66}\\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.2cm}
    \label{tab:cifar100table}
\end{table}
In the case of tiny ImageNet, we subsampled $100$ training samples per class instead of using the full training set for computing the transformation matrix $\tilde{\bm{U}}$ of the feature covariance due to the computation time, and evaluated the clean and robust performances on the $10,000$ validation samples. The results are given in Tables~\ref{tab:cifar100table} and \ref{tab:tinyImagenet}.
We observe that \emph{RFI consistenly improves the adversarial performance on the datasets with a very small drop in the clean performance.} Thus this shows RFI generalizes to larger datasets as well. Furthermore, we would like to draw the attention that \emph{our method improves the performance even with a small subsample of the dataset.}
\begin{table}[h]
    \centering
     \caption{\textbf{Adaptive attack performance of RFI on non-calibrated and calibrated models.} Robust performance evaluation of RFI on tiny ImageNet with ResNet-50 (step size $\epsilon/4$ and $40$ iterations). RFI improves robustness even on large datasets.}
     \vspace{-0.1cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{5}{c}{Clean} &
    \phantom{} & 
    \multicolumn{5}{c}{$\ell_\infty (\epsilon=\frac{4}{255})$}\\
    %&
    \cmidrule{2-6} \cmidrule{8-12}
        & Method & +RFI  & +Calibration &+Calibration+RFI & $\%$ Gain &&  Method & +RFI  & +Calibration &+Calibration+RFI& $\%$ Gain\\ \midrule 
         PGD &  \textbf{62.42} & 62.39  & 62.40 & 62.32 &\color{brightmaroon}\textbf{-0.10}&&33.38 &\textbf{33.50}  & 33.43 & 34.27&\color{applegreen}\textbf{+0.89}\\
         % IAT & \textbf{58.94} & 58.88 && 22.56 & \textbf{23.58}\\
         % C\&W attack & \textbf{49.36} & 49.31 &&     
         % 10.44 & \textbf{11.86} \\
         % TRADES&\textbf{55.17} &55.11 && 28.25 &\textbf{28.56} \\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.2cm}
    \label{tab:tinyImagenet}
\end{table}




\subsection{Adaptive attack performance of RFI on state-of-the-art models from RobustBench}

\begin{table*}[t]
    \centering
    \caption{\textbf{Adaptive attack performance evaluation of RFI on state-of-the-art methods.} We apply APGD-CE, APGD-DLR and RobustBench attacks on CIFAR-10 and CIFAR-100. The inference time for RFI is $1\times$, whereas Anti-adv and SODEF are $8\times$ and $2\times$, respectively. There is no standard deviation as the trained models are directly from RobustBench. While RFI improves the robustness to AutoAttack {\color{applegreen}\textbf{upto}} $\color{applegreen}\mathbf{1.5\%}$ without calibration, SODEF and Anti-adv results in {\color{warningyellow}\textbf{no ($\mathbf{<0.1\%}$)}} or {\color{brightmaroon}\textbf{decrease}} in robustness consistently.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lclcccccc@{}}
         \toprule 
         %Dataset & 
         & Base Method & Defense & Clean & APGD-CE  & APGD-DLR  &FAB & Square & AutoAttack\\
         \midrule
         \multirow{15}{*}{\begin{sideways}CIFAR-10\end{sideways}}&
          %\midrule
          \multirowcell{4}{~\citet{carmon2019unlabeled}\\WideResNet-28-10} & None & \textbf{89.69} &  61.82 & 60.85 & 60.18 & 66.51 & 59.53\\
          &
          &Anti-adv & \textbf{89.69} & 61.81 & 60.89 & 60.11 & 66.58 & \color{brightmaroon}\textbf{58.70}\\
          &
          &SODEF & 89.68 & 60.20 & 60.72 & 58.04 & 65.28 & \color{brightmaroon}\textbf{57.23}\\
          &
          &RFI ($K=10$) & 89.60 & 62.38 & 61.58 & 60.21 & 66.59 & 60.72\\
          &
          &RFI (opt. $K=20$) & 89.60 & \textbf{62.45} & \textbf{61.60}& \textbf{60.38}& \textbf{66.90} & \color{applegreen}\textbf{61.02}\\
         \cmidrule{2-9}
         &\multirowcell{4}{~\citet{engstrom2019adversarial}\\ResNet-50} & None & \textbf{87.03} & 51.75 & 60.10 & 49.90 & 58.00  & 49.25\\
          &
          &Anti-adv & 87.00 & 51.62 & 59.95& 49.84 & 58.06 &\color{warningyellow}\textbf{49.20}\\
          &
          &SODEF & 86.95 & 50.01 & 58.20& 48.64 & 56.68 &\color{brightmaroon}\textbf{47.92}\\
          &
          &RFI ($K=10$)&87.01 & 51.86 & 61.84 & 51.28 & 58.07 & 50.75\\
          &
          & RFI (opt. $K=15$) & \textbf{87.03} & \textbf{51.94} & \textbf{61.90} & \textbf{51.46} & \textbf{58.12} & \color{applegreen}\textbf{50.98}\\
         \cmidrule{2-9}
         &
         \multirowcell{4}{~\citet{rice2020overfitting}\\WideResNet-34-10}  & None & 85.34 & 50.12 & 56.80 & 53.87 & 56.88 & 53.42\\
          &
          &Anti-adv  & \textbf{85.40} & 50.10 &  57.50 & 53.90 & 57.00 & \color{brightmaroon}\textbf{50.98}\\
          &
          &SODEF& 85.10 & 50.60 & 56.50& 53.72 & 56.21 & \color{brightmaroon}\textbf{50.09}\\
          &
          &RFI($K=10$) & 85.30 & 51.19 & 58.55 & 53.98 & \textbf{57.13} & 54.64\\
          &
          &RFI (opt. $K=35$) & 85.30 & \textbf{51.62} &  \textbf{58.97} & \textbf{54.12}& \textbf{57.13} & \color{applegreen}\textbf{54.86} \\
          \cmidrule{2-9}
          %\midrule
         &
         \multirowcell{4}{~\citet{wang2023better}\\WideResNet-28-10}  & None & \textbf{92.44} & 70.23 &  67.82 & 67.41 & 73.13 & 67.31\\
          &
          &Anti-adv  & \textbf{92.44} & 68.90 &  65.91 & 67.55 & 73.20 & \color{brightmaroon}\textbf{66.52}\\
          &
          &SODEF & 92.01 & 67.53 & 65.08 & 65.93 & 73.01 & \color{brightmaroon}\textbf{64.20}\\
          &
          &RFI ($K=10$) & 92.33 & 70.32 & 67.86 & \textbf{67.82} & 73.52 & 67.29\\
          &
          & RFI (opt. $K=20$) & 92.34 & \textbf{70.36} & \textbf{67.90} & \textbf{67.82} & \textbf{73.54} & \color{applegreen}\textbf{67.50}\\
         %  \cmidrule{2-9}
         %  &
         % \multirowcell{4}{~\citet{debenedetti2023light}\\XCiT-S12}  & None & \textbf{90.06} & - &  -& - & - & 56.14\\
         %  &
         %  &Anti-adv  & \textbf{-} & - &  - & - & - & \color{brightmaroon}\textbf{-}\\
         %  &
         %  &SODEF & - & - & - & - & - & \color{brightmaroon}\textbf{-}\\
         %  &
         %  &RFI ($K=10$) & - & - & - & - & - & \textbf{-}\\
         %  &
         %  & RFI (opt. $K=20$) &- & \textbf{-} & \textbf{-} & \textbf{-} & \textbf{-} & \color{applegreen}\textbf{-}\\
          \midrule
         %\hline 
         %\textbf{CIFAR-100} \\
         \multirow{15}{*}{\begin{sideways}CIFAR-100\end{sideways}}&
         \multirowcell{4}{~\citet{pang2022robustness}\\WideResNet-28-10} & None & \textbf{63.66} & 35.29 & 31.71& 31.32 & 35.70 & 31.08\\
          &
          &Anti-adv & 63.41 & 32.50 & 30.32 & 31.30 & 35.76 & \color{brightmaroon}\textbf{30.10}\\
          &
          &SODEF & 63.08 & 30.96 & 29.54 & 31.44 & 32.27 &\color{brightmaroon}\textbf{30.56}\\
          &
          &RFI ($K=100$)& 63.01 & 36.03 & \textbf{31.95}& 31.88 & 35.79 & 31.29\\
          &
          & RFI (opt. $K=115$) & 63.10 & \textbf{36.07} & \textbf{31.95} & \textbf{31.96} & \textbf{35.88} & \color{applegreen}\textbf{31.91}\\
          \cmidrule{2-9}
          &
         \multirowcell{4}{~\citet{addepalli2022efficient}\\ResNet-18} & 
         None & \textbf{65.45} & 33.49 & 28.55  & 28.00 & 33.70 & 27.67\\
          &
          &Anti-adv & 65.38 & 30.92 & 26.61 & 27.92 & 33.61 & \color{brightmaroon}\textbf{26.01}\\
          &
          &SODEF & 65.23 & 29.37 & 26.90 & 24.62 & 29.60 & \color{brightmaroon}\textbf{26.53}\\
          &
          &RFI $(K=\text{opt. } K = 100)$& 65.41 & \textbf{34.09} & \textbf{29.18} & \textbf{28.10} & \textbf{33.79} & \color{applegreen}\textbf{27.80}\\
          %&
          %&RFI (opt. $K=100$) & 65.41 & \textbf{34.09} & \textbf{29.10} & \textbf{27.80}\\
          \cmidrule{2-9}
         %\midrule
         &
         \multirowcell{4}{~\citet{rice2020overfitting}\\PreActResNet-18}  & None & \textbf{53.83} & 20.83 & 20.46 & \textbf{23.82} & 19.29 & 18.95\\
          &
          &Anti-adv  & \textbf{53.83} & 20.78 &  20.06 & 23.49 & 19.27 & \color{warningyellow}\textbf{18.97}\\
          &
          &SODEF & \textbf{53.83} & 18.50 & 19.20 & 19.66 & 16.05 & \color{brightmaroon}\textbf{16.92}\\
          &
          &RFI ($K=100$)& 53.70 & 21.10 & 20.98 & 20.93 & 18.13 & 19.23\\
          &
          &RFI (opt. $K=150$) & 53.75 & \textbf{21.18} & \textbf{21.10} & 21.03 & \textbf{19.53} &\color{applegreen}\textbf{19.46}\\
           \cmidrule{2-9}
          %\midrule
         &
         \multirowcell{4}{~\citet{wang2023better}\\WideResNet-28-10}  & None & \textbf{72.58} & 44.04 &  39.78 & 39.19 & 44.46 & 38.83\\
          &
          &Anti-adv  & 72.57 & 42.98 &  38.10 & 36.85 & 44.49 & \color{brightmaroon}\textbf{34.01}\\
          &
          &SODEF & 72.34 & 38.10 & 36.95 & 34.82 & 44.42 & \color{brightmaroon}\textbf{32.29}\\
          &
          &RFI ($K=100$)& 72.55 & 44.37 & 39.91 & 39.68 & 44.50 & 39.10\\
          &
          & RFI (opt. $K=115$) & 72.55 & \textbf{44.51} & \textbf{39.96} & \textbf{39.81} & \textbf{44.53} &\color{applegreen}\textbf{39.13}\\
          \midrule
         %\hline 
         %\textbf{CIFAR-100} \\
         \multirow{4}{*}{\begin{sideways}ImageNet\end{sideways}}&
         \multirowcell{2}{~\citet{salman2020adversarially}\\ResNet-50} & 
         None & \textbf{64.02} & 38.32 & 34.02  & 34.35 & 49.52 & -\\
          &
          & Dynamic RFI & 63.91 & \textbf{38.48} & \textbf{34.68} & \textbf{34.68}& \textbf{49.98} & -\\
          \cmidrule{2-9}
          &\multirowcell{2}{~\citet{salman2020adversarially}\\WideResNet-50-2} & 
         None & \textbf{68.46} & 40.67 & 37.09  & 37.81 & 54.61 & -\\
          &
          & Dynamic RFI & 68.41 & \textbf{40.84} & \textbf{37.56} & \textbf{38.12}& \textbf{54.78} & -\\
         \bottomrule
    \end{tabular}
    }
    %\caption{(Base/Ours) Indicate the performance of adversarial training methods after adding our adaptive test time defense on APGD-CE and APGD-DLR attacks ~\cite{croce2020reliable}}
    % \vspace{-0.3cm}
    \label{tab:benchmarkcomparison_app}
\end{table*}

For table~\ref{tab:benchmarkcomparison} we benchmark our test-time defense on multiple recent SoTA methods for CIFAR-10, CIFAR-100 and ImageNet. For all our baseline methods we obtain the model weights from RobustBench~\cite{croce2020reliable}. We update the weights of the last linear layer of the models using RFI and benchmark the updated models. 
We also report the performance for optimal $K$ in RFI.
We note that the Expected Calibration Error (ECE) for the SoTA models are very small as shown in Table~\ref{tab:mce_sota} (already well calibrated), hence we do not explicitly calibrate in Table~\ref{tab:benchmarkcomparison_app}.
Moreover, the results in Table~\ref{tab:benchmarkcomparison} show that calibration will only further improve robustness with RFI. Therefore, we do conservative analysis of RFI on the SoTA models.
For \citet{salman2020adversarially} on ImageNet we compute with and without dynamic RFI and not Anti-Adv and SODEF since it increase the inference costs of the evaluation such that we could no longer run experiments with our computational resources. Also we do not report AutoAttack since it requires all 4 attacks i.e. APGD-CE, APGD-DLR, FAB and Square to be executed sequentially which is outside the scope of max runtime of our resources.
Nevertheless, we observe that \emph{RFI improves the robustness reliably $\sim 1.5\%$ on average on non-calibrated SoTA models.} Importantly, SODEF and Anti-adv reduces the robustness performance especially on AutoAttack which is inline to the findings of \citet{croce2020reliable}.
%We provide our implementation of benchmarking experiments baselines used in Table~\ref{tab:benchmarkcomparison_app}.\citet{carmon2019unlabeled} show that additional $500,000$ unlabeled data improve robustness on CIFAR-10. \citet{engstrom2019adversarial} argue that the limitation of a network to represent input data in high-level features causes two semantically different images to have similar representation and propose training with an adversarial prior improves robustness. \citet{rice2020overfitting} suggest early stopping-based training to overcome this robust overfitting. \citet{wang2023better} propose use of class-conditioned diffusion models to generate additional labeled data for adversarial training and observe state-of-the-art performance using additional $50$ million synthetic images in training.
%\citet{pang2022robustness} ..
\begin{table}[h]
    \centering
    \caption{\textbf{Expected Calibration Error (ECE) of the SoTA models} are very small, hence already well calibrated.}
    \vspace{-0.1cm}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{{@{}ccccccc@{}}}
    \toprule 
     \multicolumn{3}{c}{CIFAR-10} &
    \phantom{} & 
    \multicolumn{3}{c}{CIFAR-100} \\
    \cmidrule{1-3} \cmidrule{5-7}
    %&
       Method & ECE & ECE after Calibration && Method & ECE & ECE after Calibration \\ \midrule 
        \multirowcell{2}{\citet{carmon2019unlabeled}\\WideResNet-28-10} & 4.310     &    \textbf{0.328} && \multirowcell{2}{\citet{pang2022robustness}\\WideResNet-28-10} & 0.364  & \textbf{0.142}  \\ \\
        \multirowcell{2}{\citet{engstrom2019adversarial}\\ResNet-50} &  0.091   & \textbf{0.065}  && \multirowcell{2}{\citet{addepalli2022efficient}\\ResNet-18} & 0.418   & \textbf{0.347}  \\\\
        \multirowcell{2}{\citet{rice2020overfitting}\\WideResNet-34-10} & 0.074     &  \textbf{0.037}  &&  \multirowcell{2}{\citet{rice2020overfitting}\\PreActResNet-18} & 0.138   & \textbf{0.074} \\\\
        \multirowcell{2}{\citet{wang2023better}\\WideResNet-28-10} & 0.145   & \textbf{0.039}  && \multirowcell{2}{\citet{wang2023better}\\WideResNet-28-10} & 0.366    & \textbf{0.290} \\\\
        \bottomrule
    \end{tabular}}
    % \vspace{-0.2cm}
    % \vspace{-0.6cm}
    \label{tab:mce_sota}
\end{table}

\subsection{Transferability Study}


We conduct a more detailed transferability of attack analysis on CIFAR-10 using ResNet-18 and on CIFAR-100 using PreActResNet-18. Here, we generated adversarial examples with respect to the base model and all the defences and evaluated the robustness of different adaptive defences under all the adversary cases (Transfer attacks). 
Then we present the results for calibrated ResNet-18 on CIFAR-10 in Table~\ref{tab:rfi_calib_transferability} which completes the analysis together with the results from Table~\ref{tab:transfercifar10}. 
We observe that the robustness of the calibrated model with RFI is on par with the base calibrated model. Moreover, when attacked with examples from RFI integrated model, the base model performs worse. 
Notably, \emph{the decrease in robust performance of the base method is much more than the decrease of the performance of RFI when evaluated on adversary from the base method.}
This shows RFI's goodness and further confirms the absence of an obfuscated gradient in RFI. Similar observation using a SoTA model on CIFAR-100 are in Table \ref{tab:transfer_attack_def_comparison} (Appendix).
In contrast, transfer attacks from base model on SODEF show a significant drop in robustness (Section 4.6.2 of \citet{kang2021stable}) and on Anti-adv render the defense ineffective (Section 3.8 of \citet{croce2022evaluating}). 
These results further highlight the soundness of RFI.
%Further, we study the adversary strength from RFI by accessing the base method against adversarial samples from RFI, showing \emph{RFI is a stronger adversary in both non-calibrated models} in Appendix~\ref{app:rfi_stronger_adv}.
\subsubsection{RFI results in stronger adversary against transfer from other defenses}
\label{app:rfi_stronger_adv}
In the set of experiments, we evaluate all combinations of transfer attacks on CIFAR-100 and PreActResNet-18~\cite{rice2020overfitting} in Table~\ref{tab:transfer_attack_def_comparison}. We compare the transferability of all adaptive test-time defenses to base model and within themselves by using adversarial examples generated with one defense attacking another defense. The general observation and expectation is that the model performance is affected the most when the adversarial examples are created using the same model, i.e., adaptive attack. This observation holds in our experiments too. The most interesting and impressive observation is that \emph{RFI outperforms all other methods in almost all the cases, even when adversarial examples are generated from base model + RFI}. Notice that SODEF and Anti-adv suffer the most when adversarial examples are generated from the respective models, unlike RFI showing the impressive robustness of our method.

\begin{table*}[h]
\caption{\textbf{Transfer attack on non-calibrated PreActResNet-18 for CIFAR-100.} RFI outperforms in all the cases and also generates the strongest adversary for the base model. }
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lcccc}
    \multicolumn{5}{c}{Adversarial Examples are generated from \textbf{Method (Rice et al)}}\\
    \toprule
    Attack & Method  & +AntiAdv &+SODEF &+RFI\\
    \midrule 
    APGD-CE & 20.83  &20.06  & 27.13  & \textbf{27.30}\\
    APGD-DLR & 20.46 & 20.52 & 29.33  & \textbf{29.53}\\
    FAB & 19.29 & 19.28  & 35.38  & \textbf{35.90}\\
    Square &  23.82 & 23.58 & 36.83 & \textbf{36.88}\\   
    AutoAttack & 18.95 & 18.97 & 26.09 & \textbf{26.43}\\
    \bottomrule
    \end{tabular}}
    
}
\hfill
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lcccc}
    \multicolumn{5}{c}{Adversarial Examples are generated from \textbf{Method+SODEF}}\\
    \toprule
    Attack & Method  & +AntiAdv &+SODEF &+RFI\\
    \midrule 
    APGD-CE &  32.99&\textbf{37.32}& 18.50 & 37.30\\
    APGD-DLR & 33.65 &\textbf{38.34}& 19.20 & 38.30\\
    FAB & 39.67  &48.11 &16.05 & \textbf{48.12}\\
    Square & 39.59  &48.20  & 19.66 & \textbf{48.22}  \\    
    AutoAttack & 32.76  & 33.16 & 15.69 & \textbf{37.23}\\
    \bottomrule
    \end{tabular}}
}
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lcccc}
    \multicolumn{5}{c}{Adversarial Examples are generated from \textbf{Method+AntiAdv}}\\
    \toprule
    Attack & Method  & +AntiAdv &+SODEF &+RFI\\
    \midrule 
    APGD-CE & 20.59 &20.58 & \textbf{27.31} & 26.65\\
    APGD-DLR & 20.39 & 20.49 & \textbf{28.92} & 28.53\\
    FAB & 19.27 & 19.27 & 35.80 & \textbf{38.69}\\
    Square & 23.60 & 23.49 & 37.41 & \textbf{39.04}\\      
    AutoAttack & 18.98&  18.96 & 25.61 & \textbf{26.15} \\
    \bottomrule
    \end{tabular}}
}
\hfill
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lcccc}
    \multicolumn{5}{c}{Adversarial Examples are generated from \textbf{Method+RFI}}\\
    \toprule
    Attack & Method  & +AntiAdv &+SODEF &+RFI\\
    \midrule 
    APGD-CE & 14.70 & 18.31 & 18.40 & \textbf{21.18}\\
    APGD-DLR & 14.12 & 18.30 & 19.21 & \textbf{21.10}\\
    FAB & 12.76 & 14.12 &  14.70 & \textbf{18.13}\\
    Square &  16.29 & 18.95 & 19.50 & \textbf{20.93}\\       
    AutoAttack & 12.55 & 16.50 & 16.92 & \textbf{19.46}\\
    \bottomrule
    \end{tabular}}
}
\label{tab:transfer_attack_def_comparison}
\end{table*}
\subsection{Transfer attack: RFI with calibration is on par with the base model} 
Results on transfer attacks, where we assess the performance of RFI against adversarial samples generated from the base, for calibrated and on CIFAR-10 with Resnet 18 backbone are in Tables~\ref{tab:rfi_calib_transferability}. 
Notably, \emph{RFI demonstrates comparable robustness to the base model}, ensuring that gradient obfuscation is \emph{not} at play in RFI and affirming that it reliably improves the model robustness. 
Moreover, the transferability of adversary from RFI leads to a degradation in robustness for the base model, suggesting that \emph{RFI acts as an on-par adversary to the base }(refer to +RFI rows of the left subtable in the Table ~\ref{tab:rfi_calib_transferability}). We hypothesize that the attack from base model and attack from base model + RFI affect different semantics or examples such that on average both are on par post-calibration. As expected the adversarial samples from the base method + RFI are more powerful and reduce the robustness of the base method to a greater extent than vice versa.
% (Table~\ref{tab:basicexp_transferability_1}). Comparing `Method' columns in Tables~\ref{tab:basic_cifar10} and \ref{tab:basicexp_transferability_1}, \emph{we note that while the loss in robust performance is comparatively smaller for TRADES which is in line with observation in \citet{croce2022evaluating},  C\&W loses its robustness completely suggesting the brittleness of C\&W defense.} On the other hand, the adversarial samples from the base method are much weaker and RFI is very robust, achieving the best performance (Table~\ref{tab:basicexp_transferability_2}). The improvement in robustness compared to adaptive attack is between $2-5\%$ for the different training defenses. 


\begin{comment}
\begin{table}[h]
\caption{\textbf{Transfer attack performance of RFI on calibrated models.} Setting same as Table~\ref{tab:rfi_calib}.}
    \vspace{-0.1cm}
\parbox{.49\linewidth}{
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{|c|cc|}
    \multicolumn{3}{c}{Base method training: $\ell_\infty$ with $\epsilon=8/255$}\\
    \hline
    \backslashbox{Adversary \\ from Base }{Evaluation}
    &\makebox{\quad Base \quad}&\makebox{+RFI}\\\hline
    PGD & 42.96 \tiny{$\pm$ 0.75} & 41.08 \tiny{$\pm$ 0.00} \\
    IAT & 46.22 \tiny{$\pm$ 0.10} & 43.10 \tiny{$\pm$ 0.00}\\
    C\&W &41.62 \tiny{$\pm$ 0.90} &39.21 \tiny{$\pm$ 0.00}\\
    TRADES & 53.67 \tiny{$\pm$ 0.43}&52.01 \tiny{$\pm$ 0.00}\\\hline
    \end{tabular}}
}
    \hfill
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{|c|cc|}
    \multicolumn{3}{c}{Base method training: $\ell_2$ with $\epsilon=0.5$}\\
    \hline
    \backslashbox{Adversary \\ from Base }{Evaluation}
    &\makebox{\quad Base \quad}&\makebox{+RFI}\\\hline
    PGD & 56.48 \tiny{$\pm$ 0.42} & 54.31 \tiny{$\pm$ 0.00} \\
    IAT & 63.48 \tiny{$\pm$ 0.96} & 62.01 \tiny{$\pm$ 0.00}\\
    C\&W & 56.63 \tiny{$\pm$ 0.68} & 54.90 \tiny{$\pm$ 0.00}\\
    TRADES & 62.12 \tiny{$\pm$ 0.23}& 59.91 \tiny{$\pm$ 0.00}\\\hline
    \end{tabular}}
}
\vspace{-0.2cm}
    \label{tab:rfi_calib_transferability_1}
\end{table}
\end{comment}
\begin{table}[h]
    \centering
    \caption{\textbf{Transfer attack performance of RFI on calibrated models.} RFI is on par with the base, ensuring reliable robustness improvement without gradient obfuscation. Setting same as Table~\ref{tab:rfi_calib}. The decrease in robustness of the base model is much more than the robustness of RFI when evaluated on the adversary from the base.}
    \parbox{0.65\linewidth}{
    \centering
    \begin{tabular}{{@{}lccccc@{}}}
    \multicolumn{6}{c}{Adversary generated from \textbf{base model+RFI} } \\
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{$\ell_\infty (\epsilon = \frac{8}{255})$} &
    \phantom{} & 
    \multicolumn{2}{c}{$\ell_2 (\epsilon = 0.5)$} \\
    \cmidrule{2-3} \cmidrule{5-6}
    %&
        & Method & +RFI &&  Method & +RFI \\ \midrule 
        PGD & 42.85 \tiny{$\pm$ 0.12}     &    \textbf{50.08} \tiny{$\pm$ 0.88} && 57.18 \tiny{$\pm$ 0.54}   & \textbf{62.13}  \tiny{$\pm$ 0.92} \\
        IAT &  47.92 \tiny{$\pm$ 0.31}   & \textbf{51.34} \tiny{$\pm$ 0.83} && 64.38 \tiny{$\pm$ 0.33}   & \textbf{71.12} \tiny{$\pm$ 0.29} \\
        C\&W & 40.73 \tiny{$\pm$ 0.64}     &  \textbf{50.48} \tiny{$\pm$ 1.07} && 55.96 \tiny{$\pm$ 0.88}   & \textbf{63.21}  \tiny{$\pm$ 0.72} \\
        TRADES &  55.43 \tiny{$\pm$ 0.42}   & \textbf{58.20} \tiny{$\pm$ 0.61} && 64.34 \tiny{$\pm$ 0.40}   & \textbf{68.47} \tiny{$\pm$ 0.32} \\
        \bottomrule
    \end{tabular}}
    \hfill
    \parbox{0.65\linewidth}{
    \centering
    \begin{tabular}{{@{}lccccc@{}}}
    \multicolumn{6}{c}{Adversary generated from \textbf{base method}.} \\
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{$\ell_\infty (\epsilon = \frac{8}{255})$} &
    \phantom{} & 
    \multicolumn{2}{c}{$\ell_2 (\epsilon = 0.5)$} \\
    \cmidrule{2-3} \cmidrule{5-6}
    %&
        & Method & +RFI &&  Method & +RFI \\ \midrule 
        PGD & \textbf{42.96} \tiny{$\pm$ 0.75}     &    41.38 \tiny{$\pm$ 0.48} && \textbf{56.48} \tiny{$\pm$ 0.42}   & 54.28  \tiny{$\pm$ 0.62} \\
        IAT &  \textbf{46.22} \tiny{$\pm$ 0.10}   & 43.44 \tiny{$\pm$ 0.21} && \textbf{63.48} \tiny{$\pm$ 0.96}   & 62.19 \tiny{$\pm$ 0.09} \\
        C\&W & \textbf{41.62} \tiny{$\pm$ 0.90}     &  39.10 \tiny{$\pm$ 0.81} && \textbf{56.63} \tiny{$\pm$ 0.68}   & 55.19  \tiny{$\pm$ 0.91} \\
        TRADES &  \textbf{53.67} \tiny{$\pm$ 0.43}   & 52.88 \tiny{$\pm$ 0.33} && \textbf{62.12} \tiny{$\pm$ 0.28}   & 59.85 \tiny{$\pm$ 0.97} \\
        \bottomrule
    \end{tabular}}
    \label{tab:rfi_calib_transferability}
\end{table}



\subsection{Static vs Dynamic RFI on calibrated model}
\label{app:static_dyn_calib} 
We extend the study of static vs dynamic RFI to calibrated models in this section using the same setup as Section~\ref{ss:static_dyn_rfi}, where we consider pretrained ResNet-18 on CIFAR-10 by applying PGD ($\ell_\infty, \epsilon=8/255$) and ($\ell_2, \epsilon=0.5$) in transfer attack setting i.e. generate adversarial examples from the base method. For the dynamic setting we compute the covariance batch-wise to compute $\tilde{U}$ with the input. Table~\ref{tab:rfi_static_adaptive_extra} shows \emph{static is better than dynamic RFI similar to non-calibrated setting}.
\begin{table}[h]
    \centering
    \caption{\textbf{Additional Comparison of static and dynamic/adaptive RFI on calibrated model showing static RFI is better than dynamic RFI.} Setting same as Table~\ref{tab:rfi_calib}. Adversarial examples are generated from the base model for fair comparison.}
    \resizebox{0.6\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Static & Dynamic && Static & Dynamic &&  Static & Dynamic\\ \midrule
        Standard &  10.36 & \textbf{11.65} &&20.08 & 11.64 &&\textbf{20.91} & 12.43\\
         Robust CIFAR-10 & \textbf{78.78} & 75.23 && \textbf{15.41} & 12.89 && \textbf{17.38} & 16.32 \\
         PGD & 83.22 & 82.86 && 46.02 & \textbf{46.83}  &&58.81 & \textbf{59.23}\\
         IAT & 91.26 & \textbf{91.35} && \textbf{49.06} & 48.53 && \textbf{66.67} & 66.28\\
         C\&W & 84.97 & 83.01 && \textbf{45.48} & 43.98 &&     
         \textbf{58.95} & 57.82 \\
         TRADES& 80.76 & 78.98 && \textbf{54.33} & 
         53.58 && \textbf{65.23} & 65.00 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
    \label{tab:rfi_static_adaptive_extra}
\end{table}

\subsection{Static RFI is Optimal}
\label{app:static_vs_dynamic}
In the case of dynamic RFI implementation, one needs to know when to apply the transformation as the model should be static for the attacker and adapted only for the defender. This poses implementation difficulty as the situation is mostly unknown in practice. Hence, we explore different variants of RFI in a dynamic setting where we compute the covariance matrix and eventually the transformation matrix $\Tilde{U}$ using the full validation set or single test input. 
We observe that \emph{the dynamic RFI is only marginally better than the static RFI} when full validation set is used in Table~\ref{tab:rfi_full_val_one_test} (a). 
Similarly, we present the result for single test input in Table~\ref{tab:rfi_full_val_one_test} where \emph{the method shows improvement in clean performance} since it is only a normalization of the feature representation. We perform these comparisons to highlight the fact that these hypothetical variants of dynamic RFI which work with information of validation set are also not significantly better than static RFI, thereby implying that \textbf{static RFI is indeed the optimal way of selecting $\tilde{U}$ as indicated by our theory}.

\begin{table}[h]
\caption{\textbf{Static RFI is the optimal approach.} RFI with covariance matrix calculated using different approaches.}
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \multicolumn{9}{c}{(a) RFI with covariance matrix calculated using complete validation set} \\
    \toprule
        \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
        \phantom{} & 
        \multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
        \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
        %&
        & Method & +RFI && Method & +RFI &&  Method & +RFI\\ \midrule
        %\multirow{5}{*}{CIFAR-10}&
        Standard & \textbf{95.28} & 88.53 && 1.02  & \textbf{9.35}  && 0.39 & \textbf{11.73}\\
        Robust CIFAR-10 & 78.69 & \textbf{78.80} && 1.30 & \textbf{11.21} && 9.63 & \textbf{12.56}\\
         PGD & \textbf{83.53} & 83.29 && 42.20  & \textbf{43.82} && 54.61 & \textbf{56.13}\\
         IAT & \textbf{91.86} & 91.32 && 44.76  & \textbf{47.65}  && 62.53 & \textbf{64.88}\\
         C\&W & \textbf{85.11} & 85.06 && 40.01 & \textbf{43.48} && 55.02 & \textbf{57.83}\\
         TRADES &  \textbf{81.13} & 80.97 && 51.70 &\textbf{54.29} && 60.03  & \textbf{61.79}\\
         \bottomrule
    \end{tabular}}
    }
    % \label{tab:rifar_full_val}
\hfill
\parbox{.5\linewidth}{
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \multicolumn{9}{c}{(b) RFI with covariance matrix calculated using single test input} \\
    \toprule
        \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
        \phantom{} & 
        \multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
        \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
        %&
        & Method & +RFI && Method & +RFI &&  Method & +RFI\\ \midrule
        %\multirow{5}{*}{CIFAR-10}&
        Standard & \textbf{95.28} & 90.10 && 1.02  & \textbf{10.81}  && 0.39 & \textbf{12.16}\\
        Robust CIFAR-10 & 78.69 & \textbf{78.70} && 1.30 & \textbf{11.88} && 9.63 & \textbf{12.87}\\
         PGD & \textbf{83.53} & 83.52 && 42.20  & \textbf{44.08} && 54.61 & \textbf{56.53}\\
         IAT & \textbf{91.86} & 91.86 && 44.76  & \textbf{47.95}  && 62.53 & \textbf{65.01}\\
         C\&W & \textbf{85.11} & 85.11 && 40.01 & \textbf{43.48} && 55.02 & \textbf{58.09}\\
         TRADES &  \textbf{81.13} & 81.09 && 51.70 &\textbf{54.78} && 60.03  & \textbf{62.17}\\
         \bottomrule
    \end{tabular}}
    }
    % \label{tab:rifar_full_val_single}
    \label{tab:rfi_full_val_one_test}
    \vspace{-0.1cm}
\end{table}

\subsection{Ablation study}
\subsubsection{Effect of $K$}
Neural Collapse is a phenomenon in which the penultimate feature of each class collapses to its mean after the training error reaches zero. This implies that there is principally only $C=\#classes$ number of feature vectors, one for each class. Hence, we suggest setting $K$ to number of classes. We also justify it experimentally in Figure~\ref{fig:ablation_k}. Additional experiments for large-scale models used in Tables~\ref{tab:benchmarkcomparison} and \ref{tab:benchmarkcomparison_app} with respect to CIFAR-10 and CIFAR-100 also show drop in eigenvalues at the number of classes across models, justifying our choice for $K$. We also extend the ablation study on $K$ to report the best performance in Figure~\ref{fig:ablation_k} and the optimal $K$ row in \ref{tab:benchmarkcomparison_app}. %We have now extended it to all the large-scale models for CIFAR-10 and CIFAR-100 from the Robustbench benchmark in Figure ~\ref{fig:ablation}. 
%As mentioned in the draft, the choice of $K$ is generally equal to the number of classes since the representation of each class collapses to one mean vector. This choice of $K$ is observed empirically to be effective as it improves the robust performance over the baseline across all the settings in general. 
Note that the optimal $K$ for robust performance is not the best for standard performance as we are choosing only the top-most informative features (Corollary~\ref{cor:information}). 
% Figure environment removed
% Figure environment removed




\subsubsection{Effect of step size in PGD}
We chose $\epsilon/4$ and $\epsilon/5$ for step sizes in $\ell_\infty$ and $\ell_2$, respectively, following the benchmarks in several works in RobustBench. 
The other common choice for the step size is proportional to the iterations, that is, $2\epsilon/40$ and $
2\epsilon/100$ for $\ell_\infty$ and $\ell_2$, respectively. 
We reevaluated the models in Table~\ref{tab:rfi_calib} with and without RFI for these step sizes and the results are in Table~\ref{tab:step_size_pgd}, showing that \emph{RFI is better than the base model}, in line with the observations in the previous experiments.

\begin{table}[H]
    \centering
    \caption{\textbf{RFI is more robust than the base model irrespective of the step size in PGD.} $2\epsilon/40$ for $\ell_{\infty}$ and $2\epsilon/100$ for $\ell_2$.}
    \begin{tabular}{{@{}lcccccc@{}}}
    \toprule 
    \multirow{2.5}{*}{Training} & \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
    \phantom{} & 
    \multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
    %&
    \cmidrule{2-3} \cmidrule{5-6}
    & Method & +RFI &&  Method & +RFI\\ \midrule 
    Standard & 0.03   &\textbf{9.73}&& 3.67 &\textbf{14.13}\\
    PGD & 44.44 & \textbf{45.48} && 57.77 &\textbf{58.97}\\
    IAT  & 45.91 & \textbf{48.26} && 66.26   & \textbf{67.73} \\
    Robust CIFAR10 & 7.14 & \textbf{15.57} && 12.94  & \textbf{17.15}\\ CW Attack & 38.89 & \textbf{41.53} && 51.20 & \textbf{54.45}\\
    TRADES  & 52.90  & \textbf{54.10} &&  61.66  & \textbf{63.35}\\

        \bottomrule
    \end{tabular}
    \label{tab:step_size_pgd}
\end{table}


%\subsection{Time comparison}




\begin{comment}
\begin{table*}[t]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI on state-of-the-art methods.} We apply APGD-CE, APGD-DLR and RobustBench attacks on both CIFAR-10 and CIFAR-100. The inference time for RFI is $1\times$, whereas Anti-adv and SODEF are $8\times$ and $2\times$, respectively. }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lclcccccc@{}}
         \toprule 
         %Dataset & 
         & Base Method & Adaptive Defense & Clean & APGD-CE  & APGD-DLR  &FAB & Square & AutoAttack\\
         \midrule
         \multirow{15}{*}{\begin{sideways}CIFAR-10\end{sideways}}&
         \multirowcell{4}{~\citet{carmon2019unlabeled}\\WideResNet-28-10} & None & \textbf{89.69} &  61.82 & 60.85 & 60.18 & 66.51 & 59.53\\
          &
          &Anti-adv \cite{alfarra2022combating}& \textbf{89.69} & 61.81 & 60.89 & 60.11 & 66.58 & 58.70\\
          &
          &SODEF \cite{kang2021stable}& 89.68 & 60.20 & 60.72 & 58.04 & 65.28 & 57.23\\
          &
          &RFI ($K=10$) & 89.60 & 62.38 & 61.58 & 60.21 & 66.59 & 60.72\\
          &
          &RFI (opt. $K=20$) & 89.60 & \textbf{62.45} & \textbf{61.60}& \textbf{60.38}& \textbf{66.90} & \textbf{61.02}\\
         \cmidrule{2-9}
         &
         \multirowcell{4}{~\citet{rice2020overfitting}\\WideResNet-34-10}  & None & 85.34 & 50.12 & 56.80 & 53.87 & 56.88 & 53.42\\
          &
          &Anti-adv \cite{alfarra2022combating} & \textbf{85.40} & 50.10 &  57.50 & 53.90 & 57.00 & 50.98\\
          &
          &SODEF \cite{kang2021stable}& 85.10 & 50.60 & 56.50& 53.72 & 56.21 & 50.09\\
          &
          &RFI($K=10$) & 85.30 & 51.19 & 58.55 & 53.98 & \textbf{57.13} & 54.64\\
          &
          &RFI (opt. $K=35$) & 85.30 & \textbf{51.62} &  \textbf{58.97} & \textbf{54.12}& \textbf{57.13} & \textbf{54.86} \\
          \cmidrule{2-9}
          %\midrule
         &
         \multirowcell{4}{~\citet{wang2023better}\\WideResNet-28-10}  & None & \textbf{92.44} & 70.23 &  67.82 & 67.41 & 73.13 & 67.31\\
          &
          &Anti-adv \cite{alfarra2022combating} & \textbf{92.44} & 68.90 &  65.91 & 67.55 & 73.20 & 66.52\\
          &
          &SODEF \cite{kang2021stable}& 92.01 & 67.53 & 65.08 & 65.93 & 73.01 & 64.20\\
          &
          &RFI ($K=10$) & 92.33 & 70.32 & 67.86 & \textbf{67.82} & 73.52 & 67.29\\
          &
          & RFI (opt. $K=20$) & 92.34 & \textbf{70.36} & \textbf{67.90} & \textbf{67.82} & \textbf{73.54} & \textbf{67.50}\\
          %\cline{2-7}
          \midrule
         %\hline 
         %\textbf{CIFAR-100} \\
         \multirow{15}{*}{\begin{sideways}CIFAR-100\end{sideways}}&
         \multirowcell{4}{~\citet{addepalli2022efficient}\\ResNet-18} & 
         None & \textbf{65.45} & 33.49 & 28.55  & 28.00 & 33.70 & 27.67\\
          &
          &Anti-adv \cite{alfarra2022combating}& 65.38 & 30.92 & 26.61 & 27.92 & 33.61 & 26.01\\
          &
          &SODEF \cite{kang2021stable}& 65.23 & 29.37 & 26.90 & 24.62 & 29.60 & 26.53\\
          &
          &RFI $(K=\text{opt. } K = 100)$& 65.41 & \textbf{34.09} & \textbf{29.18} & \textbf{28.10} & \textbf{33.79} & \textbf{27.80}\\
          %&
          %&RFI (opt. $K=100$) & 65.41 & \textbf{34.09} & \textbf{29.10} & \textbf{27.80}\\
          \cmidrule{2-9}
         %\midrule
         &
         \multirowcell{4}{~\citet{rice2020overfitting}\\PreActResNet-18}  & None & \textbf{53.83} & 20.83 & 20.46 & \textbf{23.82} & 19.29 & 18.95\\
          &
          &Anti-adv \cite{alfarra2022combating} & \textbf{53.83} & 20.78 &  20.06 & 23.49 & 19.27 & 18.97\\
          &
          &SODEF \cite{kang2021stable}& \textbf{53.83} & 18.50 & 19.20 & 19.66 & 16.05 & 16.92\\
          &
          &RFI ($K=100$)& 53.70 & 21.10 & 20.98 & 20.93 & 18.13 & 19.23\\
          &
          &RFI (opt. $K=150$) & 53.75 & \textbf{21.18} & \textbf{21.10} & 21.03 & \textbf{19.53} &\textbf{19.46}\\
           \cmidrule{2-9}
          %\midrule
         &
         \multirowcell{4}{~\citet{wang2023better}\\WideResNet-28-10}  & None & \textbf{72.58} & 44.04 &  39.78 & 39.19 & 44.46 & 38.83\\
          &
          &Anti-adv \cite{alfarra2022combating} & 72.57 & 42.98 &  38.10 & 36.85 & 44.49 & 34.01\\
          &
          &SODEF \cite{kang2021stable}& 72.34 & 38.10 & 36.95 & 34.82 & 44.42 & 32.29\\
          &
          &RFI ($K=100$)& 72.55 & 44.37 & 39.91 & 39.68 & 44.50 & 39.10\\
          &
          & RFI (opt. $K=115$) & 72.55 & \textbf{44.51} & \textbf{39.96} & \textbf{39.81} & \textbf{44.53} &\textbf{39.13}\\
          \midrule
         %\hline 
         %\textbf{CIFAR-100} \\
         \multirow{4}{*}{\begin{sideways}ImageNet\end{sideways}}&
         \multirowcell{2}{~\citet{salman2020adversarially}\\ResNet-50} & 
         None & \textbf{64.02} & 38.32 & 34.02  & 34.35 & 49.52 & -\\
          &
          & Dynamic RFI & 63.91 & \textbf{38.48} & \textbf{34.68} & \textbf{34.68}& \textbf{49.98} & -\\
          \cmidrule{2-9}
          &\multirowcell{2}{~\citet{salman2020adversarially}\\WideResNet-50-2} & 
         None & \textbf{68.46} & 40.67 & 37.09  & 37.81 & 54.61 & -\\
          &
          & Dynamic RFI & 68.41 & \textbf{40.84} & \textbf{37.56} & \textbf{38.12}& \textbf{54.78} & -\\
         \bottomrule
    \end{tabular}
    }
    %\caption{(Base/Ours) Indicate the performance of adversarial training methods after adding our adaptive test time defense on APGD-CE and APGD-DLR attacks ~\cite{croce2020reliable}}
    \label{tab:benchmarkcomparison_app}
\end{table*}
\end{comment}


\subsection{Conceptual ideas similar to RFI}

\paragraph{Low dimensional last layer.}
Similar to comparing RFI on last layer vs on intermediate layer in Section~\ref{ss:rfi_last_intermediate}, here we compare RFI and directly training a network with $K$ neurons in the last layer.
We consider two ResNet-18 models with an additional fully connected hidden layer of size $512$ and $10$, respectively, and are trained with PGD. We apply RFI only to the larger model with $512$ neurons and reduce the dimension to $10$, and compare the performances in terms of clean and robust accuracies in both cases. The results are presented in Table~\ref{tab:low_dim}, showing that \emph{RFI is more robust compared to imposing a low dimensional last layer.}

\begin{table}[H]
    \centering
    \caption{\textbf{RFI is more robust compared to imposing a low dimensional last layer.} ResNet-18 with last hidden layer size $10$ and $512$. RFI done on model with $512$ hidden layer. }
    % \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule
         & +hidden layer=10 & +hidden layer=512 & +hidden layer=512 + RFI  \\
         \midrule
         Clean & 83.71 & \textbf{84.13} & \color{applegreen}84.05\\
         Robust ($\ell_\infty$)  & 42.43 & 42.73 & \color{applegreen}\textbf{43.53} \\
         \bottomrule
    \end{tabular}%}
    \label{tab:low_dim}
\end{table}

We further argue qualitatively why setting low dimension layers is not equivalent to RFI as follows.
Firstly, overparameterization is shown empirically to be the key for both generalization \cite{brutzkus2019larger} and robustness \cite{madry2018towards}. Especially in the case of CNN, there is an empirical understanding to build the network with more than one fully connected layer after the convolution layers starting with larger widths to generalize well \cite{bengio2012practical}. These findings oppose the idea of having low dimension for the last hidden layer.
Secondly, there are similar insights from the sparsity of neural networks -- a smaller subnetwork with similar performance can be obtained by sparifying the network, called a lottery ticket \cite{frankle2018lottery}. Once known, lottery tickets can be trained from scratch to reach similar performance as the original network. However, it is not possible to obtain the ticket simply by setting hyperparameters for a smaller network from the beginning.
Finally, we emphasize that with RFI the last hidden-layer dimension is reduced by a large amount in comparison to the actual model. For example, in CIFAR-10, ResNet-50 with $2048$ dimensions is reduced to $10(=K)$. So, the network with $10$ dimension conventionally would not help generalization, which is conclusively established in the above experiment.

\subsection{Visualization of robust and non-robust features}
\label{app:feature_vis}
We obtain the visualizations of robust and non-robust features for an input $\mathbf{x}$ by solving $$\arg\min_{\tilde{\mathbf{x}}} || \Phi(\tilde{\mathbf{x}}) - \Phi(\mathbf{x})\mathbf{U}\mathbf{U}^T ||_2 $$ where $\mathbf{U}$ is top $K$ eigenvectors based on $s_c(.)$ for robust features and all eigenvectors except top $K$ for non-robust features. The objective is solved using gradient descent. Figure~\ref{fig:feature_vis} shows the visualizations of features for a few classes in CIFAR-10 using the PGD adversarially trained ResNet-18 model. `Robust $K=10$' and `Non-robust $K=10$' columns are obtained by setting $\mathbf{U}$ to the top $K$ eigenvectors and everything except the top $K$ eigenvectors based on $s_c(.)$, respectively. The columns top and bottom $100$ eigenvectors are obtained by setting $\mathbf{U}$ to the top and bottom $100$ eigenvectors based on the eigenvalues.
The feature visualizations show that robust and top eigenvectors result in more similar features. The interesting observation is that the non-robust and bottom eigenvectors are equally noisy and might have some useful information that reflects the drop in clean performance. Nevertheless, it is not possible to argue based on the visual interpretation of the features since the difference is primarily coming from the eigenspace of the feature covariance.
% While the non-robust features appear to be more noisy than the robust features visually, it is also possible that the non-robust features have some useful information that reflects the drop in clean performance.
% Figure environment removed


\begin{comment}

\paragraph{Eigenvalue truncation of intermediary layers}
We evaluate effectiveness of performing RFI on intermediate layers by truncating the last but one hidden layer of ResNet-18 and evaluate the PGD trained model considered in Table 1. This hidden layer has 512*4*4 convolution which we project to 10*4*4 using RFI procedure and the results are shown below. Doing RFI on the last hidden layer shows the best performance.
\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
    \toprule
         no RFI & RFI on last layer & RFI on last but one layer  \\
         \midrule
          42.20 & 43.29 & 36.06\\
         \bottomrule
    \end{tabular}}
    \caption{ResNet-18 model. }
    \label{tab:rfi_intermediary}
\end{table}


\end{comment}