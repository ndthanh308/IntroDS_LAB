\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Addepalli et~al.(2022)Addepalli, Jain, et~al.]{addepalli2022efficient}
Sravanti Addepalli, Samyak Jain, et~al.
\newblock Efficient and effective augmentation strategy for adversarial training.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 1488--1501, 2022.

\bibitem[Alfarra et~al.(2022)Alfarra, P{\'e}rez, Thabet, Bibi, Torr, and Ghanem]{alfarra2022combating}
Motasem Alfarra, Juan~C P{\'e}rez, Ali Thabet, Adel Bibi, Philip~HS Torr, and Bernard Ghanem.
\newblock Combating adversaries with anti-adversaries.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  5992--6000, 2022.

\bibitem[Andriushchenko et~al.(2020)Andriushchenko, Croce, Flammarion, and Hein]{andriushchenko2020square}
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
\newblock Square attack: a query-efficient black-box adversarial attack via random search.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXIII}, pp.\  484--501. Springer, 2020.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Athalye et~al.(2018{\natexlab{a}})Athalye, Carlini, and Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.
\newblock In \emph{International conference on machine learning}, pp.\  274--283. PMLR, 2018{\natexlab{a}}.

\bibitem[Athalye et~al.(2018{\natexlab{b}})Athalye, Engstrom, Ilyas, and Kwok]{athalye2018synthesizing}
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock In \emph{International conference on machine learning}, pp.\  284--293. PMLR, 2018{\natexlab{b}}.

\bibitem[Bengio(2012)]{bengio2012practical}
Yoshua Bengio.
\newblock Practical recommendations for gradient-based training of deep architectures.
\newblock In \emph{Neural Networks: Tricks of the Trade: Second Edition}, pp.\  437--478. Springer, 2012.

\bibitem[Brutzkus \& Globerson(2019)Brutzkus and Globerson]{brutzkus2019larger}
Alon Brutzkus and Amir Globerson.
\newblock Why do larger models generalize better? a theoretical perspective via the xor problem.
\newblock In \emph{International Conference on Machine Learning}, pp.\  822--830. PMLR, 2019.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 ieee symposium on security and privacy (sp)}, pp.\  39--57. Ieee, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Duchi, and Liang]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Chao et~al.(2023)Chao, Wen, Yang, Mao, Shah, and Yang]{chao2023test}
Yun-Yun Tsai1 Ju-Chin Chao, Albert Wen, Zhaoyuan Yang, Chengzhi Mao, Tapan Shah, and Junfeng Yang.
\newblock Test-time detection and repair of adversarial samples via masked autoencoder.
\newblock 2023.

\bibitem[Chen et~al.(2021)Chen, Li, and Zhang]{chen2021towards}
Zhuotong Chen, Qianxiao Li, and Zheng Zhang.
\newblock Towards robust neural networks via close-loop control.
\newblock \emph{arXiv preprint arXiv:2102.01862}, 2021.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{international conference on machine learning}, pp.\  1310--1320. PMLR, 2019.

\bibitem[Croce \& Hein(2020{\natexlab{a}})Croce and Hein]{croce2020minimally}
Francesco Croce and Matthias Hein.
\newblock Minimally distorted adversarial examples with a fast adaptive boundary attack.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2196--2205. PMLR, 2020{\natexlab{a}}.

\bibitem[Croce \& Hein(2020{\natexlab{b}})Croce and Hein]{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.
\newblock In \emph{International conference on machine learning}, pp.\  2206--2216. PMLR, 2020{\natexlab{b}}.

\bibitem[Croce et~al.(2020)Croce, Andriushchenko, Sehwag, Debenedetti, Flammarion, Chiang, Mittal, and Hein]{croce2020robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock \emph{arXiv preprint arXiv:2010.09670}, 2020.

\bibitem[Croce et~al.(2022)Croce, Gowal, Brunner, Shelhamer, Hein, and Cemgil]{croce2022evaluating}
Francesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, and Taylan Cemgil.
\newblock Evaluating the adversarial robustness of adaptive test-time defenses.
\newblock In \emph{International Conference on Machine Learning}, pp.\  4421--4435. PMLR, 2022.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, Tsipras, Tran, and Madry]{engstrom2019adversarial}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander Madry.
\newblock Adversarial robustness as a prior for learned representations.
\newblock \emph{arXiv preprint arXiv:1906.00945}, 2019.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Gowal et~al.(2021)Gowal, Rebuffi, Wiles, Stimberg, Calian, and Mann]{gowalimproving}
Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan~Andrei Calian, and Timothy Mann.
\newblock Improving robustness using generated data.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Grabinski et~al.(2022)Grabinski, Gavrikov, Keuper, and Keuper]{grabinski2022robust}
Julia Grabinski, Paul Gavrikov, Janis Keuper, and Margret Keuper.
\newblock Robust models are less over-confident.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 39059--39075, 2022.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{International conference on machine learning}, pp.\  1321--1330. PMLR, 2017.

\bibitem[Guo et~al.(2018)Guo, Rana, Cisse, and van~der Maaten]{guo2018countering}
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van~der Maaten.
\newblock Countering adversarial images using input transformations.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Huang et~al.(2021)Huang, Yu, Zhang, Ma, and Yao]{huang2021adversarial}
Yifei Huang, Yaodong Yu, Hongyang Zhang, Yi~Ma, and Yuan Yao.
\newblock Adversarial robustness of stabilized neural ode might be from obfuscated gradients.
\newblock \emph{Proceedings of Machine Learning Research vol}, 145:\penalty0 1--19, 2021.

\bibitem[Hwang et~al.(2023)Hwang, Lee, and Rhee]{hwang2023aid}
Duhun Hwang, Eunjung Lee, and Wonjong Rhee.
\newblock Aid-purifier: A light auxiliary network for boosting adversarial defense.
\newblock \emph{Neurocomputing}, pp.\  126251, 2023.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and Madry]{featuresnotbugs}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacotntk}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kang et~al.(2021)Kang, Song, Ding, and Tay]{kang2021stable}
Qiyu Kang, Yang Song, Qinxu Ding, and Wee~Peng Tay.
\newblock Stable neural ode with lyapunov-stable equilibrium points for defending against adversarial attacks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 14925--14937, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar10}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lamb et~al.(2019)Lamb, Verma, Kannala, and Bengio]{lamb2019interpolated}
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio.
\newblock Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy.
\newblock In \emph{Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security}, pp.\  95--103, 2019.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Liu et~al.(2018)Liu, Cheng, Zhang, and Hsieh]{liu2018towards}
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh.
\newblock Towards robust neural networks via random self-ensemble.
\newblock In \emph{Proceedings of the European Conference on Computer Vision (ECCV)}, pp.\  369--385, 2018.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2018.

\bibitem[Mao et~al.(2021)Mao, Chiquier, Wang, Yang, and Vondrick]{mao2021adversarial}
Chengzhi Mao, Mia Chiquier, Hao Wang, Junfeng Yang, and Carl Vondrick.
\newblock Adversarial attacks are reversible with natural supervision.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  661--671, 2021.

\bibitem[Nayak et~al.(2022)Nayak, Rawal, and Chakraborty]{nayak2022dad}
Gaurav~Kumar Nayak, Ruchit Rawal, and Anirban Chakraborty.
\newblock Dad: Data-free adversarial defense at test time.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.\  3562--3571, 2022.

\bibitem[Nie et~al.(2022)Nie, Guo, Huang, Xiao, Vahdat, and Anandkumar]{nie2022diffusion}
Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar.
\newblock Diffusion models for adversarial purification.
\newblock In \emph{International Conference on Machine Learning}, pp.\  16805--16827. PMLR, 2022.

\bibitem[Pang et~al.(2022)Pang, Lin, Yang, Zhu, and Yan]{pang2022robustness}
Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan.
\newblock Robustness and accuracy could be reconcilable by (proper) definition.
\newblock In \emph{International Conference on Machine Learning}, pp.\  17258--17277. PMLR, 2022.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (40):\penalty0 24652--24663, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Peng et~al.(2023)Peng, Xu, Cornelius, Hull, Li, Duggal, Phute, Martin, and Chau]{peng2023robust}
ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, Jason Martin, and Duen~Horng Chau.
\newblock Robust principles: Architectural design principles for adversarially robust cnns.
\newblock 2023.

\bibitem[Qian et~al.(2021)Qian, Zhang, Huang, Wang, Zhang, and Yi]{qian2021improving}
Zhuang Qian, Shufei Zhang, Kaizhu Huang, Qiufeng Wang, Rui Zhang, and Xinping Yi.
\newblock Improving model robustness with latent distribution locally and globally.
\newblock \emph{arXiv preprint arXiv:2107.04401}, 2021.

\bibitem[Qin et~al.(2021)Qin, Wang, Beutel, and Chi]{qin2021improving}
Yao Qin, Xuezhi Wang, Alex Beutel, and Ed~Chi.
\newblock Improving calibration through the relationship with adversarial robustness.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 14358--14369, 2021.

\bibitem[Rebuffi et~al.(2021)Rebuffi, Gowal, Calian, Stimberg, Wiles, and Mann]{rebuffi2021fixing}
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan~A Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann.
\newblock Fixing data augmentation to improve adversarial robustness.
\newblock \emph{arXiv preprint arXiv:2103.01946}, 2021.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8093--8104. PMLR, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115:\penalty0 211--252, 2015.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and Madry]{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3533--3545, 2020.

\bibitem[Shafahi et~al.(2019)Shafahi, Huang, Studer, Feizi, and Goldstein]{shafahiadversarial}
Ali Shafahi, W~Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein.
\newblock Are adversarial examples inevitable?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Shi et~al.(2021)Shi, Holtz, and Mishne]{shionline}
Changhao Shi, Chester Holtz, and Gal Mishne.
\newblock Online adversarial purification based on self-supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Stutz et~al.(2020)Stutz, Hein, and Schiele]{stutz2020confidence}
David Stutz, Matthias Hein, and Bernt Schiele.
\newblock Confidence-calibrated adversarial training: Generalizing to unseen attacks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9155--9166. PMLR, 2020.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus]{szegedy2013}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Tao et~al.(2023)Tao, Zhu, Guo, Dong, and Xu]{tao2023benchmark}
Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, and Chang Xu.
\newblock A benchmark study on calibration.
\newblock \emph{arXiv preprint arXiv:2308.11838}, 2023.

\bibitem[Tramer et~al.(2020)Tramer, Carlini, Brendel, and Madry]{tramer2020adaptive}
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry.
\newblock On adaptive attacks to adversarial example defenses.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1633--1645, 2020.

\bibitem[Tsilivis \& Kempe(2022)Tsilivis and Kempe]{tsilivis2022can}
Nikolaos Tsilivis and Julia Kempe.
\newblock What can the neural tangent kernel tell us about adversarial robustness?
\newblock \emph{arXiv preprint arXiv:2210.05577}, 2022.

\bibitem[Tsipras et~al.(2018)Tsipras, Santurkar, Engstrom, Turner, and Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock \emph{arXiv preprint arXiv:1805.12152}, 2018.

\bibitem[Wang et~al.(2021)Wang, Ju, Shelhamer, Wagner, and Darrell]{wang2021fighting}
Dequan Wang, An~Ju, Evan Shelhamer, David Wagner, and Trevor Darrell.
\newblock Fighting gradients with gradients: Dynamic defenses against adversarial attacks.
\newblock \emph{arXiv preprint arXiv:2105.08714}, 2021.

\bibitem[Wang et~al.(2019)Wang, Yin, and Zeng]{wang2019global}
Yu~Wang, Wotao Yin, and Jinshan Zeng.
\newblock Global convergence of admm in nonconvex nonsmooth optimization.
\newblock \emph{Journal of Scientific Computing}, 78:\penalty0 29--63, 2019.

\bibitem[Wang et~al.(2023)Wang, Pang, Du, Lin, Liu, and Yan]{wang2023better}
Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan.
\newblock Better diffusion models further improve adversarial training.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Wu et~al.(2021)Wu, Pan, Shen, Gu, Zhao, Li, Cai, He, and Liu]{wu2021attacking}
Boxi Wu, Heng Pan, Li~Shen, Jindong Gu, Shuai Zhao, Zhifeng Li, Deng Cai, Xiaofei He, and Wei Liu.
\newblock Attacking adversarial attacks as a defense.
\newblock \emph{arXiv preprint arXiv:2106.04938}, 2021.

\bibitem[Yang(2019)]{gregyang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{International conference on machine learning}, pp.\  7472--7482. PMLR, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\end{thebibliography}
