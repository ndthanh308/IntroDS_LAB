@article{szegedy2013,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}
@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}
@article{featuresnotbugs,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{lee2019wide,
  title={Wide neural tangent propagation},
  author={Lee, Joonseok and Wang, Pengchuan and Mahajan, Shubhendu and Rolnick, David and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{du2020random,
  title={Random feature neural tangent kernel},
  author={Du, Yiming and Lee, Joonseok and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2020}
}

@article{jacotntk,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{chen2015net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}
@inproceedings{simon2019first,
  title={First-order adversarial vulnerability of neural networks and input dimension},
  author={Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Leon and Sch{\"o}lkopf, Bernhard and Lopez-Paz, David},
  booktitle={International Conference on Machine Learning},
  pages={5809--5817},
  year={2019},
  organization={PMLR}
}
@article{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  journal={arXiv preprint arXiv:1805.12152},
  year={2018}
}
@article{nakkiran2019adversarial,
  title={Adversarial robustness may be at odds with simplicity},
  author={Nakkiran, Preetum},
  journal={arXiv preprint arXiv:1901.00532},
  year={2019}
}
@article{moosavi2016deepfool,
title={DeepFool: a simple and accurate method to fool deep neural networks},
author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2016}
}
@inproceedings{papernot2016limitations,
title={The limitations of deep learning in adversarial settings},
author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z Berkan and Swami, Ananthram},
booktitle={IEEE European Symposium on Security and Privacy (EuroS\&P)},
year={2016}
}
@inproceedings{madry2018towards,
title={Towards deep learning models resistant to adversarial attacks},
author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018}
}
@incollection{bengio2012practical,
  title={Practical recommendations for gradient-based training of deep architectures},
  author={Bengio, Yoshua},
  booktitle={Neural Networks: Tricks of the Trade: Second Edition},
  pages={437--478},
  year={2012},
  publisher={Springer}
}
@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}
@article{tramer2017ensemble,
  title={Ensemble adversarial training: Attacks and defenses},
  author={Tram{\`e}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1705.07204},
  year={2017}
}
@article{kannan2018adversarial,
  title={Adversarial logit pairing},
  author={Kannan, Harini and Kurakin, Alexey and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1803.06373},
  year={2018}
}
@article{brendel2017decision,
  title={Decision-based adversarial attacks: Reliable attacks against black-box machine learning models},
  author={Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
  journal={arXiv preprint arXiv:1712.04248},
  year={2017}
}
@inproceedings{brendel2018decisionbased,
  title={Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models},
  author={Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}
@inproceedings{yang2019scaling,
  title={Scaling limits of neural tangent kernels},
author={Yang, Rong and Fan, Zhihua and Li, Yining},
booktitle={International Conference on Learning Representations (ICLR)},
year={2019}
}

@inproceedings{novak2019neural,
title={Neural tangent kernels for structured data},
author={Novak, Marcel and Musat, Alexandru and G{"u}nnemann, Stephan},
booktitle={International Conference on Machine Learning (ICML)},
year={2019}
}
@inproceedings{arora2019generalization,
title={Generalization error of neural networks via neural tangent kernel},
author={Arora, Sanjeev and Du, Yiming and Lee, Joonseok and Sohl-Dickstein, Jascha},
booktitle={International Conference on Learning Representations (ICLR)},
year={2019}
}
@article{tsilivis2022can,
  title={What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?},
  author={Tsilivis, Nikolaos and Kempe, Julia},
  journal={arXiv preprint arXiv:2210.05577},
  year={2022}
}
@article{geifman2020similarity,
  title={On the similarity between the laplace and neural tangent kernels},
  author={Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1451--1461},
  year={2020}
}
@article{kanagawa2018gaussian,
  title={Gaussian processes and kernel methods: A review on connections and equivalences},
  author={Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K},
  journal={arXiv preprint arXiv:1807.02582},
  year={2018}
}
@inproceedings{basri2020frequency,
  title={Frequency bias in neural networks for input of non-uniform density},
  author={Basri, Ronen and Galun, Meirav and Geifman, Amnon and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={International Conference on Machine Learning},
  pages={685--694},
  year={2020},
  organization={PMLR}
}
@article{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{ronen2019convergence,
  title={The convergence rate of neural networks for learned functions of different frequencies},
  author={Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{athalye2018synthesizing,
  title={Synthesizing robust adversarial examples},
  author={Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  booktitle={International conference on machine learning},
  pages={284--293},
  year={2018},
  organization={PMLR}
}
@article{wang2019global,
  title={Global convergence of ADMM in nonconvex nonsmooth optimization},
  author={Wang, Yu and Yin, Wotao and Zeng, Jinshan},
  journal={Journal of Scientific Computing},
  volume={78},
  pages={29--63},
  year={2019},
  publisher={Springer}
}
@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2017 ieee symposium on security and privacy (sp)},
  pages={39--57},
  year={2017},
  organization={Ieee}
}
@inproceedings{lamb2019interpolated,
  title={Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy},
  author={Lamb, Alex and Verma, Vikas and Kannala, Juho and Bengio, Yoshua},
  booktitle={Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security},
  pages={95--103},
  year={2019}
}
@inproceedings{rice2020overfitting,
  title={Overfitting in adversarially robust deep learning},
  author={Rice, Leslie and Wong, Eric and Kolter, Zico},
  booktitle={International Conference on Machine Learning},
  pages={8093--8104},
  year={2020},
  organization={PMLR}
}
@article{engstrom2019adversarial,
  title={Adversarial robustness as a prior for learned representations},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Madry, Aleksander},
  journal={arXiv preprint arXiv:1906.00945},
  year={2019}
}
@inproceedings{croce2020reliable,
  title={Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={International conference on machine learning},
  pages={2206--2216},
  year={2020},
  organization={PMLR}
}
@article{rebuffi2021fixing,
  title={Fixing data augmentation to improve adversarial robustness},
  author={Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan A and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
  journal={arXiv preprint arXiv:2103.01946},
  year={2021}
}
@inproceedings{croce2022evaluating,
  title={Evaluating the adversarial robustness of adaptive test-time defenses},
  author={Croce, Francesco and Gowal, Sven and Brunner, Thomas and Shelhamer, Evan and Hein, Matthias and Cemgil, Taylan},
  booktitle={International Conference on Machine Learning},
  pages={4421--4435},
  year={2022},
  organization={PMLR}
}
@inproceedings{alfarra2022combating,
  title={Combating adversaries with anti-adversaries},
  author={Alfarra, Motasem and P{\'e}rez, Juan C and Thabet, Ali and Bibi, Adel and Torr, Philip HS and Ghanem, Bernard},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={5992--6000},
  year={2022}
}
@article{wu2021attacking,
  title={Attacking adversarial attacks as a defense},
  author={Wu, Boxi and Pan, Heng and Shen, Li and Gu, Jindong and Zhao, Shuai and Li, Zhifeng and Cai, Deng and He, Xiaofei and Liu, Wei},
  journal={arXiv preprint arXiv:2106.04938},
  year={2021}
}
@article{kang2021stable,
  title={Stable neural ode with lyapunov-stable equilibrium points for defending against adversarial attacks},
  author={Kang, Qiyu and Song, Yang and Ding, Qinxu and Tay, Wee Peng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14925--14937},
  year={2021}
}
@article{chen2021towards,
  title={Towards robust neural networks via close-loop control},
  author={Chen, Zhuotong and Li, Qianxiao and Zhang, Zheng},
  journal={arXiv preprint arXiv:2102.01862},
  year={2021}
}
@article{carmon2019unlabeled,
  title={Unlabeled data improves adversarial robustness},
  author={Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{hwang2023aid,
  title={Aid-purifier: A light auxiliary network for boosting adversarial defense},
  author={Hwang, Duhun and Lee, Eunjung and Rhee, Wonjong},
  journal={Neurocomputing},
  pages={126251},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{nie2022diffusion,
  title={Diffusion Models for Adversarial Purification},
  author={Nie, Weili and Guo, Brandon and Huang, Yujia and Xiao, Chaowei and Vahdat, Arash and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={16805--16827},
  year={2022},
  organization={PMLR}
}
@inproceedings{mao2021adversarial,
  title={Adversarial attacks are reversible with natural supervision},
  author={Mao, Chengzhi and Chiquier, Mia and Wang, Hao and Yang, Junfeng and Vondrick, Carl},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={661--671},
  year={2021}
}
@article{wang2021fighting,
  title={Fighting gradients with gradients: Dynamic defenses against adversarial attacks},
  author={Wang, Dequan and Ju, An and Shelhamer, Evan and Wagner, David and Darrell, Trevor},
  journal={arXiv preprint arXiv:2105.08714},
  year={2021}
}
@article{qian2021improving,
  title={Improving model robustness with latent distribution locally and globally},
  author={Qian, Zhuang and Zhang, Shufei and Huang, Kaizhu and Wang, Qiufeng and Zhang, Rui and Yi, Xinping},
  journal={arXiv preprint arXiv:2107.04401},
  year={2021}
}
@inproceedings{shafahiadversarial,
  title={Are adversarial examples inevitable?},
  author={Shafahi, Ali and Huang, W Ronny and Studer, Christoph and Feizi, Soheil and Goldstein, Tom},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{wang2023better,
  title={Better diffusion models further improve adversarial training},
  author={Wang, Zekai and Pang, Tianyu and Du, Chao and Lin, Min and Liu, Weiwei and Yan, Shuicheng},
  booktitle={International Conference on Machine Learning},
  year={2023}
}
@inproceedings{gowalimproving,
  title={Improving Robustness using Generated Data},
  author={Gowal, Sven and Rebuffi, Sylvestre-Alvise and Wiles, Olivia and Stimberg, Florian and Calian, Dan Andrei and Mann, Timothy},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{gregyang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}
@inproceedings{adlam2020neural,
  title={The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization},
  author={Adlam, Ben and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={74--84},
  year={2020},
  organization={PMLR}
}
@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}
@article{sabanayagam2022representation,
  title={Representation Power of Graph Convolutions: Neural Tangent Kernel Analysis},
  author={Sabanayagam, Mahalakshmi and Esser, Pascal and Ghoshdastidar, Debarghya},
  journal={arXiv preprint arXiv:2210.09809},
  year={2022}
}
@inproceedings{brutzkus2019larger,
  title={Why do larger models generalize better? A theoretical perspective via the XOR problem},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle={International Conference on Machine Learning},
  pages={822--830},
  year={2019},
  organization={PMLR}
}
@article{huang2020deep,
  title={Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks?---A Neural Tangent Kernel Perspective},
  author={Huang, Kaixuan and Wang, Yuqing and Tao, Molei and Zhao, Tuo},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={2698--2709},
  year={2020}
}
@article{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{ju2021generalization,
  title={On the generalization power of overfitted two-layer neural tangent kernel models},
  author={Ju, Peizhong and Lin, Xiaojun and Shroff, Ness},
  booktitle={International Conference on Machine Learning},
  pages={5137--5147},
  year={2021},
  organization={PMLR}
}
@article{ortiz2021can,
  title={What can linearized neural networks actually say about generalization?},
  author={Ortiz-Jim{\'e}nez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8998--9010},
  year={2021}
}
@article{nitanda2020optimal,
  title={Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2006.12297},
  year={2020}
}
@inproceedings{yuan2021neural,
  title={Neural tangent generalization attacks},
  author={Yuan, Chia-Hung and Wu, Shan-Hung},
  booktitle={International Conference on Machine Learning},
  pages={12230--12240},
  year={2021},
  organization={PMLR}
}
@article{nguyen2021dataset,
  title={Dataset distillation with infinitely wide convolutional networks},
  author={Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5186--5198},
  year={2021}
}
@inproceedings{xu2021knas,
  title={KNAS: green neural architecture search},
  author={Xu, Jingjing and Zhao, Liang and Lin, Junyang and Gao, Rundong and Sun, Xu and Yang, Hongxia},
  booktitle={International Conference on Machine Learning},
  pages={11613--11625},
  year={2021},
  organization={PMLR}
}
@inproceedings{chen2021neural,
  title={Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective},
  author={Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}
@article{deshpande2021linearized,
  title={A linearized framework and a new benchmark for model selection for fine-tuning},
  author={Deshpande, Aditya and Achille, Alessandro and Ravichandran, Avinash and Li, Hao and Zancato, Luca and Fowlkes, Charless and Bhotika, Rahul and Soatto, Stefano and Perona, Pietro},
  journal={arXiv preprint arXiv:2102.00084},
  year={2021}
}
@inproceedings{liu2020finding,
  title={Finding trainable sparse networks through neural tangent transfer},
  author={Liu, Tianlin and Zenke, Friedemann},
  booktitle={International Conference on Machine Learning},
  pages={6336--6347},
  year={2020},
  organization={PMLR}
}
@inproceedings{yang2023neural,
  title={On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks},
  author={Yang, Hongru and Wang, Zhangyang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1513--1553},
  year={2023},
  organization={PMLR}
}
@inproceedings{shionline,
  title={Online Adversarial Purification based on Self-supervised Learning},
  author={Shi, Changhao and Holtz, Chester and Mishne, Gal},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{cifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}
@article{croce2020robustbench,
  title={Robustbench: a standardized adversarial robustness benchmark},
  author={Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
  journal={arXiv preprint arXiv:2010.09670},
  year={2020}
}
@inproceedings{pang2022robustness,
  title={Robustness and accuracy could be reconcilable by (proper) definition},
  author={Pang, Tianyu and Lin, Min and Yang, Xiao and Zhu, Jun and Yan, Shuicheng},
  booktitle={International Conference on Machine Learning},
  pages={17258--17277},
  year={2022},
  organization={PMLR}
}
@article{liu2020linearity,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15954--15964},
  year={2020}
}
@inproceedings{croce2020minimally,
  title={Minimally distorted adversarial examples with a fast adaptive boundary attack},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={International Conference on Machine Learning},
  pages={2196--2205},
  year={2020},
  organization={PMLR}
}
@inproceedings{andriushchenko2020square,
  title={Square attack: a query-efficient black-box adversarial attack via random search},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXIII},
  pages={484--501},
  year={2020},
  organization={Springer}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}
@inproceedings{nayak2022dad,
  title={Dad: Data-free adversarial defense at test time},
  author={Nayak, Gaurav Kumar and Rawal, Ruchit and Chakraborty, Anirban},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3562--3571},
  year={2022}
}
@article{addepalli2022efficient,
  title={Efficient and effective augmentation strategy for adversarial training},
  author={Addepalli, Sravanti and Jain, Samyak and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1488--1501},
  year={2022}
}
@article{mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}
@article{tramer2020adaptive,
  title={On adaptive attacks to adversarial example defenses},
  author={Tramer, Florian and Carlini, Nicholas and Brendel, Wieland and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1633--1645},
  year={2020}
}
@inproceedings{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={7472--7482},
  year={2019},
  organization={PMLR}
}
@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}
@article{le2015tiny,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}
@article{salman2020adversarially,
  title={Do adversarially robust imagenet models transfer better?},
  author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3533--3545},
  year={2020}
}
@inproceedings{athalye2018obfuscated,
  title={Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  booktitle={International conference on machine learning},
  pages={274--283},
  year={2018},
  organization={PMLR}
}
@inproceedings{cohen2019certified,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={international conference on machine learning},
  pages={1310--1320},
  year={2019},
  organization={PMLR}
}
@inproceedings{guo2018countering,
  title={Countering Adversarial Images using Input Transformations},
  author={Guo, Chuan and Rana, Mayank and Cisse, Moustapha and van der Maaten, Laurens},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{liu2018towards,
  title={Towards robust neural networks via random self-ensemble},
  author={Liu, Xuanqing and Cheng, Minhao and Zhang, Huan and Hsieh, Cho-Jui},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={369--385},
  year={2018}
}
@article{chao2023test,
  title={Test-time Detection and Repair of Adversarial Samples via Masked Autoencoder},
  author={Chao, Yun-Yun Tsai1 Ju-Chin and Wen, Albert and Yang, Zhaoyuan and Mao, Chengzhi and Shah, Tapan and Yang, Junfeng},
  year={2023}
}
@article{qin2021improving,
  title={Improving calibration through the relationship with adversarial robustness},
  author={Qin, Yao and Wang, Xuezhi and Beutel, Alex and Chi, Ed},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14358--14369},
  year={2021}
}
@article{grabinski2022robust,
  title={Robust Models are less Over-Confident},
  author={Grabinski, Julia and Gavrikov, Paul and Keuper, Janis and Keuper, Margret},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={39059--39075},
  year={2022}
}
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}
@article{peng2023robust,
  title={Robust Principles: Architectural Design Principles for Adversarially Robust CNNs},
  author={Peng, ShengYun and Xu, Weilin and Cornelius, Cory and Hull, Matthew and Li, Kevin and Duggal, Rahul and Phute, Mansi and Martin, Jason and Chau, Duen Horng},
  year={2023}
}
@article{tao2023benchmark,
  title={A benchmark study on calibration},
  author={Tao, Linwei and Zhu, Younan and Guo, Haolan and Dong, Minjing and Xu, Chang},
  journal={arXiv preprint arXiv:2308.11838},
  year={2023}
}
@inproceedings{stutz2020confidence,
  title={Confidence-calibrated adversarial training: Generalizing to unseen attacks},
  author={Stutz, David and Hein, Matthias and Schiele, Bernt},
  booktitle={International Conference on Machine Learning},
  pages={9155--9166},
  year={2020},
  organization={PMLR}
}
@article{huang2021adversarial,
  title={Adversarial Robustness of Stabilized Neural ODE Might be from Obfuscated Gradients},
  author={Huang, Yifei and Yu, Yaodong and Zhang, Hongyang and Ma, Yi and Yao, Yuan},
  journal={Proceedings of Machine Learning Research vol},
  volume={145},
  pages={1--19},
  year={2021}
}
@inproceedings{debenedetti2023light,
  title={A light recipe to train robust vision transformers},
  author={Debenedetti, Edoardo and Sehwag, Vikash and Mittal, Prateek},
  booktitle={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  pages={225--253},
  year={2023},
  organization={IEEE}
}
