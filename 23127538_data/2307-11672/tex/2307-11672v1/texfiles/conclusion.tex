\section{Conclusion}
\label{sec:conclusion}
In this paper, we present a novel adaptive test-time defense that can be seamlessly integrated with any method at the time of deployment to improve the robustness of the underlying model. While the adaptive test-time defense as an approach offers promise to improve the robustness of models at the deployment stage, the general criticism of available methods is that they significantly increase the inference time of the underlying model. Our method, Robust Feature Inference (RFI), has no effect on the inference time of the underlying model which makes it a practical alternative for adaptive test-time defense. We also present a comprehensive theoretical justification for our approach describing the motivation behind retaining features in the top eigenspectrum of the feature covariance. In addition, we also show that these top features are more robust and informative, and validate our algorithm through extensive experiments. In conclusion, we propose the first theoretically guided adaptive test-time defense algorithm that has the same inference time as the base model with impressive experimental results.
%along with the adaptive test-time defense method, we have also proposed a new perspective on adversarial vulnerability in neural networks by using NTK features and have provided a deeper understanding of the underlying causes of this vulnerability. 
Our findings contribute to the ongoing efforts to develop robust models that can resist adversarial examples and improve the security and reliability of DNNs.
%In this paper, we have presented an analysis of the dynamics of adversarial vulnerability in neural networks. We have employed analytical tools from recent theories connecting neural networks and kernels, specifically the Neural Tangent Kernel (NTK), to study the dynamics of learning robust and non-robust features. Our results indicate that robust features are learned first during standard training and that the vulnerability to adversarial examples arises from the presence of non-robust features that are learned later in the training process. We have validated our hypothesis through an empirical experiment on neural networks used in practice. Our findings provide insight into the dynamics of adversarial vulnerability and offer an explanation for the learning dynamics of robust and non-robust components in hidden layer representations of neural networks, which are commonly understood as network features in practice. These results have implications for the design of robust models and for the understanding of the behavior of neural networks under adversarial attacks. Additionally, our work provides a mathematical framework to define and study the features learned during the standard training process and could be used to develop new techniques for training robust models. In conclusion, we have proposed a new perspective on the dynamics of adversarial vulnerability in neural networks by using NTK features and have provided a deeper understanding of the underlying causes of this vulnerability. Our findings contribute to the ongoing efforts to develop robust models that can resist adversarial examples and improve the security and reliability of DNNs.