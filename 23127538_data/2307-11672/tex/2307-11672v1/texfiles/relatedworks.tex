\section{Related Works}
\label{sec:related_works}
In recent years, there has been a significant amount of research on generating adversarial examples and simultaneously improving the robustness of deep neural networks (DNNs) against such examples. We review the most relevant works below along with adaptive test-time defenses and NTK.

\textbf{Adversarial robustness.} 
\citet{szegedy2013} first observed that the adversarial examples, which are small imperceptible perturbations to the original data, can fool the DNN easily and make incorrect predictions.
%by showing that small perturbations to the input of a DNN can cause it to make a mistake, even when the perturbations are imperceptible to the human eye.
To generate adversarial examples, Fast Gradient Sign Method (FGSM) is proposed by \citet{goodfellow2014explaining}.
%for generating adversarial examples  with a finding that the `adversarial subspaces' exist in the input space where DNNs are particularly vulnerable to adversarial examples. 
\citet{madry2018towards} introduced an effective and strong defense against adversarial examples known as adversarial training, a neural network training procedure to obtain models that are robust to such examples by minimizing the maximum loss on the adversarially perturbed inputs. Adversarial training remains a promising defense strategy to significantly improve the robustness of DNNs against adversarial attacks \citep{rice2020overfitting,carmon2019unlabeled,engstrom2019adversarial,wang2023better,pang2022robustness}. However, sophisticated attacks are developed to break the defenses such as Carlini-Wagner (C\&W) attack~\cite{carlini2017towards}, a method for generating adversarial examples;
%and is highly effective at fooling deep neural networks;
`obfuscated gradients' hypothesis \citep{athalye2018synthesizing} posits that the vulnerability to adversarial examples is due to the presence of gradients in the model that are easy to manipulate; `feature collision' hypothesis ~\cite{featuresnotbugs} postulates that the vulnerability is due to the presence of features in the data that are correlated with the labels, but loses the correlation when perturbed. To counter such attacks, advanced defense strategy such as constraining the Lipschitz constant of the model \citep{wang2019global} are developed. 

\textbf{Adaptive test-time defenses.} 
%Standard defenses against adversarial attacks such as adversarial training are static, that is, inputs and model parameters are fixed during inference. Whereas 
Adaptive defenses are usually implemented at test time and can often change model parameters and inputs at inference, even arbitrarily, to defend against the attack. One strategy of adaptive test-time defenses is through \emph{input purification}, which is, the inputs to a model (usually pre-trained with a robustness objective) is optimized with a test-time objective. This test-time optimization can be hand crafted~\cite{alfarra2022combating,wu2021attacking} or learned~\cite{mao2021adversarial,hwang2023aid} with the help of an auxiliary network~\cite{nie2022diffusion,nayak2022dad}. Another strategy for building adaptive test-time defenses is \emph{model adaptation}, where model parameters are often augmented with activations~\cite{chen2021towards}, implicit representations~\cite{kang2021stable, qian2021improving} and additional normalization layers~\cite{wang2021fighting}. 
%Adaptive test-time defenses also share similarities in their approaches to compute input purifications and model adaptations, often an iterative solver such as gradient descent is used to perform the computation of purified input or update parameters~\cite{qian2021improving,mao2021adversarial}. In addition to this, randomization by adding noise to input~\cite{wang2021fighting}, and additional data~\cite{mao2021adversarial} are also exploited at the test time, such that the output at inference time does not only depend on one single input. 
Although several methods are developed for adaptive test-time defenses, all of them increase the inference cost atleast $2\times$~\citep{kang2021stable} and sometimes $500\times$~\citep{shionline} compared to the underlying model. Furthermore, some methods are not really robust and are not competitive with the static model counterpart. Refer to \citet{croce2022evaluating} for detailed evaluation.  

\textbf{Adversarial attacks using Neural Tangent Kernels (NTKs).}
\citet{jacotntk, arora2019exact, gregyang2019scaling} show the equivalence of training a large width neural network by gradient descent to a deterministic kernel machine called Neural Tangent Kernel.
In the context of adversarial attacks and robustness, \cite{tsilivis2022can} propose a method to generate adversarial examples using NTK and show transferability of the attack to the finite width neural network counterpart successfully.
%behavior and generalization properties of randomly initialized wide neural networks trained by gradient descent with infinitesimally small learning rate is equivalent to a deterministic kernel machine. %Furthermore, \citet{jacotntk} also shows that the change in the kernel during training decreases as the network width increases, and hence, asymptotically, one can represent an infinitely wide neural network by a deterministic NTK. 
%With this finding, a plethora of works used NTK to theoretically analyze the generalization properties of neural networks \citep{adlam2020neural, huang2020deep, cao2019generalization, wei2019regularization, ju2021generalization, ortiz2021can}, the convergence of gradient descent based optimization \citep{du2019gradient, nitanda2020optimal} and also different aspects of graph neural networks \citep{sabanayagam2022representation}.
%Besides the theoretical analysis, NTK based methods are proposed for neural network architecture search \citep{xu2021knas, chen2021neural}, dataset distillation \citep{nguyen2021dataset}, model selection \citep{deshpande2021linearized}, network pruning \citep{liu2020finding, yang2023neural} and adversarial example generation \citep{yuan2021neural}. 
%In the context of our work, \citet{tsilivis2022can} study adversarial attacks using NTK. 
%The authors propose a method to generate adversarial examples for infinitely-wide neural networks and show that they transfer to fool their associated (finite width) nets. 
Additionally, the authors define NTK features using the   eigenspectrum of the NTK gram matrix and observe that the robust features correspond to the top of the eigenspectrum. 
%This supports previously conjectured hypotheses that prediction relies on robust and non-robust features and that robustness is traded for accuracy during standard training~\cite{tsipras2018robustness}. 
%We also define NTK features exactly as defined in \citet{tsilivis2022can} and 
As a supplementary result of our analysis, we theoretically show that the robust NTK features are at the top of the spectrum and also give   
%study their dynamics giving 
exact relationships in closed form for their convergence during training.


%In this work, we use the NTK to analyze the dynamics of adversarial vulnerability in neural networks by adopting the NTK feature map definition from \citet{tsilivis2022can}. Using this feature definition, we derive the correspondence of the network features to the NTK features and show that the top useful and robust features are learnt first during training. 

%study the robustness of the features learned by the network. Additionally, we use the NTK to study the relationship between the robustness of the features learned by the network and the generalization properties of the network.
