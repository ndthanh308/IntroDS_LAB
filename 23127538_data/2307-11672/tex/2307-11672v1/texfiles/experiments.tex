\section{Experimental Results}
\label{sec:exp}
We conduct extensive experiments to validate the performance of our algorithm, Robust Feature Inference, to different adversarial attacks on CIFAR-10 and CIFAR-100 \citep{cifar10} by integrating our method into several deep neural network models and different training procedures. We also compare our method to other adaptive test-time defenses. We first introduce the different evaluation methods for adversarial robustness that we use, network architectures, and other adaptive test-time defenses.
%For our experimental analysis, we use the CIFAR-10 and CIFAR 100~\cite{cifar10} datasets.

\textbf{Evaluation methods.} We evaluate our adaptive defense on different adversarial attacks namely, (a) Projected Gradient Descent (PGD)~\cite{madry2018towards}, a white box attack that perturbs the input within a small $\ell_p$ radius $\epsilon$, so that it maximizes the loss of a model. We perform $\ell_{\infty}(\epsilon=8/255)$ and $\ell_2(\epsilon=0.5)$ PGD attack. We use the attack step size $\epsilon/4$ and $40$ iterations for $\ell_{\infty}$ and $\epsilon/5$ and $100$ iterations for $\ell_2$ to have an overall high strength PGD attack. 
(b) Parameter-free attacks such as Auto PGD-Cross Entropy (APGD-CE) and Auto PGD-Difference Logit Ratios (APGD-DLR) \citep{croce2020reliable} under $\ell_{\infty}(\epsilon=8/255)$ threat model, that are extensions of PGD attack with no step size parameter and stronger than PGD.
%We then obtain robust accuracy for standard and adversarially trained models before and after integrating RFI. 
%In table~\ref{tab:benchmarkcomparison} we evaluate the effectiveness of RFI on APGD-CE, APGD-DLR~\cite{croce2020reliable}, and RobustBench~\cite{croce2020robustbench} under $\ell_{\infty}(\epsilon=8/255)$ threat model. APGD-CE and APGD-DLR are parameter-free attack extensions of PGD attack which are based on cross entropy (CE) and difference of logit ratios (DLR). 
(c) RobustBench \citep{croce2020robustbench}, a suite of white-box and black-box attacks including APGD-CE, APGD-DLR, Fast Adaptive Boundary Attack (FAB) \citep{croce2020minimally}, and Square Attacks \citep{andriushchenko2020square}. RobustBench overcomes the limitations of auto-attack by flagging potential overestimation of robustness.

\textbf{Benchmarking on SOTA methods.}
We evaluate our method on different settings: (a) Resnet-18 backbone with standard training and the popular adversarial training methods such as PGD~\cite{madry2018towards}, Interpolated Adversarial Training (IAT)~\cite{lamb2019interpolated}, Carlini-Wagner (CW) loss~\cite{carlini2017towards} and also use Robust CIFAR-10 dataset created by \citet{featuresnotbugs}.
%We observe that the addition of RFI at inference time does improve the performance across all the methods. 
%For our table~\ref{tab:benchmarkcomparison} 
(b) We select different state-of-the-art adversarially trained  models~\cite{carmon2019unlabeled,engstrom2019adversarial,rice2020overfitting,wang2023better,pang2022robustness} from RobustBench upon which at the test time we integrate RFI.
These methods either use additional data ~\citep{carmon2019unlabeled, wang2023better}, informed adversarial prior~\citep{engstrom2019adversarial} or early stopping based training ~\citep{rice2020overfitting} to improve the robustness of models.
\citet{carmon2019unlabeled} show that additional $500,000$ unlabeled data improve robustness on CIFAR-10. \citet{engstrom2019adversarial} argue that the limitation of a network to represent input data in high-level features causes two semantically different images to have similar representation and propose training with an adversarial prior improves robustness. \citet{rice2020overfitting} suggest early stopping-based training to overcome this robust overfitting. \citet{wang2023better} propose use of class-conditioned diffusion models to generate additional labeled data for adversarial training and observe state-of-the-art performance using additional $50$ million synthetic images in training.
%\citet{pang2022robustness} .. 
We detail each training method in appendix. 

\textbf{Comparison to other adaptive test-time defenses.}
Recently, \citet{croce2022evaluating} provide an overview of $9$ adaptive test-time defenses of which only four methods actually improve robustness and are competitive with the current state-of-the-art performances. We compare RFI with two out of the four methods: SODEF~\cite{kang2021stable} and Anti-Adv~\cite{alfarra2022combating}. The choice of SODEF and Anti-Adv is due to their relatively faster inference costs $2\times$ and $8\times$, respectively, and that they are representative of model adaptation and input modification strategies for adaptive test-time defenses, respectively.

\begin{table*}[t]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI.} $\ell_\infty$ and $\ell_2$ PGD attack on CIFAR-10 with Resnet-18. $\ell_\infty$ attack with step size $\epsilon/4$ and $40$ iterations. $\ell_2$ attack with size $\epsilon/5$ and $100$ iterations.}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule
%\multirow{2.5}{*}{\textbf{Dataset}} &
\multirow{2.5}{*}{Training} & \multicolumn{2}{c}{Clean} & \phantom{}& \multicolumn{2}{c}{$\ell_\infty (\epsilon=\frac{8}{255})$} &
\phantom{} & 
\multicolumn{2}{c}{$\ell_2 (\epsilon=0.5)$}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
%&
& Method & +RFI && Method & +RFI &&  Method & +RFI\\ \midrule
%\multirow{5}{*}{CIFAR-10}&
Standard & \textbf{95.28} & 88.53 && 1.02  & \textbf{4.35}  && 0.39 & \textbf{9.73}\\
         PGD~\cite{madry2018towards} & \textbf{83.53} & 83.22 && 42.20  & \textbf{43.29} && 54.61 & \textbf{55.03}\\
         IAT~\cite{lamb2019interpolated} & \textbf{91.86} & 91.26 && 44.76  & \textbf{46.95}  && 62.53 & \textbf{64.31}\\
         Robust CIFAR-10~\cite{featuresnotbugs} & 78.69 & \textbf{78.75} && 1.30 & \textbf{7.01} && 9.63 & \textbf{11.00}\\
         C\&W attack~\cite{carlini2017towards} & \textbf{85.11} & 84.97 && 40.01 & \textbf{42.56} && 55.02 & \textbf{56.79}\\
         \bottomrule
    \end{tabular}}
    \label{tab:basicexp}
\end{table*}

\subsection{Results and Discussion}
\textbf{Robust performance evaluation of RFI.} We obtain clean and robust accuracy for standard and adversarially trained models before and after integrating RFI and setting $K$ to the number of classes, that is, $K=10$ and $K=100$ for CIFAR-10 and CIFAR-100, respectively. The results for different training procedures on CIFAR-10 with ResNet-18 are presented in Table~\ref{tab:basicexp}. 
We observe that our method improves the robust performance of adversarially trained models consistently, on an average by $2\%$ and maximum by $6\%$ for $\ell_\infty$ attack on Robust CIFAR-10 training.
In table~\ref{tab:benchmarkcomparison}, we present the effectiveness of RFI on APGD-CE, APGD-DLR and RobustBench under $\ell_{\infty}(\epsilon=8/255)$ threat model for different state-of-the-art methods.
We observe that adding RFI at inference time improves the performance across all the methods. 
Importantly, RFI improves the robust performance (atleast marginally) for all the state-of-the-art methods, even though each one of them achieves robustness by incorporating very different strategies like additional data, early stopping or informed prior. This shows that strength and effectiveness of our method.

\textbf{Effect of adversary strength.} 
We study the effect of adversary strength on our method, RFI, by taking the adversarially trained ResNet-18 on CIFAR-10 using PGD ($\epsilon=8/255$ for $\ell_{\infty}$ and $ \epsilon=0.5$ for $\ell_2$) as the base model. % where 10 is the number of steps for gradient descent. 
Table~\ref{tab:strengthpgd} shows the evaluation of RFI with $\ell_{\infty}$ PGD attack for $\epsilon=\{2/255,4/255,12/255,16/255\}$ and $40$ iterations, and $\ell_2$ attack for $\epsilon=\{0.25,0.75,1\}$ and $100$ iterations. We observe that the underlying model augmented with RFI consistently improves over baseline across the perturbations of various strengths, especially by over $1\%$ for adversary that is stronger than the base model ($\epsilon=\{12/255,16/255\}$ for $\ell_\infty$ and $\epsilon=\{0.75, 1.00\}$ for $\ell_2$). 

\begin{table*}[t]
    \centering
    \caption{\textbf{Effect of Adversary Strength.} Evaluation of RFI for $\ell_{\infty}$ and $\ell_2$ PGD attack. }
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{{@{}lcccccccc@{}}}
    \toprule
\multirow{2.5}{*}{Method} & \multicolumn{4}{c}{$\ell_\infty$} & \phantom{}& \multicolumn{3}{c}{$\ell_2 $}\\
\cmidrule{2-5} \cmidrule{7-9}
& $\epsilon=\frac{2}{255}$ & $\epsilon=\frac{4}{255}$ & $\epsilon=\frac{12}{255}$ & $\epsilon=\frac{16}{255}$ && $\epsilon=0.25$ & $\epsilon=0.75$ & $\epsilon=1.00$\\ \midrule
PGD &  74.60 & 64.02 & 23.34 & 11.66 && 71.34 & 40.91 & 28.25\\
         PGD+RFI & \textbf{74.99} & \textbf{64.91} & \textbf{24.32} & \textbf{12.55} && \textbf{71.48} & \textbf{41.95} & \textbf{29.24}\\ 
         \bottomrule
    \end{tabular}}
    \label{tab:strengthpgd}
\end{table*}

\textbf{Effect of parameter $K$.} 
While we set $K$ to be the number of classes, in Table~\ref{tab:benchmarkcomparison} we also report the best performance of RFI by finding the optimal $K$ using grid search.
To study the effect of the parameter in detail, we vary $K$ for the adversarial training methods on CIFAR-10 with ResNet-18 under $\ell_{\infty}(\epsilon=8/255)$ threat model. 
Plot 1 of Figure~\ref{fig:topk_ntknorm} shows
%In fig~\ref{fig:alltopk} we observe the effect of changing hyperparameter $K$ across different adversarial training methods on CIFAR10 dataset under $\ell_{\infty}(\epsilon=8/255)$ threat model. We observe 
that the adversarial training methods (PGD, IAT and C\&W) behave similarly in their accuracy profile as compared to standard training and Robust CIFAR-10 dataset. Moreover, the best performance is for $K=10$ for all the robust training methods. The corresponding eigenvalue spectrum exhibits a knee drop after top-$10$ eigenvalues  (plot 2), which motivates our choice of $K$ as top-$10$ features for each class, equivalent to the number of classes. As a complementary explanation for our choice of $K$, neural collapse phenomenon observes that the penultimate feature of each class collapses to its mean during the terminal phase of training, that is, after the training error is almost zero \citep{papyan2020prevalence}. This implies that there is principally only $C$ number of feature vectors, one for each class, justifying our choice. 
%Since top-10 useful features across classes are not the same we effectively select the first 15 eigenvectors after the union.

\textbf{Top NTK features are indeed robust.}
To verify Proposition~\ref{prop:robust_top}, we construct a sanity experiment using a simple 1-layer NN $f(\mathbf{x})=\frac{1}{d}\mathbf{w}^T\mathbf{x}$ with parameters $\mathbf{w}\in \mathbb{R}^{d}$ initialized from $\mathcal{N}(\mathbf{0}, \mathbf{I}_d)$. 
Let the data dimension $d$ be $100$, the number of training samples $n$ be $1000$ and the data is sampled from a Gaussian $\mathcal{N}(\mathbf{0}, \Sigma)$ where the covariance $\Sigma$ is a diagonal spiked matrix, that is, $\Sigma_{11} := 1+\sqrt{d/n}$ and $\Sigma_{ii}:=1 \, \forall i \ne 1$.
%zero mean gaussian. However, in order to obtain an isolated eigenvector with a large eigenvalue we alter the isotropic covariance matrix with $cov_{0,0}=1+\sqrt{\frac{d}{n_{train}}}$. 
We then construct NTK features from the spectral decomposition of the exact NTK. Plot 3 of Figure~\ref{fig:topk_ntknorm} shows the norm of difference in the original, and adversarially perturbed NTK features with respect to the eigenvalues of the NTK spectrum for different perturbation strengths of $\Delta=\{0.01, 0.05, 0.1\}$. This validates our 
theory that the NTK features corresponding to the large eigenvalues are more robust and hence remain closer to the original feature even when perturbed.

% % Figure environment removed

\begin{table}[t]
    \centering
    \caption{\textbf{Robust performance evaluation of RFI on state-of-the-art methods.} We apply APGD-CE, APGD-DLR and RobustBench attacks on both CIFAR-10 and CIFAR-100. The inference time for RFI is $1\times$, whereas Anti-adv and SODEF are $8\times$ and $2\times$, respectively. }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lclcccc@{}}
         \toprule 
         %Dataset & 
         & Base Method & Adaptive Defense & Clean & APGD-CE  & APGD-DLR  & RobustBench\\
         \midrule
         \multirow{20}{*}{\begin{sideways}CIFAR-10\end{sideways}}&
         \multirowcell{4}{~\citet{carmon2019unlabeled}\\WideResNet-28-10} & None & \textbf{89.69} &  61.82 & 60.85  & 59.53\\
          &
          &Anti-adv \cite{alfarra2022combating}& \textbf{89.69} & 61.81 & 60.89 & 58.70\\
          &
          &SODEF \cite{kang2021stable}& 89.68 & 60.20 & 60.72& 57.23\\
          &
          &RFI ($K=10$) & 89.60 & 62.38 & 61.58 & 60.72\\
          &
          &RFI (opt. $K=20$) & 89.60 & \textbf{62.45} & \textbf{61.60} & \textbf{61.02}\\
         \cmidrule{2-7}
         %\midrule
         &
         \multirowcell{4}{~\citet{engstrom2019adversarial}\\ResNet-50} & None & \textbf{87.03} & 51.75 & 60.10 & 49.25\\
          &
          &Anti-adv \cite{alfarra2022combating}& 87.00 & 51.62 & 59.95& 49.20\\
          &
          &SODEF \cite{kang2021stable}& 86.95 & 50.01 & 58.20& 47.92\\
          &
          &RFI ($K=10$)&87.01 & 51.86 & 61.84 & 50.75\\
          &
          & RFI (opt. $K=15$) & \textbf{87.03} & \textbf{51.94} & \textbf{61.90} & \textbf{50.98}\\
         \cmidrule{2-7}
         %\midrule
         &
         \multirowcell{4}{~\citet{rice2020overfitting}\\WideResNet-34-10}  & None & 85.34 & 50.12 & 56.80& 53.42\\
          &
          &Anti-adv \cite{alfarra2022combating} & \textbf{85.40} & 50.10 &  57.50 & 50.98\\
          &
          &SODEF \cite{kang2021stable}& 85.10 & 50.60 & 56.50& 50.09\\
          &
          &RFI($K=10$) & 85.30 & 51.19 & 58.55 & 54.64\\
          &
          &RFI (opt. $K=35$) & 85.30 & \textbf{51.62} & \textbf{58.97} & \textbf{54.86} \\
          \cmidrule{2-7}
          %\midrule
         &
         \multirowcell{4}{~\citet{wang2023better}\\WideResNet-28-10}  & None & \textbf{92.44} & 70.23 &  67.82 & 67.31\\
          &
          &Anti-adv \cite{alfarra2022combating} & \textbf{92.44} & 68.90 &  65.91 & 66.52\\
          &
          &SODEF \cite{kang2021stable}& 92.01 & 67.53 & 65.08 & 64.20\\
          &
          &RFI ($K=10$) & 92.33 & 70.32 & 67.86 & 67.29\\
          &
          & RFI (opt. $K=20$) & 92.34 & \textbf{70.36} & \textbf{67.90} & \textbf{67.50}\\
          %\cline{2-7}
          \midrule
         %\hline 
         %\textbf{CIFAR-100} \\
         \multirow{20}{*}{\begin{sideways}CIFAR-100\end{sideways}}&
         \multirowcell{4}{~\citet{addepalli2022efficient}\\ResNet-18} & 
         None & \textbf{65.45} & 33.49 & 28.55  & 27.67\\
          &
          &Anti-adv \cite{alfarra2022combating}& 65.38 & 30.92 & 26.61 & 26.01\\
          &
          &SODEF \cite{kang2021stable}& 65.23 & 29.37 & 26.90 & 26.53\\
          &
          &RFI $(K=\text{opt. } K = 100)$& 65.41 & \textbf{34.09} & \textbf{29.18} & \textbf{27.80}\\
          %&
          %&RFI (opt. $K=100$) & 65.41 & \textbf{34.09} & \textbf{29.10} & \textbf{27.80}\\
          \cmidrule{2-7}
         %\midrule
         &
         \multirowcell{4}{~\citet{pang2022robustness}\\WideResNet-28-10} & None & \textbf{63.66} & 35.29 & 31.71 & 31.08\\
          &
          &Anti-adv \cite{alfarra2022combating}& 63.41 & 32.50 & 30.32 & 30.10\\
          &
          &SODEF \cite{kang2021stable}& 63.08 & 30.96 & 29.54 & 30.56\\
          &
          &RFI ($K=100$)& 63.01 & 36.03 & \textbf{31.95} & 31.29\\
          &
          & RFI (opt. $K=115$) & 63.10 & \textbf{36.07} & \textbf{31.95} & \textbf{31.91}\\
          \cmidrule{2-7}
         %\midrule
         &
         \multirowcell{4}{~\citet{rice2020overfitting}\\WideResNet-34-10}  & None & \textbf{53.83} & 20.83 & 20.46 & 18.95\\
          &
          &Anti-adv \cite{alfarra2022combating} & \textbf{53.83} & 20.78 &  20.06 & 18.97\\
          &
          &SODEF \cite{kang2021stable}& \textbf{53.83} & 18.50 & 19.20 & 16.92\\
          &
          &RFI ($K=100$)& 53.70 & 21.10 & 20.98 & 19.23\\
          &
          &RFI (opt. $K=150$) & 53.75 & \textbf{21.18} & \textbf{21.10} & \textbf{19.46}\\
           \cmidrule{2-7}
          %\midrule
         &
         \multirowcell{4}{~\citet{wang2023better}\\WideResNet-28-10}  & None & \textbf{72.58} & 44.04 &  39.78 & 38.83\\
          &
          &Anti-adv \cite{alfarra2022combating} & 72.57 & 42.98 &  38.10 & 34.01\\
          &
          &SODEF \cite{kang2021stable}& 72.34 & 38.10 & 36.95 & 32.29\\
          &
          &RFI ($K=100$)& 72.55 & 44.37 & 39.91 & 39.10\\
          &
          & RFI (opt. $K=115$) & 72.55 & \textbf{44.51} & \textbf{39.96} & \textbf{39.13}\\
         \bottomrule
    \end{tabular}
    }
    %\caption{(Base/Ours) Indicate the performance of adversarial training methods after adding our adaptive test time defense on APGD-CE and APGD-DLR attacks ~\cite{croce2020reliable}}
    \label{tab:benchmarkcomparison}
\end{table}


% Figure environment removed


%\textbf{Robust features are learned first.} 
%The theoretical results from Theorem~\ref{theorem:gradientdescentineigenbasis} conclude that the robust features are learned first for the random features model. 
%Using the theoretical results from our analysis and the feature definition of NTK by looking at ~\ref{eq:dynamicsntk} after substituting NTK features. 
%We compute the contribution of every feature to overall risk of the model and define the order of their risk contribution as the risk order. We observe that the risk order initially in the training follows $\lambda$ order in the spectrum and then reverses, which means initially larger risk comes from robust features and as they are learned the order changes. 
%Detailed discussion is provided in the appendix. 

