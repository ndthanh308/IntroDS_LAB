\section{Proofs of the Main Results}

In this section, we prove Theorem \ref{thm:compute-robustness}, and related results, Corollaries \ref{cor:robust_score}--\ref{cor:information} and Remark \ref{rem:thm-robust-linear}.

\subsection{Proof of Theorem \ref{thm:compute-robustness}}

\begin{proof}
%\textbf{To prove Theorem \ref{thm:compute-robustness}} and subsequent results, 
Recall that we assume 
$y = h(\mathbf{x}) + \bm{\epsilon} = \bm{\beta}^\top \phi(\mathbf{x}) + \bm{\epsilon}$, where $\bm{\epsilon} \in \mathbb{R}^C$ has independent coordinates, each satisfying $\mathbb{E}[\epsilon_c] = 0$, $\mathbb{E}[\epsilon_c^2] \leq \sigma^2$ for all $c \in \{1,\ldots,C\}$.
The features for which we wish to compute robustness are of the form $f = \bm{M}\phi$ where $\bm{M}$ is a linear map.

We are interested in robustness with respect to the $c$-th component, which is computed as
\begin{align}
s_{\mathcal{D},\bm{\beta},c}(f) &= \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top  f(\tilde{\mathbf{x}})\right] 
\nonumber
\\&= \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[y_c \bm{\beta}_c^\top  f(\mathbf{x})\right] 
+ \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)\right] .
\label{eq:robust-decomp}
\end{align}
%
We compute the first term exactly as
\begin{align*}
   \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[y_c \bm{\beta}_c^\top  f(\mathbf{x})\right] 
   &= \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ (\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c) \bm{\beta}_c^\top  f(\mathbf{x})\right] 
   \\&= \mathbb{E}_{\mathbf{x}}\left[ \bm{\beta}_c^\top \phi(\mathbf{x}) \phi(\mathbf{x})^\top \bm{M} \bm{\beta}_c\right] 
   &\text{since } \mathbb{E}[\epsilon_c] = 0
   \\&= \bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c,
   &\text{where } \Sigma = \mathbb{E}_\mathbf{x} \left[\phi(\mathbf{x})\phi(\mathbf{x})^\top\right].
\end{align*}
%
For the second term in \eqref{eq:robust-decomp}, we aim to derive a lower bound. Observe that
\begin{align*}
    y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)
    &= y_c \bm{\beta}_c^\top \bm{M} \big( \phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\big)
    \\&\geq - |y_c|\cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot \Vert \bm{M}\big(\phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\big)\Vert_\mathcal{H}
    \\&\geq - |y_c|\cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot \Vert\bm{M} \Vert_{op} \cdot \Vert \phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\Vert_\mathcal{H}.
\end{align*}
Using $L$-Lipschitzness of $\phi$, we have $\Vert \phi(\tilde{\mathbf{x}}) - \phi(\mathbf{x})\Vert_\mathcal{H} \leq L||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq L\Delta$. Hence, the second term in \eqref{eq:robust-decomp} can be bounded from below as
\begin{align*}
    \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)\right] 
    &= - \Vert\bm{M} \Vert_{op} \cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot L\Delta \cdot \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ |y_c|\right]\\
    &\geq - \Vert\bm{M} \Vert_{op} \cdot \Vert\bm{\beta}_c\Vert_{\mathcal{H}} \cdot L\Delta \cdot \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ |\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c|\right]  
\end{align*}
Finally, using Jensen's inequality, we can write
\begin{align*}
   \mathbb{E}_{\mathbf{x},\epsilon_c}\left[ |\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c|\right] \leq \sqrt{\mathbb{E}_{\mathbf{x},\epsilon_c}\left[ (\bm{\beta}_c^\top \phi(\mathbf{x}) + \epsilon_c)^2\right] }
   \leq \sqrt{\sigma^2+ \bm{\beta}_c^\top \Sigma \bm{\beta}_c}.
\end{align*}
Combining the above computation leads to 
\begin{align*}
s_{\mathcal{D},\bm{\beta},c}(f) ~\geq~ \bm{\beta}_c^\top \Sigma \bm{M}  \bm{\beta}_c - L \Delta \Vert \bm{M}\Vert_{op} \Vert \bm{\beta}_c\Vert_\mathcal{H} \sqrt{\sigma^2 + \bm{\beta}_c^\top \Sigma \bm{\beta}_c},
\end{align*}
which proves Theorem \ref{thm:compute-robustness}.
\end{proof}

%%%%%
\subsection{Proof of Remark \ref{rem:thm-robust-linear}}

\begin{proof}
The proof requires assumption of a Gaussian model, i.e., $\mathbf{x} \sim \mathcal{N}(0,\Sigma)$ and $\epsilon_c\sim \mathcal{N}(0,\sigma^2)$. Since the feature map is assumed to be linear, $\phi(\mathbf{x}) =\mathbf{x}$, it follows that $y_c = \bm{\beta}_c^\top \mathbf{x} + \epsilon_c$ is also Gaussian $y_c \sim \mathcal{N}(0,\sigma^2 + \bm{\beta}_c^\top \Sigma \bm{\beta}_c)$ and hence, $|y_c|$ is half-normal distributed.

Now recall that the first term in \eqref{eq:robust-decomp} can be computed exactly as $\bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c$. To compute the second term in \eqref{eq:robust-decomp}, note that
\begin{align*}
    \inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)
    &= \inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \bm{M} (\tilde{\mathbf{x}}- \mathbf{x})
\end{align*}
and the infimum is achieved when the difference is aligned with $\bm{M}^\top \bm{\beta}_c$, that is,  $\tilde{\mathbf{x}} = \mathbf{x} \pm \Delta \frac{\bm{M}^\top \bm{\beta}_c}{\Vert \bm{M}^\top \bm{\beta}_c \Vert_\mathcal{H}}$.
The sign depends on the sign of $y_c$, which leads to the second term in \eqref{eq:robust-decomp} compute to
\begin{align*}
    \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}}\left[\inf\limits_{||\tilde{\mathbf{x}}-\mathbf{x}||_2 \leq \Delta} y_c \bm{\beta}_c^\top \big( f(\tilde{\mathbf{x}}) - f(\mathbf{x})\big)\right]
    =  - \mathbb{E}_{\mathbf{x},\epsilon_c} \big[ |y_c|  \big] \cdot \Delta \cdot \Vert \bm{M}^\top \bm{\beta}_c \Vert_\mathcal{H}.
\end{align*}
Since $|y_c|$ is half-normal, $\mathbb{E}[|y_c|] = \sqrt{2/\pi}\sqrt{\sigma^2 + \bm{\beta}_c^\top\Sigma\bm{\beta}_c}$, while $\Vert\bm{M}^\top \bm{\beta}_c \Vert_\mathcal{H} \leq \Vert\bm{M}\Vert_{op} \Vert \bm{\beta}_c \Vert_\mathcal{H}$, with the inequality being tight when $\bm{\beta}_c$ is the eigenvector of $\bm{M}$, corresponding to the largest eigenvalue.
\end{proof}

%%%%%
\subsection{Proofs of Corollary \ref{cor:robust_score} and Corollary \ref{cor:information}}

\begin{proof}
In what follows, we restrict the linear map $\bm{M}$ as $\bm{M} = \tilde{\mathbf{U}}\tilde{\mathbf{U}}^\top = \sum_{i=1}^K \mathbf{u}_i\mathbf{u}_i^\top$ where $\tilde{\mathbf{U}} =[ \mathbf{u}_1, \ldots, \mathbf{u}_K]$ is an orthonormal matrix of basis for a $K$-dimensional subspace.
Since the operator norm $\Vert \bm{M} \Vert_{op} =1$ for projection matrix, the problem of finding the most robust subspace corresponds to maximising $\bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c = \sum_{i=1}^K \bm{\beta}_c^\top \Sigma \mathbf{u}_i\mathbf{u}_i^\top \bm{\beta}_c$. 


Note that if $(\lambda,\mathbf{u})$ is an eigenpair of $\Sigma$, then $\bm{\beta}_c^\top \Sigma \mathbf{u}\mathbf{u}^\top \bm{\beta}_c = \lambda (\bm{\beta}_c^\top \mathbf{u})^2$.
Hence, if we restrict the choice of $\mathbf{u}_1,\ldots,\mathbf{u}_K$ to the eigenvectors of $\Sigma$, the optimal projection is obtained by choosing the $K$ eigenvectors for which the robustness score $s_c(\mathbf{u}) = \lambda (\bm{\beta}_c^\top \mathbf{u})^2$ are largest. So the claim of Corollary \ref{cor:robust_score} holds only if the projections are restricted to eigenspaces of $\Sigma$.
The claim of Corollary \ref{cor:information} follows along the same line as the information of the feature $f = \bm{M}\phi$ can be computed as $\rho_{\mathcal{D},\bm{\beta},c}(f) = \bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c$. 
For the case of $\bm{M} = \tilde{\mathbf{U}}\tilde{\mathbf{U}}^\top$ where $\tilde{\mathbf{U}}$ is matrix of $K$ eigenvectors of $\Sigma$, we have $\rho_{\mathcal{D},\bm{\beta},c}(f) = \sum_{i=1}^K \lambda_i (\bm{\beta}_c^\top \mathbf{u}_i)^2$. Hence, if the search is restricted to eigenspaces of $\Sigma$, the most robust features also correspond to the most informative ones.
\end{proof}


\paragraph{Robust and informative features over all possible $K$-dimensional subspaces.}
If we consider $\bm{M} = \tilde{\mathbf{U}}\tilde{\mathbf{U}}^\top$ for any $\tilde{\mathbf{U}} =[ \mathbf{u}_1,\ldots,\mathbf{u}_K]$ with orthonormal columns, as assumed in Corollary \ref{cor:robust_score}, then 
\[\bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c = \sum_{i=1}^K \bm{\beta}_c^\top \Sigma \mathbf{u}_i\mathbf{u}_i^\top \bm{\beta}_c = \text{Trace}\left(\tilde{\mathbf{U}} \bm{\beta}_c \bm{\beta}_c^\top \Sigma \tilde{\mathbf{U}}^\top \right) = \text{Trace}\left(\tilde{\mathbf{U}} \Sigma \bm{\beta}_c \bm{\beta}_c^\top \tilde{\mathbf{U}}^\top \right),
\]
where the last equality follows from taking transpose. Hence, the resulting maximisation problem can be written as 
\begin{align}
\underset{\tilde{\mathbf{U}}}{\max} ~\bm{\beta}_c^\top \Sigma \bm{M} \bm{\beta}_c 
~\equiv~ \underset{\tilde{\mathbf{U}}}{\max} ~\text{Trace}\left(\tilde{\mathbf{U}} \bm{\beta}_c \bm{\beta}_c^\top \Sigma \tilde{\mathbf{U}}^\top \right)
~\equiv~ \underset{\tilde{\mathbf{U}}}{\max} ~\text{Trace}\left(\tilde{\mathbf{U}} \mathbf{B}_c \tilde{\mathbf{U}}^\top \right),
\label{eq:alternate-max}
\end{align}
where $\mathbf{B}_c = \frac12 (\bm{\beta}_c \bm{\beta}_c^\top \Sigma + \Sigma \bm{\beta}_c \bm{\beta}_c^\top)$.
The above trace maximisation problem corresponds to finding the $K$ dominant eigenvectors of the matrix $\mathbf{B}_c$.
This leads to an alternative to Algorithm \ref{alg:cap} for finding robust projections for the test-time defense. 
The approach comprises of computing the dominant eigenvectors $\tilde{\mathbf{U}}_c$ of the matrix $\mathbf{B}_c$ for every class component $c\in\{1,\ldots,C\}$ and defining the robust output as $\tilde{h}(\mathbf{x}) = [\bm{\beta}_1^\top \tilde{\mathbf{U}}_1\tilde{\mathbf{U}}_1^\top \phi(\mathbf{x}),\ldots,\bm{\beta}_C^\top \tilde{\mathbf{U}}_C\tilde{\mathbf{U}}_C^\top \phi(\mathbf{x})]$.
The approach would result in theoretically more robust projections, but suffers computationally since it requires $(C+1)$ eigendecompositions instead of only one eigendecomposition in Algorithm \ref{alg:cap}. 
Hence, it has $O(C)$ more one-time computation than Algorithm \ref{alg:cap}, but with identical inference time.
The conclusion of Corollary \ref{cor:information} that the most robust features, obtained from the maximisation in \eqref{eq:alternate-max}, are also the most informative features still holds in this case.


\subsection{Proof of Proposition \ref{prop:robust_top}}

\begin{proof}
Suppose that the NTK feature $f_i^{ker}$ is $L$-Lipschitz continuous in gradient of NTK with respect to $\mathbf{x}$. Then, we have 
\begin{align}
    \left\Vert \nabla_\mathbf{x}\mathbf{\Theta}(\mathbf{x}+\bm\delta, \mathbf{X}) - \nabla_\mathbf{x}\mathbf{\Theta}(\mathbf{x}, \mathbf{X}) \Vert_2 \leq L \Vert \bm\delta \right\Vert_2. \label{eq:ntk_feature_grad_lip}
\end{align}
Recall that we can write the $i$-th NTK feature as $f^{ker}_{i}(\mathbf{x}) := \lambda_i^{-1}\mathbf{\Theta}(\mathbf{x},\mathbf{X})^{\top} \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y}$. 
Bounding $\Vert f^{ker}_{i}(\mathbf{x}+\bm\delta)-f^{ker}_{i}(\mathbf{x}) \Vert_2$ by Taylor's expansion and applying \eqref{eq:ntk_feature_grad_lip} yield
\begin{align}
    \Vert f^{ker}_{i}(\mathbf{x}+\bm\delta)-f^{ker}_{i}(\mathbf{x}) \Vert_2 &\stackrel{(a)}{=} \left\Vert \lambda_i^{-1}\bm\delta^{\top} \nabla_{\mathbf{x}}\mathbf{\Theta}(\mathbf{x}, \mathbf{X})\mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} + \lambda_i^{-1}\mathbf{R} \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} \right\Vert_2  \qquad\mathbf{R} \text{ : remainder} \nonumber \\
    &\stackrel{(b)}{\leq} \left\Vert \lambda_i^{-1}\bm\delta^{\top} \nabla_{\mathbf{x}}\mathbf{\Theta}(\mathbf{x}, \mathbf{X})\mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} + \dfrac{\lambda_i^{-1}L}{2}\Vert \bm\delta \Vert_2 \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y}  \right\Vert_2 \qquad\text{from \eqref{eq:ntk_feature_grad_lip}} \nonumber \\
    &\leq \lambda_i^{-1} \left\Vert \left(\bm\delta^{\top} \nabla_{\mathbf{x}}\mathbf{\Theta}(\mathbf{x}, \mathbf{X}) + \dfrac{L}{2}\Vert \bm\delta \Vert_2 \right) \mathbf{v_i}\mathbf{v_i}^{\top}\mathbf{Y} \right\Vert_2 = \Theta \left(\dfrac{1}{\lambda_i}\right) \nonumber
\end{align}
where $(a)$ follows from the Taylor's expansion of $f^{ker}_{i}(\mathbf{x}+\bm\delta)$ where $\mathbf{R}$ is the remainder terms and $(b)$ follows from \eqref{eq:ntk_feature_grad_lip}, i.e., $\mathbf{R}\leq (L/2) \Vert \bm\delta \Vert_2$.
\end{proof}
%%%%%%%%%%%%%%%
%\section{Dynamics of robust feature learning under GAM}

%In this short analysis, we argue that if the trained model is a Generalized Additive Model (GAM), $h(\mathbf{x}) = \bm{\beta}^T \phi(x)$, the test-time defense of Algorithm \ref{alg:cap} could also be replicated through an early stopping of the training process. In other words, we argue that the components of $\bm{\beta}_c^T \phi(x)$ along the robust features---the eigen directions for which $s(\bm{u}) = \lambda (\bm{\beta}_c^\top \bm{u})^2$ are higher---are learned earlier.

%For simplicity of analysis, we consider only the learning for $c$-th components, which corresponds to the following regression problem under GAM: Given training sample $\mathcal{D}_{\text{train}} := \{(\mathbf{x_i},y_i)\}_{i=1}^n \subseteq \mathcal{X}\times \mathbb{R}$, minimize the squared loss 
%\[\underset{\bm{b}\in\mathbb{R}^p}{\text{minimize}} ~\frac1n \sum_{i=1}^n \Vert y_i - \bm{b}^\top \phi(\mathbf{x}_i) \Vert_2^2.\]
%
%The optimal solution for $\bm{b}$ for the above problem is given by $\bm{\beta}_c = (\Phi \Phi^\top)^{-1} \Phi \bm{y}$, where $\Phi = [\phi(\mathbf{x}_1), \ldots, \phi(\mathbf{x}_n)]$ and $\bm{y}=[y_1 \ldots y_n]^\top$.
%Furthermore, if the above optimisation is solved using gradient descent with learning rate $\eta >0$ and initialisation $\bm{b}^{(0)} = 0$, the parameters $\bm{b}^{(t)}$ are learned over the iterations as
%\[\bm{b}^{(t)} ~=~ \left( I - \eta \Phi\Phi^\top \right)\bm{b}^{(t-1)} + \eta \Phi \bm{y} ~=~ \eta \sum_{k=0}^{t-1} \left( I - \eta \Phi\Phi^\top \right)^k \Phi \bm{y} ~=~ \eta \sum_{k=0}^{t-1} \left( I - \eta \Phi\Phi^\top \right)^k \cdot \Phi\Phi^\top \bm{\beta}_c,\]
%with $\bm{b}^{(t)} \to \bm{\beta}_c =(\Phi \Phi^\top)^{-1} \Phi \bm{y}$ as $t\to\infty$. Suppose the eigen decomposition $\Sigma_{\text{train}} = \Phi\Phi^\top$ is given by  $\Sigma_{\text{train}} = \sum\limits_{i=1}^p \lambda_i \bm{u}_i\bm{u}_i^\top$.

\section{Experimental Details}
\subsection{Details of Benchmarking baseline SOTA methods}
We perform Bench-marking of our adaptive test time defense on multiple SOTA methods that are used achieving adversarial robustness in the model. For our analysis of proposed adaptive test time defense on CIFAR-10 in table~\ref{tab:basicexp} we used PGD~\cite{madry2018towards}, Interpolated Adversarial Training~\cite{lamb2019interpolated}, Carlini-Warger Loss~\cite{carlini2017towards} and Robust CIFAR-10~\cite{featuresnotbugs} to adversarially train the baseline model. In general for PGD, IAT and CW Attack the adversarial training works as generating an adversarial example using the underlying attack and the objective to to minimize loss on these adversarial examples. PGD attack uses gradient descent to iteratively maximize the classification loss with respect to the input while projecting the perturbed example into the norm ball defined for the attack. IAT uses a joint objective that minimizes the classification loss of perturbed examples generated from PGD or any other attack along with classification loss on clean data with MixUP~\cite{mixup}. We use Robust CIFAR-10 proposed in ~\cite{featuresnotbugs}, although is not an adversarial training method but rather the final dataset from a procedure to only retain robust features in the dataset. \citet{featuresnotbugs} disentangle the robust and non robust features by creating a one to one mapping of an input to its robustified image. From an adversarially pretrained backbone (ResNet-18 using PGD $\ell_2-$norm and $\epsilon=0.25$) linear layer features are extracted for the natural image and also from a noise input. Then by minimizing the distance between these two representations in the input space over the noise, a image which only retains robust features of the original input is obtained. \\~\\
For training using all these baseline adversarial training methods we set the batch size as 128. We use SGD with momentum as the optimizer where we set the momentum to be 0.1, we also set the weight decay to 0.0002. We run our training for 200 epochs and set the learning rate schedule as starting with 0.1 for first 100 epochs and then we reduce the learning rate by 90 percent every 50 epochs.\\~\\
For table~\ref{tab:benchmarkcomparison} we benchmark our adaptive test time defense on multiple recent state-of-the-art methods for both CIFAR-10 and CIFAR100 datasets. For all our baseline methods we obtain the model weights from RobustBench~\cite{croce2020reliable}. We then update the weights of all underlying models in their last linear layer using our adaptive test time defense method and benchmark the updated model with RobustBench. We provide our implementation of benchmarking experiments along with supplementary material.