\section{Introduction}
Despite the phenomenal success of deep learning in several challenging tasks, they are prone to vulnerabilities such as the addition of carefully crafted small imperceptible perturbations to the input data known as adversarial examples \citep{szegedy2013, goodfellow2014explaining}. 
While adversarial examples are semantically similar to the input data, they cause the networks to make wrong predictions with high confidence. 
The primary focus of the community in building models that are robust to adversarial examples is through modified training procedures.
One of the most popular and promising approaches is adversarial training \citep{madry2018towards}, which minimizes the maximum loss on the perturbed input samples. % or directly maximizes the margin of all the samples in the input space \citep{}.
Extensive empirical and theoretical studies on the robustness of deep neural networks using adversarial training reveals that adversarial examples are inevitable \citep{featuresnotbugs,athalye2018synthesizing,shafahiadversarial,tsipras2018robustness} and developing robust deployable models with safety guarantees require a huge amount of data and model complexity \citep{nie2022diffusion,wang2023better,carmon2019unlabeled}. 
For instance, the current state-of-the-art methods ~\citep{wang2023better, gowalimproving, rebuffi2021fixing} use additional one million to 100 million generated or synthetic data along with the original 50000 training samples of CIFAR-10 and CIFAR-100. Although the improvement in robust performance is convincing with this approach, there is an evident tradeoff with huge computational costs both in terms of data and model. %This shows that despite recent advancements in the design of robust models ~\cite{rebuffi2021fixing,wang2019global,madry2018towards} and in the understanding of the existence of adversarial attacks ~\cite{featuresnotbugs,athalye2018synthesizing}, the challenge of designing adversarially robust methods which work on several benchmarks has not been achieved yet. One of the drawbacks of static defenses against adversarial robustness is that they have no scope to alter the input or update their own parameters on the input at test time given additional knowledge of the input test sample.

%Adversarial training methods are only suitable for training a robust model from scratch, but not for making an existing trained model robust to adversarial attacks. 
While the deep learning community has focused on achieving robustness through different training paradigms such as adversarial training, little attention has been on improving the robustness of pre-trained models during inference. 
%Besides improvement to the training procedure to achieve robust models, A
\emph{Adaptive test-time defenses} refer to the class of methods that improve the robustness of any trained model at test time. 
This is typically achieved through two main strategies, modifying the test samples before making prediction \citep{alfarra2022combating, wu2021attacking} or updating the model parameters by adding network components like activations or new layers \citep{kang2021stable, chen2021towards}. 
%While \citet{} developed hand-crafted methods for processing the input, \citet{}  suggests using generative models to alter the inputs. Two, the model parameters \citep{}.   
%This, however, significantly increases the test time due to the additional computation or data processing at inference. 
However, inference becomes more computationally demanding because of the additional computation or data processing at test time.
In fact, current state-of-the-art adaptive test-time defenses often require $40\times$ to $400\times$ additional computation at inference \citep{wu2021attacking, hwang2023aid, mao2021adversarial, shionline}, while not necessarily improving the robustness of the underlying model~\citep{croce2022evaluating}.
Thus, efficiently improving the adversarial robustness of the trained models at test time without additional data or model cost remains an unresolved problem.

%A major criticism that casts doubts on the potential of adaptive test-time defense methods is the increase in inference time due to the modifications to the inputs and/or model parameters. 

% Figure environment removed

\textbf{Our contribution.} In this work, we develop a novel adaptive test-time defense strategy with the \emph{same inference cost as the underlying model} and no additional data or model complexity. In Section~\ref{sec:robust-non-robust}, we define robust features, inspired by \citet{athalye2018synthesizing, featuresnotbugs}, and subsequently describe the proposed method in Section~\ref{sec:our-algorithm} that relies on the idea of retaining the most robust features of the trained model. Importantly, our method is easy to integrate with any existing training paradigm.
We provide a theoretical justification for our method by analyzing the robustness of features in generalized additive model setting (Corollary~\ref{cor:robust_score}) and extend it to large width neural networks using the Neural Tangent Kernels (NTKs) equivalence~\citep{jacotntk, arora2019exact}.
%and computing the robustness of NTK features~\citep{tsilivis2022can} (see Theorem \ref{prop:toparerobustanduseful}).
An interesting by-product of our analysis is a proof of the fact that robust features are learned first during standard training and that the vulnerability to adversarial examples arises from the presence of non-robust features that are learned later in the training process. This phenomenon has been observed empirically for NTK features~\citep{tsilivis2022can} without theoretical proof. As a supplementary analysis, we prove it for NTK features (Section~\ref{sec:ntk}) and the dynamics in appendix. 
We conduct extensive experiments in Section~\ref{sec:exp} using different architectures such as ResNet-18, ResNet-50, WideResNet-28-10 and WideResNet-34-10 on CIFAR-10 and CIFAR-100 where our method consistently outperforms the robustness of the base models, as well as other adaptive test-time defenses.
Thus, we provide the first theoretically guided method that does inference in $1\times$ time as the base model, overcoming the major criticism of the adaptive test-time defenses.
%
%In recent years, deep neural networks have achieved state-of-the-art results on several tasks in computer vision, natural language processing, and other areas outside of computer science. Despite their success, neural networks also show numerous vulnerabilities. One of such properties of neural networks is the existence of adversarial examples  ~\citep{szegedy2013,goodfellow2014explaining}. Adversarial examples, which are semantically similar inputs crafted to cause a model to produce incorrect output, have been widely studied in the context of deep neural networks (DNNs). These examples demonstrate that DNNs, despite their impressive performance on several tasks, do not necessarily emulate human-like decision-making and highlight the need for robust AI models that can resist such malicious inputs. This vulnerability of DNNs to adversarial examples has become an important area of research in machine learning, with the goal of developing models that are more robust to these types of attacks and explanations for their existence. For example, Linear operating mode of neural nets ~\cite{goodfellow2014explaining}, the curse of dimensionality carried by the input space ~\cite{goodfellow2014explaining,simon2019first}, insufficient model capacity ~\cite{tsipras2018robustness,nakkiran2019adversarial} or spurious correlations found in common datasets~\cite{featuresnotbugs}.
%\\~\\
%Despite recent advancements in the design of robust models ~\cite{rebuffi2021fixing,wang2019global,madry2018towards} and in the understanding of the existence of adversarial attacks ~\cite{featuresnotbugs,athalye2018synthesizing}, the challenge of designing adversarially robust methods which work on several benchmarks has not been achieved yet. Along with the development of more robust training methods, adaptive test time defense is also a way to improve the robustness of a method at the test time. Adaptive test time defense strategies usually try two different strategies for improving the robustness of the model at test time, input purification~\cite{alfarra2022combating,wu2021attacking} and model adaptation~\cite{chen2021towards,kang2021stable}. However, current state-of-the-art adaptive test time defenses suffer from a big challenge of additional computational costs at inference and also may not always improve the robustness of underlying model~\cite{croce2022evaluating}. 
%Given these challenges with the existing adaptive test time defense of adversarial attacks, we propose a novel method for adaptive test time defense that has the same inference cost as the underlying model. 
%Our method abstracts every network as a feature extractor and a linear layer that consists of class prototypes. We compute the covariance of the features obtained for the training examples from the feature extractor. We then define the usefulness measure over the spectrum of the covariance matrix in the context of each class prototype. Then for each class prototype, we retain only the top most eigenvectors with respect to the usefulness measure. Our method doesn't result in any increase in the inference time because the selected useful/robust eigenbasis can be used to simply transform the linear layer weights into the eigenbasis resulting in exactly the same number of parameters in the network at inference time. We empirically evaluate our proposed method on CIFAR10 and CIFAR-100 under different attacks and defenses. 
%\\~\\
%We also theoretically justify our choice of the usefulness metric, which we show is indicative of the usefulness and robustness of a feature~\cite{athalye2018synthesizing, featuresnotbugs}. Using analytical tools from recent theories connecting neural networks and kernels, specifically the Neural Tangent Kernel (NTK) ~\cite{jacotntk, lee2019wide}, we define NTK features~\cite{tsilivis2022can} in the eigenspectrum of the NTK gram matrix. NTK features empirically show robustness at the top of their specturm~\cite{tsilivis2022can}, and our results provide a justification for the same. We also interpret these NTK features in a network for a setting where only the last layer is fine-tuned. Thus the usefulness metric used in our method corresponds to the usefulness of these NTK features. And by only retaining features corresponding to the highest usefulness metric at the inference we effectively only retain the top NTK features which are both useful and robust. Thus allowing us to improve the robustness of the underlying model at test time. Although NTK features are defined at the end of training since our analysis allows us to obtain the dynamics of every NTK feature during training, our results also indicate that robust features are learned first during standard training and that the vulnerability to adversarial examples arises from the presence of non-robust features that are learned later in the training process. 
%In summary, the main contributions of our work are
%\begin{enumerate}
%    \item We propose a novel adaptive test-time defense algorithm against adversarial attacks in Section~\ref{sec:method} which takes the same inference time as the underlying model with no additional data or model parameters.   
%    \item In Section~\ref{sec:theory}, we support our approach with theoretical analysis through NTK features and show that the most useful and robust features lie at the top of NTK spectrum for a network. 
%    \item We conduct extensive experiments on different state-of-the-art methods in Section~\ref{sec:exp} and compare our method with other adaptive test-time defenses. The results show impressive improvements over the base models and outperforms the existing adaptive defenses. To highlight, our method achieves *1\%* improvement over the current state-of-the-art method~\citep{wang2023better}. 
%\end{enumerate}

\textbf{Illustration of our method (see  Figure~\ref{fig:method_overview}).}
The proposed method abstracts any deep neural network as a feature extractor $\phi(\mathbf{x})$ and a linear layer $\bm\beta^\top \phi(\mathbf{x})$ that consists of class prototypes. We compute the covariance of the features $\Sigma_{\textup{train}}$ obtained from the training examples of the feature extractor. We define a robustness measure for the eigenvectors of $\Sigma_{\textup{train}}$ as $s_c(\mathbf{u_k})$ and for each class prototype, we retain only the top most eigenvectors with respect to the robustness measure. 
%We empirically evaluate our proposed method on CIFAR10 and CIFAR-100 under different attacks and defenses. 
%
%%theory 
This choice of the robustness metric as well as the principle of sorting the eigenvector are mathematically justified through Corollary~\ref{cor:robust_score}, where we consider generalized additive models and compute the robustness score of features (Defnition~\ref{def:robust}) showing that the top eigenvectors of the feature matrix are more robust.
%use a Neural Tangent Kernel (NTK) based analysis to identify the eigenfunctions of the NTK gram matrix~\citep{tsilivis2022can} that correspond to the most robust (and also the most useful) NTK features.
For a finite-width network, the above theoretical argument essentially corresponds to projecting the weights of only the last layer $\bm\beta$ onto the space spanned by the most robust features $\tilde{\bm\beta} = \tilde{\mathbf{U}}\tilde{\mathbf{U}}^\top\bm\beta$,
%Thus the usefulness metric used in our method corresponds to the usefulness of these NTK features. And by only retaining features corresponding to the highest usefulness metric at the inference we effectively only retain the top NTK features which are both useful and robust. 
thereby improving the robustness of the underlying model at test time. 
%Although NTK features are defined at the end of training since our analysis allows us to obtain the dynamics of every NTK feature during training, our results also indicate that robust features are learned first during standard training and that the vulnerability to adversarial examples arises from the presence of non-robust features that are learned later in the training process. 
Our method does not increase the inference time because the selected robust eigenbasis can be used to simply transform the linear layer weights into the eigenbasis resulting in exactly the same number of parameters in the network at inference time. 

%In the rest of the paper, we detail the related works in Section~\ref{sec:related_works}, describe our adaptive test-time defense algorithm in Section~\ref{sec:method}, followed by the theoretical analysis in Section~\ref{sec:theory} with experimental results in Section~\ref{sec:exp}, finally conclude with discussions in Section~\ref{sec:conclusion}.