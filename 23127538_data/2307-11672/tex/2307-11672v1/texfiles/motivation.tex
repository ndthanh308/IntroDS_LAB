\section{Theoretical Justification Using Neural Tangent Kernel}
\label{sec:theory}

In this section we provide a theoretical justification for achieving robustness improvements by projecting on to the top eigenspace of the features $\phi$ in Algorithm \ref{alg:cap}. Basically, we show that the top eigenspace of the feature covariance matrix $\Sigma_{\text{train}}$ is both robust and useful.

\subsection{Neural Tangent Kernel Features}
\citet{jacotntk} shows that the behavior and generalization properties of randomly initialized infinite width neural networks trained by gradient descent with infinitesimally small learning rate is equivalent to a deterministic kernel machine.
%Neural Tangent Kernel (NTK) is a theoretical framework that allows us to understand the dynamics of learning in neural networks~\cite {jacotntk}~\cite{lee2019wide}. 
The kernel emerges as a result of the linearization of the network (linear in its parameters) which holds very well in the infinite width limit for linear output networks \citep{liu2020linearity}.
In this case, the NTK between $\mathbf{x_i}$ and $\mathbf{x_j}$ for a network that outputs $f(\mathbf{w},\mathbf{x})$ at data point $\mathbf{x} \in \mathbb{R}^d$ parameterized by $\mathbf{w} \in \mathbb{R}^p$ is defined by the gradient of the network with respect to $\mathbf{w}$ as
\begin{equation}
     \mathbf{\Theta}(\mathbf{x_i},\mathbf{x_j}) = \mathbb{E}_{\mathbf{w}\sim \mathcal{N}(0,\mathbf{I}_p)}[\nabla_\mathbf{w} f(\mathbf{w},\mathbf{x_i})^T\nabla_\mathbf{w} f(\mathbf{w},\mathbf{x_j})].
\end{equation}
Moreover, \citet{jacotntk, lee2019wide, arora2019exact} show that the kernel remains constant during training.
Consequently, the training dynamics of the infinitely wide network for least squares loss is equivalent to kernel regression with NTK as the kernel. Hence, at time $t$, the function output on data $\mathbf{x}$ is
\begin{equation}
    f(\mathbf{x})_{t}= \mathbf{\Theta}(\mathbf{x},\mathbf{X})^{T}\mathbf{\Theta}(\mathbf{X},\mathbf{X})^{-1}(\mathbf{I}_n-e^{-\gamma t\mathbf{\Theta}(\mathbf{X},\mathbf{X})})\mathbf{Y}
\label{eq:dynamicsntk}
\end{equation}
where $\mathbf{X} \in \mathbb{R}^{n \times d}$ and $\mathbf{Y} \in \mathbb{R}^{n\times C}$ denote the entire training set $\{\mathbf{x_i},y_i\}_{i=1}^n$, and the NTK matrix $\mathbf{\Theta}(\mathbf{X}, \mathbf{X}) \in \mathbb{R}^{n\times n}$ in which $\mathbf{\Theta}(\mathbf{X}, \mathbf{X})_{ij} = \mathbf{\Theta}(\mathbf{x_i}, \mathbf{x_j})$ and similarly for $\mathbf{\Theta}(\mathbf{x}, \mathbf{X}) \in \mathbb{R}^n$. $\mathbf{I}_n$ and $e$ represent the identity matrix of size $n$ and matrix exponential, respectively.
%as the kernel function that characterizes the infinitesimal change in the output of a neural network as a function of the infinitesimal change in its parameters. In other words, it describes how the output of a neural network changes as its parameters are varied.
%In the limit of infinite width, for functions obtained from neural networks $f(w,x)$, one can reasonably approximate them using a linear function.
% \begin{equation}
%     f^{lin}(w,x) = f(w_0,x) + \nabla_w f(w_0,x) (w-w_0)
% \end{equation}
%In the above equation, $f^{lin}(w,x)\approx f(w,x)$ using first-order Taylor expansion. The above approximation is accurate for wide neural networks since during training change in parameters is infinitesimal with an increase in the width of the network, i.e. $w-w_0=O(\frac{1}{\sqrt{n}})$. Here the function is linear in parameters and $\nabla_w f(w_0,x)$ are random features equivalent to the random features regression model. Thus we can define an equivalent tangent kernel that can determine the dynamics of training in infinite width limit as
% \begin{equation}
%     \mathcal{O}(x_i,x_j) = \mathbb{E}_{w\sim p(w)}[\nabla_w f(w,x_i)^T\nabla_w f(w,x_j)]
% \end{equation}
The NTK can be computed analytically for a wide range of neural network architectures, including fully-connected and convolutional networks \citep{arora2019exact, sabanayagam2022representation}. %The NTK of a neural network can be computed at any point during the training process, including the initial point, when the weights of the network are initialized randomly.


%\subsection{NTK Features}
Recent studies have used the NTK and its eigenspectrum to examine various aspects of neural network approximation and generalization ~\cite{tsilivis2022can,arora2019generalization,basri2020frequency,bietti2019inductive,ronen2019convergence}. 
We build the motivation for our method through the NTK features defined in \citet{tsilivis2022can}, which uses the eigendecomposition of $\mathbf{\Theta}(\mathbf{X},\mathbf{X}) = \sum_{i=1}^{n}\lambda_i \mathbf{v_i}\mathbf{v_i}^{T}$ then at $t \to \infty$,
\begin{equation}
    f(\mathbf{x})_{\infty}= \mathbf{\Theta}(\mathbf{x},\mathbf{X})^{T}\mathbf{\Theta}(\mathbf{X},\mathbf{X})^{-1}\mathbf{Y} = \sum_{i=1}^{n}\lambda_i^{-1}\mathbf{\Theta}(\mathbf{x},\mathbf{X})^{T} \mathbf{v_i}\mathbf{v_i}^{T}\mathbf{Y} := \sum_{i=1}^{n}f^{ker}_{(i)}(\mathbf{x}) 
\label{eq:ntk_features}
\end{equation}
where $f^{ker}_{(i)}(\mathbf{x}) \in \mathbb{R}^C$ is the $i$-th NTK feature of $\mathbf{x}$. 
Additionally, \citet{tsilivis2022can} observe empirically that the top eigenvalue features of the NTK spectrum-induced features $f^{ker}(.)$ are more robust. 
Before providing justification for our algorithm (RFI), we first establish that the NTK features are indeed more robust at the top of the spectrum.
%We know the time evolution of predictions using NTK are given by:
% \begin{equation}
%     f(x)_{t}= \mathcal{O}(x,\mathcal{X})^{T}\mathcal{O}(\mathcal{X},\mathcal{X})^{-1}(I-e^{-\gamma t\mathcal{O}(\mathcal{X},\mathcal{X})})\mathcal{Y}
% \end{equation}
%Now, considering the predictions at $t->\infty$ we obtain $f(x)_{\infty}= \mathcal{O}(x,\mathcal{X})^{T}\mathcal{O}(\mathcal{X},\mathcal{X})^{-1}\mathcal{Y}$
%We know that kernel can also be written in terms of its eigendecomposition since it is a Gram matrix $\mathcal{O}(\mathcal{X},\mathcal{X}) = \sum_{i=1}^{n}\lambda_i v_iv_i^{T}$. 
%Thus NTK features from~\cite{tsilivis2022can} can be written using the Eigen-decomposition of the gram matrix at the end of the training.
% \begin{equation}
%   f(x)_{\infty}= \sum_{i=1}^{n}\lambda_i^{-1}\mathcal{O}(x,\mathcal{X})^{T} v_iv_i^{T}\mathcal{Y} = \sum_{i=1}^{n}f^{ker}_{(i)}(x)  
% \end{equation}
%Here, each $f^{ker}_{(i)}$ can be seen as a unique feature captured from the (training) data. Note that these functions map the input to the output space and fulfill the definition of features. 

\begin{prop}[Robustness lies at the top.]
\label{prop:robust_top}
    Consider a Lipschitz continuous feature $f^{ker}_{i}$ and an adversarial perturbation $\bm{\delta}$ for input $\mathbf{x}$ such that $||\bm{\delta}||_p \leq \Delta$. Then,
    %of max $\Delta$ radius around input $\mathbf{x}$ i.e. $\delta\in \mathcal{B}(\Delta)(\mathbf{x})$ under $\ell_p$-norm. We can say that
    $||f^{ker}_{i}(\mathbf{x}+\bm\delta)- f^{ker}_{i}(\mathbf{x})||_2\leq\Theta(\frac{1}{\lambda_i})$.
% \begin{equation}
% \begin{aligned}
%     ||f^{ker}_{i}(x+\delta)-f^{ker}_{i}(x)||_2 
%     \leq \frac{c|| \delta^T\nabla_x\mathcal{O}(x,\mathcal{X}) v_iv_i^T\mathcal{Y}||_2}{\lambda_i}
%     \leq \frac{c||\delta||_2||\nabla_x\mathcal{O}(x,\mathcal{X})||_2||\mathcal{Y}||_2}{\lambda_i}
%     =\Theta(\frac{1}{\lambda_i})
% \end{aligned}
% \label{eq:ntknorm}
% \end{equation} 
\end{prop}
Proposition~\ref{prop:robust_top} shows that the NTK features corresponding to the higher eigenvalues are more robust to adversarial perturbations, and hence robustness lies at the top. 
This complements and strengthens the empirical observation in \citet{tsilivis2022can}. The proof is provided in appendix.
While the consequence of this result with regards to the network is not direct, we establish the connection in the following section.

%Robustness lies on top and NTK features with larger eigenvalues are more robust. Consider a Lipschitz continuous feature $f^{ker}_{i}$ and an adversarial perturbation $\delta$ of max $\Delta$ radius around input $x$ i.e. $\delta\in \mathcal{B}_(\Delta)(x)$ under $\ell_p$-norm. We can say that $||f^{ker}_{i}(x+\delta)- f^{ker}_{i}(x)||_2\leq\Theta(\frac{1}{\lambda_i})$.
% \begin{equation}
% \begin{aligned}
%     ||f^{ker}_{i}(x+\delta)-f^{ker}_{i}(x)||_2 
%     \leq \frac{c|| \delta^T\nabla_x\mathcal{O}(x,\mathcal{X}) v_iv_i^T\mathcal{Y}||_2}{\lambda_i}
%     \leq \frac{c||\delta||_2||\nabla_x\mathcal{O}(x,\mathcal{X})||_2||\mathcal{Y}||_2}{\lambda_i}
%     =\Theta(\frac{1}{\lambda_i})
% \end{aligned}
% \label{eq:ntknorm}
% \end{equation} 
%Given justification in Eq~\ref{eq:ntknorm}, robust features have larger eigenvalues. This agrees with the empirical observations in ~\cite{tsilivis2022can} that robustness in a network lies at the top of the spectrum. 

\subsection{Correspondence of NTK Features to Network Features}
%\subsection{What do these NTK features represent in a network?}
\label{sectionrepresents}
In order to understand the correspondence of NTK features to network features, we consider a random features model which is equivalent to a pre-trained network with only fine-tuning on the final layer. 
%This assumption is necessary for our analysis because the kernel feature of NTK is equivalent to the gradient of parameters which reduces to hidden layer output in this setting. 
We define the network (random features model), which closely resembles the formulation in our Algorithm~\ref{alg:cap}, as $f(\mathbf{x})=\bm{\beta}^T\phi(\mathbf{x})$ where $\phi : \mathbb{R}^d \to \mathbb{R}^p$ is the pretrained network component and $\bm{\beta} \in \mathbb{R}^p$ is the trainable parameter. The underlying label for data $\mathbf{x}$ is assumed to be generated as $y=\tilde{\bm\beta}^T\phi(\mathbf{x})+\epsilon$ where $\epsilon\sim \mathcal{N}(0,\sigma^2)$ is the noise and $\tilde{\bm\beta}$ are the true weights. Let the training data be $\mathbf{X} \times \mathbf{Y} \in \mathbb{R}^{n \times d} \times \mathbb{R}^n$ where $\mathbf{Y}$ is generated as per the above rule. We learn parameter $\bm\beta$ by minimizing the least squares error as 
%Then, the objective can be written as:
\begin{equation}
    \bm\beta = \arg\min_{\bm\beta}\frac{1}{2n}\left\Vert\mathbf{Y}-\bm\beta^T\phi(\mathbf{X})\right\Vert_2^2
\label{eq:randomfeatureobj}
\end{equation}
Where $\phi(\mathbf{X})\in \mathbb{R}^{p\times n}$ and $\phi(\mathbf{X})=[\phi(\mathbf{x_1}),\ldots, \phi(\mathbf{x_n})]$. The sample covariance of the hidden layer output is represented as $\Sigma=\frac{1}{n}\phi(\mathbf{X})\phi(\mathbf{X})^T=\sum_{i=1}^p\lambda_i\mathbf{u_i}\mathbf{u_i}^T$. Where $\lambda_i$ and $\mathbf{u_i}$ are the eigenvalues and eigenvectors of $\Sigma$, respectively. Similar to the NTK features, 
%at $t \to \infty$, 
we define the network features 
%at the end of the training 
based on the eigendecomposition of the sample covariance of the hidden layer $\Sigma$ as
$f(\mathbf{x}) = \Tilde{\bm\beta}^T \phi(\mathbf{x}) = \phi(\mathbf{x})^T\tilde{\bm\beta}=\sum_{i=1}^p\phi(\mathbf{x})^T\mathbf{u_i}\mathbf{u_i}^T\tilde{\bm\beta}=\sum_{i=1}^pf^{net}_i(\mathbf{x})$.

% \begin{equation}
%     f(x)_\infty = \phi(x)^T\beta_\infty = \phi(x)^T\tilde{\beta}=\sum_{j=1}^p\phi(x)^Tu_ju_j^T\tilde{\beta}=\sum_{j=1}^pf^{net}_j(x)
% \end{equation}
Under the infinite width setting of the network $f(\mathbf{x})$, $p\to \infty$, the NTK for $f(\mathbf{x})$ is simply $\Theta(\mathbf{X}, \mathbf{X}) = \frac{1}{n}\phi(\mathbf{X})^T \phi(\mathbf{X}) \in \mathbb{R}^{n \times n}$, since $f(\mathbf{x})$ is linear in $\bm\beta$. Let the NTK features of the network be $f^{ker}_{(i)}(\mathbf{x}) = \lambda_i^{-1}\mathbf{\Theta}(\mathbf{x},\mathbf{X})^{T} \mathbf{v_i}\mathbf{v_i}^{T}\mathbf{Y}$ as per \eqref{eq:ntk_features}. Note that $\mathbf{u_i}$ and $\mathbf{v_i}$ are the right and left singular vectors of $\phi(\mathbf{X})$.
With these definitions and under infinite width assumption $p\to\infty$, we show the correspondence between the network features $f^{net}_{(i)}(\mathbf{x})$ and the NTK features $f^{ker}_{(i)}(\mathbf{x})$ of the data $\mathbf{x}$ in the eigenbasis. 
\begin{prop}[Correspondence of the Network and the NTK features.]
    The network features $f^{net}_{(i)}(\mathbf{x})$ have an exact correspondence with the NTK features $f^{ker}_{(i)}(\mathbf{x})$ of the data $\mathbf{x}$ in the eigenbasis:
    $\mathbb{E}_{\epsilon\sim \mathcal{N}(0,\sigma^2)}[f^{ker}_i(\mathbf{x})]=f^{net}_i(\mathbf{x})$.
\label{prop:ntkrfsame}
\end{prop}
We provide the proof of Proportion~\ref{prop:ntkrfsame} in appendix. With this feature correspondence and Proposition~\ref{prop:robust_top}, we derive the usefulness and robustness of the network features in the following.

%\subsection{Why consider Features in Eigenbasis?}
\subsection{Usefulness and Robustness of the Network Features}
We derive the dynamics of the usefulness and robustness of the network features by studying the gradient descent dynamics of 
%Considering the random features model described in Section~\ref{sectionrepresents} and 
the objective described in ~\eqref{eq:randomfeatureobj} with finite step size $\eta$. 
We observe that the update of weights in the components along the eigenbasis are independent of one other, which allows us to analytically understand the dynamics of these features. 
\begin{theorem}
The evolution of weight vector $\bm\beta$ at time $t$ can be written as
$\beta_t = \sum_{j=1}^p(1-(1-\eta\lambda_j^2)^t) x^Tu_ju_j^T\tilde{\beta}$.
Thus, the prediction at time $t$, $f(x)_t$ can be written as $f(x)_t= \sum_{j=1}^{p}f_j^{net}(x)(1-(1-\eta\lambda_j^2)^t)$.
\label{theorem:gradientdescentineigenbasis}
\end{theorem}
The expression for prediction at time $t$ in Theorem ~\ref{theorem:gradientdescentineigenbasis} allows us to also understand the dynamics of each feature in eigenbasis where we can decompose the $f(x)_t= \sum_{j=0}^{p}f_j^{net}(x)(1-(1-\eta\lambda_j^2)^t)=\sum_{j=1}^p{f_j^{net}(x)}_t$.
We can also compute the $\rho$-usefulness and $\gamma$-robustness of ${f_j^{net}(x)}_t$ which shows that the top useful features are also robust. 
\begin{theorem}
Considering the feature ${f_j^{net}(x)}_t$, the $\rho$-usefulness and $\gamma$-robustness of features is
%given as follows
\begin{equation*}
    \begin{aligned}
    \mathbb{E}_{(\mathbf{x},y)}[yf_j^{net}(\mathbf{x})_t]&=(\tilde{\bm\beta}_u^j)^2\lambda_j^2(1-(1-\eta\lambda_j^2)^t):=\rho_j\\
    \mathbb{E}_{(x,y)}[\inf_{||\bm\delta||_p \leq \Delta}yf_j^{net}(\mathbf{x}+\bm\delta)_t]&=(1-(1-\eta\lambda_j^2)^t)[({\tilde{\beta}}_u^j)^2\lambda_j^2-\Delta|\tilde{\bm\beta}_u^j|\sqrt{\frac{2}{\pi}(\sigma^2+\Sigma(\tilde{\bm\beta_u^j})^2\lambda_j^2)}
    \end{aligned}
\end{equation*}
\label{prop:toparerobustanduseful}
\end{theorem}

