% !TEX root = jmlr_version.tex



The following lemma allows us to establish the behavior of eigenvectors.
\begin{lemma}
\label{lemma: eigenvector power expansion}
  Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} it holds that
  \begin{align*}
    \adjacencyEigenvectors_{ik} & = \probEigenvectors_{ik} + \frac{\ev_i^{\T} \displaceMatrix \uv_k}{t_k} + \frac{\ev_i^{\T} \displaceMatrix^2 \uv_k}{t_k^2} - \frac{3}{2} \cdot \probEigenvectors_{ik} \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} \\
    &  \quad \, + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot  \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k + O_{\ell} \left(\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right).
  \end{align*}
\end{lemma}

\begin{proof}
  For further derivations, we need to introduce some notations. All necessary variables are defined in Table~\ref{tab: expansion notation}. Then, we define $t_k$ as a solution of 
  \begin{align}
  \label{eq: t_k definition}
    1 + \lambda_k(\probMatrix) 
    \left\{ 
      \resolvent(\uv_k, \uv_k, z) 
      - 
      \resolvent(\uv_k, \probEigenvectors_{-k}, z) 
      [
        \probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, z)
      ]^{-1}
      \resolvent(\probEigenvectors_{-k}, \uv_k, z)
    \right\} = 0
  \end{align}
  on the closed interval $[a_k, b_k]$, where
  \begin{align*}
    a_k
    = 
    \begin{cases}
      \lambda_k(\probMatrix)/(1 + 2^{-1}c_0), & \lambda_k(\probMatrix) > 0, \\
      (1 + 2^{-1} c_0) \lambda_k(\probMatrix), & \lambda_k(\probMatrix) < 0,
      \end{cases}
      \text{ and }
      b_k = \begin{cases}
      (1 + 2^{-1} c_0) \lambda_k(\probMatrix), & \lambda_k(\probMatrix) > 0, \\
      \lambda_k(\probMatrix) / (1 + 2^{-1} c_0), & \lambda_k(\probMatrix) < 0, 
    \end{cases}
  \end{align*}
  and $c_0$ is defined in Condition~\ref{cond: eigenvalues divergency}.

  Throughout this proof, a lot of auxiliary variables appear. For them, we exploit asymptotics established in Lemma~\ref{lemma: auxiliary variables expansion}. Lemma~\ref{lemma: log estimate vector difference} guarantees that $\xv^{\T} \displaceMatrix \yv = O_\ell(\sqrt{\sparsityParam \log \nsize})$ whenever unit $\xv$ or $\yv$ is $\uv_k$ because of Condition~\ref{cond: sparsity param bound} ($\sparsityParam \gg \nsize^{-1/3}$) and Lemma~\ref{lemma: eigenvectors max norm} ($\Vert \uv_k \Vert_{\infty} = O(\nsize^{-1/2}$)). Thus, any term of the form $\vv^{\T} \displaceMatrix \uv_k$ becomes
  \begin{align*}
    \vv^{\T} \displaceMatrix \uv_k = O_\ell(\sqrt{\sparsityParam \log \nsize}) \cdot \Vert \vv \Vert_2.
  \end{align*}
  %
  First, from Lemma~\ref{lemma: fan asymptotic expansion w/o sigma},
  \begin{align*}
    \uv_k^{\T} \estimator[\uv]_k \estimator[\uv]_k^{\T} \uv_k & = \meanFactor_{\uv_k, k, t_k} \meanFactor_{\uv_k, k, t_k} \pFactor_{k, t_k} +  \tr \left[\displaceMatrix \jMatrix_{\uv_k, \uv_k, k, t_k} - (\displaceMatrix^2 - \EE \displaceMatrix^2) \lMatrix_{\uv_k, \uv_k, k, t_k} \right] \\ 
    & + \tr(\displaceMatrix \uv_k \uv_k^{\T}) \tr (\displaceMatrix \qMatrix_{\uv_k, \uv_k, k, t_k})
    + O_{\prec} \left(\frac{1}{\nsize^2 \sparsityParam^2}\right).
  \end{align*}
  %
  Notice, that $\jMatrix_{\uv_k, \uv_k, k, t_k} = \uv_k \vv^{\T}_{\jMatrix}$ for
  \begin{align*}
    \vv_{\jMatrix}^{\T} & = -2 \meanFactor_{\uv_k, k, t_k} \pFactor_{k, t_k} t_k^{-1} \left(\bv_{\uv_k, k, t_k}^{\T} + \meanFactor_{\uv_k, k, t_k} \pFactor_{k, t_k} \uv_k^{\T} \right) \\
    & = - 2 \left[-1 - \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + O(t_k^{-3/2})\right] 
    \times 
    \left[1 - \frac{3}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2})\right] t_k^{-1} \times \\
    & \quad \, \times \left[ \uv_k + O(t_k^{-1}) + \left( 
      -1 - \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + O(t_k^{-3/2})
    \right) \left( 1 - \frac{3}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2}) \right) \uv_k \right]^{\T} \\
    & = O(t_k^{-2}),
  \end{align*}
  where we use Lemma~\ref{lemma: power expectation} for estimation of $\uv_k^{\T} \EE \displaceMatrix^2 \uv_k$ and Lemma~\ref{lemma: auxiliary variables expansion} for asymptotic behaviour of the auxiliary variables. Consequently,
  \begin{align*}
    \tr (\displaceMatrix \jMatrix_{\uv_k, \uv_k, k, t_k}) = O_\ell\left(\frac{\sqrt{\sparsityParam \log \nsize}}{\nsize^2 \sparsityParam^2}\right)
  \end{align*}
  because $t_k = \Theta\bigl(\lambda_k(\probMatrix)\bigr)$ due to Lemma~\ref{lemma: t_k is well-definied} and $\lambda_k(\probMatrix) = \Theta(\nsize \sparsityParam)$ due to Lemma~\ref{lemma: eigenvalues asymptotics}.

  Next, consider $\lMatrix_{\uv_k, \uv_k, k, t_k}$ which is also can represented as $\uv_k \vv_{\lMatrix}^{\T}$, where
  \begin{equation*}
    \vv_{\lMatrix} = \pFactor_{k, t_k} t_k^{-2} 
    \big( 
      (3 \meanFactor^2_{\uv_k, k, t_k} + 2 \meanFactor_{\uv_k, k, t_k}) \uv_k
      + 
      2 \meanFactor_{\uv_k, k, t_k} \probEigenvectors_{-k} [\probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)]^{-1} \resolvent(\uv_k, \probEigenvectors_{-k}, t_k)^{\T}
    \big).
  \end{equation*}
  %
  According to Lemma~\ref{lemma: auxiliary variables expansion}, we have
  \begin{align*}
    &\quad \left \Vert 2 \meanFactor_{\uv_k, k, t_k} \probEigenvectors_{-k} [\probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)]^{-1} \resolvent(\uv_k, \probEigenvectors_{-k}, t_k)^{\T} \right \Vert \\
    & = O(1) \cdot \bigl\Vert [\probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)]^{-1} \bigr\Vert \times t_k^{-3} \bigl\Vert \uv_k^{\T} \EE \displaceMatrix^2 \probEigenvectors_{-k} \bigr\Vert
    = O(t_k^{-1}),
  \end{align*}
  and, consequently, 
  \begin{align*}
    \vv_{\lMatrix} = \pFactor_{k, t_k} t_k^{-2} (3 \meanFactor_{\uv_k, k, t_k}^2 + 2 \meanFactor_{\uv_k, k, t_k}) \uv_k + O\bigl(t_k^{-3}\bigr).
  \end{align*}
  %
  While $3 \meanFactor_{\uv_k, k, t_k}^2 + 2 \meanFactor_{\uv_k, k, t_k} = 3 + \frac{6 \uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + O(t_k^{-3/2}) - 2 - \frac{2 \uv_k^{\T} \EE \displaceMatrix \uv_k}{t_k^2} + O(t_k^{-3/2}) = 1 + 4 \cdot \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + O(t_k^{-3/2})$, and, hence,
  \begin{align*}
    \vv_{\lMatrix} = \left(1 + \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} \right) \cdot t_k^{-2} \uv_k + O(t_k^{-7/2}) = t_k^{-2} \uv_k + O(t_k^{-3}).
  \end{align*}
  %
  That implies
  \begin{align*}
    \tr\bigl[(\displaceMatrix^2 - \EE \displaceMatrix^2) \lMatrix_{\uv_k, \uv_k, k, t_k}\bigr] & = \frac{\uv_k^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k}{t_k^2} + O (t_k^{-3}) \cdot O_{\prec} (t_k^{1/2}) \\
    & = \frac{\uv_k^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k}{t_k^2} + O_{\prec}(t_k^{-5/2}),
  \end{align*}
  where Lemma~\ref{lemma: power deviation} was used.

  Next, representing $\qMatrix_{\uv_k, \uv_k, k, t_k}$ as $\uv_k \vv_{\qMatrix}$ with
  \begin{align*}
    \vv_{\qMatrix} = \vv_{\lMatrix} - \pFactor_{k, t_k} t_k^{-2} \meanFactor_{\uv_k, k, t_k}^2 \uv_k + 4 \pFactor^2 t_k^{-2} \meanFactor_{\uv_k, k, t_k} \bv_{\uv_k, k, t_k} = O(t_k^{-2}),
  \end{align*}
  we obtain
  \begin{align*}
    \tr(\displaceMatrix \uv_k \uv_k^{\T}) \tr(\displaceMatrix \qMatrix_{\uv_k, \uv_k, k, t_k}) = O_\ell(\sqrt{\sparsityParam \log \nsize}) \cdot O_\ell(\sqrt{\sparsityParam \log \nsize}) \cdot O(t_k^{-2}) = O_\ell(\sparsityParam \cdot t_k^{-2} \log \nsize).
  \end{align*}
  %
  Finally, obtained via Lemma~\ref{lemma: auxiliary variables expansion}, the decomposition
  \begin{align*}
    \pFactor_{k, t_k} \meanFactor_{\uv_k, k, t_k}^2 = 1 - \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + O\bigl(t_k^{-3/2}\bigr).
  \end{align*}
  provides us with expansion
  \begin{align}
    \uv_k^{\T} \estimator[\uv]_k \estimator[\uv]_k^{\T} \uv_k = 1 - \frac{\uv_k^{\T} \displaceMatrix^2 \uv_k}{t_k^2} + O_\ell\bigl(t_k^{-3/2}\bigr), \nonumber \\
    \label{eq: eigenvector angle}
    \langle \uv_k, \estimator[\uv]_k \rangle = 1 - \frac{\uv_k^{\T} \displaceMatrix^2 \uv_k}{2 t_k^2} + O_\ell\bigl(t_k^{-3/2}\bigr).
  \end{align}

  Now, we should estimate 
  \begin{align*}
    \ev_i^{\T} \estimator[\uv]_k \estimator[\uv]_k^{\T} \uv_k = & 
    \meanFactor_{\ev_i, k, t_k} \meanFactor_{\uv_k, k, t_k} \pFactor_{k, t_k} 
    + \tr \left[ 
        \displaceMatrix \jMatrix_{\ev_i, \uv_k, k, t_k}
        - 
        (\displaceMatrix^2 - \EE \displaceMatrix^2) \lMatrix_{\ev_i, \uv_k, k, t_k}
    \right] \\
    & + \tr (\displaceMatrix \uv_k \uv_k^{\T})
    \tr (\displaceMatrix \qMatrix_{\ev_i, \uv_k, k, t_k}) + O_{\prec} \left(\frac{1}{\nsize^2 \sparsityParam^2}\right),
  \end{align*}
  obtained from Lemma~\ref{lemma: fan asymptotic expansion w/o sigma}.
  %
  For a reminder
  \begin{align*}
    \jMatrix_{\ev_i, \uv_k, k, t_k} & = - \pFactor_{k, t_k} t_k^{-1} \uv_k \left(
        \meanFactor_{\ev_i, k, t_k} \bv_{\uv_k, k, t_k}^{\T} + 
        \meanFactor_{\uv_k, k, t_k} \bv_{\ev_i, k, t_k}^{\T} +
        2 \meanFactor_{\uv_k, k, t_k} \meanFactor_{\ev_i, k, t_k} \pFactor_{k, t_k} \uv_k^{\T}
    \right),
    \\
    \lMatrix_{\ev_i, \uv_k, k, t_k} & = \pFactor_{k, t_k} t_k^{-2} \uv_k \bigg\{
        \bigl[
          \meanFactor_{\uv_k, k, t_k} 
          \resolvent(\ev_i, \probEigenvectors_{-k}, t_k) 
          + 
          \meanFactor_{\ev_i, k, t_k} \resolvent(\uv_k, \probEigenvectors_{-k}, t_k)
        \bigr]
        \\
        & \quad \, \times \left[
          \probEigenvalues_{-k}^{-1}
          +
          \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
        \right]^{-1} \probEigenvectors_{-k}
        + \meanFactor_{\ev_i, k, t_k} \uv_k^{\T} + \meanFactor_{\uv_k, k, t_k} \ev_i^{\T} + 3 \meanFactor_{\ev_i, k, t_k} \meanFactor_{\uv_k, k, t_k} \uv_k^{\T}
    \bigg\},
    \\
    \qMatrix_{\ev_i, \uv_k, k, t_k} & = \lMatrix_{\ev_i, \uv_k, k, t_k} - \pFactor_{k, t_k} t_k^{-2} \meanFactor_{\ev_i, k, t_k} \meanFactor_{\uv_k, k, t_k} \uv_k \uv_k^{\T}
    + 2 \pFactor_{k, t_k}^2 t_k^{-2} \uv_k \left(\meanFactor_{\ev_i, k, t_k} \bv_{\ev_i, k, t_k}^{\T} + \meanFactor_{\uv_k, k, t_k} \bv_{\uv_k, k, t_k}^{\T} \right).
  \end{align*}
    
  Applying asymptotic expansions from Lemma~\ref{lemma: auxiliary variables expansion}, we obtain
  \begin{align*}
    \meanFactor_{\uv_k, k, t_k} \bv_{\ev_i, k, t_k}^{\T} &= \left( - 1 - \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + O(t_k^{-3/2}) \right) \times \left(\ev_i + O(\nsize^{-1/2}) \right) = - \ev_i + O(\nsize^{-1/2}), \\
    \meanFactor_{\ev_i, k, t_k} \bv_{\uv_k, k, t_k}^{\T} & = \left(- \probEigenvectors_{ik} + O(t_k^{-1} / \sqrt{\nsize}) \right) \times \left( \uv_k + O(t_k^{-1}) \right) = - \probEigenvectors_{ik} \uv_k + O(t_k^{-1} / \sqrt{\nsize}) = O(\nsize^{-1/2}), \\
    2 \meanFactor_{\uv_k, k, t_k} \meanFactor_{\ev_i, k, t_k} \pFactor_{k, t_k} \uv_k^{\T} & = O(\nsize^{-1/2}).
  \end{align*}
  %
  Using the same notation as previously, we observe
  \begin{align*}
    \vv_{\jMatrix} & = t_k^{-1} \ev_i + O(t_k^{-1} \nsize^{-1/2}), \\
    \tr(\displaceMatrix \jMatrix_{\ev_i, \uv_k, k, t_k}) & = \frac{\ev_i^{\T} \displaceMatrix \uv_k}{t_k} + O_\ell\left(\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right).
  \end{align*}
  
  To estimate $\tr\bigl[(\displaceMatrix^2 - \EE \displaceMatrix^2) \lMatrix_{\ev_i, \uv_k, k, t_k}\bigr]$, we obtain
  \begin{align*}
    & \bigl[
      \meanFactor_{\uv_k, k, t_k} 
      \resolvent(\ev_i, \probEigenvectors_{-k}, t_k) 
      + 
      \meanFactor_{\ev_i, k, t_k} \resolvent(\uv_k, \probEigenvectors_{-k}, t_k)
    \bigr] 
    \left[
      \probEigenvalues_{-k}^{-1}
      +
      \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
        \right]^{-1} \probEigenvectors_{-k}
     \\
    & = \left [  (-1 + O(t_k^{-1})) \left(-\frac{1}{t_k}\ev_i^\T \probEigenvectors_{-k} + O(t_k^{-2}/\sqrt{\nsize}) \right) + (-\probEigenvectors_{ik} + O(t_k^{-1} /\sqrt{\nsize})) (- t_k^{-3} \uv_k^\T \EE \displaceMatrix^2 \probEigenvectors_{-k}  + O(t_k^{-5/2}) \right ] \\
    & \quad \times \left( \diag \left( \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}}\right)_{k' \in [\nclusters] \setminus \{k\}} + O(1) \right) \probEigenvectors_{-k} 
    = \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'}}{t_k - \lambda_{k'}} \probEigenvectors_{i k'} \uv_{k'}^\T + O(t_k^{-2}),
  \end{align*}
  where we use Lemma~\ref{lemma: auxiliary variables expansion} and $\uv_k^\T \EE \displaceMatrix^2 \uv_{k'} = O(t_k), k' \in [\nclusters]$, $\ev_i^\T \EE \displaceMatrix^2 \uv_k = O(t_k / \sqrt{\nsize})$ from Lemma~\ref{lemma: power expectation}. 
  Consequently, we have
  \begin{align*}
    \vv_{\lMatrix} & = -\frac{\ev_i}{t_k^2} + t_k^{-2} \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'}}{t_k - \lambda_{k'}} \probEigenvectors_{i k'} \uv_{k'} \\
    & \quad - t_k^{-2} ( \probEigenvectors_{i k} + O(t_k^{-1} / \sqrt{\nsize})) \uv_k + 3 t_k^{-2} ( \probEigenvectors_{ik} + O(t_k^{-1} / \sqrt{\nsize})) \uv_k + O(t_k^{-2}/ \sqrt{\nsize}) \\
    & = -\frac{\ev_i}{t_k^2} + t_k^{-2} \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'}}{t_k - \lambda_{k'}} \probEigenvectors_{i k'} + \frac{\uv_k^\T \EE \displaceMatrix^2 \uv_{k'}}{t_k^2} \uv_{k'} + 2 t_k^{-2} \probEigenvectors_{ik} \uv_k + O(t_k^{-2}/ \sqrt{\nsize}).
  \end{align*}
  %
  Thus, we get
  \begin{align*}
    & \tr\bigl[(\displaceMatrix^2 - \EE \displaceMatrix^2) \lMatrix_{\ev_i, \uv_k, k, t_k}\bigr] = \vv_{\lMatrix}^\T (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k \\
    & \overset{\text{Lemma~\ref{lemma: power deviation}}}{=} - \ev_i^\T (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k
    + t_k^{-2} \sum_{k' \in [\nclusters] \setminus \{k\} } \frac{\lambda_{k'}}{t_k - \lambda_{k'}} \probEigenvectors_{ik'} \cdot \uv_{k'} (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k \\
    & \quad + 2 t_k^{-2} \probEigenvectors_{ik} \cdot \uv_k^\T (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k + O(t_k^{-2} / \sqrt{\nsize}) \cdot O_{\prec}(t_k / \sqrt{\nsize}) \\
    & \overset{\text{Lemma~\ref{lemma: W squared bilinear form}}}{=} -\frac{1}{t_k^2} \ev_i^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k
    + O(t_k^{-2} / \sqrt{\nsize}) \cdot O_\ell(\sparsityParam \sqrt{\nsize} \log \nsize) + O_\prec \left(t_k^{-1} / \nsize\right) \\
    & = -\frac{1}{t_k^2} \ev_i^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k + O_{\ell} \left( \frac{\log \nsize}{\nsize^2 \sparsityParam}\right) + O_{\prec} \left(\frac{1}{\nsize^2 \sparsityParam} \right).
  \end{align*}
  %
  Finally, we obtain
  \begin{align*}
    \vv_{\qMatrix} = \vv_{\lMatrix} + O(t_k^{-2}),
  \end{align*}
  and
  \begin{align*}
    \tr(\displaceMatrix \uv_k \uv_k^{\T}) \tr(\displaceMatrix \qMatrix_{\ev_i, \uv_k, k, t_k}) = O_\ell(\sqrt{\sparsityParam \log \nsize}) \cdot O_\ell(\sqrt{\sparsityParam \log \nsize}) O(t_k^{-2}) = O_\ell\left(
      \frac{\log \nsize}{\nsize^2 \sparsityParam}
    \right).
  \end{align*}
  %
  Approximating $\pFactor_{k, t_k} \meanFactor_{\ev_i, k, t_k} \meanFactor_{\uv_k, k, t_k}$ with
  \begin{align*}
    \pFactor_{k, t_k} \meanFactor_{\ev_i, k, t_k} \meanFactor_{\uv_k, k, t_k} & =
    \left(1 - \frac{3}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2}) \right) \left(1 + \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + O(t_k^{-3/2}) \right) \times \\
    & \quad \, \times \left(\probEigenvectors_{i k} + \frac{\ev_i^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \frac{\uv_{k'}^\T \EE \displaceMatrix^2 \uv_k}{t_k^2} + O(t_k^{-5/2}) \right) \\
    & = \probEigenvectors_{ik} - \frac{2}{t_k^2} \probEigenvectors_{ik} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + \frac{1}{t_k^2} \ev_i^{\T} \EE \displaceMatrix^2 \uv_k \\
    & \quad \, + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2} \nsize^{-1/2}),
  \end{align*}
  we obtain 
  \begin{align}
  \notag
    \langle \ev_i, \estimator[\uv]_k \rangle \langle \estimator[\uv]_k, \uv_k \rangle & = \probEigenvectors_{i k} + \frac{\ev_i^{\T} \displaceMatrix \uv_k}{t_k} + \frac{\ev_i^{\T} \displaceMatrix^2 \uv_k}{t_k^2} - \frac{2}{t_k^2} \probEigenvectors_{i k}\uv_k^{\T} \EE \displaceMatrix^2 \uv_k \\
    & \quad \, + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k + O_\ell\left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right).
  \label{eq: second vector decomposition}
  \end{align}
  %
  Here we use Condition~\ref{cond: sparsity param bound} to ensure that the reminder $O_{\prec} \left ( \frac{1}{\nsize^2 \sparsityParam^2} \right )$ provided by Lemma~\ref{lemma: fan asymptotic expansion w/o sigma} is less than $O_{\ell}\left ( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}}\right )$. Dividing~\eqref{eq: second vector decomposition} by~\eqref{eq: eigenvector angle} results in:
  \begin{align*}
    \adjacencyEigenvectors_{ik} & = \probEigenvectors_{ik} + \frac{\ev_i^{\T} \displaceMatrix \uv_k}{t_k} + \frac{\ev_i^{\T} \displaceMatrix^2 \uv_k}{t_k^2} - 2 \probEigenvectors_{ik} \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + \frac{1}{2} \probEigenvectors_{ik} \frac{\uv_k^{\T} \displaceMatrix^2 \uv_k}{t_k^2} \\
    & \quad \, + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k+ O_{\ell} \left(\frac{\sqrt{\log \nsize}}{\nsize\sqrt{\nsize \sparsityParam}} \right)
  \end{align*}
  due to Lemma~\ref{lemma: W squared bilinear form}. Additionally, this lemma guarantees that
  \begin{align*}
    \probEigenvectors_{ik} \frac{\uv_k^{\T} \displaceMatrix^2 \uv_k}{t_k^2} - \probEigenvectors_{ik} \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} = \probEigenvectors_{ik} \cdot O_\ell\left( \frac{\sparsityParam \sqrt{\nsize} \log \nsize}{\nsize^2 \sparsityParam^2}\right) = O_\ell\left(\frac{\log \nsize}{\nsize^2 \sparsityParam} \right).
  \end{align*}
  %
  This leads us to the statement of the lemma.
\end{proof}
