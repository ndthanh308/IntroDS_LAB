% \subsection{Tools}

\subsection{Lower bound on risk based on two hypotheses}

Let $\Theta$ be an arbitrary parameter space, equipped with semi-distance $d: \Theta \times \Theta \to [0, +\infty)$, i.e.
  \begin{enumerate}
    \item for any $\theta, \theta' \in \Theta$, we have $d(\theta', \theta) = d(\theta, \theta')$,

    \item for any $\theta_1, \theta_2, \theta_3 \in \Theta$, we have $d(\theta_1, \theta_2) + d(\theta_2, \theta_3) \ge d(\theta_1, \theta_3)$,

    \item for any $\theta \in \Theta$, we have $d(\theta, \theta) = 0$.
  \end{enumerate}
  %
  For $\theta \in \Theta$, we denote the corresponding distribution by $\PP_{\theta}$. The following lemma bounds below the risk of estimation of parameter $\theta$ for the loss function $d(\cdot, \cdot)$ and any estimator $\estimator[\theta]$.

  \begin{lemma}[Lemma 2.9,~\citealt{tsybakov_introduction_2009}]
    \label{lemma: minimax bounds for 2 hypotheses}
    Suppose that for two parameters $\theta_1, \theta_0$ such that we have $d(\theta_1, \theta_0) \ge s$ and $\KL(\PP_{\theta_1} \Vert \PP_{\theta_0}) \le \alpha$. Then
    \begin{align*}
      \inf_{\hat{\theta}} \sup_{\theta \in \{\theta_1, \theta_0\} } \PP \left (d (\estimator[\theta], \theta) \ge s \right ) \ge \frac{1}{4} e^{-\alpha}.
    \end{align*}    
  \end{lemma}

\subsection{Asymptotically good codes}
  To prove Theorem~\ref{theorem: lower bound}, we use a variation of Fano's lemma based on many hypotheses. A common tool to construct such hypotheses is the following lemma from the coding theory.

  \begin{lemma}[Lemma 2.9,~\citealt{tsybakov_introduction_2009}]
  \label{lemma: varshamov-gilbert bound}
    Let $m \ge 8$. Then there exists a subset $\{\omega^{(0)}, \omega^{(1)}, \ldots, \omega^{(M)}\}$ of $\{0, 1\}^m$ such that $\omega^{(0)} = \zero$, for any distinct $i, j = 0, \ldots, M$, we have
    \begin{align*}
      d_H(\omega^{(i)}, \omega^{(j)}) \ge \frac{m}{8},
    \end{align*}
    and
    \begin{align*}
      M \ge 2^{m / 8}.
    \end{align*}
  \end{lemma}


\subsection{Lower bound on risk based on many hypotheses} 

 The following lemma generalizes Lemma~\ref{lemma: minimax bounds for 2 hypotheses} in the case of many hypotheses.

  \begin{lemma}[Theorem 2.5,~\citealt{tsybakov_introduction_2009}]
  \label{lemma: fano lemma}
    Assume that $M \ge 2$ and suppose that $\Theta$ contains elements $\theta_0, \theta_1, \ldots, \theta_M$ such that:
    \begin{enumerate}[label=(\roman*)]
      \item for all distinct $i,j$, we have $d(\theta_i, \theta_j) \ge 2 s > 0$,
      \item for the KL-divergence it holds that
      \begin{align*}
          \frac{1}{M} \sum_{j = 1}^M \KL(\PP_{\theta_j} \Vert \PP_{\theta_0}) \le \alpha \log M
      \end{align*}
      for $\alpha \in (0, 1/8)$.
    \end{enumerate}
    %
    Then
    \begin{align*}
      \inf_{\estimator[\theta]} \sup_{\theta \in \Theta} \PP \left( d(\estimator[\theta], \theta) \ge s \right)
      \ge 
      \frac{\sqrt{M}}{1 + \sqrt{M}}
      \left( 
          1 - 2 \alpha - \sqrt{
              \frac{2 \alpha}{M}
          }
      \right).
    \end{align*}
  \end{lemma}


\subsubsection{Gershgorin's circle theorem}
  We use the following theorem that is a common tool to bound eigenvalues of arbitrary matrix. For the proof, one can see the book by~\citet{horn_johnson_2012}.

  \begin{lemma}
  \label{lemma: gershgorin circle theorem}
    Let $\mathbf{X}$ be a complex $\nsize \times \nsize$ matrix. For $i \in [\nsize]$, define
    \begin{align*}
      R_i = \sum_{j \neq i} |\mathbf{X}_{ij}|.
    \end{align*}
    %
    Let $B({\bf X}_{ii}, R_i) \subset \mathbb{C}$, $i \in [\nsize]$, be a circle on the complex plane with the center ${\bf X}_{ii}$ and the radius $R_i$. Then all eigenvalues of $\mathbf{X}$ are contained in $\bigcup_{i \in [\nsize]} B({\bf X}_{ii}, R_i)$, and each connected component of $\bigcup_{i \in [\nsize]} B({\bf X}_{ii}, R_i)$ contains at least one eigenvalue.
  \end{lemma}