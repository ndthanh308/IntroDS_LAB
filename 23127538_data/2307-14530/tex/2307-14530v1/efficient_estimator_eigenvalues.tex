% !TEX root = jmlr_version.tex

\begin{lemma}
\label{lemma: second order term estimation}
  Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a}, we have
  \begin{align*}
    \frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k}{t_k^2}
    -
    \frac{\widehat{\uv}_k \diagAdjecencyMatrix \widehat{\uv}_k}{ \adjacencyEigenvalues^{2}_{k k}} = O_\ell \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right).
  \end{align*}
\end{lemma}

\begin{proof}
  We decompose the initial difference in the following way:
  \begin{align*}
    \frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k}{t_k^2}
    -
    \frac{\widehat{\uv}_k \diagAdjecencyMatrix \widehat{\uv}_k}{ \adjacencyEigenvalues^{2}_{k k}}
    & =
    \left(\frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k}{t_k^2}
    -
    \frac{\widehat{\uv}_k^{\T} \EE \diagAdjecencyMatrix \widehat{\uv}_k}{t_k^2} \right)
    +
    \left(\frac{\widehat{\uv}_k^{\T} \EE \diagAdjecencyMatrix \widehat{\uv}_k}{t_k^2}
    -
    \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_k}{t_k^2} \right)
    +
    \left(
      \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_k}{t_k^2}
      -
      \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_k}{ \adjacencyEigenvalues^{2}_{k k}}
    \right) \\
    & = : \Delta_1 + \Delta_2 + \Delta_3.
  \end{align*}
  %
  We analyze each term separately. First, from Lemma~\ref{lemma: adj eigenvectors displacement}, $\adjacencyEigenvectors_{ik}^2 = \probEigenvectors_{ik}^2 + \frac{2 \displaceMatrix_i \uv_k}{\lambda_k} + O_\prec \left( \frac{1}{\sparsityParam \nsize \sqrt{\nsize}}\right)$, and, consequently,
  \begin{align*}
    \sum_{i = 1}^\nsize (\EE \diagAdjecencyMatrix_{ii}) (\probEigenvectors_{ik}^2 - \adjacencyEigenvectors_{i k}^2) 
    & =
    \frac{2}{\lambda_k} 
    \sum_{i = 1}^\nsize (\EE \diagAdjecencyMatrix_{ii}) \displaceMatrix_i \uv_k 
    +
    O_\ell \left( \frac{\log \nsize}{\sparsityParam \sqrt{\nsize}} \right) \\
    & =
    \frac{2}{\lambda_k}
    \sum_{i = 1}^\nsize 
    \sum_{i \le j} \left( 
      \displaceMatrix_{i j} \probEigenvectors_{jk} \EE \diagAdjecencyMatrix_{ii}
      +
      \displaceMatrix_{j i} \probEigenvectors_{ik} \EE \diagAdjecencyMatrix_{jj}
    \right) \left(1 - \frac{\delta_{ij}}{2}\right)
    +
     O_\ell \left(\frac{\log \nsize}{\sparsityParam \sqrt{\nsize}} \right).
  \end{align*}
  %
  Here $\delta_{ij}$ is the Kronecker symbol. The double sum consists of $\binom{\nsize + 1}{2}$ mutually independent random variables and, thus, the Bernstein inequality can be applied. Bounding $\EE \diagAdjecencyMatrix_{ii}$, $\probEigenvectors_{j k}$ and $\Var \displaceMatrix_{i j}$ by $\nsize \sparsityParam$, $C_{\probEigenvectors} \nsize^{-1/2}$ and $\sparsityParam$ respectively, we observe
  \begin{align*}
    \sum_{i = 1}^\nsize (\EE \diagAdjecencyMatrix_{ii}) \displaceMatrix_i \uv_k = O_\ell \left( 
      \sqrt{\nsize^3 \sparsityParam^3 \log \nsize}
    \right).
  \end{align*}
  %
  Consequently, $\Delta_1 = O_\ell \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right)$. Second, we estimate $\Delta_2$. Notice, that
  \begin{align*}
    \EE \diagAdjecencyMatrix_{ii} - \diagAdjecencyMatrix_{ii} =
    \sum_{i = j}^\nsize
    (
      \probMatrix_{i j} - \adjacencyMatrix_{ij} 
    ) = O_\ell ( \sqrt{\nsize \sparsityParam \log \nsize}),
  \end{align*}
  since this sum consists of sub-Gaussian random variables again and, whence, its order can be established via the Bernstein inequality. Thus, $\Delta_2 = O_\ell \left( \frac{1}{\sqrt{\nsize^3 \sparsityParam^3}}\right)$.

  Finally, $\Delta_3 = \frac{\estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k}{\lambda^3_k} O_\ell \left( t_k - \adjacencyEigenvalues_{kk} \right) = O_\ell \left( (\nsize \sparsityParam)^{-5/2} \right)$ due to Lemma~\ref{lemma: eigenvalues difference}.
\end{proof}

  
\begin{lemma}
  Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a},
  \begin{align*}
    \lambda_k(\probMatrix) - \debiasedEigenvalues_{kk} = O_{\ell} (\sqrt{\sparsityParam \log \nsize}).
  \end{align*}
\end{lemma}

\begin{proof}
  By the definition of $t_k$~\eqref{eq: t_k definition},
  \begin{align*}
      1 + \lambda_k(\probMatrix) 
    \left\{ 
      \resolvent(\uv_k, \uv_k, t_k) 
      - 
      \resolvent(\uv_k, \probEigenvectors_{-k}, t_k) 
      [
        \probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
      ]^{-1}
      \resolvent(\probEigenvectors_{-k}, \uv_k, t_k)
    \right\} = 0.
  \end{align*}
  %
  Applying asymptotics from Lemma~\ref{lemma: auxiliary variables expansion}, we observe
  \begin{align*}
    \resolvent(\uv_k, \probEigenvectors_{-k}, t_k) 
    [
      \probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
    ]^{-1}
    \resolvent(\probEigenvectors_{-k}, \uv_k, t_k) = O(t_k^{-2}) \cdot  O(t_k) O(t_k^{-2}) = O(t_k^{-3}),
  \end{align*}
  and, consequently,
  \begin{align}
  \label{eq: t_k - lambda_k approximation}
    1 + \lambda_k(\probMatrix) \left \{ 
      - \frac{1}{t_k} - \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^{3}} + O(t_k^{-5/2}) + O(t_k^{-3})
    \right \} & = 0, \nonumber \\
    t_k - \lambda_k(\probMatrix) - \frac{\lambda_k(\probMatrix)}{t_k} \cdot \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k} & = O(t_k^{-1/2}).
  \end{align}
  Since $(\EE \displaceMatrix^2){ij} = \delta_{i j} \sum_{t} \probMatrix_{i t} (1 - \probMatrix_{i t}) = (\EE \diagAdjecencyMatrix)_{ij} + O(\sparsityParam^2 \nsize)$, we have
  \begin{align*}
    \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} = \frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k}{t_k^2} + O(t_k^{-2}) \cdot O(\sparsityParam^2 \nsize).
  \end{align*}
  Substituting this into~\eqref{eq: t_k - lambda_k approximation}, we obtain
  \begin{align*}
    t_k - \lambda_k(\probMatrix) - \lambda_k(\probMatrix) \cdot \frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k}{t_k^2} = O(\sparsityParam).
  \end{align*}
  The term $(\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k)/t_k^2$ can be efficiently estimated via {\color{red} YALemma}~\ref{lemma: second order term estimation}. Thus,
  \begin{align*}
    t_k - \lambda_k(\probMatrix) \left [1 + \frac{\estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k}{\adjacencyEigenvalues_{kk}^2} \right ] = O(\sparsityParam).
  \end{align*}
  Meanwhile, due to Lemma~\ref{lemma: eigenvalues difference}, $\adjacencyEigenvalues_{kk} = t_k + \uv_k^{\T} \displaceMatrix \uv_k + O_{\prec}(\nsize^{-1/2})$. Lemma~\ref{lemma: log estimate vector difference} guarantees that $\uv_k^{\T} \displaceMatrix \uv_k = O_\ell (\sqrt{\sparsityParam \log \nsize})$. Thus, $t_k - \adjacencyEigenvalues_{k k} = O_\ell(\sqrt{\sparsityParam \log \nsize})$, and
  \begin{align*}
    \adjacencyEigenvalues_{k k} - \lambda_k(\probMatrix) \left [1 + \frac{\estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k}{\adjacencyEigenvalues_{kk}^2} \right ] = O_\ell (\sqrt{\sparsityParam \log \nsize}), \\
    \lambda_k(\probMatrix)  = \left [ \frac{1}{\adjacencyEigenvalues_{k k}} + \frac{\estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k}{\adjacencyEigenvalues_{kk}^3}\right ]^{-1} + O_\ell (\sqrt{\sparsityParam \log \nsize}).
  \end{align*}
  %
  By the definition of $\debiasedEigenvalues_{kk}$ the statement of the lemma holds.
\end{proof}
  