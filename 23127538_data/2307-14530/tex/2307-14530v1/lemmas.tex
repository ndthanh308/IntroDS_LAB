% !TEX root = jmlr_version.tex

\subsubsection{Concentration of the bilinear form with the matrix $\displaceMatrix^2$}

To prove we may efficiently approximate the second-order terms in the asymptotic expansion of $\adjacencyEigenvectors$, we need the following result on the concentration of the bilinear form $\xv^\T \displaceMatrix^2 \yv$.

\begin{lemma}
\label{lemma: W squared bilinear form}
  Let $\xv$ and $\yv$ be such unit vectors that $\Vert \xv \Vert_{\infty} = O(\nsize^{-1/2})$ and $\Vert \yv \Vert_{\infty} = O(\nsize^{-1/2})$. Then it holds that
  \begin{align*}
    \xv^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \yv = O_\ell\left( \rho \sqrt{\nsize} \log \nsize \right).
  \end{align*}
\end{lemma}

\begin{proof}
  First,
  \begin{align}
  \label{eq: second order eigenvectors L}
    \xv^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \yv = \sum_{\substack{i, t, j \\ j \neq i}} \xv(i) \yv(j) \displaceMatrix_{i t} \displaceMatrix_{tj} + \sum_{i, t} \xv(i) \yv(i) (\displaceMatrix_{it}^2 - \EE \displaceMatrix^2_{it}).
  \end{align}
  %
  The first term can be considered as a sum over all tuples $(i, t, j)$ where $i \neq j$. Corresponding to two different tuples $(i, t, j)$ and $(i', t', j')$, summands are dependent if $\bigl\{\{i, t\}, \{t, j\}\bigr\} \cap \bigl\{\{i', t'\}, \{t', j'\}\bigr\} \neq \varnothing$. Define multisets $e_1 = \{i, t\}$ and $e_2 = \{t, j\}$ for tuple $(i, t, j)$. By definition $|e_1 \cap e_2| = 1$. Moreover, tuple $(i, t, j)$ can be reconstructed from $(e_1, e_2)$ using the following rule: $(i, t, j) = (e_1 \setminus e_2, e_1 \cap e_2, e_2 \setminus e_1)$. The condition $i \neq j$ guarantees that $e_1$ and $e_2$ can not be singletons at the same time. If $e = \{i, j\}$ then define $\displaceMatrix_e = \displaceMatrix_{ij}$. If $e = \{i, i\}$ then $\displaceMatrix_{e} = \displaceMatrix_{ii}$. Thus,
  \begin{align*}
    \sum_{\substack{i, t, j \\ j \neq i}} 
      \xv(i) \yv(j) 
      \displaceMatrix_{i t} \displaceMatrix_{t j}
    =
    \sum_{\substack{(e_1, e_2) \\ |e_1 \cap e_2| = 1}}
      \xv(e_1 \setminus e_2) \yv(e_2 \setminus e_1)
      \displaceMatrix_{e_1} \displaceMatrix_{e_2}.
  \end{align*}
  %
  To study its concentration we will utilize Freedman inequality, see Lemma~\ref{lemma: freedman inequality}. First, we somehow order 2-multisets of $[\nsize]$. For a multiset $e$ we denote the next element in the considered order by $S(e)$. Moreover, define $\sigma$-algebra $\mathcal{F}_e$ as a $\sigma$-algebra generated by random variables $\{\displaceMatrix_{e'}\}_{e' \leqslant e}$. Then variables 
  \begin{align*}
    Y_{e} = \EE \left( \sum_{\substack{(e_1, e_2) \\ |e_1 \cap e_2| = 1}}
    \xv(e_1 \setminus e_2) \yv(e_2 \setminus e_1)
    \displaceMatrix_{e_1} \displaceMatrix_{e_2} \, \bigg | \, \mathcal{F}_e \right)
  \end{align*}
  define an exposure martingale. The difference $X_{S(e)} = Y_{S(e)}
   - Y_e$ is
  \begin{equation*}
    X_{S(e)} =
    \sum_{\substack{(e', S(e)) \\ |e' \cap S(e)| = 1 \\ e' \leqslant e}}
      \xv\bigl(e' \setminus S(e)\bigr) \yv(S(e) \setminus e')
      \displaceMatrix_{S(e)} \displaceMatrix_{e'}
    +
    \sum_{\substack{(S(e), e') \\ |e' \cap S(e)| = 1 \\ e' \leqslant e}}
      \xv(S(e) \setminus e') \yv\bigl(e' \setminus S(e)\bigr)
      \displaceMatrix_{e'} \displaceMatrix_{S(e)}.
  \end{equation*}
  %
  Thus,
  \begin{align*}
    X_e = \displaceMatrix_{e} \sum_{\substack{e' < e \\ |e' \cap e| = 1}} 
      \bigl(
        \xv(e' \setminus e) \,
        \yv(e \setminus e')
        +
        \xv(e \setminus e') \,
        \yv(e' \setminus e)
     \bigr)
     \displaceMatrix_{e'}.
  \end{align*}
  %
  Denote for two $e$ and $e'$ such that $|e \cap e'| = 1$, 
  \begin{align*}
    T_{e, e'}
    =
    \xv(e' \setminus e) \,
    \yv(e \setminus e')
    +
    \xv(e \setminus e') \,
    \yv(e' \setminus e).
  \end{align*}
  %
  Obviously, we have $|X_e| \leqslant 2 \nsize \cdot \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty}$. Due to conditions of the lemma, $|X_e|$ is bounded by some constant $C_X$. Then
  \begin{align*}
    \EE \left[
      X_{S(e)}^2 \mid \mathcal{F}_e
    \right] = \left(\sum_{\substack{e' \leqslant e \\ |e' \cap e| = 1}}T_{e', S(e)} \displaceMatrix_{e'}\right)^2 \EE \displaceMatrix^2_{S(e)}.
  \end{align*}
  %
  Define 
  \begin{align*}
    W = \sum_{e} \EE \left[
      X_{S(e)}^2 \mid \mathcal{F}_e
    \right] = \sum_e \left( \sum_{\substack{e' < e \\ |e' \cap e| = 1}} T_{e', e} \displaceMatrix_{e'} \right)^2 \EE \displaceMatrix^2_e.
  \end{align*}
  %
  Consider the expression in brackets. We will bound it using the Bernstein inequality. By the definition of $T_{e', e}$ and conditions of the lemma we have
  \begin{align*}
    T_{e', e} = O(\nsize^{-1}).
  \end{align*}
  At the same time, $\Var \displaceMatrix_{e'} = \probMatrix_{e'} (1 - \probMatrix_{e'}) = O(\sparsityParam)$. Since the number of edges $e'$ sharing with an edge $e$ a vertex is at most $O(\nsize)$, applying Lemma~\ref{lemma: bernstein inequality} we obtain
  \begin{align*}
    \sum_{\substack{e' < e \\ |e' \cap e| = 1}} T_{e', e} \displaceMatrix_{e'} 
    = O_\ell(\sqrt{\sparsityParam \nsize^{-1} \log \nsize})
  \end{align*}
  for any $e$. That implies 
  \begin{align*}
    W = O_\ell\left(\sparsityParam^2 \nsize \log \nsize \right).
  \end{align*}
  %
  According to the Freedman inequality (Lemma~\ref{lemma: freedman inequality}),
  \begin{align}
  \label{eq: freedman application}
    \PP \left(
      \max_e Y_e \geqslant r \text{ and } W \leqslant \sigma^2
    \right) 
    \leqslant
    \exp \left(
      -\frac{r^2/2}{\sigma^2 + \frac{1}{3} r C_{X}^2}
    \right).
  \end{align}
  %
  According to Definition~\ref{def: O_l definition}, if we want the probability $\PP \left(\max_e Y_e \ge \delta \sparsityParam \sqrt{\nsize} \log \nsize \right)$ to be less than $\nsize^{-\varepsilon}$ for large enough $\nsize$ and appropriate $\delta$, choose $\delta_W$ and $n_0$ such that $\PP \left( W \ge \delta_W \sparsityParam^2 \nsize \log \nsize \right) \le \nsize^{-\varepsilon} / 2$. Next choose $\delta$ such that
  $\PP \left(
    \max_e Y_e \geqslant \delta \sparsityParam \sqrt{\nsize} \log \nsize \text{ and } W \leqslant \delta_W \sparsityParam^2 \nsize \log \nsize
  \right) \le \nsize^{-\varepsilon} /2$. 
  Equation~\eqref{eq: freedman application} shows that such $\delta$ exists. Thus,
  \begin{align*}
    \sum_{\substack{i, t, j \\ j \neq i}} 
      \xv(i) \yv(j)
      \displaceMatrix_{i t} \displaceMatrix_{t j}
   = O_\ell\left( \sparsityParam \sqrt{\nsize} \log \nsize \right).
  \end{align*}
  %
  We can analyze the second term of the sum~\eqref{eq: second order eigenvectors L} analogously. That finishes the proof.
\end{proof}

\subsubsection{Efficient estimation of eigenvalues}

\begin{lemma}
    \label{lemma: second order term estimation}
  Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a}, we have
  \begin{align*}
    \frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_{k'}}{t_k^2}
    -
    \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_{k'}}{ \adjacencyEigenvalues^{2}_{k k}} = O_\ell \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right),
  \end{align*}
  for any not necessarily distinct $k, k'$.
\end{lemma}

\begin{proof}
  We decompose the initial difference in the following way:
  \begin{align*}
    \frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_{k'}}{t_k^2}
    -
    \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_{k'}}{ \adjacencyEigenvalues^{2}_{k k}}
    & =
    \left(\frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_{k'}}{t_k^2}
    -
    \frac{\widehat{\uv}_k^{\T} \EE \diagAdjecencyMatrix \widehat{\uv}_{k'}}{t_k^2} \right)
    +
    \left(\frac{\widehat{\uv}_k^{\T} \EE \diagAdjecencyMatrix \widehat{\uv}_{k'}}{t_k^2}
    -
    \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_{k'}}{t_k^2} \right) \\
    & \quad +
    \left(
      \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_{k'}}{t_k^2}
      -
      \frac{\widehat{\uv}_k^{\T} \diagAdjecencyMatrix \widehat{\uv}_{k'}}{ \adjacencyEigenvalues^{2}_{k k}}
    \right) \\
    & = : \Delta_1 + \Delta_2 + \Delta_3.
  \end{align*}
  %
  We analyze each term separately. First, from Lemma~\ref{lemma: adj_eigenvectors_displacement}, we have
  \begin{align*}
    \adjacencyEigenvectors_{ik} \adjacencyEigenvectors_{ik'} = \probEigenvectors_{ik} \probEigenvectors_{i k'} + \probEigenvectors_{i k'} \frac{\displaceMatrix_i \uv_k}{t_k} + \probEigenvectors_{i k}\frac{\displaceMatrix_i \uv_{k'}}{t_{k'}} + (\probEigenvectors_{ik} + \probEigenvectors_{i k'} ) \cdot O_{\prec} \Bigl ( \frac{1}{\nsize \sparsityParam \sqrt{\nsize}} \Bigr ) + \frac{\displaceMatrix_i \uv_k}{t_k}  \cdot \frac{\displaceMatrix_i \uv_{k'}}{t_{k'}}.
  \end{align*}
  Since $\probEigenvectors_{i k}, \probEigenvectors_{i k'} = O(\nsize^{-1/2})$ due to Lemma~\ref{lemma: eigenvectors max norm} and $t_k^{-1} \displaceMatrix_i \uv_k, t_{k'}^{-1} \displaceMatrix_i \uv_{k'} = O_\ell (\sqrt{\sparsityParam \log \nsize})$ due to Lemma~\ref{lemma: log estimate vector difference}, we get
  \begin{align*}
    \sum_{i = 1}^\nsize (\EE \diagAdjecencyMatrix_{ii}) (\adjacencyEigenvectors_{ik} \adjacencyEigenvectors_{ik'} - \probEigenvectors_{ik} \probEigenvectors_{i k'}) 
    & =
    \frac{1}{t_k} 
    \sum_{i = 1}^\nsize \probEigenvectors_{i k'} (\EE \diagAdjecencyMatrix_{ii}) \displaceMatrix_i \uv_k 
    +
    \frac{1}{t_{k'}} \sum_{i = 1}^\nsize \probEigenvectors_{i k} (\EE \diagAdjecencyMatrix_{ii}) \displaceMatrix_i \uv_{k'}
    + O_\prec \left( 1 \right).
  \end{align*}
  %
  Let us analyze the first term of the right-hand side:
  \begin{align*}
      \sum_{i = 1}^\nsize \probEigenvectors_{i k'} (\EE \diagAdjecencyMatrix_{i i}) \displaceMatrix_i \uv_{k} = 2 \sum_{i = 1}^\nsize \sum_{j \le i} \left( \probEigenvectors_{i k'} \probEigenvectors_{j k} (\EE \diagAdjecencyMatrix_{ii}) \displaceMatrix_{i j} + \probEigenvectors_{i k} \probEigenvectors_{j k'} (\EE \diagAdjecencyMatrix_{j j}) \displaceMatrix_{j i} \right) \left( 1 - \frac{\delta_{i j}}{2} \right).
  \end{align*} 
  Here $\delta_{ij}$ is the Kronecker symbol. The double sum consists of $\binom{\nsize + 1}{2}$ mutually independent random variables and, thus, the Bernstein inequality can be applied. Bounding $\EE \diagAdjecencyMatrix_{ii}$, $\probEigenvectors_{j k}$ and $\Var \displaceMatrix_{i j}$ by $\nsize \sparsityParam$, $C_{\probEigenvectors} \nsize^{-1/2}$ and $\sparsityParam$ respectively, we observe
  \begin{align*}
    \sum_{i = 1}^\nsize \probEigenvectors_{i k'} (\EE \diagAdjecencyMatrix_{ii}) \displaceMatrix_i \uv_k = O_\ell \left( 
      \sqrt{\nsize^2 \sparsityParam^3 \log \nsize}
    \right).
  \end{align*}
  Analogously,
  \begin{align*}
       \sum_{i = 1}^\nsize \probEigenvectors_{i k} (\EE \diagAdjecencyMatrix_{ii}) \displaceMatrix_i \uv_{k'} = O_\ell \left( 
      \sqrt{\nsize^2 \sparsityParam^3 \log \nsize}
    \right).
  \end{align*}
  %
  Consequently, $\Delta_1 = O_{\prec} \Bigl ( \sqrt{\rho \log \nsize} / (\nsize^2 \sparsityParam^2)\Bigr ) = O_\ell \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right)$. Second, we estimate $\Delta_2$. Note that
  \begin{align*}
    \EE \diagAdjecencyMatrix_{ii} - \diagAdjecencyMatrix_{ii} =
    \sum_{j = 1}^\nsize
    (
      \probMatrix_{i j} - \adjacencyMatrix_{ij} 
    ) = O_\ell ( \sqrt{\nsize \sparsityParam \log \nsize}),
  \end{align*}
  since this sum consists of bounded random variables again and, whence, its order can be established via the Bernstein inequality. Thus,
  \begin{align*}
    \frac{
        \estimator[\uv]_{k}^\T
        (\EE \diagAdjecencyMatrix - \diagAdjecencyMatrix) 
        \estimator[\uv]_{k'}
    }{t_k^2} = t_k^{-2} \sum_{i = 1}^\nsize \adjacencyEigenvectors_{i k} \adjacencyEigenvectors_{i k'} \cdot O_\ell (\sqrt{\sparsityParam \nsize \log \nsize}).
  \end{align*}
  Due to Lemma~\ref{lemma: adj eigenvectors displacement} and Lemma~\ref{lemma: eigenvectors max norm}, we have $\adjacencyEigenvectors_{ik} = \probEigenvectors_{i k} + O_{\ell} \Bigl ( \sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}} \Bigr ) = O_{\ell} (\nsize^{-1/2})$ under Condition~\ref{cond: sparsity param bound}. Hence, we get
  \begin{align*}
      \Delta_2 = O \Bigl(\frac{1}{\nsize^2 \sparsityParam^2} \Bigr) \cdot \nsize \cdot O_\ell \Bigl( \frac{1}{\nsize} \Bigr) \cdot O_\ell (\sqrt{\sparsityParam \nsize \log \nsize}) = O_{\ell} \Biggl( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}} \Biggr).
  \end{align*}
  Finally, we bound $\Delta_3$. Using the same arguments as above, we obtain
  \begin{equation*}
    \estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k = \sum_{i = 1}^\nsize \adjacencyEigenvectors_{ik} \adjacencyEigenvectors_{i k'} (\EE \diagAdjecencyMatrix_{ii} + (\diagAdjecencyMatrix_{ii} - \EE \diagAdjecencyMatrix_{ii})
    = \nsize \cdot O_{\ell} \bigl( \nsize^{-1} \bigr) \cdot \bigl (O(\nsize \sparsityParam) + O_\ell (\sqrt{\sparsityParam \nsize \log \nsize}) \bigr )
    = O_{\ell} (\nsize \sparsityParam).
  \end{equation*}
  So, we get $\Delta_3 = O_{\ell} (\nsize \sparsityParam) \cdot (t_k^{-2} - \adjacencyEigenvalues_{kk}^{-2})$. According to Lemma~\ref{lemma: eigenvalues difference}, we have
  \begin{align*}
      \adjacencyEigenvalues_{kk} - t_k = \uv_k^\T \displaceMatrix \uv_{k'} + O_{\prec} (\nsize^{-1/2}),
  \end{align*}
  which is $O_\ell (\sqrt{\sparsityParam \log \nsize})$ due to Lemma~\ref{lemma: log estimate vector difference} and Condition~\ref{cond: sparsity param bound}. It implies
  \begin{align*}
      \Delta_3 = O_{\ell} (\nsize \sparsityParam) \cdot (t_k^{-2} - \adjacencyEigenvalues_{kk}^{-2})
                = O_{\ell} (\nsize \sparsityParam) \cdot t_k^{-2} \adjacencyEigenvalues_{kk}^{-2} (\adjacencyEigenvalues_{kk}^2 - t_k^2)
                = O_{\ell} (\nsize \sparsityParam) \cdot t_k^{-2} \adjacencyEigenvalues_{kk}^{-2} \cdot t_k \cdot O_{\ell} (\sqrt{\sparsityParam \log \nsize}).
  \end{align*}
  Since
  \begin{align*}
      \adjacencyEigenvalues^{-2}_{kk} = t_k^{-2} \Biggl ( 1 - \frac{O_\ell (\sqrt{\sparsityParam \log \nsize})}{t_k}\Biggr )^{-2} = t_k^{-2} \bigl(1 + o(1)\bigr),
  \end{align*}
  we get
  \begin{align*}
      \Delta_3 = O_{\ell} (\nsize \sparsityParam) \cdot t_k^{-3} \cdot O_{\ell} (\sqrt{\sparsityParam \log \nsize}) = O_{\ell} \Biggl ( \sqrt{\frac{\sparsityParam \log \nsize}{\nsize^4 \sparsityParam^4 }}\Biggr ) = O_{\ell} \Biggl ( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}} \Biggr).
  \end{align*}
  That concludes the lemma.
\end{proof}

  
\begin{lemma}
\label{lemma: debaised eigenvalues behaviour}
  Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} it holds
  \begin{align*}
    \lambda_k(\probMatrix) - \debiasedEigenvalues_{kk} = O_{\ell} (\sqrt{\sparsityParam \log \nsize}).
  \end{align*}
\end{lemma}

\begin{proof}
  By the definition of $t_k$ in~\eqref{eq: t_k definition},
  \begin{align*}
      1 + \lambda_k(\probMatrix) 
    \left\{ 
      \resolvent(\uv_k, \uv_k, t_k) 
      - 
      \resolvent(\uv_k, \probEigenvectors_{-k}, t_k) 
      [
        \probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
      ]^{-1}
      \resolvent(\probEigenvectors_{-k}, \uv_k, t_k)
    \right\} = 0.
  \end{align*}
  %
  Applying asymptotics from Lemma~\ref{lemma: auxiliary variables expansion}, we observe
  \begin{align*}
    \resolvent(\uv_k, \probEigenvectors_{-k}, t_k) 
    [
      \probEigenvalues^{-1}_{-k} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
    ]^{-1}
    \resolvent(\probEigenvectors_{-k}, \uv_k, t_k) = O(t_k^{-2}) \cdot O(t_k) O(t_k^{-2}) = O(t_k^{-3}),
  \end{align*}
  and, consequently,
  \begin{align}
  \label{eq: t_k - lambda_k approximation}
    1 + \lambda_k(\probMatrix) \left \{ 
      - \frac{1}{t_k} - \frac{1}{t_k^{3}} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-5/2}) + O(t_k^{-3})
    \right \} & = 0, \nonumber \\
    t_k - \lambda_k(\probMatrix) - \frac{\lambda_k(\probMatrix)}{t_k} \cdot \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k} & = O(t_k^{-1/2}).
  \end{align}
  %
  Since $(\EE \displaceMatrix^2)_{ij} = \delta_{i j} \sum_{t} \probMatrix_{i t} (1 - \probMatrix_{i t}) = (\EE \diagAdjecencyMatrix)_{ij} + O(\sparsityParam^2 \nsize)$, we have
  \begin{align*}
    \frac{1}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k = \frac{1}{t_k^2} \uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k + O(t_k^{-2}) \cdot O(\sparsityParam^2 \nsize).
  \end{align*}
  %
  Substituting this into~\eqref{eq: t_k - lambda_k approximation}, we obtain
  \begin{align*}
    t_k - \lambda_k(\probMatrix) - \lambda_k(\probMatrix) \cdot \frac{\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k}{t_k^2} = O(\sparsityParam).
  \end{align*}
  %
  The term $(\uv_k^{\T} \EE \diagAdjecencyMatrix \uv_k)/t_k^2$ can be efficiently estimated via Lemma~\ref{lemma: second order term estimation}. Thus,
  \begin{align*}
    t_k - \lambda_k(\probMatrix) \left [1 + \frac{\estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k}{\adjacencyEigenvalues_{kk}^2} \right ] = O(\sparsityParam).
  \end{align*}
  Meanwhile, due to Lemma~\ref{lemma: eigenvalues difference}, $\adjacencyEigenvalues_{kk} = t_k + \uv_k^{\T} \displaceMatrix \uv_k + O_{\prec}(\nsize^{-1/2})$. Lemma~\ref{lemma: log estimate vector difference} guarantees that $\uv_k^{\T} \displaceMatrix \uv_k = O_\ell (\sqrt{\sparsityParam \log \nsize})$. Thus, $t_k - \adjacencyEigenvalues_{k k} = O_\ell(\sqrt{\sparsityParam \log \nsize})$, and
  \begin{align*}
    \adjacencyEigenvalues_{k k} - \lambda_k(\probMatrix) \left [1 + \frac{\estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k}{\adjacencyEigenvalues_{kk}^2} \right ] = O_\ell (\sqrt{\sparsityParam \log \nsize}), \\
    \lambda_k(\probMatrix) = \left [ \frac{1}{\adjacencyEigenvalues_{k k}} + \frac{\estimator[\uv]_k^{\T} \diagAdjecencyMatrix \estimator[\uv]_k}{\adjacencyEigenvalues_{kk}^3}\right ]^{-1} + O_\ell (\sqrt{\sparsityParam \log \nsize}).
  \end{align*}
  %
  By the definition of $\debiasedEigenvalues_{kk}$ the statement of the lemma holds.
\end{proof}
  
\subsubsection{Important properties of the equality statistic}
  \begin{lemma}
  \label{lemma: bounds of statistic center}
    Suppose that $a = \Theta(\nsize^{-2} \sparsityParam^{-1})$. Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: eigenvalues divergency} there are such constants $C_1$, $C_2$ that 
    \begin{align*}
      \frac{C_1}{\nsize^2 \sparsityParam} 
      \leqslant 
      \lambda_{\min} \bigl(
        \asymptoticVariance(i, j) + \penalizer \identity
      \bigr)
      \leqslant
      \lambda_{\max} \bigl(
        \asymptoticVariance(i, j) + \penalizer \identity
      \bigr)
      \leqslant
      \frac{C_2}{\nsize^2 \sparsityParam}
    \end{align*}
    and such constants $C_1'$ and $C_2'$ that
    \begin{align*}
      C_1' \Vert \nodeCommunityMatrix_i - \nodeCommunityMatrix_j \Vert^2 
      \leqslant
      \frac{\equalityStatisticCenter_{ij}^\penalizer}{\nsize \sparsityParam}
      \leqslant
      C_2' \Vert \nodeCommunityMatrix_i - \nodeCommunityMatrix_j \Vert^2
    \end{align*}
  for any $i$ and $j$.
  \end{lemma}

  \begin{proof}
    Let us estimate eigenvalues of matrix $\asymptoticVariance(i, j)$. After some straightforward calculations we have
    \begin{align*}
      \asymptoticVariance (i, j) &= 
      \probEigenvalues^{-1}
      \probEigenvectors^{\T}
      \EE \left(
        \displaceMatrix_i
        -
        \displaceMatrix_j
      \right)^{\T}
      \left(
        \displaceMatrix_i
        -
        \displaceMatrix_j
      \right)
      \probEigenvectors
      \probEigenvalues^{-1}  \\
      & = 
      \probEigenvalues^{-1}
      \probEigenvectors^{\T}
      \left(
        \operatorname{diag} (
          \EE \displaceMatrix_i^2
          +
          \EE \displaceMatrix_j^2
        )
        - \EE \displaceMatrix_{ij}^2
        (\ev_i \ev_j^{\T} + \ev_j \ev_i^{\T})
      \right)
      \probEigenvectors
      \probEigenvalues^{-1}.
    \end{align*}
    The maximum eigenvalue can be estimated using a norm of the matrix:
    \begin{align*}
      \lambda_{\max} \bigl( 
        \asymptoticVariance (i, j) + \penalizer \identity
      \bigr)
      =
      \Vert
        \asymptoticVariance (i, j)
      \Vert + \penalizer
      \leqslant \penalizer + 
      \Vert
        \probEigenvalues^{-1}
      \Vert^2
      \Vert
        \probEigenvectors
      \Vert^2
      \left(
        \Vert 
          \operatorname{diag} (
            \EE \displaceMatrix_i^2
            +
            \EE \displaceMatrix_j^2
          )
        \Vert
        + 2 \EE \displaceMatrix_{ij}^2
      \right),
    \end{align*}
    \begin{align*}
      \lambda_{\max} \bigl(
        \asymptoticVariance(i, j)
      \bigr)
      \leqslant 
      \frac{
        4 \sparsityParam
      }{
        \lambda_{\nclusters}^2 (\probMatrix)
      } + O(\nsize^{-2} \sparsityParam^{-1}),
    \end{align*}
    since $\EE \displaceMatrix_{ij}^2 = \probMatrix_{ij} - \probMatrix_{ij}^2$. Since $\lambda_{\nclusters}(\probMatrix) = \Theta(\nsize \sparsityParam)$ due to Lemma~\ref{lemma: eigenvalues asymptotics}, we have
    \begin{align*}
        \lambda_{\max}(\asymptoticVariance(i, j) +\penalizer \identity) = O(\nsize^{-2} \sparsityParam^{-1})
    \end{align*}
    Clearly, $\asymptoticVariance(i, j)$ is non-negative. Thus, we get
    \begin{align*}
        \lambda_{\min}(\asymptoticVariance(i, j) + \penalizer \identity) \ge a = \Omega(\nsize^{-2} \sparsityParam^{-1}).
    \end{align*}
    
    Now we state
    \begin{align*}
      \equalityStatisticCenter_{ij}^\penalizer 
      \leqslant 
      \frac{1}{\lambda_{\min} \bigl(\asymptoticVariance(i, j) + \penalizer \identity\bigr)} 
      \Vert
        \probEigenvectors_i - \probEigenvectors_j
      \Vert^2
      \leqslant
      \frac{
        \sigma_{\max}^2 (\basisMatrix)
      }{
        \lambda_{\min} \bigl(\asymptoticVariance(i, j) + \penalizer \identity \bigr)
      }
      \Vert
        \nodeCommunityMatrix_i
        -
        \nodeCommunityMatrix_j
      \Vert^2.
    \end{align*}
    %
    In the same way, we obtain
    \begin{align*}
      \equalityStatisticCenter_{ij}^\penalizer
      \geqslant
      \frac{
        \sigma_{\min}^2 (\basisMatrix)
      }{
        \lambda_{\max} \bigl(\asymptoticVariance(i, j) + \penalizer \identity \bigr)
      }
      \Vert
        \nodeCommunityMatrix_i
        -
        \nodeCommunityMatrix_j
      \Vert^2.
    \end{align*}
    %
    Applying asymptotic properties of singular values from Lemma~\ref{lemma: F rows tensor product}, we complete the proof.
  \end{proof}

  \begin{lemma}
  \label{lemma: uniformly covariance estimation}
    Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} it holds that
    \begin{align}
      \max_{i, j} 
          \left \Vert
            \asymptoticVariance(i, j)
            -
            \estimator[\asymptoticVariance](i,j)
          \right \Vert
      =
      O_{\prec} \left(
          \frac{
            1
          }{
            \nsize^2 \rho \sqrt{\nsize \sparsityParam}
          }
      \right).
    \end{align}
  \end{lemma}

  \begin{proof}
    This proof is a slight modification of the corresponding one of Theorem~5 from~\cite{Fan2020_ASYMPTOTICS}. We start considering
    \begin{align*}
      \asymptoticVariance(i, j)
      & =
      \probEigenvalues^{-1}
      \probEigenvectors^{\T}
      \left(
        \operatorname{diag} (
          \EE \displaceMatrix_i^2
          +
          \EE \displaceMatrix_j^2
        )
        - \EE \displaceMatrix_{ij}^2
        (\ev_i \ev_j^{\T} + \ev_j \ev_i^{\T})
      \right)
      \probEigenvectors
      \probEigenvalues^{-1}, \\
      \estimator[\asymptoticVariance](i, j)
      & =
      \debiasedEigenvalues^{-1}
      \adjacencyEigenvectors^{\T}
      \bigl(
        \operatorname{diag} (
          \estimator[\displaceMatrix]_i^2
          +
          \estimator[\displaceMatrix]_j^2
        )
        - \estimator[\displaceMatrix]_{ij}
        (\ev_i \ev_j^{\T} + \ev_j \ev_i^{\T})
      \bigr)
      \adjacencyEigenvectors
      \debiasedEigenvalues^{-1}.
    \end{align*}
    %
    We begin with studying the sum for some particular values $k_1$ and $k_2$:
    \begin{align*}
      \sum_{l = 1}^\nsize \probEigenvectors_{l k_1} \probEigenvectors_{l k_2} (\displaceMatrix_{il}^2 - \EE \displaceMatrix_{il}^2).
    \end{align*}
    %
    It is a sum of independent random variables. According to the Bernstein inequality, the above is greater than $t$ with probability at most
    \begin{align*}
      \exp \left(
        -\frac{
          t^2
        }{
          \sum_{l = 1}^\nsize \probEigenvectors_{l k_1}^2 \probEigenvectors_{l k_2}^2 \EE \displaceMatrix_{il}^4 + \frac{C_{\probEigenvectors}^2 t}{3 \nsize} 
        }
      \right) 
      \leqslant
      \exp \left(
        -\frac{
          t^2
        }{
          \frac{C_\probEigenvectors^2}{\nsize} \max_l \EE \displaceMatrix_{il}^4 + \frac{C_{\probEigenvectors}^2 t}{3 \nsize} 
        }
      \right)
      \leqslant
      \exp \left(
        -\frac{
          t^2
        }{
          \frac{C_\probEigenvectors^2}{\nsize} 2 \sparsityParam + \frac{C_{\probEigenvectors}^2 t}{3 \nsize} 
        }
      \right),
    \end{align*}
    where $C_{\probEigenvectors}$ is the uniform constant from Lemma~\ref{lemma: eigenvectors max norm}. For arbitrary $\varepsilon$ taking appropriate $t = \sqrt{\frac{\rho}{\nsize}} \nsize^{\delta}$, we observe that
    \begin{align*}
      \sum_{l = 1}^\nsize 
          \probEigenvectors_{l k_1} 
          \probEigenvectors_{l k_2} 
          (
            \displaceMatrix_{il}^2
            - \EE \displaceMatrix_{il}^2
            + \displaceMatrix_{jl}^2 
            - \EE \displaceMatrix_{jl}^2
          )
      =
      O_\prec \left(
        \sqrt{\frac{\sparsityParam}{\nsize}}
      \right)
    \end{align*}
    due to the definition of $O_\prec (\cdot)$. Moreover, due to Lemma~\ref{lemma: corrected eigs and noise},
    \begin{align*}
      & \quad \sum_{l = 1}^\nsize 
        \probEigenvectors_{l k_1} 
        \probEigenvectors_{l k_2} 
        (
          \estimator[\displaceMatrix]_{il}^2
          - \EE \displaceMatrix_{il}^2
          + \estimator[\displaceMatrix]_{jl}^2 
          - \EE \displaceMatrix_{jl}^2
        ) \\
      & =
      \sum_{l = 1}^\nsize 
        \probEigenvectors_{l k_1} 
        \probEigenvectors_{l k_2} 
        (
          \displaceMatrix_{il}^2
          - \EE \displaceMatrix_{il}^2
          + \displaceMatrix_{jl}^2 
          - \EE \displaceMatrix_{jl}^2
        )
      +
      O_\prec \left(
        \sqrt{\frac{\sparsityParam}{\nsize}}
      \right),
    \end{align*}
    and, consequently,
    \begin{align*}
      \sum_{l = 1}^\nsize 
        \probEigenvectors_{l k_1} 
        \probEigenvectors_{l k_2} 
        (
          \estimator[\displaceMatrix]_{il}^2
          - \EE \displaceMatrix_{il}^2
          + \estimator[\displaceMatrix]_{jl}^2 
          - \EE \displaceMatrix_{jl}^2
        )
      =
      O_\prec \left(
        \sqrt{\frac{\sparsityParam}{\nsize}}
      \right).
    \end{align*}
    %
    Due to Lemma~\ref{lemma: adj eigenvectors displacement}, we have
    \begin{align*}
        \adjacencyEigenvectors_{i k} = \probEigenvectors_{ik} + O_{\ell} \Biggl ( \sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}}\Biggr ).
    \end{align*}
    We may bound $\probEigenvectors_{ik} = O(\nsize^{-1/2})$ due to Lemma~\ref{lemma: eigenvectors max norm} and $(\nsize \sparsityParam)^{-1} \log \nsize = O(1)$ due to Condition~\ref{cond: sparsity param bound}. So $\adjacencyEigenvectors_{i k} = O_{\prec} \left( \nsize^{-1/2} \right)$. Hence, we get
    \begin{align*}
      & \quad \sum_{l = 1}^\nsize 
        \adjacencyEigenvectors_{l k_1} 
        \adjacencyEigenvectors_{l k_2}
        (
          \estimator[\displaceMatrix]_{il}^2
          + \estimator[\displaceMatrix]_{jl}^2 
        )
      =
      \sum_{l = 1}^\nsize 
        (\adjacencyEigenvectors_{l k_1} - \probEigenvectors_{l k_1})
        \adjacencyEigenvectors_{l k_2}
        (
          \estimator[\displaceMatrix]_{il}^2
          + \estimator[\displaceMatrix]_{jl}^2 
        ) \\
      & + 
      \sum_{l = 1}^\nsize 
        \probEigenvectors_{l k_1} 
        (\adjacencyEigenvectors_{l k_2} - \probEigenvectors_{l k_2})
        (
          \estimator[\displaceMatrix]_{il}^2
          + \estimator[\displaceMatrix]_{jl}^2 
        )
      +
      \sum_{l = 1}^\nsize 
        \probEigenvectors_{l k_1} 
        \probEigenvectors_{l k_2}
        (
          \estimator[\displaceMatrix]_{il}^2
          + \estimator[\displaceMatrix]_{jl}^2 
        )
      +
      O_\prec \left(
        \sqrt{\frac{\sparsityParam}{\nsize}}
      \right),
    \end{align*}
    and, finally,
    \begin{align*}
      \sum_{l = 1}^\nsize 
        \adjacencyEigenvectors_{l k_1} 
        \adjacencyEigenvectors_{l k_2} 
        (
          \estimator[\displaceMatrix]_{il}^2
          + \estimator[\displaceMatrix]_{jl}^2 
        )
      = 
      \sum_{l = 1}^\nsize 
        \probEigenvectors_{l k_1} 
        \probEigenvectors_{l k_2} 
        (
          \EE \displaceMatrix_{il}^2
          + \EE \displaceMatrix_{jl}^2
        )
      +
      O_\prec \left(
        \sqrt{\frac{\sparsityParam}{\nsize}}
      \right).
    \end{align*}
    %
    In the same way,
    \begin{align*}
      \estimator[\displaceMatrix]_{ij}^2
      \left(
        \adjacencyEigenvectors_{i k_1}
        \adjacencyEigenvectors_{j k_2}
        +
        \adjacencyEigenvectors_{j k_1}
        \adjacencyEigenvectors_{i k_2}
      \right)
      = 
      \EE \displaceMatrix_{ij}^2
      \left(
        \probEigenvectors_{i k_1}
        \probEigenvectors_{j k_2}
        +
        \probEigenvectors_{j k_1}
        \probEigenvectors_{i k_2}
      \right)
      +
      O_\prec \left(
        \sqrt{\frac{\sparsityParam}{\nsize}}
      \right).
    \end{align*}
    Define
    \begin{align*}
        V(i, j) & = \probEigenvectors^{\T}
      \left(
        \operatorname{diag} (
          \EE \displaceMatrix_i^2
          +
          \EE \displaceMatrix_j^2
        )
        - \EE \displaceMatrix_{ij}^2
        (\ev_i \ev_j^{\T} + \ev_j \ev_i^{\T})
      \right)
      \probEigenvectors, \\
    \estimator[V](i, j) & = \adjacencyEigenvectors^{\T}
      \bigl(
        \operatorname{diag} (
          \estimator[\displaceMatrix]_i^2
          +
          \estimator[\displaceMatrix]_j^2
        )
        - \estimator[\displaceMatrix]_{ij}
        (\ev_i \ev_j^{\T} + \ev_j \ev_i^{\T})
      \bigr)
      \adjacencyEigenvectors, \\
    \Delta_{\probEigenvectors}(i, j) & =  V(i, j) - \estimator[V](i, j).
    \end{align*}
    Then $\Delta_{\probEigenvectors} = O_{\prec} \Bigl ( \sqrt{\frac{\sparsityParam}{\nsize}} \Bigr )$ and 
    \begin{align*}
       \Vert V(i, j) \Vert \le \Vert \operatorname{diag} (
          \EE \displaceMatrix_i^2
          +
          \EE \displaceMatrix_j^2
        )
        - \EE \displaceMatrix_{ij}^2
        (\ev_i \ev_j^{\T} + \ev_j \ev_i^{\T}) \Vert \le 4 \rho,
    \end{align*}
    so $\Vert \estimator[V](i, j) \Vert = O_{\prec} (\rho)$. We have
    \begin{align}
    \label{eq: lemma asymptotic variance - decomposition with spectral noise}
         \asymptoticVariance(i, j) - \estimator[\asymptoticVariance](i, j) = \probEigenvalues^{-1} \Delta_{\probEigenvectors}(i, j) \probEigenvalues^{-1} + \probEigenvalues^{-1} \estimator[V](i, j) (\probEigenvalues^{-1} - \debiasedEigenvalues^{-1}) + \debiasedEigenvalues^{-1} \estimator[V](i, j) (\probEigenvalues^{-1} - \debiasedEigenvalues^{-1})
    \end{align}
    %
   Meanwhile, we have
    \begin{align*}
        & \quad \Vert \probEigenvalues^{-1} - \debiasedEigenvalues^{-1} \Vert = \bigl \Vert \probEigenvalues^{-1} - \probEigenvalues^{-1} \bigl(\identity + \probEigenvalues^{-1} (\debiasedEigenvalues - \probEigenvalues)\bigr)^{-1} \bigr \Vert \\
        & = \Bigl \Vert \probEigenvalues^{-1} - \probEigenvalues^{-1} \sum_{i = 0}^{\infty} (-1)^i \probEigenvalues^{-i} (\debiasedEigenvalues - \probEigenvalues)^{i} \Bigr \Vert 
        = \Bigl \Vert - \probEigenvalues^{-1} \sum_{i = 1}^{\infty} (-1)^{i} \probEigenvalues^{-i} (\debiasedEigenvalues - \probEigenvalues)^{i} \Bigr \Vert \\
        & = \Bigl \Vert \probEigenvalues^{-2}  (\debiasedEigenvalues - \probEigenvalues) \cdot \sum_{i = 0}^\infty (-1)^{i} \probEigenvalues^{-i} (\debiasedEigenvalues - \probEigenvalues)^{i} \Bigr \Vert
        \le \Vert \probEigenvalues \Vert^{-2}  \Vert \debiasedEigenvalues - \probEigenvalues \Vert  \cdot \frac{1}{1 + \Vert \probEigenvalues^{-1} (\debiasedEigenvalues - \probEigenvalues ) \Vert}.
    \end{align*}
    Since $\debiasedEigenvalues_{kk} = \probEigenvalues_{kk} + O_\ell(\sqrt{\sparsityParam \log \nsize})$ due to Lemma~\ref{lemma: debaised eigenvalues behaviour} and $\probEigenvalues_{kk} = \Theta(\nsize \sparsityParam)$ due to Lemma~\ref{lemma: eigenvalues asymptotics}, we obtain
    \begin{align*}
        \Vert \probEigenvalues^{-1} - \debiasedEigenvalues^{-1} \Vert = O\Bigl( \frac{1}{\nsize^2 \rho^2} \Bigr) \cdot O_{\ell} (\sqrt{\sparsityParam \log \nsize}).
    \end{align*}
    Thus, the dominating term in~\eqref{eq: lemma asymptotic variance - decomposition with spectral noise} is the first one, so
    \begin{align}
      \estimator[\asymptoticVariance](i, j) = 
      \asymptoticVariance(i, j) 
      + O_\prec \left( 
        \frac{1}{\nsize^2 \sparsityParam} \cdot \frac{1}{\sqrt{\nsize \sparsityParam}}
      \right).
    \end{align}
  \end{proof}


\subsubsection{Applicability of Lemma~\ref{lemma: fan eigenvectors series decomposition}}
  First, we compute the asymptotic expansion of some values presented in Table~\ref{tab: expansion notation}. Variables $\probEigenvalues_{-k}$ and $\probEigenvectors_{-k}$ are defined in the caption of Table~\ref{tab: expansion notation}.
  \begin{lemma}
  \label{lemma: auxiliary variables expansion}
    Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} we have asymptotic expansions described in Table~\ref{tab: auxiliary variables expansion}.
  \end{lemma}

  \begin{table}[t!]
    \centering
    \begin{tabular}{|l|}
        \hline
        ``Resolvents'' approximation \\
        \hline \\
        $\resolvent(\ev_i, \probEigenvectors_{-k}, t_k) = -\frac{1}{t_k} \ev_i^{\T} \probEigenvectors_{-k} + O \left( t_k^{-2} / \sqrt{\nsize} \right)$ \\ \\
        $\resolvent(\uv_k, \probEigenvectors_{-k}, t_k) = -\frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \probEigenvectors_{-k} + O(t_k^{-5/2}) $ \\ \\
        $\resolvent(\uv_k, \uv_k, t_k) = - \frac{1}{t_k} - \frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-5/2})$ \\ \\
        \hline
        0-degree coefficients approximation \\
        \hline \\
        $\meanFactor_{\uv_k, k, t_k} = -1 - \frac{1}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2})$ \\ \\
        $\meanFactor_{\ev_i, k, t_k} = 
        - \probEigenvectors_{i k} 
      - \frac{1}{t_k^2} \ev_i^{\T} \EE \displaceMatrix^2 \uv_k 
      - \frac{1}{t_k^2} \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'} }{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2}/\sqrt{\nsize})$ \\ \\
        $\pFactor_{k, t_k} = 1 - \frac{3}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2})$ \\ \\
        \hline
        Vector auxiliary variables \\
        \hline \\
        $\bv_{\ev_i, k, t_k} = \ev_i + O(\nsize^{-1/2}) $ \\ \\
        $\bv_{\uv_k, k, t_k} = \uv_k + O(t_k^{-1}) $\\ \\
        \hline
        Matrix auxiliary variables \\
        \hline \\
        $\left[
          \probEigenvalues_{-k}^{-1}
          +
          \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
        \right]^{-1}
        =
        \diag \left(\frac{\lambda_{k'} t_k}{t_{k} - \lambda_{k'}}\right)_{k' \in [\nclusters] \setminus \{k\}} + O(1) $ \\ \\
        \hline
    \end{tabular}
    \caption{Asymptotic expansion of some variables from Lemma~\ref{lemma: auxiliary variables expansion}.}
  \label{tab: auxiliary variables expansion}
  \end{table}

  \begin{proof}
    From Lemma~\ref{lemma: power expectation} we have for any distinct $k, k'$ and $l \ge 2$:
    \begin{equation*}
        \ev_i^{\T} \EE \displaceMatrix^{l} \uv_{k} = O(\alpha_{\nsize}^l \Vert \uv_{k} \Vert_{\infty}), \qquad
        \uv_{k}^{\T} \EE \displaceMatrix^l \uv_{k'} = O(\alpha_{\nsize}^l).
    \end{equation*}
    According to Lemma~\ref{lemma: eigenvectors max norm}, we have $\Vert \uv_{k} \Vert_{\infty} = O(\nsize^{-1/2})$. Theorem~\ref{theorem: conditions satisfuction}, Lemma~\ref{lemma: eigenvalues asymptotics} and Lemma~\ref{lemma: t_k is well-definied} guarantee that $\alpha_{\nsize} = O(t_{k}^{1/2})$. Finally, $\uv_k^{\T} \probEigenvectors_{-k} = \mathbf{O}$ and $\probEigenvectors_{-k}^{\T} \probEigenvectors_{-k} = \identity$ because of eigenvectors' orthogonality.
    All the above deliver us the following expansion:
    \begin{align*}
      \resolvent(\ev_i, \probEigenvectors_{-k}, t_k) 
      & = - \frac{1}{t_k} \ev_i^{\T} \probEigenvectors_{-k} - \sum_{l = 2}^L t_k^{- (l + 1)} \ev_i^{\T} \EE \displaceMatrix^l \probEigenvectors_{-k}\\
      & = -\frac{1}{t_k} \ev_i^{\T} \probEigenvectors_{-k} + O \left( t_k^{-3} \alpha_{\nsize}^2 / \sqrt{\nsize} \right) 
      = 
      -\frac{1}{t_k} \ev_i^{\T} \probEigenvectors_{-k} + O \left(t_k^{-2} / \sqrt{\nsize}\right),
    \end{align*}
    \begin{align*}
      \resolvent(\uv_k, \probEigenvectors_{-k}, t_k)
      & = - \frac{1}{t_k} \uv_{k}^{\T} \probEigenvectors_{-k} - \frac{1}{t_k^3} \uv_{k}^{\T} \EE \displaceMatrix^2 \probEigenvectors_{-k} - \sum_{l = 3}^L t_{k}^{-(l + 1)} \uv_{k}^{\T} \EE \displaceMatrix^l \probEigenvectors_{-k} \\
      & = -\frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \probEigenvectors_{-k} + O(t_k^{-4} \alpha_{\nsize}^3)
      =
      -\frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \probEigenvectors_{-k} + O(t_k^{-5/2}),
      \\
      \resolvent(\uv_k, \uv_k, t_k)
      & = - \frac{1}{t_k} \uv_k^{\T} \uv_k - \frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k - \sum_{l = 3}^L t_k^{- (l + 1)} \uv_k^{\T} \EE \displaceMatrix^{l} \uv_k \\
      & = -\frac{1}{t_k} - \frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-4} \alpha_{\nsize}^3 )
      =
      -\frac{1}{t_k} - \frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-5/2}),
    \end{align*}
    %
    \begin{align*}
      \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k) & = - \frac{1}{t_k} \probEigenvectors_{-k}^{\T} \probEigenvectors_{-k} - \sum_{l = 2}^{L} t_k^{-(l + 1)} \probEigenvectors_{-k}^{\T} \EE \displaceMatrix^l \probEigenvectors_{-k}
      = - \frac{1}{t_k} \identity + O(t_k^{-2}),
      \\
      \resolvent(\ev_i, \uv_k, t_k)
      & = -\frac{1}{t_k}\ev_i^{\T} \uv_k - \frac{1}{t_k^3} \ev_i^{\T} \EE \displaceMatrix^2 \uv_k - \sum_{l = 3}^L t_k^{-(l + 1)} \ev_i^{\T} \EE \displaceMatrix^l \uv_k \\
      & = -\frac{1}{t_k} \probEigenvectors_{i k} - \frac{1}{t_k^3} \ev_i^{\T} \EE \displaceMatrix^2 \uv_k + O\left(t_k^{-4} \alpha_{\nsize}^3 / \sqrt{\nsize}\right) \\
      & =
      -\frac{1}{t_k} \probEigenvectors_{i k} - \frac{1}{t_k^3} \ev_i^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-5/2} / \sqrt{\nsize}).
    \end{align*}
    % where $\alpha_{\nsize}$ is defined in Condition~\ref{cond: fan_condition_1}. Above we use the fact that $\alpha_{\nsize} = \Theta (\sqrt{\nsize \sparsityParam}) = \Theta(t_k^{1/2})$ due to Theorem~\ref{theorem: conditions satisfuction}, Lemma~\ref{lemma: eigenvalues asymptotics} and Lemma~\ref{lemma: t_k is well-definied}. We widely use Lemma~\ref{lemma: power expectation} to bound terms of higher degrees.
    
    Next we estimate 
    $\left[
      \probEigenvalues_{-k}^{-1}
      +
      \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
      \right]^{-1}
    $. Since 
    \begin{align*}
      \probEigenvalues_{-k}^{-1} - \frac{1}{t_k} \identity =
      \diag \left(
        \frac{ 
          t_k - \lambda_{k'}
        }{
          \lambda_{k'} t_k
        } 
      \right)_{k' \in [\nclusters] \setminus \{k\}}
    \end{align*}
    has order $\Omega(t_k^{-1})$ due to Condition~\ref{cond: eigenvalues divergency} and Lemma~\ref{lemma: t_k is well-definied}, 
    \begin{equation*}
      \left[
        \probEigenvalues_{-k}^{-1}
        +
        \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
      \right]^{-1}
      =
      \diag \left( \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}}\right) 
      \left[
        \identity + O(t_k^{-1})
      \right]^{-1}
      = 
      \diag \left(\frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}}\right) + O(1).
    \end{equation*}
    %
    After that, we are able to establish asymptotics of $\meanFactor_{\uv_k, k, t_k}$ and $\meanFactor_{\ev_i, k, t_k}$. Indeed,
    \begin{align*}
      \meanFactor_{\uv_k, k, t_k}
      & = 
      - 1
      - \frac{1}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k 
      + O(t_k^{-3 / 2}) 
      - \left[ 
        -\frac{1}{t_k^3} \uv_k^{\T} \EE \displaceMatrix^2 \probEigenvectors_{-k} + O(t_k^{-5/2})
      \right] \times \\
      & \quad \, \times \left[
        \diag \left( \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} \right)_{k' \in [\nclusters] \setminus \{k\}} + O(1)
      \right] \times \left[ 
        -\frac{1}{t_k^2} \probEigenvectors_{-k}^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2}) 
      \right] \\
      & = - 1
      - \frac{1}{t_k^2} \uv_k^{\T} \EE \displaceMatrix^2 \uv_k 
      + O(t_k^{-3/2})
    \end{align*}
    since $\frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} = O(t_k)$ and $\uv_k^{\T} \EE \displaceMatrix^2 \probEigenvectors_{-k} = O(t_k)$. Similarly,
    \begin{align*}
      \meanFactor_{\ev_i, k, t_k} 
      & = - \probEigenvectors_{i k} - \frac{1}{t_k^2} \ev_i^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2} / \sqrt{\nsize})
      -
      \left[ 
        - \frac{1}{t_k} \ev_i^{\T} \probEigenvectors_{-k} + O(t_k^{-2} / \sqrt{\nsize})
      \right] \times \\
      & \quad \, \times \left[ 
        \diag \left( \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} \right)_{k' \in [\nclusters] \setminus \{k\}} + O(1)
      \right] \times \left[ 
        - \frac{1}{t_k^2} \probEigenvectors_{-k}^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-5/2}) 
      \right] \\
      & = - \probEigenvectors_{i k} 
      - \frac{1}{t_k^2} \ev_i^{\T} \EE \displaceMatrix^2 \uv_k 
      - \frac{1}{t_k^2} \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'} }{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2} / \sqrt{\nsize}),
    \end{align*}
    where we use Lemma~\ref{lemma: eigenvectors max norm} to estimate $\ev_i^{\T} \probEigenvectors_{-k}$. After that we are able to approximate $\pFactor_{k, t_k}$:
    \begin{align*}
        \pFactor_{k, t_k} & = \left [ t_k^2 \frac{d}{d t_k} \frac{\meanFactor_{\uv_k, k, t_k}}{t_k} \right ]^{-1} = \left [ t_k^2 \frac{d}{d t_k} \left(
            - \frac{1}{t_k} - \frac{1}{t_k^3} \uv_k^\T \EE \displaceMatrix^2 \uv_k + O (t_k^{-5/2})
        \right)\right ]^{-1} \\
        & = \left [ 1 + \frac{3}{t_k^2} \uv_k^\T \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2}) \right ]^{-1}
        g= 1 - \frac{3}{t_k^2} \uv_k^\T \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2}).
    \end{align*}
    Finally,
    \begin{align*}
      \bv_{\ev_i, k, t_k} & = \ev_i - \probEigenvectors_{-k} 
      \left[ 
        \diag \left(\frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}}\right) + O(1)
      \right] \times \left[ - \frac{1}{t_k}\probEigenvectors^{\T}_{-k} \ev_i + O(t_k^{-2} / \sqrt{\nsize}) \right] \\
      & = \ev_i + \frac{1}{t_k} \probEigenvectors_{-k} \left( \sum_{k' \in [\nclusters] \setminus k} \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} \ev_{k'} \ev_{k'}^\T \right) \probEigenvectors_{-k}^{\T} \ev_{i} + O(t_k^{-1} / \sqrt{\nsize}) \\
      & = \ev_i + \frac{1}{t_k} \sum_{k' \in [\nclusters] \setminus k} \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} (\probEigenvectors_{-k} \ev_{k'}) (\ev_{k'}^\T  \probEigenvectors_{-k}^{\T} \ev_{i}) + O(t_k^{-1} / \sqrt{\nsize}) \\
      & = \ev_i + \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'}}{t_k - \lambda_{k'}} \uv_{k'} \cdot \probEigenvectors_{i k'} + O(t_k^{-1} / \sqrt{\nsize}) \\
      & = \ev_i + O(\nsize^{-1/2}),
    \end{align*}
    since, slightly abusing notation, we have $\probEigenvectors_{-k} \ev_{k'} = \uv_{k'}$, $\Vert \uv_{k'} \Vert = 1$ and $\probEigenvectors_{i k'} = O(\nsize^{-1/2})$. Analogously,
    \begin{align*}
      \bv_{\uv_k, k, t_k} & = \uv_k - \probEigenvectors_{-k} 
      \left[ 
        \diag \left( \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} \right) + O(1)
      \right] \times \left[ 
        -\frac{1}{t_k^{3}} \probEigenvectors_{-k}^{\T} \EE \displaceMatrix^2 \uv_k + O(t_k^{-5/2})
      \right] \\
      & = \uv_k + \frac{1}{t_k^3} \probEigenvectors_{-k} \left(
        \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} \ev_{k'} \ev_{k'}^\T
      \right) \probEigenvectors_{-k}^\T \EE \displaceMatrix^2 \uv_k + O(t_k^{-3/2}) \\
      & = \uv_k + \frac{1}{t_k^3} 
        \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} t_k}{t_k - \lambda_{k'}} (\probEigenvectors_{-k} \ev_{k'})  (\ev_{k'}^\T
       \probEigenvectors_{-k}^\T \EE \displaceMatrix^2 \uv_k) + O(t_k^{-3/2}) \\
      & = \uv_k + 
      \sum_{k' \in [\nclusters] \setminus \{k'\}} 
        \frac{\lambda_{k'}}{t_k - \lambda_{k'}} \uv_{k'} \cdot
        \frac{\uv_{k'} \EE \displaceMatrix^2 \uv_k}{t_k^2} 
      + O(t_{k}^{-3/2}) \\
      & = \uv_k + O(t_k^{-1}),
    \end{align*}
    where we use $\uv_{k'} \EE \displaceMatrix^2 \uv_k = O(t_k)$ and $\Vert \uv_k \Vert = 1$.
  \end{proof}
  
  \begin{lemma}
  \label{lemma: fan asymptotic expansion w/o sigma}
    Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a}, for $\xv \in \{ \uv_k, \ev_i\}$, it holds that
    \begin{align*}
      \xv^{\T} \estimator[\uv]_k \estimator[\uv]_k^{\T} \uv_k
      & = a_k 
      + 
      \tr [
        \displaceMatrix \jMatrix_{\xv, \uv_k, k, t_k} - (\displaceMatrix^2 - \EE \displaceMatrix^2) \lMatrix_{\xv, \uv_k, k, t_k}
      ] \\
      & \quad + \tr (\displaceMatrix \uv_k \uv_k^{\T}) \tr (\displaceMatrix \qMatrix_{\xv, \uv_k, k, t_k})
      + O_{\prec} \left( \frac{1}{\nsize^2 \sparsityParam^2}\right),
    \end{align*}
    where $a_k = \meanFactor_{\xv, k, t_k} \meanFactor_{\uv_k, k, t_k} \pFactor_{k, t_k}$.
  \end{lemma}

  \begin{proof} In Lemma~\ref{lemma: fan eigenvectors series decomposition}, we present the statement provided by~\cite{Fan2020_ASYMPTOTICS}. The authors need $\sigma_k^2$ and $\tilde{\sigma}_k^2$ to establish asymptotic distribution of the form $\xv^\T \widehat{\uv}_k \widehat{\uv}_k^\T \yv$, while we require only concentration properties. Thus, the condition regrading $\sigma_k^2$ and $\tilde{\sigma}_k^2$ can be omitted.
    
    The only remaining issue is to replace $O_p(t_k^{-2})$ with $O_{\prec}(t_k^{-2})$. Notice that the source of $O_p(\cdot)$ in Lemma~\ref{lemma: fan eigenvectors series decomposition} are random values of the form
    \begin{align*}
      \xv^{\T} (\displaceMatrix^\ell - \EE \displaceMatrix^\ell) \yv,
    \end{align*}
    where $\xv$ and $\yv$ are unit vectors. In~\cite{Fan2020_ASYMPTOTICS}, authors bounded it using the second moment. At the same time, they obtain an estimation
    \begin{align*}
      \xv^{\T} (\displaceMatrix^\ell - \EE \displaceMatrix^\ell) \yv = O_{\prec} \left( \min(\alpha_{\nsize}^{\ell - 1}, \Vert \xv \Vert_{\infty} \alpha_{\nsize}^\ell, \Vert \yv \Vert_{\infty} \alpha_{\nsize}^\ell) \right)
    \end{align*}
    in~\cite{Fan2019_SIMPLE} using all moments provided by Lemma~\ref{lemma: power deviation}.

    Due to Lemma~\ref{lemma: eigenvalues asymptotics} and Lemma~\ref{lemma: t_k is well-definied}, we have $O_{\prec}(t_k^{-2}) = O_{\prec}\left( [\nsize \sparsityParam]^{-2}\right)$.  That delivers the statement of the lemma.
  \end{proof}


\subsubsection{SPA consistency}
  \begin{lemma}
  \label{lemma: log estimate vector difference}
    For any unit $\xv$ and $\yv$, we have 
    \begin{align*}
      \xv^{\T} \displaceMatrix \yv = O_\ell \left(\max \left\{\sqrt{\frac{\sparsityParam}{\log \nsize}}, \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty}\right\}\log \nsize \right).
    \end{align*}
  \end{lemma}

  \begin{proof}
    We rewrite the bilinear form using the Kronecker delta:
    \begin{align*}
      \xv^{\T} \displaceMatrix \yv = 
      \sum_{1 \le i \le j \le \nsize} \displaceMatrix_{i j} (\xv_i \yv_j + \xv_j \yv_i) \left(1 - \frac{\delta_{i j}}{2} \right).
    \end{align*}
    %
    Now it is the sum of independent random variables with variance
    \begin{align*}
      & \quad \Var \sum_{1 \le i \le j \le \nsize} \displaceMatrix_{i j} (\xv_i \yv_j + \xv_j \yv_i) \left(1 - \frac{\delta_{i j}}{2} \right) = 
      \sum_{1 \le i \le j \le \nsize} \EE \displaceMatrix_{i j}^2 (\xv_i \yv_j + \xv_j \yv_i)^2 \left(1 - \frac{\delta_{i j}}{2} \right)^2 \\
      & \le \sparsityParam \sum_{1 \le i \le j \le \nsize} (\xv_i^2 \yv_j^2 + \xv_j^2 \yv_i^2 + 2 \xv_i \xv_j \yv_i \yv_j ) \left(1 - \frac{\delta_{i j}}{2} \right)^2
      \le \sparsityParam \left(
        \Vert \xv \Vert^2 \cdot \Vert \yv \Vert^2 + \langle \xv, \yv \rangle^2
      \right) \le 2 \sparsityParam,
    \end{align*}
    and each element bounded by
    \begin{align*}
      \left|
        \displaceMatrix_{i j} (\xv_i \yv_j + \xv_j \yv_i) \left(1 - \frac{\delta_{i j}}{2} \right)
      \right| \le 2 \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty}.
    \end{align*}
    %
    Applying the Bernstein inequality (Lemma~\ref{lemma: bernstein inequality}), we obtain
    \begin{align*}
      \PP \left( 
        \xv^{\T} \displaceMatrix \yv \ge t
      \right)
      \le
      \exp \left( 
        -\frac{t^2 / 2}{2 \sparsityParam + \frac{2 \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty}}{3} t}
      \right).
    \end{align*}
    %
    Given $\varepsilon$, choose $\delta$ such that $\frac{\delta}{1 + \sqrt{\delta} / 3} \ge 4 \varepsilon$. If $\sqrt{\frac{\sparsityParam}{\log \nsize}} \ge \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty}$, then for $t = \sqrt{\delta \sparsityParam \log \nsize}$
    \begin{equation*}
      \frac{t^2/4}{\sparsityParam + \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty} t/3} = \frac{\delta \sparsityParam \log \nsize / 4}{\sparsityParam + \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty} \sqrt{\delta \sparsityParam \log \nsize} / 3}
      \ge \frac{\delta \sparsityParam \log \nsize / 4}{\sparsityParam + \sparsityParam \sqrt{\delta} / 3} 
      \ge \frac{\delta / 4}{1 + \sqrt{\delta} / 3} \log \nsize \ge \varepsilon \log \nsize.
    \end{equation*}
    %
    That implies $\PP \left( \xv^{\T} \displaceMatrix \yv \ge t\right) \le \nsize^{-\varepsilon}$. The case of $\sqrt{\frac{\sparsityParam}{\log \nsize}} \le \Vert \xv \Vert_{\infty} \cdot \Vert \yv \Vert_{\infty}$ can be processed analogously. Thus, the statement holds.
  \end{proof}

  \begin{lemma}
  \label{lemma: adj eigenvectors displacement}
    Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} we have
    \begin{align*}
      \max_i 
          \Vert 
            \adjacencyEigenvectors_i
            -
            \probEigenvectors_i
          \Vert
      = O_\ell \left( 
          \sqrt{
            \frac{\log \nsize}{\nsize^2 \sparsityParam}
          }
      \right).
    \end{align*}
  \end{lemma}

  \begin{proof}
    Due to Lemma~\ref{lemma: adj_eigenvectors_displacement}:
    \begin{align}
    \label{eq: lemma 13 U initial decomposition}
      \adjacencyEigenvectors_{ik} = 
      \probEigenvectors_{ik} + 
      \frac{1}{t_k}\displaceMatrix_i \uv_k +
      O_{\prec} \left(
        \frac{1}{\sqrt{\nsize} \lambda_k(\probMatrix)}
      \right)
    \end{align}
    as $t_k = \Theta\bigl(\lambda_k(\probMatrix)\bigr)$ due to Lemma~\ref{lemma: t_k is well-definied}, $\lambda_k(\probMatrix) = \Theta(\nsize \sparsityParam)$ due to Lemma~\ref{lemma: eigenvalues asymptotics} and $\alpha_{\nsize} = \Theta(\sqrt{\nsize \sparsityParam})$ due to Theorem~\ref{theorem: conditions satisfuction}.
    %
    Thus, we can rewrite it in the following way:
    \begin{align*}
      \adjacencyEigenvectors_i = \probEigenvectors_i +
      \displaceMatrix_i \probEigenvectors \meanEigs^{-1} + 
      O_{\prec} \left(
          \frac{
            1
          }{
            \sqrt{\nsize} \lambda_\nclusters(\probMatrix)
          } 
      \right)
    \end{align*}
    for $\meanEigs = \diag(t_k)_{k \in [\nclusters]}$. Due to Lemma~\ref{lemma: log estimate vector difference}, Condition~\ref{cond: sparsity param bound} and Lemma~\ref{lemma: eigenvectors max norm}, we obtain
    \begin{align}
    \label{eq: displace estimation}
      \Vert 
        \displaceMatrix_i 
        \probEigenvectors
        \meanEigs^{-1} 
      \Vert 
      = O_{\ell}(\sqrt{\sparsityParam \log \nsize}) \cdot \Vert \meanEigs^{-1} \Vert.
    \end{align}
    %
    Lemma~\ref{lemma: eigenvalues asymptotics} and Lemma~\ref{lemma: t_k is well-definied} guarantee that $\Vert \meanEigs^{-1} \Vert_{2} = O \left(\frac{1}{\nsize \sparsityParam} \right)$. Thus, 
    \begin{align*}
      \Vert \probEigenvectors_{i} - \adjacencyEigenvectors_i \Vert = O_\ell \left( \sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}}\right).
    \end{align*}
    %
    For each $i$, we have the same probabilistic reminder in~\eqref{eq: lemma 13 U initial decomposition}. In~\cite{Fan2019_SIMPLE}, it appears due to superpolynomial moment bounds of probability obtained from Lemma~\ref{lemma: power expectation} uniformly over $i$. Thus, the maximal reminder over $i \in [\nsize]$ has the same order. Similarly, we can take the maximum over $i$ for inequality~\eqref{eq: displace estimation} since superpolynomial bounds are provided via the Bernstein inequality and do not depend on $i$.
  \end{proof}


  \begin{lemma}
  \label{lemma: spa selection}
    Assumed Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} to be satisfied, SPA chooses nodes $i_{1}, \ldots, i_{\nclusters}$ such that
    \begin{align*}
      \max_k \Vert
            \probEigenvectors_{i_k} - \basisMatrix_k
          \Vert
      =
      O_\ell \left(
        \frac{\sqrt{\log \nsize}}{\nsize \sqrt{\sparsityParam}}
      \right).
    \end{align*}
  \end{lemma}

  \begin{proof}
    To estimate error of SPA we need to apply Lemma~\ref{lemma: spa robustness} and, hence, we should estimate the difference between observed and real eigenvectors. From Lemma~\ref{lemma: adj eigenvectors displacement} we obtain that
    \begin{align*}
      \max_i
        \Vert
          \adjacencyEigenvectors_i
          -
          \probEigenvectors_i
        \Vert
      \leqslant
      \frac{\delta_1 \sqrt{\log \nsize}}{\nsize \sqrt{\sparsityParam}}
    \end{align*}
    with probability at least $1 - \nsize^{- \varepsilon}$ for any $\varepsilon$ and large enough $\delta_1$. Thus, due to Lemma~\ref{lemma: spa robustness} we conclude that SPA chooses some indices $i_1, \ldots, i_k$ such that
    \begin{align*}
      \PP \left(
          \max_k
          \Vert 
            \adjacencyEigenvectors_{i_k} - \basisMatrix_k
          \Vert
          \geqslant
          \frac{
            \delta_1 \sqrt{\log \nsize }
          }{
            \nsize \sqrt{\sparsityParam}
            \bigl(
              1 + 80 \kappa (\mathbf{F})
            \bigr)^{-1}
          }
      \right)
      \leqslant \nsize^{-\varepsilon}.
    \end{align*}
    %
    Using triangle inequality, we notice
    \begin{align*}
      \Vert
        \probEigenvectors_{i_k} - \basisMatrix_k 
      \Vert
      \leqslant
      \Vert
        \probEigenvectors_{i_k} -
        \adjacencyEigenvectors_{i_k}
      \Vert
      +
      \Vert
        \adjacencyEigenvectors_{i_k}
        -
        \basisMatrix_k
      \Vert,
    \end{align*}
    and it implies that there is some constant $C$ such that:
    \begin{align*}
      \PP \left(
        \max_k \Vert
          \probEigenvectors_{i_k} - \basisMatrix_k
        \Vert
        \geqslant
        \frac{
          C \sqrt{\log \nsize}
        }{
          \nsize \sqrt{\sparsityParam}
        }
      \right)
      \leqslant
      \nsize^{ -\varepsilon}
    \end{align*}
    since $\kappa (\basisMatrix)$ is bounded by a constant due to Lemma~\ref{lemma: F rows tensor product}.
  \end{proof}


\subsubsection{Eigenvalues behavior}
  \begin{lemma}
  \label{lemma: F rows tensor product}
    Under Condition~\ref{cond: theta distribution-a} the singular numbers of the matrix $\sqrt{\nsize} \basisMatrix$ are bounded away from 0 and $\infty$. Moreover, for any set $\beta_1, \ldots, \beta_\nclusters$ of positive numbers, bounded away from 0 and $\infty$, the matrix
    \begin{align*}
      \sumMatrix = \sum_{k = 1}^\nclusters \beta_k \basisMatrix_k^{\T} \basisMatrix_k
    \end{align*}
    is full rank, and there are such constants $C_1$, $C_2$ that
    \begin{align*}
      \frac{C_1}{\nsize} 
      \leqslant 
      \lambda_{\min} (\sumMatrix)
      \leqslant
      \lambda_{\max} (\sumMatrix)
      \leqslant
      \frac{C_2}{\nsize}.
    \end{align*}
  \end{lemma}

  \begin{proof}
    Since the matrix $\basisMatrix$ is full rank, its rows are linearly independent. Hence, if $\beta_k > 0$, matrix $\sumMatrix$ is full rank. Now we want to estimate eigenvalues of $\sumMatrix$:
    \begin{align*}
      \lambda_{\min} (\sumMatrix) & = 
      \inf_{\Vert \vv \Vert = 1} \vv^{\T} \sumMatrix \vv
      = 
      \inf_{\Vert \vv \Vert = 1}
         \sum_{k = 1}^\nclusters
              \beta_k
              (\vv^{\T} \basisMatrix_k^{\T})^2 \\
      & \geqslant
          (\min_{k} \beta_k)
          \inf_{\Vert \vv \Vert = 1}
          \sum_{k = 1}^{\nclusters}
            \vv^{\T}
            \basisMatrix^{\T}
            \ev_k
            \ev_k^{\T}
            \basisMatrix
            \vv \\
      & = (\min_k \beta_k)
      \inf_{\Vert \vv \Vert = 1}
      \vv^{\T} \basisMatrix^{\T} \basisMatrix \vv = 
      \lambda_{\min} (\basisMatrix^{\T} \basisMatrix) \min_k \beta_k.
    \end{align*}
    %
    In the other side, using multiplicative Weyl's inequality we obtain
    \begin{align*}
      \sigma_{\min} (\communityMatrix) = 
      \sigma_{\min} (\basisMatrix \probEigenvalues \basisMatrix^{\T}) =
      \sigma_{\min} (\basisMatrix^{\T} \basisMatrix \probEigenvalues)
      \leqslant
      \sigma_{\min} (\basisMatrix^{\T} \basisMatrix) \sigma_{\max} (\probEigenvalues).
    \end{align*}
    %
    Hence,
    \begin{align*}
      \lambda_{\min} (\basisMatrix^{\T} \basisMatrix) 
      \geqslant
      \frac{
        |\lambda_{\min} (\communityMatrix)|
      }{
        |\lambda_1(\probMatrix)|
      }
      \geqslant
      \frac{
        |\lambda_{\min} (\constCommunityMatrix)|
      }{
        C' \nsize
      },
    \end{align*}
    where constant $C'$ was taken from Lemma~\ref{lemma: eigenvalues asymptotics}. Similarly, we have 
    \begin{align*}
        \lambda_{\max}(\sumMatrix) \le (\max_{k} \beta_k) \sigma_{\max}(\basisMatrix^{\T} \basisMatrix) \le \frac{\sigma_{\max}(\communityMatrix)}{\sigma_{\nclusters}(\probMatrix)}.
    \end{align*} 
    We finally conclude that
    \begin{align*}
      \frac{C_1}{\nsize} 
      \leqslant 
      \lambda_{\min} (\sumMatrix)
      \leqslant
      \lambda_{\max} (\sumMatrix)
      \leqslant
      \frac{C_2}{\nsize},
    \end{align*}
    where
    \begin{equation*}
      C_1 = 
      \frac{
        \lambda_{\min} (\constCommunityMatrix) \min_k \beta_k 
      }{
        C' \nsize
      }, \quad
      C_2 = 
      \frac{
        \lambda_{\max} (\constCommunityMatrix) \max_k \beta_k 
      }{
        c' \nsize
      }.
    \end{equation*}
  \end{proof}


  \begin{lemma}
  \label{lemma: eigenvalues asymptotics}
    Under Condition~\ref{cond: theta distribution-a} there are such constants $c, C, c', C'$ that
    \begin{align*}
      c \nsize \leqslant \lambda_{\nclusters} (\nodeCommunityMatrix^{\T} \nodeCommunityMatrix) \leqslant \lambda_{\max}(\nodeCommunityMatrix^{\T} \nodeCommunityMatrix) \leqslant C \nsize 
    \end{align*}
    and
    \begin{align*}
      c' \nsize \sparsityParam 
      \leqslant| \lambda_{\nclusters} (\probMatrix)| 
      \leqslant |\lambda_{\max}(\probMatrix)| 
      \leqslant C' \nsize \sparsityParam.
    \end{align*}
  \end{lemma}

  \begin{proof}
    We claim that
    \begin{align*}
      \nodeCommunityMatrix^{\T} \nodeCommunityMatrix = 
      \sum_{i = 1}^\nsize
        \nodeCommunityMatrix_i^{\T} \nodeCommunityMatrix_i
      =
      \operatorname{diag} \left(
        |\pureNodesSet_k|
      \right)_{k = 1}^\nclusters
      +
      \sum_{i \not \in \pureNodesSet} \nodeCommunityMatrix_i^{\T} \nodeCommunityMatrix_i.
    \end{align*}
    %
    Besides, from Weyl's inequality and positive definiteness of $\sum_{i \not \in \pureNodesSet} \nodeCommunityMatrix_i^{\T} \nodeCommunityMatrix_i$ we get
    \begin{align*}
      0
      \leqslant
      \lambda_{\min} \left(
        \sum_{i \not \in \pureNodesSet} 
          \nodeCommunityMatrix_i^{\T} \nodeCommunityMatrix_i
      \right)
      \leqslant
      \sum_{i \not \in \pureNodesSet} 
        \lambda_{\max} (\nodeCommunityMatrix_i^{\T} \nodeCommunityMatrix_i)
      \leqslant
      \sum_{i \not \in \pureNodesSet}
        \Vert \nodeCommunityMatrix_i \Vert^2
      \leqslant
      \nsize.
    \end{align*}
    %
    Since from Condition~\ref{cond: theta distribution-a} $|\pureNodesSet_k| = \Theta (\nsize)$ the first statement of the lemma holds. The eigenvalues of $\probMatrix$ we estimate using multiplicative Weyl's inequality for singular numbers:
    \begin{align*}
      |\lambda_k (\nodeCommunityMatrix \communityMatrix \nodeCommunityMatrix^{\T})| 
      = \sigma_{k} (\nodeCommunityMatrix \communityMatrix \nodeCommunityMatrix^{\T}),
      \quad
      \sigma_{\min}^2 (\nodeCommunityMatrix) \sigma_{\min} (\communityMatrix)
      \leqslant
      \sigma_{k} (\nodeCommunityMatrix \communityMatrix \nodeCommunityMatrix^{\T})
      \leqslant
      \sigma_{\max}^2 (\nodeCommunityMatrix) \sigma_{\max} (\communityMatrix).
    \end{align*}
    %
    The previous statement and the fact that $\sigma_{k} (\communityMatrix) = \sparsityParam \sigma_k (\constCommunityMatrix)$ prove the lemma.
  \end{proof}
