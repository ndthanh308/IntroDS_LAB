\documentclass[twoside,11pt]{article}

% math packages
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{easyeqn}
\usepackage{epigraph}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{jmlr2e}
\usepackage{blindtext}


% bibliograthy
%\usepackage{biblatex}
%\addbibresource{overlapping.bib}



\usepackage{xcolor}
\usepackage{enumitem}
% General symbol notations
\newcommand{\T}{\mathrm{T}}
\def\F{\mathrm{F}}
\def\t{\times}

% General math notations
\def\nsize{n}
\def\RR{\mathbb{R}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\eps{\varepsilon}
\def\cv{\mathbf{c}}
\def\ev{\mathbf{e}}
\def\fv{\mathbf{f}}
\def\gv{\mathbf{g}}
\def\nv{\mathbf{n}}
\def\uv{\mathbf{u}}
\def\xv{\mathbf{x}}
\def\yv{\mathbf{y}}
\def\IC{\mathcal{I}}
\def\BC{\mathcal{B}}
\def\alphav{\boldsymbol{\alpha}}
\def\thetav{\boldsymbol{\theta}}
\def\ex{\mathrm{e}}
\def\identity{\mathbf{I}}
\def\totalOnes{\mathbf{J}}
\def\centralMatrix{\mathbf{C}}
\def\indicator[#1]{{\operatorname{I}\left\{#1 \right\}}}
\def\cluster[#1]{\operatorname{cl}(#1)}
\def\predicate[#1]{\varphi\left(#1\right)}
\def\threshold{t}
\def\le{\leqslant}
\def\ge{\geqslant}


% Overlapping community detection notations
\def\nclusters{K}
\def\probMatrix{\mathbf{P}}
\def\adjacencyMatrix{\mathbf{A}}
\def\communityMatrix{\mathbf{B}}
\def\constCommunityMatrix{\bar{\communityMatrix}}
\def\nodeCommunityMatrix{\pmb{\mathrm{\Theta}}}
\def\nodeWeights{\bm{\theta}}
\def\sparsityParam{\rho}
\def\probEigenvectors{\mathbf{U}}
\def\probEigenvalues{\mathbf{L}}
\def\adjacencyEigenvectors{\widehat{\probEigenvectors}}
\def\adjacencyEigenvalues{\widehat{\probEigenvalues}}
\def\noiseMatrix{\mathbf{N}}
\def\noiseVector{\nv}
\def\communityMatrixEstimate{\widehat{\mathbf{B}}}
\def\nodeCommunityMatrixEstimate{\widehat{\nodeCommunityMatrix}}
\def\sparsityParamEstimate{\widehat{\sparsityParam}}
\def\nodeCommunitySet{\bar{\nodeCommunityMatrix}_{\nsize, \nclusters}}
\def\condNumber{\kappa}
\def\displaceMatrix{\mathbf{W}}
\def\pureNodesSet{\mathcal{P}}


% statistics
\def\equalityStatistic{T}
\def\equalityStatisticCenter{\bar{T}}
\def\asymptoticVariance{\mathbf{\Sigma}}
\def\chisquareDistribution[#1]{\chi^2_{#1}}
\def\chiDistribution[#1]{\chi_{#1}}
\def\noncentralChisquare[#1#2]{\chi^2_{#1}(#2)}
\def\displaceMatrixEstimator{\mathbf{\widehat{W}}}
\def\var{\sigma^2}
\def\cov{\operatorname{cov}}
\def\Var{\operatorname{Var}}
\def\estimator[#1]{{\widehat{#1}}}
\def\debiasedEigenvalues{\tilde{\probEigenvalues}}
\def\weakConverges{\overset{d}{\longrightarrow}}
\def\probConverges{\overset{\PP}{\longrightarrow}}
\def\normDistribution{\mathcal{N}}
\def\selectionQuantile{\alpha}
\def\standardDeviation{\operatorname{SD}}
\def\CDF[#1]{\operatorname{CDF}_{#1}}
\def\conditionalThetaProbability{\PP_{\nodeCommunityMatrix}}
\def\conditionalPredicateProbability{\PP_{\varphi}}
\def\weakThetaConstConvergence{\overset{d_{\nodeCommunityMatrix}}{\longrightarrow}}
\def\weakPredicateConstConvergence{\overset{d_{\varphi}}{\longrightarrow}}
\def\equalityStatisticCenterCDF{F_{T}}
\def\simplexProb{\PP_{\bigtriangleup}}
\def\diagAdjecencyMatrix{\mathbf{D}}

\def\avg{\operatorname{avg}}
\newcommand{\penalizer}{a}


% NMF notations
\def\targetMatrix{\mathbf{G}}
\def\basisMatrix{\mathbf{F}}
\def\weightsMatrix{\mathbf{W}}
\def\noisyTargetMatrix{\tilde{\targetMatrix}}
\def\basisMatrixEstimate{\widehat{\mathbf{F}}}

% SPOC with Fan notations
\def\averagingMatrix{\mathbf{R}}
\def\permutationMatrix{\mathbf{\Pi}}
\def\averageMatrixCenter{\bar{\averagingMatrix}}

% Some additional notations for asymptotics of eigenvalues
\def\resolvent{\mathcal{R}}
\def\tResolvent{\mathcal{P}}
\def\meanFactor{A}
\def\pFactor{\widetilde{\mathcal{P}}}
\def\bv{\mathbf{b}}
\def\tr{\operatorname{tr}}
\def\jMatrix{\mathbf{J}}
\def\lMatrix{\mathbf{L}}
\def\qMatrix{\mathbf{Q}}
\def\diag{\operatorname{diag}}
\def\vv{\mathbf{v}}
\def\purePart{\mathbf{S}_1(i, j)}
\def\notPurePart{\mathbf{S}_2(i, j)}
\def\negativePart{\mathbf{S}_3(i, j)}
\newcommand{\bias}{\mathbf{\Omega}}
\def\sumMatrix{\mathbf{H}}

\def\debiasedEigenvalues{\tilde{\probEigenvalues}}
\def\meanEigs{\mathbf{T}}

\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\support}{supp}

% lower bound
\newcommand{\ones}{\mathbf{1}}
\newcommand{\mixedCardinality}{n_{\textbf{mix}}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\probeMatrixFunction}{{\mathtt B}}

\newcommand{\todo}[1]{{\bf \color{red} (TODO) #1}}

\newtheorem{condition}{Condition}
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}{Lemma}
% \newtheorem{assumption}{Assumption}
% \newtheorem{remark}{Remark}
% \newtheorem{definition}{Definition}
% \newtheorem{question}{Question}
% \newtheorem{proposition}{Proposition}


\newtheorem{innercustomthm}{Condition}
\newenvironment{partialCondition}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}


% \setcounter{theorem}{1}

% \allowdisplaybreaks

\ShortHeadings{Optimal Estimation in MMSB}{Noskov and Panov}
\firstpageno{1}

\begin{document}

\title{Optimal Estimation in Mixed-Membership \\ Stochastic Block Models}

\author{\name Fedor Noskov \email  fnoskov@hse.ru \\
       \addr HSE University, Moscow, Russia%and Skoltech, Moscow, Russia
       % \addr HSE University, \\
       % Institute for Information Transmission Problems RAS, \\
       % and Moscow Institute of Physics and Technology, Moscow, Russia
       \AND
       \name  Maxim Panov \email  maxim.panov@tii.ae \\
       \addr Technology Innovation Institute, Abu Dhabi, UAE}

\editor{}

\maketitle


\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by~\citet{Airoldi2008}. MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
\end{abstract}

\begin{keywords}
  minimax bounds, mixed-membership stochastic block model, spectral estimators
\end{keywords}


\section{Introduction}
\label{section: introduction}
  Over the past ten years, network analysis has gained significant importance as a research field, driven by its numerous applications in various disciplines, including social sciences~\citep{jin_mixed_2023}, computer sciences~\citep{bedru_big_2020}, genomics~\citep{li_application_2018}, ecology~\citep{geary_guide_2020}, and many others. As a result, a growing body of literature has been dedicated to fitting observed networks with parametric or non-parametric models of random graphs~\citep{borgs_graphons_2017, goldenberg_survey_2010}. In this work, we are focusing on studying some particular parametric graph models, while it is worth mentioning {\it graphons}~\citep{lovasz_large_2012} as the most common non-parametric model.

  The simplest parametric model in network analysis is the Erdős-Rényi model~\citep{erdos1960evolution}, which assumes that edges in a network are generated independently with a fixed probability $p$, the single parameter of the model. The stochastic block model (SBM; \citealt{holland_stochastic_1983}) is a more flexible parametric model that allows for communities or groups within a network. In this model, the network nodes are partitioned into $\nclusters$ communities, and the probability $p_{ij}$ of an edge between nodes $i$ and $j$ depends on only what communities these nodes belong to. The mixed-membership stochastic block model (MMSB; \citealt{Airoldi2008}) is a stochastic block model generalization, allowing nodes to belong to multiple communities with varying degrees of membership. This model is characterized by a set of community membership vectors, representing the probability of a node belonging to each community. The MMSB model is the focus of research in the present paper.

  In the MMSB model, for each node $i$, we assume that there exists a vector $\nodeWeights_i \in [0, 1]^\nclusters$ drawn from the $(\nclusters - 1)$-dimensional simplex that determines the community membership probabilities for the given node. Then, a symmetric matrix $\communityMatrix \in [0, 1]^{\nclusters \times \nclusters}$ determines the relations inside and between communities. According to the model, the probability of obtaining the edge between nodes $i$ and $j$ is $\nodeWeights_i^\T \communityMatrix \nodeWeights_j$. Importantly, in the considered model, we allow for self-loops.

  More precisely, let us observe the adjacency matrix of the undirected unweighted graph $\adjacencyMatrix \in \{0, 1\}^{\nsize \t \nsize}$. 
  Under MMSB model $\adjacencyMatrix_{i j} = Bern(\probMatrix_{i j})$ for $1 \le i \le j \le \nsize$, where $\probMatrix_{ij} = \nodeWeights_i^{\T} \communityMatrix \nodeWeights_j = \sparsityParam \, \nodeWeights_i^{\T} \constCommunityMatrix \nodeWeights_j$. Here we denote $\communityMatrix = \sparsityParam \constCommunityMatrix$ with $\constCommunityMatrix \in [0, 1]^{\nclusters \times \nclusters}$ being a matrix with the maximum value equal to $1$ and $\sparsityParam \in (0, 1]$ being the sparsity parameter that is crucial for the properties of this model. Stacking vectors $\nodeWeights_i$ into matrix $\nodeCommunityMatrix$, $\nodeCommunityMatrix_i = \nodeWeights_i^\T$, we get the following formula for the matrix of edge probabilities $\probMatrix$:
  \begin{align*}
    \probMatrix = \nodeCommunityMatrix \communityMatrix \nodeCommunityMatrix^{\T} = \sparsityParam \, \nodeCommunityMatrix \constCommunityMatrix \nodeCommunityMatrix^{\T}.
  \end{align*}
  %
  In this work, we aim to propose the minimax-optimal parameter estimation algorithm for the mixed-membership stochastic block model. For any estimators $\estimator[\nodeWeights]_i$ and $\estimator[\communityMatrix]$, \citet{Jin2017} and \citet{Marshakov2018} established lower bounds on the mean squared risk for vectors $\nodeWeights_i$, $i \in [\nsize]$, and matrix $\communityMatrix$. While the optimal estimators for $\nodeWeights_i$ were constructed~\citep{Panov2018,jin_mixed_2023}, there is a polynomial gap between the lower bound and known theoretical guarantees for estimators of $\communityMatrix$.
  

\paragraph{Related works.} 
  A large body of literature exists on parameter estimation in various parametric graph models. The most well-studied is the Stochastic Block Model, but methods for different graph models can share the same ideas. The maximum likelihood estimator is consistent for both SBM and MMSB, but it is intractable in practice~\citep{celisse_2012, huang_2020}. Several variational algorithms were proposed to overcome this issue; see the original paper of~\citet{Airoldi2008}, the survey of~\citet{lee_review_2019} and references therein. In the case of MMSB, the most common prior on vectors $\nodeWeights_i$, $i \in [\nsize]$, is Dirichlet distribution on a $(\nclusters - 1)$-dimensional simplex with unknown parameter $\bm{\alpha}$. Unfortunately, a finite sample analysis of convergence rates for variational inference is hard to establish. In the case of SBM, it is known that the maximizer of the evidence lower bound over a variational family is optimal~\citep{gaucher_optimality_2021}. Still, there are no theoretical guarantees that the corresponding EM algorithm converges to it.

  Other algorithms do not require any specified distribution of membership vectors $\nodeWeights_i$. For example, spectral algorithms work well under the general assumption of {\it identifiability} of communities~\citep{Mao17}. In the case of SBM, it is proved that they achieve optimal estimation bounds, see the paper by~\citet{Yun2015OptimalCR} and references therein. 
  These results motivated several authors to develop spectral approaches for MMSB. For example, almost identical and simultaneously proposed algorithms SPOC~\citep{Panov2018}, SPACL~\citep{mao2021estimating} and Mixed-SCORE~\citep{jin_mixed_2023} optimally reconstruct $\nodeWeights_i$ under the mean-squared error risk~\citep{jin_mixed_2023}. For the matrix $\communityMatrix$, their proposed estimators $\estimator[\communityMatrix]$ achieves the following error rate:
  \begin{align}
    \min_{\permutationMatrix \in \mathbb{S}_\nclusters} \Vert \estimator[\communityMatrix] - \permutationMatrix \communityMatrix \permutationMatrix^{\T} \Vert_{\F} \lesssim C(\nclusters) \sqrt{\frac{\sparsityParam \log \nsize}{\nsize}}
  \label{eq:spoc_rate}
  \end{align}
  with high probability, where $C(\nclusters)$ is some constant depending on $\nclusters$. Here $\mathbb{S}_\nclusters$ stands for the set of $\nclusters \t \nclusters$ permutation matrices, and $\Vert \cdot \Vert_{\F}$ denotes the Frobenius norm. The algorithm by~\citet{Anandkumar13}, which uses the tensor-based approach, provides the same rate. But the latter has high computational costs and assumes that $\nodeWeights_i$ is drawn from the Dirichlet distribution. Finally, \citet{Mao17} obtained the estimator for diagonal $\communityMatrix$ with convergence rate
  \begin{align*}
    \min_{\permutationMatrix \in \mathbb{S}_\nclusters} \Vert \estimator[\communityMatrix] - \permutationMatrix \communityMatrix \permutationMatrix^{\T} \Vert_{\F} \lesssim \frac{\nclusters^3 \log \nsize}{\sqrt{\nsize}}
  \end{align*}
  with high probability. However, their algorithm can be modified for arbitrary $\communityMatrix$.

  None of the algorithms above match the known lower bound over $\nsize$ on the estimator of $\communityMatrix$ in the Frobenius norm. In his MSc thesis~\citep{Marshakov2018}, E. Marshakov showed that
  \begin{align}
  \label{eq: B lower bound}
    \inf_{\communityMatrixEstimate} 
    \sup_{\communityMatrix \in \bar{\mathcal{B}}_\sparsityParam}
    \PP \left(
       \Vert
        \communityMatrixEstimate
        -
        \communityMatrix
     \Vert_{\F}
      \geqslant
      C_{\communityMatrix}
      \frac{
        \sqrt{
          \sparsityParam
        }
      }{\nsize}
    \right)
    >
    0.15,
  \end{align}
  where
  \begin{align}
    \bar{\mathcal{B}}_{\rho} = \left\{
        \rho \constCommunityMatrix 
        \mid
        \constCommunityMatrix \in [0, 1]^{\nclusters \times \nclusters}, \; 
        \max_{k k'} \constCommunityMatrix_{k k'} = 1,
        \forall k \neq k' \; 
        \constCommunityMatrix_{kk} > \constCommunityMatrix_{k' k}
    \right\},
  \end{align}
  the infimum is taken over all possible estimators, and $C_\communityMatrix$ is some constant depending on $\nclusters$. In what follows, we slightly modify his result to establish the same lower bound for the following loss function: 
  \begin{align*}
    \mathcal{L}(\estimator[\communityMatrix], \communityMatrix) = \min_{\permutationMatrix \in \mathbb{S}_\nclusters} \Vert \estimator[\communityMatrix] - \permutationMatrix \communityMatrix \permutationMatrix^\T \Vert_{\F}.
  \end{align*}
  
  It is worth mentioning models that also introduce overlapping communities but in a distinct way from MMSB and estimators for them. One example is OCCAM~\citep{zhang2020} which is similar to MMSB but uses $l_2$-normalization for membership vectors. Another example is the Stochastic Block Model with Overlapping Communities~\citep{KAUFMANN20183,Peixoto2015}.

\paragraph{Contributions.}  
  We aim to propose the estimator $\estimator[\communityMatrix]$ that it is computationally tractable and achieves the following error bound:
  \begin{align}
  \label{eq: desired rate}
    \min_{\permutationMatrix \in \mathbb{S}_\nclusters}
      \Vert 
        \estimator[\communityMatrix]
        - 
        \permutationMatrix \communityMatrix \permutationMatrix^\T
      \Vert_{\F}
      \le
      C(\nclusters) \frac{\sqrt{\sparsityParam}}{\nsize}
  \end{align}
  with high probability. This paper focuses on optimal estimation up to dependence on $\nclusters$, while optimal dependence on $\nclusters$ remains an interesting open problem.

  We need to impose some conditions to establish the required upper bound. We want these conditions to be non-restrictive and, ideally, satisfied in practice. The question of the optimality of proposed estimates achieving the rate~\eqref{eq: desired rate} is central to this research. In what follows, we give a positive answer to this question under a fairly general set of conditions.

  %\subsection{Contribution and organization of the paper} 

  %In this paper, we construct an estimator that achieves the rate~\eqref{eq: desired rate}.
  The rest of the paper is organized as follows. We introduce a new \textit{SPOC++} algorithm in Section~\ref{section: algorithm}. Then, in Section~\ref{section: provable guarantees}, we establish the convergence rate for the proposed algorithm and show its optimality. Finally, in Section~\ref{section: numerical experiments}, we conduct numerical experiments that illustrate our theoretical results. Section~\ref{section: discussion} concludes the study with a discussion of the results and highlights the directions for future work. All proofs of ancillary lemmas can be found in Appendix.


% \section{Model and previous research}
%   We consider a random graph with each edge in this graph appearing with some probability. Moreover, we suppose that graph nodes are grouped in some communities. The probability of an edge between two nodes is defined by a strength of connectivity between the corresponding communities and and by a strength of their affiliation with this communities.

%   More precisely, we consider the Mixed Membership Stochastic Block model~\citep{Airoldi2008}. We assume that the considered graph has $\nclusters$ communities and define by $\communityMatrix \in [0, 1]^{\nclusters \times \nclusters}$ the matrix which is responsible for connectivity between communities. 
%   In the following, it will be convenient for us to write $\communityMatrix = \sparsityParam \constCommunityMatrix$ where $\sparsityParam$ is responsible for the sparsity of the graph and $\constCommunityMatrix \in [0, 1]^{\nclusters \times \nclusters}$ is the matrix with the largest element equal to $1$. For each node in a graph, we represent its membership in communities by a vector $\thetav$ from $\nclusters$-dimensional standard simplex. Stacking these vectors we get communities' membership matrix $\nodeCommunityMatrix \in [0, 1]^{\nsize \times \nclusters}$ where $\nsize$ is the number of nodes in the graph. Thus, the probability of connection between two nodes is represented by a quadratic form $\thetav^{\T} \communityMatrix \thetav$. Then, we obtain the matrix $\probMatrix = \nodeCommunityMatrix \communityMatrix \nodeCommunityMatrix^{\T}$ of edge probabilities for the considered graph. Subsequently, we observe a matrix $\adjacencyMatrix$ with each element $\adjacencyMatrix_{ij}$ being a random variable from Bernoulli distribution: 
%   \begin{align}
%     \adjacencyMatrix_{ij} \sim Bernoulli(\probMatrix_{ij}), ~~ 1 \le i < j \le \nsize.
%   \end{align}
%   %
%   The considered model has some close relations to other models. First of all, if each $\thetav$ has only one non-zero entry equal to $1$ then we obtain the celebrated stochastic block model~\citep{Lei2015}. Then, one can consider a prior Dirichlet distribution on $\nodeCommunityMatrix_i$: $\nodeCommunityMatrix_i \sim \mathrm{Dirichlet}(\alpha)$ for some $\alpha$. Such a Bayesian model was suggested in the original paper~\citep{Airoldi2008} and considered in several subsequent works~\citep{Anandkumar13,Mao17}. However, one doesn't need to limit the distribution choice and can consider $\nodeCommunityMatrix_i$ to be independently generated from some distribution on a standard simplex with non-zero mass in its vertices. %Moreover, the vectors $\nodeCommunityMatrix_i$ can be considered even fixed unknown quantities 

%   From the statistical point of view, one can target a problem of estimation for matrices $\communityMatrix$ and $\nodeCommunityMatrix$. The problem of estimation of $\nodeCommunityMatrix$ is well-studied and can be considered solved. Even in the most general case of degree-corrected mixed membership stochastic block model under the fairly general assumptions it holds that lower and upper bounds for $\nodeCommunityMatrix$'s estimation error $\max_{1 \leqslant i \leqslant \nsize} \Vert \estimator[{\nodeCommunityMatrix}]_i - \nodeCommunityMatrix_i \Vert_1$ match and have an order of $(\nsize \sparsityParam)^{-1/2}$ (see~\citep{Jin2017}). However, for the matrix $\communityMatrix$ the situation is different. Let us review several approaches:
%   \begin{enumerate}
%     \item \textit{Bayesian approach from the original paper~\citep{Airoldi2008}}. The vectors $\nodeCommunityMatrix_i$ are considered as latent variables generated from Dirichlet distribution. The EM-algorithm is used to estimate $\communityMatrix$ as parameter and rows of $\nodeCommunityMatrix$ as latent variables. The convergence properties of the resulting estimates are not studied in this paper.
    
%     \item \textit{Tensor approach from paper~\citep{Anandkumar13}}. This approach delivers convergence rate of the error $\Vert \estimator[\communityMatrix] - \communityMatrix \Vert_F$ being proportional to $\sqrt{\sparsityParam / \nsize}$. However, such rate requires additional assumptions on the parameter $\alpha$ of Dirichlet distribution. Besides, this method has high computational complexity.
    
%     \item \textit{Spectral embedding approach}. Since eigenvectors of $\probMatrix$ lie in the $\nclusters$-dimensional simplex (see Section~\ref{section:spoc} for details), there are several methods which use this fact including SPOC~\citep{Panov2018}, Mixed-SCORE~\citep{Jin2017} and GeoMNF~\citep{Mao17}. %Moreover, the previous approach can be mentioned here too.
%     All of them provide an upper bound on $\Vert \estimator[\communityMatrix] - \communityMatrix \Vert_F$ being proportional to $\sqrt{\sparsityParam/\nsize}$. %Besides, SPOC requires large amount of pure nodes, but its convergence rate can be improved by the factor $\sqrt{\sparsityParam}$.
%   \end{enumerate}

%   Thus, all the previously proposed estimators of $\communityMatrix$ guarantee error of order at least $\sqrt{\sparsityParam / \nsize}$. However, no matching lower bound is known in the literature. The best existing lower bound~\citep{Marshakov2018} has the following form:
%   \begin{align}
%   \label{eq: B lower bound}
%     \inf_{\communityMatrixEstimate} 
%     \sup_{\communityMatrix \in \bar{\mathcal{B}}_\sparsityParam}
%     \PP \left(
%       \left \Vert
%         \communityMatrixEstimate
%         -
%         \communityMatrix
%       \right \Vert_2
%       \geqslant
%       C_{\communityMatrix}
%       \frac{
%         \sqrt{
%           \sparsityParam
%         }
%       }{\nsize}
%     \right)
%     >
%     0.15,
%   \end{align}
%   where
%   \begin{align}
%     \bar{\mathcal{B}}_{\rho} = \left\{
%       \rho \constCommunityMatrix 
%       \mid
%       \constCommunityMatrix \in [0, 1]^{\nclusters \times \nclusters}, \; 
%       \max_{k k'} \constCommunityMatrix_{k k'} = 1,
%       \forall k \neq k' \; 
%       \constCommunityMatrix_{kk} > \constCommunityMatrix_{k' k}
%     \right\},
%   \end{align}
%   and infimum is taken over all possible estimators and $C_\communityMatrix$ is some constant. It leads us to the following question: is it possible to construct an estimator of $\communityMatrix$ such that it delivers the lower bound~\eqref{eq: B lower bound}? It will require construction of an estimator that improves by $\nsize^{1/2}$ over the existing approaches. %Otherwise, can we argue that the lower bound~\eqref{eq: B lower bound} is not tight?

%   In this paper, we give a positive answer on the question above and propose a new method which provides a convergence rate of order $\sqrt{\rho} / \nsize$. We should note that our approach requires certain restrictions on the model parameters. However, we strongly believe that they can be removed in the course of further research.


\section{Beyond successive projections for parameter estimation in MMSB}%{Algorithm}
\label{section: algorithm}

\subsection{SPOC algorithm}
\label{section:spoc}
  Various estimators of $\communityMatrix$ and $\nodeCommunityMatrix$ were proposed in previous works~\citep{Mao17,Panov2018,jin_mixed_2023}. In this work, we will focus on the \textit{Successive Projections Overlapping Clustering (SPOC)} algorithm~\citep{Panov2018} that we present in Algorithm~\ref{algo: spoc}. However, we should note that any ``vertex hunting'' method~\citep{jin_mixed_2023} can be used instead of a successive projections algorithm as a base method for our approach.

  The main idea of SPOC is as follows. Consider a $\nclusters$-eigenvalue decomposition of $\probMatrix = \probEigenvectors \probEigenvalues \probEigenvectors^{\T}$. Then, there exists a full-rank matrix $\basisMatrix$ such that $\probEigenvectors = \nodeCommunityMatrix \basisMatrix$ and $\communityMatrix = \basisMatrix \probEigenvalues \basisMatrix^{\T}$. The proof of this statement can be found, for example, in~\citep{Panov2018}. Hence, if we build an estimator of $\basisMatrix$ and $\probEigenvalues$, we immediately get the estimator of $\communityMatrix$. Besides, since $\probEigenvectors = \nodeCommunityMatrix \basisMatrix$, rows of $\probEigenvectors$ lie in a simplex. The vertices of this simplex are rows of matrix $\basisMatrix$. Consequently, we may estimate $\probEigenvectors$ by some estimator $\adjacencyEigenvectors$ and find vertices of the simplex using rows of $\adjacencyEigenvectors$.

  % Figure environment removed
  
  \begin{algorithm}[t!]
  \caption{SPA~\citep{Mizutani2016}}
  \label{algo: spa}
  \begin{algorithmic}[1]
    \Require{Matrix $\mathbf{V} \in \RR^{\nsize \times \nclusters}$ and integer $r \le \nclusters$}
    \Ensure{Set of indices $J \subset [\nsize]$}
    \State Set $\mathbf{S}^0 = \mathbf{V}$, $J_0 = \varnothing$
    \For{$t = 1 \ldots r$}
        \State Find $j_t = \arg \min_{i \in [\nsize]} \Vert \mathbf{S}^{t - 1}_i \Vert$
        \State Project rows of $\mathbf{S}^{t - 1}$ on the plane orthogonal to $\mathbf{S}^{t - 1}_{j_t}$:
        \begin{align*}
            \mathbf{S}^{t} = \mathbf{S}^{t - 1} \left(\identity_\nclusters - \frac{\mathbf{S}^{t - 1}_{j_t} (\mathbf{S}^{t - 1}_{j_t})^{\T}}{\Vert \mathbf{S}^{t - 1}_{j_t} \Vert^2_2} \right).
        \end{align*}
        \State Add $j_t$ to the set $J$: $J_t = J_{t - 1} \cup \{j_t\}$.
    \EndFor
    \State \Return $J_t$
  \end{algorithmic}
  \end{algorithm}

  The most natural way to estimate $\probEigenvectors$ and $\probEigenvalues$ is to use a $\nclusters$-eigenvalue decomposition of the adjacency matrix $\adjacencyMatrix \simeq \adjacencyEigenvectors \adjacencyEigenvalues \adjacencyEigenvectors^{\T}$, where columns of $\adjacencyEigenvectors$ are first $\nclusters$ eigenvectors of $\adjacencyMatrix$ and $\adjacencyEigenvalues$ is the diagonal matrix of eigenvalues. The rows of matrix $\adjacencyEigenvectors$ lie in a perturbed version of the simplex corresponding to matrix $\probEigenvectors$, see illustration on Figure~\ref{fig: simplex illustration}. To find vertices of the perturbed simplex, we run \textit{Successive Projections Algorithm (SPA)}, see Algorithm~\ref{algo: spa}. The resulting SPOC algorithm is given in Algorithm~\ref{algo: spoc}.

  \begin{algorithm}[t!]
  \caption{SPOC}
  \label{algo: spoc}
    \begin{algorithmic}[1]
    \Require{Adjacency matrix $\adjacencyMatrix$, number of communities $\nclusters$.}
    \Ensure{Estimators $\nodeCommunityMatrixEstimate$, $\communityMatrixEstimate$}
    
    \State Get the rank-$\nclusters$ eigenvalue decomposition $\adjacencyMatrix \simeq \adjacencyEigenvectors \adjacencyEigenvalues \adjacencyEigenvectors^{\T}$
    \State Run SPA algorithm with input $(\adjacencyEigenvectors, \nclusters)$, which outputs the set of indices $J$ of cardinality $\nclusters$
    \State $\basisMatrixEstimate = \adjacencyEigenvectors[J, :]$
    \State $\communityMatrixEstimate = \basisMatrixEstimate \adjacencyEigenvalues \basisMatrixEstimate^{\T}$
    \State $\nodeCommunityMatrixEstimate = \adjacencyEigenvectors \basisMatrixEstimate^{-1}$
    \end{algorithmic}
  \end{algorithm}

  However, the SPOC-based estimator $\communityMatrixEstimate$ does not allow for obtaining the optimal rate of estimation~\eqref{eq: desired rate}, only achieving the suboptimal one~\eqref{eq:spoc_rate}. The nature of the problem is in the SPA algorithm whose error is driven by the properties of rows of matrix $\adjacencyEigenvectors$ that might be too noisy. In what follows, we will provide a noise reduction procedure for it.


\subsection{Denoising via averaging}
\label{subsection: averaging}
  
  \begin{algorithm}[t!]
  \caption{Averaging procedure}
  \label{algo: averaging procedure}
    \begin{algorithmic}[1]
      \Require{Matrix of eigenvectors $\adjacencyEigenvectors$, diagonal matrix of eigenvalues $\adjacencyEigenvalues$, estimator $\debiasedEigenvalues$, number of communities $\nclusters$, threshold $\threshold_\nsize$, indices $J$, regularization parameter $a$}
      \Ensure{$\basisMatrixEstimate$ --- an estimator of the matrix $\basisMatrix$}.
      
      % \State Calculate the initial estimator of noise matrix $\estimator[\displaceMatrix]_0 = \adjacencyMatrix - \adjacencyEigenvectors \adjacencyEigenvalues \adjacencyEigenvectors^{\T}$.
      
      \State Calculate an estimator $\estimator[\displaceMatrix] =\adjacencyMatrix - \adjacencyEigenvectors \debiasedEigenvalues \adjacencyEigenvectors^{\T}$.
      
      \For{$j$ in $J$}
          \For{$j'= 1$ to $\nsize$}
              \State Calculate covariance matrix estimator 
              \begin{equation}
                \estimator[\asymptoticVariance](j, j') = 
                \debiasedEigenvalues^{-1} 
                \adjacencyEigenvectors^{\T} 
                \left(
                  \operatorname{diag}(
                    \estimator[\displaceMatrix]_j^2 + \estimator[\displaceMatrix]_{j'}^2
                  ) - 
                  \estimator[\displaceMatrix]_{j j'}^2 
                  (\ev_{j} \ev_{j'}^{\T} + \ev_{j'} \ev_j^{\T}) 
                \right) 
                \adjacencyEigenvectors 
                \debiasedEigenvalues^{-1},
              \label{eq:cov_matrix_est}
              \end{equation}
              \quad \quad \quad where the square is an element-wise operation.
              
              \State Calculate statistic $\estimator[\equalityStatistic]_{j j'}^{\penalizer} = (\adjacencyEigenvectors_{j} - \adjacencyEigenvectors_{j'}) \left(\estimator[\asymptoticVariance](j, j') + \penalizer \identity \right)^{-1} (\adjacencyEigenvectors_{j} - \adjacencyEigenvectors_{j'})^{\T}$.
          \EndFor
          
          \State Select nodes $\IC_j = \{j' \in [\nsize] \mid \equalityStatistic_{j j'} < \threshold_\nsize\}$
          \State Reduce bias in estimation of $\probEigenvectors:$
          \begin{align}
            \label{eq:diag_adj_matrix}
            \diagAdjecencyMatrix & = \operatorname{diag}\left( \sum_{t = 1}^\nsize \adjacencyMatrix_{it} \right)_{i = 1}^\nsize,
            \\
            \tilde{\probEigenvectors}_{ik} & = \adjacencyEigenvectors_{ik} \left(1 - \frac{\diagAdjecencyMatrix_{ii} - 3/2 \sum_{j = 1}^\nsize \diagAdjecencyMatrix_{jj} \adjacencyEigenvectors_{jk}^2}{\adjacencyEigenvalues^2_{k' k'}} \right) - \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \cdot \sum_{j = 1}^{\nsize} \frac{\diagAdjecencyMatrix_{j j} \adjacencyEigenvectors_{j k'} \adjacencyEigenvectors_{j k}}{\adjacencyEigenvalues_{k k}^2}.
          \label{eq:adjusted_eigenvector}
          \end{align}
          \State Average rows of matrix $\tilde{\probEigenvectors}$ over the set $\IC_j$ and write result into vector $\estimator[\fv](j)$:
          \begin{align}
            \estimator[\fv]^{\T}(j) = \frac{1}{|\IC_j|} \sum_{j' \in \IC_j} \tilde{\probEigenvectors}_{j'}.
          \end{align}
      \EndFor
       
      \State Stack together row-vectors $\estimator[\fv]^{\T}(j)$ into matrix $\basisMatrixEstimate$:
      
      \begin{align}
        \basisMatrixEstimate = \left(\estimator[\fv]^{\T}(j) \right)_{j \in J}.
      \end{align}
      
      \State Return matrix $\basisMatrixEstimate$
    \end{algorithmic}
  \end{algorithm}

  The most common denoising tool is averaging because it decreases the variance of i.i.d. variables by $\sqrt{N}$ where $N$ is a sample size. In this work, our key idea is to reduce the error rate of the estimation of the matrix $\basisMatrix$ by $\sqrt{\nsize}$ times through averaging $\Theta(\nsize)$ rows of $\adjacencyEigenvectors$. The key contribution of this work is in establishing the procedure for finding the rows similar to the rows of $\basisMatrix$ and dealing with their weak dependence on each other. 

  We call the $i$-th node ``pure'' if the corresponding row $\nodeCommunityMatrix_i$ of the matrix $\nodeCommunityMatrix$ consists only of zeros except for one particular entry, equal to $1$. Thus, for the pure node $\probEigenvectors_i = \basisMatrix_k$ for some $k \in [\nclusters]$. If we find many pure nodes and average corresponding rows of $\adjacencyEigenvectors$, we can get a better estimator of rows of $\basisMatrix$ and, consequently, matrix $\communityMatrix$. %Thus, the key condition leading to the success of our algorithm is the existence of $\Theta(\nsize)$ pure nodes.

  To find pure nodes, we employ the following strategy. In the first step, we run the SPA algorithm and obtain one vertex per community. Below, we prove under some conditions that SPA chooses ``almost'' pure nodes with high probability. In the second step, we detect the nodes which are ``similar'' to the ones selected by SPA and use the resulting pure nodes set for averaging. The complete averaging procedure is given in Algorithm~\ref{algo: averaging procedure}, while we discuss its particular steps below.

  The choice of similarity measure for detection on similar nodes is crucial for our approach. \citet{Fan2019_SIMPLE} provide a statistical test for equality of node membership vectors $\nodeCommunityMatrix_i$ and $\nodeCommunityMatrix_j$ based on the statistic $\equalityStatistic_{ij}$. This statistic is closely connected to the displace matrix
  \begin{align*}
    \displaceMatrix = \adjacencyMatrix - \probMatrix
  \end{align*}
  and covariance matrix $\asymptoticVariance(i, j)$ of the vector $(\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1}$:
  \begin{align*}
    \asymptoticVariance(i, j) = \EE \bigl [ \probEigenvalues^{-1} \probEigenvectors^\T (\displaceMatrix_i - \displaceMatrix_j)^\T (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1} \bigr ].
  \end{align*}
  %
  Thus, the test statistic $\equalityStatistic_{ij}$ is given by
  \begin{align*}
    \equalityStatistic_{ij}
    = 
    (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j) 
    \asymptoticVariance(i, j)^{-1}
    (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j)^{\T}.
  \end{align*}
  %
  However, we do not observe the matrix $\asymptoticVariance(i, j)$. Instead, we use its plug-in estimator $\estimator[\asymptoticVariance](i, j)$ which is described below in Algorithm~\ref{algo: averaging procedure}, see equation~\eqref{eq:cov_matrix_est}. Thus, the resulting test statistic is given by
  \begin{align}
    \estimator[\equalityStatistic]_{ij}
    = 
    (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j)
    \estimator[\asymptoticVariance](i, j)^{-1}
    (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j)^{\T}.
  \end{align}

  \cite{Fan2019_SIMPLE} prove that under some conditions $\equalityStatistic_{ij}$ and $\estimator[\equalityStatistic]_{ij}$ both converge to non-central chi-squared distribution with $\nclusters$ degrees of freedom  and center
  \begin{align}
    \equalityStatisticCenter_{ij} = (\probEigenvectors_i - \probEigenvectors_j)
    \asymptoticVariance(i, j)^{-1}
    (\probEigenvectors_i - \probEigenvectors_j)^{\T}.
  \end{align}
  %
  Thus, $\estimator[\equalityStatistic]_{ij}$ can be considered as a measure of closeness for two nodes. For each node $i$ we can define its neighborhood $\IC_i$ as all nodes $j$ such that $\estimator[\equalityStatistic]_{ij}$ is less than some threshold $\threshold_\nsize$: $\IC_i = \{j \in [\nsize] \mid \estimator[\equalityStatistic]_{ij} < \threshold_\nsize\}$.

  To evaluate $\equalityStatisticCenter_{ij}$, one needs to invert the matrix $\asymptoticVariance(i, j)$. However, matrix $\asymptoticVariance(i, j)$ can be degenerate in the general case. Nevertheless, one can specify some conditions on matrix $\communityMatrix$ to ensure it is well-conditioned. To illustrate it, let us consider the following proposition.
  
  \begin{proposition}
  \label{proposition: when penalizer can be zero}
   Let Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a}, defined below, hold. Assume additionally that entries of the matrix $\communityMatrix$ are bounded away from 0 and 1. Then there exist constants $C_1, C_2$ such that for large enough $\nsize$ it holds
   \begin{align*}
        \frac{C_1}{\nsize^2 \sparsityParam} \le \lambda_{\min}(\asymptoticVariance(i, j)) \le \lambda_{\max}(\asymptoticVariance(i, j)) \le \frac{C_2}{\nsize^2 \sparsityParam}
   \end{align*}
   for any nodes $i$ and $j$.
  \end{proposition}
  
  However, the condition on the entries of the community matrix above might be too strong, while we only need concentration bounds on $\estimator[\equalityStatistic]_{ij}$. To not limit ourselves to matrices $\communityMatrix$ with no zero entries, we consider a regularized version of $\estimator[\equalityStatistic]_{ij}$:
  \begin{align*}
    \estimator[\equalityStatistic]_{ij}^\penalizer = (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j) 
    \left(\estimator[\asymptoticVariance](i, j) + \penalizer \identity\right)^{-1}
    (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j)^{\T}
  \end{align*}
  for some $a > 0$. When $\penalizer = \Theta(\nsize^{-2} \sparsityParam^{-1})$, the statistic $\estimator[\equalityStatistic]^\penalizer_{ij}$ %does not converge to $\chi^2$-distribution as $\nsize$ tends to infinity, but it
  concentrates around 
  \begin{align*}
    \equalityStatisticCenter^a_{ij} = (\probEigenvectors_i - \probEigenvectors_j)
    \left( \asymptoticVariance(i, j) + \penalizer \identity \right)^{-1}
    (\probEigenvectors_i - \probEigenvectors_j)^{\T}.
  \end{align*}
  %
  Practically, if $\estimator[\asymptoticVariance](i, j)$ is well-conditioned, one can use the statistic $\estimator[\equalityStatistic]_{ij}$ without any regularization. In other words, all of our results still hold if $\penalizer = 0$ and $\lambda_{\min}\bigl(\asymptoticVariance(i, j)\bigr) \ge C \nsize^{-2} \sparsityParam^{-1}$ for all $i, j$. But to not impose additional assumptions on either matrix $\communityMatrix$ or $\nodeCommunityMatrix$, in what follows we will use $\estimator[\equalityStatistic]^\penalizer_{ij}$ with $\penalizer = \Theta(\nsize^{-2} \sparsityParam^{-1})$. 


\subsection{Estimation of eigenvalues and eigenvectors}
  It turns out that the eigenvalues $\adjacencyEigenvalues$ and eigenvectors $\adjacencyEigenvectors$ of $\adjacencyMatrix$ are not optimal estimators of $\probEigenvalues, \probEigenvectors$ respectively. The asymptotic expansion of $\probEigenvectors$ described in Lemma~\ref{lemma: eigenvector power expansion} suggests a new estimator $\tilde{\probEigenvectors}$ that suppresses some high-order terms in the expansion. For the exact formula, see equation~\eqref{eq:adjusted_eigenvector} in Algorithm~\ref{algo: averaging procedure}. Similarly, a better estimator $\debiasedEigenvalues$ of eigenvalues exists; see equation~\eqref{eq:improved_eigenvalues} in Algorithm~\ref{algo: general scheme}.

  Proposed estimators admit better asymptotic properties than $\adjacencyEigenvalues$ and $\adjacencyEigenvectors$, see Lemma~\ref{lemma: averaging lemma} and~\ref{lemma: debaised eigenvalues behaviour} below. In particular, it allows us to achieve the convergence rate~\eqref{eq: desired rate} instead of $1 / \nsize$.

  \begin{algorithm}[t!]
    \caption{SPOC++}
    \label{algo: general scheme}
    \begin{algorithmic}[1]
      \Require{Adjacency matrix $\adjacencyMatrix$, threshold $\threshold_\nsize$, regularization parameter $a$}
      \Ensure{Estimators $\nodeCommunityMatrixEstimate$, $\communityMatrixEstimate$}
      
      \State Estimate rank with $\estimator[\nclusters] = \max \bigl \{j \mid \lambda_j(\adjacencyMatrix) \ge 2 \max_i \sqrt{\sum\nolimits_{t = 1}^\nsize \adjacencyMatrix_{it} \log^2 \nsize} \bigr \}$
      
      \State Get the rank-$\estimator[\nclusters]$ eigenvalue decomposition of $\adjacencyMatrix \simeq \adjacencyEigenvectors \adjacencyEigenvalues \adjacencyEigenvectors^{\T}$
      \State Run SPA algorithm with input $(\adjacencyEigenvectors, \estimator[\nclusters])$, which outputs the set of indices $J$ of cardinality $\nclusters$
      % \State Generate some averaging matrix $\averagingMatrix$ using any information from previous steps and satisfying condition~\ref{cond: selection quantile}.
      \State Calculate the estimator of the eigenvalues' matrix:
      \begin{equation}
        \debiasedEigenvalues_{kk} = \left[ \frac{1}{\adjacencyEigenvalues_{kk}} + \frac{\sum_{i = 1}^{\nsize} \adjacencyEigenvectors_{ik}^2 \cdot \sum_{t = 1}^{\nsize} \adjacencyMatrix_{it}}{\adjacencyEigenvalues_{kk}^3}\right]^{-1}.
      \label{eq:improved_eigenvalues}
      \end{equation}
      \State $\basisMatrixEstimate = \avg(\adjacencyEigenvectors, \adjacencyEigenvalues, \debiasedEigenvalues, \threshold_\nsize, J, a)$, where $\avg$ is the averaging procedure described in Algorithm~\ref{algo: averaging procedure}.
      \State $\communityMatrixEstimate = \basisMatrixEstimate \debiasedEigenvalues \basisMatrixEstimate^{\T}$
      \State $\nodeCommunityMatrixEstimate = \adjacencyEigenvectors \basisMatrixEstimate^{-1}$
    \end{algorithmic}
  \end{algorithm}


\subsection{Estimation of $\nclusters$}
  In the previous sections, we assumed that the number of communities $\nclusters$ is known. However, in practical scenarios, this assumption often does not hold. This section presents an approach to estimating the number of communities.

  The idea is to find the efficient rank of the matrix $\adjacencyMatrix$. Due to Weyl's inequality $|\lambda_j(\adjacencyMatrix) - \lambda_j(\probMatrix)| \le \Vert \adjacencyMatrix - \probMatrix \Vert$. Efficiently bounding the norm $\Vert \adjacencyMatrix - \probMatrix \Vert$, we obtain that it much less than  $2 \max_{i \in [\nsize]} \sqrt{\sum_{t = 1}^\nsize \adjacencyMatrix_{it} \log^2 \nsize}$. However, in its turn, $2 \max_{i \in [\nsize]} \sqrt{\sum_{t = 1}^\nsize \adjacencyMatrix_{it} \log^2 \nsize} \ll \lambda_\nclusters(\probMatrix)$. Thus, we suggest the following estimator:
  \begin{equation*}
  %\label{eq: K estimator}
    \estimator[\nclusters] = \max \left\{j \mid \lambda_j(\adjacencyMatrix) \ge 2 \max_{i \in [\nsize]} \sqrt{\sum\nolimits_{t = 1}^\nsize \adjacencyMatrix_{it} \log^2 \nsize} \right\}.
  \end{equation*}
  %
  In what follows, we prove that it coincides with $\nclusters$ with high probability if $\nsize$ is large enough; see Section~\ref{sec:number_communities} of Appendix for details.


\subsection{Resulting SPOC++ algorithm}
  Combining ideas from previous sections, we split our algorithm into two procedures: \newline Averaging Procedure (Algorithm~\ref{algo: averaging procedure}) and the resulting SPOC++ method (Algorithm~\ref{algo: general scheme}). 

  However, the critical question remains: how to select the threshold $\threshold_{\nsize}$? In our theoretical analysis (see Theorem~\ref{theorem: main result} below), we demonstrate that by setting $\threshold_{\nsize}$ to be logarithmic in $\nsize$, SPOC++ can recover the matrix $\communityMatrix$ with a high probability and up to the desired error level. However, for practical purposes, we recommend defining the threshold just considering the distribution of the statistics $\estimator[\equalityStatistic]_{i_k j}^{a}$ for different $j$, where $i_k$ is an index chosen by Algorithm~\ref{algo: spa}; see Section~\ref{section: threshold choosing} for details.


\section{Provable guarantees}
\label{section: provable guarantees}

\subsection{Sketch of the proof of consistency}
  We will need several conditions to be satisfied to obtain optimal convergence rates. The most important one is to have many nodes placed near the vertices of the simplex. We will give the exact conditions and statements below, but first, discuss the key steps that allow us to achieve the result. They are listed below.

  \textbf{Step 1. Asymptotics of $\adjacencyEigenvectors_{ik}$.} First, using results of~\citep{Fan2020_ASYMPTOTICS}, we obtain the asymptotic expansion of $\adjacencyEigenvectors_{ik}$. We show that up to a residual term of order $\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}}$ we have
  \begin{align*}
    \adjacencyEigenvectors_{ik} & \approx \probEigenvectors_{ik} + \frac{\ev_i^{\T} \displaceMatrix \uv_k}{t_k} + \frac{\ev_i^{\T} \displaceMatrix^2 \uv_k}{t_k^2} - \frac{3}{2} \cdot \probEigenvectors_{ik} \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k,
  \end{align*}
  where $t_k \approx \lambda_k(\probMatrix)$. Matrices $\EE \displaceMatrix^2$ and $\displaceMatrix^2$ can be efficiently estimated by diagonal matrix $\diagAdjecencyMatrix = \operatorname{diag}\left( \sum_{t = 1}^\nsize \adjacencyMatrix_{it} \right)_{i = 1}^\nsize$, see also equation~\eqref{eq:diag_adj_matrix} in Algorithm~\ref{algo: averaging procedure}. Thus, we proceed with plug-in estimation of the second-order terms and obtain the estimator $\tilde{\probEigenvectors}$ defined in~\eqref{eq:adjusted_eigenvector}. Most importantly, the term linear in $\displaceMatrix$ can be suppressed using averaging.

  \textbf{Step 2. Approximating the set of pure nodes.} We show that the difference $|\estimator[\equalityStatistic]^\penalizer_{ij} - \equalityStatisticCenter^\penalizer_{ij}|$ can be efficiently bounded by sum of two terms: one depends on the difference $\Vert \nodeCommunityMatrix_i - \nodeCommunityMatrix_j \Vert_2$ and the other is at most logarithmic. If $i_k$ is an index chosen by SPA and $j \in \pureNodesSet_k$, then $\equalityStatisticCenter_{i_k j}^\penalizer$ is small. Thus, logarithmic threshold $\threshold_n$ will ensure that for all $j \in \pureNodesSet_k$ we have $\estimator[\equalityStatistic]^\penalizer_{i_k j} \le t_n$. Next, Condition~\ref{cond: theta distribution-b} implies that there are a few non-pure nodes in the set $\{ j \mid \estimator[\equalityStatistic]^\penalizer_{i_k j} \le \threshold_n\}$.

  \textbf{Step 3. Averaging.} Finally, we show that redundant terms in the asymptotic expansion of $\tilde{\probEigenvectors}_{i} - \probEigenvectors_i$ vanish after averaging, and it delivers an appropriate estimator of the simplex vertices. After that, we can obtain a good estimator of the matrix $\communityMatrix$.


\subsection{Main result}
  In order to perform theoretical analysis, we state some conditions. Most of these conditions are not restrictive, and below we discuss their limitations, if any.
  \begin{condition}
  \label{cond: nonzero B elements}
    Singular values of the matrix $\constCommunityMatrix$ are bounded away from 0.
  \end{condition}
  %
  The full rank condition is essential as, otherwise, one loses the identifiability of communities~\citep{Mao17}.

  \begin{condition}
  \label{cond: sparsity param bound}
    There is some constant $c$ such that $0 \leqslant c < 1/3$ and $\sparsityParam > \nsize^{-c}$.
  \end{condition}
  %
  Parameter $\rho$ is responsible for the sparsity of the resulting graph. The most general results on statistical properties of random graphs require $\sparsityParam \nsize \to \infty$ as $\nsize \to \infty$~\citep{minh_tang_asymptotically_2022}. In this work, we require a stronger condition to achieve the relatively strong statements we aim at. We think this condition can be relaxed though it would most likely need a proof technique substantially different from ours.

  Next, we demand the technical condition for the probability matrix $\probMatrix$.% required to use results from the paper by~\citet{Fan2019_SIMPLE}.
  \begin{condition}
  \label{cond: eigenvalues divergency}
    There exists some constant $c_0 > 0$ such that 
    \begin{align*}
      \min \left\{ 
        \frac{
          |\lambda_i(\probMatrix)|
        }{
          |\lambda_j(\probMatrix)|
        }
        \mid 
        1 \leqslant i < j \leqslant \nclusters, 
        \lambda_i(\probMatrix) \neq \lambda_j(\probMatrix) 
      \right\} \geqslant 1 + c_0.
    \end{align*}
    In addition, we have
    \begin{align}
    \label{eq: variance tends to infinity}
      \max_j \sum_{i = 1}^\nsize \probMatrix_{ij} (1 - \probMatrix_{ij}) \to \infty
    \end{align}
    as $\nsize$ tends to $\infty$.
  \end{condition}
  %
  This condition is required because of the method to obtain asymptotics of eigenvectors of $\adjacencyMatrix$. The idea is to apply the Cauchy residue theorem to the resolvent. Let $\estimator[\uv]_k$ be the $k$-th eigenvector of $\adjacencyMatrix$ and $\uv_k$ be the $k$-th eigenvector of $\probMatrix$. Let $\mathcal{C}_k$ be a contour in the complex plane that contains both $\lambda_k(\probMatrix)$ and $\lambda_k(\adjacencyMatrix)$. If no other eigenvalues are contained in $\mathcal{C}_k$ then
  \begin{align*}
      \oint_{\mathcal{C}_k} \frac{\xv^\T \estimator[\uv]_k \estimator[\uv]_k^\T \yv}{\lambda_k(\adjacencyMatrix) - z} dz = \oint_{\mathcal{C}_k} \xv^\T (\adjacencyMatrix - z \identity)^{-1} \yv dz = \oint_{\mathcal{C}_k} \xv^\T \left( \sum_{k = 1}^\nclusters \lambda_k(\probMatrix) \uv_k \uv_k^\T + \displaceMatrix - z \identity \right)^{-1} \yv dz
  \end{align*}
  for any vectors $\xv, \yv$. The leftmost side is simplified by calculating the residue at $\lambda_k(\adjacencyMatrix)$, and the rightmost side is analyzed via Sherman--Morrison--Woodbury formula. For the example of obtained asymptotics, see Lemma~\ref{lemma: eigenvector power expansion}.

  The second part of Condition~\ref{cond: eigenvalues divergency} can be omitted if $\sparsityParam < 1$ or there exist $k, k' \in [\nclusters]$ such that $\communityMatrix_{k k'}$ is bounded away from $0$ and 1, since~\eqref{eq: variance tends to infinity} is granted by Conditions~\ref{cond: nonzero B elements}-\ref{cond: sparsity param bound} and~\ref{cond: theta distribution-a} in this case. However, we decided not to impose additional assumptions and left this condition as proposed by~\citet{Fan2019_SIMPLE}.

  Next, we call the $i$-th node in our graph \textit{pure} if $\nodeCommunityMatrix_i$ has $1$ in some position and $0$ in others. We also denote this non-zero position by $\cluster[i]$ and the set of pure nodes by $\pureNodesSet$. Moreover, we define $\pureNodesSet_k = \{ i \in \pureNodesSet \mid \cluster[i] = k\}$. Thus, $\pureNodesSet_k$ is a set of nodes completely belonging to the $k$-th community. It leads us to the following conditions.

  \begin{condition}
  \label{cond: theta distribution-a}
    Cardinality of $\pureNodesSet_k$ has an asymptotic order of $\nsize$ for all $k = 1, \dots, \nclusters$. More formally, 
    \begin{align*}
      |\pureNodesSet_k| = \Theta(\nsize).
    \end{align*}
  \end{condition}

  \begin{condition}
  \label{cond: theta distribution-b}
    Fix $\eta \ge 1$. For any community index $k$, $\delta > 0$ and $\nsize > n_0(\delta)$ there exists $C_\delta$ such that
    \begin{align}
      \sum_{j \not \in \pureNodesSet_k} 
        \indicator[
          \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2 
            \leqslant
            \delta \sqrt{
                \frac{
                   \log \nsize
                }{
                  \nsize \sparsityParam
                }
            }
        ] \le C_\delta \log^{\eta} \nsize,
    \end{align}
    where $\ev_k$ is the $k$-th standard basis vector in $\RR^{\nclusters}$.
  \end{condition}
  %
  Condition~\ref{cond: theta distribution-a} is essential as it requires that all the communities have asymptotically significant mass. As discussed in Section~\ref{subsection: averaging}, we employ row averaging on the eigenmatrix $\adjacencyEigenvectors$ to mitigate noise, specifically focusing on rows corresponding to pure nodes. This averaging process effectively reduces noise by a factor of $\sqrt{\nsize}$.  While this condition is not commonly encountered in the context of MMSB, it covers an important intermediate case bridging the gap between the Stochastic Block Model and the Mixed-Membership Stochastic Block Model. If this condition is not satisfied, we suppose it is possible to obtain a higher minimax lower bound than the one provided by Theorem~\ref{theorem: lower bound}.
  
  Condition~\ref{cond: theta distribution-b} can be naturally fulfilled if non-pure $\nodeCommunityMatrix_j$ are sampled from the Dirichlet distribution. Indeed, the number of $\nodeCommunityMatrix_j$ in a ball of radius $\sqrt{\frac{
    \log \nsize
    }{
        \nsize \sparsityParam
    }}$ is proportional to $\nsize \cdot \left[ \frac{
    \log \nsize
    }{
        \nsize \sparsityParam
    } \right]^{\frac{\nclusters - 1}{2}}
  $. For example, if $\sparsityParam = \Theta(1)$ and $\nclusters \ge 3$, then we have
  \begin{align*}
    \sum_{j \not \in \pureNodesSet_k} 
        \indicator[
          \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2 
            \leqslant
            \delta \sqrt{
                \frac{
                   \log \nsize
                }{
                  \nsize \sparsityParam
                }
            }
        ] \sim C_\delta \nsize \cdot \left[ \frac{
    \log \nsize
    }{
        \nsize \sparsityParam
    } \right]^{\frac{\nclusters - 1}{2}} \lesssim C_\delta \log^{(\nclusters - 1)/2} \nsize 
  \end{align*}
  with high probability. In this case, one can take $\eta = (\nclusters - 1) / 2$.

  One may prove the above by bounding the sum of Bernoulli random variables on the left-hand side using the Bernstein inequality.
  
  These conditions allow us to state the main result of this work.
  \begin{theorem}
  \label{theorem: main result}
  Suppose that $a = \Theta(\nsize^{-2} \sparsityParam^{-1})$. Under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-b}, for each positive $\varepsilon$ there are constants $C_t, C_{\communityMatrix}$ depending on $\varepsilon, \nclusters$ such that if we apply Algorithm~\ref{algo: general scheme} with 
    \begin{align}
    \label{cond: selection threshold}
      \threshold_\nsize = C_t \log \nsize,
    \end{align}
    then there is $n_0$ such that for all $\nsize > n_0$ the following inequality holds:
    \begin{align*}
      % & \forall \varepsilon \, 
      % \forall \delta \,
      % \exists N \,
      % \forall \nsize > N \\
      \PP \left(
        \min_{\permutationMatrix \in \mathbb{S}_\nclusters} 
        \Vert 
          \estimator[\communityMatrix] - 
          \permutationMatrix 
          \communityMatrix
          \permutationMatrix^{\T}
        \Vert_\F
        \geqslant
        \frac{
          C_{\communityMatrix} \sqrt{\sparsityParam} \log^{\eta} \nsize
        }{
          \nsize
        }
      \right)
      \leqslant
      \nsize^{- \varepsilon}.
    \end{align*}
  \end{theorem}
  %
  The theorem demands $a = \Theta(\nsize^{-2} \sparsityParam^{-1})$, but the sparsity parameter $\sparsityParam$ is not observed in practice. We suppose that the most convenient choice is $a = 0$, see discussion in Section~\ref{subsection: averaging}. However, if one need to construct a quantity of order $\nsize^{-2} \sparsityParam^{-1}$, one can choose $\bigl(\nsize \lambda_1(\adjacencyMatrix)\bigr)^{-1}$, see Lemma~\ref{lemma: eigenvalues asymptotics}.


\subsection{Proof of Theorem~\ref{theorem: main result}}
  %\label{section: proof of theorem}

  %\begin{proof}
    Assume that $\nclusters$ is known. Given $\varepsilon$, choose $t_n = C(\varepsilon) \log \nsize$ such that the event
    \begin{align}
    \label{eq: F permutation upper bound}
      \Vert \estimator[\basisMatrix] - \basisMatrix \permutationMatrix_{\basisMatrix} \Vert_\F \le \frac{C_{\basisMatrix} \log^\eta \nsize}{\nsize \sqrt{\nsize \sparsityParam}}
    \end{align}
    has probability at least $1 - \nsize^{- \varepsilon} / 3$ for some constant $C_\basisMatrix$ and permutation matrix $\permutationMatrix_\basisMatrix$. Such $t_n$ exists due to Lemma~\ref{lemma: averaging lemma}. WLOG, we assume that the minimum in~\eqref{eq: F permutation upper bound} is attained when  $\permutationMatrix_\basisMatrix = \identity$, since changing order of communities does not change the model.
    %
    Meanwhile, due to Lemma~\ref{lemma: debaised eigenvalues behaviour}:
    \begin{align*}
      \debiasedEigenvalues_{k k} = \probEigenvalues_{k k} + O_{\ell} (\sqrt{\sparsityParam \log \nsize}).
    \end{align*}
    Thus, we have
    \begin{align*}
      \max_k |\debiasedEigenvalues_{k k} - \probEigenvalues_{k k}| \le C_{\probEigenvalues} \sqrt{\sparsityParam \log \nsize}
    \end{align*}
    with probability $1 - \nsize^{-\varepsilon} / 3$.
    Hence, we obtain
    \begin{equation}
      \Vert \communityMatrix - \estimator[\communityMatrix] \Vert_\F 
      \le 
      \Vert \basisMatrix - \estimator[\basisMatrix] \Vert
      \Vert \probEigenvalues \Vert
      \Vert \basisMatrix \Vert_\F
      +
      \Vert \estimator[\basisMatrix] \Vert
      \Vert \probEigenvalues - \debiasedEigenvalues \Vert
      \Vert \basisMatrix \Vert_\F
      +
      \Vert \estimator[\basisMatrix] \Vert
      \Vert \debiasedEigenvalues \Vert
      \Vert \basisMatrix - \estimator[\basisMatrix] \Vert_\F \nonumber \\
      = O \left( \frac{\sqrt{\sparsityParam}}{\nsize } \cdot \log^\eta \nsize \right),
    \label{eq: decomposition B error} 
    \end{equation}
    where we use $\Vert \basisMatrix \Vert_\F = O(\nsize^{-1/2})$ and $\Vert \probEigenvalues \Vert = O(\nsize \sparsityParam)$ from Lemmas~\ref{lemma: F rows tensor product} and~\ref{lemma: eigenvalues asymptotics}.
    
    Before we supposed that $\nclusters$ is known. Now consider the case when it does not hold. Due to Lemma~\ref{lemma: estimation of nclusters}, we have $\estimator[\nclusters] = \nclusters$ with probability $1 - \nsize^{-\varepsilon} / 3$ for large enough $\nsize$. It implies that the bound~\eqref{eq: decomposition B error} also holds for the estimator based on $\estimator[\nclusters]$ with probability $1 - \nsize^{-\varepsilon}$.
  %\end{proof}

  \subsection{Lower bound}
  \label{section: lower bound}

  In this section, we show that Theorem~\ref{theorem: main result} is optimal under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-b}. Precisely, we state the following.

  \begin{theorem}
   \label{theorem: lower bound} 
   If $\nsize > n_0$ for some $n_0$ that may depend on $\nclusters$, then there exist a matrix $\nodeCommunityMatrix_0$ and a set of symmetric matrices $\mathcal{B} \subset [0, 1]^{\nclusters \times \nclusters}$ such that
   \begin{enumerate}[label=(\roman*), ref=(\roman{enumi})]
        \item \label{theorem lower bound, property i} for each matrix $\constCommunityMatrix \in \mathcal{B}$, its singular values are at least $1/8$,
        \item \label{theorem lower bound, property ii} for each $k \in [\nclusters - 1]$ and $\constCommunityMatrix \in \mathcal{B}$, we have $\sigma_k(\probMatrix) / \sigma_{k + 1}(\probMatrix) > 1 + c_0$, where $c_0 = 0.2$ and $\probMatrix = \nodeCommunityMatrix_0 \constCommunityMatrix \nodeCommunityMatrix_0^\T$, and, additionally, 
        \begin{align*}
            \max_j \sum_{i = 1}^\nsize \probMatrix_{ij} (1 - \probMatrix_{ij}) \ge \frac{\nsize \sparsityParam}{16},
        \end{align*}
        \item \label{theorem lower bound, property iii} each set $|\pureNodesSet_k|$, $k \in [\nclusters]$, has cardinality at least $\left(1 - 2^{-(\nclusters + 5)} \right) 2^{-\nclusters} \nsize$,
        \item \label{theorem lower bound, property iv} for each $k$, we have 
        \begin{align*}
            \sum_{j \not \in \pureNodesSet_k} 
                \indicator[
                  \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2 
                    \leqslant
                    \delta \sqrt{
                        \frac{
                           \log \nsize
                        }{
                          \nsize \sparsityParam
                        }
                    }
                ] = 0,
        \end{align*}
        provided $\nsize \sparsityParam > \delta^2 \left(1 - \nclusters^{-2} \right) \log \nsize$,
   \end{enumerate}
   and
   \begin{align*}
        \inf_{\estimator[\communityMatrix]} \sup_{\constCommunityMatrix \in \mathcal{B}} \PP \left( 
            \min_{\permutationMatrix \in \mathbb{S}_\nclusters} \Vert \sparsityParam \constCommunityMatrix - \permutationMatrix \estimator[\communityMatrix] \permutationMatrix^\T \Vert_\F \ge \frac{\nclusters^{2} \sqrt{\sparsityParam}}{1152 \cdot \nsize}
        \right) \ge \frac{1}{4 e^{3.2}}
   \end{align*}
   for any $\rho > \nsize^{-1/3}$.
  \end{theorem}
  %
  The proof is similar to that of~\eqref{eq: B lower bound}. The main difference is that we consider the minimum over the set of permutation matrices.

  One can see that Condition~\ref{cond: nonzero B elements} is satisfied by property~\ref{theorem lower bound, property i}, Condition~\ref{cond: sparsity param bound} is satisfied since we guarantee the conclusion of Theorem~\ref{theorem: lower bound} for any $\sparsityParam > \nsize^{-1/3}$, Condition~\ref{cond: eigenvalues divergency} is satisfied by property~\ref{theorem lower bound, property ii}, Condition~\ref{cond: theta distribution-a} is satisfied by property~\ref{theorem lower bound, property iii}, and Condition~\ref{cond: theta distribution-b} is satisfied by property~\ref{theorem lower bound, property iv}. Thus, the estimator defined by Algorithm~\ref{algo: general scheme} is indeed optimal up to the dependence on $\nclusters$.


\section{Numerical experiments}
\label{section: numerical experiments}

\subsection{How to choose an appropriate threshold?}
\label{section: threshold choosing}
  In the considered experiments, we fix $\nclusters$ equal to $3$ and assume that $\communityMatrix$ is well-conditioned. Empirically we show that well-conditioning is vital to achieving a high probability of choosing pure nodes with SPA (see Figure~\ref{fig: curves of t_n}).

  The crucial question in practice for the SPOC++ algorithm is the choice of the threshold. Theoretically, we have established that $t = C \log \nsize$ gives the right threshold to achieve good estimation quality. In practice, there is a simple way to choose the appropriate threshold for nodes $i_1, \ldots, i_\nclusters$ chosen by SPA. For each $i_k$, it is necessary to plot distribution of $\estimator[\equalityStatistic]_{i_k j}$ over $j$. Thus, if the averaging procedure improves the results of SPOC, then there is a corresponding plateau on the plot (see Figure~\ref{fig: distribution of T}).

  Besides, our experiments show that for small $\nclusters$, $\threshold_\nsize = 2 \log \nsize$ is good enough if nodes are generated to satisfy Conditions~\ref{cond: theta distribution-a} and~\ref{cond: theta distribution-b}. This choice corresponds well to the theory developed in this paper.

  % Figure environment removed

  % Figure environment removed


\subsection{Illustration of theoretical results}
\label{subsection: convergence rate}
  We run two experiments to illustrate our theoretical studies. First, we check the dependence of the estimation error on the number of vertices $\nsize$. Second, we study how the sparsity parameter $\sparsityParam$ influences the error.

  For the first experiment, we provide the following experimental setup. The number of clusters is chosen equal to $3$, and for each $\nsize \in \{500, 1000, 1500, \ldots, 5000\}$ we generate a matrix $\nodeCommunityMatrix$, where the fractions of pure nodes are $\frac{|\pureNodesSet_k|}{\nsize} = 0.09$ and other (not pure) node community memberships are distributed in simplex according to $Dirichlet(1, 1, 1)$. Then we calculated the matrix $\probMatrix$ with $\sparsityParam = 1$. Besides, for each $\nsize$ (and, consequently, matrix $\probMatrix$) we generate the graph $\adjacencyMatrix$ 40 times and compute the error $\min_{\permutationMatrix} \Vert \estimator[\communityMatrix] - \permutationMatrix \communityMatrix \permutationMatrix^{\T} \Vert_\F$, where minimum is taken over all permutation matrices. Hence, for each $\nsize$, we obtain 40 different errors, and, finally, we compute their mean and their quantiles for confidence intervals. The threshold is equal to $2 \log \nsize$.

  % Figure environment removed

  We plot the error curves in logarithmic coordinates to estimate the convergence rate. The results are presented in Figure~\ref{fig: convergence rate}, left. It is easy to see that the observed error rate is a bit faster than the predicted one. The slope of the mean error is $-1.21 \pm 0.03$. However, it does not contradict the theory since the provided lower bound holds for some matrix $\communityMatrix$ that may not occur in the experiment.

  We fix $\nsize = 5000$ for the second experiment and generate some matrix $\probMatrix$ as before. Then, we generate 40 symmetric matrices $\mathbf{E}^{(1)}, \ldots, \mathbf{E}^{(40)} \in [0, 1]^{\nsize \times \nsize}$. Entries of each matrix $\mathbf{E}^{(p)}$ are uniformly distributed random variables with the support $[0, 1]$. Given the sparsity parameter $\sparsityParam$ and a matrix $\mathbf{E}^{(p)}$, we generate a matrix $\adjacencyMatrix$ as follows:
  \begin{align*}
    \adjacencyMatrix_{ij} = \indicator[\mathbf{E}^{(p)}_{ij} < \sparsityParam \cdot \probMatrix_{ij}].
  \end{align*}
  %
  We apply our algorithm to $\adjacencyMatrix$ and compute the error of $\estimator[\communityMatrix]$.

  We study our algorithm for 20 different values of $\sparsityParam$. The results are presented on Figure~\ref{fig: convergence rate}, right. Visually, the observed rate of convergence is a bit faster than the predicted. We calculate the slope of the mean error which turns out to be $0.56 \pm 0.03$.
%   Meanwhile, this conclusion seems to be unconvincing we believe in it since the slope was increasing while we were increasing the number of nodes $\nsize$. For example, for $\nsize$ equals 5000 we observe the slope's value being equal to $0.28\pm0.02$ (see Figure~\ref{fig: dependence on rho:5000}).
% % Figure environment removed

  % % Figure environment removed

% \begin{table}[]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         variable & values \\
%         \hline
%         $\nclusters$ & 3 \\
%         $\nsize$ & 2000 \\
%         $\constCommunityMatrix$ & 3 well-conditioned randomly generated \\
%         $\probMatrix$ & 1 for each $\constCommunityMatrix$ \\
%         $\rho$ & $\{\nsize^{-0.0}, \nsize^{-0.05}, \ldots, \nsize^{-0.45}\}$ \\
%         $\adjacencyMatrix$ & 40 for each $\rho \probMatrix$ \\
%         \hline
%     \end{tabular}
%     \caption{Sparsity parameter study's experimental setup.}
%     \label{tab:sparsity param experemental setup}
% \end{table}

  \subsection{Comparison with other algorithms}
  \label{subsetion: comparison}

  % Figure environment removed

  We compare the performance of our algorithm with Algorithm~\ref{algo: spoc} and GeoMNF~\citep{Mao17}, as other distribution-free algorithms are similar to Algorithm~\ref{algo: spoc}. We set the number of communities to $3$. As in Section~\ref{subsection: convergence rate}, we generate a well-conditioned matrix $\constCommunityMatrix$, then, for each $\nsize \in \{500, 1000, \ldots, 8500\}$, we choose $\sparsityParam = 1$ and generate a matrix $\probMatrix$. As previously, for each community, the number of pure nodes was equal to $0.09 \cdot \nsize$, and membership vectors of non-pure nodes were sampled from the $Dirichlet(1, 1, 1)$ distribution. Given a matrix of connection probabilities $\probMatrix$, we generate 40 different matrices $\adjacencyMatrix$, and for each of them, we compute the error of reconstruction of $\communityMatrix$ and $\nodeCommunityMatrix$, defined as follows:
  \begin{align*}
    \mathcal{L}_{\communityMatrix}(\communityMatrix, \estimator[\communityMatrix]) = \min_{\permutationMatrix \in \mathbb{S}_\nclusters} \Vert \estimator[\communityMatrix] - \permutationMatrix^\T \communityMatrix \permutationMatrix \Vert_{\F}, \quad
    \mathcal{L}_{\nodeCommunityMatrix}(\nodeCommunityMatrix, \estimator[\nodeCommunityMatrix]) = \min_{\permutationMatrix \in \mathbb{S}_\nclusters} \frac{\Vert \estimator[\nodeCommunityMatrix] - \nodeCommunityMatrix \permutationMatrix \Vert_{\F}}{
        \Vert \nodeCommunityMatrix \Vert_{\F}
    }.
  \end{align*}
  %
  The results are presented in Figure~\ref{fig: comparison}. As one can see, our algorithm significantly outperforms Algorithm~\ref{algo: spoc} and GeoMNF.


\section{Discussion}
\label{section: discussion}
  The most important assumption we made is Condition~\ref{cond: theta distribution-a} since it allows us to reduce the error rate significantly. The lower bound was also obtained under this condition. We conjecture that if Condition~\ref{cond: theta distribution-a} does not hold, then the lower bound from Theorem~\ref{theorem: lower bound} is no longer optimal. We think it can be obtained by constructing families $\mathcal{B}$ of community matrices and $\mathcal{T}$ of membership matrices such that $|\pureNodesSet_k| = 1$, $k \in [\nclusters]$, for each $\nodeCommunityMatrix \in \mathcal{T}$. However, this analysis is out of the scope of our paper.

  Let us note that Condition~\ref{cond: theta distribution-a} covers the important intermediate case between Stochastic Block Model and Mixed-Membership Stochastic Block Model with almost no pure nodes. Thus, it seems pretty natural and can be satisfied in practice.


%bibliography
\bibliography{overlapping}


\newpage

\appendix

\section{Proof of Proposition~\ref{proposition: when penalizer can be zero}}
%\begin{proof}[Proof of Proposition~\ref{proposition: when penalizer can be zero}]
  Let us estimate eigenvalues of matrix $\asymptoticVariance(i, j)$. After some straightforward calculations, we have
    \begin{align}
    \label{eq: asymptoticVariance}
      \asymptoticVariance (i, j) &= 
      \probEigenvalues^{-1}
      \probEigenvectors^{\T}
      \EE \left(
        \displaceMatrix_i
        -
        \displaceMatrix_j
      \right)^{\T}
      \left(
        \displaceMatrix_i
        -
        \displaceMatrix_j
      \right)
      \probEigenvectors
      \probEigenvalues^{-1} \nonumber \\
      & = 
      \probEigenvalues^{-1}
      \probEigenvectors^{\T}
      \left(
        \operatorname{diag} (
          \EE \displaceMatrix_i^2
          +
          \EE \displaceMatrix_j^2
        )
        - \EE \displaceMatrix_{ij}^2
        (\ev_i \ev_j^{\T} + \ev_j \ev_i^{\T})
      \right)
      \probEigenvectors
      \probEigenvalues^{-1}.
    \end{align}
    %
    The maximum eigenvalue can be estimated using a norm of the matrix:
    \begin{align*}
      \lambda_{\max} \bigl( 
        \asymptoticVariance(i, j)
      \bigr)
      =
      \Vert
        \asymptoticVariance(i, j)
      \Vert
      \leqslant
      \Vert
        \probEigenvalues^{-1}
      \Vert^2
      \Vert
        \probEigenvectors
      \Vert^2
      \left(
        \Vert 
          \operatorname{diag} (
            \EE \displaceMatrix_i^2
            +
            \EE \displaceMatrix_j^2
          )
        \Vert
        + 2 \EE \displaceMatrix_{ij}^2
      \right),
    \end{align*}
    \begin{align*}
      \lambda_{\max} \bigl(
        \asymptoticVariance(i, j)
      \bigr)
      \leqslant 
      \frac{
        4 \sparsityParam
      }{
        \lambda_{\nclusters}^2 (\probMatrix)
      },
    \end{align*}
    since $\EE \displaceMatrix_{ij}^2 = \probMatrix_{ij} - \probMatrix_{ij}^2$. Due to Lemma~\ref{lemma: eigenvalues asymptotics}, we have $\lambda_{\nclusters}^2(\probMatrix) = \Omega(\nsize \sparsityParam)$, so the upper bound holds. To find the lower bound of the minimal eigenvalue of $\asymptoticVariance(i, j)$, we need Condition~\ref{cond: theta distribution-a}. Let us rewrite~\eqref{eq: asymptoticVariance} in the following way:
    \begin{align*}
      \asymptoticVariance(i, j) = 
      \probEigenvalues^{-1}
      \bigl(
        \purePart +
        \notPurePart -
        \negativePart
      \bigr)
      \probEigenvalues^{-1},
    \end{align*}
    where
    \begin{align*}
      \purePart & =
      \sum_{m \in \pureNodesSet}
          \left(
            \EE \displaceMatrix_{im}^2
            +
            \EE \displaceMatrix_{jm}^2
          \right)
          \probEigenvectors_{m}^{\T}
          \probEigenvectors_{m}, \\
      \notPurePart & =
      \sum_{m \not \in \pureNodesSet}
          \left(
            \EE \displaceMatrix_{im}^2
            +
            \EE \displaceMatrix_{jm}^2
          \right)
          \probEigenvectors_{m}^{\T}
          \probEigenvectors_{m}, \\
      \negativePart & =
      \EE \displaceMatrix_{ij}^2
      \left(
          \probEigenvectors_i^{\T}
          \probEigenvectors_j
          +
          \probEigenvectors_j^{\T}
          \probEigenvectors_i
      \right).
    \end{align*}
    %
    Now we analyze $\purePart$. Since $\probEigenvectors = \nodeCommunityMatrix \basisMatrix$, we obtain
    \begin{align*}
      \purePart & = 
      \sum_{k = 1}^\nclusters
        \nsize_k 
        \left(
          \nodeCommunityMatrix_i \communityMatrix_{k}^{\T} - 
          (\nodeCommunityMatrix_i \communityMatrix_{k}^{\T})^2
          +
          \left(
            \nodeCommunityMatrix_j \communityMatrix_k^{\T}
            -
            (\nodeCommunityMatrix_j \communityMatrix_k^{\T})^2
          \right)
        \right)
        \basisMatrix_k^{\T}
        \basisMatrix_k \\
      & \geqslant
      2 \sum_{k = 1}^\nclusters
          \nsize_k 
          \min
          \left\{
            \min_{k'} \communityMatrix_{k' k} - 
            ( \min_{k'} \communityMatrix_{k' k})^2
            ,
            \max_{k'} \communityMatrix_{k' k} - 
            ( \max_{k'} \communityMatrix_{k' k})^2
          \right\}
          \basisMatrix_k^{\T}
          \basisMatrix_k \\
      & =
      \nsize \sparsityParam
      \sum_{k = 1}^\nclusters
        \alpha_k
        \basisMatrix_k^{\T}
        \basisMatrix_k,
    \end{align*}
    where $\alpha_k$, $k \in [\nclusters]$ are bounded away from 0 since entries of $\communityMatrix$ are bounded away from 0 and 1 by the assumptions of the proposition. Lemma~\ref{lemma: F rows tensor product} implies that there are such constants $C_1, C_2$ that
    \begin{align*}
      \sparsityParam C_1
      \leqslant
      \lambda_{\min}\bigl(\purePart\bigr)
      \leqslant
      \lambda_{\max}\bigl(\purePart\bigr)
      \leqslant
      \sparsityParam C_2.
    \end{align*}
    %
    Since $\notPurePart$ is non-negative defined, we state that $\lambda_{\min}\bigl(\notPurePart\bigr) \geqslant 0$. 

    In order to estimate eigenvalues of $\negativePart$, we use Lemma~\ref{lemma: eigenvectors max norm}:
    \begin{align*}
      \lambda_{\max}\bigl(\negativePart\bigr)
      \leqslant 
      \sparsityParam 
      \left(
        \Vert
          \probEigenvectors_i^{\T}
          \probEigenvectors_j
        \Vert
        +
        \Vert
          \probEigenvectors_j^{\T}
          \probEigenvectors_i
        \Vert
      \right)
      \leqslant
      \frac{2 \sparsityParam \nclusters C_{\probEigenvectors}^2}{\nsize}.
    \end{align*}
    %
    Applying multiplicative Weyl's inequality, we get
    \begin{align}
      \lambda_{\min} \bigl(
        \asymptoticVariance(i, j)
      \bigr)
      \geqslant
      \frac{1}{\lambda^2_\nclusters(\probMatrix)}
      \bigl[
        \lambda_{\min}\bigl(\purePart\bigr)
        -
        \lambda_{\max}\bigl(\negativePart\bigr)
      \bigr]
      \geqslant
      \frac{1}{\nsize^2 \sparsityParam} \left(
        c_1 - \frac{c_2}{\nsize}
      \right)
    \end{align}
    for some positive constants $c_1$, $c_2$. Thus, the proposition follows.
%\end{proof}




\section{Proofs for Theorem~\ref{theorem: main result}}

Here and further following~\citet{Fan2019_SIMPLE} we use the notation $O_{\prec}(\cdot)$:
  \begin{definition}
  \label{def: O_prec definition}
   Suppose $\xi$ and $\eta$ to be random variables that may depend on $\nsize$. We say that $\xi = O_{\prec}(\eta)$ if and only if for any positive $\varepsilon$ and $\delta$ there exists $\nsize_0$ such that for any $\nsize > \nsize_0$
  \begin{align}
    \PP \left(
      |\xi| > \nsize^{\varepsilon} |\eta|
    \right)
    \leqslant
    \nsize^{-\delta}.
  \end{align}
  \end{definition}
  %
  It is easy to check the following properties of $O_{\prec}(\cdot)$. If $\xi_1 = O_{\prec}(\eta_1)$ and $\xi_2 = O_{\prec}(\eta_2)$ then $\xi_1 + \xi_2 = O_{\prec} (|\eta_1| + |\eta_2|)$, $\xi_1 + \xi_2 = O_{\prec} \left(\max\{|\eta_1|, |\eta_2|\} \right)$ and $\xi_1 \xi_2 = O_{\prec}(\eta_1 \eta_2)$.
  
  Additionally, we introduce a bit different type of convergence.
  \begin{definition}
  \label{def: O_l definition}
   Suppose $\xi$ and $\eta$ to be random variables that may depend on $\nsize$. Say $\xi = O_{\ell} (\eta)$ if for any $\varepsilon > 0$ there exist $n_0$ and $\delta > 0$ such that
  \begin{align*}
    \PP \left( \xi \ge \delta \eta \right) \le \nsize^{-\varepsilon}
  \end{align*}
  holds for all $\nsize \ge n_0$.
  \end{definition}
  %
  It preserves the properties of $O_{\prec}(\cdot)$ described previously. Moreover, $O_{\prec}(\eta) = O_\ell (\nsize^\alpha \cdot \eta)$ for any $\alpha > 0$.
  
  Further, we have a lot of random variables $\xi_i$ depending on index $i \in [\nsize]$. Mostly, they have the form $\ev_i^{\T} \mathbf{X}$ for some random matrix $\mathbf{X}$. Formally, if $\xi_i = O_{\prec} (\eta_\nsize)$, we are not allowed to state $\max_{i} \xi_i = O_{\prec}(\eta_\nsize)$ since $n_0$ for different $i$ may be distinct and not be bounded. Nevertheless, the source of $O_{\prec}(\cdot)$ is random variables of the form $\xv^{\T} (\displaceMatrix^\ell - \EE \displaceMatrix^\ell) \yv$, that can be uniformly bounded using all moments provided by Lemma~\ref{lemma: power deviation}. Thus, $\xi_i = O_{\prec}(\eta_\nsize)$ for any $i \in S \subset [\nsize]$ implies $\max_{i \in S} \Vert \xi_i \Vert_2 = O_{\prec}(\eta_\nsize)$.
  
  The order $O_\ell(\eta_{\nsize})$ appears when we combine $O_{\prec}(\eta_{\nsize} / \nsize^{\alpha})$ for some $\alpha > 0$ and random variable $X$ bounded by $\eta_{\nsize}$ via Freedman or Bernstein inequalities that provide exactly the same $n_0$ for different $i$. Consequently, taking maximum over any subset of $[\nsize]$ is also allowed.

\subsection{Asymptotics of eigenvectors}
  \input{asymptotics_eigenvalues}


\subsection{Pure sets approximation}
  \input{pure_set_approximation}


\subsection{Averaging over selected nodes}
  \input{averaging}


\subsection{Estimation of the number of communities}
\label{sec:number_communities}
  \input{nclusters_estimation}

\section{Proof of Theorem~\ref{theorem: lower bound}}
  \input{lower_bound}


\section{Tools and supplementary lemmas for Theorem~\ref{theorem: main result}}


\subsection{Supplementary lemmas}
  \input{lemmas}


\subsection{Tools}
  \input{tools}

\section{Tools for Theorem~\ref{theorem: lower bound}}
    \input{lower_bound_tools}




\end{document}
