% !TEX root = jmlr_version.tex

\begin{lemma}
\label{lemma: averaging lemma}
  Define
  \begin{align*}
    \diagAdjecencyMatrix & = \operatorname{diag}\left(\sum_{t = 1}^\nsize \adjacencyMatrix_{it} \right)_{i = 1}^\nsize, \\
    \tilde \probEigenvectors_{ik} & = \adjacencyEigenvectors_{ik} \left(1 - \frac{\diagAdjecencyMatrix_{ii} - 3/2 \sum_{j = 1}^\nsize \diagAdjecencyMatrix_{jj} \adjacencyEigenvectors_{jk}^2}{\adjacencyEigenvalues^2_{kk}} \right) - \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \cdot \sum_{j = 1}^{\nsize} \frac{\diagAdjecencyMatrix_{j j} \adjacencyEigenvectors_{j k'} \adjacencyEigenvectors_{j k}}{\adjacencyEigenvalues_{k k}^2}, \\
    \estimator[\basisMatrix]_k & = \frac{1}{|\estimator[\pureNodesSet]_k|} \sum_{j \in \estimator[\pureNodesSet]_k} \tilde \probEigenvectors_{ik}.
  \end{align*}
  %
  Then under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-b}, for any $\varepsilon$ there exist are constants $C_1(\varepsilon), C_2(\varepsilon)$ such that for $\threshold_n = C_1(\varepsilon) \log \nsize$, $C_{\basisMatrix} = C_2(\varepsilon)$, and $\nsize > \nsize_0(\varepsilon)$ we have
  \begin{align*}
    \PP \left( \min_{\permutationMatrix \in \mathbb{S}_\nclusters} \Vert \estimator[\basisMatrix] - \basisMatrix \permutationMatrix^\T \Vert_\F \ge \frac{C_{\basisMatrix} \log^\eta \nsize}{\sqrt{\nsize^3 \sparsityParam}}\right) \le \nsize^{-\varepsilon}.
  \end{align*}
\end{lemma}

\begin{proof}
  According to Lemma~\ref{lemma: pure set approximation}, with probability $1 - \nsize^{-\varepsilon}$, we have
  \begin{align}
  \label{eq: averaging lemma, pure node set decomposition}
    \frac{1}{|\estimator[\pureNodesSet]_k|} 
    \sum_{j \in \estimator[\pureNodesSet]_k} 
      \tilde \probEigenvectors_{j} 
    & = \frac{1 - O \left(\nsize^{-1} \log^\eta \nsize \right)}{|\pureNodesSet_k|}
    \left[ \sum_{j \in \pureNodesSet_k} \tilde \probEigenvectors_j + \sum_{j \in \estimator[\pureNodesSet]_k \setminus \pureNodesSet_k } \tilde \probEigenvectors_j \right] \\
    & = \frac{1 - O \left(\nsize^{-1} \log^\eta \nsize \right)}{|\pureNodesSet_k|} \sum_{j \in \pureNodesSet_k} \tilde \probEigenvectors_j + O \left( \frac{\log^\eta \nsize}{\nsize \sqrt{\nsize}}\right).
  \end{align}
  %
  Due to Lemma~\ref{lemma: eigenvector power expansion}, we have
  \begin{align*}
    \adjacencyEigenvectors_{ik} & = \probEigenvectors_{ik} + \frac{\ev_i^{\T} \displaceMatrix \uv_k}{t_k} + \frac{\ev_i^{\T} \displaceMatrix^2 \uv_k}{t_k^2} - \frac{3}{2} \cdot \probEigenvectors_{ik} \frac{\uv_k^{\T} \EE \displaceMatrix^2 \uv_k}{t_k^2} \\
    &  \quad \, + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot  \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k + O_{\ell} \left(\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right),
  \end{align*}
  % where $\meanEigs$ is a diagonal matrix with $t_k$ as entries. Meanwhile, $(\EE \displaceMatrix^2)_{i j} = \indicator[i = j] \sum_{t = 1}^\nsize \probMatrix_{i t} - \probMatrix_{i t}^2 = \indicator[i = j] \EE \diagAdjecencyMatrix_{i i} - \indicator[i = j] \sum_{t = 1}^\nsize \probMatrix_{i t}^2$. Next, 
  % \begin{align*}
  %   \ev_j^{\T} \diag \left(\sum_{t = 1}^\nsize \probMatrix_{i t}^2 \right)_{i = 1}^\nsize \probEigenvectors \meanEigs^{-2} = \sum_{t = 1}^\nsize \probMatrix_{jt}^2 \probEigenvectors_j \meanEigs^{-2} & = O \left(\nsize^{-3/2}\right), \\
  %   \probEigenvectors_j \diag \left[\probEigenvectors^{\T} \diag \left(\sum_{t = 1}^\nsize \probMatrix_{i t}^2 \right)_{i = 1}^\nsize \probEigenvectors \right] \meanEigs^{-2} & = O(\nsize^{-3/2}), \\
  %   \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \diag \left(\sum_{t = 1}^\nsize \probMatrix_{i t}^2 \right)_{i = 1}^\nsize \uv_k & = O(\nsize^{-3/2}).
  % \end{align*}
  Our goal is to get asymptotic expansion for $\tilde \probEigenvectors_j$. For the terms of asymptotic expansion of $\adjacencyEigenvectors_j$, we obtain
  \begin{align}
      \frac{\ev_i^{\T} \displaceMatrix \uv_k}{t_k} & \overset{\text{Lemma~\ref{lemma: log estimate vector difference}}}{=} \frac{1}{t_k} O_{\ell}(\sqrt{\sparsityParam \log \nsize}) \overset{\text{Lemmas~\ref{lemma: t_k is well-definied},\ref{lemma: eigenvalues asymptotics}}}{=} O_{\ell}\left(\sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}} \right), \label{eq: U hat terms asumptotics-f} \\
      \frac{\ev_i^\T \displaceMatrix^2 \uv_k}{t_k^2} & \overset{\text{Lemmas~\ref{lemma: power expectation}, \ref{lemma: power deviation}}}{=} \frac{1}{t_k^2} O_{\prec}(\sqrt{\nsize} \sparsityParam) \overset{\text{Lemmas~\ref{lemma: t_k is well-definied},\ref{lemma: eigenvalues asymptotics}}}{=} O_{\prec} \left( \frac{1}{\nsize^{3/2} \sparsityParam} \right), \\
      \frac{3}{2} \probEigenvectors_{ik} \cdot \frac{\uv_k ^\T \EE \displaceMatrix^2 \uv_k}{t_k^2} 
      & \overset{\text{Lemmas~\ref{lemma: power expectation},\ref{lemma: eigenvectors max norm}}}{=} O(\nsize^{-1/2}) \cdot O(\nsize \sparsityParam) \cdot t_k^{-2} \overset{\text{Lemmas~\ref{lemma: t_k is well-definied},\ref{lemma: eigenvalues asymptotics}}}{=} O \left(\frac{1}{\nsize^{3/2} \sparsityParam} \right),
  \end{align}
  \begin{align}
      \sum_{k' \in [\nclusters] \setminus \{k\} } \frac{\lambda_{k'} \probEigenvectors_{ik'}}{\lambda_{k'} - t_k} \cdot \frac{\uv_{k'}^\T \EE \displaceMatrix^2 \uv_k}{t_k^2} & \overset{
        \substack{\text{Lemma~\ref{lemma: t_k is well-definied}}, \\ \text{Condition~\ref{cond: eigenvalues divergency}}}
      }{=} O(1) \cdot \max_{k' \in [\nclusters] \setminus\{k\} } \probEigenvectors_{i k'} \cdot \frac{\uv_{k'}^\T \EE \displaceMatrix^2 \uv_k}{t_k^2} \nonumber \\
      & \overset{\text{Lemmas~\ref{lemma: eigenvectors max norm},\ref{lemma: power expectation}}}{=} t_k^{-2} O(\nsize^{1/2} \sparsityParam)
      \overset{\text{Lemmas~\ref{lemma: t_k is well-definied},\ref{lemma: eigenvalues asymptotics}}}{=} O \left( \frac{1}{\nsize^{3/2} \sparsityParam} \right).
  \label{eq: U hat asymptotics-l}
  \end{align}
  %
  Next, we analyze $\tilde \probEigenvectors_{jk}$. Note that
  \begin{align*}
      \diagAdjecencyMatrix_{ii} - \EE \diagAdjecencyMatrix_{ii} = \sum_{j = 1}^\nsize (\adjacencyMatrix_{ij} - \probMatrix_{ij}) = O_\ell(\sqrt{\nsize \sparsityParam \log \nsize})
  \end{align*}
  from the Bernstein inequality. Thus, we get
  \begin{align*}
      \diagAdjecencyMatrix_{ii} \adjacencyEigenvalues^{-2}_{kk} - t_k^{-2} \EE (\diagAdjecencyMatrix_{ii}) & = (\diagAdjecencyMatrix_{ii} - \EE \diagAdjecencyMatrix_{ii}) \adjacencyEigenvalues^{-2}_{kk} + \EE (\diagAdjecencyMatrix_{ii}) (\adjacencyEigenvalues_{kk}^{-2} - t_k^{-2}) \\
      & = O_{\ell}(\sqrt{\nsize \sparsityParam \log \nsize}) \adjacencyEigenvalues^{-2}_{kk} + O(\nsize \sparsityParam) (\adjacencyEigenvalues_{kk}^{-2} - t_k^{-2}).
  \end{align*}
  %
  Since $t_k \sim \lambda_k$ from Lemma~\ref{lemma: t_k is well-definied}, $\lambda_k = \Theta(\nsize \sparsityParam)$ from Lemma~\ref{lemma: eigenvalues asymptotics} and $\adjacencyEigenvalues_{kk} = t_k + O(\sqrt{\sparsityParam \log \nsize})$ from Lemmas~\ref{lemma: eigenvalues difference} and~\ref{lemma: log estimate vector difference}, we have $\adjacencyEigenvalues^{-2}_{kk} = O_{\ell} \left( \frac{1}{\nsize^2 \sparsityParam^2} \right)$ and $\adjacencyEigenvalues^{-2}_{k k} - t_k^{-2} = O_{\ell}(\sqrt{\sparsityParam \log \nsize}) \cdot O_\ell(\nsize^{-3} \sparsityParam^{-3}) =  O_{\ell}(\nsize^{-3} \sparsityParam^{-5/2} \log^{1/2} \nsize)$. Consequently, we have
  \begin{align}
  \label{eq: averaging lemma, D term approximation}
      \diagAdjecencyMatrix_{ii} \adjacencyEigenvalues^{-2}_{kk} - t_k^{-2} \EE (\diagAdjecencyMatrix_{ii}) = O_{\ell}\left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right).
  \end{align}
  %
  Next, we bound $\adjacencyEigenvalues^{-2}_{kk} \sum_{j = 1}^\nsize \diagAdjecencyMatrix_{jj} \adjacencyEigenvectors_{jk}^2$. We have
  \begin{align}
  \label{eq: averaging lemma, quad form factor approximation}
      \adjacencyEigenvalues^{-2}_{kk} \sum_{j = 1}^\nsize \diagAdjecencyMatrix_{jj} \adjacencyEigenvectors_{jk}^2 = \frac{\uv_k^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} +  \left( \frac{\widehat{\uv}_k^\T \diagAdjecencyMatrix \widehat{\uv}_k}{\adjacencyEigenvalues^2_{kk}} - \frac{\uv_k^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} \right) = \frac{\uv_k^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} + O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right)
  \end{align}
  due to Lemma~\ref{lemma: second order term estimation}.
  
  %  \begin{align*}
  %   \adjacencyEigenvectors_j & = 
  %   \probEigenvectors_j + 
  %   \displaceMatrix_j \probEigenvectors \meanEigs^{-1} + (\displaceMatrix^2 - \EE \displaceMatrix^2)_j \probEigenvectors \meanEigs^{-2} + 
  %   (\EE \diagAdjecencyMatrix)_{jj} \probEigenvectors_j \meanEigs^{-1} \\
  %   & \quad \, -
  %   \frac{3}{2} \probEigenvectors_j \diag (\probEigenvectors^{\T} \EE \diagAdjecencyMatrix \probEigenvectors) \meanEigs^{-2} 
  %   + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k
  %   + O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}}\right) \\
  %   & = \displaceMatrix_j \probEigenvectors \meanEigs^{-1} + (\displaceMatrix^2 - \EE \displaceMatrix^2)_j \probEigenvectors \meanEigs^{-2} + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k\\
  %   & \quad \, + \probEigenvectors_j \left( 1 + \EE \diagAdjecencyMatrix_{jj} \meanEigs^{-2} - \frac{3}{2} \diag (\probEigenvectors^{\T} \EE \diagAdjecencyMatrix \probEigenvectors) \meanEigs^{-2} \right)+ O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}}\right).
  % \end{align*}
  % %
  % Hence,
  % \begin{align*}
  %   \tilde \probEigenvectors_j = & \; 
  %   \displaceMatrix_j 
  %   \probEigenvectors \meanEigs^{-1} 
  %   \left(
  %     1 - \diagAdjecencyMatrix_{jj} \adjacencyEigenvalues^{-2} + \frac{3}{2} \diag(\adjacencyEigenvectors^{\T} \diagAdjecencyMatrix \adjacencyEigenvectors) \adjacencyEigenvalues^{-2}
  %   \right) \\
  %   & + (\displaceMatrix^2 - \EE \displaceMatrix^2)_j \probEigenvectors \meanEigs^{-2} \left(
  %     1 - \diagAdjecencyMatrix_{jj} \adjacencyEigenvalues^{-2} + \frac{3}{2} \diag(\adjacencyEigenvectors^\T \diagAdjecencyMatrix \adjacencyEigenvectors) \adjacencyEigenvalues^{-2}
  %   \right) \\
  %   & + \probEigenvectors_j \left(1 + \EE \diagAdjecencyMatrix_{jj} \meanEigs^{-2} - \frac{3}{2} \diag (\probEigenvectors^{\T} \EE \diagAdjecencyMatrix \probEigenvectors) \meanEigs^{-2} \right) \left(
  %     1 - \diagAdjecencyMatrix_{jj} \adjacencyEigenvalues^{-2} + \frac{3}{2} \diag(\adjacencyEigenvectors^\T \diagAdjecencyMatrix \adjacencyEigenvectors) \adjacencyEigenvalues^{-2}
  %   \right) \\
  %   & + \frac{1}{t_k^2} \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k \left(
  %     1 - \diagAdjecencyMatrix_{jj} \adjacencyEigenvalues^{-2} + \frac{3}{2} \diag(\adjacencyEigenvectors^\T \diagAdjecencyMatrix \adjacencyEigenvectors) \adjacencyEigenvalues^{-2}
  %   \right) \\
  %   & - \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \cdot \sum_{j = 1}^{\nsize} \frac{\diagAdjecencyMatrix_{j j} \adjacencyEigenvectors_{j k'} \adjacencyEigenvectors_{j k}}{\adjacencyEigenvalues_{k k}^2}
  %   + O_\ell \left(\frac{1}{\nsize \sqrt{\nsize \sparsityParam}}\right).
  % \end{align*}
  % %
  % Omitting terms less than $O_\ell \left(\frac{\log \nsize}{\nsize \sqrt{\nsize \sparsityParam}}\right)$, we obtain
  % \begin{align*}
  %   \tilde \probEigenvectors_j & = \displaceMatrix_j 
  %   \probEigenvectors \meanEigs^{-1}
  %   + (\displaceMatrix^2 - \EE \displaceMatrix^2)_j \probEigenvectors \meanEigs^{-2}\\
  %   & + \probEigenvectors_j 
  %   \left( 1
  %     + \EE \diagAdjecencyMatrix_{jj} \meanEigs^{-2} - \diagAdjecencyMatrix_{jj} \adjacencyEigenvalues^{-2} + 
  %     \frac{3}{2} \diag(\adjacencyEigenvectors^\T \diagAdjecencyMatrix \adjacencyEigenvectors) \adjacencyEigenvalues^{-2}
  %     - 
  %     \frac{3}{2} \diag (\probEigenvectors^\T \EE \diagAdjecencyMatrix \probEigenvectors) \meanEigs^{-2}
  %   \right) \\
  %   & + \frac{1}{t_k^2}\sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k 
  %   - \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \cdot \sum_{j = 1}^{\nsize} \frac{\diagAdjecencyMatrix_{j j} \adjacencyEigenvectors_{j k'} \adjacencyEigenvectors_{j k}}{\adjacencyEigenvalues_{k k}^2} 
  %   + O_\ell \left(\frac{\log \nsize}{\sqrt{\nsize^3 \sparsityParam}}\right).
  % \end{align*}
  % %
  % From Lemma~\ref{lemma: second order term estimation}, each diagonal entry of $\frac{3}{2} \diag(\adjacencyEigenvectors^\T \diagAdjecencyMatrix \adjacencyEigenvectors) \adjacencyEigenvalues^{-2} - \frac{3}{2} \diag (\probEigenvectors^\T \EE \diagAdjecencyMatrix \probEigenvectors) \meanEigs^{-2}$ is at most $O_\ell \left(\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}} \right)$. 
  % Thus, $\Vert \frac{3}{2} \diag(\adjacencyEigenvectors^\T \diagAdjecencyMatrix \adjacencyEigenvectors) \adjacencyEigenvalues^{-2} - \frac{3}{2} \diag (\probEigenvectors^\T \EE \diagAdjecencyMatrix \probEigenvectors) \meanEigs^{-2} \Vert = O_\ell \left(\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right)$. 
  % Meanwhile, applying the Bernstein inequality as previously, we obtain
  % \begin{align*}
  %   \EE \diagAdjecencyMatrix_{jj} \meanEigs^{-2} - \diagAdjecencyMatrix_{jj} \adjacencyEigenvalues^{-2} = O_\ell \left(\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right).
  % \end{align*}
  % %
  At the same time, given $k'$, we have
  \begin{align*}
    & \frac{1}{t_k^2} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k 
    -  \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \cdot \sum_{j = 1}^{\nsize} \frac{\diagAdjecencyMatrix_{j j} \adjacencyEigenvectors_{j k'} \adjacencyEigenvectors_{j k}}{\adjacencyEigenvalues_{k k}^2} \\
    & = \left(\frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} - \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}}\right)\cdot \frac{\uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2}
    + \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \left(\frac{\uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} - \frac{\widehat{\uv}_{k'}^\T \diagAdjecencyMatrix \widehat{\uv}_k}{\adjacencyEigenvalues_{k k}}\right).
  \end{align*}
  %
  From Lemma~\ref{lemma: second order term estimation}, we get
  \begin{align*}
    \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \left(\frac{\uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} - \frac{\widehat{\uv}_{k'}^\T \diagAdjecencyMatrix \widehat{\uv}_k}{\adjacencyEigenvalues_{k k}}\right) = \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \cdot O_\ell \left(\sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam^3}}\right).
  \end{align*}
  %
  Next, from Condition~\ref{cond: eigenvalues divergency}, we have $\lambda_{k'} - \lambda_k = \Omega(\nsize \sparsityParam)$. Since $\debiasedEigenvalues_{k' k'} = \lambda_{k'} + O_{\ell}(\sqrt{\sparsityParam \log \nsize})$ due to Lemma~\ref{lemma: debaised eigenvalues behaviour} and $\adjacencyEigenvalues_{kk} = t_k + O_{\ell}(\sqrt{\sparsityParam \log \nsize})$ due to Lemmas~\ref{lemma: log estimate vector difference} and~\ref{lemma: eigenvalues difference}, we have
  \begin{align*}
      \frac{\debiasedEigenvalues_{k' k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} 
      = \frac{O(\nsize \sparsityParam)}{\Omega(\nsize \sparsityParam)} = O(1).
  \end{align*}
  %
  Finally, we have $\adjacencyEigenvectors_{i k'} = \probEigenvectors_{i k'} + O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}} \right)$ due to Lemma~\ref{lemma: adj eigenvectors displacement}. Since $\probEigenvectors_{i k'} = O(\nsize^{-1 / 2})$ due to Lemma~\ref{lemma: eigenvectors max norm}, we conclude that $\adjacencyEigenvectors_{i k'} = O_{\ell}(\nsize^{-1/2})$. Thus, we obtain
  \begin{align*}
      \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \left(\frac{\uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} - \frac{\widehat{\uv}_{k}^\T \diagAdjecencyMatrix \widehat{\uv}_k}{\adjacencyEigenvalues_{k k}}\right) = O \left( \sqrt{\frac{\log \nsize}{\nsize^4 \sparsityParam^3}} \right).
  \end{align*}
  %
  Next, we have
  \begin{align*}
      \left(\frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} - \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}}\right)
      & = \frac{
            \lambda_{k'} (\probEigenvectors_{i k'} - \adjacencyEigenvectors_{i k'}) + (\lambda_{k'} - \debiasedEigenvalues_{k' k'}) \adjacencyEigenvectors_{i k'}
        }{\lambda_{k'} - t_k} \\
        & \quad - \frac{
            (\lambda_{k'} - \debiasedEigenvalues_{k' k'}) - (t_k - \adjacencyEigenvalues_{k k})
        }{
            (\lambda_{k'} - t_k)(\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k})
        } \cdot \debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'} \\
     & = \frac{
        O(\nsize \sparsityParam) \cdot O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}}\right) + O_{\ell}(\sqrt{\sparsityParam \log \nsize}) O(\nsize^{-1/2})
     }{\Omega(\nsize \sparsityParam)} \\
     & \quad + \frac{O_{\ell}(\sqrt{\sparsityParam \log \nsize}) + O_{\ell}(\sqrt{\sparsityParam \log \nsize})}{\Omega(\nsize^2 \sparsityParam^2)} \cdot O(\nsize \sparsityParam) \cdot O(\nsize^{-1/2}) \\
     & = O_\ell \left( \sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}}\right).
  \end{align*}
  %
  The terms above were bounded via Lemmas~\ref{lemma: debaised eigenvalues behaviour},~\ref{lemma: log estimate vector difference},~\ref{lemma: adj eigenvectors displacement} and~\ref{lemma: eigenvalues difference}.
  %
  Since $|\uv_k^\T \EE \diagAdjecencyMatrix \uv_{k'}| \le \Vert \EE \diagAdjecencyMatrix \Vert \le \nsize \sparsityParam$, we have
  \begin{align}
    & \frac{1}{t_k^2} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k 
    -  \frac{\debiasedEigenvalues_{k' k'} \cdot \adjacencyEigenvectors_{i k'}}{\debiasedEigenvalues_{k' k'} - \adjacencyEigenvalues_{k k}} \cdot \sum_{j = 1}^{\nsize} \frac{\diagAdjecencyMatrix_{j j} \adjacencyEigenvectors_{j k'} \adjacencyEigenvectors_{j k}}{\adjacencyEigenvalues_{k k}^2} \label{eq: averaging lemma, bias term} \\
    &= O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}}\right) \cdot \frac{\uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} + O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^4 \sparsityParam^3}}\right) \nonumber
    = O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^4 \sparsityParam^3}}\right). \nonumber
  \end{align}
  % \begin{align*}
  %   \tilde{\probEigenvectors}_{j} = \probEigenvectors_{j} + \displaceMatrix_j \probEigenvectors \meanEigs^{-1} + (\displaceMatrix^2 - \EE \displaceMatrix^2)_j \probEigenvectors \meanEigs^{-2} + O_\ell \left(\sqrt{\frac{\log \nsize }{\nsize^4 \sparsityParam^3}}\right) + O_\ell \left( \sqrt{\frac{\log \nsize }{\nsize^3 \sparsityParam}} \right).
  % \end{align*}
  %
  Combining~\eqref{eq: averaging lemma, D term approximation},~\eqref{eq: averaging lemma, quad form factor approximation} and~\eqref{eq: averaging lemma, bias term} and using $\adjacencyEigenvectors_{ik} = O_{\ell}(\nsize^{-1/2})$, we obtain
  \begin{align*}
      \tilde \probEigenvectors_{ik} = \adjacencyEigenvectors_{ik} \left( 1 - \frac{\EE \diagAdjecencyMatrix_{ii} - 3/2 \cdot \widehat{\uv}_{k}^\T \EE \diagAdjecencyMatrix \widehat{\uv}_k}{t_k^2}\right) - \sum_{k' \in [\nclusters] \setminus \{k\} } \frac{\lambda_{k'} \probEigenvectors_{i k'} }{\lambda_{k'} - t_k} \cdot \frac{\uv_{k'}^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} + O_{\ell} \left(\sqrt{\frac{\log \nsize}{\nsize^4 \sparsityParam^3}} \right).
  \end{align*}
  %
  We substitute asymptotic expansion from Lemma~\ref{lemma: eigenvector power expansion} instead of $\adjacencyEigenvectors_{ik}$, and, using bounds~\eqref{eq: U hat terms asumptotics-f}-\eqref{eq: U hat asymptotics-l}, obtain:
  \begin{align*}
      \tilde \probEigenvectors_{i k} & = \probEigenvectors_{i k} + \frac{\ev_i^\T \displaceMatrix \uv_k}{t_k} + \frac{\ev_i^\T \displaceMatrix^2 \uv_k}{t_k^2} - \frac{3}{2} \cdot \probEigenvectors_{ik} \frac{\uv_k^\T \EE \displaceMatrix^2 \uv_k}{t_k^2} \\
      & \quad + \frac{1}{t_k^2} \sum_{k' \in [\nclusters] \setminus\{k\} } \frac{\lambda_{k'} \probEigenvectors_{i k'} }{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k - \probEigenvectors_{ik} \frac{\EE \diagAdjecencyMatrix_{ii}}{t_k^2} + \frac{3}{2} \probEigenvectors_{i k} \frac{\uv_k^\T \EE \diagAdjecencyMatrix \uv_k}{t_k^2} \\
      & \quad - \frac{1}{t_k^2} \sum_{k' \in [\nclusters] \setminus \{k\} } \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T \EE \displaceMatrix^2 \uv_k + O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right) \\
      & = \probEigenvectors_{i k} + \frac{\ev_i^\T \displaceMatrix \uv_k}{t_k} + \frac{\ev_i^\T (\displaceMatrix^2 - \EE \displaceMatrix^2) \uv_k}{t_k^2} + \frac{\ev_i^\T (\EE \displaceMatrix^2 - \EE \diagAdjecencyMatrix) \uv_k}{t_k^2} \\
      & \quad + \frac{3}{2} \probEigenvectors_{ik} \cdot \frac{\uv_k^\T (\EE \diagAdjecencyMatrix - \EE \displaceMatrix^2) \uv_k}{t_k^2} \\
      & \quad + \frac{1}{t_k^2} \sum_{k' \in [\nclusters] \setminus \{k\}} \frac{\lambda_{k'} \probEigenvectors_{i k'}}{\lambda_{k'} - t_k} \cdot \uv_{k'}^\T (\EE \displaceMatrix^2 - \EE \diagAdjecencyMatrix) \uv_k+ O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right),
  \end{align*}
  where we use $\nsize^4 \sparsityParam^3 \ge \nsize^3 \sparsityParam$, provided $\sparsityParam \ge \nsize^{-1/2}$ due to Condition~\ref{cond: sparsity param bound}. We have 
  \begin{equation*}
      (\EE \displaceMatrix^2)_{ij} = \indicator[i = j] \sum_{t = 1}^\nsize \probMatrix_{it} - \probMatrix_{i t}^2
      = \EE \diagAdjecencyMatrix_{ij} - \indicator[i = j] \sum_{t = 1}^\nsize \probMatrix^2_{it}.
  \end{equation*}
  %
  Consequently, we have $\Vert \EE \diagAdjecencyMatrix - \EE \displaceMatrix^2 \Vert = O(\nsize \sparsityParam^2)$ and
  \begin{align*}
      \ev_i^\T (\EE \displaceMatrix^2 - \EE \diagAdjecencyMatrix) \uv_k = (\EE \displaceMatrix^2 - \EE \diagAdjecencyMatrix)_i \probEigenvectors_{ik} \overset{\text{Lemma~\ref{lemma: eigenvectors max norm}}}{=} O(\nsize^{1/2} \sparsityParam^2).
  \end{align*}
  %
  Analogously, we have
  \begin{align*}
      \probEigenvectors_{i k'} \cdot \frac{\uv_{k'}^\T (\EE \diagAdjecencyMatrix - \EE \displaceMatrix^2) \uv_k}{t_k^2} \le t_k^{-2} |\probEigenvectors_{i k'}| \cdot \Vert \EE \diagAdjecencyMatrix - \EE \displaceMatrix^2 \Vert = O \left( \sqrt{\frac{1}{\nsize^3}} \right)
  \end{align*}
  for any $k' \in [\nclusters]$. Since $\frac{\lambda_{k'}}{\lambda_{k'} - t_k} = O(1)$ for any $k' \in [\nclusters] \setminus \{k\}$, we get
  \begin{align*}
      \tilde \probEigenvectors_i = \probEigenvectors_i + \ev_i^\T \displaceMatrix \probEigenvectors \meanEigs^{-1} + \ev_i^\T (\displaceMatrix^2 - \EE \displaceMatrix^2) \probEigenvectors \meanEigs^{-2} + O_{\ell} \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right),
  \end{align*}
  where $\meanEigs = \diag(t_k)_{k = 1}^\nclusters$.
  
  If $j \in \pureNodesSet_k$, then $\probEigenvectors_j = \basisMatrix_{k}$. Hence,
  \begin{align*}
    \frac{1}{|\pureNodesSet_k|} 
    \sum_{j \in \pureNodesSet_k} 
    \tilde{\probEigenvectors}_{j} 
    = 
    \basisMatrix_k 
    + 
    \frac{1}{\sqrt{|\pureNodesSet_k|}} 
    \mathbf{r}^{\T}
    \displaceMatrix \probEigenvectors \meanEigs^{-1}
    + 
    \frac{1}{\sqrt{|\pureNodesSet_k|}} 
    \mathbf{r}^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \probEigenvectors \meanEigs^{-2}
    +
    O_\ell \left( \sqrt{\frac{\log \nsize}{\nsize^3 \sparsityParam}} \right)
  \end{align*}
  for a unit vector $\mathbf{r} = \frac{1}{\sqrt{|\pureNodesSet_k|}} \sum_{j \in \pureNodesSet_k} \ev_j$. Finally, applying Lemma~\ref{lemma: W squared bilinear form} and Lemma~\ref{lemma: log estimate vector difference}, we derive
  \begin{align*}
    \mathbf{r}^{\T} \displaceMatrix \probEigenvectors \meanEigs^{-1} = O_\ell \left(\sqrt{\frac{\log \nsize}{\nsize^2 \sparsityParam}} \right), \\
    \mathbf{r}^{\T} (\displaceMatrix^2 - \EE \displaceMatrix^2) \probEigenvectors \meanEigs^{-2} = O_\ell \left(\frac{\log \nsize}{\sparsityParam \sqrt{\nsize^3}} \right).
  \end{align*}
  %
  Consequently, we have $\frac{1}{|\pureNodesSet_k|} 
    \sum_{j \in \pureNodesSet_k} 
      \tilde{\probEigenvectors}_{j} 
    = \basisMatrix_k + O_\ell \left( \frac{\log \nsize}{\nsize \sqrt{\nsize \sparsityParam}}\right)$. Next, we substitute this average into~\eqref{eq: averaging lemma, pure node set decomposition} and obtain the statement of the lemma.
\end{proof}
