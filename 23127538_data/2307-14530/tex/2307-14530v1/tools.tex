% !TEX root = jmlr_version.tex

\subsubsection{Useful lemmas from previous studies}
  \newtheorem{conditionFan}{Condition}
  \renewcommand*{\theconditionFan}{\Alph{conditionFan}}
  \newtheorem{theoremFan}{Theorem}
  \renewcommand*{\thetheoremFan}{\Alph{theoremFan}}

  We widely use results from~\cite{Fan2019_SIMPLE} and~\cite{Fan2020_ASYMPTOTICS}, so we write a special section that summarizes these results.


\subsubsection{Conditions}
  First, we must show that conditions demanded in~\cite{Fan2019_SIMPLE} and~\cite{Fan2020_ASYMPTOTICS} hold under our conditions. Let us first review these conditions. 

  \begin{conditionFan}
  \label{cond: fan_condition_1}
    There exists some positive constant $c_0$ such that 
    \begin{align*}
      \min \left\{
        \frac{|\lambda_i(\probMatrix)|}{|\lambda_j (\probMatrix)|} 
        \mid
        1 \leqslant i < j \leqslant \nclusters,
        \lambda_i(\probMatrix) \neq \lambda_j(\probMatrix)
      \right\}
      \geqslant
      1 + c_0.
    \end{align*}
    %
    In addition, 
    \begin{align*}
      \alpha_{\nsize} := \left\{
        \max_{1 \leqslant j \leqslant \nsize}
          \sum_{i = 1}^\nsize
          \Var(\displaceMatrix_{ij})
      \right\}^{1/2} \underset{\nsize \to \infty}{\longrightarrow} \infty.
    \end{align*}
  \end{conditionFan}

  \begin{conditionFan}
  \label{cond: fan_condition_2}
    There exist some constants $0 < c_0, c_1 < 1$ such that $\lambda_{\nclusters}(\nodeCommunityMatrix^{\T} \nodeCommunityMatrix) \geqslant c_0 \nsize$, $|\lambda_{\nclusters}(\probMatrix)| \geqslant c_0$, and $\sparsityParam \geqslant \nsize^{-c_1}$.
  \end{conditionFan}

  %Third condition is defined for a pair of nodes $i$, $j$ and covariance matrix $\asymptoticVariance(i, j)$:
  % \begin{conditionFan}
  % \label{cond: fan_condition_3}
  % As $\nsize \to \infty$, all the eigenvalues of $\nsize^2 \sparsityParam \asymptoticVariance(i, j)$ are bounded away from 0 and $\infty$.
  % \end{conditionFan}
  %
  %It is necessary for statistical properties of $\estimator[\equalityStatistic]_{ij}$.

  In this way, we prove the following theorem.
  \begin{theoremFan}
  \label{theorem: conditions satisfuction}
    Assume Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} hold. Then Conditions~\ref{cond: fan_condition_1}-\ref{cond: fan_condition_2} are satisfied. Moreover, $\alpha_{\nsize} = O(\sqrt{\nsize \sparsityParam})$.
  \end{theoremFan}

  \begin{proof}
    Condition~\ref{cond: eigenvalues divergency} implies Condition~\ref{cond: fan_condition_1} directly. Condition~\ref{cond: fan_condition_2} is valid due to Lemma~\ref{lemma: eigenvalues asymptotics} and Condition~\ref{cond: sparsity param bound}. Finally, we have
    \begin{align*}
        \alpha_n^2 = \max_{j} \sum_{i = 1}^\nsize \probMatrix_{ij} (1 - \probMatrix_{i j}) \le \sparsityParam \nsize. & \qedhere
    \end{align*}
  \end{proof}

  Thus, under Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-a} we can use key statements from~\cite{Fan2019_SIMPLE} and~\cite{Fan2020_ASYMPTOTICS} that are summarized below.


\subsubsection{Lemmas}
  \begin{lemma}[Lemma 6 from~\cite{Fan2019_SIMPLE}]
  \label{lemma: eigenvectors max norm}
    Under Conditions~\ref{cond: fan_condition_1}-\ref{cond: fan_condition_2} there exists such constant $C_{\probEigenvectors}$ that
    \begin{align}
      \max_{ij} |\probEigenvectors_{ij}| \leqslant \frac{C_{\probEigenvectors}}{\sqrt{\nsize}}.
    \end{align}
  \end{lemma}

  Next, we provide an asymptotic expansion of $\xv^{\T} \estimator[\uv]_k \estimator[\uv]_k^{\T} \yv$. Its form is a bit sophisticated and demands auxiliary notation described in Table~\ref{tab: expansion notation}. In addition, it involves the solution of equation~\eqref{eq: t_k definition}. The following lemma guarantees that it is well-defined.
  \begin{lemma}[Lemma 3 from~\cite{Fan2020_ASYMPTOTICS}]
  \label{lemma: t_k is well-definied}
    Under Condition~\ref{cond: fan_condition_1}, equation~\eqref{eq: t_k definition} has an unique solution in the interval $z \in [a_k, b_k]$ and thus $t_k$'s are well-defined. Moreover, for each $1 \le k \le \nclusters$, we have $t_k/\lambda_k(\probMatrix) \to 1$ as $\nsize \to \infty$.
  \end{lemma}

  Now we provide the necessary asymptotics.
  \begin{lemma}[Theorem 5 from~\cite{Fan2020_ASYMPTOTICS}]
  \label{lemma: fan eigenvectors series decomposition}
    Assume that Conditions~\ref{cond: fan_condition_1}-\ref{cond: fan_condition_2} hold and $\xv$ and $\yv$ are two $\nsize$-dimensional unit vectors. Then for each $1 \le k \le \nclusters$, if $\sigma_k^2 = O(\tilde{\sigma}_k^2)$ and $\tilde{\sigma}_k^2 \gg t_k^{-4} (|\meanFactor_{\xv, k, t_k}| + |\meanFactor_{\yv, k, t_k}|)^2 + t_k^{-6}$, we have the asymptotic expansion
    \begin{align*}
      \xv^{\T} \estimator[\uv]_k \estimator[\uv]_k^{\T} \yv = \, & a_k 
      + 
      \tr [
        \displaceMatrix \jMatrix_{\xv, \yv, k, t_k} - (\displaceMatrix^2 - \EE \displaceMatrix^2) \lMatrix_{\xv, \yv, k, t_k}
      ]
      + \tr (\displaceMatrix \uv_k \uv_k^{\T}) \tr (\displaceMatrix \qMatrix_{\xv, \yv, k, t_k}) \\
      & + O_p \left(|t_k|^{-3} \alpha_\nsize^2 (|\meanFactor_{\xv, k, t_k}| + |\meanFactor_{\yv, k, t_k}|) + |t_k|^{-3} \right),
    \end{align*}
    where $a_k = \meanFactor_{\xv, k, t_k} \meanFactor_{\yv, k, t_k} \pFactor_{k, t_k}$.
  \end{lemma}

  \begin{lemma}[see Lemma~10 from~\cite{Fan2019_SIMPLE} and its proof]
  \label{lemma: corrected eigs and noise}
    Under Conditions~\ref{cond: fan_condition_1}-\ref{cond: fan_condition_2} it holds that
    \begin{align}
      \debiasedEigenvalues_{kk} = \lambda_k(\probMatrix) + O_{\prec} \left(
        \sqrt{\sparsityParam} + \frac{1}{\sqrt{\nsize \sparsityParam}}
      \right)
    \end{align}
    and uniformly over all $i, j$ 
    \begin{align}
      \estimator[\displaceMatrix]_{ij} = \displaceMatrix_{ij} + O_{\prec}\left( \sqrt{\frac{\sparsityParam}{\nsize}}\right).
    \end{align}
  \end{lemma}

  \begin{lemma}[Lemma 9 from~\cite{Fan2019_SIMPLE}]
  \label{lemma: adj_eigenvectors_displacement}
    Under Conditions~\ref{cond: fan_condition_1}-\ref{cond: fan_condition_2}, we have
    \begin{align}
      \adjacencyEigenvectors_{ik} = \probEigenvectors_{ik} + \frac{1}{t_{k}} \displaceMatrix_{i} \uv_k + O_{\prec} \left( 
        \frac{\alpha_\nsize^2}{\sqrt{\nsize} t_{k}^2}
        +
        \frac{1}{|t_k| \sqrt{\nsize}}
      \right),
    \end{align}
    where $\uv_k$ is the $k$-th column of the matrix $\probEigenvectors$.
  \end{lemma}

  \begin{lemma}[Lemma 8 from~\cite{Fan2019_SIMPLE}]
  \label{lemma: eigenvalues difference}
    Under Conditions~\ref{cond: fan_condition_1}-\ref{cond: fan_condition_2}, for each $1 \le k \le \nclusters$ we have
    \begin{align*}
      \adjacencyEigenvalues_{kk} - t_k = \uv_k^{\T} \displaceMatrix \uv_k + O_{\prec} \left( \frac{\alpha_{\nsize}^2}{\sqrt{\nsize} \lambda_k(\probMatrix)}\right).
    \end{align*}
  \end{lemma}

  \begin{lemma}[Lemma 11 and Corollary 3 from~\cite{Fan2019_SIMPLE}]
  \label{lemma: power deviation}
    For any $\nsize$-dimensional unit vectors $\xv, \yv$ and any positive integer $r$, we have 
    \begin{align*}
      \EE \left[ 
        \xv^{\T} (\displaceMatrix^\ell - \EE \displaceMatrix^\ell) \yv
      \right]^{2r} \le C_r (\min(\alpha_{\nsize}^{\ell - 1}, \Vert \xv \Vert_{\infty} \alpha_{\nsize}^\ell, \Vert \yv \Vert_{\infty} \alpha_{\nsize}^\ell)^{2r},
    \end{align*}
    where $\ell$ is any positive integer and $C_r$ is some positive constant determined only by $r$. Additionnally, we have
    \begin{align*}
        \xv^\T (\displaceMatrix^\ell - \EE \displaceMatrix^\ell ) \yv = O_{\prec} (\min(\alpha_{\nsize}^{\ell - 1}, \Vert \xv \Vert_{\infty} \alpha_{\nsize}^\ell, \Vert \yv \Vert_{\infty} \alpha_{\nsize}^\ell).
    \end{align*}
  \end{lemma}

  \begin{lemma}[Lemma 12 from~\cite{Fan2019_SIMPLE}]
  \label{lemma: power expectation}
    For any $\nsize$-dimensional unit vectors $\xv$ and $\yv$, we have
    \begin{align*}
      \EE \xv^{\T} \displaceMatrix^\ell \yv = O(\alpha_{\nsize}^\ell),
    \end{align*}
    where $\ell \ge 2$ is a positive integer. Furthermore, if the number of nonzero components of $\xv$ is bounded, then it holds that
    \begin{align*}
      \EE \xv^{\T} \displaceMatrix^\ell \yv = O(\alpha_{\nsize}^{\ell} \Vert \yv \Vert_{\infty}).
    \end{align*}
  \end{lemma}

  Table~\ref{tab: expansion notation} summarizes the notations from~\cite{Fan2020_ASYMPTOTICS} that are needed for the proofs of our results.
  \begin{table}[t!]
    \centering
    \begin{tabular}{|l|}
        \hline
         Auxiliary variables \\
        \hline
        \\
         $L = \min \left\{\ell \mid \left( \frac{\alpha_{\nsize}}{\max\{|a_k|, |b_k|\}}\right)^\ell \le \min \left\{\frac{1}{\nsize^4}, \frac{1}{\max\{|a_k|^4, |b_k|^4\}}\right\} \right\}$ \\ \\
        $\resolvent(\mathbf{M}_1, \mathbf{M}_2, t) = 
      - \frac{1}{t} \mathbf{M}_1^{\T} \mathbf{M}_2 - \sum_{l = 2}^{L} 
      t^{-(l + 1)} \mathbf{M}_1^{\T} \EE \displaceMatrix^{l} \mathbf{M}_2$
      \\ \\
      $\tResolvent(\mathbf{M}_1, \mathbf{M}_2, t) = t \resolvent(\mathbf{M}_1, \mathbf{M}_2, t)$
      \\ \\
      $\bv_{\xv, k, t} = \xv - \probEigenvectors_{-k}
      \left[ 
        \probEigenvalues^{-1}_{-k} + 
        \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t)
      \right]^{-1}
      \resolvent^{\T}(\xv, \probEigenvectors_{-k}, t)$
      \\ \\
      \hline
      0-degree coefficients \\
      \hline
      \\
      $\meanFactor_{\xv, k, t} = \tResolvent (\xv, \uv_k, t) - 
      \tResolvent(\xv, \probEigenvectors_{-k}, t)
      \left[
        t \probEigenvalues_{-k}^{-1} + 
        \tResolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t)
      \right]^{-1}
      \tResolvent (\probEigenvectors_{-k}, \uv_k, t)$
      \\ \\
      $\pFactor_{k, t} = \left[
        t^2 \frac{d}{dt} \left(
          \frac{\meanFactor_{\uv_k, k, t}}{t}
        \right)
      \right]^{-1}$
      \\ \\
      \hline
      First degree coefficients \\
      \hline \\
      $\jMatrix_{\xv, \yv, k, t_k} 
          = 
          - \pFactor_{k, t_k} t_k^{-1} \uv_k
          \left(
            \meanFactor_{\yv, k, t_k} \bv_{\xv, k, t_k}^{\T} 
            + 
            \meanFactor_{\xv, k, t_k} \bv_{\yv, k, t_k}^{\T} 
            +
            2 \meanFactor_{\xv, k, t_k} \meanFactor_{\yv, k, t_k} \pFactor_{k, t_k} \uv_k^{\T}
         \right)$\\ \\
        \hline
        Second degree coefficients \\
        \hline \\
        $\lMatrix_{\xv, \yv, k, t_k} = 
        \pFactor_{k, t_k} t_k^{-2} \uv_k 
        \Bigl\{ 
          \bigl[
            \meanFactor_{\yv, k, t_k} \resolvent(\xv, \probEigenvectors_{-k}, t_k) 
            +
            \meanFactor_{\xv, k, t_k} \resolvent(\yv, \probEigenvectors_{-k}, t_k)
          \bigr] \times $ \\
          \qquad \qquad \; 
          $\times \bigl[
            \probEigenvalues_{-k}^{-1} + \resolvent(\probEigenvectors_{-k}, \probEigenvectors_{-k}, t_k)
          \bigr]^{-1} \probEigenvectors_{-k}^{\T} + \meanFactor_{\yv, k, t_k} \xv^{\T} + \meanFactor_{\xv, k, t_k} \yv^{\T} + 3 \meanFactor_{\xv, k, t_k} \meanFactor_{\yv, k, t_k} \uv_k^{\T}
        \Bigr\}$ \\ \\
        $\qMatrix_{\xv, \yv, k, t_k} = 
        \lMatrix_{\xv, \yv, k, t_k} 
        - 
        \pFactor_{k, t_k} t_k^{-2} \meanFactor_{\xv, k, t_k} 
        \meanFactor_{\yv, k, t_k} \uv_k \uv_k^{\T} 
        + 
        2 \pFactor^2_{k, t_k} t_k^{-2} \uv_k 
        \left(
          \meanFactor_{\xv, k, t_k} \bv_{\xv, k, t_k}^{\T} + \meanFactor_{\yv, k, t_k} \bv_{\yv, k, t_k}^{\T}
        \right)
        $ \\ \\
        \hline
        Applicability parameters \\
        \hline \\
        $\sigma_k^2 = \Var\bigl[\tr(\displaceMatrix \jMatrix_{\xv, \yv, k, t_k})\bigr]$ \\ \\
        $\tilde{\sigma}_k^2 = \Var \left\{
          \tr \bigl[
            \displaceMatrix \jMatrix_{\xv, \yv, k, t_k}
            -
            (\displaceMatrix^2 - \EE \displaceMatrix) \lMatrix_{\xv, \yv, k, t_k}
          \bigr]
          +
          \tr \left( \displaceMatrix \uv_k \uv_k^{\T} \right)
          \tr \left( \displaceMatrix \qMatrix_{\xv, \yv, k, t_k} \right)
        \right\}$ \\ \\
        \hline
    \end{tabular}
    \caption{
    Here $\probEigenvectors_{-k}$ is the matrix $\probEigenvectors$ with a $k$-th column removed and $\probEigenvalues_{-k}$ is a diagonal matrix that contains all eigenvalues except $k$-th one, while $t_k$ is the solution of~\eqref{eq: t_k definition}.
    }
    \label{tab: expansion notation}
  \end{table}

% \begin{lemma}[Theorem 5 from~\cite{Fan2020_ASYMPTOTICS}]
% \label{lemma: asymptotic variance estimator}
% Under Conditions~\ref{cond: fan_condition_1}-\ref{cond: fan_condition_3}
% \begin{align}
%     \left \Vert
%         \estimator[\asymptoticVariance](i, j)
%         -
%         \asymptoticVariance(i, j)
%     \right \Vert_2
%     =
%     o_p \left(
%         \frac{1}{\nsize^2 \sparsityParam}
%     \right).
% \end{align}
% \end{lemma}

% \begin{lemma}[Lemma 4 from~\cite{Fan2019}]
% \label{lemma: convergence to normal}
% Let $m$ be a fixed positive integer, $\mathbf{x}_i$ and $\mathbf{y}_i$ be $\nsize$-dimensional unit vectors for $1 \leqslant i \leqslant m$, and $\asymptoticVariance = (\Sigma_{ij})$ the covariance matrix with $\Sigma_{ij} = \cov \left( \mathbf{x}_i^{\T} \displaceMatrix \mathbf{y}_i, \mathbf{x}_j^{\T} \displaceMatrix \mathbf{y}_j \right)$. Assume that there exists some positive sequence $(h_\nsize)$ such that 
% \begin{align}
%     \Vert \asymptoticVariance^{-1} \Vert
%     \sim 
%     \Vert \asymptoticVariance \Vert 
%     \sim h_\nsize
%     \text{ and } 
%     \max_k \left\{ 
%         \Vert \mathbf{x}_k \Vert_{\infty}
%         \Vert \mathbf{y}_k \Vert_{\infty} 
%     \right\}
%     \ll 
%     \Vert \Sigma^{-\frac{1}{2}} \Vert_2.
% \end{align} 
% Then it holds that
% \begin{align}
%     \asymptoticVariance^{- \frac{1}{2}}
%     \left( 
%         \xv_1^{\T} (\displaceMatrix - \EE \displaceMatrix) \yv_1,
%         \ldots,
%         \xv_\nsize^{\T} (\displaceMatrix - \EE \displaceMatrix) \yv_\nsize
%     \right)
%     \weakConverges
%     \normDistribution(0, \identity).
% \end{align}
% \end{lemma}


\subsubsection{Concentration inequalities}
  Across this paper, we use several concentration inequalities. We listed them here. The first one is the Bernstein inequality. For the proof one can see, for example, \S~2.8 in the book by~\citet{Boucheron2013}.

  \begin{lemma}[Bernstein inequality]
  \label{lemma: bernstein inequality}
    Let $X_1, \ldots, X_\nsize$ be independent random variables with zero mean. Assume that each of them is bounded by some constant $M$. Then for all $t > 0$:
    \begin{align*}
      \PP \left( 
        \sum_{i = 1}^\nsize X_i \ge t
      \right)
      \le 
      \exp \left(
        -\frac{t^2 / 2}{\sum_{i = 1}^\nsize \EE X_i^2 + M t / 3}
      \right).
    \end{align*}
  \end{lemma}

  The Bernstein inequality can be generalized in two ways. For martingales, it appears to have almost the same form.
  \begin{lemma}[Freedman inequality]
  \label{lemma: freedman inequality}
    Let $X_1, \ldots, X_\nsize$ be a sequence of martingale differences with respect to filtration $\mathcal{F}_0 \subset \mathcal{F}_1 \subset \mathcal{F}_2 \subset \ldots \subset \mathcal{F}_\nsize$, where $\mathcal{F}_0$ is the trivial $\sigma$-algebra. Assume that each $X_i$ is bounded by some constant $M$. Then for any $\sigma^2 > 0$ and $t > 0$ it holds that
    \begin{align*}
      \PP \left( 
        \exists k \text{ s.t. } \sum_{i = 1}^k \EE (X_i^2 \mid \mathcal{F}_{i - 1})
        \le \sigma^2 \text{ and } \sum_{i = 1}^k X_i \ge t
      \right)
       \le \exp \left(
       - \frac{t^2 / 2}{\sigma^2 + M t / 3}
       \right).
    \end{align*}
  \end{lemma}
  %
  This inequality was obtained by David Freedman in his seminal work of~\citet{Freedman75}. 

  Another way of generalization is extension of the Bernstein inequality for random matrices:
  \begin{lemma}[Matrix Bernstein inequality]
  \label{lemma: matrix bernstein inequality}
    Let $\mathbf{X}_1, \ldots, \mathbf{X}_\nsize$ be independent zero-mean $a \times b$ random matrices such that their norms are bounded by some constant $M$. Then, for all $t > 0$ it holds that
    \begin{align*}
      \PP \left( 
        \left \Vert \sum_{i = 1}^\nsize \mathbf{X}_i \right \Vert \ge t
      \right)
      \le (a + b) \exp \left(
        -\frac{t^2/2}{\sigma^2 + M t / 3}
      \right),
    \end{align*}
    where 
    \begin{align*}
      \sigma^2 = \max \left( 
        \left \Vert 
          \sum_{i = 1}^\nsize \EE (\mathbf{X}_i \mathbf{X}_i^{\T})
        \right \Vert,
        \left \Vert
          \sum_{i = 1}^\nsize \EE (\mathbf{X}_i^{\T} \mathbf{X}_i)
        \right \Vert
      \right).
    \end{align*}
  \end{lemma}
  %
  For the proof we refer reader to the book by~\citet{Tropp2015}.


\subsubsection{Properties of SPA}
  This part describes the properties of SPA procedure, see Algorithm~\ref{algo: spa}. Here we use the same notation as~\citet{Mizutani2016}. Thus, we denote
  \begin{align}
    \mathbf{A} = \mathbf{F} \mathbf{W} \text{ for } \mathbf{F} \in \RR^{d \times r}_+ \text{ and } \textbf{W} = (\identity, \mathbf{K}) \mathbf{\Pi} \in \RR^{r \times m}_+,
  \label{eq:spa_decomposition}
  \end{align}
  where $\identity$ is an $r \t r$ identity matrix, $\mathbf{K}$ is an $r \t (m - r)$ nonnegative matrix, and $\mathbf{\Pi}$ is an $m \t m$ permutation matrix. Then, the following theorem holds.
  \begin{lemma}[Theorem 1 from~\cite{Mizutani2016}]
  \label{lemma: spa robustness}
    Let $\widetilde{\mathbf{A}} = \mathbf{A} + \mathbf{N}$ for $\mathbf{A} \in \RR^{d \times m}$ and $\mathbf{N} \in \RR^{d \times \nsize}$. Suppose that $r > 2$ and $\mathbf{A}$ satisfies equation~\eqref{eq:spa_decomposition}. If row $\mathbf{n}_i$ of $\mathbf{N}$ satisfies $\Vert \mathbf{n}_i \Vert_2 \leqslant \varepsilon$ for all $i = 1, \ldots, m$ with
    \begin{align}
      \varepsilon < \min \left(
        \frac{1}{
          2 \sqrt{r - 1}
        },
        \frac{1}{4}
      \right)
      \frac{
        \sigma_{min} (\mathbf{F})
      }{
        1 + 80 \kappa(\mathbf{F})
      },
    \end{align}
    then, SPA with input $(\widetilde{\mathbf{A}}, r)$ returns the output $\mathcal{I}$ such that there is an order of the elements in $\mathcal{I}$ satisfying
    \begin{align}
      \Vert
        \widetilde{\mathbf{a}}_{\mathcal{I}(j)} - \mathbf{f}_j
      \Vert_2
      \leqslant
      \varepsilon 
      \bigl(1 + 80 \kappa (\mathbf{F})\bigr).
    \end{align}
  \end{lemma}


