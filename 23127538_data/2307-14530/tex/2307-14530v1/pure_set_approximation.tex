% !TEX root = jmlr_version.tex

The aim of this section is to investigate the difference between $\estimator[\pureNodesSet]_k = \{j \mid \estimator[\equalityStatistic]_{i_k j}^\penalizer < t_{\nsize}\}$ and $\pureNodesSet_k$. For a reminder, we have defined
\begin{align*}
    \equalityStatisticCenter^\penalizer_{ij} & = (\probEigenvectors_i - \probEigenvectors_j)
    \left( \asymptoticVariance(i, j) + \penalizer \identity \right)^{-1}
    (\probEigenvectors_i - \probEigenvectors_j)^{\T}, \\
    \equalityStatistic^\penalizer_{ij} & = (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j) 
    \left(\asymptoticVariance(i, j) + \penalizer \identity\right)^{-1}
    (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j)^{\T}, \\
    \estimator[\equalityStatistic]^\penalizer_{ij} & = (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j) 
    \left(\estimator[\asymptoticVariance](i, j) + \penalizer \identity\right)^{-1}
    (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j)^{\T}.
\end{align*}
We start with concentration of $\estimator[\equalityStatistic]_{i j}^\penalizer$.

\begin{lemma}
\label{lemma: statistic difference}
  Consider two arbitrary indices $i, j \in [\nsize]$. Then for each $\varepsilon$ there exist $n_0 \in \mathbb{N}$ and $\delta_1, \delta_2 > 0$ such that for any $\nsize \ge n_0$
  \begin{align*}
    \PP \left(
      \bigl| 
        \estimator[\equalityStatistic]_{ij}
        -
        \equalityStatisticCenter^\penalizer_{ij}
      \bigr|
      \ge \sqrt{\nsize \sparsityParam} \Vert \nodeCommunityMatrix_i - \nodeCommunityMatrix_j \Vert_2 \cdot \delta_1 \sqrt{\log \nsize}
      + 
      \delta_2 \log \nsize + \nsize^{1 - 1/12} \sparsityParam \Vert \nodeCommunityMatrix_i - \nodeCommunityMatrix_j \Vert^2
    \right)
    \le \nsize^{-\varepsilon}.
  \end{align*}
\end{lemma}

\begin{proof}
  Define
  \begin{equation*}
      \asymptoticVariance_\penalizer(i, j) = \asymptoticVariance(i, j) + \penalizer \identity, \quad \quad
      \estimator[\asymptoticVariance]_{\penalizer}(i, j) = \estimator[\asymptoticVariance](i, j) + \penalizer \identity.
  \end{equation*}
  %
  We denote $\xi_i = \adjacencyEigenvectors_i - \probEigenvectors_i - \displaceMatrix_i \probEigenvectors \probEigenvalues^{-1}$ and observe:
  \begin{align}
    \, & \equalityStatistic^\penalizer_{ij} = 
    \equalityStatisticCenter^\penalizer_{ij} + 
    (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1}
    \asymptoticVariance_\penalizer^{-1}(i, j)
    (\probEigenvectors_i - \probEigenvectors_j)^{\T}
    + (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1}
    \asymptoticVariance_\penalizer(i, j)^{-1}
    \left(
      \adjacencyEigenvectors_i - \adjacencyEigenvectors_j
    \right)^{\T} \nonumber \\
    & +
    (\xi_i - \xi_j) \asymptoticVariance_\penalizer(i, j)^{-1} (\probEigenvectors_i - \probEigenvectors_j)^{\T}
    +
    (\xi_i - \xi_j) \asymptoticVariance_\penalizer(i, j)^{-1} (\adjacencyEigenvectors_i - \adjacencyEigenvectors_j)^{\T} \nonumber \\
    \label{eq: statistic-center difference}
    = \, &
    \equalityStatisticCenter^\penalizer_{ij} + 
    2 (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1}
    \asymptoticVariance_\penalizer^{-1}(i, j)
    (\probEigenvectors_i - \probEigenvectors_j)^{\T}
    + 
    (\displaceMatrix_i - \displaceMatrix_j) 
    \probEigenvectors \probEigenvalues^{-1}
    \asymptoticVariance_\penalizer(i, j)^{-1}
    \probEigenvalues^{-1}
    \probEigenvectors^{\T}
    (\displaceMatrix_i - \displaceMatrix_j)^{\T} \nonumber \\
    & +
    2 (\xi_i - \xi_j)
    \asymptoticVariance_\penalizer(i, j)^{-1}
    \left(
      \probEigenvectors_i - \probEigenvectors_j +
      (\displaceMatrix_i - \displaceMatrix_j)
      \probEigenvectors 
      \probEigenvalues^{-1}
    \right)^{\T}
    +
    (\xi_i - \xi_j) 
    \asymptoticVariance_\penalizer(i, j)^{-1}
    (\xi_i - \xi_j)^{\T}.
  \end{align}
  %
  Due to Lemma~\ref{lemma: log estimate vector difference} and Lemma~\ref{lemma: eigenvectors max norm}, we have $\ev_i^\T \displaceMatrix \uv_k = O_\ell(\sqrt{\sparsityParam \log \nsize})$ for any $i$. So, from Lemma~\ref{lemma: eigenvalues asymptotics} we get
  \begin{align*}
      \displaceMatrix_i \probEigenvectors \probEigenvalues^{-1} = O_\ell(\sqrt{\sparsityParam \log \nsize}) \cdot O \Biggl ( \frac{1}{\nsize \sparsityParam} \Biggr ) = O_\ell \Biggl (
          \frac{\sqrt{\log \nsize}}{\nsize \sqrt{\sparsityParam}}
        \Biggr ),
  \end{align*}
  %
  Thus, we have
  \begin{align}
  \label{eq: displacement 1 statistic differennce lemma}
    \max_{i, j}
    \Vert
      (\displaceMatrix_i - \displaceMatrix_j) 
      \probEigenvectors
      \probEigenvalues^{-1}
    \Vert_2
    =
    O_\ell \left(
      \frac{\sqrt{\log \nsize}}{\nsize \sqrt{\sparsityParam}}
    \right).
  \end{align}
  %
  Besides, according to Lemma~\ref{lemma: adj_eigenvectors_displacement}, we have $\xi_i = O_{\prec} \left( \frac{1}{\sqrt{\nsize} \nsize \sparsityParam} \right)$ and so
  \begin{align}
  \label{eq: dispacement 2 statistic difference lemma}
    \max_{i, j}
    \Vert
      \xi_i - \xi_j
    \Vert_2
    =
    O_{\prec} \left(
      \frac{1}{ \sparsityParam \sqrt{\nsize^3}}
    \right)
  \end{align}
  holds. From Lemma~\ref{lemma: F rows tensor product} there is the constant $C$ such that
  \begin{align}
  \label{eq: displacement 3 statistic difference lemma}
    \left \Vert
      \probEigenvectors_i
      -
      \probEigenvectors_j
    \right \Vert_2
    \leqslant
    \frac{C_1 \Vert \nodeCommunityMatrix_i - \nodeCommunityMatrix_j \Vert_2}{\sqrt{\nsize}}.
  \end{align}
  %
  In addition, from Lemma~\ref{lemma: bounds of statistic center} we get $\Vert \asymptoticVariance_\penalizer(i, j)^{-1} \Vert_2 \leqslant C_2 \nsize^2 \sparsityParam$ for some constant $C_2$. Define $\Delta_{ij} = \Vert \nodeCommunityMatrix_i - \nodeCommunityMatrix_j\Vert_2$. Using bounds~\eqref{eq: displacement 1 statistic differennce lemma}-\eqref{eq: displacement 3 statistic difference lemma}, we may bound all terms of~\eqref{eq: statistic-center difference} uniformly over $i$ and $j$ as follows:
  \begin{align*}
    (i) \quad &
      \bigl\Vert 
        2 (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1}
          \asymptoticVariance_\penalizer^{-1}(i, j)
          (\probEigenvectors_i - \probEigenvectors_j)^{\T}
      \bigr\Vert_2
        \le 2  \Vert (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1} \Vert_2 \cdot \Vert \asymptoticVariance_\penalizer^{-1}(i, j) \Vert_2 \cdot \Vert\probEigenvectors_i - \probEigenvectors_j \Vert_2 \\
      = ~ & O_\ell \left( \frac{\sqrt{\log \nsize}}{\nsize \sqrt{\sparsityParam}} \right) \cdot O(\nsize^2 \sparsityParam) \cdot O \left(\nsize^{-1/2}\right) \Delta_{ij}
      = O_\ell \left(\sqrt{\nsize \sparsityParam \log \nsize}\right) \Delta_{i j}, \\
    (ii) \quad &
      \bigl\Vert
        (\displaceMatrix_i - \displaceMatrix_j) 
        \probEigenvectors \probEigenvalues^{-1}
        \asymptoticVariance_\penalizer(i, j)^{-1}
        \probEigenvalues^{-1}
        \probEigenvectors^{\T}
        (\displaceMatrix_i - \displaceMatrix_j)^{\T}
      \bigr\Vert_2
      \le \Vert (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1} \Vert_2^2 \cdot \Vert \asymptoticVariance_\penalizer(i, j)^{-1} \Vert_2 \\
      = ~ & O_\ell \left(\frac{\log \nsize}{\nsize^2 \sparsityParam} \right) O(\nsize^2 \sparsityParam)
      = O_\ell \left(\log \nsize \right), \\
    (iii) \quad &
        \bigl\Vert
          2 (\xi_i - \xi_j)
          \asymptoticVariance_\penalizer(i, j)^{-1}
          \left(
            \probEigenvectors_i - \probEigenvectors_j +
            (\displaceMatrix_i - \displaceMatrix_j)
            \probEigenvectors 
            \probEigenvalues^{-1}
          \right)^{\T}
        \bigr\Vert_2 \\
        \le ~ & 2 \Vert \xi_i - \xi_j \Vert_2 \cdot \Vert \asymptoticVariance_\penalizer(i, j)^{-1} \Vert_2 \left(\Vert \probEigenvectors_i - \probEigenvectors_j \Vert_2 + \Vert (\displaceMatrix_i - \displaceMatrix_j) \probEigenvectors \probEigenvalues^{-1} \Vert_2 \right) \\
        = ~ & O_\prec \left( \sparsityParam^{-1} \nsize^{-3/2} \right) O(\nsize^2 \sparsityParam) \left(O(\nsize^{-1/2}) \cdot \Delta_{ij}+ O_\ell \left(\frac{\sqrt{\log \nsize}}{\nsize \sqrt{\sparsityParam}} \right)\right)
        = O_{\prec}(1) \cdot \Delta_{ij} + O_\prec \left( \sqrt{\frac{\log \nsize}{\nsize \sparsityParam}} \right), \\
        %= O_{\ell} \left(\sqrt{\nsize \sparsityParam \log \nsize} \right) \cdot \Delta_{ij} + O_\ell \left( \log \nsize \right), \\
    (iv) \quad &
      \bigl\Vert
        (\xi_i - \xi_j) 
        \asymptoticVariance_\penalizer(i, j)^{-1}
        (\xi_i - \xi_j)^{\T}
      \bigr\Vert_2
      =
      O_{\prec} \left( \nsize^{-3} \sparsityParam^{-2} \right) O(\nsize^2 \sparsityParam)
      = O_\prec \left(\frac{1}{\nsize \sparsityParam}\right).
      %= O_\ell \left(\log \nsize\right).
  \end{align*}
  %
  Thus, we obtain
  \begin{align}
      \left|\equalityStatistic^\penalizer_{i j} - \equalityStatisticCenter^\penalizer_{ij} \right| = O_\ell \left(\Delta_{i j} \sqrt{\nsize \sparsityParam \log \nsize} \right) + O_\ell \left(\log \nsize \right).
  \label{eq:bound_test_1}
  \end{align}
  %
  Next, we get
  \begin{align}
    |\estimator[\equalityStatistic]^\penalizer_{ij} - \equalityStatistic^\penalizer_{ij}| \le \Vert \adjacencyEigenvectors_i - \adjacencyEigenvectors_j \Vert^2_2 \cdot
    \Vert \asymptoticVariance_\penalizer^{-1}(i, j) - \estimator[\asymptoticVariance]^{-1}_\penalizer(i, j)\Vert.
  \label{eq:bound_test_2}
  \end{align}
  %
  Define $\Delta_{\asymptoticVariance}$ and $\Delta_{\asymptoticVariance}'$ as follows:
  \begin{equation*}
      \Delta_{\asymptoticVariance} = \estimator[\asymptoticVariance](i, j) - \asymptoticVariance(i, j), \quad \quad
      \Delta_{\asymptoticVariance}' = \estimator[\asymptoticVariance]_\penalizer^{-1}(i, j) - \asymptoticVariance_\penalizer^{-1}(i, j).
  \end{equation*}
  %
  Since
  \begin{align*}
      0 & = \estimator[\asymptoticVariance]^{-1}_\penalizer(i, j) \estimator[\asymptoticVariance]_\penalizer(i, j) - \asymptoticVariance^{-1}_\penalizer(i,j) \asymptoticVariance_\penalizer(i, j) = \estimator[\asymptoticVariance]^{-1}_\penalizer(i, j) \Delta_{\asymptoticVariance} + \Delta_{\asymptoticVariance}' \asymptoticVariance_\penalizer(i,j),
  \end{align*}
  we get
  \begin{equation*}
      \Delta_{\asymptoticVariance}' = -\estimator[\asymptoticVariance]^{-1}_\penalizer(i, j) \Delta_{\asymptoticVariance} \asymptoticVariance^{-1}_\penalizer(i, j)
      =  -\bigl(\Delta_{\asymptoticVariance}' + \asymptoticVariance_\penalizer^{-1}(i, j)\bigr) \Delta_{\asymptoticVariance} \asymptoticVariance^{-1}_\penalizer(i, j).
  \end{equation*}
  %
  Rearranging terms, we obtain
  \begin{align*}
      \Delta_{\asymptoticVariance}' & = -\bigl(\identity + \Delta_{\asymptoticVariance} \asymptoticVariance_\penalizer(i, j) \bigr)^{-1} \asymptoticVariance_\penalizer^{-1}(i, j) \Delta_{\asymptoticVariance} \asymptoticVariance_\penalizer^{-1}(i, j).
  \end{align*}
  Due to Lemma~\ref{lemma: uniformly covariance estimation}, we have $\Vert \Delta_{\asymptoticVariance} \Vert = O_{\prec}(\nsize^{-5/2} \sparsityParam^{-3/2})$. Applying Lemma~\ref{lemma: bounds of statistic center}, we obtain
  \begin{align*}
      \Vert \Delta_{\asymptoticVariance}' \Vert & \le \left(1 - \Vert \Delta_{\asymptoticVariance} \Vert \cdot \Vert \asymptoticVariance_\penalizer^{-1}(i, j) \Vert \right)^{-1} \Vert \asymptoticVariance^{-1}_\penalizer(i, j)\Vert^2 \Vert \Delta_{\asymptoticVariance} \Vert \\
      & = O(1) \cdot O(\nsize^4 \sparsityParam^2) \cdot O_{\prec}(\nsize^{-5/2} \sparsityParam^{-3/2}) 
      = O_{\prec}(\nsize^{3/2} \sparsityParam^{1/2}).
  \end{align*}
  %
  Substituting the above into~\eqref{eq:bound_test_2} and applying~\eqref{eq: displacement 3 statistic difference lemma}, we get
  \begin{align*}
      |\estimator[\equalityStatistic]^\penalizer_{ij} - \equalityStatistic^\penalizer_{ij}| = O_{\prec}(\sqrt{\nsize \sparsityParam}) \cdot \Delta_{ij}^2.
  \end{align*}
  %
  With probability $1 - \nsize^{-\varepsilon}$ this term is less than $\nsize^{1- 1/12} \sparsityParam \Delta^2_{ij}$ for any $\varepsilon$, provided $\sparsityParam > \nsize^{-1/3}$ and $\nsize$ is large enough. Thus, the lemma follows.
\end{proof}

The result of next lemma ensures that the proposed method allows to select the set of vertices that contains all the pure nodes and does not contain many non-pure ones.
\begin{lemma}
\label{lemma: pure set approximation}
  Assume that Conditions~\ref{cond: nonzero B elements}-\ref{cond: theta distribution-b} hold and SPA chooses an index $i_k$, then for each $\varepsilon$ there is $n_0$ such that for all $\nsize > n_0$ the following holds with probability at least $1 - \nsize^{-\varepsilon}$: $t_{\nsize} = C(\varepsilon) \log \nsize$ ensures that the set $\pureNodesSet_k$ is a subset of $\estimator[\pureNodesSet]_k = \{j \mid \estimator[\equalityStatistic]^\penalizer_{i_k j} \le t_{\nsize} \}$, and $\estimator[\pureNodesSet]_k \setminus \pureNodesSet_k$  has cardinality at most $C'(\varepsilon) \log^\eta \nsize$.
\end{lemma}

\begin{proof}
  According to Lemma~\ref{lemma: statistic difference}, a set $\{j \mid \estimator[\equalityStatistic]_{i_k j} \le \threshold_\nsize\}$ contains
  \begin{align*}
    \left\{j \mid \equalityStatisticCenter^\penalizer_{i_k j} \le \threshold_{\nsize} - \delta_1(\varepsilon) \sqrt{\nsize \sparsityParam \log \nsize} \Vert \nodeCommunityMatrix_{i_k} - \nodeCommunityMatrix_j \Vert_2 - \delta_2(\varepsilon) \log \nsize - \nsize^{1 - 1/12} \sparsityParam \Vert \nodeCommunityMatrix_{i_k} - \nodeCommunityMatrix_j \Vert^2 \right\}.
  \end{align*}
  with probability at least $1 - \nsize^{-\varepsilon}$. Due to Lemma~\ref{lemma: bounds of statistic center}, this set contains
  \begin{align}
  \label{eq: lemma pure set est, upper set}
     \left\{j \mid C \Vert \nodeCommunityMatrix_{i_k} - \nodeCommunityMatrix_j \Vert_2^2 \nsize \sparsityParam \le \threshold_\nsize - \delta_1(\varepsilon) \sqrt{\nsize \sparsityParam \log \nsize} \Vert \nodeCommunityMatrix_{i_k} - \nodeCommunityMatrix_j \Vert_2 - \delta_2(\varepsilon) \log \nsize \right\},
  \end{align}
  for some constant $C$. Here we use $\nsize^{1 - 1/12} \sparsityParam \le \nsize \sparsityParam$ for large enough $\nsize$. Since $\sigma_{\min}(\basisMatrix) \ge C \sqrt{\nsize}$ due to Lemma~\ref{lemma: F rows tensor product} and $\probEigenvectors = \nodeCommunityMatrix \basisMatrix$, Lemma~\ref{lemma: spa selection} guarantees that there is a constant $\delta_3(\varepsilon)$ such that 
  \begin{align*}
      \Vert \nodeCommunityMatrix_{i_k} - \ev_k \Vert_2 \le \frac{1}{\sigma_{\min}(\basisMatrix)} \Vert \probEigenvectors_{i_k} - \ev_k^\T \basisMatrix \Vert_2 \le \delta_3(\varepsilon) \sqrt{\log \nsize / (\nsize \sparsityParam)}
  \end{align*} 
  with probability $\nsize^{-\varepsilon}$. Thus, set~\eqref{eq: lemma pure set est, upper set} contains $\pureNodesSet_k$ if
  \begin{align*}
      C \delta_3(\varepsilon) \log \nsize \le \threshold_\nsize - \delta_1(\varepsilon) \cdot \delta_3(\varepsilon) \log \nsize - \delta_2(\varepsilon) \log \nsize.
  \end{align*}
  Choose $\threshold_\nsize = \left\{\bigl(C + \delta_1(\varepsilon)\bigr) \delta_3(\varepsilon) + \delta_2(\varepsilon) \right\} \log \nsize$, then the pure node set $\pureNodesSet_k$ is contained in set~\eqref{eq: lemma pure set est, upper set} with probability $1 - 2 \nsize^{-\varepsilon}$. Similarly, we have
  \begin{align}
  \label{eq: lemma pure set est, remainder}
    \{j \mid \estimator[\equalityStatistic]_{i_k j} \le \threshold_\nsize \} \subset \left\{j \mid C' \Vert \nodeCommunityMatrix_{i_k} - \nodeCommunityMatrix_j \Vert_2^2 \nsize \sparsityParam \le \threshold_\nsize + \delta_1(\varepsilon) \sqrt{\nsize \sparsityParam \log \nsize} \Vert \nodeCommunityMatrix_{i_k} - \nodeCommunityMatrix_j \Vert_2 + \delta_2(\varepsilon) \log \nsize \right\} 
  \end{align}
  for some other constant $C'$. Since
  \begin{align*}
      \Vert \nodeCommunityMatrix_{j} - \ev_k \Vert_2 - \Vert \nodeCommunityMatrix_{i_k} - \ev_k \Vert_2 \le \Vert \nodeCommunityMatrix_j - \nodeCommunityMatrix_{i_k} \Vert_2 & \le \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2 + \Vert \nodeCommunityMatrix_{i_k} - \ev_k \Vert,  \\
      \Vert \nodeCommunityMatrix_{j} - \ev_k \Vert_2 - \delta_3(\varepsilon) \sqrt{\frac{\log \nsize}{\nsize \sparsityParam}} \le \Vert \nodeCommunityMatrix_j - \nodeCommunityMatrix_{i_k} \Vert_2 & \le \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2 + \delta_3(\varepsilon) \sqrt{\frac{\log \nsize}{\nsize \sparsityParam}},
  \end{align*}
  set~\eqref{eq: lemma pure set est, remainder} belongs to a larger set
  \begin{align*}
    S 
    % & = \left\{
    %   j 
    %   \mid
    %   C' \Vert \nodeCommunityMatrix_{j} - \ev_k \Vert_2^2 \nsize \sparsityParam
    %   \le 
    %   \threshold_\nsize + \delta_1(\varepsilon) \sqrt{\nsize \sparsityParam \log \nsize} \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2 + \bigl(C' + \delta_1 (\varepsilon)\bigr) \delta_3(\varepsilon) \log \nsize + \delta_2(\varepsilon) \log \nsize
    % \right\} \\
    & = \left\{
      j 
      \mid
       C' \Vert \nodeCommunityMatrix_{j} - \ev_k \Vert_2^2 \nsize \sparsityParam \le \delta_4(\varepsilon) \log \nsize + \delta_5(\varepsilon) \sqrt{\nsize \sparsityParam \log \nsize} \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2
    \right\}
  \end{align*}
  with probability at least $1 - 2 \nsize^{-\varepsilon}$. Hence, if $j \in S$, then
  \begin{equation*}
        \Vert \nodeCommunityMatrix_j - \ev_k \Vert_2 \le  \frac{\sqrt{\delta_5^2(\varepsilon) \nsize \sparsityParam \log \nsize + 4 C' \delta_4(\varepsilon) \nsize \sparsityParam \log \nsize} - \delta_5(\varepsilon) \sqrt{\nsize \sparsityParam \log \nsize}}{2 C' \nsize \sparsityParam}
        \le \delta_6(\varepsilon) \sqrt{\frac{\log \nsize}{\nsize \sparsityParam}}.
  \end{equation*}
  Condition~\ref{cond: theta distribution-b} ensures that $|S \setminus \pureNodesSet_k| \le C_{\delta_6} \log^\eta \nsize$, and that concludes the proof.
\end{proof}
