%-------------------------------------------------------------------------------
\section{Evaluation}
\label{sec:evaluation}
%-------------------------------------------------------------------------------

The logical next step after designing the cybersecurity cards was to conduct a user
evaluation to see if the participants could use the cards, and whether the cards
provided a clear communication of these topics and an interface for discussing them.

\subsection{Methodology}

% The focus of the evaluation of the cybersecurity cards was to determine the extent to
% which the knowledge and understanding the cards provided the participant with key
% information and learning regarding cybersecurity topics.
Participants were recruited to take part in a workshop, advertised to university students, where they interacted with
the cybersecurity cards in order to analyse and devise a cybersecurity scenario. Ethical approval was granted by the
University ethics committee for the workshop recruitment and procedure before the study took place. A participant
information sheet was provided via a link in the online registration form, as well as being used to obtain written
consent from participants. Furthermore, the workshop was held in Spring 2022 and we had a special clause added to state
we would follow local government guidelines on the Covid-19 protocol~\cite{covidspring22} which provided guidance on
reducing risks from transmitting Covid-19, such as staying home if participants had symptoms or tested positive for
Covid-19. In total, we managed to recruit 13 participants but only 11 had usable data. Upon registration for the
workshop, we found that the 11 participants were masters-level students (8M, 3F) who were between 22 and 35 years old
(mean 26.3 years). The participants were enrolled in a conversion masters program in computer science and came from
either an engineering (6), mathematics (3), computing (1) or biology (1) background. When asked if the participants had
any prior experience or skills with cybersecurity, only one participant mentioned they had experienced anomaly detection
but others stated they had no prior experience or skills in cybersecurity. Further, when asked if they had any
experience or skills with secure coding, 8 of the participants said they had no experience or skills with secure coding,
2 responded neutral and 1 said they had some experience. We hypothesise that the low number of participants to be
attributed to the Covid-19 global pandemic leaving individuals with more dynamic priorities and commitments.

Before the workshop took place, the participants were split into three groups (consisting of three or four participants).
During the workshop, participants were first given a presentation to introduce the cybersecurity deck of cards. Each
group was then given a deck of cards each where they could browse through and create links between cards themselves,
devising various cybersecurity scenarios. Cybersecurity experts were also present in the room, being available whenever
participants may have had questions or wanted to hold a discussion, and only periodically went round the groups to
check on them if nothing was asked. After the workshop, each participant was individually asked to fill in a
self-assessment questionnaire online (Appendix~\ref{app:questionnaire}) on how using the cards contributed to the
following themes: {\em providing a knowledge base, including: cybersecurity concepts, scope and relationships
(vulnerability-attack-defence)}; {\em independent learning and self-efficacy}; and {\em providing an interface for
discussion on key cybersecurity topics}. The rationale behind this questionnaire was to collect both quantitative and
qualitative data regarding a participant's experience of interacting with the cybersecurity cards. The quantitative
aspect of the questionnaire (Q1) makes use of a 7-point Likert scale, ranging from strongly disagree to strongly agree,
to assess the performance and effort from participants while using the cards. The qualitative aspect (Q2--5) aims to
understand motivations and thoughts behind using the cards. Question 2 and 4 gave participants a series of options
(checkboxes), which were then coupled with a follow-up question (Q3 and Q5 respectively) to further elaborate on their
choices. Two experienced postdoctoral researchers independently analysed and coded the qualitative responses, grouping
them into themes, which were then discussed systematically and final agreements on codes were made in consensus.
This analytical technique has been applied successfully in various bodies of
work~\cite{zade2018conceptualizing,chinh2019ways}. The data from questionnaires is presented as italics in quotation
marks, alongside a participant ID (e.g. PF6 or PM11 for a female or male participant respectively) where relevant. As
well as this, mappings to the aforementioned themes were also decided in consensus among the coders.

\subsection{Results}
\label{sec:results1}

For subsequent discussion, the results from the questionnaires will initially follow
three themes: (1) {\em providing a knowledge base, including: cybersecurity concepts,
scope and relationships (vulnerability-attack-defence)}; (2) {\em independent learning and
self-efficacy}; (3) {\em providing an interface for discussion on key cybersecurity topics};
and (4) any {\em drawbacks of using the cards}.

\subsubsection{Providing a Knowledge Base}
\label{sec:provideknowledge1}

For the first theme the aim was to determine whether the cards provided participants
with introductory cybersecurity knowledge that supports learning in a well-documented
manner, we look at the first four items (\textbf{a--d}) in Question 1 of the
questionnaire (Appendix~\ref{app:questionnaire}). On average, 80\% of the participants agreed
that the cybersecurity cards provided them with knowledge of cybersecurity concepts,
terminology and topics. For individual concepts \textbf{(a)}, 10 participants
{\em agreed} that the cards provided them with knowledge of individual
cybersecurity concepts, with three of them strongly agreeing. One participant had a
neutral response. For knowledge about wider scope \textbf{(b)}, 9 participants
{\em agreed} that the cards provided them with this knowledge, with four of
them strongly agreeing. For the third item regarding knowledge about the relationships
between attacks, defences and vulnerabilities \textbf{(c)}, 10 participants
agreed the cards provided this knowledge with three strongly agreeing. Finally, for the
fourth item regarding cybersecurity terminology \textbf{(d)}, only 7 agreed
the cards provided the participants with knowledge on terminology. Two of
the participants scored neutral and two responded with somewhat disagree.
% As a whole, it is
% clear that
% the cards have indeed achieved a positive result with regard to %with 9 of the participants declaring that
% % the cards have provided
% providing participants with knowledge of fundamental cybersecurity concepts.
% Interestingly, the lower scoring for cards providing participants with cybersecurity terminology
% may be due to the text on the cards using too much technical terminology.


\subsubsection{Independent Learning and Self-Efficacy}
\label{sec:independentlearning1}

The theme of independent learning and self-efficacy relates to items \textbf{(e,f)}
(see Appendix~\ref{app:questionnaire}). The first question \textbf{(e)} asked participants if
the cards enabled them to undertake independent learning about cybersecurity. We found that 8 of
the participants had
agreed with this statement with three strongly agreeing. One participant disagreed
with this, while the other two were neutral. The second question \textbf{(f)} asked
participants whether the cards provided access to cybersecurity knowledge when one of the
rotating cybersecurity experts was not present in the group. We found that 9 out of the 11 participants agreed with this, with 1 participant stating they were neutral and another
disagreeing with this statement.


\subsubsection{Providing an Interface for Discussion}
\label{sec:provideinterface1}

The theme of providing an interface for discussion relates to items \textbf{(g,h)} in
Question 1 of the questionnaire (see Appendix~\ref{app:questionnaire}). 
The first question relating to this theme, \textbf{(g)}, asked participants if the cards
enabled them to discuss cybersecurity topics with the expert. We found that 9 participants
said they agree, with 3 participants strongly agreeing that the cards enabled them to discuss
topics with the expert in their group. One participant strongly disagreed with this.
% This could
% be due to reasons such as already having knowledge of cybersecurity topics being discussed
% at the time or simply not wanting to discuss topics with the experts.
The second question in
this theme, \textbf{(h)}, was used to determine whether the cards allowed participants to hold
discussions on key cybersecurity topics with others in their group, without the cybersecurity
expert. Interestingly, 7 of the participants agreed that the cards enabled a discussion with
others. Three participants were neutral to this and only one participant said they disagreed
with this statement.
% While overall we see a positive outcome regarding the cards providing an
% interface for the discussion of key cybersecurity topics, it is important to determine what was not
% clear and why this was the case, as well as understanding how the cards could be improved.


% \subsubsection{Understanding Limitations}
\subsubsection{Understanding Drawbacks Of Using The Cards}
\label{sec:limitations1}

The next three questions in the evaluation questionnaire were designed to help
uncover and understand any limitations of the first deck of cards, such that
improvements can be made to better fulfill the goals of the research questions described in
Section~\ref{sec:cards}. The first step was to determine which category or subset of
the cybersecurity cards the participants did not use and why this was the case.
Overall, the category or subset of the cards that were used the least were the general
and detailed defence and vulnerability cards. Looking at their responses in Question 3
regarding why these were the choices, one participant stated that {\em "general cards
gave some idea about the content" (PF6)} with another mentioning that they were only
{\em "engaged in attack and defence"} (PM11). With this said, however, most participants
described that they had used all of the categories, for example stating they had used
{\em "at least one card [from] each"} (PM1) or only to {\em "concentrate on a few cards"} (PF9).
This provokes the notion that some participants made some selections in Question 2 under the
presumption that these cards were not used for primary interactions but not necessarily that
they were not looked at or thought about.

For Question 4, we asked participants about how they may think the cybersecurity cards
could be improved based on a few checkboxes that they felt applied to their
experience. 5 of participants recorded that the number of cards was too high. This
may be due to the use of a physical medium and the number of categories and types of
each General card. This corresponds with 3 of the participants recording that there
were too many types or categories. As well as this, 5 of the participants also
recorded that both the color coding of types/categories were not clear, as well as the
relationship between the cards. In Question 5, one participant stated that {\em "Threat cards
[were] difficult to handle/understand"} (PM7). This may also correspond with
the feeling that the number of cards was too high, but also that perhaps the difficulty
of understanding the cards may be linked with the relationships between them, as well as
the terminology used. In contrast to this, however, one participant mentioned that the {\em "Cards
provides an entry point for more detailed scenarios of cyber security and helps to create
relationship between attack and defense situations more clear"} (PM4). Interestingly, 3 participants
recorded that the information on the cards were too abstract, with no participants stating that
they were too detailed. Thus, this helps reinforce the understanding that the difficulty relating
to understanding the cards may be linked to a lack of clarity on the relationships between them.
Further, one participant suggested an {\em "update [to] the colour-coding of the cards"} (PM4),
as the red color with minor style changes leaves them {\em "difficult to go through"}.
% TODO: Are we going to talk about the digital medium stuff?
% Finally, 4 participants stated that they would prefer to have the combination of both
% physical and digital cards - 1 person said just prefer to have digital cards..

% \MMcomment{Add a small paragraph here explicitly stating the key takeaways from limitations of v1.0 (i.e. what is needing to be changed in v1.1)}

% 5 - \#cards too high
% 0 - \#cards too low
% 3 - too many types/categories
% 2 - logos and icons not clear
% 5 - color coding of types/categories not clear
% 5 - relationships between cards not clear
% 3 - information on cards too abstract
% 0 - information on cards too detailed
% 2 - information on cards too technical or difficult to understand
% 0 - prefer to have physical cards
% 1 - prefer to have digital cards
% 4 - prefer to have the combination of physical and digital cards