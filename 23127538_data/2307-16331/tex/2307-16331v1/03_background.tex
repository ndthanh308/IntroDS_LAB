\section{Background}

\subsection{Black-box Attacks}
Adversarial Examples are perturbed inputs that intentionally mislead or deceive machine learning models. Specifically, given an image $\mathbf{x}$ with label $y$ and a classifier $f$, such attacks aim to construct an adversarial example $\textbf{x}_{adv}$ such that:
\begin{equation}
    f(\mathbf{x}_{adv}) \neq y ~~\text{and}~~ ||\mathbf{x}_{adv} - \mathbf{x}||_p \leq \epsilon 
\end{equation}
where $\epsilon$ is the perturbation budget per some $\ell_p$ norm. In the black-box setting, these attacks only have on query access to the model. One common characteristic of black-box attacks is the use of similar queries to gather information about the model's behavior. Specifically, by making queries with slight perturbations to the input and observing the corresponding model outputs, attackers can gain insights into the model's decision-making process. 

Consider the initial stage of many black-box adversarial attacks, which involves estimating the direction to move the input to achieve the desired adversarial effect. For example, the NES~\cite{ilyas2018black}, HSJA~\cite{chen2020hopskipjumpattack}, and QEBA~\cite{li2020qeba} attacks estimate the gradient by sampling nearby points from a Gaussian (or similar) probability distributions, and computing finite differences over these points. Other attacks such as SurFree~\cite{maho2021surfree} and Square~\cite{andriushchenko2020square} also sample nearby points to estimate a ``random search'' direction (not a gradient) in which to move the input. We will often refer to the interplay between such queries made during the direction estimation stage and a stateful defense, particularly because the attack's overall convergence properties are often directly influenced by choice of direction. 

% - Common property is making similar queries to estimate what direction to move in. Although the attacks involve iterations, in this work we only focus on the direction estimation part - determine convergence and success of attack. We only look at NES gradient estimation : for HSJA, QEBA it is similar but they just sample noise from a different distribution.

\subsection{Stateful Defenses}
The overall intuition behind stateful defenses is that black-box attackers often submit highly similar queries as part of the optimization procedure for their chosen adversarial task. These highly similar queries can then be detected. Defenses such as Blacklight~\cite{li2022blacklight} have reduced attack success rate (ASR) of state-of-the-art black-box attacks to as low as 0\%.

A stateful defense typically comprises a classifier $f$, feature extractor $H$ (with some associated distance metric), query store $q$, and threshold $\tau$. The defense then compares an incoming query against all queries stored in $q$. If similarity with any example in $q$ exceeds $\tau$, the defense deploys preventive measures such as query rejection or account banning.

Different stateful defenses primarily vary in their choices of $H$. Specifically, some defenses such as Blacklight and PIHA~\cite{li2022blacklight, choi2023piha} leverage discrete-valued metrics such as hamming distance over hashes, e.g., SHA-256 hashes of quantized pixels. Others, such as Stateful Defense (SD)~\cite{chen2020stateful}, employ real-valued metrics, e.g., $\ell_2$ distance between embeddings from neural similarity encoders. In this work, we evaluate  Blacklight and PIHA since they are available for both the CIFAR-10 and ImageNet datasets.

\textbf{Model Stealing}
Recent work has also proposed stateful defenses against model-stealing attacks. Such attacks aim to steal a local ``clone'' model $f^{c}$ such that the behavior of $f^{c}$ is similar to that of $f$. Defenses such as SEAT~\cite{zhang2021seat} have also been successful here and can force the attacker to create as many as 65 accounts to steal a single model. This success can be similarly explained by the submission of highly similar queries. For example, at iteration $t$ of a Jacobian-based Augmentation (JBA) attack~\cite{papernot2017practical}, the adversary constructs a ``useful'' but highly similar query $\mathbf{x}_{t+1}$ by perturbing previous query $\mathbf{x}_{t}$ so that it maximizes the loss $\mathcal{L}$ of $f^{c}_t$:
\begin{equation}
    \mathbf{x}_{t+1} = \mathbf{x}_t + \eta * sgn(\nabla_{\textbf{x}_t} \mathcal{L}(\mathbf{x}_t, f(\mathbf{x}_t)))
\end{equation}
where $\eta$ is some step size.

% \subsubsection{Adversarial Examples}
% - how the 4 defenses fit here. We only focus on blacklight and piha since they are available for both cifar and imagenet. 

% \subsection{Model Stealing}
% Similarly how this fits in the general terminology.

% \subsection{Overview}


%\subsection{Defending Against Black-Box Adversaries}
%\subsubsection{Adversarial Examples}
%Stateful defenses have primarily been designed to defend against black-box adversarial examples. Given an image $\mathbf{x}$ with label $y$ and a classifier $f$, such attacks aim to construct an adversarial example $\textbf{x}_{adv}$ such that:
% \begin{equation}
%     f(\mathbf{x}_{adv}) \neq y ~~\text{and}~~ ||\mathbf{x}_{adv} - \mathbf{x}||_p \leq \epsilon 
% \end{equation}
% where $\epsilon$ is the perturbation budget per some $\ell_p$ norm. Defenses such as Blacklight~\cite{li2022blacklight} have reduced the attack success rate (ASR) of state-of-the-art black-box attacks to as low as 0\% .

%To understand this efficacy, we again emphasize that black-box attacks often solve this problem using a variety of direction estimation and boundary walking heuristics. These heuristics often require hundreds and thousands of similar queries. For instance, at iteration $t$ of the popular HopSkipJumpAttack (HSJA) attack~\cite{chen2020hopskipjumpattack}, the adversary estimates the gradient at $\mathbf{x}_t$ by submitting $100 * \sqrt{t}$ highly similar queries sampled from a uniform hyper-sphere around $\mathbf{x}_t$. 



