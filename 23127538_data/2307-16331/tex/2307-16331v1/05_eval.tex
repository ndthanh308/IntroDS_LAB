

\section{Experiments}
Motivated by our analysis in Section~\ref{sec:tradeoff}, we conduct experiments to validate our findings empirically, and thus answer the following questions:

\noindent\textbf{Q1. How does the trade-off empirically depend upon the spread, i.e., variance $\beta$ of the attack queries?}

\noindent\textbf{Q2. How does the trade-off empirically depend upon the Lipschitz constant ratio $K_U / K_L$ of the feature extractor?}

\noindent\textbf{Q3. What are the implications of the trade-off for the convergence of black-box attacks?}


\subsection{Experimental Setup}
\noindent\textbf{Feature extractors.} We focus our evaluation on feature extractors from two state-of-the-art stateful defenses: Blacklight~\cite{li2022blacklight} and PIHA~\cite{choi2023piha}. Below we provide detailed descriptions and hyper-parameters for both.

Blacklight operates on an input image with pixel values in the range of [0, 255]. First, it discretizes the pixels into bins of size 50. Second, a sliding window technique is applied to the discretized image, utilizing a window size of 20 for TinyImages~\cite{torralba200880} and 50 for ImageNet~\cite{russakovsky2015imagenet}. During this process, each window is hashed using the SHA-256 algorithm. Finally, the resulting set of hashes obtained from all the windows is considered as the ``feature'' for the image. For efficiency purposes, Blacklight utilizes only the top 50 hashes. To quantify the distance between two hash sets, Blacklight computes the number of non-common hashes, which can be interpreted as an $\ell_1$ distance. 

PIHA also operates on input images with the same pixel range. First, it runs a 3x3 low-pass Gaussian filter with standard deviation 1 over the image. Second, the image is converted to the HSV color space with the S and V components discarded. Finally, PIHA runs a sum-pooling operation over 7x7 image blocks, and the ``feature'' is computed as the output of the local binary pattern algorithm~\cite{ojala1994performance} on the sum-pooled image.

\noindent\textbf{Datasets}. We evaluate Blacklight and PIHA using two datasets, TinyImages and ImageNet. The TinyImages dataset is a collection of 32x32 images and is the superset collection from which the popular CIFAR-10 dataset is sampled (providing nearly 80 million images as opposed to only 60,000). The ImageNet dataset comprises over 1 million 256x256 images. We sample a random subset of 1 million images from both datasets for our experiments.

% Figure environment removed

\subsection{Q1. Variance of Attack Queries}\label{sec:q1} 
Theorem~\ref{theorem:general} suggests a clear inverse relationship between the ($\alpha^{det}$, $\alpha^{fp}$) trade-off and $\beta$. We now empirically validate this relationship, i.e., for any given feature extractor and dataset, we plot $\alpha^{fp}$ against $\alpha^{det}$ for a variety of thresholds $\tau$. We compute $\alpha^{fp}$ over 1 million images for all settings except PIHA on ImageNet, for which we compute on $100$k and extrapolate due to computational complexity. We compute $\alpha^{det}$ over 100 images by sampling perturbations from Gaussians with different standard deviations $\beta$. 

Results are presented in Figure~\ref{fig:beta}. Notably, we first observe that for any $\beta$, the trade-off between $\alpha^{det}$ and $\alpha^{fp}$ indeed exists across all thresholds.  More specifically, to obtain a larger $\alpha^{det}$ always requires an increase in $\alpha^{fp}$ as well. This validates the takeaways from Theorems~\ref{theorem:toy} and~\ref{theorem:general}. Furthermore, the inverse relationship with $\beta$ also exists, i.e., achieving the same $\alpha^{det}$ requires a larger $\alpha^{fp}$ when $\beta$ is increased. Interestingly, PIHA can achieve higher $\alpha^{det}$ on the low-dimensional TinyImages compared to Blacklight, but both suffer on ImageNet when $\beta$ increases beyond $\beta=0.01$. 

\subsection{Q2. Lipschitz Constants of the Feature Extractor}
Theorem \ref{theorem:general} also suggests that the ($\alpha^{det}$, $\alpha^{fp}$) trade-off is influenced by Lipschitz constants $K_U$ and $K_L$ of the feature extractor. However, this assumes a continuous feature extractor --- although the feature extractors from Blacklight and PIHA are not continuous, they are still designed to approximate the perceptual likeness of images (yielding closer features for similar queries and further features for dissimilar ones). Given the lack of closed-form expressions, we resort to an empirical estimation of $K_U$ and $K_L$.

We create image pairs $\mathbf{x}$ and $\mathbf{x}+\boldsymbol{\delta}$ where $\mathbf{x}$ is sampled from the dataset (TinyImages/ImageNet), and $\boldsymbol{\delta} \sim \mathcal{N}(0,\mathbf{I_d}\beta^2); \beta = 0.01$. For each pair, we then calculate the ratio between the $\ell_2$ distance in the feature space and the input space, i.e., $\frac{||H(\mathbf{x}) - H(\mathbf{x}+\boldsymbol{\delta})||}{||\boldsymbol{\delta}||}$. We construct $10000$ such pairs and plot the distribution of these distance ratios. 

Figure~\ref{fig:lipschitz} plots these distributions for ImageNet images processed by both Blacklight and PIHA feature extractors. We note a larger distribution spread in the histogram for PIHA compared to Blacklight, hinting at a greater value for $\frac{K_U}{K_L}$ for PIHA. As per Theorem~\ref{theorem:general}, this suggests that PIHA possesses the potential for superior detection rates compared to Blacklight. We corroborate this empirically by plotting $\alpha^{fp}$ against $\alpha^{det}$ in a manner akin to that in Section \ref{sec:q1}. As presented in Figure \ref{fig:lip_tradeoff}, PIHA indeed manifests higher detection rates when compared with Blacklight.

% Figure environment removed

% Figure environment removed

\subsection{Q2. The Trade-off and Attack Convergence}
Given that increasing $\beta$ worsened the trade-off of the defense (Q1 in Section~\ref{sec:q1}), we now question the impact of increasing $\beta$ on the attack convergence itself. We specifically consider the adversary goal of gradient estimation via finite differences. Formally, it can be shown through the following result that increasing $\beta$ should worsen the quality of the estimated gradient:

\begin{theorem}\label{lemma:min_var_grad} 
Let $\nabla_x$ be the true gradient of $\mathbf{x}$ for the classifier's loss, and $G$ be a matrix of rows $g_1, \cdots, g_k \sim \mathcal{N}(0, \mathbf{I_d}\beta^2)$. Then, the norm of estimated gradient $G\cdot\nabla_\mathbf{x} $ is bounded in probability by:
\begin{equation*}
\begin{split}
    \mathbb{P}[(1-\epsilon)\|\nabla_\mathbf{x}\| \leq \|G\cdot\nabla_\mathbf{x}\| & \leq (1+\epsilon) \|\nabla_\mathbf{x}\|] \geq  \\
     & 1 - 2\cdot exp{\left(- k - \dfrac{1 + \epsilon}{2\beta^2}\right)}
\end{split} 
\end{equation*}
where $0 \leq \epsilon \leq 1$ is the estimation error.
\end{theorem}
A detailed proof of this result can be found in Appendix~\ref{app:proof_min_grad_var}. The left-hand side represents the probability that our estimated gradient is ``good'', i.e., produces the same increase-in-loss as the true gradient. As $\beta$ increases, the lower bound on this probability decreases (right-hand side), suggesting that the estimate is less likely to produce the same increase-in-loss.

We empirically validate this impact of increasing $\beta$ in Figure~\ref{fig:change_of_loss}, which plots the increase in loss when following gradients estimated with different $\beta$. These figures present a clearer overall picture --- for any given $\alpha^{fp}$, even though larger $\beta$ decreases the detection rate, a gradient estimated with larger $\beta$ is also strictly worse for the adversary, i.e., does not increase the loss as much (see the gradation from red to blue). In other words, these findings suggest that the worsening of the ($\alpha^{det}$, $\alpha^{fp}$) trade-off at larger $\beta$ is not without a negative impact on the adversary.





