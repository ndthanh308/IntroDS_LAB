\section{Introduction}

% stateful defenses
% analyzing trade-off / provide bounds

% A line of research focuses on detecting and rejecting “similar” queries which prevents query based black box attacks from making progress. These defenses, termed Stateful Defenses, compare the similarity of any incoming queries with historical queries. In case the similarity is high, they take defensive actions such as rejecting the query or banning the account to prevent attack progress. These defenses have been empirically shown to be resilient against state of the art black box attacks such as HSJA, Surfree etc. However, these defenses may also reject non-attack queries, which would harm the utility of the model by increasing the false positive rate. This points to a trade-off between a Stateful defense’s ability to detect attack queries and avoiding false positives. 

% In this work, we theoretically analyze this tradeoff.
Adversarial examples pose a significant threat to the security and integrity of machine learning systems~\cite{8578273,Sayles_2021_CVPR}. These examples are subtly manipulated inputs that deceive the models and cause misclassifications~\cite{DBLP:journals/corr/SzegedyZSBEGF13,7958570,hooda2022adversarially}. Even in the challenging black-box setting, where the attacker has limited information access, adversarial examples have been remarkably successful~\cite{ilyas2018black,chen2020hopskipjumpattack,feng2022graphite,maho2021surfree,andriushchenko2020square,li2020qeba}.

Recent research has shown that stateful defenses offer a promising approach to mitigate the impact of such attacks~\cite{li2022blacklight, choi2023piha, chen2020stateful}. These defenses leverage the observation that black-box attackers often submit numerous highly similar queries, e.g., querying nearby points for gradient estimation. To counter this, stateful defenses maintain a buffer of recent queries and compare incoming queries in some feature space to identify potential attacks. If the similarity between queries exceeds a predefined threshold, defensive action is taken, e.g., banning the user's account~\cite{chen2020stateful} or rejecting queries~\cite{li2022blacklight}. 

The success of stateful defenses hinges on their ability to detect and flag attack queries without flagging benign ones. 
% Therefore, the level of security provided by stateful defenses is tied to their proficiency in detecting attack queries, while their usability is linked to their ability to prevent erroneous flagging of harmless queries.
This suggests the existence of a trade-off between the detection and false positive rates of a stateful defense (much like the trade-off between robustness and accuracy for existing white-box defenses~\cite{tsipras2018robustness,yang2020closer,raghunathan2020understanding}).
% The trade-off can be balanced by tuning the similarity threshold used to detect attack queries. 
% This similarity is usually computed in the embedding space where the feature extractors are designed to approximate the perceptual similarity of images. For instance, Blacklight~\cite{li2022blacklight} and PIHA~\cite{choi2023piha} use rolling window and quantization based feature extractors. 
In light of this, existing defenses typically tune their similarity threshold to manipulate the trade-off, i.e., such that the defense only permits an empirically computed false positive rate. However, this does not provide any guarantees for the detection rate, and little is currently known about the exact properties of the feature spaces and problem domains that influence this trade-off. This work aims to address this gap by theoretically characterizing the trade-off between the detection rate and the false positive rate of stateful defenses. Specifically, we provide upper bounds for the detection rate for a general class of feature extractors. We then empirically validate that the takeaways from these bounds hold for multiple datasets and defenses and also analyze how this trade-off affects the convergence of black-box attacks.