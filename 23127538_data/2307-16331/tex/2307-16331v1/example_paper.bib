
@inproceedings{ilyas2018black,
  title={Black-box adversarial attacks with limited queries and information},
  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  booktitle={International conference on machine learning},
  pages={2137--2146},
  year={2018},
  organization={PMLR}
}

 @inproceedings{chen2020hopskipjumpattack,
  title={Hopskipjumpattack: A query-efficient decision-based attack},
  author={Chen, Jianbo and Jordan, Michael I and Wainwright, Martin J},
  booktitle={2020 ieee symposium on security and privacy (sp)},
  pages={1277--1294},
  year={2020},
  organization={IEEE}
}

@inproceedings{li2020qeba,
  title={Qeba: Query-efficient boundary-based blackbox attack},
  author={Li, Huichen and Xu, Xiaojun and Zhang, Xiaolu and Yang, Shuang and Li, Bo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1221--1230},
  year={2020}
}

@inproceedings{maho2021surfree,
  title={Surfree: a fast surrogate-free black-box attack},
  author={Maho, Thibault and Furon, Teddy and Le Merrer, Erwan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10430--10439},
  year={2021}
}

@inproceedings{andriushchenko2020square,
  title={Square attack: a query-efficient black-box adversarial attack via random search},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXIII},
  pages={484--501},
  year={2020},
  organization={Springer}
}

@inproceedings{li2022blacklight,
  title={Blacklight: Scalable Defense for Neural Networks against $\{$Query-Based$\}$$\{$Black-Box$\}$ Attacks},
  author={Li, Huiying and Shan, Shawn and Wenger, Emily and Zhang, Jiayun and Zheng, Haitao and Zhao, Ben Y},
  booktitle={31st USENIX Security Symposium (USENIX Security 22)},
  pages={2117--2134},
  year={2022}
}

@article{choi2023piha,
  title={PIHA: Detection method using perceptual image hashing against query-based adversarial attacks},
  author={Choi, Seok-Hwan and Shin, Jinmyeong and Choi, Yoon-Ho},
  journal={Future Generation Computer Systems},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{chen2020stateful,
  title={Stateful detection of black-box adversarial attacks},
  author={Chen, Steven and Carlini, Nicholas and Wagner, David},
  booktitle={Proceedings of the 1st ACM Workshop on Security and Privacy on Artificial Intelligence},
  pages={30--39},
  year={2020}
}

@inproceedings{zhang2021seat,
  title={Seat: similarity encoder by adversarial training for detecting model extraction attack queries},
  author={Zhang, Zhanyuan and Chen, Yizheng and Wagner, David},
  booktitle={Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security},
  pages={37--48},
  year={2021}
}

@inproceedings{papernot2017practical,
  title={Practical black-box attacks against machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  booktitle={Proceedings of the 2017 ACM on Asia conference on computer and communications security},
  pages={506--519},
  year={2017}
}

@inproceedings{ojala1994performance,
  title={Performance evaluation of texture measures with classification based on Kullback discrimination of distributions},
  author={Ojala, Timo and Pietikainen, Matti and Harwood, David},
  booktitle={Proceedings of 12th international conference on pattern recognition},
  volume={1},
  pages={582--585},
  year={1994},
  organization={IEEE}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{torralba200880,
  title={80 million tiny images: A large data set for nonparametric object and scene recognition},
  author={Torralba, Antonio and Fergus, Rob and Freeman, William T},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={30},
  number={11},
  pages={1958--1970},
  year={2008},
  publisher={IEEE}
}

@inproceedings{feng2022graphite,
  title={GRAPHITE: Generating Automatic Physical Examples for Machine-Learning Attacks on Computer Vision Systems},
  author={Feng, Ryan and Mangaokar, Neal and Chen, Jiefeng and Fernandes, Earlence and Jha, Somesh and Prakash, Atul},
  booktitle={2022 IEEE 7th European Symposium on Security and Privacy (EuroS\&P)},
  pages={664--683},
  year={2022},
  organization={IEEE}
}

@misc{hooda2022adversarially,
      title={Towards Adversarially Robust Deepfake Detection: An Ensemble Approach}, 
      author={Ashish Hooda and Neal Mangaokar and Ryan Feng and Kassem Fawaz and Somesh Jha and Atul Prakash},
      year={2022},
      eprint={2202.05687},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{Sayles_2021_CVPR,
    author    = {Sayles, Athena and Hooda, Ashish and Gupta, Mohit and Chatterjee, Rahul and Fernandes, Earlence},
    title     = {Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {14666-14675}
}

@INPROCEEDINGS {7958570,
author = {N. Carlini and D. Wagner},
booktitle = {2017 IEEE Symposium on Security and Privacy (SP)},
title = {Towards Evaluating the Robustness of Neural Networks},
year = {2017},
volume = {},
issn = {2375-1207},
pages = {39-57},
abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x&#x27; that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks&#x27; ability to find adversarial examples from 95% to 0.5%. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
keywords = {neural networks;robustness;measurement;speech recognition;security;malware;resists},
doi = {10.1109/SP.2017.49},
url = {https://doi.ieeecomputersociety.org/10.1109/SP.2017.49},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@article{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  journal={arXiv preprint arXiv:1805.12152},
  year={2018}
}

@INPROCEEDINGS{8578273,
  author={Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Robust Physical-World Attacks on Deep Learning Visual Classification}, 
  year={2018},
  volume={},
  number={},
  pages={1625-1634},
  doi={10.1109/CVPR.2018.00175}}

@article{yang2020closer,
  title={A closer look at accuracy vs. robustness},
  author={Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={8588--8601},
  year={2020}
}

@inproceedings{DBLP:journals/corr/SzegedyZSBEGF13,
  author       = {Christian Szegedy and
                  Wojciech Zaremba and
                  Ilya Sutskever and
                  Joan Bruna and
                  Dumitru Erhan and
                  Ian J. Goodfellow and
                  Rob Fergus},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Intriguing properties of neural networks},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6199},
  timestamp    = {Thu, 25 Jul 2019 14:35:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{raghunathan2020understanding,
  title={Understanding and mitigating the tradeoff between robustness and accuracy},
  author={Raghunathan, Aditi and Xie, Sang Michael and Yang, Fanny and Duchi, John and Liang, Percy},
  journal={arXiv preprint arXiv:2002.10716},
  year={2020}
}