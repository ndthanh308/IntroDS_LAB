\section{Trade-offs between Detection and False Positives}\label{sec:tradeoff}
%\nm{We can probably take inspiration from layout of Yue's toy model in his randomization paper: https://arxiv.org/pdf/2206.09491.pdf}
% state assumtions here

In this section, we demonstrate that there exists an implicit trade-off between detecting attack queries and avoiding false positives in the context of stateful defenses. We begin with a constructive model through which we provide explicit characterizations of the feature extractor and data distributions. We use this toy model to highlight the trade-off, and then relax the assumptions to provide a more general bound that highlights the direct influence of the feature extractor and the problem domain.

\subsection{Toy Model}

%\noindent\textbf{Data Distribution.} we need to find the base data distribution so that the hash function makes sense.


\noindent\textbf{Feature extractor.} We begin by considering an explicit class of feature extractors based on simple quantization. The feature extractor is given by $H : \mathbb{R}^d \rightarrow \mathbb{Z}^d$ with a discrete output space. Specifically, 
\begin{equation}\label{eqn:hash_fn_toy_model}
    H(\mathbf{x}) = \lfloor \mathbf{x} + \mathbf{0.5}\rfloor
\end{equation}
where the $\lfloor . \rfloor$ operation is element-wise. Many defenses employ quantization to provide perceptual similarity~\cite{li2022blacklight,choi2023piha}. In this model, we consider a query to be an attack query if and only if it produces the exact same features as that of a prior query. Later, in Section~\ref{sec:general_analysis} we expand beyond the toy model to consider the case where $H$ is a generic feature extractor, and queries are considered attack queries when their features are within some distance $\tau$ of a prior query.

% Figure environment removed

\noindent\textbf{Natural Query Distribution.} Stateful defenses assume that natural images are sufficiently ``spread out'', or dissimilar enough such that they can be distinguished. Therefore, for our model we assume that natural images originate from one of several Gaussian distributions, which are uniformly dispersed across input space $\mathbb{R}^d$ \footnote{For the case where the input space is constrained, for instance to [0,255], the natural images can instead be sampled from truncated Gaussian distributions.}. Each natural image is obtained from a distinct Gaussian distribution. This may be viewed as a ``best case'' situation for the defense, where natural images are sufficiently spread out across the input space to avoid false positives. For simplicity, we assume isotropic Gaussian distributions: $\mathcal{N}(\mathbf{p}, \mathbf{I_d} \sigma^2)$ where $\mathbf{p} \in \mathbb{Z}^d$. Intuitively, when applying $H$ to a natural image $\mathbf{x} \sim \mathcal{N}(\mathbf{p}, \mathbf{I_d} \sigma^2)$, it should output the discrete feature vector $\mathbf{p}$ with high probability.

\noindent\textbf{Attack Query Distribution.} To estimate the gradient at input $\mathbf{x}$, a Monte Carlo simulation approach would require sampling a total of $q$ perturbations $\{\mathbf{x},\mathbf{x + \boldsymbol{\delta}_1},...,\mathbf{x + \boldsymbol{\delta}_q}\}$. For our model, we consider the distribution of perturbations for $\mathbf{x}$ to be $\mathcal{N}(0, \mathbf{I_d}\beta^2)$, i.e., the adversary is estimating a gradient using finite differences on a Gaussian basis~\cite{ilyas2018black}.


Given the setting described above (also illustrated in Figure~\ref{fig:toy_model}), we now present the following result, which bounds the detection rate with the false positive rate:

\begin{theorem}\label{theorem:toy}
{Let the adversary sample a natural image $\mathbf{x}$ from one of the above distributions $\mathcal{N}(\mathbf{p},\mathbf{I_d}\sigma^2)$, and perturb it with $\boldsymbol{\delta} \sim \mathcal{N}(0, \mathbf{I_d}\beta^2)$ to estimate a gradient. Given that the stateful defense incurs a false positive rate $\alpha^{fp}$, the detection rate $\alpha^{det}$ for the perturbed query $\mathbf{x} + \boldsymbol{\delta}$ is then bounded as follows:}
\begin{equation}
    \alpha^{det} \leq 1 - \left(2 - 2\Phi\left(0.5\beta^{-1}\right) \right)^d(1-\alpha^{fp})
\end{equation}
\end{theorem}
\begin{proof}[Proof]
$H$ fails to detect the attack query $\mathbf{x + \boldsymbol{\delta}}$ if and only if $H(\mathbf{x} + \boldsymbol{\delta}) \neq H(\mathbf{x})$. Therefore,
\begin{align}
    \alpha^{det} &= 1 - \mathbb{P}[H(\mathbf{x}) \neq H(\mathbf{x+\boldsymbol{\delta}})]\\
    & \leq 1 - \mathbb{P}[H(\mathbf{x+\boldsymbol{\delta}}) \neq \mathbf{p},H(\mathbf{x}) = \mathbf{p}]\\
    &= 1 - \mathbb{P}[H(\mathbf{x+\boldsymbol{\delta}}) \neq \mathbf{p}~|~H(\mathbf{x}) = \mathbf{p}]\mathbb{P}[H(\mathbf{x}) = \mathbf{p}]\\
    &\leq 1 - \mathbb{P}[H\left(\mathbf{p}+\mathbf{\boldsymbol{\delta}}\right) \neq \mathbf{p}]\mathbb{P}[H(\mathbf{x}) = \mathbf{p}]\\
    %&= 1 - \left(\Phi\left(\frac{1}{\beta K}\right) - \frac{1}{2} \right)^d\left(2\Phi\left(\frac{1}{2\sigma K}\right) - 1 \right)^d
    &= 1 - \mathbb{P}[||\mathbf{\boldsymbol{\delta}}||_{\infty} > 0.5]\mathbb{P}[H(\mathbf{x}) = \mathbf{p}]\\
    &= 1 - \left(2 - 2\Phi\left(0.5\beta^{-1}\right) \right)^d(1-\alpha^{fp})
    %&\leq 1 - (2K\sigma)^d\frac{1}{(2\pi)^{\frac{d}{2}}}e^{-\frac{d}{8K^2\sigma^2}}
\end{align}
where $\Phi$ is the cummulative distribution function of $\mathcal{N}(0,1)$. Note that to go from (7) to the inequality in (8), we assign a specific value $\mathbf{x} = \mathbf{p}$, i.e., placing $\mathbf{x}$ at the center of the quantization bin for $H$ (see Equation~\ref{eqn:hash_fn_toy_model}). By placing it at the center, the probability of evasion when adding $\boldsymbol{\delta}$ is minimized, and the resulting event is also independent of event $H(\mathbf{x})=\mathbf{p}$. Finally, going from (9) to (10) uses standard results for the CDF of a multivariate Gaussian.  %A detailed proof can be found in Appendix~\ref{}.
\end{proof}

% \ah{explain better that we start with a simplistic setting and then generalize; also use consistent notation}

%\underline{\textit{Lower Bound}} Similarly,

%\begin{align}
%    \alpha^{det} &= \mathbb{P}[H(\mathbf{x}) = H(\mathbf{x+\delta})]\\
%    & \geq \mathbb{P}[H(\mathbf{x}) = \mathbf{p_x},H(\mathbf{x+\delta}) = \mathbf{p_x}]\\
    %&= \mathbb{P}[H(\mathbf{x+\delta}) = K\mathbf{p_x}|H(\mathbf{x}) = K\mathbf{p_x}]\mathbb{P}[H(\mathbf{x}) = \mathbf{p_x}]\\
%    &\geq \mathbb{P}[H(\mathbf{p_x}+\frac{\mathbf{1}}{2K}+\mathbf{\delta}) = K\mathbf{p_x}]\mathbb{P}[H(\mathbf{x}) = K\mathbf{p_x}]\\
%    &= \left(\Phi\left(\frac{m}{\beta K}\right) - \frac{1}{2} \right)^d (1-\alpha^{fp})
%\end{align}

\pitfallenv{Takeaway}
{\label{takeaway_toy_model} There exists a trade-off between the detection rate $\alpha^{det}$ and the false positive rate $\alpha^{fp}$, i.e., decreasing $\alpha^{fp}$ also decreases the upper bound for $\alpha^{det}$. Furthermore, this trade-off also depends on the standard deviation $\beta$ of the perturbation distribution, i.e., high values of $\beta$ lead to a lower detection rate.}


\subsection{General Analysis}\label{sec:general_analysis}
% \ah{clearly state what assumptions are for making problem simple and what are for making it general}
% \ah{reasons for assumptions - relate lipshitz to need for perceptual hash}
Recall that our toy model assumed a quantization-based feature extractor and a uniform natural image distribution. We now extend our results to a more generic perceptual feature extractor and image distribution. Specifically, consider $H : \mathbb{R}^d \rightarrow \mathbb{R}^y$ where $y$ is the dimensionality of the output feature space. We assume $H$ to be Lipschitz continuous with constants $K_L$ and $K_U$ :
% \ah{where do we need to KL and KU to hold}
\begin{align}\label{eq:lipshitz}
    K_L ||\mathbf{x_1-x_2}|| \leq ||H(\mathbf{x_1})-H(\mathbf{x_2})|| \leq K_U ||\mathbf{x_1-x_2}||,
\end{align}
$\forall~(\mathbf{x_1,x_2}) \in \mathbb{R}^d$. Note that we no longer assume the implementation of $H$ as in the toy model; the continuity assumption here is only needed to ensure that $H$ captures perceptual similarity, i.e., similar images should indeed have similar features. Furthermore, since $H$ is now continuous, we extend to a threshold based detection setting i.e. a query $\mathbf{x}$ is considered an attack query if and only if $||H(\mathbf{x})-H(\mathbf{x_h})|| \leq \tau$ where $\mathbf{x_h}$ is any historical query. Given these changes, we can now re-analyze the detection $\alpha^{det}$ for a perturbed query $\mathbf{x} + \boldsymbol{\delta}$:

\begin{theorem}\label{theorem:general}
{Let the adversary sample natural image $\mathbf{x}$, and perturb it with $\boldsymbol{\delta} \sim \mathcal{N}(0, \mathbf{I_d}\beta^2)$ to estimate a gradient. For a false positive rate $\alpha^{fp}$, the detection rate $\alpha^{det}$ for perturbed query $\mathbf{x} + \boldsymbol{\delta}$ is then bounded as follows:}
\begin{align}
    \alpha^{det} \leq \frac{1}{\Gamma(\frac{d}{2})} \gamma\left(\frac{d}{2},\frac{1}{2}\left(\frac{K_U}{K_L}\frac{M_\mathcal{D}}{\beta}\frac{1}{1-\alpha^{fp}}\right)^2\right)
\end{align}
where $M_{\mathcal{D}} = {\mathbb{E}}[||\mathbf{x_1}-\mathbf{x_2}|| ]$, i.e., the expected spread of natural queries, and $\gamma$ and $\Gamma$ are the monotonic lower incomplete and complete Gamma functions respectively.
\end{theorem}

\begin{proof}[Proof] $H$ fails to detect the attack query $\mathbf{x + \boldsymbol{\delta}}$ if and only if $||H(\mathbf{x}) - H(\mathbf{x+\boldsymbol{\delta}})|| > \tau$. Therefore,
\begin{align}\label{eq:general_alpha_det}
    \alpha^{det} = {\mathbb{P}}\left[ ||H(\mathbf{x}) - H(\mathbf{x+\boldsymbol{\delta}})|| \leq \tau \right]
\end{align}
Similarly, $H$ produces a false positive for two natural images $\mathbf{x_1}$ and $\mathbf{x_2}$ if and only if $||H(\mathbf{x_1}) - H(\mathbf{x_2})|| \leq \tau$. Therefore,
\begin{align}\label{eq:general_alpha_fp}
    \alpha^{fp} = {\mathbb{P}}\left[ ||H(\mathbf{x_1}) - H(\mathbf{x_2})|| \leq \tau \right]
\end{align}
Using Equation~\ref{eq:lipshitz} with~\ref{eq:general_alpha_det} and~\ref{eq:general_alpha_fp}:
\begin{align}\label{eq:det}
    \alpha^{det} \leq {\mathbb{P}}\left[ ||\mathbf{\boldsymbol{\delta}}|| \leq \frac{\tau}{K_L} \right]
\end{align}
\begin{align}\label{eq:fp}
    \alpha^{fp} \geq {\mathbb{P}}\left[ ||\mathbf{x_1} - \mathbf{x_2}|| \leq \frac{\tau}{K_U} \right]
\end{align}

% Before proceeding, we state the following lemma :
% \begin{lemma}\label{eq:cher}
% For a $n$-dimensional gaussian random vector $\textbf{u} \sim \mathcal{N}(0,\sigma^2)$ and $\frac{b}{\sigma} < 1$, 
% \begin{equation}
%     \underset{\textbf{u} \sim \mathcal{N}(0,\sigma^2)}{\mathbb{P}}[||\textbf{u}||_2 \leq  \sqrt{n} b] \leq e^{\frac{n}{2}}\frac{b^n}{\sigma^n}
% \end{equation}
% \end{lemma}

%Finally, using a concentration bound for the norm of a Gaussian, i.e., a chi-distribution (Lemma~\ref{lem:concentration} in Appendix~\ref{sec:concentration}) in Equation~\ref{eq:det} and Markov's inequality in Equation~\ref{eq:fp}, we get:
Finally, using a CDF for the norm of a Gaussian, i.e., a chi-distribution in Equation~\ref{eq:det} and Markov's inequality in Equation~\ref{eq:fp}, we get:
% \begin{align}
%     (\alpha^{det})^{\frac{1}{d}} (1-\alpha^{fp}) \leq \left(\frac{e}{d}\right)^{\frac{1}{2}} \left(\frac{M_{\mathcal{D}}}{\beta}\right) \left(\frac{K_U}{K_L}\right)
% \end{align}
\begin{align}
    \alpha^{det} \leq \frac{1}{\Gamma(\frac{d}{2})} \gamma\left(\frac{d}{2},\frac{1}{2}\left(\frac{K_U}{K_L}\frac{M_\mathcal{D}}{\beta}\frac{1}{1-\alpha^{fp}}\right)^2\right)
\end{align}
where: 
\begin{align}
    \gamma(s,x) = \int_0^x t^{s-1}e^{-t}dt\\
    \Gamma(s) = \int_0^\infty t^{s-1}e^{-t}dt
\end{align}
\end{proof}

\pitfallenv{Takeaway}
{\label{takeaway_general} The trade-off observed in the toy model also extends to the more general setting, i.e., the detection rate $\alpha^{det}$ and the false positive rate $\alpha^{fp}$ are still at odds with each other. Furthermore, this trade-off depends upon the standard deviation $\beta$ of the perturbation distribution, the expected spread $M_{\mathcal{D}}$ of natural queries, and the Lipschitz constant ratio $K_U/K_L$ of $H$.}






