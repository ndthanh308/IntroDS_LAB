{
  "title": "A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$λ$ Smoothness",
  "authors": [
    "Hengshuai Yao"
  ],
  "submission_date": "2023-07-29T05:42:23+00:00",
  "revised_dates": [
    "2023-09-02T16:54:32+00:00"
  ],
  "abstract": "Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-α/2})$ for some $α\\in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a \"single-time-scale\" formulation of GTD. However, this formulation still has two step-size parameters.\n  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 2019), called $L$-$λ$ smoothness, we are able to prove that the new GTD converges even faster, in fact, with a linear rate. Our rate actually also improves Gower et al.'s result with a tighter bound under a weaker assumption. Besides Impression GTD, we also prove the rates of three other GTD algorithms, one by Yao and Liu (2008), another called A-transpose-TD (Sutton et al., 2008), and a counterpart of A-transpose-TD. The convergence rates of all the four GTD algorithms are proved in a single generic GTD framework to which $L$-$λ$ smoothness applies. Empirical results on Random walks, Boyan chain, and Baird counterexample show that Impression GTD converges much faster than existing GTD algorithms for both on-policy and off-policy learning problems, with well-performing step-sizes in a big range.",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15892",
  "pdf_url": "https://arxiv.org/pdf/2307.15892v2",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 6746268,
  "size_after_bytes": 758025
}