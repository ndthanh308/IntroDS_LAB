\section{Experiments}\label{sec:experiments}
This section contains empirical results of Impression GTD, for on-policy learning on Boyan chain, and off-policy learning on Random Walks (with tabular representation). Experiments on the inverted- and the dependent- representation for Random Walks are in Appendix \ref{exp:rwinv} and Appendix \ref{exp:rwdep}, respectively. Baird counterexample is in Appendix \ref{exp:baird}. All  the curves reported were averaged over 100 independent runs.

\subsection{Boyan Chain}
The problem is the same as \citep{lstd}. It has 13 states and the rewards are all -3.0 except that the transition from state 1 to state 0 incurs a reward of -2.0. The features are generated by a linear interpolation from four unit basis vectors at states $4i$, $i=0, 1, 2, 3$. Each episode starts from state $12$, and from state $i$ it goes to either $i+1$ or $i+1$, with an equal probability of 0.5. The features can represent the value function for this policy accurately. 

The compared algorithms include GTD, HTD, Vtrace, GTD2, TDC, TDRC, Impression GTD and mini-batch TD. 
At time step $t$, an algorithm gives $\theta_t$, and the metrics is computed by
\[
\mbox{RMSVE}(\theta_t) = \sqrt{\frac{1}{N}\sum_{s=1}^N (V^\pi(s) - \phi(s)^\tr \theta_t)^2}. 
\]


% Figure environment removed

Figure \ref{fig:boyan_rmse} compares the RMSVE of the algorithms.
The batch size for mini-batch TD and Impression GTD are both 10. For Impression GTD, it converges with large step-sizes for this example. So a step-size of 10.0 is used. MiniBatchTD used a step-size of 0.05. All the hyper-parameter of the other algorithms were the same as in \citep{martha2020gradient}. Impression GTD waited until both buffers are bigger than the batch size. So there is a flat curve in the beginning. 
HTDâ€™s curve was almost the same as TD and thus it is not shown.


Figure \ref{fig:boyan_rmse_imGTD_bsz} shows the effect of the batch size for Impression GTD. We select the top two baselines after about 1500 steps in Figure \ref{fig:boyan_rmse}, which are TD and TDRC. Because this problem is on-policy learning, TD converges fast and it stands for the ceiling  for $O(n)$ gradient TD methods in the convergence rate. 
Comparing to TD and TDRC, all the impression GTD algorithms have a steeper drop in the loss, though bigger batch sizes need to wait a bit longer to kick start learning.    Impression GTD with bigger batch sizes (e.g, 32, 64, and 128) is able to learn significantly faster than TD and TDRC. The acceleration in convergence rate seems to decrease after batch size 32.  

% Figure environment removed

Figure \ref{fig:boyan_rmse_imGTD_stepsize} shows the effect of the step-size for Impression GTD. All the four step-sizes performed faster than TD whose step-size was tuned near optimal by \citeauthor{martha2020gradient}, which was 0.0625. The four step-sizes used for plotting this figure were 0.1, 1.0, 5.0 and 10.0. Their value range being big whilst learning all faster than TD means that tuning the step-size for Impression GTD is not as sensitive as the GTD algorithms.  

% Figure environment removed


\subsection{Random Walks}
There are five intermediate states, and two terminal states (which can be treated as one terminal state). The problem is off-policy learning. The target policy goes to left with probability 40\% and to the right with 60\% probability. The behavior policy chooses the left and right actions with equal (50\%) probabilities. There are three representation used \citep{martha2020gradient}.

% Figure environment removed

First, for the tabular representation, Figure \ref{fig:rwtab} shows the results. 
All the algorithms were run with the same, near-optimal hyper-parameters as used in the git repository provided by \citet{martha2020gradient}. Impression GTD used step-size 1.0, and MiniBatchTD used a step-size of 0.05.  The batch size for both algorithms is 32. Impression GTD converges much faster than the baselines including TDRC. Vtrace is a very simple algorithm. It just modifies the importance sampling ratio so that it is upper clipped at 1.0. The motivation of the algorithm is to control the variances caused by importance sampling ratios. It looks Vtrace introduces a bias with the variance reduction. 
TD, HTD and TDRC are  faster than the other baselines, and the gap among the three are small. 

% Figure environment removed
The effect of the batch size for Impression GTD is shown in Figure \ref{fig:rwtab_batch}. The step-size of Impression GTD is uniformly 0.5.   After 4500 steps, all the Impression GTD agents were faster than TD. The acceleration is more with a bigger batch size. However, note that a bigger batch size also means more computation complexity per time step.  This can be accelerated with GPU computation for the mini-batch policy evaluation procedure.

% Figure environment removed

Figure \ref{fig:rwtab_alpha} shows the effect of the step-size for Impression GTD. This shows that a bigger step-size is faster in the beginning. However, there is a {\em convergence rate  overturn}. For example, Impression GTD with $\alpha=1.0$ crosses with $\alpha=0.25$ and 0.5 at about 8100 steps and 7000 steps, respectively.  After the crossing points, $\alpha=1.0$ is slower than a smaller step-size.  This is consistent with the Boyan chain results. We also plotted the MiniBatchTD, which used the same step-size as the TD algorithm. It converges faster than TD after about 2000 steps. In the end, MiniBatchTD was fastest algorithm. However, keep in mind that TD and MiniBatchTD just converges for this off-policy learning task by chance, and they may diverge for general off-policy learning. The batchsize used for this plot is 8.



\subsection{Discussion on the Empirical Results}
In a summary, Impression GTD is fastest in all the compared algorithms, with a single step-size that is much easier to tune than the two-time scale GTD algorithms. Our results show that MSPBE should be interpreted with care, as the literature seems to favor this measure over NEU or RMSVE \citep{sutton2018reinforcement,martha2020gradient}. Our results show that a larger MSPBE does not necessarily mean a worse RMSVE. Vice versa. A small MSPBE does not necessarily imply good learning either. 
What this finding means is that {\em MSPBE is a surrogate loss for NEU}. %If the preconditioner is not well conditioned,  MSPBE does not delegate well. In numerical analysis and iterative algorithms, no one would choose a matrix that has a bad conditioning as the preconditioner. This preconditioner view was first proposed by \citep{ptd_yao}. \citet{ran_gtd} had similar comments on the influence of the conditioning of $C$ on MSPBE.

In practice, % of off-policy learning, the preconditioner in MSPBE is not directly chosen by the users. Instead, the matrix $C$ is dependent on the behavior policy and the features.  
%One can achieve a low MSPBE but still a poor solution. 
%Given that the current literature has a favor of MSPBE over NEU, 
we should perhaps avoid the pitfalls of using MSPBE and also take caution in interpreting the learning results according to it. 