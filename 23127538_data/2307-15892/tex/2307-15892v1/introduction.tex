
\section{Introduction}\label{sec:introduction}
Off-policy learning is an important learning paradigm in reinforcement learning \citep{sutton2018reinforcement}. An agent selects actions according to a policy (called the {\em behavior policy}), and in the meanwhile, the algorithm evaluates another policy (the {\em target policy}).  
When the two policies are the same, it is called {\em on-policy learning} or simply policy evaluation if the context is clear. When they are different, the problem is called {\em off-policy learning}. Temporal Difference (TD) methods \citep{td} are guaranteed to converge for on-policy learning with linear function approximation \citep{dayan1994td,tsi_td,csaba_book,bertsekas2012dynamic}. However, bootstrapping, such as TD(0), is problematic for off-policy learning because the methods can diverge with function approximation even in the linear case \citep{bertsekas1995counterexample,boyan1994generalization,baird1995residual,gordon1995stable,tsi_td}. Off-policy learning, bootstrapping, and function approximation are problematic for reinforcement learning, and they are often referred to as the ``deadly triad'' \citep{sutton2018reinforcement}.


The Gradient TD family \citep*{gtd,tdc} is an important class of off-policy learning algorithms by stablizing the Ordinary Differential Equation (O.D.E.) underlying the TD update. For example, GTD \citep{gtd} is based on minimizing the NEU objective \citep{tdc}, which gives a stable O.D.E. whose underlying system is essentially a normal equation, transforming the TD update into a system with symmetry. The high computation efficiency of the GTD algorithms makes themselves appealing to learn many policies in parallel from a single stream of data, such as the Horde architecture \citep*{sutton2011horde} and universal off-policy evaluator \citep*{chandak2021universal_off_eval} (with an experiment for type-1 diabetes treatment via simulation). 


The GTD algorithms do not suffer from high variances like importance sampling and Emphatic TD methods which are reviewed in Section \ref{sec:background}.  
However, GTD algorithms need further improvement as well because they are not easy to use. Also see \citep*{martha2020gradient} for the ``difficult-to-use'' problem of GTD algorithms from the authors who has used the algorithms for a long time  \citep{sutton2011horde}. GTD algorithms are usually $O(d)$ and this high computation efficiency is due to a two-time-scale formulation of the algorithm in order to mitigate the ``two-sample'' problem for approaching the O.D.E. solution in a stable manner.\footnote{Note the GTD algorithm was formulated in a two-time-scale update \citep{gtd}, but the proof works in a setting that has actually one time scale because the ratio between the two step-sizes is a constant instead of a standard diminishing rate in a strict two-time-scale framework. Also see, e.g., \citep{bo_gtd_finite}. In this constant-ratio formulation, which is called the ``single-time scale'' by literature \citep{martha2020gradient,dalal2020tale_twotimescale},  although the update rates of the two iterators are of the same order, there are still two step-size parameters. The performance of the algorithm is dependent on the step-size ratio as well. The same holds for GTD2 \citep{tdc}. However, the TDC proof was under the standard two-time-scale step-size condition \citep{tdc}. 
} The two-time-scale formulation 
comes with the price of an additional step-size parameter, and
intensive tuning efforts are required in practice in order to have a stable and fast convergence. This poses a great challenge to practitioners. A large family of step-size adaption methods in both supervised learning and reinforcement learning may mitigate the issue, which is however not the scope of this paper. In this paper, we focus on the problem formulation for off-policy learning, and re-examine the necessity of resorting to two time scales for Gradient TD algorithms.   

\citet{martha2020gradient} discussed that the saddle-point formulation of
the MSPBE \citep{bo_gtd_finite} can be utilized to view GTD2 as a single-time-scale update with the joint weight vector from the main and the helper iterators. This enables their TDRC to use $1/\eta$, the ratio between the main step-size and the helper step-size, to be one, i.e., essentially reducing to one step-size. This paper can be viewed as a further continuation of the motivation of TDRC. Furthermore, we examine a few issues that still remain to be solved. For example, the second condition of their Theorem 3.1 requires a condition for $\eta$ to be bigger than the negative of the minimum eigenvalue of some matrix, which is the maximum of some positive eigenvalue. This $\eta$ is much bigger than one in most cases. Thus this theorem still has the flavor of two time scales. 
Our motivation in this paper is to develop a policy evaluation algorithm that has the following desiderata:
\begin{enumerate}
    \item The algorithm should be convergent for off-policy learning. 
    \item It should be stable and guaranteed to converge with bootstrapping.
    \item The convergence is also guaranteed with linear function approximation.
    \item The complexity of the algorithm should be linear in the dimensionality of the weight vector.
\item 
The algorithm should have only one step-size parameter.\footnote{Our work is not to be confused with the GTD formulation in which the ratio between the two step-sizes is a constant.}
\item The algorithm should not suffer from high variances and it should converge much faster than existing GTD algorithms. 
\end{enumerate}
The first four are the continuing goals from existing GTD algorithms. The last two are additional goals that we seek in this paper.   
In the literature there is a belief that off-policy learning ``inherently'' has high variances and it is ``common'' when we conduct off-policy learning, e.g. see \citep*{mahmood2015etd,sutton2018reinforcement}. One point we make in this paper is that this is not necessarily true, at least for achieving the off-policy TD solution. 

Nonetheless, we agree that it seems a good summary of our hard lessons for off-policy learning, for which a few counterexamples for TD were proposed in 1990s \citep{boyan1994generalization,bertsekas1995counterexample,baird1995residual,tsitsiklis1996feature,bertsekas1996neuro}. This motivated importance sampling for off-policy learning \citep{precup2000eligibility,precup2001off}, which solves the divergence issue, however with high variances. Gradient TD algorithms were developed afterwards. They enjoy guaranteed convergence under mild conditions \citep{gtd,tdc},  without the issue of importance sampling TD algorithms. However, GTD algorithms have two step-sizes, which make the algorithms hard to use in practice. Emphatic TD is a recent new off-policy learning approach, which incrementally corrects the sample distribution towards a stable O.D.E underlying the update \citep{etd,mahmood2015etd}. However, the variances of ETD are huge \citep{sutton2018reinforcement}. Empirical results on simple domains showed that stable performance of ETD was only obtainable with tiny step-sizes \citep{ghiassian2017first_etd}, which gave extremely slow convergence.      
This paper continues the exploration of first-order off-policy learning and the linear-complexity triumph by GTD methods. We propose a single-time-scale formulation so that there is only one step-size to tune or adapt in practice, without the high-variance price of importance sampling or emphatic TD algorithms.
% \footnote{Shangtong Zhang mentioned briefly that he and his student were working on an idea that may have similar motivations with the present paper, at a dinner together with Hengshuai at AAAI, February 2023. At the time, an internally reviewed document of this paper at Sony AI had written with the algorithm and experiments on three domains (without the proofs). Their paper will be cited in our revision once theirs is online. }


The paper is organized as follows. In Section \ref{sec:background}, we review the background on the MDP framework, TD, GTD, GTD2 and TDC, and some latest progress. Section \ref{sec:imgtd} presents the basic version of Impression GTD, and Section \ref{sec:minibatchPE} extends the algorithm to the minibatch off-policy evaluation. In Section \ref{sec:theory}, we first give a convergence theorem of Impression GTD under the diminishing step-size. Then we continue to analyze the convergence of Impression GTD under constant step-sizes. For that purpose, we first introduce a condition called $L$-$\lambda$ smoothness, give a few lemmas, and then present a main convergence rate result of SGD under $L$-$\lambda$ smoothness. The analysis is finally conducted on a generic GTD algorithm that includes Impression GTD, Expected GTD, A$^\tr$TD, and a counterpart of A$^\tr$TD. We first show that all the four GTD algorithms converge at least as fast as $O(1/t)$. Furthermore, we show that the generic GTD algorithm converges at a linear rate that depends on the variances in the feature transitions, $\ell_2$ norm of the mean of the feature transition matrix ($A$), and the batch sizes of two buffers. This result is achieved by showing that the GTD problem (i.e., the NEU objective and our sampling method called independent sampling) satisfies the $L$-$\lambda$ smoothness, and applying our main SGD rate result. Section \ref{sec:experiments} contains the empirical results of Impression GTD for on/off-policy learning. Finally, Section \ref{sec:conclusion} concludes the paper. 

