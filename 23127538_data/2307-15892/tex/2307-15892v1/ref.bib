@article{neural_gtd2,
  title={Provably efficient neural GTD for off-policy learning},
  author={Wai, Hoi-To and Yang, Zhuoran and Wang, Zhaoran and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10431--10442},
  year={2020}
}

@inproceedings{asadi2017alternative,
  title={An alternative softmax operator for reinforcement learning},
  author={Asadi, Kavosh and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={243--252},
  year={2017},
  organization={PMLR}
}

@inproceedings{xu2021sample_twotimescale,
  title={Sample complexity bounds for two timescale value-based reinforcement learning algorithms},
  author={Xu, Tengyu and Liang, Yingbin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={811--819},
  year={2021},
  organization={PMLR}
}

@article{hong2020two_bilevel_optimization,
  title={A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic},
  author={Hong, Mingyi and Wai, Hoi-To and Wang, Zhaoran and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2007.05170},
  year={2020}
}

@misc{genTD,
  doi = {10.48550/ARXIV.2107.02711},
  
  url = {https://arxiv.org/abs/2107.02711},
  
  author = {Xu, Tengyu and Yang, Zhuoran and Wang, Zhaoran and Liang, Yingbin},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {A Unified Off-Policy Evaluation Approach for General Value Function},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{martha2018two_time_scale,
  title={Two-timescale networks for nonlinear value function approximation},
  author={Chung, Wesley and Nath, Somjit and Joseph, Ajin and White, Martha},
  booktitle={International conference on learning representations},
  year={2018}
}

@inproceedings{dalal2018finite_twotimescale,
  title={Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning},
  author={Dalal, Gal and Thoppe, Gugan and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Mannor, Shie},
  booktitle={Conference On Learning Theory},
  pages={1199--1233},
  year={2018},
  organization={PMLR}
}

@article{doan2021finite,
  title={Finite-time analysis and restarting scheme for linear two-time-scale stochastic approximation},
  author={Doan, Thinh T},
  journal={SIAM Journal on Control and Optimization},
  volume={59},
  number={4},
  pages={2798--2819},
  year={2021},
  publisher={SIAM}
}

@inproceedings{silver_gtdnet,
  title={Gradient temporal difference networks},
  author={Silver, David},
  booktitle={European Workshop on Reinforcement Learning},
  pages={117--130},
  year={2013},
  organization={PMLR}
}

@inproceedings{xu2020finite_q_learning,
  title={A finite-time analysis of Q-learning with neural network function approximation},
  author={Xu, Pan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={10555--10565},
  year={2020},
  organization={PMLR}
}


@article{neyshabur2018towards_overpara_net,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}

@article{zhang2021understanding_deepnet_generalization,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{scherrer2010should_bellerr,
  title={Should one compute the temporal difference fix point or minimize the bellman residual? the unified oblique projection view},
  author={Scherrer, Bruno},
  journal={arXiv preprint arXiv:1011.4362},
  year={2010}
}

@article{zhang2019_rg,
  title={Deep residual reinforcement learning},
  author={Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1905.01072},
  year={2019}
}

@article{kleinberg1999authoritative,
  title={Authoritative sources in a hyperlinked environment},
  author={Kleinberg, Jon M},
  journal={Journal of the ACM (JACM)},
  volume={46},
  number={5},
  pages={604--632},
  year={1999},
  publisher={ACM New York, NY, USA}
}

@article{zhang2020learning_retrospective,
  title={Learning retrospective knowledge with reverse reinforcement learning},
  author={Zhang, Shangtong and Veeriah, Vivek and Whiteson, Shimon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19976--19987},
  year={2020}
}

@inproceedings{zhang2020gradientdice,
  title={Gradientdice: Rethinking generalized offline estimation of stationary values},
  author={Zhang, Shangtong and Liu, Bo and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={11194--11203},
  year={2020},
  organization={PMLR}
}

@article{zhang2020gendice,
  title={Gendice: Generalized offline estimation of stationary values},
  author={Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2002.09072},
  year={2020}
}

@inproceedings{kearns2000bias,
  title={Bias-Variance Error Bounds for Temporal Difference Updates.},
  author={Kearns, Michael J and Singh, Satinder},
  booktitle={COLT},
  pages={142--147},
  year={2000}
}

@article{Kiwiel_quasi_convex,
author = {Kiwiel, Krzysztof},
year = {2001},
month = {01},
pages = {1-25},
title = {Convergence and efficiency of subgradient methods for quasiconvex minimization},
volume = {90},
journal = {Mathematical Programming, Series B},
doi = {10.1007/PL00011414}
}

@misc{quasi_convex_hu2019convergence,
      title={Convergence Rates of Subgradient Methods for Quasi-convex Optimization Problems}, 
      author={Yaohua Hu and Jiawen Li and Carisa Kwok Wai Yu},
      year={2019},
      eprint={1910.10879},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{vaswani2021painless,
      title={Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates}, 
      author={Sharan Vaswani and Aaron Mishkin and Issam Laradji and Mark Schmidt and Gauthier Gidel and Simon Lacoste-Julien},
      year={2021},
      eprint={1905.09997},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{weak_growth_schmit,
  author       = {Sharan Vaswani and
                  Francis R. Bach and
                  Mark Schmidt},
  title        = {Fast and Faster Convergence of {SGD} for Over-Parameterized Models
                  and an Accelerated Perceptron},
  journal      = {CoRR},
  volume       = {abs/1810.07288},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.07288},
  eprinttype    = {arXiv},
  eprint       = {1810.07288},
  timestamp    = {Mon, 09 Nov 2020 08:50:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-07288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{quasi_convex1,
      title={Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\L{}ojasiewicz Condition}, 
      author={Hamed Karimi and Julie Nutini and Mark Schmidt},
      year={2020},
      eprint={1608.04636},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{quasi_convex2,
      title={Linear convergence of first order methods for non-strongly convex optimization}, 
      author={I. Necoara and Yu. Nesterov and F. Glineur},
      year={2016},
      eprint={1504.06298},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{reddi2016stochastic,
      title={Stochastic Variance Reduction for Nonconvex Optimization}, 
      author={Sashank J. Reddi and Ahmed Hefny and Suvrit Sra and Barnabas Poczos and Alex Smola},
      year={2016},
      eprint={1603.06160},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{ghadimi2013stochastic,
      title={Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming}, 
      author={Saeed Ghadimi and Guanghui Lan},
      year={2013},
      eprint={1309.5549},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@inproceedings{ran_gtd,
      title={Toward Efficient Gradient-Based Value Estimation}, 
      author={Arsalan Sharifnassab and Richard Sutton},
      year={2023},
      booktitle={ICML},
}

@misc{vtrace,
  doi = {10.48550/ARXIV.1802.01561},
  
  url = {https://arxiv.org/abs/1802.01561},
  
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{tadic_td,
  title={On the convergence of temporal-difference learning with linear function approximation},
  author={Tadi{\'c}, Vladislav},
  journal={Machine learning},
  volume={42},
  pages={241--267},
  year={2001},
  publisher={Springer}
}

@inproceedings{martha2020gradient,
    author = {Ghiassian, Sina
              and Patterson, Andrew
              and Garg, Shivam
              and Gupta, Dhawal
              and White, Adam
              and White, Martha},
    title = {Gradient Temporal Difference Learning with Regularized Corrections},
    booktitle = {Proceedings of the 37th International Conference on International Conference on Machine Learning},
    year={2020}
}

@article{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{xu2019two_time_gtd,
  title={Two time-scale off-policy TD learning: Non-asymptotic analysis over Markovian samples},
  author={Xu, Tengyu and Zou, Shaofeng and Liang, Yingbin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}



@article{wang2017finite_gtd,
  title={Finite sample analysis of the GTD policy evaluation algorithms in Markov setting},
  author={Wang, Yue and Chen, Wei and Liu, Yuting and Ma, Zhi-Ming and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@article{wang2019neural_policygradient,
  title={Neural policy gradient methods: Global optimality and rates of convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1909.01150},
  year={2019}
}


@article{wai2019variance_rl,
  title={Variance reduced policy evaluation with smooth function approximation},
  author={Wai, Hoi-To and Hong, Mingyi and Yang, Zhuoran and Wang, Zhaoran and Tang, Kexin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{csaba_book,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@inproceedings{dalal2020tale_twotimescale,
  title={A tale of two-timescale reinforcement learning with the tightest finite-time bound},
  author={Dalal, Gal and Szorenyi, Balazs and Thoppe, Gugan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3701--3708},
  year={2020}
}

@article{chandak2021universal_off_eval,
  title={Universal off-policy evaluation},
  author={Chandak, Yash and Niekum, Scott and da Silva, Bruno and Learned-Miller, Erik and Brunskill, Emma and Thomas, Philip S},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27475--27490},
  year={2021}
}

@article{unreal,
  author    = {Max Jaderberg and
               Volodymyr Mnih and
               Wojciech Marian Czarnecki and
               Tom Schaul and
               Joel Z. Leibo and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Reinforcement Learning with Unsupervised Auxiliary Tasks},
  journal   = {CoRR},
  volume    = {abs/1611.05397},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05397},
  eprinttype = {arXiv},
  eprint    = {1611.05397},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/JaderbergMCSLSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{doina_variance_offac,
  title={Variance penalized on-policy and off-policy actor-critic},
  author={Jain, Arushi and Patil, Gandharv and Jain, Ayush and Khetarpal, Khimya and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={7899--7907},
  year={2021}
}

@article{etd,
  title={An emphatic approach to the problem of off-policy temporal-difference learning},
  author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2603--2631},
  year={2016},
  publisher={JMLR. org}
}



@inproceedings{hallak2017consistent,
  title={Consistent on-line off-policy evaluation},
  author={Hallak, Assaf and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={1372--1383},
  year={2017},
  organization={PMLR}
}

@inproceedings{gelada2019off_IS_stationary,
  title={Off-policy deep reinforcement learning by bootstrapping the covariate shift},
  author={Gelada, Carles and Bellemare, Marc G},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3647--3655},
  year={2019}
}


@article{liu2018breaking_IS_stationary,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@misc{htd_marhta,
      title={Investigating practical linear temporal difference learning}, 
      author={Adam White and Martha White},
      year={2016},
      eprint={1602.08771},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{HTD_leah,
  title={University of Alberta Faster Gradient-td Algorithms},
  author={Leah Hackman},
  year={2012}
}

@article{xie2019towards_IS,
  title={Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling},
  author={Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@InProceedings{IS_conditioned_return,
  title = 	 {Conditional Importance Sampling for Off-Policy Learning},
  author =       {Rowland, Mark and Harutyunyan, Anna and van Hasselt, Hado and Borsa, Diana and Schaul, Tom and Munos, Remi and Dabney, Will},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {45--55},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/rowland20b/rowland20b.pdf},
  url = 	 {https://proceedings.mlr.press/v108/rowland20b.html},
  abstract = 	 {The principal contribution of this paper is a conceptual framework for off-policy reinforcement learning, based on conditional expectations of importance sampling ratios. This framework yields new perspectives and understanding of existing off-policy algorithms, and reveals a broad space of unexplored algorithms. We theoretically analyse this space, and concretely investigate several algorithms that arise from this framework.}
}


@article{mahmood2014weighted,
  title={Weighted importance sampling for off-policy learning with linear function approximation},
  author={Mahmood, A Rupam and Van Hasselt, Hado P and Sutton, Richard S},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}


@inproceedings{bo_gtd_finite,
  author    = {Bo Liu and
               Ji Liu and
               Mohammad Ghavamzadeh and
               Sridhar Mahadevan and
               Marek Petrik},
  editor    = {Marina Meila and
               Tom Heskes},
  title     = {Finite-Sample Analysis of Proximal Gradient {TD} Algorithms},
  booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2015, July 12-16, 2015, Amsterdam, The Netherlands},
  pages     = {504--513},
  publisher = {{AUAI} Press},
  year      = {2015},
  url       = {http://auai.org/uai2015/proceedings/papers/38.pdf},
  timestamp = {Thu, 12 Mar 2020 11:31:12 +0100},
  biburl    = {https://dblp.org/rec/conf/uai/LiuLGMP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{gupta2019finite,
  title={Finite-time performance bounds and adaptive learning rate selection for two time-scale reinforcement learning},
  author={Gupta, Harsh and Srikant, Rayadurgam and Ying, Lei},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{zhou2020stochastic_variancereduction,
  title={Stochastic nested variance reduction for nonconvex optimization},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={4130--4192},
  year={2020},
  publisher={JMLRORG}
}



@inproceedings{dai2018sbeed,
  title={SBEED: Convergent reinforcement learning with nonlinear function approximation},
  author={Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
  booktitle={International Conference on Machine Learning},
  pages={1125--1134},
  year={2018},
  organization={PMLR}
}


@article{maei2009convergent_nonlinear,
  title={Convergent temporal-difference learning with arbitrary smooth function approximation},
  author={Maei, Hamid and Szepesvari, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

@article{cai2019neural_td,
  title={Neural temporal-difference learning converges to global optima},
  author={Cai, Qi and Yang, Zhuoran and Lee, Jason D and Wang, Zhaoran},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@misc{td_finite_time_analysis,
  doi = {10.48550/ARXIV.1806.02450},
  
  url = {https://arxiv.org/abs/1806.02450},
  
  author = {Bhandari, Jalaj and Russo, Daniel and Singal, Raghav},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{gen_drl_dabney,
  doi = {10.48550/ARXIV.2206.02126},
  
  url = {https://arxiv.org/abs/2206.02126},
  
  author = {Lyle, Clare and Rowland, Mark and Dabney, Will and Kwiatkowska, Marta and Gal, Yarin},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Dynamics and Generalization in Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{loss_contour_vis,
      title={Visualizing the Loss Landscape of Neural Nets}, 
      author={Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
      year={2018},
      eprint={1712.09913},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rosenbrock1960automatic,
  title={An automatic method for finding the greatest or least value of a function},
  author={Rosenbrock, HoHo},
  journal={The computer journal},
  volume={3},
  number={3},
  pages={175--184},
  year={1960},
  publisher={Oxford University Press}
}

@misc{lr_cosine_anneal,
  doi = {10.48550/ARXIV.1608.03983},
  
  url = {https://arxiv.org/abs/1608.03983},
  
  author = {Loshchilov, Ilya and Hutter, Frank},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{large_batch_more_updates,
  doi = {10.48550/ARXIV.1705.08741},
  
  url = {https://arxiv.org/abs/1705.08741},
  
  author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{large_batch_kaiming,
  author    = {Priya Goyal and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick and
               Pieter Noordhuis and
               Lukasz Wesolowski and
               Aapo Kyrola and
               Andrew Tulloch and
               Yangqing Jia and
               Kaiming He},
  title     = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal   = {CoRR},
  volume    = {abs/1706.02677},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02677},
  eprinttype = {arXiv},
  eprint    = {1706.02677},
  timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalDGNWKTJH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{large_batch_goldstein,
  title = 	 {{Automated Inference with Adaptive Batches}},
  author = 	 {De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1504--1513},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/de17a/de17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/de17a.html},
  abstract = 	 {Classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution. The large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping. We propose alternative “big batch” SGD schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation. The resulting methods have similar convergence rates to classical SGD, and do not require convexity of the objective. The high fidelity gradients enable automated learning rate selection and do not require stepsize decay. Big batch methods are thus easily automated and can run with little or no oversight.}
}


@article{sharpness_yoshua,
  title={Generalization in deep learning},
  author={Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.05468},
  year={2017}
}

@article{decision_boundary_dl,
  author    = {David Mickisch and
               Felix Assion and
               Florens Gre{\ss}ner and
               Wiebke G{\"{u}}nther and
               Mariele Motta},
  title     = {Understanding the Decision Boundary of Deep Neural Networks: An Empirical
               Study},
  journal   = {CoRR},
  volume    = {abs/2002.01810},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.01810},
  eprinttype = {arXiv},
  eprint    = {2002.01810},
  timestamp = {Mon, 10 Feb 2020 15:12:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-01810.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{ostermeier1994step,
  title={Step-size adaptation based on non-local use of selection information},
  author={Ostermeier, Andreas and Gawelczyk, Andreas and Hansen, Nikolaus},
  booktitle={International Conference on Parallel Problem Solving from Nature},
  pages={189--198},
  year={1994},
  organization={Springer}
}

@article{uom,
  title={Universal option models},
  author={Yao, Hengshuai and Szepesvari, Csaba and Sutton, Richard S and Modayil, Joseph and Bhatnagar, Shalabh},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}

@article{ma2018universal,
  title={Universal successor representations for transfer reinforcement learning},
  author={Ma, Chen and Wen, Junfeng and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1804.03758},
  year={2018}
}

@article{touati2021learning_uoms,
  title={Learning one representation to optimize all rewards},
  author={Touati, Ahmed and Ollivier, Yann},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13--23},
  year={2021}
}

@article{van2017hybrid,
  title={Hybrid reward architecture for reinforcement learning},
  author={Van Seijen, Harm and Fatemi, Mehdi and Romoff, Joshua and Laroche, Romain and Barnes, Tavian and Tsang, Jeffrey},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{svrg,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@inproceedings{gupta2021uneven_uoms,
  title={Uneven: Universal value exploration for multi-agent reinforcement learning},
  author={Gupta, Tarun and Mahajan, Anuj and Peng, Bei and B{\"o}hmer, Wendelin and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={3930--3941},
  year={2021},
  organization={PMLR}
}

@inproceedings{schaul2015universal,
  title={Universal value function approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International conference on machine learning},
  pages={1312--1320},
  year={2015},
  organization={PMLR}
}

@misc{zhou_strongly_convex,
  doi = {10.48550/ARXIV.1803.06573},
  
  url = {https://arxiv.org/abs/1803.06573},
  
  author = {Zhou, Xingyu},
  
  keywords = {Optimization and Control (math.OC), FOS: Mathematics, FOS: Mathematics},
  
  title = {On the Fenchel Duality between Strong Convexity and Lipschitz Continuous Gradient},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{wright2015coordinate,
  title={Coordinate descent algorithms},
  author={Wright, Stephen J},
  journal={Mathematical Programming},
  volume={151},
  number={1},
  pages={3--34},
  year={2015},
  publisher={Springer}
}

@inproceedings{YaoSze12,
	abstract = {In this paper we consider the problem of finding a good policy given some batch data. We propose a new approach, LAM-API, that first builds a so-called linear action model (LAM) from the data and then uses the learned model and the collected data in approximate policy iteration (API) to find a good policy. A natural choice for the policy evaluation step in this algorithm is to use least-squares temporal difference (LSTD) learning algorithm. Empirical results on three benchmark problems show that this particular instance of LAM-API performs competitively as compared with LSPI, both from the point of view of data and computational efficiency.},
	acceptrate = {294 out of 1129=26\%},
	author = {Yao, H. and Szepesv{\'a}ri, Cs.},
	booktitle = {AAAI-2012},
	keywords = {reinforcement learning, Markov Decision Processes,function approximation, control, planning, control learning, temporal difference learning, LSTD},
	month = {July},
	pages = {1212--1217},
	title = {Approximate Policy Iteration with Linear Action Models},
	url_paper = {lamapi.pdf},
	year = {2012}}
	
@article{vanschoren2018meta,
  title={Meta-learning: A survey},
  author={Vanschoren, Joaquin},
  journal={arXiv preprint arXiv:1810.03548},
  year={2018}
}

@article{khodak2019adaptive,
  title={Adaptive gradient-based meta-learning methods},
  author={Khodak, Mikhail and Balcan, Maria-Florina F and Talwalkar, Ameet S},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{lee2019meta,
  title={Meta-learning with differentiable convex optimization},
  author={Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10657--10665},
  year={2019}
}

@article{yin2019meta,
  title={Meta-learning without memorization},
  author={Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1912.03820},
  year={2019}
}

@article{rajeswaran2019meta,
  title={Meta-learning with implicit gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{linear_inter_connect_minima,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@misc{linear_inter_monotonic,
  doi = {10.48550/ARXIV.2104.11044},
  
  url = {https://arxiv.org/abs/2104.11044},
  
  author = {Lucas, James and Bae, Juhan and Zhang, Michael R. and Fort, Stanislav and Zemel, Richard and Grosse, Roger},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{linear_inter_loss_surface,
  doi = {10.48550/ARXIV.1612.04010},
  
  url = {https://arxiv.org/abs/1612.04010},
  
  author = {Im, Daniel Jiwoong and Tao, Michael and Branson, Kristin},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An empirical analysis of the optimization of deep network loss surfaces},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{NLMS,
  title={A generalized normalized gradient descent algorithm},
  author={Mandic, Danilo P},
  journal={IEEE signal processing letters},
  volume={11},
  number={2},
  pages={115--118},
  year={2004},
  publisher={IEEE}
}

@article{wolfe1969convergence,
  title={Convergence conditions for ascent methods},
  author={Wolfe, Philip},
  journal={SIAM review},
  volume={11},
  number={2},
  pages={226--235},
  year={1969},
  publisher={SIAM}
}

@misc{linear_inter_fullyconnected,
  doi = {10.48550/ARXIV.2204.04511},
  
  url = {https://arxiv.org/abs/2204.04511},
  
  author = {Doknic, Aleksandar and Möller, Torsten},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {FuNNscope: Visual microscope for interactively exploring the loss landscape of fully connected neural networks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{linear_inter_model_icml22,
  author    = {Tiffany Vlaar and
               Jonathan Frankle},
  title     = {What can linear interpolation of neural network loss landscapes tell
               us?},
  journal   = {ICML},
  year      = {2022},
  url       = {https://arxiv.org/abs/2106.16004},
  eprinttype = {arXiv},
  eprint    = {2106.16004},
  timestamp = {Mon, 05 Jul 2021 15:15:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-16004.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cifar-dataset,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@misc{dist_rl,
  doi = {10.48550/ARXIV.1707.06887},
  
  url = {https://arxiv.org/abs/1707.06887},
  
  author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Distributional Perspective on Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{pr_bishop,
  title={Neural networks for pattern recognition},
  author={Bishop, Christopher M and others},
  year={1995},
  publisher={Oxford university press}
}

@book{hanfu_2006stochastic,
  title={Stochastic approximation and its applications},
  author={Chen, Han-Fu},
  volume={64},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{simon_book,
author = {Haykin, Simon},
title = {Neural Networks: A Comprehensive Foundation},
year = {1994},
isbn = {0023527617},
publisher = {Prentice Hall PTR},
address = {USA},
edition = {1st},
abstract = {From the Publisher:This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.}
}

@article{rusu2018meta,
  title={Meta-learning with latent embedding optimization},
  author={Rusu, Andrei A and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
  journal={arXiv preprint arXiv:1807.05960},
  year={2018}
}

@inproceedings{santoro2016meta,
  title={Meta-learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle={International conference on machine learning},
  pages={1842--1850},
  year={2016},
  organization={PMLR}
}

@article{lei1996dynamic,
  title={Dynamic core competences through meta-learning and strategic context},
  author={Lei, David and Hitt, Michael A and Bettis, Richard},
  journal={Journal of management},
  volume={22},
  number={4},
  pages={549--569},
  year={1996},
  publisher={Elsevier}
}

@inproceedings{chan1993experiments,
  title={Experiments on multistrategy learning by meta-learning},
  author={Chan, Philip K and Stolfo, Salvatore J},
  booktitle={Proceedings of the second international conference on information and knowledge management},
  pages={314--323},
  year={1993}
}

@article{javed2019meta,
  title={Meta-learning representations for continual learning},
  author={Javed, Khurram and White, Martha},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{autostep,
  title={Tuning-free step-size adaptation},
  author={Mahmood, Ashique Rupam and Sutton, Richard S and Degris, Thomas and Pilarski, Patrick M},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2121--2124},
  year={2012},
  organization={IEEE}
}

@misc{PL_nonuniform,
  doi = {10.48550/ARXIV.2105.06072},
  
  url = {https://arxiv.org/abs/2105.06072},
  
  author = {Mei, Jincheng and Gao, Yue and Dai, Bo and Szepesvari, Csaba and Schuurmans, Dale},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Leveraging Non-uniformity in First-order Non-convex Optimization},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{meta-gradient-sutton,
  doi = {10.48550/ARXIV.2202.09701},
  
  url = {https://arxiv.org/abs/2202.09701},
  
  author = {Sutton, Richard S.},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A History of Meta-gradient: Gradient Methods for Meta-learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@misc{meta_trace,
  doi = {10.48550/ARXIV.1805.04514},
  
  url = {https://arxiv.org/abs/1805.04514},
  
  author = {Young, Kenny and Wang, Baoxiang and Taylor, Matthew E.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Metatrace Actor-Critic: Online Step-size Tuning by Meta-gradient Descent for Reinforcement Learning Control},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{idbd_td,
  doi = {10.48550/ARXIV.1804.03334},
  
  url = {https://arxiv.org/abs/1804.03334},
  
  author = {Kearney, Alex and Veeriah, Vivek and Travnik, Jaden B. and Sutton, Richard S. and Pilarski, Patrick M.},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {TIDBD: Adapting Temporal-difference Step-sizes Through Stochastic Meta-descent},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{meta-gradient-questions,
  doi = {10.48550/ARXIV.1909.04607},
  
  url = {https://arxiv.org/abs/1909.04607},
  
  author = {Veeriah, Vivek and Hessel, Matteo and Xu, Zhongwen and Lewis, Richard and Rajendran, Janarthanan and Oh, Junhyuk and van Hasselt, Hado and Silver, David and Singh, Satinder},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Discovery of Useful Questions as Auxiliary Tasks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{shangtong_idbd,
  author    = {Vivek Veeriah, and Shangtong Zhang, and Richard S. Sutton},
  title     = {Learning representations through stochastic gradient descent in cross-validation
               error},
  journal   = {CoRR},
  volume    = {abs/1612.02879},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.02879},
  eprinttype = {arXiv},
  eprint    = {1612.02879},
  timestamp = {Mon, 13 Aug 2018 16:46:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SuttonV16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{meta-gradient-rl,
  doi = {10.48550/ARXIV.1805.09801},
  
  url = {https://arxiv.org/abs/1805.09801},
  
  author = {Xu, Zhongwen and van Hasselt, Hado and Silver, David},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Meta-Gradient Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{hospedales2020meta,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={arXiv preprint arXiv:2004.05439},
  year={2020}
}

@inproceedings{finn2019online,
  title={Online meta-learning},
  author={Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1920--1930},
  year={2019},
  organization={PMLR}
}

@misc{ppo,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{nichol2018first,
  title={On first-order meta-learning algorithms},
  author={Nichol, Alex and Achiam, Joshua and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@article{vilalta2002perspective,
  title={A perspective view and survey of meta-learning},
  author={Vilalta, Ricardo and Drissi, Youssef},
  journal={Artificial intelligence review},
  volume={18},
  number={2},
  pages={77--95},
  year={2002},
  publisher={Springer}
}


@article{BB_step_1988,
  title={Two-point step size gradient methods},
  author={Barzilai, Jonathan and Borwein, Jonathan M},
  journal={IMA journal of numerical analysis},
  volume={8},
  number={1},
  pages={141--148},
  year={1988},
  publisher={Oxford University Press}
}

@inproceedings{BB_sgd,
 author = {Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Barzilai-Borwein Step Size for Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2016/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf},
 volume = {29},
 year = {2016}
}

@misc{BB_improvement1,
  doi = {10.48550/ARXIV.2001.02335},
  
  url = {https://arxiv.org/abs/2001.02335},
  
  author = {Huang, Yakui and Dai, Yu-Hong and Liu, Xin-Wei and Zhang, Hongchao},
  
  keywords = {Optimization and Control (math.OC), FOS: Mathematics, FOS: Mathematics},
  
  title = {On the acceleration of the Barzilai-Borwein method},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{BB_dai2013new,
  title={A new analysis on the Barzilai-Borwein gradient method},
  author={Dai, Yu-Hong},
  journal={Journal of the operations Research Society of China},
  volume={1},
  number={2},
  pages={187--198},
  year={2013},
  publisher={Springer}
}

@article{BB_dai2002r,
  title={R-linear convergence of the Barzilai and Borwein gradient method},
  author={Dai, Yu-Hong and Liao, Li-Zhi},
  journal={IMA Journal of Numerical Analysis},
  volume={22},
  number={1},
  pages={1--10},
  year={2002},
  publisher={Oxford University Press}
}

@book{wang2009model,
  title={Model predictive control system design and implementation using MATLAB{\textregistered}},
  author={Wang, Liuping},
  year={2009},
  publisher={Springer Science \& Business Media}
}


@inproceedings{lima2015clothoid,
  title={Clothoid-based model predictive control for autonomous driving},
  author={Lima, Pedro F and Trincavelli, Marco and M{\aa}rtensson, Jonas and Wahlberg, Bo},
  booktitle={2015 European Control Conference (ECC)},
  pages={2983--2990},
  year={2015},
  organization={IEEE}
}

@article{forbes2015model,
  title={Model predictive control in industry: Challenges and opportunities},
  author={Forbes, Michael G and Patwardhan, Rohit S and Hamadah, Hamza and Gopaluni, R Bhushan},
  journal={IFAC-PapersOnLine},
  volume={48},
  number={8},
  pages={531--538},
  year={2015},
  publisher={Elsevier}
}

@article{rawlings2000tutorial,
  title={Tutorial overview of model predictive control},
  author={Rawlings, James B},
  journal={IEEE control systems magazine},
  volume={20},
  number={3},
  pages={38--52},
  year={2000},
  publisher={IEEE}
}

@book{berteskas1987dynamic,
  title={Dynamic programming: deterministic and stochastic models},
  author={Berteskas, D},
  year={1987},
  publisher={Englewood Cliffs, NJ: Prentice-Hall}
}

@article{bradtke1992reinforcement,
  title={Reinforcement learning applied to linear quadratic regulation},
  author={Bradtke, Steven},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@article{silver2009reinforcement,
  title={Reinforcement learning and simulation-based search in computer Go},
  author={Silver, David},
  year={2009}
}

@inproceedings{helicopter1,
 author = {Ng, Andrew, Kim, H. and Jordan, Michael and Sastry, Shankar},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Autonomous Helicopter Flight via Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2003/file/b427426b8acd2c2e53827970f2c2f526-Paper.pdf},
 volume = {16},
 year = {2003}
}



@inproceedings{sppi,
author = {Bowling, Michael and Geramifard, Alborz and Wingate, David},
title = {Sigma Point Policy Iteration},
year = {2008},
isbn = {9780981738109},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In reinforcement learning, least-squares temporal difference methods (e.g., LSTD and LSPI) are effective, data-efficient techniques for policy evaluation and control with linear value function approximation. These algorithms rely on policy-dependent expectations of the transition and reward functions, which require all experience to be remembered and iterated over for each new policy evaluated. We propose to summarize experience with a compact policy-independent Gaussian model. We show how this policy-independent model can be transformed into a policy-dependent form and used to perform policy evaluation. Because closed-form transformations are rarely available, we introduce an efficient sigma point approximation. We show that the resulting Sigma-Point Policy Iteration algorithm (SPPI) is mathematically equivalent to LSPI for tabular representations and empirically demonstrate comparable performance for approximate representations. However, the experience does not need to be saved or replayed, meaning that for even moderate amounts of experience, SPPI is an order of magnitude faster than LSPI.},
booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {379–386},
numpages = {8},
keywords = {least-squares, policy iteration, reinforcement learning},
location = {Estoril, Portugal},
series = {AAMAS '08}
}

@article{qin2003survey,
  title={A survey of industrial model predictive control technology},
  author={Qin, S Joe and Badgwell, Thomas A},
  journal={Control engineering practice},
  volume={11},
  number={7},
  pages={733--764},
  year={2003},
  publisher={Elsevier}
}

@book{camacho2013model,
  title={Model predictive control},
  author={Camacho, Eduardo F and Alba, Carlos Bordons},
  year={2013},
  publisher={Springer science \& business media}
}

@article{kalman1960contributions,
  title={Contributions to the theory of optimal control},
  author={Kalman, Rudolf Emil and others},
  journal={Bol. soc. mat. mexicana},
  volume={5},
  number={2},
  pages={102--119},
  year={1960}
}

@misc{cosine_lr_annealing,
  doi = {10.48550/ARXIV.1608.03983},
  
  url = {https://arxiv.org/abs/1608.03983},
  
  author = {Loshchilov, Ilya and Hutter, Frank},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{BB_zhou2006gradient,
  title={Gradient methods with adaptive step-sizes},
  author={Zhou, Bin and Gao, Li and Dai, Yu-Hong},
  journal={Computational Optimization and Applications},
  volume={35},
  number={1},
  pages={69--86},
  year={2006},
  publisher={Springer}
}

@book{fletcher2000practical,
  title={Practical methods of optimization, 2nd Edition},
  author={Fletcher, Roger},
  year={2000},
  publisher={John Wiley \& Sons}
}

@article{broyden1965class,
  title={A class of methods for solving nonlinear simultaneous equations},
  author={Broyden, Charles G},
  journal={Mathematics of computation},
  volume={19},
  number={92},
  pages={577--593},
  year={1965},
  publisher={JSTOR}
}

@InProceedings{pmlr-v39-givchi14,
  title = 	 {Quasi Newton Temporal Difference Learning},
  author = 	 {Givchi, Arash and Palhang, Maziar},
  booktitle = 	 {Proceedings of the Sixth Asian Conference on Machine Learning},
  pages = 	 {159--172},
  year = 	 {2015},
  editor = 	 {Phung, Dinh and Li, Hang},
  volume = 	 {39},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Nha Trang City, Vietnam},
  month = 	 {26--28 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v39/givchi14.pdf},
  url = 	 {https://proceedings.mlr.press/v39/givchi14.html},
  abstract = 	 {Fast convergent and computationally inexpensive policy evaluation is an essential part of reinforcement learning algorithms based on policy iteration. Algorithms such as LSTD, LSPE, FPKF and NTD, have faster convergence rate but they are computationally slow. On the other hand, there are algorithms that are computationally fast but with slower convergence rate, among them are TD, RG, GTD2 and TDC. This paper presents a regularized Quasi Newton Temporal Difference learning algorithm which uses the second-order information while maintaining a fast convergence rate. In simple language, we combine the idea of TD learning with Quasi Newton algorithm SGD-QN. We explore the development of QNTD algorithm and discuss its convergence properties. We support our ideas with empirical results on 4 standard benchmarks in reinforcement learning literature with 2 small problems, Random Walk and Boyan chain and 2 bigger problems, cart-pole and linked-pole balancing. Empirical studies show that QNTD speeds up convergence and provides better accuracy in comparison to the conventional TD.}
}


@inproceedings{Yao_direct_preconditioning,
    author = {Hengshuai Yao and Shalabh Bhatnagar and Csaba Szepesv{\'a}ri},
    title = {Temporal Difference Learning by Direct Preconditioning},
    booktitle={Multidisciplinary Symposium on Reinforcement Learning (MSRL)},
    year = {2009}
}

@article{BB_sgd2,
title = {Random Barzilai–Borwein step size for mini-batch algorithms},
journal = {Engineering Applications of Artificial Intelligence},
volume = {72},
pages = {124-135},
year = {2018},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618300666},
author = {Zhuang Yang and Cheng Wang and Zhemin Zhang and Jonathan Li},
keywords = {Stochastic gradient descent, Mini batches, Barzilai–Borwein method, Variance reduction, Convex optimization},
abstract = {Mini-batch algorithms, a well-studied, highly popular approach in stochastic optimization methods, are used by practitioners because of their ability to accelerate training through better use of parallel processing power and reduction of stochastic variance. However, mini-batch algorithms often employ either a diminishing step size or a tuning step size by hand, which, in practice, can be time consuming. In this paper, we propose using the improved Barzilai–Borwein (BB) method to automatically compute step sizes for the state of the art mini-batch algorithm (mini-batch semi-stochastic gradient descent (mS2GD) method), which leads to a new algorithm: mS2GD-RBB. We theoretically prove that mS2GD-RBB converges with a linear convergence rate for strongly convex objective functions. To further validate the efficacy and scalability of the improved BB method, we introduce it into another modern mini-batch algorithm, Accelerated Mini-Batch Prox SVRG (Acc-Prox-SVRG) method. In a machine learning context, numerical experiments on three benchmark data sets indicate that the proposed methods outperform some advanced stochastic optimization methods.}
}


@misc{BB_regularization,
  doi = {10.48550/ARXIV.1910.06532},
  
  url = {https://arxiv.org/abs/1910.06532},
  
  author = {Li, Bingcong and Giannakis, Georgios B.},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adaptive Step Sizes in Variance Reduction via Regularization},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{BB_VI,
title = {Some projection methods with the BB step sizes for variational inequalities},
journal = {Journal of Computational and Applied Mathematics},
volume = {236},
number = {9},
pages = {2590-2604},
year = {2012},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2011.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0377042711006315},
author = {Hongjin He and Deren Han and Zhibao Li},
keywords = {BB step size, Variational inequalities, Projection methods, Complementarity problems, Image deblurring problems, Nash equilibrium problems},
abstract = {Since the appearance of the Barzilai–Borwein (BB) step sizes strategy for unconstrained optimization problems, it received more and more attention of the researchers. It was applied in various fields of the nonlinear optimization problems and recently was also extended to optimization problems with bound constraints. In this paper, we further extend the BB step sizes to more general variational inequality (VI) problems, i.e., we adopt them in projection methods. Under the condition that the underlying mapping of the VI problem is strongly monotone and Lipschitz continuous and the modulus of strong monotonicity and the Lipschitz constant satisfy some further conditions, we establish the global convergence of the projection methods with BB step sizes. A series of numerical examples are presented, which demonstrate that the proposed methods are convergent under mild conditions, and are more efficient than some classical projection-like methods.}
}

@article{BB_prediction_correction,
author = {Xiaomei DONG, Xingju CAI, Deren HAN},
title = {Prediction-correction method with BB step sizes},
publisher = {Front. Math. China},
year = {2018},
journal = {Frontiers of Mathematics in China},
volume = {13},
number = {6},
eid = {1325},
numpages = {15},
pages = {1325},
keywords = {BB step sizes;projection method;prediction-correction method;line search},
url = {https://academic.hep.com.cn/fmc/EN/abstract/article_24098.shtml},
doi = {10.1007/s11464-018-0739-3}
}    



@incollection{thrun1998learning,
  title={Learning to learn: Introduction and overview},
  author={Thrun, Sebastian and Pratt, Lorien},
  booktitle={Learning to learn},
  pages={3--17},
  year={1998},
  publisher={Springer}
}

@article{baydin2017online,
  title={Online learning rate adaptation with hypergradient descent},
  author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  journal={arXiv preprint arXiv:1703.04782},
  year={2017}
}

@article{moore1993prioritized,
  title={Prioritized sweeping: Reinforcement learning with less data and less time},
  author={Moore, Andrew W and Atkeson, Christopher G},
  journal={Machine learning},
  volume={13},
  number={1},
  pages={103--130},
  year={1993},
  publisher={Springer}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  edition={2nd},
  publisher={MIT press}
}

@misc{https://doi.org/10.48550/arxiv.1206.4655,
  doi = {10.48550/ARXIV.1206.4655},
  
  url = {https://arxiv.org/abs/1206.4655},
  
  author = {Grunewalder, Steffen and Lever, Guy and Baldassarre, Luca and Pontil, Massi and Gretton, Arthur},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Modelling transition dynamics in MDPs with RKHS embeddings},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{course_gd_rate,
  author        = {Mark Schmidt},
  title         = {CPSC 540: Machine Learning,
Rates of Convergence},
note={University of British Columbia},
  year          = {2017},
  howpublished = {\url{https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L5.pdf}}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={983--992},
  year={2019},
  organization={PMLR}
}

@misc{course_gd_rate_bubeck,
  author        = {Sebastien Bubeck},
  title         = {ORF523: Nesterov’s Accelerated Gradient Descent},
note={Princeton University},
  year          = {2013},
  howpublished = {\url{https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/}}
}

@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. akad. nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}

@article{bubeck2015geometric,
  title={A geometric alternative to Nesterov's accelerated gradient descent},
  author={Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
  journal={arXiv preprint arXiv:1506.08187},
  year={2015}
}


@inproceedings{yao2014pseudo,
  title={Pseudo-MDPs and factored linear action models},
  author={Yao, Hengshuai and Szepesv{\'a}ri, Csaba and Pires, Bernardo Avila and Zhang, Xinhua},
  booktitle={2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={1--9},
  year={2014},
  organization={IEEE}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@misc{rkhs-mdps,
  doi = {10.48550/ARXIV.1206.4655},
  
  url = {https://arxiv.org/abs/1206.4655},
  
  author = {Grunewalder, Steffen and Lever, Guy and Baldassarre, Luca and Pontil, Massi and Gretton, Arthur},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Modelling transition dynamics in MDPs with RKHS embeddings},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@TECHREPORT{lspe96,
    author = {Dimitri P. Bertsekas and Sergey Ioffe},
    title = {Temporal differences-based policy iteration and applications in neuro-dynamic programming},
    institution = {MIT},
    year = {1996}
}


@article{lspe03,
  author    = {Angelia Nedic and
               Dimitri P. Bertsekas},
  title     = {Least Squares Policy Evaluation Algorithms with Linear Function Approximation},
  journal   = {Discret. Event Dyn. Syst.},
  volume    = {13},
  number    = {1-2},
  pages     = {79--110},
  year      = {2003},
  url       = {https://doi.org/10.1023/A:1022192903948},
  doi       = {10.1023/A:1022192903948},
  timestamp = {Thu, 14 Oct 2021 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/journals/deds/NedicB03.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{multi-step-dyna,
author = {Yao, Hengshuai and Bhatnagar, Shalabh and Diao, Dongcui},
title = {Multi-Step Linear Dyna-Style Planning},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper we introduce a multi-step linear Dyna-style planning algorithm. The key element of the multi-step linear Dyna is a multi-step linear model that enables multi-step projection of a sampled feature and multi-step planning based on the simulated multi-step transition experience. We propose two multi-step linear models. The first iterates the one-step linear model, but is generally computationally complex. The second interpolates between the one-step model and the infinite-step model (which turns out to be the LSTD solution), and can be learned efficiently online. Policy evaluation on Boyan Chain shows that multi-step linear Dyna learns a policy faster than single-step linear Dyna, and generally learns faster as the number of projection steps increases. Results on Mountain-car show that multi-step linear Dyna leads to much better online performance than single-step linear Dyna and model-free algorithms; however, the performance of multi-step linear Dyna does not always improve as the number of projection steps increases. Our results also suggest that previous attempts on extending LSTD for online control were unsuccessful because LSTD looks infinite steps into the future, and suffers from the model errors in non-stationary (control) environments.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {2187–2195},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@book{bertsekas2012dynamic,
  title={Dynamic programming and optimal control: Volume I},
  author={Bertsekas, Dimitri},
  volume={1},
  year={2012},
  publisher={Athena scientific}
}

@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

@article{atari_marc,
  author    = {Marc G. Bellemare and
               Yavar Naddaf and
               Joel Veness and
               Michael Bowling},
  title     = {The Arcade Learning Environment: An Evaluation Platform for General
               Agents},
  journal   = {CoRR},
  volume    = {abs/1207.4708},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.4708},
  eprinttype = {arXiv},
  eprint    = {1207.4708},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1207-4708.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}


@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}



@article{lee2015variable,
  title={A variable step-size diffusion LMS algorithm for distributed estimation},
  author={Lee, Han-Sol and Kim, Seong-Eun and Lee, Jae-Woo and Song, Woo-Jin},
  journal={IEEE Transactions on Signal Processing},
  volume={63},
  number={7},
  pages={1808--1820},
  year={2015},
  publisher={IEEE}
}

@article{mathews1993stochastic,
  title={A stochastic gradient adaptive filter with gradient adaptive step size},
  author={Mathews, V John and Xie, Zhenhua},
  journal={IEEE transactions on Signal Processing},
  volume={41},
  number={6},
  pages={2075--2087},
  year={1993},
  publisher={IEEE}
}


@article{aboulnasr1997robust,
  title={A robust variable step-size LMS-type algorithm: analysis and simulations},
  author={Aboulnasr, Tyseer and Mayyas, K},
  journal={IEEE Transactions on signal processing},
  volume={45},
  number={3},
  pages={631--639},
  year={1997},
  publisher={IEEE}
}

@inproceedings{NEURIPS2018_a41b3bb3,
 author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Visualizing the Loss Landscape of Neural Nets},
 url = {https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{lin1992experiencereplay,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{zhang2020gendice,
  title={Gendice: Generalized offline estimation of stationary values},
  author={Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2002.09072},
  year={2020}
}

@article{tdgammon,
author = {Tesauro, Gerald},
title = {Temporal Difference Learning and TD-Gammon},
year = {1995},
issue_date = {March 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/203330.203343},
doi = {10.1145/203330.203343},
abstract = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.},
journal = {Commun. ACM},
month = {mar},
pages = {58–68},
numpages = {11}
}

@article{boyan1994generalization,
  title={Generalization in reinforcement learning: Safely approximating the value function},
  author={Boyan, Justin and Moore, Andrew},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}

@article{bertsekas1995counterexample,
  title={A counterexample to temporal differences learning},
  author={Bertsekas, Dimitri P},
  journal={Neural computation},
  volume={7},
  number={2},
  pages={270--279},
  year={1995},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{givchi2015quasi_newtontd,
  title={Quasi newton temporal difference learning},
  author={Givchi, Arash and Palhang, Maziar},
  booktitle={Asian Conference on Machine Learning},
  pages={159--172},
  year={2015},
  organization={PMLR}
}

@article{scherrer2010should_td_mr,
  title={Should one compute the temporal difference fix point or minimize the bellman residual? the unified oblique projection view},
  author={Scherrer, Bruno},
  journal={arXiv:1011.4362},
  year={2010}
}


@article{csawg,
      title={Learning to Accelerate by the Methods of Step-size Planning}, 
      author={Hengshuai Yao},
      year={2022},
      journal={arXiv:2204.01705},
}

@article{white2014factorization,
  title={A factorization perspective for learning representations in reinforcement learning},
  author={White, Martha},
  year={2014}
}


@InProceedings{mspbe_du,
  title = 	 {Stochastic Variance Reduction Methods for Policy Evaluation},
  author =       {Simon S. Du and Jianshu Chen and Lihong Li and Lin Xiao and Dengyong Zhou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1049--1058},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/du17a/du17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/du17a.html},
  abstract = 	 {Policy evaluation is concerned with estimating the value function that predicts long-term values of states under a given policy. It is a crucial step in many reinforcement-learning algorithms. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.}
}


@inproceedings{zhang2021breaking,
  title={Breaking the deadly triad with a target network},
  author={Zhang, Shangtong and Yao, Hengshuai and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={12621--12631},
  year={2021},
  organization={PMLR}
}

@article{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{zhang2020provably,
  title={Provably convergent two-timescale off-policy actor-critic with function approximation},
  author={Zhang, Shangtong and Liu, Bo and Yao, Hengshuai and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={11204--11213},
  year={2020},
  organization={PMLR}
}


@article{bo_gtd,
  title={Regularized off-policy TD-learning},
  author={Liu, Bo and Mahadevan, Sridhar and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  year={2012}
}


@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@article{DBLP:journals/corr/SudreLVOC17,
  author    = {Carole H. Sudre and
               Wenqi Li and
               Tom Vercauteren and
               S{\'{e}}bastien Ourselin and
               M. Jorge Cardoso},
  title     = {Generalised Dice overlap as a deep learning loss function for highly
               unbalanced segmentations},
  journal   = {CoRR},
  volume    = {abs/1707.03237},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.03237},
  archivePrefix = {arXiv},
  eprint    = {1707.03237},
  timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SudreLVOC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/DauphinVCB15,
  author    = {Yann N. Dauphin and
               Harm de Vries and
               Junyoung Chung and
               Yoshua Bengio},
  title     = {RMSProp and equilibrated adaptive learning rates for non-convex optimization},
  journal   = {CoRR},
  volume    = {abs/1502.04390},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.04390},
  archivePrefix = {arXiv},
  eprint    = {1502.04390},
  timestamp = {Mon, 13 Aug 2018 16:48:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DauphinVCB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/DauphinPGCGB14,
  author    = {Yann N. Dauphin and
               Razvan Pascanu and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Kyunghyun Cho and
               Surya Ganguli and
               Yoshua Bengio},
  title     = {Identifying and attacking the saddle point problem in high-dimensional
               non-convex optimization},
  journal   = {CoRR},
  volume    = {abs/1406.2572},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.2572},
  archivePrefix = {arXiv},
  eprint    = {1406.2572},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DauphinPGCGB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/ChoromanskaHMAL14,
  author    = {Anna Choromanska and
               Mikael Henaff and
               Micha{\"{e}}l Mathieu and
               G{\'{e}}rard Ben Arous and
               Yann LeCun},
  title     = {The Loss Surface of Multilayer Networks},
  journal   = {CoRR},
  volume    = {abs/1412.0233},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.0233},
  archivePrefix = {arXiv},
  eprint    = {1412.0233},
  timestamp = {Mon, 13 Aug 2018 16:48:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoromanskaHMAL14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@misc{Kfac,
      title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature}, 
      author={James Martens and Roger Grosse},
      year={2020},
      eprint={1503.05671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{td,
  author =       "Sutton, Richard S.",
  title =        "Learning to Predict By the Methods of Temporal Differences",
  journal =      "Machine Learning",
  year =         "1988",
  volume =    "3",
  number =    "1",
  pages =     "9--44",
  month =     "August",
  publisher = "Springer Netherlands",
  url = "http://www.cs.ualberta.ca/~sutton/papers/sutton-88.pdf",
  bib2html_rescat = "General RL, Function Approximation",
}

@article{shortreed2011informing,
  title={Informing sequential clinical decision-making through reinforcement learning: an empirical study},
  author={Shortreed, Susan M and Laber, Eric and Lizotte, Daniel J and Stroup, T Scott and Pineau, Joelle and Murphy, Susan A},
  journal={Machine learning},
  volume={84},
  number={1-2},
  pages={109--136},
  year={2011},
  publisher={Springer}
}

@article{dollar2011pedestrian,
  title={Pedestrian detection: An evaluation of the state of the art},
  author={Dollar, Piotr and Wojek, Christian and Schiele, Bernt and Perona, Pietro},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={4},
  pages={743--761},
  year={2011},
  publisher={IEEE}
}

@misc{munos2016safe,
      title={Safe and Efficient Off-Policy Reinforcement Learning}, 
      author={Rémi Munos and Tom Stepleton and Anna Harutyunyan and Marc G. Bellemare},
      year={2016},
      eprint={1606.02647},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{levinson2011towards,  author={Levinson, Jesse and Askeland, Jake and Becker, Jan and Dolson, Jennifer and Held, David and Kammel, Soeren and Kolter, J. Zico and Langer, Dirk and Pink, Oliver and Pratt, Vaughan and Sokolsky, Michael and Stanek, Ganymed and Stavens, David and Teichman, Alex and Werling, Moritz and Thrun, Sebastian},  booktitle={2011 IEEE Intelligent Vehicles Symposium (IV)},   title={Towards fully autonomous driving: Systems and algorithms},   year={2011},  volume={},  number={},  pages={163-168},  doi={10.1109/IVS.2011.5940562}}

@article{munos2000study,
  title={A study of reinforcement learning in the continuous case by the means of viscosity solutions},
  author={Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={40},
  number={3},
  pages={265--299},
  year={2000},
  publisher={Springer}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pascanu2014revisiting,
      title={Revisiting Natural Gradient for Deep Networks}, 
      author={Razvan Pascanu and Yoshua Bengio},
      year={2014},
      eprint={1301.3584},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rmsprop,
author={Hinton, Geoffrey},
title ={Lecture 6e RMSprop: Divide the gradient by a running average of its recent magnitude (PDF). p. 26.},
year={Retrieved September 2021}
}

@misc{adadelta,
      title={ADADELTA: An Adaptive Learning Rate Method}, 
      author={Matthew D. Zeiler},
      year={2012},
      eprint={1212.5701},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schaul2013pesky,
      title={No More Pesky Learning Rates}, 
      author={Tom Schaul and Sixin Zhang and Yann LeCun},
      year={2013},
      eprint={1206.1106},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{amsgrad,
title={On the Convergence of Adam and Beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@misc{yoshua_practical,
  doi = {10.48550/ARXIV.1206.5533},
  
  url = {https://arxiv.org/abs/1206.5533},
  
  author = {Bengio, Yoshua},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Practical recommendations for gradient-based training of deep architectures},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@book{luenberger1984linear,
  title={Linear and nonlinear programming},
  author={Luenberger, David G and Ye, Yinyu and others},
  volume={2},
  year={1984},
  publisher={Springer}
}


@article{kolda2003optimization,
  title={Optimization by direct search: New perspectives on some classical and modern methods},
  author={Kolda, Tamara G and Lewis, Robert Michael and Torczon, Virginia},
  journal={SIAM review},
  volume={45},
  number={3},
  pages={385--482},
  year={2003},
  publisher={SIAM}
}


@article{becker1988improving,
  title={Improving the Convergence of Back-Propagation Learning with Second-order Methods},
  author={Becker, Sue and Le Cun, Yann},
  year={1988}
}

@inproceedings{changshui_linesearch,
  title={A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems},
  author={Gong, Pinghua and Zhang, Changshui and Lu, Zhaosong and Huang, Jianhua and Ye, Jieping},
  booktitle={international conference on machine learning},
  pages={37--45},
  year={2013},
  organization={PMLR}
}

@misc{yin1997stochastic,
  title={Stochastic Approximation Algorithms and Applications},
  author={Kushner, Harold J and Yin, George},
  year={1997},
  publisher={Springer, New York}
}

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}

@article{Mirror_prox,
author = {Anatoli Juditsky and Arkadi Nemirovski and Claire Tauvel},
title = {{Solving variational inequalities with stochastic mirror-prox algorithm}},
volume = {1},
journal = {Stochastic Systems},
number = {1},
publisher = {INFORMS Applied Probability Society},
pages = {17 -- 58},
keywords = {large scale stochastic approximation, reduced complexity algorithms for convex optimization, stochastic convex-concave saddle-point problem, Variational inequalities with monotone operators},
year = {2011},
doi = {10.1214/10-SSY011},
URL = {https://doi.org/10.1214/10-SSY011}
}


@book{borkar2008book,
author={Vivek S Borkar}, 
title={Stochastic approximation: a dynamical systems viewpoint}, 
year={2008}
}




@article{saddle_point_Nemirovski,
author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
title = {Robust Stochastic Approximation Approach to Stochastic Programming},
journal = {SIAM Journal on Optimization},
volume = {19},
number = {4},
pages = {1574-1609},
year = {2009},
doi = {10.1137/070704277},

URL = { 
    
        https://doi.org/10.1137/070704277
    
    

},
eprint = { 
    
        https://doi.org/10.1137/070704277
    
    

}
,
    abstract = { In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments. }
}




@article{vaswani2019painless,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{armijo1966minimization,
  title={Minimization of functions having Lipschitz continuous first partial derivatives},
  author={Armijo, Larry},
  journal={Pacific Journal of mathematics},
  volume={16},
  number={1},
  pages={1--3},
  year={1966},
  publisher={Mathematical Sciences Publishers}
}

@article{smd_schraudolph1999local,
  title={Local gain adaptation in stochastic gradient descent},
  author={Schraudolph, Nicol N},
  year={1999},
  publisher={IET}
}

@article{hutter2007temporal,
  title={Temporal difference updating without a learning rate},
  author={Hutter, Marcus and Legg, Shane},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{parameter_free_2022making,
  title={Making SGD Parameter-Free},
  author={Carmon, Yair and Hinder, Oliver},
  journal={arXiv preprint arXiv:2205.02160},
  year={2022}
}

@article{duchi_2019stochastic,
  title={Stochastic (approximate) proximal point methods: Convergence, optimality, and adaptivity},
  author={Asi, Hilal and Duchi, John C},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={3},
  pages={2257--2290},
  year={2019},
  publisher={SIAM}
}

@article{adagrad,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jul,
pages = {2121–2159},
numpages = {39}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@incollection{ljung1998system,
  title={System identification},
  author={Ljung, Lennart},
  booktitle={Signal analysis and prediction},
  pages={163--173},
  year={1998},
  publisher={Springer}
}

@book{simon_book,
author = {Haykin, Simon},
title = {Neural Networks: A Comprehensive Foundation},
year = {1994},
isbn = {0023527617},
publisher = {Prentice Hall PTR},
address = {USA},
edition = {1st},
abstract = {From the Publisher:This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.}
}

@article{franccois2018introduction,
  title={An introduction to deep reinforcement learning},
  author={Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle},
  journal={arXiv preprint arXiv:1811.12560},
  year={2018}
}

@misc{resnet,
  doi = {10.48550/ARXIV.1512.03385},
  
  url = {https://arxiv.org/abs/1512.03385},
  
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Residual Learning for Image Recognition},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{arulkumaran2017brief,
  title={A brief survey of deep reinforcement learning},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={arXiv preprint arXiv:1708.05866},
  year={2017}
}

@misc{online_learning_sgd_regret_tutorial,
author={F. Orabona and A. Cutkosky}, 
title={ICML tutorial on parameter-free stochastic optimization},
year={2020}, 
howpublished={\url{https://parameterfree.com/icml-tutorial/}}
}

@article{chen2020better,
  title={Better Parameter-free Stochastic Optimization with ODE Updates for Coin-Betting},
  author={Chen, Keyi and Langford, John and Orabona, Francesco},
  journal={arXiv preprint arXiv:2006.07507},
  year={2020}
}

@article{jacobs1988increased,
  title={Increased rates of convergence through learning rate adaptation},
  author={Jacobs, Robert A},
  journal={Neural networks},
  volume={1},
  number={4},
  pages={295--307},
  year={1988},
  publisher={Elsevier}
}

@article{curry1944method,
  title={The method of steepest descent for non-linear minimization problems},
  author={Curry, Haskell B},
  journal={Quarterly of Applied Mathematics},
  howpublished={\url{https://cs.uwaterloo.ca/~y328yu/classics/curry.pdf}},
  volume={2},
  number={3},
  pages={258--261},
  year={1944}
}

@inproceedings{two_time_scale_shangtong,
author = {Zhang, Shangtong and Liu, Bo and Yao, Hengshuai and Whiteson, Shimon},
title = {Provably Convergent Two-Timescale off-Policy Actor-Critic with Function Approximation},
year = {2020},
publisher = {JMLR.org},
abstract = {We present the first provably convergent two-timescale off-policy actor-critic algorithm (COF-PAC) with function approximation. Key to COF-PAC is the introduction of a new critic, the emphasis critic, which is trained via Gradient Emphasis Learning (GEM), a novel combination of the key ideas of Gradient Temporal Difference Learning and Emphatic Temporal Difference Learning. With the help of the emphasis critic and the canonical value function critic, we show convergence for COF-PAC, where the critics are linear, and the actor can be nonlinear.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1039},
numpages = {10},
series = {ICML'20}
}

@article{li2017deep,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{bubeck_book,
  doi = {10.48550/ARXIV.1405.4980},
  
  url = {https://arxiv.org/abs/1405.4980},
  
  author = {Bubeck, Sébastien},
  
  keywords = {Optimization and Control (math.OC), Computational Complexity (cs.CC), Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Convex Optimization: Algorithms and Complexity},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{boyd_book,
author = {Boyd, Stephen and Vandenberghe, Lieven},
title = {Convex Optimization},
year = {2004},
isbn = {0521833787},
publisher = {Cambridge University Press},
address = {USA}
}

@book{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}

@article{bertsekas2000gradient,
  title={Gradient convergence in gradient methods with errors},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  journal={SIAM Journal on Optimization},
  volume={10},
  number={3},
  pages={627--642},
  year={2000},
  publisher={SIAM}
}

@article{borkar2000ode,
  title={The {ODE} method for convergence of stochastic approximation and reinforcement learning},
  author={Borkar, Vivek S and Meyn, Sean P},
  journal={SIAM Journal on Control and Optimization},
  volume={38},
  number={2},
  pages={447--469},
  year={2000},
  publisher={SIAM}
}

@article{lspe04,
  title={Improved temporal difference methods with linear function approximation},
  author={Bertsekas, Dimitri P and Borkar, Vivek S and Nedic, Angelia},
  journal={Learning and Approximate Dynamic Programming},
  pages={231--255},
  year={2004},
  publisher={New York: IEEE Press}
}

@ARTICLE{tsi_td,  author={Tsitsiklis, J.N. and Van Roy, B.},  journal={IEEE Transactions on Automatic Control},   title={An analysis of temporal-difference learning with function approximation},   year={1997},  volume={42},  number={5},  pages={674-690},  doi={10.1109/9.580874}}

@inproceedings{santurkar2018does,
  title={How does batch normalization help optimization?},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle={Proceedings of the 32nd international conference on neural information processing systems},
  pages={2488--2498},
  year={2018}
}

@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}


@article{bertsekas_2000_gradient_error,
author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
title = {Gradient Convergence in Gradient methods with Errors},
journal = {SIAM Journal on Optimization},
volume = {10},
number = {3},
pages = {627-642},
year = {2000},
doi = {10.1137/S1052623497331063},

URL = { 
        https://doi.org/10.1137/S1052623497331063
    
},
eprint = { 
        https://doi.org/10.1137/S1052623497331063
    
}
,
    abstract = { We consider the gradient method \$x\_{t+1}=x\_t+\g\_t(s\_t+w\_t)\$, where \$s\_t\$ is a descent direction of a function \$f:\rn\to\re\$ and \$w\_t\$ is a deterministic or stochastic error. We assume that \$\gr f\$ is Lipschitz continuous, that the stepsize \$\g\_t\$ diminishes to 0, and that \$s\_t\$ and \$w\_t\$ satisfy standard conditions. We show that either \$f(x\_t)\to-\infty\$ or \$f(x\_t)\$ converges to a finite value and \$\gr f(x\_t)\to0\$ (with probability 1 in the stochastic case), and in doing so, we remove various boundedness conditions that are assumed in existing results, such as boundedness from below of f, boundedness of \$\gr f(x\_t)\$, or boundedness of xt. }
}


@InProceedings{pmlr-v70-jin17a, title = {How to Escape Saddle Points Efficiently}, author = {Chi Jin and Rong Ge and Praneeth Netrapalli and Sham M. Kakade and Michael I. Jordan}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1724--1732}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/jin17a/jin17a.pdf}, url = { http://proceedings.mlr.press/v70/jin17a.html }, abstract = {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost “dimension-free”). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.} }



@book{polyak_book, 
title = {Introduction to optimization},
series ={ translations series in mathematics and engineering},  
author = {Boris T Polyak},
year = {1987}
} 

@book{saad03:IMS,
  added-at = {2017-06-29T07:13:07.000+0200},
  author = {Saad, Yousef},
  biburl = {https://www.bibsonomy.org/bibtex/29e582a0b82e58d7516f3b855c7e733de/gdmcbain},
  doi = {10.1137/1.9780898718003},
  edition = {Second},
  interhash = {c88e77e3ace202e51fd598c55fe34077},
  intrahash = {9e582a0b82e58d7516f3b855c7e733de},
  isbn = {978-0-89871-534-7},
  keywords = {65f08-preconditioners-for-iterative-methods 65f10-iterative-methods-for-linear-systems 65f50-sparse-matrices},
  posted-at = {2008-02-28 10:11:19},
  priority = {2},
  publisher = {SIAM},
  series = {Other Titles in Applied Mathematics},
  timestamp = {2019-04-16T05:54:53.000+0200},
  title = {{Iterative Methods for Sparse Linear Systems}},
  url = {http://www-users.cs.umn.edu/\~{}saad/IterMethBook\_2ndEd.pdf},
  year = 2003
}

@inproceedings{mr_yao,
  author    = {Hengshuai Yao and
               Zhi{-}Qiang Liu},
  title     = {Minimal Residual Approaches for Policy Evaluation in Large Sparse
               Markov Chains},
  booktitle = {International Symposium on Artificial Intelligence and Mathematics,
               {ISAIM} 2008, Fort Lauderdale, Florida, USA, January 2-4, 2008},
  year      = {2008},
  url       = {http://isaim2008.unl.edu/PAPERS/TechnicalProgram/ISAIM2008\_0009\_77385862752c35a48b3aefe3636066fa.pdf},
  timestamp = {Thu, 12 Mar 2020 11:39:24 +0100},
  biburl    = {https://dblp.org/rec/conf/isaim/YaoL08.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{LMS2,  author={Yao, Hengshuai and Bhatnagar, Shalabh and Szepesvári, Csaba},  booktitle={Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference},   title={LMS-2: Towards an algorithm that is as cheap as LMS and almost as efficient as RLS},   year={2009},  volume={},  number={},  pages={1181-1188},  doi={10.1109/CDC.2009.5400370}}


@inproceedings{ptd_yao,
author = {Yao, Hengshuai and Liu, Zhi-Qiang},
title = {Preconditioned Temporal Difference Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390308},
doi = {10.1145/1390156.1390308},
abstract = {This paper extends many of the recent popular policy evaluation algorithms to a generalized
framework that includes least-squares temporal difference (LSTD) learning, least-squares
policy evaluation (LSPE) and a variant of incremental LSTD (iLSTD). The basis of this
extension is a preconditioning technique that solves a stochastic model equation.
This paper also studies three significant issues of the new framework: it presents
a new rule of step-size that can be computed online, provides an iterative way to
apply preconditioning, and reduces the complexity of related algorithms to near that
of temporal difference (TD) learning.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1208–1215},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@article{Rumelhart:1986we,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
  doi = {10.1038/323533a0},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
  journal = {Nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{Learning Representations by Back-propagating Errors}},
  url = {http://www.nature.com/articles/323533a0},
  volume = 323,
  year = 1986
}

@article{10.1016/S0893-6080(98)00116-6,
author = {Qian, Ning},
title = {On the Momentum Term in Gradient Descent Learning Algorithms},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {12},
number = {1},
issn = {0893-6080},
url = {https://doi.org/10.1016/S0893-6080(98)00116-6},
doi = {10.1016/S0893-6080(98)00116-6},
journal = {Neural Netw.},
month = jan,
pages = {145–151},
numpages = {7},
keywords = {speed of convergence, momentum, learning rate, gradient descent learning algorithm, damped harmonic oscillator, critical damping}
}

@misc{adamw,
  doi = {10.48550/ARXIV.1711.05101},
  url = {https://arxiv.org/abs/1711.05101},
  author = {Loshchilov, Ilya and Hutter, Frank},
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Decoupled Weight Decay Regularization},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Nelder1965ASM,
  title={A Simplex Method for Function Minimization},
  author={John A. Nelder and Roger Mead},
  journal={Comput. J.},
  year={1965},
  volume={7},
  pages={308-313}
}


@inproceedings{loshchilov2011adaptive,
  title={Adaptive coordinate descent},
  author={Loshchilov, Ilya and Schoenauer, Marc and Sebag, Michele},
  booktitle={Proceedings of the 13th annual conference on Genetic and evolutionary computation},
  pages={885--892},
  year={2011}
}
%test optimization functions
@article{10.1093/comjnl/3.3.175,
    author = {Rosenbrock, H. H.},
    title = "{An Automatic Method for Finding the Greatest or Least Value of a Function}",
    journal = {The Computer Journal},
    volume = {3},
    number = {3},
    pages = {175-184},
    year = {1960},
    month = {01},
    abstract = "{The greatest or least value of a function of several variables is to be found when the variables are restricted to a given region. A method is developed for dealing with this problem and is compared with possible alternatives. The method can be used on a digital computer, and is incorporated in a program for Mercury.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/3.3.175},
    url = {https://doi.org/10.1093/comjnl/3.3.175},
    eprint = {https://academic.oup.com/comjnl/article-pdf/3/3/175/988633/030175.pdf},
}

@article{Bukin:1997cu,
    author = "Bukin, A. D.",
    title = "{New minimization strategy for nonsmooth functions}",
    reportNumber = "BUDKER-INP-1997-79",
    month = "10",
    year = "1997"
}

@book{10.5555/40713,
author = {Ackley, David H.},
title = {A Connectionist Machine for Genetic Hillclimbing},
year = {1987},
isbn = {089838236X},
publisher = {Kluwer Academic Publishers},
address = {USA}
}


@MISC{test_functions,
author = {Andrea Gavana},
title = {Global Optimization Test Functions Index},
month = August,
year = {Retrieved 2021},
howpublished={\url{http://infinity77.net/global_optimization/test_functions.html#test-functions-index}}
}

@MISC{test_functions_sfu,
author = {Derek Bingham},
title = {Optimization Test Problems},
month = August,
year = {Retrieved 2021},
howpublished={\url{https://www.sfu.ca/~ssurjano/optimization.html}}
}

@article{rolinek2018l4,
  title={L4: Practical loss-based stepsize adaptation for deep learning},
  author={Rolinek, Michal and Martius, Georg},
  journal={arXiv preprint arXiv:1802.05074},
  year={2018}
}

@article{wojcik2019lossgrad,
  title={LOSSGRAD: automatic learning rate in gradient descent},
  author={W{\'o}jcik, Bartosz and Maziarka, {\L}ukasz and Tabor, Jacek},
  journal={arXiv preprint arXiv:1902.07656},
  year={2019}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{tur20,
title={Project {Turing}},
author={Microsoft},
journal={Microsoft research blog},
year={Feb 2020},
}

@article{yao2018hessian,
  title={Hessian-based analysis of large batch training and robustness to adversaries},
  author={Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1802.08241},
  year={2018}
}

@article{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1605.07110},
  year={2016}
}

@inproceedings{martens2010deep,
  title={Deep learning via hessian-free optimization.},
  author={Martens, James and others},
  booktitle={ICML},
  volume={27},
  pages={735--742},
  year={2010}
}

@misc{bottou2018optimization,
      title={Optimization Methods for Large-Scale Machine Learning}, 
      author={Léon Bottou and Frank E. Curtis and Jorge Nocedal},
      year={2018},
      eprint={1606.04838},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{sagun2017eigenvalues,
      title={Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond}, 
      author={Levent Sagun and Leon Bottou and Yann LeCun},
      year={2017},
      eprint={1611.07476},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{li2020hessian,
  title={Hessian based analysis of sgd for deep nets: Dynamics and generalization},
  author={Li, Xinyan and Gu, Qilong and Zhou, Yingxue and Chen, Tiancong and Banerjee, Arindam},
  booktitle={Proceedings of the 2020 SIAM International Conference on Data Mining},
  pages={190--198},
  year={2020},
  organization={SIAM}
}

@article{bandeira2014convergence,
  title={Convergence of trust-region methods based on probabilistic models},
  author={Bandeira, Afonso S and Scheinberg, Katya and Vicente, Luis Nunes},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={3},
  pages={1238--1264},
  year={2014},
  publisher={SIAM}
}

@article{powell1984global,
  title={On the global convergence of trust region algorithms for unconstrained minimization},
  author={Powell, MJD},
  journal={Mathematical Programming},
  volume={29},
  number={3},
  pages={297--303},
  year={1984},
  publisher={Springer}
}

@misc{qiu2020stochastic,
      title={Stochastic Approximate Gradient Descent via the Langevin Algorithm}, 
      author={Yixuan Qiu and Xiao Wang},
      year={2020},
      eprint={2002.05519},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{yuan1985conditions,
  title={Conditions for convergence of trust region algorithms for nonsmooth optimization},
  author={Yuan, Ya-xiang},
  journal={Mathematical Programming},
  volume={31},
  number={2},
  pages={220--228},
  year={1985},
  publisher={Springer}
}

@misc{sagun2018empirical,
      title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks}, 
      author={Levent Sagun and Utku Evci and V. Ugur Guney and Yann Dauphin and Leon Bottou},
      year={2018},
      eprint={1706.04454},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{conn2000trust,
  title={Trust region methods},
  author={Conn, Andrew R and Gould, Nicholas IM and Toint, Philippe L},
  year={2000},
  publisher={SIAM}
}

@inproceedings{yuan2000review,
  title={A review of trust region algorithms for optimization},
  author={Yuan, Ya-xiang},
  booktitle={Iciam},
  volume={99},
  number={1},
  pages={271--282},
  year={2000}
}

@article{yuan2015recent,
  title={Recent advances in trust region algorithms},
  author={Yuan, Ya-xiang},
  journal={Mathematical Programming},
  volume={151},
  number={1},
  pages={249--281},
  year={2015},
  publisher={Springer}
}

@inproceedings{lihong_kernel_sim,
 author = {Feng, Yihao and Li, Lihong and Liu, Qiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Kernel Loss for Solving the Bellman Equation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/48e59000d7dfcf6c1d96ce4a603ed738-Paper.pdf},
 volume = {32},
 year = {2019}
}


@misc{batch_normalization,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{boundary_tsne,
  doi = {10.48550/ARXIV.2012.10282},
  
  url = {https://arxiv.org/abs/2012.10282},
  
  author = {Chen, Jinyin and Wang, Zhen and Zheng, Haibin and Xiao, Jun and Ming, Zhaoyan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ROBY: Evaluating the Robustness of a Deep Model by its Decision Boundaries},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{linear_interpolation,
title	= {Qualitatively Characterizing Neural Network Optimization Problems},
author	= {Ian Goodfellow and Oriol Vinyals and Andrew Saxe},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6544},
booktitle	= {International Conference on Learning Representations}
}

@misc{DLA,
  doi = {10.48550/ARXIV.1707.06484},
  
  url = {https://arxiv.org/abs/1707.06484},
  
  author = {Yu, Fisher and Wang, Dequan and Shelhamer, Evan and Darrell, Trevor},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Layer Aggregation},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{three_factors,
  author    = {Stanislaw Jastrzebski and
               Zachary Kenton and
               Devansh Arpit and
               Nicolas Ballas and
               Asja Fischer and
               Yoshua Bengio and
               Amos J. Storkey},
  title     = {Three Factors Influencing Minima in {SGD}},
  journal   = {CoRR},
  volume    = {abs/1711.04623},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.04623},
  archivePrefix = {arXiv},
  eprint    = {1711.04623},
  timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-04623.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{svrg,
author = {Johnson, Rie and Zhang, Tong},
title = {Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
pages = {315–323},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@article{GGT_svd_lowrank_fullmatrix,
  author    = {Naman Agarwal and
               Brian Bullins and
               Xinyi Chen and
               Elad Hazan and
               Karan Singh and
               Cyril Zhang and
               Yi Zhang},
  title     = {The Case for Full-Matrix Adaptive Regularization},
  journal   = {CoRR},
  volume    = {abs/1806.02958},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.02958},
  eprinttype = {arXiv},
  eprint    = {1806.02958},
  timestamp = {Mon, 13 Aug 2018 16:46:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-02958.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{drummond2018accelerated,
      title={Accelerated Gradient Methods with Memory}, 
      author={Ross Drummond and Stephen Duncan},
      year={2018},
      eprint={1805.09077},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{hyperparam_gd,
  doi = {10.48550/ARXIV.1502.03492},
  
  url = {https://arxiv.org/abs/1502.03492},
  
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Gradient-based Hyperparameter Optimization through Reversible Learning},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{bengio2000gradient,
  title={Gradient-based optimization of hyperparameters},
  author={Bengio, Yoshua},
  journal={Neural computation},
  volume={12},
  number={8},
  pages={1889--1900},
  year={2000},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{householder_theory,
  title={The Theory of Matrices in Numerical Analysis, Blaisdell, New York},
  author={Householder, ALSTON S},
  journal={MR},
  volume={30},
  pages={5475},
  year={1964}
}

@article{horn1985cr,
  title={Cr johnson matrix analysis},
  author={Horn, Roger A},
  journal={Cambridge UP, New York},
  year={1985}
}

@article{Konda_2004,
	doi = {10.1214/105051604000000116},
  
	url = {https://doi.org/10.1214%2F105051604000000116},
  
	year = 2004,
	month = {may},
  
	publisher = {Institute of Mathematical Statistics},
  
	volume = {14},
  
	number = {2},
  
	author = {Vijay R. Konda and John N. Tsitsiklis},
  
	title = {Convergence rate of linear two-time-scale stochastic approximation},
  
	journal = {The Annals of Applied Probability}
}

@article{bradtke1996linear,
  title={Linear least-squares algorithms for temporal difference learning},
  author={Bradtke, Steven J and Barto, Andrew G},
  journal={Machine learning},
  volume={22},
  number={1},
  pages={33--57},
  year={1996},
  publisher={Springer}
}

@article{konda1999actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{konda2004convergence,
  title={Convergence rate of linear two-time-scale stochastic approximation},
  author={Konda, Vijay R and Tsitsiklis, John N},
  journal={The Annals of Applied Probability},
  volume={14},
  number={2},
  pages={796--819},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}

@article{bhatnagar1998two,
  title={A two timescale stochastic approximation scheme for simulation-based parametric optimization},
  author={Bhatnagar, Shalabh and Borkar, Vivek S},
  journal={Probability in the Engineering and Informational Sciences},
  volume={12},
  number={4},
  pages={519--531},
  year={1998},
  publisher={Cambridge University Press}
}

@article{tsitsiklis1996feature,
  title={Feature-based methods for large scale dynamic programming},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  journal={Machine Learning},
  volume={22},
  number={1},
  pages={59--94},
  year={1996},
  publisher={Springer}
}

@article{bertsekas2015dynamic,
  title={Dynamic programming and optimal control 4th edition, volume ii},
  author={Bertsekas, Dimitri P},
  journal={Athena Scientific},
  year={2015}
}

@incollection{gordon1995stable,
  title={Stable function approximation in dynamic programming},
  author={Gordon, Geoffrey J},
  booktitle={Machine learning proceedings 1995},
  pages={261--268},
  year={1995},
  publisher={Elsevier}
}

@inproceedings{pan2022understanding,
  title={Understanding and Mitigating the Limitations of Prioritized Replay},
  author={Pan, Yangchen and Mei, Jincheng and Farahmand, Amir-massoud and White, Martha and Yao, Hengshuai and Rohani, Mohsen and Luo, Jun},
  booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
  year={2022}
}

@inproceedings{orvieto2020role,
  title={The role of memory in stochastic optimization},
  author={Orvieto, Antonio and Kohler, Jonas and Lucchi, Aurelien},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={356--366},
  year={2020},
  organization={PMLR}
}
@inproceedings{agarwal2018lower,
  title={Lower bounds for higher-order convex optimization},
  author={Agarwal, Naman and Hazan, Elad},
  booktitle={Conference On Learning Theory},
  pages={774--792},
  year={2018},
  organization={PMLR}
}

@article{polyak1964ball,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@incollection{langville2011google,
  title={Google's PageRank and beyond},
  author={Langville, Amy N and Meyer, Carl D},
  booktitle={Google's PageRank and Beyond},
  year={2011},
  publisher={Princeton university press}
}

@inproceedings{jiang_emphatic_drl,
  title={Emphatic algorithms for deep reinforcement learning},
  author={Jiang, Ray and Zahavy, Tom and Xu, Zhongwen and White, Adam and Hessel, Matteo and Blundell, Charles and Van Hasselt, Hado},
  booktitle={International Conference on Machine Learning},
  pages={5023--5033},
  year={2021},
  organization={PMLR}
}

@inproceedings{hallak2016generalized_etd,
  title={Generalized emphatic temporal difference learning: Bias-variance analysis},
  author={Hallak, Assaf and Tamar, Aviv and Munos, R{\'e}mi and Mannor, Shie},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{jiang2022learning_etd,
  title={Learning expected emphatic traces for deep RL},
  author={Jiang, Ray and Zhang, Shangtong and Chelu, Veronica and White, Adam and van Hasselt, Hado},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={7015--7023},
  year={2022}
}

@article{ghiassian2017first_etd,
  title={A first empirical study of emphatic temporal difference learning},
  author={Ghiassian, Sina and Rafiee, Banafsheh and Sutton, Richard S},
  journal={arXiv:1705.04185},
  year={2017}
}

@inproceedings{pan2017accelerated_atd,
  title={Accelerated gradient temporal difference learning},
  author={Pan, Yangchen and White, Adam and White, Martha},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{td_survey,
  author  = {Christoph Dann and Gerhard Neumann and Jan Peters},
  title   = {Policy Evaluation with Temporal Differences: A Survey and Comparison},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {24},
  pages   = {809--883},
  url     = {http://jmlr.org/papers/v15/dann14a.html}
}

@article{td_l1_par,
  title={L1 regularized linear temporal difference learning},
  author={Painter-Wakefield, Christopher and Parr, Ronald and Durham, NC},
  journal={Technical report: Department of Computer Science, Duke University, Durham, NC, TR-2012--01},
  year={2012},
  publisher={Citeseer}
}

@misc{gtd_l1_regularization,
      title={$\ell_1$ Regularized Gradient Temporal-Difference Learning}, 
      author={Dominik Meyer and Hao Shen and Klaus Diepold},
      year={2016},
      eprint={1610.01476},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{kolter2009regularization_td,
  title={Regularization and feature selection in least-squares temporal difference learning},
  author={Kolter, J Zico and Ng, Andrew Y},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={521--528},
  year={2009}
}

@article{ghiassian2021empirical_etd,
      title={An Empirical Comparison of Off-policy Prediction Learning Algorithms in the Four Rooms Environment}, 
      author={Sina Ghiassian and Richard S. Sutton},
      year={2021},
      journal={arXiv:2109.05110},
      primaryClass={cs.LG}
}

@article{zhang2019geoff_first_etd_success,
  title={Generalized off-policy actor-critic},
  author={Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{mahmood2015etd,
  title={Emphatic temporal-difference learning},
  author={Mahmood, Ashique Rupam and Yu, Huizhen and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1507.01569},
  year={2015}
}

@techreport{page1999pagerank,
  title={The PageRank citation ranking: Bringing order to the web.},
  author={Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  year={1999},
  institution={Stanford InfoLab}
}


@article{berkhin2005survey,
  title={A survey on PageRank computing},
  author={Berkhin, Pavel},
  journal={Internet mathematics},
  volume={2},
  number={1},
  pages={73--120},
  year={2005},
  publisher={Taylor \& Francis}
}

@inproceedings{jeh2003scaling,
  title={Scaling personalized web search},
  author={Jeh, Glen and Widom, Jennifer},
  booktitle={Proceedings of the 12th international conference on World Wide Web},
  pages={271--279},
  year={2003}
}

@article{pirouz2017toward,
  title={Toward efficient hub-less real time personalized pagerank},
  author={Pirouz, Matin and Zhan, Justin},
  journal={IEEE Access},
  volume={5},
  pages={26364--26375},
  year={2017},
  publisher={IEEE}
}

@article{park2019survey,
  title={A survey on personalized PageRank computation algorithms},
  author={Park, Sungchan and Lee, Wonseok and Choe, Byeongseo and Lee, Sang-Goo},
  journal={IEEE Access},
  volume={7},
  pages={163049--163062},
  year={2019},
  publisher={IEEE}
}

@article{gleich2006approximating,
  title={Approximating personalized pagerank with minimal use of web graph data},
  author={Gleich, David and Polito, Marzia},
  journal={Internet Mathematics},
  volume={3},
  number={3},
  pages={257--294},
  year={2006},
  publisher={Taylor \& Francis}
}

@article{fogaras2005towards,
  title={Towards scaling fully personalized pagerank: Algorithms, lower bounds, and experiments},
  author={Fogaras, D{\'a}niel and R{\'a}cz, Bal{\'a}zs and Csalog{\'a}ny, K{\'a}roly and Sarl{\'o}s, Tam{\'a}s},
  journal={Internet Mathematics},
  volume={2},
  number={3},
  pages={333--358},
  year={2005},
  publisher={Taylor \& Francis}
}

@article{richardson2001_pagerank,
  title={The intelligent surfer: Probabilistic combination of link and content information in pagerank},
  author={Richardson, Matthew and Domingos, Pedro},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@misc{yao_ranking,
  doi = {10.48550/ARXIV.1303.5988},
  
  url = {https://arxiv.org/abs/1303.5988},
  
  author = {Yao, Hengshuai and Schuurmans, Dale},
  
  keywords = {Information Retrieval (cs.IR), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Reinforcement Ranking},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{xu2002efficient,
  title={Efficient reinforcement learning using recursive least-squares methods},
  author={Xu, Xin and He, Han-gen and Hu, Dewen},
  journal={Journal of Artificial Intelligence Research},
  volume={16},
  pages={259--292},
  year={2002}
}

@article{dayan1994td,
  title={{TD}($\lambda$) converges with probability 1},
  author={Dayan, Peter and Sejnowski, Terrence J},
  journal={Machine Learning},
  volume={14},
  number={3},
  pages={295--301},
  year={1994},
  publisher={Springer}
}

@misc{patterson2021investigating_etd,
  title={Investigating Objectives for Off-policy Value Estimation in Reinforcement Learning},
  author={Patterson, Andrew and Ghiassian, Sina and Gupta, D and White, A and White, M},
  year={2021},
  publisher={Preparation}
}

@article{mahmood2017multi_etd_stilluse_pbe,
  title={Multi-step off-policy learning without importance sampling ratios},
  author={Mahmood, Ashique Rupam and Yu, Huizhen and Sutton, Richard S},
  journal={arXiv preprint arXiv:1702.03006},
  year={2017}
}

@article{imani2018off_martha,
  title={An off-policy policy gradient theorem using emphatic weightings},
  author={Imani, Ehsan and Graves, Eric and White, Martha},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{cao2021gradient_etd,
  title={Gradient temporal-difference learning for off-policy evaluation using emphatic weightings},
  author={Cao, Jiaqing and Liu, Quan and Zhu, Fei and Fu, Qiming and Zhong, Shan},
  journal={Information Sciences},
  volume={580},
  pages={311--330},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{yu2015convergence_etd,
  title={On convergence of emphatic temporal-difference learning},
  author={Yu, Huizhen},
  booktitle={Conference on learning theory},
  pages={1724--1751},
  year={2015},
  organization={PMLR}
}

@misc{sps_stepsize,
      title={Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence}, 
      author={Nicolas Loizou and Sharan Vaswani and Issam Laradji and Simon Lacoste-Julien},
      year={2021},
      eprint={2002.10542},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{polyak_averaging,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@article{bubeck,
author = {Bubeck, S\'{e}bastien},
title = {Convex Optimization: Algorithms and Complexity},
year = {2015},
issue_date = {11 2015},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {8},
number = {3–4},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000050},
doi = {10.1561/2200000050},
journal = {Found. Trends Mach. Learn.},
month = nov,
pages = {231–357},
numpages = {127}
}


@inproceedings{nesterov,
  title={Introductory Lectures on Convex Optimization - A Basic Course},
  author={Y. Nesterov},
  booktitle={Applied Optimization},
  year={2004}
}

@inproceedings{mohammadi2019performance,
  title={Performance of noisy Nesterov's accelerated method for strongly convex optimization problems},
  author={Mohammadi, Hesameddin and Razaviyayn, Meisam and Jovanovi{\'c}, Mihailo R},
  booktitle={2019 American Control Conference (ACC)},
  pages={3426--3431},
  year={2019},
  organization={IEEE}
}

@article{agarwal2009information,
  title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  pages={1--9},
  year={2009}
}

@article{tseng2010approximation,
  title={Approximation accuracy, gradient methods, and error bound for structured convex optimization},
  author={Tseng, Paul},
  journal={Mathematical Programming},
  volume={125},
  number={2},
  pages={263--295},
  year={2010},
  publisher={Springer}
}


@inproceedings{foster2019complexity,
  title={The complexity of making the gradient small in stochastic convex optimization},
  author={Foster, Dylan J and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
  booktitle={Conference on Learning Theory},
  pages={1319--1345},
  year={2019},
  organization={PMLR}
}

@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}

@inproceedings{ilstd,
  title={Incremental least-squares temporal difference learning},
  author={Geramifard, Alborz and Bowling, Michael and Sutton, Richard S},
  booktitle={Proceedings of the National Conference on Artificial Intelligence},
  volume={21},
  number={1},
  pages={356},
  year={2006},
  organization={Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}
}


@incollection{baird1995residual,
  title={Residual algorithms: Reinforcement learning with function approximation},
  author={Baird, Leemon},
  booktitle={Machine Learning Proceedings 1995},
  pages={30--37},
  year={1995},
  publisher={Elsevier}
}

@article{sutton1995generalization,
  title={Generalization in reinforcement learning: Successful examples using sparse coarse coding},
  author={Sutton, Richard S},
  journal={Advances in neural information processing systems},
  volume={8},
  year={1995}
}


@article{lstd,
  title={Technical update: Least-squares temporal difference learning},
  author={Boyan, Justin A},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={233--246},
  year={2002},
  publisher={Springer}
}

@article{gtd,
  title={A convergent {$O(n)$} temporal-difference algorithm for off-policy learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@inproceedings{sutton2011horde,
  title={Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction},
  author={Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={761--768},
  year={2011}
}

@inproceedings{precup2001off,
  title={Off-policy temporal-difference learning with function approximation},
  author={Precup, Doina and Sutton, Richard S and Dasgupta, Sanjoy},
  booktitle={ICML},
  pages={417--424},
  year={2001}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@inproceedings{tdc,
  title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={993--1000},
  year={2009}
}

@incollection{combettes2011proximal,
  title={Proximal splitting methods in signal processing},
  author={Combettes, Patrick L and Pesquet, Jean-Christophe},
  booktitle={Fixed-point algorithms for inverse problems in science and engineering},
  pages={185--212},
  year={2011},
  publisher={Springer}
}
@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}


@article{nesterov2015universal,
  title={Universal gradient methods for convex optimization problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={152},
  number={1},
  pages={381--404},
  year={2015},
  publisher={Springer}
}

@article{necoara2019linear,
  title={Linear convergence of first order methods for non-strongly convex optimization},
  author={Necoara, Ion and Nesterov, Yu and Glineur, Francois},
  journal={Mathematical Programming},
  volume={175},
  number={1},
  pages={69--107},
  year={2019},
  publisher={Springer}
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@book{tang2006spectral,
  title={Spectral and high-order methods with applications},
  author={Tang, Tao},
  year={2006},
  publisher={Science Press Beijing}
}

@book{beck2017first,
  title={First-order methods in optimization},
  author={Beck, Amir},
  year={2017},
  publisher={SIAM}
}


@article{arjevani2016lower,
  title={On lower and upper bounds in smooth and strongly convex optimization},
  author={Arjevani, Yossi and Shalev-Shwartz, Shai and Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={4303--4353},
  year={2016},
  publisher={JMLR. org}
}

@article{gadat2018stochastic,
  title={Stochastic heavy ball},
  author={Gadat, S{\'e}bastien and Panloup, Fabien and Saadane, Sofiane},
  journal={Electronic Journal of Statistics},
  volume={12},
  number={1},
  pages={461--529},
  year={2018},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@inproceedings{sebbouh2021almost,
  title={Almost sure convergence rates for Stochastic Gradient Descent and Stochastic Heavy Ball},
  author={Sebbouh, Othmane and Gower, Robert M and Defazio, Aaron},
  booktitle={Conference on Learning Theory},
  pages={3935--3971},
  year={2021},
  organization={PMLR}
}

@inproceedings{gadat2017non,
  title={Non-asymptotic bound for stochastic averaging},
  author={Gadat, S{\'e}bastien and Panloup, Fabien},
  booktitle={Book of Abstracts},
  pages={56},
  year={2017}
}


@article{sharp_minima,
  added-at = {2017-03-06T15:03:47.000+0100},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {https://www.bibsonomy.org/bibtex/25fb7fbfdd48fe54a4e687fcda87ecfab/bsc},
  interhash = {7a321493ef739181c4d243e2145b0da5},
  intrahash = {5fb7fbfdd48fe54a4e687fcda87ecfab},
  journal = {CoRR},
  keywords = {deep-learning neural-networks},
  timestamp = {2017-03-06T15:03:47.000+0100},
  title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.},
  url = {http://arxiv.org/abs/1609.04836},
  volume = {abs/1609.04836},
  year = 2016
}


@article{minima_valley,
  author    = {Haowei He and
               Gao Huang and
               Yang Yuan},
  title     = {Asymmetric Valleys: Beyond Sharp and Flat Local Minima},
  journal   = {CoRR},
  volume    = {abs/1902.00744},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.00744},
  eprinttype = {arXiv},
  eprint    = {1902.00744},
  timestamp = {Tue, 21 May 2019 18:03:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-00744.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sharp_minima2,
  author    = {Laurent Dinh and
               Razvan Pascanu and
               Samy Bengio and
               Yoshua Bengio},
  title     = {Sharp Minima Can Generalize For Deep Nets},
  journal   = {CoRR},
  volume    = {abs/1703.04933},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.04933},
  eprinttype = {arXiv},
  eprint    = {1703.04933},
  timestamp = {Mon, 13 Aug 2018 16:45:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DinhPBB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{three_factors,
      title={Three Factors Influencing Minima in SGD}, 
      author={Stanisław Jastrzębski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},
      year={2018},
      eprint={1711.04623},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-1711-04623,
  author    = {Stanislaw Jastrzebski and
               Zachary Kenton and
               Devansh Arpit and
               Nicolas Ballas and
               Asja Fischer and
               Yoshua Bengio and
               Amos J. Storkey},
  title     = {Three Factors Influencing Minima in {SGD}},
  journal   = {CoRR},
  volume    = {abs/1711.04623},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.04623},
  eprinttype = {arXiv},
  eprint    = {1711.04623},
  timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-04623.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{increase_batch_size,
      title={Don't Decay the Learning Rate, Increase the Batch Size}, 
      author={Samuel L. Smith and Pieter-Jan Kindermans and Chris Ying and Quoc V. Le},
      year={2018},
      eprint={1711.00489},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{sutton1992adapting,
  title={Adapting bias by gradient descent: An incremental version of delta-bar-delta},
  author={Sutton, Richard S},
  booktitle={AAAI},
  pages={171--176},
  year={1992},
  organization={San Jose, CA}
}


@inproceedings{thrun1996discovering,
  title={Discovering structure in multiple learning tasks: The TC algorithm},
  author={Thrun, Sebastian and O'Sullivan, Joseph},
  booktitle={ICML},
  volume={96},
  pages={489--497},
  year={1996}
}
@misc{mathieu2014fast,
      title={Fast Approximation of Rotations and Hessians matrices}, 
      author={Michael Mathieu and Yann LeCun},
      year={2014},
      eprint={1404.7195},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{NairH10,
  added-at = {2019-04-03T00:00:00.000+0200},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  biburl = {https://www.bibsonomy.org/bibtex/2059683ca9b2457d248942520babbe000/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2010},
  editor = {Fürnkranz, Johannes and Joachims, Thorsten},
  ee = {https://icml.cc/Conferences/2010/papers/432.pdf},
  interhash = {acefcb0a5d1a937232f02f3fe0d5ab86},
  intrahash = {059683ca9b2457d248942520babbe000},
  keywords = {dblp},
  pages = {807-814},
  publisher = {Omnipress},
  timestamp = {2019-04-04T11:48:32.000+0200},
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2010.html#NairH10},
  year = 2010
}

@misc{zhang2021survey,
      title={A Survey on Multi-Task Learning}, 
      author={Yu Zhang and Qiang Yang},
      year={2021},
      eprint={1707.08114},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ge2015escaping,
      title={Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition}, 
      author={Rong Ge and Furong Huang and Chi Jin and Yang Yuan},
      year={2015},
      eprint={1503.02101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{drummond2018accelerated,
      title={Accelerated Gradient Methods with Memory}, 
      author={Ross Drummond and Stephen Duncan},
      year={2018},
      eprint={1805.09077},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{sgd_bounded_variance_faster_than_quadratic_growth,
      title={Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness}, 
      author={Vien V. Mai and Mikael Johansson},
      year={2021},
      eprint={2102.06489},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{grad_clipping22,
  title={Clipped stochastic methods for variational inequalities with heavy-tailed noise},
  author={Gorbunov, Eduard and Danilova, Marina and Dobre, David and Dvurechenskii, Pavel and Gasnikov, Alexander and Gidel, Gauthier},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31319--31332},
  year={2022}
}

@misc{es_abc,
      title={Better Theory for SGD in the Nonconvex World}, 
      author={Ahmed Khaled and Peter Richtárik},
      year={2020},
      eprint={2002.03329},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@InProceedings{abc-gower21a,
  title = 	 { SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation },
  author =       {Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1315--1323},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/gower21a/gower21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/gower21a.html},
  abstract = 	 { Stochastic Gradient Descent (SGD) is being used routinely for optimizing non-convex functions. Yet, the standard convergence theory for SGD in the smooth non-convex setting gives a slow sublinear convergence to a stationary point. In this work, we provide several convergence theorems for SGD showing convergence to a global minimum for non-convex problems satisfying some extra structural assumptions. In particular, we focus on two large classes of structured non-convex functions: (i) Quasar (Strongly) Convex functions (a generalization of convex functions) and (ii) functions satisfying the Polyak-Łojasiewicz condition (a generalization of strongly-convex functions). Our analysis relies on an Expected Residual condition which we show is a strictly weaker assumption than previously used growth conditions, expected smoothness or bounded variance assumptions. We provide theoretical guarantees for the convergence of SGD for different step-size selections including constant, decreasing and the recently proposed stochastic Polyak step-size. In addition, all of our analysis holds for the arbitrary sampling paradigm, and as such, we give insights into the complexity of minibatching and determine an optimal minibatch size. Finally, we show that for models that interpolate the training data, we can dispense of our Expected Residual condition and give state-of-the-art results in this setting. }
}

@article{es_generalization_louzou,
  author       = {Nicolas Loizou and
                  Hugo Berard and
                  Gauthier Gidel and
                  Ioannis Mitliagkas and
                  Simon Lacoste{-}Julien},
  title        = {Stochastic Gradient Descent-Ascent and Consensus Optimization for
                  Smooth Games: Convergence Analysis under Expected Co-coercivity},
  journal      = {CoRR},
  volume       = {abs/2107.00052},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.00052},
  eprinttype    = {arXiv},
  eprint       = {2107.00052},
  timestamp    = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-00052.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sgd_vr_survey,
  author       = {Robert M. Gower and
                  Mark Schmidt and
                  Francis R. Bach and
                  Peter Richt{\'{a}}rik},
  title        = {Variance-Reduced Methods for Machine Learning},
  journal      = {CoRR},
  volume       = {abs/2010.00892},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.00892},
  eprinttype    = {arXiv},
  eprint       = {2010.00892},
  timestamp    = {Sun, 02 Oct 2022 15:32:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-00892.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{schmit_PL,
  author       = {Hamed Karimi and
                  Julie Nutini and
                  Mark Schmidt},
  title        = {Linear Convergence of Gradient and Proximal-Gradient Methods Under
                  the Polyak-{\L}ojasiewicz Condition},
  journal      = {CoRR},
  volume       = {abs/1608.04636},
  year         = {2016},
  url          = {http://arxiv.org/abs/1608.04636},
  eprinttype    = {arXiv},
  eprint       = {1608.04636},
  timestamp    = {Thu, 23 Apr 2020 11:53:21 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KarimiNS16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
  
@article{rakhlin2011making_boundedg2,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1109.5647},
  year={2011}
}

@article{hazan2014beyond_boundedg2,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  publisher={JMLR. org}
}

@misc{niu2011hogwild_boundedg2,
      title={HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent}, 
      author={Feng Niu and Benjamin Recht and Christopher Re and Stephen J. Wright},
      year={2011},
      eprint={1106.5730},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{li2021improved,
      title={Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints}, 
      author={Shaojie Li and Yong Liu},
      year={2021},
      eprint={2107.08686},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gradient_clipping23,
      title={Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees}, 
      author={Anastasia Koloskova and Hadrien Hendrikx and Sebastian U. Stich},
      year={2023},
      eprint={2305.01588},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{seg_louzou22,
      title={Stochastic Extragradient: General Analysis and Improved Rates}, 
      author={Eduard Gorbunov and Hugo Berard and Gauthier Gidel and Nicolas Loizou},
      year={2022},
      eprint={2111.08611},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{allenzhu2018katyusha,
      title={Katyusha: The First Direct Acceleration of Stochastic Gradient Methods}, 
      author={Zeyuan Allen-Zhu},
      year={2018},
      eprint={1603.05953},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}


@misc{defazio2014_saga,
      title={SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives}, 
      author={Aaron Defazio and Francis Bach and Simon Lacoste-Julien},
      year={2014},
      eprint={1407.0202},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{roux2013stochastic_sag,
      title={A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets}, 
      author={Nicolas Le Roux and Mark Schmidt and Francis Bach},
      year={2013},
      eprint={1202.6258},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{ma2018power_batchsize,
      title={The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning}, 
      author={Siyuan Ma and Raef Bassily and Mikhail Belkin},
      year={2018},
      eprint={1712.06559},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{singla2019understanding,
      title={Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation}, 
      author={Sahil Singla and Eric Wallace and Shi Feng and Soheil Feizi},
      year={2019},
      eprint={1902.00407},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bach_adam,
      title={A Simple Convergence Proof of Adam and Adagrad}, 
      author={Alexandre Défossez and Léon Bottou and Francis Bach and Nicolas Usunier},
      year={2020},
      eprint={2003.02395},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@article{flat_minima_97,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@misc{chen2022sufficiency,
      title={The Sufficiency of Off-Policyness and Soft Clipping: {PPO} is still Insufficient according to an Off-Policy Measure}, 
      author={Xing Chen and Dongcui Diao and Hechang Chen and Hengshuai Yao and Haiyin Piao and Zhixiao Sun and Zhiwei Yang and Randy Goebel and Bei Jiang and Yi Chang},
      year={2022},
      eprint={2205.10047},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{10.1023/A:1007379606734,
author = {Caruana, Rich},
title = {Multitask Learning},
year = {1997},
issue_date = {July 1997},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1007379606734},
doi = {10.1023/A:1007379606734},
abstract = {Multitask Learning is an approach to inductive transfer that improves
generalization by using the domain information contained in the
training signals of related tasks as an inductive bias. It
does this by learning tasks in parallel while using a shared
representation; what is learned for each task can help other tasks be
learned better. This paper reviews prior work on MTL, presents new
evidence that MTL in backprop nets discovers task relatedness without
the need of supervisory signals, and presents new results for MTL
with k-nearest neighbor and kernel regression. In this paper we
demonstrate multitask learning in three domains. We explain how
multitask learning works, and show that there are many opportunities
for multitask learning in real domains. We present an algorithm and
results for multitask learning with case-based methods like k-nearest
neighbor and kernel regression, and sketch an algorithm for multitask
learning in decision trees. Because multitask learning works, can be
applied to many different kinds of domains, and can be used with
different learning algorithms, we conjecture there will be many
opportunities for its use on real-world problems.},
journal = {Mach. Learn.},
month = jul,
pages = {41–75},
numpages = {35},
keywords = {kernel regression, multitask learning, inductive transfer, k-nearest neighbor, generalization, backpropagation, parallel transfer, supervised learning}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}


@misc{vanschoren2018metalearning,
      title={Meta-Learning: A Survey}, 
      author={Joaquin Vanschoren},
      year={2018},
      eprint={1810.03548},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{googlenet,
  doi = {10.48550/ARXIV.1409.4842},
  
  url = {https://arxiv.org/abs/1409.4842},
  
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Going Deeper with Convolutions},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{VGG,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

@ARTICLE{Vilalta02aperspective,
    author = {Ricardo Vilalta and Youssef Drissi},
    title = {A Perspective View And Survey Of Meta-Learning},
    journal = {Artificial Intelligence Review},
    year = {2002},
    volume = {18},
    pages = {77--95}
}


@article{doya2002metalearning,
  title={Metalearning and neuromodulation},
  author={Doya, Kenji},
  journal={Neural networks},
  volume={15},
  number={4-6},
  pages={495--506},
  year={2002},
  publisher={Elsevier}
}

@article{schweighofer2003meta,
  title={Meta-learning in reinforcement learning},
  author={Schweighofer, Nicolas and Doya, Kenji},
  journal={Neural Networks},
  volume={16},
  number={1},
  pages={5--9},
  year={2003},
  publisher={Elsevier}
}

@article{schraudolph1999local,
  title={Local gain adaptation in stochastic gradient descent},
  author={Schraudolph, Nicol N},
  year={1999},
  publisher={IET}
}

@misc{gower2018_es,
      title={Stochastic Quasi-Gradient Methods: Variance Reduction via Jacobian Sketching}, 
      author={Robert M. Gower and Peter Richtárik and Francis Bach},
      year={2018},
      eprint={1805.02632},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{gower2021stochastic_es_jacobian,
  title={Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching},
  author={Gower, Robert M and Richt{\'a}rik, Peter and Bach, Francis},
  journal={Mathematical Programming},
  volume={188},
  pages={135--192},
  year={2021},
  publisher={Springer}
}

@misc{duboistaine2021svrg,
      title={SVRG Meets AdaGrad: Painless Variance Reduction}, 
      author={Benjamin Dubois-Taine and Sharan Vaswani and Reza Babanezhad and Mark Schmidt and Simon Lacoste-Julien},
      year={2021},
      eprint={2102.09645},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{1261952,
  author={Mandic, D.P.},
  journal={IEEE Signal Processing Letters}, 
  title={A generalized normalized gradient descent algorithm}, 
  year={2004},
  volume={11},
  number={2},
  pages={115-118},
  doi={10.1109/LSP.2003.821649}}
  
@ARTICLE{143435,
  author={Kwong, R.H. and Johnston, E.W.},
  journal={IEEE Transactions on Signal Processing}, 
  title={A variable step size LMS algorithm}, 
  year={1992},
  volume={40},
  number={7},
  pages={1633-1642},
  doi={10.1109/78.143435}}
  
@article{step-size_error_t_t-1,
author = {Aboulnasr, Tyseer and Mayyas, K.},
year = {1997},
month = {04},
pages = {631 - 639},
title = {A robust variable step-size LMS-type algorithm: Analysis and simulations},
volume = {45},
journal = {Signal Processing, IEEE Transactions on},
doi = {10.1109/78.558478}
}

@misc{sag,
      title={A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets}, 
      author={Nicolas Le Roux and Mark Schmidt and Francis Bach},
      year={2013},
      eprint={1202.6258},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{sdca,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  year={2013}
}


@Article{Rastrigin1963,
  author   = {L. A. Rastrigin},
  title    = {The convergence of the random search method in the extremal control of many-parameter system},
  journal  = {Automation and Remote Control},
  year     = {1963},
  volume   = {24},
  number   = {10},
  pages    = {1337--1342},
  url     = {https://scholar.google.com/scholar?cluster=1484480983410715230},
  language = {Russian},
}

@misc{lecun_hessian,
      title={Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond}, 
      author={Levent Sagun and Leon Bottou and Yann LeCun},
      year={2017},
      eprint={1611.07476},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{vinyals2011krylov,
      title={Krylov Subspace Descent for Deep Learning}, 
      author={Oriol Vinyals and Daniel Povey},
      year={2011},
      eprint={1111.4259},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Pearlmutter,
author = {Pearlmutter, Barak A.},
title = {Fast Exact Multiplication by the Hessian},
year = {1994},
issue_date = {Jan. 1994},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {6},
number = {1},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1994.6.1.147},
doi = {10.1162/neco.1994.6.1.147},
journal = {Neural Comput.},
month = jan,
pages = {147–160},
numpages = {14}
}

@article{schumer1968adaptive,
  title={Adaptive step size random search},
  author={Schumer, MA and Steiglitz, Kenneth},
  journal={IEEE Transactions on Automatic Control},
  volume={13},
  number={3},
  pages={270--276},
  year={1968},
  publisher={IEEE}
}

@misc{step_size_BB_SGD,
  doi = {10.48550/ARXIV.1605.04131},
  
  url = {https://arxiv.org/abs/1605.04131},
  
  author = {Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Barzilai-Borwein Step Size for Stochastic Gradient Descent},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


%LMS analysis: not cited, but good to read for analysis
@article{gardner1984learning,
  title={Learning characteristics of stochastic-gradient-descent algorithms: A general study, analysis, and critique},
  author={Gardner, William A},
  journal={Signal processing},
  volume={6},
  number={2},
  pages={113--133},
  year={1984},
  publisher={Elsevier}
}


@article{young2018metatrace,
  title={Metatrace: Online step-size tuning by meta-gradient descent for reinforcement learning control},
  author={Young, Kenny and Wang, Baoxiang and Taylor, Matthew E},
  journal={arXiv preprint arXiv:1805.04514},
  year={2018}
}

@article{dyna,
author = {Sutton, Richard S.},
title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
year = {1991},
issue_date = {Aug. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {0163-5719},
url = {https://doi.org/10.1145/122344.122377},
doi = {10.1145/122344.122377},
journal = {SIGART Bull.},
month = jul,
pages = {160–163},
numpages = {4}
}

@inproceedings{lin_dyna,
author = {Sutton, Richard S. and Szepesv\'{a}ri, Csaba and Geramifard, Alborz and Bowling, Michael},
title = {Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping},
year = {2008},
isbn = {0974903949},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
pages = {528–536},
numpages = {9},
location = {Helsinki, Finland},
series = {UAI'08}
}

@article{papari2011waypoint,
  title={Waypoint averaging and step size control in learning by gradient descent},
  author={Papari, G and Bunte, K and Biehl, M},
  journal={Machine Learning Reports},
  volume={6},
  pages={16},
  year={2011}
}

@misc{jain2018accelerating,
      title={Accelerating Stochastic Gradient Descent For Least Squares Regression}, 
      author={Prateek Jain and Sham M. Kakade and Rahul Kidambi and Praneeth Netrapalli and Aaron Sidford},
      year={2018},
      eprint={1704.08227},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{10.1137/0330046,
author = {Polyak, B. T. and Juditsky, A. B.},
title = {Acceleration of Stochastic Approximation by Averaging},
year = {1992},
issue_date = {July 1992},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {30},
number = {4},
issn = {0363-0129},
url = {https://doi.org/10.1137/0330046},
doi = {10.1137/0330046},
journal = {SIAM J. Control Optim.},
month = jul,
pages = {838–855},
numpages = {18},
keywords = {recursive estimation, optimal algorithms, stochastic approximation, stochastic optimization}
}

@article{mandic2004generalized,
  title={A generalized normalized gradient descent algorithm},
  author={Mandic, Danilo P},
  journal={IEEE signal processing letters},
  volume={11},
  number={2},
  pages={115--118},
  year={2004},
  publisher={IEEE}
}

% this is an interesting paper! not cited, but need to read!
@article{schraudolph1999local,
  title={Local gain adaptation in stochastic gradient descent},
  author={Schraudolph, Nicol N},
  year={1999},
  publisher={IET}
}

@inproceedings{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  booktitle={Advances in neural information processing systems},
  pages={3981--3989},
  year={2016}
}

@article{gower2019sgd_general,
      title={{SGD}: General Analysis and Improved Rates}, 
      author={Robert Mansel Gower and Nicolas Loizou and Xun Qian and Alibek Sailanbayev and Egor Shulgin and Peter Richtarik},
      year={2019},
      journal={arXiv:1901.09401},
      primaryClass={cs.LG}
}

@inproceedings{harris2002evidence,
  title={Evidence that incremental delta-bar-delta is an attribute-efficient linear learner},
  author={Harris, Harlan D},
  booktitle={European Conference on Machine Learning},
  pages={135--147},
  year={2002},
  organization={Springer}
}

@misc{agarwal2017lower,
      title={Lower Bounds for Higher-Order Convex Optimization}, 
      author={Naman Agarwal and Elad Hazan},
      year={2017},
      eprint={1710.10329},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{wright2015coordinate,
  title={Coordinate descent algorithms},
  author={Wright, Stephen J},
  journal={Mathematical Programming},
  volume={151},
  number={1},
  pages={3--34},
  year={2015},
  publisher={Springer}
}